{
  "title": "A Natural Interaction Method of Multi-Sensory Channels for Virtual Assembly System of Power Transformer Control Cabinet",
  "url": "https://openalex.org/W3011981524",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2125200169",
      "name": "Xuqiang Shao",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2142217383",
      "name": "Xiaohua Feng",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A3010993073",
      "name": "Yelu Yu",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2096792895",
      "name": "Zhaohui Wu",
      "affiliations": [
        "China Academy of Transportation Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2121060548",
      "name": "Peng Mei",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2125200169",
      "name": "Xuqiang Shao",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2142217383",
      "name": "Xiaohua Feng",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A3010993073",
      "name": "Yelu Yu",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2096792895",
      "name": "Zhaohui Wu",
      "affiliations": [
        "China Academy of Transportation Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2121060548",
      "name": "Peng Mei",
      "affiliations": [
        "North China Electric Power University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964192401",
    "https://openalex.org/W2889474126",
    "https://openalex.org/W2921633749",
    "https://openalex.org/W2728904064",
    "https://openalex.org/W2217371521",
    "https://openalex.org/W2570080371",
    "https://openalex.org/W2321079037",
    "https://openalex.org/W2082586922",
    "https://openalex.org/W1984187457",
    "https://openalex.org/W2885046344",
    "https://openalex.org/W6983351932",
    "https://openalex.org/W2784300307",
    "https://openalex.org/W2604426688",
    "https://openalex.org/W2014991284",
    "https://openalex.org/W1547309983",
    "https://openalex.org/W2942824218",
    "https://openalex.org/W2968095115",
    "https://openalex.org/W2889399702",
    "https://openalex.org/W2968299162",
    "https://openalex.org/W2968796459",
    "https://openalex.org/W2888821279",
    "https://openalex.org/W2102380381",
    "https://openalex.org/W2563841464",
    "https://openalex.org/W2953260964",
    "https://openalex.org/W2981899786",
    "https://openalex.org/W2807228823",
    "https://openalex.org/W2802068533",
    "https://openalex.org/W2810491376",
    "https://openalex.org/W2132952740",
    "https://openalex.org/W2888953485",
    "https://openalex.org/W2889026885",
    "https://openalex.org/W2889283093",
    "https://openalex.org/W2532372538",
    "https://openalex.org/W2797552926",
    "https://openalex.org/W2751398839"
  ],
  "abstract": "The more sensory channels are equipped in a virtual assembly system, the more real users feel in the whole system, however, most of the existing virtual assembly systems are based on the natural interaction method of one or two sensory channels. Thus, this paper proposes a novel virtual assembly system integrating multi-sensory channels, including gesture interaction, Chinese speech interaction, tactile interaction, 3D display and real-time display of real environment pictures in the virtual environment. To improve the operability of the virtual environment, we analyze the parallel virtual assembly sequence on the basis of two-hand interaction, and the assembly priority is prompted based on UI interface. For ease of operation, we present a method of viewpoint control based on gesture interaction and the coordinate threshold of spatial position. A hierarchical bounding box collision detection algorithm based on volume difference is proposed to improve the efficiency of collision feedback and collision avoidance. In addition, the power equipment models are exhibited in the virtual scene, as the exhibits of the virtual roaming process. Finally, to evaluate the training effect of this system, a comparative experiment is designed to compare the participants' experience and the effect of assembly training. The experimental results show that the virtual assembly system of natural interaction with multi-sensory channels is flexible and immersive.",
  "full_text": "Received February 26, 2020, accepted March 13, 2020, date of publication March 17, 2020, date of current version March 27, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.2981539\nA Natural Interaction Method of Multi-Sensory\nChannels for Virtual Assembly System of\nPower Transformer Control Cabinet\nXUQIANG SHAO\n1, XIAOHUA FENG\n 1, YELU YU1, ZHAOHUI WU\n 2, AND PENG MEI\n1\n1School of Control and Computer Engineering, North China Electric Power University at Baoding, Baoding 071000, China\n2China Academy of Transportation Sciences, Beijing 100019, China\nCorresponding author: Xiaohua Feng (1064522174@qq.com)\nThis work was supported in part by the National Natural Science Foundation of China under Grant 61502168, in part by the Fundamental\nResearch Funds for the Central Universities under Grant 2018MS068, and in part by the Beijing Natural Science Foundation Project under\nGrant 4182018.\nABSTRACT The more sensory channels are equipped in a virtual assembly system, the more real users\nfeel in the whole system, however, most of the existing virtual assembly systems are based on the natural\ninteraction method of one or two sensory channels. Thus, this paper proposes a novel virtual assembly\nsystem integrating multi-sensory channels, including gesture interaction, Chinese speech interaction, tactile\ninteraction, 3D display and real-time display of real environment pictures in the virtual environment.\nTo improve the operability of the virtual environment, we analyze the parallel virtual assembly sequence\non the basis of two-hand interaction, and the assembly priority is prompted based on UI interface. For ease\nof operation, we present a method of viewpoint control based on gesture interaction and the coordinate\nthreshold of spatial position. A hierarchical bounding box collision detection algorithm based on volume\ndifference is proposed to improve the efﬁciency of collision feedback and collision avoidance. In addition,\nthe power equipment models are exhibited in the virtual scene, as the exhibits of the virtual roaming process.\nFinally, to evaluate the training effect of this system, a comparative experiment is designed to compare the\nparticipants’ experience and the effect of assembly training. The experimental results show that the virtual\nassembly system of natural interaction with multi-sensory channels is ﬂexible and immersive.\nINDEX TERMS Virtual assembly, natural interaction, virtual reality, multisensory channel.\nI. INTRODUCTION\nWith the continuous development of human-computer inter-\naction technology, virtual reality technology has been inte-\ngrated into various industries, such as military, medical,\nindustrial production and manufacturing, interior design and\nso on [1]–[4]. Virtual assembly is a typical application of\nvirtual reality technology in industrial production and manu-\nfacturing. The virtual operating space is built in the computer\nby the accurate 3D digital modeling of equipment compo-\nnents, and the assembly of these components is completed in a\nvirtual environment with the help of natural human-computer\ninteraction technology. At present, most of the equipment\nassembly training for enterprises is based on hands-on train-\ning which requires speciﬁc training ground and installed\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Chua Chin Heng Matthew\n.\nequipment. And these conditions lead to higher training cost\ncompared with the virtual one. Taking the virtual assembly as\nthe main or auxiliary method for personnel assembly training\nand simulating the whole process of equipment assembly\noperation on the computer will greatly overcome the con-\nstraints of time and space, so as to improve the efﬁciency of\ntraining and save resources.\nIn the virtual assembly systems, immersion and interac-\ntivity have always been highly concerned. Highly realistic\nscene construction and model simulation of components are\nthe key roles to improve the immersion of the whole system,\nincluding accurate model size, real surface rendering and\nnatural human-computer interaction. At present, the virtual\nassembly systems mainly complete the whole simulated oper-\nation process by one or two human-computer interaction\nmodes, while people in the real environment can receive\nthe environmental information through a variety of sensory\nVOLUME 8, 2020 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 54699\nX. Shaoet al.: Natural Interaction Method of Multi-Sensory Channels for Virtual Assembly System\nchannels such as sight, hearing, touch, smell, etc. So, the\nlimitation of the sensory channels will directly affect the\nimmersive experience.\nTo solve the problems mentioned above, this paper takes\nthe control cabinet of power transformer as the virtual object\nto assemble, and proposes a natural interaction method\nof multi-sensory channels for the virtual assembly system.\nMeanwhile, this method will adopt various interactive ways,\nsuch as speech interaction, gesture interaction, embodied\nand tangible interaction, stereo vision, dynamic tracking,\nreal-time video transmission of virtual reality combina-\ntion, etc. The system mainly involves the virtual roaming,\nthe object selection and collection, translation of models,\nthe collision response and assembly sequence optimization\nof modeling components in the virtual assembly environ-\nment. So as to enhance the sense of reality of the system,\nreduce the assembly time of learners and improve the learning\nefﬁciency.\nII. RELATED WORK\nAt present, the commonly used virtual reality interactive\nmodes include the gesture interaction supported by Leap\nMotion, the somato-sensory interaction of the head-mounted\ndisplay and virtual handle supported by Oculus Rift or HTC\nVive, the posture recognition interaction supported by Kinect,\nthe speech recognition interaction and so on. The equipment\nof Leap Motion is small and inexpensive, and it can accurately\nrecognize palms and ﬁngers, even including the shape of\nﬁnger joints, the curvature of ﬁngers and the extension and\ngrip of palms. Oculus Rift and HTC Vive equip the head-\nmounted display and virtual handle buttons with the func-\ntion of spatial orientation and vibration feedback interaction.\nMoreover, through the functions of handle buttons, the system\ncan recognize gestures indirectly, which provides ﬂexible\ninteraction modes to users for picking, shifting and rotating\ntasks in virtual assembly scenes. Kinect collects human body\nimages and extracts features to identify human bones and\npostures. Besides, some UI-based display interactions and\nalgorithm-based implicit interactions also have many appli-\ncations.\nIn recent years, many scholars in the ﬁeld of vir-\ntual reality have been studying the application of natu-\nral human-computer interaction in virtual reality scenes.\nRobert et al.[5] ﬁrstly integrated the head-mounted display\nwith spatial positioning, motion capture and inertial sensing\nin a virtual reality scene. They realized the stereoscopic dis-\nplay and broke through the visual interaction of traditional\ndesktop display. Li et al. [6] proposed a cutting system of\nmouse ovarian based on Leap Motion’s simulation of two-\nhanded operation, and developed a training and learning\nsystem of virtual surgery. In addition, the Leap Motion has\nmany applications in upper limb rehabilitation and medical\nsurgery [7]–[9]. In order to solve the occlusion problem in\ngesture recognition of Leap Motion, Sun et al.[10] proposed\ntwo collaborative algorithms for Leap Motion, and effectively\nsolved the recognition limitation which caused by occlusion.\nDing et al. [11] proposed a hand movement simulation on\nthe basis of visual interaction. They realized the application\nof gesture interaction in the virtual rehabilitation system\nthrough feature extraction and gesture recognition of hand.\nTsuda et al.[12] realized the virtual writing system by hand\nmovement calculation, and the system was used for writing\ntraining. Yao et al.[13] proposed a virtual simulation system\nthat supported simultaneous interaction between stereoscopic\nhelmet of Oculus Rift and Kinect. The system combined\nvisual interaction and gesture interaction. They achieved the\nnatural interaction of two sensory channels in the virtual\nsimulation system of cutter simulation dredger. The combi-\nnation of stereo vision and other interactive methods is also\na research hotspot [14], [15]. Zhang et al. [16] put forward\na ship virtual ﬁre training system based on HMD and UI\ninteractive modes. They improved the ﬂexibility and immer-\nsion of the system through the spatial positioning, dynamic\ntracking of the head-mounted display and the interaction\nof UI interface. They designed the scene interaction based\non the hand-held controller and head-mounted display. The\nscene switching and mode switching was provided by the UI\ninterface. Liu et al. [17] created a virtual assembly method\nof Chinese speech interaction. They comprehensively used\nKinect’s functions of bone tracking and speech recognition.\nThey converted speeches into Chinese words and matched\nthe instructions. The instructions were simulated to keyboard\noperations to ﬁnish tasks. The Chinese speech recognition\nresults are inserted into the message queue as a keyboard\nmessage, therefore a virtual assembly system based on speech\nrecognition was implemented. In many other virtual reality\nstudies, speech recognition technology was used to enhance\nthe intelligence of the system [18], [19]. Qi et al. [20]\nput forward a virtual roaming system based on the HMD\nstereoscopic display and eye-tracking interactive mode. They\nrealized the redirection algorithm and analyzed the viewpoint\nby tracking the eyes in real-time. Therefore, users can roam\naround in a large virtual environment in a small physical\nspace combining with the. Hirtd et al. [21] proposed an\ninteractive mode combining the HMD stereo display with the\nSLAM visual recognition. SLAM generated a 3D model of\nthe real environment, and transmitted it into the virtual scene\nin real-time. Thus the system realized the fusion of virtual and\nreal environment. It is worth mentioning that mixed reality\nsynchronizes the virtual world with the real world. They\nenhanced the reality of the virtual system through real-time\ndata transmission [22], [23]. Martinez et al.[24] proposed the\ncontactless tactile feedback that can simultaneously interacte\nLeap Motion’s gesture recognition with laser tactile system.\nThey improved the immersion of system by Leap Motion’s\nprecise gesture recognition interaction and touchless tactile\nsimulation of laser array. Besides, Mendes et al. [25] real-\nized the selection and control of objects in the virtual scene\nbased on the head-mounted display. In addition to the virtual\nhead-mounted display, the UI interface interaction and touch\ninteraction of the 3D digital display have also been used\nin the virtual reality environment. Gugenheimer et al. [26]\n54700 VOLUME 8, 2020\nX. Shaoet al.: Natural Interaction Method of Multi-Sensory Channels for Virtual Assembly System\nimplemented the human-machine interaction through touch\non a ﬁxed 3D display in a mobile virtual environment.\nMenzner et al.[27] proposed to utilize touch-sensitive phys-\nical keyboards for text entry as an alternative sensing mech-\nanism. They presented a ﬁrst prototype for VR by tracking\nuser’s ﬁngertips. Because of its strong operability, touch\ninteraction has been widely used in various virtual reality\nresearches in recent years [28], [29]. Existing tactile interac-\ntions are based on vibrations, pressure, electrical stimulation,\nand force feedback [30]–[33].\nDu and Zhang [34] and Du et al. [35], [36] proposed\na Markerless Human-Manipulator Interface with Kalman\nFilter, Particle Filter, improved Particle Filter and adaptive\nmultispace transfor-mation. The method effectively over-\ncomed the measurement errors that increase over time due to\nthe noise of the devices and the tracking error. They solved the\nissue that the translation error increases in a short period of\ntime when the sensor fails to sense hand movement. They also\nproposed an AMT method to assist the operator in improving\nthe accuracy and reliability of determining the movement\nof the robot. Zhang and Wang et al. [37], [38] proposed to\nrecognize human behavior with wireless technology, such\nas Wi-Fi RF signals and Wi-Fi CSI. They detected human\nactivities and behavioral statuses such as respiration rate,\ngestures, and falls by analyzing the received wireless signal\npatterns and characteristics.\nOnly one or two sensory channels for human-computer\ninteraction have been pervasively used in previous studies,\nwhile the multiple sensory channels need receiving external\ninformation in the real world, so the natural interaction of\nmulti-sensory channels can better enhance the immersion\nexperience of the virtual environment. This paper hopes to\ndesign a virtual assembly system, which will integrate var-\nious natural interaction modes of sensory channels, make\nfull use of the advantages of different interactive devices\nand methods, and improve the operability and immersion\nexperience of the system. Meanwhile, the system possesses\nthe virtual roaming modules to exhibit multiple models of\npower equipment for visitors. In this paper, a virtual assem-\nbly environment is built based on Unity platform, and a\nvariety of interactive methods are adopted, mainly including\nLeap Motion, Oculus Rift CV1, Chinese speech recognition,\nreal-time video transmission and so on. Besides, the assembly\nsequence optimization, real-time collision response and col-\nlision avoidance are also employed to improve the system’s\nﬂuency for the optimiza-tion of the assembly sequence.\nIII. SYSTEM DESIGN\nThe virtual assembly environment of the control cabinets of\npower transformer has two functions: virtual roaming and\nvirtual assembly. It is mainly oriented to the assembly train-\ning of the control cabinets. The system mainly purposes to\nimprove the effectiveness of training and promote the appli-\ncability of the system. We select the control cabinets of power\ntransformers as virtual assembly objects, and the system\nFIGURE 1. The virtual assembly system framework.\ncontains three stages as follows: the component modeling,\nthe construction of virtual assembly scene and multi-sensory\nchannels interaction. As shown in ﬁgure 1, the natural inter-\naction modes include the gesture interaction, stereo display of\nVR helmet, virtual operating handle, Chinese speech recogni-\ntion, real-time video interaction and vibration-based somato-\nsensory interaction.\nChoosing 3DS MAX as the modeling tool, this paper\nadopts a modeling method based on the design drawings\nof the equipment. And we model the components in accord\nwith their real sizes to improve the dimensional accuracy\nand minimize the error that caused by modeling. Meanwhile,\nthe surface of the device models is precisely rendered accord-\ning to its appearance in the actual environment. After that,\nit converts the models into FBX format and uploads it into the\nUnity platform. To improve the sense of reality of the whole\nvirtual environment, we construct the simulation of virtual\nassembly scene, and reasonably set up the lighting, shadow\nand shielding effect in the platform. And it regards Microsoft\nVisual Studio 2017 as a script editing tool to integrate the\nprocedures of system. The whole virtual assembly scene\nincludes the following parts:\n(1)Unassembled components model of transformer control\ncabinet and the whole model of assembled transformer con-\ntrol cabinet.\n(2)Stimulating the real environment in the Unity platform\nwith lighting setting, shadow setting and surface rendering,\nas shown in ﬁgure 2.\n(3)Models such as transformers, reactors, current trans-\nformers, capacitors and generators are placed in the exhibi-\ntion area for participants to visit during the virtual touring,\nas shown in ﬁgure 3.\nVOLUME 8, 2020 54701\nX. Shaoet al.: Natural Interaction Method of Multi-Sensory Channels for Virtual Assembly System\nFIGURE 2. Virtual assembly scene.\nFIGURE 3. Electrical equipment model.\n(4)Displaying the real-time video of the real external envi-\nronment on the screen in virtual assembly scene.\n(5)The natural interaction methods of multi-sensory chan-\nnels including Leap Motion, Oculus Rift CV1, Chinese\nspeech recognition and real-time video of the real external\nenvironment, etc.\nIV. PARALLEL ASSEMBLY SEQUENCE PRIORITY\nThrough analyzing the overall structure and composition\nof components of the power transformer control cabinets,\nwe present the main components after disassembling in\nﬁgure 4. As the device owns a tremendous amount of compo-\nnents, the priority of the assembly sequence is determined by\nthe hierarchical and constraint relationship of the component\nmodels’ assembly. The topological diagram of the connected\nrelationship of the main components is shown in ﬁgure 5. The\nsystem supports the simultaneous interaction of two hands.\nThe priority analysis of parallel assembly is performed on the\ncomponent models, as shown in ﬁgure 6.\nThe control cabinet of power transformer is made up of\nnumerous components, thus, the virtual assembly system pro-\nvides the assembly priority prompt based on the UI interface,\nas shown in ﬁgure 7. The priority diagram of parallel assem-\nbly of main components is provided for the UI interface.\nIn the UI interface, users can trigger the events when they\nFIGURE 4. Main components of transformer control cabinet.\nFIGURE 5. Topological diagram of connection relationship of main parts.\nclick the ‘‘A key’’ on the right handle of Oculus. Moreover,\nin order to avoid the wrong operation of ‘‘pick up’’, the UI\ninterface is not displayed when other key events of the handle\nexist.\nV. NATURAL INTERACTION METHOD\nThe proposed system mainly provides two functions: virtual\nroaming and virtual assembly. In the process of virtual roam-\ning, a variety of electric equipments are exhibited. And the\n54702 VOLUME 8, 2020\nX. Shaoet al.: Natural Interaction Method of Multi-Sensory Channels for Virtual Assembly System\nFIGURE 6. Parallel assembly component priority.\nFIGURE 7. Assembly priority prompt based on UI interface.\noverall composition of equipment will be clearly observed.\nThe virtual assembly process is mainly to select and assemble\nthe parts of the power transformer control cabinet into whole.\nBesides, a variety of interactive methods are adopted, mainly\nincluding Leap Motion, Oculus Rift CV1, Chinese speech\nrecognition, real-time video transmission and so on.\nA. COLLISION DETECTION\nIn order to improve the accuracy and calculation rate of\ncollision detection, we proposed a hierarchical bounding box\nalgorithm based on volume difference. When two objects\nwith large volume differences collide with each other, it will\ncause a waste of computing resources if the bounding boxes\nof two objects are divided into synchronous hierarchies,\nTherefore, we divide the bounding boxes hierarchy of large\nobjects based on octree structure. The collision detection\nis carried out for the sub-bounding box and the smaller\nobjects. And this process is repeated, as shown in ﬁgure 8.\nWhen the volume of the sub-bounding box is less than\nFIGURE 8. Collision detection of bounding box based on volume\ndifference.\n1cm×1cm×1cm, the calculation is completed and the col-\nlision result is determined.\nB. OCULUS RIFT BASED INTERACTION\nCompared with Oculus Rift CV1 and HTC Vive, Oculus Rift\nCV1 has the advantages of higher spatial positioning accu-\nracy, more ﬂexible and portative, and lower require-ments for\nphysical environment. Therefore, Oculus Rift CV1 is selected\nas the main interactive device. In the system, the Oculus\nofﬁcial SDK is employed in the Unity platform and the input\nand output interfaces are conﬁgured.\nThe Oculus Rift CV1 head-mounted display is connected\nto a PC through three USB interface and an HDM1 interface.\nThe head-mounted display provides the stereoscopic display\nof a virtual scene and uses active optical positioning technol-\nogy to obtain the device’s location. Through infrared lights\nand the utilization of the PnP algorithm, it transforms the\nphysical coordinates of the head-mounted display into world\ncoordinates in a virtual assembly environment. The real-time\nstereo vision interaction and stereo auditory interaction are\nalso provided. The Oculus Rift CV1 handle is used to drive\nthe virtual hand model in the virtual assembly. Moreover,\nthe pressing, touching and releasing of the handle buttons\nVOLUME 8, 2020 54703\nX. Shaoet al.: Natural Interaction Method of Multi-Sensory Channels for Virtual Assembly System\nFIGURE 9. Virtual hand model gesture.\ncontrol the bending degree of the ﬁngers, so as to identify the\nstate of ﬁngers. In addition, the component models should be\ndesigned into rigid body attributes with bounding box. When\nthe virtual hand model has three ﬁngers in a gripped state and\ninterferes with the bounding box of the component models,\nthe component models will be ‘‘picked’’. In the ‘‘picking’’\nstate, the component models follow the virtual hand model to\ntranslate and rotate. In this way, the picking, translation and\nrotation operations of the component models in the virtual\nassembly scene based on two-hand interaction are realized,\nas shown in ﬁgure 9.\nThe operation of ‘‘pick up’’ is mainly realized through\nthe synchronous transformation of the component models’\ncoordinates following up the virtual hand model coordinates.\nThe coordinate transformation of the virtual hand model is\ncalculated by the following formula:\n(X2,Y2,z2) =(X1,Y1,z1) ∗T ∗R(θ)\nHerein, T is the translation matrix and R(θ ) is the rota-\ntion matrix. During the ‘‘pick up’’ process, compared with\nthe virtual hand model, the local coordinates of the target\ncomponent model remain unchanged, as shown in ﬁgure 10.\nWhen the virtual hand model moves and rotates, the world\ncoordinates of the component models are calculated accord-\ning to the local coordinates and the world coordinates of the\nvirtual hand model, which means that the component models\nare taken as the sub-objects of the virtual hand model. The\ntranslation and rotation of the component models is realized\nthrough following up the virtual hand model.\nA vibrating motor is built into the handles of Oculus Rift\nCV1 to provide the vibration feedback. When collision occurs\nbetween the component models in the virtual assembly scene,\nthe tactile feedback is given to operators through vibration.\nThe vibration triggering and frequency control are imple-\nmented through the SteamVR interface. When the component\nmodels are colliding with each other, their attributes of rigid\nbody will be disabled by a corresponding script to avoid\nfurther deepening caused by the collision, so that the model\nFIGURE 10. Schematic diagram of part model of power transformer\ncontrol cabinet.\ncannot be ‘‘picked up’’. When picking up the model again,\nusers can press the ‘‘X key’’ of the left handle to wake up the\nattribute of rigid body of the component models.\nEssentially, the movement of an object in the virtual assem-\nbly scene is the change of its own world coordinates in each\nframe. The collision cannot be determined when the bounding\nbox is in contact with each other, but the moment when the\ncomponent models of cabinet collide can be taken as the key\nframe where the interference is detected.\nC. LEAP MOTION BASED INTERACTION\nIn the virtual assembly scene of this system, the exhibition\narea shows various models of power equipments, such as\ntransformers, reactors, current transformers, capacitors, and\ngenerators. These equips provide for users to take a virtual\nroaming. In the process of virtual assembly, it is also nec-\nessary to move the viewpoint of camera to the vicinity of\nthe selected models when ‘‘picking up’’ a component model.\nTherefore, the system in this paper should be equipped with\na virtual roaming module. Since the Leap Motion is small\nand convenient. And it is accurate for the gesture recognition\nand the calculation of the gesture positioning. The system\nadopts the gesture interaction based on Leap Motion to realize\nthe virtual roaming operations. The algorithm ﬂow is shown\nin ﬁgure 11.\nWe perform the gesture recognition, and convert the degree\nof ﬁngers’ curvature into a speciﬁc value, with the fully\nextended palm deﬁned as 1 and the clenched ﬁst deﬁned as 0.\nWhen the palm is fully extended, the virtual camera is allowed\nto make the spatial position of the palm. Especially, users\ncan drive the virtual camera to roam according to the spatial\nposition of the palm. The roaming is terminated in the state of\nclenched ﬁst. In previous studies, the virtual roaming based\non the interactive mode of Leap Motion mostly controlled\nthe changes of viewpoint by identifying the moving modes\nof hand, such as forward, backward, up and down. However,\ndue to the limitation of recognition space and the instability\nof hands’ moving speed, the accuracy of recognition is insuf-\nﬁcient.\nTherefore, this paper proposes a virtual roaming control\nmethod of Leap Motion on the basis of spatial position\nthreshold and gesture recognition. Leap Motion provides the\ngesture sensing space of 4 cubic feet. Taking Leap Motion\n54704 VOLUME 8, 2020\nX. Shaoet al.: Natural Interaction Method of Multi-Sensory Channels for Virtual Assembly System\nFIGURE 11. The algorithm flow chart of virtual roam by Leap Motion.\nFIGURE 12. Palm space and threshold space of Leap Motion.\nequipment as the origin of coordinates, we establish the right\ncoordinate system and deﬁne the threshold values of x−, x+,\ny−, y+, z−, z+in different coordinate axis directions. When\nthe coordinates of the spatial position of the palm exceed\nthe threshold value in a certain direction, the virtual camera\nis driven by the script to translate in that direction. That is,\nwe realize the virtual roaming in the scene by uniform coor-\ndinate transformation. The setting of threshold also provides\n‘‘free space’’ for participants to interact with gestures, which\nis conducive to more ﬂexible and convenient control of the\nvirtual camera. The recognizable space of Leap Motion is\nshown in the red border area in ﬁgure 12. Figure 13 shows\nthe state of gesture recognition.\nFIGURE 13. Virtual roam based on gesture recognition of Leap Motion.\nD. CHINESE SPEECH INTERACTION\nIn the system’s virtual assembly process, two main methods\nfor selecting component models are provided. One is the\nvirtual roaming function implemented by the leap motion as\nmentioned above. The virtual player moves to the vicinity\nof the unassembled component models in the virtual envi-\nronment, and directly grabs the object through the virtual\nhand driven by the Oculus Rift CV1 handle. Another object\nselection method is based on the Chinese speech interaction.\nAs we know, the control cabinet of a transformer has\nnumerous models, and each component model owns differ-\nent sizes. It is more convenient and ﬂexible to ‘‘pick up’’\nthe larger component model objects directly with the virtual\nroaming handle. However, the extra operating time will be\nincreased when players pick up other small-sized component\nmodels. Especially, when the model distance is getting close,\nthe insufﬁcient accuracy of ‘‘pick up’’ by the virtual hand will\nlead to the wrong operation, so the Chinese speech interaction\nis used as an auxiliary method to select objects to improve\noperability.\nWe select the BaiduAPI as the Chinese speech recognition\ninterface in this paper. We control the start and end of speech\nrecognition by the ‘‘Trigger key’’ on the right handle of Ocu-\nlus Rift CV1. The speech information is collected through\nthe microphone and stored in memory. The acquired speech\ninformation is recognized through the BaiduAPI interface.\nAnd the information returned in the form of Chinese char-\nacters. By comparing the feedback of partial speech contents\nwith the keywords of components’ names in the information\nbase, the targeted component models can be chosen. We pick\nup and assemble them in the virtual hand model, and move\nthem to the assembly location by coordination transforma-\ntion. The algorithm ﬂow is shown in ﬁgure 14.\nE. REAL-TIME VIDEO INTERACTION\nHerein, the system is oriented to train staff. Training and\nlearning are divided into two methods. The instructors teach\nstudents by their own operation while the students is watching\nVOLUME 8, 2020 54705\nX. Shaoet al.: Natural Interaction Method of Multi-Sensory Channels for Virtual Assembly System\nFIGURE 14. A schematic diagram of object selection based on Chinese\nspeech recognition.\nand learning. And the students also could operate and learn\nin the system. During the instructor’s teaching process of\nassembly, they cannot view the student’s learning status\nbecause of the covering of head-mounted display. To solve\nthe above problems, the WebCamTexture interface of Unity\n3D platform can be used. We utilize the camera to collect\nreal environment images in real time, and transmit them to\nthe virtual assembly environment in real time. The received\nimage is rendered as a texture to the physical object sur-\nface in the virtual assembly environment by the C# script,\nas given in ﬁgure 15. Continuous rendering is performed at\nthe speed of 25 frames per second. The three-way interactions\nthat between the real environment, virtual environment and\noperators in the form of video are realized, as shown in\nﬁgure 16.\nThe instructor can make a real-time observation of the\nstudents’ status in the virtual scene and point out the key\npoints of virtual assembly. Therefore the teaching quality of\nvirtual assembly is improved.\nVI. EXPERIMENTAL DESIGN\nThe results of the comparative experiment of the virtual\nassembly system which for teaching and training can be\nFIGURE 15. The external scene image is imported into the virtual\nassembly scene in real time.\nFIGURE 16. Algorithm flow chart of real-time video interaction between\nvirtual scene and external scene.\nanalyzed in ﬁve aspects: assembly operation time, learning\nexperience, immersion experience, the operability and the\nvertigo of system. A virtual assembly system that only sup-\nports Leap Motion interaction and a virtual assembly system\nthat only supports Oculus Rift interaction are respectively\nset as the experimental groups of comparison. Both systems\nhave the real-time video access modules for external scenes.\nThis experiment recruits 60 volunteers (male: 36; female: 24),\naged between 22-40 years old. All volunteers have never\nreceived any form of study on the control cabinet of power\ntransformer, and they have no theoretical basis of virtual\n54706 VOLUME 8, 2020\nX. Shaoet al.: Natural Interaction Method of Multi-Sensory Channels for Virtual Assembly System\nFIGURE 17. Bar chart of mean completion time of virtual tour.\nreality technology. All volunteers will be uniformly explained\nabout the device. We divide them into 3 groups of 10 people\nand ensure the smallest difference in age and gender.\nA. PROCESS OF EXPERIMENTS\nThe ﬁrst group is trained for 1 hour to operate the virtual\nassembly system with gesture interaction and virtual roaming\nfunction based on Leap Motion. After the training, the vol-\nunteers will experience the virtual roaming and assembly\nby themselves, and their experiences and ﬁnish time will be\nrecorded.\nThe second group is taught for 1 hour about the virtual\nroaming and assembly based on the Oculus Rift CV1. After\nthe training is completed, the volunteers will experience the\nvirtual roaming and assembly by themselves, and their expe-\nriences and completion time will be recorded.\nThe third group is trained for 1 hour about the operation of\nthe virtual assembly system of natural interaction with multi-\nsensory channels. After the training, the volunteers will act in\nthe same way as the ﬁrst group.\nThe operation tasks for volunteers consist of two parts:\nvirtual roaming and virtual assembly. The virtual roaming\nprocess requires the volunteers to visit each model of power\nequipment in the virtual environment. They start from the ini-\ntial position and return to it after ﬁnishing the virtual roaming.\nIn addition, the virtual assembly process needs volunteers\nto complete the assembly of the entire transformer control\ncabinet.\nB. EXPERIMENTAL RESULT\nThe time that the three groups cost to complete the virtual\nroaming and assembly is shown in table 1. From the bar\ncharts of average completion time of volunteers in each group\nin ﬁgure 17 and ﬁgure 18, it is clearly presented that the third\ngroup spent the shortest time in total, and they take the short-\nest time to complete the virtual assembly. Besides, the ﬁrst\ngroup completes the virtual roaming in the shortest time,\nwhile they spent the longest time on the virtual assembly. The\nwhole completion time of the second group ranks the second\nFIGURE 18. Bar chart of mean completion time of virtual assembly.\nTABLE 1. The summary of Volunteer operation completion time.\nin all experimental groups. The experiment shows that the\nvirtual assembly system with multiple interactive modes has\nthe highest teaching efﬁciency, more ﬂexible and convenient\noperation. And it is easier for trainees to learn.\nTo make the results more objective and convincing, three\ngroups are trained about the operation of the other two\nvirtual assembly systems. For example, the ﬁrst group is\ntrained in the virtual assembly system based on Oculus Rift\nCV1 and the virtual assembly system of natural interaction\nVOLUME 8, 2020 54707\nX. Shaoet al.: Natural Interaction Method of Multi-Sensory Channels for Virtual Assembly System\nFIGURE 19. Experimental evaluation results of virtual assembly system.\nwith multi-sensory channels. After the training, each vol-\nunteer completes the virtual roaming process and virtual\nassembly respectively in the other two virtual assembly sys-\ntems. After the experiment, each volunteer is interviewed\nfor the immersion, learning experience, the operability and\nthe vertigo of each system. We analyze the interview results\nand choose the system with the best learning experience,\nthe strongest sense of immersion, the best operability or the\nweakest sense of vertigo.\nThe results of the volunteers’ evaluation on the learning\nexperience, immersion, operability and sense of vertigo in the\nthree experimental systems are given in ﬁgure 19 separately.\nIt can be seen that 67% volunteers think that the learning\nexperience in the virtual assembly system of natural interac-\ntion with multi-sensory channels is the best. Moreover, 83%\nvolunteers agree that the virtual assembly system of natural\ninteraction with multi-sensory channels owns the strongest\nimmersion, 87% volunteers agree that the multi-sensory\nchannels virtual assembly system has the best operability, and\n53% volunteers feel the weakest sense of vertigo in the pro-\ncess of experiment. The experiment reﬂects that the virtual\nassembly system of natural interaction with multi-sensory\nchannels surpasses the traditional virtual assembly system\nof single interaction, promotes the teaching efﬁciency and\nincreases the immersion of system.\nVII. CONCLUSION\nTaking the control cabinet of power transformer as the object\nof virtual assembly, this paper proposes a natural interaction\nmethod integrating multi-sensory channels, which greatly\nimproves the immersion of the virtual assembly system.\nBesides, we improve the assembly efﬁciency through analyz-\ning the parallel assembly sequence, and we provide prompt\nby the assembly priority based on UI interface. A colli-\nsion response mechanism based on vibration sensation and\na collision avoidance method based on controlling object’s\nattribute are implemented for the virtual assembly system.\nWhat’s more, a viewpoint control method based on gesture\ninteraction and coordinate threshold of spatial position is put\nforward to enhance the ﬂuency of virtual roaming operation.\nWe build the virtual assembly environ-ment with the Unity\nengine, and accomplish the organic integration of interactive\nmodes with C# scripts, such as stereo display, stereo sound,\ngesture interaction, tactile feedback, two-handed interaction\nbased on virtual handles and Chinese speech recognition, etc.\nThe experiment embodies that the virtual assembly system\nof natural interaction with multi-sensory channels is more\nﬂexible, more immersive, easier to operate and more con-\nducive for device assembly learners than the systems of single\ninteraction.\nIn order to further improve the sense of reality of the\nsystem, we consider to integrate more natural interaction\nmodes into the multi-sensory system in the future, such\nas olfactory interaction and force feedback. Human behav-\nior recognition with wireless technology and Markerless\nHuman-Manipulator Interface with Kalman Filter will also\nbe our further research direction.\nREFERENCES\n[1] W. Viyanon and S. Sasananan, ‘‘Usability and performance of the leap\nmotion controller and oculus rift for interior decoration,’’ in Proc. Int. Conf.\nInf. Comput. Technol. (ICICT), DeKalb, IL, USA, Mar. 2018, pp. 47–51.\n[2] J. Rodrigues, P. Menezes, and M. T. Restivo, ‘‘Travelling in a virtual city:\nA physical exercise promoting game,’’ in Proc. 5th Exp. Int. Conf. (exp.at),\nFunchal, Portugal, Jun. 2019, pp. 256–257.\n[3] K. Takahashi, D. Mikami, M. Isogawa, Y . Kusachi, and N. Saijo,\n‘‘VR-based batter training system with motion sensing and performance\nvisualization,’’ in Proc. IEEE Conf. Virtual Reality 3D User Interfaces\n(VR), Osaka, Japan, Mar. 2019, pp. 1353–1354.\n[4] W. Huang, ‘‘Evaluating the effectiveness of head-mounted display virtual\nreality (HMD VR) environment on Students’ learning for a virtual collabo-\nrative engineering assembly task,’’ in Proc. IEEE Conf. Virtual Reality 3D\nUser Interfaces (VR), Reutlingen, Germany, Mar. 2018, pp. 827–829.\n[5] R. W. Lindeman, J. L. Sibert, and J. K. Hahn, ‘‘Towards usable VR:\nAn empirical study of user interfaces for immersive virtual environments,’’\nin Proc. SIGCHI Conf. Hum. Factors Comput. Syst. (CHI), New York, NY ,\nUSA, 1999, pp. 64–71.\n[6] H. Li, M. Yang, J. Hu, J. Hao, Y . Zhai, W. Qie, and G. Yang, ‘‘Algorithm\nof Simulation of Cutting Mouse’s Ovary Based on Leap Motion,’’ J. Syst.\nSimul., vol. 28, no. 9, pp. 2207–2213, 2016.\n[7] J. E. Naranjo, F. Urrutia Urrutia, M. V . Garcia, F. Gallardo-Cardenas,\nT. O. Franklin, and E. Lozada-Martinez, ‘‘User experience evaluation of\nan interactive virtual reality-based system for upper limb rehabilitation,’’ in\nProc. 6th Int. Conf. eDemocracy eGovernment (ICEDEG), Quito, Ecuador,\nApr. 2019, pp. 328–333.\n[8] R. G. Lupu, N. Botezatu, F. Ungureanu, D. Ignat, and A. Moldoveanu,\n‘‘Virtual reality based stroke recovery for upper limbs using leap motion,’’\nin Proc. 20th Int. Conf. Syst. Theory, Control Comput. (ICSTCC), Sinaia,\nRomania, Oct. 2016, pp. 295–299.\n[9] J. Cecil, A. Gupta, and M. Pirela-Cruz, ‘‘Design of VR based ortho-\npedic simulation environments using emerging technologies,’’ in Proc.\nAnnu. IEEE Int. Syst. Conf. (SysCon), Vancouver, BC, Canada, Apr. 2018,\npp. 1–7.\n[10] G. Sun, P. Huang, Y . Liu, and R. Liang, ‘‘Interactive 3D visualization with\ndual leap motions,’’ J. Comput.-Aided Des. Comput. Graph., vol. 30, no. 7,\npp. 1268–1275, 2018.\n[11] W. Ding, Y . Dai, Y . Su, X. Cao, and X. Li, ‘‘Low cost virtual rehabilitation\nsystem of arm based on vision interaction,’’ J. Syst. Simul., vol. 24, no. 9,\npp. 2027–2029, 2012.\n[12] N. Tsuda, A. Morikawa, Y . Nomura, and N. Kato, ‘‘Pressure presenta-\ntion strength for calligraphy brushwork instruction,’’ in Proc. IEEE Int.\nSymp. Robot. Intell. Sensors (IRIS), Ottawa, ON, Canada, Oct. 2017,\npp. 198–202.\n[13] P. Yao, Z. Chen, J. Tong, and L. Qian, ‘‘Virtual simulation system of\ncutter suction dredger based on unity 3D,’’ J. Syst. Simul., vol. 28, no. 9,\npp. 2069–2075 and 2084, 2016.\n54708 VOLUME 8, 2020\nX. Shaoet al.: Natural Interaction Method of Multi-Sensory Channels for Virtual Assembly System\n[14] S. Gunkel, M. Prins, H. Stokking, and O. Niamut, ‘‘WebVR meets\nWebRTC: Towards 360-degree social VR experiences,’’ in Proc. IEEE\nVirtual Reality (VR), Los Angeles, CA, USA, 2017, pp. 457–458.\n[15] D. Zielinski, B. Macdonald, and R. Kopper, ‘‘Comparative study of input\ndevices for a VR mine simulation,’’ in Proc. IEEE Virtual Reality (VR),\nMinneapolis, MN, USA, Mar. 2014, pp. 125–126.\n[16] B. Zhang, J. Sun, L. Shang, H. Xu, and C. Yang, ‘‘Design and realization\nof ship virtual ﬁre training system based on HMD,’’ J. Syst. Simul., vol. 31,\nno. 1, pp. 43–52, 2019.\n[17] X. Liu, L. Wan, H. Ji, and Z. Miao, ‘‘Research on virtual assembly\ntechnology based on Chinese speech interaction,’’ J. Syst. Simul., vol. 26,\nno. 9, pp. 2056–2061, 2014.\n[18] S. L. Bernadin, R. Patel, and E. Smith, ‘‘Work-in-progress: Evaluating the\nperformance of voice recognition approaches for autonomous vehicular\nsystems,’’ in Proc. SoutheastCon, Fort Lauderdale, FL, USA, Apr. 2015,\npp. 1–2.\n[19] W. Park, D. Park, B. Ahn, S. Kang, H. Kim, R. Kim, and J. Na, ‘‘Interactive\nAI for linguistic education built on VR environment using user gener-\nated contents,’’ in Proc. 21st Int. Conf. Adv. Commun. Technol. (ICACT),\nPyeongChang Kwangwoon_Do, South Korea, Feb. 2019, pp. 385–389.\n[20] S. Qi, K. Arie, and P. Anjul, ‘‘Towards virtual reality inﬁnite walking,’’\nACM Trans. Graph., vol. 37, no. 4, pp. 1–13, 2018.\n[21] C. Hirt, M. Zank, and A. Kunz, ‘‘Preliminary environment mapping for\nredirected walking,’’ in Proc. IEEE Conf. Virtual Reality 3D User Inter-\nfaces (VR), Reutlingen, Germany, Mar. 2018, pp. 573–574.\n[22] Z. Shujun, ‘‘An engine of virtual reality mixing environment based on\nreal-time modeling and interaction,’’ in Proc. IEEE Int. Symp. VR Innov.,\nSingapore, Mar. 2011, pp. 155–159.\n[23] Z. Zhang, B. Cao, D. Weng, Y . Liu, Y . Wang, and H. Huang, ‘‘Evaluation\nof hand-based interaction for near-ﬁeld mixed reality with optical see-\nthrough head-mounted displays,’’ in Proc. IEEE Conf. Virtual Reality 3D\nUser Interface (VR), Reutlingen, Germany, Mar. 2018, pp. 739–740.\n[24] J. Martinez, D. Grifﬁths, V . Biscione, O. Georgiou, and T. Carter, ‘‘Touch-\nless haptic feedback for supernatural VR experiences,’’ in Proc. IEEE Conf.\nVirtual Reality 3D User Interface (VR), Reutlingen, Germany, Mar. 2018,\npp. 629–630.\n[25] D. Mendes, F. M. Caputo, A. Giachetti, A. Ferreira, and J. Jorge, ‘‘A survey\non 3D virtual object manipulation: From the desktop to immersive virtual\nenvironments,’’ Comput. Graph. Forum, vol. 38, no. 1, pp. 21–45, 2019,\ndoi: 10.1111/cgf.13390.\n[26] J. Gugenheimer, D. Dobbelstein, C. Winkler, G. Haas, and E. Rukzio,\n‘‘FaceTouch: Enabling touch interaction in display ﬁxed UIs for mobile\nvirtual reality,’’ inProc. 29th Annu. Symp. Interface Softw. Technol. (UIST),\nNew York, NY , USA, 2016, pp. 49–60.\n[27] T. Menzner, A. Otte, T. Gesslein, J. Grubert, P. Gagel, and D. Schneider,\n‘‘A capacitive-sensing physical keyboard for VR text entry,’’ in Proc. IEEE\nConf. Virtual Reality 3D User Interface (VR), Osaka, Japan, Mar. 2019,\npp. 1080–1081.\n[28] K. E. L. Palmerius and J. Lundberg, ‘‘Interaction design for selection and\nmanipulation on immersive touch table display systems for 3D geographic\nvisualization,’’ inProc. IEEE Conf. Virtual Reality 3D User Interface (VR),\nOsaka, Japan, Mar. 2019, pp. 1064–1065.\n[29] T. Jung and P. Bauer, ‘‘3D Touch-and-drag: Gesture-free 3D manipulation\nwith ﬁnger tracking,’’ in Proc. IEEE Conf. Virtual Reality 3D User Inter-\nface (VR), Reutlingen, Germany, Mar. 2018, pp. 589–590.\n[30] P. Lopes, A. Ion, and P. Baudisch, ‘‘Impacto: Simulating physical impact\nby combining tactile stimulation with electrical muscle stimulation,’’ in\nProc. 28th Annu. ACM Symp. Interface Softw. Technol. (UIST), New York,\nNY , USA, 2015, pp. 11–19.\n[31] D. Vajak and C. Livada, ‘‘Combining photogrammetry, 3D modeling and\nreal time information gathering for highly immersive VR experience,’’ in\nProc. Zooming Innov. Consum. Electron. Int. Conf. (ZINC), Novi Sad,\nSerbia, May 2017, pp. 82–85.\n[32] A. Rahimi, H. Patel, H. Ajmal, and S. Haghani, ‘‘The design and imple-\nmentation of a VR gun controller with haptic feedback,’’ in Proc. IEEE Int.\nConf. Consum. Electron. (ICCE), Las Vegas, NV , USA, Jan. 2019, pp. 1–2.\n[33] Y . Zhao, M. Forte, and R. Kopper, ‘‘VR touch museum,’’ in Proc. IEEE\nConf. Virtual Reality 3D Interface (VR), Reutlingen, Germany, Mar. 2018,\npp. 741–742.\n[34] G. Du and P. Zhang, ‘‘A markerless human–robot interface using particle\nﬁlter and Kalman ﬁlter for dual robots,’’ IEEE Trans. Ind. Electron.,\nvol. 62, no. 4, pp. 2257–2264, Apr. 2015.\n[35] G. Du, P. Zhang, and D. Li, ‘‘Human–manipulator interface based on\nmultisensory process via Kalman ﬁlters,’’ IEEE Trans. Ind. Electron.,\nvol. 61, no. 10, pp. 5411–5418, Oct. 2014.\n[36] G. Du, P. Zhang, and X. Liu, ‘‘Markerless human–manipulator interface\nusing leap motion with interval Kalman ﬁlter and improved particle ﬁlter,’’\nIEEE Trans. Ind. Informat., vol. 12, no. 2, pp. 694–704, Apr. 2016.\n[37] D. Zhang, H. Wang, and D. Wu, ‘‘Toward centimeter-scale human activ-\nity sensing with Wi-Fi signals,’’ Computer, vol. 50, no. 1, pp. 48–57,\nJan. 2017.\n[38] Z. Wang, B. Guo, Z. Yu, and X. Zhou, ‘‘Wi-Fi CSI-based behavior recogni-\ntion: From signals and actions to activities,’’ IEEE Commun. Mag., vol. 56,\nno. 5, pp. 109–115, May 2018.\nXUQIANG SHAOreceived the Ph.D. degree from\nBeihang University. He is currently an Associate\nProfessor with the School of Control and Com-\nputer Engineering, North China Electric Power\nUniversity at Baoding, China. His research inter-\nests include computer graphics, parallel comput-\ning on GPU, and virtual reality (VR).\nXIAOHUA FENG received the B.S. degree in oil\nand gas storage and transportation engineering\nfrom Southwest Petroleum University. He is cur-\nrently pursuing the M.S. degree with the School of\nControl and Computer Engineering, North China\nElectric Power University at Baoding. His research\ninterests include virtual reality (VR) and graphics.\nYELU YUis currently pursuing the B.S. degree in\ninformation security with the Department of Com-\nputer Science, School of Control and Computer\nEngineering, North China Electric Power Univer-\nsity at Baoding. Her research interests include vir-\ntual reality (VR) and graphics.\nZHAOHUI WU received the Ph.D. degree in\ncomputer science and technology from Beihang\nUniversity, Beijing, China, in 2015. He is cur-\nrently an Associate Researcher with the China\nAcademy of Transportation Sciences. His cur-\nrent research interests include transportation vir-\ntual reality, transportation data virtualization, and\ntransportation simulation.\nPENG MEI received the B.S. degree in network\nengineering from the Department of Computer\nScience, School of Control and Computer Engi-\nneering, North China Electric Power University\nat Baoding, where he is currently pursuing the\nM.S. degree. His research interests include virtual\nreality (VR) and computer graphics.\nVOLUME 8, 2020 54709",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7823165655136108
    },
    {
      "name": "Virtual reality",
      "score": 0.5993514060974121
    },
    {
      "name": "Collision detection",
      "score": 0.5953930616378784
    },
    {
      "name": "Operability",
      "score": 0.47010013461112976
    },
    {
      "name": "Roaming",
      "score": 0.4473973512649536
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3346589207649231
    },
    {
      "name": "Collision",
      "score": 0.22606903314590454
    },
    {
      "name": "Computer network",
      "score": 0.08043327927589417
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Software engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I153473198",
      "name": "North China Electric Power University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210107055",
      "name": "China Academy of Transportation Sciences",
      "country": "CN"
    }
  ],
  "cited_by": 13
}