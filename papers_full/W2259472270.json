{
    "title": "Exploring the Limits of Language Modeling",
    "url": "https://openalex.org/W2259472270",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4297705242",
            "name": "Jozefowicz, Rafal",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3025185919",
            "name": "Vinyals, Oriol",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4292355538",
            "name": "Schuster, Mike",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224379215",
            "name": "Shazeer, Noam",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1993644842",
            "name": "Wu Yonghui",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2197913429",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W1575384945",
        "https://openalex.org/W1810943226",
        "https://openalex.org/W2152808281",
        "https://openalex.org/W100623710",
        "https://openalex.org/W2293634267",
        "https://openalex.org/W2949563612",
        "https://openalex.org/W2131462252",
        "https://openalex.org/W2296712013",
        "https://openalex.org/W1899794420",
        "https://openalex.org/W2952453038",
        "https://openalex.org/W2207587218",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W36903255",
        "https://openalex.org/W2251654079",
        "https://openalex.org/W2120615054",
        "https://openalex.org/W1843891098",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W1026270304",
        "https://openalex.org/W2057653135",
        "https://openalex.org/W196214544",
        "https://openalex.org/W2185726469",
        "https://openalex.org/W1558797106",
        "https://openalex.org/W1999965501",
        "https://openalex.org/W1591706642",
        "https://openalex.org/W2154579312",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2131774270",
        "https://openalex.org/W2183112036",
        "https://openalex.org/W2136848157",
        "https://openalex.org/W2118434577",
        "https://openalex.org/W2951336364",
        "https://openalex.org/W2097732278",
        "https://openalex.org/W581956982",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W932413789",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2963603213",
        "https://openalex.org/W2950580142",
        "https://openalex.org/W1917432393",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2963208801",
        "https://openalex.org/W2949190276",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W3023071679",
        "https://openalex.org/W2152790380",
        "https://openalex.org/W2951714314",
        "https://openalex.org/W2250379827"
    ],
    "abstract": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.",
    "full_text": "Exploring the Limits of Language Modeling\nRafal Jozefowicz RAFALJ @GOOGLE .COM\nOriol Vinyals VINYALS @GOOGLE .COM\nMike Schuster SCHUSTER @GOOGLE .COM\nNoam Shazeer NOAM @GOOGLE .COM\nYonghui Wu YONGHUI @GOOGLE .COM\nGoogle Brain\nAbstract\nIn this work we explore recent advances in Re-\ncurrent Neural Networks for large scale Lan-\nguage Modeling, a task central to language un-\nderstanding. We extend current models to deal\nwith two key challenges present in this task: cor-\npora and vocabulary sizes, and complex, long\nterm structure of language. We perform an ex-\nhaustive study on techniques such as character\nConvolutional Neural Networks or Long-Short\nTerm Memory, on the One Billion Word Bench-\nmark. Our best single model signiﬁcantly im-\nproves state-of-the-art perplexity from 51.3 down\nto 30.0 (whilst reducing the number of param-\neters by a factor of 20), while an ensemble of\nmodels sets a new record by improving perplex-\nity from 41.0 down to 23.7. We also release these\nmodels for the NLP and ML community to study\nand improve upon.\n1. Introduction\nLanguage Modeling (LM) is a task central to Natural\nLanguage Processing (NLP) and Language Understanding.\nModels which can accurately place distributions over sen-\ntences not only encode complexities of language such as\ngrammatical structure, but also distill a fair amount of in-\nformation about the knowledge that a corpora may con-\ntain. Indeed, models that are able to assign a low probabil-\nity to sentences that are grammatically correct but unlikely\nmay help other tasks in fundamental language understand-\ning like question answering, machine translation, or text\nsummarization.\nLMs have played a key role in traditional NLP tasks such\nas speech recognition (Mikolov et al., 2010; Arisoy et al.,\n2012), machine translation (Schwenk et al., 2012; Vaswani\net al.), or text summarization (Rush et al., 2015; Filippova\net al., 2015). Often (although not always), training better\nlanguage models improves the underlying metrics of the\ndownstream task (such as word error rate for speech recog-\nnition, or BLEU score for translation), which makes the\ntask of training better LMs valuable by itself.\nFurther, when trained on vast amounts of data, language\nmodels compactly extract knowledge encoded in the train-\ning data. For example, when trained on movie subti-\ntles (Serban et al., 2015; Vinyals & Le, 2015), these lan-\nguage models are able to generate basic answers to ques-\ntions about object colors, facts about people, etc. Lastly,\nrecently proposed sequence-to-sequence models employ\nconditional language models (Mikolov & Zweig, 2012) as\ntheir key component to solve diverse tasks like machine\ntranslation (Sutskever et al., 2014; Cho et al., 2014; Kalch-\nbrenner et al., 2014) or video generation (Srivastava et al.,\n2015a).\nDeep Learning and Recurrent Neural Networks (RNNs)\nhave fueled language modeling research in the past years\nas it allowed researchers to explore many tasks for which\nthe strong conditional independence assumptions are unre-\nalistic. Despite the fact that simpler models, such as N-\ngrams, only use a short history of previous words to predict\nthe next word, they are still a key component to high qual-\nity, low perplexity LMs. Indeed, most recent work on large\nscale LM has shown that RNNs are great in combination\nwith N-grams, as they may have different strengths that\ncomplement N-gram models, but worse when considered\nin isolation (Mikolov et al., 2011; Mikolov, 2012; Chelba\net al., 2013; Williams et al., 2015; Ji et al., 2015a; Shazeer\net al., 2015).\nWe believe that, despite much work being devoted to small\ndata sets like the Penn Tree Bank (PTB) (Marcus et al.,\n1993), research on larger tasks is very relevant as overﬁt-\nting is not the main limitation in current language model-\ning, but is the main characteristic of the PTB task. Results\non larger corpora usually show better what matters as many\nideas work well on small data sets but fail to improve on\narXiv:1602.02410v2  [cs.CL]  11 Feb 2016\nExploring the Limits of Language Modeling\nFigure 1.A high-level diagram of the models presented in this pa-\nper. (a) is a standard LSTM LM. (b) represents an LM where both\ninput and Softmax embeddings have been replaced by a character\nCNN. In (c) we replace the Softmax by a next character prediction\nLSTM network.\nlarger data sets. Further, given current hardware trends and\nvast amounts of text available on the Web, it is much more\nstraightforward to tackle large scale modeling than it used\nto be. Thus, we hope that our work will help and motivate\nresearchers to work on traditional LM beyond PTB – for\nthis purpose, we will open-source our models and training\nrecipes.\nWe focused on a well known, large scale LM benchmark:\nthe One Billion Word Benchmark data set (Chelba et al.,\n2013). This data set is much larger than PTB (one thou-\nsand fold, 800k word vocabulary and 1B words training\ndata) and far more challenging. Similar to Imagenet (Deng\net al., 2009), which helped advance computer vision, we\nbelieve that releasing and working on large data sets and\nmodels with clear benchmarks will help advance Language\nModeling.\nThe contributions of our work are as follows:\n• We explored, extended and tried to unify some of the\ncurrent research on large scale LM.\n• Speciﬁcally, we designed a Softmax loss which is\nbased on character level CNNs, is efﬁcient to train,\nand is as precise as a full Softmax which has orders of\nmagnitude more parameters.\n• Our study yielded signiﬁcant improvements to the\nstate-of-the-art on a well known, large scale LM task:\nfrom 51.3 down to 30.0 perplexity for single models\nwhilst reducing the number of parameters by a factor\nof 20.\n• We show that an ensemble of a number of different\nmodels can bring down perplexity on this task to 23.7,\na large improvement compared to current state-of-art.\n• We share the model and recipes in order to help and\nmotivate further research in this area.\nIn Section 2 we review important concepts and previous\nwork on language modeling. Section 3 presents our contri-\nbutions to the ﬁeld of neural language modeling, emphasiz-\ning large scale recurrent neural network training. Sections\n4 and 5 aim at exhaustively describing our experience and\nunderstanding throughout the project, as well as emplacing\nour work relative to other known approaches.\n2. Related Work\nIn this section we describe previous work relevant to the\napproaches discussed in this paper. A more detailed dis-\ncussion on language modeling research is provided in\n(Mikolov, 2012).\n2.1. Language Models\nLanguage Modeling (LM) has been a central task in NLP.\nThe goal of LM is to learn a probability distribution over\nsequences of symbols pertaining to a language. Much work\nhas been done on both parametric (e.g., log-linear models)\nand non-parametric approaches (e.g., count-based LMs).\nCount-based approaches (based on statistics of N-grams)\ntypically add smoothing which account for unseen (yet pos-\nsible) sequences, and have been quite successful. To this\nextent, Kneser-Ney smoothed 5-gram models (Kneser &\nNey, 1995) are a fairly strong baseline which, for large\namounts of training data, have challenged other paramet-\nric approaches based on Neural Networks (Bengio et al.,\n2006).\nMost of our work is based on Recurrent Neural Networks\n(RNN) models which retain long term dependencies. To\nthis extent, we used the Long-Short Term Memory model\n(Hochreiter & Schmidhuber, 1997) which uses a gating\nmechanism (Gers et al., 2000) to ensure proper propaga-\ntion of information through many time steps. Much work\nhas been done on small and large scale RNN-based LMs\n(Mikolov et al., 2010; Mikolov, 2012; Chelba et al., 2013;\nZaremba et al., 2014; Williams et al., 2015; Ji et al., 2015a;\nWang & Cho, 2015; Ji et al., 2015b). The architectures that\nwe considered in this paper are represented in Figure 1.\nIn our work, we train models on the popular One Bil-\nlion Word Benchmark, which can be considered to be a\nmedium-sized data set for count-based LMs but a very large\ndata set for NN-based LMs. This regime is most interesting\nto us as we believe learning a very good model of human\nlanguage is a complex task which will require large models,\nExploring the Limits of Language Modeling\nand thus large amounts of data. Further advances in data\navailability and computational resources helped our study.\nWe argue this leap in scale enabled tremendous advances in\ndeep learning. A clear example found in computer vision is\nImagenet (Deng et al., 2009), which enabled learning com-\nplex vision models from large amounts of data (Krizhevsky\net al., 2012).\nA crucial aspect which we discuss in detail in later sections\nis the size of our models. Despite the large number of pa-\nrameters, we try to minimize computation as much as pos-\nsible by adopting a strategy proposed in (Sak et al., 2014)\nof projecting a relatively big recurrent state space down so\nthat the matrices involved remain relatively small, yet the\nmodel has large memory capacity.\n2.2. Convolutional Embedding Models\nThere is an increased interest in incorporating character-\nlevel inputs to build word embeddings for various NLP\nproblems, including part-of-speech tagging, parsing and\nlanguage modeling (Ling et al., 2015; Kim et al., 2015;\nBallesteros et al., 2015). The additional character informa-\ntion has been shown useful on relatively small benchmark\ndata sets.\nThe approach proposed in (Ling et al., 2015) builds word\nembeddings using bidirectional LSTMs (Schuster & Pali-\nwal, 1997; Graves & Schmidhuber, 2005) over the charac-\nters. The recurrent networks process sequences of charac-\nters from both sides and their ﬁnal state vectors are concate-\nnated. The resulting representation is then fed to a Neural\nNetwork. This model achieved very good results on a part-\nof-speech tagging task.\nIn (Kim et al., 2015), the words characters are processed by\na 1-d CNN (Le Cun et al., 1990) with max-pooling across\nthe sequence for each convolutional feature. The result-\ning features are fed to a 2-layer highway network (Srivas-\ntava et al., 2015b), which allows the embedding to learn se-\nmantic representations. The model was evaluated on small-\nscale language modeling experiments for various languages\nand matched the best results on the PTB data set despite\nhaving 60% fewer parameters.\n2.3. Softmax Over Large Vocabularies\nAssigning probability distributions over large vocabularies\nis computationally challenging. For modeling language,\nmaximizing log-likelihood of a given word sequence leads\nto optimizing cross-entropy between the target probability\ndistribution (e.g., the target word we should be predicting),\nand our model predictions p. Generally, predictions come\nfrom a linear layer followed by a Softmax non-linearity:\np(w) = exp(zw)∑\nw′∈V exp(zw′) where zw is the logit correspond-\ning to a word w. The logit is generally computed as an\ninner product zw = hTew where his a context vector and\new is a “word embedding” for w.\nThe main challenge when |V|is very large (in the order\nof one million in this paper) is the fact that computing\nall inner products between hand all embeddings becomes\nprohibitively slow during training (even when exploiting\nmatrix-matrix multiplications and modern GPUs). Several\napproaches have been proposed to cope with the scaling is-\nsue: importance sampling (Bengio et al., 2003; Bengio &\nSen´ecal, 2008), Noise Contrastive Estimation (NCE) (Gut-\nmann & Hyv ¨arinen, 2010; Mnih & Kavukcuoglu, 2013),\nself normalizing partition functions (Vincent et al., 2015)\nor Hierarchical Softmax (Morin & Bengio, 2005; Mnih &\nHinton, 2009) – they all offer good solutions to this prob-\nlem. We found importance sampling to be quite effective\non this task, and explain the connection between it and\nNCE in the following section, as they are closely related.\n3. Language Modeling Improvements\nRecurrent Neural Networks based LMs employ the chain\nrule to model joint probabilities over word sequences:\np(w1,...,w N) =\nN∏\ni=1\np(wi|w1,...,w i−1)\nwhere the context of all previous words is encoded with an\nLSTM, and the probability over words uses a Softmax (see\nFigure 1(a)).\n3.1. Relationship between Noise Contrastive\nEstimation and Importance Sampling\nAs discussed in Section 2.3, a large scale Softmax is neces-\nsary for training good LMs because of the vocabulary size.\nA Hierarchical Softmax (Mnih & Hinton, 2009) employs\na tree in which the probability distribution over words is\ndecomposed into a product of two probabilities for each\nword, greatly reducing training and inference time as only\nthe path speciﬁed by the hierarchy needs to be computed\nand updated. Choosing a good hierarchy is important for\nobtaining good results and we did not explore this approach\nfurther for this paper as sampling methods worked well for\nour setup.\nSampling approaches are only useful during training, as\nthey propose an approximation to the loss which is cheap to\ncompute (also in a distributed setting) – however, at infer-\nence time one still has to compute the normalization term\nover all words. Noise Contrastive Estimation (NCE) pro-\nposes to consider a surrogate binary classiﬁcation task in\nwhich a classiﬁer is trained to discriminate between true\ndata, or samples coming from some arbitrary distribution.\nIf both the noise and data distributions were known, the\nExploring the Limits of Language Modeling\noptimal classiﬁer would be:\np(Y = true|w) = pd(w)\npd(w) +kpn(w)\nwhere Y is the binary random variable indicating whether\nwcomes from the true data distribution, kis the number of\nnegative samples per positive word, and pd and pn are the\ndata and noise distribution respectively (we dropped any\ndependency on previous words for notational simplicity).\nIt is easy to show that if we train a logistic classiﬁer\npθ(Y = true|w) = σ(sθ(w,h) −log kpn(w)) where σ\nis the logistic function, then, p′(w) =softmax(sθ(w,h))\nis a good approximation of pd(w) (sθ is a logit which e.g.\nan LSTM LM computes).\nThe other technique, which is based on importance sam-\npling (IS), proposes to directly approximate the partition\nfunction (which comprises a sum over all words) with an\nestimate of it through importance sampling. Though the\nmethods look superﬁcially similar, we will derive a similar\nsurrogate classiﬁcation task akin to NCE which arrives at\nIS, showing a strong connection between the two.\nSuppose that, instead of having a binary task to decide if\na word comes from the data or from the noise distribution,\nwe want to identify the words coming from the true data\ndistribution in a set W = {w1,...,w k+1}, comprised of\nk noise samples and one data distribution sample. Thus,\nwe can train a multiclass loss over a multinomial random\nvariable Y which maximizes log p(Y = 1|W), assuming\nw.l.o.g. that w1 ∈W is always the word coming from true\ndata. By Bayes rule, and ignoring terms that are constant\nwith respect to Y, we can write:\np(Y = k|W) ∝Y\npd(wk)\npn(wk)\nand, following a similar argument than for NCE, if we de-\nﬁne p(Y = k|W) =softmax(sθ(wk) −log pn(wk)) then\np′(w) = softmax(sθ(w,h)) is a good approximation of\npd(word). Note that the only difference between NCE and\nIS is that, in NCE, we deﬁne a binary classiﬁcation task\nbetween true or noise words with a logistic loss, whereas\nin IS we deﬁne a multiclass classiﬁcation problem with a\nSoftmax and cross entropy loss. We hope that our deriva-\ntion helps clarify the similarities and differences between\nthe two. In particular, we observe that IS, as it optimizes\na multiclass classiﬁcation task (in contrast to solving a bi-\nnary task), may be a better choice. Indeed, the updates to\nthe logits with IS are tied whereas in NCE they are inde-\npendent.\n3.2. CNN Softmax\nThe character-level features allow for a smoother and com-\npact parametrization of the word embeddings. Recent ef-\nforts on small scale language modeling have used CNN\ncharacter embeddings for the input embeddings (Kim et al.,\n2015). Although not as straightforward, we propose an ex-\ntension to this idea to also reduce the number of param-\neters of the Softmax layer. Recall from Section 2.3 that\nthe Softmax computes a logit as zw = hTew where h is\na context vector and ew the word embedding. Instead of\nbuilding a matrix of |V|×| h|(whose rows correspond to\new), we produce ew with a CNN over the characters ofwas\new = CNN(charsw) – we call this a CNN Softmax. We\nused the same network architecture to dynamically gener-\nate the Softmax word embeddings without sharing the pa-\nrameters with the input word-embedding sub-network. For\ninference, the vectorsewcan be precomputed, so there is no\ncomputational complexity increase w.r.t. the regular Soft-\nmax.\nWe note that, when using an importance sampling loss such\nas the one described in Section 3.1, only a few logits have\nnon-zero gradient (those corresponding to the true and sam-\npled words). With a Softmax where ew are independently\nlearned word embeddings, this is not a problem. But we\nobserved that, when using a CNN, all the logits become\ntied as the function mapping from wto ew is quite smooth.\nAs a result, a much smaller learning rate had to be used.\nEven with this, the model lacks capacity to differentiate\nbetween words that have very different meanings but that\nare spelled similarly. Thus, a reasonable compromise was\nto add a small correction factor which is learned per word,\nsuch that:\nzw = hTCNN(charsw) +hTMcorrw\nwhere M is a matrix projecting a low-dimensional embed-\nding vector corrw back up to the dimensionality of the pro-\njected LSTM hidden state of h. This amounts to adding a\nbottleneck linear layer, and brings the CNN Softmax much\ncloser to our best result, as can be seen in Table 1, where\nadding a 128-dim correction halves the gap between regu-\nlar and the CNN Softmax.\nAside from a big reduction in the number of parameters\nand incorporating morphological knowledge from words,\nthe other beneﬁt of this approach is that out-of-vocabulary\n(OOV) words can easily be scored. This may be useful for\nother problems such as Machine Translation where han-\ndling out-of-vocabulary words is very important (Luong\net al., 2014). This approach also allows parallel training\nover various data sets since the model is no longer explic-\nitly parametrized by the vocabulary size – or the language.\nThis has shown to help when using byte-level input embed-\ndings for named entity recognition (Gillick et al., 2015),\nExploring the Limits of Language Modeling\nand we hope it will enable similar gains when used to map\nonto words.\n3.3. Char LSTM Predictions\nThe CNN Softmax layer can handle arbitrary words and is\nmuch more efﬁcient in terms of number of parameters than\nthe full Softmax matrix. It is, though, still considerably\nslow, as to evaluate perplexities we need to compute the\npartition function. A class of models that solve this prob-\nlem more efﬁciently are character-level LSTMs (Sutskever\net al., 2011; Graves, 2013). They make predictions one\ncharacter at a time, thus allowing to compute probabili-\nties over a much smaller vocabulary. On the other hand,\nthese models are more difﬁcult to train and seem to per-\nform worse even in small tasks like PTB (Graves, 2013).\nMost likely this is due to the sequences becoming much\nlonger on average as the LSTM reads the input character\nby character instead of word by word.\nThus, we combine the word and character-level models by\nfeeding a word-level LSTM hidden state h into a small\nLSTM that predicts the target word one character at a time\n(see Figure 1(c)). In order to make the whole process rea-\nsonably efﬁcient, we train the standard LSTM model un-\ntil convergence, freeze its weights, and replace the stan-\ndard word-level Softmax layer with the aforementioned\ncharacter-level LSTM.\nThe resulting model scales independently of vocabulary\nsize – both for training and inference. However, it does\nseem to be worse than regular and CNN Softmax – we are\nhopeful that further research will enable these models to\nreplace ﬁxed vocabulary models whilst being computation-\nally attractive.\n4. Experiments\nAll experiments were run using the TensorFlow system\n(Abadi et al., 2015), with the exception of some older mod-\nels which were used in the ensemble.\n4.1. Data Set\nThe experiments are performed on the 1B Word Bench-\nmark data set introduced by (Chelba et al., 2013), which is\na publicly available benchmark for measuring progress of\nstatistical language modeling. The data set contains about\n0.8B words with a vocabulary of 793471 words, including\nsentence boundary markers. All the sentences are shufﬂed\nand the duplicates are removed. The words that are out of\nvocabulary (OOV) are marked with a special UNK token\n(there are approximately 0.3% such words).\n4.2. Model Setup\nThe typical measure used for reporting progress in\nlanguage modeling is perplexity, which is the aver-\nage per-word log-probability on the holdout data set:\ne−1\nN\n∑\ni ln pwi . We follow the standard procedure and sum\nover all the words (including the end of sentence symbol).\nWe used the 1B Word Benchmark data set without any pre-\nprocessing. Given the shufﬂed sentences, they are input to\nthe network as a batch of independent streams of words.\nWhenever a sentence ends, a new one starts without any\npadding (thus maximizing the occupancy per batch).\nFor the models that consume characters as inputs or as tar-\ngets, each word is fed to the model as a sequence of charac-\nter IDs of preespeciﬁed length (see Figure 1(b)). The words\nwere processed to include special begin and end of word to-\nkens and were padded to reach the expected length. I.e. if\nthe maximum word length was 10, the word “cat” would\nbe transformed to “$catˆ ” due to the CNN model.\nIn our experiments we found that limiting the maximum\nword length in training to 50 was sufﬁcient to reach very\ngood results while 32 was clearly insufﬁcient. We used\n256 characters in our vocabulary and the non-ascii symbols\nwere represented as a sequence of bytes.\n4.3. Model Architecture\nWe evaluated many variations of RNN LM architectures.\nThese include the dimensionalities of the embedding lay-\ners, the state, projection sizes, and number of LSTM layers\nto use. Exhaustively trying all combinations would be ex-\ntremely time consuming for such a large data set, but our\nﬁndings suggest that LSTMs with a projection layer (i.e.,\na bottleneck between hidden states as in (Sak et al., 2014))\ntrained with truncated BPTT (Williams & Peng, 1990) for\n20 steps performed well.\nFollowing (Zaremba et al., 2014) we use dropout (Srivas-\ntava, 2013) before and after every LSTM layer. The bi-\nases of LSTM forget gate were initialized to 1.0 (Jozefow-\nicz et al., 2015). The size of the models will be described\nin more detail in the following sections, and the choices\nof hyper-parameters will be released as open source upon\npublication.\nFor any model using character embedding CNNs, we\nclosely follow the architecture from (Kim et al., 2015). The\nonly important difference is that we use a larger number of\nconvolutional features of 4096 to give enough capacity to\nthe model. The resulting embedding is then linearly trans-\nformed to match the LSTM projection sizes. This allows it\nto match the performance of regular word embeddings but\nonly uses a small fraction of parameters.\nExploring the Limits of Language Modeling\nTable 1.Best results of single models on the 1B word benchmark. Our results are shown below previous work.\nMODEL TEST PERPLEXITY NUMBER OF PARAMS [BILLIONS ]\nSIGMOID -RNN-2048 (J I ET AL ., 2015 A) 68.3 4.1\nINTERPOLATED KN 5- GRAM , 1.1B N-GRAMS (CHELBA ET AL ., 2013) 67.6 1.76\nSPARSE NON-NEGATIVE MATRIX LM (S HAZEER ET AL ., 2015) 52.9 33\nRNN-1024 + M AXENT 9-GRAM FEATURES (CHELBA ET AL ., 2013) 51.3 20\nLSTM-512-512 54.1 0.82\nLSTM-1024-512 48.2 0.82\nLSTM-2048-512 43.7 0.83\nLSTM-8192-2048 (N O DROPOUT ) 37.9 3.3\nLSTM-8192-2048 (50% D ROPOUT ) 32.2 3.3\n2-L AYER LSTM-8192-1024 (BIG LSTM) 30.6 1.8\nBIG LSTM+CNN I NPUTS 30.0 1.04\nBIG LSTM+CNN I NPUTS + CNN S OFTMAX 39.8 0.29\nBIG LSTM+CNN I NPUTS + CNN S OFTMAX + 128- DIM CORRECTION 35.8 0.39\nBIG LSTM+CNN I NPUTS + CHAR LSTM PREDICTIONS 47.9 0.23\nTable 2.Best results of ensembles on the 1B Word Benchmark.\nMODEL TEST PERPLEXITY\nLARGE ENSEMBLE (CHELBA ET AL ., 2013) 43.8\nRNN+KN-5 (W ILLIAMS ET AL ., 2015) 42.4\nRNN+KN-5 (J I ET AL ., 2015 A) 42.0\nRNN+SNM10- SKIP (SHAZEER ET AL ., 2015) 41.3\nLARGE ENSEMBLE (SHAZEER ET AL ., 2015) 41.0\nOUR 10 BEST LSTM MODELS (EQUAL WEIGHTS ) 26.3\nOUR 10 BEST LSTM MODELS (OPTIMAL WEIGHTS ) 26.1\n10 LSTM S + KN-5 ( EQUAL WEIGHTS ) 25.3\n10 LSTM S + KN-5 ( OPTIMAL WEIGHTS ) 25.1\n10 LSTM S + SNM10- SKIP (SHAZEER ET AL ., 2015) 23.7\n4.4. Training Procedure\nThe models were trained until convergence with an Ada-\nGrad optimizer using a learning rate of 0.2. In all the exper-\niments the RNNs were unrolled for 20 steps without ever\nresetting the LSTM states. We used a batch size of 128.\nWe clip the gradients of the LSTM weights such that their\nnorm is bounded by 1.0 (Pascanu et al., 2012).\nUsing these hyper-parameters we found large LSTMs to be\nrelatively easy to train. The same learning rate was used in\nalmost all of the experiments. In a few cases we had to re-\nduce it by an order of magnitude. Unless otherwise stated,\nthe experiments were performed with 32 GPU workers and\nasynchronous gradient updates. Further details will be fully\nspeciﬁed with the code upon publication.\nTraining a model for such large target vocabulary (793471\nwords) required to be careful with some details about the\napproximation to full Softmax using importance sampling.\nWe used a large number of negative (or noise) samples:\n8192 such samples were drawn per step, but were shared\nacross all the target words in the batch (2560 total, i.e. 128\ntimes 20 unrolled steps). This results in multiplying (2560\nx 1024) times (1024 x (8192+1)) (instead of (2560 x 1024)\ntimes (1024 x 793471)), i.e. about 100-fold less computa-\ntion.\n5. Results and Analysis\nIn this section we summarize the results of our experiments\nand do an in-depth analysis. Table 1 contains all results for\nour models compared to previously published work. Ta-\nble 2 shows previous and our own work on ensembles of\nmodels. We hope that our encouraging results, which im-\nproved the best perplexity of a single model from 51.3 to\n30.0 (whilst reducing the model size considerably), and set\na new record with ensembles at 23.7, will enable rapid re-\nsearch and progress to advance Language Modeling. For\nExploring the Limits of Language Modeling\nthis purpose, we will release the model weights and recipes\nupon publication.\n5.1. Size Matters\nUnsurprisingly, size matters: when training on a very large\nand complex data set, ﬁtting the training data with an\nLSTM is fairly challenging. Thus, the size of the LSTM\nlayer is a very important factor that inﬂuences the results,\nas seen in Table 1. The best models are the largest we were\nable to ﬁt into a GPU memory. Our largest model was a 2-\nlayer LSTM with 8192+1024 dimensional recurrent state\nin each of the layers. Increasing the embedding and projec-\ntion size also helps but causes a large increase in the num-\nber of parameters, which is less desirable. Lastly, training\nan RNN instead of an LSTM yields poorer results (about 5\nperplexity worse) for a comparable model size.\n5.2. Regularization Importance\nAs shown in Table 1, using dropout improves the results.\nTo our surprise, even relatively small models (e.g., single\nlayer LSTM with 2048 units projected to 512 dimensional\noutputs) can over-ﬁt the training set if trained long enough,\neventually yielding holdout set degradation.\nUsing dropout on non-recurrent connections largely miti-\ngates these issues. While over-ﬁtting still occurs, there is\nno more need for early stopping. For models that had 4096\nor less units in the LSTM layer, we used 10% dropout prob-\nability. For larger models, 25% was signiﬁcantly better.\nEven with such regularization, perplexities on the training\nset can be as much as 6 points below test.\nIn one experiment we tried to use a smaller vocabulary\ncomprising of the 100,000 most frequent words and found\nthe difference between train and test to be smaller – which\nsuggests that too much capacity is given to rare words. This\nis less of an issue with character CNN embedding models\nas the embeddings are shared across all words.\n5.3. Importance Sampling is Data Efﬁcient\nTable 3 shows the test perplexities of NCE vs IS loss after a\nfew epochs of 2048 unit LSTM with 512 projection. The IS\nobjective signiﬁcantly improves the speed and the overall\nperformance of the model when compared to NCE.\n5.4. Word Embeddings vs Character CNN\nReplacing the embedding layer with a parametrized neural\nnetwork that process characters of a given word allows the\nmodel to consume arbitrary words and is not restricted to\na ﬁxed vocabulary. This property is useful for data sets\nwith conversational or informal text as well as for mor-\nphologically rich languages. Our experiments show that\nTable 3.The test perplexities of an LSTM-2048-512 trained with\ndifferent losses versus number of epochs. The model needs about\n40 minutes per epoch. First epoch is a bit slower because we\nslowly increase the number of workers.\nEPOCHS NCE IS T RAINING TIME [HOURS ]\n1 97 60 1\n5 58 47.5 4\n10 53 45 8\n20 49 44 14\n50 46.1 43.7 34\nTable 4.Nearest neighbors in the character CNN embedding\nspace of a few out-of-vocabulary words. Even for words that\nthe model has never seen, the model usually still ﬁnds reasonable\nneighbors.\nWORD TOP-1 T OP-2 T OP-3\nINCERDIBLE INCREDIBLE NONEDIBLE EXTENDIBLE\nWWW.A.COM WWW .AA.COM WWW .AAA .COM WWW .CA.COM\n7546 7646 7534 8566\nTOWN HAL1 T OWN HALL DJC2 M OODSWING 360\nKOMARSKI KOHARSKI KONARSKI KOMANSKI\nusing character-level embeddings is feasible and does not\ndegrade performance – in fact, our best single model uses\na Character CNN embedding.\nAn additional advantage is that the number of parameters of\nthe input layer is reduced by a factor of 11 (though training\nspeed is slightly worse). For inference, the embeddings\ncan be precomputed so there is no speed penalty. Overall,\nthe embedding of the best model is parametrized by 72M\nweights (down from 820M weights).\nTable 4 shows a few examples of nearest neighbor embed-\ndings for some out-of-vocabulary words when character\nCNNs are used.\n5.5. Smaller Models with CNN Softmax\nEven with character-level embeddings, the model is still\nfairly large (though much smaller than the best competing\nmodels from previous work). Most of the parameters are in\nthe linear layer before the Softmax: 820M versus a total of\n1.04B parameters.\nIn one of the experiments we froze the word-LSTM after\nconvergence and replaced the Softmax layer with the CNN\nSoftmax sub-network. Without any ﬁne-tuning that model\nwas able to reach 39.8 perplexity with only 293M weights\n(as seen in Table 1).\nAs described in Section 3.2, adding a “correction” word\nembedding term alleviates the gap between regular and\nExploring the Limits of Language Modeling\nCNN Softmax. Indeed, we can trade-off model size versus\nperplexity. For instance, by adding 100M weights (through\na 128 dimensional bottleneck embedding) we achieve 35.8\nperplexity (see Table 1).\nTo contrast with the CNN Softmax, we also evaluated a\nmodel that replaces the Softmax layer with a smaller LSTM\nthat predicts one character at a time (see Section 3.3). Such\na model does not have to learn long dependencies because\nthe base LSTM still operates at the word-level (see Fig-\nure 1(c)). With a single-layer LSTM of 1024 units we\nreached 49.0 test perplexity, far below the best model. In\norder to make the comparisons more fair, we performed a\nvery expensive marginalization over the words in the vo-\ncabulary (to rule out words not in the dictionary which the\ncharacter LSTM would assign some probability). When\ndoing this marginalization, the perplexity improved a bit\ndown to 47.9.\nWords buckets of equal size (less frequent words on the right)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5Mean difference in log perplexity\nFigure 2.The difference in log probabilities between the best\nLSTM and KN-5 (higher is better). The words from the hold-\nout set are grouped into 25 buckets of equal size based on their\nfrequencies.\n5.6. Training Speed\nWe used 32 Tesla K40 GPUs to train our models. The\nsmaller version of the LSTM model with 2048 units and\n512 projections needs less than 10 hours to reach below\n45 perplexity and after only 2 hours of training the model\nbeats previous state-of-the art on this data set. The best\nmodel needs about 5 days to get to 35 perplexity and 10\ndays to 32.5. The best results were achieved after 3 weeks\nof training. See Table 3 for more details.\n5.7. Ensembles\nWe averaged several of our best models and we were able\nto reach 23.7 test perplexity (more details and results can\nbe seen in Table 2), which is more than 40% improve-\nment over previous work. Interestingly, including the best\nN-gram model reduces the perplexity by 1.2 point even\nthough the model is rather weak on its own (67.6 perplex-\nity). Most previous work had to either ensemble with the\nbest N-gram model (as their RNN only used a limited out-\nput vocabulary of a few thousand words), or use N-gram\nfeatures as additional input to the RNN. Our results, on\nthe contrary, suggest that N-grams are of limited beneﬁt,\nand suggest that a carefully trained LSTM LM is the most\ncompetitive model.\n5.8. LSTMs are best on the tail words\nFigure 2 shows the difference in log probabilities between\nour best model (at 30.0 perplexity) and the KN-5. As can be\nseen from the plot, the LSTM is better across all the buckets\nand signiﬁcantly outperforms KN-5 on the rare words. This\nis encouraging as it seems to suggest that LSTM LMs may\nfare even better for languages or data sets where the number\nof rare words is larger than traditional N-gram models.\n5.9. Samples from the model\nTo qualitatively evaluate the model, we sampled many sen-\ntences. We discarded short and politically incorrect ones,\nbut the sample shown below is otherwise “raw” (i.e., not\nhand picked). The samples are of high quality – which is\nnot a surprise, given the perplexities attained – but there are\nstill some occasional mistakes.\nSentences generated by the ensemble (about 26 perplexity):\n< S >With even more new technologies coming onto the market\nquickly during the past three years , an increasing number of compa-\nnies now must tackle the ever-changing and ever-changing environ-\nmental challenges online . < S >Check back for updates on this\nbreaking news story . < S >About 800 people gathered at Hever\nCastle on Long Beach from noon to 2pm , three to four times that of\nthe funeral cort `ege . < S >We are aware of written instructions\nfrom the copyright holder not to , in any way , mention Rosenberg ’s\nnegative comments if they are relevant as indicated in the documents\n, ” eBay said in a statement . <S >It is now known that coffee and\ncacao products can do no harm on the body . < S >Yuri Zhirkov\nwas in attendance at the Stamford Bridge at the start of the second\nhalf but neither Drogba nor Malouda was able to push on through the\nBarcelona defence .\n6. Discussion and Conclusions\nIn this paper we have shown that RNN LMs can be trained\non large amounts of data, and outperform competing mod-\nels including carefully tuned N-grams. The reduction in\nperplexity from 51.3 to 30.0 is due to several key compo-\nnents which we studied in this paper. Thus, a large, regular-\nized LSTM LM, with projection layers and trained with an\napproximation to the true Softmax with importance sam-\npling performs much better than N-grams. Unlike previ-\nous work, we do not require to interpolate both the RNN\nLM and the N-gram, and the gains of doing so are rather\nmarginal.\nExploring the Limits of Language Modeling\nBy exploring recent advances in model architectures (e.g.\nLSTMs), exploiting small character CNNs, and by sharing\nour ﬁndings in this paper and accompanying code and mod-\nels (to be released upon publication), we hope to inspire\nresearch on large scale Language Modeling, a problem we\nconsider crucial towards language understanding. We hope\nfor future research to focus on reasonably sized datasets\ntaking inspiration from recent advances seen in the com-\nputer vision community thanks to efforts such as Imagenet\n(Deng et al., 2009).\nAcknowledgements\nWe thank Ciprian Chelba, Ilya Sutskever, and the Google\nBrain Team for their help and discussions. We also thank\nKoray Kavukcuoglu for his help with the manuscript.\nReferences\nAbadi, Mart ´ın, Agarwal, Ashish, Barham, Paul, Brevdo,\nEugene, Chen, Zhifeng, Citro, Craig, Corrado, Greg S.,\nDavis, Andy, Dean, Jeffrey, Devin, Matthieu, Ghe-\nmawat, Sanjay, Goodfellow, Ian, Harp, Andrew, Irv-\ning, Geoffrey, Isard, Michael, Jia, Yangqing, Jozefowicz,\nRafal, Kaiser, Lukasz, Kudlur, Manjunath, Levenberg,\nJosh, Man´e, Dan, Monga, Rajat, Moore, Sherry, Murray,\nDerek, Olah, Chris, Schuster, Mike, Shlens, Jonathon,\nSteiner, Benoit, Sutskever, Ilya, Talwar, Kunal, Tucker,\nPaul, Vanhoucke, Vincent, Vasudevan, Vijay, Vi ´egas,\nFernanda, Vinyals, Oriol, Warden, Pete, Wattenberg,\nMartin, Wicke, Martin, Yu, Yuan, and Zheng, Xiaoqiang.\nTensorFlow: Large-scale machine learning on heteroge-\nneous systems, 2015. URL http://tensorflow.\norg/. Software available from tensorﬂow.org.\nArisoy, Ebru, Sainath, Tara N, Kingsbury, Brian, and Ram-\nabhadran, Bhuvana. Deep neural network language mod-\nels. In Proceedings of the NAACL-HLT 2012 Workshop:\nWill We Ever Really Replace the N-gram Model? On the\nFuture of Language Modeling for HLT , pp. 20–28. As-\nsociation for Computational Linguistics, 2012.\nBallesteros, Miguel, Dyer, Chris, and Smith, Noah A.\nImproved transition-based parsing by modeling char-\nacters instead of words with lstms. arXiv preprint\narXiv:1508.00657, 2015.\nBengio, Yoshua and Sen´ecal, Jean-S´ebastien. Adaptive im-\nportance sampling to accelerate training of a neural prob-\nabilistic language model. Neural Networks, IEEE Trans-\nactions on, 19(4):713–722, 2008.\nBengio, Yoshua, Sen ´ecal, Jean-S ´ebastien, et al. Quick\ntraining of probabilistic neural nets by importance sam-\npling. In AISTATS, 2003.\nBengio, Yoshua, Schwenk, Holger, Sen ´ecal, Jean-\nS´ebastien, Morin, Fr ´ederic, and Gauvain, Jean-Luc.\nNeural probabilistic language models. In Innovations in\nMachine Learning, pp. 137–186. Springer, 2006.\nChelba, Ciprian, Mikolov, Tomas, Schuster, Mike, Ge,\nQi, Brants, Thorsten, Koehn, Phillipp, and Robinson,\nTony. One billion word benchmark for measuring\nprogress in statistical language modeling. arXiv preprint\narXiv:1312.3005, 2013.\nCho, Kyunghyun, Van Merri ¨enboer, Bart, Gulcehre,\nCaglar, Bahdanau, Dzmitry, Bougares, Fethi, Schwenk,\nHolger, and Bengio, Yoshua. Learning phrase represen-\ntations using rnn encoder-decoder for statistical machine\ntranslation. arXiv preprint arXiv:1406.1078, 2014.\nDeng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai,\nand Fei-Fei, Li. Imagenet: A large-scale hierarchical\nimage database. In Computer Vision and Pattern Recog-\nnition, 2009. CVPR 2009. IEEE Conference on, pp. 248–\n255. IEEE, 2009.\nFilippova, Katja, Alfonseca, Enrique, Colmenares, Car-\nlos A, Kaiser, Lukasz, and Vinyals, Oriol. Sentence com-\npression by deletion with lstms. In Proceedings of the\n2015 Conference on Empirical Methods in Natural Lan-\nguage Processing, pp. 360–368, 2015.\nGers, Felix A, Schmidhuber, J ¨urgen, and Cummins, Fred.\nLearning to forget: Continual prediction with lstm. Neu-\nral computation, 12(10):2451–2471, 2000.\nGillick, Dan, Brunk, Cliff, Vinyals, Oriol, and Subra-\nmanya, Amarnag. Multilingual language processing\nfrom bytes. arXiv preprint arXiv:1512.00103, 2015.\nGraves, Alex. Generating sequences with recurrent neural\nnetworks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex and Schmidhuber, J ¨urgen. Framewise\nphoneme classiﬁcation with bidirectional lstm and other\nneural network architectures. Neural Networks, 18(5):\n602–610, 2005.\nGutmann, Michael and Hyv ¨arinen, Aapo. Noise-\ncontrastive estimation: A new estimation principle for\nunnormalized statistical models. In International Con-\nference on Artiﬁcial Intelligence and Statistics, pp. 297–\n304, 2010.\nHochreiter, Sepp and Schmidhuber, J ¨urgen. Long short-\nterm memory. Neural computation , 9(8):1735–1780,\n1997.\nJi, Shihao, Vishwanathan, S. V . N., Satish, Nadathur, An-\nderson, Michael J., and Dubey, Pradeep. Blackout:\nSpeeding up recurrent neural network language models\nExploring the Limits of Language Modeling\nwith very large vocabularies. CoRR, abs/1511.06909,\n2015a. URL http://arxiv.org/abs/1511.\n06909.\nJi, Yangfeng, Cohn, Trevor, Kong, Lingpeng, Dyer, Chris,\nand Eisenstein, Jacob. Document context language mod-\nels. arXiv preprint arXiv:1511.03962, 2015b.\nJozefowicz, Rafal, Zaremba, Wojciech, and Sutskever,\nIlya. An empirical exploration of recurrent network ar-\nchitectures. In Proceedings of the 32nd International\nConference on Machine Learning (ICML-15), pp. 2342–\n2350, 2015.\nKalchbrenner, Nal, Grefenstette, Edward, and Blunsom,\nPhil. A convolutional neural network for modelling sen-\ntences. arXiv preprint arXiv:1404.2188, 2014.\nKim, Yoon, Jernite, Yacine, Sontag, David, and Rush,\nAlexander M. Character-aware neural language models.\narXiv preprint arXiv:1508.06615, 2015.\nKneser, Reinhard and Ney, Hermann. Improved backing-\noff for m-gram language modeling. InAcoustics, Speech,\nand Signal Processing, 1995. ICASSP-95., 1995 Inter-\nnational Conference on, volume 1, pp. 181–184. IEEE,\n1995.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.\nImagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in neural information processing\nsystems, pp. 1097–1105, 2012.\nLe Cun, B Boser, Denker, John S, Henderson, D, Howard,\nRichard E, Hubbard, W, and Jackel, Lawrence D. Hand-\nwritten digit recognition with a back-propagation net-\nwork. In Advances in neural information processing sys-\ntems. Citeseer, 1990.\nLing, Wang, Lu ´ıs, Tiago, Marujo, Lu ´ıs, Astudillo,\nRam´on Fernandez, Amir, Silvio, Dyer, Chris, Black,\nAlan W, and Trancoso, Isabel. Finding function in form:\nCompositional character models for open vocabulary\nword representation. arXiv preprint arXiv:1508.02096,\n2015.\nLuong, Minh-Thang, Sutskever, Ilya, Le, Quoc V , Vinyals,\nOriol, and Zaremba, Wojciech. Addressing the rare word\nproblem in neural machine translation. arXiv preprint\narXiv:1410.8206, 2014.\nMarcus, Mitchell P, Marcinkiewicz, Mary Ann, and San-\ntorini, Beatrice. Building a large annotated corpus of\nenglish: The penn treebank. Computational linguistics,\n19(2):313–330, 1993.\nMikolov, Tom´aˇs. Statistical language models based on neu-\nral networks. Presentation at Google, Mountain View,\n2nd April, 2012.\nMikolov, Tomas and Zweig, Geoffrey. Context dependent\nrecurrent neural network language model. In SLT, pp.\n234–239, 2012.\nMikolov, Tomas, Karaﬁ ´at, Martin, Burget, Lukas, Cer-\nnock`y, Jan, and Khudanpur, Sanjeev. Recurrent neural\nnetwork based language model. In INTERSPEECH, vol-\nume 2, pp. 3, 2010.\nMikolov, Tomas, Deoras, Anoop, Kombrink, Stefan, Bur-\nget, Lukas, and Cernock`y, Jan. Empirical evaluation and\ncombination of advanced language modeling techniques.\nIn INTERSPEECH, number s 1, pp. 605–608, 2011.\nMnih, Andriy and Hinton, Geoffrey E. A scalable hierar-\nchical distributed language model. InAdvances in neural\ninformation processing systems, pp. 1081–1088, 2009.\nMnih, Andriy and Kavukcuoglu, Koray. Learning word\nembeddings efﬁciently with noise-contrastive estima-\ntion. In Advances in Neural Information Processing Sys-\ntems, pp. 2265–2273, 2013.\nMorin, Frederic and Bengio, Yoshua. Hierarchical proba-\nbilistic neural network language model. In Aistats, vol-\nume 5, pp. 246–252. Citeseer, 2005.\nPascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.\nOn the difﬁculty of training recurrent neural networks.\narXiv preprint arXiv:1211.5063, 2012.\nRush, Alexander M, Chopra, Sumit, and Weston, Jason. A\nneural attention model for abstractive sentence summa-\nrization. arXiv preprint arXiv:1509.00685, 2015.\nSak, Hasim, Senior, Andrew W, and Beaufays, Franc ¸oise.\nLong short-term memory recurrent neural network archi-\ntectures for large scale acoustic modeling. In INTER-\nSPEECH, pp. 338–342, 2014.\nSchuster, Mike and Paliwal, Kuldip K. Bidirectional recur-\nrent neural networks. Signal Processing, IEEE Transac-\ntions on, 45(11):2673–2681, 1997.\nSchwenk, Holger, Rousseau, Anthony, and Attik, Mo-\nhammed. Large, pruned or continuous space language\nmodels on a gpu for statistical machine translation. In\nProceedings of the NAACL-HLT 2012 Workshop: Will\nWe Ever Really Replace the N-gram Model? On the Fu-\nture of Language Modeling for HLT, pp. 11–19. Associ-\nation for Computational Linguistics, 2012.\nSerban, Iulian Vlad, Sordoni, Alessandro, Bengio, Yoshua,\nCourville, Aaron C., and Pineau, Joelle. Hierarchical\nneural network generative models for movie dialogues.\nCoRR, abs/1507.04808, 2015. URL http://arxiv.\norg/abs/1507.04808.\nExploring the Limits of Language Modeling\nShazeer, Noam, Pelemans, Joris, and Chelba, Ciprian.\nSparse non-negative matrix language modeling for skip-\ngrams. Proceedings of Interspeech , pp. 1428–1432,\n2015.\nSrivastava, Nitish. Improving neural networks with\ndropout. PhD thesis, University of Toronto, 2013.\nSrivastava, Nitish, Mansimov, Elman, and Salakhutdinov,\nRuslan. Unsupervised learning of video representations\nusing lstms. arXiv preprint arXiv:1502.04681, 2015a.\nSrivastava, Rupesh K, Greff, Klaus, and Schmidhuber,\nJ¨urgen. Training very deep networks. In Advances in\nNeural Information Processing Systems, pp. 2368–2376,\n2015b.\nSutskever, Ilya, Martens, James, and Hinton, Geoffrey E.\nGenerating text with recurrent neural networks. In Pro-\nceedings of the 28th International Conference on Ma-\nchine Learning (ICML-11), pp. 1017–1024, 2011.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V . Se-\nquence to sequence learning with neural networks. In\nAdvances in neural information processing systems, pp.\n3104–3112, 2014.\nVaswani, Ashish, Zhao, Yinggong, Fossum, Victoria, and\nChiang, David. Decoding with large-scale neural lan-\nguage models improves translation. Citeseer.\nVincent, Pascal, de Br´ebisson, Alexandre, and Bouthillier,\nXavier. Efﬁcient exact gradient update for training deep\nnetworks with very large sparse targets. In Advances in\nNeural Information Processing Systems, pp. 1108–1116,\n2015.\nVinyals, Oriol and Le, Quoc. A neural conversational\nmodel. arXiv preprint arXiv:1506.05869, 2015.\nWang, Tian and Cho, Kyunghyun. Larger-context language\nmodelling. arXiv preprint arXiv:1511.03729, 2015.\nWilliams, Ronald J and Peng, Jing. An efﬁcient gradient-\nbased algorithm for on-line training of recurrent network\ntrajectories. Neural computation, 2(4):490–501, 1990.\nWilliams, Will, Prasad, Niranjani, Mrva, David, Ash, Tom,\nand Robinson, Tony. Scaling recurrent neural network\nlanguage models. In Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2015 IEEE International Conference\non, pp. 5391–5395. IEEE, 2015.\nZaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol.\nRecurrent neural network regularization. arXiv preprint\narXiv:1409.2329, 2014."
}