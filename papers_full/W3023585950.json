{
  "title": "Investigating Transferability in Pretrained Language Models",
  "url": "https://openalex.org/W3023585950",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2925357447",
      "name": "Alex Tamkin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2467784549",
      "name": "Trisha Singh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2137085090",
      "name": "Davide Giovanardi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2259447828",
      "name": "Noah Goodman",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2149933564",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2109553965",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2952502547",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2991391304",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2970854840",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "How does language model pretraining help transfer learning? We consider a simple ablation technique for determining the impact of each pretrained layer on transfer task performance. This method, partial reinitialization, involves replacing different layers of a pretrained model with random weights, then finetuning the entire model on the transfer task and observing the change in performance. This technique reveals that in BERT, layers with high probing performance on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks. Furthermore, the benefit of using pretrained parameters for a layer varies dramatically with finetuning dataset size: parameters that provide tremendous performance improvement when data is plentiful may provide negligible benefits in data-scarce settings. These results reveal the complexity of the transfer learning process, highlighting the limitations of methods that operate on frozen models or single data samples.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1393–1401\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n1393\nInvestigating Transferability in Pretrained Language Models\nAlex Tamkin†\nStanford University\nTrisha Singh\nStanford University\nDavide Giovanardi\nStanford University\nNoah Goodman\nStanford University\nAbstract\nHow does language model pretraining help\ntransfer learning? We consider a simple ab-\nlation technique for determining the impact\nof each pretrained layer on transfer task per-\nformance. This method, partial reinitializa-\ntion, involves replacing different layers of a\npretrained model with random weights, then\nﬁnetuning the entire model on the transfer\ntask and observing the change in performance.\nThis technique reveals that in BERT, lay-\ners with high probing performance on down-\nstream GLUE tasks are neither necessary nor\nsufﬁcient for high accuracy on those tasks. Fur-\nthermore, the beneﬁt of using pretrained pa-\nrameters for a layer varies dramatically with\nﬁnetuning dataset size: parameters that pro-\nvide tremendous performance improvement\nwhen data is plentiful may provide negligible\nbeneﬁts in data-scarce settings. These results\nreveal the complexity of the transfer learning\nprocess, highlighting the limitations of meth-\nods that operate on frozen models or single\ndata samples.\n1 Introduction\nDespite the striking success of transfer learning\nin NLP, remarkably little is understood about how\nthese pretrained models improve downstream task\nperformance. Recent work on understanding deep\nNLP models has centered on probing, a methodol-\nogy that involves training classiﬁers for different\ntasks on model representations (Alain and Bengio,\n2016; Conneau et al., 2018; Hupkes et al., 2018;\nLiu et al., 2019; Tenney et al., 2019a,b; Goldberg,\n2019; Hewitt and Manning, 2019). While prob-\ning aims to uncover what a network has already\nlearned, a major goal of machine learning is trans-\nfer: systems that build upon what they have learned\nto expand what they can learn. Given that most\n† atamkin@stanford.edu\nFigure 1: The three experiments we explore. Lighter\nshades indicate randomly reinitialized layers, while\ndarker shades indicate layers with BERT parameters.\nFor layer permutations, all layers hold BERT param-\neters, what changes between trials is their order. In all\nthree experiments, the entire model is ﬁnetuned end-to-\nend on the GLUE task.\nrecent models are updated end-to-end during ﬁne-\ntuning (e.g. Devlin et al., 2019; Howard and Ruder,\n2018; Radford et al., 2019), it is unclear how, or\neven whether, the knowledge uncovered by prob-\ning contributes to these models’ transfer learning\nsuccess.\nIn a sense, probing can be seen as quantifying\nthe transferability of representations from one task\nto another, as it measures how well a simple model\n(e.g., a softmax classiﬁer) can perform the second\ntask using only features from a model trained on\nthe ﬁrst. However, when pretrained models are\nﬁnetuned end-to-end on a downstream task, what\nis transferred is not the features from each layer\nof the pretrained model, but its parameters, which\ndeﬁne a sequence of functions for processing rep-\nresentations. Critically, these functions and their\ninteractions may shift considerably during training,\npotentially enabling higher performance despite\nnot initially extracting features correlated with this\ntask. We refer to this phenomenon of how layer\nparameters from one task can help transfer learning\n1394\nFigure 2: The beneﬁt of using BERT parameters in-\nstead of random parameters at a particular layer\nvaries dramatically depending on the size of the ﬁne-\ntuning dataset. However, as ﬁnetuning dataset size\ndecreases, the curves align more closely with prob-\ning performance at each layer. Solid lines show ﬁne-\ntuning results after reinitializing all layers past layer k\nin BERT-Base. 12 shows the full BERT model, while 0\nshows a model with all layers reinitialized. Line dark-\nness indicates subsampled dataset size. The dashed\nlines show probing performance at each layer. Error\nbars are 95% CIs.\non another task as transferability of parameters.\nIn this work, we investigate a methodology for\nmeasuring the transferability of different layer pa-\nrameters in a pretrained language model to different\ntransfer tasks, using BERT (Devlin et al., 2019) as\nour subject of analysis. Our methods, described\nmore fully in Section 2 and Figure 1, involve par-\ntially reinitializing BERT: replacing different lay-\ners with random weights and then observing the\nchange in task performance after ﬁnetuning the\nentire model end-to-end. Compared to possible\nalternatives like freezing parts of the network or re-\nmoving layers, partial reinitialization enables fairer\ncomparisons by keeping the network’s architec-\nture and capacity constant between trials, changing\nonly the parameters at initialization. Through ex-\nperiments across different layers, tasks, and dataset\nsizes, this approach enables us to shed light on mul-\ntiple dimensions of the transfer learning process:\nAre the early layers of the network more important\nthan later ones for transfer learning? Do individ-\nual layers become more or less critical depending\non the task or amount of ﬁnetuning data? Does\nthe position of a particular layer within the net-\nwork matter, or do its parameters aid optimization\nregardless of where they are in the network?\nWe ﬁnd that when ﬁnetuning on a new task:\n1. Transferability of BERT layers varies dramat-\nically depending on the amount of ﬁnetuning\ndata available. Thus, claims that certain lay-\ners are universally responsible or important\nfor learning certain linguistic tasks should be\ntreated with caution. (Figure 2)\n2. Transferability of BERT layers is not in gen-\neral predicted by the layer’s probing perfor-\nmance for that task. However, as ﬁnetuning\ndataset size decreases, the two quantities ex-\nhibit a greater correspondence. (Figure 2,\ndashed lines)\n3. Even holding dataset size constant, the most\ntransferable BERT layers differ by task: for\nsome tasks, only the early layers are impor-\ntant, while for others the beneﬁts are more\ndistributed across layers. (Figure 3)\n4. Reordering the pretrained BERT layers be-\nfore ﬁnetuning decreases downstream accu-\nracy signiﬁcantly, conﬁrming that pretraining\ndoes not simply provide better-initialized indi-\nvidual layers; instead, transferability through\nlearned interactions across layers is crucial to\nthe success of ﬁnetuning. (Figure 4)\n2 How many pretrained layers are\nnecessary for ﬁnetuning?\nOur ﬁrst set of experiments aims to uncover how\nmany pretrained layers are sufﬁcient for accu-\nrate learning of a downstream task. To do this,\nwe perform a series of incremental reinitializa-\ntion experiments, where we reinitialize all lay-\ners after the kth layer of BERT-Base, for values\nk ∈ {0,1,... 12}, replacing them with random\nweights. We then ﬁnetune the entire model end-to-\nend on the target task. Note that k= 0corresponds\nto a BERT model with all layers reinitialized, while\nk = 12 is the original BERT model. We do not\nreinitialize the BERT word embeddings. As BERT\nuses residual connections (He et al., 2016) around\nlayers, the model can simply learn to ignore any of\nthe reinitialized layers if they are not helpful during\nﬁnetuning.\nWe use the BERT-Base uncased model, imple-\nmented in PyTorch (Paszke et al., 2019) via the\nTransformers library (Wolf et al., 2019). We ﬁne-\ntune the network using Adam (Kingma and Ba,\n2015), with a batch size of 8, a learning rate of\n2e-5, and default parameters otherwise. More de-\n1395\ntails about reinitialization, training, statistical sig-\nniﬁcance, and other methodological choices can\nbe found in the Appendix. We conduct our exper-\niments on three English language tasks from the\nGLUE benchmark, spanning the domains of senti-\nment, reasoning, and syntax (Wang et al., 2018):\nSST-2 Stanford Sentiment Treebank involves bi-\nnary classiﬁcation of a single sentence from a\nmovie review as positive or negative (Socher et al.,\n2013).\nQNLI Question Natural Language Inference is a\nbinary classiﬁcation task derived from SQuAD (Ra-\njpurkar et al., 2016; Wang et al., 2018). The task re-\nquires determining whether for a given (QUESTION ,\nANSWER ) pair the QUESTION is answered by the\nANSWER .\nCoLA The Corpus of Linguistic Acceptability\nis a binary classiﬁcation task that requires deter-\nmining whether a single sentence is linguistically\nacceptable (Warstadt et al., 2019).\nBecause pretraining appears to be especially\nhelpful in the small-data regime (Peters et al.,\n2018), it is crucial to isolate task-speciﬁc effects\nfrom data quantity effects by controlling for ﬁne-\ntuning dataset size. To do this, we perform our\nincremental reinitializations on randomly-sampled\nsubsets of the data: 500, 5k, and 50k examples (ex-\ncluding 50k for CoLA, which contains only 8.5k\nexamples). The 5k subset size is then used as the\ndefault for our other experiments. To ensure that an\nunrepresentative sample is not chosen by chance,\nwe run multiple trials with different subsamples.\nConﬁdence intervals produced through multiple tri-\nals also demonstrate that trends hold regardless of\nintrinsic task variability.\nWhile similar reinitialization schemes have been\nexplored by Yosinski et al. (2014); Raghu et al.\n(2019) in computer vision and brieﬂy by Radford\net al. (2019) in an NLP context, none investigate\nthese data quantity- and task-speciﬁc effects.\nFigure 2 shows the results of our incremental\nreinitialization experiments. These results show\nthat the transferability of a BERT layer varies\ndramatically based on the ﬁnetuning dataset size.\nAcross all but the 500 example trials of SST-2, a\nmore speciﬁc trend holds: earlier layers provide\nmore of an improvement on ﬁnetuning performance\nwhen the ﬁnetuning dataset is large. This trend sug-\ngests that larger ﬁnetuning datasets may enable the\nnetwork to learn a substitute for the parameters in\nthe middle and later layers. In contrast, smaller\ndatasets may leave the network reliant on exist-\ning feature processing in those layers. However,\nacross all tasks and dataset sizes, it is clear that\nthe pretrained parameters by themselves do not de-\ntermine the impact they will have on ﬁnetuning\nperformance: instead, a more complex interaction\noccurs between the parameters, optimizer, and the\navailable data.\n3 Does probing predict layer\ntransferability?\nWhat is the relationship between transferability of\nrepresentations, measured by probing, and trans-\nferability of parameters, measured by partial reini-\ntialization? To compare, we conduct probing ex-\nperiments for our ﬁnetuning tasks on each layer of\nthe pretrained BERT model. Our probing model\naverages each layer’s hidden states, then passes the\npooled representation through a linear layer and\nsoftmax to produce probabilities for each class.\nThese task-speciﬁc components are identical to\nthose in our reinitialization experiments; however,\nwe keep the BERT model’s parameters frozen when\ntraining our probes.\nOur results, presented in Figure 2 (dashed lines),\nshow a signiﬁcant difference between the layers\nwith the highest probing performance and reinitial-\nization curves for the data-rich settings (darkest\nsolid lines). For example, the probing accuracy\non all tasks is near chance for the ﬁrst six layers.\nDespite this, these early layer parameters exhibit\nsigniﬁcant transferability to the ﬁnetuning tasks:\npreserving them while reinitializing all other layers\nenables large gains in ﬁnetuning accuracy across\ntasks. Interestingly, however, we observe that the\nsmallest-data regime’s curves are much more simi-\nlar to the probing curves across all tasks than the\nlarger-data regimes. Smaller ﬁnetuning datasets\nenable fewer updates to the network before over-\nﬁtting occurs; thus, it may be that ﬁnetuning inter-\npolates between the extremes of probing (no data)\nand fully-supervised learning (enough data to com-\npletely overwrite the pretrained parameters). We\nleave a more in-depth exploration of this connec-\ntion to future work.\n4 Which layers are most useful for\nﬁnetuning?\nWhile the incremental reinitializations measure\neach BERT layer’s incremental effect on transfer\n1396\nFigure 3: Early layers provide the most QNLI gains,\nbut middle ones yield an added boost for CoLA and\nSST-2. Finetuning results for 1) reinitializing a con-\nsecutive three-layer block (“block reinitialized”) and\n2) reinitializing all other layers (“block preserved”).\nDashed horizontal lines show the ﬁnetuning perfor-\nmance of the full BERT model and the performance\nof a model with only embedding parameters preserved.\nFinetuning trials with 5k examples. Error bars are 95%\nCIs.\nlearning, they do not assess each layer’s contribu-\ntion in isolation, relative to either the full BERT\nmodel or an entirely reinitialized model. Measur-\ning this requires eliminating the number of pre-\ntrained layers as a possible confounder. To do\nso, we conduct a series of localized reinitializa-\ntion experiments, where we take all blocks of three\nconsecutive layers and either 1) reinitialize those\nlayers or 2) preserve those layers while reinitial-\nizing the others in the network.1 These localized\nreinitializations help determine the extent to which\nBERT’s different layers are either necessary (per-\nformance decreases when they are removed) or\nsufﬁcient (performance is higher than random ini-\ntialization when they are kept) for a speciﬁc level\nof performance. Again, BERT’s residual connec-\ntions permit the model to ignore reinitialized layers’\noutputs if they harm ﬁnetuning performance.\nThese results, shown in Figure 3, demonstrate\nthat the earlier layers appear to be generally more\nhelpful for ﬁnetuning relative to the later layers,\neven when controlling for the amount of ﬁnetun-\ning data. However, there are strong task-speciﬁc\neffects: SST-2 appears to be particularly damaged\nby removing middle layers, while the effects on\nCoLA are distributed more uniformly. The effects\n1See the Appendix for more discussion and experiments\nwhere only one layer is reinitialized.\nFigure 4: Changing the order of pretrained layers\nharms ﬁnetuning performance signiﬁcantly. Dashed\nlines mark the performance of the original BERT model\nand the randomly-initialized model (surrounded by\n±2σ error bars). Circles denote ﬁnetuning perfor-\nmance for different layer permutations, while the solid\nline denotes the mean across runs (with 95% CIs). The\ncurved shaded region is a kernel density plot, which il-\nlustrates the distribution of outcomes. Finetuning trials\nwith 5k examples.\non QNLI appear to be concentrated almost entirely\nin the ﬁrst four layers of BERT—suggesting op-\nportunities for future work on whether sparsity of\nthis sort indicates the presence of easy-to-extract\nfeatures correlated with the task label. These re-\nsults support the hypothesis that different kinds of\nfeature processing learned during BERT pretrain-\ning are helpful for different ﬁnetuning tasks, and\nprovide a new way to gauge similarity between\ndifferent tasks.\n5 How vital is the ordering of pretrained\nlayers?\nWe also investigate whether the success of BERT\ndepends mostly on learned inter-layer phenomena,\nsuch as learned feature processing pipelines (Ten-\nney et al., 2019a), or intra-layer phenomena, such\nas a learned feature-agnostic initialization scheme\nwhich aid optimization (e.g. Glorot and Bengio,\n2010). To approach this question, we perform\nseveral layer permutation experiments, where we\nrandomly shufﬂe the order of BERT’s layers before\nﬁnetuning. The degree that ﬁnetuning performance\nis degraded in these runs indicates the extent to\nwhich BERT’s ﬁnetuning success is dependent on\na learned composition of feature processors, as\nopposed to providing better-initialized individual\nlayers which would help optimization anywhere in\nthe network.\nThese results, plotted in Figure 4, show that\nscrambling BERT’s layers reduces their ﬁnetuning\n1397\nability to not much above a randomly-initialized\nnetwork, on average. This decrease suggests that\nBERT’s transfer abilities are highly dependent on\nthe intra-layer interactions learned during pretrain-\ning.\nWe also test for correlation of performance be-\ntween tasks. We do this by comparing task-pairs for\neach permutation, as we use the same permutation\nfor the nth run of each task. The high correlation\ncoefﬁcients for most pairs shown in Table 1 suggest\nthat BERT ﬁnetuning relies on similar inter-layer\nstructures across tasks.\nTasks compared Spearman Pearson\nSST-2, QNLI 0.72 (0.02) 0.46 (0.18)\nSST-2, CoLA 0.74 (0.02) 0.77 (0.01)\nQNLI, CoLA 0.83 (0.00) 0.68 (0.03)\nTable 1: Speciﬁc permutations of layers have simi-\nlar impacts on ﬁnetuning across tasks. Paired cor-\nrelation coefﬁcients between task performances for the\nsame permutations. Two-sided p-value in parentheses\n(N=10).\n6 Conclusion\nWe present a set of experiments to better under-\nstand how the different pretrained layers in BERT\ninﬂuence its transfer learning ability. Our results\nreveal the unique importance of transferability of\nparameters to successful transfer learning, distinct\nfrom the transferability of ﬁxed representations as-\nsessed by probing. We also disentangle important\nfactors affecting the role of layers in transfer learn-\ning: task vs. quantity of ﬁnetuning data, number\nvs. location of pretrained layers, and presence vs.\norder of layers.\nWhile probing continues to advance our under-\nstanding of linguistic structures in pretrained mod-\nels, these results indicate that new techniques are\nneeded to connect these ﬁndings to their potential\nimpacts on ﬁnetuning. The insights and methods\npresented here are one contribution toward this\ngoal, and we hope they enable more work on un-\nderstanding why and how these models work.\nAcknowledgements\nWe would like to thank Dan Jurafsky, Pranav Ra-\njpurkar, Shyamal Buch, Isabel Papadimitriou, John\nHewitt, Peng Qi, Kawin Ethayarajh, Nelson Liu,\nand Jesse Michel for useful discussions and com-\nments on drafts. This work was supported in part\nby DARPA under agreement FA8650-19-C-7923.\nReferences\nGuillaume Alain and Yoshua Bengio. 2016. Under-\nstanding intermediate layers using linear classiﬁer\nprobes.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth interna-\ntional conference on artiﬁcial intelligence and statis-\ntics, pages 249–256.\nYoav Goldberg. 2019. Assessing BERT’s syntactic\nabilities. arXiv preprint arXiv:1901.05287.\nKaiming He, Ross Girshick, and Piotr Doll ´ar. 2019.\nRethinking imagenet pre-training. In Proceedings\nof the IEEE International Conference on Computer\nVision, pages 4918–4927.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\n1398\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.\n2018. Visualisation and’diagnostic classiﬁers’ re-\nveal how recurrent and recursive neural networks\nprocess hierarchical structure. Journal of Artiﬁcial\nIntelligence Research, 61:907–926.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nBrian W Matthews. 1975. Comparison of the pre-\ndicted and observed secondary structure of t4 phage\nlysozyme. Biochimica et Biophysica Acta (BBA)-\nProtein Structure, 405(2):442–451.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. PyTorch: An imperative style,\nhigh-performance deep learning library. In Ad-\nvances in Neural Information Processing Systems ,\npages 8024–8035.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nMaithra Raghu, Chiyuan Zhang, Jon Kleinberg, and\nSamy Bengio. 2019. Transfusion: Understanding\ntransfer learning for medical imaging. In Advances\nin neural information processing systems , pages\n3347–3357.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019b. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Fun-\ntowicz, et al. 2019. Transformers: State-of-the-\nart natural language processing. arXiv preprint\narXiv:1910.03771.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in neural information\nprocessing systems, pages 3320–3328.\n1399\nA Code\nOur code is available at https://github.com/\ndgiova/bert-lm-transferability.\nB Reinitialization\nWe reinitialize all parameters in each layer, ex-\ncept those for layer normalization (Ba et al., 2016),\nby sampling from a truncated normal distribu-\ntion with µ = 0,σ = 0.02 and truncation range\n(−0.04,0.04). For the layer norm parameters, we\nset β = 0,γ = 1. This matches how BERT was ini-\ntialized (see the original BERT code on GitHub and\nthe corresponding TensorFlow documentation).\nC Subsampling, number of trials, and\nerror bars\nThe particular datapoints subsampled can have a\nlarge impact on downstream performance, espe-\ncially when data is scarce. To capture the full range\nof outcomes due to subsampling, we randomly sam-\nple a different dataset for each trial index. Due to\nthis larger variation when data is scarce, we per-\nform 50 trials for the experiments with 500 exam-\nples, while we perform three trials for the other\nincremental reinitialization experiments. A scatter-\nplot of the 500-example trials is shown in Figure 5.\nFor the localized reinitialization experiments, we\nperform ten trials each.\nError bars shown on all graphs in the main text\nare 95% conﬁdence intervals calculated with a t-\ndistribution.\nFigure 5: Finetuning results after reinitializing all lay-\ners past layer kin BERT-Base. 12 shows the full BERT\nmodel, while 0 shows a model with all layers reinitial-\nized. Scatterplot of 50 trials per layer shown for sub-\nsampled dataset size 500. Dotted line shows the mean.\nD Localized reinitializations of single\nlayers\nWe also experiment with performing our localized\nreinitialization experiments at the level of a single\nlayer. To do so, we perform three trials of reinitial-\nizing each layer k∈{1 ... 12}and then ﬁnetuning\non each of the three GLUE tasks. Our results are\nplotted in Figure 6. Interestingly, we observe little\neffect on ﬁnetuning performance from reinitializing\neach layer (except for reinitializing the ﬁrst layer on\nCoLA performance). This lack of effect suggests\neither redundant information between layers or that\nthe “interface” exposed by the two neighboring lay-\ners somehow beneﬁcially constrains optimization.\nFigure 6: Performance on ﬁnetuning tasks after reini-\ntializing an individual layer of BERT. Error bars are\n±2 standard deviations.\nE Number of ﬁnetuning epochs\nHe et al. (2019) found that much or all of the perfor-\nmance gap between an ImageNet-pretrained model\nand a model trained from random initialization\ncould be closed when the latter model was trained\nfor longer. To evaluate this, we track validation\nlosses up to ten epochs in our incremental experi-\nments, for k ∈{0,6,12}across all tasks and for\n500 and 5k examples. We ﬁnd minimal effects\nof training longer than three epochs for the sub-\nsamples of 5k, but ﬁnd improvements of several\npercentage points for training for ﬁve epochs for\nthe trials with 500 examples. Thus, for the trials\nof 500 in Figure 2, we train for ﬁve epochs, while\ntraining for three epochs for all other trials. We\ntrain our probing experiments (8 trials per layer)\nwith early stopping for a maximum of 40 epochs\non the full dataset.\n1400\nF Higher learning rate for reinitialized\nlayers\nIn their reinitialization experiments on a convolu-\ntional neural network for medical images, Raghu\net al. (2019) found that a 5x larger rate on the\nreinitialized layers enabled their model to achieve\nhigher ﬁnetuning accuracy. To evaluate this possi-\nbility in our setting, we increase the learning rate\nby a factor of ﬁve for the reinitialized layers. The\nresults for our incremental reinitializations are plot-\nted in Figure 7. A higher learning rate appears\nto increase the variance of the evaluation metrics\nwhile not improving performance. Thus, we keep\nthe learning rate the same across layers.\nFigure 7: Finetuning the reinitialized layers with a\nlarger learning rate does not improve ﬁnetuning perfor-\nmance. Error bars are ±2 standard deviations.\nG Layer norm\nBecause the residual connections around each sub-\nlayer in BERT are of the form LayerNorm(x+\nSublayer(x)), reinitializing a particular layer neu-\ntralizes the effect of the last layer norm application\nfrom the previous layer in a way that cannot be cir-\ncumvented through the residual connections. How-\never, for brevity we simply refer to “reinitializing a\nlayer” in this paper.\nWe also assessed whether preserving the layer\nnorm parameters in each layer might aid optimiza-\ntion. To do so, we preserved these parameters in\nour incremental trials with 5k examples. These\ntrials are plotted in Figure 8, and demonstrate that\npreserving layer norm does not aid (and may even\nharm) ﬁnetuning of reinitialized layers.\nH Dataset descriptions and statistics\nWe display more information about the ﬁnetuning\ndatasets, including the full size of the datasets, in\nFigure 8: Preserving the layer norm parameters when\nreinitializing each layer does not improve ﬁnetuning\nperformance. Error bars are ±2 standard deviations.\nTable 2.\nI Additional experimental information\nI.1 Link to data\nScripts to download the GLUE data can be found\nat https://github.com/nyu-mll/jiant/blob/\nmaster/scripts/download_glue_data.py.\nI.2 Computing infrastructure\nAll experiments were run on single Titan XP GPUs.\nI.3 Model\nWe use the BERT-Base uncased model (110 mil-\nlion parameters) from https://huggingface.co/\ntransformers/pretrained_models.html.\nI.4 Average runtime\nAverage runtime for each approach:\n1. 500 incremental: 0.3 min / epoch * 5 epochs\n/ trial * 50 trials / layer * 12 layers / task * 3\ntasks ≈45 GPU-hrs\n2. 5k incremental: 3 min / epoch * 3 epochs /\ntrial * 3 trials / layer * 12 layers / task * 3\ntasks ≈16 GPU-hrs.\n3. 50k incremental: 30 min / epoch * 3 epochs\n/ trial * 3 trials / layer * 12 layers / task * 3\ntasks ≈7 GPU-days.\n4. 5k localized (block size 3) : 3 min / epoch *\n3 epochs / trial * 3 trials / layer * 10 layers /\ntask * 3 tasks ≈14 GPU-hrs\n5. Probing: 2.8 min / epoch * 40 epochs / trial\n* 8 trials / layer * 12 layers / task * 3 tasks\n1401\nTable 2: Task description and statistics. SST-2 and CoLA are single sentence classiﬁcation tasks, while QNLI is\na sentence-pair classiﬁcation task.\nTask # Train # Val Input, labels Eval metric\nSST-2 67k 872k sentence, {positive, negative} Accuracy\nQNLI 105k 5.4k (question, paragraph), {answer, non-answer} Accuracy\nCoLA 8.5k 1k sentence, {acceptable, not acceptable} MCC\n≈22 GPU-days. Note: 2.8 min / epoch is\nan average across layers and tasks. Earlier\nlayers take less time than later ones because\nlayers after the target layer do not need to be\ncomputed.\nI.5 Evaluation method\nTo evaluate the performance of our method, we\ncompute accuracy for SST-2 and QNLI and\nMatthews Correlation Coefﬁcient(Matthews, 1975)\nfor CoLA. We compute these metrics always on\nthe ofﬁcial validation sets, which are never seen by\nthe model during training.\nAccuracy measures the ratio of correctly pre-\ndicted labels over the size of the test set. Formally:\naccuracy = TP +TN\nTP +TN +FP +FN\nSince CoLA presents class imbalances, MCC\nis used, which is better suited for unbalanced bi-\nnary classiﬁers (Warstadt et al., 2019). It mea-\nsures the correlation of two Boolean distributions,\ngiving a value between -1 and 1. A value of\n0 means that the two distributions are uncorre-\nlated, regardless of any class imbalance. MCC =\n(TP ·TN )−(FP ·FN ))√\n(TP +FP )(TP +FN )(FP +TN )(TN +FN )\nI.6 Hyperparameters\nWe performed one experiment with a 5x learning\nrate and implemented early stopping to choose the\nnumber of epochs for the probing experiments.\nFor batch size and learning rate, we kept the\ndefault parameters for all tasks:\n•Learning rate: 2e-5\n•Batch size: 8",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8440736532211304
    },
    {
      "name": "Transferability",
      "score": 0.7926065921783447
    },
    {
      "name": "Task (project management)",
      "score": 0.6511915326118469
    },
    {
      "name": "Transfer of learning",
      "score": 0.6352299451828003
    },
    {
      "name": "Process (computing)",
      "score": 0.5502521991729736
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5411028861999512
    },
    {
      "name": "Machine learning",
      "score": 0.5259711146354675
    },
    {
      "name": "Transfer (computing)",
      "score": 0.5153131484985352
    },
    {
      "name": "Layer (electronics)",
      "score": 0.4863172471523285
    },
    {
      "name": "Natural language processing",
      "score": 0.32855045795440674
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Logit",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Parallel computing",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}