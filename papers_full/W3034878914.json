{
  "title": "Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order",
  "url": "https://openalex.org/W3034878914",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2096215548",
      "name": "Yi Liao",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2097242334",
      "name": "Xin Jiang",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2995923603",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2971167892",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2953345635",
    "https://openalex.org/W2947898088",
    "https://openalex.org/W2949644922",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W1866230956",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2966989210",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2920538220",
    "https://openalex.org/W2912937082",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2952838738",
    "https://openalex.org/W2970832665",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2971274815"
  ],
  "abstract": "Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG). In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM). We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive permutated language model. One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation. Besides, the pretrained u-PMLM also outperforms BERT on a bunch of downstream NLU tasks.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 263–274\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n263\nProbabilistically Masked Language Model Capable of\nAutoregressive Generation in Arbitrary Word Order\nYi Liao, Xin Jiang, Qun Liu\nHuawei Noah’s Ark Lab\n{liaoyi9, jiang.xin, qun.liu}@huawei.com\nAbstract\nMasked language model and autoregressive\nlanguage model are two types of language\nmodels. While pretrained masked language\nmodels such as BERT (Devlin et al., 2019)\noverwhelm the line of natural language un-\nderstanding (NLU) tasks, autoregressive lan-\nguage models such as GPT (Radford et al.,\n2018) are especially capable in natural lan-\nguage generation (NLG). In this paper, we pro-\npose a probabilistic masking scheme for the\nmasked language model, which we call prob-\nabilistically masked language model (PMLM).\nWe implement a speciﬁc PMLM with a uni-\nform prior distribution on the masking ratio\nnamed u-PMLM. We prove that u-PMLM is\nequivalent to an autoregressive permutated lan-\nguage model. One main advantage of the\nmodel is that it supports text generation in ar-\nbitrary order with surprisingly good quality,\nwhich could potentially enable new applica-\ntions over traditional unidirectional generation.\nBesides, the pretrained u-PMLM also outper-\nforms BERT on a set of downstream NLU\ntasks.\n1 Introduction\nLarge-scale pretrained language models (Raffel\net al., 2019; Wang et al., 2019; Lan et al., 2019;\nLiu et al., 2019; Jiao et al., 2019) have drawn lots\nof research attention as these models have brought\nsigniﬁcant improvements to many NLU and NLG\ntasks. As a major category of pretrained language\nmodels, masked language model (MLM) (Devlin\net al., 2019; Joshi et al., 2019) is trained using a de-\nnoising autoencoding objective. In a typical MLM,\nsome tokens in a sentence are replaced by a special\ntoken [MASK]. The training objective is to predict\nthe original tokens that are masked in the sentence.\nAs the ﬁrst large-scale pretrained masked language\nmodel, BERT chooses to mask 15% of the tokens\nin sentences randomly. Following BERT, various\nThe wolf has an extraordinary speed ,\nand it can often jump from a spot quick\nenough to escape a spot already occupied\nby an adult wolf . Unlike the brown and\nblack bear , where it is easily distracted\nby wolves , the gray fox does not run over\na wolf , and is often driven mad . Hav-\ning jumps with high speed that breaks the\nwolf ’ s legs before it is runover , a grey\nwolf could defend itself against an adult\nof other species as the best predator at any\ntime . The black bear may kill packs of\nfour lazy , though the gray fox can inﬂict\nsigniﬁcant wounds on a dog .\nFigure 1: A piece of text generated by a PMLM in ran-\ndom order. The bolded words, which compose the in-\nput sentence “The quick brown fox jumps over the lazy\ndog”, are distributed across the paragraph with a prede-\nﬁned length. The blank spaces are ﬁlled by the model\nin a random order to form the complete paragraph.\nlanguage models have been proposed with different\nmasking schemes.\nWhile the pretrained masked language models\nachieve state-of-the-art performances in a line of\ndownstream NLU tasks, researchers pay more at-\ntention to autoregressive language model when it\ncomes to text generation. Unlike predicting the\nmasked tokens, the autoregressive language model\nlearns a sequential generative process of text se-\nquences. Hence it naturally performs better for\nnatural language generation. For example, GPT-2\n(Radford et al., 2019) as well as Transformer-XL\n(Dai et al., 2019), is able to generate ﬂuent and\ncoherent paragraphs of text that highly resembles\nhuman writings.\nIn this paper, we propose a probabilistically\nmasked language model (PMLM) to bridge the gap\nbetween masked and autoregressive language mod-\n264\n \n \nPredictions: \nHidden States: \n \nTransformer \nLayers: \nInputs: \nFigure 2: The structures of autoregressive language model (left) and masked language model (right).\nels. The basic idea behind the connection of two\ncategories of models is similar to MADE (Germain\net al., 2015). PMLM is a masked language model\nwith a probabilistic masking scheme, which de-\nﬁnes the way sequences are masked by following a\nprobabilistic distribution. While the existing work\nproposes masking strategies aiming at improving\nthe NLU abilities, PMLM addresses the generation\ncapability in particular. Besides, as a masked lan-\nguage model, PMLM maintains its strong ability\nin natural language understanding.\nIn addition to the traditional unidirectional\n(e.g., left-to-right) generation, a unique ability for\nPMLM is to autoregressively generate sequences\nin arbitrary order , and the generated sequences\nare still of high quality. In contrast to traditional\nleft-to-right generation, arbitrarily ordered text gen-\neration has two main characteristics. First, the next\ntoken to be predicted could be in any position that\nis masked. Second, the next token to be predicted\ndepends on all the previous observed/generated to-\nkens. Arbitrarily ordered generation enables more\ninteresting applications than unidirectional gener-\nation. For example, Figure 1 shows an example\nof cloze test, where the prompted text “The quick\nbrown fox jumps over the lazy dog” is distributed\nacross a paragraph with a predeﬁned length, and\nthe task is to predict all the surrounding words and\ncomplete the paragraph. This is actually very chal-\nlenging for conventional generation models since\nwhen predicting each word, the ﬂuency and coher-\nence of text are hard to be guaranteed given the\ncontextual constraints on both sides. More applica-\ntions may include acrostic poetry generation, news\ngeneration based on given facts, machine transla-\ntion with lexical constraints, etc.\nWe employ a simple uniform distribution of the\nmasking ratio and name the model as u-PMLM.\nWe prove that u-PMLM actually learns an autore-\ngressive language model on random permutations\nof training sequences. The experiments show that\nthe quality of text generated by u-PMLM in arbi-\ntrary order is as good as that generated by GPT in\nsequential order. Besides, u-PMLM outperforms\nBERT signiﬁcantly on the GLUE benchmark for\nnatural language understanding.\n2 Preliminary\n2.1 Transformer\nTransformer (Vaswani et al., 2017) is the backbone\nmodel for many pretrained language models. Trans-\nformer is composed of a stack of multi-head self-\nattention and token-wise feed-forward layers. At\neach layer, the hidden state of each token is updated\nbased on the historical hidden states computed in\nthe lower layer. Let X = {x1,x2,...,x N}denote\nthe sequence of tokens, whereNis the length of the\nsequence. Fed with X as input, the ﬁnal output of\nthe Transformer, denoted as H = {h1,h2,...,h N},\ncaptures the contextual representation of the tokens\nin the sequence.\n2.2 Autoregressive Language Model\nIn autoregressive language model, the sequence\ngeneration process is modeled as a Markov chain,\nwhere the token to be predicted depends on all\nthe previous tokens. The training objective can be\nformulated as:\nLalm(X) =\nN∑\nn=1\nlog p(xn|x1,...,x n−1; θ), (1)\nwhere θdenotes the parameters of the model. Fig-\nure 2(a) shows the diagram of autoregressive LM.\nIn the model, the n-th token can only attend on\n265\nthe tokens at positions less than n. The autore-\ngressive model is usually trained in the way of\nteacher-forcing, i.e., always using the ground-truth\ntokens as inputs and outputs in training.\nPretrained autoregressive models such as GPT\n(Radford et al., 2018, 2019) are especially capable\nof generating ﬂuent and coherent text that highly\nresembles human-written text. However, unidi-\nrectional attention brings two limitations. Firstly,\nautoregressive model as in Figure 2(a) can only\ngenerate text from left to right; Secondly, unidirec-\ntional attention blocks the contextual information\nfrom the right side of the current token, affecting\nthe completeness of the contextual representation.\n2.3 Masked Language Model\nTo obtain complete representations of the tokens\nin a sequence, researchers resort to bidirectional\nattention as shown in Figure 2(b). Speciﬁcally,\nthe training instances are created by replacing a\nsubset of tokens in the inputXwith a special token\n[MASK], and the objective is to predict the masked\ntokens. Such model is called masked language\nmodel (MLM). Let Π = {π1,π2,...,π K}denote\nthe indexes of the masked tokens in the sentenceX,\nwhere Kis the number of masked tokens. Let XΠ\ndenote the set of masked tokens in X, and X−Π\ndenote the set of observed (unmasked) tokens. The\nobjective of MLM is:\nLmlm(XΠ|X−Π) = 1\nK\nK∑\nk=1\nlog p(xπk|X−Π; θ).\n(2)\nThe assumption in Equation 2 is that the probability\nof predicting a masked token is independent of\neach other. BERT (Devlin et al., 2019) is a typical\nmasked language model.\nDue to the incorporation of bidirectional atten-\ntion, masked language model can capture the con-\ntextual information on both sides. Consequently,\nit usually achieves better performances when ﬁne-\ntuned in downstream NLU tasks than the conven-\ntional autoregressive models. However, the mask-\ning scheme and the independence assumption also\naffect its performance on text generation compared\nto autoregressive models (Wang and Cho, 2019).\n3 Probabilistically Masked Language\nModel\nDifferent masking schemes have been proposed for\npretraining the masked language model. The most\nstraightforward masking scheme is to randomly\nmask tokens in sentences in a ﬁxed ratio, e.g., 15%\nin BERT. Following BERT, various models have\nproposed modifying the masking scheme to im-\nprove its NLU capability. ERNIE (Sun et al., 2019)\nproposes the entity-level masking and phrase-level\nmasking, where the words composing an entity or\nphrase are masked as a whole. SpanBERT (Joshi\net al., 2019) proposes to mask a continuous ran-\ndom span of text rather than random tokens. These\nmasking strategies have shown to be effective for\ncertain classes of NLU tasks.\nIn contrast to the existing work, we propose a\nprobabilistic masking scheme that tries to improve\nthe text generation ability of the masked language\nmodel. Probabilistically masked language mode\n(PMLM) is a natural generalization of the MLM\nwith a probabilistic masking ratio. It assumes that\nthe masking ratio is drawn from a probabilistic\ndistribution. Therefore, each training instance is\nassociated with a different masking ratio sampled\nfrom the given distribution.\n3.1 Model Formulation\nTo give a formal deﬁnition of the PMLM, we need\nto elaborate the training objective deﬁned in Equa-\ntion 2. Let M = {m1,m2,...,m N}denote a se-\nquence of binary variables indicating which token\nin X = {x1,x2,...,x N}is masked. mn = 1 indi-\ncates xn is masked, and mn = 0 otherwise. Noted\nthat since Π = {π1,π2,...,π K}denotes the in-\ndexes of masked tokens, mπk = 1 holds for any\nπk ∈Π. Considering M as latent variables, the\nexpected log-likelihood function of observing XΠ\nconditioning on X−Π over all possible M is:\nLpmlm(XΠ|X; θ)\n=EM|X[log p(XΠ|X−Π)]\n=\n∑\nM\n[log p(XΠ|X−Π; θ)]p(M|X)\n(3)\nThe term log p(XΠ|X−Π; θ) is identical to the ob-\njective function in Equation 2 for a deterministic\nmask M. In the vanilla MLM, it is assumed that\nM are i.i.d. for each position and independent to\nX, namely,\np(M|X) = p(M) = rK(1 −r)N−K, (4)\nwhere ris the masking ratio.\nMost existing MLMs such as BERT simply set a\nﬁxed value to the masking ratio r. In our proposed\n266\nPMLM, however, we assumeris a random variable\ndrawn from a prior distributionp(r). Therefore, the\ndistribution p(M) becomes:\np(M) = αM =\n∫\np(M|r)p(r)dr\n=\n∫\nrK(1 −r)N−Kp(r)dr\n(5)\nWith above derivations, we can formulate the\nexpected log-likelihood function of PMLM as:\nLpmlm(XΠ|X; θ)\n=\n∑\nM\n[log p(XΠ|X−Π; θ)]αM\n=\n∑\nM\nαM\nK\nK∑\nk=1\nlog p(xπk|X−Π; θ)\n(6)\nEquation 6 is optimized by sampling M accord-\ning to the prior distribution over the training set. By\ncontrolling the prior distribution, we can cover a\nwider range of sequence prediction tasks in training,\nwhich can potentially enhance the representation\npower of the pretrained model. For instance, in\nthe left-to-right autoregressive model, the masking\nratio is uniformly distributed across different posi-\ntions, which makes the model learn to generate the\nnext token given the previous context of different\nlengths. This inspires us to try the uniform prior on\nmasking ratio for PMLM.\n3.2 PMLM with a uniform prior\nu-PMLM is an implementation of PMLM with a\ncontinuous uniform distribution on the masking\nratio:\np(r) =\n{\n1,0 ≤r≤1\n0,otherwise. (7)\nLike most pretrained language models, the back-\nbone model for u-PMLM is Transformer as well.\nWe prove that u-PMLM is equivalent to the au-\ntoregressive permutated language model (APLM)\nby recombination of the factorized log-likelihood\nfunction, which is basically the autoregressive lan-\nguage model trained on all possible permutations\nof the training instances:\nLaplm(X) = Eσ\n[ N∑\nt=1\nlog p(xσt|xσ1 ,...,x σt−1 ; θ)\n]\n,\n(8)\nwhere σdenote random permutations. The detail\nderivation is included in the Appendix A.\nOrdinary autoregressive model can be regarded\nas a special case of the permutated model. There-\nfore, we can expect that the u-PMLM is able to\nwork as the autoregressive model in sequential pre-\ndiction. Moreover, since it can handle any permu-\ntation of the sequence, it should have the ability to\ngenerate sequences in arbitrary word order.\n3.3 Generation with u-PMLM\nAlgorithm 1 depicts the algorithm to autoregres-\nsively generate a sequence in random order with\nu-PMLM. The process starts with a sequence con-\ntaining full of the special token [MASK]. Then the\nmodel iteratively replaces a [MASK] token in a\nrandom position with a predicted token, until all\nthe tokens are predicted. An example showing the\nstates of the sequence during the generation pro-\ncess is presented in Table 1. The generation order\ncould be arbitrary, which is much more ﬂexible\nthan the traditional unidirectional generation. On\nthe other hand, our model can not automatically\ndetermine a best generation order, which could be\na interesting problem for future research.\nAlgorithm 1:Generation with u-PMLM\nResult: Generated Text Sequence\nS = {s1,s2,...,s N}\n. Initialization:\ni. A sequence Swith all [MASK] tokens.\nii. Unvisited index set U = {1,2,...,N }.\nwhile U is not empty do\n1. Randomly pick a number nfrom U;\n2. Input u-PMLM with Sand predict\nthe n-th token xn;\n3. Replace the n-th token of Swith the\npredicted token xn, i.e., S(n) ←xn;\n4. Remove nfrom U.\nPositional Embedding Most pretrained masked\nlanguage models have employed absolute posi-\ntional embedding to incorporate the positional in-\nformation of the input tokens. We train two variants\nfor u-PMLM, one with absolute positional embed-\nding and the other with relative positional embed-\nding (Shaw et al., 2018). The experiments show\nthat NLG ability is not sensitive to relative or ab-\nsolute positional embedding, while NLU ability is\nimproved with relative positional embeddings.\nModel Inference Although both u-PMLM and\nGPT generate sequences autoregressively based on\n267\nStep Prediction Index State of the sequence\n0 n/a\n1 3 a\n2 7 a random\n3 1 This a random\n4 2 This is a random\n5 4 This is a sentence random\n6 6 This is a sentence in random\n7 5 This is a sentence generated in random\n8 8 This is a sentence generated in random order\nGeneration Order: 3→7→1→2→4→6→5→8\nOutput: This is a sentence generated in random order\nTable 1: An example of how u-PMLM generates a sequence in random order. The special token [MASK] is\nsimpliﬁed as the symbol “ ”.\nTransformer, they are slightly different at inference\ntime. For u-PMLM, since we use the bidirectional\nTransformer, each time a token is generated, the\nhidden states of all the tokens need an update. For\nGPT, since the unidirectional Transformer is em-\nployed, the latter generated token does not affect\nthe hidden states of previous tokens. This can result\nin different computational complexity. However,\nsince a typical Graphics Processing Unit (GPU)\ncomputes matrices in parallel, the actual difference\nin inference time is not that signiﬁcant. We re-\nport the comparison of time consumption in the\nexperimental section.\n3.4 Training Settings\nModel Size : The size of our pretrained u-PMLM\nis identical to BERT-base, which contains 12 hid-\nden layers and 12 attention heads. The hidden\nsize is 768, and the intermediate size is 3072. The\ndropout rate is set to 0.1.\nTraining Data We employ the commonly\nadopted training data, namely BookCorpus and\nWikipedia to train our u-PMLM model. We obtain\n4.1 Gb for the BookCorpus dataset and 11.9 GB\nfor the Wikipedia dataset after data cleaning. We\nfurther employ the same vocabulary and tokeniza-\ntion techniques as BERT for converting the text\nsequences to ID sequences. The vocabulary con-\ntains 28,996 cased tokens. We set the maximum\nsequence length to 128.\nTraining Platform We train u-PMLM using\nHorovod framework with 56 NVIDIA V100\n(32GB) GPUs. To speed up the training process,\nwe employ mix-precision training technique. The\nbatch size is set to 150 for every single GPU, thus\nthe total batch size is 8400. The optimizer is Lamb\nOptimizer (You et al., 2019), which is more suit-\nable for large batch size than Adam Optimizer. We\ntrain u-PMLM for 600K steps, taking roughly 135\nhours in total.\n4 Experiments\nWe evaluate both the natural language generation\nability and natural language understanding ability\nof u-PMLM trained in the settings described in\nSection 3.4.\n4.1 Comparative Models\nWe train the BERT model and GPT model as the\ncomparative models in the experiments. BERT\nand GPT are representative models for masked lan-\nguage model and autoregressive language model,\nrespectively. To make fair comparisons, we train\nboth models from scratch using the same settings\ndescribed in Section 3.4, including the same train-\ning platform, model size, training data, vocabu-\nlary, and training steps. Note that since BERT\nadopts absolute positional embedding, the variant\nfor u-PMLM with absolute positional embedding is\ntrained for a fair comparison with BERT. Through-\nout the experimental section, u-PMLM-R and u-\nPMLM-A are short for the variants with relative\nand absolute positional embeddings, respectively.\n4.2 Autoregressive Generation\nPerplexity Evaluation Perplexity (PPL) mea-\nsures the quality of a language model, where the\ntask is to predict the next word or character in a\ndocument. Typically, the predicting order follows\n268\nModel PPL(sequential) PPL(random)\nBERT 23.12 25.54\nGPT 21.23 N/A\nu-PMLM-R 19.58 21.51\nu-PMLM-A 19.32 21.30\nTable 2: Perplexity on Wikitext103.\nModel PPL(sequential) PPL(random)\nBERT 140.67 56.97\nGPT 24.25 N/A\nu-PMLM-R 35.24 38.45\nu-PMLM-A 49.32 42.46\nTable 3: Perplexity on One-Billion Words.\nthe generation order. However, as bidirectional\nu-PMLM and BERT supports text generation in ar-\nbitrary order. Hence we also evaluate the perplexity\nwhen predicting words in arbitrary order.\nWe evaluate the perplexity using two datasets\nfor evaluating perplexity. The ﬁrst dataset, Wiki-\ntext103, is a collection of over 100 million tokens\nextracted from the set of veriﬁed Good and Fea-\ntured articles on Wikipedia. The second dataset,\nOne-Billion Words, consists of 829 million to-\nkens derived from a news-commentary site. Both\ndatasets are widely adopted for evaluating language\nmodels. However, there are signiﬁcant differences\nbetween these two datasets in terms of the length\nof sequences. The Wikitext103 dataset is more\nsimilar to the pretraining datasets, containing long\narticles. On the other hand, the One-Billion Words\ndataset contains only single sentences, roughly half\nof which contain less than 24 tokens. We have\nensured that all the three models have the same\ncontext length, the same vocabulary, as well as\nthe same tokenization method, which would af-\nfect the perplexity values. For Wikitext103 dataset,\nthe context length is set to 128, and each context\ncontaining multiple coherent sentences. For the\nOne-Billion Words dataset, context length is set to\n50. Short sentences are appended with [PAD] to\nreach length 50. Actually, the context for nearly\nall the sentences is shorter than 50. Both datasets\nprovide training and test sets. We ﬁrst ﬁnetune\nthe model using the training set before evaluating\nperplexity on the test set. For each model, the algo-\nrithm for the ﬁnetune phase is the same as that for\nthe pretraining phase.\nThe evaluation results of perplexity are shown\nin Table 2 and Table 3. “Sequential” refers to the\ntraditional left-to-right text generation, while for\n“random”, each sentence in the test set is assigned\na random generation order. Smaller PPL indicates\nbetter language model performance. We ﬁrst in-\nvestigate the performance on Wikitext103 dataset.\nWe observe that the PPL for u-PMLM is compa-\nrable to GPT on Wikitext103 dataset, indicating\nthat the language model learned by u-PMLM is\nas good as GPT when the context length is sufﬁ-\nciently long. In such case, the text generated by\nu-PMLM is as good as GPT. Moreover, the PPL of\nu-PMLM for randomly ordered language model is\ncomparable to the left-to-right generation, which\nimplies that u-PMLM has a strong ability for ar-\nbitrarily ordered generation. Besides, the results\nshow that there are few differences between relative\npositional embedding and absolute positional em-\nbedding for u-PMLM. On the other hand, although\nBERT supports generation in arbitrary word order\nas well, the PPL for BERT is signiﬁcantly worse\nthan our proposed u-PMLM for both “sequential”\nand “random” settings, demonstrating the effective-\nness of the proposed probabilistic masking scheme.\nWe show more cases of text generation in random\norder for u-PMLM-A and BERT in Appendix B.\nHowever, for PPL on One-Billion Words, the per-\nformances of u-PMLM and BERT are not satisfac-\ntory in comparison with GPT. Generally, PPL for\nall these models increases on One-Billion Words\ndataset as the context length becomes much smaller,\nwhich also reﬂects PPL’s relationship to context\nlength. The reason might be the large portions of\n[PAD] in the One-Billion Words dataset, i.e., more\nthan 50% of the context for nearly 50% of the train-\ning instances are ﬁlled by [PAD]. We suspect that\nthe [PAD]s affect the prediction process for bidi-\nrectional models. On the other hand, unidirectional\nmodels such as GPT naturally ignore the effect of\n[PAD] tokens in the tail of context. The results\nimply that u-PMLM could be further improved in\nthe future to be more robust.\nLatency As analyzed in Section 4, the time com-\nplexity for generation for masked language model\nis N times of autoregressive language model when\ncomputing the hidden states in each Transformer\nlayer. However, when employed for text generation\non GPU, the difference might be less signiﬁcant.\nWe test the latency for generating 100 128-length\nsentences for GPT and u-PMLM respectively. The\ncomputational platform is NVIDIA V100 GPU.\n269\nModels Cost Time\nGPT 105.6 s\nu-PMLM-A 126.8 s\nTable 4: Latency for generating 100 128-length se-\nquences.\nTom is a cat and Jerry is a mouse .“ It ’ s\nvery sad ! ” . The writers had wanted Tom\nto have “ something big to tell it . . . and\na fun place to get excited ” . The writers\nbelieved that the “ little animal ” and the “\nlittle black dog ” at the end of the episode\nwould have attracted more attention from\nviewers , but it never took place . Tom ’\ns ﬁrst television role was that of the boy\nscout “ Mr . Krabs ” in the 1978 NBC\nWestern comedy pilot , The Search for Mr\n. Krabs .\nFigure 3: Unidirectional Text Generation with GPT\nThe results are shown in Table 4. The results show\nthat u-PMLM costs roughly 20.1% more time than\nGPT for generating sentences, which is much less\nthan the theoretical time complexity difference.\nComparison With GPT for GenerationIn the\nintroduction section, we have shown an example\nshowing the application of arbitrarily ordered text\ngeneration, where the tokens in the input sentences\nare distributed across the generated sentences. In-\ndeed, the major difference with GPT is that the\ninput text could be inserted anywhere in the gener-\nated text, which makes the generation process more\ncontrollable. Meanwhile, the output text contains\ncertain predeﬁned tokens.\nFigure 3 and Figure 4 shows the generated para-\ngraphs of GPT and u-PMLM, respectively. For\nGPT, the input text can only be placed in the begin-\nning and the generation process become uncontrol-\nlable, resulting in generating sentences with topic\ndrift. In contrast, u-PMLM allows manually plac-\ning anchor sentences in the middle or end of the\ngenerated text to guide the topic of the generated\ntext. As shown in Figure 4, we place “Tom is a cat\nand Jerry is a mouse .” and “Tom and Jerry become\ngood friends in the end .” at the beginning and end\nof the paragraph. The middle parts are generated\nby u-PMLM from left-to-right. Such generation\nmethod allows us to better retain the topic of the\ngenerated content.\nTom is a cat and Jerry is a mouse .How-\never , the two have a common . The ﬁrst\npart is a joke about Jerry and Tom ﬁghting\nin the middle of the episode . The two get\non the run from the restaurant , and Tom ’\ns mother is shocked that they would have\nto do so . After a few minutes , Jerry ar-\nrives and decides to have a ﬁght . The two\ngo to the casino , where Jerry tries to ﬁght\nthem back by using a splint of grease and\na bucket of wine in the bar . They reunite\nat a restaurant dance , and Tom and Jerry\nbecome good friends in the end .\nFigure 4: Bidirectional Text Generation with u-PMLM\n4.3 Natural Language Understanding\nBesides evaluating the ability of u-PMLM for nat-\nural language generation, we also evaluate its per-\nformance on natural language understanding. Two\nwidely adopted tasks, GLUE (Wang et al., 2018)\nand SQUAD 2.0 (Rajpurkar et al., 2018), are em-\nployed for evaluating u-PMLM. We have ensured\nthat the evaluation for u-PMLM is inﬂuenced by\nas less model-irrelevant factors as possibles. For\nexample, we do not tune the hyper-parameters\nand just follow the settings of BERT, including\nwarming-up steps, learning rate, etc. In addition,\nsince BERT employs absolute positional embed-\ndings, the variant with absolute positional em-\nbeddings, u-PMLM-A, is intentionally trained for\nfairly evaluating the probabilistic masking scheme.\nThe results are shown in Table 5 and Table\n6. u-PMLM-A general performs better than\nBERT, demonstrating that the probabilistic mask-\ning scheme is more effective than the ﬁxed masking\nscheme. The reason could be that the probabilis-\ntic masking scheme covers more a wider range of\nmasking patterns, which beneﬁts pretraining for\na masked language model. Moreover, u-PMLM-\nR performs better than u-PMLM-A consistently.\nThe only difference between these two models is\nthe way to handle positional embedding. Relative\npositional embedding emphasizes more on the rel-\native positions between two tokens, which could\nbe a better option to capture contextual represen-\ntation. Recall that relative and absolute positional\nembedding do not make many differences regard-\ning generation ability if the dataset is proper. Hence\nwe conclude u-PMLM-R is a better model than u-\nPMLM-A considering both NLU and NLG tasks.\n270\nModel COLA SST2 MRPC STSB QQP MNLI-m/mm QNLI RTE A VG.\nBERT(A) 52.1 93.5 88.9/84.8 87.1/85.8 71.2/89.2 84.6/83.4 90.5 66.4 78.3\nu-PMLM-A 56.5 94.3 88.8/84.4 87.0/85.9 71.4/89.2 84.5/83.5 91.8 66.1 79.0\nu-PMLM-R 58.0 94.0 89.7/85.8 87.7/86.8 71.2/89.2 85.0/84.1 92.3 69.8 80.0\nu-PMLM-R* 56.9 94.2 90.7/87.7 89.7/89.1 72.2/89.4 86.1/85.4 92.1 78.5 81.3\nTable 5: Evaluation on GLUE test set.\nModel F1 EM\nBERT(A) 76.85 73.97\nu-PMLM-A 78.31 74.62\nu-PMLM-R 81.52 78.46\nTable 6: Evaluation on SQUAD 2.0.\nModel SQUAD 2.0 MNLI SST2\nF1/EM m/mm\nXLNet (R) 81.33/78.46 85.84/85.43 92.66\nu-PMLM-R 81.52/78.46 85.99/85.60 93.58\nTable 7: Comparison with XLNet.\nIn addition, u-PMLM-R*, ﬁnetuned with a com-\nmonly used technique by sharing data from multi-\nple tasks, is the state-of-the-art base model (with\n110M parameters) trained on the BookCorpus and\nWikipedia datasets on GLUE leaderboard on the\ndate of paper submission. 1\nComparison with XLNet We also compare our\nproposed model with XLNet-base, which adopts\nrelative positional embedding. As will be discussed\nin Section 5, XLNet is the most relevant model\nto u-PMLM. We are not able to train an XLNet\nusing the same settings except that we make sure\nboth u-PMLM-R and XLNet-base are of the same\nmodel size and are both trained using the same\ndatasets. The comparison results shown in Table 7\ndemonstrate that the performance of our proposed\nu-PMLM-R is comparable to XLNet.\n5 Related Work\n5.1 Non-traditional Text Generation\nConventionally, text is commonly generated autore-\ngressively in the left-to-right direction. Recently,\nsome research works have proposed several mod-\nels for non-autoregressive text generation (Welleck\net al., 2019; Gu et al., 2019). Stern et al. (2019)\nproposes insertion Transformer, where text are gen-\nerated in an iterative and partially autoregressive\nmanner based on insertion operations. Ma et al.\n(2019) design a latent variable based method to\ngenerate all the tokens in one pass. Ghazvinine-\n1http://gluebenchmark.com/leaderboard/\njad et al. (2019) and Wang and Cho (2019) em-\nploy masked language model for reﬁnement-based\nnon-autoregressive text generation, when a sub-\nset of tokens in a sequence are reﬁned iteratively.\nLater, Mansimov et al. (2019) propose a gen-\neralized framework of sequence generation ac-\ncommodating autoregressive, semi-autoregressive,\nand reﬁnement-based non-autoregressive model.\nStrictly speaking, our proposed arbitrarily ordered\nautoregressive text generation is a special case of\nthis generalized framework. We are the ﬁrst work\nto address such kind of text generation, which en-\nables a lot of new applications over tradition text\ngeneration.\nUNILM (Dong et al., 2019) and MASS (Song\net al., 2019) are another two works that combine\nmasked language model and autoregressive lan-\nguage model. However, UNILM only combines\nthe training objective of GPT and BERT. MASS\nemploys mask mechanism to train sequence to se-\nquence language model. Both models do not ad-\ndress arbitrarily ordered text generation.\n5.2 XLNet\nXLNet (Yang et al., 2019) is the most relevant pre-\ntrained language model to u-PMLM. Both of them\ncan be treated as an autoregressive permutated lan-\nguage model. However, XLNet is trained by per-\nmutating only a small fraction of the sequences,\nwhich does not fully address the generation prob-\nlem. Though, we suppose that the training method\nfor XLNet is feasible to train a model for arbitrarily\nordered text generation as well. The main differ-\nence between these two models is that XLNet em-\nploys unidirectional Transformer, while u-PMLM\nis based on bidirectional Transformer. Regarding\nthe training algorithm, XLNet shufﬂes the atten-\ntion matrix and introduce two-stream self-attention,\nwhich is a bit complex and memory consuming. On\nthe other hand, PMLM takes the simple training\nobjective of masked language model and approxi-\nmates permutated language model.\n271\n6 Conclusion\nWe have proposed a probabilistically masked lan-\nguage model for autoregressive generation in arbi-\ntrary word order. The experiments show that the\ntext generated in arbitrary order has comparable\nquality with GPT. Besides, the proposed proba-\nbilistic masking scheme also improves the NLU\ncapability of a masked language model.\nReferences\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4171–4186.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems 32, pages 13042–13054.\nMathieu Germain, Karol Gregor, Iain Murray, and\nHugo Larochelle. 2015. Made: Masked autoencoder\nfor distribution estimation. In Proceedings of the\n32nd International Conference on Machine Learn-\ning, pages 881–889.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, pages 6114–6123.\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019.\nLevenshtein transformer. In Advances in Neural In-\nformation Processing Systems, pages 11179–11189.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling bert for natural language\nunderstanding. arXiv preprint arXiv:1909.10351.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2019. Spanbert:\nImproving pre-training by representing and predict-\ning spans. arXiv preprint arXiv:1907.10529.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nXuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-\nbig, and Eduard Hovy. 2019. FlowSeq: Non-\nautoregressive conditional sequence generation with\ngenerative ﬂow. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing, pages 4281–\n4291.\nElman Mansimov, Alex Wang, and Kyunghyun Cho.\n2019. A generalized framework of sequence genera-\ntion with application to undirected sequence models.\narXiv preprint arXiv:1905.12790.\nAlec Radford, Karthik Narasimhan, Tim Sali-\nmans, and Ilya Sutskever. 2018. Improving\nlanguage understanding by generative pre-training.\nURL https://s3-us-west-2.amazonaws.com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. arXiv preprint arXiv:1806.03822.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 464–468.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: Masked sequence to se-\nquence pre-training for language generation. In Pro-\nceedings of the 36th International Conference on\nMachine Learning, pages 5926–5936.\nMitchell Stern, William Chan, Jamie Kiros, and Jakob\nUszkoreit. 2019. Insertion transformer: Flexible se-\nquence generation via insertion operations. In Pro-\nceedings of the 36th International Conference on\nMachine Learning, pages 5976–5985.\n272\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a\nmouth, and it must speak: BERT as a Markov ran-\ndom ﬁeld language model. In Proceedings of the\nWorkshop on Methods for Optimizing and Evaluat-\ning Neural Language Generation, pages 30–36.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao,\nLiwei Peng, and Luo Si. 2019. Structbert: In-\ncorporating language structures into pre-training\nfor deep language understanding. arXiv preprint\narXiv:1908.04577.\nSean Welleck, Kiant ´e Brantley, Hal Daum ´e III, and\nKyunghyun Cho. 2019. Non-monotonic sequential\ntext generation. In Proceedings of the 36th Interna-\ntional Conference on Machine Learning.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems 32, pages 5754–5764.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2019. Large batch optimization for deep learning:\nTraining bert in 76 minutes. In International Con-\nference on Learning Representations.\nA Proof of Equivalence\nWe prove that PMLM with a continuous uniform\ndistribution on the masking ratio, namely u-PMLM,\nis equivalent to an autoregressive permutated lan-\nguage model.\nWhen p(r) is a continuous uniform distribution,\nthe probability p(M) is analytical, denoted as:\np(M) =\n∫\nrK(1 −r)N−Kp(r)d(r)\n=\n∫ 1\n0\nrK(1 −r)N−Kd(r)\n= B(N −K+ 1,K + 1)\n= Γ(N −K+ 1)Γ(K+ 1)\nΓ(N + 2)\n= (N −K)!K!\n(N + 1)!\n(9)\nwhere B(·) is Beta function and Γ(·) is Gamma\nfunction. Thus for u-PMLM, the expected loss-\nlikelihood function denoted in Equation 6 becomes:\nLpmlm(XΠ|X; θ)\n=\n∑\nM\n[ 1\nK\nK∑\nk=1\nlog p(xπk|X−Π; θ)](N −K)!K!\n(N + 1)!\n=\n∑\nM\n∑K\nk=1(N −K)!(K−1)! logp(xπk|X−Π; θ)\n(N + 1)!\n(10)\nOn the other hand, we rewrite the formulation\nof an autoregressive permutated language model\n(APLM) denoted in Equation 8 as:\nLaplm(X) = Eσ\n[ N∑\nt=1\nlog p(xσt|xσ1 ,...,x σt−1 ; θ)\n]\n=\n∑\nσ[∑N\nt=1 log p(xσt|xσ1 ,...,x σt−1 ; θ)]\nC\n(11)\nwhere the numerator sums over the log-likelihood\nfor all the possible permutations and the denomi-\nnator C is a constant. In fact, we can rewrite the\nterm p(xσt|xσ1,...,x σt−1; θ) by p(xσt|X−Πσ\nt ; θ),\nwhere Πσ\nt = X−{σ1,σ2,...,σ t−1}. Noted that K\nis the size of Πσ\nt. Thus the size of Πσ\nt is denoted\nas |Πσ\nt|= K = N −t+ 1. Therefore we rewrite\nEquation 11 as:\nLaplm(X)\n= 1\nC\n∑\nσ\n[log p(XΠσ\nt+1 |X−Πσ\nt+1 ; θ) + logp(xσt|X−Πσ\nt ; θ)\n+ logp(X−Πσ\nt ; θ)]\n(12)\nAccording to the above equation, we can de-\nrive the duplication factor for speciﬁc term\nlog p(xσt|X−Πσ\nt ) when summing over all the per-\nmutations, which is exactly the product of numbers\nof permutations for Πσ\nt+1 and −Πσ\nt in the ﬁrst and\nlast term respectively. Speciﬁcally, the number of\n273\npermutations for Πσ\nt+1 and −Πσ\nt are factorials of\n|Πσ\nt+1|and |−Πσ\nt|, denoted as:\npermutation(Πσ\nt+1) = |Πσ\nt+1|! = (N −K)!\npermutation(−Πσ\nt) = |−Πσ\nt|! = (K−1)! (13)\nHence the duplication factor for log p(xσt|X−Πσ\nt )\nis computed as (N −K)!(K −1)!, which is the\ncoefﬁcient of the expected log-likelihood function\nof u-PMLM denoted in Equation 10. Thus we\nconclude that Equation 10 is equivalent to Equation\n8, where the constant C = (N + 1)!.\nB Generation Examples of u-PMLM and\nBERT\nWe show more examples of the text generated by\nu-PMLM-A and BERT respectively. Note that we\ndo not manually select examples. Instead, these\nexamples are picked randomly for fair comparison.\nBelow are texts generated by u-PMLM-A:\n1. Tom is a cat and Jerry is a mouse .Though\ninitially wary that he might steal the pie , Jerry\neventually admits his fears . Tom ’ s father ,\nSelma , is upset when the cat is not present .\nShe tries making a play about the pie but the\ngirl ignores her and insists that she will be\nglad if he ’ s able to steal the pie . Tom and\nJerry become good friends in the end .\n2. Tom is a cat and Jerry is a mouse .“ Jerry\nand Tom meet again in a restaurant and in\na confrontation Tom tries to call the house\npolice . Jerry ’ s neighbor Tom Martin ( Tom\nLee ) ﬁnds them ﬂirting by taking Jerry , but\nTom ﬁnds out . With Jerry ’ s help Tom gets\ntheir hand together in a car wash and the two\nhave a date . Tom and Jerry become good\nfriends in the end .\n3. Tom is a cat and Jerry is a mouse .When\nthe kids get their ﬁrst game of cricket at his\nhome , he has him as catcher . When all of the\nteams are on the ﬁeld except the school cricket\nplayer , he takes part in every game , which is\nalways the cricket . He later breaks his leg on\nthe last day of cricket and falls down a ladder\n. Tom and Jerry become good friends in the\nend .\n4. Tom is a cat and Jerry is a mouse .The two\nstart off on a ﬁshing trip together to see what\ntheir parents would think of them . The boys\nﬁrst discover a bad ﬁsh on the ocean and de-\ncide to try the game . A boathouse is built in\ntown where they see a big picture of Tom ’ s\ngrandfather . The boys ﬁght it out by chance\n, and Tom and Jerry become good friends in\nthe end .\n5. Tom is a cat and Jerry is a mouse .This\nmakes Tom very hard to understand why he\nand the other citizens like him , so he tries to\nhave to get rid of the mouse . His best friend\n, Pete , tries to keep his dog quiet about this ,\neven while Jerry and Marge insist they are just\nstarting on a date due to Tom ’ s dog . Tom\nand Jerry become good friends in the end .\nBelow are texts generated by BERT:\n1. Tom is a cat and Jerry is a mouse .“ That\nsame month the two characters were por-\ntrayed as their characters at the NBC se-\nries The Ofﬁce , with the character played\nby Critrean Figgins and the mouse played by\nthe cat . The two characters make a romantic\nconnection and meet in the end of the episode\n” When We Were Meeted Together , ” seeing\nwhy Tom and Jerry become good friends in\nthe end .\n2. Tom is a cat and Jerry is a mouse .. . And\nit ’ s about when Tom is in trouble and he\nbecomes frustrated when everything gets up-\nsetting . I make my own movie . . . But I do\nnot want to be a ﬁlm so sad or about what\nI like about a kid ’ s life . I want to make a\nmovie that is more real and Tom and Jerry\nbecome good friends in the end .\n3. Tom is a cat and Jerry is a mouse .“ On\nAugust 7 , 1999 , DeLanto started a Twitter\naccount , which included an online forum ad-\ndress on NBC ’ s show 30 Rock through his\naccount of his life on stage and on the Internet\n. During 2008 , he also posted on his personal\nblog a message saying ” This world ’ s really\ngetting bigger . Tom and Jerry become good\nfriends in the end .\n4. Tom is a cat and Jerry is a mouse. He is a\ncat and Tom is a mouse . At McKinley High\nSchool , Tom enters the Class 3A , and then\nis elected President of High School ( H . F\n. R . ) , the district ’ s popular high school .\nHe becomes the principal and a student ’ s\n274\nsupervisor at the High School in 2004 . Tom\nand Jerry become good friends in the end .\n5. Tom is a cat and Jerry is a mouse .In April\n1997 , Jack was murdered and he and Jack\nwent on a similar out of wedlock . Tom even-\ntually had a teenage son named Tim . In the\npilot episode , Tom is shot in a car crash , and\neventually re @ - @ takes his life after an-\nother accident , giving him a more ” normal\n” appearance . Tom and Jerry become good\nfriends in the end .",
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.8392550945281982
    },
    {
      "name": "Computer science",
      "score": 0.8145376443862915
    },
    {
      "name": "Natural language generation",
      "score": 0.7559467554092407
    },
    {
      "name": "Autoregressive model",
      "score": 0.7498350143432617
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5552720427513123
    },
    {
      "name": "Probabilistic logic",
      "score": 0.5291500687599182
    },
    {
      "name": "Word (group theory)",
      "score": 0.5210644602775574
    },
    {
      "name": "Natural language understanding",
      "score": 0.5066736340522766
    },
    {
      "name": "Natural language",
      "score": 0.4837058186531067
    },
    {
      "name": "Natural language processing",
      "score": 0.46646249294281006
    },
    {
      "name": "Speech recognition",
      "score": 0.3726329505443573
    },
    {
      "name": "Linguistics",
      "score": 0.12799251079559326
    },
    {
      "name": "Statistics",
      "score": 0.09863463044166565
    },
    {
      "name": "Mathematics",
      "score": 0.08861804008483887
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}