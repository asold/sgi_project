{
  "title": "An Analysis of Neural Language Modeling at Multiple Scales",
  "url": "https://openalex.org/W2792376130",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4301100015",
      "name": "Merity, Stephen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281371573",
      "name": "Keskar, Nitish Shirish",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176399732",
      "name": "Socher, Richard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2150355110",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W2409027918",
    "https://openalex.org/W2964269252",
    "https://openalex.org/W2963938518",
    "https://openalex.org/W2742448943",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2057653135",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W330298975",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963983719",
    "https://openalex.org/W2510842514",
    "https://openalex.org/W1916559533",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2546325545",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2963034893"
  ],
  "abstract": "Many of the leading approaches in language modeling introduce novel, complex and specialized architectures. We take existing state-of-the-art word level language models based on LSTMs and QRNNs and extend them to both larger vocabularies as well as character-level granularity. When properly tuned, LSTMs and QRNNs achieve state-of-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103) datasets, respectively. Results are obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single modern GPU.",
  "full_text": "An Analysis of Neural Language Modeling at Multiple Scales\nStephen Merity 1 * Nitish Shirish Keskar 1 * Richard Socher 1\nAbstract\nMany of the leading approaches in language mod-\neling introduce novel, complex and specialized\narchitectures. We take existing state-of-the-art\nword level language models based on LSTMs and\nQRNNs and extend them to both larger vocabular-\nies as well as character-level granularity. When\nproperly tuned, LSTMs and QRNNs achieve state-\nof-the-art results on character-level (Penn Tree-\nbank, enwik8) and word-level (WikiText-103)\ndatasets, respectively. Results are obtained in only\n12 hours (WikiText-103) to 2 days (enwik8) us-\ning a single modern GPU.\n1. Introduction\nLanguage modeling (LM) is one of the foundational tasks\nof natural language processing. The task involves predicting\nthe (n + 1)th token in a sequence given the n preceding to-\nkens. Trained LMs are useful in many applications including\nspeech recognition (Yu & Deng, 2014), machine translation\n(Koehn, 2009), natural language generation (Radford et al.,\n2017; Merity et al., 2017a), learning token embeddings, and\nas a general-purpose feature extractor for downstream tasks\n(P. Matthew, 2017).\nLanguage models can operate at various granularities, with\nthese tokens formed from either words, sub-words, or char-\nacters. While the underlying objective remains the same\nacross all sub-tasks, each has its own unique set of beneﬁts\nand challenges.\nIn practice, word-level LMs appear to perform better at\ndownstream tasks compared to character-level LMs but suf-\nfer from increased computational cost due to large vocabu-\nlary sizes. Even with a large vocabulary, word-level LMs\nstill need to replace infrequent words with out-of-vocabulary\n(OoV) tokens. Character-level LMs do not suffer this OoV\nproblem as the potentially inﬁnite set of potential words\n*Equal contribution 1Salesforce Research, Palo Alto, CA\n– 94301. Correspondence to: Stephen Merity <smer-\nity@salesforce.com>.\nCode available at https://github.com/salesforce/\nawd-lstm-lm\ncan be constructed by repeatedly selecting from a limited\nset of characters. This introduces two issues for character-\nlevel LMs however - they are slower to process than their\nword-level counterparts as the number of tokens increases\nsubstantially, and due to this, they experience more extreme\nissues with vanishing gradients. Tokens are also far less\ninformative in a character-level LM. For a word-level LM\nto understand the likely topics a sentence may cover, one or\ntwo discriminative words may be sufﬁcient. In comparison,\na character-level LM would require at least half a dozen.\nGiven this distinction, word- and character-level LMs are\ncommonly trained with vastly different architectures. While\nvanilla Long Short Term Memory (LSTM) networks have\nbeen shown to achieve state-of-the-art performance on word-\nlevel LM they have been hitherto considered insufﬁcient for\ncompetitive character-level LM; see e.g., (Melis et al., 2018)\nIn this paper, we show that a given baseline model frame-\nwork, composed of a vanilla RNN (LSTM or its cheaper\ncounterpart, QRNN (Bradbury et al., 2017)) and an adap-\ntive softmax, is capable of modeling both character- and\nword-level tasks at multiple scales of data whilst achiev-\ning state-of-the-art results. Further, we present additional\nanalysis regarding a comparison of the LSTM and QRNN\narchitectures and the importance of various hyperparame-\nters in the model. We conclude the paper with a discussion\nconcerning the choice of datasets and model metrics.\n2. Motivation\nRecent research has shown that a well tuned LSTM baseline\ncan outperform more complex architectures in the task of\nword-level language modeling (Merity et al., 2018; Melis\net al., 2018). The model in Merity et al. (2018) also aims\nto use well optimized components, such as the NVIDIA\ncuDNN LSTM or highly parallel Quasi-Recurrent Neural\nNetwork (Bradbury et al., 2017), allowing for rapid conver-\ngence and experimentation due to efﬁcient hardware usage.\nWhile the models were shown to achieve state-of-the-art\nresults on modest language modeling datasets, their ap-\nplication to larger-scale word-level language modeling or\ncharacter-level language modeling had not been successful.\nLarge-scale word- and character-level datasets can both\nrequire training over hundreds of millions of tokens. This\nrequires an efﬁcient model such that experimentation does\narXiv:1803.08240v1  [cs.CL]  22 Mar 2018\nAn Analysis of Neural Language Modeling at Multiple Scales\nnot require vast amounts of time or resources. From this,\nwe aim to ensure our model can train in a matter of days\nor hours on a single modern GPU and can achieve results\ncompetitive with current state-of-the-art results.\n3. Model architecture\nOur underlying architecture is based upon the model used\nin Merity et al. (2018). Their model consists of a trainable\nembedding layer, one or more layers of a stacked recur-\nrent neural network, and a softmax classiﬁer. The embed-\nding and softmax classiﬁer utilize tied weights (Inan et al.,\n2016; Press & Wolf, 2016) to both decrease the total pa-\nrameter count as well as improve classiﬁcation accuracy\nover rare words. Their experimental setup features various\noptimization and regularization variants such as randomized-\nlength backpropagation through time (BPTT), embedding\ndropout, variational dropout, activation regularization (AR),\nand temporal activation regularization (TAR). This model\nframework has been shown to achieve state-of-the-art results\nusing either an LSTM or QRNN based model in under a day\non an NVIDIA Quadro GP100.\n3.1. LSTM and QRNN\nIn Merity et al. (2018), two different recurrent neural net-\nwork cells are evaluated: the Long Short Term Memory\n(LSTM) (Hochreiter & Schmidhuber, 1997) and the Quasi-\nRecurrent Neural Network (QRNN) (Bradbury et al., 2017).\nWhile the LSTM works well in terms of task performance,\nit is not an optimal ﬁt for ensuring high GPU utilization.\nThe LSTM is sequential in nature, relying on the output of\nthe previous timestep before work can begin on the current\ntimestep. This limits the concurrency of the LSTM to the\nsize of the batch and even then can result in substantial\nCUDA kernel overhead if each timestep must be processed\nindividually.\nThe QRNN attempts to improve GPU utilization for recur-\nrent neural networks in two ways: it uses convolutional lay-\ners for processing the input, which apply in parallel across\ntimesteps, and then uses a minimalist recurrent pooling\nfunction that applies in parallel across channels. As the\nconvolutional layer does not rely on the output of the pre-\nvious timestep, all input processing can be batched into a\nsingle matrix multiplication. While the recurrent pooling\nfunction is sequential, it is a fast element-wise operation\nthat is applied to existing prepared inputs, thus producing\nnext to no overhead. In our investigations, the overhead of\napplying dropout to the inputs was more signiﬁcant than the\nrecurrent pooling function.\nThe QRNN can be up to 16 times faster than the optimized\nNVIDIA cuDNN LSTM for timings only over the RNN\nitself (Bradbury et al., 2017). When two networks are\ncomposed of approximately equal size with the LSTM and\nQRNN, (Merity et al., 2018) found that QRNN based mod-\nels were overall 2 − 4× faster per epoch. For word-level\nLMs, QRNNs were also found to require fewer epochs to\nconverge and achieved comparable state-of-the-art results\non the word-level Penn Treebank and WikiText-2 datasets.\nGiven the sizes of the datasets we intend to process can\nbe many millions of tokens in length, the potential speed\nbeneﬁt of the QRNN is of interest, especially if the results\nremain competitive to that of the LSTM.\n3.2. Longer BPTT lengths\nTruncated backpropagation through time (BPTT) (Williams\n& Peng, 1990; Werbos, 1990) is necessary for long form\ncontinuous datasets. Traditional LMs use relatively small\nBPTT windows of 50 or less for word-level LM and 100 or\nless for character-level. For both granularities, however, the\ngradient approximation made by truncated BPTT is highly\nproblematic. A single sentence may well be longer than the\ntruncated BPTT window used in character- or even word-\nlevel modeling, preventing useful long term dependencies\nfrom being discovered. This is exacerbated if the long term\ndependencies are split across paragraphs or pages. The\nlonger the sequence length used during truncated BPTT,\nthe further back long term dependencies can be explicitly\nformed, potentially beneﬁting the model’s accuracy.\nIn addition to a potential accuracy beneﬁt, longer BPTT\nwindows can improve GPU utilization. In most models,\nthe primary manner to improve GPU utilization is through\nincreasing the number of examples per batch, shown to be\nhighly effective for tasks such as image classiﬁcation (Goyal\net al., 2017). Due to the more parallel nature of the QRNN,\nthe QRNN can process long sequences entirely in parallel,\nin comparison to the more sequential LSTM. This is as the\nQRNN does not feature a slow sequential hidden-to-hidden\nmatrix multiplication at each timestep, instead relying on a\nfast element-wise operator.\n3.3. Tied adaptive softmax\nLarge vocabulary sizes are a major issue on large-scale\nword-level datasets and can result in impractically slow\nmodels (softmax overhead) or models that are impossible to\ntrain due to a lack of GPU memory (parameter overhead).\nTo address these issues, we use a modiﬁed version of the\nadaptive softmax (Grave et al., 2016b), extended to allow\nfor tied weights (Inan et al., 2016; Press & Wolf, 2016).\nWhile other softmax approximation strategies exist (Morin\n& Bengio, 2005; Gutmann & Hyv¨arinen, 2010), the adaptive\nsoftmax has been shown to achieve results close to that of\nthe full softmax whilst maintaining high GPU efﬁciency.\nThe adaptive softmax uses a hierarchy determined by word\nAn Analysis of Neural Language Modeling at Multiple Scales\nfrequency to reduce computation time. Words are split into\ntwo levels: the ﬁrst level (the short-list) and the second level,\nwhich forms clusters of rare words. Each cluster has a repre-\nsentative token in the short-list which determines the overall\nprobability assigned across all words within that cluster.\nDue to Zipf’s law, most words will only require a softmax\nover the short-list during training, reducing computation\nand memory usage. Clusters on the second level are also\nconstructed such that the matrix multiplications required for\na standard batch are near optimal for GPU efﬁciency.\nIn the original adaptive softmax implementation, words\nwithin clusters on the second level feature a reduced em-\nbedding size, reducing from embedding dimensionality d\nto d\n4 . This minimizes both total parameter count and the\nsize of the softmax matrix multiplications. The paper justi-\nﬁes this by noting that rare words are unlikely to need full\nembedding size ﬁdelity due to how infrequently they occur.\nWeight tying cannot be used with this memory optimization\nhowever. As weight tying re-uses the word vectors from\nthe embedding as the targets in the softmax, the embedding\ndimensionality must be equal for all words. We discard\nthe adaptive softmax memory optimization in order to uti-\nlize tied weights. Counter-intuitively, weight tying actually\nreduces the memory usage even more than the adaptive soft-\nmax memory optimization, as weight tying allows us to\nhalve the memory usage by re-using the word vectors from\nthe embedding layer.\nWhen a dataset is processed using a small vocabulary, all\nthe words can be placed into the short-list, thus reducing the\nadaptive softmax to a standard softmax.\n4. Experiments\nOur work is over three datasets, two character-level datasets\n(Penn Treebank, enwik8) and a large-scale word-level\n(WikiText-103) dataset. A set of overview statistics for\nthese datasets are presented in Table 1.\n4.1. Penn Treebank\nIn Mikolov et al. (2010), the Penn Treebank dataset (Marcus\net al., 1994) was processed to generate a character-level\nlanguage modeling dataset. The dataset exists in both a\nword- and character-level form.\nWhile the dataset was originally composed of Wall Street\nJournal articles, the preprocessed dataset removes many fea-\ntures considered important for capturing language modeling.\nOddly, the vocabulary of the words in the character-level\ndataset is limited to 10,000 - the same vocabulary as used\nin the word level dataset. This vastly simpliﬁes the task of\ncharacter-level language modeling as character transitions\nwill be limited to those found within the limited word level\nvocabulary.\nIn addition to the limited vocabulary, the character-level\ndataset is all lower case, has all punctuation removed (other\nthan as part of certain words such as u.s. or mr.), and re-\nplaces all numbers with N. All of these would be considered\nimportant subtasks for a character-level language model to\ncover.\nWhen rare words are encountered in both the character and\nword level datasets, they’re replaced with the token<unk>.\nThis makes little sense for the character-level dataset, which\ndoesn’t suffer from out of vocabulary issues as the word\nlevel dataset would. As the <unk> token is the only token\nto use the < and > characters, a sufﬁciently advanced model\nshould always output unk> upon seeing < as this is the only\ntime angle brackets are used.\nWe later explore these issues and their impact by comparing\nmodels trained on character-level Penn Treebank with a\nmore realistic character-level dataset, enwik8.\nWe report our results, using the bits per character (BPC)\nmetric, in Table 2. The model uses a three-layered LSTM\nwith a BPTT length of 150, embedding sizes of 200 and\nhidden layers of size 1000. We regularize the model using\nLSTM dropout, weight dropout (Merity et al., 2018), and\nweight decay. These values are tuned coarsely. For full\nhyper-parameter values, refer to Table 6. We train the model\nusing the Adam (Kingma & Ba, 2014) optimizer with a\nlearning rate of 2.7 × 10−3 for 500 epochs and reduce the\nlearning rate by 10 on epochs 300 and 400. While the\nmodel was not optimized for convergence speed, it takes\nonly 8 hours to train on an NVIDIA V olta GPU. Both our\nLSTM and QRNN model beat the current state-of-the-art\nthough using more parameters. Of note is that the QRNN\nmodel uses 6 layers to achieve the same result as the LSTM\nwith only 3 layers, suggesting that the limited recurrent\ncomputation capacity of the QRNN may be an issue.\n4.2. Hutter Wikipedia Prize (enwik8)\nThe Hutter Prize Wikipedia dataset (Hutter, 2018), also\nknown as enwik8, is a byte-level dataset consisting of the\nﬁrst 100 million bytes of a Wikipedia XML dump. For\nsimplicity we shall refer to it as a character-level dataset.\nWithin these 100 million bytes are 205 unique tokens. The\nHutter Prize was launched in 2006 and is focused around\ncompressing the enwik8 dataset as efﬁciently as possible.\nThe XML dump contains a wide array of content, including\nEnglish articles, XML data, hyperlinks, and special charac-\nters. As the data has not been processed, it features many\nof the intricacies of language modeling, both natural and\nartiﬁcial, that we would like our models to capture. For our\nexperiments, we follow a standard setup where the train,\nvalidation and test sets consist of the ﬁrst 90M, 5M, and 5M\nAn Analysis of Neural Language Modeling at Multiple Scales\nPenn Treebank (Character) enwik8 WikiText-103\nTrain Valid Test Train Valid Test Train Valid Test\nTokens 5.01M 393k 442k 90M 5M 5M 103.2M 217k 245k\nV ocab size 51 205 267,735\nOoV rate - - 0.4%\nTable 1.Statistics of the character-level Penn Treebank, character-level enwik8 dataset, and WikiText-103. The out of vocabulary (OoV)\nrate notes what percentage of tokens have been replaced by an ⟨unk⟩ token, not applicable to character-level datasets.\nModel BPC Params\nZoneout LSTM (Krueger et al., 2016) 1.27 -\n2-Layers LSTM (Mujika et al., 2017) 1.243 6.6M\nHM-LSTM (Chung et al., 2016) 1.24 -\nHyperLSTM - small (Ha et al., 2016) 1.233 5.1M\nHyperLSTM (Ha et al., 2016) 1.219 14.4M\nNASCell (small) (Zoph & Le, 2016) 1.228 6.6M\nNASCell (Zoph & Le, 2016) 1.214 16.3M\nFS-LSTM-2 (Mujika et al., 2017) 1.190 7.2M\nFS-LSTM-4 (Mujika et al., 2017) 1.193 6.5M\n6 layer QRNN (h = 1024) (ours) 1.187 13.8M\n3 layer LSTM (h = 1000) (ours) 1.175 13.8M\nTable 2.Bits Per Character (BPC) on character-level Penn Tree-\nbank.\nModel BPC Params\nLSTM, 2000 units (Mujika et al., 2017) 1.461 18M\nLayer Norm LSTM, 1800 units 1.402 14M\nHyperLSTM (Ha et al., 2016) 1.340 27M\nHM-LSTM (Chung et al., 2016) 1.32 35M\nSD Zoneout (Rocki et al., 2016) 1.31 64M\nRHN - depth 5 (Zilly et al., 2016) 1.31 23M\nRHN - depth 10 (Zilly et al., 2016) 1.30 21M\nLarge RHN (Zilly et al., 2016) 1.27 46M\nFS-LSTM-2 (Mujika et al., 2017) 1.290 27M\nFS-LSTM-4 (Mujika et al., 2017) 1.277 27M\nLarge FS-LSTM-4 (Mujika et al., 2017) 1.245 47M\n4 layer QRNN (h = 1800) (ours) 1.336 26M\n3 layer LSTM (h = 1840) (ours) 1.232 47M\ncmix v13 (Knoll, 2018) 1.225 -\nTable 3.Bits Per Character (BPC) on enwik8.\ncharacters, respectively.\nWe report our results in Table 3. The model uses a three-\nlayered LSTM with a BPTT length of 200, embedding sizes\nof 400 and hidden layers of size 1850. The only explicit\nregularizer we employ is the weight dropped LSTM (Merity\net al., 2018) of magnitude 0.2. For full hyper-parameter\nvalues, refer to Table 6. We train the model using the Adam\n(Kingma & Ba, 2014) optimizer with default hyperparam-\neter for 50 epochs and reduce the learning rate by 10 on\nepochs 25 and 35.\nThis dataset is far more challenging than character-level\nPenn Treebank for multiple reasons. The dataset has 18\ntimes more training tokens and the data has not been pro-\ncessed at all, maintaining far more complex character to\ncharacter transitions than that of the character-level Penn\nTreebank. Potentially as a result of this, the QRNN based\nmodel underperforms models with comparable numbers of\nparameters. The limited recurrent computation capacity of\nthe QRNN appears to become a major issue when moving\ntoward realistic character-level datasets.\n4.3. WikiText\nThe WikiText-2 (WT2) and WikiText-103 (WT103) datasets\nintroduced in Merity et al. (2017b) contain lightly prepro-\ncessed Wikipedia articles, retaining the majority of punctua-\ntion and numbers. The WT2 data set contains approximately\n2 million words in the training set and 0.2 million in vali-\ndation and test sets. The WT103 data set contains a larger\ntraining set of 103 million words and the same validation\nand testing set as WT2. As the Wikipedia articles are rel-\natively long and are focused on a single topic, capturing\nand utilizing long term dependencies can be key to models\nobtaining strong performance.\nThe underlying model used in this paper achieved state-of-\nthe-art on language modeling for the word-level PTB and\nWikiText-2 datasets (Merity et al., 2018) We show that, in\nconjunction with the tied adaptive softmax, we achieve state-\nof-the-art perplexity on the WikiText-103 data set using\nthe AWD-QRNN framework; see Table 4. We opt to use\nthe QRNN as the LSTM is 3 times slower, as reported in\nTable 5, and a QRNN based model’s performance on word-\nAn Analysis of Neural Language Modeling at Multiple Scales\nModel Val Test\nGrave et al. (2016a) – 48.7\nDauphin et al. (2016), 1 GPU – 44.9\nDauphin et al. (2016), 4 GPUs – 37.2\n4 layer QRNN (h = 2500), 1 GPU 32.0 33 .0\nTable 4.Perplexity on the word-level WikiText-103 dataset. The\nmodel was trained for 12 hours (14 epochs) on an NVIDIA V olta.\nModel Time per batch\nLSTM 726ms\nQRNN 233ms\nTable 5.Mini-batch timings during training on WikiText-103. This\nis a 3.1 times speed-up over the NVIDIA cuDNN LSTM baseline\nwhich uses the same model hyperparameters.\nlevel datasets has been found to equal that of an LSTM\nbased model Merity et al. (2018).\nBy utilizing the QRNN and tied adaptive softmax, we were\nable to train to a state-of-the-art result with a NVIDIA V olta\nGPU in 12 hours. The model used consisted of a 4-layered\nQRNN model with an embedding size of 400 and 2500\nnodes in each hidden layer. We trained the model using a\nbatch size of 60 and a BPTT length of 140 using the Adam\noptimizer (Kingma & Ba, 2014) for 14 epochs, reducing\nthe learning rate by 10 on epoch 12. To avoid over-ﬁtting,\nwe employ the regularization strategies proposed in (Merity\net al., 2018) including variational dropout, random sequence\nlengths, and L2-norm decay. The values for the model\nhyperparameters were tuned only coarsely; for full hyper-\nparameter values, refer to Table 6.\n5. Analysis\n5.1. QRNN vs LSTM\nQRNNs and LSTMs operate on sequential data in vastly\ndifferent ways. For word-level language modeling, QRNNs\nhave allowed for similar training and generalization out-\ncomes at a fraction of the LSTM’s cost (Merity et al., 2018).\nIn our work however, we have found QRNNs less successful\nat character-level tasks, even with substantial hyperparame-\nter tuning.\nTo investigate this, Figure 1 shows a comparison between\nboth word- and character-level tasks as well as between the\nLSTM and the QRNN. In this experiment, we plot the proba-\nbility assigned to the correct token as a function of the token\nposition with the model conditioned on the ground-truth\nlabels up to the token position. We attempt to ﬁnd simi-\nlar situations in the word- and character-level datasets to\nsee how the LSTM and QRNN models respond differently.\nFor character-level datasets, model confusion is highest at\nthe beginning of a word, as at that stage there is little to\nno information about it. This confusion decreases as more\ncharacters from the word are seen. The closest analogy to\nthis on word-level datasets may be just after the start of\na sentence. As a proxy for ﬁnding sentences, we ﬁnd the\ntoken The and record the model confusion after that point.\nSimilarly to the character-level dataset, we assumed that\nconfusion would be highest at this early stage before any\ninformation has been gathered, and decreased as more to-\nkens are seen. Surprisingly, we can clearly see the behavior\nof the datasets is quite different. Word-level datasets do\nnot gain the same clarity after seeing additional information\ncompared to the character-level datasets. We also see the\nQRNN underperforming the LSTM on both the Penn Tree-\nbank and enwik8 character-level datasets. This is not the\ncase for the word-level datasets.\nOur hypothesis is that character-level language modeling\nrequires a more complex hidden-to-hidden transition. As the\nLSTM has a full matrix multiplication between timesteps,\nit is able to more quickly adapt to changing situations. The\nQRNN on the other hand suffers from a simpler hidden-to-\nhidden transition function that is only element-wise, prevent-\ning full communication between hidden units in the RNN.\nThis might also explain why QRNNs need to be deeper than\nthe LSTM to achieve comparable results, such as the 6 layer\nQRNN in Table 2, as the QRNN can perform more complex\ninteractions of the hidden state by sending it to the next\nQRNN layer. This may suggest why state-of-the-art archi-\ntectures for character- and word-level language modeling\ncan be quite different and not as readily transferable.\n5.2. Hyperparameter importance\nGiven the large number of hyperparameters in most neu-\nral language models, the process of tuning models for new\ndatasets can be laborious and expensive. In this section,\nwe attempt to determine the relative importance of the var-\nious hyperparameters. To do this, we train 200 models\nwith random hyperparameters and then regress them against\nthe validation perplexity using MSE-based Random Forests\n(Breiman, 2001). We then use the Random Forest’s feature\nimportance as a proxy for the hyperparameter importance.\nWhile the hyperparameters are random, we place natural\nbounds on them to prevent unrealistic models given the\nAWD-QRNN framework and its recommended hyperparam-\neters. For this purpose, all dropout values are bounded to\n[0, 1], the truncated BPTT length is bounded between 30\nand 300, number of layers are bounded between 1 and 10\nand the embedding and hidden sizes are bounded between\n100 and 500. We present the results for the word-level task\non the WikiText-2 data set using the AWD-QRNN model\nin Figure 3. The models were trained for 300 epochs with\nAn Analysis of Neural Language Modeling at Multiple Scales\nCharacter PTB enwik8 WikiText-103\nRNN Cell LSTM LSTM QRNN\nLayers 3 3 4\nRNN hidden size 1000 1840 2500\nAR/TAR 0/0 0/0 0/0\nDropout (e/h/i/o) 0/0.25/0.1/0.1 0/0.01/0.01/0.4 0/0.1/0.1/0.1\nWeight drop 0.5 0.2 0\nWeight decay 1.2e−6 1 .2e−6 0\nBPTT length 150 200 140\nBatch size 128 128 60\nInput embedding size 128 400 400\nLearning rate 0.002 0.001 0.001\nEpochs 500 50 14\nLR reduction ( lr\n10 ) [300, 400] [25, 35] [12]\nTotal parameters 13.8M 47M 151M\nTraining time (hours) 8 47 12\nTable 6.Hyper-parameters for word- and character-level language modeling experiments. Training time is for all noted epochs on an\nNVIDIA V olta. Dropout refers to embedding, (RNN) hidden, input, and output.\n1 2 3 4 5 6 7 8 9\nToken position\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Correct token probability (average)\nEnwik8 LSTM\nEnwik8 QRNN\nPTBC QRNN\nPTBC LSTM\nWT2 QRNN\nWT2 LSTM\nFigure 1.To understand how the models respond to uncertainty\nin different datasets, we visualize probability assigned to the cor-\nrect token after a deﬁned starting point. For character datasets\n(enwik8 and PTB character-level), token position refers to char-\nacters within space delimited words. For word level datasets, token\nposition refers to words following the word The, approximating\nthe start of a sentence.\n1 2 3 4 5 6 7 8 9 10\n0.00\n0.25\n0.50\n0.75\n1.00\nEnwik8\n1 2 3 4 5 6 7 8 9 10\nCharacter position\n0.00\n0.25\n0.50\n0.75\n1.00\nPTB (Character)\nFigure 2.To better understand the character level datasets, we an-\nalyze the average probability of getting a character at different\npositions correct within a word. Words are deﬁned as two or more\n[A-Za-z] characters between spaces and different length words\nhave different colors. We include the prediction of the ﬁnal space\nas it allows for capturing the uncertainty of when to end a word\n(i.e. puzzle vs puzzles ). Due to the bounded vocabulary\nof PTBC, we can see the model is far more conﬁdent in a word’s\nending than for model for enwik8.\nAn Analysis of Neural Language Modeling at Multiple Scales\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40\nParameter Importance\nweight dropout\ndropout\ninput dropout\nhidden dropout\nembedding size\nhidden units\nbptt length\nlayers\nembedding dropout\nFigure 3.We analyze the relative importance of the hyperparam-\neters deﬁning the model using a Random Forest approach for\nthe word-level task on the smaller WikiText-2 data set for AWD-\nQRNN model. The results show that weight dropout, hidden\ndropout and embedding dropout impact performance the most\nwhile the number of layers and the embedding and hidden dimen-\nsion sizes matters relatively less. Similar results are observed on\nthe PTB word level data set.\nAdam with an initial learning rate of 10−3 which is reduced\nby 10 on epochs 150 and 225. The results show that weight\ndropout, hidden dropout and embedding dropout impact per-\nformance the most while the number of layers and the sizes\nof the embedding and hidden layers matter relatively less.\nExperiments on the Penn Treebank data set also yielded\nsimilar results. Given these results, it is evident that in the\npresence of limited tuning resources, educated choices can\nbe made for the layer sizes and the dropout values can be\nﬁnely tuned.\nIn Figure 4, we plot the joint inﬂuence heatmaps of pairs of\nparameters to understand their couplings and bounds. For\nthis experiment, we consider three pairs of hyperparame-\nters: weight dropout - hidden dropout, weight dropout -\nembedding dropout and weight dropout - embedding size\nand plot the perplexity, obtained from the the WikiText-2\nexperiment above, in the form of a projected triangulated\nsurface plot. The results suggest a strong coupling for the\nhigh-inﬂuence hyperparameters with narrower bounds for\nacceptable performance for hidden dropout as compared to\nweight dropout. The low inﬂuence of the embedding size\nhyperparameter is also evident from the heatmap; so long as\nthe embeddings are not too small or too large, the inﬂuence\nof this hyperparameter on the performance is not as drastic.\nFinally, the plots also suggest that an educated guess for\nthe dropout values lies in the range of [0.1, 0.5] and tuning\nthe weight dropout ﬁrst (to say, 0.2) leaving the rest of the\nhyperparameters to estimates would be a good starting point\nfor ﬁne-tuning the model performance further.\n6. Discussion\n6.1. Penn Treebank is ﬂawed for character-level work\nWhile the Mikolov processed Penn Treebank data set has\nlong been a central dataset for experimenting with language\nmodeling, we poset that it is fundamentally ﬂawed. As\nnoted earlier, the character-level dataset does not feature\npunctuation, capitalization, or numbers - all important as-\npects we would wish for a language model to capture. The\nlimited vocabulary and use of <unk> tokens also result in\nsubstantial problems.\nIn Figure 2 we compare the level of average surprise be-\ntween two models when producing words of a given length:\none trained on Penn Treebank and the other trained on\nenwik8. enwik8 has a noticeable drop when predict-\ning the ﬁnal character (the space) compared to the Penn\nTreebank results. The only character transitions that ex-\nist within the Penn Treebank dataset are those contained\nwithin the 10,000 word vocabulary. Models trained on the\nenwik8 dataset can’t have equivalent conﬁdence as words\ncan branch unpredictably due to an unrestricted vocabulary.\nIn addition, the lack of capitalization, punctuation, or nu-\nmerals removes many aspects of language that should be\nfundamental to the task of character-level language model-\ning. Potentially counter-intuitively, we do not recommend\nits use as a benchmark even though we achieve state-of-the-\nart results on this dataset.\n6.2. Parameter count as a proxy for complexity\nTo measure the complexity and computational cost of train-\ning and evaluating models, the number of parameters in a\nmodel are often reported and compared. For certain use\ncases, such as when a given model is intended for embedded\nhardware, parameter counts may be an appropriate metric.\nConversely, if the parameter count is unusually high and\nmay require substantial resources (Shazeer et al., 2017), the\nparameter count may still function as an actionable metric.\nIn general however a model’s parameter count acts as a\npoor proxy for the complexity and hardware requirements\nof the given model. If a model with high parameter count\nruns quickly and on modest hardware, we would argue this\nis better than a model with a lower parameter count that\nruns slowly or requires more resources. More generally, pa-\nrameter counts may be discouraging proper use of modern\nhardware, especially when the parameter counts were his-\ntorically motivated by a now defunct hardware requirement.\n7. Related work\nOur work is not the ﬁrst work to show that well tuned and\nfast baseline models can be highly competitive with state-\nof-the-art work.\nAn Analysis of Neural Language Modeling at Multiple Scales\n(a) Joint inﬂuence of weight dropout and\nhidden-to-hidden dropout.\n(b) Joint inﬂuence of weight dropout and\nembedding dropout.\n(c) Joint inﬂuence of weight dropout and\nembedding size.\nFigure 4.Heatmaps plotting the joint inﬂuence of three sets of hyperparameters (weight dropout - hidden dropout, weight dropout -\nembedding dropout and weight dropout - embedding size) on the ﬁnal perplexity of the AWD-QRNN language model trained on the\nWikiText-2 data set. Results show ranges of permissible values for both experiments and the strong coupling between them. For the\nhidden dropout experiment, results suggest a narrower band of acceptable values while the embedding dropout experiment suggests a\nmore forgiving dependence so long as the embedding dropout is kept low. The joint plot between weight dropout (high inﬂuence) and\nembedding size (low inﬂuence) suggests relative insensitivity to the embedding size so long as the size is not too small or too large.\nIn Melis et al. (2018), several state-of-the-art language\nmodel architectures are re-evaluated using large-scale auto-\nmatic black-box hyper-parameter tuning. Their results show\nthat a standard LSTM baseline, when properly regularized\nand tuned, can outperform many of the recently proposed\nstate-of-the-art models on a word-level language modeling\ntasks (PTB and WikiText-2).\nMerity et al. (2017b) proposes the weight-dropped LSTM,\nwhich uses DropConnect on hidden-to-hidden weights as a\nform of recurrent regularization, and NT-ASGD, a variant\nof the averaged stochastic gradient method. Applying these\ntwo techniques to a standard LSTM language modeling\nbaseline, they achieve state-of-the-art results similarly to\nMelis et al. (2018).\nThe most recent results on character-level datasets generally\ninvolve more complex architectures however. Mujika et al.\n(2017) introduce the Fast-Slow RNN, which splits the stan-\ndard language model architecture into a fast changing RNN\ncell and a slow changing RNN cell. They show the slow\nRNN’s hidden state experiences less change than that of the\nfast RNN, allowing for longer term dependency similar to\nthat of multiscale RNNs.\nThe Recurrent Highway Network (Zilly et al., 2016) focuses\non a deeper hidden-to-hidden transition function, allowing\nthe RNN to spend more than one timestep processing a\nsingle input token.\nAdditional improvements in the model can be obtained\nthrough dynamic evaluation (Krause et al., 2018) and\nmixture-of-softmaxes (Yang et al., 2018) but since our goal\nis to evaluate the underlying model, we employ no such\nstrategies in addition.\n8. Conclusion\nFast and well tuned baselines are an important part of our\nresearch community. Without such baselines, we lose our\nability to accurately measure our progress over time. By\nextending an existing state-of-the-art word level language\nmodel based on LSTMs and QRNNs, we show that a well\ntuned baseline can achieve state-of-the-art results on both\ncharacter-level (Penn Treebank, enwik8) and word-level\n(WikiText-103) datasets without relying on complex or spe-\ncialized architectures. We additionally perform an empirical\ninvestigation of the learning and network dynamics of both\nLSTM and QRNN cells across different language model-\ning tasks, highlighting the differences between the learned\ncharacter and word level models. Finally, we present re-\nsults which shed light on the relative importance of the\nvarious hyperparameters in neural language models. On\nthe WikiText-2 data set, the AWD-QRNN model exhibited\nhigher sensitivity to the hidden-to-hidden weight dropout\nand input dropout terms and relative insensitivity to the em-\nbedding and hidden layer sizes. We hope that this insight\nwould be useful for practitioners intending to tune similar\nmodels on new datasets.\nAn Analysis of Neural Language Modeling at Multiple Scales\nReferences\nBradbury, J., Merity, S., Xiong, C., and Socher, R. Quasi-\nRecurrent Neural Networks. International Conference on\nLearning Representations, 2017.\nBreiman, L. Random forests. Machine learning, 45(1):\n5–32, 2001.\nChung, J., Ahn, S., and Bengio, Y . Hierarchical mul-\ntiscale recurrent neural networks. arXiv preprint\narXiv:1609.01704, 2016.\nDauphin, Y ., Fan, A., Auli, M., and Grangier, D. Language\nmodeling with Gated Convolutional Networks. arXiv\npreprint arXiv:1612.08083, 2016.\nGoyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and\nHe, K. Accurate, large minibatch SGD: training imagenet\nin 1 hour. arXiv preprint arXiv:1706.02677, 2017.\nGrave, E., Joulin, A., and Usunier, N. Improving Neu-\nral Language Models with a Continuous Cache. arXiv\npreprint arXiv:1612.04426, 2016a.\nGrave, Edouard, Joulin, Armand, Ciss´e, Moustapha, Grang-\nier, David, and J´egou, Herv´e. Efﬁcient softmax approx-\nimation for GPUs. arXiv preprint arXiv:1609.04309 ,\n2016b.\nGutmann, M. and Hyv ¨arinen, A. Noise-contrastive esti-\nmation: A new estimation principle for unnormalized\nstatistical models. In Proceedings of the Thirteenth Inter-\nnational Conference on Artiﬁcial Intelligence and Statis-\ntics, pp. 297–304, 2010.\nHa, D., Dai, A., and Le, Q. V . Hypernetworks. arXiv\npreprint arXiv:1609.09106, 2016.\nHochreiter, S. and Schmidhuber, J. Long Short-Term Mem-\nory. Neural Computation, 1997.\nHutter, M. The Human Knowledge Compression Contest.\nhttp://prize.hutter1.net/, 2018. Accessed:\n2018-02-08.\nInan, H., Khosravi, K., and Socher, R. Tying Word Vectors\nand Word Classiﬁers: A Loss Framework for Language\nModeling. arXiv preprint arXiv:1611.01462, 2016.\nKingma, D. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nKnoll, B. CMIX. www.byronknoll.com/cmix.\nhtml, 2018. Accessed: 2018-02-08.\nKoehn, P. Statistical Machine Translation . Cambridge\nUniversity Press, 2009.\nKrause, B., Kahembwe, E., Murray, I., and Renals, S. Dy-\nnamic Evaluation of Neural Sequence Models. 2018.\nURL https://arxiv.org/abs/1709.07432.\nKrueger, D., Maharaj, T., Kram ´ar, J., Pezeshki, M., Bal-\nlas, N., Ke, N., Goyal, A., Bengio, Y ., Larochelle, H.,\nCourville, A., et al. Zoneout: Regularizing RNNss by\nrandomly preserving hidden activations. arXiv preprint\narXiv:1606.01305, 2016.\nMarcus, M., Kim, G., Marcinkiewicz, M. A., MacIntyre, R.,\nBies, A., Ferguson, M., Katz, K., and Schasberger, B. The\nPenn Treebank: annotating predicate argument structure.\nIn Proceedings of the workshop on Human Language\nTechnology, pp. 114–119. Association for Computational\nLinguistics, 1994.\nMelis, G., Dyer, C., and Blunsom, P. On the State of\nthe Art of Evaluation in Neural Language Models. In-\nternational Conference on Learning Representations ,\n2018. URL https://openreview.net/forum?\nid=ByJHuTgA-.\nMerity, S., McCann, B., and Socher, R. Revisiting acti-\nvation regularization for language rnns. arXiv preprint\narXiv:1708.01009, 2017a.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nSentinel Mixture Models. International Conference on\nLearning Representations, 2017b.\nMerity, S., Keskar, N., and Socher, R. Regularizing and Op-\ntimizing LSTM Language Models. International Confer-\nence on Learning Representations, 2018. URL https:\n//openreview.net/forum?id=SyyGPP0TZ.\nMikolov, T., Karaﬁ ´at, M., Burget, L., Cernock ´y, J., and\nKhudanpur, S. Recurrent neural network based language\nmodel. In INTERSPEECH, 2010.\nMorin, F. and Bengio, Y . Hierarchical Probabilistic Neural\nNetwork Language Model. In Aistats, volume 5, pp. 246–\n252, 2005.\nMujika, A., Meier, F., and Steger, A. Fast-Slow Recurrent\nNeural Networks. In Advances in Neural Information\nProcessing Systems, pp. 5917–5926, 2017.\nP. Matthew, M. Neumann, M. Iyyer M. Gardner C. Clark K.\nLee L. Zettlemoyer. Deep contextualized word represen-\ntations. OpenReview, 2017.\nPress, O. and Wolf, L. Using the output embedding to im-\nprove language models. arXiv preprint arXiv:1608.05859,\n2016.\nRadford, A., Jozefowicz, R., and Sutskever, I. Learning\nto generate reviews and discovering sentiment. arXiv\npreprint arXiv:1704.01444, 2017.\nAn Analysis of Neural Language Modeling at Multiple Scales\nRocki, K., Kornuta, T., and Maharaj, T. Surprisal-driven\nzoneout. arXiv preprint arXiv:1610.07675, 2016.\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,\nQ. V ., Hinton, G. E., and Dean, J. Outrageously\nLarge Neural Networks: The Sparsely-Gated Mixture-\nof-Experts Layer. CoRR, abs/1701.06538, 2017.\nWerbos, P. J. Backpropagation through time: what it does\nand how to do it. Proceedings of the IEEE, 78(10):1550–\n1560, 1990.\nWilliams, R. J. and Peng, J. An efﬁcient gradient-based\nalgorithm for on-line training of recurrent network trajec-\ntories. Neural computation, 2(4):490–501, 1990.\nYang, Z., Dai, Z., Salakhutdinov, R., and Cohen, W. Break-\ning the Softmax Bottleneck: A High-Rank RNN Lan-\nguage Model. International Conference on Learning\nRepresentations, 2018. URL https://arxiv.org/\nabs/1711.03953.\nYu, D. and Deng, L. Automatic speech recognition: A deep\nlearning approach. Springer, 2014.\nZilly, J. G., Srivastava, R. K., Koutn ´ık, J., and Schmid-\nhuber, J. Recurrent highway networks. arXiv preprint\narXiv:1607.03474, 2016.\nZoph, B. and Le, Q. V . Neural architecture search with\nreinforcement learning. arXiv preprint arXiv:1611.01578,\n2016.",
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.9401137232780457
    },
    {
      "name": "Computer science",
      "score": 0.8318672180175781
    },
    {
      "name": "Character (mathematics)",
      "score": 0.8224546909332275
    },
    {
      "name": "Granularity",
      "score": 0.7251651883125305
    },
    {
      "name": "Natural language processing",
      "score": 0.6917409896850586
    },
    {
      "name": "Language model",
      "score": 0.6301882863044739
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6264879703521729
    },
    {
      "name": "Word (group theory)",
      "score": 0.6158737540245056
    },
    {
      "name": "State (computer science)",
      "score": 0.4496881067752838
    },
    {
      "name": "Artificial neural network",
      "score": 0.442155659198761
    },
    {
      "name": "Linguistics",
      "score": 0.20486849546432495
    },
    {
      "name": "Programming language",
      "score": 0.1579330861568451
    },
    {
      "name": "Parsing",
      "score": 0.05245858430862427
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": []
}