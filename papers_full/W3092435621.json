{
  "title": "On Losses for Modern Language Models",
  "url": "https://openalex.org/W3092435621",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4286972104",
      "name": "Aroca-Ouellette, Stephane",
      "affiliations": [
        "Vector Institute",
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A3179531004",
      "name": "Rudzicz, Frank",
      "affiliations": [
        "St Michael's Hospital",
        "Vector Institute",
        "University of Toronto"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2271328876",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3037191812",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2886690398",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2786464815",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2965210982",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2950726992",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2971959267",
    "https://openalex.org/W2174786457",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2787035179",
    "https://openalex.org/W2886424491"
  ],
  "abstract": "BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP's effect on BERT pre-training, 2) explore fourteen possible auxiliary pre-training tasks, of which seven are novel to modern language models, and 3) investigate different ways to include multiple tasks into pre-training. We show that NSP is detrimental to training due to its context splitting and shallow semantic signal. We also identify six auxiliary pre-training tasks -- sentence ordering, adjacent sentence prediction, TF prediction, TF-IDF prediction, a FastSent variant, and a Quick Thoughts variant -- that outperform a pure MLM baseline. Finally, we demonstrate that using multiple tasks in a multi-task pre-training framework provides better results than using any single auxiliary task. Using these methods, we outperform BERT Base on the GLUE benchmark using fewer than a quarter of the training tokens.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4970–4981,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n4970\nOn Losses for Modern Language Models\nSt´ephane Aroca-Ouellette1,2 and Frank Rudzicz1,2,3\n1University of Toronto\n2Vector Institute for Artiﬁcial Intelligence\n3Li Ka Shing Knowledge Institute, St Michael’s Hospital\n{stephaneao, frank}@cs.toronto.edu\nAbstract\nBERT set many state-of-the-art results over\nvaried NLU benchmarks by pre-training over\ntwo tasks: masked language modelling\n(MLM) and next sentence prediction (NSP),\nthe latter of which has been highly criticized.\nIn this paper, we 1) clarify NSP’s effect on\nBERT pre-training, 2) explore fourteen possi-\nble auxiliary pre-training tasks, of which seven\nare novel to modern language models, and\n3) investigate different ways to include mul-\ntiple tasks into pre-training. We show that\nNSP is detrimental to training due to its con-\ntext splitting and shallow semantic signal. We\nalso identify six auxiliary pre-training tasks\n– sentence ordering, adjacent sentence predic-\ntion, TF prediction, TF-IDF prediction, a Fast-\nSent variant, and a Quick Thoughts variant\n– that outperform a pure MLM baseline. Fi-\nnally, we demonstrate that using multiple tasks\nin a multi-task pre-training framework pro-\nvides better results than using any single aux-\niliary task. Using these methods, we outper-\nform BERTBase on the GLUE benchmark using\nfewer than a quarter of the training tokens.\n1 Introduction\nWhen Devlin et al. (2018) released BERT, a trans-\nformer network (Vaswani et al., 2017) trained using\na ‘masked language model’ (MLM) task and a ‘next\nsentence prediction’ (NSP), it redeﬁned the NLP\nlandscape, establishing itself as the state-of-the-art\n(SoTA) on many natural language understanding\n(NLU) benchmarks including the GLUE (Wang\net al., 2018), SQuAD (Rajpurkar et al., 2016), and\nSW AG (Zellers et al., 2018) benchmarks.\nMany models inspired by BERT have since sur-\npassed its performance. However, in contrast to the\noriginal BERT paper, many obtained better results\nby excluding the NSP task. Some, such as XLNET\n(Yang et al., 2019) and RoBERTa (Liu et al., 2019),\nrely solely on a MLM variant, while others (Wang\net al., 2020; Joshi et al., 2019; Cheng et al., 2019;\nSun et al., 2019b) incorporate one or more different\nauxiliary loss functions. To our knowledge, there\nis no published work comparing or fully exploring\nauxiliary tasks for modern language models.\nWith multi-task learning’s long history in trans-\nfer learning (Caruana, 1997; Parisotto et al., 2015;\nRen et al., 2018), its use in language understand-\ning models deserves further exploration. In this\npaper, we study existing and novel auxiliary tasks\nin a BERT paradigm to guide future research in\nan informed manner. Speciﬁcally, we test and\nprovide insight on: 1) NSP’s effect on BERT pre-\ntraining; 2) the result of 14 other auxiliary tasks\non BERT pre-training; 3) how to combine multiple\ntasks in BERT pre-training; and 4) the advantages\nof multi-task learning in BERT pre-training. Al-\nthough all experiments in this paper are conducted\nusing BERT, we believe the results are applicable\nto BERT’s successors (e.g. XLNET, RoBERTa,\nERNIE...) and future models. The code is available\nat https://github.com/StephAO/olfmlm.\n2 Related work\nAs with most deep learning, language representa-\ntions require large datasets. While there exists cor-\npora of labelled text, the vast majority of language\ndata exists as raw, unlabelled text. Accordingly,\nmany language embedding methods, and all those\ndescribed below, rely solely on unsupervised or\nself-supervised tasks.\n2.1 Pre-transformer sentence embeddings\nSkip-Thoughts (Kiros et al., 2015) was the ﬁrst\ndeep learning sentence embedding model. Its\ntraining objective, inspired by word2vec (Mikolov\net al., 2013), used RNNs to reconstruct the previ-\nous and next sentence from a given sentence. Like\nword2vec, similar sentences shared similar embed-\ndings, and while it exhibited promising results, it\n4971\nwas slow to train due to its encoding and double\ndecoding of sentences through RNNs. Hill et al.\n(2016)’s FastSent tried to follow the same sequen-\ntial sentence paradigm at a reduced training cost\nby encoding a sentence using a bag-of-words ap-\nproach and maximizing the probability of words\nin adjacent sentences. Later, Quick Thoughts (Lo-\ngeswaran and Lee, 2018) managed to maintain the\nsequential sentences objective while supporting or-\ndered words. Using two RNN models, f(s) and\ng(s), they embedded a ﬁrst set of sentences using\nf(s) and a second set consisting of the subsequent\nsentences using g(s). They jointly train the two\nmodels to predict the consecutive sentences from\na set of candidates by comparing inner products.\nThis resembles a referential game (David, 1969)\nwhere f(s) and g(s) are the sender and receiver\nrespectively.\nThe previous methods relied on the premise that\nadjacent text is semantically similar, but other sen-\ntence embedding methods have relied on other lin-\nguistic properties. The sequential denoising au-\ntoencoder (SDAE) (Hill et al., 2016) corrupts a\nsentence through deletion or word order swapping,\nencodes it, then attempts to decode the original un-\ncorrupted sentence from the encoding. This shares\nthe same underlying sequence-consistency concept\nas BERT’s MLM. Brahma (2018) also focused on\nsequence consistency by predicting whether or not\na sentence had been corrupted through deletion, in-\nsertion, replacement, or permutation. These meth-\nods only require individual sentences rather than a\nset of sequential sentences.\n2.2 Transformer-based sentence embeddings\nThe development of the transformer network\n(Vaswani et al., 2017) overcame the sequential bot-\ntleneck of RNNs by fully utilizing the paralleliza-\ntion of modern processing units, and enabling lan-\nguage models to train on signiﬁcantly more data in\nless time. GPT (Radford et al., 2018) and its succes-\nsor GPT-2 (Radford et al., 2019) were the ﬁrst mod-\nels to fully leverage this breakthrough. Following\ntraditional language modelling, their training ob-\njective is to maximize the probability of a sequence\nof tokens xusing the products of their conditional\nprobabilities p(x) =∏n\ni=1(tn |tn−1,...,t 1).\nDevlin et al. (2018) addressed the limitation of\nunidirectional context in traditional language mod-\nelling in their development of Bidirectional En-\ncoder Representations from Transformers (BERT)\n– a transformer trained using a masked language\nmodelling (MLM) task and next-sentence predic-\ntion (NSP) task on approximately 137 billion to-\nkens from a 3.3 billion word corpus created from\nthe concatenation the BooksCorpus (Zhu et al.,\n2015) and English Wikipedia datasets. The masked\nlanguage model modiﬁes the traditional language\nmodel to consider the bidirectional context in its\nprediction. For each sequence, 15% of tokens are\nreplaced with a [MASK] token. The model is then\ntrained to predict the masked words. The NSP task\nuses the output embedding of the [CLS] token that\nprepends the sequence to predict whether the sec-\nond sentence follows the ﬁrst or is from a different\ndocument. BERT’s original paper claimed that this\ntask improved performance on downstream natural\nlanguage inference (NLI) tasks.\nMASS (Song et al., 2019), ERNIE (Sun et al.,\n2019a), and SpanBERT (Joshi et al., 2019) ex-\ntended the MLM task by masking a sequence of\ncontiguous tokens instead of a single token. All\nthree demonstrated the superiority of this approach.\nMASS used a seq2seq model (Sutskever et al.,\n2014) to decode the sequence of masked tokens.\nERNIE used larger sequences of tokens over the\ncourse of three stages – ﬁrst identical to BERT, then\nmasking phrases, then masking full entities. They\nadditionally added a dialogue language model task\nusing the CLS token to classify between question-\nresponse pairs and random pairs. SpanBERT uses\nspans of sampled lengths and a ‘span boundary\nobjective’ where the token embeddings adjacent\nto the span are used to predict the masked span.\nEach of their additions provided gains on a range\nof downstream tasks, with maximal gains using\nboth. They additionally showed that NSP is detri-\nmental to training, hypothesizing that the context\nsplitting required for NSP is more detrimental than\nthe advantages provided from the task. Cheng et al.\n(2019) argued that NSP is semantically shallow\nand does not leverage BERT’S bidirectional nature,\nand replaced NSP with a three-way classiﬁcation\ntask of identifying whether one sentence follows or\nprecedes another, or is from a different document.\nUsing this simple change, they achieved a modest\nimprovement over the BERT baseline.\nXLNET (Yang et al., 2019) used permuted sen-\ntences to combine the true language modelling ob-\njective of GPT-2 (Radford et al., 2019) and BERT’s\ninsight of bi-directional context. It included the ad-\nvancements from transformer-XL (Dai et al., 2019)\n4972\nto increase the context length, and created a larger\ntraining dataset. It also ran a small ablation study\nand found that removing the NSP task improved\noverall results. XLNET beat BERT on 20 tasks,\nachieving SoTA on 18. Shortly thereafter, Liu\net al. (2019) introduced RoBERTa, which followed\nthe core concepts of BERT closely, but optimized\ndesign choices, such as dynamically masking to-\nkens each epoch instead of pre-masking the entire\ndataset, increasing the batch size, using full sen-\ntences in each batch, and removing the NSP loss.\nWith these changes, and an increased dataset, they\nmatched XLNET’s performance.\nSun et al. (2019b)’s ERNIE 2.0 made further\ngains in the GLUE leaderboard∗by incrementally\nadding seven tasks in a “continual multi-task learn-\ning” framework. They trained on ERNIE’s origi-\nnal token/phrase/entity masking, capitalization pre-\ndiction, token-document prediction, sentence re-\nordering, sentence distance prediction, discourse\nrelations, and information retrieval (IR) relevance.\nThey provided no information on the beneﬁt from\nany of the individual tasks or the ordering of the\ntasks. Raffel et al. (2019)’s T5, also high on\nthe leaderboard, achieved their results using an\nencoder-decoder variant of BERT. Through rigor-\nous experimentation of implementation details, cul-\nminating in a gigantic 11 billion parameter model\ntrained on more data, using spans, multi-task learn-\ning on the supervised downstream tasks, and using\nbeam search, they achieved SoTA on a vast array\nof tasks. Wang et al. (2020)’s StructBERT used\na word structural objective, where the model has\nto recover a shufﬂed tri-gram, and sentence struc-\ntural objective – identical to Cheng et al. (2019)’s\nthree-way classiﬁcation task, to place high on the\nleaderboard as well.\n3 Method\n3.1 Baselines\nOur primary motivation in this paper is to study and\nsurvey auxiliary pre-training tasks for multi-task\nlearning for modern language understanding mod-\nels. In this case, ‘modern’ is a transformer-based\nmodel pre-trained on a large unlabelled corpus us-\ning a form of masked language modelling. This\ndeﬁnition captures the large majority of recently\nsuccessful language models. For our baseline we\nchoose BERT, as it is the basis for subsequent mod-\n∗https://gluebenchmark.com/\nleaderboard\nels, but only include the MLM task as the beneﬁts\nof the NSP task are debated (Liu et al., 2019; Yang\net al., 2019). For computational reasons we use\nBERTBase (L = 12, H = 768, A = 12, Total Pa-\nrameters=110M), and use the uncased WordPiece\ntokenizer (Wu et al., 2016) with vocabulary size of\n30522 provided by Google†.\n3.2 Auxiliary pre-training tasks\nTo provide a fair comparison and due to compu-\ntational constraints, we limit the scope of our\ninvestigation to auxiliary tasks that can be directly\nused on any corpus of unlabelled data, do not\nrequire any language decoding, and require at most\none additional classiﬁcation layer. This excludes\nthe discourse relation task, the IR relevance task\n(Sun et al., 2019b), and the dialogue language\nmodelling task (Sun et al., 2019a) as they require\ndatasets that violate these constraints. We also\nexclude a Skip-Thoughts approach as sequentially\ndecoding outputs would require signiﬁcantly more\ncomputational resources. Token level tasks only\nuse token embeddings as input. Sentence-level\ntasks use the [CLS] token embedding as input.\nThe FastSent variant uses both, but we label it\nas a sentence-level task as it does require the\nsentence embedding (from the [CLS] token). Tasks\nthat have not previously been applied to modern\nlanguage models are italicized. We investigate the\nfollowing tasks:\nToken level tasks\n1. Term Frequency prediction (TF): Regression\npredicting a token’s frequency in the rest of\nthe document. The frequency is re-scaled be-\ntween 0 and 10 per document.\n2. Term Frequency-Inverse Document Frequency\nprediction (TF-IDF): Regression predicting a\ntoken’s tf-idf that has been re-scaled between\n0 and 10 per document.\n3. Sentence Boundary Objective (SBO): Predict\nthe masked token given the embeddings of the\nadjacent tokens.\n4. Trigram-Shufﬂing (TGS): 6-way classiﬁca-\ntion predicting the original order of shufﬂed\ntri-grams.\n5. Token Corruption Prediction (TCP): Binary\n†https://github.com/google-research/\nbert\n4973\nclassiﬁcation of whether a token has been cor-\nrupted (inserted, replaced, permuted) or not.\n6. Capitalization Prediction (Cap.): Binary,\nwhether a token is capitalized or not.\n7. Token Length Prediction (TLP): Regression to\npredict the length of the WordPiece token.\nSentence level tasks\n8. Next Sentence Prediction (NSP): Binary,\nwhether the second sentence follows the ﬁrst\nor comes from a separate document.\n9. Adjacent Sentence Prediction (ASP): 3-way\nclassiﬁcation whether the second sentence pro-\nceeds the ﬁrst, precedes the ﬁrst, or they come\nfrom separate documents.\n10. Sentence Ordering (SO): Binary, predicting if\nthe two sentences are in or out of order.\n11. Sentence Distance Prediction (SDP): 3-way\nclassiﬁcation of whether the second sen-\ntence proceeds, the two sentences are non-\ncontiguous from the same document, or come\nfrom separate documents.\n12. Sentence Corruption Prediction (SCP): Bi-\nnary classiﬁcation of whether a tokens in a\nsentence have been corrupted (inserted, re-\nplaced, permuted) or not.\n13. Quick Thoughts variant (QT): Split each batch\ninto two, where the second half contains the\nsubsequent sentences of the ﬁrst half (e.g.\nwith batch size 32, sentence 17 follows sen-\ntence 1, sentence 18 follows sentence 2,...).\nWe use an energy-based model to predict the\ncorrect continuation for each sentence in the\nﬁrst half where the energy between two sen-\ntences is deﬁned by the negative cosine sim-\nilarity of their [CLS] embeddings. We use\none model to encode both halves concurrently.\nSee Figure 1.\n14. FastSent variant (FS): Split each batch into\ntwo, where the second half contains the subse-\nquent sentences of the ﬁrst half (same as QT\nabove). The loss is deﬁned as cross-entropy\nbetween 1.0 and the cosine similarity of a\nsentence [CLS] embedding and the other sen-\ntence token embeddings ([CLS] embedding\nfrom the ﬁrst half with token embeddings\nfrom the second half and [CLS] embeddings\nfrom second half with token embeddigns from\nthe ﬁrst half). We use one model to encode\nboth halves concurrently.\n3.3 Combining tasks\nBERT originally proposed summing the MLM and\nNSP losses directly. ERNIE uses signiﬁcantly more\nlosses and proposes a continual multi-task learn-\ning framework to incorporate them, in which they\nincrementally add new tasks while sampling pre-\nviously learnt tasks. To provide insight on how\nbest to combine tasks, we investigate the six fol-\nlowing ways of combining a set of tasks for BERT\npre-training:\n1. Sum losses from all tasks (sum.)\n2. Incrementally add tasks, summing the losses\nfrom all added tasks (Inc.)\n3. Alternating between tasks each iteration (Alt.)\n4. Alternating between auxiliary tasks each iter-\nation and summing it with MLM (Alt.+)\n5. ERNIE’s continual multi-task learning\n(CMTL), for more detail see Appendix A\n6. ERNIE’s continual multi-task learning on aux-\niliary tasks summed with MLM (CMTL+)\nWe note that both a direct summation and a simple\nincremental approach cannot accommodate tasks\nthat require different input structures – for example\nsentence ordering, which requires that the two sen-\ntences are always adjacent, cannot be trained simul-\ntaneously with next sentence prediction, which re-\nquires sentences from different documents at times\n– or different corpora, such as ERNIE 2.0’s IR rele-\nvance dataset.\n3.4 Input Representation\nTo construct the input embedding to the trans-\nformer, we sum token embeddings, learned position\nembeddings, learned sentence type (sentence A or\nB) embeddings, and, to enable ERNIE’s continual\nmulti-task learning, a learned task id embeddings .\n3.5 Dataset\nWe follow precedent in using the BookCorpus ‡\n(Zhu et al., 2015) and Wikipedia dataset as our cor-\npora. We ﬁlter the Wikipedia corpus in the same\nfashion as BERT, ignoring lists, tables, and headers.\nWe additionally ﬁlter documents that have: fewer\nthan 10 words or fewer than 4 sentences. This\nexcludes small uninformative documents. We addi-\ntionally segment long documents into documents\nof roughly 1024 tokens. This creates a corpus with\n2.7 billion words (3.8 billion tokens) divided into\n6.8 million documents.\n‡Unfortunately, the BookCorpus is no longer publicly\navailable.\n4974\nAux. Task MNLI\n392k\nQQP\n363k\nQNLI\n108k\nSST-2\n67k\nCoLA\n8.5k\nSTS-B\n5.7k\nMRPC\n3.5K\nRTE\n2.5k\nAvg.\n-\nNone (MLM) 80.3 88.0 86.7 91.6 51.5 84.9 86.9 60.6 78.8\nTF 81.5 88.7 89.1 90.9 46.8 87.0 87.3 62.1 79.2\nTF-IDF 81.2 88.6 89.4 90.5 46.7 86.8 88.8 63.2 79.4\nSBO 80.5 88.0 89.1 92.5 48.8 85.4 86.6 56.0 78.4\nTGS 80.5 88.2 87.1 90.6 50.3 85.4 87.8 58.1 78.5\nTCP 81.3 88.5 88.0 91.5 49.7 85.7 87.0 58.1 78.7\nCap. 81.1 88.6 87.0 91.3 48.0 85.8 86.0 57.8 78.2\nTLP 80.8 88.3 87.7 91.5 47.0 86.0 86.1 59.6 78.4\nNSP 79.9 87.1 86.0 90.9 48.3 84.0 85.4 58.1 77.5\nASP 80.4 88.4 88.9 89.9 42.2 86.9 87.3 68.2 79.0\nSO 80.9 88.6 89.2 89.8 44.1 87.4 86.4 66.1 79.1\nSDP 79.9 87.9 87.8 90.3 47.7 85.9 87.7 62.5 78.7\nQT 81.6 88.6 88.7 91.4 55.6 86.2 87.1 63.5 80.3\nFS 81.9 88.6 88.4 91.5 55.1 86.6 88.3 59.2 80.0\nSCP 80.4 88.4 87.6 90.4 46.6 85.3 86.4 59.2 78.0\nTable 1: Test results on GLUE development set for models pre-trained on MLM (No Aux.) and MLM + auxiliary\ntasks trained over 10 billion tokens. F1 scores are reported for QQP and MRPC, Spearman correlations are reported\nfor STS-B, and accuracy scores are reported for the other tasks. Refer to section 3.2 for a description of each task.\nBest results in each column are underlined. Averages above two estimated σs of the MLM baseline are bolded.\nMNLI\n392k\nQQP\n363k\nQNLI\n108k\nSST-2\n67k\nCoLA\n8.5k\nSTS-B\n5.7k\nMRPC\n3.5K\nRTE\n2.5k\nAvg.\n-\nMLM 80.3 88.0 86.7 91.6 51.5 84.9 86.9 60.6 78.8\nQT 81.6 88.6 88.7 91.4 55.6 86.2 87.1 63.5 80.3\nSum. 82.0 89.0 90.5 91.2 49.4 88.3 89.1 70.8 81.4\nInc. 80.9 88.8 89.6 90.8 50.6 87.6 86.3 69.3 80.6\nAlt. 79.8 88.4 89.3 89.3 44.3 86.8 86.2 70.4 79.4\nAlt.+ 81.5 89.0 90.1 90.6 55.3 87.9 87.0 68.6 81.3\nCMTL 79.6 88.2 88.8 89.7 40.3 87.1 86.1 66.8 78.4\nCMTL+ 81.7 88.6 90.3 91.3 53.9 88.5 89.2 70.4 81.7\nTable 2: Results on GLUE development set for models pre-trained on MLM (our baseline), MLM + QT (best\nsingle auxiliary task model) and different combinations of the best performing tasks. Refer to section 3.3 for more\ndetail. Best results in each column are underlined. Averages above two estimated σs of the MLM baseline are\nbolded.\n3.6 Pre-Training Details\nFor all tests, we train on 10 billion tokens using\nan Adam optimizer (Kingma and Ba, 2014) with a\nlearning rate of 1e-4 that warms-up over the ﬁrst\n1% of tokens and linearly decays after, batch size\n= 128, max sequence length = 128, β1 = 0.9, β2\n= 0.999, L2 weight decay of 0.01, and a dropout\nprobability of 0.1. In accordance with other papers,\nwe use a gelu activation (Hendrycks and Gimpel,\n2016). Using four p100 GPUs, it takes between 13\nand 15 hours to train a model for each one billion\ntoken epoch depending on the tasks used.\n3.7 Fine-Tuning Details\nAll models are tested on the GLUE (Wang et al.,\n2018) benchmark, as it has been accepted by the\ncommunity as a benchmark for NLU. We also com-\npare the ﬁnal best model and our baseline on the\nSuperGLUE (Wang et al., 2019a) benchmark. Fol-\nlowing Devlin et al. (2018); Cheng et al. (2019), we\ndisregard GLUE’s problematic WNLI task. Due to\nGLUE’s private test set, and the number of exper-\niments performed, the results are on the available\ndevelopment set except for the ﬁnal results in Ta-\nbles 3 and 4. To ﬁne-tune the model on the GLUE\ndataset, we use Jiant’s (Wang et al., 2019b) pro-\nvided code§. We limit the maximum number of\nepochs to 3 and we run the ﬁne-tuning procedure\nthree times with learning rates = 5e-5, 3e-5, 2e-5\nand take the best results for each task individu-\nally across these runs. This is done to reduce the\nvariance in the results that comes from the low-\nresource tasks CoLA, RTE, and MRPC. For all\nother ﬁne-tuning parameters, we use the default\nvalues provided by jiant unless otherwise stated.\n3.8 Final Model\nOur ﬁnal CMLT+ model is shown in Figure 1 to\nhelp visualize the inputs to each task.\n§https://github.com/nyu-mll/jiant\n4975\nFigure 1: Architecture used for the combined tasks tests. Sentences 17-32 are the continuations of sentences 1-16\nrespectively (1-17, 2-18, 3-19...). The two halves of the batch are only split for clarity of the Quick Thoughts\nvariant task; they are embedded at the same time by the same network. Though only depicted on only one token,\nthe token level tasks (MLM, TF-IDF prediction) are trained across all token embeddings.\n4 Results\nIn this section, we present the results from an ar-\nray of different tests. Due to the stochastic nature\nof the training, we would ideally run each test nu-\nmerous times. However, this is prohibitively ex-\npensive due to the computational costs. Building\nfrom Raffel et al. (2019)’s experimental approach,\nwe instead calculate the standard deviation for 5\nindependent trainings of the baseline MLM-only\nmodel, the MLM + NSP model, and our CMTL+\nmodel. We ﬁnd σMLM = 0.198, σNSP = 0.222,\nσCMTL + = 0.273, and use the highest,σ= 0.273,\nas an estimate for the standard deviation across\nall experiments. See Appendix B for more detail.\nThis is comparable to Raffel et al. (2019)’s esti-\nmated standard deviation of 0.235. In each table,\nwe boldface all average GLUE scores that are two\nestimated standard deviations above the MLM base-\nline. For the average GLUE score, we follow Wang\net al. (2018) and average the macro averages of\neach task. This is different than averaging the num-\nbers in a row as we only report one metric per task.\n4.1 Understanding NSP\nTo understand the role of NSP in BERT pre-training\nwe compare the performance of three models: the\nﬁrst trained on MLM; the second trained on MLM\nand NSP; and the third trained on MLM with NSP’s\ncontext split, but without NSP’s loss, which we la-\nbel split. Contrasting the MLM model to the split\nmodel explicates the impact of splitting the inputs\ncontext, while comparing the NSP model to the\nsplit model clariﬁes the beneﬁts of the NSP loss.\nAs expected, ﬁgure 2 a) demonstrates a clear perfor-\nmance drop when splitting contexts. From ﬁgure 2\nb) and 2 c), we see the biggest drops are from infer-\nence tasks. We hypothesize that providing a model\nsplit contexts and no signal to differentiate it from\ncontiguous text hinders it’s ability to understand\nthe logical ﬂow of language. As we contrast the\nNSP model and the split model, we see that adding\nsuch a signal does indeed improve the results on\ninference tasks, especially in early stages of train-\ning. However, as training progresses, its beneﬁt\nstagnate. This may be because, as other papers\n4976\nFigure 2: Average results on a) all GLUE tasks, b) only inference tasks (MNLI, QNLI, RTE), and c) non-inference\ntasks (QQP, SST-2, CoLA, STS-B, MRPC) for models trained on MLM, MLM + NSP, and MLM with NSP’s split\ncontext but no NSP loss (Split) throughout training over 10 billion tokens.\nhave proposed, NSP is semantically shallow and\ncan often be solved easily through lexical overlap.\nInterestingly, ﬁgure 2 c) shows that the NSP loss\nprovides no beneﬁts, and may indeed be detrimen-\ntal towards non-inference tasks even when com-\npared to the split model. Finally, we see that the\nMLM model continues to improve throughout each\nstage of training, whereas both the NSP model and\nthe split model see have diminishing returns with\nmore training, indicating that splitting the context\nimposes inherent limitations on language models.\n4.2 Auxiliary Tasks\nWe ﬁrst compare the 14 auxiliary tasks in Table\n1 to a MLM baseline (No Aux.). As noted in the\nprevious section, and supporting many recent pa-\npers (Liu et al., 2019; Yang et al., 2019; Joshi et al.,\n2019), NSP is detrimental to training. As discussed\nby Cheng et al. (2019) and reinforced by the results\nof (Wang et al., 2020), next sentence prediction\nprovides a shallow supervision signal, and is often\nsolvable through lexical overlap. Adjacent sen-\ntence prediction and sentence ordering on the other\nhand require deeper semantic understanding of the\nstructure of language. Our results clearly support\nthis claim, with SO and ASP outperforming MLM\nand NSP on all inference tasks and greatly out-\nperforming all auxiliary tasks on RTE, the only\nlow-resource inference task. Additionally, they are\nless penalized by context splitting, which we have\nshown to degrade performance; NSP and SDP cut\nthe context in half 50% of the time, ASP cuts the\ncontext in half a third of the time, and sentence\nordering preserves the full context in all cases, al-\nbeit shufﬂed. The model trained using the Quick\nThoughts variant (QT) performs the best out of all\nthe above models. We hypothesize that the loss,\nbased on cosine similarity, provides a soft clus-\ntering around semantically similar topics, which\nproduces more distinguishable embeddings. The\nFastSent variant (FS) provides a similar signal and\nperforms the second best, suggesting that some\nform of soft clustering does provide substantial\nbeneﬁt to pre-training. TF-IDF, and to a lesser ex-\ntent TF, prediction also improve performance on\na range of downstram tasks. This aligns with Sun\net al. (2019b)’s observations that identifying high\nvalue words (and discounting ﬁller words) provides\na useful signal for language models. All other tasks\nfail to provide any meaningful gains. Of these, the\ncontext distortion from the corruption prediction\ntasks (TC and SC) likely outweigh their beneﬁt.\nAdditionally, MLM is already a form of corruption,\nmaking TC and SC partially redundant. Our re-\nsults did not ﬁnd the Sentence Boundary Objective\n(SBO) to be beneﬁcial. However, as it was origi-\nnally implemented for spans, this does not discount\nthe results of Joshi et al. (2019); in our context,\nwhich only masks a single word, it is likely re-\ndundant with MLM. The trigram shufﬂing (TGS)\ntasks similarly did not provide the value exhibited\nin Wang et al. (2020). However, due to a lack of\ndetails and code in the original paper, implemen-\ntation details may be at fault. Token length and\ncapitalization prediction, which were implemented\nas other proxies for word importance prediction,\nappear to be too noisy for their intended purpose.\n4977\nMNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg. Dev. Set Avg.\nBERTBase 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6 -\nMLM baseline 81.8/81.3 70.0 87.1 90.4 45.3 80.6 87.3 59.2 76.1 80.0\nCMTL+ 83.8/82.9 71.7 90.7 92.2 56.3 83.4 88.8 66.9 80.1 83.2\nBERTLarge (330M) 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1 -\nRoBERTa (355M) 90.8/90.2 74.3 95.4 96.7 67.8 91.9 92.3 88.2 87.88 -\nT5 (11B) 92.2/91.9 75.1 96.9 97.5 71.6 92.8 92.8 92.8 89.78 -\nTable 3: GLUE test results, scored by the evaluation server excluding the problematic WNLI task.\nMatched/mismatched accuracy are reported for MNLI, F1 scores are reported for QQP and MRPC, Spearman\ncorrelations are reported for STS-B, and accuracy scores are reported for the other tasks. The BERT Base results\nare from the original BERT paper (Devlin et al., 2018). The MLM baseline and CMTL+ models are our imple-\nmentations. We include the performance of our models on the development set for reproducibility. Best results in\neach column for models of comparable size are underlined. For context, we additionally include results from the\nGLUE leaderboard for BERTLarge, RoBERTa, and T5, and their respective size measured by number of parameters.\nBERTBase, MLM baseline, and CMTL+ all have a size of 110M parameters.\nBoolQ CB COPA MultiRC ReCoRD RTE WiC WSC Avg.\nMLM baseline 69.2 67.8 59.0 30.7 33.0 59.4 61.0 65.1 55.7\nCMTL+ 72.0 72.9 62.8 34.9 33.0 64.0 64.9 65.1 58.7\nBertLarge (330M) 77.4 79.5 70.6 47.1 71.7 71.7 69.6 65.1 69.0\nT5 (11B) 91.2 95.4 94.8 75.7 93.8 92.5 76.9 93.8 89.3\nTable 4: SuperGLUE test results, scored by the evaluation server. Both models use most common class prediction\nfor ReCoRD and WSC. The MLM baseline also uses most common class prediction for MultiRC. Best results\nin each column for models of comparable size are underlined. For context, we additionally include results from\nthe SuperGLUE leaderboard for BERTLarge and T5, and their respective size measured by number of parameters.\nCMTL+ and MLM baseline both have sizes of 110M parameters.\n4.3 Combining Tasks\nTo test combining multiple tasks, we use all aux-\niliary losses that substantially outperform a pure\nMLM baseline. For tasks that provide similar sig-\nnals, we select the one that achieved a higher aver-\nage on the previous test; QT is chosen over FS and\nTF-IDF is chosen over TF. Between ASP and SO,\nwhich have a statistically insigniﬁcant difference,\nwe choose SO as it retains the full context, as any\nsplit context from ASP would likely be detrimental\nto the other tasks. This provides 4 tasks for the\nmulti-task training: MLM, QT, SO, and TF-IDF.\nTable 2 shows a stark contrast between incor-\nporating an MLM loss term in each iteration com-\npared to treating MLM equivalent to other tasks\nwhen switching between them; Alt.+ and CMTL+\nboth outperform their counterparts by 1.9 and 3.3\npercent respectively. Our results indicate that multi-\ntask training with MLM preserves the beneﬁts of\neach individual task, with the combined models re-\ntaining QT’s high CoLA score and SO’s high RTE\nscore. Further, these gains are additive in most\ncases: for QNLI, MNLI, and STS-B the combined\nmodels performs better than any single auxiliary\ntask models. This leads to a model that vastly out-\nperforms the baseline MLM model or using any\nsingular auxiliary task.\nBetween combination methods that use MLM in\nevery iteration, the incremental approach appears to\nbe the worse, while summing everything, alternat-\ning auxiliary tasks (Alt.+), and continual multi-task\nlearning on auxiliary tasks (CMTL+) all perform\nsimilarly, with CMTL+ slightly outperforming the\nother two, which supports Sun et al. (2019b)’s re-\nsults. Interestingly, both approaches where tasks\nvary each iteration (Alt.+ and CMTL+) see a signiﬁ-\ncant beneﬁt on the CoLA task. While not beneﬁcial\nin our framework, an alternating pattern or CMTL\nhave the additional beneﬁt of enabling different in-\nput structures or the use of different corpora (such\nas ERNIE 2.0’s IR relevance corpora), which can-\nnot be done using a direct summation.\n4.4 Final Results\nFor our ﬁnal test, we train our baseline MLM model\nand CMTL+ model on 32 billion tokens and present\nthe results using the GLUE and SuperGLUE evalu-\nation servers in Tables 3 and 4 respectively. When\nﬁne-tuning these models, we run an exhaustive hy-\nper parameter search on learning rates = 1e-5, 2e-5,\n3e-5, 5e-5, batch sizes = 16, 32, and number of\nepochs = 2, 3, 4. The results show that the CMTL+\nmodel – trained on MLM, QT, SO, and TF-IDF in\n4978\na continual multi-task learning framework – vastly\noutperforms the MLM baseline in every task. Fur-\nther, our model trained on 32 billion tokens outper-\nforms the original BERTBase, which required 137\nbillion tokens. While we include larger models –\nBERTLarge, RoBERTa, and T5 – in the tables for\ncontext, we remind the readers that these results\nare not comparable to our results. First, they are\nlarger, with sizes of 330 million, 335 million, and\n11 billion parameters respectively, compared to our\n110 million parameters. Second, RoBERTa and T5\nare trained using a much larger dataset of 160 GB\nand 750 GB compared to our (and BERT Large’s)\n13 GB. Finally, BERTLarge, RoBERTa, and T5 are\ntrained on more tokens, training on 137 billion, 2.2\ntrillion, and 1 trillion tokens respectively compared\nto our 32 billion tokens. While the results are not\ncomparable, we hope that the tasks we used in our\nmodel can be utilized by newer and larger models\nto improve their understanding of language.\n5 Discussion\nOur results support several recent papers: we sup-\nport Liu et al. (2019); Yang et al. (2019); Joshi\net al. (2019)’s claim that NSP hinders BERT pre-\ntraining, especially for non-inference tasks, due to\ncutting context half the time; we reinforce Cheng\net al. (2019); Wang et al. (2020)’s proposal that\nNSP prediction is a semantically shallow and of-\nten solvable through lexical overlap and that using\na task that requires understanding the ordering of\ncontiguous text provides a stronger semantic sig-\nnal; and we uphold Sun et al. (2019a,b)’s idea that\na language model should be trained in a multi-task\nsetting. Further, we offer novel methods and in-\nsights. Providing a signal to reduce the embedding\ndistance between semantically similar sentences,\nas in our FastSent or QuickThought variants do,\nproduces a strong boosts to downstream tasks, with\nthe hypothesis that they produce more distinguish-\nable embeddings. Providing a signal that relays\nword importance, such as TF-IDF and TF, likewise\nproduces substantial beneﬁt to BERT pre-training.\nWe show strong evidence that a MLM variant loss\nshould always be included when multi-task learn-\ning. Finally, we demonstrate the value of multi-task\nlearning for language model pre-training; combin-\ning multiple beneﬁcial tasks leads to better results\nthan using any of the individual tasks alone.\nAs our focus was a breadth-based search of pos-\nsible auxiliary tasks, we believe that further gains\nare possible through a deeper exploration of each\ntask. Using soft labels in ASP for sentences that\nare near (but not directly adjacent to) the other sen-\ntence has been shown to provide improvements\n(Cheng et al., 2019). ( n!)-way classiﬁcation with\nnsentence-pieces for sentence ordering is a more\nchallenging task that could provide additional ben-\neﬁts. Other similarity metrics, such as dot product\nor Euclidean distance, may provide more useful for\nthe FS or QT methods. Beyond using a loss based\non a similarity metric, it is possible that other unsu-\npervised clustering algorithms could be beneﬁcial.\nCurrently, each task has different loss ranges based\non the nature, and not the inherent value, of the task.\nAs some tasks may be more useful than others, it is\nlikely that weighting each task based on some value\nmetric could prove beneﬁcial. Groups with sufﬁ-\ncient computational resources may also be inter-\nested in exploring how the ordering in the continual\nmulti-task learning framework affects downstream\ntasks. Lastly, we do not tune hyperparameters, us-\ning only the stated values from previous papers for\nall our experiments. We leave the above potential\nto future work.\n6 Conclusion\nWe investigate and support several reasons why\nnext-sentence prediction is ill-suited for BERT pre-\ntraining, we provide better inference-based alterna-\ntives, and we develop other novel auxiliary tasks\nbased on word importance and soft clustering that\nprovide substantial beneﬁts to BERT pre-training.\nWe also demonstrate the beneﬁt of multi-task learn-\ning in BERT pre-training, and identify key factors\non how to best combine tasks. We hope the insights\nprovided here will help guide the development of\nbetter language models in the future.\nAcknowledgements\nRudzicz is supported by a CIFAR Chair in Artiﬁcial\nIntelligence.\n4979\nReferences\nSiddhartha Brahma. 2018. Unsupervised learning of\nsentence representations using sequence consistency.\narXiv preprint arXiv:1808.04217.\nRich Caruana. 1997. ”multitask learning”. Machine\nLearning, 28(1):41–75.\nXingyi Cheng, Weidi Xu, Kunlong Chen, Wei Wang,\nBin Bi, Ming Yan, Chen Wu, Luo Si, Wei Chu,\nand Taifeng Wang. 2019. Symmetric Regulariza-\ntion based BERT for Pair-wise Semantic Reasoning.\narXiv preprint arXiv:1909.03405.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Rus-\nlan Salakhutdinov. 2019. Transformer-XL: Atten-\ntive language models beyond a ﬁxed-length context.\narXiv preprint arXiv:1901.02860.\nLewis David. 1969. Convention: a philosophical study.\nCambridge, Harvard University Press.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. CoRR, abs/1810.04805.\nDan Hendrycks and Kevin Gimpel. 2016. Bridg-\ning Nonlinearities and Stochastic Regularizers\nwith Gaussian Error Linear Units. CoRR,\nabs/1606.08415.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen.\n2016. Learning distributed representations of\nsentences from unlabelled data. arXiv preprint\narXiv:1602.03483.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2019. Span-\nBERT: Improving pre-training by representing and\npredicting spans. arXiv preprint arXiv:1907.10529.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems,\npages 3294–3302.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nLajanugen Logeswaran and Honglak Lee. 2018. An\nefﬁcient framework for learning sentence represen-\ntations. arXiv preprint arXiv:1803.02893.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nEmilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhut-\ndinov. 2015. Actor-mimic: Deep multitask and\ntransfer reinforcement learning. arXiv preprint\narXiv:1511.06342.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning. Technical re-\nport, Technical report, OpenAI.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. arXiv preprint\narXiv:1606.05250.\nMengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake\nSnell, Kevin Swersky, Joshua B Tenenbaum, Hugo\nLarochelle, and Richard S Zemel. 2018. Meta-\nlearning for semi-supervised few-shot classiﬁcation.\narXiv preprint arXiv:1803.00676.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: Masked sequence to se-\nquence pre-training for language generation. arXiv\npreprint arXiv:1905.02450.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019a. ERNIE: Enhanced Rep-\nresentation through Knowledge Integration. arXiv\npreprint arXiv:1904.09223.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng,\nHao Tian, Hua Wu, and Haifeng Wang. 2019b.\nERNIE 2.0: A continual pre-training frame-\nwork for language understanding. arXiv preprint\narXiv:1907.12412.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019a. Super-\nGLUE: A Stickier Benchmark for General-Purpose\nLanguage Understanding Systems. arXiv preprint\n1905.00537.\n4980\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGLUE: A Multi-Task Benchmark and Analysis Plat-\nform for Natural Language Understanding. ArXiv\npreprint 1804.07461.\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun,\nKatherin Yu, Jan Hula, Patrick Xia, Raghu Pappa-\ngari, Shuning Jin, R. Thomas McCoy, Roma Pa-\ntel, Yinghui Huang, Jason Phang, Edouard Grave,\nHaokun Liu, Najoung Kim, Phu Mon Htut, Thibault\nF’evry, Berlin Chen, Nikita Nangia, Anhad Mo-\nhananey, Katharina Kann, Shikha Bordia, Nicolas\nPatry, David Benton, Ellie Pavlick, and Samuel R.\nBowman. 2019b. jiant 1.2: A software toolkit\nfor research on general-purpose text understanding\nmodels. http://jiant.info/.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia,\nZuyi Bao, Liwei Peng, and Luo Si. 2020. Struct-\nBERT: Incorporating Language Structures into Pre-\ntraining for Deep Language Understanding. In Inter-\nnational Conference on Learning Representations.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. XLNet: Generalized Autoregressive Pretrain-\ning for Language Understanding. arXiv preprint\narXiv:1906.08237.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. SW AG: A large-scale adversarial\ndataset for grounded commonsense inference. arXiv\npreprint arXiv:1808.05326.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning Books and Movies: To-\nwards Story-Like Visual Explanations by Watching\nMovies and Reading Books. In The IEEE Interna-\ntional Conference on Computer Vision (ICCV).\nA Continual Multi-Task Learning\nThough no explicit details are provided for ERNIE\n2.0’s continual multi-task learning (CMTL), we\ninfer the following algorithm from their description\nand example:\n1. Split training into N stages where N is the\nnumber of non-jointly trained tasks (e.g. in\nour ﬁnal model, only auxiliary tasks are\ncounted, MLM is not counted). Each stage\ndeﬁnes a number of tokens to be trained on for\neach task. When each task has been trained on\nfor the speciﬁed number of tokens, the train-\ning moves to the next stage.\n2. Calculate the token chunk size, C = T/(N ∗\n(N+ 1)), where T is the total number of train-\ning tokens.\n3. Each stage, Si, a new task is introduced.\nDuring that stage the new task is trained on\nC∗(i+1) tokens, previously introduced tasks\nare trained on C tokens, and yet to be intro-\nduced tasks are trained on 0 tokens.\nThe method can use iterations or tokens. The\nabove method trains on each task using the same\nnumber of tokens/iterations, gradually incorporat-\ning more tasks, while still training on previous\ntasks. Below we provide two examples. The ﬁrst\nfrom (Sun et al., 2019b) which uses four tasks and\n200k iterations, the second from our ﬁnal combined\nmodel which uses three tasks (MLM not included)\nand 10 billion tokens.\nTask Stage 1 Stage 2 Stage 3 Stage 4\n1 20k 10k 10k 10k\n2 0 30k 10k 10k\n3 0 0 40k 10k\n4 0 0 0 50k\nTable 5: Training using CMTL with 4 tasks over 200k\ntotal iterations. Example from Sun et al. (2019b)\nTask Stage 1 Stage 2 Stage 3\n1 1.67B 0.83B 0.83B\n2 0 2.5B 0.83B\n3 0 0 3.33B\nTable 6: Training using CMTL with 3 tasks over 10B\ntotal iterations.\n4981\nB Signiﬁcance testing\nTo further solidify our claims, we perform signiﬁ-\ncance testing on our results between NSP and the\nMLM baseline, as well as our CMTL+ model and\nthe MLM baseline. For each NSP, MLM, and the\nCMTL+ model we evaluate 5 runs, found in table\n7. We ﬁrst run a Lilliefors test, and ﬁnd that the\np-values are large enough that we accept the null\nhypothesis that our the data follows a normal distri-\nbution for each of our sets of experiments. We then\nrun an independent t-test between NSP and MLM,\nand between the our CMTL+ and MLM. We correct\nthe p-values using Bonferroni correction and ﬁnd a\np-val of 2.547e−03 between NSP and MLM and\na p-val of 1.069e−06 between CMTL+ and MLM.\nIn both cases, the p-values are small enough that\nwe reject the null hypothesis that the samples come\nfrom the same distribution, supporting our hypoth-\nesis that MLM is better than NSP, and CMTL+ is\nbetter than MLM.\nRun MLM NSP CMTL+\n1 78.18 77.663 80.56\n2 77.90 77.363 80.30\n3 78.38 77.775 80.66\n4 78.25 77.275 80.45\n5 77.96 77.338 81.03\nMean: 78.13 77.483 80.60\nStd. Dev.: 0.20 0.22 0.27\nLilliefors p-val 0.712 0.148 0.659\nTable 7: Average GLUE score results on 5 different\ntrainings.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8242651224136353
    },
    {
      "name": "Sentence",
      "score": 0.7598434090614319
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7365555167198181
    },
    {
      "name": "Task (project management)",
      "score": 0.6950464248657227
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5772672891616821
    },
    {
      "name": "Natural language processing",
      "score": 0.5718997120857239
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5663555860519409
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5506014823913574
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5408980846405029
    },
    {
      "name": "Training set",
      "score": 0.5366389751434326
    },
    {
      "name": "Machine learning",
      "score": 0.42126673460006714
    },
    {
      "name": "Language model",
      "score": 0.41257476806640625
    },
    {
      "name": "Speech recognition",
      "score": 0.3249427080154419
    },
    {
      "name": "Programming language",
      "score": 0.060095369815826416
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I1297363086",
      "name": "St. Michael's Hospital",
      "country": "CA"
    }
  ],
  "cited_by": 2
}