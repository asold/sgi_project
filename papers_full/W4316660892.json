{
  "title": "Fusion of Satellite Images and Weather Data With Transformer Networks for Downy Mildew Disease Detection",
  "url": "https://openalex.org/W4316660892",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5055296568",
      "name": "William Maillet",
      "affiliations": [
        "Institut National des Sciences Appliquées Centre Val de Loire",
        "Laboratoire Pluridisciplinaire de Recherche en Ingénierie des Systèmes, Mécanique et Energétique"
      ]
    },
    {
      "id": "https://openalex.org/A5010355957",
      "name": "Maryam Ouhami",
      "affiliations": [
        "Institut National des Sciences Appliquées Centre Val de Loire",
        "Laboratoire Pluridisciplinaire de Recherche en Ingénierie des Systèmes, Mécanique et Energétique"
      ]
    },
    {
      "id": "https://openalex.org/A5061197809",
      "name": "Adel Hafiane",
      "affiliations": [
        "Institut National des Sciences Appliquées Centre Val de Loire",
        "Laboratoire Pluridisciplinaire de Recherche en Ingénierie des Systèmes, Mécanique et Energétique"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3176466780",
    "https://openalex.org/W3154326567",
    "https://openalex.org/W3087613434",
    "https://openalex.org/W2963131120",
    "https://openalex.org/W3093647646",
    "https://openalex.org/W3024717699",
    "https://openalex.org/W3215899623",
    "https://openalex.org/W3137384391",
    "https://openalex.org/W6793736971",
    "https://openalex.org/W4312384316",
    "https://openalex.org/W2968823298",
    "https://openalex.org/W3174385379",
    "https://openalex.org/W2589171657",
    "https://openalex.org/W2983376237",
    "https://openalex.org/W3207506706",
    "https://openalex.org/W3217153199",
    "https://openalex.org/W3180045188",
    "https://openalex.org/W3174285742",
    "https://openalex.org/W3200870516",
    "https://openalex.org/W2923136550",
    "https://openalex.org/W4210576848",
    "https://openalex.org/W6805163190",
    "https://openalex.org/W6797613833",
    "https://openalex.org/W2977430056",
    "https://openalex.org/W2912663118",
    "https://openalex.org/W3081638962",
    "https://openalex.org/W2996041315",
    "https://openalex.org/W3042926432",
    "https://openalex.org/W3006993985",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3128592650",
    "https://openalex.org/W2971456001",
    "https://openalex.org/W2898152330",
    "https://openalex.org/W2003341332",
    "https://openalex.org/W3091015953",
    "https://openalex.org/W2991345892",
    "https://openalex.org/W3055327921",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W2984954051",
    "https://openalex.org/W6986472653",
    "https://openalex.org/W2085965017",
    "https://openalex.org/W4221101273",
    "https://openalex.org/W6810346685",
    "https://openalex.org/W2768505892",
    "https://openalex.org/W6810427767",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4380763457",
    "https://openalex.org/W4225916372",
    "https://openalex.org/W3174906557",
    "https://openalex.org/W4225149043",
    "https://openalex.org/W4200550577",
    "https://openalex.org/W3107505554",
    "https://openalex.org/W3154596443"
  ],
  "abstract": "International audience",
  "full_text": "Received 20 December 2022, accepted 6 January 2023, date of publication 16 January 2023, date of current version 19 January 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3237082\nFusion of Satellite Images and Weather Data\nWith Transformer Networks for Downy\nMildew Disease Detection\nWILLIAM MAILLET\n , MARYAM OUHAMI\n , AND ADEL HAFIANE\n, (Member, IEEE)\nPRISME Laboratory, INSA CVL, University of Orléans, 45100 Bourges, France\nCorresponding author: Maryam Ouhami (maryam.ouhami1@insa-cvl.fr)\nThis work was supported in part by the European Consortium ERA-NET ICT-AGRI-FOOD, in part by the French National Research\nAgency (ANR) under Grant ANR-21-ICAF-0002-01, in part by the European Project MERIA VINO, and in part by ICT-AGRIFOOD\nERA-NET.\nABSTRACT Crop diseases significantly affect the quantity and quality of agricultural production. In a\ncontext where the goal of precision agriculture is to minimize or even avoid the use of pesticides, weather and\nremote sensing data with deep learning can play a pivotal role in detecting crop diseases, allowing localized\ntreatment of crops. However, combining heterogeneous data such as weather and images remains a hot topic\nand challenging task. Recent developments in transformer architectures have shown the possibility of fusion\nof data from different domains, such as text-image. The current trend is to custom only one transformer to\ncreate a multimodal fusion model. Conversely, we propose a new approach to realize data fusion using three\ntransformers. In this paper, we first solved the missing satellite images problem, by interpolating them with a\nConvLSTM model. Then, we proposed a multimodal fusion architecture that jointly learns to process visual\nand weather information. The architecture is built from three main components, a Vision Transformer and\ntwo transformer-encoders, allowing to fuse both image and weather modalities. The results of the proposed\nmethod are promising achieving an overall accuracy of 97%.\nINDEX TERMS Remote sensing, image processing, deep learning, data fusion, vegetation indices, crop\nmonitoring, agriculture.\nI. INTRODUCTION\nAgricultural chemicals such as fungicides and pesticides are\nincreasingly used to avoid and minimize disease damage.\nNonetheless, the overuse of these chemicals has become\nproblematic from an ecological and ethical point of view,\nwhile the European Union even aims to halve the use of\nchemicals by 2030 [1]. The challenge for farmers in the\ncoming decades is, therefore, to find a way to better monitor\ntheir lands in order to maximize localized treatments for\nthe infected crops instead of spraying chemicals on a large\nscale. Remote sensing is one of the popular tools in precision\nagriculture as it helps monitoring crops problems such as\ndiseases, weed infestation, lack of water, etc [2].\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Manuel Rosa-Zurera.\nSatellite imagery and deep learning have been widely used\nin crop monitoring as they provides applications in multiple\nareas: lands classification [3], yields predictions [4], wildfires\nmanagement [5], disease detection [6] and various detection\ntasks [7]. In the recent years, multiple technologies have been\napplied to accomplish these tasks from CNNs [8] to RNNs\n[9] and recently transformers [10] which are becoming very\npopular state-of-the-art models, particularly for multimodal\ndata such as images, text, etc.\nAlong with the satellite imagery, the sensor-based weather\nmonitoring devices become an essential part of precision agri-\nculture. Nowadays, most of the farmers have weather condi-\ntions monitoring facilities. Furthermore, combining weather\ndata with the satellite images is becoming more and more\nattractive to prevent crop diseases. In this paper, we propose\na fusion architecture combining the weather measurements\n5406 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 11, 2023\nW. Maillet et al.: Fusion of Satellite Images and Weather Data With Transformer Networks\nwith the satellites images in order to predict downy mildew in\nthe crops. This architecture uses the Vision Transformer (ViT)\n[11] as image features extractor, the fusion with weather data\nis performed in bottleneck mode. The architecture was trained\non images and weather data over a period of two years, and\ntested on the same type of data over two years, in order\nto evaluate the performance of the model in predicting the\npresence of downy mildew. The contributions of this paper\nare mainly:\n• A new deep learning-based approach for temporal satel-\nlite image generation to handle missing information;\n• A new multimodal ViT fusion architecture using\nthree-encoder components to integrate heterogeneous\ndata;\n• An application of disease detection and identification on\nvine crops using weather and satellite images.\nThe paper is organized as follows. Section II provides a\nreview of related works, section III describes the proposed\napproach. Section IV presents the experiments conducted in\nthis study and the results. The discussion and conclusion are\npresented in Sections V and VI respectively.\nII. RELATED WORK\nPromising approaches for detecting diseases were proposed\nin recent years using machine learning and deep learning\non weather data [12], [13]. In addition, Convolution Neural\nNetworks (CNNs) have shown interesting precision results on\nremote sensing images for disease detection [14], [15], [16],\n[17]. However, CNN-based models require a large dataset\nand the training process takes a lot of time due to the mod-\nels’ complexity. Recently, vision transformers were intro-\nduced as a better-performing solution for computer vision\nproblems.\nA. VISION TRANSFORMERS\nTransformers were first proposed by [18], and they soon rev-\nolutionized the Natural Language Processing (NLP) domain.\nThey replaced recurrent neural networks thanks to their\nshorter training time and parallelism. Yet, multiple architec-\ntures using the attention mechanism of the NLP transformers\nwere created shortly after. ViT [11] on the other hand, is the\nmost popular transformer used in image classification. It is\nknown for its simple architecture allowing to process images\nwith very few modifications and exploiting the full power\nof transformers. Transformers are becoming popular because\nthey have proven to be more efficient than traditional CNNs.\nIn fact, CNNs can recognize relationships between distant\nobjects because of the convolutional operations whereas the\nattention mechanisms of transformers allow getting a bet-\nter global understanding of the images and their intrinsic\nrelationships.\nTransformers have been used for multiple computer vision\ntasks, namely scene classification [19], [20], change detection\n[21], and image segmentation [22], [23]. ViTs were also\nused for various tasks in satellite imagery, such as change\ndetection [24] and deforestation monitoring [25]. The ViTs\nresults were convincing, they even outperformed the classical\nconvolutional architectures.\nB. MULTIMODAL FUSION\nSince the agricultural field provides a rich variety of data\nfrom multiple sources, it seems more judicious to merge the\nvariety of data in one model to achieve better performance.\nIn disease detection, multimodal fusion is still an ongoing\narea of study [26], [27]. Multimodal fusion based on machine\nlearning is able to jointly learn to process different modal-\nities. Depending on the level of data abstraction, different\nfusion architectures are possible, such as input data level\nfusion, feature fusion allowing data integration using feature\nvectors, etc. We can divide deep learning approaches for\nmultimodal fusion into two categories, those based on CNNs\nand transformer-based architectures.\n1) CNN BASED MULTIMODAL FUSION\nUsing multiple modalities to learn is one of the most impor-\ntant topics in machine learning as it is the closest to human\nlearning. Various multimodal fusion techniques presented in\nthe literature were based on CNNs to extract feature maps\nfrom the input images. Those techniques have different appli-\ncations, namely in the medical field [28], in mechanics [29]\nand in agriculture specifically data fusion for yield prediction\n[30], land monitoring [31], crop identification [32] and dis-\nease detection [33].\n2) MULTIMODAL TRANSFORMERS ARCHITECTURES\nMultimodal learning with transformers has been tested in\nmultiple areas, especially in the audiovisual field to join\nvideo, language and audio features [34], [35], [36], [37] but\nalso in deepfake detection [38], medical imagery synthesis\n[39], etc. Most of these proposed models extract embed-\ndings from the modalities without transformers to make the\nfusion in a single custom transformer. In fact, they rarely\nuse multiple transformers to extract features. [40] proposed\na multimodal architecture using a custom bottleneck trans-\nformer to combine features. Nonetheless, this bottleneck\narchitecture is within the transformer and not as a single\ntransformer. In the satellite imagery areas, [41] proposed a\nmultimodal fusion architecture using multiple image sources.\nThe modalities features are extracted using LSTM cells and a\nmodified ViT transformer. [42] proposed a multimodal archi-\ntecture for object detection, using ViT as a feature extrac-\ntor. Recently, [43] also proposed modifications of the ViT\ntransformer allowing to use LIDAR data along with multi-\nspectral images. So far, very little attention has been paid\nto the fusion of satellite images with weather data for crop\ndisease detection.\nIII. METHODOLOGY\nIn this section we describe the proposed method that com-\nbines Sentinel-2 satellite images as 2D information, with\nground weather data as 1D information. First, we detail\nVOLUME 11, 2023 5407\nW. Maillet et al.: Fusion of Satellite Images and Weather Data With Transformer Networks\nFIGURE 1. An overview of the proposed approach, from dataset\nprocessing to predictions.\nthe process of increasing the temporal frequency of satellite\nimages by interpolation and deep learning procedures. Sec-\nond, we describe the fusion architecture between the two het-\nerogeneous data. An overview of the entire process flowchart\nis depicted in Fig. 1.\nA. SATELLITE IMAGES GENERATION\nThe Sentinel-2 satellite provides multispectral images of a\ngiven location, usually on a 5-day cycle. In cloudy conditions,\nthe temporal resolution between exploitable images can be\nmuch lower. As the weather data is daily, it is essential to\nhave one image per day to enable a consistent fusion of 2D/1D\ninformation. To overcome this limitation, we propose a data\ngeneration method that combines linear interpolation with\nnoise injection and Convolutional Long Short-Term Memory\n(ConvLSTM). The goal is to train a ConvLSTM model to\noutput the missing intermediate images between two dates.\nWe believe that the ConvLSTM ability to capture temporal\nand spatial features would give more interesting predictions,\ncompared to only interpolation based on an analytical model.\nThe proposed method is split into two parts: first, we generate\nlinearly interpolated images with gaussian noise, and then we\nuse those images to train a ConvLSTM model outputting the\nfinal images that we use for our fusion model (see Fig. 2).\n1) TEMPORAL INTERPOLATION OF IMAGES\nTo generate the training images, we use linear interpolation\nand gaussian noise. Let In and In+k be two real images\n(matrices) captured k days apart. To generate all the missing\nimages from day n+1 to day n+k −1, we use the Equation 1,\ni is the day where an intermediate image is needed (i ∈]n;\nn+k[). Once the image is computed, we add gaussian noise to\nthe image In+i to create another image IG,(n+i) including small\nvariations as indicated in Equation 2. This noise is added to\nintegrate randomness to the linear interpolation and avoid the\nConvLSTM model to overfit the linear interpolation. Note\nthat Equation 1 and Equation 2 are element-wise operations.\nIn+i = In × (k − i) + In+k × k\nk , i ∈]n; n + k[ (1)\nIG,(n+i) = In+i + η(µ, σ) (2)\n2) ConvLSTM IMAGE GENERATION\nThe next phase consists in training the ConvLSTM model\non the interpolated and real images, to generate a final\ndataset of daily artificial images. The ConvLSTM model is\ncomposed of four consecutive blocks, each one containing\na ConvLSTM layer, a BatchNorm layer, and a leakyReLU\nlayer. The model takes as input a set of daily ordered images:\n{Dn−3, Dn−2, Dn−1} and outputs the next image, Dn. We train\nthe model using the Adam optimizer and the Root Mean\nSquare Error (RMSE) loss function. Once the model is\ntrained, it is used to generate all the missing images and create\na dataset for subsequent use.\nit = σ(WDi ∗ Dt + Whi ∗ ht−1 + Wci ◦ Ct−1 + bi)\nft = σ(WDf ∗ Dt + Whf ∗ ht−1 + Wcf ◦ Ct−1 + bf )\nCt = ft ◦ Ct−1 + it ◦ tanh(WDc ∗ Dt + Whc ∗ ht−1 + bc)\not = σ(WDo ∗ Dt + Who ∗ ht−1 + Wco ◦ Ct + bo)\nht = ot ◦ tanh Ct (3)\nEquation 3 illustrates the mathematical aspect of the Con-\nvLSTM model where, WDi, Whi, WDf , Whf , WDc, Whc, WDo,\nWho, Wci, Wcf and Wco denote the weights; bf , bi, bc and bo\nare the biases. The operation ◦ denotes element-wise product,\nDt denotes the current inputs and ht−1 denotes the output of\nLSTM unit at the previous moment, and σ as well as tanh(.)\nare nonlinear activation functions.\nThe ConvLSTM provides intermediate images to fill the\ngap of the missing information. Using deep learning to create\nthese images will enhance basic linear interpolation since the\nConvLSTM can extract both temporal and spatial features\nwithin images.\nB. 2D-1D FUSION MODEL\nTo combine satellite imagery with weather data, we propose\na fusion model based on transformers-encoders. Its archi-\ntecture can be split into three main components: the ViT,\nthe weather attention encoder, and a bottleneck transformer-\nencoder. The proposed architecture uses transformers to gen-\nerate embeddings from the data, and then these embeddings\nare combined through the fusion bottleneck encoder. The\noutput layer indicates whether crops are infected or not with\na specific disease (here the downy mildew). Fig. 1 illustrates\nthe proposed approach.\n5408 VOLUME 11, 2023\nW. Maillet et al.: Fusion of Satellite Images and Weather Data With Transformer Networks\nFIGURE 2. The interpolation process: (a) The satellite image set with temporal gaps (one day here) (b) being interpolated and noised with Gaussian noise\nto create an artificial dataset, which is then used (c) to train a ConvLSTM model made of multiple ConvLSTM layers, and (d) the model eventually predicts\ndaily images.\n1) VISION TRANSFORMER (ViT)\nThe ViT introduced by [11] is based on a transformer-\nencoder. It becomes a popular architecture for its simplicity,\nand efficiency in terms of computation and performance. The\nViT splits images in small patches ordered as a sequence and\nuses only the encoder part of the traditional encoder-decoder\ntransformer architecture, which makes it light. The trans-\nformer learns by measuring the relationship between image\npatches. This relationship can be learned by providing atten-\ntion in the network. It is also possible to extract attention\nmaps, which are useful for understanding the output of the\nmodel and the parts of the image on which the focus is. This\nis a key concept for anomaly detection, as we could interpret\ncertain areas of the attention maps as infected crops.\nFIGURE 3. The ViT Encoder Architecture.\nThe attention function is computed based on a set of\nqueries simultaneously [18], packed together into a matrix Q,\nwhere K and V denote the keys and values of dimension dk ,\nas in Equation 4.\nAttention(Q, K, V )\n= softmax QKT\n√dk\nV (4)\nMultiHeadAtt(Q, K, V )\n= Concat(Head1, Head2, . . . ,Headh)W o,\nHeadi = Attention(QW Q\ni , KW K\ni , VW V\ni ) (5)\nMulti-Head Attention is basically a linear projection of the\nqueries, keys, and values. VIT is based on three main com-\nponents, patch embedding Equation 6, feature extraction via\nstacked transformer encoders Equation 7 and classification\nhead Equation 8 and Equation 9.\nz0 = [xclass; x1\np E; x2\np E; . . .; xN\np E]\n+ Epos, E ∈ R(P2.C)×D, Epos ∈ R(N+1)×D (6)\nz′\nl = MSA(LN(zl−1)) + zl−1, l = 1 . . .L (7)\nzl = MLP(LN(z′\nl)) + z′\nl, l = 1 . . .L (8)\ny = LN(z0\nL) (9)\nIn the proposed architecture, the ViT takes inputs IV =\n{x1, x2, x3}, where xi has a dimension of (3, 224, 224) which\nrepresents a vegetation index encoded with RGB image.\nThree different RGB vegetation indices, resulting in a tensor\nof size (9, 224, 224). It outputs extracted features OV of\ndimension (14, 14). Fig. 3 illustrates the ViT encoder archi-\ntecture used.\n2) WEATHER ATTENTION ENCODER\nThe Weather Attention Encoder is a simple Transformer\nEncoder that has the role of embedding the weather data.\nIt takes as inputs IW = {w1, . . . ,wn} a vector of weather\nfeatures, such as rain duration and quantity, potential evapo-\nration, sunlight amount, etc. First, the input IW is expanded to\na high dimension using linear and dropout layers Equation 10.\nThen, this expanded vector is passed to the first encoder block\n(out of 12 encoder blocks) Equation 11. After these blocks,\nthe output representation OW is produced using a network\ncomposed of linear, flatten, and dropout layers Equation 12.\nWe call this output OW the weather embeddings of dimension\n(14, 14).\nAW = Linear(Dropout(IW )) (10)\nA′\nW = EncoderBlock(AW ) (11)\nOW = Linear(Flatten(Dropout(A′\nW ))) (12)\nThe encoder uses the architecture proposed in [18]. The\nencoder block can be decomposed into two sub-parts. The\nfirst part consists of multi-head attention and residual con-\nnections on the input vector Ie to produce an intermediate\nVOLUME 11, 2023 5409\nW. Maillet et al.: Fusion of Satellite Images and Weather Data With Transformer Networks\nfeature He as indicated in Equation 13, on which layer nor-\nmalization is applied (LN). The same process is made using\na feed-forward network in the second sub-part of the encoder\nblock described in Equation 14.\nHe = LN(Ie + Dropout(MultiHeadAtt(Ie))) (13)\nOe = LN(He + Dropout(FFN(He))) (14)\n3) FUSION BOTTLENECK ENCODER\nThe Fusion Bottleneck Encoder takes as input IF , the concate-\nnation of the two previous embedded vectors that represent\nthe visual and weather information, as described in Equa-\ntion 15. The concatenation ends up being a (28, 14) tensor,\nsince the OV and OW are of dimension (14, 14) each. This\nconcatenated vector IF is then passed to the three parts of\nthe encoder, using the same process as the weather attention\nmodel. First, the vector is expanded Equation 16, then passed\nto the encoder Equation 17. The resulted A′\nF is then mapped to\na vector using a feed-forward network, made of linear, flatten\nand dropout layers Equation 18. The last layer OF consists of\n2 outputs to predict whether downy mildew disease exists or\nnot (yes or no output).\nIF = OW ⊕ OV (15)\nAF = Linear(Dropout(IF )) (16)\nA′\nF = Encoder(AF ) (17)\nOF = Linear(Flatten(Dropout(A′\nF ))) (18)\nFIGURE 4. The Fusion Bottleneck Architecture.\nIV. EXPERIMENTS AND RESULTS\nThis section presents the different experiments conducted in\nthis research work. It includes data collection, implementa-\ntion details, evaluation results of the image generation, and\nthe fusion method.\nA. DATA COLLECTION\nTwo types of data were collected, satellite images and weather\nconditions. To evaluate the presence of downy mildew within\nthe change in weather conditions for the same periods in each\nyear, we selected images and weather data of June, July and\nAugust for the period 2018 until 2021 from the plots of Lycée\nAgricole d’Amboise in Centre Val de Loire region, France.\n1) IMAGE COLLECTION\nRemote sensing is an important data source for crop monitor-\ning. The images of the vine crops from the studied site were\ncollected using the Sentinel-2 Level-2A product. Sentinel-2 is\na satellite mission of the European earth surveillance program\nCopernicus. It provides multispectral images with different\nspatial resolutions 10, 20, and 60 m [44]. The orthoimages\nwere provided with atmospheric correction which is known to\nimprove the images for subsequent use. In addition, Level-2A\nimages have useful features such as cloud detection.\nSentinel-2 images provide rich sources of data related to\nvegetation. The spectral bands allow the calculation of veg-\netation indices that are useful for measuring crop conditions\nsuch as vigor, biomass, chlorophyll content, and disease. The\nDifference Vegetation Index (NDVI) is a widely used vegeta-\ntion index to measure the health status of vegetation, based\non visible and near-infrared light reflected by vegetation.\nHowever, other types of indices can be useful for extracting\nmore relevant information on vegetation or land condition.\nThe Normalized Difference Chlorophyll Index (NDCI) intro-\nduced by [45] is an index that describes the chlorophyll\nconcentration, designed for water regions. The Normalized\nDifference Moisture Index (NDMI) measures the moisture\nlevels of the crops and helps monitor droughts.\nIn this study we used the three indices individually and in\ncombination, all extracted from the SentinelHub API coded\non RGB images. Table 1 lists the Sentinel-2 bands of the\nthree indices with their spatial resolution. Fig. 5 shows the\nindices NDVI, NDCI, and NDMI extracted from the studied\nvineyards zone.\nTABLE 1. Indices used in the study and their characteristics.\n2) IMAGES PROCESSING\nEven if the images are requested with a maximum cloud per-\ncentage of 10%, there are still many images with large parts\nof clouds that slip through the filtering process. We deleted\nsome of these remaining images using a cloud detection\nalgorithm with pixel values and we cleaned the rest by hand.\nThe satellite images cover a zone of four different vine plots.\nSince this zone contains buildings, trees, etc. We cropped\nthe images to only keep the appropriate vine parcels (see\nFig. 5-a). In total, The dataset is composed of 1472 images\n(including the generated ones). We have 92 images per year\nper plot from June to August in four plots over four years\nfrom 2018 to 2021, where the numbers of real images per year\nare 38, 33, 40, and 29 respectively, more details are presented\nin Table 2. Each image is labeled positive or negative to\nmildew depending on the ground truth. For our dataset, July\nand August of 2018 and 2021 were labeled as positive, and\nthe rest of the dataset was labeled negative).\n3) WEATHER DATA\nResearch indicates that downy mildew infections are due\nnecessary to an accumulation of days with favorable weather\n5410 VOLUME 11, 2023\nW. Maillet et al.: Fusion of Satellite Images and Weather Data With Transformer Networks\nFIGURE 5. Sample images for each vegetation index: (a) True Colors (studied crops outlined in red), (b) NDCI, (c) NDMI, (d) NDVI.\nTABLE 2. Dataset partitions.\nconditions [46]. In France, the downy mildew of the vine\ngenerally starts to appear at the end of spring and begins to\ndisappear with the fall of the leaves. In spring, after the matu-\nration of vine mildew ascospores, they germinate in rainwater\nfrom an average temperature of 11 ◦C and cause primary\ncontamination. After an incubation period of 10 to 20 days\ndepending on the temperature, the secondary contaminations\nphase arrives in the presence of rain [46], [47].\nThe weather data used in this study was collected from the\nlocal meteorological station implemented on the studied site\nwhich tracks daily weather conditions. The data contains pre-\ncipitation (in mm), rainfall variability, rainfall duration, mini-\nmum, average, and maximum temperature (in ◦C), humidity,\npotential evaporation, and on-site sun exposure (min, max,\nmean). These 11 features form a vector of shape [1], [11]\nwhich is passed to the weather-dedicated transformer.\nB. IMPLEMENTATION DETAILS AND SETUP\nThe experiments were performed on a computer with\nan Intel®Xeon(R) W-2123 CPU and an Nvidia GeForce\nGTX 1080 Ti, on Ubuntu 20.04 LTS. The model has been\ntrained using the Adam optimizer with a learning rate of\n1e−6. We used a cosine warmup learning rate scheduler from\nthe ‘‘transformers’’ library [48], with a warmup of 100 epochs\nof 600 total epochs. The model early stopped at the\n120/130th epoch.\nThe used hyperparameters for the ViT are 12 encoder-\nblocks layers with 8 heads each, a patch size of 16 pixels,\nan embedding size of dimension 768, and images of size\n224 × 224. For the other encoders of the model, we also\nuse 12 encoder-blocks layers using 8 heads each and a\nfeed-forward network size of 128. The model embedding\ndimension is 64.\nThe next section presents the evaluation results of the\ntemporal image generation described in section III-A, and\nthen the performance of the 2D-1D fusion architecture (see\nsection III-B) for downy mildew detection. We carried out the\nexperimental study with the variation of several parameters of\nthe proposed methods.\nC. EVALUATION OF THE IMAGE GENERATION METHOD\nThe performance was evaluated with the one-leave-out cross-\nvalidation procedure. Each fold contains images of one year,\nso we have four folds for these experiments. The ConvL-\nSTM network was trained on three folds and tested on one\nfold. Note that only the real images are involved in the test.\nThree previous images were used to predict the actual image.\nThe RMSE metric measures the error between the predicted\nimage and the real one. Table 3 shows the results obtained\nfor each year, where the values are an average score on the\nset of images predicted by ConvLSTM trained with different\nnoise values added to the interpolated images Equation 2.\nThe noise tested is of Gaussian type with mean µ = 0 and\ndifferent standard deviation σ, varying from 0.04 to 0.2, with\na step of 0.02. We can observe in Table 3 that the RMSE is\nof the range of 10 −3 for all indices, which represents a low\nprediction error. The NDCI error is the smallest, followed\nby the NDVI and NDMI. Fig. 6 shows the RMSE error for\nthe three indices as a function of the noise added to the\ninterpolated images. Similarly, the RMSE error remains low\n(range of 10 −3) for the different values of the noise variance.\nWe can notice some spikes in the curves, NDVI and NDMI.\nThe RMSE of NDCI is more stable and smaller than the other\ntwo indices. Nevertheless, small noise values seem to work\nbetter for all indices.\nTABLE 3. Average of RMSE value for each indice and for each years over\nsigma values.\nVOLUME 11, 2023 5411\nW. Maillet et al.: Fusion of Satellite Images and Weather Data With Transformer Networks\nFIGURE 6. RMSE error according to different standard deviation of the\nnoise added to the interpolated images of NDCI, NDVI and NDMI indices.\nFIGURE 7. Three processed (resize, normalized, etc) vegetation indices\nand ViT attention map: (a) NDMI, (b) NDCI, (c) NDVI, (d) Attention map.\nD. EVALUATION OF THE 2D-1D FUSION ARCHITECTURE\nThe aim is to evaluate the performance of the proposed fusion\nmethod with different setups. We investigated the influence\nof combining different vegetation indices, ablation of some\ncomponents, pruning of connections and changes in model\nlayers. For this purpose, we used real images and those\ngenerated by the proposed method, as well as weather data,\nfor training and testing. Data from 2018 (presence of downy\nmildew) and 2019 (no downy mildew) were considered for\ntraining. For the test, we used data from 2020 (no downy\nmildew), 2021 (downy mildew). The performance measure-\nment was performed by the well-known accuracy measure\nand the F1 score.\n1) MODEL COMPARISON\nTransformers were primarily designed to process sequential\ninput data, encoding this information is a native capability\nof the transformer. The critical part of our architecture is\nthe encoding of the generated images, which needs to be\nconsistent in embedding space with the weather data. Hence,\nin addition to ViT we tested two different popular archi-\ntectures to compare their capabilities to embed the gener-\nated image information into the fusion module. To assess\nthe performances of different models, we replaced the ViT\nmodel by the Microsoft CvT model [49] and a classical Fully\nConvolutional Network. The CvT model is pre-trained on\nImageNet-1k, and the FCN model uses a ResNet50 back-\nbone, and it is pre-trained with a subset of COCO-train2017\ncontaining 20 categories present in the Pascal VOC dataset.\nAll models have been adapted to the inputs data, where the\nfirst embedding is changed to use 9 channels (NDCI, NDVI,\nNDMI) instead of 3 and the classifiers of the model have been\nreplaced with a simple output network to suit the attention\nmaps used with the ViT model. The current results presented\nin Table 4 show that the ViT model seems the best performing\namong the other options, The analysis of the results shows\nthat the ViT model provides the best performance among\nthe other models, indicating that it is better suited for our\nproblem.\nTABLE 4. Comparison of the image encoding models in the fusion\narchitecture. For each evaluation, the ‘‘Vision Transformer’’ module was\nreplaced by the ViT, CvT or FCN model.\n2) VEGETATION INDICES COMBINATIONS\nMultiple vegetation indices combinations were tested, and the\nbest accuracy and F1-Score were obtained with the combi-\nnation of all three indices. As we can observe in Table 5,\nNDCI might be the one playing the most important role as\nit produces a very good accuracy on its own and performs\nwell with both NDVI and NDMI. Even though this vegetation\nindex seems to be the most important, we cannot neglect\nothers as the combination of all three outperforms by more\nthan 2%.\nFig. 7 shows vegetation indices images used as an input for\nthe ViT part of the model (see section III-B1). We can observe\nthat the attention map looks similar to the vegetation indices\nimages, as it highlights the shape of some parts of the image\nthat may be related to downy mildew. This also indicates that\nthe model seems to learn to recognize patterns in vegetation\nindices and that it’s not highlighting uninteresting parts of the\nimage.\n3) ABLATION STUDY\nTo evaluate whether the network architecture is biased by the\nweather features or the image features, we tested the network\nwith an ablation method to cut off parts of the network. This\nconsists in cutting one of the parts of the network, either the\nViT encoder or the Weather Attention Encoder. We replace\nthe concatenation of weather and image features of dimension\n(28, 14) by a (14, 14) features map as if the other part of\nthe model did not exist, and we adapt the Fusion Bottleneck\n5412 VOLUME 11, 2023\nW. Maillet et al.: Fusion of Satellite Images and Weather Data With Transformer Networks\nTABLE 5. Evaluation of the fusion method with different combinations of vegetation indices.\nTransformer to use these new dimensions as input by tweak-\ning the input parameters. We then train this new model using\nthe same process as for the global model, ignoring the missing\nparts.\nTABLE 6. Results of the ablation study of the fusion model.\nAfter the training, we obtained the results described in\nTable 6. When cutting off the weather data, the model\nachieves 48.1% accuracy, which is fairly bad, and when\ncutting the images part, the accuracy goes up to 66.2%; which\nis better but still not good. This study shows that the model\ndoes not appear to be biased by weather or images and that it\nneeds both inputs for correct learning.\n4) PRUNING STUDY\nNetwork pruning has become common to study models, as it\ncan enlighten a potential overparameterization. We tested\npruning by taking the parameters that give the best accuracy\n(97% accuracy and 97.5% F1-Score), and we prune parame-\nters of either part of the network (only the Weather Attention\nEncoder for instance) or the whole network.\nTABLE 7. Results of the fusion model pruning.\nTable 7 presents the accuracy of the model when pruning\n1%, 5%, 7.5% and 10% of each model part. We can observe\nthat the ViT is more resilient to these prunings, and it can be\nexplained by its number of parameters; the ViT has approx-\nimately 86 million parameters which makes it robust, while\nthe other encoders of the model have almost three times fewer\nparameters. As we saw in the ablation study, keeping only the\nweather gives better results than keeping only the images, and\nhere we can see that removing parts of the ViT does not affect\nthe accuracy by a lot. This means our model gives the best\nresults when using images with the weather, but the weather\nmodule is still the most important part of the network.\n5) ENCODER LAYERS\nThe number of layers in the encoder has an impact on the\naccuracy of the model, as shown in Figure 8. The perfor-\nmances remain correct for a number of layers between 6 and\n16. Otherwise, we observe a degradation in performances\nbeyond these values. The optimal number of layers providing\nthe best performances in these experiments is 12.\nFIGURE 8. Model precision depending on the number of encoder layers.\nV. DISCUSSIONS\nThe first hypothesis of this research states that a deep neural\nnetwork trained with data obtained by linear interpolation will\nbe able to generate artificial images intermediate between\ntwo temporally distant acquisitions. Thus allowing to match\nand merge of heterogeneous data acquired with different\nfrequencies. In this research, it was found that interpolation\ncombined with a deep network (ConvLSTM) provides good\nresults to fill the information gap in satellite images. The error\nis small when the generated images are compared to the real\nones. Another important result is that perturbing the linear\ninterpolation with Gaussian noise helps to improve the train-\ning and convergence of the ConvLSTM network. Therefore,\nwe can assume that this method could be an interesting tool\nto complete the data and allow better performance of deep\nnetworks.\nEven though ConvLSTM achieves good RMSE, further\nresearch should be undertaken to study the effectiveness of\nthis model using more data. Real satellite images are not\nvery frequent and there is no guarantee that the intermediate\nimages ( interpolated images) are a good representation of\nthe ground truth. Nevertheless, given the results obtained,\nVOLUME 11, 2023 5413\nW. Maillet et al.: Fusion of Satellite Images and Weather Data With Transformer Networks\nwe can expect that the images generated will be close to the\nreal images, and this will not significantly affect the machine\nlearning process. On the other hand, the artificial generation\nof intermediate images could also induce noise in the images\nor pixels invisible to the naked eye, which could bias the\nnetwork. This is certainly not the case because we can observe\nin Table 6 that when the ViT is cut, the model still seems able\nto learn, even if it is not very accurate.\nThe second working hypothesis consisted of showing the\neffectiveness of transformers for the implementation of infor-\nmation fusion with the aim of crop disease detection. The\ndifficulty lies in the heterogeneity of the data and their dimen-\nsions, as well as the difference in the acquisition frequencies.\nThe proposed method was able to address these problems.\nThe obtained scores showed that the fusion of multispectral\nsatellite images (2D) and weather data (1D) offers better\nperformances for the detection and identification of downy\nmildew. Indeed, for the test set, our model outputs predictions\nwith a 97% accuracy and a 97.5% F1-Score. These results\nseem promising, and the qualitative analysis of attention over\nimages seems to confirm that the network learned how to\nrecognize downy mildew on vegetation indices of satellite\nimages.\nThe weather has an impact on the development of downy\nmildew, the vegetation indices include indications of the\nhealth status of a crop. The results showed that the combi-\nnation of the two parts gives better scores than a single type\nof data. Comparing the results of ablation experiments shows\nthat each module (weather data, image), affects the fusion\nresults, as depicted in Table 6. From the results obtained,\nwe can assume that each module provides complementary\ninformation which increases significantly the performance.\nIt was also found that using the three vegetation indices\n(NDVI, NDCI, NDMI) is more effective than using only\none or two. Nevertheless, it seems that NDCI has a few\nmore discriminating elements compared to the other two (see\nTable 5).\nThe proposed fusion model performs well, but there is\nroom for improvement in several aspects of the methodology.\nFor instance, improvements could be made to the fusion\nmodel. It seems that the model gains a little bit in accuracy\nwhen a small percentage of its parameters are cut, which\npossibly indicates redundancies in the parameters. We believe\nit is possible to modify the Weather Attention Encoder and\nFusion Bottleneck Encoder to enhance them for this task,\nusing a custom loss function or small modifications in the\narchitecture.\nVI. CONCLUSION\nIn this research work, the objective was to take advantage of\nthe complementarity of heterogeneous data to detect downy\nmildew disease in vine plots. For this purpose, we have\nproposed a new framework where the first part consists\nin generating intermediate satellite images, to create daily\nimages and pairing them with the weather data. The second\npart consists of a new data fusion architecture based on\ntransformer networks that combine weather data and satellite\nimages. We used two transformers (an encoder and ViT) to\nextract modalities features/embeddings (weather and satellite\nimages), and a third bottleneck transformer-encoder outputs\nthe prediction based on the embeddings/features. We achieve\npromising results which validates the hypothesis that fusion\nusing multiple transformers instead of a single custom one\nis possible and can achieve good results. We believe these\nresults are interesting for agriculture challenges as they could\nlead to better crop monitoring. In addition, the proposed\narchitecture can be used for other application domains.\nACKNOWLEDGMENT\nThe authors would like to thank the Institut Francais de\nla Vigne (IFV) for providing ground truth information and\nLycée Agricole d’Amboise for providing studied plots.\nREFERENCES\n[1] Farm to Fork Targets Progress. Accessed: Jul. 18, 2022. [Online].\nAvailable: https://food.ec.europa.eu/plants/pesticides/sustainable-use-\npesticides/farm-fork-targets-progress_en\n[2] M. Weiss, F. Jacob, and G. Duveiller, ‘‘Remote sensing for agricultural\napplications: A meta-review,’’ Remote Sens. Environ., vol. 236, Jan. 2020,\nArt. no. 111402.\n[3] M. Persson, E. Lindberg, and H. Reese, ‘‘Tree species classification with\nmulti-temporal Sentinel-2 data,’’ Remote Sens., vol. 10, no. 11, p. 1794,\nNov. 2018.\n[4] M. L. Hunt, G. A. Blackburn, L. Carrasco, J. W. Redhead, and\nC. S. Rowland, ‘‘High resolution wheat yield mapping using Sentinel-2,’’\nRemote Sens. Environ., vol. 233, Nov. 2019, Art. no. 111410.\n[5] E. B. Castillo, E. T. Cayo, C. de Almeida, R. S. López, N. R. Briceño,\nj. S. López, M. B. Gurbillón, M. Oliva, and R. Espinoza-Villar, ‘‘Monitor-\ning wildfires in the northeastern peruvian Amazon using Landsat-8 and\nSentinel-2 imagery in the GEE platform,’’ ISPRS Int. J. Geo-Inf., vol. 9,\nno. 10, p. 564, Sep. 2020.\n[6] H. Santoso, T. Gunawan, R. H. Jatmiko, W. Darmosarkoro, and\nB. Minasny, ‘‘Mapping and identifying basal stem rot disease in oil palms\nin north Sumatra with QuickBird imagery,’’ Precis. Agricult., vol. 12, no. 2,\npp. 233–248, Apr. 2011.\n[7] K. Themistocleous, C. Papoutsa, S. Michaelides, and D. Hadjimitsis,\n‘‘Investigating detection of floating plastic litter from space using Sentinel-\n2 imagery,’’ Remote Sens., vol. 12, no. 16, p. 2648, Aug. 2020.\n[8] J. Rosentreter, R. Hagensieker, and B. Waske, ‘‘Towards large-scale map-\nping of local climate zones using multitemporal sentinel 2 data and con-\nvolutional neural networks,’’ Remote Sens. Environ., vol. 237, Feb. 2020,\nArt. no. 111472.\n[9] M. Papadomanolaki, S. Verma, M. Vakalopoulou, S. Gupta, and\nK. Karantzalos, ‘‘Detecting urban changes with recurrent neural networks\nfrom multitemporal Sentinel-2 data,’’ in Proc. IEEE Int. Geosci. Remote\nSens. Symp., Jul. 2019, pp. 214–217.\n[10] Y . Yuan, L. Lin, Q. Liu, R. Hang, and Z.-G. Zhou, ‘‘SITS-former: A\npre-trained spatio-spectral-temporal representation model for Sentinel-2\ntime series classification,’’ Int. J. Appl. Earth Observ. Geoinf., vol. 106,\nFeb. 2022, Art. no. 102651.\n[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16×16 words: Trans-\nformers for image recognition at scale,’’ 2020, arXiv:2010.11929.\n[12] A. Khattab, S. E. Habib, H. Ismail, S. Zayan, Y . Fahmy, and\nM. M. Khairy, ‘‘An IoT-based cognitive monitoring system for early\nplant disease forecast,’’ Comput. Electron. Agricult., vol. 166, Nov. 2019,\nArt. no. 105028. [Online]. Available: https://linkinghub.elsevier.\ncom/retrieve/pii/S0168169919311949\n[13] S. Trilles, J. Torres-Sospedra, Ó. Belmonte, F. J. Zarazaga-Soria,\nA. González-Pérez, and J. Huerta, ‘‘Development of an open sensorized\nplatform in a smart agriculture context: A vineyard support system for\nmonitoring mildew disease,’’ Sustain. Comput., Informat. Syst., vol. 28,\nDec. 2020, Art. no. 100309.\n5414 VOLUME 11, 2023\nW. Maillet et al.: Fusion of Satellite Images and Weather Data With Transformer Networks\n[14] M. Kerkech, A. Hafiane, and R. Canals, ‘‘VddNet: Vine\ndisease detection network based on multispectral images\nand depth map,’’ Remote Sens., vol. 12, no. 20, p. 3305,\nOct. 2020. [Online]. Available: https://www.mdpi.com/2072-4292/12/20/\n3305\n[15] M. Maimaitijiang, V . Sagan, P. Sidike, S. Hartling, F. Esposito, and\nF. B. Fritschi, ‘‘Soybean yield prediction from UA V using multimodal data\nfusion and deep learning,’’ Remote Sens. Environ., vol. 237, Feb. 2020,\nArt. no. 111599.\n[16] J. Abdulridha, Y . Ampatzidis, P. Roberts, and S. C. Kakarla, ‘‘Detecting\npowdery mildew disease in squash at different stages using UA V-based\nhyperspectral imaging and artificial intelligence,’’ Biosyst. Eng., vol. 197,\npp. 135–148, Sep. 2020.\n[17] T. Poblete, C. Camino, P. S. A. Beck, A. Hornero, T. Kattenborn,\nM. Saponari, D. Boscia, J. A. Navas-Cortes, and P. J. Zarco-Tejada,\n‘‘Detection of Xylella fastidiosa infection symptoms with airborne mul-\ntispectral and thermal imagery: Assessing bandset reduction performance\nfrom hyperspectral analysis,’’ ISPRS J. Photogramm. Remote Sens.,\nvol. 162, pp. 27–40, Apr. 2020.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. U. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’\nin Proc. Adv. Neural Inf. Process. Syst., vol. 30, I. Guyon, U. V .\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and\nR. Garnett, Eds. Red Hook, NY , USA: Curran Associates, 2017, pp. 1–11.\n[Online]. Available: https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[19] Y . Bazi, L. Bashmal, M. M. A. Rahhal, R. A. Dayil, and N. A. Ajlan,\n‘‘Vision transformers for remote sensing image classification,’’ Remote\nSens., vol. 13, no. 3, p. 516, Feb. 2021.\n[20] J. Zhang, H. Zhao, and J. Li, ‘‘TRS: Transformers for remote sens-\ning scene classification,’’ Remote Sens., vol. 13, no. 20, p. 4143,\nOct. 2021.\n[21] H. Chen, Z. Qi, and Z. Shi, ‘‘Remote sensing image change detection\nwith transformers,’’ IEEE Trans. Geosci. Remote Sens., vol. 60, 2022,\nArt. no. 5607514.\n[22] X. Xu, Z. Feng, C. Cao, M. Li, J. Wu, Z. Wu, Y . Shang, and S. Ye,\n‘‘An improved swin transformer-based model for remote sensing object\ndetection and instance segmentation,’’ Remote Sens., vol. 13, no. 23,\np. 4779, Nov. 2021.\n[23] Z. Xu, W. Zhang, T. Zhang, Z. Yang, and J. Li, ‘‘Efficient transformer\nfor remote sensing image segmentation,’’ Remote Sens., vol. 13, no. 18,\np. 3585, Sep. 2021.\n[24] J. Horvath, S. Baireddy, H. Hao, D. M. Montserrat, and E. J. Delp, ‘‘Manip-\nulation detection in satellite images using vision transformer,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW),\nJun. 2021, pp. 1032–1041.\n[25] M. Kaselimi, A. V oulodimos, I. Daskalopoulos, N. Doulamis, and\nA. Doulamis, ‘‘A vision transformer model for convolution-free mul-\ntilabel classification of satellite imagery in deforestation monitoring,’’\nIEEE Trans. Neural Netw. Learn. Syst., early access, Feb. 2, 2022, doi:\n10.1109/TNNLS.2022.3144791.\n[26] P. Ghamisi, ‘‘Multisource and multitemporal data fusion in remote sensing:\nA comprehensive review of the state of the art,’’ IEEE Geosci. Remote Sens.\nMag., vol. 7, no. 1, pp. 6–39, Mar. 2019.\n[27] M. Ouhami, A. Hafiane, Y . Es-Saady, M. El Hajji, and R. Canals, ‘‘Com-\nputer vision, IoT and data fusion for crop disease detection using machine\nlearning: A survey and ongoing research,’’ Remote Sens., vol. 13, no. 13,\np. 2486, Jun. 2021. [Online]. Available: https://www.mdpi.com/2072-\n4292/13/13/2486\n[28] Z. Liu, Y . Cao, Y . Li, X. Xiao, Q. Qiu, M. Yang, Y . Zhao, and\nL. Cui, ‘‘Automatic diagnosis of fungal keratitis using data aug-\nmentation and image fusion with deep convolutional neural net-\nwork,’’ Comput. Methods Programs Biomed., vol. 187, Apr. 2020,\nArt. no. 105019.\n[29] L. Jing, T. Wang, M. Zhao, and P. Wang, ‘‘An adaptive multi-sensor\ndata fusion method based on deep convolutional neural networks for\nfault diagnosis of planetary gearbox,’’ Sensors, vol. 17, no. 2, p. 414,\nFeb. 2017.\n[30] Z. Chu and J. Yu, ‘‘An end-to-end model for Rice yield prediction using\ndeep learning fusion,’’ Comput. Electron. Agricult., vol. 174, Jul. 2020,\nArt. no. 105471.\n[31] Z. Song, Z. Zhang, S. Yang, D. Ding, and J. Ning, ‘‘Identifying sunflower\nlodging based on image fusion and deep semantic segmentation with\nUA V remote sensing imaging,’’ Comput. Electron. Agricult., vol. 179,\nDec. 2020, Art. no. 105812.\n[32] C. Pelletier, G. I. Webb, and F. Petitjean, ‘‘Temporal convolutional neural\nnetwork for the classification of satellite image time series,’’ Remote Sens.,\nvol. 11, no. 5, p. 523, 2019.\n[33] M. G. Selvaraj, A. Vergara, F. Montenegro, H. A. Ruiz, N. Safari,\nD. Raymaekers, W. Ocimati, J. Ntamwira, L. Tits, A. B. Omondi, and\nG. Blomme, ‘‘Detection of banana plants and their major diseases through\naerial images and machine learning methods: A case study in DR Congo\nand republic of Benin,’’ ISPRS J. Photogramm. Remote Sens., vol. 169,\npp. 110–124, Nov. 2020.\n[34] N. Shvetsova, B. Chen, A. Rouditchenko, S. Thomas, B. Kingsbury,\nR. Feris, D. Harwath, J. Glass, and H. Kuehne, ‘‘Everything at\nonce—Multi-modal fusion transformer for video retrieval,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022,\npp. 20020–20029.\n[35] H. Akbari, ‘‘V ATT: Transformers for multimodal self-supervised\nlearning from raw video, audio and text,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 34, 2021, pp. 24206–24221.\n[Online]. Available: https://proceedings.neurips.cc/paper/2021/file/\ncb3213ada48302953cb0f166464ab356-Paper.pdf\n[36] Z. Li, Z. Li, J. Zhang, Y . Feng, and J. Zhou, ‘‘Bridging text and video:\nA universal multimodal transformer for audio-visual scene-aware dia-\nlog,’’ IEEE/ACM Trans. Audio, Speech, Language Process., vol. 29,\npp. 2476–2483, 2021.\n[37] A. Botach, E. Zheltonozhskii, and C. Baskin, ‘‘End-to-end referring video\nobject segmentation with multimodal transformers,’’ in Proc. IEEE/CVF\nConf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022, pp. 4985–4995.\n[38] J. Wang, Z. Wu, W. Ouyang, X. Han, J. Chen, Y .-G. Jiang, and S.-N. Li,\n‘‘M2TR: Multi-modal multi-scale transformers for deepfake detection,’’\nin Proc. Int. Conf. Multimedia Retr., Jun. 2022, pp. 615–623, doi:\n10.1145/3512527.3531415.\n[39] O. Dalmaz, M. Yurt, and T. Cukur, ‘‘ResViT: Residual vision transform-\ners for multimodal medical image synthesis,’’ IEEE Trans. Med. Imag.,\nvol. 41, no. 10, pp. 2598–2614, Oct. 2022.\n[40] A. Nagrani, S. Yang, A. Arnab, A. Jansen, C. Schmid, and\nC. Sun, ‘‘Attention bottlenecks for multimodal fusion,’’ in Proc.\nAdv. Neural Inf. Process. Syst., vol. 34, 2021, pp. 14200–14213.\n[Online]. Available: https://proceedings.neurips.cc/paper/2021/file/\n76ba9f564ebbc35b1014ac498fafadd0-Paper.pdf\n[41] B. Chen, Q. Feng, B. Niu, F. Yan, B. Gao, J. Yang, J. Gong, and J. Liu,\n‘‘Multi-modal fusion of satellite and street-view images for urban village\nclassification based on a dual-branch deep neural network,’’ Int. J. Appl.\nEarth Observ. Geoinf., vol. 109, May 2022, Art. no. 102794.\n[42] N. AlDahoul, H. A. Karim, and M. A. Momo, ‘‘RGB-D based multi-modal\ndeep learning for spacecraft and debris recognition,’’ Sci. Rep., vol. 12,\nno. 1, pp. 1–18, Mar. 2022.\n[43] S. K. Roy, A. Deria, D. Hong, B. Rasti, A. Plaza, and J. Chanussot,\n‘‘Multimodal fusion transformer for remote sensing image classification,’’\n2022, arXiv:2203.16952.\n[44] V . Cazaubiel, V . Chorvalli, and C. Miesch, ‘‘The multispectral instru-\nment of the Sentinel2 program,’’ in Proc. Int. Conf. Space Opt. (ICSO),\nNov. 2017, pp. 110–115.\n[45] S. Mishra and D. R. Mishra, ‘‘Normalized difference chlorophyll index:\nA novel model for remote estimation of chlorophyll-a concentration in\nturbid productive waters,’’ Remote Sens. Environ., vol. 117, pp. 394–406,\nFeb. 2012.\n[46] C. Gessler, I. Pertot, and M. Perazzolli, ‘‘Plasmopara viticola : A review\nof knowledge on downy mildew of grapevine and effective disease man-\nagement,’’Phytopathologia Mediterranea, vol. 50, no. 1, pp. 3–44, 2011.\n[Online]. Available: https://www.jstor.org/stable/26458675\n[47] V . Bordeaux-Aquitaine. Le Mildiou De La Vigne. Accessed: Jan. 15, 2022.\n[Online]. Available: https://www.vignevin-occitanie.com/fiches-\npratiques/le-mildiou-de-la-vigne/\n[48] Optimization (Transformers Library). Accessed: Jul. 7, 2022. [Online].\nAvailable: https://huggingface.co/docs/transformers/main_classes/\noptimizer_schedules\n[49] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, ‘‘CvT:\nIntroducing convolutions to vision transformers,’’ in Proc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV), Oct. 2021, pp. 22–31. [Online]. Available:\nhttps://www.microsoft.com/en-us/research/publication/cvt-introducing-\nconvolutions-to-vision-transformers/\nVOLUME 11, 2023 5415\nW. Maillet et al.: Fusion of Satellite Images and Weather Data With Transformer Networks\nWILLIAM MAILLET received the Baccalaure-\nate degree from the Montesquieu High School,\nLe Mans, France. He joined the PRISME Labora-\ntory as an Intern, from April 2022 to August 2022,\nin the field of artificial intelligence and computer\nvision. He has been specialized in big data, since\n2018. He is currently a Student Engineer at the\nINSA Centre Val-de-Loire.\nMARYAM OUHAMI received the B.S. degree\nin mathematics and informatics from the Uni-\nversity of Hassan 2, Morocco, in 2016, and the\nM.S. degree in data sciences and big data from\nthe National School of Computer Science and\nSystems Analysis ENSIAS, Morocco, in 2018.\nShe is currently pursuing the joint Ph.D. degree\nwith the PRISME Laboratory, National Institute\nof Applied Sciences Centre Val de Loire (INSA\nCVL), France, and the IRF-SIC Laboratory, Ibn\nZohr University, Morocco. Her research interests include machine learning\nalgorithms and data fusion applied to agriculture.\nADEL HAFIANE (Member, IEEE) received the\nM.S. degree in embedded systems and information\nprocessing and the Ph.D. degree from the Univer-\nsity of Paris-Sud, in 2002 and 2005, respectively.\nHe worked one year in teaching and research at the\nUniversity of Paris-Sud and later for one year at the\nINSA Centre Val de Loire (INSA CVL) (Former\nENSI de Bourges). He was a Postdoctoral Fellow\nat the Computer Science Department, University\nof Missouri, from 2007 to 2008. He has been with\nINSA CVL, since September 2008. Since 2020, he has been the Head of the\nPRISME Laboratory, Images and Vision Group, University of Orléans-INSA\nCVL. He was also a Visiting Researcher at the University of Missouri for\nseveral periods, from 2009 to 2013. He is currently an Associate Professor\nwith the Electrical Engineering and Computer Science Department, INSA\nCVL. His research interests include theory and methods of image processing,\ncomputer vision, and machine learning for several applications: medical,\nagriculture, and robotics.\n5416 VOLUME 11, 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7391172647476196
    },
    {
      "name": "Downy mildew",
      "score": 0.6460684537887573
    },
    {
      "name": "Sensor fusion",
      "score": 0.5640154480934143
    },
    {
      "name": "Transformer",
      "score": 0.5512509942054749
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5137730836868286
    },
    {
      "name": "Encoder",
      "score": 0.41994667053222656
    },
    {
      "name": "Machine learning",
      "score": 0.3535199761390686
    },
    {
      "name": "Computer vision",
      "score": 0.34732717275619507
    },
    {
      "name": "Engineering",
      "score": 0.11302369832992554
    },
    {
      "name": "Voltage",
      "score": 0.10849684476852417
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210143826",
      "name": "Institut National des Sciences Appliquées Centre Val de Loire",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210096782",
      "name": "Laboratoire Pluridisciplinaire de Recherche en Ingénierie des Systèmes, Mécanique et Energétique",
      "country": "FR"
    }
  ]
}