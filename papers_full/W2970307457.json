{
  "title": "Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation",
  "url": "https://openalex.org/W2970307457",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2907343665",
      "name": "Cristina Gârbacea",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224892635",
      "name": "Samuel Carton",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2120550219",
      "name": "Shiyan Yan",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2166036605",
      "name": "Qiaozhu Mei",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1590378318",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2963857374",
    "https://openalex.org/W2625357353",
    "https://openalex.org/W2963018920",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W2996068536",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2617539464",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2003155177",
    "https://openalex.org/W2780155557",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W179179905",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2161283199",
    "https://openalex.org/W4293355737",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2798054415",
    "https://openalex.org/W2883158411",
    "https://openalex.org/W2060833990",
    "https://openalex.org/W2964144352",
    "https://openalex.org/W2058373514",
    "https://openalex.org/W2250234233",
    "https://openalex.org/W2603766943",
    "https://openalex.org/W2155027007",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2122261166",
    "https://openalex.org/W2474824677",
    "https://openalex.org/W2581637843",
    "https://openalex.org/W4293398859",
    "https://openalex.org/W2963574252",
    "https://openalex.org/W1982897610",
    "https://openalex.org/W2963730239",
    "https://openalex.org/W2018758420",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2963466651",
    "https://openalex.org/W2964028737",
    "https://openalex.org/W2138857742",
    "https://openalex.org/W2963456134",
    "https://openalex.org/W2951824008",
    "https://openalex.org/W2150290224",
    "https://openalex.org/W2554423077",
    "https://openalex.org/W2557508245",
    "https://openalex.org/W2792795990",
    "https://openalex.org/W2784823820",
    "https://openalex.org/W4294149591",
    "https://openalex.org/W2963527228",
    "https://openalex.org/W2596458094",
    "https://openalex.org/W2542835211",
    "https://openalex.org/W2799184518",
    "https://openalex.org/W4394643672",
    "https://openalex.org/W4302353911",
    "https://openalex.org/W2605035112",
    "https://openalex.org/W2577946330",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W2174424190",
    "https://openalex.org/W2964017345",
    "https://openalex.org/W2104626092",
    "https://openalex.org/W2785896739",
    "https://openalex.org/W2169279899",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2963142510",
    "https://openalex.org/W2962879001",
    "https://openalex.org/W2028602070",
    "https://openalex.org/W2100059775",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W2807747378",
    "https://openalex.org/W2553897675",
    "https://openalex.org/W2603089249",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2729046720",
    "https://openalex.org/W273955616",
    "https://openalex.org/W4205428528",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W2620623908",
    "https://openalex.org/W2963903950",
    "https://openalex.org/W2152921782",
    "https://openalex.org/W2593383075",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1899504021",
    "https://openalex.org/W2963248348",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2524985544",
    "https://openalex.org/W3102476541",
    "https://openalex.org/W2519536754",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2963878748",
    "https://openalex.org/W2583679610",
    "https://openalex.org/W2616969219",
    "https://openalex.org/W2740167620",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2963373786",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2962788902",
    "https://openalex.org/W2099057450",
    "https://openalex.org/W4294563867",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W1993509040",
    "https://openalex.org/W648786980",
    "https://openalex.org/W2786472963",
    "https://openalex.org/W590442793",
    "https://openalex.org/W2604721644",
    "https://openalex.org/W2962968835",
    "https://openalex.org/W2328886022",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2900260828",
    "https://openalex.org/W2126316555",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2898718449",
    "https://openalex.org/W1724972948",
    "https://openalex.org/W1489525520",
    "https://openalex.org/W2078483536"
  ],
  "abstract": "Cristina Garbacea, Samuel Carton, Shiyan Yan, Qiaozhu Mei. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 3968–3981,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n3968\nJudge the Judges: A Large-Scale Evaluation Study of Neural\nLanguage Models for Online Review Generation\nCristina Gˆarbacea1, Samuel Carton2, Shiyan Yan2, Qiaozhu Mei1,2\n1Department of EECS, University of Michigan, Ann Arbor, MI, USA\n2School of Information, University of Michigan, Ann Arbor, MI, USA\n{garbacea, scarton, shiyansi, qmei}@umich.edu\nAbstract\nWe conduct a large-scale, systematic study\nto evaluate the existing evaluation methods\nfor natural language generation in the con-\ntext of generating online product reviews. We\ncompare human-based evaluators with a va-\nriety of automated evaluation procedures, in-\ncluding discriminative evaluators that measure\nhow well machine-generated text can be dis-\ntinguished from human-written text, as well as\nword overlap metrics that assess how similar\nthe generated text compares to human-written\nreferences. We determine to what extent these\ndifferent evaluators agree on the ranking of a\ndozen of state-of-the-art generators for online\nproduct reviews. We ﬁnd that human eval-\nuators do not correlate well with discrimina-\ntive evaluators, leaving a bigger question of\nwhether adversarial accuracy is the correct ob-\njective for natural language generation. In gen-\neral, distinguishing machine-generated text is\nchallenging even for human evaluators, and\nhuman decisions correlate better with lexical\noverlaps. We ﬁnd lexical diversity an intrigu-\ning metric that is indicative of the assessments\nof different evaluators. A post-experiment sur-\nvey of participants provides insights into how\nto evaluate and improve the quality of natural\nlanguage generation systems 1.\n1 Introduction\nRecent developments in neural language models\n(Mikolov and Zweig, 2012), (Reiter and Belz,\n2009), (Mikolov et al., 2011b), (Mikolov et al.,\n2011a) have inspired the use of neural network\nbased architectures for the task of natural language\ngeneration (NLG). Despite fast development of al-\ngorithms, there is an urgency to ﬁll the huge gap in\nevaluating NLG systems. On one hand, a rigorous,\n1 The experimental setup, data, and annotations are pub-\nlicly available at: https://github.com/Crista23/\nJudgeTheJudges\nefﬁcient, and reproducible evaluation procedure is\ncritical for the development of any machine learn-\ning technology and for correct interpretation of the\nstate-of-the-art. On the other hand, evaluating the\nquality of language generation is inherently dif-\nﬁcult due to the special properties of text, such\nas subjectivity and non-compositionality. Indeed,\n“there is no agreed objective criterion for comparing\nthe goodness of texts” (Dale and Mellish, 1998),\nand there lacks a clear model of text quality (Hard-\ncastle and Scott, 2008).\nConventionally, most NLG systems have been\nevaluated in a rather informal manner. (Reiter\nand Belz, 2009) divide existing evaluation meth-\nods commonly employed in text generation into\nthree categories: i) evaluations based on task per-\nformance, ii) human judgments and ratings, where\nhuman subjects are recruited to rate different di-\nmensions of the generated texts, and iii) evalua-\ntions based on comparison to a reference corpus\nusing automated metrics. Task based evaluation\nconsiders that the value of a piece of functional text\nlies in how well it serves the user to fulﬁll a speciﬁc\napplication. It can be expensive, time-consuming,\nand often dependent on the good will of partici-\npants in the study. Besides that, it is hard to toss\nout the general quality of text generation from the\nspecial context (and confounds) of the task, or to\ngeneralize the evaluation conclusions across tasks.\nHuman annotation is able to assess the quality of\ntext more directly than task based evaluation. How-\never, rigorously evaluating NLG systems with real\nusers can be expensive and time consuming, and it\ndoes not scale well (Reiter et al., 2001). Human as-\nsessments also need to be consistent and repeatable\nfor a meaningful evaluation (Lopez, 2012). Alter-\nnative strategies which are more effective in terms\nof cost and time are used more frequently.\nAutomated evaluation compares texts generated\nby the candidate algorithms to human-written texts.\n3969\nWord overlap metrics and more recent automated\nadversarial evaluators are widely employed in NLG\nas they are cheap, quick, repeatable, and do not re-\nquire human subjects when a reference corpus is\nalready available. In addition, they allow devel-\nopers to make rapid changes to their systems and\nautomatically tune parameters without human in-\ntervention. Despite the beneﬁts, however, the use\nof automated metrics in the ﬁeld of NLG is con-\ntroversial (Reiter and Belz, 2009), and their results\nare often criticized as not meaningful for a number\nof reasons. First, these automatic evaluations rely\non a high-quality corpus of references, which is\nnot often available. Second, comparisons with a\nreference corpus do not assess the usefulness and\nthe impact of the generated text on the readers as in\nhuman-based evaluations. Third, creating human\nwritten reference texts speciﬁcally for the purpose\nof evaluation could still be expensive, especially if\nthese reference texts need to be created by skilled\ndomain experts. Finally and most importantly, us-\ning automated evaluation metrics is sensible only\nif they correlate with results of human-based eval-\nuations and if they are accurate predictors of text\nquality, which is never formally veriﬁed at scale.\nWe present a large-scale, systematic experiment\nthat evaluates the evaluators for NLG. We compare\nthree types of evaluators including human evalu-\nators, automated adversarial evaluators trained to\ndistinguish human-written from machine-generated\nproduct reviews, and word overlap metrics (such as\nBLEU and ROUGE) in a particular scenario, gen-\nerating online product reviews. The preferences of\ndifferent evaluators on a dozen representative deep-\nlearning based NLG algorithms are compared with\nhuman assessments of the quality of the generated\nreviews. Our ﬁndings reveal signiﬁcant differences\namong the evaluators and shed light on the poten-\ntial factors that contribute to these differences. The\nanalysis of a post experiment survey also provides\nimportant implications on how to guide the devel-\nopment of new NLG algorithms.\n2 Related Work\n2.1 Deep Learning Based NLG\nRecently, a decent number of deep learning\nbased models have been proposed for text gener-\nation. Recurrent Neural Networks (RNNs) and\ntheir variants, such as Long Short Term Memory\n(LSTM) (Hochreiter and Schmidhuber, 1997) mod-\nels, Google LM (Jozefowicz et al., 2016), and\nScheduled Sampling (SS) (Bengio et al., 2015) are\nwidely used for generating textual data.\nGenerative Adversarial Networks (Goodfellow\net al., 2014), or GANs, train generative models\nthrough an adversarial process. Generating text\nwith GANs is challenging due to the discrete nature\nof text data. SeqGAN (Yu et al., 2017) is one of the\nearliest GAN-based model for sequence generation,\nwhich treats the procedure as a sequential decision\nmaking process. RankGAN (Lin et al., 2017) pro-\nposes a framework that addresses the quality of\na set of generated sequences collectively. Many\nGAN-based models (Yu et al., 2017), (Lin et al.,\n2017), (Rajeswar et al., 2017), (Che et al., 2017),\n(Li et al., 2017), (Zhang et al., 2017) are only capa-\nble of generating short texts. LeakGAN (Guo et al.,\n2018) is proposed for generating longer texts.\nDeep learning architectures other than LSTM or\nGAN have also been proposed for text generation.\n(Tang et al., 2016) study NLG given particular con-\ntexts or situations and proposes two approaches\nbased on the encoder-decoder framework. (Dong\net al., 2017) address the same task and employ an\nadditional soft attention mechanism. Pre-training\nenables better generalization in deep neural net-\nworks (Erhan et al., 2010), especially when com-\nbined with supervised discriminative ﬁne-tuning\nto learn universal robust representations (Radford\net al., 2018), (Devlin et al., 2018), (Radford et al.,\n2019). (Guu et al., 2018) use a prototype-then-edit\ngenerative language model for sentences.\n2.2 Automated Evaluation Metrics\nThe variety of NLG models are also evaluated with\nvarious approaches. Arguably, the most natural\nway to evaluate the quality of a generator is to\ninvolve humans as judges, either through some\ntype of Turing test (Machinery, 1950) to distinguish\ngenerated text from human input texts, or to directly\ncompare the texts generated by different generators\n(Mellish and Dale, 1998). Such approaches are\nhard to scale and have to be redesigned whenever a\nnew generator is included. Practically, it is critical\nto ﬁnd automated metrics to evaluate the quality\nof a generator independent of human judges or an\nexhaustive set of competing generators.\nPerplexity (Jelinek et al., 1977) is commonly\nused to evaluate the quality of a language model,\nwhich has also been employed to evaluate genera-\ntors (Yarats and Lewis, 2018), (Ficler and Goldberg,\n2017), (Gerz et al., 2018) even though it is com-\n3970\nmonly criticized for not being a direct measure of\nthe quality of generated text (Fedus et al., 2018).\nPerplexity is a model dependent metric, and “how\nlikely a sentence is generated by a given model” is\nnot comparable across different models. Therefore\nwe do not include perplexity in this study.\nDiscriminative Evaluation is an alternative\nway to evaluate a generator, which measures how\nlikely its generated text can fool a classiﬁer that\naims to distinguish the generated text from human-\nwritten texts. In a way, this is an automated approx-\nimation of the Turing test, where machine judges\nare used to replace human judges. Discriminative\nmachine judges can be trained either using a data\nset with explicit labels (Ott et al., 2011), or using\na mixture of text written by real humans and those\ngenerated by the model being evaluated. The lat-\nter is usually referred to as adversarial evaluation.\n(Bowman et al., 2016) proposes one of the earliest\nstudies that uses adversarial error to assess the qual-\nity of generated sentences. Notably, maximizing\nthe adversarial error is consistent to the objective\nof the generator in generative adversarial networks.\n(Kannan and Vinyals, 2017) propose an adversarial\nloss to discriminate a dialogue model’s output from\nhuman output. The discriminator prefers longer\noutput and rarer language instead of the common\nresponses generated. There however lacks evidence\nthat a model that obtains a lower adversarial loss is\nbetter according to human evaluations.\nAutomatic dialogue evaluation is formulated as a\nlearning problem in (Lowe et al., 2017), who train\nan RNN to predict the scores a human would assign\nto dialogue responses. RNN predictions correlate\nwith human judgments at the utterance and system\nlevel, however each response is evaluated in a very\nspeciﬁc context and the system requires substantial\nhuman judgments for training. (Li et al., 2017)\nemploy a discriminator (analogous to the human\nevaluator in the Turing test) both in training and\ntesting and deﬁne adversarial success. Other work\nﬁnds the performance of a discriminative agent\n(e.g., attention-based bidirectional LSTM binary\nclassiﬁer) is comparable with human judges at dis-\ntinguishing between real and fake dialogue excerpts\n(Bruni and Fern ´andez, 2017). However, results\nshow there is limited consensus among humans on\nwhat is considered as coherent dialogue passages.\nWord Overlap Metrics, such as BLEU (Pap-\nineni et al., 2002), ROUGE (Lin, 2004), and ME-\nTEOR (Banerjee and Lavie, 2005), are commonly\nused to measure the similarity between the gener-\nated text and human written references. (Liu et al.,\n2016) ﬁnd that word overlap metrics present weak\nor no correlation with human judgments in non-task\noriented dialogue systems and thus should be used\nwith caution or in combination with user studies.\nIn contrary, it is reported in (Sharma et al., 2017)\nthat text overlap metrics are indicative of human\njudgments in task-oriented dialogue settings, when\nused on datasets which contain multiple ground\ntruth references. (Dai et al., 2017) ﬁnd text overlap\nmetrics too restrictive as they focus on ﬁdelity of\nwording instead of ﬁdelity of semantics. (Callison-\nBurch et al., 2006) consider an increase in BLEU\ninsufﬁcient for an actual improvement in the quality\nof a system and posit in favor of human evaluation.\nBLEU and its variants (e.g., Self-BLEU) are\nused to evaluate GAN models (Caccia et al., 2018;\nZhu et al., 2018). (Shi et al., 2018) compare frame-\nworks for text generation including MLE, SeqGAN,\nLeakGAN and Inverse Reinforcement Learning us-\ning a simulated Turing test. A benchmarking exper-\niment with GAN models is conducted in (Lu et al.,\n2018); results show LeakGAN presents the highest\nBLEU scores on the test data. Similarly, BLEU and\nMETEOR present highest correlations with human\njudgements (Callison-Burch et al., 2008), (Graham\nand Baldwin, 2014). However, evaluation metrics\nare not robust across conditions, and no single met-\nric consistently outperforms other metrics across\nall correlation levels (Przybocki et al., 2009).\nConventional neural language models trained\nwith maximum likelihood can be on par or better\nthan GANs (Caccia et al., 2018), (Semeniuta et al.,\n2018), (Tevet et al., 2018). However, log-likelihood\nis often computationally intractable (Theis et al.,\n2016). Models with good likelihood can produce\nbad samples, and vice-versa (Goodfellow, 2016).\nGenerative models should be evaluated with re-\ngards to the task they are intended for over the\nfull quality-diversity spectrum (C´ıfka et al., 2018),\n(Hashimoto et al., 2019), (Montahaei et al., 2019).\nWhile many generators are proposed and eval-\nuated with various metrics, no existing work has\nsystematically evaluated the different evaluators at\nscale, especially in the context of online review\ngeneration. Our work ﬁlls in this gap.\n3 Experiment Design\nWe design a large-scale experiment to systemati-\ncally analyze the procedures and metrics used for\n3971\nevaluating NLG models. To test the differentevalu-\nators, the experiment carefully chooses a particular\napplication context and a variety of natural lan-\nguage generators in this context. Ideally, a sound\nautomated evaluator should be able to distinguish\ngood generators from suboptimal ones. Its prefer-\nences (on ordering the generators) should be con-\nsistent to humans in the exact application context.\n3.1 Experiment Context and Procedure\nWe design the experiment in the context of gen-\nerating online product reviews. There are several\nreasons why review generation is a desirable task\nfor the experiment: 1) online product reviews are\nwidely available, and it is easy to collect a large\nnumber of examples for training/ testing the gener-\nators; 2) Internet users are used to reading online\nreviews, and it is easy to recruit capable human\njudges to assess the quality of reviews; and 3) com-\nparing to tasks like image caption generation or\ndialogue systems, review generation has minimal\ndependency on the conversation context or on non-\ntextual data, which reduces possible confounds.\nFigure 1: Overview of the Experiment Procedure.\nThe general experiment procedure is presented\nin Figure 1. We start from the publicly available\nAmazon Product Reviews dataset 2 and select three\nmost popular domains: books, electronics, and\nmovies. After ﬁltering rare products, inactive users,\nand overly long reviews, the dataset is randomly\nsplit into three parts, to train, to validate, and to\n2http://jmcauley.ucsd.edu/data/amazon/\ntest the candidate review generators (denoted as G-\ntrain, G-valid, and G-test). Every generative model\nis trained and validated using the same datasets,\nand then charged to generate a number of product\nreviews (details are included in the next section).\nThese generated reviews, mixed with the real re-\nviews in G-test, are randomly split into three new\nsubsets for training, validating, and testing candi-\ndate (discriminative) evaluators, denoted asD-train,\nD-valid, and D-test. Finally, a random sample of\nreviews from D-test are sent for human evaluation.\n3.2 Review Generators\nAlthough our goal is to evaluate the evaluators, it is\ncritical to include a wide range of text generators\nwith various degrees of quality. A good evalua-\ntor should be able to distinguish the high-quality\ngenerators from the low-quality ones. We select a\ndiverse set of generative models from recent litera-\nture. The goal of this study is not to name the best\ngenerative model, and it is unfeasible to include all\nexisting models. Our criteria are: (1) the models\nare published before 2018, when our experiment is\nconducted; (2) the models represent different learn-\ning strategies and quality levels; (3) the models\nhave publicly available implementations, for repro-\nducibility purposes. In Table 1 we list the candidate\ngenerators. It is not an exhaustive list of what are\ncurrently available. For implementation details of\nthese models please see Appendix A.1.\nTable 1: Candidate models for review generation.\nGenerative Model Adversarial\nFramework\nWord LSTM temp 1.0 (Hochreiter and Schmidhuber, 1997)No\nWord LSTM temp 0.7 (Hochreiter and Schmidhuber, 1997)No\nWord LSTM temp 0.5 (Hochreiter and Schmidhuber, 1997)No\nScheduled Sampling (Bengio et al., 2015) No\nGoogle LM (Jozefowicz et al., 2016) No\nAttention Attribute to Sequence* (Dong et al., 2017)No\nContexts to Sequences* (Tang et al., 2016)No\nGated Contexts to Sequences* (Tang et al., 2016)No\nMLE SeqGAN (Yu et al., 2017) Yes\nSeqGAN (Yu et al., 2017) Yes\nRankGAN (Lin et al., 2017) Yes\nLeakGAN (Guo et al., 2018) Yes\n* indicates that review generation using these models are conditional on\ncontext information such as product ids; other models are context independent.\nEvery generator (except Google LM) is trained\nand validated on G-train and G-valid datasets, and\nused to generate the same number of machine-\ngenerated (a.k.a., fake) reviews (see Table 2). We\nfollow the best practice in literature to train these\nmodels, although it is possible that the performance\nof models might not be optimal due to various con-\n3972\nstraints. This will not affect the validity of the\nexperiment as our goal is to evaluate the evalua-\ntors instead of the individual generators. Google\nLM was not trained on reviews, but it provides a\nsanity check for the experiment - a reasonable eval-\nuator should not rank it higher than those trained\nfor generating reviews.\nTable 2: Number of generated reviews by each model.\nGenerative Model TotalD-TrainD-ValidD-Test\n∀model in Table 1 except Google LM32,50022,750 3,2506,500Google LM 6,680 4,676 668 1,336\n3.3 Evaluators\nWe include a comprehensive set of evaluators for\nthe quality of the aforementioned generators: i)\nhuman evaluators, ii) discriminative evaluators, and\niii) text overlap evaluators. The evaluators are the\nmain subjects of the experiment.\n3.3.1 Human evaluators\nWe conduct a careful power analysis (Christensen,\n2007), which suggests that at least 111 examples\nper generative model should be human annotated to\ninfer that the machine-generated reviews are com-\nparable in quality to human-written reviews, at a\nminimal statistically signiﬁcance level of 0.05. Per\nthis calculation, we sample 150 examples for each\nof the 12 generators for human evaluation. This\ntotals 1,800 machine-generated reviews, to which\nwe add 1,800 human-written reviews, or a total\nof 3,600 product reviews sent for human annota-\ntion. We markup out-of-vocabulary words in both\nhuman-written and machine-generated reviews to\ncontrol for confounds of using certain rare words.\nThere is no signiﬁcant difference in proportion of\nthe markup token between the two classes (2.5%-\nreal vs. 2.2%-fake). We recruit 900 human annota-\ntors through the Amazon Mechanical Turk (AMT)\nplatform. Each annotator is presented 20 reviews,\na mixture of 10 real (i.e., human written) and 10\nfake (i.e., machine generated), and they are charged\nto label each review as real or fake based on their\nown judgment. Clear instructions are presented\nto the workers that markup tokens are present in\nboth classes and cannot be used to decide whether\na review is real or fake. Each page is annotated\nby 5 distinct human evaluators. The 5 judgments\non every review are used to assemble two distinct\nhuman evaluators: H1 - individual votes, treat-\ning all human annotations independently, and H2\n- majority votesof the 5 human judgments. For\nevery annotated review, the human evaluator (H1\nor H2) makes a call which can be either right or\nwrong with regard to the ground truth. A generator\nis high quality if the human evaluator achieves low\naccuracy identifying the reviews as fake.\n3.3.2 Discriminative evaluators\nThe inclusion of multiple generators provides the\nopportunity of creating meta-adversarial evalu-\nators, trained using a pool of generated reviews\nby many generators, mixed with a larger number\nof “real” reviews (D-train and D-valid datasets).\nSuch a “pooling” strategy is similar to the standard\npractice used by the TREC conferences to evalu-\nate different information retrieval systems (Harman\nand V oorhees, 2006). Comparing to individual ad-\nversarial evaluators, a meta-evaluator is supposed\nto be more robust and fair, and it can be applied to\nevaluate new generators without being retrained. In\nour experiment, we ﬁnd that the meta-adversarial\nevaluators rank the generators in similar orders to\nthe best individual adversarial evaluators.\nWe employ a total of 7 meta-adversarial eval-\nuators: 3 deep, among which one using LSTM\n(Hochreiter and Schmidhuber, 1997), one using\nConvolutional Neural Network (CNN) (LeCun\net al., 1998), and one using a combination of\nLSTM and CNN architectures; 4 shallow, based on\nNaive Bayes (NB) (Rish, 2001), Random Forest\n(RF) (Liaw et al., 2002), Support Vector Machines\n(SVM) (Cortes and Vapnik, 1995), and XGBoost\n(Chen and Guestrin, 2016), with unigrams, bigrams,\nand trigrams as features and on balanced training\nsets. We ﬁnd the best hyper-parameters using ran-\ndom search and prevent the models from overﬁtting\nby using early stopping. For every review in D-test\n(either annotated or not), a meta-adversarial evalu-\nator makes a judgment call. A generator is consid-\nered high quality if the meta-adversarial evaluator\nmakes more mistakes on reviews it generated.\n3.3.3 Word-overlap evaluators\nWe include a set of 4 text-overlap metrics used for\nNLG evaluation: BLEU and METEOR (speciﬁc to\nmachine translation), ROUGE (used in text summa-\nrization), and CIDEr (Vedantam et al., 2015) (used\nin image description evaluation). These metrics\nrely on matching n-grams in the target text (i.e.,\ngenerated reviews) to the “references” (i.e., human-\nwritten reviews). The higher the overlap (similar-\nity), the higher the quality of generated text. For\n3973\nevery generated review inD-test Fake, we assemble\nthe set of references by retrieving the top-10 most\nsimilar human-written reviews in D-test Real using\na simple vector space model. We compute 600-\ndimensional vector representation of reviews using\nSent2Vec (Pagliardini et al., 2018), pretrained on\nEnglish Wikipedia, and retrieve the top-k nearest\nneighbors for each review based on cosine similar-\nity of the embedding vectors. The rationale of using\nnearest neighbors of each generated review as ref-\nerences is that to appear “real”, a generated review\njust need to be similar to some real reviews instead\nof all. A generator is considered high quality if its\ngenerated reviews obtain a high average score by\na text overlap evaluator. In total, we analyze and\ncompare 13 candidate evaluators (2 human evalua-\ntors, 7 discriminative evaluators, and 4 text-overlap\nmetrics), based on the D-test dataset.\n4 Results\nFirst, we are interested in the accuracy of individual\nevaluators - how well they can distinguish “fake”\n(machine-generated) reviews from “real” (human-\nwritten) reviews. Second, we are interested in how\nan evaluator assesses the quality of the 12 gener-\nators instead of individual reviews. The absolute\nscores an evaluator gives to the generators are not\nas informative as how it ranks them: a good evalu-\nator should be able to rank good generators above\nbad generators. Last but not least, we are inter-\nested in how the rankings by different evaluators\ncorrelate with each other. Intuitively, an automated\nevaluator that ranks the generators in similar orders\nas the human evaluators is more reasonable and can\npotentially be used as the surrogate of humans.\n4.1 Results of Individual Evaluators\n4.1.1 Human evaluators\nEvery review is annotated by 5 human judges as\neither “fake” or “real.” The inter-annotator agree-\nment (Fleiss-Kappa score (Fleiss et al., 2013)) is\nk = 0 .2748. This suggests that distinguishing\nmachine-generated reviews from real in general is\na hard task even for humans; there is limited con-\nsensus on what counts as a realistic review. The low\nagreement also implies that any automated evalu-\nator that mimics human judges is not necessarily\nthe most “accurate.”\nIn Figure 2 we present the accuracy of two hu-\nman evaluators on individual annotated reviews,\nbased on either all 5 annotations or their majority\nFigure 2: Accuracy of human evaluators on individual\nreviews: H1 - individual votes; H2 - majority votes.\nvotes for each review. Comparing to the ground-\ntruth (of whether a review is machine-generated\nor collected from Amazon), individual human de-\ncisions are 66.61% accurate, while their majority\nvotes can reach 72.63%. Neither of them is close to\nperfect. We observe that human evaluators gener-\nally do better at correctly labelling human-written\nreviews as real (true positive rate of 78.96% for\nH1 and 88.31% for H2), and they are confused\nby machine-generated reviews in close to half of\nthe cases (true negative rate of 54.26% forH1 and\n56.95% for H2). This trend reassures previous\nobservations (Tang et al., 2016).\nWe then look at how the human evaluators rank\nthe 12 generators, according to the accuracy of hu-\nman evaluators on all (fake) reviews generated by\neach of the generators. The lower the accuracy,\nthe more likely the human evaluator is confused\nby the generated reviews, and thus the better the\ngenerator. We observe a substantial variance in\nthe accuracy of both human evaluators on different\ngenerators, which suggests that human evaluators\nare able to distinguish between generators. The\ngenerator ranked the highest by both human evalu-\nators is Gated Contexts to Sequences. Google LM\nis ranked on the lower side, which makes sense as\nthe model is not trained to generate reviews. Inter-\nestingly, humans tend not to be fooled by reviews\ngenerated by the GAN-based models (MLE Seq-\nGAN, SeqGAN, RankGAN and LeakGAN), even\nthough their objective is to confuse fake from real.\nGAN-generated reviews tend to be easily distin-\nguishable from the real reviews by human judges.\n4.1.2 Discriminative evaluators\nWe then analyze the 7 meta-adversarial evaluators.\nDifferent from human evaluators that are applied\n3974\nto the 3,600 annotated reviews, the discriminative\nevaluators are applied to all reviews in D-test.\nMeta-adversarial Evaluators. On individual\nreviews, the three deep learning based and the\none SVM based evaluators achieve higher accu-\nracy than the two human evaluators, indicating\nthat adversarial evaluators can distinguish a sin-\ngle machine-generated review from human-written\nbetter than humans (Figure 3 and Table 4 in Ap-\npendix A.3.2). Their true positive rates and true\nnegative rates are more balanced than human eval-\nuators. Meta-discriminators commonly rank GAN-\nbased generators the highest. This makes sense as\nthe objective of GAN is consistent to the (reversed)\nevaluator accuracy. Interestingly, by simply setting\nthe temperature parameter of Word LSTM to 1.0,\nit achieves comparable performance to the GANs.\nFigure 3: Accuracy of human (H1, H2) and meta-\nadversarial evaluators (LSTM, SVM) on reviews gener-\nated by individual generators. The lower the accuracy,\nthe better the generator.\n4.1.3 Word-Overlap Evaluators\nThe generators are ranked based on the average\nscores of their generated reviews. In Figure 4 we\npresent the average scores of the 12 generators by\neach evaluator. Different word-overlap evaluators\nalso tend to rank the generators in similar orders.\nInterestingly, the top-ranked generator according\nto three evaluators is Contexts to Sequences, while\nCIDEr scores highest the Gated Contexts to Se-\nquences model. GAN-based generators are gener-\nally ranked low; please also see Appendix A.3.3.\n4.2 Comparing Evaluators\nTo what degree do the evaluators agree on the rank-\ning of generators? We are more interested in how\nthe automated evaluators compare to the human\nevaluators, and whether there is any suitable au-\ntomated surrogate for human judges at all. To do\nFigure 4: Text-Overlap Evaluators (BLEU and CIDEr)\nscores for individual generators. The higher the bet-\nter. The rankings are overall similar, as GAN-based\ngenerators are ranked low.\nthis, we compute the correlations between H1, H2\nand each discriminative evaluator and correlations\nbetween H1, H2 and the text-overlap evaluators,\nbased on either their decisions on individual re-\nviews, their scores of the generators (by Pearson’s\ncoefﬁcient (Fieller et al., 1957)), and their rank-\nings of the generators (by Spearman’sρ(Spearman,\n1904) and Kendall’sτ (Daniel et al., 1978)). Pat-\nterns of the three correlation metrics are similar;\nplease see Figure 5 and Table 5 in Appendix A.3.4.\nFigure 5: Kendall τ-b between human and automated\nevaluators. Human’s ranking is positively correlated\nto text-overlap evaluators and negatively correlated to\nadversarial evaluators (∗ is p≤0.05).\nSurprisingly, none of the discriminative evalu-\nators have a positive correlation with the human\nevaluators. That says, generators that fool ma-\nchine judges easily are less likely to confuse human\njudges, and vice versa. Word-overlap evaluators\ntend to have a positive correlation with the human\nevaluators in ranking the generators. Among them,\nBLEU appears to be closer to humans. This pat-\ntern is consistent in all three types of correlations.\nThese two observations are intriguing, which indi-\n3975\ncates that when identifying fake reviews, humans\nmight focus more on word usage rather than trying\nto construct a “decision boundary” mentally.\nIn summary, we ﬁnd that 1) human evalua-\ntors cannot distinguish machine-generated reviews\nfrom real reviews perfectly, with signiﬁcant bias\nbetween the two classes; 2) meta-adversarial evalu-\nators can better distinguish individual fake reviews,\nbut their rankings at the generator level tend to be\nnegatively correlated with human evaluators; and 3)\ntext-overlap evaluators are highly correlated with\nhuman evaluators in ranking generators.\n5 Discussion\nWe carried a systematic experiment that evaluates\nthe evaluators for NLG. Results have intriguing im-\nplications to both the evaluation and the construc-\ntion of natural language generators. We conduct\nin-depth analysis to discover possible explanations.\n5.1 Granularity of Judgments\nWe charged the Turkers to label individual reviews\nas either fake or real instead of evaluating each\ngenerator as a whole. Comparing to an adversarial\ndiscriminator, a human judge has not seen many\n“training” examples offake reviews or generators.\nThat explains why the meta-adversarial evaluators\nare better at identifying fake reviews. In this con-\ntext, humans are likely to judge whether a review\nis real based on how “similar” it appears to the true\nreviews they are used to seeing online.\nThis ﬁnding provides interesting implications to\nthe selection of evaluation methods for different\ntasks. In tasks that are set up to judge individual\npieces of generated text (e.g., reviews, translations,\nsummaries, captions, fake news) where there ex-\nists human-written ground-truth, it is better to use\nword-overlap metrics instead of adversarial eval-\nuators. When judgments are made on the agent/\nsystem level (e.g., whether a Twitter account is a\nbot), signals like how similar the agent outputs are\nor how much the agent memorizes the training ex-\namples may become more useful than word usage,\nand a discriminative evaluator may be more effec-\ntive than word-overlap metrics. Our ﬁnding also\nimplies that adversarial accuracy might not be the\noptimal objective for NLG if the goal is to generate\ndocuments that humans consider as real. Indeed, a\nfake review that fools humans does not necessarily\nneed to fool a machine that has seen everything. In\nAppendix B.2 we provide more details.\n5.2 Imperfect Ground Truth\nOne important thing to note is that all discrimi-\nnative evaluators are trained using natural labels\n(i.e., treating all examples from the Amazon review\ndataset as positive and examples generated by the\ncandidate models as negative) instead of human-\nannotated labels. Some reviews posted on Amazon\nmay have been generated by bots, and if that is the\ncase, treating them as human-written examples may\nbias the discriminators. To verify this, we apply the\nalready trained meta-discriminators to the human-\nannotated subset (3,600 reviews) instead of the full\nD-test set, and we use the majority vote of human\njudges (whether a review is fake or real) to surro-\ngate the natural “ground-truth” labels (whether a\nreview is generated or sampled from Amazon).\nFigure 6: Kendall τ-b correlation coefﬁcients between\nhuman evaluators and automated evaluators, tested on\nthe annotated subset ofD-test with majority votes as\nground-truth (∗ denotes p≤0.05).\nWhen the meta-adversarial evaluators are tested\nusing human majority-votes as ground-truth, the\nscores and rankings of these discriminative eval-\nuators are more inline with the human evaluators,\nalthough still not as highly correlated as BLEU;\nplease see Figure 6. Indeed, discriminative evalua-\ntors suffer the most from low-quality labels, as they\nwere directly trained using the imperfect ground-\ntruth. Word-overlap evaluators are more robust, as\nthey only take the most relevant parts of the test\nset as references (more likely to be high quality).\nOur results also suggest that when adversarial train-\ning is used, selection of training examples must be\ndone with caution. If the “ground-truth” is hijacked\nby low quality or “fake” examples, models trained\nby GAN may be signiﬁcantly biased. This ﬁnding\nis related to the recent literature of the robustness\nand security of machine learning models (Papernot\net al., 2017). Appendix B.3 contains further details.\n3976\n5.3 Role of Diversity\nWe assess the role diversity plays in rankings the\ngenerators. Diversity of a generator is measured\nby either the lexical diversity (Bache et al., 2013)\nor Self-BLEU (Zhu et al., 2018) of the samples\nproduced by the generator. Results obtained (see\nFigure 7) indicate generators that produce the least\ndiverse samples are easily distinguished by the\nmeta-discriminators, while they confuse humans\nthe most. This conﬁrms that adversarial discrimi-\nnators capture limitations of generative models in\nlack of diversity (Kannan and Vinyals, 2017).\nFigure 7: Self-BLEU scores (the lower the more di-\nverse) and lexical diversity scores (the higher the more\ndiverse) are highly correlated in ranking the generators.\nSimilarly, we measure to what extent the gen-\nerators are memorizing the training set G-train as\nthe average BLEU scores of generated reviews us-\ning their nearest neighbors in G-train as references.\nWe observe the generators do not memorize the\ntraining set, and GAN models generate reviews\nthat have fewer overlap with G-train; this ﬁnding\nis in line with recent theoretical studies on memo-\nrization in GANs (Nagarajan et al.).\nThe effects of diversity indicate that when hu-\nmans are distinguishing individual reviews as real\nor fake, whether or not a fake review is sufﬁciently\ndifferent from the other generated reviews is not a\nmajor factor for their decision. Instead, they tend\nto focus on whether the review looks similar to the\nreviews they have seen in reality. A discriminative\nevaluator is more powerful in making decisions at\na system level (e.g., a dialog system or a bot ac-\ncount), where diversity becomes a major factor. In\nAppendix B.4 we provide more details.\n5.4 User Study\nFinally, we are interested in the reasons why human\nannotators label certain reviews as fake (machine-\nwritten). After annotating a batch of reviews, work-\ners are asked to explain their decisions by ﬁlling in\nan optional free-text comment. This enables us to\nhave a better understanding of what differentiates\nmachine-generated from human-written reviews\nfrom human’s perspective. Analyzing their com-\nments, we identify the main reasons why human\nevaluators annotate a review as machine-written.\nThese are mainly related to the presence of gram-\nmatical errors in the review text, wrong wording or\ninappropriate choice of expressions, redundant use\nof speciﬁc phrases or contradictory arguments in\nthe review. Interestingly, human evaluators’ innate\nbiases are also reﬂected in their decisions: they\nare likely to categorize a review as fake if it is too\nformal, lacks emotion and personal pronouns, or is\ntoo vague and generic. Please see Appendix B.1.\n5.5 Summary\nIn summary, our ﬁndings represent a preliminary\nfoundation for proposing more solid and robust\nevaluation metrics and objectives of natural lan-\nguage generation. The low inter-rater agreement\nwe observe suggests that judging individual pieces\nof text as machine- or human-generated is a dif-\nﬁcult task even for humans. In this context, dis-\ncriminative evaluators are not as correlated with\nhuman judges as word-overlap evaluators. That\nimplies that adversarial accuracy might not be the\noptimal objective for generating individual docu-\nments when realism is the main concern. In con-\ntrast, GAN based models may more easily pass a\nTuring test on a system level, or in a conversational\ncontext. When the judges have seen enough exam-\nples from the same generator, the next example had\nbetter be somewhat different.\nOur results also suggest that when adversarial\nevaluation is used, the training examples must be\ncarefully selected to avoid false-positives. We also\nﬁnd that when humans are distinguishing fake re-\nviews from real ones, they tend to focus more on the\nusage of words, expressions, emotions, and other\ndetails. This may affect the design of objectives for\nthe next generation of NLG models.\nAcknowledgement\nWe thank Wei Ai for his help on the power analysis,\nand Yue Wang and Teng Ye for helpful discussions.\nThis work is in part supported by the National Sci-\nence Foundation under grant numbers 1633370 and\n1620319 and by the National Library of Medicine\nunder grant number 2R01LM010681-05.\n3977\nReferences\nKevin Bache, David Newman, and Padhraic Smyth.\n2013. Text-based measures of document diversity.\nIn Proceedings of the 19th ACM SIGKDD interna-\ntional conference on Knowledge discovery and data\nmining, pages 23–31. ACM.\nPhilip Bachman and Doina Precup. 2015. Data gener-\nation as sequential decision making. In Advances\nin Neural Information Processing Systems , pages\n3249–3257.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings\nof the acl workshop on intrinsic and extrinsic evalu-\nation measures for machine translation and/or sum-\nmarization, pages 65–72.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. 2015. Scheduled sampling for se-\nquence prediction with recurrent neural networks.\nIn Advances in Neural Information Processing Sys-\ntems, pages 1171–1179.\nYoshua Bengio, J ´erˆome Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th annual international confer-\nence on machine learning, pages 41–48. ACM.\nYoshua Bengio, Patrice Simard, and Paolo Frasconi.\n1994. Learning long-term dependencies with gradi-\nent descent is difﬁcult. IEEE transactions on neural\nnetworks, 5(2):157–166.\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous\nspace. In Proceedings of The 20th SIGNLL Confer-\nence on Computational Natural Language Learning,\npages 10–21.\nAndrew Brock, Theodore Lim, James M Ritchie, and\nNick Weston. 2016. Neural photo editing with in-\ntrospective adversarial networks. arXiv preprint\narXiv:1609.07093.\nCameron B Browne, Edward Powley, Daniel White-\nhouse, Simon M Lucas, Peter I Cowling, Philipp\nRohlfshagen, Stephen Tavener, Diego Perez, Spyri-\ndon Samothrakis, and Simon Colton. 2012. A sur-\nvey of monte carlo tree search methods.IEEE Trans-\nactions on Computational Intelligence and AI in\ngames, 4(1):1–43.\nElia Bruni and Raquel Fern ´andez. 2017. Adversarial\nevaluation for open-domain dialogue generation. In\nProceedings of the 18th Annual SIGdial Meeting on\nDiscourse and Dialogue, pages 284–288.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin.\n2018. Language gans falling short. arXiv preprint\narXiv:1811.02549.\nChris Callison-Burch, Cameron Fordyce, Philipp\nKoehn, Christof Monz, and Josh Schroeder. 2008.\nFurther meta-evaluation of machine translation. In\nProceedings of the third workshop on statistical ma-\nchine translation , pages 70–106. Association for\nComputational Linguistics.\nChris Callison-Burch, Miles Osborne, and Philipp\nKoehn. 2006. Re-evaluation the role of bleu in ma-\nchine translation research. In 11th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics.\nTong Che, Yanran Li, Ruixiang Zhang, R Devon\nHjelm, Wenjie Li, Yangqiu Song, and Yoshua Ben-\ngio. 2017. Maximum-likelihood augmented discrete\ngenerative adversarial networks. arXiv preprint\narXiv:1702.07983.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nTianqi Chen and Carlos Guestrin. 2016. Xgboost: A\nscalable tree boosting system. In Proceedings of\nthe 22nd acm sigkdd international conference on\nknowledge discovery and data mining , pages 785–\n794. ACM.\nErik Christensen. 2007. Methodology of superiority vs.\nequivalence trials and non-inferiority trials. Journal\nof hepatology, 46(5):947–954.\nOndˇrej C ´ıfka, Aliaksei Severyn, Enrique Alfonseca,\nand Katja Filippova. 2018. Eval all, trust a few,\ndo wrong to none: Comparing sentence generation\nmodels. arXiv preprint arXiv:1804.07972.\nCorinna Cortes and Vladimir Vapnik. 1995. Support-\nvector networks. Machine learning, 20(3):273–297.\nBo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin.\n2017. Towards diverse and natural image descrip-\ntions via a conditional gan. In Proceedings of the\nIEEE International Conference on Computer Vision,\npages 2970–2979.\nRobert Dale and Chris Mellish. 1998. Towards evalu-\nation in natural language generation. In In Proceed-\nings of First International Conference on Language\nResources and Evaluation.\nWayne W Daniel et al. 1978. Applied nonparametric\nstatistics. Houghton Mifﬂin.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\n3978\nLi Dong, Shaohan Huang, Furu Wei, Mirella Lapata,\nMing Zhou, and Ke Xu. 2017. Learning to generate\nproduct reviews from attributes. In Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume\n1, Long Papers, volume 1, pages 623–632.\nDumitru Erhan, Yoshua Bengio, Aaron Courville,\nPierre-Antoine Manzagol, Pascal Vincent, and Samy\nBengio. 2010. Why does unsupervised pre-training\nhelp deep learning? Journal of Machine Learning\nResearch, 11(Feb):625–660.\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via ﬁlling in\nthe\n. arXiv preprint arXiv:1801.07736.\nJessica Ficler and Yoav Goldberg. 2017. Controlling\nlinguistic style aspects in neural language genera-\ntion. EMNLP 2017, page 94.\nEdgar C Fieller, Herman O Hartley, and Egon S Pear-\nson. 1957. Tests for rank correlation coefﬁcients. i.\nBiometrika, 44(3/4):470–481.\nJoseph L Fleiss, Bruce Levin, and Myunghee Cho Paik.\n2013. Statistical methods for rates and proportions.\nJohn Wiley & Sons.\nDaniela Gerz, Ivan Vuli´c, Edoardo Ponti, Jason Narad-\nowsky, Roi Reichart, and Anna Korhonen. 2018.\nLanguage modeling for morphologically rich lan-\nguages: Character-aware modeling for word-level\nprediction. Transactions of the Association of Com-\nputational Linguistics, 6:451–465.\nIan Goodfellow. 2016. Nips 2016 tutorial: Gen-\nerative adversarial networks. arXiv preprint\narXiv:1701.00160.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Advances in neural information\nprocessing systems, pages 2672–2680.\nYvette Graham and Timothy Baldwin. 2014. Testing\nfor signiﬁcance of increased correlation with human\njudgment. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning, pages 172–176.\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850.\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong\nYu, and Jun Wang. 2018. Long text generation\nvia adversarial training with leaked information. In\nThirty-Second AAAI Conference on Artiﬁcial Intelli-\ngence.\nKelvin Guu, Tatsunori B Hashimoto, Yonatan Oren,\nand Percy Liang. 2018. Generating sentences by\nediting prototypes. Transactions of the Association\nof Computational Linguistics, 6:437–450.\nDavid Hardcastle and Donia Scott. 2008. Can we evalu-\nate the quality of generated text? In LREC. Citeseer.\nDonna K Harman and Ellen M V oorhees. 2006. Trec:\nAn overview. Annual review of information science\nand technology, 40(1):113–155.\nTatsunori B Hashimoto, Hugh Zhang, and Percy Liang.\n2019. Unifying human and statistical evaluation\nfor natural language generation. arXiv preprint\narXiv:1904.02792.\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi,\nJ¨urgen Schmidhuber, et al. 2001. Gradient ﬂow in\nrecurrent nets: the difﬁculty of learning long-term\ndependencies.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nFerenc Husz´ar. 2015. How (not) to train your genera-\ntive model: Scheduled sampling, likelihood, adver-\nsary? arXiv preprint arXiv:1511.05101.\nEric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-\nical reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144.\nFred Jelinek, Robert L Mercer, Lalit R Bahl, and\nJames K Baker. 1977. Perplexity?a measure of the\ndifﬁculty of speech recognition tasks. The Journal\nof the Acoustical Society of America , 62(S1):S63–\nS63.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nAnjuli Kannan and Oriol Vinyals. 2017. Adversar-\nial evaluation of dialogue models. arXiv preprint\narXiv:1701.08198.\nYoon Kim. 2014. Convolutional neural net-\nworks for sentence classiﬁcation. arXiv preprint\narXiv:1408.5882.\nDiederik P Kingma and Max Welling. 2013. Auto-\nencoding variational bayes. arXiv preprint\narXiv:1312.6114.\nAlex M Lamb, Anirudh Goyal ALIAS PARTH\nGOY AL, Ying Zhang, Saizheng Zhang, Aaron C\nCourville, and Yoshua Bengio. 2016. Professor forc-\ning: A new algorithm for training recurrent net-\nworks. In Advances In Neural Information Process-\ning Systems, pages 4601–4609.\nYann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick\nHaffner. 1998. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE ,\n86(11):2278–2324.\n3979\nJiwei Li, Will Monroe, Tianlin Shi, S ´ebastien Jean,\nAlan Ritter, and Dan Jurafsky. 2017. Adversarial\nlearning for neural dialogue generation. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 2157–2169.\nAndy Liaw, Matthew Wiener, et al. 2002. Classiﬁ-\ncation and regression by randomforest. R news ,\n2(3):18–22.\nChin-Yew Lin. 2004. Rouge: A package for auto-\nmatic evaluation of summaries. Text Summarization\nBranches Out.\nKevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang,\nand Ming-Ting Sun. 2017. Adversarial ranking for\nlanguage generation. In Advances in Neural Infor-\nmation Processing Systems, pages 3155–3165.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow not to evaluate your dialogue system: An em-\npirical study of unsupervised evaluation metrics for\ndialogue response generation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2122–2132.\nAdam Lopez. 2012. Putting human assessments of ma-\nchine translation systems in order. InProceedings of\nthe Seventh Workshop on Statistical Machine Trans-\nlation, pages 1–9. Association for Computational\nLinguistics.\nRyan Lowe, Michael Noseworthy, Iulian Vlad Ser-\nban, Nicolas Angelard-Gontier, Yoshua Bengio, and\nJoelle Pineau. 2017. Towards an automatic turing\ntest: Learning to evaluate dialogue responses. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1116–1126.\nSidi Lu, Yaoming Zhu, Weinan Zhang, Jun Wang, and\nYong Yu. 2018. Neural text generation: Past, present\nand beyond. arXiv preprint arXiv:1803.07133.\nFujun Luan, Sylvain Paris, Eli Shechtman, and Kavita\nBala. 2017. Deep photo style transfer. CoRR,\nabs/1703.07511.\nPauline Luc, Camille Couprie, Soumith Chintala,\nand Jakob Verbeek. 2016. Semantic segmenta-\ntion using adversarial networks. arXiv preprint\narXiv:1611.08408.\nComputing Machinery. 1950. Computing machinery\nand intelligence-am turing. Mind, 59(236):433.\nChris Mellish and Robert Dale. 1998. Evaluation in the\ncontext of natural language generation. Computer\nSpeech & Language, 12(4):349–373.\nTom´aˇs Mikolov, Stefan Kombrink, Luk ´aˇs Burget,\nJan ˇCernock`y, and Sanjeev Khudanpur. 2011a.\nExtensions of recurrent neural network language\nmodel. In Acoustics, Speech and Signal Processing\n(ICASSP), 2011 IEEE International Conference on ,\npages 5528–5531. IEEE.\nTomas Mikolov, Stefan Kombrink, Anoop Deoras,\nLukar Burget, and Jan Cernocky. 2011b. Rnnlm-\nrecurrent neural network language modeling toolkit.\nIn Proc. of the 2011 ASRU Workshop , pages 196–\n201.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nSLT, 12:234–239.\nEhsan Montahaei, Danial Alihosseini, and Mahdieh So-\nleymani Baghshah. 2019. Jointly measuring diver-\nsity and quality in text generation models. arXiv\npreprint arXiv:1904.03971.\nVaishnavh Nagarajan, Colin Raffel, and Ian J. Good-\nfellow. Theoretical Insights into Memorization in\nGANs.\nMyle Ott, Yejin Choi, Claire Cardie, and Jeffrey T\nHancock. 2011. Finding deceptive opinion spam\nby any stretch of the imagination. In Proceed-\nings of the 49th Annual Meeting of the Association\nfor Computational Linguistics: Human Language\nTechnologies-Volume 1, pages 309–319. Association\nfor Computational Linguistics.\nMatteo Pagliardini, Prakhar Gupta, and Martin Jaggi.\n2018. Unsupervised learning of sentence embed-\ndings using compositional n-gram features. In Pro-\nceedings of NAACL-HLT, pages 528–540.\nNicolas Papernot, Patrick McDaniel, Ian Goodfellow,\nSomesh Jha, Z Berkay Celik, and Ananthram Swami.\n2017. Practical black-box attacks against machine\nlearning. In Proceedings of the 2017 ACM on Asia\nconference on computer and communications secu-\nrity, pages 506–519. ACM.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nMark A. Przybocki, Kay Peterson, Sebastien Bronsart,\nand Gregory A. Sanders. 2009. The NIST 2008\nmetrics for machine translation challenge - overview,\nmethodology, metrics, and results. Machine Trans-\nlation, 23(2-3):71–103.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language understand-\ning paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1:8.\n3980\nSai Rajeswar, Sandeep Subramanian, Francis Dutil,\nChristopher Pal, and Aaron Courville. 2017. Adver-\nsarial generation of natural language. arXiv preprint\narXiv:1705.10929.\nEhud Reiter and Anja Belz. 2009. An investigation into\nthe validity of some metrics for automatically evalu-\nating natural language generation systems. Compu-\ntational Linguistics, 35(4):529–558.\nEhud Reiter, Roma Robertson, A Scott Lennox, and\nLiesl Osman. 2001. Using a randomised controlled\nclinical trial to evaluate an nlg system. In Proceed-\nings of the 39th Annual Meeting on Association for\nComputational Linguistics, pages 442–449. Associ-\nation for Computational Linguistics.\nIrina Rish. 2001. An empirical study of the naive bayes\nclassiﬁer. In IJCAI 2001 workshop on empirical\nmethods in artiﬁcial intelligence , volume 3, pages\n41–46. IBM.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba,\nVicki Cheung, Alec Radford, and Xi Chen. 2016.\nImproved techniques for training gans. In Advances\nin Neural Information Processing Systems , pages\n2234–2242.\nJ¨urgen Schmidhuber and Sepp Hochreiter. 1997. Long\nshort-term memory. Neural Comput , 9(8):1735–\n1780.\nStanislau Semeniuta, Aliaksei Severyn, and Syl-\nvain Gelly. 2018. On accurate evaluation of\ngans for language generation. arXiv preprint\narXiv:1806.04936.\nShikhar Sharma, Layla El Asri, Hannes Schulz, and\nJeremie Zumer. 2017. Relevance of unsuper-\nvised metrics in task-oriented dialogue for evalu-\nating natural language generation. arXiv preprint\narXiv:1706.09799.\nZhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing\nHuang. 2018. Toward diverse text generation with\ninverse reinforcement learning. In Proceedings of\nthe 27th International Joint Conference on Artiﬁcial\nIntelligence, pages 4361–4367. AAAI Press.\nNasim Souly, Concetto Spampinato, and Mubarak\nShah. 2017. Semi and weakly supervised semantic\nsegmentation using generative adversarial network.\narXiv preprint arXiv:1703.09695.\nCharles Spearman. 1904. The proof and measurement\nof association between two things. The American\njournal of psychology, 15(1):72–101.\nAkash Srivastava, Lazar Valkoz, Chris Russell,\nMichael U Gutmann, and Charles Sutton. 2017. Vee-\ngan: Reducing mode collapse in gans using implicit\nvariational learning. In Advances in Neural Informa-\ntion Processing Systems, pages 3310–3320.\nRupesh Kumar Srivastava, Klaus Greff, and J ¨urgen\nSchmidhuber. 2015. Highway networks. arXiv\npreprint arXiv:1505.00387.\nRichard S Sutton, David A McAllester, Satinder P\nSingh, and Yishay Mansour. 2000. Policy gradient\nmethods for reinforcement learning with function ap-\nproximation. In Advances in neural information pro-\ncessing systems, pages 1057–1063.\nYaniv Taigman, Adam Polyak, and Lior Wolf.\n2016. Unsupervised cross-domain image genera-\ntion. arXiv preprint arXiv:1611.02200.\nJian Tang, Yifan Yang, Sam Carton, Ming Zhang, and\nQiaozhu Mei. 2016. Context-aware natural lan-\nguage generation with recurrent neural networks.\narXiv preprint arXiv:1611.09900.\nGuy Tevet, Gavriel Habib, Vered Shwartz, and\nJonathan Berant. 2018. Evaluating text gans as lan-\nguage models. arXiv preprint arXiv:1810.12686.\nL Theis, A van den Oord, and M Bethge. 2016. A note\non the evaluation of generative models. In Inter-\nnational Conference on Learning Representations\n(ICLR 2016), pages 1–10.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4566–4575.\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. In Reinforcement Learning, pages\n5–32. Springer.\nRonald J Williams and David Zipser. 1989. A learn-\ning algorithm for continually running fully recurrent\nneural networks. Neural computation , 1(2):270–\n280.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. 2015. Show, attend and tell:\nNeural image caption generation with visual at-\ntention. In International Conference on Machine\nLearning, pages 2048–2057.\nDenis Yarats and Mike Lewis. 2018. Hierarchical text\ngeneration and planning for strategic dialogue. InIn-\nternational Conference on Machine Learning, pages\n5587–5595.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n2017. Seqgan: Sequence generative adversarial nets\nwith policy gradient. In AAAI, pages 2852–2858.\nYizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo\nHenao, Dinghan Shen, and Lawrence Carin. 2017.\nAdversarial feature matching for text generation. In\nProceedings of the 34th International Conference\non Machine Learning-Volume 70, pages 4006–4015.\nJMLR. org.\nJun-Yan Zhu, Philipp Kr¨ahenb¨uhl, Eli Shechtman, and\nAlexei A Efros. 2016. Generative visual manip-\nulation on the natural image manifold. In Euro-\npean Conference on Computer Vision , pages 597–\n613. Springer.\n3981\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\nEfros. 2017. Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. arXiv\npreprint arXiv:1703.10593.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,\nWeinan Zhang, Jun Wang, and Yong Yu. 2018. Texy-\ngen: A benchmarking platform for text generation\nmodels. In The 41st International ACM SIGIR Con-\nference on Research & Development in Information\nRetrieval, pages 1097–1100. ACM.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6667313575744629
    },
    {
      "name": "Natural language processing",
      "score": 0.584417998790741
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5791714787483215
    },
    {
      "name": "Natural language",
      "score": 0.5496618747711182
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4703923463821411
    },
    {
      "name": "Artificial neural network",
      "score": 0.4423864185810089
    },
    {
      "name": "Joint (building)",
      "score": 0.4409765303134918
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.43220239877700806
    },
    {
      "name": "Engineering",
      "score": 0.17923378944396973
    },
    {
      "name": "History",
      "score": 0.14067280292510986
    },
    {
      "name": "Geography",
      "score": 0.0998060405254364
    },
    {
      "name": "Archaeology",
      "score": 0.08995738625526428
    },
    {
      "name": "Cartography",
      "score": 0.08298113942146301
    },
    {
      "name": "Civil engineering",
      "score": 0.062069088220596313
    }
  ]
}