{
    "title": "CRAFT: Camera-Radar 3D Object Detection with Spatio-Contextual Fusion Transformer",
    "url": "https://openalex.org/W4382469124",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2098920907",
            "name": "Young-seok Kim",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3069555433",
            "name": "Sanmin Kim",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2137569899",
            "name": "Jun Won Choi",
            "affiliations": [
                "Hanyang University"
            ]
        },
        {
            "id": "https://openalex.org/A2034556142",
            "name": "Dongsuk Kum",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2098920907",
            "name": "Young-seok Kim",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3069555433",
            "name": "Sanmin Kim",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2034556142",
            "name": "Dongsuk Kum",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4221166211",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W2798410215",
        "https://openalex.org/W3042403800",
        "https://openalex.org/W4283027141",
        "https://openalex.org/W1506558619",
        "https://openalex.org/W3130190409",
        "https://openalex.org/W2774996270",
        "https://openalex.org/W2905076052",
        "https://openalex.org/W6753494528",
        "https://openalex.org/W4226521892",
        "https://openalex.org/W3089778819",
        "https://openalex.org/W2893047521",
        "https://openalex.org/W4221158448",
        "https://openalex.org/W6799702355",
        "https://openalex.org/W3168472259",
        "https://openalex.org/W3151531796",
        "https://openalex.org/W2996476754",
        "https://openalex.org/W2995042771",
        "https://openalex.org/W2995549977",
        "https://openalex.org/W6801377519",
        "https://openalex.org/W3118479569",
        "https://openalex.org/W6787910462",
        "https://openalex.org/W3048770793",
        "https://openalex.org/W6745673378",
        "https://openalex.org/W2950642167",
        "https://openalex.org/W2624503621",
        "https://openalex.org/W3133598367",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2903870443",
        "https://openalex.org/W3173916380",
        "https://openalex.org/W2938428612",
        "https://openalex.org/W2925359305",
        "https://openalex.org/W4229030786",
        "https://openalex.org/W2989996808",
        "https://openalex.org/W3183579734",
        "https://openalex.org/W3154510993",
        "https://openalex.org/W6772381481",
        "https://openalex.org/W6773990164",
        "https://openalex.org/W6779712747",
        "https://openalex.org/W3107819843",
        "https://openalex.org/W6742028206",
        "https://openalex.org/W6760424586",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2968296999",
        "https://openalex.org/W2963121255",
        "https://openalex.org/W2963400571",
        "https://openalex.org/W3035574168",
        "https://openalex.org/W2963488291",
        "https://openalex.org/W4214558638",
        "https://openalex.org/W2990613095",
        "https://openalex.org/W2949708697",
        "https://openalex.org/W3108426750",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3166470370",
        "https://openalex.org/W2560609797",
        "https://openalex.org/W3173668541",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4214624153",
        "https://openalex.org/W3167095230",
        "https://openalex.org/W4312707458",
        "https://openalex.org/W3215100485",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3034772996",
        "https://openalex.org/W4287125269",
        "https://openalex.org/W3129529899",
        "https://openalex.org/W2600383743",
        "https://openalex.org/W2982770724",
        "https://openalex.org/W3169690993",
        "https://openalex.org/W4394657967",
        "https://openalex.org/W4287021060",
        "https://openalex.org/W4308068569",
        "https://openalex.org/W2964062501",
        "https://openalex.org/W4312894406",
        "https://openalex.org/W3035461736",
        "https://openalex.org/W4225793049",
        "https://openalex.org/W2963323244",
        "https://openalex.org/W2886335102",
        "https://openalex.org/W3006754479",
        "https://openalex.org/W3109395584"
    ],
    "abstract": "Camera and radar sensors have significant advantages in cost, reliability, and maintenance compared to LiDAR. Existing fusion methods often fuse the outputs of single modalities at the result-level, called the late fusion strategy. This can benefit from using off-the-shelf single sensor detection algorithms, but late fusion cannot fully exploit the complementary properties of sensors, thus having limited performance despite the huge potential of camera-radar fusion. Here we propose a novel proposal-level early fusion approach that effectively exploits both spatial and contextual properties of camera and radar for 3D object detection. Our fusion framework first associates image proposal with radar points in the polar coordinate system to efficiently handle the discrepancy between the coordinate system and spatial properties. Using this as a first stage, following consecutive cross-attention based feature fusion layers adaptively exchange spatio-contextual information between camera and radar, leading to a robust and attentive fusion. Our camera-radar fusion approach achieves the state-of-the-art 41.1% mAP and 52.3% NDS on the nuScenes test set, which is 8.7 and 10.8 points higher than the camera-only baseline, as well as yielding competitive performance on the LiDAR method.",
    "full_text": "CRAFT: Camera-Radar 3D Object Detection\nwith Spatio-Contextual Fusion Transformer\nYoungseok Kim1, Sanmin Kim1, Jun Won Choi2, Dongsuk Kum1\n1Korea Advanced Institute of Science and Technology\n2Hanyang University\n{youngseok.kim, sanmin.kim, dskum}@kaist.ac.kr, junwchoi@hanyang.ac.kr\nAbstract\nCamera and radar sensors have significant advantages in cost,\nreliability, and maintenance compared to LiDAR. Existing fu-\nsion methods often fuse the outputs of single modalities at the\nresult-level, called the late fusion strategy. This can benefit\nfrom using off-the-shelf single sensor detection algorithms,\nbut late fusion cannot fully exploit the complementary prop-\nerties of sensors, thus having limited performance despite the\nhuge potential of camera-radar fusion. Here we propose a\nnovel proposal-level early fusion approach that effectively ex-\nploits both spatial and contextual properties of camera and\nradar for 3D object detection. Our fusion framework first as-\nsociates image proposal with radar points in the polar coordi-\nnate system to efficiently handle the discrepancy between the\ncoordinate system and spatial properties. Using this as a first\nstage, following consecutive cross-attention based feature fu-\nsion layers adaptively exchange spatio-contextual informa-\ntion between camera and radar, leading to a robust and atten-\ntive fusion. Our camera-radar fusion approach achieves the\nstate-of-the-art 41.1% mAP and 52.3% NDS on the nuScenes\ntest set, which is 8.7 and 10.8 points higher than the camera-\nonly baseline, as well as yielding competitive performance on\nthe LiDAR method.\n1 Introduction\n3D object detection is an essential task for autonomous driv-\ning as well as mobile robots. Thanks to the low-cost, high-\nreliability, and low-maintenance of camera and radar sen-\nsors, they are already deployed to a large number of mass\nproduction vehicles for active safety systems. Moreover, the\nrich semantic information of the camera and long-range de-\ntection with weather condition robustness of radar are es-\nsential attributes that LiDAR cannot provide. Nevertheless,\nlearning-based camera-radar 3D object detection (Lim et al.\n2019; Kim, Choi, and Kum 2020; Nabati and Qi 2021) for\nautonomous driving has not been thoroughly explored.\nExisting camera-radar fusion methods often use a late fu-\nsion strategy that fuses detection-level outputs of separated\ncamera and radar detection algorithms with heuristic logic.\nSuch methods can have the benefit of exploiting the off-\nthe-shelf detection algorithms that are independently devel-\noped by automotive suppliers as modular components. How-\nCopyright ¬© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nVelocity\nInfo.\nDetection Range\n3D \nDepth\nInfo.\nResolution\nIllumination\nRobustness\nWeather Robustness\nSize & \nCost\nSem-\nantic\nInfo.Camera\nRadar Camera + Radar\nLiDAR\nFigure 1: Sensor characteristics of camera, radar, and Li-\nDAR. Camera-radar fusion has high potential considering\nspatially and contextually complementary properties.\never, late fusion strategies that rely on heuristics and post-\nprocessing techniques suffer from performance-reliability\ntrade-offs, especially when two sensor predictions disagree;\nthus, late fusion cannot exploit the full potential of each sen-\nsor. In contrast, a learning-based early fusion strategy that\nfuses the intermediate information of each sensor at an early\nstage has much higher potential, but it requires an in-depth\nunderstanding of each sensor‚Äôs characteristics to find the\n‚Äúoptimal‚Äù way to fuse.\nHowever, it is non-trivial to develop early fusion for cam-\nera and radar due to the unique characteristics of each sen-\nsor, as illustrated in Fig. 1. Camera-LiDAR early fusion\nmethods, which are relatively well-studied, fuse point-wise\nfeatures (V ora et al. 2020; Huang et al. 2020) by associat-\ning image pixel with projected LiDAR point or fuse image\nand LiDAR proposal features (Ku et al. 2018) by extract-\ning a region of interests (RoIs) with RoIPool (Ren et al.\n2015). However, adapting camera-LiDAR fusion methods\nfor camera-radar is unsuitable since those strategies are es-\ntablished by accurate LiDAR measurement (¬±2cm ), but\nradar has low accuracy and measurement ambiguities. Due\nto the nature of the radar mechanism, the radar has high\nresolution and accuracy in the radial direction (0.4m and\n¬±0.1m), which is measured by Fast Fourier Transformation\nFFT. Meanwhile, the azimuthal measurement obtained by\ndigital beamforming using multiple receive antennas (John-\nson and Dudgeon 1992) is inaccurate (4.5 ‚ó¶ and ¬±1‚ó¶, ap-\nproximately 4m and ¬±1m at 50m distance). Measurement\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n1160\nambiguities are the cases when radar points are occasion-\nally missed on objects (false negatives due to the low radar\ncross-section) or shown in the background (false positives\ndue to radar multi-path problem), thus these have to be con-\nsidered. The camera, meanwhile, has very complementary\nspatial characteristics to radar (Ma et al. 2021; Hung et al.\n2022). Dense camera pixels provide accurate azimuthal res-\nolution and accuracy, but the depth information is not pro-\nvided due to the perspective projection.\nMotivated by the aforementioned challenges of camera\nand radar, camera-radar fusion has to be able to 1) robustly\noperate even radar points are not reflected from the object\nand 2) effectively complement spatial (range/azimuth) and\ncontextual (semantic/Doppler) information. To this end, we\npresent Camera RAdar Fusion Transformer (CRAFT), a ro-\nbust and effective fusion framework for 3D object detection.\nWe propose the soft association strategy between image pro-\nposal and radar points and cross-attention based feature fu-\nsion method, which can effectively exploit spatial charac-\nteristics and robustly handle radar measurement ambiguity.\nSpecifically, we first extract camera and radar features by\nmodality-specific feature extractors (i.e., DLA-34 (Yu et al.\n2018), PointNet++ (Qi et al. 2017a)) and predict 3D ob-\njects with a lightweight camera 3D object detector. Given\nimage proposals and radar points, we associate image pro-\nposal with radar points by Soft Polar Association, which\nqueries points within the uncertainty-aware adaptive ellip-\nsoidal threshold in the polar coordinate. To deal with the\nwrongly associated radar points (background clutters), we\nthen attentively fuse the image proposal feature and radar\npoint features by consecutive cross-attention based encoder\nlayers. Our Spatio-Contextual Fusion Transformer can ef-\nfectively exchange spatial and contextual information and\nadaptively determine where and what information to use for\nfusion. Finally, the class-specific decoder head predicts fu-\nsion score and offsets in the polar coordinate to refine the\nimage proposal. Since some objects do not have valid radar\npoints, fused output with a low fusion score is discarded, and\nthe image proposal is used as the final prediction.\nTo summarize, our contributions are as follows:\n‚Ä¢ We investigate the characteristics of camera-radar and\npropose a proposal-level early fusion framework that can\nmitigate the coordinate system discrepancy and measure-\nment ambiguities.\n‚Ä¢ We propose Soft Polar Association and Spatio-\nContextual Fusion Transformer that effectively ex-\nchanges complementary information between camera\nand radar features.\n‚Ä¢ We achieve the state-of-the-art 41.1% mAP and 52.3%\nNDS on nuScenes test set, which significantly boosts the\ncamera-only baseline with a marginal additional compu-\ntation cost.\n2 Related Work\n2.1 Camera and Radar for 3D Object Detection\nWith an advance in 2D object detection (Ren et al.\n2015; Tian et al. 2019), image view approaches extend\na 2D detector with additional 3D regression branches.\nCenterNet (Zhou, Wang, and Kr ¬®ahenb¬®uhl 2019) and\nFCOS3D (Wang et al. 2021b) directly regress depth to the\nobject and 3D size from its center without using an anchor,\nwhich is later improved in PGD (Wang et al. 2021a) by\nutilizing geometric prior. Image view approaches can well\nexploit GPU-friendly operations and therefore runs fast but\nsuffer from inaccurate depth estimation, which is the natu-\nrally ill-posed problem.\nAnother approaches are to transform image features in\nthe perspective view into a top-down view and predict 3D\nbounding boxes from the BEV features using LiDAR de-\ntection heads (Lang et al. 2019; Yin, Zhou, and Krahen-\nbuhl 2021). CaDDN (Reading et al. 2021) and LSS (Phil-\nion and Fidler 2020) generate the BEV features using the\ndepth distribution, and PETR (Liu et al. 2022) and BEV-\nFormer (Li et al. 2022) generate the BEV features by us-\ning pre-defined grid-shaped BEV queries to image features.\nSuch BEV methods can utilize more depth information dur-\ning training, thus showing better localization performance,\nbut operates slow.\nRadar data can have various representations such as 2-D\nFFT (Lin et al. 2018), Range-Azimuth-Doppler Tensor (Ma-\njor et al. 2019; Kim, Kim, and Kum 2020); and a radar\npoint cloud (Caesar et al. 2020; Meyer and Kuschk 2019a)\nis an affordable representation for autonomous driving ap-\nplications. Although the radar point cloud is similar to Li-\nDAR, naively adopting LiDAR methods to radar is inap-\npropriate. (Ulrich et al. 2022) adapt PointPillars with KP-\nConv (Thomas et al. 2019) and Radar-PointGNN (Sven-\nningsson, Fioranelli, and Yarovoy 2021) adapt GNN to\nradar, but their performances are much inferior to LiDAR\nmethods. Although the high potential of radar, radars have\nnot yet been thoroughly investigated in autonomous driving.\n2.2 Camera-LiDAR for 3D Object Detection\nCamera-LiDAR fusion has gained significant interest for 3D\ndetection, and existing approaches can be roughly classi-\nfied into result-, proposal- and point-level fusion. Thanks to\nthe rich intermediate feature representation of the learning-\nbased method, a line of work (Ku et al. 2018; Yoo et al.\n2020) fuses RoI proposal features to complement two\nmodalities‚Äô information. Recent works (V ora et al. 2020;\nBai et al. 2022) further propose point-level fusion that can\nfuse more fine-grained features at an earlier stage. However,\nmethods using LiDAR are established on the strong assump-\ntion that LiDAR measurement and camera-LiDAR calibra-\ntion is accurate, which is not valid under a radar setting.\n2.3 Camera-Radar Fusion\nOnly a few methods attempt to detect 3D objects using cam-\nera radar fusion in autonomous driving. (Lim et al. 2019)\ntransforms the camera image into BEV using inverse per-\nspective mapping, assuming a planar road scene such as\na highway to mitigate the coordinate discrepancy between\ncamera and radar. (Meyer and Kuschk 2019b; Kim, Kim,\nand Kum 2020) adapt A VOD (Ku et al. 2018) to fuse front\nview camera image with point- and tensor-like radar data,\n1161\nCamera RGB Images\n(b) Spatio-Contextual\nFusion Transformer\nCamera Backbone\nCamera 3D Detector\nRadar Point Cloud\nRadar Backbone\nFinal Predictions\n(c) Fusion\nDetection\nHeads\nBEV NMS\nRadar-to-Image \nFeature Encoder\nùêæùêæ, ùëâùëâ\nùêæùêæ, ùëâùëâ\nImage-to-Radar \nFeature Encoder\nùëÑùëÑ\nùëÑùëÑ\nAdaptive Radial\nThreshold\n ùúëùúë\nùëüùëü\nAzimuthal\nThreshold\nRadar \nPoint \nFeature\nPatched\nImage\nFeatures\nAssociated\nRadar\nFeatures\nFused \nProposal\nFeature\nImage \nProposal\nFeature\n(a) Soft Polar \nAssociation\nFigure 2: Overall architecture of CRAFT. Modality-specific backbones extract features, and camera 3D detector predicts 3D\nimage proposals. (a) Given the image proposals, we associate radar points around the image proposals using adaptive thresholds\nin polar coordinates. (b) Then, consecutive cross-attention layers adaptively complement spatial and contextual information of\ncamera and radar features. (c) Finally, fusion detection heads predict fusion score and offset to refine the image proposal and to\noutput final predictions.\nrespectively. GRIF Net (Kim, Choi, and Kum 2020) fur-\nther proposes a gating mechanism to adaptively fuse camera\nand radar RoI features handling the radar sparsity. Center-\nFusion (Nabati and Qi 2021) lifts image proposals into a 3D\nfrustum and associates a single closest radar point inside RoI\nto fuse. Thus, existing camera-radar 3D detectors have not\nthoroughly considered the spatial properties of camera and\nradar.\nSome literature exploits camera and radar for depth com-\npletion tasks considering radar sparsity, inaccurate measure-\nment, and ambiguity. (Lin, Dai, and Van Gool 2020) pro-\nposes a radar noise filtering module using a space-increasing\ndiscretization (SID) threshold to filter outlier noises, then\nuses filtered radar points to improve depth prediction. RC-\nPDA (Long et al. 2021b) further proposes pixel depth asso-\nciation to find the one-to-many mapping between radar point\nand image pixels that filter and densify radar depth map. In\nour work, we explore a robust fusion method using an atten-\ntion mechanism to handle these limitations of radar effec-\ntively.\n3 Method\nIn the camera-radar fusion-based 3D detection task, we take\nsurrounding images and radar point clouds with correspond-\ning intrinsics and extrinsics as input. Given camera feature\nmaps from the backbone, camera detection heads first gen-\nerate object proposals with a 3D bounding box. A Soft Po-\nlar Association (SPA)module then associates radar points\nwith object proposals using adaptive thresholds in the po-\nlar coordinate to handle the spatial characteristics of sen-\nsors effectively. Further, a Spatio-Contextual Fusion Trans-\nformer (SCFT)fuses camera and radar features to comple-\nment spatial and contextual information of a single modal-\nity to another. Finally, Fusion Detection Headsdecode the\nfused object features to refine initial object proposals. An\noverall framework is provided in Fig. 2.\n3.1 Backbones and Camera 3D Object Detector\nGiven N camera images I ‚àà RN√óH√óW√ó3, the 3D detector\naims to classify and localize objects with a set of 3D bound-\ning boxes. Specifically, we feed multi-camera images to the\nbackbone network (e.g., DLA-34 (Yu et al. 2018)) and ob-\ntain downsampled image features F ‚àà RN√óh√ów√óC of mul-\ntiple camera views. Taking camera view featuresF as input,\nthe convolutional detection heads predict the projected 3D\ncenter (u, v) (called keypoint), depth d, and other attributes\nin the image plane. We additionally predict the variance of\ndepth œÉ following (Kendall and Gal 2017) to represent the\nuncertainty of depth regression. Keypoints are transformed\ninto the 3D camera coordinate system with camera intrinsics\n(x‚Ä≤, y‚Ä≤, fu, fv) as follows:\nzc = d, xc = (u ‚àí x‚Ä≤) √ó z\nfu\n, yc = (v ‚àí y‚Ä≤) √ó z\nfv\n, (1)\nthen 3D bounding boxes in each camera coordinate sys-\ntem are transformed into the vehicle coordinate system. The\nbounding box b = (x, y, z, œÉ, w, l, h, Œ∏, v) consists of the 3D\ncenter location (x, y, z) with depth variance œÉ, 3D dimen-\nsion (w, l, h), yaw orientation Œ∏, and velocity v. In the key-\npoint detection setting (Zhou, Wang, and Kr¬®ahenb¬®uhl 2019),\nthe feature at the keypoint fn,u,v ‚àà RC implicitly repre-\nsents the object. Thus, we denote a set of M image propos-\nals {im}M\nm=1, where im =\n\u0002\nbm, fi\nm\n\u0003\nindicates the bounding\nbox and its features. Note that proposing a camera 3D de-\ntector is out of the scope of this paper, and we expect that\nother research progress (Wang et al. 2021b; Park et al. 2021)\ncould further improve our results.\nFor a set of K radar points {pk = [vk, fk]}K\nk=1 with 3D\nlocation vi ‚àà R3 and properties fi ‚àà RF (e.g., radar cross-\nsection (RCS), ego-motion compensated radial velocity), we\nextract the high-dimensional featuresfr\nk ‚àà RC using a point\nfeature extractor (e.g., PointNet++ (Qi et al. 2017b)). As the\n1162\nRadar‚ÄêEncoded¬†\nImage¬†Feature\n+\n+\nN√ó\nLN\nMSA\n+\nLN\nD‚ÄêMCA\n+\nLN\nMLP\n+\n+\nN√ó\nLN\nMSA\n+\nLN\nMCA\n+\nLN\nMLP\nImage‚ÄêEncoded¬†\nPoint¬†Feature\nRadar¬†Point\nFeature\nPatched\nImage\nFeatures\nSampled\nPoint\nFeatures\nImage¬†Proposal\nFeature\nProjection\n2D¬†Learned\nPositional\nEmbedding\n3D¬†Learned\nPositional\nEmbedding\nFigure 3: Architectures of the Image-to-Radar (left) and\nRadar-to-Image (right) Feature Encoder. Radar and im-\nage features exchange spatial and contextual information\nby consecutive feature encoders. We use RGB image for\npatched image features only for visualization.\nnumber of radar points K is considerably small, we dis-\ncard subsampling strategies such as farthest point sampling\n(FPS) (Shi, Wang, and Li 2019; Yang et al. 2020). The de-\ntailed architectures for the camera 3D detector and radar\nbackbone are provided in Appendix A.\n3.2 Soft Polar Association\nSpecifically, we calculate eight corners from the image\nproposal bounding box b and transform corners and radar\npoints in Cartesian coordinate to polar coordinate O(P) =\n{(r, œï, z)m,j}M,8\nm,j=1, v(P) = {(r, œï, z)k}K\nk=1. Inspired by\nBall Query (Qi et al. 2017b), we find a subset of radar points\n{pm = [vi, fi]}K\n‚Ä≤\ni=1 for each object proposal within a certain\ndistance to the image proposal but with adaptive thresholds.\nœïm,l < œïi < œïm,r (2)\nrm,f ‚àí (Œ≥ + œÉ rc\nŒ¥ ) < ri < rm,b + (Œ≥ + œÉ rc\nŒ¥ ) (3)\nEq. 2 is the azimuthal threshold for radar points inpm where\nœïm,l and œïm,r denote the angle of the most left and right cor-\nner points in polar coordinate. The radial threshold is repre-\nsented in Eq. 3, where rm,f , rm,b refer to radial distances of\nmost front and back corner points of the image proposal,Œ≥ is\na minimum range, Œ¥ is a hyper-parameter to modulate the ra-\ndius size, and œÉ is the depth variance of the image proposal.\nrc = rm,f+rm,b\n2 denotes the radial distance of the center\npoint of the image proposal. In this way, the proposed asso-\nciation using an adaptive threshold can maximize the chance\nof using informative foreground radar points while exclud-\ning most clutter points in the azimuthal direction.\n3.3 Spatio-Contextual Fusion Transformer\nIn our camera-radar fusion framework, the primary role of\nradar is to refine the location of the image proposal if there\nare radar returns from the object. The Spatio-Contextual Fu-\nsion Transformer (SCFT) aims to exchange the spatial and\ncontextual information between camera and radar, but de-\ntermining which image pixels corresponding to the radar\npoint is a difficult problem. Thus we design fusion modules\nwith cross-attention layers to make the fusion network learn\nwhere and what information should be taken from image and\nradar.\nImage-to-Radar Feature Encoder For spatio-contextual\nfusion, Image-to-Radar Feature Encoder (I2R FE) first pro-\nvides the semantic information from image to radar. Each\nradar point is projected to the image plane, and then a patch\nis defined around the projected location. Instead of setting a\nfixed size patch, we design the size of the patch to be a func-\ntion of radar distance so that the radar points can attend to\na wider region when the object is closer, which takes more\npixels in the image. The adaptive size is similar to the in-\nverse function of space-increasing discretization (SID) (Fu\net al. 2018):\nœÑ(d) =‚åäW exp(‚àíd/Œ≤ + Œ±)‚åã, (4)\nwhere we heuristically set W = 3.5, Œ± = 2, and Œ≤ = 55.\nThen, œÑ √óœÑ size patched image feature map is resized to the\nfixed size w √ó w using bilinear interpolation.\nInspired by Deformable DETR (Zhu et al. 2021), we\nadopt a deformable multi-head cross-attention (D-MCA)\nmodule to attend to a small set of key sampling points\n(patched image features) around a reference point (projected\nradar pixel). Given a radar query feature fr\nk ‚àà RC, I2R\nFE adaptively extracts features from the patched image fea-\ntures fi\nk ‚àà Rw√ów√óC even radar point is not accurately pro-\njected into the image. However, the attention network de-\nsires proper supervision to learn which image feature is in-\nformative for the fusion effectively. Accordingly, we design\nan auxiliary task for each image-encoded radar feature to\npredict the probability that the radar point is inside the 3D\nbounding box.\nRadar-to-Image Feature Encoder Following Radar-to-\nImage Feature Encoder (R2I FE) provides the spatial infor-\nmation of radar points to image proposal. Unlike I2R FE\noperates in a 2D camera coordinate, R2I FE takes inputs\nin a polar coordinate since a regression target of 3D ob-\nject detection is to predict the location in 3D space. Follow-\ning Transformer methods designed for 3D point cloud (Pan\net al. 2021; Misra, Girdhar, and Joulin 2021), we use a\ncross-attention (Vaswani et al. 2017) with ‚Äôpre-norm‚Äô tech-\nniques (Xiong et al. 2020). Specifically, R2I FE takes the\nimage proposal feature query fi\nm ‚àà RC and a set of radar\npoint features fr\nk‚Ä≤ ‚àà RK\n‚Ä≤\n√óC as input, then produces a\nradar-encoded image proposal feature that is later used to\nrefine the proposal. We additionally add a batch of zeros\nto the key-value sequences (add\nzero attn in PyTorch\nnn.MultiHeadAttention) so that attention can be as-\nsigned to it when none of the associated radar points are\nreflected from the object.\n1163\nMethod Input Backbone NDS‚Üë mAP‚Üë mATE\n‚Üì mASE‚Üì mAOE‚Üì mA VE‚Üì mAAE‚Üì FPS\nPointPillars (2019) L - 45.3 30.5 0.517 0.290\n0.500 0.316 0.368 61\nCenterPoint (2021) L - 67.3 60.3 0.262 0.239\n0.361 0.288 0.136 30\nRadar-PointGNN\n(2021) R - 3.4 0.5 1.024 0.859\n0.897 1.020 0.931 -\nKPConvPillars\n(2022) R - 13.9 4.9 0.823 0.428\n0.607 2.081 1.000 -\nCenterNet (2019) C HGLS 40.0 33.8 0.658 0.255\n0.629 1.629 0.142 -\nFCOS3D‚Ä† (2021b) C R101 42.8 35.8 0.690 0.249\n0.452 1.434 0.124 1.7\nPGD‚Ä† (2021a) C R101 44.8 38.6 0.626 0.245 0.451 1.509\n0.127 1.4\nPETR (2022) C R101‚àó 45.5 39.1 0.647 0.251 0.433 0.933\n0.143 1.7\nBEVFormer\n-S (2022) C R101‚àó 46.2 40.9 0.650 0.261\n0.439 0.925 0.147 -\nCenterFusion‚Ä† (2021) C+R DLA34 44.9 32.6 0.631 0.261\n0.516 0.614 0.115 -\nCRAFT‚Ä† C+R DLA34 52.3 41.1 0.467 0.268 0.456 0.519\n0.114 4.1\nTable 1: State-of-the-art comparison on nuScenestest set. ‚ÄòL‚Äô, ‚ÄòC‚Äô, and ‚ÄòR‚Äô represent LiDAR, camera, and radar, respectively.\n‚àó are initialized with FCOS3D (Wang et al. 2021b) checkpoint and ‚Ä† use image flipping test time augmentation.\n3.4 Detection Heads and Training Objectives\nThe fusion detection head decodes the above radar-encoded\nimage proposal feature to refine the image proposal lo-\ncalization and other attributes in polar coordinate. Specifi-\ncally, category-specific regression heads on the top of shared\nMLP layers predict (a) fusion score, (b) location offsets, (c)\ncenter-ness, and (d) velocity.\n(a) Fusion score: Due to radar sparsity, some image pro-\nposals are associated with only clutter radar points, which\ncan degrade performance. Thus we predict the probability\nof radar points associated with SPA containing at least one\nradar point from the object.\n(b) Location offsets: Given the image proposal center, we\npredict offsets in polar coordinate instead of Cartesian. We\nempirically find that this simple and easy to implement tech-\nnique effectively reduces the significant localization error\ndisagreement in polar coordinates.\n(c) Center-ness: Following (Yang et al. 2020), we assign\nhigher center-ness scores to predictions closer to ground-\ntruth centers.\n(d) Velocity: To mitigate the absence of azimuthal velocity\nin radar Doppler measurement (Long et al. 2021a), we pre-\ndict the speed and then transform it to velocity using object\norientation.\nTo train the model, we assign the same ground-truth used\nfor training the keypoint-based camera 3D object detec-\ntor (Law and Deng 2018) to image proposal; thus it can be\ndirectly matched to the ground truth without an additional\nmatching algorithm (Carion et al. 2020). Our final loss func-\ntion can be formulated as a weighted sum of aforementioned\nclassification and regression losses. The full loss function is\ndetailed in Appendix B.\n4 Experiments\nDataset and Metrics We evaluate our method on a\nlarge-scale and challenging nuScenes dataset (Caesar\net al. 2020), which consists of 700/150/150 scenes for\ntrain/val/test set. Each 20 second long sequence has\n3D bounding box annotations of 10 classes by 2Hz fre-\nquency, and it contains six surrounding camera images, one\nLiDAR point cloud, and five radar point clouds covering\n360 degrees. The official evaluation metrics are mean Av-\nerage Precision (mAP), True Positive metrics (i.e., trans-\nlation, scale, orientation, velocity, and attribute error), and\nthe nuScenes detection score (NDS). Particularly, NDS is a\nweighted sum of mAP and True Positive metrics. The match-\ning thresholds for calculating AP are the center distance of\n0.5m, 1m, 2m, and 4m, instead of IoU.\nImplementation Details We implement our fusion net-\nwork using CenterNet (Zhou, Wang, and Kr¬®ahenb¬®uhl 2019)\nwith DLA34 (Yu et al. 2018) backbone. Note that our cam-\nera 3D object detector is trained from scratch on nuScenes\nwithout large-scale depth pre-training on DDAD15M pro-\nposed in DD3D (Park et al. 2021). For the image backbone,\nwe use 448 √ó 800 size image as network input and use a\nsingle feature map of the last layer for the image proposal\nfeature. For radar, we accumulate six radar sweeps following\nGRIF Net (Kim, Choi, and Kum 2020) and set the maximum\nnumber of radar points as 2048. To maximize the recall of\nthe camera 3D detector, we set the image proposal threshold\nto 0.05 and apply Non-Maximum Suppression (NMS) af-\nter fusion. More detailed implementation details are in Ap-\npendix B.\nTraining and Inference As a proof of concept that the\nproposed method is flexible, we pre-train the camera 3D ob-\nject detector and keep its weights frozen during training the\ncamera-radar fusion network, while other fusion modules\nare trained in an end-to-end manner. Our model takes six\nsurrounding images and five radar point clouds as a single\nbatch to perform 360-degree detection, and data augmenta-\ntion is applied to both image and radar. We train our mod-\nels for 24 epochs with a batch size of 32, cosine annealing\nscheduler, and 2√ó10‚àí4 learning rate on 4 RTX 3090 GPUs.\nInference time is measured on an Intel Core i9 CPU and an\nRTX 3090 GPU without test time augmentation for fusion.\nSee Appendix C for more detailed experimantal settings.\n4.1 Comparison with State-of-the-Arts\nWe compare our method with state-of-the-arts on nuScenes\ntest set. To eliminate the effect of multi-sequence input\nand large-scale depth pre-training and compare with pre-\nvious methods fairly, we report methods with single frame\ninput trained on nuScenes dataset only. Additional compar-\nisons with other methods and results onval set are provided\nin Appendix D.\n1164\nMethod Input Car T\nruck Bus Trailer C.V . Ped. M.C. Bicycle T.C. Barrier mAP\nPointPillars (2019) L 79.9 35.7\n42.8 26.1 5.5 71.7 39.4 10.6 33.4 52.0 39.7\nFCOS3D (2021b) C 47.9 23.3\n31.4 11.2 5.7 41.1 30.5 30.2 55.0 45.5 32.2\nCenterNet (2019) C 48.4 23.1\n34.0 13.1 3.5 37.7 24.9 23.4 55.0 45.6 30.6\nCenterFusion (2021) C+R 52.4(+4.0) 26.5(+3.4) 36.2(+2.2) 15.4(+2.3) 5.5(+2.0) 38.9(+1.2) 30.5(+5.6) 22.9(-0.5) 56.3(+1.3) 47.0(+1.4) 33.2(+2.6)\nCRAFT-I C 52.4 25.7\n30.0 15.8 5.4 39.3 28.6 29.8 57.5 47.8 33.2\nCRAFT C+R 69.6(+17.2) 37.6(+11.9) 47.3(+17.3) 20.1(+4.3) 10.7(+5.3) 46.2(+6.9) 39.5(+10.9) 31.0(+1.2) 57.1(-0.4) 51.1(+3.3) 41.1(+7.9)\nTable 2: Per-class AP comparison on nuScenesval set. ‚ÄòC.V .‚Äô, ‚ÄòM.C.‚Äô, and ‚ÄòT.C.‚Äô denote construction vehicle, motorcycle, and\ntraffic cone, respectively.\n(a) Fusion\nmethod\nAP‚Üë ATE\n‚Üì0.5m mean\nImg. 19.6 52.4\n0.49\nFP 34.7 61.8\n0.37\nFC 38.9 63.1\n0.34\nFT 41.3 65.1\n0.32\n(b) Association\nmethod\nAP‚Üë ATE\n‚Üì Assoc.\nRC0.5m mean\nNo Assoc. OOM 83.5\nRoIPool 29.8 58.7\n0.39 50.7\nBall Query 39.5 64.3\n0.33 78.2\nSPA 41.3 65.1\n0.32 77.1\n(c) Coordinate\nsystem\nAP‚Üë Rad.\nErr.\nAzim.\nErr\n.0.5m mean\nCart. 27.7 58.6 1.73 0.52\nPolar 41.3 65.1 1.26 0.25\nImp. +13.6 +6.5 -0.47 -0.27\nTable 3: Ablation experiments to validate our fusion framework design choices.\nAs shown in Table 1, our CRAFT outperforms all com-\npeting camera-radar and single-frame camera methods on\ntest set. Importantly, our CRAFT has more than two times\nfaster inference speed compared to other methods. The most\nperformance gain comes from the improved localization and\nvelocity estimation, which improve the recall at strict thresh-\nolds (e.g., 0.5m or 1m) and NDS scores, respectively. As\nour lightweight CenterNet-like camera 3D detector, denote\nas CRAFT-I, has no bells and whistles as a proof of concept,\nwe leave a more advanced camera 3D detector for camera-\nradar fusion as a future work.\nTable 2 shows the per-class mAP comparison with Li-\nDAR, camera, and camera-radar methods on val set. Al-\nthough CenterFusion (Nabati and Qi 2021) and ours have\na similar camera 3D detector architecture, our CRAFT\nachieves a remarkable performance boost (+17.2% vs. 4.0%\non Car, +17.3% vs. 2.2% on Bus), which shows the impor-\ntance of an appropriate fusion strategy considering sensor\ncharacteristics. The performance improvement is higher in\nmetallic objects (car, truck, bus and motorcycle) than non-\nmetallic objects (pedestrian, bicycle, traffic cone, and bar-\nrier) since metallic objects have more valid radar returns and\nare thus easier to be distinguished from background clutters.\nTrailer and construction vehicle are metallic but have less\nperformance gain, assuming these classes are commonly\nsurrounded by other trailers or construction materials, which\ncan possibly harm the radar performance. More experiments\nof robustness against the number of radar points are pro-\nvided in Appendix D.\n4.2 Ablation Studies\nWe conduct a series of ablation studies to validate the design\nof CRAFT on nuScenes val set. We report the results of\nthe car class for ablation experiments since the performance\ngains of other classes are less consistent and easier to be\nfluctuated by other factors.\nFusion Method On top of our CRAFT-I, we conduct ad-\nditional fusion methods: using only radar points similar to\n90.1\n71.8\n46.0\n73.4\n46.6\n13.8\n33.9\n72.6\n83.0\n89.0\n27.7\n39.2\n52.1\n60.3\n90\n70\n50\n30\n10\n100\n80\n60\n40\n20\n0-15m 15-35m >35m 0-5 5-10 10-20 >20\n(a) Object distances (b) Number of radar points\n+16.7%\n+32.2%\n/ Fused Recall / AP / Camera-only Recall / AP\n+33.4%\n+28.7%\nFigure 4: Analysis of different object distances and the num-\nber of radar points.\nFPointNet (FP) (Qi et al. 2018) and fusing features by con-\ncatenation (FC) after aggregating radar features by max-\npooling. Table 3a shows that our fusion transformer (FT)\nbrings large AP improvement over radar-only (FP, +6.6%)\nand naive concatenation (FC, +2.4%), especially at the strict\n0.5m threshold. It shows that using both image feature and\nattention-based fusion is beneficial for improving recall and\nreducing localization error.\nAssociation Method In Table 3b, we ablate the proposed\nassociation strategy by replacing it with RoIPool (query\npoints inside image proposal) and Ball Query (query points\naround image proposal center within a 6m radius) to study\nhow the soft polar association benefits the following fu-\nsion method. Association recall denotes the percentage of a\nproposal containing at least one valid point over associated\npoints. Taking all points to Radar-to-Image Feature Encoder\nwithout association suffers from a substantial computational\nburden, leading to an out-of-memory in our setting. RoIPool\noften fails to associate valid radar points due to inaccurate\nimage proposal localization, and large size ball query con-\n1165\n: Ground Truth : Camera-only Prediction : Fused Prediction : Association: Radar Points\nFigure 5: Qualitative results of CRAFT. Blue circles indicate samples that are refined by fusing radar points and have more\naccurate localization, and red circles indicate samples that are predicted by camera-only since there are no valid radar returns\namong associated points. Best viewed in color with zoom in.\ntains many clutters that can degrade the fusion performance.\nOur SPA enables the best trade-off between recall and pre-\ncision for effective camera-radar feature fusion.\nCoordinate System Table 3c ablates the coordinate sys-\ntem of point association and regression target. For the Carte-\nsian setting, we associate points using SPA and transform\nthem back to the Cartesian coordinate before feeding them to\nR2I FE, and also regress offsets in the same coordinate. Us-\ning polar coordinate is similar to applying Principal Compo-\nnent Analysis (PCA) since the error variance of image pro-\nposals in the azimuthal direction is minimized in polar co-\nordinate. Thus, the simple coordinate transformation makes\nthe network learn spatial information easier and effectively\nreduces the localization error of image proposal.\n4.3 Analysis\nWe analyze maximum recall and AP performance under dif-\nferent object distances and the number of radar points using\na 1m matching threshold.\nPerformance by Distances Our camera-radar fusion\nachieves significant improvements over the camera-only\nbaseline and brings larger improvements for distant ob-\njects, as shown in Fig. 4a. As the depth estimation becomes\nmore inaccurate on distant objects farther than 35m, our fu-\nsion method benefits more from the radar measurement and\nachieves a significant improvement of 32.2%.\nPerformance by Radar Points We demonstrate the per-\nformance under a different number of valid radar points re-\nturned from the object in Fig. 4b. CRAFT achieves consis-\ntent improvements over a various number of radar points,\nwhich shows that our fusion method is robust to the radar\nsparsity. Our method yields robust performance although\nradar points are not available or only a few (+6.2%) and\nshows better performance improvement when more radar\npoints are provided (+28.7%).\n4.4 Qualitative Results\nWe show the detection results in a complex scene in Fig. 5.\nAs highlighted with blue circles, CRAFT precisely refines\nimage proposals by adaptively fusing valid radar returns if\nthey are available. More qualitative results are provided in\nAppendix E.\n5 Conclusions\nIn this paper, we have proposed an effective and robust\ncamera-radar 3D detection framework. With a soft polar as-\nsociation and spatio-contextual fusion transformer, the spa-\ntial and contextual information of the camera and radar\ncan be effectively complemented to yield a more accu-\nrate prediction given an inaccurate image proposal. CRAFT\nachieves state-of-the-art performance on nuScenes by out-\nperforming previous camera-radar and camera-only meth-\nods and shows the potential of camera-radar fusion. Exten-\nsive experiments validate the design choice of our frame-\nwork and show the advantages over a camera-only baseline.\nWe hope our work will inspire future research on camera-\nradar fusion for 3D scene understanding.\n1166\nAcknowledgments\nThis work was supported by Institute of Information &\ncommunications Technology Planning & Evaluation (IITP)\ngrant funded by the Korea government (MSIT) (No.2021-0-\n00951, Development of Cloud based Autonomous Driving\nAI learning Software).\nReferences\nBai, X.; Hu, Z.; Zhu, X.; Huang, Q.; Chen, Y .; Fu, H.; and\nTai, C.-L. 2022. Transfusion: Robust lidar-camera fusion\nfor 3d object detection with transformers. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 1090‚Äì1099.\nCaesar, H.; Bankiti, V .; Lang, A. H.; V ora, S.; Liong, V . E.;\nXu, Q.; Krishnan, A.; Pan, Y .; Baldan, G.; and Beijbom,\nO. 2020. nuScenes: A multimodal dataset for autonomous\ndriving. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 11621‚Äì\n11631.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), 213‚Äì229.\nFu, H.; Gong, M.; Wang, C.; Batmanghelich, K.; and Tao,\nD. 2018. Deep Ordinal Regression Network for Monocular\nDepth Estimation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2002‚Äì2011.\nHuang, T.; Liu, Z.; Chen, X.; and Bai, X. 2020. Epnet: En-\nhancing point features with image semantics for 3d object\ndetection. In Proceedings of the European Conference on\nComputer Vision (ECCV), 35‚Äì52.\nHung, W.-C.; Kretzschmar, H.; Casser, V .; Hwang, J.-J.; and\nAnguelov, D. 2022. Let-3d-ap: Longitudinal error tolerant\n3d average precision for camera-only 3d detection. In arXiv\npreprint arXiv:2206.07705.\nJohnson, D. H.; and Dudgeon, D. E. 1992. Array signal\nprocessing: concepts and techniques. Simon & Schuster,\nInc.\nKendall, A.; and Gal, Y . 2017. What Uncertainties Do\nWe Need in Bayesian Deep Learning for Computer Vi-\nsion? In Advances in Neural Information Processing Sys-\ntems (NeurIPS), 5574‚Äì5584.\nKim, J.; Kim, Y .; and Kum, D. 2020. Low-level sensor fu-\nsion network for 3D vehicle detection using radar range-\nazimuth heatmap and monocular image. In Proceedings\nof the Asian Conference on Computer Vision (ACCV), 388‚Äì\n402.\nKim, Y .; Choi, J. W.; and Kum, D. 2020. GRIF Net: Gated\nregion of interest fusion network for robust 3D object de-\ntection from radar point cloud and monocular image. In\nProceedings of the IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), 10857‚Äì10864.\nKu, J.; Mozifian, M.; Lee, J.; Harakeh, A.; and Waslander,\nS. L. 2018. Joint 3d proposal generation and object detec-\ntion from view aggregation. InProceedings of the IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems\n(IROS), 5750‚Äì5757.\nLang, A. H.; V ora, S.; Caesar, H.; Zhou, L.; Yang, J.; and\nBeijbom, O. 2019. Pointpillars: Fast encoders for object de-\ntection from point clouds. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 12697‚Äì12705.\nLaw, H.; and Deng, J. 2018. Cornernet: Detecting objects as\npaired keypoints. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), 734‚Äì750.\nLi, Z.; Wang, W.; Li, H.; Xie, E.; Sima, C.; Lu, T.; Yu, Q.;\nand Dai, J. 2022. BEVFormer: Learning Bird‚Äôs-Eye-View\nRepresentation from Multi-Camera Images via Spatiotem-\nporal Transformers. In Proceedings of the European Con-\nference on Computer Vision (ECCV).\nLim, T.-Y .; Ansari, A.; Major, B.; Fontijne, D.; Hamilton,\nM.; Gowaikar, R.; and Subramanian, S. 2019. Radar and\ncamera early fusion for vehicle detection in advanced driver\nassistance systems. In Advances in Neural Information Pro-\ncessing Systems Workshops (NeurIPSW).\nLin, J.-T.; Dai, D.; and Van Gool, L. 2020. Depth estimation\nfrom monocular images and sparse radar data. In Proceed-\nings of the IEEE/RSJ International Conference on Intelli-\ngent Robots and Systems (IROS), 10233‚Äì10240.\nLin, Y .; Le Kernec, J.; Yang, S.; Fioranelli, F.; Romain, O.;\nand Zhao, Z. 2018. Human activity classification with radar:\nOptimization and noise robustness with iterative convolu-\ntional neural networks followed with random forests. IEEE\nSensors Journal, 18(23): 9669‚Äì9681.\nLiu, Y .; Wang, T.; Zhang, X.; and Sun, J. 2022. PETR: Posi-\ntion Embedding Transformation for Multi-View 3D Object\nDetection. In Proceedings of the European Conference on\nComputer Vision (ECCV), 531‚Äì‚Äì548.\nLong, Y .; Morris, D.; Liu, X.; Castro, M.; Chakravarty, P.;\nand Narayanan, P. 2021a. Full-Velocity Radar Returns by\nRadar-Camera Fusion. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), 16198‚Äì\n16207.\nLong, Y .; Morris, D.; Liu, X.; Castro, M.; Chakravarty, P.;\nand Narayanan, P. 2021b. Radar-camera pixel depth associ-\nation for depth completion. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 12507‚Äì12516.\nMa, X.; Zhang, Y .; Xu, D.; Zhou, D.; Yi, S.; Li, H.; and\nOuyang, W. 2021. Delving into Localization Errors for\nMonocular 3D Object Detection. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 4721‚Äì4730.\nMajor, B.; Fontijne, D.; Ansari, A.; Teja Sukhavasi, R.;\nGowaikar, R.; Hamilton, M.; Lee, S.; Grzechnik, S.; and\nSubramanian, S. 2019. Vehicle detection with automotive\nradar using deep learning on range-azimuth-doppler tensors.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision Workshops (ICCVW), 924‚Äì932.\nMeyer, M.; and Kuschk, G. 2019a. Automotive radar dataset\nfor deep learning based 3d object detection. In Proceedings\nof the European Radar Conference (EuRAD), 129‚Äì132.\n1167\nMeyer, M.; and Kuschk, G. 2019b. Deep learning based\n3d object detection for automotive radar and camera. In\nProceedings of the European Radar Conference (EuRAD),\n133‚Äì136.\nMisra, I.; Girdhar, R.; and Joulin, A. 2021. An end-to-end\ntransformer model for 3d object detection. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 2906‚Äì2917.\nNabati, R.; and Qi, H. 2021. Centerfusion: Center-based\nradar and camera fusion for 3d object detection. InProceed-\nings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision (WACV), 1527‚Äì1536.\nPan, X.; Xia, Z.; Song, S.; Li, L. E.; and Huang, G. 2021.\n3d object detection with pointformer. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 7463‚Äì7472.\nPark, D.; Ambrus, R.; Guizilini, V .; Li, J.; and Gaidon, A.\n2021. Is Pseudo-Lidar needed for Monocular 3D Object\ndetection? In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 3142‚Äì3152.\nPhilion, J.; and Fidler, S. 2020. Lift, splat, shoot: Encoding\nimages from arbitrary camera rigs by implicitly unproject-\ning to 3d. In Proceedings of the European Conference on\nComputer Vision (ECCV), 194‚Äì210.\nQi, C. R.; Liu, W.; Wu, C.; Su, H.; and Guibas, L. J. 2018.\nFrustum pointnets for 3d object detection from rgb-d data.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 918‚Äì927.\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017a. Point-\nnet: Deep learning on point sets for 3d classification and\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 652‚Äì\n660.\nQi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017b. Point-\nnet++: Deep hierarchical feature learning on point sets in a\nmetric space. In Advances in Neural Information Processing\nSystems (NeurIPS), 5105‚Äì5114.\nReading, C.; Harakeh, A.; Chae, J.; and Waslander, S. L.\n2021. Categorical depth distribution network for monocu-\nlar 3d object detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 8555‚Äì8564.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster\nR-CNN: Towards Real-Time Object Detection with Region\nProposal Networks. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS), 91‚Äì99.\nShi, S.; Wang, X.; and Li, H. 2019. Pointrcnn: 3d object\nproposal generation and detection from point cloud. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 770‚Äì779.\nSvenningsson, P.; Fioranelli, F.; and Yarovoy, A. 2021.\nRadar-pointgnn: Graph based object recognition for unstruc-\ntured radar point-cloud data. In Proceedings of the IEEE\nRadar Conference (RadarConf), 1‚Äì6.\nThomas, H.; Qi, C. R.; Deschaud, J.-E.; Marcotegui, B.;\nGoulette, F.; and Guibas, L. J. 2019. Kpconv: Flexible and\ndeformable convolution for point clouds. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 6411‚Äì6420.\nTian, Z.; Shen, C.; Chen, H.; and He, T. 2019. Fcos: Fully\nconvolutional one-stage object detection. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 9627‚Äì9636.\nUlrich, M.; Braun, S.; K ¬®ohler, D.; Niederl ¬®ohner, D.; Faion,\nF.; Gl ¬®aser, C.; and Blume, H. 2022. Improved Orienta-\ntion Estimation and Detection with Hybrid Object Detec-\ntion Networks for Automotive Radar. In arXiv preprint\narXiv:2205.02111.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in Neural Information\nProcessing Systems (NeurIPS), 6000‚Äì6010.\nV ora, S.; Lang, A. H.; Helou, B.; and Beijbom, O. 2020.\nPointpainting: Sequential fusion for 3d object detection. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 4604‚Äì4612.\nWang, T.; Xinge, Z.; Pang, J.; and Lin, D. 2021a. Probabilis-\ntic and geometric depth: Detecting objects in perspective. In\nProceedings of the Conference on Robot Learning (CoRL),\n1475‚Äì1485.\nWang, T.; Zhu, X.; Pang, J.; and Lin, D. 2021b. Fcos3d:\nFully convolutional one-stage monocular 3d object detec-\ntion. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision Workshops (ICCVW), 913‚Äì922.\nXiong, R.; Yang, Y .; He, D.; Zheng, K.; Zheng, S.; Xing, C.;\nZhang, H.; Lan, Y .; Wang, L.; and Liu, T. 2020. On layer\nnormalization in the transformer architecture. In Proceed-\nings of the International Conference on Machine Learning\n(ICML), 10524‚Äì10533.\nYang, Z.; Sun, Y .; Liu, S.; and Jia, J. 2020. 3dssd: Point-\nbased 3d single stage object detector. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 11040‚Äì11048.\nYin, T.; Zhou, X.; and Krahenbuhl, P. 2021. Center-based\n3d object detection and tracking. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 11784‚Äì11793.\nYoo, J. H.; Kim, Y .; Kim, J.; and Choi, J. W. 2020. 3d-cvf:\nGenerating joint camera and lidar features using cross-view\nspatial feature fusion for 3d object detection. InProceedings\nof the European Conference on Computer Vision (ECCV),\n720‚Äì736.\nYu, F.; Wang, D.; Shelhamer, E.; and Darrell, T. 2018.\nDeep Layer Aggregation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2403‚Äì2412.\nZhou, X.; Wang, D.; and Kr ¬®ahenb¬®uhl, P. 2019. Objects as\nPoints. In arXiv preprint arXiv:1904.07850.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2021.\nDeformable detr: Deformable transformers for end-to-end\nobject detection. In Proceedings of the International Con-\nference on Learning Representations (ICLR).\n1168"
}