{
  "title": "Deep learning model-transformer based wind power forecasting approach",
  "url": "https://openalex.org/W4316664916",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099745880",
      "name": "Sheng Huang",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A2106119810",
      "name": "Chang Yan",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A2386662720",
      "name": "Yinpeng Qu",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A2099745880",
      "name": "Sheng Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106119810",
      "name": "Chang Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2386662720",
      "name": "Yinpeng Qu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2959395565",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W1994170512",
    "https://openalex.org/W6729418321",
    "https://openalex.org/W3000144322",
    "https://openalex.org/W3044963726",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6632992330",
    "https://openalex.org/W6786530122",
    "https://openalex.org/W1705374184",
    "https://openalex.org/W6677285437",
    "https://openalex.org/W3112298559",
    "https://openalex.org/W2600292797",
    "https://openalex.org/W6736282669",
    "https://openalex.org/W4284962670",
    "https://openalex.org/W6801710425",
    "https://openalex.org/W3059934811",
    "https://openalex.org/W2973960501",
    "https://openalex.org/W2048857692",
    "https://openalex.org/W6785298061",
    "https://openalex.org/W3023221330",
    "https://openalex.org/W3089243611",
    "https://openalex.org/W2002093078",
    "https://openalex.org/W6792155083",
    "https://openalex.org/W4286215040",
    "https://openalex.org/W2896742282",
    "https://openalex.org/W3004665554",
    "https://openalex.org/W6839110953",
    "https://openalex.org/W1989098521",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W3132882148",
    "https://openalex.org/W2974313728",
    "https://openalex.org/W6792240715",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3016073894",
    "https://openalex.org/W6791943378",
    "https://openalex.org/W3121826667",
    "https://openalex.org/W2162331448",
    "https://openalex.org/W2568673758",
    "https://openalex.org/W2896920734",
    "https://openalex.org/W2114668738",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2604847698",
    "https://openalex.org/W2551805626",
    "https://openalex.org/W4376983087",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3111305055",
    "https://openalex.org/W4300938272",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4280553353",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3202552514"
  ],
  "abstract": "The uncertainty and fluctuation are the major challenges casted by the large penetration of wind power (WP). As one of the most important solutions for tackling these issues, accurate forecasting is able to enhance the wind energy consumption and improve the penetration rate of WP. In this paper, we propose a deep learning model-transformer based wind power forecasting (WPF) model. The transformer is a neural network architecture based on the attention mechanism, which is clearly different from other deep learning models such as CNN or RNN. The basic unit of the transformer network consists of residual structure, self-attention mechanism and feedforward network. The overall multilayer encoder to decoder structure enables the network to complete modeling of sequential data. By comparing the forecasting results with other four deep learning models, such as LSTM, the accuracy and efficiency of transformer have been validated. Furthermore, the migration learning experiments show that transformer can also provide good migration performance.",
  "full_text": "Deep learning\nmodel-transformer based wind\npower forecasting approach\nSheng Huang, Chang Yan and Yinpeng Qu*\nCollege of Electrical and Information Engineering, Hunan University, Changsha, China\nThe uncertainty andﬂuctuation are the major challenges casted by the large\npenetration of wind power (WP). As one of the most important solutions for\ntackling these issues, accurate forecasting is able to enhance the wind energy\nconsumption and improve the penetration rate of WP. In this paper, we propose\na deep learning model-transformer based wind power forecasting (WPF)\nmodel. The transformer is a neural network architecture based on the\nattention mechanism, which is clearly different from other deep learning\nmodels such as CNN or RNN. The basic unit of the transformer network\nconsists of residual structure, self-attention mechanism and feedforward\nnetwork. The overall multilayer encoder to decoder structure enables the\nnetwork to complete modeling of sequential data. By comparing the\nforecasting results with other four deep learning models, such as LSTM, the\naccuracy and efﬁciency of transformer have been validated. Furthermore, the\nmigration learning experiments show that transformer can also provide good\nmigration performance.\nKEYWORDS\nwind power forecasting, transformer, deep learning, data driven, attention mechanism\n1 Introduction\nWind energy is an economical, efﬁcient and environment friendly renewable energy\nsource that plays an important role in reducing global carbon emissions (Lin and Liu,\n2020). According to Global Wind Report 2022, total installed WP capacity had reached\n837 GW by the end of 2021 (Council, 2022). As the proportion of installed wind turbines\n(WTs) increases year by year, the strong randomness, volatility and intermittency of WP\nlead to the contradiction between the safe operation of the power grid and the efﬁcient\nconsumption of WP (Yang et al., 2022). Accurate forecasting can reduce the uncertainty\nand increase the penetration rate of WP.\nThe WPF mentioned in this paper refers to the forecasting of speciﬁc point values of\nfuture wind speed or WP. It is called the deterministic forecasting model, which mainly\nincludes physical forecasting models, statistical forecasting models and hybrid forecasting\nmodels (Haniﬁ et al., 2020; Sun et al., 2021).\nPhysical forecasting modeling obtains wind speed forecasting information based on\nnumerical weather forecast data with mathematical models, and then predicts WP with\nthe help of relevant WP curves using the wind speed forecasts (Li et al., 2013). Therefore\nOPEN ACCESS\nEDITED BY\nXinran Zhang,\nBeihang University, China\nREVIEWED BY\nLeijiao Ge,\nTianjin University, China\nCongying Wei,\nState Grid Corporation of China (SGCC),\nChina\n*CORRESPONDENCE\nYinpeng Qu,\nquyinpeng@hnu.edu.cn\nSPECIALTY SECTION\nThis article was submitted\nto Smart Grids,\na section of the journal\nFrontiers in Energy Research\nRECEIVED 28 September 2022\nACCEPTED 25 November 2022\nPUBLISHED 16 January 2023\nCITATION\nHuang S, Yan C and Qu Y (2023), Deep\nlearning model-transformer based wind\npower forecasting approach.\nFront. Energy Res.10:1055683.\ndoi: 10.3389/fenrg.2022.1055683\nCOPYRIGHT\n© 2023 Huang, Yan and Qu. This is an\nopen-access article distributed under\nthe terms of theCreative Commons\nAttribution License (CC BY). The use,\ndistribution or reproduction in other\nforums is permitted, provided the\noriginal author(s) and the copyright\nowner(s) are credited and that the\noriginal publication in this journal is\ncited, in accordance with accepted\nacademic practice. No use, distribution\nor reproduction is permitted which does\nnot comply with these terms.\nFrontiers inEnergy Research frontiersin.org01\nTYPE Methods\nPUBLISHED 16 January 2023\nDOI 10.3389/fenrg.2022.1055683\nimproving the accuracy of the NWP model directly affects the\nforecasting accuracy of the physical model ( Cassola and\nBurlando, 2012).\nStatistical forecasting modeling is establishing a mapping\nrelationship between historical data and forecasted data.\nStatistical models can be classiﬁed into traditional statistical\nmodels, time series models, traditional machine learning\nmodels and deep learning models. The persistence method,\nknown as the most classical traditional statistical method, uses\nthe wind power at the current moment as the forecasted value.\nThis method is simple but limited to the use of ultra-short-term\nforecasting (Wu and Hong, 2007). Commonly used time series\nmodels include AutoreGressive (AR) (Poggi et al., 2003), Auto\nRegression Moving Average (ARMA) ( Huang et al., 2012 ),\nAutoregressive Integrated Moving Average (ARIMA) (Hodge\net al., 2011), etc. Time series models are difﬁcult to explore the\nnon-linear relationship in the data. So such models are only\nsuitable for static data analysis. Traditional machine learning\nmodels can predict future wind power value adaptively based on\nhistorical WP data. Machine learning models are widely used in\nwind power forecasting and relatedﬁelds. The popular methods\ninclude artiﬁcial neuro network (ANN) (Hu et al., 2016), support\nvector machine (SVM) (Li et al., 2020), Piecewise support vector\nmachine (PSVM) (Liu et al., 2009), Least Square support vector\nmachine (LSSVM) (Chen et al., 2016), Random Forest (RF)\n(Lahouar and Slama, 2017 ), Bayesian Additive\nRegressionTrees (Alipour et al., 2019 ), K-Nearest-Neighbors\n(KNN) (Yesilbudak et al., 2017), etc. These machine learning\nmodels require additional time to extract features from\nmultidimensional data with good accuracy and relevance.\nOptimization algorithms can effectively solve this problem\n(Shahid et al., 2021 ). Li et al. (2021) proposed a hybrid\nimproved cuckoo search algorithm to optimize the\nhyperparameters of support vector machines for short-term\nwind power forecasting.\nIn recent years, deep lea rning models have provided\npromising performance in natural language processing\n(NLP), computer vision and other ﬁelds, while related\ntechniques are also applied to wind power forecasting.\nAmong them, two recurrent neural networks (RNN), Long\nShort Term Memory (LSTM) and Gated Recurrent Unit\n(GRU), are mainly utilized for wind power forecasting\nresearch ( Lu et al., 2018 ; Deng et al., 2020 ; Wang et al.,\n2020). used wavelet decomposition to reduce the volatility\nof the original series. They transformed non-stationary time\nseries into stable and predictable series to forecast by LSTM\nLiu et al. (2020) . enhanced the effect of forgetting gate in\nLSTM, optimized the convergence speed, and ﬁltered the\nfeature data within a certain distance based on correlation.\nThe forecasting permance was futher improved by clustering\nYu et al. (2019). used variable mode decomposition to stratify\nwind power sequences accordin g to different frequencies.\nThen similar ﬂuctuating patterns were identi ﬁed in each\nlayer by K-means clustering algorithm. Furthermore, the\nunstable features were captured in each set by LSTM Sun\net al. (2019). To address the overﬁtting problem, employed\nmulti-level residual network s and DenseNet to improve the\noverall performance Ko et al. (2020). introduced the attention\nmechanism into the GRU to obtain a novel sequence-to-\nsequence model Niu et al. (2020) . The combination of\nmultiple deep learning models can also improve the\naccuracy of WPF. proposed a novel spatio-temporal\ncorrelation model (STCM) for ultra-short-term wind power\nforecasting Wu et al. (2021). proposed a hybrid deep learning\nalgorithm, which consists of GRU, LSTM, and fully connected\nneural networks, to accurately predict ultra-short-term wind\npower generation at the Boco Rock wind farm in Australia,\nHossain et al. (2020). The RNN model is unable to capture the\nlong periods temporal corre lation due to the gradient\ndisappearance problem. To address this problem, Lai et al.\n(2018) developed an RNN-skip structure with time-hopping\nconnections to extend the time span of the informationﬂow.\nRNN also suffers from the inability of recursive computation\nto parallelize problem. The transformer is the ﬁrst sequence\ntranscription model based solelyo nt h ea t t e n t i o nm e c h a n i s m ,\nwhich has been proved that it cans o l v et h ea f o r e m e n t i o n e d\nproblems (Vaswani et al., 2017 ). The transformer was ﬁrst\nproposed in NLP. BERT (Devlin et al., 2018), GPT-2 (Radford\net al., 2019), RoBERTa (Liu et al., 2019), T5 (Raffel et al., 2020)\nand BART (Lewis et al., 2019 ) based on transformer have\nmade a huge impact in the NLP ﬁeld. Recently, almost all\nadvanced NLP models have been adapted from one of above\nbasic models (Bommasani et al., 2021). Transformer made a\nbig splash in the ﬁeld of computer visiona along with the\npublication of the VIT (Dosovitskiy et al., 2020), CvT (Wu\net al., 2021), CaiT (Touvron et al., 2021), DETR (Carion et al.,\n2020), and Swin Transformer (Liu et al., 2021). Transformer\nw a sa l s oa p p l i e dt ot h eﬁeld of power system time series\nforecasting. Lin et al. employed the Spring DWT attention\nlayer to measure the similarity of query-key pairs of sequences\n(Lin et al., 2020). Santos et al. and Phan et al. employed the\ntransformer-based time series forecasting model to predict the\nPV power generation for each hour (López Santos et al., 2022;\nPhan et al., 2022). L’Heureux et al. proposed a transformer-\nbased architecture for load forecasting ( L\n’Heureux et al.,\n2022).\nTransformer architecture has become a mainstream\ntechnology in NLP which performs better than RNN or\nSeq2Seq algorithms. For this reason, this paper used the\ntransformer as the basic model for wind power forecasting\nresearch.\nThe remainder of the paper is organized as follows.Section 2\npresents the forecasting problem. Section 3 introduces Data-\ndriven model of wind power forecasting.Section 4 shows the\nanalysis and discussion of the numerical simulation results.\nSection 5 concludes this paper.\nFrontiers inEnergy Research frontiersin.org02\nHuang et al. 10.3389/fenrg.2022.1055683\n2 Problem description\nIn this paper, wind power forecasting refers to making\nspeculations about the possible levels of wind power in several\nfuture periods.\nSuppose D /equals D\n1,D 2, /,D n{} is the historical information\ncollected from WPAPs, wheren is the number of WPAPs.Di /equals\nPi,F i{} is the historical information ofi th WPAP, wherePi.i st h e\npower output of thei th WPAP and Fi is other characteristic\ninformation of thei th WPAP. For eachPt\ni in Pi /equals P1\ni ,P 2\ni\n, /,P t\ni{}\nis the power outputs of thei th WPAP at timestampt. For eachFt\ni,j\nin Fi /equals F1\ni,1,F 2\ni,1, /,F t\ni,1,F 1\ni,2,F 2\ni,2, /,F t\ni,2, /,F 1\ni,j,F 2\ni,j\n, /,F t\ni,j{} is\nthe j th feature data of thei th WPAP at timestampt.C o m m o n\ncharacteristics are wind speed and WPAP ambient temperature,\netc. The one-step ahead wind power sequence forecasting modelf\ncan be denoted as:\nPpre\ni /equals fD i() ,i ∈ 0,n[]\nWhere Ppre\ni denotes the power forecasting sequence of the i\nth WPAP.\n3 Deep learning model for wind\npower forecasting\nIn this paper, the transformer is chosen as the basic deep\nlearning model for wind powe r forecasting because it is\nconsidered to use a broader inductive bias compared to\nRNN, allowing it to handle more generalized information.\nThe inductive bias of a learning algorithm is the set of\nassumptions that the learner uses to predict outputs of\ngiven inputs that it has not encountered. For example, the\nloop structure and gate structure are the inductive bias of\nRNNs. The transformer model mainly includes self-attentive\nmechanisms, position-wise feed-forward networks and\nresidual connections. These thr ee neural network structures\ndo not rely on strong assumptions on the objective function.\nFurthermore, they do not have the inductive bias as\ntranslation invariance or the time invariance. So, a much\nmore general form makes the transformer model applicable\nto more subjects. In this section, we introduce the structure of\nthe transformer.\n3.1 Encoder to decoder structure\nNumerous wind power sequence forecasting models\nfollow the encoder to decoder structure ( Lu et al., 2018 ;\nNiu et al., 2020 ; Li and Armandpour, 2022 ), which is\nillustrated in Figure 1 .T h ee n c o d e rm a p st h eW P A P\nhistorical sequence data D /equals D1,D 2, /,D n{} to the hidden\nstate H /equals H1, H2, /, Hn{} . The decoder then outputs the\nforecasted power sequence Ppre /equals Ppre\n1 ,P pre\n2\n, /,P pre\nn{}\nbased on the hidden state H. As shown in Figure 2 ,\ntransformer architecture also follows this architecture and\nuses stacked self-attentive mechanisms, pointwise fully\nconnected layers and the RetNet structure ( He et al., 2016)\nto build the decoder and encoder. Encoder consist of a self-\ndeﬁned number of identical encoder layers stacked on top of\neach other. Each encoder layer has two sub-layers: multi-head\nself-attention mechanism and p osition-wise fully connected\nfeed-forward network. Each sub-layer uses a residual\nstructure and then the output data is layer-normalized\nwhich can be expressed as:\nOsub /equals LN x + SL x()()\nWhere Osub is the output of sub-layer,x is the input of the sub-\nlayer, LN is the layer normalization function, SL is the function\nemployed in the sub-layer.\nTo facilitate residual connectivity, outputs produced from all\nsublayers in the model as well as the embedding layer have the\nsame self-deﬁned dimension d\nmodel.\nThe decoder has the same number of stack layers as the\nencoder. each decoder layer consists of three sub-layers. Theﬁrst\nsublayer is the Masked Multi-head attention layer, whose main\nfunction is to ensure that the forecasting of position i only\ndepends on the known outputs of positions smaller than i.\nThe last two layers use the same sub-layers as the encoder\nlayer. Each sub-layer has a residual architecture and layer\nnormalization of the output.\n3.2 Self-attentive mechanism\nThe attention mechanism (AM) isa resource allocation scheme\nthat allocates computational resources to more important tasks\nwhile solving the information overload problem in the presence\nFIGURE 1\nEncoder to decoder structure.\nFrontiers inEnergy Research frontiersin.org03\nHuang et al. 10.3389/fenrg.2022.1055683\nof limited computational power. The input information of AM can\nbe represented by key vector -value vector pairs\n[(k1,v 1), (k2,v 2), ... , (km,v m)]. The target value information\ncan be represented by query vector. The weight of the value\nvectors are calculated based on the similarity of query vector and\nkey vector. And then, theﬁnal attention value can be obtained by\nweighted summation of value vector. The core idea of the attention\nmechanism can be expressed as the following equation.\nS\natt /equals W×V\nW /equals func Q , K()\nWhere Satt is the attention value, V is the value vector of key-\nvalue pairs, K is the key vector of key-value pairs, Q is the query\nvector, W is the corresponding weight of V and func is the weight\ntransformation function.\nThe self-attentive mechanism (SAM) uses three learnable\nparameter matrices W\nq, Wk and Wv to transform the input\nsequence X into the query vectorQs, key vector Ks and value\nvector Vs.T h em o d e lu s e saS o f t M a xf u n c t i o na st h ew e i g h t\ntransformation function. The weights of theVs are obtained by\ncalculating the dot product ofQs and Ks divided by\n/radicaltpext/radicaltpext\ndk\n√\n.T h e\noutput of SAM is obtained by weighted summation ofVs,a s\ndepicted inFigure 3.\nQs /equals WQX ∈ Rdk×N\nKs /equals WKX ∈ Rdk×N\nVs /equals WVX ∈ Rdv×N\nAttention Q, K, V() /equals softmax QKT\n/radicaltpext/radicaltpext\ndk\n√() V\nWhere dk is the dimension ofKs.\nFIGURE 2\nEncoder and decoder stacks of transformer.\nFIGURE 3\nSelf-attention with masking function.\nFrontiers inEnergy Research frontiersin.org04\nHuang et al. 10.3389/fenrg.2022.1055683\n3.3 Multi-head attention and masked\nmulti-head attention\nMulti-head attention mechanismuses different weight matrices\nto project the single attention head input sequence into different\nsubspaces, which allows the modelt of o c u so nd i f f e r e n ta s p e c t so f\ninformation. The different weight matricesWQ\ni , WK\ni and WV\ni\ntransform the vectorsQ, K and V of dimension dmodel into h\nvectors Qi, Ki and Vi of dimensionddmodel/h and input them into\nthe corresponding parallel attention layers, where h is the number of\nparallel layers. Then the outputs of each layer are concatenated and\nthe results outputvia t h el i n e a rl a y e r ,a sd e p i c t e di nFigure 4.\nMultiHead Q, K, V() /equals Concat head1, ... , headh() WO\nwhere headi/equals Attention Qi,K i,V i()\nQi /equals QWQ\ni\nKi /equals KWK\ni\nVi /equals VWV\ni\ni /equals 1, 2, ... ,h\nWhere WQ\ni ∈ Rdmodel ×dk , WK\ni ∈ Rdmodel ×dk , WV\ni ∈ Rdmodel ×dv ,\nWO ∈ Rhdv×dmodel , and dk /equals dv /equals dmodel/h\nMasked multi-head attention mechanism is proposed to\nprevent the decoder from seeing future information. An upper\ntriangular matrix with all values of \"-inf” is added to the dot\nproduct matrix before it is softmaxed, as depicted inFigure 3.\n3.4 Position-wise feed-forward networks\nand positional encoding\nEach encoder and decoder layer contains a position-wise\nfeed-forward networks, which is composed of two linear\ntransformations and uses the ReLu function as the\nactivation function. Due to the existence of two linear\ntransformations, the inner layer dimension can be adjusted\nwhile the input and output dimensions are guaranteed to be\nequal to dmodel .T h ef o r m u l ai sa sf o l l o w s .\nFFN x() /equals ReLu xW1 + b1() W2 + b2\nwhere ReLu x() /equals max 0,x()\nwhere W1 and W2 are the two linear transformation matrixes,b1\nand b2 are biases of the two linear transformations andx is the\ninput data.\nSince transformer architecture does not contain recursion\nand there is no relative or absolute position information of\neach value in the inputs of the transformer, it is necessary to\nthere is no relative or absolute position information of each\nvalue in the inputs of the transformer so that the model can\nmake use of the sequential information. Transformer uses sine\nand cosine functions of different frequencies.\nPE\npos,2id() /equals sin pos/100002id/dmodel()\nPE pos,2id+1() /equals cos pos/100002id/dmodel()\nwhere pos is the position andid is the dimension.\nFIGURE 4\nMulti-head attention.\nFIGURE 5\nData input.\nFIGURE 6\nData subset allocation ratio.\nFrontiers inEnergy Research frontiersin.org05\nHuang et al. 10.3389/fenrg.2022.1055683\n3.5 Power forecasting and model\nmigration\nIn this paper, transformer is used as the power prediction\nmodel. The historical feature data needs to be processed before\nit can be input into transformer. The transformation of\nhistorical data into feature vectors and positional encoding\nare shown in the Figure 5 . The feature vector at each\ntimestamp consists of different WPAP feature values in the\nspeciﬁed order. Each encoder layer extracts features from the\ninput data using the multi-head attention mechanism,\nposition-wise feed-forward networks, normalization layer\nand residual structure. The last encoder layer passes the\nfeature information to each decoder layer. The ﬁrst sub-\nlayer of each decoder layer extracts the sequence feature\ninformation from the predicted data. Finally, the predicted\ndata of the speciﬁed length is processed by the fully-connected\nlayer and output.\nMigrating the trained modelparameters to another model\nfor a related task can effectively speed up the model\nconvergence and reduce the over ﬁtting problem. The data\nbetween different WPAPs has some similarity. This paper\nproposes to train untrained WPAP prediction models which\nwe migrate the trained WPAP power prediction model\nparameters to.\n4 Experimental results and discussion\nTo verify the effectiveness of transformer for wind power\nforecasting, we conducted a case study using areal-world wind\nfarm operation dataset.\n4.1 Dataset preparation\nIn this paper, experiments are conducted by using the Spatial\nDynamic Wind Power Forecasting (SDWPF) dataset, which is\nconstructed based on real-world wind farm data from Longyuan\nPower Group Corp. Ltd. (Zhou et al., 2022). SDWPF contains\n134 WPAPs output power, wind speed, ambient temperature and\nother characteristic information, which is sampled at 10-min\nintervals and covers 245 days of data. From them, we selected the\npower, wind speed and ambient temperature of eight WPAPs\ndata as the feature information used for single turbine one-step\nahead wind power prediction. Three data subsets are used in the\nevaluation: training set, validation set, and test set, and the three\nsubsets are assigned in the ratio of 6:2:2 as shown inFigure 6. The\ntraining set is used to update the model parameters. First, the\nresults of the forward calculation are stored for each parameter.\nThen, the partial derivatives of each parameter can be calculated\nthrough loss function based on the chain rule subsequently. At\nlast, the partial derivatives are multiplied with the learning rate to\nobtain the optimized values of the parameters. The validation set\nis used for hyperparameter tuning during the model training, and\nthe test set is used to evaluate the generalization ability of the\nmodel.\n4.2 Data processing\nThe input variables used in this study are normalized in order\nto speed up the gradient descent for optimal solutions and to\nimprove the accuracy of the model after training. The feature\ninformation is scaled to the range (0, 1) by min-max\nnormalization, and the model output is denormalized.\nFIGURE 7\nOne-step power forecasting experimental results of NO.1-NO.8 WPAP.\nFrontiers inEnergy Research frontiersin.org06\nHuang et al. 10.3389/fenrg.2022.1055683\nxinp\n′ /equals normal xinp()\nnormal xinp() /equals\nxinp − max xinp()\nmax xinp() − min xinp()\nWhere xinp′ is the normalized output of the model input dataxinp\nxout\n′ /equals denormal xout()\ndenormal xout() /equals xout − max xin()\nmax xin() − min xin()\nWhere xout′ is the denormalized output of the model output\ndata xout\nTABLE 1 Each prediction model corresponds to the performance index of each WPAP.\nModel Number MSE MAE RMSE r2score\nTransformer WPAP 1 17.85 2.79 4.22 0.9927\nWPAP 2 81.79 5.28 9.04 0.9873\nWPAP 3 22.18 3.11 4.71 0.9916\nWPAP 4 31.35 3.11 5.60 0.9917\nWPAP 5 34.81 3.56 5.90 0.9907\nWPAP 6 349.80 10.96 18.70 0.9708\nWPAP 7 1854.07 12.75 43.06 0.9659\nWPAP 8 43.18 3.82 6.57 0.9888\nLSTM WPAP 1 30,054.43 102.95 173.36 0.7670\nWPAP 2 19,369.12 80.15 139.17 0.7914\nWPAP 3 24,852.67 95.47 157.65 0.7419\nWPAP 4 33,919.56 110.45 184.17 0.7033\nWPAP 5 41,330.30 122.23 203.30 0.6806\nWPAP 6 22,473.57 86.20 149.91 0.7702\nWPAP 7 38,449.08 118.99 196.08 0.6815\nWPAP 8 19,042.41 79.31 138.00 0.7676\nLSTM (encoder-decoder) WPAP 1 25,685.14 92.67 160.27 0.7762\nWPAP 2 26,958.49 99.14 164.19 0.7135\nWPAP 3 24,751.02 93.21 157.32 0.7166\nWPAP 4 24,181.57 93.14 155.50 0.7207\nWPAP 5 25,359.76 94.54 159.25 0.7282\nWPAP 6 25,101.07 94.25 158.43 0.7171\nWPAP 7 30,025.81 105.08 173.28 0.6911\nWPAP 8 31,325.91 105.32 176.99 0.6667\nGRU WPAP 1 19,987.32 85.33 141.38 0.8069\nWPAP 2 21,242.36 86.55 145.75 0.7747\nWPAP 3 19,528.68 85.69 139.75 0.7684\nWPAP 4 20,628.77 85.88 143.63 0.7693\nWPAP 5 19,067.65 80.58 138.09 0.7894\nWPAP 6 28,290.43 99.28 168.20 0.7353\nWPAP 7 25,172.07 95.83 158.66 0.7435\nWPAP 8 17,800.28 77.21 133.42 0.7737\nGRU (encoder-decoder) WPAP 1 27,126.60 92.52 164.70 0.7766\nWPAP 2 22,599.75 85.18 150.33 0.7538\nWPAP 3 23,005.86 89.85 151.68 0.7268\nWPAP 4 21,207.50 80.54 145.63 0.7585\nWPAP 5 21,693.08 82.02 147.29 0.7642\nWPAP 6 25,015.56 88.84 158.16 0.7334\nWPAP 7 24,351.19 93.96 156.05 0.7238\nWPAP 8 27,082.16 94.26 164.57 0.7017\nFrontiers inEnergy Research frontiersin.org07\nHuang et al. 10.3389/fenrg.2022.1055683\n4.3 Performance evaluation\nIn this paper, we use four metrics to evaluate the prediction\nperformance of transformer, namely mean squared error (MSE),\nmean absolute error (MAE), mean square root error (RMSE),\nr2score, and explained variance (EV). They can be expressed\nmathematically as:\nMSE /equals 1\nl ∑\nl\ni/equals 1\np − ^p()\n2\nMAE /equals 1\nl ∑\nl\ni/equals 1\np − ^p\n⏐⏐\n⏐\n⏐\n⏐⏐\n⏐\n⏐\nRMSE /equals\n/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext\n1\nl ∑\nl\ni/equals 1\np − ^p()\n2\n√\nr2score /equals 1 − ∑ p − ^p\n⏐⏐⏐\n⏐\n⏐⏐⏐\n⏐\n∑ p − p′\n⏐⏐\n⏐\n⏐\n⏐⏐\n⏐\n⏐\nWhere p denotes the original power,^p denotes the forecasted\npower, l denotes the length of the forecast series andp′ denotes\nthe mean value of original power.\nThe better theﬁt between the prediction structure and the\nactual results, the betterMSE, MAE and RMSE tend to 0 and\nr2score tend to one\n4.4 Experimental numerical results\nIn this paper, the experiments performed by all the models\nuse the historical wind power data of the 40 h to predict the wind\npower value of the next 8 h.\nFirst, we use transformer to perform a one-step power\nforecasting on eight WPAPs datasets. A comparison of the\npredicted and actual power curves for each WPAP is shown\nin Figure 7. It can be seen that the predicted power of each WPAP\ncan match the actual power well, and the two curves have similar\ntrends. This power comparison graph shows that transformer has\ngood prediction capability. Also, we perform the same\nexperiments using LSTM, GRU models and LSTM and GRU\nmodels with encoder-decoder structure. The performance\nindexes for each WPAP power forecasting using the ﬁve\nmodels are shown in Table 1 . It can be seen that the\nforecasting performance of transformer on this dataset is\nmuch better than the four models. The mean MSE, MAE and\nRMSE of transformer prediction results are 304.38, 5.67 and\nFIGURE 8\nTransformer model migration based one-step power forecasting experimental results of NO.9-NO.20 WPAP.\nTABLE 2 Performance indicators of WPAPs 9 to 20 and the distance of\nrelative location between each WPAP and WPAP one.\nNumber Distance MSE MAE RMSE r2score\nWPAP 9 476.91 31.52 3.27 5.61 0.9914\nWPAP 10 949.88 37.13 3.91 6.09 0.9895\nWPAP 11 1448.69 49.21 3.99 7.01 0.9896\nWPAP 12 2,373.70 38.76 4.77 6.23 0.9869\nWPAP 13 3,251.40 29.15 3.61 5.40 0.9891\nWPAP 14 3,863.73 107.50 5.00 10.37 0.9850\nWPAP 15 4,162.78 23.67 3.23 4.87 0.9895\nWPAP 16 4,326.15 23.61 3.10 4.86 0.9906\nWPAP 17 5,228.90 14.42 2.22 3.80 0.9941\nWPAP 18 5,697.92 6.04 1.39 2.46 0.9961\nWPAP 19 6,173.15 46.62 3.63 6.83 0.9899\nWPAP 20 6,648.17 10.76 2.04 3.28 0.9942\nFrontiers inEnergy Research frontiersin.org08\nHuang et al. 10.3389/fenrg.2022.1055683\n12.23 respectively. They are small compared to the mean power\noutput value of 393.47 and the maximum value of 1552.76. The\nmean r2score of transformer forecasting results is 0.9849, which\nis 33.47%, 37.50%, 27.88% and 32.66% improvement compared\nto 0.7379, 0.7163, 0.7702 and 0.7424 of the other four models. It\ncan be seen that transformer forecasts very accurately, thanks to\nthe structure of encoder-decoder, the design of multi-headed self-\nattentiveness, the ability of masked multi-headed self-\nattentiveness to extract sequence information and the\nstructure of residuals, etc.\nTransformer has certain generalization performance, and we\nrandomly selected 12 WPAPs datasets, using the model\nparameters already trained by WPAP 1, to train the model\nand complete the prediction task. The experimental results are\nshown in Figure 8, and the prediction performance indexes of\ntransformer migration learning on each t WPAP dataset and the\ndistance of relative location between each WPAP and\nWPAP1 are shown inTable 2. The MSE, MAE and RMSE of\nforecasting results are 34.87, 3.35 and 5.57, which are also small.\nThe r2score of 0.9904 is likewise very close to 1. Transformer has\na better model migration effect due to its minimal inductive bias.\nIt can be seen that other WPAPs within the same area can use the\ntrained transformer model parameters for model training and\nachieve good prediction accuracy.\n5 Conclusion\nIn this paper, we illustrate the principle of transformer with\npowerful sequence modeling capabilities such as encoder to\ndecoder architecture, self-attentive mechanism, multi-headed\nattention, and sequence modeling using masks, and use it for\nWPAP power forecasting. We use 40 h of historical power data,\nwind speed data and ambient temperature data to predict the\noutput power of WPAPs for the next 8 h. The mean values of MSE,\nMAE and RMSE of the transformer model prediction results are\n304.38, 5.67 and 12.23, respectively, which are relative small\ncompared to the mean power output value and the maximum\nvalue. The r2score is 0.9849 which is very close to 1. We then use\nthe 12 WPAPs dataset for transformer ’s migration learning\nexperiment. The predicted results show that the MSE, MAE\nand RMSE are also small and the r2score is also very close to\n1. The transformer can have good migration performance within\nthe same area.\nData availability statement\nThe original contributions presented in the study are\nincluded in the article/supplementary material; further\ninquiries can bedirected to the corresponding author.\nAuthor contributions\nSH proposed the concept of the study and reviewed the\nmanuscript. YQ designed the project and revised the manuscript.\nCY completed the experiments and wrote the original draft.\nFunding\nThis work was supported by the National Key Research and\nDevelopment Program of China (No. 2022YFE0118500), the\nNational Natural Science Foundation of China (No.\n52207095) and Natural Science Foundation of Hunan\nProvince (No. 2022JJ40075).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could\nbe construed as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their afﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nA l i p o u r ,P . ,M u k h e r j e e ,S . ,a n dN a t e g h i ,R .( 2 019). Assessing climate sensitivity of peak\nelectricity load for resilientpower systems planning and operation: A study applied to the\nTexas region.Energy 185, 1143–1153. doi:10.1016/j.energy.2019.07.074\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., et al.\n(2021). On the opportunities and risks of foundation models. arXiv https://arxiv.org/\nabs/2108.07258.\nC a r i o n ,N . ,M a s s a ,F . ,S y n n a e v e ,G . ,U s u n i e r ,N . ,K i r i l l o v ,A . ,a n dZ a g o r u y k o ,\nS. (2020)., 12346. Springer, 213 –229. End-to-end object detection with\ntransformers Eur. Conf. Comput. Vis.\nCassola, F., and Burlando, M. (2012).Wind speed and wind energy forecast\nthrough Kalman ﬁltering of Numerical Weather Prediction model output.\nAppl. energy 99, 154–166. doi:10.1016/j.apenergy.2012.03.054\nChen, T., Lehr, J., Lavrova, O., and Martinez-Ramonz, M. (2016).“Distribution-\nlevel peak load prediction based on bayesian additive regression trees, ” in\nProceedings of the 2016 IEEE Power and Energy Society General Meeting\n(PESGM): IEEE), Boston, MA, USA, 1–5.\nCouncil, G. W. E. (2022). GWEC global wind Report 2022. Bonn, Germany:\nGlobal Wind Energy Council.\nFrontiers inEnergy Research frontiersin.org09\nHuang et al. 10.3389/fenrg.2022.1055683\nDeng, X., Shao, H., Hu, C., Jiang, D., and Jiang, Y. (2020). Wind power forecasting\nmethods based on deep learning: A survey. Comput. Model. Eng. Sci. 122 (1),\n273–301. doi:10.32604/cmes.2020.08768\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018).Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv https://arxiv.org/\nabs/1810.04805.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., et al. (2020).An image is worth 16x16 words: Transformers for image recognition\nat scale. https://arxiv.org/abs/2010.11929.\nHaniﬁ, S., Liu, X., Lin, Z., and Lotﬁan, S. (2020). A critical review of wind power\nforecasting methods— Past, present and future.Energies 13 (15), 3764. doi:10.3390/\nen13153764\nHe, K., Zhang, X., Ren, S., and Sun, J. (2002).“Deep residual learning for image\nrecognition,”in Proceedings of the IEEE conference on computer vision and pattern\nrecognition, San Juan, PR, USA, 770–778.\nHodge, B.-M., Zeiler, A., Brooks, D., Blau, G., Pekny, J., and Reklatis, G. (2011).,\n29. Elsevier, 1789 –1793. Improved wind power forecasting with ARIMA\nmodelsComput. Aided Chem. Eng.\nHossain, M. A., Chakrabortty, R. K., Elsawah, S., and Ryan, M. J. (2020).“Hybrid\ndeep learning model for ultra-short-term wind power forecasting,” in Proceedings\nof the 2020 IEEE International Conference on Applied Superconductivity and\nElectromagnetic Devices (ASEMD): IEEE, Tianjin, China, 1–2.\nHu, Q., Zhang, R., and Zhou, Y. (2016). Transfer learning for short-term wind\nspeed prediction with deep neural networks.Renew. Energy85, 83–95. doi:10.1016/\nj.renene.2015.06.034\nHuang, R., Huang, T., Gadh, R., and Li, N. (2012).“Solar generation prediction\nusing the ARMA model in a laboratory-level micro-grid,” in Proceedings of the\n2012 IEEE third international conference on smart grid communications\n(SmartGridComm): IEEE, Tainan, Taiwan, 528–533.\nKo, M.-S., Lee, K., Kim, J.-K., Hong, C. W., Dong, Z. Y., and Hur, K. (2020). Deep\nconcatenated residual network with bidirectional LSTM for one-hour-ahead wind\npower forecasting.IEEE Trans. Sustain. Energy12 (2), 1321–1335. doi:10.1109/tste.\n2020.3043884\nLahouar, A., and Slama, J. B. H. (2017). Hour-ahead wind power forecast based on\nrandom forests. Renew. energy 109, 529–541. doi:10.1016/j.renene.2017.03.064\nLai, G., Chang, W.-C., Yang, Y., and Liu, H. (2018).“Modeling long-and short-\nterm temporal patterns with deep neural networks,” in Proceedings of the The 41st\ninternational ACM SIGIR conference on research & development in information\nretrieval), Ann Arbor MI USA, 95–104.\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., et al.\n(2019). Bart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. arXiv https://arxiv.org/abs/1910.13461.\nL’Heureux, A., Grolinger, K., and Capretz, M. A. (2022). Transformer-based model for\nelectrical load forecasting.Energies15 (14), 4993. doi:10.3390/en15144993\nLi, J., and Armandpour, M. (2022). “Deep spatio-temporal wind power\nforecasting,” in Proceedings of the ICASSP 2022-2022 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP): IEEE),\nSingapore, 4138–4142.\nLi, L.-l., Cen, Z.-Y., Tseng, M.-L., Shen, Q., and Ali, M. H. (2021). Improving\nshort-term wind power prediction using hybrid improved cuckoo search\narithmetic-Support vector regression machine. J. Clean. Prod. 279, 123739.\ndoi:10.1016/j.jclepro.2020.123739\nLi, L.-L., Zhao, X., Tseng, M.-L., and Tan, R. R. (2020). Short-term wind power\nforecasting based on support vector machine with improved dragonﬂy algorithm.\nJ. Clean. Prod.242, 118447. doi:10.1016/j.jclepro.2019.118447\nLi, L., Liu, Y.-q., Yang, Y.-p., Shuang, H., and Wang, Y.-m. (2013). A physical\napproach of the short-term wind power prediction based on CFD pre-calculated\nﬂow ﬁelds. J. Hydrodyn. 25 (1), 56–61. doi:10.1016/s1001-6058(13)60338-8\nLin, Y., Koprinska, I., and Rana, M. (2020)., 12534. Springer, 616–628.SpringNet:\nTransformer and Spring DTW for time series forecastingInt. Conf. Neural Inf.\nProcess.\nLin, Z., and Liu, X. (2020). Assessment of wind turbine aero-hydro-servo-elastic\nmodelling on the effects of mooring line tension via deep learning.Energies 13 (9),\n2264. doi:10.3390/en13092264\nLiu, B., Zhao, S., Yu, X., Zhang, L., and Wang, Q. (2020). A novel deep learning\napproach for wind power forecasting based on WD-LSTM model.Energies 13 (18),\n4964. doi:10.3390/en13184964\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., et al. (2019).Roberta: A\nrobustly optimized bert pretraining approach . arXiv https://arxiv.org/abs/1907.\n11692.\nLiu, Y., Shi, J., Yang, Y., and Han, S. (2009). Piecewise support vector machine\nmodel for short-term wind-power prediction.Int. J. Green Energy6 (5), 479–489.\ndoi:10.1080/15435070903228050\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (2021). “Swin\ntransformer: Hierarchical vision transformer using shifted windows, ” in\nProceedings of the IEEE/CVF International Conference on Computer Vision),\nMontreal, BC, Canada, 10012–10022.\nLópez Santos, M., García-Santiago, X., Echevarría Camarero, F., Blázquez Gil, G.,\nand Carrasco Ortega, P. (2022). Application of temporal fusion transformer for day-\nahead PV power forecasting.Energies 15 (14), 5232. doi:10.3390/en15145232\nLu, K., Sun, W. X., Wang, X., Meng, X. R., Zhai, Y., Li, H. H., et al. (2018)., 186.\nIOP Publishing, 012020.Short-term wind power prediction model based on\nencoder-decoder LSTM, IOP Conf. Ser. Earth Environ. Sci.\nNiu, Z., Yu, Z., Tang, W., Wu, Q., and Reformat, M. (2020). Wind power\nforecasting using attention-based gated recurrent unit network.Energy 196, 117081.\ndoi:10.1016/j.energy.2020.117081\nPhan, Q.-T., Wu, Y.-K., and Phan, Q.-D. (2022). “An approach using\ntransformer-based model for short-term PV generation forecasting,\n” in\nProceedings of the 2022 8th International Conference on Applied System\nInnovation (ICASI): IEEE, Nantou, Taiwan, 17–20.\nPoggi, P., Muselli, M., Notton, G., Cristofari, C., and Louche, A. (2003).\nForecasting and simulating wind speed in Corsica by using an autoregressive\nmodel. Energy Convers. Manag.44 (20), 3177–3196. doi:10.1016/s0196-8904(03)\n00108-0\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019).\nLanguage models are unsupervised multitask learners.OpenAI blog 1( 8 ) ,9 .\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., et al. (2020).\nExploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nJ. Mach. Learn. Res.21 (140), 1–67.\nShahid, F., Zameer, A., and Muneeb, M. (2021). A novel genetic LSTM model for\nwind power forecast.Energy 223, 120069. doi:10.1016/j.energy.2021.120069\nSun, R., Zhang, T., He, Q., and Xu, H. (2021). Review on key technologies and\napplications in wind power forecasting.High. Volt. Eng.47, 1129–1143.\nSun, Z., Zhao, S., and Zhang, J. (2019). Short-term wind power forecasting on\nmultiple scales using VMD decomposition, K-means clustering and LSTM\nprincipal computing. IEEE Access 7, 166917–166929. doi:10.1109/access.2019.\n2942040\nTouvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., and Jégou, H. (2021).\n“Going deeper with image transformers, ” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision), Montreal, BC, Canada, 32–42.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need.Adv. neural Inf. Process. Syst.30.\nWang, Y., Gao, J., Xu, Z., and Li, L. (2020). A short-term output power prediction\nmodel of wind power based on deep learning of grouped time series.Eur. J. Electr.\nEng. 22 (1), 29–38. doi:10.18280/ejee.220104\nWu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., et al. (2021).“Cvt:\nIntroducing convolutions to vision transformers,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision), Montreal, BC, Canada, 22–31.\nWu, Q., Guan, F., Lv, C., and Huang, Y. (2021). Ultra-short-term multi-step wind\npower forecasting based on CNN-LSTM.IET Renew. Power Gen.15 (5), 1019–1029.\ndoi:10.1049/rpg2.12085\nWu, Y.-K., and Hong, J.-S. (2007). A literature review of wind forecasting\ntechnology in the world.IEEE Lausanne Power Tech.2007, 504–509.\nYesilbudak, M., Sagiroglu, S., and Colak, I. (2017). A novel implementation of\nkNN classiﬁer based on multi-tupled meteorological input data for wind power\nprediction. Energy Convers. Manag. 135, 434–444. doi:10.1016/j.enconman.2016.\n12.094\nYu, R., Gao, J., Yu, M., Lu, W., Xu, T., Zhao, M., et al. (2019). LSTM-EFG for wind\npower forecasting based on sequential correlation features.Future Gener. Comput.\nSyst. 93, 33–42. doi:10.1016/j.future.2018.09.054\nZhou, J., Lu, X., Xiao, Y., Su, J., Lyu, J., Ma, Y., et al. (2022).Sdwpf: A dataset for\nspatial dynamic wind power forecasting challenge at kdd cup 2022. arXiv https://\narxiv.org/abs/2208.04360.\nFrontiers inEnergy Research frontiersin.org10\nHuang et al. 10.3389/fenrg.2022.1055683",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.683731198310852
    },
    {
      "name": "Feed forward",
      "score": 0.6559911966323853
    },
    {
      "name": "Computer science",
      "score": 0.6123714447021484
    },
    {
      "name": "Deep learning",
      "score": 0.6085186004638672
    },
    {
      "name": "Wind power",
      "score": 0.5276374220848083
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5074543356895447
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5065523386001587
    },
    {
      "name": "Encoder",
      "score": 0.4929923713207245
    },
    {
      "name": "Artificial neural network",
      "score": 0.4799961745738983
    },
    {
      "name": "Residual",
      "score": 0.4485020041465759
    },
    {
      "name": "Machine learning",
      "score": 0.3719736933708191
    },
    {
      "name": "Engineering",
      "score": 0.25079306960105896
    },
    {
      "name": "Control engineering",
      "score": 0.22941717505455017
    },
    {
      "name": "Electrical engineering",
      "score": 0.1596457064151764
    },
    {
      "name": "Algorithm",
      "score": 0.11104759573936462
    },
    {
      "name": "Voltage",
      "score": 0.07115793228149414
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}