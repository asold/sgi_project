{
  "title": "Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining",
  "url": "https://openalex.org/W3104763958",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2108884846",
      "name": "Chengyu Wang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2114674632",
      "name": "Minghui Qiu",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2097175565",
      "name": "Jun Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2115656599",
      "name": "Xiaofeng He",
      "affiliations": [
        "East China Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2970678056",
    "https://openalex.org/W3005445152",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4293350112",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4322588812",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2982426914",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2962970380",
    "https://openalex.org/W2911300548",
    "https://openalex.org/W2962897020",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3008424731",
    "https://openalex.org/W2996193212",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2971167006",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2970106668",
    "https://openalex.org/W2788768841",
    "https://openalex.org/W4287870915",
    "https://openalex.org/W2895531857",
    "https://openalex.org/W1986614398",
    "https://openalex.org/W2951221758",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2890459330",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2739945392",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2945978556",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2809324505",
    "https://openalex.org/W2982455176",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2966989210",
    "https://openalex.org/W3041133507",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W2770645414",
    "https://openalex.org/W2995923603",
    "https://openalex.org/W2565948902",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W4288122335",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2163302275",
    "https://openalex.org/W4288087680",
    "https://openalex.org/W2964113870",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2592170186",
    "https://openalex.org/W2963777311",
    "https://openalex.org/W2965491249",
    "https://openalex.org/W2950434427",
    "https://openalex.org/W2970692876",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W2998139906",
    "https://openalex.org/W2029344051",
    "https://openalex.org/W2786559811"
  ],
  "abstract": "Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), serving as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions. After MFT, the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3094–3104,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n3094\nMeta Fine-Tuning Neural Language Models for Multi-Domain Text\nMining\nChengyu Wang1, Minghui Qiu1∗, Jun Huang1, Xiaofeng He2\n1 Alibaba Group 2 East China Normal University\n{chengyu.wcy, minghui.qmh, huangjun.hj}@alibaba-inc.com\nhexf@cs.ecnu.edu.cn\nAbstract\nPre-trained neural language models bring sig-\nniﬁcant improvement for various NLP tasks,\nby ﬁne-tuning the models on task-speciﬁc\ntraining sets. During ﬁne-tuning, the param-\neters are initialized from pre-trained models\ndirectly, which ignores how the learning pro-\ncess of similar NLP tasks in different domains\nis correlated and mutually reinforced. In this\npaper, we propose an effective learning proce-\ndure named Meta Fine-Tuning(MFT), serving\nas a meta-learner to solve a group of similar\nNLP tasks for neural language models. In-\nstead of simply multi-task training over all\nthe datasets, MFT only learns from typical in-\nstances of various domains to acquire highly\ntransferable knowledge. It further encour-\nages the language model to encode domain-\ninvariant representations by optimizing a se-\nries of novel domain corruption loss functions.\nAfter MFT, the model can be ﬁne-tuned for\neach domain with better parameter initializa-\ntion and higher generalization ability. We im-\nplement MFT upon BERT to solve several\nmulti-domain text mining tasks. Experimental\nresults conﬁrm the effectiveness of MFT and\nits usefulness for few-shot learning. 1\n1 Introduction\nRecent years has witnessed a boom in pre-trained\nneural language models. Notable works include\nELMo (Peters et al., 2018), BERT (Devlin et al.,\n2019), Transformer-XL (Dai et al., 2019), AL-\nBERT (Lan et al., 2019), StructBERT (Wang et al.,\n2019b) and many others. These models revolution-\nize the learning paradigms of various NLP tasks.\nAfter pre-training, only a few ﬁne-tuning epochs\nare required to train models for these tasks.\nThe “secrets” behind this phenomenon owe to\nthe models’ strong representation learning power\n∗Corresponding author.\n1Our code will be available at: https://github.\ncom/alibaba/EasyTransfer/.\nto encode the semantics and linguistic knowledge\nfrom massive text corpora (Jawahar et al., 2019; Ko-\nvaleva et al., 2019; Liu et al., 2019a; Tenney et al.,\n2019). By simple ﬁne-tuning, models can trans-\nfer the universal Natural Language Understand-\ning (NLU) abilities to speciﬁc tasks (Wang et al.,\n2019a). However, state-of-the art language mod-\nels mostly utilize self-supervised tasks during pre-\ntraining (for instance, masked language modeling\nand next sentence prediction in BERT (Devlin et al.,\n2019)). This unavoidably creates a learning gap\nbetween pre-training and ﬁne-tuning. Besides, for\na group of similar tasks, conventional practices re-\nquire the parameters of all task-speciﬁc models to\nbe initialized from the same pre-trained language\nmodel, ignoring how the learning process in differ-\nent domains is correlated and mutually reinforced.\nA basic solution is ﬁne-tuning models by multi-\ntask learning. Unfortunately, multi-task ﬁne-tuning\nof BERT does not necessarily yield better perfor-\nmance across all the tasks (Sun et al., 2019a). A\nprobable cause is that learning too much from\nother tasks may force the model to acquire non-\ntransferable knowledge, which harms the overall\nperformance. A similar ﬁnding is presented in Bin-\ngel and Søgaard (2017); McCann et al. (2018) on\nmulti-task training of neural networks. Addition-\nally, language models such as BERT do not have\nthe “shared-private” architecture (Liu et al., 2017)\nto enable effective learning of domain-speciﬁc and\ndomain-invariant features. Other approaches mod-\nify the structures of language models to accom-\nmodate multi-task learning and mostly focus on\nspeciﬁc applications, without providing a uniﬁed\nsolution for all the tasks (Stickland and Murray,\n2019; Zhou et al., 2019b; Gulyaev et al., 2020).\nA recent study (Finn et al., 2017) reveals that\nmeta-learning achieves better parameter initializa-\ntion for a group of tasks, which improves the mod-\nels’ generalization abilities in different domains\n3095\nLM\nDomain A\nDomain B\nDomain C\nθA\nθB\nθC\nFine-tune\nMeta Task\nLearning\nDomain A\nDomain B\nDomain C\nθ’A\nθ’B\nθ’C\nFine-tune\nLM\nMeta Fine-tune\nθM\na) Conventional Approach b) The Proposed Approach\nFigure 1: Comparison between ﬁne-tuning and MFT.\n“LM” refers to pre-trained language models.\nand makes them easier to ﬁne-tune. As pre-trained\nlanguage models have general NLU abilities, they\nshould also have the ability to learn solving a group\nof similar NLP tasks. In this work, we propose a\nseparate learning procedure, inserted between pre-\ntraining and ﬁne-tuning, named Meta Fine-Tuning\n(MFT). This work is one of the early attempts for\nimproving ﬁne-tuning of neural language models\nby meta-learning. Take the review analysis task\nas an example. MFT only targets at learning the\npolarity of reviews (positive or negative) in general,\nignoring features of speciﬁc aspects or domains.\nAfter that, the learned model can be adapted to\nany domains by ﬁne-tuning. The comparison be-\ntween ﬁne-tuning and MFT is shown in Figure 1.\nSpeciﬁcally, MFT ﬁrst learns the embeddings of\nclass prototypes from multi-domain training sets,\nand assigns typicality scores to individuals, indi-\ncating the transferability of each instance. Apart\nfrom minimizing the multi-task classiﬁcation loss\nover typical instances, MFT further encourages the\nlanguage model to learn domain-invariant repre-\nsentations by jointly optimizing a series of novel\ndomain corruption loss functions.\nFor evaluation, we implement the MFT strategy\nupon BERT (Devlin et al., 2019) for three multi-\ndomain text mining tasks: i) natural language infer-\nence (Williams et al., 2018) (sentence-pair classi-\nﬁcation), ii) review analysis (Blitzer et al., 2007)\n(sentence classiﬁcation) and iii) domain taxonomy\nconstruction (Luu et al., 2016) (word-pair classiﬁ-\ncation). Experimental results show that the effec-\ntiveness and superiority of MFT. We also show that\nMFT is highly useful for multi-domain text mining\nin the few-shot learning setting. 2\n2 Related Work\nWe overview recent advances on pre-trained lan-\nguage models, transfer learning and meta-learning.\n2Although we focus on MFT for BERT only, MFT is gen-\neral and can be applied to other language models easily.\n2.1 Pre-trained Language Models\nPre-trained language models have gained much at-\ntention from the NLP community recently (Qiu\net al., 2020). Among these models, ELMo (Peters\net al., 2018) learns context-sensitive embeddings\nfor each token form both left-to-right and right-to-\nleft directions. BERT (Devlin et al., 2019) is usu-\nally regarded as the most representative work, em-\nploying transformer encoders to learn language rep-\nresentations. The pre-training technique of BERT\nis improved in Liu et al. (2019c). Follow-up works\nemploy transformer-based architectures, including\nTransformer-XL (Dai et al., 2019), XLNet (Yang\net al., 2019), ALBERT (Lan et al., 2019), Struct-\nBERT (Wang et al., 2019b) and many more. They\nchange the unsupervised learning objectives of\nBERT in pre-training. MT-DNN (Liu et al., 2019b)\nis the representative of another type of pre-trained\nlanguage models, which employs supervised learn-\ning objectives across tasks to learn representations.\nAfter language models are pre-trained, they can\nbe ﬁne-tuned for a variety of NLP tasks. The tech-\nniques of ﬁne-tuning BERT are summarized in Sun\net al. (2019a). Cui et al. (2019) improve BERT’s\nﬁne-tuning by sparse self-attention. Arase and Tsu-\njii (2019) introduce the concept of “transfer ﬁne-\ntuning”, which injects phrasal paraphrase relations\ninto BERT. Compared to previous methods, ﬁne-\ntuning for multi-domain learning has not been suf-\nﬁciently studied.\n2.2 Transfer Learning\nTransfer learning aims to transfer the resources\nor models from one domain (the source domain)\nto another (the target domain), in order to im-\nprove the model performance of the target domain.\nDue to space limitation, we refer readers to the\nsurveys (Pan and Yang, 2010; Lu et al., 2015;\nZhuang et al., 2019) for an overview. For NLP\napplications, the “shared-private” architecture (Liu\net al., 2017) is highly popular, which include sub-\nnetworks for learning domain-speciﬁc represen-\ntations and a shared sub-network for knowledge\ntransfer and domain-invariant representation learn-\ning. Recently, adversarial training has been fre-\nquently applied (Shen et al., 2018; Hu et al., 2019;\nCao et al., 2018; Li et al., 2019b; Zhou et al.,\n2019a), where the domain adversarial classiﬁers are\ntrained to help the models to learn domain-invariant\nfeatures. Multi-domain learning is a special case of\ntransfer learning whose goal is to transfer knowl-\n3096\nInput Tokens\n[CLS] \nEmbeddings Token Embeddings\nMean Pooling \nof Token\nEmbeddings\nDomain \nEPEHGGLQJV\nCorrupted Domain \nClassiﬁer 2Domain A\nDomain B\nDomain C\nMulti-Domain Class Prototypes\n[CLS] \nEmbeddings Token Embeddings\n[CLS] \nEmbeddings Token Embeddings\n[CLS]\n+\n+\nTrue Domain Label\nCorrupted Domain \nClassiﬁer 1\nPre-trained BERT Model\nLabel Prediction \nClassiﬁer\nTypicality Scores\nWeight \nInjection to \nClassiﬁers\nLow\nHigh\nFigure 2: The neural architecture of MFT for BERT (Devlin et al., 2019). Due to space limitation, we only show\ntwo corrupted domain classiﬁers and three layers of transformer encoders, with others omitted.\nedge across multiple domains for mutual training\nreinforcement (Pei et al., 2018; Li et al., 2019a; Cai\nand Wan, 2019). Our work also addresses multi-\ndomain learning, but solves the problem from a\nmeta-learning aspect.\n2.3 Meta-learning\nCompared to transfer learning, meta-learning is a\nslightly different learning paradigm. Its goal is to\ntrain meta-learners that can adapt to a variety of\ndifferent tasks with little training data (Vanschoren,\n2018), mostly applied to few-shot learning, which\nis typically formulated as a K-way N-shot learning\nproblem. In NLP, existing meta-learning models\nmostly focus on training meta-learners for single\napplications, such as link prediction (Chen et al.,\n2019), dialog systems (Madotto et al., 2019), lexi-\ncal relation classiﬁcation (Wang et al., 2020) and se-\nmantic parsing (Guo et al., 2019). Dou et al. (2019)\nleverage meta-learning for low-resource NLU.\nCompared with traditional meta-learning re-\nsearch, the task of our work is not K-way N-shot.\nInstead, we aim to employ meta-learning to train a\nbetter “meta-learner” which captures transferable\nknowledge across domains. In this sense, our work\ncan be also viewed as a transfer learning algorithm\nwhich employs meta-learning for better knowledge\ntransfer and fast domain adaptation.\n3 MFT: The Proposed Framework\nIn this section, we start with some basic notations\nand an overview of MFT. After that, we describe\nthe algorithmic techniques of MFT in detail.\n3.1 Overview\nDenote Dk = {xk\ni ,yk\ni |i∈[1,Nk]}as the training\nset of the kth domain, where xk\ni and yk\ni are the\ninput text and the class label of the ith sample, re-\nspectively3. Nk is the number of total samples in\nDk. The goal of MFT is to train a meta-learner ini-\ntialized from a pre-trained language model, based\non the training sets of Kdomains: D= ⋃K\nk=1 Dk.\nThe meta-learner provides better parameter initial-\nizations, such that it can be easily adapted to each\nof the Kdomains by ﬁne-tuning the meta-learner\nover the training set of the kth domain separately.\nDue to the large parameter space of neural lan-\nguage models, it is computationally challenging to\nsearch for the optimal values of the meta-learner’s\nparameters. As discussed earlier, building a trivial\nmulti-task learner over Ddoes not guarantee sat-\nisfactory results either (Sun et al., 2019a). Here,\nwe set up two design principles for MFT: Learn-\ning from Typicalityand Learning Domain-invariant\nRepresentations, introduced as follows:\nLearning from Typicality To make the meta-\nlearner easier to be ﬁne-tuned to any domains, the\nencoded knowledge should be highly general and\ntransferable, not biased towards speciﬁc domains.\nHence, only typical instances w.r.t. all the domains\nshould be the priority learning targets. We ﬁrst\ngenerate multi-domain class prototypes from Dto\nsummarize the semantics of training data. Based\non the prototypes, we compute typicality scores for\nall training instances, treated as weights for MFT.\nLearning Domain-invariant Representations\nA good meta-learner should be adapted to any do-\nmains quickly. Since BERT has strong represen-\ntation power, this naturally motivates us to learn\ndomain-invariant representations are vital for fast\n3Note that xk\ni can be either a single sentence, a sentence\npair, or any other possible input texts of the target NLP task.\n3097\ndomain adaptation (Shen et al., 2018). In MFT, be-\nsides minimizing the classiﬁcation loss, we jointly\nminimize new learning objectives to force the lan-\nguage model to have domain-invariant encoders.\nBased on the two general principles, we design\nthe neural architecture of MFT, with the example\nfor BERT (Devlin et al., 2019) shown in Figure 2.\nThe technical details are introduced subsequently.\n3.2 Learning from Typicality\nDenote Mas the class label set of all K do-\nmains. Dk\nm is the collection of input texts in\nDk that have class label m ∈ M, i.e., Dk\nm =\n{xk\ni |(xk\ni ,yk\ni ) ∈ Dk,yk\ni = m}. As class proto-\ntypes can summarize the key characteristics of the\ncorresponding data (Yao et al., 2020), we treat the\nclass prototype ck\nm as the averaged embeddings\nof all the input texts in Dk\nm. Formally, we have\nck\nm = 1\n|Dkm|\n∑\nxk\ni ∈Dkm\nE(xk\ni ) where E(·) maps xk\ni to\nits d-dimensional embeddings. As for BERT (De-\nvlin et al., 2019), we utilize the mean pooling of\nrepresentations of xk\ni from the last transformer en-\ncoder as E(xk\ni ).\nIdeally, we regard a training instance (xk\ni ,yk\ni ) to\nbe typical if it is semantically close to its class pro-\ntotype ck\nm, and also is not too far away from class\nprototypes generated from other domains for high\ntransferability. Therefore, the typicality score tk\ni of\n(xk\ni ,yk\ni ) can be deﬁned as follows:4\ntk\ni =αcos(E(xk\ni ),ck\nm)\n+ 1 −α\nK−1 ·\nK∑\n˜k=1\n1(˜k̸=k) cos(E(xk\ni ),c\n˜k\nm),\nwhere αis the pre-deﬁned balancing factor ( 0 <\nα <1), cos(·,·) is the cosine similarity function\nand 1(·) is the indicator function that returns 1 if\nthe input Boolean function is true and 0 otherwise.\nAs one prototype may not be insufﬁcient to rep-\nresent the complicated semantics of the class (Cao\net al., 2017), we can also generate multiple proto-\ntypes by clustering, with the jth prototype to be\nck\nmj . Here, tk\ni is extended by the following formula:\ntk\ni = α\n∑\nn∈Mβn cos(E(xk\ni ),ck\nn)∑\nn∈Mβn\n+ 1 −α\nK−1 ·\nK∑\n˜k=1\n1(˜k̸=k)\n∑\nn∈Mβn cos(E(xk\ni ),c˜k\nn)\n∑\nn∈Mβn\n,\n4Here, we assume that the training instance (xk\ni , yk\ni ) has\nthe class label m ∈ M. Because each instance is associated\nwith only one typicality score, for simplicity, we denote it as\ntk\ni , instead of tk\ni,m.\nwhere βn >0 is the cluster membership of xk\ni w.r.t.\neach class label n∈M.\nAfter typicality scores are computed, we discuss\nhow to set up the learning objectives for MFT. The\nﬁrst loss is the multi-task typicality-sensitive label\nclassiﬁcation loss LTLC . It penalizes the text clas-\nsiﬁer for predicting the labels of typical instances\nof all Kdomains incorrectly, which is deﬁned as:5\nLTLC = − 1\n|D|\n∑\n(xk\ni ,yk\ni )∈D\n∑\nm∈M\n1(yk\ni =m)tk\ni ·\nlog τm(f(xk\ni )),\nwhere tk\ni serves as the weight of each training in-\nstance. τm(f(xk\ni )) is the predicted probability of\nxk\ni having the class label m ∈ M, with the d-\ndimensional “[CLS]” token embeddings of the last\nlayer of BERT (denoted as f(xk\ni )) as features.\n3.3 Learning Domain-invariant\nRepresentations\nBased on previous research of domain-invariant\nlearning (Shen et al., 2018; Hu et al., 2019), we\ncould add an additional domain adversarial clas-\nsiﬁer for MFT to optimize. However, we observe\nthat adding such classiﬁers to models such as BERT\nmay be sub-optimal. For ease of understanding, we\nonly consider two domains k1 and k2. The loss of\nthe adversarial domain classiﬁer LAD is:\nLAD = − 1\nNk1 + Nk2\n∑\n(xk\ni ,yk\ni )∈Dk1 ∪Dk2\n(yk\ni log σ(xk\ni ) + (1−yk\ni ) log(1−σ(xk\ni ))),\nwhere yk\ni = 1if the domain is k1 and 0 otherwise.\nσ(xk\ni ) is the predicated probability of such adver-\nsarial domain classiﬁer. In the min-max game of\nadversarial learning (Shen et al., 2018), we need\nto maximize the loss LAD such that the domain\nclassiﬁer fails to predict the true domain label. The\nmin-max game between encoders and adversarial\nclassiﬁers is computationally expensive, which is\nless appealing to MFT over large language models.\nAdditionally, models such as BERT do not have\nthe “shared-private” architecture (Liu et al., 2017),\nfrequently used for transfer learning. One can also\nreplace LAD by asking the classiﬁer to predict the\nﬂipped domain labels directly (Shu et al., 2018; Hu\n5For clarity, we omit all the regularization terms in objec-\ntive functions throughout this paper.\n3098\net al., 2019). Hence, we can instead minimize the\nﬂipped domain loss LFD :\nLFD = − 1\nNk1 + Nk2\n∑\n(xk\ni ,yk\ni )∈Dk1 ∪Dk2\n((1 −yk\ni ) logσ(xk\ni ) +yk\ni log(1 −σ(xk\ni ))).\nWe claim that, applyingLFD to BERT as an aux-\niliary loss does not necessarily generate domain-\ninvariant features. When LFD is minimized, σ(xk\ni )\nalways tends to predict the wrong domain label\n(which predicts k1 for k2 and k2 for k1). The opti-\nmization of LFD still makes the learned features to\nbe domain-dependent, since the domain informa-\ntion is encoded implicitly in σ(xk\ni ), only with do-\nmain labels inter-changed. A similar case holds for\nmultiple domains where we only force the classiﬁer\nto predict the domain of the input text xkj\ni into any\none of the reminder K−1 domains (excluding kj).\nTherefore, it is necessary to modify LFD which\ntruly guarantees domain invariance and avoids the\nexpensive (and sometimes unstable) computation\nof adversarial training.\nIn this work, we propose the domain corruption\nstrategy to address this issue. Given a training in-\nstance (xk\ni ,yk\ni ) of the kth domain, we generate a\ncorrupt domain label zi from a corrupted domain\ndistribution Pr(zi). zi is unrelated to the true do-\nmain label k, which may or may not be the same\nas k. The goal of the domain classiﬁer is to ap-\nproximate Pr(zi) instead of always predicting the\nincorrect domains as in Hu et al. (2019). In practice,\nPr(zi) can be deﬁned with each domain uniformly\ndistributed, if the Kdomain datasets are relatively\nbalanced in size. To incorporate prior knowledge\nof domain distributions into the model, Pr(zi) can\nalso be non-uniform, with domain probabilities esti-\nmated from Dby maximum likelihood estimation.\nConsider the neural architecture of transformer\nencoders in BERT (Devlin et al., 2019). Let hl(xk\ni )\nbe the d-dimensional mean pooling of the token\nembeddings of xk\ni in the lth layer (excluding the\n“[CLS]” embeddings), i.e.,\nhl(xk\ni ) =Avg(hl,1(xk\ni ),··· ,hl,Max (xk\ni )),\nwhere hl,j(xk\ni ) represents the l-the layer embed-\nding of the jth token in xk\ni , and Max is the max-\nimum sequence length. Additionally, we learn a\nd-dimensional domain embedding of the true do-\nmain label of (xk\ni ,yk\ni ), denoted as ED(k). The\ninput features are constructed by adding the two\nAlgorithm 1Learning Algorithm for MFT\n1: Restore BERT’s parameters from the pre-trained model,\nwith others randomly initialized;\n2: for each domain k and each class m do\n3: Compute prototype embeddings ck\nm;\n4: end for\n5: for each training instance (xk\ni , yk\ni ) ∈ Ddo\n6: Compute typicality score tk\ni ;\n7: end for\n8: while number of training steps do not reach a limit do\n9: Sample a batch {(xk\ni , yk\ni )} from D;\n10: Shufﬂe domain labels of {(xk\ni , yk\ni )} to generate {zi};\n11: Estimate model predictions of inputs {(xk\ni , k)} and\ncompare them against {(yk\ni , zi)};\n12: Update all model parameters by back propagation;\n13: end while\nembeddings: hl(xk\ni ) +ED(k), with the typicality-\nsensitive domain corruption loss LTDC as:\nLTDC = − 1\n|D|\n∑\n(xk\ni ,yk\ni )∈D\nK∑\nk=1\n1(k=zi)tk\ni\n·log δzi(hl(xk\ni ) +ED(k)),\nwhere δzi(·) is the predicted probability of the input\nfeatures having the corrupted domain label zi. We\ndeliberately feed the true domain embeddingED(k)\ninto the classiﬁer to make sure even if the classiﬁer\nknows the true domain information from ED(k), it\ncan only generate corrupted outputs. In this way,\nwe force the BERT’s representationshl(xk\ni ) to hide\nany domain information from being revealed, mak-\ning the representations of xk\ni domain-invariant.\nWe further notice that neural language models\nmay have deep layers. To improve the level of\ndomain invariance of all layers and create a balance\nbetween effectiveness and efﬁciency, we follow the\nwork (Sun et al., 2019b) to train a series of skip-\nlayer classiﬁers. Denote Ls as the collection of\nselected indices of layers (for example, one can set\nLs = {4,8,12}for BERT-base). The skip-layer\ndomain corruption loss LSDC is deﬁned as the\naverage cross-entropy loss of all |Ls|classiﬁers,\ndeﬁned as follows:\nLSDC = − 1\n|Ls|·|D|\n∑\n(xk\ni ,yk\ni )∈D\n∑\nl∈Ls\nK∑\nk=1\n1(k=zi)tk\ni ·log δzi(hl(xk\ni ) +ED(k)).\nIn summary, the overall loss Lfor MFT to mini-\nmize is a linear combination of LTLC and LSDC ,\ni.e., L= LTLC + λLSDC , where λ> 0 is a tuned\nhyper-parameter.\n3099\n3.4 Joint Optimization\nWe describe how to optimize Lfor MFT. Based on\nthe formation of L, it is trivial to derive that:\nL= − 1\n|D|\n∑\n(xk\ni ,yk\ni )∈D\n(\n∑\nm∈M\n1(yk\ni =m)tk\ni ·\nlog τm(f(xk\ni )) + λ\n|Ls|\n∑\nl∈Ls\nK∑\nk=1\n1(k=zi)tk\ni ·\nlog δzi(hl(xk\ni ) +ED(k))).\nAs seen, MFT can be efﬁciently optimized via\ngradient-based algorithms with slight modiﬁca-\ntions. The procedure is shown in Algorithm 1.\nIt linearly scans the multi-domain training set D\nto compute prototypes ck\nm and typicality scores tk\ni .\nNext, it updates model parameters by batch-wise\ntraining. For each batch {(xk\ni ,yk\ni )}, as an efﬁcient\nimplementation, we shufﬂe the domain labels to\ngenerate the corrupted labels {zi}, as an approx-\nimation of sampling from the original corrupted\ndomain distribution Pr(zi). This trick avoids the\ncomputation over the whole dataset, and be adapted\nto the changes of domain distributions if new train-\ning data is continuously added to Dthrough time.\nWhen the iterations stop, we remove all the ad-\nditional layers that we have added for MFT, and\nﬁne-tune BERT for the K domains over their re-\nspective training sets, separately.\n4 Experiments\nWe conduct extensive experiments to evaluate MFT\non multiple multi-domain text mining tasks.\n4.1 Datasets and Experimental Settings\nWe employ the Google’s pre-trained BERT model6\nas the language model, with dimension d = 768.\nThree multi-domain NLP tasks are used for evalua-\ntion, with statistics of datasets reported in Table 1:\n• Natural language inference: predicting the\nrelation between two sentences as “entail-\nment”, “neutral” or “contradiction”, using the\ndataset MNLI (Williams et al., 2018). MNLI\nis a large-scale benchmark corpus for evaluat-\ning natural language inference models, with\nmulti-domain data divided into ﬁve genres.\n• Review analysis: classifying the product re-\nview sentiment of the famous Amazon Review\n6We use the uncased, base version of BERT. See:https:\n//github.com/google-research/bert.\nDataset Domain #Train #Dev #Test\nMNLI\nTelephone 83,348 2,000 -\nGovernment 77,350 2,000 -\nSlate 77,306 2,000 -\nTravel 77,350 2,000 -\nFiction 77,348 2,000 -\nAmazon\nBook 1,763 120 117\nDVD 1,752 120 128\nElectronics 1,729 144 127\nKitchen 1,756 119 125\nTaxonomy\nAnimal 8,650 1,081 1,076\nPlant 6,188 769 781\nVehicle 842 114 103\nTable 1: Statistical summarization of datasets.\nDataset (Blitzer et al., 2007) (containing prod-\nuct reviews of four domains crawled from the\nAmazon website) as positive or negative.\n• Domain taxonomy construction: predict-\ning whether there exists a hypernymy (“is-\na”) relation between two terms (words/noun\nphrases) for taxonomy derivation. Labeled\nterm pairs sampled from three domain tax-\nonomies are used for evaluation. The domain\ntaxonomies are constructed by Velardi et al.\n(2013). with labeled datasets created and re-\nleased by Luu et al. (2016) 7 .\nBecause MNLI does not contain public labeled\ntest sets that can be used for single-genre evalua-\ntion, we hold out 10 thousand training instances\nfrom the original training set for parameter tuning\nand report the performance of the original develop-\nment sets. We hold out 2,000 labeled reviews from\nthe Amazon dataset (Blitzer et al., 2007) and split\nthem into development and test sets. As for the\ntaxonomy construction task, because BERT does\nnot naturally support word-pair classiﬁcation, we\ncombine a term pair to form a sequence of tokens\nseparated by the special token “[SEP]” as input.\nThe ratios of training, development and testing sets\nof the three domain taxonomy datasets are set as\n80%:10%:10%.\nThe default hyper-parameter settings of MFT\nare as follows: α = 0.5, Ls = {4,8,12}and\nλ = 0.1. During model training, we run 1 ∼2\nepochs of MFT and further ﬁne-tune the model in\n2 ∼4 epochs for each domain, separately. The\ninitial learning rate is set as 2 ×10−5 in all exper-\niments. The regularization hyper-parameters, the\n7Following Luu et al. (2016), in this task, we only do the\nbinary classiﬁcation of domain term pairs as hypernymy or\nnon-hypernymy and do not consider reconstructing the graph\nstructures of the domain taxonomies.\n3100\noptimizer and the reminder settings are the same as\nin Devlin et al. (2019). In MFT, only 7K ∼11.5K\nadditional parameters need to be added (depend-\ning on the number of domains), compared to the\noriginal BERT model. All the algorithms are imple-\nmented with TensorFlow and trained with NVIDIA\nTesla P100 GPU. The training time takes less than\none hour. For evaluation, we use Accuracy as the\nevaluation metric for all models trained via MFT\nand ﬁne-tuning. All the experiments are conducted\nthree times, with the average scores reported.\n4.2 General Experimental Results\nWe report the general testing results of MFT. For\nfair comparison, we implement following the ﬁne-\ntuning methods as strong baselines:\n• BERT (S): It ﬁne-tunes K BERT models,\neach with single-domain data.\n• BERT (Mix): It combines all the domain\ndata and ﬁne-tunes a single BERT model only.\n• BERT (MTL): It ﬁne-tunes BERT by multi-\ntask ﬁne-tuning (Sun et al., 2019a).\n• BERT (Adv): It ﬁne-tunes BERT by BERT\n(C) with an additional adversarial domain loss\nproposed in Hu et al. (2019).\nWe also evaluate the performance of MFT and\nits variants under the following three settings:\n• MFT (DC): It is MFT with domain corrup-\ntion. All the typicality scores in the objective\nfunction are removed.\n• MFT (TW): It is MFT with typicality weight-\ning. The domain corruption loss is removed.\n• MFT (Full): It is the full implementation.\nThe results of three multi-domain NLP tasks are\nreported in Table 2, Table 3 and Table 4, respec-\ntively. Generally, the performance trends of all\nthree tasks are pretty consistent. With MFT, the ac-\ncuracy of ﬁne-tuned BERT boosts 2.4% for natural\nlanguage inference, 2.6% for review analysis and\n3% for domain taxonomy construction. The simple\nmulti-task ﬁne-tuning methods do not have large\nimprovement, of which the conclusion is consistent\nwith Sun et al. (2019a). Our method has the highest\nperformance in 10 domains (genres) out of a total of\n12 domains of the three tasks, outperforming previ-\nous ﬁne-tuning approaches. For ablation study, we\n0 1 2 3 4\n80\n85\n90\n95\n100\nEpoch of Meta Fine-tuning\nAccuracy\nBook\nDVD\nElectronics\nKitchen\n(a) Dataset: Amazon\n0 1 2 3 4\n80\n85\n90\n95\n100\nEpoch of Meta Fine-tuning\nAccuracy\nAnimal\nPlant\nVehicle\n(b) Dataset: Taxonomy\nFigure 3: Tuning the learning epochs of MFT.\ncompare MFT (DC), MFT (TW) and MFT (Full).\nThe results show that domain corruption is more\neffective than typicality weighting in MNLI, but\nless effective in Amazon and Taxonomy.\n4.3 Detailed Model Analysis\nIn this section, we present more experiments on\ndetailed analysis of MFT. We ﬁrst study how many\ntraining steps of MFT we should do before ﬁne-\ntuning. As datasets of different tasks vary in size,\nwe tune the epochs of MFT instead. In this set of\nexperiments, we ﬁx parameters as default, vary the\ntraining epochs of MFT and then run ﬁne-tuning\nfor 2 epochs for all domains. The results of two\nNLP tasks are shown in Figure 3. It can be seen that\ntoo many epochs of MFT can hurt the performance\nbecause BERT may learn too much from other do-\nmains before ﬁne-tuning on the target domain. We\nsuggest that a small number of MFT epochs are\nsufﬁcient for most cases. Next, we tune the hyper-\nparameter λfrom 0 to 0.5, with the results shown in\nFigure 4. The inverted-V trends clearly reﬂect the\nbalance between the two types of losses in MFT,\nwith very few exceptions due to the ﬂuctuation of\nthe stochastic learning process.\nWe also vary the number of corrupted domain\nclassiﬁers by changing Ls. Due to space limita-\ntion, we only report averaged accuracy across all\n3101\nMethod Telephone Government Slate Travel Fiction Average\nBERT (S) 82.5 84.9 78.2 83.1 82.0 82.1\nBERT (Mix) 83.1 85.2 79.3 85.1 82.4 83.0\nBERT (MTL) 83.8 86.1 80.2 85.2 83.6 83.8\nBERT (Adv) 81.9 84.2 79.8 82.0 82.2 82.0\nMFT (DC) 84.2 86.3 80.2 85.8 84.0 84.1\nMFT (TW) 83.8 86.5 81.3 83.7 84.4 83.9\nMFT (Full) 84.6 86.3 81.5 85.4 84.6 84.5\nTable 2: Natural language inference results over MNLI (divided into ﬁve genres) in terms of accuracy (%).\nMethod Book DVD Elec. Kit. Avg.\nBERT (S) 90.7 88.2 89.0 85.7 88.4\nBERT (Mix) 91.8 89.4 87.8 88.4 89.3\nBERT (MTL) 92.2 89.0 88.3 88.2 89.0\nBERT (Adv) 89.3 87.4 86.5 86.7 87.5\nMFT (DC) 90.6 89.4 92.5 88.7 90.3\nMFT (TW) 90.4 88.9 94.5 89.1 90.7\nMFT (Full) 91.2 88.8 94.8 89.2 91.0\nTable 3: Review analysis results over Amazon Review\nDataset in terms of accuracy (%).\nMethod Animal Plant Vehicle Avg.\nBERT (S) 93.6 91.8 84.2 89.3\nBERT (Mix) 93.8 88.2 83.6 88.5\nBERT (MTL) 94.2 89.2 82.4 88.6\nBERT (Adv) 92.8 86.3 83.2 87.4\nMFT (DC) 94.3 91.8 86.8 91.0\nMFT (TW) 94.0 92.0 89.2 91.7\nMFT (Full) 94.5 92.3 90.2 92.3\nTable 4: Domain taxonomy construction results over\nTaxonomy Dataset in terms of accuracy (%).\n0.0 0.1 0.2 0.3 0.4 0.5\n80\n85\n90\n95\n100\nλ\nAccuracy\nBook\nDVD\nElectronics\nKitchen\n(a) Dataset: Amazon\n0.0 0.1 0.2 0.3 0.4 0.5\n80\n85\n90\n95\n100\nλ\nAccuracy\nAnimal\nPlant\nVehicle\n(b) Dataset: Taxonomy\nFigure 4: Tuning the hyper-parameter λ.\ndomains, shown in Table 7. In a majority of sce-\nnarios, adding more corrupted domain classiﬁers\nslightly improve the performance, as it poses strong\ndomain-invariance constraints to deeper layers of\ntransformer encoders in BERT.\nFor more intuitive understanding of MFT, we\npresent some cases from Amazon Review Dataset\nwith relatively extreme (low and high) typicality\nscores, shown in Table 5. As seen, review texts with\nlow scores are usually related to certain aspects of\nspeciﬁc products (for example, “crooked spoon\nhandle” and “fragile glass”), whose knowledge is\nnon-transferable on how to do review analysis in\ngeneral. In contrast, reviews with high typicality\nscores may contain expressions on the review polar-\nity that can be frequently found in various domains\n(for example, “huge deal” and “a waste of money”).\nFrom the cases, we can see how MFT can create\na meta-learner that is capable of learning to solve\nNLP tasks in different domains.\n4.4 Experiments for Few-shot Learning\nAcquiring a sufﬁcient amount of training data for\nemerging domains often poses challenges for NLP\nresearchers. In this part, we study how MFT can\nbeneﬁt few-shot learning when the size of the train-\ning data in a speciﬁc domain is small 8. Because\nthe original MNLI dataset (Williams et al., 2018)\nis large in size, we randomly sample 5%, 10% and\n20% of the original training set for each genre, and\ndo MFT and ﬁne-tuning over BERT. For model\nevaluation, we use the entire development set with-\nout sampling. In this set of experiments, we do not\ntune any hyper-parameters and set them as default.\nDue to the small sizes of our few-shot training sets,\nwe run MFT for only one epoch, and then ﬁne-tune\nBERT for 2 epochs per genre.\nIn Table 6, we report the few-shot learning re-\nsults, and compare them against ﬁne-tuned BERT\nwithout MFT. From the experimental results, we\n8Note that this experiment is not conducted using the K-\nway N-shot setting. Refer to Related Work for discussion.\n3102\nTypicality Domain Label Review Text\nBook NEG More hate books. How could anyone write anything so wrong.\nLow Kitchen NEG The spoon handle is crooked and there are marks/damage to the wood. Avoid.\nKitchen NEG The glass is quite fragile. I had two breaks.\nKitchen POS I would recommend them to everyone..and at their price, it’s a HUGE DEAL!\nHigh Electronics NEG What a waste of money. For $300 you shouldn’t HA VE to buy a protection plan for...\nElectronics NEG Do not waste your money, this was under recommendations, but I would NOT...\nTable 5: Cases of review texts from Amazon Review Dataset with high and low typicality scores.\nGenre With MFT? Improvement With MFT? Improvement With MFT? ImprovementNo Yes No Yes No Yes\nTraining data 5% of the original 10% of the original 20% of the original\nTelephone 70.5 74.7 4.2%↑ 74.1 76.4 2.3%↑ 75.9 79.8 3.9%↑\nGovernment 76.5 78.1 1.6%↑ 78.8 81.0 2.2%↑ 80.5 82.9 2.4%↑\nSlate 64.2 69.8 5.7%↑ 67.6 71.8 4.2%↑ 71.8 74.1 2.3%↑\nTravel 71.9 75.4 3.5%↑ 74.8 78.1 3.3%↑ 78.3 80.3 2.0%↑\nFiction 69.7 73.8 4.1%↑ 73.3 76.6 3.3%↑ 76.2 78.4 2.2%↑\nAverage 70.5 74.4 3.9%↑ 73.7 76.8 3.1%↑ 76.5 79.1 2.6%↑\nTable 6: Few-shot natural language inference results over MNLI in terms of accuracy (%).\nLs ↓ Dataset→ Amazon Taxonomy\n{12} 90.7 91.3\n{6, 12} 90.8 92.0\n{4, 8, 12} 91.4 92.5\n{3, 6, 9, 12} 91.2 92.8\nTable 7: Change of prediction accuracy when the se-\nlected layer indices Ls take different values (%).\ncan safely come to the following conclusions. i)\nMFT improves the performance for text mining of\nall genres in MNLI, regardless of the percentages\nof the original training sets we use. ii) MFT has\na larger impact on smaller training sets (a 3.9%\nimprovement in accuracy for 5% few-shot learn-\ning, compared to a 2.6% improvement for 20%).\niii) The improvement of applying MFT before ﬁne-\ntuning is almost the same as doubling the training\ndata size. Therefore, MFT is highly useful for\nfew-shot learning when the training data of other\ndomains are available.\n5 Conclusion and Future Work\nIn this paper, we propose a new training procedure\nnamed Meta Fine-Tuning (MFT) used in neural\nlanguage models for multi-domain text mining. Ex-\nperimental results show the effectiveness of MFT\nfrom various aspects. In the future, we plan to apply\nMFT to other language models (e.g., Transformer-\nXL (Dai et al., 2019) and ALBERT (Lan et al.,\n2019)) and for other NLP tasks.\nAcknowledgements\nWe would like to thank anonymous reviewers for\ntheir valuable comments. This work is partially\nsupported by the National Key Research and De-\nvelopment Program of China under Grant No.\n2016YFB1000904. M. Qiu is partially funded\nby China Postdoctoral Science Foundation (No.\n2019M652038).\nReferences\nYuki Arase and Jun’ichi Tsujii. 2019. Transfer ﬁne-\ntuning: A BERT case study. In EMNLP-IJCNLP,\npages 5392–5403.\nJoachim Bingel and Anders Søgaard. 2017. Identify-\ning beneﬁcial task relations for multi-task learning\nin deep neural networks. In EACL, pages 164–169.\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, bollywood, boom-boxes and blenders:\nDomain adaptation for sentiment classiﬁcation. In\nACL.\nYitao Cai and Xiaojun Wan. 2019. Multi-domain sen-\ntiment classiﬁcation based on domain-aware embed-\nding and attention. In IJCAI, pages 4904–4910.\nPengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, and\nShengping Liu. 2018. Adversarial transfer learn-\ning for chinese named entity recognition with self-\nattention mechanism. In EMNLP, pages 182–192.\nYixin Cao, Lifu Huang, Heng Ji, Xu Chen, and Juanzi\nLi. 2017. Bridge text and knowledge by learning\nmulti-prototype entity mention embedding. In ACL,\npages 1623–1633.\nMingyang Chen, Wen Zhang, Wei Zhang, Qiang Chen,\nand Huajun Chen. 2019. Meta relational learning\nfor few-shot link prediction in knowledge graphs. In\nEMNLP-IJCNLP, pages 4216–4225.\n3103\nBaiyun Cui, Yingming Li, Ming Chen, and Zhongfei\nZhang. 2019. Fine-tune BERT with sparse self-\nattention mechanism. In EMNLP-IJCNLP, pages\n3546–3551.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a ﬁxed-length context. In ACL, pages 2978–\n2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT, pages 4171–4186.\nZi-Yi Dou, Keyi Yu, and Antonios Anastasopoulos.\n2019. Investigating meta-learning algorithms for\nlow-resource natural language understanding tasks.\nIn EMNLP-IJCNLP, pages 1192–1197.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In ICML, pages 1126–1135.\nPavel Gulyaev, Eugenia Elistratova, Vasily Konovalov,\nYuri Kuratov, Leonid Pugachev, and Mikhail Burt-\nsev. 2020. Goal-oriented multi-task bert-based dia-\nlogue state tracker. CoRR, abs/2002.02450.\nDaya Guo, Duyu Tang, Nan Duan, Ming Zhou, and\nJian Yin. 2019. Coupling retrieval and meta-\nlearning for context-dependent semantic parsing. In\nACL, pages 855–866.\nMengting Hu, Yike Wu, Shiwan Zhao, Honglei Guo,\nRenhong Cheng, and Zhong Su. 2019. Domain-\ninvariant feature distillation for cross-domain sen-\ntiment classiﬁcation. In EMNLP-IJCNLP, pages\n5558–5567.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure\nof language? In ACL, pages 3651–3657.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In EMNLP-IJCNLP, pages 4364–4373.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. 2019. ALBERT: A lite BERT for self-\nsupervised learning of language representations.\nCoRR, abs/1909.11942.\nYitong Li, Timothy Baldwin, and Trevor Cohn. 2019a.\nSemi-supervised stochastic multi-domain learning\nusing variational inference. In ACL, pages 1923–\n1934. Association for Computational Linguistics.\nZheng Li, Xin Li, Ying Wei, Lidong Bing, Yu Zhang,\nand Qiang Yang. 2019b. Transferable end-to-end\naspect-based sentiment analysis with selective adver-\nsarial learning. In EMNLP-IJCNLP, pages 4589–\n4599.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In NAACL-HLT, pages 1073–1094.\nPengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.\nAdversarial multi-task learning for text classiﬁca-\ntion. In ACL, pages 1–10.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019b. Multi-task deep neural networks\nfor natural language understanding. In ACL, pages\n4487–4496.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019c.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nJie Lu, Vahid Behbood, Peng Hao, Hua Zuo, Shan Xue,\nand Guangquan Zhang. 2015. Transfer learning us-\ning computational intelligence: A survey. Knowl.\nBased Syst., 80:14–23.\nAnh Tuan Luu, Yi Tay, Siu Cheung Hui, and See-Kiong\nNg. 2016. Learning term embeddings for taxonomic\nrelation identiﬁcation using dynamic weighting neu-\nral network. In EMNLP, pages 403–413.\nAndrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, and\nPascale Fung. 2019. Personalizing dialogue agents\nvia meta-learning. In ACL, pages 5454–5459.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language de-\ncathlon: Multitask learning as question answering.\nCoRR, abs/1806.08730.\nSinno Jialin Pan and Qiang Yang. 2010. A survey on\ntransfer learning. IEEE Trans. Knowl. Data Eng. ,\n22(10):1345–1359.\nZhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jian-\nmin Wang. 2018. Multi-adversarial domain adapta-\ntion. In AAAI, pages 3934–3941.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL-HLT, pages 2227–2237.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nCoRR, abs/2003.08271.\nJian Shen, Yanru Qu, Weinan Zhang, and Yong Yu.\n2018. Wasserstein distance guided representation\nlearning for domain adaptation. In AAAI, pages\n4058–4065.\nRui Shu, Hung H. Bui, Hirokazu Narui, and Stefano\nErmon. 2018. A DIRT-T approach to unsupervised\ndomain adaptation. In ICLR.\n3104\nAsa Cooper Stickland and Iain Murray. 2019. BERT\nand pals: Projected attention layers for efﬁcient\nadaptation in multi-task learning. In ICML, pages\n5986–5995.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019a. How to ﬁne-tune BERT for text classiﬁca-\ntion? In CCL, pages 194–206.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019b.\nPatient knowledge distillation for BERT model com-\npression. In EMNLP-IJCNLP, pages 4322–4331.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nACL, pages 4593–4601.\nJoaquin Vanschoren. 2018. Meta-learning: A survey.\nCoRR, abs/1810.03548.\nPaola Velardi, Stefano Faralli, and Roberto Navigli.\n2013. Ontolearn reloaded: A graph-based algorithm\nfor taxonomy induction. Computational Linguistics,\n39(3):665–707.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019a.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In ICLR.\nChengyu Wang, Minghui Qiu, Jun Huang, and Xi-\naofeng He. 2020. KEML: A knowledge-enriched\nmeta-learning framework for lexical relation classi-\nﬁcation. CoRR, abs/2002.10903.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Li-\nwei Peng, and Luo Si. 2019b. Structbert: Incorpo-\nrating language structures into pre-training for deep\nlanguage understanding. CoRR, abs/1908.04577.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNAACL-HLT, pages 1112–1122.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS, pages 5754–\n5764.\nHuaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin\nDing, Ruirui Li, and Zhenhui Li. 2020. Automated\nrelational meta-learning. CoRR, abs/2001.00745.\nJoey Tianyi Zhou, Hao Zhang, Di Jin, Hongyuan Zhu,\nMeng Fang, Rick Siow Mong Goh, and Kenneth\nKwok. 2019a. Dual adversarial neural transfer for\nlow-resource named entity recognition. In ACL,\npages 3461–3471.\nJunru Zhou, Zhuosheng Zhang, and Hai Zhao.\n2019b. LIMIT-BERT : Linguistic informed multi-\ntask BERT. CoRR, abs/1910.14296.\nFuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi,\nYongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing\nHe. 2019. A comprehensive survey on transfer learn-\ning. CoRR, abs/1911.02685.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.816674530506134
    },
    {
      "name": "Artificial intelligence",
      "score": 0.676824152469635
    },
    {
      "name": "Generalization",
      "score": 0.5994287729263306
    },
    {
      "name": "Language model",
      "score": 0.569361686706543
    },
    {
      "name": "Task (project management)",
      "score": 0.5511599779129028
    },
    {
      "name": "Machine learning",
      "score": 0.5191128253936768
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4650140404701233
    },
    {
      "name": "Artificial neural network",
      "score": 0.4600486755371094
    },
    {
      "name": "Natural language processing",
      "score": 0.45077937841415405
    },
    {
      "name": "Fine-tuning",
      "score": 0.4443283677101135
    },
    {
      "name": "Process (computing)",
      "score": 0.43141013383865356
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I66867065",
      "name": "East China Normal University",
      "country": "CN"
    }
  ]
}