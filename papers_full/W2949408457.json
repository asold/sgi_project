{
  "title": "Improved Chord Recognition by Combining Duration and Harmonic Language Models",
  "url": "https://openalex.org/W2949408457",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5047928537",
      "name": "Filip Korzeniowski",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5003768123",
      "name": "Gerhard Widmer",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2296557957",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W296482991",
    "https://openalex.org/W1982446897",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2395935897",
    "https://openalex.org/W2537159808",
    "https://openalex.org/W2350911425",
    "https://openalex.org/W3122695829",
    "https://openalex.org/W2795825978",
    "https://openalex.org/W1989216934",
    "https://openalex.org/W2789784794",
    "https://openalex.org/W2048522525",
    "https://openalex.org/W2522090280",
    "https://openalex.org/W2963285578",
    "https://openalex.org/W2398597216",
    "https://openalex.org/W2172140247",
    "https://openalex.org/W2552400013",
    "https://openalex.org/W2155955521",
    "https://openalex.org/W2165911145",
    "https://openalex.org/W2951587447",
    "https://openalex.org/W2771265026",
    "https://openalex.org/W2111007352",
    "https://openalex.org/W2577760017",
    "https://openalex.org/W2950903920",
    "https://openalex.org/W2407022421"
  ],
  "abstract": "Chord recognition systems typically comprise an acoustic model that predicts chords for each audio frame, and a temporal model that casts these predictions into labelled chord segments. However, temporal models have been shown to only smooth predictions, without being able to incorporate musical information about chord progressions. Recent research discovered that it might be the low hierarchical level such models have been applied to (directly on audio frames) which prevents learning musical relationships, even for expressive models such as recurrent neural networks (RNNs). However, if applied on the level of chord sequences, RNNs indeed can become powerful chord predictors. In this paper, we disentangle temporal models into a harmonic language model---to be applied on chord sequences---and a chord duration model that connects the chord-level predictions of the language model to the frame-level predictions of the acoustic model. In our experiments, we explore the impact of each model on the chord recognition score, and show that using harmonic language and duration models improves the results.",
  "full_text": "IMPROVED CHORD RECOGNITION BY COMBINING DURATION AND\nHARMONIC LANGUAGE MODELS\nFilip Korzeniowski and Gerhard Widmer\nInstitute of Computational Perception,\nJohannes Kepler University, Linz, Austria\nﬁlip.korzeniowski@jku.at\nABSTRACT\nChord recognition systems typically comprise an acoustic\nmodel that predicts chords for each audio frame, and a tem-\nporal model that casts these predictions into labelled chord\nsegments. However, temporal models have been shown to\nonly smooth predictions, without being able to incorporate\nmusical information about chord progressions. Recent re-\nsearch discovered that it might be the low hierarchical level\nsuch models have been applied to (directly on audio frames)\nwhich prevents learning musical relationships, even for ex-\npressive models such as recurrent neural networks (RNNs).\nHowever, if applied on the level of chord sequences, RNNs\nindeed can become powerful chord predictors. In this paper,\nwe disentangle temporal models into a harmonic language\nmodel—to be applied on chord sequences—and a chord\nduration model that connects the chord-level predictions of\nthe language model to the frame-level predictions of the\nacoustic model. In our experiments, we explore the impact\nof each model on the chord recognition score, and show that\nusing harmonic language and duration models improves the\nresults.\n1. INTRODUCTION\nChord recognition methods recognise and transcribe mu-\nsical chords from audio recordings. Chords are highly de-\nscriptive harmonic features that form the basis of many\nkinds of applications: theoretical, such as computational\nharmonic analysis of music; practical, such as automatic\nlead-sheet creation for musicians 1 or music tutoring sys-\ntems 2 ; and ﬁnally, as basis for higher-level tasks such as\ncover song identiﬁcation or key classiﬁcation. Chord recog-\nnition systems face the two key problems of extracting\nmeaningful information from noisy audio, and casting this\ninformation into sensible output. These translate toacoustic\nmodelling (how to predict a chord label for each position or\nframe in the audio), and temporal modelling(how to create\n1 https://chordify.net/\n2 https://yousician.com\n© Filip Korzeniowski and Gerhard Widmer. Licensed under\na Creative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Filip Korzeniowski and Gerhard Widmer. “Improved Chord\nRecognition by Combining Duration and Harmonic Language Models”,\n19th International Society for Music Information Retrieval Conference,\nParis, France, 2018.\nmeaningful segments of chords from these possibly volatile\nframe-wise predictions).\nAcoustic models extract frame-wise chord predictions,\ntypically in the form of a distribution over chord la-\nbels. Originally, these models were hand-crafted and split\ninto feature extraction and pattern matching, where the\nformer computed some form of pitch-class proﬁles (e.g.\n[26, 29, 33]), and the latter used template matching or Gaus-\nsian mixtures [6, 14] to model these features. Recently,\nhowever, neural networks became predominant for acoustic\nmodelling [18, 22, 23, 27]. These models usually compute a\ndistribution over chord labels directly from spectral repre-\nsentations and thus fuse both feature extraction and pattern\nmatching. Due to the discriminative power of deep neural\nnetworks, these models achieve superior results.\nTemporal models process the predictions of an acous-\ntic model and cast them into coherent chord segments.\nSuch models are either task-speciﬁc, such as hand-designed\nBayesian networks [26], or general models learned from\ndata. Here, it is common to use hidden Markov mod-\nels [8] (HMMs), conditional random ﬁelds [23] (CRFs),\nor recurrent neural networks (RNNs) [2, 32]. However,\nexisting models have shown only limited capabilities to\nimprove chord recognition results. First-order models are\nnot capable of learning meaningful musical relations, and\nonly smooth the predictions [4, 7]. More powerful mod-\nels, such as RNNs, do not perform better than their ﬁrst-\norder counterparts [24]. In addition to the fundamental ﬂaw\nof ﬁrst-order models (chord patterns comprise more than\ntwo chords) both approaches are limited by the low hier-\narchical level they are applied on: the temporal model is\nrequired to predict the next symbol for each audio frame.\nThis makes the model focus on short-term smoothing, and\nneglect longer-term musical relations between chords, be-\ncause, most of the time, the chord in the next audio frame\nis the same as in the current one. However, exploiting these\nlonger-term relations is crucial to improve the prediction\nof chords. RNNs, if applied on chord sequences, are capa-\nble of learning these relations, and become powerful chord\npredictors [21].\nOur contributions in this paper are as follows: i) we de-\nscribe a probabilistic model that allows for the integration\nof chord-level language models with frame-level acoustic\nmodels, by connecting the two using chord duration models;\nii) we develop and apply chord language models and chord\nduration models based on RNNs within this framework;\narXiv:1808.05335v1  [cs.SD]  16 Aug 2018\nand iii) we explore how these models affect chord recogni-\ntion results, and show that the proposed integrated model\nout-performs existing temporal models.\n2. CHORD SEQUENCE MODELLING\nChord recognition is a sequence labelling task, i.e. we need\nto assign a categorical label yt ∈Y (a chord from a chord\nalphabet) to each member of the observed sequence xt (an\naudio frame), such that yt is the harmonic interpretation of\nthe music represented by xt. Formally,\nˆy1:T = argmax\ny1:T\nP(y1:T |x1:T) . (1)\nAssuming a generative structure as shown in Fig. 1, the\nprobability distribution factorises as\nP(y1:T |x1:T) ∝\n∏\nt\n1\nP(yt)PA(yt |xt) PT (yt |y1:t−1) ,\nwhere PA is the acoustic model, PT the temporal model,\nand P(yt) the label prior which we assume to be uniform\nas in [31].\ny1 y2 y3 ··· yT\nx1 x2 x3 xT\nFigure 1. Generative chord sequence model. Each chord\nlabel yt depends on all previous labels y1:t−1.\nThe temporal model PT predicts the chord symbol of\neach audio frame. As discussed earlier, this prevents both\nﬁnite-context models (such as HMMs or CRFs) and unre-\nstricted models (such as RNNs) to learn meaningful har-\nmonic relations. To enable this, we disentangle PT into a\nharmonic language modelPL and a duration modelPD,\nwhere the former models the harmonic progression of a\npiece, and the latter models the duration of chords.\nThe language model PL is deﬁned as PL(¯yk |¯y1:k−1),\nwhere ¯y1:k = C(y1:t), and C(·) is a sequence compression\nmapping that removes all consecutive duplicates of a chord\n(e.g. C((C,C,F,F,G )) = ( C,F,G )). The frame-wise\nlabels y1:t are thus reduced to chord changes, and PL can\nfocus on modelling these.\nThe duration model PD is deﬁned as PD(st |y1:t−1),\nwhere st ∈ {c,s}indicates whether the chord changes\n(c) or stays the same (s) at time t. PD thus only predicts\nwhether the chord will change or not, but not which chord\nwill follow—this is left to the language model PL. This\ndeﬁnition allows PD to consider the preceding chord labels\ny1:t−1; in practice, we restrict the model to only depend on\nPD(s |y1:t)\nPL\n( ¯y|¯y1:k−1\n) ·\nPD\n(c|y1:t)\nt −1 t t + 1\n¯yk−1\n¯yk\n¯yk+1\nAudio Frames\nChord Sequence\nFigure 2. Chord-time lattice representing the temporal\nmodel PT, split into a language model PL and duration\nmodel PD. Here, ¯y1:K represents a concrete chord se-\nquence. For each audio frame, we move along the time-axis\nto the right. If the chord changes, we move diagonally to\nthe upper right. This corresponds to the ﬁrst case in Eq. 2.\nIf the chord stays the same, we move only to the right. This\ncorresponds to the second case of the equation.\nthe preceding chord changes, i.e. PD(st |s1:t−1). Explor-\ning more complex models of harmonic rhythm is left for\nfuture work.\nUsing these deﬁnitions, the temporal model PT fac-\ntorises as\nPT (yt |y1:t−1) = (2)\n{\nPL(¯yk |¯y1:k−1) PD(c |y1:t−1) if yt ̸= yt−1\nPD(s |y1:t−1) else .\nThe chord progression can then be interpreted as a path\nthrough a chord-time lattice as shown in Fig. 2.\nThis model cannot be decoded efﬁciently at test-time be-\ncause each yt depends on all predecessors. We will thus use\neither models that restrict these connections to a ﬁnite past\n(such as higher-order Markov models) or use approximate\ninference methods for other models (such as RNNs).\n3. MODELS\nThe general model described above requires three sub-\nmodels: an acoustic model PA that predicts a chord distri-\nbution from each audio frame, a duration model PD that\npredicts when chords change, and a language model PL\nthat predicts the progression of chords in the piece.\n3.1 Acoustic Model\nThe acoustic model we use is a VGG-style convolutional\nneural network, similar to the one presented in [23]. It uses\nthree convolutional blocks: the ﬁrst consists of 4 layers of\n32 3×3 ﬁlters (with zero-padding in each layer), followed\nby 2 ×1 max-pooling in frequency; the second comprises\n2 layers of 64 such ﬁlters followed by the same pooling\nscheme; the third is a single layer of 128 12×9 ﬁlters. Each\nof the blocks is followed by feature-map-wise dropout with\nh0 h1 h2 ··· hK\nv(z0) v(z1) v(zK−1)\nP(z1 |h1) P(z2 |h2) P(zK |hK)\nFigure 3. Sketch of a RNN used for next step prediction,\nwhere zk refers to an arbitrary categorical input, v(·) is a\n(learnable) input embedding vector, and hk the hidden state\nat step k. Arrows denote matrix multiplications followed\nby a non-linear activation function. The input is padded\nwith a dummy input z0 in the beginning. The network then\ncomputes the probability distribution for the next symbol.\nprobability 0.2, and each layer is followed by batch normal-\nisation [19] and an ELU activation function [10]. Finally, a\nlinear convolution with 25 1×1 ﬁlters followed by global\naverage pooling and a softmax produces the chord class\nprobabilities PA(yt |xt).\nThe input to the network is a 1.5 s patch of a quarter-\ntone spectrogram computed using a logarithmically spaced\ntriangular ﬁlter bank. Concretely, we process the audio at\na sample rate of 44 100 Hz using the STFT with a frame\nsize of 8192 and a hop size of 4410. Then, we apply to\nthe magnitude of the STFT a triangular ﬁlter bank with 24\nﬁlters per octave between 65 Hz and 2 100 Hz. Finally, we\ntake the logarithm of the resulting magnitudes to compress\nthe input range.\nNeural networks tend to produce over-conﬁdent pre-\ndictions, which in further consequence could over-rule\nthe predictions of a temporal model [9]. To mitigate\nthis, we use two techniques: ﬁrst, we train the model\nusing uniform smoothing (i.e. we assign a proportion\nof 1 −β to other classes during training); second, dur-\ning inference, we apply the temperature softmaxfunction\nστ (z)j = e\nzj/τ\n/∑K\nk=1 e\nzk/τ instead of the standard softmax\nin the ﬁnal layer. Higher values of τ produce smoother\nprobability distributions. In this paper, we use β = 0.9 and\nτ = 1.3, as determined in preliminary experiments.\n3.2 Language Model\nThe language model PL predicts the next chord, regardless\nof its duration, given the chord sequence it has previously\nseen. As shown in [21], RNN-based models perform bet-\nter than n-gram models at this task. We thus adopt this\napproach, and refer the reader to [21] for details.\nTo give an overview, we follow the set-up introduced\nby [28] and use a recurrent neural network for next-chord\nprediction. The network’s task is to compute a probability\ndistribution over all possible next chord symbols, given the\nchord symbols it has observed before. Figure 3 shows an\nRNN in a general next-step prediction task. In our case, the\ninputs zk are the chord symbols given by C(y1:T).\nWe will describe in detail the network’s hyper-\nparameters in Section 4, where we will also evaluate the\neffect the language models have on chord recognition.\n3.3 Duration Model\nThe duration model PD predicts whether the chord will\nchange in the next time step. This corresponds to modelling\nthe duration of chords. Existing temporal models induce\nimplicit duration models: for example, an HMM implies an\nexponential chord duration distribution (if one state is used\nto model a chord), or a negative binomial distribution (if\nmultiple left-to-right states are used per chord). However,\nsuch duration models are simplistic, static, and do not adapt\nto the processed piece.\nAn explicit duration model has been explored in [4],\nwhere beat-synchronised chord durations were stored\nas discrete distributions. Their approach is useful for\nbeat-synchronised models, but impractical for frame-wise\nmodels—the probability tables would become too large,\nand data too sparse to estimate them. Since our approach\navoids the potentially error-prone beat synchronisation, the\napproach of [4] does not work in our case.\nInstead, we opt to use recurrent neural networks to model\nchord durations. These models are able to adapt to charac-\nteristics of the processed data [21], and have shown great\npotential in processing periodic signals [1] (and chords\ndo change periodically within a piece). To train an RNN-\nbased duration model, we set up a next-step-prediction\ntask, identical in principle to the set-up for harmonic lan-\nguage modelling: the network has to compute the proba-\nbility of a chord change in the next time step, given the\nchord changes it has seen in the past. We thus simplify\nPD(st |y1:t−1)ˆ=PD(st |s1:t−1), as mentioned earlier.\nAgain, see Fig. 3 for an overview (for duration modelling,\nreplace zk with st).\nIn Section 4, we will describe in detail the hyper-\nparameters of the networks we employed, and compare the\nproperties of various settings to baseline duration models.\nWe will also assess the impact on the duration modelling\nquality on the ﬁnal chord recognition result.\n3.4 Model Integration\nDynamic models such as RNNs have one main advantage\nover their static counter-parts (e.g. n-gram models for\nlanguage modelling or HMMs for duration modelling): they\nconsider all previous observations when predicting the next\none. As a consequence, they are able to adapt to the piece\nthat is currently processed—they assign higher probabilities\nto sub-sequences of chords they have seen earlier [21], or\npredict chord changes according to the harmonic rhythm of\na song (see Sec. 4.3). The ﬂip side of the coin is, however,\nthat this property prohibits the use of dynamic programming\napproaches for efﬁcient decoding. We cannot exactly and\nefﬁciently decode the best chord sequence given the input\naudio.\nHence we have to resort to approximate inference. In par-\nticular, we employ hashed beam search[32] to decode the\nchord sequence. General beam search restricts the search\nspace by keeping only the Nb best solutions up to the cur-\nrent time step. (In our case, the Nb best paths through\nall possible chord-time lattices, see Fig. 2.) However, as\npointed out in [32], the beam might saturate with almost\nidentical solutions, e.g. the same chord sequence differing\nonly marginally in the times the chords change. Such patho-\nlogical cases may impair the ﬁnal estimate. To mitigate\nthis problem, hashed beam search forces the tracked solu-\ntions to be diverse by pruning similar solutions with lower\nprobability.\nThe similarity of solutions is determined by a task-\nspeciﬁc hash function. For our purpose, we deﬁne the\nhash function of a solution to be the last Nh chord sym-\nbols in the sequence, regardless of their duration; formally,\nthe hash function fh(y1:t) = ¯ y(k−Nh):k. (Recall that\n¯y1:k = C(y1:t).) In contrast to the hash function originally\nproposed in [32], which directly uses y(t−Nh):t, our formu-\nlation ensures that sequences that differ only in timing, but\nnot in chord sequence, are considered similar.\nTo summarise, we approximately decode the optimal\nchord transcription as deﬁned in Eq. 1 using hashed beam\nsearch, which at each time step keeps the best Nb solutions,\nand at most Ns similar solutions.\n4. EXPERIMENTS\nIn our experiments, we will ﬁrst evaluate harmonic language\nand duration models individually. Here, we will compare\nthe proposed models to common baselines. Then, we will\nintegrate these models into the chord recognition framework\nwe outlined in Section 2, and evaluate how the individual\nparts interact in terms of chord recognition score.\n4.1 Data\nWe use the following datasets in 4-fold cross-validation.Iso-\nphonics 3 : 180 songs by the Beatles, 19 songs by Queen,\nand 18 songs by Zweieck, 10:21 hours of audio;RWC Pop-\nular [15]:100 songs in the style of American and Japanese\npop music, 6:46 hours of audio; Robbie Williams [13]:65\nsongs by Robbie Williams, 4:30 of audio; and McGill Bill-\nboard [3]:742 songs sampled from the American billboard\ncharts between 1958 and 1991, 44:42 hours of audio. The\ncompound dataset thus comprises 1125 unique songs, and\na total of 66:21 hours of audio.\nFurthermore, we used the following data sets (with dupli-\ncate songs removed) as additional data for training the lan-\nguage and duration models: 173 songs from the Rock [11]\ncorpus; a subset of 160 songs from UsPop2002 4 for which\nchord annotations are available 5 ; 291 songs from Weimar\nJazz 6 , with chord annotations taken from lead sheets of\nJazz standards; and Jay Chou[12], a small collection of 29\nChinese pop songs.\nWe focus on the major/minor chord vocabulary, and\nfollowing [7], map all chords containing a minor third to\nminor, and all others to major. This leaves us with 25\nclasses: 12 root notes ×{major,minor}and the ‘no- chord’\nclass.\n3 http://isophonics.net/datasets\n4 https://labrosa.ee.columbia.edu/projects/musicsim/uspop2002.html\n5 https://github.com/tmc323/Chord-Annotations\n6 http://jazzomat.hfm-weimar.de/dbformat/dboverview.html\nGRU-512 GRU-32 4-gram 2-gram\nlog-P −1.293 −1.576 −1.887 −2.393\nTable 1. Language model results: average log-probability\nof the correct next chord computed by each model.\n4.2 Language Models\nThe performance of neural networks depends on a good\nchoice of hyper-parameters, such as number of layers, num-\nber of units per layer, or unit type (e.g. vanilla RNN, gated\nrecurrent unit (GRU) [5] or long short-term memory unit\n(LSTM) [17]). The ﬁndings in [21] provide a good start-\ning point for choosing hyper-parameter settings that work\nwell. However, we strive to ﬁnd a simpler model to re-\nduce the computational burden at test time. To this end,\nwe perform a grid search in a restricted search space, us-\ning the validation score of the ﬁrst fold. We search over\nthe following settings: number of layers ∈{1,2,3}, num-\nber of units ∈ {256,512}, unit type ∈ {GRU,LSTM},\ninput embedding ∈{one-hot,R8,R16,R24}, learning rate\n∈{0.001,0.005}, and skip connections ∈{on,off}. Other\nhyper-parameters were ﬁxed for all trials: we train the net-\nworks for 100 epochs using stochastic gradient descent with\nmini-batches of size 4, employ the Adam update rule [ ?],\nand starting from epoch 50, linearly anneal the learning rate\nto 0.\nTo increase the diversity in the training data, we use two\ndata augmentation techniques, applied each time we show a\npiece to the network. First, we randomly shift the key of the\npiece; the network can thus learn that harmonic relations\nare independent of the key, as in roman numeral analysis.\nSecond, we select a sub-sequence of random length instead\nof the complete chord sequence; the network thus has to\nlearn to cope with varying context sizes.\nThe best model turned out to be a single-layer network\nof 512 GRUs, with a learnable 16-dimensional input embed-\nding and without skip connections, trained using a learning\nrate of 0.005 7 . We compare this model and a smaller, but\notherwise identical RNN with 32 units, to two baselines:\na 2-gram model, and a 4-gram model. Both can be used\nfor chord recognition in a higher-order HMM [25]. We\ntrain the n-gram models using maximum likelihood estima-\ntion with Lidstone smoothing as described in [21], using\nthe key-shift data augmentation technique (sub-sequence\ncropping is futile for ﬁnite context models). As evaluation\nmeasure, we use the average log-probability of predicting\nthe correct next chord. Table 1 presents the test results. The\nGRU models predict chord sequences with much higher\nprobability than the baselines.\nWhen we look into the input embeddingv(·), which was\nlearned by the RNN during training from a random initiali-\nsation, we observe an interesting positioning of the chord\nsymbols (see Figure 4). We found that similar patterns de-\nvelop for all 1-layer GRUs we tried, and these patterns are\nconsistent for all folds we trained on. We observe i) that\n7 Due to space constraints, we cannot present the complete grid search\nresults.\n. C\nD♭\nD\nE♭\nE\nF\nF♯\nG\nA♭\nA\nB♭\nB\nC\nD♭\nD\nE♭\nE\nF\nF♯\nG\nA♭\nA\nB♭\nB\nFigure 4. Chord embedding projected into 2D using PCA\n(left); Tonnetz of triads (right). The “no-chord” class resides\nin the center of the embedding. Major chords are upper-case\nand orange, minor chords lower-case and blue. Clusters in\nthe projected embedding and the corresponding positions in\nthe Tonnetz are marked in color. If projected into 3D (not\nshown here), the chord clusters split into a lower and upper\nhalf of four chords each. The chords in the lower halves are\nshaded in the Tonnetz representation.\nchords form three clusters around the center, in which the\nminor chords are farther from the center than major chords;\nii) that the clusters group major and minor chords with the\nsame root, and the distance between the roots are minor\nthirds (e.g. C, E ♭, F♯, A); iii) that clockwise movement\nin the circle of ﬁfths corresponds to clockwise movement\nin the projected embedding; and iv) that the way chords\nare grouped in the embedding corresponds to how they are\nconnected in the Tonnetz.\nAt this time, we cannot provide an explanation for these\nautomatically emerging patterns. However, they warrant a\nfurther investigation to uncover why this speciﬁc arrange-\nment seems to beneﬁt the predictions of the model.\n4.3 Duration Models\nAs for the language model, we performed a grid search\non the ﬁrst fold to ﬁnd good choices for the recurrent unit\ntype ∈{vanilla RNN,GRU,LSTM}, and number of recur-\nrent units ∈{16,32,64,128,256}for the LSTM and GRU,\nand {128,256,512}for the vanilla RNN. We use only one\nrecurrent layer for simplicity. We found networks of 256\nGRU units to perform best; although this indicates that even\nbigger models might give better results, for the purposes of\nthis study, we think that this conﬁguration is a good balance\nbetween prediction quality and model complexity.\nThe models were trained for 100 epochs using the Adam\nupdate rule [?] with a learning rate linearly decreasing from\n0.001 to 0. The data was processed in mini-batches of 10,\nwhere the sequences were cut in excerpts of 200 time steps\n(20 s). We also applied gradient clipping at a value of 0.001\nto ensure a smooth learning progress.\nWe compare the best RNN-based duration model with\ntwo baselines. The baselines are selected because both are\nimplicit consequences of using HMMs as temporal model,\nas it is common in chord recognition. We assume a single\nparametrisation for each chord; this ostensible simpliﬁca-\ntion is justiﬁed, because simple temporal models such as\nHMMs do not proﬁt from chord information, as shown\n55 60 65 70 75 80\n4iME[S]\n0.0\n0.1\n0.2\n0.3\nPD(st | s1:t−1)\n.EGATivEBiNoMiAL G25-16 G25-128\nFigure 5. Probability of chord change computed by differ-\nent models. Gray vertical dashed lines indicate true chord\nchanges.\nGRU-256 GRU-16 Neg. Binom. Exp.\nlog-P −2.014 −2.868 −3.946 −4.003\nTable 2. Duration model results: average log-probability of\nchord durations computed by each model.\nby [4, 7]. The ﬁrst baseline we consider is a negative bi-\nnomial distribution. It can be modelled by a HMM us-\ning nstates per chord, connected in a left-to-right manner,\nwith transitions of probability pbetween the states (self-\ntransitions thus have probability 1 −p). The second, a\nspecial case of the ﬁrst with n= 1, is an exponential distri-\nbution; this is the implicit duration distribution used by all\nchord recognition models that employ a simple 1-state-per-\nchord HMM as temporal model. Both baselines are trained\nusing maximum likelihood estimation.\nTo measure the quality of a duration model, we consider\nthe average log-probability it assigns to a chord duration.\nThe results are shown in Table 2. We further added results\nfor the simplest GRU model we tried—using only 16 recur-\nrent units—to indicate the performance of small models of\nthis type. We will also use this simple model when judg-\ning the effect of duration modelling on the ﬁnal result in\nSec. 4.4. As seen in the table, both GRU models clearly\nout-perform the baselines.\nFigure 5 shows the reason why the GRU performs so\nmuch better than the baselines: as a dynamic model, it\ncan adapt to the harmonic rhythm of a piece, while static\nmodels are not capable of doing so. We see that a GRU with\n128 units predicts chord changes with high probability at\nperiods of the harmonic rhythm. It also reliably remembers\nthe period over large gaps in which the chord did not change\n(between seconds 61 and 76). During this time, the peaks\ndecay differently for different multiples of the period, which\nindicates that the network simultaneously tracks multiple\nperiods of varying importance. In contrast, the negative\nbinomial distribution statically yields a higher chord change\nprobability that rises with the number of audio frames since\nthe last chord change. Finally, the smaller GRU model with\nonly 16 units also manages to adapt to the harmonic rhythm;\nhowever, its predictions between the peaks are noisier, and\nit fails to remember the period correctly in the time without\nchord changes.\nModel Root Maj/Min Seg.\n2-gram / neg. binom. 0.812 0.795 0.804\nGRU-512 / GRU-256 0.821 0.805 0.814\nTable 3. Results of the standard model (2-gram language\nmodel with negative binomial durations) compared to the\nbest one (GRU language and duration models).\n4.4 Integrated Models\nThe individual results for the language and duration models\nare encouraging, but only meaningful if they translate to\nbetter chord recognition scores. This section will thus eval-\nuate if and how the duration and language models affect the\nperformance of a chord recognition system.\nThe acoustic model used in these experiments was\ntrained for 300 epochs (with 200 parameter updates per\nepoch) using a mini-batch size of 512 and the Adam up-\ndate rule with standard parameters. We linearly decay the\nlearning rate to 0 in the last 100 epochs.\nWe compare all combinations of language and duration\nmodels presented in the previous sections. For language\nmodelling, these are the GRU-512, GRU-32, 4-gram, and\n2-gram models; for duration modelling, these are the GRU-\n256, GRU-16, and negative binomial models. (We leave out\nthe exponential model, because its results differ negligibly\nfrom the negative binomial one). The models are decoded\nusing the Hashed Beam Search algorithm, as described in\nSec. 3.4: we use a beam width of Nb = 25 , where we\ntrack at most Ns = 4 similar solutions as deﬁned by the\nhash function fh, where the number of chords considered\nis set to Nh = 5. These values were determined by a small\nnumber of preliminary experiments.\nAdditionally, we evaluate exact decoding results for the\nn-gram language models in combination with the negative\nbinomial duration distribution. This will indicate how much\nthe results suffer due to the approximate beam search.\nAs main evaluation metric, we use the weighted chord\nsymbol recall (WCSR) over the major/minor chord alpha-\nbet, as deﬁned in [30]. We thus compute WCSR = tc/ta,\nwhere tc is the total duration of chord segments that have\nbeen recognised correctly, and ta is the total duration of\nchord segments annotated with chords from the target al-\nphabet. We also report chord root accuracy and a measure\nof segmentation (see [16], Sec. 8.3). Table 3 compares the\nresults of the standard model (the combination that implic-\nitly emerges in simple HMM-based temporal models) to the\nbest model found in this study. Although the improvements\nare modest, they are consistent, as shown by a paired t-test\n(p< 2.487 ×10−23 for all differences).\nFigure 6 presents the effects of duration and language\nmodels on the WCSR. Better language and duration models\ndirectly improve chord recognition results, as the WCSR\nincreases linearly with higher log-probability of each model.\nAs this relationship does not seem to ﬂatten out, further\nimprovement of each model type can still increase the score.\nWe also observe that the approximate beam search does\nnot impair the result by much compared to exact decoding\n(compare the dotted blue line with the solid one).\n2.393 1.887 1.576 1.293\n,ANGUAGE-oDEL,oG-0\n0.796\n0.798\n0.800\n0.802\n0.804\n7C32(MAj/MiN)\nDURATioN-oDEL\n.EG.BiNoMiAL,ExACT\n.EG.BiNoMiAL\nG25-16\nG25-256\n2-GRAM 4-GRAM G25-32 G25-512\n,ANGUAGE-oDEL\n3.946 2.979 2.014\nDURATioN-oDEL,oG-0\n0.796\n0.798\n0.800\n0.802\n0.804\n7C32(MAj/MiN)\n,ANGUAGE-oDEL\n2-GRAM\n4-GRAM\nG25-32\nG25-512\n2-GRAM,ExACT\n4-GRAM,ExACT\n.EG.BiNoMiAL G25-16 G25-256\nDURATioN-oDEL\nFigure 6. Effect of language and duration models on the\nﬁnal result. Both plots show the same results from different\nperspectives.\n5. CONCLUSION AND DISCUSSION\nWe described a probabilistic model that disentangles three\ncomponents of a chord recognition system: the acoustic\nmodel, the duration model, and the language model. We\nthen developed better duration and language models than\nhave been used for chord recognition, and illustrated why\nthe RNN-based duration models perform better and are\nmore meaningful than their static counterparts implicitly\nemployed in HMMs. (For a similar investigation for chord\nlanguage models, see [21].) Finally, we showed that im-\nprovements in each of these models directly inﬂuence chord\nrecognition results.\nWe hope that our contribution facilitates further research\nin harmonic language and duration models for chord recog-\nnition. These aspects have been neglected because they did\nnot show great potential for improving the ﬁnal result [4, 7].\nHowever, we believe (see [24] for some evidence) that this\nwas due to the improper assumption that temporal models\napplied on the time-frame level can appropriately model\nmusical knowledge. The results in this paper indicate that\nchord transitions modelled on the chord level, and con-\nnected to audio frames via strong duration models, indeed\nhave the capability to improve chord recognition results.\n6. ACKNOWLEDGEMENTS\nThis work is supported by the European Research Coun-\ncil (ERC) under the EU’s Horizon 2020 Framework Pro-\ngramme (ERC Grant Agreement number 670035, project\n“Con Espressione”).\n7. REFERENCES\n[1] Sebastian B ¨ock and Markus Schedl. Enhanced Beat\nTracking With Context-Aware Neural Networks. InPro-\nceedings of the 14th International Conference on Digi-\ntal Audio Effects (DAFx-11), Paris, France, September\n2011.\n[2] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and\nPascal Vincent. Audio chord recognition with recurrent\nneural networks. In Proceedings of the 14th Interna-\ntional Society for Music Information Retrieval Confer-\nence (ISMIR), Curitiba, Brazil, 2013.\n[3] John Ashley Burgoyne, Jonathan Wild, and Ichiro Fu-\njinaga. An Expert Ground Truth Set for Audio Chord\nRecognition and Music Analysis. In Proceedings of\nthe 12th International Society for Music Information\nRetrieval Conference (ISMIR), Miami, USA, October\n2011.\n[4] Ruofeng Chen, Weibin Shen, Ajay Srinivasamurthy,\nand Parag Chordia. Chord Recognition Using Duration-\nExplicit Hidden Markov Models. In Proceedings of\nthe 13th International Society for Music Information\nRetrieval Conference (ISMIR), Porto, Portugal, 2012.\n[5] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. On the Properties of Neural\nMachine Translation: Encoder-Decoder Approaches.\narXiv:1409.1259 [cs, stat], September 2014.\n[6] Taemin Cho. Improved Techniques for Automatic Chord\nRecognition from Music Audio Signals. Dissertation,\nNew York University, New York, 2014.\n[7] Taemin Cho and Juan P. Bello. On the Relative Impor-\ntance of Individual Components of Chord Recognition\nSystems. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 22(2):477–492, February\n2014.\n[8] Taemin Cho, Ron J Weiss, and Juan Pablo Bello. Ex-\nploring common variations in state of the art chord\nrecognition systems. In Proceedings of the Sound and\nMusic Computing Conference (SMC), pages 1–8, 2010.\n[9] Jan Chorowski and Navdeep Jaitly. Towards better de-\ncoding and language model integration in sequence to\nsequence models. arXiv:1612.02695 [cs, stat], Decem-\nber 2016.\n[10] Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp\nHochreiter. Fast and Accurate Deep Network Learn-\ning by Exponential Linear Units (ELUs). In Interna-\ntional Conference on Learning Representations (ICLR),\narXiv:1511.07289, San Juan, Puerto Rico, February\n2016.\n[11] Trevor de Clercq and David Temperley. A corpus anal-\nysis of rock harmony. Popular Music, 30(01):47–70,\nJanuary 2011.\n[12] Junqi Deng and Yu-Kwong Kwok. Automatic Chord es-\ntimation on seventhsbass Chord vocabulary using deep\nneural network. In International Conference on Acous-\ntics Speech and Signal Processing (ICASSP), Shanghai,\nChina, March 2016.\n[13] Bruno Di Giorgi, Massimiliano Zanoni, Augusto Sarti,\nand Stefano Tubaro. Automatic chord recognition based\non the probabilistic modeling of diatonic modal har-\nmony. In Proceedings of the 8th International Work-\nshop on Multidimensional Systems, Erlangen, Germany,\n2013.\n[14] Takuya Fujishima. Realtime Chord Recognition of Mu-\nsical Sound: A System Using Common Lisp Music.\nIn Proceedings of the International Computer Music\nConference (ICMC), Beijing, China, 1999.\n[15] Masataka Goto, Hiroki Hashiguchi, Takuichi\nNishimura, and Ryuichi Oka. RWC Music Database:\nPopular, Classical and Jazz Music Databases. In\nProceedings of the 3rd International Conference on\nMusic Information Retrieval (ISMIR), Paris, France,\n2002.\n[16] Christopher Harte. Towards Automatic Extraction of\nHarmony Information from Music Signals. Dissertation,\nDepartment of Electronic Engineering, Queen Mary,\nUniversity of London, London, United Kingdom, 2010.\n[17] Sepp Hochreiter and J¨urgen Schmidhuber. Long Short-\nTerm Memory. Neural Computation, 9(8):1735–1780,\nNovember 1997.\n[18] Eric J. Humphrey and Juan P. Bello. Rethinking Au-\ntomatic Chord Recognition with Convolutional Neural\nNetworks. In 11th International Conference on Machine\nLearning and Applications (ICMLA), Boca Raton, USA,\nDecember 2012. IEEE.\n[19] Sergey Ioffe and Christian Szegedy. Batch Normaliza-\ntion: Accelerating deep network training by reducing in-\nternal covariate shift. arXiv preprint arXiv:1502.03167,\n2015.\n[20] Diederik Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[21] Filip Korzeniowski, David R. W. Sears, and Gerhard\nWidmer. A Large-Scale Study of Language Models for\nChord Prediction. In IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP),\nCalgary, Canada, April 2018.\n[22] Filip Korzeniowski and Gerhard Widmer. Feature Learn-\ning for Chord Recognition: The Deep Chroma Extrac-\ntor. In Proceedings of the 17th International Society for\nMusic Information Retrieval Conference (ISMIR), New\nYork, USA, August 2016.\n[23] Filip Korzeniowski and Gerhard Widmer. A Fully Con-\nvolutional Deep Auditory Model for Musical Chord\nRecognition. In Proceedings of the IEEE International\nWorkshop on Machine Learning for Signal Processing\n(MLSP), Salerno, Italy, September 2016.\n[24] Filip Korzeniowski and Gerhard Widmer. On the Futil-\nity of Learning Complex Frame-Level Language Mod-\nels for Chord Recognition. In Proceedings of the AES\nInternational Conference on Semantic Audio, Erlangen,\nGermany, June 2017.\n[25] Filip Korzeniowski and Gerhard Widmer. Automatic\nChord Recognition with Higher-Order Harmonic Lan-\nguage Modelling. In Proceedings of the 26th European\nSignal Processing Conference (EUSIPCO), Rome, Italy,\nSeptember 2018.\n[26] M. Mauch and S. Dixon. Simultaneous Estimation of\nChords and Musical Context From Audio. IEEE Trans-\nactions on Audio, Speech, and Language Processing,\n18(6):1280–1289, August 2010.\n[27] Brian McFee and Juan Pablo Bello. Structured Training\nfor Large-V ocabulary Chord Recognition. InProceed-\nings of the 18th International Society for Music Infor-\nmation Retrieval Conference (ISMIR), Suzhou, China,\nOctober 2017.\n[28] Tomas Mikolov, Martin Karaﬁ´at, Luk´as Burget, Jan Cer-\nnock´y, and Sanjeev Khudanpur. Recurrent neural net-\nwork based language model. In INTERSPEECH 2010,\n11th Annual Conference of the International Speech\nCommunication Association, Makuhari, Chiba, Japan,\nSeptember 26-30, 2010, pages 1045–1048, Chiba,\nJapan, 2010.\n[29] Meinard M ¨uller, Sebastian Ewert, and Sebastian\nKreuzer. Making chroma features more robust to tim-\nbre changes. In International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2009.\n[30] Johan Pauwels and Geoffroy Peeters. Evaluating au-\ntomatically estimated chord sequences. In 2013 IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing, pages 749–753. IEEE, 2013.\n[31] S. Renals, N. Morgan, H. Bourlard, M. Cohen, and\nH. Franco. Connectionist Probability Estimators in\nHMM Speech Recognition. IEEE Transactions on\nSpeech and Audio Processing, 2(1):161–174, January\n1994.\n[32] Siddharth Sigtia, Nicolas Boulanger-Lewandowski, and\nSimon Dixon. Audio chord recognition with a hybrid\nrecurrent neural network. In 16th International Society\nfor Music Information Retrieval Conference (ISMIR),\nM´alaga, Spain, October 2015.\n[33] Yushi Ueda, Yuki Uchiyama, Takuya Nishimoto, Nobu-\ntaka Ono, and Shigeki Sagayama. HMM-based ap-\nproach for automatic chord detection using reﬁned\nacoustic features. In International Conference on Acous-\ntics Speech and Signal Processing (ICASSP), Dallas,\nUSA, March 2010.",
  "topic": "Chord (peer-to-peer)",
  "concepts": [
    {
      "name": "Chord (peer-to-peer)",
      "score": 0.9493979215621948
    },
    {
      "name": "Computer science",
      "score": 0.6239619851112366
    },
    {
      "name": "Speech recognition",
      "score": 0.5707725286483765
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4215899705886841
    },
    {
      "name": "Musical",
      "score": 0.42086061835289
    },
    {
      "name": "Natural language processing",
      "score": 0.36304575204849243
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Distributed computing",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 7
}