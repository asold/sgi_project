{
  "title": "A study on richer syntactic dependencies for structured language modeling",
  "url": "https://openalex.org/W2167980204",
  "year": 2001,
  "authors": [
    {
      "id": "https://openalex.org/A1982578470",
      "name": "Peng Xu",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2198452902",
      "name": "Ciprian Chelba",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2019534230",
      "name": "FREDERICK JELINEK",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A1982578470",
      "name": "Peng Xu",
      "affiliations": [
        "Johns Hopkins University",
        "Johns Hopkins Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2198452902",
      "name": "Ciprian Chelba",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2019534230",
      "name": "FREDERICK JELINEK",
      "affiliations": [
        "Johns Hopkins Medicine",
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1551104980",
    "https://openalex.org/W1810157568",
    "https://openalex.org/W1543082528",
    "https://openalex.org/W1989705153",
    "https://openalex.org/W2024490156",
    "https://openalex.org/W2096175520",
    "https://openalex.org/W1642730643",
    "https://openalex.org/W2155693943",
    "https://openalex.org/W1530801890",
    "https://openalex.org/W2092654472",
    "https://openalex.org/W2121651659",
    "https://openalex.org/W2049633694",
    "https://openalex.org/W2963847008",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W1535015163"
  ],
  "abstract": "We study the impact of richer syntactic dependencies on the performance of the structured language model (SLM) along three dimensions: parsing accuracy (LP/LR), perplexity (PPL) and word-error-rate (WER, N-best re-scoring). We show that our models achieve an improvement in LP/LR, PPL and/or WER over the reported baseline results using the SLM on the UPenn Treebank and Wall Street Journal (WSJ) corpora, respectively. Analysis of parsing performance shows correlation between the quality of the parser (as measured by precision/recall) and the language model performance (PPL and WER). A remarkable fact is that the enriched SLM outperforms the baseline 3-gram model in terms of WER by 10% when used in isolation as a second pass (N-best re-scoring) language model.",
  "full_text": null,
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.9167114496231079
    },
    {
      "name": "Perplexity",
      "score": 0.898022472858429
    },
    {
      "name": "Computer science",
      "score": 0.7848409414291382
    },
    {
      "name": "Parsing",
      "score": 0.7817971110343933
    },
    {
      "name": "Natural language processing",
      "score": 0.7018741369247437
    },
    {
      "name": "Language model",
      "score": 0.6870222687721252
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6580961346626282
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5082178115844727
    },
    {
      "name": "Recall rate",
      "score": 0.49583563208580017
    },
    {
      "name": "Precision and recall",
      "score": 0.48764005303382874
    },
    {
      "name": "Word (group theory)",
      "score": 0.4736945629119873
    },
    {
      "name": "Word error rate",
      "score": 0.47350412607192993
    },
    {
      "name": "Speech recognition",
      "score": 0.380088210105896
    },
    {
      "name": "Linguistics",
      "score": 0.11119571328163147
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}