{
  "title": "OperA: Attention-Regularized Transformers for Surgical Phase Recognition",
  "url": "https://openalex.org/W3133570188",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5003203833",
      "name": "Tobias Czempiel",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5048218427",
      "name": "Magdalini Paschali",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5112383290",
      "name": "Daniel Ostler",
      "affiliations": [
        "Klinikum rechts der Isar",
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5100646694",
      "name": "Seong Tae Kim",
      "affiliations": [
        "Kyung Hee University"
      ]
    },
    {
      "id": "https://openalex.org/A5067135033",
      "name": "Benjamin Busam",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5046896448",
      "name": "Nassir Navab",
      "affiliations": [
        "Technical University of Munich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3100964249",
    "https://openalex.org/W2921303908",
    "https://openalex.org/W2266464013",
    "https://openalex.org/W2951058133",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W193465674",
    "https://openalex.org/W2107466310",
    "https://openalex.org/W2777273430",
    "https://openalex.org/W2979797179",
    "https://openalex.org/W3092562667",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2987451562",
    "https://openalex.org/W3094480255",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W6828894009",
    "https://openalex.org/W1905829557",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2804845319",
    "https://openalex.org/W3105393320",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3010209635",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2599653512",
    "https://openalex.org/W2805166516"
  ],
  "abstract": null,
  "full_text": "OperA: Attention-Regularized Transformers\nfor Surgical Phase Recognition\nTobias Czempiel1, Magdalini Paschali1, Daniel Ostler2, Seong Tae Kim1,\nBenjamin Busam1, Nassir Navab1,3\n1 Computer Aided Medical Procedures, Technische Universit¨ at M¨ unchen, Germany\n2 MITI, Klinikum Rechts der Isar, Technische Universit¨ at M¨ unchen, Germany\n3 Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, USA\nAbstract. In this paper we introduce OperA, a transformer-based model\nthat accurately predicts surgical phases from long video sequences. A\nnovel attention regularization loss encourages the model to focus on\nhigh-quality frames during training. Moreover, the attention weights are\nutilized to identify characteristic high attention frames for each surgical\nphase, which could further be used for surgery summarization. OperA\nis thoroughly evaluated on two datasets of laparoscopic cholecystectomy\nvideos, outperforming various state-of-the-art temporal reﬁnement ap-\nproaches.\nKeywords: Surgical Workﬂow Analysis · Surgical Phase Recognition ·\nTransformers · Self-Attention · Cholecystectomy\n1 Introduction\nSurgical workﬂow analysis is a crucial task for the operating room (OR) of the\nfuture [1]. Speciﬁcally, automatic detection of surgical phases is one of its most\nessential components. An eﬃcient surgical phase recognition system will build\nthe foundation for automated surgical assistance and cognitive guidance [2,3].\nOnline analysis during an ongoing intervention can provide feedback to surgeons\nand alarm the staﬀ in case of erroneous or adverse events [4]. Additionally,\nextracting surgical phases during an operation and group diﬀerent procedures\nbased on their unique characteristics plays an important role for modern surgical\ntraining. Since automatic extraction of surgical phases is particularly challeng-\ning, advanced Machine Learning (ML) methodologies [5] have been employed\ntowards solving it. However, factors such as variability of patient anatomy, sur-\ngeon style [6] as well as limited availability and quality of training data present\nproblems for modern ML algorithms.\nA recent development in ML that could help overcome these challenges in\nsurgical workﬂow analysis is transformer networks [7]. Transformers have shown\ntheir vast potential for sequential modeling in Natural Language Processing\n(NLP) [8] and have quickly become the gold standard in this area. Transformer\nnetworks have the capability to create temporal relationships between current\narXiv:2103.03873v1  [cs.CV]  5 Mar 2021\n2 Czempiel et al.\nand previous frames using self-attention, much like frequently used LSTM meth-\nods [9]. However, self-attention enables learning in long sequences without for-\ngetting of previous information which often hampers LSTM-based methods.\nAn additional advantage of transformer networks and self-attention over\nother approaches used for surgical phase recognition is their ability to visualize\nthe attention weights for a sequence, which could yield further insights into the\ndecision-making process of a model.\n1.1 Related Work\nAutomatic extraction of surgical phases was initially performed using binary\nsurgical signals [10], where a comparison with an average surgery determined\nthe surgical phase. Hidden Markov Models (HMM), provided an extension of\nthis idea capable of online predictions [11]. In EndoNet, Twinanda et al. [5]\nutilized image features extracted with a Convolutional Neural Network (CNN)\nto predict the surgical phase and surgical tool presence directly from surgi-\ncal images. EndoLSTM [12] additionally performed temporal reﬁnement with\nLSTMs [9], which improved the results substantially. A variety of works com-\nbined pre-trained CNNs as feature extractors, followed by temporal reﬁnement\nwith LSTMs [13,14]. In MTRCNet-CL [15], Jin et al. proposed a CNN/LSTM\nmodel to reﬁne the prediction over short sequences in an end-to-end fashion\nincluding a correlation loss to identify phase and tool correlations in an ex-\nplicit manner. Czempiel et al. [16] proposed TeCNO, which combined Temporal\nConvolutional Networks (TCN) with a ResNet-50 [17] feature extractor. Trans-\nformer models were ﬁrst introduced for NLP [7] where they quickly became the\nstate-of-the-art in a plethora of downstream tasks [18,8]. Furthermore, the ver-\nsatility of transformers has been showcased not only for vision tasks such as\nimage classiﬁcation [19] and text-to-image generation [20] but also in biology\nfor the challenging protein folding problem with AlphaFold [21]. In surgical data\nsciences, transformers have been explored only for surgical tool [22] classiﬁcation.\nAn additional aspect of transformers and self-attention is the fact that their\nattention weights could be used for model insights and explanation. Some works\nhave claimed that attention has limited explanation capabilities [23]. However,\nthis assumption has been challenged [24] suggesting that each work should deﬁne\ntheir notion of explanation since it could be dependent on the task at hand.\nIn this paper, we introduce for the ﬁrst time, OperA, a transformer-based\nmethod for online surgical phase prediction for laparoscopic operations. Our\ncontributions are:\n◦We successfully leverage a transformer-based model for surgical phase recog-\nnition that outperforms other temporal reﬁnement methods.\n◦We propose a novel attention regularizer, that improves the automatic ex-\ntraction of the most relevant frames with high feature-quality.\n◦We utilize the attention weights to extract and visualize characteristic frames.\n◦We carefully evaluate OperA on two challenging surgical video datasets.\nOperA: Attention-Regularized Transformers 3\nImage Features \nOutput Prob.\nConvolutional Layers\nFC\nLinear\nSoftmax\nAdd & norm\nMatMul\nSoftmax\nScale\nMask\nMatMul\nAdd & norm\nFC\nCEE\n11 Layers\ncausal\nmask\nn\nTime ( )\nQ\nK\nV\nLinear\nFig. 1: Overview of the proposed OperA model. Image features F are used\nas input for the transformer. The output logits p(F) of the feature extraction\nbackbone are used in combination with the normalized frame-wise attention\nweights n to regularize the attention.\n2 Methodology\nOur proposed model, OperA, consists of a CNN for visual feature extraction fol-\nlowed by multiple self-attention layers. The attention map is regularized during\ntraining to focus on reliable CNN image features. The full network architecture\nis visualized in Fig. 1.\nFor our feature extraction backbone we trained a ResNet-50 [17] frame-wise\nCNN without sequential modeling. We trained this model on phase recognition\nand additionally on surgical tool detection, if tool information was available in\nthe dataset. The result of the feature extraction backbone are per frame image\nfeatures F∈ R2048 and their corresponding class probabilities p(F) ∈[0,1]c with\nc the number of classes.\n2.1 Sequential Transformer Network\nOur model expands on the well-known Transformer architecture [7] with the ad-\ndition of our attention regularization that will be discussed below. Transformers\nhave the capabilities to model long sequences in a parallel manner using self-\nattention by relating every input feature with other input features regardless of\ntheir distance in the sequence [25]. Visualized in Fig. 1, we ﬁrst calculate the\nquery Q, key K and value V, the inputs for the scaled dot product attention\nusing a linear layer such that ( Q,K,V ) = Linear (F) ∈R3d with d= 64.\nAttentionWeights(Q,K) = softmax\n(\nmask\n(QKT\n√\nd\n))\n(1)\nAttention(Q,K,V ) = AttentionWeights(Q,K)V (2)\n4 Czempiel et al.\nOur architecture uses 11 consecutive layers each consisting of a linear layer,\na scaled dot-product attention layer, a layer normalization [26] and residual\nconnections [17]. Similar to the architecture of the Vision Tranformer [19] after\nthe last encoding layer, a linear layer followed by a softmax is used to estimate\nthe class-wise output probabilities y for each frame of the sequence. For the\ntraining of the model we use a median frequency balanced cross-entropy loss [27]\nLc. Causal masking [28] with the binary mask M ∋{0,1}is performed on the\nattention map of the model, to prevent information leakage from future frames to\nthe current frame prediction. This allows us to use OperA for real-time surgical\nphase prediction.\n2.2 Normalized Frame-Wise Attention\nFor each frame, each layer of our transformer generates one column in A =\nAttentionWeights(Q,K). Due to the softmax activation function (Eq. 1) each\nrow of the attention map A sums up to 1. Column-wise summation of the at-\ntention weights in Aresults in the total attention value for each frame at time t.\nDue to the causal mask visualized in Fig. 1, the ﬁrst frame has the opportunity\nto contribute T times, where T is the number of frames in a video, while the\nlast frame is considered only once. We therefore need to normalize the frame-\nwise attention by dividing the total attention of each frame with the number of\ntimes this frame is considered, thus equally weighting the attention of all frames\nregardless of their position in the video. The normalized frame-wise attention is\ncalculated by: n = (n1,...,n T ) with nj =\n∑\ni Aij∑\ni Mij\n.\n2.3 Attention Regularization\nDiﬀerent from NLP and Visual Transformers, the input of OperA is generated by\na CNN backbone network. The quality of each frame embedding of the CNN can\nvary drastically, especially for frames where the CNN predictions are incorrect.\nTo this end, OperA should focus on higher quality CNN features, that were\ncorrectly classiﬁed by the backbone CNN. Such features have higher softmax\nprobabilities, or conﬁdence, and lower cross-entropy values.\nWe learn this relationship by comparing the normalized frame-wise attention\nweights with the prediction error of our CNN. The regularization then reads:\nLreg = ⟨n,CEE (p(F),y)⟩ (3)\nThe Cross Entropy Evaluation value (CEE) describes the residual error of\np(F) compared to the ground truth label y. It should be noted that the weights\nof the backbone CNN remain frozen and CEE is only used for the optimization of\nthe attention weights. Multiplying CEE with the normalized frame-wise atten-\ntion n explicitly penalizes the model if a high attention value was generated for\na feature with low CNN conﬁdence. We apply the proposed regularization to the\nﬁrst attention layer as it has a direct relationship with the input visual features.\nThe ﬁnal loss function used for model training is denoted as: L= Lc + λ·Lreg\nOperA: Attention-Regularized Transformers 5\nSumming up all the normalized attention weights for each layer we generate a\nﬁnal attention value for each frame. In order to interpret whether OperA focuses\non highly-informative frames that correctly represent each phase, we extract the\nframes with Highest Attention (HA) and the ones with Lowest Attention (LA).\n3 Experimental Setup\nDatasets For the evaluation of OperA we use two challenging surgical workﬂow\nintra-operative video datasets of laparoscopic cholecystectomy procedures. The\npublicly available Cholec80 [5] includes 80 videos with a resolutions of 1920×1080\nor 854 ×480 pixels recorded at 25 frames-per-second (fps). For this work, the\ndataset was sub-sampled to 1fps. Every frame in the video has been manually\nassigned to one out of seven surgical phases. Additionally, seven diﬀerent tool\nannotation labels sampled at 1fps are provided. We randomly select 20 videos\nfor testing and the remaining 60 videos for training (48) and validation (12).\nMitiSW (Miti Surgical Workﬂow) was collected and annotated by the MITI\ngroup at the Klinikum rechts der Isar in Munich. The dataset consists of 85\nlaparoscopic cholecystectomy videos with resolution 1920×1080 pixels and sam-\npling rate of 1fps. MitiSW includes the 7 surgical phases of Cholec80, shown in\nFig. 3, along with one additional phase Pre-preparation, used to describe frames\nbefore the Preparation phase. The phases have been annotated by expert physi-\ncians with no additional tool-presence information. 20 videos are utilized for\ntesting and the remaining 65 videos for training (52) and validation (13). For all\nthe experiments 5-fold cross validation is performed. To balance our combined\nloss function we set λ to 1.\nModel Training OperA was trained for the task of surgical phase recognition\nusing the Adam optimizer with an initial learning rate of 1e-5 for 30 epochs.\nWe report the test results extracted by the model that performed best on the\nvalidation set for each fold. The batch size is identical to the length of each video.\nOur method was implemented in PyTorch and our models were trained on an\nNVIDIA Titan V 12GB GPU using Polyaxon4. The source code for OperA along\nwith the Evaluation scripts is publicly available 5.\nEvaluation Metrics and Baselines To comprehensively measure the results\nwe report the video-level Accuracy (Acc) the harmonic mean (F1) of Precision\nand Recall [11] and average the results over the 5 splits. We perform ablative\ntesting to identify the most suitable number of attention layers and to test the\neﬀect of the regularization term (Sec. 2.3). Finally, we compare OperA with a\nvariety of surgical phase recognition baselines.\n4 https://polyaxon.com/\n5 https://github.com/tobiascz/OperA/\n6 Czempiel et al.\nTable 1: Ablative testing results for 6 and 11 transformer layers and with the\naddition of Attention Regularization (Reg). Average metrics over 5 folds are\nreported (%) with the corresponding standard deviation ( ±).\nCholec80 MitiSW\nLayers Reg Acc F1 Acc F1\n6 - 90.35 ± 0.71 83.45 ± 0.32 84.88 ± 1.43 84.92 ± 1.12\n6 ✓ 90.49 ± 0.70 84.01 ± 0.39 85.41 ± 1.26 85.14 ± 1.20\n11 - 90.37 ± 0.86 83.85 ± 0.33 85.02 ± 1.01 85.28 ± 0.98\n11 ✓ 91.26 ± 0.64 84.49 ± 0.60 85.77 ± 0.95 85.44 ± 0.79\n4 Results and Discussion\nEﬀect of Layers and Regularization In Table 1 we compare models trained\nwith 6 and 11 attention layers and evaluate the impact of the attention regu-\nlarization. 11 attention layers is the highest amount of layers that we could ﬁt\nin the VRAM. For both datasets, the results slightly increase for 11 attention\nlayers by ∼1% both in terms of Accuracy and F1-score. Regarding the attention\nregularization, a ∼1% improvement is reported for both datasets and number\nof layers. As we will discuss later, the attention regularization does not only\nmarginally increases the model performance but also the quality of the highest\nattention video frames.\nBaseline Comparison We compare OperA with various methods for surgical\nphase recognition. ResNet-50 is the feature extraction backbone, ResLSTM [14]\nand MTRCNet-CL [15] utilize LSTMs for the temporal reﬁnement. Diﬀerent to\nthe other models MTRCNet-CL is trained in an end-to-end fashion combining\na CNN feature extraction with LSTM training. One of the downsides to this\napproach is that due to memory constraint only a limited sequence of the video\ncan be used per-batch. ResLSTM, TeCNO and OperA use pre-trained image\nfeatures and can therefore analyze a full video sequence at once.\nFirst, we see that temporal reﬁnement achieves a substantial improvement over\nResNet-50 ranging from 4-10% for Cholec80 and 9-12% for MitiSW, showcasing\nits advantage. MTRCNet-CL is outperformed by the other temporal models by\n2-6% potentially potentially due to the limited sequence length that can be pro-\ncessed in every batch. OperA with or without positional encoding (PE) [7] out-\nperforms the other temporal models by 2-6% in terms of accuracy for Cholec80\nshowcasing the abilities of transformers to model long temporal dependencies.\nRegarding MitiSW, OperA without PE has increased accuracy by 0.6-3% and\nF1-score by 1% over all baselines. For both datasets it can be seen that PE\nmarginally decreases the performance, potentially due to the increased sequence\nlength in surgical videos in comparison to NLP tasks.\nOperA: Attention-Regularized Transformers 7\nTable 2: Baseline comparisons for Cholec80 and MitiSW. MTRCNet-CL re-\nquires tool information, therefore cannot be used for MitiSW. We report the\naverage metrics over 5-fold cross validation along with their respective standard\ndeviation (±).\nCholec80 MitiSW\nAcc F1 Acc F1\nResNet-50 81.21 ± 1.16 72.98 ± 1.17 73.90 ± 1.89 71.53 ± 1.41\nResLSTM 87.94 ± 0.80 82.29 ± 0.78 82.97 ± 1.18 84.06 ± 1.15\nMTRCNet-CL 85.64 ± 0.21 80.94 ± 0.95 – –\nTeCNO 89.05 ± 0.79 84.04 ± 0.64 85.09 ± 1.67 84.18 ± 1.53\nOperA + PE 90.20 ± 1.45 83.34 ± 0.97 83.67 ± 1.54 84.04 ± 1.20\nOperA 91.26 ± 0.64 84.49 ± 0.64 85.77 ± 0.95 85.44 ± 0.79\nGT Label\nPrediction\nOperA\nFrame \nWise\nAttention\nPrediction\nCNN\nFig. 2: Qualitative results of the predictions per phase for video 66 from\nCholec80 using the feature extraction CNN and OperA compared to the ground\ntruth labels. In the frame-wise attention, brighter (yellow) color corresponds to\nhigher attention, darker (blue) color to lower attention. The position of the LA\nframes for each phase is denoted with ▽and the HA frames with △.\nPredictions and Attention Values In Fig. 2 we visualize the ground truth,\npredictions and attention values for video 66 of Cholec80. First, we see that\nthe predictions of OperA are smoother and more consistent than the ones of\nthe CNN. Moreover, the HA frames (denoted with △) in most cases correspond\nwith frames, where the CNN and OperA predictions were correct, while LA\nframes (denoted with ▽) are at positions where the CNN predictions were wrong,\nconﬁrming that attention regularization works as intended.\nHighest and Lowest Attention Frames In Fig. 3 we visualize the HA and\nLA frames per phase for the models trained with and without attention regular-\nization. Visual inspection revealed that LA frames are generally less descriptive\nfor the respective surgical phase. However, as we can see highlighted by the blue\nboxes, the model trained without attention regularization has minimum atten-\ntion for frames containing surgical tools that are quite characteristic of their\nrespective phase.\n8 Czempiel et al.\nHighest Attention Frames (HA)\nLowest Attention Frames (LA)\nPreparation Calot Triangle\nDissection\nClipping &\nCutting\nGallbladder\nDissection\nCleaning\nCoagulation\nGallbladder\nRetraction\nGallbladder\nPackaging\nFig. 3: Visualization of frames of video 66 of Cholec80 with highest (HA) and\nlowest (LA) attention per phase for the models with and without attention regu-\nlarization. Blue and red boxes denote frames of the model without regularization\nthat have low attention, while they are descriptive of their phase and high at-\ntention, while they are not.\nRegarding the HA frames, they are more diverse and representative of their sur-\ngical phase. For the model trained with attention regularization surgical tools\nare present in all phases besides “Preparation”, highlighting the strong correla-\ntion between tools and surgical phases even though the attention model was not\ntrained with tool information. With red boxes, we are showcasing the HA frames\nfor the model trained without attention regularization. These frames were not\ndescriptive of their phase and very similar to each other in the case of “Cleaning\nCoagulation” and “Gallbladder Retraction”. These ﬁndings highlight the bene-\nﬁts of the proposed attention regularization and its potential for surgery video\nsummarization.\n5 Conclusion\nIn this paper we introduced OperA, a transformer-based model that accurately\npredicted surgical phases of cholecystectomy procedures, outperforming a vari-\nety of baselines on two challenging laparoscopic video datasets. Additionally, our\nnovel attention regularizer enabled OperA to extract characteristic high atten-\ntion frames. Future work includes applying our method to diﬀerent laparoscopic\nprocedures and explore its potential for surgical video summarization.\nOperA: Attention-Regularized Transformers 9\nAcknowledgements\nThis work is partially funded by the DFG research unit PLAFOKON (FKZ:\nNA620/33-2) and BMBF project ARTEKMED (FKZ: 16SV8088) in collabo-\nration with the Minimal-invasive Interdisciplinary Intervention Group (MITI).\nFinally, we would like to thank NVIDIA for the GPU donation.\nReferences\n1. L. Maier-Hein, M. Eisenmann, C. Feldmann, H. Feussner, G. Forestier, S. Gian-\nnarou, B. Gibaud, G. D. Hager, M. Hashizume, D. Katic, et al. , “Surgical data\nscience: A consensus perspective,” arXiv preprint arXiv:1806.03184 , 2018.\n2. C. R. Garrow, K.-F. Kowalewski, L. Li, M. Wagner, M. W. Schmidt, S. Engelhardt,\nD. A. Hashimoto, H. G. Kenngott, S. Bodenstedt, S. Speidel, et al. , “Machine\nlearning for surgical phase recognition: A systematic review,” Annals of Surgery ,\n2020.\n3. N. Padoy, “Machine and deep learning for workﬂow recognition during surgery,”\nMinimally Invasive Therapy and Allied Technologies , vol. 28, pp. 82–90, 2019.\n4. A. Huaulm´ e, P. Jannin, F. Reche, J. L. Faucheron, A. Moreau-Gaudry, and\nS. Voros, “Oﬄine identiﬁcation of surgical deviations in laparoscopic rectopexy,”\nArtiﬁcial Intelligence in Medicine , vol. 104, no. May 2019, 2020.\n5. A. P. Twinanda, S. Shehata, D. Mutter, J. Marescaux, M. De Mathelin, and\nN. Padoy, “EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic\nVideos,” IEEE Transactions on Medical Imaging , vol. 36, no. 1, pp. 86–97, 2017.\n6. I. Funke, S. T. Mees, J. Weitz, and S. Speidel, “Video-based surgical skill assess-\nment using 3D convolutional neural networks,” International Journal of Computer\nAssisted Radiology and Surgery, vol. 14, no. 7, pp. 1217–1225, 2019.\n7. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,  L. Kaiser,\nand I. Polosukhin, “Attention is all you need,” Advances in Neural Information\nProcessing Systems, vol. 2017-Decem, no. Nips, pp. 5999–6009, 2017.\n8. J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep\nbidirectional transformers for language understanding,” NAACL HLT 2019 - 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies - Proceedings of the Conference, vol. 1,\nno. Mlm, pp. 4171–4186, 2019.\n9. S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” Neural Computa-\ntion, vol. 9, no. 8, pp. 1735–1780, 1997.\n10. S. A. Ahmadi, T. Sielhorst, R. Stauder, M. Horn, H. Feussner, and N. Navab, “Re-\ncovery of surgical workﬂow without explicit models,” Lecture Notes in Computer\nScience (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture\nNotes in Bioinformatics) , vol. 4190 LNCS, pp. 420–428, 2006.\n11. N. Padoy, T. Blum, S. A. Ahmadi, H. Feussner, M. O. Berger, and N. Navab, “Sta-\ntistical modeling and recognition of surgical workﬂow,” Medical Image Analysis ,\nvol. 16, no. 3, pp. 632–641, 2012.\n12. A. P. Twinanda, N. Padoy, M. J. Troccaz, and G. Hager, “Vision-based Approaches\nfor Surgical Activity Recognition Using Laparoscopic and RBGD Videos,” Thesis,\nno. Umr 7357, 2017.\n13. G. Yengera, D. Mutter, J. Marescaux, and N. Padoy, “Less is more: Surgical phase\nrecognition with less annotations through self-supervised pre-training of cnn-lstm\nnetworks,” arXiv preprint arXiv:1805.08569 , 2018.\n10 Czempiel et al.\n14. Y. Jin, Q. Dou, H. Chen, L. Yu, J. Qin, C. W. Fu, and P. A. Heng, “SV-RCNet:\nWorkﬂow recognition from surgical videos using recurrent convolutional network,”\nIEEE Transactions on Medical Imaging , vol. 37, no. 5, pp. 1114–1126, 2018.\n15. Y. Jin, H. Li, Q. Dou, H. Chen, J. Qin, C. W. Fu, and P. A. Heng, “Multi-task\nrecurrent convolutional network with correlation loss for surgical video analysis,”\nMedical Image Analysis, vol. 59, 2020.\n16. T. Czempiel, M. Paschali, M. Keicher, W. Simson, H. Feussner, S. T. Kim, and\nN. Navab, “TeCNO: Surgical Phase Recognition with Multi-stage Temporal Con-\nvolutional Networks,” in Medical Image Computing and Computer Assisted Inter-\nvention – MICCAI 2020 , (Cham), pp. 343–352, Springer Publishing, 2020.\n17. K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recog-\nnition,” in 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), IEEE, jun 2016.\n18. T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Nee-\nlakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,\nT. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse,\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCan-\ndlish, A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot\nlearners,” arXiv, 2020.\n19. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. , “An image is\nworth 16x16 words: Transformers for image recognition at scale,” arXiv preprint\narXiv:2010.11929, 2020.\n20. A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen,\nand I. Sutskever, “Zero-shot text-to-image generation,” arXiv preprint\narXiv:2102.12092, 2021.\n21. L. Heo and M. Feig, “High-accuracy protein structures by combining machine-\nlearning with physics-based reﬁnement.,” Proteins, vol. 88, pp. 637–642, may 2020.\n22. S. Kondo, “Lapformer: surgical tool detection in laparoscopic surgical video us-\ning transformer architecture,” Computer Methods in Biomechanics and Biomedical\nEngineering: Imaging & Visualization , pp. 1–6, 2020.\n23. S. Jain and B. C. Wallace, “Attention is not explanation,” in Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapo-\nlis, MN, USA, June 2-7, 2019, Volume 1 , pp. 3543–3556, Association for Compu-\ntational Linguistics, 2019.\n24. S. Wiegreﬀe and Y. Pinter, “Attention is not not explanation,” in Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), (Hong Kong, China), pp. 11–20, Association for Computational Linguis-\ntics, Nov. 2019.\n25. Y. Kim, C. Denton, L. Hoang, and A. M. Rush, “Structured attention networks,”\nInternational Conference on Learning Representations, pp. 1–21, 2017.\n26. J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv preprint\narXiv:1607.06450, 2016.\n27. D. Eigen and R. Fergus, “Predicting Depth, Surface Normals and Semantic Labels\nwith a Common Multi-scale Convolutional Architecture,” in 2015 IEEE Interna-\ntional Conference on Computer Vision (ICCV) , IEEE, dec 2015.\n28. R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones, “Character-Level Lan-\nguage Modeling with Deeper Self-Attention,” Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , vol. 33, jul 2019.\nOperA: Attention-Regularized Transformers 11\n6 Supplementary\n(a) Ablative testing results for 6 and 11 transformer layers and with the addition of\nAttention Regularization (Reg). Average metrics over 5 folds are reported (%) with\nthe corresponding standard deviation ( ±).\nLayers Reg Accuracy F1 Precision Recall\nCholec80\n6 - 90.35 ± 0.71 83.45 ± 0.32 80.64 ± 1.41 86.48 ± 0.61\n6 ✓ 90.49 ± 0.70 84.01 ± 0.39 81.38 ± 0.29 86.98 ± 0.61\n11 - 90.37 ± 0.86 83.85 ± 0.33 81.60 ± 0.40 86.23 ± 0.34\n11 ✓ 91.26 ± 0.64 84.48 ± 0.60 82.19 ± 0.70 86.92 ± 0.86\nMitiSW\n6 - 84.88 ± 1.43 84.92 ± 1.12 82.76 ± 1.43 87.20 ± 1.02\n6 ✓ 85.41 ± 0.95 85.14 ± 1.20 83.00 ± 1.34 87.41 ± 1.66\n11 - 85.02 ± 1.01 85.28 ± 0.98 82.89 ± 1.20 87.82 ± 0.75\n11 ✓ 85.77 ± 0.95 85.44 ± 0.78 83.32 ± 1.52 87.68 ± 1.08\n(b) Baseline comparisons for Cholec80 and MitiSW. MTRCNET-CL requires tool\ninformation, therefore cannot be used for MitiSW. We report the average metrics over\n5-fold cross validation along with their respective standard deviation ( ±).\nAccuracy F1 Precision Recall\nCholec80\nResNet-50 81.21 ± 1.16 72.98 ± 1.17 68.35 ± 1.61 78.31 ± 1.14\nResLSTM 87.94 ± 0.80 82.29 ± 0.78 80.26 ± 1.12 84.43 ± 0.85\nMTRCNet-cl 85.64 ± 0.21 80.94 ± 0.95 79.31 ± 0.97 82.67 ± 0.114\nTeCNO 89.05 ± 0.79 84.04 ± 0.64 80.90 ± 0.75 87.44 ± 0.64\nOperA + PE 90.20 ± 1.45 83.34 ± 0.97 80.78 ± 1.42 86.08 ± 0.89\nOperA 91.26 ± 0.64 84.48 ± 0.60 82.19 ± 0.70 86.92 ± 0.86\nMitiSW\nResNet-50 73.90 ± 1.89 71.53 ± 1.41 69.06 ± 1.38 74.20 ± 1.63\nResLSTM 82.97 ± 1.18 84.06 ± 1.15 82.08 ± 1.57 86.15 ± 0.94\nTeCNO 85.09 ± 1.67 84.18 ± 1.53 82.03 ± 0.20 86.50 ± 0.43\nOperA + PE 83.67 ± 1.54 84.04 ± 1.20 81.34 ± 1.60 86.94 ± 0.98\nOperA 85.77 ± 0.95 85.44 ± 0.79 83.32 ± 1.10 87.68 ± 0.71\n12 Czempiel et al.\nHighest Attention Frames (HA)\nLowest Attention Frames (LA)\nPreparation\nCholec80 - Video77\nCholec80 - Video80\nCalot Triangle\nDissection\nClipping &\nCutting\nGallbladder\nDissection\nCleaning\nCoagulation\nGallbladder\nRetraction\nGallbladder\nPackaging\nHighest Attention Frames (HA)\nLowest Attention Frames (LA)\nPreparation Calot Triangle\nDissection\nClipping &\nCutting\nGallbladder\nDissection\nCleaning\nCoagulation\nGallbladder\nRetraction\nGallbladder\nPackaging\nFig. 4: Two more videos from Cholec80 are visualized with maximum and min-\nimum attention per phase for the models with and without attention regulariza-\ntion. Blue and red boxes denote frames of the model without regularization that\nhave low attention, while they are descriptive of their phase and high attention,\nwhile they are not.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7571057677268982
    },
    {
      "name": "Automatic summarization",
      "score": 0.7473857402801514
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.7073659896850586
    },
    {
      "name": "Laparoscopic cholecystectomy",
      "score": 0.607526957988739
    },
    {
      "name": "Opera",
      "score": 0.5982773303985596
    },
    {
      "name": "Computer science",
      "score": 0.567716121673584
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4165685772895813
    },
    {
      "name": "Surgery",
      "score": 0.1622009575366974
    },
    {
      "name": "Art",
      "score": 0.15313661098480225
    },
    {
      "name": "Medicine",
      "score": 0.1430354118347168
    },
    {
      "name": "Engineering",
      "score": 0.1427558958530426
    },
    {
      "name": "Electrical engineering",
      "score": 0.10393515229225159
    },
    {
      "name": "Literature",
      "score": 0.08490142226219177
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I2802619606",
      "name": "Klinikum rechts der Isar",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I35928602",
      "name": "Kyung Hee University",
      "country": "KR"
    }
  ]
}