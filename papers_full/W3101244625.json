{
  "title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense",
  "url": "https://openalex.org/W3101244625",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4223874739",
      "name": "Zhou, Wangchunshu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2217405970",
      "name": "Lee Dong Ho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4377778731",
      "name": "Selvam, Ravi Kiran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2945590072",
      "name": "Lee， Se-Yeon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224878954",
      "name": "Lin, Bill Yuchen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127770351",
      "name": "Ren Xiang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2968629361",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2973722444",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W3102187933",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3023419341",
    "https://openalex.org/W2995643077",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2250770256",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3176750236",
    "https://openalex.org/W2789566302",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3040558716",
    "https://openalex.org/W2073302931",
    "https://openalex.org/W11298561",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2980282514"
  ],
  "abstract": "Pre-trained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational commonsense knowledge about everyday concepts, which is crucial to many downstream tasks that need common sense to understand or generate. To augment PTLMs with concept-centric commonsense knowledge, in this paper, we propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). Furthermore, we develop a joint pre-training framework to unify generative and contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that our method, concept-aware language model (CALM), can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks. We show that while only incrementally pre-trained on a relatively small corpus for a few steps, CALM outperforms baseline methods by a consistent margin and even comparable with some larger PTLMs, which suggests that CALM can serve as a general, plug-and-play method for improving the commonsense reasoning ability of a PTLM.",
  "full_text": "Preprint. Work in progress.\nPRE-TRAINING TEXT-TO-TEXT TRANSFORMERS FOR\nCONCEPT -CENTRIC COMMON SENSE\nWangchunshu Zhou1∗, Dong-Ho Lee2∗, Ravi Kiran Selvam2,\nSeyeon Lee2, Bill Yuchen Lin2, Xiang Ren2\n1 Beihang University 2 University of Southern California\nzhouwangchunshu@buaa.edu.cn, {dongho.lee, xiangren}@usc.edu\nABSTRACT\nPre-trained language models (PTLM) have achieved impressive results in a range\nof natural language understanding (NLU) and generation (NLG) tasks. However,\ncurrent pre-training objectives such as masked token prediction (for BERT-style\nPTLMs) and masked span inﬁlling (for T5-style PTLMs) do not explicitly model\nthe relational commonsense knowledge about everyday concepts, which is crucial\nto many downstream tasks that need common sense to understand or generate. To\naugment PTLMs with concept-centric commonsense knowledge, in this paper, we\npropose both generative and contrastive objectives for learning common sense\nfrom the text, and use them as intermediate self-supervised learning tasks for\nincrementally pre-training PTLMs (before task-speciﬁc ﬁne-tuning on downstream\ndatasets). Furthermore, we develop a joint pre-training framework to unify gen-\nerative and contrastive objectives so that they can mutually reinforce each other.\nExtensive experimental results show that our method, concept-aware language\nmodel (CALM)1, can pack more commonsense knowledge into the parameters of\na pre-trained text-to-text transformer without relying on external knowledge graphs,\nyielding better performance on both NLU and NLG tasks. We show that while only\nincrementally pre-trained on a relatively small corpus for a few steps, CALM out-\nperforms baseline methods by a consistent margin and even comparable with some\nlarger PTLMs, which suggests that CALM can serve as a general, “plug-and-play”\nmethod for improving the commonsense reasoning ability of a PTLM.\n1 I NTRODUCTION\nPre-trained language models (PLTMs) such as BERT (Devlin et al., 2018) and T5 (Raffel et al.,\n2019) have revolutionized the ﬁeld of NLP, yielding impressive performance on various conventional\nnatural language understanding (NLU) and generation (NLG) tasks. BERT and its novel variants\nsuch as RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2019) capture syntactical and semantic\nknowledge mainly from the pre-training task of masked language modeling, while T5-style models\nsuch as BART (Lewis et al., 2019) instead focus onmasked span inﬁllingtasks. Though yielding better\nperformance on many downstream tasks, these pre-training objectives, however, do not explicitly\nguide the models to reason with concept-centric commonsense knowledge from language, including\nthe relation and composition of daily concepts in our lives. This leaves room for equipping current\nPTLMs with richer commonsense reasoning ability.\nFor example, consider a multi-choice question “What do you ﬁll with ink to write notes on a piece of\ncopy paper? (A) fountain pen (B) pencil case (C) printer (D) notepad”. The current state-of-the-art\nquestion answering model, UniﬁedQA (Khashabi et al., 2020), which was ﬁne-tuned on T5-large\nwith multiple datasets, still predicts ‘(C)printer’ as its answer. The model may be overly sensitive to\nthe co-occurrence between phrases in question sentence like ‘ink’ and ‘copy paper’ and the answer\nchoice ‘printer’, but fails to reason with the concept-centric knowledge that ‘fountain pen’ is a writing\ninstrument that needs to be ﬁlled with ‘ink’. Such mistake in commonsense reasoning becomes a\nbottleneck for current PTLMs (Davis & Marcus, 2015). Towards augmenting PTLMs with more\n∗Equal contribution. The work was done when Wangchunshu was visiting USC.\n1Code and data have been uploaded and will be published: https://anonymous.4open.science/repository/\n6fdeed55-ec2c-4ffa-aee8-0cc3b7f5ade5\n1\narXiv:2011.07956v2  [cs.CL]  25 Nov 2020\nPreprint. Work in progress.\nknowledge, prior works mainly focus on training larger models (Brown et al., 2020), adding speciﬁc\narchitectures to exploit external knowledge (Peters et al., 2019), or incorporating knowledge bases\nfor pre-training (Xiong et al., 2020). In this paper, we instead look to explicitly teach pre-trained\nmodels to write and reason with common concepts through novel pre-training strategies.\nWe present two kinds of self-supervised pre-training tasks: concept-to-sentence generation (C2S)\nand concept order recovering (COR). C2S trains the pre-trained model to compose (“write\")\nsentences given a set of concepts, and expects the generated sentences to be ﬂuent and plausible in\nterms of commonsense. COR aims to teach models to detect and revise a corrupted sentence with\nincorrect ordering of concepts. As illustrated in Figure 1, both tasks require a pre-trained model to\nrecall relevant commonsense facts about the concepts and to understand the underlying commonsense\nrelations between them. Both of the proposed objectives can explicitly encourage the model to capture\nthe relational concept-centric commonsense knowledge and perform compositional reasoning.\nSpeciﬁcally, we need a generative pre-training objective to encourage models to capture this genera-\ntive commonsensereasoning ability, so that models can learn to generate sentences with common-\nsense knowledge for both C2S and COR. Also, to teach modes to distinguish truth sentences from\nless plausible ones, we need to teach models with discriminative commonsensethrough contrastive\nself-training. To unify both generative and contrastive objectives within a joint learning framework\nso that the model can learn both generative and discriminative commonsense knowledge at the same\ntime, we propose to use the sentences generated by the model itself as the distractors and train the\nmodel to distinguish the generated sentences from real sentences. In this way, the model is forced\nto acquire new commonsense knowledge in order to distinguish the distractors generated by itself,\nwhich probably exploit the knowledge the model already possesses. Therefore, the model is trained\nto iteratively improve upon itself in a self-play fashion. We share all the parameters between the\ngenerator (trained with the generative objective) and the discriminator (trained with the contrastive\nobjective), then train multiple objectives with different preﬁxes. Compared to previous works (Peters\net al., 2019; Li et al., 2019; Xiong et al., 2020) that utilize external knowledge bases like Wikidata\nor ConceptNet, our approach can directly improve the generative and discriminative commonsense\nreasoning ability of PTLMs at the same time without relying on external knowledge bases.\nTo evaluate the effectiveness of our proposed method, we apply our method in an intermediate-task\ntransfer learning setting (Pruksachatkun et al., 2020) based on the pre-trained T5-base model to train\na Concept-Aware Language Model (CALM). While only continually pre-trained on a small dataset\nfor a relatively fewer number of updates (compared to conventional pre-training), CALM consistently\noutperforms T5-base on four commonsense-related NLU datasets (i.e., COMMONSENSE QA, OPEN -\nBOOK QA, PIQA , and ANLI) and COMMON GEN, a commonsense-related NLG dataset. Our results\nand careful ablation studies demonstrate the potential of our method to serve as a “plug-and-play”\nmethod for any pre-trained text-to-text transformer before ﬁne-tuning on commonsense-related tasks.\nTo the best of our knowledge, our work is the ﬁrst to investigate concept-centric self-supervised\nobjectives that improve both generative and discriminative commonsense reasoning ability of a\npre-trained language model.\n2 S ELF -SUPERVISED OBJECTIVES FOR CONCEPT -CENTRIC LEARNING\nIn this section, we ﬁrst describe the proposed generative and contrastive objectives used for improving\nthe commonsense reasoning ability of pre-trained text-to-text transformers. Then, we introduce the\njoint learning framework which uniﬁes the proposed self-supervised objectives and learn a uniﬁed\ntext-to-text transformer based on pre-trained models such as T5.\n2.1 G ENERATIVE OBJECTIVES\nSimilar to many other pre-training tasks such as masked language modeling, we aim to teach models\nto recover original sentences from corrupted inputs, which is often regarded as a denoising process.\nWe propose two generative self-supervised pre-training objectives: concept-to-sentence generation\n(C2S) and concept order recovering (COR).\nConcept Extraction.Given an input x = [x1,x2,...,x n], we ﬁrst conduct part-of-speech tagging\nwith Spacy for the sentence and extract Verb, Noun, and Proper Nouns from the sentence to use as\n2\nPreprint. Work in progress.\nConcept-to-Sentence\nInput: <c2s> Generate a sentence with the concepts: \nforward, Simpson, ignore, information, prosecutor\nOutput: The information was forwarded to \nSimpson 's prosecutors, but it was ignored.\nConcept Order Recovering\nInput: <cor> Correct the order of the given sentence:\nRahul stops him, fights his bar, and drives to a \nlocal performance.\nOutput: \nRahul fights him, stops his performance, and \ndrives to a local bar.\ngenerative common sense \nText-to-Text \nTransformer\nText-to-Text \nTransformer\nFigure 1: Two self-supervised pre-training objectives that teach text-to-text transformers with\ngenerative common sense:(1) Concept-to-Sentence Generation(C2S) pre-trains the model to\nrecover the original sentence with a shufﬂed concept set, e.g., {forward, Simpson, ignore, information,\nprosecutor} →“The information was forwarded to Simpson’s prosecutors, but it was ignored.”\n(2) Concept Order Recovering(COR), similarly, teaches the model to correct the mispositioned\nconcepts in the original sentence. For example, the concepts (stops, ﬁghts, bar, drives, performance),\nare randomly reordered in the input, while the model should recover the original sentence.\nconcepts2. Next, we form concept-sets C= [v1,v2,...,v p,n1,n2 ...,n q] where vi and ni denotes\nthe i-th verb or noun/proper noun concept (token) in x. We denote Cv and Cn as the set of verb and\nnoun/proper noun concepts respectively in C. (i.e. Cv = [v1,v2,...,v p] and Cn = [n1,n2,...,n q].)\nConcept-to-Sentence Generation (C2S).The concept-to-sentence generation (C2S) objective re-\nquires the text-to-text transformer to recover the original sentence given only a few unordered\nkeywords of the sentence. Speciﬁcally, given a sentence, we shufﬂe the extracted concept-set Cto\ncreate the perturbed source sequence and train the model to generate the original sentence with a\npreﬁx (denoted as <c2s>) as described in Fig. 1. Formally, the C2S objective can be formulated as:\nLc2s = E\n( n∑\ni=1\n−log p(xi|<c2s>; PERMUTE (C); x1:i−1)\n)\n(1)\nwhere the PERMUTE () function randomly shufﬂe the concepts in the concept-set. This objective\nrequires the model to construct an acceptable commonsense sentence by adhering to and reasoning\nover the commonsense relations between the given concepts. Therefore, relational commonsense\nknowledge is implicitly injected into the parameters of the model. The C2S objective is motivated by\nthe task proposed in Lin et al. (2020). Compared to their work, the concept-set used in C2S covers\nmore concepts such as named entities, while the original task only includes the concepts appearing in\nConceptNet. We apply the task in a general domain and as a pre-training objective, instead of merely\nserving as an evaluation task.\nConcept Order Recovering (COR).As for the concept order recovering (COR) objective, we\nshufﬂe the order of concept in a sentence and train the model to recover the original sentence. As\nillustrated in Figure 1, given an input sentence “tree grows on the apple,”, the models would shufﬂe the\nconcepts including “tree”, “grow”, and “apple” to recover the original sentence “apple grows on the\ntree.” The noise introduced by concept shufﬂing is different from that by traditional self-supervised\nobjectives like mask language modeling and mask span prediction because the corrupted source\nsentences are in general complete (i.e., no tokens or spans are masked) and grammatically correct,\nwhile not acceptable in terms of commonsense because the order and relation between concepts are\nshufﬂed. By training the model to detect and correct the disorder of concepts in a sentence, the model\nis expected to acquire some relational commonsense knowledge like “apple generally grows on a\ntree” instead of “tree grows on an apple.”\nFormally, the COR objective can be formulated as:\nLcor = E\n( n∑\ni=1\n−log p(xi|<cor>; CONCEPT -PERMUTE (x,C); x1:i−1)\n)\n, (2)\n2We split the concepts with multiple tokens (under Spacy tokenization) into single token to ensure the\nconcepts discussed afterwards all contain a single token.\n3\nPreprint. Work in progress.\nwhere <cor> is the preﬁx for the COR objective illustrated in Figure 1. The function CONCEPT -\nPERMUTE () permutes the order between concepts in the same category (i.e. noun or verb) in the\nsentence, which can be formally deﬁned as:\nCONCEPT -PERMUTE (x,C) = [x′\n1,x′\n2,...,x ′\nn] where x′\ni =\n\n\n\nxi xi /∈C\nPERMUTE (Cv)[j] xi = vj\nPERMUTE (Cn)[j] xi = nj\n(3)\nOur proposed objectives require the model to capture the relational commonsense knowledge between\nconcepts and perform relational (COR) and compositional (C2S) commonsense reasoning in order to\nsuccessfully reconstruct the original sentence. Therefore, the model is encouraged to acquire concept-\ncentric commonsense knowledge more effectively. In contrast, conventional pre-training objectives\nlike masked language modeling and masked span inﬁlling mainly focus on general token-level\nco-occurrence patterns and thus are less effective for learning commonsense knowledge.\n2.2 C ONTRASTIVE OBJECTIVE\nGenerative QAInput:<cont> Which sentence is correct?: options:1.The increased numberof male visitors inspiredby the articleraisedsecurity concerns2.The increased articleof male visitors raisedby the numberinspiredsecurity concerns\nOutput: The increased numberof male visitors inspiredby the articleraisedsecurity concerns\ndiscriminative common sense \nText-to-Text Transformer\nFigure 2: Overview of Contrastive self-supervised\npre-training objectives. Generative QA style con-\ntrastive objective requires the model to distinguish\ntruth sentences from less plausible ones.\nThe contrastive objective encourages the pre-\ntrained model to distinguish the real sen-\ntence from a distractor sentence: a sentence\nthat is similar to the real sentence, gener-\nally grammatically correct, but may not fol-\nlow common sense. We expect it to im-\nprove the pre-trained model’s discrimina-\ntive commonsense reasoning ability so that\nthe model’s performance on commonsense-\nreasoning-discriminative tasks, like Com-\nmonsenseQA, can be improved. We formu-\nlate the contrastive objective as aGenerative\nQA task: we take the concatenation of a pre-\nﬁx <cont> (question / context), the real sen-\ntence x(answer), and the distractor x′(dis-\ntractor) as the input and train the model to\noutput the real sentence x. Formally, we have\nthe loss function of the contrastive objective\ndeﬁned as:\nLcont = E\n(\n−log p(x|<cont>; PERMUTE (x; x′))\n)\n, (4)\nwhere the preﬁx <cont> is described in Figure 2. The distractor x′is either constructed by concept\nshufﬂing as described previously (i.e. x′= CONCEPT -PERMUTE (x,C)) when used independently, or\ngenerated by a generator trained with the aforementioned generative objectives when used in the joint\ntraining framework, which will be described in the next section.\n3 J OINT TRAINING WITH GENERATIVE AND CONTRASTIVE OBJECTIVES\nAlgorithm 1: Pre-training Concept-\nAware Language Model (CALM).\nInput: Text-to-Text TransformerTθ, Text\ncorpus X=[x1, x2,. . . ,xn].\nrepeat\nfor each xi ∈ X do\nExtract the concept-set Ci;\nConstruct the distractor sentence\nx′= CONCEPT -PERMUTE (xi, Ci);\nUpdate Tθ with Eq.(1, 2, 4);\nuntil maximum iterations reached;\nrepeat\nfor each xi ∈ X do\nUpdate Tθ with Eq.(7)\nuntil maximum iterations reached;\nThe aforementioned generative and contrastive self-\nsupervised objectives can be applied independently\nor simply combined in a multi-task learning fashion.\nWe argue that these two objectives can mutually re-\ninforce each other: the generated sentences from the\ngenerative objective can help the contrastive mod-\nule learn to distinguish commonsense sentences from\nless plausible ones.\nTherefore, we propose a joint training framework\nto unify generative objectives and contrastive objec-\ntives by using the generator to produce distractors\nfor learning towards contrastive objective.\nSpeciﬁcally, we have a generator Gθ (trained with\nthe generative objectives) and a discriminator Dφ\n4\nPreprint. Work in progress.\nOriginal Sentence x\nShe was the first woman to hold the position Discriminator (Text-to-Text Transformer)\nWeight Sharing\nExtracting Concept Set C\n<cor> She was the first position to hold the woman  \n<c2s> Hold Woman Position\nCONCEPT-PERMUTE(x,C)\nPERMUTE(C)\nDistractor Sentence 1\nDistractor Sentence 2\nShe was the first woman to position the hold\nWoman holds the position\nGenerator\n(Text-to-Text Transformer)\nC2S\nCOR\nOriginal Sentence x\nDistractor Sentence  \nGenerative QA\nWoman holds the position\nShe was the first woman to hold the position\nShe was the first woman to hold the position\nFigure 3: Proposed Joint Training Framework.Given an input sentence x(“She was the ﬁrst\nwoman to hold the position.”), we extract concept-set C(woman, hold, position). Given xand C,\nwe produce corrupted source sequence x′either for C2S and COR. The generator trained with the\ncorresponding objective recovers sentences as distractors x′′to the discriminator. The discriminator\nis trained to distinguish truth sentences from randomly selected distractor among two objectives.\nParameters between the generator and discriminator are shared.\n(trained with the contrastive objective). Given an input sentence x, we ﬁrst use the method for either\nC2S or COR to produce the corrupted source sequence x′. Then, we use the generator Gθ trained\nwith the corresponding objective to generate the recovered sentence x′′= Gθ(x′). We then take x′′\nas the distractor to train the discriminator Dφ with the contrastive objective. The loss function of\nthe proposed joint training framework consists of two parts: the ﬁrst part is the loss of generative\nobjectives, which is identical to the loss described in Eq.(1) and Eq.(2) and is used to update the\ngenerator Gθ. The second part is the loss of the contrastive objective as described in Eq.(4), which\ncan be formulated as:\nLcont_joint_c2s = E\n(\n−log Dφ(y|<cont>; x; Gθ(<c2s>; PERMUTE (C))\n)\n(5)\nLcont_joint_cor = E\n(\n−log Dφ(y|<cont>; x; Gθ(<cor>; CONCEPT -PERMUTE (x,C))\n)\n(6)\nwhere Lcont_joint_c2s and Lcont_joint_cor is the contrastive loss with the distractor generated with\neither the C2S or the COR objective and yis the original sentence. We then have the overall objective\nfor the joint training framework deﬁned as :\nLjoint = (Lc2s + Lcor) +β(Lcont_joint_c2s + Lcont_joint_cor). (7)\nLc2s and Lcor are deﬁned in Eq.(1) and Eq.(2) respectively and βis a hyperparameter controlling\nthe relative weight between the generative and contrastive objectives. Note that since we would\nlike to inject both generative and discriminative commonsense reasoning ability into the parameters\nof a single text-to-text transformer, we share the parameters between the generator Gθ and the\ndiscriminator Dφ.\nFinally, we describe the overall procedure to apply the proposed self-supervised objectives and the\njoint training framework on a pre-trained text-to-text transformer. We apply a two-stage training strat-\negy. During the ﬁrst stage, we apply our proposed generative and contrastive objectives individually\non the model in a multi-task learning fashion with different preﬁxes. This provides a good starting\npoint for the second stage where the joint training framework is applied. We summarize the workﬂow\nof our method in Algorithm 1.\n4 E XPERIMENTS\nIn this section, motivated by the observation of Pruksachatkun et al. (2020) that tasks requiring\ncommonsense reasoning ability generally serve as good intermediate task, we test our method in the\nintermediate task transfer setting. Speciﬁcally, we initialize our model with T5-base, a pre-trained\ntext-to-text transformer model, and training the model with our proposed method as intermediate task\nbefore ﬁne-tuning and target downstream tasks. Another reason for adopting this setting is because\nwe expect our method to serve as a “plug-and-play” method that can be applied to any pre-trained\ntext-to-text transformer by simply continually training for a few steps.\nDetails for Pre-training and Fine-tuningCALM is continually pre-trained with our proposed\nself-supervised objectives as intermediate tasks based on the pre-trained T5-base model following\n5\nPreprint. Work in progress.\nthe setting in Pruksachatkun et al. (2020). We randomly sample 500K sentences from the English\nWikipedia corpus3, which is used for pre-training BERT and its variants, as the source dataset for our\nproposed self-supervised objectives which serve as intermediate tasks. We then ﬁne-tune the CALM\non each downstream task individually and report the average performance of three runs with different\nrandom seeds for ﬁne-tuning on each dataset since the performance is sensitive to different random\nseeds. Training details and hyperparameter settings are presented in Appendix A.1 and A.2.\nDatasets We consider ﬁve commonsense benchmark datasets as target tasks. We categorize these\ndatasets into discriminative and generative tasks. Discriminative tasks are classiﬁcation tasks while\ngenerative tasks are text generation tasks. We consider four datasets for discriminative task:Com-\nmonsenseQA (Talmor et al., 2018),OpenbookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020),\naNLI (Bhagavatula et al., 2019) and one dataset for generative task:CommonGEN (Lin et al., 2020).\nDetails on datasets are discussed in Appendix A.3.\nCompared MethodsWe compare our model with following models continually trained with different\nintermediate tasks based on the pre-trained T5-base model: (1) T5-base is the pre-trained T5-base\nmodel without continually training on any intermediate task. (2) T5-base w/ additional epochs\nis continually pre-trained using the original pre-training objective of T5 with additional training\nsteps. The total number of additional training steps is equal to that of our ﬁnal model. (3) T5-base\n+ SSM is continual pre-trained with a variant of the salient span masking objective (Guu et al.,\n2020; Roberts et al., 2020) objective that masks text spans of concepts extracted with POS tagging\ninstead of named entities extracted by a pre-trained NER model, which makes it more focused on\nconcepts. (4) CALM(Generative-Only) is continually pre-trained with the proposed generative\nobjectives including concept-to-sentence generation(C2S) and concept order recovering(COR) as\nintermediate tasks. (5) CALM(Contrastive-Only) is continually pre-trained with the proposed\ncontrastive objective as described in section 2.2 using the distractor generated by concept shufﬂing. (6)\nCALM(Mix-only) is continually pre-trained with both the generative objectives and the contrastive\nobjective, combined with a multi-task learning fashion with identical weights for each objective as the\nintermediate task. (7) CALM (w/o Mix warmup)is continually pre-trained with the joint training\nobjective described in Eq (7) directly from the pre-trained T5-base model. (8) CALM is our main\nmodel trained as described in Algorithm 1. The difference between CALM and CALM (Joint) is that\nthe former is initialized by the CALM(Mix). We also include the performance of the BERT-base\nmodel and two knowledge enhanced PTLMs that have similar architecture to BERT-base.\nEvaluation MetricsFor discriminative tasks, we choose accuracy as our metric following other\nconventional question answering tasks. For generative tasks, we report automated metrics including\nBLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), CIDEr (Vedantam et al., 2015),\nand SPICE (Anderson et al., 2016) following the leaderboard of COMMON GEN (Lin et al., 2020).\nResults for COMMON GEN are on the test set and others are on the ofﬁcial development set. We tune\nthe hyperparameters based on the models’ performance on a in-house split dev set.\n4.1 E XPERIMENTAL RESULTS\nThe result is presented in Table 1. First, we can see that our CALM model consistently and\nsigniﬁcantly (with p-value <0.01) outperforms the backbone T5-base model on all ﬁve datasets by\na margin range from 1.5 to 2.9 accuracy on discriminative tasks and 1.5/0.6 BLEU/SPICE score\non CommonGEN. This is an impressive result since we are only performing intermediate training\non a relatively small dataset for only around 20k updates. It demonstrates the potential of our\nmethod for serving as a “plug-and-play” method for packing more commonsense knowledge into a\npre-trained text-to-text transformer. Table 3 also shows that CALM performs comparably with several\nlarge-size PTLMs like BART, T5-large, and GPT-2 on theCOMMON GEN dataset. The performance\nis worse than KG-BART (Liu et al., 2020), the current state-of-the-art on COMMON GEN, which is a\ncontemporary work that exploits external knowledge bases as additional information, and is based on\na larger backbone(i.e., BART (Lewis et al., 2019)).\nIn addition, we can observe that both the proposed generative and contrastive objective outperforms\nthe backbone T5-base model, as well as its variants that continually pre-trained with the original\nmasked span prediction objective and the concept-speciﬁc salient span masking scheme, when applied\nindependently. Note that we ﬁnd the variant of salient span masking that focuses on concept is not\n3https://dumps.wikimedia.org/enwiki/latest/\n6\nPreprint. Work in progress.\nMethods CSQA OBQA PIQA aNLI CommonGEN\nAccuracy (ofﬁcial dev) BLEU-4 METEOR CIDEr SPICE\nBERT-base 53.08( ±0.16) 57.60(±0.8) 64.86(±0.52) 61.88(±0.56) - - - -ERNIE 54.06( ±0.12) 58.90(±0.9) 66.47(±0.58) 63.04(±0.46) - - - -KnowBERT 53.88( ±0.15) 58.50(±0.8) 66.61(±0.63) 63.18(±0.52) - - - -\nT5-base 61.88( ±0.08) 58.20(±1.0) 68.14(±0.73) 61.10(±0.38) 24.90 31.20 12.99 32.40T5-base + cont. pretraining 61.92(±0.45) 58.10(±0.9) 68.19(±0.77) 61.15(±0.52) 25.10 31.00 13.12 32.40T5-base + SSM 62.08( ±0.41) 58.30(±0.8) 68.27(±0.71) 61.25(±0.51) 25.20 31.20 13.28 32.40\nCALM (Generative-Only) 62.28(±0.36) 58.90(±0.4) 68.91(±0.88) 60.95(±0.46) 25.80 31.20 13.81 32.60CALM (Contrastive-Only) 62.73(±0.41) 59.30(±0.3) 70.67(±0.98) 61.35(±0.06) 25.50 31.20 13.58 32.60CALM (w/o Mix warmup) 62.18(±0.48) 59.00(±0.5) 69.21(±0.57) 61.25(±0.55) 25.80 31.20 13.77 32.60CALM (Mix-only) 63.02(±0.47) 60.40(±0.4) 70.07(±0.98) 62.79(±0.55) 26.00 31.20 13.82 32.80CALM 63.32(±0.35) 60.90(±0.4) 71.01(±0.61) 63.20(±0.52) 26.40 31.40 13.88 33.00\nTable 1: Experimental results on commonsense reasoning datasets.The ﬁrst group of models are\nbaselines. The models in the middle group and last group except the CALM model are trained with\nthe proposed objectives independently and the ﬁnal CALM model is trained by joint training. Best\nmodels are bold and second best ones are underlined within each metric.\nMethods CSQA OBQA PIQA aNLI CommonGEN\nAccuracy (ofﬁcial dev) BLEU-4 METEOR CIDEr SPICE\nBERT-large 57.06( ±0.12) 60.40(±0.6) 67.08(±0.61) 66.75(±0.61) - - - -T5-large 69.81( ±1.02) 61.40(±1.0) 72.19(±1.09) 75.54(±1.22) 28.60 30.10 14.96 31.60CALM-large (Mix-only) 70.26(±0.23) 62.50(±1.0) 73.70(±1.09) 75.99(±1.26) 29.20 31.30 15.24 33.10CALM-large 71.31( ±0.04) 66.00(±1.0) 75.11(±1.65) 77.12(±0.34) 29.50 31.90 15.61 33.20\nRoBERTa-large4 71.81(±0.25) 63.90(±0.8) 76.90(±0.62) 82.35(±0.54) - - - -\nTable 2: Experimental results on large model.Comparison between large models of other PTLMs\nand CALM. Best models are bold and second best ones are underlined within each metric.\nvery effective. We suspect this is because the resulting training data would be somewhat similar to\nthe original text inﬁlling objective because concepts are very common in the corpus and we only train\nfor a few steps. The combination of the generative and contrastive objectives (i.e., CALM(Mix-only))\nyields further improvement upon the model trained independently with either generative or contrastive\nobjectives. Also, we ﬁnd that the CALM model consistently outperforms CALM(Mix), demonstrating\nthe effectiveness of the proposed joint training framework. Applying joint training directly on top\nof a pre-trained model (i.e., CALM(w/o Mix warmup)) does not work very well, demonstrating the\nnecessity of applying mixed training to initialize the model before starting joint training.\nMethods Params CommonGEN\nBLEU-4 METEOR CIDEr SPICE\nGPT-2 (Radford et al., 2019) 774M 21.10 26.20 12.15 25.90UniLM (Dong et al., 2019) 340M 27.70 29.70 14.85 30.20BART (Lewis et al., 2020) 406M 26.30 30.90 13.92 30.60T5-base (Raffel et al., 2019) 220M 16.40 23.00 9.16 22.00T5-large5(Raffel et al., 2019) 770M 28.60 30.10 14.96 31.60KG-BART6(Liu et al., 2020) 406M30.90 32.40 16.83 32.70\nT5-base (our implementation) 220M 24.90 31.20 12.99 32.40CALM-base 220M 26.40 31.40 13.88 33.00CALM-large 774M 29.50 31.90 15.61 33.20\nTable 3: Comparison between PTLMs on CommonGEN.\nAbove baselines are reported number in the leaderboard.\nT5-base(our implementation) uses different hyperparme-\nter setting than that reported in the leaderboard.\nTo further conﬁrm the effectiveness of\nour approach, we also apply our method\nto continually pre-train T5-large with the\nsame data and number of training steps.\nWe then compare the performance of the\nresulting model with that of the original\nT5-large model in Table 10. We ﬁnd that\nboth the proposed training objectives and\nthe joint training framework consistently\nand signiﬁcantly (with p-value < 0.01)\nimprove upon the original T5-large, show-\ning our approach is effective for models\nwith different sizes. Our model also out-\nperforms BERT-large by a large margin.\nHowever, our model performs slightly worse compared to RoBERTa-large. We suspect this is because\nRoBERTa-large is optimized for more steps than T5-large and our CALM-large. This is also observed\nin many other tasks and datasets.\n4.2 P ERFORMANCE ANALYSIS\nAnalysis on Generative objectiveTo investigate the contribution of each generative objective, we\nconduct an ablation study by continually pre-training three models from the same T5-base model with\nC2S, COR, and text inﬁlling, which is the original objective for pre-training T5, as the objective for\nthe intermediate task. We continually pre-train these models for the same number of steps and then\nevaluate their performance by ﬁne-tuning on different target tasks. The result is shown in Table 4.\n7\nPreprint. Work in progress.\nMethods CSQA PIQA CommonGEN\nAccuracy BLEU-4 METEOR CIDEr SPICE\nT5 - Text Inﬁlling 61.92 68.19 25.10 31.00 13.13 32.40CALM - COR62.36 68.77 25.70 31.20 13.65 32.60CALM - C2S 62.24 68.7525.90 31.40 13.94 32.80\n(a) Generative objectives\nMethods CSQA PIQA CommonGEN\nAccuracy BLEU-4 METEOR CIDEr SPICE\nMulti-choice QA 62.21 68.82 25.00 31.20 13.28 32.60True/False 62.24 67.81 25.10 31.20 13.41 32.60Generative QA62.73 70.67 25.50 31.20 13.58 32.60\n(b) Contrastive objectives\nTable 4: Analysis on Contrastive and Generative objectives.Left table shows the performance\non downstream tasks by pre-training with different generative objective (COR, C2S, and original\nobjective for pre-training T5). Right table shows the performance on downstream tasks by pre-training\nwith different task formats of contrastive objective.\nWe can see that both C2S and COR works better than the original masked span inﬁlling objective\non itself. This conﬁrms the effectiveness of our proposed generative objectives on improving the\ncommonsense reasoning ability of pre-trained text-to-text transformers.\nTask Formulation of the Contrastive objectivesFor contrastive objectives, we test three different\ntask formats: Multi-choice QA, Generative QA, and True/False. Multi-choice QA and Generative\nQA takes the concatenation of the real sentence and the distractor. Then, Multi-choice QA output the\nindex of the real sentence following other conventional Multi-choice QA tasks, and Generative QA\noutput the real sentence respectively. True/False takes either the real sentence or the distractor and\ntrain the model to perform a binary classiﬁcation problem of whether the input sentence makes sense.\nThe result is shown in Table 4. We could ﬁnd that the format of Generative QA performs the best.\nWe suspect this is because the Generative QA format is closer to the format used during the original\npre-training stage of the T5 model and the format used for ﬁne-tuning.\n20 40 60 80 100\nPercent of Training Data (%)\n50\n52\n54\n56\n58\n60\n62\n64Accuracy\nCommonsense QA\nT5-base\nCALM(Mix-only)\nCALM\n20 40 60 80 100\nPercent of Training Data (%)\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65Accuracy\nOpenbook QA\nT5-base\nCALM(Mix-only)\nCALM\nFigure 4: Performance of compared models ﬁne-tuned with\ndifferent fraction of the datasets.\nPerformance with fewer training\nexamples To investigate the effec-\ntiveness of our objective in the low-\nresource setting, we explore the per-\nformance of our model and baselines\nﬁne-tuning with different fractions of\nthe training data. From Figure 4, we\ncan see that the performance improve-\nment yielded by our models upon the\nT5-base model is more signiﬁcant in\nthe low-resource regime. This shows\nthat CALM may already pack some\ncommonsense knowledge in its parameters so that it does not require much data for ﬁne-tuning before\nobtaining a good performance. In contrast, the original T5 model requires much data for ﬁne-tuning,\nwhich suggests it may fail to encode much commonsense knowledge and must ﬁt the correlation\npatterns in the downstream datasets to get a good performance.\nComparison of Generated DataTable 5 shows the comparison of generated examples for the\nCOMMON GEN test set between T5-base and CALM. We can see that the sentences generated by\nCALM are generally more acceptable in terms of commonsense plausibility while T5-base sometimes\ngenerates sentences that do not make sense.\nConcept-setT5-base CALM\nGrass, Dog, Ball, Chasea dog is chased by a ball on the grass. dog chasing a ball in the grass.Net, Cast, Boat, Waterﬁshing boat casts a net in the water. ﬁsherman casts a net into the water from a ﬁshing boat.Hole, Tree, Plant, Diga man digs a hole in a tree to plant a new tree . he digs the man digging a hole to plant a tree.Ingredient, Add, Pan, Frya pan ﬁlled with ingredients adds a touch of spice to the fry . add the ingredients to a pan and fry.Water, Hold, Hand, WalkA man holding a hand and walking in the water. A man is holding water. man holding a bottle of water in his hand as he walks down the street.Place, Use, Metal toolA man uses a metal tool to make a piece of metal. woman uses a metal tool to make a piece of jewelry.Hair, Wax, Apply, Removeremove the wax from your hair and apply it to your hair . woman applies wax to her hair and then removes it with a comb.Sidewalk, Dog, Walk, LeashA dog walking on a leash on the sidewalk. dog walking on a sidewalk with a leash.\nTable 5: Comparison of generated sentences with same concept-set.For same concept-set which\nis from CommonGEN test set, we compare generated sentences between T5-base and CALM.\nKnowledge Probing To investigate how much concept-centric knowledge our model pack, we\nconducted two probing methods with our model : Language Model Analysis (LAMA) probe (Petroni\net al., 2019), Knowledge Intensive Language Task (KILT) (Petroni et al., 2020). We summarize the\nresults on Table 6 and Appendix A.4. We could ﬁnd that our model outperforms the baseline.\n8\nPreprint. Work in progress.\nMethods MRR Precision@50 Precision@10 Precision@1\nT5-Base 11.53 38.52 21.60 5.93\nCALM (Mix-only) 11.77 38.93 21.92 6.10\nCALM 12.09 39.69 22.53 6.46\n(a) LAMA probe\nMethods FEVER AY2\nT5-base 76.65 74.97\nCALM (Mix-only) 77.05 76.27\nCALM 77.44 77.24\n(b) KILT task\nTable 6: Experimental results on Knowledge Probing.Left table shows the mean precision on\nLAMA probing task of ConceptNET. Right table shows the performance on Fact checking and Entity\nlinking, which are from KILT task.\n5 R ELATED WORK\nSelf-Supervised Language Representation Pre-Training.Motivated by the fact that words can\nhave different meanings in different contexts, contextual language representation methods (McCann\net al., 2017; Peters et al., 2018) have been developed and shown superior performance on downstream\ntasks compared with static word embeddings Mikolov et al. (2013); Pennington et al. (2014). More\nrecently, large scale language models based on transformer architecture (Vaswani et al., 2017) pre-\ntrained with either mask language modeling objective (Devlin et al., 2018; Liu et al., 2019; Lan et al.,\n2019) or mask span inﬁlling objective (Lewis et al., 2019; Raffel et al., 2019) have been explored\nfurther advanced the state-of-the-art on multiple NLU and NLG tasks. Our method is based on these\ntechniques and we focus on improving the commonsense reasoning ability of pre-trained text-to-text\ntransformers. More recently, Clark et al. (2020) propose a new self-supervised pre-training objective\ncalled Replaced Token Detection (RTD). RTD uses a mask language model like BERT to ﬁll in the\nmask and train a discriminator to predict whether a token is generated or real. This pre-training\nparadigm is related to our proposed joint training framework. Some major differences include that (1)\nOur method employs sentence-level distractors that are in general grammatically correct but not in\nline with commonsense, thus require the model to perform relational commonsense reasoning while\nRTD is a token-level discrimination task and can often be solved with syntactic and shallow semantic\nknowledge (Rosset et al., 2020); (2) Our method uniﬁes generative and contrastive objectives with one\nmodel, which can be applied to both NLU and NLG downstream tasks; and (3) The discriminator in\nour framework is “contrastive”, takes both the real sentence and the distractor as input simultaneously.\nKnowledge-augmented PTLMs.As standard pre-trained language models usually do not explicitly\nmodel knowledge, a number of works have examined the problem of incorporating world knowledge\nwith the PTLMs. Recent work Zhang et al. (2019); Peters et al. (2019); Wang et al. (2020); Liu et al.\n(2020) utilizes an external knowledge base to incorporate entity knowledge with PTLMs; however,\nthese approaches require specialized resources like knowledge bases, which limits the domain they\ncan be applied to. Xiong et al. (2020) proposes WikiLM that encodes world knowledge into the\nparameters of a BERT(Devlin et al., 2018)-like pre-trained model with a novel entity replacement\ndetection objective that incorporates Wikipedia to form distractors. Their approach differs from\nours because it requires an external knowledge base (i.e., Wikipedia) which limits the domain it can\nbe applied, is limited to discriminative pre-training objectives and downstream tasks, and focuses\non world knowledge instead of relational commonsense knowledge. More recently, (Rosset et al.,\n2020) propose KALM, an entity-aware language model with more world knowledge packed into its\nparameters. Their method is restricted to the training of language models instead of masked language\nmodels or text-to-text transformers which can be used for more downstream tasks. Also, all the\naforementioned work mainly focuses on world knowledge of named entities. In contrast, our work\nmainly focuses on commonsense knowledge about quotidian concepts.\n6 C ONCLUSION\nWe propose novel self-supervised strategies that encourage the model to focus on concept-centric\ninformation that is related to commonsense understanding and reasoning instead of simple word co-\nocurrence patterns so that the commonsense learning capability of pre-trained text-to-text transformers\ncan be improved. Despite merely continually pre-trained on a small dataset with only around 20k steps,\nour CALM model consistently outperforms the T5-base model on all commonsense-related datasets,\nand even yields better performance compared with some larger size PTLMs on the COMMON GEN\ndataset. The performance gain is larger when we use fewer examples for ﬁne-tuning on different\ndownstream tasks, indicating that CALM effectively encodes more commonsense knowledge and rely\nless on ﬁtting superﬁcial patterns of datasets compared to traditional pre-trained language models. Our\n9\nPreprint. Work in progress.\nwork suggests that text-to-text models can be pre-trained with better parameter and sample efﬁciency\nby carefully designed self-supervised objectives that focus more on the ability (e.g., commonsense\nreasoning ability) required by target tasks. As for future work, we plan to explore training with our\nmethod with larger text corpora for more steps, as well as the combination of our method and other\nmethods that focus on injecting world knowledge into pre-trained language models.\nREFERENCES\nPeter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional\nimage caption evaluation. In ECCV, 2016.\nSatanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with\nimproved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic\nand Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp. 65–72,\nAnn Arbor, Michigan, June 2005. Association for Computational Linguistics. URL https:\n//www.aclweb.org/anthology/W05-0909.\nChandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman,\nHannah Rashkin, Doug Downey, Scott Wen-tau Yih, and Yejin Choi. Abductive commonsense\nreasoning. arXiv preprint arXiv:1908.05739, 2019.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning\nabout physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artiﬁcial\nIntelligence, 2020.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text\nencoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\nErnest Davis and Gary Marcus. Commonsense reasoning and commonsense knowledge in artiﬁcial\nintelligence. Communications of the ACM, 58:92–103, 08 2015. doi: 10.1145/2701413.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. Uniﬁed language model pre-training for natural language understanding and\ngeneration. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett\n(eds.), Advances in Neural Information Processing Systems 32, pp. 13063–13075. 2019.\nW A Falcon. Pytorch lightning.GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning,\n3, 2019.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020.\nJohannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal, Marc\nSpaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. Robust disambiguation of named\nentities in text. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language\nProcessing, pp. 782–792, Edinburgh, Scotland, UK., July 2011. Association for Computational\nLinguistics. URL https://www.aclweb.org/anthology/D11-1072.\nD. Khashabi, S. Min, T. Khot, A. Sabhwaral, O. Tafjord, P. Clark, and H. Hajishirzi. Uniﬁedqa:\nCrossing format boundaries with a single qa system. EMNLP - ﬁndings, 2020.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942, 2019.\n10\nPreprint. Work in progress.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461,\n2019.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehension. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics , Online, July 2020. Association for\nComputational Linguistics.\nShiyang Li, Jianshu Chen, and Dian Yu. Teaching pretrained models with commonsense reasoning:\nA preliminary kb-based approach. arXiv preprint arXiv:1909.09743, 2019.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and\nXiang Ren. Commongen: A constrained text generation challenge for generative commonsense\nreasoning. Findings of EMNLP, 2020. to appear.\nYe Liu, Yao Wan, Lifang He, Hao Peng, and Philip S. Yu. Kg-bart: Knowledge graph-augmented\nbart for generative commonsense reasoning, 2020.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:\nContextualized word vectors. In Advances in Neural Information Processing Systems, pp. 6294–\n6305, 2017.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,\n2018.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations\nof words and phrases and their compositionality. In Advances in neural information processing\nsystems, pp. 3111–3119, 2013.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for\nComputational Linguistics, pp. 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association\nfor Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://www.aclweb.\norg/anthology/P02-1040.\nJeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP), pp. 1532–1543, 2014.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365,\n2018.\nMatthew E Peters, Mark Neumann, Robert L Logan IV , Roy Schwartz, Vidur Joshi, Sameer Singh,\nand Noah A Smith. Knowledge enhanced contextual word representations. arXiv preprint\narXiv:1909.04164, 2019.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463–2473, Hong Kong,\nChina, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250.\nURL https://www.aclweb.org/anthology/D19-1250.\n11\nPreprint. Work in progress.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vassilis Plachouras, Tim Rocktäschel, et al. Kilt: a benchmark for\nknowledge intensive language tasks. arXiv preprint arXiv:2009.02252, 2020.\nYada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe\nPang, Clara Vania, Katharina Kann, and Samuel R Bowman. Intermediate-task transfer learning\nwith pretrained models for natural language understanding: When and why does it work? arXiv\npreprint arXiv:2005.00628, 2020.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\nparameters of a language model? arXiv preprint arXiv:2002.08910, 2020.\nCorby Rosset, Chenyan Xiong, Minh Phan, Xia Song, Paul Bennett, and Saurabh Tiwary. Knowledge-\naware language model pretraining. arXiv preprint arXiv:2007.00655, 2020.\nRobyn Speer and Catherine Havasi. Representing general relational knowledge in ConceptNet 5.\nIn Proceedings of the Eighth International Conference on Language Resources and Evaluation\n(LREC’12), pp. 3679–3686, Istanbul, Turkey, May 2012. European Language Resources Associ-\nation (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2012/pdf/\n1072_Paper.pdf.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question\nanswering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\nlarge-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Papers), pp. 809–819, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\nhttps://www.aclweb.org/anthology/N18-1074.\nErik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In Proceedings of the Seventh Conference on\nNatural Language Learning at HLT-NAACL 2003, pp. 142–147, 2003. URL https://www.\naclweb.org/anthology/W03-0419.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 4566–4575, 2015.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Cuihong Cao, Daxin Jiang,\nMing Zhou, et al. K-adapter: Infusing knowledge into pre-trained models with adapters. arXiv\npreprint arXiv:2002.01808, 2020.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-of-the-art\nnatural language processing. ArXiv, abs/1910.03771, 2019.\nWenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language model. InInternational Conference on Learning\nRepresentations, 2020. URL https://openreview.net/forum?id=BJlzm64tDH.\n12\nPreprint. Work in progress.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced\nlanguage representation with informative entities. arXiv preprint arXiv:1905.07129, 2019.\n13\nPreprint. Work in progress.\nA A PPENDIX\nA.1 P RE-TRAINING DETAILS\nThe following details apply to both base architecture and joint-training architecture. We implement\nour pre-train models using Pytorch-lightning (Falcon, 2019) and Hugginface’s Pytorch Transform-\ners (Wolf et al., 2019). For pre-training phase, we use the Adam optimizer with maximum sequence\nlength 256, train batch size 8, gradient accumulation 8, warmup steps 10000, weight decay 0.01 and\nadam epsilon 1e-6. We train the models with 8 V100 GPUs and FP32 precision for 17 hours. The\nmodel is pre-trained for at most 3 epochs to prevent overﬁtting. We searched for the best learning\nrate for our model out of [1e-4, 2e-5, 2e-6, 5e-7].\nA.2 F INE -TUNING DETAILS\nFor ﬁne-tuning, we use 4 V100 GPUs and use FP32. For all discriminative tasks, we use the Adam\noptimizer with maximum sequence length 256, batch size 4 and gradient accumulation 16. For\ngenerative task, we use the Adam optimizer with maximum source length 32, maximum target length\n32, batch size 8, gradient accumulation 16. For all tasks, we use warmup fraction 0.01. Learning\nrates and train epochs are listed in Table 7.\nHyperparameter CommonsenseQA OpenbookQA PIQA aNLI CommonGEN\nLearning rate [1e-4, 2e-4, 3e-4] [5e-5, 1e-4, 2e-4, 3e-4] [1e-4, 2e-4, 3e-4] [2e-5, 3e-5] [2e-5]\nTrain Epochs 20 20 20 10 20\nTable 7: Fine-tuning hyperparameters.\nA.3 D ATASET PROPERTIES\n• CommonsenseQA (Talmor et al., 2018) is a multiple-choice question answering task, which\npicks the most appropriate answer on general commonsense questions.\n• OpenbookQA (Mihaylov et al., 2018) is a multiple-choice question answering task, which\nis modeled after open book exams on elementary-level core science questions. The task\nrequires open book fact and additional commonsense which is not contained in the book. To\ntest the commonsense reasoning ability, we do not use open book fact.\n• PIQA (Bisk et al., 2020) is multiple-choice question answering task, which chooses the\nmost appropriate solution for physical commonsense questions.\n• aNLI (Bhagavatula et al., 2019) is a binary-classiﬁcation task, which picks the most plausible\nexplanatory hypothesis given two observations from narrative contexts.\n• CommonGEN (Lin et al., 2020) is a constrained text generation task, which generates a\ncoherent sentence describing an everyday scenario using common concepts.\nDataset Train Development Test Source Example Target Example\nCommonsenseQA 9,741 1,221 1,140context:What home entertainment equipment requires cable?options: 1:radio shack2:substation3:cabinet4:television5:desk 4\nOpenbookQA 4,957 500 500 context:You can make a telescope withoptions: 1:straw2:glass3:candle4:mailing tube 2\nPIQA 16,113 1,838 3,084 context:When boiling butter, when it’s ready, you canoptions: 1:Pour it onto a plate2:Pour it into a jar 2\naNLI 169,654 1,532 3,040 context:It was my birthday. When I got home the party was set up for my brother.options: 1:I was so excited.2:I was so mad. 2\nCommonGEN 67,389 4,018 6,042 generate a sentence with these concepts:Apple Grow Tree Apple grows on thetree\nTable 8: Properties of Commonsense benchmark datasets.\n14\nPreprint. Work in progress.\nA.4 K NOWLEDGE PROBING\nLAMA probe is consisting of a set of knowledge sources, each comprised of a set of fact. It deﬁnes\nthat a pre-trained language model knows a fact (subject, relation, object) such as (Bird, CapableOf,\nFly) if it can predict masked objects in cloze statement such as \"Birds can [MASK]\". For evaluation,\nwe ﬁrst ﬁltered out examples that mask label is not in vocabulary list of T5. Then, we evaluate\nthe model based on how highly it ranks the ground truth token against every other word in a ﬁxed\nvocabulary list of T5, and get mean precision at k to check whether the object is ranked among the\ntop k results. We summarize the results of ConceptNet (Speer & Havasi, 2012) in Table 6. Unlike\nother language models which are optimised to masked word anywhere in a given sequence, T5 is\ntrained with different denoising method. It might cause low performance on such slot ﬁlling task, but\ncompared to T5, our model shows better performance compared to base model.\nKILT task is a benchmark for assessing models that need to access speciﬁc knowledge in a deﬁned\nsnapshot of Wikipedia to solve tasks spanning ﬁve domains. The goal is to analyze the model whether\nit has task-agnostic representations of knowledge. We test our model on domain of fact checking,\nentity linking. Fact checking veriﬁes textual claims against textual sources. For this task, we use\nFEVER (Thorne et al., 2018) which is a large dataset for claim veracity that requires evidence from\nmultiple Wikipedia pages to determine whether the claim is supported or refuted. Entity Linking\nassigns Wikipedia page to entities mentioned in text. We use AIDA CoNLL-Y AGO (AY2) (Hoffart\net al., 2011) which supplements the CoNLL 2003 (Tjong Kim Sang & De Meulder, 2003) with\nWikipedia URL annotations for all entities.\nA.5 E XPERIMENTS WITH BART AS BACKBONE\nTo show that our approach is versatile to different pre-trained models, we conduct experiments with\nBART as the backbone model. We can see that our approach consistently and signiﬁcantly (with\np-value < 0.01) improves BART-base on all datasets. This result shows that our method is versatile to\ndifferent pre-trained models.\nMethods CSQA OBQA PIQA aNLI CommonGEN\nAccuracy (ofﬁcial dev) BLEU-4 METEOR CIDEr SPICE\nBART-base (Mix-only) 56.31(±0.28) 58.30(±1.1) 67.53(±1.01) 59.85(±1.14) 25.10 29.50 13.16 30.20CALM (BART-base)58.22(±0.21) 59.10(±1.0) 69.40(±1.23) 61.28(±0.30) 26.40 29.90 13.71 31.10\nTable 9: Experimental results with BART as backbone model.Best models are bold.\nA.6 E XPERIMENTS WITH NOUN /VERB AS CONCEPTS\nWe also conducted an ablation study about the choice of using either nouns or verbs as concepts. We\ncan see that using either nouns-only or verbs-only as concepts for our approach leads to substantial\nperformance drop. This supports our choice about using both nouns and verbs as concepts.\nMethods CSQA OBQA PIQA aNLI CommonGEN\nAccuracy (ofﬁcial dev) BLEU-4 METEOR CIDEr SPICE\nCALM 63.32(±0.35) 60.90(±0.4) 71.01(±0.61) 63.20(±0.52) 26.40 31.40 13.88 33.00\nCALM-nouns 62.45(±0.42) 59.40(±0.5) 69.05(±0.70) 61.55(±0.58) 25.70 31.20 13.17 32.60\nCALM-verbs 62.51(±0.47) 59.10(±0.7) 69.24(±0.65) 61.40(±0.51) 25.60 31.20 13.24 32.60\nTable 10: Experimental results with Noun/Verb as Concepts.Best models are bold.\nA.7 H UMAN EVALUATION ON COMMON GEN GENERATIONS\nWe conducted a human evaluation of CommonGEN predictions between T5 and CALM. We asked\nthree annotators to choose the most reasonable sentence between T5-base and CALM-base predictions.\nThe evaluation was conducted on 50 test sentences in binary selection by majority voting. Cohen’s\nKappa score, which is a measurement of inter-annotator agreement, was 0.73. Annotators say that for\n60% of test sentences, CALM-base generated better.\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7624180316925049
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.701617419719696
    },
    {
      "name": "Transformer",
      "score": 0.6628706455230713
    },
    {
      "name": "Generative grammar",
      "score": 0.6616515517234802
    },
    {
      "name": "Natural language understanding",
      "score": 0.5960279107093811
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.5714919567108154
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5595970153808594
    },
    {
      "name": "Natural language processing",
      "score": 0.5561556816101074
    },
    {
      "name": "Natural language",
      "score": 0.3121738135814667
    },
    {
      "name": "Knowledge base",
      "score": 0.2312035858631134
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 13
}