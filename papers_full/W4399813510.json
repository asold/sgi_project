{
  "title": "Exploring large language models for microstructure evolution in materials",
  "url": "https://openalex.org/W4399813510",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Satpute, Prathamesh",
      "affiliations": [
        "Indian Institute of Technology Roorkee"
      ]
    },
    {
      "id": "https://openalex.org/A2509136203",
      "name": "Tiwari Saurabh",
      "affiliations": [
        "Indian Institute of Technology Roorkee"
      ]
    },
    {
      "id": null,
      "name": "Gupta, Maneet",
      "affiliations": [
        "Indian Institute of Technology Roorkee"
      ]
    },
    {
      "id": "https://openalex.org/A4202113049",
      "name": "Ghosh, Supriyo",
      "affiliations": [
        "Indian Institute of Technology Roorkee"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4319083882",
    "https://openalex.org/W4392296327",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4392737546",
    "https://openalex.org/W4378188762",
    "https://openalex.org/W4384135372",
    "https://openalex.org/W4368367885",
    "https://openalex.org/W4385741486",
    "https://openalex.org/W6849422333",
    "https://openalex.org/W4353034336",
    "https://openalex.org/W4390966669",
    "https://openalex.org/W2097973750",
    "https://openalex.org/W2001176165",
    "https://openalex.org/W2153353817",
    "https://openalex.org/W2107436920",
    "https://openalex.org/W2524556479",
    "https://openalex.org/W2793441420",
    "https://openalex.org/W2046528357",
    "https://openalex.org/W2041001120",
    "https://openalex.org/W1977032047",
    "https://openalex.org/W2102182691",
    "https://openalex.org/W1988888548",
    "https://openalex.org/W2033932779",
    "https://openalex.org/W2617883911",
    "https://openalex.org/W3035270446",
    "https://openalex.org/W4392708643",
    "https://openalex.org/W3202213622",
    "https://openalex.org/W2147536800",
    "https://openalex.org/W2032916735",
    "https://openalex.org/W6646873111",
    "https://openalex.org/W2739227175",
    "https://openalex.org/W2744742504",
    "https://openalex.org/W2062460553",
    "https://openalex.org/W2073276469",
    "https://openalex.org/W2901973189",
    "https://openalex.org/W3031573220",
    "https://openalex.org/W1991418998",
    "https://openalex.org/W2610394307",
    "https://openalex.org/W3105012211",
    "https://openalex.org/W4320009668",
    "https://openalex.org/W1587919209",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W2499905000",
    "https://openalex.org/W2612686989",
    "https://openalex.org/W2501507403",
    "https://openalex.org/W1986784974"
  ],
  "abstract": null,
  "full_text": "Exploring large language models for microstructure evolution in materials\nPrathamesh Satpute∗, Saurabh Tiwari∗, Maneet Gupta, Supriyo Ghosh ∗∗\nDepartment of Metallurgical and Materials Engineering, Indian Institute of Technology, Roorkee, UK 247667, India\nAbstract\nThere is a significant potential for coding skills to transition fully to natural language in the future. In\nthis context, large language models (LLMs) have shown impressive natural language processing abilities\nto generate sophisticated computer code for research tasks in various domains. We report the first study\non the applicability of LLMs to perform computer experiments on microstructure pattern formation in\nmodel materials. In particular, we exploit LLM’s ability to generate code for solving various types of\nphase-field-based partial differential equations (PDEs) that integrate additional physics to model material\nmicrostructures. The results indicate that LLMs have a remarkable capacity to generate multi-physics code\nand can effectively deal with materials microstructure problems up to a certain complexity. However, for\ncomplex multi-physics coupled PDEs for which a detailed understanding of the problem is required, LLMs fail\nto perform the task efficiently, since much more detailed instructions with many iterations of the same query\nare required to generate the desired output. Nonetheless, at their current stage of development and potential\nfuture advancements, LLMs offer a promising outlook for accelerating materials education and research by\nsupporting beginners and experts in their physics-based methodology. We hope this paper will spur further\ninterest to leverage LLMs as a supporting tool in the integrated computational materials engineering (ICME)\napproach to materials modeling and design.\nKeywords: Large language models, Materials science, Phase-field models, Microstructure evolution, Partial\ndifferential equations\n1. Introduction\nLarge language models (LLMs) are a conversational, generative artificial intelligence (AI) tool that offers\nmassive potential for transforming science and society by redefining human-computer interactions [1, 2]. In\nprinciple, LLMs take an input text prompt through a free chat interface and spontaneously, as if by magic,\nwrite a human-like response virtually on any topic. Some of the well-known LLMs are given in Table 1.\nThese models are based on massive machine learning (ML) architectures (specifically transformer neural\n∗These authors contributed equally.\n∗∗Corresponding author.\nEmail address: supriyo.ghosh@mt.iitr.ac.in; gsupriyo2004@gmail.com (Supriyo Ghosh)\nPreprint submitted to Elsevier June 25, 2024\narXiv:2406.15499v1  [cond-mat.mtrl-sci]  19 Jun 2024\nnetworks [3, 4]) pre-trained on a massive corpus of text data from diverse internet sources like Common\nCrawl [5] and Wikipedia [6], each contributing to varying degrees of weightage to different aspects of the\nmodel’s knowledge [7]. This allows LLMs to perform natural language processing (NLP) tasks, including\nhuman-like conversation and contextual understanding. The LLMs continue to learn in a self-supervised\nmanner and correct errors by themselves through the process of reinforcement learning from human feedback\n(RLHF) [8, 9]. Therefore, for a given task, one needs to craft the question prompts more carefully, allowing\nLLMs to be more effective at handling complex tasks and generating better and more reliable answers. It is\neven possible to obtain the desired output with “zero-shot” prompting (i.e, giving no example of how the task\nshould be solved) or typically with a series of prompts or “chain prompting” . In addition, prompt processing,\nsuch as reducing the length of prompts, can result in faster inference times and lower computational costs [10].\nThus, LLMs are increasingly becoming useful to complete research tasks quickly that include generating data,\ncode, models, essays, and literature reviews. However, the performance of LLMs is still not well-tested for\nspecialized research purposes that require domain-specific knowledge. This work explores how far we can\npush LLMs to generate desired output for complex research tasks in an example materials science domain.\nTable 1: Overview of major LLMs. Model size indicates LLM’s complexity and capacity for learning. The token limit refers to\nthe maximum length of input/output text it can handle in a single prompt.\nModel Name Developer Release Date Model Size (Parameters) Token limit\nChatGPT-3.5 [11] OpenAI June 2020 175 billion 4096\nChatGPT-4 (Turbo) [12] OpenAI March 2023 1 trillion 128000\nGrok [13] xAI November 2023 314 billion 8000\nGemini [14] Google AI March 2023 2 billion - 7 billion 32000\nLlama-2 [15] Meta July 2023 7 - 70 billion 2048\nLlama-3 [15] Meta April 2024 8 - 70 billion 8000\nCopilot [16] Microsoft February 2023 Based on GPT-3.5 and 4 4096\nCluade-3 Sonnet [17] Anthropic February 2024 > 20-70 billion 200000\nMistral [18] Mistral AI September 2023 7-12 billion 32000\nAs computational scientists, we are always eager to embrace new AI tools that will save us time and\noffer a great deal of convenience. Among the LLMs (Table 1), ChatGPT (Chat Generative Pre-trained\nTransformer) has garnered significant attention from the public, academia, industry, and media. As we will\nshow later, ChatGPT outperforms other LLMs when it comes to coding and data analysis. With over 175\nbillion ML parameters, its ability to understand/generate text, analyze data, generating code, and even\nrun Python codes sets it apart from other LLMs. The ChatGPT-4, the latest iteration of the GPT model\n(as of April 1, 2024), is even more powerful as it is pre-trained with 1 trillion data set with increased\ntoken/context capacity, dramatically enhancing its performance in solving complex tasks in applications\nreaching diverse fields including mathematics [19–21], coding [9, 21], medicine [22], biology [23], and even\nlaw [24]. Also, the incredible capability of ChatGPT-4 in working with multimodal data (i.e., images, text,\naudio, and video) and conversational interactivity offer a promising outlook for scientific computing. In this\n2\nwork, we use ChatGPT-4 as a representative LLM to explore its knowledge within the realm of computational\nmaterials science, emphasizing its potential benefits efficiently. In the context of materials science, only a few\npublished studies (three as of April 1, 2024) demonstrated applications of ChatGPT, particularly in atomistic\nmaterials modeling for generating/querying different crystal symmetry structures useful for density functional\ntheory calculations [9, 25, 26]. However, in the integrated computational materials engineering (ICME)\nframework, materials models at different length scales are linked together that fully capture material behavior\nin technological applications. In this work, we examine several popular LLMs, including ChatGPT, to\nexplore their understanding of computational materials science at the microscale, particularly the modeling of\nmicrostructure evolution described by complex partial differential equations (PDEs). There have been a few\nprevious studies on the mathematical and coding capabilities of ChatGPT for solving differential equations\nand PDEs [20, 21]. However, these general studies are not explicitly related to materials applications. To our\nknowledge, this is the first attempt to systematically explore ChatGPT as an alternative to microstructure\nevolution modeling.\nOn the micro-to-meso scale, a critical factor in guaranteeing the superior performance of materials is to\nengineer the complex phase organizations and interface structures in the microstructure patterns generated\nduring phase transformations. The phase-field (PF) method [27–30] is the most powerful technique for the\nstudy of the thermodynamics and kinetics of interface migration and the associated microstructural evolution.\nA particular strength of the method is that the sharp interface is modeled by a diffuse interface with a finite\nwidth, which reduces to a set of nonlinear PDEs that are handled by standard numerical techniques (e.g.,\nfinite difference discretization method with explicit time marching scheme). In principle, these PDEs are\nthe equations of motion obtained by minimizing the material’s free energy function with respect to the field\nvariables representing physical or phenomenological microstructure descriptors. Depending on the physics\nbeing modeled, these variables can be conserved (e.g., concentration) and non-conserved (e.g., phase fraction)\norder parameters. In a microstructure, these fields vary smoothly within the diffuse interface between their\n(scalar) values in the adjacent bulk solid and liquid phases. This approach avoids explicit tracking of the\ninterface, and thus, the complex evolution of the interface, as seen in complex geometries and topology\nchanges in materials, can be modeled efficiently. The temporal evolution of the field variables obtained by\nsolving the PDEs represents the microstructure patterns. In this work, we explore the process of ChatGPT\nfor solving several well-know PDEs that are central to materials science. We mainly address the following\nkey questions:\n• Can the computer experiments on microstructure pattern formation be outsourced to LLMs?\n• How do the LLMs impact traditional materials education and research?\nThe structure of the rest of the article is as follows. In Sec. 2, we label four example problems that we\ntest with ChatGPT. In Sec. 3, we present the outputs generated by ChatGPT for all the case studies and\n3\nPrompt: \nMaterials \nPDEs\nFick's \nSecond \nLaw\nGinzburg-Landau \nModel\nCahn-Hilliard \nModel\nKobayashi \nDendrite \nModel\nAI \nchatbot\nMicrostructure \nevolution\nLLMs\nFigure 1: The schematic summarizes our workflow. The artificial intelligence (AI) chatbot provides a user interface to interact\nwith large language models (LLMs) as a “black box” . We input a partial differential equation (PDE) with appropriate initial\nand boundary conditions and material parameters as a prompt in the chatbot of a given LLM like ChatGPT (Table 1). Then, it\ngenerates a Python code to solve the given PDE and produces the output in terms of the time evolution of the microstructural\npattern formation.\ncompare their accuracy against our simulations using in-house codes. In Sec. 4, we summarize and conclude\nwith a detailed outlook on the present topic.\n2. General Approach\nTo examine the capability of LLMs, we select four benchmark phase-field model problems [31, 32] (or\nPDEs) with increasing order of model complexity in terms of additional physics. The first problem we study\nis the diffusion equation [33] (i.e., Fick’s second law); the second example is the Cahn-Hilliard equation [34];\nthe third problem is the coupled Cahn-Hilliard and Allen-Cahn [35] (or Ginzburg-Landau) equations; the\nfourth problem is the dendritic crystal growth in pure systems by Kobayashi [36]. In this way, we focus on\na single, fundamental aspect of physics (diffusion) in the first problem and increase the model complexity\nin terms of additional physics (diffusion and phase separation) in the second example, and so on. We also\nuse simplified formulations of the above systems with dimensionless parameters to make the tests more\nstraightforward to implement in ChatGPT. Also, we use one and two-dimensional simple domain geometries\nwith suitable initial and boundary conditions so that the governing physics can be simulated without making\nthe test problems unreasonably large or computationally demanding. In this way, we will be able to capture\nthe essential physical behavior (e.g., solute diffusion, two-phase growth, coarsening, and solidification) in a\nvast majority of materials and models. A schematic of our workflow is given in Fig. 1.\nThe LLMs, including ChatGPT, can be used as a mathematical tool for solving PDEs [19–21]. When we\ntypically solve a PDE using analytical methods or computer experiments, the solution of the PDE is sought\nunder certain initial and boundary conditions for the given material-specific parameters. Similarly, to solve\na given PDE in ChatGPT, it needs to know the explicit form of the equation, the initial and boundary\nconditions, and the material parameters. Also, the user must specify the numerical implementation scheme\nthe ChatGPT will employ. For example, we use an explicit Euler finite difference algorithm throughout this\nstudy. In a typical PDE solver, the above input parameters and constraints must be specified before the\n4\nPDE can be solved. This is essentially what we input in the prompt to solve PDEs, as shown in Figs. 2-5,\nproviding the equations and parameters formatted in natural language. As a response, ChatGPT produces\nthe output and visualization of the results.\nBesides the AI chatbot provided by several LLMs (Table 2), we also explore Hugging Face’s [37] Hug-\nging Chat interface to test out many excellent ready-to-use LLMs such as Llama-2 [15], Llama-3 [15], and\nMistral [18] to solve material PDEs. In these LLMs, we use the default inference-time model parameters\nsuch as Temperature = 0.6, Top-P = 0.9, and Top-K = 50 as a reference. It is clear that ChatGPT-4\noutperforms other popular LLMs when it comes to generating the correct complete code and the ability to\nexecute it. Therefore, we instruct ChatGPT to do code writing to solve the given PDEs, noting that it can\nwrite and run Python codes. Also, for each task, we implement the identical problem on the client-side and\ncompare the execution results with the ChatGPT outputs. To solve PDEs corresponding to each case study,\nwe enter a text prompt in its AI chatbot, describing the PDEs and parameters to code in Python. Then,\nthe ChatGPT responds with an explanation of the topic, algorithmic description, the Python program, and\nvisualization/analysis of the execution results. To present these results in a concise manner, we here only\nreport the prompt given and the output visualization files. We include the ChatGPT-generated codes for\neach task in the supplementary material.\nTable 2: Summary of the capability of various LLMs (accessed as of March 31, 2024) on the four input prompts correspond to\neach task is presented with Y (Yes) and N (No). We access the latest Llama family model, Llama-3 [15], from Hugging Face [37]\non May 24, 2024.\nLarge Language AI\nModel\nFull Code (Python) Correct Code\n(Task/Prompt 1-4)\nExecution\nAbility/Data\nVisualization\n(Task/Prompt 1-4)\nGPT-3.5 [11] Y Y, N, N, N N, N, N, N\nGPT-4 [12] Y Y, Y, Y, Y Y, Y, Y, Y\nGemini [14] Y Y, N, N, N N, N, N, N\nLlama-2 [15] Y N, N, N, N N, N, N, N\nLlama-3 [15] Y Y, N, N, N N, N, N, N\nCopilot [16] Y Y, Y, N, N N, N, N, N\nClaude-3 [17] Y Y, Y, N, N N, N, N, N\nMistral [18] Y N, N, N, N N, N, N, N\n3. Simulation examples\n3.1. Task 1: Transient solute diffusion equation\nDiffusive phase transformations and associated microstructure evolution involve long-range diffusion.\nTherefore, we begin the first example with a very simple yet well-known PDE, the transient solute diffusion\nequation given by Fick’s second law (mass conservation). In this particular example, we focus on the\n5\naccuracy and efficiency of ChatGPT implementation. We consider the following form of Fick’s second law\nof diffusion [33, 38] in 1D,\n∂c(x,t)\n∂t = D∂2c(x,t)\n∂x2 , (1)\nwhere c(x,t) is the concentration profile at a position x and time t, and D is the diffusivity of solute.\nWe solve on a domain of x ∈[0, 99], assuming the initial concentration as a step function between 0.6\n(24 < x <75) and 0.2 (otherwise) and periodic boundary condition. We use ∆ x = 1 .0, ∆ t = 0 .01,\nD = 1, and Nt = 100000 time steps. We use an iteration scheme using the explicit Euler method in time\nand a second-order central finite difference scheme in space. To solve Eq. (1), we input the PDE and the\nsimulation setup and parameters in the ChatGPT dialogue box using a single prompt, as shown in Fig. 2.\nThen ChatGPT performs the necessary calculations and analysis, that is, describes its understanding of the\nalgorithmic program steps, writes the code in Python, executes the code, and finally generates the output\nplot of c(x,t) (using Matplotlib). We show the given prompt and the ChatGPT output in Fig. 2. When\ncompared with our in-house implementation of the identical problem, ChatGPT produces accurate execution\nresults. Note the use of question prompt in a zero-shot manner and that too formatted in natural language,\ndemonstrating efficiency in the ChatGPT approach.\n3.1.1. General observations\nWe note that ChatGPT works perfectly when the explicit form of the PDE is given, as expected, and\nalso when queried as Fick’s second law. Besides Python, it can support programming in many languages,\nincluding Matlab/Octave, C/C++, Java, and R; however, as per ChatGPT, “it can’t directly compile or\nexecute these codes or visualize the plots in its environment. ” Importantly, it can autonomously learn from\nan incorrect code that it writes and then be able to “correct the code and run the simulation again” by\nitself. It should be noted that for repeated query of the same prompt may generate different code structures.\nThe prompt does not need to be case-sensitive. Importantly, ChatGPT is context-aware, meaning, even if a\nvariable is defined differently at various places in the prompt, it understands the context of the variable and\nconsiders them as the same. It extensively uses NumPy, SciPy, Matplotlib, scikit-learn, and other Python\nlibraries and wrappers as required for coding and data analysis; and it can save the output data and images\nin arbitrary formats. Although not shown here, ChatGPT is able to solve Eq. (1) correctly using the Fourier\nspectral method [39] and implicit time-stepping schemes. Also, it is somewhat capable of modifying the code\nthat can be run on external parallel CPU and GPU clusters using OpenMP [40], MPI [41], and CUDA [42]. As\nwe will show later, ChatGPT can efficiently perform 2D simulations on reasonably-sized domains; however,\nas per ChatGPT, “3D simulations might be constrained to domains with dimensions in the order of hundreds\nto a few thousand grid points along each axis depending on the complexity of the problem. ” Importantly,\nChatGPT allows the user to copy the code to perform large-scale simulations on external systems. It is\n6\nDevelop \na \nPython \ncode \nto \nsimulate \nthe \ndifferential \nequation \ndC/dt=D*d^2C/dx^2 \nusing \nthe \nfinite \ndifference \nmethod \nfor \none-dimension. \nSpecify \nsimulation \nparameters \nas \nfollows:\nN \n= \n100 \nspatial \npoints, \ndelta_t \n= \n0.01, \nD \n= \n1 \ndiffusion \ncoefficient, \ndelta_x \n= \n1, \ninitialize \nC \nwith \nC[i]=0.2 \nif \ni \nbelongs \nto[0,24] \nU \n[75,100] \nand \nC[i]=0.6 \notherwise. \nUse \nfinite \ndifference \nmethod \nwith \nperiodic \nboundary \nconditions. \nSave \nthe \ndata \nfor \neach \niteration. \nRun \nfor \nN_t \n= \n100000 \ntime \nsteps. \nPlot \ninitial,  \nt \n= \n20000 \n, \nt \n= \n50000 \nand \nt \n= \n100000 \ncomposition \nprofiles.\nPrompt\nFigure 2: (Color online) As a first case study (Sec. 3.1), we depict the ChatGPT response generated after entering the given\nprompt in a zero-shot manner directly in its AI chatbot. The time evolution of the composition field generated by ChatGPT\nis compared with excellent accuracy with the in-house implementation of the identical problem. Over time, the initial solute\nconcentration profile (step function) approaches the equilibrium value ( c= 0.4) due to diffusion.\nable to give reasonable material-specific data after a literature review (e.g., D ≈7 ×10−9) when we ask:\n“what is the diffusion coefficient for 3 wt% Cu in Al at 877 K?”, noting that repeating the same query\ngenerated a more accurate answer ( D≈2 ×10−9) [43]. However, it is unable to provide this data for many\nother material systems but recommends correct online resources where to look for to obtain these parameter\nvalues. Also, when asked for more technical details like, “is the above analysis valid when D is a function\nof c?”, it correctly responded as “no” and provided the conceptual modifications required in the simulation\ncode. These are the general observations we had while interacting with ChatGPT; hence, for the sake of\nbrevity, we do not repeat the above queries for other case studies henceforth. However, depending on the\ncomplexity of the PDE requested to solve by the user, the above specific observations may vary from task\nto task.\n7\n3.2. Task 2: Cahn-Hilliard equation\nThe second case study corresponds to material problems involving long-range diffusion, as seen in mi-\ncrostructure evolution via spinodal decomposition or phase separation in general [44–46]. We use the Cahn-\nHilliard PDE [34] (hyper-diffusion equation with fourth-order derivatives of c) to simulate spinodal decom-\nposition in a binary alloy of two-phase systems,\n∂c(r,t)\n∂t = ∇·M∇\n(∂f(c)\n∂c −κc∇2c\n)\n, (2)\nwhere ris the position, M is the mobility, f is the chemical free energy of the system, and the second term is\nthe interfacial free energy of the diffuse interface, in which κc is the gradient energy coefficient. Equation (2)\nis based on the total free energy of the given material given by,\nF(c,∇c) =\n∫\nv\n[\nf(c) + κc\n2 |∇c|2\n]\ndv, (3)\nwhich is the starting point for most phase-field models [46, 47], but there can be additional terms due\nto the coupling of the additional physics, as we will explore this in subsequent case studies. We consider\nf(c) = Acc2(1 −c)2, a double-well potential whose wells define the phases. We solve Eq. (2) on a 128 ×128\ndomain with ∆ x = ∆y = 1.0, ∆ t = 0.1, κc = 2, M = 0.1, Ac = 1, Nt = 40000, and periodic boundary\nconditions in all directions. We assume a random noise of variance ±0.01 that is added to the initial matrix\ncomposition (c0 = 0.5). We implement with the explicit Euler finite-difference algorithm.\nThe corresponding prompt and the ChatGPT output are shown in Fig. 3. The general observations are\nas follows. With time, the initial binary matrix phase separates into A-rich and B-rich phases, followed by\nthe coarsening of the phases (i.e., Ostwald ripening) [48, 49]. The phase separation and coarsening processes\nare direct consequences of the reduction in total free energy of the system (right subpanel in Fig. 3) owing\nto a significant reduction in the amount of interfaces in the microstructure [50]. Although not shown here,\nwe confirm the accuracy of the output against our in-house code simulation of the identical problem.\nNext, we test the capability of ChatGPT to perform parameter studies. For reference, the parameter\nk controls the interfacial energy between different phases in the system, hence critical for phase separation\ndynamics. We ask ChatGPT to run Cahn-Hilliard simulation (Eq. (2)) for three different k values, as\ndemonstrated in the prompt given in Fig. 3. For each value of k, ChatGPT calculates the microstructure\nevolution over time and the corresponding reduction in the free energy of the system. ChatGPT clearly\ncaptures the effect of increasing value of k: compared to systems with small k, large k values lead to faster\ndiffusion and more diffuse interfaces, which promote faster coarsening of phases to reduce the total free energy\nof the system more rapidly. Since such a general parameter study can be performed in a straightforward\nmanner by providing different values of the parameter of choice in the prompt, henceforth, we do not repeat\n8\nsuch a study for simplicity.\nDue to severe time step restrictions (due to the ∇4c term in Eq. (2)), ChatGPT is unable to work\non complex domain geometries, including 3D and non-rectangular 2D, and even on square geometry for\nlarge-scale simulations. When we ask it to simulate for more time steps, it responded, “it took too long\nand exceeded the time limit for processing. ” Also, when asked for materials-specific values for κc and M, it\nprovided general values, κc ≈10−11 to 10−9 J/m and M ≈10−9 to 10−12 m2/s, but not for any specific alloy,\nlikely due to lack of domain-specific knowledge in the training data set. We note that repeated queries of the\nsame prompt may be needed to produce the desired results and plots. Further, when we ask ChatGPT about\na causal reasoning question like “what is the relationship between k and equilibrium”, it correctly responds:\n“a larger k value increases the energy penalty for sharp concentration gradients, which drives the system\nto smooth out these gradients more rapidly and thus leads to a faster approach to equilibrium. ” Finally,\nthe ChatGPT code can be run on the client-side system to simulate large-scale, long-time microstructure\nevolution process with complex domain geometries.\n3.3. Task 3: Coupled Cahn-Hilliard and Ginzburg-Landau or Allen-Cahn equations\nNext, we couple generalized PDEs of Ginzburg-Landau or Allen-Cahn [35] and Cahn-Hilliard [34] type to\nstudy the coupled physics of conserved (concentration:c) and non-conserved (order parameter: ϕ) parameters\nduring microstructure evolution. The general approach could be used to model curvature-driven problems\n(e.g., grain growth and coarsening) or order-disorder transformations [51]. The corresponding PDE for c is\ngiven by Eq. (2); and for ϕ we consider,\n∂ϕ(r,t)\n∂t = −L\n(∂f\n∂ϕ −κϕ∇2ϕ\n)\n, (4)\nwhere L is a mobility parameter, f is a double-well free energy potential, and κϕ is the gradient energy\ncoefficient. For simplicity, we use f(c,ϕ) = Acc2(1 −c)2 + Aϕϕ2(1 −ϕ)2 that couples ϕ and c. In terms of\nsolution complexity, the coupled system contains a ∇2ϕterm in addition to the usual ∇2cand ∇4cterms in\nEq. (2). In this case, the free energy of the material is given by,\nF(c,ϕ, ∇c,∇ϕ) =\n∫\nv\nf(c,ϕ) + κc\n2 |∇c|2 + κϕ\n2 |∇ϕ|2 dv, (5)\nwhich makes the starting point for many advanced phase-field models [52, 53].\nWe solve coupled PDEs, Eq. (2) and (4), on a 128 ×128 domain with ∆ x = ∆ y = 1 .0, ∆ t = 0 .01,\nκc = κϕ = 2, M = 0.1, L = 0.1, Ac = 1, Aϕ = 2, Nt = 20000, and periodic boundary conditions in all\ndirections. We assume an initial circular grain (radius of 20 grids, ϕ= 1 and c= 1) embedded into a large\nsecond grain (ϕ= 0 and c= 0). We implement the system with an explicit Euler finite-difference algorithm.\n9\nProgram \nin \npython  \nto \nsimulate \nthe \nPDE: \ndc/dt=del(M*del(df/dc-k*del^2(c))) \nusing \nexplicit \nfinite \ndifference \nmethod. \nAssume \na \nuniform \nconcentration \nprofile \nof \n0.5 \nin \n128x128 \nmatrix \nwith \nrandom \nvariation \nof \n0.01 \n. \nAssume \nf \n= \nAc^2(1-c)^2. \nnx, \nny \n= \n128,128; \ndx, \ndy \n= \n1.0, \n1.0; \nNt \n= \n40000; \ndt \n= \n0.1; \nM \n= \n0.1; \nc0 \n= \n0.5; \nA \n=1.0; \nk \n= \n2. \nAssume \nperiodic \nboundary \nconditions.\nPlot \ninitial, \none \nintermediate \nand \nfinal \nconcentration \nprofiles. \nAlso \nplot \nfree \nenergy \nover \ntime.  \nGet \nthe \nsolution \nby \nderiving \nthe \nnecessary \ncomponents \nfirst \nbefore \nimplementing \nit \nin \ncode\nPerform \na \nparameter \nstudy \nwith \nk \n= \n0.5, \nk \n= \n1, \nand \nk \n= \n2\n.\nPlot \nall \nthe \nresults \nfor \na \ncomparative \nstudy.\nPrompt\nk \n= \n0.5\nk \n= \n1.0\nk \n= \n2.0\nt \n= \n0\nt \n= \n20000\nt \n= \n40000\nFigure 3: (Color online) As a second case study (Sec. 3.2), we depict the ChatGPT response generated after entering the given\nprompt in a zero-shot manner directly in its AI chatbot. The time evolution of the composition field in the simulation box leads\nto phase separation of the binary alloy into A-rich (red) and B-rich (blue) phases. The coarsening of the microstructure phases\nover time (t) is evident. As expected, the total free energy of the system decreases with time (subplots in the right panel). We\nask ChatGPT to perform a parameter study with varying values of the gradient energy coefficient k. The output in terms of\nmicrostructure and free energy evolution over time is shown in the top row for k = 0.5, middle row for k = 1.0, and bottom\nrow for k = 2.0. The interface becomes more diffuse with increasing value of k, promoting faster coarsening of phases to reduce\nthe interfacial (and hence total) free energy of the phase-separating system.\n10\nSince our objective is to test the capability of ChatGPT in solving coupled PDEs with additional physics,\nwe keep the above model parameters non-dimensional and numerically affordable.\nThe corresponding prompt and the ChatGPT response are shown in Fig. 4. The initial interface becomes\ndiffuse with time, and the circular grain continues to shrink over the time of the simulation, as expected.\nThe ChatGPT is able to characterize such shrinkage behavior of the grain following the change in its radius\nwith increasing time, as evident from Fig. 4 (bottom right panel). We observe the identical grain behavior\nusing our in-house code. Note that due to the increased complexity of the coupled system, it takes a\nlonger time for each iteration when compared to that with Eq. (2), thus reducing the timescale limit that\nChatGPT can process (i.e., 40000 in Task 3 vs. 20000 in Task 4). To further speed up the simulations,\nfor example, GPU-parallel resources are required. Unfortunately, as per ChatGPT “the environment here\ndoes not support direct execution of GPU-accelerated code, as it requires access to physical GPU hardware\nand specific software libraries that are not available in this setup. ” Also, when asked to “fit the area\nfraction vs. time curve using a line of best fit and provide the R2 value”, it correctly estimated the linear\nrelationship [54] between the time and area fraction with R2 ≈0.957, demonstrating basic data analysis\nability of ChatGPT “on-the-fly. ” Furthermore, as a query for domain knowledge, “why is the area fraction\ndecreasing with time?”, it correctly recognized the critical role of curvature-driven interface migration and\nthe Gibbs-Thomson effect [38]. We note that for more complex model formulations and parameters, such as\npolycrystalline microstructure evolution due to grain growth [55], ChatGPT should be able to generate the\ncode, which can then be executed with external software.\nWe must admit that without specific numerical instructions, ChatGPT and LLMs in general produce\ndifferent code structures with different numerical solution schemes (in terms of different Python functions,\noptimizations, array loop structures, boundary condition implementations, etc.). Hence, the stability and\naccuracy of the resulting code will differ for each output generation. Consequently, the quantitative details of\nthe output will vary, although the simulation code may be correct, as we note in this particular case. While\nsimulating the same problem using our in-house code, we observe aR2 ≈0.923 compared to ChatGPT output\nwith R2 ≈0.957. Such differences in the quantitative measure of the simulation results vary depending on\nthe specific implementation schemes ChatGPT uses to produce the output, noting that the solution schemes\nmay differ upon repeated user requests even for solving the same problem.\n3.4. Task 4: Solidification and dendritic growth\nIn materials solidification, the most common microstructures are made of dendrite crystals [56, 57].\nCompared to previous case studies, this case study involves additional physics, including anisotropic diffusion,\ninterfacial energy, and solidification, among others. We use the famous Kobayashi [36] phase-field formulation\n(based on Ginzburg-Landau or Allen-Cahn Eq. (4)) to simulate dendritic growth in a pure material, noting\nthat this model makes the basis for more complex dendrite models derived later [58, 59]. We solve two\n11\nDevelop \na \nPython \nprogram \nto \nsimulate \nthe \ncoupled \nequations \n\"dc/dt \n= \ndel(M \n* \ndel(df/dc \n- \nkc* \ndel^2(c)))\" \nand \n\"dphi/dt=-L*(df/dphi-kp*del^2(phi))\" \nusing \nexplicit \nfinite \ndifference \nmethod. \nCalculate \nthe \ntime \nevolution \nof \na \ncircular \ngrain \nof \n20 \nunits \nradius \nin \na \n128x128 \ndomain; \nwith \ninitial \nphi \nand \nc \nare \n1 \ninside \ngrain \nand \nboth \nare \n0 \noutside \ngrain. \n\"f \n= \nA1*c^2(1-c)^2 \n+ \nA2*phi^2(1-phi)^2 \n\". \nAssume \nperiodic \nboundary \nconditions. \nUse \nthe \nfollowing \nconstants: \nnx, \nny \n= \n128, \n128; \ndx, \ndy \n= \n1.0, \n1.0; \nNt \n= \n20000; \ndt \n= \n0.01; \nM \n= \n0.1; \nL= \n0.1; \nkc \n= \nkp \n= \n2; \nA1=1.0, \nA2=2.0. \nPlot \nthe \ninitial, \n2 \nintermediate, \nand \nfinal \nphi \nprofiles. \nAlso, \nplot \nthe \narea \nfraction \nof \nthe \ncircular \ngrain \nwith \ntime. \nPlot \nonly \nfor \nt \n= \n0, \nt \n= \n10000, \nand \nt \n= \n20000. \nIn \nthe \narea \nfraction \nvs \ntime, \ncan \nyou \nuse \nthe \nline \nof \nbest \nfit \nto \nthe \ndata \nand \ngive \nthe \nR^2 \nvalue.\nPrompt\nFigure 4: (Color online) As a third case study (Sec. 3.3), we depict the ChatGPT response generated after entering the given\nprompt in a zero-shot manner directly in its AI chatbot. For reference, we only show the time evolution of the ϕ field. The\ncircular grain shrinks over time due to the curvature-induced diffusion effects. The area fraction of this grain decreases linearly\nwith time, with the R2 of a line of best fit ≈ 0.957.\ncoupled PDEs describing the time evolution of phase-field ϕ and temperature T as,\nτ∂ϕ\n∂t = ∂\n∂y\n(\nϵ∂ϵ\n∂θ\n∂ϕ\n∂x\n)\n− ∂\n∂x\n(\nϵ∂ϵ\n∂θ\n∂ϕ\n∂y)\n)\n+ ∇·\n(\nϵ2∇ϕ\n)\n+ ϕ(1 −ϕ)(ϕ−1\n2 + α\nπ arctan [γ(Tm −T)]), (6)\nand\n∂T\n∂t = ∇2T + K∂ϕ\n∂t. (7)\nHere, τ is the relaxation time; ϵ(θ) = ¯ϵ[1 + δcos(m(θ−θ0))] is a m-fold crystal anisotropy function with\nθ = arctan\n(\n∂ϕ/∂y\n∂ϕ/∂x\n)\nthe interface orientation angle and δ the anisotropy strength; α and γ are positive\nconstants; Tm is the melting temperature; and K is the latent heat. Although not shown here for brevity, a\nGinzburg-Landau type system free energy (similar to Eq. (5)) is assumed to obtain Eqs. (6) and (7).\nWe solve coupled PDEs, Eqs. (6) and (7), assuming a solid ( ϕ = 1) circular seed of radius of 5 grid\nunits at the center of a 2D 200 ×200 domain filled with liquid ( ϕ = 0). We use ∆ x = ∆y = 0.03, T = 0,\n∆t = 0.0001, τ = 0.0003, ¯ϵ = 0.01, δ = 0.02, m = 6, θ0 = 0.05, α = 0.9, Tm = 1.0, γ = 10.0, K = 2.0,\nNt = 2500 time steps, and periodic boundary conditions for both ϕ and T in all directions. We use the\nexplicit time marching scheme with a central finite difference method to discretize the PDEs.\nThe corresponding prompt and the ChatGPT response are shown in Fig. 5. After providing the equation\n12\nand parameters in a prompt formatted in natural language to ChatGPT (Prompt 1 in Fig. 5), it generates\nan executable yet logically incorrect code and thus fails to produce the correct output (bottom left subpanel\nin Fig. 5). In particular, it could not handle the derivatives involving anisotropy (first two terms in Eq. (6))\nand thus could not resolve the dependency between these equations via ∂ϕ/∂t in Eq. (7). Therefore, we\nhad to provide more detailed technical instructions (inspired by our in-house code of the same problem)\non how to solve the coupled PDEs. We update the prompt with specific rules on (a) how to calculate the\nderivatives and Laplacian using a five-point stencil; (b) how to work on the explicit forms of the derivatives,\nparticularly, involving with the complex anisotropy terms in Eq. (6); (c) how to code the spatial derivatives\nof anisotropy with respect to θ, and (d) how to explicitly code the iteration loops to correctly calculate\nthe time derivatives. With all the extensive feedback provided via a series of prompt iterations, so-called\n“Chain-of-Thought” (CoT) prompting, ChatGPT was able to learn from these interactions and correct errors\nby itself, eventually generating the correct code (see supplementary material). We present these series of\nprompt iterations in a zero-shot manner in Fig. 5 (Prompt 2). Due to the complex nature of the PDEs and\nsevere time restriction (∆ t= 10−4), processing of the code takes a long time; hence, ChatGPT was unable\nto execute the code in its environment. Thus, we export this code to an external system and execute it\nlocally to obtain the solution output. The output ϕ and T profiles are shown in Fig. 5, where the initial\ncircular solid seed (not shown) grows with time to produce dendritic structures with six primary arms with\nwell-defined side branches.\nIt is evident that several expert-driven iterations and feedback need to be given to solve multi-physics\ncoupled PDEs in ChatGPT. Also, how much information needs to be input to generate accurate code/output\nthat remains a challenge. We note that since LLMs are essentially a text-completer [3], the user should input\nnearly all the intricate code tasking details, as if like “forcing” the model to generate the desired output,\nas evident in this particular case. Nevertheless, this approach will almost take away the convenience of the\nChatGPT approach, as seen in previous case studies (Sec. 3.1-3.3). Thus, it will be more prudent to write the\ncoding task from scratch or build it on top of the code base provided by ChatGPT for a better investment\nof time and skills.\n4. Summary and Outlook\nIn this work, we test the capability of ChatGPT to solve several well-known PDEs central to materials\nscience. The solution of these PDEs, under appropriate simulation setup and parameters, provides the\nspatial and temporal dependence of the microstructure variables in metallic alloys. To this purpose, we\nuse four benchmark case studies for numerical implementations of phase-field model PDEs with increasing\norder of complexity in terms of additional physics. In this context, our objective is to replace programming\nand compiling with prompt writing how far we can explore ChatGPT for generating the correct solutions\n13\nWrite \nPython \ncode \nfor \na \n2D \ndendritic \ngrowth \nmodel \nwith \nspecifications:\nparameters: \nDefine \nNX=NY=200, \nN_t=2500, \ndelta_x=delta_y=0.03, \ndelta_t=0.0001, \nepsilon_bar=0.01, \ndelta=0.02, \nalpha=0.9, \ngamma=10.0, \nTm=1.0, \nm=6.0, \ntheta_0=0.05, \nK=2, \ntau=0.0003.\nInitial \nsetup:  \nSet \nphi=1 \nin \na \ncircular \nregion \nof \nradius \n5 \nat \nthe \ncenter, \nphi \n= \n0 \noutside \nthe \ncircle. \nInitial \nT \n= \n0 \nthroughout \ndomain.\nEquations:              \nepsilon(theta) \n= \nepsilon_bar*(1 \n+ \ndelta \n* \ncos \n(m(theta-theta_0)))\ncalculate \nepsilon'                                                                                                                                                                \nterm1 \n= \nepsilon*epsilon'*dphi/dx; \nterm2 \n= \nepsilon \n* \nepsilon'*dphi/dy; \nterm3 \n= \n(alpha/pi) \n* \narctan*(gamma*(Tm \n- \nT)); \nterm4 \n= \nphi*(1-phi)*(phi-0.5+term3)\ndphi/dt \n= \n(d(term1)/dy \n-d(term2)/dx \n+ \ndivergence \nof(epsilon^2 \n* \ngradient \nof \nphi) \n+ \nterm4)/tau\ndT/dt \n= \nlaplacian_T \n+ \nK*dphi/dt\nApply \nperiodic \nboundary \nconditions. \nPlot \nphi \nand \nT \nat \nt \n= \n100 \nintervals.\nWrite \nPython \ncode \nfor \na \n2D \ndendritic \ngrowth \nmodel \nwith \nspecifications:\nparameters: \nDefine \nNX=NY=200, \nN_t=2500, \ndelta_x=delta_y=0.03, \ndelta_t=0.0001, \nepsilon_bar=0.01, \ndelta=0.02, \nalpha=0.9, \ngamma=10.0, \nTm=1.0, \nm=6.0, \ntheta_0=0.05, \nK=2, \ntau=0.0003.\nInitial \nsetup:  \nSet \nphi=1 \nin \na \ncircular \nregion \nof \nradius \n5 \nat \nthe \ncenter. \nInitial \nT \n= \n0 \nthroughout \ndomain.\nfirst \nloop:\ngradPhiX \n= \n(phi[i+1, \nj] \n- \nphi[i-1, \nj]) \n/ \n(2*delta_x), \ngradPhiY \n= \n(phi[i, \nj+1] \n- \nphi[i, \nj-1]) \n/ \n(2*delta_y).\nlapPhi[i, \nj] \n= \n(phi[i+1, \nj] \n+ \nphi[i-1, \nj] \n- \n2*phi[i, \nj]) \n/ \n(delta_x^2) \n+ \n(phi[i, \nj+1] \n+ \nphi[i, \nj-1] \n- \n2*phi[i, \nj]) \n/ \n(delta_y^2)\nsimilarly \ncalculate \nLaplacian \nof \nT. \nCalculate \norientation \ntheta \nby \nthe \nfollowing \napproach\nif \ngradPhiX \n== \n0:\n    \ntheta \n= \n0.5 \n* \npi \nif \ngradPhiY \n> \n0 \nelse \n-0.5 \n* \npi\nelse:\n    \natanValue \n= \narctan(gradPhiY \n/ \ngradPhiX)\n    \nif \ngradPhiX \n> \n0:\n        \ntheta \n= \natanValue \nif \ngradPhiY \n>= \n0 \nelse \n2.0 \n* \npi \n+ \natanValue\n    \nelse:\n        \nangle \n= \npi \n+ \natanValue\nepsilon \n= \nepsilon_bar \n* \n(1 \n+ \ndelta \n* \ncos(m* \n(theta- \ntheta_0)))\ndel_epsilon \n= \n-epsilon_bar \n* \nm \n* \ndelta \n* \nsin(m \n* \n(theta \n- \ntheta_0)).\nax \n= \ndelta \n* \ndel_delta \n* \ngradPhiX; \nay \n= \n-delta \n* \ndel_delta \n* \ngradPhiY; \nepsilon_sq \n= \nepsilon^2\nsecond \nloop:\ndxdy \n= \n(ay[(i \n+ \n1) \n%NX, \nj] \n- \nay[(i \n- \n1)%NX, \nj]) \n/ \n(2.0 \n* \ndelta_y); \ndydx \n= \n(ax[i, \n(j \n+ \n1)%NY] \n- \nax[i, \n(j \n- \n1)%NY]) \n/ \n(2.0 \n* \ndelta_x)\ngradEps2X \n= \n(epsilon_sq[(i \n+ \n1)%NX \n, \nj] \n- \nepsilon_sq[(i \n- \n1)%NX \n, \nj]) \n/ \n(2.0 \n* \ndelta_y);  \ngradEps2Y \n= \n(epsilon_sq[i, \n(j \n+ \n1) \n%NY] \n- \nepsilon_sq[i, \n(j \n- \n1)%NY]) \n/ \n(2.0 \n* \ndelta_x)\nphi_init \n= \nphi[i, \nj]\nterm \n= \nalpha \n/ \nPI \n* \narctan(gamma \n* \n(Tm \n- \nT[i, \nj]))\nscal \n= \ngradEps2X \n* \ngradPhiX \n+ \ngradEps2Y \n* \ngradPhiY\nphi[i, \nj] \n+= \n(dxdy \n+ \ndydx \n+ \nepsilon_sq[i, \nj] \n* \nlapPhi[i, \nj] \n+ \nscal \n+ \n(phi_init \n* \n(1.0 \n- \nphi_init) \n* \n(phi_init \n- \n0.5 \n+ \nterm))) \n* \ndelta_t \n/ \ntau\nT[i, \nj] \n+= \nlapT[i, \nj] \n* \ndelta_t \n+ \nK \n* \n(phi[i, \nj] \n- \nphi_init)\nDo \nnot \ninitialize \nvariables \nto \nzero \ninside \ntime \nloop. \nplot \nphi \nand \nT \nat \nt \n= \n100 \nintervals. \nPrompt\nPrompt \n1 \n(zero-shot)\nPrompt \n1 \n(one \nof \nthe \nwrong \noutputs)\nPrompt \n2 \n(correct \noutput)\nPrompt \n2 \n(zero-shot)\nFigure 5: (Color online) As a fourth case study (Sec. 3.4), we depict the ChatGPT-4 response after entering the prompts in a\nzero-shot manner in the AI chatbot. Prompt 1 produces the wrong code and output. Since ChatGPT produces different codes\nfor repeated user calls with the same prompt, this is just one of the wrong outputs that we present for reference. Prompt 2\nproduces the correct code and output. Using Prompt 2, we show the dendrite growth at time t= 2400 by plotting the ϕ and\nT fields. The six primary arms and the well-developed secondary branches can be clearly seen. These results perfectly match\nwith the in-house code execution results for the identical problem.\n14\nof the input PDEs, noting that there is a significant potential for coding skills to transition fully to natural\nlanguage in the future. From this work, we highlight the following observations and remarks:\n• ChatGPT can be effectively leveraged to address a range of tasks aiding materials research, such as\nwriting and explaining scientific computer codes, running Python codes, generating visualization files,\nperforming data analysis, and even writing scripts for running simulations on mainstream software\nsuch as MATLAB [60]. As per our experience (Table 2), other LLMs are not so robust yet when it\ncomes to scientific computing and data analysis. Thus, many of the latest conversational AI chatbots,\nsuch as Copilot [16], are powered by GPT-4 Turbo [61], the latest iteration in the GPT models (as of\nApril 1, 2024).\n• In particular, ChatGPT is seemingly professional at generating multi-physics codes, with its particular\nstrength in scientific computing in Python. However, we do not recommend executing the code on\nChatGPT due to its processing constraints on simulation length and time scales (see Sec. 3.4). With\nincreasing problem complexity, we note that repetition of the same query is often needed to produce\nthe desired output. The dependence on reliable internet speed aggravates the situation even further.\nFortunately, ChatGPT allows the user to copy the code to use in external software systems.\n• In case of complex PDEs with interacting many physical parameters (Sec. 3.4), ChatGPT is likely\nto introduce inaccuracies in the code and thus fails to produce the full solution process, indicating\nthe computing constraints and its low capability to complex PDE solution. Thus, additional, specific\ndetails must be provided based on the complexity of the coding task. However, the risk remains for\nmore hidden errors and output inconsistencies, which one can often experience when working with\nChatGPT. Thus, expert-driven fact-checking and verification processes become essential. Although\nChatGPT is capable of providing a code base as a good starting point to tackle such complex tasks, a\nmeticulous amount of coding tasks still remains at hand.\n• We find that the performance of ChatGPT significantly decreases as the complexity of PDEs increases.\nThis is due to the lack of domain knowledge that aggravates the gap between its understanding of the\nproblem and the real situation. With the increasing complexity of PDEs, we note that it is becoming\nmore of a trial-and-error approach to solve PDEs and less of a scientific approach. Thus, ChatGPT\nis not yet efficient at handling domain-specific specialized research involving complex PDEs for which\nan in-depth understanding of the given problem is required, as shown in Sec. 3.4. However, with\nmore technical advancements of these models in the next iterations of development (e.g., GPT-5), they\nshould be able to handle increasingly complex PDEs.\n• It is clear from our work (Sec. 3.1-3.3) that ChatGPT shows acceptable performance in exploring\ngeneral scientific tasks, that is, to solve PDEs up to a certain complexity. In this regard, the ability\n15\nof ChatGPT can be further improved to some extent via chain prompting by providing iteration and\n“intelligent” feedback to ChatGPT. These aspects will be indeed helpful for classroom teaching and\ntraining of new materials researchers where mostly low-to-medium level complex queries are addressed.\n• ChatGPT can fail spectacularly in generating code that does not work. For the most difficult problem\nto solve in this study (i.e., Task 4 for dendrite crystal growth), ChatGPT fails to produce the correct\ncode. We carefully evaluate the errors in the generated codes after making repeated calls with the same\nprompt in ChatGPT. However, the errors are not consistent as they differ in different codes. We note\nthat typical errors range from implementation errors in the initial and boundary conditions, logical\nerrors in computing derivatives and implementing array loops, broken code due to the timeout limit\nin ChatGPT, and sometimes ignored instructions in the input/output. Since showing all the wrong\noutputs is impractical, we show a few of these in the supplementary material for demonstration. In\nSec. 3.4, we discuss the error mitigation strategies to produce the desired output in the context of\nthe dendrite growth problem. However, solving dendrite growth in ChatGPT is not trivial because\nof the large-sized prompt, and there is a limit to how many characters within a chat ChatGPT will\n“remember” so it will frequently make mistakes due to losing the context of early prompts or its early\nanswers and sometimes generate a broken code due to the timeout limit. Therefore, we attempt to use\na single prompt that will minimize inference time and reduce the computational cost, improving the\neffectiveness of the ChatGPT approach in error mitigation. Note that evaluating errors and improving\nthe performance of ChatGPT is a broad topic, which is not the focus of this work, but it makes an\ninteresting future study. Remarkably, the newest and latest GPT model, ChatGPT-4o [62], leads to\nbetter results for coding tasks to simulate the dendritic growth problem even using a single prompt\n(prompt 1 in Fig. 5, see supplementary for details). The ChatGPT-4 [12] could not produce the correct\ncode and output using the same prompt (see left subpanel in Fig. 5).\n• We show that LLMs in general are useful in coding tasks. Moreover, even if the generated code pro-\nduces the correct output, the accuracy of the results may not be useful for the targeted applications.\nFor instance, the simulation accuracy required for materials education purposes is different from that\nrequired in critical applications such as designing materials for healthcare and aerospace, where ac-\ncuracy and reliability are paramount. Perhaps more troubling is that the LLM algorithms are “black\nboxes”; thus, it is difficult to know why they are generating the code that they do, and there is no\nmeasure of uncertainty associated with those outputs. Thus, uncertainty quantification (UQ) may\nbe essential in the future to enhance the reliability and trustworthiness of LLMs, fostering greater\nacceptance and utilization in practical applications.\n• Finally, every reward comes with a cost. ChatGPT will undoubtedly reduce the need for certain skills;\n16\nmost importantly, scientific coding can be accomplished more efficiently with reduced time investment.\nThus, it remains to be seen how the use of LLMs challenges traditional ways of education and training\nof researchers. Most importantly, beginners must remain cautious with answers from ChatGPT and\nin relying heavily on it, as it may hinder the development of their research skills. Also, the challenge\nremains to be seen in terms of the extent to which the research process should be outsourced to\nLLMs and what academic skills remain essential to researchers. On the other hand, these AI tools\nwill eventually open up new frontiers in various domains and introduce new skills, such as prompt\nengineering and generative AI in general, which may significantly boost research productivity. However,\nwith increasingly complex problems, more expert guidance is required for LLMs to perform the task\naccurately. Thus, researchers should gain sufficient expertise in their domain knowledge before they\ncan leverage ChatGPT to expedite their workflow.\nThere is no denying that LLMs have the potential to impoverish our own writing and thinking skills since\noften we have to do only a little to obtain a sophisticated text or code [2]. While the debate will continue\nregarding its implication on scientific practice and responsible use of LLMs for research and the security risk\nit poses, the benefit-to-risk ratio remains very high [1] and is improving with each passing day. For example,\nChatGPT-4 now cites the sources for its answers, addressing some of the plagiarism issues inherent to LLM-\nbased applications (inherited from training data) [63]. With the rapid increase in model “parameters” and\n“tokens” (Table 1) and collaborative synergy with human expertise, LLMs offer a promising outlook for\naccelerating materials research and exploration that seem out of reach today. Consequently, the enhanced\nproficiency of LLMs in materials modeling at different scales could enable them to evolve into AI-powered\nICME agents in the future.\nAcknowledgments\nS. Ghosh acknowledges the support from FIG (SRIC office, IIT Roorkee) and SERB (Government of\nIndia).\nSupplementary Material\nWe provide the Python codes generated by ChatGPT as a response to the given prompts for four tasks,\nas depicted in Figs. 2-5. These files are named as task1.py, task2.py, task3.py, and task4.py. We share\nthe ChatGPT chat history to solve the diffusion problem (Task 1) in Task1-chatgpt.pdf. We share the\nChatGPT workspace for dendritic growth problem (Task 4) with incorrect output inTask4-wrong.pdf. The\noutput from ChatGPT-4o for Task 4 is given in Task4-chatgpt4o-output.pdf.\n17\nConflict of Interest Statement\nThe authors have no conflicts to disclose.\nData Availability Statement\nThe data that supports the findings of the study are available from the corresponding author upon\nreasonable request.\nReferences\n[1] E. A. Van Dis, J. Bollen, W. Zuidema, R. Van Rooij, C. L. Bockting, ChatGPT: five priorities for\nresearch, Nature 614 (7947) (2023) 224–226. doi:https://doi.org/10.1038/d41586-023-00288-7 .\n[2] L. Schulze Balhorn, J. M. Weber, S. Buijsman, J. R. Hildebrandt, M. Ziefle, A. M. Schweidtmann,\nEmpirical assessment of ChatGPT’s answering capabilities in natural science and engineering, Scientific\nReports 14 (1) (2024) 4998. doi:https://doi.org/10.1038/s41598-024-54936-7 .\n[3] A. G´ eron, Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow, O’Reilly Media, Inc.,\nNewton, MA, 2022.\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin,\nAttention is all you need, in: Advances in neural information processing systems, 2017, pp. 5998–6008.\n[5] Common Crawl, https://commoncrawl.org/ (2024).\n[6] Wikipedia, https://www.wikipedia.org/ (2023).\n[7] V. Alto, Modern Generative AI with ChatGPT and OpenAI Models: Leverage the capabilities of\nOpenAI’s LLM for productivity and innovation with GPT3 and GPT4, Packt Publishing, Birmingham,\nUK, 2023.\n[8] G. Lei, R. Docherty, S. J. Cooper, Materials science in the era of large language models: a perspective,\narXiv preprint arXiv:2403.06949doi:https://doi.org/10.48550/arXiv.2403.06949.\n[9] Y. Liu, Z. Yang, Z. Yu, Z. Liu, D. Liu, H. Lin, M. Li, S. Ma, M. Avdeev, S. Shi, Generative artificial\nintelligence and its applications in materials science: Current situation and future perspectives, Journal\nof Materiomics 9 (4) (2023) 798–816. doi:https://doi.org/10.1016/j.jmat.2023.05.001.\n[10] LLMLingua-2, https://llmlingua.com/ (2023).\n[11] GPT-3, https://openai.com/blog/gpt-3-apps/ (2021).\n18\n[12] GPT-4, https://openai.com/gpt-4/ (2023).\n[13] Grok, https://grok.x.ai/ (2023).\n[14] Gemini, https://gemini.google.com/app/ (2024).\n[15] Llama 2, https://llama.meta.com/ (2023).\n[16] Copilot, https://copilot.microsoft.com/ (2023).\n[17] Cluade-3, https://claude.ai/ (2024).\n[18] Mistral AI, https://mistral.ai/ (2023).\n[19] S. Frieder, L. Pinchetti, R.-R. Griffiths, T. Salvatori, T. Lukasiewicz, P. Petersen, J. Berner, Mathe-\nmatical capabilities of ChatGPT, Advances in Neural Information Processing Systems 36.\n[20] N. Koceska, S. Koceski, L. K. Lazarova, M. Miteva, B. Zlatanovska, Can chatgpt be used for solving\nordinary differential equations, Balkan Journal of Applied Mathematics and Informatics 6 (2) (2023)\n103–114.\n[21] G. Orlando, Assessing chatgpt for coding finite element methods, Journal of Machine Learning for\nModeling and Computing 4 (2).\n[22] T. Dave, S. A. Athaluri, S. Singh, ChatGPT in medicine: an overview of its applications, advantages,\nlimitations, future prospects, and ethical considerations, Frontiers in artificial intelligence 6 (2023)\n1169595. doi:10.3389/frai.2023.1169595.\n[23] T. Lubiana, R. Lopes, P. Medeiros, J. C. Silva, A. N. A. Goncalves, V. Maracaja-Coutinho, H. I. Nakaya,\nTen quick tips for harnessing the power of ChatGPT in computational biology, PLOS Computational\nBiology 19 (8) (2023) e1011319. doi:10.1371/journal.pcbi.1011319.\n[24] J. H. Choi, K. E. Hickman, A. B. Monahan, D. Schwarcz, ChatGPT goes to law school, J. Legal Educ.\n71 (2021) 387.\n[25] Z. Hong, ChatGPT for computational materials science: A perspective, Energy Material Advances 4\n(2023) 0026. doi:10.34133/energymatadv.0026.\n[26] J. Deb, L. Saikia, K. D. Dihingia, G. N. Sastry, ChatGPT in the material design: Selected case studies to\nassess the potential of ChatGPT, Journal of Chemical Information and Modeling 64 (3) (2024) 799–811.\ndoi:10.1021/acs.jcim.3c01702.\n[27] L. Q. Chen, Phase-field models for microstructure evolution, Annu. Rev. Mater. Res. 32 (2002) 113–140.\n19\n[28] N. Moelans, B. Blanpain, P. Wollants, An introduction to phase-field modeling of microstructure evo-\nlution, Calphad 32 (2) (2008) 268 – 294. doi:https://doi.org/10.1016/j.calphad.2007.11.003.\n[29] W. J. Boettinger, J. A. Warren, C. Beckermann, A. Karma, Phase-field simulation of solidification,\nAnnu. Rev. Mater. Res. 32 (2002) 163–194. doi:https://doi.org/10.1146/annurev.matsci.32.\n101901.155803.\n[30] I. Steinbach, Phase-field models in materials science, Modelling and Simulation in Materials Science\nand Engineering 17 (2009) 073001. doi:10.1088/0965-0393/17/7/073001.\n[31] A. M. Jokisaari, P. Voorhees, J. E. Guyer, J. Warren, O. Heinonen, Benchmark problems for numerical\nimplementations of phase field models, Computational Materials Science 126 (2017) 139–151. doi:\nhttps://doi.org/10.1016/j.commatsci.2016.09.022.\n[32] A. M. Jokisaari, P. W. Voorhees, J. E. Guyer, J. A. Warren, O. G. Heinonen, Phase field benchmark\nproblems for dendritic growth and linear elasticity, Computational Materials Science 149 (2018) 336–347.\ndoi:https://doi.org/10.1016/j.commatsci.2018.03.015.\n[33] W. Callister, D. Rethwisch, Materials Science and Engineering: An Introduction, Wiley Plus Products\nSeries, John Wiley & Sons, New York, NY, 2010.\n[34] J. W. Cahn, J. E. Hilliard, Free energy of a nonuniform system. I. Interfacial free energy, The Journal\nof chemical physics 28 (2) (1958) 258–267. doi:https://doi.org/10.1063/1.1744102.\n[35] S. M. Allen, J. W. Cahn, A microscopic theory for antiphase boundary motion and its application to\nantiphase domain coarsening, Acta Metallurgica 27 (6) (1979) 1085 – 1095. doi:https://doi.org/10.\n1016/0001-6160(79)90196-2.\n[36] R. Kobayashi, Modeling and numerical simulations of dendritic crystal growth, Physica D: Nonlinear\nPhenomena 63 (3-4) (1993) 410–423. doi:https://doi.org/10.1016/0167-2789(93)90120-P.\n[37] Hugging Face, https://huggingface.co/models (2024).\n[38] D. A. Porter, K. E. Easterling, Phase transformations in metals and alloys, CRC press, Boca Raton,\nFL, 2009.\n[39] M. Frigo, S. G. Johnson, The design and implementation of FFTW3, Proceedings of the IEEE 93 (2)\n(2005) 216–231, special issue on “Program Generation, Optimization, and Platform Adaptation” .\n[40] L. Dagum, R. Menon, OpenMP: An Industry-Standard API for Shared-Memory Programming, IEEE\nComput. Sci. Eng. 5 (1) (1998) 46–55. doi:10.1109/99.660313.\n20\n[41] W. Gropp, E. Lusk, A. Skjellum, Using MPI: portable parallel programming with the message-passing\ninterface, Vol. 1, MIT press, 1999.\n[42] NVIDIA Corporation, NVIDIA CUDA C programming guide, version 3.2 (2010).\n[43] A. Farzadi, M. Do-Quang, S. Serajzadeh, A. Kokabi, G. Amberg, Phase-field simulation of weld solidifi-\ncation microstructure in an Al-Cu alloy, Modelling and Simulation in Materials Science and Engineering\n16 (6) (2008) 065005. doi:10.1088/0965-0393/16/6/065005.\n[44] S. Ghosh, A. Mukherjee, T. Abinandanan, S. Bose, Particles with selective wetting affect spinodal\ndecomposition microstructures, Physical Chemistry Chemical Physics 19 (23) (2017) 15424–15432. doi:\nhttps://doi.org/10.1039/C7CP01816A.\n[45] S. Ghosh, A. Mukherjee, R. Arroyave, J. F. Douglas, Impact of particle arrays on phase separation\ncomposition patterns, The Journal of chemical physics 152 (22) (2020) 224902. doi:https://doi.org/\n10.1063/5.0007859.\n[46] S. Ghosh, J. F. Douglas, Phase separation in the presence of fractal aggregates, The Journal of Chemical\nPhysics 160 (10) (2024) 104903. doi:10.1063/5.0190196.\n[47] S. Ghosh, C. K. Newman, M. M. Francois, Tusas: A fully implicit parallel approach for coupled phase-\nfield equations, Journal of Computational Physics 448 (2022) 110734. doi:https://doi.org/10.1016/\nj.jcp.2021.110734.\n[48] I. Steinbach, Phase-field model for microstructure evolution at the mesoscopic scale, An-\nnual Review of Materials Research 43 (2013) 89–107. doi:https://doi.org/10.1146/\nannurev-matsci-071312-121703 .\n[49] P. W. Voorhees, The theory of Ostwald ripening, J. Stat. Phys 38 (1-2) (1985) 231–252. doi:https:\n//doi.org/10.1007/BF01017860].\n[50] S. Lee, C. Lee, H. G. Lee, J. Kim, Comparison of different numerical schemes for the cahn-hilliard\nequation, Journal of the Korean Society for Industrial and Applied Mathematics 17 (3) (2013) 197–207.\ndoi:https://doi.org/10.12941/jksiam.2013.17.197.\n[51] N. Provatas, K. Elder, Phase-field methods in materials science and engineering, John Wiley & Sons,\nNew York, NY, 2011.\n[52] S. Ghosh, N. Ofori-Opoku, J. E. Guyer, Simulation and analysis of γ-ni cellular growth during laser\npowder deposition of ni-based superalloys, Computational Materials Science 144 (2018) 256–264. doi:\nhttps://doi.org/10.1016/j.commatsci.2017.12.037.\n21\n[53] S. Ghosh, M. Plapp, Influence of interphase boundary anisotropy on bulk eutectic solidification mi-\ncrostructures, Acta Materialia 140 (2017) 140–148. doi:https://doi.org/10.1016/j.actamat.2017.\n08.023.\n[54] M. Hillert, On the theory of normal and abnormal grain growth, Acta metallurgica 13 (3) (1965) 227–\n238. doi:https://doi.org/10.1016/0001-6160(65)90200-2.\n[55] D. Fan, L.-Q. Chen, Computer simulation of grain growth using a continuum field model, Acta Materialia\n45 (2) (1997) 611–622. doi:https://doi.org/10.1016/S1359-6454(96)00200-5.\n[56] W. Kurz, D. J. Fisher, R. Trivedi, Progress in modelling solidification microstructures in metals and\nalloys: dendrites and cells from 1700 to 2000, International Materials Reviews 64 (6) (2019) 311–354.\ndoi:https://doi.org/10.1080/09506608.2018.1537090.\n[57] W. Kurz, M. Rappaz, R. Trivedi, Progress in modelling solidification microstructures in metals and\nalloys. part ii: dendrites from 2001 to 2018, International Materials Reviews 66 (1) (2021) 30–76.\ndoi:https://doi.org/10.1080/09506608.2020.1757894.\n[58] B. Echebarria, R. Folch, A. Karma, M. Plapp, Quantitative phase-field model of alloy solidification,\nPhysical Review E 70 (6) (2004) 061604. doi:https://doi.org/10.1103/PhysRevE.70.061604.\n[59] T. Keller, G. Lindwall, S. Ghosh, L. Ma, B. Lane, F. Zhang, U. R. Kattner, E. A. Lass, J. C. Heigel,\nY. Idell, M. E. Williams, A. J. Allen, J. E. Guyer, L. E. Levine, Application of Finite Element, Phase-\nfield, and CALPHAD-based Methods to Additive Manufacturing of Ni-based Superalloys, Acta Materi-\nalia 139 (2017) 244–253. doi:https://doi.org/10.1016/j.actamat.2017.05.003.\n[60] MATLAB, Version R2018a, The MathWorks Inc., Natick, Massachusetts, 2018.\n[61] GPT-4 Turbo, https://platform.openai.com/docs/models/overview/ (29 March 2024).\n[62] GPT-4o, https://openai.com/index/hello-gpt-4o/ (May 13, 2024).\n[63] OpenAI@OpenAI, https://twitter.com/OpenAI/status/1773738074041717109 (29 March 2024).\n22",
  "topic": "Leverage (statistics)",
  "concepts": [
    {
      "name": "Leverage (statistics)",
      "score": 0.6314489245414734
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5108160972595215
    },
    {
      "name": "Computer science",
      "score": 0.4789764881134033
    },
    {
      "name": "Code (set theory)",
      "score": 0.4455465078353882
    },
    {
      "name": "Exploit",
      "score": 0.43234705924987793
    },
    {
      "name": "Management science",
      "score": 0.3532414734363556
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2285309135913849
    },
    {
      "name": "Engineering",
      "score": 0.17936724424362183
    },
    {
      "name": "Programming language",
      "score": 0.13741254806518555
    },
    {
      "name": "Biology",
      "score": 0.11302486062049866
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154851008",
      "name": "Indian Institute of Technology Roorkee",
      "country": "IN"
    }
  ]
}