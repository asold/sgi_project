{
  "title": "Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers",
  "url": "https://openalex.org/W4382877880",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1997810020",
      "name": "Dong Bo",
      "affiliations": [
        "Nankai University"
      ]
    },
    {
      "id": "https://openalex.org/A2133578637",
      "name": "Wenhai Wang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2242721764",
      "name": "Deng-Ping Fan",
      "affiliations": [
        "Nankai University"
      ]
    },
    {
      "id": "https://openalex.org/A2108460511",
      "name": "Jinpeng Li",
      "affiliations": [
        "Inception Institute of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2230394843",
      "name": "Huazhu Fu",
      "affiliations": [
        "Institute of High Performance Computing",
        "Agency for Science, Technology and Research"
      ]
    },
    {
      "id": "https://openalex.org/A2136903162",
      "name": "Ling Shao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1965415664",
    "https://openalex.org/W2077474654",
    "https://openalex.org/W2766773666",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3092344722",
    "https://openalex.org/W3118152168",
    "https://openalex.org/W2008359794",
    "https://openalex.org/W2021088830",
    "https://openalex.org/W2285968993",
    "https://openalex.org/W3164098653",
    "https://openalex.org/W3034684132",
    "https://openalex.org/W2997286550",
    "https://openalex.org/W2560328367",
    "https://openalex.org/W3082954719",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2922509574",
    "https://openalex.org/W2962943776",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W4295936768",
    "https://openalex.org/W4295915728",
    "https://openalex.org/W4295934810",
    "https://openalex.org/W4290715209",
    "https://openalex.org/W4295916856",
    "https://openalex.org/W2962927567",
    "https://openalex.org/W2789559946",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W2999580839",
    "https://openalex.org/W3008070655",
    "https://openalex.org/W2979600871",
    "https://openalex.org/W2963649926",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3048524582",
    "https://openalex.org/W3049128439",
    "https://openalex.org/W3104061658",
    "https://openalex.org/W3134036841",
    "https://openalex.org/W2987175876",
    "https://openalex.org/W3204166336",
    "https://openalex.org/W3135262214",
    "https://openalex.org/W3202263958",
    "https://openalex.org/W2750023899",
    "https://openalex.org/W2346062110",
    "https://openalex.org/W3095725246",
    "https://openalex.org/W3090492687",
    "https://openalex.org/W3181013887",
    "https://openalex.org/W3187286114",
    "https://openalex.org/W4206250312",
    "https://openalex.org/W3157655791",
    "https://openalex.org/W3120333390",
    "https://openalex.org/W3180624251",
    "https://openalex.org/W3187378100",
    "https://openalex.org/W3177634011",
    "https://openalex.org/W3189999308",
    "https://openalex.org/W3202285299",
    "https://openalex.org/W4214633470",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W4214709605",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W2963112696",
    "https://openalex.org/W1994922096",
    "https://openalex.org/W3177004386",
    "https://openalex.org/W3199914841",
    "https://openalex.org/W2963529609",
    "https://openalex.org/W2979515228",
    "https://openalex.org/W4308456711",
    "https://openalex.org/W2034269173",
    "https://openalex.org/W3204995672",
    "https://openalex.org/W3157358059",
    "https://openalex.org/W3098995103",
    "https://openalex.org/W3106168076"
  ],
  "abstract": "Most polyp segmentation methods use convolutional neural networks (CNNs) as their backbone, leading to two key issues when exchanging information between the encoder and decoder: (1) taking into account the differences in contribution between different-level features, and (2) designing an effective mechanism for fusing these features. Unlike existing CNN-based methods, we adopt a transformer encoder, which learns more powerful and robust representations. In addition, considering the image acquisition influence and elusive properties of polyps, we introduce three standard modules, including a cascaded fusion module (CFM), a camouflage identification module (CIM), and a similarity aggregation module (SAM). Among these, the CFM is used to collect the semantic and location information of polyps from high-level features; the CIM is applied to capture polyp information disguised in low-level features, and the SAM extends the pixel features of the polyp area with high-level semantic position information to the entire polyp area, thereby effectively fusing cross-level features. The proposed model, named Polyp-PVT, effectively suppresses noises in the features and significantly improves their expressive capabilities. Extensive experiments on five widely adopted datasets show that the proposed model is more robust to various challenging situations (e.g., appearance changes, small objects, and rotation) than existing representative methods. The proposed model is available at https://github.com/DengPingFan/Polyp-PVT.",
  "full_text": " \nPolyp-PVT: Polyp Segmentation with Pyramid Vision\nTransformers\nBo Dong1, Wenhai Wang2, Deng-Ping Fan1 ✉, Jinpeng Li3, Huazhu Fu4, and Ling Shao5\n \nABSTRACT\nMost polyp segmentation methods use convolutional neural networks (CNNs) as their backbone, leading to two key issues when\nexchanging information between the encoder and decoder: (1) taking into account the differences in contribution between different-\nlevel features, and (2) designing an effective mechanism for fusing these features. Unlike existing CNN-based methods, we adopt\na transformer encoder, which learns more powerful and robust representations. In addition, considering the image acquisition\ninfluence and elusive properties of polyps, we introduce three standard modules, including a cascaded fusion module (CFM), a\ncamouflage identification module (CIM), and a similarity aggregation module (SAM). Among these, the CFM is used to collect the\nsemantic and location information of polyps from high-level features; the CIM is applied to capture polyp information disguised in\nlow-level features, and the SAM extends the pixel features of the polyp area with high-level semantic position information to the\nentire polyp area, thereby effectively fusing cross-level features. The proposed model, named Polyp-PVT, effectively suppresses\nnoises in the features and significantly improves their expressive capabilities. Extensive experiments on five widely adopted\ndatasets show that the proposed model is more robust to various challenging situations (e.g., appearance changes, small objects,\nand rotation) than existing representative methods. The proposed model is available at https://github.com/DengPingFan/Polyp-\nPVT.\nKEYWORDS\npolyp segmentation; pyramid vision transformer; colonoscopy; computer vision\n  \nColonoscopy  is  the  gold  standard  for  detecting  colorectal\nlesions  since  it  enables  colorectal  polyps  to  be  identified\nand  removed  in  time,  thereby  preventing  further  spread.\nAs  a  fundamental  task  in  medical  image  analysis,  polyp\nsegmentation  (PS)  aims  to  locate  polyps  accurately  in  the  early\nstage,  which  is  of  great  significance  in  the  clinical  prevention  of\nrectal  cancer.  Traditional  PS  models  mainly  rely  on  low-level\nfeatures,  e.g.,  texture[1],  geometric  features[2],  and  simple  linear\niterative  clustering  superpixels[3].  However,  these  methods  yield\nlow-quality  results  and  suffer  from  poor  generalization  ability.\nWith  the  development  of  deep  learning,  PS  has  achieved\npromising  progress.  In  particular,  the  U-shaped[4] has  attracted\nsignificant attention due to its ability to adopt multi-level features\nfor reconstructing high-resolution results. PraNet[5] employs a two-\nstage  segmentation  approach,  adopting  a  parallel  decoder  to\npredict  rough  regions  and  an  attention  mechanism  to  restore  a\npolyp’s  edges  and  internal  structure  for  fine-grained\nsegmentation.  ThresholdNet[6] is  a  confidence-guided  data\nenhancement method based on a hybrid manifold for solving the\nproblems  caused  by  limited  annotated  data  and  imbalanced  data\ndistributions.\nAlthough  these  methods  have  greatly  improved  accuracy  and\ngeneralization  ability  compared  to  traditional  methods,  it  is  still\nchallenging for them to locate the boundaries of polyps, as shown\nin Fig. 1, for several reasons:\n(1) Image  noise.  During  the  data  collection  process,  the  lens\nrotates  in  the  intestine  to  obtain  polyp  images  from  different\nangles, which also causes motion blur and reflector problems. As a\nresult, this greatly increases the difficulty of polyp detection;\n(2) Camouflage.  The  color  and  texture  of  polyps  are  very\nsimilar to surrounding tissues, with low contrast, providing them\nwith  powerful  camouflage  properties[11, 12],  and  making  them\ndifficult to identify;\n(3) Polycentric  data.  Current  models  struggle  to  generalize  to\nmulticenter (or unseen) data with different domains/distributions.\nTo address the above issues, our contributions in this paper are\nas follows:\n● We  present  a  novel  polyp  segmentation  framework,  termed\nPolyp-PVT. Unlike existing convolutional neural network (CNN)-\nbased  methods,  we  adopt  the  pyramid  vision  transformer  as  an\nencoder to extract more robust features.\n● To  support  our  framework,  we  introduce  three  simple\nmodules.  Specifically,  the  cascaded  fusion  module  (CFM)  collects\npolyps’ semantic  and  location  information  from  the  high-level\nfeatures  through  progressive  integration.  Meanwhile,  the\ncamouflage  identification  module  (CIM)  is  applied  to  capture\npolyp  cues  disguised  in  low-level  features,  using  an  attention\nmechanism  to  pay  more  attention  to  potential  polyps,  reducing\nincorrect information in the lower features. We further introduce\nthe  similarity  aggregation  module  (SAM)  equipped  with  a  non- \n1 College of Computer Science, Nankai University, Tianjin 300350, China\n2 Shanghai Artificial Intelligence Laboratory, Shanghai 200232, China\n3 Computer Vision Lab, Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates\n4 Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore 138632, Singapore\n5 UCAS-Terminus AI Lab, Terminus Group, Chongqing 400042, China\nAddress correspondence to Deng-Ping Fan, dengpfan@gmail.com\n© The author(s) 2023. The articles published in this open access journal are distributed under the terms of the Creative Commons Attribution\n4.0 International License (http://creativecommons.org/licenses/by/4.0/).\nhttps://doi.org/10.26599/AIR.2023.9150015 CAAI Artificial Intelligence Research\n \nCAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023 1\nlocal and convolutional graph layer to mine local pixels and global\nsemantic cues from the polyp area.\n● Finally, we conduct extensive experiments on five challenging\nbenchmark  datasets,  including  Kvasir-SEG[13],  ClinicDB[8],\nColonDB[10], Endoscene[14], and ETIS[9], to evaluate the performance\nof the proposed Polyp-PVT. On ColonDB, our method achieves a\nmean Dice (mDic) of 0.808, which is 5.5% higher than the existing\ncutting-edge  method  SANet[7].  On  the  ETIS  dataset,  our  model\nachieves an mDic of 0.787, which is 3.7% higher than SANet[7].\n 1    Related Work\n 1.1    Polyp segmentation\nTraditional  methods. Computer-aided  detection  is  an  effective\nalternative  to  manual  detection,  and  a  detailed  survey  has  been\nconducted  on  detecting  ulcers,  polyps,  and  tumors  in  wireless\ncapsule  endoscopy  imaging[15].  Early  solutions  for  polyp\nsegmentation  were  mainly  based  on  low-level  features,  such  as\ntexture[2],  geometric  features[2],  or  simple  linear  iterative  clustering\nsuperpixels[3]. However, these methods have a high risk of missed\nor  false  detection  due  to  the  high  similarity  between  polyps  and\nsurrounding tissues.\nDeep  learning-based  methods. Deep  learning  techniques[16−25]\nhave  greatly  promoted  the  development  of  polyp  segmentation\ntasks. Akbari et al.[26] proposed a polyp segmentation model using\na  fully  convolutional  neural  network,  whose  segmentation  results\nare  significantly  better  than  traditional  solutions.  Brandao  et  al.[27]\nused  the  shape  from  the  shading  strategy  to  restore  depth,\nmerging  the  result  into  an  RGB  model  to  provide  richer  feature\nrepresentations.  More  recently,  encoder-decoder-based  models,\nsuch  as  U-Net[4],  UNet++[28],  and  ResUNet++[29],  have  gradually\ncome  to  dominate  the  field  with  excellent  performance.  Sun  et\nal.[30] introduced  a  dilated  convolution  to  extract  and  aggregate\nhigh-level  semantic  features  with  resolution  retention  to  improve\nthe  encoder  network.  Psi-Net[31] introduces  a  multi-task\nsegmentation  model  that  combines  contour  and  distance  map\nestimation  to  assist  segmentation  mask  prediction.  Ali  Qadir  et\nal.[32] first  attempted  to  use  a  deeper  feature  extractor  to  perform\npolyp segmentation based on Mask R-CNN[33].\nDifferent from the methods based on U-Net[4, 28, 34], PraNet[5] uses\nreverse  attention  modules  to  mine  boundary  information  with  a\nglobal feature map, which is generated by a parallel partial decoder\nfrom high-level features. Polyp-Net[35] proposes a dual-tree wavelet\npooling CNN with a local gradient-weighted embedding level set,\neffectively  avoiding  erroneous  information  in  high  signal  areas,\nthereby significantly reducing the false positive rate. Rahim et al.[36]\nproposed to use different convolution kernels for the same hidden\nlayer  for  deeper  feature  extraction  with  Mish  function  and\nrectified  linear  unit  activation  functions  for  deep  feature\npropagation  and  smooth  non-monotonicity.  In  addition,  they\nadopted  joint  generalized  intersections,  which  overcome  scale\ninvariance,  rotation,  and  shape  differences.  Jha  et  al.[37] designed  a\nreal-time  polyp  segmentation  method  called  ColonSNet.  For  the\nfirst time, Ahmed[38] applied the generative adversarial network to\nthe field of polyp segmentation. Another interesting idea proposed\nby  Thambawita  et  al.[39] is  introducing  pyramid-based\naugmentation  into  the  polyp  segmentation  task.  Further,  Tomar\net  al.[40] designed  a  dual  decoder  attention  network  based  on\nResUNet++  for  polyp  segmentation.  More  recently,  MSEG[41]\nimproves  the  PraNet  and  proposes  a  simple  encoder-decoder\nstructure.  Specifically,  they  use  HarDNet[42] to  replace  the  original\nbackbone network Res2Net50 backbone network and removes the\nattention  mechanism  to  achieve  faster  and  more  accurate  polyp\nsegmentation.  As  an  early  attempt,  Transfuse[43] is  the  first  to\nemploy  a  two-branch  architecture  combining  CNNs  and\ntransformers  in  a  parallel  style.  DCRNet[44] uses  external  and\ninternal  context  relations  modules  to  separately  estimate  the\nsimilarity  between  each  location  and  all  other  locations  in  the\nsame  and  different  images.  MSNet[45] introduces  a  multi-scale\nsubtraction network to eliminate redundancy and complementary\ninformation  between  the  multi-scale  features.  Providing  a\ncomprehensive review of polyp segmentation is beyond the scope\nof this paper. In Table 1, however, we briefly survey representative\nworks related to ours.\n 1.2    Vision transformer\nTransformers  use  multi-head  self-attention  (MHSA)  layers  to\nmodel  long-term  dependencies.  Unlike  the  convolutional  layer,\nthe MHSA layer has dynamic weights and a global receptive field,\nmaking  it  more  flexible  and  effective.  Transformer[65] was  first\nproposed  by  Vaswani  et  al.  for  the  machine  translation  task  and\nhas  since  extensively  influenced  the  natural  language  processing\nfield. To apply transformers to computer vision tasks, Dosovitskiy\net al.[66] proposed a vision transformer (ViT), which is the first pure\ntransformer  for  image  classification.  ViT  divides  an  image  into\nmultiple  patches,  which  are  sequentially  sent  to  a  transformer\nencoder  after  being  encoded,  and  then  a  multilayer  perceptron\n(MLP) is used to perform image classification. HVT[67] is based on\na  hierarchical  progressive  pooling  method  to  compress  the\nsequence  length  of  a  token  and  reduce  the  redundancy  and\nnumber  of  calculations  in  ViT.  The  pooling-based  vision\ntransformer[68] draws  on  the  principle  of  CNNs  whereby,  as  the\ndepth  increases,  the  number  of  feature  map  channels  increases,\nand the spatial dimension decreases. Yuan et al.[69] pointed out that\nthe simple token structure in ViT cannot capture important local\nfeatures,  such  as  edges  and  lines,  which  reduces  the  training\nefficiency  and  leads  to  redundant  attention  mechanisms.  T2T\nViT[69] was  thus  proposed  to  use  layer-by-layer  tokens-to-token\ntransformation to gradually merge neighboring tokens and model\nlocal features while reducing the token’s length. TNT[70] employs a\ntransformer  suitable  for  fine-grained  image  tasks,  which  divides\nthe  original  image  patch  and  conducts  self-attention  mechanism\ncalculations  in  smaller  units.  Meanwhile,  external  and  internal\ntransformers are used to extract global and local features.\nTo  adapt  to  dense  prediction  tasks  such  as  semantic\n \nImage Ground truth Ours SANet\n \nFig. 1    Segmentation  examples  of  our  model  and  SANet[7] with  different\nchallenge  cases,  e.g.,  camouflage  (1st  and  2nd  rows)  and  image  acquisition\ninfluence  (3rd  row).  The  images  from  top  to  bottom  are  from  ClinicDB[8],\nETIS[9],  and  ColonDB[10],  which  show  that  our  model  has  better\ngeneralization ability.\nCAAI Artificial Intelligence Research\n \n2 CAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023\nsegmentation,  several  methods[71−77] have  also  introduced  the\npyramid  structure  of  CNNs  to  the  design  of  transformer\nbackbones. For instance, PVT-based models[71, 72] use a hierarchical\ntransformer  with  four  stages,  showing  that  a  pure  transformer\nbackbone  can  be  as  versatile  as  its  CNN  counterparts,  and\nperforms better in detection and segmentation tasks. In this work,\n \nTable 1    A survey on polyp segmentation. (The meanings of the abbreviation are listed as follows. CL: CVC-CLINIC, EL: ETIS-Larib, C6: CVC-612, AM: ASU-\nMayo[46, 47], ES: EndoScene, DB: ColonDB, CV: CVC-VideoClinicDB, C: Colon, ED: Endotect 2020, KS: Kvasir-SEG, KCS: Kvasir Capsule-SEG, PraNet: same to\ndatasets used in PraNet[5], IS: image segmentation, VS: video segmentation, CF: classfication, OD: object detection, Own: private data.)\nNo. Model Publication Code Type Dataset Core component Reference\n1 CSCPD IJPRAI − IS Own Adaptive-scale candidate [1]\n2 APD TMI − IS Own Geometrical analysis, binary classifier [2]\n3 SBCP SPMB − IS Own Superpixel [3]\n4 FCN EMBC − IS DB FCN and patch selection [26]\n5 D-FCN JMRR − IS CL, EL, AM, and DB FCN and Shape-from-Shading (SfS) [27]\n6 UNet++ DLMIA PyTorch IS AM Skip pathways and deep supervision [28]\n7 Psi-Net EMBC PyTorch IS Endovis Shape and boundary aware [31]\n8 Mask R-CNN ISMICT − IS C6, EL, and DB Deep feature extractors [32]\n9 UDC ICMLA − IS C6 and EL Dilation convolution [30]\n10 ThresholdNet TMI PyTorch IS ES and WCE Learn to threshold Confidence-guided\nmanifold mixup [6]\n11 MI2GAN MICCAI − IS C6 and EL GAN-based model [48]\n12 ACSNet MICCAI PyTorch IS ES and KS Adaptive context selection [49]\n13 PraNet MICCAI PyTorch IS PraNet Parallel partial decoder attention [5]\n14 GAN MediaEval − IS KS Image-to-image translation [38]\n15 APS MediaEval − IS KS Variants of U-shaped structure [50]\n16 PFA MediaEval PyTorch IS KS Pyramid focus augmentation [39]\n17 MMT MediaEval − IS KS Competition introduction [51]\n18 U-Net-ResNet50 MediaEval − IS KS Variants of U-shaped structure [34]\n19 Survey CMIG − CF Own Classification [15]\n20 Polyp-Net TIM − IS DB and CV Multimodel fusion network [35]\n21 Deep CNN BSPC − OD EL Convolutional neural network [36]\n22 EU-Net CRV PyTorch IS PraNet Semantic information enhancement [52]\n23 DSAS MIDL {Matlab IS KS Stochastic activation selection [53]\n24 U-Net-\nMobileNetV2 arXiv − IS KS Variants of U-shaped structure [54]\n25 DCRNet ISBI PyTorch IS ES, KS, andPICCOLO Within-image and cross-image contextual\nrelations [44]\n26 MSEG arXiv PyTorch IS PraNet HarDNet and partial decoder [41]\n27 FSSNet arXiv − IS C6 and KS Meta-learning [55]\n28 AG-CUResNeSt RIVF − IS PraNet ResNeSt, attention gates [56]\n29 MPAPS JBHI PyTorch IS DB, KS, and EL Mutual-prototype adaptation network [57]\n30 ResUNet++ JBHI PyTorch IS, VS PraNet and AM ResUNet++, CRF and TTA [58]\n31 NanoNet CBMS PyTorch IS, VS ED, KS, and KCS Real-Time polyp segmentation [59]\n32 ColonSegNet Access PyTorch IS KS Residual block and SENet [37]\n33 Segtran IJCAI PyTorch IS C6 and KS Transformer [60]\n34 DDANet ICPR PyTorch IS KS Dual decoder attention network [40]\n35 UACANet ACM MM PyTorch IS PraNet Uncertainty augmented Context attention\nnetwork [61]\n36 DivergentNet ISBI PyTorch IS EndoCV 2021 Combine multiple models [62]\n37 DWHieraSeg MIA PyTorch IS ES Dynamic-weighting [63]\n38 Transfuse MICCAI PyTorch IS PraNet Transformer and CNN [43]\n39 SANet MICCAI PyTorch IS PraNet Shallow attention network [7]\n40 PNS-Net MICCAI PyTorch VS C6, KS, ES, and AM Progressively normalized self-attention\nnetwork [64]\nPolyp-PVT: Polyp Segmentation with Pyramid Vision Transformers\n \nCAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023 3\nwe  design  a  new  transformer-based  polyp  segmentation\nframework,  which  can  accurately  locate  the  boundaries  of  polyps\neven in extreme scenarios.\n 2    Proposed Polyp-PVT\n 2.1    Overall architecture\nAs  shown  in Fig. 2,  our  Polyp-PVT  consists  of  4  key  modules,\nnamely,  a  pyramid  vision  transformer  (PVT)  encoder,  cascaded\nfusion  module  (CFM),  camouflage  identification  module  (CIM),\nand  similarity  aggregation  module  (SAM).  Specifically,  the  PVT\nextracts  multi-scale  long-range  dependencies  features  from  the\ninput  image.  The  CFM  is  employed  to  collect  semantic  cues  and\nlocate polyps by aggregating high-level features progressively. The\nCIM  is  designed  to  remove  noise  and  enhance  low-level\nrepresentation information of polyps, including texture, color, and\nedges. The SAM is adopted to fuse the low- and high-level features\nprovided  by  the  CIM  and  CFM,  effectively  transmitting  the\ninformation from the pixel-level polyp to the entire polyp.\nI 2 RH\u0002W\u00023\nXi 2 R\nH\n2i+1 \u0002 W\n2i+1 \u0002Ci\nCi 2 f64; 128; 320; 512g\ni 2 f1; 2; 3; 4g\nX2\nX3\nX4\nX\n′\n2\nX\n′\n3\nX\n′\n4\nT1 2 R\nH\n8 \u0002 W\n8 \u000232\nX1\nT2 2 R\nH\n4 \u0002 W\n4 \u000264\nT1\nT2\nF 2 R\nH\n8 \u0002 W\n8 \u000232\nF\n1 \u00021\nP2\nP1\nP2\nmain\naux\nP2\nP1\nGiven an input image , we use the transformer-based\nbackbone[71] to  extract  four  pyramid  features ,\nwhere  and .  Then,  we  adjust\nthe  channel  of  three  high-level  features , ,  and  to  32\nthrough three convolutional units and feed them (i.e., , , and\n )  to  CFM  to  fuse,  leading  a  feature  map .\nMeanwhile,  low-level  features  are  converted  to \nby  the  CIM.  After  that,  the  and  are  aligned  and  fused  by\nSAM, yielding the final feature map . Finally,  is fed\ninto a  convolutional layer to predict the polyp segmentation\nresult .  We  use  the  sum  of  and  as  the  final  prediction.\nDuring training, we optimize the model with a main loss  and\nan auxiliary loss . The main loss is calculated between the final\nsegmentation result  and the ground truth (GT), which is used\nto  optimize  the  final  polyp  segmentation  result.  Similarly,  the\nauxiliary  loss  is  used  to  supervise  the  intermediate  result \ngenerated by the CFM.\n 2.2    Transformer encoder\nDue to uncontrolled factors in their acquisition, polyp images tend\nX1\nX2\nX3\nX4\nX1\nX2\nX3\nX4\nto  contain  significant  noise,  such  as  motion  blur,  rotation,  and\nreflection.  Some  recent  works[78, 79] have  found  that  the  vision\ntransformer[66, 71, 72] demonstrates  stronger  performance  and  better\nrobustness to input disturbances than CNNs[16, 17]. Inspired by this,\nwe  use  a  vision  transformer  as  our  backbone  network  to  extract\nmore  robust  and  powerful  features  for  polyp  segmentation.\nDifferent  from  Refs.  [66, 73]  that  uses  a  fixed “columnar”\nstructure  or  shifted  windowing  manner,  the  PVT[71] is  a  pyramid\narchitecture  whose  representation  is  calculated  with  spatial-\nreduction  attention  operations;  thus  it  enables  to  reduce  the\nresource consumption. Note that the proposed model is backbone-\nindependent;  other  famous  transformer  backbones  are  feasible  in\nour  framework.  Specifically,  we  adopt  the  PVTv2[72],  which  is  the\nimproved version of PVT with a more powerful feature extraction\nability.  To  adapt  PVTv2  to  the  polyp  segmentation  task,  we\nremove  the  last  classification  layer  and  design  a  polyp\nsegmentation  head  on  top  of  four  multi-scale  feature  maps  (i.e.,\n , , ,  and )  generated  by  different  stages.  Among  these\nfeature maps,  gives detailed appearance information of polyps,\nand , , and  provide high-level features.\n 2.3    Cascaded fusion module\n(\u0001)\n3 \u00023\nTo  balance  the  accuracy  and  computational  resources,  we  follow\nrecent  popular  practices[5, 80] to  implement  the  CFM.  Specifically,\nwe  define  as  a  convolutional  unit  composed  of  a \nconvolutional  layer  with  padding  set  to  1,  batch  normalization[81]\nand ReLU[82]. As shown in Fig. 2b, the CFM mainly consists of two\ncascaded parts, as follows:\nX\n′\n4\nX\n′\n3\n1(\u0001)\n2(\u0001)\nX1\n4\nX2\n4\nX1\n4\nX\n′\n3\nX2\n4\n3(\u0001)\nX34 2 R\nH\n16 \u0002 W\n16 \u000232\n(1) In part one, we up-sample the highest-level feature map \nto  the  same  size  as  and  then  pass  the  result  through  two\nconvolutional  units  { and },  yielding  and .  Then,\nwe multiply  and  and concatenate the result with . Finally,\nwe  use  a  convolution  unit  to  smooth  the  concatenated\nfeature,  yielding  fused  feature  map .  The  process\ncan be summarized as Eq. (1).\nX34 = 3(Concat(1(X\n′\n4)⊙X\n′\n3; 2(X\n′\n4))) (1)\n \n⊙\nConcat(\u0001)\nwhere  denotes  the  Hadamard  product,  and  is  the\nconcatenation operation along the channel dimension.\n(2)  As  shown  in  Eq.  (2),  the  second  part  follows  a  similar\n \nConv\nup×2\nup×2\nup×2\nup×2up×4 X34\nP1 P2\nT1\nT2\nConv\nConv\nConv\nC\nConv\n(d) SAM\nC\nConv\nOut Out\nConv\nOut Conv 1×1\nConv 3×3+BN+ReLu\nHadamard product\nC Concatenation\nLmain/Laux\n(b) CFM\n(c) CIM\nCIM\n4 × 4 ×64\n8 × 8 ×128\n16×16×320\n32\nH W X4′\nX3′\nX2′\nX1\nH W\nH W\nH W\nH×W×3\nH′×W′×C′ H′×W′×1 H′×W′×C′1×1×C′\n×32×512\n(a) PVT encoder\nChannel Spatial\nF\nConv\nConv\n \nFig. 2    Framework of our Polyp-PVT, which consists of (a) a pyramid vision transformer (PVT) as the encoder network, (b) cascaded fusion module (CFM) for\nfusing the high-level feature, (c) camouflage identification module (CIM) to filter out the low-level information, and (d) similarity aggregation module (SAM)\nfor integrating the high- and low-level features for the final output.\nCAAI Artificial Intelligence Research\n \n4 CAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023\nX\n′\n4\nX\n′\n3\nX34\nX\n′\n2\n4(\u0001)\n5(\u0001)\n6(\u0001)\nX\n′\n4\nX\n′\n3\nX\n′\n2\nX34\n7(\u0001)\n8(\u0001)\nT1 2 R\nH\n8 \u0002 W\n8 \u000232\nprocess  to  part  one.  Firstly,  we  up-sample , ,  and  to  the\nsame size as , and smooth them using convolutional units ,\n , and , respectively. Then, we multiply the smoothed \nand  with ,  and  concatenate  the  resulting  map  with  up-\nsampled  and  smoothed .  Finally,  we  feed  the  concatenated\nfeature map into two convolutional units (i.e.,  and ) to\nreduce the dimension, and obtain , which is also the\noutput of the CFM.\nT1 = 8(7(Concat(4(X\n′\n4)⊙5(X\n′\n3)⊙X\n′\n2; 6(X34)))) (2)\n \n 2.4    Camouflage identification module\nLow-level  features  often  contain  rich  detail  information,  such  as\ntexture, color, and edges. However, polyps tend to be very similar\nin  appearance  to  the  background.  Therefore,  we  need  a  powerful\nextractor to identify the polyp details.\nX1\nAttc(\u0001)\nAtts(\u0001)\nAs shown in Fig. 2c, we introduce a CIM to capture the details\nof  polyps  from  different  dimensions  of  the  low-level  feature  map\n .  Specifically,  the  CIM  consists  of  a  channel  attention\noperation[83]  and  a  spatial  attention  operation[84] ,\nwhich can be formulated as\nT2 = Atts (Attc (X1)) (3)\n \nAttc(\u0001)\nThe  channel  attention  operation  can  be  written  as\nfollow:\nAttc(x) =σ (1 (Pmax (x))+ 2 (Pavg (x)))⊙x (4)\n \nx\nσ(\u0001)\nPmax(\u0001)\nPavg(\u0001)\ni(\u0001); i 2 f1; 2g\n1 \u00021\n1 \u00021\nAtts(\u0001)\nwhere  is  the  input  tensor  and  is  the  Softmax  function.\n  and  denote  adaptive  maximum  pooling  and\nadaptive  average  pooling  functions,  respectively. \nshares parameters and consists of a convolutional layer with \nkernel size to reduce the channel dimension 16 times, followed by\na ReLU layer and another  convolutional layer to recover the\noriginal channel dimension. The spatial attention operation \ncan be formulated as\nAtts(x) =σ((Concat(Rmax(x); Ravg(x))))⊙x (5)\n \nRmax(\u0001)\nRavg(\u0001)\n(\u0001)\n7 \u00027\nwhere  and  represent  the  maximum  and  average\nvalues obtained along the channel dimension, respectively.  is\na  convolutional layer with padding set to 3.\n 2.5    Similarity aggregation module\nTo  explore  high-order  relations  between  the  lower-level  local\nfeatures  from  CIM  and  higher-level  cues  from  CFM.  We\nintroduce  the  non-local[85, 86] operation  under  graph  convolution\ndomain[87] to  implement  our  SAM.  As  a  result,  SAM  can  inject\ndetailed  appearance  features  into  high-level  semantic  features\nusing global attention.\nT1\nT2\nWθ (\u0001)\nWφ(\u0001)\nT1\nQ 2 R\nH\n8 \u0002 W\n8 \u000216\nK 2 R\nH\n8 \u0002 W\n8 \u000216\n1 \u00021\nGiven  the  feature  map ,  which  contains  high-level  semantic\ninformation,  and  with  rich  appearance  details,  we  fuse  them\nthrough  self-attention.  First,  two  linear  mapping  functions \nand  are  applied  on  to  reduce  the  dimension  and  obtain\nfeature  maps  and .  Here,  we  take  a\nconvolution  operation  with  a  kernel  size  of  as  the  linear\nmapping process. This process can be expressed as follows:\nQ = Wθ (T1); K = Wφ(T1) (6)\n \nT2\nWθ (\u0001)\nWg(\u0001)\nT1\nT\n′\n2 2 R\nH\n8 \u0002 W\n8 \u00021\nF(\u0001)\nFor ,  similar  to ,  we  use  a  convolutional  unit  to\nreduce the channel dimension to 32 and interpolate it to the same\nsize  as .  Then,  we  apply  a  Softmax  function  on  the  channel\ndimension  and  choose  the  second  channel  as  the  attention  map,\nleading to . These operations are represented as \nK\nT\n′\n2\nV 2 R4\u00024\u000216\nin Fig. 3.  Next,  we  calculate  the  Hadamard  product  between \nand . This operation assigns different weights to different pixels,\nincreasing the weight of edge pixels. After that, we use an adaptive\npooling  operation  to  reduce  the  displacement  of  features  and\napply a center crop on it to obtain the feature map . In\nsummary, the process can be formulated as follows:\nV = AP(K ⊙F(Wg(T2))) (7)\n \nAP(\u0001)\nwhere  denotes the pooling and crop operations.\nV\nK\nThen, we establish the correlation between each pixel in  and\n  through an inner product, which is written as follows:\nf = σ(VT \nK) (8)\n \n\nVT\nV\nf\nwhere  denotes the inner product operation,  is the transpose\nof , and  is the correlation attention map.\nf\nQ\nGCN(\u0001)\nG 2 R4\u00024\u000216\nf\nG\nAfter  obtaining  the  correlation  attention  map ,  we  multiply  it\nwith  the  feature  map ,  and  the  result  features  are  fed  to  the\ngraph convolutional layer[86] , leading to . Same\nto  Ref.  [86],  we  calculate  the  inner  product  between  and  as\nEq. (9), reconstructing the graph domain features into the original\nstructural features.\nY′ = fT \nGCN(fT \nQ) (9)\n \nY′\nY\nWz(\u0001)\n1 \u00021\nT1\nZ 2 R\nH\n8 \u0002 W\n8 \u000232\nThe  reconstructed  feature  map  is  adjusted  to  the  same\nchannel  sizes  with  by  a  convolutional  layer  with \nkernel  size,  and  then  combined  with  the  feature  to  obtain  the\nfinal output  of the SAM. Equation (10) summarizes\nthe details of this process.\nZ = T1 +Wz(Y′) (10)\n \n 2.6    Loss function\nOur loss function can be formulated as Eq. (11).\n = main +aux (11)\n \nmain\naux\nwhere  and  are  the  main  loss  and  auxiliary  loss,\n \nQ\nY′\nK V\nSoftmax\nGCN(·)\nZ\nf\nf\nElement-wise addition\nMatrix multiplication\nT1\nT2′\nT2\nWθ: 1×1\nWZ: 1×1\nWϕ: 1×1\nWg: 1×1\nF(·)\nAP(·)\nH/8×W/8×16\nH/8×W/8×16\n4×4×16\n4×4×16\nH/8×W/8×32\nH/8×W/8×32\nH/8×W/8×4×4\nH/8×W/8×16H/8×W/8×16\nH/8×W/8×32\nH/8×W/8×1\nH/4×W/4×32\nH/4×W/4×64\n4×4×16\n \nFig. 3    Details  of  the  introduced  SAM.  It  is  composed  of  GCN  and  non-\nlocal,  which  extend  the  pixel  features  of  polyp  regions  with  high-level\nsemantic location cues to the entire region.\nPolyp-PVT: Polyp Segmentation with Pyramid Vision Transformers\n \nCAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023 5\nmain\nP2\nG\nrespectively.  The  main  loss  is  calculated  between  the  final\nsegmentation result  and ground truth , which can be written\nas\nmain = w\nIoU(P2; G)+ w\nBCE(P2; G) (12)\n \naux\nP1\nG\nThe  auxiliary  loss  is  calculated  between  the  intermediate\nresult  from  the  CFM  and  ground  truth ,  which  can  be\nformulated as\naux = w\nIoU(P1; G)+ w\nBCE(P1; G) (13)\n \nw\nIoU(\u0001)\nw\nBCE(\u0001)\nw\nBCE(\u0001)\nw\nIoU(\u0001)\nwhere  and  are the weighted intersection over union\n(IoU)  loss[88] and  weighted  binary  cross  entropy  (BCE)  loss[88],\nwhich restrict the prediction map in terms of the global structure\n(object-level) and local details (pixel-level) perspectives. Unlike the\nstandard BCE loss function, which treats all pixels equally, \nconsiders the importance of each pixel and assigns higher weights\nto  hard  pixels.  Furthermore,  compared  to  the  standard  IoU  loss,\n  pays more attention to the hard pixels.\n 2.7    Implementation details\n352 \u0002352\n352 \u0002352\nWe implement our Polyp-PVT with the PyTorch framework and\nuse  a  Tesla  P100  to  accelerate  the  calculations.  Considering  the\ndifferences in the sizes of each polyp image, we adopt a multi-scale\nstrategy[5, 41] in the training stage. The hyperparameter details are as\nfollows. To update the network parameters, we use the AdamW[89]\noptimizer, which is widely used in transformer networks[71−73]. The\nlearning rate (lr) is set to 10−4 and the weight decay is adjusted to\n10−4 too.  Further,  we  resize  the  input  images  to  with  a\nmini-batch  size  of  16  for  100  epochs.  More  details  about  the\ntraining loss cures, parameter setting, and network parameters are\nshown  in Fig. 4, Tables  2 and 3,  respectively.  The  total  training\ntime  is  nearly  3  hours  to  achieve  the  best  (e.g.,  30  epochs)\nperformance.  For  testing,  we  only  resize  the  images  to \nwithout any post-processing optimization strategies.\n 3    Experiment\n 3.1    Evaluation metrics\nFw\nβ\nSα\nEξ\nWe  employ  six  widely-used  evaluation  metrics,  including  Dice[90],\nIoU,  mean  absolute  error  (MAE),  weighted  F-measure  ()[91],  S-\nmeasure  ()[92],  and  E-measure  ()[93, 94] to  evaluate  the  model\nFw\nβ\nSα\nEξ\nEm\nξ\nEmax\nξ\nperformances.  Among  these  metrics,  Dice  and  IoU  are  similarity\nmeasures at the regional level, which mainly focus on the internal\nconsistency of segmented objects. Here, we report the mean value\nof Dice and IoU, denoted as mDic and mIoU, respectively. MAE\nis  a  pixel-by-pixel  comparison  indicator  that  represents  the\naverage  value  of  the  absolute  error  between  the  predicted  value\nand  the  true  value.  Weighted  F-measure  ()  comprehensively\nconsiders  the  recall  and  precision  and  eliminates  the  effect  of\nconsidering  each  pixel  equally  in  conventional  indicators.  S-\nmeasure  ()  focuses  on  the  structural  similarity  of  target\nprospects at the region and object level. E-measure () is used to\nevaluate the segmentation results at the pixel and image level. We\nreport the mean and max value of E-measure, denoted as  and\n ,  respectively.  The  evaluation  toolbox  is  derived  from\nhttps://github.com/DengPingFan/PraNet.\n 3.2    Datasets and compared model\nDataset. Following the experimental setups in PraNet[5], we adopt\nfive  challenging  public  datasets,  including  Kvasir-SEG[13],\nClinicDB[8],  ColonDB[10],  Endoscene[14],  and  ETIS[9] to  verify  the\neffectiveness of our framework.\nModel. We collect several open source models from the field of\npolyp  segmentation,  for  a  total  of  nine  comparative  models,\nincluding  U-Net[4],  UNet++[28],  PraNet[5],  SFA[95],  MSEG[41],\nACSNet[49],  DCRNet[44],  EU-Net[52],  and  SANet[7].  For  a  fair\ncomparison, we use their open-source codes to evaluate the same\ntraining  and  testing  sets.  Note  that  the  SFA  results  are  generated\nusing the released test model.\n 3.3    Analysi of learning ability\nSettings. We  use  the  ClinicDB  and  Kvasir-SEG  datasets  to\nevaluate  the  learning  ability  of  the  proposed  model.  ClinicDB\ncontains  612  images,  which  are  extracted  from  31  colonoscopy\nvideos. Kvasir-SEG is collected from the polyp class in the Kvasir\ndataset  and  includes  1000  polyp  images.  Following  PraNet,  we\nadopt  the  same  900  and  548  images  from  ClinicDB  and  Kvasir-\nSEG  datasets  as  the  training  set,  and  the  remaining  64  and  100\nimages are employed as the respective test sets.\nResults. As can be seen in Table 4, our model is superior to the\ncurrent  methods,  demonstrating  that  it  has  a  better  learning\nability. On the Kvasir-SEG dataset, the mDic score of our model is\n1.3% higher than that of the second-best model, SANet, and 1.9%\nhigher  than  that  of  PraNet.  On  the  ClinicDB  dataset,  the  mDic\nscore  of  our  model  is  2.1%  higher  than  that  of  SANet  and  3.8%\nhigher than that of PraNet.\n 3.4    Analysis of generalization ability\nSettings. To  verify  the  generalization  performance  of  the  model,\nwe test it on three unseen (i.e., Polycentric) datasets, namely ETIS,\nColonDB,  and  EndoScene.  There  are  196  images  in  ETIS,  380\nimages  in  ColonDB,  and  60  images  in  EndoScene.  It  is  worth\nnoting  that  the  images  in  these  datasets  belong  to  different\nmedical  centers.  In  other  words,  the  model  has  not  seen  their\ntraining  data,  which  is  different  from  the  verification  methods  of\nClinicDB and Kvasir-SEG.\nResults. The  results  are  shown  in Tables  5 and 6.  As  can  be\nseen, our Polyp-PVT achieves a good generalization performance\ncompared  with  the  existing  models.  And  our  model  generalizes\n \n0 200 400 600 800 1000 1200 1400\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0Loss value\nIteration\n Auxiliary loss, lr=10−3, decay_rate=0.5\n Main loss, lr=10−3, decay_rate=0.5\n Total loss, lr=10−3, decay_rate=0.5\n Auxiliary loss, lr=10−4, decay_rate=0.1\n Main loss, lr=10−4, decay_rate=0.1\n Total loss, lr=10−4, decay_rate=0.1\n \nFig. 4    Loss curves under different training parameter settings.\n \nTable 2    Parameter setting during the training stage.\nOptimizer Learning rate Multi-scale Clip Decay rate Weight decay Number of epochs Input size\nAdamW 10−4 [0.75, 1, 1.25] 0.5 0.1 10−4 100 352×352\nCAAI Artificial Intelligence Research\n \n6 CAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023\neasily  to  multicentric  (or  unseen)  data  with  different\ndomains/distributions.  On  ColonDB,  it  is  ahead  of  the  second-\nbest  SANet  and  classical  PraNet  by  5.5%  and  9.6%,  respectively.\nOn  ETIS,  we  exceed  the  SANet  and  PraNet  by  3.7%  and  15.9%,\nrespectively.  In  addition,  on  EndoScene,  our  model  is  better  than\nSANet  and  PraNet  by  1.2%  and  2.9%,  respectively.  Moreover,  to\nprove the generalization ability of Polyp-PVT, we present the max\nDice  results  in Fig. 5,  where  our  model  shows  a  steady\nimprovement  on  both  ColonDB  and  ETIS.  In  addition,  we  show\nthe standard deviation (SD) of the mDic between our model and\nothers  in Table  7.  As  seen,  there  is  not  much  difference  in  SD\nbetween our model and the comparison model, and they are both\nstable and balanced.\n 3.5    Qualitative analysis\nFigures  6 and 7 show  the  visualization  results  of  our  model  and\nthe  compared  models.  We  find  that  our  results  have  two\nadvantages.\n● Our model is able to adapt to data under different conditions.\nThat is, it maintains a stable recognition and segmentation ability\nunder  different  acquisition  environments  (different  lighting,\ncontrast, reflection, motion blur, small objects, and rotation).\n● The  model  segmentation  results  have  internal  consistency\nand predicted edges are closer to the ground-truth labels. We also\nprovide  FROC  curves  on  ColonDB  in Fig. 8,  and  our  result  is  at\nthe top, indicating that our effect achieves the best.\n 3.6    Ablation study\nWe describe in detail the effectiveness of each component on the\noverall  model.  The  training,  testing,  and  hyperparameter  settings\nare the same as mentioned in Section 2.7. The results are shown in\nTable 8.\nComponents. We  use  PVTv2[72] as  our  baseline  and  evaluate\nmodule effectiveness by removing or replacing components from\nthe  complete  Polyp-PVT  and  comparing  the  variants  with  the\nstandard  version.  The  standard  version  is  denoted  as  Polyp-PVT\n(PVT+CFM+CIM+SAM),  where  CFM,  CIM,  and  SAM  indicate\nthe usage of the CFM, CIM, and SAM, respectively.\nEffectiveness of CFM. To analyze the effectiveness of the CFM,\na version of Polyp-PVT (without CFM) is trained. Table 8 shows\nthat the model without the CFM drops sharply on all five datasets\ncompared  to  the  standard  Polyp-PVT.  In  particular,  the  mDic  is\nreduced from 0.937 to 0.915 on ClinicDB.\n \nTable 3    Network  parameters  of  each  module.  Note  that  the  encoder\nparameters  are  the  same  as  PVT  without  any  changes.  BasicConv2d  and\nConv2d  are  with  the  parameters  [in_channel,  out_channel,  kernel_size,\npadding] and GCN [num_state, num_node].\nModule Parameter Value\nEncoder\npatch_size [4]\nembed_dims [64, 128, 320, 512]\nnum_heads [1, 2, 5, 8]\nmlp_ratios [8, 8, 4, 4]\ndepths [3, 4, 18, 3]\nsr_ratios [8, 4, 2, 1]\ndrop_rate [0]\ndrop_path_rate [0.1]\nSAM\nAvgPool2d [6]\nConv2d [32, 16, 1, 1]\nConv2d [32, 16, 1, 1]\nConv2d [16, 32, 1, 1]\nGCN [16, 16]\nBasicConv2d [64, 32, 1, 0]\nCFM\nBasicConv2d [32, 32, 3, 1]\nBasicConv2d [32, 32, 3, 1]\nBasicConv2d [32, 32, 3, 1]\nBasicConv2d [32, 32, 3, 1]\nBasicConv2d [64, 64, 3, 1]\nBasicConv2d [64, 64, 3, 1]\nBasicConv2d [96, 96, 3, 1]\nBasicConv2d [96, 32, 3, 1]\nCIM\nAvgPool2d [1]\nAvgPool2d [1]\nConv2d [64, 4, 1, 0]\nReLU −\nConv2d [4, 64, 1, 0]\nSigmoid −\nConv2d [2, 1, 7, 3]\nSigmoid −\n \nTable 4    Quantitative results of the test datasets, i.e., Kvasir-SEG and ClinicDB.\nModel\nKvasir-SEG[13] ClinicDB[8]\nmDic mIoU\nFw\nβ \nSα \nEm\nξ \nEmax\nξ MAE mDic mIoU\nFw\nβ \nSα \nEm\nξ \nEmax\nξ MAE\nU-Net 0.818 0.746 0.794 0.858 0.881 0.893 0.055 0.823 0.755 0.811 0.889 0.913 0.954 0.019\nUNet++ 0.821 0.743 0.808 0.862 0.886 0.909 0.048 0.794 0.729 0.785 0.873 0.891 0.931 0.022\nSFA 0.723 0.611 0.670 0.782 0.834 0.849 0.075 0.700 0.607 0.647 0.793 0.840 0.885 0.042\nMSEG 0.897 0.839 0.885 0.912 0.942 0.948 0.028 0.909 0.864 0.907 0.938 0.961 0.969 0.007\nDCRNet 0.886 0.825 0.868 0.911 0.933 0.941 0.035 0.896 0.844 0.890 0.933 0.964 0.978 0.010\nACSNet 0.898 0.838 0.882 0.920 0.941 0.952 0.032 0.882 0.826 0.873 0.927 0.947 0.959 0.011\nPraNet 0.898 0.840 0.885 0.915 0.944 0.948 0.030 0.899 0.849 0.896 0.936 0.963 0.979 0.009\nEU-Net 0.908 0.854 0.893 0.917 0.951 0.954 0.028 0.902 0.846 0.891 0.936 0.959 0.965 0.011\nSANet 0.904 0.847 0.892 0.915 0.949 0.953 0.028 0.916 0.859 0.909 0.939 0.971 0.976 0.012\nPolyp-PVT (Ours) 0.917 0.864 0.911 0.925 0.956 0.962 0.023 0.937 0.889 0.936 0.949 0.985 0.989 0.006\nPolyp-PVT: Polyp Segmentation with Pyramid Vision Transformers\n \nCAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023 7\nEffectiveness  of  CIM. To  demonstrate  the  ability  of  the  CIM,\nwe  also  remove  it  from  Polyp-PVT,  denoting  this  as  Polyp-PVT\n(without CIM). As shown in Table 8, this variant performs worse\nthan  the  overall  Polyp-PVT.  Specifically,  removing  the  CIM\ncauses the mDic to decrease by 1.8% on Endoscene. Meanwhile, it\nis  obvious  that  the  lack  of  the  CIM  introduces  significant  noise\n(see Fig. 9).  In  order  to  further  explore  the  internal  of  CIM,  the\nfeature  visualizations  of  the  two  main  configurations  inside  the\nCIM are shown in Fig. 10. It can be seen that the low-level features\nhave  a  large  amount  of  detailed  information.  Still,  the  differences\nbetween polyps and other normal tissues cannot be mined directly\nfrom this information. Thanks to the channel attention and spatial\nattention  mechanism,  information  such  as  details  and  edges  of\npolyps  can  be  discerned  from  a  large  amount  of  redundant\ninformation.\nEffectiveness of SAM. Similarly, we test the effectiveness of the\nSAM  module  by  removing  it  from  the  overall  Polyp-PVT  and\nreplacing  it  with  an  element-wise  addition  operation,  which  is\ndenoted  as  Polyp-PVT  (without  SAM).  The  performance  of  the\ncomplete Polyp-PVT shows an improvement of 2.9% and 3.1% in\nterms  of  mDic  and  mIoU,  respectively,  on  ColonDB. Figure  9\nshows  the  benefits  of  SAM  more  intuitively.  It  is  found  that  the\nlack  of  the  SAM  leads  to  more  detailed  errors  or  even  missed\ninspections.  As  reported  in Table  9,  we  add  more  results  on  the\nGCN  in  the  SAM  module.  The  experimental  results  further\nillustrate that GCN plays a key role. The effect of the lack of GCN\nis significantly reduced, and the effect is improved after replacing\nit  with  convolution.  Still,  GCN  can  significantly  exceed  the\n \nTable 5    Quantitative results of the test dataset Endoscene. The SFA result is generated using the published code.\nModel mDic mIoU\nFw\nβ \nSα \nEm\nξ \nEmax\nξ MAE\nU-Net 0.710 0.627 0.684 0.843 0.847 0.875 0.022\nUNet++ 0.707 0.624 0.687 0.839 0.834 0.898 0.018\nSFA 0.467 0.329 0.341 0.640 0.644 0.817 0.065\nMSEG 0.874 0.804 0.852 0.924 0.948 0.957 0.009\nACSNet 0.863 0.787 0.825 0.923 0.939 0.968 0.013\nDCRNet 0.856 0.788 0.830 0.921 0.943 0.960 0.010\nPraNet 0.871 0.797 0.843 0.925 0.950 0.972 0.010\nEU-Net 0.837 0.765 0.805 0.904 0.919 0.933 0.015\nSANet 0.888 0.815 0.859 0.928 0.962 0.972 0.008\nPolyp-PVT (Ours) 0.900 0.833 0.884 0.935 0.973 0.981 0.007\n \nTable 6    Quantitative results of the test datasets ColonDB and ETIS. The SFA result is generated using the published code.\nModel\nColonDB[10] ETIS[9]\nmDic mIoU\nFw\nβ \nSα \nEm\nξ \nEmax\nξ MAE mDic mIoU\nFw\nβ \nSα \nEm\nξ \nEmax\nξ MAE\nU-Net 0.512 0.444 0.498 0.712 0.696 0.776 0.061 0.398 0.335 0.366 0.684 0.643 0.740 0.036\nUNet++ 0.483 0.410 0.467 0.691 0.680 0.760 0.064 0.401 0.344 0.390 0.683 0.629 0.776 0.035\nSFA 0.469 0.347 0.379 0.634 0.675 0.764 0.094 0.297 0.217 0.231 0.557 0.531 0.632 0.109\nACSNet 0.716 0.649 0.697 0.829 0.839 0.851 0.039 0.578 0.509 0.530 0.754 0.737 0.764 0.059\nMSEG 0.735 0.666 0.724 0.834 0.859 0.875 0.038 0.700 0.630 0.671 0.828 0.854 0.890 0.015\nDCRNet 0.704 0.631 0.684 0.821 0.840 0.848 0.052 0.556 0.496 0.506 0.736 0.742 0.773 0.096\nPraNet 0.712 0.640 0.699 0.820 0.847 0.872 0.043 0.628 0.567 0.600 0.794 0.808 0.841 0.031\nEU-Net 0.756 0.681 0.730 0.831 0.863 0.872 0.045 0.687 0.609 0.636 0.793 0.807 0.841 0.067\nSANet 0.753 0.670 0.726 0.837 0.869 0.878 0.043 0.750 0.654 0.685 0.849 0.881 0.897 0.015\nPolyp-PVT (Ours) 0.808 0.727 0.795 0.865 0.913 0.919 0.031 0.787 0.706 0.750 0.871 0.906 0.910 0.013\n \n0.521\n0.5660.549\n0.728\n0.756\n0.7280.713\n0.763 0.759\n0.814\n0.343\n0.444\n0.509\n0.639\n0.711\n0.603 0.584\n0.715\n0.765\n0.791\n0.33\n0.38\n0.43\n0.48\n0.53\n0.58\n0.63\n0.68\n0.73\n0.78\n0.83Max Dice\nColonDB\nSFAUnet\nUnet++PraNetMSEGACSNetDCRNetEU-NetSANetOurs\nETIS\n \nFig. 5    Evaluation of model generalization ability. We provide the max Dice\nresults on ColonDB and ETIS.\nCAAI Artificial Intelligence Research\n \n8 CAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023\ncapabilities  of  the  convolution  module.  The  experimental  results\nalso  verified  the  importance  of  GCN’s  large  receptive  field  and\nrotation  insensitivity  to  polyp  segmentation.  The  rotational\nrobustness  of  GCN  is  stronger  than  convolutions.  As  shown  in\nTable 10, under the condition of large rotation (15 degrees), GCN\nhas  better  adaptability  to  image  rotation  than  convolutions.  To\nfurther explore the role of SAM, we visualized P1 and P2, and the\nresults of P1 and P2 are shown in Fig. 11. Compared with P1, P2\nhas  higher  reliability  in  error  recognition  and  identification  of\nuncertain regions. This is mainly due to the large number of low-\nlevel  details  collected  by  CIM  and  mining  local  pixels  and  global\nsemantic cues from the polyp area of SAM.\n 3.7    Video polyp segmentation\nTo  validate  the  superiority  of  the  proposed  model,  we  conduct\nexperiments  on  the  video  polyp  segmentation  datasets.  For  a  fair\ncomparison,  we  re-train  our  model  with  the  same  training\ndatasets and use the same testing set as PNS-Net[64, 96]. We compare\nour  model  on  three  standard  benchmarks  (i.e.,  CVC-300-TV[97],\nCVC-612-T[8],  and  CVC-612-V[8])  against  six  cutting-edge\napproaches,  including  U-Net[4],  UNet++[28],  ResUNet++[29],\nACSNet[49],  PraNet[5],  and  PNS-Net[64],  in Tables  11 and 12.  Note\nthat  PNS-Net  provides  all  the  prediction  maps  of  the  compared\nmethods.  As  seen,  our  method  is  very  competitive  and  far  ahead\nof the best existing model, PNS-Net, by 3.1% and 6.7% on CVC-\n612-V and CVC-300-TV, respectively, in terms of mDic.\n 3.8    Limitation\nAlthough  the  proposed  Polyp-PVT  model  surpasses  existing\nalgorithms,  it  still  performs  poorly  in  certain  cases.  We  present\n \nTable 7    Standard deviation (SD) of the mDic of our model and the comparison models.\nModel\nmDic±SD\nKvasir-SEG ClinicDB ColonDB ETIS Endoscene\nU-Net 0.818±0.039 0.823±0.047 0.483±0.034 0.398±0.033 0.710±0.049\nUNet++ 0.821±0.040 0.794±0.044 0.456±0.037 0.401±0.057 0.707±0.053\nSFA 0.723±0.052 0.701±0.054 0.444±0.037 0.297±0.025 0.468±0.050\nMSEG 0.897±0.041 0.910±0.048 0.735±0.039 0.700±0.039 0.874±0.051\nACSNet 0.898±0.045 0.882±0.048 0.716±0.040 0.578±0.035 0.863±0.055\nDCRNet 0.886±0.043 0.896±0.049 0.704±0.039 0.556±0.039 0.857±0.052\nPraNet 0.898±0.041 0.899±0.048 0.712±0.038 0.628±0.036 0.871±0.051\nEU-Net 0.908±0.042 0.902±0.048 0.756±0.040 0.687±0.039 0.837±0.049\nSANet 0.904±0.042 0.916±0.049 0.752±0.040 0.750±0.047 0.888±0.054\nPolyp-PVT (Ours) 0.917±0.042 0.937±0.050 0.808±0.043 0.787±0.044 0.900±0.052\n \nImage Ground truth Ours SANet PraNet ACSNer DCRNet\n \nFig. 6    Visualization results with the current models. Green indicates a correct polyp. Yellow is the missed polyp. Red is the wrong prediction. As we can see,\nthe proposed model can accurately locate and segment polyps, regardless of size.\nPolyp-PVT: Polyp Segmentation with Pyramid Vision Transformers\n \nCAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023 9\nsome failure cases in Fig. 12. As can be seen, one major limitation\nis  the  inability  to  detect  accurate  polyp  boundaries  with\noverlapping  light  and  shadow  (1st  row).  Our  model  can  identify\nthe location information of polyps (green mask in 1st row), but it\nregards  the  light  and  shadow  part  of  the  edge  as  the  polyp  (red\nmask in 1st row). More deadly, our model incorrectly predicts the\nreflective  point  as  a  polyp  (red  mask  in  2nd  and  3rd  rows).  We\nnotice  that  the  reflective  points  are  very  salient  in  the  image.\nTherefore, we speculate that the prediction may be based on only\nthese points. More importantly, we believe that a simple way is to\nconvert  the  input  image  into  a  gray  image,  which  can  eliminate\nthe reflection and overlap of light and shadow to assist the model\nin judgment.\n 4    Conclusion\nIn  this  paper,  we  propose  a  new  image  polyp  segmentation\nframework,  named  Polyp-PVT,  which  utilizes  a  pyramid  vision\ntransformer  backbone  as  the  encoder  to  explicitly  extract  more\npowerful  and  robust  features.  Extensive  experiments  show  that\nPolyp-PVT  consistently  outperforms  all  current  cutting-edge\nmodels  on  five  challenging  datasets  without  any  pre-/post-\nprocessing.  In  particular,  for  the  unseen  ColonDB  dataset,  the\n \nImage Ground truth Ours EU-Net HarDNet SFA U-Net UNet++\n \nFig. 7    Visualization results with the current models.\n \n1.0\nACSNet DCRNet\nEU-Net MSEG\nPraNet SANet\nSFA UNet\nUNet++ PolypPVT\n0.8\n0.6 True positive rate\n0.4\n0.2\n0\nAverage number of false positives per image\n2500 5000 750010 00012 50015 00017 50020 000\n \nFig. 8    FROC curves of different methods on ColonDB.\n \nTable 8    Quantitative results for ablation studies.\nDataset Metric Baseline Without CFM Without CIM Without SAM Final\nEndoscene\nmDic 0.869 0.892 0.882 0.874 0.900\nmIoU 0.792 0.826 0.808 0.801 0.833\nClinicDB\nmDic 0.903 0.915 0.930 0.930 0.937\nmIoU 0.847 0.865 0.881 0.877 0.889\nColonDB\nmDic 0.796 0.802 0.805 0.779 0.808\nmIoU 0.707 0.721 0.724 0.696 0.727\nETIS\nmDic 0.759 0.771 0.785 0.778 0.787\nmIoU 0.668 0.690 0.711 0.693 0.706\nKvasir-SEG\nmDic 0.910 0.922 0.910 0.910 0.917\nmIoU 0.856 0.872 0.858 0.853 0.864\nCAAI Artificial Intelligence Research\n \n10 CAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023\nproposed  model  reaches  a  mean  Dice  score  of  above  0.8  for  the\nfirst  time.  Interestingly,  we  also  surpass  the  current  cutting-edge\nPNS-Net  in  terms  of  the  video  polyp  segmentation  task,\ndemonstrating excellent learning ability. Specifically, we obtain the\nabove-mentioned  achievements  by  introducing  three  simple\ncomponents,  i.e.,  a  cascaded  fusion  module,  a  camouflage\nidentification module, and a similarity aggregation module, which\neffectively  extract  high  and  low-level  cues  separately,  and\neffectively  fuse  them  for  the  final  output.  We  hope  this  research\nwill stimulate more novel ideas for solving the polyp segmentation\ntask.\n \nOursWithout SAMWithout CIMWithout CFMBaselineGround truthImage\n \nFig. 9    Visualization of the ablation study results, which are converted from the output into heat maps. As can be seen, removing any module leads to missed or\nincorrectly detected results.\n \nImage Ground truth Input feature Channel attention Spatial attention\n \nFig. 10    Visualization of the feature map in the CIM module.\n \nTable 9    Ablation  study  of  GCN  in  the  SAM  module.  The  mDic  scores  are\nprovided.\nSetting Endoscene ClinicDB ColonDB ETIS Kvasir-SEG\nWithout GCN 0.876 0.928 0.784 0.725 0.894\nWith Conv 0.894 0.919 0.787 0.742 0.909\nWith GCN 0.900 0.937 0.808 0.787 0.917\n \nTable 10    Ablation  experiments  of  the  powerful  rotation  adaptability.  All\nexperiments  are  under  the  condition  of  large  rotation  (15  degrees).  The\nmDic scores are provided.\nSetting Endoscene ClinicDB ColonDB ETIS Kvasir-SEG\nWithout GCN 0.857 0.909 0.756 0.667 0.894\nWith Conv 0.865 0.898 0.789 0.719 0.893\nWith GCN 0.874 0.929 0.806 0.744 0.915\n \nImage Ground truth P1\n P2\n \nFig. 11    Visualization of the P1 and P2 predictions.\nPolyp-PVT: Polyp Segmentation with Pyramid Vision Transformers\n \nCAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023 11\nArticle History\nReceived: 6 December 2022; Revised: 10 February 2023; Accepted:\n22 March 2023\nReferences\n M. Fiori, P. Musé, and G. Sapiro, A complete system for candidate\npolyps detection in virtual colonoscopy, Int.  J.  Patt.  Recogn.  Artif.\nIntell., vol. 28, no. 7, p. 1460014, 2014.\n[1]\n A.  V.  Mamonov,  I.  N.  Figueiredo,  P.  N.  Figueiredo,  and  Y.  H.\nRichard  Tsai,  Automated  polyp  detection  in  colon  capsule\nendoscopy, IEEE Trans. Med. Imag., vol. 33, no. 7, pp. 1488–1502,\n2014.\n[2]\n O. H. Maghsoudi. Superpixel based segmentation andclassification[3]\nof polyps in wireless capsule endoscopy, in Proc. 2017 IEEE Signal\nProcessing  in  Medicine  and  Biology  Symposium  (SPMB),\nPhiladelphia, PA, USA, 2017, pp. 1–4.\n O.  Ronneberger,  P.  Fischer,  and  T.  Brox,  U-net:  Convolutional\nnetworks  for  biomedical  image  segmentation,  in  Proc.  18th  Int.\nConf.  Medical  Image  Computing  and  Computer  Assisted\nIntervention, Munich, Germany, 2015, pp. 234–241.\n[4]\n D. P. Fan, G. P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao,\nPraNet: parallel reverse attention network for polyp segmentation, in\nProc  23th  Int.  Conf.  Medical  Image  Computing  and  Computer\nAssisted Intervention, Lima, Peru, 2020, pp. 263–273.\n[5]\n X.  Guo,  C.  Yang,  Y.  Liu,  and  Y.  Yuan,  Learn  to  threshold:\nThresholdNet  with  confidence-guided  manifold  mixup  for  polyp\nsegmentation,  IEEE  Trans.  Med.  Imag.,  vol.  40,  no.  4,  pp.\n[6]\n \nTable 11    Result of video polyp segmentation on CVC-612-T and CVC-612-V.\nModel\nCVC-612-T[8] CVC-612-V[8]\nmDic mIoU\nFw\nβ \nSα \nEm\nξ \nEmax\nξ MAE mDic mIoU\nFw\nβ \nSα \nEm\nξ \nEmax\nξ MAE\nU-Net 0.711 0.618 0.694 0.810 0.836 0.853 0.058 0.709 0.597 0.680 0.826 0.855 0.872 0.023\nUNet++ 0.697 0.603 0.688 0.800 0.817 0.865 0.059 0.668 0.557 0.642 0.805 0.830 0.846 0.025\nResUNet++ 0.616 0.512 0.604 0.727 0.758 0.760 0.084 0.750 0.646 0.717 0.829 0.877 0.879 0.023\nACSNet 0.780 0.697 0.772 0.838 0.864 0.866 0.053 0.801 0.710 0.765 0.847 0.887 0.890 0.054\nPraNet 0.833 0.767 0.834 0.886 0.904 0.926 0.038 0.857 0.793 0.855 0.915 0.936 0.965 0.013\nPNS-Net 0.837 0.765 0.838 0.903 0.903 0.923 0.038 0.851 0.769 0.836 0.923 0.944 0.962 0.012\nPolyp-PVT (Ours) 0.846 0.776 0.850 0.895 0.908 0.926 0.037 0.882 0.810 0.874 0.924 0.963 0.967 0.012\n \nTable 12    Video polyp segmentation results on the CVC-300-TV[96].\nModel mDic mIoU\nFw\nβ \nSα \nEm\nξ \nEmax\nξ MAE\nU-Net 0.631 0.516 0.567 0.793 0.826 0.849 0.027\nUNet++ 0.638 0.527 0.581 0.796 0.831 0.847 0.024\nResUNet++ 0.533 0.410 0.469 0.703 0.718 0.720 0.052\nACSNet 0.732 0.627 0.703 0.837 0.871 0.875 0.016\nPraNet 0.716 0.624 0.700 0.833 0.852 0.904 0.016\nPNS-Net 0.813 0.710 0.778 0.909 0.921 0.942 0.013\nPolyp-PVT (Ours) 0.880 0.802 0.869 0.915 0.961 0.965 0.011\n \nPraNetSANetOursGround truthImage\n \nFig. 12    Visualization of some failure cases. Green indicates a correct polyp. Yellow is the missed polyp. Red is the wrong prediction.\nCAAI Artificial Intelligence Research\n \n12 CAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023\n1134–1146, 2021.\n J. Wei, Y. Hu, R. Zhang, Z. Li, S. K. Zhou, and S. Cui, Shallow\nattention  network  for  polyp  segmentation,  arXiv  preprint  arXiv:\n2108.00882, 2021.\n[7]\n J.  Bernal,  F.  J.  Sánchez,  G.  Fernández-Esparrach,  D.  Gil,  C.\nRodríguez, and F. Vilariño, WM-DOVA maps for accurate polyp\nhighlighting  in  colonoscopy:  Validation  vs.  saliency  maps  from\nphysicians, Comput. Med. Imag. Graph., vol. 43, pp. 99–111, 2015.\n[8]\n J. Silva, A. Histace, O. Romain, X. Dray, and B. Granado, Toward\nembedded detection of polyps in WCE images for early diagnosis of\ncolorectal cancer, Int. J. Comput. Assist. Radiol. Surg., vol. 9, no. 2,\npp. 283–293, 2014.\n[9]\n N.  Tajbakhsh,  S.  R.  Gurudu,  and  J.  Liang,  Automated  polyp\ndetection  in  colonoscopy  videos  using  shape  and  context\ninformation, IEEE Trans. Med. Imag., vol. 35, no. 2, pp. 630–644,\n2016.\n[10]\n D. P. Fan, G. P. Ji, M. M. Cheng, and L. Shao, Concealed object\ndetection, IEEE Trans. Pattern Anal. Mach. Intell, vol. 44, no. 10, p.\n6024, 6042.\n[11]\n D. P. Fan, G. P. Ji, M. M. Cheng, and L. Shao, Concealed object\ndetection, in Proc. 2020 IEEE/CVF Conference on Computer Vision\nand  Pattern  Recognition  (CVPR),  Seattle,  WA,  USA,  2020,  pp.\n2774–2784.\n[12]\n D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. de Lange,\nD. Johansen, and H. D. Johansen, Kvasir-SEG: A segmented polyp\ndataset,  in  Proc.  26th  Int.  Conf.  Multimedia  Modeling,  Daejeon,\nKorea, 2020, pp. 451–462.\n[13]\n D. Vázquez, J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, A. M.\nLópez, A. Romero, M. Drozdzal, and A. Courville, A benchmark for\nendoluminal scene segmentation of colonoscopy images, J. Healthc.\nEng., vol. 2017, pp. 1–9, 2017.\n[14]\n T. Rahim, M. A. Usman, and S. Y. Shin, A survey on contemporary\ncomputer-aided  tumor,  polyp,  and  ulcer  detection  methods  in\nwireless capsule endoscopy imaging, Comput.  Med.  Imag.  Graph.,\nvol. 85, p. 101767, 2020.\n[15]\n K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for\nimage recognition, arXiv preprint arXiv: 1512.03385, 2015.\n[16]\n K. Simonyan and A. Zisserman, Very deep convolutional networks\nfor  large-scale  image  recognition,  in  Proc.  3rd  International\nConference  on  Learning  Representations,  San  Diego,  CA,  USA,\n2014.\n[17]\n X. Li, W. Wang, X. Hu, and J. Yang, Selective kernel networks, in\nProc. 2019 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), Long Beach, CA, USA, 2019, pp. 510–519.\n[18]\n W. Wang, X. Li, J. Yang, and T. Lu, Mixed link networks, in Proc.\n27th  Int.  Joint  Conf.  Artificial  Intelligence,  Stockholm,  Sweden,\n2018, pp. 2819–2825.\n[19]\n J. Long, E. Shelhamer, and T. Darrell, Fully convolutional networks\nfor  semantic  segmentation,  in  Proc.  2015  IEEE  Conf.  Computer\nVision and Pattern Recognition (CVPR), Boston, MA, USA, 2015,\npp. 3431–3440.\n[20]\n L. Cai, M. Wu, L. Chen, W. Bai, M. Yang, S. Lyu, and Q. Zhao,\nUsing  guided  self-attention  with  local  information  for  polyp\nsegmentation, in Proc.  25th  Int.  Conf.  Medical  Image  Computing\nand Computer Assisted Intervention, Singapore, 2022, pp. 629–638.\n[21]\n N. K. Tomar, D. Jha, U. Bagci, and S. Ali, TGANet: Text-guided\nattention for improved polyp segmentation, in Proc. 25th Int. Conf.\nMedical  Image  Computing  and  Computer  Assisted  Intervention,\nSingapore, 2022, pp. 151–160.\n[22]\n R. Zhang, P. Lai, X. Wan, D. J. Fan, F. Gao, X. J. Wu, and G. Li,\nLesion-aware dynamic kernel for polyp segmentation, in Proc. 25th\nInt.  Conf.  Medical  Image  Computing  and  Computer  Assisted\nIntervention, Singapore, 2022, pp. 99–109.\n[23]\n J. H. Shi, Q. Zhang, Y. H. Tang, and Z. Q. Zhang, Polyp-mixer: An\nefficient  context-aware  MLP-based  paradigm  for  polyp\nsegmentation, IEEE  Trans.  Circuits  Syst.  Video  Technol., vol. 33,\nno. 1, pp. 30–42, 2023.\n[24]\n X. Zhao, Z. Wu, S. Tan, D. J. Fan, Z. Li, X. Wan, and G. Li, Semi-[25]\nsupervised  spatial  temporal  attention  network  for  video  polyp\nsegmentation, in Proc.  25th  Int.  Conf.  Medical  Image  Computing\nand Computer Assisted Intervention, Singapore, 2022, pp. 456–466.\n M.  Akbari,  M.  Mohrekesh,  E.  Nasr-Esfahani,  S.  M.  Reza\nSoroushmehr,  N.  Karimi,  S.  Samavi,  and  K.  Najarian,  Polyp\nsegmentation  in  colonoscopy  images  using  fully  convolutional\nnetwork, in Proc. 2018 40th Annual Int. Conf. IEEE Engineering in\nMedicine  and  Biology  Society (EMBC), Honolulu, HI, USA, 2018,\npp. 69–72.\n[26]\n P. Brandao, O. Zisimopoulos, E. Mazomenos, G. Ciuti, J. Bernal, M.\nVisentini-Scarzanella, A. Menciassi, P. Dario, A. Koulaouzidis, A.\nArezzo,  et  al.,  Towards  a  computed-aided  diagnosis  system  in\ncolonoscopy:  Automatic  polyp  segmentation  using  convolution\nneural networks, J.  Med.  Robot.  Res., vol. 3, no. 2, p. 1840002,\n2018.\n[27]\n Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, UNet++:\nA  nested  U-net  architecture  for  medical  image  segmentation,  in\nProc.  4th  International  Workshop,  DLMIA  2018,  and  8th\nInternational  Workshop,  ML-CDS  2018,  held  in  conjunction  with\nMICCAI 2018, Granada, Spain, 2018, pp. 3–11.\n[28]\n D. Jha, P. H. Smedsrud, M. A. Riegler, D. Johansen, T. De Lange, P.\nHalvorsen, and H. D Johansen, ResUNet: an advanced architecture\nfor medical image segmentation, in Proc. 2019 IEEE Int. Symp. on\nMultimedia (ISM), San Diego, CA, USA, 2019, pp. 225–230.\n[29]\n X. Sun, P. Zhang, D. Wang, Y. Cao, and B. Liu, Colorectal polyp\nsegmentation by U-net with dilation convolution, in Proc. 2019 18th\nIEEE Int. Conf. Machine Learning and Applications (ICMLA), Boca\nRaton, FL, USA, 2020, pp. 851–858.\n[30]\n B. Murugesan, K. Sarveswaran, S. M. Shankaranarayana, K. Ram, J.\nJoseph, and M. Sivaprakasam, Psi-Net: Shape and boundary aware\njoint multi-task deep network for medical image segmentation, in\nProc.  2019  41st  Annual  Int.  Conf.  IEEE  Engineering  in  Medicine\nand  Biology  Society  (EMBC),  Berlin,  Germany,  2019,  pp.\n7223–7226.\n[31]\n H. Ali Qadir, Y. Shin, J. Solhusvik, J. Bergsland, L. Aabakken, and\nI. Balasingham, Polyp detection and segmentation using mask R-\nCNN: Does a deeper feature extractor CNN always perform better?\nin  Proc.  2019  13th  Int.  Symp.  on  Medical  Information  and\nCommunication  Technology  (ISMICT),  Oslo,  Norway,  2019,  pp.\n1–6.\n[32]\n K. He, G. Gkioxari, P. Dollár, and R. Girshick, Mask R-CNN, in\nProc.  2017  IEEE  International  Conference  on  Computer  Vision\n(ICCV), Venice, Italy, 2017, pp. 2980–2988.\n[33]\n S.  Alam,  N.  K.  Tomar,  A.  Thakur,  D.  Jha,  and  A.  Rauniyar,\nAutomatic  polyp  segmentation  using  U-net-ResNet50,  in  Proc.\nMediaEval 2020 Workshop, virtual, 2020.\n[34]\n D. Banik, K. Roy, D. Bhattacharjee, M. Nasipuri, and O. Krejcar,\nPolyp-net: A multimodel fusion network for polyp segmentation,\nIEEE Trans. Instrum. Meas., vol. 70, pp. 1–12, 2021.\n[35]\n T. Rahim, S. Ali Hassan, and S. Y. Shin, A deep convolutional\nneural network for the detection of polyps in colonoscopy images,\nBiomed. Signal Process. Contr., vol. 68, p. 102654, 2021.\n[36]\n D.  Jha,  S.  Ali,  N.  K.  Tomar,  H.  D.  Johansen,  D.  Johansen,  J.\nRittscher,  M.  A.  Riegler,  and  P.  Halvorsen,  Real-time  polyp\ndetection, localization and segmentation in colonoscopy using deep\nlearning, IEEE Access, vol. 9, pp. 40496–40510, 2021.\n[37]\n A. M. A. Ahmed, Generative adversarial networks for automatic\npolyp segmentation, in Proc.  MediaEval  2020  Workshop, virtual,\n2020.\n[38]\n V. Thambawita, S. Hicks, P. Halvorsen, and M. A. Riegler, Pyramid-\nfocus-augmentation:  Medical  image  segmentation  with  step-wise\nfocus, in Proc. MediaEval 2020 Workshop, virtual, 2020.\n[39]\n N. K. Tomar, D. Jha, S. Ali, H. D. Johansen, D. Johansen, M. A.\nRiegler, and P. Halvorsen, DDANet: dual decoder attention network\nfor automatic polyp segmentation, in Proc. 2021 Int. Conf. Pattern\nRecognition, virtual, 2021, 307–314.\n[40]\n C. H. Huang, H. Y. Wu, and Y. L. Lin, HarDNet-MSEG: A simple\nencoder-decoder polyp segmentation neural network that achieves\n[41]\nPolyp-PVT: Polyp Segmentation with Pyramid Vision Transformers\n \nCAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023 13\nover 0.9 mean dice and 86 FPS, arXiv preprint arXiv: 2101.07172,\n2021.\n P. Chao, C. Y. Kao, Y. Ruan, C. H. Huang, and Y. L. Lin, HarDNet:\nA  low  memory  traffic  network,  in  Proc.  2019  IEEE/CVF\nInternational  Conference  on  Computer  Vision  (ICCV),  Seoul,\nRepublic of Korea, 2019, pp. 3551–3560.\n[42]\n Y. Zhang, H. Liu, and Q. Hu, Transfuse: Fusing transformers and\nCNNs  for  medical  image  segmentation,  in  Proc.  24th  Int.  Conf.\nMedical  Image  Computing  and  Computer  Assisted  Intervention,\nStrasbourg, France, 2021, pp. 14–24.\n[43]\n Z. Yin, K. Liang, Z. Ma, and J. Guo, Duplex contextual relation\nnetwork for polyp segmentation, in Proc. 2022 IEEE 19th Int. Symp.\non Biomedical Imaging (ISBI), Kolkata, India, 2022, pp. 1–5.\n[44]\n X. Zhao, L. Zhang, and H. Lu, Automatic polyp segmentation via\nmulti-scale subtraction network, in Proc.  24th  Int.  Conf.  Medical\nImage  Computing  and  Computer  Assisted  Intervention, Strasbourg,\nFrance, 2021, pp. 120–130.\n[45]\n Z. Zhou, J. Shin, L. Zhang, S. Gurudu, M. Gotway, and J. Liang,\nFine-tuning  convolutional  neural  networks  for  biomedical  image\nanalysis:  Actively  and  incrementally,  in  Proc.  2017  IEEE  Conf.\nComputer  Vision  and  Pattern  Recognition (CVPR), Honolulu, HI,\nUSA, 2017, pp. 4761–4772.\n[46]\n N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst, C. B. Kendall,\nM.  B.  Gotway,  and  J.  Liang,  Convolutional  neural  networks  for\nmedical  image  analysis:  Fulltraining  or  fine  tuning,  IEEE  Trans.\nMed. Imaging, vol. 35, no. 5, pp. 1299–1312, 2016.\n[47]\n X. Xie, J. Chen, Y. Li, L. Shen, K. Ma, and Y. Zheng, MI2GAN:\nGenerative  adversarial  network  for  medical  image  domain\nadaptation using mutual information constraint, in Proc  23th  Int.\nConf.  Medical  Image  Computing  and  Computer  Assisted\nIntervention, Lima, Peru, 2020, pp. 516–525.\n[48]\n R. Zhang, G. Li, Z. Li, S. Cui, D. Qian, and Y. Yu, Adaptive context\nselection for polyp segmentation, in Proc  23th  Int.  Conf.  Medical\nImage Computing and Computer Assisted Intervention, Lima, Peru,\n2020, pp. 253–262.\n[49]\n N.  K.  Tomar,  Automatic  polyp  segmentation  using  fully\nconvolutional neural network, in Proc. MediaEval 2020 Workshop,\nvirtual, 2020.\n[50]\n D. Jha, S. Hicks, K. Emanuelsen, H. D. Johansen, D. Johansen, T.\nLange, M. Riegler, and P. Halvorsen, Medico multimedia task at\nMediaEval  2020:  Automatic  polyp  segmentation,  in  Proc.\nMediaEval 2020 Workshop, virtual, 2020.\n[51]\n K.  Patel,  A.  M.  Bur,  and  G.  Wang,  Enhanced  U-net:  A  feature\nenhancement network for polyp segmentation, in Proc.  2021  18th\nConf.  Robots  and  Vision  (CRV),  Burnaby,  Canada,  2021,  pp.\n181–188.\n[52]\n A. Lumini, L. Nanni, and G. Maguolo, Deep ensembles based on\nstochastic activation selection for polyp segmentation, in Proc. 2021\nMedical Imaging with Deep Learning, Lübeck, Germany, 2021.\n[53]\n M.  V.  L.  Branch  and  A.  S.  Carvalho,  Polyp  segmentation  in\ncolonoscopy  images  using  U-net-MobileNetV2,  arXiv  preprint\narXiv: 2103.15715, 2021.\n[54]\n R. Khadga, D. Jha, S. Ali, S. Hicks, V. Thambawita, M. A. Riegler,\nand P. Halvorsen, Meta-learning with implicit gradients in a few-\nshot setting for medical image segmentation, arXiv preprint arXiv:\n2106.03223, 2021.\n[55]\n D. V. Sang, T. Q. Chung, P. N. Lan, D. V. Hang, D. V. Long, and N.\nT.  Thuy,  Ag-CUResNeSt:  A  novel  method  for  colon  polyp\nsegmentation, arXiv preprint arXiv: 2105.00402, 2021.\n[56]\n C. Yang, X. Guo, M. Zhu, B. Ibragimov, and Y. Yuan, Mutual-\nprototype adaptation for cross-domain polyp segmentation, IEEE J.\nBiomed. Health Inform., vol. 25, no. 10, pp. 3886–3897, 2021.\n[57]\n D. Jha, P. H. Smedsrud, D. Johansen, T. de Lange, H. D. Johansen,\nP.  Halvorsen,  and  M.  A.  Riegler,  A  comprehensive  study  on\ncolorectal polyp segmentation with ResUNet++, conditional random\nfield and test-time augmentation, IEEE  J.  Biomed.  Health  Inform.,\nvol. 25, no. 6, pp. 2029–2040, 2021.\n[58]\n D. Jha, N. K. Tomar, S. Ali, M. A. Riegler, H. D. Johansen, D.[59]\nJohansen, T. de Lange, and P. Halvorsen, NanoNet: real-time polyp\nsegmentation in video capsule endoscopy and colonoscopy, in Proc.\n2021  IEEE  34th  Int.  Symp.  on  Computer-Based  Medical  Systems\n(CBMS), Aveiro, Portugal, 2021, pp. 37–43.\n S. Li, X. Sui, X. Luo, X. Xu, Y. Liu, and R. Goh, Medical image\nsegmentation  using  squeeze-and-expansion  transformers,  in  Proc.\n30th  Int.  Joint  Conf.  Artificial  Intelligence,  virtual,  2021,  pp.\n807–815.\n[60]\n T. Kim, H. Lee, and D. Kim, UACANet: Uncertainty augmented\ncontext attention for polyp semgnetaion, in Proc.  29th  ACM  Int.\nConf. Multimedia, virtual, 2021, pp. 2167–2175.\n[61]\n V.  L.  Thambawita,  S.  Hicks,  P.  Halvorsen,  and  M.  Riegler,\nDivergentNets: Medical image segmentation by network ensemble,\nin Proc.  3rd  Int.  Workshop  and  Challenge  on  Computer  Vision  in\nEndoscopy (EndoCV2021) in  conjunction  with  the  18th  IEEE  Int.\nSymp.  Biomedical  Imaging  (ISBI2021),  Nice,  France,  2021,  pp.\n27–38.\n[62]\n X. Guo, C. Yang, and Y. Yuan, Dynamic-weighting hierarchical\nsegmentation network for medical images, Med.  Image  Anal., vol.\n73, p. 102196, 2021.\n[63]\n G. P. Ji, Y. C. Chou, D. P. Fan, G. Chen, H. Fu, D. Jha, and L. Shao,\nProgressively  normalized  self-attention  network  for  video  polyp\nsegmentation, in Proc.  24th  Int.  Conf.  Medical  Image  Computing\nand  Computer  Assisted  Intervention, Strasbourg, France, 2021, pp.\n142–152.\n[64]\n A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in\nProc.  31st  Conf.  Neural  Information  Processing  Systems,  Long\nBeach, CA, USA, 2017, pp. 5998–6008.\n[65]\n A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et\nal.,  An  image  is  worth  16×16  words:  Transformers  for  image\nrecognition  at  scale,  in  Proc.  9th  Int.  Conf.  Learning\nRepresentations, Vienna, Austria, 2021.\n[66]\n Z.  Pan,  B.  Zhuang,  J.  Liu,  H.  He,  and  J.  Cai,  Scalable  vision\ntransformers  with  hierarchical  pooling,  in  Proc.  2021  IEEE/CVF\nInternational  Conference  on  Computer  Vision  (ICCV),  Montreal,\nCanada, 2021, pp. 367–376.\n[67]\n B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh, Rethinking\nspatial dimensions of vision transformers, in Proc. 2021 IEEE/CVF\nInternational  Conference  on  Computer  Vision  (ICCV),  Montreal,\nCanada, 2021, pp. 11916–11925.\n[68]\n L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z. Jiang, F. E. H. Tay, J.\nFeng,  and  S.  Yan,  Tokens-to-token  ViT:  Training  vision\ntransformers from scratch on ImageNet, in Proc.  2021  IEEE/CVF\nInt.  Conf.  Computer  Vision (ICCV), Montreal, Canada, 2021, pp.\n538–547.\n[69]\n K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, Transformer\nin transformer, in Proc.  35th  Conf.  Neural  Information  Processing\nSystems, virtual, 2021, pp. 15908–15919.\n[70]\n W. Wang, E. Xie, X. Li, D. P. Fan, K. Song, D. Liang, T. Lu, P.\nLuo, and L. Shao, Pyramid vision transformer: A versatile backbone\nfor dense prediction without convolutions, in Proc. 2021 IEEE/CVF\nInt.  Conf.  Computer  Vision (ICCV), Montreal, Canada, 2021, pp.\n548–558.\n[71]\n W. Wang, E. Xie, X. Li, D. P. Fan, K. Song, D. Liang, T. Lu, P.\nLuo, and L. Shao, PVT v2: Improved baselines with pyramid vision\ntransformer,  Computational  Visual  Media,  vol.  8,  no.  3,  pp.\n415–424, 2022.\n[72]\n Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B.\nGuo, Swin transformer: Hierarchical vision transformer using shifted\nwindows,  in  Proc.  2021  IEEE/CVF  International  Conference  on\nComputer Vision (ICCV), Montreal, Canada, 2021, pp. 9992–10002.\n[73]\n H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\nCvT: introducing convolutions to vision transformers, in Proc. 2021\nIEEE/CVF  Int.  Conf.  Computer  Vision (ICCV), Montreal, Canada,\n2021, pp. 22–31.\n[74]\n W. Xu, Y. Xu, T. Chang, and Z. Tu, Co-scale conv-attentional image[75]\nCAAI Artificial Intelligence Research\n \n14 CAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023\ntransformers, in Proc. 2021 IEEE/CVF International Conference on\nComputer Vision (ICCV), Montreal, Canada, 2021, pp. 9961–9970.\n X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C.\nShen,  Twins:  Revisiting  the  design  of  spatial  attention  in  vision\ntransformers,  in  Proc.  34th  Conf.  Neural  Information  Processing\nSystems, virtual, 2021, pp. 9355–9366.\n[76]\n B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. Jegou,\nand M. Douze, LeViT: A vision transformer in ConvNet’s clothing\nfor faster inference, in Proc.  2021  IEEE/CVF  Int.  Conf.  Computer\nVision (ICCV). Montreal, Canada, 2021, 12239–12249.\n[77]\n S. Bhojanapalli, A. Chakrabarti, D. Glasner, D. Li, T. Unterthiner,\nand A. Veit, Understanding robustness of transformers for image\nclassification, in Proc. 2021 IEEE/CVF Int. Conf. Computer Vision\n(ICCV), Montreal, Canada, 2021, 10211–10221.\n[78]\n E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P.\nLuo,  SegFormer:  Simple  and  efficient  design  for  semantic\nsegmentation  with  transformers,  in  Proc.  34th  Conf.  Neural\nInformation Processing Systems, virtual, 2021, pp. 12077–12090.\n[79]\n Z. Wu, L. Su, and Q. Huang, Cascaded partial decoder for fast and\naccurate salient object detection, in Proc.  2019  IEEE/CVF  Conf.\nComputer  Vision  and  Pattern  Recognition  (CVPR),  Long  Beach,\nCA, USA, 2020, pp. 3902–3911.\n[80]\n S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift, in Proc. 32nd\nInt.  Conf.  Machine  Learning  (ICML),  Lille,  France,  2015,  pp.\n448–456.\n[81]\n X. Glorot, A. Bordes, and Y. Bengio, Deep sparse rectifier neural\nnetworks,  in  Proc.  14th  Int.  Conf.  Artificial  Intelligence  and\nStatistics, Fort Lauderdale, FL, USA, 2011, pp. 315–323.\n[82]\n S. Woo, J. Park, J. Y. Lee, and I. S. Kweon, CBAM: Convolutional\nblock attention module, in Proc.  15th  European  Conf.  Computer\nVision, Munich, Germany, 2018, pp. 3–19.\n[83]\n J. Hu, L. Shen, and G. Sun, Squeeze-and-excitation networks, in\nProc.  2018  IEEE  Conf.  Computer  Vision  and  Pattern  Recognition\n(CVPR), Salt Lake City, UT, USA, 2018, pp. 7132–7141.\n[84]\n X. Wang, R. B. Girshick, A. Gupta, and K. He, Non-local neural\nnetworks, in Proc.  2018  IEEE  Conf.  Computer  Vision  and  Pattern\nRecognition  (CVPR),  Salt  Lake  City,  UT,  USA,  2018,  pp.\n7794–7803.\n[85]\n G.  Te,  Y.  Liu,  W.  Hu,  H.  Shi,  and  T.  Mei,  Edge-aware  graph\nrepresentation learning and reasoning for face parsing, in Proc. 16th\nEuropean  Conf.  Computer  Vision,  Glasgow,  UK,  2020,  pp.\n[86]\n258–274.\n Y.  Lu,  Y.  Chen,  D.  Zhao,  and  J.  Chen,  Graph-FCN  for  image\nsemantic segmentation, in Proc.  16th  Int.  Symp.  Neural  Networks,\nMoscow, Russia, 2019, pp. 97–105.\n[87]\n J. Wei, S. Wang, and Q. Huang, F³Net: Fusion, feedback and focus\nfor  salient  object  detection,  in  Proc.  34th  AAAI  Conf.  Artificial\nIntelligence  (2020),  32nd  Innovative  Applications  of  Artificial\nIntelligence Conf. (IAAI), 10th AAAI Symp. Educational Advances in\nArtificial  Intelligence  (EAAI),  New  York,  NY,  USA,  2020,  pp.\n12321–12328.\n[88]\n I. Loshchilov and F. Hutter, Decoupled weight decay regularization,\nin  Proc.  7th  Int.  Conf.  Learning  Representations  (ICLR),  New\nOrleans, LA, USA, 2017.\n[89]\n F.  Milletari,  N.  Navab,  and  S.  A.  Ahmadi,  V-net:  Fully\nconvolutional  neural  networks  for  volumetric  medical  image\nsegmentation, in Proc.  2016  Fourth  Int.  Conf.  3D  Vision (3DV),\nStanford, CA, USA, 2016, pp. 565–571.\n[90]\n R.  Margolin,  L.  Zelnik-Manor,  and  A.  Tal,  How  to  evaluate\nforeground maps, in Proc.  2014  IEEE  Conf.  Computer  Vision  and\nPattern Recognition, Columbus, OH, USA, 2014, pp. 248–255.\n[91]\n M.  M.  Cheng  and  D.  P.  Fan,  Structure-measure:  A  new  way  to\nevaluate foreground maps, Int. J. Comput. Vis., vol. 129, no. 9, pp.\n2622–2638, 2021.\n[92]\n D. P. Fan, G. P. Ji, X. B. Qin, M. M. Cheng, Cognitive vision\ninspired object segmentation metric and loss function, (in Chinese),\nSCIENTIA SINICA Informat., vol. 51, no. 9, pp. 1475–1489, 2021.\n[93]\n D. P. Fan, C. Gong, Y. Cao, B. Ren, M. M. Cheng, and A. Borji,\nEnhanced-alignment measure for binary foreground map evaluation,\nin  Proc.  27th  Int.  Joint  Conf.  Artificial  Intelligence,  Stockholm,\nSweden, 2018, pp. 698–704.\n[94]\n Y. Fang, C. Chen, Y. Yuan, and K. -Y. Tong, Selective feature\naggregation  network  with  area-boundary  constraints  for  polyp\nsegmentation, in Proc.  22nd  Int.  Conf.  Medical  Image  Computing\nand  Computer  Assisted  Intervention,  Shenzhen,  China,  2022,  pp.\n302–310.\n[95]\n G. P. Ji, G. Xiao, Y. C. Chou, D. P. Fan, K. Zhao, G. Chen, and L.\nVan Gool, Video polyp segmentation: A deep learning perspective,\nMach. Intell. Res., vol. 19, no. 6, pp. 531–549, 2022.\n[96]\n J.  Bernal,  J.  Sánchez,  and  F.  Vilariño,  Towards  automatic  polyp\ndetection with a polyp appearance model, Pattern Recognit., vol. 45,\nno. 9, pp. 3166–3182, 2012.\n[97]\nPolyp-PVT: Polyp Segmentation with Pyramid Vision Transformers\n \nCAAI Artificial Intelligence Research | VOL. 2 Article No. 9150015 | 2023 15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7795579433441162
    },
    {
      "name": "Encoder",
      "score": 0.7146075963973999
    },
    {
      "name": "Artificial intelligence",
      "score": 0.642874002456665
    },
    {
      "name": "Segmentation",
      "score": 0.5928266048431396
    },
    {
      "name": "Transformer",
      "score": 0.5189175605773926
    },
    {
      "name": "Convolutional neural network",
      "score": 0.49875473976135254
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4442859888076782
    },
    {
      "name": "Computer vision",
      "score": 0.4232110381126404
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.4108086824417114
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210116052",
      "name": "Inception Institute of Artificial Intelligence",
      "country": "AE"
    }
  ]
}