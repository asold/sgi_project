{
  "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
  "url": "https://openalex.org/W4385574386",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101508992",
      "name": "Mingyu Lee",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101787663",
      "name": "Jun-Hyung Park",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A5100659574",
      "name": "Junho Kim",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5065688046",
      "name": "Kang-Min Kim",
      "affiliations": [
        "Catholic University of Korea"
      ]
    },
    {
      "id": "https://openalex.org/A5101595608",
      "name": "SangKeun Lee",
      "affiliations": [
        null,
        "Korea University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3011574394",
    "https://openalex.org/W3034201598",
    "https://openalex.org/W3187255235",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2132984949",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2151834591",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2950695840",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3212940836",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W4251558359",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3174805484",
    "https://openalex.org/W2511770992",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4287646293",
    "https://openalex.org/W4230563027",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W3034623328",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2007347635",
    "https://openalex.org/W3191350816",
    "https://openalex.org/W2898846200",
    "https://openalex.org/W3047171714",
    "https://openalex.org/W2963748441"
  ],
  "abstract": "Self-supervised pre-training has achieved remarkable success in extensive natural language processing tasks. Masked language modeling (MLM) has been widely used for pre-training effective bidirectional representations but comes at a substantial training cost. In this paper, we propose a novel concept-based curriculum masking (CCM) method to efficiently pre-train a language model. CCM has two key differences from existing curriculum learning approaches to effectively reflect the nature of MLM. First, we introduce a novel curriculum that evaluates the MLM difficulty of each token based on a carefully-designed linguistic difficulty criterion. Second, we construct a curriculum that masks easy words and phrases first and gradually masks related ones to the previously masked ones based on a knowledge graph. Experimental results show that CCM significantly improves pre-training efficiency. Specifically, the model trained with CCM shows comparative performance with the original BERT on the General Language Understanding Evaluation benchmark at half of the training cost.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7417–7427\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nEfficient Pre-training of Masked Language Model\nvia Concept-based Curriculum Masking\nMingyu Lee1∗, Jun-Hyung Park 2∗, Junho Kim 1, Kang-Min Kim 3, SangKeun Lee 1,2\n1Department of Artificial Intelligence 2Department of Computer Science and Engineering\nKorea University, Seoul, Republic of Korea\n3Department of Data Science, The Catholic University of Korea, Bucheon, Republic of Korea\n{decon9201, irish07, monocrat}@korea.ac.kr\nkangmin89@catholic.ac.kr, yalphy@korea.ac.kr\nAbstract\nMasked language modeling (MLM) has been\nwidely used for pre-training effective bidirec-\ntional representations, but incurs substantial\ntraining costs. In this paper, we propose\na novel concept-based curriculum masking\n(CCM) method to efficiently pre-train a lan-\nguage model. CCM has two key differences\nfrom existing curriculum learning approaches\nto effectively reflect the nature of MLM. First,\nwe introduce a carefully-designed linguistic dif-\nficulty criterion that evaluates the MLM diffi-\nculty of each token. Second, we construct a\ncurriculum that gradually masks words related\nto the previously masked words by retrieving\na knowledge graph. Experimental results show\nthat CCM significantly improves pre-training\nefficiency. Specifically, the model trained\nwith CCM shows comparative performance\nwith the original BERT on the General Lan-\nguage Understanding Evaluation benchmark\nat half of the training cost. Code is available\nat https://github.com/KoreaMGLEE/Concept-\nbased-curriculum-masking.\n1 Introduction\nSelf-supervised pre-training has achieved consider-\nable performance improvements in various natural\nlanguage processing (NLP) tasks (Devlin et al.,\n2019; Yang et al., 2019; Zhang et al., 2019; Lewis\net al., 2020; Clark et al., 2020). Masked language\nmodeling (MLM) (Devlin et al., 2019), where the\nmodel predicts original tokens of a masked subset\nof the text using the unmasked subset as clues, has\ncontributed significantly to these improvements.\nHowever, MLM typically requires a high number\nof compute operations, resulting in unrealistically\nlarge training costs (Clark et al., 2020). Therefore,\nthe consideration on the pre-training costs has be-\ncome an increasingly important issue (Jiang et al.,\n2020; Narayanan et al., 2021).\n∗These authors contributed equally to this work.\nMethods CoLA MRPC SST Avg.\nNo curriculum 44.9 85.4 89.6 73.3\nRarity 45.5 85.1 89.4 73.4\nLength 41.6 86.3 89.4 72.4\nMasking ratio 43.2 87.0 89.7 73.3\nCCM (ours) 48.0 86.7 90.9 75.2\nTable 1: Comparison of different CL methods with\nBERTMedium. Rarity denotes a curriculum that first\nlearns sentences composed with frequent words. Length\ndenotes a curriculum that incrementally increases the\nsequence length from 64 to 512. Masking ratio denotes\na curriculum that increases the masking ratio linearly\nfrom 0.1 to 0.15.\nRecent NLP studies have shown that curricu-\nlum learning (CL), presenting examples in an easy-\nto-difficult order rather than presenting them ran-\ndomly, can accelerate the model convergence and\nimprove the generalization performance (Zhang\net al., 2018; Tay et al., 2019; Zhan et al., 2021).\nThere mainly exist two criteria for assessing the dif-\nficulty of examples, 1) model-based criteria (Zhou\net al., 2020; Xu et al., 2020) and 2) linguistic-\ninspired criteria (Sachan and Xing, 2016; Tay\net al., 2019; Nagatsuka et al., 2021; Campos, 2021).\nModel-based criteria measure the difficulty of each\nexample using task-specific models. However,\nthese criteria are unsuitable for reducing the com-\nputation cost of pre-training, given that they require\ncalculating the loss of every example in a large pre-\ntraining corpus using language models. In contrast,\nlinguistic-inspired criteria can efficiently assess the\ndifficulty of examples based on prior knowledge\nand rules. Therefore, we adopt CL with linguistic\ndifficulty criteria into MLM to improve the effi-\nciency of pre-training.\nHowever, we argue that existing linguistics-\ninspired criteria, such as length, rarity, and masking\nratio of a sequence, do not effectively reflect the\nnature of MLM, as verified empirically in Table 1.\n7417\nThe difficulty associated with MLM is significantly\naffected by the choice of tokens to be masked in\nthe given sequence, rather than by the given se-\nquence itself. For example, given \"The man is\na Stanford <mask> student\", we can easily pre-\ndict that the masked token would be University,\nwhereas given \"The man is a <mask> University\nstudent\", it would be relatively difficult to predict\nthe original token due to the insufficient clues in the\ncontext. Then, how can we measure the MLM dif-\nficulty? MLM can be viewed as predicting masked\ntokens based on other contextual tokens related to\nmasked tokens. Therefore, if a word is related to\nmany other words and phrases, it is likely that it\nhas several clues in the context that make MLM\neasier.\nIn this paper, we propose a novel concept-based\ncurriculum masking (CCM) for improving pre-\ntraining efficiency by considering the nature of\nMLM. We consider words and phrases that are\nrelated to several other concepts as easy ones and\ndefine them as the initial concepts to be masked\nfirst. To identify them, we utilize millions of syn-\ntactic and semantic relationships between words or\nphrases, referring as concepts, within a large-scale\nknowledge graph, ConceptNet (Speer et al., 2017).\nFirst, we measure the number of related concepts\nfor each concept in ConceptNet and construct the\nset of concepts with the highest number, which will\nbe masked during the first stage of our curriculum.\nThen, we gradually mask concepts related to the\npreviously masked concepts during the consecutive\nstages, inspired by human language acquisition, in\nwhich simple concepts are learned first and more\ncomplex concepts are gradually learned (Anglin,\n1978; Horton and Markman, 1980).\nTo verify the effectiveness of the proposed cur-\nriculum, we conduct experiments on the Gen-\neral Language Understanding Evaluation (GLUE)\nbenchmark (Wang et al., 2019). Experimental re-\nsults show that CCM significantly improves the\npre-training efficiency of MLM. Particularly, CCM\nonly requires 50% of the computational cost in\nachieving the comparable GLUE score with MLM,\nusing the same BERTBase architecture. In addition,\nby training on commensurate computational cost\nas if MLM, CCM outperforms MLM by 1.9 points\non the GLUE average score. The contributions of\nour study are as follows:\n• We investigate and demonstrate the effective-\nness of CL for pre-training. To the best of our\nknowledge, our work is one of the few that\nintroduces CL to MLM.\n• We propose a novel curriculum masking\nframework that progressively masks concepts\nbased on our relation-based difficulty criteria.\n• We demonstrate that our framework signifi-\ncantly improves the pre-training efficiency of\nMLM through extensive experiments on the\nGLUE benchmark.\n2 Related Work\n2.1 Self-supervised Pre-training\nSelf-supervised pre-training has been employed\nto learn universal language representations from\nlarge corpora (Collobert et al., 2011; Pennington\net al., 2014). Recently, BERT (Devlin et al., 2019)\nhas achieved tremendous success in various NLP\ntasks by learning bidirectional representations via\nMLM. Variants of BERT have been proposed to fur-\nther improve the performance in NLP tasks. Yang\net al. (2019) have used an input sequence autore-\ngressively and randomly generated to alleviate the\nmismatch caused by the masked tokens that ap-\npear only during pre-training. Zhang et al. (2019)\nhave incorporated external knowledge graphs into\nthe language model to introduce structural knowl-\nedge. Sun et al. (2019) and Joshi et al. (2020) have\nmasked contiguous tokens to improve the span rep-\nresentation. Lewis et al. (2020) have replaced multi-\nple tokens with one mask token for noise flexibility.\nLevine et al. (2021) have masked highly collocat-\ning n-grams in the corpus for preventing the model\nto use shallow local signals. Although these studies\nhave resulted in significant performance improve-\nments in NLP tasks, they still incur tremendous\ncomputing costs (Qiu et al., 2020). To the best\nof our knowledge, there are only limited studies\nthat have explored the improvement of MLM pre-\ntraining efficiency.\n2.2 Curriculum Learning\nIn the domain of machine learning, CL is a training\nstrategy that gradually learns the complex exam-\nples after learning the easy ones, instead of learning\nall data simultaneously, which has been firstly pro-\nposed by (Bengio et al., 2009). The concept of\nCL can be traced back to Elman (1993) that has\nattempted to train machines from easy to difficult\n7418\nFigure 1: Illustration of CCM. In the first stage, CCM masks the initial concepts. Then it progressively includes the\nconcepts within k-hops from the concepts used in the previous stages. Note that, \"S\" represents a set of concepts to\nbe masked, \"O\" represents the original token sequence, and \"M\" represents a masked token sequence.\ntasks. Krueger and Dayan (2009) have tried to di-\nvide complex tasks into easy sub-tasks, and then\ntrained machines on these sub-tasks.\nRecent studies have shown that a curriculum-\nbased approach can improve the convergence speed\nand generalization performance of NLP systems\n(Sachan and Xing, 2016; Zhang et al., 2018; Tay\net al., 2019; Zhou et al., 2020; Zhan et al., 2021).\nThe identification of easy samples in a given train-\ning dataset is important in applying CL to a target\ntask (Kumar et al., 2010). There are two main\nstrategies for assessing the data difficulty; 1) using\nhuman prior knowledge regarding specific tasks\n(Zhang et al., 2018; Tay et al., 2019; Zhan et al.,\n2021), and 2) using trained models to measure the\ndifficulty of examples (Zhou et al., 2020; Xu et al.,\n2020). However, existing difficulty criteria do not\nwork well with CL for MLM, as they have over-\nlooked that the MLM difficulty of examples can\nbe changed according to which tokens are masked.\nTo better assess the difficulty of MLM, we propose\na novel curriculum that intervenes in the masking\nprocess based on the relation-based difficulty crite-\nria.\n3 Concept-based Curriculum Masking\nOur curriculum framework is inspired by the hu-\nman language acquisition. People learn simple\nconcepts (e.g., car) first and then gradually learn\nmore complex concepts (e.g., self-driving car). By\nleveraging the related basic concepts, the gradual\nlearning style enables humans to learn abstract con-\ncepts easily (Anglin, 1978; Horton and Markman,\n1980). To mimic this, we construct a multi-stage\ncurriculum by gradually adding concepts relating\nto the initial concepts using a knowledge graph. To\nthis end, our CCM framework consists of three pro-\ncesses: initial concept selection, curriculum con-\nstruction, and pre-training.\nIn the following sections, we denote a knowl-\nedge graph as G = (N,E), where N and E are\nsets of nodes and edges, respectively. We define a\nword or phrase corresponding to a node as a con-\ncept c∈ N. We denote Si as the set of concepts to\nbe masked in the i-th stage. The final curriculum\nincludes multiple stages {S1...SK}.\n3.1 Initial Concept Selection\nA key for CL is to learn easy examples first. Consid-\nering that MLM is the task for predicting a masked\nsubset of a text using the surrounding context, we\nsuppose concepts related to many others are easy\nconcepts. Thus, we construct a set of initial con-\ncepts for masking by selecting concepts that have\nthe highest degree of connection in the knowledge\ngraph. To select the initial concepts, we first rank\neach concept c ∈ N according to the degree of\nconnection in the knowledge graph G. In addition,\nwe exclude concepts that appear less than 100k\ntimes in the pre-training corpus, since frequent con-\ncepts are more influential in learning their related\nconcepts than rare concepts.1 Then, given the pre-\ndefined number of initial concepts M, the top M\nconcepts are selected.\n1We have observed that rare concepts connected to many\nconcepts are mostly medical or scientific jargons, which are\nintuitively considered to be complex (e.g., heraldry, carbohy-\ndrate).\n7419\n3.2 Curriculum Construction\nIn this section, we describe how to arrange the\nlearning stages based on the knowledge graph G.\nThe set of concepts to be masked in the i-th stage,\nSi, is constructed following the principle\nSi is constructed by progressively ex-\npanding Si−1 to concepts that are re-\nlated to concepts inSi−1.\nThis facilitates to utilize previously learned con-\ncepts as clues for modeling new related concepts\nlike human language acquisition. We identify re-\nlated concepts using the relationships in the knowl-\nedge graph. Intuitively, the more closely connected\nthe two concepts are in the knowledge graph, the\nmore related they are. For example, as shown in\nFigure 1, given a concept University, student and\nStanford (1-hop distance) are more related than\nman (2-hop distance). Consequently, student or\nStanford may be utilized as a stronger predictive\nclue than man, such as in the sentence \"The man\nis a Stanford <mask> student\" where University is\nmasked.\nBased on the principle, we gradually mask con-\ncepts connected to previously learned concepts\nwithin k-hops in the knowledge graph throughout\ncurriculum stages as follows:\nSi = Si−1 ∪ Nk(Si−1) (1)\nHere, Nk(Si−1) denotes the set of concepts, each\nconcept of which is within k-hops from s∈ Si−1.\n3.3 Pre-training\nTo introduce concept-based curriculum masking\ninto pre-training language models, we first search\nfor concepts included in Si for each i-th stage from\nthe token sequences in the pre-training corpus and\nthen mask identified concepts for MLM training.\nConcept search We first compile a lexicon of\nconcepts. Specifically, from the knowledge graph,\nwe extract the concepts of less than 5 words that oc-\ncur over 10 times in the pre-training corpus. Then\nwe search for the extracted concepts from the to-\nken sequences in the pre-training corpus by string\nmatch. The cost of this process is negligible com-\npared to MLM as it is conducted only once during\npre-processing.2\n2In our experiment, it takes about 46 minutes to search for\nconcepts in the pre-training corpus with two Intel(R) Xeon(R)\nSilver 4210R CPUs.\nAlgorithm 1 Curriculum Masking Scheme\nInput: Concept-based curriculum {S1...SK},\nlanguage model parameters θ, input data D,\nmaximum training step τ, dynamic masking\nprobability pd\n1: for token sequence T in Ddo\n2: Search concepts cin T.\n3: Ordered concepts according to {S1...SK}\n4: end for\n5: Initialize θ\n6: while step < τ do\n7: for stage i= 1,...,K do\n8: Generate examples eby masking c\n(c∈ Si and c∈ T) based on a rate of pd.\n9: Training the model on e.\n10: end for\n11: end while\n12: return Trained model parameters θ\nConcept masking After searching for the con-\ncepts, we mask the token sequences following our\ncurriculum. We introduce whole concept masking\n(WCM) that masks all the tokens consisting of a\nsingle concept simultaneously. For example, if we\nmask the concept Stanford, all the tokens Stan\nand ##ford will be masked together. Following the\nmasking strategy in (Devlin et al., 2019), 80% of\nthe total masked concepts are replaced into mask\ntokens, and 10% into random tokens, and the rest\nare not replaced.\nThe number of identified concepts changes sig-\nnificantly for each stage and sentence. Hence, the\nstatic masking probability often leads to either too\nlittle or too much masking. Therefore, we dynam-\nically calculate a masking probability pd to mask\napproximately 15% of the total tokens for each\nsequence.\nIn addition, even if a concept includes other con-\ncepts in a given input, all the concepts are treated\nindependently. For example, if two concepts Stan-\nford Universityand Stanford appear in a sequence,\neach concept is masked independently. The entire\nCCM process is presented in Algorithm 1.\n4 Experiment\nWe examine the efficacy of CCM using the BERT\narchitectures (Devlin et al., 2019). We measure\nthe performance of the pre-trained models on the\nGLUE benchmark (Wang et al., 2019).\n7420\n(a) Small-sized model\n (b) Medium-sized model\n (c) Base-sized model\nFigure 2: Comparison of MLM and CCM on different-sized BERT models. The reported results are average scores\non the GLUE benchmark with respect to steps.\nModel Method Params CoLA SST MRPC STS RTE MNLI QQP QNLI Avg.\nBERTSmall\nMLM 14M 38.0 88.7 82.8 82.0 59.2 76.8 88.4 85.8 75.2\nCCM (ours) 14M 42.8 89.1 84.1 83.3 61.3 77.5 88.6 86.3 76.6\nBERTMedium\nMLM 26M 44.9 89.6 85.4 82.7 60.3 78.9 89.4 87.6 77.4\nCCM (ours) 26M 48.0 90.9 86.7 83.6 61.4 80.0 89.2 87.6 78.4\nBERTBase\nMLM 110M 49.7 90.8 87.8 85.4 67.8 81.7 90.4 89.5 80.4\nCCM (ours) 110M 60.3 93.1 88.3 85.5 65.0 84.1 91.0 91.4 82.3\nTable 2: Results of small, medium, and large-sized models on the development sets of GLUE.\n4.1 Experimental Settings\nIn the experiments, we denote the token embed-\nding size as E, hidden dimension size as H, num-\nber of layers as L, intermediate layer size of the\nfeed-forward module as F, and the number of self-\nattention heads as A, respectively. We report ex-\nperimental results on three model sizes: Small (E\n= 128, H = 256, L= 12, F = 1024, A= 4, 14M\nparameters), Medium (E= 128, H = 384, L= 12,\nF = 1536, A= 8, 26M parameters), and Base (E\n= 768, H = 768, L= 12, F = 3072, A= 12, 110M\nparameters). We conduct experiments with four\nRTX A5000 GPUs.\nPre-training. Since the performance of pre-\ntrained language models heavily depends on the\ncorpus size, we manually pre-train all the BERT\nmodels using CCM and MLM on the BERT pre-\ntraining corpus released in HuggingFace Datasets3,\nincluding BooksCorpus (Zhu et al., 2015) and the\nEnglish Wikipedia to ensure a fair comparison. We\npre-train small and medium-sized models for 1M\nsteps with a batch size of 128, a sequence length\nof 128, and a maximum learning rate of 5e-4. Fur-\nthermore, we pre-train base-sized models for 1M\nsteps with a batch size of 256, a sequence length\n3https://huggingface.co/datasets\nof 128, and a maximum learning rate of 1e-4. We\nuse Adam as optimizer withβ1 = 0.9,β2 = 0.999,\nand L2 weight decay of 0.01. For pre-training, after\na warmup of 10k steps, we used a linear learning\nrate decay. It is noteworthy that during CCM, we\nwarmup for 100k steps using MLM and then train\n100k steps for each stage. When the final stage\nends, the model returns to the MLM stage and re-\npeats the curriculum for the remaining steps. In\nthese experiments, we use a four-stage curriculum,\nwhere the final stage can mask all concepts and all\nwords that do not comprise a concept. We pre-train\n3 randomly initialized models and use the model\nwith the lowest validation MLM loss.\nEvaluation. We evaluate our pre-trained mod-\nels on the GLUE benchmark. The GLUE bench-\nmark consists of eight datasets for the evaluation\nof natural language understanding systems: RTE\n(Giampiccolo et al., 2007) and MNLI (Williams,\n1992) cover textual entailment; QNLI (Rajpurkar\net al., 2016) covers question-answer entailment;\nMRPC (Dolan and Brockett, 2005) covers para-\nphrase; QQP covers question paraphrase; STS (Cer\net al., 2017) covers textual similarity; SST (Socher\net al., 2013) covers sentiment; and CoLA (Warstadt\net al., 2019) covers linguistic acceptability. We use\nthe Mathew correlation for CoLA, Spearman cor-\n7421\nModel Method GLUE\nBERTMedium\nCCM (ours) 78.4\nw/o CL 76.8\nw/o CL, WCM 77.4\nTable 3: Ablation studies of CCM using medium-sized\nBERT architecture. Here, \"WCM\" represents whole\nconcept masking and \"CL\" represents curriculum learn-\ning.\nrelation for STS, and accuracy for the rest of the\ntasks as evaluation metrics. We further report an\naverage score over the eight tasks.\nWe fine-tune the pre-trained models on CoLA,\nMRPC, SST, QQP, QNLI, and MNLI for three\nepochs while STS and RTE for 10 epochs. For\neach task, we select the best fine-tuned learning\nrates (among the 5e-5, 4e-5, 3e-5, and 2e-5 fol-\nlowing the setting in Devlin et al. (2019)) on the\ndevelopment sets. In addition, we run five random\nrestarts and report the median score. For each ran-\ndom restart, we use the same checkpoint but with\ndifferent data shuffling and classifier initialization\nmethods. Note that we conduct all experiments\nusing a single model, not using an ensemble.\n4.2 GLUE Results\nAs aforementioned, the goal of CCM is to acceler-\nate the pre-training convergence. By examining the\nperformance of models on the GLUE benchmark\nafter every 100k steps, we demonstrate that CCM\naccelerates model convergence speed.\nFigure 2 shows the learning curves of different\nsize models on the GLUE benchmark. We observe\nthat CCM allows all models to achieve baseline\nperformance with significantly less computation.\nIn particular, CCM outperforms MLM at 50% of\nthe computational cost with base-sized models. In\naddition, we can observe that CCM is more effec-\ntive with a larger model. With a small-sized model,\nCCM shows slightly lower performance during the\ninitial pre-training steps. We speculate that concept-\nwise masking may be too hard for a small-sized\nmodel, resulting in a degradation of convergence\nspeed. Nevertheless, after training with enough\niterations, CCM can achieve the GLUE scores of\nMLM with fewer training steps.\nTable 2 shows the performance of the models pre-\ntrained for 1M steps on each GLUE task. We can\nobserve that CCM allows all models to outperform\ntheir baselines on all tasks, except for BERT Base\nMethods CoLA MRPC SST Avg.\nNo curriculum 44.9 85.4 89.6 73.3\nRarity 45.5 85.1 89.4 73.4\nLength 41.6 86.3 89.4 72.4\nReverse 29.0 83.6 87.6 66.7\nMasking ratio 43.2 87.0 89.7 73.3\nTeacher review 48.0 85.4 89.9 74.4\nCCM (ours) 48.0 86.7 90.9 75.2\nTable 4: Comparison of different curriculum designs\nwith BERTMedium.\non RTE. Specifically, CCM-applied BERT Small\nand BERTMedium outperform their baselines by 1.4\npoints and 1.0 points on the GLUE average score,\nrespectively, when fully trained. CCM-applied\nBERTBase achieves the best performance improve-\nment on GLUE average score. Although CCM-\napplied BERTBase shows worse performance on\nRTE, possibly due to the significantly different con-\ncept distribution compared with the pre-training\ncorpus, we have observed CCM-applied BERTBase\nfinally outperforms the baseline performance with\nslightly more training steps.\n5 Analysis\n5.1 Ablation Study\nTo investigate the contribution of each component\nin CCM, we conduct ablation studies for whole\nconcept masking (WCM) and CL. The ablation\nstudy is conducted with medium-sized models and\nGLUE scores are reported. The GLUE score is\nthe average score of all eight tasks. As shown in\nTable 3, we can observe that CCM achieves the best\nGLUE scores when using both CL and WCM while\nhaving the worst score with the setting without CL.\nThese results indicate that CL greatly contributes\nto the improvements in pre-training efficiency.\n5.2 Effect of the Curriculum\nTo demonstrate our curriculum design choice, we\ncompare it with the non-curriculum and other\ncurriculum-based approaches on BERTMedium. For\ncomparison with other curricula, we adopt rarity,\nlength, reverse, masking ratio, and teacher review\nas baselines. All curriculum-based approaches use\nthe same curriculum configuration (e.g., the num-\nber of stages) unless mentioned otherwise.\nRarity. We adopt the rarity of concepts in the\ntraining corpus as a difficulty metric, similar to\n7422\nthose used in (Tay et al., 2019). We initially train\nthe model with the most frequent concepts in the\ntraining corpus, and then progressively add the less\nfrequent ones.\nLength. We first train the model on short token\nsequences and then progressively train on longer\nsequences. Following (Campos, 2021), we initially\nuse a sequence length of 64, then gradually increase\nthe length to 128, 256, and 512 at the end of each\nepoch.\nReverse. For comparison with a hard-to-easy cur-\nriculum, we train the model with the reverse order\nof curriculum in CCM. Specifically, MLM is used\nto warmup the hard-to-easy model for 100k steps,\nsubsequently, we train the model from stage 3 to\nstage 1.\nTeacher review. For teacher review, the\nBERTBase model is used as the teacher. We use\nthe MLM loss from the pre-trained teacher as the\ndifficulty score (Xu et al., 2020) and distribute\nexamples according to the measured difficulty.\nMasking ratio. For the masking ratio curriculum,\nwe only mask 10% of the first full sequence. Sub-\nsequently, we increase the masking ratio linearly\nto 15% of tokens when 1M is reached.\nResults. As shown in Table 4, all curriculum-\nbased approaches except the reverse curriculum im-\nprove generalization performance on various tasks\ncompared to non-curriculum. The hard-to-easy\ncurriculum shows a significant performance degra-\ndation. A possible reason could be that concepts\nadded in the last step are too difficult for language\nmodels to learn without prior learning any relevant\nconcepts, leading to the degradation of convergence\nspeed. Finally, our CCM outperforms all other ap-\nproaches in the experiment on the GLUE tasks.\nThese results indicate that our curriculum for pro-\ngressively learning relevant concepts is effective to\nimprove pre-training efficiency.\n5.3 Difficulty of Curriculum Stages\nMost CL studies argue that the generalization per-\nformance and convergence speed are improved\nwhen training models in an easy-to-difficult order.\nTo examine whether our curriculum arranges ex-\namples in an easy-to-difficult order as in Bengio\net al. (2009), we measure the difficulty of examples\nin each stage of our curriculum. For measuring\nthe difficulty of examples, we report training loss\nFigure 3: Training curve of the medium-sized models\non pre-training. We report the training loss at every\nhundred steps.\nLoss mean\nWarmup 1.95±0.61\nStage 1 1.89±0.53\nStage 2 2.50±0.95\nStage 3 2.72±1.14\nStage 4 2.42±0.93\nTable 5: Inference loss of MLM-applied BERTBase for\neach curriculum stage.\nof the CCM-applied BERTMedium model at every\nhundred steps. In addition, we evaluate a mean loss\nof 3,000 examples for each stage using pre-trained\nBERTBase, consistent with Xu et al. (2020).\nFigure 3 shows a training curve of CCM-applied\nBERTMedium. We observe a trend of increasing\nlosses from stage 1 to stage 4 during training, which\nvalidates that our curriculum effectively trains the\nmodel in an easy-to-difficult order. In addition, Ta-\nble 5 shows the inference MLM loss of pre-trained\nBERT. In Table 5, we also observe a similar ten-\ndency to Figure 3, except for stage 4. We expect\nthat this is due to the difference in the masking\ndistribution on which the model is trained in this\nexperiment.\n5.4 Analysis of Initial Concept\nWe analyze the effect of the number of initial con-\ncepts and the selection criteria in medium-sized\nmodels to gain a better understanding of the initial\nconcept. The detailed results are presented in Table\n6, Table 7, and Figure 4.\nEffect of the number of concepts. We first inves-\ntigate the effect of the number of initial concepts\n7423\n#concepts CoLA MRPC SST Avg.\nMLM 44.9 85.4 89.6 73.3\n1k 47.6 85.1 88.9 73.8\n3k 48.0 86.7 90.9 75.2\n5k 46.8 86.2 90.7 74.5\n10k 46.6 86.1 90.7 74.4\nTable 6: Comparison of different number of initial con-\ncepts.\nInitial concepts CoLA MRPC SST Avg.\nHF 44.3 85.9 89.1 73.1\nRC 47.7 86.1 89.3 74.3\nHF, RC 48.0 86.7 90.9 75.2\nTable 7: Comparison of different criteria for initial con-\ncepts. Here, \"HF\" represents high frequency and \"RC\"\nrepresents the number of related concepts.\nby changing the number of the first stage concepts\nfrom 1k to 10k and analyze the impact on the per-\nformance. As shown in Table 6, the highest and\nlowest performances are obtained when the number\nof concepts is 3k, and 1k, respectively. Specifically,\nin the case of 1k, CCM does not have any improve-\nment over the MLM baseline in SST and MRPC\ntasks. These results imply that some enough num-\nber of initial concepts should be learned for our\nmethod to be effective (e.g., 3k in our experimental\nsetting).\nEffect of selection criteria. To investigate the\ninfluence of the criteria for selecting the easiest\nconcepts, we compare with the criteria of selecting\nthe easiest concepts from 1) many related concepts,\n2) high-frequency concepts, and 3) high frequency-\nmany related concepts. As shown in Table 7, our\ncriterion for high frequency-many related concepts\nperforms the best among other criteria. In addition,\nFigure 4 shows examples of concepts classified by\nthe above criteria. It shows that when a concept is\nselected only by frequency, stopwords such as an\nor the would be included, and when selecting only\nby the number of related concepts, many academic\nterms such as carbohydrate would be included\nin the initial concepts. These results indicate that\nconcepts that satisfy both criteria simultaneously\nare easier to learn and helpful in learning other\nconcepts later.\nFigure 4: A plotted graph demonstrating some concrete\nexamples of concepts according to the balance between\nthe \"frequency\" and the \"number of related concepts\".\nEach color represents a group of classes. For example,\nred refers to concepts with a high frequency but a low\nnumber of related concepts.\nMethods MRPC SST STS Avg.\nCCM1-hop 85.8 89.9 83.6 86.4\nCCM2-hop 86.7 90.9 83.6 87.0\nCCM3-hop 84.9 89.2 82.8 85.6\nCCM4-hop 85.1 89.1 82.2 85.5\nTable 8: Comparison of different hops.\n5.5 Analysis on Different Hops\nIn this work, we add the concepts within 2-hop\nwhen creating the curriculum to form the next stage\nconcept set. In this section, we compare the 2-hop\nstrategy with the hop strategies of other numbers\non BERTMedium. We only report the performance\nfrom 1-hop to 4-hop given that observation of the\nresults from 5-hop shows a negligible difference.\nThe results are shown in Table 8. It is observed\nthat CCM 2-hop outperforms the other strategies.\nWe observe that CCM1-hop comprises too few con-\ncepts at the second stage, leading to the degrada-\ntion of generalization performance. By contrast,\nCCM3-hop and CCM4-hop already comprise almost\nall concepts in the second stage, and thus fail to\nlearn concepts progressively.\n5.6 Number of Curriculum Stages\nTable 9 shows the results of the CCM experiments\naccording to the different number of curriculum\nstages. In this experiment, we expand the concepts\nwithin 2-hops at every stage, and at the last stage\nof all curricula, the concept set includes all the con-\n7424\nMRPC SST STS Avg.\n2 stage 84.5 90.5 83.3 86.4\n3 stage 85.1 90.7 84.4 86.7\n4 stage(w/o warmup) 82.1 89.7 83.2 85.0\n4 stage 86.7 90.9 83.6 87.0\n5 stage 85.7 90.5 83.8 86.6\nTable 9: Comparison of numbers of curriculum stages.\ncepts in the knowledge graph. Experimental results\nshow the highest performance when the curriculum\nis composed of four stages. We find that few con-\ncepts are added after 4 stages, and speculate that\nthis undermines curriculum effectiveness.\n6 Conclusion\nIn this work, we have proposed CCM that masks\neasy concepts first and gradually masks related con-\ncepts to the previously masked ones for language\nmodel pre-training. With the help of our carefully\ndesigned linguistic difficulty criteria and curricu-\nlum construction, CCM can offer an effective mask-\ning schedule for MLM. The experimental results\ndemonstrate that our curriculum improves conver-\ngence speeds and the generalization performance\nof MLM.\n7 Limitations\nCCM has achieved impressive results in improving\nthe efficiency of the LM pre-training, but some\nlimitations need to be tackled in future work. First,\nwe have not tested the efficacy of other knowledge\ngraphs such as Wikidata. We believe that utilizing\nseveral knowledge graphs jointly would expand the\ncoverage of concepts and relations.\nFurthermore, although we think all the relation-\nship in the knowledge graph reflects the relevance\nbetween concepts, it is necessary to study the de-\ngree to which each relationship reflects the rele-\nvance between concepts. The concepts connected\nby specific relationships may benefit more from pre-\nvious learning than concepts connected by others.\nWe plan to construct a more sophisticated curricu-\nlum by handling each relationship differently.\n8 Acknowledgement\nWe thank the anonymous reviewers for their helpful\ncomments. This work was supported by the Basic\nResearch Program through the National Research\nFoundation of Korea (NRF) grant funded by the\nKorea government (MSIT) (2020R1A4A1018309),\nNational Research Foundation of Korea (NRF)\ngrant funded by the Korea government (MSIT)\n(2021R1A2C3010430) and Institute of Informa-\ntion communications Technology Planning Evalua-\ntion (IITP) grant funded by the Korea government\n(MSIT) (No. 2019-0-00079, Artificial Intelligence\nGraduate School Program (Korea University)).\nReferences\nJeremy M. Anglin. 1978. From reference to meaning.\nChild Development, 49:969–976.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th Annual International Confer-\nence on Machine Learning, pages 41–48.\nDaniel Campos. 2021. Curriculum learning for lan-\nguage modeling. Computing Research Repository,\nabs/2108.02170.\nDaniel M. Cer, Mona T. Diab, Eneko Agirre, Iñigo\nLopez-Gazpio, and Lucia Specia. 2017. Semeval-\n2017 task 1: Semantic textual similarity multilingual\nand crosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic Eval-\nuation, pages 1–14.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations.\nRonan Collobert, Jason Weston, Léon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.\n2011. Natural language processing (almost) from\nscratch. J. Mach. Learn. Res., 12:2493–2537.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 4171–4186.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing.\nJeffrey L. Elman. 1993. Learning and development in\nneural networks: the importance of starting small.\nCognition, 48:71–99.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nBill Dolan. 2007. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the\nACL-PASCAL workshop on textual entailment and\nparaphrasing, pages 1–9.\n7425\nMarjorie S. Horton and Ellen M. Markman. 1980. De-\nvelopmental differences in the acquisition of basic\nand superordinate categories. Child Development,\n51:708–719.\nZihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen,\nJiashi Feng, and Shuicheng Yan. 2020. Convbert: Im-\nproving BERT with span-based dynamic convolution.\nIn Advances in Neural Information Processing Sys-\ntems 33.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Trans. Assoc. Comput. Linguistics, 8:64–\n77.\nKai A. Krueger and Peter Dayan. 2009. Flexible shap-\ning: how learning in small steps helps. Cognition,\n110:380–394.\nM. Pawan Kumar, Benjamin Packer, and Daphne Koller.\n2010. Self-paced learning for latent variable mod-\nels. In Advances in Neural Information Processing\nSystems 23, pages 1189–1197.\nYoav Levine, Barak Lenz, Opher Lieber, Omri Abend,\nKevin Leyton-Brown, Moshe Tennenholtz, and Yoav\nShoham. 2021. Pmi-masking: Principled masking of\ncorrelated spans. In Proceedings of 9th International\nConference on Learning Representations.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880.\nKoichi Nagatsuka, Clifford Broni-Bediako, and\nMasayasu Atsumi. 2021. Pre-training a BERT with\ncurriculum learning by increasing block-size of input\ntext. In Proceedings of the International Conference\non Recent Advances in Natural Language Processing,\npages 989–996.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper,\nPatrick LeGresley, Mostofa Patwary, Vijay Kor-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\nJulie Bernauer, Bryan Catanzaro, Amar Phanishayee,\nand Matei Zaharia. 2021. Efficient large-scale\nlanguage model training on GPU clusters using\nmegatron-lm. In Proceedings of the International\nConference for High Performance Computing, Net-\nworking, Storage and Analysis, pages 1–58.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1532–1543.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nCoRR, abs/2003.08271.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nMrinmaya Sachan and Eric P. Xing. 2016. Easy ques-\ntions first? A case study on curriculum learning for\nquestion answering. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artificial Intelligence , pages\n4444–4451.\nYu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng,\nXuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu,\nHao Tian, and Hua Wu. 2019. ERNIE: enhanced\nrepresentation through knowledge integration. CoRR,\nabs/1904.09223.\nYi Tay, Shuohang Wang, Anh Tuan Luu, Jie Fu, Minh C.\nPhan, Xingdi Yuan, Jinfeng Rao, Siu Cheung Hui,\nand Aston Zhang. 2019. Simple and effective cur-\nriculum pointer-generator networks for reading com-\nprehension over long narratives. In Proceedings of\nthe 57th Conference of the Association for Computa-\ntionalx Linguistics, pages 4922–4931.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTrans. Assoc. Comput. Linguistics, 7.\nRonald J. Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforcement\nlearning. Mach. Learn., 8.\nBenfeng Xu, Licheng Zhang, Zhendong Mao, Quan\nWang, Hongtao Xie, and Yongdong Zhang. 2020.\nCurriculum learning for natural language understand-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6095–6104.\n7426\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Advances in Neural Infor-\nmation Processing Systems 32, pages 5754–5764.\nRunzhe Zhan, Xuebo Liu, Derek F. Wong, and Lidia S.\nChao. 2021. Meta-curriculum learning for domain\nadaptation in neural machine translation. In Proceed-\nings of Thirty-Fifth AAAI Conference on Artificial\nIntelligence, pages 14310–14318.\nXuan Zhang, Gaurav Kumar, Huda Khayrallah, Ken-\nton Murray, Jeremy Gwinnup, Marianna J. Mar-\ntindale, Paul McNamee, Kevin Duh, and Marine\nCarpuat. 2018. An empirical exploration of curricu-\nlum learning for neural machine translation. CoRR,\nabs/1811.00739.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: enhanced\nlanguage representation with informative entities. In\nProceedings of the 57th Conference of the Associ-\nation for Computational Linguistics , pages 1441–\n1451.\nYikai Zhou, Baosong Yang, Derek F. Wong, Yu Wan,\nand Lidia S. Chao. 2020. Uncertainty-aware cur-\nriculum learning for neural machine translation. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6934–\n6944.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the\nIEEE International Conference on Computer Vision,\npages 19–27.\n7427",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8259543180465698
    },
    {
      "name": "Curriculum",
      "score": 0.684741199016571
    },
    {
      "name": "Masking (illustration)",
      "score": 0.6644197702407837
    },
    {
      "name": "Language model",
      "score": 0.5937553644180298
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5781602263450623
    },
    {
      "name": "Construct (python library)",
      "score": 0.542100191116333
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5089051723480225
    },
    {
      "name": "Security token",
      "score": 0.5070781707763672
    },
    {
      "name": "Natural language processing",
      "score": 0.44537782669067383
    },
    {
      "name": "Key (lock)",
      "score": 0.43767350912094116
    },
    {
      "name": "Training (meteorology)",
      "score": 0.41398629546165466
    },
    {
      "name": "Graph",
      "score": 0.41392406821250916
    },
    {
      "name": "Machine learning",
      "score": 0.4076463282108307
    },
    {
      "name": "Theoretical computer science",
      "score": 0.12651848793029785
    },
    {
      "name": "Programming language",
      "score": 0.09856075048446655
    },
    {
      "name": "Psychology",
      "score": 0.08646044135093689
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I197347611",
      "name": "Korea University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I87111246",
      "name": "Catholic University of Korea",
      "country": "KR"
    }
  ],
  "cited_by": 8
}