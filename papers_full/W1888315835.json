{
    "title": "Language modeling using x-grams",
    "url": "https://openalex.org/W1888315835",
    "year": 2002,
    "authors": [
        {
            "id": "https://openalex.org/A2182381325",
            "name": "A. Bonafonte",
            "affiliations": [
                "Universitat Politècnica de Catalunya"
            ]
        },
        {
            "id": "https://openalex.org/A2143468948",
            "name": "J.B. Marino",
            "affiliations": [
                "Universitat Politècnica de Catalunya"
            ]
        },
        {
            "id": "https://openalex.org/A2182381325",
            "name": "A. Bonafonte",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2143468948",
            "name": "J.B. Marino",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6635138082",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W1585489087"
    ],
    "abstract": "In this paper, an extension of n-grams is proposed. In this extension, the memory of the model (n) is not fixed a priori. Instead, first, large memories are accepted and afterwards, merging criteria are applied to reduce complexity and to ensure reliable estimations. The results show how the perplexity obtained with x-grams is smaller than that of n-grams. Furthermore, the complexity is smaller than trigrams and can become close to bigrams.",
    "full_text": "LANGUAGE MODELING USING X-GRAMS \nABSTRACT \nAntonio Bonafonte and Jose' B. Marin'o \n{antonio, canton) @gps. tsc.upc.es \nUniversitat Polithcnica de Catalunya \nclGran Capita s/n \n08034 Barcelona (SPAIN) \nIn this paper, an extension of n-grams is proposed. In this \nextension, the memory of the model (n) is not fixed U priori. \nInstead, first, large memories are accepted and afterwards, \nmerging criteria are applied to reduce complexity and to ensure \nreliable estimations. The results show how the perplexity \nobtained with x-grums is smaller than that of n-grums. \nFurthermore, the complexity is smaller than nigrum and can \nbecome close to bigrums. \n1. INTRODUCTION \nLanguage modeling has been studied under two different points \nof view. First, as a problem of grammar inference: in this case the \nmodel has to discriminate the sentences which belong to the \nlanguage from those which do not belong. Second, as a problem \nof probability estimation. \nIf the model is used to recognize (usually speech, but also printed \ncharacters or other panem recognition taks) the decision is \nusually based on the mimum U posrerion rule. The best \nsentence L is chosen so that the probability of the sentence, \nknowing the observations 0, is maximized \nThe rule of the language model is to provide a good estimation of \npfL) for each sentence which has to be evaluated on the \nmaximization. If a grammar is used it should be a stochastic \ngrammar where a probability is assigned to each rule of the \ngrammar. \nIf the sentence L is composed of m words, then the probability \ncan be expressed as \nThe number of parameters to estimate becomes intractable as the \nlength of the sentence (m) increases. N-grams are the most \nextended method to reduce this number approximating the \nprobability of a word as if only the n-1 most recent words have \ninfluence. Thus, \nThis research was supponed by am under contract TIC95-0884-CO4-02 \nFor each hisrory <w,,+I ... Wi-l> a probability dismbution has to \nbe estimated. If the lexicon size is K, the number of distributions \nis K\"\", and the number of probabilities to be estimated are K\". \nEven for a lexicon of moderated size (K=lOW), the number of \nprobabilities to be estimated for nigrums (n=3) is larger than \n1 OOO millions. \nThe training material to estimate such number of parameters is \nalways sparse. Therefore, the mimum likelihood estimator is \nnot appropriated. A number of proposals have appeared to \nsmooth the probabilities, being buck-of [l] the most standard \nreference. \nThe value of n is usually chosen small (2 or 3) in order to reduce \nthe number of parameters of the model. In this way the estimation \nis more reliable. Furthemore, both, the storage needed and the \nrecognition search space, are smaller. \nHowever, the n-gram models can also be studied from a \ngrammatical point of view [2] since the n-grum are a well \nknown subset of the regular grammars: the k-zesttable grammars \nin smct sense [4]. Therefore, the model can be represented as a \nfinite state automaton. Next section illustrates how the number of \nstates is the same than the number of histories observed in the \ntraining data. Therefore, as far as the training corpus is finite, the \nsire of the model does not grow exponentially with the value of n. \nFurthermore, the goodness of the estimation of the probabilities \ncan be established by other criteria which are not the length Of \nthe conditioning history. Then, the number of conditioning words \ndepends on each particular case. That is the reason why we have \nname these models as x-grom. \nThe paper is organized as follows. Next section illustrates how \nthe n-grums can be represented as a finite state automaton (FSA). \nFurthermore, the number of states of the FSA and perplexity for \ndifferent values of n are evaluated. This representation allows the \nuse of some new smoothing methods, as those proposed by \nBordel et al [2]. Some methods are proposed on section 3 to \nreduce the perplexity of the model. Section 4 is the core of the \npaper and introduces two criteria which are used to validate the \nestimation of probabilities. The use of these criteria allows to \ncontrol the tradeoff between goodness of the model and \ncomplexity. For instance, it is shown how x-grum can reduce \nsignificantly the complexity with respect to m.gramr with the \nsame performance. On the other hand, if the complexity of the \nmodel is the same than mgrams, the perplexity decreases. \n- \n394 \n2. N-GRAM REPRESENTATION \nIf a proper method is used to smooth n-grams, the number of zero \nprobabilities is null. However, the smoothing methods usually \nallows a compact representation. In this section we show a proper \nrepresentation of a n-gram which has been smoothed by the back- \noff [l] method. This representation allows low storage for the \nprobabilities values and fast access. This representation will be \nused in the rest of the paper. \nFirst, we briefly review the theory of back-off. The basic idea is \nthat if a history <wi,+l __. wi.l> is not present in the training data \nthen pdw; /wi -,,+ I... wi.1) is approximated by pdw; /w;,+t ... w;.~). \nOn the other hand, if <wi.,,+l ... w;.I> exists, then some discount \n(dC,, I 1) is applied to the relative frequency so that a mass of \nprobability is reserved. This mass can be distributed among those \nwords which have not been observed following <w;.,,+I ... w;.+ \non the wining data. Therefore, \npdwi /Wi.,,+i ... w;-1) = \npdw; /w;,+2.. . Wi-1) ifc(w;-\"+ I... Wi.1) = 0 \nif C(Wi.,,+l.. . w;) > 0 \nR(wl .*I... w;.1)pdwi/w .\"+ Z... Wi.1) if C(Wj-*l ... Wi) = 0 \nIn the above expression, C(h) gives the number of times that the \nhistory h appears on the training data, R is a normalization \nconstant chosen so that the stochastic restriction is verified and N \nis the total number of words (and n-grum) in the training data. \nN-grams can be represented by FSA in the following way: for \neach possible history cwln+l _.. W;-Q a state is defined. Each \ntransition departing from this state is labeled by a word w; and \nthe probabilitypfwdw;.,,+l ... wj.1). The end of the transition is the \nstate associated to the following history, i.e., <w;~+I ... w,>. The \nprobability of a sentence ~(wI. .. w,) can be computed multiplying \nthe probabilities of a path of the automaton. Furthermore, the \npath can be determined deterministically because there is exactly \none transition leaving each state for each word of the lexicon. \nThe number of possible histories increases exponentially with the \nmemory (n-1) of the n-gram. However, if a history does not exist \non the training data, then the assodated state is not needed. For \ninstance, suppose that being at state CW;,*I ... w~-I>, w; arrives. \nThe transition labeled with the pair (w;, p(wi/w,,+~ ... wi.1)) \nshould go to state <w1+,+2 ... w;>. However, if this history does \nnot exists on the training data, then when a new word W;+I \narrives, the probability p(Wj+dwi++? ... w;) is backed off to \np(wl+~/wl,,+3 ... w;). Therefore, the transition labeled with w; can \ngo directly to <w,.,,*J ... wi>. Therefore, the state cw1+,+2 ... wi> is \nnot needed. \nFigure 1 shows the number of states of a FSA which represents a \nn-gram as a function of n. The n-gram has been applied to model \ngeographic inquires to a database. 14.000 inquires have been \nused to estimate the probabilities while 1.OOO have been reserved \nto evaluate the perplexity on the test set. The lexicon consist of \naround 1400 different words. This task, referenced on [2,3], will \nbe used on the rest of the paper. \nIt can be seen how the number of nodes increases significantly \nwith the value n. However, the increasing is far away from being \nexponential. For instance, for n = 8, the number of different \npossible histones exceeds IOz, but less than 150,000 have been \nobserved on the training data. \nFigure 1: Number of states as a function of the memory (n) of \nthe n-grams. \n3. SYNTACTIC SMOOTHING \nThe back-off method reviewed in last section bases its \nfundaments on the join distribution probabilities: p(w,,+~ ... w;). \nHowever, the merhod has some difficulties. For instance, \nfollowing suggestions of Katz [I], the constants dC,, can become \nzero. In that case no probability is reserved for unseen events. To \navoid this situation a minimum probability mass has to be \nreserved. \nAnother possibility is to smooth directly the conditioning \nprobabilities. This can be viewed as a smoothing of the \nprobabilities of transitions leaving each state of the FSA. \nBordel et al [2] proposed to reserve a mass probability Q at each \nsrate E, which is distributed among the unseen words departing \nthis state. The estimation of Q proposed by Bordell is \nwhere r is the number of different transitions departing the state \nE which has been seen on the training; q is the number of times \nthat state Q has been visited while parsing the training data. For \ninstance, if all transitions have been used once (r=q), the mass \nprobability reserved is 0.5. \nThe above expression estimates the reserved mass probability \nbased on the number of different words which have been \nobserved leaving a state. On the other hand, the original back off \nmethod estimates mass probability based on words which have \nbeen observed leaving states exactly once. Other expressions to \nestimate the reserved mass probability have been derived on [3]. \nThe lower perplexity is obtained if Q is estimated as \n395 \nI refers to the transitions leaving the state E, and C(I) is the \nnumber of times that transition f is used to parse the training \ndata. The method proposed by Bordel, equivalent to use a \nfunction f(.) which always returns 1, gives better results than the \noriginal back-off method. \nStill, slightly smaller perplexities are obtained if function f(.) \nreturns smaller values for higher C(.). Particularly, best results \nare obtained iff decreases linearly, being zero for values higher \nthan ten. Note that in this case, the mass probability reserved for \nunseen words is smaller. Figure 2 shows the perplexity on the \ntest set for this smoothing method compared with original back- \noff. The graphs are plotted as a function of the memory n of the \nn-gram. The following points can be observed: \nThe perplexity obtained with 4-grm is \nnoticeable smaller that with trigrums. \nFor high values of n, the syntactic back-off is \nbetter than the original back-off. \nThe minimum perplexity for syntactic back-off is \nobtained for 5-gram. However, the difference \nwith 4-grwns is small. \n12345678 \nFigure 2: Perplexity as a function of the memory n of n-grams \nfor original back-off smoothing and syntactic smoothing. \n4. X-GRAMS: SIMPLIFYING THE FSA \nAs it has been stated in the introduction, the n-grums is the \nlanguage model more extensively used in speech recognition. The \nvalue of n is chosen small, usually 2 or 3, in order to avoid \ncomputational and estimation problems. In section 2, we have \nshown how the size of the n-gram does not increases \nexponentially with R. On the other hand, in section 3, we have \nshown how the best estimation is achieved for n = 5. However \nwhen trigram and 5-grams are compared, it is observed that a \nhigh prize has to be paid to decrease the perplexity (?'e: \ntrigram: 8,63 1 states; PP = 7.03 \n5-gram: 49,697 states; PP = 6.45 \nThe problem is that if 5-grams are used, all the words are \nconditioned by the 4 preceding words. However, it can happen \nthat the probabilities estiimated for these long histones are not \nalways reliable. On the other hand, even if the estimation of the \nprobabilities is good, perhaps it gives no additional information \nwith respect to the probability given the three preceding words. \nTo cope with the problenn, the idea is to apply a merging-state \nalgorithm. Each state associated to a history ai, ... w;> is \ncandidate to be merged with state *;,I ... w;>. Two criteria \nhave been applied: \n1. The states are merged if the number of times that \nhistory 0;-m ... wj> has been observed on the \ntraining data is smaller than h,,. A similar \nprocedure which is usually applied to n-gram \nestimation is to force zero to those counts with a \nlow value. However, in our case, the value of \nC(w; .,,,... w;) is used to estimatep(wi/w;.m ... Wj.1) \n2. The states are merged if the information of \ndismbution p = ~(W/Wi~...Wj) is similar to that of \ndismbution q = p(w/wi+,,+~ ... wi). The difference \nbetweenp and q is measured by the divergence, a \nwell known function in information theory. If the \nsize of the lexicon is J then the divergence D is \ndefined as: \nThe divergence is zero only if p equals q. On \nother cases it is greater than zero. If the \nlogarithms are taken to the base 2, the \ninformation is measured in bits. If natural \nlogarithms are taken, the measure is in M~S. \nIf state ai-m ... Wi> is merged with state <W;-m+l.-.Wi>, then the \ntransitions which arrived to <w;+,...w;> have IO be addressed to \nOi,] ... Wi>. \nTables 1 and 2 show the perplexity and the number of states \nwhen either criterion 1 or 2 are applied. \nTable 1: Perplexity and nunnber of states of the FSA needed to \nrepresent a S-grum and perplexity as a function of the minimum \nnumber of times that a histoiy has to be observed in the training \ndata in order to include the associattd state. \nThe analysis of table 1 shows how the number of states can be \nreduced greatly without decreasing the perplexity. In fact, if k,,,,,, \nis 3, the perplexity is better dim if merging is not applied. \n.. \n396 \n12.550 I 6.45 I \n0.3 \n0.4 \n0.5 \n- .- I \n9,395 633 \n7,415 6,67 \n5,800 6,85 \n0.15 \n0.2 \n0.25 \n0.3 \nTable 2: Perplexity and number of states of the FSA needed to \nrepresent a 5-gram and perplexity as a function ot the minimum \ndiscrimination of each state, measured by the divergence function \nin MIS. \nOn the other hand, it can be observed how the perplexity is \nalways smaller than the value obtained with rrigramr. \nFurthermore, for large values of k,,, the number of states is \nmuch smaller than with trigrum. \nThe results of using the divergence as merging criterion are \nsimilar. However, for the same number of states the first criterion \ngives better performance. \nThe states which are merged using each criteria are not the same. \nAn analysis reveals that there are a large number of states with \nlow occupancy (bad estimation of the probabilities) and other \nstates which present little discrimination. Therefore, both criteria \ncan be applied simultaneously on the merging algorithm. \nFor instance, if the value of kmn is fixed, the number of states of \nthe n-grum can be controlled adjusting the minimum \ndiscriminations required. For instance, table 3 illustrates this idea \nshowing the perplexity and the number of states as a function of \nthe discnmination when k,,,,,, is fved to 3. \n5,156 6.41 \n4,566 6.46 \n4,172 6.54 \n3,841 I 6.56 \nrepresent a 5-gram and perplexity as a function of the minimum \ndiscrimination of each state, measured by the divergence function \nin mu. Each state should be visited at least 3 times. \nIt is shown how it is worth to combine both criteria. For instance, \nperplexity smaller than 6.5 can be obtained with around 4,500 \nstates. However, if only one criterion is applied, either 8000 or \n12000 states are needed. \nIn fact, as changes on the perplexity values are small, the \nconclusion which can be drawn from table 3 is that complexity of \nthe n-gram can be enormously reduced without degrading \nsignificantly the perfoxmance of the language model. \nThe experiments reveal how the criteria to merge states can \neffectively select the states which are relevant from those which \nare not. The idea beyond x-gramr is that, instead of \napproximating the value of ~(wdwl---wi-l) by P(wc/win+~...wi.l), it \nis better to accept all the states associated KO hisrories of all the \nlen,@s as candidates and, afterwards, select those which are \nrelevant. In practice, this can be implemented by merging states \nfrom an n-gram with sufficient large value of n. \nTable 4 shows the number of states and the perplexity of x-grums \nfor two cases. In the first case (x-grum(l)), the parameters of the \nmerging cnteria are k,, = 5 and D,, = 0.35 nats. These \nparameters are chosen so that the number of states is low. The \nsecond case (x-grwn(2)) uses k,, = 2 and D,, = 0.1 nats. In this \ncase, the parameters are chosen KO have low perplexity. It should \nbe noted that the choice of the parameters is not a critical issue; \nboth criteria exhibit a smooth evolution. To ease comparisons, \nthe performance of bigrums, and trigrums is included on the \nsame table. It can be shown how x-gram outperform n-gram \nwith controlled complexity. As a curiosity, the maximum length \nof histories associated to the states of x-grams (1) and (2) \nhappens to be 7 and 8 respectively. \n2,943 I 6.67 \nx-grm(2) I 7,661 1 6.39 \nTable 4 Perplexity and number of states of the FSA needed to \nrepresent x-grams compared with bigrams and trigrums. X- \ngram(1): k,,,,\" = 5, D,, = 0.35 nats; x-grm(2): k,,i, = 2, Dmin = \nOS nats \n5. DISCUSSION \nIn this paper x-grams has been proposed as an extension of R- \ngrams. The evaluation over a task with low perplexity and \nmoderate lexicon (1 300 words) shows the good performance of x- \ngrams. The goodness of this model for more complex tasks will \nbe evaluated on the near future. \n6. REFERENCES \n1. S.M. Ka& \"Estimation of Frobabiiities from Sparse Data \nfor the Language Model Component of a Speech \nRecognizer\", IEEE Truns. on ASSP, Vol. ASSP-35, N\" 3, \npp. 4OO-401, March 1987. \n2. G. Bordel, L Torres and E. Vidal, \"Back-off smoothing in \na syntactic approach to language modelling\", Proc. of the \nIcsLp194, pp. 851-854, Yokohama, 1994. \n3. A. Bonafonte, Speech Understanding on Semantic \nResmhed Tmks. PhD. Dissertation, Universitat \nPolitknica de Catalunya, Barcelona, 1995. \n4. E. Segarra, UM Aproximcidn Inductiva a la \nComprensidn del Discurso Continuo, PhD. Dissertation, \nUniversidad Politicnica de Valencia, 1993. \n397 "
}