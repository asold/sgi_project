{
  "title": "Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language Models",
  "url": "https://openalex.org/W4388142096",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2102085487",
      "name": "Andrew L. Smith",
      "affiliations": [
        "Ottawa Hospital",
        "University of Ottawa"
      ]
    },
    {
      "id": "https://openalex.org/A2088182740",
      "name": "Felix Greaves",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A2692887871",
      "name": "Trishan Panch",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2102085487",
      "name": "Andrew L. Smith",
      "affiliations": [
        "Ottawa Hospital",
        "University of Ottawa"
      ]
    },
    {
      "id": "https://openalex.org/A2088182740",
      "name": "Felix Greaves",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2692887871",
      "name": "Trishan Panch",
      "affiliations": [
        "Harvard University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2476277876",
    "https://openalex.org/W2490243977",
    "https://openalex.org/W6917611913",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4319301677",
    "https://openalex.org/W4366085722",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385245566"
  ],
  "abstract": null,
  "full_text": "OPINIO N\nHallucination or Confabulation?\nNeuroanatomy as metaphor in Large\nLanguage Models\nAndrew L. Smith\nID\n1,2\n*, Felix Greaves\n3\n, Trishan Panch\n4\n1 Department of Psychiatry, University of Ottawa, Ottawa, Ontario, Canada, 2 Department of Mental Health,\nThe Ottawa Hospital, Ottawa, Ontario, Canada, 3 School of Primary Care and Public Health, Imperial College\nLondon, London, United Kingdom, 4 Harvard TH Chan School of Public Health, Harvard University,\nCambridge , Massach usetts, United States of America\n* andrewl smith@to h.ca\nAs Large Language Models (LLMs) and their capabilities become an increasingly prominent\naspect of our workflows and our lives, it is important that we are thoughtful and deliberate\nwith the words we use to refer to the inner workings and outputs of this technology. We think\nthat conveying the complex functions (and malfunctions) of LLMs using metaphorical lan-\nguage that is precise and accurate can lead to a better understanding of these powerful tools\namong both the academic community and the public. If we are meticulous in our choice of\nmetaphors, we open ourselves up to the possibility of achieving a better shared understanding\nof the complex concepts in this exciting new field. Here, we give the specific example of AI\n“hallucinations” and demonstrate how a change in metaphorical language can lead to new\nways of understanding and may even foreshadow future directions in the development of arti-\nficial intelligence systems.\nIn psychiatry, hallucinations are a relatively well-defined perceptual phenomenon referring\nto sensory experiences without associated external, or ‘real-world’ stimuli. Clinically, halluci-\nnations are commonly associated with conditions such as schizophrenia, bipolar disorder, and\nParkinson’s disease [1]. Employing the term \"hallucination\" to characterize the inaccurate and\nnon-factual outputs generated by LLMs implies acceptance of the notion that LLMs are\nengaged in perceiving, that is, becoming consciously aware of a sensory input. While this is a\nsubject of some ongoing debate, there is currently no evidence that AI has gained conscious\nawareness [2]. LLMs do not have sensory experiences, and thus cannot mistakenly perceive\nthem as real. As such, we believe the term “hallucination” misrepresents the nature of the pro-\ncess occurring within LLMs which it has been used to describe. The model is not “seeing”\nsomething that is not there, but it is making things up.\nMore accurate terminology is found in the psychiatric concept of confabulation, which\nrefers to the generation of narrative details that, while incorrect, are not recognized as such.\nUnlike hallucinations, confabulations are not perceived experiences but instead mistaken\nreconstructions of information which are influenced by existing knowledge, experiences,\nexpectations, and context. Confabulation can occur in various clinical conditions including\ndementia, Wernicke-Korsako ff’s syndrome, schizophrenia, traumatic brain injury (TBI), and\ncerebrovascular accidents (CVAs) [3]. Confabulation is frequently associated with a general-\nised lack of awareness of one’s deficits often seen in right sided CVAs or TBIs, as well as in\nbipolar disorder, schizophrenia, and the dementias [4]. When answering questions, LLMs gen-\nerate responses based on learned patterns in very large datasets [5]. The output of LLMs can\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000038 8 Novemb er 1, 2023 1 / 3\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Smith AL, Greaves F, Panch T (2023)\nHallucination or Confabula tion? Neuroanatom y as\nmetaphor in Large Language Models. PLOS Digit\nHealth 2(11): e0000388. https:// doi.org/10.1371 /\njournal.pdi g.0000388\nEditor: Shlomo Berkovsky, Macquarie University,\nAUSTRALIA\nPublished: November 1, 2023\nCopyright: © 2023 Smith et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nFunding: The authors received no specific funding\nfor this work.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nvary significantly due to the probabilistic nature of the transformer architecture [6] and this\nsame lack of deterministic, or rules-based, output likely accounts for the tendency of LLMs to\nproduce non-factual or “confabulated” narrative details. For ease of reference, we provide a\nsummary of the relevant points of comparison between these terms in Table 1.\nIn using language derived from human neurocognitive processes, we recognize the risk of\nseeming to advance an anthropomorphic view of LLMs [7]. This is not our intention and does\nnot accurately reflect our thinking. Rather than anthropomorphizing LLMs as displaying\nhuman traits and behaviours and thereby implying capacity for empathic connection, motiva-\ntion for power, and other similarly far-fetched ideas, we instead believe that using linguistic\nterms more faithful to the underlying technology provides needed clarity upon which to build\ntrust and drive adoption. Using metaphoric language with precision, functional aspects of\nLLMs can be subjected to analogical reasoning, thereby yielding plausible roadmaps for devel-\nopment of more powerful, more advanced, and more helpful models.\nAs noted above, confabulation is often associated with clinical conditions involving right\nhemispheric deficits. The right hemisphere of the brain is thought to be responsible for a wide\nrange of cognitive functions, including processing non-verbal cues, understanding the emo-\ntional state of others, and appreciating the nuances of music [8]. When this hemisphere is\ndamaged the left hemisphere predominates but does so in a more literal and simplistic way.\nThis can lead to a focus on detail, a preference for sequencing and ordering, and an overly\noptimistic and often unrealistic assessment of the self [8].\nWe suggest that the analogy between the left hemisphere’s orientation to the world and cur-\nrent LLMs is instructive. With the availability of massive computational power, LLMs vastly\noutperform the human brain’s ability to absorb and retain large amounts of information and\ncan produce outputs on a scale that no individual human could. Yet LLMs, like the unmiti-\ngated, confabulating left hemisphere, may confidently produce false information. We propose\nthat the “human-in-the-loop” approach [9] to responsible use of AI in the medical context\nmay be seen as a reintroduction of the contextualising and sense-making functions of the right\nhemisphere, in the form of direct human oversight. While the landscape here changes rapidly,\nat this time humans remain better suited to real-world decision making under conditions of\nuncertainty and are for now alone in our capacity for empathy, embodiment, and the complex\nvalue-based prioritisation required to make judgements in medical care. In the collaboration\nbetween humans and AI technology, we witness a synergistic relationship reminiscent of the\nbrain’s right and left hemispheres. Just as these brain regions possess unique strengths that\ncomplement each other, humans and AI each contribute capabilities that may compensate for\nthe other’s limitations. This partnership bears the promise of reshaping industries and solving\nunimaginably complex problems.\nOver the longer term will we see development of artificial intelligence analogues to right\nhemispheric ways of thinking? We think the answer is in many ways, yes, though perhaps not\nto the dystopian extent that some may fear. Artificial correlates of empathy may yet be years\naway, but we see multiple specialised LLMs interacting in a structured way, much like the\nTable 1. \"Hallucinat ion\" vs \"Confabul ation\" for LLM Output.\nTerm Pros Cons\nHallucinat ion\nSensory experiences not associated with an\nexternal stimulus\nCommonly underst ood term\nCaptures the ’creation’ aspect of LLM outputs\nSuggests a sensor y perception, not applicable to LLMs\nImplies the presence of consciousnes s or subjectiv e\nexperience , which LLMs do not have\nConfabul ation\nProduction of a false memory that is not\nintended to deceive\nAccurate ly describ es the pattern-ba sed, context-de pendent\ngeneratio n of content by LLMs\nDoes not imply consciousnes s or subjectiv e experience\nLess commonly understood term\nLess evocative\nhttps://do i.org/10.1371/j ournal.pdig. 0000388.t00 1\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000038 8 Novemb er 1, 2023 2 / 3\ngated interconnectivity of our neuroanatomy, on the very near-term horizon [9]. There is col-\nlective wisdom preserved in evolution’s partitioning of selectively interconnected brain struc-\ntures, which we believe provides a map for our approach to the development of advanced AI\nsystems.\nThe use of inaccurate language often leads to pervasive misunderstandings which become\nincreasingly difficult to correct over time. Particularly in the medical context, where adoption\nof new technologies can have both immediate and long-term implications for the health and\nwell-being of the population, it is important that we choose our words, and thus our metaphors\ncarefully. We propose using the term “confabulation” not merely to correct a misnomer, but\nbecause the neuroanatomical analogy it implies unlocks new ways of understanding and sug-\ngests exciting new paths for technological advancement.\nAuthor Contributions\nConceptualization: Andrew L. Smith.\nWriting – original draft: Andrew L. Smith, Felix Greaves, Trishan Panch.\nWriting – review & editing: Andrew L. Smith, Felix Greaves, Trishan Panch.\nReferences\n1. Tamminga CA. Schizoph renia and other psychotic disorde rs: Introducti on and overview. In: Sadock BJ,\nSadock VA, Ruiz P, eds. Kaplan and Sadock’s Comp rehensive Textbook of Psychiatry. 9th edition . Phil-\nadelphia: Lippinco tt Williams and Wilkins; 2009. p. 1432.\n2. Bubeck S, Chandrase karan V, Eldan R, Gehrke J, Horvitz E, Kamar E, et al. Sparks of Artificial General\nIntelligenc e: Early Experime nts with GPT-4. arXiv preprint arXiv:2303.12 712. 2023 Mar 22. https://doi.\norg/10.48550 /arXiv.2 303.12712\n3. Schnider A. Aetiologies and anatomy of confabula tion. In: The Confabula ting Mind, 75–144 . Oxford\nUniversity Press; 2008.\n4. Schnider A. Disorders associat ed with confabula tion. In: The Confabula ting Mind, 145–192. Oxford\nUniversity Press; 2008.\n5. Radford A, Sutskever I, Le Q, Vinyals O, Wu J, Olah C, et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.1 4165. 2020 May 14.\n6. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A N, et al. Attentio n is all you need.\nAdvances in neural informati on processing systems, 30; 2017.\n7. Tan C. On AI Anthropomor phism [Interne t]. Human-C entered AI; 2023. Availab le from: https://m edium.\ncom/hum an-centered- ai/on-ai-an thropomorphi sm-abff4c ecc5ae\n8. McGilchri st I. Judgemen t. In: The Matter with Things: Our Brains, Our Delusions and the Unmaking of\nthe World, 135–18 0. Perspecti va Press; 2023.\n9. Sejnowski TJ. Large Language Models and the Revers e Turing Test. Neural Comput. 2023 Feb 17; 35\n(3):309–34 2. https://do i.org/10.1162 /neco_a_ 01563 PMID: 367461 44; PMCID: PMC101770 05.\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000038 8 Novemb er 1, 2023 3 / 3",
  "topic": "Confabulation (neural networks)",
  "concepts": [
    {
      "name": "Confabulation (neural networks)",
      "score": 0.8185721635818481
    },
    {
      "name": "Metaphor",
      "score": 0.6697625517845154
    },
    {
      "name": "Psychology",
      "score": 0.6318738460540771
    },
    {
      "name": "Cognitive psychology",
      "score": 0.5150184631347656
    },
    {
      "name": "Cognitive science",
      "score": 0.5034512877464294
    },
    {
      "name": "Linguistics",
      "score": 0.42882102727890015
    },
    {
      "name": "Cognition",
      "score": 0.21495717763900757
    },
    {
      "name": "Neuroscience",
      "score": 0.19314157962799072
    },
    {
      "name": "Philosophy",
      "score": 0.15679070353507996
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800722420",
      "name": "Ottawa Hospital",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I153718931",
      "name": "University of Ottawa",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I47508984",
      "name": "Imperial College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    }
  ],
  "cited_by": 55
}