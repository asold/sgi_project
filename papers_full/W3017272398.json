{
    "title": "DSTC8-AVSD: Multimodal Semantic Transformer Network with Retrieval Style Word Generator",
    "url": "https://openalex.org/W3017272398",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2604649472",
            "name": "Lee Hwanhee",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3154006519",
            "name": "Yoon, Seunghyun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2754032703",
            "name": "Dernoncourt, Franck",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3081809512",
            "name": "Kim, Doo Soon",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2302713070",
            "name": "Bui, Trung",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2788288377",
            "name": "Jung, Kyomin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2810643877",
        "https://openalex.org/W2768661419",
        "https://openalex.org/W2560730294",
        "https://openalex.org/W2964324871",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2980339970",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2805707570",
        "https://openalex.org/W2911400095",
        "https://openalex.org/W2972745026",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3099388488",
        "https://openalex.org/W2986292373",
        "https://openalex.org/W2980075309",
        "https://openalex.org/W2964213933"
    ],
    "abstract": "Audio Visual Scene-aware Dialog (AVSD) is the task of generating a response for a question with a given scene, video, audio, and the history of previous turns in the dialog. Existing systems for this task employ the transformers or recurrent neural network-based architecture with the encoder-decoder framework. Even though these techniques show superior performance for this task, they have significant limitations: the model easily overfits only to memorize the grammatical patterns; the model follows the prior distribution of the vocabularies in a dataset. To alleviate the problems, we propose a Multimodal Semantic Transformer Network. It employs a transformer-based architecture with an attention-based word embedding layer that generates words by querying word embeddings. With this design, our model keeps considering the meaning of the words at the generation stage. The empirical results demonstrate the superiority of our proposed model that outperforms most of the previous works for the AVSD task.",
    "full_text": "DSTC8-A VSD: Multimodal Semantic Transformer Network\nwith Retrieval Style Word Generator\nHwanhee Lee1, Seunghyun Yoon1, Franck Dernoncourt2, Doo Soon Kim2, Trung Bui2 and Kyomin Jung1\n1Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea\n2Adobe Research, San Jose, CA, USA\n{wanted1007, mysmilesh, kjung}@snu.ac.kr, {franck.dernoncourt, dkim, bui}@adobe.com\nAbstract\nAudio Visual Scene-aware Dialog (A VSD) is the task of gen-\nerating a response for a question with a given scene, video,\naudio, and the history of previous turns in the dialog. Exist-\ning systems for this task employ the transformers or recurrent\nneural network-based architecture with the encoder-decoder\nframework. Even though these techniques show superior per-\nformance for this task, they have signiﬁcant limitations: the\nmodel easily overﬁts only to memorize the grammatical pat-\nterns; the model follows the prior distribution of the vocab-\nularies in a dataset. To alleviate the problems, we propose\na Multimodal Semantic Transformer Network. It employs a\ntransformer-based architecture with an attention-based word\nembedding layer that generates words by querying word em-\nbeddings. With this design, our model keeps considering the\nmeaning of the words at the generation stage. The empiri-\ncal results demonstrate the superiority of our proposed model\nthat outperforms most of the previous works for the A VSD\ntask.\nIntroduction\nRecently, multimodal tasks such as Visual Question An-\nswering (VQA) and visual dialog have attracted much atten-\ntion in the ﬁeld of artiﬁcial intelligence (Goyal et al. 2017;\nDas et al. 2017). This task is considered difﬁcult to tackle\nsince incorporating various types of input (i.e., audio, video,\nand text) requires different techniques to be jointly ap-\nplied. One such example, Audio Visual Scene-aware Dialog\n(A VSD), is the task of answering a question for a video clip\nand its corresponding dialogue history (Alamri et al. 2019;\nKim et al. 2019). The A VSD is considered a more chal-\nlenging task than a visual dialog task because the model\nneeds to deal with additional audio context, which is con-\nsists of sequential information. Furthermore, it needs to rec-\nognize the history of dialog along with the visual and acous-\ntic data for accurately answering the question. Previous\nworks (Hori et al. 2019b; Le et al. 2019; Schwartz, Schwing,\nand Hazan 2019) tackled the A VSD task with the sequence-\nto-sequence framework (Sutskever, Vinyals, and Le 2014),\nwhich is composed of encoder and decoder architecture. Al-\nthough sequence-to-sequence approaches show successful\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nperformance in various sentence generation tasks, the tech-\nnique shows major limitation when applying to multimodal\nquestion answering tasks: it easily overﬁts only to memo-\nrize the grammatical patterns and the prior distribution of\nthe vocabularies in a dataset; it fails to model the semantic\ninformation in the multimodal dataset.\nFor the A VSD task, this problem can be exacerbated since\nthe pattern of the question in the dataset is constructed in a\nstraightforward form. For example, for the question “What\ncolor is the door,” the model quickly learns the answer pat-\ntern, “the door is [any color] in color,” regardless of the ac-\ntual color information of the door. In other words, most to-\nkens for generating the answer will be selected without con-\nsidering what the door’s real color is. With this approach,\nthe output of the model receives a high automatic evalua-\ntion score (i.e., BLEU score) for a sentence without incor-\nporating information from other modalities (i.e., video and\naudio).\nTo alleviate the problem, we proposed multimodal seman-\ntic transformer networks (MSTN) that employ an attention-\nbased word embedding layer, which is inspired by Ma et al.\n(2018)’s work, in the decoder part of the Transformer archi-\ntecture (Vaswani et al. 2017). With the proposed architec-\nture, the MSTN keeps considering the meaning of the words\nat the generation stage. Experimental results show that our\nproposed method surpasses all of the baseline models.\nRelated Work\nMultimodal Transformer Network Recently (Le et al.\n2019) proposed Multimodal Transformer Networks (MTN),\nwhich is a state-of-the-art system for A VSD tasks. The MTN\nis a transformer-based encoder-decoder framework that has\nseveral attention blocks to incorporate multiple modalities\nsuch as video, audio, and text. The MTN is composed of\nthree major components: encoder layers, decoder layers, and\nquery-aware auto-encoders. Encoder layers encode each in-\nput by layer normalization and positional encoding. Decoder\nlayers generate the target sequences and use the multi-head\nattention mechanism to incorporate information from multi-\nple modalities. Finally, Auto-encoder layers encode video\nfeatures with query-aware attention and then tries to re-\nconstruct the queries to learn the attention in an unsuper-\nFigure 1: Overall Architecture of our model\nvised way. Our model is also based on the transformer\nencoder-decoder framework, but our model does not use\nauto-encoder loss and have more simple architecture.\nWord Embedding Attention Network Sequence-to-\nsequence architecture generally memorizes word patterns in\nthe training dataset other than understanding the meaning of\nthe word itself. Hence, the generated results from the model\nmay not reﬂect the semantic information of words. (Ma et\nal. 2018) point the main reason for this phenomenon is the\noutput layer of the decoder that consists of a linear transfor-\nmation and a softmax operation. To solve this problem, Ma\net al. (2018) proposes Word Embedding Attention Network\n(WEAN), which is a retrieval style word generator com-\npared to the linear projection layer in general sequence-to-\nsequence architecture. By substituting the linear projection\nlayer with WEAN, the model produces words by querying\nword embeddings, considering word meanings other than\nmemorizing the sequence pattern. We adopt this retrieval\nstyle word generator to this task.\nModel\nInspired by MTN (Le et al. 2019), we propose Multimodal\nSemantic Transformer Network (MSTN) that is also based\non transformer encoder-decoder framework. Our model is\ncomposed of two parts: encoder and decoder. Since we treat\ncaption, history as different modalities, we use ﬁve modali-\nties for this model.\nEncoder\nFor the text data, we embed the source sequence to continu-\nous representation and learn its embedding during training.\nWe use the features extracted from i3d-ﬂow for the video\ndata and use the features extracted from veggies for the au-\ndio data. For each source sequence Xm = (xm\n1 , ..., xm\nn ) for\nmodality m, we encode Xm with the encoder similar to that\nof transformer (Vaswani et al. 2017) except self-attention\nlayer which is similar to (Le et al. 2019). It is composed of\npositional encoding, layer normalization and feed-forward\nneural network. The result representation for each modality\nis Zm = (zm\n1 , ..., zm\nn ).\nFigure 2: Word Embedding Attention network\nDecoder\nGiven the output of the encoderZm = (zm\n1 , ..., zm\nn ) for each\nmodality m and target sequence Zy\n<t = (zy\n1 , ..., zy\nt−1) the\ndecoder generates t-th word yt. The model ﬁrstly encodes\nthe target sequence Zy\n<t with multi-head self-attention as\nin the encoder part. For video and audio data, we addition-\nally compute question-aware attention as in (Le et al. 2019),\nwhich is multi-head attention that uses a question as a query\nand uses other modalities as key and value. And then model\ncomputes multi-head attention with each modality m as (1).\nMulti-Head is standard multi-head attention in transformer\nwith 1-layer and 8-attention heads, and we use the same hy-\nperparameter for question-aware attention. After computing\nattention with all of the modalities, we concatenate them and\napply the linear projection. M is the number of modalities.\nZy\nm = MultiHead(Zy, Zm, Zm),\nOy\n<t = Linear(Concat(Zy\n1 , ..., Zy\nM)). (1)\nFinally, unlike the output layer of a typical decoder in\nsequence-to-sequence, we adopt word embedding attention\nnetwork to get a score through the dot-product betweenOy\n<t\nand embedding ei (i = 1, 2, .., n), where n is the vocabulary\nsize. And then take its maximum to generate the next word\nas follows:\nscore(oy\nt-1, ei) =oy\nt−1eT\ni ,\nP(yt) =softmax(score(oy\nt−1, ei)). (2)\nscore(ht, ei) =hteT\ni ,\nP(yt) =softmax(score(ht, ei)). (3)\nExperiments\nImplementation Details\nWe train the model for 20 epochs using Adam Opti-\nmizer (Kingma and Ba 2014) and use warmup steps like\n(Vaswani et al. 2017; Le et al. 2019). We train word em-\nbeddings with size 512 and add positional encoding to em-\nbeddings as in (Vaswani et al. 2017). Pre-trained embedding\nsuch as GloVe(Pennington, Socher, and Manning 2014) does\nnot show the difference in performance. We use only one-\ndecoder layer and use dropout with probability 0.2 and 512\nhidden units for all of the feed-forward layers. The maxi-\nmum length for decoding is 30, and we use a beam search\nwith beam size 5.\nExperimental Results\nTable 1 show the model performance on the A VSD dataset\nin DSTC7(Alamri et al. 2018) and DSTC8 (Kim et al. 2019).\nDSTC7 (single reference) DSTC7 (6 references) DSTC8 (6 references)\nB-4 M R-L C B-4 M R-L C B-4 M R-L C\nText+Video\nMTN [1] 0.135 0.165 0.365 1.366 - - - - - - - -\nMSTN 0.135 0.165 0.369 1.352 0.377 0.275 0.566 1.115 0.385 0.270 0.564 1.073\nMSTN w/o\nWEAN 0.131 0.162 0.360 1.298 0.370 0.271 0.559 1.067 0.376 0.270 0.563 1.040\nText+Video w/o caption/summary\nS-T LJST [2] 0.115 0.144 0.335 1.148 0.382 0.254 0.537 1.005 - - - -\nMulti Attn[3] 0.078 0.113 0.277 0.727 - - - - - - - -\nLSTM Attn[4] 0.096 0.128 0.311 0.941 - - - - - - - -\nQG-BLSTM [5] 0.109 0.138 0.366 1.132 - - - - - - - -\nMSTN 0.118 0.148 0.348 1.211 0.379 0.261 0.548 1.028 0.375 0.251 0.544 0.975\nMSTN w/o\nWEAN 0.115 0.145 0.34 1.159 0.362 0.254 0.532 0.982 0.367 0.252 0.541 0.967\nTable 1: Model performance on the DSTC dataset (top scores marked in bold). Models [1-5] are from (Le et al. 2019; Hori et\nal. 2019b; 2019a; Schwartz, Schwing, and Hazan 2019; Chao et al. 2019), respectively. Note that ours* is the system that we\nsubmitted to the challenge and ours** is the system that performed additional hyperparameter tuning after the challenge. The\nB-4, M, R-L, C stands for BLEU4, METEOR, ROUGE-L, and CIDEr, respectively.\nAs the DSTC7 dataset is organized as two versions (single\nreference and six references), we report the model perfor-\nmance on both sets. Also, note that we measure the model\nperformance with/without caption data and report the result\nin a separate section. To compare the model performance,\nwe chose recently published models such as MTN (Le\net al. 2019), Student-Teacher LJST (Hori et al. 2019b),\nMultimodal Attention (Hori et al. 2019a), LSTM atten-\ntion (Schwartz, Schwing, and Hazan 2019), and Question\nGuided BiLSTM attention (Chao et al. 2019) that are pro-\nposed to tackle the audiovisual scene-aware task. As shown\nin the Table 1, our proposed model ( MSTN) with retrieval\nstyle word generator outperforms the baseline model w/o it.\nThis means that adopting a retrieval style word generator is\nhelpful for A VSD. Also, our model (MSTN) surpasses most\nof the other methods.\nConclusion\nIn this paper, we propose MSTN(Multimodal Semantic\nTransformer Networks) for audio visual scene-aware dialog.\nThe proposed transformer-based encoder-decoder model\nemploys an attention-based word embedding layer by sub-\nstituting the linear projection layer in the decoder part. With\nthis design, the model allows considering semantic informa-\ntion more and more when generating the answer. Experi-\nmental results show that our model with thhis design sur-\npasses baseline models.\nAcknowledgments\nK. Jung is with ASRI, Seoul National University, Korea.\nThis work was supported by the Ministry of Trade, Indus-\ntry & Energy (MOTIE, Korea) under Industrial Technology\nInnovation Program (No.10073144).\nReferences\nAlamri, H.; Cartillier, V .; Lopes, R. G.; Das, A.; Wang, J.;\nEssa, I.; Batra, D.; Parikh, D.; Cherian, A.; Marks, T. K.;\net al. 2018. Audio visual scene-aware dialog (avsd) chal-\nlenge at dstc7. arXiv preprint arXiv:1806.00525.\nAlamri, H.; Cartillier, V .; Das, A.; Wang, J.; Cherian, A.;\nEssa, I.; Batra, D.; Marks, T. K.; Hori, C.; Anderson, P.; et al.\n2019. Audio visual scene-aware dialog. In Proceedings of\nthe IEEE CVPR, 7558–7567.\nChao, G.-L.; Rastogi, A.; Yavuz, S.; Hakkani-Tur, D.; Chen,\nJ.; and Lane, I. 2019. Learning question-guided video rep-\nresentation for multi-turn video question answering. In Pro-\nceedings of the 20th Annual SIGdial Meeting on Discourse\nand Dialogue, 215–225.\nDas, A.; Kottur, S.; Gupta, K.; Singh, A.; Yadav, D.; Moura,\nJ. M.; Parikh, D.; and Batra, D. 2017. Visual dialog. In\nProceedings of the IEEE CVPR, 326–335.\nGoyal, Y .; Khot, T.; Summers-Stay, D.; Batra, D.; and\nParikh, D. 2017. Making the v in vqa matter: Elevating the\nrole of image understanding in visual question answering.\nIn Proceedings of the IEEE CVPR, 6904–6913.\nHori, C.; Alamri, H.; Wang, J.; Wichern, G.; Hori, T.;\nCherian, A.; Marks, T. K.; Cartillier, V .; Lopes, R. G.; Das,\nA.; et al. 2019a. End-to-end audio visual scene-aware dia-\nlog using multimodal attention-based video features. InPro-\nceedings of the IEEE ICASSP, 2352–2356. IEEE.\nHori, C.; Cherian, A.; Marks, T. K.; and Hori, T. 2019b.\nJoint student-teacher learning for audio-visual scene-aware\ndialog. Proceedings of the Interspeech1886–1890.\nKim, S.; Galley, M.; Gunasekara, C.; Lee, S.; Atkinson,\nA.; Peng, B.; Schulz, H.; Gao, J.; Li, J.; Adada, M.; et al.\n2019. The eighth dialog system technology challenge.arXiv\npreprint arXiv:1911.06394.\nKingma, D. P., and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nLe, H.; Sahoo, D.; Chen, N.; and Hoi, S. 2019. Multi-\nmodal transformer networks for end-to-end video-grounded\ndialogue systems. In Proceedings of the ACL, 5612–5623.\nMa, S.; Sun, X.; Li, W.; Li, S.; Li, W.; and Ren, X. 2018.\nQuery and output: Generating words by querying distributed\nword representations for paraphrase generation. In Proceed-\nings of the NAACL-HLT, 196–206.\nPennington, J.; Socher, R.; and Manning, C. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nthe EMNLP, 1532–1543.\nSchwartz, I.; Schwing, A. G.; and Hazan, T. 2019. A simple\nbaseline for audio-visual scene-aware dialog. In Proceed-\nings of the IEEE CVPR, 12548–12558.\nSutskever, I.; Vinyals, O.; and Le, Q. 2014. Sequence to\nsequence learning with neural networks. Advances in NIPS.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008."
}