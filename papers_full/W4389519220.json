{
    "title": "Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model",
    "url": "https://openalex.org/W4389519220",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2121815342",
            "name": "Zeyu Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2732476013",
            "name": "Tim Dettmers",
            "affiliations": [
                "University of Washington",
                "Seattle University"
            ]
        },
        {
            "id": "https://openalex.org/A2106187023",
            "name": "Xi LIN",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2130709233",
            "name": "Veselin Stoyanov",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096862909",
            "name": "Xian Li",
            "affiliations": [
                "University of Washington",
                "Seattle University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4225591000",
        "https://openalex.org/W4385567093",
        "https://openalex.org/W4288284003",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4297631950",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W2150884987",
        "https://openalex.org/W1593114658",
        "https://openalex.org/W4287667694",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4285134706",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3118781290",
        "https://openalex.org/W3147874613",
        "https://openalex.org/W4286850694",
        "https://openalex.org/W4205460703",
        "https://openalex.org/W4200634402",
        "https://openalex.org/W3170796112",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W3040573126",
        "https://openalex.org/W3024786184",
        "https://openalex.org/W4293718192",
        "https://openalex.org/W3190540921",
        "https://openalex.org/W2805206884",
        "https://openalex.org/W2951008357",
        "https://openalex.org/W4288289156",
        "https://openalex.org/W3204998121",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W4295838474",
        "https://openalex.org/W2998702515",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2989539713"
    ],
    "abstract": "Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for pretraining large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method — Avg-K that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLayer (Roller et al., 2021).",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15038–15061\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nTowards A Unified View of Sparse Feed-Forward Network in Pretraining\nLarge Language Model\nLeo Z. Liu♠♡∗ Tim Dettmers♠ Xi Victoria Lin♡\n♠University of Washington, Seattle ♡Meta AI.\nVeselin Stoyanov♡ Xian Li♡\nAbstract\nLarge and sparse feed-forward layers (S-FFN)\nsuch as Mixture-of-Experts (MoE) have proven\neffective in scaling up Transformers model size\nfor pretraining large language models. By only\nactivating part of the FFN parameters condi-\ntioning on input, S-FFN improves generaliza-\ntion performance while keeping training and\ninference costs (in FLOPs) fixed. In this work,\nwe analyzed two major design choices of S-\nFFN: the memory block (a.k.a. expert) size and\nthe memory block selection method under a\ngeneral conceptual framework of sparse neu-\nral memory. Using this unified framework, we\ncompare several S-FFN architectures for lan-\nguage modeling and provide insights into their\nrelative efficacy and efficiency. We found a\nsimpler selection method — Avg-K that selects\nblocks through their mean aggregated hidden\nstates, achieving lower perplexity in language\nmodel pretraining compared to existing MoE ar-\nchitectures including Switch Transformer (Fe-\ndus et al., 2021) and HashLayer (Roller et al.,\n2021).\n1 Introduction\nLarge-scale pretrained language models (LLMs)\nachieve remarkable performance and general-\nization ability for NLP tasks (Radford and\nNarasimhan, 2018; Devlin et al., 2019; Liu et al.,\n2019; Radford et al., 2019; Brown et al., 2020; Raf-\nfel et al., 2020; Chowdhery et al., 2022). Scaling up\nthe model size (the number of parameters) has been\nshown as a reliable recipe for better generalization,\nunlocking new capabilities, while the performance\nhas not shown signs of plateauing (Kaplan et al.,\n2020; Zhang et al., 2022a; Chowdhery et al., 2022;\nHoffmann et al., 2022; Wei et al., 2022). However,\nthe computational resources required to train larger\nlanguage models are formidable, calling for more\nefficient training and inference solutions of LLMs\n∗ Work done as a Meta AI Resident. Correspondence to:\nLeo Zeyu Liu <zeyuliu2@cs.washington.edu>\n(Borgeaud et al., 2022; Schwartz et al., 2020; Tay\net al., 2020).\nOne promising direction is sparse scaling which\nincreases the number of parameters while keeping\nthe training and inference cost (in FLOPs) fixed.\nRecent work focuses on scaling up a transformer’s\nfeed-forward network (FFN) with sparsely acti-\nvated parameters, resulting in a scaled and sparse\nFFN ( S-FFN). There have been two major ap-\nproaches to achieve S-FFN. One treats S-FFN as\na neural memory (Sukhbaatar et al., 2015a) where\na sparse memory retrieves and activates only parts\nof the memory cells (Lample et al., 2019). The\nother adopts Mixture-of-Expert Network (MoE)\n(Lepikhin et al., 2021; Fedus et al., 2021; Du et al.,\n2021; Roller et al., 2021; Lewis et al., 2021; Chi\net al., 2022) that replaces a single FFN module\nwith multiple equal-sized ones (called “experts\")\nand only activates a few among many experts for a\nparticular input.\nWhile both memory and MoE models achieve\nS-FFN, they have been considered two distinct ap-\nproaches. We aim to draw the connections between\nthese two classes of S-FFN: What critical design\nchoices do they have in common? Which design\nchoices are essential for their modeling capabil-\nity and computation efficiency? Can the effective\ningredients of each method be transferred and com-\nbined to improve performance further?\nIn order to answer these questions, we start from\nthe neural memory view of FFN (Sukhbaatar et al.,\n2015a) (§2.1) and reduce all S-FFN’s to the same\nmathematical form (§3.1). Then, we characterize\nthese methods along two dimensions — memory\nblock size (e.g. expert size) and memory block\nselection method (e.g. gating) (§3.2). Using this\nframework, we made the following contributions:\n• We study a wider range of memory block sizes\nbesides the commonly used block size in MoE\narchitectures (Fedus et al., 2022) and show\nthat reducing the block size keeps improving\n15038\nSelf-Attention\n\"The\"\nAdd + Normalize\nAdd + Normalize\ny\nDown-projection\nKey Table\nValue Table\nTransformer LayerOption 1: Sparse Mixture-of-Expert Option 2: Sparse Neural Memory\n  ReplaceReplace  \nTop-1\nGShard, Switch Transformer, HashLayer, BASELayer... LowRank Key, Product Key\n+ Load Balancing Constraint GATE\nTop-6\nFFN 2FFN 1 FFN 3 FFN 4\nFFN\nFigure 1: Sparse Mixture-of-Expert and Sparse Neural Memory as two different methods.\nthe perplexity with little incurred extra FLOPs\n(§5.1 §5.2), leading to better perplexity/com-\nputation trade-offs.\n• We conduct a systematic exploration of block\nselection methods to quantify their relative ef-\nficacy and efficiency (§5.2). Specifically, we\nfind that the selection method through a gat-\ning function, in general, improves the FLOPs-\nPerplexity trade-off. However, the parame-\nterization of the current MoE’s (Fedus et al.,\n2021; Lewis et al., 2021) gating function —\nmultiplying token representation with a sepa-\nrately learned matrix — has worse perplexity\nthan using the FFN hidden states — multiply-\ning token representation with FFN’s matrix\n(§2.1).\n• Drawing on the insights above, we propose\na simple gate for S-FFN— Avg-K (§3.3) as\na hybrid design choice between sparse neu-\nral memory and a mixture of experts. It effi-\nciently selects memory blocks based on the\nmean aggregated hidden states of each block.\nWith 1% additional FLOPs, Avg-K achieves\nlower perplexity (14.80) on the validation set\nof The Pile (Gao et al., 2020) than both Switch\nTransformer (16.45) (Fedus et al., 2021) and\nHashLayer (15.75) (Roller et al., 2021). More-\nover, Avg-K is the first MoE model that per-\nforms well without load balancing constraint,\nwhile conventional MoE transformers like\nSwitch Transformer will degenerate (Fedus\net al., 2021; Shazeer et al., 2017; Eigen et al.,\n2014).\n2 Background\n2.1 Feed-Forward Network\nA transformer layer (Vaswani et al., 2017) consists\nof a self-attention block and a Feed-Forward Net-\nwork block (FFN). FFN receives an input vector\nx ∈Rd from the self-attention block, multiplies\nit with parameter matrix K ∈Rdm×d, applies a\nnon-linear function f to obtain the hidden states\nm ∈Rdm and applies another affine transforma-\ntion V ∈Rdm×d to produce a d-dimensional out-\nput y. This multi-layer perceptron architecture can\nbe expressed as:\ny = FFN(x) =f(x ·K⊤) ·V = m ·V. (1)\nAdditionally, we could also view it as a neural mem-\nory (Sukhbaatar et al., 2015b, 2019; Geva et al.,\n2021) (Eq. 2).\ny =\ndm−1∑\ni=0\nf(x ·ki) ·vi =\ndm−1∑\ni=0\nmi ·vi. (2)\nIn this view, FFN consists of dm key-value pairs,\nknown as memory cells. Each key is represented by\na d-dimensional ki ∈Rd, and together form thekey\ntable; likewise, value vectors vi ∈Rd constitutes a\nvalue table V ∈Rdm×d. The memory multiplies\nthe query input x ∈Rd with every ki; followed\nby the non-linear function, it produces memory\ncoefficient mi = f(x ·ki) for the i-th memory\ncell. Finally, the output of FFN is the sum of its\nvalues vi weighted by their corresponding memory\ncoefficient mi. Conventionally, the size of FFN —\ndm — is set to be 4 ·d.\n2.2 Scaling up FFN\nAs discussed in §1, scaling up the number of pa-\nrameters in FFN serves as a lever to improve trans-\n15039\nformer performance. Since a standard FFN ac-\ncounts for about two-thirds of a transformer layer’s\nparameters (Geva et al., 2021), scaling up FFN\nwill greatly affect the parameter size of a trans-\nformer model. However, one could sparsely acti-\nvate the parameter to control the required compute.\nIn this section, we and discuss two approaches to\nachieve a scaled and sparse FFN (S-FFN). One has\na mixture-of-expert model activate a few experts\n(§2.2.1), and the other specifies a memory model\nto sparsify (§2.2.2).\n2.2.1 Mixture of Experts (MoE)\nMixture of experts (MoE; Jacobs et al. (1991)) con-\nsists of a set of expert models {fi(x)}B−1\ni=0 and a\ngating function g : Rd →RB to estimates the rele-\nvance of each expert. Finally, the output is the sum\nof experts’ output weighted by the gate’s weight\nestimation for that particular expert.\nMoE(x) =\n∑\ni∈E={0,1,···,B−1}\ngi(x) ·fi(x) (3)\nRecent work (Du et al., 2021; Lepikhin et al., 2021;\nRoller et al., 2021; Lewis et al., 2021; Zhou et al.,\n2022) have applied this approach to transformer by\ndissecting the FFN into multiple expert blocks and\nsparse activation of the MoE (SMoE). In SMoE,\nthe gating function (or “router”) routes an input\ntoken x to a subset1 (e.g. 1 or 2) of experts, E=\nsubset(g(x)). Previous work mainly adopts two\ntypes of gates.\nLearned gate is parameterized by a set of learnable\nexpert embeddings θ = [e0; ··· ; eB−1] ∈RB×d,\nwhere each embedding corresponds to one expert.\nThe relevance of the i-th expert is obtained by\ngi(x) = exp (ei ·x)∑\nj exp (ej ·x)\nTo enforce load balancing when routing, previ-\nous work have employed an additional auxiliary\nloss (Lepikhin et al., 2021; Fedus et al., 2021;\nArtetxe et al., 2021; Du et al., 2021; Chi et al.,\n2022) or framed expert utilization as a constrained\noptimization problem (Lewis et al., 2021; Zhou\net al., 2022).\n1Conventionally, SMoE implements load balancing con-\nstraints to prevent the overuse of certain experts and the under-\nutilization of others, and avoid convergence to local optima\n(Shazeer et al., 2017; Eigen et al., 2014).\nStatic gate, in contrast to a learnable gate, does\nnot have any differentiable parameters. Instead, it\nuses a static mapping that encodes load-balancing\nconstraints to route input (Roller et al., 2021; Guru-\nrangan et al., 2021). For example, RandHash from\nHashLayer (Roller et al., 2021) uses a hash table\nthat maps from token type to randomly selected\nexpert(s). DEMix (Gururangan et al., 2021) en-\nsures each expert only sees data from a pre-defined\ndomain.\n2.2.2 Sparse Neural Memory\nThe other line of work follows the memory view of\nFFN (Eq. 2). It is straightforward to increase the\nmemory size dm to a much larger valuedm ≫4·d.\nBy only using the top-kentries in the memory co-\nefficient m = x ·K⊤(Eq. 2), one could sparsely\nactivate the value table, resulting in a vanilla sparse\nmemory (VanillaM). However, the naive imple-\nmentation of this approach results in computation\ncost proportional linearly to the memory size. Lam-\nple et al. (2019) explored the following two tech-\nniques in this direction to scale computation sub-\nlinearly.\nLow-Rank Key Memory (LoRKM) A straight-\nforward technique is to assume that the full key\ntable K⊤ ∈Rd×dm is composed of and approxi-\nmated by a downward projection D ∈Rd×dℓ and\na low-rank key table ˜K ∈Rdm×dℓ,\nK⊤= D ·˜K⊤\nwhere dℓ ≪d. LoRKM produces memory coeffi-\ncients by mi = f((x ·D) ·˜ki) =f(t ·˜ki).\nProduct Key Memory ( PKM) Building upon\nLoRKM, PKM further decomposes the low-rank\nkey table by assuming different low-rank keys have\nstructured sharing with each other. See Appendix\nB for more technical details. Due to such factoriza-\ntion, PKM has a negligible key tableK⊤= D·˜K⊤\n(e.g., < 0.3%) relative to the parameters in the\nvalue table.\n3 A Unified View of Sparse FFNs\nWe show the connections between MoE and neural\nmemory despite their different formulations on the\nsurface. We first derive a variant form of MoE to\nestablish its connection with sparse memory (§3.1).\nThen, we propose a unified framework for S-FFN\n(§3.2).\n15040\n3.1 A Neural Memory View of MoE\nMoEs use a gating function to estimate the im-\nportance of all experts and combine each expert’s\noutput through linear combination. Here, inspired\nby the memory view on FFNs (§2), we could view\nMoE as a wide neural memory chunked into B\nFFNs:\nMoE(x) =\nB−1∑\ni=0\ngi(x) ·FFN(i)(x)\n=\nB−1∑\ni=0\ngi(x) ·\n\n\ndm−1∑\nj=0\nm(i)\nj ·v(i)\nj\n\n\n=\nB−1∑\ni=0\ndm−1∑\nj=0\n(\ngi(x) ·m(i)\nj\n)\n·v(i)\nj\n=\nB·dm−1∑\nl=0 s.t. l=i·dm+j\n(\ngi(x) ·m(i)\nj\n)\n·v(i)\nj\n=\nB·dm−1∑\nl=0\nml ·vl, (4)\nwhere FFN(i) denotes the i-th FFN.\nBased on linear algebra, we are able to write\nthe standard MoE formulation (§2.2.1) in a similar\nsummation form as that of neural memory (Eq 4;\n2.1). MoE in its neural memory form has a value ta-\nble V =\n[\nV(0); ··· ; V(B−1)]\n∈RB·dm×d — the\nconcatenation of B value tables V(i) ∈Rdm×d\nfrom all FFN(i). In this view, vl with l= i·dm +j\ncorresponds to the j-th value vector in the i-th\nchunk of value table V(i). Thus, its correspond-\ning memory coefficient ml = gi(x) ·m(i)\nj is pro-\nduced by weighting the j-th memory coefficient\nof FFN(i), m(i)\nj = x ·k(i)\nj , by the relevance score\nof FFN(i), gi(x). Through this memory view, one\ncould see that a sparse MoE(x) is a sparse mem-\nory operating in terms of Bmemory blocks; it uses\nthe gate g(x) to narrow down the calculation over\nthe stacked value tables V to the value tables from\nFFN(i),for i∈subset(g(x)) (i.e. sparsify).\nComparison with Sparse Memory Both SMoE\nand sparse neural memory are neural memory, but\nthere are several differences: 1) whether memory\ncells share the same relevance weight: in sparse\nneural memory, each memory cell receives an in-\ndividual weight mi. In contrast, in SMoE, each\ngroup of 4 ·dmemory cells shares the same rele-\nvance weight gi(x). 2) memory selection criterion:\nif we center the key-value computation (Eq. 2) at\nthe core of S-FFN computation, the sparse memory\ndirectly uses the memory parameters for selection\n— the dot product between input token x and key\nvectors ki, whereas SMoE depends on a separately\nparameterized gate g.\n3.2 The Unified Framework\nWe propose a general framework that unifies the\ntwo different approaches to achieve S-FFN. We\nview both as instances of a memory with large key\nand value table — K ∈Rdm×dk ,V ∈Rdm×dv ,\nwhere dm ≫4 ·d. We distinguish the different\nmethods along two dimensions illustrated below\nand summarized in Table 1:\nMemory block size specifies how many memory\ncells share the same relevance weight at selection\ntime, and thus together treated as a memory block.\nWe use gto denote the size of one block. In other\nwords, we split the K,V along the dm-dimension\ninto g-size blocks. Therefore, a memory consists\nof B = dm/gblocks in total. Formally, we write\nKg =\n[\nK(0); K(1); ··· ; K(B−1)\n]\n∈Rdm×dk\nVg =\n[\nV(0); V(1); ··· ; V(B−1)\n]\n∈Rdm×dv\nFor example, sparse memory has block size g=\n1 — trivially treating 1 memory cell as a “block”;\nand SMoE has the block size g = 4 ·d (§3.1).\nCurrent approaches generally use fixed block sizes,\nbut this is mostly an artifact of how the methods\nwere derived rather than a mathematical constraint.\nFor example, we can designSMoE versions instead\nof 1 expert of size 4 ·d, or uses 2 experts of size\n2 ·d. We can similarly chunk memory coefficients\nm into blocks of size gin sparse memories.\nMemory block selection method is the specific\nfunction that compute the relevance of each mem-\nory blocks for selection. Since SMoE is also a\ntype of sparse memory, we distinguish the selec-\ntion method by a new criterion — whether one\nallows input x to directly interact with the key ta-\nble Kg. As discussed in §3.1, SMoE uses the es-\ntimation from an individually parameterized gate\nto select, while sparse memory solely and directly\nuses a key table. Thus, current SMoE is a type of\nindirect selection method, and sparse memory a\ndirect one. Various SMoEs are further character-\nized by whether their gating function has learned\nparameters or consists of a static mapping (§2.2.1).\n15041\nMemory block\nsize (g) Memory block selection method Model Name\n1 Direct Full-parameter Key VanillaM\nLow-rank Key LoRKM, PKM (Lample et al., 2019)\n4 ·d Indirect\nLearned gate\nSwitch Transformer(Fedus et al., 2021),\nGShard (Lepikhin et al., 2021),\nGLaM (Du et al., 2021),\nBASELayer (Lewis et al., 2021),\nX-MoE (Chi et al., 2022)\nStatic gate HashLayer(Roller et al., 2021),\nDEMix (Gururangan et al., 2021)\nTable 1: S-FFN methods decomposed along the defined design dimensions.\nMeanwhile, sparse memory is characterized by how\nmuch factorization the key table uses (§2.2.2).\n3.3 A New Selection Method — Avg-K\nThe gate design in the Mixture-of-Expert methods\nensures that not all experts are activated for rout-\ning tokens. Without the gate design, conventional\nsparse neural memory methods (§2.2.2) uses the\nfull key table before sparsifying the computation in\nthe value tables, which explains the increasing com-\nputation cost for scaling up sparse neural memory.\nAs later shown in Fig. 3 (§5.1), this additional com-\nputation doesn’t bring proportional improvement\nto its performance, compared with MoE.\nHowever, there is still merit in sparse neural\nmemory that MoE could acquire. Based on the\ncontrastive analysis from §5.2, we found that when\nthe model makes more use of each expert’s key\ntable for routing tokens, the more its performance\nwill increase. This is precisely the deficiency in\nthe conventional Mixture of Experts (MoE) model,\nas it relies on a separately learned parameter or a\nstatic gating mechanism (§2.2.1).\nTo this end, we propose a new routing method —\nAvg-K— as a hybrid design choice between sparse\nneural memory and MoE methods. Avg-K repre-\nsents each block with the average of its key table\nK(i) ∈Rg×d along g-dimension:\nei = 1\ng ·\ng−1∑\nj=0\nk(i)\nj = Avg(K(i), dim=0)\nThen, we use the dot product between x and the\naverages to select the top-bselected block and route\nthe token there for memory calculation (Eq. 2):\ngi(x) =\n{\n1 i∈{top-bof [e0,··· ,eB−1] ·x}\n0 otherwise\nDue to the linearity of averaging, the operation\nei ·x is equivalent to calculating the average of\ndot products within a block without GeLU. Since\nall tokens share the set of averaged key vectors,\nour method is efficient. In summary, Avg-K mi-\ngrates MoE’s advantages: a gate design, and a\nfull-parameterized key table in each expert. As\nan advantage of sparse memory, it uses the average\nkey vectors of each expert to route tokens and thus\nincreases the memory selection method’s depen-\ndency on the key table. We provide more rationale\nfor our choice of average function in Appendix\nD.1.\n4 Experiment Setup\n4.1 Models\nWe choose Dense Baseline using transformer ar-\nchitectures used in GPT-3 models (Brown et al.,\n2020), which has 24 transformer layers, with d=\n1024, f = GeLU as activation function, and with a\nmemory size (or FFN hidden size) to be 4 ·d. This\nmodel is also referred to as the \"base model\" be-\ncause S-FFN’s size is chosen based on the configu-\nration of this model. Our choice of architecture size\nleads to a base model with 355M parameters (See\nAppendix A). Our reason for the chosen model size\nis two-fold: 1) this size is similar to the community-\nestimated size of OpenAI text-ada-001; 2) as in-\ndicated in Clark et al. (2022), 355M is the smallest\nsize that separates the performances of different\narchitecture designs.\nS-FFN Given a model above, we replace some of\nits FFNs with anS-FFN. Similar to (Lepikhin et al.,\n2021), we replace the FFN at every 6 layers (layer\n5,11,17,23, indexed from 0), leading to 4 S-FFNs\nin total across 24 layers. We use k to denote the\nnumber of memory blocks used and control how\n15042\nactivated the S-FFN is. We use the formulation\nof dm = E·(4 ·d) to control the size of S-FFN,\nso the S-FFN will activate b = k\ng out of B =\ndm\ng memory blocks. In Table 2, we list all S-FFN\nmodels used for analysis in §5. We count FLOPs\nanalytically following Narayanan et al. (2021) and\ndo not account if a worker finishes computation\nbefore another (when using model parallelism). We\nuse the number of learnable parametersto consider\nwhether two models are equally expressive. In\nTable 2, we list all S-FFN models used for analysis\nin §5. For experimentation of our Avg-K, we don’t\nenforce load balancing for simplicity.\nPKM-FFN Since the factorized key table in\nPKM has little (<0.3%) learnable parameter rel-\native to the value table, we propose an indirect\nvariant called PKM-FFN to match the number of\nparameters of other models like RandHash. This\nvariant has memory block size g= 1and the same\nkey-value table as RandHash. PKM-FFN has a\ngate whose g(x) is the same as the m from a PKM\nand gi = mi; and no load-balancing is enforced.\n4.2 Language Modeling\nPretraining Data We pretrain all S-FFN models\non a total of 453GB text with 112B tokens from a\nunion of six English-only datasets, including En-\nglish subset of CC100 and the five datasets used\nto pretrain RoBERTa (Liu et al., 2019) — specif-\nically BookCorpus, English Wikipedia, CC-News,\nOpenWebText, CC-Stories (details in Appendix\nA.3). We adopt the same Byte-Pair Encoding as\nGPT-2 (Radford et al., 2019) and RoBERTa (Liu\net al., 2019) with a vocabulary of 50K subword\nunits. All models are trained for 60B tokens.\nEvaluation settings We evaluate our models’\nability to predict the next token in a sequence as\nmeasured by perplexity. We report bothin-domain\nand out-of-domain perplexity to indicate general-\nization ability. For out-of-domain, we use valida-\ntion data from The Pile (Gao et al., 2020), a public\ndataset that combines data from 22 diverse sources.\n5 Analysis Results\nIn this section, we use the proposed unified view to\nsystematically study the design choice of S-FFN.\nSpecifically, (1). we study a wide range of block\nsizes other than the incidental choice used in ex-\nisting work and investigate its impact on language\nmodeling perplexity (§5.1). (2). Both direct and\nindirect block selection methods lead to lower per-\nplexity than a standard FFN, but which type of\nmethod has better FLOPs-Perplexity trade-off and\nwhat are the relative efficacy and efficiency of dif-\nferent methods require further study (§5.2).\n5.1 Memory block size\nSince block size is a natural number, we aim\nto answer a straightforward question — given\na fixed number of active memory cells k, does\nsmaller memory block size lead to lower per-\nplexity? We use simple and robust selection meth-\nods to disentangle the impact of hyperparameter\nchoices. Specifically, we use random hash as rec-\nommended in HashLayer (Roller et al., 2021) (de-\nnoted RandHash) for indirect block selection and\nexact top-kmemory block (denoted VanillaM) for\ndirect block selection. For all experiments, we use\nE = 16.\nRandHash randomly selects b = k/g unique\nmemory blocks among all B = dm/g blocks\n— essentially sampling b unique values from\nUniform([0,··· ,B −1]). Originally, with block\nsize g = 4096, a RandHash assigns a token to\n4096/4096 = 1block; with block size g = 2048,\n4096/2048 = 2blocks.\nVanillaM originally has a block size g = 1and\nselects top-kscalars in memory coefficients m =\nGeLU(x ·K⊤). We made a minimal change to\nextend it to larger block size g: given m, we chunk\nit into B blocks — mg = [m(0); ··· ; m(B−1)];\nthen, we select the top-bblocks using the average\nof each block:\nAvg(GeLU(x ·(K(i))⊤), dim=0)2\nIn Fig. 2, we observe that smaller block size\nleads to an improvement of 0.4(15.75 →15.35)\nperplexity for RandHash and an improvement of\n0.87(15.56 →14.69) for VanillaM.\nIn Appendix C.2, we provide theoretical justi-\nfications for this observation which shows that a\nsmaller block size improves model capacity by in-\ncluding more combinations of memory cells. For\nexample, with g/2, half memory cells of expert-1\ncould be activated together with half of the expert-\n2; however, this combination is impossible with a\nlarger block size.\n15043\nSelection Method name g E kmethod type\nDirect\nVanillaM {1, 64, 256, 1024, 2048, 4096}(§5.1)\n{4, 16,\n(32)∗}\n{4096,\n(8192)∗}\nLoRKM {1}\nPKM (Lample et al., 2019) {1}\nIndirect\nRandHash (Roller et al., 2021) {1, 64, 256, 1024, 2048, 4096}(§5.1)\nSwitch (Fedus et al., 2021) {4096}\nPKM-FFN (§4.1) {1}\nTable 2: All the S-FFN models used in experiments and analysis in §5 — gis the number of memory cells grouped\nin a memory block, kis the active memory cells, and E control the sizes of a memory dm = E·(4 ·d). Some\nsettings(*) are only used for PKM.\n1\n64\n256\n1024\n2048\n4096\ng\n15\n16\n17Perplexity( )\n Dense Baseline\nDomain = Out-of-Domain\n1\n64\n256\n1024\n2048\n4096\ng\n15\n16\n17\n Dense Baseline\nDomain = In-Domain Val.\n1\n64\n256\n1024\n2048\n4096\ng\n18\n19\nDense Baseline\nDomain = In-Domain Train\nVanillaM RandomHash\nFigure 2: Perplexity (lower the better) consistently improves as memory block size g decreases for both direct\n(VanillaM) and indirect (RandHash) selection method in S-FFN models. Ranking on individual out-of-domain test\nsets generally follows the ranking by average perplexity (e.g. 20 out of 22).\n5.2 Memory block selection method\nNext, we investigate the impact of the selection\nmethod, specifically, the FLOPs-perplexity trade-\noff for direct and indirect methods to determine the\noverall usefulness of each S-FFN method.\nFLOPs-perplexity trade-off We study the effi-\nciency of direct and indirect selection methods in\nS-FFN models characterized by FLOPS-perplexity\ntrade-off. We conduct experiments across differ-\nent scales of the memory by varying E ∈{4,16};\nadditionally, we run E = 32for PKM.\nIn Fig. 3, we marginalize different factors used\nin the two selection methods — i.e. types of gates,\nfactorization techniques on key table, etc. — and\nconsider each type of selection method as a whole.\nWhen we change different marginalized factors, we\nobserve that indirect methods tend to improve more\nas we use more FLOPs (with larger memory sizes\ncontrolled by E). Thus, the indirect method has a\nbetter FLOPs-perplexity trade-off.\n2Avg(·) performs better than other simple aggregators —\nMin(·), Max(·), and Avg(|·| ); see ablations in Table 7.\nEffect of gating function We start with con-\ntrastive comparisons among PKM-FFN E=16,\nPKME=32, RandHashE=16 with memory block\nsize g= 1and 4096 active memory blocks. From\nthe three parameter-matched models, we can learn\nimportant lessons to improve the design of the gate:\n1. Comparing with PKM-FFNE=16, PKME=32\nessentially moves the parameters from a full-\nparameter key table to double the size of the\nvalue table.\n2. PKM-FFNE=16 and RandHashE=16 have the\nsame (in size) key and value tables. But the\nformer uses a gate jointly learned with a key\ntable, while the latter uses a learning-free gate.\nAs shown in Table 3, on out-of-domain, PKM-\nFFNE=16 outperforms PKME=32(16.06) by 0.87\nperplexity and slightly outperform RandHashE=16\nby 0.16. Therefore, it is essential to have a full-\nparameter, and thus expressive enough, key ta-\nble to produce memory coefficients.\nTable 3 shows the improvement of\nVanillaME=16, PKM-FFN E=16, RandHashE=16\n15044\n0.20 0.25 0.30\nTotal Train FLOPs (ZFLOPs)\n15.0\n15.5\n16.0\n16.5\n17.0Perplexity( )\nDomain = Out-of-Domain\n0.20 0.25 0.30\nTotal Train FLOPs (ZFLOPs)\n15.0\n15.5\n16.0\n16.5\n17.0\n17.5\nDomain = In-Domain Val. Selection method type\nDirect\nIndirect\nMemory size controlled by E\n4\n16\n32\nFigure 3: FLOPs-perplexity trade-off of indirect block selection is better than that of direct block selection. Indirect\nmethods (orange cross) have more perplexity improvement relative to increases in FLOPs than direct methods (blue\ndots). See a more detailed legend (e.g. include methods like LoRKM) in Fig. 5.\nSelection\nmethod type\nName\n# Active\nmemory\ncells (k)\n#Parameters\n(Entire Model)\nTrain\nZFLOPs\nOut-of-Domain Avg. (↓)\n(See Table 9 for each domain)\nIn-Domain (↓)\nTrain Val.\nDense Baseline 4096 354.7M 0.212 16.96 19.60 17.16\nDirect\nPKME=16 4096 590.2M 0.205 16.66 19.45 16.87\nPKME=32 4096 858.7M 0.205 16.06 18.93 16.36\nPKME=32 8192 858.7M 0.213 16.16 19.05 16.45\nVanillaME=16 4096 858.3M 0.333 14.69 17.48 14.90\nIndirect\nPKM-FFNE=16 4096 858.9M 0.213 15.19 17.82 15.48\nRandHashE=16 4096 858.3M 0.212 15.35 17.88 15.45\nTable 3: List of experiments for contrastively comparing designs. This table assume each memory cell is a memory\nblock, i.e. g= 1. The top two best performing models (bolded) have full-parameter key table and depend more on\ndot product to activate parameters. Ranking on individual out-of-domain test set generally follows the ranking by\naverage perplexity (e.g. 21 out of 22).\nover Dense Baseline (16.96) are 2.27, 1.77, and\n1.61 respectively on out-of-domain. They only\ndiffer by how much they depend on the key\ntable for selection — VanillaM directly uses it,\nPKM-FFN indirectly gains information from it,\nand RandHash completely ignores it. Due to the\nconsistent observation across out-of-domain test\nsets, we conclude that the more dependent on\nthe key table the selection method is, the better\nlanguage model it will lead to; and indirect usage\n(PKM-FFN) is not enough.\n5.3 Performance of Avg-K\nWe benchmark our proposed Avg-K approach\n(§3.3) in this section.\nLanguage Modeling Pretraining. Table 4 shows\nthat the proposed S-FFN design outperforms all\nother indirect methods. With < 1% additional\nFLOPs, Avg-K achieves 2.16 lower perplexity than\nDense Baseline (16.96), outperform Fedus et al.\n(2021) by 1.65 and Roller et al. (2021) by 0.5.\nAnalysis In Fig 6, although VanillaM increases\nits performance from block size g = 4096 to\n256, the improvement is less significant than that\nof Avg-K. The comparison suggests that, with a\nlarger block size, GeLU activation protects the\naverage operation in VanillaM (after GeLU) af-\nfected by (potentially many) large negatives be-\ncause limx→−∞GeLU(x) = 0. In contrast, this\n“negative value” problem is mitigated by using a\nsmaller block size, due to more blocks available for\n15045\nE #Parameters Selection method g Train Out-of-Domain Avg. (↓) In-Domain ( ↓)\n(Entire Model) ZFLOPs (See Table 10 for details) Train Val.\n1 354.7M Dense Baseline 1 0.212 16.96 19.60 17.16\n16 ≈858.3M\nRandHash\n(Roller et al., 2021)\n4096 0.212 15.75 18.78 16.26\n1 0.212 15.35 17.88 15.45\nSwitch\n(Fedus et al., 2021) 4096 0.212 16.45 18.20 16.00\nPKM-FFN 1 0.213 15.19 17.82 15.48\nAvg-K\n4096 0.212 16.44 19.04 16.59\n256 0.213 14.91 17.57 15.19\n64 0.214 14.80 17.51 15.11\nTable 4: Avg-K out-performs other indirect block selection methods. Switch transformer is trained with the load\nbalancing loss to prevent model degradation (Shazeer et al., 2017; Eigen et al., 2014). Ranking on individual\nout-of-domain test set mostly follows the ranking by average perplexity (e.g. 21 out of 22).\nselection. Since negative dot products affect Avg-K\nmore, it prefers blocks with more or very posi-\ntive dot products, whereas VanillaMis not shielded\nfrom extreme negatives, so it might fail to detect\nthose blocks. Therefore, Avg-K could achieve an\neven slightly better perplexity than VanillaM for\nblock size g ≤256. See full discussions in Ap-\npendix D.2.\nIn Figure 7, we also include a load balancing\nanalysis of Avg-K. To our surprise, the mode col-\nlapse (i.e., imbalanced usage of memory block)\nissue in Avg-K still exists as load balancing loss is\nnot enforced. Given its superior performance, this\nsuggests that Avg-K learned a good representation\nfor each memory block despite the disadvantage of\nload imbalance.\n6 Related Work\nExcluded S-FFN Terraformer’s Jaszczur et al.\n(2021) technique on FFN is closest to our\nPKM-FFN because there is a low-rank learned\ngate to operate on each memory cells for selec-\ntion. However, we exclude this method because\nour framework uses all memory cells in each block,\nbut Terraformer selects1 cell in each memory block\n(see our study in Appendix E.1). In finetuning sce-\nnarios, Zhang et al. (2022b) studies the connection\nbetween FFN and SMoE by turning trained FFN\ninto experts and separately learning a gate. In con-\ntrast, we focus on pretraining from scratch.\nApproximate Nearest Neighbour (ANN) search\nOne might wonder whether ANN techniques could\nhelp to search for the best key in VanillaM rather\nthan trade the expressiveness of the key table for\nefficiency. For example, one could process the un-\nfactorized key table by ANN methods like FAISS\n(Johnson et al., 2021) and ScaNN (Guo et al.,\n2020). One successful example is applying vanilla\nLocality-Sensitive Hashing to Reformer (Kitaev\net al., 2020). However, in our preliminary study,\nwe found that perplexity is greatly affected by the\nsearch quality, and building a data structure after\nevery update is expensive and hard to avoid. We\nleave detailed discussion to Appendix E.2.\n7 Conclusion\nWe provide a unified framework for designing\nsparse FFN in transformers and analyze existing\nS-FFN methods such as MoEs in the language mod-\neling task. Using this framework based on sparse\nneural memory, we found that smaller memory\nblock (e.g. expert) size improves perplexity at the\ncost of slightly higher computation cost. Selection\nmethods with gates have better FLOPs-Perplexity\ntrade-offs than without, while the gating function in\ncurrent MoEs is sub-optimal. This framework en-\nables us to instantiate a simpler S-FFN architecture\nthat outperforms MoEs while still being efficient\nin training and inference.\nLimitations\nLimitations of a smaller block size g With\nmodel parallelism (Lepikhin et al., 2021), multiple\nGPUs contains different memory block and paral-\nlelize the calculations. If with block size g= 4·d,\na token is only routed to 1 memory block on one\ndevice, each device doubles its chance to receive\nmore tokens with block size g= 2·d. Therefore,\neach GPU processes more tokens and requires more\n15046\ncomputation time, but we didn’t measure the wall\ntime difference in our work. Better implementa-\ntions could be developed to make a smaller block\nsize for practical usage. In addition, since each\nmemory block has its representation stored in the\ngating function, the smaller block will lead to more\nblock representation stored in the gate, e.g., more\nlearned parameters in the learned gate and a larger\ntable for the static gate. Although RandHash with\nmemory block size g = 4·dcost essentially the\nsame with memory block size g = 1, computing\ng(x) for learned gates requires more cost (details\nin Appendix C.3.1).\nAs discussed in Appendix C.3.2, smaller mem-\nory block size will induce higher communication\ncost given the current all_to_all-based imple-\nmentation framework (e.g. Switch Transformer).\nWe think reducing memory block size to 1 is too\nextreme to be practical; and there should be a sweet\nspot between 1 and 4096 (or the chosen expert size)\nallowed by the implementation and hardware sta-\ntus.\nLimitations of the unified framework Since our\nmethod Avg-K essentially applies an average pool-\ning to the key table Kg, a better alternative may\nexist. Our method also heavily depends on dot\nproduct information, but this might not be the best\ninformation to be used. Due to the curse of di-\nmensionality, future work might want to focus on\nfinding a better metric than dot product and other\naggregation methods than average to measure dis-\ntance between high-dimensional vectors.\nAlso, we didn’t train Avg-K with load-balancing\ndue to our current limit in budget, but we in-\nclude our rationale in Appendix D.3 for whyAvg-K\nshould work with load-balancing.\nAdditionally, in large-scale SMoE training, the\nspeed is limited by the most heavy-loaded GPU\nwhen model parallelism is used. Therefore, load\nbalancing is essential. We also note that our scale\nis relatively small and does not use model paral-\nlelism, so the problem is not pronounced for us.\nFuture follow-up should look at how to incorpo-\nrate load balancing into the unified framework and\ninspire better actionable design choice. We think\nsuch unification requires more advanced theoreti-\ncal connection with memory block and block selec-\ntion method, which likely involves consideration\nof training procedure.\nEthics Statements\nDue to the nature of pretraining, the carbon foot-\nprint of our work is large estimated by the amount\nof FLOPs and GPUs reported in the paper. We\ndid make the effort to minimize the cost at design\nstage of the project. In our preliminary study, we\nask for recommendation from one of the authors\nof Artetxe et al. (2021) to choose and verify the\nminimal model size and amount of tokens that suf-\nficiently differentiate different design choices.\nAnother ethical concern of the paper is from the\npretraining data we use. As we used the same data\nsource as Artetxe et al. (2021), we refer the reader\nto the ethics statements in Artetxe et al. (2021) for\nhow much trained model absorbs bias and toxicity\nfrom training data.\nAcknowledgement\nWe would like to thank (in random order) helpful\nfeedbacks from Suchin Gururangan, Xiaochuang\nHan, Luke Zettlemoyer, Noah Smith, pre-doctoral\nmembers of Noah’s Ark, and anonymous reviewers.\nIn developing the experimentation of this work,\ngreat help has been received from Jingfei Du, Susan\nZhang, and researchers at Meta AI. We would also\nwant to thank Zhaoheng Billy Li for his help in\ndeveloping analytical formula in Appendix C.2.\nReferences\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor\nMihaylov, Myle Ott, Sam Shleifer, Xi Victoria\nLin, Jingfei Du, Srinivasan Iyer, Ramakanth Pa-\nsunuru, Giri Anantharaman, Xian Li, Shuohui Chen,\nHalil Akin, Mandeep Baines, Louis Martin, Xing\nZhou, Punit Singh Koura, Brian O’Horo, Jeff Wang,\nLuke Zettlemoyer, Mona T. Diab, Zornitsa Kozareva,\nand Ves Stoyanov. 2021. Efficient large scale lan-\nguage modeling with mixtures of experts. CoRR,\nabs/2112.10684.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206–2240. PMLR.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\n15047\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nZewen Chi, Li Dong, Shaohan Huang, Damai Dai,\nShuming Ma, Barun Patra, Saksham Singhal, Payal\nBajaj, Xia Song, and Furu Wei. 2022. On the rep-\nresentation collapse of sparse mixture of experts.\nCoRR, abs/2204.09179.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nArthur Mensch, Michela Paganini, Jordan Hoff-\nmann, Bogdan Damoc, Blake A. Hechtman, Trevor\nCai, Sebastian Borgeaud, George van den Driessche,\nEliza Rutherford, Tom Hennigan, Matthew Johnson,\nKatie Millican, Albin Cassirer, Chris Jones, Elena\nBuchatskaya, David Budden, Laurent Sifre, Simon\nOsindero, Oriol Vinyals, Jack W. Rae, Erich Elsen,\nKoray Kavukcuoglu, and Karen Simonyan. 2022.\nUnified scaling laws for routed language models.\nCoRR, abs/2202.01169.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nNan Du, Yanping Huang, Andrew M. Dai, Simon\nTong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat,\nBarret Zoph, Liam Fedus, Maarten Bosma, Zong-\nwei Zhou, Tao Wang, Yu Emma Wang, Kellie Web-\nster, Marie Pellat, Kevin Robinson, Kathy Meier-\nHellstern, Toju Duke, Lucas Dixon, Kun Zhang,\nQuoc V . Le, Yonghui Wu, Zhifeng Chen, and Claire\nCui. 2021. Glam: Efficient scaling of language mod-\nels with mixture-of-experts. CoRR, abs/2112.06905.\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever.\n2014. Learning factored representations in a deep\nmixture of experts. In 2nd International Conference\non Learning Representations, ICLR 2014, Banff, AB,\nCanada, April 14-16, 2014, Workshop Track Proceed-\nings.\nWilliam Fedus, Jeff Dean, and Barret Zoph. 2022. A\nreview of sparse expert models in deep learning.\nWilliam Fedus, Barret Zoph, and Noam M. Shazeer.\n2021. Switch transformers: Scaling to trillion param-\neter models with simple and efficient sparsity. ArXiv,\nabs/2101.03961.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The Pile: An\n800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 7-11 November, 2021 , pages\n5484–5495. Association for Computational Linguis-\ntics.\nAaron Gokaslan and Vanya Cohen. 2019. Openwebtext\ncorpus. http://web.archive.org/save/http://\nskylion007.github.io/OpenWebTextCorpus.\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\nAccelerating large-scale inference with anisotropic\nvector quantization. In International Conference on\nMachine Learning.\nSuchin Gururangan, Mike Lewis, Ari Holtzman,\nNoah A. Smith, and Luke Zettlemoyer. 2021. Demix\nlayers: Disentangling domains for modular language\nmodeling. CoRR, abs/2108.05036.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models. CoRR, abs/2203.15556.\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan,\nand Geoffrey E. Hinton. 1991. Adaptive mixtures of\nlocal experts. Neural Computation, 3:79–87.\n15048\nSebastian Jaszczur, Aakanksha Chowdhery, Afroz Mo-\nhiuddin, Lukasz Kaiser, Wojciech Gajewski, Henryk\nMichalewski, and Jonni Kanerva. 2021. Sparse is\nenough in scaling transformers. In Advances in Neu-\nral Information Processing Systems 34: Annual Con-\nference on Neural Information Processing Systems\n2021, NeurIPS 2021, December 6-14, 2021, virtual,\npages 9895–9907.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2021.\nBillion-scale similarity search with gpus. IEEE\nTrans. Big Data, 7(3):535–547.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. CoRR,\nabs/2001.08361.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nGuillaume Lample, Alexandre Sablayrolles,\nMarc’Aurelio Ranzato, Ludovic Denoyer, and\nHervé Jégou. 2019. Large memory layers with\nproduct keys. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 8546–8557.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam M. Shazeer, and Z. Chen. 2021.\nGshard: Scaling giant models with conditional\ncomputation and automatic sharding. ArXiv,\nabs/2006.16668.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\nGoyal, and Luke Zettlemoyer. 2021. BASE layers:\nSimplifying training of large, sparse models. In Pro-\nceedings of the 38th International Conference on\nMachine Learning, ICML 2021, 18-24 July 2021, Vir-\ntual Event, volume 139 of Proceedings of Machine\nLearning Research, pages 6265–6274. PMLR.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach.\nSebastian Nagel. 2016. Cc-news. http:\n//web.archive.org/save/http://commoncrawl.\norg/2016/10/news-dataset-available.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper,\nPatrick LeGresley, Mostofa Patwary, Vijay Kor-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\nJulie Bernauer, Bryan Catanzaro, Amar Phanishayee,\nand Matei Zaharia. 2021. Efficient large-scale\nlanguage model training on GPU clusters using\nmegatron-lm. In SC ’21: The International Con-\nference for High Performance Computing, Network-\ning, Storage and Analysis, St. Louis, Missouri, USA,\nNovember 14 - 19, 2021, pages 58:1–58:15. ACM.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward Z.\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada, pages\n8024–8035.\nAlec Radford and Karthik Narasimhan. 2018. Im-\nproving language understanding by generative pre-\ntraining.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam,\nand Jason Weston. 2021. Hash layers for large sparse\nmodels. CoRR, abs/2106.04426.\nRoy Schwartz, Jesse Dodge, Noah Smith, and Oren\nEtzioni. 2020. Green ai. Communications of the\nACM, 63:54 – 63.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc V . Le, Geoffrey E. Hinton, and\nJeff Dean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nSainbayar Sukhbaatar, Edouard Grave, Guillaume Lam-\nple, Herve Jegou, and Armand Joulin. 2019. Aug-\nmenting self-attention with persistent memory. arXiv\npreprint arXiv:1907.01470.\nSainbayar Sukhbaatar, Arthur D. Szlam, Jason Weston,\nand Rob Fergus. 2015a. End-to-end memory net-\nworks. In NIPS.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.\n2015b. End-to-end memory networks. Advances in\nneural information processing systems, 28.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. 2020. Efficient transformers: A survey. ACM\nComputing Surveys (CSUR).\n15049\nTrieu H. Trinh and Quoc V . Le. 2018. A simple method\nfor commonsense reasoning. ArXiv, abs/1806.02847.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. ArXiv, abs/1706.03762.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\ngent abilities of large language models. CoRR,\nabs/2206.07682.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm’an, Ar-\nmand Joulin, and Edouard Grave. 2020. Ccnet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. In LREC.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022a. OPT: open\npre-trained transformer language models. CoRR,\nabs/2205.01068.\nZhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li,\nMaosong Sun, and Jie Zhou. 2022b. MoEfication:\nTransformer feed-forward layers are mixtures of ex-\nperts. In Findings of the Association for Compu-\ntational Linguistics: ACL 2022 , pages 877–890,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping\nHuang, Vincent Y . Zhao, Andrew M. Dai, Zhifeng\nChen, Quoc Le, and James Laudon. 2022. Mixture-\nof-experts with expert choice routing. CoRR,\nabs/2202.09368.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. 2015 IEEE International\nConference on Computer Vision (ICCV), pages 19–\n27.\nA Experimental details\nA.1 Hyperparameters\nTable 5 specifies shared hyperparameters across all\nexperiments, in which Table 5a contains ones for\ntraining data, optimizer, and efficient infrastructure\ntechniques; and Table 5b for architecture. Then,\nTable 6a describes the hyperparameters specifically\nfor Switch, Table 6b for LoRKM, Table 6c for\nPKM. In preliminary study, we train baselines\nwith different random seeds and found the results\nare almost identical. Therefore, in the interest of\ntime and carbon cost, we didn’t run with different\nrandom seeds for the experiments in this paper.\nA.2 Infrastructure\nWe used fairseq as our codebase to conduct ex-\nperiments. For training, all the training are done on\nV100 GPUs. Each training takes 32 GPUs; in rare\ncircumstances, the number of GPU is adjusted for\ntraining speed.\nA.3 Data\nHere is a detailed description of our pretraining\ncorpus.\n• BookCorpus (Zhu et al., 2015) consists of\nmore than 10K unpublished books (4GB);\n• English Wikipedia, excluding lists, tables\nand headers (12GB);\n• CC-News (Nagel, 2016) contains 63 mil-\nlions English news articles crawled between\nSeptember 2016 and February 2019 (76GB);\n• OpenWebText(Gokaslan and Cohen, 2019),\nan open source recreation of the WebText\ndataset used to train GPT-2 (38GB);\n• CC-Stories (Trinh and Le, 2018) contains a\nsubset of CommonCrawl data filtered to match\nthe story-like style of Winograd schemas\n(31GB);\n• English CC100 (Wenzek et al., 2020), a\ndataset extracted from CommonCrawl snap-\nshots between January 2018 and December\n2018, filtered to match the style of Wikipedia\n(292GB).\nB Product Key Memory\nAs mentioned in §2.2.2, LoRKM assumes that the\nfull key table K⊤∈Rd×dm is composed of and ap-\nproximated by a downward projection D ∈Rd×dℓ\nand a low-rank key table ˜K ∈Rdm×dℓ,\nK⊤= D ·˜K⊤\nLample et al. (2019) further approximated the\nlow-rank key table ˜K. It assumes that each low-\nrank key vector ˜ki ∈Rdℓ is created from the con-\ncatenation from two sub-key vectors c,c′∈R\ndℓ\n2 ;\n15050\n(a) Shared configuration for data, optimizer, and efficient infrastructure\nName Values\n#Tokens for training 60e9\n#Tokens for warm-up 375 ·10242\n#Tokens per batch 0.5 ·10242\n#Tokens per sample 2048\n#GPU 32\nGPU NVIDIA Tesla V100 32GB\nOptimizer Adam (βs = (0.9,0.98), ϵ= 1e−8)\nWeight Decay 0.01\nPeak Learning Rate 3e-4\nLearning Rate Scheduler polynomial_decay\nclip_norm 0.0\nDistributedDataParallel backend FullyShardedDataParallel\nmemory-efficient-fp16 True\nfp16-init-scale 4\ncheckpoint-activations True\n(b) Shared configuration for architecture.\nName Values\nObjective Causal Language Model (CLM)\nActivation function(f) GeLU\nModel dimension (d) 1024\ndm of non-S-FFN 4 ·1024\n#Attention Head 16\n#Layer 24\nDropout Rate 0.0\nAttention Dropout Rate 0.0\nshare-decoder-input-output-embed True\nTable 5: Shared configuration. This is also used for training base model.\n(a) Switch\nName Values\nmoe-gating-use-fp32 True\nmoe-gate-loss-wt 0.01\ni.e. CLM loss + 0.01·auxiliary loss (Fedus et al., 2021)\nDivide expert gradients by √# Expert =\n√\nB\n(b) LoRKM\nName Values\ndℓ 128\nBatchNorm after x ·D False\n(c) PKM\nName Values\ndℓ 128\n# key table (§2.2.2) 1\nBatchNorm after x ·D True\nTable 6: Specific architecture configuration\n15051\nand the two sub-key vectors is from two smaller and\nnon-overlapped sub-key tables C,C′∈R\n√dm×dℓ\n2 .\nUnlike LoRKM where key vectors are indepen-\ndent of each other, key vectors in PKM have some\noverlaps with each other and have astructured shar-\ning,\n˜ki =\n[\nc⌊i/√dm⌋, c′\ni (mod √dm)\n]\n∈Rdℓ\nOne could exploit this structure to efficiently com-\npute the memory coefficientm. One first calculates\nthe dot product between sub-key tables and down-\nprojected input t individually and combines them\nwith a negligible cost to form the full dot product\nm ∈Rdm:\nmi = f\n(\ns⌊i/√dm⌋+ s′\ni (mod √dm)\n)\nwhere s = t\n[\n: dℓ\n2\n]\n·C⊤∈R\n√dm,\ns′= t\n[dℓ\n2 :\n]\n·(C′)⊤∈R\n√dm\nt = x ·D ∈R\n√dℓ\nC Block size\nC.1 VanillaM with block size g >1\nFor VanillaM with block size g >1, we also tried\nthree other simple aggregation function, but they\nall under-perform Average. We show their results\nin Table 7.\nC.2 Analysis of smaller block sizes\nWe first quantify the intuition —“usage of model\nmemory is more spread out\" by number of activated\nmemory cells shared between two random tokens\n— E[r]. We define this quantity to be average of\nevery S-FFN layer, to reflect the overall behavior\nE[r] = 1\nLS-FFN\n∑\nℓ\nE[rℓ]\nwhere LS-FFN is the number of S-FFN. Because\nblock selection usually depends on a contextualized\ntoken embedding, it’s hard to draw tokens in an i.i.d.\nfashion. Therefore, we estimate the the quantity by\nevaluating the model on a validation set. We sample\nN token pairs from each sequence for estimation:\nE[rℓ] = 1\n|val|·N\n∑\ns∈val\nN−1∑\ni=0:(x,y)i∼Uniform(s×s)\n|Ix ∩Iy|·g\nwhere Ix is the indices of selected memory block\nfor token at position x, and similarly for Iy.\nRandHash, though, is an exception where uni-\nform sampling is used. Therefore, E[r] could also\nbe analytically calculated for various g, when as-\nsuming tokens are also uniformly distributed.3\nE[r] = 1\nLS-FFN\n·LS-FFN ·E[rℓ]\n=\nb∑\ni=1\n(b\ni\n)\n\nNo. of such\nblock assignments\n×\ni−1∏\nj=0\nb−j\nB−j\n  \nProbability of\ni overlaps\n·\nb−i−1∏\nk=0\nB−b−k\nB−k\n  \nProbability of\nj non-overlaps\n×\ni·g\nr cells\nin an overlap\nIn Fig. 4a, 4b, we evaluate our model with\nE = 16 on our validation subset and calculate\nthe estimations across various g. It is observed\nthat less sharing happens as block size decreases.\nHowever, the empirical estimation for RandHash\nare relatively constant across granularity. We sus-\npect this is due to the Zipf’s law of tokens. Also,\nwe note that the magnitude of E[r] are different\nfor different methods. We defer the reason of this\nphenomena to future work.\nC.3 Cost of smaller block sizes\nC.3.1 Cost of gate\nRandHash is efficient for computation because a\nhash table theoretically has time complexity O(1).\nIn contrast, a conventional learned gate 2.2.1 has an\nd-dimensional embedding for each memory block.\nTherefore, with total of B memory blocks, it has\nthe time complexity of O(d·B). In Table 8 we\nshow how the FLOPs percentage of learned gate\nin a single forward-backward computation changes\nwith respect to the change in memory block size,\nwhere we assume setup in §4 is adopted.\n3In the formulae, we use·and ×interchangeably for better\npresentation\n15052\nSelection method g #Parameters Train Aggregator Out-of-Domain In-Domain\n(Entire Model) ZFLOPs (22 domains; Avg. ±Std.) Train Val.\nDense Baseline 1 354.7M 0.212 N/A 16.96 ±5.20 19.60 17.16\nVanillaM 4096 858.3M 0.333\nAvg(·) 15.56 ±4.62 18.33 15.87\nAvg(|·|) 15.67 ±4.66 — 15.94\nMax(·) 16.11 ±4.86 — 16.33\nMin(·) 94.86 ±57.63 — 17.08\nTable 7: VanillaM with different simple aggregators\nTFLOPs of\nMemory block size (g)\n4096 2048 1024 512 256 128 64 32 1\n4 learned gates\n(across 24 layers)\n0.275 0.552 1.10 2.20 4.40 8.80 17.6 35.2 1124\nEntire model 1850 1850 1860 1860 1860 1860 1870 1890 2980\n% 0.0149 0.0298 0.0595 0.118 0.237 0.473 0.941 1.86 37.718\nTable 8: FLOPs percentage of learned gate increases when memory block size gdecreases\nC.3.2 Cost of communication\nThe conventional training framework of MoE (Fe-\ndus et al., 2021) depends on all_to_all opera-\ntions (Paszke et al., 2019) to route tokens to dif-\nferent devices. One might expect the communica-\ntion cost remains the same if the number of device\ndoesn’t change. However, this assumes the tokens\nare identified by their type. In fact, the training\nframework further identify the routed tokens type\nby the experts it routed to. Therefore, the com-\nmunication cost scales linearly with respect to the\nchange in the number of memory block.\nD Avg-K\nD.1 Rationale to use Avg in Avg-K\nWe heavily base our choice on experiments with ag-\ngregators in VanillaM (in Table 7). From the exper-\niments with average absolute value (after GeLU),\nwe hypothesized that a positive feature is good at\npredicting the value of a label/token against all oth-\ners. In contrast, a negative value is good at negating\nthe prediction of a single token. As such, positive\nfeatures are more predictive than negative ones. Al-\nthough the situation might be different for Avg-K\n(before GeLU), we expect the selection will only\nbe affected more because of the larger impact of\nnegative value.\nAlso, we consider the experiment with max-\npooled hidden states (i.e., Max(·)). This experiment\nshows that a memory block hardly has a single key-\nvalue cell that dominates over others since Max(·)\nunderperforms Avg(·) and Avg(|·|). What makes\nit worse, the max operation will overlook lots of\nhidden states at selection, but the overlooked hid-\nden states still contribute to the computation. In\ncontrast, the performance increases when we con-\nsider the average (or average of the absolute values)\nwhere every hidden state contributes to the decision.\nAlthough the situation is slightly different inAvg-K,\nthe “max-pooled” version of Avg-K will only over-\nestimate the hidden states information even more,\nand the aggregated value won’t be indicative of the\nhidden states used for computation.\nThe last consideration we have is that the average\nfunction is linear. When we select experts, we\nuse the dot product between input and averaged\nkeys. Due to the linearity, this value is equivalent\nto taking the dot product between the input and\nevery key and taking the average (See Appendix\nD.2). Thus, using this design choice saves a great\namount of computation compared with VanillaM,\nwhile keeping the neural memory analogy.\n15053\n1000\n1200\n1400\n1600[r]\nLayer = 5\n1000\n1500\n2000\n2500\nLayer = 11\n64\n256\n1024\n2048\n4096\ng\n600\n800\n1000\n1200\n1400\nLayer = 17\n64\n256\n1024\n2048\n4096\ng\n800\n900\n1000\n1100[r]\nLayer = 23\n64\n256\n1024\n2048\n4096\ng\n800\n1000\n1200\n1400\n1600\nLayer = Average\nVanillaM\n(a) VanillaM: empirical estimation from sampling\n0\n100\n200\n300[r]\nLayer = 5\n0\n100\n200\n300\nLayer = 11\n2\n4\n16\n32\n128\n256\n512\n1024\n4096\n2048\n64\n8\n1\ng\n0\n100\n200\n300\nLayer = 17\n2\n4\n16\n32\n128\n256\n512\n1024\n4096\n2048\n64\n8\n1\ng\n0\n100\n200\n300[r]\nLayer = 23\n2\n4\n16\n32\n128\n256\n512\n1024\n4096\n2048\n64\n8\n1\ng\n0\n100\n200\n300\nLayer = Average\nRandHash\nAnalytical Empirical\n(b) RandHash: both analytical value from close form calculation and empirical value from sampling\nFigure 4: Expected of shared memory cells across various block size g\nD.2 Avg-K analysis through comparison with\nVanillaM\nAvg-K essentially applies an average pooling to the\nunfactorized Kg to create representation of each\nblock. Due to the linearity of averaging, the oper-\nation ei ·x is equivalent to calculate the average\nof dot products within a block before GeLU and\nselect blocks with the average of dot products:\nei ·x =\n\n1\ng ·\ng−1∑\nj=0\nk(i)\nj\n\n·x\n= 1\ng ·\ng−1∑\nj=0\n(\nk(i)\nj ·x\n)\n= Avg\n(\nx ·\n(\nK(i)\n)⊤\n,dim=0\n)\n(Avg-K)\n15054\n0.20 0.25 0.30\nTotal Train FLOPs (ZFLOPs)\n14.5\n15.0\n15.5\n16.0\n16.5\n17.0Perplexity( )\nDense Baseline\nDense Baseline\nDomain = Out-of-Domain\n0.20 0.25 0.30\nTotal Train FLOPs (ZFLOPs)\n15.0\n15.5\n16.0\n16.5\n17.0\n17.5\nDense Baseline\nDense Baseline\nDomain = In-Domain Val.\nSelection method\nPKM\nVanillaM\nRandomHash(g = 1)\nLoRKM\nPKM-FFN\nSwitch\nRandomHash(g = 4 d)\nMemory size controlled by E\n4\n16\n32\nSelection method type\nDirect\nIndirect\nFigure 5: FLOPs-Perplexity trade-off different models where direct/indirect methods are further distinguished by\nmodel name.\nIn contrast, VanillaM uses average after\nGeLU(§5.1):\n1\ng\ng−1∑\nj=0\nGeLU\n(\nk(i)\nj ·x\n)\n(VanillaM)\nBecause GeLU is a non-linear function, average\nfrom Avg-K could be shared across tokens. In con-\ntrast, VanillaM can’t, and thus making Avg-K effi-\ncient.\nIn Fig 6, we experiment both methods with var-\nious g. We observe when gdecreases from 4096,\nthe perplexity of Avg-K drops more drastically than\nVanillaM. We believe this observation highlights\nthe impact of GeLU. Because lim\nx→−∞\nGeLU(x) =\n0, it protects the average in VanillaM from some\nvery negative values. Thus, Avg-K with larger gin-\ncluded more and potentially very negative values to\naverage over, and thus leads to worse choices than\nones made by VanillaM. On the other hand, when\ngdecreases, this “negative value” problem is mit-\nigated. When there are more blocks available for\nselection (smaller g), because negative dot products\naffects Avg-K more, it prefers blocks with more or\nvery positive dot products; whereas, VanillaM is\nprotected from negative value so it fails to detect\nthose blocks. Therefore, Avg-K with g≤256 could\nachieve an even better perplexity.\nD.3 Why Avg-K with load balancing should\nwork?\nComparing VanillaMand Avg-K, one would expect\nAvg-K to be greatly affected by extremely negative\nhidden states (before GeLU). Yet, the final model\nwith Avg-K could even outperform VanillaM with\nthe same block size (Fig. 6). This means the model\nwill accommodate small design changes.\nAdditionally, the requirement of a load balanc-\ning loss is determined by the sparsity of gradients.\nIf one “expert” gets updated with gradients while\nother “experts” are starved then the one expert will\nbe selected all of the time leading to a form of mode\ncollapse. In fact, with the same memory block size\n(g = 4096), we are surprised to observe Avg-K\n(w/o load balancing loss; row 6 in Table 4) could\nstill perform on par with Switch (w/ load balancing;\nrow 4 in Table 4). As our load balancing analysis\nsuggests in Appendix D.4, when the number of ex-\nperts is small, the mode collapse issue in Avg-K is\nsevere. This makes us more confident that Avg-K\nwill perform better with standard load balancing\nloss added.\nTherefore, we believe that with the loss added,\nthe model will accommodate and could still per-\nform competitively.\nD.4 Avg-K load balancing analysis\nOn the same validation set as used in §C, we\nalso conduct a load balancing analysis of mem-\nory blocks. Fig. 7 shows that Avg-K and VanillaM\ndisproportionally used some memory blocks.\nE Preliminary study for related work\nE.1 Terraformer analysis\nController in Terraformer Jaszczur et al. (2021)\nuses a controller to score all memory cells and pre-\nselect a subsets — Controller(x) — for computa-\n15055\n1\n64\n256\n1024\n2048\n4096\ng\n15\n16\n17Perplexity( )\n Dense Baseline\nDomain = Out-of-Domain\n1\n64\n256\n1024\n2048\n4096\ng\n15\n16\n17\n Dense Baseline\nDomain = In-Domain Val.\n1\n64\n256\n1024\n2048\n4096\ng\n18\n19\nDense Baseline\nDomain = In-Domain Train\nAvg-K VanillaM\nFigure 6: Perplexity performance (lower the better) of Avg-K and VanillaM across various g. We observe large drop\nin perplexity when gdecreases in Avg-K and less so in VanillaM; and Avg-K slightly outperform VanillaM with\ng≤256.\ntion.\ny =\n∑\ni∈Controller(x)\nf(x ·ki) ·vi\nThis is closest to our PKM-FFN, since their con-\ntroller is essentially a gate with low-rank key ta-\nble in LoRKM— g(x) = (x ·D) ·(K′)⊤, where\nD ∈Rd×dℓ, K ∈Rdm×dℓ, and dℓ ≪d. The differ-\nence is that they additionally assume the estimation\nfrom gate (and memory) could be seen as chun-\nked into blocks and only select top-1 memory cell\nscored by the controller from each blocks:\ny =\nB−1∑\ni=0\ng(x)(i)\nj∗ ·f(x ·k(i)\nj∗ ) ·v(i)\nj∗\nwhere j∗ = arg maxjg(x)(i)\nj . Therefore, their\nnumber of active memory cells kis equal to dm/g.\nSimilar to our contrastive pair of PKM-FFN and\nVanillaM, we hypothesize a “vanilla” version of\ntheir methods. Memory is chunked into blocks\nof size g — Kg = [K(0); ··· ; K(B−1)] and sim-\nilarly for Vg. Then, one chooses the top-1 with\nx ·(K(i))⊤. We call it VanillaController.\ny =\nB−1∑\ni=0\nf(x ·k(i)\nj∗ ) ·v(i)\nj∗\nwhere j∗ = arg maxjx ·(K(i))⊤. In Fig. 8, we\ncompare VanillaControllerto VanillaMwith g= 1,\nbecause the actual section is at the level of g= 1.\nWe set k in VanillaM to the one determined by\nequation above. We observe VanillaM outperforms\nVanillaController. Although the controller design\nas a gating function is justified (§5.2), the decision\nchoice of “chunking memory but only select the\nbest memory cells” seems unmotivated. Thus, we\nexclude this design setup from our analysis.\nE.2 ANN\nSince ANN is an approximation to exact search,\nwe propose to randomly sabotage VanillaM, which\nuses the exact search. Given a k, we randomly\nswap n% of the top- k of memory coefficient m\n(exact search results) with non-top-kvalues (during\ntraining and validation), and has accuracy (100 −\nn)% We call it Naive-ANN. This is meant to set\nup a random baseline for ANN, because different\nANN techniques might make systematic mistakes,\nrather than a random one. However, we believe this\ncould still serve as a proxy and shed light on how\nit affects performance. As we see in Fig. 9, the\nmodel quality is sensitive to the quality of ANN.\nIn our preliminary study, we found building data\nstructure after every update is expensive. This leads\nto some critical drawback when we apply the tech-\nniques to model parameter. Although one could\namortize the cost by periodically building, the out-\ndated data structure will lead to lower accuracy. If\none chooses a hyperparameter that leads to higher\nquality, the cost of preprocessing and the corre-\nsponding search will be even higher. What makes\nit worse, the current ANN methods’ search either\ndon’t support speedup by using GPU, or is not very\nwell-integrated with GPUs — slower than calcu-\nlating the exact dot product with CUDA kernel.\n15056\nSorted Index\n0.0\n0.2\n0.4\n0.6\n0.8Proportion(%)\nLayer = 5\nSorted Index\n0.0\n0.2\n0.4\n0.6\n0.8\nLayer = 11\nSorted Index\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nLayer = 17\nSorted Index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLayer = 23\nSorted Index\n0.0\n0.2\n0.4\n0.6\nLayer = Average\nVanillaM RandHash Avg-K\n(a) g = 4096\nSorted Index\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30Proportion(%)\nLayer = 5\nSorted Index\n0.0\n0.1\n0.2\n0.3\n0.4\nLayer = 11\nSorted Index\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nLayer = 17\nSorted Index\n0.00\n0.05\n0.10\n0.15\nLayer = 23\nSorted Index\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nLayer = Average\nVanillaM RandHash\n(b) g = 2048\nSorted Index\n0.00\n0.05\n0.10\n0.15\n0.20Proportion(%)\nLayer = 5\nSorted Index\n0.00\n0.05\n0.10\n0.15\n0.20\nLayer = 11\nSorted Index\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\nLayer = 17\nSorted Index\n0.00\n0.02\n0.04\n0.06\nLayer = 23\nSorted Index\n0.00\n0.05\n0.10\n0.15\nLayer = Average\nVanillaM RandHash\n(c) g = 1024\nSorted Index\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06Proportion(%)\nLayer = 5\nSorted Index\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nLayer = 11\nSorted Index\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nLayer = 17\nSorted Index\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nLayer = 23\nSorted Index\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nLayer = Average\nVanillaM RandHash Avg-K\n(d) g = 256\nSorted Index\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150Proportion(%)\nLayer = 5\nSorted Index\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\nLayer = 11\nSorted Index\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\nLayer = 17\nSorted Index\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\nLayer = 23\nSorted Index\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\nLayer = Average\nVanillaM RandHash Avg-K\n(e) g = 64\nFigure 7: Load balancing of VanillaM, RandHash, Avg-K. The height of bar represents the proportion of memory\nblock usage with which the memory block are sorted (in descending order).\n15057\n409664 1024\nk\n15\n16\n17Perplexity( )\n Dense Baseline\nDomain = Out-of-Domain\n409664 1024\nk\n15\n16\n17\n Dense Baseline\nDomain = In-Domain Val.\n409664 1024\nk\n16\n18\nDense Baseline\ng=16g=64\ng=1024\nDomain = In-Domain Train\nVanillaM VanillaController\n(a) Average perplexity performance (lower the better)\n11.0\n11.5\n12.0Perplexity( )\nDense Baseline\nDomain = ArXiv\n18.5\n19.0\n19.5\n20.0\n20.5\n21.0\n Dense Baseline\nDomain = Bibliotik\n17.0\n17.5\n18.0\n18.5\n19.0\nDense Baseline\nDomain = BookCorpus\n16.5\n17.0\n17.5\n18.0\n18.5\n19.0\nDense Baseline\nDomain = CommonCrawl\n9.6\n9.8\n10.0\n10.2\n Dense Baseline\nDomain = DM_Mathematics\n15.5\n16.0\n16.5\n17.0\n17.5\nDense Baseline\nDomain = Enron_Emails\n15\n16\n17\n18Perplexity( )\nDense Baseline\nDomain = EuroParl\n12.0\n12.5\n13.0\n13.5\n Dense Baseline\nDomain = FreeLaw\n6.0\n6.2\n6.4\n6.6\n6.8\n7.0\n Dense Baseline\nDomain = Github\n20\n21\n22\n23\n Dense Baseline\nDomain = Gutenberg_PG-19\n20.5\n21.0\n21.5\n22.0\n22.5\n23.0\n23.5\n Dense Baseline\nDomain = HackerNews\n19.0\n19.5\n20.0\n20.5\n21.0\n21.5\n22.0\nDense Baseline\nDomain = NIH_ExPorter\n16.5\n17.0\n17.5\n18.0Perplexity( )\nDense Baseline\nDomain = OpenSubtitles\n14.5\n15.0\n15.5\n16.0\n16.5\nDense Baseline\nDomain = OpenWebText2\n25\n26\n27\n28\n29\n30\n Dense Baseline\nDomain = PhilPapers\n16.0\n16.5\n17.0\n17.5\n18.0\n18.5\n19.0\nDense Baseline\nDomain = PubMed_Abstracts\n9.4\n9.6\n9.8\n10.0\n10.2\n10.4\n10.6\nDense Baseline\nDomain = PubMed_Central\n12.0\n12.5\n13.0\n13.5\n14.0\n Dense Baseline\nDomain = StackExchange\n409664 1024\nk\n12.5\n13.0\n13.5\n14.0Perplexity( )\nDense Baseline\nDomain = USPTO\n409664 1024\nk\n13.5\n14.0\n14.5\n15.0\n15.5\n16.0\n Dense Baseline\nDomain = Ubuntu_IRC\n409664 1024\nk\n14.0\n14.5\n15.0\n15.5\n16.0\n Dense Baseline\nDomain = Wikipedia_en\n409664 1024\nk\n9.50\n9.75\n10.00\n10.25\n10.50\n10.75\n11.00\nDense Baseline\nDomain = YoutubeSubtitles\n409664 1024\nk\n15.0\n15.5\n16.0\n16.5\n17.0\nDense Baseline\ng=16g=64\ng=1024\nDomain = Out-of-Domain\n409664 1024\nk\n15.0\n15.5\n16.0\n16.5\n17.0\n Dense Baseline\nDomain = In-Domain Val.\nVanillaM VanillaController\n(b) Performance on individual domain in PILE perplexity (lower the better)\nFigure 8: Perplexity performance (lower the better) of VanillaM (g=1) and VanillaController withE = 16.\n15058\n100\n75\n50\n25\n90\n95\nAccuracy(% )\n16.5\n17.0Perplexity( )\nDense Baseline\nDomain = Out-of-Domain\n100\n75\n50\n25\n90\n95\nAccuracy(% )\n16.5\n17.0\nDense Baseline\nDomain = In-Domain Val.\n100\n75\n50\n25\n90\n95\nAccuracy(% )\n19.0\n19.5\n Dense Baseline\nDomain = In-Domain Train\nNaive-ANN\n(a) Average perplexity performance (lower the better)\n11.8\n12.0\n12.2\n12.4\n12.6Perplexity( )\nDense Baseline\nDomain = ArXiv\n20.5\n21.0\n21.5\nDense Baseline\nDomain = Bibliotik\n18.00\n18.25\n18.50\n18.75\n19.00\n19.25\nDense Baseline\nDomain = BookCorpus\n18.25\n18.50\n18.75\n19.00\n19.25\nDense Baseline\nDomain = CommonCrawl\n10.1\n10.2\n10.3\n10.4\n10.5\nDense Baseline\nDomain = DM_Mathematics\n16.75\n17.00\n17.25\n17.50\n17.75\nDense Baseline\nDomain = Enron_Emails\n17.0\n17.5\n18.0\n18.5\n19.0Perplexity( )\nDense Baseline\nDomain = EuroParl\n12.8\n13.0\n13.2\n13.4\n13.6\n13.8\nDense Baseline\nDomain = FreeLaw\n6.7\n6.8\n6.9\n7.0\n7.1\n7.2\nDense Baseline\nDomain = Github\n22.0\n22.5\n23.0\n23.5\nDense Baseline\nDomain = Gutenberg_PG-19\n22.5\n23.0\n23.5\n24.0\nDense Baseline\nDomain = HackerNews\n21.0\n21.5\n22.0\nDense Baseline\nDomain = NIH_ExPorter\n17.6\n17.8\n18.0\n18.2Perplexity( )\nDense Baseline\nDomain = OpenSubtitles\n15.75\n16.00\n16.25\n16.50\n16.75\nDense Baseline\nDomain = OpenWebText2\n29\n30\n31\nDense Baseline\nDomain = PhilPapers\n18.0\n18.5\n19.0\nDense Baseline\nDomain = PubMed_Abstracts\n10.2\n10.4\n10.6\nDense Baseline\nDomain = PubMed_Central\n100\n75\n50\n25\n90\n95\nAccuracy(% )\n13.4\n13.6\n13.8\n14.0\n14.2\n14.4\nDense Baseline\nDomain = StackExchange\n100\n75\n50\n25\n90\n95\nAccuracy(% )\n13.6\n13.8\n14.0\n14.2\n14.4Perplexity( )\nDense Baseline\nDomain = USPTO\n100\n75\n50\n25\n90\n95\nAccuracy(% )\n15.0\n15.5\n16.0\n16.5\nDense Baseline\nDomain = Ubuntu_IRC\n100\n75\n50\n25\n90\n95\nAccuracy(% )\n15.50\n15.75\n16.00\n16.25\n16.50\nDense Baseline\nDomain = Wikipedia_en\n100\n75\n50\n25\n90\n95\nAccuracy(% )\n10.6\n10.8\n11.0\nDense Baseline\nDomain = YoutubeSubtitles\n100\n75\n50\n25\n90\n95\nAccuracy(% )\n16.25\n16.50\n16.75\n17.00\n17.25\nDense Baseline\nDomain = Out-of-Domain\nNaive-ANN\n(b) Performance on individual domain perplexity in PILE (lower the better)\nFigure 9: Perplexity performance (lower the better) of Naive-ANN accuracy with E = 4.\n15059\nSelection method\nType Direct Indirect\nSelection method Dense\nBaseline PKM VanillaM PKM-FFN RandHash\nE 1 16 32 32 16 16 16\nk 4096 4096 4096 8192 4096 4096 4096\nOut-of-Domain\nArXiv 12.39 12.20 11.82 11.89 10.75 11.05 11.22\nBibliotik 21.17 20.82 20.15 20.25 18.49 19.11 19.33\nBookCorpus 18.90 18.61 18.09 18.19 16.80 17.26 17.49\nCommonCrawl 18.98 18.68 18.09 18.20 16.68 17.23 17.40\nDM_Mathematics 10.27 10.34 10.05 10.28 9.70 9.72 9.91\nEnron_Emails 17.51 17.23 16.67 16.68 15.55 15.90 16.18\nEuroParl 18.35 17.79 17.01 17.05 14.48 15.50 15.03\nFreeLaw 13.62 13.34 12.84 12.93 11.70 12.11 12.29\nGithub 7.01 6.91 6.67 6.68 6.08 6.25 6.37\nGutenberg_PG-19 23.14 22.61 21.83 22.03 19.88 20.74 21.07\nHackerNews 23.58 23.35 22.44 22.62 20.71 21.36 21.76\nNIH_ExPorter 21.87 21.45 20.59 20.77 18.81 19.48 19.69\nOpenSubtitles 18.03 17.99 17.46 17.44 16.48 16.84 17.10\nOpenWebText2 16.45 16.15 15.60 15.68 14.19 14.73 14.74\nPhilPapers 30.63 30.02 28.50 28.74 25.14 26.44 26.60\nPubMed_Abstracts 18.88 18.50 17.71 17.92 16.11 16.75 16.90\nPubMed_Central 10.57 10.40 10.08 10.14 9.37 9.66 9.71\nStackExchange 14.10 13.85 13.37 13.45 12.05 12.46 12.72\nUSPTO 14.28 14.07 13.58 13.68 12.55 12.96 13.09\nUbuntu_IRC 16.14 15.40 14.95 15.08 14.14 14.35 14.62\nWikipedia_en 16.26 16.03 15.33 15.48 14.07 14.51 14.59\nYoutubeSubtitles 10.98 10.86 10.41 10.41 9.48 9.87 9.77\nAverage 16.96 16.66 16.06 16.16 14.69 15.19 15.35\nTable 9: Detailed out-of-domain perplexity for Table 3. Best two performance on each domain is in bold. Relative\nranking on each domain generally follows the relative ranking by averaged performance (i.e. last row).\n15060\nSelection method Dense\nBaseline RandHash Switch PKM-FFN Avg-K\ng 1 4096 1 4096 1 4096 256 64\nOut-of-Domain\nArXiv 12.39 11.42 11.22 12.32 11.05 12.09 10.99 10.85\nBibliotik 21.17 19.84 19.33 19.87 19.11 20.49 18.70 18.61\nBookCorpus 18.90 17.94 17.49 17.85 17.26 18.33 16.86 16.82\nCommonCrawl 18.98 17.84 17.40 17.70 17.23 18.42 16.89 16.81\nDM_Mathematics 10.27 10.22 9.91 10.63 9.72 10.25 9.62 9.61\nEnron_Emails 17.51 16.70 16.18 17.36 15.90 17.20 15.65 15.55\nEuroParl 18.35 15.55 15.03 19.63 15.50 17.38 15.32 15.13\nFreeLaw 13.62 12.56 12.29 12.80 12.11 13.20 11.84 11.77\nGithub 7.01 6.51 6.37 6.96 6.25 6.89 6.18 6.12\nGutenberg_PG-19 23.14 21.55 21.07 21.81 20.74 22.36 20.07 19.98\nHackerNews 23.58 22.30 21.76 22.59 21.36 22.95 20.82 20.65\nNIH_ExPorter 21.87 20.22 19.69 20.99 19.48 21.09 19.09 19.01\nOpenSubtitles 18.03 17.33 17.10 17.31 16.84 17.74 16.62 16.66\nOpenWebText2 16.45 15.16 14.74 15.51 14.73 15.87 14.48 14.39\nPhilPapers 30.63 27.53 26.60 30.84 26.44 29.51 25.90 25.72\nPubMed_Abstracts 18.88 17.39 16.90 18.36 16.75 18.24 16.34 16.33\nPubMed_Central 10.57 9.94 9.71 10.28 9.66 10.30 9.50 9.45\nStackExchange 14.10 13.04 12.72 13.78 12.46 13.73 12.23 12.13\nUSPTO 14.28 13.41 13.09 13.54 12.96 13.89 12.73 12.62\nUbuntu_IRC 16.14 14.95 14.62 14.78 14.35 15.50 14.26 13.59\nWikipedia_en 16.26 14.98 14.59 15.67 14.51 15.73 14.23 14.12\nYoutubeSubtitles 10.98 10.06 9.77 11.25 9.87 10.59 9.73 9.67\nAverage 16.96 15.75 15.35 16.45 15.19 16.44 14.91 14.80\nTable 10: Detailed out-of-domain perplexity for Table 4. Best performance on each domain is in bold. Relative\nranking on each domain generally follows the relative ranking by averaged performance (i.e. last row).\n15061"
}