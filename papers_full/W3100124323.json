{
  "title": "Unsupervised Extractive Summarization by Pre-training Hierarchical Transformers",
  "url": "https://openalex.org/W3100124323",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2096524408",
      "name": "Shusheng Xu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2101736369",
      "name": "Xingxing Zhang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2099459923",
      "name": "Yi Wu",
      "affiliations": [
        "Tsinghua University",
        "ShangHai JiAi Genetics & IVF Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2096867225",
      "name": "Ming Zhou",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2293771131",
    "https://openalex.org/W1602831581",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1486649854",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2955471745",
    "https://openalex.org/W2574535369",
    "https://openalex.org/W2251803607",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1525595230",
    "https://openalex.org/W3158986179",
    "https://openalex.org/W3138773240",
    "https://openalex.org/W2927431361",
    "https://openalex.org/W3035027743",
    "https://openalex.org/W1854214752",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2952138241",
    "https://openalex.org/W2307381258",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3105245805",
    "https://openalex.org/W2785910460",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2054211469",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3101913037",
    "https://openalex.org/W2083305840",
    "https://openalex.org/W2101390659",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2913407944",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2950342809",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2128672521",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2083778364",
    "https://openalex.org/W2964165364",
    "https://openalex.org/W2250968833",
    "https://openalex.org/W2888556271",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2081174553",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W1620608722",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W2141514700",
    "https://openalex.org/W2890116189",
    "https://openalex.org/W2953285682",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2140440594",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2963768805",
    "https://openalex.org/W2962972512"
  ],
  "abstract": "Unsupervised extractive document summarization aims to select important sentences from a document without using labeled summaries during training. Existing methods are mostly graph-based with sentences as nodes and edge weights measured by sentence similarities. In this work, we find that transformer attentions can be used to rank sentences for unsupervised extractive summarization. Specifically, we first pre-train a hierarchical transformer model using unlabeled documents only. Then we propose a method to rank sentences using sentence-level self-attentions and pre-training objectives. Experiments on CNN/DailyMail and New York Times datasets show our model achieves state-of-the-art performance on unsupervised summarization. We also find in experiments that our model is less dependent on sentence positions. When using a linear combination of our model and a recent unsupervised model explicitly modeling sentence positions, we obtain even better results.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1784–1795\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n1784\nUnsupervised Extractive Summarization by Pre-training Hierarchical\nTransformers\nShusheng Xu1∗, Xingxing Zhang2, Yi Wu1,3, Furu Wei2 and Ming Zhou2\n1 IIIS, Tsinghua Unveristy, Beijing, China\n2 Microsoft Research Asia, Beijing, China\n3 Shanghai Qi Zhi institute, Shanghai China\nxuss20@mails.tsinghua.edu.cn\n{xizhang,fuwei,mingzhou}@microsoft.com\njxwuyi@gmail.com\nAbstract\nUnsupervised extractive document summariza-\ntion aims to select important sentences from\na document without using labeled summaries\nduring training. Existing methods are mostly\ngraph-based with sentences as nodes and edge\nweights measured by sentence similarities. In\nthis work, we ﬁnd that transformer attentions\ncan be used to rank sentences for unsuper-\nvised extractive summarization. Speciﬁcally,\nwe ﬁrst pre-train a hierarchical transformer\nmodel using unlabeled documents only. Then\nwe propose a method to rank sentences using\nsentence-level self-attentions and pre-training\nobjectives. Experiments on CNN/DailyMail\nand New York Times datasets show our model\nachieves state-of-the-art performance on unsu-\npervised summarization. We also ﬁnd in ex-\nperiments that our model is less dependent on\nsentence positions. When using a linear combi-\nnation of our model and a recent unsupervised\nmodel explicitly modeling sentence positions,\nwe obtain even better results.\n1 Introduction\nDocument summarization is the task of transform-\ning a long document into its shorter version while\nstill retaining its important content. Researchers\nhave explored many paradigms for summarization,\nwhile the most popular ones are extractive summa-\nrization and abstractive summarization (Nenkova\nand McKeown, 2011). As their names suggest, ex-\ntractive summarization generates summiries by ex-\ntracting text from original documents, and abstrac-\ntive summarization rewrites documents by para-\nphrasing or deleting some words or phrases.\nMost summarization models require labeled data,\nwhere documents are paired with human written\nsummaries. Unfortunately, human labeling for sum-\nmarization task is expensive and therefore high\n∗ Work done during the ﬁrst author’s internship at\nMicrosoft Research Asia.\nquality large scale labeled summarization datasets\nare rear (Hermann et al., 2015) compared to grow-\ning web documents created everyday. It is also not\npossible to create summaries for documents in all\ntext domains and styles. In this paper, we focus on\nunsupervised summarization, where we only need\nunlabeled documents during training.\nMany attempts for unsupervised summariza-\ntion are extractive (Carbonell and Goldstein, 1998;\nRadev et al., 2000; Lin and Hovy, 2002; Mihal-\ncea and Tarau, 2004; Erkan and Radev, 2004; Wan,\n2008; Wan and Yang, 2008; Hirao et al., 2013;\nParveen et al., 2015). The core problem is to iden-\ntify salient sentences in a document. The most\npopular approaches among these work rank sen-\ntences in the document using graph based algo-\nrithms, where each node is a sentence and weights\nof edges are measured by sentence similarities.\nThen a graph ranking method is employed to es-\ntimate sentence importance. For example, Tex-\ntRank (Mihalcea and Tarau, 2004) utilizes word\nco-occurrence statistics to compute similarity and\nthen employs PageRank (Page et al., 1997) to rank\nsentences. Sentence similarities in (Zheng and La-\npata, 2019) are measured with BERT (Devlin et al.,\n2019) and sentences are sorted w.r.t. their centrali-\nties in a directed graph.\nRecently, there has been increasing interest in de-\nveloping unsupervised abstractive summarization\nmodels (Wang and Lee, 2018; Fevry and Phang,\n2018; Chu and Liu, 2019; Yang et al., 2020). These\nmodels are mostly based on sequence to sequence\nlearning (Sutskever et al., 2014) and sequential\ndenoising auto-encoding (Dai and Le, 2015). Un-\nfortunately, there is no guarantee that summaries\nproduced by these models are grammatical and\nconsistent with facts described original documents.\nZhang et al. (2019) propose an unsupervised\nmethod to pre-train a hierarchical transformer\nmodel (i.e., HIBERT) for document modeling. The\n1785\nhierarchical transformer has a token-level trans-\nformer to learn sentence representations and a\nsentence-level transformer to learn interactions be-\ntween sentences with self-attention. In Zhang et al.\n(2019), HIBERT is applied to supervised extrac-\ntive summarization. However, we believe that af-\nter pre-training HIBERT on large scale unlabeled\ndata, the self-attention scores in the sentence-level\ntransformer becomes meaningful for estimating\nthe importance of sentences. Intuitively, if many\nsentences in a document attend to one particu-\nlar sentence with high attention scores, then this\nsentence should be important. In this paper, we\nﬁnd that (sentence-level) transformer attentions (in\na hierarchical transformer) can be used to rank\nsentences for unsupervised extractive summariza-\ntion, while previous work mostly leverage graph\nbased (or rule based) methods and sentence simi-\nlarities computed with off-the-shelf sentence em-\nbeddings. Speciﬁcally, we ﬁrst introduce two pre-\ntraining tasks for hierarchical transformers (i.e.,\nextended HIBERT) to obtain sentence-level self-\nattentions using unlabled documents only. Then,\nwe design a method to rank sentences by using\nsentence-level self-attentions and pre-training ob-\njectives. Experiments on CNN/DailyMail and New\nYork Times datasets show our model achieves state-\nof-the-art performance on unsupervised summa-\nrization. We also ﬁnd in experiments that our\nmodel is less dependent on sentence positions.\nWhen using a linear combination of our model\nand a recent unsupervised model explicitly mod-\neling sentence positions, we obtain even better\nresults. Our code and models are available at\nhttps://github.com/xssstory/STAS.\n2 Related Work\nIn this section, we introduce work on supervised\nsummarization, unsupervised summarization and\npre-training.\nSupervised Summarization Most summariza-\ntion models require supervision from labeled\ndatasets, where documents are paired with human\nwritten summaries. As mentioned earlier, extrac-\ntive summarization aims to extract important sen-\ntences from documents and it is usually viewed\nas a (sentence) ranking problem by using scores\nfrom classiﬁers (Kupiec et al., 1995) or sequential\nlabeling models (Conroy and O’leary, 2001). Sum-\nmarization performance of this class of methods are\ngreatly improved, when human engineered features\n(Radev et al., 2004; Nenkova et al., 2006; Filatova\nand Hatzivassiloglou, 2004) are replaced with con-\nvolutional neural networks (CNN) and long short-\nterm memory networks (LSTM) (Cheng and La-\npata, 2016; Nallapati et al., 2017; Narayan et al.,\n2018; Zhang et al., 2018).\nAbstractive summarization on the other hand\ncan generate new words or phrases and are mostly\nbased on sequence to sequence (seq2seq) learn-\ning (Bahdanau et al., 2015). To better ﬁt in the\nsummarization task, the original seq2seq model is\nextended with copy mechanism (Gu et al., 2016),\ncoverage model (See et al., 2017), reinforcement\nlearning (Paulus et al., 2018) as well as bottom-up\nattention (Gehrmann et al., 2018). Recently, pre-\ntrained transformers (Vaswani et al., 2017) achieve\ntremendous success in many NLP tasks (Devlin\net al., 2019; Liu et al., 2019). Pre-trained methods\ncustomized for both extractive (Zhang et al., 2019;\nLiu and Lapata, 2019) and abstractive (Dong et al.,\n2019; Lewis et al., 2019) summarization again ad-\nvance the state-of-the-art in supervised summariza-\ntion. Our model also leverages pre-trained methods\nand models, but it is unsupervised.\nUnsupervised Summarization Compared to su-\npervised models, unsupervised models only need\nunlabeled documents during training. Most unsu-\npervised extractive models are graph based (Car-\nbonell and Goldstein, 1998; Radev et al., 2000; Lin\nand Hovy, 2002; Mihalcea and Tarau, 2004; Erkan\nand Radev, 2004; Wan, 2008; Wan and Yang, 2008;\nHirao et al., 2013; Parveen et al., 2015). For exam-\nple, TextRank (Mihalcea and Tarau, 2004) treats\nsentences in a document as nodes in an undirected\ngraph, and edge weights are measured with co-\noccurrence based similarities between sentences.\nThen PageRank (Page et al., 1999) is employed to\ndetermine the ﬁnal ranking scores for sentences.\nZheng and Lapata (2019) builds directed graph\nby utilizing BERT (Devlin et al., 2019) to com-\npute sentence similarities. The importance score\nof a sentence is the weighted sum of all its out\nedges, where weights for edges between the cur-\nrent sentence and preceding sentences are negative.\nThus, leading sentences tend to obtain high scores.\nUnlike Zheng and Lapata (2019), sentence posi-\ntions are not explicitly modeled in our model and\ntherefore our model is less dependent on sentence\npositions (as shown in experiments).\nThere are also an interesting line of work on un-\nsupervised abstractive summarization. Yang et al.\n1786\n(2020) pre-trains a seq2seq Transformer by pre-\ndicting the ﬁrst three sentences of news documents\nand then further tunes the model with semantic\nclassiﬁcation and denoising auto-encoding objec-\ntives. The model described in Wang and Lee (2018)\nutilizes seq2seq auto-encoding coupled with adver-\nsarial training and reinforcement learning. Fevry\nand Phang (2018) and Baziotis et al. (2019) fo-\ncus on sentence summarization (i.e., compression).\nChu and Liu (2019) proposes yet another denois-\ning auto-encoding based model in multi-document\nsummarization domain. However, the performance\nof these unsupervised models are still unsatisfac-\ntory compared to their extractive counterparts.\nPre-training Pre-training methods in NLP learn\nto encode text by leveraging unlabeled text. Early\nwork mostly concentrate on pre-training word em-\nbeddings (Mikolov et al., 2013; Pennington et al.,\n2014; Bojanowski et al., 2017). Later, sentence en-\ncoder can also be pre-trained with language model\n(or masked language model) objectives (Peters\net al., 2018; Radford et al., 2018; Devlin et al.,\n2019; Liu et al., 2019). Zhang et al. (2019) propose\na method to pre-train a hierarchical transformer en-\ncoder (document encoder) by predicting masked\nsentences in a document for supervised summariza-\ntion, while we focus on unsupervised summariza-\ntion. In our method, we also propose a new task\n(sentence shufﬂing) for pre-training hierarchical\ntransformer encoders. Iter et al. (2020) propose a\ncontrastive pre-training objective to predict relative\ndistances of surrounding sentences to the anchor\nsentence, while our sentence shufﬂing task pre-\ndicts original positions of sentences from a shufﬂed\ndocuemt. Besides, pre-training methods mentioned\nabove focus on learning good word, sentence or\ndocument representations for downstream tasks,\nwhile our method focuses on learning sentence\nlevel attention distributions (i.e., sentence associ-\nations), which is shown in our experiments to be\nvery helpful for unsupervised summarization.\n3 Model\nIn this section, we describe our unsupervised sum-\nmarization model STAS (as shorthand for Sentence-\nlevel Transformer based Attentive Summarization).\nWe ﬁrst introduce how documents are encoded in\nour model. Then we present methods to pre-trained\nour document encoder. Finally we apply the pre-\ntrained encoder to unsupervised summarization.\nFigure 1: The architecture of our hierarchical encoder,\nthe token level Transformer encodes tokens and then\nthe sentence level Transformer learns ﬁnal sentence\nrepresentations from representations at <s>.\n3.1 Document Modeling\nLet D = (S1,S2,...,S |D|) denote a document,\nwhere Si = (wi\n0,wi\n1,wi\n2,...,w i\n|Si|) is a sentence\nin Dand wi\nj is a token in Si. As a common\nwisdom, we also add two special tokens (i.e.,\nwi\n0 = <s> and wi\n|Si| = </s>) to Si, which\nrepresents the begin and end of a sentence, respec-\ntively. Transformer models (Vaswani et al., 2017),\nwhich are composed of multiple self-attentive\nlayers and skip connections (He et al., 2016),\nhave shown tremendous success in text encoding\n(Devlin et al., 2019). Due to the hierarchical\nnature of documents, we encode the document D\nusing a hierarchical Transformer encoder, which\ncontains a token-level Transformer TransT and a\nsentence-level Transformer TransS as shown in\nFigure 1. Let ||denote an operator for sequences\nconcatenation. TransT views Das a ﬂat sequence\nof tokens denoted as D = (S1 ||S2 ||... ||S|D|).\nAfter we apply TransT to D, we obtain\ncontextual representations for all tokens\n(v1\n0,v1\n1,..., v1\n|S1|,..., vi\nj,..., v|D|\n0 ,..., v|D|\n|S|D||).\nWe use the representation at each <s> token\nas the representation for that sentence and\ntherefore representations for all sentences in D\nare V = (v1\n0,v2\n0,..., v|D|\n0 ). The sentence-level\nTransformer TransS takes V as input and learns\nsentence representations given other sentences in\nDas context:\nH,A = TransS(V) (1)\nwhere H = (h1,h2,..., h|D|) and hi is the ﬁnal\nrepresentation of Si; A is the self-attention matrix\n1787\nand Ai,j is the attention score from sentence Si\nto sentence Sj. TransS contains multiple layers\nand each layer contains multiple attention heads.\nTo obtain A, we ﬁrst average the attention scores\nacross different heads and then across different lay-\ners. Our hierarchical document encoder is similar\nto the hierarchical Transformer model described in\nZhang et al. (2019). The main difference is that our\ntoken-level Transformer encodes all sentences in a\ndocument as a whole rather than separately.\n3.2 Pre-training\nIn this section, we pre-train the hierarchical doc-\nument encoder introduced in Section 3.1 using\nunlabeled documents only. We expect that after\npre-training, the encoder would obtain the ability\nof modeling interactions (i.e., attentions) among\nsentences in a document. In this following, we in-\ntroduce two tasks we used to pre-train the encoder.\nMasked Sentences Prediction The ﬁrst task is\nMasked Sentences Prediction (MSP) described in\nZhang et al. (2019). We randomly mask 15% of\nsentences in a document and then predict the origi-\nnal sentences. Let D= (S1,S2,...,S |D|) denote\na document and ˜D= (˜S1,..., ˜S|D|) the document\nwith some sentences masked, where\n˜Si =\n{\nSi 85% of cases\nmask(Si) 15% of cases (2)\nmask(Si) is a function to mask Si, which in 80%\nof cases replaces each word inSi with the [MASK]\ntoken, in 10% of cases replaces Si with a random\nsentence and in the remaining 10% of cases keep\nSi unchanged. The masking strategy is similar to\nthat of BERT (Devlin et al., 2019), but it is applied\non sentence level. Let I= {i|˜Si = mask(Si)}\ndenote the set of indices for masked sentences and\nO= {Si|i∈I} the original sentences correspond-\ning to masked sentences.\nSupposing i∈I , we demonstrate how we pre-\ndict the original sentence Si = (wi\n0,wi\n1,...,w i\n|Si|)\ngiven ˜D. As shown in Figure 2, we ﬁrst encode\n˜Dusing the encoder in Section 3.1 and obtain\n˜H = (˜h1,˜h2,..., ˜h|D|). Then we use ˜hi (i.e.,\nthe contextual representation of ˜Si) to predict Si\none token at a time with a conditional Transformer\ndecoder TransDecM . We inject the information\nof ˜Si to TransDecM by adding ˜hi after the self\nattention sub-layer of each Transformer block in\nFigure 2: An example of masked sentences predic-\ntion. The third sentence in the document is masked\nand the hierarchical encoder encodes the masked docu-\nment. We then use TransDecS to predict the original\nsentence one token at a time.\nTransDecM . Assuming wi\n0:j−1 has been gener-\nated, the probability of wi\nj given wi\n0:j−1 and ˜Dis\nˆhi\nj = TransDecM (wi\n0:j−1,˜hi) (3)\np(wi\nj|wi\n0:j−1, ˜D) =softmax(Woˆhi\nj) (4)\nThe probability of all original sentences given ˜Dis\np(O|˜D) =\n∏\nSi∈O\n|Si|∏\nj=1\np(wi\nj|wi\n0:j−1, ˜D) (5)\nMSP is proposed in HIBERT (Zhang et al., 2019)\nfor supervised summarization, while we use MSP\nand transformer attention for sentence ranking in\nunsupervised summarization (Section 3.3). Note\nthat the goal and the way of using MSP in this work\nis different from these in HIBERT.\nSentence Shufﬂing We propose a new task that\nshufﬂes the sentences in a document and then select\nsentences in the original order one by one. We\nexpect that the hierarchical document encoder can\nlearn to select sentences based on their contents\nrather than positions.\nRecall that D = (S1,S2,...,S |D|) is a docu-\nment, we shufﬂe the sentences in Dand obtain\na permuted document D′ = (S′\n1,S′\n2,...,S ′\n|D|)\nwhere Si is the ith sentence in the original doc-\nument and there exists a sentence S′\nPi = Si in\nthe permuted document D′(i.e., Pi ∈[1,|D|] is\nthe position of Si in D′). In this task, we predict\nP= (P1,P2,...,P |D|).\nAs shown in Figure 3, we ﬁrst use the docu-\nment encoder in Section 3.1 to encode D′ and\nyields its context dependent sentence representa-\ntions H′ = (h′\n1,h′\n2,..., h′\n|D|). Supposing that\nP0,P1,P2,...,P t−1 are known1, we predict Pt us-\ning a Pointer Network (Vinyals et al., 2015) with\n1We set P0 = 0; h′\n0 is a zero vector.\n1788\nFigure 3: An example of Sentence Shufﬂing. The\nsentences in the document are shufﬂed and then pass\nthrough the hierarchical encoder, then a Pointer Net-\nwork with TransDecP as its decoder is adopted to pre-\ndict the positions of original sentences in the shufﬂed\ndocument.\nTransformer as its decoder. Let TransDecP de-\nnote the transformer decoder in PointerNet, EPi is\nthe absolute positional embedding of Pi in origi-\nnal document and pi the positional embedding of\nPi during decoding. The input of TransDecP is\nthe sum of sentence representations and positional\nembeddings:\nMt−1 =(h′\nP1 +p1+EP1 ,..., h′\nPt−1 +pt−1+EPt−1 )\nThe output ho\nt summaries the sentences de-\npermutated so far.\nho\nt = TransDecP (Mt−1) (6)\nThen the probability of selecting S′\npt is estimated\nwith the attention (Bahdanau et al., 2015) between\nho\nt and all sentences in D′as follows:\np(Pt|P1:t−1,D′) = exp(g(ho\nt ,h′\nPt ))∑\ni exp(g(ho\nt ,h′\ni)) (7)\nwhere gis a feed forward neural network with the\nfollowing parametrization:\ng(ho\nt ,h′\ni) =v⊤\na tanh(Uaho\nt + Wah′\ni) (8)\nwhere va ∈ Rd×1, Ua ∈ Rd×d, Wa ∈ Rd×d\nare trainable parameters. Finally the probability\nof positions of original sentences in the shufﬂed\ndocument is:\np(P|D′) =\n|D|∏\nt=1\np(Pt|P0:t−1,D′) (9)\nDuring training, for each batch of documents we\napply both the masked sentence prediction and sen-\ntence shufﬂing tasks. One document Dgenerates\na masked document ˜Dand a shufﬂed document\nD′. Note that 15% of sentences are masked in the\nmasked document ˜D, and all sentences are shufﬂed\nin the shufﬂed document D′. The whole model is\noptimized with the following objective:\nL(θ) =−\n∑\nD∈X\nlog p(O|˜D) + logp(P|D′)\nwhere Dis a document in the training document\nset X.\n3.3 Unsupervised Summarization\nIn this section, we propose our unsupervised extrac-\ntive summarization method. Extractive summariza-\ntion aims to select the most important sentences in\ndocument. Once we have obtained a hierarchical\nencoder using the pre-training methods in Section\n3.2, we are ready to rank sentences and no addi-\ntional ﬁne-tuning is needed in this step.\nOur ﬁrst ranking criteria is based on the prob-\nabilities of sentences in a document. Recall that\nD= (S1,S2,...,S |D|) is a document and its prob-\nability is\np(D) =\n|D|∏\ni=1\np(Si|S1:i−1) ≈\n|D|∏\ni=1\np(Si|D¬Si ) (10)\nIt is not straight forward to estimate p(Si|S1:i−1)\ndirectly since document models in this work are\nall bidirectional. However, we can estimate\np(Si|D¬Si ) using the masked sentences prediction\ntask in Section 3.2. We therefore use p(Si|D¬Si )\nto approximate p(Si|S1:i−1). Finding the most im-\nportant sentence is equivalent to ﬁnding the sen-\ntence with highest probability (i.e., p(Si|D¬Si )).\nIn the following we demonstrate how to estimate\np(Si|D¬Si ). As in Section 3.2, we create D¬Si\nby masking Si in D(i.e., replacing all tokens in\nSi with [MASK] tokens). p(Si|D¬Si ) can be es-\ntimated using Equation (5). To make the proba-\nbilities of different sentences comparable, we nor-\nmalize them by their length. Then we obtain ˆri as\nfollows2 (also see Equation (5))\nˆri = 1\n|Si|\n|Si|∑\nj=1\np(wi\nj|wi\n0:j−1,D¬Si ) (11)\nWe also normalize ˆri across sentences (in a docu-\nment) and obtain our ﬁrst ranking criteria ˜ri:\n˜ri = ˆri\n∑|D|\nj=1 ˆrj\n(12)\n2We also tried the geometric average, but the effect is not\nas good as the arithmetic average.\n1789\nIn the second ranking criteria, we model the con-\ntributions of other sentences to the current sentence\nexplicitly. We view a document Das a directed\ngraph, where each sentence in it is a node. The con-\nnections between sentences (i.e., edge weights) can\nbe modeled using the self-attention matrix A of\nthe sentence level Transformer encoder described\nin Section 3.1, which is produced by a pre-trained\nhierarchical document encoder. We assume that a\nsentence Sj can transmit its importance score ˜ri to\nan arbitrary sentence Si through the edge between\nthem. Let Aj,i denote the attention score from Sj\nto Si. After receiving all transmissions from all\nsentences, the second ranking score for Si is as\nfollows:\nr′\ni =\n|D|∑\nj=1,j̸=i\nAj,i ×˜rj (13)\nThe ﬁnal ranking score of Si combines the score\nfrom itself as well as other sentences:\nri = γ1 ˜ri + γ2 r′\ni (14)\nγ1 and γ2 are coefﬁcients tuned on development\nset. ri can be computed iteratively by assigning\nri to ˜ri and repeating Equation (13) and Equation\n(14) for T iterations. We ﬁnd a small T (T ≤3)\nworks well according to the development set.\n4 Experiments\nIn this section we assess the performance of STAS\non the document summarization task. We ﬁrstly\nintroduce datasets we used and then give our imple-\nmentation details. Finally we compare our method\nagainst previous methods.\n4.1 Datasets\nWe evaluate STAS on two summarization datasets,\nnamely the CNN/DailyMail (CNN/DM; Hermann\net al. 2015) dataset and the New York Times (NYT;\nSandhaus 2008) dataset. CNN/DM is composed\nof articles from CNN and Daily Mail news web-\nsites, which uses their associated highlights as ref-\nerence summaries. NYT dataset contains articles\npublished by the New York Times between Jan-\nuary 1, 1987 and June 19, 2007 and summaries are\nwritten by library scientists. For the CNN/DM\ndataset, we follow the standard splits and pre-\nprocessing steps used in supervised summariza-\ntion3 (See et al., 2017; Liu and Lapata, 2019), and\n3scripts available at https://github.com/nlpyang/PreSumm\nthe resulting dataset contains 287,226 articles for\ntraining, 13,368 for validation and 11,490 for test.\nFollowing Zheng and Lapata (2019), we adopted\nthe splits widely used in abstractive summarization\n(Paulus et al., 2018) for the NYT dataset, which\nranks articles by their publication date and used the\nﬁrst 589,284 for training, the next 32,736 for vali-\ndation and the remaining 32,739 for test. Then, we\nﬁlter out documents whose summaries are shorter\nthan 50 words as in (Zheng and Lapata, 2019) and\nﬁnally retain 36,745 for training, 5,531 for valida-\ntion and 4,375 for test.\nWe segment sentences using the Stanford\nCoreNLP toolkit (Manning et al., 2014). Sentences\nare then tokenized with the UTF-8 based BPE tok-\nenizer used in RoBERTa and GPT-2 (Radford et al.,\n2019) and the resulting vocabulary contains 50,265\nsubwords. During training, we only leverage arti-\ncles in CNN/DM or NYT; while we do use both\narticles and summaries in validation sets to tune\nhyper-parameters of our models.\nWe evaluated the quality of summaries from dif-\nferent models using ROUGE (Lin, 2004). We re-\nport the full length F1 based ROUGE-1, ROUGE-2,\nROUGE-L on both CNN/DM and NYT datasets.\nThese ROUGE scores are computed using the\nROUGE-1.5.5.pl script4.\n4.2 Implementation Details\nThe main building blocks of STAS are Transform-\ners (Vaswani et al., 2017). In the following, we\ndescribe the sizes of them using the number of lay-\ners L, the number of attention heads A, and the\nhidden size N. As in (Vaswani et al., 2017; Devlin\net al., 2019), the hidden size of the feed-forward\nsublayer is always 4H. STAS contains one hierar-\nchical encoder (see Section 3.1) and two decoders,\nwhere they are used for the masked sentences pre-\ndiction and sentence shufﬂing pre-training tasks\n(see Section 3.2). The token-level encoder is ini-\ntialized with the parameters of RoBERTaBASE (Liu\net al., 2019)5 and we set L = 12,H = 768,A =\n12. The sentence-level encoder and the two de-\ncoders are shallower and we all adopt the setting\nL= 6,H = 768,A = 12.\nWe trained our models with 4 Nvidia Tesla V100\nGPUs and optimized them using Adam (Kingma\nand Ba, 2015) with β1 = 0.9,β2 = 0.999. Since\nthe encoder is partly pre-trained (initialized with\n4https://shorturl.at/nAG49\n5We also tried RoBERTaLARGE and obtained worse results.\n1790\nRoBERTa) and the decoders are initialized ran-\ndomly, we set a lager learning rate for decoders.\nSpeciﬁcally, we used 4e-5 for the encoder and\n4e-4 for the decoders. Since CNN/DM is larger\nthan NYT, we employed a batch size of 512 for\nCNN/DM and 64 for NYT (to ensure a sufﬁcient\nnumber of model updates)6. Limited by the posi-\ntional embedding of RoBERTa, all documents are\ntruncated to 512 subword tokens. We trained our\nmodels on both CNN/DM and NYT for 100 epochs.\nIt takes around one hour training on the CNN/DM\nand 30 minutes on the NYT for each epoch. The\nbest checkpoint is at around epoch 85 on CNN/DM\nand epoch 65 on NYT according to validation sets.\nWhen extracting the summary for a new docu-\nment during test time, we rank all sentences using\nEquation (14) and select the top-3 sentences as\nthe summary. When selecting sentences on the\nCNN/DM dataset, we ﬁnd that trigram blocking\n(i.e., removing sentences with repeating trigrams to\nexisting summary sentences) (Paulus et al., 2018)\ncan reduce the redundancy, while trigram blocking\ndoes not help on NYT.\n4.3 Results\nOur main results are shown in Table 1. The ﬁrst\nblock includes several recent supervised models\nfor document summarization. REFRESH (Narayan\net al., 2018) is an extractive model, which is trained\nby globally optimizing the ROUGE metric with re-\ninforcement learning. PTR-GEN (See et al., 2017)\nis a sequence to sequence based abstractive model\nwith copy and coverage mechanism. Liu and Lap-\nata (2019) initialize encoders of extractive model\n(BertSumExt) and abstractive model (BertSumAbs)\nwith pre-trained BERT.\nWe present the results of previous unsupervised\nmethods in the second block. LEAD-3 simply se-\nlects the ﬁrst three sentences as the summary for\neach document. TEXTRANK (Mihalcea and Tarau,\n2004) views a document as a graph with sentences\nas nodes and edge weights using the sentence simi-\nlarities. It selects top sentences as summary w.r.t.\nPageRank (Page et al., 1999) scores. PACSUM\n(Zheng and Lapata, 2019) is yet another graph-\nbased extractive model using BERT as sentence fea-\ntures. Sentences are ranked using centralities (sum\nof all out edge weights). They made the ranking\ncriterion positional sensitive by forcing negative\n6We used gradient accumulation technique (Ott et al.,\n2019) to increase the actual batch size.\nedge weights for edges between the current sen-\ntence and its preceding sentences. Adv-RF (Wang\nand Lee, 2018) and TED (Yang et al., 2020) are\nall based on unsupervised seq2seq auto-encoding\nwith additional objectives of adversarial training,\nreinforcement learning and seq2seq pre-training to\npredict leading sentences.\nPACSUM is based on the BERT (Devlin et al.,\n2019) initialization. RoBERTa (Liu et al., 2019),\nwhich extends BERT with better training strate-\ngies and more training data, outperforms BERT\non many tasks. We therefore re-implemented\nPACSUM and extended it with both BERT and\nRoBERTa initialization (i.e., PACSUM (BERT)\nand PACSUM (RoBERTa))7. On CNN/DM, our\nre-implementation PACSUM (BERT)is compara-\nble with Zheng and Lapata (2019). The results\nof PACSUM (BERT)and the RoBERTa initialized\nPACSUM (RoBERTa)are almost the same. Per-\nhaps because it relies more on position informa-\ntion rather than sentence similarities computed by\nBERT or RoBERTa. STAS outperforms all unsu-\npervised models in comparison on CNN/DM and\nthe difference between STAS and all other unsu-\npervised models are signiﬁcant with a 0.95 con-\nﬁdence interval according to the ROUGE script.\nIn the following, all signiﬁcant tests on ROUGE\nare measured with a 0.95 conﬁdence interval using\nthe ROUGE script. Since STAS does not model\nsentence positions explicitly during ranking, while\nPACSUM does, we linearly combine the ranking\nscores of STAS and PACSUM (i.e., STAS + PAC-\nSUM)8. The combination further improves the per-\nformance.\nOn NYT, the trend is similar. STAS is slightly\nbetter than PACSUM although not signiﬁcantly bet-\nter (STAS is siginiﬁcantly better than all the other\nunsupervised models in comparison). Interestingly,\nthere are also no signiﬁcant differences between\nSTAS and two supervised models (REFRESH and\nPTR-GEN). STAS + PACSUM even signiﬁcantly\noutperforms the supervised REFRESH. The signif-\nicant tests above all utilize the ROUGE script.\nExamples of gold summaries and system out-\nputs of REFRESH (Narayan et al., 2018), STAS\nand PACSUM (Zheng and Lapata, 2019) on the\n7We re-implemented PACSUM, because the training code\nof PACSUM is not available.\n8We ﬁrst normalize sentence scores in each document for\nboth STAS and PACSUM. In the combination, weight forSTAS\nis 0.9 and weight for PACSUM is 0.1 (tuned on validation\nsets).\n1791\nMethod CNN/DM NYT\nR-1 R-2 R-L R-1 R-2 R-L\nREFRESH (Narayan et al., 2018) 41.30 18.40 37.50 41.30 22.00 37.80\nPTR-GEN (See et al., 2017) 39.50 17.30 36.40 42.70 22.10 38.00\nBertSumExt (Liu and Lapata, 2019) 43.25 20.24 39.63 – – –\nBertSumAbs (Liu and Lapata, 2019) 41.72 19.39 38.76 – – –\nLEAD-3 40.50 17.70 36.70 35.50 17.20 32.00\nTEXTRANK (tf-idf) 33.20 11.80 29.60 33.20 13.10 29.00\nTEXTRANK (skip-thought) 31.40 10.20 28.20 30.10 9.60 26.10\nTEXTRANK (BERT) 30.80 9.60 27.40 29.70 9.00 25.30\nPACSUM (Zheng and Lapata, 2019) 40.70 17.80 36.90 41.40 21.70 37.50\nPACSUM (BERT) * 40.69 17.82 36.91 40.67 21.09 36.76\nPACSUM (RoBERTa) * 40.74 17.82 36.96 40.84 21.28 37.03\nAdv-RF (Wang and Lee, 2018) 35.51 9.38 20.98 – – –\nTED (Yang et al., 2020) 38.73 16.84 35.40 37.78 17.63 34.33\nSTAS 40.90 18.02 37.21 41.46 21.80 37.57\nSTAS + PACSUM 41.26 18.18 37.48 42.42 22.66 38.50\nTable 1: Results on CNN/DM and NYT test sets using ROUGE F1. * means our own re-implementation. Results\nof TEXTRANK (tf-idf), TEXTRANK (skip-thought) and TEXTRANK (BERT) are reported in (Zheng and Lapata,\n2019) and they use tf-idf, skip-thought vectors (Kiros et al., 2015) and BERT as sentence features, respectively.\nSettings valid set test set\nR-1 R-2 R-L R-1 R-2 R-L\nMSP 41.61 18.30 37.92 40.76 17.78 37.03\nMSP+SS (STAS) 41.67 18.47 38.00 40.90 18.02 37.21\n˜r = 1/|D| 41.58 18.43 37.89 40.74 17.88 37.04\nr′ = 0 33.92 12.93 30.99 33.30 12.61 30.33\nTable 2: Ablation study on CNN/DM validation and\ntest sets using ROUGE F1.\nCNN/DM dataset can be found in appendix B.\n4.4 Analysis\nAblation Study In Section 3.2, we proposed two\npre-training tasks. Are they are all useful for\nsummarization? As shown in Table 2, when we\nonly employ the masked sentences prediction task\n(MSP), we can obtain a ROUGE-2 of 17.73, which\nis already very close to the result of PACSUM (see\nTable 1). When we add the sentence shufﬂing task\n(denoted as MSP+SS (STAS)), we improves the per-\nformance over MSP. Note that we can not use only\nthe sentence shufﬂing task (SS), because the ﬁrst\nterm in our sentence scoring equation (see Equa-\ntion (14)) depends on the probabilities produced by\ndecoder in the MSP task.\nIn section 3.3, we propose two criteria to score\nsentences (see the two terms in Equation (14)). The\neffects of them are shown in the second block of\nTable 2. Since the attention based criterion r′re-\nlies on sentence probability based criterion ˜r, we\ncannot remove ˜rand instead we set ˜r= 1\nD to see\nthe effect of ˜r. As a result, ROUGE-2 decreases by\n0.14, which indicates that˜ris necessary for ranking.\nAlso note that when setting ˜r= 1\nD, our method is\nvalid set test set\nR-1 R-2 R-L R-1 R-2 R-L\nw/ Ai,j 33.66 12.78 30.75 33.02 12.48 30.08\nw/ Aj,i 41.67 18.47 38.00 40.90 18.02 37.21\nTable 3: Aj,i v.s. Ai,j on CNN/DM validation and test\nsets using ROUGE F1.\nequivalent to PageRank using sentence level atten-\ntion scores as edge weights. Instead of iterating\nuntil convergence as in the original PageRank al-\ngorithm, we ﬁnd a small iteration number (T ≤3)\nis sufﬁcient. To study the effect of the attention\nbased criterion r′, we set r′= 0, which means sen-\ntences are ranked using sentence probability based\ncriterion ˜r. We can see that the performance drops\ndramatically by 5 ROUGE-2.\nAj,i v.s. Ai,j In Equation (13), we compute r′\ni\nwith Aj,i (attention score from Sj to Si). The intu-\nition of using Aj,i is that a sentence is important\nif the interpretation of other important sentences\ndepends on it. However, an alternative is to use\nAi,j. It shows in Table 3 that Aj,i is indeed better.\nSentence Position Distribution We also ana-\nlyze how extractive sentences by different models\nare distributed in documents? We compare STAS\nagainst LEAD-3, PACSUM and ORACLE using\nthe ﬁrst 12 sentences in all documents on CNN/DM\ntest set. ORACLE is the upper bound for extractive\nmodels. Extractive summaries of ORACLE are\ngenerated by selecting a subset of sentences in a\ndocument, which maximize ROUGE score (Nalla-\npati et al., 2017). As shown in Figure 4, we can see\n1792\nFigure 4: Proportion of extracted sentences by different\nunsupervised models against their positions.\nthat sentences selected by ORACLE are smoothly\ndistributed across all positions, while LEAD-3 only\nselects the ﬁrst 3 sentences. Compared to STAS, the\nsentence distribution of PACSUM is closer to that\nof LEAD-3 and STAS produces a sentence distribu-\ntion that is more similar to that of ORACLE. The\nobservation above indicates that our model relies\nless on sentence positions compared to PACSUM.\nWe further computed the Kullback Leibler diver-\ngence between the sentence position distribution of\nan unsupervised model and the distribution of OR-\nACLE and we denote it asKL(·||ORC). We found\nKL(PACSUM||ORC) = 0.614 is much large than\nKL(STAS||ORC) = 0.098, indicating STAS is bet-\nter correlated with ORACLE. We introduce the\nsentence shufﬂing task to encourage STAS to se-\nlect sentences based on their contents rather than\ntheir positions only (see Section 3.2). After we\nremove the sentence shufﬂing task from STAS dur-\ning pre-training (see MSP in Figure 4), there is\na clear trend that leading sentences are more fre-\nquently selected. Moreover, KL(STAS||ORC) <\nKL(MSP||ORC) = 0.108. By introducing the\nsentence shufﬂe task, sentence positional distribu-\ntion of STAS is closer to that of ORACLE.\nvalid set test set\nR-1 R-2 R-L R-1 R-2 R-L\nMSP 41.61 18.30 37.92 40.76 17.78 37.03\nMSP+(a) 40.93 17.73 37.27 40.15 17.25 36.46\nMSP+(b) 37.59 14.71 33.95 36.91 14.26 33.25\nMSP+SS 41.67 18.47 38.00 40.90 18.02 37.21\nTable 4: Compare SS with other two different methods\nwhich remove sentence positions. (a) remove the sen-\ntence level positional embedding; (b) for each sentence\nin the token level, use a positional embedding from po-\nsitional 0. Results are reported on CNN/DM.\nWhy Sentence Shufﬂing? Since the Sentences\nShufﬂing task aims to make STAS less dependent\non sentence positions. However, there are poten-\ntially simpler methods to remove sentence posi-\ntion information. For example, (a) we can remove\nthe sentence-level positional embedding and (b)\nfor each sentence in the token level, we can use a\npositional embedding from positional 0. Results\nin Table 4 indicates that upon the MSP objective,\nstrategies (a) and (b) hurt the performance of MSP\nsigniﬁcant, while SS improves over MSP. It may\nbecause positional embeddings, whether on token\nor sentence level, are important (at least for the\nMSP task). One advantage of SS over (a) and (b)\nis that it can make our model less dependent on\npositions (see Section 4.4) and retain the power of\npositional embeddings and the MSP objective at\nthe same time.\n5 Conclusions\nIn this paper, we ﬁnd that (sentence-level) trans-\nformer attention (in a hierarchical transformer) can\nbe used to rank sentences for unsupervised ex-\ntractive summarization, while previous work lever-\nage graph based (or rule based) methods and sen-\ntence similarities computed with off-the-shelf sen-\ntence embeddings. We propose the sentence shuf-\nﬂing task for pre-training hierarchical transformers,\nwhich helps our model to select sentences based on\ntheir contents rather than their positions only. Ex-\nperimental results on CNN/DM and NYT datasets\nshow that our model outperforms other recently\nproposed unsupervised methods. The sentence po-\nsition distribution analysis shows that our method is\nless dependent on sentence positions. When com-\nbined with recent unsupervised model explicitly\nmodeling sentence positions, we obtain even better\nresults. In the next step, we plan to apply our mod-\nels to unsupervised abstractive summarization.\nAcknowledgments\nWe gratefully acknowledge Hao Zheng for the\ntechnical advice during our re-implementation of\nPACSUM (Zheng and Lapata, 2019). We would\nalso like to thank the anonymous reviewers for their\ninsightful feedback.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\n1793\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015.\nChristos Baziotis, Ion Androutsopoulos, Ioannis\nKonstas, and Alexandros Potamianos. 2019. SEQˆ3:\nDifferentiable sequence-to-sequence-to-sequence\nautoencoder for unsupervised abstractive sentence\ncompression. In Proceedings of the 2019 Con-\nference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 673–681, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nJaime Carbonell and Jade Goldstein. 1998. The use of\nmmr, diversity-based reranking for reordering doc-\numents and producing summaries. In Proceedings\nof the 21st annual international ACM SIGIR confer-\nence on Research and development in information\nretrieval, pages 335–336.\nJianpeng Cheng and Mirella Lapata. 2016. Neural sum-\nmarization by extracting sentences and words. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 484–494, Berlin, Germany. As-\nsociation for Computational Linguistics.\nEric Chu and Peter J. Liu. 2019. Meansum: A neural\nmodel for unsupervised multi-document abstractive\nsummarization. In ICML.\nJohn M Conroy and Dianne P O’leary. 2001. Text sum-\nmarization via hidden markov models. In Proceed-\nings of the 24th annual international ACM SIGIR\nconference on Research and development in infor-\nmation retrieval, pages 406–407.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems, pages 13042–13054.\nG¨unes Erkan and Dragomir R Radev. 2004. Lexrank:\nGraph-based lexical centrality as salience in text\nsummarization. Journal of artiﬁcial intelligence re-\nsearch, 22:457–479.\nThibault Fevry and Jason Phang. 2018. Unsuper-\nvised sentence compression using denoising auto-\nencoders. In Proceedings of the 22nd Conference on\nComputational Natural Language Learning , pages\n413–422, Brussels, Belgium. Association for Com-\nputational Linguistics.\nElena Filatova and Vasileios Hatzivassiloglou. 2004.\nEvent-based extractive summarization. In Text\nSummarization Branches Out , pages 104–111,\nBarcelona, Spain. Association for Computational\nLinguistics.\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summarization.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 4098–4109, Brussels, Belgium. Association\nfor Computational Linguistics.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.\nLi. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. In Proceedings of\nthe 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1631–1640, Berlin, Germany. Association for\nComputational Linguistics.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in neural information\nprocessing systems, pages 1693–1701.\nTsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,\nNorihito Yasuda, and Masaaki Nagata. 2013. Single-\ndocument summarization as a tree knapsack prob-\nlem. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1515–1520, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nDan Iter, Kelvin Guu, Larry Lansing, and Dan Jurafsky.\n2020. Pretraining with contrastive sentence objec-\ntives improves discourse performance of language\nmodels. arXiv preprint arXiv:2005.10389.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nRyan Kiros, Yukun Zhu, Russ R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems ,\npages 3294–3302.\n1794\nJulian Kupiec, Jan Pedersen, and Francine Chen. 1995.\nA trainable document summarizer. In Proceedings\nof the 18th annual international ACM SIGIR confer-\nence on Research and development in information\nretrieval, pages 68–73.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nChin-Yew Lin and Eduard Hovy. 2002. From single to\nmulti-document summarization. In Proceedings of\nthe 40th annual meeting of the association for com-\nputational linguistics, pages 457–464.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven J. Bethard, and David Mc-\nClosky. 2014. The Stanford CoreNLP natural lan-\nguage processing toolkit. In Association for Compu-\ntational Linguistics (ACL) System Demonstrations ,\npages 55–60.\nRada Mihalcea and Paul Tarau. 2004. Textrank: Bring-\ning order into text. In Proceedings of the 2004 con-\nference on empirical methods in natural language\nprocessing, pages 404–411.\nTomas Mikolov, Kai Chen, Gregory S. Corrado, and\nJeffrey Dean. 2013. Efﬁcient estimation of word rep-\nresentations in vector space. CoRR, abs/1301.3781.\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.\nSummarunner: A recurrent neural network based se-\nquence model for extractive summarization of docu-\nments. In Thirty-First AAAI Conference on Artiﬁcial\nIntelligence.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Ranking sentences for extractive summariza-\ntion with reinforcement learning. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1747–1759, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nAni Nenkova and Kathleen McKeown. 2011. Auto-\nmatic summarization. Foundations and Trends in\nInformation Retrieval, 5(2–3):103–233.\nAni Nenkova, Lucy Vanderwende, and Kathleen McKe-\nown. 2006. A compositional context sensitive multi-\ndocument summarizer: exploring the factors that in-\nﬂuence summarization. In Proceedings of the 29th\nannual international ACM SIGIR conference on Re-\nsearch and development in information retrieval ,\npages 573–580.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nLarry Page, Sergey Brin, Rajeev Motwani, and Terry\nWinograd. 1997. Pagerank: Bringing order to the\nweb. Technical report, Stanford Digital Libraries\nWorking Paper.\nLawrence Page, Sergey Brin, Rajeev Motwani, and\nTerry Winograd. 1999. The pagerank citation rank-\ning: Bringing order to the web. Technical report,\nStanford InfoLab.\nDaraksha Parveen, Hans-Martin Ramsl, and Michael\nStrube. 2015. Topical coherence for graph-based ex-\ntractive summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1949–1954, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2018. A deep reinforced model for abstractive sum-\nmarization. In International Conference on Learn-\ning Representations.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nDragomir R Radev, Timothy Allison, Sasha Blair-\nGoldensohn, John Blitzer, Arda Celebi, Stanko\nDimitrov, Elliott Drabek, Ali Hakim, Wai Lam,\nDanyu Liu, et al. 2004. Mead-a platform for mul-\ntidocument multilingual text summarization.(2004).\nLREC, Lisbon, Portugal.\n1795\nDragomir R. Radev, Hongyan Jing, and Malgorzata\nBudzikowska. 2000. Centroid-based summarization\nof multiple documents: sentence extraction, utility-\nbased evaluation, and user studies. InNAACL-ANLP\n2000 Workshop: Automatic Summarization.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nEvan Sandhaus. 2008. The new york times annotated\ncorpus. Linguistic Data Consortium, Philadelphia ,\n6(12):e26752.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Advances in neural in-\nformation processing systems, pages 2692–2700.\nXiaojun Wan. 2008. An exploration of document im-\npact on graph-based multi-document summarization.\nIn Proceedings of the 2008 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n755–762, Honolulu, Hawaii. Association for Com-\nputational Linguistics.\nXiaojun Wan and Jianwu Yang. 2008. Multi-document\nsummarization using cluster-based link analysis. In\nProceedings of the 31st annual international ACM\nSIGIR conference on Research and development in\ninformation retrieval, pages 299–306.\nYaushian Wang and Hung-Yi Lee. 2018. Learning\nto encode text as human-readable summaries using\ngenerative adversarial networks. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4187–4195, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nZi-Yi Yang, Chenguang Zhu, Robert Gmyr, Michael\nZeng, and Xuedong Huang. 2020. Ted: A pretrained\nunsupervised summarization model with theme\nmodeling and denoising. ArXiv, abs/2001.00725.\nXingxing Zhang, Mirella Lapata, Furu Wei, and Ming\nZhou. 2018. Neural latent extractive document sum-\nmarization. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 779–784, Brussels, Belgium. Association\nfor Computational Linguistics.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. HI-\nBERT: Document level pre-training of hierarchical\nbidirectional transformers for document summariza-\ntion. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 5059–5069, Florence, Italy. Association for\nComputational Linguistics.\nHao Zheng and Mirella Lapata. 2019. Sentence cen-\ntrality revisited for unsupervised summarization. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n6236–6247, Florence, Italy. Association for Compu-\ntational Linguistics.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9270676970481873
    },
    {
      "name": "Computer science",
      "score": 0.8196983337402344
    },
    {
      "name": "Transformer",
      "score": 0.7778794765472412
    },
    {
      "name": "Sentence",
      "score": 0.7365785837173462
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6823742985725403
    },
    {
      "name": "Natural language processing",
      "score": 0.5352122783660889
    },
    {
      "name": "Unsupervised learning",
      "score": 0.4811916947364807
    },
    {
      "name": "Graph",
      "score": 0.42022499442100525
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33383867144584656
    },
    {
      "name": "Machine learning",
      "score": 0.3211285173892975
    },
    {
      "name": "Theoretical computer science",
      "score": 0.05891790986061096
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}