{
  "title": "Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT",
  "url": "https://openalex.org/W4389523939",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2111410033",
      "name": "Xiaoshuai Song",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2102858772",
      "name": "Keqing He",
      "affiliations": [
        "Meizu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2102797220",
      "name": "Pei Wang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2891502129",
      "name": "Guanting Dong",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A3126627398",
      "name": "Yutao Mou",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2124326937",
      "name": "Jingang Wang",
      "affiliations": [
        "Meizu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A4367539844",
      "name": "Yunsen Xian",
      "affiliations": [
        "Meizu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A3154473620",
      "name": "Xunliang Cai",
      "affiliations": [
        "Meizu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2099469527",
      "name": "Weiran Xu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3045492832",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4375869176",
    "https://openalex.org/W4382652828",
    "https://openalex.org/W4318899036",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W4387427670",
    "https://openalex.org/W3205597769",
    "https://openalex.org/W2950180292",
    "https://openalex.org/W4292692470",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4385573210",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4387428070",
    "https://openalex.org/W2917128112",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4327525855",
    "https://openalex.org/W4368304377",
    "https://openalex.org/W4285275136",
    "https://openalex.org/W4386977634",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4386265460",
    "https://openalex.org/W4385572841",
    "https://openalex.org/W4295884370",
    "https://openalex.org/W4293167432",
    "https://openalex.org/W2986193249",
    "https://openalex.org/W4321854923",
    "https://openalex.org/W2127218421",
    "https://openalex.org/W2998721586"
  ],
  "abstract": "Xiaoshuai Song, Keqing He, Pei Wang, Guanting Dong, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10291‚Äì10304\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nLarge Language Models Meet Open-World Intent Discovery and\nRecognition: An Evaluation of ChatGPT\nXiaoshuai Song1‚àó, Keqing He2‚àó, Pei Wang1, Guanting Dong1, Yutao Mou1\nJingang Wang2, Yunsen Xian2, Xunliang Cai2,Weiran Xu1‚àó\n1Beijing University of Posts and Telecommunications, Beijing, China\n2Meituan, Beijing, China\n{songxiaoshuai,wangpei,dongguanting,myt,xuweiran}@bupt.edu.cn\n{hekeqing,wangjingang,xianyunsen,caixunliang}@meituan.com\nAbstract\nThe tasks of out-of-domain (OOD) intent dis-\ncovery and generalized intent discovery (GID)\naim to extend a closed intent classifier to open-\nworld intent sets, which is crucial to task-\noriented dialogue (TOD) systems. Previous\nmethods address them by fine-tuning discrimi-\nnative models. Recently, although some studies\nhave been exploring the application of large lan-\nguage models (LLMs) represented by ChatGPT\nto various downstream tasks, it is still unclear\nfor the ability of ChatGPT to discover and incre-\nmentally extent OOD intents. In this paper, we\ncomprehensively evaluate ChatGPT on OOD\nintent discovery and GID, and then outline the\nstrengths and weaknesses of ChatGPT. Overall,\nChatGPT exhibits consistent advantages under\nzero-shot settings, but is still at a disadvantage\ncompared to fine-tuned models. More deeply,\nthrough a series of analytical experiments, we\nsummarize and discuss the challenges faced by\nLLMs including clustering, domain-specific un-\nderstanding, and cross-domain in-context learn-\ning scenarios. Finally, we provide empirical\nguidance for future directions to address these\nchallenges.1\n1 Introduction\nTraditional task-oriented dialogue (TOD) systems\nare based on the closed-set hypothesis (Chen et al.,\n2019; Yang et al., 2021; Zeng et al., 2022) and\ncan only handle queries within a limited scope of\nin-domain (IND) intents. However, users may in-\nput queries with out-of-domain (OOD) intents in\nthe real open world, which poses new challenges\nfor TOD systems. Recently, a series of tasks tar-\ngeting OOD queries have received extensive re-\nsearch. OOD intent discovery (Lin et al., 2020;\nZhang et al., 2021; Mou et al., 2022a) aims to\ngroup OOD queries into different clusters based\n‚àóThe first two authors contribute equally. Weiran Xu is\nthe corresponding author.\n1We release our code at https://github.com/\nsongxiaoshuai/OOD-Evaluation\nOOD Intent\nDiscovery\nGeneraled Intent\nDiscovery\nJoint ClassifierOOD Intent Clusters\nIND Labelled\nSamples\nOOD Unlabelled\nSamples\nFigure 1: Illustration of OOD Intent Discovery and GID.\non their intents, which facilitates identifying po-\ntential directions and developing new skills. The\nGeneral Intent Discovery (GID) task (Mou et al.,\n2022b) further considers the increment of OOD\nintents and aims to automatically discover and in-\ncrement the intent classifier, thereby extending the\nscope of recognition from existing IND intent set\nto the open world, as shown in Fig 1.\nPrevious work studied above OOD tasks by fine-\ntuning the discriminative pre-training model BERT\n(Devlin et al., 2018). Recently, a series of pow-\nerful generative LLMs have been proposed one\nafter another, such as GPT-3 (Brown et al., 2020),\nPaLM (Chowdhery et al., 2022) and LLaMA (Tou-\nvron et al., 2023). The emergence of LLMs has\nbrought revolutionary changes to the field of natu-\nral language processing (NLP). Given the superior\nin-context learning ability, prompting LLMs has\nbecome a widely adopted paradigm for NLP re-\nsearch and applications (Dong et al., 2022b). Since\nthese LLMs are trained on a large amount of gen-\neral text corpus and have excellent generalization\nability, this triggers the thinking of what benefits\nLLMs can bring and what challenges will LLMs\nface when applying them to open-scenario intent\ndiscovery and recognition?\nAs one of the representative LLMs, ChatGPT,\ndeveloped by OpenAI, has attracted significant at-\ntention from researchers and practitioners in a short\nperiod of time. While the NLP community has\n10291\nbeen studying the ability of LLMs to be applied to\nvarious downstream tasks, such as translation (Jiao\net al., 2023), mathematics (Frieder et al., 2023),\neducation (Malinka et al., 2023),emotion recogni-\ntion(Lei et al., 2023), its ability in OOD is still not\nfully explored. Different from Wang et al. (2023)\nwhich evaluate the robustness of ChatGPT from\nthe adversarial and out-of-distribution perspective,\nOOD intent discovery and GID focuses on using\nIND knowledge transfer to help improve TOD sys-\ntems with OOD data. In this paper, on the one hand,\nwe focus on OOD intent discovery and GID the\ntwo open-scenario intent tasks to explore whether\nprompting ChatGPT can achieve good performance\nin discovering and incrementally recognizing OOD\nintents; on the other hand, we aim to gain insights\ninto the challenges faced by LLMs in handling\nopen-domain tasks and potential directions for im-\nprovement.\nTo the best of our knowledge, we are the first to\ncomprehensively evaluate ChatGPT‚Äôs performance\non OOD intent discovery and GID. In detail, we\nfirst design three prompt-based methods based on\ndifferent IND prior to guide ChatGPT to perform\nOOD discovery in an end-to-end manner. For GID,\nwe innovatively propose a pipeline framework for\nperforming GID task under a generative LLM (Sec-\ntion 3). Then we conduct detailed comparative\nexperiments between ChatGPT and representative\nbaselines under three dataset partitions (Section 4).\nIn order to further explore the underlying reasons\nbehind the experiments, we conduct a series of an-\nalytical experiments, including in-context learning\nunder cross-domain demonstrations, recall analysis,\nand factors that affect the performance of ChatGPT\non OOD discovery and GID. Finally, we compare\nthe performance of different LLMs on these OOD\ntasks (Section 5).\nOur findings. The major findings of the study in-\nclude:\nWhat ChatGPT does well:\n‚Ä¢ ChatGPT can perform far better than the non-\nfine-tuned BERT on OOD tasks without any\nIND prior, thanks to its powerful semantic\nunderstanding ability.\n‚Ä¢ For OOD intent discovery, when there are\nfew samples for clustering, ChatGPT‚Äôs perfor-\nmance can rival that of fine-tuned baselines.\n‚Ä¢ ChatGPT can simultaneously perform text\nclustering and induce the intent of each clus-\nter, which is not available in the discriminant\nmodel.\nWhat ChatGPT does not do well:\n‚Ä¢ For OOD intent discovery, ChatGPT performs\nfar worse than the fine-tuned baselines under\nmulti-sample or multi-category scenes, and is\nseverely affected by the number of clusters\nand samples, with poor robustness.\n‚Ä¢ For GID, the overall performance of ChatGPT\nis inferior to that of the fine-tuned baselines.\nThe main reason is the lack of domain knowl-\nedge, and the secondary reason is the quality\nof the pseudo-intent set.\n‚Ä¢ There are obvious recall errors in both OOD\ndiscovery and GID. In OOD discovery, this\nis mainly due to the generative architecture\nof ChatGPT. In GID, recall errors are mainly\ncaused by ChatGPT‚Äôs lack of domain knowl-\nedge and unclear understanding of intent set\nboundaries.\n‚Ä¢ ChatGPT can hardly learn knowledge from\nIND demonstrations that helps OOD tasks and\nmay treat IND demonstrations as noise, which\nbrings negative effects to OOD tasks.\nIn addition to the above findings, we further sum-\nmarize and discuss the challenging scenarios faced\nby LLMs including large scale clustering, seman-\ntic understanding of specific domain and cross-\ndomain in-context learning in Section 6 as well\nas provide guidance for future directions.\n2 Related Work\n2.1 Large Language Models\nRecently, there are growing interest in leveraging\nlarge language models (LLMs) to perform various\nNLP tasks, especially in evaluating ChatGPT in\nvarious aspects. For example, Frieder et al. (2023)\ninvestigate the mathematical capabilities of Chat-\nGPT by testing it on publicly available datasets,\nas well as hand-crafted ones. Tan et al. (2023) ex-\nplore the performance of ChatGPT on knowledge-\nbased question answering (KBQA). Wang et al.\n(2023) evaluate the robustness of ChatGPT from\nthe out-of-distribution (OOD) perspective. A se-\nries works by Liu et al. (2023); Guo et al. (2023);\nDong et al. (2022a, 2023) explore the impact of\ninput perturbation problems on model performance\n10292\nNext, I will first give you a set of sentences , which will be recorded as Set 1. First,Please classify the sentences in Set 1 into 5 (Number of Clusters)\ncategories according to their intentions. You only need to output the category number and the corresponding sentence number i n the following format:\nCategory 1: 1,2,3,4,5 ‚Ä¶‚Ä¶\nSet 1(Dùë°ùëíùë†ùë°\nùëÇùëÇùê∑}): 1. can I choose a date for delivery? ‚Ä¶‚Ä¶\nNext, I will first give you a set of intent categories, which will be recorded as Set 1. Then I will give you another set of sentences without intention labels, \nrecorded as Set 2. You first need to learn the knowledge in Set 1, and then use the learned knowledge to classify the sentenc es in Set 2 into 5 (Number of Clusters) \ncategories according to their intentions. You only need to output the category number and the corresponding sentence number i n the following format:\nCategory 1Ôºö1,2,3,4,5 ‚Ä¶‚Ä¶\nIt should be noted that the intention in set 1 and the intention in set 2 do not overlap.\nSet 1(ùëåùêºùëÅùê∑): pin_blocked; ‚Ä¶‚Ä¶\nSet 2(Dùë°ùëíùë†ùë°\nùëÇùëÇùê∑): 1. can I choose a date for delivery? ‚Ä¶‚Ä¶\nNext, I will first give you a set of sentences with intention labels, which will be recorded as Set 1.Then I will give you another set of sentences without intention labels, \nrecorded as Set 2. You first need to learn the knowledge in Set 1, and then use the learned knowledge to classify the sentenc es in Set 2 into 5 (Number of Clusters) \ncategories according to their intentions. You only need to output the category number and the corresponding sentence number i n the following format:\nCategory 1Ôºö1,2,3,4,5 ‚Ä¶‚Ä¶\nIt should be noted that the intention in set 1 and the intention in set 2 do not overlap.\nSet 1 (Dùëëùëíùëöùëú\nùêºùëÅùê∑ ): How do I unblock my card?     intention:pin_blocked; ‚Ä¶‚Ä¶\nSet 2 (Dùë°ùëíùë†ùë°\nùëÇùëÇùê∑): 1. can I choose a date for delivery? ‚Ä¶‚Ä¶\nDirect clustering (DC)\nZero Shot Discovery (ZSD)\nFew Shot Discovery (FSD)\nFigure 2: The different prompts for three methods of OOD intent discovery.\nfrom small models to LLMs. In this paper, we aim\nto investigate the ability of ChatGPT to discover\nand increment OOD intents and further explore the\nchallenges faced by LLMs and potential directions\nfor improvement.\n2.2 OOD Intent Discovery\nUnlike the simple text clustering task, OOD intent\ndiscovery considers how to facilitate the discovery\nof unknown OOD intents using prior knowledge of\nIND intents . Lin et al. (2020) use OOD represen-\ntations to compute similarities as weak supervision\nsignals. Zhang et al. (2021) propose an iterative\nmethod, DeepAligned, that performs representation\nlearning and clustering assignment iteratively while\nMou et al. (2022c) perform contrastive clustering to\njointly learn representations and clustering assign-\nments. In this paper, we evaluate the performance\nof methods based ChatGPT about OOD discovery\nand provide a detailed qualitative analysis.\n2.3 General Intent Discovery\nSince OOD intent discovery ignores the fusion of\nIND and OOD intents, it cannot further expand\nthe recognition range of existing TOD systems. In-\nspired by the above problem, Mou et al. (2022b)\npropose the General Intent Discovery (GID) task,\nwhich requires the system to discover semantic\nconcepts from unlabeld OOD data and then jointly\nclassifying IND and OOD intents automatically.\nFurthermore, Mou et al. (2022b) proposes two\nframeworks for performing GID task under dis-\ncriminative models: pipeline-based and end-to-end\nframeworks. In this paper, we propose a new GID\npipeline under generative LLMs and explore the\nperformance of ChatGPT in different scenarios.\n3 Methodology\n3.1 Problem Formulation\nOOD Intent Discovery Given a set of labeled\nIND dataset DIND = {(xIND\ni , yIND\ni )}n\ni=1 and\nunlabeled OOD dataset DOOD = {(xOOD\ni )}m\ni=1,\nwhere all queries from DIND belong to a prede-\nfined intent set Y IND containing N intents, and\nall queries from DOOD belong to an unknown set\nY OOD containing M intents2. OOD intent discov-\nery aims to cluster M OOD groups from DOOD\nunder the transfer of IND prior from DIND .\nGeneral Intent Discovery GID aims to train a\nnetwork that can simultaneously classify a set of\nlabeled IND intent classes Y IND containing N\nintents and discover new intent set Y OOD con-\ntaining M intents from an unlabeled OOD set\nDOOD = {(xOOD\ni )}m\ni=1. Unlike OOD discov-\nery clustering that obtains M OOD groups, the\nultimate goal of GID is to expand the network‚Äôs\nclassification capability of intent query to the total\nlabel set Y = Y IND ‚à™ Y OOD containing N + M\nintents.\n3.2 ChatGPT for OOD Discovery\nWe evaluate the performance of ChatGPT on OOD\nintent discovery by designing prompts that include\ntask instructions, test samples, and IND prior. We\nheuristically propose the following three methods\nbased on different IND prior:\nDirect clustering (DC): Since OOD intent dis-\ncovery is essentially a clustering task, a naive\napproach is to cluster directly without utilizing\nany IND prior. The prompt is in the following\n2Estimating M is out of the scope of this paper. In the\nfollowing experiment, we assume that M is ground-truth and\nprovide an analysis in Section 5.5. It should be noted that the\nspecific semantics of intents in Y OODare unknown.\n10293\nStep1. Obtain OOD pseudo-intents of clusters\nStep 1. Obtain OOD pseudo-label index of samples\nStep2. Joint Classification\nLLMsUnlabeled\nOOD Samples\nBERTUnlabeled\nOOD Samples Joint\nclassifier\nPseudo Labelled\nOOD Samples\nLabeled\nIND Samples\nStep2. Joint Classification\n{OOD Pseudo-Intents}\nPrompt\n{IND Intents}\n{OOD Pseudo-Intents}\n User Query\nfine-tune(FT)\nFT\nPr evious : \nNew: \nLLMs\nFigure 3: Comparison of the discriminant and generative\nGID framework.\nformat: <Cluster Instruction><Number of Clus-\nters><Response Format><DOOD\ntest >.\nZero Shot Discovery (ZSD): This method\nprovides the IND intent set in the prompt as\nprior knowledge, but does not provide any\nIND samples. It can be used in scenarios\nwhere user privacy needs to be protected. The\nprompt is in the following format: <Prior:\nY IND ><Cluster Instruction><Number of Clus-\nters><Response Format><DOOD\ntest >.\nFew Shot Discovery (FSD): FSD provides sev-\neral labelled samples for each IND intent in the\nprompt, hoping that ChatGPT can mine domain\nknowledge from IND demonstration and transfer\nit to assist OOD intent clustering. The prompt\nis in the following format: <Prior: DIND\ndemo &\nY IND ><Cluster Instruction><Number of Clus-\nters><Response Format><DOOD\ntest >.\nAccording to the input, ChatGPT outputs the\nindex of OOD samples contained in each cluster, in\nthe form of <Cluster Index><OOD Sample Index>.\nWe show the prompts of these methods in Fig 2.\n3.3 ChatGPT for GID\nPrevious discriminative GID framework first assign\na pseudo-label index to each OOD sample through\nclustering and then jointly training the classifier\nwith labeled IND data. However, the classification\nof queries by generative LLMs depends on specific\nintent semantics rather than abstract pseudo-label\nindex symbols. Based on this, we innovatively\npropose a new framework that is suitable for gen-\nerative LLMs, which relies on LLMs to generate\nan intent description with specific semantics as the\npseudo-intent for each cluster, as shown in Fig 3.\nIn the first stage, on the basis of OOD intent\ndiscovery prompts, we add an additional instruc-\ntion for generating intent descriptions, which are\nformally <OOD Discovery Prompt><Intent De-\nscribe Instruction>, input to ChatGPT, and obtain\nthe intent description of each cluster. By aggregat-\ning these intent descriptions , we obtain the OOD\npseudo-intent set Y OOD\npseudo. Then, we incrementally\nadd the pseudo-intent set to the existing IND intent\nset, i.e., Y joint = Y IND ‚à™ Y OOD\npseudo.\nNext, we input prompts for joint IND and OOD\nintent classification in the following form: <Classi-\nfication Instruction><Intent Set: Y joint >< x|x ‚àà\nDIND\ntest ‚à™ DOOD\ntest >. According to the three differ-\nent OOD discovery methods in Section 3.2, there\nare also three GID methods for ChatGPT: GID\nunder Direct clustering (GID-DC), GID under\nZero Shot Discovery (GID-ZSD), and GID un-\nder Few Shot Discovery (GID-FSD). It should be\nnoted that in the GID-FSD method, we provide few\nlabeled samples of IND as demonstrations. The\ncomplete prompts are provided in Appendix D.\n4 Experiment\n4.1 Datasets\nWe conduct experiments on the widely used intent\ndataset Banking (Casanueva et al., 2020). Bank-\ning contains 13,083 user queries with 77 intents\nin the banking domain. Due to the length limi-\ntation of ChatGPT‚Äôs conversations, we randomly\nsample 15 categories from Banking as IND intents\nand considered three OOD category quantity set-\ntings. Specifically, the OOD category quantity is\n5 (IND/OOD=3:1), 10 (IND/OOD=3:2), and 15\n(IND/OOD=1:1), respectively. For OOD intent dis-\ncovery, we randomly sample 5 queries from the\ntest set for each OOD class, where the ground-truth\nnumber of OOD classes is given as a prior. For\nGID, we randomly sample 10 queries from the test\nset for testing. In addition, only when using FSD or\nGID-FSD method, we randomly sample 3 queries\nfor each IND class from the training set for demon-\nstration. We provide detailed statistics of datasets\nin Appendix A.\n4.2 Baselines\nFor OOD intent discovery, we choose to use BERT\ndirectly for k-means clustering (MacQueen, 1965)\nand two representative fine-tuned methods:\n‚Ä¢ DeepAligned (Zhang et al., 2021) is an im-\nproved version of DeepCluster(Caron et al.,\n2018). It designed a pseudo label alignment\n10294\nPrior Method IND/OOD=3:1 IND/OOD=3:2 IND/OOD=1:1\nACC NMI ARI ACC NMI ARI ACC NMI ARI\nW/O IND PriorBERT 52.00 41.58 15.36 36.00 42.33 5.616 29.33 45.26 2.499\nChatGPT(DC) 88.00 84.62 73.36 78.00 78.20 55.32 58.22 65.30 28.21\nWith IND Prior\nDeepAligned 100.0 100.0 100.0 78.67 82.18 61.01 74.67 80.83 55.35\nDKT 93.33 91.71 84.82 80.67 82.59 64.53 76.45 83.23 60.92\nChatGPT(ZSD)92.00 87.25 80.16 67.33 68.72 39.32 50.67 60.73 21.25\nChatGPT(FSD)74.67 64.77 45.92 56.67 63.56 31.47 49.78 60.98 20.72\nTable 1: Performance comparison on OOD intent discovery. Results are averaged over three random run (p < 0.01\nunder t-test).\nMethod IND/OOD=3:1 IND/OOD=3:2 IND/OOD=1:1IND OOD ALL IND OOD ALL IND OOD ALLF1 ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 ACCDeepAligned-GID95.36 94.5097.00 97.0095.77 95.1294.49 92.6785.99 86.0091.09 90.0094.16 91.5076.38 77.3385.27 84.42E2E 96.13 95.5097.00 97.0096.35 95.8895.21 93.3378.69 80.588.602 88.294.22 92.1771.92 74.0083.07 83.08ChatGPT(GID-DC)67.41 68.4470.26 75.3367.96 70.1762.15 64.6757.53 61.3360.30 63.3363.06 66.4459.72 62.0061.39 64.22ChatGPT(GID-ZSD)64.47 65.1161.50 70.0063.73 66.3355.14 58.2246.83 50.3351.81 55.0753.94 57.7852.20 57.5753.07 57.67ChatGPT(GID-FSD)72.77 79.1120.27 17.3359.65 63.6768.74 74.8950.75 52.0061.54 65.7368.29 74.8952.57 51.7860.43 63.33\nTable 2: Performance comparison on GID.\nstrategy to produce aligned cluster assign-\nments for better representation learning.\n‚Ä¢ DKT (Mou et al., 2022c) designs a unified\nmulti-head contrastive learning framework to\nmatch the IND pretraining objectives and the\nOOD clustering objectives. In the IND pre-\ntraining stage, the CE and SCL objective func-\ntions are jointly optimized, and in the OOD\nclustering stage, instance-level CL and cluster-\nlevel CL objectives are used to jointly learn\nrepresentation and cluster assignment.\nFor GID, the baselines are as follows:\n‚Ä¢ DeepAligned-GID is a representative\npipeline method constructed by (Mou et al.,\n2022b) based on DeepAligned, which first\nuses the clustering algorithm DeepAligned\nto cluster OOD data and obtains pseudo\nOOD labels, and then trains a new classifier\ntogether with IND data.\n‚Ä¢ E2E (Mou et al., 2022b) mixes IND and\nOOD data in the training process and si-\nmultaneously learns pseudo OOD cluster as\nsignments and classifies all classes via self-\nlabeling. Given an input query, E2E connects\nthe encoder output through two independent\nprojection layers, IND head and OOD head, as\nthe final logit and optimize the model through\nthe unified classification loss, where the OOD\npseudo label is obtained through swapped pre-\ndiction (Caron et al., 2020).\nWe only use the samples belonging to IND and\nOOD intents in training set of Banking to train all\nfine-tuned methods.\n4.3 Evaluation Metrics\nFor OOD intent discovery, We adopt three widely\nused metrics to evaluate the clustering results: Ac-\ncuracy (ACC)3, Normalized Mutual Information\n(NMI), and Adjusted Rand Index (ARI). For GID,\nwe adopt two metrics: Accuracy (ACC) and F1-\nscore (F1), to assess the performance of the joint\nclassification results. Besides, we observe that all\nChatGPT methods have the phenomenon that some\nsamples are not assigned any cluster or intent (miss-\ning recall), while some are assigned multiple clus-\nters or intents (repeated recall). For samples with\nmissing recall, we randomly assign a cluster or\nintent; for those with repeated recall, we only re-\ntain the first assigned cluster or intent. We provide\ndetailed recall analysis in Section 5.3.\n4.4 Main Results\nTable 1 and Table 2 respectively show the main re-\nsults of the ChatGPT methods and baselines under\nOOD discovery and GID for three dataset divisions.\nNext, we analyze the results from three aspects:\n(1) Compare Method without IND PriorFrom\nTable 1, we can see that without any IND prior\nknowledge, i.e., direct clustering, ChatGPT‚Äôs per-\nformance is significantly better than BERT un-\nder three dataset partitions, indicating ChatGPT‚Äôs\n3We use the Hungarian algorithm (Kuhn, 1955) to obtain\nthe mapping between the prediction and ground truth classes.\n10295\nstrengths in natural language understanding with-\nout using any private specific data. For example,\nunder IND/OOD=3:1, ChatGPT(DC) outperforms\nBERT by 36.00% (ACC). As the number of OOD\nclasses increases, the clustering metrics of Chat-\nGPT(DC) and BERT decrease rapidly, but Chat-\nGPT(DC) still achieves better performance than\nBERT, exceeding BERT by 28.89% (ACC) under\nIND/OOD=1:1.\n(2) Compare ChatGPT with Finetuned BERT\nFor OOD discovery, when the OOD ratio is rela-\ntively low, the optimal ChatGPT method is slightly\ninferior to fine-tuned baselines. However, as the\nOOD ratio increases, ChatGPT is significantly\nlower than fine-tuned model. We believe this is\nbecause as the OOD ratio increases, the number of\nclustered samples increases and more data brings\nmore difficult semantic understanding challenges\nto generative LLMs. However, discriminative fine-\ntuned methods encode the samples one by one and\nare therefore less affected by the OOD ratio.\nFor GID, ChatGPT is significantly weaker than\nfine-tuned model in both IND and OOD metrics.\nAccording to Table 2, on average in three scenarios,\nthe optimal ChatGPT method is weaker than the\noptimal fine-tuned method by 17.37% (IND ACC),\n20.56% (OOD ACC), and 23.40% (ALL ACC),\nrespectively. We believe this is because ChatGPT\nis pre-trained on large-scale general training data,\nwhich makes it difficult to perform better than fine-\ntuned models on specific domain data.\n(3) Compare different ChatGPT methods For\nOOD discovery, DC generally achieves the best\nperformance, while ZSD is slightly inferior, and\nFSD performs the worst. Although DC is slightly\ninferior to ZSD in the IND/OOD=3:1 scenario, it\nsignificantly outperforms other ChatGPT methods\nin the other two scenarios. FSD almost performs\nthe worst among the three methods. ZSD pro-\nvides additional prior knowledge of IND categories,\nwhile FSD provides labeled IND samples as con-\ntext. However, more IND priors actually lead to\nworse performance for ChatGPT.\nFor GID, GID-FSD performs best on IND clas-\nsification, while GID-DC performs best on OOD\nintents. Comparing GID-ZSD and GID-DC, the\ndifference lies in the pseudo-intent set used. GID-\nZSD is on average 6.22% (ALL ACC) behind GID-\nDC, indicating the importance of the pseudo-intent\nset. For GID-FSD, due to the IND demonstration,\nIND classification ability is significantly improved\nACC NMI ARI\nCluster Metric\n0\n10\n20\n30\n40\n50\n60\n70\n80Value\nW/O Demos\nIND Demos\nOOD Demos\nF1-Score ACC\nClassification Metric\n70\n75\n80\n85\n90\n95\n100Value\nW/O Demos\nIND Demos\nOOD Demos\nFigure 4: The impact of different demonstrations. W/O\nDemos means \"no samples for demonstration\". Under\nIND/OOD Demos, we demonstrate 3 labeled samples\nfor each IND/OOD class.\nthrough in-context learning. However, its OOD\nclassification metric is not as good as that of GID-\nDC. We think this is because the quality of the\npseudo-intent set induced by FSD is poor and the\nIND demonstration may be treated as noise. We\nleave the further analysis of demonstrations in Sec-\ntion 5.1 and GID exploration in Section 5.2.\n5 Qualitative Analysis\n5.1 In-context Learning from IND to OOD\nIn Section 4.4, we find that the IND prior does not\nbring positive effects to OOD tasks. To further\nexplore the influence of different types of demon-\nstrations, we compare the performance of OOD\ntasks under three demonstration strategies: W/O\nDemos, IND Demos, and OOD Demos. Specif-\nically, for OOD discovery, we perform 15 OOD\nclasses clustering; for classification, we test OOD\nclassification under 10 classes, where the OOD in-\ntent used the ground-truth intent set to avoid the\ninfluence of pseudo intent set.\nThe results are reported in Fig 4. Compared\nwith W/O Demos, OOD Demos achieve a signifi-\ncant performance improvement in both clustering\nand classification tasks. In contrast, IND Demos\nresult in a performance decline in clustering and\nalmost no improvement in classification. For OOD\nDemos, it can be considered that the demonstra-\ntion and testing are of the same distribution, so\nChatGPT can improve task performance through\nin-context learning. For IND Demos, the differ-\nent distribution between demonstration and testing\ncauses ChatGPT not only unable to bring perfor-\nmance gains through in-context learning but also\nregard demonstrations as in-context noise that inter-\nferes with task performance. This shows that the\ndistribution of demonstration text has a great\nimpact on the effect of in-context learning , as\n10296\n3:1 3:2 1:1\nOOD Ratio\n30\n40\n50\n60\n70\n80\n90\n100IND Acc\nPseudo Intent\nGT Intent\nGT Intent w/ Desc\n3:1 3:2 1:1\nOOD Ratio\n30\n40\n50\n60\n70\n80\n90\n100OOD Acc\nPseudo Intent\nGT Intent\nGT Intent w/ Desc\nFigure 5: The impact of different intent sets on GID.\nThe pseudo-intent set comes from ChatGPT(GID-SD).\nalso mentioned in (Min et al., 2022). It should be\nnoted that since both IND and OOD classes come\nfrom the banking domain, the fine-tuned model can\nimprove OOD task performance through knowl-\nedge transfer from IND samples. However, this\nfails on ChatGPT, indicating that current in-context\nlearning of LLMs lacks deep mining and transfer\ndemonstration knowledge (i.e., from IND to OOD)\ncapabilities.\n5.2 Reasons for limiting GID performance\nSince ChatGPT performs GID in a pipeline man-\nner, we analyze ChatGPT‚Äôs performance separately\nin generating pseudo intent sets and performing\njoint classification. We show a set of pseudo intent\nsets in Table 3. It can be seen that the three sets\nof pseudo intents are roughly similar to the real\nintents in semantics but show some randomness\nin granularity. Taking intent of ID 5 as an exam-\nple, run-1 explains the payment method as ‚Äúgoogle\npay or apple pay‚Äù, which is consistent with the\ngranularity of the real label; run-2 expands it to\n‚Äúdifferent methods‚Äù, and run-3 further broadens it\nto ‚Äúpayment related‚Äù, with coarser granularity.\nNext, we use the OOD ground-truth intent set\nto replace the OOD pseudo intent set and further\nadd artificial descriptions for each IND&OOD in-\ntent, as shown in Fig 5. Compared with using\npseudo intent set, using the ground truth intent set\ncan only bring slight performance improvement,\nwhile adding intent descriptions can significantly\nimprove classification performance. This shows\nthat the main reason for limiting ChatGPT‚Äôs fur-\nther improvement in GID task is the lack of\ndomain knowledge, and the secondary reason is\nthe quality of the pseudo intents.\n5.3 Recall Analysis\nAs mentioned in Section 4.3, ChatGPT has the\nproblem of missing and repeated recall, which is a\n3:1 3:2 1:1\nIND/OOD Ratio\n0\n5\n10\n15\n20\n25\n30Misssing Recall Ratio\nOOD Discovery\nGID\n3:1 3:2 1:1\nIND/OOD Ratio\n0\n1\n2\n3\n4\n5\n6Repeated Recall Ratio\nOOD Discovery\nGID\nFigure 6: The incorrect recall ratio of ChatGPT. We\naverage the statistics of three ChatGPT methods.\n3 5 10 15 20\nNumber of Samples Per Class\n0\n20\n40\n60\n80\n100NMI\nChatGPT(DC)\nChatGPT(ZSD)\nChatGPT(FSD)\nDeepAligned\nDKT\nFigure 7: The impact of the number of samples in each\ncluster on OOD discovery under IND/OOD=3:1.\nunique problem of generative LLMs.4 Fig 6 shows\nthe statistics of ChatGPT‚Äôs incorrect recall. For\nOOD discovery, as the OOD ratio (clustering num-\nber) increases, the proportion of missing recall and\nrepeated recall both increase significantly. For ex-\nample, under IND/OOD=1:1, the probability of\nmissing and repeated recall reaches 24.15% and\n4.44% respectively, which has seriously damaged\ntask performance. Since clustering tasks require\ninputting all samples into ChatGPT simultaneously,\nmore samples bring more difficult task understand-\ning and processing to ChatGPT, resulting in higher\nincorrect recall rates. For GID, the proportion of\nincorrect recall is almost unaffected by the OOD\nratio, as GID is performed on a sample-by-sample\nbasis. Furthermore, we find that incorrect recall on\nGID is mainly due to the lack of domain knowledge.\nThis results in ChatGPT being unable to clearly\nidentify intent set boundaries and may proactively\nallocate a query to multiple intents or refuse to\nallocate the query to predefined intent sets.\n4We provide relevant cases in Appendix E.\n10297\nID Ground Truth Intent Pseudo Intent 1 (run-1) Pseudo Intent 2 (run-2) Pseudo Intent 3 (run-3)\n1 card_delivery_estimateInquires about delivery time or scheduleDelivery and shipment related questions.Delivery related inquiries\n2 cancel_transfer Requests for cancellation, urgent card\nneeds, and transaction reversion\nTransaction cancellation or\nreversion related questions. Account/card related inquiries\n3 verify_my_identity Inquires about identity verification processIdentity verification related questionsIdentity verification inquiries\n4 cash_withdrawal_chargeQuestions related to fees charged for\ntransactions or withdrawals\nFee related questions, especially related\nto cash withdrawals. Fee related inquiries\n5 apple_pay_or_google_pay\nIssues related to topping up accounts\nwith mobile payment services such as\nGoogle Pay or Apple Pay.\nVarious questions related to top-ups\nand adding money to an account using\ndifferent methods.\nPayment related inquiries\nTable 3: Pseudo-intent set generated by three random runs of ChatGPT(GID-DC) under IND/OOD=3:1.\nMethod IND/OOD=3:1IND/OOD=3:2IND/OOD=1:1K(Pred) ErrorK(Pred) ErrorK(Pred) ErrorDeepAligned(K‚Ä≤=2) 4 1 11 1 11 4DeepAligned(K‚Ä≤=3) 8 3 16 6 16 1ChatGPT(DC) 5 0 14 4 23 8\nTable 4: The results of estimating the number K of\nclusters, where K\n‚Ä≤\nis a hyperparameter for DeepAligned.\nThe real number of clusters are 5,10,15, respectively.\n5.4 Effect of Cluster Sample Number\nWe explore the effect of the number of clustered\nsamples on ChatGPT by changing the ground-truth\nnumber of each OOD intent. As shown in Fig\n7, ChatGPT has poor robustness to the num-\nber of samples. The clustering performance first\nreaches an optimal effect between 5 and 10 sam-\nples per class, and then drops rapidly. In contrast,\ndiscriminative fine-tuned methods exhibits good\nrobustness. We believe this is because when there\nare too few samples, it‚Äôs difficult for ChatGPT to\ndiscover clustering patterns. And when there are\ntoo many samples, ChatGPT needs to process too\nmany samples at the same time, leading to more\ndifficult clustering.\n5.5 Estimate the Number of Cluster K\nIn the experiments above, the number of OOD\nclasses was assumed to be ground truth. How-\never, in real-world applications, the number of\nOOD clusters often needs to be estimated auto-\nmatically. We use the same estimation algorithm\nDeepAligned as a baseline following (Zhang et al.,\n2021; Mou et al., 2022a). For ChatGPT, we re-\nmove the ground-truth cluster number from the\nprompt and count the estimated cluster number\nbased on the result. The results are reported in\nTable 4. When the number of clusters is small,\nChatGPT can obtain more accurate estimates. How-\never, as the number of clusters increases, ChatGPT\nperforms worse than the baseline and is prone to\noverestimate. For the baseline, an appropriate hy-\nModel OOD Discovery GID\nACC NMI ARI INDACC OODACC ALLACC\ntext-davinci-00238.00 39.11 9.57731.56 28.67 30.40\ntext-davinci-00371.33 72.12 49.8459.56 63.33 61.07\nClaude 70.00 84.27 62.2456.67 70.00 62.00\nChatGPT 78.00 78.20 55.3268.44 75.33 70.17\nTable 5: Comparison of different LLMs. All LLMs use\nDC and GID-DC methods under IND/OOD=3:2.\nperparameter can achieve good results, but the ro-\nbustness of the hyperparameter is poor. Therefore,\nChatGPT is more suitable for estimating a small\nnumber of clusters.\n5.6 Comparison of Different LLMs\nIn this section, we evaluate the performance of\nother mainstream LLMs and compare them with\nChatGPT. Text-davinci-002 and text-davinci-003\nbelong to InstructGPT and text-davinci-003 is an\nimproved version of text-davinci-002.5 Compared\nwith GPT-3, the biggest difference of Instruct-\nGPT is that it is fine-tuned for human instructions.\nIn addition to the GPT family models, we also\nevaluate a new LLM Claude developed by An-\nthropic6. As shown in Table 5, ChatGPT performs\nbetter than text-davinci-002 and text-davinci-003\nbecause ChatGPT is further optimized based on\ntext-davinci-003. Claude shows competitive per-\nformance with ChatGPT on OOD discovery, but\nis weaker on GID. In addition, we try to evaluate\nGPT-3 (davinci) and find that GPT-3 fails to per-\nform the tasks, which illustrates the importance of\ninstruction tuning.\n5.7 Effect of Different Prompt\nThorough prompt engineering is crucial to mitigate\nthe variability introduced by different prompts. To\naddress this, we devise three additional variations\n5https://platform.openai.com/docs/models\n6https://www.anthropic.com/product\n10298\nPrompt/Baseline OOD Discovery GID\nACC NMI ARI INDACC OODACC ALLACC\nOriginal 58.2265.3028.21 66.4462.0064.22\nParaphrase 58.6766.8030.10 69.3362.0065.67\nverbosity 60.8968.7434.85 68.6760.6764.67\nSimplification 53.7864.2025.26 65.3358.6762.00\nAverage 57.8966.2629.60 67.4460.8364.14\nDeepaligned(-GID)74.6780.8355.35 91.5077.3384.42\nTable 6: Results on different prompt of DC/GID-DC\nunder IND/OOD=1:1.\n(Paraphrase, Verbosity, Simplification) for Chat-\nGPT (DC/GID-DC) beyond the original prompt\nand conduct experiments with an IND/OOD ratio\nof 1:1.7 The results are shown in Table 6. In sum-\nmary, different prompts led to slight fluctuations in\nthe experimental outcomes, but the results still sup-\nport existing conclusions. For example, the results\nof ChatGPT(DC) are still lower than the baseline\nDeepaligned.\n6 Challenge & Future Work\nBased on above experiments and analysis, we sum-\nmarize three challenging scenarios faced by LLMs\nand provide guidance for the future.\n6.1 Large scale Clustering\nExperiments show that there are three main rea-\nsons why LLMs is limited in performing large-\nscale clustering tasks: (1) The maximum length\nof input tokens limits the number of clusters. (2)\nWhen the number of clusters increases, LLMs will\nhave serious recall errors. (3) LLMs have poor\nrobustness to the number of cluster samples.\nThere have been some work attempts to solve the\nsequence length constraints of transformer-based\nmodels, such as (Bertsch et al., 2023). Another\nfeasible approach is to summarize the samples into\ntopic words before clustering. For recall problem,\none post-remediation method is to first screen out\nthe sample index of the recall error after cluster-\ning, and then prompt LLMs to complete or delete\nthe sample allocation through multiple rounds of\ndialogue. For robustness, a possible method is to\nfirst estimate the optimal number of cluster, select\na small portion of seed samples from the original\nsample set and cluster them, and then classify the\nremaining samples into seed clusters.\n7These prompt are also shown in Appendix D.\n6.2 Semantic understanding of specific\ndomains\nIn Section 5.2, we find that the main reason for the\nlimited performance of LLMs on GID is the lack\nof semantic understanding of specific domains. To\nimprove the performance of general LLMs in spe-\ncific domains, one approach is through fine-tuning\nLLMs, which often requires high training costs\nand hardware resources. Another approach is to\ninject domain knowledge into prompts to enhance\nLLMs. Section 5.1 and 5.2 show that providing\ndemonstration examples or describing label sets\ncan significantly improve performance, but long\nprompts will increase the inference cost of each\nquery. How to efficiently adapt LLMs to specific\ndomains without increasing inference costs is\nstill an area under exploration.\n6.3 Cross-domain in-context learning\nIn some practical scenarios, such as the need to\nperform a new task or expand business scope, there\nis often a lack of demonstration examples directly\nrelated to the new task. We hope to improve the\nperformance of new tasks by leveraging previous\ndomain demonstrations. However, previous exper-\niments show that cross-domain in-context learn-\ning has failed in current LLMs. A meaningful but\nchallenging question is how in-context learning\nwith IND demonstrations performs well in OOD\ntasks? A preliminary idea is to use manual chains\nof thought to provide inference paths from IND\ndemonstration samples to the labels, thereby pro-\nducing more fine-grained domain-specific knowl-\nedge. These fine-grained intermediate knowledge\nmay help generalize to OOD tasks.\n7 Conclusion\nIn this paper, we conduct a comprehensive evalu-\nation of ChatGPT on OOD intent discovery and\nGID, and summarize the pros and cons of Chat-\nGPT in these two tasks. Although ChatGPT has\nmade significant improvements in zero or few-shot\nperformance, our experiments show that ChatGPT\nstill lags behind fine-tuned models. In addition, we\nperform extensive analysis experiments to deeply\nexplore three challenging scenarios faced by LLMs:\nlarge-scale clustering, domain-specific understand-\ning, cross-domain in-context learning and provide\nguidance for future directions.\n10299\nLimitations\nIn this paper, we investigate the advantages, dis-\nadvantages and challenges of large language mod-\nels (LLMs) in open-domain intent discovery and\nrecognition through evaluating ChatGPT on out-of-\ndomain (OOD) intent discovery and generalized\nintent discovery (GID) tasks. Although we conduct\nextensive experiments, there are still several direc-\ntions to be improved: (1) Given the paper‚Äôs focus\non large-scale language models like ChatGPT, it‚Äôs\nworth noting that ChatGPT is only accessible for\noutput, which makes it challenging to thoroughly\ninvestigate and analyze its internal workings. (2)\nAlthough we perform three different data splits\nfor each task, they all come from the same source\ndataset, which makes their intent granularity consis-\ntent. The analysis of different intent granularity is\nnot further explored in this paper. (3) Although we\nensure that all experiments on ChatGPT in this pa-\nper are based on the same version, further updates\nof ChatGPT may lead to changes in the results of\nthis paper.\nReferences\nAmanda Bertsch, Uri Alon, Graham Neubig, and\nMatthew R Gormley. 2023. Unlimiformer: Long-\nrange transformers with unlimited length input.\narXiv preprint arXiv:2305.01625.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. 2018. Deep clustering for unsuper-\nvised learning of visual features. In Proceedings of\nthe European conference on computer vision (ECCV),\npages 132‚Äì149.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya\nGoyal, Piotr Bojanowski, and Armand Joulin. 2020.\nUnsupervised learning of visual features by contrast-\ning cluster assignments. Advances in neural informa-\ntion processing systems, 33:9912‚Äì9924.\nI√±igo Casanueva, Tadas Tem Àácinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vuli¬¥c. 2020. Efficient\nintent detection with dual sentence encoders. arXiv\npreprint arXiv:2003.04807.\nQian Chen, Zhu Zhuo, and Wen Wang. 2019. Bert\nfor joint intent classification and slot filling. arXiv\npreprint arXiv:1902.10909.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nGuanting Dong, Daichi Guo, Liwen Wang, Xuefeng\nLi, Zechen Wang, Chen Zeng, Keqing He, Jinzheng\nZhao, Hao Lei, Xinyue Cui, Yi Huang, Junlan Feng,\nand Weiran Xu. 2022a. Pssat: A perturbed se-\nmantic structure awareness transferring method for\nperturbation-robust slot filling.\nGuanting Dong, Jinxu Zhao, Tingfeng Hui, Daichi Guo,\nWenlong Wang, Boqi Feng, Yueyan Qiu, Zhuoma\nGongque, Keqing He, Zechen Wang, and Weiran Xu.\n2023. Revisit input perturbation problems for llms:\nA unified robustness evaluation framework for noisy\nslot filling task. In Natural Language Processing and\nChinese Computing, pages 682‚Äì694, Cham. Springer\nNature Switzerland.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhi-\nfang Sui. 2022b. A survey for in-context learning.\narXiv preprint arXiv:2301.00234.\nSimon Frieder, Luca Pinchetti, Ryan-Rhys Grif-\nfiths, Tommaso Salvatori, Thomas Lukasiewicz,\nPhilipp Christian Petersen, Alexis Chevalier, and\nJulius Berner. 2023. Mathematical capabilities of\nchatgpt. arXiv preprint arXiv:2301.13867.\nDaichi Guo, Guanting Dong, Dayuan Fu, Yuxiang Wu,\nChen Zeng, Tingfeng Hui, Liwen Wang, Xuefeng Li,\nZechen Wang, Keqing He, Xinyue Cui, and Weiran\nXu. 2023. Revisit out-of-vocabulary problem for slot\nfilling: A unified contrastive framework with multi-\nlevel data augmentations. In ICASSP 2023 - 2023\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 1‚Äì5.\nWX Jiao, WX Wang, JT Huang, Xing Wang, and ZP Tu.\n2023. Is chatgpt a good translator? yes with gpt-4 as\nthe engine. arXiv preprint arXiv:2301.08745.\nHarold W Kuhn. 1955. The hungarian method for the\nassignment problem. Naval research logistics quar-\nterly, 2(1-2):83‚Äì97.\nStefan Larson, Anish Mahendran, Joseph J. Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K. Kummerfeld, Kevin Leach, Michael A.\nLaurenzano, Lingjia Tang, and Jason Mars. 2019. An\nevaluation dataset for intent classification and out-of-\nscope prediction. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\n10300\npages 1311‚Äì1316, Hong Kong, China. Association\nfor Computational Linguistics.\nShanglin Lei, Guanting Dong, Xiaoping Wang, Keheng\nWang, and Sirui Wang. 2023. Instructerc: Reform-\ning emotion recognition in conversation with a re-\ntrieval multi-task llms framework. arXiv preprint\narXiv:2309.11911.\nTing-En Lin, Hua Xu, and Hanlei Zhang. 2020. Dis-\ncovering new intents via constrained deep adaptive\nclustering with cluster refinement. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 34, pages 8360‚Äì8367.\nJiachi Liu, Liwen Wang, Guanting Dong, Xiaoshuai\nSong, Zechen Wang, Zhengyang Wang, Shanglin Lei,\nJinzheng Zhao, Keqing He, Bo Xiao, and Weiran Xu.\n2023. Towards robust and generalizable training: An\nempirical study of noisy slot filling for input pertur-\nbations.\nJ MacQueen. 1965. Some methods for classification\nand analysis of multivariate observations. In Proc.\n5th Berkeley Symposium on Math., Stat., and Prob,\npage 281.\nKamil Malinka, Martin Pere≈°√≠ni, Anton Firc, Ond Àárej\nHujÀán√°k, and Filip Janu≈°. 2023. On the educational\nimpact of chatgpt: Is artificial intelligence ready\nto obtain a university degree? arXiv preprint\narXiv:2303.11146.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048‚Äì11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nYutao Mou, Keqing He, Pei Wang, Yanan Wu, Jingang\nWang, Wei Wu, and Weiran Xu. 2022a. Watch the\nneighbors: A unified k-nearest neighbor contrastive\nlearning framework for OOD intent discovery. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing , pages\n1517‚Äì1529, Abu Dhabi, United Arab Emirates. Asso-\nciation for Computational Linguistics.\nYutao Mou, Keqing He, Yanan Wu, Pei Wang, Jingang\nWang, Wei Wu, Yi Huang, Junlan Feng, and Weiran\nXu. 2022b. Generalized intent discovery: Learning\nfrom open world dialogue system. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 707‚Äì720, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nYutao Mou, Keqing He, Yanan Wu, Zhiyuan Zeng,\nHong Xu, Huixing Jiang, Wei Wu, and Weiran Xu.\n2022c. Disentangled knowledge transfer for OOD\nintent discovery with unified contrastive learning. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 46‚Äì53, Dublin, Ireland. Associ-\nation for Computational Linguistics.\nYiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan\nHu, Yongrui Chen, and Guilin Qi. 2023. Evalu-\nation of chatgpt as a question answering system\nfor answering complex questions. arXiv preprint\narXiv:2303.07992.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nJindong Wang, Xixu Hu, Wenxin Hou, Hao Chen,\nRunkai Zheng, Yidong Wang, Linyi Yang, Hao-\njun Huang, Wei Ye, Xiubo Geng, et al. 2023.\nOn the robustness of chatgpt: An adversarial\nand out-of-distribution perspective. arXiv preprint\narXiv:2302.12095.\nJingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei\nLiu. 2021. Generalized out-of-distribution detection:\nA survey. arXiv preprint arXiv:2110.11334.\nWeihao Zeng, Keqing He, Zechen Wang, Dayuan Fu,\nGuanting Dong, Ruotong Geng, Pei Wang, Jingang\nWang, Chaobo Sun, Wei Wu, and Weiran Xu. 2022.\nSemi-supervised knowledge-grounded pre-training\nfor task-oriented dialog systems. In Proceedings of\nthe Towards Semi-Supervised and Reinforced Task-\nOriented Dialog Systems (SereTOD), pages 39‚Äì47,\nAbu Dhabi, Beijing (Hybrid). Association for Com-\nputational Linguistics.\nHanlei Zhang, Hua Xu, Ting-En Lin, and Rui Lyu. 2021.\nDiscovering new intents with deep aligned clustering.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 14365‚Äì14373.\nA Datasets\nThe original dataset Banking contains 77 classes\nand is class-imbalanced. Its training set has 9003\nsamples, the validation set has 1000 samples, and\nthe test set has 3080 samples. The average token\nlength of the samples is 11.91 , with the longest\nbeing 79. We show 15 IND classes and 15 OOD\nclasses sampled from the 77 class in Table 7.\nB Implementation Details\nFor all baselines, we use the pre-trained BERT\nmodel (bert-base-uncased 8, with 12-layer trans-\nformer) as the backbone, and freeze all but the\nlast transformer layer parameters to achieve better\nperformance and speed up the training procedure\nas suggested in (Zhang et al., 2021). In addition,\n8https://huggingface.com/bert-base-uncased\n10301\nIND Class OOD Classpin_blocked card_delivery_estimatebalance_not_updated_after_bank_transfercancel_transferpending_card_payment verify_my_identityverify_source_of_funds cash_withdrawal_chargedisposable_card_limits apple_pay_or_google_paycard_about_to_expire pending_top_updirect_debit_payment_not_recognisedrequest_refundtop_up_failed card_linkingcard_payment_fee_charged transfer_not_received_by_recipientcard_arrival declined_card_paymentcard_payment_not_recognisedget_disposable_virtual_cardactivate_my_card card_acceptancetransfer_timing get_physical_cardgetting_spare_card exchange_ratecontactless_not_working compromised_card\nTable 7: Sampled 15 IND and 15 OOD classes for the\nexperiments. For IND/OOD=3:1 and IND/OOD=3:2,\nthe first 5 and 10 OOD classes are used respectively.\nOOD Discovery GID\nMethod ACCNMIARI Method INDACC OODACC ALLACC\nDeepAligned94.2295.2790.21DeepAligned-GID98.6794.2296.44DKT 97.7896.9795.16E2E 99.1197.7898.44ChatGPT(DC)80.8984.0262.70ChatGPT(GID-DC)86.0082.6784.33ChatGPT(ZSD)65.3371.1139.15ChatGPT(GID-ZSD)88.0072.0080.00ChatGPT(FSD)56.0064.1526.32ChatGPT(GID-FSD)90.0056.6773.33\nTable 8: The reults on CLINC under IND/OOD=1:1.\nwe keep the hyperparameters consistent with those\nin the official open-source code of all baselines.\nFor ChatGPT-based methods, we perform the ex-\nperiments by calling OpenAI‚Äôs official API, and\nthe version of ChatGPT we used is gpt-3.5-turbo-\n0301. For hyperparameters such as temperature in\nthe API, we keep the default value of OpenAI un-\nchanged.To reduce randomness, we average results\nover three random runs for all methods of ChatGPT\nand baselines.\nC Results on the CLINC Dataset\nFurthermore, we undertake an exploration of ex-\nperimental outcomes utilizing an alternative widely\nemployed dataset, CLINC (Larson et al., 2019), in\nan IND/OOD ratio of 1:1, as presented in Tabel 8.\nDespite the disparate label granularity and domain\nbetween CLINC and Banking Datasets, the experi-\nmental results from CLINC align with and bolster\nthe generalizability of our conclusions drawn from\nthe Banking dataset.\nD Details on Prompts\nAs we perform the GID task on ChatGPT in the\nmanner of a pipeline, we list the complete prompts\nwe used in each stage of the three GID methods in\nTable 9. In addition, we present the prompt variants\nof ChatGPT(DC) in Table 10.\nE Cases of Incorrect Recall\nWe provide cases of incorrect recall in OOD dis-\ncovery and GID in Figures 11 and 12, respectively.\n10302\nMethod Stage Prompt\nGID-DC 1\nNext, I will first give you a set of sentences, which will be recorded as Set 1. First, please classify the sentences\nin Set 1 into 5 (Number of Clusters) categories according to their intentions. You only need to output the\ncategory number and the corresponding sentence number in the following format:\nCategory 1: 1,2,3,4,5 . . . . . .\nThen, you need to summarize the intent of each class from Category 1 to Category 5.\nSet 1(DOODtest ): 1. can I choose a date for delivery? . . . . . .\n2\nBelow is a predefined set of intent categories, recorded as Set 1:1. pin_blocked;. . . . . .\nPlease classify the following sentence into Set 1 according to its intention, just response the corresponding category\nnumber:<test sample>\nGID-ZSD 1\nNext, I will first give you a set of intent categories, which will be recorded as Set 1. Then I will give you another set\nof sentences without intention labels, recorded as Set 2. You first need to learn the knowledge in Set 1, and then\nuse the learned knowledge to classify the sentences in Set 2 into 5 (Number of Clusters) categories according to their\nintentions. You only need to output the category number and the corresponding sentence number in the following format:\nCategory 1: 1,2,3,4,5 . . . . . .\nIt should be noted that the intention in set 1 and the intention in set 2 do not overlap.\nThen, you need to summarize the intent of each class from Category 1 to Category 5.\nSet 1(YIND): pin_blocked; . . . . . .\nSet 2(DOODtest ): 1. can I choose a date for delivery? . . . . . .\n2\nBelow is a predefined set of intent categories, recorded as Set 1:1. pin_blocked;. . . . . .\nPlease classify the following sentence into Set 1 according to its intention, just response the corresponding category\nnumber:<test sample>\nGID-FSD 1\nNext, I will first give you a set of sentences with intention labels, which will be recorded as Set 1.Then I will give you\nanother set of sentences without intention labels, recorded as Set 2. You first need to learn the knowledge in Set 1, and then\nuse the learned knowledge to classify the sentences in Set 2 into 5 (Number of Clusters) categories according to\ntheir intentions. You only need to output the category number and the corresponding sentence number in the following format:\nCategory 1: 1,2,3,4,5 . . . . . .\nIt should be noted that the intention in set 1 and the intention in set 2 do not overlap.\nThen, you need to summarize the intent of each class from Category 1 to Category 5.\nSet 1 (DINDdemo): How do I unblock my card? intention:pin_blocked; . . . . . .\nSet 2 (DOODtest ): 1. can I choose a date for delivery? . . . . . .\n2\nNext, I will first give you a predefined set of intent categories, which will be recorded as Set 1. Then I will give you another set\nof sentences with intention labels, recorded as Set 2.\nSet 1:1. pin_blocked;. . . . . .\nSet 2:sentence: How do I unblock my card? intention:pin_blocked;. . . . . .\nYou first need to learn the knowledge in Set 2, and then use the learned knowledge to classify the following sentence into Set 1\naccording to its intention , just response the corresponding category : <test sample>\nTable 9: The complete prompts used in the three GID methods.\nType Prompt\nOriginal\nNext, I will first give you a set of sentences , which will be recorded as Set 1. First,Please classify the sentences in Set 1\ninto 5 categories according to their intentions. You only need to output the category number and the corresponding\nsentence number in the following format:\nCategory 1: 1,2,3,4,5 . . . . . .\nParaphrase\nI will provide you with a collection of sentences, noted as Set 1. Your task is to categorize the sentences in Set 1 into 5\ndistinct groups based on their underlying intentions. Your output should include the category number along with the\ncorresponding sentence number, formatted as follows:\nCategory 1: 1, 2, 3, 4, 5, and so on...\nVerbosity\nNext, I will be presenting you with a compilation of sentences, collectively labeled as \"Set 1\". Your task is to\ncategorize these sentences into 5 distinct groups according to their underlying intentions. Upon completing the task,\nyour response is anticipated to take the form of a structured enumeration. Your response should consist of the\nassigned category number along with the respective sentence numbers following this format:\nCategory 1: 1, 2, 3, 4, 5...\nSimplification\nNext, I‚Äôll provide sentences in Set 1. Please categorize them into 5 groups based on intentions. Output the category\nnumber and sentence number in this format:\nCategory 1: 1, 2, 3, 4, 5. . .\nTable 10: The three prompt variants of ChatGPT(DC).\n10303\nInput Response Missing\nRecall\nRepeated\nRecall\nPlease classify the sentences in\nSet 2 into 10 categories\naccording to their intentions.\nCategory 1: 2 , 10, 30, 31, 50\n20,26,29,\n35,40,41,\n42,46,48\n2,27\nCategory 2: 4, 25, 27 , 34, 44\nCategory 3: 5, 6, 15, 16, 36\nCategory 4: 1, 2 , 3\nCategory 5: 7, 17, 27\nCategory 6: 8, 18, 28, 38\nCategory 7: 9, 22, 32, 39, 49\nCategory 8: 11, 12, 21\nCategory 9: 13, 23, 33, 43\nCategory 10: 14, 19, 24, 37, 45, 47\nTable 11: Cases of incorrect recall in OOD Discovery. The index marked with an underline denotes the repeatedly\nrecalled sample.\nType Input Response Ground-Truth Intent\nMissing\nRecall\nHow many days until the money will\nbe in my account? It does not fit in any category in Set 1. transfer_timing (IND)\nI would like to cancel a payment. I\npurchased something several days\nago and i still have not received it.\nThe sentence does not match any\nintents in as it is not related to issues with payment\nor card management. It could potentially fall\nunder a \"purchase delivery inquiry\" category.\nrequest_refund (OOD)\nRepeated\nRecall\nI don‚Äôt know where this transaction\ncame from?\n8. top_up_failed. ( Note: The sentence could also fit\nunder 3. pending_card_payment or 11.\ncard_payment_not_recognised, depending on the\ncontext of the conversation.)\ncard_payment_not_\nrecognised (IND)\nMy credit card was declined. 12. activate_my_card; 24. Issues related to card\npayme declined_card_payment (OOD)\nTable 12: Cases of incorrect recall in GID.\n10304",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6409868597984314
    },
    {
      "name": "Natural language processing",
      "score": 0.4909122586250305
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4030637741088867
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210087373",
      "name": "Meizu (China)",
      "country": "CN"
    }
  ],
  "cited_by": 3
}