{
  "title": "PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers",
  "url": "https://openalex.org/W3215434919",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A41270877",
      "name": "Dong, Xiaoyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2143844023",
      "name": "Bao Jianmin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1877972943",
      "name": "Zhang Ting",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1943263669",
      "name": "Chen Dongdong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1862070408",
      "name": "Zhang, Weiming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101029194",
      "name": "Yuan Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1931409288",
      "name": "Chen Dong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103600719",
      "name": "Wen, Fang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A734139434",
      "name": "Yu, Nenghai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3010309834",
      "name": "Guo, Baining",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2141983208",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2242818861",
    "https://openalex.org/W2604737827",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W2948012107",
    "https://openalex.org/W2339754110",
    "https://openalex.org/W2267126114",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W2963522749",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2519430864",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2423557781",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3211983881",
    "https://openalex.org/W3036982689",
    "https://openalex.org/W2962785568",
    "https://openalex.org/W2156566307",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W1580389772",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2196707239",
    "https://openalex.org/W2148349024",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W2795783309",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W2475287302",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2535388113",
    "https://openalex.org/W2731516742",
    "https://openalex.org/W3173151551",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3174738881",
    "https://openalex.org/W2949718784"
  ],
  "abstract": "This paper explores a better prediction target for BERT pre-training of vision transformers. We observe that current prediction targets disagree with human perception judgment.This contradiction motivates us to learn a perceptual prediction target. We argue that perceptually similar images should stay close to each other in the prediction target space. We surprisingly find one simple yet effective idea: enforcing perceptual similarity during the dVAE training. Moreover, we adopt a self-supervised transformer model for deep feature extraction and show that it works well for calculating perceptual similarity.We demonstrate that such learned visual tokens indeed exhibit better semantic meanings, and help pre-training achieve superior transfer performance in various downstream tasks. For example, we achieve $\\textbf{84.5\\%}$ Top-1 accuracy on ImageNet-1K with ViT-B backbone, outperforming the competitive method BEiT by $\\textbf{+1.3\\%}$ under the same pre-training epochs. Our approach also gets significant improvement on object detection and segmentation on COCO and semantic segmentation on ADE20K. Equipped with a larger backbone ViT-H, we achieve the state-of-the-art ImageNet accuracy (\\textbf{88.3\\%}) among methods using only ImageNet-1K data.",
  "full_text": "PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers\nXiaoyi Dong1* ‚Ä†, Jianmin Bao2‚àó, Ting Zhang2, Dongdong Chen3,‚Ä†, Weiming Zhang1\nLu Yuan3, Dong Chen2, Fang Wen2, Nenghai Yu1, Baining Guo2\n1University of Science and Technology of China\n2Microsoft Research Asia 3Microsoft Cloud + AI\n{dlight@mail., zhangwm@, ynh@}.ustc.edu.cn cddlyf@gmail.com\n{jianbao, ting.zhang, luyuan, doch, fangwen, bainguo }@microsoft.com\nAbstract\nThis paper explores a better prediction target for BERT pre-\ntraining of vision transformers. We observe that current pre-\ndiction targets disagree with human perception judgment.\nThis contradiction motivates us to learn a perceptual predic-\ntion target. We argue that perceptually similar images should\nstay close to each other in the prediction target space. We sur-\nprisingly Ô¨Ånd one simple yet effective idea: enforcing percep-\ntual similarity during the dV AE training. Moreover, we adopt\na self-supervised transformer model for deep feature extrac-\ntion and show that it works well for calculating perceptual\nsimilarity. We demonstrate that such learned visual tokens in-\ndeed exhibit better semantic meanings, and help pre-training\nachieve superior transfer performance in various downstream\ntasks. For example, we achieve 84.5% Top-1 accuracy on\nImageNet-1K with ViT-B backbone, outperforming the com-\npetitive method BEiT by +1.3% under the same pre-training\nepochs. Our approach also gets signiÔ¨Åcant improvement on\nobject detection and segmentation on COCO and semantic\nsegmentation on ADE20K. Equipped with a larger backbone\nViT-H, we achieve the state-of-the-art ImageNet accuracy\n(88.3%) among methods using only ImageNet-1K data.\nIntroduction\nCurrent state-of-the-art self-supervised pre-training meth-\nods (Dosovitskiy et al. 2020; Bao, Dong, and Wei 2021;\nHe et al. 2021; Xie et al. 2021; Chen et al. 2022; Wei\net al. 2021) for vision transformers focus on masked image\nmodeling (MIM), a task of making predictions for masked\npatches from the visible patches. The input is usually an\nimage consisting of visible patches and randomly masked\npatches and each patch is associated with corresponding\npositional embedding. The prediction target for masked\npatches varies for different methods, ranging from pixel-\nlevel prediction (Dosovitskiy et al. 2020; He et al. 2021; Xie\net al. 2021) to feature-level prediction (Bao, Dong, and Wei\n2021; Chen et al. 2022; Wei et al. 2021). In this paper, we\nstudy the prediction targets and introduce a better prediction\ntarget for MIM.\nWe point out that current prediction targets disagree with\nhuman judgment when evaluating the similarity between\n*Equal contribution, ‚Ä† Corresponding Author\n‚Ä†Work done during an internship at Microsoft Research Asia\nCopyright ¬© 2023, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nL2 \nDALL-E\nPeCo (Ours)\nHuman \n(48.80)\n(5.54)\n(1.75)\n(19.12)\n(4.79)\n(3.95)\n(36.35)\n(7.04)\n(3.61)\n(22.37)\n(5.55)\n(4.44)\nReference View1 View2Reference View1 View2\nFigure 1: Several examples illustrating the results of dif-\nferent prediction targets on the question that which image\n(View1 or View2) is ‚Äúcloser‚Äù to the Reference image. The\nnumber denotes the distance between View1 or View2 and\nthe Reference image. The images with smaller distances are\nconsidered more similar. We observe that the proposed PeCo\nagrees with human judgments while L2 or DALL-E dis-\nagree.\ntwo different images. There are two representative predic-\ntion targets in current MIM methods: per-pixel regression\nand discrete token prediction. Figure 1 illustrates the results\nof different prediction targets on the question that which im-\nage (View1 or View2) is ‚Äúcloser‚Äù to the ‚ÄúReference‚Äù for\nthese examples. The reason for such disagreement of cur-\nrent prediction targets may come from the per-pixel loss.\nNote that the discrete tokens are obtained by a VQ-V AE\ntrained under the objective of reconstruction loss, i.e. , per-\npixel loss. The per-pixel measure assuming pixel-wise inde-\npendence is insufÔ¨Åcient for assessing structured outputs. For\nexample, blurring causes large perceptual change but small\npixel error, while shifting incurs small perceptual change but\nlarge pixel error (Zhang et al. 2018). Such disagreement with\nhuman visual perception indicates that perceptually similar\npatches may have divergent prediction targets. This under-\nmines the capability of MIM as it, in principle, is based on\ncontext prediction.\nWe propose that a good prediction target for MIM should\ncoincide with human judgment. In other words, perceptually\nsimilar images should be close to each other in the prediction\ntarget space. Inspired from the observation in (Zhang et al.\n2018) that deep features model low-level perceptual simi-\nlarity surprisingly well, we introduce this so-called percep-\ntual loss in VQ-V AE for discrete token learning. This loss\ncan be viewed as per-feature loss as it aims to minimize the\narXiv:2111.12710v3  [cs.CV]  7 Dec 2022\nfeature-wise distance between the original image and the re-\nconstructed image. SpeciÔ¨Åcally, we adopt multi-scale deep\nfeatures from multiple layers at different depth of a self-\nsupervised Transformer. As shown in Figure 1, our proposed\nnew prediction target indeed aligns with human perception\njudgment. We also show that the proposed visual tokens get\nmuch higher linear accuracy than the one without the per-\nceptual loss. It indicates that our new visual tokens exhibit\nmore semantic meanings, which is analogous to texts whose\ndiscrete tokens often contain highly semantic information.\nWe denote MIM using the introduced perceptual visual\ntokens for targets as ‚ÄúPeCo‚Äù, i.e. Perceptual Codebook for\nBERT pre-training of vision transformers. In the experi-\nments, we demonstrate that equipped with such perceptual\nvisual tokens, PeCo achieves better performance compared\nwith the strong competitor BEiT (Bao, Dong, and Wei 2021)\nusing DALL-E (Ramesh et al. 2021) codebook trained over\n250M images without the perceptual loss. We Ô¨Åne-tune the\npre-trained model on various downstream tasks: image clas-\nsiÔ¨Åcation, object detection, and semantic segmentation. Ex-\nperimental results show that our pre-trained model transfers\nbetter than BEiT with only the prediction target changed.\nConcretely, we achieve84.5% Top-1 accuracy on ImageNet-\n1K with ViT-B model, outperforming BEiT by +1.3% with\nthe same 800 pre-training epochs. Our approach also gets\nsigniÔ¨Åcant improvement on COCO object detection and se-\nmantic segmentation as well as on ADE20K semantic seg-\nmentation. Our PeCo also shows strong scalability that when\nequipped with a larger backbone ViT-H, we achieve the\nstate-of-the-art ImageNet accuracy (88.3%) among methods\nusing only ImageNet-1K data.\nRelated Works\nSelf-supervised Learning. Self-supervised learning has at-\ntracted increasing attention over the past few years, as deep\nlearning networks become more and more data-hungry and\nit‚Äôs impossible to label everything in the world. There are\ntwo main categories along this path, contrastive and genera-\ntive (Liu et al. 2021a). One emerging Ô¨Åeld is self-supervised\ncontrastive learning, training an encoder to the representa-\ntion measured by contrastive loss (Hadsell, Chopra, and Le-\nCun 2006; Dosovitskiy et al. 2014) via comparing similar\nand dissimilar samples. The representative methods include\nMOCO (He et al. 2020; Chen et al. 2020d), SimCLR (Chen\net al. 2020b,c), BYOL (Grill et al. 2020), SwA V (Caron\net al. 2020) and more (Oord, Li, and Vinyals 2018; Li et al.\n2021; Bachman, Hjelm, and Buchwalter 2019). However,\ncontrastive-based methods heavily depend on the strong data\naugmentation and effective negative sampling.\nThe other recent resurgent Ô¨Åeld is generative self-\nsupervised learning, training an encoder and a decoder under\nthe objective of reconstruction loss. The typical objectives,\nautoregressive and denoising autoencoder, aiming at recov-\nering the corrupted or masked input, has yielded the most\nsuccessful frameworks (Devlin et al. 2018; Radford et al.\n2018, 2019; Brown et al. 2020; Liu et al. 2019; Joshi et al.\n2020) in NLP. Thanks to the pre-existing vocabulary in lan-\nguage, recovering the missing word can be transformed into\npredicting all the possible words with the probability esti-\nmation, converting the prediction problem to an easier clas-\nsiÔ¨Åcation problem. While in CV , on the other hand, most\nattempts (Van Oord, Kalchbrenner, and Kavukcuoglu 2016;\nOord et al. 2016; Chen et al. 2020a; He et al. 2021) still\nresort to regression for generative methods due to the lack\nof visual vocabulary, e.g. iGPT (Chen et al. 2020a). Re-\ncently, BEiT (Bao, Dong, and Wei 2021) successfully adopts\na classiÔ¨Åer for prediction by directly adopting a VQ-V AE as\nthe visual tokenizer. Yet there exists a major difference be-\ntween the language vocabulary and the visual vocabulary.\nThat is, the words of language are highly semantic, while\nthe visual words of images are mostly not. Most recently,\nnumerous works (Bao, Dong, and Wei 2021; He et al. 2021;\nXie et al. 2021; Chen et al. 2022; Wang et al. 2022b; Dong\net al. 2022; Baevski et al. 2022; Zheng et al. 2022) based\non MIM have been concurrently developed, yet few stud-\nied the perceptual level of the prediction targets. In this\nwork, we attempt to learn a perceptual visual vocabulary for\nBERT pre-training, showing superior transfer performance\nthan BEiT (Bao, Dong, and Wei 2021) and MAE (He et al.\n2021).\nDiscrete Visual Supervision. Exploring masked image\nmodeling or image inpainting task for self-supervised pre-\ntrained tasks has never been stopped in vision community,\nespecially when BERT (Devlin et al. 2018) achieves great\nsuccess in various tasks of NLP. To apply the cross-entropy\nloss function for vision tasks, iGPT (Chen et al. 2020a) clus-\nters the pixel values to simulate the process of BPE (Sen-\nnrich, Haddow, and Birch 2015) process for different words\nin language. ViT (Dosovitskiy et al. 2020) attempts to di-\nrectly divide the raw pixel values into multiple groups and\nassign a discrete label for each group GRB value. Recent\nwork VQ-V AE (Oord, Vinyals, and Kavukcuoglu 2017) pro-\nposes to adopt encoder and decoder to quantize the visual\ncontents to a learnable codebook with Ô¨Åxed size.\nPerceptual Similarity. The perceptual similarity, as its\nname suggests, is to mimic the human perceptual judg-\nment of image similarity. Numerous efforts have been pro-\nposed to achieve that, such as SSIM (Wang et al. 2004),\nMSSIM (Wang, Simoncelli, and Bovik 2003), FSIM (Zhang\net al. 2011), and HDR-VDP (Mantiuk et al. 2011). It has\nbeen shown in (Zhang et al. 2018) that the internal activa-\ntions of network trained for classiÔ¨Åcation task surprisingly\ncoincide with human judgment. Such deep features have\nbeen widely used in image generation (Gatys, Ecker, and\nBethge 2016; Johnson, Alahi, and Fei-Fei 2016; Chen et al.\n2017; Bruna, Sprechmann, and LeCun 2015; Ledig et al.\n2017; Esser, Rombach, and Ommer 2021) with the goal of\nsynthesizing realistic images. The loss is called perceptual\nloss or VGG loss as the network used is often VGG architec-\nture. In this paper, we surprisingly discover that this simple\nloss is super effective in building a better prediction target\nand signiÔ¨Åcantly improves vision BERT pretraining. More-\nover, to enable self-supervised learning, we adopt a self-\nsupervised trained network rather than ImageNet-trained\nnetworks and show it also works comparably well. Both\nthese two discoveries are conceptually simple yet super-\neffective and valuable.\nMethod\nIn the natural language processing Ô¨Åeld, the words are nat-\nurally discrete tokens which contain high semantic informa-\ntion. By contrast, vision signals are continuous with redun-\ndant low-level information. While there are various ways to\ndiscretize the image in prior works, the semantic level of the\nresulting visual tokens has been largely ignored. In this sec-\ntion, we start by brieÔ¨Çy describing the discrete representa-\ntion learning from VQ-V AE, and then introduce the process\nof how to learn a perceptual codebook, followed by BERT\npre-training over the learned perceptual visual tokens.\nLearning Discrete Codebook for Visual Content\nWe utilize VQ-V AE (Oord, Vinyals, and Kavukcuoglu 2017)\nto convert the continuous image content into the form of\ndiscrete tokens. Consider an image x ‚àà RH√óW√ó3, VQ-\nV AE is able to represent it with discrete visual codewords\n{z1\nq ,z2\nq ,¬∑¬∑¬∑ ,zN\nq }‚ààV 1 √óV2 √ó¬∑¬∑¬∑√óV N , where zi\nq comes\nfrom a visual codebook (vocabulary) Vi = {ei\nk ‚ààRD}Ki\nk=1\nconsisting of Ki D-dimensional codewords. Usually we\nhave K1 = K2 = ¬∑¬∑¬∑ = KN = K for simplicity, and\nN = h√ów with h√ów being the spatial resolution of the\nlatent space.\nSpeciÔ¨Åcally, to learn such latent codeooks, VQ-V AE con-\ntains three major parts: an encoder, a quantizer and a de-\ncoder. The encoder maps the input image to intermediate\nlatent vectors z = Enc(x), where z ‚ààRh√ów√óD.The quan-\ntizer is in charge of quantizing each vector at position (i,j)\nto be codewords coming from the corresponding codebook\nVi,j = {ei,j\nk }K\nk=1 ‚äÇRD according to nearest neighbor as-\nsignment. That is,\nk‚àó= q(zi,j) = arg min\nk‚àà{1,2,¬∑¬∑¬∑,K}\n‚à•zi,j ‚àíei,j\nk ‚à•. (1)\nzi,j\nq = r(k‚àó) = ei,j\nk‚àó , (2)\nwhere q is the quantization encoder that maps the vector\nto an index of the codebook, and r is the quantization de-\ncoder that reconstructs the vector from the index. Based on\nthe quantized codewords zq, the decoder aims to reconstruct\nthe input image x. Suppose the reconstruct result is ÀÜx =\nDec(zq). Since the quantizer is non-differentiable, to back-\npropagate gradient into encoder, the gradient is approxi-\nmated like the straight-through estimator (Bengio, L¬¥eonard,\nand Courville 2013) and just copied from decoder to en-\ncoder (Oord, Vinyals, and Kavukcuoglu 2017). The training\nobjective of VQ-V AE is deÔ¨Åned as,\nLVQ-V AE(Enc,Dec,{V}) = Lpixel + ‚à•sg[Enc(x)] ‚àízq‚à•2\n2\n+ Œ≤‚à•sg[zq] ‚àíEnc(x)‚à•2\n2. (3)\nHere, Lpixel = 1\nH√óW√ó3 ‚à•x‚àíÀÜx‚à•is the per-pixel loss, sg [¬∑]\nis the stop-gradient operator, Œ≤is a loss weight set to 0.25 in\nall our experiments.\nLearning Perceptual Codebook for Visual Content\nIn the vanilla VQ-V AE, the codebook is learned by an\nelement-wise pixel loss, i.e. Lpixel, between the original im-\nage and the reconstructed image. However, this per-pixel\nloss may prevent the network from capturingperceptual dif-\nference since the loss only accounts for the correctness of\nindividual pixels. Therefore, a small shift and rotation oper-\nation on the original image may not cause perceptual change\nbut large ‚Ñì1/‚Ñì2 error.\nTherefore, we propose a simple yet effective strategy by\nenforcing perceptual similarity between the original image\nand the reconstructed one beyond the pixel loss. The percep-\ntual similarity is not based on pixel differences but instead\nfeature differences where the high-level image features ex-\ntracted from a pre-trained deep neural network. We hope this\nfeature-wise loss will better capture perceptual difference\nand offer invariance towards low-level variations. We show\nthe comparison of using different losses in Figure 3 from\nthe perspective of image reconstruction, suggesting that im-\nages with lower pixel-wise loss may not appear perceptually\nsimilar.\nPrevious works usually adopt a supervised pretrained\nVGG (Simonyan and Zisserman 2014) network to calcu-\nlate perceptual loss, since using supervision is not consistent\nwith our purpose of self-supervised pre-training. We turn to\nthe self-supervised models and replace the ConvNet-based\nmodel with Vision Transformer, which have a better model-\ning capability and efÔ¨Åciency. On the other hand, pre-trained\nmodels usually encode different levels of semantic infor-\nmation in different layers, to enable our codebook to have\nrich perceptual information, we adopt multi-scale features\nfrom multiple layers of the model to calculate the percep-\ntual loss. Our experiments show that a vision Transformer\n(ViT-B model) from self-supervised learning works well for\ncalculating perceptual loss.\nFormally, let fl(x) be the normalized activations of the l-\nth layer of a network F when processing the image x. The\nsize of the feature map is Hl √óWl √óCl with Hl being the\nheight, Wl being the width and Cl being the channel dimen-\nsion. Usually, multi-scale features, more comprehensive and\ndiscriminative, from multiple layers at different depth are\nextracted to calculate the perceptual similarity for better se-\nmantic capture. The perceptual metric for the input image x\nand the reconstructed image ÀÜxcan be formulated as,\nLpercep =\n‚àë\nl‚ààS\n1\nClHlWl\n‚à•fl(x) ‚àífl(ÀÜx)‚à•2\n2, (4)\nwhere Sdenotes the number of layers from which the fea-\ntures are extracted.\nTherefore, the overall objective function is,\nLVQ-V AEpercep = Lpixel + ŒªLpercep\n+ ‚à•sg[Enc(x)] ‚àízq‚à•2\n2\n+ Œ≤‚à•sg[zq] ‚àíEnc(x)‚à•2\n2, (5)\nwhere Œª is the hyper-parameter for the loss weight of\nLpercep, we will study different vaules of loss weight Œª in\nthe experiments. The training pipeline of perceptual code-\nbook is illustrated in Figure 2 (a). After training, the encoder\nand the quantizer are used as tokenizer in the subsequent\npre-training process.\nTokenizer\n¬∑¬∑¬∑ N-11 2 3\nMSE Loss\nPerceptual\nLoss\nEncoder Nearest \nSearching Decoder\nPeCo Tokenizer\n Perceptual CodeBook ùí±\nInput ùë• Output ‡∑úùë•\n1 75 23 21\n76 1 17 64\n3 43 2 32\n3 12 85 66\nùëßùëû\n1 75 23 21\n76 1 17 64\n3 43 2 32\n3 12 85 66\n1 75 23 21\n76 1 17 64\n3 43 2 32\n3 12 85 66\nùëù1 ùëù2 ùëù3 ùëù4\nùëù5 ùëù6 ùëù7 ùëù8\nùëù9 ùëù10 ùëù11 ùëù12\nùëù13 ùëù14 ùëù15 ùëù16\nMIM Block\n√óL\nCE Loss\n(a)PeCo Training Stage\n(b) Apply PeCo in BERT-like Pretraining\nMasked Input “ßùë• Input ùë•Prediction target ùëòOutput probability ùëù\nLearnable model\nFixed model\nPerceptual\nModel\nPerceptual\nModel\nùëß\nCodeword Index Matrix ùëò\nFigure 2: (a) Training pipeline of our Perceptual Coodbook. (b) Apply PeCo in BERT-Like pretraining. Our PeCo provides a\nmore semantic prediction target to the Mask Image Modeling Task.\nInput ùêøùëùùëñùë• ùêøùëùùëñùë•+ùêøùëùùëíùëüùëê Input ùêøùëùùëñùë• ùêøùëùùëñùë•+ùêøùëùùëíùëüùëê\nFigure 3: Image reconstruction with different losses. An\nexample contains three images showing input (left), re-\nconstructed image using pixel-wise loss (middle), and re-\nconstructed image using pixel-wise and feature-wise losses\n(right). We can see that perceptually the right image appears\nmore similar to the input compared with the middle image,\nalthough the middle image gets lower pixel-wise loss.\nBERT Objective over Perceptual Codebook\nWe adopt the BERT objective to perform the masked\nimage modeling task over the discrete visual tokens as\nin BEiT (Bao, Dong, and Wei 2021), illustrated in Fig-\nure 2. For a given image x, the input tokens are image\npatches which are non-overlappingly split from the whole\nimage, and the output tokens are discrete perceptual vi-\nsual words obtained through learning Eqn 5. Let the in-\nput be {x1,x2,¬∑¬∑¬∑ ,xN }, and the groundtruth output be\n{k1,k2,¬∑¬∑¬∑ ,kN } = q(Enc(x)). The goal of the masked\nimage modeling is to recover the corresponding visual to-\nkens from the masked input where a portion of input tokens\nhave been masked.\nPrecisely, let Mbe the set of masked index. Then the\nmasked input ¬Øxis represented as,\n¬Øxi =\n{\nxi, i /‚ààM\nm, i ‚ààM ,i = 1,2,¬∑¬∑¬∑ ,N, (6)\nwhere m is a learnable mask token as same dimension as\nnon-mask tokens. The masked input tokens are fed into a\nL-layer vision Transformer with the last layer‚Äôs hidden out-\nput being denoted as {h1,h2,¬∑¬∑¬∑ ,hN }. We aim at recover-\ning the corresponding visual token from the hidden vector\nat masked positions. To achieve that with the classiÔ¨Åcation\nloss, a K-way classiÔ¨Åer is appended after the hidden vec-\ntor hi to get the probability estimation about all possible\ndiscrete tokens in the corresponding codebook Vi. Suppose\nthe groundtruth discrete visual tokens corresponding to the\nmasked patches are kt with t ‚ààM, the pre-training objec-\ntive can be formulated as,\nLpre-training = ‚àí\n‚àë\nt‚ààM\nlogP(kt|¬Øx), (7)\nwhere P(kt|¬Øx) is the estimated target token probability for\nmasked patches of corrupted image ¬Øx.\nAfter pre-training the model, we apply the model to\nvarious downstream tasks including ImageNet- 1K (Deng\net al. 2009) classiÔ¨Åcation, COCO object detection (Lin et al.\n2014), and ADE20K (Zhou et al. 2017) Segmentation.\nPre-training Details\nVector Quantizer. We use the standard k-means algorithm\nfor vector quantization. We set the codebook size K as\n8192 for fair comparison. When the size of the discrete la-\ntent space K is large, we observe that only a few code-\nwords are selected to represent image and get trained. Many\nother codewords are wasted. To overcome this issue, we\nadopt exponential moving averages (Oord, Vinyals, and\nKavukcuoglu 2017) to update the codebook which is proved\nto be useful for increasing utilization of codewords in a\ncodebook.\nPerceptual Codebook Learning Setup. We train the per-\nceptual codebook using the training set of ImageNet-1K\ndataset by default. For the encoder and decoder of VQ-V AE,\nwe choose traditional convolutional based backbone. The\nnetwork contains two residual blocks at each resolution. A\nself-attention block is applied to the smallest resolution for\nboth encoder and decoder. For perceptual loss, we use the\npre-trained 100 epochs ViT-B model from self-supervised\nmethod MoCo v3 (Chen, Xie, and He 2021) by default.\nBERT Pre-training Setup. For computation resource con-\nsideration, we use the original ViT-B/16 (Dosovitskiy et al.\n2020) as the basic architecture of our backbone to vali-\ndate the effectiveness of the learned visual codebook, as in\nBEiT (Bao, Dong, and Wei 2021). The model is pre-trained\nfor 300/800 epochs with the batchsize of 2048. We use a\nblock-wise masking strategy for obtaining the corrupted im-\nages with the same setup as BEiT (Bao, Dong, and Wei\n2021). We further demonstrate the effectiveness of our ap-\nproach when scaling to ViT-Large and ViT-Huge backbones.\nExperiments\nDownstream Tasks\nImage ClassiÔ¨Åcation aims to classify a given image into its\ncorresponding class category. We use the popular ImageNet-\n1K dataset. To enable classiÔ¨Åcation, a global average pool-\ning layer is appended after the pre-trained model. We Ô¨Åne-\ntune the model with 100 epochs and a cosine decay learning\nrate that warmups to 4e‚àí3 with 20 epochs and decays to 0.\nFollowing (Bao, Dong, and Wei 2021), the layer-wise learn-\ning rate decay is also used and set to 0.65 by default. For\nmore details, please refer to the supplementary materials.\nSemantic Segmentation is the task of assigning a label to\neach pixel of the input image. We compare on the seman-\ntic segmentation dataset ADE 20K benchmark (Zhou et al.\n2017). Here we employ the Upernet (Xiao et al. 2018) as the\nbasic framework. For fair comparison, we follow previous\nworks (Bao, Dong, and Wei 2021) and train Upernet 160k\niterations with batch size set as 16, more details are provided\nin the supplementary material.\nObject Detection and Segmentation.Object detection is to\nlocate objects in a given image and identify each object. We\nperform Ô¨Åne-tuning on the COCO objection detection and\nsegmentation with the Mask R-CNN (He et al. 2017) frame-\nwork. SpeciÔ¨Åcally, we add four different scale FPNs to scale\nthe feature map into different size following (Bao, Dong, and\nWei 2021). The Ô¨Åne-tuning is conducted with ‚Äú1x‚Äù (12 train-\ning epochs) schedule and single-scale input on the COCO\ntraining set and test the performance on COCO validation\nset, following the strategy used in Swin Transformer (Liu\net al. 2021b).\nMethods pre-train pre-train ViT-B ViT-L ViT-H ViT-H 448dataset epochs\nTraining from scratch (i.e., random initialization)\nViT384 - - 77.9 76.5 ‚Äì ‚Äì\nDeiT - - 81.8 ‚Äì ‚Äì ‚Äì\nViT - - 82.3 82.6 83.1 ‚Äì\nSelf-Supervised Pre-Training on ImageNet-1K\nDINO IN- 1K 300 82.8 ‚Äì ‚Äì ‚Äì\nMoCo v3 IN- 1K 300 83.2 84.1 ‚Äì ‚Äì\nBEiT IN- 1K 800 83.2 85.2 ‚Äì ‚Äì\nBootMAE IN- 1K 800 84.2 85.9 ‚Äì ‚Äì\nMAE IN- 1K 1600 83.6 85.9 86.9 87.8\nPeCo IN- 1K 800 84.5 86.5 87.5 88.3\nTable 1: Image classiÔ¨Åcation accuracy (%) comparison on\nImageNet-1K (IN-1K) of different self-supervised methods\nusing various backbones. We report Top-1 accuracy and our\nmethod PeCo outperforms previous self-supervised meth-\nods.\nMethods tokenizer tokenizer BERT pre- IN-1K\ndataset #params train epoch Top-1\nBEiT DALLE(400M) 53.8M 300/800 82.8/83.2\nPeCo IN-1K(1.3M) 37.5M 300/800 84.1/84.5\nPeColite IN-1K(1.3M) 25.7M 300/800 84.0/84.5\nTable 2: Tokenizer comparison with BEiT. Here we report\ntokenizer training dataset and #parameters. PeColite is a lite\nversion of PeCo that reduces the channel number of tok-\nenizer by half.\nComparison with previous works\nWe Ô¨Årst compare our PeCo with previous state-of-the-art\nworks. Here we report ImageNet-1K results with various\nmodel sizes. For object detection on CoCo and semantic seg-\nmentation on ADE20K, we use ViT-B as the backbone.\nImage ClassiÔ¨Åcation. The Top-1 accuracy on ImageNet-\n1K classiÔ¨Åcation is reported in Table 1. We compare our\nmethod with 1) ViT (Dosovitskiy et al. 2020) and DeiT (Tou-\nvron et al. 2021) that are supervisedly trained from scratch\nwith random initialization; and 2) MoCo v3 (Chen, Xie,\nand He 2021) and DINO (Caron et al. 2021), represent the\ncontrastive learning for self-supervised pre-training; and 3)\nBEiT (Bao, Dong, and Wei 2021), MAE (He et al. 2021) and\nBootMAE (Dong et al. 2022) based on masked image mod-\neling for self-supervised pre-training. It can be seen that our\nmodel (PeCo) signiÔ¨Åcantly improves the performance com-\npared with the models trained from scratch, suggesting the\neffectiveness of pre-training.\nCompared with prior self-supervised pre-training models,\nour model achieves the best performance. For example, our\nmodel using ViT-B backbone pre-trained with 800 epochs\nreaches 84.5% Top-1 accuracy, 1.3% higher than BEiT and\n0.9% higher than MAE. Furthermore, we also compare the\nresults on larger backbones, e.g. ViT-L and ViT-H. The re-\nsults are reported in the Table1, showing signiÔ¨Åcantly bet-\nMethods pre-train pre-train ADE-20K COCO\ndataset epochs mIoU AP bb APmk\nDEiT IN-1K 300 47.4 44.1 39.8\nMoCo IN-1K 300 47.3 44.9 40.4\nBEiT DALLE+IN-1K 800 47.1 46.3 41.1\nMAE IN-1K 800 47.6 46.8 41.9\nMAE IN-1K 1600 48.1 47.2 42.0\nPeCo IN-1K 800 48.5 47.8 42.6\nTable 3: Semantic segmentation mIoU (%) comparison on\nADE20K and object detection and instance segmentation\ncomparison in terms of box AP (APbb) and mask AP (APmk)\non COCO. The backbones for all the methods are the ViT-B.\nter performance than previous counterparts. This validates\nthat our perceptual codebook is indeed beneÔ¨Åcial for pre-\ntraining. Concretely, our model PeCo-H448 achieves the best\nTop-1 accuracy, 88.3%, on ImageNet-1K without external\ndata, outperforming MAE by 0.5%. This is a new state-of-\nthe-art result using only ImageNet-1K data.\nWe also report the results pre-trained with 300 epochs in\nTable 2. Compared with the baseline BEiT (Bao, Dong, and\nWei 2021), our model gets +1.3% improvement for both\n300 and 800 pre-training epochs. We further investigate a\nlite version of tokenizer which reduces the channel number\nof the original by half. This decreases the extra timecost in-\ntroduced by the tokenizer by about 2√ó. We can see from\nTable 2 that with a lite tokenizer, our model still gets com-\npetitive performance.\nSemantic segmentation. We compare our method with\n1) DEiT, which is a supervised pre-training method on\nImageNet-1K , 2) MoCo, the contrastive learning based\nmethods, and 3) BEiT (Bao, Dong, and Wei 2021),\nMAE (He et al. 2021), the state-of-the-art self-supervised\nlearning model. Here we use UperNet (Xiao et al. 2018)\nframework with 512 √ó512 input and trained for 160K iter-\nations. The evaluation metric is mean Intersection of Union\n(mIoU) averaged over all semantic categories and we report\nsingle-scale results here. The results are given in Table 3.\nOur method achieve 48.5 mIoU, +1.1 mIoU than supervised\nbased methods. It is also + 1.2 mIoU than MoCo, +1.4 mIoU\nthan BEiT, and +0.9 mIoU than MAE. Our model even\nachieve better results(+0.4 mIoU) than MAE pre-training\nwith 1600 epochs. This veriÔ¨Åes the effectiveness of the per-\nceptual codebook.\nObject detection and segmentation. We further investigate\nour transfer performance on object detection and segmen-\ntation. Here we use Mask-RCNN (He et al. 2017) frame-\nwork with single-scale input and 1√óschedule (12 epochs).\nWe compare with the strong competitor BEiT (Bao, Dong,\nand Wei 2021) on this dataset. The evaluation metric is box\nAP for detection and mask AP for segmentation. The com-\nparison is presented in Table 3. Our model with ViT-B as\nbackbone achieve 47.8 box AP and 42.6 mask AP, +3.7 box\nAP and +2.8 mask AP over supervised methods. Our model\nalso outperform recent work MAE by +1.0 box AP, + 0.7\nbox AP under the same pre-training epochs. Our model is\nMethods LinearProb. ClassiÔ¨Åcation.\non codewords on recon.\nDALL-E 6.1 18.2\nPeCo(w/o Lpercep) 10.2 17.9\nPeCo(ours) 29.7 51.7\nTable 4: Evaluation of the semantics of the codewords from\nlinear probling accuracy (%) of codewords on ImageNet-1K\nand classiÔ¨Åcation accuracy (%) on the reconstructed Ima-\ngeNet validation images using Deit-T.\nLoss for Tokenizer Training acc. on IN-1K\nLpixel 82.9\nLpixel + Lpercep from SSL ResNet-50 84.0\nLpixel + Lpercep from SSL ViT-B 84.1\nLpixel + Lpercep from Supervised VGG 84.1\nTable 5: The performance comparison when using different\narchitectures for calculating the perceptual similarity.\nalso higher than MAE pre-training with 1600 epochs.\nAnalysis of Perceptual Codebook\nIn this section, we ablate our perceptual codebook by using\nthe setting of self-supervised pre-training on ImageNet-1K.\nThe pre-raining epochs is 800.\nSemantics of the Codewords. The most important ques-\ntion would be:will the learned perceptual codewords exhibit\n(more) semantic meanings? To answer this, we quantita-\ntively evaluate the codewords‚Äô semantics from two aspects.\n(1) We use the codewords of the image as features for classi-\nÔ¨Åcation. An average pooling is conducted over the quantized\ncodewords of the image and we test its linear probing accu-\nracy over ImageNet dataset. (2) We use an ImageNet-1K su-\npervisedly pre-trained DeiT-T (Touvron et al. 2021) (72.2%\nTop1 accuracy on clean ImageNet val set) to test the clas-\nsiÔ¨Åcation accuracy over the reconstructed images. We com-\npare with the variant without using the perceptual similarity.\nThe results are given in Table 4. We Ô¨Ånd that our perceptual\ncodewords get much higher accuracy for both linear eval-\nuation on codewords and classiÔ¨Åcation on the reconstructed\nimages. This indicates that our perceptual codebook exhibits\nmore semantic meanings and beneÔ¨Åts the image reconstruc-\ntion process. We also provide a visualization of the masked\nregion prediction using BEiT (Bao, Dong, and Wei 2021)\nand our PeCo in Figure 4, showing that our PeCo, with the\naid of perceptual codebook, is able to make more semantic\npredictions for the masked region.\nDeep Architectures for Perceptual Similarity. Another\nkey question would be: will the deep architectures for deep\nperceptual features affect the perceptual codebook learning\nand thus affect the pre-training performance?Therefore, we\ninvestigate two different deep architectures: convolutional-\nbased backbone ResNet50 (He et al. 2016) and Transformer-\nbased model ViT-B (Dosovitskiy et al. 2020). We study the\nself-supervised models in order to enable unsupervised pre-\ntraining. The results are reported in Table 10. We can see\nthat using convolution-based or Transformer-based network\nOriginal Masked BEiT Ours Original Masked BEiT Ours Original Masked BEiT Ours\nFigure 4: Examples of reconstruction results on ImageNet-1K using BEiT and our PeCo.\nPerceptual mechanism Top-1 acc. on IN-1K\nClassiÔ¨Åcation loss on codewords 82.9\nContrastive loss on codewords 82.9\nPerceptual loss on images 84.1\nTable 6: Performance comparison of our implicit way (per-\nceptual loss on images) and the explicit ways (classiÔ¨Åca-\ntion/conrastive loss on codewords) for improving the per-\nceptual level of codebook.\nachieves similar performance. In addition, we also report\nthe results using the classical supervised ( i.e. using label)\ntrained VGG (Simonyan and Zisserman 2014) in Table 10.\nIt can be seen that using supervised model for perceptual\nmetric achieve comparable performance as self-supervised\nmodel.\nDiscussions\nWe present several in-depth discussions about the proposed\nmodel in this section.\nImplicit vs. Explicit. The key contribution of our paper\nis improving the perceptual level of the discrete visual to-\nkens for the subsequent pre-training. We have successfully\ndemonstrated that through a simple strategy, i.e. enforcing\nperceptual similarity over images. One may think that it\nseems quite implicit for learning perceptual codebook by\nconstraining on images instead of directly exploiting some\nconstraint over the codebook. Indeed, we also experiment\nin two explicit ways: 1) supervised classiÔ¨Åcation loss over\nthe codewords; 2) constraining a momentum contrastive loss\nover the quantized codewords through data augmentation in\na self-supervised way. We hope that leveraging those forms\nof high-level classiÔ¨Åcation objective may encode some se-\nmantics into the codewords. But empirically we found that\nsuch explicit ways are not as effective as the proposed im-\nplicit strategy. The results are reported in Table 6. We con-\njecture that the codebook may learn global semantics from\nthe classiÔ¨Åcation/contrastive loss and thus fail to differen-\ntiate different codewords, which is not suitable for pre-\ntraining. In contrast, deep features from a pre-trained deep\nmodel contain rich and dense semantics.\nPerceptual Loss vs. GAN Loss. The perceptual loss is\nLoss functions Top-1 acc. on IN-1K\nLpixel 82.9\nLpixel + Lpercep 84.1\nLpixel + Lpercep + Ladv 83.9\nTable 7: Performance comparison when using different loss\nfunctions. Adding an extra adversarial loss can not bring\ngain to the transfer performance.\nwidely used in generation tasks with the goal of improv-\ning the image quality. We ask the question that is there a\npositive relation with the image quality and the perceptual\nlevel of the codebook. In order to explore this, we adopt\nanother technique, adversarial loss in Generative Adversar-\nial Nets(GANs) (Goodfellow et al. 2014), which has been\nproved to be effective in enhancing the reconstructed image.\nSpeciÔ¨Åcally, we add a patch-based discriminator D (Li and\nWand 2016), aiming to make the original image and the re-\nconstructed one indistinguishable. The adversarial loss is,\nmin\nEnc,{V},Dec\nmax\nD\nLadv = logD(x) + log(1‚àíD(ÀÜx)). (8)\nWe add this loss with a suitable weight 0.4 to Eqn 5 and\nuse the learned codebook for pre-training. The resulting per-\nformance is shown in Table 7. We can see that adversarial\nloss can not bring gain to the transfer performance of pre-\ntraining.\nConclusion\nIn this paper, we argue that a good prediction target for\nmasked image modeling should agree with human percep-\ntion judgment. Motivated by this observation, we propose\na simple yet effective strategy to obtain perceptually dis-\ncrete tokens, beneÔ¨Åcial for BERT pre-training of vision\ntransformers. We present extensive comparisons on various\ndownstream tasks. Our results indeed validate our hypoth-\nesis and show superior performance compared with previ-\nous state-of-the-art methods. We hope that the deep anal-\nysis about the prediction target in our work will lead to a\nbroader exploration of this perspective and even help exist-\ning multi-modality foundation model pretraining (Yuan et al.\n2021; Wang et al. 2022a).\nAcknowledgements\nThis work was supported in part by the Natural Science\nFoundation of China under Grant U20B2047, 62072421,\n62002334, 62102386 and 62121002.\nReferences\nBachman, P.; Hjelm, R. D.; and Buchwalter, W. 2019.\nLearning representations by maximizing mutual information\nacross views. arXiv preprint arXiv:1906.00910.\nBaevski, A.; Hsu, W.-N.; Xu, Q.; Babu, A.; Gu, J.; and\nAuli, M. 2022. Data2vec: A general framework for self-\nsupervised learning in speech, vision and language. arXiv\npreprint arXiv:2202.03555.\nBao, H.; Dong, L.; and Wei, F. 2021. BEiT: BERT\nPre-Training of Image Transformers. arXiv preprint\narXiv:2106.08254.\nBengio, Y .; L¬¥eonard, N.; and Courville, A. 2013. Estimat-\ning or propagating gradients through stochastic neurons for\nconditional computation. arXiv preprint arXiv:1308.3432.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165.\nBruna, J.; Sprechmann, P.; and LeCun, Y . 2015. Super-\nresolution with deep convolutional sufÔ¨Åcient statistics.arXiv\npreprint arXiv:1511.05666.\nCaron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.;\nand Joulin, A. 2020. Unsupervised learning of visual fea-\ntures by contrasting cluster assignments. arXiv preprint\narXiv:2006.09882.\nCaron, M.; Touvron, H.; Misra, I.; J ¬¥egou, H.; Mairal, J.;\nBojanowski, P.; and Joulin, A. 2021. Emerging proper-\nties in self-supervised vision transformers. arXiv preprint\narXiv:2104.14294.\nChen, D.; Yuan, L.; Liao, J.; Yu, N.; and Hua, G. 2017.\nStylebank: An explicit representation for neural image style\ntransfer. In CVPR, 1897‚Äì1906.\nChen, K.; Wang, J.; Pang, J.; Cao, Y .; Xiong, Y .; Li, X.; Sun,\nS.; Feng, W.; Liu, Z.; Xu, J.; Zhang, Z.; Cheng, D.; Zhu, C.;\nCheng, T.; Zhao, Q.; Li, B.; Lu, X.; Zhu, R.; Wu, Y .; Dai,\nJ.; Wang, J.; Shi, J.; Ouyang, W.; Loy, C. C.; and Lin, D.\n2019. MMDetection: Open MMLab Detection Toolbox and\nBenchmark. arXiv preprint arXiv:1906.07155.\nChen, M.; Radford, A.; Child, R.; Wu, J.; Jun, H.; Luan, D.;\nand Sutskever, I. 2020a. Generative pretraining from pixels.\nIn ICML, 1691‚Äì1703. PMLR.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020b.\nA simple framework for contrastive learning of visual repre-\nsentations. In ICML, 1597‚Äì1607. PMLR.\nChen, T.; Kornblith, S.; Swersky, K.; Norouzi, M.; and Hin-\nton, G. 2020c. Big self-supervised models are strong semi-\nsupervised learners. arXiv preprint arXiv:2006.10029.\nChen, X.; Ding, M.; Wang, X.; Xin, Y .; Mo, S.; Wang, Y .;\nHan, S.; Luo, P.; Zeng, G.; and Wang, J. 2022. Context\nAutoencoder for Self-Supervised Representation Learning.\narXiv preprint arXiv:2202.03026.\nChen, X.; Fan, H.; Girshick, R.; and He, K. 2020d. Improved\nbaselines with momentum contrastive learning. arXiv\npreprint arXiv:2003.04297.\nChen, X.; Xie, S.; and He, K. 2021. An empirical study of\ntraining self-supervised vision transformers. arXiv preprint\narXiv:2104.02057.\nContributors, M. 2020. MMSegmentation, an Open Source\nSemantic Segmentation Toolbox. https://github.com/open-\nmmlab/mmsegmentation.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 248‚Äì255. Ieee.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDong, X.; Bao, J.; Zhang, T.; Chen, D.; Zhang, W.; Yuan, L.;\nChen, D.; Wen, F.; and Yu, N. 2022. Bootstrapped Masked\nAutoencoders for Vision BERT Pretraining. In ECCV.\nDong, Y .; Liao, F.; Pang, T.; Su, H.; Zhu, J.; Hu, X.; and Li,\nJ. 2018. Boosting adversarial attacks with momentum. In\nCVPR, 9185‚Äì9193.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nDosovitskiy, A.; Springenberg, J. T.; Riedmiller, M.; and\nBrox, T. 2014. Discriminative unsupervised feature learning\nwith convolutional neural networks. NeurIPs, 27: 766‚Äì774.\nEsser, P.; Rombach, R.; and Ommer, B. 2021. Taming\ntransformers for high-resolution image synthesis. In CVPR,\n12873‚Äì12883.\nGatys, L. A.; Ecker, A. S.; and Bethge, M. 2016. Image\nstyle transfer using convolutional neural networks. InCVPR,\n2414‚Äì2423.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2014. Generative adversarial nets. NeurIPs, 27.\nGoyal, R.; Ebrahimi Kahou, S.; Michalski, V .; Materzyn-\nska, J.; Westphal, S.; Kim, H.; Haenel, V .; Fruend, I.; Yian-\nilos, P.; Mueller-Freitag, M.; et al. 2017. The‚Äù something\nsomething‚Äù video database for learning and evaluating vi-\nsual common sense. In ICCV, 5842‚Äì5850.\nGrill, J.-B.; Strub, F.; Altch ¬¥e, F.; Tallec, C.; Richemond,\nP. H.; Buchatskaya, E.; Doersch, C.; Pires, B. A.; Guo, Z. D.;\nAzar, M. G.; et al. 2020. Bootstrap your own latent: A\nnew approach to self-supervised learning. arXiv preprint\narXiv:2006.07733.\nHadsell, R.; Chopra, S.; and LeCun, Y . 2006. Dimension-\nality reduction by learning an invariant mapping. In CVPR,\nvolume 2, 1735‚Äì1742. IEEE.\nHe, K.; Chen, X.; Xie, S.; Li, Y .; Doll¬¥ar, P.; and Girshick, R.\n2021. Masked Autoencoders Are Scalable Vision Learners.\narXiv preprint arXiv:2111.06377.\nHe, K.; Fan, H.; Wu, Y .; Xie, S.; and Girshick, R. 2020.\nMomentum contrast for unsupervised visual representation\nlearning. In CVPR, 9729‚Äì9738.\nHe, K.; Gkioxari, G.; Doll¬¥ar, P.; and Girshick, R. 2017. Mask\nr-cnn. In ICCV, 2961‚Äì2969.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR, 770‚Äì778.\nHuang, G.; Sun, Y .; Liu, Z.; Sedra, D.; and Weinberger,\nK. Q. 2016. Deep networks with stochastic depth. In ECCV,\n646‚Äì661. Springer.\nJohnson, J.; Alahi, A.; and Fei-Fei, L. 2016. Perceptual\nlosses for real-time style transfer and super-resolution. In\nECCV, 694‚Äì711. Springer.\nJoshi, M.; Chen, D.; Liu, Y .; Weld, D. S.; Zettlemoyer, L.;\nand Levy, O. 2020. Spanbert: Improving pre-training by rep-\nresenting and predicting spans. Transactions of the Associ-\nation for Computational Linguistics, 8: 64‚Äì77.\nKay, W.; Carreira, J.; Simonyan, K.; Zhang, B.; Hillier, C.;\nVijayanarasimhan, S.; Viola, F.; Green, T.; Back, T.; Natsev,\nP.; et al. 2017. The kinetics human action video dataset.\narXiv preprint arXiv:1705.06950.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKurakin, A.; Goodfellow, I.; Bengio, S.; et al. 2016. Adver-\nsarial examples in the physical world.\nLedig, C.; Theis, L.; Husz¬¥ar, F.; Caballero, J.; Cunningham,\nA.; Acosta, A.; Aitken, A.; Tejani, A.; Totz, J.; Wang, Z.;\net al. 2017. Photo-realistic single image super-resolution us-\ning a generative adversarial network. In CVPR, 4681‚Äì4690.\nLi, C.; and Wand, M. 2016. Precomputed real-time texture\nsynthesis with markovian generative adversarial networks.\nIn ECCV, 702‚Äì716. Springer.\nLi, S.; Chen, D.; Chen, Y .; Yuan, L.; Zhang, L.; Chu, Q.; Liu,\nB.; and Yu, N. 2021. Improve Unsupervised Pretraining for\nFew-label Transfer. In ICCV.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ¬¥ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In ECCV, 740‚Äì755.\nSpringer.\nLiu, X.; Zhang, F.; Hou, Z.; Mian, L.; Wang, Z.; Zhang, J.;\nand Tang, J. 2021a. Self-supervised learning: Generative or\ncontrastive. TKDE.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021b. Swin transformer: Hierarchical\nvision transformer using shifted windows. arXiv preprint\narXiv:2103.14030.\nLoshchilov, I.; and Hutter, F. 2017. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101.\nMantiuk, R.; Kim, K. J.; Rempel, A. G.; and Heidrich, W.\n2011. HDR-VDP-2: A calibrated visual metric for visibility\nand quality predictions in all luminance conditions. TOG,\n30(4): 1‚Äì14.\nOord, A. v. d.; Kalchbrenner, N.; Vinyals, O.; Espeholt,\nL.; Graves, A.; and Kavukcuoglu, K. 2016. Conditional\nimage generation with pixelcnn decoders. arXiv preprint\narXiv:1606.05328.\nOord, A. v. d.; Li, Y .; and Vinyals, O. 2018. Representation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748.\nOord, A. v. d.; Vinyals, O.; and Kavukcuoglu, K. 2017.\nNeural discrete representation learning. arXiv preprint\narXiv:1711.00937.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\nSutskever, I.; et al. 2019. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8): 9.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; V oss, C.; Rad-\nford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-\nto-image generation. arXiv preprint arXiv:2102.12092.\nSennrich, R.; Haddow, B.; and Birch, A. 2015. Neural ma-\nchine translation of rare words with subword units. arXiv\npreprint arXiv:1508.07909.\nSimonyan, K.; and Zisserman, A. 2014. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J¬¥egou, H. 2021. Training data-efÔ¨Åcient image trans-\nformers & distillation through attention. In ICML, 10347‚Äì\n10357. PMLR.\nVan Oord, A.; Kalchbrenner, N.; and Kavukcuoglu, K. 2016.\nPixel recurrent neural networks. In ICML, 1747‚Äì1756.\nPMLR.\nWang, J.; Chen, D.; Wu, Z.; Luo, C.; Zhou, L.; Zhao, Y .;\nXie, Y .; Liu, C.; Jiang, Y .-G.; and Yuan, L. 2022a. OmniVL:\nOne Foundation Model for Image-Language and Video-\nLanguage Tasks. In NeurIPS.\nWang, R.; Chen, D.; Wu, Z.; Chen, Y .; Dai, X.; Liu, M.;\nJiang, Y .-G.; Zhou, L.; and Yuan, L. 2022b. BEVT: BERT\nPretraining of Video Transformers. In CVPR.\nWang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P.\n2004. Image quality assessment: from error visibility to\nstructural similarity. TIP, 13(4): 600‚Äì612.\nWang, Z.; Simoncelli, E. P.; and Bovik, A. C. 2003. Multi-\nscale structural similarity for image quality assessment. In\nThe Thrity-Seventh Asilomar Conference on Signals, Sys-\ntems & Computers, 2003, volume 2, 1398‚Äì1402. Ieee.\nWei, C.; Fan, H.; Xie, S.; Wu, C.-Y .; Yuille, A.; and\nFeichtenhofer, C. 2021. Masked Feature Prediction\nfor Self-Supervised Visual Pre-Training. arXiv preprint\narXiv:2112.09133.\nWu, Y .; and He, K. 2018. Group normalization. In ECCV,\n3‚Äì19.\nXiao, T.; Liu, Y .; Zhou, B.; Jiang, Y .; and Sun, J. 2018. Uni-\nÔ¨Åed perceptual parsing for scene understanding. In ECCV,\n418‚Äì434.\nXie, Z.; Zhang, Z.; Cao, Y .; Lin, Y .; Bao, J.; Yao, Z.; Dai, Q.;\nand Hu, H. 2021. Simmim: A simple framework for masked\nimage modeling. arXiv preprint arXiv:2111.09886.\nYuan, L.; Chen, D.; Chen, Y .-L.; Codella, N.; Dai, X.; Gao,\nJ.; Hu, H.; Huang, X.; Li, B.; Li, C.; et al. 2021. Florence: A\nnew foundation model for computer vision. arXiv preprint\narXiv:2111.11432.\nZhang, L.; Zhang, L.; Mou, X.; and Zhang, D. 2011. FSIM:\nA feature similarity index for image quality assessment.TIP,\n20(8): 2378‚Äì2386.\nZhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang,\nO. 2018. The unreasonable effectiveness of deep features as\na perceptual metric. In CVPR, 586‚Äì595.\nZheng, Y .; Yang, H.; Zhang, T.; Bao, J.; Chen, D.; Huang,\nY .; Yuan, L.; Chen, D.; Zeng, M.; and Wen, F. 2022. Gen-\neral Facial Representation Learning in a Visual-Linguistic\nManner. In CVPR.\nZhou, B.; Zhao, H.; Puig, X.; Fidler, S.; Barriuso, A.; and\nTorralba, A. 2017. Scene parsing through ade20k dataset. In\nCVPR, 633‚Äì641.\nMore Experiments\nAccelerated BERT pre-training over PeCo. Recent work\nMAE (He et al. 2021) introduces an asymmetric encoder-\ndecoder design where the masked tokens are shifted from\nthe encoder to the small decoder. This results in a large re-\nduction in computation. Inspired from this, we can also ac-\ncelerate PeCo by adopting the network structure in (He et al.\n2021) for BERT pre-training but with the proposed percep-\ntual codebook as prediction targets. We denote perceptual\ncodebook using MAE framework as PeCo MAE. We show\nthat PeCoMAE enjoys the efÔ¨Åciency of the framework while\nimproving the performance through the proposed prediction\ntarget.\nHere we show the results of adopting the accelerated\nBERT pre-training paradigm but with the proposed percep-\ntual codebook as prediction target. The comparison is shown\nin Table 8 for all the three downstream tasks. We can see that\nour new prediction target enjoys the efÔ¨Åciency of the frame-\nwork and also gets a higher downstream performance.\nMethods pre-train IN-1K ADE-20K COCO\nepochs Acc. mIoU AP bb APmk\nMAE 800 83.4 47.6 46.8 41.9\nPeCo MAE 800 84.2 48.2 47.3 42.2\nTable 8: The effect of PeCo using accelerated BERT pre-\ntraining compared on (a) image classiÔ¨Åcation, (b) semantic\nsegmentation, and (c) object detection and instance segmen-\ntation.\nExtend PeCo to Video-level Tasks. In our main paper, we\nexplore PeCo on different image-level downstream tasks,\nhere we further apply PeCo on video-level tasks. We apply\nPeCo to video recognition task with two wildly used dataset\nKinetics-400 (K400) (Kay et al. 2017) and something-\nsomething-v2 (SSv2) (Goyal et al. 2017). We use TimeS-\nformer and initial it with model pretrained on ImageNet-1K,\nand we use clips of size 8 √ó224 √ó224 and patch size is\nset to 16 √ó16. For PeCo and BEiT, we Ô¨Ånetune it with 15\nepochs, learning rate is set to1.2e‚àí3 and layer decay is 0.65.\nWeight decay is set to 1e‚àí4. For supervised baseline DEiT,\nwe set the learning rate as 1e‚àí4 for the backbone and 1e‚àí3\nfor the classiÔ¨Åcation head. The batch size is set to 64 for all\nexperiments.\nAs shown in Table 9, our PeCo outperforms the super-\nvised baseline DEiT and previous BEiT with a large margin,\nthis proves the effectiveness and generalizability of PeCo.\nModels Pre-Train Epoch K400 Acc SSv2 Acc\nTraining from scratch (i.e., random initialization)\nDEiT-B ‚Äì 75.4 57.6\nSelf-Supervised Pre-Training on ImageNet-1K\nBEiT 800 75.5 60.2\nPeCo(ours) 800 76.5 61.8\nTable 9: Extend PeCo to video recognition task.\nThe Loss Weight of Perceptual Similarity. In the exper-\niments, the loss weight Œª in Eqn 5 is set as 1. Here we\npresent the performance under various values of Œª among\n0, 0.3, 1, 3, 10. The results are shown in Table 10. We can\nsee that using perceptual loss yields 84.1% accuracy out-\nperforming 82.9% from the model without perceptual loss.\nHowever, further enlarging the loss weight gets performance\ndrop. One possible explanation is that large perceptual loss\nleads the model to pay more attention to semantic while lose\nsome local details, while a good codebook for BERT pre-\ntraining needs both semantic and local details.\nŒª 0 0.3 1 3 10\nImagenNet-1K 82.9 84.1 84.1 83.6 83.5\nTable 10: Illustrating the effect of loss weight of perceptual\nsimilarity. We show Ô¨Åne-tune accuracy (%) on ImageNet-\n1K. Enlarging the loss weight can not get consistent im-\nprovement, may due to the loss of local details.\nAdversarial Robustness Analysis. Here we provide anal-\nysis about the Ô¨Åne-tuned model adversarial robustness of\ndifferent pretraining methods. Here we use two classical\nwhite-box attack method Basic Iteration Attach Method\n(BIM) (Kurakin et al. 2016) and Momentum Iteration At-\ntach Method (MIM) (Dong et al. 2018) The attack threshold\nis 2/255 and iterations are 20.\nAs shown in Table 11, we Ô¨Ånd that compared with the\nvanilla DEiT, both contrastive-learning based method MoCo\nand mask image modeling based method BEiT and PeCo im-\nproves the adversarial robustness and PeCo performs best.\nWhile an interesting point is that only the robustness of\nMAE is worse than the baseline. We argue this may be\nbecause the prediction target of MAE is raw pixels (with\nsimple pixel norm), so it pays more attention to the high-\nfrequency of the input, which makes it sensitive to the high-\nfrequency change of the input. On the contrary, BEiT and\nPeCo predicts tokens, which could be viewed as clustered\nand distillate target, so the model could focus on the struc-\nture or semantic information of the input, rather than the\nhigh-frequency information.\nModels Pre-Train Epoch Clean BIM MIM\nTraining from scratch (i.e., random initialization)\nDeiT-B ‚Äì 81.8 46.2 50.2\nSelf-Supervised Pre-Training on ImageNet-1K\nMoCo v3 300 83.2 51.8 54.9\nBEiT 300 82.8 50.2 53.2\nMAE 1600 83.6 37.2 42.2\nPeCo(ours) 300 84.1 52.5 55.3\nTable 11: Adversarial robustness analysis on different self-\nlearning methods.\nDifferent Architectures for VQ-V AE.Here we investigate\nthe performance when using different architectures for VQ-\nV AE. We consider several variants of the network architec-\nture. For encoder, we explore three models: 1) 16 √ódown-\nsample encoder (our default setting); 2) 8 √ódown sample\nencoder; 3) ViT-B (16 √ódown-sample). For the 8 √ódown-\nsample encoder, we remove one stage and train it with im-\nages of 112 √ó112 resolution. For decoder, we use the in-\nversed version of the corresponding decoder. The results on\nImageNet-1K dataset are shown in Table 12. We observe\nthat CNN based encoders and decoders achieve better results\nthan vision Transformer. We further reduce the parameters\nof decoder by decreasing the channel number or decreasing\nthe depth of the network by half. Results shown in Table 12\nsuggest that reducing the parameters of decoder may not hurt\nthe Ô¨Åne-tuning performance of PeCo.\nEncoder of VQ-V AE Decoder of VQ-V AE IN-1K Acc\nViT-B ViT-B 83.6\nCNN(8x) CNN(8x) 83.8\nCNN(16x) CNN(16x) 84.1\nCNN(16x) CNN(16x) (Half Channel) 84.0\nCNN(16x) CNN(16x) (Half Depth) 84.1\nTable 12: Illustrating the effect of different architectures for\ntraining PeCo. CNN based encoders and decoders achieve\nbetter results than vision Transformer.\nExperiment Details\nIn this section, we provide more detailed experimental set-\ntings about downstream tasks.\nVQ-V AE Architectures. For convolutional encoder, the\nnumber of channels at the Ô¨Årst stage is set to 64, then it will\nbe doubled in every downsample operation. we apply the\nGroup Normalization (Wu and He 2018) as introduced in\nTaming Transformer (Esser, Rombach, and Ommer 2021).\nThe convolutional decoder is an inverse version of the en-\ncoder. For ViT-base encoder, we use the original structure,\nand use the inverse version of ViT as decoder.\nPerceptual Codebook Learning Setup. We train the per-\nceptual codebook using the training set of ImageNet-1K\ndataset by default. For the encoder and decoder of VQ-V AE,\nwe choose traditional convolutional based backbone. The\nnetwork contains two residual blocks at each resolution. A\nself-attention block is applied to the smallest resolution for\nboth encoder and decoder. For perceptual loss, we use the\npre-trained 100 epochs ViT-B model from self-supervised\nmethod MoCo v3 (Chen, Xie, and He 2021) by default,\nand the 3rd, 6th, 9th, and 12nd layer are selected for deep\nfeatures. We also apply the ResNet50 (He et al. 2016) and\nVGG (Simonyan and Zisserman 2014) model with the per-\nceptual similarity calculated at the end of each stage. We set\nthe perceptual loss weightŒªto 1 without special noting. Dif-\nferent models for providing deep features for perceptual loss\nare ablated in the experiments section. The input image size\nis 224 √ó224, which is consistent with pre-training image\ninput size, the latent codes are in a resolution of 16 √ó16.\nWe use EMA vector quantizer as the default quantizer algo-\nrithm. The learning rate is set 5e‚àí5 with batchsize 128. We\ntrain the PeCo for 100 epochs and warm up the Ô¨Årst 5000 it-\nerations to stabilize the training process. The Adam (Kingma\nand Ba 2014) optimizer is used withŒ≤1 and Œ≤2 set to 0.5 and\n0.95 respectively.\nBERT Pre-training Setup. For computation resource con-\nsideration, we use the original ViT-B/16 (Dosovitskiy et al.\n2020) as the basic architecture of our backbone to vali-\ndate the effectiveness of the learned visual codebook, as in\nBEiT (Bao, Dong, and Wei 2021). The model is pre-trained\nfor 300/800 epochs with the batchsize of 2048. AdamW op-\ntimizer is adopted with learning rate, Œ≤1, Œ≤2, weight decay\nset to 1.5e‚àí3, 0.9, 0.999, and 0.05 respectively. We also ap-\nply stochastic depth (Huang et al. 2016) with 0.1 rate. We\nuse a block-wise masking strategy for obtaining the cor-\nrupted images with the same setup as BEiT (Bao, Dong, and\nWei 2021). We further demonstrate the effectiveness of our\napproach when scaling to ViT-Large and ViT-Huge back-\nbones.\nADE20K Semantic segmentation. Here we use: Uper-\nNet (Xiao et al. 2018) based on the implementation from\nmmsegmentaion (Contributors 2020). For UperNet, we fol-\nlow the settings in (Bao, Dong, and Wei 2021) and use\nAdamW (Loshchilov and Hutter 2017) optimizer with initial\nlearning rate 3e‚àí4, weight decay of 0.05 and batch size of 16\n(8 GPUs with 2 images per GPU) for 160K iterations. The\nlearning rate warmups with 1500 iterations at the beginning\nand decays with a linear decay strategy. We use the layer\ndecay (Bao, Dong, and Wei 2021) for the backbone and\nwe set it as 0.65. As the ViT architecture outputs features\nwith the same size, here we add four different scale FPNs\nto scale the feature map into different size. SpeciÔ¨Åcally, we\nupsample the output feature of the 4thblock 4√ó, upsample\nthe output feature of the 6thblock 2√ó, keep the output fea-\nture of the 8thblock unchanged and downsample the output\nfeature of the 12thblock 2√ó. We use the default augmenta-\ntion setting in mmsegmentation including random horizontal\nÔ¨Çipping, random re-scaling (ratio range [0.5, 2.0]) and ran-\ndom photo-metric distortion. All the models are trained with\ninput size 512√ó512. The stochastic depth is set to 0.2. When\nit comes to testing, we report single-scale test result.\nCOCO Object Detection and Instance Segmentation.\nWe use the classical object detection framework Mask R-\nCNN (He et al. 2017) based on the implementation from\nmmdetection (Chen et al. 2019). We train it the1√óschedule\nwith single-scale input (image is resized so that the shorter\nside is 800 pixels, while the longer side does not exceed\n1333 pixels) for 12 epochs. We use AdamW (Loshchilov and\nHutter 2017) optimizer with a learning rate of 4e‚àí4, weight\ndecay of 0.05 and batch size of 16. We also use the layer\ndecay (Bao, Dong, and Wei 2021) for the backbone and we\nset it as 0.75. The learning rate declines at the 8thand 11th\nepoch with decay rate being 0.1. The stochastic depth is set\nto 0.1. Similar to the implementation of semantic segmen-\ntation above, we also use four different scale FPNs to scale\nthe feature map into different size.\nMore visual results\nIn Figure.5, we show the reconstruction results with a dif-\nferent number of patches masked. We Ô¨Ånd that PeCo learns\nstrong semantic that could predict a reasonable object with\nlimited visible patches.\nFigure 5: Example samples of reconstruction task on ImageNet-1kusing our PeCo with different mask regions. For each sample,\nthe Ô¨Årst image is the original image, the second one is the corresponding masked image, the third one is the reconstruction from\nperceptual codebook (PeCo). The Ô¨Årst row masks 45 patches, the second row masks 75 patches and the lost row masks 120\npatches.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7026962041854858
    },
    {
      "name": "Codebook",
      "score": 0.6891430020332336
    },
    {
      "name": "Transformer",
      "score": 0.6609716415405273
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6589616537094116
    },
    {
      "name": "Segmentation",
      "score": 0.6495071649551392
    },
    {
      "name": "Perception",
      "score": 0.6415982246398926
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4417518079280853
    },
    {
      "name": "Machine learning",
      "score": 0.38512012362480164
    },
    {
      "name": "Speech recognition",
      "score": 0.3202534317970276
    },
    {
      "name": "Engineering",
      "score": 0.09369823336601257
    },
    {
      "name": "Psychology",
      "score": 0.077765554189682
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}