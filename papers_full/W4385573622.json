{
  "title": "Discovering Language-neutral Sub-networks in Multilingual Language Models",
  "url": "https://openalex.org/W4385573622",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2591832737",
      "name": "Negar Foroutan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3033555308",
      "name": "Mohammadreza Banaei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2022854812",
      "name": "Rémi Lebret",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2770217800",
      "name": "Antoine Bosselut",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A150096297",
      "name": "Karl Aberer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W2996074092",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W3175374354",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W2962735107",
    "https://openalex.org/W3087873698",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2987270981",
    "https://openalex.org/W3100308117",
    "https://openalex.org/W2948130861",
    "https://openalex.org/W3103754749",
    "https://openalex.org/W3105005398",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2985620815",
    "https://openalex.org/W4297730150",
    "https://openalex.org/W4286856918",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W4297790889",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2100235303",
    "https://openalex.org/W3105788222",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W1554336200",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3122044994",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W2948253213",
    "https://openalex.org/W2942810103",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W3103490574",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3186903869",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W3206907172",
    "https://openalex.org/W2915589364",
    "https://openalex.org/W4294103325"
  ],
  "abstract": "Multilingual pre-trained language models transfer remarkably well on cross-lingual downstream tasks. However, the extent to which they learn language-neutral representations (i.e., shared representations that encode similar phenomena across languages), and the effect of such representations on cross-lingual transfer performance, remain open questions.In this work, we conceptualize language neutrality of multilingual models as a function of the overlap between language-encoding sub-networks of these models. We employ the lottery ticket hypothesis to discover sub-networks that are individually optimized for various languages and tasks. Our evaluation across three distinct tasks and eleven typologically-diverse languages demonstrates that sub-networks for different languages are topologically similar (i.e., language-neutral), making them effective initializations for cross-lingual transfer with limited performance degradation.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7560–7575\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nDiscovering Language-neutral Sub-networks\nin Multilingual Language Models\nNegar Foroutan Mohammadreza Banaei Rémi Lebret\nAntoine Bosselut Karl Aberer\n{firstname.lastname}@epfl.ch\nEPFL\nAbstract\nMultilingual pre-trained language models trans-\nfer remarkably well on cross-lingual down-\nstream tasks. However, the extent to which\nthey learn language-neutral representations\n(i.e., shared representations that encode similar\nphenomena across languages), and the effect of\nsuch representations on cross-lingual transfer\nperformance, remain open questions.\nIn this work, we conceptualize language neu-\ntrality of multilingual models as a function of\nthe overlap between language-encoding sub-\nnetworks of these models. We employ the lot-\ntery ticket hypothesis to discover sub-networks\nthat are individually optimized for various lan-\nguages and tasks. Our evaluation across three\ndistinct tasks and eleven typologically-diverse\nlanguages demonstrates that sub-networks for\ndifferent languages are topologically similar\n(i.e., language-neutral), making them effective\ninitializations for cross-lingual transfer with\nlimited performance degradation.1\n1 Introduction\nMultilingual language models (MultiLMs) such\nas mBERT (Devlin et al., 2019), XLM (Conneau\nand Lample, 2019), and XLM-R (Conneau et al.,\n2020a) are pre-trained jointly on raw data from mul-\ntiple languages. Later, when they are fine-tuned\nfor a task using perhaps a single high-resource lan-\nguage dataset, they demonstrate promising zero-\nshot cross-lingual performance, generalizing to the\nsame task in different languages despite not having\nbeen fine-tuned for those languages.\nThe facilitator of this cross-lingual transfer abil-\nity is often hypothesized to be a learned language\nneutrality in these models ( i.e., similar linguistic\nphenomena across languages are represented sim-\nilarly by the model). However, the source of this\nability remains an open question. Certain studies\n1Our code is available at https://github.com/negar-\nforoutan/multiLMs-lang-neutral-subnets.\nUrdu\nFrench\n(a) Step 1: Discover sub-networks in multilingual\nlanguage models that encode particular languages\nFrenchUrduFrenchUrdu\n✓ ✓✓\n(b) Step 2: Transfer sub-networks to new lan-\nguages to evaluate their language neutrality degree\nFigure 1: Overview of our approach. We discover sub-\nnetworks in the original multilingual language model\nthat are good foundations for learning various tasks and\nlanguages (a). Then, we investigate to what extent these\nsub-networks are similar by transferring them across\nother task-language pairs (b). In this example, the blue\nand red lines show sub-networks found for French and\nUrdu, respectively, and purple connections are shared\nin both sub-networks. Dashed lines show the weights\nthat are removed in the pruning phase.\nhave investigated mBERT (a prominent MultiLM)\nfor universal language-neutral components that\nwould facilitate cross-lingual transfer (Libovický\net al., 2019; Pires et al., 2019; Libovický et al.,\n2020). Meanwhile, other studies claim mBERT is\nnot language-neutral as it partitions its multilingual\nsemantic space into separate language-specific sub-\nspaces (Singh et al., 2019; Choenni and Shutova,\n2020; Wu and Dredze, 2019). However, most of\nthese prior works analyze the language neutrality\n7560\nof MultiLMs by probing their output contextual\nrepresentations.\nIn this paper, language-neutrality is instead con-\nceptualized in terms of the parameters of Multi-\nLMs. We hypothesize that a multilingual model\nwith language-neutral representations would have\nlearned different languages using the same sub-\nset of parameters in its network structure. Sub-\nnetworks in MultiLMs that overlap across lan-\nguages, and transfer well when re-trained on other\nlanguages from the pre-training corpus, would in-\ndicate that the model is comprised of language-\nneutral representations that jointly encode multiple\nlanguages. To demonstrate this effect, we extract\nsub-networks from MultiLMs by pruning them\nfor individual languages and task pairs using it-\nerative magnitude pruning (Frankle and Carbin,\n2019) (Figure 1a). Then, we evaluate the cross-\nlingual transfer of sub-networks by re-initializing\nto the unpruned parameters of the original Mul-\ntiLM, and re-training on task data for different lan-\nguages (Figure 1b).\nOur results on three tasks, namely masked lan-\nguage modeling, named entity recognition (Pan\net al., 2017), and natural language inference (Con-\nneau et al., 2018), show high absolute parameter\noverlap among sub-networks discovered for differ-\nent languages and effective cross-lingual transfer\nbetween languages on the same task (even outper-\nforming the original multilingual model for certain\nlow-resource languages). However, cross-lingual\nsub-network transfer deteriorates as we increase\nthe sparsity level of the sub-networks, suggesting\nthat language-neutral components of the MultiLMs\nare pruned in favor of retaining necessary language-\nand task-specific components for the language-task\npair used to discover the sub-network.\n2 Background & Motivation\nMultilingual Language Models. Multilingual\nlanguage models (MultiLMs) such as mBERT (De-\nvlin et al., 2019), XLM (Conneau and Lample,\n2019), and XLM-R (Conneau et al., 2020a) have\nachieved state of the art results in cross-lingual\nunderstanding tasks by jointly pre-training Trans-\nformer models (Vaswani et al., 2017) on many lan-\nguages. Specifically, mBERT has shown effective\ncross-lingual transfer for many tasks, including\nnamed entity recognition (Pires et al., 2019; Wu\nand Dredze, 2019), cross-lingual natural language\ninference (Conneau et al., 2018; Hu et al., 2020),\nand question answering (Lewis et al., 2020).\nDue to these impressive cross-lingual transfer\nresults, many recent works investigate the source\nof this capacity. One line of study investigates the\neffect of different pre-training settings (e.g., shared\nvocabulary, shared parameters, joint multilingual\npre-training, etc.) on cross-lingual transfer (K et al.,\n2020; Artetxe et al., 2020; Conneau et al., 2020b).\nOther works explore how learned multilingual rep-\nresentations are partitioned into language-specific\nsubspaces (Singh et al., 2019; Wu and Dredze,\n2019; Choenni and Shutova, 2020). In contrast\nto these works, our study explores the existence\nof language-neutral parameters in MultiLMs. Us-\ning iterative magnitude pruning, we extract pruned\nsub-networks from multilingual LMs for various\ntasks and languages, and investigate to what extent\nthese sub-networks are similar by transferring them\nacross languages.\nAnalysis of Multilingual Representations.\nPrior work has demonstrated the lack of language\nneutrality in mBERT by comparing the similarity\nof mBERT’s encodings for semantically-similar\nsentences across multiple languages (Singh et al.,\n2019), concluding that mBERT partitions the repre-\nsentation space among languages rather than using\na shared, interlingual space.2 In opposition, other\nefforts show that mBERT learns language-neutral\nrepresentation space that facilitates cross-lingual\ntransfer (Pires et al., 2019; Libovický et al., 2019;\nLibovický et al., 2020). These findings have led\nto attempts to disentangle the language-specific\nand language-neutral components of mBERT to\nimprove its performance (Libovický et al., 2019;\nLibovický et al., 2020; Wang et al., 2020; Gonen\net al., 2020; Zhao et al., 2021; Lin et al., 2021).\nThe language-neutral component itself can be\nviewed as the stacking of two sub-networks: a\nmultilingual encoder followed by a task-specific\nlanguage-agnostic predictor (Müller et al., 2021).\nIn this work, we discover language-neutral com-\nponents in the parameters of MultiLMs by pruning\nthem for different language-task pairs. By transfer-\nring sub-networks across languages, we investigate\nto what extent these sub-networks are language-\nneutral.\nLottery Ticket Hypothesis. The lottery ticket\nhypothesis (LTH) shows that dense, randomly-\n2In Appendix F, we discuss how the choice of the similarity\nmetric can affect the results of such analysis.\n7561\ninitialized neural networks contain small, sparse\nsub-networks (i.e., winning tickets) capable of be-\ning trained in isolation to reach the accuracy of the\noriginal network (Frankle and Carbin, 2019). This\nphenomenon has been observed in multiple appli-\ncations, including computer vision (Morcos et al.,\n2019; Frankle et al., 2020) and natural language\nprocessing (Gale et al., 2019; Yu et al., 2020).\nIn particular, the lottery ticket hypothesis has pre-\nviously been applied to the BERT model (Devlin\net al., 2019) using the GLUE benchmark (Wang\net al., 2019; Prasanna et al., 2020; Chen et al.,\n2020). Importantly, Chen et al. (2020) show that\nsub-networks found using the masked language\nmodeling task can transfer to other tasks. In the\ncontext of multilingual language models, (Ansell\net al., 2022) propose a sparse LTH-based fine-\ntuning method to benefit from both modular and\nexpressive fine-tuning approaches. In this work,\nwe specifically use the lottery ticket hypothesis\nto examine the possibility of transferring winning\ntickets across languages, thereby disentangling\nwhether language-neutral and language-specific\ncomponents of mBERT emerge in winning tick-\nets of different languages.\n3 Methodology\nIn this section, we define sub-networks (i.e., win-\nning tickets in the LTH) and identify our approach\nto discovering them through a pruning algorithm\nderived from the lottery ticket hypothesis.\nSub-networks. For any network f(x; θ) with ini-\ntial parameters θ, we define a sub-network for f\nas f(x; m⊙θ) where m ∈{0,1}|θ| is a binary\nmask on its parameters and ⊙is the element-wise\nproduct. The mask msets many of the parameters\nof the original network to zero, which prunes edges\nin the computation graph and yields a sub-network\nof the original network.\nWinning ticket. When training network f(x; θ)\non a task, f reaches maximum evaluation perfor-\nmance aat iteration i. Similarly, when training sub-\nnetwork f(x; m⊙θ) on the same task, f reaches\nmaximum evaluation performance a′at iteration\ni′. A sub-network f(x; m⊙θ) is a winning ticket\nf∗(x; m⊙θ) if it achieves similar or better perfor-\nmance than the original network: a−a′≤ϵwhen\ni′ ≤i (we set ϵ to be one standard deviation of\nperformance of the original mBERT model).\nAlgorithm 1Iterative magnitude pruning to reach\nsparsity level s.\nStart with mBERT pre-trained weights θ0\nSet p←10 and n←s\np\nfor r←0 to n−1 do\nTrain network for iiterations, arriving at parameters θi\nPrune p% of the parameters in θ0\nReset the remaining parameters to their values in θ0\nend for\nReturn the resulted pruned network\nIdentifying winning tickets. As shown in Algo-\nrithm 1, we use unstructured magnitude iterative\npruning (Han et al., 2015) to discover winning tick-\nets. In this approach, an original model is repeat-\nedly trained on a task over multiple rounds r, and a\nsubset of its parameters is pruned in each round un-\ntil a desired sparsity level sis reached (Frankle and\nCarbin, 2019; Chen et al., 2020). More specifically,\nin each round rof training, the model is trained to\nits peak performance on the task. Then we prune\np% of the original parameters with the lowest mag-\nnitudes. After pruning the subset of the weights in\na round r, the remaining weights are reset to their\noriginal pre-trained initialization, and the model is\nre-trained on the same dataset again. We set the\niterative pruning rate p = 10 in all experiments\n(e.g., five rounds to reach s = 50%sparsity). To\nevaluate whether a sub-network is a winning ticket\nfor a task, we check the network’s task-specific\nperformance once it has reached the desired spar-\nsity level s. Figure 1 shows an overview of our\napproach.\n4 Experimental Setup\n4.1 Model\nIn our experiments, we use the cased version of\nmultilingual BERT (mBERT; Devlin et al., 2019).3\nThis model is a 12-layer transformer with around\n110M parameters. It is pre-trained using the\nmasked language modeling and next sentence pre-\ndiction training objectives on the Wikipedia dumps\nfor 104 languages. A shared Wordpiece (Wu et al.,\n2016) vocabulary of size 110k is initialized for\nthese 104 languages.\n4.2 Tasks & Datasets\nWe perform our experiments on three different NLP\ntasks. We have chosen typologically diverse lan-\nguages covering different language families: Ger-\nmanic, Romance, Indo-Aryan, and Semitic, and\n3https://huggingface.co/bert-base-multilingual-cased\n7562\n02468101214\n102030405060708090\nPerplexity\nSparsity Level (%)\nardeenesfrruurzhhiswfa\n(a) MLM\n405060708090100\n102030405060708090\nF1-Score (%)\nSparsity Level (%)\nardeenesfrruurzhfa (b) NER\n405060708090100\n102030405060708090\nAccuracy (%)\nSparsity Level (%)\nardeenesfrruurzhhisw (c) XNLI\nFigure 2: Performance of pruned sub-networks for each task-language pair at different sparsity levels\nincluding both high- and low-resource languages\nfrom the NLP perspective. These languages are\nincluding English (en), French (fr), German (de),\nChinese (zh), Russian (ru), Spanish (es), Farsi (fa),\nUrdu (ur), Arabic (ar), Hindi (hi), and Swahili (sw).\nMasked Language Modeling (MLM).For this\ntask, we use 512-token sequences from Wikipedia\nin each language as the training data.\nNamed Entity Recognition (NER). We use\nWikiAnn (Pan et al., 2017), a multilingual named\nentity recognition and linking dataset built on\nWikipedia articles for 282 languages4. Swahili (sw)\nis not included in this dataset, and Hindi (hi) has\na small data size, so these two languages are ex-\ncluded from the NER experiments.\nNatural Language Inference (NLI).We use the\ncross-lingual natural language inference (XNLI)\ndataset (Conneau et al., 2018). This dataset in-\ncludes a subset of examples from MNLI (Williams\net al., 2018) translated into 14 languages5. Farsi is\nnot included in this dataset.\n4.3 Training Details\nWe tune hyperparameters and select evaluation met-\nrics for each task based on prior work (Chen et al.,\n2020)6. All results are reported on the development\nsets of these datasets and are the average of three\nruns using different random seeds. We use the same\nnumber of training and evaluation examples for all\nlanguages of a task. Table 7 in the appendix reports\npre-training and fine-tuning details. Computational\ndetails are reported in Appendix H.\n4https://huggingface.co/datasets/wikiann\n5https://huggingface.co/datasets/xnli\n6https://github.com/VITA-Group/BERT-Tickets\n5 Experimental Results\nIn this section, we report our evaluations to study\nthe presence of language-neutral sub-networks in\nmultilingual language models. First, we validate\nthe lottery ticket hypothesis for the mBERT model\nfor various languages and tasks. Then, we compare\nthe winning tickets for different languages by an-\nalyzing their parameter overlap and cross-lingual\nperformance. Finally, to further assess the similar-\nity of the winning tickets in a zero-shot setting, we\nevaluate their performance in a sentence retrieval\ntask.\nExistence of Winning Tickets.To discover win-\nning tickets for each task-language pair, we run Al-\ngorithm 1 to identify sub-networks at various spar-\nsity levels. Then, we train the pruned sub-networks\non the same task and language to identify whether\nit qualifies as a winning ticket, f∗(x,m ⊙θ).\nIn Figure 2, we report the performance of pruned\nsub-networks for each task-language pair at differ-\nent sparsity levels between 0 and 90%. Winning\ntickets are found for each language and task at mul-\ntiple sparsity levels, though as these sub-networks\nget sparser, they no longer satisfy the winning ticket\ncriterion (performance must be within 1 standard\ndeviation of the full mBERT performance). How-\never, they still often perform within 10% of their\nwinning ticket performance. Interestingly, we find\nthat the performance drop at higher sparsity lev-\nels is greater for the MLM and NER tasks than for\nXNLI, highlighting the importance of evaluating on\ntasks that induce diverse behavior in sparse models.\nAbsolute Sub-network Overlap. Given that we\ncan discover suitable winning tickets for every lan-\nguage and task, we now assess the similarity of\nthe discovered sub-networks by comparing their\nparameter overlap. Sub-network pairs (or larger\ngroups) with higher overlap will be more likely to\n7563\n0.650.700.750.800.850.90\n123456789101112\nSub-network Overlap\nmBERT Layer\n(a) MLM\n0.9500.9550.9600.9650.9700.9750.980\n123456789101112\nSub-network Overlap\nmBERT Layer (b) NER\n0.850.870.890.910.930.95\n123456789101112\nSub-network Overlap\nmBERT Layer (c) XNLI\nFigure 3: Sparsity pattern overlap between pruned sub-networks across different layers at 50% sparsity level.\ncorrespond to language-neutral representations in\nthe original mBERT model. To measure parame-\nter overlap between discovered sub-networks for\ndifferent task-language pairs, we compute the Jac-\ncard similarity between masks (mi, mj) from two\nsub-networks\n(mi∩mj\nmi∪mj\n)\n. In Figure 3, we report\nthe parameter overlap across mBERT’s layers for\ndifferent language pairs across each of the tasks.\nThese results are for sub-networks at 50% sparsity,\nso we note that the expected Jaccard similarity of\ntwo randomly sampleds=50% sparse sub-networks\nwould be 33%.\nWe find that the NER task generally discovers\nsub-networks with much more overlap across lan-\nguages, followed by XNLI and MLM. Both NER\nand XNLI exhibit increasingly overlapping sub-\nnetworks at higher layers, while sub-networks dis-\ncovered for the MLM task increase in overlap up\nuntil the middle layers and then drop again. This\nphenomenon is perhaps partly explained by the up-\nper layers being more task-focused (Merchant et al.,\n2020). For the MLM task, the model must predict\nlanguage-specific tokens as the task. While we do\nnot prune the task heads as part of our algorithm,\nthe upper layers of mBERT may still need to be\nspecialized to particular languages to predict their\nunique vocabularies7.\nAs a comparison, we establish an upper bound\non the expected overlap between languages, by\ncomputing the overlap of sub-networks for the\nsame language (pruned using different random\nseeds). The average overlap of sub-networks across\nmultiple runs is 98.66, 92.23, and 92.23 for the\nNER, XNLI, and MLM tasks, respectively. In\nall three tasks, the overlaps across runs are much\nhigher than the mask overlap across languages.\n7Further analysis of these MLM results is in Appendix B.\nCross-lingual Transfer of Winning Tickets.We\nnow evaluate the transfer of winning tickets across\nlanguages for a given task. Using the discovered\nsub-networks for each task-language pair (set at\na sparsity level of 50% to maintain consistency\nacross languages), we train these sub-networks on\nthe other task-language pairs for the same task and\ncompare their performance against the original sub-\nnetwork trained on identical data ( i.e., a(s,t) vs\na(t,t)8 where sstands for the source language and\ntstands for the target language). If a transferred\nsub-network’s performance on the target language\nis within one standard deviation of the target lan-\nguage sub-network’s performance, we identify it\nas a successful transfer, indicating that this win-\nning ticket is more language-neutral than language-\nspecific, as its original parameters are adaptable for\ndifferent languages.\nFigure 4a shows the transfer performance (i.e.,\nthe performance drop or gain of the sub-network\nwhen trained on the target language compared to\nwhen trained on the source language) of MLM win-\nning tickets. For this task, none of the winning\ntickets for source languages are winning tickets for\nother languages, meaning that the sub-networks\ndo not achieve similar perplexity as to the target\nsub-networks when transferred to the target tasks.\nHowever, the perplexity increase is relatively lim-\nited (within 0.2-0.5 points) compared to a50% sub-\nnetwork pruned randomly (rand), suggesting that\nthese language-specific sub-networks do contain\nshared multi-lingual components.\nA similar transfer pattern emerges for the NER\ntask in Figure 4b. None of the transferred sub-\nnetworks match the performance of the source lan-\n8astands for the performance metric of the model as men-\ntioned in Section 3.\n7564\n(a) MLM\n(b) NER\n(c) XNLI\nFigure 4: The performance difference of transferring\nwinning tickets across languages (50% sparsity). Each\nrow indicates the source language and each column indi-\ncates a target language. Each cell shows the difference\nin performance between the transferred sub-network\nand the sub-network discovered from training directly\non the target language. We also report the performance\ndrop of a random sub-network (rand) compared to the\nlanguage sub-network. Blue and red colors indicate per-\nformance losses and gains respectively.\nguage sub-network. However, the performance re-\nmains high, with most target languages maintaining\n98% of the target sub-network performance when\ntrained on the source sub-networks (compared to\nwhen trained on a random sub-network) 9. Chi-\nnese (zh) and Urdu (ur) experience the worst per-\nformance drop when we use sub-networks trained\nfor other languages to transfer for these two lan-\nguages. One explanation could be that Chinese\nNER is more challenging than other languages due\nto the lack of capitalization information and the\nchallenge of word segmentation in Chinese. Con-\nsequently, Chinese may require more language-\nspecific information that may be pruned from the\nsub-networks of other languages. The relatively\nlow transfer performance for Urdu may be more\nempirical. Urdu’s sub-network at 50% sparsity has\na high performance, outperforming mBERT by 1.2\npoints, making it a strong baseline.\nThe results for the XNLI task (depicted in Fig-\nure 4c) exhibit a different pattern. For XNLI,\nthe sub-networks found for a language often per-\nform well for other languages, hinting at signif-\nicant language-neutral components in these sub-\nnetworks. At times, these transferred sub-networks\neven exceed the performance of the source lan-\nguage sub-networks. For example, the French and\nEnglish winning tickets are also winning tickets\nfor all other languages we examined. The Spanish,\nRussian, and German sub-networks also transfer\nwell to most other languages. However, the transfer\nperformance of Urdu10 and Swahili sub-networks\nare worse than other languages, possibly because\nthese two languages were under-represented during\nmBERT’s pre-training, and so contributed less to\nthe model’s final parameters compared to other lan-\nguages11. We also note that Arabic’s winning ticket\noutperforms mBERT by ∼1.5 points for Swahili\nand Urdu. As both Urdu and Swahili have been\nhistorically influenced by Arabic (Spear, 2000; Ver-\nsteegh, 2014), they may benefit from Arabic being\na high resource language in the pre-training corpus.\n9We also developed additional random baselines where\nLTH-discovered sub-networks at 10, 20, 30, and 40% spar-\nsity are randomly pruned to reach a 50% sparse sub-network.\nAlthough the performance of these baselines is higher, trans-\nferring sub-networks across languages still outperforms them.\n10For XNLI, the sparsest Urdu winning ticket was at s=\n30%. For consistency, we used a 50% sparse sub-network for\nUrdu in these experiments, but the 30% sparse sub-network\ndoes not transfer well to other languages either.\n11Urdu and Swahili have less than 200k Wikipedia arti-\ncles while the rest are on the scale of millions of articles:\nhttps://meta.wikimedia.org/wiki/List_of_Wikipedias\n7565\nFigure 5: The performance difference of transferring\nwinning tickets across languages (50% sparsity) for the\nXNLI task on the mT5 model.\nInterestingly, the parameter overlaps of all the\nlanguage pairs do not always predict their cross-\nlingual transfer performance. For MLM, the lower\nrelative degree of sub-network overlap between lan-\nguages does align with the observed performance\ndrop when transferring winning tickets between\nlanguages (Figure 4a). Similarly, an increased\noverlap is observed among XNLI sub-networks,\ncorresponding to improved transfer performance\nbetween languages. When looking at the NER sub-\nnetworks, however, which have the highest pair-\nwise overlap, we do not observe successful trans-\nfer results (Figure 4b). We conjecture that trans-\nfer for NER may require specialized knowledge\nabout the entities likely to be discussed in a par-\nticular language (e.g., Chinese Wikipedia articles\nin the NER dataset may contain more information\nabout Chinese public and historical figures). Con-\nsequently, even if a high overlap is observed, the\nnon-overlapping parameters in these sub-networks\nare crucial for successful task performance12.\nTo investigate to what extent these observations\ngeneralize to other models, we repeat the same ex-\nperiments for the mT5 model (Xue et al., 2021).13\nThis model is a multilingual text-to-text trans-\nformer with 12 layers and 580 million parameters\nand is trained on a multilingual variant of the C4\ndataset (mC4; Raffel et al., 2020) covering 101\nlanguages. We formulate the XNLI task into a\ntext-to-text format similar to the mT5 paper by\ngenerating the label text from the concatenation of\nthe premise and hypothesis. Figure 5 shows the\n12Further analysis is in Appendix C.\n13https://huggingface.co/google/mt5-base\n-8-6-4-20ardeenesfafrruurzh\nAvg. Performance Drop\nSource Language\n50%80%\n(a) NER\n-8-6-4-202ardeenesfrhiruswurzh\nAvg. Performance Drop\nSource Language\n50%80%\n(b) XNLI\nFigure 6: Average cross-transfer performance drop for\nsub-networks with sparsity levels 50% and 80%. For\neach source language and task, the relative average is\ncomputed across the other languages and the same task.\ncross-lingual transfer performance for the XNLI\ntask at 50% sparsity level. 14 For most cases, the\ntransfer performance drop is relatively small, and,\nas with mBERT, similar languages such as English,\nSpanish, French, and German transfer well to most\nof the other languages, suggesting that language-\nneutral parameters are a common phenomenon in\ndifferent MultiLMs.\nImpact of Sub-network Density.To investigate\nthe effect of sparsity on the retainment of language-\nneutral components, we compare the cross-lingual\ntransferability of mBERT sub-networks at 50% and\n80% sparsity levels. Figure 6 shows the average of\nrelative transfer performance drop15 per language\nfor the NER and XNLI tasks. Each bar repre-\nsents an average performance drop after retraining\n(and evaluating) the sub-network for the source lan-\nguage on all the target languages, individually. As\nwe increase the sparsity level of a sub-network, its\ncross-lingual transferability degrades considerably\n(i.e., the relative performance drop increases), indi-\n14We note that winning tickets for these languages are not\nfound for this sparsity level, but chose to maintain consistency\nwith the mBERT experiments.\n15 1\n|L|−1\n∑\nt∈L\\s\na(s,t)−a(t,t)\na(t,t) where sand tare source and\ntarget languages and Lis the set of languages for each task.\n7566\ncating that there were language-neutral parameters\nat 50% sparsity level facilitating the cross-lingual\ntransfer that were pruned in the 80% sparse sub-\nnetwork. As we decrease the model’s capacity by\npruning more parameters, the model relies more on\nlanguage- and task-specific parameters than those\nthat may facilitate the cross-lingual transfer.\nParallel Sentence Retrieval. To further assess\nthe language neutrality of mBERT, we compare\nthe behavior of different sub-networks in a zero-\nshot setting where they are used as feature extrac-\ntors. We evaluate each sub-network’s sentence re-\ntrieval accuracy (Pires et al., 2019) on the English-\nto-French translation test set from the WMT14\ndataset (3003 sentence pairs). 16 This task aims\nto find sentence pairs from two corpora in two\ndifferent languages, where the two sentences of\neach pair are corresponding translations of one an-\nother. In these experiments, we use the parallel\nretrieval implementation from LASER17 with the\nmargin-based scoring function from (Artetxe and\nSchwenk, 2019).18 We use the average of the token\nembeddings for each sentence (encoded using the\nsub-networks) as the retrieval inputs.\nWe observe that both the similarity across the\nsub-networks (Figure 3a) and the top-5 retrieval\naccuracy (Figure 7) increase as we go into the\nmiddle layers of mBERT. XNLI’s winning tick-\nets perform better than the other two tasks, and\nsome of these sub-networks even reach the same\nzero-shot accuracy as the full mBERT model. One\npossible explanation is that both the XNLI task\nand parallel sentence retrieval require a richer se-\nmantic interpretation of the text. Hence, XNLI\nwinning tickets are better sub-networks for parallel\nsentence retrieval as they must capture semantic\nknowledge to successfully complete the task. In\nthe case of NER, the sub-networks of different lan-\nguages perform similarly across all layers, which\nis unsurprising given the high amount of overlap\nbetween these sub-networks. In a zero-shot setting\nwith no language-specific tuning for the task, these\nsub-networks are basically identical. For MLM,\nthe sub-networks perform similarly up to the mid-\ndle layers, where they begin to diverge, support-\ning the hypothesis that higher layers in the MLM\nsub-networks are dedicated toward representing the\ntask, which is language-specific for MLM.\n16https://huggingface.co/datasets/wmt14\n17https://github.com/facebookresearch/LASER\n18The definition of the function is available in Appendix E.\nModel / Target Language\nSub-network ar de es ru ur\nmBERT 88.69 89.08 91.11 89.22 95.52\nf∗ 88.5 89.01 91.08 89.03 96.44\nen 87.23 87.52 90.44 88.47 93.93\nfa 87.56 87.57 90.18 88.34 94.10\nfr 87.41 87.83 90.56 88.50 94.42\nzh 87.01 87.67 90.11 88.21 93.65\nen-fa-fr-zh 87.64 87.81 90.77 88.34 94.59\nTable 1: Performance (F1-score) of winning tickets at\n50% sparsity level for combining training datasets of en,\nfa, fr, and zh languages for the NER task.\nMultilingual Sub-networks. Observing the ben-\nefit of transferring sub-networks from high-\nresource languages to lower resource ones ( e.g.,\nArabic to Urdu and Swahili for NER), we study\nthe impact of combining datasets from multiple\nlanguages to discover multilingual winning tickets.\nIn this set of experiments, we combined English,\nFarsi, French, and Chinese datasets together to\nprune mBERT for the NER task. For parity, we use\nan identically-sized combined training dataset with\n20,000 samples (the same as other experiments on\nthis task), keeping only 25% of each of each lan-\nguage’s dataset. Table 1 shows the transfer results\nof the resulting sub-network at 50% sparsity level.\nFor three out of five languages that we evaluated\ntransfer performance on, the sub-network extracted\nusing the combination of languages outperforms\nthe sub-networks found using each training lan-\nguage separately. The inclusion of multiple lan-\nguages in the corpus may encourage recovering\nmore language-neutral sub-networks than discover-\ning sub-networks using only a single language.\n6 Conclusion\nWhile multilingual pre-trained language models\nhave shown impressive performance across lan-\nguages, the role of language neutrality in achieving\nsuch a performance is not well understood. In\nthis work, we analyze the language-neutrality of\nmultilingual models by investigating the overlap\nbetween language-encoding sub-networks of these\nmodels. Using mBERT as a foundation, we expose\nthe extent of its language neutrality by employing\nthe lottery ticket hypothesis and comparing the sub-\nnetworks within mBERT obtained for various lan-\nguages and tasks. We show that such sub-networks\nachieve high performance after being transferred\nacross tasks and languages and are similar to one\n7567\n020406080100\n123456789101112\nAccuracy (%)\nmBERT Layer\nar de en esfr ru sw zhhi ur fa mbert\n(a) MLM\n020406080100\n123456789101112\nAccuracy (%)\nmBERT Layer\nardeenesfrruurzhfa mbert (b) NER\n020406080100\n123456789101112\nAccuracy (%)\nmBERT Layer\nar de en esfr ru ur zhhi sw mbert (c) XNLI\nFigure 7: Parallel sentence retrieval accuracy on English-to-French translation using winning tickets. We use the\naverage of the contextual embeddings of each sentence as the retrieval inputs.\nanother. However, at higher levels of sparsity, this\ntransferability evaporates. Our results suggest that\nmultilingual language models include two sepa-\nrate language-neutral and language-specific com-\nponents, with the former playing a more prominent\nrole in cross-lingual transfer performance.\nLimitations\nIn this work, we use network parameter overlap to\nmeasure similarity between sub-networks discov-\nered for different languages. However, while cross-\nlingual transfer performance coarsely tracks with\nsub-network overlap, these results may not hold\nat a fine-grained level. While high sub-network\noverlap is an indicator that a sub-network will ef-\nfectively transfer to a target language, small rela-\ntive differences in cross-lingual performance across\nlanguages do not correlate strongly with overlap.\nConsequently, absolute overlap may be limited as\nan analogue for identifying language-neutral com-\nponents of multilingual models. Another limitation\nis that we only use the lottery ticket hypothesis\nas a method for discovering language-specific sub-\nnetworks, while other pruning and masking meth-\nods may provide complementary or competing in-\nsights. Finally, due to computational limitations,\nwe only apply our work to three tasks and eleven\nlanguages. While our selection is diverse, new in-\nsights may emerge from a larger cross-section.\nAcknowledgements\nThe authors thank the anonymous reviewers for\ntheir valuable comments and feedback. We also\nthank the members of LSIR and NLP labs at EPFL\nfor their feedback and support. Antoine Bosselut\ngratefully acknowledges the support of Innosuisse\nunder PFFS-21-29, the EPFL Science Seed Fund,\nthe EPFL Center for Imaging, Sony Group Corpo-\nration, and the Allen Institute for AI.\nReferences\nAlan Ansell, Edoardo Maria Ponti, Anna Korhonen, and\nIvan Vulic. 2022. Composable sparse fine-tuning for\ncross-lingual transfer. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022, pages 1778–1796.\nAssociation for Computational Linguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the Cross-lingual Transferability of Mono-\nlingual Representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL), pages 4623–4637.\nMikel Artetxe and Holger Schwenk. 2019. Margin-\nbased parallel corpus mining with multilingual sen-\ntence embeddings. In Proceedings of the 57th Con-\nference of the Association for Computational Lin-\nguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, Volume 1: Long Papers, pages 3197–3203.\nAssociation for Computational Linguistics.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020. The Lottery Ticket Hypothesis for Pre-\ntrained BERT Networks. In Proceedings of the 2020\nAnnual Conference on Neural Information Process-\ning Systems (NeurIPS).\nEthan A. Chi, John Hewitt, and Christopher D. Manning.\n2020. Finding Universal Grammatical Relations in\nMultilingual BERT. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL), pages 5564–5577.\nRochelle Choenni and Ekaterina Shutova. 2020. What\ndoes it mean to be language-agnostic? Probing multi-\nlingual sentence encoders for typological properties.\nCoRR, abs/2009.12862.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL), pages\n8440–8451.\n7568\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual Language Model Pretraining. In Proceed-\nings of the 2019 Annual Conference on Neural Infor-\nmation Processing Systems (NeurIPS), pages 7057–\n7067.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel R. Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating Cross-\nlingual Sentence Representations. In Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 2475–\n2485.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b. Emerging\nCross-lingual Structure in Pretrained Language Mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics (ACL),\npages 6022–6034.\nWietse de Vries, Andreas van Cranenburgh, and Malv-\nina Nissim. 2020. What’s so special about bert’s\nlayers? A closer look at the NLP pipeline in mono-\nlingual and multilingual models. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, Online Event, 16-20 November 2020, volume\nEMNLP 2020 of Findings of ACL, pages 4339–4350.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nJonathan Frankle and Michael Carbin. 2019. The Lot-\ntery Ticket Hypothesis: Finding Sparse, Trainable\nNeural Networks. In Proceedings of the 7th Inter-\nnational Conference on Learning Representations\n(ICLR).\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel M.\nRoy, and Michael Carbin. 2020. Linear Mode Con-\nnectivity and the Lottery Ticket Hypothesis. In Pro-\nceedings of the 37th International Conference on\nMachine Learning (ICML), pages 3259–3269.\nTrevor Gale, Erich Elsen, and Sara Hooker. 2019. The\nState of Sparsity in Deep Neural Networks. CoRR,\nabs/1902.09574.\nHila Gonen, Shauli Ravfogel, Yanai Elazar, and Yoav\nGoldberg. 2020. It’s not greek to mbert: Inducing\nword-level translations from multilingual BERT. In\nProceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for\nNLP , BlackboxNLP@EMNLP 2020, Online, Novem-\nber 2020, pages 45–56. Association for Computa-\ntional Linguistics.\nSong Han, Jeff Pool, John Tran, and William J. Dally.\n2015. Learning both Weights and Connections for\nEfficient Neural Networks. CoRR, abs/1506.02626.\nDavid R. Hardoon, Sándor Szedmák, and John Shawe-\nTaylor. 2004. Canonical Correlation Analysis: An\nOverview with Application to Learning Methods.\nNeural Comput., 16(12):2639–2664.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A Massively Multilingual Multi-\ntask Benchmark for Evaluating Cross-lingual Gener-\nalization. CoRR, abs/2003.11080.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and Dan\nRoth. 2020. Cross-Lingual Ability of Multilingual\nBERT: An Empirical Study. In Proceedings of the\n8th International Conference on Learning Represen-\ntations (ICLR).\nSimon Kornblith, Mohammad Norouzi, Honglak Lee,\nand Geoffrey E. Hinton. 2019. Similarity of Neural\nNetwork Representations Revisited. In Proceedings\nof the 36th International Conference on Machine\nLearning (ICML), pages 3519–3529.\nPatrick S. H. Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evaluat-\ning Cross-lingual Extractive Question Answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL), pages\n7315–7330.\nJindrich Libovický, Rudolf Rosa, and Alexander Fraser.\n2020. On the language neutrality of pre-trained\nmultilingual representations. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2020, Online Event, 16-20 November 2020, volume\nEMNLP 2020 of Findings of ACL, pages 1663–1674.\nAssociation for Computational Linguistics.\nJindrich Libovický, Rudolf Rosa, and Alexander Fraser.\n2019. How Language-Neutral is Multilingual BERT?\nCoRR, abs/1911.03310.\nZehui Lin, Liwei Wu, Mingxuan Wang, and Lei Li.\n2021. Learning language specific sub-network for\nmultilingual machine translation. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 293–305. Association\nfor Computational Linguistics.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick,\nand Ian Tenney. 2020. What happens to BERT\nembeddings during fine-tuning? In Proceedings\nof the Third BlackboxNLP Workshop on Analyzing\nand Interpreting Neural Networks for NLP , Black-\nboxNLP@EMNLP 2020, Online, November 2020,\npages 33–44. Association for Computational Linguis-\ntics.\n7569\nAri S. Morcos, Maithra Raghu, and Samy Bengio. 2018.\nInsights on representational similarity in neural net-\nworks with canonical correlation. In Proceedings of\nthe 2018 Annual Conference on Neural Information\nProcessing Systems (NeurIPS), pages 5732–5741.\nAri S. Morcos, Haonan Yu, Michela Paganini, and Yuan-\ndong Tian. 2019. One ticket to win them all: general-\nizing lottery ticket initializations across datasets and\noptimizers. In Proceedings of the 2019 Annual Con-\nference on Neural Information Processing Systems\n(NeurIPS), pages 4933–4943.\nBenjamin Müller, Yanai Elazar, Benoît Sagot, and\nDjamé Seddah. 2021. First align, then predict: Un-\nderstanding the cross-lingual ability of multilingual\nBERT. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, EACL 2021, Online,\nApril 19 - 23, 2021, pages 2214–2231. Association\nfor Computational Linguistics.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman, Kevin Knight, and Heng Ji. 2017. Cross-lingual\nname tagging and linking for 282 languages. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2017, Van-\ncouver, Canada, July 30 - August 4, Volume 1: Long\nPapers, pages 1946–1958. Association for Computa-\ntional Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual bert? In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 4996–5001. Association for Computa-\ntional Linguistics.\nSai Prasanna, Anna Rogers, and Anna Rumshisky. 2020.\nWhen BERT plays the lottery, all tickets are winning.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 3208–\n3229. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and\nJascha Sohl-Dickstein. 2017. SVCCA: Singular Vec-\ntor Canonical Correlation Analysis for Deep Learn-\ning Dynamics and Interpretability. In Proceedings of\nthe 2017 Annual Conference on Neural Information\nProcessing Systems (NIPS), pages 6076–6085.\nJasdeep Singh, Bryan McCann, Richard Socher, and\nCaiming Xiong. 2019. BERT is Not an Interlingua\nand the Bias of Tokenization. In Proceedings of\nthe 2nd Workshop on Deep Learning Approaches\nfor Low-Resource NLP (DeepLo@EMNLP-IJCNLP),\npages 47–55.\nThomas Spear. 2000. Early swahili history reconsid-\nered. The International Journal of African Historical\nStudies, 33(2):257–290.\nYi-Lin Sung, Varun Nair, and Colin Raffel. 2021. Train-\ning neural networks with fixed sparse masks. In Ad-\nvances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Process-\ning Systems 2021, NeurIPS 2021, December 6-14,\n2021, virtual, pages 24193–24205.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Proceedings of the 2017 Annual Con-\nference on Neural Information Processing Systems\n(NIPS), pages 5998–6008.\nKees Versteegh. 2014. Arabic language. Edinburgh\nUniversity Press.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nZirui Wang, Zachary C. Lipton, and Yulia Tsvetkov.\n2020. On negative interference in multilingual mod-\nels: Findings and A meta-learning treatment. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 4438–\n4450. Association for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus for\nsentence understanding through inference. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-\nHLT 2018, New Orleans, Louisiana, USA, June 1-6,\n2018, Volume 1 (Long Papers), pages 1112–1122.\nAssociation for Computational Linguistics.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 833–844.\nAssociation for Computational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing\nLiu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\nTaku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, Wei Wang, Cliff Young, Jason\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals,\nGreg Corrado, Macduff Hughes, and Jeffrey Dean.\n7570\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. CoRR, abs/1609.08144.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 483–498. Association\nfor Computational Linguistics.\nHaonan Yu, Sergey Edunov, Yuandong Tian, and Ari S.\nMorcos. 2020. Playing the lottery with rewards and\nmultiple languages: lottery tickets in RL and NLP.\nIn Proceedings of the 8th International Conference\non Learning Representations (ICLR).\nWei Zhao, Steffen Eger, Johannes Bjerva, and Isabelle\nAugenstein. 2021. Inducing language-agnostic mul-\ntilingual representations. In Proceedings of *SEM\n2021: The Tenth Joint Conference on Lexical and\nComputational Semantics, *SEM 2021, Online, Au-\ngust 5-6, 2021, pages 229–240. Association for Com-\nputational Linguistics.\nA Additional Lottery Ticket Results\nTables 3, 4, and 5 show the performance of win-\nning tickets across languages for the MLM, XNLI,\nand NER tasks respectively. The absolute perfor-\nmance values of all three tasks on mBERT model\nare depicted in Tables 3, 4, and 5. Table 6 shows\nthe absolute performance for the XNLI task on the\nmT5 model.\nB Sub-network Overlap\nIn Figure 3a, the parameter overlap plot for the\nMLM task, we note three different clusters. We\nobserve that these clusters correspond to language\npairs between two different groups of languages.\nThe first set includes English, French, German,\nRussian, and Chinese and the second set includes\nthe rest of the languages. The first cluster (at the\nbottom) shows the overlaps between the languages\nof the first set, the third cluster (on top) includes the\noverlaps among languages of the second set. The\nsecond cluster (in the middle) covers the overlaps\namong languages from the first and the second set.\nC Disentangling Task Neutrality\nGiven the significant role of the language-neutral\ncomponents of the winning tickets, we want to\ninvestigate to what extent these components are\ntask-specific. We evaluate the similarity of win-\nning tickets across tasks for a given language to\nidentify whether the winning ticket for a given lan-\nguage and task can be transferred successfully to\nother tasks for the same language. A successful\ntransfer shows that the language-neutral component\nis not only task-specific but rather a common space\nacross tasks. Similar to the previous section, we\nfirst identify winning tickets for each task-language\npair and then train each sub-network on other tasks\nusing the data from the same language. We again\nuse sub-networks with 50% sparsity. We limit the\ntested languages to those whose sub-networks are\npresent for all three tasks: English (en), French (fr),\nand Chinese (zh).\nWe report cross-task performance in Figure 9\nfor the MLM (9a), NER (9b), and XNLI (9c) tasks.\nAlthough in none of the cases the transferred sub-\nnetwork’s performance matches the original sub-\nnetwork’s performance, the average performance\ndrop is less than 2 points for each task. This ob-\nservation suggests that a considerable fraction of\nparameters are shared across tasks even if a certain\nnumber of them remain task-specific. Contrary to\nprevious observations for the BERT model (Chen\net al., 2020), we find that MLM winning tickets do\nnot robustly transfer for training other tasks.\nD Alternative Masking/Pruning\nStrategies\nIn our experiments, we transfer obtained sparse sub-\nnetworks by re-training their original weights on a\ngiven task and various languages. Consequently, it\nis important that these sub-networks match the test\naccuracy of the full model when trained in isola-\ntion (i.e., winning tickets in the LTH), to be a good\nsub-network for the task at hand. By using such a\nsub-network, we want to keep task-specific param-\neters as much as possible. Any pruning/masking\nmethodology that gives us sub-networks perform-\ning at the same level as the full model and also\nuses enough data to detect the language- and task-\nspecific parameters could be used in our analysis.\nTo broaden our analysis, we ran additional ex-\nperiments using two alternative masking/pruning\nstrategies. In the first set of experiments, we fol-\nlowed the pruning approach introduced by (Ansell\net al., 2022). This method proposes a sparse fine-\ntuning method to discover task and language sub-\nnetworks that can be composed for cross-lingual\ntransfer. However, unlike the LTH, which prunes\nparameters with the lowest magnitudes after fine-\ntuning, this method prunes the parameters that have\n7571\nMLM (Perplexity) NER (F1) XNLI (Accuracy)\nmBERT f∗ s(%) rand mBERT f∗ s(%) rand mBERT f∗ s(%) rand\nar 3.5247 3.5546 50 6.4001 88.64 88.50 50 72.43 70.26 70.29 50 61.48\nde 3.5143 3.5092 50 9.6489 89.11 88.81 50 75.68 77.33 77.10 50 65.98\nen 4.6523 4.6347 50 10.9182 83.47 83.60 50 68.05 82.16 82.14 50 74.49\nes 3.6775 3.6712 50 8.3084 91.11 91.08 50 79.44 78.80 78.93 60 67.67\nfa 3.8033 3.8038 50 7.3315 92.33 92.10 60 75.66 - - - -\nfr 3.1151 3.0936 50 6.8504 90.51 90.31 50 77.61 78.00 77.61 50 69.11\nhi 2.8728 2.8757 50 5.1345 - - - - 68.48 68.04 50 58.75\nru 2.5927 2.5907 50 6.1112 89.39 89.03 50 74.91 72.98 72.73 60 62.20\nsw 2.5001 2.4657 50 4.292 - - - - 66.28 66.14 50 59.55\nur 2.8624 2.8638 50 5.0286 95.31 95.94 60 84.33 63.06 63.13 30 58.75\nzh 3.6096 3.5754 50 8.4811 79.53 79.33 50 57.44 76.60 76.01 50 68.23\nTable 2: Performance of winning tickets (f∗) on the XNLI, NER, and MLM tasks at the highest sparsity (s) for\nwhich iterative pruning finds them. We also report the performance of a random sub-network of mBERT (rand) at\nthe same sparsity level as the winning ticket.\nModel / Target Language\nSub-network ar de en es fa fr hi ru sw ur zh\nmBERT 3.5247 3.5143 4.6523 3.6775 3.8033 3.1151 2.8728 2.5927 2.5001 2.8624 3.6096\nar 3.5546 3.7510 4.7967 3.7568 4.0791 3.2360 3.1133 2.7075 2.7315 3.0873 3.7821\nde 3.8861 3.5092 4.9626 3.9055 4.1539 3.3429 3.1189 2.8173 2.7535 3.0852 3.9186\nen 3.8742 3.8270 4.6347 3.8585 4.2320 3.3177 3.1034 2.8129 2.7436 3.0899 3.9407\nes 3.8339 3.7929 4.8842 3.6712 4.1942 3.2895 3.0974 2.7693 2.7569 3.1046 3.8816\nfa 3.7464 3.6334 4.7350 3.6938 3.8038 3.1910 3.0848 2.6704 2.7339 3.0595 3.7303\nfr 3.8722 3.8528 4.9441 3.8526 4.1329 3.0936 3.1116 2.8150 2.7406 3.1577 3.9641\nhi 3.8431 3.7554 4.8606 3.8120 4.2017 3.2725 2.8757 2.7432 2.7612 3.1281 3.8329\nru 3.8820 3.8683 5.0007 3.9182 4.1466 3.3656 3.0847 2.5907 2.7566 3.1553 3.9728\nsw 3.8650 3.7613 4.8704 3.8184 4.2288 3.2863 3.1022 2.7505 2.4657 3.0666 3.8442\nur 3.8644 3.7545 4.8701 3.8153 4.1892 3.2778 3.0937 2.7456 2.7575 2.8638 3.8410\nzh 3.8852 3.8804 5.0004 3.9216 4.1565 3.3698 3.1025 2.8302 2.7675 3.1015 3.5754\nTable 3: Performance (perplexity) of winning tickets at 50% sparsity for the MLM task.\nthe smallest absolute difference from the initial pa-\nrameters. Following this method, we prune the base\nmodel to obtain 50% sparse sub-networks (simi-\nlar to our previous method) for a selection of lan-\nguages (i.e., en, fr, de, and ar). For the NER and\nXNLI tasks, we observed drops of ∼6% and ∼5%,\nrespectively compared to the full model (averaged\nacross three random seeds). As our goal was to\nfind sub-networks that perform as well as the full\nmodel for our analysis, these sub-networks would\nnot be suitable candidates.\nWe also test the method introduced by (Sung\net al., 2021). This paper proposes a method to\napproximate the Fisher information matrix as a\nmeasure of the importance of each parameter when\nconstructing sparse masks for a given task. Using\na sufficiently large sample size (1024 examples),\nthey compute this matrix and prune the parameters\nthat contain the least information about the task.\nWhen we use this method (again pruning to 50%\nsparsity), we find that end task performance drops\nby ∼8% compared to the full model (on XNLI and\nNER). As a result, this approach also does not find\nhigh-quality sub-networks that can be tested for\ncross-lingual transfer.\nE Margin-based Scoring\nFor the parallel sentence retrieval task, we use\nthe scoring function from (Artetxe and Schwenk,\n2019):\ns(x,y) = cos(x,y)\n∑\nz∈Nk(x)\ncos(x,z)\n2k + ∑\nz∈Nk(y)\ncos(y,z)\n2k\nwhere xand yare two sentence representations and\nNk(x) denotes the knearest neighbors of xin the\nother language. We use margin(a,b) =a/bas our\nmargin function.\n7572\nModel / Target Language\nSub-network ar de en es fr hi ru sw ur zh\nmBERT 70.26 77.33 82.16 78.80 78.00 68.48 72.98 66.28 63.06 76.60\nar 70.29 76.72 82.00 79.07 77.06 67.58 72.96 67.48 64.68 76.88\nde 70.22 77.10 82.44 79.09 77.73 68.11 73.35 65.40 64.03 76.50\nen 71.17 77.12 82.14 79.73 78.23 67.90 74.38 66.37 64.03 76.66\nes 70.57 77.42 82.43 79.15 77.36 68.06 74.31 64.97 63.61 76.98\nfr 70.52 76.96 82.80 79.56 77.61 68.39 73.31 66.77 63.85 77.31\nhi 70.94 76.24 82.54 77.96 77.15 68.04 72.93 65.78 64.07 76.50\nru 70.33 76.05 82.69 79.49 77.54 68.55 73.31 65.62 63.37 76.52\nsw 69.76 76.42 81.50 77.45 76.75 67.84 72.36 66.14 63.39 76.18\nur 69.11 75.17 80.81 77.65 75.71 66.98 72.20 65.33 62.14 75.82\nzh 69.79 76.16 81.45 78.73 77.80 67.74 72.67 65.51 63.85 76.01\nTable 4: Performance (accuracy) of winning tickets at 50% sparsity for the XNLI task.\nModel / Target Language\nSub-network ar de en es fa fr ru ur zh\nmBERT 88.64 89.11 83.47 91.11 92.33 90.51 89.39 95.31 79.53\nar 88.50 87.59 82.92 90.39 91.73 89.84 88.34 94.67 76.97\nde 87.09 88.81 83.06 90.43 91.20 89.59 88.38 93.93 77.25\nen 87.21 87.53 83.60 90.44 91.23 89.67 88.47 93.93 76.92\nes 87.10 87.59 83.10 91.08 91.33 89.93 88.25 94.22 76.51\nfa 87.47 87.55 82.73 90.18 92.25 89.59 88.34 94.10 76.73\nfr 87.33 87.81 82.85 90.56 91.40 90.31 88.50 94.42 76.72\nru 87.26 87.84 82.99 90.43 91.41 89.75 89.03 94.35 76.96\nur 86.81 87.35 82.59 90.08 91.35 89.50 87.94 96.44 76.73\nzh 86.94 87.48 82.42 90.11 91.17 89.47 88.21 93.65 79.33\nTable 5: Performance (F1-score) of winning tickets at 50% sparsity for the NER task.\nModel / Target Language\nSub-network ar de en es fr hi ru sw ur zh\nmT5 74.17 78.52 83.13 79.64 80.14 69.98 75.56 70.00 63.25 75.10\nar 71.25 74.43 79.69 76.14 75.47 67.18 73.51 67.24 61.14 69.15\nde 69.94 75.71 80.31 76.94 76.60 66.93 74.57 67.00 60.70 70.09\nen 69.97 76.02 80.64 77.41 76.36 67.58 75.12 67.38 61.62 68.76\nes 70.68 76.76 80.72 77.64 76.96 67.73 74.78 66.80 61.22 69.82\nfr 70.49 75.76 79.66 76.67 76.59 66.93 74.66 69.43 60.21 69.00\nhi 68.79 73.50 78.29 74.63 74.01 66.19 71.96 65.74 61.06 67.64\nru 69.67 74.89 79.33 76.16 75.07 67.37 74.22 67.13 60.44 70.22\nsw 69.96 74.60 79.53 75.65 74.93 66.32 73.31 67.22 60.41 67.45\nur 66.20 71.28 75.76 72.04 71.58 65.17 69.99 64.07 60.48 67.60\nzh 70.40 75.15 80.05 75.74 75.36 66.99 73.75 65.98 59.80 71.44\nrand 61.20 66.70 73.57 68.11 67.67 61.08 66.30 61.84 56.82 62.57\nTable 6: Performance (accuracy) of at 50% sparsity for the XNLI task on the mT5 model.\nF Language Representation Similarity\nIn this section, we discuss a prior work that demon-\nstrated the lack of language neutrality in mBERT\nby comparing the mBERT’s representations for\nsemantically-similar sentences across multiple lan-\nguages (Singh et al., 2019). They conclude that\nmBERT partitions the representation space among\nlanguages rather than using a shared, interlingual\nspace. They used projection weighted canonical\ncorrelation analysis (PWCCA; Morcos et al., 2018)\nto compute the similarity between representations\nfrom parallel sentences (XNLI dataset) for five lan-\nguage pairs. Figure 10a depicts our reproduced\nversion of their results.\nAs shown in the figure, the CLS representations\nof various languages are most similar at the shal-\nlower layers of mBERT, and their differences grow\nin deeper layers until the final layer. We argue\nthat these results contradict mBERT’s cross-lingual\n7573\n0.200.300.400.500.600.700.800.901.00\n123456789101112\nSub-network Overlap\nmT5 Layer\nen-fren-deen-esen-ruen-uren-swen-hien-aren-zhfr-defr-esfr-rufr-urfr-swfr-hifr-arfr-zhde-esde-rude-urde-swde-hide-arde-zhes-rues-ures-swes-hies-ares-zhru-urru-swru-hiru-arru-zhur-swur-hiur-arur-zhsw-hisw-arsw-zhhi-arhi-zhar-zh\n(a) Encoder\n0.200.300.400.500.600.700.800.901.00\n123456789101112\nSub-network Overlap\nmT5 Layer\nen-fren-deen-esen-ruen-uren-swen-hien-aren-zhfr-defr-esfr-rufr-urfr-swfr-hifr-arfr-zhde-esde-rude-urde-swde-hide-arde-zhes-rues-ures-swes-hies-ares-zhru-urru-swru-hiru-arru-zhur-swur-hiur-arur-zhsw-hisw-arsw-zhhi-arhi-zhar-zh (b) Decoder\nFigure 8: Sparsity pattern overlap between pruned sub-networks across different layers at 50% sparsity level for the\nXNLI task on the mT5 model.\n(a) Target task: MLM\n(b) Target task: NER\n(c) Target task: XNLI\nFigure 9: The performance of transferring winning tick-\nets between tasks (50% sparsity). Each cell shows the\ndifference in performance between the transferred sub-\nnetwork and the performance of the sub-network dis-\ncovered for the target task.\n0.40.50.60.70.80.9\n123456789101112\nPWCCA Similarity\nmBERT Layer\nen-aren-azen-bgen-csen-da\n(a) PWCCA\n0.20.30.40.50.60.7\n123456789101112\nSVCCA Similarity\nmBERT Layer\nen-aren-azen-bgen-csen-da\n(b) SVCCA\nFigure 10: Similarity scores between CLS representa-\ntions of English and five other languages using PWCCA\nand SVCCA.\n7574\ntransfer performance since for almost all of the\ndownstream tasks, the best zero-shot performance\nis obtained using the representations from middle\nto deeper layers of mBERT (Conneau et al., 2020b;\nChi et al., 2020; de Vries et al., 2020). Hence, we\nexpect the representations of deeper layers to be\nmore similar than in shallower layers.\nPWCCA is not a reliable tool in this scenario as it\nis not invariant to orthogonal transformations (Ko-\nrnblith et al., 2019). Invariance to orthogonal trans-\nformation implies invariance to (neuron) permu-\ntation, which is necessary to accommodate sym-\nmetries of neural networks (Kornblith et al., 2019).\nHence, we use singular vector canonical correlation\nanalysis (SVCCA; Raghu et al., 2017), which is\ninvariant to orthogonal transformations, to measure\nthe similarities between mBERT’s representations\nin different languages. SVCCA performs canonical\ncorrelation analysis (CCA) on truncated singular\nvalue decomposition of input matrices.\nFigure 10b presents our obtained results for the\nsame five language pairs using SVCCA. We ob-\nserve that the similarity of representations increases\nin deeper layers. Moreover, the middle and upper\nlayers’ representations are more similar across lan-\nguages than the representations from shallower lay-\ners, which is in line with mBERT’s cross-lingual\ntransfer performance. Hence, contrary to prior\nwork (Singh et al., 2019), we argue that mBERT\nmaps semantically-similar data points close to each\nother by learning a common, interlingual space.\nG Canonical Correlation Analysis\nCanonical correlation analysis (CCA) is a sta-\ntistical tool to identify and measure the associa-\ntions between two sets of random variables X and\nY (Hardoon et al., 2004). CCA finds bases for\nthe two input matrices, with the maximum cor-\nrelation between X and Y projected onto these\nbases. For X ∈Rd1×n, Y ∈Rd2×n, and 1 ≤\ni≤min(d1,d2), the ith canonical correlation coef-\nficient ρi is given by:\nρi = max\nwi\nX,wi\nY\ncorr(Xwi\nX,Yw i\nY)\nsubject to ∀j<i Xwi\nX ⊥Xwj\nX\n∀j<i Ywi\nY ⊥Ywj\nY\nWhere wi\nX ∈ Rd1 , wi\nY ∈ Rd2 and the con-\nstraints enforce orthogonality of the canonical vari-\nables. The average of {ρ1,...,ρ m}where m =\nmin(d1,d2) is often used as an overall similarity\nmeasure:\nρCCA =\n∑m\ni=1 ρi\nm\nTwo of CCA variants, projection weighted\ncanonical correlation analysis (PWCCA) and singu-\nlar vector canonical correlation analysis (SVCCA),\nare used for analyzing neural network represen-\ntations because they are invariant to linear trans-\nforms.\nTo improve the robustness of CCA, SVCCA per-\nforms CCA on top of truncated singular value de-\ncomposition of X and Y. PWCCA increases the\nrobustness of CCA by using the weighted average\nof canonical correlation coefficients:\nρPW =\n∑c\ni=1 αiρi∑\ni=1 αi\n, α i =\n∑\nj\n|⟨hi,xj⟩|\nwhere xj is the jthcolumn of X, and hi = Xwi\nX is\nthe projection of X to the ith canonical coordinate\nframe.\nH Computation details\nOur experiments are executed on a server with\n500GB of RAM, 2×16-core Intel Xeon Silver\n4216 (2.10GHz) CPUs, and an NVIDIA Quadro\nRTX 8000 GPU. For each language, the pruning\nstep for MLM, NER, and XNLI takes around 15,\n4.5, and 45 GPU hours, respectively, and fine-\ntuning takes around 6, 0.5, and 15 GPU hours for\nMLM, NER, and XNLI, respectively.\nDataset MLM NER XNLI\n# Train Ex. 1,600,000 20,000 392,702\n# Valid. Ex. 30,000 10,000 2,490\n# Epochs 1 3 3\n# Iters/Epoch 100,000 625 12,270\nBatch Size 16 32 32\nLearning Rate5×10−5 2×10−5 mbert:5×10−5\nmt5:2×10−4\nEval. Metric Perplexity F1 Accuracy\nOptimizer Adam withϵ= 1×10−8\nTable 7: Details of pre-training and fine-tuning. Learn-\ning rate decays linearly from initial value to zero.\n7575",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7969775199890137
    },
    {
      "name": "Natural language processing",
      "score": 0.6185747981071472
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5557964444160461
    },
    {
      "name": "Language model",
      "score": 0.4827643930912018
    },
    {
      "name": "Encoding (memory)",
      "score": 0.47509467601776123
    },
    {
      "name": "Transfer (computing)",
      "score": 0.4222591817378998
    },
    {
      "name": "Linguistics",
      "score": 0.33043575286865234
    },
    {
      "name": "Parallel computing",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}