{
    "title": "Preliminary assessment of automated radiology report generation with generative pre-trained transformers: comparing results to radiologist-generated reports",
    "url": "https://openalex.org/W4386757338",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A48082429",
            "name": "Takeshi Nakaura",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A2098854171",
            "name": "Naofumi Yoshida",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A1990997668",
            "name": "Naoki Kobayashi",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A2111349570",
            "name": "Kaori Shiraishi",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A2499407759",
            "name": "Yasunori Nagayama",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A2562861383",
            "name": "Hiroyuki Uetani",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A1989317687",
            "name": "Masafumi Kidoh",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A5058892987",
            "name": "Masamichi Hokamura",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A2228396312",
            "name": "Yoshinori Funama",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A2163461840",
            "name": "Toshinori Hirai",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A48082429",
            "name": "Takeshi Nakaura",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A2098854171",
            "name": "Naofumi Yoshida",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A1990997668",
            "name": "Naoki Kobayashi",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A2111349570",
            "name": "Kaori Shiraishi",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A2499407759",
            "name": "Yasunori Nagayama",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A2562861383",
            "name": "Hiroyuki Uetani",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A1989317687",
            "name": "Masafumi Kidoh",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A5058892987",
            "name": "Masamichi Hokamura",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A2228396312",
            "name": "Yoshinori Funama",
            "affiliations": [
                "Kumamoto University"
            ]
        },
        {
            "id": "https://openalex.org/A2163461840",
            "name": "Toshinori Hirai",
            "affiliations": [
                "Kumamoto University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3089474066",
        "https://openalex.org/W2965300902",
        "https://openalex.org/W3184019679",
        "https://openalex.org/W3127382155",
        "https://openalex.org/W4308578764",
        "https://openalex.org/W4220823455",
        "https://openalex.org/W3214912989",
        "https://openalex.org/W4382182493",
        "https://openalex.org/W4206368078",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4362522726",
        "https://openalex.org/W4385570802",
        "https://openalex.org/W3155332104",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W4287855108"
    ],
    "abstract": "Abstract Purpose In this preliminary study, we aimed to evaluate the potential of the generative pre-trained transformer (GPT) series for generating radiology reports from concise imaging findings and compare its performance with radiologist-generated reports. Methods This retrospective study involved 28 patients who underwent computed tomography (CT) scans and had a diagnosed disease with typical imaging findings. Radiology reports were generated using GPT-2, GPT-3.5, and GPT-4 based on the patient’s age, gender, disease site, and imaging findings. We calculated the top-1, top-5 accuracy, and mean average precision (MAP) of differential diagnoses for GPT-2, GPT-3.5, GPT-4, and radiologists. Two board-certified radiologists evaluated the grammar and readability, image findings, impression, differential diagnosis, and overall quality of all reports using a 4-point scale. Results Top-1 and Top-5 accuracies for the different diagnoses were highest for radiologists, followed by GPT-4, GPT-3.5, and GPT-2, in that order (Top-1: 1.00, 0.54, 0.54, and 0.21, respectively; Top-5: 1.00, 0.96, 0.89, and 0.54, respectively). There were no significant differences in qualitative scores about grammar and readability, image findings, and overall quality between radiologists and GPT-3.5 or GPT-4 ( p &gt; 0.05). However, qualitative scores of the GPT series in impression and differential diagnosis scores were significantly lower than those of radiologists ( p &lt; 0.05). Conclusions Our preliminary study suggests that GPT-3.5 and GPT-4 have the possibility to generate radiology reports with high readability and reasonable image findings from very short keywords; however, concerns persist regarding the accuracy of impressions and differential diagnoses, thereby requiring verification by radiologists.",
    "full_text": "Vol:.(1234567890)\nJapanese Journal of Radiology (2024) 42:190–200\nhttps://doi.org/10.1007/s11604-023-01487-y\n1 3\nORIGINAL ARTICLE\nPreliminary assessment of automated radiology report generation \nwith generative pre‑trained transformers: comparing results \nto radiologist‑generated reports\nTakeshi Nakaura1  · Naofumi Yoshida1 · Naoki Kobayashi1 · Kaori Shiraishi1 · Yasunori Nagayama1 · \nHiroyuki Uetani1 · Masafumi Kidoh1 · Masamichi Hokamura1 · Yoshinori Funama2 · Toshinori Hirai1\nReceived: 7 July 2023 / Accepted: 29 August 2023 / Published online: 15 September 2023 \n© The Author(s) 2023\nAbstract\nPurpose In this preliminary study, we aimed to evaluate the potential of the generative pre-trained transformer (GPT) series \nfor generating radiology reports from concise imaging findings and compare its performance with radiologist-generated \nreports.\nMethods This retrospective study involved 28 patients who underwent computed tomography (CT) scans and had a diag-\nnosed disease with typical imaging findings. Radiology reports were generated using GPT-2, GPT-3.5, and GPT-4 based \non the patient’s age, gender, disease site, and imaging findings. We calculated the top-1, top-5 accuracy, and mean average \nprecision (MAP) of differential diagnoses for GPT-2, GPT-3.5, GPT-4, and radiologists. Two board-certified radiologists \nevaluated the grammar and readability, image findings, impression, differential diagnosis, and overall quality of all reports \nusing a 4-point scale.\nResults Top-1 and Top-5 accuracies for the different diagnoses were highest for radiologists, followed by GPT-4, GPT-3.5, \nand GPT-2, in that order (Top-1: 1.00, 0.54, 0.54, and 0.21, respectively; Top-5: 1.00, 0.96, 0.89, and 0.54, respectively). \nThere were no significant differences in qualitative scores about grammar and readability, image findings, and overall qual-\nity between radiologists and GPT-3.5 or GPT-4 (p > 0.05). However, qualitative scores of the GPT series in impression and \ndifferential diagnosis scores were significantly lower than those of radiologists (p  < 0.05).\nConclusions Our preliminary study suggests that GPT-3.5 and GPT-4 have the possibility to generate radiology reports with \nhigh readability and reasonable image findings from very short keywords; however, concerns persist regarding the accuracy \nof impressions and differential diagnoses, thereby requiring verification by radiologists.\nKeywords Radiology report · Computed tomography · Deep learning · Large language model · Generative pre-trained \ntransformer\n * Takeshi Nakaura \n kff00712@nifty.com\n Naofumi Yoshida \n yoshida.nfm25@gmail.com\n Naoki Kobayashi \n kobayashi.qm@gmail.com\n Kaori Shiraishi \n kaorinpa27@gmail.com\n Yasunori Nagayama \n nag_poo777@yahoo.co.jp\n Hiroyuki Uetani \n hiromaelen5@gmail.com\n Masafumi Kidoh \n masafkidoh@yahoo.co.jp\n Masamichi Hokamura \n deepimpacted@gmail.com\n Yoshinori Funama \n funama@kumamoto-u.ac.jp\n Toshinori Hirai \n t-hirai@kumamoto-u.ac.jp\n1 Department of Diagnostic Radiology, Graduate School \nof Medical Sciences, Kumamoto University, 1-1-1 Honjo, \nChuo-ku, Kumamoto-shi, Kumamoto 860-8556, Japan\n2 Department of Medical Physics, Faculty of Life Sciences, \nKumamoto University, Honjo 1-1-1, Kumamoto 860-8556, \nJapan\n191Japanese Journal of Radiology (2024) 42:190–200 \n1 3\nAbbreviations\nAI  Artificial intelligence\nCT  Computed tomography\nGPT  Generative pre-trained transformer\nLLM  Large language model\nMAP  Mean average precision\nMRI  Magnetic resonance imaging\nUSMLE  United States medical licensing examination\nIntroduction\nIn recent years, significant advancements have been made \nin the field of diagnostic imaging due to the development of \ncomputed tomography (CT), magnetic resonance imaging \n(MRI), and other imaging equipment, as well as advances \nin scanning methods. As a result, the utilization of these \nimaging modalities has been on the rise in many countries. \nThe role of radiology reports has become extremely impor-\ntant in the diagnosis and treatment of various diseases [1 ]. \nHowever, the increase in reading and reporting work has \nled to the problem of radiologist burnout [2 ]. Radiologists \nare facing an overwhelming amount of data to analyze and \nreport, which can lead to errors and delays in making radiol-\nogy reports. Therefore, it is crucial to find ways to address \nthe issue of radiologist burnout while maintaining the qual-\nity of radiology reports.\nThe rapid development of artificial intelligence (AI) \nhas demonstrated its value across various fields, and it has \nbecome a prominent topic in the realm of diagnostic radi-\nology [3 –7]. In radiology reports, AI has shown promise \nin the interpretation of simple chest radiographs [8 , 9], but \nits performance in more complex modalities like CT and \nMRI remains limited. The generative pre-trained transformer \n(GPT), an advanced large language model (LLM) [10], has \nrecently gained significant attention due to its ability to pro-\nduce human-like sentences with ChatGPT, a chat system \nbased on GPT-3.5. Additionally, recent research has shown \nthat GPT performed at or near the passing threshold for all \nthree exams of the USMLE without any specialized training \nor reinforcement [11], and the usefulness for the transforma-\ntion and the summarization of radiology reports [12, 13]. \nOn the other hand, it has been reported that LLMs such \nas the GPT series have the potential to generate inaccurate \ncontent, referred to as hallucinations [14]. Consequently, we \nhypothesize that GPT models have the potential for generat-\ning radiology reports or assisting radiologists in writing such \nreports; however, research exploring these specific applica-\ntions remains limited.\nIn this preliminary study, we embarked on an initial eval-\nuation of the GPT series’ potential to generate radiology \nreports from simple imaging findings, comparing its perfor-\nmance to that of radiologist-generated reports.\nMethods\nStudy design and population\nThis retrospective study was approved by the institutional \nreview board. Informed consent for this retrospective \nstudy was waived by the institutional ethics committee. \nThis single-center, retrospective study was conducted to \nevaluate the potential of the GPT series in generating radi-\nology reports and compare their performance with radi-\nologist-generated reports. For the purpose of calculating \nthe Mean Average Precision (MAP) in this study, a list of \nimage findings and their corresponding differential diag-\nnoses were required to feed into the GPT series. Hence, \nwe distilled a concise list of 28 image findings along with \ntheir associated differential diagnoses from the “Radiol-\nogy Review Manual, 8th ed” ( https://  www. wolte rsklu wer. \ncom/ en/ solut ions/ ovid/ radio logy- review- manual- 3568), a \nreference extensively used in radiology, as presented in \nTable  1. The 28 image findings selected for this study \nare commonplace in radiology reports, and it has been \nconfirmed by the authors, as per the review manual, that \ndifferential diagnoses can be reasonably inferred solely \nfrom these image findings. Subsequently, a search was \nperformed within the radiology reporting system, focus-\ning on reports containing these basic findings within the \ndesignated timeframe of January 2020 to February 2023. \nThe portions of the radiologists’ reports corresponding to \n“image findings”, “impression”, and “differential diagno-\nsis” of the selected patients were utilized as benchmarks \nfor comparison against the reports generated by the GPT \nseries.\nWe identified 28 patients whose radiology reports \nincluded selected basic findings as shown in Table  1 \nbetween October 2002 and February 2023, and these \nindividuals were the subjects of this investigation. Subse-\nquently, we included 28 patients (12 males and 16 females) \naged 5 to 86 years, with a mean age of 59.0 ± 24.1 years. \nWe searched the patients’ medical records and confirmed \ntheir final diagnoses, which were determined by surgical \nprocedures, biopsy, or follow-up observations, and docu-\nmented these in Table  1. All assessments were conducted \nsuccessfully for these patients.\nRadiology report generation\nRadiology reports were generated using GPT-2, GPT-3.5, \nand GPT-4 models based on the patient’s age, gender, \ndisease site, and imaging findings. To generate radiol-\nogy reports using the GPT series, we used the custom-\nized prompt before the above information of each patient \n192 Japanese Journal of Radiology (2024) 42:190–200\n1 3\n(Fig. 1). This prompt was generated based on a previously \nreported radiology reporting guide [1 ]. To access GPT-2 \nand GPT-3.5, the researchers utilized OpenAI’s applica-\ntion programming interface (API), which can be found at \nhttps:// openai. com/. This API is a system that processes \nuser inputs automatically on the service provider’s end and \nsubsequently returns the processed data. In this case, the \nsystem was employed to send prompts and patient infor -\nmation to OpenAI, which then generated the radiology \nreports. During the period of this study, the model equiva-\nlent to GPT-2 was “text-davinci-002”, and the one cor -\nresponding to GPT-3.5 was “gpt-3.5-turbo”; both models \nwere used in this study. Unfortunately, in the study period, \nGPT-4’s API had not yet been officially released, and the \npermission for use by the public was limited to a cus -\ntomized version for Microsoft Bing Chat. As a result, the \nresearchers input the above prompt and information into \nMicrosoft Bing Chat to obtain the radiology reports. The \nGPT series utilized in this paper is an advanced form of \nthe transformer, undergoing a conversion process termed \n“token”. Details about this process can be found in the \nsupplementary material. Figure  2 illustrates an example of \na radiology report with visualized tokens generated using \nthe “tokenizer” (https:// platf  orm. openai. com/ token izer).\nFor the sake of comparison, previously generated radi-\nologist-authored reports written in Japanese from the same \npatients were also included in the study. Any unstructured \nsections or content written in Japanese was translated \ninto English by a board-certified radiologist with over \n20 years of research experience in the English language, \nand another board-certified radiologist with 10 years of \nEnglish research experience checked translated English \nreports. The translations were based on the previously \nreported Radiology Reporting Guide [1 ] as well as reports \ngenerated by GPT series and followed the categories \n“Findings”, “Impressions”, and “Differential Diagnosis”. \nWe used these translated reports as the radiology reports \nTable 1  Findings and final diagnoses\n*The findings are as stated in the “Radiology Review Manual, 8th ed.” and the capitalization and other formatting adhere to that source\nOrgan Finding (*) Final diagnosis\nBrain 1. Calcified intracranial mass 1. Anaplastic oligodendroglioma\n2. Cyst with mural nodule 2. Pilocytic astrocytoma\n3. Dense cerebral mass 3. Medulloblastoma\n4. Deep ring-enhancing lesion 4. Metastatic brain tumor\n5. Well-defined superficial enhancing mass 5. Meningioma\n6. Multifocal enhancing lesions 6. Malignant lymphoma\n7. Cystic mass in cerebellar hemisphere 7. Hemangioblastoma\n8. Enhancing supra- and intrasellar mass 8. Pituitary adenoma\n9. Suprasellar mass with calcification 9. Craniopharyngioma\nLung 10. Random nodules 10. Multiple pulmonary metastases\n11. Centrilobular nodules without ground-glass opacities 11. Atypical mycobacterial infection\n12. Pulmonary mass with air bronchogram 12. Lung cancer\n13. Multiple pulmonary calcifications 13. Old tuberculosis\nMediastinum 14. Fat-containing mediastinal mass left atrial mass 14. Lipoma\n15. Right atrial mass 15. Myxoma\n16. Left atrial mass 16. Thrombus\nLiver 17. Ring-enhancing targetlike liver mass 17. Intrahepatic cholangiocarcinoma\n18. Hypervascular mass in normal liver 18. Hepatic adenoma\n19. Hypervascular mass in chronic liver disease 19. Hepatocellular carcinoma\n20. Hypervascular mass with central scar 20. Focal nodular hyperplasia\n21. Delayed phase-enhancing lesion 21. Hemangioma\nPancreas 22. Hypervascular pancreatic tumors 22. Islet cell tumor\n23. Pancreatic cyst with solid component 23. Mucinous cystadenocarcinoma\n24. Macrocystic lesion of pancreas 24. Intraductal papillary mucinous neoplasm\nOthers 25. Mucocele of appendix 25. Appendiceal mucinous neoplasm\n26. Heterogeneous fat-containing retroperitoneal mass 26. Liposarcoma\n27. Fat-containing adrenal mass 27. Adrenal adenoma\n28. Fat-containing renal mass 28. Angiomyolipoma\n193Japanese Journal of Radiology (2024) 42:190–200 \n1 3\nwritten by radiologists for comparison with radiology \nreports generated by GPT series.\nQuantitative analysis of differential diagnosis\nThe quality of different diagnoses in radiology reports \nwas assessed quantitatively by calculating the top-1, top-5 \naccuracy, and MAP of radiology reports for radiologists, \nGPT-2, GPT-3.5, and GPT-4. Top-1 and top-5 accura -\ncies were defined as the proportion of cases where the \nfinal diagnosis was ranked first and within the top five, \nrespectively. The MAP was calculated to provide a com-\nprehensive evaluation of the ranking of correct diagnoses \nby comparing the generated differential diagnoses to the \nreference differential diagnoses in the “Radiology Review \nManual, 8th ed”. Precision was calculated for each rel-\nevant diagnosis at each rank (i.e., if the correct diagnosis \nappeared at rank 1, 2, 3, etc.), and the average of these \nprecision values was computed to obtain the MAP. This \nmeasure accounts for both the precision and the recall of \nFig. 1  An example of “Prompt” \nand “Information of a patient”. \nBefore inputting actual patient \ndata into the GPT series, it is \nnecessary to provide guidance \non the role and type of text to \nbe generated. This instruction is \ncalled a “Prompt”. The prompt \nserves as a way to inform the \nlanguage model about the \ncontext and desired output. In \nthis case, the example prompt \nexplains that the output should \nbe from the perspective of a \nradiologist and the purpose \nof each part of the radiology \nreport. The “Prompt” is com-\nmon for all patients, and only \nthe “Main text” portion varies \nfor each individual patient. This \napproach ensures that the lan-\nguage model receives consistent \ncontextual information while \ntailoring the generated report \nto the specific details of each \npatient's case\n\n194 Japanese Journal of Radiology (2024) 42:190–200\n1 3\nthe generated differential diagnoses, offering a more holis-\ntic assessment of the radiology report quality.\nQualitative analysis\nOne board-certified radiologist and one radiology resident, \nblinded to the report’s origin, were tasked with indepen-\ndently evaluating multiple aspects of the radiology reports. \nThese aspects included grammar and readability, image find-\nings, impression, differential diagnosis, and overall quality. \nTo facilitate a systematic and objective evaluation, a 4-point \nLikert scale was employed, with the following rating catego-\nries: 1 (poor), 2 (fair), 3 (good), and 4 (excellent).\nFor the assessment of grammar and readability, the evalu-\nators considered the clarity and coherence of the report, the \naccuracy of syntax, and the appropriateness of the language \nused. In evaluating imaging findings, we assessed whether \nimaging findings that corresponded to the abnormal find-\nings entered were appropriately described and whether con-\ntradictory findings were written. In assessing impressions, \nthe evaluator considered whether the considerations drawn \nfrom the imaging findings were consistent and reasonable \nFig. 2  A visualization of tokens used in generative pre-trained trans-\nformer (GPT) series. GPT series and other transformer-based mod-\nels perform language processing (a) in units called “tokens”, each of \nwhich has a unique identifier (b). The task of text generation is inter -\nnally processed as selecting the token with the highest probability of \nappearing after a particular sequence of tokens. This approach allows \nthe model to generate coherent and contextually appropriate text by \npredicting and selecting the most likely tokens to follow a given input \nsequence\n195Japanese Journal of Radiology (2024) 42:190–200 \n1 3\nto guide an accurate differential diagnosis. In evaluating dif-\nferential diagnoses, the validity, and scope of the alternative \ndiagnoses listed in the report were reviewed based on the \nfindings entered, age, and sex. Lastly, the overall quality \nof the reports was evaluated based on the integration of the \naforementioned criteria. Disagreements between the two \nevaluators were resolved through discussion until a consen-\nsus was reached.\nStatistical analysis\nWe compared the accuracy of differential diagnoses between \nradiologists and the GPT series using McNemar’s test. Qual-\nitative results are presented as the median and interquar -\ntile range (IQR) due to the non-normal distribution of the \ndata, and differences between the scores of radiologists and \nthose of the GPT series were compared using the Wilcoxon \nsigned-rank test. For multiple comparisons, Holm’s correc-\ntion was applied. To assess inter-reader agreement, weighted \nCohen’s Kappa analyses were conducted (kappa ≤ 0.40, \npoor agreement; 0.40 < kappa ≤ 0.60, moderate agreement; \n0.60 < kappa ≤ 0.80, good agreement; and kappa > 0.80, \nexcellent agreement). All statistical analyses were performed \nusing the free programming software Python version 3.8.5 \n(https:// www. python. org). A two-tailed p-value < 0.05 was \ndeemed significant.\nResults\nTable 2 shows the results of the quantitative analysis of the \ndifferent diagnoses. Top-1 and Top-5 accuracies for the dif-\nferent diagnoses were highest for radiologists, followed by \nGPT-4, GPT-3.5, and GPT-2, in that order (Top-1: 1.00, \n0.54, 0.54, and 0.21, respectively; Top-5: 1.00, 0.96, 0.89, \nand 0.54, respectively). In Top-1 accuracies, there were \nsignificant differences between the radiologists and GPT \nseries (p < 0.01). In Top-5 accuracies, there was a significant \ndifference between the radiologists and GPT-2 (p  < 0.05); \nhowever, there were no significant differences between the \nradiologists and GPT-3.5 ( p = 0.50) and GPT-4 ( p = 1.00). \nMAPs for the different diagnoses were highest for radiolo-\ngists, followed by GPT-4, GPT-3.5, and GPT-2, in that order \n(0.97, 0.26, 0.45, and 0.54, respectively).\nTable  3 and Fig.  3 show the results of the qualitative \nanalysis of the radiology reports. There were no signifi-\ncant differences in qualitative scores about grammar and \nreadability, image findings, and overall quality between \nradiologists and GPT-3.5 or GPT-4 ( p > 0.05). However, \nTable 2  Quantitative analysis\nGPT Generative pre-trained transformer, MAP mean average precision\n1 **p < 0.01; ***p < 0.001\nRadiologists GPT-2 GPT-3.5 GPT-4 (Bing) p-value1\nRadiologists vs. GPT-2 Radiologists vs. \nGPT-3.5\nRadi-\nologists vs. \nGPT-4\nTop-1 accuracy 1.00 0.21 0.54 0.54  < 0.001*** 0.002** 0.002**\nTop-5 accuracy 1.00 0.46 0.89 0.96  < 0.001*** 0.50 1.00\nMAP 0.97 0.26 0.45 0.54\nTable 3  Qualitative analysis\nGPT Generative pre-trained transformer, MAP mean average precision\n1 Median (interquartile range)\n2 *p < 0.05; **p < 0.01; ***p < 0.001\nRadiologists1 GPT-21 GPT-3.51 GPT-4 (Bing)1 p-value2\nRadiologistss \nvs. GPT-2\nRadiologists \nvs. GPT-3.5\nRadiologists \nvs. GPT-4\nGrammar & readability 4.0 (3.0, 4.0) 3.0 (3.0, 3.25) 4.0 (4.0, 4.0) 4.0 (4.0, 4.0) 0.02* 0.29 0.29\nImage finding 4.0 (3.0, 4.0) 3.0 (2.0, 4.0) 4.0 (3.0, 4.0) 4.0 (4.0, 4.0) 0.069 0.477 0.060\nImpression 3.0 (3.0, 4.0) 2.0 (1.0, 2.25) 2.5 (2.0, 3.0) 3.0 (2.0, 3.0)  < 0.001*** 0.002** 0.003**\nDifferential diagnosis 3.0 (3.0, 3.0) 2.0 (2.0, 2.0) 2.0 (2.0, 3.0) 2.0 (2.0, 3.0)  < 0.001*** 0.002** 0.034\nOverall quality 3.0 (2.75, 3.0) 2.0 (1.0, 2.0) 2.0 (2.0, 3.0) 3.0 (2.0, 3.0) 0.001** 0.072 0.23\n196 Japanese Journal of Radiology (2024) 42:190–200\n1 3\nqualitative scores of radiologists for impression and dif-\nferential diagnosis were significantly higher than those of \nthe GPT series (p  < 0.05). The Kappa analysis indicated \npoor to moderate concordance (grammar and readability: \nκ = 0.72, image findings: κ = 0.59, impression: κ = 0.46, \ndifferential diagnosis: κ = 0.41, overall quality: κ = 0.42).\nRepresentative cases and reports produced by GPT \nseries are shown in Figs.  4, 5 and 6.\nDiscussion\nThe present study evaluated the feasibility of employing \nthe GPT series (GPT-2, GPT-3.5, and GPT-4) to gener -\nate radiology reports based on concise imaging findings \nin comparison to those generated by radiologists. Our \nfindings reveal that, although GPT-3.5 and GPT-4 are \ncommendable in their ability to generate readable and \nappropriate “image findings” and “Top-5 differential diag-\nnoses” from very limited information, they fall short in the \naccuracy of impressions and differential diagnoses even \nfor very basic and representative findings of CT. Conse-\nquently, these results underscore the continued importance \nof radiologist involvement in the validation and interpreta -\ntion of radiology reports generated by GPT models.\nGPT is a pre-trained language model that uses a trans-\nformer architecture [15] for natural language processing. It \nwas first introduced by OpenAI in 2018 with the release \nof GPT-1, which was trained on a large corpus of text data \nusing unsupervised learning techniques and had 117 mil-\nlion parameters [10]. GPT-2 with 1.5 billion parameters was \nreleased in 2019 and was notable for its ability to generate \ncoherent and realistic text [ 16]. In 2020, OpenAI released \nGPT-3 with 175 billion parameters, which was even more \npowerful and capable of performing a wide range of lan-\nguage tasks [17]. ChatGPT is a specific implementation of \nthe GPT model that is designed for conversational applica-\ntions using GPT-3.5, which employs a similar architecture \nto the original GPT-3 but is fine-tuned using reinforcement \nlearning from human feedback [18]. While details about \nGPT-4 have not been fully disclosed at the time of this study, \nit is said to be a more natural and accurate natural language \nmodel than GPT-3.5. However, the ability of the GPT series \nto generate radiology reports has not been fully evaluated.\nThe preferred results of this study are that GPT-3.5 and \nGPT-4 have the potential to produce radiology reports with \nhuman-like readability and grammar from simple imaging \nfindings, and the Top-5 differential diagnoses generated \nby GPT-3.5 and GPT-4 also demonstrated human-like \nabilities. This is a noteworthy finding, as it highlights the \nability of these advanced language models to synthesize \ncomplex and coherent reports from minimal input data, \nwhich could facilitate faster and more efficient radiology \nreporting. In addition, although it may be slightly differ -\nent from the original usage of LLM, the results of our \nstudy suggest GPT3.5 and GPT-4 have internal medical \nknowledge reserves about diagnostic radiology, and it has \na possibility of the models’ capacity to provide a compre-\nhensive and relevant list of potential diagnoses that can \naid in clinical decision-making and improve patient care.\nOn the other hand, it is essential to recognize that the \naccuracy of Top-1 differential diagnosis, MAP, and the \nFig. 3  Qualitative analysis. Violin plots show qualitative analysis of \nthe image findings (a) and the overall quality (b)\n197Japanese Journal of Radiology (2024) 42:190–200 \n1 3\nqualitative score of the impression and different diag-\nnoses of the GPT-generated reports are inferior to those \nof radiologist-generated reports. The result, in which \nthe differential diagnosis from a radiologist’s report—\nderived from a more comprehensive set of information—is \nsuperior to a GPT-generated report based solely on age, \ngender, location, and rudimentary imaging findings, is \nindeed expected. However, it is clear from the qualita-\ntive analysis that GPT-4 can generate reports similar to \nthose of radiologists, and it would be quite difficult for a \nphysician ordering a CT scan to differentiate them based \nsolely on the generated reports. The results of this study \nsuggest that LLM, which can generate practical “image \nfindings”, “impression”, and “differential diagnosis” with \nonly very simple keywords, is likely to be very useful as an \naid in creating diagnostic imaging reports. However, LLM \nshould be used only as an aid, and differential diagnosis \nshould be made by a radiologist who can make a compre-\nhensive judgment.\nAnother important potential limitation of the LLM, \nincluding the GPT series, is the possibility that these models \nmay generate findings with content that cannot be directly \nlinked to the input information, a phenomenon known as \n‘hallucination” [ 19]. LLMs fundamentally select the next \nmost probable word without necessarily considering logical \nconnections or coherence. Consequently, they may gener -\nate content unrelated to the input data but highly associated \nwith the generated text. The mechanism of hallucinations \nhas not been fully elucidated yet [19], and preventing hallu-\ncinations can be difficult even in language models designed \nto mitigate this issue, with reports indicating that hallucina-\ntions may sometimes be amplified instead [20]. This issue \nwas observed in the current study, suggesting that caution \nis needed when applying LLMs to medical reports. Addi -\ntionally, the evaluators had differing opinions on whether \nto consider the output of the GPT as detailed or as a hallu-\ncination, which resulted in poor to moderate agreements in \nqualitative analysis. One evaluator may have appreciated the \nFig. 4  A 69-year-old female \npatient with a suspected \npituitary adenoma. Non-contrast \nCT axial image (a), contrast-\nenhanced CT sagittal image \n(b) and generated radiology \nreports by GPT series (c) are \nshown. A tumor with homoge-\nneous enhancement is observed \nfrom the sella turcica to the \nsuprasellar region, suggesting a \npituitary adenoma. Information \ninput other than the prompt is \n“Age (years): 69, sex: female, \nmodality: contrast enhanced \nCT, location: the vicinity of the \nsella turcica, diameter: 49 mm, \nfindings: enhancing supra- and \nintrasellar mass”. The GPT-2 \nreport is a simple report written \naccording to the input informa-\ntion, and the differential diagno-\nsis seems relatively reasonable. \nIn the GPT-3.5 report, both the \nfindings and impression sec-\ntions are more detailed than in \nthe GPT-2 report. The GPT-4.0 \nreport is overall quite similar \nto a human-generated report, \nand the differential diagnosis is \nreasonable. However, it includes \ninformation that was not input, \nsuch as calcification and cystic \ndegeneration\n\n198 Japanese Journal of Radiology (2024) 42:190–200\n1 3\nadditional details provided by the GPT, believing that they \ncould potentially enhance the radiology report. On the other \nhand, other evaluators might have viewed the extra details \nas hallucinations, as they were not directly related to the \ninput information and could lead to inaccuracies or confu-\nsion in the diagnostic process. As such, it might be crucial \nto develop strategies for mitigating these hallucinations and \nensuring the accuracy of the generated radiology reports. In \nthe future, incorporating domain-specific knowledge into the \ntraining of GPT models may allow them to generate more \ncontextually accurate reports. However, confirmation by a \nradiologist would be essential to ensure the clinical utility \nand safety of the radiology reports generated by GPT.\nThere are limitations in our investigation. Firstly, a \nlimitation of this preliminary study is the relatively small \nsample size of 28 patients with simple image findings. In \nclinical settings, amalgamating multiple findings is impor -\ntant; however, the chosen image findings in this study were \ngrounded in the authors’ judgment, solely referencing one \ntextbook. Such a methodology can induce selection bias, \npotentially constraining the applicability of the outcomes. \nTherefore, additional investigations with a larger and more \ndiverse patient population, as well as more complex image \nfindings, are necessary to corroborate and expand upon \nthese initial results. Secondly, this study was conducted at \na single center and by non-native English speakers, poten-\ntially introducing biases related to specific institutional \npractices and the ability of English reading or writing of \nthe radiologists involved. Thirdly, LLMs, including the \nGPT series, are still in development, and in fact, GPT-4 \nhas become available for use via API before submission. \nAdditionally, GPT-4 (Microsoft Bing), used in this study, \nhas the advantage of being able to search for the latest \ninformation on the internet, which potentially makes it \nmore advantageous compared to other models. Moreover, \nvarious GPT models are being refined. The results of this \nstudy pertain to the time of journal submission and may \nchange in the future. Fourth, the potential for hallucina-\ntion, a notable weakness inherent to LLMs, could not be \nspecifically assessed in this study. This is due to the evalu-\nators not being given the patient’s history to prevent biased \nassessments of the report quality. As such, unless the eval-\nuator has a comprehensive understanding of the patient’s \nbackground, differentiating between detailed reporting \nand hallucination could prove difficult in such applica -\ntions. Lastly, the retrospective design of the study could \nhave led to various biases. Furthermore, with this study \ndesign, we cannot evaluate the impact of GPT-generated \nFig. 5  A 31-year-old female \nwith a hepatic hemangioma. \nThe contrast-enhanced CT arte-\nrial phase (a) shows heteroge-\nneous enhancement within the \nlesion, and the venous phase \n(b) reveals a generally stronger \nenhancement than the liver \nparenchyma, consistent with \ntypical findings of a heman-\ngioma. Generated radiology \nreports by GPT series (c) are \nalso shown. The information \ninputted besides the prompt is \n“Age (years): 31; sex: female; \nmodality: contrast enhanced \nCT; location: segment 8 in the \nliver; diameter: 22 mm; find-\nings: delayed phase-enhancing \nlesion”. In the GPT-2 gener-\nated report, a list of differential \ndiagnoses is not even created, \nand the impression primarily \nsuspects hepatocellular carci-\nnoma. In the GPT-3.5 generated \nreport, although the format is \nwell organized, hepatocellular \ncarcinoma is still listed as the \ntop differential diagnosis. The \nGPT-4.0 generated report is \ngenerally quite good, with rea-\nsonable differential diagnoses\n\n199Japanese Journal of Radiology (2024) 42:190–200 \n1 3\nreports on the radiologists’ efforts. Future research involv -\ning larger, multicenter, prospective studies with diverse \npatient populations and a focus on evaluating the impact \non radiologists’ efforts is warranted to validate and expand \nupon these findings.\nIn conclusion, our preliminary study suggests that GPT-\n3.5 and GPT-4 have the possibility to generate radiology \nreports with high readability and reasonable image find-\nings from very short keywords; however, concerns persist \nregarding the accuracy of impressions and differential \ndiagnoses, thereby requiring verification by radiologists.\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/ s11604- 023- 01487-y.\nAcknowledgements We used the deepL write and GPT-4 for English \nproofreading. The generated text was read, revised and proofed by the \nauthors.\nFunding No funding.\nData sharing statement The datasets generated or analyzed during \nthe study are available from thecorresponding author on reasonable \nrequest.\nDeclarations \nConflict of interest Toshinori Hirai has received research support from \nCanon Medical Systems.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\n 1. Hartung MP, Bickle IC, Gaillard F, Kanne JP. How to create a \ngreat radiology report. Radiographics. 2020;40:1658–70.\nFig. 6  A 75-year-old male with \nan angiomyolipoma in the right \nkidney. A non-contrast CT (a) \nand contrast-enhanced CT (b) \nreveal a fatty renal mass in the \nright kidney. Generated radiol-\nogy reports by GPT series (c) \nare also shown. The information \ninputted besides the prompt is \n“Age (years): 75; sex: male; \nmodality: contrast-enhanced \nCT; location: the inferior \nportion of the right kidney; \ndiameter: 35 mm; findings: \nfat-containing renal mass”. In \nthe GPT-2 generated report, \nthe possibility of a renal tumor \naccompanied by surround-\ning edema is low, and a list of \ndifferential diagnoses is not \neven created. In the GPT-3.5 \ngenerated report, although the \nlesion is located in the kidney, \nthe differential diagnoses \ninclude adrenal adenoma. In the \nGPT-4.0 generated report, the \noverall quality is quite good; \nhowever, there is a description \nof “calcification” in the image \nfindings, which was not part of \nthe input information\n\n200 Japanese Journal of Radiology (2024) 42:190–200\n1 3\n 2. Parikh JR, Wolfman D, Bender CE, Arleo E. Radiologist burn -\nout according to surveyed radiology practice leaders. J Am Coll \nRadiol. 2020;17:78–81.\n 3. Kitahara H, Nagatani Y, Otani H, Nakayama R, Kida Y, Sonoda \nA, et al. A novel strategy to develop deep learning for image \nsuper-resolution using original ultra-high-resolution computed \ntomography images of lung as training dataset. Jpn J Radiol. \n2022;40:38–47.\n 4. Barat M, Chassagnon G, Dohan A, Gaujoux S, Coriat R, Hoeffel \nC, et al. Artificial intelligence: a critical review of current applica-\ntions in pancreatic imaging. Jpn J Radiol. 2021;39:514–23.\n 5. Chassagnon G, De Margerie-Mellon C, Vakalopoulou M, Marini \nR, Hoang-Thi T-N, Revel M-P, et al. Artificial intelligence in \nlung cancer: current applications and perspectives. Jpn J Radiol. \n2023;41:235–44.\n 6. Yan S, Zhang H, Wang J. Trends and hot topics in radiology, \nnuclear medicine and medical imaging from 2011–2021: a \nbibliometric analysis of highly cited papers. Jpn J Radiol. \n2022;40:847–56.\n 7. Yasaka K, Akai H, Sugawara H, Tajima T, Akahane M, Yoshioka \nN, et al. Impact of deep learning reconstruction on intracranial 1.5 \nT magnetic resonance angiography. Jpn J Radiol. 2022;40:476–83.\n 8. Sun Z, Ong H, Kennedy P, Tang L, Chen S, Elias J, et al. Evaluat-\ning GPT-4 on impressions generation in radiology reports. Radiol-\nogy. 2023;307: e231259.\n 9. Sirshar M, Paracha MFK, Akram MU, Alghamdi NS, Zaidi SZY, \nFatima T. Attention based automated radiology report generation \nusing CNN and LSTM. PLoS ONE. 2022;17: e0262209.\n 10. Radford A, Narasimhan K. Improving language understanding \nby generative pre-training. 2018 [cited 2023 Apr 9]. Available \nfrom: https:// www. seman ticsc holar. org/ paper/ Impro ving- Langu \nage- Under stand ing- by- Gener ative- Radfo rd- Naras imhan/ cd188 \n00a0f e0b66 8a1cc 19f2e c95b5 003d0 a5035. Accessed 2 Apr 2023.\n 11. Kung TH, Cheatham M, Medenilla A, Sillos C, Leon LD, Elepaño \nC, et al. Performance of ChatGPT on USMLE: potential for AI-\nassisted medical education using large language models. Plos \nDigital Health. 2023;2: e0000198.\n 12. Adams LC, Truhn D, Busch F, Kader A, Niehues SM, Makowski \nMR, et al. Leveraging GPT-4 for post hoc transformation of free-\ntext radiology reports into structured reporting: a multilingual \nfeasibility study. Radiology. 2023;307: e230725.\n 13. Van Veen D, Van Uden C, Attias M, Pareek A, Bluethgen C, \nPolacin M, et al. RadAdapt: Radiology report summarization via \nlightweight domain adaptation of large language models [Inter -\nnet]. arXiv; 2023 [cited 2023 Aug 21]. Available from: http://  \narxiv. org/ abs/ 2305. 01146. Accessed 2 Apr 2023.\n 14. Liu T, Zhang Y, Brockett C, Mao Y, Sui Z, Chen W, et al. A \ntoken-level Reference-free hallucination detection benchmark for \nfree-form text generation. Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: \nLong Papers) [Internet]. Dublin, Ireland: Association for Compu-\ntational Linguistics; 2022 [cited 2023 Apr 12]. p. 6723–37. Avail-\nable from: https:// aclan tholo gy. org/ 2022. acl- long. 464. Accessed \n2 Apr 2023.\n 15. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez \nAN, et al. Attention Is All You Need [Internet]. arXiv; 2017 [cited \n2023 Feb 27]. Available from: http:// arxiv. org/ abs/ 1706. 03762. \nAccessed 2 Apr 2023.\n 16. Papers with Code - language models are unsupervised multitask \nlearners [Internet]. [cited 2023 Apr 17]. Available from: https://  \npaper swith code. com/ paper/ langu age- models- are- unsup ervis ed- \nmulti task. Accessed 2 Apr 2023.\n 17. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, \net al. Language models are few-shot learners. Advances in Neural \nInformation Processing Systems [Internet]. Curran Associates, \nInc.; 2020 [cited 2023 Apr 17]. p. 1877–901. Available from: \nhttps:// papers. nips. cc/ paper/ 2020/ hash/ 1457c 0d6bf cb496 7418b \nfb8ac 142f6 4a- Abstr act. html. Accessed 2 Apr 2023.\n 18. Ouyang L, Wu J, Jiang X, Almeida D, Wainwright CL, Mishkin P, \net al. Training language models to follow instructions with human \nfeedback [Internet]. arXiv; 2022 [cited 2023 Apr 17]. Available \nfrom: http:// arxiv. org/ abs/ 2203. 02155. Accessed 2 Apr 2023.\n 19. Ji Z, Lee N, Frieske R, Yu T, Su D, Xu Y, et al. Survey of hal-\nlucination in natural language generation. ACM Comput Surv. \n2023;55:1–38.\n 20. Sullivan Jr. J, Brackenbury W, McNutt A, Bryson K, Byll K, Chen \nY, et al. Explaining Why: How Instructions and User Interfaces \nImpact Annotator Rationales When Labeling Text Data. Proceed-\nings of the 2022 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human Language \nTechnologies [Internet]. Seattle, United States: Association for \nComputational Linguistics; 2022 [cited 2023 Apr 18]. p. 521–31. \nAvailable from: https:// aclan tholo gy. org/ 2022. naacl- main. 38. \nAccessed 2 Apr 2023.\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}