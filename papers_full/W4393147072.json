{
    "title": "Video-Context Aligned Transformer for Video Question Answering",
    "url": "https://openalex.org/W4393147072",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5030649664",
            "name": "Linlin Zong",
            "affiliations": [
                "Dalian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5109613450",
            "name": "Jiahui Wan",
            "affiliations": [
                "Dalian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5006269113",
            "name": "Xianchao Zhang",
            "affiliations": [
                "Dalian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5036472945",
            "name": "Xinyue Liu",
            "affiliations": [
                "Dalian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5000836998",
            "name": "Wenxin Liang",
            "affiliations": [
                "Dalian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5062895972",
            "name": "Bo Xu",
            "affiliations": [
                "Dalian University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2745461083",
        "https://openalex.org/W3015538334",
        "https://openalex.org/W4312052137",
        "https://openalex.org/W3174001836",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W2997805943",
        "https://openalex.org/W3159630763",
        "https://openalex.org/W3008341877",
        "https://openalex.org/W4382469051",
        "https://openalex.org/W4288099285",
        "https://openalex.org/W6838751577",
        "https://openalex.org/W4297947725",
        "https://openalex.org/W3203995540",
        "https://openalex.org/W3158340498",
        "https://openalex.org/W6691431627",
        "https://openalex.org/W6818723395",
        "https://openalex.org/W3112385539",
        "https://openalex.org/W4281663301",
        "https://openalex.org/W3177174258",
        "https://openalex.org/W4200631219",
        "https://openalex.org/W2765716052",
        "https://openalex.org/W2425121537",
        "https://openalex.org/W3216740993",
        "https://openalex.org/W4386075754",
        "https://openalex.org/W3168154341",
        "https://openalex.org/W4385571423",
        "https://openalex.org/W4287125738",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4383112678",
        "https://openalex.org/W2962934715",
        "https://openalex.org/W4307123990",
        "https://openalex.org/W4304098887",
        "https://openalex.org/W4287113019",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3034730770",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4312974690",
        "https://openalex.org/W4298613318",
        "https://openalex.org/W4226289673",
        "https://openalex.org/W3172523222",
        "https://openalex.org/W4287112932",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W4385572712",
        "https://openalex.org/W4386071468",
        "https://openalex.org/W4382457584",
        "https://openalex.org/W3034572008",
        "https://openalex.org/W4376312579",
        "https://openalex.org/W3167092180"
    ],
    "abstract": "Video question answering involves understanding video content to generate accurate answers to questions. Recent studies have successfully modeled video features and achieved diverse multimodal interaction, yielding impressive outcomes. However, they have overlooked the fact that the video contains richer instances and events beyond the scope of the stated question. Extremely imbalanced alignment of information from both sides leads to significant instability in reasoning. To address this concern, we propose the Video-Context Aligned Transformer (V-CAT), which leverages the context to achieve semantic and content alignment between video and question. Specifically, the video and text are encoded into a shared semantic space initially. We apply contrastive learning to global video token and context token to enhance the semantic alignment. Then, the pooled context feature is utilized to obtain corresponding visual content. Finally, the answer is decoded by integrating the refined video and question features. We evaluate the effectiveness of V-CAT on MSVD-QA and MSRVTT-QA dataset, both achieving state-of-the-art performance. Extended experiments further analyze and demonstrate the effectiveness of each proposed module.",
    "full_text": "Video-Context Aligned Transformer for Video Question Answering\nLinlin Zong1, Jiahui Wan1, Xianchao Zhang1, Xinyue Liu1*, Wenxin Liang1, Bo Xu2\n1 Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province,\nSchool of Software, Dalian University of Technology, Dalian 116620, China\n2School of Computer Science and Technology, Dalian University of Technology\n{llzong, xczhang, xyliu, wxliang, xubo}@dlut.edu.cn, wanjiahui1011@gmail.com\nAbstract\nVideo question answering involves understanding video con-\ntent to generate accurate answers to questions. Recent studies\nhave successfully modeled video features and achieved di-\nverse multimodal interaction, yielding impressive outcomes.\nHowever, they have overlooked the fact that the video con-\ntains richer instances and events beyond the scope of the\nstated question. Extremely imbalanced alignment of informa-\ntion from both sides leads to significant instability in reason-\ning. To address this concern, we propose the Video-Context\nAligned Transformer (V-CAT), which leverages the context\nto achieve semantic and content alignment between video and\nquestion. Specifically, the video and text are encoded into a\nshared semantic space initially. We apply contrastive learn-\ning to global video token and context token to enhance the\nsemantic alignment. Then, the pooled context feature is uti-\nlized to obtain corresponding visual content. Finally, the an-\nswer is decoded by integrating the refined video and question\nfeatures. We evaluate the effectiveness of V-CAT on MSVD-\nQA and MSRVTT-QA dataset, both achieving state-of-the-\nart performance. Extended experiments further analyze and\ndemonstrate the effectiveness of each proposed module.\nIntroduction\nVideo Question Answering (VideoQA) is a challenging task\nwithin the multimodal learning domain, aiming to under-\nstand videos and answer questions (Zhong et al. 2022).\nVideoQA not only requires precise semantic understanding\nof both the video and the question, but also need effectively\ninteractions to locate the most critical feature within the\nvideo with rich spatiotemporal information.\nCurrently, the mainstream paradigm for video question\nanswering is to encode the video and question using sepa-\nrate pre-trained models (He et al. 2016; Hara, Kataoka, and\nSatoh 2018; Devlin et al. 2018), and then fuse the visual\nand textual features through a complex interaction model\nfor classifying the final answer. Many existing works have\nexhibited powerful video content modeling abilities and\nachieved high performance on multi-modal information in-\nteraction. They input extracted features into spatio-temporal\n(Jin et al. 2021; Jiang et al. 2020; Dang et al. 2021; Gao\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nTwo elephants \nare pushing a \ncar forward.\nwho is singing\nin a car?\nhow many elep-\nhants are \npushing a car \nforward?\nWhat the color \nof the car?\n(a) Semantic Alignment\n(b) Content Alignment\nCommon Space\nVideo: Text:\nVideo:\nMatch\nContext:\nVideo\nEnc\nText\nEnc\nfeature\nFigure 1: The two challenges of video question answering\nare semantic alignment, which refers to ensuring that the se-\nmantics of the video and related text are consistent in the en-\ncoded features, and content alignment, which entails match-\ning the questions and related content in the video.\net al. 2023), hierarchical (Le et al. 2020; Liu et al. 2021;\nPeng et al. 2022; Xiao et al. 2022; Dang et al. 2021), multi-\nscale (Peng et al. 2022; Guo et al. 2021), or multi-granularity\n(Xiao et al. 2022) structured models, and generate useful\nanswers through complex and precise interactions. Further-\nmore, some researchers have focused on causal analysis (Li\net al. 2022b,a; Yu et al. 2023; Zang et al. 2023), answer-\ning questions by finding relevant information in the video.\nAlthough achieving promising performance improvement,\nthere exist two main challenges that hinder precisely match-\ning the exact answers. The first challenge lies in the semantic\nunalignment of visual contents in videos and textual con-\ntents in questions. To obtain accurately matched answers,\nsemantics in video and question need to be precisely aligned\ninto the same common space, and then generate the exact an-\nswer for the question. As shown in Figure 1(a), the text ’two\nelephants are pushing a car forward’ and its corresponding\nvideo event should be semantically aligned into the same\nfeature space for effectively question answering.\nThe second challenge lies in the content unalignment\nwithin videos that contain much richer information than a\nsingle question sentence. More complete coverage of video\ninformation provides a deeper understanding than a sin-\ngle question sentence that only aligns with a specific event\nwithin the video. To improve the content alignment, multiple\nsentences can be used to offer a more comprehensive cover-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19795\nage of video contents, which provides a large space for an-\nswer acquisition. Existing methods directly align the video\nand question, requiring the model to correspond vastly in-\nformative videos with information-scarce question-answer\npairs. This extremely imbalanced alignment of information\nfrom both sides leads to significant instability in reasoning.\nTherefore, we propose to first expand the information vol-\nume on the textual side to align it with the video, and then\nfurther obtain accurate answers through single-question in-\nteraction with the video. This gradually progressive video\nrefinement approach is smoother, resulting in more stable\nalignment between the two modalities, which is shown in\nFigure 1(b).\nTo address the above mentioned challenges, we treat the\nVideoQA task from a perspective of reading comprehension\n(Sun et al. 2022). Intuitively, To accelerate human reading\ncomprehension, one effective approach is to quickly skim\nthrough all the questions, and then read the article while\nbearing in mind an understanding of these questions. This\nhelps pinpoint the most important areas in the article that\nrequire attention. Afterwards, a more thorough reading of\nthe current question allows for a speedy and correct answer.\nThus, drawing on this thought process, we propose a Video\nContext Alignment Transformer (V-CAT), utilizing all the\nquestions related to videos as the context. Firstly, the pre-\ntrained models and a trainable encoder are used to extract\neach single modality features. Then, contrastive learning are\nused to semantically constrain global features of both the\nvideo and context, achieving semantic alignment for both\nmodalities. Afterward, aligning the contents of the video\nwith the context, we preliminarily extract the vital informa-\ntion that requires attention. Finally, interact granularly with\nthe video by the answer decoder, and use the global token of\nquestion to classify answer.\nOur work makes the following three contributions:\n• We revise the existing traditional paradigm and pro-\nposed V-CAT, the Video-Context Alignment Transformer\nmethod. By introducing context, we balance the informa-\ntional levels of both modalities and achieve preliminary\nsemantic and content alignment between the video and\ncontext. We then proceed to interact the current granular\nquestion and refined video content to predict answer.\n• We propose a semantic alignment method based on con-\ntrastive learning. By utilizing contrastive loss on global\nfeatures of videos and contexts within the same batch,\nthe semantic matching features are pulled closer while\nthe non-matching features are pushed apart, effectively\nenhancing the effectiveness of semantic alignment.\n• We conduct experiments on the traditional datasets\nMSVD-QA (Xu et al. 2017) and MSRVTT-QA (Xu\net al. 2016), and obtained state-of-the-art performance,\ndemonstrating the effectiveness of our proposed method.\nIn addition, we conducted further analysis on each mod-\nule through extended experiments, confirming the re-\nmarkable ability enhancement brought about by each\nmodule.\nRelated Work\nIn recent years, the VideoQA paradigm has mainly followed\na three-step process (Zhong et al. 2022): firstly, extracting\nfeatures using pre-trained models; secondly, performing fea-\nture interaction between videos and questions; and finally,\nclassifying answers in open-ended manner. Typically, pre-\ntrained models in the computer vision field, such as ResNet\n(He et al. 2016) and ResNeXt (Hara, Kataoka, and Satoh\n2018), are used to extract video features, while word em-\nbedding vector like Glove (Pennington, Socher, and Man-\nning 2014) or pre-trained models such as Bert (Devlin et al.\n2018) are used to extract question features in natural lan-\nguage processing. In recent works, various attempts have\nbeen made to improve the most critical interaction pro-\ncess of the model. Due to the time and space dimensions\nin videos, some works have attempted to model them spa-\ntiotemporally(Jin et al. 2021; Jiang et al. 2020; Dang et al.\n2021; Gao et al. 2023), focusing on the relationship between\nvideo clips and regions. To further improve the model’s abil-\nity to learn high-order semantic information, some works\nhave modeled the structure hierarchically(Le et al. 2020;\nLiu et al. 2021; Peng et al. 2022; Xiao et al. 2022; Dang\net al. 2021). In these structures, the model can learn differ-\nent semantic characteristics at different levels, making the\nlearning process more enriched. Similarly, some works have\nextracted multi-scale(Peng et al. 2022; Guo et al. 2021) or\nmulti-granularity(Xiao et al. 2022) features from videos for\ninteraction with questions. Previous studies have concluded\nthat videos contain more diverse and informative instances\nand events than a sentence (Lin et al. 2022). Although these\nsophisticated models have strengthened the representation\nability of video features, excessive redundant video infor-\nmation may cause instability during interaction with ques-\ntions. Therefore, some works(Li et al. 2022b,a; Yu et al.\n2023; Zang et al. 2023) have attempted to locate key video\nclips using causal analysis to filter out irrelevant informa-\ntion and obtained satisfactory results. With the rise of pre-\ntrained models, some works (Xue et al. 2022; Zellers et al.\n2021; Seo, Nagrani, and Schmid 2021) hope to enhance the\nmodel’s semantic alignment and generalization ability by\npre-training on large-scale datasets, followed by fine-tuning\non VideoQA subtasks. However, these methods require a\nconsiderable amount of dataset and training resources, and\nlack interpretability. Additionally, due to the extremely un-\nequal distribution of semantic information between videos\nand question answers, abrupt alignment methods are highly\nunstable and prone to failure.\nOur approach differs from previous work in that we at-\ntempt to address the lack of information on the textual side\nto achieve balance in aligning features of the two modalities\nas much as possible. We introduce all video-related ques-\ntions as context, initially aligning them with semantic and\ncontent of the video. We then progressively interact the cur-\nrent question and refined video to obtain the final answer.\nDespite using simple modules compared to previous work,\nour method still achieves decent performance because of sta-\nble and smooth semantic and content alignment between the\nvideo and text.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19796\nResNet\nguitar\nredrun\nsky\nfly\ntomato\npotato\ncat\nelephant\n...\nBERT\nContext Align\nVideo\nBERT\nCLF\nWhat is someone\nrinsing out can with \nwater?\nContext:\nVideo:\nQuestion:\nAnswer Set:\ncls token  pool token\nEncoder\nDecoder\nClassifierEnc\nEnc\nEnc\nq\nk, v\nq\nk, v\nContent Alignment \nAnswer Decoder\nDec\nDec\nContext-aligned Video\nFeature Extractor Semantic  Alignment\n•\n•\n•\nWhere does someone \nswish water around \nto remove the sauce?\nWho poured water \ninto the sauce can?\n......\nResNeXt\nCLF\nDec\nEnc\nFigure 2: The V-CAT model. All the questions related to videos are utilized as the context. Firstly, the pre-trained model is used\nto extract the text and video features. Then, a trainable encoder and contrastive learning is used to semantically constrain global\nfeatures of both the video and context, achieving semantic alignment for both modalities by the encoder. Afterward, aligning\nthe content of the video with the context, we preliminarily extract the vital information that requires attention. Finally, the video\nand the question interact at a fine-grained level by a decoder, after that the global token is utilized to classify answers.\nMethod\nAs shown in Figure2, V-CAT consists of four main mod-\nules: a feature extractor module, which extracts video and\nquestion features separately; a semantic alignment module,\nwhich encodes the videos and questions and applies con-\ntrastive learning to constrain video and question features,\nthereby enhancing the semantic alignment of the encoder;\na content alignment module, which enables the capture of\nkey video features in the given context and ensures the sta-\nbility of subsequent training; and a decoding module, which\ngenerates more precise answers using question features.\nFeature Extractor\nPre-trained model In this module, the conventional ap-\nproach involves using pre-trained models to extract video\nand text features separately. First, we sample an equal num-\nber of frames from the video, following the previous work\nand adhering to a simple and efficient principle. We choose\nthe traditional ResNet(He et al. 2016) and ResNeXt(Hara,\nKataoka, and Satoh 2018) models to extract appearance and\nmotion features from each frame of the video, denoted as\nva ∈ Rf×dv and vm ∈ Rf×dv respectively. The variables f\nand d represent the number of frames sampled per video and\nthe dimensionality of the embedded features, respectively.\nNext, we concatenate the two types of features at the frame\nlevel to obtain the video feature v ∈ R2f×dv . For extracting\ntext features, we utilize a pre-trained BERT model(Devlin\net al. 2018). We perform pooling on the extracted context\nfeatures in order to capture the global contextual informa-\ntion. This feature is represented as c ∈ R1×dq . And the\nquestion feature is denoted as q ∈ Rl×dq . Here, l represents\nthe maximum length of a single sentence, and dq represents\nthe feature dimension. It is worth mentioning that these three\nfeature extractors are not involved in the subsequent training\nprocess.\nEncoder The pre-trained models are trained in their re-\nspective modal domains, resulting in significant semantic\ndiscrepancies in the learned feature representation spaces.\nTo ensure that the encoded semantic features of both modal-\nities are more consistent, we introduce a learnable seman-\ntic alignment encoder after the pre-trained models. Through\nthis encoder, we effectively project the video and question\nfeatures to a common semantic space, achieving alignment\nbetween the visual and textual modalities.\nWe design the encoder based on the encoder of the Trans-\nformer model (Vaswani et al. 2017). For visual features, we\nfirst use a linear layer to project the features to the model\ndimension. Additionally, before inputting them into the en-\ncoder, we introduce a learnable token vg to capture global\ninformation for semantic alignment. Then, we incorporate\npositional encoding to capture the relative temporal posi-\ntion information of each frame. After normalization, we ob-\ntain visually features that are more uniformly standardized.\nThese operations can be represented by\nv = LN([vW, vg] + pos) (1)\nwhere pos represents the positional encoding, LN denotes\nthe normalization operation, and W ∈ Rdv×d is trainable\nparameter, d denotes the hidden size of model.\nNext, through self-attention and feed-forward networks,\nwe obtain visual featuresv with contextual information. The\nformulas for the attention mechanism and feed-forward net-\nwork are as\nvi+1 = F F N(MHA (vi, vi, vi)) (2)\nwhere F F Nis feed-forward network and MHA represents\nmulti-head attention. F F Nformulation are denoted as\nF F N(x) = LN(ReLU(xW1)W2 + x) (3)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19797\n•\n•\n•\nBERT Enc\nResNet\nResNeXt\nEnc\nContexts in the batch:\nVideos in the batch:\nC1 C2 C3 ... Cn\nV1\nV2\nV3\n...\nVn\n...\n...\nWhere does someone \nswish water around \nto remove the sauce?\nWho poured water \ninto the sauce can?\n......\nFigure 3: An illustration of contrastive alignment. A higher\nsimilarity is expected in the alignment tokens of the same\nsample’s video and context within a batch, while a lower\nsimilarity is preferred for different samples.\nwhere Wi ∈ Rd×d is trainable parameter, ReLU represents\nthe activation function. Also, MHA are denoted as\nMHA (Q, K, V) = Softmax ((QW1)(KW2)T\n√\nt )(V W3)\n(4)\nwhere Wi ∈ Rd×d is trainable parameter, Softmax is a\nactivation function,t denotes as the temperature of attention.\nIn this process, Q, K, and V are all video features v.\nFor context and question features, we similarly use lin-\near layers to transform the features to the model dimension.\nThen, we introduce learnable tokens qg and cg to capture\nglobal information for both context and question. And we\nobtain the textural features that are more uniformly stan-\ndardized, which can be denoted as\nq = LN([qWq, qg] + pos), (5)\nc = LN([cWc, cg] + pos) (6)\nwhere Wq and Wc are learnable parameters. Afterward, sim-\nilar to visual features, they are input into the encoder for pro-\ncessing. The formulas for this process can be represented as\nqi+1 = F F N(MHA (qi, qi, qi)), (7)\nci+1 = F F N(MHA (ci, ci, ci)). (8)\nSemantic Alignment Module\nIn the encoder module, we have connected an encoder af-\nter the pre-trained models to encode the visual and tex-\ntual modality features separately, ensuring that their feature\nrepresentations have consistent semantics. However, since\nthe information contained in a single question text is much\nless than that in a video, the model may not always learn\nsemantic-aligned features as desired. To better align the two\nmodalities and draw inspiration from the work of multi-\nmodal alignment (Radford et al. 2021a; Tsimpoukelli et al.\n2021; Hou et al. 2022; Li et al. 2023), we utilize context\nwith richer textual information for semantic aligning with\nthe video, and introduce contrastive learning to supervise\nthe learning process of the encoder. This encourages seman-\ntically similar multimodal features to be closer in the feature\nspace, while pushing semantically dissimilar features further\napart. Specifically, we adopt a contrastive learning approach\nas shown in the Figure 3, where the global alignment token\nof the video and the global alignment token of the context\nare normalized and then used to compute a large similarity\nmatrix. The formula for this process can be represented as\nvg = v[0], (9)\ncg = c[0], (10)\nsimij = vi\ng · (cj\ng)T (11)\nwhere symbol [i] represents the i-th token in the sequence, i\nand j denotes the number of v and c in a batch.\nIt is evident that the video and the context from the same\nsample are more related, while different samples within a\nbatch are unrelated. Therefore, we naturally desire the di-\nagonal of this similarity matrix to be 1, while the other po-\nsitions are 0. To achieve this, we adopt a loss calculation\nmethod inspired by (Radford et al. 2021a) and construct\nsample labels as\nz′ = I(bsz) (12)\nwhere I(x) denotes a Identity matrix with the size of x,\nbsz denotes the size of batch. Then, we use cross-entropy\nto achieve the goal of semantic similarity differentiation, al-\nlowing the features of the two modalities to be embedded\ninto a common feature space, which represents as\nLcl = −Σbsz\ni=1(z′\ni)T Simi. (13)\nUnlike simple embedding features in the past, we fur-\nther extract video and context global features as interface,\nto achieve video-context semantic alignment by utilizing the\ncontrastive learning. Consistent features in a shared feature\nspace facilitate subsequent interactions between two modal-\nities.\nContent Alignment Module\nAfter obtaining semantically aligned visual and textual in-\nformation, previous work (Lin et al. 2022) has shown that\nvideos often contain richer instance and event information\ncompared to individual sentences. However, for video ques-\ntion answering tasks, redundant information can often lead\nto interference when answering questions, resulting in in-\nstability during modality interaction. For answering the cur-\nrent question, we only need video information relevant to the\nquestion. Refined visual features can enable faster and more\naccurate modality interaction.\nHere, to better align with human thinking, we first roughly\nexamine the context, which allows us to identify the video\ninformation of interest. Therefore, we use the query tokencq\nof context as the key and value in the cross-attention mod-\nule, while the video features serve as the query. This enables\nus to use the contextual semantic information to search for\nand aggregate video features while reducing the attention\non redundant and irrelevant video features. It also avoid the\ninstability of alignment between video and single sentence.\nThis way, we can obtain more refined and effective visual\nfeatures. Specifically, we interact the localized video fea-\ntures with the contextual content alignment features through\na decoder. The decoder consists of cross-attention, and feed-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19798\nforward networks. The formulas for this process can be rep-\nresented as\ncq = c[1], (14)\nvl = v[1 :], (15)\nvi+1\nl = F F N(MHA (vi\nl , cq, cq)). (16)\nAnswer Decoder\nAfter obtaining the refined video information that can be\nused to answer the question, we can proceed to answer the\nquestion using this content. Inspired by previous work (Sun\net al. 2022), video question answering is similar to reading\ncomprehension. After a rough examination of the context\nto identify key content, we re-examine the question to pro-\nvide an answer. By using question features, we can facilitate\nmore comprehensive modality interaction and obtain more\naccurate answers.\nTherefore, here we use a decoder module consisting of\ncross-attention, and feed-forward networks to interact be-\ntween the visual and textual modalities and decode the an-\nswer. We use word-level question features, including learn-\nable global features, as the query, and the video features as\nthe key and value, which are input into the decoder. The for-\nmula for this process can be represented as follow:\nqi+1 = F F N(MHA (qi, vl, vl)). (17)\nNext, we use the global token of the question, which en-\ncapsulates rich multimodal interactions, as input to the sub-\nsequent answer generation module. It can be represented as\nfollows:\nqg = q[0], (18)\np = ELU (qgW1)W2 (19)\nwhere W1 ∈ Rd×d and W2 ∈ Rd×m denotes the trainable\nparameters, m denotes the size of answer set, ELU repre-\nsents the activation function. The obtained p can be used for\npredicting various answers. In the training process, cross-\nentropy loss is used:\nLce = −Σm\ni=1ziln(pi) (20)\nwhere zi = 1 if the answer index corresponds to the ith sam-\nple’s ground-truth answer and 0 otherwise. Finally, our con-\nstruct overall loss:\nL = Lce + αLcl (21)\nwhere α denotes the weight of loss.\nExperiment\nDatasets\nWe experiment on the traditional and widely used datasets\nin the video question answering domain, MSVD-QA(Xu\net al. 2017) and MSRVTT-QA(Xu et al. 2016). They are\nboth open VideoQA datasets constructed using videos and\ndescriptions. The videos mainly consist of short videos, and\nMSVD-QA contains 1,970 short videos with 50,505 open-\nended Q&A pairs. MSRVTT-QA includes 10,000 videos\nwith 243,000 Q&A pairs. These two datasets are both open-\nended, which are more challenging compared to the current\nmultiple-choice question answering datasets. They require\nmodels to have a strong ability to understand both visual and\ntextual modalities and generate answers from a large cond-\nstructed answer set accordingly.\nImplementation Details\nIn our experiments, each video is uniformly sampled into\nsegments of 16 frames. For videos with insufficient frames,\nwe pad using either the initial or terminal frame. In order\nto balance the information of video and text to better align-\nment, we introduce the context, which possess richer infor-\nmation than one sentence. Video context can be constructed\nby many method such as video description and video com-\nment. In our work, we obtain context by concatenating ex-\ntracted features from all questions related to current video\nand utilizing pooling operation. It is worth mentioning that,\nthere are no same video in both train set and test set, which\navoid the leakage of test set problems. The extracted visual\nand textual features possess dimensions of 2048 and 768, re-\nspectively, while the model’s dimension stands at 1024. The\nmodel’s context encoder, video encoder, question encoder,\ncontent decoder, and answer decoder are all composed of\nstackable transformer layers, facilitating adaptability to di-\nverse datasets. When employing the MSVD-QA dataset, the\nnumbers of layer for each module are set at 8, 1, 1, 7 and 4,\nrespectively, which are searched from 1 to 8. For MSRVTT-\nQA, the numbers are 1, 1, 2, 2, and 1. Concerning the loss\nweight α, they are designated at 1e-5 for MSVD-QA and\n1e-6 for MSRVTT-QA, which are searched from 1e-6 to\n1 increasing by multiples of 10 each time. Throughout the\ntraining process, the model underwent 30 epochs of iterative\ntraining with a batch size of 128 and a learning rate of 1e-4.\nComparison with State-of-the-arts\nTable 1 presents the evaluation results of our approach and\nthe performance of state-of-the-art models in the VideoQA\ndomain, where accuracy is utilized as metric to evaluate the\nperformance of the models. To ensure a comprehensive com-\nparison, we also showcase the visual and textual feature ex-\ntractors employed by each method, as well as whether addi-\ntional datasets were used for pretraining.\nOur approach V-CAT leverages conventional computer\nvision techniques, namely ResNet(He et al. 2016) and\nResNeXt(Hara, Kataoka, and Satoh 2018), to extract video\nfeatures. And we employ the classical BERT(Devlin et al.\n2018) model to extract textual features. Similarly, HCRN(Le\net al. 2020), B2A(Park, Lee, and Sohn 2021), HAIR(Liu\net al. 2021), MHN(Peng et al. 2022), HQGA(Xiao et al.\n2022) and EIGV(Li et al. 2022a) also utilize traditional\nimage-based models such as Faster R-CNN(Anderson et al.\n2018). However, their performance falls significantly short\nof our method. Our accuracy surpasses that of the highest-\nperforming EIGV model by 2.6 on MSVD-QA and 4 on\nMSRVTT-QA, respectively.\nFurthermore, even when compared to CLIP-QA(Radford\net al. 2021b), which incorporates the powerful multi-\nmodal CLIP (Radford et al. 2021a) model, and PMT(Peng\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19799\nMethod Video Ex. Text Ex. PT MSVD↑ MSRVTT↑\nHCRN(Le et al. 2020) ResNet, ResNeXt Glove - 36.1 35.6\nB2A(Park, Lee, and Sohn 2021) ResNet, ResNeXt Glove - 37.2 36.9\nHAIR(Liu et al. 2021) ResNet, Faster R-CNN Glove - 37.5 36.9\nCLIP-QA(Radford et al. 2021b) CLIP Bert - 38.5 39\nMHN(Peng et al. 2022) ResNet, ResNeXt Glove - 40.4 38.6\nHQGA(Xiao et al. 2022) ResNet, ResNeXt, Faster R-CNN Bert - 41.2 38.6\nEIGV(Li et al. 2022a) ResNet, ResNeXt Bert - 42.6 39.3\nCoVGT(Xiao et al. 2023) ResNet, Faster R-CNN RoBERTa - - 40.0\nPMT (Peng et al. 2023) X3D-M Glove - 41.8 40.3\nHD-VILA(Xue et al. 2022) ResNet, TimeSformer Bert 100M 41.8 40.3\nMERLOT(Zellers et al. 2021) ViT RoBERTa 180M - 43.1\nCoMVT (Seo, Nagrani, and Schmid 2021) S3D Bert 100M 42.6 39.5\nV-CAT(ours) ResNet, ResNeXt Bert - 45.2 43.3\nTable 1: Comparison with state-of-the-art methods on VideoQA datasets. Video Ex. and Text Ex. denote the video and text\nfeature extractor, respectively. PT denotes the a mount of dataset used for pre-training.\nMethod MSVD↑ MSRVTT↑\nV-CAT w/o context 35.7 34.2\nV-CAT w/ question 31.3 31.5\nV-CAT w/o SA 45.1 43.0\nV-CAT w/o CA 37.2 36.6\nV-CAT 45.2 43.3\nTable 2: Ablation study of the alignment module.\net al. 2023), which is based on a video-based model\nX3D(Feichtenhofer 2020), and CoVGT(Xiao et al. 2023),\nwhich utilizes the robust RoBERTa(Liu et al. 2019) model\nfor textual feature extracting, our approach still exhibits a\nnotable advantage. On the MSVD-QA and MSRVTT-QA\ndatasets, our accuracy exceeds that of the leading PMT\nmodel by 3.4 and 3, respectively. Evidently, although our\npretrained model is straightforward, the feature processing\npipeline remains stable, and the extracted features are effec-\ntively utilized.\nMeanwhile, HD-VILA(Xue et al. 2022), MER-\nLOT(Zellers et al. 2021) and CoMVT(Seo, Nagrani,\nand Schmid 2021) aim to enhance the model’s modality\nalignment and generalization capabilities through large-\nscale dataset pretraining, with the goal of improving\naccuracy. Despite not employing any data pretraining, our\nmodel achieves accuracy surpassing that of the highest-\nperforming CoMVT model by 2.6 on MSVD-QA and\nMERLOT model by 0.2 on MSRVTT-QA. This demon-\nstrates that our balanced approach to video and context\nensures a more stable and efficient alignment process.\nAblation Analysis\nMethod MSVD↑ MSRVTT↑\nV-CAT w/o CL 45.1 43.0\nV-CAT w/ L1 43.6 42.8\nV-CAT w/ L2 44.1 42.2\nV-CAT w/ KL 42.2 41.5\nV-CAT 45.2 43.3\nTable 3: Variants of our model specifically in loss function.\nMSVD-QA MSRVTT-QA\n2468 2468\n20\n25\n30\n35\n40\n45\nLayer Num\nAccuracy\nModule\nVideo Encoder\nContext Encoder\nQuestion Encoder\nContent Alignment\nAnswer Decoder\nFigure 4: The figure showcases the impact of stacking layers\nin different modules of the model on its performance on the\nMSVD-QA and MSRVTT-QA datasets. The horizontal axis\nrepresents the number of layers in the encoder or decoder,\nwhile the vertical axis represents the model’s accuracy. Each\npoint of a specific color corresponds to a respective module.\nAlignment We conducted an ablation analysis on the strat-\negy of using context for alignment, which result is shown in\nTable 2. The ’w/o context’ indicates a model that does not\nuse any context, while ’w/ question’ represents the model\nusing the current question as context. It was observed that\nthe video features without contextual alignment struggle to\nalign effectively with the semantic and content of the text,\nresulting in a significant impact on the model. Meanwhile,\nusing only the question as context yields unsatisfactory re-\nsults. The primary reason is that the amount of informa-\ntion contained in a single question is much less than that\nin the video (Lin et al. 2022), leading to an extremely imbal-\nanced of two semantic information during alignment, which\ndestabilizes the model. Additionally, aligning with a single\nsentence with minimal information reduces the richness of\nvideo information, resulting in a loss of smoothness and po-\ntentially causing the video features to lose crucial informa-\ntion for reasoning. Furthermore, to further analyze the roles\nof semantic and content alignment in the model, we eval-\nuated ’w/o SA’ and ’w/o CA’ separately. It was found that\nboth cases led to a decrease in model accuracy, with a greater\nloss observed when content alignment was removed.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19800\nVideo:\nAttention:\nContext:\nContext: Video:\nAttention:\n• what does a man \ncollect from a \ngroup of people?\n• what is a man \nselling goods on?\n• ......\n• what did the man \njump the hill into \nthe lake with?\n• what crashes into \na lake?\n• ......\nFigure 5: Two case of the attention of different video contents under the context. The context is constructed by collecting video-\nrelated questions. It is evident that, within the context alignment, the model is more adept at capturing the primary events in the\nvideo, while disregarding some peripheral and unrelated scenes.\nContrastive Learning Table 3 shows the analysis impact\nof contrastive learning loss. Firstly, we present the model’s\nperformance removing the loss constraint. The accuracy of\nthe model decreases, which shows the evidence that the\ncontrastive loss imposes constraints on video and contex-\ntual features, enabling the model to align video and text se-\nmantically more effectively, thereby enhancing the model’s\nperformance. Also, we replaced the contrastive loss func-\ntion to obtain multiple variants of the model and analyze\nthe effectiveness. It was observed that regardless of whether\nL1 loss, L2 loss or KL(Kullback-Leibler) divergence loss\nwas used as the loss function, they all led to a decrease in\nmodel performance, even lower than without incorporating\nthe loss constraint. Although these three types of loss func-\ntions aim to encourage similarity between the feature dis-\ntributions of two global tokens, the contrastive loss used in\naddition to this is able to further push apart semantically dis-\nsimilar pairs. This makes the model more stable and reliable\nin semantic alignment.\nEncoders and Decoders For each module, we employed\ntransformer-based encoders and decoders. Such stackable\nmodules enhance the model’s scalability and can adapt to\ndifferent datasets by altering the number of layers. To ex-\nplore the impact of layer numbers on the model’s perfor-\nmance on the MSVD-QA and MSRVTT-QA, we present\nthe results for different parameters, as shown in the Fig-\nure 4. Overall, both datasets achieve satisfactory results with\nfewer layers, and an excessive number of stacked layers ac-\ntually leads to a decline in model performance. Regarding\nthe MSVD-QA dataset, apart from the layers in the Video\nEncoder, an appropriate increase in other layers contributes\nto model improvement. However, for the MSRVTT dataset,\nincreasing the number of layers generally results in a de-\ncrease in model accuracy, particularly for the Question En-\ncoder and Answer Decoder, where the accuracy drops below\n30 when the number of layers reaches 8. It is evident that by\nadjusting the layer numbers of each module, we can adapt\nthe model to different datasets.\nContent Attention Visualization\nWe conducted a visual analysis of the attention mechanism\nduring the content alignment process to explore the selec-\ntion and filtering of actual video contents, which is shown in\nFigure 5. In the first example, the event in the video where\nthe man sells a product to earn money is assigned greater\nattention, while certain shots of the crowd are given lower\nweight. In contrast, in the second example, the entire video\nemphasizes the scene where the man leaps on a motorcy-\ncle, soaring through the air before plunging into the water,\ngarnering higher attention, while some non-essential shots\nreceive lower attention. It is evident that, within the con-\ntext alignment, the model is more adept at capturing the\nprimary events in the video, while disregarding some pe-\nripheral and unrelated scenes. These primary events are pre-\ncisely the subjects that tend to be inquired about. Further-\nmore, the model effectively assigns higher weight to these\ncrucial events, which aids in subsequent answer decoding,\nwhile appropriately disregarding environmental scenarios.\nConclusion\nWe proposes V-CAT, which strengthens the information on\nthe textual side by introducing context to align with the\nvideo. Specifically, the feature extractor and encoder em-\nbed visual and textual information into a shared feature\nspace, and innovatively introduce contrastive learning to\nalign global visual and contextual semantics, ensuring the\nconsistency of multimodal features. Subsequently, more rel-\nevant video information is extracted through contextual in-\nformation refinement to stabilize the subsequent answering\nprocess. Finally, the interaction between the current ques-\ntion features and video features yields the final answer. Our\nmodel structure is simple yet remarkably effective. Evalua-\ntion results on the MSVD-QA and MSRVTT-QA datasets\ndemonstrate that our approach outperforms existing mod-\nels. In future, we will consider replacing the simplistic en-\ncoder module during answer generation and introduce more\nrefined interaction methods. Additionally, alternative ap-\nproaches can be explored for modeling the context of videos.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19801\nAcknowledgments\nThis work was supported in part by the National Natu-\nral Science Foundation of China under Grant 62006034;\nin part by the Ministry of Education Humanities and So-\ncial Science Project under Grant 22YJC740110; in part by\nthe Dalian High-level Talent Innovation Support Plan un-\nder Grant 2021RQ056; and in part by the Fundamental\nResearch Funds for the Central Universities under Grant\nDUT23YG136.\nReferences\nAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;\nGould, S.; and Zhang, L. 2018. Bottom-up and top-down at-\ntention for image captioning and visual question answering.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 6077–6086.\nDang, L. H.; Le, T. M.; Le, V .; and Tran, T. 2021. Hier-\narchical object-oriented spatio-temporal reasoning for video\nquestion answering. arXiv preprint arXiv:2106.13432.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nFeichtenhofer, C. 2020. X3d: Expanding architectures for\nefficient video recognition. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition ,\n203–213.\nGao, D.; Zhou, L.; Ji, L.; Zhu, L.; Yang, Y .; and Shou, M. Z.\n2023. MIST: Multi-modal Iterative Spatial-Temporal Trans-\nformer for Long-form Video Question Answering. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 14773–14783.\nGuo, Z.; Zhao, J.; Jiao, L.; Liu, X.; and Li, L. 2021. Multi-\nscale progressive attention network for video question an-\nswering. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language Process-\ning (Volume 2: Short Papers), 973–978.\nHara, K.; Kataoka, H.; and Satoh, Y . 2018. Can spatiotem-\nporal 3d cnns retrace the history of 2d cnns and imagenet?\nIn Proceedings of the IEEE conference on Computer Vision\nand Pattern Recognition, 6546–6555.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHou, Z.; Zhong, W.; Ji, L.; Gao, D.; Yan, K.; Chan, W.-K.;\nNgo, C.-W.; Shou, Z.; and Duan, N. 2022. Cone: An efficient\ncoarse-to-fine alignment framework for long video temporal\ngrounding. arXiv preprint arXiv:2209.10918.\nJiang, J.; Chen, Z.; Lin, H.; Zhao, X.; and Gao, Y . 2020.\nDivide and conquer: Question-guided spatio-temporal con-\ntextual attention for video question answering. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol-\nume 34, 11101–11108.\nJin, W.; Zhao, Z.; Cao, X.; Zhu, J.; He, X.; and Zhuang,\nY . 2021. Adaptive spatio-temporal graph enhanced vision-\nlanguage representation for video qa. IEEE Transactions on\nImage Processing, 30: 5477–5489.\nLe, T. M.; Le, V .; Venkatesh, S.; and Tran, T. 2020. Hi-\nerarchical conditional relation networks for video question\nanswering. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 9972–9981.\nLi, M.; Shi, X.; Leng, H.; Zhou, W.; Zheng, H.-T.; and\nZhang, K. 2023. Learning Semantic Alignment with Global\nModality Reconstruction for Video-Language Pre-training\ntowards Retrieval. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 37, 1377–1385.\nLi, Y .; Wang, X.; Xiao, J.; and Chua, T.-S. 2022a. Equivari-\nant and invariant grounding for video question answering. In\nProceedings of the 30th ACM International Conference on\nMultimedia, 4714–4722.\nLi, Y .; Wang, X.; Xiao, J.; Ji, W.; and Chua, T.-S. 2022b.\nInvariant grounding for video question answering. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2928–2937.\nLin, C.; Wu, A.; Liang, J.; Zhang, J.; Ge, W.; Zheng, W.-S.;\nand Shen, C. 2022. Text-adaptive multiple visual prototype\nmatching for video-text retrieval. Advances in Neural Infor-\nmation Processing Systems, 35: 38655–38666.\nLiu, F.; Liu, J.; Wang, W.; and Lu, H. 2021. Hair: Hierarchi-\ncal visual-semantic relational reasoning for video question\nanswering. In Proceedings of the IEEE/CVF international\nconference on computer vision, 1698–1707.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nPark, J.; Lee, J.; and Sohn, K. 2021. Bridge to answer:\nStructure-aware graph interaction network for video ques-\ntion answering. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 15526–15535.\nPeng, M.; Wang, C.; Gao, Y .; Shi, Y .; and Zhou, X.-\nD. 2022. Multilevel hierarchical network with multiscale\nsampling for video question answering. arXiv preprint\narXiv:2205.04061.\nPeng, M.; Wang, C.; Shi, Y .; and Zhou, X.-D. 2023. Efficient\nEnd-to-End Video Question Answering with Pyramidal\nMultimodal Transformer. arXiv preprint arXiv:2302.02136.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nthe 2014 conference on empirical methods in natural lan-\nguage processing (EMNLP), 1532–1543.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021a. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748–8763. PMLR.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19802\net al. 2021b. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748–8763. PMLR.\nSeo, P. H.; Nagrani, A.; and Schmid, C. 2021. Look before\nyou speak: Visually contextualized utterances. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 16877–16887.\nSun, X.; Wang, X.; Gao, J.; Liu, Q.; and Zhou, X. 2022. You\nneed to read again: Multi-granularity perception network for\nmoment retrieval in videos. In Proceedings of the 45th In-\nternational ACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval, 1022–1032.\nTsimpoukelli, M.; Menick, J. L.; Cabi, S.; Eslami, S.;\nVinyals, O.; and Hill, F. 2021. Multimodal few-shot learning\nwith frozen language models. Advances in Neural Informa-\ntion Processing Systems, 34: 200–212.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nXiao, J.; Yao, A.; Liu, Z.; Li, Y .; Ji, W.; and Chua, T.-\nS. 2022. Video as conditional graph hierarchy for multi-\ngranular question answering. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 36, 2804–\n2812.\nXiao, J.; Zhou, P.; Yao, A.; Li, Y .; Hong, R.; Yan, S.;\nand Chua, T.-S. 2023. Contrastive Video Question An-\nswering via Video Graph Transformer. arXiv preprint\narXiv:2302.13668.\nXu, D.; Zhao, Z.; Xiao, J.; Wu, F.; Zhang, H.; He, X.; and\nZhuang, Y . 2017. Video question answering via gradually\nrefined attention over appearance and motion. In Proceed-\nings of the 25th ACM international conference on Multime-\ndia, 1645–1653.\nXu, J.; Mei, T.; Yao, T.; and Rui, Y . 2016. Msr-vtt: A large\nvideo description dataset for bridging video and language.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 5288–5296.\nXue, H.; Hang, T.; Zeng, Y .; Sun, Y .; Liu, B.; Yang, H.; Fu,\nJ.; and Guo, B. 2022. Advancing high-resolution video-\nlanguage representation with large-scale video transcrip-\ntions. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 5036–5045.\nYu, S.; Cho, J.; Yadav, P.; and Bansal, M. 2023. Self-\nChained Image-Language Model for Video Localization and\nQuestion Answering. arXiv preprint arXiv:2305.06988.\nZang, C.; Wang, H.; Pei, M.; and Liang, W. 2023. Dis-\ncovering the Real Association: Multimodal Causal Rea-\nsoning in Video Question Answering. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 19027–19036.\nZellers, R.; Lu, X.; Hessel, J.; Yu, Y .; Park, J. S.; Cao, J.;\nFarhadi, A.; and Choi, Y . 2021. Merlot: Multimodal neural\nscript knowledge models. Advances in Neural Information\nProcessing Systems, 34: 23634–23651.\nZhong, Y .; Xiao, J.; Ji, W.; Li, Y .; Deng, W.; and Chua, T.-S.\n2022. Video question answering: Datasets, algorithms and\nchallenges. arXiv preprint arXiv:2203.01225.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19803"
}