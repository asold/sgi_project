{
  "title": "Deep Transformer Network for Hyperspectral Image Classification",
  "url": "https://openalex.org/W4295798562",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5074045214",
      "name": "Kuiliang Gao",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A5039991862",
      "name": "Bing Liu",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A5089040016",
      "name": "Zhixiang Xue",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A5027117106",
      "name": "Xibing Zuo",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A5056740310",
      "name": "Yifan Sun",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A5009943113",
      "name": "Mofan Dai",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962770389",
    "https://openalex.org/W2603834682",
    "https://openalex.org/W2793272303",
    "https://openalex.org/W2991616716",
    "https://openalex.org/W3103753223",
    "https://openalex.org/W2996433564",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2548791488",
    "https://openalex.org/W2577238056",
    "https://openalex.org/W2768537477",
    "https://openalex.org/W3122028341",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3098388691",
    "https://openalex.org/W4226363311",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Different from the existing CNN-based models, a novel method based on the transformer model is proposed in this paper, to further improve the classification accuracy of hyperspectral image (HSI). Specifically, a deep network model is constructed with the Transformer-iN-Transformer (TNT) modules, to carry out end-to-end classification. The outer and inner transformer models in the TNT module can extract the patch-level and pixel-level features respectively, to make full use of the global and local information in the input cubes. Experimental results show that the proposed method can achieve better classification performance than the existing CNN-based models. In addition, using the transformer-baesd deep model without convolution to classify HSI provides a new idea for the related researches.",
  "full_text": "Academic Journal of Computing & Information Science \nISSN 2616-5775 Vol. 4, Issue 7: 11-17, DOI: 10.25236/AJCIS.2021.040703 \nPublished by Francis Academic Press, UK \n-11- \nDeep Transformer Network for Hyperspectral Image \nClassification \nKuiliang Gaoa,*, Bing Liub, Zhixiang Xuec, Xibing Zuod, Yifan Sune, Mofan Daif \nPLA Strategic Support Force Information Engineering University, Zhengzhou, China \nagokling1219@163.com, bliubing220524@126.com, c xuegeeker@163.com, d2812463301@qq.com, \nesincere_sunyf@163.com, fdaidai_1115@163.com \nAbstract: Different from the existing CNN -based models, a novel method based on the transformer \nmodel is proposed in this paper, to further improve the clas sification accuracy of hyperspectral image \n(HSI). Specifically, a deep network model is constructed with the Transformer -iN-Transformer (TNT) \nmodules, to carry out end -to-end classification. The outer and inner transformer models in the TNT \nmodule can extract the patch-level and pixel-level features respectively, to make full use of the global \nand local information in the input cubes. Experimental results show that the proposed method can \nachieve better classification performance than the existing CNN -based models. In addition, using the \ntransformer-baesd deep model without convolution to classify HSI provides a new idea for the related \nresearches. \nKeywords: Hyperspectral image classification, deep learning, transformer, TNT, self-attention \n1. Introduction \nIn the field of remote sensing, hyperspectral image (HSI) classification has always been one of the \nmost attractive research hotspots. In the whole process of HSI processing and analysis, HSI \nclassification is one of the most important links, and its accurate classification results can provide \nstrong data support for the follow -up tasks, which has been widely used in fine agriculture, land -use \nplanning and many other fields[1]. \nIn the early research of HSI classification, machine learning methods such as suppor t vector \nmachine (SVM), principal component analysis (PCA) and extended morphological profile (EMP) have \nbeen widely used[2]. However, the above methods cannot make full use of the deep abstract features in \nHSI, so they cannot obtain satisfactory classific ation results. In contrast, deep learning methods can \nautomatically learn the deep abstract features conducive to the target tasks layer by layer, and these \nfeatures are often highly informative and robust. Deep learning models such as stacked autoencoder \n(SAE), recurrent neural network (RNN), deep belief networks (DBN) and convolutional neural \nnetworks (CNN) have been widely applied in HSI classification[3], and have achieved better \nclassification performance than traditional classification methods with su fficient training samples. \nCompared with other deep learning models, 2D -CNN and 3D -CNN can make more full use of the \nspatial-spectral information utilizing unique convolution operation to directly process the HSI data with \ngrid structure, so they can obtai n higher classification accuracy[4]. For example, Lee et al designed a \nnovel contextual deep CNN model by introducing multi-scale 2D convolutional filter bank and residual \nconnection[5]. Liu et al. constructed a deep spatial -spectral network utilizing 3D c onvolution, and the \nobtained classification accuracy was obviously better than that of the traditional methods[6]. In recent \nyears, novel network structures such as generative adversarial networks (GAN), capsule network (CN) \nand graph convolutional network  (GCN) have been introduced into HSI classification, to further \nimprove the classification performance[7]. In the above network models, CNN is always an \nindispensable and important module.  \nAs is known to all, CNN remains dominant in HSI classification. Ho wever, CNN indeed possesses \nsome defects: it is not good at modeling the long -distance dependencies and obtaining global context \ninformation[8]. By contrast, the transformer model can better utilize the global context information \nwithin a large range by tr eating the input image as the sequential patches[9]. In view of this, this paper \npresents a novel deep transformer network, to further improve the accuracy of HSI classification. \nSpecifically, the deep network model is constructed with Transformer -iN-Transformer (TNT)[10] \nstructure as the basic module. The inner transformer block and the outer transformer block in the TNT \nAcademic Journal of Computing & Information Science \nISSN 2616-5775 Vol. 4, Issue 7: 11-17, DOI: 10.25236/AJCIS.2021.040703 \nPublished by Francis Academic Press, UK \n-12- \nmodule can respectively model the pixel -level and patch -level features, to make full use of the local \nand global information in the input  cubes. Experimental results on two public HSI data sets show that \nthe proposed method can achieve better classification performance than the existing CNN models. \nMeanwhile, using transformer-based deep model without any convolution operation to classify H SI in \nthis paper provides a new idea for HSI classification. \n2. Proposed Method \n2.1. Workflow \n \nFigure 1: Workflow of the proposed method. \nFigure. 1 shows the workflow of the proposed method. Firstly, the HSI is directly divided into a \nnumber of cubes without dimensionality reduction, to make full use of the spatial -spectral information. \nThen, each cube is sequentially divided into multiple patches referring to [9]. After unfold and linear \ntransformation operations, each patch is converted into a patch embedding and several pixel \nembeddings. Finally, the patch embeddings and pixel embeddings are r espectively added to their \ncorresponding position encodings, and the resulting vectors are input together into the designed deep \nnetwork model containing L TNT modules for classification. In the TNT modules, the outer \ntransformer models can take full advantage of the global information, and the inner transformer models \ncan take full advantage of the local information. The proposed method takes HSI as input and \nclassification results as output, and possesses an end-to-end structure. \n2.2. TNT Module \nThe TNT module  is the core of the proposed method which can make full use of the global and \nlocal information in the input cubes. Before introducing the TNT module, we first review the vision \ntransformer model that is widely used for the computer vision tasks. \n \nFigure 2: Illustration of a vision transformer model. \nFigure 2 describes the network structure of a vision transformer model, which consists of three \nmain parts: multi -head attention mechanism (MHA), multilayer perceptron and layer normalization \n(LN). MHA is the key part in the transformer model for feature learning, and MLP is introduced for \nfeature transformation and nonlinearity. As a data normalization layer, LN can ensure the training \nstability and rapid convergence of the model. In addition, residual connect ions are introduced to take \nfull advantage of the abstract features at different levels. Self -attention mechanism is the basic unit of \nAcademic Journal of Computing & Information Science \nISSN 2616-5775 Vol. 4, Issue 7: 11-17, DOI: 10.25236/AJCIS.2021.040703 \nPublished by Francis Academic Press, UK \n-13- \nMHA. In the self -attention mechanism, the input embeddings are firstly transformed to query matrix, \nkey matrix and value matrix. The output actually is the weighted sum of the value vectors, and the \nweights assigned to each value vector are calculated from the query vectors and the corresponding key \nvectors. \n \nFigure 3: Illustration of a TNT module. \nThe visual transformer mo del views an input image as a sequence of patches, but ignores the \nintrinsic structure information inside each patch.  To make full use of the global and local information \nwithin the input cubes, TNT module is introduced as the core of constructing the deep  network model. \nFor the input cube, we first split it into n patches \n , where \n( , )pp  is the \nsize of each patch.  Then, each patch is further transformed into multiple \n( ', ')pp  pixel embeddings \nwith unfold operation and linear projection. Formally, the sequence of patch tensors is as follows: \n                                   (1) \nwhere each patch tensor \n0\niY  is viewed as a sequence of pixel embeddings, and \n2'mp . \nAs shown i n Fig ure 3, a TNT module contains two transformer models, one of which operates \nacross the patch tensors and the other operates across the pixel embeddings. For the pixel embeddings, \nthe inner transformer is used to explore the relation between pixels and extract the pixel-level features: \n11' , ( ( )),\n' , ( ( ' ))\n\n ，\ni i i\nl l l\ni i i\nl l l\nY Y LN MSA Y\nY Y LN MLP Y\n                                  (2) \nwhere l = 1, 2,..., L index the layers and L is the total number of layers. This process can build the \nrelationship among pixels by computing interactions bet ween two pixel embeddings, to effectively \nutilize the local information within the input cubes. As for the patch level, the patch embedding \nmemories are created to store the sequence of patch -level features:\n  , \nwhere \nclassZ  represents the class token. In each TNT module, the patch tensors are transformed into the \ndomain of patch embeddings by linear projection and added into the patch embeddings: \n1 1 1 1 1 ()       ，i i i\nl l l l lZ Z Vec Y W b\n                            (3) \nwhere Vec represents the flatten operation, W and b are the learnable weights and bias respectively. \nSimilarly, a transformer module (outer transformer) is used for feature learning: \n11' ( ( ))\n' , ( ( ' )).\n\n\n，i i i\nl l l\ni i i\nl l l\nLN MSA\nLN MLP\n                                (4) \nThis process can effectively learn the patch -level features by capturing the intrinsic information \nfrom the sequence of patches. In other words, the outer transformer can make full use of the global \ninformation in the input data. \nAs is mentioned above, the TNT module can process pixel -level and patc h-level data \nsimultaneously. This means that the deep network model built by stacking TNT modules can take full \nadvantage of the global and local information in the cubes and learn richer and more robust features, to \nfurther improve the accuracy of HSI. \nAcademic Journal of Computing & Information Science \nISSN 2616-5775 Vol. 4, Issue 7: 11-17, DOI: 10.25236/AJCIS.2021.040703 \nPublished by Francis Academic Press, UK \n-14- \n3. Experimental Results and Analysis \n3.1. Data Sets and Evaluation Criteria \nTo verify the effectiveness of the proposed method, two public HSI, Salinas (SA) and Houston \n2013(HS) are selected for experiments. The SA data set is collected by AVIRIS sensor. It possesse s \n204 spectral bands and 16 labelled classes. The spatial size is 512 × 217 with 3.7 m/ pixel spatial \nresolution. The HS data set is collected by ITRES -CASI1500 sensor. It possesses 144 spectral bands \nand 15 labelled classes. The spatial size is 349 × 1905 with 2.5 m/ pixel spatial resolution. For each \ndata set, 200 labeled samples per class are randomly selected as training samples, and the remaining \nsamples are used as testing samples to evaluate the classification performance of the model. To \nquantitatively evaluate the classification results, the overall accuracy (OA), average accuracy (AA) and \nkappa coefficient are selected as the evaluation criteria. \n3.2. Hyperparameters Settings \nFirstly, we explore the influence of the depth of network on classification ac curacy. Figure 4 \ndescribes the relationship between the depth of network and the classification accuracy. For the SA and \nHS data sets, the classification accuracy of the model generally increases first and then decreases with \nthe increase in the number of TNT modules. This indicates that, appropriate network structure can \nachieve the best classification performance, while too many or too few network layers may lead to a \ndecline in classification accuracy. \n \nFigure 4: Relationship between the number of the TNT modules and classification accuracy on the SA \nand HS data sets. \n \nFigure 5: Relationship between the number of attention heads (H) and classification accuracy on the \nSA and HS data sets. \nSecondly, the relationship between the number of attention heads a nd the classification accuracy is \nanalysed. Theoretically, similar to the convolution kernels in the CNN models, an appropriate increase \nAcademic Journal of Computing & Information Science \nISSN 2616-5775 Vol. 4, Issue 7: 11-17, DOI: 10.25236/AJCIS.2021.040703 \nPublished by Francis Academic Press, UK \n-15- \nin the number of attention heads should enable the model to learn richer and more robust features. The \nresults are shown in Fig. 5. In  general, with the increase of H, the classification accuracy of the model \nincreases gradually at first and then decreases slowly. When H is equal to 6, the classification accuracy \nreaches the maximum value. \nIn addition, a combination of lar ge iteration times and small learning rate is adopted for training. \nSpecifically, the number of training iterations is set as 500, the learning rate is set as 0.00001, and the \nAdam optimization algorithm is adopted to ensure that the designed model is full y trained. The batch \nsize is set as 64. The input cube is first split into 16 patches in a spatial order, and each patch is further \nsplit into several pixel patches with a width of 2. In the TNT modules, the dimensions of patch \nembedding and pixel embedding are set to 128 and 64 respectively. \n3.3. Comparison and Analysis \nDifferent from the dominant CNN -based model for HSI classification, this paper proposes a novel \ndeep transformer network, to further improve the accuracy of HSI classification. To verify the \neffectiveness of the proposed method, the classification results are compared with the classic machine \nlearning method RBF-SVM and 5 advanced CNN-based models including CNN-PPF[11], CDCNN[5], \nRES-3D-CNN[6], DCCNN[12] and S -CNN[13]. In addition, to reduce the fluctuation of classification \nresults caused by the randomness of sample selection, the average value of 10 experiments is used as \nthe final result, further enhancing the persuasiveness of the experimental results. \nTable 1: The classification results OF different methods on the SALINAS data set. \nClass No. SVM CNN-PPF CDCNN RES-3D-CNN S-CNN Ours \nOA 91.20 92.04 95.54 97.39 96.06 99.32 \nAA 95.46 95.03 97.32 98.99 98.19 99.54 \nkappa 90.20 91.17 95.04 97.09 95.48 99.25 \n1 99.20 100.00 99.50 100.00 100.00 100.00 \n2 99.62 99.76 100.00 100.00 100.00 100.00 \n3 99.70 97.26 98.21 100.00 99.10 100.00 \n4 99.50 97.20 99.43 99.36 99.86 99.43 \n5 96.75 98.61 99.96 99.78 99.93 99.87 \n6 99.42 99.62 99.95 100.00 100.00 99.98 \n7 99.36 99.97 99.71 99.97 99.78 100.00 \n8 84.75 88.15 94.58 91.15 92.80 99.42 \n9 99.10 98.83 99.98 99.94 99.97 99.90 \n10 93.29 85.42 99.78 99.15 97.80 99.13 \n11 97.85 91.21 91.96 99.07 98.40 100.00 \n12 99.84 99.23 99.90 100.00 100.00 98.17 \n13 98.58 98.49 100.00 100.00 100.00 99.63 \n14 95.79 96.56 99.17 99.91 100.00 100 \n15 65.74 73.75 81.13 95.55 83.51 97.16 \n16 98.89 96.49 93.89 100.00 99.83 100.00 \nTables 1-2 lists the classification results of different methods on the SA and HS data sets. As we \ncan see, the classification accuracy of SVM is obviously lo wer than that of the other 5 deep learning -\nbased methods. Deep learning models can extract the deep abstract features that are more informative \nand robust, so they can obtain higher classification accuracy. Among the four CNN -based models, \nRES-3D-CNN using  3D convolution and metric learning -based S -CNN can achieve better \nclassification performance. The proposed method can achieve better classification performance. \nAmong all the listed methods, the proposed method can obtain the best classification results a ccording \nto OA, AA and kappa. On the one hand, MHA enables the model to focus on a wealth of features \nconducive to the classification tasks, on the other hand, the TNT modules containing the inner and \nAcademic Journal of Computing & Information Science \nISSN 2616-5775 Vol. 4, Issue 7: 11-17, DOI: 10.25236/AJCIS.2021.040703 \nPublished by Francis Academic Press, UK \n-16- \nouter transformer can enable the model to make full use of the global and local information in the input \ncubes, to further improve the classification accuracy. \nTable 2: The classification results OF different methods on the HOUSTON 2013 data set. \nClass No. SVM CNN-PPF CDCNN RES-3D-CNN S-CNN Ours \nOA 91.41 94.50 95.34 96.03 98.12 98.68 \nAA 91.95 94.76 95.84 96.56 98.41 98.77 \nkappa 90.71 94.06 94.97 95.71 97.96 98.57 \n1 95.73 96.71 84.49 87.14 99.66 99.76 \n2 97.94 98.71 97.42 97.97 93.67 97.73 \n3 100.00 99.86 99.71 99.85 99.71 99.86 \n4 99.83 99.27 99.32 96.55 98.78 97.13 \n5 96.39 97.85 97.69 99.92 99.84 99.76 \n6 99.69 100.00 94.20 95.03 100.00 98.78 \n7 84.71 94.35 97.36 97.86 97.51 95.77 \n8 95.19 96.67 96.65 95.10 100.00 99.51 \n9 82.19 88.23 92.31 95.58 97.66 97.98 \n10 86.03 92.26 94.02 92.50 95.64 99.11 \n11 86.68 88.70 98.01 99.50 99.19 99.76 \n12 83.59 88.42 94.48 94.05 97.44 98.95 \n13 73.57 83.96 97.61 97.40 99.12 99.58 \n14 97.71 96.61 98.16 100.00 99.77 98.39 \n15 100.00 99.85 96.21 100.00 98.21 99.55 \n \nFigure 6: Classification maps resulting from the different methods on the HS data set. \nFigure 6 shows the classification maps of the different classification methods on the HS data set. It \ncan be seen that the proposed method can produce the classification map closest to the ground truth, \nwhich visually verifies the effectiveness of the proposed method. \n4. Conclusions \nDifferent the dominant CNN -based methods, this paper designs a novel deep transformer network \nby stacking the TNT modules, to further improve the accuracy of HSI classification. The inner and \nouter tran sformer block in the TNT modules can extract the pixel -level and patch -level features \nAcademic Journal of Computing & Information Science \nISSN 2616-5775 Vol. 4, Issue 7: 11-17, DOI: 10.25236/AJCIS.2021.040703 \nPublished by Francis Academic Press, UK \n-17- \nrespectively, making full use of the global and local information in the input HSI cubes. Experimental \nresults on two public HSI data sets show that the proposed method p erforms better than SVM and \nseveral existing CNN-based models. \nAcknowledgments \nWe gratefully acknowledge the financial support by the National Natural Science Foundation of \nChina (Grants Nr. 41801388). \nReferences \n[1] N. Audebert, B. L. Saux, and S. Lefevre , \"Deep Learning for Classification of Hyperspectral Data: \nA Comparative Review,\" IEEE Geoscience and Remote Sensing Magazine, vol. 7, no. 2, pp. 159 -173, \n2019, doi: 10.1109/MGRS.2019.2912563. \n[2] P. Ghamisi, J. Plaza, Y. Chen, J. Li, and A. J. Plaza, \"Adv anced Spectral Classifiers for \nHyperspectral Images: A review,\" IEEE Geoscience and Remote Sensing Magazine, vol. 5, no. 1, pp. 8-\n32, 2017. \n[3] M. J. Khan, H. S. Khan, A. Yousaf, K. Khurshid, and A. Abbas, \"Modern Trends in Hyperspectral \nImage Analysis: A Review,\" IEEE Access, vol. 6, pp. 14118-14129, 2018. \n[4] M. E. Paoletti, J. M. Haut, J. Plaza, and A. Plaza, \"Deep learning classifiers for hyperspectral \nimaging: A review,\" ISPRS Journal of Photogrammetry and Remote Sensing, vol. 158, no. Dec., pp. \n279-317, 2019. \n[5] H. Lee and H. Kwon, \"Going Deeper With Contextual CNN for Hyperspectral Image \nClassification,\" IEEE Trans Image Process, vol. 26, no. 10, pp. 4843-4855, 2017. \n[6] Y. X. LIU Bing, ZHANG Pengqiang, TAN Xiong, \"Deep 3D convolutional network combi ned with \nspatial-spectral features for hyperspectral image classification,\" Acta Geodaetica et Cartographica \nSinica, vol. 48, no. 1, pp. 53-63, 2019-01-20 2019, doi: 10.11947/j.AGCS.2019.20170578. \n[7] S. Shabbir and M. Ahmad, Hyperspectral Image Classifica tion -- Traditional to Deep Models: A \nSurvey for Future Prospects. 2021. \n[8] A. Vaswani et al., \"Attention Is All You Need,\" 06/12 2017. \n[9] A. Dosovitskiy et al., An Image is Worth 16x16 Words: Transformers for Image Recognition at \nScale. 2020. \n[10] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, Transformer in Transformer. 2021. \n[11] W. Li, G. Wu, F. Zhang, and Q. Du, \"Hyperspectral Image Classification Using Deep Pixel -Pair \nFeatures,\" IEEE Transactions on Geoscience and Remote Sensing, vol. 55, no. 2 , pp. 844 -853, 2017, \ndoi: 10.1109/TGRS.2016.2616355. \n[12] H. Zhang, Y. Li, Y. Zhang, and Q. Shen, \"Spectral -spatial classification of hyperspectral imagery \nusing a dual-channel convolutional neural network,\" Remote Sensing Letters, vol. 8, no. 5, pp. 438-447, \n2017/05/04 2017. \n[13] B. Liu, X. Yu, P. Zhang, A. Yu, Q. Fu, and X. Wei, \"Supervised Deep Feature Extraction for \nHyperspectral Image Classification,\" IEEE Transactions on Geoscience and Remote Sensing, vol. 56, \nno. 4, pp. 1909-1921, 2018. ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.762614905834198
    },
    {
      "name": "Computer science",
      "score": 0.6967997550964355
    },
    {
      "name": "Hyperspectral imaging",
      "score": 0.6881588697433472
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6401087045669556
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5121846199035645
    },
    {
      "name": "Pixel",
      "score": 0.4479331076145172
    },
    {
      "name": "Deep learning",
      "score": 0.4331853985786438
    },
    {
      "name": "Engineering",
      "score": 0.1493387520313263
    },
    {
      "name": "Voltage",
      "score": 0.08256605267524719
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I169689159",
      "name": "PLA Information Engineering University",
      "country": "CN"
    }
  ],
  "cited_by": 9
}