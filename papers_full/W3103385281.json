{
  "title": "Using Pre-Trained Transformer for Better Lay Summarization",
  "url": "https://openalex.org/W3103385281",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2103556748",
      "name": "Seungwon Kim",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2574535369",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W2024013984",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W3099396524",
    "https://openalex.org/W2939896848",
    "https://openalex.org/W2112077341",
    "https://openalex.org/W2963385935",
    "https://openalex.org/W2065427498",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2952138241",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W3007504716",
    "https://openalex.org/W2743673813",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2010378115",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2888211956",
    "https://openalex.org/W2251329024",
    "https://openalex.org/W2964580980",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2972111982",
    "https://openalex.org/W1982633109",
    "https://openalex.org/W2571932860",
    "https://openalex.org/W2598956343",
    "https://openalex.org/W3006661753",
    "https://openalex.org/W4394650659",
    "https://openalex.org/W1907286193",
    "https://openalex.org/W2799068938",
    "https://openalex.org/W2963386804",
    "https://openalex.org/W2985808369",
    "https://openalex.org/W2307381258",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2145907631"
  ],
  "abstract": "In this paper, we tack lay summarization tasks, which aim to automatically produce lay summaries for scientific papers, to participate in the first CL-LaySumm 2020 in SDP workshop at EMNLP 2020. We present our approach of using Pre-training with Extracted Gap-sentences for Abstractive Summarization (PEGASUS; Zhang et al., 2019b) to produce the lay summary and combining those with the extractive summarization model using Bidirectional Encoder Representations from Transformers (BERT; Devlin et al., 2018) and readability metrics that measure the readability of the sentence to further improve the quality of the summary. Our model achieves a remarkable performance on ROUGE metrics, demonstrating the produced summary is more readable while it summarizes the main points of the document.",
  "full_text": "Proceedings of the First Workshop on Scholarly Document Processing, pages 328–335\nOnline, November 19, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n328\nUsing Pre-Trained Transformer for Better Lay Summarization\nSeungwon Kim\nIncheon Airport Corporation, Georgia Institute of Technology\nskim3222@gatech.edu\nAbstract\nIn this paper, we tack lay summarization tasks,\nwhich aim to automatically produce lay sum-\nmaries for scientiﬁc papers, to participate in\nthe ﬁrst CL-LaySumm 2020 in SDP work-\nshop at EMNLP 2020. We present our ap-\nproach of using Pre-training with Extracted\nGap-sentences for Abstractive Summarization\n(PEGASUS ; Zhang et al., 2019b) to produce\nthe lay summary and combining those with the\nextractive summarization model using Bidi-\nrectional Encoder Representations from Trans-\nformers (B ERT; Devlin et al., 2019) and read-\nability metrics that measure the readability of\nthe sentence to further improve the quality of\nthe summary. Our model achieves a remark-\nable performance on ROUGE metrics, demon-\nstrating the produced summary is more read-\nable while it summarizes the main points of\nthe document.\n1 Introduction\nRecent summarization techniques have greatly ben-\neﬁtted from the advancement of language models\nand successfully produced plausible summaries for\nboth general news articles in real-life and techni-\ncal scholarly documents in the expert domain. An\ninformative but concise summary can help people\nto reduce the search time and boost the decision\nmaking by expeditiously providing more relevant\ndocuments (Mani et al., 2002; Roussinov and Chen,\n2001; Ma˜na-L´opez et al., 2004; McKeown et al.,\n2005). For processing scholarly documents, au-\ntomatic summarization is promising since it can\nbeneﬁt researchers to cope with the pace of the ex-\nponentially growing number of publications (Born-\nmann and Mutz, 2015).\nDespite the recent advancement in automatic\nsummarization literature, summarization for schol-\narly documents has been less explored compared to\nthe works regarding summarization for ordinary\nnews articles due to the absence of large-scale\ndatasets. Developing human-written lay summaries\nfor scholarly documents is challenging since it in-\nvolves expert knowledge to understand the techni-\ncal jargon and the complex structure of scientiﬁc\ndocuments. Because of these inherent challenges,\nexisting summarization techniques for scientiﬁc\ndocuments is limited in a sense, which the pro-\nduced summary is either too concise to provide im-\nportant information (Vasilyev et al., 2019; Cachola\net al., 2020) or aiming to directly extract the con-\ntent from abstract or citation sentences (Yasunaga\net al., 2019), which mostly resembles the abstract,\nmaking it hard for the public and researchers from\noutside of the particular domain to understand the\nmain points of the scientiﬁc papers. Although the\nreadability of the abstracts in scientiﬁc papers had\ncontinuously decreased due to the increase in the\nuse of technical jargon (Plav´en-Sigray et al., 2017),\nthe summarization of scientiﬁc papers for the pub-\nlic and researchers from outside of the certain ﬁeld\nhas been remained elusive.\nTo provide a better summary for the public\nand researchers, we participated in the ﬁrst Com-\nputational Linguistics Lay Summary Challenge\n(CL-LaySumm 2020) Shared task (Chandrasekaran\net al., (Forthcoming) and developed a summariza-\ntion system that automatically produces lay sum-\nmaries for scholarly documents. The main task of\nCL-LaySumm is producing a corresponding lay\nsummary given the full-text and abstract of the re-\nsearch paper. We employed the dataset from the\nCL-LaySumm 2020 committee and performed ex-\nperiments using recent summarization models in-\ncluding Pre-training with Extracted Gap-sentences\nfor Abstractive Summarization (PEGASUS ; Zhang\net al., 2019b), extractive summarization with Bidi-\nrectional Encoder Representations from Transform-\ners (BERT; Devlin et al., 2019), and a new eval-\nuation protocol that measures the readability of\nthe sentence in the summary. We showcase how\nPEGASUS , BERT, and readability metric improve\nthe summarization system and demonstrate that\n329\nthe produced summary is more readable while it\nsummarizes the main ideas of the documents.\n2 Related Work\nThe type of recent benchmark datasets that are\nwidely utilized for evaluating the performance of\nthe summarization system can be categorized into\ntwo themes: news articles and scientiﬁc documents.\nSummarization of news articles have been more\nactively explored since it is relatively easy to de-\nvelop human-written summaries. Woodsend and\nLapata (2010) and Cheng and Lapata (2016) cre-\nated a large-scale dataset that contains 200K news\narticles with manually written gold summaries.\nOwing to the large-scale dataset and relatively\nsimple structure of the articles, neural abstrac-\ntive summarization using sequence models such\nas Long Term Short Memory (LSTM, Hochreiter\nand Schmidhuber, 1997) with attention mechanism\n(Bahdanau et al., 2014) has been actively used in\nabstractive summarization for news articles. The\nattention-based encoder-decoder network has been\nimproved by others. See et al. (2017) used LSTM\nwith two different networks: pointer-generator net-\nwork that produces accurate expression by pointing\neach word in the source and coverage network that\navoids repetition. Paulus et al. (2017) incorporated\nreinforcement learning (RL) into sequence mod-\nels for summarization tasks and Celikyilmaz et al.\n(2018) developed multi-agent encoders that com-\nmunicate with each other by sharing outputs for\neach layer in the encoder network.\nAfter the advent of pre-trained language mod-\nels such as Transformer, BERT, and Bidirectional\nand Auto-Regressive Transformers ( BART), the\nsummarization literature beneﬁts from these pre-\ntrained language models that provide more contex-\ntual word representation (Vaswani et al., 2017; De-\nvlin et al., 2019; Lewis et al., 2020). Liu and Lapata\n(2019) used BERT model as an encoder, Zhang et al.\n(2019a) applied BERT to both encoder and decoder\nnetworks, Scialom et al. (2020) constructed genera-\ntive adversarial networks using BERT models, and\nYoon et al. (2020) appended semantic similarity\nlayers on top of the pre-trained BART. While neu-\nral sequence models have been successfully applied\nto the summarization for news articles, applying\nthe same techniques to scientiﬁc documents would\nbe challenging since the scholarly documents are\nfar longer than ordinary news articles and have a\ncomplicated structure. Our work is different from\nthe described works as we tackle summarization\nfor scientiﬁc documents.\nAlthough summarization for scientiﬁc texts is\nless explored, Cohan et al. (2018) proposed hier-\narchical encoder-decoder network to address the\nlong scholarly documents for constructing abstract\nsummary, Yasunaga et al. (2019) suggested summa-\nrization using abstract and citation sentences with\ngraph convolutional networks (Kipf and Welling,\n2016) and LSTM, and released the medium-scale\ndataset that contains 1000 scientiﬁc papers in\nthe computational linguistic domain with human-\nwritten summaries and citation sentences for each\npaper. Cachola et al. (2020) implemented an ex-\ntreme summarization system, which is TLDR (Too\nLong; Don’t Read) summarization, for scientiﬁc\ndocuments using multi-task learning with headline\ngeneration models (Vasilyev et al., 2019). Zhang\net al. (2019b) proposed PEGASUS by masking\nimportant sentences in the input document with\na Transformer-based encoder-decoder network to\nforce the model to summarize main points of the\ncontents given the remainder of the text. PEGA -\nSUS tackled summarization for both news articles\nand scholarly documents but it only aimed to pro-\nduce the abstract. In contrast, our work is distinct\nfrom the previous approaches as we aim to pro-\nduce lay summaries for scientiﬁc documents rather\nthan generating extremely short sentences or sum-\nmaries that contain technical words which makes it\ndifﬁcult for lay audiences to understand.\nTo facilitate scholarly document processing,\nthere have been annual workshops regarding\ndata mining, natural language processing (NLP),\ninformation retrieval for scientiﬁc publications:\nBIRNDL (Bibliometric-enhanced Information Re-\ntrieval and Natural Language Processing for Digital\nLibraries), WOSP (Workshop on Mining Scientiﬁc\nPublications), TAC (Text Analytics Conference).\nIn particular, the annual CL-SciSumm (Jaidka et al.,\n2016, 2018; Chandrasekaran et al., 2019) encour-\naged participants to research on scientiﬁc docu-\nments summarization. Our work is closely related\nto the CL-LaySumm 2020, which is the ﬁrst lay\nsummary challenge shared task. We employed the\nLaySumm dataset provided by the workshop or-\nganizing committee and performed experiments\nusing a variety of recent summarization models to\ndevelop the lay summarization system.\n330\n3 Data Analysis\n3.1 Overview\nLaysumm dataset consists of around 600 scientiﬁc\npapers in epilepsy, archeology, and materials en-\ngineering domain, including full-text, abstract and\ncorresponding lay summaries written by authors\nand journalists. The task for CL-LaySumm 2020\nis creating a lay summary with less than 150 words\ngiven the full-text and the abstract of the paper.\nFor evaluation, a test set which contains 37 scien-\ntiﬁc papers without ground truth lay summary is\ngiven. The below table shows the average number\nof words and sentences for each document. Here\nSpacy (Honnibal and Johnson, 2015) is used for\nword tokenization.\n- Train Test\nwords sentences words sentences\nFull-text 4915.31 254.41 5696.57 306.36\nAbstract 271.96 13.27 264.28 12.51\nLaysum 109.07 3.82 — —\nTable 1: Average word, sentence length of dataset.\n3.2 Sentence similarity\nBefore developing a speciﬁc summarization model,\nwe measured the sentence similarity to determine\nwhich type of summarization is suitable for lay\nsummarization. There are two types of summariza-\ntion: extractive summarization and abstractive sum-\nmarization. The extractive summarization scores\nthe importance of sentences in the source and di-\nrectly extracts the sentences based on the score. In\ncontrast, the abstractive summarization generates\nthe summary from scratch while it maintains the\nrepresentative content of the source. We assumed\nthat this resembles the way humans summarize the\ncontents and the lay summarization can be catego-\nrized into the abstractive summarization. However,\nif the sentences in the lay summary exist in the\nabstract or full-text of the paper, extractive sum-\nmarization is more promising. Table 2 shows the\naverage number of overlapping sentences between\nthe sentences in the lay summary and the abstract\nand full-text for the training set.\n— # overlapping sentences\nFull-text 0.01\nAbstract 0.12\nTable 2: Average number of overlapping sentences.\nAs shown in Table 2, the lay summaries were\nwritten from scratch rather than directly using the\n(a) TF-IDF similarity\n(b) Jaccard similarity\nFigure 1: Sentence similarity between the sentences in\nthe lay summary and sentences in the full-text.\nsentences from the abstract or full-text of the paper.\nWe observed that overlapping occurs only 7% of\nthe training set (40 of 572) and if any sentence\nin the lay summary exists in the abstract, there\nare 1.73 overlapping sentences in the abstract on\naverage.\nWe also measured the similarity between the sen-\ntences in the lay summary and the sentences in the\nfull-text. For this task, Term Frequency–Inverse\nDocument Frequency (TF-IDF) and Jaccard simi-\nlarity are used (Sammut and Webb, 2010; Hamers\net al., 1989). Figure 1 shows the maximum value of\nsimilarity for each sentence in the lay summary in\nterms of TF-IDF and Jaccard similarity. As shown\nin the ﬁgures, the similarity is below 0.4 on aver-\nage for TF-IDF and it becomes lower for Jaccard\nsimilarity. From the results of the analysis, we\nexcluded full-text and aimed to produce the lay\nsummary solely with the abstract.\n4 Method\nThere are two main summarization models used in\nour system to generate the lay summary. We tried\nto use PEGASUS which is an abstractive summariza-\ntion model and Presumm (Liu and Lapata, 2019)\nfor extractive summarization to produce the sum-\nmary. We trained the summarization model on the\n331\nlay summary dataset in a supervised way by pairing\nthe abstract and the corresponding lay summary of\nthe paper. After producing the lay summary using\nPEGASUS , we improved the quality of the produced\nsummary by appending important sentences to the\nsummary of which the number of words is under\na certain threshold. For example, if the number\nof the lay summary generated by the abstractive\nmodel was under 90, we added the sentences from\nthe corresponding summry geneated by extractive\nmodel up to this threshold. When appending the\nsentences to the produced summary, we prioritized\nthe sentences in the abstract based on the score pre-\ndicted by Presumm model and readability metric\nand applied Tri-gram blocking to avoid repetition\n(Paulus et al., 2017). Detailed descriptions of each\nsummarization model and the readability metric\nare presented in the following sections.\n4.1 PEGASUS\nWe used PEGASUS that is trained on large text cor-\npus of news text from the web pages to produce\nabstractive summaries (Zhang et al., 2019b). The\narchitecture of PEGASUS model is Transformer-\nbased encoder-decoder network and the model tar-\ngets to output the important sentences by masking\nprincipal sentences or greedily selected sentences\nbased on the ROUGE (Nallapati et al., 2016) in the\ninput text during the training process. We used the\nofﬁcial implementation and the checkpoint of the\npre-trained PEGASUS model without any modiﬁ-\ncation and trained this model directly on the lay\nsummary dataset.\n4.2 PreSumm\nFor extractive summarization, we used the Pre-\nsumm model (Liu and Lapata, 2019) which uses\nBERT, a pre-trained language model, for news arti-\ncle summarization without any modiﬁcation. Pre-\nsumm model uses BERT as a pre-trained encoder.\nThe authors added [CLS] token between the sen-\ntences as the input of BERT to obtain sentence\nrepresentation. This token is used to calculate the\nscore to determine whether each sentence is in-\ncluded in the lay summary.\nFor training this summarization model, we as-\nsumed that the model needs a large-scale dataset\nthat contains thousands of instances to train over\none hundred million of parameters. Since the lay\nsummary dataset only consists of 600 documents,\nwe used the CNN/DM dataset that consists of 300K\nnews articles for the pre-training stage before train-\ning the lay summary dataset. CNN/DM dataset is\na common benchmark used in the summarization\nliterature and the target summary for this dataset\nis somewhat extractive rather than abstractive, thus\nwe considered this dataset seemed suitable for the\nextractive summarization. After training the model\non the CNN/DM dataset for a few iterations, we\nswitched the dataset with the lay summary dataset.\n4.3 Readability of the Sentence\nThe evaluation metric that is widely used in the\nsummarization literature is ROUGE, which reﬂects\nthe ratio of overlapping vocabulary between the\nproduced summary and the ground-truth summary.\nHowever, ROUGE only focuses on counting the\noverlapping words and it is unable to determine\nwhether the sentence is difﬁcult or not to under-\nstand. We believe the produced lay summary has\nto be more readable for the lay audience, thus we\nadopted the readability of the sentence as an addi-\ntional metric and we combine this metric with ex-\ntractive summarization. Speciﬁcally, we combine\nthis metric with extractive summarization. When\nwe produced the extractive summary based on the\nimportant score predicted by Presumm model, we\npruned the sentence of which readability score is\nunder a certain threshold.\nThe readability of the sentence is measured by\nconsidering the ratio of jargon. We used the cor-\npus of words developed by Rakedzon et al. (2017).\nThe authors collected 900 million words published\non the BBC site and classifying the word as easy,\nmedium, and rare (jargon) based on the frequency\nof words used on the BBC site. The dictionary\ncontains around 500K words which were the most\nfrequently used. To measure the readability of the\nsentence, we followed the authors as shown in equa-\ntion (1) with different constant factors (c1, c2, c3)\nin front of each ratio ( r1 : medium, r2 : rare, r3 :\nout of dictionary). We used 10, 20, 30 as constant\nfactors in front of each ratio.\nScore = 100− (c1r1 + c2r2 + c3r3) (1)\nUsing this metric, we measured the average sen-\ntence readability of the abstract and the lay sum-\nmary in the Laysumm dataset. As shown in table 3,\nthe lay summary achieves high readability than the\nabstract since it avoids using technical words. In\nthe next section, we present the readability of the\nproduced summary with ROUGE metric to show\nwhether the summarization model can achieve a\nhigh score in both ROUGE and readability metrics.\n332\n— # Average Sentence Readability\nAbstract 92.25\nLaysumm 96.18\nTable 3: Average sentence readability.\n5 Experiments\n5.1 Dataset and Evaluation\nWe evaluated the performance of the model on the\nlay summary dataset. The lay summary dataset is\ndivided into the train, validation, and test set (8/1/1\nsplit). Evaluation metrics are ROUGE recall and F1\nscore in terms of unigram, bigram, and the longest\ncommon subsequence overlap.\n5.2 Implementation Details\nWe mainly used the ofﬁcial implementation of the\nPEGASUS , Presumm, and pre-trained checkpoints\nprovided by the authors. We did not modify any\nnetwork architecture and for Presumm model, the\ndataset was switched from CNN/DM to lay sum-\nmary data after sufﬁcient training steps. After\nswitching the dataset, all the trainable parameters\nare gradually ﬁne-tuned with a lower learning rate.\nPresumm extractive models were trained on dual\nGPUs (NVIDIA RTX 2080ti) with gradient accu-\nmulating every 4 steps. The model was trained for\n50,000 steps for the pre-training stage and 10,000\nsteps after switching the data into the Laysumm\ndataset. We saved the checkpoints of the model\nevery 200 steps after switching the dataset and per-\nformed validation by choosing the top three check-\npoints, which have the lowest validation loss, to\nevaluate the model on the test set. To generate the\nextractive summary, we selected the sentence from\nthe highest score only if the readability score is\nover 85 until the number of words in the produced\nsummary is over 150. Trigram Blocking (Paulus\net al., 2017) is applied when generating the sum-\nmary to reduce the redundancy.\nPEGASUS model was trained for 20,000 steps on\na single GPU (NVIDIA RTX 2080ti) with hyper-\nparameters provided by the authors except for batch\nsize and learning rate. Due to the memory con-\nstraints, we decreased batch size to 1 with a de-\ncreased learning rate at 0.0001. We saved the\ncheckpoints of the model every 1000 steps and\nperformed the same validation done in the extrac-\ntive summarization and chose beam search at size\n10 to encourage the model not to generate short\nsentences.\n5.3 Results\nThe best results were achieved by submitting differ-\nent checkpoints from the validation and test stage\nfor each model. The performance of extractive and\nabstractive models are summarized in table 4. EXT\nand ABS indicate Presumm extractive model and\nPEGASUS abstractive model respectively. ABSEXT\nmeans sentences produced by the extractive model\nare appended to the abstractive summary until the\nnumber of words is over 90. As reported in the\ntable 4, the hybrid approach outperforms a solely\nextractive or abstractive model. Hybrid model ben-\neﬁts from high recall in the extractive model and\nhigh precision in the abstractive model.\nModel ROUGE-F1(1/2/L) ROUGE-R(1/2/L)\nEXT 42.96 17.85 23.38 45.85 18.92 25.02\nABS 43.61 20.51 28.98 43.16 20.35 28.59\nABSEXT 45.96 21.46 29.77 48.10 22.37 31.05\nTable 4: Best ROUGE F1, Recall results on test set.\n5.4 Analysis of Threshold\nIn this section, we investigate how the number of\nwords in the produced lay summary affects the per-\nformance of the summarization model. We ﬁrst pro-\nduced lay summaries using PEGASUS(ABS) and\nmeasured the number of words for each summary.\nThen, we set a standard threshold and appended\nsentences from the extractive summary produced\nby PRESUMM (EXT) if the number of words in\nthe abstractive summary is below that limit. Ta-\nble 5 shows the ROUGE F1 score with respect to\ndifferent threshold values and the average number\nof words of lay summaries after appending sen-\ntences. ABSEXT with a threshold at 90 performs\nbest and it shows appending sentences from the\nextractive model to the abstractive summary consis-\ntently improves the performance. This makes sense\nas the abstractive model( ABS ) tends to produce\nshort summaries: the average number of words\nin abstractive summary is 82, whereas the average\nnumber of words in the ground-truth lay summaries\nin the Laysumm dataset is around 110.\n5.5 Readability of Summary\nWe provide ROUGE-F1 and readability scores for\neach model. As shown in Table 6, for the extractive\nsummary, EXT performs better than EXT W /O R,\ndemonstrating excluding hard sentences improves\nthe performance on both ROUGE and readability\nmetrics. When the extractive summary is combined\n333\nThreshold # of words ROUGE-F1 (1/2/L)\n— 82.54 43.61 20.51 28.98\n70 85.92 44.12 20.33 29.12\n90 95.19 45.96 21.46 29.77\n115 105.78 45.19 21.00 29.02\n135 114.54 45.17 20.95 28.22\nTable 5: F1 with different threshold on test set. The\nﬁrst row indicates the abstractive model without being\nembedded with sentences from the extractive summary.\nwith abstractive summary (ABSEXT W/O R, AB-\nSEXT ), readability constraints slightly improves\nthe performance on both ROUGE-F1 and readabil-\nity metric. Overall, we observed that our models\nsuccessfully produce lay summaries that are more\nreadable than the abstract.\nModel(dataset) Readability ROUGE-F1 (1/2/L)\nEXT w/o R 93.09 42.35 17.66 23.37\nEXT 93.39 42.69 17.85 23.38\nABSEXT W/O R 93.83 45.86 21.46 29.76\nABSEXT 93.85 45.96 21.46 29.77\n(Abstract) 92.25 — — —\n(LaySumm) 96.18 — — —\nTable 6: ROUGE-F1 and readability score on test\nset. E XT W /O R means P RESUMM extractive summa-\nrization model without readability constraints, whereas\nEXT model involves pruning sentences whose readabil-\nity is under 85 from the produced summary. ABS +\nEXT W/O R and ABS + EXT indicate sentences from\nEXT W /O R and E XT W /O R are embedded to abstrac-\ntive summary respectively. (Abstract) and (Lay sum-\nmary) are the abstract and the lay summary of Lay-\nsumm dataset.\n6 Discussion and Future work\nWe applied transfer learning to mitigate the absence\nof large-scale datasets to tackle the lay summariza-\ntion task. While we demonstrated that transfer\nlearning can result in a good performance, it can\ncreate a bottleneck for the model due to the discrep-\nancy between the distributions of datasets, result-\ning in sub-optimal solutions. Our summarization\nmodel also excludes the full-text of the paper and\ntries to produce the summary solely based on the\nabstract. Although the model achieves good per-\nformance, there might exist important points in the\nbody of the paper. It is obvious for humans to\nutilize the full-text of the paper to write a better\nlay summary. Creating a large-scale lay summary\ndataset that handles scholarly documents and con-\nsidering important sentences from the body text\ncan be a promising direction to address these issues.\nThe readability score might be usefully utilized for\nconstructing the large-scale dataset since it is nec-\nessary to pair the difﬁcult sentences and a more\nreadable lay summary.\nSecondly, in the optimization process during\ntraining the model, we only focused on predict-\ning the only ground truth lay summary. This might\nlimit the capability of the summarization model.\nApplying the readability score as an additional fea-\nture in the training stage would make the model\nmore creative and help the system to summarize the\ncontents while it selectively chooses easier words.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nLutz Bornmann and R¨udiger Mutz. 2015. Growth rates\nof modern science: A bibliometric analysis based\non the number of publications and cited references.\nJournal of the Association for Information Science\nand Technology, 66(11):2215–2222.\nIsabel Cachola, Kyle Lo, Arman Cohan, and Daniel S\nWeld. 2020. Tldr: Extreme summarization of scien-\ntiﬁc documents. arXiv preprint arXiv:2004.15011.\nAsli Celikyilmaz, Antoine Bosselut, Xiaodong He, and\nYejin Choi. 2018. Deep communicating agents for\nabstractive summarization. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 1662–1675, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nM. K. Chandrasekaran, G. Feigenblat, E. Hovy,\nA. Ravichander, M. Shmueli-Scheuer, and\nA. De Waard. (Forthcoming). Overview and\ninsights from scientiﬁc document summarization\nshared tasks 2020: Cl-scisumm, laysumm and\nlongsumm. In Proceedings of the First Workshop\non Scholarly Document Processing (SDP 2020).\nMuthu Kumar Chandrasekaran, Michihiro Yasunaga,\nDragomir Radev, Dayne Freitag, and Min-Yen Kan.\n2019. Overview and results: Cl-scisumm shared\ntask 2019. arXiv preprint arXiv:1907.09854.\nJianpeng Cheng and Mirella Lapata. 2016. Neural sum-\nmarization by extracting sentences and words. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 484–494, Berlin, Germany. As-\nsociation for Computational Linguistics.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Na-\nzli Goharian. 2018. A discourse-aware attention\n334\nmodel for abstractive summarization of long docu-\nments. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 615–621,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLieve Hamers et al. 1989. Similarity measures in scien-\ntometric research: The jaccard index versus salton’s\ncosine formula. Information Processing and Man-\nagement, 25(3):315–18.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nMatthew Honnibal and Mark Johnson. 2015. An im-\nproved non-monotonic transition system for depen-\ndency parsing. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1373–1378, Lisbon, Portugal. As-\nsociation for Computational Linguistics.\nKokil Jaidka, Niyati Chhaya, and Lyle Ungar. 2018. Di-\nachronic degradation of language models: Insights\nfrom social media. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) , pages 195–\n200, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nKokil Jaidka, Muthu Kumar Chandrasekaran, Sajal\nRustagi, and Min-Yen Kan. 2016. Overview of the\nCL-SciSumm 2016 shared task. In Proceedings of\nthe Joint Workshop on Bibliometric-enhanced Infor-\nmation Retrieval and Natural Language Processing\nfor Digital Libraries (BIRNDL), pages 93–102.\nThomas N Kipf and Max Welling. 2016. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nManuel J Ma ˜na-L´opez, Manuel De Buenaga, and\nJos´e M G´omez-Hidalgo. 2004. Multidocument sum-\nmarization: An added value to clustering in inter-\nactive retrieval. ACM Transactions on Information\nSystems (TOIS), 22(2):215–241.\nInderjeet Mani, Gary Klein, David House, Lynette\nHirschman, Therese Firmin, and Beth Sundheim.\n2002. Summac: a text summarization evaluation.\nNatural Language Engineering, 8(1):43–68.\nKathleen McKeown, Rebecca J Passonneau, David K\nElson, Ani Nenkova, and Julia Hirschberg. 2005.\nDo summaries help? In Proceedings of the 28th\nannual international ACM SIGIR conference on Re-\nsearch and development in information retrieval ,\npages 210–217.\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2016.\nSummarunner: A recurrent neural network based se-\nquence model for extractive summarization of docu-\nments. arXiv preprint arXiv:1611.04230.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2017. A deep reinforced model for abstractive sum-\nmarization. arXiv preprint arXiv:1705.04304.\nPontus Plav ´en-Sigray, Granville James Matheson,\nBj¨orn Christian Schifﬂer, and William Hedley\nThompson. 2017. The readability of scientiﬁc texts\nis decreasing over time. Elife, 6:e27725.\nTzipora Rakedzon, Elad Segev, Noam Chapnik, Roy\nYosef, and Ayelet Baram-Tsabari. 2017. Automatic\njargon identiﬁer for scientists engaging with the pub-\nlic and science communication educators. PloS one,\n12(8):e0181742.\nDmitri G Roussinov and Hsinchun Chen. 2001. Infor-\nmation navigation on the web by clustering and sum-\nmarizing query results. Information Processing &\nManagement, 37(6):789–816.\nClaude Sammut and Geoffrey I Webb. 2010. Tf–idf.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, and Jacopo Staiano. 2020.\nDiscriminative adversarial search for abstractive\nsummarization. arXiv preprint arXiv:2002.10375.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nOleg Vasilyev, Tom Grek, and John Bohannon. 2019.\nHeadline generation: Learning from decomposable\ndocument titles. arXiv preprint arXiv:1904.08455.\n335\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nKristian Woodsend and Mirella Lapata. 2010. Auto-\nmatic generation of story highlights. In Proceed-\nings of the 48th Annual Meeting of the Association\nfor Computational Linguistics, pages 565–574, Up-\npsala, Sweden. Association for Computational Lin-\nguistics.\nMichihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexan-\nder R Fabbri, Irene Li, Dan Friedman, and\nDragomir R Radev. 2019. Scisummnet: A large an-\nnotated corpus and content-impact models for scien-\ntiﬁc paper summarization with citation networks. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 33, pages 7386–7393.\nWonjin Yoon, Yoon Sun Yeo, Minbyul Jeong, Bong-\nJun Yi, and Jaewoo Kang. 2020. Learning by se-\nmantic similarity makes abstractive summarization\nbetter. arXiv preprint arXiv:2002.07767.\nHaoyu Zhang, Jingjing Cai, Jianjun Xu, and Ji Wang.\n2019a. Pretraining-based natural language gener-\nation for text summarization. In Proceedings of\nthe 23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL) , pages 789–797, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J Liu. 2019b. Pegasus: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\narXiv preprint arXiv:1912.08777.\nA Lay Summary Example\nWe present our approach of using Pre-training\nwith Extracted Gap-sentences for Abstractive\nSummarization (PEGASUS ; Zhang et al., 2019b)\nto produce the lay summary and combining\nthose with the extractive summarization model\nusing Bidirectional Encoder Representations\nfrom Transformers (BERT; Devlin et al., 2019)\nand readability metrics that measure the read-\nability of the sentence to further improve the\nquality of the summary. Our model achieves\na remarkable performance on ROUGE metrics,\ndemonstrating the produced summary is more\nreadable while it summarizes the main points of\nthe document.\nTable 7: An example of a lay summary generated by\nABSEXT model. The model only considers the ab-\nstract to produce the lay summary.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9725726246833801
    },
    {
      "name": "Readability",
      "score": 0.9348678588867188
    },
    {
      "name": "Computer science",
      "score": 0.7999246120452881
    },
    {
      "name": "Transformer",
      "score": 0.7775437831878662
    },
    {
      "name": "Natural language processing",
      "score": 0.6309898495674133
    },
    {
      "name": "Sentence",
      "score": 0.581515908241272
    },
    {
      "name": "Encoder",
      "score": 0.5802310109138489
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5335605144500732
    },
    {
      "name": "Information retrieval",
      "score": 0.4910407066345215
    },
    {
      "name": "Engineering",
      "score": 0.0707939863204956
    },
    {
      "name": "Programming language",
      "score": 0.058809876441955566
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}