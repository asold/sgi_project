{
  "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
  "url": "https://openalex.org/W2973727699",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5072436307",
      "name": "Mohammad Shoeybi",
      "affiliations": [
        "Nvidia (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5031170568",
      "name": "Mostofa Patwary",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5041667198",
      "name": "Raul Puri",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5068437386",
      "name": "Patrick LeGresley",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5010396318",
      "name": "Jared Casper",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5066242985",
      "name": "Bryan Catanzaro",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2884700152",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2951991713",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2507974895",
    "https://openalex.org/W2185726469",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W2945785363",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W2936652946",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2158139315",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W2473344385",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2891017939",
    "https://openalex.org/W2757910899",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2947813521",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963959597",
    "https://openalex.org/W2914526845",
    "https://openalex.org/W2083842231",
    "https://openalex.org/W2988841832",
    "https://openalex.org/W2338908902",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2807147113"
  ],
  "abstract": "Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",
  "full_text": "Megatron-LM: Training Multi-Billion Parameter Language Models Using\nModel Parallelism\nMohammad Shoeybi1 2 Mostofa Patwary1 2 Raul Puri1 2 Patrick LeGresley2 Jared Casper2\nBryan Catanzaro2\nAbstract\nRecent work in language modeling demonstrates\nthat training large transformer models advances\nthe state of the art in Natural Language Processing\napplications. However, very large models can be\nquite difﬁcult to train due to memory constraints.\nIn this work, we present our techniques for train-\ning very large transformer models and implement\na simple, efﬁcient intra-layer model parallel ap-\nproach that enables training transformer models\nwith billions of parameters. Our approach does\nnot require a new compiler or library changes, is\northogonal and complimentary to pipeline model\nparallelism, and can be fully implemented with\nthe insertion of a few communication operations\nin native PyTorch. We illustrate this approach\nby converging transformer based models up to\n8.3 billion parameters using 512 GPUs. We sus-\ntain 15.1 PetaFLOPs across the entire applica-\ntion with 76% scaling efﬁciency when compared\nto a strong single GPU baseline that sustains 39\nTeraFLOPs, which is 30% of peak FLOPs. To\ndemonstrate that large language models can fur-\nther advance the state of the art (SOTA), we train\nan 8.3 billion parameter transformer language\nmodel similar to GPT-2 and a 3.9 billion parame-\nter model similar to BERT. We show that careful\nattention to the placement of layer normalization\nin BERT-like models is critical to achieving in-\ncreased performance as the model size grows. Us-\ning the GPT-2 model we achieve SOTA results\non the WikiText103 (10.8 compared to SOTA per-\nplexity of 15.8) and LAMBADA (66.5% com-\npared to SOTA accuracy of 63.2%) datasets. Our\nBERT model achieves SOTA results on the RACE\ndataset (90.9% compared to SOTA accuracy of\n89.4%).\n1Equal contribution 2NVIDIA. Correspondence to: Mohammad\nShoeybi <mshoeybi@nvidia.com>.\n1. Introduction\nNatural Language Processing (NLP) is advancing quickly in\npart due to an increase in available compute and dataset size.\nThe abundance of compute and data enables training increas-\ningly larger language models via unsupervised pretraining\n(Devlin et al., 2018; Radford et al., 2019). Empirical evi-\ndence indicates that larger language models are dramatically\nmore useful for NLP tasks such as article completion, ques-\ntion answering, and natural language inference (Lan et al.,\n2019; Raffel et al., 2019). By ﬁnetuning these pretrained\nlanguage models on downstream natural language tasks,\none can achieve state of the art results as shown in recent\nwork (Devlin et al., 2018; Peters et al., 2018; Howard &\nRuder, 2018; Radford et al., 2018; 2017; Ramachandran\net al., 2016; Liu et al., 2019b; Dai et al., 2019; Yang et al.,\n2019; Liu et al., 2019a; Lan et al., 2019).\nAs these models become larger, they exceed the memory\nlimit of modern processors, and require additional memory\nmanagement techniques such as activation checkpointing\n(Chen et al., 2016). Widely used optimization algorithms\nsuch as ADAM require additional memory per parameter to\nstore momentum and other optimizer state, which reduces\nthe size of models that can be effectively trained. Several\napproaches to model parallelism overcome this limit by\npartitioning the model such that the weights and their asso-\nciated optimizer state do not need to reside concurrently on\nthe processor. For example, GPipe (Huang et al., 2018) and\nMesh-Tensorﬂow (Shazeer et al., 2018) provide frameworks\nfor model parallelism of different kinds. However, they\nrequire rewriting the model, and rely on custom compilers\nand frameworks that are still under development.\nIn this work, we implement a simple and efﬁcient model\nparallel approach using intra-layer model-parallelism. We\nexploit the inherent structure in transformer based language\nmodels to make a simple model-parallel implementation that\ntrains efﬁciently in PyTorch, with no custom C++ code or\ncompiler required. This approach is orthogonal to pipeline-\nbased model parallelism as advocated by approaches such\nas GPipe (Huang et al., 2018).\nTo demonstrate the scalability of our approach, we establish\narXiv:1909.08053v4  [cs.CL]  13 Mar 2020\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 1.Model (blue) and model+data (green) parallel FLOPS\nas a function of number of GPUs. Model parallel (blue): up to\n8-way model parallel weak scaling with approximately 1 billion\nparameters per GPU (e.g. 2 billion for 2 GPUs and 4 billion for\n4 GPUs). Model+data parallel (green): similar conﬁguration as\nmodel parallel combined with 64-way data parallel.\na baseline by training a model of 1.2 billion parameters\non a single NVIDIA V100 32GB GPU, that sustains 39\nTeraFLOPs. This is 30% of the theoretical peak FLOPS\nfor a single GPU as conﬁgured in a DGX-2H server, and\nis thus a strong baseline. Scaling the model to 8.3 billion\nparameters on 512 GPUs with 8-way model parallelism,\nwe achieve up to 15.1 PetaFLOPs per second sustained\nover the entire application. This is 76% scaling efﬁciency\ncompared to the single GPU case. Figure 1 shows more\ndetailed scaling results.\nTo analyze the effect of model size scaling on accuracy,\nwe train both left-to-right GPT-2 (Radford et al., 2019) lan-\nguage models as well as BERT (Devlin et al., 2018) bidi-\nrectional transformers and evaluate them on several down-\nstream tasks. We show that the existing BERT architecture\nresults in model degradation as the size increases. We over-\ncome this challenge by rearranging the layer normalization\nand residual connection in the transformer layers and show\nthat with this change, results for the downstream tasks on\ndevelopment sets improve monotonically as the model size\nincreases. In addition, we show that our models achieve\ntest set state of the art (SOTA) results on WikiText103,\ncloze-style prediction accuracy on LAMBADA, and reading\ncomprehension RACE datasets.\nIn summary, our contributions are as follows:\n• We implement a simple and efﬁcient model parallel\napproach by making only a few targeted modiﬁcations\nto an existing PyTorch transformer implementation.\n• We perform an in-depth empirical analysis of our\nmodel and data parallel technique and demonstrate\nup to 76% scaling efﬁciency using 512 GPUs.\n• We show that careful attention to the placement of\nlayer normalization in BERT-like models is critical to\nachieving increased accuracies as the model grows.\n• We demonstrate that scaling the model size results in\nimproved accuracies for both GPT-2 (studied up to\n8.3 billion parameters) and BERT (studied up to 3.9B\nparameters) models.\n• We showcase that our models achieve state of the art\nresults on test sets: perplexity on WikiText103 (10.8\nppl), accuracy on LAMBADA (66.5%), and accuracy\non RACE (90.9%).\n• We open source our code along with the training\nand evaluation pipelines athttps://github.com/\nNVIDIA/Megatron-LM\n2. Background and Challenges\n2.1. Neural Language Model Pretraining\nPretrained language models have become an indispensable\npart of NLP researchers’ toolkits. Leveraging large corpus\npretraining to learn robust neural representations of lan-\nguage is an active area of research that has spanned the\npast decade. Early examples of pretraining and transferring\nneural representations of language demonstrated that pre-\ntrained word embedding tables improve downstream task\nresults compared to word embedding tables learned from\nscratch (Mikolov et al., 2013; Pennington et al., 2014; Turian\net al., 2010). Later work advanced research in this area by\nlearning and transferring neural models that capture contex-\ntual representations of words (Melamud et al., 2016; Mc-\nCann et al., 2017; Peters et al., 2018; Radford et al., 2017;\n2019). Recent parallel work (Ramachandran et al., 2016;\nHoward & Ruder, 2018; Radford et al., 2018; Devlin et al.,\n2018; Liu et al., 2019b; Dai et al., 2019; Yang et al., 2019;\nLiu et al., 2019a; Lan et al., 2019) further builds upon these\nideas by not just transferring the language model to extract\ncontextual word representations, but by also ﬁnetuning the\nlanguage model in an end to end fashion on downstream\ntasks. Through these works, the state of the art has advanced\nfrom transferring just word embedding tables to transferring\nentire multi-billion parameter language models. This pro-\ngression of methods has necessitated the need for hardware,\nsystems techniques, and frameworks that are able to oper-\nate efﬁciently at scale and satisfy increasing computational\nneeds. Our work aims to provide the tools necessary to take\nanother step forward in this trend.\n2.2. Transformer Language Models and Multi-Head\nAttention\nCurrent work in NLP trends towards usingtransformer mod-\nels (Vaswani et al., 2017) due to their superior accuracy\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 2.Transformer Architecture. Purple blocks correspond to\nfully connected layers. Each blue block represents a single trans-\nformer layer that is replicated N times.\nand compute efﬁciency. The original transformer formula-\ntion was designed as a machine translation architecture that\ntransforms an input sequence into another output sequence\nusing two parts, an Encoder and Decoder. However, recent\nwork leveraging transformers for language modeling such as\nBERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019)\nuse only the Encoder or Decoder depending on their needs.\nThis work explores both a decoder architecture, GPT-2, and\nan encoder architecture, BERT.\nFigure 2 shows a schematic diagram of the model we used.\nWe refer the reader to prior work for a detailed descrip-\ntion of the model architecture (Vaswani et al., 2017; Devlin\net al., 2018; Radford et al., 2019). It is worthwhile to men-\ntion that both GPT-2 and BERT use GeLU (Hendrycks &\nGimpel, 2016) nonlinearities and layer normalization (Ba\net al., 2016) to the input of the multi-head attention and feed\nforward layers, whereas the original transformer (Vaswani\net al., 2017) uses ReLU nonlinearities and applies layer\nnormalization to outputs.\n2.3. Data and Model Parallelism in Deep Learning\nThere are two central paradigms for scaling out deep neu-\nral network training to numerous hardware accelerators:\ndata parallelism (Valiant, 1990) where a training minibatch\nis split across multiple workers, and model parallelism in\nwhich the memory usage and computation of a model is\ndistributed across multiple workers. By increasing the mini-\nbatch size proportionally to the number of available work-\ners (i.e. weak scaling), one observes near linear scaling\nin training data throughput. However, large batch train-\ning introduces complications into the optimization process\nthat can result in reduced accuracy or longer time to conver-\ngence, offsetting the beneﬁt of increased training throughput\n(Keskar et al., 2017). Further research (Goyal et al., 2017;\nYou et al., 2017; 2019) has developed techniques to miti-\ngate these effects and drive down the training time of large\nneural networks. To scale out training even further, parallel\nwork (Chen et al., 2016) has combined data parallelism with\nactivation checkpointing: recomputing activations in the\nbackward pass without storing them in the forward pass to\nreduce memory requirements.\nHowever, these techniques have one fundamental limitation\nin the problem size they can tackle: the model must ﬁt\nentirely on one worker. With language models of increasing\nsize and complexity like BERT and GPT-2, neural networks\nhave approached the memory capacity of modern hardware\naccelerators. One solution to this problem is to employ\nparameter sharing to reduce the memory footprint of the\nmodel (Lan et al., 2019), but this limits the overall capacity\nof the model. Our approach is to utilize model parallelism\nto split the model across multiple accelerators. This not\nonly alleviates the memory pressure, but also increases the\namount of parallelism independently of the microbatch size.\nWithin model parallelism, there are two further paradigms:\nlayer-wise pipeline parallelism, and more general distributed\ntensor computation. In pipeline model parallelism, groups\nof operations are performed on one device before the outputs\nare passed to the next device in the pipeline where a differ-\nent group of operations are performed. Some approaches\n(Harlap et al., 2018; Chen et al., 2018) use a parameter\nserver (Li et al., 2014) in conjunction with pipeline par-\nallelism. However these suffer from inconsistency issues.\nThe GPipe framework for TensorFlow (Huang et al., 2018)\novercomes this inconsistency issue by using synchronous\ngradient decent. This approach requires additional logic to\nhandle the efﬁcient pipelining of these communication and\ncomputation operations, and suffers from pipeline bubbles\nthat reduce efﬁciency, or changes to the optimizer itself\nwhich impact accuracy.\nDistributed tensor computation is an orthogonal and more\ngeneral approach that partitions a tensor operation across\nmultiple devices to accelerate computation or increase\nmodel size. FlexFlow (Jia et al., 2018), a deep learning\nframework orchestrating such parallel computation, pro-\nvides a method to pick the best parallelization strategy. Re-\ncently, Mesh-TensorFlow (Shazeer et al., 2018) introduced\na language for specifying a general class of distributed ten-\nsor computations in TensorFlow (Abadi et al., 2015). The\nparallel dimensions are speciﬁed in the language by the\nend user and the resulting graph is compiled with proper\ncollective primitives. We utilize similar insights to those\nleveraged in Mesh-TensorFlow and exploit parallelism in\ncomputing the transformer’s attention heads to parallelize\nour transformer model. However, rather than implementing\na framework and compiler for model parallelism, we make\nonly a few targeted modiﬁcations to existing PyTorch trans-\nformer implementations. Our approach is simple, does not\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nrequire any new compiler or code re-writing, and can be\nfully implemented by inserting a few simple primitives, as\ndescribed in the next section.\n3. Model Parallel Transformers\nWe take advantage of the structure of transformer networks\nto create a simple model parallel implementation by adding a\nfew synchronization primitives. A transformer layer consists\nof a self attention block followed by a two-layer, multi-layer\nperceptron (MLP) as shown in Figure 2. We introduce\nmodel parallelism in both of these blocks separately.\nWe start by detailing the MLP block. The ﬁrst part of the\nblock is a GEMM followed by a GeLU nonlinearity:\nY = GeLU(XA) (1)\nOne option to parallelize the GEMM is to split the weight\nmatrix Aalong its rows and input X along its columns as:\nX = [X1,X2], A=\n[A1\nA2\n]\n. (2)\nThis partitioning will result in Y = GeLU(X1A1 +\nX2A2). Since GeLU is a nonlinear function,GeLU(X1A1+\nX2A2) ̸= GeLU(X1A1)+GeLU(X2A2) and this approach\nwill require a synchronization point before the GeLU func-\ntion.\nAnother option is to splitAalong its columns A= [A1,A2].\nThis partitioning allows the GeLU nonlinearity to be inde-\npendently applied to the output of each partitioned GEMM:\n[Y1,Y2] = [GeLU(XA1),GeLU(XA2)] (3)\nThis is advantageous as it removes a synchronization point.\nHence, we partition the ﬁrst GEMM in this column parallel\nfashion and split the second GEMM along its rows so it takes\nthe output of the GeLU layer directly without requiring any\ncommunication as shown in Figure 3a. The output of the\nsecond GEMM is then reduced across the GPUs before\npassing the output to the dropout layer. This approach splits\nboth GEMMs in the MLP block across GPUs and requires\nonly a single all-reduce operation in the forward pass ( g\noperator) and a single all-reduce in the backward pass ( f\noperator). These two operators are conjugates of each other\nand can be implemented in PyTorch with only a few lines of\ncode. As an example, the implementation of the f operator\nis provided below:\nclass f(torch.autograd.Function):\ndef forward(ctx, x):\nreturn x\ndef backward(ctx, gradient):\nall_reduce(gradient)\nreturn gradient\nCode 1.Implementation of f operator. g is similar to f with\nidentity in the backward and all-reduce in the forward\nfunctions.\n(a) MLP\n(b) Self-Attention\nFigure 3.Blocks of Transformer with Model Parallelism. f and g\nare conjugate. f is an identity operator in the forward pass and all\nreduce in the backward pass while g is an all reduce in the forward\npass and identity in the backward pass.\nAs shown in Figure 3b, for the self attention block we exploit\ninherent parallelism in the multihead attention operation,\npartitioning the GEMMs associated with key ( K), query\n(Q), and value (V) in a column parallel fashion such that\nthe matrix multiply corresponding to each attention head is\ndone locally on one GPU. This allows us to split per atten-\ntion head parameters and workload across the GPUs, and\ndoesnt require any immediate communication to complete\nthe self-attention. The subsequent GEMM from the output\nlinear layer (after self attention) is parallelized along its\nrows and takes the output of the parallel attention layer di-\nrectly, without requiring communication between the GPUs.\nThis approach for both the MLP and self attention layer\nfuses groups of two GEMMs, eliminates a synchronization\npoint in between, and results in better scaling. This enables\nus to perform all GEMMs in a simple transformer layer\nusing only two all-reduces in the forward path and two in\nthe backward path (see Figure 4).\nThe transformer language model has an output embedding\nwith the dimension of hidden-size ( H) times vocabulary-\nsize (v). Since the vocabulary size is on the order of tens\nof thousands of tokens for modern language models (for\nexample, GPT-2 used a vocabulary size of 50,257), it is ben-\neﬁcial to parallelize the output embedding GEMM. How-\never, in transformer language models, the output embed-\nding layer shares weights with the input embedding, requir-\ning modiﬁcations to both. We parallelize the input embed-\nding weight matrix EH×v along the vocabulary dimension\nE = [E1,E2] (column-wise). Since each partition now only\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 4.Communication operations in a transformer layer. There\nare 4 total communication operations in the forward and backward\npass of a single model parallel transformer layer.\ncontains a portion of the embedding table, an all-reduce (g\noperator) is required after the input embedding. For the\noutput embedding, one approach is to perform the parallel\nGEMM [Y1,Y2] = [XE1,XE2] to obtain the logits, add an\nall-gather Y = all-gather([Y1,Y2]), and send the results to\nthe cross-entropy loss function. However, for this case, the\nall-gather will communicate b×s×v elements (bis the\nbatch-size and sis the sequence length) which is huge due to\nvocabulary size being large. To reduce the communication\nsize, we fuse the output of the parallel GEMM [Y1,Y2] with\nthe cross entropy loss which reduces the dimension to b×s.\nCommunicating scalar losses instead of logits is a huge re-\nduction in communication that improves the efﬁciency of\nour model parallel approach.\nMuch of our model parallel approach can be characterized\nas techniques aimed at reducing communication and keep-\ning the GPUs compute bound. Rather than having one GPU\ncompute part of the dropout, layer normalization, or residual\nconnections and broadcast the results to other GPUs, we\nchoose to duplicate the computation across GPUs. Speciﬁ-\ncally, we maintain duplicate copies of layer normalization\nparameters on each GPU, and take the output of the model\nparallel region and run dropout and residual connection\non these tensors before feeding them as input to the next\nmodel parallel regions. To optimize the model we allow\neach model parallel worker to optimize its own set of pa-\nrameters. Since all values are either local to or duplicated\non a GPU, there is no need for communicating updated\nparameter values in this formulation.\nWe present further details about the hybrid model and data\nparallelism and handling random number generation in Ap-\npendix B for reference. In summary, our approach as de-\nscribed above is simple to implement, requiring only a few\nextra all-reduce operations added to the forward and back-\nward pass. It does not require a compiler, and is orthogonal\nand complementary to the pipeline model parallelism advo-\ncated by approaches such as (Huang et al., 2018).\n4. Setup\nPretrained language understanding models are central tasks\nin natural language processing and language understanding.\nThere are several formulations of language modeling. In\nthis work we focus on GPT-2 (Radford et al., 2019), a left-\nto-right generative transformer based language model, and\nBERT (Devlin et al., 2018), a bi-directional transformer\nmodel based on language model masking. We explain our\nconﬁgurations for these models in the following section and\nrefer to the original papers for more details.\n4.1. Training Dataset\nTo collect a large diverse training set with longterm de-\npendencies we aggregate several of the largest language\nmodeling datasets. We create an aggregate dataset consist-\ning of Wikipedia (Devlin et al., 2018), CC-Stories (Trinh &\nLe, 2018), RealNews (Zellers et al., 2019), and OpenWeb-\ntext (Radford et al., 2019). To avoid training set leakage\ninto our downstream tasks we remove the Wikipedia articles\npresent in the WikiText103 test set (Merity et al., 2016).\nWe also remove unnecessary newlines from the CC-Stories\ncorpus introduced by preprocessing artifacts. For BERT\nmodels we include BooksCorpus (Zhu et al., 2015) in the\ntraining dataset, however, this dataset is excluded for GPT-2\ntrainings as it overlaps with LAMBADA task.\nWe combined all the datasets and then ﬁltered out all the\ndocuments with content length less than 128 tokens from\nthe aggregated dataset. Since similar content might appear\nmultiple times in the aggregated datasets, we used locality-\nsensitive hashing (LSH) to deduplicate content with a jac-\ncard similarity greater than 0.7. The resulting aggregate\ncorpus contains 174 GB of deduplicated text.\n4.2. Training Optimization and Hyperparameters\nTo train our models efﬁciently we utilize mixed precision\ntraining with dynamic loss scaling to take advantage of the\nV100’s Tensor Cores (Micikevicius et al., 2017; NVIDIA,\n2018). We start by initializing our weights W with a sim-\nple normal distribution W ∼N (0,0.02). We then scale\nweights immediately before residual layers by 1√\n2N where\nN is the number of transformer layers comprised of self at-\ntention and MLP blocks. For our optimizer we utilize Adam\n(Kingma & Ba, 2014) with weight decay (Loshchilov &\nHutter, 2019) λ= 0.01. Additionally, we use global gradi-\nent norm clipping of 1.0 to improve the stability of training\nlarge models. In all cases, a dropout of 0.1 is used. Lastly,\nto better manage our memory footprint we utilize activation\ncheckpointing (Chen et al., 2016) after every transformer\nlayer.\nFor GPT-2 models, all training is performed with sequences\nof 1024 subword units at a batch size of 512 for 300k itera-\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\ntions. Our learning rate of 1.5e-4 utilizes a warmup period\nof 3k iterations before following a single cycle cosine decay\nover the remaining 297k iterations. We stop the decay at a\nminimum learning rate of 1e-5.\nFor BERT models, we largely follow the training process\ndescribed in (Lan et al., 2019). We use the original BERT\ndictionary with vocab size of 30,522. In addition, we re-\nplace the next sentence prediction head with sentence order\nprediction as suggested by (Lan et al., 2019) and use whole\nword n-gram masking of (Joshi et al., 2019). For all cases,\nwe set the batch size to 1024 and use a learning rate of 1.0e-\n4 warmed up over 10,000 iterations and decayed linearly\nover 2 million iterations. Other training parameters are kept\nthe same as (Devlin et al., 2018).\n5. Experiments\nAll of our experiments use up to 32 DGX-2H servers (a total\nof 512 Tesla V100 SXM3 32GB GPUs). Our infrastruc-\nture is optimized for multi-node deep learning applications,\nwith 300 GB/sec bandwidth between GPUs inside a server\nvia NVSwitch and 100 GB/sec of interconnect bandwidth\nbetween servers using 8 InﬁniBand adapters per server.\n5.1. Scaling Analysis\nTo test the scalability of our implementation, we consider\nGPT-2 models with four sets of parameters detailed in Table\n1. To have consistent GEMM sizes in the self attention layer,\nthe hidden size per attention head is kept constant at 96\nwhile the number of heads and layers are varied to obtain\nconﬁgurations ranging from 1 billion to 8 billion parameters.\nThe conﬁguration with 1.2 billion parameters ﬁts on a single\nGPU whereas the 8 billion parameter model requires 8-way\nmodel parallelism (8 GPUs). The original vocabulary size\nwas 50,257, however, to have efﬁcient GEMMs for the logit\nlayer, it is beneﬁcial for the per-GPU vocabulary size to\nbe a multiple of 128. Since we study up to 8-way model\nparallelism, we pad the vocabulary such that it is divisible\nby 128 ×8 = 1024, resulting in a padded vocabulary size\nof 51,200. We study both model and model+data parallel\nscaling. For the model parallel scaling, a ﬁxed batch size of\n8 is used across all conﬁgurations. Data parallel scaling is\nnecessary for training many state of the art models which\ntypically use a much larger global batch size. To this end,\nfor the model+data parallel cases we ﬁx the global batch\nsize to 512 for all experiments which corresponds to 64-way\ndata parallelism.\n5.1.1. M ODEL AND DATA PARALLELISM\nThroughout this section, we will showcase weak scaling\nwith respect to the model parameters for both model parallel\nand model+data parallel cases. Weak scaling is typically\nTable 1.Parameters used for scaling studies. Hidden size per atten-\ntion head is kept constant at 96.\nNumber Number Model Model\nHidden Attention of of parallel +data\nSize heads layers parameters GPUs parallel\n(billions) GPUs\n1536 16 40 1.2 1 64\n1920 20 54 2.5 2 128\n2304 24 64 4.2 4 256\n3072 32 72 8.3 8 512\n100% 95%\n82% 77%\n96%\n83% 79% 74%\n0%\n20%\n40%\n60%\n80%\n100%\n1 2 4 8 … 64 128 256 512\nWeak Scaling\nNumber of GPUS\nModel Parallel Model + Data Parallel\nFigure 5.Model and model + data parallel weak scaling efﬁciency\nas a function of the number of GPUs.\ndone by scaling the batch-size, however, this approach does\nnot address training large models that do not ﬁt on a single\nGPU and it leads to training convergence degradation for\nlarge batch sizes. In contrast, here we use weak scaling to\ntrain larger models that were not possible otherwise. The\nbaseline for all the scaling numbers is the ﬁrst conﬁguration\n(1.2 billion parameters) in Table 1 running on a single GPU.\nThis is a strong baseline as it achieves 39 TeraFLOPS during\nthe overall training process, which is 30% of the theoretical\npeak FLOPS for a single GPU in a DGX-2H server.\nFigure 5 shows scaling values for both model and\nmodel+data parallelism. We observe excellent scaling num-\nbers in both settings. For example, the 8.3 billion parame-\nters case with 8-way (8 GPU) model parallelism achieves\n77% of linear scaling. Model+data parallelism requires fur-\nther communication of gradients and as a result the scaling\nnumbers drop slightly. However, even for the largest conﬁg-\nuration (8.3 billion parameters) running on 512 GPUs, we\nachieve 74% scaling relative to linear scaling of the strong\nsingle GPU baseline conﬁguration (1.2 billion parameters).\nFurther scaling analysis is provided in Appendix D\n5.2. Language Modeling Results Using GPT-2\nTo demonstrate that large language models can further ad-\nvance the state of the art, we consider training GPT-2 models\nof the sizes and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2.Model conﬁgurations used for GPT-2.\nHidden Time\nParameter Layers Hidden Attn Size Total per\nCount Size Heads per GPUs Epoch\nHead (days)\n355M 24 1024 16 64 64 0.86\n2.5B 54 1920 20 96 128 2.27\n8.3B 72 3072 24 128 512 2.10\nTable 3.Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel Wikitext103 LAMBADA\nPerplexity ↓ Accuracy ↑\n355M 19.31 45.18%\n2.5B 12.76 61.73%\n8.3B 10.81 66.51%\nPrevious SOTA 15.79 63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the conﬁgurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the LAMBADA and WikiText103\ndatasets in Table 3. For more details on evaluation method-\nology, see Appendix E. We observe the trend that increasing\nmodel size also leads to lower perplexity on WikiText103\nand higher cloze accuracy on LAMBADA. Our 8.3B model\nachieves state of the art perplexity on the WikiText103 test\nset at a properly adjusted perplexity of 10.81. At 66.51%\naccuracy, the 8.3B model similarly surpasses prior cloze\naccuracy results on the LAMBADA task. We have included\nsamples generated from the 8.3 billion parameters model\nin the Appendix C. Recently researchers from Microsoft in\ncollaboration with NVIDIA trained a 17 billion parameter\nGPT-2 model called Turing-NLG (Microsoft, 2020) using\nMegatron and showed that the accuracies further improve\nas they scale the model, highlighting the value of larger\nmodels.\nTo ensure we do not train on any data found in our test sets,\nwe calculate the percentage of test set 8-grams that also\nappear in our training set as done in previous work (Rad-\nford et al., 2019). The WikiText103 test set has at most\nFigure 6.Validation set perplexity. All language models are trained\nfor 300k iterations. Larger language models converge notice-\nably faster and converge to lower validation perplexities than their\nsmaller counterparts.\nTable 4.Model conﬁgurations used for BERT.\nParameter Layers Hidden Attention Total\nCount Size Heads GPUs\n336M 24 1024 16 128\n1.3B 24 2048 32 256\n3.9B 48 2560 40 512\n10.8% overlap and the LAMBADA test set (Paperno et al.,\n2016) has at most 1.4% overlap. We should note that the\nWikiText103 test set has already 9.09% overlap with the\nWikiText103 training set (Radford et al., 2019). As these\nare consistent with previous work, we are conﬁdent that no\ndocuments from our test data are inadvertently included in\nour training data.\n5.3. Bi-directional Transformer Results Using BERT\nIn this section, we apply our methodology to BERT-style\ntransformer models and study the effect of model scaling\non several downstream tasks. Prior work (Lan et al., 2019)\nfound that increasing model size beyond BERT-large with\n336M parameters results in unexpected model degradation.\nTo address this degradation, the authors of that work (Lan\net al., 2019) introduced parameter sharing and showed that\nthat their models scale much better compared to the original\nBERT model.\nWe further investigated this behaviour and empirically\ndemonstrated that rearranging the order of the layer nor-\nmalization and the residual connections as shown in Figure\n7 is critical to enable the scaling of the BERT-style mod-\nels beyond BERT-Large. The architecture (b) in Figure 7\neliminates instabilities observed using the original BERT\narchitecture in (a) and also has a lower training loss. To\nthe best of our knowledge, we are the ﬁrst to report such a\nchange enables training larger BERT models.\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 5.Development set results for MNLI, QQP, SQuAD 1.1 and SQuAD 2.0 and test set results for RACE. The trained tokens represents\nconsumed tokens during model pretraining (proportional to batch size times number of iterations) normalized by consumed tokens during\nmodel pretraining for our 336M model.\nModel\ntrained tokens MNLI m/mm QQP SQuAD 1.1 SQuAD 2.0 RACE m/h\nratio accuracy accuracy F1 / EM F1 / EM accuracy\n(dev set) (dev set) (dev set) (dev set) (test set)\nRoBERTa (Liu et al., 2019b) 2 90.2 / 90.2 92.2 94.6 / 88.9 89.4 / 86.5 83.2 (86.5 / 81.8)\nALBERT (Lan et al., 2019) 3 90.8 92.2 94.8 / 89.3 90.2 / 87.4 86.5 (89.0 / 85.5)\nXLNet (Yang et al., 2019) 2 90.8 / 90.8 92.3 95.1 / 89.7 90.6 / 87.9 85.4 (88.6 / 84.0)\nMegatron-336M 1 89.7 / 90.0 92.3 94.2 / 88.0 88.1 / 84.8 83.0 (86.9 / 81.5)\nMegatron-1.3B 1 90.9 / 91.0 92.6 94.9 / 89.1 90.2 / 87.1 87.3 (90.4 / 86.1)\nMegatron-3.9B 1 91.4 / 91.4 92.7 95.5 / 90.0 91.2 / 88.5 89.5 (91.8 / 88.6)\nALBERT ensemble (Lan et al., 2019) 95.5 / 90.1 91.4 / 88.9 89.4 (91.2 / 88.6)\nMegatron-3.9B ensemble 95.8 / 90.5 91.7 / 89.0 90.9 (93.1 / 90.0)\nFigure 7.Training loss for BERT model using the original architec-\nture (a) and the rearranged architecture (b). Left ﬁgure shows the\ntraining loss for 336M and 752M BERT model. While the original\narchitecture performs well on the 336M model, the modiﬁcations\nin (b) enable stable training with lower training loss.\nUsing the architecture change in Figure 7(b), we consider\nthree different cases as detailed in Table 4. The 336M model\nhas the same size as BERT-large. The 1.3B is the same as\nthe BERT-xlarge conﬁguration that was previously shown\nto get worse results than the 336M BERT-large model (Lan\net al., 2019). We further scale the BERT model using both\nlarger hidden size as well as more layers to arrive at the 3.9B\nparameter case. In all cases, the hidden size per attention\nhead is kept constant at 64. 336M and 1.3B models are\ntrained for 2 million iterations while the 3.9B model is\ntrained for 1.5 million iterations and is still training.\nOn a 3% held-out set, 336M, 1.3B, and 3.9B models achieve\nvalidation set perplexity of 1.58, 1.30, and 1.16, respectively,\na monotonic decrease with the model size. We ﬁnetune\nthe trained models on several downstream tasks including\nMNLI and QQP from the GLUE benchmark (Wang et al.,\n2019), SQuAD 1.1 and SQuAD 2.0 from the Stanford Ques-\ntion answering dataset (Rajpurkar et al., 2016; 2018), and\nthe reading comprehension RACE dataset (Lai et al., 2017).\nFor ﬁnetuning, we follow the same procedure as (Liu et al.,\n2019b). We ﬁrst perform hyperparameter tuning on batch\nsize and learning rate. Once we obtain the best values, we\nreport the median development set results over 5 different\nrandom seeds for initialization. The hyperparameters used\nfor each model and task are provided in the Appendix A.\nTable 5 shows the development set results for MNLI, QQP,\nSQuAD 1.1, and SQuAD 2.0 and test set results for RACE.\nFor the test set results of RACE, we ﬁrst use the develop-\nment set to ﬁnd the checkpoint that gives us the median\nscore on the 5 random seeds and we report the results from\nthat checkpoint on the test set. We also report 5-way ensem-\nble results for the development set of SQuAD and test set\nof RACE. From Table 5 we observe that (a) as the model\nsize increases, the downstream task performance improves\nin all cases, (b) our 3.9B model establishes state of the art\nresults on the development set compared to other BERT\nbased models, and (c) our 3.9B model achieves both single\nmodel as well as ensembled SOTA results on RACE test set.\n6. Conclusion and Future Work\nIn this work, we successfully surpassed the limitations posed\nby traditional single-GPU-per-model training by implement-\ning model parallelism with only a few modiﬁcations to\nthe existing PyTorch transformer implementations. We ef-\nﬁciently trained transformer based models up to 8.3 bil-\nlion parameter on 512 NVIDIA V100 GPUs with 8-way\nmodel parallelism and achieved up to 15.1 PetaFLOPs sus-\ntained over the entire application. We also showed that for\nBERT models, careful attention to the placement of layer\nnormalization in BERT-like models is critical to achieving\nincreased accuracies as the model size increases. We study\nthe effect of model size on down-stream task accuracy and\nachieve far superior results on downstream tasks and estab-\nlish new SOTA for WikiText103, LAMBADA, and RACE\ndatasets. Finally, we open sourced our code to enable future\nwork leveraging model parallel transformers.\nThere are several directions for future work. Continuing\nto increase the scale of pretraining is a promising line of\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\ninvestigation that will further test existing deep learning\nhardware and software. To realize this, improvements in\nthe efﬁciency and memory footprint of optimizers will be\nneeded. In addition, training a model with more than 16\nbillion parameters will demand more memory than is avail-\nable within 16 GPUs of a DGX-2H box. For such models, a\nhybrid intra-layer and inter-layer model parallelism along\nwith inter-node model parallelism would be more suitable.\nThree other directions of investigation include (a) pretrain-\ning different model families (XLNet, T5), (b) evaluating per-\nformance of large models across more difﬁcult and diverse\ndownstream tasks (e.g. Generative Question Answering,\nSummarization, and Conversation), and (c) using knowl-\nedge distillation to train small student models from these\nlarge pretrained teacher models.\nReferences\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,\nCitro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\nGhemawat, S., Goodfellow, I., Harp, A., Irving, G., Is-\nard, M., Jia, Y ., Jozefowicz, R., Kaiser, L., Kudlur, M.,\nLevenberg, J., Man ´e, D., Monga, R., Moore, S., Mur-\nray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B.,\nSutskever, I., Talwar, K., Tucker, P., Vanhoucke, V ., Va-\nsudevan, V ., Vi´egas, F., Vinyals, O., Warden, P., Watten-\nberg, M., Wicke, M., Yu, Y ., and Zheng, X. TensorFlow:\nLarge-scale machine learning on heterogeneous systems,\n2015. URL http://tensorflow.org/. Software\navailable from tensorﬂow.org.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layernorm. CoRR,\nabs/1607.06450, 2016. URL http://arxiv .org/\nabs/1607.06450.\nChen, C.-C., Yang, C.-L., and Cheng, H.-Y . Efﬁcient and\nrobust parallel dnn training through model parallelism on\nmulti-gpu platform. arXiv:1809.02839, 2018.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Train-\ning deep nets with sublinear memory cost. CoRR,\nabs/1604.06174, 2016. URL http://arxiv .org/\nabs/1604.06174.\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J. G., Le, Q. V .,\nand Salakhutdinov, R. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. CoRR,\nabs/1901.02860, 2019. URL http://arxiv .org/\nabs/1901.02860.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding, 2018.\nGoyal, P., Doll ´ar, P., Girshick, R. B., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and\nHe, K. Accurate, large minibatch SGD: training imagenet\nin 1 hour. CoRR, abs/1706.02677, 2017.\nHarlap, A., Narayanan, D., Phanishayee, A., Se-\nshadri, V ., Devanur, N., Ganger, G., and Gibbons, P.\nPipedream: Fast and efﬁcient pipeline parallel dnn train-\ning. arXiv:1806.03377, 2018.\nHendrycks, D. and Gimpel, K. Bridging nonlinearities\nand stochastic regularizers with gaussian error linear\nunits. CoRR, abs/1606.08415, 2016. URL http:\n//arxiv.org/abs/1606.08415.\nHoward, J. and Ruder, S. Fine-tuned language models for\ntext classiﬁcation. CoRR, abs/1801.06146, 2018.\nHuang, Y ., Cheng, Y ., Chen, D., Lee, H., Ngiam, J., Le,\nQ. V ., and Chen, Z. Gpipe: Efﬁcient training of gi-\nant neural networks using pipeline parallelism. CoRR,\nabs/1811.06965, 2018. URL http://arxiv .org/\nabs/1811.06965.\nJia, Z., Zaharia, M., and Aiken, A. Beyond data and model\nparallelism for deep neural networks. arXiv:1807.05358,\n2018.\nJoshi, M., Chen, D., Liu, Y ., Weld, D. S., Zettlemoyer,\nL., and Levy, O. Spanbert: Improving pre-training by\nrepresenting and predicting spans. arXiv:1907.10529,\n2019.\nKeskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy,\nM., and Tang, P. T. P. On large- batch training for deep\nlearning: Generalization gap and sharp minima. ICLR,\n2017.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and\nLewis, M. Generalization through memorization: Nearest\nneighbor language models. arXiv:1911.00172, 2019.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nLai, G., Xie, Q., Liu, H., Yang, Y ., and Hovy, E. Race:\nLarge-scale reading comprehension dataset from exami-\nnations. arXiv:1704.04683, 2017.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., and Soricut, P.\nS. R. Albert: A lite bert for self-supervised learning of\nlanguage representations. arXiv:1909.11942, 2019.\nLi, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed,\nA., Josifovski, V ., Long, J., Shekita, E. J., and Su, B.-Y .\nScaling distributed machine learning with the parameter\nserver, 2014.\nLiu, X., He, P., Chen, W., and Gao, J. Multi-task deep neu-\nral networks for natural language understanding. CoRR,\nabs/1901.11504, 2019a. URL http://arxiv.org/\nabs/1901.11504.\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy,\nO., Lewis, M., Zettlemoyer, L., and Stoyanov, V . Roberta:\nA robustly optimized BERT pretraining approach. CoRR,\nabs/1907.11692, 2019b. URL http://arxiv.org/\nabs/1907.11692.\nLoshchilov, I. and Hutter, F. Decoupled weight de-\ncay regularization. In International Conference on\nLearning Representations, 2019. URL https://\nopenreview.net/forum?id=Bkg6RiCqY7.\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R.\nLearned in translation: Contextualized word vectors.\nCoRR, abs/1708.00107, 2017.\nMelamud, O., Goldberger, J., and Dagan, I. context2vec:\nLearning generic context embedding with bidirectional\nlstm. In Proceedings of The 20th SIGNLL Conference on\nComputational Natural Language Learning, pp. 51–61,\n01 2016.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nsentinel mixture models. CoRR, abs/1609.07843, 2016.\nURL http://arxiv.org/abs/1609.07843.\nMicikevicius, P., Narang, S., Alben, J., Diamos, G. F., Elsen,\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\nVenkatesh, G., and Wu, H. Mixed precision training.\nCoRR, abs/1710.03740, 2017.\nMicrosoft. Turing-nlg: A 17-billion-parameter lan-\nguage model by microsoft, 2020. URL https://\nwww.microsoft.com/en-us/research/blog/\nturing - nlg - a - 17 - billion - parameter -\nlanguage-model-by-microsoft/ .\nMikolov, T., Deoras, A., Kombrink, S., Burget, L., and\nˇCernock`y, J. Empirical evaluation and combination of ad-\nvanced language modeling techniques. In Twelfth Annual\nConference of the International Speech Communication\nAssociation, 2011.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean,\nJ. Distributed representations of words and phrases and\ntheir compositionality. CoRR, abs/1310.4546, 2013.\nNVIDIA. Mixed precision training: Choosing a scaling\nfactor, 2018. URL https://docs .nvidia.com/\ndeeplearning / sdk / mixed - precision -\ntraining/index.html#scalefactor.\nPaperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N.,\nBernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and\nFern´andez, R. The LAMBADA dataset: Word pre-\ndiction requiring a broad discourse context. CoRR,\nabs/1606.06031, 2016. URL http://arxiv .org/\nabs/1606.06031.\nPennington, J., Socher, R., and Manning, C. D. Glove:\nGlobal vectors for word representation, 2014. URL\nhttps://www .aclweb.org/anthology/D14-\n1162.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. CoRR, abs/1802.05365, 2018. URL\nhttp://arxiv.org/abs/1802.05365.\nRadford, A., J ´ozefowicz, R., and Sutskever, I. Learning\nto generate reviews and discovering sentiment. CoRR,\nabs/1704.01444, 2017.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI. Improving language understanding by generative pre-\ntraining, 2018. URL https://blog.openai.com/\nlanguage-unsupervised/.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Better language models and their impli-\ncations, 2019. URL https://openai.com/blog/\nbetter-language-models/.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv:1910.10683, 2019.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad:\n100,000+ questions for machine comprehension of text.\nEMNLP, 2016.\nRajpurkar, P., Jia, R., and Liang, P. Know what you dont\nknow: Unanswerable questions for squad. ACL, 2018.\nRamachandran, P., Liu, P. J., and Le, Q. V . Unsupervised\npretraining for sequence to sequence learning. CoRR,\nabs/1611.02683, 2016. URL http://arxiv .org/\nabs/1611.02683.\nShazeer, N., Cheng, Y ., Parmar, N., Tran, D., Vaswani, A.,\nKoanantakool, P., Hawkins, P., Lee, H., Hong, M., Young,\nC., Sepassi, R., and Hechtman, B. Mesh-TensorFlow:\nDeep learning for supercomputers. In Neural Information\nProcessing Systems, 2018.\nTrinh, T. H. and Le, Q. V . A simple method for common-\nsense reasoning. CoRR, abs/1806.02847, 2018. URL\nhttp://arxiv.org/abs/1806.02847.\nTurian, J., Ratinov, L., and Bengio, Y . Word representations:\nA simple and general method for semi-supervised learn-\ning. In Proceedings of the 48th Annual Meeting of the\nAssociation for Computational Linguistics, ACL ’10, pp.\n384–394, Stroudsburg, PA, USA, 2010. Association for\nComputational Linguistics.\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nValiant, L. G. A bridging model for parallel computation.\nCommunications of the ACM, 33(8):103-111, 1990.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. CoRR, abs/1706.03762, 2017.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. Glue: A multi-task benchmark and analy-\nsis platform for natural language understanding. ICLR,\n2019.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J. G., Salakhut-\ndinov, R., and Le, Q. V . Xlnet: Generalized autore-\ngressive pretraining for language understanding. CoRR,\nabs/1906.08237, 2019. URL http://arxiv .org/\nabs/1906.08237.\nYou, Y ., Gitman, I., and Ginsburg, B. Large batch training\nof convolutional networks. arXiv:1708.03888, 2017.\nYou, Y ., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojana-\npalli, S., Song, X., Demmel, J., and Hsieh, C.-J. Large\nbatch optimization for deep learning: Training bert in 76\nminutes. arXiv:1904.00962, 2019.\nZellers, R., Holtzman, A., Rashkin, H., Bisk, Y ., Farhadi,\nA., Roesner, F., and Choi, Y . Defending against neural\nfake news. CoRR, abs/1905.12616, 2019. URL http:\n//arxiv.org/abs/1905.12616.\nZhu, Y ., Kiros, R., Zemel, R. S., Salakhutdinov, R., Urta-\nsun, R., Torralba, A., and Fidler, S. Aligning books and\nmovies: Towards story-like visual explanations by watch-\ning movies and reading books. CoRR, abs/1506.06724,\n2015.\nA. BERT Finetuning Hyperparameters\nTable 6 presents the hyperparameters used for each model\nand task during ﬁnetuning.\nB. Model Parallel Supplementary Material\nIn this section, we present further details about the hybrid\nmodel and data parallelism and handling random number\ngeneration.\nB.1. Hybrid Model and Data Parallelism\nModel parallelism is orthogonal to data parallelism, and so\nwe can use both simultaneously to train large models in a\nreasonable amount of time. Figure 8 shows a grouping of\nGPUs for hybrid model and data parallelism. Two or more\nGPUs within the same server form model parallel groups\n(for example GPUs 1 to 8 in Figure 8), and contain one\nTable 6.Hyperparameters for ﬁnetuning BERT model on down-\nstream tasks.\nTask Model Batch Learning Training\nsize rate epochs\n336M\nMNLI 1.3B 128 1e-5 10\n3.8B\n336M 128 5e-5\nQQP 1.3B 128 3e-5 12\n3.8B 256 4e-5\n336M 64 3e-5\nSQUAD 1.1 1.3B 48 3e-5 2\n3.8B 48 1e-5\n336M 48 3e-5\nSQUAD 2.0 1.3B 64 3e-5 2\n3.8B 48 1e-5\n336M 32 2e-5\nRACE 1.3B 16 1e-5 3\n3.8B 32 2e-5\ninstance of the model distributed across these GPUs. The\nremaining GPUs, which could be within the same server but\nmore typically are located in other servers, run additional\nmodel parallel groups. GPUs with the same position in each\nof the model parallel groups (for example GPUs 1, 9, ...,\n505 in Figure 8) form data parallel groups so that all GPUs\nwithin a data parallel group hold the same model param-\neters. During back propagation we run multiple gradient\nall-reduce operations in parallel to reduce weight gradients\nwithin each distinct data parallel group. The total number\nof required GPUs is the product of the number of model\nand data parallel groups. For example, for the 8.3 billion\nparameter model we use 8 GPUs per model parallel group\nand 64-way data parallelism, for a total of 512 GPUs. All\ncommunication is implemented in PyTorch by Python calls\nto NCCL. GPUs within each model parallel group perform\nall-reduces amongst all GPUs within the group. For data\nparallelism, each of the all-reduce operations takes place\nwith one of the GPUs from each model parallel group.\nB.2. Model Parallel Random Number Generation\nTechniques that utilize random number generation, such\nas dropout, are a staple of modern deep learning training.\nTransformers have dropout layers outside the model parallel\nregions before residual connections and within model paral-\nlel regions in the self attention block. Because some dropout\nlayers are in a model parallel region, while others are not,\nwe need to treat random number generation carefully to\nensure dropout works correctly. To synchronize residual\nconnection dropout across model parallel workers we seed\nthe random number generators at the beginning of training\nwith the same seed. This results in identical dropout patterns\nacross all model parallel workers. However, dropout within\na model parallel region should result in different random\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\npatterns for each worker to achieve randomness across the\nentire operation. To achieve this we maintain a separate\nrandom number generator for dropout within model parallel\nregions. This random number generator is uniquely seeded\nfor each model parallel worker.\nFigure 8.Grouping of GPUs for hybrid model and data parallelism\nwith 8-way model parallel and 64-way data parallel.\nC. Text Samples\nBelow are some text samples generated by Megatron-LM\nusing a context prompt. Some of the texts are cut short.\nContext: There is a major force uniting Americas ﬁercely\npartisan politicians: big technology companies. Democrats\nand Republicans at the federal and state levels are coming\ntogether to scrutinize the power of the Silicon Valley giants\nand, potentially, to rein them in. Letitia James, the Demo-\ncratic attorney general of New York, announced on Friday\nthat attorneys general in eight states four Democrats and\nfour Republicans and the District of Columbia had begun\nan antitrust investigation of Facebook.\nMegatron-LM: The probe, which Ms. James said was\nlikely to result in a lawsuit, is the latest in a string of high-\nproﬁle regulatory actions, in state and federal courts, that\nchallenge the market dominance of tech companies. The\nJustice Department is also investigating Facebook’s data\npractices and whether the company violated a 2011 consent\ndecree with the Federal Trade Commission that required\nthe company to obtain explicit consent from users before\nsharing their personal information with third parties.\nWhat might prompt a bipartisan coalition to sue Facebook?\nIt isn’t clear that there’s any one thing. But lawmakers and\nregulators have been increasingly alarmed by the company’s\nrecent controversies, which include a data breach affecting\nmore than 50 million users, the spread of fake news on\nthe platform, and the spreading of propaganda by Russian-\nbacked accounts during the 2016 election.\nIt’s also not clear how serious the Democratic attorneys gen-\neral are about ﬁling a lawsuit. While the attorneys general\nannounced their investigation in a press release, they said\nthey had not yet reached a decision on whether to sue. In\nthe past, some of the attorneys general have focused their\nefforts on trying to extract settlements from the tech compa-\nnies without ﬁling lawsuits.\n”As attorneys general who have investigated and prosecuted\nhealth-care fraud, price-ﬁxing, and other violations of our\nstates’ antitrust laws, we are committed to holding compa-\nnies accountable when they fail to protect consumers and\ncompetition in the market,” Ms. James said in a statement.\nThe investigation, which was ﬁrst reported by The New\nYork Times, was announced as the companies that dom-\ninate the tech industry are facing scrutiny from antitrust\nregulators around the world. Google, Twitter, Facebook\nand Amazon have faced a series of antitrust probes and\nclass-action lawsuits in the United States and Europe ...\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nContext: A ﬂower, sometimes known as a bloom or\nblossom, is the reproductive structure found in ﬂowering\nplants (plants of the division Magnoliophyta, also called\nangiosperms). The biological function of a ﬂower is to ef-\nfect reproduction, usually by providing a mechanism for the\nunion of sperm with eggs.\nMegatron-LM: The structural components of the ﬂower\nare the petals and sepals, which are the reproductive organs,\nthe hypanthium, which forms the base of the ﬂower, and\nthe calyx, which is the stigma. A ﬂower is a composite\nstructure, made up of sepals and petals, the sepals being\ncomposed of three fused tepals and the petals of two fused\npetals, though there are a few exceptions to this rule. The\nthree sepals are fused to form a tube called the calyx, which\nis the structure that attracts the pollinating insects. The\npetals are composed of two fused sepals (the outer ones)\nwhich open and fold inward (clasp) at the tips, forming\nthe petals. The four sepals and petals are united to form\na corolla, the outermost structure of a ﬂower that carries\npollen.\nThe structure of the ﬂower has important functional signif-\nicance, including protection of the ovary from insects and\nthe transfer of pollen between the male and female parts\nof a ﬂower. The structure of the ﬂower is also used by the\nplants to attract pollinators, for example by the shape of the\npetals and the colours of the stamens.\nThe structure of a ﬂower varies widely between plants. The\nsimplest ﬂowers are unbranched and made up of only two\nsepals and a single petal. Other examples are composed\nof several petals joined together, with the innermost sepals\ncompletely fused to form a tube called the calyx. The petals\nare usually coloured to attract insects, which can only see\ncertain wavelengths of light. The sepals are often coloured\ntoo, but usually have a more limited range of colours.\nSome ﬂowers have additional structures, such as stamens,\nanthers and ovaries. The presence of stamens and anthers\nin some ﬂowers is used for cross-pollination. The ovary is\nthe organ where the female gametes are produced, and is\nthe ﬁrst structure that is developed in an embryo of a ﬂower.\nThe ovary is the female ﬂower’s reproductive system, and\nconsists of the ovary wall, the ovule, and the oviduct.\nFlowers vary in their size and shape, from tiny to huge, and\nare often brightly coloured. Flowers may be seen as part of\na larger structure, such as a ﬂower head, an inﬂorescence, a\nbract, a spathe, or a carpels.\nThe term ”ﬂower” is usually applied to the ﬂoral structures\nthat are visible externally, but many ...\nContext: OpenAI did not release the full GPT-2 model\ndue to concerns of malicious use, but they did release a\nsmaller version equivalent in size to the original GPT (117\nM parameters), trained on the new, larger dataset. Although\nnot as powerful as the large model, the smaller version still\nhas some language generation chops.\nMegatron-LM: In December 2016, DeepMind released an\neven larger model, GPT-3, with more than 1.5 billion pa-\nrameters. DeepMind has no plans to release the model, so\nthe question remains whether other AI researchers will be\nable to copy and build on it.\nOther language models have been built using other tech-\nniques. In 2016, a team of researchers at Columbia Uni-\nversity, the University of Toronto, and Google DeepMind\nshowed that neural network-based models, including deep\nneural networks, can learn the grammatical rules of a lan-\nguage. A group at Google DeepMind demonstrated that\nthese rules could be represented in a network as a kind of\ndecision tree, where the nodes in the tree are the words in\nthe sentence, and the rules for the words are represented\nby the decision nodes at the end of the tree. The network\nwould go through the network in order to choose the next\nword in the sentence based on whether it follows the rules\nor not. In 2017, the team at DeepMind released a paper\ndetailing the training and performance of their model.\nThe problems with the model are many. One of the biggest\nissues is that grammatical rules are ambiguous. For exam-\nple, the phrase ”I came down” could mean I came down\nfrom the stairs, or I came down from the sky. In English,\nthere are numerous other examples of sentences that are\nambiguous. The model does not have any way to tell the dif-\nference between the ambiguous sentences, and so it simply\nmakes a prediction about the entire sentence based on what\nrules it does know. This makes the model vulnerable to the\nphenomenon of overﬁtting. In order to avoid this, models\nmust be trained using a large set of data, so that the model\ncan ﬁnd useful patterns in the data, and use those patterns\nto help make predictions. This approach is also prone to\nerrors, as it relies on the training data to help make correct\npredictions, and the data might be incorrectly labeled, re-\nsulting in a model that produces a poor set of predictions.\nIn the case of GPT-2, the model only has about 100 training\nsentences for each type of phrase (subject-verb-object).\nAdditionally, languages have different rules for different\ntypes of phrasing. In English, the subject-verb-object con-\nstruction is fairly universal, but other languages have differ-\nent rules, which makes training a GPT more complicated.\nFinally, there is an inherent trade-off between the number\nof rules and the performance of the model ...\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nContext: The 36th International Conference on Machine\nLearning (ICML 2019) will be held in Long Beach, CA,\nUSA from June 10th to June 15th, 2019. The conference\nwill consist of one day of tutorials (June 10), followed\nby three days of main conference sessions (June 11-13),\nfollowed by two days of workshops (June 14-15).\nMegatron-LM: With a broad scope, the conference ad-\ndresses the challenges and opportunities in machine learning\nfor practitioners and researchers. The conference program\nwill feature sessions that cover different machine learning\ntopics, from reinforcement learning to computational biol-\nogy and from AI to deep learning. It is the world’s premier\nforum for exploring and presenting advances in machine\nlearning.\nICML 2019 will offer an impressive range of sessions,\nwhich will be of interest to practitioners, researchers, and\nstudents. Some of the topics we will cover include:\nReinforcement learning\nSoft Computing\nComputational biology\nConvolutional networks\nDeep learning\nStatistical learning theory\nStatistical learning machine\nNatural language processing\nBiology\nAwards\nSince its ﬁrst edition, the conference has grown and evolved.\nToday, it is the premier forum for showcasing state-of-the-\nart research in machine learning. The conference is widely\nrecognized as the preeminent venue for showcasing the\nlatest research in machine learning, and a place where prac-\ntitioners, researchers, and students can exchange ideas and\ninsights.\nThe conference is a peer-reviewed venue that draws its pres-\ntige and inﬂuence from the excellence of the presentations\nand the quality of the invited speakers. With over 50% of all\ntalks selected based on their content by an expert committee,\nit is clear that the conference’s high standards of research\nand quality keep a very high bar.\nContext: Rescue workers on Tuesday found a ﬂight\nrecorder from the Russian plane that crashed into the Black\nSea over the weekend, the defense ministry said. All 92\npeople aboard the Russian military’s Tu-154 plane are be-\nlieved to have died Sunday morning when it crashed two\nminutes after taking off from the southern Russian city of\nSochi.\nMegatron-LM: A child holds up a portrait of a dead relative\nat a makeshift morgue in a building belonging to a local\ngovernment in the town of Petropavlovsk-Kamchatsky. A\nRussian Defense Ministry Tu-154 plane with 92 people\non board crashed into the Black Sea on Sunday morning\nshortly after taking off from the southern Russian city of\nSochi. (Yuri Kochetkov/EPA)\nAccording to the ministry, the plane was found at a depth\nof more than 1,000 feet and the recorder was damaged.\n”The defense ministry has decided to hand over the recorder\nto the Interstate Aviation Committee, which is the main\ninvestigative body in the air crash,” the statement said.\nThe search for the plane’s black boxes is being hampered by\nstrong winds and rough seas. Investigators have launched a\ncriminal probe into the crash, which ofﬁcials said was likely\ncaused by a technical fault.\nRussian President Vladimir Putin declared Wednesday a\nday of mourning for the victims.\nD. Further Scaling Analysis\nIn this section we study the effect of number of attention\nheads on the scaling results. We also present strong scaling\nresults for our 1.2 billion parameter model.\nD.1. Attention Heads and Scaling\nThis section studies the effect of attention heads on model\nparallel scaling. To this end, we consider the 8.3 billion\nparameter conﬁguration with 8-way model parallelism and\nvary the number of heads from 16 to 32. The results are\npresented in Table 7. As the number of attention heads\nincreases, some of the GEMMS inside the self-attention\nlayer become smaller and also the number of elements in\nthe self attention softmax increases. This results in a slight\ndecrease in scaling efﬁciency. Future research should be\nwary of this hyperparameter to design large transformer\nmodels that balance model speed and model accuracy.\nD.2. Strong Scaling\nOur model parallelism is primarily designed to enable train-\ning models larger than what can ﬁt in the memory of a\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 7.Effect of number of attention heads on scaling on 8.3\nbillion of parameters with 8-way model parallelism.\nAttention heads Hidden size per head Scaling Efﬁciency\n16 192 82%\n24 128 80%\n32 96 77%\nTable 8.Speedup obtained for the 1.2 billion parameters model\nusing model parallelism while keeping the batch size constant.\n# of GPUs 1 2 4 8\nSpeedup 1.0 1.64 2.34 2.98\nsingle GPU, but it can also accelerate the training of smaller\nmodels without increasing the batch size. To measure this\nacceleration we train a model with a ﬁxed 1.2 billion parame-\nters. We use a ﬁxed batch size of 8 samples per iteration and\nincrease the number of GPUs using model parallelism. The\nresults are listed in Table 8. Using two GPUs makes training\n64% faster. Above that we see diminishing returns as the\nper-GPU computation decreases and the memory bandwidth\nand communication overheads begin to dominate.\nE. Evaluating Language Models Using\nWikiText103 and LAMBADA\nIn this section we detail our evaluation methodology for the\nWikiText103 dataset (Merity et al., 2016) and cloze-style\nprediction accuracy on the LAMBADA dataset(Paperno\net al., 2016).\nE.1. Wikitext103 Perplexity\nWikiText103 perplexity is an evaluation criterion that has\nbeen well studied over the past few years since the creation\nof the benchmark dataset. Perplexity is the exponentiation\nof the average cross entropy of a corpus (Mikolov et al.,\n2011). This makes it a natural evaluation metric for lan-\nguage models which represent a probability distribution\nover entire sentences or texts.\nPPL = exp(−1\nTo\nT∑\nt\nlogP(t|0 :t−1)) (4)\nTo calculate perplexity in (4) we tokenize the WikiText103\ntest corpus according to our subword vocabulary and sum\nthe cross entropy loss from each token [0,T]. We then nor-\nmalize the cross entropy loss by the number of tokens in the\noriginal tokenization scheme To. The WikiText103 test cor-\npus already comes pre-tokenized with word level tokens that\nprior works have used to compute perplexity. To evaluate\nour models’ perplexities on a level playing ﬁeld with prior\nworks we must normalize by the original number of tokens,\nTo, rather than the number of tokens, T, actually in the tok-\nenized data fed as input to our model. This pre-tokenization\nalso introduces artifacts in the text that are not present in our\ntraining data. To alleviate this distributional mismatch, we\nﬁrst preprocess the WikiText103 test dataset with invertible\ndetokenizers to remove various artifacts related to punctua-\ntion and whitespace. The value of To is calculated before\nthis preprocessing. For WikiText103’s test setTo = 245566\nand T = 270329.\nWe must also make one further transformer-speciﬁc mod-\niﬁcation to the perplexity calculation. Unlike RNN-based\nlanguage models, transformers operate on a ﬁxed window in-\nput size. Therefore they cannot fully calculate P(t|0 :t−1)\nand can only calculate P(t|t−w : t−1) where wis the\nsize of our context: 1024 tokens. However, calculating this\nvalue for every token in our dataset is prohibitively expen-\nsive since we must compute approximately T evaluations\nof a wsized context. To evaluate our models efﬁciently we\ntake a middle ground approach termed overlapping evalu-\nation where we advance the sliding window by some over-\nlap oeach time and only compute the cross entropy losses\ncorresponding to the last otokens of the window. In our\nexperiments we utilize an overlap o of 32, and compute\nlosses over all sliding windows in such a fashion.\nE.2. LAMBADA Cloze Accuracy\nThe capability to handle long term contexts is crucial for\nstate of the art language models and is a necessary prerequi-\nsite for problems like long-form generation and document-\nbased question answering. Cloze-style datasets like LAM-\nBADA are designed to measure a model’s ability to operate\nin and reason about these types of long term contexts. Cloze-\nstyle reading comprehension uses a context of word tokens\nx= x1:t with one token xj masked; the models objective\nis to correctly predict the value of the missing jth token. To\naccurately predict the missing token, the model requires an\nin-depth understanding of the surrounding context and how\nlanguage should be used in such a context. LAMBADA\nuses cloze-style reading comprehension to test generative\nleft-to-right language models by constructing examples of 4-\n5 sentences where the last word in the context xt is masked.\nOur models utilize subword units, so for LAMBADA evalu-\nation we utilize the raw, unprocessed LAMBADA dataset\nand require that our model predict the multiple subword\ntokens that make up the word token. We use teacher forc-\ning, and consider an answer correct only when all output\npredictions are correct. This formulation is equivalent to the\noriginal task of word token prediction.",
  "topic": "Parallelism (grammar)",
  "concepts": [
    {
      "name": "Parallelism (grammar)",
      "score": 0.8294600248336792
    },
    {
      "name": "Training (meteorology)",
      "score": 0.6174384951591492
    },
    {
      "name": "Computer science",
      "score": 0.6088832020759583
    },
    {
      "name": "Parallel computing",
      "score": 0.47513487935066223
    },
    {
      "name": "Data parallelism",
      "score": 0.47500336170196533
    },
    {
      "name": "Geography",
      "score": 0.077382892370224
    },
    {
      "name": "Meteorology",
      "score": 0.052408576011657715
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1304085615",
      "name": "Nvidia (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 815
}