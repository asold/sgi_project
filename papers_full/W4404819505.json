{
    "title": "Comparative Analysis of Prompt Strategies for Large Language Models: Single-Task vs. Multitask Prompts",
    "url": "https://openalex.org/W4404819505",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4381750777",
            "name": "Manuel Gozzi",
            "affiliations": [
                "Marconi University"
            ]
        },
        {
            "id": "https://openalex.org/A2004238289",
            "name": "Federico Di Maio",
            "affiliations": [
                "Marconi University"
            ]
        },
        {
            "id": "https://openalex.org/A4381750777",
            "name": "Manuel Gozzi",
            "affiliations": [
                "Marconi University"
            ]
        },
        {
            "id": "https://openalex.org/A2004238289",
            "name": "Federico Di Maio",
            "affiliations": [
                "Marconi University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6838865847",
        "https://openalex.org/W4387074826",
        "https://openalex.org/W4394567450",
        "https://openalex.org/W4404840084",
        "https://openalex.org/W4391129153",
        "https://openalex.org/W4367309809",
        "https://openalex.org/W3026908503",
        "https://openalex.org/W4403503813",
        "https://openalex.org/W4399362671",
        "https://openalex.org/W6893640197",
        "https://openalex.org/W4385571157"
    ],
    "abstract": "This study investigates the effectiveness of prompt engineering strategies for Large Language Models (LLMs), comparing single-task and multitasking prompts. Specifically, we analyze whether a single prompt handling multiple tasks—such as named entity recognition (NER), sentiment analysis, and JSON output formatting—can achieve performance comparable to dedicated single-task prompts. To substantiate our findings, we employ statistical analyses, including paired Wilcoxon tests, McNemar tests, and Friedman tests, to validate claims of performance similarity or superiority. Experiments were conducted using five open-weight LLMs: LLama3.1 8B, Qwen2 7B, Mistral 7B, Phi3 Medium, and Gemma2 9B. The results indicate that there is no definitive rule favoring single-task prompts over multitask prompts; rather, their relative performance is highly contingent on the specific model’s data and architecture. This study highlights the nuanced interplay between prompt strategies and LLM characteristics, offering insights into optimizing their use for specific NLP tasks. Limitations and future directions, such as expanding task types, are also discussed.",
    "full_text": null
}