{
  "title": "Leveraging Fine-Tuned Large Language Models in Bioinformatics: A Research Perspective",
  "url": "https://openalex.org/W4384407059",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2696668551",
      "name": "Usama Shahid",
      "affiliations": [
        "University of Gloucestershire"
      ]
    },
    {
      "id": "https://openalex.org/A2696668551",
      "name": "Usama Shahid",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4310419543",
    "https://openalex.org/W4382703364",
    "https://openalex.org/W4362598952",
    "https://openalex.org/W4384407059"
  ],
  "abstract": "Bioinformatics synergizes biology, computer science, and statistics and is further propelled by the integration of deep learning and natural language processing (NLP). This analysis extensively explores the applications of fine-tuned language models within bioinformatics, providing empirical evidence and unique perspectives on the impact, challenges, and limitations in this field. The broad scope includes biomedical literature analysis, drug discovery, clinical decision support, protein structure prediction, and pharmacovigilance, among others. This analysis underscores the need to overcome hurdles such as data availability, domain-specific knowledge, bias, interpretability, resource efficiency, ethical implications, and validation for a reliable application of these models. Collaborative efforts between computational and experimental biologists, ethicists, and regulatory bodies are vital to establish ethical guidelines and best practices for their use.",
  "full_text": "Open Peer Review on Qeios\nLeveraging Fine-Tuned Large Language Models in\nBioinformatics: A Research Perspective\nUsama Shahid\n1\n1\n University of Gloucestershire\nFunding:\n No specific funding was received for this work.\nPotential competing interests:\n No potential competing interests to declare.\nAbstract\nBioinformatics synergizes biology, computer science, and statistics and is further propelled by the integration of deep\nlearning and natural language processing (NLP). This analysis extensively explores the applications of fine-tuned\nlanguage models within bioinformatics, providing empirical evidence and unique perspectives on the impact,\nchallenges, and limitations in this field. The broad scope includes biomedical literature analysis, drug discovery, clinical\ndecision support, protein structure prediction, and pharmacovigilance, among others. This analysis underscores the\nneed to overcome hurdles such as data availability, domain-specific knowledge, bias, interpretability, resource\nefficiency, ethical implications, and validation for a reliable application of these models. Collaborative efforts between\ncomputational and experimental biologists, ethicists, and regulatory bodies are vital to establish ethical guidelines and\nbest practices for their use.\nUsama Shahid\nUniversity of Gloucestershire\nComputing Department, Cheltenham\nThe Park, Cheltenham, UoG, GL50 2RH\nusamashahid.us8@gmail.com\n \nKeywords:\n Bioinformatics, Large Language Models, Natural Language Processing, Fine-tuned Models, Biomedical\nLiterature Analysis, Drug Discovery, Clinical Decision Support, Protein Structure Prediction, Pharmacovigilance.\n \nBackground\nLarge language models (LLMs) have been instrumental in various domains, including finance, programming, medicine,\nand consensus building. In finance, the development of open-source financial LLMs, such as FinGPT, has democratized\nQeios, CC-BY 4.0   ·   Article, \nJuly 15, 2023\nQeios ID: WE7UMN.2   ·   https://doi.org/10.32388/WE7UMN.2\n1\n/\n5\naccess to high-quality financial data, enabling applications like robo-advising, algorithmic trading, and low-code\ndevelopment (Yang1, Liu et al. 2023). The success of fine-tuning LLMs for answering programming questions with code\nsnippets highlights their potential in assisting with specific tasks in software development (Lomshakov, Kovalchuk et al.\n2023). In the medical domain, efforts like DoctorGLM aim to address the limitations of LLMs by collecting medical\ndialogues in Chinese and fine-tuning LLMs for healthcare purposes, making it affordable and practical for hospitals\n(Xiong, Wang et al. 2023). Additionally, fine-tuning LLMs has been explored to find agreement among humans with\ndiverse preferences, helping individuals with different views to reach a consensus on moral and political issues (Bakker,\nChadwick et al.). Furthermore, MotionGPT has demonstrated the ability to generate human motion using multimodal\ncontrol signals, showcasing the versatility of fine-tuned LLMs in the field of digital humans (Zhan, Huang et al.). Drawing\ninspiration from these diverse applications, this research explores the potential of leveraging fine-tuned language models\nin the field of bioinformatics, aiming to address specific challenges and unlock new opportunities in this domain.\nIntroduction\nBioinformatics is a dynamic field that consolidates biology, computer science, and statistics to yield valuable insights from\nbiological data. As the integration of deep learning and natural language processing (NLP) techniques with bioinformatics\nsurges, the application of fine-tuned language models has garnered substantial attention. This comprehensive research\nanalysis delves into the vast applications of these models within bioinformatics, discussing the associated challenges and\nlimitations through a data-backed lens.\nApplications\nBiomedical Literature Analysis\nFine-tuned language models are transformative tools in bioinformatics, especially for biomedical literature analysis.\nLeveraging models like GPT-3.5 can greatly enhance information extraction from scientific papers, helping researchers\nnavigate and synthesize enormous knowledge databases. Furthermore, synthetic sequence generation - an emerging\napplication in DNA, RNA, and protein studies - is significantly facilitated by these models when fine-tuned with specific\nbiological datasets. The models also exhibit prowess in genomics and variant analysis by predicting the functional\nimplications of genetic variations, identifying potential disease-causing mutations, and aiding personalized medicine by\noffering tailored treatment recommendations based on individual genetic profiles.\nDrug Discovery and Repurposing\nThe landscape of drug discovery and repurposing has been revolutionized with the advent of fine-tuned language models.\nThrough the analysis of extensive chemical and biological databases, these models can expedite the identification of\nnovel drug targets and predict the binding affinity of small molecules to target proteins. They also aid drug repurposing by\nQeios, CC-BY 4.0   ·   Article, \nJuly 15, 2023\nQeios ID: WE7UMN.2   ·   https://doi.org/10.32388/WE7UMN.2\n2\n/\n5\nanalysing scientific literature and drug databases, pinpointing potential drug candidates for repurposing and substantially\nreducing the time and cost associated with conventional drug discovery processes.\nClinical Decision Support\nClinical decision-making involves intricate synthesis of patient data, medical knowledge, and up-to-date research findings.\nFine-tuned language models are now being employed to provide clinical decision support, aiding healthcare professionals\nin diagnosing diseases, selecting treatments, and predicting prognosis. By scrutinizing patient electronic health records,\nmedical literature, and clinical guidelines, these models offer personalized recommendations and warn clinicians about\npotential adverse events or drug interactions.\nProtein Structure Prediction\nThe accurate prediction of protein structure, a longstanding challenge in bioinformatics, has been significantly improved by\nfine-tuned language models combined with advanced deep learning techniques. By training these models on large protein\nstructure databases, researchers can predict protein folding patterns and tertiary structures with increased precision.\nAccurate protein structure prediction can expedite drug design, target identification, and understanding of disease\nmechanisms.\nPharmacovigilance and Adverse Event Detection\nPharmacovigilance involves real-world monitoring and detection of adverse drug reactions. Fine-tuned language models\nplay a critical role here by analysing large volumes of unstructured data like social media posts, patient forums, and\nelectronic health records. They can facilitate early detection of adverse events, improving patient safety and enabling\ntimely intervention.\nChallenges & Limitations\nDespite their potential, fine-tuned language models in bioinformatics encounter several challenges and limitations for their\nextensive adoption and dependable application.\nData availability and quality\nThese models need large, diverse, and high-quality datasets for efficient training. This is a daunting task in bioinformatics\ndue to privacy concerns, data scarcity, and standardization issues. Therefore, promoting data sharing initiatives,\ndeveloping standardized formats, and ensuring ethical use of sensitive patient data are critical.\nDomain-specific knowledge\nQeios, CC-BY 4.0   ·   Article, \nJuly 15, 2023\nQeios ID: WE7UMN.2   ·   https://doi.org/10.32388/WE7UMN.2\n3\n/\n5\nBioinformatics requires in-depth knowledge of biological concepts and terminology. Fine-tuned language models may\nstruggle with capturing domain-specific knowledge and may produce inaccurate or misleading results. Incorporating\ndomain-specific ontologies, curated databases, and expert annotations can enhance the performance and reliability of\nthese models.\nBias and generalization\nBiases in the training data may be inherited by fine-tuned language models, leading to skewed outputs and potentially\nimpacting decision-making in sensitive areas like clinical practice or drug development. Regular audits, bias detection, and\ndebiasing techniques are essential to ensure fairness, transparency, and accountability in the application of these models.\nInterpretability and explainability\nFine-tuned language models, particularly deep learning models, often face criticism for their lack of interpretability. The\nopaque nature of these models raises concerns regarding trust and ethical implications. Development of explainable AI\ntechniques such as attention mechanisms and rule-based explanations can provide interpretable insights and justify the\ndecisions made by these models.\nComputational resources and efficiency\nThese models are computationally intensive and require significant computational resources and energy consumption.\nThis poses a challenge particularly for resource-limited settings or applications that require real-time or near real-time\nresponses. Advancements in hardware acceleration, model compression techniques, and efficient model architectures\ncan address these challenges and enhance the practicality of deploying these models.\nEthical considerations\nThe ethical implications of using fine-tuned language models in bioinformatics cannot be ignored. Data privacy, informed\nconsent, transparency, and responsible data handling practices must be rigorously followed. Collaboration between\nbioinformatics researchers, ethicists, and regulatory bodies is essential to establish guidelines, policies, and best\npractices for the ethical use of these models.\nValidation and experimental constraints\nFine-tuned language models show promising results in various bioinformatics applications, but rigorous validation and\nexperimental verification are crucial before translating these findings into clinical practice or real-world scenarios.\nExperimental constraints, limited availability of ground truth data, and the need for reproducibility pose challenges that\nrequire close collaboration between computational and experimental biologists.\nQeios, CC-BY 4.0   ·   Article, \nJuly 15, 2023\nQeios ID: WE7UMN.2   ·   https://doi.org/10.32388/WE7UMN.2\n4\n/\n5\nConclusion\nThe scope of fine-tuned language models in bioinformatics is vast and offers considerable potential to expedite research\nand decision-making processes. From biomedical literature analysis to drug discovery, clinical decision support, protein\nstructure prediction, and pharmacovigilance, these models contribute to solving complex bioinformatics challenges.\nAddressing limitations and challenges such as data availability and quality, domain-specific knowledge, bias and\ngeneralization, interpretability and explainability, computational resources and efficiency, ethical considerations, and\nvalidation and experimental constraints is crucial. Tackling these issues will fully unleash the potential of fine-tuned\nlanguage models, paving the way for major advancements in bioinformatics to the advantage of human health and\nscientific discovery.\nReferences\nBakker, M. A., et al. \"Fine-tuning language models to find agreement among\nhumans with diverse preferences.\"\nLomshakov, V., et al. (2023). Fine-Tuning Large Language Models for Answering Programming Questions with Code\nSnippets. Computational Science – ICCS 2023, Cham, Springer Nature Switzerland.\nWe study the ability of pretrained large language models (LLM) to answer questions from online question answering\nfora such as Stack Overflow. We consider question-answer pairs where the main part of the answer consists of\nsource code. On two benchmark datasets—CoNaLa and a newly collected dataset based on Stack Overflow—we\ninvestigate how a closed-book question answering system can be improved by fine-tuning the LLM for the\ndownstream task, prompt engineering, and data preprocessing. We use publicly available autoregressive language\nmodels such as GPT-Neo, CodeGen, and PanGu-Coder, and after the proposed fine-tuning achieve a BLEU score\nof 0.4432 on the CoNaLa test set, significantly exceeding previous state of the art for this task.\nXiong, H., et al. (2023). \"DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task.\"\nYang1, H. B., et al. (2023). \"FinGPT: Open-Source Financial Large Language Models.\"\nZhan, Y., et al. \"MotionGPT: Finetuned LLMs are General-Purpose Motion Generators.\"\nQeios, CC-BY 4.0   ·   Article, \nJuly 15, 2023\nQeios ID: WE7UMN.2   ·   https://doi.org/10.32388/WE7UMN.2\n5\n/\n5",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.8022953271865845
    },
    {
      "name": "Data science",
      "score": 0.7090891599655151
    },
    {
      "name": "Computer science",
      "score": 0.658406138420105
    },
    {
      "name": "Scope (computer science)",
      "score": 0.6522115468978882
    },
    {
      "name": "Field (mathematics)",
      "score": 0.5544028282165527
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.5509761571884155
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5047308206558228
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4202902913093567
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39337092638015747
    },
    {
      "name": "Management science",
      "score": 0.3792434632778168
    },
    {
      "name": "Engineering",
      "score": 0.10667628049850464
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114203471",
      "name": "University of Gloucestershire",
      "country": "GB"
    }
  ]
}