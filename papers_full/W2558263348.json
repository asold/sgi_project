{
  "title": "Attention-based Memory Selection Recurrent Network for Language Modeling",
  "url": "https://openalex.org/W2558263348",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5084868332",
      "name": "Da-Rong Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5028930617",
      "name": "Shun-Po Chuang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5040508737",
      "name": "Hung-yi Lee",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2100506586",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2952191002",
    "https://openalex.org/W2174492417",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2342395274",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W2119615570",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2227523508",
    "https://openalex.org/W2963954913",
    "https://openalex.org/W2288502450",
    "https://openalex.org/W2026149468",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1785460851",
    "https://openalex.org/W2424413254",
    "https://openalex.org/W2013436217",
    "https://openalex.org/W2166637769"
  ],
  "abstract": "Recurrent neural networks (RNNs) have achieved great success in language modeling. However, since the RNNs have fixed size of memory, their memory cannot store all the information about the words it have seen before in the sentence, and thus the useful long-term information may be ignored when predicting the next words. In this paper, we propose Attention-based Memory Selection Recurrent Network (AMSRN), in which the model can review the information stored in the memory at each previous time step and select the relevant information to help generate the outputs. In AMSRN, the attention mechanism finds the time steps storing the relevant information in the memory, and memory selection determines which dimensions of the memory are involved in computing the attention weights and from which the information is extracted.In the experiments, AMSRN outperformed long short-term memory (LSTM) based language models on both English and Chinese corpora. Moreover, we investigate using entropy as a regularizer for attention weights and visualize how the attention mechanism helps language modeling.",
  "full_text": "ATTENTION-BASED MEMORY SELECTION RECURRENT NETWORK\nFOR LANGUAGE MODELING\nDa-Rong Liu, Shun-Po Chuang, Hung-yi Lee\nNational Taiwan University\nABSTRACT\nRecurrent neural networks (RNNs) have achieved great success\nin language modeling. However, since the RNNs have ﬁxed size\nof memory, their memory cannot store all the information about the\nwords it have seen before in the sentence, and thus the useful long-\nterm information may be ignored when predicting the next words. In\nthis paper, we propose Attention-based Memory Selection Recurrent\nNetwork (AMSRN), in which the model can review the information\nstored in the memory at each previous time step and select the rel-\nevant information to help generate the outputs. In AMSRN, the at-\ntention mechanism ﬁnds the time steps storing the relevant informa-\ntion in the memory, and memory selection determines which dimen-\nsions of the memory are involved in computing the attention weights\nand from which the information is extracted. In the experiments,\nAMSRN outperformed long short-term memory (LSTM) based lan-\nguage models on both English and Chinese corpora. Moreover, we\ninvestigate using entropy as a regularizer for attention weights and\nvisualize how the attention mechanism helps language modeling.\nIndex Terms— Language Modeling, Recurrent Network, At-\ntention Model\n1. INTRODUCTION\nRecurrent neural networks (RNNs) [1] have been shown to perform\nwell in many sequence modeling tasks[2]. In RNNs, the gated mem-\nory cells like long short-term memory (LSTM) [3] and gated recur-\nrent unit (GRU) [4] are widely used. The attention mechanism has\nbeen applied on RNN models. Neural Turing Machine (NTM) [5] is\none of the examples. The idea of attention mechanism is to let the\nmodel automatically ﬁnd the related part of information from mem-\nory (usually represented as a vector sequence), and use the infor-\nmation to obtain the results. Attention mechanism shows promising\nresults on many tasks including machine translation[6, 7, 8], caption\ngeneration[9, 10] and question answering [11, 12, 13, 14].\nLanguage modeling has been recognized as an important task in\nhuman language processing. The statistical models such as N-gram\nlanguage model [15, 16] were widely used to solve this task. Re-\ncently, RNNs are introduced in language modeling [17, 18, 19, 20,\n21, 22, 23, 24] and have shown great improvement compared to the\ntraditional counterpart. However, since the RNNs have ﬁxed size\nof memory, their memory cannot store all the information about the\nwords have seen in the sentence, and thus the useful long-term in-\nformation may be ignored when predicting the next words. By mak-\ning the RNN models have the ability to review the information ob-\ntained in every previous time step, the attention mechanism improves\nRNN model. The examples of integrating attention mechanism and\nLSTM-based RNN model for language modeling are Long Short-\nterm Memory-Network (LSTMN) [25] and Recurrent Memory Net-\nwork (RMN) [26]. LSTMN uses an expandable hidden memory to\nexplicitly store every past memory segments, making use of all the\nprevious values in the memory to compute every update and generate\nthe results. RMN uses the hidden memory of LSTM to generate the\nattention weights, and then uses the attention weights and another\ntrainable memory to generate the outputs. Both LSTMN and RMN\nare shown to outperform original LSTM on language modeling.\nIn this paper, we propose Attention-based Memory Selection\nRecurrent Network (AMSRN), a novel RNN architecture that ap-\nplies the attention mechanism on LSTM. In AMSRN, the attention\nmechanism extracts the relevant information from the LSTM mem-\nory states in all the previous time steps for predicting the next word.\nThe information in different dimensions of LSTM memory states has\ndifferent degrees of involvement in attention weight generation and\nrelevant information extraction. The degree of the involvement for\neach dimension is different for each time step. The memory selec-\ntion mechanism is automatically learned from data. In this paper, we\nmainly make the following contributions:\n1. We investigate different ways of integrating LSTM and atten-\ntion mechanism. The experimental results show that the at-\ntention mechanism helps the LSTM language model on three\ndifferent corpora including English and Chinese.\n2. Different from LSTMN which makes some modiﬁcation on\nthe way LSTM updates the memory, in AMSRN the attention\nmechanism is stacked on original LSTM, so the architecture\nof the original LSTM has remained. Therefore, the LSTM\npart in AMSRN can be initialized by a typical LSTM lan-\nguage model.\n3. In RMN, there are two sets of memory, one for computing\nthe attention weights and the other for extracting the infor-\nmation. On the other hand, the proposed model learns to de-\ntermine which memory dimensions should be involved more\nin computing the attention weights and which should be con-\nsidered more when extracting the information, and the role\nof each dimension can be different at different time steps.\nFrom this point of view, RMN can be considered as a special\ncase of the proposed model. The experimental results show\nthat the proposed model has more stable performance across\ndifferent corpora than RMN.\n4. We investigate to use entropy as regularizer for attention\nweights.\n5. Finally, we make a visualization analysis of how the attention\nmechanism helps language modeling.\n2. ATTENTION-BASED MEMORY SELECTION\nRECURRENT NETWORK\nThe overall structure of the proposed Attention-based Memory Se-\nlection Recurrent Network (AMSRN) is shown in Fig. 1. AMSRN\narXiv:1611.08656v1  [cs.CL]  26 Nov 2016\nFig. 1. The overall structure of the proposed Attention-based Mem-\nory Selection Recurrent Network (AMSRN) model. The notation ⊙\nin the ﬁgure represents elementwise multiplication of two vectors.\nThe notation •represents inner product.\nconsists of two major parts: the typical LSTM described in Sec-\ntion 2.1 and the attention mechanism module stacking on LSTM in\nSection 2.2. The LSTM reads through the input word sequence, and\nstores the hidden layer outputs generated at each time step. The at-\ntention mechanism module takes the stored information as input, and\ngenerates a vector relevant to the prediction of the next words. Then\nthe relevant vector and the current LSTM hidden state are used to\ngenerate the distributions for the next words. In the attention mech-\nanism module, the memory selection is applied on the LSTM mem-\nory to determine which dimensions of LSTM hidden states should\nbe involved in computing the attention weights and extracting rele-\nvant information, which will be described in Section 2.3. Finally, the\nattention weights can be regularized by their entropy in Section 2.4.\n2.1. LSTM\nThe input of the LSTM is a sequence of words represented by 1-of-\nN encoding, {x1,x2,···xt,···}, at the bottom of Fig. 1. At each\ntime step t, the hidden layer output of the LSTM is a d-dimensional\nvector ht, where dis the number of memory cells in LSTM, and ht\nwould be stored for further use. Therefore, at the time step t(when\nthe model has read the ﬁrst twords in the sentence in question), the\ninformation stored is Mt,\nMt = [h0,h1,··· ht−1], (1)\nwhere h0 is the initial state of the LSTM.Mt in (1) is a d×tmatrix,\nwhich grows as t increases. The attention module will extract the\ninformation from Mt.\n2.2. Attention Mechanism\nIn the attention mechanism, the memory selection module generates\ntwo d-dimensional vectors, wh1 and wh2, from the current LSTM\nstate ht. wh1 and wh2 are used to select the stored information.\nHere all the elements in wh1 and wh2 are between 0 and 1. How to\ngenerate wh1 and wh2 will be described in the next subsection.\nThen the current hidden state ht and the two memory selection\nvectors, wh1 and wh2, are used to extract the relevant information,\nrepresented as a d-dimensional vector rt, from Mt in (1). The model\nﬁrst generates a d-dimensional vector kt from the current hidden\nstate ht as the ‘key’ for attention weight generation,\nkt = Wkhht + bk, (2)\nwhere the d×dmatrix Wkh and d-dimensional vectorbk are network\nparameters to be learned. Then the inner product similarity eti be-\ntween the key kt and each hi in Mt = [h0,h1,h2,···hi,··· ht−1]\nis computed.\neti = (hi ⊙wh1) •kt, (3)\nwhere ⊙denotes the elementwise multiplication, and •denotes the\ninner product. By multiplying each element inhi by the correspond-\ning element in wh1 (that is, hi ⊙wh1 in (3)), the model determines\nthe degree of each dimension of hi involved in computing the sim-\nilarity (for example, the dimension multiplied by 0 would be totally\nignored in generating the attention weights). The similarity eti is\nfurther normalized by softmax normalization to obtain the attention\nweights αti,\nαti = exp(eti)∑t−1\ni=0 exp(eti)\n. (4)\nTo generate the relevant vector rt, each hi is selected by wh2 to\nobtain h′\ni,\nh′\ni = hi ⊙wh2, (5)\nin which the degree each dimension of hi is involved in extracting\nthe relevant vector rt is determined. Finally, rt is generated by the\nweighted sum of h′\ni according to αti,\nrt =\nt−1∑\ni=0\nαtih′\ni. (6)\nThe attention vector rt and the hidden state ht predicts the distribu-\ntion of the next word Pw,\nPw = softmax(Wphht + Wprrt + bp),\nwhere Wph, Wpr and bp are network parameters to be learned, and\nsoftmax is the softmax activation function. The cost C to be min-\nimized by optimizing the network parameters is the cross-entropy\nbetween the word distribution Pw and the reference distribution for\nall the words in the training set.\n2.3. Memory Selection\nIn this paper, we investigate three different ways to obtain wh1 used\nin (3) and wh2 in (6):\n1. wh1 and wh2 are generated independently. The current state\nof LSTM, ht, is passed into two different fully connected\nlayers with sigmoid activation function to generate wh1 and\nwh2 as below,\nwh1 = sigmoid(Whh1ht + bh1)\nwh2 = sigmoid(Whh2ht + bh2),\nwhere the Whh1, bh1, Whh2 and bh2 denote the weights and\nthe biases of the fully connected layer.\n2. The two vectors wh1 and wh2 are forced to be the same. wh1\nis generated by the same way as the ﬁrst approach, and the\nmodel simply sets wh2 = wh1.\n3. The only difference between the third and the second ap-\nproaches is that here we set wh1 = 1 −wh2, where 1 is a d-\ndimensional vector with all ones, and ‘−’ here represents el-\nementwise subtraction. The inspiration of the third approach\nis that in the attention models the memory used to generate\nattention distribution and the memory used to generate the\nﬁnal attention vector can be different [26, 27]. Therefore,\nby constraining the sum of the two weights, wh1 and wh2,\nit simulates the situation that there are two different sets of\nmemory for attention weights and information extraction re-\nspectively.\n2.4. Regularizer\nWhen training model, a regularization term is usually used to prevent\noverﬁtting. For example, the two-norm of the model parameters are\nwidely used as a regularizer. Here we investigate to use the entropy\nof the attention weights as the regularization term [28]. The purpose\nof using entropy as regularizer is because only part of the informa-\ntion in the previous steps is relevant to the prediction of the next\nword. Therefore, the attention weights that extract useful informa-\ntion from the previous time steps are sparse. The entropy regularizer\nto keep the attention weights sparse is designed as below.\nLreg =\n∑\nu\nTu∑\nt=1\nt−1∑\ni=0\n−wti log wti, (7)\nwhere uis a sentence in the training corpus, and Tu is the length of\nu. wti in (7) denotes the attention weight of hi at the time step t\nwhen reading sentence u, and ∑t−1\ni=0 −wti log wti is the entropy of\nthe attention weights obtained at the time step t. With the regular-\nization term, the network is learned to minimize C+ λLreg, where\nC has been mentioned in Subsection 2.2 and λis determined by a\nvalidation set.\n3. EXPERIMENTS\n3.1. Experimental Setups\nWe tested the proposed model on two English data sets and one Chi-\nnese data set. The ﬁrst data set we used is the Penn Treebank Cor-\npus [29], which is a widely used data set to evaluate the effectiveness\nof a language model. It contains about 40K training sentences, 3K\nvalidation sentences and 4K testing sentences. The other English\ndata set we used is from the Switchboard Corpus[30]. Switchboard\nis a Telephone Speech Corpus which collect two-sided telephone\nconversations among speakers in the United States. We used about\n945K sentences for training, 10K for validation and about 5.2K for\ntesting. For Chinese, we used Chinese Gigaword data set[31] to eval-\nuate the model. Chinese Gigaword data set consists of around 25K\nChinese news articles. After parsing, there are 531K sentences for\ntraining, 165K for validation and about 260K for testing. Table 1\nsummaries the statistics of the three data sets we used in the follow-\ning experiments. The perplexities (PPLs) on the testing data sets are\nused to evaluate different methods.\nAlthough it is possible to train AMSRN model from scratch,\nsince the AMSRN model contains a LSTM part, it is possible to ini-\ntialize the LSTM part by a LSTM langauge model. Therefore, in\nthe following experiments, the AMSRN is always pretrained by a\nLSTM language model. Because of the limited computing resource,\nwe ﬁxed both the dimension of the LSTM hidden state and the em-\nbedding layer to be 50.\n3.2. Memory Selection Methods\nIn this experiment, we investigate different memory selection meth-\nods for generating wh1 and wh2 respectively. The three methods in\nTable 1. The statistics of the three data sets we used in the following\nexperiments.\nCorpus Lang train dev test |s| |v|\nPT Eng 40K 3K 4K 21.1 9999\nSB Eng 945K 10K 5.2K 10.39 47283\nGW Chi 531K 165K 260K 10.79 5123\n|s| denotes the average number of words in the sentences.\n|v| denotes the size of the vocabulary.\nPT denotes Penn Treebank Corpus.\nSB denotes Switchboard Corpus.\nGW denotes Gigaword Corpus.\nTable 2. Comparison of the three memory selection approaches in\nSubsection 2.3 on the Penn Treebank corpus.\nweight relation perplexity\n(1) wh1 and wh2 are independent 133.80\n(2) wh1 = wh2 133.36\n(3) wh1 = 1 −wh2 133.62\nSubsection 2.3 are (1) generating wh1 and wh2 are independently,\n(2) setting wh1 = wh2, and (3) setting wh1 = 1 −wh2. The results\nof the three methods are shown in Table 2. Generatingwh1 and wh2\nindependently leads to the worst result (133.80). This may be be-\ncause this approach need twice parameters compared with the other\ntwo. Constraining wh1 and wh2 to be the same is better than making\nthem complementary (133.36 v.s. 133.62). The results suggest that\nprobably the information for computing similarity and information\nextraction are contained in the same dimension of the LSTM hidden\nstates. Since the second method achieves the best result, it is used in\nthe following experiments.\n3.3. Comparison of Different Models\nThe experimental results of different models are shown in Table 3.\nColumns (1), (2) and (3) are the results on Penn Treebank Corpus,\nSwitchboard Corpus and Chinese Gigaword data set, respectively.\nExperiments were done step by step. First, a typical LSTM lan-\nguage model was trained, and PPLs of the LSTM model on the test-\ning sets are in row (A). Then in row (B) the attention module was\nadded on top of LSTM but without memory selection (or all the el-\nements in wh1 and wh2 are one) and entropy regularizer. It is found\nthat attention mechanism was helpful on both Penn Treebank and\nSwitchboard (Rows (B) v.s. (A) on Columns (1) and (2)), but it does\nnot improve the LSTM on Chinese Gigaword (Rows (B) v.s. (A) on\ncolumns (3)). In row (C), we show the results of wit memory selec-\ntion based on the second approach in Subsection 2.3. We found that\nmemory selection is essential for attention mechanism here. With\nmemory selection, attention-based model outperformed LSTM on\nall the three corpora (Rows (C) v.s. (B)). Then the entropy regular-\nization for the attention weights was applied on the attention-based\nmodel with memory selection. The results are in row (D). The results\nof entropy regularization are mixed. It improved the performance on\nPenn Treebank, but degrades the performance on the rest two cor-\npora (Rows (D) v.s. (C)). The experimental results suggest that the\nassumption of sparse attention weights is probably not very accurate.\nWe further compare the proposed model with another two\nattention-based language model, recurrent memory network (RMN) [26]\nand Recurrent-Memory-Recurrent (RMR) [26]. Comparing LSTM\nwith the two attention-based model in the literature, the conclusion\nFig. 2. The visualization of attention weights. The arrows point to the words to be predicted. The words with attention are highlighted.\nFor the Chinese examples, the order of the Chinese characters in a sentence will not be the same as its English translation. There are\nfour examples in the ﬁgure: (a) Trigger: attend to the same place name shown before in the sentence. (b) Causal Relationship: There are\npoisonous insects, so the task is a challenge. (c) Phrases:’on amount of’ is a phrase, so the ﬁrst two words will help determine the last word.\n(d) Grammar: we should use ’are’ right after a plural noun.\nTable 3. Perplexity Evaluation.\nmodel (1) PT (2) SB (3) GW\n(A) LSTM 143.31 93.90 94.03\n(B) LSTM+att 134.09 93.74 102.04\n(C) LSTM+att+select 133.36 92.49 86.85\n(D) LSTM+att+select+entropy 131.43 99.79 88.25\n(E) RMN [26] 123.32 64.41 121.28\n(F) RMR [26] 134.30 71.04 145.24\natt: attention mechanism\nselect: memory selection mechanism\nentropy: entropy regularizer\nis also mixed. RMN and RMR outperformed drastically the two\nEnglish corpora (Rows (E), (F) v.s. (A) on Columns (1) and (2)),\nbut contrary conclusion is obtained on the Chinese corpus (Rows\n(E), (F) v.s. (A) on Column (3)). This is probably RMN and RMR\nhave only be veriﬁed on English, German, and Italian, and there\nare some special techniques on Chinese that should be specially\nconsidered. The proposed approach consistently improves LSTM,\nand better than RMN and RMR on Chinese, but worse than them\non English corpora. The proposed model seems to be more robust\nacross different corpora, but the improvements are limited.\n3.4. Analysis\nTo illustrate how attention mechanism works, we visualize the at-\ntention weights in some sentences. We ﬁrst compute the perplex-\nities of each sentence in Gigaword (Chinese) and Penn Treebank\n(English) data sets, then select the sentences which improved the\nmost by the proposed model (Row (C) in Table 3) compared with\nthe LSTM baselines. We chose ten sentences from Gigaword (Chi-\nnese) and Penn Treebank data sets, and visualize and analysis the\nattention weights. Four examples are shown in Fig. 2. In Fig. 2,\nthe arrows point to the words to be predicted, and we highlight the\nwords whose attention weights are higher than a threshold when pre-\ndicting the words with arrows. We found that a word will have large\nattention under one of the following four conditions:\n1. Trigger (example (a) in Fig. 2: When the information is re-\npeated, the model attends to the part where the same infor-\nmation is mentioned before.\n2. Causal Relationship (example (b)): If A is the cause of B,\nwhen prediction the words related toB, the model will attend\nto the words related to A.\n3. Phrases (example (c)): When predicting a word in the later\npart of a phrase, the model will attend on the former part of\nthe same phrase.\n4. Grammar (example (d)): Some grammar rules are considered\nby the attention-based model. For example, to predict the\nword ’are’, the model attends on a plural noun.\n4. CONCLUDING REMARKS\nIn this paper, we propose Attention-based Memory Selection Re-\ncurrent Network (AMSRN) for language modeling and investigate\nthe integration of attention mechanism and LSTM. The results were\nveriﬁed on two English corpora and a Chinese corpus. The re-\nsults show that AMSRN consistently outperformed LSTM-based\nlanguage model, and memory selection is essential for attention\nmechanism. We further visualize how the attention mechanism\nworks in language modeling. Some questions unresolved in this\npaper will be studied in the future, for example, the inﬂuence of the\nlanguage characteristics to the attention-based model.\n5. REFERENCES\n[1] Jeffrey L Elman, “Finding structure in time,” Cognitive sci-\nence, vol. 14, no. 2, pp. 179–211, 1990.\n[2] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, “Sequence to\nsequence learning with neural networks,” in Advances in neu-\nral information processing systems, 2014, pp. 3104–3112.\n[3] Sepp Hochreiter and J ¨urgen Schmidhuber, “Long short-term\nmemory,” Neural computation, vol. 9, no. 8, pp. 1735–1780,\n1997.\n[4] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and\nYoshua Bengio, “Empirical evaluation of gated recurrent\nneural networks on sequence modeling,” arXiv preprint\narXiv:1412.3555, 2014.\n[5] Alex Graves, Greg Wayne, and Ivo Danihelka, “Neural turing\nmachines,” arXiv preprint arXiv:1410.5401, 2014.\n[6] Minh-Thang Luong, Hieu Pham, and Christopher D Manning,\n“Effective approaches to attention-based neural machine trans-\nlation,” arXiv preprint arXiv:1508.04025, 2015.\n[7] Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova,\nKaisheng Yao, Chris Dyer, and Gholamreza Haffari, “Incor-\nporating structural alignment biases into an attentional neural\ntranslation model,” arXiv preprint arXiv:1601.01085, 2016.\n[8] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio,\n“Neural machine translation by jointly learning to align and\ntranslate,” arXiv preprint arXiv:1409.0473, 2014.\n[9] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron\nCourville, Ruslan Salakhutdinov, Richard S Zemel, and\nYoshua Bengio, “Show, attend and tell: Neural image\ncaption generation with visual attention,” arXiv preprint\narXiv:1502.03044, vol. 2, no. 3, pp. 5, 2015.\n[10] Junqi Jin, Kun Fu, Runpeng Cui, Fei Sha, and Changshui\nZhang, “Aligning where to see and what to tell: image cap-\ntion with region-based attention and scene factorization,”arXiv\npreprint arXiv:1506.06272, 2015.\n[11] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex\nSmola, “Stacked attention networks for image question an-\nswering,” arXiv preprint arXiv:1511.02274, 2015.\n[12] Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei\nXu, and Ram Nevatia, “Abc-cnn: An attention based convolu-\ntional neural network for visual question answering,” arXiv\npreprint arXiv:1511.05960, 2015.\n[13] Kevin J. Shih, Saurabh Singh, and Derek Hoiem, “Where to\nlook: Focus regions for visual question answering,” in The\nIEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2016.\n[14] Abhishek Das, Harsh Agrawal, C Lawrence Zitnick, Devi\nParikh, and Dhruv Batra, “Human attention in visual question\nanswering: Do humans and deep networks look at the same\nregions?,” arXiv preprint arXiv:1606.03556, 2016.\n[15] Roni Rosenfeld, “Two decades of statistical language model-\ning: Where do we go from here?,” 2000.\n[16] Fred Jelinek, “Up from trigrams,” .\n[17] Tomas Mikolov, “Recurrent neural network based language\nmodel.,” .\n[18] Tom ´aˇs Mikolov, Stefan Kombrink, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur, “Extensions of recur-\nrent neural network language model,” in 2011 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2011, pp. 5528–5531.\n[19] Siva Reddy Gangireddy, Fergus McInnes, and Steve Renals,\n“Feed forward pre-training for recurrent neural network lan-\nguage models.,” 2014.\n[20] Xunying Liu, Yongqiang Wang, Xie Chen, Mark JF Gales, and\nPhilip C Woodland, “Efﬁcient lattice rescoring using recur-\nrent neural network language models,” in 2014 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2014, pp. 4908–4912.\n[21] Bing Zhao and Yik-Cheung Tam, “Bilingual recurrent neural\nnetworks for improved statistical machine translation,” inSpo-\nken Language Technology Workshop (SLT), 2014 IEEE. IEEE,\n2014, pp. 66–70.\n[22] Jen-Tzung Chien and Yuan-Chu Ku, “Bayesian recurrent neu-\nral network for language modeling,”IEEE transactions on neu-\nral networks and learning systems, vol. 27, no. 2, pp. 361–374,\n2016.\n[23] Abdel-rahman Mohamed, Frank Seide, Dong Yu, Jasha\nDroppo, Andreas Stoicke, Geoffrey Zweig, and Gerald Penn,\n“Deep bi-directional recurrent networks over spectral win-\ndows,” in 2015 IEEE Workshop on Automatic Speech Recog-\nnition and Understanding (ASRU). IEEE, 2015, pp. 78–83.\n[24] Xiangang Li and Xihong Wu, “Constructing long short-term\nmemory based deep recurrent neural networks for large vocab-\nulary speech recognition,” in 2015 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2015, pp. 4520–4524.\n[25] Jianpeng Cheng, Li Dong, and Mirella Lapata, “Long short-\nterm memory-networks for machine reading,” arXiv preprint\narXiv:1601.06733, 2016.\n[26] Ke Tran, Arianna Bisazza, and Christof Monz, “Recurrent\nmemory network for language modeling,” arXiv preprint\narXiv:1601.01272, 2016.\n[27] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al., “End-\nto-end memory networks,” in Advances in neural information\nprocessing systems, 2015, pp. 2440–2448.\n[28] Yves Grandvalet and Yoshua Bengio, “Semi-supervised learn-\ning by entropy minimization,” in Advances in neural informa-\ntion processing systems, 2004, pp. 529–536.\n[29] Ann Taylor, Mitchell Marcus, and Beatrice Santorini, “The\npenn treebank: an overview,” inTreebanks, pp. 5–22. Springer,\n2003.\n[30] John J Godfrey, Edward C Holliman, and Jane McDaniel,\n“Switchboard: Telephone speech corpus for research and de-\nvelopment,” in Acoustics, Speech, and Signal Processing,\n1992. ICASSP-92., 1992 IEEE International Conference on.\nIEEE, 1992, vol. 1, pp. 517–520.\n[31] David Graff and Ke Chen, “Chinese gigaword,” LDC Catalog\nNo.: LDC2003T09, ISBN, vol. 1, pp. 58563–58230, 2005.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8424946069717407
    },
    {
      "name": "Recurrent neural network",
      "score": 0.7446287870407104
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.5956641435623169
    },
    {
      "name": "Language model",
      "score": 0.5672662258148193
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5618839859962463
    },
    {
      "name": "Long short term memory",
      "score": 0.5545929670333862
    },
    {
      "name": "Sentence",
      "score": 0.5213740468025208
    },
    {
      "name": "Mechanism (biology)",
      "score": 0.4808436334133148
    },
    {
      "name": "Natural language processing",
      "score": 0.4584495723247528
    },
    {
      "name": "Memory model",
      "score": 0.44107741117477417
    },
    {
      "name": "Machine learning",
      "score": 0.37697651982307434
    },
    {
      "name": "Artificial neural network",
      "score": 0.36201009154319763
    },
    {
      "name": "Shared memory",
      "score": 0.09439313411712646
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}