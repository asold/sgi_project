{
    "title": "Sparse Universal Transformer",
    "url": "https://openalex.org/W4389524452",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2112810962",
            "name": "Shawn Tan",
            "affiliations": [
                "Centre Universitaire de Mila",
                "Université de Montréal"
            ]
        },
        {
            "id": "https://openalex.org/A2105902150",
            "name": "Yikang Shen",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2486425054",
            "name": "Zhenfang Chen",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2328522601",
            "name": "Aaron Courville",
            "affiliations": [
                "Université de Montréal",
                "Centre Universitaire de Mila"
            ]
        },
        {
            "id": "https://openalex.org/A2112762928",
            "name": "Chuang Gan",
            "affiliations": [
                "IBM (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3066373881",
        "https://openalex.org/W4385570274",
        "https://openalex.org/W3043172396",
        "https://openalex.org/W4394665201",
        "https://openalex.org/W2952744660",
        "https://openalex.org/W1593045043",
        "https://openalex.org/W2952809536",
        "https://openalex.org/W4385573095",
        "https://openalex.org/W4298187912",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2996132992",
        "https://openalex.org/W4224874866",
        "https://openalex.org/W3034296505",
        "https://openalex.org/W4379958589",
        "https://openalex.org/W3103682594",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4378942694",
        "https://openalex.org/W3098339586",
        "https://openalex.org/W4294871258",
        "https://openalex.org/W4389518729",
        "https://openalex.org/W4288094673",
        "https://openalex.org/W3197009789",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W2403599343",
        "https://openalex.org/W4285595056",
        "https://openalex.org/W2939413764",
        "https://openalex.org/W4290994975",
        "https://openalex.org/W4319991279",
        "https://openalex.org/W2962911926",
        "https://openalex.org/W3017374003",
        "https://openalex.org/W2257408573",
        "https://openalex.org/W4311730295",
        "https://openalex.org/W4293718192",
        "https://openalex.org/W4308233871",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2173051530",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W3214124416",
        "https://openalex.org/W2963807318",
        "https://openalex.org/W2325237720",
        "https://openalex.org/W2981757109",
        "https://openalex.org/W4284701759",
        "https://openalex.org/W2132547887"
    ],
    "abstract": "The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers and is Turing-complete under certain assumptions. Empirical evidence also shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, most state-of-the-art NLP systems use VTs as their backbone model instead of UTs. This is mainly because scaling UT parameters is more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT combines the best of both worlds, achieving strong generalization results on formal language tasks (Logical inference and CFQ) and impressive parameter and computation efficiency on standard natural language benchmarks like WMT'14.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 169–179\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSparse Universal Transformer\nShawn Tan1 *\ntanjings@mila.quebec\nYikang Shen2 *\nyikang.shen@ibm.com\nZhenfang Chen2\nzfchen@ibm.com\nAaron Courville1\ncourvila@iro.umontreal.ca\nChuang Gan2\nchuangg@ibm.com\n1Mila, University of Montreal 2MIT-IBM Watson AI Lab\nAbstract\nThe Universal Transformer (UT) is a variant of\nthe Transformer that shares parameters across\nits layers. Empirical evidence shows that UTs\nhave better compositional generalization than\nVanilla Transformers (VTs) in formal language\ntasks. The parameter-sharing also affords it\nbetter parameter efficiency than VTs. Despite\nits many advantages, scaling UT parameters\nis much more compute and memory intensive\nthan scaling up a VT. This paper proposes the\nSparse Universal Transformer (SUT), which\nleverages Sparse Mixture of Experts (SMoE)\nand a new stick-breaking-based dynamic halt-\ning mechanism to reduce UT’s computation\ncomplexity while retaining its parameter effi-\nciency and generalization ability. Experiments\nshow that SUT achieves the same performance\nas strong baseline models while only using\nhalf computation and parameters on WMT’14\nand strong generalization results on formal lan-\nguage tasks (Logical inference and CFQ). The\nnew halting mechanism also enables around\n50% reduction in computation during inference\nwith very little performance decrease on formal\nlanguage tasks.\n1 Introduction\nRecent theoretical work has pointed out that finite-\ndepth Transformers have an issue of expressibility\nthat will result in failure to generalize (Hahn, 2020;\nHao et al., 2022; Merrill et al., 2022; Liu et al.,\n2022). Delétang et al. (2022) ran several neural\narchitectures on a suite of different synthetic lan-\nguages generated from different levels of the Chom-\nsky hierarchy and empirically confirmed these re-\nsults, showing that VTs have difficulty generaliz-\ning to Regular languages. Universal Transformers\n(UTs; Dehghani et al. 2018) are Transformers that\nshare parameters at every layer of the architecture.\nCsordás et al. (2021) performed several composi-\ntional generalization experiments on VTs and UTs\n∗Equal contribution\nFigure 1: A VT has separate Transformer blocks for\neach layer, with different parameters. For a UT with\nthe same number of parameters, the UT block will be\n∼3 times the dimensions of each VT block. Running\nthis block for 3 layers would then incur approximately\n9 times the runtime memory. Using SMoEs can recover\napproximately the same computational cost as the VT.\nalong with absolute and relative position embed-\ndings, and showed that UTs with relative positional\nembeddings performed better on these tasks.\nHowever, the task of scaling UTs is challenging\ndue to its computation complexity (Kaplan et al.,\n2020; Tay et al., 2022; Takase and Kiyono, 2021).\nConsider a VT with P parameters for each layer\nand Llayers. Evaluating such a VT has compu-\ntation complexity associated with the model size\nLP. A size-equivalent UT would have a UT block\nwith LP parameters and computation complexity\nof approximately LP to run the block once. To run\nsuch a UT for equivalent Llayers would incur a\ncomplexity of L2P. This increased computation\ncomplexity directly translates to increased train-\ning and inference time. According to Takase and\nKiyono (2021), UT requires two times the training\ntime and far more GPU memory than VT in WMT\nEnglish-German translation task.\nSparsely activated neural networks were intro-\n169\nduced to reduce the computation complexity of\nlarge models. These networks activate parts of the\nnetwork conditioned on the input, computing only\nparts of the model, thereby disentangling the num-\nber of parameters from the computation complex-\nity. This method allows for drastically increasing\nthe number of parameters without proportionally\nincreasing the computation complexity. Shazeer\net al. (2017) introduced Sparse Mixture of Ex-\nperts (SMoE), using the top- k operator to allow\nfor sparse computation of experts. This allows\nfor replacing the FeedForword (FFD) layer in the\nTransformer with an ensemble of Effd FFDs, but\nonly k FFDs (where k < E) would have to be\nevaluated, conditioned on the input. Zhang et al.\n(2022) then introduced the Mixture of Attention\nheads (MoA), which allows Transformers to re-\nplace its Multihead Attention (MHA) layer with an\nensemble of Eatt attention heads and only activates\nkheads condition on the input, further sparsifying\nthe model.\nThis paper introduces the Sparse Universal\nTransformer (SUT), which applies the above sparse\ncomputation techniques to UT. Additionally, we re-\nplace the per-position halting mechanism in UT\nwith a new stick-breaking formulation that has a\nprobabilistic interpretation, allowing us to intro-\nduce an Adaptive Computation Time (ACT; Graves\n2016) penalty to minimize layer use. It also pro-\nvides an easy way to adjust the trade-off between\nthe amount of computation and model performance\nduring inference, further improving the efficiency\nof the SUT at inference time.\nTo demonstrate effective scaling, we perform\nexperiments on WMT’14 English to German trans-\nlation, showing that an SUT can achieve better\nperformance for the same parameter count, while\nincurring less computation cost than an equivalent\ndense UT. Since the UT setting is a specific case\nof SUT, we show on the Compositional Freebase\nQuestions (CFQ; Keysers et al. 2019) tasks that\nUTs have better compositional generalization prop-\nerties, improving upon CFQ results from Csordás\net al. (2021). Using the Logical Inference task\n(Bowman et al., 2015), we analyse the behaviour of\nour UT on length and compositional generalization.\nFinally, we show that the halting mechanism can be\nused to achieve further efficiency during inference\ntime, and study the trade-off between efficiency\nand performance.\nFigure 2: Example of the compositional generalization\nsplits from (Shen et al., 2019). The combination of not\nand and are never seen in successive combination during\ntraining, and a VT may learn a shortcut that prevents\ngeneralisation during test.\n2 Background & Related Work\nOvercoming VT limitations with UTDziri et al.\n(2023) and Liu et al. (2022) find that Vanilla Trans-\nformers learn shortcuts for tasks that require multi-\nstep compositional operations, and fail to general-\nize on larger instances of the problem that require\nmore steps. Theoretical results have also shown\nthat Vanilla Transformers have limitations in what\nthey can compute that support these findings (Hahn,\n2020; Hao et al., 2022; Merrill et al., 2022). Univer-\nsal Transformers (Dehghani et al., 2018) are Trans-\nformers with tied weights across all layers, and an\nadditional halting mechanism to decide when to\nstop. In an ideal scenario of infinite layers (now\nthat all layers have the same parameters) the UT,\nlike the Neural GPU (Kaiser and Sutskever, 2015),\nis Turing-complete, which overcomes many of the\nabovementioned issues.\nIn practice, even with limited depth, UTs have\nexhibited properties that afford them better perfor-\nmance in compositional generalization tasks (Csor-\ndás et al., 2021). UTs allow operations learned in\nthe Transformer during training to be depth-order\ninvariant. If some operations during training are\nlearned to be performed in a certain order, during\ntest time, the UT could generalize to an unseen\norder of operations.\nChallenges with Scaling the UTDespite these\ncompositional abilities, performance tends to de-\ncrease on real-world tasks when using UTs. AL-\nBERT (Lan et al., 2019) improved parameter effi-\nciency by sharing parameters across layers. This\nwas motivated by an observation that Transform-\ners tend to learn to perform similar operations in\nthe layers, and that sharing these parameters would\n170\nreduce this redundancy 1. However, the authors\nobserve a dip in performance when sharing param-\neters, contrary to Dehghani et al. (2018).\nCould the issue be one of model capacity? Ex-\nperiments with ALBERT show that scaling up AL-\nBERT can outperform the BERT baseline, even on\nreal-world tasks (Lan et al., 2019). Kaplan et al.\n(2020) also show that a shared-parameter Trans-\nformer has better scaling properties in terms of\nparameter-to-performance, but poorer properties\nin terms of computation-to-performance, since pa-\nrameter count causes the computation to increase.\nTay et al. (2022) scale up different sequence mod-\nels, and remark on difficulties with scaling up UTs,\nlimiting the experiments they can perform on UT.\nTakase and Kiyono (2021) outline several strategies\nof scaling up shared-parameter Transformers to\ndeal with these issues by using different parameter-\nsharing schemes.\nOur experiments show that SMoE techniques\ncan be applied successfully to the UT to scale it up,\nachieving the UT’s parameter efficiency while not\nincurring the same computation costs. We also per-\nform experiments that support the compositional\ngeneralization claims of prior work, and provide\nbetter baselines for those tasks.\n3 Method\nLike UT, we reuse the same SUT block for every\nlayer of the Transformer. Within each SUT block,\nwe use SMoEs to achieve sparsity for the feed-\nforward network (FFD) and attention heads sepa-\nrately. We use the Mutual Information Maximiza-\ntion loss proposed in Chen et al. (2022) and mod-\nified for unsupervised tasks in Shen et al. (2023).\nFinally, we propose a stick-breaking process for-\nmulation of dynamic halting, which affects how\nthe attention mechanism works in the SUT, and the\nAdaptive Computation Time (ACT) auxiliary loss\nwe use to minimize the number of layers used.\n3.1 Sparse Mixture of Experts\nA Mixture of Experts module consists of E sub-\nmodules f1,...,f E. There is also a gating network,\nwhich we will denote by g(e|h) – for any input\nh to the MoE module, the gating network would\npredict a distribution over the E experts. When\nk < E, we refer to this as a Sparse Mixture of\nExperts (SMoE), and g(e |h) > 0 for only kex-\n1https://ai.googleblog.com/2019/12/\nalbert-lite-bert-for-self-supervised.html\nperts, while maintaining that ∑E\ne g(e |h) = 1.\nThe final output of the SMoE is then given by\ny = ∑E\ne=1 g(e |h) ·fe(h), where g(e |h) = 0,\nfe(h) will not need to be evaluated, reducing\ncomputation cost during training and inference.\nWe replace the Feed-forward layer (FFD) in the\nTransformer block with a mixture of FFDs. Each\nMixture of FFD can be described with a 3-tuple,\n(E,k,D ): Eexperts, kfor the number of experts\nto be used in the top- k operation, and D for the\ndimension of the hidden layer for each FFD expert.\nFor the attention layer, we use the Mixture of Multi-\nhead Attention (MoMHA) proposed by Zhang et al.\n(2022) and Shen et al. (2023). Each MoMHA mod-\nule can be described by a 5-tuple,(E,k,H,D,W ),\nwith Erepresenting the number of experts, Krep-\nresenting the parameter kin a top-koperation, H\nrepresenting the number of attention heads per ex-\npert, and Dfor the dimensions per head. Like in\nMoA, MoMHA maintains only a single set of H\nkey-value projections shared among the experts,\nwhile there are E query projections of H heads\neach. W represents the relative position embed-\nding window size, parameterizing 2W + 1embed-\ndings for W positions before and after the present\nposition. Figure 3 (Left) shows the schematic of a\nSUT block.\nThis technique has been used to reduce compu-\ntation costs both during training and inference time\nfor large models.\nMutual Information Maximisation Like other\nmodels that rely on conditional activation, auxiliary\nlosses are needed in order to aid learning a mod-\nule that decides which experts are activated, and\nto ensure that all experts are used, balancing the\nload for processing. For this, we use the Mutual\nInformation Maximization introduced in Chen et al.\n(2022) for the auxiliary loss (to be maximised):\nLMIM =\nE∑\ne=1\ng(e) logg(e)\n  \n−H(e)\n− 1\n|X|\n∑\nh∈X\nE∑\ne=1\ng(e|h) logg(e|h)\n  \nH(e|h)\n, (1)\nwhere,\ng(e) = 1\n|X|\n∑\nh∈X\ng(e|h)\n171\nMLP 1\nRouter\nMLP 2 MLP 3 MLP 4\nLayer Norm\nLayer Norm\nAtt 1\nRouter\nAtt 2 Att 3 Att 4\nHalting\nFigure 3: Left: Schematic of a SUT block. Right: While the input of each SUT block is the output of the previous\nlayer, the attention mechanism attends to the halted state of the timestep. When the halting probability exceeds\nαthresh, the hidden state is simply copied. Finally, the halted state is used as the output of the SUT.\n. Specifically, we use the unsupervised version pro-\nposed by Shen et al. (2023) that assumes a uniform\ndistribution over all tokens and layers, resulting\nin the following auxiliary objective. In the SUT\nsetting, the gating network is used |X| = L·T\ntimes, where Lis the number of layers, and T is\nthe number of timesteps.\nIntuitively, the entropy term increases the en-\ntropy of the marginal probability of the gating net-\nwork predictions, which at its maximum means that\nthe weight for each gating network across the entire\nminibatch is uniform. The conditional entropy term\ndecreases the conditional entropy, which causes the\nprediction of the gating network to be sharp, and\nalso penalizes the uniform distribution solution for\nthe gating network.\n3.2 Stick-breaking Dynamic Halting\nThere have been several methods for imbuing\nmodels with the ability to make a prediction with-\nout having to use all layers of the model (Graves,\n2016; Tan and Sim, 2016; Dehghani et al., 2018;\nElbayad et al., 2019; Schuster et al., 2022). Moti-\nvations for this include: (1) different inputs require\ndifferent amounts of iteration to make a prediction,\n(2) reducing computation cost.\nUT implements a similar mechanism, but the UT\nversion of halting is difficult to interpret. Here we\nchoose a principled version of the dynamic halting\nmechanism based on the stick-breaking process,\nviewing it as a probability distribution. First, ˆα(t)\nl\nare the halting probabilities predicted by halt(h(t)\nl )\nAlgorithm 1 Halting mechanism at a given\ntimestep t\nfor l= 1to Ldo\nif ∑l−1\nl′=1 α(t)\nl′ <αthresh then\nˆα(t)\nl−1 = halt(h(t)\nl−1)\nα(t)\nl−1 = ˆα(t)\nl−1\nl−2∏\nl′=1\n(1 − ˆα(t)\nl′ )\na(t)\nl = Attention(h(t)\nl−1\nQ\n,Sl−1\nK\n,Sl−1\nV\n)\nh(t)\nl = FeedForward(h(t)\nl−1,a(t)\nl )\ns(t)\nl =\n(\n1 − ∑l−1\nl′=1 α(t)\nl′\n)\n· h(t)\nl\n+\n(∑l−1\nl′=1 α(t)\nl′ · h(t)\nl′\n)\nelse\nh(t)\nl = h(t)\nl−1\ns(t)\nl = s(t)\nl−1\nend if\nend for\n, a function which is implemented by an MLP that\ntakes in the previous layer’s embedding. Then, the\nprobability of any layer halting is computed by\nα(t)\nl = ˆα(t)\nl\nl−1∏\nl′=1\n(1 −ˆα(t)\nl′ ). (2)\nA similar formulation is described in Graves (2016)\nand Tan and Sim (2016). Algorithm 1 shows\nhow the mechanism is implemented at any given\ntimestep. hl−1 is the output of the previous layer\nfor the current timestep.\nConditioned on the fact that we are computing\nh(t)\nl , time-step tmust not have halted before or at\nl−1. So we can use h(t)\nl , the unhalted state, as\n172\ninput to the computation of the attention query of\nthe block. However, since time-step tcan attend\nto all other timesteps, and it these other steps may\nhave halted, we use the halted states Sl−1 for the\nprevious layers.\nHowever, because the halting is a ‘soft’ decision,\nwe can relax the requirement for evaluating all pos-\nsible halted states and use the expected halted state\nas a substitute. Previous halting mechanisms use a\n‘gating’ mechanism of convex sums between pre-\nviously gated outputs and the current step’s output\nhl = αl ·ˆhl + (1−αl) ·hl−1 (Dehghani et al.,\n2018). This can lead to vanishingly small gradients\ngoing up the layers as (1 −αl) multiplies. We can\ninstead compute the expected halted embedding at\nany l,\ns(t)\nl =\n(\n1 −\nl−1∑\nl′=1\nα(t)\nl′\n)\n·h(t)\nl\n  \nprevious layer if not halted\n+\nl−1∑\nl′=1\nα(t)\nl′ h(t)\nl′\n  \nhalted at < l\n(3)\nIf α(t)\nl = 1 for some l, s(t)\nl = h(t)\nl , recovering\nthe behavior of the discrete halting decision. We\nuse s(t)\nl as input to the attention key and value\ntransformations.\nThis probabilistic interpretation also allows us\nto impose a loss on the expected number of layers\nused at each step, biasing the model towards fewer\niterations, thereby saving computational cost.\nLACT = 1\nT\nT∑\nt=1\nL∑\nl=1\nα(t)\nl ·l. (4)\nWe use a threshold αthresh = 0.999, such that\nthe cumulative sum of the halting probabilities has\nexceeded this, no computation will be performed\nfor that time step, and the previous layer’s embed-\ndings will be copied. Due to the routing operation\nrequired in the implementation fo SMoEs, we can\nsimply route halted states to a “No Op” expert,\nleading to real savings in computation cost when\nhalting hits the threshold early. We find that ad-\njusting this threshold after training can maintain\nperformance while saving computation steps.\n4 Experiments\nFirst, we show that we can scale the UT with\nSUT on the WMT’14 English-German (Bojar et al.,\n2014) translation task. We then ran experiments on\nCompositional Freebase Questions (CFQ; Keysers\net al. 2019) to test for compositional generalization\nTable 1: BLEU score on WMT14 En-De translation\ndatasets. MACs (Multiply–Accumulate Operations) 1\nmeasures the computational complexity of each model.\naVaswani et al. (2017), bLiu et al. (2020), cPeng et al.\n(2020), dZhang et al. (2022), eDehghani et al. (2018),\nf Myle et al. (2018), gWu et al. (2018)\nModel #Params BLEU MACs 1\nTransformer basea 65M 27.3 604M\nAdmin 6L-6Lb 61M 27.7 604M\nMAE-7 c 63M 28.4 -\nMoA based 65M 28.4 628M\nUTe 65M 28.9 -\nUT base + SB halting 64M 29.3 1998M\nSUT base 66M 29.2 787M\nTransformer bigf 210M 29.3 2090M\nLightConvg 202M 28.9 1750M 2\nDynamicConvg 213M 29.7 1790M 2\nAdmin 18L-18Lh 151M 29.0 1490M\nAdmin 60L-12Li 256M 30.1 2550M\nMoA bigd 200M 29.4 1220M\nUT big + SB halting 105M 29.6 3707M\nSUT big 110M 29.4 787M\nTable 2: Ablation Study. “– MIM loss” means replacing\nthe MIM loss with the load balancing loss used in (Fedus\net al., 2021). “– MoMHA” means replacing MoMHA\nwith the MoA introduced in (Zhang et al., 2022).\nModel Valid loss BLEU #Params\nSUT base 2.192 29.2 66M\n– MIM loss 2.221 28.9 66M\n– MoMHA 2.232 28.7 66M\n– ACT loss 2.217 29.0 66M\n– halting 2.219 29.1 65M\nproperties. To further analyze the behaviour of the\nmodel under compositional generalization settings,\nwe test our model on the Logical inference task\nfrom (Bowman et al., 2015). All experiments were\nimplemented within the Fairseq framework (Ott\net al., 2019) 2.\n4.1 English to German Translation\nWe perform experiments on the WMT’14 English-\nGerman translation dataset (Bojar et al., 2014). We\nuse the pre-processing from Liu et al. (2020). We\nuse a joined dictionary and share all word embed-\ndings of the encoder and decoder. For evaluation,\nwe average the last 5 best models according to\ntheir negative log-likelihood scores. We report the\nBLEU scores (Papineni et al., 2002), and also re-\nport the MACs (Multiply-Accumulate Operations)\nto evaluate the runtime computational costs of the\n2Fairseq-based implementation available on: https://\ngithub.com/shawntan/sut\n173\nTable 3: FFD Expert-Word co-occurrences.\nExp. 6 17 41 46Top 5\na he ed team\ntheir they ing children\nhis his ted police\nthis He y devices\nan you red system\nDet. Pronouns Suffixes Nouns\ndifferent models. MACs of previous models were\ncomputed in Zhang et al. (2022).\nThe results are reported in Table 1. We compare\nagainst strong baselines while accounting for the\nnumber of parameters in these models. In addition,\nwe train two UTs by setting E = 1,k = 1, and\nparameterizing the FFD and Attention layers with\nparameters to match our ∼65M, and ∼110M set-\nting for SUT. The SUTs and UTs both demonstrate\ngood parameter efficiency when compared to pre-\nvious models. In the ∼110M parameter class, SUT\nand UT perform at around 29.4 and 29.6 BLEU re-\nspectively, while previous models require∼200M\nparameters. While the SUT does not perform as\nwell as the UT, but the computations required dur-\ning runtime could be as low as one-fifth of UT.\nAlso, because we keep k constant for SUT, the\nMACs stays constant as SUT scales up.\nWe ran experiments removing different aspects\nof the model and its training process, including:\nMIM auxiliary loss, Mixture of MHA, the ACT\nloss, and the halting mechanism. The results are\nin Table 2. The introduction of multiple heads to\nthe MoA was crucial in seeing performance gains\non this task, as well as having the MIM loss as a\nload-balancing auxiliary objective. Interestingly,\nhalting does contribute as much of a performance\ngain as it does in CFQ.\nAdditionally, we compute the top 5 tokens that\noccur in conjunction with each expert, regardless\nof layers, and find that certain associations exist.\nWe pick several experts in Table 3 that show a clear\nsign of co-occurring with tokens that seem to show\na pattern. This suggests that while there may be\nredundancy between the experts, groups of experts\ncan specialize on certain tasks, resulting in some\nmodularity. Future work can investigate if such\n1The open-source tool PTFLOPS (https://github.com/\nsovrasov/flops-counter.pytorch) is used to calculate the\nMACs.\n2The MACs values of DynamicConv and LightConv are\nunderestimated. Because the PTFLOPS does not support the\ncustomized convolution layers.\nmodularity can result in more robust generalization.\n4.2 Compositional Freebase Questions\nWe run experiments on the Compositional Freebase\nQuestions (CFQ; Keysers et al. 2019) dataset to de-\ntermine the compositional generalization abilities\nof the SUT. This is a translation task from natu-\nral language to a SPARQL query. As an example,\nthe sequence Who wrote M1 and wrote a film\nwould be translated to the target sequence SELECT\nDISTINCT ?x0 WHERE { ?x0 a people.person\n. ?x0 film.writer.film ?x1 M1 . ?x1 a\nfilm.film }. CFQ tests for compositional gener-\nalization using the notion of compound divergence,\nwhich measures how different the training set and\ntest set are in terms of combinations of tokens,\nwhich they refer to as compounds. To our knowl-\nedge, the current best-performing models either\nfinetune a pretrained language model or, use knowl-\nedge about the task to design a suitable prompt for a\nlarge language model (Drozdov et al., 2022). While\nthe prompting approach is extremely effective at\nthe CFQ task, we view the task as a benchmark for\ncompositional generalization in general and should\nbe viewed in concert with other experiments, es-\npecially real-world data (like translation). When\nusing domain knowledge of the task in the prompt,\nthe results may indicate better performance with a\nspecific approach for CFQ (and perhaps other SQL\ntranslation tasks) but might be difficult to extrapo-\nlate to other settings.\nIn our experiments, we use preprocessing scripts\nfrom Zheng and Lapata (2021). The scripts per-\nform preprocessing to the target sequence that\nsimplifies the target sequence the same way per-\nformed in Furrer et al. (2020). Accordingly, we\ntrain a baseline Transformer on the transformed\ntarget. We performed a search on the SUT hyper-\nparameters, using the MCD1 validation set, and\nthe best-performing set of parameters are Attention\n(E = 1,k = 1,H = 8,D = 64,W = 1)and FFD\n(E = 1,k = 1,D = 1024), which corresponds\nto the UT setting. Refer to Appendix A for fur-\nther details. Since CFQ is a relatively small task,\nlarger scale is not a factor and might suggest that\nexpert specialization may not be as helpful. The\nresults are shown in Table 4. In cases with and\nwithout halting, the model already outperforms pre-\nvious benchmarks, including the UT baseline from\nBergen et al. (2021). For a fairer comparison, we\nuse the same hyperparameters as our UT imple-\n174\nTable 4: CFQ Results. Results on UT are an average of 5 runs on different seeds.\nModel Pretraining MCD1 MCD2 MCD3 Avg. MACs 1\nT5-based UT (Bergen et al., 2021) /enc-3742.7 9.5 11.6 21.3 1154M\nEdge Transformer (Bergen et al., 2021) /enc-3747.7 13.1 13.2 24.7 6504M\nTransformer (Keysers et al., 2019) /enc-3742.5 11.2 10.6 21.4 1154M\nT5 (Furrer et al., 2020) /enc-3361.6 31.3 33.3 42.1 1154M\nRoberta (Zheng and Lapata, 2021) /enc-3360.6 33.6 36.0 43.4 1660M\nDangle (Zheng and Lapata, 2021) /enc-3378.3 59.5 60.4 66.1 51033M\nT5-based UT (ours) /enc-3768.3 ± 2.9 43.1 ± 1.5 45.7 ± 1.8 52.3 ± 1.6 441M\nUT w/o halting /enc-3771.0 ± 3.5 48.6 ± 2.3 51.3 ± 0.2 56.9 ± 1.5 654M\nUT with halting /enc-3772.4 ± 3.5 51.1 ± 1.8 51.7 ± 2.3 58.4 ± 1.2 654M\nTable 5: Test accuracy of the models, trained on op-\neration lengths of ≤6, with their out-of-distribution\nresults shown here (lengths 7-12). LSTM baseline from\nBowman et al. (2015), and Transformer baseline from\nShen et al. (2019)\nModel Number of Operations Comp. Gen.\n7 8 9 10 11 12 A B C\nLSTM 88 84 80 78 71 69 80 60 59\nTransformer 51 52 51 51 51 48 53 51 51\nSUT 98 97 94 90 88 81 97 94 52\nmentation, we modify our UT implementation to\nbe more similar to the T5-based UT in Bergen et al.\n(2021). These changes include: the bucketed rela-\ntive position bias used by T5, and going from post\nlayer-norm to pre layer-norm. While this results\nin much improved results compared to the original\npaper, our implementation of UT still outperforms\nit.\nThe Dangle (Zheng and Lapata, 2021) model,\nwhich beats our model, also requires re-running\nthe encoder for every token decoded. This is an\nexpensive process, but given that both our method\nand Dangle perform well at this task, is additional\nevidence that iterative processes are beneficial for\ncompositional generalization.\n4.3 Logical Inference\nWe use the logical inference task from (Bowman\net al., 2015) as a test bench for UT. Despite the\napparent simplicity of the language, the task inher-\nently requires the model to learn the hierarchical\nstructure of the problem. Each instance of the task\ncomprises of two logical statements, and the goal\nis to predict if the statements are equivalent, con-\ntradictory, disjoint, or entail in either direction. For\nexample, given s1 = a and s2 = a ( or b ) ,\nthen s1 ⊏ s2. The crux of the task is in training\nthe model on sequences that have 0-6 logical op-\nerators and evaluating it on sequences that have\n7-12 operators. Given our sequence-to-sequence\nsetting, we convert the task into a translation task.\nThe model takes sentence1 #SEP# sentence2as\nits source sentence, with the target sentence being\nthe single-token label for that pair.\nWe train a 12 layer model with Attention (E =\n12,k = 4,H = 2,D = 32,W = 1) and FFD\n(E = 12,K = 4,D = 128) and halting. Refer\nto Appendix A for further details. Training a 12-\nlayer Vanilla Transformer achieves approximately\nthe same results as in Shen et al. (2019), so we\nreport their results. Our results in Table 5 confirm\nthe findings of Tran et al. (2018), showing that with\nrecurrence in SUTs, we are able to generalize to\nlonger sequences of the task. While there are other\nmodels that induce a tree structure that performs\nexceedingly well on the task, we wanted to evaluate\nour model against other popular architectures. The\nLSTM is a strong baseline, and we find that UT\noutperforms it in generalization. We also evaluate\nUTs on the compositional generalization splits as\nproposed in (Shen et al., 2019), where the splits A,\nB, and C are in increasing difficulty. The results\nshow that UTs are able to generalize better for\nthe A and B splits, outperforming the LSTM and\nVT. Split C is still presents a challenge for the\nTransformer variants.\nAdditionally, we compute the average halting\ndepth for the test data segmented by operator\ncounts. Because more operators require more nest-\ning of expressions in the sequences, more recur-\nsion is required to properly parse the sequence. As\nexpected, in Figure 4, the average halting depth\nincreases as more operators are used. The operator\ncount for these clauses are correlated with length,\n175\n2 4 6 8 10 12\nNumber of operators\n5\n6\n7\n8\n9Average dynamic halting step\nFigure 4: The average dynamic halting depth of the UT\nmodel as the number of operators increases in the test\nset. The model learns to think more when the problem\nis harder.\nwhich suggests that SUTs may be suited to gener-\nalize for length. We include further experiments on\nlength generalization in the Appendix Table 8.\n4.4 Post-training Computation Reduction\nDoes lowering αthresh after training cause the\nmodel to halt earlier, saving computation? How\nmuch would that cost us in terms of accuracy?\nWe estimate the skipped SUT block com-\nputations given different values of αthresh ∈\n{0.1,0.2,..., 0.9}by looking at the halting pat-\nterns of the decoder given the ground truth source-\ntarget pairs. We pass the source-target pair into\nthe model and analyze the halting patterns of the\nmodel, giving us a rough estimate of how much\ncomputation would be saved as a percentage of\ncomputing all layers of the SUT.\nLogical Inference We observe the resulting per-\nformance on the hardest split of the test set with\n12 operations. Due to the already saturated halting\npattern, the halting probability αl spikes rapidly\nfrom close to 0 to higher values, resulting in a near\nconstant ∼50% reduction of the computation time\nregardless of the threshold.\nCFQ Using the MCD1 test split of the dataset,\nand our best-performing model on MCD1, we per-\nform the αthresh adjustment. The halting patterns\nreflect the repeated structure of SQL, using fewer\nsteps for ‘.‘ and ‘WHERE‘, while the main bulk of\nthe region within {...} requires more SUT steps\nbefore halting. Surprisingly, when0.8 ≤αthresh ≤\n0.999, the accuracy remains fairly constant. An es-\ntimated 33% of the computation steps were skipped\nat αthresh = 0.8. At αthresh = 0.1, there is a slight\nincrease in the number of computed steps, which\nFigure 5: Above: Plot of 1−∑l−1\nl′=1 α(t)\nl′ , for an example\nLogical Inference input — x-axis: timesteps, y-axis:\nlayers. This visualizes the halting pattern of the model:\ndark blue represents halted, while yellow represents\nactive. Below: Efficiency vs. Performance tradeoff\ncurves when αthresh is adjusted.\nis possible since halting earlier will result in dif-\nferent embeddings, and result in different halting\ndecisions in other timesteps. Overall, the results\nsuggest that we can save about 20% of the SUT\ncomputation steps without any drop in accuracy,\nand about ∼50% for a 0.2% decrease.\nEnglish-German Translation For this larger\ndataset, we find that these translation models halt\nmuch later, suggesting that the translation task re-\nquires more computational steps than the 6-layer\nSUT we used. However, further increasing the\nnumber of layers to 12 layers does not bring about\nmuch benefit, as evidenced by the halting in Figure\n4, which is an example of the halting mechanism\nusing nearly all layers. For comparison, Admin\n60L-12L model, requires a 60-layer encoder to\nachieve its performance. Even when αthresh = 1,\nthe skipped computation steps remain at about 33%,\ncompared to 80% in the CFQ task. We find that\nwe can reduce the computation by 9% while still\n176\nFigure 6: Halting plot and trade-off curves for CFQ.\n(See Figure 5 for description)\nretaining a BLEU score of 29.1.\n5 Conclusion\nWe show that it is possible to scale up the UT\nvia SUTs, and SUTs outperforms models of the\nsame capacity in the WMT’14 English-to-German\ntranslation task. The recursive nature of both\nUTs and SUTs allows for better inductive biases,\nwhich we have demonstrated in synthetic tasks like\nCFQ and logical inference. VTs have been shown\nto be poor at these compositional generalization\ntasks without additional domain knowledge. The\nstick-breaking dynamic halting mechanism also al-\nlows post-training adjustment of computation cost,\nwhich is a boon for deployment at scale.\nLimitations While the experiments in this pa-\nper show the desirable generalization properties\nof UTs, there are some aspects of compositional\ngeneralization that SUTs do not solve. Importantly,\nwhile we demonstrate scaling UTs up via SMoEs,\nfurther experiments on larger settings are needed\nFigure 7: Halting plot and trade-off curves for English-\nGerman Translation. (See Figure 5 for description)\nto ascertain viability in large scale systems. Other\nissues may also crop up in the further scaling of\nSUTs, but we believe there is ample literature to\ndraw on to finding solutions for these problems.\nReferences\nLeon Bergen, Timothy O’Donnell, and Dzmitry Bah-\ndanau. 2021. Systematic generalization with edge\ntransformers. Advances in Neural Information Pro-\ncessing Systems, 34:1390–1402.\nOndˇrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve Saint-\nAmand, et al. 2014. Findings of the 2014 workshop\non statistical machine translation. In Proceedings of\nthe ninth workshop on statistical machine translation,\npages 12–58.\nSamuel R Bowman, Christopher D Manning, and\nChristopher Potts. 2015. Tree-structured composi-\ntion in neural networks without tree-structured archi-\ntectures. arXiv preprint arXiv:1506.04834.\nZitian Chen, Yikang Shen, Mingyu Ding, Zhenfang\n177\nChen, Hengshuang Zhao, Erik Learned-Miller, and\nChuang Gan. 2022. Mod-squad: Designing mix-\nture of experts as modular multi-task learners. arXiv\npreprint arXiv:2212.08066.\nKenneth Church and Patrick Hanks. 1990. Word associ-\nation norms, mutual information, and lexicography.\nComputational linguistics, 16(1):22–29.\nRóbert Csordás, Kazuki Irie, and Jürgen Schmidhu-\nber. 2021. The devil is in the detail: Simple tricks\nimprove systematic generalization of transformers.\narXiv preprint arXiv:2108.12284.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Łukasz Kaiser. 2018. Universal\ntransformers. arXiv preprint arXiv:1807.03819.\nGrégoire Delétang, Anian Ruoss, Jordi Grau-Moya,\nTim Genewein, Li Kevin Wenliang, Elliot Catt, Mar-\ncus Hutter, Shane Legg, and Pedro A Ortega. 2022.\nNeural networks and the chomsky hierarchy. arXiv\npreprint arXiv:2207.02098.\nAndrew Drozdov, Nathanael Schärli, Ekin Akyürek,\nNathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. 2022. Compositional\nsemantic parsing with large language models. arXiv\npreprint arXiv:2209.15003.\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine\nLi, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra\nBhagavatula, Ronan Le Bras, Jena D Hwang, et al.\n2023. Faith and fate: Limits of transformers on com-\npositionality. arXiv preprint arXiv:2305.18654.\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael\nAuli. 2019. Depth-adaptive transformer. arXiv\npreprint arXiv:1910.10073.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity.\nDaniel Furrer, Marc van Zee, Nathan Scales, and\nNathanael Schärli. 2020. Compositional generaliza-\ntion in semantic parsing: Pre-training vs. specialized\narchitectures. arXiv preprint arXiv:2007.08970.\nAlex Graves. 2016. Adaptive computation time\nfor recurrent neural networks. arXiv preprint\narXiv:1603.08983.\nMichael Hahn. 2020. Theoretical limitations of self-\nattention in neural sequence models. Transactions of\nthe Association for Computational Linguistics, 8:156–\n171.\nYiding Hao, Dana Angluin, and Robert Frank. 2022.\nFormal language recognition by hard attention trans-\nformers: Perspectives from circuit complexity. Trans-\nactions of the Association for Computational Linguis-\ntics, 10:800–810.\nDieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia\nBruni. 2020. Compositionality decomposed: How\ndo neural networks generalise? Journal of Artificial\nIntelligence Research, 67:757–795.\nŁukasz Kaiser and Ilya Sutskever. 2015. Neural gpus\nlearn algorithms. arXiv preprint arXiv:1511.08228.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nDaniel Keysers, Nathanael Schärli, Nathan Scales,\nHylke Buisman, Daniel Furrer, Sergii Kashubin,\nNikola Momchev, Danila Sinopalnikov, Lukasz\nStafiniak, Tibor Tihon, et al. 2019. Measuring com-\npositional generalization: A comprehensive method\non realistic data. arXiv preprint arXiv:1912.09713.\nYoon Kim. 2021. Sequence-to-sequence learning with\nlatent neural grammars. Advances in Neural Infor-\nmation Processing Systems, 34:26302–26317.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nYuanpeng Li, Liang Zhao, Jianyu Wang, and Joel Hest-\nness. 2019. Compositional generalization for primi-\ntive substitutions. arXiv preprint arXiv:1910.02612.\nBingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Kr-\nishnamurthy, and Cyril Zhang. 2022. Transform-\ners learn shortcuts to automata. arXiv preprint\narXiv:2210.10749.\nXiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng\nGao. 2020. Very deep transformers for neural ma-\nchine translation. arXiv preprint arXiv:2008.07772.\nWilliam Merrill, Ashish Sabharwal, and Noah A Smith.\n2022. Saturated transformers are constant-depth\nthreshold circuits. Transactions of the Association\nfor Computational Linguistics, 10:843–856.\nOtt Myle, Edunov Sergey, Grangier David, Auli\nMichael, et al. 2018. Scaling neural machine transla-\ntion. WMT, pages 1–9.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for se-\nquence modeling. arXiv preprint arXiv:1904.01038.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\n178\nHao Peng, Roy Schwartz, Dianqi Li, and Noah A Smith.\n2020. A mixture of h-1 heads is better than h heads.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 6566–\n6577.\nJake Russin, Jason Jo, Randall C O’Reilly, and Yoshua\nBengio. 2019. Compositional generalization in a\ndeep seq2seq model by separating syntax and seman-\ntics. arXiv preprint arXiv:1904.09708.\nTal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani,\nDara Bahri, Vinh Q Tran, Yi Tay, and Donald Metzler.\n2022. Confident adaptive language modeling. arXiv\npreprint arXiv:2207.07061.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538.\nYikang Shen, Shawn Tan, Arian Hosseini, Zhouhan Lin,\nAlessandro Sordoni, and Aaron C Courville. 2019.\nOrdered memory. Advances in Neural Information\nProcessing Systems, 32.\nYikang Shen, Zheyu Zhang, Tianyou Cao, Shawn\nTan, Zhenfang Chen, and Chuang Gan. 2023.\nModuleformer: Learning modular large language\nmodels from uncurated data. arXiv preprint\narXiv:2306.04640.\nVsevolod Sourkov. 2018. Igloo: Slicing the fea-\ntures space to represent sequences. arXiv preprint\narXiv:1807.03402.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nProceedings of the IEEE conference on computer\nvision and pattern recognition, pages 2818–2826.\nSho Takase and Shun Kiyono. 2021. Lessons on pa-\nrameter sharing across layers in transformers. arXiv\npreprint arXiv:2104.06022.\nShawn Tan, Yikang Shen, Timothy J O’Donnell,\nAlessandro Sordoni, and Aaron Courville. 2020. Re-\ncursive top-down production for sentence generation\nwith latent trees. arXiv preprint arXiv:2010.04704.\nShawn Tan and Khe Chai Sim. 2016. Towards implicit\ncomplexity control using variable-depth deep neural\nnetworks for automatic speech recognition. In 2016\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 5965–5969.\nIEEE.\nYi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won\nChung, William Fedus, Jinfeng Rao, Sharan Narang,\nVinh Q Tran, Dani Yogatama, and Donald Metzler.\n2022. Scaling laws vs model architectures: How\ndoes inductive bias influence scaling? arXiv preprint\narXiv:2207.10551.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2020. Long\nrange arena: A benchmark for efficient transformers.\narXiv preprint arXiv:2011.04006.\nKe Tran, Arianna Bisazza, and Christof Monz. 2018.\nThe importance of being recurrent for modeling hier-\narchical structure. arXiv preprint arXiv:1803.03585.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2018. Pay less attention with\nlightweight and dynamic convolutions. In Interna-\ntional Conference on Learning Representations.\nXiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou,\nWenge Rong, and Zhang Xiong. 2022. Mixture of\nattention heads: Selecting attention heads per token.\narXiv e-prints, pages arXiv–2210.\nHao Zheng and Mirella Lapata. 2021. Disentangled\nsequence to sequence learning for compositional gen-\neralization. arXiv preprint arXiv:2110.04655.\n179"
}