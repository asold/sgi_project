{
  "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
  "url": "https://openalex.org/W4221145199",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2736809814",
      "name": "Wagner, Johannes",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A4221672382",
      "name": "Triantafyllopoulos, Andreas",
      "affiliations": [
        "University of Augsburg"
      ]
    },
    {
      "id": "https://openalex.org/A2215262865",
      "name": "Wierstorf, Hagen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A4221672384",
      "name": "Schmitt, Maximilian",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A4221672385",
      "name": "Burkhardt, Felix",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2279702709",
      "name": "Eyben, Florian",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A4221198056",
      "name": "Schuller, Björn W.",
      "affiliations": [
        null
      ]
    },
    {
      "id": null,
      "name": "Schuller, Bj\\\"orn W.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3208152093",
    "https://openalex.org/W4297841899",
    "https://openalex.org/W3082167223",
    "https://openalex.org/W3015988193",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W2785722081",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W2052666245",
    "https://openalex.org/W2972852081",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2972372393",
    "https://openalex.org/W2966384645",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W3209059054",
    "https://openalex.org/W6780218876",
    "https://openalex.org/W2746763037",
    "https://openalex.org/W2897444637",
    "https://openalex.org/W2128837546",
    "https://openalex.org/W4393695169",
    "https://openalex.org/W3007157104",
    "https://openalex.org/W2811466185",
    "https://openalex.org/W3161663055",
    "https://openalex.org/W2399733683",
    "https://openalex.org/W2556418146",
    "https://openalex.org/W3094550259",
    "https://openalex.org/W2313339984",
    "https://openalex.org/W2973034847",
    "https://openalex.org/W2910165986",
    "https://openalex.org/W6802103249",
    "https://openalex.org/W4221089191",
    "https://openalex.org/W2003653478",
    "https://openalex.org/W2117645142",
    "https://openalex.org/W2156503193",
    "https://openalex.org/W6804030475",
    "https://openalex.org/W6802301941",
    "https://openalex.org/W3162890625",
    "https://openalex.org/W1533303231",
    "https://openalex.org/W2972935927",
    "https://openalex.org/W2742542661",
    "https://openalex.org/W6803979805",
    "https://openalex.org/W3162811262",
    "https://openalex.org/W1966797434",
    "https://openalex.org/W2803098682",
    "https://openalex.org/W3119308075",
    "https://openalex.org/W3198771897",
    "https://openalex.org/W6799856993",
    "https://openalex.org/W3193714551",
    "https://openalex.org/W2781692313",
    "https://openalex.org/W3197642003",
    "https://openalex.org/W3197580070",
    "https://openalex.org/W6803378298",
    "https://openalex.org/W6680300913",
    "https://openalex.org/W6931421704",
    "https://openalex.org/W6776129198",
    "https://openalex.org/W6782436372",
    "https://openalex.org/W3215440557",
    "https://openalex.org/W6802546489",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W3169022486",
    "https://openalex.org/W3100511085",
    "https://openalex.org/W4285111045"
  ],
  "abstract": "Recent advances in transformer-based architectures which are pre-trained in self-supervised manner have shown great promise in several machine learning tasks. In the audio domain, such architectures have also been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of .638 on MSP-Podcast. Furthermore, our investigations reveal that transformer-based architectures are more robust to small perturbations compared to a CNN-based baseline and fair with respect to biological sex groups, but not towards individual speakers. Finally, we are the first to show that their extraordinary success on valence is based on implicit linguistic information learnt during fine-tuning of the transformer layers, which explains why they perform on-par with recent multimodal approaches that explicitly utilise textual information. Our findings collectively paint the following picture: transformer-based architectures constitute the new state-of-the-art in SER, but further advances are needed to mitigate remaining robustness and individual speaker issues. To make our findings reproducible, we release the best performing model to the community.",
  "full_text": "DAWN OF THE TRANSFORMER ERA IN SPEECH EMOTION\nRECOGNITION : CLOSING THE VALENCE GAP ∗\nJohannes Wagner1, Andreas Triantafyllopoulos2, Hagen Wierstorf1, Maximilian Schmitt1,\nFelix Burkhardt1, Florian Eyben1, Bj¨orn W. Schuller1,2,3\n1 audEERING GmbH, Gilching, Germany\n2 EIHW, University of Augsburg, Augsburg, Germany\n3 GLAM, Imperial College, London, UK\nSeptember 11, 2023\nABSTRACT\nRecent advances in transformer-based architectures have shown promise in several machine learning\ntasks. In the audio domain, such architectures have been successfully utilised in the field of speech\nemotion recognition (SER). However, existing works have not evaluated the influence ofmodel size\nand pre-training data on downstream performance, and have shown limited attention to generalisa-\ntion, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of\nthese aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the\ndimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP\nand MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top per-\nformance for valence prediction without use of explicit linguistic information, with a concordance\ncorrelation coefficient (CCC) of .638 on MSP-Podcast. Our investigations reveal that transformer-\nbased architectures are more robust compared to a CNN-based baseline and fair with respect to\ngender groups, but not towards individual speakers. Finally, we show that their success on valence is\nbased on implicit linguistic information, which explains why they perform on-par with recent multi-\nmodal approaches that explicitly utilise textual information. To make our findings reproducible, we\nrelease the best performing model to the community.\nKeywords Affective Computing · Speech Emotion Recognition · Transformers.\n1 Introduction\nAutomatic speech emotion recognition (SER) is a key enabling technology for facilitating better human-to-machine\ninteractions [1]. SER research is dominated by two conceptual paradigms: discrete emotions [2] and emotional di-\nmensions [3]. The first investigates emotional categories like happy or sad, while the latter focuses on the dimensions\nof arousal, valence, and dominance [3].\nA SER system achieves this through the linguistic ( what has been said) or the paralinguistic ( how it has been said)\nstream [1, 4, 5]. The linguistic stream is better suited for valence recognition [6, 7] and can draw from recent advances\nin automatic speech recognition (ASR) and natural language processing (NLP) [8], but might be limited to a single\nlanguage. Paralinguistics works better for arousal and dominance [6, 7] and has the potential to generalise across\ndifferent languages. Both paradigms can be combined in bimodal architectures [4], which require to execute several\ndifferent models. Instead, we aim towards a model that only implicitly utilises the linguistic information stream during\ndeployment, and does not require access to ASR and NLP frontends.\n∗Citation: Wagner, J., Triantafyllopoulos, A., Wierstorf, H., Schmitt, M., Burkhardt, F., Eyben, F., & Schuller, B. W.\n(2023). Dawn of the transformer era in speech emotion recognition: closing the valence gap. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence. 10.1109/TPAMI.2023.3263585\narXiv:2203.07378v4  [eess.AS]  7 Sep 2023\nDawn of the transformer era in speech emotion recognition PREPRINT\nAlthough the field has seen tremendous progress in the last decades [1], three major challenges remain for real-world\nparalinguistics-based SER applications: a) improving on its inferior valence performance [7, 9], b) overcoming issues\nof generalisation and robustness [10, 11], and c) alleviating individual- and group-level fairness concerns, which is a\nprerequisite for ethical emotion recognition technology [12, 13]. Previous works have attempted to tackle these issues\nin isolation, but combining them is not straightforward.\nIn recent years, the artificial intelligence (AI) field is undergoing a major paradigm shift, moving from specialised ar-\nchitectures trained for a given task to general-purposefoundation models that can be adapted to several use-cases [14].\nSuch models have seen tremendous success in computer vision [15, 16], NLP [17], and computer audition [18, 19],\nincluding SER [20, 21]. Among others, wav2vec 2.0 [18] and HuBERT [19] have emerged as foundation model can-\ndidates for speech-related applications. We evaluate several publicly-available pre-trained variants of those models for\ndimensional SER, and show that they can achieve state-of-the art results for valence. We further analyze the influence\nof the model architecture, the pre-training data, how well the models generalise, their robustness, fairness, and effi-\nciency. Moreover, we make our best performing model publicly available [22]. To our best knowledge this is the first\ntransformer-based dimensional SER model released to the community. For an introduction on how to use it, please\nvisit: https://github.com/audeering/w2v2-how-to.\nThe remainder of this paper is organised as follows. Section 2 discusses related work, Section 3 presents the models,\ndatabases, and evaluation methods. Section 4 shows the results and investigates why transformer models are able\nto close the valence gap and improve performance with respect to robustness and fairness. Section 5 investigates\nefficiency improvements, before Section 6 summarises the results, and Section 7 concludes the paper.\n2 Related Work\nTable 1: State-of-the-art 4-class emotion recognition performance on IEMOCAP using transformer-based architec-\ntures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the\nbase (b) or large (L) architecture was used as well as whether the pre-trained model was fine-tuned for speech recog-\nnition (FT-SR). The column FT-D marks if the transformer layers were further fine-tuned during the down-stream\nclassification task.\nWork Model L FT-SR FT-D UAR W AR\n1 [23] w2v2-L ✓ 60.0\n2 [24] * w2v2-L ✓ 62.5 62.6\n3 [20] w2v2-b 63.4\n4 [25] w2v2-b 63.4\n5 [26] w2v2-b ✓ 63.8\n6 [20] hubert-b 64.9\n7 [25] hubert-b 64.9\n8 [20] w2v2-L ✓ 65.6\n9 [25] w2v2-L ✓ 65.6\n10 [26] w2v2-b 67.2\n11 [20] hubert-L ✓ 67.6\n12 [25] hubert-L ✓ 67.6\n13 [27] w2v2-b ✓ 69.9\n14 [28] w2v2-L ✓ 70.7\n15 [20] w2v2-b ✓ ✓ 73.8 (68.3)**\n16 [27] w2v2-b ✓ 74.3\n17 [20] hubert-b ✓ 76.6 (69.7)**\n18 [20] w2v2-L ✓ ✓ ✓ 76.8 (69.1)**\n19 [20] w2v2-b ✓ 77.0 (71.0)**\n20 [20] w2v2-L ✓ ✓ 77.5 (71.0)**\n21 [20] hubert-L ✓ ✓ ✓ 79.0 (73.0)**\n22 [20] hubert-L ✓ ✓ 79.6 (73.0)**\n* For a fair comparison we report the result on the utterance level. Au-\nthors report better performance on the phonetic level.\n** We updated the table with their speaker independent results in paren-\nthesis. Thanks to Dr. Leyuan Qu for the correction.\n2\nDawn of the transformer era in speech emotion recognition PREPRINT\nThe focus of our work is the recognition of emotional dimensions. However, most related studies target emotional\ncategories. Since the approaches are closely related, we consider both in this section.\nIn Table 1, we provide a summary of recent works based on wav2vec 2.0 and HuBERT on the IEMOCAP dataset [29],\non which most prior works have focused. Results are ranked by unweighted average recall (UAR) / weighted average\nrecall (W AR) on the four emotional categories of anger (1103 utterances), happiness (+ excitement) (1636), sadness\n(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply\nleave-one-session-out cross validation (5 folds), except Yuan et al. [24], using leave-one-speaker-out cross validation\n(10 folds), and Wang et al. [20], who do not explicitly mention which folds they used. Even though authors have used\ndifferent head architectures and training procedures in their studies, we can draw some general observations:\n1. Fine-tuning pre-trained weights yields a 10% boost.\n2. Additional ASR fine-tuning does not help with SER (e. g. row 15 vs row 19 −3.2%).\n3. The large architecture is typically better than the base one (e. g. row 17 vs row 22 +3.0%), but differences\ncan be quite small (e. g. row 19 vs row 20 +.5% ).\n4. HuBERT outperforms wav2vec 2.0 (e. g. row 22 vs row 20: +2.1%).\n5. When performing a fine-tuning of the transformer layers, a simple average pooling in combination with a lin-\near classifier built over wav2vec 2.0 or HuBERT as proposed by Wanget al. [20] seems sufficient and shows\nbest performance in the ranking. However, some of the more complex models like the cross-representation\nencoder-decoder model proposed by Makiuchi et al. [28] only report results without fine-tuning the pre-\ntrained model during the down-stream task.\nWhile the aforementioned studies have focused on emotional categories, there also exist several ones which concen-\ntrate on dimensions. The most comparable to ours is that of Srinivasan et al. [30], who fine-tuned wav2vec 2.0 /\nHuBERT on arousal, dominance, and valence. Their results show that pre-trained models are particularly good in\npredicting valence. When additionally joining audio embeddings from the fine-tuned models and text representations\nobtained with a pre-trained BERT model, they got a concordance correlation coefficient (CCC) for valence of.683 on\nthe MSP-Podcast corpus [31]. Furthermore, they were able to distill the multi-model system to an audio-only model\nusing student-teacher transfer learning, while still reaching a concordance correlation coefficient (CCC) of .627 (a\nmassive improvement compared to the previous state-of-the-art performance of only .377 [32]). However, this im-\nprovement was the result of cross-modal transfer learning, and it remains unclear whether speech-based architectures\nare by themselves able to reach such performance level – a fact we further explore in our work.\nThe presented results demonstrate the great potential of wav2vec 2.0 and HuBERT for emotion recognition. However,\nthe influence of pre-training data quantity and domain remains unclear. For instance, even though the large model\nshows consistently better performance, it is unclear if that can be attributed to the additional layers or to an 60 fold\nincrease of training data compared to the base model. Likewise there is little understanding on the impact of language,\nas previous work focused in pre-training on English speech data. In this contribution, we present a systematic com-\nparison of different models pre-trained under various conditions (e. g. including noisy speech) and evaluate them on\nseveral datasets (in-domain and cross-corpus).\nMoreover, it is important to show that SER models work well under noisy conditions. Oates et al. [10], Triantafyl-\nlopoulos et al. [11], Jaiswal and Provost [33], and Pappagari et al. [34] have shown that previous SER models suffer\nfrom robustness issues. We systematically investigate robustness of transformer-based models against a variety of\naugmentations that do not change the human perception of the underlying emotion [33].\nFinally, we consider fairness an important, but challenging topic for machine learning models. Discussions in the\nspeech processing community focus mainly on group fairness, e. g. gender [35]. For SER models, only a few eval-\nuations are available. Gorrostieta et al. [36] found a decrease in CCC for females compared to males for arousal in\nMSP-Podcast (v1.3) of .234. Besides group fairness, this contribution investigates individual fairness by estimating\nthe influence of the speaker on the model performance, which is a known problem for speaker verification models [37].\n3 Experimental setup\n3.1 Pre-trained models\nThroughout the paper, we discuss results obtained with transformer-based models pre-trained on large amounts of\nunlabelled data. We investigate two main variants: wav2vec 2.0 [18] and HuBERT [19]. The network architecture\nof both models is the same. As input, it expects a raw waveform normalised to have zero mean and unit variance,\n3\nDawn of the transformer era in speech emotion recognition PREPRINT\nTable 2: Transformer-based models included in this study and details on the data used during pre-training. Models\ncomprised of two architecture designs (wav2vec 2.0 and HuBERT), each with two different variants (base and large).\nFor each model, we list included dataset(s), total number of hours (h), number of languages (eng if only English), and\ncovered domains (Read speech, Telephone conversions, Parliamentary speech, Youtube).\nModel Datasets h Lang Domain\nw2v2-b [18] LibriSpeech 960 eng R\nhubert-b [19] LibriSpeech 960 eng R\nw2v2-L [18] LibriSpeech * 60k eng R\nhubert-L [19] Libri-Light 60k eng R\nw2v2-L-robust [38] Libri-Light (60k)\nFisher (2k)\nCommonV oice (700)\nSwitchboard (300)\n63k eng R, T\nw2v2-L-vox [39] V oxPopuli 100k 23 P\nw2v2-L-xls-r [40] V oxPopuli (372k)\nML LibriSpeech (50k)\nCommonV oice (7k)\nV oxLingua107 (6.6k)\nBABEL (1k)\n436k 128 R, T, P, Y\n* In our initial version, we erroneously specified Libri-Light as the training set of\nw2v2-L. However, it was actually trained on LibriSpeech, same as w2v2-b.\nwhich is fed into a feature encoder consisting of 7 convolutional layers that extracts feature vectors over time, with a\ndimensionality of 512 and a step size of20 ms. These features are projected to a higher dimension (768 or 1024 hidden\nunits, see below) and then fed into the encoder. The encoder is a series of transformer layers, each of them consisting\nof a multi-head self-attention module and several fully-connected layers. In order to inject temporal information, the\noutput of a convolutional layer is added at the input of the encoder.\nThe only difference between the main variants is the way they are pre-trained on unlabelled data. In wav2vec 2.0,\nthe features of a certain ratio of time steps are masked, by replacing them with a learnt fixed feature vector at the\ninput of the encoder. A contrastive loss between the encoder outputs and a quantised version of the input features is\nthen minimised [18]. In order to avoid learning too simple representations, the quantisation is done using a codebook,\nwhose diversity loss is minimised as well. In contrast, HuBERT minimises a cross-entropy loss for the masked time\nsteps, where the targets are not trained simultaneously with the model. The pre-training is performed in several steps,\nwhere in the first step, clusters obtained by k-means clustering of MFCCs are employed as targets and in later steps,\nclusters of the outputs of certain transformer layers are taken into account [19]. In following these strategies, the\nmodels try to learn the structure of speech, resulting in a reduced need for labelled task-specific training data.\nBoth wav2vec 2.0 and HuBERT exist in two forms: a base architecture with 12 transformer layers of 768 hidden\nunits each ( 95M parameters), and a large architecture with 24 transformer layers of 1024 hidden units each ( 317M\nparameters). Apart from that, we further distinguish them by the data used for pre-training. We included the four\nmodels found in previous work (cf. Section 2), which are pre-trained on English audiobooks, namely wav2vec2-\nbase (w2v2-b), hubert-base-ls960 (hubert-b), wav2vec2-large (w2v2-L), hubert-large-ll60k (hubert-L); the wav2vec2-\nlarge-robust model ( w2v2-L-robust), additionally trained on telephone speech; the wav2vec2-large-100k-voxpopuli\nmodel (w2v2-L-vox), trained only on parliamentary speech in multiple languages; and the wav2vec2-xls-r-300m model\n(w2v2-L-xls-r), trained on more than 400k hours across all domains and multiple languages. Compare Table 2 for\ncitations and an overview of the included data. We did not include models fine-tuned on speech recognition as previous\nwork showed that this does not lead to better performance. Also note that we refer to their fine-tuned versions when\nwe report results (cf. Section 3.2).\n3.2 Architecture\nInspired by Wang et al. [20] we apply average pooling over the hidden states of the last transformer layer and feed\nthe result through a hidden layer and a final output layer. For fine-tuning on the downstream task, we use the ADAM\noptimiser with CCC loss, which is the standard loss function used for dimensional SER [9, 32, 41], and a fixed\nlearning rate of 1e−4. We run for 5 epochs with a batch size of 32 and keep the checkpoint with best performance on\nthe development set.\nDuring training, we freeze the CNN layers but fine-tune the transformer ones. According to Wang et al. [20], such\na partial fine-tuning yields better results. When using the term fine-tuning, we will henceforth refer to this partial\nfine-tuning. These models are trained using a single random seed, for which the performance is reported.\n4\nDawn of the transformer era in speech emotion recognition PREPRINT\n...\n...\nTransformerCNN\nW A V\nPooling Hidden Output Dominance\nFreeze Fine-tune\nPre-trained wav2vec 2.0 / HuBERT\nArousal\nV alence\nFigure 1: Proposed architecture built on wav2vec 2.0 / HuBERT.\nWe compare results to a 14-layer Convolutional Neural Network (CNN14) as a standard baseline we have been using\nfor SER in previous work [9, 42]. It follows the architecture proposed by Konget al. [43] for audio pattern recognition.\nDifferent to the transformer-based models, which operate on the raw audio signal, this takes log-Mel spectrograms as\ninput. CNN14 has 6 convolutional blocks with two layers each, each followed by max pooling. Convolution layers\nhave a 3 × 3 kernel and a stride of 1 × 1, whereas max pooling layers use a stride of 2 × 2. After the last convolution\nlayer, features are pooled using both mean and max pooling, and subsequently fed into two linear layers. Dropout\nwith a probability of 0.2 is applied after every each convolution block. Log-Mel spectrograms are computed with 64\nMel bins, a window size of 32 ms, and a hop size of 10 ms. Note that the CNN14 model is not pre-trained, i. e. it is\nalways trained from scratch in our experiments. We train for60 epochs, with a learning rate of .01, and a batch size of\n64 using stochastic gradient descent (SGD) with a Nesterov momentum of .9. We select the model that performs best\non the validation set.\n3.3 Datasets\nWe used theMSP-Podcast corpus [31] (v1.7) to run multitask training on the three dimensions of arousal, dominance,\nand valence for speech from podcast recordings. The original labels cover a range from 1 to 7, which we map into\nthe interval of 0 to 1. Its train split contains 62 hours of recordings. In-domain results are reported on the test-1 split,\nwhich contains 21 hours of audio provided by 12, 902 samples (54% female / 46% male) from 60 speakers (30 female\n/ 30 male). The samples per speaker vary between 42 and 912.\nWe report cross-domain results IEMOCAP (Interactive Emotional Dyadic Motion Capture) dataset [29], which con-\ntains 12 hours of scripted and improvised dialogues by ten speakers ( 5 female / 5 male). It provides the same dimen-\nsional labels as MSP-Podcast, but in a range of 1 to 5, which we map to the interval 0 to 1. Since we use the dataset\nonly during evaluation, we do not apply the usual speaker cross-validation, but treat the corpus as a whole. It includes\n10, 039 samples (49% female / 51% male).\nFinally, we report cross-corpus results for valence on the test set of the Multimodal Opinion Sentiment Intensity\n(MOSI) [44] corpus. The dataset is a collection of YouTube movie review videos spoken by 41 female and 48 male\nspeakers. They are annotated for sentiment on a7-point Likert scale ranging from−3 to 3, which we map to the interval\n0 to 1. The test set contains 1 hour audio recordings given as 685 samples ( 51% female / 49% male), annotated for\nsentiment. As the gender labels are not part of the distributed database, we re-annotated them ourselves [45].\nWhile sentiment is a different concept than valence, as the former corresponds to an attitude held towards a specific\nobject and the latter more generally characterises a person’s feeling [46], there is evidence that sentiment annotations\ncan be decomposed to two constituents: intensity and polarity [47], which roughly correspond to arousal and valence.\nWe therefore expect some correlation between (predicted) valence and (annotated) sentiment scores.\n5\nDawn of the transformer era in speech emotion recognition PREPRINT\n3.4 Evaluation\nMachine learning models for speech emotion recognition are expected to work under different acoustic conditions and\nfor different speakers. To cover this, we evaluate them for correctness, robustness, and fairness [48].\nCorrectness measures how well predictions match the ground truth. The concordance correlation coefficient (CCC)\nprovides an estimate of how well the predicted distribution matches the ground truth one [49], and is the typical\nmeasure for evaluating dimensional SER models [50].\nRobustness (cf. Section 4.8) measures how model performance is affected by changes to the input signals such as\nadding background noise. Applying changes to the input signals must be carefully done for SER, as they might affect\nthe ground truth label [33, 51]. We focus on testing the robustness of the models against data augmentations that do not\nchange the human perception of the underlying emotion. We select the following five augmentations from Jaiswal and\nProvost [33] to enable direct comparison with previous results:Natural soundscapes adds a randomly selected sample\nfrom the natural class of the ESC-50 dataset [52] with a signal-to-noise ratio (SNR) of 0 dB, 10 dB or 20 dB; Human,\nnon-speech adds a randomly selected sample from the human class of the ESC-50 dataset with a SNR of 0 dB, 10 dB\nor 20 dB; Interior/domestic adds a randomly selected sample from the interior class of the ESC-50 dataset with a SNR\nof 0 dB, 10 dB or 20 dB;Speed up segment selects a random segment of 10% to 20% length within the utterance and\nincreases its speed by 1.25; Fade-in/fade-out decreases or increases the amplitude of the signal by 2% every second.\nFairness (cf. Section 4.9) evaluates if the model predictions show biases for certain protected attributes like race,\ngender, or age [53]. We focus on gender due to the lack of sufficient available information and/or datasets for other\nattributes. For regression problems, there is no clear definition how to measure fairness, but most approaches try\nto achieve an equal average expected outcome for population A and B [54]. We measure fairness by estimating the\ngender fairness score as the difference in the correctness metric (CCC) between female and male groups. A positive\ngender fairness score indicates a better performance of the model for female speakers.\n4 Evaluation\nWe begin our investigation with a thorough evaluation of transformer-based models. We show that valence is the\nprimary beneficiary of pre-training as it enables the models to implicitly learn linguistic information during the fine-\ntuning of the transformer layers. Utilising a comprehensive testing scheme, we attempt to identify how different\naspects of foundation models impact performance and generalisation. We place particular emphasis onrobustness and\nfairness, which are critical considerations for SER systems targeted to real-world applications.\n4.1 Can foundation models close the performance gap for valence?\nAnswer: The best models achieve a similar performance for arousal and dominance as non-transformer architectures\n[32], but improve the CCC score for valence by .26 and close the performance gap for valence.\nDetails: In Figure 2, we show in-domain and cross-domain CCC performance for different wav2vec 2.0 and HuBERT\nmodels as well as for the CNN14 baseline.\nWe first focus on arousal and dominance. For MSP-Podcast (in-domain) and IEMOCAP (cross-domain), all\ntransformer-based models score very similar, with w2v2-L-robust showing the overall best performance by reach-\ning a CCC score of .745/.634 (arousal/dominance) on MSP-Podcast, and .663/.518 on IEMOCAP. For MSP-Podcast,\nresults are similar compared to the CCC scores of .745/.655 achieved by Li et al. [32] and .757/.671 achieved by\nSrinivasan et al. [30].\nFor valence, we see a larger fluctuation of CCC scores for different transformer models ranging from .359 for\nw2v2-L-xls-r to .636 for hubert-b, both on MSP-Podcast. Overall, w2v2-L-robust shows again the best overall perfor-\nmance by reaching a CCC score of .635 on MSP-Podcast, .448 on IEMOCAP, and .539 for predicting sentiment on\nMOSI. For MSP-Podcast, results are better compared to the CCC score of .377 achieved by Li et al. [32] and similar\nto .627 by Srinivasan et al. [30] achieved with a model distilled from an audio + text based teacher.\n4.2 Does explicit linguistic information further improve performance?\nAnswer: Adding linguistic information does not improve predictions for arousal and dominance, and only in some\ncases for valence. However, especially models pre-trained on multiple languages seem to benefit when tested on\nEnglish speech.\n6\nDawn of the transformer era in speech emotion recognition PREPRINT\n0.0\n0.5\n1.0 Arousal\n.658\n.431\n.717\n.602\n.711\n.630\n.725\n.654\n.739\n.639\n.745\n.663\n.739\n.658\n.723\n.657\nCNN14 w2v2-b hubert-b w2v2-L hubert-L w2v2-L-robust w2v2-L-vox w2v2-L-xls-r\n0.0\n0.5\n1.0CCC\nDominance\n.564\n.272\n.629\n.488\n.621\n.487\n.633\n.478\n.655\n.465\n.634\n.518\n.650\n.496\n.632\n.496\nMSP-Podcast IEMOCAP MOSI0.0\n0.5\n1.0 Valence / sentiment\n.248\n.259\n.046\n.482\n.409\n.309\n.532\n.413\n.454\n.421\n.384\n.196\n.636\n.425\n.490\n.635\n.448\n.539\n.448\n.412\n.330\n.359\n.399\n.169\nFigure 2: CCC scores for arousal, dominance, valence (MSP-Podcast / IEMOCAP), and sentiment (MOSI). All mod-\nels have been trained for emotional dimension prediction using multitasking only on MSP-Podcast, and subsequently\nevaluated on its test set (in-domain), as well as to the test set of MOSI and the entire IEMOCAP dataset (cross-corpus).\nArousal Dominance Valence\n0.0\n0.2\n0.4\nCCCfus - CCCft\n-.023\n-.054\n.098\n-.027\n-.044\n.030\n-.057\n-.106\n.101\n-.061\n-.101\n-.042\n-.049\n-.066\n-.025\n-.065\n-.111\n.103\n-.028\n-.087\n.176\nw2v2-b hubert-b w2v2-L hubert-L w2v2-L-robust w2v2-L-vox w2v2-L-xls-r\nFigure 3: Text & audio fusion results for arousal, dominance, and valence prediction on MSP-Podcast. Embeddings\nfrom the already fine-tuned models are concatenated with BERT embeddings extracted from automatic transcriptions,\nwhereupon a two-layer feed-forward neural network is trained. We show the difference to results with the fine-tuned\n(ft) models from Figure 2.\nDetails: To evaluate whether adding linguistic information improves the predictions, the following experiment is\nconducted: a regression head is pre-trained, using as input pooled BERT embeddings in addition to the pooled states\nof the fine-tuned transformer models.\nBERT (Bidirectional Encoder Representations from Transformers) is a transformer model for natural language, pre-\ntrained on English language corpora consisting of more than 3 billion words [55]. The BERT embeddings have a\ndimensionality of 768 and are extracted from the transcriptions generated by thewav2vec2-base-960h speech recogni-\ntion model2. The fusion is done by concatenating the representations of both modalities. As regression head, exactly\nthe same architecture as for the fine-tuning of wav2vec 2.0 and HuBERT models is employed. For training, the weights\nof both models are frozen. The training is done with multi-target CCC-loss for a maximum of 100 epochs, with early\nstopping based on CCC development set performance.\n2https://huggingface.co/facebook/wav2vec2-base-960h\n7\nDawn of the transformer era in speech emotion recognition PREPRINT\nOriginal files Synthetic files Synthetic files (frz)0.0\n0.5\n1.0CCC\nValence\n.248\n-.003\n-.003\n.482\n.269\n.006\n.532\n.378\n.071\n.421\n.045\n-.021\n.636\n.465\n.100\n.635\n.426\n.021\n.448\n.217\n.007\n.359\n.031\n-.001\nCNN14 w2v2-b hubert-b w2v2-L hubert-L w2v2-L-robust w2v2-L-vox w2v2-L-xls-r\nFigure 4: CCC performance for valence on the original and synthetic files on MSP-Podcast. We see that models with\na high performance on the original files are more sensitive to sentiment (cf. left and center section). To prove that a\nfine-tuning of the transformer layers is required to learn linguistic content, we additionally show the correlation for\nmodels where the transformer layers were frozen (frz) during training (cf. Section 4.4).\nIn Figure 3, we report deviations from the results achieved with the fine-tuned acoustic models alone (cf. Figure 2).\nWe can see that a fusion with embeddings from the text domain helps with valence, but not with arousal and domi-\nnance, where performance actually deteriorates. This is in line with our previous findings, where we also found that\nintroducing linguistic information sometimes hampered performance for those two dimensions on MSP-Podcast [9].\nWhat is interesting, though, are the relatively large differences between the models, and that, especially, our best\nmodels hubert-L and w2v2-L-robust do not improve. The models that benefit most are the two multi-lingual models\nw2v2-L-vox and w2v2-L-xls-r, showing that models pre-trained on multiple languages gain from a fusion with text\nfeatures from the test set domain language.\n4.3 Do the models implicitly learn linguistic information?\nAnswer: The models implicitly capture linguistic information from the audio signal. The extent in which they learn\nsentiment during fine-tuning depends on the data used for pre-training (e. g. multi-lingual data makes it more difficult).\nGenerally, we see that valence performance correlates with a model’s ability to predict sentiment.\nDetails: Previous findings suggest that during fine-tuning, the models implicitly learn linguistic information. To asses\nhow sensitive the models are to linguistic content, we generated a synthesised version of a subset of the test set from\nthe transcriptions of MSP-Podcast. 3 In Figure 4, we finally show CCC performance for valence on the original and\nsynthesised files for all models. We see that performance gaps between the models in Figure 2 are directly linked with\ntheir ability to predict sentiment. Models reaching a high performance on the original files also do so on their synthetic\nversions and vice versa. However, to learn linguistic content, a fine-tuning of the transformer layers is essential. If we\npredict the synthetic test set with models where the transformer layers were frozen during training (cf. Section 4.4),\ncorrelation drops to almost zero.\nThis finding is also important for works doing in-domain training on IEMOCAP, as parts of the conversations are\nscripted which results in a leakage of text information that may result in overoptimistic results [56] when that text\ninformation is exploited by transformer models. Furthermore, our models may inherit similar biases as those found in\nNLP models [57].\n4.4 How important is a fine-tuning of the transformer layers?\nAnswer: Fine-tuning the transformer layers is necessary to obtain state-of-the-art performance, in particular for the\nvalence dimension. The highest gain is observed for hubert-L and w2v2-L-robust, which are the models that benefit\nthe least from a fusion with text.\nDetails: So far, we have fine-tuned all transformer layers along with the added output layer. However, practitioners\noften choose to use a pre-trained model as a frozen feature extractor, and subsequently train just an output layer on the\ngenerated embeddings. Nevertheless, prior studies have shown that it is necessary to fine-tune several or all layers on\nthe target task to get good downstream performance [20, 42, 43, 58]. In this sub-section, we experiment with training\nonly the last output layer and keeping all others frozen. This is compared to our previous experiments where we jointly\nfine-tune the last layer and the transformer layers.\n3Partial audio transcripts are available with MSP-Podcast v1.9 and cover 55% of the test-1 split from v1.7 we used for our\nexperiments.\n8\nDawn of the transformer era in speech emotion recognition PREPRINT\nArousal Dominance Valence0.0\n0.1\n0.2\n0.3\nCCCft - CCCfrz\n.047\n.088\n.182\n.054\n.107\n.177\n.099\n.114\n.147\n.064\n.109\n.220\n.049\n.042\n.235\n.068\n.060\n.134\n.034\n.073\n.034\nw2v2-b hubert-b w2v2-L hubert-L w2v2-L-robust w2v2-L-vox w2v2-L-xls-r\nFigure 5: Difference of fine-tuned (ft) to frozen (frz) CCC performance for arousal, dominance, and valence prediction\non MSP-Podcast. The fine-tuned results are from Figure 2, where transformer and output layers are jointly trained.\nFor the frozen results, we keep all transformer layers frozen and simply train the output head. Results show that fine-\ntuning the transformer layer is worth the computational cost it incurs.\nFigure 5 shows the difference between CCC values for the fine-tuned and frozen models. We observe performance\ngains when fine-tuning in all cases, demonstrating that fine-tuning of the transformer layers is necessary. Moreover,\nthe models that see the biggest performance gain are hubert-L and w2v2-L-robust. In Section 4.2, these models were\nfound to benefit less from additional text information. These findings indicate that a fine-tuning of the transformer\nlayers enables the models to capture the linguistic information needed to perform well on valence.\n4.5 Do the models generalise better across different domains?\nAnswer: Transformer-based models generalise better than a non-transformer baseline.\nDetails: As we see a similar trend for different transformer-based models between in-domain and cross-corpus re-\nsults in Figure 2, we focus on the best-performing one ( w2v2-L-robust). The drop in CCC between in-domain and\ncross-corpus results for w2v2-L-robust on IEMOCAP is 11% for arousal, 21% for dominance, and 30% for valence\non IEMOCAP, and15% for sentiment on MOSI. For CNN14, the drop in CCC is 34% for arousal, and 52% for domi-\nnance, while for valence, we do not estimate the drop in cross-domain performance as the in-domain CCC is already\nvery low. The drop in CCC is smaller forw2v2-L-robust for arousal and dominance, indicating that transformer-based\nmodels generalise better. For valence, we cannot derive a final conclusion, but the trend we see for sentiment in MOSI\nseems very promising.\n4.6 Does more data during pre-training lead to better performance?\nAnswer: For arousal and dominance, all tested models perform equally well, whereas with respect to valence / sen-\ntiment the data used for pre-training has a strong effect. Mixing data from several domains leads to a considerable\nimprovement for w2v2-L-robust compared to w2v2-L, which is only trained on clean speech. However, hubert-L,\nwhich uses the same pre-training data as w2v2-L, which is also trained on clean speech, still performs as good as\nw2v2-L-robust. For models pre-trained on multi-lingual data, we see a performance drop when tested on English\nspeech.\nDetails: To understand what influence the size and domain of the pre-training data have on downstream performance,\nwe included several wav2vec 2.0 models with same large architecture but different pre-training (see Table 2).\nThe results in Figure 2 show only differences in terms of CCC between the transformer models for valence and senti-\nment, not for arousal or dominance. Previous studies uniformly report that HuBERT outperforms wav2vec 2.0 which\nis replicated by our results with w2v2-b showing a smaller CCC than hubert-b for the valence task on MSP-Podcast\nand IEMOCAP, and for the sentiment task on MOSI. The increase in performance for w2v2-L-robust compared to\nhubert-L is most likely explained by the additional 3k hours of telephone conversations used for pre-training. How-\never, by comparing w2v2-L-vox and w2v2-L-xls-r, it also becomes clear that more data does not necessarily lead to\nbetter results. Though both models are trained on significantly more data than hubert-L and w2v2-L-robust (100k\nand 463k vs 63k hours), they perform clearly worse. Notably, both were pre-trained on multiple languages. Since\nthe databases we use for evaluation contain only English speakers, this could be a disadvantage to models that are\nexclusively pre-trained on English.\n9\nDawn of the transformer era in speech emotion recognition PREPRINT\n0.0\n0.5\n1.0 Arousal\n.505\n.442\n.618\n.538\n.604\n.559\n.642\n.599\n.654\n.569\n.677\n.616\n.659\n.595\n.640\n.599\nCNN14 w2v2-b hubert-b w2v2-L hubert-L w2v2-L-robust w2v2-L-vox w2v2-L-xls-r\n0.0\n0.5\n1.0CCC\nDominance\n.467\n.283\n.541\n.417\n.540\n.429\n.563\n.430\n.584\n.397\n.591\n.462\n.582\n.439\n.563\n.444\nMSP-Podcast IEMOCAP MOSI0.0\n0.5\n1.0 Valence / sentiment\n.171\n.201\n.079\n.373\n.300\n.218\n.378\n.281\n.304\n.317\n.267\n.141\n.535\n.327\n.386\n.552\n.367\n.439\n.378\n.331\n.253\n.270\n.295\n.131\nFigure 6: CCC scores for arousal, dominance, valence (MSP-Podcast/IEMOCAP), and sentiment (MOSI) when aug-\nmenting the test data. The scores are averaged over all eleven different augmented versions of the test data.\n4.7 Does a larger architecture lead to better performance?\nAnswer: A larger architecture does not lead to better performance per se. Larger architectures using different data\nduring pre-training might perform worse than smaller architectures.\nDetails: We can draw some conclusions about the influence that the size of the architecture has on performance based\non Figure 2. The size of the architecture, i. e. base vs large, seems not to be the decisive point: w2v2-b and w2v2-L\nshow very similar performance. In addition, the small models w2v2-b and hubert-b have comparable performance\nto the large models w2v2-L, w2v2-L-vox, and w2v2-L-xls-r for arousal and dominance, both in- and cross-domain.\nFor valence, the small models outperform w2v2-L, w2v2-L-vox, and w2v2-L-xls-r in most cases for MSP-Podcast and\nMOSI, and achieve a similar performance on IEMOCAP.\n4.8 Are the models robust against changes to the input signals?\nAnswer: The tested models are reasonably robust against changes to the input signals, with w2v2-L-robust showing\nthe highest and hubert-b the lowest robustness.\nDetails: Figure 6 summarises the average CCC scores of the models averaged over all augmentations described\nin Section 4.8. All models show a drop in CCC compared to the CCC scores for the clean data from Figure 2.\nw2v2-L-robust has now the highest CCC score for all datasets and all dimensions. The average change in CCC for\nw2v2-L-robust is −0.068. The model with the highest average change in CCC is hubert-b (−0.108). The model with\nthe lowest average change in CCC is CNN14 (−0.047), which is mostly due to its results for IEMOCAP for which it\nshows no impairment of its relatively low performance by augmentations.\nTable 3 shows changes in CCC for single augmentations for each dataset and dimension for the best performing model\nw2v2-L-robust. The performance of the model is only sligthly affected (absolute change in CCC score below .05)\nfor added background sounds with a SNR of 20 dB or a fade-in/fade-out of the signal. When speeding up parts of\nthe signal or adding background sounds with more severe SNRs the change in CCC can be up to −.278. The model\ninvestigated on the same augmentations by Jaiswal and Provost [33] shows an equal drop in unweighted average recall\n(UAR) when adding background sounds with 0 dB, 10 dB, 20 dB SNR of at least −.30. w2v2-L-robust is more robust\nwhen adding background sounds with a moderate SNR. It shows a drop in CCC of up to to−.28 for 0 dB SNR, but only\na drop in CCC of up to −.036 for 20 dB SNR. Whereas the model investigated by Jaiswal and Provost [33] is similar\naffected by adding human, non-speech, interior/domestic, or natural sounds as background sounds, w2v2-L-robust is\n10\nDawn of the transformer era in speech emotion recognition PREPRINT\nTable 3: Change in CCC forw2v2-L-robust predictions on augmented data compared to its predictions on clean data.\nAugmentation SNR Arousal Dominance Valence / sentiment\nMSP-Podcast IEMOCAP MSP-Podcast IEMOCAP MSP-Podcast IEMOCAP MOSI\nNatural soundscapes 0 dB -.145 -.109 -.081 -.119 -.221 -.183 -.210\n10 dB -.038 -.018 +.001 -.035 -.051 -.064 -.071\n20 dB -.019 -.009 +.012 -.018 -.003 -.015 -.025\nHuman, non-speech 0 dB -.254 -.205 -.195 -.167 -.278 -.214 -.264\n10 dB -.053 -.036 -.036 -.058 -.080 -.092 -.143\n20 dB -.011 -.001 +.005 -.017 -.007 -.030 -.036\nInterior/domestic 0 dB -.124 -.089 -.081 -.097 -.170 -.160 -.149\n10 dB -.026 -.011 -.003 -.029 -.031 -.049 -.059\n20 dB -.011 -.004 +.007 -.014 -.005 -.015 -.030\nSpeed up segment -.080 -.030 -.099 -.056 -.062 -.069 -.113\nFade-in/fade-out -.001 -.001 .000 -.001 .000 .000 -.001\n0.2\n0.0\n0.2 Arousal\n-.018\n-.016\n.020\n-.010\n-.003\n-.012\n-.015\n-.005\n-.017\n-.001\n-.028\n-.004\n-.016\n.002\n-.023\n-.003\nCNN14 w2v2-b hubert-b w2v2-L hubert-L w2v2-L-robust w2v2-L-vox w2v2-L-xls-r\n0.2\n0.0\n0.2\nGender Fairness Score\nDominance\n-.085\n-.054\n-.020\n-.027\n-.025\n-.024\n-.040\n-.028\n-.016\n-.026\n-.015\n-.020\n-.032\n-.028\n-.039\n-.032\nMSP-Podcast IEMOCAP MOSI0.2\n0.0\n0.2\nValence / sentiment\n-.080\n-.011\n-.203\n.054\n.020\n.040\n.083\n.029\n-.000\n.017\n.032\n-.045\n.062\n.026\n.036\n.028\n.018\n.020\n.038\n.014\n-.030\n.035\n.022\n.019\nFigure 7: Gender fairness scores for arousal, dominance, valence (MSP-Podcast / IEMOCAP), and sentiment (MOSI).\nThe gender fairness score is given by CCC female − CCCmale. A positive value indicates that the model under test\nperforms better for female speaker and a negative value that it performs better for male speaker. A model with desired\nequal performance would have a gender fairness score of 0.\nthe most affected when addinghuman, non-speech sounds (average drop in CCC of−.103), and the least when adding\ninterior/domestic sounds (average drop in CCC of −.055).\n4.9 Are the models fair regarding the gender of the speaker?\nAnswer: Models are more fair for arousal and dominance than for valence. For valence, most models show a higher\nCCC for females than for males.\nDetails: Figure 7 shows gender fairness scores for the speakers in MSP-Podcast, IEMOCAP, and MOSI. As in-\ntroduced in Section 3.4, the gender fairness score is expressed by the difference in CCC between female and male\nspeakers with positive values indicating higher values of the underlying metric for females. For MSP-Podcast, nearly\nall models show a slightly worse female CCC for arousal and dominance. For IEMOCAP, nearly all models show a\nslightly better female CCC for arousal and dominance.\n11\nDawn of the transformer era in speech emotion recognition PREPRINT\n0.00\n0.25\n0.50\n0.75\n1.00\n Arousal\nCNN14 w2v2-b hubert-b w2v2-L hubert-L w2v2-L-robust w2v2-L-vox w2v2-L-xls-r\n0.00\n0.25\n0.50\n0.75\n1.00Speaker-level CCC\nDominance\n7 931 968 170 159 867 160 151 138 148 8 131 94 13 152 22 59 130 23\nSpeaker ID\n0.00\n0.25\n0.50\n0.75\n1.00\n Valence\nFigure 8: Speaker-level performance (CCC) on MSP-Podcast for the different models. We only use speakers with at\nleast 200 test set samples for robust CCC estimates. All models show low CCC for at least one speaker on all 3 tasks.\nSpeakers have been ordered according to the mean CCC over all dimensions and models.\nFor valence in MSP-Podcast and IEMOCAP, most models show a better CCC for female speakers than male ones –\nwith the exception of CNN14. For sentiment in MOSI, the CNN14 model shows a bias towards better performance for\nmale speaker, whereas all other models show very small biases in the different direction.\nAveraging over all databases and dimensions the model with the best gender fairness score is w2v2-L with .007,\nfollowed by w2v2-L-vox with .015, w2v2-L-xls-r with .018, w2v2-L-robust, with .019, hubert-b with .025, hubert-L\nwith .027, and w2v2-b with .029 up to CNN14 with −.043.\n4.10 Is performance equal across all speakers?\nAnswer: Performance for the best foundation models is similar between most speakers in MSP-Podcast, but can\ndeteriorate to low CCC values for some speakers.\nDetails: The performance of speech processing is dependent on individual speaker characteristics [37]. This has led\nseveral prior SER works to target personalisation to different speakers [59, 60, 61]. To investigate this phenomenon\nfor transformer-based models, we examine the per-speaker performance, where instead of computing a global CCC\nvalue over all test set values, we compute one for each speaker. As discussed (cf. Section 3.3), the MSP-Podcast\ntest set consists of 12902 samples from 60 speakers; however, the samples are not equally distributed across them\n(minimum samples: 41, maximum samples 912). In order to make our subsequent analysis more robust, we only keep\nspeakers with more than 200 samples, resulting in 19 speakers. We use bootstrapping, where we randomly sample\n(with replacement) 200 samples from each speaker to compute the CCC. This process is repeated 1000 times, and we\nreport the mean value.\nOur results are presented in Figure 8. For visualisation purposes, we ordered speakers based on the average CCC\nvalue over all models and across arousal, dominance, and valence. For arousal and dominance models perform well\nfor most speakers, and show similar performance. For speakers 7 and 931 all models show a low CCC, whereas for\nspeaker 931 the CNN14 model performs worse than the others. For valence, CCC values per speaker differ between\nmodels replicating the findings of Figure 2. The best model ( w2v2-L-robust) performs relatively similar for most of\nthe speaker groups and shows only a drop for speaker 7, a similar result as for valence and dominance.\nDifferent models broadly, but not perfectly, agree on ‘good’ and ‘bad’ speakers, with pairwise Spearman correlations\nranging from .960 to .725 for arousal, .972 to .825 for dominance, and .947 to .333 for valence. This could be a\nmanifestation of the underspecification phenomenon plaguing machine learning architectures [62], as models which\nhave similar performance on the entire test set, nevertheless behave differently across different subsets of it.\n12\nDawn of the transformer era in speech emotion recognition PREPRINT\n0.0\n0.5\n1.0 Arousal\n.658\n.431\n.624\n.531\n.745\n.663\nCNN14 w2v2-L-w/o-pretrain w2v2-L-robust\n0.0\n0.5\n1.0CCC\nDominance\n.564\n.272\n.471\n.381\n.634\n.518\nMSP-Podcast IEMOCAP MOSI0.0\n0.5\n1.0 Valence / sentiment\n.248\n.259\n.046\n.123\n.183\n.015\n.635\n.448\n.539\nFigure 9: CCC performance of randomly-initialised wav2vec 2.0 model (w2v2-L-w/o-pretrain) on in-domain and\ncross-corpus arousal, dominance, valence / sentiment prediction. We compare the performance with that of CNN14\nand w2v2-L-robust. We observe that valence and sentiment benefit massively from pre-training, without which\nwav2vec 2.0 performs worse than a classic CNN approach.\n4.11 Why do foundation models generalise so well?\nAnswer: Even without pre-training, the latent space provided by the transformer architecture generalises better than\nCNN14, as it abstracts away domain and speaker. Pre-training marginally improves arousal and dominance perfor-\nmance but is critical for valence.\nDetails: So far, we were able to confirm the superiority of transformer-based models. However, even though pre-\ntraining seems important, it remains unclear to what extent the transformer architecture itself contributes to that suc-\ncess. To shed more light into this, we trained wav2vec 2.0 from a random initialisation. As our architecture, we chose\nthe large wav2vec 2.0 architecture, which is also used by the best performing model w2v2-L-robust. In the following,\nwe will refer to this model as w2v2-L-w/o-pretrain.\nWe trained the model for50 epochs and selected the best checkpoint according to the performance on the development\nset (epoch 17).4 In Figure 9, we compare in- and cross-domain performance with CNN14 and w2v2-L-robust. We see\nthat especially valence / sentiment detection benefits massively from pre-training (both in-domain and cross-domain),\nand that without pre-training wav2vec 2.0 performs in most cases worse thanCNN14.\nIn the introduction of wav2vec 2.0, Baevski et al. [18] postulate that pre-training helps learn more general represen-\ntations that abstract away from speaker or background information. However, it is not entirely clear if these benefits\nare a result of pre-training or are a consequence of the specific inductive biases introduced by the architecture. To\ninvestigate this, we compare embeddings extracted withCNN14, w2v2-L-w/o-pretrain, and w2v2-L-robust,5 which are\nshown in Figure 10. The embeddings are projected to two dimensions using t-SNE [63] and different information is\nchromatically superimposed.\nFor CNN14, two main clusters almost perfectly separate the two data sources MSP-Podcast and IEMOCAP, whereas\nseveral smaller blobs represent gender groups and individual speakers. In fact, speaker and domain are more pro-\nnounced than valence information. Hence, similar emotional content can translate into entirely different latent repre-\n4Even though we used the same data (MSP-Podcast) for fine-tuning, we expected it would take longer for the model to convert\nif we start from scratch. Also, this time we trained all encoder layers (including the CNN ones). Apart from that we followed the\nmethodology described in Section 3.2.\n5We use average pooling on the output of the last CNN layer for CNN14 and the last transformer layer for wav2vec 2.0.\n13\nDawn of the transformer era in speech emotion recognition PREPRINT\nCNN14\nDomain\n Gender\n Speaker\n Valence\nw2v2-L-w/o-pretrain\nw2v2-L-robust\nMSP-Podcast\nIEMOCAP\nFemale\nMale\nLow\nHigh\nFigure 10: Visualisation of embeddings extracted with different models overlayed with meta information for a com-\nbined dataset of MSP-Podcast and IEMOCAP. We observe that the latent space of wav2vec 2.0 offers a better abstrac-\ntion from domain, gender, and speaker compared to the CNN14 baseline – even without pre-training. However, only\na pre-trained model is able to separate low from high valence. To reduce the dimensionality of the latent space, we\napplied T-SNE [63].\nsentations. In contrast, the latent space of both wav2vec 2.0 models shows no clusters for domain, gender, or speaker.\nThe architecture itself seems to introduce specific inductive biases which are well-suited to learning robust represen-\ntations. Nevertheless, only the pre-trained model (w2v2-L-robust) shows a smooth transition from low to high valence\nscores, showing that pre-training is still necessary for good downstream performance. Moreover, the strong speaker\ndependency presented in Section 4.10 shows that the two dimensional t-SNE visualisations help comparing general-\nisation abilities between models, but are not necessarily sufficient for deriving conclusions w. r. t. generalisation over\ndifferent factors.\n5 Efficiency\nFor our last experimental evaluation, we focus on efficiency. We concentrate on three facets: optimisation stability,\ncomputational complexity, and data efficiency.\n5.1 Does pre-training help with training stability and convergence?\nAnswer: A pre-trained model reduces the number of epochs needed to converge and improves performance stability\nacross training runs with different seeds.\nDetails: To balance the effects of randomness (either in the initialisation of network weights or the data sampling), it is\na common strategy to perform several runs with different random seeds. Starting from pre-trained weights, however,\nwe expect less volatility [64, 65]. Figure 11 shows the mean and standard deviation over the performance on the\ndevelopment set across three trials for CNN14 and w2v2-b. CNN14 shows a constant jittering across all 60 epochs,\nwhereas w2v2-b converges faster and we can reduce the number of epochs to 5.\n14\nDawn of the transformer era in speech emotion recognition PREPRINT\nEpoch\n0.3\n0.4\n0.5\n0.6CCC\n  CNN14\n  (60 epochs)\n w2v2-b\n (5 epochs)\nFigure 11: Mean and standard deviation of development set performance on MSP-Podcast across three training runs.\nCompared to CNN14, w2v2-b converges earlier and shows less fluctuation.\n0.0\n0.5\n1.0 Arousal\n.730\n.644\n.721\n.645\n.734\n.645\n.744\n.660\n.745\n.663\n6 8 10 12 24 (original)\n0.0\n0.5\n1.0CCC\nDominance\n.614\n.441\n.648\n.496\n.647\n.502\n.655\n.486\n.634\n.518\nMSP-Podcast IEMOCAP MOSI0.0\n0.5\n1.0 Valence / sentiment\n.416\n.431\n.218\n.450\n.371\n.292\n.598\n.462\n.478\n.638\n.478\n.540\n.635\n.448\n.539\nFigure 12: CCC scores for arousal, dominance, and valence / sentiment for w2v2-L-robust and pruned versions. The\nlegend shows the number of bottom layers kept during fine-tuning. We see that half of the layers can be removed\nwithout any loss in performance.\n5.2 How many transformer layers do we really need?\nAnswer: We can reduce the number of transformer layers to 12 without a degradation in performance. With less than\n12 layers we begin to see a negative effect on valence.\nDetails: In Section 4.7, we mentioned that w2v2-b and hubert-b outperform some of the large models. From that,\nwe concluded that the size of the architecture seems less important, but it is rather the data used for pre-training that\ndetermines success. If this is really the case, we should be able to partially reduce the size of a model without losing\nperformance.\nSajjad et al. [66] investigated different layer pruning strategies and identified top-layer dropping as the best strategy\noffering a good trade-off between accuracy and model size. Inspired by their findings, we set up an experiment\nwhere we successively removed transformer layers from the top of the original pre-trained model before fine-tuning.\nIn Figure 12, we report the effect on CCC for w2v2-L-robust (our overall best performing model). Results show\nthat half of the layers can be removed without a loss in performance. We denote the resulting 12-layer model as\nw2v2-L-robust-12. Only with 10 or less layers we actually begin to see a drop for valence / sentiment on IEMOCAP\nand MOSI. For arousal and dominance, we still achieve good performance with only 8 layers.\n15\nDawn of the transformer era in speech emotion recognition PREPRINT\n0.0\n0.5\n1.0 Arousal\n.586\n.587\n.618\n.614\n.637\n.606\n.690\n.646\n.674\n.625\n.725\n.639\n.737\n.656\n.735\n.643\n.744\n.660\n5% 7.5% 10% 12.5% 15% 25% 50% 75% 100%\n0.0\n0.5\n1.0CCC\nDominance\n.418\n.340\n.512\n.388\n.564\n.370\n.603\n.450\n.613\n.444\n.641\n.466\n.648\n.497\n.647\n.503\n.655\n.486\nMSP-Podcast IEMOCAP MOSI0.0\n0.5\n1.0 Valence / sentiment\n.240\n.360\n.069\n.288\n.430\n.125\n.409\n.462\n.291\n.441\n.444\n.287\n.480\n.440\n.329\n.558\n.460\n.427\n.606\n.460\n.498\n.629\n.466\n.460\n.638\n.478\n.540\nFigure 13: CCC scores for arousal, dominance, and valence / sentiment for w2v2-L-robust on sparse training data.\nThe legend shows the fraction of data used for fine-tuning. Please note that steps are not linear.\n5.3 Can we reduce the training data without a loss in performance?\nAnswer: A reduction of training samples without loss in performance is only possible for arousal and dominance.\nDetails: Reducing the amount of training data offers another way to speed up model building. To find out what\neffect the removal of training samples has, we conducted an experiment where we fine-tuned several versions of the\nsame pre-trained model with different fractions of the training set (MSP-Podcast). We leave development and test set\nuntouched.\nFigure 13 shows CCC for arousal, dominance, valence / sentiment on MSP-Podcast, IEMOCAP and MOSI. For\nefficiency, we start from the reduced 12-layer architecture and therefore compare results to w2v2-L-robust-12 (cf.\nSection 5.2). There is no noteworthy degradation for arousal and dominance when keeping close to the entire training\nset. The only exception is dominance on IEMOCAP, where we achieve best results with just 75% of the data. For\nthese dimensions, however, performance already saturates at 25% yielding a loss of less than .02 on MSP-Podcast,\nwhereas for IEMOCAP, even 12.5% of the training samples seem sufficient to stay within a margin of .05.\nOnce again, it is a different story for valence. For MSP-Podcast, we see a constant improvement that only begins to\ndecrease when reaching 75% of the data. For MOSI, we even see a boost in CCC of almost .1 for the remaining 25%.\nHowever, in light of our findings from Section 4.3, this does not come as a surprise. Providing more linguistic diversity\nmakes it more likely a model can detect associations between key words and emotional context. What is a surprise,\nthough, is that on IEMOCAP, using just 7.5% of the data, results in a drop of less than .05. A possible explanation is\nthat the vocabulary of IEMOCAP does not resemble that of MSP-Podcast and that, therefore, the impact of linguistic\ninformation is limited. This would also explain why the differences in valence performance are less pronounced for\nIEMOCAP (cf. Figure 2).\n6 Summary\nWe explored the use of (pre-trained) transformer-based architectures for speech emotion recognition. In the previous\nsections, we dealt with several questions in isolation. We now attempt a unified summary by collectively considering\nall findings.\nEffect of pre-training: pre-training is essential to get good performance (Section 4.6), especially for the valence di-\nmension. This is particularly evident when training wav2vec 2.0 from a random initialisation (Section 4.11): the model\n16\nDawn of the transformer era in speech emotion recognition PREPRINT\nperforms substantially worse on all three dimensions, and its embeddings are unable to capture valence information. In\naddition, pre-training serves as a form of regularisation which helps stabilise the training (Section 5.1), thus resulting\nin models which require less iterations, and less data to train on (Section 5.3). However, we were unable to determine\na clear relationship of the form ‘more pre-training data leads to better performance’. In fact, downstream performance\ncan be negatively impacted by the introduction of more data, as seen by the comparison between w2v2-L-vox and\nw2v2-L-xls-r, which differ only in the fact that w2v2-L-xls-r has been trained on more (and more diverse) data, yet\nperforms worse on all three dimensions.\nGeneralisation: transformer-based models show very good cross-corpus generalisation (Section 4.6), robustness (Sec-\ntion 4.8), and appear invariant to domain, speaker, and gender characteristics (Section 4.11). These are all very im-\nportant traits for any model that is intended for production use in realistic environments. However, they seem to\nstem primarily from the architecture rather than the pre-training as they are also evident in models initialised from\nrandom weights (Section 4.11). We also showed that several self-attention layers can be removed without hampering\ndownstream performance (Section 5.2), though they might still be necessary for successful pre-training.\nFairness: fairness remains a challenging topic for contemporary machine learning architectures. Community discus-\nsions primarily concern the issue of group fairness. In the present, we investigate this for the only group variable\navailable in our datasets: gender (Section 4.9), where we observe that transformer-based architectures are more fair\nthan the CNN14 baseline. However, we argue that individual fairness is important for SER. This refers to how models\nperform across different speakers; a feat which proves challenging even for the top-performing models investigated\nhere (Section 4.10). We consider this an important topic which has not been sufficiently investigated for SER, though\nit is long known to impact other speech analysis models [35, 37].\nIntegration of linguistic and paralinguistic streams: finally, one of our most intriguing findings is that transformers\nseem capable of integrating both information streams of the voice signal. This is evident in how well-performing va-\nlence prediction models retain their effectiveness for synthesised speech lacking emotional intonation (Section 4.3) and\nfail to benefit from fusion with explicit textual information (cf. Section 4.2). Interestingly, this is only possible when\nfine-tuning the self-attention layers (Section 4.4), as keeping them frozen results to complete failure for synthesised\nspeech (Section 4.3). This draws attention to an under investigated aspect of fine-tuning, namely, how it qualitatively\naffects the nature of internal representations. Common understanding sees it as a mechanism through which to obtain\nbetter performance, but our analysis shows that it leads to a fundamental change in how the underlying signal is rep-\nresented (moving from almost no sensitivity to linguistic content to increased reactivity to it). This mechanism may\nbe crucial in the pursuit of paralinguistic and linguistic integration which is key to a holistic understanding of human\ncommunication. However, this integration might prove problematic in cases where the two modalities disagree, e. g. in\ncases of irony [67]. Our results also highlight that good valence performance might be language dependent as models\npre-trained on a variety of languages perform worse for valence compared with comparable models pre-trained only\nfor English (Section 4.1).\n7 Conclusion\nTransformers have already revolutionised a very diverse set of artificial intelligence tasks, including speech emotion\nrecognition. The present contribution goes beyond previous works that already established their effectiveness for SER\nby conducting a thorough evaluation and analysis of prominent transformer-based speech models for dimensional\nemotion recognition. We obtain state-of-the-art valence recognition performance on MSP-Podcast of .638 without\nusing explicit linguistic information, and manage to attribute this exceptional result to implicit linguistic information\nlearnt through a fine-tuning of the self-attention layers. We release our best performing model ( w2v2-L-robust-12) to\nthe community [22].6 Transformer architectures are more robust to small perturbations, fair on the (gender) group- if\nnot on the individual-level, and generalise across different domains. Our findings demonstrate that a new era is dawn-\ning in speech emotion recognition: that of pre-trained, transformer-based foundation models, which can finally lead to\nthe coveted integration of the two dominant information streams of spoken language, linguistics, and paralinguistics.\n8 References\n[1] B. Schuller, “Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends,” Communications of\nthe ACM, vol. 61, no. 5, pp. 90–99, 2018.\n[2] P. Ekman, “An argument for basic emotions,” Cognition & emotion, vol. 6, no. 3-4, pp. 169–200, 1992.\n[3] J. A. Russell and A. Mehrabian, “Evidence for a three-factor theory of emotions,” Journal of research in Personality, vol. 11,\nno. 3, pp. 273–294, 1977.\n6https://github.com/audeering/w2v2-how-to\n17\nDawn of the transformer era in speech emotion recognition PREPRINT\n[4] B. T. Atmaja, A. Sasou, and M. Akagi, “Survey on bimodal speech emotion recognition from acoustic and linguistic infor-\nmation fusion,” Speech Communication, 2022.\n[5] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang, “A survey of affect recognition methods: Audio, visual, and spontaneous\nexpressions,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 1, pp. 39–58, 2009.\n[6] R. A. Calvo and S. D’Mello, “Affect detection: An interdisciplinary review of models, methods, and their applications,”\nIEEE Transactions on Affective Computing, vol. 1, no. 1, pp. 18–37, 2010.\n[7] J. Kossaifi, R. Walecki, Y . Panagakis, J. Shen, M. Schmitt, F. Ringeval, J. Han, V . Pandit, A. Toisoul, B. Schuller, K. Star,\nE. Hajiyev, and M. Pantic, “SEW A DB: A rich database for audio-visual emotion and sentiment research in the wild,”IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 3, pp. 1022–1040, 2021.\n[8] S. Sahu, V . Mitra, N. Seneviratne, and C. Y . Espy-Wilson, “Multi-modal learning for speech emotion recognition: An analysis\nand comparison of asr outputs with ground truth transcription,” inProceedings of the Annual Conference of the International\nSpeech Communication Association (INTERSPEECH), Graz, Austria: ISCA, 2019, pp. 3302–3306.\n[9] A. Triantafyllopoulos, U. Reichel, S. Liu, S. Huber, F. Eyben, and B. W. Schuller, “Multistage linguistic conditioning of\nconvolutional layers for speech emotion recognition,”arXiv preprint arXiv:2110.06650, 2021.\n[10] C. Oates, A. Triantafyllopoulos, I. Steiner, and B. W. Schuller, “Robust speech emotion recognition under different encod-\ning conditions,” in Proceedings of the Annual Conference of the International Speech Communication Association (INTER-\nSPEECH), Graz, Austria: ISCA, 2019, pp. 3935–3939.\n[11] A. Triantafyllopoulos, G. Keren, J. Wagner, I. Steiner, and B. W. Schuller, “Towards robust speech emotion recognition\nusing deep residual networks for speech enhancement,” inProceedings of the Annual Conference of the International Speech\nCommunication Association (INTERSPEECH), Graz, Austria: ISCA, 2019, pp. 1691–1695.\n[12] A. Batliner, S. Hantke, and B. W. Schuller, “Ethics and good practice in computational paralinguistics,” IEEE Transactions\non Affective Computing, 2020.\n[13] J. Cheong, S. Kalkan, and H. Gunes, “The hitchhiker’s guide to bias and fairness in facial affective signal processing:\nOverview and techniques,” IEEE Signal Processing Magazine, vol. 38, no. 6, pp. 39–49, 2021.\n[14] R. Bommasani et al., “On the opportunities and risks of foundation models,” arXiv preprint arXiv:2108.07258, 2021.\n[15] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual representations,”\nin Proceedings of the International Conference on Machine Learning (ICML) , Vienna, Austria (virtual), 2020, pp. 1597–\n1607.\n[16] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao, C. Xu, Y . Xu, Z. Yang, Y . Zhang, and D. Tao, “A\nsurvey on vision transformer,” IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2022.\n[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you\nneed,” in Advances in Neural Information Processing Systems (NeurIPS), Long Beach, CA, USA, 2017, pp. 5998–6008.\n[18] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, “Wav2vec 2.0: A framework for self-supervised learning of speech repre-\nsentations,” in Advances in Neural Information Processing Systems (NeurIPS) , Vancouver, BC, Canada, 2020, pp. 12 449–\n12 460.\n[19] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “Hubert: Self-supervised speech\nrepresentation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 29, pp. 3451–3460, 2021.\n[20] Y . Wang, A. Boumadane, and A. Heba, “A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition,\nspeaker verification and spoken language understanding,” arXiv preprint arXiv:2111.02735, 2021.\n[21] S. Latif, R. Rana, S. Khalifa, R. Jurdak, J. Qadir, and B. Schuller, “Survey of deep representation learning for speech emotion\nrecognition,” IEEE Transactions on Affective Computing, vol. 12, 2021.\n[22] J. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Eyben, and B. W. Schuller, Model for Dimensional Speech\nEmotion Recognition based on Wav2vec 2.0, 2022. DOI : 10.5281/zenodo.6221127.\n[23] D. N. Krishna, “Using large pre-trained models with cross-modal attention for multi-modal emotion recognition,” arXiv\npreprint arXiv:2108.09669, 2021.\n[24] J. Yuan, X. Cai, R. Zheng, L. Huang, and K. Church, “The role of phonetic units in speech emotion recognition,” arXiv\npreprint arXiv:2108.01132, 2021.\n[25] S.-w. Yang, P.-H. Chi, Y .-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y . Y . Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang,\nW.-C. Tseng, K.-t. Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H.-y. Lee,Superb: Speech\nprocessing universal performance benchmark, 2021.\n[26] L. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from speech using wav2vec 2.0 embeddings,” Proceedings of the\nAnnual Conference of the International Speech Communication Association (INTERSPEECH), pp. 3400–3404, 2021.\n[27] L.-W. Chen and A. Rudnicky, “Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition,” arXiv preprint\narXiv:2110.06309, 2021.\n[28] M. R. Makiuchi, K. Uto, and K. Shinoda, “Multimodal emotion recognition with high-level speech and text features,” arXiv\npreprint arXiv:2111.10202, 2021.\n[29] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:\nInteractive emotional dyadic motion capture database,” Language resources and evaluation , vol. 42, no. 4, pp. 335–359,\n2008.\n18\nDawn of the transformer era in speech emotion recognition PREPRINT\n[30] S. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation learning through cross-modal conditional teacher-student train-\ning for speech emotion recognition,” arXiv preprint arXiv:2112.00158, 2021.\n[31] R. Lotfian and C. Busso, “Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from\nexisting podcast recordings,” IEEE Transactions on Affective Computing, vol. 10, no. 4, pp. 471–483, 2019.\n[32] M. Li, B. Yang, J. Levy, A. Stolcke, V . Rozgic, S. Matsoukas, C. Papayiannis, D. Bone, and C. Wang, “Contrastive unsuper-\nvised learning for speech emotion recognition,” in Proceedings of the IEEE International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP), Toronto, ON, Canada: IEEE, 2021, pp. 6329–6333.\n[33] M. Jaiswal and E. M. Provost, “Best practices for noise-based augmentation to improve the performance of emotion recog-\nnition ”in the wild”,” arXiv preprint arXiv:2104.08806, 2021.\n[34] R. Pappagari, J. Villalba, P. ˙Zelasko, L. Moro-Velazquez, and N. Dehak, “Copypaste: An augmentation method for speech\nemotion recognition,” in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), IEEE, 2021, pp. 6324–6328.\n[35] S. S. Rajan, S. Udeshi, and S. Chattopadhyay, “Aequevox: Automated fairness testing of speech recognition systems,” arXiv\npreprint arXiv:2110.09843, 2021.\n[36] C. Gorrostieta, R. Lotfian, K. Taylor, R. Brutti, and J. Kane, “Gender de-biasing in speech emotion recognition,” in Proceed-\nings of the Annual Conference of the International Speech Communication Association (INTERSPEECH) , Graz, Austria:\nISCA, 2019, pp. 2823–2827.\n[37] G. Doddington, W. Liggett, A. Martin, M. Przybocki, and D. A. Reynolds, “Sheep, goats, lambs and wolves: A statistical\nanalysis of speaker performance in the nist 1998 speaker recognition evaluation,” in Proceedings of the 5th International\nConference on Spoken Language Processing (ICSLP), Sydney, Australia: ISCA, 1998, pp. 1–4.\n[38] W.-N. Hsu, A. Sriram, A. Baevski, T. Likhomanenko, Q. Xu, V . Pratap, J. Kahn, A. Lee, R. Collobert, G. Synnaeve, and\nM. Auli, “Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training,” arXiv preprint arXiv:2104.01027,\n2021.\n[39] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux, “V oxpopuli: A large-\nscale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,” inProceedings of\nthe 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), Bangkok, Thailand, 2021, pp. 993–1003.\n[40] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y . Saraf, J. Pino, A. Baevski,\nA. Conneau, and M. Auli, “Xls-r: Self-supervised cross-lingual speech representation learning at scale,” arXiv preprint\narXiv:2111.09296, 2021.\n[41] G. Trigeorgis, F. Ringeval, R. Br ¨uckner, E. Marchi, M. Nicolaou, B. Schuller, and S. Zafeiriou, “Adieu features? end-to-\nend speech emotion recognition using a deep convolutional recurrent network,” in Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSP), Shanghai, China: IEEE, 2016, pp. 5200–5204.\n[42] A. Triantafyllopoulos and B. W. Schuller, “The role of task and acoustic similarity in audio transfer learning: Insights from\nthe speech emotion recognition case,” inProceedings of the IEEE International Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP), Toronto, ON, Canada: IEEE, 2021, pp. 7268–7272.\n[43] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and M. D. Plumbley, “Panns: Large-scale pretrained audio neural networks for\naudio pattern recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2880–2894,\n2020.\n[44] A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, “Multimodal sentiment intensity analysis in videos: Facial gestures and\nverbal messages,” IEEE Intelligent Systems, vol. 31, no. 6, pp. 82–88, 2016.\n[45] H. Wierstorf, Gender annotations for Multimodal Opinion-level Sentiment Intensity dataset (MOSI) , Zenodo, 2023. DOI :\n10.5281/zenodo.7554349.\n[46] M. Munezero, C. S. Montero, E. Sutinen, and J. Pajunen, “Are they different? affect, feeling, emotion, sentiment, and opinion\ndetection in text,” IEEE Transactions on Affective Computing, vol. 5, no. 2, pp. 101–111, 2014.\n[47] L. Tian, C. Lai, and J. Moore, “Polarity and intensity: The two aspects of sentiment analysis,” in Proceedings of the Grand\nChallenge and Workshop on Human Multimodal Language, Melbourne, Australia: ACL, 2018, pp. 40–47.\n[48] J. M. Zhang, M. Harman, L. Ma, and Y . Liu, “Machine learning testing: Survey, landscapes and horizons,”IEEE Transactions\non Software Engineering, vol. 48, no. 1, pp. 1–36, 2020.\n[49] L. I.-K. Lin, “A concordance correlation coefficient to evaluate reproducibility,” Biometrics, vol. 45, no. 1, pp. 255–268,\n1989.\n[50] F. Ringeval, B. Schuller, M. Valstar, R. Cowie, H. Kaya, M. Schmitt, S. Amiriparian, N. Cummins, D. Lalanne, A. Michaud,\net al., “Avec 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition,” in Proceedings of the\n2018 on audio/visual emotion challenge and workshop, 2018, pp. 3–13.\n[51] E. Parada-Cabaleiro, A. Baird, A. Batliner, N. Cummins, S. Hantke, and B. Schuller, “The perception of emotions in nois-\nified nonsense speech,” in Proceedings of the Annual Conference of the International Speech Communication Association\n(INTERSPEECH), Stockholm, Sweden: ISCA, Aug. 2017, pp. 3246–3250.\n[52] K. J. Piczak, “ESC: Dataset for Environmental Sound Classification,” in Proceedings of the 23rd Annual ACM Conference\non Multimedia, Brisbane, Australia: ACM Press, Oct. 13, 2015, pp. 1015–1018, ISBN : 978-1-4503-3459-4. DOI : 10.1145/\n2733373.2806390. [Online]. Available: http://dl.acm.org/citation.cfm?doid=2733373.2806390.\n19\nDawn of the transformer era in speech emotion recognition PREPRINT\n[53] S. Corbett-Davies and S. Goel, “The measure and mismeasure of fairness: A critical review of fair machine learning,” arXiv\npreprint arXiv:1808.00023, 2018.\n[54] J. K. Fitzsimons, A. A. Ali, M. A. Osborne, and S. J. Roberts, “A general framework for fair regression,” Entropy, vol. 21,\np. 741, 8 2019.\n[55] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language\nunderstanding,” inProceedings of NAACL-HLT, Minneapolis, MN, USA: Association for Computational Linguistics (ACL),\n2019, pp. 4171–4186.\n[56] L. Pepino, P. Riera, L. Ferrer, and A. Gravano, “Fusion approaches for emotion recognition from speech using acoustic\nand text-based features,” in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing\n(ICASSP), IEEE, Barcelona, Spain, 2020, pp. 6484–6488.\n[57] A. Triantafyllopoulos, J. Wagner, H. Wierstorf, M. Schmitt, U. Reichel, F. Eyben, F. Burkhardt, and B. W. Schuller, “Probing\nspeech emotion recognition transformers for linguistic knowledge,” Proceedings of the Annual Conference of the Interna-\ntional Speech Communication Association (INTERSPEECH), pp. 146–150, 2022.\n[58] A. T. Liu, S.-w. Yang, P.-H. Chi, P.-c. Hsu, and H.-y. Lee, “Mockingjay: Unsupervised speech representation learning with\ndeep bidirectional transformer encoders,” in Proceedings of the IEEE International Conference on Acoustics, Speech, and\nSignal Processing (ICASSP), Barcelona, Spain: IEEE, 2020, pp. 6419–6423.\n[59] O. Rudovic, J. Lee, M. Dai, B. Schuller, and R. W. Picard, “Personalized machine learning for robot perception of affect and\nengagement in autism therapy,” Science Robotics, vol. 3, no. 19, pp. 1–11, 2018.\n[60] A. Triantafyllopoulos, S. Liu, and B. W. Schuller, “Deep speaker conditioning for speech emotion recognition,” in Proceed-\nings of the IEEE International Conference on Multimedia and Expo (ICME), Shenzhen, China: IEEE, 2021, pp. 1–6.\n[61] K. Sridhar and C. Busso, “Unsupervised personalization of an emotion recognition system: The unique properties of the\nexternalization of valence in speech,” arXiv preprint arXiv:2201.07876, 2022.\n[62] A. D’Amour et al., “Underspecification presents challenges for credibility in modern machine learning,” arXiv preprint\narXiv:2011.03395, 2020.\n[63] L. van der Maaten and G. Hinton, “Visualizing data using t-SNE,” Journal of Machine Learning Research (JMLR), vol. 9,\npp. 2579–2605, 2008.\n[64] D. Erhan, A. Courville, Y . Bengio, and P. Vincent, “Why does unsupervised pre-training help deep learning?” In Proceed-\nings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS) , Sardinia, Italy: PMLR, 2010,\npp. 201–208.\n[65] B. Neyshabur, H. Sedghi, and C. Zhang, “What is being transferred in transfer learning?” In Advances in Neural Information\nProcessing Systems (NeurIPS), Vancouver, BC, Canada, 2020, pp. 512–523.\n[66] H. Sajjad, F. Dalvi, N. Durrani, and P. Nakov, “Poor man’s BERT: smaller and faster transformer models,” arXiv preprint\narXiv:2004.03844, 2020.\n[67] F. Burkhardt, B. Weiss, F. Eyben, J. Deng, and B. Schuller, “Detecting vocal irony,” in Language Technologies for the\nChallenges of the Digital Age, G. Rehm and T. Declerck, Eds., Cham: Springer International Publishing, 2018, pp. 11–22.\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6483219861984253
    },
    {
      "name": "Valence (chemistry)",
      "score": 0.6345716118812561
    },
    {
      "name": "Transformer",
      "score": 0.5952818393707275
    },
    {
      "name": "Concordance correlation coefficient",
      "score": 0.5573077201843262
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.47143855690956116
    },
    {
      "name": "Speech recognition",
      "score": 0.45131444931030273
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3896080255508423
    },
    {
      "name": "Natural language processing",
      "score": 0.37231552600860596
    },
    {
      "name": "Mathematics",
      "score": 0.1460115611553192
    },
    {
      "name": "Engineering",
      "score": 0.12406972050666809
    },
    {
      "name": "Physics",
      "score": 0.08193069696426392
    },
    {
      "name": "Electrical engineering",
      "score": 0.08153939247131348
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": []
}