{
  "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers",
  "url": "https://openalex.org/W3157528469",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2357614981",
      "name": "Chu, Xiangxiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2079118816",
      "name": "Tian, Zhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1584106083",
      "name": "Wang, Yuqing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1910347209",
      "name": "Zhang Bo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2378679984",
      "name": "Ren Hai-bing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350832834",
      "name": "Wei Xiaolin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2708786869",
      "name": "Xia, Huaxia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2360656043",
      "name": "Shen, Chunhua",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3139445856",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W1549358575",
    "https://openalex.org/W2598666589",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W1817277359",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2910628332",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W2890782586",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2194775991"
  ],
  "abstract": "Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at https://github.com/Meituan-AutoML/Twins .",
  "full_text": "Twins: Revisiting the Design of Spatial Attention in\nVision Transformers\nXiangxiang Chu1, Zhi Tian 2, Yuqing Wang 1, Bo Zhang 1,\nHaibing Ren1, Xiaolin Wei1, Huaxia Xia1, Chunhua Shen2∗\n1 Meituan Inc. 2 The University of Adelaide, Australia\n1 {chuxiangxiang,wangyuqing06,zhangbo97,renhaibing,weixiaolin02,xiahuaxia}@meituan.com\n2 zhi.tian@outlook.com, chunhua@me.com\nAbstract\nVery recently, a variety of vision transformer architectures for dense prediction\ntasks have been proposed and they show that the design of spatial attention is\ncritical to their success in these tasks. In this work, we revisit the design of the spa-\ntial attention and demonstrate that a carefully devised yet simple spatial attention\nmechanism performs favorably against the state-of-the-art schemes. As a result, we\npropose two vision transformer architectures, namely, Twins-PCPVT and Twins-\nSVT. Our proposed architectures are highly efﬁcient and easy to implement, only\ninvolving matrix multiplications that are highly optimized in modern deep learning\nframeworks. More importantly, the proposed architectures achieve excellent per-\nformance on a wide range of visual tasks including image-level classiﬁcation as\nwell as dense detection and segmentation. The simplicity and strong performance\nsuggest that our proposed architectures may serve as stronger backbones for many\nvision tasks. Our Code is available at: https://git.io/Twins.\n1 Introduction\nRecently, Vision Transformers [1–3] have received increasing research interest. Compared to the\nwidely-used convolutional neural networks (CNNs) in visual perception, Vision Transformers enjoy\ngreat ﬂexibility in modeling long-range dependencies in vision tasks, introduce less inductive bias,\nand can naturally process multi-modality input data including images, videos, texts, speech signals,\nand point clouds. Thus, they have been considered to be a strong alternative to CNNs. It is expected\nthat vision transformers are likely to replace CNNs and serve as the most basic component in the\nnext-generation visual perception systems.\nOne of the prominent problems when applying transformers to vision tasks is the heavy computational\ncomplexity incurred by the spatial self-attention operation in transformers, which grows quadratically\nin the number of pixels of the input image. A workaround is the locally-grouped self-attention (or\nself-attention in non-overlapped windows as in the recent Swin Transformer [4]), where the input\nis spatially grouped into non-overlapped windows and the standard self-attention is computed only\nwithin each sub-window. Although it can signiﬁcantly reduce the complexity, it lacks the connections\nbetween different windows and thus results in a limited receptive ﬁeld. As pointed out by many\nprevious works [5–7], a sufﬁciently large receptive ﬁeld is crucial to the performance, particularly for\ndense prediction tasks such as image segmentation and object detection. Swin [4] proposes a shifted\nwindow operation to tackle the issue, where the boundaries of these local windows are gradually\nmoved as the network proceeds. Despite being effective, the shifted windows may have uneven sizes.\nThe uneven windows result in difﬁculties when the models are deployed with ONNX or TensorRT,\n∗Corresponding author.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2104.13840v4  [cs.CV]  30 Sep 2021\nwhich prefers the windows of equal sizes. Another solution is proposed in PVT [ 8]. Unlike the\nstandard self-attention operation, where each query computes the attention weights with all the input\ntokens, in PVT, each query only computes the attention with a sub-sampled version of the input\ntokens. Although its computational complexity in theory is still quadratic, it is already manageable in\npractice.\nFrom a uniﬁed perspective, the core in the aforementioned vision transformers is how the spatial\nattention is designed. Thus, in this work, we revisit the design of the spatial attention in vision\ntransformers. Our ﬁrst ﬁnding is that the global sub-sampled attention in PVT is highly effective,\nand with the applicable positional encodings [ 9], its performance can be on par or even better\nthan state-of-the-art vision transformers (e.g., Swin). This results in our ﬁrst proposed architecture,\ntermed Twins-PCPVT. On top of that, we further propose a carefully-designed yet simple spatial\nattention mechanism, making our architectures more efﬁcient than PVT. Our attention mechanism is\ninspired by the widely-used separable depthwise convolutions and thus we name itspatially separable\nself-attention (SSSA). Our proposed SSSA is composed of two types of attention operations—(i)\nlocally-grouped self-attention (LSA), and (ii) global sub-sampled attention (GSA), where LSA\ncaptures the ﬁne-grained and short-distance information and GSA deals with the long-distance\nand global information. This leads to the second proposed vision transformer architecture, termed\nTwins-SVT. It is worth noting that both attention operations in the architecture are efﬁcient and\neasy-to-implement with matrix multiplications in a few lines of code. Thus, all of our architectures\nhere have great applicability and can be easily deployed.\nWe benchmark our proposed architectures on a number of visual tasks, ranging from image-level\nclassiﬁcation to pixel-level semantic/instance segmentation and object detection. Extensive experi-\nments show that both of our proposed architectures perform favorably against other state-of-the-art\nvision transformers with similar or even reduced computational complexity.\n2 Related Work\nConvolutional neural networks. Characterized by local connectivity, weight sharing, shift-\ninvariance and pooling, CNNs have been the de facto standard model for computer vision tasks.\nThe top-performing models [10–13] in image classiﬁcation also serve as the strong backbones for\ndownstream detection and segmentation tasks.\nVision Transformers. Transformer was ﬁrstly proposed by [14] for machine translation tasks, and\nsince then they have become the state-of-the-art models for NLP tasks, overtaking the sequence-to-\nsequence approach built on LSTM. Its core component is multi-head self-attention which models the\nrelationship between input tokens and shows great ﬂexibility.\nIn 2020, Transformer was introduced to computer vision for image and video processing [1–3, 9, 15–\n17, 17–32]. In the image classiﬁcation task, ViT [ 1] and DeiT [ 2] divide the images into patch\nembedding sequences and feed them into the standard transformers. Although vision transformers\nhave been proved compelling in image classiﬁcation compared with CNNs, a challenge remains when\nit is applied to dense prediction tasks such as object detection and segmentation. These tasks often\nrequire feature pyramids for better processing objects of different scales, and take as inputs the high-\nresolution images, which signiﬁcantly increase the computational complexity of the self-attention\noperations.\nRecently, Pyramid Vision Transformer (PVT) [8] is proposed and can output the feature pyramid [33]\nas in CNNs. PVT has demonstrated good performance in a number of dense prediction tasks. The\nrecent Swin Transformer [4] introduces non-overlapping window partitions and restricts self-attention\nwithin each local window, resulting in linear computational complexity in the number of input\ntokens. To interchange information among different local areas, its window partitions are particularly\ndesigned to shift between two adjacent self-attention layers. The semantic segmentation framework\nOCNet [34] shares some similarities with us and they also interleave the local and global attention.\nHere, we demonstrate this is a general design paradigm in vision transformer backbones rather than\nmerely an incremental module in semantic segmentation.\nGrouped and Separable Convolutions. Grouped convolutions are originally proposed in AlexNet\n[35] for distributed computing. They were proved both efﬁcient and effective in speeding up the\nnetworks. As an extreme case, depthwise convolutions [ 12, 36] use the number of groups that is\n2\nequal to the input or output channels, which is followed by point-wise convolutions to aggregate the\ninformation across different channels. Here, the proposed spatially separable self-attention shares\nsome similarities with them.\nPositional Encodings. Most vision transformers use absolute/relative positional encodings, de-\npending on downstream tasks, which are based on sinusoidal functions [ 14] or learnable [ 1, 2].\nIn CPVT [ 9], the authors propose the conditional positional encodings, which are dynamically\nconditioned on the inputs and show better performance than the absolute and relative ones.\n3 Our Method: Twins\nWe present two simple yet powerful spatial designs for vision transformers. The ﬁrst method is built\nupon PVT [8] and CPVT [9], which only uses the global attention. The architecture is thus termed\nTwins-PCPVT. The second one, termed Twins-SVT, is based on the proposed SSSA which interleaves\nlocal and global attention.\n3.1 Twins-PCPVT\nPVT [8] introduces the pyramid multi-stage design to better tackle dense prediction tasks such as\nobject detection and semantic segmentation. It inherits the absolute positional encoding designed in\nViT [1] and DeiT [2]. All layers utilize the global attention mechanism and rely on spatial reduction\nto cut down the computation cost of processing the whole sequence. It is surprising to see that\nthe recently-proposed Swin transformer [4], which is based on shifted local windows, can perform\nconsiderably better than PVT, even on dense prediction tasks where a sufﬁciently large receptive ﬁeld\nis even more crucial to good performance.\nIn this work, we surprisingly found that the less favored performance of PVT is mainly due to the\nabsolute positional encodings employed in PVT [8]. As shown in CPVT [9], the absolute positional\nencoding encounter difﬁculties in processing the inputs with varying sizes (which are common in\ndense prediction tasks). Moreover, this positional encoding also breaks the translation invariance.\nOn the contrary, Swin transformer makes use of the relative positional encodings, which bypasses the\nabove issues. Here, we demonstrate that this is the main cause why Swin outperforms PVT, and we\nshow that if the appropriate positional encodings are used, PVT can actually achieve on par or even\nbetter performance than the Swin transformer.\nHere, we use the conditional position encoding (CPE) proposed in CPVT [9] to replace the absolute\nPE in PVT. CPE is conditioned on the inputs and can naturally avoid the above issues of the absolute\nencodings. The position encoding generator (PEG) [9], which generates the CPE, is placed after the\nﬁrst encoder block of each stage. We use the simplest form of PEG, i.e., a 2D depth-wise convolution\nwithout batch normalization. For image-level classiﬁcation, following CPVT, we remove the class\ntoken and use global average pooling (GAP) at the end of the stage [9]. For other vision tasks, we\nfollow the design of PVT. Twins-PCPVT inherits the advantages of both PVT and CPVT, which\nmakes it easy to be implemented efﬁciently. Our extensive experimental results show that this simple\ndesign can match the performance of the recent state-of-the-art Swin transformer. We have also\nattempted to replace the relative PE with CPE in Swin, which however does not result in noticeable\nperformance gains, as shown in our experiments. We conjecture that this maybe due to the use of\nshifted windows in Swin, which might not work well with CPE.\nArchitecture settings We report the detailed settings of Twins-PCPVT in Table 9 (in supplemen-\ntary), which are similar to PVT [ 8]. Therefore, Twins-PCPVT has similar FLOPs and number of\nparameters to [8].\n3.2 Twins-SVT\nVision transformers suffer severely from the heavy computational complexity in dense prediction\ntasks due to high-resolution inputs. Given an input of H ×W resolution, the complexity of self-\nattention with dimension d is O(H2W2d). Here, we propose the spatially separable self-attention\n(SSSA) to alleviate this challenge. SSSA is composed of locally-grouped self-attention (LSA) and\nglobal sub-sampled attention (GSA).\n3\nPatch embedding!\"×$\"×64 !'×$'×128 !+,×$+,×320 !/0×$/0×512\nTransformer Encoder\nPEG\nStage 1\n×2 Transformer Encoder\nTransformer Encoder\nPEG\nStage 2\n×2 Transformer Encoder\nTransformer Encoder\nPEG\nStage 3\n×5 Transformer Encoder\nTransformer Encoder\nPEG\nStage 4\n×2 Transformer Encoder\nFigure 1 – Architecture of Twins-PCPVT-S. “PEG\" is the positional encoding generator from CPVT [9].\nLocal AttentionGlobal Attention\nW/n\nH W\nH/m\nmn\n(a) (b)\nFigure 2 – (a) Twins-SVT interleaves locally-grouped\nattention (LSA) and global sub-sampled attention (GSA).\n(b) Schematic view of the locally-grouped attention\n(LSA) and global sub-sampled attention (GSA).\nLocally-grouped self-attention (LSA). Mo-\ntivated by the group design in depthwise convo-\nlutions for efﬁcient inference, we ﬁrst equally\ndivide the 2D feature maps into sub-windows,\nmaking self-attention communications only hap-\npen within each sub-window. This design also\nresonates with the multi-head design in self-\nattention, where the communications only occur\nwithin the channels of the same head. To be spe-\nciﬁc, the feature maps are divided into m ×n\nsub-windows. Without loss of generality, we\nassume H%m = 0and W%n = 0. Each group\ncontains HW\nmn elements, and thus the computa-\ntion cost of the self-attention in this window is\nO(H2W2\nm2n2 d), and the total cost is O(H2W2\nmn d). If\nwe let k1 = H\nm and k2 = W\nn , the cost can be\ncomputed as O(k1k2HWd ), which is signiﬁcantly more efﬁcient when k1 ≪H and k2 ≪W and\ngrows linearly with HW if k1 and k2 are ﬁxed.\nAlthough the locally-grouped self-attention mechanism is computation friendly, the image is divided\ninto non-overlapping sub-windows. Thus, we need a mechanism to communicate between different\nsub-windows, as in Swin. Otherwise, the information would be limited to be processed locally,\nwhich makes the receptive ﬁeld small and signiﬁcantly degrades the performance as shown in our\nexperiments. This resembles the fact that we cannot replace all standard convolutions by depth-wise\nconvolutions in CNNs.\nGlobal sub-sampled attention (GSA). A simple solution is to add extra standard global self-\nattention layers after each local attention block, which can enable cross-group information exchange.\nHowever, this approach would come with the computation complexity ofO(H2W2d).\nHere, we use a single representative to summarize the important information for each of m ×n\nsub-windows and the representative is used to communicate with other sub-windows (serving as the\nkey in self-attention), which can dramatically reduce the cost to O(mnHWd ) =O(H2W2d\nk1k2\n). This\nis essentially equivalent to using the sub-sampled feature maps as the key in attention operations,\nand thus we term it global sub-sampled attention (GSA). If we alternatively use the aforementioned\nLSA and GSA like separable convolutions (depth-wise + point-wise). The total computation cost\nis O(H2W2d\nk1k2\n+ k1k2HWd ). We have H2W2d\nk1k2\n+ k1k2HWd ≥2HWd\n√\nHW . The minimum is\nobtained when k1 ·k2 =\n√\nHW . We note that H = W = 224is popular in classiﬁcation. Without\nloss of generality, we use square sub-windows, i.e., k1 = k2. Therefore, k1 = k2 = 15is close\nto the global minimum for H = W = 224. However, our network is designed to include several\nstages with variable resolutions. Stage 1 has feature maps of 56 ×56, the minimum is obtained when\nk1 = k2 =\n√\n56 ≈7. Theoretically, we can calibrate optimal k1 and k2 for each of the stages. For\nsimplicity, we use k1 = k2 = 7everywhere. As for stages with lower resolutions, we control the\nsummarizing window-size of GSA to avoid too small amount of generated keys. Speciﬁcally, we use\nthe size of 4, 2 and 1 for the last three stages respectively.\n4\nAs for the sub-sampling function, we investigate several options including average pooling, depth-\nwise strided convolutions, and regular strided convolutions. Empirical results show that regular\nstrided convolutions perform best here. Formally, our spatially separable self-attention (SSSA) can\nbe written as\nˆzl\nij = LSA\n(\nLayerNorm\n(\nzl−1\nij\n))\n+ zl−1\nij ,\nzl\nij = FFN\n(\nLayerNorm\n(ˆzl\nij\n))\n+ ˆzl\nij,\nˆzl+1 = GSA\n(\nLayerNorm\n(\nzl))\n+ zl,\nzl+1 = FFN\n(\nLayerNorm\n(ˆzl+1))\n+ ˆzl+1,\ni ∈{1, 2, ...., m}, j∈{1, 2, ...., n}\n(1)\nwhere LSA means locally-grouped self-attention within a sub-window; GSA is the global sub-sampled\nattention by interacting with the representative keys (generated by the sub-sampling functions) from\neach sub-window ˆzij ∈Rk1×k2×C. Both LSA and GSA have multiple heads as in the standard\nself-attention.The PyTorch code of LSA is given in Algorithm 1 (in supplementary).\nAgain, we use the PEG of CPVT [ 9] to encode position information and process variable-length\ninputs on the ﬂy. It is inserted after the ﬁrst block in each stage.\nModel variants. The detailed conﬁgure of Twins-SVT is shown in Table 10 (in supplementary). We\ntry our best to use the similar settings as in Swin [4] to make sure that the good performance is due to\nthe new design paradigm.\nComparison with PVT. PVT entirely utilizes global attentions as DeiT does while our method\nmakes use of spatial separable-like design with LSA and GSA, which is more efﬁcient.\nComparison with Swin. Swin utilizes the alternation of local window based attention where the\nwindow partitions in successive layers are shifted. This is used to introduce communication among\ndifferent patches and to increase the receptive ﬁeld. However, this procedure is relatively complicated\nand may not be optimized for speed on devices such as mobile devices. Swin Transformer depends on\ntorch.roll() to perform cyclic shift and its reverse on features. This operation is memory unfriendly and\nrarely supported by popular inference frameworks such as NVIDIA TensorRT, Google Tensorﬂow-\nLite, and Snapdragon Neural Processing Engine SDK (SNPE), etc. This hinders the deployment of\nSwin either on the server-side or on end devices in a production environment. In contrast, Twins\nmodels don’t require such an operation and only involve matrix multiplications that are already\noptimized well in modern deep learning frameworks. Therefore, it can further beneﬁt from the\noptimization in a production environment. For example, we converted Twins-SVT-S from PyTorch\nto TensorRT , and its throughput is boosted by 1.7×. Moreover, our local-global design can better\nexploit the global context, which is known to play an important role in many vision tasks.\nFinally, one may note that the network conﬁgures (e.g., such as depths, hidden dimensions, number\nof heads, and the expansion ratio of MLP) of our two variants are sightly different. This is intended\nbecause we want to make fair comparisons to the two recent well-known transformers PVT and Swin.\nPVT prefers a slimmer and deeper design while Swin is wider and shallower. This difference makes\nPVT have slower training than Swin. Twins-PCPVT is designed to compare with PVT and shows\nthat a proper positional encoding design can greatly boost the performance and make it on par with\nrecent state-of-the-art models like Swin. On the other hand, Twins-SVT demonstrates the potential of\na new paradigm as to spatially separable self-attention is highly competitive to recent transformers.\n4 Experiments\n4.1 Classiﬁcation on ImageNet-1K\nWe ﬁrst present the ImageNet classiﬁcation results with our proposed models. We carefully control\nthe experiment settings to make fair comparisons against recent works [2, 8, 9]. All our models are\ntrained for 300 epochs with a batch size of 1024 using the AdamW optimizer [37]. The learning rate\nis initialized to be 0.001 and decayed to zero within 300 epochs following the cosine strategy. We use\na linear warm-up in the ﬁrst ﬁve epochs and the same regularization setting as in [2]. Note that we\ndo not utilize extra tricks in [26, 28] to make fair comparisons although it may further improve the\n5\nperformance of our method. We use increasing stochastic depth [38] augmentation of 0.2, 0.3, 0.5 for\nsmall, base and large model respectively. Following Swin [4], we use gradient clipping with a max\nnorm of 5.0 to stabilize the training process, which is especially important for the training of large\nmodels.\nWe report the classiﬁcation results on ImageNet-1K [39] in Table 1. Twins-PCPVT-Soutperforms\nPVT-small by 1.4% and obtains similar result as Swin-T with 18% fewer FLOPs. Twins-SVT-Sis\nbetter than Swin-T with about 35% fewer FLOPs. Other models demonstrate similar advantages.\nIt is interesting to see that, without bells and whistles, Twins-PCPVT performs on par with the\nrecent state-of-the-art Swin, which is based on much more sophisticated designs as mentioned above.\nMoreover, Twins-SVT also achieves similar or better results, compared to Swin, indicating that the\nspatial separable-like design is an effective and promising paradigm.\nOne may challenge our improvements are due to the use of the better positional encoding PEG. Thus,\nwe also replace the relative PE in Swin-T with PEG [ 9], but the Swin-T’s performance cannot be\nimproved (being 81.2%).\n4.2 Semantic Segmentation on ADE20K\nWe further evaluate the performance on segmentation tasks. We test on the ADE20K dataset [42], a\nchallenging scene parsing task for semantic segmentation, which is popularly evaluated by recent\nTransformer-based methods. This dataset contains 20K images for training and 2K images for\nvalidation. Following the common practices, we use the training set to train our models and report\nthe mIoU on the validation set. All models are pretrained on the ImageNet-1k dataset.\nTwins-PCPVT vs. PVT. We compare our Twins-PCPVT with PVT [8] because they have similar\ndesign and computational complexity. To make fair comparisons, we use the Semantic FPN framework\n[43] and exactly the same training settings as in PVT. Speciﬁcally, we train 80K steps with a batch\nsize of 16 using AdamW [ 37]. The learning rate is initialized as 1 ×10−4 and scheduled by the\n‘poly’ strategy with the power coefﬁcient of 0.9. We apply the drop-path regularization of 0.2 for the\nbackbone and weight decay 0.0005 for the whole network. Note that we use a stronger drop-path\nregularization of 0.4 for the large model to avoid over-ﬁtting. For Swin, we use their ofﬁcial code\nand trained models. We report the results in Table 2. With comparable FLOPs, Twins-PCPVT-S\noutperforms PVT-Small with a large margin (+4.5% mIoU), which also surpasses ResNet-50 by 7.6%\nmIoU. It also outperforms Swin-T with a clear margin. Besides, Twins-PCPVT-Balso achieves 3.3%\nhigher mIoU than PVT-Medium, and Twins-PCPVT-Lsurpasses PVT-Large with 4.3% higher mIoU.\nTwins-SVT vs. Swin. We also compare our Twins-SVT with the recent state-of-the-art model\nSwin [4]. With the Semantic FPN framework and the above settings, Twins-SVT-Sachieves better\nperformance (+1.7%) than Swin-T. Twins-SVT-B obtains comparable performance with Swin-S\nand Twins-SVT-Loutperforms Swin-B by 0.7% mIoU (left columns in Table 2). In addition, Swin\nevaluates its performance using the UperNet framework [ 44]. We transfer our method to this\nframework and use exactly the same training settings as [ 4]. To be speciﬁc, we use the AdamW\noptimizer to train all models for 160k iterations with a global batch size of 16. The initial learning\nrate is 6×10−5 and linearly decayed to zero. We also utilize warm-up during the ﬁrst 1500 iterations.\nMoreover, we apply the drop-path regularization of 0.2 for the backbone and weight decay 0.01\nfor the whole network. We report the mIoU of both single scale and multi-scale testing (we use\nscales from 0.5 to 1.75 with step 0.25) in the right columns of Table 2. Both with multi-scale\ntesting, Twins-SVT-Soutperforms Swin-T by 1.3% mIoU. Moreover, Twins-SVT-Lachieves new\nstate of the art result 50.2% mIoU under comparable FLOPs and outperforms Swin-B by 0.5% mIoU.\nTwins-PCPVT also achieves comparable performance to Swin [4].\n4.3 Object Detection and Segmentation on COCO\nWe evaluate the performance of our method using two representative frameworks: RetinaNet [46]\nand Mask RCNN [47]. Speciﬁcally, we use our transformer models to build the backbones of these\ndetectors. All the models are trained under the same setting as in [ 8]. Since PVT and Swin report\ntheir results using different frameworks, we try to make fair comparison and build consistent settings\nfor future methods. Speciﬁcally, we report standard 1×-schedule (12 epochs) detection results on\nthe COCO 2017 dataset [48] in Tables 3 and 4. As for the evaluation based on RetinaNet, we train\n6\nTable 1 – Comparisons with state-of-the-art methods for ImageNet-1K classiﬁcation. Throughput is tested on\nthe batch size of 192 on a single V100 GPU. All models are trained and evaluated on 224×224 resolution on\nImageNet-1K dataset. †: w/ CPVT’s position encodings [9].\nMethod Param (M) FLOPs (G) Throughput (Images/s) Top-1 (%)\nConvNet\nRegNetY-4G [40] 21 4.0 1157 80.0\nRegNetY-8G [40] 39 8.0 592 81.7\nRegNetY-16G [40] 84 16.0 335 82.9\nTransformer\nDeiT-Small/16 [2] 22.1 4.6 437 79.9\nCrossViT-S [30] 26.7 5.6 - 81.0\nT2T-ViT-14 [27] 22 5.2 - 81.5\nTNT-S [15] 23.8 5.2 - 81.3\nCoaT Mini [17] 10 6.8 - 80.8\nCoaT-Lite Small [17] 20 4.0 - 81.9\nPVT-Small [8] 24.5 3.8 820 79.8\nCPVT-Small-GAP [9] 23 4.6 817 81.5\nTwins-PCPVT-S (ours) 24.1 3.8 815 81.2 (+1.3)\nSwin-T [4] 29 4.5 766 81.3\nSwin-T + CPVT† 28 4.4 766 81.2\nTwins-SVT-S (ours) 24 2.9 1059 81.7 (+1.8)\nT2T-ViT-19 [27] 39.2 8.9 - 81.9\nPVT-Medium [8] 44.2 6.7 526 81.2\nTwins-PCPVT-B(ours) 43.8 6.7 525 82.7 (+0.8)\nSwin-S [4] 50 8.7 444 83.0\nTwins-SVT-B (ours) 56 8.6 469 83.2 (+1.3)\nViT-Base/16 [1] 86.6 17.6 86 77.9\nDeiT-Base/16 [2] 86.6 17.6 292 81.8\nT2T-ViT-24 [27] 64.1 14.1 - 82.3\nCrossViT-B [30] 104.7 21.2 - 82.2\nTNT-B [15] 66 14.1 - 82.8\nCPVT-B [9] 88 17.6 292 82.3\nPVT-Large [8] 61.4 9.8 367 81.7\nTwins-PCPVT-L(ours) 60.9 9.8 367 83.1 (+5.2)\nSwin-B [4] 88 15.4 275 83.3\nTwins-SVT-L (ours) 99.2 15.1 288 83.7 (+5.8)\nHybrid\nBoTNet-S1-59 [29] 33.5 7.3 - 81.7\nBossNet-T1 [41] - 7.9 - 81.9\nCvT-13 [31] 20 4.5 - 81.6\nBoTNet-S1-110 [29] 54.7 10.9 - 82.8\nCvT-21 [31] 32 7.1 - 82.5\nall the models using AdamW [ 37] optimizer for 12 epochs with a batch size of 16. The initial\nlearning rate is 1×10−4, started with 500-iteration warmup and decayed by 10×at the 8th and 11th\nepoch, respectively. We use stochastic drop path regularization of 0.2 and weight decay 0.0001.\nThe implementation is based on MMDetection [49]. For the Mask R-CNN framework, we use the\ninitial learning rate of 2×10−4 as in [8]. All other hyper-parameters follow the default settings in\nMMDetection. As for 3 ×experiments, we follow the common multi-scale training in [ 3, 4], i.e.,\nrandomly resizing the input image so that its shorter side is between 480 and 800 while keeping\nlonger one less than 1333. Moreover, for 3×training of Mask R-CNN, we use an initial learning rate\nof 0.0001 and weight decay of 0.05 for the whole network as [4].\nFor 1×schedule object detection with RetinaNet, Twins-PCPVT-Ssurpasses PVT-Small with 2.6%\nmAP and Twins-PCPVT-B exceeds PVT-Medium by 2.4% mAP on the COCO val2017 split.\nTwins-SVT-S outperforms Swin-T with 1.5% mAP while using 12% fewer FLOPs. Our method\noutperform the others with similar advantage in 3×experiments.\n7\nTable 2 – Performance comparisons with different backbones on ADE20K validation dataset. FLOPs are\ntested on 512×512 resolution. All backbones are pretrained on ImageNet-1k except SETR [ 45], which is\npretrained on ImageNet-21k dataset.\nBackbone Semantic FPN 80k (PVT [8] setting) Upernet 160k (Swin [4] setting)\nFLOPs Param mIoU FLOPs Param mIoU/MS mIoU\n(G) (M) (%) (G) (M) (%)\nResNet50 [10] 45 28.5 36.7 - - -\nPVT-Small [8] 40 28.2 39.8 - - -\nTwins-PCPVT-S (ours) 40 28.4 44.3 (+7.6) 234 54.6 46.2/47.5\nSwin-T [4] 46 31.9 41.5 237 59.9 44.5/45.8\nTwins-SVT-S (ours) 37 28.3 43.2 (+6.5) 228 54.4 46.2/47.1\nResNet101 [10] 66 47.5 38.8 258 86 -/44.9\nPVT-Medium [8] 55 48.0 41.6 - - -\nTwins-PCPVT-B (ours) 55 48.1 44.9 (+6.1) 250 74.3 47.1/48.4\nSwin-S [4] 70 53.2 45.2 261 81.3 47.6/49.5\nTwins-SVT-B (ours) 67 60.4 45.3 (+6.5) 261 88.5 47.7/48.9\nResNetXt101-64×4d [13] - 86.4 40.2 - - -\nPVT-Large [8] 71 65.1 42.1 - - -\nTwins-PCPVT-L (ours) 71 65.3 46.4 (+6.2) 269 91.5 48.6/49.8\nSwin-B [4] 107 91.2 46.0 299 121 48.1/49.7\nTwins-SVT-L (ours) 102 103.7 46.7 (+6.5) 297 133 48.8/50.2\nBackbone PUP (SETR [45] setting) MLA (SETR [45] setting)\nT-Large (SETR) [45] - 310 50.1 - 308 48.6/50.3\nFor 1×object segmentation with the Mask R-CNN framework, Twins-PCPVT-S brings similar\nimprovements (+2.5% mAP) over PVT-Small. Compared with PVT-Medium, Twins-PCPVT-B\nobtains 2.6% higher mAP, which is also on par with that of Swin. Both Twins-SVT-S and Twins-SVT-\nB achieve better or slightly better performance compared to the counterparts of Swin. As for large\nmodels, our results are shown in Table 8 (in supplementary) and we also achieve better performance\nwith comparable FLOPs.\nTable 3 – Object detection performance on the COCO val2017 split using the RetinaNet framework. 1× is\n12 epochs and 3× is 36 epochs. “MS”: Multi-scale training. FLOPs are evaluated on 800×600 resolution.\nBackboneFLOPs\n(G)\nParam\n(M)\nRetinaNet 1× RetinaNet 3× + MS\nAP AP 50AP75APSAPM APLAP AP 50AP75APSAPM APL\nResNet50 [10] 111 37.7 36.3 55.3 38.6 19.3 40.0 48.8 39.0 58.4 41.8 22.4 42.8 51.6\nPVT-Small [8] 118 34.2 40.4 61.3 43.0 25.0 42.9 55.7 42.2 62.7 45.0 26.2 45.2 57.2\nTwins-PCPVT-S (ours) 118 34.4 43.0(+6.7) 64.1 46.0 27.5 46.3 57.3 45.2(+6.2) 66.5 48.6 30.0 48.8 58.9\nSwin-T [4] 118 38.5 41.5 62.1 44.2 25.1 44.9 55.5 43.9 64.8 47.1 28.4 47.2 57.8\nTwins-SVT-S (ours) 104 34.3 43.0(+6.7) 64.2 46.3 28.0 46.4 57.5 45.6(+6.6) 67.1 48.6 29.8 49.3 60.0\nResNet101 [10] 149 56.7 38.5 57.8 41.2 21.4 42.6 51.1 40.9 60.1 44.0 23.7 45.0 53.8\nResNeXt101-32×4d [13] 151 56.4 39.9 59.6 42.7 22.3 44.2 52.5 41.4 61.0 44.3 23.9 45.5 53.7\nPVT-Medium [8] 151 53.9 41.9 63.1 44.3 25.0 44.9 57.6 43.2 63.8 46.1 27.3 46.3 58.9\nTwins-PCPVT-B (ours) 151 54.1 44.3(+5.8) 65.6 47.3 27.9 47.9 59.6 46.4(+5.5) 67.7 49.8 31.3 50.2 61.4\nSwin-S [4] 162 59.8 44.5 65.7 47.5 27.4 48.0 59.9 46.3 67.4 49.8 31.1 50.3 60.9\nTwins-SVT-B (ours) 163 67.0 45.3(+6.8) 66.7 48.1 28.5 48.9 60.6 46.9(+6.0) 68.0 50.2 31.7 50.3 61.8\n4.4 Ablation Studies\nTable 5 – Classiﬁcation performance for different combinations of LSA\n(L) and GSA (G) blocks based on the small model.\nFunction Type Params FLOPs Top-1\n(M) (G) (%)\n(L, L, L) 8.8 2.2 76.9\n(L, LLG, LLG, G) 23.5 2.8 81.5\n(L, LG, LG, G) 24.1 2.8 81.7\n(L, L, L, G) 22.2 2.9 80.5\nPVT-small (G, G, G, G) [8] 24.5 3.8 79.8\nConﬁgurations of LSA and\nGSA blocks. We evaluate dif-\nferent combinations of LSA and\nGSA based on our small model\nand present the ablation results in\nTable 5. The models with only\nlocally-grouped attention fail to\n8\nTable 4 – Object detection and instance segmentation performance on the COCO val2017 dataset using the\nMask R-CNN framework. FLOPs are evaluated on a 800×600 image.\nBackboneFLOPs\n(G)\nParam\n(M)\nMask R-CNN 1× Mask R-CNN 3× + MS\nAPb APb\n50APb\n75APm APm\n50APm\n75 APb APb\n50APb\n75APmAPm\n50APm\n75\nResNet50 [10] 174 44.2 38.0 58.6 41.4 34.4 55.1 36.7 41.0 61.7 44.9 37.1 58.4 40.1\nPVT-Small [8] 178 44.1 40.4 62.9 43.8 37.8 60.1 40.3 43.0 65.3 46.9 39.9 62.5 42.8\nTwins-PCPVT-S (ours) 178 44.3 42.9(+4.9) 65.8 47.1 40.0 (+5.6) 62.7 42.9 46.8(+5.8) 69.3 51.8 42.6 66.3 46.0\nSwin-T [4] 177 47.8 42.2 64.6 46.2 39.1 61.6 42.0 46.0 68.2 50.2 41.6 65.1 44.8\nTwins-SVT-S (ours) 164 44.0 43.4(+5.4) 66.0 47.3 40.3 (+5.9) 63.2 43.4 46.8(+5.8) 69.2 51.2 42.6 66.3 45.8\nResNet101 [10] 210 63.2 40.4 61.1 44.2 36.4 57.7 38.8 42.8 63.2 47.1 38.5 60.1 41.3\nResNeXt101-32×4d [13] 212 62.8 41.9 62.5 45.9 37.5 59.4 40.2 44.0 64.4 48.0 39.2 61.4 41.9\nPVT-Medium [8] 211 63.9 42.0 64.4 45.6 39.0 61.6 42.1 44.2 66.0 48.2 40.5 63.1 43.5\nTwins-PCPVT-B (ours) 211 64.0 44.6(+4.2) 66.7 48.9 40.9 (+4.5) 63.8 44.2 47.9(+5.1) 70.1 52.5 43.2 67.2 46.3\nSwin-S [4] 222 69.1 44.8 66.6 48.9 40.9 63.4 44.2 47.6 69.4 52.5 42.8 66.5 46.4\nTwins-SVT-B (ours) 224 76.3 45.2(+4.8) 67.6 49.3 41.5 (+5.1) 64.5 44.8 48.0(+5.2) 69.5 52.7 43.0 66.8 46.6\nobtain good performance (76.9%)\nbecause this setting has a limited and small receptive ﬁeld. An extra global attention layer in the last\nstage can improve the classiﬁcation performance by 3.6%. Local-Local-Global ( abbr. LLG) also\nachieves good performance (81.5%), but we do not use this design in this work.\nTable 6 – ImageNet classiﬁcation perfor-\nmance of different forms of sub-sampled\nfunctions for the global sub-sampled atten-\ntion (GSA).\nFunction Type Top-1(%)\n2D Conv. 81.7\n2D Separable Conv. 81.2\nAverage Pooling 81.2\nSub-sampling functions. We further study how the dif-\nferent sub-sampling functions affect the performance.\nSpeciﬁcally, we compare the regular strided convolutions,\nseparable convolutions and average pooling based on the\n‘small’ model and present the results in Table 6. The ﬁrst\noption performs best and therefore we choose it as our\ndefault implementation.\nPositional Encodings. We replace the relative positional encoding with CPVT for Swin-T and\nreport the detection performance on COCO with RetinaNet and Mask R-CNN in Table 7. The\nCPVT-based Swin cannot achieve improved performance with both frameworks, which indicates\nthat our performance improvements should be owing to the paradigm of Twins-SVT instead of the\npositional encodings.\nTable 7 – Object detection performance on the COCO using different positional encoding strategies.\nBackbone RetinaNet Mask RCNN\nFLOPs(G) Param(M) AP AP 50 AP75 FLOPs(G) Param(M) AP AP 50 AP75\nSwin-T [4] 245 38.5 41.5 62.1 44.2 264 47.8 42.2 64.6 46.2\nSwin-T+CPVT 245 38.5 41.3 62.4 44.1 263 47.8 42.0 64.5 45.9\n5 Conclusion\nIn this paper, we have presented two powerful vision transformer backbones for both image-level\nclassiﬁcation and a few downstream dense prediction tasks. We dub them as twin transformers:\nTwins-PCPVTand Twins-SVT. The former variant explores the applicability of conditional positional\nencodings [9] in pyramid vision transformer [8], conﬁrming its potential for improving backbones in\nmany vision tasks. In the latter variant we revisit current attention design to proffer a more efﬁcient\nattention paradigm. We ﬁnd that interleaving local and global attention can produce impressive\nresults, yet it comes with higher throughputs. Both transformer models set a new state of the art in\nimage classiﬁcation, objection detection and semantic/instance segmentation.\n9\nReferences\n[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In Proc. Int. Conf. Learn. Representations, 2021.\n[2] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efﬁcient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In Proc. Eur. Conf. Comp.\nVis., pages 213–229. Springer, 2020.\n[4] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\n[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and\nfully connected crfs. IEEE Trans. Pattern Anal. Mach. Intell., 40(4):834–848, 2017.\n[6] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel matters–\nimprove semantic segmentation by global convolutional network. In Proc. IEEE Conf. Comp.\nVis. Patt. Recogn., pages 4353–4361, 2017.\n[7] Wei Liu, Andrew Rabinovich, and Alexander C Berg. Parsenet: Looking wider to see better.\narXiv preprint arXiv:1506.04579, 2015.\n[8] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction\nwithout convolutions. arXiv preprint arXiv:2102.12122, 2021.\n[9] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint\narXiv:2102.10882, 2021.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 770–778, 2016.\n[11] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural\nnetworks. In Proc. Int. Conf. Mach. Learn., pages 6105–6114. PMLR, 2019.\n[12] François Chollet. Xception: Deep learning with depthwise separable convolutions. In Proc.\nIEEE Conf. Comp. Vis. Patt. Recogn., pages 1251–1258, 2017.\n[13] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual\ntransformations for deep neural networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages\n1492–1500, 2017.\n[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. Advances in Neural Inf.\nProcess. Syst., pages 6000–6010, 2017.\n[15] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in\ntransformer. arXiv preprint arXiv:2103.00112, 2021.\n[16] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon\nShlens. Stand-alone self-attention in vision models. In Proc. Advances in Neural Inf. Process.\nSyst., volume 32, pages 68–80, 2019.\n[17] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image\ntransformers, 2021.\n[18] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and\nHuaxia Xia. End-to-end video instance segmentation with transformers. In Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., 2021.\n[19] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,\nChunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer.arXiv preprint\narXiv:2012.00364, 2020.\n[20] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture trans-\nformer network for image super-resolution. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,\npages 5791–5800, 2020.\n10\n[21] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning joint spatial-temporal transforma-\ntions for video inpainting. In Proc. Eur. Conf. Comp. Vis., pages 528–543. Springer, 2020.\n[22] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. UP-DETR: Unsupervised pre-training\nfor object detection with transformers. arXiv preprint arXiv:2011.09094, 2020.\n[23] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish\nVaswani. Bottleneck transformers for visual recognition. arXiv: Comp. Res. Repository, 2021.\n[24] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\nsequence-to-sequence perspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\n[25] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,\nand Dustin Tran. Image transformer. In Proc. Int. Conf. Mach. Learn. , volume 80, pages\n4055–4064, 2018.\n[26] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou.\nGoing deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021.\n[27] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis Tay, Jiashi Feng, and\nShuicheng Yan. Tokens-to-token ViT: Training vision transformers from scratch on imagenet.\narXiv preprint arXiv:2101.11986, 2021.\n[28] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie Jin, Anran Wang, and Jiashi Feng.\nToken labeling: Training an 85.4% top-1 accuracy vision transformer with 56M parameters on\nImageNet. arXiv: Comp. Res. Repository, 2021.\n[29] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish\nVaswani. Bottleneck transformers for visual recognition. arXiv preprint arXiv:2101.11605,\n2021.\n[30] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. CrossViT: Cross-attention multi-scale\nvision transformer for image classiﬁcation. arXiv preprint arXiv:2103.14899, 2021.\n[31] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.\nCvT: Introducing convolutions to vision transformers. arXiv: Comp. Res. Repository, 2021.\n[32] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable\ndetr: Deformable transformers for end-to-end object detection. In Proc. Int. Conf. Learn.\nRepresentations, 2021.\n[33] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.\nFeature pyramid networks for object detection. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,\npages 2117–2125, 2017.\n[34] Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. OCNet:\nObject context network for scene parsing. arXiv: Comp. Res. Repository, 2021.\n[35] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classiﬁcation with deep\nconvolutional neural networks. In Proc. Advances in Neural Inf. Process. Syst. , volume 25,\npages 1097–1105, 2012.\n[36] Laurent Sifre and Stéphane Mallat. Rigid-motion scattering for texture classiﬁcation. arXiv\npreprint arXiv:1403.1687, 2014.\n[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proc. Int. Conf.\nLearn. Representations, 2019.\n[38] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with\nstochastic depth. In Proc. Eur. Conf. Comp. Vis., pages 646–661. Springer, 2016.\n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision, 115(3):211–252, 2015.\n[40] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Design-\ning network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 10428–10436, 2020.\n[41] Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng, Bing Wang, Xiaodan Liang, and Xiaojun\nChang. Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural\narchitecture search. arXiv preprint arXiv:2103.12424, 2021.\n[42] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\nScene parsing through ADE20k dataset. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages\n633–641, 2017.\n11\n[43] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollár. Panoptic feature pyramid\nnetworks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 6399–6408, 2019.\n[44] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual parsing\nfor scene understanding. In Proceedings of the European Conference on Computer Vision\n(ECCV), pages 418–434, 2018.\n[45] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers. In CVPR, 2021.\n[46] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense\nobject detection. In Proc. IEEE Int. Conf. Comp. Vis., pages 2980–2988, 2017.\n[47] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask R-CNN. In Proc. IEEE\nInt. Conf. Comp. Vis., pages 2961–2969, 2017.\n[48] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proc.\nEur. Conf. Comp. Vis., pages 740–755, 2014.\n[49] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun,\nWansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng\nCheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping\nShi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open MMLab detection\ntoolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\n12\nA Experiment\nTable 8 – Large models’ object detection performance on the COCOval2017 split using 1× schedule.\nBackbone RetinaNet 1× Mask R-CNN 1×\nParam(M)AP AP 50 AP75 Param(M) AP b APm\nResNeXt101-64×4d [13] 95.5 41.0 60.9 44.0 101.9 42.8 38.4\nPVT-Large [8] 71.1 42.6 63.7 45.4 81.0 42.9 39.5\nTwins-PCPVT-L (ours) 71.2 45.1 (+4.1) 66.4 48.4 81.2 45.4 (+2.6) 41.5\nSwin-B [4] 98.4 44.7 65.9 47.8 107.2 45.5 41.3\nTwins-SVT-L (ours) 110.9 45.7 (+4.7) 67.1 49.2 119.7 45.9 (+3.1) 41.6\nB Algorithm\nAlgorithm 1 PyTorch snippet of LSA.\nclass GroupAttention(nn.Module):\ndef __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., k1=7,\nk2=7):\nsuper(GroupAttention, self).__init__()\nself.dim = dim\nself.num_heads = num_heads\nhead_dim = dim // num_heads\nself.scale = qk_scale or head_dim ** -0.5\nself.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\nself.attn_drop = nn.Dropout(attn_drop)\nself.proj = nn.Linear(dim, dim)\nself.proj_drop = nn.Dropout(proj_drop)\nself.k1 = k1\nself.k2 = k2\ndef forward(self, x, H, W):\nB, N, C = x.shape\nh_group, w_group = H // self.k1, W // self.k2\ntotal_groups = h_group * w_group\nx = x.reshape(B, h_group, self.k1, w_group, self.k2, C).transpose(2, 3)\nqkv = self.qkv(x).reshape(B, total_groups, -1, 3, self.num_heads, C // self.num_heads).permute(3, 0,\n1, 4, 2, 5)\nq, k, v = qkv[0], qkv[1], qkv[2]\nattn = (q @ k.transpose(-2, -1)) * self.scale\nattn = attn.softmax(dim=-1)\nattn = self.attn_drop(attn)\nattn = (attn @ v).transpose(2, 3).reshape(B, h_group, w_group, self.k1, self.k2, C)\nx = attn.transpose(2, 3).reshape(B, N, C)\nx = self.proj(x)\nx = self.proj_drop(x)\nreturn x\nC Architecture Setting\n13\nTable 9 – Conﬁguration details of Twins-PCPVT.\nOutput Size Layer Name Twins-PCPVT-S Twins-PCPVT-B Twins-PCPVT-L\nStage 1 H\n4 × W\n4\nPatch Embedding P1 = 4; C1 = 64\nTransformer\nEncoder with PEG\n\n\nR1 = 8\nN1 = 1\nE1 = 8\n\n× 3\n\n\nR1 = 8\nN1 = 1\nE1 = 8\n\n× 3\n\n\nR1 = 8\nN1 = 1\nE1 = 8\n\n× 3\nStage 2 H\n8 × W\n8\nPatch Embedding P2 = 2; C2 = 128\nTransformer\nEncoder with PEG\n\n\nR2 = 4\nN2 = 2\nE2 = 8\n\n× 3\n\n\nR2 = 4\nN2 = 2\nE2 = 8\n\n× 3\n\n\nR2 = 4\nN2 = 2\nE2 = 8\n\n× 8\nStage 3 H\n16 × W\n16\nPatch Embedding P3 = 2; C3 = 320\nTransformer\nEncoder with PEG\n\n\nR3 = 2\nN3 = 5\nE3 = 4\n\n× 6\n\n\nR3 = 2\nN3 = 5\nE3 = 4\n\n× 18\n\n\nR3 = 2\nN3 = 5\nE3 = 4\n\n× 27\nStage 4 H\n32 × W\n32\nPatch Embedding P4 = 2; C4 =512\nTransformer\nEncoder with PEG\n\n\nR4 = 1\nN4 = 8\nE4 = 4\n\n× 3\n\n\nR4 = 1\nN4 = 8\nE4 = 4\n\n× 3\n\n\nR4 = 1\nN4 = 8\nE4 = 4\n\n× 3\nTable 10 – Conﬁguration details of Twins-SVT.\nOutput Size Layer Name Twins-SVT-S Twins-SVT-B Twins-SVT-L\nStage 1 H\n4 × W\n4\nPatch Embedding P1 = 4; C1 = 64 P1 = 4; C1 = 96 P1 = 4; C1 = 128\nTransformer\nEncoder w/ PEG\n[\nLSA\nGSA\n]\n× 1\n[\nLSA\nGSA\n]\n× 1\n[\nLSA\nGSA\n]\n× 1\nStage 2 H\n8 × W\n8\nPatch Embedding P2 = 2; C2 = 128P2 = 2; C2 = 192 P2 = 2; C2 = 256\nTransformer\nEncoder w/ PEG\n[\nLSA\nGSA\n]\n× 1\n[\nLSA\nGSA\n]\n× 1\n[\nLSA\nGSA\n]\n× 1\nStage 3 H\n16 × W\n16\nPatch Embedding P3 = 2; C3 = 256P3 = 2; C3 = 384 P3 = 2; C3 = 512\nTransformer\nEncoder w/ PEG\n[\nLSA\nGSA\n]\n× 5\n[\nLSA\nGSA\n]\n× 9\n[\nLSA\nGSA\n]\n× 9\nStage 4 H\n32 × W\n32\nPatch Embedding P4 = 2; C4 =512 P4 = 2; C4 =768 P4 = 2; C4 =1024\nTransformer\nEncoder w/ PEG\n[ GSA ]\n× 4\n[ GSA ]\n× 2\n[ GSA ]\n× 2\n14",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5500843524932861
    },
    {
      "name": "Engineering",
      "score": 0.24199774861335754
    },
    {
      "name": "Electrical engineering",
      "score": 0.1560594141483307
    },
    {
      "name": "Voltage",
      "score": 0.06215929985046387
    }
  ],
  "institutions": [],
  "cited_by": 615
}