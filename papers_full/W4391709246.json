{
  "title": "Large language model for table processing: a survey",
  "url": "https://openalex.org/W4391709246",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A3038711751",
      "name": "Weizheng Lu",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2087085944",
      "name": "Jing Zhang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2119191832",
      "name": "Ju Fan",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2547197247",
      "name": "Zihao Fu",
      "affiliations": [
        "Kingsoft (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2133275895",
      "name": "Yueguo Chen",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2124748885",
      "name": "Xiaoyong Du",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A3038711751",
      "name": "Weizheng Lu",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2087085944",
      "name": "Jing Zhang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2119191832",
      "name": "Ju Fan",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2547197247",
      "name": "Zihao Fu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133275895",
      "name": "Yueguo Chen",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2124748885",
      "name": "Xiaoyong Du",
      "affiliations": [
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2595233870",
    "https://openalex.org/W4310390625",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W4205922070",
    "https://openalex.org/W2602856279",
    "https://openalex.org/W1847618513",
    "https://openalex.org/W4399175313",
    "https://openalex.org/W136741500",
    "https://openalex.org/W4392366650",
    "https://openalex.org/W4399208468",
    "https://openalex.org/W2911293880",
    "https://openalex.org/W4393065402",
    "https://openalex.org/W4312600202",
    "https://openalex.org/W3030690619",
    "https://openalex.org/W154507433",
    "https://openalex.org/W2398606196",
    "https://openalex.org/W2593581739",
    "https://openalex.org/W4384642600",
    "https://openalex.org/W1502957213",
    "https://openalex.org/W4399175046",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W89279510",
    "https://openalex.org/W3107064625",
    "https://openalex.org/W2544486974",
    "https://openalex.org/W4362659486",
    "https://openalex.org/W4389523875",
    "https://openalex.org/W4402684297",
    "https://openalex.org/W2997154779",
    "https://openalex.org/W4402671399",
    "https://openalex.org/W4234552385",
    "https://openalex.org/W4402043038",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4402042542",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4402683779",
    "https://openalex.org/W4389520779",
    "https://openalex.org/W4404781897",
    "https://openalex.org/W4389524107",
    "https://openalex.org/W55204438",
    "https://openalex.org/W4392453936",
    "https://openalex.org/W4385571965",
    "https://openalex.org/W2911809258",
    "https://openalex.org/W4365456672",
    "https://openalex.org/W4399175215",
    "https://openalex.org/W4404181113",
    "https://openalex.org/W4402670731",
    "https://openalex.org/W4387321091"
  ],
  "abstract": null,
  "full_text": "Large language model for table processing: a survey\nWeizheng LU1, Jing ZHANG (✉)1, Ju FAN1,2, Zihao FU3, Yueguo CHEN (✉)1,2, Xiaoyong DU1,2\n1   School of Information, Renmin University of China, Beijing 100872, China\n2   Key Laboratory of Data Engineering and Knowledge Engineering, Beijing 100872, China\n3   WPS Office, Kingsoft Co., Zhuhai 519080, China\n The Author(s) 2024. This article is published with open access at link.springer.com and journal.hep.com.cn\n \nAbstract    Tables, typically two-dimensional and structured to\nstore large amounts of data, are essential in daily activities like\ndatabase  queries,  spreadsheet  manipulations,  Web  table\nquestion  answering,  and  image  table  information  extraction.\nAutomating  these  table-centric  tasks  with  Large  Language\nModels  (LLMs)  or  Visual  Language  Models  (VLMs)  offers\nsignificant  public  benefits,  garnering  interest  from  academia\nand  industry.  This  survey  provides  a  comprehensive  overview\nof  table-related  tasks,  examining  both  user  scenarios  and\ntechnical aspects. It covers traditional tasks like table question\nanswering  as  well  as  emerging  fields  such  as  spreadsheet\nmanipulation  and  table  data  analysis.  We  summarize  the\ntraining  techniques  for  LLMs  and  VLMs  tailored  for  table\nprocessing.  Additionally,  we  discuss  prompt  engineering,\nparticularly the use of LLM-powered agents, for various table-\nrelated tasks. Finally, we highlight several challenges, including\ndiverse user input when serving and slow thinking using chain-\nof-thought.\nKeywords    data  mining  and  knowledge  discovery,  table\nprocessing, large language model\n \n1    Introduction\nIn  this  data-driven  era,  a  substantial  volume  of  data  is\nstructured  and  stored  in  the  form  of  tables  [\n1,2].  Everyday\ntasks  involving  tables,  such  as  database  queries,  spreadsheet\nmanipulations,  question  answering  on  Web  tables,  and\ninformation extraction from image tables are common in our\ndaily  lives.  While  some  of  these  tasks  are  tedious  and  error-\nprone,  others  require  specialized  skills  and  should  be\nsimplified for broader accessibility. The automation of table-\nrelated  tasks  provides  substantial  benefits  to  the  general\npublic, garnering significant interest from both academic and\nindustrial sectors [\n2−4].\nRecently, large language models (LLMs) have demonstrated\ntheir effectiveness and versatility across diverse tasks, leading\nto  significant  advancements  in  natural  language  processing\n[\n5].  This  success  has  spurred  researchers  to  investigate  the\napplication  of  LLMs  to  table-related  tasks.  However,  the\nstructure of tables differs from the plain text [ 6] typically used\nduring LLM pre-training.\n● Structured  data  Tables  are  inherently  structured,\ncomposed  of  rows  and  columns,  each  with  its  own\nschema  that  outlines  the  data’s  semantics  and  their\ninterrelations.  Humans  can  effortlessly  interpret  tables\nboth  vertically  and  horizontally,  but  LLMs,  primarily\ntrained  with  sequential  text  data,  struggle  with\nunderstanding the multidimensional aspects of tables.\n●  Complex  reasoning  Tasks  in  table  processing  often\nrequire  numerical  operations  (like  comparisons  or\naggregations),  data  preparation  (such  as  column  type\nannotation  and  missing  value  detection),  and  more\nsophisticated  analyses  (including  feature  engineering\nand  visualization).  These  tasks  demand  intricate\nreasoning,  the  ability  to  decompose  problems  into\nmultiple  steps,  and  logical  operations,  thereby  posing\nsignificant challenges to machine intelligence.\n● Utilizing  external  tools   In  real-world  scenarios,\nhumans  often  depend  on  specialized  tools  such  as\nMicrosoft  Excel,  Python,  or  SQL  for  interacting  with\ntables. For effective table processing, LLMs need to be\nadept at integrating and using these external tools.\nAlthough  many  text-related  tasks,  such  as  those  in  STEM\n(Science,  Technology,  Engineering,  and  Mathematics)  fields,\nrequire complex reasoning and external tools, table processing\ntasks are different due to the structural nature of tables and the\nuser  intent  of  querying  knowledge  from  tables.  For  instance,\nLLMs  need  to  understand  table  schemas,  locate  data  within\ntwo-dimensional  tables,  and  execute  SQL  queries  to  retrieve\ndata.  The  unique  challenges  presented  by  table  processing\ntasks  emphasize  the  need  to  tailor  LLMs  for  these  specific\npurposes.  Early  research,  such  as  TaBERT  [\n7],  TaPas  [8],\nTURL  [9],  and  TaPEx  [10],  adhere  to  the  paradigm  of  pre-\ntraining  or  fine-tuning  neural  language  models  for  tables.\nThese  methods  adapt  model  architectures,  including  position\nembeddings,  attention  mechanisms,  and  learning  objectives\nfor  pretraining  tasks.  While  these  approaches  yield  good\nresults,  they  are  largely  confined  to  specific  table  tasks  like\ntable  question  answering  (table  QA)  and  fact  verification.\nAdditionally, the BERT or BART models they utilize are not\n \nReceived July 26, 2024; accepted November 3, 2024\nE-mail: zhang-jing@ruc.edu.cn; chenyueguo@ruc.edu.cn\nFront. Comput. Sci., 2025, 19(2): 192350\nhttps://doi.org/10.1007/s11704-024-40763-6\nREVIEW ARTICLE\nsufficiently  large  or  versatile  to  handle  a  broader  range  of\ntable tasks. Latest LLM-based approaches tackle table tasks in\ntwo primary ways: (1) curating table datasets and pre-train or\nfine-tune  a  table  model  [\n11−13];  (2)  prompting  an  LLM  or\nbuilding  an  LLM-powered  agent  by  utilizing  the  LLM’s\nstrong  reasoning  ability  to  understand  table  data  [\n14−17].\nThese  newer  methods  leverage  LLM-specific  technologies,\nsuch  as  instruction-tuning  [\n18],  in-context  learning  [19],\nchain-of-thought reasoning [20], and autonomous agents [ 21],\nshowcasing  a  more  versatile  and  comprehensive  approach  to\ntable processing.\nTaxonomy  The  goal  of  this  survey  is  to  offer  a\ncomprehensive review of technological advancements in LLM\nfor  table  processing  and  to  summarize  current  research\ndirections.  As  depicted  in \nFig. 1,  we  have  categorized  the\nliterature into a taxonomy of four key categories: table types\nand  table  tasks,  table  data  representation,  table  training,  and\ntable  prompting.  These  four  categories  cover  distinct  and\ninterrelated  research  topics,  offering  a  systematic  and\ncomprehensive review of LLM for table processing research.\nContribution  The  main  contribution  of  this  survey  is  its\nextensive  coverage  of  a  wide  range  of  table  tasks,  including\nrecently proposed spreadsheet manipulation and data analysis.\nWe  discuss  table  tasks  not  only  from  a  technical  perspective\nbut also from the table data lifecycle and from the end-user’s\nviewpoint.  We  categorize  methods  based  on  the  latest\nparadigms in LLM usage, focusing on instruction-tuning, data\nsynthesis,  chain-of-thought,  ReAct,  and  LLM-powered  agent\napproaches.  We  compile  recent  datasets,  benchmarks,  and\ntraining  corpora.  We  collect  resources  such  as  papers,  code,\nand datasets, which can be accessed at our website [\n22].\nComparison with related surveys  Earlier surveys, such as\nthose  by  Dong  et  al.  [1]  and  Badaro  et  al.  [2],  primarily\nconcentrate  on  pre-training  or  fine-tuning  techniques  using\nsmaller models like BERT [\n7,8] or BART [10]. However, they\ndo  not  address  methods  based  on  LLMs,  particularly  those\ninvolving  prompting  strategies  and  agent-based  approaches.\nAdditionally, some surveys are confined to limited table tasks.\nFor instance, Jin et al. [\n23] focus solely on table QA. Zhang\net al. [ 4] focus on table reasoning, overlooking tasks such as\nspreadsheet  manipulation.  Fang  et  al.  [3]  review  research  on\ntable data prediction, generation, and understanding; however,\nthe discussion on table processing lacks depth. Qin et al. [ 24]\nand Hong et al. [ 25] concentrate on natural language to SQL\n(NL2SQL),  overlooking  spreadsheet  manipulation  and  data\nanalysis tasks.\n \n2    Table types and table tasks\nTables  are  prevalent  data  structures  that  organize  and\nmanipulate  knowledge  and  information  in  almost  every\ndomain. We briefly summarize table formats, table tasks and\ntable data lifecycle.\n \n2.1    Table definition\nThis paper mainly focuses on (semi-)structured tables, which\nare  grids  of  cells  arranged  in  rows  and  columns  [\n26].  Every\ncolumn in a table signifies a specific attribute, each having its\nown data type (e.g., numeric, string, or date). Each row forms\na record populated with diverse attribute values. A cell is the\nintersection  of  a  row  and  a  column.  Cells  are  basic  units  to\nstore  text,  numerical  values,  formulas,  etc.  [\n1].  The  two-\ndimensional structure endows the data with a schema, which\nincludes  column  names,  data  types,  constraints,  and\nrelationships  with  other  tables.  Tables  are  mainly  stored  and\npresented in the following formats:\n●  Spreadsheet  (SS):  Spreadsheets  are  used  by  one-tenth\nof  the  global  population  [\n27],  with  Google  Sheets  and\nMicrosoft  Excel  being  the  two  most  popular  systems.\nSpreadsheet  systems  allow  users  to  format  tables  by\ncustomizing styles (such as font or color) to display or\nhighlight  certain  data.  Spreadsheet  tables  often  feature\nirregular  layouts  with  merged  cells,  hierarchical\ncolumns,  and  annotations,  as  shown  in \nFig. 2(a).  The\nirregular  layout  is  designed  to  improve  human\nunderstanding  of  the  table  data,  but  it  makes  machine\nparsing  difficult.  Spreadsheet  systems  provide  features\nlike  sorting,  filtering,  formulas,  data  visualization,  or\ntable programming (e.g., Visual Basic for Applications,\nVBA)  for  automated  or  semi-automated  data  analysis.\nTypical applications of spreadsheets include: teachers in\nschools  recording  grades,  human-resource  departments\nrecording  employee  information,  and  sales  personnel\ntracking sales data, among others.\n \n \nFig. 1    Taxonomy of LLMs for table processing\n2 Front. Comput. Sci., 2025, 19(2): 192350\n● Web table (WT): The Web hosts a multitude of tables,\ncreated  for  diverse  purposes  and  containing  valuable\ninformation  [\n26,28,29].  These  tables  exist  in  various\nformats,  from  HTML  to  markdown,  JSON,  and  XML.\nAs Web tables are embedded in Web pages, they have\nmuch  contextual  information,  such  as  the  page’s  title,\nthe  surrounding  text,  and  so  on.  Web  tables  can  also\nhave  various  styles  and  embed  URLs.  A  well-known\nWeb  table  example  is  the  table  on  a  Wikipedia  page\nshown  in \nFig. 2(b).  Web  tables  contain  substantial\nfactual  knowledge,  and  the  community  has  been\nworking  to  extract  and  structure  these  tables.  For\nexample,  the  WDC  (Web  Data  Commons)  Web  Table\nCorpus project converts billions of Web pages from the\nCommon Crawl corpus into structured tables [\n26], and\nBhagavatula  et  al.  [29]  extract  millions  of  tables  from\nWikipedia.  The  extraction  process  involves  tasks  like\nentity  linking  or  relation  extraction,  and  the  extracted\ntables can be utilized for question answering.\n● Database  (DB):  Tables  in  relational  databases  are\nhighly  structured,  i.e.,  each  database  table  is  explicitly\ndefined  with  a  schema  at  the  creation  time.  Users\nshould  use  SQL  to  interact  with  database  tables.\nDatabase  systems  are  categorized  into  online\ntransaction  processing  (OLTP)  and  online  analytic\nprocessing  (OLAP).  OLTP  systems  frequently  utilize\nforeign  keys  to  establish  relationships  between  tables,\nas  illustrated  in \nFig. 2(c).  Tables  in  OLAP  systems  or\ndata  warehouses,  often  comprising  many  columns,  are\ncalled wide tables. Wide tables can minimize the need\nfor  complex  joins.  Database  systems  are  scalable  and\nrobust  and  are  employed  in  enterprises  requiring  data\naccuracy and integrity. Companies typically employ an\nIT  team  proficient  in  programming  to  manage  these\ndatabase systems.\n● Document (DOC) : There is another category of tables\nembedded in various document formats, such as images\n(.png,  .jpg,  etc.),  PDFs  (.pdf),  or  Microsoft  Word  files\n(.docx).  Common  examples  of  documents  with  tables\ninclude  purchase  orders,  financial  reports,  sales\ncontracts,  receipts,  academic  papers  (\nFig. 2(d)),  and\nnumerous other types. Users want to extract tables from\nthese documents, structure them, and convert them into\ntable-native  formats  (spreadsheet  or  HTML).  These\ntables  are  often  surrounded  by  text,  necessitating\nidentifying their location before extracting their content,\nand  the  layout  of  these  tables  may  be  irregular.\nFurthermore,  unlike  ordinary  images  or  plain  text\ndocuments, document-embedded tables rely on an exact\ntwo-dimensional coordinate system. Any misalignment\nin  the  rows  and  columns  can  significantly  impair  the\nunderstanding of the information presented.\nSince these four types of tables are tailored to different user\nscenarios  and  address  diverse  problems,  integrating  artificial\nintelligence (AI) models with these four types of tables varies\naccordingly.  Spreadsheet  systems  aim  to  copilot  users  by\nautomating  various  manipulation  operations.  Web  tables  can\nbe  used  for  table  QA.  Databases  now  widely  incorporate\nNL2SQL  technologies,  aiding  human  engineers  in  data\nengineering  and  analytical  tasks.  Tables  within  documents\nneed  to  be  identified,  structured,  and  transformed  into  table-\nnative formats.\n \n2.2    Differences between table and text\nMany AI methodologies migrate text modeling techniques to\ntables. Therefore, we should consider the differences between\ntables  and  text.  Li  et  al.  [\n12]  outlines  the  main  distinctions\nbetween them. Texts are (1) one-directional; (2) typically read\nfrom left to right; and (3) the swapping of two tokens usually\nalters  the  sentence’s  meaning.  On  the  other  hand,  tables  are\n(1)  two-dimensional,  requiring  both  horizontal  and  vertical\nreading; (2) their understanding heavily relies on schemas or\nheader names; and (3) some of them remain unaffected by row\nand column permutations.\n \n2.3    Table tasks\nTable 1  summarizes  table  tasks  that  can  be  automated  by\nLLMs.  We  also  list  the  description  of  the  table  task,  along\nwith one or two related works, the types of tables addressed,\nand the datasets used.\nTable QA and fact verification are the most traditional table\ntasks, which extract knowledge from tables to answer natural\nlanguage  (NL)  questions.  Table-to-text  produces  an  NL  text\nbased  on  table  data.  Data  cleaning  identifies  and  corrects\nerrors  in  table  data.  Column/Row/Cell  population  generates\npossible  column/row/cell  for  a  table.  Entity  linking\ndisambiguates specific entities mentioned, while column type\nannotation  categorizes  columns  with  types  from  knowledge\nbases. These two tasks often utilize external knowledge bases.\nSpreadsheet systems are originally designed for human users.\nSpreadsheet manipulation is a task that leverages AI to modify\nspreadsheets  automatically,  where  AI  accesses  spreadsheet\nsystems’ APIs or formulas. NL2SQL translates NL questions\ninto  SQL  queries  and  can  improve  the  efficiency  of  data\nanalysts  when  writing  SQL  queries.  This  task  has  been\nextensively studied for years, and LLMs enhance accuracy in\nthis  field.  Data  analysis  consists  of  feature  engineering,\nmachine learning, etc. Table detection identifies tables within\n \n \nFig. 2    Four types of tables: spreadsheet, Web table, database, and document. (a) Spreadsheet; (b) Web table; (c) database; (d) document\nWeizheng LU et al.    Large language model for table processing: a survey 3\ndocuments,  while  table  extraction  converts  them  into  table-\nnative formats such as markdown, HTML, or spreadsheet. The\ntasks mentioned above can broadly be categorized into table-\nrelated,  spreadsheet-related,  database-related,  and  document-\nrelated  tasks.  These  tasks  require  AI  models  to  directly\nunderstand  table  contents,  write  code  to  manipulate\nspreadsheets,  write  SQL  to  access  databases,  or  extract  table\ndata from documents.\n \n2.4    Data lifecycle and end-users’ perspective\nResearchers  often  concentrate  on  designing  new  methods  to\nimprove performance on benchmarks. However, end-users are\nprimarily interested in how table-related AI systems can boost\ntheir productivity rather than benchmark results. To meet end-\nusers’ needs, the industry focuses on developing products and\ntools for them.\n \n2.4.1    Data lifecycle\nEnd-users’  requirements  vary  based  on  their  roles;  common\nusers  typically  need  table  querying  and  manipulation\ncapabilities,  while  data  engineers  need  data  preparation  and\nmodeling  tools.  Different  end  users  are  at  different  stages  of\nthe data lifecycle. We divide the table data lifecycle into the\nfollowing five stages: Data Entry, Data Cleaning, Data CRUD\n(Create, Read, Update, and Delete), Data Analysis, and Data\nVisualization. \nFigure 3  shows  the  five  stages  in  table  data\nprocessing, with corresponding table tasks annotated below.\nData  entry  consists  of  two  parts,  one  is  helping  users  to\ncreate the table structure, and the other is the precise entry of\ndata  by  converting  unstructured  data  formats  into  (semi-)\nstructured  tables.  When  creating  a  table,  LLMs  can  help  list\nthe  possible  column  headers.  For  instance,  Google  Sheets\noffers  a  feature  that  generates  a  new  table  with  suggested\ncolumn  headers  and  example  table  data.  Another  application\nscenario involves converting tables from images or PDFs into\nformats  natively  suited  for  tables,  facilitating  subsequent\nstages  of  table  processing.  This  feature  requires  AI  systems\ncapable  of  multimodal  table  understanding  [\n13,46],  and\nsystems  like  ChatGPT-4o  can  now  convert  table  images  into\nstructured formats.\nData  cleaning  identifies  and  corrects  errors,  inaccuracies,\nmissing values, and duplicates in a table dataset to improve its\nquality  and  reliability  for  further  analysis  [\n50].  This  stage\nneeds to identify erroneous parts and impute errors or missing\nvalues, utilizing techniques like cell population or column type\nannotation.\nData  CRUD  includes  the  following  tasks:  table  QA,  table\nfact  verification,  NL2SQL,  and  spreadsheet  manipulation.\nThis  stage  involves  querying  Web  table  knowledge,\ntransforming upstream database tables into downstream tables\nin  data  warehouses  or  data  lakes,  or  managing  spreadsheet\ntables by calling the system’s APIs or formulas. In this stage,\nAI  systems  usually  enable  individuals  to  process  tables\nthrough NL questions or instructions.\nData  analysis   includes  feature  engineering,  outlier\ndetection,  machine  learning,  visualization,  etc.  Thus,  it\nrequires higher intelligence, as it involves understanding table\ndata, having some domain knowledge, and utilizing tools (e.g.,\nSQL, Python, or VBA) to model tables and give insights.\nData  visualization  is  an  essential  step  to  improve  the\nexpressiveness  of  data.  Different  data  types  paired  with\ndistinctive  chart  types  will  show  completely  different\nexpressiveness.  Users  expect  that  AI  systems  can\nautomatically select the best chart type and graph descriptions.\n \n2.4.2    End-users’ experience\nAs  shown  in \nFig. 4,  from  the  perspective  of  product  design\nand usage, the interaction between end-users and AI systems\ncan  be  categorized  into  three  types:  Beside,  Inside,  and\nOutside.  “Beside”  refers  to  adding  a  copilot  beside  the\napplication.  “Inside”  indicates  that  AI  is  the  core  component\n  \nTable 1    Table tasks, input types, descriptions (related work), and representative datasets. In the Table Type column, the abbreviations are as follows: WT for\nWeb table, SS for spreadsheet, DB for database, and DOC for document\nTask name Table type Description (related work) Example dataset\nTable QA WT Answer a NL question given a table ([\n30,31]) WikiTableQuestion [ 32]\nTable fact verification WT Verifying facts given a table ([ 31,33]) TabFact [ 34]\nTable-to-text WT Produce a NL question given a table ([ 11]) ToTTo [ 35]\nData cleaning WT/SS/DB Correct errors of table data ([ 36,37]) −\nColumn/Row/Cell population WT/SS/DB Populate possible column/row/cell for a table ([ 11,12]) TURL [ 9]\nEntity linking WT Link the selected entity to the knowledge base ([ 11,12]) TURL [ 9]\nColumn type annotation WT Choose types for the column in the table ([ 11,12]) TURL [ 9]\nSpreadsheet manipulation SS Manipulate spreadsheets ([ 16,38]) SpreadsheetBench [ 39]\nNL2SQL DB Translate a NL question to a SQL query ([ 40,41]) Spider [ 42]\nData analysis SS/DB Table data analysis pipeline, consists of feature engineering,\nmachine learning, etc. ([43,44]) DS-1000 [45]\nTable detection DOC Locate tables in documents ([ 46]) TableBank [ 47]\nTable extraction DOC Extract and structuralize tables from documents ([ 46,48]) PubTabNet [ 49]\n \n \nFig. 3    Table processing lifecycle with table tasks annotated\n4 Front. Comput. Sci., 2025, 19(2): 192350\nof  the  product.  “Outside”  implies  that  the  AI  system\norchestrates across different applications and tasks. No matter\nthe  type,  users  tend  to  interact  directly  with  AI  through  NL,\nand the quality of the user experience is strongly linked to the\nunderlying table AI system [\n51]. \n3    Table data representation\nWhen processing tables and table tasks, AI systems must first\nconvert tables into machine-readable representations. Modern\nneural  networks,  which  are  widely  adopted  in  AI  systems,\nneed to encode text or images into numerical representations\nor  embeddings,  and  then  perform  computations  on  these\nembeddings.  Consequently,  we  must  format  table  data\naccordingly  and  feed  it  into  language  or  visual  language\nmodels. This section focuses on the serialization of text tables\nand the processing of document tables.\n \n3.1    Text representation\nLLMs  require  prompts  in  a  linear,  sequential  text  format,\ncontrasting with the two-dimensional structure of tables with a\ndefined  schema.  So,  we  should  maintain  their  semantic\nintegrity  when  converting  tables  into  prompts.  The  prompt\nmay contain the table content and the table schema.\nTable  content  A  straightforward  yet  common  way  is  to\nlinearize tables into markdown format, i.e., separating rows by\nnew lines and inserting column separators (e.g., “|”) between\ncells.  Several  studies  [\n14,52]  evaluate  different  table\nserialization  formats  to  assess  whether  LLMs  accurately\nunderstand structured tables. They compare formats like CSV,\nmarkdown,  JSON,  HTML  and  pandas  DataFrame.  The\nevaluation results suggest that HTML and NL with separators\n(markdown or CSV) are the two most effective options, which\ncan  be  assumed  that  the  training  corpora  include  substantial\ncode and Web tables. Spreadsheets may contain merged cells\nand  hierarchical  columns,  meaning  simple  row-by-row  and\ncolumn-by-column serialization is insufficient. Tian et al. [\n53]\nargue  that  homogeneous  rows  or  columns  in  spreadsheets\ncontribute  little  to  understanding  their  layout  and  structure.\nConsequently,  they  introduce  an  anchor-based  approach  that\npinpoints  heterogeneous  rows  and  columns  as  anchors.\nSubsequently,  they  utilize  an  inverted-index  style  encoding\ntechnique to convert cell locations, values, and schemas into a\nJSON dictionary format.\nCREATE TABLE\nTable  schema  Some  research  [41,54]  explores  schema\nrepresentation methods for NL2SQL tasks. In NL2SQL tasks,\nthe  prompt  should  include  the  NL  question,  table  schemas,\ninstructions,  etc.  Table  schemas  can  be  represented  in  plain\ntext or coded forms using   statements. Foreign\nkeys  can  suggest  the  relationships  among  different  relational\ntables. Special prompt rules like “ with no explanation ” force\nLLMs to provide clear and concise responses that align more\nclosely with the standard answers in the benchmarks. Results\n[\n41,54] show that information about foreign keys and the rules\nlike  “with  no  explanation”  instruction  can  benefit  the\nNL2SQL task.\nText embedding Like other text data, once text-based tables\nare  serialized,  the  serialized  table  data  will  be  embedded  by\nLLMs.\n \n3.2    Visual and layout representation\nWeb tables, spreadsheets, and document-embedded tables may\nhave  visual  cues,  such  as  color  highlighting,  which  could\npotentially  aid  VLMs  in  identifying  accurate  table\ninformation. Deng et al. [\n55] explore the capability of VLMs\nto  comprehend  image  tables  and  assess  the  comparative\nperformance  of  LLMs  with  text  tables  versus  VLMs  with\nimage tables. Their findings indicate that representing tables in\nimage  form  can  facilitate  complex  reasoning  for  VLMs.  To\nprocess  these  tables,  AI  systems  require  tools  that  convert\nvisual cues into visual and layout embeddings.\nPreprocessing  Some  works  like  LayoutLM  [\n56,57]\nleverage  pre-built  optical  character  recognition  (OCR)  tools\nand  PDF  parsers  for  preprocessing  images  and  PDF  files.\nOther  research  in  multimodal  table  understanding  involves\nconverting  tables  into  images.  For  instance,  Table-LLaVA\n[\n13]  converts  HTML  Web  tables  into  images  to  augment\ntraining data, enabling the model to understand tables within\ndocuments  better.  Xia  et  al.  [\n58]  transform  spreadsheets  into\nimages to explore the capabilities of VLMs in comprehending\nspreadsheets.\nVisual embedding Visual embedding is the combination of\nimage,  position,  and  segment  embeddings.  To  handle  table\nimages  in  documents,  AI  systems  must  encode  images  into\nfeatures. For example, LayoutLM and TableVLM [\n46] utilize\nResNet  [59]  to  convert  images  into  visual  embeddings,\nwhereas  Table-LLaVA  [13]  employs  a  Vision  Transformer\n(ViT) [60].\n[0,1000]\nLayout embedding Layout embeddings capture the spatial\nlayout  information  present  in  table  images.  Both  LayoutLM\nand TableVLM normalize and discretize coordinates to integer\nvalues  within  the  range  ,  employing  separate\nembedding  layers  for  the  x  and  y  axes  to  represent  the  two-\ndimensional features distinctly.\n \n4    Table training\nIn  this  section,  we  explore  the  training  techniques  of  large\nmodels  for  table  tasks.  There  are  primarily  two  types  of\nLLMs:  large  language  models  (LLMs)  that  accept  only  text\ninput  and  visual  language  models  (VLMs)  that  can  process\nvisual  inputs.  Given  the  differences  in  inputs,  model\narchitecture, and training techniques between these two types,\nwe  summarize  table  training  techniques  in  Section  4.2.1  and\nwill  discuss  each  category  individually.  We  begin  by\nreviewing  the  literature  on  small  language  models  (SLMs)\nprior  to  the  LLM  era,  where  these  studies  train  models  with\nfewer  than  a  billion  parameters.  We  will  then  delve  into  the\ndetails of LLMs and VLMs for tables.\n \n \n \nFig. 4    Three  types  of  human  interaction  with  table  AI  system.  (a)  Beside;\n(b) inside; (c) outside\nWeizheng LU et al.    Large language model for table processing: a survey 5\n4.1    Pre-LLM era\nBefore  the  era  of  LLMs,  many  researchers  were  already\nemploying  language  models  to  address  table  tasks.  Their\nworks  primarily  focus  on  modifying  model  structures,\ndevising encoding methods, and designing training objectives\nto  tailor  the  models  for  table  tasks.  For  example,  TaPas  [\n8]\nextends  BERT’s  [61]  model  architecture  and  mask  language\nmodeling objective to pre-train and fine-tune with tables and\nrelated text segments. TaBERT [\n7] encodes a subset of table\ncontent  most  relevant  to  the  input  utterance  and  employs  a\nvertical attention mechanism. TURL [\n9] encodes information\nof  table  components  (e.g.,  caption,  headers,  and  cells)  into\nseparate input embeddings and fuses them together. TABBIE\n[\n62] modifies the training objective to detect corrupted cells.\nTaPEx  [10]  learns  a  synthetic  corpus,  which  is  obtained  by\nautomatically  synthesizing  executable  SQL  queries  and\nexecuting  these  SQLs.  RESDSQL  [\n63]  injects  the  most\nrelevant  schema  items  into  the  model  during  training  and\nranks schema items during inference to find the optimal one.\nTo  process  image  tables  and  tables  within  documents,\nresearchers  often  adopt  the  encoder-decoder  architecture.  In\nthis architecture, the encoder encodes visual information while\nthe decoder generates textual output. For instance, LayoutLM\n[\n56,57] integrates textual, visual, and layout embeddings into\nits BERT backbone.\nHowever,  the  foundation  models  of  these  methods  are\nrelatively  small.  Some  cannot  adapt  to  various  downstream\ntasks, and some require annotated data during fine-tuning.\n \n4.2    Table LLM training \n4.2.1    What’s new in table LLM training\nMethods such as instruction tuning and continued pre-training\nare  widely  utilized  in  the  era  of  LLMs.  Although  these\napproaches have already been employed in the pre-LLM era,\ntheir training techniques differ from those used previously.\nInstruction  tuning  involves  directing  the  model  with\nspecific instructions or guidance within the prompt. It enables\nthe  language  model  to  follow  the  instructions  given  in  the\nprompt.  As  shown  in \nFig. 5,  for  table processing  tasks,  there\nare three types of instruction tuning: table tuning, code tuning,\nand a hybrid of both table and code.\n● Table tuning focuses on LLMs’ understanding of tables\nso  that  LLMs  can  handle  various  table  tasks  such  as\ntable QA, table-to-text, entity linking, etc. This type of\nresearch  utilizes  general-purpose  foundation  LLMs\n(e.g.,  Llama)  and  a  substantial  volume  of  table-related\ndata  for  instruction  tuning.  Table  tuning  examples\ninclude TableLlama [\n11] and Table-GPT [12].\n●  Code  tuning  addresses  table  processing  tasks  from  a\ncode  generation  perspective  by  generating  code  (e.g.,\nSQL or Python) to manipulate table data. Code tuning\nexamples  include  Magicoder  [\n64],  Lemur  [44],  and\nDAAgent [17].\n●  Hybrid  of  table  and  code  research  reveals  that  table\ninstruction  tuning  based  on  code  LLMs  is  more\neffective,  i.e.,  code  LLMs  are  tuned  using  table\ninstruction  datasets.  For  example,  TableLLM  [\n67]  and\nStructLM [68] tune code LLMs on table datasets.\nBuilding  Instruction  Datasets  is  of  great  importance  for\nhigh-quality  instruction  tuning.  Manually  crafting  these\ndatasets  is  highly  time-consuming  and  labor-intensive,\npresenting  a  major  challenge  in  constructing  high-quality\ndatasets.  So  research  focuses  on  constructing  datasets  via\nautomatic  approaches  like  using  templates  to  transform\nexisting  annotated  datasets  for  instruction  tuning  or  distilling\ndata  from  more  powerful  LLMs.  Therefore,  researchers\nconcentrate on constructing instruction datasets automatically,\nemploying methods such as template-based transformation of\nexisting  annotated  datasets,  or  data  distillation  from  more\npowerful LLMs.\nContinue  pre-training  takes  an  existing  trained  model,\nfeeds new data, and adapts it for a specific task [\n71]. SLMs +\nLLMs is a paradigm to fine-tune an SLM to guide the LLM\ntowards  desired  outputs  where  two  types  of  models\ncomplement each other [\n72]. \n4.2.2    Table tuning\nTable  tuning,  short  for  table  instruction  tuning,  constructs\ninstruction  tuning  datasets  by  leveraging  multiple  existing\ntable-related datasets. The instruction tuning dataset can be in\nthe  form  of  (Instruction, Table , Output ). \nFigure 6  is  an\nexample  entry  from  TableInstruct,  the  instruction  tuning\ndataset  for  TableLlama  [\n11].  In  this  example,  the Instruction\nspecifies  the  task;  the Table  describes  table  content,  table\nmetadata, or task-specific content. The  Output features natural\nlanguage  outputs  like  table  QA  answers,  text  from  table-to-\ntext transformations, result tables after manipulation, or a mix\nof text and tables. This example is about fact verification and\n \n \nFig. 5    Summary of Table LLM training techniques\n6 Front. Comput. Sci., 2025, 19(2): 192350\nhas a NL question. For other table tasks, the  Question element\nmay be optional. When using Web tables, titles or captions are\nincluded in the table content to provide context information to\nLLMs.\nTableLlama  emphasizes  using  more  realistic  data  and  uses\nthe  template  approach  to  collect  14  existing  datasets  (e.g.,\nWikiTableQuestions  [\n32]  or  Spider  [42])  of  11  table  tasks.\nTable-GPT  employs  the  synthesis-then-augment  approach.\nThe  synthesis-then-augment  approach  resembles  the  method\nused in computer vision, where images are randomly cropped\nor  flipped  to  create  variations.  Table-GPT  designs  18\nsynthesis  processes,  ranging  from  table  QA  to  row/column\nswapping.  For  example,  the  row/column  swapping  synthesis\nprocess  swaps  rows  or  columns,  and  the Output  is  the\nswapped  table.  In  this  way,  the  model  can  understand  the\norder  of  rows/columns.  Additionally,  Table-GPT  implements\naugmentation  strategies  at  the  instruction  level,  table  level,\nand  output  level  to  increase  task  and  data  diversity.  For\nexample,  the  instruction  level  augmentation  uses  powerful\nLLM  to  paraphrase  the  canonical  human-written  instruction\ninto  many  different  variants.  These  augmentation  approaches\ncan  prevent  the  model  from  overfitting.  TableLlama  and\nTable-GPT  demonstrate  that  after  table  tuning  on  seen  table\ntasks,  LLMs  could  exhibit  robust  generalization  capabilities\nand tackle unseen table tasks.\n \n4.2.3    Code tuing\nTables  can  be  manipulated  through  programming  languages\nsuch  as  Python  or  SQL.  Consequently,  the  code  instruction\ntuning  approach  develops  LLMs  specializing  in  code\ngeneration.  Models  tuned  with  table  instructions  typically\ngenerate  table-related  content  directly.  Code  LLMs  first\ngenerate  code  that  is  subsequently  executed  in  environments\nlike Python interpreters or database engines. Code LLMs are\nparticularly  adept  at  tasks  like  NL2SQL  and  data  analysis.\nSeveral code LLMs achieve top rankings on the DS-1000 [\n45]\nand  InfiAgent-DABench  [ 17]  leaderboards,  which  are\nbenchmarks for data analysis code generation. Here, we list a\nfew examples and discuss how they build instruction datasets.\nProblem\nMethod\nWizardCoder  [73]  employs  the  “Evol-Instruct”  method,\nwhere “Evol” denotes evolution, indicating the use of existing\ninstruction data as a seed to prompt a more powerful LLM to\ngenerate  new  instructions. \nFigure 7  shows  a  sample  prompt\nfrom  Evol-Instruct  for  code.    refers  to  the  current\ncode  instruction  awaiting  evolution  and    is  evolution\ntype. WizardCoder uses five heuristic evolution methods, and\nhere in \nFig. 7, we provide one example heuristic method.\nMagicoder  [64]  propose  a  process  called  “OSS-Instruct”.\nOSS-Instruct first collects open-sourced code snippets and lets\na  powerful  LLM  draw  inspiration  from  the  code  snippets  to\nproduce realistic code instructions. OSS-Instruct is orthogonal\nto  existing  data  generation  methods  like  Evol-Instruct.  Thus,\nthese  two  methods  can  be  combined  together.  Both\nWizardCoder and Magicoder utilize powerful LLMs to distill\nand  generate  additional  data.  Lemur  [\n44]  argues  that  code\nLLM should balance general-purpose ability with code ability.\nThe general-purpose ability is for reasoning and planning and\ncan  be  learned  by  NL  text.  The  code  ability,  which  can  be\nlearned  from  code,  ensures  grounding  in  programming\nenvironments. So, the authors build a corpus with a 10:1 code-\nto-text ratio to ensure that the trained code LLMs have coding\nability  while  maintaining  performance  in  NL  ability.  Lemur\nalso  evaluates  whether  the  model  performs  effectively  with\nagents  that  heavily  depend  on  tool  usage  and  environment\nfeedback.  DAAgent  [\n17]  is  a  series  of  specialized  agent\nmodels  focused  on  data  analysis.  Their  instruction  tuning\ndataset  is  crafted  by  crawling  CSVs  from  GitHub  and\ngenerating data analysis keywords and questions by iteratively\nprompting  GPT-4  given  a  specific  crawled  CSV  file.  These\nstudies show that enhancing models with code instructions can\nboost performance on table tasks, especially on data analysis\ntasks.  When  constructing  code  instruction  datasets,  these\nmethods  more  or  less  distill  data  from  powerful  LLMs  (e.g.,\nGPT).\nSENSE [\n65] is an NL2SQL model that utilizes two types of\nsynthetic training data: “strong data” and “weak data.” “Strong\ndata”  is  distilled  from  powerful  LLMs  that  provide  more\nreliable responses, while “weak data” is produced by inferior\n \n \nTableInstructFig. 6    An  example  entry  from  ,  a  table  instruction  tuning\ndataset\n \n \nEvol-InstructFig. 7    A sample prompt from   for code\nWeizheng LU et al.    Large language model for table processing: a survey 7\nmodels  that  may  result  in  errors  during  SQL  execution.  The\nstrong and weak data are then trained with Direct Preference\nOptimization (DPO) [\n74], enabling the SENSE model to learn\nfrom both correct and incorrect samples. FinSQL [ 66] is also\nan  NL2SQL  solution.  It  creates  a  dataset  specialized  for  the\nfinancial sector and employs a powerful LLM to augment the\ndata,  followed  by  parameter-efficient  fine-tuning  (PEFT)\n[\n75,76] of a LLM. \n4.2.4    Hybrid of table and code\nCurrently,  there  are  many  open-source  general-purpose\nmodels and code LLMs available. One question that arises is\nwhich foundation model should be selected for further training\nspecifically  for  table  tasks.  StructLM  [\n68]  performs  an\nablation study using code LLM, general LLM, and math LLM\nas foundational models, fine-tuning them on tabular datasets.\nStudies [\n67,77], including StructLM reveal that using the code\nLLM as the foundation model achieves superior performance\non  table  tasks.  Like  table  instruction  tuning,  studies  of  the\nhybrid  type  focus  on  how  to  construct  table  instruction\ndatasets.\n \n4.2.5    Continue pre-training\nSmall-sized  LLMs  have  lower  deployment  costs,  but  their\ncode generation or reasoning abilities are inferior to those of\nlarge-sized  LLMs.  CodeS  [\n40]  proposes  continued  pre-\ntraining  [ 71]  on  small-sized  LLMs  to  enhance  their\nperformance  in  NL2SQL  tasks.  Specifically,  CodeS  feeds\nSQL-related,  NL  text,  and  NL-to-code  data  into  the  pre-\ntrained StarCoder models [\n78], thereby enhancing the models’\ncapabilities  in  natural  language  processing,  reasoning,  and\ncoding.\n \n4.2.6    SLMs + LLMs\nSLMs  are  easier  to  fine-tune  to  understand  table  schemas,\nwhereas  LLMs  exhibit  strong  reasoning  capabilities  but  may\nencounter “hallucination” [\n79] problems owing to the lack of\ndomain knowledge. ZeroNL2SQL [69] combines the strengths\nof  both  SLMs  and  LLMs  to  mitigate  their  respective\nweaknesses.  It  fine-tunes  an  Encoder-Decoder  SLM\nresponsible  for  generating  SQL  sketch  candidates,  and  it\nemploys  an  LLM  to  fill  in  missing  parts  in  the  SQL  sketch,\ncorrect errors in the SQL query, and generate the final query.\n \n4.3    Table VLM training\nResearch in utilizing VLMs for table tasks can be divided into\nthree types. The first type adheres to the conventional pattern\nrecognition  method  [\n56,80],  which  involves  table  detection\nand  extraction  tasks.  A  notable  example  is  the  TableVLM\n[\n46].  The  second  approach  tackles  various  table  tasks  in  an\nend-to-end  manner.  The  example  is  Table-LLaVA  [13].  The\nthird  type  integrates  features  of  the  first  two,  enabling  the\ndetection,  extraction,  and  end-to-end  tasks  such  as  question\nanswering on image tables, with TabPedia [\n48] serving as an\nexample. The third type is a hybrid of the first two, which can\ndetect and extract tables and answer questions based on table\nimages, exemplified by TabPedia [\n48].\nPre-training + fine-tuning Most LLMs are designed with a\ndecoder-only architecture. Similar to the network architecture\nof  visual  language  models  from  the  pre-LLM  era,  VLMs\ntypically  employ  an  encoder-decoder  architecture,  where  a\nvisual encoder converts visual data into embeddings while the\ndecoder generates texts. The encoder may utilize architectures\nsuch as ResNet or ViT, while the decoder typically consists of\na pre-trained LLM. Compared to the decoder, which has been\nthoroughly  trained,  the  encoder  has  not  yet  mastered  much\nvisual  information  about  tables.  It  is  necessary  to  align  the\nvisual  cues  with  the  textual  information.  Therefore,  the\ntraining  process  for  a table VLM  is  usually  divided  into  two\nphases:  1)  Pre-training  the  visual  encoder  while  freezing  the\nparameters  of  the  LLM  decoder,  and  2)  fine-tuning  or\ninstruction  tuning  the  entire  model.  Both  phases  require  a\nsubstantial amount of high-quality training data. PixT3 [\n70] is\na multimodal table-to-text model that takes table-to-text tasks\nas table visual recognition tasks and generates texts, removing\nthe need to process tables in text formats.\n \n5    Table prompting\nIn  this  section,  we  explore  the  strategies  for  prompting  large\nmodels  to  handle  table  tasks.  LLMs  struggle  with\nfunctionalities,  such  as  complex  reasoning,  arithmetic\ncalculation,  factual  lookups,  and  correcting  erroneous\ndecisions, all essential for table tasks. Thus, the key challenges\ninclude  guiding  the  model  towards  complex  reasoning,\nenabling it to reflect and revise rather than fast thinking, and\nutilizing external tools for executing Python or SQL code. To\naddress  the  issues  above,  researchers  have  been  dedicated  to\ndeveloping LLM-powered agents.\n \n5.1    Common workflow of LLM-powered agents\nE5\nResearch  such  as  Chain-of-Thought  (CoT)  [20]  and  ReAct\n[81]  prompt  LLMs  iteratively,  organizing  the  reasoning\nprocess into multiple intermediate steps. Thus, LLMs address\nsimpler  subproblems  step  by  step  and  progressively  build  a\ncoherent response. Agent systems are designed to follow this\nparadigm  and  typically  include  modules  such  as memory,\nplanning, and action [\n21]. The memory module stores state or\nobservations  from  the  environment  or  records  past  actions.\nThis  information  can  be  utilized  for  future  planning.  The\nplanning module chooses which action the agent needs to do\nin the current step, while the action module interacts with the\nenvironment and executes the action to get the outcomes. As\nshown  in \nFig. 8,  for  table  tasks,  the  agent  first  observes  the\ntable  data  and  user  intent.  It  generates  prompts,  decomposes\ncomplex  tasks,  plans  actions,  executes  them  on  the  table\nenvironment,  and  then  updates  the  state  or  observation.  This\niterative process is repeated until expectations are met. Studies\nlike  SheetAgent  [\n38],  Chain-of-Table  [31],  ReAcTable  [15],\nTAPERA [82], and   [ 83] follow this workflow.\nThe  memory  module  enables  the  agent  to  accumulate\nexperiences,  self-evolve,  and  act  with  greater  consistency,\nrationality, and effectiveness [\n21]. The memory of table agents\nstores  planning  history,  that  is,  the  outcomes  of  specific\nactions and table states, typically in a structured table format.\nIn  other  aspects,  the  memory  module  of  table  agents  is  not\nsignificantly  different  from  that  of  other  LLM-based  agents.\nThis  paper  focuses  on  the  planning  and  action  modules  of\ntable agents.\n \n8 Front. Comput. Sci., 2025, 19(2): 192350\n5.2    Planning\nThe  planning  module  plans  actions  by  prompting  LLMs.  It\nmust  carefully  handle  two  key  aspects:  1)  breaking  down\ncomplex  problems  into  smaller  sub-problems,  and\n2) reflecting on and revising previous decisions.\n \n5.2.1    Formalizing planning\nI S t t\nQ Ht−1\nAt t\nIn table agents, the planning module interacts with the target\ntables using a ReAct approach, with feedback and reflection.\nSheetAgent [\n38] provides a formal definition of planning for\nspreadsheet  manipulation,  and  here,  we  further  extend  it  to\nencompass  more  table  tasks  like  table  QA  and  NL2SQL.\nTypically, the input of the planning module usually consists of\nthe  task  instruction  ,  table  state    at  step  ,  user  query  or\ncurrent (sub-)problem  , planning history  . Utilizing this\ninformation,  the  planning  module  prompts  LLMs  and\nformulates an action   for step   by:\n \nAt =P(At |I,S t ,Q,Ht−1 ).\nOt\nHt =(Ht−1 ,Ot ,At )\nThe action, which we will discuss in Section 5.3, is executed\non the target table, yielding a new observation and the output\ndenoted  as  .  The  table  state  and  the  planning  history  are\nthen updated by  .\n \n5.2.2    Complex task decomposition\nInspired  by  the  CoT  and  least-to-most  [\n84]  prompting\nmethods,  researchers  instruct  LLMs  to  decompose  complex\ntable  tasks  into  simpler  sub-tasks.  DIN-SQL  [\n85]  proposes\nbreaking  down  the  NL2SQL  task  into  subtasks.  The  process\ninvolves  identifying  the  relevant  tables  and  columns\nassociated  with  the  query  and  producing  intermediate  sub-\nqueries.  DEA-SQL  [\n86]  and  TabSQLify  [87]  also  follow  the\nprinciple  of  decomposing  tables  into  smaller,  simplifying\nsubtasks and reducing irrelevant information. For Web tables,\nDater [\n33] exploits LLMs as decomposers by breaking down\nhuge evidence (a huge table) into sub-evidence (a small table)\nand  decomposing  a  complex  question  into  simpler  sub-\nquestions and getting SQLs.\n \n5.2.3    Reflection and revision\nLLMs  often  generate  answers  “without  thinking”,  while\nagents can try different approaches, vote to select the best one,\nand  even  reflect  on  past  actions,  learn  from  mistakes,  and\nrefine them for future steps.\nSelf-consistency  and  voting  The  self-consistency  [\n88]\ndecoding strategy plans multiple reasoning paths and chooses\nthe most consistent answer. It can significantly enhance LLM’s\naccuracy on complex tasks. This has also been demonstrated\nin  table  tasks,  which  are  reported  in  papers  of  Dater  [\n33],\nDAIL-SQL  [41],  MCS-SQL  [89],  and  ReAcTable  [15]  on\ntable  QA  and  NL2SQL  tasks.  However,  several  studies\n[\n15,41]  argue  that  while  self-consistency  and  voting  enhance\naccuracy,  these  methods  are  time-consuming,  and  the  cost  is\nhigher, considering that prompting LLMs is not cheap.\nRevising In many table tasks, a mistake in one step’s action\ncan significantly impact the following analysis and processing.\nTo  handle  this  challenge,  SheetCopilot  [\n16]  and  SheetAgent\n[38] adopt a mechanism that reflects and refines past actions.\nSheetAgent  features  a  Retriever  component  that  retrieves\nhigh-quality  code  examples  from  their  curated  repository,\nwhich  helps  to  prevent  the  generation  of  erroneous  code.\nSelfEvolve  [\n90]  achieves  decent  results  on  the  Python  data\nanalysis task by asking the LLM to perform debugging if the\ngenerated code cannot execute in the Python interpreter. These\ntechniques prevent error actions like incorrect APIs or wrong\narguments.\n \n5.3    Action\nIn  table  tasks,  actions  are  intermediaries  between  LLMs  and\nsoftware  tools  like  database  engines,  spreadsheet  systems,  or\nPython  interpreters.  Utilizing  these  external  tools  involves\nmore  than  just  converting  NL  text  into  software  APIs.  The\nagent system must ensure that the API calls are error-free and\ncan  address  complex  tasks.  These  external  tools  extract  data\nfrom  tables  and  are  similar  to  the  Retrieval-Augmented\nGeneration  (RAG)  [\n91]  concept.  Given  the  inherent\ncharacteristics of different table tasks, we will discuss how to\ndefine actions based on the specific tasks.\n \n \n \nFig. 8    The iterative process of an LLM-powered agent system for table tasks\nWeizheng LU et al.    Large language model for table processing: a survey 9\n5.3.1    Table QA and NL2SQL\nFor tasks like table QA and NL2SQL, agent systems usually\nutilize Python or SQL to interact with tables. In Binder [\n30],\nactions  are  categorized  into  two  types:  extended  Python  and\nSQL,  which  allow  the  injection  of  LLM  as  operators  within\nstandard Python and SQL. ReAcTable [\n15] incorporates three\nkinds  of  actions:  (1)  generating  a  SQL  query,  (2)  generating\nPython  code,  and  (3)  directly  answering  the  question.  It\nextends  the  ReAct  framework  with  an observation-action-\nreflection loop specifically for table tasks. Within this iterative\nloop, it progressively refines the table data by generating and\nexecuting  an  SQL  query  to  retrieve  table  knowledge.  If  the\nexisting table lacks the required information or if the answer\ncannot be directly queried via an SQL query, it generates and\nexecutes  Python  code  to  produce  an  intermediate  table,\nthereby filling missing information into an intermediate table.\n \n5.3.2    Spreadsheet manipulation and data analysis\nSpreadsheet  manipulation  and  data  analysis  are  highly\nflexible.  First,  users’  expectations  and  requirements  vary.\nSecond, there is a wide range of operations and software APIs.\nThird,  a  complex  task  may  contain  multiple  operations,\nleading to dynamic alterations in the table content.\nSheetCopilot [\n16], an agent system for spreadsheets, models\nexisting  spreadsheet  VBA  APIs  into  atomic  actions.  An\natomic  action  comprises  the  API  name,  argument  lists,\ndocument  string,  and  several  usage  examples.  These  atomic\nactions are not specific to one system and can be implemented\non  different  spreadsheet  systems.  The  authors  design  these\natomic  actions  by  crawling  spreadsheet-related  questions\nonline,  embedding  and  clustering  them  into  categories,  and\nabstracting  them  to  VBA  APIs  by  choosing  the  most\nrepresentative  ones.  SheetAgent  [\n38]  finds  that  Python  and\nSQL  are  more  appropriate  for  spreadsheet  manipulation  than\nVBA,  owing  to  the  two  programming  languages  being  more\naligned  with  the  training  data  of  existing  LLMs.  SheetAgent\ncomprises  two  essential  components  for  agent  actions:  a\nPlanner and an Informer. The Planner generates Python code\nand employs a ReAct-style reasoning approach to manipulate\nthe  spreadsheet  table.  The  Informer  serves  as  an  evidence\nprovider, supplying subtask-specific SQL queries to assist the\nPlanner  in  tackling  complex  tasks.  Data-Copilot  [\n43]  is  for\ndata  science  and  visualization.  Its  actions  are  two-level:  the\ninterface is high-level pseudo-code descriptions, and the code\nis  low-level  executable.  When  designing  interfaces,  Data-\nCopilot generates code for each user request. It then assesses\nwhether there are similarities among these requests that can be\nmerged or abstracted.\n \n5.3.3    Multiple table tasks\nMost prompting methods are for a single table task; here, we\nlist  several  studies  that  address  multiple  tasks.  These\nframeworks  share  common  characteristics:  they  first  explore\nthe data, including the data itself and its schema, and then plan\nand optimize actions.\nStructGPT  [\n92]  is  capable  of  solving  table  QA,  NL2SQL,\nand knowledge graph QA by developing three types of actions\nthat  target  Web  tables,  databases,  and  knowledge  graphs.\nThese actions function as a reader, extracting knowledge from\nthe  data,  which  is  then  used  to  assist  the  LLM  in  its  future\nplanning.  The  actions  for  Web  tables  include  extracting  data\nor  column  names,  whereas  for  databases,  the  actions  involve\nextracting  table  data  or  metadata.  UniDM  [\n36]  manages  a\nvariety  of  data  manipulation  tasks,  such  as  data  cleaning,  on\ndata  lakes  through  a  three-action  process.  The  first  action  is\nextracting  context  information,  including  table  metadata  and\nrelated  records  associated  with  the  task,  utilizing  this\ninformation as demonstrations or background knowledge. The\nsecond  action  involves  converting  this  context  information\ninto  NL  texts  suitable  for  LLMs  to  reason.  The  final  action\nemploys prompt engineering to formulate the desired prompt.\nTAP4LLM [\n93] is a pre-processing toolbox to generate table\nprompts. Its action includes: 1) selecting appropriate rows and\ncolumns from the table (called Table Sampling), 2) integrating\nthem  with  other  table  data  (relevant  external  knowledge,\nmetadata) (called Table Augmentation), and 3) serializing this\ninformation  to  fit  the  context  length  of  the  LLM.  ChatPipe\n[\n94]  is  a  system  designed  to  facilitate  effortless  interaction\nbetween  users  and  ChatGPT  for  table  data  analysis  tasks.\nInitially,  users  upload  their  dataset  along  with  their  query.\nChatPipe assists in analyzing the dataset and suggests various\ndata  analysis  and  feature  engineering  operations.  Given  the\nnumerous  potential  data  operations,  ChatPipe  employs  a\nreinforcement  learning-based  method,  utilizing  a  Deep  Q-\nNetwork model [\n95] to learn and recommend the most suitable\noperations. \n5.4    Other techniques\nThis  subsection  discusses  other  prompting  techniques  like\nfew-shot examples of in-context learning [\n19] and role-play. \n5.4.1    Few-shot examples selection\nIn-context learning refers to the ability of models to learn from\nexamples  within  the  context  of  prompts,  an  ability  that\nemerges  in  LLMs.  The  key  to  in-context  learning  is  how  to\nselect and  organize the most helpful demonstrative examples\ninto  the  prompt.  For  the  NL2SQL  task,  selecting  the  most\nrelevant  example  can  be  achieved  by  choosing  example\nqueries  more  related  to  the  target  NL  question  or  example\nSQL more similar to the potential SQL. Nan et al. [\n54] argue\nthat  diversity  should  be  considered  in  addition  to  similarity.\nAs  the  context  length  is  limited,  organizing  all  the  example\ninformation (i.e., NL question, schema, SQL) into the prompt\nensures the quality of examples but sometimes may exceed the\ncontext  length.  DAIL-SQL  [\n41]  proposes  a  method  that\nbalances  the  quality  and  token  quantity  by  removing\nexamples’ schemas, which are token-cost.\n \n5.4.2    Role-play\nLLMs  can  be  assigned  roles  when  prompting  them.  For\ninstance, Zhao et al. [\n96] test a prompt like “Suppose you are\nan expert in statistical analysis”. Tapilot-Crossing [ 97] utilizes\na  multi-agent  environment  to  generate  user  intents  and\nsimulate  use  cases  from  real-world  scenarios.  Within  this\nenvironment,  each  agent  takes  on  a  unique  role,  such  as\nAdministrator,  Client,  Data  Scientist,  or  AI  Chatbot,\ninteracting  with  each  other  to  mimic  a  realistic  data  analysis\ncontext.\n \n10 Front. Comput. Sci., 2025, 19(2): 192350\n6    Resources\nIn  this  section,  we  summarize  open-source  datasets,\nbenchmarks, and software, as these artifacts can facilitate the\ncommunity’s progress.\n \n6.1    Datasets and benchmarks\nTraditional benchmarks, such as WikiTQ [\n32], WikiSQL [98],\nand Spider [42], are already widely used and studied. We will\nnot  elaborate  on  those  here  but  rather  focus  on  new\nbenchmarks. \nTable 2  presents  recently  proposed  datasets  and\nbenchmarks,  along  with  their  data  sources,  sizes.  We\nsummarize the features of these new benchmarks as follows:\n●  Robustness  For  most  table  tasks  (such  as  Table  QA),\nswapping  rows  and  columns,  or  replacing  column\nnames  with  synonyms  or  abbreviations,  should  not\naffect the final results. For a large table (that cannot fit\ninto  the  LLM’s  context),  the  placement  of  the  wanted\ncell,  whether  at  the  beginning,  end,  or  middle  of  the\ntable, should not affect the query result. To evaluate the\nrobustness,  Dr.  Spider  [\n103]  and  RobuT  [99]  are\nproposed.  RobuT  reveals  that  the  performance  of  all\ntable  methods  degrades  when  perturbations  are\nintroduced,  yet  close-source  LLMs  (e.g.,  GPT)  exhibit\ngreater robustness.\n●  Human  involved  labeling  Some  new  datasets,  which\ntarget  data  analysis,  require  extensive  manual\nannotation.  DS-1000  [\n45],  InfiAgent-DABench  [17],\nTapilot-Crossing  [97]  are  designed  to  assess  data\nanalysis  tasks.  They  gather  data  from  the  internet  and\nannotate  it  either  automatically  or  semi-automatically.\nDS-1000  collects  questions  from  StackOverflow,\nassesses their usefulness manually, and curates to form\nthe benchmark. The authors manually adapt the original\nquestions  by  providing  input  and  output  context  into\ntest cases and rewriting problems to prevent LLMs from\nlearning and memorizing the data. InfiAgent-DABench\ninvites  human  experts  to  evaluate  the  dataset  quality\nand  compare  human-made  and  GPT-4  generated  data\nanalysis  questions  via  multiple  metrics.  Tapilot-\nCrossing aims to build a benchmark for real-world data\nanalysis.  An  issue  that  cannot  be  overlooked  is  the\nquality  of  these  datasets.  Wretblad  et  al.  conduct  a\nthorough analysis of the NL2SQL dataset BIRD [\n101],\ndiscovering inaccuracies in some of the gold SQLs and\nnoise  within  certain  NL  queries  [\n109].  The  creators  of\nthe BIRD dataset introduce this noise during the dataset\ncreation process. After correcting these errors, Wretblad\net al. [109] find that complex prompting methods (e.g.,\nDIN-SQL) might be less effective than simple zero-shot\nprompting.  This  research  reveals  the  potential\nunreliability of table datasets such as BIRD, considering\nthey  are  generated  through  human  labeling,  a  process\nprone to introducing noise and errors.\n●  Real-world  workload  Most  datasets  are  derived  from\nthe Web or synthesized using templates. These publicly\navailable  online  data  are  usually  simple,  as  some  of\nthem are just tutorials for beginners. In contrast, tables\nin real-world scenarios are far more complex than these.\nScienceBenchmark  [\n104]  introduces  a  real-world\nbenchmark  developed  in  collaboration  with  SQL\nexperts  and  researchers  specializing  in  policy-making,\nastrophysics, and cancer research. Given the scarcity of\nreal-world  data  in  these  domain-specific  databases,\nScienceBenchmark  employs  a  data  augmentation\nstrategy  that  starts  with  hundreds  of  human-labeled\nNL2SQL  pairs  to  create  thousands  more  data  points.\nSpreadsheetBench [\n39] proposes a benchmark aimed at\nreal-world  scenarios,  where  the  authors  meticulously\nanalyzed user questions from four Excel forums. They\nargue  that  their  benchmark  could  assess  the\nperformance of handling complex user instructions.\n●  Larger  scale  AnaMeta  [\n105]  is  a  large-scale  table\nmetadata  dataset.  GitTables  [107]  downloads  millions\nof  CSV  tables  from  GitHub  and  aligns  them  with\nknowledge  bases.  SchemaPile  [\n108]  is  a  corpus  with\n221k  database  schemas  and  1.7  million  table\ndefinitions.  These  datasets  can  help  evaluate  whether\nthe  AI  system  could  understand  the  table  schema  and\nbenchmark tasks like column type annotation. They can\nalso  be  fed  into  neural  models  as  training  data  for\nsolving  various  downstream  tasks.  ComplexTable  [\n46]\ncontains  more  than  1  million  image  tables  featuring\ncomplex structures and can be utilized for visual tasks\nrelated to tables.\n \n  \nTable 2    Datasets and benchmarks for table processing tasks\nDataset Table task Data sources Size\nRobuT [\n99] Table QA WikiTQ [ 32], WikiSQL [98], SQA [100] 138,149 perturbed examples\nBIRD [101] NL2SQL kaggle.com, CTU Prague [ 102] Open tables, 81 DBs, 12,751 NL2SQL pairs\nDr.Spider [103] NL2SQL Spider [ 42] 200 DBs, 15k perturbed examples\nScienceBenchmark [104] NL2SQL Human + AI Augmented 3 DBs, 6k NL2SQL pairs\nSheetCopilot [16] Spreadsheet manipulation superuser.com 28 SSs, 13k QA pairs\nSpreadsheetBench [39] Spreadsheet manipulation 4 Excel online forums, e.g., excelfourm.com 912 instructions, 2,729 test cases\nDS-1000 [45] Data analysis stackoverflow.com 451 problems\nInfiAgent-DABench [17] Data analysis github.com 631 CSVs, 5131 samples\nTapilot-Crossing [97] Data analysis kaggle.com 1176 user intents\nAnaMeta [105] Entity linking, Column type annotation public Web, TURL [ 9], SemTab [106] 467k WT/SSs\nGitTables [107] Column type annotation, Column population Github.com 1M CSVs\nSchemaPile [108] Column type annotation,\nColumn population NL2SQL Data analysis Github.com 221k DB schemas 1.7M table definitions\nComplexTable [46] Table detection, Table extraction Synthetically generated 1M tables (png, HTML)\nWeizheng LU et al.    Large language model for table processing: a survey 11\n6.2    Open-source software\nThe  academia  and  industry  have  developed  numerous  open-\nsource software; here, we select a few to discuss. LlamaIndex\n[\n110]  is  a  popular  and  versatile  RAG  framework  that,  while\nsupporting  some  table  tasks,  is  not  specialized  in  this  area.\nBoth  DB-GPT  [\n111]  and  Vanna  [112]  are  AI  tools  designed\nfor database interactions. They allow users to train models or\nutilize  their  built-in  prompting  features.  PandasAI  [\n113]\nenables  users  to  clean,  query,  and  visualize  pandas\nDataFrames  using  NL  questions.  Most  of  these  tools  are\ndesigned for NL2SQL and table QA tasks, which enable users\nto  query  tables  with  NL  texts.  This  trend  underscores  the\ndemand  among  general  users  to  utilize  NL  to  enhance  the\nefficiency  of  table  processing.  RetClean  [\n37]  is  a  tool  that\nleverages LLMs for data cleaning operations, such as missing\nvalue imputation.\n \n7    Analysis and discussion\nIn  order  to  demonstrate  the  accuracy  and  costs  of  various\nmethods  to  the  readers,  this  section  presents  a  comparative\nanalysis  and  discusses  the  advantages  and  disadvantages  of\ndifferent approaches. Specifically, utilizing data from several\npapers [\n17,39,44,67,77,114], we summarize both accuracy and\ncost  for  four  table  tasks:  table  QA,  NL2SQL,  spreadsheet\nmanipulation, and data analysis.\n \n7.1    Discussion on LLM training\nThe advantages of training-based methods are that they offer\ngreat control, allowing enterprises to conduct private training\nand  deployment  without  data  leakage  to  third-party  model\nservice  providers.  However,  two  issues  can  not  be  ignored:\ncost and accuracy.\nCost  The  expense  of  pre-training  or  fine-tuning  a  large\nmodel  is  substantial.  Fine-tuning  a  7B  model  typically\nrequires eight 80 GB GPUs, and the number of training tokens\ndictates the total time needed for training. The training speed,\nmeasured  in  tokens  per  second,  is  affected  by  both  the\nhardware and the software and is reported in several technical\nreports,  such  as  Llama  [\n115].  Table 3  lists  the  number  of\nparameters  and  training  tokens  for  two  LLM  training\napproaches,  thereby  providing  readers  with  an  understanding\nof  the  training  costs.  The  number  of  parameters  can  also  be\nused to estimate the minimal costs for private deployment. For\ninstance,  loading  a  7B  model  with  float16  format  requires  at\nleast  14  GB  of  memory,  while  serving  with  concurrent\nrequests  necessitates  additional  memory  for  the  transformer\narchitecture’s key-value (KV) cache [\n116].\nAccuracy As indicated in  Table 3, across four benchmarks,\nthe  LLM  training  approach  excels  in  only  one,  whereas\nprompting  a  robust  LLM  (GPT-4)  is  superior.  The  7B\nTableLlama can’t outperform task-specific fine-tuning models\non  specific  tasks  (see  the  WiKiSQL  column  in \nTable 3).\nAnother  example  is  the  NL2SQL  task,  where  Li  et  al.  [77]\nperform  a  systematic  evaluation  with  various  methods,\ncomparing  LLM-based  and  SLM-based  solutions.  Results\nshow  that  there  is  no  clear  winner  between  LLM  and  SLM\nsolutions  on  different  metrics  and  domains.  On  data  analysis\ntasks, instruction tuned models like Lemur [\n44] and DAAgent\n[17]  cannot  surpass  GPT-4.  Table-GPT  continues  to  train  on\nGPT-3.5 and achieves better results on all table-related tasks\nthan GPT-3.5 and ChatGPT. However, the cost of training is\nprohibitively  high  for  ordinary  enterprise  users  who  wish  to\ndeploy privately. Regarding training data, the cost of manual\nannotation is also high. Although synthesis is a cheap choice,\nthe data quality is another concern. A more prevalent approach\nis  using  GPT  as  a  teacher  model  for  data  distillation.\nEvidently, GPT is the performance ceiling.\n \n7.2    Discussion on LLM prompting\nThe  LLM-powered  agent  method  combines  the  power  of\nLLMs  and  the  flexibility  of  external  tools.  On  many  table\ntasks,  prompting  strong  LLMs  like  GPT  still  performs  the\nbest.\nLimited  transferability  However,  these  approaches  often\nnecessitate  hard-coded  prompt  templates,  which  are  long\nstrings  of  instructions  and  demonstrations  manually  crafted\nthrough  trial  and  error.  A  given  string  prompt  might  not\ngeneralize  well  to  other  pipelines,  LLMs,  domains,  or  data\ninputs. Thus, it has limited transferability.\nCost As shown in the “Avg. # of Infers” column of \nTable 3,\nagents  must  prompt  LLMs  repeatedly  to  fulfill  users’\nexpectations. Ma et al. [\n39] conduct experiments on agents for\nspreadsheet  manipulation,  and  the  most  effective  agent\nsolution  is  the  one  that  utilizes  the  ReAct  framework  [\n81],\nexecutes  code  within  the  execution  environment  and  offers\nfeedback to LLMs upon failure. Deploying multiple rounds of\n  \nTable 3    A comparative analysis of various methodologies was conducted using four benchmarks in table QA and NL2SQL tasks. The experimental setup and\nperformance metrics are referenced from corresponding papers. The number of parameters and training tokens are derived from the respective papers detailing\neach method\nType Method # of parameters # of tokens to train Benchmarks Avg. # of infersWikiTQ FeTaQA WikiSQL Spider\nSLM training TaPEX 0.14B − 38.55 − 83.90 15.04 1\nTaPas 0.11B − 31.60 − 74.20 23.05 1\nLLM training TableLlama 7B 3.3B 48.82 67.73 43.70 − 1\nTableLLM 13B 1.1B 62.40 74.50 90.70 83.40 1\nPrompting\nCodeLlama 13B − 43,44 57.24 38.30 21.88 1\nGPT-3.5 − − 58.45 71.18 81.70 67.38 1\nGPT-4 − − 74.09 78.35 84.00 69.53 1\nPrompting agent\nStructGPT(GPT-3.5) − − 52.45 11.80 67.80 84.80 3\nBinder(GPT-3.5) − − 61.61 12.77 78.60 52.55 50\nDATER(GPT-3.5) − − 53.40 18.26 58.20 26.52 100\n12 Front. Comput. Sci., 2025, 19(2): 192350\nReAct prompting with execution feedback takes a substantial\ntime.  Consequently,  all  these  factors  increase  the  total  time\nand financial costs.\nPrivacy  issue  Most  prompting  methods  involve  requesting\nthird-party  model  service  providers,  such  as  OpenAI,  which\ncould  lead  to  data  leakage,  a  situation  many  enterprises  are\nunacceptable. As open-source models advance, enterprises can\nalso  deploy  open-source  models  or  their  own  fine-tuned\nversions, thus gradually mitigating this issue.\nStructure understanding Foundation models, without fine-\ntuning  for  tables,  still  have  difficulty  understanding  table\nstructure and hierarchy. Pang et al. [\n114] create a benchmark\nnamed TIS to assess how effectively LLMs seek information\nfrom tables. They discover that LLMs struggle with tables that\nhave complex hierarchies and exhibit a poor understanding of\ntable  structures,  such  as  locating  a  specific  regin  in  a  two-\ndimensional  table.  Notably,  most  LLMs  perform  at  nearly\nrandom  accuracy  levels  (around  50%)  in  the  table  structure\nunderstanding task, while GPT-4 achieve 66.1%.\n \n8    Challenges and future directions\nIn this section, we outline some challenges and considerations\nfor future research.\n \n8.1    Diverse user input when serving\nUser inputs refer not only to users’ NL queries but also to the\ntable schema and content.\nUser query In real-world applications, user NL queries are\nfrequently ambiguous. For example, when users are unfamiliar\nwith  the  table  they  want  to  query,  they  often  pose  general\nrequests  without  a  clear  objective,  such  as  “help  me  analyze\nthis  table”.  Users  might  also  pose  questions  unrelated  to  the\ntable  within  a  table  AI  system.  Even  when  users  have  some\nknowledge of the table schema and content, they may struggle\nto formulate their query accurately, leading to ambiguity in the\nquery sentences.\nTable schema and content  In practice, table schemas and\ncontent are highly diverse and often proprietary. For instance,\nin  a  domain  or  industry  with  which  the  model  has  limited\nknowledge, it faces challenges in effectively transferring into\nthe field. Existing table training datasets are relatively simple,\neither  consisting  of  simple  table  structures  scraped  from  the\nWeb or synthesized based on powerful LLMs. Constructing a\ncomplex and diverse training dataset is quite costly. Ensuring\nthat  the  training  data  covers  real-world  business  scenarios\nposes a significant challenge.\nFuture table LLMs should adapt quickly and cheaply to real-\nworld  business  needs.  Research  directions  include\nsynthesizing high-quality training data that reflects the diverse\nneeds of specific domains by cost-effective methods.\n \n8.2    Slow and deep thinking\nDaniel Kahneman has revealed that slow and deep thinking is\nthe underlying mechanism of the human brain for processing\ncomplex  problems  [\n117].  The  chain-of-thought  approach  is\nconsidered  to  guide  LLMs  in  solving  complex  reasoning\nproblems, and the recent OpenAI’s o1 shows that LLMs can\nachieve substantial enhancements in performance by allowing\nthem  to  generate  extended  internal  chains-of-thought.\nAdditionally, this internal chain-of-thought can facilitate both\nthe  model  training  processes  and  the  pure  model  prompting\nmethods.\nTraining Typically, there is a considerable gap between the\nuser instructions and the ultimate answer. Therefore, a chain-\nof-thought can serve as an internal mechanism to explain the\nprocess of deriving the final answer, potentially mitigating the\ntraining  difficulty  faced  by  LLMs.  However,  acquiring  an\naccurate  chain-of-thought  is  challenging.  It  can  be\nprohibitively  expensive  when  achieved  through  human\nannotation and difficult to ensure accuracy when automatically\ngenerated by LLMs themselves, particularly in tasks involving\ncomplex  logical  reasoning  over  tables.  Exploring  cost-\neffective  methods  to  guarantee  an  accurate  chain-of-thought\nprocess is a valuable area of research.\nPrompting  In  existing  prompting  methods  on  table  tasks,\nresearchers  develop  their  own  chain-of-thought  workflows,\nwhich  heavily  rely  on  hard-coded  prompt  templates  to  solve\nspecific  table  tasks.  This  results  in  methods  with  weak\ntransferability.  Given  that  OpenAI’s  o1  possesses  inherent\nchain-of-thought abilities, prompting and agent methods must\nreconsider  how  to  construct  their  workflows.  On  the  other\nhand,  the  chain-of-thought  approach  requires  numerous\ninference  iterations,  which  is  time-consuming;  future  table\nprompting  methods  should  balance  inference  times  with\naccuracy.\n \n9    Conclusion\nThis survey is the first comprehensive investigation into Large\nLanguage Models (LLMs) for table processing across various\ntasks, encompassing table QA, spreadsheet manipulation, data\nanalysis,  etc.  We  provide  a  summary  and  categorization  of\ntable tasks from both academic and end-user perspectives. We\nexplore  the  popular  and  essential  techniques  for  table\nprocessing,  including  data  representation,  training,  and\nprompting. We collect and discuss resources like open-source\ndatasets,  benchmarks,  and  software.  Beyond  reviewing\nexisting work, we also identify several challenges within this\ndomain that could inform and guide future research directions.\n \nAcknowledgements    This  work  was  supported  by  the  National  Key  R&D\nProgram  of  China  (2023YFF0725100),  the  National  Natural  Science\nFoundation of China (Grant Nos. 62322214, 62272466), and the Fundamental\nResearch  Funds  for  the  Central  Universities  and  the  Research  Funds  of\nRenmin University of China (24XNKJ22).\n \nCompeting  interests    The  authors  declare  that  they  have  no  competing\ninterests or financial conflicts to disclose. \nOpen Access    This article is licensed under a Creative Commons Attribution\n4.0 International License, which permits use, sharing, adaptation, distribution\nand  reproduction  in  any  medium  or  format,  as  long  as  you  give  appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if changes were made.\nThe images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line\nto the material. If material is not included in the article’s Creative Commons\nlicence  and  your  intended  use  is  not  permitted  by  statutory  regulation  or\nexceeds  the  permitted  use,  you  will  need  to  obtain  permission  directly  from\nthe copyright holder.\nTo view a copy of this licence, visit \nhttp://creativecommons.org/licenses/by/\n4.0/.\nWeizheng LU et al.    Large language model for table processing: a survey 13\nReferences \n  Dong  H,  Cheng  Z,  He  X,  Zhou  M,  Zhou  A,  Zhou  F,  Liu  A,  Han  S,\nZhang  D.  Table  pre-training:  a  survey  on  model  architectures,  pre-\ntraining objectives, and downstream tasks. In: Proceedings of the 31st\nInternational  Joint  Conference  on  Artificial  Intelligence.  2022,\n5426−5435\n1.\n  Badaro  G,  Saeed  M,  Papotti  P.  Transformers  for  tabular  data\nrepresentation:  a  survey  of  models  and  applications.  Transactions  of\nthe Association for Computational Linguistics, 2023, 11: 227−249\n2.\n  Fang  X,  Xu  W,  Tan  F  A,  Zhang  J,  Hu  Z,  Qi  Y,  Nickleach  S,\nSocolinsky  D,  Sengamedu  S,  Faloutsos  C.  Large  language\nmodels(LLMs)  on  tabular  data:  prediction,  generation,  and\nunderstanding — a survey. 2024, arXiv preprint arXiv: 2402.17944\n3.\n  Zhang X, Wang D, Dou L, Zhu Q, Che W. A survey of table reasoning\nwith large language models. 2024, arXiv preprint arXiv: 2402.08259\n4.\n  Zhao W X, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B,\nZhang J, Dong Z, Du Y, Yang C, Chen Y, Chen Z, Jiang J, Ren R, Li\nY, Tang X, Liu Z, Liu P, Nie J Y, Wen J R. A survey of large language\nmodels. 2023, arXiv preprint arXiv: 2303.18223\n5.\n  Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y,\nLi W, Liu P J. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning Research, 2020,\n21(1): 140\n6.\n  Yin  P,  Neubig  G,  Yih  W,  Riedel  S.  TaBERT:  pretraining  for  joint\nunderstanding of textual and tabular data. In: Proceedings of the 58th\nAnnual  Meeting  of  the  Association  for  Computational  Linguistics.\n2020, 8413−8426\n7.\n  Herzig  J,  Nowak  P  K,  Müller  T,  Piccinno  F,  Eisenschlos  J.  TaPas:\nweakly supervised table parsing via pre-training. In: Proceedings of the\n58th  Annual  Meeting  of  the  Association  for  Computational\nLinguistics. 2020, 4320−4333\n8.\n  Deng  X,  Sun  H,  Lees  A,  Wu  Y,  Yu  C.  TURL:  table  understanding\nthrough  representation  learning.  Proceedings  of  the  VLDB\nEndowment, 2020, 14(3): 307−319\n9.\n  Liu Q, Chen B, Guo J, Ziyadi M, Lin Z, Chen W, Lou J. TAPEX: table\npre-training via learning a neural SQL executor. In: Proceedings of the\n10th International Conference on Learning Representations. 2022\n10.\n  Zhang  T,  Yue  X,  Li  Y,  Sun  H.  TableLlama:  towards  open  large\ngeneralist models for tables. In: Proceedings of 2024 Conference of the\nNorth  American  Chapter  of  the  Association  for  Computational\nLinguistics: Human Language Technologies. 2024, 6024−6044\n11.\n  Li P, He Y, Yashar D, Cui W, Ge S, Zhang H, Fainman D R, Zhang D,\nChaudhuri S. Table-GPT: table fine-tuned GPT for diverse table tasks.\nProceedings of the ACM on Management of Data, 2024, 2(3): 176\n12.\n  Zheng M, Feng X, Si Q, She Q, Lin Z, Jiang W, Wang W. Multimodal\ntable  understanding.  In:  Proceedings  of  the  62nd  Annual  Meeting  of\nthe Association for Computational Linguistics. 2024, 9102−9124\n13.\n  Sui  Y,  Zhou  M,  Zhou  M,  Han  S,  Zhang  D.  Table  meets  LLM:  can\nlarge language models understand structured table data? A benchmark\nand  empirical  study.  In:  Proceedings  of  the  17th  ACM  International\nConference on Web Search and Data Mining. 2024, 645−654\n14.\n  Zhang  Y,  Henkel  J,  Floratou  A,  Cahoon  J,  Deep  S,  Patel  J  M.\nReAcTable:  enhancing  ReAct  for  table  question  answering.\nProceedings of the VLDB Endowment, 2024, 17(8): 1981−1994\n15.\n  Li H, Su J, Chen Y, Li Q, Zhang Z. SheetCopilot: bringing software\nproductivity  to  the  next  level  through  large  language  models.  In:\nProceedings  of  the  37th  International  Conference  on  Neural\nInformation Processing Systems. 2023, 4952−4984\n16.\n  Hu X, Zhao Z, Wei S, Chai Z, Ma Q, Wang G, Wang X, Su J, Xu J,\nZhu  M,  Cheng  Y,  Yuan  J,  Li  J,  Kuang  K,  Yang  Y,  Yang  H,  Wu  F.\nInfiAgent-DABench:  evaluating  agents  on  data  analysis  tasks.  In:\nProceedings  of  the  41st  International  Conference  on  Machine\nLearning. 2024, 19544−19572\n17.\n Wei J, Bosma M, Zhao V, Guu K, Yu A W, Lester B, Du N, Dai A M,\nLe  Q  V.  Finetuned  language  models  are  zero-shot  learners.  In:\nProceedings  of  the  10th  International  Conference  on  Learning\nRepresentations. 2022\n18.\n Brown  T  B,  Mann  B,  Ryder  N,  Subbiah  M,  Kaplan  J,  Dhariwal  P,\nNeelakantan  A,  Shyam  P,  Sastry  G,  Askell  A,  Agarwal  S,  Herbert-\nVoss  A,  Krueger  G,  Henighan  T,  Child  R,  Ramesh  A,  Ziegler  D  M,\nWu J, Winter C, Hesse C, Chen M, Sigler E, Litwin M, Gray S, Chess\nB, Clark J, Berner C, McCandlish S, Radford A, Sutskever I, Amodei\nD. Language models are few-shot learners. In: Proceedings of the 34th\nInternational  Conference  on  Neural  Information  Processing  Systems.\n2020, 159\n19.\n Wei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F, Chi E H,\nLe Q V, Zhou D. Chain-of-thought prompting elicits reasoning in large\nlanguage models. In: Proceedings of the 36th International Conference\non Neural Information Processing Systems. 2022, 24824−24837\n20.\n Wang L, Ma C, Feng X, Zhang Z, Yang H, Zhang J, Chen Z, Tang J,\nChen X, Lin Y, Zhao W X, Wei Z, Wen J. A survey on large language\nmodel based autonomous agents. Frontiers of Computer Science, 2024,\n18(6): 186345\n21.\n Lu  W.  Survey  resources.  See  Github.com/godaai/llm-table-survey\nwebsite, 2024\n22.\n Jin N, Siebert J, Li D, Chen Q. A survey on table question answering:\nrecent  advances.  In:  Proceedings  of  the  7th  China  Conference  on\nKnowledge  Graph  and  Semantic  Computing:  Knowledge  Graph\nEmpowers the Digital Economy. 2022, 174−186\n23.\n Qin B, Hui B, Wang L, Yang M, Li J, Li B, Geng R, Cao R, Sun J, Si\nL,  Huang  F,  Li  Y.  A  survey  on  text-to-SQL  parsing:  concepts,\nmethods,  and  future  directions.  2022,  arXiv  preprint  arXiv:\n2208.13629\n24.\n Hong Z, Yuan Z, Zhang Q, Chen H, Dong J, Huang F, Huang X. Next-\nGeneration  database  interfaces:  a  survey  of  LLM-based  text-to-SQL.\n2024, arXiv preprint arXiv: 2406.08426\n25.\n Zhang S, Balog K. Web table extraction, retrieval, and augmentation: a\nsurvey.  ACM  Transactions  on  Intelligent  Systems  and  Technology\n(TIST), 2020, 11(2): 13\n26.\n Rahman S, Mack K, Bendre M, Zhang R, Karahalios K, Parameswaran\nA. Benchmarking spreadsheet systems. In: Proceedings of 2020 ACM\nSIGMOD  International  Conference  on  Management  of  Data.  2020,\n1589−1599\n27.\n Ritze D, Bizer C. Matching Web tables to DBpedia - a feature utility\nstudy.  In:  Proceedings  of  the  20th  International  Conference  on\nExtending Database Technology. 2017, 210−221\n28.\n Bhagavatula C S, Noraset T, Downey D. TabEL: entity linking in Web\ntables.  In:  Proceedings  of  the  14th  International  Semantic  Web\nConference on The Semantic Web - ISWC 2015. 2015, 425−441\n29.\n Cheng Z, Xie T, Shi P, Li C, Nadkarni R, Hu Y, Xiong C, Radev D,\nOstendorf  M,  Zettlemoyer  L,  Smith  N  A,  Yu  T.  Binding  language\nmodels  in  symbolic  languages.  In:  Proceedings  of  the  11th\nInternational Conference on Learning Representations. 2023\n30.\n Wang  Z,  Zhang  H,  Li  C  L,  Eisenschlos  J  M,  Perot  V,  Wang  Z,\nMiculicich  L,  Fujii  Y,  Shang  J,  Lee  C  Y,  Pfister  T.  Chain-of-table:\nevolving  tables  in  the  reasoning  chain  for  table  understanding.  In:\nProceedings  of  the  12th  International  Conference  on  Learning\nRepresentations. 2024\n31.\n Pasupat  P,  Liang  P.  Compositional  semantic  parsing  on  semi-\nstructured  tables.  In:  Proceedings  of  the  53rd  Annual  Meeting  of  the\nAssociation  for  Computational  Linguistics  and  the  7th  International\nJoint Conference on Natural Language Processing. 2015, 1470−1480\n32.\n Ye Y, Hui B, Yang M, Li B, Huang F, Li Y. Large language models\nare  versatile  decomposers:  decomposing  evidence  and  questions  for\ntable-based reasoning. In: Proceedings of the 46th International ACM\nSIGIR  Conference  on  Research  and  Development  in  Information\nRetrieval. 2023, 174−184\n33.\n14 Front. Comput. Sci., 2025, 19(2): 192350\n  Chen W, Wang H, Chen J, Zhang Y, Wang H, Li S, Zhou X, Wang W\nY. TabFact: a large-scale dataset for table-based fact Verification. In:\nProceedings of the 33rd Neural Information Processing Systems. 2019\n34.\n  Parikh A, Wang X, Gehrmann S, Faruqui M, Dhingra B, Yang D, Das\nD.  ToTTo:  a  controlled  table-to-text  generation  dataset.  In:\nProceedings  of  2020  Conference  on  Empirical  Methods  in  Natural\nLanguage Processing. 2020, 1173−1186\n35.\n  Qian Y, He Y, Zhu R, Huang J, Ma Z, Wang H, Wang Y, Sun X, Lian\nD,  Ding  B,  Zhou  J.  UniDM:  a  Unified  framework  for  data\nmanipulation with large language models. In: Proceedings of Machine\nLearning and Systems 6 (MLSys 2024) Conference. 2024\n36.\n  Ahmad M S, Naeem Z A, Eltabakh M, Ouzzani M, Tang N. RetClean:\nretrieval-based data cleaning using foundation models and data lakes.\n2023, arXiv preprint arXiv: 2303.16909\n37.\n  Chen Y, Yuan Y, Zhang Z, Zheng Y, Liu J, Ni F, Hao J. SheetAgent:\ntowards a generalist agent for spreadsheet reasoning and manipulation\nvia large language models. 2024, arXiv preprint arXiv: 2403.03636\n38.\n  Ma Z, Zhang B, Zhang J, Yu J, Zhang X, Zhang X, Luo S, Wang X,\nTang J. SpreadsheetBench: towards challenging real world spreadsheet\nmanipulation. 2024, arXiv preprint arXiv: 2406.14991\n39.\n  Li  H,  Zhang  J,  Liu  H,  Fan  J,  Zhang  X,  Zhu  J,  Wei  R,  Pan  H,  Li  C,\nChen  H.  CodeS:  towards  building  open-source  language  models  for\ntext-to-SQL. Proceedings of the ACM on Management of Data, 2024,\n2(3): 127\n40.\n  Gao D, Wang H, Li Y, Sun X, Qian Y, Ding B, Zhou J. Text-to-SQL\nempowered  by  large  language  models:  a  benchmark  evaluation.  In:\nProceedings of the VLDB Endowment, 2024, 17(5): 1132−1145\n41.\n  Yu T, Zhang R, Yang K, Yasunaga M, Wang D, Li Z, Ma J, Li I, Yao\nQ, Roman S, Zhang Z, Radev D. Spider: a large-scale human-labeled\ndataset  for  complex  and  cross-domain  semantic  parsing  and  text-to-\nSQL task. In: Proceedings of 2018 Conference on Empirical Methods\nin Natural Language Processing. 2018, 3911−3921\n42.\n  Zhang W, Shen Y, Lu W, Zhuang Y T. Data-Copilot: bridging billions\nof data and humans with autonomous workflow. 2023, arXiv preprint\narXiv: 2306.07209\n43.\n  Xu Y, Su H, Xing C, Mi B, Liu Q, Shi W, Hui B, Zhou F, Liu Y, Xie\nT,  Cheng  Z,  Zhao  S,  Kong  L,  Wang  B,  Xiong  C,  Yu  T.  Lemur:\nharmonizing  natural  language  and  code  for  language  agents.  In:\nProceedings  of  the  12th  International  Conference  on  Learning\nRepresentations. 2024\n44.\n  Lai Y, Li C, Wang Y, Zhang T, Zhong R, Zettlemoyer L, Yih W T,\nFried D, Wang S, Yu T. DS-1000: a natural and reliable benchmark for\ndata science code generation. In: Proceedings of the 40th International\nConference on Machine Learning. 2023, 18319−18345\n45.\n  Chen L, Huang C, Zheng X, Lin J, Huang X. TableVLM: multi-modal\npre-training for table structure recognition. In: Proceedings of the 61st\nAnnual  Meeting  of  the  Association  for  Computational  Linguistics.\n2023, 2437−2449\n46.\n  Li  M,  Cui  L,  Huang  S,  Wei  F,  Zhou  M,  Li  Z.  TableBank:  table\nbenchmark  for  Image-based  table  detection  and  recognition.  In:\nProceedings  of  the  12th  Language  Resources  and  Evaluation\nConference. 2020, 1918−1925\n47.\n  Zhao W, Feng H, Liu Q, Tang J, Wei S, Wu B, Liao L, Ye Y, Liu H,\nZhou  W,  Li  H,  Huang  C.  TabPedia:  towards  comprehensive  visual\ntable understanding with concept synergy. 2024, arXiv preprint arXiv:\n2406.01326\n48.\n  Zhong  X,  ShafieiBavani  E,  Jimeno  Yepes  A.  Image-based  table\nrecognition:  data,  model,  and  evaluation.  In:  Proceedings  of  the  16th\nEuropean  Conference  on  Computer  Vision  -  ECCV  2020.  2020,\n564−580\n49.\n  Abedjan  Z,  Chu  X,  Deng  D,  Fernandez  R  C,  Ilyas  I  F,  Ouzzani  M,\nPapotti P, Stonebraker M, Tang N. Detecting data errors: where are we\nand  what  needs  to  be  done?  Proceedings  of  the  VLDB  Endowment,\n50.\n2016, 9(12): 993−1004\n Barke  S,  James  M  B,  Polikarpova  N.  Grounded  copilot:  how\nprogrammers interact with code-generating models. Proceedings of the\nACM on Programming Languages, 2023, 7(OOPSLA1): 78\n51.\n Singha  A,  Cambronero  J,  Gulwani  S,  Le  V,  Parnin  C.  Tabular\nrepresentation,  noisy  operators,  and  impacts  on  table  structure\nunderstanding  tasks  in  LLMs.  In:  Proceedings  of  the  Table\nRepresentation Learning Workshop at NeurIPS 2023. 2023\n52.\n Tian Y, Zhao J, Dong H, Xiong J, Xia S, Zhou M, Lin Y, Cambronero\nJ, He Y, Han S, Zhang D. SpreadsheetLLM: encoding spreadsheets for\nlarge language models. 2024, arXiv preprint arXiv: 2407.09025\n53.\n Nan  L,  Zhao  Y,  Zou  W,  Ri  N,  Tae  J,  Zhang  E,  Cohan  A,  Radev  D.\nEnhancing text-to-SQL capabilities of large language models: a study\non  prompt  design  strategies.  In:  Proceedings  of  the  Findings  of  the\nAssociation  for  Computational  Linguistics:  EMNLP  2023.  2023,\n14935−14956\n54.\n Deng N, Sun Z, He R, Sikka A, Chen Y, Ma L, Zhang Y, Mihalcea R.\nTables  as  texts  or  images:  evaluating  the  table  reasoning  ability  of\nLLMs and MLLMs. In: Proceedings of the Findings of the Association\nfor Computational Linguistics: ACL 2024. 2024, 407−426\n55.\n Xu Y, Li M, Cui L, Huang S, Wei F, Zhou M. LayoutLM: pre-training\nof text and layout for document image understanding. In: Proceedings\nof  the  26th  ACM  SIGKDD  International  Conference  on  Knowledge\nDiscovery & Data Mining. 2020, 1192−1200\n56.\n Xu Y, Xu Y, Lv T, Cui L, Wei F, Wang G, Lu Y, Florencio D, Zhang\nC, Che W, Zhang M, Zhou L. LayoutLMv2: multi-modal pre-training\nfor visually-rich document understanding. In: Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and\nthe  11th  International  Joint  Conference  on  Natural  Language\nProcessing. 2021, 2579−2591\n57.\n Xia S, Xiong J, Dong H, Zhao J, Tian Y, Zhou M, He Y, Han S, Zhang\nD. Vision language models for spreadsheet understanding: challenges\nand opportunities. In: Proceedings of the 3rd Workshop on Advances\nin Language and Vision Research (ALVR). 2024, 116−128\n58.\n He  K,  Zhang  X,  Ren  S,  Sun  J.  Deep  residual  learning  for  image\nrecognition.  In:  Proceedings  of  2016  IEEE  Conference  on  Computer\nVision and Pattern Recognition. 2016, 770−778\n59.\n Dosovitskiy  A,  Beyer  L,  Kolesnikov  A,  Weissenborn  D,  Zhai  X,\nUnterthiner  T,  Dehghani  M,  Minderer  M,  Heigold  G,  Gelly  S,\nUszkoreit J, Houlsby N. An image is worth 16x16 words: transformers\nfor  image  recognition  at  scale.  In:  Proceedings  of  International\nConference on Learning Representations. 2021\n60.\n Devlin  J,  Chang  M  W,  Lee  K,  Toutanova  K.  BERT:  pre-training  of\ndeep  bidirectional  transformers  for  language  understanding.  In:\nProceedings of 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics. 2019, 4171−4186\n61.\n Iida  H,  Thai  D,  Manjunatha  V,  Iyyer  M.  TABBIE:  pretrained\nrepresentations of tabular data. In: Proceedings of 2021 Conference of\nthe  North  American  Chapter  of  the  Association  for  Computational\nLinguistics: Human Language Technologies. 2021, 3446−3456\n62.\n Li H, Zhang J, Li C, Chen H. RESDSQL: decoupling schema linking\nand  skeleton  parsing  for  text-to-SQL.  In:  Proceedings  of  the  37th\nAAAI Conference on Artificial Intelligence. 2023, 13067−13075\n63.\n Wei  Y,  Wang  Z,  Liu  J,  Ding  Y,  Zhang  L.  Magicoder:  empowering\ncode  generation  with  OSS-instruct.  In:  Proceedings  of  the  41st\nInternational Conference on Machine Learning. 2024\n64.\n Yang J, Hui B, Yang M, Yang J, Lin J, Zhou C. Synthesizing text-to-\nSQL  data  from  weak  and  strong  LLMs.  In:  Proceedings  of  the  62nd\nAnnual  Meeting  of  the  Association  for  Computational  Linguistics.\n2024, 7864−7875\n65.\n Zhang  C,  Mao  Y,  Fan  Y,  Mi  Y,  Gao  Y,  Chen  L,  Lou  D,  Lin  J.\nFinSQL:  model-agnostic  LLMs-based  text-to-SQL  framework  for\nfinancial analysis. In: Proceedings of Companion of 2024 International\nConference on Management of Data. 2024, 93−105\n66.\nWeizheng LU et al.    Large language model for table processing: a survey 15\n  Zhang X, Zhang J, Ma Z, Li Y, Zhang B, Li G, Yao Z, Xu K, Zhou J,\nZhang-Li D, Yu J, Zhao S, Li J, Tang J. TableLLM: enabling tabular\ndata manipulation by LLMs in real office usage scenarios. 2024, arXiv\npreprint arXiv: 2403.19318\n67.\n  Zhuang A, Zhang G, Zheng T, Du X, Wang J, Ren W, Huang S W, Fu\nJ, Yue X, Chen W. StructLM: towards building generalist models for\nstructured  knowledge  grounding.  2024,  arXiv  preprint  arXiv:\n2402.16671\n68.\n  Fan J, Gu Z, Zhang S, Zhang Y, Chen Z, Cao L, Li G, Madden S, Du\nX,  Tang  N.  Combining  small  language  models  and  large  language\nmodels for zero-shot NL2SQL. Proceedings of the VLDB Endowment,\n2024, 17(11): 2750−2763\n69.\n  Alonso  I,  Agirre  E,  Lapata  M.  PixT3:  pixel-based  table-to-text\ngeneration.  In:  Proceedings  of  the  62nd  Annual  Meeting  of  the\nAssociation for Computational Linguistics. 2024, 6721−6736\n70.\n  Parmar  J,  Satheesh  S,  Patwary  M,  Shoeybi  M,  Catanzaro  B.  Reuse,\ndon’t  retrain:  a  recipe  for  continued  pretraining  of  language  models.\n2024, arXiv preprint arXiv: 2407.07263\n71.\n  Li Z, Peng B, He P, Galley M, Gao J, Yan X. Guiding large language\nmodels via directional stimulus prompting. In: Proceedings of the 37th\nInternational  Conference  on  Neural  Information  Processing  System.\n2023, 2735\n72.\n  Luo Z, Xu C, Zhao P, Sun Q, Geng X, Hu W, Tao C, Ma J, Lin Q,\nJiang D. WizardCoder: empowering code large language models with\nEvol-Instruct. In: Proceedings of the 12th International Conference on\nLearning Representations. 2024\n73.\n  Rafailov R, Sharma A, Mitchell E, Ermon S, Manning C D, Finn C.\nDirect  preference  optimization:  your  language  model  is  secretly  a\nreward model. In: Proceedings of the 37th International Conference on\nNeural Information Processing System. 2023, 2338\n74.\n  Lester B, Al-Rfou R, Constant N. The power of scale for parameter-\nefficient  prompt  tuning.  In:  Proceedings  of  2021  Conference  on\nEmpirical Methods in Natural Language Processing. 2021, 3045−3059\n75.\n  Hu E, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen\nW.  LoRA:  lowrank  adaptation  of  large  language  models.  In:\nProceedings  of  the  10th  International  Conference  on  Learning\nRepresentations. 2022\n76.\n  Li B, Luo Y, Chai C, Li G, Tang N. The dawn of natural language to\nSQL:  are  we  fully  ready?  Proceedings  of  the  VLDB  Endowment,\n2024, 17(11): 3318−3331\n77.\n  Li R, Allal L B, Zi Y, Muennighoff N, Kocetkov D, Mou C, Marone\nM, Akiki C, Li J, Chim J, Liu Q, Zheltonozhskii E, Zhuo T Y, Wang\nT, Dehaene O, Davaadorj M, Lamy-Poirier J, Monteiro J, Shliazhko O,\nGontier  N,  Meade  N,  Zebaze  A,  Yee  M  H,  Umapathi  L  K,  Zhu  J,\nLipkin B, Oblokulov M, Wang Z R, Murthy R, Stillerman J, Patel S S,\nAbulkhanov D, Zocca M, Dey M, Zhang Z, Fahmy N, Bhattacharyya\nU, Yu W, Singh S, Luccioni S, Villegas P, Kunakov M, Zhdanov F,\nRomero  M,  Lee  T,  Timor  N,  Ding  J,  Schlesinger  C,  Schoelkopf  H,\nEbert J, Dao T, Mishra M, Gu A, Robinson J, Anderson C J, Dolan-\nGavitt  B,  Contractor  D,  Reddy  S,  Fried  D,  Bahdanau  D,  Jernite  Y,\nFerrandis C M, Hughes S, Wolf T, Guha A, von Werra L, de Vries H.\nStarCoder:  may  the  source  be  with  you!  Transactions  on  Machine\nLearning Research, 2023\n78.\n  Ji Z, Lee N, Frieske R, Yu T, Su D, Xu Y, Ishii E, Bang Y J, Madotto\nA,  Fung  P.  Survey  of  hallucination  in  natural  language  generation.\nACM Computing Surveys, 2023, 55(12): 248\n79.\n  Nassar  A,  Livathinos  N,  Lysak  M,  Staar  P.  TableFormer:  table\nstructure  understanding  with  transformers.  In:  Proceedings  of  2022\nIEEE/CVF Conference on Computer Vision and Pattern Recognition.\n2022, 4604−4613\n80.\n  Yao  S,  Zhao  J,  Yu  D,  Du  N,  Shafran  I,  Narasimhan  K  R,  Cao  Y.\nReAct:  synergizing  reasoning  and  acting  in  language  models.  In:\nProceedings  of  the  11th  International  Conference  on  Learning\nRepresentations. 2023\n81.\n Zhao Y, Chen L, Cohan A, Zhao C. TaPERA: enhancing faithfulness\nand  interpretability  in  long-form  table  QA  by  content  planning  and\nexecution-based  reasoning.  In:  Proceedings  of  the  62nd  Annual\nMeeting  of  the  Association  for  Computational  Linguistics.  2024,\n12824−12840\n82.\n Zhang  Z,  Gao  Y,  Lou  J  G.  E5:  zero-shot  hierarchical  table  analysis\nusing  augmented  LLMs  via  explain,  extract,  execute,  exhibit  and\nextrapolate.  In:  Proceedings  of  2024  Conference  of  the  North\nAmerican  Chapter  of  the  Association  for  Computational  Linguistics.\n2024, 1244−1258\n83.\n Zhou D, Schaerli N, Hou L, Wei J, Scales N, Wang X, Schuurmans D,\nCui  C,  Bousquet  O,  Le  Q  V,  Chi  E  H.  Least-to-most  prompting\nenables complex reasoning in large language models. In: Proceedings\nof  the  11th  International  Conference  on  Learning  Representations.\n2023\n84.\n Pourreza  M,  Rafiei  D.  DIN-SQL:  decomposed  in-context  learning  of\ntext-to-SQL  with  self-correction.  In:  Proceedings  of  the  37th\nConference on Neural Information Processing Systems. 2023\n85.\n Xie Y, Jin X, Xie T, Matrixmxlin M, Chen L, Yu C, Lei C, Zhuo C,\nHu B, Li Z. Decomposition for enhancing attention: improving LLM-\nbased  text-to-SQL  through  workflow  paradigm.  In:  Proceedings  of\nFindings of the Association for Computational Linguistics: ACL 2024.\n2024, 10796−10816\n86.\n Nahid  M,  Rafiei  D.  TabSQLify:  enhancing  reasoning  capabilities  of\nLLMs  through  table  decomposition.  In:  Proceedings  of  2024\nConference  of  the  North  American  Chapter  of  the  Association  for\nComputational  Linguistics:  Human  Language  Technologies.  2024,\n5725−5737\n87.\n Wang  X,  Wei  J,  Schuurmans  D,  Le  Q  V,  Chi  E  H,  Narang  S,\nChowdhery  A,  Zhou  D.  Self-consistency  improves  chain  of  thought\nreasoning  in  language  models.  In:  Proceedings  of  the  11th\nInternational Conference on Learning Representations. 2023\n88.\n Lee  D,  Park  C,  Kim  J,  Park  H.  MCS-SQL:  leveraging  multiple\nprompts  and  multiple-choice  selection  for  text-to-SQL  generation.\n2024, arXiv preprint arXiv: 2405.07467\n89.\n Jiang  S,  Wang  Y,  Wang  Y.  SelfEvolve:  a  code  evolution  framework\nvia large language models. 2023, arXiv preprint arXiv: 2306.02907\n90.\n Karpukhin V, Oguz B, Min S, Lewis P, Wu L, Edunov S, Chen D, Yih\nW T. Dense passage retrieval for open-domain question answering. In:\nProceedings  of  2020  Conference  on  Empirical  Methods  in  Natural\nLanguage Processing. 2020, 6769−6781\n91.\n Jiang  J,  Zhou  K,  Dong  Z,  Ye  K,  Zhao  X,  Wen  J  R.  StructGPT:  a\ngeneral framework for large language model to reason over structured\ndata.  In:  Proceedings  of  2023  Conference  on  Empirical  Methods  in\nNatural Language Processing. 2023, 9237−9251\n92.\n Sui Y, Zou J, Zhou M, He X, Du L, Han S, Zhang D M. TAP4LLM:\ntable provider on sampling, augmenting, and packing semi-structured\ndata for large language model reasoning. 2023, arXiv preprint arXiv:\n2312.09039\n93.\n Chen S, Liu H, Jin W, Sun X, Feng X, Fan J, Du X, Tang N. ChatPipe:\norchestrating  data  preparation  pipelines  by  optimizing  human-\nChatGPT  interactions.  In:  Proceedings  of  Companion  of  2024\nInternational Conference on Management of Data. 2024, 484−487\n94.\n Fan  J,  Wang  Z,  Xie  Y,  Yang  Z.  A  theoretical  analysis  of  deep  Q-\nlearning.  In:  Proceedings  of  the  2nd  Annual  Conference  on  Learning\nfor Dynamics and Control. 2020, 486−489\n95.\n Zhao B, Ji C, Zhang Y, He W, Wang Y, Wang Q, Feng R, Zhang X.\nLarge language models are complex table parsers. In: Proceedings of\n2023  Conference  on  Empirical  Methods  in  Natural  Language\nProcessing. 2023, 14786−14802\n96.\n Li  J,  Huo  N,  Gao  Y,  Shi  J,  Zhao  Y,  Qu  G,  Wu  Y,  Ma  C,  Lou  J  G,\nCheng R. Tapilot-crossing: benchmarking and evolving LLMs towards\ninteractive  data  analysis  agents.  2024,  arXiv  preprint  arXiv:\n2403.05307v1\n97.\n16 Front. Comput. Sci., 2025, 19(2): 192350\n  Zhong V, Xiong C, Socher R. Seq2SQL: generating structured queries\nfrom  natural  language  using  reinforcement  learning.  2017,  arXiv\npreprint arXiv: 1709.00103\n98.\n  Zhao  Y,  Zhao  C,  Nan  L,  Qi  Z,  Zhang  W,  Tang  X,  Mi  B,  Radev  D.\nRobuT:  a  systematic  study  of  table  QA  robustness  against  human-\nannotated adversarial perturbations. In: Proceedings of the 61st Annual\nMeeting  of  the  Association  for  Computational  Linguistics.  2023,\n6064−6081\n99.\n  Iyyer  M,  Yih  W  T,  Chang  M  W.  Search-based  neural  structured\nlearning for sequential question answering. In: Proceedings of the 55th\nAnnual  Meeting  of  the  Association  for  Computational  Linguistics.\n2017, 1821−1831\n100.\n  Li J, Hui B, Qu G, Yang J, Li B, Li B, Wang B, Qin B, Geng R, Huo\nN, Zhou X, Ma C, Li G, Chang K C C, Huang F, Cheng R, Li Y. Can\nLLM already serve as a database interface? A big bench for large-scale\ndatabase  grounded  text-to-SQLs.  In:  Proceedings  of  the  37th\nInternational  Conference  on  Neural  Information  Processing  Systems.\n2023, 1835\n101.\n  Motl  J,  Schulte  O.  The  CTU  Prague  relational  learning  repository.\n2024, arXiv preprint arXiv: 1511.03086\n102.\n  Chang  S,  Wang  J,  Dong  M,  Pan  L,  Zhu  H,  Li  A,  Lan  W,  Zhang\nS,·Jiang J, Lilien J, Ash S, Wang W,·Wang Z,·Castelli V, Ng P,·Xiang\nB. Dr.Spider: a diagnostic evaluation benchmark towards text-to-SQL\nrobustness.  In:  Proceedings  of  the  11th  International  Conference  on\nLearning Representations. 2023\n103.\n  Zhang Y, Deriu J, Katsogiannis-Meimarakis G, Kosten C, Koutrika G,\nStockinger  K.  ScienceBenchmark:  a  complex  real-world  benchmark\nfor  evaluating  natural  language  to  SQL  systems.  Proceedings  of  the\nVLDB Endowment, 2024, 17(4): 685−698\n104.\n  He X, Zhou M, Zhou M, Xu J, Lv X, Li T, Shao Y, Han S, Yuan Z,\nZhang  D.  AnaMeta:  a  table  understanding  dataset  of  field  metadata\nknowledge  shared  by  multi-dimensional  data  analysis  tasks.  In:\nProceedings  of  Findings  of  the  Association  for  Computational\nLinguistics: ACL 2023. 2023, 9471−9492\n105.\n  Jiménez-Ruiz  E,  Hassanzadeh  O,  Efthymiou  V,  Chen  J,  Srinivas  K.\nSemTab  2019:  resources  to  benchmark  tabular  data  to  knowledge\ngraph  matching  systems.  In:  Proceedings  of  the  17th  International\nConference on the Semantic Web. 2020, 514−530\n106.\n  Hulsebos M, Demiralp Ç, Groth P. GitTables: a large-scale corpus of\nrelational  tables.  Proceedings  of  the  ACM  on  Management  of  Data,\n2023, 1(1): 30\n107.\n  Döhmen  T,  Geacu  R,  Hulsebos  M,  Schelter  S.  SchemaPile:  a  large\ncollection of relational database schemas. Proceedings of the ACM on\nManagement of Data, 2024, 2(3): 172\n108.\n  Wretblad  N,  Riseby  F,  Biswas  R,  Ahmadi  A,  Holmström  O.\nUnderstanding the effects of noise in text-to-SQL: an examination of\nthe  BIRD-bench  benchmark.  In:  Proceedings  of  the  62nd  Annual\nMeeting  of  the  Association  for  Computational  Linguistics.  2024,\n356−369\n109.\n  Liu J. LlamaIndex. See Docs.llamaindex.ai/en/stable/ website, 2022110.\n  Xue S, Qi D, Jiang C, Cheng F, Chen K, Zhang Z, Zhang H, Wei G,\nZhao W, Zhou F, Yi H, Liu S, Yang H, Chen F. Demonstration of DB-\nGPT:  next  generation  data  interaction  system  empowered  by  large\nlanguage  models.  Proceedings  of  the  VLDB  Endowment,  2024,\n17(12): 4365−4368\n111.\n  Vanna. AI. Vanna. See Github.com/vanna-ai/vanna website, 2023112.\n  Venturi G. Pandas-ai. See Github.com/Sinaptik-AI/pandas-ai website,\n2023\n113.\n  Pang  C,  Cao  Y,  Yang  C,  Luo  P.  Uncovering  limitations  of  large\nlanguage  models  in  information  seeking  from  tables.  In:  Proceedings\nof  Findings  of  the  Association  for  Computational  Linguistics:  ACL\n2024. 2024, 1388−1409\n114.\n  Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M A, Lacroix T,\nRozière  B,  Goyal  N,  Hambro  E,  Azhar  F,  Rodriguez  A,  Joulin  A,\n115.\nGrave E, Lample G. LLaMA: open and efficient foundation language\nmodels. 2023, arXiv preprint arXiv: 2302.13971\n Kwon W, Li Z, Zhuang S, Sheng Y, Zheng L, Yu C H, Gonzalez J,\nZhang H, Stoica I. Efficient memory management for large language\nmodel  serving  with  PagedAttention.  In:  Proceedings  of  the  29th\nSymposium on Operating Systems Principles. 2023, 611−626\n116.\n Kahneman  D.  Thinking,  Fast  and  Slow.  London:  Farrar,  Straus  and\nGiroux, 2011\n117.\nWeizheng  Lu  is  a  senior  research  engineer  at\nRenmin University of China. His current research\ninterests include high-performance data science.\nJing  Zhang  is  a  professor  at  School  of\nInformation,  Renmin  University  of  China.  Her\nresearch  focuses  on  data  mining  and  knowledge\ndiscovery.\nJu  Fan  is  a  professor  at  School  of  Information,\nRenmin University of China. His research focuses\non artificial intelligence for databases.\nZihao  Fu  is  a  senior  AI  product  manager  at\nKingsoft  Office,  specializing  in  spreadsheet  AI.\nHe focuses on AI-powered productivity tools and\nsoftware.\nYueguo  Chen  is  a  professor  at  School  of\nInformation,  Renmin  University  of  China.  He\nfocuses  on  the  interdisciplinary  fields  of  big  data\nand artificial intelligence with social science.\nXiaoyong  Du  is  a  professor  at  School  of\nInformation,  Renmin  University  of  China.  His\ncurrent  research  interests  include  databases  and\nintelligent information retrieval.\nWeizheng LU et al.    Large language model for table processing: a survey 17",
  "topic": "Table (database)",
  "concepts": [
    {
      "name": "Table (database)",
      "score": 0.7398198246955872
    },
    {
      "name": "Computer science",
      "score": 0.5370411276817322
    },
    {
      "name": "Natural language processing",
      "score": 0.3381780982017517
    },
    {
      "name": "Database",
      "score": 0.2545265555381775
    }
  ],
  "institutions": []
}