{
  "title": "BBTv2: Towards a Gradient-Free Future with Large Language Models",
  "url": "https://openalex.org/W4385573069",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2261684028",
      "name": "Tianxiang Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2305259341",
      "name": "Zhengfu He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113879456",
      "name": "Hong Qian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102441169",
      "name": "Yunhua Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2161482855",
      "name": "Xuanjing Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115470192",
      "name": "Xipeng Qiu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2876111955",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221150345",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4225619898",
    "https://openalex.org/W4293454876",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W3096580779",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2112036188",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2573774999",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W2138537392",
    "https://openalex.org/W4221142421",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W1480330138",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2413634576",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W3198249268",
    "https://openalex.org/W4286981949",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W2192203593",
    "https://openalex.org/W4281684631",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2139805416",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3202031169",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W4294808066",
    "https://openalex.org/W4205991051"
  ],
  "abstract": "Most downstream adaptation methods tune all or part of the parameters of pre-trained models (PTMs) through gradient descent, where the tuning cost increases linearly with the growth of the model size.By contrast, gradient-free methods only require the forward computation of the PTM to tune the prompt, retaining the benefits of efficient tuning and deployment.Though, past work on gradient-free tuning often introduces gradient descent to seek a good initialization of prompt and lacks versatility across tasks and PTMs.In this paper, we present BBTv2, an improved version of Black-Box Tuning, to drive PTMs for few-shot learning.We prepend continuous prompts to every layer of the PTM and propose a divide-and-conquer gradient-free algorithm to optimize the prompts at different layers alternately.Extensive experiments across various tasks and PTMs show that BBTv2 can achieve comparable performance to full model tuning and state-of-the-art parameter-efficient methods (e.g., Adapter, LoRA, BitFit, etc.) under few-shot settings while maintaining much fewer tunable parameters.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3916‚Äì3930\nDecember 7-11, 2022 ¬©2022 Association for Computational Linguistics\nBBTv2: Towards a Gradient-Free Future with Large Language Models\nTianxiang Sun‚ô¢‚ô° Zhengfu He‚ô¢ Hong Qian‚ô†\nYunhua Zhou‚ô¢‚ô° Xuanjing Huang‚ô¢‚ô° Xipeng Qiu‚ô¢‚ô°‚àó\n‚ô¢School of Computer Science, Fudan University\n‚ô°Shanghai Key Laboratory of Intelligent Information Processing, Fudan University\n‚ô†School of Computer Science and Technology, East China Normal University\n{txsun19,zfhe19,zhouyh20,xjhuang,xpqiu}@fudan.edu.cn hqian@cs.ecnu.edu.cn\nAbstract\nMost downstream adaptation methods tune all\nor part of the parameters of pre-trained models\n(PTMs) through gradient descent, where the\ntuning cost increases linearly with the growth\nof the model size. By contrast, gradient-free\nmethods only require the forward computa-\ntion of the PTM to tune the prompt, retain-\ning the benefits of efficient tuning and deploy-\nment. Though, past work on gradient-free tun-\ning often introduces gradient descent to seek a\ngood initialization of prompt and lacks versa-\ntility across tasks and PTMs. In this paper, we\npresent BBTv2, an improved version of Black-\nBox Tuning (Sun et al., 2022b), to drive PTMs\nfor few-shot learning. We prepend continuous\nprompts to every layer of the PTM and propose\na divide-and-conquer gradient-free algorithm\nto optimize the prompts at different layers al-\nternately. Extensive experiments across var-\nious tasks and PTMs show that BBTv2 can\nachieve comparable performance to full model\ntuning and state-of-the-art parameter-efficient\nmethods (e.g., Adapter, LoRA, BitFit, etc.) un-\nder few-shot settings while maintaining much\nfewer tunable parameters.\n1 Introduction\nThe past few years have witnessed remarkable\nprogress of large language models (LLMs) (Devlin\net al., 2019; Raffel et al., 2020; Brown et al., 2020).\nIt has been repeatedly demonstrated that scaling\nup the model size is promising to achieve better\nperformance. However, the growing model size\nalso leads to a linear increase in tuning cost. Fine-\ntuning and deploying a separate copy of the LLM\nfor each downstream task become prohibitively ex-\npensive. To that end, much effort has been devoted\nto parameter-efficient tuning(PET) (He et al., 2021;\nDing et al., 2022), which only tunes a small portion\nof parameters while keeping most of the parame-\nters of the LLM unchanged. By PET, LLMs can\n‚àó Corresponding author.\nFigure 1: BBTv2 achieves comparable results to\ngradient-based methods on average performance over 7\nlanguage understanding tasks (¬ß5.1) with much fewer\ntunable parameters. Size of the circle is proportional\nto the standard deviation of the performance. All the\nmethods are evaluated on RoBERTaLARGE.\nbe specialized to a downstream task at inference\ntime by activating a small number of task-specific\nparameters. Though it is deployment-efficient, tun-\ning the small portion of parameters still requires\nback-propagation through the entire LLM, which is\nexpensive or even infeasible for many practitioners.\nTo make LLMs benefit a wider range of au-\ndiences, a common practice is to release LLMs\nas a service and allow users to access the pow-\nerful LLMs through their inference APIs (Brown\net al., 2020). In such a scenario, called Languaged-\nModel-as-a-Service (LMaaS) (Sun et al., 2022b),\nusers cannot access or tune model parameters but\ncan only tune their prompts to accomplish language\ntasks of interest. Brown et al. (2020) propose to re-\ncast downstream tasks as a language modeling task\nand perform different tasks by conditioning on task-\nspecific text prompts. Further, they demonstrate\nthat LLMs exhibit an emergent ability of in-context\nlearning, i.e., LLMs can learn to perform tasks with\na few demonstrations provided in the input context\n3916\nwithout updating parameters. Nevertheless, its per-\nformance has been shown to highly depend on the\nchoice of the prompt or the demonstrations (Zhao\net al., 2021; Liu et al., 2022) and still lags far be-\nhind full model tuning.\nRecently, Sun et al. (2022b) proposed Black-Box\nTuning (BBT), which optimizes continuous prompt\nby only accessing model inference APIs. In con-\ntrast to gradient-based tuning that requires expen-\nsive back-propagation1, BBT only requires model\nforward computation, which can be highly opti-\nmized by acceleration frameworks such as ONNX\nRunetime and TensorRT. In addition, the optimiza-\ntion cost of BBT is decoupled from the scale of the\nmodel. Instead, larger models can be more favor-\nable to BBT due to the lower intrinsic dimensional-\nity (Aghajanyan et al., 2021). Despite its efficiency\nsuperiority and comparable performance to gradi-\nent descent, our pilot experiments (¬ß3) show that\nBBT lacks versatility across tasks and PTMs.\nIn this paper, we present BBTv2, an improved\nversion of BBT, to address these issues. Instead\nof injecting continuous prompt tokens merely in\nthe input layer, BBTv2 prepends prompts to hid-\nden states at every layer of the PTM (termed as\ndeep prompts), incorporating an order of magni-\ntude more parameters to handle more difficult tasks.\nHowever, the increased number of parameters also\nposes a challenge for high-dimensional derivative-\nfree optimization (DFO, Shahriari et al. (2016);\nQian and Yu (2021)). Fortunately, we show that the\nforward computation of modern PTMs can be de-\ncomposed into an additive form w.r.t. hidden states\nof each layer thanks to the residual connections (He\net al., 2016). Hence, the optimization of the deep\nprompts can be decomposed into multiple low-\ndimensional sub-problems, each corresponding to\nthe optimization of prompt at one layer. Based\non this insight, we propose a divide-and-conquer\nalgorithm to alternately optimize prompt at each\nlayer. For the optimization at each layer, we main-\ntain a random linear transformation that projects\nthe prompt parameters into a low-dimensional sub-\nspace and perform DFO in the generated subspace.\nTo generalize BBTv2 to a variety of PTMs, we\ngenerate the random projections using normal dis-\ntributions with PTM-related standard deviations.\nExperimental results show that BBTv2 signif-\n1The computation and storage cost of back-propagation\nis proportional to the forward compute. More widely used\nvariants of gradient descent, such as Adam (Kingma and Ba,\n2015), even require higher compute resources.\nicantly improves BBT on average performance\nacross 7 language understanding tasks. As shown\nin Figure 1, BBTv2 achieves comparable perfor-\nmance to full model tuning and state-of-the-art\nPET methods including Adapter (Houlsby et al.,\n2019), BitFit (Zaken et al., 2022), LoRA (Hu\net al., 2021), and P-Tuning v2 (Liu et al., 2021b)\nwhile with much fewer tunable parameters. Code\nis publicly available at https://github.com/\ntxsun1997/Black-Box-Tuning.\n2 Black-Box Tuning\nBlack-Box Tuning (BBT) (Sun et al., 2022b) is a\nderivative-free framework to drive PTMs for few-\nshot learning. In particular, for a batch of training\ndata (X,Y ), we first convert the textsXwith some\npre-defined templates (e.g., \"It was [MASK]\") into\nÀúX, and the labels Y with a pre-defined map into\nlabel words ÀúY (e.g., \"great\" and \" terrible\"). By\nthis, we can formulate various downstream tasks\ninto a general-purpose (masked) language model-\ning task and utilize the pre-trained (masked) lan-\nguage modeling head to solve them. Assume the\nPTM inference API f takes a continuous prompt\np and a batch of converted texts ÀúX as input, and\noutputs the logits of the tokens of interest (e.g.,\nthe [MASK] token). BBT seeks to find the optimal\nprompt p‚ãÜ = arg minp‚ààPL(f(p, ÀúX),ÀúY), where\nPis the prompt space and Lis some loss function\nsuch as cross entropy. The closed form and the\ngradients of f are not accessible to BBT.\nThe prompt p ‚ààRD usually has tens of thou-\nsands of dimensions, making it infeasible to be\noptimized with derivative-free optimization (DFO)\nalgorithms. Hence, BBT adopts a random projec-\ntion A ‚àà RD√ód to generate a low-dimensional\nsubspace Z‚àà Rd and performs optimization in the\ngenerated subspace, i.e.,\nz‚ãÜ = arg min\nz‚ààZ\nL(f(Az + p0, ÀúX),ÀúY), (1)\nwhere p0 is the initial prompt embedding. If not us-\ning pre-trained prompt embedding, p0 is the word\nembeddings randomly drawn from the vocabulary.\nBBT adopts the Covariance Matrix Adaptation\nEvolution Strategy (CMA-ES) (Hansen and Oster-\nmeier, 2001; Hansen et al., 2003) to optimize Eq.(1)\nand obtain the desired prompt p‚ãÜ = Az‚ãÜ. The ran-\ndom projection A is frozen during optimization.\n3917\nFigure 2: Performance on three entailment tasks. We\nreport F1 score for MRPC and accuracy for SNLI and\nRTE. Without pre-trained prompt embedding, BBTv2\ncan still match the performance of full model tuning on\nentailment tasks under the 16-shot setting.\n3 Pilot Experiments\n3.1 Limitations of BBT Across Tasks\nUnsatisfactory Performance on Entailment\nTasks. As demonstrated by Sun et al. (2022b),\nBBT can outperform model tuning on entailment\ntasks when using pre-trained prompt embedding for\ninitialization. However, pre-trained prompt embed-\nding is not always available for many languages\nand PTMs. Without pre-trained prompt embed-\nding, BBT still lags far behind model tuning on\nentailment tasks. In other words, BBT does not\ncompletely get rid of the dependence on gradients\nto exceed model tuning . In contrast, as depicted\nin Figure 2, BBTv2 can match the performance\nof model tuning on three entailment tasks, namely\nMRPC (Dolan and Brockett, 2005), SNLI (Bow-\nman et al., 2015), and RTE (Wang et al., 2019)\nwithout using pre-trained prompt embedding.\nSlow Convergence on Many-Label Classifica-\ntion Tasks. BBT suffers from slow convergence\nrate when the number of labels becomes large. As\nreported by Sun et al. (2022b), BBT cannot con-\nverge within the budget of 8,000 API calls, which is\nsufficient for common tasks to converge, on DBPe-\ndia (Zhang et al., 2015), a topic classification task\nwith 14 labels. Figure 3 shows the cross entropy\nloss and the training accuracy during optimization.\nCompared with BBT, the proposed BBTv2 signifi-\ncantly accelerates convergence on DBPedia.\n3.2 Limitations of BBT Across PTMs\nOverfitting on Training Data. When switching\nthe backbone model from RoBERTa (Liu et al.,\n2019) to other PTMs, we find that BBT tends\nto overfit training data. As shown in Figure 4,\nthe original BBT with BERTLARGE (Devlin et al.,\nFigure 3: Comparison of the convergence rates of BBT\nand BBTv2 on DBPedia (14 classes).\n(a) BERTLARGE\n(b) BARTLARGE\nFigure 4: Accuracy on the 16-shot training and devel-\nopment set of SST-2 with BERTLARGE and BARTLARGE.\nThe original BBT tends to overfit training data. By us-\ning normal distributions with the standard deviations\ncalculated by Eq.(6) to generate random projections,\nthe modified BBT and BBTv2 can generalize well to\ndevelopment sets.\n2019) and BARTLARGE (Lewis et al., 2020) can\nachieve 100% accuracy on the SST-2 training set,\nbut achieves little improvement on the develop-\nment set. We conjecture that the random projection\nadopted by the original BBT hinders its general-\nization. By generating random projections using\nnormal distributions with model-related standard\ndeviations (¬ß4.2), our modified BBT and BBTv2\nexhibit stronger generalization ability.\n4 BBTv2\n4.1 Deep Black-Box Tuning\nThough BBT has achieved comparable perfor-\nmance to model tuning on simple classification\n3918\nùíõ‚àà‚Ñù!\nRandomProjection(ùë®‚àà‚Ñù\"√ó!)\n%ùíë‚àà‚Ñù\"\nùíëùüé‚àà‚Ñù\" ùíë‚àà‚Ñù\" EmbeddingHidden StatesHidden StatesHidden States‚Ä¶Hidden States(M)LM Head\nInput:x1x2‚ãØxn\nlossPredictions\nLabels\nùíõùüè‚àà‚Ñù! ùíëùüè‚àà‚Ñù\" Embedding\n(M)LM Head\nInput:x1x2‚ãØxn\nloss Predictions\nLabels\nRand Proj Hidden StatesHidden StatesHidden States‚Ä¶Hidden States\nùíëùüê‚àà‚Ñù\"ùíëùüë‚àà‚Ñù\"ùíëùüí‚àà‚Ñù\"ùíëùë≥‚àà‚Ñù\"\nùíõùüê‚àà‚Ñù! Rand Projùíõùüë‚àà‚Ñù! Rand Projùíõùüí‚àà‚Ñù! Rand Projùíõùë≥‚àà‚Ñù! Rand Proj‚Ä¶‚Ä¶\n(a) BBT (b) BBTv2\nBlack-Box Black-Box\n1\n2\n3\n2\nFigure 5: An illustration of (a) BBT (Sun et al., 2022b) and (b) BBTv2.\n is some derivative-free optimizer such\nas CMA-ES. Compared with BBT, BBTv2 has 3 differences: (1) BBT requires pre-trained prompt embedding p0 to\nmatch the performance of model tuning on entailment tasks, and therefore does not completely get rid of gradients.\nIn contrast, BBTv2 requires no prompt pre-training. (2) BBT generates the random projection using a uniform\ndistribution while BBTv2 adopts model-specific normal distributions. (3) Instead of optimizing the prompt merely\nin the input layer, BBTv2 uses a divide-and-conquer algorithm to alternately optimize prompt at each layer.\nAlgorithm 1: DC Algorithm for BBTv2\nRequire: L-layer PTM Inference API f,\nLoss function L,\nBudget of API calls B,\nDerivative-free optimizers {Mj}L\nj=1\n1: Initialize random projections A1,..., AL\n2: Initialize parameters z(0)\n1 ,..., z(0)\nL\n3: Deep prompts p = ‚ü®A1z(0)\n1 ,..., ALz(0)\nL ‚ü©\n4: for i= 1to B/Ldo\n5: for j = 1to Ldo\n6: Evaluate: loss= L(f(p))\n7: Update: z(i)\nj ‚ÜêMj(z(i‚àí1)\nj ,loss)\n8: Replace: pj ‚ÜêAjz(i)\nj\n9: end for\n10: end for\n11: return Optimized deep prompts p\ntasks (e.g., SST-2), our pilot experiments (¬ß3) show\nthat it lacks versatility across tasks and PTMs. As\nan improved variant of BBT, BBTv2 seeks to gen-\neralize BBT across tasks and PTMs.\nInspired by the recent success of deep prompt\ntuning (Li and Liang, 2021; Qin and Eisner, 2021;\nLiu et al., 2021b), we manage to inject continu-\nous prompt tokens to every layer of the PTM and\noptimize them with derivative-free methods. Com-\npared to BBT that optimizes the prompt merely\nin the input layer, BBTv2 has an order of mag-\nnitude more parameters. For a PTM with Llay-\ners, BBTv2 seeks to optimize p = ‚ü®p1,..., pL‚ü©,\nwhere pi ‚ààRD. Hence, the number of param-\neters to be optimized becomes LD. Say we are\nusing RoBERTaLARGE with 24 layers and insert 50\nprompt tokens at each layer, the total number of\nparameters to be optimized is 1.2M, posing a chal-\nlenge of high-dimensional DFO. Instead of sim-\nply extending the dimensionality of the random\nprojection matrix A to RLD√ód 2, we propose a\ndivide-and-conquer (DC) algorithm to handle the\nincreased parameters.\nIn fact, DC has been well explored in prior\nwork (Kandasamy et al., 2015; Mei et al., 2016)\nto cope with high-dimensional DFO problems by\ndecomposing the original high-dimensional prob-\nlem into multiple low-dimensional sub-problems,\nand solving them separately. The key assumption\nof applying DC is that, the objective f can be de-\ncomposed into some additive form. Fortunately,\nthe forward computation of modern PTMs can be\nexpanded into an additive form due to the residual\nconnections (He et al., 2016). For instance, the\nforward computation of a three-layered PTM can\nbe decomposed as\nf(x1) =f3(x3) +x3 (2)\n= f3(x3) +f2(x2) +x2 (3)\n= f3(x3) +f2(x2) +f1(x1) +x1, (4)\nwhere fi is the transformation function of the i-th\nlayer, xi is the input of the i-th layer, and x1 is\nthe input embedding. Thus, optimizing the con-\ntinuous prompts {pi}L\ni=1 attached to the hidden\nstates at every layer {xi}L\ni=1 can be regarded as in-\ndependent sub-problems.3 Since the assumption is\n2In our preliminary experiments, we implement this ver-\nsion but did not obtain positive results.\n3We omit the classification head on the top of the PTM\n3919\nsatisfied, we propose a DC-based algorithm, which\nis described in Algorithm 1, to implement BBTv2.\nThe prompts at different layers are optimized\nalternately from bottom to up. For the optimization\nat each layer, we maintain a specific random pro-\njection Aj and a CMA-ES optimizer Mj. When\nalternating to layer j(Line 6-8 in Algorithm 1), a\nsingle CMA-ES iteration is performed in the same\nfashion as BBT, i.e., a new zj is generated by Mj\nand is then projected to pj using Aj. A graphical\nillustration is shown in Figure 5.\nDuring PTM inference, pj = Ajzj is first added\nwith an initial prompt embedding p0\nj and then con-\ncatenated with the hidden states xj. Thus, ac-\ncording to Eq.(4), the forward computation of a\nL-layered PTM can be viewed as\nf(x1,p1:L) =[A1z1 + p0\n1; x1]\n+\nL‚àë\nj=1\nfj([Ajzj + p0\nj ; xj]), (5)\nwhere [¬∑; ¬∑] means concatenation. Tunable param-\neters are highlighted in color. We set p0\n1 as the\nword embeddings randomly drawn from the PTM\nvocabulary for all tasks. p0\nj (1 < j‚â§L) is the\nhidden states of the prompt tokens at the j-th layer.\n4.2 Revisiting Random Projection\nIn the original BBT, each entry in the random\nprojection A is sampled from a uniform distribu-\ntion (He et al., 2015). In their experiments, using\nnormal distribution N(0,1/d) to generate the ran-\ndom projection results in slow convergence and\ninferior performance. However, we show in pilot\nexperiments that the uniform distribution exhibits\npoor generalization on PTMs other than RoBERTa.\nIn this section, we shed some light on the effect\nof the random projection, and propose to use nor-\nmal distributions with model-related standard devi-\nations to generate random projections. In fact, most\nprior works in high-dimensional DFO (Wang et al.,\n2016; Qian et al., 2016) also adopt normal distribu-\ntions to generate random projections. However,\nthey usually simply use N(0,1) or N(0,1/d),\nboth of which underperform in our scenario.\nTo take a closer look into the effect of the ran-\ndom projection, we draw distribution of the initial\nprompt p that is projected from z by the projec-\ntion matrix A. Here, z is sampled from the normal\nsince it is usually a linear transformation and would not affect\nthe additive decomposition.\nFigure 6: Distributions of RoBERTa word embeddings\nand generated prompt p = Az where A is sampled\nfrom different distributions. When using our designed\nnormal distribution to generate the random projection\nA, the distribution of the projected prompt well matches\nthe shape of the word embedding distribution, leading\nto faster convergence and stronger generalization.\ndistribution maintained by the CMA-ES, which is\ninitially set to N(0,0.5) in BBT. By generating A\nfrom different distributions, we simulate the dis-\ntribution of the projected prompt p and compare\nwith the distribution of RoBERTaLARGE word em-\nbeddings.4 As revealed by Figure 6, when A is\nsampled from the normal distribution used in the\noriginal BBT, i.e.,N(0,1/d), the projected prompt\np cannot cover the range of word embeddings, and\ntherefore suffers from slow convergence. In con-\ntrast, using uniform distribution can cover the range\nof word embeddings, which explains why it per-\nforms well on RoBERTaLARGE.\nThus, to generalize BBT (and BBTv2) across\ndifferent PTMs, we have to take into account the\ndistribution of word embeddings (and hidden states\nfor BBTv2) of the PTM for generating random\nprojections. In particular, we use the normal distri-\nbution with standard deviation as follows,\nœÉA = Œ±ÀÜœÉ‚àö\ndœÉz\n, (6)\nwhere ÀÜœÉ is observed standard deviation of word\nembeddings (or hidden states for BBTv2), œÉz is the\nstandard deviation of the normal distribution main-\ntained by CMA-ES, Œ±is a constant scalar to stretch\nthe distribution. Initially, we set ¬µz = ¬µA = 0\nso that no prior knowledge about the optimization\ndirection is incorporated. The main idea behind the\nabove calculation is to match the distribution (more\nspecifically the variance) between the projected\n4We hypothesis that a high-quality prompt p should lie\nwithin the distribution of word embeddings (hidden states).\n3920\nprompt and word embeddings (or hidden states).\nWhen Œ±= 1, as we can see in Figure 6, the distri-\nbution of the projected prompt can perfectly match\nthe distribution of the word embeddings. Detailed\nderivation of Eq.(6) is provided in Appendix A.\n5 Experiments\n5.1 Datasets and Tasks\nFor comparison, we evaluate BBTv2 on the same\ndatasets as BBT, i.e., SST-2 (Socher et al., 2013),\nYelp (Zhang et al., 2015), AG‚Äôs News (Zhang et al.,\n2015), DBPedia (Zhang et al., 2015), SNLI (Bow-\nman et al., 2015), RTE (Wang et al., 2019), and\nMRPC (Dolan and Brockett, 2005). SST-2 and\nYelp are sentiment analysis tasks, AG‚Äôs News\nand DBPedia are topic classification tasks, SNLI\nand RTE are natural language inference (NLI)\ntasks, and MRPC is a paraphrase task. In addi-\ntion, we include two Chinese tasks, ChnSent5 and\nLCQMC (Liu et al., 2018), for evaluation on CPM-\n2 (Zhang et al., 2021b), a Chinese PTM with‚àº11B\nparameters. ChnSent is a sentiment analysis task\nwhile LCQMC is a question matching task.\nWe follow the same procedure as Zhang et al.\n(2021a); Gu et al. (2021); Sun et al. (2022b) to\nconstruct the true few-shot learning settings (Perez\net al., 2021). In particular, we randomly draw k\nsamples for each class to construct ak-shot training\nset Dtrain, and construct a development set Ddev by\nrandomly selecting another k samples from the\noriginal training set such that |Dtrain|= |Ddev|. We\nuse the original development sets as the test sets.\nFor datasets without development sets, we use the\noriginal test sets. Therefore, in our experiments we\nhave |Dtest|‚â´|D train|= |Ddev|.\n5.2 Baselines\nWe consider two types of methods as our baselines:\ngradient-based methods and gradient-free methods.\nFor gradient-based methods, we compare with\n(1) Model Tuning and state-of-the-art parameter-\nefficient methods including (2) Adapter (Houlsby\net al., 2019), (3) BitFit (Zaken et al., 2022),\n(4) LoRA (Hu et al., 2021), (5) Prompt Tun-\ning (Lester et al., 2021), and (6) P-Tuning v2 (Liu\net al., 2021b). We implement Adapter, BitFit, and\nLoRA using OpenDelta6, and evaluate P-Tuning\nv2 in our experimental settings based on the official\n5https://github.com/SophonPlus/\nChineseNlpCorpus\n6https://github.com/thunlp/OpenDelta\nimplementation7. The results of Model Tuning and\nPrompt Tuning are taken from Sun et al. (2022b).\nFor gradient-free methods, we compare with two\nnon-learning prompt-based methods: (1) Manual\nPrompt and (2) In-Context Learning (Brown\net al., 2020); two feature-based methods: (3)\nFeature-MLP and (4) Feature-BiLSTM, which is\nto train a MLP/BiLSTM classifier on the features\nextracted by the PTM; and (5) BBT (Sun et al.,\n2022b). The results of these gradient-free baselines\nare taken from Sun et al. (2022b). One exception is\nthe performance of BBT on DBPedia. In the origi-\nnal paper, BBT is performed given a larger budget\n(20,000 API calls) on DBPedia for convergence. In\nthis work, we reimplement BBT on DBPedia with\nthe same budget (8,000 API calls) as other tasks\nfor fair comparison.\n5.3 Implementation Details\nBackbones. To compare with BBT, we mainly\nuse RoBERTaLARGE (Liu et al., 2019) as our back-\nbone model. To verify the versatility, we also eval-\nuate on other PTMs including BERTLARGE (Devlin\net al., 2019), GPT-2 LARGE, BARTLARGE (Lewis\net al., 2020), and T5LARGE (Raffel et al., 2020). In\naddition, we also evaluate BBTv2 on a supersized\nChinese PTM, CPM-2 (Zhang et al., 2021b), which\nhas ‚àº11B parameters.\nHyperparameters. Most of the hyperparameters\nremain the same as BBT. We insert 50 continu-\nous prompt tokens at each layer. The subspace\ndimensionality is set to 500. The CMA-ES with\npopulation size of 20 and budget of 8,000 API calls\nis applied to all the tasks. We adopt cross entropy\nas the loss function. For generating random pro-\njections, we use normal distributions with standard\ndeviations calculated by Eq.(6) instead of uniform\ndistributions.\n5.4 Results\nOverall Comparison. As shown in Table 1,\nBBTv2 outperforms BBT and other gradient-free\nmethods on 6/7 tasks. In contrast to BBT, the\nimprovement of BBTv2 mainly comes from DB-\nPedia, which has 14 classes, and hard entailment\ntasks, namely MRPC and SNLI. On simple tasks\nsuch as SST-2 and Yelp, BBT can perform on par\nwith BBTv2. When compared with gradient-based\nmethods, BBTv2 achieves the best result in av-\nerage across the 7 tasks while maintaining much\n7https://github.com/THUDM/P-tuning-v2\n3921\nMethod Tunable SST-2 Yelp P. AG‚Äôs News DBPedia MRPC SNLI RTE Avg.Params acc acc acc acc F1 acc acc\nGradient-Based Methods\nModel Tuning 355M 85.39¬±2.84 91.82¬±0.79 86.36¬±1.85 97.98¬±0.14 77.35¬±5.70 54.64¬±5.29 58.60¬±6.21 78.88\nAdapter 2.4M 83.91¬±2.90 90.99¬±2.86 86.01¬±2.18 97.99¬±0.07 69.20¬±3.58 57.46¬±6.63 48.62¬±4.74 76.31\nBitFit 172K 81.19¬±6.08 88.63¬±6.69 86.83¬±0.62 94.42¬±0.94 66.26¬±6.81 53.42¬±10.63 52.59¬±5.31 74.76\nLoRA 786K 88.49¬±2.90 90.21¬±4.00 87.09¬±0.85 97.86¬±0.17 72.14¬±2.23 61.03¬±8.55 49.22¬±5.12 78.01\nPrompt Tuning 50K 68.23 ¬±3.78 61.02¬±6.65 84.81¬±0.66 87.75¬±1.48 51.61¬±8.67 36.13¬±1.51 54.69¬±3.79 63.46\nP-Tuning v2 1.2M 64.33¬±3.05 92.63¬±1.39 83.46¬±1.01 97.05¬±0.41 68.14¬±3.89 36.89¬±0.79 50.78¬±2.28 70.47\nGradient-Free Methods\nManual Prompt 0 79.82 89.65 76.96 41.33 67.40 31.11 51.62 62.56\nIn-Context Learning 0 79.79 ¬±3.06 85.38¬±3.92 62.21¬±13.46 34.83¬±7.59 45.81¬±6.67 47.11¬±0.63 60.36¬±1.56 59.36\nFeature-MLP 1M 64.80 ¬±1.78 79.20¬±2.26 70.77¬±0.67 87.78¬±0.61 68.40¬±0.86 42.01¬±0.33 53.43¬±1.57 66.63\nFeature-BiLSTM 17M 65.95¬±0.99 74.68¬±0.10 77.28¬±2.83 90.37¬±3.10 71.55¬±7.10 46.02¬±0.38 52.17¬±0.25 68.29\nBBT 500 89.56 ¬±0.25 91.50¬±0.16 81.51¬±0.79 79.99‚ãÜ¬±2.95 61.56¬±4.34 46.58¬±1.33 52.59¬±2.21 71.90\nBBTv2 12K 90.33¬±1.73 92.86¬±0.62 85.28¬±0.49 93.64¬±0.68 77.01¬±4.73 57.27¬±2.27 56.68¬±3.32 79.01\nTable 1: Overall comparison on various language understanding tasks. We report mean and standard deviation of\nperformance over 3 different splits (¬ß5.1). All of the results are obtained with pre-trained RoBERTaLARGE in the\n16-shot (per class) setting. In each track, the best results are highlighted in bold and the second best results are\nmarked with underline. ‚ãÜ We reimplement BBT on DBPedia given a budget of 8,000 API calls for fair comparison.\nSST-2 AG‚Äôs News\n(max seq len: 47)(max seq len: 107)\nBBT BBTv2 BBT BBTv2\nAccuracy 89.4 91.4 82.6 85.5\nTraining Time\nPyTorch (mins)14.8 11.0 28.3 25.0\nONNX (mins) 6.1 4.6 17.7 10.4\nMemory\nUser (MB) 30 143 30 143\nServer (GB) 3.0 3.0 4.6 4.6\nNetwork\nUpload (KB) 6 52 22 68\nDownload (KB)0.25 0.25 1 1\nTable 2: Comparison of BBT and BBTv2 on accuracy,\ntraining time, memory use, and network load.\nfewer tunable parameters. It is worth noting that\nBBTv2, without any gradient-based components\n(e.g., the pre-trained prompt embedding used in\nBBT on entailment tasks (Sun et al., 2022b) or the\nwhite-box prompt optimization required by Diao\net al. (2022)), is the first pure black-box method\nthat matches the performance of full model tuning\non various understanding tasks.\nDetailed Comparison. In Table 2, we compare\nBBTv2 with BBT in other dimensions than accu-\nracy. In addition to the improvement in accuracy,\nBBTv2 also confers faster convergence than BBT.\nFor fair comparison of training time, we perform\nearly stopping if the development accuracy does not\nincrease after 1,000 steps. We report training times\nunder two implementations, PyTorch (Paszke et al.,\n2019) and ONNX Runtime8, on a single NVIDIA\nGTX 3090 GPU. In terms of memory footprint and\nnetwork load, BBTv2 slightly increases the mem-\nory use on the user side and the amount of data to\nbe uploaded.\nBBTv2 Across PTMs. To verify the universal-\nity of BBTv2 across PTMs, we also evaluate on\nBERT, GPT-2, BART, T5, and CPM-2. As shown\nin Table 3, BBTv2 achieves superior performance\nover BBT on PTMs with varying architectures, i.e.,\nencoder-only, decoder-only, and encoder-decoder\nPTMs.9 In addition, we also verify the effective-\nness of BBTv2 on a supersized Chinese PTM,\nCPM-2, which has ‚àº11B parameters. As shown\nin Table 4, when using CPM-2 as the backbone,\nBBTv2 outperforms full model tuning on two Chi-\nnese tasks. The results of Vanilla PT, Hybrid PT,\nand LM Adaption, which are three variants of\nprompt tuning, are taken from Gu et al. (2021).\n5.5 Ablations\nEffect of Subspace Dimensionality. We explore\nthe subspace dimensionality dfrom 10 to 1000 us-\ning both BBT and BBTv2. The population size is\nset to Œª= 4 + 3 log(d) accordingly. Experimental\nresults on SST-2 and SNLI are demonstrated in Fig-\nure 7, from which we observe that: (1) Increasing\nsubspace dimensionality d generally confers im-\nproved performance for both BBT and BBTv2, but\n8https://onnxruntime.ai/\n9We use the normal distribution (Eq.(6)) for BBT for gen-\neralizing to PTMs other than RoBERTa.\n3922\nLM Method SST-2 AG‚Äôs News DBPedia\nEncoder-only PTMs\nBERT BBT 76.26 ¬±2.64 76.67¬±1.12 89.58¬±0.51\nBBTv2 79.32¬±0.29 79.58¬±1.15 93.74¬±0.50\nRoBERTaBBT 89.56 ¬±0.25 81.51¬±0.79 79.99¬±2.95\nBBTv2 90.33¬±1.73 85.28¬±0.49 93.64¬±0.68\nDecoder-only PTMs\nGPT-2 BBT 75.53 ¬±1.98 77.63¬±1.89 77.46¬±0.69\nBBTv2 83.72¬±3.05 79.96¬±0.75 91.36¬±0.73\nEncoder-Decoder PTMs\nBART BBT 77.87 ¬±2.57 77.70¬±2.46 79.64¬±1.55\nBBTv2 89.53¬±2.02 81.30¬±2.58 87.10¬±2.01\nT5 BBT 89.15 ¬±2.01 83.98¬±1.87 92.76¬±0.83\nBBTv2 91.40¬±1.17 85.11¬±1.11 93.36¬±0.80\nTable 3: Comparison of BBT and BBTv2 on the large\nversions of BERT, RoBERTa, GPT-2, BART and T5.\nMethod Tunable ChnSent LCQMC\nParams acc acc\nModel Tuning 11B 86.1 ¬±1.8 58.8¬±1.8\nVanilla PT 410K 62.1 ¬±3.1 51.5¬±3.4\nHybrid PT 410K 79.2 ¬±4.0 54.6¬±2.3\nLM Adaption 410K 74.3 ¬±5.2 51.4¬±2.9\nBBTv2 4.8K 86.4¬±0.8 59.1¬±2.5\nTable 4: Results on two Chinese tasks with CPM-2 as\nthe backbone PTM.\nmarginal effect is also observed when d> 100; (2)\nBBTv2 almost always performs better than BBT\nwith the same subspace dimensionality.\nEffect of Prompt Length. As reported in prior\nwork (Lester et al., 2021; Sun et al., 2022b), prompt\nlength can be a sensitive hyperparameter to model\nperformance. Hence, we explore the prompt length\nfrom 5 to 100 using BBT and BBTv2. As shown\nin Figure 7: (1) The optimal prompt length lies in\nthe range from 5 to 100 and varies across tasks; (2)\nThe effect of prompt length is somehow consistent\nbetween BBT and BBTv2.\nEffect of Model Scale. It has been demonstrated\nthat larger PTMs have a lower intrinsic dimension-\nality (Aghajanyan et al., 2021) and therefore, BBT\nand BBTv2 should be more favorable to larger\nPTMs. To verify this, we conduct experiments\non AG‚Äôs News using T5 (Raffel et al., 2020) with\nvarying scales, i.e., T5 SMALL, T5BASE, T5LARGE,\nand T5XL, corresponding to 60M, 220M, 740M,\nand 3B parameters. As shown in Figure 8, in the\nAG‚Äôs News 16-shot learning setting, the gradient-\nbased counterpart, namely deep prompt tuning\nFigure 7: Ablation results on subspace dimensionality\nand prompt length. We show mean and standard devia-\ntion of performance over 5 different runs.\nFigure 8: The power of scale for black-box tuning with\nT5 on AG‚Äôs News. DPT: deep prompt tuning.\n(DPT, Li and Liang (2021); Liu et al. (2021b);\nQin and Eisner (2021)), performs better than BBT\nand BBTv2 on T5SMALL and T5BASE. When using\nT5LARGE and T5XL, black-box tuning outperforms\nDPT, demonstrating its power of scale.\n6 Related Work\nParameter-Efficient Tuning (PET). PET is to\noptimize only a small portion of parameters while\nkeeping the main body of the model unchanged.\nPET has achieved comparable performance to full\nmodel tuning when training data is sufficient (He\net al., 2021). The tunable parameters can be in-\njected into different positions of the PTM. Houlsby\net al. (2019) insert lightweight adapters to each\nPTM layer; Lester et al. (2021) prepend continu-\nous prompt tokens to the input layer; Li and Liang\n3923\n(2021); Liu et al. (2021b) inject tunable prompt\ntokens to hidden states of every layer; Zaken et al.\n(2022) only optimize the bias-terms in the PTM;\nHu et al. (2021) learn to adapt attention weights via\nlow-rank matrices. Though the number of tunable\nparameters is reduced, back-propagation through\nthe entire model is still required to calculate the gra-\ndients to update the small portion of parameters. To\nthat end, gradient-free methods are also proposed\nto optimize continuous prompt (Sun et al., 2022b;\nDiao et al., 2022) or discrete prompt (Prasad et al.,\n2022; Deng et al., 2022).\nPrompt-Based Learning. Prompt-based learn-\ning is to formulate downstream tasks as a (masked)\nlanguage modeling task, and therefore reduces the\ngap between PTM pre-training and fine-tuning (Liu\net al., 2021a; Sun et al., 2022a). The prompt can\nbe manually designed (Brown et al., 2020; Schick\net al., 2020; Schick and Sch√ºtze, 2021), mined\nfrom corpora (Jiang et al., 2020), generated by gen-\nerative PTMs (Gao et al., 2021), or be constructed\nusing gradient-guided search (Shin et al., 2020). In\nthis work, we also insert manually crafted textual\nprompts into input samples but only optimize the\nprepended continuous prompt tokens.\n7 Conclusion\nIn this work, we present BBTv2, an improved ver-\nsion of BBT (Sun et al., 2022b) with deep prompts\nthat are attached to every layer of the PTM. To\noptimize the high-dimensional prompt parameters,\nwe propose a divide-and-conquer (DC) algorithm\ncombined with random projections to alternately\noptimize the continuous prompt at each layer. Ex-\nperimental results demonstrate that BBTv2, with-\nout any gradient-based component, can achieve\ncomparable performance to state-of-the-art PET\nmethods and full model tuning while maintaining\nmuch fewer tunable parameters.\nLimitations\nWe summarize the limitations of this work as fol-\nlows: (1) BBTv2 adopts a divide-and-conquer algo-\nrithm to alternately optimize prompt at each PTM\nlayer. We use a unique CMA-ES optimizer, which\nhas two hyperparameters ¬µz and œÉz, for the opti-\nmization at each layer. As mentioned previously,\nwe set ¬µz = 0for not incorporating any prior to the\noptimization direction. Therefore, we have totally\nL(number of layers) hyperparameters for optimiza-\ntion, i.e., [œÉ(1)\nz ,...,œÉ (L)\nz ]. For simplicity, we con-\nstrain the œÉz at different layers to be identical, i.e.,\nœÉ(1)\nz = œÉ(2)\nz = ¬∑¬∑¬∑ = œÉ(L)\nz . In addition, Eq.(6) in-\ntroduces another hyperparameter Œ±, which can also\nbe different across different layers. Similarly, we\nconstrain Œ±at all layers to be identical. Hence, in\ncontrast to BBT that has only one hyperparameter\nfor optimization, BBTv2 introduces an additional\nhyperparameter and therefore increases the cost\nfor hyperparameter search. (2) We conduct experi-\nments on 9 language understanding tasks across 4\ntypes (i.e., sentiment analysis, topic classification,\nparaphrasing, and natural language inference) and\n2 languages (i.e., English and Chinese). However,\nthe performance of BBTv2 on a wider range of\nunderstanding tasks and generation tasks is still\nunder-explored. (3) We limit our work to few-shot\nlearning because the training data can be wrapped\ninto a single batch to be fed into the PTM such that\nthe model inference API is a deterministic func-\ntion whose output only depends on the prompt. In\nsuch a low-noise scenario, we can adopt the CMA-\nES to successfully perform optimization. In the\nfull data setting where the training samples are di-\nvided into mini-batches, we need to explore other\nderivative-free optimizers to handle the stochastic\n(noisy) optimization. We leave the investigation of\nadapting BBTv2 to a wider range of tasks and to\nfull data settings as future work, further removing\nthe remaining barriers to a gradient-free future.\nAcknowledgements\nThis work was supported by National Natural\nScience Foundation of China (No. 62106076),\nNatural Science Foundation of Shanghai (No.\n21ZR1420300), and National Natural Science\nFoundation of China (No. 62022027).\nReferences\nArmen Aghajanyan, Sonal Gupta, and Luke Zettle-\nmoyer. 2021. Intrinsic dimensionality explains the\neffectiveness of language model fine-tuning. In Pro-\nceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pages 7319‚Äì\n7328. Association for Computational Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large an-\nnotated corpus for learning natural language infer-\nence. In Proceedings of the 2015 Conference on\n3924\nEmpirical Methods in Natural Language Processing,\nEMNLP 2015, Lisbon, Portugal, September 17-21,\n2015, pages 632‚Äì642. The Association for Computa-\ntional Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\nWang, Han Guo, Tianmin Shu, Meng Song, Eric P.\nXing, and Zhiting Hu. 2022. Rlprompt: Optimizing\ndiscrete text prompts with reinforcement learning.\nCoRR, abs/2205.12548.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171‚Äì4186. Association for Computational\nLinguistics.\nShizhe Diao, Xuechun Li, Yong Lin, Zhichao Huang,\nand Tong Zhang. 2022. Black-box prompt learn-\ning for pre-trained language models. CoRR,\nabs/2201.08531.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei\nChen, Yang Liu, Jie Tang, Juanzi Li, and Maosong\nSun. 2022. Delta tuning: A comprehensive study of\nparameter efficient methods for pre-trained language\nmodels. CoRR, abs/2203.06904.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing, IWP@IJCNLP 2005, Jeju Island,\nKorea, October 2005, 2005. Asian Federation of Nat-\nural Language Processing.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n3816‚Äì3830. Association for Computational Linguis-\ntics.\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.\n2021. PPT: pre-trained prompt tuning for few-shot\nlearning. CoRR, abs/2109.04332.\nNikolaus Hansen, Sibylle D. M√ºller, and Petros\nKoumoutsakos. 2003. Reducing the time complexity\nof the derandomized evolution strategy with covari-\nance matrix adaptation (CMA-ES). Evol. Comput.,\n11(1):1‚Äì18.\nNikolaus Hansen and Andreas Ostermeier. 2001. Com-\npletely derandomized self-adaptation in evolution\nstrategies. Evol. Comput., 9(2):159‚Äì195.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2021. Towards a\nunified view of parameter-efficient transfer learning.\nCoRR, abs/2110.04366.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Delving deep into rectifiers: Surpassing\nhuman-level performance on imagenet classification.\nIn 2015 IEEE International Conference on Computer\nVision, ICCV 2015, Santiago, Chile, December 7-13,\n2015, pages 1026‚Äì1034. IEEE Computer Society.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2016, Las Vegas,\nNV , USA, June 27-30, 2016, pages 770‚Äì778. IEEE\nComputer Society.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research , pages 2790‚Äì2799.\nPMLR.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large\nlanguage models. CoRR, abs/2106.09685.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know. Trans. Assoc. Comput. Linguistics ,\n8:423‚Äì438.\nKirthevasan Kandasamy, Jeff G. Schneider, and Barn-\nab√°s P√≥czos. 2015. High dimensional bayesian op-\ntimisation and bandits via additive models. In Pro-\nceedings of the 32nd International Conference on\nMachine Learning, ICML 2015, Lille, France, 6-11\nJuly 2015, volume 37 of JMLR Workshop and Con-\nference Proceedings, pages 295‚Äì304. JMLR.org.\n3925\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021 , pages 3045‚Äì\n3059. Association for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871‚Äì7880.\nAssociation for Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pages 4582‚Äì\n4597. Association for Computational Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for gpt-3? In Pro-\nceedings of Deep Learning Inside Out: The 3rd Work-\nshop on Knowledge Extraction and Integration for\nDeep Learning Architectures, DeeLIO@ACL 2022,\nDublin, Ireland and Online, May 27, 2022 , pages\n100‚Äì114. Association for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nCoRR, abs/2107.13586.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin\nYang, and Jie Tang. 2021b. P-tuning v2: Prompt\ntuning can be comparable to fine-tuning universally\nacross scales and tasks. CoRR, abs/2110.07602.\nXin Liu, Qingcai Chen, Chong Deng, Huajun Zeng,\nJing Chen, Dongfang Li, and Buzhou Tang. 2018.\nLCQMC: A large-scale chinese question matching\ncorpus. In Proceedings of the 27th International\nConference on Computational Linguistics, COLING\n2018, Santa Fe, New Mexico, USA, August 20-26,\n2018, pages 1952‚Äì1962. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nYi Mei, Mohammad Nabi Omidvar, Xiaodong Li, and\nXin Yao. 2016. A competitive divide-and-conquer\nalgorithm for unconstrained large-scale black-box\noptimization. ACM Trans. Math. Softw., 42(2):13:1‚Äì\n13:24.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas K√∂pf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada, pages\n8024‚Äì8035.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. CoRR,\nabs/2105.11447.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\nBansal. 2022. Grips: Gradient-free, edit-based in-\nstruction search for prompting large language models.\nCoRR, abs/2203.07281.\nHong Qian, Yi-Qi Hu, and Yang Yu. 2016. Derivative-\nfree optimization of high-dimensional non-convex\nfunctions by sequential random embeddings. In Pro-\nceedings of the Twenty-Fifth International Joint Con-\nference on Artificial Intelligence, IJCAI 2016, New\nYork, NY, USA, 9-15 July 2016 , pages 1946‚Äì1952.\nIJCAI/AAAI Press.\nHong Qian and Yang Yu. 2021. Derivative-free rein-\nforcement learning: a review. Frontiers Comput. Sci.,\n15(6):156336.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying lms with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n5203‚Äì5212. Association for Computational Linguis-\ntics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67.\nTimo Schick, Helmut Schmid, and Hinrich Sch√ºtze.\n2020. Automatically identifying words that can serve\nas labels for few-shot text classification. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, COLING 2020, Barcelona,\n3926\nSpain (Online), December 8-13, 2020, pages 5569‚Äì\n5578. International Committee on Computational\nLinguistics.\nTimo Schick and Hinrich Sch√ºtze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, EACL 2021, Online, April 19 - 23, 2021, pages\n255‚Äì269. Association for Computational Linguistics.\nBobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P.\nAdams, and Nando de Freitas. 2016. Taking the\nhuman out of the loop: A review of bayesian opti-\nmization. Proc. IEEE, 104(1):148‚Äì175.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2020, Online, Novem-\nber 16-20, 2020, pages 4222‚Äì4235. Association for\nComputational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2013, 18-21 October 2013, Grand Hyatt\nSeattle, Seattle, Washington, USA, A meeting of SIG-\nDAT, a Special Interest Group of the ACL , pages\n1631‚Äì1642. ACL.\nTianxiang Sun, Xiangyang Liu, Xipeng Qiu, and Xu-\nanjing Huang. 2022a. Paradigm shift in natural lan-\nguage processing. Machine Intelligence Research.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\nHuang, and Xipeng Qiu. 2022b. Black-box tuning for\nlanguage-model-as-a-service. In Proceedings of the\n39th International Conference on Machine Learning,\nICML 2022, Baltimore, Maryland, USA.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nZiyu Wang, Frank Hutter, Masrour Zoghi, David Mathe-\nson, and Nando de Freitas. 2016. Bayesian optimiza-\ntion in a billion dimensions via random embeddings.\nJ. Artif. Intell. Res., 55:361‚Äì387.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.\n2022. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), ACL 2022, Dublin, Ireland, May 22-\n27, 2022, pages 1‚Äì9. Association for Computational\nLinguistics.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q.\nWeinberger, and Yoav Artzi. 2021a. Revisiting few-\nsample BERT fine-tuning. In 9th International Con-\nference on Learning Representations, ICLR 2021, Vir-\ntual Event, Austria, May 3-7, 2021. OpenReview.net.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems 28: Annual Conference on Neural In-\nformation Processing Systems 2015, December 7-12,\n2015, Montreal, Quebec, Canada, pages 649‚Äì657.\nZhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen,\nChaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi,\nJian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng,\nZhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao\nHan, Yang Liu, Xiaoyan Zhu, and Maosong Sun.\n2021b. CPM-2: large-scale cost-effective pre-trained\nlanguage models. CoRR, abs/2106.10715.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference on\nMachine Learning, ICML 2021, 18-24 July 2021, Vir-\ntual Event, volume 139 of Proceedings of Machine\nLearning Research, pages 12697‚Äì12706. PMLR.\n3927\nA Deviation of œÉ for Normal Distribution\nAssume the variable z ‚ààRd is sampled from a\nnormal distribution N(¬µz,œÉz) that is maintained\nby the CMA-ES, the random projection A ‚àà\nRD√ód is generated from another normal distribu-\ntion N(¬µA,œÉA). Considering each entry of the\nprompt pij = ‚àë\nk Aikzkj, the variance is as fol-\nlows,\nV(\nd‚àë\nk=1\nAikzkj) =\nd‚àë\nk=1\nV(Aikzkj)\n+ 2\nd‚àí1‚àë\nk=1\nd‚àë\np=k+1\nCov(Aikzkj,Aipzpj). (7)\nSince A and z, and each entry in A and z are\nindependent random variables and therefore the\nvariance of ‚àë\nk Aikzkj can be simplified as\nV(\nd‚àë\nk=1\nAikzkj) =\nd‚àë\nk=1\nV(Aikzkj) (8)\n= dV(Aikzkj). (9)\nIt is easy to obtain that\nV(Aikzkj) =V(Aik)V(zkj) +V(Aik)(E[zkj])2\n+ V(zkj)(E[Aik])2 (10)\n=œÉ2\nAœÉ2\nz + œÉ2\nA¬µ2\nz + œÉ2\nz¬µ2\nA. (11)\nInitially, we do not incorporate any prior on the\noptimization direction of the embedding (or hidden\nstates) and therefore ¬µA = ¬µz = 0. So we have\nV(Aikzkj) = œÉ2\nAœÉ2\nz. Combined with Eq.(9), the\nvariance of the entries in the randomly projected\nprompt is as follows,\nV(pij) =dœÉ2\nAœÉ2\nz. (12)\nIdeally, we expect the generated prompt to lie\nin a reasonable solution space (e.g., the space of\nthe embedding or hidden states) such that the em-\nbedding (hidden states) added by the prompt can\nstill lie in a reasonable space. A natural idea is\nto match the variance of the generated prompt p\nand the embedding (or hidden states). Formally,\nwe expect that V(pij) = (Œ±ÀÜœÉ)2, where ÀÜœÉ is the\nobserved standard deviation of the embedding (or\nhidden states) and Œ±is a constant scalar that con-\ntrols the range (relative to that of the embedding\nor hidden states) where p falls in. We can obtain a\nrecommendation value of the standard deviation of\nthe random projection, that is\nœÉA = Œ±ÀÜœÉ‚àö\ndœÉz\n. (13)\nIn practice, Œ±and œÉz are hyperparameters.10 It is\nworth noting that the random projections are static\nduring the optimization process and therefore we\nonly need to observe the standard deviation of the\nword embeddings and the hidden states at every\nlayer once at the beginning of the optimization. A\npossible concern is that such observation breaks\nthe black-box and therefore it may be controversial\nto call it \"black-box tuning\". We take a perspective\nof feature-based approaches that views the embed-\ndings and hidden states as features, which are the\noutputs of the model. Thus, we do not really access\nthe inside information of the black-box model.\nB Data Preprocessing\nB.1 Statistics of Datasets\nWe list the statistics of the 7 English tasks and 2\nChinese tasks used in our experiments in Table 5.\nAmong the 9 tasks, 5 are single-sentence classifi-\ncation tasks and 4 are sentence-pair classification\ntasks. The types of the tasks range from sentiment\nanalysis, topic classification, paraphrase, and natu-\nral language inference (NLI).\nCategory Dataset Lang.|Y| |Train| |Test| Type\nsingle-\nsentence\nSST-2 En 2 67k 0.9k sentiment\nYelp P. En 2 560k 38k sentiment\nAG‚Äôs News En 4 120k 7.6k topic\nDBPedia En 14 560k 70k topic\nChnSent Zh 2 6k 1.2k sentiment\nsentence-\npair\nMRPC En 2 3.7k 0.4k paraphrase\nRTE En 2 2.5k 0.3k NLI\nSNLI En 3 549k 9.8k NLI\nLCQMC Zh 2 239k 8.8k NLI\nTable 5: Statistics of datasets used in our experiments.\n|Y| : number of classes. \"En\" means English and \"Zh\"\nmeans Chinese.\nB.2 Templates and Label Words\nFor both BBT and BBTv2, we convert input texts\nX with pre-defined templates into ÀúX, and convert\noutput labels Y into label words ÀúY, such that down-\nstream tasks can be reformulated into a (masked)\nlanguage modeling task and therefore we can reuse\n10The specific hyperparameters to reproduce the results\non each dataset can be found in our code: https://github.\ncom/txsun1997/Black-Box-Tuning.\n3928\nthe pre-trained (masked) language modeling head.\nIn Table 6, we list the input and output formats for\ndifferent PTMs.\nC Implementation Details\nClipping Hidden States In practice, we find that\nthe standard deviation ÀÜœÉof hidden states (especially\nat high layers) in some PTMs can be very large due\nto a few outliers. As a result, our calculated œÉA\n(Eq.(6)) becomes large accordingly, and therefore\nthe generated prompt has a wider range of values\nthan expected. To address this issue, we iteratively\nclip hidden states into the range of ÀÜ¬µ¬±3ÀÜœÉ. We\nperform clipping for 5 rounds and then compute\nthe standard deviation ÀÜœÉof hidden states. Note that\nthe clipping is only performed for statistics but is\nnot applied during model forward compute.\nD Additional Results\nOn Convergence of Normal Distributions Pre-\nviously in Figure 4, we show that using our de-\nsigned normal distribution leads to better gener-\nalization from training data to development data.\nNevertheless, as reported by Sun et al. (2022b),\nusing normal distributions can suffer from slow\nconvergence. Therefore, we compare the conver-\ngence rates using random projections generated\nfrom different distributions. As demonstrated in\nFigure 9: (1) For BBT, the convergence rate of\nusing our designed normal distribution is signifi-\ncantly faster than the normal distribution used in\nthe original BBT, and is comparable to uniform\ndistribution; (2) For BBTv2, using our normal dis-\ntribution converges more stably on both SST-2 and\nAG‚Äôs News. Especially, we observe that using our\nnormal distribution converges faster than uniform\ndistribution on AG‚Äôs News.\nE Estimation of Memory Use and\nNetwork Load\nFor measurement of memory footprint on user side,\nwe use psutil to monitor CPU memory when run-\nning CMA-ES. For memory footprint on service\nside, we use nvidia-smi to monitor GPU memory\nwhen serving PTM inference.\nFor estimation of network load, we measure\nthe amount of data to be uploaded and down-\nloaded. For BBT and BBTv2, there are two\nkinds of data to be uploaded: (1) training sam-\nples, and (2) continuous prompt. A training sam-\nple is comprised of two parts: input_ids and\nFigure 9: Comparison of convergence rates with random\nprojections with different distributions. When using\nour designed normal distribution to generate random\nprojections, both BBT and BBTv2 achieve fast and\nstable convergence.\nattention_mask. We can use the unsigned short\n(representation range: 0‚àº65535, 2 bytes per value)\nfor input_ids and use the bool type (1 byte\nper value) for attention_mask. For continuous\nprompt, which contains hundreds of values for\nBBT or tens of thousands of values for BBTv2,\nwe can use the float type (4 bytes per value) for\nrepresentation. Take SST-2 16-shot split as an ex-\nample, the input_ids and attention_mask are\nin shape of 32 √ó47, where 32 is the batch size\nand 47 is the maximum sequence length, so there\nare ‚àº2.9KB data for input_ids and ‚àº1.5KB data\nfor attention_mask. Assume the subspace di-\nmensionality is 500, we need to upload additional\n‚àº2KB data for prompt if using BBT and ‚àº48KB\ndata if using BBTv2. The data to be downloaded is\nthe output logits of the candidate words, which is a\ndictionary containing |Y| float values. Take SST-\n2 16-shot split as an example, the size of data to be\ndownloaded is 32 √ó2 √ó4bytes = 0.25KB. We as-\nsume the random projections are generated on the\nserver side therefore there is no need to download\nhidden states to compute standard deviations for\nusers.\n3929\nDataset Input Output\nBackbone:RoBERTa, BERT, BART\nSST-2 ‚ü®P‚ü©‚ü®S‚ü©. It was[MASK] great, bad\nYelp P. ‚ü®P‚ü©‚ü®S‚ü©. It was[MASK] great, bad\nAG‚Äôs News‚ü®P‚ü©[MASK]News:‚ü®S‚ü© World, Sports, Business, Tech\nDBPedia ‚ü®P‚ü©[Category:[MASK]]‚ü®S‚ü© Company, Education, Artist, Athlete, Office, Transportation, Building,\nNatural, Village, Animal, Plant, Album, Film, Written\nMRPC ‚ü®P‚ü©‚ü®S1‚ü©?[MASK],‚ü®S2‚ü© Yes, No\nRTE ‚ü®P‚ü©‚ü®S1‚ü©?[MASK],‚ü®S2‚ü© Yes, No\nSNLI ‚ü®P‚ü©‚ü®S1‚ü©?[MASK],‚ü®S2‚ü© Yes, Maybe, No\nBackbone:GPT-2\nSST-2 ‚ü®P‚ü©‚ü®S‚ü©. The sentiment is positive, negative\nAG‚Äôs News‚ü®P‚ü©‚ü®S‚ü©. The news above is about world, sports, business, tech\nDBPedia ‚ü®P‚ü©‚ü®S‚ü©. The text above is about company, education, artist, athlete, office, transportation, building,\nnatural, village, animal, plant, album, film, written\nBackbone:T5\nSST-2 ‚ü®P‚ü©‚ü®S‚ü©. It was[X] [X] positive/negative\nAG‚Äôs News‚ü®P‚ü©[X]News:‚ü®S‚ü© [X]World/Sports/Business/Tech\nDBPedia ‚ü®P‚ü©[Category:[X]]‚ü®S‚ü© [X]Company/Education/Artist/Athlete/Office/Transportation/Building/...\nBackbone:CPM-2\nChnSent ‚ü®P‚ü©‚ü®S‚ü©„ÄÇÊÄª‰πãÂæà[X]„ÄÇ [X]Â•Ω/Â∑Æ\nLCQMC ‚ü®P‚ü©Âà§Êñ≠Ôºö‚ü®S1‚ü©Âíå‚ü®S2‚ü©‰∏§Âè•ËØùÁöÑÊÑèÊÄùÊòØ[X]ÁöÑ„ÄÇ [X]ÁüõÁõæ/Áõ∏‰ºº\nTable 6: Input templates and output label words used in our experiments. ‚ü®P‚ü©is a sequence of continuous prompt\ntokens. ‚ü®S‚ü©is the original input text. For BART, which outputs denoised input in an auto-regressive fashion, we\nonly use the prediction of the masked position such that it follows the same output format as BERT and RoBERTa.\nFor T5 and CPM-2, [X] is a special token similar to [MASK].\n3930",
  "topic": "Initialization",
  "concepts": [
    {
      "name": "Initialization",
      "score": 0.8003697991371155
    },
    {
      "name": "Computer science",
      "score": 0.7929937243461609
    },
    {
      "name": "Gradient descent",
      "score": 0.7026709318161011
    },
    {
      "name": "Computation",
      "score": 0.6364261507987976
    },
    {
      "name": "Software deployment",
      "score": 0.4201648235321045
    },
    {
      "name": "Algorithm",
      "score": 0.3377114534378052
    },
    {
      "name": "Artificial intelligence",
      "score": 0.30413591861724854
    },
    {
      "name": "Artificial neural network",
      "score": 0.08154961466789246
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}