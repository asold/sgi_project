{
    "title": "3D-RETR: End-to-End Single and Multi-View 3D Reconstruction with Transformers",
    "url": "https://openalex.org/W3210001003",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5049176069",
            "name": "Zai Shi",
            "affiliations": [
                "ETH Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A5046234885",
            "name": "Zhao Meng",
            "affiliations": [
                "ETH Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A5041984758",
            "name": "Yiran Xing",
            "affiliations": [
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A5066743613",
            "name": "Yunpu Ma",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5078339613",
            "name": "Roger Wattenhofer",
            "affiliations": [
                "ETH Zurich"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2168676389",
        "https://openalex.org/W2190691619",
        "https://openalex.org/W2099940712",
        "https://openalex.org/W2971278627",
        "https://openalex.org/W2963966978",
        "https://openalex.org/W2010642153",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2950025457",
        "https://openalex.org/W3171516518",
        "https://openalex.org/W3035022492",
        "https://openalex.org/W3125056032",
        "https://openalex.org/W2968370607",
        "https://openalex.org/W2585185777",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2962849139",
        "https://openalex.org/W2990026901",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2603429625",
        "https://openalex.org/W2963012151",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3109585842",
        "https://openalex.org/W2795587607",
        "https://openalex.org/W2115520217",
        "https://openalex.org/W2784125538",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2963111259",
        "https://openalex.org/W2963926543",
        "https://openalex.org/W2963739349",
        "https://openalex.org/W2884561390",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963799213",
        "https://openalex.org/W2903435684",
        "https://openalex.org/W2962885944",
        "https://openalex.org/W3129576130",
        "https://openalex.org/W2536680313",
        "https://openalex.org/W2796911485",
        "https://openalex.org/W2796312544",
        "https://openalex.org/W3034968345",
        "https://openalex.org/W2734349601",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W2798311622",
        "https://openalex.org/W2151103935",
        "https://openalex.org/W3105863736",
        "https://openalex.org/W2895596173",
        "https://openalex.org/W2342277278",
        "https://openalex.org/W1968115598",
        "https://openalex.org/W2738835886",
        "https://openalex.org/W3099201369",
        "https://openalex.org/W2143864104"
    ],
    "abstract": "3D reconstruction aims to reconstruct 3D objects from 2D views. Previous works for 3D reconstruction mainly focus on feature matching between views or using CNNs as backbones. Recently, Transformers have been shown effective in multiple applications of computer vision. However, whether or not Transformers can be used for 3D reconstruction is still unclear. In this paper, we fill this gap by proposing 3D-RETR, which is able to perform end-to-end 3D REconstruction with TRansformers. 3D-RETR first uses a pretrained Transformer to extract visual features from 2D input images. 3D-RETR then uses another Transformer Decoder to obtain the voxel features. A CNN Decoder then takes as input the voxel features to obtain the reconstructed objects. 3D-RETR is capable of 3D reconstruction from a single view or multiple views. Experimental results on two datasets show that 3DRETR reaches state-of-the-art performance on 3D reconstruction. Additional ablation study also demonstrates that 3D-DETR benefits from using Transformers.",
    "full_text": "SHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR 1\n3D-RETR: End-to-End Single and Multi-View\n3D Reconstruction with Transformers\nZai Shi*1\nzaishi@ethz.ch\nZhao Meng*1\nzhmeng@ethz.ch\nYiran Xing2\nyiran.xing@rwth-aachen.de\nYunpu Ma3\ncognitive.yunpu@gmail.com\nRoger Wattenhofer1\nwattenhofer@ethz.ch\n1 ETH Zurich\n2 RWTH Aachen\n3 LMU Munich\nAbstract\n3D reconstruction aims to reconstruct 3D objects from 2D views. Previous works for\n3D reconstruction mainly focus on feature matching between views or using CNNs as back-\nbones. Recently, Transformers have been shown effective in multiple applications of com-\nputer vision. However, whether or not Transformers can be used for 3D reconstruction is\nstill unclear. In this paper, we ﬁll this gap by proposing 3D-RETR, which is able to per-\nform end-to-end 3D REconstruction with TRansformers. 3D-RETR ﬁrst uses a pretrained\nTransformer to extract visual features from 2D input images. 3D-RETR then uses another\nTransformer Decoder to obtain the voxel features. A CNN Decoder then takes as input the\nvoxel features to obtain the reconstructed objects. 3D-RETR is capable of 3D reconstruction\nfrom a single view or multiple views. Experimental results on two datasets show that 3D-\nRETR reaches state-of-the-art performance on 3D reconstruction. Additional ablation study\nalso demonstrates that 3D-DETR beneﬁts from using Transformers.\n1 Introduction\n3D reconstruction focuses on using a single or multiple 2D images of an object to rebuild its\n3D representations. 3D reconstruction has played an important role in various downstream\napplications, including CAD [2], human detection [31], architecture [18], etc. The wide ap-\nplications of 3D reconstruction have motivated researchers to develop numerous methods for\n3D reconstruction. Early works for 3D reconstruction mostly use feature matching between\ndifferent views of an object [8, 11, 14]. However, the performance of such methods largely\ndepends on accurate and consistent margins between different views of objects and are thus\n© 2021. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\n*Equal contribution. arXiv:2110.08861v2  [cs.CV]  16 Nov 2021\n2 SHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR\nvulnerable to rapid changes between views [4, 26, 34]. Additionally, these methods are not\nsuitable for single-view 3D reconstruction, where only one view of an object is available.\nThe advances of deep learning have shed some light on neural network-based approaches\nfor 3D reconstruction [16]. On the one hand, some researchers formulate 3D reconstruction\nas a sequence learning problem and use recurrent neural networks to solve the problem [10,\n21]. On the other hand, other researchers employ the encoder-decoder architecture for 3D\nreconstruction [44, 49]. Furthermore, researchers have also used Generative Adversarial\nNetworks (GANs) for 3D reconstruction [20]. However, these approaches often rely on\nsophisticated pipelines of convolutional neural networks (CNNs), and build models with\nlarge amounts of parameters, which are computationally expensive.\nRecently, Transformers [46] have gained attention from the computer vision commu-\nnity. Transformer-based models have achieved state-of-the-art performance in many down-\nstream applications of computer vision, including image classiﬁcation [12], semantic seg-\nmentation [24], image super-resolution [52], etc. Despite these achievements, whether or\nnot Transformers can be used in 3D reconstruction is still unclear.\nIn this paper, we propose 3D-RETR 1, which is capable of performing end-to-end sin-\ngle and multi-view 3D REconstruction with TRansformers. 3D-RETR uses a pretrained\nTransformer to extract visual features from 2D images. 3D-RETR then obtains the 3D voxel\nfeatures by using another Transformer Decoder. Finally, a CNN Decoder outputs the 3D\nrepresentation from the voxel features. Our contributions in this paper are three-folded:\n1. We propose 3D-RETR for end-to-end single and multi-view 3D reconstruction with\nTransformers. To the best of our knowledge, we are the ﬁrst to use Transformers\nfor end-to-end 3D reconstruction. Experimental results show that 3D-RETR reaches\nstate-of-the-art performance under both synthetic and real-world settings.\n2. We conduct additional ablation studies to understand how each part of 3D-RETR con-\ntributes to the ﬁnal performance. The experimental results show that our choices of\nthe encoder, decoder, and loss are beneﬁcial.\n3. 3D-RETR is efﬁcient compared to previous models. 3D-RETR reaches higher perfor-\nmance than previous models, despite that it uses far fewer parameters.\n2 Related Work\nIn this Section, we brieﬂy review previous works. Section 2.1 gives an overview of previous\nworks on 3D reconstruction. Section 2.2 introduces Transformers.\n2.1 3D reconstruction\n3D reconstruction has been widely used in various downstream applications, including ar-\nchitecture [18], CAD [2], human detection [31], etc. Researchers have mainly focused on\ntwo types of methods for 3D reconstruction. Some researchers use depth cameras such as\nKinect to collect images with depth information [19], which is subsequently processed for\n3D reconstruction. However, such methods require sophisticated hardware and data collec-\ntion procedures and are thus not practical in many scenarios.\n1Code: https://github.com/FomalhautB/3D-RETR\nSHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR 3\nTo mitigate this problem, other researchers have resorted to 3D reconstruction from sin-\ngle or multiple views, where only 2D images are available. Early researchers leverage feature\nmatching between different views for 3D reconstruction with 2D images. For example, [1]\nuses a multi-stage parallel matching algorithm for feature matching, and [8] proposes a cas-\ncade hashing strategy for efﬁcient image matching and 3D reconstruction. Although these\nmethods are useful, their performance degrades when margins between different views are\nlarge, making these methods hard to generalize.\nRecent works mainly focus on neural network-based approaches. Some researchers have\nformulated multi-view 3D construction as a sequence learning problem. For example, [10]\nproposes a 3D recurrent neural network, which takes as input one view at each timestep and\noutputs the reconstructed object representation. Others employ an encoder-decoder architec-\nture by ﬁrst encoding the 2D images into ﬁxed-size vectors, from which a decoder decodes\nthe 3D representations [43, 44, 49]. Furthermore, researchers have also used Generative\nAdversarial Networks (GANs) [20] and 3D-V AEs [27, 36] for 3D reconstruction. However,\nthese neural network-based methods often rely on sophisticated pipelines of different convo-\nlutional neural networks, and are often with models of large amounts of parameters, which\nare computationally expensive.\n2.2 Transformers\nResearchers ﬁrst propose Transformers for applications in natural language processing [46],\nincluding machine translation, language modeling, etc. Transformers use a multi-head self-\nattention mechanism, in which inputs from a speciﬁc time step would attend to the entire\ninput sequence.\nRecently, Transformers have also gained attention from the computer vision community.\nIn image classiﬁcation, Vision Transformer (ViT) [12] reach state-of-the-art performance on\nimage classiﬁcation by feeding images as patches into a Transformer. DeiT [41] achieves\nbetter performance than ViT [12] with much less pretraining data and a smaller parame-\nter size. Transformers are also useful in other computer vision applications. For example,\nDETR [5], consisting of a Transformer Encoder and a Transformer Decoder, has reached\nstate-of-the-art performance on object detection. Other applications of Transformers also\ninclude super-resolution [52], semantic segmentation [24], video understanding [48], etc.\nIn this paper, we propose 3D-RETR for end-to-end single and multi-view 3D reconstruc-\ntion. 3D-RETR consists of a Transformer Encoder, a Transformer Decoder, and another\nCNN Decoder. We show in our experiments (see Section 4) that 3D-RETR reaches state-\nof-the-art performance, while using much fewer parameters than previous models, including\npix2vox++ [50], 3D-R2N2 [10], etc.\n2.3 Differentiable Rendering\nRecently, differentiable rendering methods like SRN [35], DVR [30], NeRF [29], and IDR [53]\nhave become popular. These methods implicitly represent the scene using deep neural net-\nworks and have achieved impressive results. However, these methods need to evaluate their\nneural network thousands of times to extract geometry, which results in a long inference\ntime.\nIn contrast, our method, along with other previous 3D reconstruction methods includ-\ning 3D-R2N2 [10], OGN [40] and pix2vox [49], aims to reconstruct the volume without\n4 SHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR\nFigure 1: An illustration of our 3D-RETR. A Transformer Encoder ﬁrsts extract image fea-\ntures from 2D images. 3D-RETR then obtain the voxel features by using another Trans-\nformer Decoder. A CNN Decoder ﬁnally outputs the 3D object representation.\nrendering the 2D images. 3D-RETR learns the 3D shape prior out of input 2D images and\ngenerates 3D-voxels during the inference time.\n3 Methodology\nFrom a high level, 3D-RETR consists of three main components (see Figure 1): a Trans-\nformer Encoder, a Transformer Decoder, and a CNN Decoder. The Transformer Encoder\ntakes as input the images, which are subsequently encoded into ﬁxed-size image feature\nvectors. Then, the Transformer Decoder obtains voxel features by cross-attending to the im-\nage features. Finally, the CNN Decoder decodes 3D object representations from the voxel\nfeatures. Figure 1 illustrates the architecture of 3D-RETR.\nIn this paper, we have two variants of 3D-RETR: (1) The base model, 3D-RETR-B, has\n163M parameters; (2) The smaller model, 3D-RETR-S, has 11M parameters. We describe\nthe details of these two models in Section 4.\nWe denote the input images of an object fromV different views as x1, . . . ,xV ∈RC×H×W ,\nwhere C = 3 is the RGB channel, and H and W are the height and width of the images,\nrespectively. We denote the reconstructed voxel by Y = {y1, y2, ···, yN3 |yi ∈{0, 1}, 1 ≤i ≤\nN3}, where i is the index to the voxel grids, 0 indicates an empty voxel grid, 1 indicates an\noccupied grid, and N is the resolution of the voxel representation.\n3.1 Transformer Encoder\nA Vision Transformer takes as input image xi by splitting the image into B2 patches. At\neach time step, the corresponding patch is embedded by ﬁrst linearly transformed into a\nﬁxed-size vector, which is then added with positional embeddings. The Transformer takes\nthe embedded patch feature as input and outputs B2 encoded dense image feature vectors.\nFor single-view reconstruction, we keep all the B2 dense image vectors. For multi-view\nreconstruction, at each time step, we take the average across different views, and keep the\naveraged B2 dense vectors.\nIn our implementation, we use the Data-efﬁcient image Transformer (DeiT) [41]. Our\nbase model, 3D-RETR-B, uses the DeiT Base (DeiT-B) as the Transformer Encoder. DeiT-B\nconsists of 12 layers, each of which has 12 heads and 768-dimensional hidden embeddings.\nThe smaller model, 3D-RETR-S, uses the DeiT Tiny (DeiT-Ti) as the Transformer Encoder.\nDeiT-Ti has 12 layers, each of which has 3 heads and 192-dimensional hidden embeddings.\nBoth 3D-RETR-B and 3D-RETR-S have B = 16. We feed all B2 dense image vectors in the\nnext stage into the Transformer Decoder, which we introduce in Section 3.2.\nSHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR 5\nFigure 2: Details of the CNN Decoder in 3D-RETR. The CNN Decoder consists of two\nresidual blocks and three transposed 3D convolutional layers. D is the hidden size of the\nTransformer.\n3.2 Transformer Decoder\nThe Transformer Decoder takes M3 learned positional embeddings as its input and cross-\nattends to the output of the Transformer Encoder. Our Transformer Decoder is similar to\nthat of DETR [5], where the Transformer decodes all input vectors in parallel, instead of\nautoregressively as in the original Transformer [46].\nThe 3D-RETR-B model has a Transformer Decoder of 8 layers, each of which has 12\nheads and 768-dimensional hidden embeddings. For the 3D-RETR-S, we use a Transformer\nDecoder of 6 layers, each of which 3 heads and 192-dimensional hidden embeddings. To\nenable the Transformer Decoder to understand the spatial relations between voxel features,\nwe create M3 positional embeddings for the Transformer Decoder. The positional embed-\ndings are learnable and are updated during training. We useM = 4 for both 3D-RETR-B and\n3D-RETR-S.\n3.3 CNN Decoder\nThe CNN Decoder takes as input the voxel features from the Transformer Decoder and out-\nputs the voxel representation. As the Transformer Encoder and Transformer Decoder al-\nready give rich information, we use a relatively simple architecture for the CNN Decoder.\nThe CNN Decoder ﬁrst stacks the voxel feature vectors into a cube of size M3, and then\nupsample the cube iteratively until the desired resolution is obtained.\nFigure 2 illustrates the architecture of our CNN Decoder. Speciﬁcally, the CNN Decoder\nhas two residual blocks [17], each consisting of four transposed 3D convolutional layers. For\nthe residual blocks, the ﬁrst two convolutional layers have a kernel size of 3, and the last one\nuses a kernel size of 1. In addition, all three layers have 64 channels. For the transposed 3D\nconvolutional layers, all three layers have a kernel size of 4, a stride of 2, a channel size of\n64, and a padding size of 1. We add an additional 1 ×1 convolutional layer at the end of\nthe CNN Decoder to compress the 64 channels into one channel. The model ﬁnally outputs\ncubes of size 323.\n3.4 Loss Function\nWhile previous works on 3D reconstruction mainly use the cross-entropy loss, researchers\nhave also shown that other losses such as Dice and Focal loss are better for optimizing\n6 SHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR\nIoUs [3, 23, 38]. Although these losses are originally proposed for 2D image tasks, they can\nbe easily adapted to 3D tasks.\nThis paper uses Dice loss as the loss function, which is suitable for 3D reconstruction\nas the voxel occupancy is highly unbalanced. Formally, we have the Dice loss for the 3D\nvoxels as follows:\nLDice = 1 − ∑N3\nn=1 pnyn\n∑N3\nn=1 pn + yn\n−∑N3\nn=1(1 −pn)(1 −yn)\n∑N3\nn=1 2 −pn −yn\nwhere pn is the n-th predicted probability of the voxel occupation and yn is the n-th ground-\ntruth voxel.\n3.5 Optimization\nTo train the 3D-RETR. We use the AdamW [25] optimizer with a learning rate of 1 e −4,\nβ1 = 0.9, β2 = 0.999, and a weight decay of 1 e −2. The batch size is set to 16 for all the\nexperiments. We use two RTX Titan GPUs in our experiments. Training takes 1 to 3 days,\ndepending on the exact setting. We use mixed-precision to speed up training.\n4 Experiments\nWe show in this Section our experimental results. We evaluate 3D-RETR on ShapeNet [6]\nand Pix3d [39]. Following previous works [10, 49], we use Intersection of Union (IoU) as\nour evaluation metric.\n4.1 ShapeNet\nShapeNet is a large-scale 3D object dataset consisting of 55 object categories with 51 , 300\n3D models. Following the setting in Pix2V ox [49], we use the same subset of 13 categories\nand about 44, 000 models. The 3D models are pre-processed using Binvox2 with a resolution\nof 323.3 The images are then rendered in the resolution of 137×137 from 24 random views.\nFor single-view 3D reconstruction on ShapeNet, we compare our results with previous\nstate-of-the-art models, including 3D-R2N2 [10], OGN [40], Matryoshka Networks [33], At-\nlasNet [15], Pixel2Mesh [47], OccNet [28], IM-Net [7], AttSets [51], and Pix2V ox++ [50].\nTable 1 shows the results. We can observe that both 3D-RETR-S and 3D-RETR-B outper-\nform all previous models in terms of overall IoU. Additionally, 3D-RETR-S outperforms all\nother baselines in 6 of the 13 categories, while 3D-RETR-B is the best among other baselines\nin 9 of the 13 categories.\nFor the multi-view setting, we take the number of input 2D imagesV ∈{1, 2, 3, 4, 5, 8, 12,\n16, 20}and compare the performance of 3D-RETR with previous state-of-the-art models,\nincluding 3D-R2N2 [10], AttSets [51], and Pix2V ox++ [50]. As we can see in Table 2,\n3D-RETR outperforms all previous works on all different views. Furthermore, Figure 3\nillustrates the relation between the number of views and model performance. One can ob-\nserve that the performance of 3D-RETR increases rapidly compared to other methods as\n2https://www.patrickmin.com/binvox/\n3We asked the authors of previous works for higher resolution datasets. Unfortunately, the authors do not have\naccess to the datasets anymore. Therefore, we cannot compare and evaluate the performance of other resolutions.\nSHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR 7\nCategory 3D-R2N2 OGN Matroyoshka AtlasNet Pixel2Mesh OccNet IM-Net AttSets Pix2V ox++3D-RETR-S 3D-RETR-B\naeroplane 0.513 0.587 0.647 0.493 0.508 0.532 0.702 0.594 0.674 0.696 0.704bench 0.421 0.481 0.577 0.431 0.379 0.597 0.564 0.552 0.608 0.643 0.650cabinet 0.716 0.729 0.776 0.257 0.732 0.674 0.680 0.783 0.799 0.804 0.802car 0.798 0.816 0.850 0.282 0.670 0.671 0.756 0.844 0.858 0.858 0.861chair 0.466 0.483 0.547 0.328 0.484 0.583 0.644 0.559 0.581 0.579 0.592display 0.468 0.502 0.532 0.457 0.582 0.651 0.585 0.565 0.548 0.576 0.574lamp 0.381 0.398 0.408 0.261 0.399 0.474 0.433 0.445 0.457 0.463 0.483riﬂe 0.544 0.593 0.616 0.573 0.468 0.656 0.723 0.601 0.721 0.665 0.668sofa 0.628 0.646 0.681 0.354 0.622 0.669 0.694 0.703 0.725 0.729 0.735speaker 0.662 0.637 0.701 0.296 0.672 0.655 0.683 0.721 0.617 0.719 0.724table 0.513 0.536 0.573 0.301 0.536 0.659 0.621 0.590 0.620 0.615 0.633telephone 0.661 0.702 0.756 0.543 0.762 0.794 0.762 0.743 0.809 0.796 0.781watercraft 0.5130.632 0.591 0.355 0.471 0.579 0.607 0.601 0.603 0.621 0.636\noverall 0.560 0.596 0.635 0.352 0.552 0.626 0.659 0.642* 0.670* 0.674 0.680\nTable 1: Results of single-view 3D reconstruction on the ShapeNet dataset. Bold is the\nbest performance, while Italic is the second best. For overall IoU, we report the mean IoU\nacross all 13 categories. However, for entries with ∗, the overall IoU is NOT the averaged\nIoU across categories. We nevertheless use the original number from Pix2V ox++ [50] and\nAttSets [51]. As a reference, the average IoU across categories for AttSets and Pix2V ox++\nare 0.638 and 0.663, respectively.\nModel 1 view 2 views 3 views 4 views 5 views 8 views 12 views 16 views 20 views\n3D-R2N2 0.560 0.603 0.617 0.625 0.634 0.635 0.636 0.636 0.636AttSets 0.642 0.662 0.670 0.675 0.677 0.685 0.688 0.692 0.693Pix2V ox++* 0.670 0.695 0.704 0.708 0.711 0.715 0.717 0.718 0.719\n3D-RETR-S 0.674 0.695 0.707 0.715 0.719 0.728 0.734 0.737 0.7383D-RETR-B(3 views) 0.6740.707 0.716 0.720 0.723 0.727 0.729 0.730 0.7313D-RETR-B 0.6800.701 0.716 0.725 0.736 0.739 0.747 0.755 0.757\nTable 2: Results of multi-view 3D reconstruction on the ShapeNet dataset. Our smallest\nmodel (3D-RETR-S) already reaches state-of-the-art performance. *: As mentioned in Ta-\nble 1, while all other models report mean IoU across categories, Pix2V ox++ [50] reports\ntheir overall IoU by taking the average across all the examples. For Pix2V ox++, we cannot\ncompute mean IoU across different categories as Pix2V ox++ does not report per-category\nIoU for multi-view reconstruction.\nmore views become available. Additionally, while models like 3D-R2N2 and AttSets gradu-\nally become saturated, our best model, 3D-RETR-B, continues to beneﬁt from more views,\nindicating that 3D-RETR has a higher capacity.\nAs 3D-RETR simply takes the average over different views in the Transformer Encoder,\nwe can train and evaluate 3D-RETR with different numbers of views. To understand how\n3D-RETR performs when the number of views is different during training and evaluation,\nwe conduct additional experiments to train 3D-RETR-B with 3 views and evaluate its perfor-\nmance under different numbers of views. We show the results in Table 2 (See the row of 3D-\nRETR-B (3 views)). Surprisingly, 3D-RETR-B still outperforms previous state-of-the-art\nmodels, even if the number of views during training and evaluation is different. In particular,\nthe model seeing different numbers of views during training and evaluation demonstrates\nthat 3D-RETR is ﬂexible.\n4.2 Pix3D\nDifferent from ShapeNet, in which all examples are synthetic, Pix3D [39] is a dataset of\naligned 3D models and real-world 2D images. Evaluating models on Pix3D gives a better\n8 SHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR\nInput 3D-R2N2 AtlasNet OccNet IM-NET AttSets Pix2V ox++3D-RETR-S 3D-RETR-BGT\nTable 3: Examples of single-view 3D reconstruction from the ShapeNet dataset.\nFigure 3: Model performance with different views. 3D-RETR-B continues to beneﬁt from\nmore views, while baselines including AttSets and Pix2V ox++ become saturated.\n3D-R2N2 DRC Pix3D Pix2V ox++3D-RETR-S 3D-RETR-B\n0.136 0.265 0.282 0.288 0.283 0.290\nTable 4: Results of single-view reconstruction on the Pix3D-Chair dataset.\nunderstanding of the model performance under practical settings. Following the same set-\nting in Pix3D [39], we use the subset consisting of 2,894 untruncated and unoccluded chair\nimages as the test set. Moreover, we follow [37] to synthesize 60 random images for each\nimage in the ShapeNet-Chair category and use these synthesized images as our training set.\nWe compare 3D-RETR with previous state-of-art models, including DRC [42], 3D-\nR2N2 [10], Pix3D [39], and Pix2V ox++ [50]. Table 4 shows the results. 3D-RETR-B\noutperforms all previous models, and 3D-RETR-S reaches comparable performance despite\nthat 3D-RETR-S is much smaller than 3D-RETR-B in terms of parameter size.\nSHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR 9\nInput 3D-RETR-S 3D-RETR-B GT\nTable 5: Example outputs of 3D-RETR on single-view 3D reconstruction from the Pix3D\ndataset. We do not show examples from baseline models as none of the baselines have\nreleased their implementation on Pix3D.\nName Encoder First Decoder Second Decoder Loss IoU\n3D-RETR-B Base Base CNN Dice 0.680\n3D-RETR-S Small Small CNN Dice 0.674\nSetup 1 Base Tiny CNN Dice 0.667\nSetup 2 Base (w/o pre.) Base CNN Dice 0.279\nSetup 3 ResNet-50 Base CNN Dice 0.670\nSetup 4 Base Base VQ-V AE - 0.598\nSetup 5 Base Base CNN CE 0.668\nSetup 6 Base Base MLP Dice 0.658\nTable 6: Ablation Study. We ablate 3D-RETR by using different encoders, decoders, and\nloss functions.\n4.3 Ablation Study\nWe ablate 3D-RETR by using different Transformer Encoders, Transformer Decoders, CNN\nDecoders, and loss functions. Table 6 shows the results of our ablation study. Speciﬁcally,\nwe discuss the following model variants:\n• Setup 1: One might think that the Transformer Decoder is redundant, as the Trans-\nformer Encoder and CNN Decoder are already available. We show that the Trans-\nformer Decoder is necessary by replacing it with a tiny Transformer Decoder. The\ntiny Transformer Decoder has only 1 layer and 1 head, which serves only as a simple\nmapping between the outputs of the Transformer Encoder and the input of the CNN\nDecoder. We can see that the performance decreases from 0.680 to 0.667 after using\nthe tiny Transformer Decoder.\n• Setup 2: Pretraining for the Transformer Encoder is crucial since Transformers large\namounts of data to gain prior knowledge for images. In this setup, we observe that the\nperformance of 3D-RETR decreases signiﬁcantly without pretraining.\n• Setup 3: We show the advantage of the Transformer Encoder over a CNN Encoder by\nreplacing the Transformer Encoder with a pretrained ResNet-50 [17]. After replacing,\nthe model performance decreases from 0.680 to 0.670.\n10 SHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR\nModel 3D-R2N2 OGN Matryoshka Pix2V ox++3D-RETR-S 3D-RETR-B\n#Parameter 36M 12M 46M 98M 11M 163M\nIoU 0.560 0.596 0.635 0.670* 0.674 0.680\nTable 7: Parameter size and performance comparison between 3D-RETR and other baseline\nmodels. 3D-RETR reaches better performance with fewer parameters. *See Table 1 and\nTable 2.\n• Setup 4: Previous studies, including VQ-V AE [45], VQ-GAN [13], and DALL·E [32]\nhave employed a two-stage approach for generating images. We adopt a similar ap-\nproach to 3D-RETR by ﬁrst training a 3D VQ-V AE [45] and replacing the CNN De-\ncoder with the VQ-V AE Decoder. In this setting, the Transformer Decoder decodes\nautoregressively. The training process for this variant is also different from the stan-\ndard 3D-RETR. We ﬁrst generate the discretized features using ground-truth voxels\nand the VQ-V AE Encoder. These discretized features are then used as the ground\ntruth for the Transformer Decoder. During the evaluation, the Transformer Decoder\ngenerates the discretized features one by one and then feeds them into the VQ-V AE\nDecoder. We show in Table 6 that the performance of this two-stage approach is not\nas good as our single-stage setup.\n• Setup 5: To understand how loss functions affect model performance, we train a 3D-\nRETR-B with the standard cross-entropy loss. From Table 6, we can see that replacing\nDice loss with cross-entropy loss results in performance degradation, indicating that\nDice loss is optimal for 3D-RETR.\n• Setup 6: We replace the CNN Decoder with a simple one-layer MLP, so the model\nbecomes a pure Transformer model. The performance is not as good as the original\nmodel with CNN Decoder, but still achieves comparable results.\nWe give further comparisons of parameter size and model performance in Table 7. De-\nspite that our 3D-RETR-S is smaller than previous state-of-the-art models, it still reaches\nbetter performance. Furthermore, 3D-RETR-B outperforms 3D-RETR-S, showing that in-\ncreasing the parameter size is helpful for 3D-RETR.\n5 Conclusion\nDespite that Transformers have been widely used in various applications in computer vi-\nsion [5, 12, 52], whether or not Transformers can be used for single and multi-view 3D\nreconstruction remains unclear. In this paper, we ﬁll in this gap by proposing 3D-RETR,\nwhich is capable of performing end-to-end single and multi-view 3D REconstruction with\nTRansformers. 3D-RETR consists of a Transformer Encoder, a Transformer Decoder, and\na CNN Decoder. Experimental results show that 3D-RETR reaches state-of-the-art perfor-\nmance on 3D reconstruction under both synthetic and real-world settings. 3D-RETR is more\nefﬁcient than previous models [33, 40, 49], as 3D-RETR reaches better performance with\nmuch fewer parameters. In the future, we plan to improve 3D-RETR by using other variants\nof Transformers, including Performer [9], Reformer [22], etc.\nSHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR 11\nReferences\n[1] Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz, and Richard Szeliski.\nBuilding rome in a day. In ICCV, 2009.\n[2] Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X Chang, and\nMatthias Nießner. Scan2cad: Learning cad model alignment in rgb-d scans. In CVPR,\n2019.\n[3] M. Berman, A. Triki, and Matthew B. Blaschko. The lovasz-softmax loss: A tractable\nsurrogate for the optimization of the intersection-over-union measure in neural net-\nworks. CVPR, 2018.\n[4] Dinkar N Bhat and Shree K Nayar. Ordinal measures for image correspondence.\nTPAMI, 1998.\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kir-\nillov, and Sergey Zagoruyko. End-to-end object detection with transformers. InECCV,\n2020.\n[6] Angel X. Chang, Thomas A. Funkhouser, Leonidas J. Guibas, Pat Hanrahan, Qi-Xing\nHuang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong\nXiao, Li Yi, and Fisher Yu. Shapenet: An information-rich 3d model repository.CoRR,\nabs/1512.03012, 2015.\n[7] Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for generative shape modeling.\nCVPR, 2019.\n[8] Jian Cheng, Cong Leng, Jiaxiang Wu, Hainan Cui, and Hanqing Lu. Fast and accurate\nimage matching with cascade hashing for 3d reconstruction. In CVPR, 2014.\n[9] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\nAndreea Gane, Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Re-\nthinking attention with performers. In ICLR, 2021.\n[10] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese.\n3d-r2n2: A uniﬁed approach for single and multi-view 3d object reconstruction. In\nECCV, 2016.\n[11] MWM Gamini Dissanayake, Paul Newman, Steve Clark, Hugh F Durrant-Whyte, and\nMichael Csorba. A solution to the simultaneous localization and map building (slam)\nproblem. IEEE Transactions on robotics and automation, 2001.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021.\n[13] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-\nresolution image synthesis. In CVPR, 2021.\n12 SHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR\n[14] Andreas Geiger, Julius Ziegler, and Christoph Stiller. Stereoscan: Dense 3d reconstruc-\ntion in real-time. In IEEE intelligent vehicles symposium, 2011.\n[15] Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan Russell, and Mathieu\nAubry. AtlasNet: A Papier-Mâché Approach to Learning 3D Surface Generation. In\nCVPR, 2018.\n[16] Xianfeng Han, Hamid Laga, and Mohammed Bennamoun. Image-based 3d object\nreconstruction: State-of-the-art and trends in the deep learning era. TPAMI, 2019.\n[17] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. CVPR, 2016.\n[18] Renato Hermoza and Ivan Sipiran. 3d reconstruction of incomplete archaeological\nobjects using a generative adversarial network. In CGI, 2018.\n[19] Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe,\nPushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison,\net al. Kinectfusion: real-time 3d reconstruction and interaction using a moving depth\ncamera. In UIST, 2011.\n[20] Li Jiang, Shaoshuai Shi, Xiaojuan Qi, and Jiaya Jia. Gal: Geometric adversarial loss\nfor single-view 3d-object reconstruction. In ECCV, 2018.\n[21] Abhishek Kar, Christian Häne, and Jitendra Malik. Learning a multi-view stereo ma-\nchine. In NeurIPS, 2017.\n[22] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient trans-\nformer. In ICLR, 2020.\n[23] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollár. Focal loss\nfor dense object detection. TPAMI, 2020.\n[24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and\nBaining Guo. Swin transformer: Hierarchical vision transformer using shifted win-\ndows. ICCV, 2021.\n[25] Ilya Loshchilov and F. Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n[26] David G Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004.\n[27] Priyanka Mandikal, Navaneet K. L., Mayank Agarwal, and Venkatesh Babu Radhakr-\nishnan. 3d-lmnet: Latent embedding matching for accurate and diverse 3d point cloud\nreconstruction from a single image. In BMVC, 2018.\n[28] Lars M. Mescheder, Michael Oechsle, Michael Niemeyer, S. Nowozin, and Andreas\nGeiger. Occupancy networks: Learning 3d reconstruction in function space. CVPR,\n2019.\n[29] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-\nmamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance ﬁelds for view\nsynthesis. In ECCV, 2020.\nSHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR 13\n[30] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differen-\ntiable volumetric rendering: Learning implicit 3d representations without 3d supervi-\nsion. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),\n2020.\n[31] Alin-Ionut Popa, Mihai Zanﬁr, and Cristian Sminchisescu. Deep multitask architecture\nfor integrated 2d and 3d human sensing. In CVPR, 2017.\n[32] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford,\nMark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila\nand Tong Zhang, editors, ICML, 2021.\n[33] Stephan R. Richter and S. Roth. Matryoshka networks: Predicting 3d geometry via\nnested shape layers. CVPR, 2018.\n[34] Philip Saponaro, Scott Sorensen, Stephen Rhein, Andrew R Mahoney, and Chandra\nKambhamettu. Reconstruction of textureless regions using structure from motion and\nimage-based interpolation. In ICIP, 2014.\n[35] Vincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein. Scene representation\nnetworks: Continuous 3d-structure-aware neural scene representations. In Advances in\nNeural Information Processing Systems, 2019.\n[36] Amir Arsalan Soltani, Haibin Huang, Jiajun Wu, Tejas D. Kulkarni, and Joshua B.\nTenenbaum. Synthesizing 3d shapes via modeling multi-view depth maps and silhou-\nettes with deep generative networks. In CVPR, 2017.\n[37] Hao Su, C. Qi, Yangyan Li, and L. Guibas. Render for cnn: Viewpoint estimation in\nimages using cnns trained with rendered 3d model views. ICCV, 2015.\n[38] C. Sudre, Wenqi Li, Tom Kamiel Magda Vercauteren, S. Ourselin, and M. Jorge Car-\ndoso. Generalised dice overlap as a deep learning loss function for highly unbalanced\nsegmentations. DLMIA, 2017.\n[39] Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan\nXue, J. Tenenbaum, and W. Freeman. Pix3d: Dataset and methods for single-image 3d\nshape modeling. CVPR, 2018.\n[40] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree generating networks: Efﬁcient\nconvolutional architectures for high-resolution 3d outputs. In ICCV, 2017.\n[41] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablay-\nrolles, and Hervé Jégou. Training data-efﬁcient image transformers & distillation\nthrough attention. In Marina Meila and Tong Zhang, editors, ICML, 2021.\n[42] Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, and Jitendra Malik. Multi-view\nsupervision for single-view reconstruction via differentiable ray consistency. InCVPR,\n2017.\n[43] Shubham Tulsiani, Tinghui Zhou, Alexei A Efros, and Jitendra Malik. Multi-view\nsupervision for single-view reconstruction via differentiable ray consistency. InCVPR,\n2017.\n14 SHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR\n[44] Shubham Tulsiani, Alexei A Efros, and Jitendra Malik. Multi-view consistency as\nsupervisory signal for learning shape and pose prediction. In CVPR, 2018.\n[45] Aäron van den Oord, Oriol Vinyals, and K. Kavukcuoglu. Neural discrete representa-\ntion learning. In NeurIPS, 2017.\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS,\n2017.\n[47] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, W. Liu, and Yu-Gang Jiang.\nPixel2mesh: Generating 3d mesh models from single rgb images. In ECCV, 2018.\n[48] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao\nShen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In\nCVPR, 2021.\n[49] Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, and Shengping Zhang.\nPix2vox: Context-aware 3d reconstruction from single and multi-view images. In\nCVPR, 2019.\n[50] Haozhe Xie, Hongxun Yao, Shengping Zhang, Shangchen Zhou, and Wenxiu Sun.\nPix2vox++: Multi-scale context-aware 3d object reconstruction from single and multi-\nple images. IJCV, 2020.\n[51] Bo Yang, Sen Wang, A. Markham, and Niki Trigoni. Robust attentional aggregation of\ndeep feature sets for multi-view 3d reconstruction. IJCV, 2019.\n[52] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture\ntransformer network for image super-resolution. In CVPR, 2020.\n[53] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and\nYaron Lipman. Multiview neural surface reconstruction by disentangling geometry and\nappearance. NeurIPS, 2020.\nSHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR 15\nFigure 4: 3D-RETR with VQ-V AE. This corresponds to Setup 4 of our ablation study.\nA 3D-RETR with VQ-V AE\nWe describe in detail the VQ-V AE setting in our ablation study of Section 4.3 (see Figure 4).\nWe train 3D-RETR with VQ-V AE in two separate stages.\nIn the ﬁrst stage, we pretrain a VQ-V AE with a codebook size of 2048, where each\ncodebook vector has 512 dimensions. The VQ-V AE Encoder and Decoder have three layers,\nrespectively. For the VQ-V AE Decoder, we use the same residual blocks as in the CNN\nDecoder. The VQ-V AE Encoder encodes the 32 ×32 ×32 voxel into a discrete sequence\nof length 64, where each element in the sequence is an integer between 0 and 2047. The\nVQ-V AE is trained with cross-entropy loss. The reconstruction IoU is about 0.885.\nIn the second stage, for every input image x and its correspondent ground-truth voxel\nY, we ﬁrst generate a discrete sequence D using the pretrained VQ-V AE Encoder. Then,\nthe Transformer Encoder generates the hidden representation for the input image x, and\nthe Transformer Decoder uses the output of the Transformer Encoder to generate another\ndiscrete sequence D′. To generate D′, we use a linear layer with softmax at the output of the\nTransformer Decoder. We use the sequence D as the ground truth and train the Transformer\nEncoder and Decoder with cross-entropy loss to generate D′, which should be as close as\npossible to D.\nB Additional Examples\nWe show more examples of the ShapeNet dataset and the Pix3D dataset from our 3D-RETR-\nB model. Table 8 shows additional examples of the Pix3D dataset. Table 9 shows examples\nfrom the ShapeNet dataset with different numbers of views as inputs. We can see a clear\nquality improvement when more views become available.\n16 SHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR\nInput\nPrediction\nGT\nTable 8: Examples from the Pix3D dataset. All predictions are generated by 3D-RETR-B.\nFigure 5: Models performance with different views.\nC Model Performance with Different Views\nIn Table 2 of the paper, we show that 3D-RETR trained on three views still outperforms pre-\nvious state-of-the-art results even when evaluated under different numbers of input views. In\nTable 10 and Figure 5, we give additional results on training and evaluating under different\nnumbers of views. We can observe that more views during evaluation can boost model per-\nformance. Another observation is that models trained with more views are not necessarily\nbetter than models trained with fewer views, especially when the number of views available\nduring evaluation is far fewer than the number of available views during training. For ex-\nample, when only one view is available, the model trained with one view reaches an IoU of\n0.680, while the model trained with 20 views only reaches an IoU of 0.534.\nSHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR 17\nInput\n1 view\n2 views\n3 views\n4 views\n5 views\n8 views\n12 views\n16 views\n20 views\nGT\nTable 9: Examples from the ShapeNet dataset. All predictions are generated by 3D-RETR-B.\n18 SHI, MENG, XING, MA AND WATTENHOFER: 3D-RETR\nTrain\nEval 1 view 2 views 3 views 4 views 5 views 8 views 12 views 16 views 20 views\n1 view 0.680 0.688 0.688 0.687 0.687 0.686 0.686 0.685 0.684\n2 views 0.676 0.701 0.709 0.711 0.713 0.716 0.718 0.719 0.720\n3 views 0.674 0.707 0.716 0.720 0.723 0.729 0.729 0.730 0.731\n4 views 0.674 0.711 0.721 0.725 0.728 0.731 0.734 0.735 0.736\n5 views 0.667 0.712 0.724 0.729 0.734 0.738 0.741 0.743 0.743\n8 views 0.634 0.699 0.719 0.726 0.732 0.739 0.742 0.745 0.746\n12 views 0.606 0.691 0.714 0.724 0.733 0.742 0.747 0.750 0.751\n16 views 0.588 0.687 0.713 0.726 0.735 0.745 0.752 0.755 0.757\n20 views 0.534 0.657 0.694 0.712 0.727 0.742 0.750 0.755 0.757\nTable 10: Model performance with different views during training and evaluation. Bold\nindicates the best performance in an evaluation setting."
}