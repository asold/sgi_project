{
  "title": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers",
  "url": "https://openalex.org/W4385574383",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3216541807",
      "name": "Michael Hassid",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A2009379665",
      "name": "Hao Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5044655830",
      "name": "Daniel Rotem",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A2760024017",
      "name": "Jungo Kasai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3048621856",
      "name": "Ivan Montero",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2183947846",
      "name": "Noah A. Smith",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098207400",
      "name": "Roy Schwartz",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2991265431",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3022810513",
    "https://openalex.org/W4285175926",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W3132041002",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W4287367446",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3202070718",
    "https://openalex.org/W4206281850",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4226084637",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4309087688",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W4287183852",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4206688402",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4225525539",
    "https://openalex.org/W4226261765",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3034296505"
  ],
  "abstract": "The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones—the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance—an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1403–1416\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nHow Much Does Attention Actually Attend?\nQuestioning the Importance of Attention in Pretrained Transformers\nMichael Hassid♡ Hao Peng♢∗ Daniel Rotem♡ Jungo Kasai♠ Ivan Montero⋆∗\nNoah A. Smith♠♢ Roy Schwartz♡\n♡School of Computer Science & Engineering, Hebrew University of Jerusalem\n♢Allen Institute for Artificial Intelligence ⋆Apple, Inc.\n♠Paul G. Allen School of Computer Science & Engineering, University of Washington\n{michael.hassid,daniel.rotem,roy.schwartz1}@mail.huji.ac.il\nhaop@allenai.org {jkasai,nasmith}@cs.washington.edu ivamon@apple.com\nAbstract\nThe attention mechanism is considered the\nbackbone of the widely-used Transformer ar-\nchitecture. It contextualizes the input by com-\nputing input-specific attention matrices. We\nfind that this mechanism, while powerful and\nelegant, is not as important as typically thought\nfor pretrained language models. We introduce\nPAPA,1 a new probing method that replaces\nthe input-dependent attention matrices with\nconstant ones—the average attention weights\nover multiple inputs. We use PAPA to ana-\nlyze several established pretrained Transform-\ners on six downstream tasks. We find that\nwithout any input-dependent attention, all mod-\nels achieve competitive performance—an av-\nerage relative drop of only 8% from the prob-\ning baseline. Further, little or no performance\ndrop is observed when replacing half of the\ninput-dependent attention matrices with con-\nstant (input-independent) ones. Interestingly,\nwe show that better-performing models lose\nmore from applying our method than weaker\nmodels, suggesting that the utilization of the\ninput-dependent attention mechanism might\nbe a factor in their success. Our results mo-\ntivate research on simpler alternatives to input-\ndependent attention, as well as on methods\nfor better utilization of this mechanism in the\nTransformer architecture.\n1 Introduction\nPretrained Transformer (Vaswani et al., 2017) mod-\nels have enabled great progress in NLP in recent\nyears (Devlin et al., 2019; Liu et al., 2019b; Raffel\net al., 2020; Brown et al., 2020; Chowdhery et al.,\n2022). A common belief is that the backbone of the\nTransformer model—and pretrained language mod-\nels (PLMs) in particular—is the attention mech-\nanism, which applies multiple attention heads in\n∗This work was done while Hao Peng and Ivan Montero\nwere at the University of Washington.\n1PAPA stands forProbing Analysis for PLMs’ Attention.\n“ This is a sentence ”\n “ This is a sentence ”\nFigure 1: Illustration of the PAPA method, which mea-\nsures how much PLMs use the attention mechanism.\nPAPA replaces the input-dependent attention matrices\n(left) with constant ones (right). We then measure the\nperformance gap between the two. Moderate drop\nindicates minor reliance on the attention mechanism.\nparallel, each generating an input-dependent atten-\ntion weight matrix.\nInterestingly, recent work found that atten-\ntion patterns tend to focus on constant (input-\nindependent) positions (Clark et al., 2019; V oita\net al., 2019), while other works showed that it is\npossible to pretrain language models where the\nattention matrices are replaced with constant matri-\nces without major loss in performance (Liu et al.,\n2021; Lee-Thorp et al., 2021; Hua et al., 2022). A\nnatural question that follows is how much standard\nPLMs, pretrained with the attention mechanism, ac-\ntually rely on this input-dependent property. This\npaper shows that they are less dependent on it than\npreviously thought.\nWe present a new analysis method for PLMs:\nProbing Analysis for PLMs’ Attention (PAPA). For\neach attention head h, PAPA replaces the attention\nmatrix with a constant one: a simple average of the\nattention matrices for hcomputed on some unla-\nbeled corpus. Replacing all attention matrices with\nsuch constant matrices results in an attention-free\nvariant of the original PLM (See Fig. 1). We then\ncompute, for some downstream tasks, the probing\nperformance gap between an original model and\nits attention-free variant. This provides a tool to\n1403\nquantify the models’ reliance on attention. Intu-\nitively, a larger performance drop indicates that the\nmodel relies more on the input-dependent attention\nmechanism.\nWe use PAPA to study three established pre-\ntrained Transformers: BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019b), and DeBERTa (He\net al., 2021), each with BASE- and LARGE-sized\nversions. We evaluate these models on six diverse\nbenchmarks, spanning text classification and struc-\ntured prediction tasks.\nOur results suggest that attention is not as im-\nportant to pretrained Transformers as previously\nthought. First, the performance of the attention-\nfree variants is comparable to original models: an\naverage relative drop of only 8%. Second, replac-\ning half of the attention matrices with constant ones\nhas little effect on performance, and in some cases\nmay even lead to performance improvements. In-\nterestingly, our results hint that better models use\ntheir attention capability more than weaker ones;\nwhen comparing the effect of PAPA on different\nmodels, we find that the better the model’s original\nperformance is, the more it suffers from replacing\nthe attention matrices with constant ones. This sug-\ngests a potential explanation for the source of the\nempirical superiority of some models over others—\nthey make better use of the attention mechanism.\nThis work grants a better understanding of the\nattention mechanism in pretrained Transformers. It\nalso motivates further research on simpler or more\nefficient Transformer models, either for pretraining\n(Lee-Thorp et al., 2021; Liu et al., 2021; Hua et al.,\n2022) or potentially as an adaptation of existing\npretrained models (Peng et al., 2020a, 2022; Kasai\net al., 2021). It also provides a potential path to im-\nprove the Transformer architecture—by designing\ninductive bias mechanisms for better utilization of\nattention (Peng et al., 2020b; Wang et al., 2022).\nFinally, our work may contribute to the “at-\ntention as explanation” debate (Jain and Wallace,\n2019; Serrano and Smith, 2019; Wiegreffe and Pin-\nter, 2019; Bibal et al., 2022). By showing that\nsome PLMs can perform reasonably well with con-\nstant matrices, we suggest that explanations arising\nfrom the attention matrices might not be crucial for\nmodels’ success.\nWe summarize our main contributions. (1) We\npresent a novel probing method—PAPA—which\nquantifies the reliance of a given PLM on its at-\ntention mechanism by “disabling” that mechanism\nfor this PLM. (2) We apply PAPA to six leading\nPLMs, and find that our manipulation leads to mod-\nest performance drops on average, which hints that\nattention might not be as important as thought. (3)\nWe show that better-performing PLMs tend to suf-\nfer more from our manipulation, which suggests\nthat the input-dependent attention is a factor in\ntheir success. (4) Finally, we release our code and\nexperimental results.2\n2 Background: Attention in Transformers\nTransformers consist of interleaving attention and\nfeed-forward layers. In this work, we focus on\nTransformer encoder models, such as BERT, which\nare commonly used in many NLP applications.\nThe (multi headed) self-attention module takes\nas input a matrix X ∈Rn×d and produces a matrix\nXout ∈ Rn×d, where n denotes the number of\ninput tokens, each represented as a d-dimensional\nvector. Each attention layer consists of H heads,\nand each head h∈{1,...,H }has three learnable\nmatrices: Wh\nQ,Wh\nK,Wh\nV ∈Rd×d′\n.3 Multiplying\nthem with the input X results in: Qh,Kh,V h ∈\nRn×d′\n(the queries, keys and values, respectively).\nThe queries and the keys compute a n×natten-\ntion weight matrix Ah between all pairs of tokens\nas softmax-normalized dot products:4\nAh = softmax\n(︂(︁\nX·Wh\nQ\n)︁\n⏞ ⏟⏟ ⏞\nQh\n·\n(︁\nX·Wh\nK⏞ ⏟⏟ ⏞\nKh\n)︁⊤)︂\n∈Rn×n\n(1)\nwhere the softmax operation is taken row-wise. The\nvalue matrix Vh is then left-multiplied by the at-\ntention matrix Ah to generate the attention head\noutput.\nImportantly, the attention matrix Ah is input-\ndependent, i.e., defined by the input X. This\nproperty is considered to be the backbone of the\nattention mechanism (Vaswani et al., 2017).\nAn intriguing question is the extent to which\nPLMs actually rely on the attention mechanism. In\nthe following, we study this question by replacing\nthe attention matrices of PLMs with constant ma-\ntrices. We hypothesize that if models make heavy\nuse of attention, we will see a large drop in perfor-\nmance when preventing the model from using it.\nAs shown below, such performance drop is often\nnot observed.\n2https://github.com/schwartz-lab-NLP/papa\n3d′is the head-dimension, and usually defined as d′= d\nH .\n4Some attention variants (e.g., He et al., 2021) incorporate\npositional information as part of the calculation of Ah.\n1404\n3 The PAPA Method\nWe present PAPA, a probing method for quanti-\nfying the extent to which pretrained Transformer\nmodels use the attention mechanism. PAPA works\nby replacing the Transformer attention weights\nwith constant matrices, computed by averaging the\nvalues of the attention matrices over unlabeled in-\nputs (Sec. 3.1). PAPA also allows for replacing any\nsubset (not just all) of the attention matrices. We\npropose a method for selecting which heads to re-\nplace (Sec. 3.2). The resulting model is then probed\nagainst different downstream tasks (Sec. 3.3). The\nperformance difference between the original and\nthe new models can be seen as an indication of how\nmuch the model uses its attention mechanism.\n3.1 Generating Constant Matrices\nTo estimate how much a pretrained Transformer m\nuses the attention mechanism, we replace its atten-\ntion matrices with a set of constant ones, one for\neach head. To do so, PAPA constructs, for a given\nhead h,5 a constant matrix Ch by averaging the at-\ntention matrix Ah over a corpus of raw text. More\nspecifically, given a corpus D = {e1,...,e |D|},\nCh is defined as:\nCh = 1\n|D|\n|D|∑︂\ni=1\nAh\ni , (2)\nwhere Ah\ni is the input-dependent attention matrix\nthat h constructs while processing ei. We note\nthat the average is taken entry-wise, and only over\nnon-padded entries (padding tokens are ingored).\nWe emphasize that the construction process of\nCh matrices requires no labels. In Sec. 5.2 we com-\npare our method of constructing constant matrices\nfrom unlabeled data to other alternatives that either\nuse no data at all, or use labeled data.\n3.2 Replacing a Subset of the Heads\nDifferent attention heads may have different levels\nof dependence on attention. We therefore study\nthe effect of replacing a subset of the heads, and\nkeeping the rest intact. To do so, we would like\nto estimate the reliance of each head on the input-\ndependent attention, which would allow replacing\nonly the heads that are least input-dependent for\nthe model.\n5We do so for all layers in parallel. Layer indices omitted\nfor simplicity.\nTo estimate this dependence, we introduce a new\nweighting parameter λh ∈ (0,1), initialized as\nλh = 0.5, for each attention head h.6 λh is a\nlearned weighting of the two matrices: the attention\nmatrix Ah and the constant matrix Ch from (1) and\n(2) respectively. For each input ei, a new matrix\nBh is constructed as:\nBh\ni = λh ·Ah\ni + (1−λh) ·Ch (3)\nWe interpret a smaller λh as an indication of hless\ndepending on the attention mechanism.\nWe then train the probing classifier (Sec. 3.3)\nalong with the additional λh parameters. We use\nthe learned λhs to decide which heads should be\nreplaced with constant matrices, by only replacing\nthe k% attention heads with the smallest λh values\nfor some hyperparameter k.7 Importantly, this pro-\ncedure is only used as a pre-processing step; our\nexperiments are trained and evaluated without it,\nwhere k% of each model’s heads are replaced, and\n(1 −k%) remain unchanged.\n3.3 Probing\nOur goal is to evaluate how much attention a given\nPLM uses. Therefore, we want to avoid finetuning\nit for a specific downstream task, as this would lead\nto changing all of its weights, and arguably answer\na different question (e.g., how much attention does\na task-finetuned PLM use). Instead, we use a prob-\ning approach (Liu et al., 2019a; Belinkov, 2022) by\nfreezing the model and adding a classifier on top.\nOur classifier calculates for each layer a\nweighted (learned, non-attentive) representation of\nthe different token representations. It then concate-\nnates the different layer weighted representations,\nand applies a 2-layer MLP. For structured predic-\ntion tasks (e.g., NER and POS), where a represen-\ntation for each token is needed, we concatenate for\neach token the representations across layers, and\napply a 2-layer MLP.\nWhen PAPA is applied to some input, we replace\nthe attention matrices Ah with the corresponding\nconstant matrices Ch.8 We then compare the down-\nstream performance of the original model mwith\nthe new model m′. The larger the performance gap\nbetween mand m′, the higher m’s dependence on\nthe attention mechanism.\n6λh is the output of a sigmoid over a learned parameter.\n7In Sec. 5.4 we show that this head selection method out-\nperforms other alternatives.\n8To minimize model changes, we also mask theCh entries\ncorresponding to padded tokens, and normalize the matrix\n(row-wise), as in a regular Transformer.\n1405\n3.4 Method Discussion\nContextualization with PAPA PAPA replaces\nthe attention matrices with constant ones, which\nresults in an attention-free model. Importantly, un-\nlike a feed-forward network, the representations\ncomputed via the resulting model are still contextu-\nalized, i.e., the representation of each word depends\non the representations of all other words. The key\ndifference between the standard Transformer model\nand our attention-free model is that in the former\nthe contextualization varies by the input, and for\nthe latter it remains fixed for all inputs.\nPotential Computational Gains The replace-\nment of the attention matrix with a constant one\nmotivates the search for efficient attention alterna-\ntives. Using constant matrices is indeed more effi-\ncient, reducing the attention head time complexity\nfrom 2n2d′+ 3nd′2 to n2d′+ nd′2,9 which shows\npotential for efficiency improvement.\nSeveral works used various approaches for re-\nplacing the attention mechanism with constant ones\nduring the pretraining phase (Lee-Thorp et al.,\n2021; Liu et al., 2021; Hua et al., 2022), and indeed\nsome of them showed high computational gains.\nOur work tackles a different question—how much\ndo PLMs, which trained with the attention mecha-\nnism, actually use it. Thus, unlike the approaches\nabove, we choose to make minimal changes to the\noriginal models. Nonetheless, our results further\nmotivate the search for efficient attention variants.\n4 Experiments\nWe now turn to use PAPA to study the attention\nusage of various PLMs.\n4.1 Experimental Setup\nOur experiments are conducted over both text\nclassification and structured prediction tasks, all\nin English. For the former we use four diverse\nbenchmarks from the GLUE benchmark (Wang\net al., 2019): MNLI (Williams et al., 2018), SST-2\n(Socher et al., 2013), MRPC (Dolan and Brock-\nett, 2005), and CoLA (Warstadt et al., 2019). For\nthe latter we use named entity recognition (NER)\nand part of speech tagging (POS) from the CoNLL-\n2003 shared task (Tjong Kim Sang and De Meulder,\n2003).10 We use the standard train/validation splits,\n9nis the sequence length and d′is head-dimension.\n10We report accuracy for SST2 and MNLI, F1 score for\nMRPC, NER and POS, and MCC for CoLA.\nand report validation results in all cases.11\nWe use three widely-used pretrained Trans-\nformer encoder models: BERT (Devlin et al.,\n2019), RoBERTa (Liu et al., 2019b), and DeBERTa\n(He et al., 2021). We use both BASE (12 layers,\n12 heads in each layer) and LARGE (24 layers,\n16 heads per layer) versions. For each model and\neach task, we generate the constant matrices with\nthe given (unlabeled) training set of that task. In\nSec. 5.3 we show that PAPA is not very sensitive\nto the specific training set being used.\nAll experiments are done with three different\nrandom seeds, average result is reported (95% con-\nfidence intervals are shown). Pre-processing and\nadditional experimental details are described in\nApp. A and B, respectively.\n4.2 Probing Results\nThe results of the BASE and LARGE models are\npresented in Fig. 2a and 2b, respectively. We mea-\nsure the performance of each model on each task us-\ning {1,1\n2 ,1\n8 , 1\n16 ,0}of the model’s input-dependent\nattention matrices and replacing the rest with con-\nstant ones.\nWe first consider the original, fully-attentive,\nmodels, and find that performance decreases in the\norder of DeBERTa, RoBERTa, and BERT. This\norder is roughly maintained across tasks and model\nsizes, which conforms with previous results of fine-\ntuning these PLMs (He et al., 2021). This suggests\nthat the model ranking of our probing method is\nconsistent with the standard fine-tuning setup.\nWe note that the trends across tasks and models\nare similar; hence we discuss them all together in\nthe following (up to specific exceptions).\nReplacing all attention matrices with constant\nones incurs a moderate performance dropAs\nshown in Fig. 2, applying PAPA on all attention\nheads leads to an 8% relative performance drop on\naverage and not greater than 20% from the origi-\nnal model.12 This result suggests that pretrained\nmodels only moderately rely on the attention mech-\nanism.\nHalf of the attention matrices can be replaced\nwithout loss in performance We note that in\nalmost all cases replacing half of the models’ atten-\ntion matrices leads to no major drop in performance.\nIn fact, in some cases, performance even improves\n11For MNLI, we report the mismatched validation split.\n12For the MRPC task, some of the attention-free models do\nget close to the majority baseline, though still above it.\n1406\n1 1\n2\n1\n8\n1\n16 0\n0.00\n0.25\n0.50\n0.75\nCoLA MCC\nCoLA\n1 1\n2\n1\n8\n1\n16 0\n0.70\n0.75\n0.80\n0.85\n0.90\nMRPC F1\nMRPC\n1 1\n2\n1\n8\n1\n16 0\n0.65\n0.75\n0.85\n0.95\nSST2 ACC\nSST-2\n1 1\n2\n1\n8\n1\n16 0\n0.5\n0.6\n0.7\n0.8\n0.9\nMNLI ACC\nMNLI\nBERT-BASE\nRoBERTa-BASE\nDeBERTa-BASE\n1 1\n2\n1\n8\n1\n16 0\n0.7\n0.8\n0.9\n1.0\nNER F1\nNER\n1 1\n2\n1\n8\n1\n16 0\n0.850\n0.875\n0.900\n0.925\n0.950\nPOS F1\nPOS\nX axis: Fraction of input-dependent attention heads\n(a) BASE models\n1 1\n2\n1\n8\n1\n16 0\n0.00\n0.25\n0.50\n0.75\nCoLA MCC\nCoLA\n1 1\n2\n1\n8\n1\n16 0\n0.70\n0.75\n0.80\n0.85\n0.90\nMRPC F1\nMRPC\n1 1\n2\n1\n8\n1\n16 0\n0.65\n0.75\n0.85\n0.95\nSST2 ACC\nSST-2\n1 1\n2\n1\n8\n1\n16 0\n0.5\n0.6\n0.7\n0.8\n0.9\nMNLI ACC\nMNLI\nBERT-LARGE\nRoBERTa-LARGE\nDeBERTa-LARGE\n1 1\n2\n1\n8\n1\n16 0\n0.7\n0.8\n0.9\n1.0\nNER F1\nNER\n1 1\n2\n1\n8\n1\n16 0\n0.850\n0.875\n0.900\n0.925\n0.950\nPOS F1\nPOS\nX axis: Fraction of input-dependent attention heads\n(b) LARGE models\nFigure 2: Probing results (y-axis) with decreasing number of attention heads (x-axis). BASE models are shown in\nFig. 2a, and LARGE models are shown in Fig. 2b. Higher is better in all cases.\n1407\n0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11\nRelative reduced score\n0.82\n0.83\n0.84\n0.85\n0.86\n0.87\nOriginal model averaged score\nBERT-BASE\nRoBERTa-BASE\nDeBERTa-BASE\nBERT-LARGE\nRoBERTa-LARGE\nDeBERTa-LARGE\nFigure 3: Stronger-performing PLMs use their atten-\ntion capability more. y-axis: original model average\nperformance; x-axis: relative reduced score when all\nattention matrices are replaced with constant ones.\ncompared to the original model (e.g., BERTBASE\nand DeBERTaLARGE), suggesting that some of the\nmodels’ heads have a slight preference towards\nconstant matrices. This result is consistent with\nsome of the findings of recent hybrid models that\nuse both constant and regular attention (Liu et al.,\n2021; Lee-Thorp et al., 2021) to build efficient\nmodels.\nPerformant models rely more on attention\nFig. 3 shows for each model the relation between\nthe original performance (averaged across tasks)\nand the averaged (relative) reduced score when re-\nplacing all attention heads. We observe a clear\ntrend between the models’ performance and their\nrelative reduced score, which suggests that better\nperforming models use their attention mechanism\nmore.\n5 Further Analysis\nWe present an analysis of PAPA, to better under-\nstand its properties. We first discuss the patterns of\nthe constant matrices produced by PAPA (Sec. 5.1).\nNext, we consider other alternatives to generat-\ning constant matrices (Sec. 5.2); we then examine\nwhether the constant matrices are data-dependent\n(Sec. 5.3); we continue by exploring alternative\nmethods for selecting which attention heads to\nreplace (Sec. 5.4). Finally, we present MLM re-\nsults, and discuss the challenges in interpreting\nthem (Sec. 5.5). In all experiments below, we\nuse RoBERTaBASE. RoBERTaLARGE experiments\nshow very similar trends, see App. C.\n5.1 Patterns of the Constant Matrices\nWe first explore the attention patterns captured by\ndifferent heads by observing the constant matrices\n(Ch). We first notice a diagonal pattern, in which\neach token mostly attends to itself or to its neigh-\nboring words. This pattern is observed in about\n90% of the constant matrices produced by PAPA.\nSecond, about 40% of the heads put most of their\nweight mass on the [CLS] and/or [SEP] tokens\n(perhaps in combination with the diagonal pattern\ndescribed above). Lastly, while for some of the\nheads the weight mass is concentrated only in spe-\ncific entry per row (which corresponding only to a\nspecific token), in most of cases the weight mass is\ndistributed over several entries (corresponding to\nseveral different tokens). These patterns are sim-\nilar to those identified by Clark et al. (2019), and\nexplain in part our findings—many of the attention\nheads mostly focus on fixed patterns that can also\nbe captured by a constant matrix. Fig. 4 shows\nthree representative attention heads that illustrate\nthe patterns above.\n5.2 Alternative Constant Matrices\nPAPA replaces the attention matrices with constant\nones. As described in Sec. 3.1, this procedure re-\nquires only an unlabeled corpus. In this section, we\ncompare this choice with constant matrices that are\nconstructed without any data (data-free matrices),\nand those that require labeled data for construction\n(labeled matrices).\nFor the former we consider three types of ma-\ntrices: (1) Identity matrix—in which each token\n‘attends’ only to itself, and essentially makes self-\nattention a regular feed-forward (each token is pro-\ncessed separately); (2) Toeplitz matrix—we use a\nsimple Toeplitz matrix (as suggested in Liu et al.,\n2021), where the weight mass is on the current\ntoken, and it decreases as the attended token is\nfurther from the current one (the entries of the ma-\ntrix are based on the harmonic series);13 (3) Zeros\nmatrix—essentially pruning the heads.\nWe also consider two types of labeled-matrices:\n(4) initialized as the Toeplitz matrices from (2);\nand (5) initialized as our average matrices. These\nmatrices are updated during the training procedure\nof the probing classifier.14\nTab. 1 shows the performance of each attention-\nfree resulting model for all downstream tasks. We\nobserve that for all tasks, our average-based model\n13Similar to the Gaussian matrices suggested by You et al.\n(2020).\n14To make minimal changes to the frozen model, all con-\nstant matrices are masked and normalized (row-wise), the\nsame as the output of the original softmax operation.\n1408\n(0, 11) (4, 4) (6, 3)\nFigure 4: Generated constant matrices Ch by the PAPA method for representative heads (layer, head). These\nmatrices used for the attention-free variant of RoBERTaBASE for the SST-2 task.\nMatrix Construction Matrix Type CoLA MRPC SST2 MNLI-mm NER POS\nAttention based Original 0.47 0.85 0.91 0.78 0.92 0.93\nData-Free\nIdentity 0.04 0.80 0.80 0.63 0.55 0.87\nToeplitz 0.08 0.81 0.79 0.65 0.77 0.90\nZeros 0.09 0.80 0.80 0.66 0.57 0.87\nLabeled Data Toeplitz init. 0.08 0.81 0.79 0.68 0.78 0.91\nAverage init. 0.34 0.81 0.87 0.72 0.89 0.93\nUnlabeled Data Average (Ours) 0.31 0.82 0.87 0.69 0.89 0.93\nTable 1: Probe task of performance of RoBERTaBASE with different constant matrix types as a replacement to the\ninput-dependent attention matrix. Bold numbers indicate the best constant model for the task. Our approach based\non an average of multiple attention matrices outperforms all other data-free matrix types across all tasks, and gets\nsimilar results to the best labeled-data based model. In all tasks higher is better.\noutperforms all other data-free models by a no-\ntable margin. As for the labeled-matrices models,\nour model also outperforms the one initialized with\nToeplitz matrices (4), and in most cases gets similar\nresults to the model initialized with average matri-\nces (5). It should be noted that the original models\n(with regular attention) do not update their inner\nparameters in the probing training phase, which\nmakes the comparison to the labeled-matrices mod-\nels somewhat unfair. The above suggests that our\nchoice of constant matrix replacement better esti-\nmates the performance of the attention-free PLMs.\n5.3 Are the Constant Matrices\nData-Dependent?\nPAPA constructs the constant matrix for a given\nhead Ch as the average of the model’s attention\nmatrices over a given corpus D, which in our ex-\nperiments is set to be the training set of the task\nat hand (labels are not used). Here we examine\nthe importance of this experimental choice by gen-\nerating Ch using a different dataset—the MNLI\ntraining set, which is out of distribution for the\nother tasks.\nTask CoLA MRPC SST2 NER POS\nPer-Task 0.31 0.82 0.87 0.89 0.93\nMNLI 0.32 0.81 0.87 0.89 0.93\nTable 2: Comparison of probe task performance of\nRoBERTaBASE between two setups of constructing the\naveraged constant matrices Ch: Per-Task uses the task\ntraining set, while MNLI uses the constant matrices gen-\nerated with the MNLI dataset. The results are similar\nbetween the two setups, which indicates a low depen-\ndence of the constant matrices on the dataset used for\nconstructing them.\nResults are presented in Tab. 2. The performance\nacross all tasks is remarkably similar between gen-\nerating the matrices using the specific task training\nset and MNLI, which suggests that the constant\nmatrices might be somewhat data-independent.\n5.4 Alternative Head Selection Methods\nWe compare our method for selecting which heads\nto replace (Sec. 3.2) with a few alternatives. The\nfirst two replace the heads by layer order: (1) we\nsort the heads from the model’s first layer to the\n1409\n1 1\n2\n1\n8\n1\n16 0\n0.5\n0.6\n0.7\n0.8\n0.9\nMNLI ACC\nOurs\nStart to End\nEnd to Start\nRandom\nReversed\nFigure 5: Comparison between different heads selection\nmethods over MNLI. Our method outperforms all other\nalternatives. The x-axis represents the fraction of input-\ndependent attention heads.\nlast and (2) from the model’s last layer to the first.\nIn both cases we use the internal head ordering\nper layer for ordering within the layer. We then\nreplace the first k% of the heads. We also add (3)\na random baseline that randomly replaces k% of\nthe heads, and a (4) ‘Reversed’ one which replaces\nthe heads with the highest (rather than lowest) λh\nvalues (Sec. 3.2).\nFig. 5 shows the MNLI performance of each\nmethod as a function of the fraction of heads re-\nplaced. We observe that our method, which is based\non learned estimation of attention importance, out-\nperforms all other methods for every fraction of\nheads replaced. Moreover, the ‘Reversed’ method\nis the worst among the examined methods, which\nsuggests that our method not only replaces the least\nattention dependent heads first, but also replaces\nthe most dependent ones last. Although our head\nreplacement order outperforms the above methods,\nwe note that our order is an overestimation of the\nmodel attention dependency, and better methods\nmight show that even less attention is needed.\n5.5 Effects on MLM Perplexity\nSo far we have shown that applying PAPA on down-\nstream tasks only incurs a moderate accuracy drop.\nThis section aims to explore its impact on masked\nlanguage modeling (MLM). We find that while our\nmodels suffer a larger performance drop on this\ntask compared to the other tasks, this can be ex-\nplained by their pretraining procedure.\nFig. 6a plots the negative log perplexity (higher\nis better) of all BASE models on the WikiText-103\n(Merity et al., 2017) validation set. When replac-\ning attention matrices using PAPA, MLM suffers\na larger performance drop compared to the down-\nstream tasks (Sec. 4.2). We hypothesize that this\nis because these pretrained Transformers are more\nspecialized in MLM, the task they are pretrained\non. As a result, they are less able to adapt to archi-\ntectural changes in MLM than in downstream tasks.\nTo test our hypothesis, we probe ELECTRABASE\n(Clark et al., 2020) using PAPA. ELECTRA is an\nestablished pretrained Transformer trained with\nthe replaced token detection objective, instead of\nMLM. It has proven successful on a variety of\ndownstream tasks.\nELECTRABASE’s probing performance on\nMLM supports our hypothesis: We first note that\nits original performance is much worse compared\nto the other models (–3.51 compared to around –2\nfor the MLM-based models), despite showing sim-\nilar performance on downstream tasks (Fig. 6b),\nwhich hints that this model is much less adapted to\nMLM. Moreover, the drop when gradually remov-\ning heads is more modest (a 0.44 drop compared\nto 1–1.5 for the other models), and looks more\nsimilar to ELECTRABASE’s probing performance\non MNLI (Fig. 6b). Our results suggest a poten-\ntial explanation for the fact that some pretrained\nTransformers suffer a larger performance drop on\nMLM than on downstream tasks; rather than MLM\ndemanding higher attention use, this is likely be-\ncause these models are pretrained with the MLM\nobjective.\n6 Related Work\nAttention alternatives Various efforts have been\nmade in search of a simple or efficient alternative\nfor the attention mechanism. Some works focused\non building a Transformer variant based on an ef-\nficient approximation of the attention mechanism\n(Kitaev et al., 2020; Wang et al., 2020; Peng et al.,\n2020a; Choromanski et al., 2021; Schlag et al.,\n2021; Qin et al., 2022). Another line of research,\nwhich is more related to our work, replaced the\nattention mechanism in Transformers with a con-\nstant (and efficient) one. For instance, FNet (Lee-\nThorp et al., 2021) replaced the attention matrix\nwith the Vandermonde matrix, while gMLP (Liu\net al., 2021) and FLASH (Hua et al., 2022) replaced\nit with a learned matrix. 15 These works showed\nthat pretraining attention-free LMs can lead to com-\npetitive performance. Our work shows that PLMs\ntrained with attention can get competitive perfor-\nmance even if they are denied access to this mecha-\nnism during transfer learning.\n15These models also added a gating mechanism, which does\nnot change the input-independent nature of their component.\n1410\n1 1\n2\n1\n8\n1\n16 0\n−5\n−4\n−3\n−2\n−1\nnegative log PPL\nMLM\nBERT-BASE\nRoBERTa-BASE\nDeBERTa-BASE\nELECTRA-BASE\n(a) MLM results\n1 1\n2\n1\n8\n1\n16 0\n0.5\n0.6\n0.7\n0.8\n0.9\nMNLI ACC\nMNLI (b) MNLI mismatched results\nFigure 6: ELECTRA BASE model compared with other BASE models on MLM and MNLI. In Fig. 6a ELECTRABASE\nbehaves similarly to its behavior on MNLI, but not to the other models, which are MLM-based. In Fig. 6b\nELECTRABASE behaves similar to other models. In both graphs the x-axis represents the fraction of input-\ndependent attention heads, and the y-axis is the score of the specific task (higher is better).\nAnalysis of attention patternsSome investiga-\ntions of how attention patterns in Transformers\nwork use probing techniques. Clark et al. (2019),\nRavishankar et al. (2021) and Htut et al. (2019)\nstudied the attention behavior in BERT. Unlike the\nabove, which only focuses on the attention patterns\nof the PLM, our work sheds light on the depen-\ndence of PLMs on their attention mechanism.\nPruning methods In this work we replaced the\nattention matrix with a constant one in order to mea-\nsure the importance of the input-dependent ability.\nWorks like Michel et al. (2019) and Li et al. (2021)\npruned attention heads in order to measure their\nimportance for the task examined. These works\nfind that for some tasks, only a small number of\nunpruned attention heads is sufficient, and thus re-\nlate to the question of how much attention does\na PLM use. In this work we argue that replacing\nattention matrices with constant ones provides a\nmore accurate answer for this question compared\nto pruning these matrices, and propose PAPA, a\nmethod for constructing such constant matrices.\n7 Conclusion\nIn this work, we found that PLMs are not as de-\npendent on their attention mechanism as previously\nthought. To do so, we presented PAPA—a method\nfor analyzing the attention usage in PLMs. We ap-\nplied PAPA to several widely-used PLMs and six\ndownstream tasks. Our results show that replac-\ning all of the attention matrices with constant ones\nachieves competitive performance to the original\nmodel, and that half of the attention matrices can be\nreplaced without any loss in performance. We also\nshow a clear relation between a PLM’s aggregate\nperformance across tasks and its degradation when\nreplacing all attention matrices with constant ones,\nwhich hints that performant models make better\nuse of their attention.\nOur results motivate further work on novel Trans-\nformer architectures with more efficient attention\nmechanisms, both for pretraining and for knowl-\nedge distillation of existing PLMs. They also moti-\nvate the development of Transformer variants that\nimprove performance by making better use of the\nattention mechanism.\n8 Limitations\nThis work provides an analysis of the attention\nmechanism in PLMs. Our PAPA method is based\non probing rather than finetuning, which is more\ncommon use to PLMs. We recognize that the at-\ntention mechanism in finetuned PLMs might act\ndifferently than the original model, but our main\nfocus is investigating the PLM itself, rather than its\nfinetuned version.\nOur analysis method is built on replacing the\nattention matrices with constant ones (Sec. 3.1).\nWe build these constant matrices by averaging the\nattention matrices over a given dataset. Because of\nthis choice, our results reflect a lower bound on the\nresults of the optimal attention-free model, and we\nacknowledge that there might be methods for con-\nstructing the constant matrices that would lead to\neven smaller gaps from the original model. A simi-\nlar argument can be applied for our heads selection\nmethod (Sec. 3.2). Importantly, better methods for\nthese sub-tasks might further reduce the gap be-\ntween the original models and the attention-free\nones, which will only strengthen our argument.\nFinally, we note that we used the PAPA method\n1411\nwith six English tasks, and recognize that results\nmight be different for other tasks and other lan-\nguages.\nAcknowledgments\nWe thank Miri Varshavsky for the great feedback\nand moral support. This work was supported in part\nby NSF-BSF grant 2020793, NSF grant 2113530,\nan Ulman Fellowship, a Google Fellowship, a Leib-\nnitz Fellowship, and a research gift from Intel.\nReferences\nYonatan Belinkov. 2022. Probing Classifiers: Promises,\nShortcomings, and Advances. Computational Lin-\nguistics, 48(1):207–219.\nAdrien Bibal, Rémi Cardon, David Alfter, Rodrigo\nWilkens, Xiaoou Wang, Thomas François, and\nPatrick Watrin. 2022. Is attention explanation? an\nintroduction to the debate. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3889–3900, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamás Sar-\nlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Belanger, Lucy Colwell, and\nAdrian Weller. 2021. Rethinking attention with Per-\nformers. In Proc. of ICLR.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. PaLM: Scaling language\nmodeling with pathways. arXiv:2204.02311.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Proc. of\nBlackboxNLP.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In Proc. of ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proc. of NAACL.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proc. of IWP.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. DeBERTa: decoding-enhanced\nbert with disentangled attention. In Proc. of ICLR.\nPhu Mon Htut, Jason Phang, Shikha Bordia, and\nSamuel R. Bowman. 2019. Do attention heads in bert\ntrack syntactic dependencies? arXiv:1911.12246.\nWeizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V .\nLe. 2022. Transformer quality in linear time.\narXiv:2202.10447.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama,\nGabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu\nChen, and Noah A. Smith. 2021. Finetuning pre-\ntrained transformers into RNNs. In Proc. of EMNLP.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. In Proc.\nof ICLR.\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and\nSantiago Ontanon. 2021. Fnet: Mixing tokens with\nfourier transforms. arXiv:2105.03824.\nJiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. 2021.\nDifferentiable subset pruning of transformer heads.\nTransactions of the Association for Computational\nLinguistics, 9:1442–1459.\nHanxiao Liu, Zihang Dai, David So, and Quoc V Le.\n2021. Pay attention to MLPs. In Advances in Neural\nInformation Processing Systems, volume 34, pages\n9204–9215. Curran Associates, Inc.\n1412\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proc. of NAACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv:1907.11692.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In Proc. of ICLR.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In NeurIPS.\nHao Peng, Jungo Kasai, Nikolaos Pappas, Dani\nYogatama, Zhaofeng Wu, Lingpeng Kong, Roy\nSchwartz, and Noah Smith. 2022. ABC: Attention\nwith bounded-memory control. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n7469–7483, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong. 2020a.\nRandom feature attention. In International Confer-\nence on Learning Representations.\nHao Peng, Roy Schwartz, Dianqi Li, and Noah A. Smith.\n2020b. A mixture of h - 1 heads is better than h\nheads. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6566–6577, Online. Association for Computational\nLinguistics.\nZhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yun-\nshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong,\nand Yiran Zhong. 2022. cosformer: Rethinking soft-\nmax in attention. arXiv:2202.08791.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR.\nVinit Ravishankar, Artur Kulmizev, Mostafa Abdou,\nAnders Søgaard, and Joakim Nivre. 2021. Atten-\ntion can reflect syntactic structure (if you let it).\narXiv:2101.10927.\nImanol Schlag, Kazuki Irie, and Jürgen Schmidhuber.\n2021. Linear transformers are secretly fast weight\nmemory systems. In Proc. of ICML.\nSofia Serrano and Noah A. Smith. 2019. Is attention in-\nterpretable? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2931–2951, Florence, Italy. Association for\nComputational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proc. of EMNLP.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProc. of CoNLL.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. of NeurIPS.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 5797–5808, Florence, Italy.\nAssociation for Computational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Proc. of\nICLR.\nShanshan Wang, Zhumin Chen, Zhaochun Ren,\nHuasheng Liang, Qiang Yan, and Pengjie Ren. 2022.\nPaying more attention to self-attention: Improving\npre-trained language models via attention guiding.\narXiv:2204.02922.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity. arXiv:2006.04768.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTACL.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not\nnot explanation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 11–20, Hong Kong, China. Association for\nComputational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proc. of\nNAACL.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\n1413\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nWeiqiu You, Simeng Sun, and Mohit Iyyer. 2020. Hard-\ncoded gaussian attention for neural machine transla-\ntion. arXiv:2005.00742.\n1414\nA Pre-Processing\nTo make the replacement of the attention matrix\nwith a constant one reasonable, we fix the position\nof the [SEP] token to always be the last token of\nthe model’s input, rather than separating the last\ninput token from the padding tokens (i.e., it comes\nafter the padding tokens rather than before them).\nFor tasks with two sequences per example (e.g.,\nMNLI), which are typically separated by an ad-\nditional [SEP] token, we fix this token to always\nbe the middle token of the sequence, followed by\nthe second sentence. We recognize that this might\nlead to suboptimal usage of the input’s sequence\nlength, e.g., if one of the sentences is substantially\nlonger than the other and particularly if it is longer\nthan half of the sequence length, it would thus be\ntrimmed. In our experiments this only happened in\nless than 0.2% of input samples for a single task\n(MNLI), but we recognize that this might happen\nmore frequently in other datasets.\nB Hyperparameters\nAll of our code was implemented with the Trans-\nformers library (Wolf et al., 2020). Hyperparame-\nters for the probing classifier on downstream tasks\nare shown in Tab. 3.\nLearning Rate Batch Epochs Seq. Len.\nCoLA 2.00E-05 16 15 64\nSST-2 1.00E-04 32 4 64\nMNLI 2.00E-04 8 4 256\nMRPC 2.00E-05 16 15 128\nNER 1.00E-04 8 4 128\nPOS 5.00E-04 8 4 128\nMLM 5.00E-04 8 2 128\nTable 3: Probing classifier hyperparameters for down-\nstream tasks.\nC Further Analsys results for\nRoBERTaLARGE\nTab. 4 and 5 show RoBERTaLARGE’s analysis re-\nsults for the experiments described in Sec. 5.2 and\n5.3, respectively.\n1415\nMatrix Construction Matrix Type CoLA MRPC SST2 MNLI-mm NER POS\nAttention based Original 0.53 0.87 0.93 0.81 0.93 0.93\nData-Free\nIdentity 0.09 0.72 0.80 0.65 0.56 0.87\nToeplitz 0.11 0.79 0.80 0.65 0.74 0.89\nZeros 0.09 0.80 0.81 0.66 0.57 0.87\nLabeled Data Toeplitz init. 0.11 0.78 0.80 0.68 0.75 0.89\nAverage init. 0.35 0.81 0.88 0.73 0.91 0.93\nUnlabeled Data Average (Ours) 0.34 0.81 0.85 0.68 0.89 0.92\nTable 4: Probe task of performance of RoBERTaLARGE with different constant matrix types as a replacement to the\ninput-dependent attention matrix. Tab. 1 shows the results for RoBERTaBASE.\nTask CoLA MRPC SST2 NER POS\nPer-Task 0.34 0.80 0.85 0.89 0.92\nMNLI 0.35 0.81 0.85 0.88 0.92\nTable 5: Comparison of probe task performance of\nRoBERTaLARGE between two setups of constructing the\naveraged constant matrices Ch: Per-Task uses the task\ntraining set, while MNLI uses the constant matrices\ngenerated with the MNLI dataset. Tab. 2 shows the\nresults for RoBERTaBASE.\n1416",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8077383041381836
    },
    {
      "name": "Computer science",
      "score": 0.7424131631851196
    },
    {
      "name": "Architecture",
      "score": 0.6329460144042969
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44002461433410645
    },
    {
      "name": "Machine learning",
      "score": 0.3411547541618347
    },
    {
      "name": "Engineering",
      "score": 0.08738470077514648
    },
    {
      "name": "Electrical engineering",
      "score": 0.08261135220527649
    },
    {
      "name": "Voltage",
      "score": 0.07359486818313599
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I197251160",
      "name": "Hebrew University of Jerusalem",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210153776",
      "name": "Apple (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ],
  "cited_by": 12
}