{
  "title": "Probabilistic Spatial Transformer Networks",
  "url": "https://openalex.org/W4287816707",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5004316887",
      "name": "Pola Schwöbel",
      "affiliations": [
        "Technical University of Denmark"
      ]
    },
    {
      "id": "https://openalex.org/A5011019757",
      "name": "Frederik Warburg",
      "affiliations": [
        "Technical University of Denmark"
      ]
    },
    {
      "id": "https://openalex.org/A5077781881",
      "name": "Martin Jørgensen",
      "affiliations": [
        "Oxford Research Group",
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A5032991538",
      "name": "Kristoffer H. Madsen",
      "affiliations": [
        "Copenhagen University Hospital",
        "Hvidovre Hospital",
        "Technical University of Denmark"
      ]
    },
    {
      "id": "https://openalex.org/A5072186051",
      "name": "Søren Hauberg",
      "affiliations": [
        "Technical University of Denmark"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287111872",
    "https://openalex.org/W3034764327",
    "https://openalex.org/W2970187703",
    "https://openalex.org/W2911546748",
    "https://openalex.org/W2808266363",
    "https://openalex.org/W603908379",
    "https://openalex.org/W1667072054",
    "https://openalex.org/W3101380508",
    "https://openalex.org/W2562066862",
    "https://openalex.org/W2156163116",
    "https://openalex.org/W2105728138",
    "https://openalex.org/W2949736877",
    "https://openalex.org/W3209643580",
    "https://openalex.org/W3093554712",
    "https://openalex.org/W2963238274",
    "https://openalex.org/W2972307389",
    "https://openalex.org/W4287120006",
    "https://openalex.org/W3021332602",
    "https://openalex.org/W4293388793",
    "https://openalex.org/W2963065075",
    "https://openalex.org/W2753738274",
    "https://openalex.org/W2572384567",
    "https://openalex.org/W2412002662",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W4288336059",
    "https://openalex.org/W2174479785",
    "https://openalex.org/W2750384547",
    "https://openalex.org/W3035711539",
    "https://openalex.org/W4288333413",
    "https://openalex.org/W2798896071",
    "https://openalex.org/W1592735339",
    "https://openalex.org/W3094869948",
    "https://openalex.org/W4391602018",
    "https://openalex.org/W582134693",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2951266961"
  ],
  "abstract": "Spatial Transformer Networks (STNs) estimate image transformations that can improve downstream tasks by `zooming in' on relevant regions in an image. However, STNs are hard to train and sensitive to mis-predictions of transformations. To circumvent these limitations, we propose a probabilistic extension that estimates a stochastic transformation rather than a deterministic one. Marginalizing transformations allows us to consider each image at multiple poses, which makes the localization task easier and the training more robust. As an additional benefit, the stochastic transformations act as a localized, learned data augmentation that improves the downstream tasks. We show across standard imaging benchmarks and on a challenging real-world dataset that these two properties lead to improved classification performance, robustness and model calibration. We further demonstrate that the approach generalizes to non-visual domains by improving model performance on time-series data.",
  "full_text": "Probabilistic Spatial Transformer Networks\nPola Schwöbel1 Frederik Warburg1 Martin Jørgensen2 Kristoffer H. Madsen1, 3 Søren Hauberg1\n1 Section for Cognitive Systems, DTU Compute, Technical University of Denmark, Copenhagen, Denmark\n2Machine Learning Research Group, Department of Engineering Science, University of Oxford, Oxford, UK\n3Danish Research Centre for Magnetic Resonance, Centre for Functional and Diagnostic Imaging and Research,\nCopenhagen University Hospital Hvidovre, Hvidovre, Denmark\nAbstract\nSpatial Transformer Networks (STNs) estimate im-\nage transformations that can improve downstream\ntasks by ‘zooming in’ on relevant regions in an im-\nage. However, STNs are hard to train and sensitive\nto mis-predictions of transformations. To circum-\nvent these limitations, we propose a probabilistic\nextension that estimates a stochastic transforma-\ntion rather than a deterministic one. Marginalizing\ntransformations allows us to consider each image\nat multiple poses, which makes the localization\ntask easier and the training more robust. As an\nadditional beneﬁt, the stochastic transformations\nact as a localized, learned data augmentation that\nimproves the downstream tasks. We show across\nstandard imaging benchmarks and on a challenging\nreal-world dataset that these two properties lead\nto improved classiﬁcation performance, robustness\nand model calibration. We further demonstrate that\nthe approach generalizes to non-visual domains by\nimproving model performance on time-series data.\n1 INTRODUCTION\nThe Spatial Transformer Network (STN) [Jaderberg et al.,\n2015] predicts a transformation on input data in order to\nsimplify a downstream task. For example, a neural network\nmight beneﬁt from e.g. ‘zooming in’ on relevant parts of\nan image, remove unwarranted image rotations, or time-\nnormalize sequence data before making predictions. In prin-\nciple, this can improve robustness, interpretability and ef-\nﬁciency of the model. However, in practice, the situation\nis not as ideal. Both at training and test time, the STN is\nsensitive to small mis-predictions of transformations. For\nexample, if the STN zooms in on the wrong part of an image,\nthen the signal is lost for the downstream task, e.g. see crop\nA and C in Fig. 1. The empirical impact is that STNs are\nSTNP-STN\nA\nB\nC\nNegative log-likelihood\nA\nB\nC\nA\nB\nC\nA\nB\nC\nFigure 1: The Probabilistic Spatial Transformer Network\n(P-STN) marginalizes over a distribution of possible input\ntransformations. By ‘looking in multiple places’ we hope to\nstabilize the brittle nature of the regular spatial transformer:\nThe P-STN loss landscape is signiﬁcantly more smooth and\nwith fewer local minima compared to the STN.\ndifﬁcult to train and often do not live up to their promise.\nFrom a probabilistic perspective, this sensitivity has an ob-\nvious solution: we should estimate the posterior over the\napplied transformation and marginalize accordingly. This\namounts to ‘trying many different transformations’, and\nshould improve robustness. It is exactly this approach we\ninvestigate.\nSTNs consist of two parts. A localization network performs\nthe transformation task, i.e. it estimates the transformation\nparameters θ for a given image I and applies the corre-\nsponding transformation Tθ(I). A standard neural network\nperforms the downstream task on the transformed image, i.e.\ncomputing p(y|Tθ(I)). Since we are concerned with classi-\nAccepted for the 38th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2022).\narXiv:2004.03637v2  [cs.LG]  15 Jun 2022\nﬁcation tasks, we will refer to the latter as the classiﬁer, but\nnote that the approach generalizes to other tasks.\nIn our probabilistic STN (P-STN), we estimate a distribu-\ntion over transformations that we marginalize: p(y|I) =∫\np(y|Tθ(I))dθ. We approximate this intractable integral\nvia Monte Carlo, i.e. we sample transformations. These\ntransformation samples produce different transformed ver-\nsions of the input image, {Ts\nθ(I)}s=1...S. The classiﬁer\nmakes predictions on all samples, and we aggregate the\npredictions. Figure 2 shows the model architecture.\nWe hypothesize that marginalizing image transformation\nhas beneﬁts for both parts of the model. For the localization\nnetwork, our model gets to ‘try many different transfor-\nmations’ through random sampling. This should improve\nthe localization. Secondly, the classiﬁer now gets presented\nwith different transformed versions of the input image\nthrough Monte Carlo samples {Ts\nθ(I)}s=1...S. Interestingly,\nthis corresponds to a type of data augmentation, which\nshould improve classiﬁcation.\nWe verify these hypotheses by making the following contri-\nbutions:\n1. We develop the Probabilistic Spatial Transformer; a hi-\nerarchical Bayesian model over image transformations.\n2. We perform variational inference to ﬁt the transforma-\ntion model as well as downstream model end-to-end,\nusing only label information.\n3. We experimentally demonstrate that our model\nachieves better localization, increased classiﬁcation\naccuracy (resulting from learned per-image data\naugmentation) and improved calibration.\n2 RELATED WORK\nSpatial transformer networks apply a spatial transforma-\ntion to the input data as part of an end-to-end trained model\n[Jaderberg et al., 2015]. The transformation parameters\nare estimated from each input separately through a neural\nnetwork. Most commonly, STNs implement simple afﬁne\ntransformations, such that the network can learn to zoom\nin on relevant parts of an image before solving the task\nat hand. STNs have shown themselves to be useful for\nboth generative and discriminative tasks, and have seen\napplications to different data modalities [Jaderberg et al.,\n2015, Detlefsen and Hauberg, 2019, Detlefsen et al., 2018,\nShapira Weber et al., 2019, Sønderby et al., 2015, Lin and\nLucey, 2016, Kanazawa et al., 2016]. We propose a proba-\nbilistic extension of this idea, replacing the usual likelihood\nmaximization with marginalization over transformations.\nBayesian deep learning aims to solve probabilistic compu-\ntations in deep neural networks. Priors are put on weights\nand marginalized at training and test time, often yielding\nuseful uncertainties in the posterior predictive. The required\ncomputations are in general intractable, and approaches\ndiffer mainly in the type of approximation to the weight pos-\nterior. Gal and Ghahramani [2016] propose to view dropout\nas a Bernoulli approximation to the weight posterior (i.e.\nrandomly switching each weight on or off). The Laplace ap-\nproximation [MacKay, 1992, Daxberger et al., 2021] places\na Gaussian posterior over a trained neural network’s weights.\nAnother generally successful way to obtain predictive un-\ncertainties is to simply train an ensemble of models. Orig-\ninally proposed as an alternative to Bayesian DL [Laksh-\nminarayanan et al., 2017], the approach can be interpreted\nin the Bayesian framework by interpreting the weights of\nthe trained ensemble members as samples from a weight\nposterior [Gustafsson et al., 2020]. Similar to our method,\nBlundell et al. [2015] choose a variational approach with\na simple Gaussian mean ﬁeld posterior over weights. Our\napproach differs from standard Bayesian DL in that we\nare not reasoning about distributions over neural network\nweights p(w), but instead a subnetwork’s (i.e. the local-\nizer’s)outputs p(θ). Drawing from the posterior over image\ntransformations, we effectively recover data augmentation.\nData augmentation (DA) is a useful way to increase the\namount of available data [LeCun et al., 1995, Krizhevsky\net al., 2012]. DA requires prior knowledge about the struc-\nture of the data: the target yis assumed to be invariant to\ncertain transformations of the observation I. Invariance as-\nsumptions are usually straight forward for natural images.\nThus, DA is common for image data, where the transfor-\nmation family is often chosen to be rotations, scalings, and\nsimilar [Goodfellow et al., 2009, Baird, 1992, Simard et al.,\n2003, Krizhevsky et al., 2012, Loosli et al., 2007]. The gen-\neral trend is that, beyond ‘intuitive’ data such as images,\ngathering an invariance prior is difﬁcult, and DA is often\nhard to realize through manual tuning.\nLearned data augmentation provides a more principled\napproach to artiﬁcially extending datasets. Hauberg et al.\n[2016] estimate an augmentation scheme from the training\ndata via pre-aligning images in an unsupervised manner.\nThe approach allows for signiﬁcantly more complex trans-\nformations than the usual afﬁne family, but the unsupervised\nnature and the implied two-step training process render the\napproach suboptimal. Similarly, Cubuk et al. [2019, 2020]\nuse reinforcement learning and grid search to learn data\naugmentation schemes, but rely on validation data rather\nthan an end-to-end formulation.\nLearning data augmentation end-to-end requires a loss func-\ntion suitable for model selection, as we are effectively trying\nto learn an inductive bias. Based on this realization, Van der\nWilk et al. [2018] learn DA end-to-end in Gaussian pro-\ncesses (GPs) via the marginal likelihood, a suitable loss\nfor model selection and thus invariance learning [MacKay,\n2003]. The marginal likelihood is hard to compute for NNs,\nso Schwöbel et al. [2022] extend this idea to NNs by consid-\nFigure 2: The P-STN pipeline. From the observed image I, a distribution of transformations is estimated. Samples from this\ndistribution are applied to the observed image to produce augmented samples, which are fed to a classiﬁer that averages\nacross samples. In the deterministic STN case, the localizer only computes one transformationθ(I), which can be thought of\nas the maximum likelihood solution. Instead of the multiple transformation samples, we obtain a single Tθ(I) in this case.\nering a deep kernel model, i.e. a neural network with a GP in\nthe last layer. Benton et al. [2020] instead use the standard,\nmaximum likelihood loss and explicitly regularize towards\nnon-zero augmentations. Our model differs from existing\ndata augmentation approaches — learned and non-learned\n— in that we estimate local, i.e. per-image transformations\ninstead of a global augmentation scheme.\n3 BACKGROUND\nThe STN localiser module estimates a transformation θ(x)\nthat transforms a coordinate grid and interpolates an image\naccordingly. The classiﬁer module takes the transformed im-\nage and computes p(y|Tθ(x)). Both the localizer and classi-\nﬁer are neural networks. The STN can be trained end-to-end\nwith only label information as long as the image transfor-\nmations are parameterized in a differentiable manner.\nAfﬁne transformations are a simple class of transforma-\ntions that can be differentiably parameterized. We limit\nourselves to the subset of afﬁne transformations contain-\ning rotation, isotropic scaling and translation in xand y. In\ntwo dimensions (and the corresponding three-dimensional\nhomogeneous coordinates), we thus learn θ= (r,s,tx,ty)\nwhich parameterizes the afﬁne matrix\nAθ =\n\n\ns·cos r −s·sin r t x\ns·sin r s ·cos r t y\n0 0 1\n\n∈R3×3, s>0. (1)\nSince det(Aθ) = s2, the constraint s> 0 ensures invertibil-\nity and can be implemented as seen in Detlefsen et al. [2018].\nIn practice, the STN estimates well-behaved, non-collapsing\ntransformations without implementing the constraint explic-\nitly. Tθ(I) is applied by transforming a grid of the target\nimage size by Aθ and interpolating the source image at the\nresulting coordinates (see Jaderberg et al. [2015] for details).\nDiffeomorphic transformations (i.e. transformations that\nare differentiable, invertible and possess a differentiable in-\nverse) are more general than afﬁne transformations, and are\nnot limited to the spatial domain. Freifeld et al. [2017] con-\nstruct diffeomorphisms from continuous piecewise-afﬁne\nvelocity ﬁelds as follows. The transformation domain Ω is\ndivided into subsets and an afﬁne matrix is deﬁned on each\ncell cof such a tessellation. Each afﬁne matrix Aθc induces\na vector ﬁeld mapping each point x∈cto a new position\nvθc : x↦→Aθc x. These velocity ﬁelds are then integrated\nto form a trajectory for each image point x\nφθ(x; 1) = x+\n∫ 1\n0\nvθ(φ(x; τ))dτ.\nGiven boundary and invertibility constraints [Freifeld et al.,\n2017], such a collection of afﬁne matrices{Aθc }c⊂Ω deﬁnes\na diffeomorphic transformation Tθ : x↦→φθ(x,1).\nThe libcpab library [Detlefsen, 2018] provides an efﬁcient\nimplementation for this approach, speciﬁcally optimized\nfor use in a deep learning context where fast gradient\nevaluations are crucial. The author successfully employs\nCPAB-transformations within a Spatial Transformer Net-\nwork [Detlefsen et al., 2018].\n4 PROBABILISTIC SPATIAL\nTRANSFORMER NETWORK\nThe P-STN is a probabilistic extension of the STN, where\nwe replace the deterministic transformation θ(I) with a pos-\nterior over transformations p(θ|I). Figure 2 illustrates the\nproposed pipeline. We assume observed data of the form\nD= {yi,Ii}N\ni=1, where yis the target variable (e.g. class\nlabel), and I are observations of the covariates. For presen-\ntation purposes, we will consider the latter to be images, but\nthe approach applies to any spatio-temporal data.\n4.1 THE MODEL\nRecall that STNs are trained end-to-end for the downstream\ntask using only label information. Thus, while we observey,\nθis a latent variable. We model it to be governed by a second\nlatent variable λ. λis a precision parameter, effectively stop-\nping the localization distribution (i.e. the amount of ‘data\naugmentation’ we introduce) from collapsing. The neces-\nsity for non-collapsing augmentation is discussed in Benton\net al. [2020], Van der Wilk et al. [2018] and Schwöbel et al.\n[2022].\ny θ\nλI\nN\nFigure 3: A graphical representation of the model structure.\nGrey nodes are observables and white are latents.\nWe wish to infer the latent variables in a Bayesian manner.\nThis entails computing the (log-)marginal likelihood of the\nobserved\nlog p(I,y) = log\n∫∫\np(I,y,θ,λ )dθdλ. (2)\nWe let the joint distribution factorize as (see Fig. 3)\np(y,I,θ,λ ) = p(y|I,θ,λ )p(I,θ,λ ) (3)\n= p(y|I,θ)p(θ|λ,I)p(λ)p(I). (4)\nNotice p(I) is unaffected by model parameters λand θ, and\nin this sense can be speciﬁed without affecting the model.\nThe distribution over θdepends on observed covariates in\nthe following way\np(θ|λ,I) = N(θ|µ(I),1/λ), (5)\nwhere µ(I) is a function parametrised by a neural network,\ni.e. µ(I) := µΦ(I) for model parameters Φ. The prior over\nλis a Gamma distribution, i.e.\np(λi) = Γ(α0,β0). (6)\nWe note here that there is one λi associated to\neach observation, and they are assumed to factorize:\np(λ) = ∏N\ni=1 p(λi). This choice of conjugate priors\nfor variance estimation is similar to [Stirn and Knowles,\n2020, Takahashi et al., 2018, Detlefsen et al., 2019]. Finally,\nwe assume that, conditional on I and θ, we have marginal\nindependence in y, i.e. p(y|I,θ) = ∏N\ni=1 p(yi|Ii,θi).\n4.2 V ARIATIONAL APPROXIMATION\nThe integral equation (2) for the marginal likelihood is in-\ntractable and, thus, the posterior p(λ,θ|I,y) is too. We de-\nrive a lower bound on the log marginal likelihood to utilize\nvariational inference [Blei et al., 2017]. We choose the vari-\national approximation qof the posterior p(θ,λ|I,y) as\nq(θ,λ) := p(θ|λ,I)q(λ). (7)\nHere p(θ|λ,I) is given as before and q(λ) :=∏N\ni=1 Γ (αi,β(Ii)). In our approximation, βis a neural net-\nwork: hence, we use amortized inference in a similar way to\nthe V AE model [Kingma and Welling, 2014].\nWe derive our lower bound using Jensen’s inequality\nlog p(y,I) = log\n∫∫\np(y,I,θ,λ )dθdλ (8)\n≥\n∫∫\nlog\n(p(y,I,θ,λ )\nq(θ,λ)\n)\nq(θ,λ)dθdλ (9)\n=\n∫∫\nlog\n(p(y|I,θ)p(λ)p(I)\nq(λ)\n)\np(θ|λ,I)q(λ)dθdλ\n= Eq(θ,λ) log p(y|I,θ)  \nclassiﬁcation loss\n+ logp(I) −KL(q(λ)∥p(λ)) .\n(10)\nThus, our evidence lower bound (ELBO) objective func-\ntion (10), consists of two terms: a classiﬁcation loss and a\nKL-term controlling the distance of the approximate poste-\nrior to the prior. During inference, we can disregardlog p(I)\nas it does not depend on parameters of interest.\n4.3 INFERENCE\nThe choice of variational posterior implies the following for\nthe classiﬁcation loss\nEq(θ,λ) log p(y|I,θ) (11)\n=\n∫∫\nlog p(y|I,θ)q(θ,λ)dθdλ (12)\n=\n∫∫\nlog p(y|I,θ)p(θ|λ,I)q(λ)dθdλ (13)\n=\n∫\nlog p(y|I,θ)\n∫\nN(θ|µ(I),λ)Γ(λ|α,β(I))dλdθ\n=\n∫\nlog p(y|I,θ)t2α(θ|µ(I)),β(I)\nα )dθ. (14)\nHere t denotes a scaled and location-shifted Student’s t-\ndistribution with mean µ(I), scaling β, and αdegrees of\nfreedom. For clarity, the marginalized q(θ) is t-distributed.\nHere p(y|I,θ) is what previously was referred to as\np(y|Tθ(I)), i.e. the classiﬁer conditioned the transformed I.\nWe approximate Eq. 14 using an unbiased estimate\nEq(θ,λ) log p(yi|Ii,θi) ≈1\nS\nS∑\ns=1\nlog p(yi|Ii,θi,s), (15)\nwith θi,s ∼t2αi (·|µ(Ii)),β(Ii)\nαi\n) (16)\nand backpropagate through neural networks µ(I) and β(I)\nwith the reparametrization trick. In all experiments αi=1.\nCombining terms, the ﬁnal ELBO we maximize becomes\nLp,q(I,y) ≈\nN∑\ni=1\n1\nS\nS∑\ns=1\nlog p(yi|Ii,θi,s)\n−KL (q(λ)||p(λ)) + const,\n(17)\nwhich is readily optimized using any gradient-based method.\nThe KL-term is analytically tractable and differentiable be-\ntween two gamma distributions.\nIn practice, following Higgins et al. [2016] we introduce\na weight parameter wto the KL-term. This requires us to\ntune w but in turn makes the model robust to the choice\nof prior. We perform a grid-search on a validation set to\nﬁnd the optimal w. Alternatively, we could have done a grid\nsearch over β0; instead we chose α0 = β0 = 1 for all ex-\nperiments. Similar to Kingma and Welling [2014], we often\nﬁnd it sufﬁcient to draw only S = 1 samples during train-\ning. Note that our model naturally implies marginalization,\nand correspondingly data augmentation, at test-time as well\nas the usual training time. At test time, we draw S = 10\ntransformation samples.\n5 EXPERIMENTS & RESULTS\nOur model consists of two parts, the classiﬁer p(y|Tθ(I))\nand the probabilistic localizer estimating the distribution\nover transformations. In the following experiments, we\naim to disentangle our model’s beneﬁts for localization\n(Sec. 5.1), classiﬁcation (Sec. 5.2) and calibration (Sec. 5.3).\nThe probabilistic localizer estimates q(θ) =\nt2(θ|µ(I),β(I)), i.e. in practice we implement a mean and\na variance network, µ(I) and β(I), respectively (see Fig. 2\nfor the architecture). We employ a small convolutional\nnetwork ( Conv2d, Maxpool2d, ReLU, Conv2d,\nMaxpool2d, ReLU) followed by two fully connected\nlayers for both the localizer and classiﬁer unless stated\notherwise. The P-STN localizer has two heads; one for the\nmean and one for the variance. The number of parameters\nis stated in each experimental subsection. Unless stated\notherwise, we keep the number of parameters constant, i.e.\nwhen adding a localization network we remove the extra\nparameters from the classiﬁer for fair comparison.\nOur model is implemented in PyTorch and experi-\nments are run on 12 GB Nvidia Titan X GPUs.\nThe code is available at https://github.com/\nFrederikWarburg/pSTN-baselines.\n5.1 MARGINALIZING TRANSFORMATIONS\nIMPROVES LOCALIZATION ACCURACY\nThe appeal of STN models is that they are trained end-to-\nend, i.e. based only on labels for the downstream task, and\nnot the transformations. This same property, however, is\nwhat makes the STN hard to ﬁt. The only signal we obtain is\nthrough the supervised downstream task (i.e. the classiﬁca-\ntion labels) and thus gradient information is sparse. We will\nnow investigate whether estimating a posterior over trans-\nformations and marginalizing, i.e. ‘getting to try multiple\ntransformations’, simpliﬁes the task as suggested by Fig. 1.\nFigure 4: Rotated MNIST experiment. Left panel: Ground-\ntruth transformation (rotation angles in radians) against re-\ncovered transformations (mean). Top right:Example images\nfrom the data set and samples from the P-STN localizer. The\nlocalizer learns to pose-normalize. Bottom right: Outputs of\nthe variance network. When the transformation recovery is\npoor (the error εis above the median, in orange) the vari-\nances are slightly higher than when the localization works\nwell (blue).\nIn order to disentangle the localization from the classiﬁca-\ntion task, we construct the following experiments. We ﬁrst\ntrain a CNN on a pose-normalized dataset (regular MNIST\nand Fashion MNIST). We then generate a new dataset by\nrandomly sampling transformations θtrue and applying them\nto the MNIST images. Saving these transformations pro-\nvides us with ground truth. We freeze the CNN weights and\ntrain STN and P-STN with this ﬁxed classiﬁer, effectively\nlearning to recover and ‘undo’ the true transformations.\n5.1.1 Rotated MNIST\nFrom this data-generating process, we obtain a rotated ver-\nsion of the MNIST dataset (i.e. regular MNIST with ground-\ntruth transformations given by rotation angles, θtrue(I) =\nrtrue(I)). See Fig. 4, top right panel for example data.\nOur CNN classiﬁer ( 28k weights) obtains 99.4% test ac-\ncuracy on MNIST and 41.2% on rotated MNIST (frozen\nweights, no re-training). The STN and P-STN (S=10 train-\ning samples, w= 3·10−5, same CNN classiﬁer as before\n+72k params in the localizer) both learn to pose-normalize,\ni.e. to recover these transformations to a satisfactory degree.\nWhen training the localizers only (classiﬁer weights remain\nfrozen as described above), the STN test acc. is76.13%, and\n82.98% for the P-STN. We compute the expected average\ntransformation error on the N = 10krotated MNIST test\nimages as\nε= 1\nN\nN∑\ni=1\n∥θtrue(Ii) −µ(Ii)∥ mod π. (18)\nFigure 6: The P-STN learns to localize trafﬁc signs in the challenging MTSD\ndataset. At test time, we sample 10 transformations as shown with the various\nbounding boxes overlaid the images. These learned variations improve the ﬁnal\nclassiﬁcation.\nAcc. ↑ NLL ↓\nCNN 76.0 0 .49\nSTN 90.6 0 .31\nP-STN 92.2 0 .29\nTable 1: Accuracy\n(Acc.) and negative\nlog-likelihood (NLL) for\nCNN, STN and P-STN.\nWe get ε= 0.76 for the STN and ε= 0.59 for the P-STN.\nThe P-STN outperforms the STN, i.e. modeling uncertainty\nin the transformations helps in the localization task.\nUncertainty. The bottom right panel of Fig. 4 shows a\nhistogram of β(I), i.e. the localizer variance (or, correspond-\ningly, the magnitude of augmentation) per image. In orange,\nwe plot variances for images where pose-normalization is\ndifﬁcult (the transformation error εis larger than the me-\ndian). In blue, we plot variances for images that are correctly\npose-normalized (transformation error εsmaller than the\nmedian). The poorly localized images are, on average, as-\nsigned 17% larger variances β(I). The localizer uncertainty\nand thus the amount of data augmentation applied is some-\nwhat meaningful, corresponding to the difﬁculty of the task.\n5.1.2 Random placement FashionMNIST\nWe repeat a similar experiment on the slightly more chal-\nlenging FashionMNIST dataset [Xiao et al., 2017] . The\nCNN baseline accuracy is 90.63% (same model as above\nwith 28k parameters). We then randomly sample an xand y\ncoordinate and place the FashionMNIST accordingly on a\nblack background, after downscaling it by 50%. No rotation\nis applied, i.e. θtrue = [0,0.5,tx\ntrue,ty\ntrue].\nFigure 7: Random Placement Fashion MNIST. Input images\n(left) and transformed samples Tθs (I) as learned by the P-\nSTN. The P-STN learns to correctly pose-normalize and\nzoom into the relevant part of the image. The samples look\nlike plausible candidates for a data augmentation scheme.\nWe will explore this in Sec. 5.2.\nLike in the previous experiment, both localizers success-\nfully recoverθtrue, with the P-STN (S = 10 training samples,\nw= 3e−05, same classiﬁer as before+193k weights in the\nlocalizer) doing slightly better than its deterministic counter-\npart: test accuracies are 84.99% and 84.41%, respectively.\nInspecting the transformation posterior and the resulting\nsamples Tθs (Ii), we ﬁnd that these look visually pleasing,\nand, as hypothesized, might be promising candidates for a\ndata augmentation scheme. We will explore this in Sec. 5.2.\n5.1.3 Mapillary street signs\nDetection and classiﬁcation of objects in images have many\napplications, e.g. for autonomous vehicles, detecting trafﬁc\nsigns is crucial. We compare a top-performing classiﬁer, an\nSTN and our P-STN on the challenging Mapillary Trafﬁc\nSign Dataset (MTSD) [Ertler et al., 2019].\nTo focus this comparison, we select images that contain\nonly one trafﬁc sign. We obtain this subset by selecting all\nbounding boxes that do not intersect with other bounding\nboxes plus a margin of150 px to each side. We further select\nthe ten most common classes from this subset. This gives us\na training set of 4698 images and a test set of 500 images.\nFigure 6 shows example images from the chosen subset.\nOur classiﬁer is a ResNet18 pre-trained on ImageNet, where\nwe replace the last fully connected layer. We use the same\nResNet for the localizers in the STN and P-STN, where we\nsimilarly replace the last layer. As before, we wish to study\nthe behavior of the localizers. Therefore, we again start by\ntraining a classiﬁer on the ground-truth bounding boxes. We\nthen initialize the classiﬁer module of the STN and P-STN\nwith this pre-trained classiﬁer and freeze the weights of the\nclassiﬁer. We train the localizers of the STN and P-STN for\n60 epochs with learning rate 10−4 and kl weight w= 10−7.\nFigure 6 shows that the P-STN learns to localize the trafﬁc\nsigns. At test time, we sample 10 transformations illustrated\nby the multiple overlaying bounding boxes.\nTable 1 shows that both the STN and P-STN clearly outper-\nform the baseline classiﬁer when trained on the full images.\nEven though the STN and P-STN have exactly the same\nclassiﬁer, the P-STN achieves better performance because\nof the ensemble of classiﬁed transformations.\nMNIST30 MNIST100 MNIST1000 MNIST3000 MNIST10000\nCNN 70.12 ±2.46 87 .29 ±0.58 95 .80 ±0.33 97.48 ±0.21 97.82 ±0.34 -\nafﬁne STN 69.26 ±4.53 82 .16 ±2.30 92 .05 ±0.58 94 .71 ±0.22 96 .96 ±0.20\nafﬁne P-STN 81.00 ±3.92 92.70 ±0.74 96.62 ±0.58 97.33 ±0.17 97.63 ±0.23\noptimal w 0.001 0 .0003 0 .0001 0 .00003 0 .00001\nTable 2: The performance of a CNN, STN and P-STN on differently sized MNIST datasets. Bold numbers indicate that a\nmodel is signiﬁcantly better than the runner up under a two sample t-test at p= 0.05.\nMNIST30 MNIST100 MNIST1000 MNIST3000 MNIST10000\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95Accuracy\nMNIST subsets\nP-STN\nSTN\nCNN\nFigure 8: Performances of P-STN, STN and CNN on\nMNIST subsets (mean ±one STD across ﬁve folds).\n5.2 MARGINALIZING TRANSFORMATIONS\nIMPROVES CLASSIFICATION ACCURACY\nWe have argued that marginalizing transformations via sam-\nples corresponds to learned, localized data augmentations\n(the samples Tθs (I)). We will now investigate whether these\naugmentations are indeed helpful in the downstream task,\ni.e. whether they improve classiﬁcation performance.\n5.2.1 MNIST and subsets\nWe compare the performance of our P-STN against a stan-\ndard convolutional neural network (CNN) and a regular STN\non MNIST. The standard MNIST images are centered and\npose-normalized, so the localization task is easy. Improved\nclassiﬁer performance can thus be viewed as an indicator\nfor having learned a useful data augmentation scheme.\nData augmentation is particularly important when training\ndata is scarce, so we evaluate the models on small subsets\nof MNIST: MNIST30 contains 30 images (i.e. 3 per class),\nMNIST100, MNIST1000, MNIST3000 and MNIST10000.\nSTN and P-STN parameterize afﬁne transformations, i.e.\nthe learned θis interpreted as the full afﬁne matrix as de-\nscribed in Sec. 3. All models have roughly 28k parameters,\narchitecture as described at the top of Sec. 5. We use the\nAdam optimizer with weight decay 0.01 and the default\nparameters of its PyTorch implementation. The images are\ncolor-normalized. We repeat the experiment5 times, each\ntime with a different k-image subset of the MNIST dataset,\nand we report ±one standard deviation in tables and error\nbars. From Table 2 and Fig. 8, we see that the P-STN out-\nperforms both the STN and CNN on the small dataset sizes.\nFor the larger datasets, the differences vanish. This supports\nour hypothesis: data augmentation is especially useful when\ndata is a limited resource. This intuition is also supported\nby the optimal KL-weights (Table 2, bottom row) that we\ndetermine via grid search on validation data. For smaller\ndatasets, larger wand thus more regularization towards the\nvariance prior (away from 0) are beneﬁcial.\nThe fact that the STN performs less well than the standard\nCNN on this data set might be explained by the fact that the\nimages are already nearly perfectly pose-normalized, and\nwrong transformations can be detrimental.\n5.2.2 UCR time-series dataset\nFor some data modalities, such as time-series, it is not\ntrivial to craft a useful data augmentation scheme. In this\nexperiment, we show that the P-STN can learn a useful,\nnon-trivial data augmentation scheme that increases perfor-\nmance compared to a standard STN on time-series data. The\nUCR dataset [Dau et al., 2018] is composed of 108 smaller\ndatasets, where each dataset contains univariate time-series.\nThe FordA dataset, for example, contains measurements of\nengine noise over time and the goal is to classify whether or\nnot the car is faulty. We select 5 of those subsets, each large\nenough to divide into training and validation sets (75/25%),\nwhich we use to ﬁnd the optimal wvia grid-search; those\nare [0.0001,1e−05,0.001,0.0,0.0001]. We draw S = 10\ntraining samples. The test-set is pre-deﬁned by the dataset\ncurators. Learning rate and optimizer are the same as in\nSec. 5.2.1, but we do not perform normalization. All models\nhave approximately one million parameters. Table 3 shows\nthat the P-STN achieves higher mean accuracy than both\nthe STN and the CNN, indicating that we can automatically\nlearn a useful data augmentation scheme for time-series.\nWe verify this qualitatively in Fig. 10, which shows an\nFigure 10: Examples of augmentations for a time-\nseries from the FaceAll dataset. The top plot\nshows the original time-series and the bottom\nplot shows three augmented versions of the time-\nseries.\nCNN STN P-STN\nFaceAll 80.83±0.62 82 .28±0.42 84.31±0.75\nTwoPatterns 97.92±0.53 99 .79±0.04 99.96±0.04\nwafer 99.63±0.05 99 .18±0.17 - 98.86±0.20\nuWaveGestureLib.* 74.15±1.27 79 .77±0.42 - 81.13±0.46\nPhalangesOutlC.** 79.88±1.32 82.26±0.98 81.66±0.59\nMean 86.48 88 .65 89.18\nTable 3: Accuracies on a subset of the UCR timeseries dataset (full\ndataset names are *uWaveGestureLibrary and **PhalangesOutli-\nnesCorrect). ±1 STD is reported after 5 repetitions. Bold numbers\nindicate that a model is signiﬁcantly better than the runner up under\na two sample t-test at p= 0.05.\n0.0 0.2 0.4 0.6 0.8 1.0\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nMNIST100 Calibration\nCNN\nSTN\nP-STN w = 0.0003\nP-STN w = 0.0001\nFigure 11: Calibration plots for CNN, STN and two P-STN\nmodels. One with KL-weight yielding optimal performance\n(w = 0 .0003) and one with KL-weight yielding optimal\ncalibration (w = 0.0001). Both P-STN models are better\ncalibrated than CNN and STN.\nexample of the learned data augmentation. We see that the\nmodel does not simply apply a global transformation, but\nlearns to augment the time-series more in some intervals,\nsuch as in [60; 110], and augment the time-series less in\nother intervals, such as in [0; 50].\n5.3 MARGINALIZING TRANSFORMATIONS\nIMPROVES CALIBRATION\nIn Sec. 5.1, we have seen that harder images on average\nhave larger transformation uncertainties. We now investi-\ngate whether those meaningful localization uncertainties\ntranslate into meaningful uncertainties downstream, i.e. in\nthe calibration of our classiﬁer. At test-time, we evaluate\np(y|I) =\n∫\np(y|I,θ)q(θ)dθ≈1\nS\nS∑\ns=1\np(y|Tθs (I)). (19)\nWe will investigate how well the uncertainty in this distri-\nbution matches the quality of predictions. Fig. 11 shows a\ncalibration plot for the MNIST100 subset classiﬁcation task\nfrom Sec. 5.2.1 for the CNN, STN and P-STN for two differ-\nent w-parameters; w= 0.0003 yields the best performance\n(reported in Table 2) and w = 0.0001 yields the best cali-\nbration. The expected calibration errors [Guo et al., 2017,\nKüppers et al., 2020, 2021] are CNN: 0.0743 ±0.0094,\nSTN: 0.1160 ±0.0205, P-STN, w = 0.0003 (optimal per-\nformance model): 0.0567 ±0.0065, P-STN, w = 0.0001\n(optimal calibration model): 0.0271 ±0.0088. We report\nthe mean over 5 folds, ±one STD. The P-STN signiﬁcantly\nimproves calibration in the downstream classiﬁcation task.\n5.4 A TYPICAL FAILURE MODE IN STNS\nSTNs are trained end-to-end, and with only label informa-\ntion available. Thus, the aim is to learn the optimal transfor-\nmation for solving the downstream task. Depending on the\ncomplexity of the downstream task and the classiﬁcation\nmodel, it might not be necessary to transform the input at\nall, i.e. it might be possible to solve the downstream task\non the original input image. Indeed, this is a failure mode\nwe observe in practice — often, the localizer simply learns\nthe identity transform while the classiﬁer learns to classify\nthe non-transformed image. Using more complex classi-\nﬁer architectures makes the STN more prone to this failure\nmode. This has been observed by other authors [Finnve-\nden et al., 2021], and we investigate the problem in the\nexperiment in Fig. 12. We start by training differently-sized\nneural networks on MNIST (black, one layer on the x-axis\nis [Linear, ReLU, Dropout]). We compare the per-\nformance of this model with (P-)STN models trained on\nrotated MNIST, test accuracies are plotted in the left panel\nof the ﬁgure. If the localization task is performed perfectly,\nthe (P-)STN models should be able to recover the accuracy\non the original, non-rotated dataset. In the right panel, we\nplot the variance of the (mean) transformations learned by\nthe (P-)STN models. Values close to 0 indicate that the\nlocalizer does not transform the image, i.e. it learns the\n1 layers 2 layers 3 layers 4 layers 5 layers\nModel size\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\n MNIST, NN\nrotMNIST, P-STN\nrotMNIST, STN\nrotMNIST, NN\n1 layers 2 layers 3 layers 4 layers 5 layers\nModel size\n0.0\n0.25\n0.5\n0.75\n1.0\n1.25Variance of (I) (radians)\n rotMNIST, STN\nrotMNIST, PSTN\nFigure 12: Left: Test accuracies for standard NN and (P-)STNs of different depths trained on rotated MNIST, as well as\nNN baseline on original MNIST (black). The STN (green) model does not usually recover the original images and thus\nbehaves more like a standard NN (red) in most runs. P-STN (blue) un-transforms at least some of the rotations and is closer\nin accuracy to the NN on original MNIST (black). Right: The variance of the learned transformations as a function of model\ndepth. The STN learns the identity for deeper downstream models (this is consistent with the test accuracies we see on the\nleft). P-STN learns to un-transform better, at least when the classiﬁer is simple. For bigger classiﬁers it predicts the identity\ntransform as well, but performs relatively well nonetheless (see left panel). We report medians±1 median absolute deviation\nover 5 folds.\nidentity transform. Larger values indicate that the localizer\nlearns transformations. Median results are reported over 5\nruns, error bars correspond to one mean absolute deviation.\nAs hypothesized, for larger classiﬁers the localizers do not\ntransform the images. Due to the increased capacity of the\nmodel, we nonetheless achieve decent classiﬁcation accu-\nracies (left panel). The P-STN learns to localize the rotated\nimages somewhat successfully (large variance in the right\npanel, and high accuracy on the left) for smaller classiﬁers.\nThe STN does not localize the images as well, most runs\nbehave like the standard NN on rotMNIST (red), predicting\nidentity transformations only. We conclude that, thanks to\nit ‘trying out multiple transformations’, the P-STN avoids\nthis failure mode to an extent. We also note that this prop-\nerty, while useful, is somewhat orthogonal to our interest\nin this work, and we have avoided the failure mode in the\nexperiments of Sec. 5.1 by considering models with ﬁxed,\npre-trained classiﬁers.\n6 CONCLUSION\nWe have introduced a probabilistic extension to the spa-\ntial transformer network (STN) [Jaderberg et al., 2015]. Our\nwork took motivation from the empirical observation that the\nSTN is often brittle to train, as a poorly predicted transforma-\ntion may prevent the model from getting any gradient signal,\nresulting in divergent optimization. Our probabilistic STN\n(P-STN) instead approximates the posterior distribution of\ntransformations using amortized variational inference, and\nmarginalizes accordingly. As is common, marginalization\nimproves the robustness of the model.\nEmpirically, we note the following advantages of the prob-\nabilistic formulation over the deterministic. Firstly, the per-\nformance of the localization network is improved, since the\nMonte Carlo marginalization effectively amounts to trying\nmany different transformations. Secondly, the probabilistic\nformulation improves the overall model performance, since\nthe sampled transformations act as data augmentation both\nduring training and during testing. The resulting ensemble of\npredictions is more accurate and better calibrated than com-\nmon classiﬁers as well as the original spatial transformer.\nAcknowledgements\nMJ was supported by a research grant from the Carlsberg\nFoundation (CF20-0370). SH was supported by research\ngrants (15334, 42062) from VILLUM FONDEN. This\nproject has also received funding from the European Re-\nsearch Council (ERC) under the European Union’s Horizon\n2020 research and innovation programme (grant agreement\nNo. 757360). This work was funded in part by the Novo\nNordisk Foundation through the Center for Basic Machine\nLearning Research in Life Science (NNF20OC0062606).\nReferences\nHenry S Baird. Document image defect models. In SDIA,\npages 546–556. Springer, 1992.\nGregory Benton, Marc Finzi, Pavel Izmailov, and An-\ndrew Gordon Wilson. Learning invariances in neural\nnetworks. arXiv preprint arXiv:2010.11882, 2020.\nDavid M. Blei, Alp Kucukelbir, and Jon D. McAuliffe.\nVariational inference: A review for statisticians. ArXiv,\nabs/1601.00670, 2017.\nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu,\nand Daan Wierstra. Weight uncertainty in neural net-\nworks. arXiv preprint arXiv:1505.05424, 2015.\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Va-\nsudevan, and Quoc V Le. Autoaugment: Learning aug-\nmentation strategies from data. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 113–123, 2019.\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmenta-\ntion with a reduced search space. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, pages 702–703, 2020.\nHoang Anh Dau, Eamonn Keogh, Kaveh Kamgar, Chin-\nChia Michael Yeh, Yan Zhu, Shaghayegh Gharghabi,\nChotirat Ann Ratanamahatana, Yanping, Bing Hu, Nurja-\nhan Begum, Anthony Bagnall, Abdullah Mueen, Gustavo\nBatista, and Hexagon-ML. The ucr time series classiﬁca-\ntion archive, October 2018. https://www.cs.ucr.\nedu/~eamonn/time_series_data_2018/.\nErik Daxberger, Agustinus Kristiadi, Alexander Immer,\nRuna Eschenhagen, Matthias Bauer, and Philipp Hen-\nnig. Laplace redux-effortless bayesian deep learning.\nAdvances in Neural Information Processing Systems, 34,\n2021.\nNicki Skafte Detlefsen. libcpab. https://github.\ncom/SkafteNicki/libcpab, 2018.\nNicki Skafte Detlefsen and Søren Hauberg. Explicit disen-\ntanglement of appearance and perspective in generative\nmodels. In Advances in Neural Information Processing\nSystems (NeurIPS), 2019.\nNicki Skafte Detlefsen, Oren Freifeld, and Søren Hauberg.\nDeep diffeomorphic transformer networks. In 2018\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4403–4412, June 2018.\nNicki Skafte Detlefsen, Martin Jørgensen, and Søren\nHauberg. Reliable training and estimation of variance\nnetworks. In 33rd Conference on Neural Information\nProcessing Systems, 2019.\nChristian Ertler, Jerneja Mislej, Tobias Ollmann, Lorenzo\nPorzi, and Yubin Kuang. Trafﬁc sign detection and classi-\nﬁcation around the world. CoRR, abs/1909.04422, 2019.\nURL http://arxiv.org/abs/1909.04422.\nLukas Finnveden, Ylva Jansson, and Tony Lindeberg. Un-\nderstanding when spatial transformer networks do not\nsupport invariance, and what to do about it. In 2020 25th\nInternational Conference on Pattern Recognition (ICPR),\npages 3427–3434. IEEE, 2021.\nOren Freifeld, Søren Hauberg, Kayhan Batmanghelich, and\nJohn W. Fisher. Transformations based on continuous\npiecewise-afﬁne velocity ﬁelds. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2017.\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian\napproximation: Representing model uncertainty in deep\nlearning. In international conference on machine learn-\ning, pages 1050–1059. PMLR, 2016.\nIan Goodfellow, Honglak Lee, Quoc V Le, Andrew Saxe,\nand Andrew Y Ng. Measuring invariances in deep net-\nworks. In NIPS, pages 646–654, 2009.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. On calibration of modern neural networks. In\nProceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 1321–1330. JMLR. org,\n2017.\nFredrik K Gustafsson, Martin Danelljan, and Thomas B\nSchon. Evaluating scalable bayesian deep learning meth-\nods for robust computer vision. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition workshops, pages 318–319, 2020.\nSøren Hauberg, Oren Freifeld, Anders Boesen Lindbo\nLarsen, John W. Fisher, and Lars Kai Hansen. Dream-\ning more data: Class-dependent distributions over diffeo-\nmorphisms for learned data augmentation. In Artiﬁcial\nIntelligence and Statistics, pages 342–350, 2016.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,\nXavier Glorot, Matthew Botvinick, Shakir Mohamed, and\nAlexander Lerchner. beta-vae: Learning basic visual con-\ncepts with a constrained variational framework. 2016.\nMax Jaderberg, Karen Simonyan, Andrew Zisserman, and\nKoray Kavukcuoglu. Spatial transformer networks. InAd-\nvances in Neural Information Processing Systems, pages\n2017–2025, 2015.\nAngjoo Kanazawa, David W Jacobs, and Manmohan Chan-\ndraker. Warpnet: Weakly supervised matching for single-\nview reconstruction. In CVPR, 2016.\nDiederik P. Kingma and Max Welling. Auto-encoding vari-\national bayes. In 2nd International Conference on Learn-\ning Representations, ICLR 2014, Banff, AB, Canada,\nApril 14-16, 2014, Conference Track Proceedings, 2014.\nURL http://arxiv.org/abs/1312.6114.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural\nnetworks. In F. Pereira, C. J. C. Burges, L. Bottou, and\nK. Q. Weinberger, editors,Advances in Neural Informa-\ntion Processing Systems 25 , pages 1097–1105. Curran\nAssociates, Inc., 2012.\nFabian Küppers, Jan Kronenberger, Amirhossein Shantia,\nand Anselm Haselhoff. Multivariate conﬁdence calibra-\ntion for object detection. In The IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR)\nWorkshops, June 2020.\nFabian Küppers, Jan Kronenberger, Jonas Schneider, and\nAnselm Haselhoff. Bayesian conﬁdence calibration for\nepistemic uncertainty modelling. In Proceedings of the\nIEEE Intelligent Vehicles Symposium (IV), July 2021.\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles\nBlundell. Simple and scalable predictive uncertainty es-\ntimation using deep ensembles. In Advances in Neural\nInformation Processing Systems, pages 6402–6413, 2017.\nYann LeCun, LD Jackel, Leon Bottou, A Brunot, Corinna\nCortes, JS Denker, Harris Drucker, I Guyon, UA Muller,\nEduard Sackinger, et al. Comparison of learning algo-\nrithms for handwritten digit recognition. In International\nconference on artiﬁcial neural networks , volume 60,\npages 53–60. Perth, Australia, 1995.\nChen-Hsuan Lin and Simon Lucey. Inverse compositional\nspatial transformer networks. CoRR, abs/1612.03897,\n2016. URL http://arxiv.org/abs/1612.\n03897.\nGaëlle Loosli, Stéphane Canu, and Léon Bottou. Training in-\nvariant support vector machines using selective sampling.\nLarge scale kernel machines, pages 301–320, 2007.\nDavid J C MacKay. Model comparison and Occam’s razor.\nInformation Theory, Inference and Learning Algorithms,\npages 343–355, 2003.\nDavid JC MacKay. Bayesian interpolation. Neural compu-\ntation, 4(3):415–447, 1992.\nPola Schwöbel, Martin Jørgensen, Sebastian W Ober, and\nMark Van Der Wilk. Last layer marginal likelihood for\ninvariance learning. In International Conference on Artiﬁ-\ncial Intelligence and Statistics, pages 3542–3555. PMLR,\n2022.\nRon A Shapira Weber, Matan Eyal, Nicki Skafte, Oren\nShriki, and Oren Freifeld. Diffeomorphic temporal align-\nment nets. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 32,\npages 6570–6581. 2019.\nPatrice Y Simard, Dave Steinkraus, and John C Platt. Best\npractices for convolutional neural networks applied to\nvisual document analysis. In 2013 12th International\nConference on Document Analysis and Recognition, vol-\nume 2, pages 958–958. IEEE Computer Society, 2003.\nSøren Kaae Sønderby, Casper Kaae Sønderby, Lars Maaløe,\nand Ole Winther. Recurrent spatial transformer networks.\narXiv preprint arXiv:1509.05329, 2015.\nAndrew Stirn and David A Knowles. Variational variance:\nSimple and reliable predictive variance parameterization.\narXiv e-prints, pages arXiv–2006, 2020.\nHiroshi Takahashi, Tomoharu Iwata, Yuki Yamanaka,\nMasanori Yamada, and Satoshi Yagi. Student-t varia-\ntional autoencoder for robust density estimation. InIJCAI,\npages 2696–2702, 2018.\nMark Van der Wilk, Matthias Bauer, ST John, and James\nHensman. Learning invariances using the marginal like-\nlihood. In Advances in Neural Information Processing\nSystems, pages 9938–9948, 2018.\nHan Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-\nmnist: a novel image dataset for benchmarking machine\nlearning algorithms. arXiv preprint arXiv:1708.07747,\n2017.",
  "topic": "Probabilistic logic",
  "concepts": [
    {
      "name": "Probabilistic logic",
      "score": 0.6647632718086243
    },
    {
      "name": "Transformer",
      "score": 0.5153157711029053
    },
    {
      "name": "Computer science",
      "score": 0.44253039360046387
    },
    {
      "name": "Artificial intelligence",
      "score": 0.286755234003067
    },
    {
      "name": "Engineering",
      "score": 0.1611621081829071
    },
    {
      "name": "Electrical engineering",
      "score": 0.12281662225723267
    },
    {
      "name": "Voltage",
      "score": 0.05768725275993347
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I96673099",
      "name": "Technical University of Denmark",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I2802123492",
      "name": "Oxford Research Group",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2802567020",
      "name": "Copenhagen University Hospital",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I2801942218",
      "name": "Hvidovre Hospital",
      "country": "DK"
    }
  ],
  "cited_by": 4
}