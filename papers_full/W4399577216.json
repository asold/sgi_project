{
    "title": "An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets",
    "url": "https://openalex.org/W4399577216",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2996068036",
            "name": "Jonathan Katzy",
            "affiliations": [
                "Delft University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2107878683",
            "name": "Razvan Popescu",
            "affiliations": [
                "Delft University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2155442793",
            "name": "Arie van Deursen",
            "affiliations": [
                "Delft University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2521711437",
            "name": "Maliheh Izadi",
            "affiliations": [
                "Delft University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4385270018",
        "https://openalex.org/W4394744221",
        "https://openalex.org/W4307751519",
        "https://openalex.org/W2134852596",
        "https://openalex.org/W3098605233",
        "https://openalex.org/W2147209048",
        "https://openalex.org/W1992218759",
        "https://openalex.org/W4221166942",
        "https://openalex.org/W3138815606",
        "https://openalex.org/W4389009541",
        "https://openalex.org/W4312933868",
        "https://openalex.org/W4389519352",
        "https://openalex.org/W4389524330",
        "https://openalex.org/W4385572142",
        "https://openalex.org/W4396870811"
    ],
    "abstract": "Does the training of large language models potentially infringe upon code licenses? Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses? In our study, we assess the current trends in the field and the importance of incorporating code into the training of large language models. Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future. To accomplish this, we compiled a list of 53 large language models trained on file-level code. We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code.",
    "full_text": null
}