{
  "title": "AQT: Adversarial Query Transformers for Domain Adaptive Object Detection",
  "url": "https://openalex.org/W4285600811",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2257311816",
      "name": "Wei-Jie Huang",
      "affiliations": [
        "National Yang Ming Chiao Tung University"
      ]
    },
    {
      "id": "https://openalex.org/A2112083208",
      "name": "Yu‚ÄêLin Lu",
      "affiliations": [
        "National Yang Ming Chiao Tung University"
      ]
    },
    {
      "id": "https://openalex.org/A2131014102",
      "name": "Shih‚ÄêYao Lin",
      "affiliations": [
        "Sony Corporation (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2102008498",
      "name": "Yusheng Xie",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2139473404",
      "name": "Yen-Yu Lin",
      "affiliations": [
        "Academia Sinica",
        "National Yang Ming Chiao Tung University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035564946",
    "https://openalex.org/W3202277637",
    "https://openalex.org/W3034779842",
    "https://openalex.org/W3035175896",
    "https://openalex.org/W2748021867",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W2979548969",
    "https://openalex.org/W2593768305",
    "https://openalex.org/W2990069979",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964115968",
    "https://openalex.org/W3174551531",
    "https://openalex.org/W3204821053",
    "https://openalex.org/W3194643899",
    "https://openalex.org/W3034937575",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W3206713300",
    "https://openalex.org/W3176895448",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2968634921",
    "https://openalex.org/W4287281266",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3166409449",
    "https://openalex.org/W3110011650",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W3212135534",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2962808524",
    "https://openalex.org/W2963730616",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2187089797"
  ],
  "abstract": "Adversarial feature alignment is widely used in domain adaptive object detection. Despite the effectiveness on CNN-based detectors, its applicability to transformer-based detectors is less studied. In this paper, we present AQT (adversarial query transformers) to integrate adversarial feature alignment into detection transformers. The generator is a detection transformer which yields a sequence of feature tokens, and the discriminator consists of a novel adversarial token and a stack of cross-attention layers. The cross-attention layers take the adversarial token as the query and the feature tokens from the generator as the key-value pairs. Through adversarial learning, the adversarial token in the discriminator attends to the domain-specific feature tokens, while the generator produces domain-invariant features, especially on the attended tokens, hence realizing adversarial feature alignment on transformers. Thorough experiments over several domain adaptive object detection benchmarks demonstrate that our approach performs favorably against the state-of-the-art methods. Source code is available at https://github.com/weii41392/AQT.",
  "full_text": "AQT: Adversarial Query Transformers for Domain Adaptive Object Detection\nWei-Jie Huang1 , Yu-Lin Lu1 , Shih-Yao Lin2‚àó , Yusheng Xie3‚Ä† and Yen-Yu Lin1,4\n1National Yang Ming Chiao Tung University\n2Sony Corporation of America\n3Amazon\n4Academia Sinica\nAbstract\nAdversarial feature alignment is widely used in do-\nmain adaptive object detection. Despite the ef-\nfectiveness on CNN-based detectors, its applica-\nbility to transformer-based detectors is less stud-\nied. In this paper, we present AQT (adversarial\nquery transformers) to integrate adversarial feature\nalignment into detection transformers. The gen-\nerator is a detection transformer which yields a\nsequence of feature tokens, and the discriminator\nconsists of a novel adversarial token and a stack\nof cross-attention layers. The cross-attention lay-\ners take the adversarial token as the query and\nthe feature tokens from the generator as the key-\nvalue pairs. Through adversarial learning, the\nadversarial token in the discriminator attends to\nthe domain-specific feature tokens, while the gen-\nerator produces domain-invariant features, espe-\ncially on the attended tokens, hence realizing ad-\nversarial feature alignment on transformers. Thor-\nough experiments over several domain adaptive ob-\nject detection benchmarks demonstrate that our ap-\nproach performs favorably against the state-of-the-\nart methods. Source code is available at https:\n//github.com/weii41392/AQT.\n1 Introduction\nObject detection is active in computer vision and artificial in-\ntelligence researches because it is essential to a broad range of\nreal-world applications, such as surveillance and self-driving\ncars. While the latest object detection methods [Ren et al.,\n2015; Liu et al., 2016; Tian et al., 2019 ] have shown great\nsuccess in several challenging benchmarks [Lin et al., 2014],\ntheir capabilities largely rely on massive labeled data. More\nimportantly, applying pretrained object detectors to new envi-\nronments would result in performance degradation due to the\ndistribution mismatch between training data and deployed en-\nvironments.\nUnsupervised domain adaptation (UDA) [Ganin and Lem-\npitsky, 2015; Tzeng et al., 2017 ] has been developed to\n‚àóWork done outside of Sony\n‚Ä†Work done outside of Amazon\nDiscriminator\ncross-attention\nadversarial token\ncross-attention\n‚Ñíùëéùëëùë£\nQ\nQ\nGenerator\nself-attention\nfeature tokens\nself-attention\n‚Ñíùëëùëíùë°\nK V Q K V\nK V Q K V\nFigure 1: Our adversarial feature alignment for detection transform-\ners. The discriminator in adversarial learning consists of a learnable\nadversarial token and cross-attention layers, while the generator is a\ndetection transformer. For each cross-attention layer, the adversarial\ntoken serves as the query (Q) and the feature tokens as thekey-value\npair (K and V ). To minimize the adversarial loss Ladv, the adver-\nsarial token attends to the feature tokens discriminative for domain\nclassification. On the contrary, the generator is forced to eliminate\nthe domain-specific features to maximize Ladv. As constrained by\nthe detection loss Ldet, the generator prevents semantic collapse in\nthe feature tokens in the meantime. Adversarial feature alignment is\nthus carried out.\nmitigate these issues. In UDA, a model is usually trained\nwith data from a source domain and a similar but differ-\nent target domain, while the data labels are only available\nin the source domain. Under the covariate shift assump-\ntion, domain adaptation is typically carried out by minimiz-\ning cross-domain discrepancy so that the model supervised\non the source domain can learn knowledge and representa-\ntions generalizable to the target domain. Following the estab-\nlished practices for classification[Tzeng et al., 2017] and seg-\nmentation [Hoffman et al., 2018] tasks, many domain adap-\ntive object detectors [Saito et al., 2019; Chen et al., 2020;\nRezaeianaran et al., 2021 ] adopt adversarial feature align-\nment. By introducing a domain classifier, a learnable measure\nof domain shifts can be used to impose minimax objectives\nand to force domain invariance. The resultant models tend to\nbe free from biases towards the source domain.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n972\nDespite its effectiveness on CNN-based detectors, adver-\nsarial feature alignment is less studied on transformer-based\ndetectors [Carion et al., 2020; Zhu et al., 2021 ]. CNNs and\ntransformers are intrinsically different. CNNs capture vi-\nsual characteristics in features maps via convolutions, while\ntransformers model token-wise relationships via the attention\nmechanism. The recent studies [Wang et al., 2021a ] also\nshow that adversarial feature alignment on the CNN back-\nbone of detection transformers brings only limited improve-\nments for domain adaptation.\nTo address this issue, we present AQT (adversarial query\ntransformer) that combines adversarial learning and trans-\nformers for domain adaptive object detection. Since token-\nwise operations are implemented in transformers, it is logi-\ncal to conduct token-wise feature alignment and to focus on\nthe domain-specific tokens. Based on this observation, we\ndevelop a strategy for the proposed AQT. We illustrate its\nconceptual workflow in Figure 1. The generator is the orig-\ninal detection transformer (denoted by self-attention layers\nfor simplicity). It extracts a sequence of feature tokens from\nthe input image and further detects the objects from it. The\ndiscriminator is a learnable adversarial token and a stack of\ncross-attention layers. In each cross-attention layer, the ad-\nversarial token serves as the query and the intermediate fea-\nture tokens from the generator as the key-value pair. To min-\nimize the adversarial loss, the adversarial token tends to at-\ntend to the domain-specific feature tokens. On the contrary,\nthe generator is forced to generate domain-invariant features,\nespecially for the attended key-value pairs, to maximize the\nloss. As constrained by the detection loss Ldet, the generator\nalso has to prevent semantic collapse in the feature tokens.\nAdversarial feature alignment is thus carried out.\nThe proposed mechanism is flexible. First, it is aplug-and-\nadapt module and can work with many existing transformer-\nbased detectors [Carion et al., 2020; Zhu et al., 2021]. Sec-\nond, it can adversarially align features of different levels.\nFeature tokens in Figure 1 can correspond to patches, fea-\nture maps, or detected objects. Namely, our AQT can realize\nspace-, channel-, and instance-level feature alignment.\nThe main contribution of this work is three-fold. First, we\npropose a novel approach AQT, which integrates adversarial\nfeature alignment into a detection transformer via an adver-\nsarial token to identify the feature tokens hard to align at that\nmoment. Second, the proposed AQT is simple and flexible.\nIt can work with many existing transformer-based detectors\nand align features of diverse levels in a unified way. Third,\nthe proposed AQT performs favorably against state-of-the-art\nmethods on the benchmarks, including Cityscapes [Cordts et\nal., 2016 ] to Foggy Cityscapes [Sakaridis et al., 2018 ] and\nSim10k [Johnson-Roberson et al., 2017] to Cityscapes.\n2 Related Work\n2.1 Object Detection\nAs a fundamental topic in computer vision, object detec-\ntion has been actively studied for decades. Recent advances\nin object detection can be mainly attributed to CNNs, and\ncategorized by whether region-of-interest proposals are ex-\ntracted (two-stage) or not (one-stage). While two-stage de-\ntectors [Ren et al., 2015] are considered to be more accurate,\none-stage detectors [Liu et al., 2016] are benefited from their\nsimple structure and can perform faster.\nDifferent from CNN-based detectors, transformer-based\ndetectors [Carion et al., 2020; Zhuet al., 2021] explore token-\nwise dependencies for context modeling and eliminate the\nneed for many hand-crafted components, such as anchor gen-\neration and non-maximum suppression. DETR [Carion et al.,\n2020] first introduces transformers to object detection and\nyields competitive performance. To mitigate the slow conver-\ngence and prohibitive memory usage of DETR, Deformable\nDETR [Zhu et al., 2021 ] adopts a learnable sparse attention\nto speed up convergence and to process multi-scale feature\nmaps efficiently. While these methods focus on supervised\nlearning, we aim at generalizing a detection transformer to\nthe unlabeled target domain by utilizing cross-domain data.\n2.2 Domain Adaptive Object Detection\nTo circumvent performance degradation caused by distribu-\ntion shifts, the research of domain adaptive object detection\nhas drawn attention recently. The seminal work [Chen et\nal., 2018] investigates adversarial domain adaptation [Ganin\nand Lempitsky, 2015] for Faster R-CNN [Ren et al., 2015 ].\nInspired by it, many following works [Saito et al., 2019;\nXu et al., 2020a; Hsu et al., 2020; VS et al., 2021; Wang\net al., 2021b] employ domain classifiers on different aspects\nof cross-domain features. Other groups of works utilize self-\ntraining [Kim et al., 2019; Munir et al., 2021], mean teacher\nframework [Cai et al., 2019; Deng et al., 2021], and image-\nto-image translation [Chen et al., 2020 ]. Although these\nmethods are not categorized in adversarial domain adapta-\ntion, some of them also adopt adversarial learning as a part\nof their algorithms.\nThe aforementioned methods are usually constrained to\nCNN-based detectors, such as Faster R-CNN [Ren et al.,\n2015], SSD [Liu et al., 2016 ], and FCOS [Tian et al.,\n2019]. Due to the inherent differences between CNNs and\ntransformers, they may be inapplicable to or suboptimal\nfor transformer-based detectors. While detection transform-\ners [Carion et al., 2020; Zhu et al., 2021] have revealed their\npotentials, less efforts have been made for their adaptation.\nA recent study [Wang et al., 2021a ] empirically finds that\nexisting approaches bring only limited improvements for de-\ntection transformers. The authors attribute this finding to the\nlack of domain invariance in sequential features, and thus pro-\npose domain query-based feature alignment. As their domain\nquery belongs to the generator, it can aggregate global fea-\ntures, but not necessarily domain-specific ones. In contrast,\nour adversarial token works for the discriminator and attends\nto the hard-to-align feature tokens. Thus, the generator is en-\ncouraged to eliminate domain-specific features and produce\nmore domain-invariant ones. In addition, thanks to the flexi-\nble adversarial token, our AQT can carry out space-, channel-,\nand instance-level feature alignment in a unified manner.\n3 Proposed Method\nThis section describes our proposed approach. We first give a\nmethod overview and then elaborate the space-, channel-, and\ninstance-level feature alignment, respectively.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n973\nCNN ‚Ñíùëëùëíùë°\nobject queries\ndecoder layer\ndecoder layer\nDecoder ùëÆùë´ self&\ncross attn instance \nalign\nFFN\nencoder layer\nencoder layer\nEncoder ùëÆùë¨\ninput image ùíô\nself-attn\nchannel \nalign\nspace \nalign\nFFN\n(a) Overview of our method AQT\n‚Ñíùëéùëëùë£\nùë†ùëù\nquerykey-value \npairs\npatch\ntokens\nspace\nquery\ncross-attndomain \nclassifierùììùë∫\nfeature patches\n(b) Space-level feature alignment module\n‚Ñíùëéùëëùë£\nùëê‚Ñé\nquerykey-value \npairs\nchannel\ntokens\nchannel\nquery\ncross-attndomain \nclassifierùììùë™\nfeature channels\n (c) Channel-level feature alignment module\n‚Ñíùëéùëëùë£\nùëñùëõùë†\nquerykey-value \npairs\nobject \nqueries\ninstance\nquery\ncross-attndomain \nclassifierùììùë∞\ndetected instances (d) Instance-level feature alignment module\nFigure 2: Overview of our proposed Adversarial Query Transformers (AQT). (a) Our AQT framework with (b) space-level, (c) channel-level,\nand (d) instance-level feature alignment.\n3.1 Overview\nGiven a source dataset S = {xi\ns, yi\ns}Ns\ni=1 and an unlabeled\ntarget dataset T = {xi\nt}Nt\ni=1 where x denotes an image and\ny represents the ground truth for object detection, we train\nan adaptive detection transformer F with both S and T, and\nevaluate it on data in the target domain. To this end, the pro-\nposed AQT integrates adversarial feature alignment into de-\ntection transformer, and carries out domain invariance in the\nspace, channel, and instance levels.\nAn overview of our framework is shown in Figure 2a. The\nframework consists of a generator which is an object detec-\ntor and three discriminators which perform feature alignment\nin the space, channel, and instance levels, respectively. The\ndetector includes a CNN backbone GB for feature extraction\nand a detection transformer with an encoder GE and a de-\ncoder GD. Given an input image x, we apply the backbone\nGB and get its feature maps z ‚ààRC√óH√óW , where C is the\nnumber of channels, andH and W denote the map height and\nwidth, respectively. The feature mapsz are then flattened into\npatch tokens zp ‚ààRC√óL, where L = H √óW is the number\nof tokens. The encoder GE aggregates features for each token\nin zp via the self-attention mechanism. The decoder GD car-\nries out object detection by taking theobject queries as input.\nA detection loss Ldet is applied to derive the network.\nFor adversarial feature alignment, each layer of the en-\ncoder GE consists of not only a self-attention module and a\nFFN but also a space-level alignment module (green-shaded\nregion in Figure 2a) and a channel-level alignment module\n(blue-shaded region in Figure 2a). Similarly, each layer in the\ndecoder GD is associated with an additional instance-level\nalignment module (yellow-shaded region in Figure 2a). The\nthree modules act as the discriminators, and perform space-,\nchannel-, and instance-level feature alignment, respectively.\nFigure 2b illustrates the space-level alignment module,\nwhich includes a cross-attention layer. A space-level adver-\nsarial query, orspace query for short, iterates through the lay-\ners in GE. The pixels in the feature maps yield the feature to-\nkens and serve as the key-value pairs. A domain classifierDS\nis adopted in the space-level alignment module of the last en-\ncoder layer, and is learned to predict the domain of the space\nquery. By changing the feature tokens from pixels to chan-\nnels and detected objects, Figure 2c and Figure 2d show the\nchannel-level and instance-level alignment modules, respec-\ntively. We elaborate the three modules as follows.\n3.2 Space-level Feature Alignment\nAs mentioned in [Wang et al., 2021a ], direct feature align-\nment on the CNN backbone results in suboptimal perfor-\nmance on detection transformers. To address this problem,\nwe introduce the discriminator, i.e. the adversarial token and\ncross-attention layers, for adversarial feature alignment.\nThe adversarial token, similar to the class token [Dosovit-\nskiy et al., 2021], is trainable and derived to fulfill some ob-\njective. For the class token, the objective is to find discrim-\ninative features for object recognition, while for adversarial\ntoken, the objective is to identify hard-to-align feature tokens\nfor domain classification. In attention mechanisms, a queryQ\nattends to a set of keysK and maps itself into a linear combi-\nnation of the corresponding values V depending on attention\nweights. As a result, whichever tokens the query gives a high\nattention weight on, these tokens are likely to be discrimina-\ntive for domain classification, i.e. being domain-specific.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n974\nIn each layer i of the transformer encoder GE, we embed a\nspace-level alignment module after the original self-attention\nlayer. This module consists of a cross-attention layer and a\nlinear mapping layer (simplified from a FFN). As shown in\nFigure 2b, in the cross-attention layer, the adversarial to-\nken acts as a query, while the patch tokens zp, which can\nbe viewed as the output of the generator, act as the key-\nvalue pairs. Since this process is to align space-level features,\nwe term the adversarial token as the space-level adversarial\nquery, or space queryqs. Let qi\ns and zi\np denote the space query\nand patch tokens which the alignment module in the i-th en-\ncoder layer takes as input, where q1\ns is a randomly initialized\nC-dimensional vector and z1\np = zp. The module in the i-th\nencoder layer maps the space query qi\ns to its successor qi+1\ns\nw.r.t. zi\np by\nqi+1\ns = Linear(MultiHeadAttn(qi\ns, zi\np)), (1)\nwhere the multi-head attention function MultiHeadAttn de-\nfined in [Zhu et al., 2021 ] takes two inputs, query and key.\nNote that we omit positional embeddings of the keys, nor-\nmalization layers, and residual connections for simplicity.\nIn practice, the key-value pairs pass by a gradient reversal\nlayer [Ganin and Lempitsky, 2015] first to reverse the gradi-\nents backpropagated to the generator. This process continues\nuntil the end of GE, where i is the number of encoder layers\nN. The space-level domain classifier DS then identifies the\ndomain of the image given the output of the final-layer space\nquery qN+1\ns . This domain classification task is optimized by\nminimizing the following binary cross-entropy loss\nLsp\nadv = ‚àíd log DS(qN+1\ns )‚àí\n(1 ‚àíd) log(1‚àíDS(qN+1\ns )), (2)\nwhere d denotes the domain label. It takes value 0 for\nsource images, and 1 otherwise. Derived to minimize this\nadversarial loss Lsp\nadv, the space query qs and the space-level\nalignment module manage to identify domain-specific local\npatches. The generator, i.e. the original layers in detection\ntransformer, with an aim to maximize Lsp\nadv, is forced to gen-\nerate more domain-invariant features. As adversarial learning\nprogresses, space-level cross-domain features are adapted.\n3.3 Channel-level Feature Alignment\nAlthough space-level feature alignment effectively sup-\npresses domain-specific local patches, it alone does not elim-\ninate the global biases in the encoder GE. The main reason\nis that attention maps are relatively sparse. Only the keys of\nthe most discriminative patches are assigned high attention\nweights. To solve this problem, we also adopt channel-level\nfeature alignment in GE.\nWe define channel tokens zc ‚ààRÀÜL√óC as a different view\nof patch tokens zp. Each channel token comes from a feature\nmap. ÀÜL is a hyperparameter, and ÀÜL = C ‚â™L empirically.\nThe reason of resizing is to handle images of different sizes\nand reduce parametrization overloads. In our implementa-\ntion, we reshape zp back into z ‚ààRC√óH√óW , and pool z to\nyield ÀÜz ‚ààRC√óP√óP , where P2 = ÀÜL. Thus, we obtain zc by\nflattening and transposing ÀÜz.\nTo enable channel-level alignment, we leverage a channel-\nlevel adversarial query, or channel query qc. Similar to the\nspace-level alignment module, we embed a channel-level\nalignment module in each layer of GE. Let qi\nc and zi\nc denote\nthe channel query and channel tokens which the i-th align-\nment module takes as input, where q1\ns is a randomly initial-\nized ÀÜL-dimensional vector and zi\nc comes from zi\np. The mod-\nule in the i-th encoder layer maps the channel query qi\nc to its\nsuccessor qi+1\nc w.r.t. zi\nc via\nqi+1\nc = Linear(MultiHeadAttn(qi\nc, zi\nc)). (3)\nAgain, this process continues until the end of GE. The\nchannel-level domain classifierDC then identifies the domain\nof the image given the final-layer channel query qN+1\nc . The\nchannel-level adversarial loss thus yields\nLch\nadv = ‚àíd log DC(qN+1\nc )‚àí\n(1 ‚àíd) log(1‚àíDC(qN+1\nc )). (4)\n3.4 Instance-level Feature Alignment\nAlthough we make the encoder GE domain-adaptive using\nboth space-level and channel-level alignment, the decoder\nGD remains biased towards the source domain. Thus, we\nremedy this issue and show the flexibility of our method by\nextending it to instance-level feature alignment.\nIn a detection transformer, object queries zo ‚àà RC√óLo,\nare Lo learned embeddings that decode object representations\nfrom the encoder output. To align object representations in\nzo, we introduce the instance-level adversarial query, or in-\nstance query qi. Similar to the alignment modules built in\nGE, we embed an instance-level module in each layer ofGD.\nLet qj\ni and zj\no denote the instance query and object queries\nwhich the j-th alignment module takes as input, whereq1\ni is a\nrandomly initialized C-dimensional vector and z1\no = zo. The\nmodule in the j-th decoder layer maps the instance query qj\ni\nto its successor qj+1\ni w.r.t. zj\no by\nqj+1\ni = Linear(MultiHeadAttn(qj\ni , zj\no)). (5)\nThis process continues until the end of GD, where j is the\nnumber of decoder layers M. The instance-level adversarial\nloss is thus yielded\nLins\nadv = ‚àíd log DI(qM+1\ni )‚àí\n(1 ‚àíd) log(1‚àíDI(qM+1\ni )). (6)\nOverall Objective. We summarize the overall objective of\nthe proposed Adversarial Query Transformer. The overall ad-\nversarial loss function is formulated as\nLadv = ŒªspLsp\nadv + ŒªchLch\nadv + ŒªinsLins\nadv, (7)\nwhere Œªsp, Œªch, and Œªins denote the balancing weights for\nthe corresponding terms in the loss function. Ldet denotes\nthe detection loss of the adopted detection transformer. The\noverall loss function of the proposed AQT is\nLAQT = Ldet ‚àíLadv, (8)\nand the optimization objective for our domain adaptive detec-\ntion transformer F is\nF‚àó = argmin\nF\nmin\nGB,GE\nGD\nmax\nDS,DC\nDI\nLAQT . (9)\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n975\nMethod Backbone prsn rider car truck bus train motor bike mAP\nSource Only (Faster R-CNN) R-50 26.9 38.2 35.6 18.3 32.4 9.6 25.8 28.6 26.9\nDA-Faster [Chen et al., 2018] R-50 29.2 40.4 43.4 19.7 38.3 28.5 23.7 32.7 32.0\nRPA [Zhang et al., 2021] V-16 33.3 45.6 50.5 30.4 43.6 42.0 29.7 36.8 39.0\nHTCN [Chen et al., 2020] V-16 33.2 47.5 47.9 31.6 47.4 40.9 32.3 37.1 39.8\nICCR-VDD [Wu et al., 2021] V-16 33.4 44.0 51.7 33.9 52.0 34.7 34.2 36.8 40.0\nDSS [Wang et al., 2021b] R-50 42.9 51.2 53.6 33.6 49.2 18.9 36.2 41.8 40.9\nKTNet [Tian et al., 2021] V-16 46.4 43.2 60.6 25.8 41.2 40.4 30.7 38.8 40.9\nUMT [Deng et al., 2021] V-16 33.0 46.7 48.6 34.1 56.5 46.8 30.4 37.3 41.7\nMeGA-CDA [VS et al., 2021] V-16 37.7 49.0 52.4 25.4 49.2 46.9 34.5 39.0 41.8\nViSGA [Rezaeianaran et al., 2021] R-50 38.8 45.9 57.2 29.9 50.2 51.9 31.9 40.9 43.3\nSource Only (FCOS) R-50 36.9 36.3 44.1 18.6 29.3 8.4 20.3 31.9 28.2\nEPM [Hsu et al., 2020] R-50 44.2 46.6 58.5 24.8 45.2 29.1 28.6 34.6 39.0\nSSAL [Munir et al., 2021] V-16 45.1 47.4 59.4 24.5 50.0 25.7 26.0 38.7 39.6\nSource Only (Deformable DETR) R-50 37.7 39.1 44.2 17.2 26.8 5.8 21.6 35.5 28.5\nSFA‚Ä† [Wang et al., 2021a] R-50 47.1 46.4 62.2 30.0 50.3 35.5 27.9 41.2 42.6\nAQT‚Ä† (Ours) R-50 49.3 52.3 64.4 27.7 53.7 46.5 36.0 46.4 47.1\nTable 1: Results of Cityscapes to Foggy Cityscapes. ‚Äúprsn‚Äù, ‚Äúmotor‚Äù, and ‚Äúbike‚Äù denote ‚Äúperson‚Äù, ‚Äúmotorcycle‚Äù, and ‚Äúbicycle‚Äù, respectively.\nMethod prsn rider car truck bus motor bike mAP\nSource Only (Faster R-CNN) 28.8 25.4 44.1 17.9 16.1 13.9 22.4 24.1\nDA-Faster [Chen et al., 2018] 28.9 27.4 44.2 19.1 18.0 14.2 22.4 24.9\nICR-CCR-SW [Xu et al., 2020a] 32.8 29.3 45.8 22.7 20.6 14.9 25.5 27.4\nSource Only (FCOS) 38.6 24.8 54.5 17.2 16.3 15.0 18.3 26.4\nEPM [Hsu et al., 2020] 39.6 26.8 55.8 18.8 19.1 14.5 20.1 27.8\nSource Only (Deformable DETR) 38.9 26.7 55.2 15.7 19.7 10.8 16.2 26.2\nSFA [Wang et al., 2021a] 40.2 27.6 57.5 19.1 23.4 15.4 19.2 28.9\nAQT (Ours) 38.2 33.0 58.4 17.3 18.4 16.9 23.5 29.4\nTable 2: Results ofCityscapes to BDD100k daytime. ‚Äúprsn‚Äù, ‚Äúmotor‚Äù, and ‚Äúbike‚Äù denote ‚Äúperson‚Äù, ‚Äúmotorcycle‚Äù, and ‚Äúbicycle‚Äù,\nrespectively. All competing methods are developed upon ResNet-50.\n4 Experimental Results\n4.1 Datasets and Experimental Settings\nCityscapes to Foggy Cityscapes.Cityscapes [Cordts et al.,\n2016] is an urban scene dataset containing 2,975 training im-\nages and 500 validation images. Foggy Cityscapes [Sakaridis\net al., 2018] is synthesized from and shared annotations with\nCityscapes. We take the highest fog density images follow-\ning [Rezaeianaran et al., 2021]. In this setting, Cityscapes is\nused as the source domain, while Foggy Cityscapes is used as\nthe target domain. 8 categories are considered.\nCityscapes to BDD100k daytime. BDD100k [Yu et al.,\n2020] is a large-scale driving dataset with diverse scenarios.\nIn this setting, Cityscapes is used as the source domain, while\nthe daytime subset of BDD100k is selected as the target do-\nmain. Following [Xu et al., 2020a], the common 7 categories\nare considered.\nSim10k to Cityscapes. Sim10k [Johnson-Roberson et al.,\n2017] is a synthetic driving dataset containing 10,000 images.\nIn this setting, Sim10k is used as the source domain, while\nCityscapes is used as the target domain. Following [Chen et\nal., 2018], only the category car is considered.\n4.2 Implementation Details\nWe select Deformable DETR [Zhu et al., 2021 ] as our ob-\nject detector with a ResNet-50 backbone pre-trained on Im-\nageNet [Deng et al., 2009 ]. We inherit most hyperparame-\nters and training settings from Zhu et al., including the detec-\ntion loss Ldet and Xavier initialization [Glorot and Bengio,\n2010]. In Cityscapes to Foggy Cityscapes, all Œªsp, Œªch, and\nŒªins are set to 10‚àí1. In the other settings, following [Saito\net al., 2019], we adopt local alignment on the backbone and\nweak alignment using the focal loss [Lin et al., 2017 ]. The\nŒªsp, Œªch and Œªins are set to 10‚àí1, 10‚àí5, and 10‚àí4, respec-\ntively. The batch size is set to 8 in all experiments.\n4.3 Comparing with State-of-the-arts\nFrom Table 1 to Table 3, we compare the proposed AQT\nwith the existing methods based on Faster R-CNN[Ren et al.,\n2015], FCOS [Tian et al., 2019], or Deformable DETR [Zhu\net al., 2021] on three adaptation settings. ‚ÄúSource Only‚Äù indi-\ncates the baselines trained with source data only; ‚ÄúV-16‚Äù and\n‚ÄúR-50‚Äù indicate the backbone is VGG-16[Simonyan and Zis-\nserman, 2015] and ResNet-50 [He et al., 2016 ]. ‚Ä†indicates\niterative bounding box refinement [Zhu et al., 2021].\nCityscapes to Foggy Cityscapes. After adaptation, our\nmethod improves the baseline by 18.6% and outperforms all\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n976\n(a) An input image and its corresponding attention map.\nSource only\n AQT (ours)\n SFA (b) Feature distribution visualizations using t-SNE.\nFigure 3: Visualizations to analyze the proposed AQT.\nMethod Backbone car AP\nSource Only (Faster R-CNN) R-50 39.4\nDA-Faster [Chen et al., 2018] R-50 41.9\nUMT [Deng et al., 2021] V-16 43.1\nSWDA [Saito et al., 2019] R-50 44.6\nMeGA-CDA [VS et al., 2021] V-16 44.8\nRPA [Zhang et al., 2021] V-16 45.7\nGPA [Xu et al., 2020b] R-50 47.6\nViSGA [Rezaeianaran et al., 2021] R-50 49.3\nKTNet [Tian et al., 2021] V-16 50.7\nSource Only (FCOS) R-50 42.5\nEPM [Hsu et al., 2020] R-50 47.3\nSSAL [Munir et al., 2021] V-16 51.8\nSource Only (Deformable DETR) R-50 47.4\nSFA [Wang et al., 2021a] R-50 52.6\nAQT (Ours) R-50 53.4\nTable 3: Results of Sim10k to Cityscapes.\nthe other methods. We observe that our method performs\nworse on the categories ‚Äútruck‚Äù, ‚Äúbus‚Äù, and ‚Äútrain‚Äù. We\nhypothesize this results from fewer instances in these cate-\ngories, given that transformers rely on sufficient data more\nthan CNNs.\nCityscapes to BDD100k daytime. Our method outper-\nforms all the other methods in terms of mAP. All the meth-\nods reported in this experiment is developed upon ResNet-\n50. Again, our method performs worse on the categories with\nfewer instances.\nSim10k to Cityscapes. Due to the larger domain gap, our\nmethod outperforms the state-of-the-arts by a relatively small\nmargin. However, our method still outperforms SFA [Wang\net al., 2021a].\nQualitative Result. We evaluate the detection quality of\nour method by comparing it with three existing methods,\nEPM [Hsu et al., 2020], ViSGA [Rezaeianaran et al., 2021],\nand SFA [Wang et al., 2021a]. The results in Figure 4 show\nthe superior performance of our method.\n4.4 Ablation Study and Analysis\nIn this section, we provide detailed ablation study and analy-\nsis of AQT. Following [VS et al., 2021], all these studies are\nconducted on Cityscapes to Foggy Cityscapes.\nWhere the Adversarial Token Looks at.To better under-\nstand how the adversarial token works, we provide an atten-\ntion map of a space query on an input image, as shown in\nSpace Channel Instance Box-\nRefine\nTwo-\nStage mAP\n28.5\n‚úì 40.6\n‚úì 36.2\n‚úì 36.8\n‚úì ‚úì 41.4\n‚úì ‚úì 40.9\n‚úì ‚úì 40.1\n‚úì ‚úì ‚úì 44.8\n‚úì ‚úì ‚úì ‚úì 47.1\n‚úì ‚úì ‚úì ‚úì ‚úì 44.7\nTable 4: Quantitative Ablation study of AQT onCityscapes to Foggy\nCityscapes. Box-Refine indicates iterative bounding box refinement\nand Two-Stage indicates two-stage Deformable DETR.\nFigure 3a. The regions with strong responses in the attention\nmap correspond to the foggiest regions in the input image.\nThis demonstrates that the adversarial token tends to attend\nto the domain-specific features.\nVisualizing Feature Distribution Alignment. We use t-\nSNE [van der Maaten and Hinton, 2008 ] to visualize the\nactivations of different detection transformers, including the\nbaseline (Source only), SFA [Wang et al., 2021a ], and ours.\nFigure 3b shows the results. Different colors represent the\nfeatures in different domains. Empirically the aligned fea-\ntures via both AQT and SFA are more indistinguishable than\nthe baseline. Interestingly, we notice the aligned feature dis-\ntribution via AQT is more perceptually similar to the baseline.\nQuantitative Ablation Study. We analyze the effect of\nthe proposed three levels of alignment in Table 4, where\nBox-Refine and Two-Stage are inherited from Deformable\nDETR [Zhu et al., 2021]. Based on this experiment, we con-\nclude three major observations. First, each of the three levels\nof alignment leads to reasonable improvement. This demon-\nstrates the effectiveness and flexibility of the proposed mech-\nanism. Second, when a certain level of alignment is aug-\nmented with another one, a steady growth occurs. We can\nobserve a rather considerable improvement when the three\nalignments are adopted, compared to the settings where only\nany two are adopted. Lastly, our framework can also benefit\nfrom other techniques. When using Box-Refine, our method\nachieves the best result. Thus, we take it as our default setting\nwhen compared with the state-of-the-arts.\nQualitative Ablation Study. In Figure 5, we show sev-\neral detection results from the baseline (Source only), the\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n977\nFigure 4: Our detection results compared with the state-of-the-art methods. Different categories are marked with different colors.\nFigure 5: Qualitative ablation studies onCityscapes to Foggy Cityscapes. The images on the first to fourth columns are from the models trained\nwith source data only, adopting channel-level alignment (denoted as ‚ÄúCh.‚Äù), adopting both channel- & instance-level alignment (denoted as\n‚ÄúCh.+Ins.‚Äù), and adopting channel- & instance- & space-level alignment (denoted as ‚ÄúCh.+Ins.+Sp.‚Äù), respectively. The images in the last\ncolumn are ground truth bounding boxes. Different categories are marked with different colors.\nadapted models, and the ground truth bounding boxes. In\nthe first row, the baseline barely discerns any object, while\nthe adapted model with channel-level alignment (‚ÄúCh.‚Äù) rec-\nognizes some objects. With the assistance of instance-level\nalignment, the adapted model (‚ÄúCh.+Ins.‚Äù) can further lo-\ncalize the truck at the center of the image. It shows that\nchannel-level alignment effectively eliminates cross-domain\ndiscrepancy, while the categories with few instances are still\nhard to detect. This drawback is compensated when adopt-\ning instance-level alignment due to reducing biases in the ob-\nject queries. In the second row, the models in the left three\ncolumns fail to recognize the truck at the left of the image,\nwhile this false negative is resolved with space-level align-\nment (‚ÄúCh.+Ins.+Sp.‚Äù). This study empirically shows differ-\nent levels of alignment can be complementary.\n5 Conclusion\nIn this paper, we present AQT (adversarial query transform-\ners), an adaptation framework to integrate adversarial feature\nalignment into a detection transformer. It employs a novel\nadversarial token and a stack of cross-attention layers as the\ndiscriminator. As the query in each cross-attention layer, the\nadversarial token attends the feature tokens from the gener-\nator that are hard to align at that moment. Constrained by\nboth the adversarial loss and the detection loss, the genera-\ntor is forced to eliminate the domain-specific features while\nmaintaining semantics in the feature tokens, hence realizing\nadversarial feature alignment on detection transformers. The\nproposed AQT demonstrates the flexibility of the proposed\nmechanism, combines the merits from the space-, channel-,\nand instance-level alignment, and yields a new state-of-the-\nart on several domain adaptive object detection benchmarks.\nAcknowledgements\nThis work was supported in part by the Ministry of Science\nand Technology (MOST) under grants 109-2221-E-009-113-\nMY3, 110-2628-E-A49-008, 111-2634-F-007-002, and 110-\n2634-F-006-022. This work was funded in part by Qual-\ncomm and MediaTek. We thank the National Center for\nHigh-performance Computing (NCHC) of National Applied\nResearch Laboratories (NARLabs) in Taiwan for providing\ncomputational and storage resources.\nReferences\n[Cai et al., 2019] Qi Cai, Yingwei Pan, Chong-Wah Ngo, Xinmei\nTian, Lingyu Duan, and Ting Yao. Exploring object relation in\nmean teacher for cross-domain detection. In CVPR, 2019.\n[Carion et al., 2020] Nicolas Carion, Francisco Massa, Gabriel\nSynnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In\nECCV, 2020.\n[Chen et al., 2018] Yuhua Chen, Wen Li, Christos Sakaridis,\nDengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn\nfor object detection in the wild. In CVPR, 2018.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n978\n[Chen et al., 2020] Chaoqi Chen, Zebiao Zheng, Xinghao Ding,\nYue Huang, and Qi Dou. Harmonizing transferability and dis-\ncriminability for adapting object detectors. In CVPR, 2020.\n[Cordts et al., 2016] Marius Cordts, Mohamed Omran, Sebastian\nRamos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes\ndataset for semantic urban scene understanding. In CVPR, 2016.\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li,\nKai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In CVPR, 2009.\n[Deng et al., 2021] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin\nDuan. Unbiased mean teacher for cross-domain object detection.\nIn CVPR, 2021.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil\nHoulsby. An image is worth 16x16 words: Transformers for\nimage recognition at scale. In ICLR, 2021.\n[Ganin and Lempitsky, 2015] Yaroslav Ganin and Victor Lempit-\nsky. Unsupervised domain adaptation by backpropagation. In\nICML, 2015.\n[Glorot and Bengio, 2010] Xavier Glorot and Yoshua Bengio. Un-\nderstanding the difficulty of training deep feedforward neural net-\nworks. In AISTATS, 2010.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. Deep residual learning for image recognition. InCVPR,\n2016.\n[Hoffman et al., 2018] Judy Hoffman, Eric Tzeng, Taesung Park,\nJun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and\nTrevor Darrell. CyCADA: Cycle-consistent adversarial domain\nadaptation. In ICML, 2018.\n[Hsu et al., 2020] Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin,\nand Ming-Hsuan Yang. Every pixel matters: Center-aware fea-\nture alignment for domain adaptive object detector. In ECCV,\n2020.\n[Johnson-Roberson et al., 2017] Matthew Johnson-Roberson,\nCharles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl\nRosaen, and Ram Vasudevan. Driving in the matrix: Can virtual\nworlds replace human-generated annotations for real world\ntasks? In ICRA, 2017.\n[Kim et al., 2019] Seunghyeon Kim, Jaehoon Choi, Taekyung Kim,\nand Changick Kim. Self-training and adversarial background\nregularization for unsupervised domain adaptive one-stage object\ndetection. In ICCV, 2019.\n[Lin et al., 2014] Tsung-Yi Lin, Michael Maire, Serge Belongie,\nJames Hays, Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar, and\nC. Lawrence Zitnick. Microsoft coco: Common objects in con-\ntext. In ECCV, 2014.\n[Lin et al., 2017] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaim-\ning He, and Piotr Dollar. Focal loss for dense object detection. In\nICCV, 2017.\n[Liu et al., 2016] Wei Liu, Dragomir Anguelov, Dumitru Erhan,\nChristian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexan-\nder C. Berg. Ssd: Single shot multibox detector. In ECCV, 2016.\n[Munir et al., 2021] Muhammad Akhtar Munir, Muhammad Haris\nKhan, M. Sarfraz, and Mohsen Ali. Ssal: Synergizing between\nself-training and adversarial learning for domain adaptive object\ndetection. In NeurIPS, 2021.\n[Ren et al., 2015] Shaoqing Ren, Kaiming He, Ross Girshick, and\nJian Sun. Faster r-cnn: Towards real-time object detection with\nregion proposal networks. In NIPS, 2015.\n[Rezaeianaran et al., 2021] Farzaneh Rezaeianaran, Rakshith\nShetty, Rahaf Aljundi, Daniel Olmeda Reino, Shanshan Zhang,\nand Bernt Schiele. Seeking similarities over differences:\nSimilarity-based domain alignment for adaptive object detection.\nIn ICCV, 2021.\n[Saito et al., 2019] Kuniaki Saito, Yoshitaka Ushiku, Tatsuya\nHarada, and Kate Saenko. Strong-weak distribution alignment\nfor adaptive object detection. In CVPR, 2019.\n[Sakaridis et al., 2018] Christos Sakaridis, Dengxin Dai, and Luc\nVan Gool. Semantic foggy scene understanding with synthetic\ndata. IJCV, 2018.\n[Simonyan and Zisserman, 2015] Karen Simonyan and Andrew\nZisserman. Very deep convolutional networks for large-scale im-\nage recognition. In ICLR, 2015.\n[Tian et al., 2019] Zhi Tian, Chunhua Shen, Hao Chen, and Tong\nHe. Fcos: Fully convolutional one-stage object detection. In\nICCV, 2019.\n[Tian et al., 2021] Kun Tian, Chenghao Zhang, Ying Wang, Shim-\ning Xiang, and Chunhong Pan. Knowledge mining and transfer-\nring for domain adaptive object detection. In ICCV, 2021.\n[Tzeng et al., 2017] Eric Tzeng, Judy Hoffman, Kate Saenko, and\nTrevor Darrell. Adversarial discriminative domain adaptation. In\nCVPR, 2017.\n[van der Maaten and Hinton, 2008] Laurens van der Maaten and\nGeoffrey Hinton. Visualizing data using t-sne. JMLR, 2008.\n[VS et al., 2021] Vibashan VS, Vikram Gupta, Poojan Oza, Vish-\nwanath A. Sindagi, and Vishal M. Patel. Mega-cda: Memory\nguided attention for category-aware unsupervised domain adap-\ntive object detection. In CVPR, 2021.\n[Wang et al., 2021a] Wen Wang, Yang Cao, Jing Zhang, Fengxi-\nang He, Zheng-Jun Zha, Yonggang Wen, and Dacheng Tao. Ex-\nploring sequence feature alignment for domain adaptive detection\ntransformers. In ACM MM, 2021.\n[Wang et al., 2021b] Yu Wang, Rui Zhang, Shuo Zhang, Miao Li,\nYangyang Xia, Xishan Zhang, and Shaoli Liu. Domain-specific\nsuppression for adaptive object detection. In CVPR, 2021.\n[Wu et al., 2021] Aming Wu, Rui Liu, Yahong Han, Linchao Zhu,\nand Yi Yang. Vector-decomposed disentanglement for domain-\ninvariant object detection. In ICCV, 2021.\n[Xu et al., 2020a] Chang-Dong Xu, Xing-Ran Zhao, Xin Jin, and\nXiu-Shen Wei. Exploring categorical regularization for domain\nadaptive object detection. In CVPR, 2020.\n[Xu et al., 2020b] Minghao Xu, Hang Wang, Bingbing Ni, Qi Tian,\nand Wenjun Zhang. Cross-domain detection via graph-induced\nprototype alignment. In CVPR, 2020.\n[Yu et al., 2020] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian,\nYingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor\nDarrell. Bdd100k: A diverse driving dataset for heterogeneous\nmultitask learning. In CVPR, 2020.\n[Zhang et al., 2021] Yixin Zhang, Zilei Wang, and Yushi Mao. Rpn\nprototype alignment for domain adaptive object detector. In\nCVPR, 2021.\n[Zhu et al., 2021] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xi-\naogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. In ICLR, 2021.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n979",
  "topic": "Discriminator",
  "concepts": [
    {
      "name": "Discriminator",
      "score": 0.8195381760597229
    },
    {
      "name": "Computer science",
      "score": 0.757378101348877
    },
    {
      "name": "Security token",
      "score": 0.6845744252204895
    },
    {
      "name": "Adversarial system",
      "score": 0.6727821826934814
    },
    {
      "name": "Transformer",
      "score": 0.6218533515930176
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5099788904190063
    },
    {
      "name": "Object detection",
      "score": 0.5086241960525513
    },
    {
      "name": "Detector",
      "score": 0.4500730633735657
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4292380213737488
    },
    {
      "name": "Feature extraction",
      "score": 0.4239903688430786
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3744064271450043
    },
    {
      "name": "Engineering",
      "score": 0.1551331877708435
    },
    {
      "name": "Computer network",
      "score": 0.09671753644943237
    },
    {
      "name": "Electrical engineering",
      "score": 0.0707722008228302
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    }
  ]
}