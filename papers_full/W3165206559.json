{
  "title": "Predicting Semantic Similarity Between Clinical Sentence Pairs Using Transformer Models: Evaluation and Representational Analysis",
  "url": "https://openalex.org/W3165206559",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5053863372",
      "name": "Mark Ormerod",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A5014600847",
      "name": "Jesús Martínez del Rincón",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A5007017103",
      "name": "Barry Devereux",
      "affiliations": [
        "Queen's University Belfast"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2147152072",
    "https://openalex.org/W1833785989",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W2462305634",
    "https://openalex.org/W2251249502",
    "https://openalex.org/W2888285200",
    "https://openalex.org/W2889272240",
    "https://openalex.org/W6922433603",
    "https://openalex.org/W3094834348",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2826721128",
    "https://openalex.org/W4288086191",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2160654481",
    "https://openalex.org/W2040036684",
    "https://openalex.org/W3109919947",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W3102568136",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W2799051177",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2973047874",
    "https://openalex.org/W2051840895",
    "https://openalex.org/W145425800",
    "https://openalex.org/W3104033643"
  ],
  "abstract": "Background Semantic textual similarity (STS) is a natural language processing (NLP) task that involves assigning a similarity score to 2 snippets of text based on their meaning. This task is particularly difficult in the domain of clinical text, which often features specialized language and the frequent use of abbreviations. Objective We created an NLP system to predict similarity scores for sentence pairs as part of the Clinical Semantic Textual Similarity track in the 2019 n2c2/OHNLP Shared Task on Challenges in Natural Language Processing for Clinical Data. We subsequently sought to analyze the intermediary token vectors extracted from our models while processing a pair of clinical sentences to identify where and how representations of semantic similarity are built in transformer models. Methods Given a clinical sentence pair, we take the average predicted similarity score across several independently fine-tuned transformers. In our model analysis we investigated the relationship between the final model’s loss and surface features of the sentence pairs and assessed the decodability and representational similarity of the token vectors generated by each model. Results Our model achieved a correlation of 0.87 with the ground-truth similarity score, reaching 6th place out of 33 teams (with a first-place score of 0.90). In detailed qualitative and quantitative analyses of the model’s loss, we identified the system’s failure to correctly model semantic similarity when both sentence pairs contain details of medical prescriptions, as well as its general tendency to overpredict semantic similarity given significant token overlap. The token vector analysis revealed divergent representational strategies for predicting textual similarity between bidirectional encoder representations from transformers (BERT)–style models and XLNet. We also found that a large amount information relevant to predicting STS can be captured using a combination of a classification token and the cosine distance between sentence-pair representations in the first layer of a transformer model that did not produce the best predictions on the test set. Conclusions We designed and trained a system that uses state-of-the-art NLP models to achieve very competitive results on a new clinical STS data set. As our approach uses no hand-crafted rules, it serves as a strong deep learning baseline for this task. Our key contribution is a detailed analysis of the model’s outputs and an investigation of the heuristic biases learned by transformer models. We suggest future improvements based on these findings. In our representational analysis we explore how different transformer models converge or diverge in their representation of semantic signals as the tokens of the sentences are augmented by successive layers. This analysis sheds light on how these “black box” models integrate semantic similarity information in intermediate layers, and points to new research directions in model distillation and sentence embedding extraction for applications in clinical NLP.",
  "full_text": "Original Paper\nPredicting Semantic Similarity Between Clinical Sentence Pairs\nUsing Transformer Models: Evaluation and Representational\nAnalysis\nMark Ormerod, BEng; Jesús Martínez del Rincón, PhD; Barry Devereux, PhD\nInstitute of Electronics, Communications & Information Technology, School of Electronics, Electrical Engineering and Computer Science, Queen's\nUniversity Belfast, Belfast, United Kingdom\nCorresponding Author:\nMark Ormerod, BEng\nInstitute of Electronics, Communications & Information Technology\nSchool of Electronics, Electrical Engineering and Computer Science\nQueen's University Belfast\nQueen's Road\nQueen's Island\nBelfast, BT3 9DT\nUnited Kingdom\nPhone: 44 28 9097 1700\nEmail: mormerod01@qub.ac.uk\nAbstract\nBackground: Semantic textual similarity (STS) is a natural language processing (NLP) task that involves assigning a similarity\nscore to 2 snippets of text based on their meaning. This task is particularly difficult in the domain of clinical text, which often\nfeatures specialized language and the frequent use of abbreviations.\nObjective: We created an NLP system to predict similarity scores for sentence pairs as part of the Clinical Semantic Textual\nSimilarity track in the 2019 n2c2/OHNLP Shared Task on Challenges in Natural Language Processing for Clinical Data. We\nsubsequently sought to analyze the intermediary token vectors extracted from our models while processing a pair of clinical\nsentences to identify where and how representations of semantic similarity are built in transformer models.\nMethods: Given a clinical sentence pair, we take the average predicted similarity score across several independently fine-tuned\ntransformers. In our model analysis we investigated the relationship between the final model’s loss and surface features of the\nsentence pairs and assessed the decodability and representational similarity of the token vectors generated by each model.\nResults: Our model achieved a correlation of 0.87 with the ground-truth similarity score, reaching 6th place out of 33 teams\n(with a first-place score of 0.90). In detailed qualitative and quantitative analyses of the model’s loss, we identified the system’s\nfailure to correctly model semantic similarity when both sentence pairs contain details of medical prescriptions, as well as its\ngeneral tendency to overpredict semantic similarity given significant token overlap. The token vector analysis revealed divergent\nrepresentational strategies for predicting textual similarity between bidirectional encoder representations from transformers\n(BERT)–style models and XLNet. We also found that a large amount information relevant to predicting STS can be captured\nusing a combination of a classification token and the cosine distance between sentence-pair representations in the first layer of a\ntransformer model that did not produce the best predictions on the test set.\nConclusions: We designed and trained a system that uses state-of-the-art NLP models to achieve very competitive results on\na new clinical STS data set. As our approach uses no hand-crafted rules, it serves as a strong deep learning baseline for this task.\nOur key contribution is a detailed analysis of the model’s outputs and an investigation of the heuristic biases learned by transformer\nmodels. We suggest future improvements based on these findings. In our representational analysis we explore how different\ntransformer models converge or diverge in their representation of semantic signals as the tokens of the sentences are augmented\nby successive layers. This analysis sheds light on how these “black box” models integrate semantic similarity information in\nintermediate layers, and points to new research directions in model distillation and sentence embedding extraction for applications\nin clinical NLP.\n(JMIR Med Inform 2021;9(5):e23099) doi: 10.2196/23099\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 1https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nKEYWORDS\nnatural language processing; biomedical NLP; transformer models; representation learning; clinical text\nIntroduction\nClinical Semantic Textual Similarity\nSemantic textual similarity (STS) has long been an important\ntask in natural language processing (NLP) research. Early work\nbuilt document-level models for textual similarity that used an\nunsupervised approach, primarily for the purpose of indexing\ndocuments for search [1,2]. These models generally relied on\nthe assumption that greater overlap in terms indicated greater\ninterdocument similarity. This body of work was enriched by\nLee et al [3] who also modeled similarity at the document level\nbut elicited human semantic judgments of similarity to create\na small data set of interest to NLP researchers and cognitive\nscientists. It was not until SemEval-2012 Task 6 [4] that the first\nsentence-based STS data set was released, featuring 2000\ntraining and 750 test sentence pairs that were rated by humans\non a scale of 0-5 (from low to high similarity). Since then, there\nhave been many new SemEval STS tasks, building on the initial\ntask to encompass new domains of text [5] and cross-lingual\nsimilarity [6,7]. Researchers have used these models in a diverse\nset of applications such as discovering links between data sets\n[8] and identifying arguments in online discourse [9].\nRecognizing both the potential of STS for processing eHealth\nrecords and the need for specialized data sets to account for\nclinical domain knowledge and handle the use of medical\nabbreviations, Rastegar-Mojarad et al [10] introduced a corpus\nof clinical sentence pairs that were assigned semantic similarity\nlabels on a 0-5 scale by medical experts. This data set of 1068\nannotated sentence pairs, as well as an expanded corpus of\n174,629 unannotated sentence pairs, was released as MedSTS\n[11]. As with previous STS tasks, performance on this data set\nis measured by the Pearson correlation between the predicted\nlabels and the ground-truth similarity scores. In general, the\nbest systems in the BioCreative/OHNLP Challenge STS task\nused ensembles of traditional machine learning models and deep\nlearning models [12], with the overall top-performing model\nachieving a correlation of 0.83 on the test set. The clinical STS\ntask tackled in this paper, the 2019 n2c2/OHNLP Track on\nClinical Semantic Textual Similarity [13], uses an expansion of\nthe BioCreative/OHNLP Challenge STS task data set.\nTransformer Models\nIn this work we train different types of transformer language\nmodels [14]. One of the types of transformer models that we\ntrain is bidirectional encoder representations from transformers\n(BERT) [15], which uses a masked language modeling task to\ntrain fully on bidirectional context without the decoder\ncomponent of the original transformer architecture. Recently\nthere has been much work in further training BERT on data\nfrom specialized domains, including biomedical text [16] and\nclinical documents [16-18]. We also further fine-tune these\nmodels on the task of STS. The last type of transformer model\nthat we fine-tune is XLNet [19], which performs autoregressive\nlanguage modeling while also capturing bidirectional context\nby sampling different possible word orders.\nInterpreting Deep Neural Networks\nAfter we train our models, we explore the representations that\nthey build of clinical semantic similarity to identify any\nsystematic biases or heuristics they may have learned that we\ncan then work toward addressing to improve future clinical STS\ntransformer architectures. There is a substantial literature that\nuncovers the kind of linguistic representations deep neural\nnetworks learn by experimentally perturbing the model’s input\nand carefully analyzing the failure cases [20-22]. Another\napproach uses “decoding” to try to predict task-relevant\ninformation from intermediate representations generated from\nthe model [23-25]. Recently there has been further work on\ninterpreting the representations in deep neural models using\nattention weights [26,27]. While this approach is intuitive, there\nis still an ongoing debate about the extent to which the attention\nmechanism can be used to interpret a model’s decision-making\nprocess [28,29]. As such, we focus our layer-wise analysis on\nour models’ hidden token vectors [24]. Other relevant work on\nlayer-wise analyses of BERT representations include [30] and\n[31].\nOne method we use to analyze the representational geometry\nof our models is representational similarity analysis (RSA) [32],\nwhich compares models that represent stimuli using vectors\nwith different numbers of dimensions by measuring the\ncorrelation of second-order dissimilarity matrices with each\nother (ie, how dissimilar each pair of sentences is to each other\npair by some metric). RSA has been used recently to analyze\nlinguistic properties of deep learning models [33,34]. We use\nbasic RSA to correlate various representations that we extract\nfrom each layer of our fine-tuned models with a matrix that\ncorresponds to the ground-truth dissimilarity patterns found in\nthe test set. This allows us to measure the strength of a clinical\nsemantic signal through the layers of our networks and compare\nthis signal across both models and choices of representation.\nWe also employ a version of RSA that involves reweighting\nand linearly recombining the representational dissimilarity\nmatrices (RDMs) [35] to build a representational model that\nbest explains the ground-truth dissimilarity patterns in the test\nset. To our knowledge, this is the first use of this framework to\nexplore the representational space of a deep neural language\nmodel.\nContributions\nThis work presents the following contributions:\n• A transformer ensemble that achieves very competitive\nresults on a new clinical STS task (with predictions\nproducing a correlation of 0.87 with ground-truth similarity\nscores compared with the state-of-the-art correlation of\n0.9), serving as a very strong deep learning baseline for this\ntask.\n• An extensive qualitative analysis of the transformer\nensemble’s error cases in the task of clinical semantic\nsimilarity that highlights the inability of popular transformer\nmodels to capture fine-grained differences between\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 2https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nmedicinal sentence pairs, despite being trained on clinical\nor biomedical text.\n• A quantitative error analysis framework for STS that reveals\nthe shallow heuristics that transformer models learn to rely\non for this task.\n• The application of linear decoding and RSA to measure the\nsemantic similarity signal in intermediate token\nrepresentations of 5 popular transformer models, showing\nconvergent and divergent representational strategies that\nreflect the models’ performance on this task.\n• The first application (to the authors’ knowledge) of a\nreweighted and recombined version of RSA to neural\nlanguage models, indicating that better representations of\nsentence pairs may be synthesized by combining 2 layers\nfrom a relatively poorly performing biomedical transformer\nwith a simple textual feature signal, and suggesting new\ndirections for research in sentence embedding extraction.\nMethods\nData\nThe training data for this task were made up of 1642 sentence\npairs and their associated similarity scores and the test set was\nmade up of 412 sentence pairs. The similarity scores are floats\non a scale of 0 to 5, ranging from no similarity to semantically\nidentical. The annotations were performed by 2 medical experts\n(Donna Ihrke and Gang Liu [13]). The task is evaluated by the\nPearson correlation between the predictions of a model and the\nground-truth similarity scores.\nModels\nWe fine-tuned 5 transformer [14] models. These include\nBERT-Large [15], 3 variants of BERT that were fine-tuned on\ntext from the clinical domain, and XLNet-Large [19]. The 3\nBERT variants were BioBERT [16], ClinicalBERT [17,18],\nand Discharge Summary BERT (DS BERT) [17,18]. We also\ncreated a mean_score model by taking the average prediction\nof the 5 transformer models. A linear layer was added on top\nof the pooled output for each model to perform the regression.\nThe input for the BERT models was [CLS] + A + [SEP] + B\n+ [SEP], where [CLS] is the classification token, A and B are\nthe 2 text snippets, and [SEP] is the separator token. The input\nfor XLNet was A + [SEP] + B + [SEP] + [CLS]. We set the\nmaximum sequence length for each model to 128. As we add\n3 additional tokens to the input, any sentence pairs with over\n125 tokens in total were shortened. This affected 5 sentence\npairs, all of which were in the training set (with an average of\n7.6 removed tokens). Each model was trained over 23 epochs\nusing a batch size of 32. These models were trained using the\nPyTorch-Transformers library [36]. Our system architecture is\ndepicted in Figure 1. We submitted the predictions of 3 models\nfor evaluation on the n2c2 2019 Track 1 task: those from\nClinicalBERT, XLNet, and the mean_score model.\nFigure 1. Our system architecture for predicting the semantic textual similarity between two sentences using an ensemble of five Transformer models.\nResults\nOverview\nOur best performing model, the mean_score ensemble, achieved\na correlation of 0.87, reaching 6th place out of 33 teams in the\nn2c2 2019 Track 1 task. The best model on the task achieved\na correlation of 0.9 [37]. Our results are presented in Table 1.\nThe correlation between the predictions of each of 5 transformer\nmodels with all others is presented in Table 2. While the 3\nmodels that have been fine-tuned with biomedical or clinical\ntext (BioBERT, ClinicalBERT, and DS BERT) are more\ncorrelated with each other than with both XLNet and BERT,\nthe predictions of all models generally correlate strongly with\neach other.\nTable 1. Pearson correlation between the ground-truth labels and the predicted labels for each model.\nMean scoreXLNetDS BERTClinicalBERTBioBERTBERTModel\n0.8700.8370.8670.8540.8550.817Correlation\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 3https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 2. Correlation between the predictions of each transformer model on the test set.\nXLNetDS BERTClinicalBERTBioBERTBERTModel\n0.910.920.920.921BERT\n0.920.960.9510.92BioBERT\n0.920.9610.950.92ClinicalBERT\n0.9310.960.960.92DS BERT\n10.930.920.920.91XLNet\nError Analysis\nError Cases Investigation\nRather than only evaluating our transformer ensemble by the\ncorrelation between its predictions and the ground-truth\nsimilarity scores, we carried out an extensive investigation into\nthe error cases of this ensemble to shed light on any trends in\nthe biases and heuristics that the component models may have\nlearned from the training data. In this endeavor we carried out\nboth qualitative and quantitative error analyses. Both analyses\nuse a measure of loss that is calculated as the squared error\nbetween the models’ prediction and the ground-truth similarity\nscore.\nQualitative Analysis\nWe first carried out a qualitative analysis by grouping the\nsentence pairs that were most difficult to predict for the\ntransformer ensemble by the primary lexical, syntactic, or\nsemantic feature that we consider to be most salient and\ndistinguishing. By identifying common error clusters, we can\nbetter understand our models’ biases and attempt to mitigate\nthese issues in future iterations of the clinical STS system. A\nlist of these error categories as well as example sentences can\nbe found in Table 3. We took 100 sentence pairs from the test\ndata set with the highest loss and manually analyzed them to\nfind possible explanations for incorrect predictions. The main\ncategories that were identified are shown in Figures 2 and 3.\nWe divided the errors into 2 cases: those where the transformer\nensemble overpredicted sentence similarity with respect to the\nground truth (Figure 2, which includes 77 sentence pairs) and\nthose where the models underpredicted sentence similarity\n(Figure 3, which includes 23 sentence pairs).\nTable 3. Example sentence pairs and error type (ie, whether the transformer ensemble overpredicted or underpredicted semantic similarity with respect\nto the ground truth) for each error category selected for the qualitative analysis.\nNotesExample sentence pairCategoryError type\n(1) Ibuprofen [MOTRIN] 400 mg tablet 1 tablet by mouth every 4\nhours as needed. (2) Gabapentin [NEURONTIN] 300 mg capsule 1\ncapsule by mouth every bedtime.\nMedical prescriptionOverprediction\n(1) Patient to call to schedule additional treatment sessions as needed\notherwise patient dismissed from therapy. (2) Patient tolerated session\nwithout adverse reactions to therapy.\nLexical overlapOverprediction\nSome semantic overlap de-\nspite low ground-truth simi-\nlarity score of 0\n(1) The client verbalized understanding and consented to the plan of\ncare. (2) The patient consented to the possibility of blood transfusion.\nSemantic overlapOverprediction\nCommon phrase structures\noften feature lexical overlap,\nas well as strong syntactic\nsimilarity\n(1) male who presents for evaluation of Knee Pain (right). (2) female\nwho presents for evaluation of Ear Infection/ Ear Pain.\nReuse of phrase templateOverprediction\nNote quotation marks within\noriginal text\n(1) “Left upper extremity: Inspection, palpation examined and nor-\nmal.” (2) “Abdomen: Liver and spleen, bowel sounds examined and\nnormal.”\nSimilar punctuationOverprediction\nThe ensemble predicted a\nscore of 2.55/5 for this exam-\nple sentence pair\n(1) “Mental: Alert and oriented to person, place and time.” (2) She\ndemonstrated understanding and agreed to proceed as noted.\nUnknownOverprediction\n(1) He denies any shortness of breath or difficulty breathing. (2) Pa-\ntient denies any chest pain or shortness of breath.\nUnknownUnderprediction\n(1) “Thank you for choosing the Name, M.D.. care team for your\nhealth care needs!” (2) Thank you for choosing Location for your\nhealth care and wellness needs.\nDifferent punctuationUnderprediction\nSemantic similarity with lit-\ntle lexical overlap\n(1) The above has been discussed and reviewed in detail with the pa-\ntient. (2) The family was advised that the content of this interview\nwill be shared with the health care team.\nLack of lexical overlapUnderprediction\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 4https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 2. Common categories of error for cases when the model over-predicts similarity as identified by manual analysis of the 100 worst predictions.\nFigure 3. Common categories of error for cases when the model under-predicts similarity as identified by manual analysis of the 100 worst predictions.\nQuantitative Analysis\nTo complement our qualitative analysis, we developed a simple\nSTS quantitative analysis framework that allows us to\ninvestigate the relationship between surface features of the\nsentence pairs and our model’s performance. This involves\nmeasuring the correlation between model loss and various\nfeatures of the sentence pairs. In addition to providing the results\nfor all labels, we present correlations (measured using Spearman\nrho) between the loss and pair features for each similarity score\nin the test set. The results are shown in Table 4. Below is an\nexplanation of each sentence-pair feature that we investigated:\n• Average sentence length: The total amount of tokens across\nthe 2 sentences.\n• Scaled total token frequency: The number of times each\ntoken in the sentence pair appears in the training set divided\nby the average sentence length, calculated after we removed\nstop words.\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 5https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n• Scaled unseen tokens per pair: The number of tokens in the\nsentence pair that do not appear in the training corpus,\ndivided by the average sentence length.\n• Scaled difference in token frequency: The difference\nbetween the training corpus token frequency across the 2\nsentences, divided by the average sentence length,\ncalculated after we removed stop words.\n• Jaccard distance: The distance between the token sets of 2\nsentences in a pair measured as\n1 – (|A ∩ B|)/(|A ∪ B|)\nTable 4. Correlation (Spearman rho) between the model’s loss (mean score) per sentence pair and various sentence-pair features.\nJaccard distanceScaled difference in token\nfrequency\nScaled unseen tokens\nper pair\nScaled total token frequencyAverage sentence lengthLabela\n−0.0250.0740.0200.142−0.132All\n−0.554 (<.001)b0.219−0.2630.391−0.3100.0\n−0.202−0.010−0.249−0.1140.1020.5\n−0.074−0.0330.047−0.0430.0671.0\n−0.153−0.2810.033−0.1510.0041.5\n−0.3380.3540.0120.4410.1182.0\n0.1090.070−0.2380.014−0.0182.5\n0.119−0.026−0.0980.432−0.4533.0\n0.587−0.0460.257−0.051−0.4403.5\n0.1710.0520.2680.138−0.0884.0\n0.4680.033−0.221−0.266−0.1814.5\n0.5960.590−0.2420.789 (.042)−0.0405.0\naLabels are ground-truth similarity scores.\nbSignificant P value is reported in parenthesis after Bonferroni correction.\nLayer-wise Token Representation Decoding\nGiven the difficulty of analyzing how these models build\nrepresentations of clinical STS by looking at their loss alone,\nwe next performed a layer-wise decoding analysis by training\nlinear regression models to predict between-sentence semantic\nsimilarity given representations from each transformer across\ndifferent layers of the model. By decoding the semantic signal\nin the intermediate layers of each model, we can uncover the\nmechanisms that transformer models use to predict clinical\nsemantic similarity. We can then investigate whether any\nrepresentational strategies correspond to better performance on\nthis task, shedding light on why certain constituent models of\nthe transformer ensemble perform worse, and potentially\nindicating directions for sentence-pair embedding extraction\nfor STS. In the case of 12-layer models we used each layer and\nin the case of the larger 24-layer models, we used every other\nlayer. This allows for direct comparison of representations by\nrelative depth through the network.\nWe chose a variety of representations to decode. As we have\nmany tokens per sentence pair, there are many different possible\nways to map this list of vectors to a fixed-length representation.\nWe aimed to choose representations that can reveal potential\nstrategies and heuristics that our models use to predict semantic\nsimilarity. In doing so, we may also reveal how different types\nof models (ie, those trained on clinical versus general domain\ntext, or those with BERT/XLNet-style architectures) diverge\nor converge in their representational transformation strategies.\nThe chosen representations were\n• [CLS]: The token vector corresponding to the classification\ntoken input.\n• avg_reps_concat: Concatenation of the mean-pooled token\nvector representations of sentences A and B.\n• max_reps_concat: Concatenation of max-pooled token\nvectors within sentences A and B.\n• sent_avg_difference: The absolute difference in average\ntoken vector representations in sentences A and B.\n• sent_max_difference: The absolute difference in\nmax-pooled token vector representations in sentences A\nand B.\n• sent_a_avg_max_concat: Concatenation of mean- and\nmax-pooled token vectors from sentence A.\n• sent_b_avg_max_concat: Concatenation of mean- and\nmax-pooled token vectors from sentence B.\nThe linear regression models were evaluated using 10-fold\ncross-validation. Table 5 shows the overall best representations\nfor decoding similarity score. Figures 4 and 5 feature layer-wise\ncorrelation plots for representations based on the classification\ntoken vector (Figure 4) and the absolute difference between the\naverage token vectors in each sentence (Figure 5).\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 6https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 5. The overall top decoding scores ranked in descending order. All the top-performing representations were extracted from XLNet and are mostly\nmade up of the concatenation of the max-/mean-pooled token representations in the 2 sentences that were extracted from middle-late layers.\nCorrelationLayerRepresentationModel\n0.918max_reps_concatXLNet-large\n0.8918sent_a_avg_max_concatXLNet-large\n0.8818avg_reps_concatXLNet-large\n0.8820max_reps_concatXLNet-large\n0.8816avg_reps_concatXLNet-large\n0.8820avg_reps_concatXLNet-large\n0.8718sent_b_avg_max_concatXLNet-large\n0.8714sent_b_avg_max_concatXLNet-large\n0.8714max_reps_concatXLNet-large\n0.8716max_reps_concatXLNet-large\nFigure 4. Pearson correlation between linear regression models’ predictions of a sentence pair’s semantic similarity and the ground-truth score (10-fold\ncross-validated on test-set) using [CLS] token pair representations.\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 7https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 5. Pearson correlation between linear regression models’ predictions of a sentence pair’s semantic similarity and the ground-truth score (10-fold\ncross-validated on test-set) using the absolute difference between each sentence’s mean-pooled token vector.\nRepresentational Similarity Analysis\nOverview\nTo find which representations learned by our models best\nexplain the representational geometry of the semantic similarity\ntask, we carried out 2 types of investigations within the\nframework of RSA. We use RSA to complement our layer-wise\nlinear probing analysis, as it can reveal second-order\nrepresentational patterns across many samples, while the\nlayer-wise probing analysis relies on identifying particular\ndimensions of the representational space that predict semantic\nsimilarity. By taking these methods together, we can reach more\nrobust conclusions about how transformer models build\nrepresentations of semantic similarity and use this information\nto understand the performance of these models and identify how\nwe can improve them. The data RDMs that we compared with\nthe ground-truth RDM were extracted from each layer of each\nof the 5 transformer models, for each of the pair representations\ndefined in the previous decoding analysis as well as 3 additional\npotential explanatory representations:\n• avg_representation: The average across all token vectors.\n• avg_sent_cosine_dist: The cosine distance between the\nmean-pooled token vector representations in sentences A\nand B.\n• max_sent_cosine_dist: The cosine distance between the\nmax-pooled token vector representations in sentences A\nand B.\nBasic RSA\nIn our first RSA experiment, we performed a basic analysis in\nwhich we measure the Spearman correlation between a model\nRDM (calculated using the distance between all the samples in\nthe test set measured by their ground-truth similarity score) and\nvarious representations elicited from our transformer models.\nUsing the 412 test sentence pairs we produced the 412 × 412\nmatrix shown in Figure 6.\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 8https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 6. Model representational dissimilarity matrix for 412 test sentence pairs measured by distance between ground-truth semantic similarity scores.\nThe dimensions of the dissimilarity matrix are sorted by each sentence-pair’s ground-truth semantic similarity score.\nReweighted and Recombined RSA\nWe then found a combination of representations from all layers\nof each of the separate 5 transformer models and an RDM made\nup of text features (detailed in the “Quantitative Analysis”\nsection) that best explains the ground-truth model when linearly\nrecombined. Each explanatory RDM in a given trial had an\nassociated weight that altogether summed to 1. These weights\nwere found using a non-negative least squares (NNLS) solver\nusing 10-fold cross-validation. This analysis revealed that the\nbest performing explanation model was BioBERT. The final\nBioBERT-reweighted explanatory RDM is shown in Figure 7.\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 9https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 7. The final best-fitting re-weighted and linearly re-combined explanatory model found using NNLS and representations from BioBERT,\nachieving a correlation of 0.54 with the ground-truth model. The dimensions of the dissimilarity matrix are sorted by each sentence-pair’s ground-truth\nsemantic similarity score.\nLayer-wise Reweighted RSA\nIn the final part of our reweighted RSA, we revisited the\nrepresentations of BERT-Large to investigate why the\nclassification token suddenly becomes less representative of\nthe ground-truth similarity score around layers 12-16 as\nmeasured by linear regression probing (Figure 4) and RSA\ncorrelation (Figure 8). We reran the NNLS solver for the\nBERT-Large representations (using 10-fold cross-validation)\nbut this time we excluded the text features RDM and used token\nvectors from only 1 layer at a time. We performed this analysis\nfor the even layers, from layers 2 to 24 (as we had previously\nextracted every other layer of the 24-layer models to directly\ncompare representations with 12-layer models based on relative\ndepth through the network), and retrieved the values used to\nreweight the RDM for each layer. The plot of weights associated\nwith each representation can be seen in Figure 9.\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 10https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 8. Correlation between the ground-truth model RDM and explanatory RDMs constructed from [CLS] token pair representations.\nFigure 9. Weights associated with sentence-pair representations of BERT-Large found using NNLS to minimise the distance between a linearly\nre-combined set of RDMs and the ground-truth model RDM for each layer.\nDiscussion\nPrincipal Results\nQualitative Error Analysis\nIn the case of sentence pairs that caused our ensemble to\noverpredict semantic similarity (Figure 2), the most obvious\nproblem with our ensemble was its failure to model the semantic\nsimilarity of 2 sentences which contain details of medical\nprescriptions. This is likely because our models do not have the\nadvanced level of domain knowledge necessary to correctly\nmodel this problem. As these sentences are usually very similar\n(apart from the name of a drug and dosage), the models\noverpredict similarity. The second biggest issue when\noverpredicting similarity is when there is a lexical overlap\nwithout semantic overlap. This suggests that our models\nover-rely on surface features such as token overlap. In most\ncases when our model underpredicts similarity, there is no\nobvious possible explanation. However, in the interpretable\nsamples the issue was usually that synonyms were used, again\nsuggesting an over-reliance on lexical overlap, and potentially\nmotivating a concept normalization preprocessing step. In any\ncase, the qualitative approach to analysis error is relatively\nlimited for interpreting the instances of underprediction of\nsemantic similarity for this ensemble. This limitation is mitigated\nby the fact that overpredictions made up the majority of the\nlargest errors (77 out of 100). By taking both the cases of\nunderprediction and overprediction together, it is clear that\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 11https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nsimple heuristics, such as predicting similarity given lexical\noverlap, are prominent within the transformer ensemble, and\nthat these transformer models still lack the ability to produce\nthe extremely fine-grained clinical semantic representations that\nare required to implicitly calculate semantic distances between\nmedical concepts (eg, particular drugs) given a relatively small\ntask set. Any future work would have to address these issues;\nfor example, by augmenting the data using a concept\nnormalization preprocessing step, or by enriching the ensemble’s\ndomain knowledge by incorporating a clinical terms resource.\nQuantitative Error Analysis\nOverall, Table 4 shows a weak negative correlation between\nthe average sentence length and loss. This relationship is\nrelatively strong for entirely dissimilar sentence pairs and\nmoderately similar sentence pairs and may be explained by the\nfact that longer sentences provide more contextual information\nthat can be used to decide whether 2 sentences are semantically\nsimilar. Another trend is for the loss to increase with the scaled\ntotal token frequency (ie, how often the words in the pair appear\nin the training corpus), particularly in the case where the 2\nsentences are semantically identical. This relationship is difficult\nto interpret, but additional analysis could investigate the extent\nto which the loss can be explained using the relative frequency\nof the words given a more general corpus (such as Wikipedia),\nto separate the effect of clinical term frequency.\nWe also see that Jaccard distance is negatively correlated with\nloss for sentence pairs that are less semantically similar and\npositively correlated with loss for pairs that are more\nsemantically similar. One possible explanation for this\nobservation is that our deep transformer models have learned\nan appropriate strategy of predicting low similarity scores given\ntoken overlap for the extreme case when sentence pairs are\ndissimilar and have little overlap. However, the model seems\nunable to apply such a shallow heuristic in cases where sentence\npairs are very semantically similar. Further analysis showed\nJaccard distance to be very significantly negatively correlated\nwith the ground-truth label (P<.001), which may indicate that\na deep ensemble model could benefit from the presence of\ntraditional machine learning models that are trained on simple\nfeatures of the text such as relative overlap between tokens.\nThe quantitative analysis approach has both verified the\nexistence of overall heuristics that use surface features of the\nsentence pairs to predict semantic similarity as noted in the\nprevious qualitative analysis and allowed to us examine these\ntrends as they occur within certain ranges of semantic similarity\nscores. This approach to quantitative analysis of STS errors has\nthus produced a richer view of these biases, while still\nsuggesting that these deep transformer models use a set of\nrelatively shallow strategies for this task.\nLayer-wise Token Representation Decoding\nThe first striking pattern to note in Figure 4 is that the BERT\nmodels tend to drop in performance on the CLS token task in\nthe middle of the network, thereafter reaching their apexes (in\nthe extreme case this is amplified in BERT-Large), whereas\nXLNet tends to steadily increase to its highest point before\ndropping off over the rest of the network. This indicates that in\nBERT-style models, the [CLS] token does not serve as the\nprimary representation of semantic similarity in the middle\nlayers. Second, the correlation between linear model predictions\nand ground-truth scores held-out folds almost always\nmonotonically increases for the difference between average\nsentence representations for all BERT-style models (Figure 5).\nThis contrasts with the performance on the XLNet sent_avg_diff\nrepresentation, which caps half-way through the network, then\ndrops off steadily beginning a few layers later. It appears that\nXLNet builds a good representation based on the mean-pooled\ntoken representations, but that this information is integrated in\nthe middle of processing and subsequently discarded around\nlayer 18.\nAll the top 10 best decoding scores across all representations\nwere extracted from XLNet (Table 5). Overall, XLNet did best\nusing the max_reps_concat, reaching a correlation of 0.9 in\nlayer 18, which represents a 7.5% increase in that model’s initial\nperformance on the test set. This demonstrates that given the\ninitial representations of a large deep model, it may be possible\nto increase its performance very inexpensively and massively\non small amounts of held-out data using a simple linear model\nand the correct choice of representation.\nIt is clear from the linear decoding experiment that the\nrepresentational strategies of the transformers fine-tuned with\nbiomedical or clinical documents tend to align, with each model\ngradually building better representations of STS over the course\nof their layers in an almost always monotonic fashion, in both\nthe [CLS] token and the absolute difference between\nmean-pooled sentence representations. This is in contrast to the\nrelatively erratic differences between decodability over layers\nseen with BERT-Large and XLNet, where decodability will\nrapidly gain or fall over the course of 1-2 layers, especially\nwhen looking at the distance between mean-pooled sentence\nvectors representation. This result suggests that models with\nmore clinical domain knowledge (and better performance on\nthis task) learn to build robust representations of clinical\nsemantic similarity (ie, not only using the [CLS] token or the\ndistance between mean-pooled vectors) and that this information\nis gradually recovered in a steady, step-wise manner.\nRepresentational Similarity Analysis\nBasic RSA\nIn carrying out the single-correlation RSA task, we found\nconfirmation for some of the representational trends identified\nduring the decoding task. Two of such trends are presented in\nFigures 8 and 10, which include the correlation of the model\nRDM with data RDMs built using classification tokens (Figure\n8) and the absolute difference between average token vectors\nfrom the 2 sentences in a pair (Figure 10). As was previously\nshown in Figure 4, BERT-Large diverges drastically from the\nother models in how representative the classification token is\nof a sentence pair’s semantic similarity score around layers\n12-16, while all other models generally generate progressively\nbetter [CLS] tokens throughout the network, with only slight\nloss in performance around the middle of the network. The\nperformance of BERT-Large [CLS] representations on this task\nagain reflects its final score, which was the lowest of the 5\nmodels. We further analyzed the representational geometry of\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 12https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nBERT-Large in our reweighting analysis later in the current\nsection to better understand this observation. The confirmation\nof this considerable drop in decodability performance shows\nthat this trend does not simply reflect the inability of the linear\nregression models to predict semantic information due to the\nsmall amount of data. Likewise, the correlation plot featured in\nFigure 10 presents more evidence for our previous finding that\nBERT-style models seem to represent across-sentence similarity\nby minimizing the average difference in token vectors. While\nthese correlations are positive from layers 4 to 12, this signal\nis not as strong as would be indicated by the probing analysis,\nsuggesting that this strategy may not be a primary heuristic. In\nany case, taken together, these 2 layer-wise correlation plots\nshow that the probing task produces robust metrics of\nrepresentational trends, and that probing and basic RSA are\ncomplementary approaches to the analysis of transformations\nin token vectors of deep transformer models.\nFigure 10. Correlation between the ground-truth model RDM and explanatory RDMs constructed using the absolute difference between each sentence’s\nmean-pooled token vector.\nReweighted and Recombined RSA\nAfter performing the next stage in our RSA, reweighting and\nrecombining a set of RDMs (using all layers using all\nrepresentations, as well as the text features RDM) for each\ntransformer to minimize the distance between the new RDM\nand the ground-truth representation, we found that the best\nchoice of model was BioBERT. Figure 7 shows visual\nconfirmation that much of the ground-truth dissimilarity\npatterning (Figure 6) has been reproduced by this explanatory\nmodel. This result was somewhat unexpected, given that this\nmodel did not perform best on the test set. This finding suggests\nthat when generating sentence-pair vectors, it may in some cases\nbe better to reweight and combine representations from\nrunner-up models, rather than using the single best model. The\nweights learned for each RDM in the BioBERT model (Figure\n11) show that the RDM is mostly made up of the final layer’s\n[CLS] token, although it has been reweighted using the cosine\ndistance between the average token vector of the 2 sentences\nin a given pair and the Jaccard distance between the 2 sentences.\nWe believe that beyond revealing how well each representation\nexplains the ground-truth semantic similarity, this technique\nhas promising potential for generating sentence embeddings for\ndownstream tasks.\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 13https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 11. Proportion of weights learned for the best explanatory model (which used BioBERT representations and text features).\nLayer-wise Reweighted RSA\nBy looking at the weights learned for each component of the\nlayer-wise BERT explanatory model (Figure 9), we find that\nafter layer 8, the weight associated with the average token\nrepresentation drastically increases and this representation\nbecomes dominant for the remaining layers, whereas the\nexplanatory weight of the [CLS] token peaks at layer 8 before\nrapidly declining. We link this result to our finding that the\nworst linear probing and RSA correlations for BERT’s [CLS]\ntokens start to occur after layer 8 (Figures 4 and 8). This\nsuggests that in middle to late layers, BERT-Large focuses on\nbuilding better mean-pooled representations of the sentence\npairs, an interpretation which is in line with the dramatic\nincrease in correlation between BERT-Large’s representations\nand the ground-truth model when using the absolute difference\nbetween the average token vector of each sentence as the data\nRDM (Figure 10). This interpretation is also compatible with\nthe increase in linear regression performance when using\nBERT-Large token vectors and taking the absolute difference\nbetween the average token vectors in each sentence as input\n(Figure 5).\nLimitations and Future Work\nWhile we employed the use of cross-validation for our linear\nprobing and NNLS RSA tasks, it should be noted that our test\nset of 412 sentence pairs represents a relatively small amount\nof data and as such it may be difficult to assess whether our\nresults would generalize to more data-rich contexts. One\npotential method for partially mitigating this problem would be\nto cross-validate our results across the full set of 2054 sentence\npairs, rather than restricting the analysis to the original test set\nfrom the clinical STS task. While this approach may lead to\ninsights into the robustness of our interpretation, we consider\nit to be outside of the scope of this work as we aim to analyze\nthe errors and representational strategies that both result from\nthe inductive biases of transformer models and reflect biases\nlearned from the task’s data. Restricting our analysis to the\noriginal 412 sentence-pair test set thus enables direct comparison\nwith other models trained on the same data. Another issue with\ncross-validating across the whole data set is that we will always\nbe limited to a relatively small amount of data for this task, as\neven testing on a slice of 50% of the total data would still only\nallow for 1027 sentence pairs for evaluation. It could also be\ninsightful to carry out our analysis on models trained using\nlarger general domain semantic similarity tasks that feature\nmore sentence pairs. We again consider this line of research to\nbe out of scope for this work.\nIn future work we wish to investigate to what extent we can\ndirectly use a layer’s token representations to automatically\nlearn interpretable explanations that minimize the distance\nbetween a reweighted RDM and the ground-truth model RDM.\nWe expect that incorporating our models’attention weights will\nbe essential at that level of analysis. Additionally, we wish to\nset alternative target RDMs to examine how we can recombine\nthe token vectors in a sentence pair to best explain the model’s\nclassification token, thereby further exploring the inner\nrepresentational dynamics of fine-tuned transformer models.\nConclusion\nWe tackled a recent clinical STS task using a variety of\ntransformer models, including both those trained on general\ndomain language and models that were further trained on clinical\ntext. After achieving a high correlation between the predictions\nof a mean-pooled ensemble of these models and the test-set\nground truth, we analyzed the error cases of our model both\nqualitatively and quantitatively, finding groups of semantically\nrelated sentences that are generally difficult for our transformers\nto model and identifying surface features of the sentence pair\nthat significantly correlate with loss for particular ranges of the\nsemantic similarity space. These findings suggest potential\navenues for further improvement, for example, by augmenting\nour models to allow them to directly take traditional NLP textual\nfeatures into account.\nWe then carried out 2 types of representational analyses, namely,\nlinear decoding and RSA, to shed light on the heuristics on\nwhich these models have learned to rely. These approaches were\nshown to be complementary and revealed divergent\nrepresentational strategies for predicting textual similarities\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 14https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nbetween BERT-style models and XLNet. Furthermore, our\nsearch through the representational space for the best\nexplanatory model of the ground-truth data suggests that a large\namount of this information can be captured using a combination\nof a classification token and the cosine distance between\nsentence-pair representations in the first layer of a transformer\nmodel that did not produce the best predictions on the test set,\nsuggesting interesting directions for research in model\ndistillation and sentence embedding extraction.\nAcknowledgments\nThis research was funded by a Northern Ireland Health and Social Care Board eHealth Directorate grant (grant no. 24F-1801) to\nBD.\nConflicts of Interest\nNone declared.\nReferences\n1. Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R. Indexing by latent semantic analysis. J. Am. Soc. Inf.\nSci 1990 Sep;41(6):391-407. [doi: 10.1002/(sici)1097-4571(199009)41:6<391::aid-asi1>3.0.co;2-9]\n2. Salton G. Automatic text processing: the transformation, analysis, and retrieval of information by computer. Choice Reviews\nOnline 1989 Sep 01;27(01):27-0351-27-0351. [doi: 10.5860/choice.27-0351]\n3. Lee M, Pincombe B, Welsh M. An Empirical Evaluation of Models of Text Document Similarity. In: Proceedings of the\nAnnual Meeting of the Cognitive Science Society. 2005 Jul Presented at: Annual Conference of the Cognitive Science\nSociety; 21-23 July 2005; Stresa, Italy p. 1254-1259 URL: https://escholarship.org/uc/item/48g155nq\n4. Agirre E, Cer D, Diab M, Gonzalez-Agirre A. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity\n(SemEval@NAACL-HLT). 2012 Jun Presented at: *SEM 2012: The First Joint Conference on Lexical and Computational\nSemantics; June 7-8, 2012; Montréal, QC, Canada p. 285-393. [doi: 10.18653/v1/s17-2001]\n5. Agirre E, Cer D, Diab M, Gonzalez-Agirre A, Guo W. *SEM 2013 shared task: Semantic Textual Similarity\n(*SEM@NAACL-HLT). 2013 Jun Presented at: Second Joint Conference on Lexical and Computational Semantics (*SEM);\nJune 2013; Atlanta, GA, USA p. 32-43.\n6. Agirre E, Banea C, Cer D, Diab M, Gonzalez-Agirre A, Mihalcea R, et al. SemEval-2016 Task 1: Semantic Textual\nSimilarity, Monolingual and Cross-Lingual Evaluation (SemEval@NAACL-HLT). 2016 Jun Presented at: 10th International\nWorkshop on Semantic Evaluation (SemEval-2016); June 2016; San Diego, CA p. 497-511. [doi: 10.18653/v1/s16-1081]\n7. Cer D, Diab M, Agirre E, Lopez-Gazpio I, Specia L. SemEval-2017 Task 1: semantic textual similarity - multilingual and\ncross-lingual focused evaluation. arXiv. 2017 Aug. URL: https://arxiv.org/abs/1708.00055 [accessed 2021-04-19]\n8. McCrae J, Buitelaar P. Linking datasets using semantic textual similarity. Cybernetics and information technologies. 2018\nMar. URL: https://sciendo.com/article/10.2478/cait-2018-0010 [accessed 2021-04-19]\n9. Boltužić F, Šnajder J. Identifying prominent arguments in online debates using semantic textual similarity. In: Proceedings\nof the 2nd Workshop on Argumentation Mining. 2015 Jun 4 Presented at: NAACL HLT 2015; 4 June 2015; Denver,\nColorado, USA p. 110-115. [doi: 10.3115/v1/w15-0514]\n10. Rastegar-Mojarad M, Liu S, Wang Y, Afzal N, Wang L, Shen F, et al. Biocreative/OHNLP challenge 2018. In: Proceedings\nof the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics. 2018 Aug\n15 Presented at: ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics; August\n2018; Washington, DC p. 575-575. [doi: 10.1145/3233547.3233672]\n11. Wang Y, Afzal N, Fu S, Wang L, Shen F, Rastegar-Mojarad M, et al. MedSTS: a resource for clinical semantic textual\nsimilarity. Lang Resources & Evaluation 2018 Oct 24;54(1):57-72. [doi: 10.1007/s10579-018-9431-1]\n12. Wang Y, Afzal N, Liu S, Rastegar-Mojarad M, Wang L, Shen F, et al. Overview of the BioCreative/OHNLP challenge\n2018 task 2: clinical semantic textual similarity. In: Proceedings of the BioCreative/OHNLP Challenge 2018. 2018 Aug\nPresented at: BioCreative/OHNLP Challenge 2018; August 29, 2018; Virtual Conference p. 575. [doi:\n10.13140/RG.2.2.26682.24006]\n13. Wang Y, Fu S, Shen F, Henry S, Uzuner O, Liu H. The 2019 n2c2/OHNLP Track on Clinical Semantic Textual Similarity:\nOverview. JMIR Med Inform 2020 Nov 27;8(11):e23375. [doi: 10.2196/23375] [Medline: 33245291]\n14. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A, et al. Attention is all you need. 2017 Presented at: 31st\nConference on Neural Information Processing Systems (NIPS 2017); December 4-9, 2017; Long Beach, CA p. 1-11 URL:\nhttps://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n15. Devlin J, Chang M, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019 Jun Presented at: 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language Technologies; June\n3-5, 2019; Minneapolis, MN, USA p. 4171-4186. [doi: 10.18653/v1/N19-1423]\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 15https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n16. Lee J, Yoon W, Kim S, Kim D, Kim S, So C, et al. BioBERT: a pre-trained biomedical language representation model for\nbiomedical text mining. Bioinformatics 2020;36(1):1234-1240. [doi: 10.1093/bioinformatics/btz682]\n17. Huang K, Altosaar J, Ranganath R. ClinicalBERT: modeling clinical notes and predicting hospital readmission. arXiv.\n2019. URL: https://arxiv.org/abs/1904.05342 [accessed 2020-11-29]\n18. Alsentzer E, Murphy J, Boag W, Weng W, Jindi D, Naumann T, et al. Publicly available clinical BERT embeddings. arXiv.\nJun. URL: https://arxiv.org/abs/1904.03323 [accessed 2021-04-19]\n19. Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov R, Le Q. XLNet: generalized autoregressive pretraining for language\nunderstanding. arXiv. 2019 Jun 19. URL: https://arxiv.org/abs/1906.08237 [accessed 2020-01-02]\n20. Linzen T, Dupoux E, Goldberg Y. Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies. TACL 2016\nDec;4:521-535. [doi: 10.1162/tacl_a_00115]\n21. Gulordava K, Bojanowski P, Grave E, Linzen T, Baroni M. Colorless green recurrent networks dream hierarchically. arXiv.\n2018 Mar. URL: https://arxiv.org/abs/1803.11138 [accessed 2018-03-29]\n22. McCoy R. Right for the wrong reasons: diagnosing syntactic heuristics in natural language inference. arXiv. 2019 Feb.\nURL: https://arxiv.org/abs/1902.01007 [accessed 2019-06-04]\n23. Hupkes D, Veldhoen S, Zuidema W. Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural\nnetworks process hierarchical structure. Journal of Artificial Intelligence Research 2018 Apr 30:907-926 [FREE Full text]\n[doi: 10.24963/ijcai.2018/796]\n24. van Aken B, Winter B, Löser A, Gers FA. How Does BERT Answer Questions?: A Layer-Wise Analysis of Transformer\nRepresentations. In: Proceedings of the 28th ACM International Conference on Information and Knowledge Management.\n2019 Nov Presented at: ACM International Conference on Information and Knowledge Management; November 3-7, 2019;\nBeijing, China p. 1823-1832. [doi: 10.1145/3357384.3358028]\n25. Tenney I, Xia P, Chen B, Wang A, Poliak A, McCoy RT, et al. What do you learn from context? Probing for sentence\nstructure in contextualized word representations. arXiv. 2019 May 15. URL: https://arxiv.org/abs/1905.06316 [accessed\n2019-05-15]\n26. Lin Y, Tan Y, Frank R. Open sesame: getting inside BERT's linguistic knowledge. arXiv. 2019 Jun 04. URL: https://arxiv.\norg/abs/1906.01698 [accessed 2020-06-04]\n27. Voita E, Serdyukov P, Sennrich R, Titov I. Context-aware neural machine translation learns Anaphora resolution. arXiv.\n2018 May 25. URL: https://arxiv.org/abs/1805.10163 [accessed 2018-05-25]\n28. Jain S, Wallace B. Attention is not Explanation. In: Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\n2019 Jun Presented at: 2019 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies; June 3-5, 2019; Minneapolis, MN, USA p. 3543-3556.\n29. Wiegreffe S, Pinter Y. Attention is not not Explanation. In: Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\n2019 Nov Presented at: 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP); November 3-7, 2019; Hong Kong, China p. 11-20.\n[doi: 10.18653/v1/D19-1002]\n30. Reif E, Yuan A, Wattenberg M, Viegas FB, Coenen A, Pearce A, et al. Visualizing and measuring the geometry of BERT.\nIn: Advances in Neural Information Processing Systems. 2019 Dec Presented at: Advances in Neural Information Processing\nSystems; December 2019; Vancouver, BC, Canada.\n31. Hewitt J, Manning C. A Structural Probe for Finding Syntax in Word Representations. In: Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers). 2019 Jun Presented at: 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies; June 3-7, 2019; Minneapolis, MN p. 4129-4138.\n32. Kriegeskorte N, Mur M, Bandettini P. Representational similarity analysis - connecting the branches of systems neuroscience.\nFront Syst Neurosci 2008;2:4 [FREE Full text] [doi: 10.3389/neuro.06.004.2008] [Medline: 19104670]\n33. Abnar S, Beinborn L, Choenni R, Zuidema W. Blackbox meets blackbox: Representational Similarity and Stability Analysis\nof Neural Language Models and Brains. arXiv. 2019 Jun 04. URL: https://arxiv.org/abs/1906.01539 [accessed 2019-06-04]\n34. Abdou M, Kulmizev A, Hill F, Low DM, Søgaard A. Higher-order comparisons of sentence encoder representations. arXiv.\n2019 Sep 1. URL: https://arxiv.org/abs/1909.00303 [accessed 2019-09-01]\n35. Khaligh-Razavi S, Kriegeskorte N. Deep supervised, but not unsupervised, models may explain IT cortical representation.\nPLoS Comput Biol 2014 Nov;10(11):e1003915 [FREE Full text] [doi: 10.1371/journal.pcbi.1003915] [Medline: 25375136]\n36. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al. HuggingFace's transformers: state-of-the-art natural\nlanguage processing. arXiv. 2019 Oct 09. URL: https://arxiv.org/abs/1910.03771 [accessed 2019-10-09]\n37. Mahajan D, Poddar A, Liang J, Lin Y, Prager J, Suryanarayanan P, et al. Identification of Semantically Similar Sentences\nin Clinical Notes: Iterative Intermediate Training Using Multi-Task Learning. JMIR Med Inform 2020 Nov 27;8(11):e22508\n[FREE Full text] [doi: 10.2196/22508] [Medline: 33245284]\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 16https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nAbbreviations\nBERT: bidirectional encoder representations from transformers\nNLP: natural language processing\nNNLS: non-negative least squares\nRDM: representational dissimilarity matrix\nRSA: representational similarity analysis\nSTS: semantic textual similarity\nEdited by Y Wang; submitted 31.07.20; peer-reviewed by M Torii, L Ferreira; comments to author 13.11.20; revised version received\n07.01.21; accepted 23.01.21; published 26.05.21\nPlease cite as:\nOrmerod M, Martínez del Rincón J, Devereux B\nPredicting Semantic Similarity Between Clinical Sentence Pairs Using Transformer Models: Evaluation and Representational Analysis\nJMIR Med Inform 2021;9(5):e23099\nURL: https://medinform.jmir.org/2021/5/e23099\ndoi: 10.2196/23099\nPMID:\n©Mark Ormerod, Jesús Martínez del Rincón, Barry Devereux. Originally published in JMIR Medical Informatics\n(https://medinform.jmir.org), 26.05.2021. This is an open-access article distributed under the terms of the Creative Commons\nAttribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work, first published in JMIR Medical Informatics, is properly cited. The complete\nbibliographic information, a link to the original publication on https://medinform.jmir.org/, as well as this copyright and license\ninformation must be included.\nJMIR Med Inform 2021 | vol. 9 | iss. 5 | e23099 | p. 17https://medinform.jmir.org/2021/5/e23099\n(page number not for citation purposes)\nOrmerod et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.7799999117851257
    },
    {
      "name": "Semantic similarity",
      "score": 0.7354668974876404
    },
    {
      "name": "Computer science",
      "score": 0.7250999212265015
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6855772733688354
    },
    {
      "name": "Sentence",
      "score": 0.6524336934089661
    },
    {
      "name": "Security token",
      "score": 0.5957943797111511
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5887894034385681
    },
    {
      "name": "Transformer",
      "score": 0.4954596757888794
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126231945",
      "name": "Queen's University Belfast",
      "country": "GB"
    }
  ],
  "cited_by": 29
}