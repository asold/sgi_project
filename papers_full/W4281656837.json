{
  "title": "Machine Translation of English Language Using the Complexity-Reduced Transformer Model",
  "url": "https://openalex.org/W4281656837",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2099040957",
      "name": "Qin Li",
      "affiliations": [
        "Hubei University of Education"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3204406378",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W3204663006",
    "https://openalex.org/W3028876793",
    "https://openalex.org/W4224282638",
    "https://openalex.org/W3181270935",
    "https://openalex.org/W4220798378",
    "https://openalex.org/W3110027409",
    "https://openalex.org/W4226250534",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4386566861"
  ],
  "abstract": "Previous translation models like statistical machine translation (SMT), rule-based machine translation (RBMT), hybrid machine translation (HMT), and neural machine translation (NMT) have reached their performance bottleneck. The new Transformer-based machine translation model has become the favorite choice for English language translation. For instance, Google’s BERT translation model organizes the Transformer module into bidirectional encoder representations. It is aware of the users’ search intentions as well as the material that the search engine has indexed. It does not need to evaluate previous searches to comprehend what people mean, unlike RankBrain. BERT comprehends words, sentences, and complete information in the same way that we do. It achieves remarkable translation quality improvement over the other state-of-the-art benchmarks. It demonstrates the great potential of the Transformer model. The Transformer-based translation model mainly improves the performance at the cost of growing model sizes and complexity, usually requiring million-scale parameters. It is hard for the traditional computing systems to cope with the growing memory and computation requirements. However, the latest computers can easily run this model without any lag. The biggest challenge of applying the Transformer model is to deploy these models efficiently onto real-time or embedded devices. In this work, we propose a quantization scheme to reduce the parameter and computation complexity. It is of great importance to promote the usage of the Transformer model. Our experiment results show that the original Transformer model in 32 bit floating-point can be quantized to only 8 bits to 12 bits with only negligible translation quality loss. However, due to the perfect transformation of the block part, this quality loss part can easily be managed by the users. Meanwhile, our algorithm achieves <math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"M1\"> <mn>2.6</mn> <mo>×</mo> </math> to <math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"M2\"> <mn>4.0</mn> <mo>×</mo> </math> compression ratio, which is helpful to save the required complexity and energy during the inference phase.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9024019837379456
    },
    {
      "name": "Transformer",
      "score": 0.7213789224624634
    },
    {
      "name": "Machine translation",
      "score": 0.6560224294662476
    },
    {
      "name": "Translation (biology)",
      "score": 0.4649580419063568
    },
    {
      "name": "Natural language processing",
      "score": 0.462402880191803
    },
    {
      "name": "Artificial intelligence",
      "score": 0.433554470539093
    },
    {
      "name": "Voltage",
      "score": 0.11950615048408508
    },
    {
      "name": "Electrical engineering",
      "score": 0.11353358626365662
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210154851",
      "name": "Hubei University of Education",
      "country": "CN"
    }
  ],
  "cited_by": 5
}