{
  "title": "Qwen-2.5 Outperforms Other Large Language Models in the Chinese National Nursing Licensing Examination: Retrospective Cross-Sectional Comparative Study",
  "url": "https://openalex.org/W4405623292",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2124504507",
      "name": "Shiben Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2594696782",
      "name": "Wanqin Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102635910",
      "name": "Zhi Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2323964380",
      "name": "Jiani Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2002948643",
      "name": "Fang Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4230071417",
    "https://openalex.org/W4391811872",
    "https://openalex.org/W4400446121",
    "https://openalex.org/W4376116452",
    "https://openalex.org/W4384700522",
    "https://openalex.org/W4386624281",
    "https://openalex.org/W4367692415",
    "https://openalex.org/W4385620111",
    "https://openalex.org/W4387602412",
    "https://openalex.org/W4365145607",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W4393335480",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4397042146",
    "https://openalex.org/W4392632345",
    "https://openalex.org/W4386002582",
    "https://openalex.org/W4385379701",
    "https://openalex.org/W4391650958",
    "https://openalex.org/W4392632478",
    "https://openalex.org/W3187352571",
    "https://openalex.org/W4400955423",
    "https://openalex.org/W4377941003",
    "https://openalex.org/W4396673785",
    "https://openalex.org/W4389917881",
    "https://openalex.org/W4392504747",
    "https://openalex.org/W2901964459",
    "https://openalex.org/W3011484826",
    "https://openalex.org/W2768149277",
    "https://openalex.org/W2301541953",
    "https://openalex.org/W2169618946",
    "https://openalex.org/W3183637089",
    "https://openalex.org/W2553884582",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W3094948551",
    "https://openalex.org/W4392164233",
    "https://openalex.org/W4387232979",
    "https://openalex.org/W4392196456",
    "https://openalex.org/W1625728552"
  ],
  "abstract": "Background Large language models (LLMs) have been proposed as valuable tools in medical education and practice. The Chinese National Nursing Licensing Examination (CNNLE) presents unique challenges for LLMs due to its requirement for both deep domainâ€“specific nursing knowledge and the ability to make complex clinical decisions, which differentiates it from more general medical examinations. However, their potential application in the CNNLE remains unexplored. Objective This study aims to evaluates the accuracy of 7 LLMs including GPT-3.5, GPT-4.0, GPT-4o, Copilot, ERNIE Bot-3.5, SPARK, and Qwen-2.5 on the CNNLE, focusing on their ability to handle domain-specific nursing knowledge and clinical decision-making. We also explore whether combining their outputs using machine learning techniques can improve their overall accuracy. Methods This retrospective cross-sectional study analyzed all 1200 multiple-choice questions from the CNNLE conducted between 2019 and 2023. Seven LLMs were evaluated on these multiple-choice questions, and 9 machine learning models, including Logistic Regression, Support Vector Machine, Multilayer Perceptron, k-nearest neighbors, Random Forest, LightGBM, AdaBoost, XGBoost, and CatBoost, were used to optimize overall performance through ensemble techniques. Results Qwen-2.5 achieved the highest overall accuracy of 88.9%, followed by GPT-4o (80.7%), ERNIE Bot-3.5 (78.1%), GPT-4.0 (70.3%), SPARK (65.0%), and GPT-3.5 (49.5%). Qwen-2.5 demonstrated superior accuracy in the Practical Skills section compared with the Professional Practice section across most years. It also performed well in brief clinical case summaries and questions involving shared clinical scenarios. When the outputs of the 7 LLMs were combined using 9 machine learning models, XGBoost yielded the best performance, increasing accuracy to 90.8%. XGBoost also achieved an area under the curve of 0.961, sensitivity of 0.905, specificity of 0.978, F1-score of 0.901, positive predictive value of 0.901, and negative predictive value of 0.977. Conclusions This study is the first to evaluate the performance of 7 LLMs on the CNNLE and that the integration of models via machine learning significantly boosted accuracy, reaching 90.8%. These findings demonstrate the transformative potential of LLMs in revolutionizing health care education and call for further research to refine their capabilities and expand their impact on examination preparation and professional training.",
  "full_text": null,
  "topic": "Preprint",
  "concepts": [
    {
      "name": "Preprint",
      "score": 0.7422004342079163
    },
    {
      "name": "Cross-sectional study",
      "score": 0.6706393957138062
    },
    {
      "name": "Medicine",
      "score": 0.38095077872276306
    },
    {
      "name": "Nursing",
      "score": 0.3671623468399048
    },
    {
      "name": "Computer science",
      "score": 0.3603599965572357
    },
    {
      "name": "World Wide Web",
      "score": 0.24130219221115112
    },
    {
      "name": "Pathology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210098034",
      "name": "Key Laboratory of Guangdong Province",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210103346",
      "name": "Nanfang Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I58200834",
      "name": "Southern Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I168719708",
      "name": "City University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4210152664",
      "name": "Shenzhen Children's Hospital",
      "country": "CN"
    }
  ]
}