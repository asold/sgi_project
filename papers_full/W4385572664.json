{
    "title": "TranSFormer: Slow-Fast Transformer for Machine Translation",
    "url": "https://openalex.org/W4385572664",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5107028150",
            "name": "Bei Li",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A5101601049",
            "name": "Jing Yi",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A5101522530",
            "name": "Xu Tan",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5113598536",
            "name": "Zhen Xing",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5100600701",
            "name": "Tong Xiao",
            "affiliations": [
                null,
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A5100370155",
            "name": "Jingbo Zhu",
            "affiliations": [
                null,
                "Northeastern University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W2756888147",
        "https://openalex.org/W2531207078",
        "https://openalex.org/W4283319304",
        "https://openalex.org/W3164045210",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W2799051177",
        "https://openalex.org/W3036939249",
        "https://openalex.org/W3097777922",
        "https://openalex.org/W3101668578",
        "https://openalex.org/W2963807318",
        "https://openalex.org/W2989571009",
        "https://openalex.org/W2964302946",
        "https://openalex.org/W2963434219",
        "https://openalex.org/W4223967157",
        "https://openalex.org/W2899423466",
        "https://openalex.org/W3034857244",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W2889769646",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3039805635",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2767989436",
        "https://openalex.org/W2997945091",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2963887123",
        "https://openalex.org/W3173571438",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W3034407863",
        "https://openalex.org/W3046835050",
        "https://openalex.org/W2970682957",
        "https://openalex.org/W2794365787",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2990503944"
    ],
    "abstract": "Learning multiscale Transformer models has been evidenced as a viable approach to augmenting machine translation systems. Prior research has primarily focused on treating subwords as basic units in developing such systems. However, the incorporation of fine-grained character-level features into multiscale Transformer has not yet been explored. In this work, we present a Slow-Fast two-stream learning model, referred to as TranSFormer, which utilizes a “slow” branch to deal with subword sequences and a “fast” branch to deal with longer character sequences. This model is efficient since the fast branch is very lightweight by reducing the model width, and yet provides useful fine-grained features for the slow branch. Our TranSFormer shows consistent BLEU improvements (larger than 1 BLEU point) on several machine translation benchmarks.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 6883–6896\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nTranSFormer: Slow-Fast Transformer for Machine Translation\nBei Li1∗, Yi Jing1, Xu Tan2, Zhen Xing3, Tong Xiao1,4†and Jingbo Zhu1,4\n1School of Computer Science and Engineering, Northeastern University, Shenyang, China\n2Microsoft Research Asia, 3Fudan University\n4NiuTrans Research, Shenyang, China\n{libei_neu,jingyi_neu}@outlook.com, xuta@microsoft.com\nzxing20@fudan.edu.cn, {xiaotong,zhujingbo}@mail.neu.edu.cn\nAbstract\nLearning multiscale Transformer models has\nbeen evidenced as a viable approach to aug-\nmenting machine translation systems. Prior\nresearch has primarily focused on treating sub-\nwords as basic units in developing such sys-\ntems. However, the incorporation of fine-\ngrained character-level features into multiscale\nTransformer has not yet been explored. In this\nwork, we present a Slow-Fast two-stream learn-\ning model, referred to as TranSFormer, which\nutilizes a “slow” branch to deal with subword\nsequences and a “fast” branch to deal with\nlonger character sequences. This model is effi-\ncient since the fast branch is very lightweight\nby reducing the model width, and yet provides\nuseful fine-grained features for the slow branch.\nOur TranSFormer shows consistent BLEU im-\nprovements (larger than 1 BLEU point) on sev-\neral machine translation benchmarks.\n1 Introduction\nTransformer (Vaswani et al., 2017) has demon-\nstrated strong performance across a range of nat-\nural language processing (NLP) tasks. Recently,\nlearning multiscale Transformer models has been\nevidenced as a promising approach to improving\nstandard Transformer. Previous research on this\nline can be broadly categorized into two streams:\none learns local fine-grained features by using a\nfixed-length window (Yang et al., 2019; Hao et al.,\n2019; Guo et al., 2020), linguistic-inspired local\npatterns (Li et al., 2022b), and a hybrid approach\nthat combines convolution and self-attention mod-\nels (Gulati et al., 2020) or run in parallel (Zhao\net al., 2019); the other learns sequence representa-\ntions by considering multiple subword segmenta-\ntion/merging schemas (Wu et al., 2020).\nDespite the attractiveness of these approaches,\nprevious work is based on an assumption that sub-\n∗The work was done when the first author was an intern\nat Microsoft Research Asia.\n†Corresponding author.\nCharacterSubword\n...\n...\n...\n...\nFusion\nDecoder\nHf\nLf\nHs\nLs\nFast Encoder\nSlow Encoder\nFigure 1: Overview of the proposed TranSFormer, an\nencoder-decoder paradigm where the encoder involves\ntwo separated branches. One is the Slow-branch with\nsubword-level input features and the other is the Fast-\nBranch with character-level features. Hf is far smaller\nthan Hs for computation efficiency.\nwords are the basic units in sequence modeling,\nand therefore ignores smaller, more fine-grained\ncharacter-level features. In fact, the benefits of\nusing characters have long been appreciated, and\ncharacter-based models have been discussed in sev-\neral sub-fields of NLP, such as language modeling\n(Xue et al., 2022) and machine translation (Lee\net al., 2017; Li et al., 2021; Gao et al., 2020). But\nthere are still important problems one needs to ad-\ndress in multi-scale Transformer. The first of these\nis the computational challenge of dealing with long\nsequences. For example, when we represent an En-\nglish text as a character sequence, the length of this\nsequence is in general 5×longer than that of the\nsubword sequence. We therefore need to consider\nthis length difference in model design. The second\nproblem is that, from a multiscale learning perspec-\ntive, learning text representations with features at\ndifferent levels is not just making use of the syn-\ntactic hierarchy of language. To better model the\nproblem, we need some mechanism to describe the\ninteractions among these different linguistic units.\nIn this study, we aim to exploit the potential\nof character-level representations in multiscale se-\nquence models while maintaining computational\n6883\nefficiency. Drawing inspiration from the SlowFast\nconvolutional models in video classification (Fe-\nichtenhofer et al., 2019), we propose the Slow-\nFast Transformer (TranSFormer) model, which\nutilizes a fast, thin branch to learn fine-grained\ncharacter-level features and a slow, wide branch\nto capture correlations among subword features.\nA cross-granularity attention layer is placed be-\ntween the self-attention and feedforward sublayers\nto make exchanges of cross-granularity informa-\ntion. This enables the slow branch to be aware\nof fine-grained features while providing optimized\nhigh-level representations of the input sequence to\nthe fast branch.\nWe also make use of character-to-word boundary\ninformation to model the interactions among neigh-\nboring characters in a word. Additionally, we de-\nvelop a boundary-wise positional encoding method\nto better encode the positional information within\nwords for the fast branch. Through a series of exten-\nsive experiments on the WMT’14 English-German,\nWMT’17 Chinese-English and WMT’16 English-\nRomanian tasks, we demonstrate that TranSFormer\nyields consistent performance gains while having\na negligible increase in the number of parameters\nand computational cost. As a bonus, our TranS-\nFormer is robust to errors caused by suboptimal\ntokenization or subword segmentation.\n2 Related Work\nMultiscale Transformer Learning multiscale\nTransformer is a promising way to acquire for fur-\nther improvements in the machine translation task.\nA feasible way is to model global and local patterns\nto enhance Transformer models (Shaw et al., 2018;\nYang et al., 2018, 2019; Zhao et al., 2019). These\nwork mainly modeled the localness within a fixed\nwindow size upon subword input features. Apart\nfrom these, Wu et al. (2018) partitioned the input se-\nquence according to phrase-level prior knowledge,\nand build attention mechanism upon phrases. Simi-\nlarly, Hao et al. (2019) proposed a multi-granularity\nself-attention mechanism, designed to allocate dif-\nferent attention heads to phrases of varying hierar-\nchical structures. Perhaps the most related work to\nours is UMST (Li et al., 2022b). They re-defined\nthe sub-word, word and phrase scales specific to\nsequence generation, and modeling the correla-\ntions among scales. However, more fine-grained\ncharacter-level scale is not explored in the previous\nwork due to the serve challenge for encoding long\ncharacter sequences.\nCharacter-level NMT Fully character-level neu-\nral machine translation originates from recurrent\nmachine translation system in Lee et al. (2017).\nThey built a fully character-level encoder-decoder\nmodel, and utilize convolution layers to integrate\ninformation among nearby characters. Cherry et al.\n(2018) show the potential of character-level models\nwhich can outperform subword-level models under\nfully optimization. This contributes to their greater\nflexibility in processing and segmenting the input\nand output sequences, though modeling such long\nsequences is time-consuming. More recently, sev-\neral studies analyze the benefits of character-level\nsystems in multilingual translation scenarios (Gao\net al., 2020), low-resource translation and translat-\ning to typologically diverse languages (Li et al.,\n2021). But these methods all simply view char-\nacters as basic units in language hierarchy, and it\nis still rare to see the effective use of multi-scale\nlearning on character-based language features.\nMulti-Branch Transformer The utilization of\nmulti-branch architectures has been extensively\nstudied in Transformer models. Early efforts\nin this area include the Weighted Transformer\n(Ahmed et al., 2017), which replaced the vanilla\nself-attention by multiple self-attention branches.\nSubsequently, the Multi-attentive Transformer (Fan\net al., 2020) and Multi-Unit Transformer (Yan et al.,\n2020) have advanced this design schema by incor-\nporating branch-dropout and switching noise in-\nputs, respectively. Additionally, Wu et al. (2020)\ninvestigated the potential advantages of utilizing\ndual cross-attention mechanisms to simultaneously\nattend to both Sentencepiece (Kudo and Richard-\nson, 2018) and subword (Sennrich et al., 2016). In\nthis work, we take a forward step to exploit the\npotential of character features. We argue that a\nlightweight branch is sufficient to encode useful\nfine-grained features, an aspect that has not been\npreviously investigated.\n3 Method\nThe proposed TranSFormer follows a encoder-\ndecoder paradigm (see Figure 1) which involves\ntwo encoder branches operating at different input\ngranularities. The original subword encoder, which\nhas a large model capacity for fully learning corre-\nlations among input individuals, is defined as the\nslow branch. The other branch, designed to handle\n6884\nCharacter: A _ s w i s s _ b i c y c l e Subword: A swi@@ ss bicy@@ cle\nA_\nswiss_\nbicycle\nA_ s wi s s _ b i c y c l e\n(a) Af\n(b) As\n0 5 10 150\n0.2\n0.4\n0.6\n0.8\n1\nNumber of subwords or characters\nPercentage\ncharactersubword\n(c) Length Distributions\nFigure 2: An example of the character-to-word boundaries (a), subword-to-word boundaries (b), along with the\ndistribution of character/subword counts within a single word for the source language in the WMT En-De task (c).\nNote that characters/subwords in same color belong to the same word, and “_” denotes the word boundary among\ncharacters. Similar trends would be observed in Romanian and Mandarin.\ncharacter-level representations using a thin encoder\nto efficiently capture correlations among charac-\nters, is referred to as the fast branch. Our goal is\nto use the fast branch to learn a fine-grained but\nless precise representation to complement the slow\nbranch. In the following sections, we will elaborate\nthe core design of Slow branch, Fast branch and\nthe cross-granularity attention, respectively.\n3.1 The Slow Branch for Subwords\nWe use the standard Transformer as the slow branch\ndue to its strong ability to model global interac-\ntions among input sequences. The input of the\nslow branch is the mixture of subwords and words\nsince some high-frequency words have not been\nfurther divided into subwords. Following the sug-\ngestions in Li et al. (2022b), we adopt a graph\nconvolutional network to model the inner corre-\nlations among words through the adjacency ma-\ntrix As. To this end, the Slow branch then en-\ncodes the enhanced representation via the self-\nattention mechanism, SAN = Softmax(Q·KT\n√dk\n)·V,\nwhere Q, K, V are obtained through three inde-\npendent projection matrix, such as Wq, Wk, Wv.\nA point-wise feed-forward network is followed,\nFFN =max(xW1 + b1,0)W2 + b2, where W1 and\nW2 are transformation matrices and b1 and b2 are\nbias matrices. To bridge the gap between two gran-\nularities, we sandwich a new sublayer between the\nself-attention and the feed-forward network, to ac-\ncomplish the feature interaction between the slow\nand fast branches. A straightforward idea is to em-\nploy a cross-attention similar with encoder-decoder\nattention in the decoder side. We will discuss more\ndetails in the Section 3.3.\n3.2 The Fast Branch for Characters\nTo enhance the efficiency of modeling long\ncharacter-level inputs, we propose the use of a fast\nbranch with a tiny hidden size. The hidden size\nis a critical factor in the computation of the self-\nattention network (Vaswani et al., 2017), and by\nreducing it, we can achieve faster computation. To\nthe best of our knowledge, this is the first attempt\nto design multiscale Transformer models that con-\nsiders character-level features, as the long input\nsequence has previously hindered such exploration.\nWhile the fast branch may not be as powerful\nas the slow branch, it is still effective in learning\nfine-grained features. Our initial experiments have\nyielded two notable findings: 1) a slow branch\nwith hidden size of 32 is sufficient for transferring\nfine-grained knowledge to the slow branch, and\n2) cross-granularity fusion is crucial for the slow\nbranch, while removing the reversed fusion in the\nfast branch has only a moderate effect on perfor-\nmance. We would ablate this settings in the Section\n4.2. To further improve the modeling ability, we\nintroduce several techniques as follows:\nChar Boundary Information The use of word-\nboundary information has been shown to effec-\ntively reduce the redundant correlations among sub-\nwords, as demonstrated in (Li et al., 2022b). This\nleads to the consideration of character-level mod-\neling, which poses a more challenging problem\n6885\nSlow branch\nA swi@@ ss bicy@@ cle\nS-GCN\n+\nSAN\n+\nS-CGA\n+\nFFN\n+\nFast Branch\nA _ s w i s s _ b i c y c l e\nF-GCN\n+\nSAN\n+\nF-CGA\n+\nFFN\n+\nAsAfLinearLinear Linear\nMatMul\nsoftmax\nMatMul\nxLf ×Hf\nf ˆxLs×Hf\nf ˆxLs×Hf\nf\nLf ×Lf\nLf ×Hf\nLN LN\nxLf ×Hf\nf xLs×Hs\ns\nFigure 3: The encoder architecture of the proposed TranSFormer, a two-branch network with fast branch (left)\nand slow branch (right). Af and As are adjacency matrices of fast and slow branches. Here, we omit the layer\nnormalization for simplification. Actually, we follow the pre-normalization strategy as its stability.\ndue to the greater number of characters typically\npresent within a word in comparison to subwords.\nThe statistical analysis in Figure 2c further evi-\ndences it that a significant proportion of words con-\ntain more than 5 characters, while a much smaller\nnumber are divided into subwords. Thus, model\nmay be unable to discern the distinction between\nthe same character that belongs to the same word\nand that of distinct words.\nTo address this issue, we propose the use of a\ncharacter-level graph convolution network (GCN)\nto learn local, fine-grained features while also al-\nlowing each character to be aware of its proximity\nto other characters. GCN(Kipf and Welling, 2017)\nis a suitable choice for this task as it aggregates fea-\nture information from the neighbors of each node to\nencapsulate the hidden representation of that node.\nThe computation can be described as:\nGCNFast = σ( ˜D\n−1\n2\nf ˜Af ˜D\n−1\n2\nf ·xWg\nf ), (1)\n˜Af = Af + IL denotes the adjacency matrix of\nthe undirected graph with self-connections. Here\nIL denotes the identity matrix. ˜Df is the degree\nmatrix of the adjacency matrix ˜Af . Wg\nf is a linear\ntransformation which is a trainable parameter. The\ncharacter-level encoder architecture is illustrated in\nFigure 3.\nBoundary-wised Positional Encoding To fur-\nther enhance the relative positional representation\namong characters, we design a boundary-wised po-\nsitional encoding (PE) method. Our intuition is\nto provide each character with the ability to rec-\nognize which characters belong to the same word.\nThus we restrict the relative window within each\nword, as illustrated in Figure 4. Vanilla relative\npositional encoding (Shaw et al., 2018) models the\ncorrelations in a fixed window size 2k+ 1. Here\nwe set k= 3, positions exceed kwould be masked.\nDifferently, the proposed boundary-wised PE is\nutilized to enhance the inner relative positional in-\nformation among characters within each word. In\nour preliminary experiments, boundary-wised PE\nis helpful for stable training.\n3.3 Cross-Granularity Fusion\nAs depicted in Figure 3, the computation of the\ntwo branches in our TranSFormer architecture is\nseparated, with each branch operating indepen-\ndently of the other’s representation. To facilitate\ncommunication between the branches, we propose\nthe utilization of a cross-granularity information\nfusion method within each encoder block. This\nmethod can be implemented through various op-\ntions. Given the lengths of the slow and fast\nbranches as Ls and Lf , and the hidden sizes as\nHs and Hf , respectively, the goal is to seamlessly\n6886\ngood_news good_news\n(a) Relative positional rep-\nresentations\ngood_news good_news\n(b) Boundary-wised posi-\ntional encoding\nFigure 4: Comparison of the relative positional encod-\ning (a) and our proposed boundary-wised positional\nencoding method (b). Note that characters in a dark\ncolor means the mask for exceeding the max window.\nintegrate cross-scale information within each en-\ncoder block between the two branches. In the field\nof machine translation, it is straightforward to em-\nploy cross-attention mechanisms, such as encoder-\ndecoder attention (Vaswani et al., 2017) or context-\naware cross-attention (V oita et al., 2018), to capture\ncorrelations between the representations.\nOur default strategy is to employ a cross-\ngranularity attention mechanism (namely CGA)\nsandwiched between the self-attention and feed-\nforward network. The architecture is plotted in\nFigure 3. xf and xs denote the representation\nof the fast and slow branches, respectively. The\nchallenge remains here is the mismatched feature\nshape between xs and xf Here, we take the Fast\nbranch as an instance, we first normalize xs via\nˆxf = LN(xs). LN(·) denotes the layer normaliza-\ntion for stable optimization. Then ˆxf is fed into\nCGA of the fast branch, the formulation is as fol-\nlows:\nATTNf = Softmax(\nxf Wq\nf ·(ˆxf Wk\nf )T\n√\ndk\nf\n),\nCGA = ATTNf ·ˆxf Wv\nf , (2)\nwhere the query is derived from the residual output\nof SAN in the Fast branch viaxs ·Wq\nf . The key and\nvalue are derived from the Slow branch via ˆxf Wk\nf\nand ˆxf Wv\nf , respectively. It is worthy to note that,\nthe shape of Wk\nf and Wv\nf ∈RHs×Hf , to reduce\nthe hidden size. Detailed transformation could be\nfound in the left part of Figure 3.\nIt is important to note that our proposed method\nof cross-granularity fusion is bidirectional, as op-\nposed to the lateral connections used in the Slow-\nFast (Feichtenhofer et al., 2019). Other alternative\nmethods would be discussed in Section 4.3.\n3.4 Interactions Between Encoder and\nDecoder\nIn vanilla Transformer, the key and value of the\nencoder-decoder attention on the decoder side de-\nrives from the encoder output, however, there are\ntwo branches in our TranSFormer (See Figure 1). It\nis worthy to investigate how to effectively leverage\nthe multi-granularity representations. Our default\nstrategy is to regard the fast branch as an auxiliary\nto provide fine-grained features for the slow branch,\nthus only the output of the slow branch is exposed\nto the decoder. Besides this, there are also several\nfeasible options. For example, we can fuse the\noutputs of two branches as the final encoder out-\nput, or building a double-branch encoder-decoder\nattention to attend two branches independently. We\ncompares this options in our experiments.\n4 Experiments\n4.1 Experimental Setups\nDatasets The present study examines the perfor-\nmance of our proposed TranSFormer on several\nmachine translation datasets: the WMT’14 English-\nGerman (En-De), WMT’16 English-Romanian\n(En-Ro) and WMT’17 Chinese-English (Zh-En)\ndatasets. The En-De dataset comprises approxi-\nmately 4.5 million tokenized sentence pairs, which\nwere preprocessed following the same procedure as\nin Ott et al. (2018) to yield a high-quality bilingual\ntraining dataset. For validation, we use the new-\nstest2016 set, while the newstest2014 set served as\nthe test data. The En-Ro dataset consists of 610K\nbilingual sentence pairs, and we adopt the same\npreprocessing scripts as in Lee et al. (2018); Kasai\net al. (2020), using a joint source and target BPE\nfactorization with a vocabulary size of 40K. The\nnewsdev2016 set is used for validation, while the\nnewstest2016 set served as the test set. For the Zh-\nEn task, we collect all the available parallel data for\nthe WMT17 Chinese-English translation task, con-\nsisting 15.8M sentence pairs from the UN Parallel\nCorpus, 9M sentence pairs from the CWMT Cor-\npus and about 332K sentence pairs from the News\nCommentary corpus. After carefully data filtering\nsetups in Hassan et al. (2018), there are left 18M\nbilingual pairs. newsdev2017 and newstest2017 are\nserved as the validation and test sets, respectively.\nSetups For the machine translation task, we\nmainly evaluate the proposed TranSFormer on base\nand big configurations. The hidden size of slow\n6887\nModel Enc. Dec. Base Big\nParam BLEU Param BLEU\nTransformer (Vaswani et al., 2017) Sub Sub 65M 27.30 213M 28.40\nTransformer Char Sub 63M 26.56 208M 28.05\nMultiscale\nRPR (Shaw et al., 2018) Sub Sub 65M 27.60 213M 29.20\nCSAN (Yang et al., 2019) Sub Sub 88M 28.18 - 28.74\nLocalness (Yang et al., 2018) Sub Sub 89M 28.11 267M 29.18\nMG-SA (Hao et al., 2019) Sub Sub 89M 28.28 271M 29.01\nUMST (Li et al., 2022b) Sub Sub 70M 28.51 242M 29.75\nDouble-Branch\nMuse (Zhao et al., 2019) Sub Sub - - 233M 29.90\nMulti-Attentive (Fan et al., 2020) Sub Sub - - 325M 29.80\nMulti-Unit (Yan et al., 2020) Sub Sub 130M 29.30 - -\nCharacter-level\nConvTransformer (Gao et al., 2020) † Char Char 65M 23.47 - -\nFast Only (Hidden=32, L=6) Char Sub 42M 17.90 (16.9) - -\nFast Only (Hidden=512, L=6) Char Sub 64M 27.11 (26.1) 211M 28.65 (27.6)\nSlow-Fast\nSlow Only Sub Sub 63M 27.40 (26.4) 211M 28.80 (27.8)\nTranSFormer (Hidden=32, L=6) Char/Sub Sub 66M 28.56 (27.6) 231M 29.85 (28.9\nTranSFormer + ODE (Li et al., 2022a) Char/Sub Sub 66M 29.30 (28.3) - -\nTable 1: Comparison with previous studies on the WMT En-De task. Models with †denote the re-implementing\nresults based on our codebase within the same hyperparameters. BLEU at the right corner denotes the SacreBLEU.\n(a) Previous work based on Big models\nSystem Params BLEU\nTransformer-Big(Hassan et al., 2018) - 24.20\nCSAN (Yang et al., 2019) - 25.01\nLocalness (Yang et al., 2018) 307M 25.03\nUMST (Li et al., 2022b) 307M 25.23\n(b) Our Big models\nsubword-level Transformer-Big 261M 24.41\ncharacter-level Transformer-Big 258M 23.80\nTranSFormer (Hidden=64) 283M 25.55\nTable 2: Results on WMT Zh-En. We compare several\nprior work of learning local patterns.\nbranch is 512/1024 for base and big, respectively.\nAnd the filter size in FFN is 2048/4096. In our de-\nfault setting, a width of32 slow branch is enough to\nlearn fine-grained features, and the corresponding\nfilter size is set to 128. We both employ residual\ndropout, attention dropout and activation dropout.\nAll values are 0.1, except the residual dropout 0.3\nfor big counterparts.\nTraining and Evaluations The codebase is de-\nveloped upon Fairseq (Ott et al., 2019). All experi-\nments are conducted on 8 Tesla V100 GPUs. We\nuse Adam (Kingma and Ba, 2015) optimizer with\n(0.9, 0.997), and the default learning rate schedule\nwith 0.002 max value, 16,000 warmup steps. For\nmachine translation tasks, BLEU scores are com-\nputed by mult-bleu.perl, and we also provide the\nSacreBLEU1 for En-De. The beam size is 4 for\nEn-De and 8 for Zh-en, and the length penalty is\n0.6 and 1.3, respectively.\n1BLEU+case.mixed+numrefs.1+smooth.exp+\ntok.13a+version.1.2.12\nModel Param BLEU\nDELIGHT (Mehta et al., 2020) 53M 34.70\nBaseline in MBART (Liu et al., 2020) - 34.30\nBaseline in DISCO (Kasai et al., 2020) - 34.16\nTransformer † (Vaswani et al., 2017) 54M 34.21\nTNT † (Han et al., 2021) 73M 34.00\nUMST (Li et al., 2022b) 60M 34.81\nODE Transformer (Li et al., 2022a) 69M 34.94\nTranSFormer (Hidden=32) 59M 35.40\nTable 3: Results on the WMT En-Ro task.\nResults of En-De The results of the WMT En-\nDe task under both base and big configurations\nare summarized in Table 1. As evidenced by the\nresults, our TranSFormer model demonstrates sig-\nnificant improvements in BLEU when compared to\nthe Slow only model, with gains of up to 1.16/1.05\nBLEU scores under the base/big configurations.\nConversely, the Fast only baseline, which has a\nhidden size of 32, only attains a BLEU score of\n17.90, leading to a considerable performance gap\ndue to its limited capacity. However, it still con-\ntributes up to a 1.14 BLEU-point benefit to the\nSlow branch, indicating that the fine-grained corre-\nlations modeled by the Fast branch are complemen-\ntary. Additionally, we present the results of prior\nworks, which have employed both character-level\nand subword-level systems, and categorize them in\nterms of various aspects. TranSFormer can beat or\non par with prior works with less parameters. This\nindicates the investigation of character-level mutlis-\ncale models is meaningful. Note that TranSFormer\nis computationally efficient, only requiring addi-\ntional 15% training cost and negligible inference\nlatency. And TranSFormer can also benefit from\n6888\nModel Width En-De Param. Zh-En Param.\nTransformer - 27.20 63.1M 24.41 261.9M\nTranSFormer 16 28.10 66.6M 25.20 281.0M\nTranSFormer 32 28.56 66.9M 25.47 281.7M\nTranSFormer 64 28.39 67.6M 25.55 283.1M\nTranSFormer 128 28.27 69.7M 25.51 286.6M\nTranSFormer 256 28.17 76.1M 25.33 295.7M\n(a) Char-width: Comparisons of various char width on\nperformance.\nModel Fusion Methods BLEU\nSlow only - 27.20\nFast only - 17.90\nTranSFormer CGA 28.56\nTranSFormer Linear + Attention 28.47\nTranSFormer DS + Concat 27.82\nTranSFormer DS + Sum 27.96\n(b) Fusion Method : Fusing Slow and Fast\nbranches with several types of methods.\nModel Enc. Output BLEU\nTranSFormer Slow Branch 28.56\nTranSFormer Fast Branch 23.11\nTranSFormer Both Slow and Fast 28.25\n(c) Interactions: Figuring out the impact of vari-\nous encoder-decoder interaction manners on perfor-\nmance.\nModel Input Granularity BLEUSlow-Branch Fast-Branch\nTranSFormer Subword Character 28.56\nTranSFormer Subword Subword 27.50\nTranSFormer Subword Sentencepiece 28.27\nTranSFormer Sentencepiece Character 28.60\n(d) Input: Figuring out the impact of various input granularites\nfor Slow and Fast branch.\nTable 4: Ablations on TranSFormer design on the WMT En-De task. The evaluation metric is BLEU (%). We\nmainly ablate the experiments from the width of fast branch, various fusion methods, the interactions between\nencoder-decoder, and the input granularity.\nadvanced design, e.g., another 0.74 improvement\nwith ODE method (Li et al., 2022a).\n4.2 Results\nResults of Zh-En The WMT’17 Zh-En task\nposes a significant challenge due to the linguis-\ntic differences between Chinese and English. Ad-\nditionally, the Chinese language owns less char-\nacters per word than English. Table 2 shows the\nresults of our comparison of the TranSFormer with\nprior works. We observe TranSFormer yields a 1\nBLEU point improvements than the subword-level\nsystems. Our TranSFormer model demonstrates\nsuperior performance compared to previous work\nthat models local patterns, while maintaining effi-\ncient computational requirements. We will exploit\nwhether TranSFormer can gain more benefits when\nincorporating these techniques on the slow branch.\nResults of En-Ro Furthermore, our empirical\nevaluations of the proposed TranSFormer archi-\ntecture on the smaller WMT En-Ro dataset also\ndemonstrate consistent improvements in BLEU\nscores as a result of the utilization of interactions\namong granularities. Notably, the TranSFormer\nmodel even outperforms the ODE Transformer (Li\net al., 2022a), an advanced variant that leverages\nthe advantages of high-order ordinary differential\nequations (ODE) solutions, by a substantial margin\nwhile incurring less computational cost.\n4.3 Analysis\nThis section provides ablation studies of TranS-\nFormer in terms of several core techniques.\nEffect of width on Fast branch We first aim\nto explore TranSFormer under various widths of\nthe Fast branch, including 16, 32, 64, 128 and 256.\nResults in Table 4a show that even a hidden size of\n16 can provide helpful fine-grained features for the\nslow branch, and yielding almost 1 BLEU-point\ngains by bringing modest parameters. Empirically,\na hidden of 32 and 64 deliver the best performance\non base (En-De) and big (Zh-En) configurations,\nrespectively. Further increasing the hidden layer\ndimension of the model results in no more gains,\nwhile requiring more computational cost.\nFusion methods between branches In addition\nto our proposed fusion method CGA, there are sev-\neral alternative techniques that can be considered.\nThe most straightfoward one is to transform the hid-\nden with a linear projection and then use a standard\ncross-attention. It delivers similar performance\nbut consumes more parameters. Another option is\nto downsample the character-level representation\nfrom Lf to Ls, and then concatenate (namely DS\n+ Concat) or sum (DS + Sum) the two representa-\ntions. Although both of these methods have been\nfound to outperform the Slow only baseline, they\nhave not been found to be on par with CGA method.\nThis may be due to the fact that downsampling may\nimpede optimization due to the low compression\nratio of text compared with images.\nVarious interaction methods In Table 4c, we\npresent a summary of various promising options\nfor interactions between the encoder and decoder.\nEmpirical results indicate that utilizing the Slow\n6889\n# Model BLEU\n1 TranSFormer 28.56\n2 w/ unidirectional CGA 28.41\n3 w/o character-boundary 27.61\n4 w/ replace boundary with random 26.79\n5 w/o boundary-wised PE 28.13\n6 w/ linear attention (Wang et al., 2020) 28.08\nTable 5: Ablations on the fast branch in terms of several\ncore design schemas.\n# Model BLEU\n1 TranSFormer (default: all blocks) 28.56\n2 + at the last encoder block (e.g., 6) 27.99\n3 + at bottom 3 blocks (e.g., 1/2/3) 28.10\n4 + at top 3 blocks (e.g., 4/5/6) 28.40\n5 + every 2 blocks (e.g., 1/3/5) 28.20\nTable 6: Ablations on operating interactions between\ntwo branches in different levels.\nbranch as the output yields the highest performance.\nThis can be attributed to the Fast branch’s ability\nto provide fine-grained features and low-level se-\nmantic knowledge as auxiliary information to the\nSlow branch. Additionally, while utilizing the Fast\nbranch as the encoder output results in inferior per-\nformance compared to the baseline, it still yields a\nsignificant improvement over the Slow only base-\nline (17.90). This highlights the effectiveness of\nthe TranSFormer model in leveraging interactions\nbetween different granularities. Furthermore, we\nalso evaluated a two-stream approach in the de-\ncoder, in which one stream attends to the Slow\nbranch and the other attends to the Fast branch,\nwith a gated mechanism being used to fuse the fea-\ntures. However, this method was not sufficient to\nfurther improve performance. We attribute this to\nthe negative interactions brought by the Fast branch,\nincreasing the optimization difficulty.\nEffect of various input granularities To ascer-\ntain whether the observed performance gains can\nbe attributed to the complementary information\nprovided by fine-grained character-level represen-\ntations, we replaced the input of the fast branch\nwith subword-level sequences, identical to that of\nthe slow branch. The results presented in Table 4d\ndemonstrate a degradation of up to 1 BLEU point.\nThis can be attributed to the lack of distinct or com-\nplementary features provided by the fast branch\nand the limited capacity of the model in fully op-\ntimizing subword-level features. This observation\nfurther supports the hypothesis that the Slow-Fast\ndesign can learn complementary features for each\ngranularity. Furthermore, we found that the TranS-\n# Model 50K 500K 1000K\n1 TranSFormer 11.87 22.75 25.30\n2 Character-only 10.50 20.50 22.50\n3 Subword-only 7.00 22.00 23.50\nTable 7: Comparison of different low-resource settings,\nincluding 50K, 500K, and 1000K training subsets sam-\npled from the WMT En-De dataset.\n6 12 18 24\n26.5\n27.5\n28.5\n29.5\nLayer Index\nBLEU Baseline\nTranSFormer\n10 30 50\n26.0\n27.0\n28.0\n29.0\nBPE operations (K)\nBLEU Baseline\nTranSFormer\nFigure 5: BLEU against differnt BPE merging opera-\ntions and model depths.\nFormer architecture with sentencepiece (Kudo and\nRichardson, 2018) as the fast branch input can\nalso benefit from the two-branch design, due to the\ndifferent segmentations. Additionally, our TranS-\nFormer is a general design that can work well with\na character-level fast branch and a sentencepiece-\nlevel slow branch, yielding a BLEU score of 28.60,\neven slightly better than the subword-level one.\nAblations on fast branch designs It is hard to\ndirectly learn the tedious character sequence. The\nproposed character-boundary injection serves as\na crucial component in addressing this challenge.\nWithout this injection, the TranSFormer model suf-\nfers from a significant decrease in BLEU (#3). Fur-\nthermore, the situation is exacerbated when the\nboundary is replaced with a randomly initialized\none (#4), emphasizing the importance of the pro-\nposed character-boundary injection. Also, both\nremoving the boundary-wised positional encoding\n(#5) or replacing the vanilla attention by linear at-\ntention (Wang et al., 2020) (#6) lead to modest\nBLEU degradation. While, there is no significant\nimpact when using unidirectional CGA (# 2, from\nFast to Slow).\nAblations on interactions in different levels\nOur default configuration permits the model to\nallocate interactions at each encoder layer. It is\nbeneficial to determine how interaction frequency\nimpacts the performance. Table 6 compares vari-\nous interaction frequencies at different levels, in-\ncluding exclusively at the final encoder block, the\nbottom three blocks, the top three blocks and every\ntwo blocks. The experiments were conducted on\n6890\nNegation Past P-Gender P-Number\n90\n92\n94\n96\n98\n100 Baseline\nTranSFormer\nFigure 6: Results on MorphEval. Where P-Gender\nand P-Number stands for pron2nouns-gender and\npron2nouns-number.\n# Model FLOPs\n1 Baseline 1.1G FLOPs\n2 TranSFormer 1.4G FLOPs\n3 Baseline (Big) 3.9G FLOPs\n4 TranSFormer (Big) 5.0G FLOPs\nTable 8: Comparison of FLOPs of different models.\nWMT En-De. It is evident that the default configu-\nration delivers optimal performance. Interactions\nconducted at the top three blocks demonstrate supe-\nrior results compared to those at the bottom three\nblocks. Furthermore, performing fusion solely at\nthe last encoder block proves insufficient for the\nmodel to learn multiscale interactions effectively.\nBLEU v.s. Depth and BPE mergings Figure\n5 plots the performance against model depths and\nBPE merging operations. The proposed TranS-\nFormer architecture demonstrates consistent per-\nformance improvements as a result of increased\nencoder depth. Furthermore, an empirical evalua-\ntion of the TranSFormer against various byte-pair\nencoding (BPE) operations (Sennrich et al., 2016),\non the slow branch of the model yields a statisti-\ncally significant average gain of 1.1 BLEU scores\nover the Slow only baseline.\nLow resource setting and morphological evalu-\nation Li et al. (2021) has shown that character-\nlevel systems are better at handling morpholog-\nical phenomena and show strong performance in\nlow-resource scenarios than subword-level systems.\nConsequently, we evaluate how TranSFormer be-\nhaves at these scenarios. For the low-resource set-\nting, we randomly select subsets of 50K, 500K,\nand 1000K from the WMT En-De training corpus.\nTranSFormer achieves respective BLEU scores of\n11.87, 22.75, and 25.30, while the character-only\nand subword-only Transformers yield approximate\nscores of 10.50/7.00, 20.50/22.00, and 22.50/23.50.\nThis empirical evidence demonstrates that TranS-\nFormer effectively amalgamates the benefits of\nboth character-level and subword-level features.\nMoreover, Figure 6 plots the performance on Mor-\nphEval(Burlot and Yvon, 2017) benchmark. TranS-\nFormer behaves better than subword solely in terms\nof Negation, Past, P-Gender and P-Number met-\nrics.\nComparisons in Efficiency Table 8 compares\nthe FLOPs between baseline and our TranSFormer\nboth in base and big configurations. Due to the\nlight computation cost of the fast branch, TranS-\nFormer only brings additional 0.3G/1.1G FLOPS\nin base/big configurations, respectively. Note that\nthe bulk of the additional computational cost is as-\nsociated with the upsampling/downsampling oper-\nations within the cross-granularity attention mech-\nanism. This process aligns the hidden size between\nthe two representations.\n5 Conclusions\nIn this work, we comprehensively leverage the\npotential of character-level features in multiscale\nsequence models while preserving high computa-\ntional efficiency. To accomplish this, we propose\na Slow-Fast Transformer architecture consisting\nof two branches in the encoder. The slow branch,\nakin to the vanilla Transformer, handles subword-\nlevel features, while the fast branch captures fine-\ngrained correlations among characters. By lever-\naging the complementary features provided by the\nfast branch, our TranSFormer demonstrates consis-\ntent improvements in BLEU scores on three widely-\nused machine translation benchmarks. Further in-\ndepth analyses demonstrate the effectiveness of the\nTranSFormer and its potential as a universal multi-\nscale learning framework.\nAcknowledgments\nThis work was supported in part by the National\nScience Foundation of China (No. 62276056), the\nNational Key R&D Program of China, the China\nHTRD Center Project (No. 2020AAA0107904),\nthe Natural Science Foundation of Liaoning\nProvince of China (2022-KF-16-01), the Yunnan\nProvincial Major Science and Technology Special\nPlan Projects (No. 202103AA080015), the Funda-\nmental Research Funds for the Central Universities\n(Nos. N2216016, N2216001, and N2216002), and\nthe Program of Introducing Talents of Discipline\nto Universities, Plan 111 (No. B16009).\n6891\nLimitations\nThe proposed TranSFormer architecture employs\na two-branch design, which separately encodes\ncharacter-level and subword-level features. Our\noriginal design of the proposed cross-granularity\nattention is to acknowledge the correlation between\nsubwords and characters that belong to the same\nword. For example, a cross-granularity Gaussian\ndistribution to let subwords pay more attention to\nthe corresponding characters. However, the vari-\nability of word boundary information across sen-\ntences presents a challenge in effectively batching\nthem and achieving high computational efficiency.\nThis is an area of ongoing research, and will be\nthe focus of future work. On the other hand, our\ncurrent evaluation of the TranSFormer architecture\nis limited to machine translation tasks. It is worth\nexploring the potential of TranSFormer in optimiz-\ning character sequences on natural language un-\nderstanding tasks and other sequence generation\ntasks, such as abstractive summarization. These\ntasks are more challenging in terms of encoding\nlonger sequences, but we believe that TranSFormer\ncan serve as a versatile backbone. We aim to verify\nits effectiveness on these tasks in the future.\nReferences\nKarim Ahmed, Nitish Shirish Keskar, and Richard\nSocher. 2017. Weighted transformer network for\nmachine translation. CoRR, abs/1711.02132.\nFranck Burlot and François Yvon. 2017. Evaluating the\nmorphological competence of machine translation\nsystems. In Proceedings of the Second Conference\non Machine Translation, pages 43–55, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nColin Cherry, George Foster, Ankur Bapna, Orhan Firat,\nand Wolfgang Macherey. 2018. Revisiting character-\nbased neural machine translation with capacity and\ncompression. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 4295–4305, Brussels, Belgium.\nAssociation for Computational Linguistics.\nYang Fan, Shufang Xie, Yingce Xia, Lijun Wu, Tao Qin,\nXiang-Yang Li, and Tie-Yan Liu. 2020. Multi-branch\nattentive transformer. CoRR, abs/2006.10270.\nChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik,\nand Kaiming He. 2019. Slowfast networks for video\nrecognition. In 2019 IEEE/CVF International Con-\nference on Computer Vision, ICCV 2019, Seoul, Ko-\nrea (South), October 27 - November 2, 2019, pages\n6201–6210. IEEE.\nYingqiang Gao, Nikola I. Nikolov, Yuhuang Hu, and\nRichard H.R. Hahnloser. 2020. Character-level trans-\nlation with self-attention. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1591–1604, Online. Association\nfor Computational Linguistics.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki\nParmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,\nZhengdong Zhang, Yonghui Wu, and Ruoming Pang.\n2020. Conformer: Convolution-augmented trans-\nformer for speech recognition. In Interspeech 2020,\n21st Annual Conference of the International Speech\nCommunication Association, Virtual Event, Shang-\nhai, China, 25-29 October 2020, pages 5036–5040.\nISCA.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Xiangyang Xue,\nand Zheng Zhang. 2020. Multi-scale self-attention\nfor text classification. In The Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 7847–7854. AAAI Press.\nKai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chun-\njing Xu, and Yunhe Wang. 2021. Transformer in\ntransformer. arXiv preprint arXiv:2103.00112.\nJie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang,\nand Zhaopeng Tu. 2019. Multi-granularity self-\nattention for neural machine translation. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 887–897, Hong\nKong, China. Association for Computational Lin-\nguistics.\nHany Hassan, Anthony Aue, Chang Chen, Vishal\nChowdhary, Jonathan Clark, Christian Federmann,\nXuedong Huang, Marcin Junczys-Dowmunt, William\nLewis, Mu Li, et al. 2018. Achieving human par-\nity on automatic chinese to english news translation.\narXiv preprint arXiv:1803.05567.\nJungo Kasai, James Cross, Marjan Ghazvininejad, and\nJiatao Gu. 2020. Non-autoregressive machine trans-\nlation with disentangled context transformer. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 5144–5155. PMLR.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France,\n6892\nApril 24-26, 2017, Conference Track Proceedings .\nOpenReview.net.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nJason Lee, Kyunghyun Cho, and Thomas Hofmann.\n2017. Fully character-level neural machine transla-\ntion without explicit segmentation. Transactions of\nthe Association for Computational Linguistics, 5:365–\n378.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative refinement. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1173–1182,\nBrussels, Belgium. Association for Computational\nLinguistics.\nBei Li, Quan Du, Tao Zhou, Yi Jing, Shuhan Zhou, Xin\nZeng, Tong Xiao, JingBo Zhu, Xuebo Liu, and Min\nZhang. 2022a. ODE transformer: An ordinary differ-\nential equation-inspired model for sequence genera-\ntion. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 8335–8351, Dublin,\nIreland. Association for Computational Linguistics.\nBei Li, Tong Zheng, Yi Jing, Chengbo Jiao, Tong Xiao,\nand Jingbo Zhu. 2022b. Learning multiscale trans-\nformer models for sequence generation. In Interna-\ntional Conference on Machine Learning, ICML 2022,\n17-23 July 2022, Baltimore, Maryland, USA, pages\n13225–13241. PMLR.\nJiahuan Li, Yutong Shen, Shujian Huang, Xinyu Dai,\nand Jiajun Chen. 2021. When is char better than\nsubword: A systematic study of segmentation algo-\nrithms for neural machine translation. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 543–549, Online.\nAssociation for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nSachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2020.\nDelight: Very deep and light-weight transformer.\nArXiv preprint, abs/2008.00623.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 48–53, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nMyle Ott, Sergey Edunov, David Grangier, and Michael\nAuli. 2018. Scaling neural machine translation. In\nProceedings of the Third Conference on Machine\nTranslation: Research Papers, pages 1–9, Brussels,\nBelgium. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-attention with relative position representations.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 464–468, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1264–1274, Melbourne, Australia. Association\nfor Computational Linguistics.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768.\nLijun Wu, Shufang Xie, Yingce Xia, Yang Fan, Jian-\nHuang Lai, Tao Qin, and Tie-Yan Liu. 2020. Se-\nquence generation with mixed representations. In\nProceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 10388–10398. PMLR.\nWei Wu, Houfeng Wang, Tianyu Liu, and Shuming\nMa. 2018. Phrase-level self-attention networks for\nuniversal sentence encoding. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3729–3738, Brussels,\nBelgium. Association for Computational Linguistics.\n6893\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2022. ByT5: Towards a token-free\nfuture with pre-trained byte-to-byte models. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:291–306.\nJianhao Yan, Fandong Meng, and Jie Zhou. 2020. Multi-\nunit transformers for neural machine translation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 1047–1059, Online. Association for Computa-\ntional Linguistics.\nBaosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong\nMeng, Lidia S. Chao, and Tong Zhang. 2018. Model-\ning localness for self-attention networks. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4449–4458,\nBrussels, Belgium. Association for Computational\nLinguistics.\nBaosong Yang, Longyue Wang, Derek F. Wong, Lidia S.\nChao, and Zhaopeng Tu. 2019. Convolutional self-\nattention networks. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 4040–4045, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nGuangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang,\nand Liangchen Luo. 2019. MUSE: parallel multi-\nscale attention for sequence to sequence learning.\nCoRR, abs/1911.09483.\n6894\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection Limitation\n□\u0013 A2. Did you discuss any potential risks of your work?\n□\u0013\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\n□\u0017\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNot applicable. Left blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n6895\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 5\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n6896"
}