{
  "title": "PLM-ICD: Automatic ICD Coding with Pretrained Language Models",
  "url": "https://openalex.org/W4287854971",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2696577581",
      "name": "Chao‐Wei Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287857456",
      "name": "Shang-Chi Tsai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4208538295",
      "name": "Yun-Nung Chen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035294872",
    "https://openalex.org/W3106092787",
    "https://openalex.org/W3167829962",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W2987216115",
    "https://openalex.org/W3174340663",
    "https://openalex.org/W3037422790",
    "https://openalex.org/W2997050424",
    "https://openalex.org/W2985173696",
    "https://openalex.org/W2768948924",
    "https://openalex.org/W1775813496",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W3184149472",
    "https://openalex.org/W2964142373",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3213059131",
    "https://openalex.org/W1997057722",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3082278026",
    "https://openalex.org/W2987098737",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3103901889",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2962854673",
    "https://openalex.org/W4293582125",
    "https://openalex.org/W2798843374",
    "https://openalex.org/W3034503829",
    "https://openalex.org/W2985962305"
  ],
  "abstract": "Automatically classifying electronic health records (EHRs) into diagnostic codes has been challenging to the NLP community. State-of-the-art methods treated this problem as a multi-label classification problem and proposed various architectures to model this problem. However, these systems did not leverage the superb performance of pretrained language models, which achieved superb performance on natural language understanding tasks. Prior work has shown that pretrained language models underperformed on this task with the regular fine-tuning scheme. Therefore, this paper aims at analyzing the causes of the underperformance and developing a framework for automatic ICD coding with pretrained language models. We spotted three main issues through the experiments: 1) large label space, 2) long input sequences, and 3) domain mismatch between pretraining and fine-tuning. We propose PLM-ICD, a framework that tackles the challenges with various strategies. The experimental results show that our proposed framework can overcome the challenges and achieves state-of-the-art performance in terms of multiple metrics on the benchmark MIMIC data. Our source code is available at https://github.com/MiuLab/PLM-ICD.",
  "full_text": "Proceedings of the 4th Clinical Natural Language Processing Workshop, pages 10 - 20\nJuly 14, 2022 ©2022 Association for Computational Linguistics\nPLM-ICD: Automatic ICD Coding with Pretrained Language Models\nChao-Wei Huang⋆† Shang-Chi Tsai⋆ Yun-Nung Chen⋆\n⋆National Taiwan University, Taipei, Taiwan\n†Taiwan AI Labs, Taipei, Taiwan\nf07922069@csie.ntu.edu.tw y.v.chen@ieee.org\nAbstract\nAutomatically classifying electronic health\nrecords (EHRs) into diagnostic codes has been\nchallenging to the NLP community. State-of-\nthe-art methods treated this problem as a multi-\nlabel classification problem and proposed vari-\nous architectures to model this problem. How-\never, these systems did not leverage the superb\nperformance of pretrained language models,\nwhich achieved superb performance on natural\nlanguage understanding tasks. Prior work has\nshown that pretrained language models under-\nperformed on this task with the regular fine-\ntuning scheme. Therefore, this paper aims at\nanalyzing the causes of the underperformance\nand developing a framework for automatic ICD\ncoding with pretrained language models. We\nspotted three main issues through the experi-\nments: 1) large label space, 2) long input se-\nquences, and 3) domain mismatch between pre-\ntraining and fine-tuning. We propose PLM-\nICD, a framework that tackles the challenges\nwith various strategies. The experimental re-\nsults show that our proposed framework can\novercome the challenges and achieves state-of-\nthe-art performance in terms of multiple met-\nrics on the benchmark MIMIC data.1\n1 Introduction\nThe clinical notes in electronic health records\n(EHRs) are written as free-form text by clinicians\nduring patient visits. The notes can be associated\nwith diagnostic codes from the International Clas-\nsification of Diseases (ICD), which represent diag-\nnostic and procedural information of the visit. The\nICD codes are a standardized way to encode infor-\nmation systematically and internationally, which\ncould be used for tracking healthcare statistics,\nquality outcomes, and billing.\nWhile ICD codes provide several useful appli-\ncations, manually labelling ICD codes has been\n1The source code is available at https://github.\ncom/MiuLab/PLM-ICD.\nshown to be very labor-intensive and domain ex-\npertise is required (O’malley et al., 2005). Hence,\nautomatically assigning ICD codes to clinical notes\nhas been of broad interest in the medical natural\nlanguage processing (NLP) community. Prior work\nhas identified several challenges of this task, in-\ncluding the large number of labels to be classified,\nthe long input sequence, and the imbalanced la-\nbel distribution, i.e., the long-tail problem (Xie\net al., 2019). These challenges make the task\nextremely difficult, demonstrating that advanced\nmodeling techniques are required. With the intro-\nduction of deep learning models, we have seen\ntremendous performance improvement on the task\nof automatic ICD coding (Shi et al., 2017; Xie and\nXing, 2018; Mullenbach et al., 2018; Li and Yu,\n2020; Vu et al., 2020; Cao et al., 2020; Liu et al.,\n2021; Kim and Ganapathi, 2021; Zhou et al., 2021).\nThese methods utilized convolutional neural net-\nworks (CNNs) (Mullenbach et al., 2018; Li and\nYu, 2020; Liu et al., 2021) or recurrent neural net-\nworks (RNNs) (Vu et al., 2020) to transform the\nlong text in clinical notes into hidden representa-\ntions. State-of-the-art methods employed a label\nattention mechanism, i.e., performing attention to\nhidden representations independently for each la-\nbel, to combat the extremely large label set (Mul-\nlenbach et al., 2018; Vu et al., 2020).\nRecently, pretrained language models (PLMs)\nwith the Transformer (Vaswani et al., 2017) archi-\ntecture have become the dominant forces for NLP\nresearch, achieving superior performance on nu-\nmerous natural language understanding tasks (De-\nvlin et al., 2019; Liu et al., 2019). These models\nare pretrained on large amount of text with various\nlanguage modeling objectives, and then fine-tuned\non the desired downstream tasks to perform dif-\nferent functionalities such as classification (Devlin\net al., 2019) or text generation (Radford et al., 2019;\nRaffel et al., 2020).\nWhile PLMs demonstrate impressive capabili-\n10\nties across classification tasks, applying PLMs to\nautomatic ICD coding is still not well-studied. Pre-\nvious work has shown that applying PLMs to this\ntask is not straightforward (Zhang et al., 2020; Pas-\ncual et al., 2021), and the main challenges being:\n• The length of clinical notes exceeds the maxi-\nmum length of PLMs.\n• The regular fine-tuning scheme where we add\na linear layer on top of the PLMs does not per-\nform well for multi-label classification prob-\nlems with a large label set.\n• PLMs are usually pretrained on general-\ndomain corpora, while clinical notes are very\nmedical-specific and the language usage is\ndifferent.\nAs a result, the performance of PLMs reported in\nthe prior work is inferior to the state-of-the-art mod-\nels that did not use pre-trained models by a large\nmargin (Pascual et al., 2021). Their best model\nachieved 88.65% in terms of micro-AUC, com-\npared with the state-of-the-art 94.9% from the ISD\nmodel (Zhou et al., 2021). This result highlighted\nthat the performance of PLMs on this task was still\nfar from the conventional models.\nIn this paper, we aim at identifying the chal-\nlenges met during applying PLMs to automatic\nICD coding and developing a framework that could\novercome these challenges. We first conduct pre-\nliminary experiments to verify and investigate the\nchallenges mentioned above, and then we propose\nproper mechanisms to tackle each challenge. The\nproposed mechanisms are: 1) domain-specific pre-\ntraining for the domain mismatch problem, 2) seg-\nment pooling for the long input sequence problem,\nand 3) label attention for the large label set problem.\nBy integrating these techniques together, we pro-\npose PLM-ICD, a framework specifically designed\nfor automatic ICD coding with PLMs. The effec-\ntiveness of PLM-ICD is verified through experi-\nments on the benchmark MIMIC-3 and MIMIC-2\ndatasets (Saeed et al., 2011; Johnson et al., 2016).\nTo the best of our knowledge, PLM-ICD is the first\nTransformer-based pretrained language model that\nachieves competitive performance on the MIMIC\ndatasets. We further analyze several factors that\naffect the performance of PLMs, including pre-\ntraining method, pretraining corpora, vocabulary\nconstruction, and optimization schedules.\nThe contributions of this paper are 3-fold:\n• We perform experiments to verify and analyze\nthe challenges of utilizing PLMs on the task\nof automatic ICD coding.\n• We developPLM-ICD, a framework to fine-\ntune PLMs for ICD coding, that achieves\ncompetitive performance on the benchmark\nMIMIC-3 dataset.\n• We analyze the factors that affect PLMs’ per-\nformance on this task.\n2 Related Work\n2.1 Automatic ICD Coding\nICD code prediction is a challenging task in the\nmedical domain. Several recent work attempted to\napproach this task with neural models. Choi et al.\n(2016) and Baumel et al. (2018) used recurrent\nneural networks (RNN) to encode the EHR data\nfor predicting diagnostic results. Li and Yu (2020)\nrecently utilized a multi-filter convolutional layer\nand a residual layer to improve the performance of\nICD prediction. On the other hand, several work\ntried to integrate external medical knowledge into\nthis task. In order to leverage the information of\ndefinition of each ICD code, RNN and CNN were\nadopted to encode the diagnostic descriptions of\nICD codes for better prediction via attention mech-\nanism (Shi et al., 2017; Mullenbach et al., 2018).\nMoreover, the prior work tried to consider the hi-\nerarchical structure of ICD codes (Xie and Xing,\n2018), which proposed a tree-of-sequences LSTM\nto simultaneously capture the hierarchical relation-\nship among codes and the semantics of each code.\nAlso, Tsai et al. (2019) introduced various ways of\nleveraging the hierarchical knowledge of ICD by\nadding refined loss functions. Recently, Cao et al.\n(2020) proposed to train ICD code embeddings in\nhyperbolic space to model the hierarchical struc-\nture. Additionally, they used graph neural network\nto capture the code co-occurrences. LAAT (Vu\net al., 2020) integrated a bidirectional LSTM with\nan improved label-aware attention mechanism. Ef-\nfectiveCAN (Liu et al., 2021) integrated a squeeze-\nand-excitation network and residual connections\nalong with extracting representations from all en-\ncoder layers for label attention. The authors also\nintroduced focal loss to tackle the long-tail predic-\ntion problem. ISD (Zhou et al., 2021) employed\nextraction of shared representations among high-\nfrequency and low-frequency codes and a self-\ndistillation learning mechanism to alleviate the\n11\nlong-tail code distribution. Kim and Ganapathi\n(2021) proposed a framework called Read, Attend,\nand Code (RAC) to effectively predict ICD codes,\nwhich is the current state-of-the-art model on this\ntask. Most recent models focused on developing\nan effective interaction between note representa-\ntions and code representations (Cao et al., 2020;\nZhou et al., 2021; Kim and Ganapathi, 2021). Our\nwork, instead, is focusing on the choice of the note\nencoder, where we apply PLMs for their superior\nencoding capabilities.\n2.2 Pretrained Language Models\nUsing pretrained language models to extract con-\ntextualuzed representations has led to consistent\nimprovements across most NLP tasks. Notably,\nELMo (Peters et al., 2018) and BERT (Devlin\net al., 2019) showed that pretraining is effective for\nboth LSTM and transformer (Vaswani et al., 2017)\nmodels. Variants have been proposed such as XL-\nNet (Yang et al., 2019), RoBERTa (Liu et al., 2019).\nThese models are pretrained on large amount of\ngeneral domain text to grasp the capability to model\ntextual data, and fine-tuned on common classifica-\ntion tasks.\nTo tackle domain-specific problems, prior work\nadapted such models to scientific and biomedical\ndomains, including BioBERT (Lee et al., 2019),\nClinicalBERT (Alsentzer et al., 2019), PubMed-\nBERT (Gu et al., 2020) and RoBERTa-PM (Lewis\net al., 2020). These models are pretrained on\ndomain-specific text carefully crawled and pro-\ncessed for improving the downstream performance.\nThe biomedical-specific PLMs reported improved\nperformance on a variety of biomedical tasks, in-\ncluding text mining, named entity recognition, rela-\ntion extraction, and question answering (Lee et al.,\n2019).\nWhile PLMs achieved state-of-the-art perfor-\nmance on various tasks, applying PLMs to large-\nscale multi-label classification is still a challeng-\ning research direction. Chang et al. (2019) pro-\nposed X-BERT, a framework that is scalable to\nan extremely large label set of a million labels.\nLeheˇcka et al. (2020) showed that the modeling ca-\npacity of BERT’s pooling layers might be limited\nfor automatic ICD coding. Pascual et al. (2021)\nalso demonstrated inferior performance when ap-\nplying BERT to this task and pointed out several\nchallenges to be addressed. Specifically, the au-\nthors proposed 5 truncation and splitting strategies\nModel Length Macro-F Micro-F\nLAAT 4000 9.9 57.5\n512∗ 6.8 47.3\nBERT 512 ∗ 2.8 38.9\nTable 1: Results of LAAT and BERT on MIMIC-3 with\ndifferent maximum input lengths (%). ∗The length is\nnumber of words for LAAT and number of tokens for\nBERT, so their performance cannot directly comparable.\nto tackle the long input sequence problem. Their\nproposed All splitting strategies is similar to our\nsegment pooling mechanism. However, without\nthe label attention mechanism, the model failed to\nlearn.\nZhang et al. (2020) proposed BERT-XML, an\nextension of BERT for ICD coding. The model\nwas pretrained on a large cohort of EHR clinical\nnotes with an EHR-specific vocabulary. BERT-\nXML handles long input text by splitting it into\nchunks and performs prediction for each chunk\nindependently with a label attention mechanism\nfrom AttentionXML (You et al., 2019). The predic-\ntions are finally combined with max-pooling. Our\nproposed framework, PLM-ICD, shares a similar\nidea with BERT-XML that we also split clinical\nnotes into segments to compute segment represen-\ntations. The main difference is that we leverage\nan improved label attention mechanism and we\nuse document-level label-specific representations\nrather than chunk level representations as in BERT-\nXML. In Section 5, we demonstrate that PLM-ICD\ncan achieve superior results on the commonly used\nMIMIC-3 dataset compared with BERT-XML.\n3 Challenges for PLMs\nIn this section, we discuss 3 main challenges for\nPLMs to work on automatic ICD coding and con-\nduct experiments to verify the severity of the chal-\nlenges.\n3.1 Long Input Text\nPretrained language models usually set a maximum\nsequence length as the size of their positional en-\ncodings. A typical value is set to 512 tokens after\nsubword tokenization (Devlin et al., 2019). How-\never, clinical notes are long documents which of-\nten exceed the maximum length of PLMs. For in-\nstance, the average number of words in the MIMIC-\n3 dataset is 1,500 words, or 2000 tokens after sub-\n12\nModel Codes Macro-F Micro-F\nLAAT 50 66.6 71.5\nFull 9.9 57.5\nBERT 50 61.5 65.4\nFull 3.2 40.9\nTable 2: Results of LAAT and BERT on MIMIC-3 with\nfull codes and top-50 codes (%).\nword tokenization.\nTo demonstrate that this is a detrimental problem\nfor PLMs, we conduct experiments on MIMIC-3\nwhere the input text is truncated to 512 words for\nthe strong model LAAT (Vu et al., 2020), and 512\ntokens for BERT. The results are shown in Table 1.\nBoth models perform worse when the input text\nis truncated, showing that simple truncation does\nnot work for the long input text problem. Note that\nthe same trend can be found for other models for\nICD coding. The results reported by Pascual et al.\n(2021) also show similar problem where the trun-\ncation methods such as Front-512 and Back-512\nperformed much worse than models with longer\ninput context.\n3.2 Large Label Set\nAutomatic ICD coding is a large-scale multi-label\ntext classification (LMTC) problem, i.e., finding\nthe relevant labels of a document from a large set\nof labels. There are about 17,000 codes in ICD-9-\nCM and 140,000 codes in ICD-10-CM/PCS, while\nthere are 8921 codes presented in the MIMIC-3\ndataset. PLMs utilize a special token and extract\nthe hidden representation of this token to perform\nclassification tasks. For example, BERT uses a\n[CLS] token and adds a pooling layer to trans-\nform its hidden representation into a distribution\nof labels (Devlin et al., 2019). However, while this\napproach achieves impressive performance on typ-\nical multi-class classification tasks, it is not very\nsuitable for LMTC tasks. Lehe ˇcka et al. (2020)\nshowed that making predictions based on only the\nrepresentation of [CLS] token results in inferior\nperformance compared with pooling representa-\ntions of all tokens, and hypothesized that this is\ndue to the lack of modeling capacity of using the\n[CLS] token alone.\nTo examine the PLMs’ capability of perform-\ning LMTC, we conduct experiments on MIMIC-3\nin two settings, Full and Top-50. The Full\nsetting uses the full set of 8,921 labels, while the\nTop-50 uses the top-50 most frequent labels. We\nreport the numbers for LAAT directly from Vu et al.\n(2020). For the BERT model, we use the segment\npooling mechanism to handle the long input, which\nis detailed in Section 4.2. We aggregate the hid-\nden representations of the [CLS] token for each\nsegment with mean-pooling as the document rep-\nresentation. The final prediction is obtained by\ntransforming the document representation with a\nlinear layer.\nThe results are shown in Table 2. BERT achieves\nslightly worse performance than LAAT in the\nTop-50 setting. However, in the Full setting,\nBERT performs significantly worse compared with\nLAAT. The results suggest that using BERT’s\n[CLS] token for LMTC is not ideal, and advanced\ntechniques for LMTC are required for PLMs to\nwork on this task.\n3.3 Domain Mismatch\nNormally, PLMs are pretrained on large amount\nof general-domain corpora which contains billions\nof tokens. The corpora is typically crawled from\nWikipedia, novels (Zhu et al., 2015), webpages, and\nweb forums. Prior work has shown that the domain\nmismatch between the pretraining corpus and the\nfine-tuning tasks could degrade the downstream\nperformance (Gururangan et al., 2020).\nSpecifically for the biomedical domain, sev-\neral pretrained models have been proposed which\nare pretrained on biomedical corpora to mitigate\nthe domain mismatch problem (Lee et al., 2019;\nAlsentzer et al., 2019; Gu et al., 2020; Lewis et al.,\n2020). These models demonstrate improved perfor-\nmance over BERT on various medical and clinical\ntasks, showing that domain-specific pretraining is\nessential to achieve good performance.\n4 Proposed Framework\nThe task of ICD code prediction is formulated as a\nmulti-label classification problem (Kavuluru et al.,\n2015; Mullenbach et al., 2018). Given a clinical\nnote of |d|tokens d = {t1, t2, ··· , t|d|}in EHR,\nthe goal is to predict a set of ICD codes y ⊆Y,\nwhere Ydenotes the set of all possible codes. Typ-\nically, the labels are represented as a binary vector\ny ∈{0, 1}|Y|, where each bit yi indicates whether\nthe corresponding label is presented in the note.\nThe proposed framework PLM-ICD is illus-\ntrated in Figure 1. The details of the components\n13\nDomain-specific Pretraining\nTransformer encoder\nLinear\nType2 [MASK]is\ndiabetes Fine-tuning for ICD CodingPLMsegment1\nsegment2\nsegment|s| . . .PLM\nPLM. . .LabelAttention\nLinear\nLabel-aware DocumentRepresentation\nCode Distribution\nFigure 1: Illustration of our proposed framework. Left: domain-specific pretraining, where a PLM is pretrained\non text from specific domains with a language modeling objective. Right: PLM encodes segments of a document\nseparately, and a label-aware attention mechanism is to aggregate the segment representations into label-aware\ndocument representations. The document representations are linear-transformed to predict ICD codes.\nare described in this section.\n4.1 Domain-Specific Pretraining\nAutomatic ICD coding is a domain-specific task\nwhere the input text consists of clinical notes writ-\nten by clinicians. The clinical notes contain many\nbiomedical terms, and understanding these terms\nis essential in order to assign ICD codes accurately.\nWhile general PLMs are pretrained on large amount\nof text, the pretraining corpora usually does not\ncontain biomedical text, not to mention clinical\nrecords.\nIn order to mitigate the domain mismatch prob-\nlem, we propose to utilize the PLMs that are\npretrained on biomedical and clinical text, e.g.,\nBioBERT (Lee et al., 2019), PubMedBERT (Gu\net al., 2020), and RoBERTa-PM (Lewis et al.,\n2020). These PLMs are specifically pretrained for\nbiomedical tasks and proven to be effective on vari-\nous downstream tasks. We take the domain-specific\nPLMs and fine-tune them on the task of automatic\nICD coding. We can plug-and-play the domain-\nspecific PLMs since their architectural design and\npretraining objective are identical to their general-\ndomain counterparts. This makes our framework\nagnostic to the type of PLMs, i.e., we can apply\nany transformer-based PLMs as the encoder.\n4.2 Segment Pooling\nIn order to tackle the long input text problem de-\nscribed in Section 3.1, we propose segment pool-\ning to surpass the maximum length limitation of\nPLMs. The segment pooling mechanism first splits\nthe whole document into segments that are shorter\nthan the maximum length, and encodes them into\nsegment representations with PLMs. After encod-\ning segments, the segment representations are ag-\ngregated as the representations for the full docu-\nment.\nMore formally, given a document d =\n{t1, t2, ..., t|d|}of |d|tokens, we split it into |s|\nconsecutive segments si of c tokens:\nsi = {tj |c ·i ≤j < c·(i + 1)}\nThe segments are fed into PLMs separately to com-\npute hidden representations, then concatenated to\nobtain the hidden representations of all tokens:\nH = concat(PLM (s1), ··· , PLM(s|s|))\nThe token-wise hidden representations H can then\nbe used to make prediction based on the whole\ndocument.\n4.3 Label-Aware Attention\nTo combat the problem of a large label set, we pro-\npose to augment the PLMs with the label-aware\nattention mechanism proposed by Vu et al. (2020)\nto learn label-specific representations that capture\nthe important text fragments relevant to certain la-\nbels. After the token-wise hidden representations\nH are obtained, the goal is to transform H into\nlabel-specific representations with attention mecha-\nnism.\nThe label-aware attention takes H as input and\ncompute |Y|label-specific representations. This\nmechanism can be formulated into 2 steps. First, a\nlabel-wise attention weight matrix A is computed\n14\nas:\nZ = tanh(VH)\nA = softmax(WZ)\nwhere V and W are linear transforms. The ith row\nof A represents the weights of the ith label, and\nthe softmax function is performed for each label\nto form a distribution over all tokens. Then, the\nmatrix A is used to perform a weighted-sum of H\nto compute the label-specific document representa-\ntion:\nD = HA⊤\nwhere Di represents the document representations\nfor the ith label.\nFinally, we use the label-specific document rep-\nresentation D to make predictions:\npi = sigmoid(⟨Li, Di⟩)\nwhere Li is a vector for the ith label, ⟨·⟩represents\ninner product between two vectors, pi is the pre-\ndicted probability of the ith label. Note that the\ninner product could also be seen as a linear trans-\nform with output size 1. We can then assign labels\nto a document based on a predefined threshold t.\nThe training objective is to minimize the binary\ncross-entropy loss L(y, p):\n−1\n|y|\n|y|∑\ni=1\n(\nyi log pi + (1−yi) log(1−pi)\n)\n.\n5 Experiments\nIn order to evaluate the effectiveness of our pro-\nposed framework, we conduct experiments and\ncompare the results with the prior work.\n5.1 Setup\nWe evaluate PLM-ICD on two benchmark datasets\nfor ICD code prediction.\n• MIMIC-2 To be able to directly compare\nwith the prior work (Mullenbach et al., 2018;\nLi and Yu, 2020; Vu et al., 2020), we evalu-\nate PLM-ICD on the MIMIC-2 dataset (Saeed\net al., 2011). We follow the setting from Mul-\nlenbach et al. (2018), where 20,533 sum-\nmaries are used for training, and 2,282 sum-\nmaries are used for testing. There are 5,031\nlabels in the dataset.\n• MIMIC-3 The Medical Information Mart\nfor Intensive Care III (MIMIC-3) (Johnson\net al., 2016) dataset is a benchmark dataset\nwhich contains text and structured records\nfrom a hospital ICU. We use the same setting\nas Mullenbach et al. (2018), where 47,724 dis-\ncharge summaries are used for training, with\n1,632 summaries and 3,372 summaries for val-\nidation and testing, respectively. There are\n8,922 labels in the dataset.\nThe preprocessing is done by following the steps\ndescribed in Mullenbach et al. (2018) with their\nprovided scripts 2. Detailed training setting is pro-\nvided in Appendix A.\n5.2 Evaluation\nWe evaluate our methods with commonly used met-\nrics to be directly comparable to previous work.\nThe metrics used are macro F1, micro F1, macro\nAUC, micro AUC, and precision@K, whereK =\n{5, 8, 15}.\n5.3 Results\nWe present the evaluation results in this section. All\nthe reported scores are averaged over 3 runs with\ndifferent random seeds. The results of the com-\npared methods are taken directly from their original\npaper. We mainly compare our model, PLM-ICD,\nwith the models without special code description\nmodeling. The performance of models with special\ncode description modeling, i.e., HyperCore, ISD,\nand RAC, are also reported for reference.\n5.3.1 MIMIC-3\nThe results on MIMIC-3 full test set are shown in\nTable 3. PLM-ICD achieves state-of-the-art per-\nformance among all models in terms of micro F1\nand all precision@k measures, even though we do\nnot leverage any code description modeling. All\nthe improvements are statistically significant. RAC\nperforms best on AUC scores and macro F1. We\nnote that the techniques proposed by RAC are com-\nplementary to our framework, and it is possible to\nadd the techniques to further improve our results.\nHowever, this is out of the scope of this paper.\n5.3.2 MIMIC-2\nThe results on MIMIC-2 test set are shown in Ta-\nble 4. PLM-ICD achieves state-of-the-art perfor-\nmance among all models in terms of micro F1 and\n2https://github.com/jamesmullenbach/\ncaml-mimic\n15\nModel AUC F1 P@k\nMacro Micro Macro Micro P@5 P@8 P@15\nCAML (2018) 89.5 98.6 8.8 53.9 - 70.9 56.1\nDR-CAML (2018) 89.7 98.5 8.6 52.9 - 69.0 54.8\nMultiResCNN (2020) 91.0 98.6 8.5 55.2 - 73.4 58.4\nLAAT (2020) 91.9 98.8 9.9 57.5 81.3 73.8 59.1\nJointLAAT (2020) 92.1 98.8 10.7 57.5 80.6 73.5 59.0\nEffectiveCAN (2021) 91.5 98.8 10.6 58.9 - 75.8 60.6\nPLM-ICD (Ours) 92.6 (.2) 98.9 (.1) 10.4 (.1) 59.8†(.3) 84.4†(.2) 77.1†(.2) 61.3†(.1)\nModels with Special Code Description Modeling\nHyperCore (2020) 93.0 98.9 9.0 55.1 - 72.2 57.9\nISD (2021) 93.8 99.0 11.9 55.9 - 74.5 -\nRAC (2021) 94.8 99.2 12.7 58.6 82.9 75.4 60.1\nTable 3: Results on the MIMIC-3 full test set (%). The best scores among models without special code description\nmodeling are marked in bold. The best scores among all models are italic. The values in the parentheses are the\nstandard variation of runs. †indicates the significant improvement with p <0.05.\nModel AUC F1 P@k\nMacro Micro Macro Micro P@5 P@8 P@15\nCAML (2018) 82.0 96.6 4.8 44.2 - 52.3 -\nDR-CAML (2018) 82.6 96.6 4.9 45.7 - 51.5 -\nMultiResCNN (2020) 85.0 96.8 5.2 46.4 - 54.4 -\nLAAT (2020) 86.8 97.3 5.9 48.6 64.9 55.0 39.7\nJointLAAT (2020) 87.1 97.2 6.8 49.1 65.2 55.1 39.6\nPLM-ICD (Ours) 86.8 (.2) 97.3 (.1) 6.1 (.1) 50.4†(.2) 67.3†(.2) 56.1†(.2) 39.9 (.2)\nModels with Special Code Description Modeling\nHyperCore (2020) 88.5 97.1 7.0 47.7 - 53.7 -\nISD (2021) 90.1 97.7 10.1 49.8 - 56.4 -\nTable 4: Results on the MIMIC-2 test set (%). EffectiveCAN (2021) and RAC (2021) did not report results on\nMIMIC-2. The best scores among models without special code description modeling are marked in bold. The\nbest scores among all models are italicized. The values in the parentheses are the standard variation of the runs. †\nindicates that the improvement is statistically significant with p <0.05.\nall precision@k measures, similar to the results on\nMIMIC-3. All the improvements are statistically\nsignificant except for P@15.\nIn sum, these results show that PLM-ICD is gen-\neralizable to multiple datasets, achieving state-of-\nthe-art performance on multiple metrics on both\nMIMIC-3 and MIMIC-2.\n6 Analysis\nThis section provides analysis on factors that affect\nPLM’s performance on automatic ICD coding.\nModel Macro-F Micro-F\nPLM-ICD 10.4 59.8\n(a) - domain pretraining 8.9 54.2\n(b) - segment pooling 7.2 54.6\n(c) - label attention 4.6 48.0\nTable 5: Ablation results on the MIMIC-3 full test set\n(%).\n6.1 Ablation Study\nTo verify the effectiveness of the proposed tech-\nniques, we conduct an ablation study on MIMIC-3\nfull test set. The results are presented in Table 5.\nThe first ablation we perform is discarding\n16\nModel Macro-F Micro-F ˆF\nRoBERTa-PM 10.4 59.8 1.35\nBioBERT 9.1 57.9 1.60\nClinicalBERT 8.8 57.8 1.60\nPubMedBERT 9.2 59.5 1.41\nTable 6: Results with different PLMs on the MIMIC-3\nfull test set (%). ˆF is the fragmentation ratio.\ndomain-specific pretraining. In this setting, we\nuse the pretrained RoBERTa-base model as the\nPLM, and fine-tune it for ICD coding. As shown\nin row (a), the performance slightly degrades after\ndiscarding domain-specific pretraining. This re-\nsult demonstrates that domain-specific pretraining\ncontributes to the performance improvement.\nThe second ablation we perform is discarding\nsegment pooling. In this setting, we replace our\nsegment pooling with the one proposed by Zhang\net al. (2020) They applied label attention and made\ncode predictions for each segment separately, and\naggregated the predictions with max-pooling. As\nshown in row (b), replacing our segment pooling\nresults in worse performance. This result indicates\nthat our proposed segment pooling is more effective\nfor aggregating segment representations.\nThe third ablation is removing the label atten-\ntion mechanism. We fall back to the normal PLM\nparadigm, i.e., extracting representations of the\n[CLS] token for classification. This setting is iden-\ntical to the one described in Section 3.2, where we\naggregate the representation of the [CLS] token\nfor each segment with mean-pooling, and obtain\nthe final prediction by transforming the aggregated\nrepresentation with a linear layer. As shown in row\n(c), removing label attention mechanism results in\nhuge performance degradation. The micro F1 score\ndegrades by 11.8% absolute, while the macro F1\nscore degrades more than half. This result demon-\nstrates that the label attention mechanism is crucial\nto ICD coding, which is an observation aligned\nwith the prior work (Mullenbach et al., 2018).\n6.2 Effect of Pretrained Models\nWhile we have shown that domain-specific pretrain-\ning is beneficial to ICD coding, we would like to\nexplore which domain-specific PLM performs the\nbest on this task. We conduct experiments with dif-\nferent PLMs, including BioBERT (Lee et al., 2019),\nClinicalBERT (Alsentzer et al., 2019), PubMed-\nBERT (Gu et al., 2020), and RoBERTa-PM (Lewis\nModel Macro-F Micro-F\nLAAT 10.4 59.8\nCAML 8.7 58.1\nBERT-XML 8.2 56.9\nTable 7: Results with different attention mechanisms on\nthe MIMIC-3 full test set (%).\nModel Macro-F Micro-F\nOurs 10.4 59.8\nHIER-BERT 2.8 42.7\nLongformer 5.1 51.6\nTable 8: Results with different strategies for tackling the\nlong input problem on the MIMIC-3 full test set (%).\net al., 2020).\nThe results are presented in Table 6. RoBERTa-\nPM achieves the best performance among the 4\nexamined PLMs This result is in line with the re-\nported results on the BLURB leaderboard (Gu et al.,\n2020), which is a collection of biomedical tasks.\nWe also report the fragmentation ratio, i.e., the\nnumber of tokens per word after subword tokeniza-\ntion as (Chalkidis et al., 2020). We observe that the\nPLMs with vocabulary trained on biomedical texts\n(RoBERTa-PM and PubMedBERT) perform better\nthan the ones inherited vocabulary from BERT-base\n(BioBERT and ClinicalBERT). The framentation\nratio also shows that models with custom vocabu-\nlary suffer less on the over-fragmentation problem.\n6.3 Effect of Label Attention Mechanisms\nWe conduct experiments with different label atten-\ntion mechanisms and report the results in Table 7.\nWe compare the label attention mechanisms from\nLAAT (Vu et al., 2020), CAML (Mullenbach et al.,\n2018) and BERT-XML (Zhang et al., 2020). The\nresults show that the label attention used in LAAT\nis best-suited to our framework.\n6.4 Effect of Long Input Strategies\nWe also conduct experiments to verify the effect\nof different strategies for tackling the long input\nproblem. As shown in Table 8, our proposed seg-\nment pooling outperforms HIER-BERT (Chalkidis\net al., 2019) and Longformer (Beltagy et al., 2020),\ndemonstrating the effectiveness of our proposed\nmethod.\n17\nMax Segment Macro-F Micro-FLength Length\n6144 128 9.2 60.0\n3072 256 9.4 59.2\n3072 128 9.2 59.6\n3072 64 8.2 59.3\n3072 32 6.9 57.8\nTable 9: Results with different maximum lengths on the\nMIMIC-3 full dev set (%).\n6.5 Effect of Maximum Length\nWe conduct experiments where we alter the max-\nimum length of the documents and segments to\nexplore the different choices of maximum lengths.\nThe results are shown in Table 9.\nWhen fixing the maximum length of the docu-\nments to 3,072, we observe that longer segments re-\nsults in better performance until the segment length\nreaches 128. Using a longer maximum document\nlength such as 6144 results in slightly better perfor-\nmance. However, longer sequences require more\ncomputation. Considering the trade-off between\ncomputation and accuracy, we set maximum docu-\nment length to 3,072 and segment length to 128 as\nour defaults.\n6.6 Effect of Optimization Process\nSimilar to the prior work (Sun et al., 2019), we also\nnotice that the fine-tuning process is sensitive to the\nhyperparameters of the optimization process, e.g.,\nbatch size, learning rate, and warmup schedule.\nWith several preliminary experiments conducted\non these factors, we observe that the learning rate\nand the warmup schedule greatly affects the per-\nformance. When we reduce learning rate to 2e-5,\nthe model performs 3% worse than using the de-\nfault parameters in terms of micro F1. The warmup\nschedule is crucial in our framework. When we\nuse constant learning rate throughout training, the\nmodel performs about 4% worse. We do not ob-\nserve clear difference between different scheduling\nstrategies.\n6.7 Best Practices\nWith the above analyses, we provide a guideline\nand possible future directions for applying PLMs\nto ICD coding or tasks with similar properties:\n• With the input length exceeding the maximum\nlength of PLMs, segment pooling can be used\nto extract representations of all tokens. PLMs\nwith longer input length or recurrence could\nbe explored in the future.\n• The representation of the [CLS] token might\nbe insufficient when dealing with LMTC prob-\nlems. A label attention mechanism could be\nbeneficial in such scenarios.\n• The pretraining corpora plays an important\nrole for domain-specific tasks.\n• The hyperparameters of the optimization pro-\ncess greatly affect the final performance, so\ntrying different parameters is preferred when\nthe performance is not ideal.\n7 Conclusion\nIn this paper, we identify the main challenges of\napplying PLMs on automatic ICD coding, includ-\ning the long text input, the large label set and the\nmismatched domain. We propose PLM-ICD, a\nframework with PLMs that tackles the challenges\nwith various techniques. The proposed frame-\nwork achieves state-of-the-art or competitive per-\nformance on the MIMIC-3 and MIMIC-2 datasets.\nWe then further analyze factors that affect PLMs’\nperformance. We hope this work could open up the\nresearch direction of leveraging the great potential\nof PLMs on ICD coding.\nAcknowledgements\nWe thank reviewers for their insightful comments.\nThis work was financially supported from the\nYoung Scholar Fellowship Program by Ministry\nof Science and Technology (MOST) in Taiwan,\nunder Grants 111-2628-E-002-016 and 111-2634-\nF-002-014.\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical BERT\nembeddings. In Proceedings of the 2nd Clinical Nat-\nural Language Processing Workshop, pages 72–78,\nMinneapolis, Minnesota, USA. Association for Com-\nputational Linguistics.\nTal Baumel, Jumana Nassour-Kassis, Raphael Cohen,\nMichael Elhadad, and Noemie Elhadad. 2018. Multi-\nlabel classification of patient notes: case study on icd\ncode assignment. In Workshops at the Thirty-Second\nAAAI Conference on Artificial Intelligence.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\n18\nPengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Sheng-\nping Liu, and Weifeng Chong. 2020. HyperCore: Hy-\nperbolic and co-graph representation for automatic\nICD coding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3105–3114, Online. Association for Computa-\ntional Linguistics.\nIlias Chalkidis, Ion Androutsopoulos, and Nikolaos Ale-\ntras. 2019. Neural legal judgment prediction in En-\nglish. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4317–4323, Florence, Italy. Association for Compu-\ntational Linguistics.\nIlias Chalkidis, Manos Fergadiotis, Sotiris Kotitsas, Pro-\ndromos Malakasiotis, Nikolaos Aletras, and Ion An-\ndroutsopoulos. 2020. An empirical study on large-\nscale multi-label text classification including few and\nzero-shot labels. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7503–7515, Online. As-\nsociation for Computational Linguistics.\nWei-Cheng Chang, Hsiang-Fu Yu, Kai Zhong, Yiming\nYang, and Inderjit Dhillon. 2019. Taming pretrained\ntransformers for extreme multi-label text classifica-\ntion. arXiv preprint arXiv:1905.02331.\nEdward Choi, Mohammad Taha Bahadori, Andy\nSchuetz, Walter F Stewart, and Jimeng Sun. 2016.\nDoctor AI: Predicting clinical events via recurrent\nneural networks. In Machine learning for healthcare\nconference, pages 301–318. PMLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL, pages 4171–4186, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2020. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. arXiv preprint arXiv:2007.15779.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nAlistair EW Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016. Mimic-\niii, a freely accessible critical care database. Scien-\ntific data, 3:160035.\nRamakanth Kavuluru, Anthony Rios, and Yuan Lu.\n2015. An empirical evaluation of supervised learning\napproaches in assigning diagnosis codes to electronic\nmedical records. Artificial intelligence in medicine,\n65(2):155–166.\nByung-Hak Kim and Varun Ganapathi. 2021. Read,\nattend, and code: Pushing the limits of medical codes\nprediction from clinical notes by machines. In Ma-\nchine Learning for Healthcare Conference , pages\n196–208. PMLR.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2019. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics.\nJan Leheˇcka, Jan Švec, Pavel Ircing, and Luboš Šmídl.\n2020. Adjusting bert’s pooling layer for large-scale\nmulti-label text classification. In International Con-\nference on Text, Speech, and Dialogue, pages 214–\n221. Springer.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020. Pretrained language models for biomedi-\ncal and clinical tasks: Understanding and extending\nthe state-of-the-art. In Proceedings of the 3rd Clini-\ncal Natural Language Processing Workshop, pages\n146–157, Online. Association for Computational Lin-\nguistics.\nFei Li and Hong Yu. 2020. Icd coding from clinical\ntext using multi-filter residual convolutional neural\nnetwork. In AAAI.\nYang Liu, Hua Cheng, Russell Klopfer, Matthew R\nGormley, and Thomas Schaaf. 2021. Effective con-\nvolutional attention network for multi-label clinical\ndocument classification. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 5941–5953.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJames Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng\nSun, and Jacob Eisenstein. 2018. Explainable predic-\ntion of medical codes from clinical text. In NAACL,\npages 1101–1111, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nKimberly J O’malley, Karon F Cook, Matt D Price, Kim-\nberly Raiford Wildes, John F Hurdle, and Carol M\nAshton. 2005. Measuring diagnoses: ICD code accu-\nracy. Health services research, 40(5p2):1620–1639.\nDamian Pascual, Sandro Luck, and Roger Watten-\nhofer. 2021. Towards bert-based automatic icd cod-\ning: Limitations and opportunities. arXiv preprint\narXiv:2104.06709.\n19\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nMohammed Saeed, Mauricio Villarroel, Andrew T Reis-\nner, Gari Clifford, Li-Wei Lehman, George Moody,\nThomas Heldt, Tin H Kyaw, Benjamin Moody, and\nRoger G Mark. 2011. Multiparameter intelligent\nmonitoring in intensive care ii (mimic-ii): a public-\naccess intensive care unit database. Critical care\nmedicine, 39(5):952.\nHaoran Shi, Pengtao Xie, Zhiting Hu, Ming Zhang, and\nEric P Xing. 2017. Towards automated icd coding us-\ning deep learning. arXiv preprint arXiv:1711.04075.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to fine-tune bert for text classification?\nIn China National Conference on Chinese Computa-\ntional Linguistics, pages 194–206. Springer.\nShang-Chi Tsai, Ting-Yun Chang, and Yun-Nung Chen.\n2019. Leveraging hierarchical category knowledge\nfor data-imbalanced multi-label diagnostic text un-\nderstanding. In LOUHI, pages 39–43, Hong Kong.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nThanh Vu, Dat Quoc Nguyen, and Anthony Nguyen.\n2020. A label attention model for icd coding from\nclinical text. In Proceedings of the Twenty-Ninth\nInternational Joint Conference on Artificial Intel-\nligence, IJCAI-20, pages 3335–3341. International\nJoint Conferences on Artificial Intelligence Organi-\nzation.\nPengtao Xie and Eric Xing. 2018. A neural architecture\nfor automated ICD coding. In ACL, pages 1066–\n1076, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nXiancheng Xie, Yun Xiong, Philip S Yu, and Yangyong\nZhu. 2019. Ehr coding with multi-scale feature at-\ntention and structured knowledge graph propagation.\nIn Proceedings of the 28th ACM International Con-\nference on Information and Knowledge Management,\npages 649–658.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nRonghui You, Zihan Zhang, Ziye Wang, Suyang Dai,\nHiroshi Mamitsuka, and Shanfeng Zhu. 2019. At-\ntentionxml: Label tree-based attention-aware deep\nmodel for high-performance extreme multi-label text\nclassification. Advances in Neural Information Pro-\ncessing Systems, 32:5820–5830.\nZachariah Zhang, Jingshu Liu, and Narges Razavian.\n2020. BERT-XML: Large scale automated ICD cod-\ning using BERT pretraining. In Proceedings of the\n3rd Clinical Natural Language Processing Workshop,\npages 24–34, Online. Association for Computational\nLinguistics.\nTong Zhou, Pengfei Cao, Yubo Chen, Kang Liu, Jun\nZhao, Kun Niu, Weifeng Chong, and Shengping Liu.\n2021. Automatic icd coding via interactive shared\nrepresentation networks with self-distillation mech-\nanism. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 5948–5957.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE in-\nternational conference on computer vision , pages\n19–27.\nA Training Details\nWe take the pretrained weights released by original\nauthors without any modification. For the best\nPLM-ICD model, we use RoBERTa-base-PM-M3-\nV oc released by Lewis et al. (2020). During fine-\ntuning, we train our models for 20 epochs. AdamW\nis chosen as the optimizer with a learning rate of\n5e −5. We employ a linear warmup schedule with\n2000 warmup steps, and after that the learning rate\ndecays linearly to 0 throughout training. The batch\nsize is set to 8. All models are trained on a GTX\n3070 GPU. We truncate discharge summaries to\n3072 tokens due to memory consideration, and\nthe length of each segment c is set to 128. The\nvalidation set is used to find the best-performing\nthreshold t, and we use it to perform evaluation on\nthe test set.\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8299458026885986
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.8074679374694824
    },
    {
      "name": "Language model",
      "score": 0.6764174699783325
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6305949687957764
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.6022655963897705
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5722552537918091
    },
    {
      "name": "Machine learning",
      "score": 0.5089659690856934
    },
    {
      "name": "Source code",
      "score": 0.49449822306632996
    },
    {
      "name": "Natural language processing",
      "score": 0.45872849225997925
    },
    {
      "name": "Task (project management)",
      "score": 0.43312332034111023
    },
    {
      "name": "Programming language",
      "score": 0.12330365180969238
    },
    {
      "name": "Engineering",
      "score": 0.07820075750350952
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16733864",
      "name": "National Taiwan University",
      "country": "TW"
    }
  ]
}