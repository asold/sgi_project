{
    "title": "Improved biomedical word embeddings in the transformer era",
    "url": "https://openalex.org/W3116317633",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4223444093",
            "name": "Noh, Jiho",
            "affiliations": [
                "University of Kentucky"
            ]
        },
        {
            "id": "https://openalex.org/A4221367953",
            "name": "Kavuluru, Ramakanth",
            "affiliations": [
                "University of Kentucky"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6680532216",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W202767273",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2759366113",
        "https://openalex.org/W2882319491",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2970193165",
        "https://openalex.org/W2963923670",
        "https://openalex.org/W2941748305",
        "https://openalex.org/W2944400536",
        "https://openalex.org/W6744181819",
        "https://openalex.org/W1503259811",
        "https://openalex.org/W2890173166",
        "https://openalex.org/W2963611386",
        "https://openalex.org/W6695472520",
        "https://openalex.org/W1485069053",
        "https://openalex.org/W6741425342",
        "https://openalex.org/W2946690328",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6729210268",
        "https://openalex.org/W2740582239",
        "https://openalex.org/W6648116614",
        "https://openalex.org/W2118658201",
        "https://openalex.org/W6608056421",
        "https://openalex.org/W2084377579",
        "https://openalex.org/W6828911396",
        "https://openalex.org/W2515248967",
        "https://openalex.org/W2562366217",
        "https://openalex.org/W2148972377",
        "https://openalex.org/W1775813496",
        "https://openalex.org/W6786091164",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3104200514",
        "https://openalex.org/W2930601680",
        "https://openalex.org/W4239253651",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2122686681",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W2735651584",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2757849489",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2099307202",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W4247236552",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W2914874661",
        "https://openalex.org/W2911456401",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W628520641",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4234388646",
        "https://openalex.org/W1991154713",
        "https://openalex.org/W4298417552",
        "https://openalex.org/W2962986031",
        "https://openalex.org/W3106003309",
        "https://openalex.org/W2964054038",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2284851926",
        "https://openalex.org/W2887442125"
    ],
    "abstract": null,
    "full_text": "Improved Biomedical Word Embeddings in the Transformer Era\nJiho Nohb, Ramakanth Kavulurua,b\naDivision of Biomedical Informatics, Department of Internal Medicine, University of Kentucky\nbDepartment of Computer Science, University of Kentucky\nAbstract\nBackground: Recent natural language processing (NLP) research is dominated by neural net-\nwork methods that employ word embeddings as basic building blocks. Pre-training with neu-\nral methods that capture local and global distributional properties (e.g., skip-gram, GLoVE)\nusing free text corpora is often used to embed both words and concepts. Pre-trained em-\nbeddings are typically leveraged in downstream tasks using various neural architectures that\nare designed to optimize task-speciﬁc objectives that might further tune such embeddings.\nObjective: Despite advances in contextualized language model based embeddings, static\nword embeddings still form an essential starting point in BioNLP research and applications.\nThey are useful in low resource settings and in lexical semantics studies. Our main goal is\nto build improved biomedical word embeddings and make them publicly available for down-\nstream applications.\nMethods: We jointly learn word and concept embeddings by ﬁrst using the skip-gram method\nand further ﬁne-tuning them with correlational information manifesting in co-occurring Med-\nical Subject Heading (MeSH) concepts in biomedical citations. This ﬁne-tuning is accom-\nplished with the transformer-based BERT architecture in the two-sentence input mode with\na classiﬁcation objective that captures MeSH pair co-occurrence. We conduct evaluations\nof these tuned static embeddings using multiple datasets for word relatedness developed by\nprevious eﬀorts.\nResults: Both in qualitative and quantitative evaluations we demonstrate that our meth-\nods produce improved biomedical embeddings in comparison with other static embedding\neﬀorts. Without selectively culling concepts and terms (as was pursued by previous eﬀorts),\nwe believe we oﬀer the most exhaustive evaluation of biomedical embeddings to date with\nclear performance improvements across the board.\nConclusion: We repurposed a transformer architecture (typically used to generate dynamic\nembeddings) to improve static biomedical word embeddings using concept correlations. We\nprovide our code and embeddings for public use for downstream applications and research\nendeavors: https://github.com/bionlproc/BERT-CRel-Embeddings\nKeywords: Word embeddings, ﬁne-tuned embeddings, contextualized embeddings\nEmail addresses: jiho.noh@uky.edu (Jiho Noh),rvkavu2@uky.edu (Ramakanth Kavuluru)\nPreprint submitted to Elsevier July 26, 2021\narXiv:2012.11808v3  [cs.CL]  23 Jul 2021\n1. Introduction\nBiomedical natural language processing (BioNLP) continues to be a thriving ﬁeld of re-\nsearch, garnering both academic interest and industry uptake. Its applications manifest\nacross the full translational science spectrum. From extracting newly reported protein-\nprotein interactions from literature to mining adverse drug events discussed in the clinical\ntext, researchers have leveraged NLP methods to expedite tasks that would otherwise quickly\nbecome intractable to handle with a completely manual process. Computer-assisted coding\ntoolssuchas3M360Encompass, clinicaldecisionmakingassistantssuchasIBMMicromedex\nwith Watson, and information extraction API such as Amazon Comprehend Medical are pop-\nular use-cases in the industry. As textual data explodes in the form of scientiﬁc literature,\nclinical notes, and consumer discourse on social media, NLP methods have become indis-\npensable in aiding human experts in making sense of the increasingly data heavy landscape\nof biomedicine. The rise of deep neural networks (DNNs) in computer vision and NLP ﬁelds\nhas quickly spread to corresponding applications in biomedicine and healthcare. Especially,\nas of now, BioNLP almost exclusively relies on DNNs to obtain state-of-the-art results in\nnamed entity recognition (NER), relation extraction (RE), and entity/concept linking or\nnormalization (EN) — the typical components in biomedical information extraction1.\n1.1. Neural word embeddings\nThe central idea in DNNs for NLP is the notion of dense embeddings of linguistic units in\nRd (d-dimensional vector space of real numbers) fordthat generally ranges from a few dozen\nto several hundreds. The unit is typically a word [1, 2, 3], but can also be a subword [4] (e.g.,\npreﬁx/suﬃx) or even a subcharacter [5] (for Chinese characters that can be broken down\nfurther). These dense embeddings are typicallypre-trainedusing large free text corpora (e.g.,\nWikipedia, PubMed citations, public tweets) by optimizing an objective that predicts local\ncontext or exploits global context in capturing distributional properties of linguistic units.\nBased on the well-known distributional hypothesis that words appearing in similar contexts\naresemanticallyrelatedorsharemeaning[6], thispre-trainingoftenleadstoembeddingsthat\nexhibit interesting properties inRd that correspond to shared meaning. Once pre-trained,\nword embeddings are generally ﬁne-tuned in a supervised classiﬁcation task (with labeled\ndata) using a task-speciﬁc DNN architecture that builds on top of these embeddings. While\nthe notion of dense word embeddings existed in the nineties (e.g., latent semantic indexing),\nneural embeddings together with task-speciﬁc DNNs have revolutionized the ﬁeld of NLP\nover the past decade.\nA static word embedding is a function that maps each unique word in a corpus to a\nsingle dense vector, which is ﬁxed regardless of its use in the context. Word2vec [3] is one\nsuch method that had an extensive inﬂuence on NLP applications due to its simple model\narchitecture and eﬃcient training techniques. Word2vec is a shallow neural network that\npredicts which words appear in the context of a target word (or vice versa). GloVe [7]\n1Some exceptions exist when handling smaller datasets in highly speciﬁc domains where ensembles of\nlinear models may prove to be better\n2\nextends the word2vec method and aims to approximate the word co-occurrence counts, with\nfaster training and comparable performance even with a small corpus. GloVe diﬀers in\nthat word2vec is a learning-based predictive model, whereas GloVe is a count statistics-\nbased model. FastText [4] was an attempt to address OOV (Out-of-Vocabulary) problem\nby considering the representations of a word’s constituent character-leveln-grams. These\nmodels are the most representative static word embedding methods in NLP.\nSince 2018, however, the static embeddings discussed thus far have been improved upon\nto address issues with polysemy and homonymy. Around the same time, transformers (such\nas BERT [8] and RoBERTa [9]), ELMo [10], and UMLFiT [11] have been developed to\nfacilitate contextualized embeddings that generate the embedding of a word based on its\nsurrounding context. This process typically generates diﬀerent embeddings for polysemous\noccurrences of a word, such as when the word “discharge” is used to indicate bodily secretions\nor the act of releasing a patient from a hospital. Even for words that typically have a unique\nmeaning, contextual embeddings might generate embeddings that more precisely capture the\nsubtleties in how it is used in a particular context. Such contextualized embeddings might be\nbetter suited when predicting NER tags or composing word sequences toward a classiﬁcation\nend-goal.\n1.2. Motivation for improved static embeddings\nAlthough contextualized embeddings are an excellent addition to the neural NLP reper-\ntoire, we believe there is merit in improving the static embeddings for various reasons:\n(1). Contextualized models are based on language modeling and are more complex with\nmultiple layers of recurrent units or self-attention modules. Base models tend to have tens\nof millions of parameters [12] and using them without GPUs in low-resource settings such\nas smart devices used in edge computing or IoT is infeasible. Simpler models that use static\nembeddings can be built with 1–2 orders of magnitude fewer parameters and can run on\nsmaller CPUs even in low resource settings. While leaner transformers are actively being\ninvestigated (e.g., DistilBERT [13]), they oﬀer nowhere near the model size reduction needed\nfor usage in low resource settings. Increasing use-cases of “edge NLP” [14] further motivate\nour current eﬀort. (2). Static embeddings can be of inherent utility for linguists to continue\nto study lexical semantics of biomedical language by looking into word or subword embed-\ndings and how they may be indicative of lexical relations (e.g., hypernymy and meronymy).\nAnother related use case is to study noun compound decomposition [15] in the biomedical\nlanguage, which is typically treated as a bracketing task that ought to rely only on the lo-\ncal context within the noun compound. For example,candidate ((tumor suppressor) gene)\nand ((tumor suppressor) gene) listdemonstrate two diﬀerent decompositions of four-word\ncompounds. (3). Contextualized embeddings typically only make sense in languages that\nhave large digitized corpora. For less known languages that have smaller repositories, the\nlanguage modeling objective such embeddings rely on can lead to signiﬁcant overﬁtting com-\npared to static approaches [16]. (4). Improved static word embeddings can also help initialize\nthe embeddings before the process of language-modeling-based training ensues in the more\nexpensive contextualized models2 to further enhance them (when compute power is not a\n2This clearly assumes that the same tokenization is appropriately maintained in both static and the\nsubsequent contextualized models\n3\nmajor limitation).\n1.3. Related work\nIn this section, we brieﬂy discuss previously proposed methods for training domain-\nspeciﬁc word/concept embeddings, which we compare with our methods for this paper (as\nshown in Table 7). Wang et al. [17] trained word embeddings on unstructured electronic\nhealth record (EHR) data using fastText. The subword embeddings of the fastText model\nenabled them to obtain vector representations of OOV tokens. Park et al. [18] proposed a\nmodel for learning UMLS concept embeddings from their deﬁnitions combined with corre-\nspondingWikipediaarticles[18]. Thedegreeofrelatednessbetweentwoconceptsismeasured\nby the cosine similarity between the corresponding concept vectors. Zhang et al. [19] pro-\nposed a similar method to ours for preparing the training corpus. They also used the MeSH\nRDF-based graph from which they sampled random paths to generate sequences of MeSH\nterms and used them to train word embeddings; in our work, we traverse the MeSH hierarchy\nto obtain single in-order path of MeSH concepts of which each node is represented by its\npreferred concept name, unique MeSH code, and its deﬁnition. Yu et al. [20] also trained\nUMLS concept embeddings and ﬁne-tuned them using a “retroﬁtting” method developed\nby Faruqui et al. [21]. They improved pre-trained embeddings using concept relationship\nknowledge deﬁned in the UMLS semantic lexicon. Among diﬀerent relationships, they claim\nthat RO (has other relationship) and RQ (related and possibly synonymous) relationships\nreturned the most improvements on the UMNSRS evaluation dataset. Henry et al. [22] com-\nputed several association measures, such asmutual information, with concept co-occurrence\ncounts and measured the semantic similarity and relatedness between concepts. Overall, the\nPearson’s Chi squared association measure (χ2) performed the best.\n1.4. Overall contributions\nIn this paper, we propose and evaluate methods to improve static biomedical word em-\nbeddings to be made publicly available for downstream use by the community. Our main\ncontributions follow.\n• We jointly learn word and concept embeddings by leveraging deﬁnitional information\nfor rare concepts to supplement concept-annotated corpora. Through this, we transfer\nbidirectional semantic signal between words and concepts, ﬁrst using the PubTator\nconcept-annotated corpus with fastText and subsequently using concept co-occurrences\n(along with their preferred names) to further ﬁne-tune embeddings by adapting the\nBERT encoder in the two-sentence input mode.\n• We assess the quality of the resulting embeddings with qualitative analyses and quan-\ntitative comparisons with those generated by prior methods on public datasets. With-\nout selectively culling concepts and terms (as was pursued by previous eﬀorts), we\nbelieve we oﬀer the most exhaustive evaluation of static embeddings to date with\nclear performance improvements across the board. We provide our code and em-\nbeddings for public use for downstream applications and research endeavors:https:\n//github.com/bionlproc/BERT-CRel-Embeddings\n4\n2. Methods\nBefore we outline the framework and intuition behind our methods, we ﬁrst motivate the\nidea of jointly learning embeddings for biomedical concepts and words in the context of our\ngoals. Our framework toward improved biomedical word embeddings is depicted in Figure 1\nwhose components will be discussed in the rest of this section.\n2.1. High level intuition and overview\nBiomedical concepts are analogous to named entities in general English. Names of genes,\ndrugs, diseases, and procedures are typical examples of concepts. Just like entity linking in\ngeneral NLP research, concept mapping is typically needed in BioNLP where concepts are\nto be mapped to their standardized counterparts in some expert curated terminology. This\nmapping part is harder in BioNLP given the variety of ways a concept can be referred to in\nrunning text. Often, there might not be much lexical overlap between diﬀerent aliases that\npoint to the same concept. For example, the procedureulnar collateral ligament reconstruc-\ntion is also called Tommy John surgeryand they both refer to the same medical subject\nheading (MeSH) concept code D000070638. These aliases are provided in the corresponding\nterminology and the uniﬁed medical language system (UMLS) metathesaurus that integrates\nmany such terminologies.\nFigure 1: The schematic of our approach to improve word embeddings. S1 deals with pre-processing steps\nto create a concept enhanced corpus. S2 involves conventional pre-training using local context prediction\nobjectives. S3 constitutes ﬁne-tuning with distributional regularities based on co-occurrence. For S3, entity\npairs are constructed based on two relevance rules: rule-1 is concept co-occurrence in a PubMed citation\nand rule-2 is proximity in a concept hierarchy\nOur ﬁrst main idea is to use a well-known concept mapping tool to spot concepts in large\nbiomedical corpora and insert those concept codes adjacent to the concept spans. This step is\nindicated as theS1 portion in Figure 1. Subsequently, run a pre-training method to embed\nboth words and concepts in the same space inRd. This jointly learns embeddings for both\nwords and concepts and enables two-way sharing of semantic signal: ﬁrst word embeddings\n5\nare nudged to predict surrounding concepts, and as the pre-training window moves along\nthe running text, concept embeddings are also nudged to predict neighboring words. In fact,\nthis phenomenon has been exploited by multiple prior eﬀorts [23, 24, 25] including in our\nprior work [26]. Most of these eﬀorts aim to learn concept embeddings that can be used\nin downstream applications. Here we demonstrate that this process also improves the word\nembeddings themselves. This process is indicated through the S2 part of Figure 1. Our\nchoice for biomedical concepts to be jointly learned is the set of nearly 30,000 MeSH codes\nthat are used on a daily basis at the National Library of Medicine (NLM) by trained coders\nwho assign 10–15 such codes per biomedical article.\nOn top of this joint pre-training approach, we introduce a novel application of the BERT\nencoder architecture to further ﬁne-tune the word and concept embeddings with a classi-\nﬁcation objective that discriminates “co-occurring” MeSH codes (from PubMed citations)\nfrom random pairs of MeSH terms3. Here, co-occurrence refers to the two terms appear-\ning in the same citation as determined by human coders who annotated it. That is, the\npositive examples are derived from a set of MeSH codes assigned to a sampled biomedical\ncitation, and negative examples are random pairs of MeSH codes from the full terminology.\nIntuitively, if two codes are assigned to the same article, they are clearly related in some\nthematic manner. Besides this, we also derive additional positive pairs from the MeSH hier-\narchy by choosing those that are separated by at most two hops. “Jointness” is incorporated\nhere by appending each code with its preferred name. Speciﬁcally, in the two-sentence input\nmode for BERT, each sentence is a code and its preferred name appended next to it. This\ncode pair “relatedness” classiﬁcation task further transfers signal between words and codes\nleading to demonstrable gains in intrinsic evaluations of resulting word embeddings. These\nsteps are captured throughS3 in Figure 1. We present more speciﬁcs and implementational\ndetails in Sections 2.2 and 2.3.\nThe resulting embeddings are evaluated for their semantic representativeness using intrin-\nsic evaluations with well-known datasets and also through qualitative analyses. The results\nshow a substantial improvement in evaluations compared to prior best approaches. Overall,\nwe present an eﬀective novel application of transformer architectures originally developed for\ncontextualized embeddings to improve static word embeddings through joint learning and\nﬁne-tuning word/concept embeddings.\n2.2. Data Sources\nForS1andS2(inFigure1), tocarryoutconventionalpre-trainingandlearnword/concept\nembeddings, weseekafreepubliclyavailableresourcethatcomeswithannotationsofbiomed-\nical concepts from a well-known terminology. This is readily made available through the Pub-\nTator [27] initiative from BioNLP researchers at the NLM. It has over 30 million PubMed\ncitations (abstracts and titles from the 2021 baseline) and over 3 million full-text articles with\nhigh-quality annotations for genes (and their variants), diseases, chemicals, species, and cell\nlines. Our choice for the concept vocabulary was MeSH (2021 version) because the diseases\n3We chose BERT style encoding instead of EMLo and ULMFiT because BERT’s transformer architecture\nallows for a deeper sense of bidirectionality with the masked language modeling objective with multiple layers\nof attention; whereas ELMo and UMLFiT both use LSTMs, typically not amenable for parallelization with\nobjectives that do not encode bidirectionality as well as BERT\n6\nand chemicals from PubTator have mappings to MeSH codes; furthermore, with nearly 30K\nconcepts, MeSH is fairly representative of the general concept space in biomedicine. Addi-\ntionally, MeSH concepts also come with brief deﬁnitional blurbs describing their meaning in\ngeneral-purpose English (more later). We use these blurbs in pre-training for MeSH concepts\nthat do not appear in PubTator annotations.\n2.2.1. Concept annotated corpus for pre-training\nPre-training step S2 in Figure 1 uses fastText [4] for training static embeddings. Fast-\nText improves upon the basic skip-gram model by learning word embeddings as compositions\nof constituent character n-grams and their representations. The corpus for this is a sample\nsubset (1–2%) of the PubTator dataset such that each PubMed citation sampled contains\nat least two annotations with MeSH concepts. MeSH codes from the annotations are in-\nserted immediately after the corresponding concept spans in texts. To distinguish MeSH\ncodes from regular words, we represent them asConceptCode||SourceVocab, essentially a\nconcatenation of the concept code andSourceVocab, an abbreviation for the source termi-\nnology. Although MeSH codes are unique enough, we chose this formatting to be amenable\nto a general setup with multiple terminologies. With this, consider the example title: “A\nmulti-centre international study of salivary hormone oestradiol and progesterone measure-\nments in ART monitoring.” With the corresponding codes inserted, this title is transformed\ninto: A multi-centre international study of salivary hormone oestradiolD004958MeSH and\nprogesteroneD011374MeSH measurements in ART monitoring. The two codes inserted next\nto “oestradiol” and “progesterone” were identiﬁed by PubTator.\nOur goal is to imbue a two-way semantic signal between all types of concepts and related\nwords. However, only a portion of the MeSH headings (9,477 out of 29,915) is referred\nto in the PubTator annotations. Hence, we ought to supplement PubTator based training\ndata with additional texts that contain the missing MeSH codes. This is where we exploit\nthe deﬁnitional information of concepts provided by MeSH creators. With this, each MeSH\nconcept provides a textual snippet for fastText. The snippet supplied is the concatenation\nof the preferred name, source code, and deﬁnition of the concept. For example, the MeSH\ncode D008654 for the concept Mesothelioma results in the textual input: “Mesothelioma\nD008654MeSH A tumor derived from mesothelial tissue (peritoneum, pleura, pericardium).\nIt appears as broad sheets of cells, with some regions containing spindle-shaped, sarcoma-\nlike cells and other regions showing adenomatous patterns. Pleural mesotheliomas have been\nlinked to exposure to asbestos.” This means, for codes that may never show up in any\nannotated PubTator documents, we guarantee a single document that is constructed in this\nmanner tying the concept with words that are highly relevant to its meaning. These are\nthe “serialized concept deﬁnitions” referred to in theS1 component of Figure 1. These\nadditional documents are supplied in an in-order traversal sequence of the MeSH hierarchy\nto fastText as a “mega” document where adjacent documents correspond to hierarchically\nrelated concepts. Table 1 describes the statistics of the textual resources used for pre-training\nstep S2.\n2.2.2. Training examples for code pair relatedness classiﬁcation\nComponent S3 of Figure 1 involves model BERT-CRel to further ﬁne-tune word and con-\ncept embeddings by capturing concept relatedness (CRel). It is a canonical transformer [28]\n7\nTable 1: Descriptive statistics of training examples for pre-training initial static embeddings\nPubTator Total # of sampled documents: 501,639 / 30,017,978 (∼2%)\nTotal # of unique MeSH descriptors covered: 9,477\nAverage # of entity mentions per document: 7.32\nAverage # of tokens per document: 204.12\nMeSH Total # of entities (descriptors only): 29,915\nAverage # of tokens per entity name: 2.00\nAverage # of tokens per entity deﬁnitional information 28.80\nmodel for a binary classiﬁcation task. In essence, this is repurposing the BERT architecture\nwithout any pre-training for the language modeling objective; we retain the classiﬁcation\nobjective with an additional feedforward layer and sigmoid unit feeding oﬀ of the[CLS]\ntoken output. The input is a pair (mi, mj) of “related” MeSH concepts in the two-sentence\ninput mode following the format\n[CLS] miwi\n1 ···wi\nn [SEP] mj wj\n1 ···wj\nm [SEP]\nwhere mi and mj are related MeSH codes andwi\n1 ···wi\nn is thepreferred nameof mi. [CLS]\nand [SEP] are well-known special tokens used in BERT models.\nPositive training pairs (mi, mj) are generated using two rules. Rule-1 deems the pair to\nbe related if both codes were assigned to some document in the sample corpusC by coders\nat the NLM. More formally, the set of all such positive pairs\nRC =\n⋃\nc∈C\n{(mi,mj) : ∀i̸=j mi,mj ∈M(c)},\nwhere M(c) is the set of MeSH concepts assigned to citationc. Rule-2 considers a pair to be\nrelated if the codes are connected by at most two hops in the directed-acyclic MeSH graph\nGMeSH. These would capture parent/child, grand parent/child, and sibling connections\nbetween concepts. Speciﬁcally,\nRMeSH = {(mi,mj) : dGMeSH (mi,mj) ≤2, ∀i̸=j mi,mj ∈GMeSH}∪RMeSH\nSA ∪RMeSH\nPA ,\nwhere d is graph distance,RMeSH\nSA is the set of “see also” relations, andRMeSH\nPA is the set\nof “pharmacological action” relations deﬁned between MeSH concepts by the NLM. These\nauxiliary relations are not part of the MeSH hierarchy but are publicly available to mine.\nFor instance, the concept Multiple Myeloma has a see-also link to the conceptMyeloma\nProteins, which in turn has a pharm-action connection to the conceptImmunologic Factors.\nIt is not diﬃcult to see that these relations also capture strong semantic relatedness between\nconcepts. RC ∪RMeSH is the full set of positive relations used to ﬁne-tune word/concept\nembeddings with BERT-CRel. To generate the same number of negative examples, we\nrandomly sample the MeSH concept pairs across the entire vocabulary, retaining the term\nfrequency distribution. Details of the numbers of examples used are in Table 2.\n8\nTable 2: Descriptive statistics of training examples for the BERT-CRel component\nPubTator Total # of sampled documents: 136,437 (∼0.5%)\nTotal # of unique MeSH descriptors covered: 24,995\nTotal # of concept pairs generated (including negative pairs): 8,752,116\nMeSH Total # of concept pairs generated: 363,697\nTotal # of concept pairs generated from auxiliary relationships: 23,598\n2.3. Models and Conﬁgurations\n2.3.1. fastText+: adjustments to fastText for word/concept pre-training\nAs indicated in Section 2.2.1 we use fastText [4] for the initial pre-training on the concept-\nannotated corpus created through PubTator and MeSH deﬁnitional information. Building\non the skip-gram model [3], fastText additionally models and composes character n-grams\nto form word embeddings, thus accounting for subword information. This can capture re-\nlatedness among morphological variants and in exploiting regularities in lexical meaning\nmanifesting in word forms through suﬃxes, preﬁxes, and other lemmata. It also helps in\nforming better embeddings on the ﬂy for some unseen words (through the constituent char-\nacter n-grams) instead of relying on the catch-all UNK embeddings that are typically used.\nHowever, we do not want this subword decomposition to occur when dealing with concept\nembeddings because they are atomic units, and there is no scope for unseen tokens given we\nknow the full code set upfront. Hence we impose the following two constraints.\n1. Concept codes (e.g., D002289MeSH) are not decomposed into subword vectors; the\nmodel thus is forced to recognize the concept codes from the corresponding tokens by\nthe unique formatConceptCode||SourceVocab.\n2. The output vocabulary must contain the full set of concept codes (here, MeSH descrip-\ntors) regardless of their frequencies in the corpus unlike the default case where fastText\nimposes a minimum frequency for character n-grams.\nFor the full implementation details of fastText, we refer to the original paper by Bo-\njanowski et al. [4]. Here, we only highlighted the modiﬁcations we sought to handle concept\ntokens. This adapted version of fastText is henceforth called fastText+ in this paper. Table 3\nlists the empirically chosen hyperparameters for training fastText for our concept-annotated\ncorpus. Note that the dimensionality of word vectors (dim) is intentionally chosen to be di-\nvisible by 12, the number of transformer blocks in the subsequent ﬁne-tuning phase through\nthe BERT architecture.\n2.3.2. BERT-CRel: Fine-tuning static embeddings with the concept relatedness objective\nWe introduced BERT-CRel in Section 2.2.2 to further ﬁne-tune pre-trained word/concept\nembeddings learned with fastText+. BERT-CRel is a shallow transformer encoder, which\nreads the textual representations of a concept pair and predicts their relatedness as a binary\nclassiﬁcation task. Note that is unlike the original purpose of BERT — to build contex-\ntualized embeddings. Furthermore, we do not use any pre-trained BERT model (such as\n9\nTable 3: Hyperparameters for word/concept pre-training through fastText\nParameters Values\nminCount (required number of word occurrences) 5\ndim (dimensionality of word vectors) 396\nws (size of context window) 30\nepoch (number of epochs) 5\nminn (min. length of character ngrams) 3\nmaxn (max. length of character ngrams) 6\nSciBERT) because our framework does not suit theWordPiece tokenization that is typi-\ncally used. What is available at this stage are the pre-trained word/concept embeddings\nfrom fastText+. So we repurpose BERT as shown in Figure 2. Here we apply a linear\ntransformation on the initial pre-trained static embeddings.\nFigure 2: BERT-CRel concept relatedness classiﬁcation model to ﬁne-tune embeddings\nThe input texts are tokenized using a simple white space-based split function followed\nby a text clean-up process. Initially, we load the original token embeddings with the pre-\ntrained static embeddings from fastText+. We provide examples of concept pairs (as outlined\nin Section 2.2.2) along with their binary relatedness labels to the model. Each input sequence\nstarts with[CLS], followed by a pair of concept phrases (code token followed by the preferred\nname for each concept) separated by [SEP]. While training, the ﬁrst[CLS] token collects\n10\nall the features for determining the relatedness label between two concepts. We add a linear\ntransformation layer following the original token embeddings to apply subtle adjustments to\nthe given token embeddings. This linear layer is initialized with the identity matrix.\nTwo-step optimization\nWe take a two-step optimization approach where during the ﬁrst step, we focus on opti-\nmizing the classiﬁcation model before ﬁne-tuning the pre-trained embeddings. To accomplish\nthis, during the ﬁrst step, only the transformer layers are updated with the speciﬁed range\nof learning rates[lrα\nmax,lrα\nmin], starting withlrα\nmax and decreasing with time. Once the opti-\nmizer reaches the minimum learning rate (lrα\nmin), we initiate the next optimization schedule\nby applying another range of learning rates[lrβ\nmax,lrβ\nmin] and start computing gradients of the\nlinear transformation layer. This new range is to update the linear transformation layer (Θ)\nand the pre-trained embeddings from fastText+ (E).\nThis second step is implemented using multi-stage annealing within learning rate range\n[lrβ\nmax,lrβ\nmin]. That is, we ﬁrst update the linear layer with ﬁxed embeddings from the\nprevious stage. This stops when the learning rate decreases tolrβ\nmin. At this point, the\nembeddings are updated (Ei+1 = ΘiEi) at once using the state of the parameters andΘi+1\nis set back toI (identity matrix). The learning rate is then reset to a higher value that starts\nat lri+1 = γi+1 ·lrβ\nmax (γ <1); and the process of updatingΘi+1 continues with ﬁxedEi+1.\nThis alternating process of freezingE and updatingΘ and then updatingE after reaching\nminimum learning rate is repeated untillri+1 reaches lrβ\nmin (which is the default manner\nin which PyTorch’sReduceLRonPlateauoperates). E1 is the pre-trained set of embeddings\nfrom fastText+ and Θ1 is initialized withI. Intuitively, this lets the learning rate bob within\nthe [lrβ\nmax,lrβ\nmin] range inspired by cyclical learning rate schedules [29] designed to overcome\nsaddle point plateaus.\nIntuitively, this two-stage optimization approach is to ensure that the layer closer to the\nﬁnal layer is trained ﬁrst in isolation (by freezing other layers) to give it ample scope to adapt\nto the ﬁnal objective. This helps the model better leverage the word embeddings that were\npre-trained via fastText. This freezing of the embedding layer allows it to exert sustained\ninﬂuence on the ﬁne-tuning of the classiﬁcation layers without undergoing any catastrophic\nforgetting. Once this happens, all layers are unfrozen in the end to be trained a ﬁnal time for\nbetter end-to-end convergence. This process has been termed “chain thaw” and was shown\nto work better compared to conventional training methods [30].\nImplementation details\nWe use PyTorch and HuggingFace’sBertForSequenceClassiﬁcationmodel to implement\nBERT-CRel. The model is evaluated on the validation set every 10,000 steps. Binary cross-\nentropy is the loss function used. We save the improved word embeddings of the best model\naccording to the UMNS dataset (more later) evaluation results. We useReduceLRonPlateau\nwith the initial learning ratelrα\nmax = 3e-5 and the minimum learning ratelrα\nmin = 2e-5\nwith decayγ = 0.9 for the initial step of updating just the transformer layers. The scheduler\nreduces learning rates by γ once it sees no improvement on the validation results three\nconsecutive times. While ﬁne-tuning static embeddings, during the multi-stage annealing\nprocess, we set the learning rates from3e-5 (lrβ\nmax) to1e-5 (lrβ\nmin) withγ = 0.8. The values\n11\nfor γs are empirically chosen with the intention of providing slower learning rate reduction\nfor the initial model learning than the ﬁne-tuning process.\n2.4. Evaluation Scenarios\n2.4.1. Qualitative evaluations\nAs a qualitative evaluation, we examine the representation learning quality of the em-\nbeddings produced by BERT-CRel. This is done in the context of other prior approaches\nfor generating biomedical word embeddings. For the sake of comparison, we use the same\nset of biomedical query terms (usually noun phrases) used in Wang et al.’s study [17]. The\ntask is to retrieve ﬁveclosest terms in the word/concept embedding space to each query\nterm and assess how related they actually are to the query term. For example, given the\nword ‘aspirin,’ we expect to see related terms such as ‘blood thinner’, ‘anti-inﬂammatory\ndrug’, or ‘clopidogrel’ (shares functionality with aspirin). These typically include hyponyms,\nhypernyms, or co-hyponyms. Besides terms by Wang et al. [17], we also examine the neigh-\nbors of most popular acronyms used in biomedical literature; we ﬁnd up to ﬁve closest terms\nto the acronym and the corresponding MeSH codes. We used two available algorithms for\nacronym extraction, the Schwartz and Hearst algorithm [31] and ALICE [32], and obtained\n331 most frequently used acronyms in the PubMed citations for this purpose. We note that\nfor multi-word terms, we simply take the average of constituent word embeddings before\nretrieving the closest words and concepts.\n2.4.2. Quantitative evaluations\nIntrinsic evaluations for word embeddings examine the quality of representativeness that\nis independent of downstream tasks. We use publicly available reference datasets for mea-\nsuring the relatedness between biomedical concepts. With the reference standards, we can\nevaluate the quality of vector representations for computing relatedness between biomedical\nterms compared to human judgments. Each instance within a dataset consists of a pair of\nbiomedical concepts and the corresponding relatedness score judged by human experts such\nas physicians and medical coders. Some of the datasets also provide corresponding UMLS\nconcept codes. The terms that occur in these datasets are more often seen in the biomedical\ndomains than in other ﬁelds. Table 4 enumerates the reference datasets we use, where the\nmiddle column indicates the number of concept pairs within each dataset.\nTable 4: Datasets of biomedical concept pairs for similarity/relatedness evaluations.\nDataset name (alias) Size Judged by\nUMNSRS-Sim (UMNS) [33] 566 medical residents\nUMNSRS-Rel (UMNR) [33] 587 medical residents\nMayoSRS (MAYO) [34] 101 physicians and coders\nMiniMayoSRS (MMY[P/C]) [35] 29 physicians and coders\nPedersen’s (PDS[P/C]) [35] 30 physicians\nHliaoutakis’ (HLTK) [36] 36 mostly physicians\nWe expand the instances by linking the concepts to corresponding MeSH codes. We\n12\nutilize the UTS (UMLS Terminology Services) API4 to ﬁnd the most similar MeSH codes to\nthe concepts. When available, we exploit the UMLS codes provided along with the datasets;\notherwise, we query by the concept name. We use the cosine vector similarity to measure the\nsemantic match between two concepts/terms. Here also, if the concept name is composed\nof multiple words, we take the mean vector of its constituent word representations. If the\nword is OOV (Out-of-Vocabulary), the[UNK] token vector learned in BERT-CRel training\nprocess is used. If[UNK] token is not available, for the fastText+ pre-trained embeddings, we\nassume the relatedness score of the pair to be0 as default. Finally, a ranked list of concept\npairs based on cosine scores is compared against the ground truth expert ranking using the\nSpearman’s rank correlation coeﬃcientρ.\nWe test the signiﬁcance of performance gains from our best method relative to the\nbest prior score on diﬀerent datasets. Given correlation coeﬃcients (ρ’s) are not normally\ndistributed and the actual number of examples used are diﬀerent, we use theFisher Z-\ntransformation and the one-tailed p-values for comparison [37]. We compute the normalized\nmeans,\nµ′\n1 = tanh−1(ρ1) = 1\n2 ln\n(1 + ρ1\n1 −ρ1\n)\n, µ ′\n2 = tanh−1(ρ2) = 1\n2 ln\n(1 + ρ2\n1 −ρ2\n)\nWe computez-score,\nz = µ′\n1 −µ′\n2\nS ∼N(0,1),\nwith the standard error of the diﬀerence between means wheren’s are the sample sizes:\nS =\n√\nS2\n1 + S2\n2 =\n√\n1\nn1 −3 + 1\nn2 −3.\nThe p-value is computed using this Z-score.\n3. Results and Discussion\n3.1. Qualitative evaluation\nWe ﬁrst discuss observations from the qualitative assessments conducted. Table 5 shows\nthe ﬁve most related terms to a given biomedical term across several available embeddings.\nSample query terms are in three groups: disease name, symptoms, and drug names. In\nthe table, the fastText+ column denotes the results obtained from the pre-trained static\nembeddings with the joint learning of word and concept embeddings (Section 2.3.1). The\nBERT-CRel column indicates the results obtained from the improved static embeddings by\nthe concept-relatedness classiﬁcation task with the BERT encoder model. We notice that\nboth of our approaches (fastText+ and BERT-CRel) surface a coherent set of words and\nconcepts related to the query terms. Also, corresponding MeSH codes returned allow us to\ninterpret input terms in an indirect but more precise way. For example, D015179 (Colorectal\nNeoplasms) exactly matches the query term “colon cancer” while other words are indicating\n4https://documentation.uts.nlm.nih.gov\n13\nTable 5: Five most similar terms to selected biomedical concepts trained from diﬀerent models and textual resources, MeSH names: (i) Diabetes\nMelitus (ii) Diabetes Mellitus, Type 2 (iii) Ulcer (iv) Peptic Ulcer (v) Stomach Neoplasms (vi) Colorectal Neoplasms (vii) neoplasms (viii) Dyspnea\n(ix) Pharyngeal Diseases (x) Opioid-Related Disorders (xi) Aspirin\nQuery term fastText+ (PubMed)BERT-CRel(PubMed) Wang et al.’s (EHR) Wang et al.’s (PMC) GloVe (Wiki+Giga) W2V (Google News)\ndiabetes D003920 i D003920i mellitus cardiovascular hypertension diabetics\nmellitus mellitus uncontrolled nonalcoholic obesity hypertension\nnondiabetes nondiabetes cholesterolemia obesity arthritis diabetic\ndiabetic D003924 ii dyslipidemia mellitus cancer diabetes_mellitus\nD003924ii diabetic melitis polycystic alzheimer heart_disease\npeptic ulcer disease D014456iii D014456iii scleroderma gastritis ulcers ichen_planus\nulcers D010437 iv duodenal alcoholism arthritis Candida_infection\nD010437iv ulcers crohn rheumatic diseases vaginal_yeast_infections\ngastroduodenitis D013274v gastroduodenal ischaemic diabetes oral_thrush\nulceration gastroduodenitis diverticular nephropathy stomach dermopathy\ncolon cancer colorectal D015179 vi breast breast breast breast\nD015179vi colorectal ovarian mcf prostate prostate\ncancers cancers prostate cancers cancers tumor\nD009369vii colorectum postmenopausally tumor_suppressing tumor pre_cancerous_lesion\ncolorectum D009369 vii caner downregulation liver cancerous_polyp\ndyspnea D004417 viii D004417viii palpitations sweats shortness dyspnoea\ndyspnoea dyspnoea orthopnea orthopnea breathlessness pruritus\nshortness shortness exertional breathlessness cyanosis nasopharyngitis\nbreathlessness breathlessness doe hypotension photophobia symptom_severity\ndyspnoeic dyspnoeic dyspnoea rhonchi faintness rhinorrhea\nsore throat pharyngitis pharyngitis scratchy runny shoulder soreness\nthroats D010608 ix thoat rhinorrhea stomach bruised\npharyngolaryngitis pharyngolaryngitis cough myalgia nose inﬂammed\ntonsillopharyngitis pharyngotonsillitis runny swab_fecal chest contusion\nrhinopharyngitis rhinopharyngitis thraot nose neck sore_triceps\nlow blood pressure pressures pressures readings dose because splattering_tombstones\nhemodynamics ﬂow pressue cardio_ankle result Zapping_nerves_helps\nsubpressure arterial presssure ncbav high pressue\narterial high bptru preload enough Marblehead_Swampscott_VNA\nnormotension hemodynamics systolically gr higher pill_Norvasc\nopioid opioids opioids opiate opioids analgesic opioids\nopiate opiate benzodiazepine nmda_receptor opiate opioid_analgesics\nnonopioid nonopioid opioids aﬀective_motivational opioids opioid_painkillers\nnonopioids morphine sedative naloxone_precipitated anti-inﬂammatory antipsychotics\nD009293x nonopioids polypharmacy hyperlocomotion analgesics tricyclic_antidepressants\naspirin D001241 xi D001241xi ecotrin chads ibuprofen dose_aspirin\nacetylsalicylic acetylsalicylic uncoated vasc tamoxifen ibuprofen\nnonaspirin nonaspirin nonenteric newer pills statins\naspirinate aspirinate eﬃent cha statins statin\naspirinated antiplatelet onk angina medication calcium_supplements\n14\nTable 6: Nearest neighbors of most common biomedical abbreviations in the BMET-CRel trained embeddings\nAcronyms Close to Word Close to Code\nMRI\n(MeSH: D008279\nName: Magnetic Resonance Imaging)\nimaging\nmris\nweighted\ntesla\nmagnetic\nD066235 (Fluorine-19 Magnetic Resonance Imaging)\nD038524 (Diﬀusion Magnetic Resonance Imaging)\nD000074269 (Resonance Frequency Analysis)\nD000081364 (Multiparametric Magnetic Resonance Imaging)\nD017352 (Echo-Planar Imaging)\nBMI\n(MeSH: D015992\nName: Body Mass Index)\noverweight\nwaist\ncircumference\nwhr\nD009765 (Obesity)\nD065927 (Waist-Height Ratio)\nD049629 (Waist-Hip Ratio)\nD049628 (Body Size)\nD064237 (Lipid Accumulation Product)\nD001823 (Body Composition)\nCT\n(MeSH: D014057\nName: Computed Tomography)\nscans\ntomographic\ncomputed\nscan\ntomography\nD014056 (Tomography, X-Ray)\nD055114 (X-Ray Microtomography)\nD000072078 (Positron Emission Tomography Computed Tomography)\nD055032 (Electron Microscope Tomography)\nD014055 (Tomography, Emission-Computed)\nNO\n(MeSH: D009569\nName: Nitric Oxide)\nsigniﬁcant\nany\ndid\nnot\nboth\nnitric\noxide\ninos\nnos\nD013481 (Superoxides)\nROS\n(MeSH: D017382\nName: Reactive Oxygen Species)\nD017382 (Reactive Oxygen Species)\noxidative\nh2o2\noxidant\nD013481 (Superoxides)\nros\noxidative\nh2o2\nD006861 (Hydrogen Peroxide)\nD013481 (Superoxides)\nPCR\n(MeSH: D016133\nName: Polymerase Chain Reaction)\npolymerase\nqpcr\nprimers\ntaqman\nrt\nD054458 (Ampliﬁed Fragment Length Polymorphism Analysis)\nD020180 (Heteroduplex Analysis)\nD022521 (Ligase Chain Reaction)\nD060885 (Multiplex Polymerase Chain Reaction)\nD024363 (Transcription Initiation Site)\nAD\n(MeSH: D000544\nAlzheimer Disease)\nD000544 (Alzheimer Disease)\nalzheimer\nalzheimers\nabeta\ndementias\nalzheimer\nalzheimers\nad\nabeta\nD003704 (Dementia)\n15\nrelevant words but may not be as speciﬁc (e.g., “cancers”). The returned words for the\nquery term “sore throat” also demonstrate better ability in ﬁnding related terms. We were\nable to retrieve speciﬁc related disease names such aspharyngitis, pharyngolaryngitis, and\nrhinopharyngitis. The more primitive methods do not produce terms that are as tightly\nlinked with the theme conveyed by query terms compared with our methods. Between\nour fastText+ and BERT-CRel rankings, there is a non-trivial overlap of terms, but the\nrelative order seems to have changed due to the ﬁne-tuning process. We see more examples\nwhere BERT-CRel ranks MeSH codes that precisely match the query term higher than the\nfastText+ ranking. Also, BERT-CRel appears to surface related terms that are not just\nmorphological variants of the query term. For example, for the “opioid” query, it returns\nmorphine, which is not returned in any other methods. However, other methods also seem\nto surface some interesting related terms such as “analgesics”, a broader term that refers to\npain relievers.\nTable 6 shows the mapping between some commonly used biomedical acronyms and\ntheir nearest terms; the second column lists terms that are close to the acronym, and the\nthird column contains terms close to the corresponding MeSH code. The results in the\nthird column show how the distributional representations of MeSH codes are aﬀected by\nthe training sources. As mentioned earlier, PubTator annotates biomedical concepts that\nonly belong to the following categories: gene, mutation, disease names, chemical substances,\nand species. Consequently, the MeSH codes for some acronyms (e.g., MRI, BMI, CT, PCR)\nhad to learn associated representations just from MeSH deﬁnitions and the BERT-CRel\nobjective; their nearest neighbors, hence, tend to be other MeSH codes. However, other\nacronyms with enough annotation examples in the PubTator dataset (e.g., NO, ROS, AD)\nmapped to more of the related regular words. Among top ﬁve matches for AD and its MeSH\ncode is “abeta” (stands for amyloid beta), the main component in plaques in brains of people\nwith Alzheimer’s disease.\n3.2. Intrinsic quantitative evaluation\nWe now focus on quantitative evaluations based on expert curated datasets in Table 4.\nMiniMayoSRS and Pedersen’s datasets are judged by two diﬀerent groups of experts: physi-\ncians and medical coders. We compare our model against several state-of-the-art methods\nacross all the reference datasets. Table 7 shows the results of our pre-trained embeddings\n(fastText+) and the ﬁne-tuned embeddings (BERT-CRel). The metric is Spearman’sρcom-\nparing methods’ rankings with human relevance scores. Before we delve into the scores, we\nnote that the correlation coeﬃcients may not be directly comparable in all cases. Most of\nthe previous studies evaluated the models on a subset of the original reference standards.\nWe specify the number of instances used in each evaluation in parentheses next to the score;\na score without the number of instances means that the evaluation used the full dataset.\nAs indicated in Section 2.4.2, we use all instances of all datasets in the evaluation; for\nany OOV term, we use a fallback mechanism that returns a score either using the[UNK]\nembedding or the default score 0. We believe this is a more robust way of evaluating\n16\nTable 7: Results of intrinsic evaluations measured with Spearman’s correlation coeﬃcient. Note, the number\nin parenthesis indicates the number of examples used for the evaluation (with the header row indicating the\ntotal number of instances in the original datasets). Scores without parentheses use the full set of instances.\n†indicates top scores from prior results and our best result used in computing the p-value. The ranking for\nthe word+MeSH rows is computed by the reciprocal rank fusion with the rankings generated by the “word”\nand “MeSH” embeddings.\nApproach UMNS\n(n=566)\nUMNR\n(n=587)\nMAYO\n(n=101)\nMMYP\n(n=29)\nMMYC\n(n=29)\nPDSP\n(n=30)\nPDSC\n(n=30)\nHLTK\n(n=36)\nWord2vec (baseline) 0.568 0.499 0.508 † 0.744 0.748 0.738 0.736 0.434\nWang et al. [17] 0.440 n/a 0.412 n/a n/a 0.632 n/a 0.482\nPark et al. [18] n/a n/a n/a n/a n/a 0.795 † n/a 0.633 †\nChiu et al. [38] 0.652 (459) 0.601 (561) n/a n/a n/a n/a n/a n/a\nZhang et al. [19] 0.657 (521) 0.617 (532) n/a n/a n/a n/a n/a n/a\nYu et al. [39, 20] 0.689 (526) 0.624 (543) n/a 0.696 (25) 0.665 (25) n/a n/a n/a\nHenry et al. [22] 0.693 (392) † 0.641 (418)† n/a 0.842 † 0.816† n/a n/a n/a\nfastText+ (word) 0.654 0.609 0.630 0.851 0.853 0.820 0.831 0.513\nfastText+ (MeSH) 0.648 0.568 0.608 0.739 0.701 0.612 0.612 0.846 †\nfastText+ (word+MeSH) 0.689 0.623 0.685 0.836 0.832 0.756 0.769 0.753\nBERT-CRel (word) 0.683 0.643 † 0.667 0.890 † 0.844 0.850 † 0.849† 0.537\nBERT-CRel (MeSH) 0.659 0.576 0.610 0.710 0.712 0.678 0.678 0.823\nBERT-CRel (word+MeSH) 0.708† 0.637 0.695 † 0.847 0.857 † 0.803 0.835 0.743\np-value (ours vs SoTA) 0.328 0.479 0.0186 0.475 0.310 0.264 0.126 0.0221\nmethods instead of selectively ignoring some instances5. All rows except those that involve\n“MeSH” intheﬁrstcolumnuseword-embeddingbasedrankings. RowsthatinvolveMeSHare\ncomparisons that directly compute cosine score with the MeSH code embedding generated\nby our method. Rows with “word+MeSH” modeling involve reciprocal rank fusion [40] of\nrankings generated by “word” and “MeSH” conﬁgurations in the previous two rows.\nDigging into the scores from Table 7, with very few exceptions, BERT-CRel correlates\nbetter with human judgments compared with fastText+ across datasets, and improves by\naround 2.5% inρ on average. The most comparable scores with previous eﬀorts are from\nthe third row from the end (BERT-CRel with “word” level comparison) given they are word-\nbased measures. This BERT-CRel conﬁguration wins outright for the UMNR dataset even\nwhen compared to methods that fuse rankings from word and concept level scores. It also is\nbetter than almost all other prior methods across all datasets even when they use selected\nsubsets from the full dataset. The p-values displayed in the last row for each column were\ncomputed use the dagger tagged scores (ours vs prior best). Except for MAYO and HLTK\ndatasets, our improvements were not statistically signiﬁcant. An important remark here is\nthe rankings (and associated correlation) are not directly comparable in the larger datasets\n(ﬁrst two columns of Table 7). As indicated earlier, correlation scores in top scoring prior\neﬀorts were generated on smaller datasets where some test term pairs were deliberately left\n5In our observation, this was mostly done by other eﬀorts when dealing with terms that are very rare,\nhence OOV, and hence cannot be readily compared for lack of a proper representation. To some extent, we\novercame OOV by using MeSH deﬁnitions in fastText+ and the concept pair relevance setup in BERT-CRel\n17\nout; nearly a third of the full dataset was ignored in the top scoring study for the larger\nUMNS and UMNR datasets. So even though the Fisher Z-transformation method we used\naccounts for varying sample sizes, due to the way the smaller samples were curated (by\nselectively, not randomly, eliminating certain rare term pairs), our results are more robust\nfor larger datasets despite these ﬁndings regarding statistical signiﬁcance.\n3.3. Extrinsic quantitative evaluation\nWe further investigate the eﬃcacy of the ﬁne-tuning process using a semi-supervised\nlearning method in a simple downstream NLP task setup. We use the same annotation\ndataset (i.e., PubTator) used for the joint learning method for the biomedical entity linking\n(EL) problem in this evaluation. The EL goal is to disambiguate the associations between\nmentions (entity spans) and the unique entity identiﬁers (MeSH codes). In this evaluation,\na MeSH code is represented by the mean vector of the constituent word embeddings for the\nNLM deﬁned entity name or more directly the MeSH code embedding. We rank all MeSH\ncodes given a mention phrase using cosine similarity score of the average embedding vector\nof the mention phrase and the corresponding average for MeSH code’s preferred name (rows\n1 and 3 of Table 8); if the similarity is computed using the vectors for MeSH codes, we obtain\nthe MeSH ranking. The word+MeSH ranking (rows 2 and 4 of Table 8) is the ranking based\non RRF fusion of word and MeSH based rankings. As the table shows the BERT-CRel\nﬁne-tuning consistently improves EL results.\nTable 8: Results of the entity linking task with the pre-trained and ﬁne-tuned static embeddings (Number\nof test examples: 21,505 mention-entity pairs)\nTop-1 accuracy Top-5 accuracy\nfastText+ (word) 0.357 0.512\nfastText+ (word+MeSH) 0.442 0.703\nBERT-CRel (word) 0.368 0.530\nBERT-CRel (word+MeSH) 0.474 0.741\nOur eﬀort provides the most robust evaluation by exhaustively considering all instances\nacross all well-known datasets developed for evaluating embeddings. Overall, we demon-\nstrate that jointly learning word and concept embeddings by leveraging deﬁnitional infor-\nmation for concepts provides better embeddings; further enhancing these embeddings by\nexploiting distributional correlations across concepts (obtained from MeSH co-occurrences\nand hierarchical links), through transformer-based classiﬁers, oﬀers more noticeable gains in\nembedding quality.\n3.4. Limitations and future directions\nOur work can be improved in a few directions that also indicate some of the limitations of\nthis eﬀort. Although our methods are novel and they helped us improve embedding quality,\nthe performance gains are clearly not spectacular; even when the gains are substantial for\nsmaller datasets, they were not statistically signiﬁcant. Training on much bigger citation\nsubsets both for fastText and the BERT-CRel ﬁne-tuning may result in further improve-\nments. Also, expanding beyond MeSH and considering other relevant vocabularies (e.g.,\n18\nSNOMED-CT, ICD-10) may help. For example, the University of Kentucky medical center\nhas over 15 million patient visits to its clinics over the past decade. Each patient visit is as-\nsigned a set of ICD-10 codes by a trained coder (just like MeSH terms assigned to a PubMed\narticle). In our prior work, we used correlations in these ICD-10 code sets to improve au-\ntomatic electronic medical record (EMR) coding eﬀorts [41]. BERT-CRel can use disease\nconcept pairs derived from EMRs to further ﬁne-tune word embeddings. Next, in terms of\nsampling PubMed citations either for fastText pre-training or for ﬁne-tuning with concept\npairs, we chose a random order. Though this works in general, for rare words and concepts,\nthis simple strategy may not lead to high quality embeddings for them. Although we ad-\ndressed this to some extent with MeSH deﬁnitional information, a more targeted heuristic\nthat over-samples citations that have been tagged with rare concepts may naturally lead to\nbetter representations for them and associated words that describe them.\n4. Conclusion\nIn this eﬀort, we proposed a method for training and improving static embeddings for\nboth words and domain-speciﬁc concepts using a neural model for the concept-relatedness\nclassiﬁcation task. To incorporate the relational information among biomedical concepts,\nwe utilize document metadata (i.e., MeSH assignments to the PubMed articles) in corpus\nand the hierarchical relationships of the concepts deﬁned in a controlled vocabulary (i.e.,\nMeSH hierarchy structures). Our approach achieved the best performances across several\nbenchmarks. Qualitative observations indicate that our methods may be able to nudge\nembeddings to capture more precise connections among biomedical terms.\nOur proposed method for training and improving static embeddings can be utilized in\nmany BioNLP tasks. The use of joint word/concept embeddings can potentially beneﬁt\nneural models that need mutual retrievability between multiple embeddings spaces. In one\nof our recent studies, we leveraged embeddings generated with these methods in a neural\ntext summarization model for information retrieval [42]. Exploiting the joint embeddings of\nwords and MeSH codes, we were able to summarize a document into a sequence of keywords\nusing either regular English words or MeSH codes that are then compared with query words\nand codes. We will continue to explore applications of these embeddings in other future\napplications in knowledge discovery and information retrieval. Other researchers can use\nthem in their own tasks by downloading them from our publicly available repository:https:\n//github.com/bionlproc/BERT-CRel-Embeddings\nAcknowledgements\nResearch reported in this publication was supported by the National Library of Medicine\nof the U.S. National Institutes of Health under Award Number R01LM013240. The content\nis solely the responsibility of the authors and does not necessarily represent the oﬃcial views\nof the National Institutes of Health.\nReferences\n[1] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural probabilistic language\nmodel,”Journal of machine learning research, vol. 3, no. Feb, pp. 1137–1155, 2003.\n19\n[2] R. Collobert and J. Weston, “A uniﬁed architecture for natural language processing:\nDeep neural networks with multitask learning,” inProceedings of the 25th international\nconference on Machine learning, pp. 160–167, 2008.\n[3] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Eﬃcient estimation of word represen-\ntations in vector space,”arXiv preprint arXiv:1301.3781, 2013.\n[4] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word vectors with\nsubword information,”Transactions of the Association for Computational Linguistics,\nvol. 5, pp. 135–146, 2017.\n[5] J. Yu, X. Jian, H. Xin, and Y. Song, “Joint embeddings of chinese words, characters,\nand ﬁne-grained subcharacter components,” inProceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing, pp. 286–291, 2017.\n[6] Z. S. Harris, “Distributional structure,”Word, vol. 10, no. 2-3, pp. 146–162, 1954.\n[7] J. Pennington, R. Socher, and C. Manning, “Glove: Global vectors for word representa-\ntion,” inProceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP), pp. 1532–1543, 2014.\n[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidi-\nrectional transformers for language understanding,” inProceedings of the 2019 Confer-\nence of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186,\n2019.\n[9] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\nand V. Stoyanov, “Roberta: A robustly optimized bert pretraining approach,”arXiv\npreprint arXiv:1907.11692, 2019.\n[10] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer,\n“Deep contextualized word representations,” inProceedings of NAACL-HLT, pp. 2227–\n2237, 2018.\n[11] J.HowardandS.Ruder, “Universallanguagemodelﬁne-tuningfortextclassiﬁcation,” in\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 328–339, 2018.\n[12] A. Rogers, O. Kovaleva, and A. Rumshisky, “A primer in Bertology: What we know\nabout how BERT works,”arXiv preprint arXiv:2002.12327, 2020.\n[13] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a distilled version of\nBERT: smaller, faster, cheaper and lighter,”arXiv preprint arXiv:1910.01108, 2019.\n[14] P. Bartolik, “‘edge nlp’ is about doing more with less.”https://www.iotworldtoday.\ncom/2021/01/19/edge-nlp-is-about-doing-more-with-less/ , 2021.\n20\n[15] R. Kavuluru and D. Harris, “A knowledge-based approach to syntactic disambiguation\nof biomedical noun compounds,” inProceedings of COLING 2012: Posters, pp. 559–568,\n2012.\n[16] J. Eisenschlos, S. Ruder, P. Czapla, M. Kadras, S. Gugger, and J. Howard, “MultiFiT:\nEﬃcient multi-lingual language model ﬁne-tuning,” inProceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5706–5711,\n2019.\n[17] Y. Wang, S. Liu, N. Afzal, M. Rastegar-Mojarad, L. Wang, F. Shen, P. Kingsbury,\nand H. Liu, “A comparison of word embeddings for the biomedical natural language\nprocessing,”Journal of biomedical informatics, vol. 87, pp. 12–20, 2018.\n[18] J. Park, K. Kim, W. Hwang, and D. Lee, “Concept embedding to measure semantic\nrelatedness for biomedical information ontologies,”Journal of biomedical informatics,\nvol. 94, p. 103182, 2019.\n[19] Y. Zhang, Q. Chen, Z. Yang, H. Lin, and Z. Lu, “BioWordVec, improving biomedical\nword embeddings with subword information and mesh,”Scientiﬁc data, vol. 6, no. 1,\npp. 1–9, 2019.\n[20] Z. Yu, B. C. Wallace, T. Johnson, and T. Cohen, “Retroﬁtting concept vector represen-\ntations of medical concepts to improve estimates of semantic similarity and relatedness,”\nStudies in health technology and informatics, vol. 245, p. 657, 2017.\n[21] M. Faruqui, J. Dodge, S. K. Jauhar, C. Dyer, E. Hovy, and N. A. Smith, “Retroﬁtting\nword vectors to semantic lexicons,” inProceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pp. 1606–1615, 2015.\n[22] S. Henry, A. McQuilkin, and B. McInnes, “Association measures for estimating seman-\ntic similarity and relatedness between biomedical concepts.,”Artiﬁcial intelligence in\nmedicine, vol. 93, p. 1, 2019.\n[23] X. Cai, J. Gao, K. Y. Ngiam, B. C. Ooi, Y. Zhang, and X. Yuan, “Medical concept\nembedding with time-aware attention,” inProceedings of the 27th International Joint\nConference on Artiﬁcial Intelligence, pp. 3984–3990, 2018.\n[24] E. Choi, M. T. Bahadori, E. Searles, C. Coﬀey, M. Thompson, J. Bost, J. Tejedor-Sojo,\nand J. Sun, “Multi-layer representation learning for medical concepts,” inProceedings of\nthe 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, pp. 1495–1504, ACM, 2016.\n[25] L. De Vine, G. Zuccon, B. Koopman, L. Sitbon, and P. Bruza, “Medical semantic\nsimilarity with a neural language model,” inProceedings of the 23rd ACM international\nconference on conference on information and knowledge management, pp. 1819–1822,\nACM, 2014.\n21\n[26] A. Sabbir, A. Jimeno-Yepes, and R. Kavuluru, “Knowledge-based biomedical word sense\ndisambiguationwithneuralconceptembeddings,” in 2017 IEEE 17th International Con-\nference on Bioinformatics and Bioengineering (BIBE), pp. 163–170, IEEE, 2017.\n[27] C.-H. Wei, A. Allot, R. Leaman, and Z. Lu, “PubTator central: automated concept\nannotation for biomedical full text articles,”Nucleic acids research, vol. 47, no. W1,\npp. W587–W593, 2019.\n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and\nI. Polosukhin, “Attention is all you need,”Advances in neural information processing\nsystems, vol. 30, pp. 5998–6008, 2017.\n[29] L. N. Smith, “Cyclical learning rates for training neural networks,” in2017 IEEE Winter\nConference on Applications of Computer Vision (WACV), pp. 464–472, IEEE, 2017.\n[30] B. Felbo, A. Mislove, A. Søgaard, I. Rahwan, and S. Lehmann, “Using millions of\nemoji occurrences to learn any-domain representations for detecting sentiment, emotion\nand sarcasm,” inProceedings of the 2017 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 1615–1625, 2017.\n[31] A. S. Schwartz and M. A. Hearst, “A simple algorithm for identifying abbreviation\ndeﬁnitions in biomedical text,” inBiocomputing, pp. 451–462, World Scientiﬁc, 200.\n[32] H. Ao and T. Takagi, “ALICE: an algorithm to extract abbreviations from medline,”\nJournal of the American Medical Informatics Association, vol. 12, no. 5, pp. 576–586,\n2005.\n[33] S. Pakhomov, B. McInnes, T. Adam, Y. Liu, T. Pedersen, and G. B. Melton, “Se-\nmantic similarity and relatedness between clinical terms: an experimental study,” in\nAMIA annual symposium proceedings, vol. 2010, p. 572, American Medical Informatics\nAssociation, 2010.\n[34] S. Pakhomov, “Semantic relatedness and similarity reference standards for medical\nterms.”https://bit.ly/2WxjiR8, 2018.\n[35] T. Pedersen, S. V. Pakhomov, S. Patwardhan, and C. G. Chute, “Measures of semantic\nsimilarity and relatedness in the biomedical domain,”Journal of biomedical informatics,\nvol. 40, no. 3, pp. 288–299, 2007.\n[36] A. Hliaoutakis, “Semantic similarity measures in mesh ontology and their application\nto information retrieval on medline.”https://bit.ly/3atGGHr, 2005.\n[37] R. A. Fisher, “Frequency distribution of the values of the correlation coeﬃcient in sam-\nples from an indeﬁnitely large population,”Biometrika, vol. 10, no. 4, pp. 507–521,\n1915.\n[38] B. Chiu, G. Crichton, A. Korhonen, and S. Pyysalo, “How to train good word embed-\ndings for biomedical nlp,” inProceedings of the 15th workshop on biomedical natural\nlanguage processing, pp. 166–174, 2016.\n22\n[39] Z. Yu, T. Cohen, E. V. Bernstam, and B. C. Wallace, “Retroﬁtting word vectors of\nMeSH terms to improve semantic similarity measures,”EMNLP 2016, p. 43, 2016.\n[40] G. V. Cormack, C. L. Clarke, and S. Buettcher, “Reciprocal rank fusion outperforms\ncondorcet and individual rank learning methods,” inProceedings of the 32nd interna-\ntional ACM SIGIR conference on Research and development in information retrieval,\npp. 758–759, 2009.\n[41] R. Kavuluru, A. Rios, and Y. Lu, “An empirical evaluation of supervised learning ap-\nproaches in assigning diagnosis codes to electronic medical records,”Artiﬁcial intelli-\ngence in medicine, vol. 65, no. 2, pp. 155–166, 2015.\n[42] J. Noh and R. Kavuluru, “Literature retrieval for precision medicine with neural match-\ning and faceted summarization,” inFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pp. 3389–3399, 2020.\n23"
}