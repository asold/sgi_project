{
  "title": "3D Morphable Models as Spatial Transformer Networks",
  "url": "https://openalex.org/W2748448865",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A4287465226",
      "name": "Bas, Anil",
      "affiliations": [
        "University of York"
      ]
    },
    {
      "id": "https://openalex.org/A4302129710",
      "name": "Huber, Patrik",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A4282719234",
      "name": "Smith, William A. P.",
      "affiliations": [
        "University of York"
      ]
    },
    {
      "id": "https://openalex.org/A2288933954",
      "name": "Awais Muhammad",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A2278881658",
      "name": "Kittler, Josef",
      "affiliations": [
        "University of Surrey"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964014798",
    "https://openalex.org/W2952074561",
    "https://openalex.org/W6729687604",
    "https://openalex.org/W1963882359",
    "https://openalex.org/W2964145484",
    "https://openalex.org/W2489710028",
    "https://openalex.org/W6726141277",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W2465108587",
    "https://openalex.org/W2012885984",
    "https://openalex.org/W6700903540",
    "https://openalex.org/W2107037917",
    "https://openalex.org/W2237250383",
    "https://openalex.org/W6681342084",
    "https://openalex.org/W2963202462",
    "https://openalex.org/W6723856882",
    "https://openalex.org/W2584229793",
    "https://openalex.org/W2017107803",
    "https://openalex.org/W2027661541",
    "https://openalex.org/W2111501452",
    "https://openalex.org/W4243755239",
    "https://openalex.org/W2464650832",
    "https://openalex.org/W2555510177",
    "https://openalex.org/W2519131448",
    "https://openalex.org/W2547473593",
    "https://openalex.org/W2155211928",
    "https://openalex.org/W2524657946",
    "https://openalex.org/W2145287260",
    "https://openalex.org/W4298236736",
    "https://openalex.org/W2345945060",
    "https://openalex.org/W2951863354",
    "https://openalex.org/W2952808330",
    "https://openalex.org/W4385490328",
    "https://openalex.org/W2599226450",
    "https://openalex.org/W2604672468",
    "https://openalex.org/W2950701417",
    "https://openalex.org/W2602708722",
    "https://openalex.org/W2963278718",
    "https://openalex.org/W2962686123",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2325939864",
    "https://openalex.org/W2560539317",
    "https://openalex.org/W2737357585",
    "https://openalex.org/W2747233880",
    "https://openalex.org/W2962780596",
    "https://openalex.org/W4297915247",
    "https://openalex.org/W2963451207",
    "https://openalex.org/W2953066166",
    "https://openalex.org/W2551540143",
    "https://openalex.org/W2495387757",
    "https://openalex.org/W2341600683",
    "https://openalex.org/W2604240677",
    "https://openalex.org/W4299930316"
  ],
  "abstract": "In this paper, we show how a 3D Morphable Model (i.e. a statistical model of\\nthe 3D shape of a class of objects such as faces) can be used to spatially\\ntransform input data as a module (a 3DMM-STN) within a convolutional neural\\nnetwork. This is an extension of the original spatial transformer network in\\nthat we are able to interpret and normalise 3D pose changes and\\nself-occlusions. The trained localisation part of the network is independently\\nuseful since it learns to fit a 3D morphable model to a single image. We show\\nthat the localiser can be trained using only simple geometric loss functions on\\na relatively small dataset yet is able to perform robust normalisation on\\nhighly uncontrolled images including occlusion, self-occlusion and large pose\\nchanges.\\n",
  "full_text": "3D Morphable Models as Spatial Transformer Networks\nAnil Bas*, Patrik Huber†, William A. P. Smith*, Muhammad Awais†, Josef Kittler†\n*Department of Computer Science, University of York, UK\n†Centre for Vision, Speech and Signal Processing, University of Surrey, UK\n{ab1792,william.smith}@york.ac.uk, {p.huber,m.a.rana,j.kittler}@surrey.ac.uk\nAbstract\nIn this paper, we show how a 3D Morphable Model\n(i.e. a statistical model of the 3D shape of a class of ob-\njects such as faces) can be used to spatially transform input\ndata as a module (a 3DMM-STN) within a convolutional\nneural network. This is an extension of the original spa-\ntial transformer network in that we are able to interpret\nand normalise 3D pose changes and self-occlusions. The\ntrained localisation part of the network is independently\nuseful since it learns to ﬁt a 3D morphable model to a single\nimage. We show that the localiser can be trained using only\nsimple geometric loss functions on a relatively small dataset\nyet is able to perform robust normalisation on highly uncon-\ntrolled images including occlusion, self-occlusion and large\npose changes.\n1. Introduction\nConvolutional neural networks (CNNs) are usually\ntrained with such large amounts of data that they can learn\ninvariance to scale, translation, in-plane rotation and, to a\ncertain degree, out-of-plane rotations, without using any\nexplicit geometric transformation model. However, most\nnetworks do require a rough bounding box estimate as in-\nput and don’t work for larger variations. Recently, Jader-\nberg et al. [14] proposed the Spatial Transformer Network\n(STN) - a module that can be incorporated into a neural net-\nwork architecture, giving the network the ability to explic-\nitly account for the effects of pose and nonrigid deforma-\ntions (which we refer to simply as “pose”). An STN explic-\nitly estimates pose and then resamples a speciﬁc part of the\ninput image to a ﬁxed-size output image. It is thus able to\nwork on inputs with larger translation and pose variation in\ngeneral, since it can explicitly compensate for it, and feed a\ntransformed region of interest to the subsequent neural net-\nwork layers. By exploiting and “hard-coding” knowledge\nof geometric transformation, the amount of training data\nand the required complexity of the network can be vastly\nreduced.\nIn this paper, we show how to use a 3D morphable\nmodel as a spatial transformer network (we refer to this as\na 3DMM-STN). In this setting, the locations in the input\nimage that are resampled are determined by the 2D projec-\ntion of a 3D deformable mesh. Hence, our 3DMM-STN\nestimates both 3D shape and pose. This allows us to ex-\nplicitly estimate and account for 3D rotations as well as self\nocclusions. The output of our 3DMM-STN is a resampled\nimage in a ﬂattened 2D texture space in which the images\nare in dense, pixel-wise correspondence. Hence, this output\ncan be fed to subsequent CNN layers for further process-\ning. We focus on face images and use a 3D morphable face\nmodel [4, 19], though our idea is general and could be ap-\nplied to any object for which a statistical 3D shape model\nis available (though note that the loss functions proposed in\nSections 3.1 and 3.2 do assume that the object is bilaterally\nsymmetric). We release source code for our 3DMM-STN in\nthe form of new layers for the MatConvNet toolbox [30]1.\n1.1. Related work\nIn a lot of applications, the process of pose normalisa-\ntion and object recognition are disjoint. For example, in the\nbreakthrough deep learning face recognition paper Deep-\nFace, Taigman et al. [27] use a 3D mean face as preprocess-\ning, before feeding the pose-normalised image to a CNN.\nSpatial transformers The original STN [14] aimed to\ncombine these two processes into a single network that is\ntrainable end to end. The localiser network estimated a 2D\nafﬁne transformation that was applied to the regular output\ngrid meaning the network could only learn a fairly restricted\nspace of transformations. Jaderberg et al. [14] also proposed\nthe concept of a 3D transformer, which takes 3D voxel data\nas input, applies 3D rotation and translation, and outputs a\n2D projection of the transformed data. Working with 3D\n(volumetric data) removes the need to model occlusion or\ncamera projection parameters. In contrast, we work with\n1The source code is available at https://github.com/\nanilbas/3DMMasSTN.\narXiv:1708.07199v1  [cs.CV]  23 Aug 2017\nregular 2D input and output images but transform them via\na 3D model.\nA number of subsequent works were inspired by the\noriginal STN. Yan et al. [31] use an encoder-decoder ar-\nchitecture in which the encoder estimates a 3D volumetric\nshape from an image and is trained by combining with a de-\ncoder which uses a perspective transformer network to com-\npute a 2D silhouette loss. Handa et al. [11] present the gvnn\n(Geometric Vision with Neural Networks) toolbox that, like\nin this paper, has layers that explicitly implement 3D geo-\nmetric transformations. However, their goal is very differ-\nent to ours. Rather than learning to ﬁt a statistical shape\nmodel, they seek to use 3D transformations in low level vi-\nsion tasks such as relative pose estimation. Chen et al. [6]\nuse a spatial transformer that applies a 2D similarity trans-\nform as part of an end to end network for face detection.\nHenriques and Vedaldi [12] apply a spatial warp prior to\nconvolutions such that the convolution result is invariant to\na class of two-parameter spatial transformations. Like us,\nYu et al. [32] incorporate a parametric shape model, though\ntheir basis is 2D (and trainable), models only sparse shape\nand combines pose and shape into a single basis. They use a\nsecond network to locally reﬁne position estimates and train\nend to end to perform landmark localisation. Bhagavatula\net al. [3] ﬁt a generic 3D face model and estimate face land-\nmarks, before warping the projected face model to better ﬁt\nthe landmarks. They estimate 2D landmarks in a 3D-aware\nfashion, though they require known landmarks for training.\nAnalysis-by-synthesis Our localiser learns to ﬁt a 3DMM\nto a single image. This task has traditionally been posed\nas a problem of analysis-by-synthesis and solved by opti-\nmisation. The original method [4] used stochastic gradi-\nent descent to minimise an appearance error, regularised by\nstatistical priors. Subsequent work used a more complex\nfeature-based objective function [23] and the state-of-the-\nart method uses Markov Chain Monte Carlo for probabilis-\ntic image interpretation [24].\nSupervised CNN regression Analysis-by-synthesis ap-\nproaches are computationally expensive, prone to conver-\ngence on local minima and fragile when applied to in-the-\nwild images. For this reason, there has been considerable\nrecent interest in using CNNs to directly regress 3DMM pa-\nrameters from images. The majority of such work is based\non supervised learning. Jourabloo and Liu [15] ﬁt a 3DMM\nto detected landmarks and then train a CNN to directly\nregress the ﬁtted pose and shape parameters. Tr˜an et al. [29]\nuse a recent multi-image 3DMM ﬁtting algorithm [20] to\nobtain pooled 3DMM shape and texture parameters (i.e. the\nsame parameters for all images of the same subject). They\nthen train a CNN to directly regress these parameters from\na single image. They do not estimate pose and hence do\nnot compute an explicit correspondence between the model\nand image. Kim et al. [16] go further by also regressing illu-\nmination parameters (effectively performing inverse render-\ning) though they train on synthetic, rendered images (using\na breeding process to increase diversity). They estimate a\n3D rotation but rely on precisely cropped input images such\nthat scale and translation is implicit. Richardson et al. [21]\nalso train on synthetic data though they use an iteratively\napplied network architecture and a shape-from-shading re-\nﬁnement step to improve the geometry. Jackson et al. [13]\nregress shape directly using a volumetric representation.\nThe DenseReg [10] approach uses fully convolutional\nnetworks to directly compute dense correspondence be-\ntween a 3D model and a 2D image. The network does not\nexplicitly estimate or model 3D pose or shape (though these\nare implied by the correspondence) and is trained by using\nmanually annotated 2D landmarks to warp a 3D template\nonto the training images (providing the supervision). Sela\net al. [25] also use a fully convolutional network to pre-\ndict correspondence and also depth. They then merge the\nmodel-based and data-driven geometries for improved qual-\nity.\nThe weakness of all of these supervised approaches is\nthat they require labelled training data (i.e. images with ﬁt-\nted morphable model parameters). If the images are real\nworld images then the parameters must come from an ex-\nisting ﬁtting algorithm in which case the best the CNN can\ndo is learn to replicate the performance of an existing algo-\nrithm. If the images are synthetic with known ground truth\nparameters then the performance of the CNN on real world\ninput is limited by the realism and variability present in the\nsynthetic images. Alternatively, we must rely on 3D super-\nvision provided by multiview or RGBD images, in which\ncase the available training data is vastly reduced.\nUnsupervised CNN regression Richardson et al. [22]\ntake a step towards removing the need for labels by pre-\nsenting a semi-supervised approach. They still rely on su-\npervised training for learning 3DMM parameter regression\nbut then reﬁne the coarse 3DMM geometry using a second\nnetwork that is trained in an unsupervised manner. Very re-\ncently, Tewari et al. [28] presented MoFA, a completely un-\nsupervised approach for training a CNN to regress 3DMM\nparameters, pose and illumination using an autoencoder ar-\nchitecture. The regression is done by the encoder CNN. The\ndecoder then uses a hand-crafted differentiable renderer to\nsynthesise an image. The unsupervised loss is the error be-\ntween the rendered image and the input, with convergence\naided by losses for priors and landmarks. Note that the de-\ncoder is exactly equivalent to the differentiable cost function\nused in classical analysis-by-synthesis approaches. Presum-\nably, the issues caused by the non-convexity of this cost\nfunction are reduced in a CNN setting since the gradient\n2\nLocaliser\n(VGG) θ\n⊙Grid \nGenerator\nBilinear \nSampler\nVisibility\nMask\nI\nW\nM\nY''\nV\nX'\nFigure 1. Overview of the 3DMM-STN. The localiser predicts\n3DMM shape parameters and pose. The grid generator projects\nthe 3D geometry to 2D. The bilinear sampler resamples the input\nimage to a regular output grid which is then masked by an occlu-\nsion mask computed from the estimated 3D geometry.\nis averaged over many images.\nWhile the ability of [28] to learn from unlabelled data is\nimpressive, there are a number of limitations. The complex-\nity required to enable the hand-crafted decoder to produce\nphotorealistic images of any face under arbitrary real world\nillumination, captured by a camera with arbitrary geomet-\nric and photometric properties, is huge. Arguably, this has\nnot yet been achieved in computer graphics. Moreover, the\n3DMM texture should only capture intrinsic appearance pa-\nrameters such as diffuse and specular albedo (or even spec-\ntral quantities to ensure independence from the camera and\nlighting). Such a model is not currently available.\n1.2. Contributions\nIn this paper we propose a purely geometric approach in\nwhich only the shape component of a 3DMM is used to geo-\nmetrically normalise an image. Unlike [10,13,15,16,21,25,\n29], our method can be trained in an unsupervised fashion,\nand thus does not depend on synthetic training data or the\nﬁtting results of an existing algorithm. In contrast to [28],\nwe avoid the complexity and potential fragility of having\nto model illumination and reﬂectance parameters. More-\nover, our 3DMM-STN can form part of a larger network\nthat performs a face processing task and is trained end to\nend. Finally, in contrast to all previous 3DMM ﬁtting net-\nworks, the output of our 3DMM-STN is a 2D resampling of\nthe original image which contains all of the high frequency,\ndiscriminating detail in a face rather than a model-based re-\nconstruction which only captures the gross, low frequency\naspects of appearance that can be explained by a 3DMM.\n2. 3DMM-STN\nOur proposed 3DMM-STN has the same components\nas a conventional STN, however each component must be\nmodiﬁed to incorporate the statistical shape model, 3D\ntransformations and projection and self-occlusion. In this\nsection we describe each component of a 3DMM-STN and\nthe layers that are required to construct it. We show an\noverview of our architecture in Figure 1.\n2.1. Localiser network\nThe localiser network is a CNN that takes an image as\ninput and regresses the pose and shape parameters,θ, of the\nface in the image. Speciﬁcally, we predict the following\nvector of parameters:\nθ= (r,t,logs  \npose\n, α\nshape\n). (1)\nHere, t ∈R2 is a 2D translation, r ∈R3 is an axis-angle\nrepresentation of a 3D rotation with rotation angle ∥r∥and\naxis r/∥r∥. Since scale must be positive, we estimate log\nscale and later pass this through an exponentiation layer,\nensuring that the estimated scale is positive. The shape pa-\nrameters α ∈RD are the principal component weights used\nto reconstruct the shape.\nFor our localiser network, we use the pretrained VGG-\nFaces [18] architecture, delete the classiﬁcation layer and\nadd a new fully connected layer with 6 +D outputs. The\nweights for the new layer are randomly initialised but scaled\nso that the elements of the axis-angle vector are in the range\n[−π,π] for typical inputs. The whole localiser is then ﬁne-\ntuned as part of the subsequent training.\n2.2. Grid generator network\nIn contrast to a conventional STN, the warped sampling\ngrid is not obtained by applying a global transformation to\nthe regular output grid. Instead, we apply a 3D transfor-\nmation and projection to a 3D mesh that comes from the\nmorphable model. The intensities sampled from the source\nimage are then assigned to the corresponding points in a\nﬂattened 2D grid.\nFor this reason, the grid generator network in a 3DMM-\nSTN is more complex than in a conventional STN, although\nwe emphasise that it remains differentiable and hence suit-\nable for use in end to end training. The sample points in our\ngrid generator are determined by the transformation param-\neters θ estimated by the localiser network. Our grid gen-\nerator combines a linear statistical model with a scaled or-\nthographic projection as shown in Figure 2. Note that we\ncould alternatively use a perspective projection (modifying\nthe localiser to predict a 3D translation as well as camera\nparameters such as focal length). However, recent results\nshow that interpreting face shape under perspective is am-\nbiguous [2, 26] and so we use the more restrictive ortho-\ngraphic model here.\nWe now describe the transformation applied by each\nlayer in the grid generator and provide derivatives.\n3\nInput:θ= (r,t,logs,α)\nexprtoR\nRotate3DMM Project Scale Translate\nr logs\nα\nR s\nX X′ Y Y′ Y′′\nFigure 2. The grid generator network within a 3DMM-STN.\n3D morphable model layer The 3D morphable model\nlayer generates a shape X ∈R3×N comprising N 3D ver-\ntices by taking a linear combination ofDbasis shapes (prin-\ncipal components) stored in the matrix P ∈R3N×D and\nthe mean shape µ ∈R3N according to shape parameters\nα ∈RD:\nX(α)i,j = x(α)3(j−1)+i, i ∈[1,3],j ∈[1,N],\nwhere\nx(α) =Pα + µ\nand the derivatives are given by:\n∂x\n∂α = P, ∂Xi,j\n∂αk\n= P3(j−1)+i,k.\nNote that such a linear model is exactly equivalent to a fully\nconnected layer (and hence a special case of a convolutional\nlayer) with ﬁxed weights and biases. This is not at all sur-\nprising since a linear model is exactly what is implemented\nby a single layer linear decoder. In this interpretation, the\nshape parameters play the role of the input map, the prin-\ncipal components the role of weights and the mean shape\nthe role of biases. This means that this layer can be im-\nplemented using an existing implementation of a convolu-\ntion layer and also, following our later suggestion for future\nwork, that the model could itself be made trainable simply\nby having non-zero learning rate for the convolution layer.\nIn our network, we use some of the principal compo-\nnents to represent shape variation due to identity and the\nremainder to represent deformation due to expression. We\nassume that expressions are additive and we can thus com-\nbine the two into a single linear model. Note that the shape\nparameters relating to identity may contain information that\nis useful for recognition, so these could be incorporated into\na descriptor in a recognition network after the STN.\nAxis-angle to rotation matrix layer This layer converts\nan axis-angle representation of a rotation, r ∈R3, into a\nrotation matrix:\nR(r) = cosθI + sinθ\n[¯ r]\n×+ (1−cos θ)¯ r¯ rT ,\nwhere θ= ∥r∥and ¯ r= r/∥r∥and\n[a]\n×=\n\n\n0 −a3 a2\na3 0 −a1\n−a2 a1 0\n\n\nis the cross product matrix. The derivatives are given by [8]:\n∂R\n∂ri\n=\n{\n[ei]× if r = 0\nri[r]×+[r×(I−R(r))ei]×\n∥r∥2 R otherwise\nwhere ei is the ith vector of the standard basis in R3.\n3D rotation layer The rotation layer takes as input a ro-\ntation matrix R and N 3D points X ∈R3×N and applies\nthe rotation:\nX′(R,X) =RX\n∂X′\ni,j\n∂Ri,k\n= Xk,j, ∂X′\ni,j\n∂Xk,j\n= Ri,k, i,k ∈[1,3],j ∈[1,N].\nOrthographic projection layer The orthographic projec-\ntion layer takes as input a set of N 3D points X′ ∈R3×N\nand outputs N 2D points Y ∈R2×N by applying an ortho-\ngraphic projection along the z axis:\nY(X′) =PX′, P =\n[1 0 0\n0 1 0\n]\n,\n∂Yi,j\n∂X′\ni,j\n= 1, i ∈[1,2],j ∈[1,N].\nScaling The log scale estimated by the localiser is ﬁrst\ntransformed to scale by an exponentiation layer:\ns(logs) = exp(logs), ∂s\n∂logs = exp(logs).\nThen, the 2D points Y ∈R2×N are scaled:\nY′(s,Y) =sY, ∂Y′\ni,j\n∂s = Yi,j, ∂Y′\ni,j\n∂Yi,j\n= s\nTranslation Finally, the 2D sample points are generated\nby adding a 2D translation t ∈R2 to each of the scaled\npoints:\nY′′(t,Y′) =Y′+ 1N ⊗t, ∂Y′′\ni,j\n∂ti\n= 1, ∂Y′′\ni,j\n∂Y′\ni,j\n= 1,\nwhere 1N is the row vector of lengthNcontaining ones and\n⊗is the Kronecker product.\n4\nFigure 3. The output grid of our 3DMM-STN: a Tutte embedding\nof the mean shape of the Basel Face Model. On the left we show a\nvisualisation using the mean texture (though note that our 3DMM-\nSTN does not use a texture model). On the right we show the mean\nshape as a geometry image [9].\n2.3. Sampling\nIn the original STN, the sampler component used bilin-\near sampling to sample values from the input image and\ntransform them to an output grid. We make a number of\nmodiﬁcations. First, the output grid is a texture space ﬂat-\ntening of the 3DMM mesh. Second, the bilinear sampler\nlayer will incorrectly sample parts of the face onto vertices\nthat are self-occluded so we introduce additional layers that\ncalculate which vertices are occluded and mask the sampled\nimage appropriately.\nOutput grid The purpose of an STN is to transform an\ninput image into a canonical, pose-normalised view. In the\ncontext of a 3D model, one could imagine a number of anal-\nogous ways that an input image could be normalised. For\nexample, the output of the STN could be a rendering of the\nmean face shape in a frontal pose with the sampled texture\non the mesh. Instead, we choose to output sampled textures\nin a 2D embedding obtained by ﬂattening the mean shape of\nthe 3DMM. This ensures that the output image is approxi-\nmately area uniform with respect to the mean shape and also\nthat the whole output image contains face information.\nSpeciﬁcally, we compute a Tutte embedding [7] using\nconformal Laplacian weights and with the mesh boundary\nmapped to a square. To ensure a symmetric embedding we\nmap the symmetry line to the symmetry line of the square,\nﬂatten only one side of the mesh and obtain the ﬂatten-\ning of the other half by reﬂection. We show a visualisa-\ntion of our embedding using the mean texture in Figure\n3. In order that the output warped image produces a regu-\nlarly sampled image, we regularly re-sample (i.e. re-mesh)\nthe 3DMM (mean and principal components) over a uni-\nform grid of size H′×W′ in this ﬂattened space. This\neffectively makes our 3DMM a deformable geometry im-\nage [9]. The re-sampled 3DMM that we use in our STN\ntherefore has N = H′W′vertices and each vertex ihas an\nassociated UV coordinate (xt\ni,yt\ni). The corresponding sam-\nple coordinate produced by the grid generator is given by\n(xs\ni ,ys\ni ) = (Y′′\n1,i,Y ′′\n2,i).\nBilinear sampling We use bilinear sampling, exactly as\nin the original STN such that the re-sampled image Vc\ni at\nlocation (xt\ni,yt\ni) in colour channel cis given by:\nVc\ni =\nH∑\nj=1\nW∑\nk=1\nIc\njk max(0,1−|xs\ni −k|) max(0,1−|ys\ni −j|)\nwhere Ic\njk is the value in the input image at pixel (j,k) in\ncolour channel c. I has height H and width W. This bilin-\near sampling is differentiable (see [14] for derivatives) and\nso the loss can be backpropagated through the sampler and\nback into the grid generator.\nSelf-occlusions Since the 3DMM produces a 3D mesh,\nparts of the mesh may be self-occluded. The occluded\nvertices can be computed exactly using ray-tracing or z-\nbuffering or they can be precomputed and stored in a lookup\ntable. For efﬁciency, we approximate occlusion by only\ncomputing which vertices have backward facing normals.\nThis approximation would be exact for any object that is\nglobally convex. For objects with concavities, the ap-\nproximation will underestimate the set of occluded ver-\ntices. Faces are typically concave around the eyes, the nose\nboundary and the mouth interior but we ﬁnd that typically\nonly around 5% of vertices are mislabelled and the accuracy\nis sufﬁcient for our purposes.\nThis layer takes as input the rotation matrix R and the\nshape parameters α and outputs a binary occlusion mask\nM ∈{0,1}H′×W′\n. The occlusion function is binary and\nhence not differentiable at points where the visibility of a\nvertex changes, everywhere else the gradient is zero. Hence,\nwe simply pass back zero gradients:\n∂M\n∂α = 0, ∂M\n∂R = 0.\nNote that this means that the network is not able to learn\nhow changes in occlusion help to reduce the loss. Occlu-\nsions are applied in a forward pass but changes in occlusion\ndo not backpropagate.\nMasking layer The ﬁnal layer in the sampler combines\nthe sampled image and the visibility map via pixel-wise\nproducts:\nWc\ni = Vc\ni Mxt\ni,yt\ni\n, ∂Wc\ni\n∂Vc\ni\n= Mxt\ni,yt\ni\n, ∂Wc\ni\n∂Mxt\ni,yt\ni\n= Vc\ni .\n3. Geometric losses for localiser training\nAn STN is usually inserted into a network as a prepro-\ncessor of input images and its output is then passed to a\nclassiﬁcation or regression CNN. Hence, the pose normali-\nsation that is learnt by the STN is the one that produces op-\ntimal performance on the subsequent task. In the context of\n5\nFigure 4. Siamese multiview loss. An image and its horizontal\nreﬂection yield two sampled images. We penalise differences in\nthese two images.\na 3D morphable face model, an obvious task would be face\nrecognition. While this is certainly worth pursuing, we have\nobserved that the optimal normalisation for recognition may\nnot correspond to the correct model-image correspondence\none would expect. For example, if context provided by hair\nand clothing helps with recognition, then the 3DMM-STN\nmay learn to sample this.\nInstead, we show that it is possible to train an STN to\nperform accurate localisation using only some simple geo-\nmetric priors without even requiring identity labels for the\nimages. We describe these geometric loss functions in the\nfollowing sections.\n3.1. Bilateral symmetry loss\nFaces are approximately bilaterally symmetric. Ignor-\ning the effects of illumination, this means that we expect\nsampled face textures to be approximately bilaterally sym-\nmetric. We can deﬁne a loss that measures asymmetry of\nthe sampled texture over visible pixels:\nℓsym =\nN∑\ni=1\n3∑\nc=1\nMxt\ni,yt\ni\nMxt\nsym(i),yt\nsym(i)\n(Vc\ni −Vc\nsym(i))2, (2)\nwhere Vc\nsym(i) is the value in the resampled image at location\n(W′+ 1−xs\ni ,ys\ni ).\nSelection Euclidean \nLoss\nLandmarks\nY''\nFigure 5. Landmark loss. Left: The diagram shows the implemen-\ntation of the regression layer that computes the Euclidean distance\nbetween selected 2D points and ground truth positions. Right: Pre-\ndicted positions are in red and landmark positions are in green.\nFigure 6. Overview of the 3DMM-STN. From left to right: input\nimage; rendering of estimated shape in estimated pose; sampled\nimage; occlusion mask; ﬁnal output of 3DMM-STN.\n3.2. Siamese multi-view ﬁtting loss\nIf we have multiple images of the same face in different\nposes (or equivalently from different viewpoints), then we\nexpect that the sampled textures will be equal (again, ne-\nglecting the effects of illumination). If we had such multi-\nview images, this would allow us to perform Siamese train-\ning where a pair of images in different poses were sampled\ninto images Vc\ni and Wc\ni with visibility masks M and N\ngiving a loss:\nℓmultiview =\nN∑\ni=1\n3∑\nc=1\nMxt\ni,yt\ni\nNxt\ni,yt\ni\n(Vc\ni −Wc\ni )2. (3)\nIdeally, this loss would be used with a multiview face\ndatabase or even a face recognition database where images\nof the same person in different in-the-wild conditions are\npresent. We use an even simpler variant which does not\nrequire multiview images; again based on the bilateral sym-\nmetry assumption. A horizontal reﬂection of a face image\napproximates what that face would look like in a reﬂected\npose. Hence, we perform Siamese training on an input im-\nage and its horizontal reﬂection. This is different to the\nbilateral symmetry loss and is effectively encouraging the\nlocaliser to behave symmetrically.\n3.3. Landmark loss\nAs has been observed elsewhere [28], convergence of the\ntraining can be speeded up by introducing surrogate loss\nfunctions that provide supervision in the form of landmark\nlocations. It is straightforward to add a landmark loss to\nour network. First, we deﬁne a selection layer that selects\nL < Nlandmarks from the N 2D points outputted by the\ngrid generator:\nL = Y′′S (4)\nwhere S ∈{0,1}N×L is a selection matrix withST S = IL.\nGiven Llandmark locations l1,..., lL and associated detec-\ntion conﬁdence values c1,...,c L, we computed a weighted\nEuclidean loss:\nℓlandmark =\nL∑\ni=1\nci∥Li −li∥2. (5)\nLandmarks that are not visible (i.e. were not hand-labelled\nor detected) are simply assigned zero conﬁdence.\n6\nFigure 7. 3DMM-STN output for multiple images of the same per-\nson in different poses.\n3.4. Statistical prior loss\nThe statistical shape model provides a prior. We scale the\nshape basis vectors such that the shape parameters follow a\nstandard multivariate normal distribution: α ∼N (0,ID).\nHence, the statistical prior can be encoded by the following\nloss function:\nℓprior = ∥α∥2. (6)\n4. Experiments\nFor our statistical shape model, we use D = 10dimen-\nsions of which ﬁve are the ﬁrst ﬁve (identity) principal com-\nponents from the Basel Face Model [19]. The other ﬁve\nare expression components which come from FaceWare-\nhouse [5] using the correspondence to the Basel Model pro-\nvided by [33]. We re-mesh the Basel Model over a uniform\ngrid of size224×224. We trained our 3DMM-STN with the\nfour loss functions described in Section 3 using the AFLW\ndatabase [17]. This provides up to 21 landmarks per sub-\nject for over 25k in-the-wild images. This is a relatively\nsmall dataset for training a deep network so we perform\n‘ﬁne-tuning’ by setting the learning rate on the last layer\nof the localiser to four times that of the rest of the network.\nFigure 6 shows the pipeline of an image passing through a\n3DMM-STN. A by-product of the trained 3DMM-STN is\nthat it can also act as a 2D landmark localiser. After train-\ning, the localiser achieves an average landmarking error of\n2.35 pixels on the part of AFLW used as validation set, over\nthe 21 landmarks, showing that overall, the training con-\nverges well.\nWe begin by demonstrating that our 3DMM-STN learns\nto predict consistent correspondence between model and\nimage. In Figure 7 we show 3DMM-STN output for mul-\ntiple images of the same person. Note that the features are\nconsistently mapped to the same location in the transformed\noutput. In Figure 8 we go further by applying the 3DMM-\nSTN to multiple images of the same person and then aver-\nage the resulting transformed images. We show results for\n10 subjects from the UMDFaces [1] dataset. The number\nof images for each subject is shown in parentheses. The av-\nerages have well-deﬁned features despite being computed\nfrom images with large pose variation.\nElon Musk (34) Christian Bale (51) Elisha Cuthbert (53) Clint Eastwood (62) Emma Watson (73)\nChuck Palahniuk (48) Nelson Mandela (52) Kim Jong-un (60) Ben Afﬂeck (66) Courteney Cox (127)\nFigure 8. A set of mean ﬂattened images per subject. Real images\nare obtained from UMDFaces dataset. The number of images that\nare used for averaging is stated next to subject’s name.\nIn Figure 9 we provide a qualitative comparison to [29].\nThis is the only previous work on 3DMM ﬁtting using a\nCNN for which the trained network is made publicly avail-\nable. In columns one and ﬁve, we show input images from\nUMDFaces [1]. In columns two and six, we show the recon-\nstruction provided by [29]. While the reconstruction cap-\ntures the rough appearance of the input face, it lacks the\ndiscriminating detail of the original image. This method re-\ngresses shape and texture directly but not illumination or\npose. Hence, we cannot directly compare the model-image\ncorrespondence provided by this method. To overcome this,\nwe use the landmark detector used by [29] during training\nand compute the optimal pose to align their reconstruction\nto these landmarks. We replace their cropped model by the\noriginal BFM shape model and sample the image. This al-\nlows us to create the ﬂattened images in columns three and\nseven. The output of our proposed 3DMM-STN is shown\nin columns four and eight. We note that our approach less\nfrequently samples background and yields a more consis-\ntent correspondence of the resampled faces. In the bottom\nrow of the ﬁgure we show challenging examples where [29]\ndid not produce any output because the landmark detector\nfailed. Despite occlusions and large out of plane rotations,\nthe 3DMM-STN still does a good job of producing a nor-\nmalised output image.\n5. Conclusions\nIn this paper we have shown how to use a 3D mor-\nphable model as a spatial transformer within a CNN. Our\nproposed architecture has a number of interesting proper-\nties. First, the network (speciﬁcally, the localiser part of\nthe network) learns to ﬁt a 3D morphable model to a sin-\ngle 2D image without needing labelled examples of ﬁtted\nmodels. Since the problem of ﬁtting a morphable model to\nan image is an unsolved problem (and therefore no exist-\ning algorithm could be assumed to provide reliable ground\ntruth ﬁts), this kind of unsupervised learning is desirable.\nSecond, the morphable model itself is ﬁxed in our current\narchitecture. However, there is no reason that this could not\n7\nInput [29] [29] Flatten 3DMM-STN Input [29] [29] Flatten 3DMM-STN\nInput 3DMM-STN Input 3DMM-STN Input 3DMM-STN Input 3DMM-STN\nFigure 9. Qualitative comparison to [29]. The bottom row shows examples for which [29] failed to ﬁt due to failure of the landmark\ndetector.\nalso be learnt. In this way, it may be possible to learn a 3D\ndeformable model for an object class simply from a collec-\ntion of images that are labelled appropriately for the chosen\nproxy task.\nThere are many ways that this work can be extended.\nFirst, we would like to investigate training our 3DMM-STN\nin an end to end recognition network. We would hope that\nthe normalisation means that a recognition network could\nbe trained on less data and with less complexity than ex-\nisting networks that must learn pose invariance implicitly.\nSecond, the shape parameters estimated by the localiser\nmay contain discriminative information and so these could\nbe combined into subsequent descriptors for recognition.\nThird, we would like to further explore the multiview ﬁtting\nloss. Using a multiview face database or video would pro-\nvide a rich source of data for learning accurate localisation.\nFinally, the possibility of learning the shape model during\ntraining is exciting and we would like to explore other ob-\njects classes besides faces for which 3DMMs do not cur-\nrently exist.\nAcknowledgements\nWe gratefully acknowledge the support of NVIDIA Cor-\nporation with the donation of the Titan X Pascal GPU used\nfor this research.\nReferences\n[1] A. Bansal, A. Nanduri, C. D. Castillo, R. Ranjan, and\nR. Chellappa. UMDFaces: An annotated face dataset for\ntraining deep networks. arXiv preprint arXiv:1611.01484v2,\n2016. 7\n[2] A. Bas and W. A. P. Smith. What does 2D geometric infor-\nmation really tell us about 3D face shape? arXiv preprint\narXiv:1708.06703, 2017. 3\n[3] C. Bhagavatula, C. Zhu, K. Luu, and M. Savvides. Faster\nthan real-time facial alignment: A 3d spatial transformer net-\nwork approach in unconstrained poses. In Proc. ICCV, page\nto appear, 2017. 2\n[4] V . Blanz and T. Vetter. A morphable model for the synthesis\nof 3D faces. In Proc. SIGGRAPH, pages 187–194, 1999. 1,\n2\n[5] C. Cao, Y . Weng, S. Zhou, Y . Tong, and K. Zhou. Face-\nwarehouse: A 3D facial expression database for visual com-\n8\nputing. IEEE Trans. Vis. Comp. Gr., 20(3):413–425, 2014.\n7\n[6] D. Chen, G. Hua, F. Wen, and J. Sun. Supervised transformer\nnetwork for efﬁcient face detection. In Proc. ECCV, pages\n122–138, 2016. 2\n[7] M. S. Floater. Parametrization and smooth approxima-\ntion of surface triangulations. Comput. Aided Geom. Des. ,\n14(3):231–250, 1997. 5\n[8] G. Gallego and A. Yezzi. A compact formula for the deriva-\ntive of a 3-D rotation in exponential coordinates. J. Math.\nImaging Vis., 51(3):378–384, 2015. 4\n[9] X. Gu, S. J. Gortler, and H. Hoppe. Geometry images. ACM\nTrans. Graphic., 21(3):355–361, 2002. 5\n[10] R. A. G ¨uler, G. Trigeorgis, E. Antonakos, P. Snape,\nS. Zafeiriou, and I. Kokkinos. DenseReg: Fully convolu-\ntional dense shape regression in-the-wild. In Proc. CVPR,\n2017. 2, 3\n[11] A. Handa, M. Bloesch, V . P ˘atr˘aucean, S. Stent, J. McCor-\nmac, and A. Davison. gvnn: Neural network library for ge-\nometric computer vision. In Proc. ECCV Workshop on Ge-\nometry Meets Deep Learning, 2016. 2\n[12] J. F. Henriques and A. Vedaldi. Warped convolutions:\nEfﬁcient invariance to spatial transformations. CoRR,\nabs/1609.04382, 2016. 2\n[13] A. S. Jackson, A. Bulat, V . Argyriou, and G. Tzimiropou-\nlos. Large pose 3D face reconstruction from a single im-\nage via direct volumetric cnn regression. arXiv preprint\narXiv:1703.07834, 2017. 2, 3\n[14] M. Jaderberg, K. Simonyan, A. Zisserman, and\nK. Kavukcuoglu. Spatial transformer networks. In\nProc. NIPS, pages 2017–2025, 2015. 1, 5\n[15] A. Jourabloo and X. Liu. Large-pose face alignment via\nCNN-based dense 3D model ﬁtting. In Proc. CVPR, 2016.\n2, 3\n[16] H. Kim, M. Zollh ¨ofer, A. Tewari, J. Thies, C. Richardt,\nand C. Theobalt. Inversefacenet: Deep single-shot in-\nverse face rendering from a single image. arXiv preprint\narXiv:1703.10956, 2017. 2, 3\n[17] P. M. R. Martin Koestinger, Paul Wohlhart and H. Bischof.\nAnnotated Facial Landmarks in the Wild: A Large-scale,\nReal-world Database for Facial Landmark Localization. In\nProc. First IEEE International Workshop on Benchmarking\nFacial Image Analysis Technologies, 2011. 7\n[18] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face\nrecognition. In Proc. BMVC, 2015. 3\n[19] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vet-\nter. A 3D face model for pose and illumination invariant\nface recognition. In S. Tubaro and J. Dugelay, editors, Proc.\nAVSS, pages 296–301, 2009. 1, 7\n[20] M. Piotraschke and V . Blanz. Automated 3D face reconstruc-\ntion from multiple images using quality measures. In Proc.\nCVPR, pages 3418–3427, 2016. 2\n[21] E. Richardson, M. Sela, and R. Kimmel. 3D face reconstruc-\ntion by learning from synthetic data. In Proc. 3DV, pages\n460–469, 2016. 2, 3\n[22] E. Richardson, M. Sela, R. Or-El, and R. Kimmel. Learning\ndetailed face reconstruction from a single image. In Proc.\nCVPR, 2017. 2\n[23] S. Romdhani and T. Vetter. Estimating 3d shape and tex-\nture using pixel intensity, edges, specular highlights, texture\nconstraints and a prior. In Proc. CVPR, volume 2, pages\n986–993, 2005. 2\n[24] S. Sch ¨onborn, B. Egger, A. Morel-Forster, and T. Vetter.\nMarkov chain monte carlo for automated face image anal-\nysis. International Journal of Computer Vision, 123(2):160–\n183, 2017. 2\n[25] M. Sela, E. Richardson, and R. Kimmel. Unrestricted fa-\ncial geometry reconstruction using image-to-image transla-\ntion. arXiv preprint arXiv:1703.10131, 2017. 2, 3\n[26] W. A. P. Smith. The perspective face shape ambiguity. In\nPerspectives in Shape Analysis , pages 299–319. Springer,\n2016. 3\n[27] Y . Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:\nClosing the gap to human-level performance in face veriﬁca-\ntion. In Proc. CVPR, pages 1701–1708, 2014. 1\n[28] A. Tewari, M. Zollh ¨ofer, H. Kim, P. Garrido, F. Bernard,\nP. P´erez, and C. Theobalt. MoFA: Model-based deep con-\nvolutional face autoencoder for unsupervised monocular re-\nconstruction. arXiv preprint arXiv:1703.10580, 2017. 2, 3,\n6\n[29] A. T. Tr ˜an, T. Hassner, I. Masi, and G. Medioni. Regressing\nrobust and discriminative 3D morphable models with a very\ndeep neural network. In Proc. CVPR, 2017. 2, 3, 7, 8\n[30] A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural\nnetworks for MATLAB. In X. Zhou, A. F. Smeaton, Q. Tian,\nD. C. A. Bulterman, H. T. Shen, K. Mayer-Patel, and S. Yan,\neditors, Proceedings of the 23rd Annual ACM Conference on\nMultimedia Conference, MM ’15, Brisbane, Australia, Octo-\nber 26 - 30, 2015, pages 689–692. ACM, 2015. 1\n[31] X. Yan, J. Yang, E. Yumer, Y . Guo, and H. Lee. Perspec-\ntive transformer nets: Learning single-view 3D object recon-\nstruction without 3D supervision. In Advances in Neural In-\nformation Processing Systems, pages 1696–1704, 2016. 2\n[32] X. Yu, F. Zhou, and M. Chandraker. Deep deformation net-\nwork for object landmark localization. InProc. ECCV, pages\n52–70, 2016. 2\n[33] X. Zhu, Z. Lei, X. Liu, H. Shi, and S. Z. Li. Face alignment\nacross large poses: A 3D solution. In Proc. CVPR, pages\n146–155, 2016. 7\n9",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.76801997423172
    },
    {
      "name": "Transformer",
      "score": 0.7376340627670288
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7140759229660034
    },
    {
      "name": "Convolutional neural network",
      "score": 0.678214967250824
    },
    {
      "name": "Computer vision",
      "score": 0.521190345287323
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.49671417474746704
    },
    {
      "name": "3d model",
      "score": 0.4537140130996704
    },
    {
      "name": "Solid modeling",
      "score": 0.441917359828949
    },
    {
      "name": "Engineering",
      "score": 0.07060626149177551
    },
    {
      "name": "Voltage",
      "score": 0.06979775428771973
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}