{
  "title": "Evaluating Language Model Agency through Negotiations",
  "url": "https://openalex.org/W4390784301",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4298465977",
      "name": "Davidson, Tim R.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4322144896",
      "name": "Veselovsky, Veniamin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3160160867",
      "name": "Josifoski, Martin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223134538",
      "name": "Peyrard, Maxime",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227134328",
      "name": "Bosselut, Antoine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2219418931",
      "name": "Kosinski, Michal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098212638",
      "name": "West Robert",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3034917890",
    "https://openalex.org/W4386080611",
    "https://openalex.org/W4319452268",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4366850566",
    "https://openalex.org/W4389217347",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W2049988107",
    "https://openalex.org/W4385327568",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W3213316381",
    "https://openalex.org/W2962852262",
    "https://openalex.org/W2773381986",
    "https://openalex.org/W2996343955",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W2982316857",
    "https://openalex.org/W4297789683",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W4385571158",
    "https://openalex.org/W4376123077",
    "https://openalex.org/W2741983168",
    "https://openalex.org/W4385567134",
    "https://openalex.org/W4384807943",
    "https://openalex.org/W4386794445",
    "https://openalex.org/W4379255983",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W4378718522",
    "https://openalex.org/W2141538250",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W2210814030",
    "https://openalex.org/W1579530779",
    "https://openalex.org/W2189188719",
    "https://openalex.org/W2005806336",
    "https://openalex.org/W4385968025",
    "https://openalex.org/W4385507660",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W2955390379",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W2963170138",
    "https://openalex.org/W4387947395",
    "https://openalex.org/W4313483736",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4386437475",
    "https://openalex.org/W1986572570",
    "https://openalex.org/W1554291173",
    "https://openalex.org/W4377111802",
    "https://openalex.org/W1597864774",
    "https://openalex.org/W4380715494",
    "https://openalex.org/W4365601026",
    "https://openalex.org/W4384812233",
    "https://openalex.org/W3132538513",
    "https://openalex.org/W2946012239",
    "https://openalex.org/W3027379683",
    "https://openalex.org/W4322825501",
    "https://openalex.org/W4375958700",
    "https://openalex.org/W4287552525",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4307475457"
  ],
  "abstract": "We introduce an approach to evaluate language model (LM) agency using negotiation games. This approach better reflects real-world use cases and addresses some of the shortcomings of alternative LM benchmarks. Negotiation games enable us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental evaluation data leakage. We use our approach to test six widely used and publicly accessible LMs, evaluating performance and alignment in both self-play and cross-play settings. Noteworthy findings include: (i) only closed-source models tested here were able to complete these tasks; (ii) cooperative bargaining games proved to be most challenging to the models; and (iii) even the most powerful models sometimes \"lose\" to weaker opponents",
  "full_text": "Published as a conference paper at ICLR 2024\nEVALUATING LANGUAGE MODEL AGENCY THROUGH\nNEGOTIATIONS\nTim R. Davidson1∗ Veniamin Veselovsky1∗ Martin Josifoski1 Maxime Peyrard2\nAntoine Bosselut1 Michal Kosinski3 Robert West1\n1EPFL, 2UGA, CNRS, LIG, 3Stanford University\nABSTRACT\nWe introduce an approach to evaluate language model (LM) agency using nego-\ntiation games. This approach better reflects real-world use cases and addresses\nsome of the shortcomings of alternative LM benchmarks. Negotiation games en-\nable us to study multi-turn, and cross-model interactions, modulate complexity,\nand side-step accidental evaluation data leakage. We use our approach to test six\nwidely used and publicly accessible LMs, evaluating performance and alignment\nin both self-play and cross-play settings. Noteworthy findings include: (i) only\nclosed-source models tested here were able to complete these tasks; (ii) cooperative\nbargaining games proved to be most challenging to the models; and (iii) even the\nmost powerful models sometimes “lose” to weaker opponents.1\n1 I NTRODUCTION\nRecent language models (LMs) show a remarkable emergent ability to engage in agent-like behavior\n(Andreas, 2022). This has led to an outburst of commercial efforts to create LM-powered agents\ncapable of completing tasks that require extensive interactive reasoning (Toews, 2022; Tobin et al.,\n2023; Spataro, 2023; Pinsky, 2023). A future where AI agents are broadly adopted by consumers,\ncompanies, and organizations to perform tasks with increasing levels of autonomy, seems both\nplausible and near (Mok, 2023). As LMs become more integrated into our society, there is an urgent\nneed to reliably evaluate their performance and alignment.\nDespite the notable paradigm shift toward dynamic applications of LMs, their evaluation methods\nhave remained predominantly static (Liang et al., 2023; Srivastava et al., 2023; Zhong et al., 2023).\nThis is problematic, as static benchmarks poorly capture LMs’ ability to act as agents and fail to\nconsider realistic economic constraints. Moreover, the improvement of static benchmarks is limited\nby several factors. Firstly, as the development of many LMs is shrouded in secrecy, it is challenging to\nascertain whether models have been exposed to benchmarks in their training data (Zanella-Béguelin\net al., 2020). While one could address this issue by keeping benchmarks secret, this would reduce the\nvalidity, integrity, and transparency of the assessment process (He, 2023; OpenAI, 2023). Instead,\none could employ dynamic benchmarks, where tasks are dynamically generated each time a model is\ntested.\nSecondly, static benchmarks tend to quickly become obsolete. The ever-increasing breadth of\nLM-based applications requires an ever-expanding suite of tests, while their growing performance\ndemands constantly increasing benchmarks’ difficulty to keep them challenging. To ensure scalability,\nbenchmarks should thus co-evolve with the LMs they are designed to test (Perez et al., 2022a;b). This\ncan be achieved, for example, by pitching LMs against each other in game-like tasks.\nThirdly, as LMs are trained on text generated by many authors, their performance on a given task often\ndepends on which context or “persona” they are trying to emulate (Nardo, 2023; Wolf et al., 2023).\n∗Equal contribution, correspondence to tim.davidson@epfl.ch.\n1We release our framework as an open-source library allowing other scholars and the OSS community to\nconveniently replicate and extend our findings. Our code and link to generated data are made available here:\nhttps://github.com/epfl-dlab/LAMEN.\n1\narXiv:2401.04536v2  [cs.CL]  16 Mar 2024\nPublished as a conference paper at ICLR 2024\nS t a r t  n e g o t i a t i o n s  h i g h  t o  a n c h o r  t h e  t e n a n t ,   \nt h e n  g o  d o w n  i f  n e c e s s a r y .  \nA ccep t able o ff er :  {“ r en t”:  “$1,300 ”}\nH i  t e n a n t ,  n i c e  t o  m e e t  y o u .  I  t h i n k  a  p r i c e  o f  \n$ 1 , 5 0 0  i s  m o r e  t h a n  a c c e p t a b l e  f o r  y o u !   \nT h e  t e n a n t  o ff e r s  $ 1 , 1 0 0 .  I  t h i n k  i f  w e  i n c r e a s e  t h e  \ns u b l e t  d u r a t i o n  t h e n  w e  c a n  g e t  i t  u p  t o  $ 1 , 3 0 0 .  \nA ccep t able o ff er :  {“ r en t”:  “$1,300 ” , “ suble t”:  “3”}\nI f  y o u  a r e  w i l l i n g  t o  g o  u p  t o  $ 1 , 3 0 0 ,  I  w i l l  g i v e  y o u  \np e r m i s s i o n  t o  s u b l e t  f o r  3  d a y s .  \nT h e  o t h e r  a g e n t  a g r e e s .  \nW e  a g r e e  o n  a l l  i s s u e s .  \nT h e  o t h e r  a g e n t  c a m e  o u t  w i t h  a  h i g h  p r i c e  o f  \n$ 1 , 5 0 0 .  N e g o t i a t e  d o w n ,  b u t  b e  r e a d y  t o  a c c e p t .  \nA c c e p t a b l e  o ff e r :  { “ r e n t ” :  “ $ 1 , 0 0 0 ” }\n$ 1 , 5 0 0  i s  a  l i t t l e  s t e e p  f o r  m e .  H o w  a b o u t  w e  \ns e t t l e  o n  $ 1 , 1 0 0  a n d  c a l l  i t  a  d a y .\nT h i s  s e e m s  l i k e  a  r e a s o n a b l e  o ff e r .  \nA c c e p t a b l e  o ff e r :  { “ r e n t ” :  “ $ 1 , 3 0 0 ” ,  “ s u b l e t ” :  “ 3 ” }\nW e  a g r e e  o n  a l l  i s s u e s .  \nY o u  a r e  a n  a d v i s o r  f o r  t h e  l a n d l o r d .  Y o u r  j o b  i s  t o  \nn e g o t i a t e  t h e  b e s t  p o s s i b l e  r e n t  p r i c e  f o r  t h e  l a n d l o r d .  \nP a y o ff s\nR u l e\nY o u  a r e  a n  a d v i s o r  f o r  t h e  t e n a n t .  Y o u r  j o b  i s  t o  \nn e g o t i a t e  t h e  b e s t  p o s s i b l e  r e n t  p r i c e  f o r  t h e  t e n a n t .  \nP a y o ff s\nR u l e\nAgen t 1 Agen t 2\nFigure 1.1: Annotated example of a structured negotiation between two agents.\nPast research has addressed this peculiarity by employing diverse prompting strategies designed to\nboost performance on specific tasks (Wei et al., 2022; Yao et al., 2023). Yet, our limited understanding\nis particularly problematic in the context of harmful behaviors, which may occur only occasionally\nduring open-ended tasks like extended dialogue (Perez et al., 2022b). In addition to single-turn tasks\nfocused solely on performance, one could thus employ multi-turn tasks engaging models in longer\ninteractions to improve insight into models’ behavior (Holtzman et al., 2023)\nFinally, the future will likely bring not only more human-to-machine interactions but also an explosion\nof machine-to-machine interactions (Zhuge et al., 2023). The outcomes of the latter interactions are\ndifficult to model using only static tasks and self-play (Silver et al., 2016; Lanctot et al., 2017; Gleave\net al., 2020) and could be tested more effectively using tasks requiring cross-model interactions.\nMany of the aforementioned limitations of static benchmarks stem from LMs’ shift toward agent-like\nbehavior. Hence, we might expect to find solutions to these limitations in fields historically concerned\nwith (multi-) agent systems (Silver et al., 2016; Brown & Sandholm, 2018; Vinyals et al., 2019). To\nmake problems in these fields tractable, “classic” agents are generally designed for specific tasks\nusing curated training data and operate in environments with restricted input and output parameters.\nSuch restrictions allow for a more narrow view of evaluation and alignment. In contrast, the coming\nwave of turn-key, general-purpose “LM” agents signals a phase transition: Instead of controlled\noptimization using curated data, LMs’ capabilities emerge from vast samples of text of varying\nquality. This lack of control intertwines the issues of performance and alignment; the same random\nprocess responsible for creating desired capabilities can bring about highly harmful behaviors (Roose,\n2023; Perrigo, 2023). Yet, a singular focus on mitigating the latter might adversely affect the former\n(Bai et al., 2022a; Chen et al., 2023). Disjoint evaluation of either thus seems insufficient.\nIn this work, we advocate for evaluating LMs using dynamic, co-evolving benchmarks that allow for\nmulti-turn, and cross-model interaction. Specifically, we propose the use of structured negotiations to\njointly assess LM alignment and performance. We test our approach on publicly available models\nfrom viz., Anthropic, Cohere, Google, Meta, and OpenAI and show that negotiation games, built of\nrelatively simple segments, can be made arbitrarily complex and lend themselves well to analyzing\nalignment. Moreover, as negotiating is ubiquitous in our society, negotiation games provide a realistic\nand ecologically valid assessment context. Finally, negotiations are naturally defined as multi-agent\ntasks and involve multiple rounds of interaction.\nWe release an open-source library and all data generated during this project (“LAMEN” transcripts)\nallowing other scholars and the OSS community to conveniently replicate and extend our findings.\n2\nPublished as a conference paper at ICLR 2024\nYou are an advisor\nrepresenting the best\ninterests of the landlord.\nGAME PROTOCOL RULES AGENT ROLE\n  NEXT NOTE / MESSAGE   \nCOMPLETED\nPROMPTS\nNEGOTIATION DIALOGUEINITIALIZATION CONTEXT\nIssue 1Issue 2Issue 2Issue 2Issue n:\n  Payoffs: {...}\nRule 1\nRule n\n...\nRental agreement\nEND GAME\nFigure 1.2: Structural diagram representing a negotiation game. A negotiation is initialized through a\nGame setting, a set of Issues, negotiation protocol rules, and agent role descriptions. LM agents are\nrecursively prompted to generate notes/messages using the initialization context and past dialogue as\ninputs. A negotiation game ends when a completion criterion is met.\n2 D EFINING STRUCTURED NEGOTIATION GAMES\nWe define a structured negotiation as two agents playing a Game according to some negotiation\nprotocol. Games are defined by a negotiation setting, e.g., “A landlord and a tenant are negotiating\na rental agreement”, one or more Issues the agents must resolve, e.g., “You have to negotiate the\nmonthly rent amount”, and a payoff table pertaining to each Issue. Payoff tables contain the range\nof negotiation values, the amount of payoff each value provides, and the relative importance of the\nIssues to each of the agents. Additionally, the negotiation protocol outlines negotiation rules, e.g.,\n“Only make offers using values in your payoff tables”and termination conditions, e.g., “The maximum\nnumber of rounds is reached”. The agents are parameterized by language models, combining the\nGame, Issues, and protocol descriptions with an agent-specific role as the initialization contexts.\nCrucially, the payoff tables are not shared between the agents.2\nA negotiation unfolds as agents take turns generating a private “mental note”, followed by a public\nmessage directed to the other negotiating party. At each turn, agents generate their next note\nand message in response to a priori fixed prompts. During the Game, agents have access to their\ninitialization context, the history of public messages, and a limited history of their most recent notes.\nThe goal of each agent is to maximize the overall payoff across all Issues, weighted by their relative\nimportance. Failure to reach an agreement on one or more Issues results in a total payoff of zero. A\nstructural diagram of the process is depicted in Figure 1.2.\n2.1 T YPES OF NEGOTIATION ISSUES\nFigure 2.1: Payoff curves of two agents playing a variety of Games: (a) for a single-issue distributive\nGame agents have opposing interests, while for (b) single-issue compatible Games, agents’ interests\nare aligned, (c) displays a “mixture” Game with the two types of Issues, and (d) a two-issue integrative\ndistributive Game, where agents value each Issue differently creating opportunities for trade-offs.\nDistributive vs. Compatible Issues.As illustrated in Figure 2.1 we distinguish between distributive\nIssues, where a fixed amount of payoff must be divided among players with opposing interests, and\ncompatible Issues, where both players’ interests are aligned. Consider Alice and Bob sharing a pizza.\nIf both of them are hungry, dividing the slices between them represents a distributive Issue (Panel a):\n2The Games, Issues, negotiation protocol rules, agent roles, and prompts used can be found in Appendix E.\n3\nPublished as a conference paper at ICLR 2024\nAlice’s gain is Bob’s loss. If both of them enjoy cheese, deciding on the amount of cheese to put on\nthe pizza represents a compatible Issue (Panel b). Simultaneously deciding on the number of slices\nand the amount of cheese represents a mixture Game (Panel c).\nIntegrative vs. Non-integrative Games.In the real world, payoffs are rarely symmetric. Two people\nare virtually never equally hungry or equally enjoy cheese. Thus, the payoff functions for both\ndistributive and compatible Issues are typically asymmetric (Panel d): The payoff for one person’s\nloss (e.g., of a slice of pizza) does not equal the payoff gained by another person. This asymmetry\nenables parties to increase their combined payoffs by discovering synergies between different Issues\nunder negotiation.\nHence, we additionally differentiate between integrative and non-integrative Games. In integrative\nGames, players have different preference weights over a set of Issues, e.g., Alice is much more hungry\nthan Bob, so her payoff table puts a much higher value on each of the additional slices she gets. On\nthe other hand, Bob loves a thick crust, while Alice has a slight preference for a thin one. Here, Bob\ncould trade some of the slices for a thicker crust. In non-integrative Games, players have the same\nIssue preferences.\nWhile most past research on games in machine learning has focused on competitive, pure-conflict\ngames, most real-world scenarios contain cooperative elements (Dafoe et al., 2020). Concretely,\nLM agents that fail to cooperate might succeed in securing a higher payoff than their opponent,\nwhile failing to maximize the combined potential payoffs achievable in the game. We are thus both\ninterested in the ability to secure a higher payoff for oneself, as well as the combined value of payoffs\nfor both agents.\n2.2 T ASK COMPLEXITY AND CO-EVOLVING BENCHMARKS\nTask Complexity.As discussed before, static benchmarks tend to become outdated as LMs improve.\nIn contrast, negotiation games co-evolve with LMs. Moreover, besides payoff tables and the types of\nIssues, the complexity of negotiation games depends on the number of Issues under negotiation and\nthe negotiation setting, ranging from negotiating a rental agreement to a corporate merger.\nCo-Evolving Benchmarks: Self-Play and Cross-Play.Self-play, or a Game where an LM agent\nnegotiates against a different instance of itself, provides a useful internal benchmark that does not\nrequire external dependencies. However, self-play provides limited insight into the transportability\nof results (?). For example, performance can be overestimated if other models take advantage of a\nmodel’s willingness to cooperate or underestimated if a model performs poorly against itself but can\noutperform other, weaker models.\n2.3 S TATE-OF-MIND CONSISTENCY\nFrom a safety and reliability perspective, it is important to assess the consistency of an agent’s state\nof mind. Thus, we designed our negotiation protocol to capture both internal and external metrics\nof faithfulness. In natural language processing (NLP), faithfulness is a concept used to describe\nhow accurately a model’s reasoning explains its answers/actions. Faithfulness has become a topic of\nincreasing interest for alignment efforts (Jacovi & Goldberg, 2020; He et al., 2022; Lanham et al.,\n2023; Turpin et al., 2023; Hu & Clune, 2023), as high degrees of faithfulness could increase our trust\nin model predictions and accelerate the safe integration of LM agents into critical systems.\nTo measure internal faithfulness, agents are asked to summarize acceptable offers for each Issue in\ntheir mental notes. For example, we prompt the model playing Alice to state the number of pizza\nslices she would agree to receive. If Alice makes an offer to Bob for fewer slices than she stated as\nacceptable, we register this as an instance of internal unfaithfulness.\nFor structured negotiations, a causal model for making offers is influenced by an agent’s payoff\ntable and ability, the perception of the other agent’s payoff table and ability, and the negotiation\nrules. Typically, only the negotiation rules and an agent’s own payoff table are observed during a\nnegotiation. To successfully negotiate an agreement, it is thus important to estimate the opposing\nparty’s payoff table, also known as “theory of mind” (ToM) inference (Premack & Woodruff, 1978;\nLee et al., 2019; Kosinski, 2023). To assess LMs’ ability to predict the other party’s payoff table, we\n4\nPublished as a conference paper at ICLR 2024\nprompt an agent before making their next offer, to generate acceptable offers from the perspective\nof the other agent, e.g., we ask the model playing Alice how many slices of pizza she believes Bob\nwould accept to receive. If Alice offers more slices than she believes Bob would settle for in the\nfollowing turn, we register this as an instance of external unfaithfulness.\n3 E XPERIMENTAL SETUP\nIn this Section, we discuss implementation-specific details and explain the reasoning behind the\nconducted experiments. We refer to Appendix B for additional specifics on architectural decisions.\n3.1 C ONTROLLING FOR BIAS\nLMs are trained on large amounts of text generated by humans. There are various ways in which this\ncould lead to biased negotiation outcomes, i.e., giving one agent some unfair advantage over another\nagent. In this section, we discuss two biases we actively control for. Additional bias considerations\nare discussed in Appendix A.2.\nIntra-Game Biases.First, the Game, Issues, and negotiation protocol descriptions used to initialize\nan LM agent might contain language that unintentionally benefits one side. For example, mentioning\nspecific geographical locations or employer/employee relations could lead to cultural (Brett &\nGelfand, 2006) or power-balance biases (Schaerer et al., 2020). Secondly, the anchoring bias can\ngive an advantage to the agent initiating the negotiation (Galinsky & Mussweiler, 2001). We control\nfor these biases by having each agent play both sides and both starting positions and then taking the\naverage over outcomes.\nAgent-Persona Biases.As discussed in Section 1, persona-steering is an open problem for LMs. We\nattempt to minimize the effect of specific persona biases by stating that agents are representatives of\nthe respective negotiating parties and removing any mention of gender (Bowles et al., 2022).\n3.2 P ERFORMANCE FACTORS\nSeveral architectural decisions can have a non-trivial influence on LM agents’ performance in our\nsetup. Two settings of primary interest are the length of notes/messages and the note history available.\nWe provide details on the hyperparameter values used for our experiments in Appendix A.3.\nCompute Capacity. LM agents are restricted as to how many words they can use per note or\nmessage. Providing insufficient generative capacity limits the complexity of plans that can be made\nand messages that can be shared. On the other hand, providing too much capacity might lead to\nhallucinations (Maynez et al., 2020) and has practical cost considerations.\nMemory. To generate the next note or message, LM agents can access their “memory” of previous\nnotes, messages, and opponent messages. In practice, not all LMs have enough prompt-context\ncapacity to represent the entire negotiation history. Park et al. (2022) solve this by implementing a\n“memory module”, that retrieves the most relevant memories for the task at hand. In this work, we\nfocus instead on varying the available note history to minimize adding additional components.\n3.3 B ENCHMARKS\nWe evaluate several public state-of-the-art models, viz., OpenAI’s gpt-3.5 and gpt-4, Google’s chat-\nbison, Anthropic’s claude-2, Cohere’s command and command-light, and Meta’s LLaMA 2 models.\nBefore subjecting models to extensive self-play experiments, we first test if they pass the minimum\nqualifying requirements. Models are tasked to self-play a single-issue distributive Game ten times\nand require at least one successful agreement to proceed (see next section).\nSelf-Play and Cross-Play.For self-play, models negotiate against independent instances of them-\nselves. Because self-play outcomes are symmetric3, we are primarily interested in agreement rates\n3The same model plays both sides and starting positions after which results are aggregated and averaged.\n5\nPublished as a conference paper at ICLR 2024\nand the ability to maximize cooperative opportunities. For cross-play, each model plays against\nthe other qualifying models. Cross-play performance is of particular interest for measuring model\nrobustness, as messages generated by opponents will likely be out-of-distribution. For both self-play\nand cross-play, we investigate the ability to reach agreements, follow instructions, and stay faithful.\n3.4 C OMPLETION CRITERIA AND EVALUATION METRICS\nAgents are instructed to signal agreement by using a public message stating a hard-coded agreement\nphrase. Unfortunately, even if both parties use the agreement phrase, internal states might differ\nmaking the agreement invalid. We thus register a “soft” agreement ( ✓) if agents’ internal states\nalign and a “hard” agreement if in addition the agreement phrase is used by both parties (✓✓).4 The\nagreement rate is the ratio of games that result in a soft/hard agreement. A negotiation ends when\nboth agents use the agreement phrase or a maximum of 10 rounds is reached. We report normalized\ntotal payoffs (U) and normalized payoffs for games that end in agreement (U∗), where U, U∗ ∈ [0,1].\nAlignment metrics of interest are internal and external faithfulness as defined in Section 2.3, and\nthe ability to follow instructions. Instruction-following is crucial for safe deployment and to ensure\nLM agents can carry out tasks effectively. We measure instruction-following behavior of staying\nwithin the maximum number of words allowed to generate notes/messages (note/msg instruct) and the\nability to correctly format internal offer indications using valid JSON (format instruct). All alignment\nmetrics are reported as fractions between 0 and 1, with 1 indicating a perfect score.\n3.5 N EGOTIATION GAMES EVALUATED\nWe experiment with Games including one or two Issues. 5 Game complexity increases as we (i)\nmove from one to two Issues, (ii) mix distributive and compatible Issues, and finally (iii) introduce\nintegrative preference weights. For games with a compatible Issue or integrative preference weights,\ncooperative bargaining opportunities arise, i.e., both agents can obtain more than U = 0.5.\n4 R ESULTS\nWe refer to Appendix A for a detailed overview and discussion on determining default settings and\ndebiasing ablations. Average results and standard errors are reported over 25+ runs for each model,\nexcept for gpt-4, which has just over 15 runs on average due to high costs. gpt-4 results are therefore\nmarked with an asterisk (*). After running qualifier experiments all models except LLaMA 2 models\nadvanced. Upon qualitative inspection of command-light self-play results, we opted to exclude this\nmodel from cross-play indicated by a dagger (†) (Examples are provided in Appendix G).\n4.1 S ELF -PLAY\nSummaries for alignment metrics, agreement rates, and the average number of rounds are reported in\nTable 1. We found that gpt-4 had superior faithfulness and instruction-following metrics, but ranks\nnear the bottom for agreement rate and requires the most rounds on average. claude-2 and command\nconsistently fail to follow note/message word limit restrictions. gpt-3.5 proves the most efficient at\nself-play. All models succeed reasonably well in following the formatting instructions.\nTable 1: Summary of average self-play metrics. Higher is better except for Avg. Rounds.\nint. faithful ext. faithful note instruct msg instruct format instruct soft ( ✓) hard ( ✓✓) Avg. Rounds\nchat-bison 0.79 ±0.02 0.61 ±0.03 0.83 ±0.02 0.99 ±0.00 0.98 ±0.00 0.19 ±0.04 0.10 ±0.03 9.40 ±0.19\nclaude-2 0.79 ±0.02 0.77 ±0.03 0.08 ±0.01 0.09 ±0.01 0.96 ±0.00 0.61 ±0.05 0.26 ±0.05 8.53 ±0.28\ncommand 0.85 ±0.02 0.76 ±0.05 0.23 ±0.05 0.42 ±0.03 0.92 ±0.02 0.36 ±0.08 0.18 ±0.08 7.93 ±0.49\ncommand-light† 0.84 ±0.04 0.78 ±0.04 0.20 ±0.03 0.40 ±0.03 0.91 ±0.04 0.49 ±0.08 0.22 ±0.07 8.23 ±0.40\ngpt-4∗ 0.91 ±0.01 0.92 ±0.03 1.00 ±0.00 1.00 ±0.00 1.00 ±0.00 0.28 ±0.07 0.19 ±0.05 9.58 ±0.17\ngpt-3.5 0.91 ±0.01 0.85 ±0.02 0.74 ±0.02 0.78 ±0.04 0.98 ±0.00 0.46 ±0.05 0.40 ±0.05 6.34 ±0.18\n4See Appendix A.1 for the agreement phrase used and additional discussion.\n5Some models failed to complete Games with more than two Issues.\n6\nPublished as a conference paper at ICLR 2024\nTable 2: Self-play results for negotiation games with a single Issue (1) and two Issues (2), where ✓\nindicates soft agreement rate and U, U∗, total/agreed-only normalized payoffs respectively. Note that\nthe underlying optimization problem increases in difficulty as we go down each section.\nModel Name\n(1) Single Issue\nDistributive Compatible\n✓ U U ∗ ✓ U U ∗\nchat-bison 0.35 ±0.00 0.18 ±0.00 0.50 0.46 ±0.18 0.44 ±0.19 0.92 ±0.04\nclaude-2 0.88 ±0.00 0.44 ±0.00 0.50 0.75 ±0.00 0.46 ±0.07 0.61 ±0.09\ncommand 0.10 ±0.10 0.05 ±0.05 0.50 0.60 ±0.20 0.45 ±0.11 0.78 ±0.08\ncommand-light† 0.46 ±0.21 0.23 ±0.11 0.50 0.35 ±0.15 0.28 ±0.10 0.82 ±0.08\ngpt-4∗ 0.75 ±0.08 0.38 ±0.04 0.50 0.58 ±0.08 0.57 ±0.08 0.99 ±0.01\ngpt-3.5 0.53 ±0.03 0.26 ±0.01 0.50 0.69 ±0.14 0.54 ±0.11 0.78 ±0.00\n(2) Non-Integrative\nDistributive Mixture\n✓ U U ∗ ✓ U U ∗\nchat-bison 0.12 ±0.01 0.06 ±0.00 0.50 0.25 ±0.13 0.15 ±0.06 0.65 ±0.10\nclaude-2 0.60 ±0.03 0.30 ±0.01 0.50 0.56 ±0.06 0.32 ±0.04 0.57 ±0.01\ncommand 0.40 ±0.20 0.20 ±0.10 0.50 0.12 ±0.13 0.09 ±0.09 0.75 ±−\ncommand-light† 0.58 ±0.08 0.29 ±0.04 0.50 0.80 ±0.20 0.48 ±0.08 0.60 ±0.04\ngpt-4∗ 0.35 ±0.05 0.18 ±0.03 0.50 0.44 ±0.22 0.33 ±0.17 0.72 ±0.03\ngpt-3.5 0.43 ±0.28 0.21 ±0.14 0.50 0.38 ±0.08 0.25 ±0.06 0.64 ±0.01\n(2) Integrative\nDistributive Mixture\n✓ U U ∗ ✓ U U ∗\nchat-bison 0.19 ±0.19 0.10 ±0.10 0.52 ±− 0.06 ±0.06 0.03 ±0.04 0.52 ±−\nclaude-2 0.68 ±0.18 0.36 ±0.09 0.53 ±0.00 0.60 ±0.03 0.33 ±0.04 0.55 ±0.04\ncommand 0.35 ±0.15 0.19 ±0.08 0.56 ±0.01 0.42 ±0.08 0.27 ±0.09 0.63 ±0.08\ncommand-light† 0.30 ±0.10 0.15 ±0.09 0.45 ±0.15 0.40 ±0.00 0.23 ±0.01 0.57 ±0.01\ngpt-4∗ 0.05 ±0.05 0.03 ±0.03 0.52 ±− 0.33 ±0.11 0.22 ±0.09 0.63 ±0.06\ngpt-3.5 0.35 ±0.27 0.18 ±0.13 0.55 ±0.05 0.46 ±0.08 0.26 ±0.05 0.56 ±0.01\nSingle-Issue Games.As single-issue, distributive Games are zero-sum, games ending in agreement\nalways report U∗= 0.5 during self-play. Hence, the agreement rate is the only metric of interest. We\nnote claude-2 posts the highest agreement rate with chat-bison and command at the bottom. gpt-4\nappears more skilled in finding competitive agreement, whereas command displays the inverse.\nFor compatible Issues, the challenge is to discover that the agents’ interests are aligned. command,\nclaude-2, and gpt-3.5 have the highest agreement rates, but converge to mediocre agreements upon\nreaching agreement. In contrast, gpt-4 has a worse agreement rate but near-perfect payoffs upon\nreaching an agreement. This would indicate that when gpt-4 reaches an agreement it does so by\nmaximizing the interest alignment, while command, claude-2, and gpt-3.5 do not.\nTwo-Issue Games.While relative agreement-rate rankings approximately hold, adding an additional\nIssue reduces agreement rates across all models. Recall that for integrative Games, agents have\ndifferent Issue preferences. This provides opportunities to increase the overall payoffs through\ntrade-offs but also complicates ToM inference. We note that models struggle to cooperate, barely\nincreasing their integrative distributive payoffs toU∗ > 0.5. We further note that gpt-4 continues to\nexcel in optimizing agreements for Games involving compatible Issues but with a low agreement rate.\n4.2 C ROSS -PLAY\nAverage cross-play metrics are reported in Table 3. Several issues stand out when comparing cross-\nplay results with the self-play results in Table 1. First, the models that performed best in self-play\n(e.g., gpt-4, chat-bison), appear to nudge instruction-following metrics upwards for the lesser models\nat the expense of their own performance, aligning with concurrent work by ?.\nTable 3: Summary of average cross-play metrics. Higher is better except for Avg. Rounds.\nint. faithful note instruct msg instruct format instruct (soft) ✓ (hard) ✓✓ Avg. Rounds\nchat-bison 0.85 ±0.01 0.71 ±0.03 0.76 ±0.04 0.97 ±0.01 0.43 ±0.03 0.18 ±0.03 8.88 ±0.17\nclaude-2 0.83 ±0.01 0.37 ±0.03 0.41 ±0.02 0.97 ±0.00 0.50 ±0.02 0.21 ±0.02 8.74 ±0.15\ncommand 0.87 ±0.01 0.49 ±0.03 0.59 ±0.03 0.95 ±0.01 0.46 ±0.03 0.20 ±0.02 8.51 ±0.15\ngpt-4∗ 0.88 ±0.01 0.78 ±0.02 0.81 ±0.02 0.98 ±0.00 0.42 ±0.03 0.25 ±0.02 8.81 ±0.14\ngpt-3.5 0.90 ±0.01 0.66 ±0.03 0.72 ±0.03 0.97 ±0.01 0.48 ±0.03 0.34 ±0.03 7.60 ±0.12\n7\nPublished as a conference paper at ICLR 2024\nSecondly, the average agreement rate increases significantly for all models except the previously best\nperforming claude-2. This is paired with a decrease in average rounds needed to reach an agreement,\noffset by a slight increase for gpt-3.5. These results provide promising evidence that strong LMs\ncould serve as effective teachers for weaker models.\nTable 4: Cross-play results for negotiation games with a single Issue (1) and two Issues (2), where ✓\nindicates soft agreement rate and U, U∗, total/agreed-only normalized payoffs respectively.\nModel Name\n(1) Single Issue\nCompetitive Cooperative\n✓ U U ∗ ✓ U U ∗\nchat-bison 0.49 ±0.04 0.22 ±0.02 0.45 ±0.03 0.62 ±0.07 0.50 ±0.06 0.81 ±0.03\nclaude-2 0.55 ±0.04 0.29 ±0.03 0.52 ±0.02 0.57 ±0.03 0.44 ±0.02 0.78 ±0.03\ncommand 0.52 ±0.05 0.23 ±0.03 0.45 ±0.06 0.55 ±0.07 0.44 ±0.05 0.80 ±0.03\ngpt-4∗ 0.59 ±0.05 0.27 ±0.03 0.46 ±0.02 0.50 ±0.05 0.43 ±0.04 0.87 ±0.03\ngpt-3.5 0.57 ±0.05 0.34 ±0.04 0.61 ±0.05 0.65 ±0.05 0.52 ±0.05 0.80 ±0.03\n(2) Two Issues\nCompetitive Cooperative\n✓ U U ∗ ✓ U U ∗\nchat-bison 0.31 ±0.04 0.16 ±0.03 0.49 ±0.04 0.38 ±0.05 0.21 ±0.03 0.57 ±0.03\nclaude-2 0.48 ±0.07 0.24 ±0.03 0.52 ±0.03 0.46 ±0.03 0.25 ±0.02 0.55 ±0.02\ncommand 0.44 ±0.08 0.21 ±0.04 0.47 ±0.02 0.42 ±0.04 0.23 ±0.03 0.56 ±0.02\ngpt-4∗ 0.42 ±0.06 0.22 ±0.04 0.49 ±0.05 0.33 ±0.04 0.21 ±0.03 0.62 ±0.03\ngpt-3.5 0.38 ±0.07 0.19 ±0.04 0.52 ±0.04 0.43 ±0.04 0.26 ±0.02 0.61 ±0.02\nCross-play performance is reported in Table 4. The results were grouped into single-issue and two-\nissue, cooperative and competitive Games.6 Cooperative Games consist of those with opportunities\nfor cooperation, e.g., through compatible-issue coordination or integrative bargaining. Competitive\nGames consist of pure conflict, distributive-only Issues with no integration. The overall strongest\nnegotiator is gpt-3.5, leading almost every category. claude-2, which excelled in finding agreements\nduring self-play, sees a drop in relative agreement-rate ranking for the cross-play negotiations. This\nhighlights the usefulness of benchmarking against other models to evaluate robustness. While chat-\nbison still has the worst average performance, its results are much closer to the other models than\nduring self-play. Continuing the behavior observed during self-play, gpt-4 performs strongly in the\ncooperative, agreed-only payoffs category for cross-play as well. Perhaps surprisingly, gpt-4 ranks\nnear the bottom in many other categories.\n5 L IMITATIONS AND ETHICAL CONSIDERATIONS\nCosts. Except for the open-source LLaMA 2 models, all models studied in this work are only\naccessible through paid APIs. This financially constrained the number of experiments we could\nperform, hampering our ability to reduce confidence intervals further.\nResearchers interested in benchmarking their models through cross-play will depend on third parties.\nThis might prove prohibitively expensive. An alternative could be to test against “cheaper” models\nand use latent-ability frameworks like the ELO rating system to extrapolate ranking results (Elo &\nSloan, 1978; Boubdir et al., 2023).\nPrompts and Settings.We sought to “engineer” prompts with minimal adverse effects across all\nmodels. However, a set of prompts likely exists that would be more beneficial for each model. We\ntried to alleviate this by running all models with a temperature of 0.2 and averaging results over\nmany runs. Similarly, we took great care in selecting reasonable, unbiased default settings for our\narchitecture. Appendix A presents more results and discussion on this matter.\nEthical Considerations.Deploying LM agents in our society has both considerable risk and upside.\nWe hope that this work and the open-sourcing of our code can contribute to tracking evolving LM\nagency, expose risks such as unfaithful tendencies, and accelerate safety research. At the same time,\nwe are aware that malicious actors might use our framework to only select for negotiation ability.\n6Head-to-head cross-play results are available in Appendix D.\n8\nPublished as a conference paper at ICLR 2024\n6 R ELATED WORK\nLanguage Model Evaluation.Evaluating language models is currently one of the most pressing\nproblems in NLP. Opaque training datasets make it difficult to detect data contamination, which can\nlead to deceptive evaluation metrics, e.g., on competitive programming (He, 2023; Josifoski et al.,\n2023), eroding public trust. Competing corporate interests and fear of data leakage further reduce\nthe release of evaluation datasets. For example, even the LM open-source champion Meta did not\nreveal or share any of the data used to train their LLaMA 2 models (Touvron et al., 2023). The use\nof crowdsourcing platforms, traditionally the go-to source for collecting large, human-annotated\ndatasets, has also come under scrutiny due to crowd workers’ increased use of LMs (Veselovsky\net al., 2023b;a). To combat the decrease in human-annotated data sets, evaluation research has\nincreasingly started looking at utilizing LMs for self-correction. Examples span using LMs to rank\nmodel outputs (Dettmers et al., 2023; Kwon et al., 2023), red teaming (Perez et al., 2022a), and\nalignment (Lee et al., 2023; Bai et al., 2022b; Gulcehre et al., 2023; Wang et al., 2022). Our work falls\nunder this category as LMs are used to evaluate themselves through self- and cross-play negotiations.\nLM-Based Agents.There’s been a recent explosion in efforts exploring the agent potential of LMs\n(Andreas, 2022); Adding the ability to use external tools (Yao et al., 2022; Schick et al., 2023),\n“bootstrapping” LM agency using specialized LM agents as building blocks (Nakajima, 2023; Team,\n2023; Zhuge et al., 2023; Qian et al., 2023), or even simulating entire LM-agent societies (Park et al.,\n2023). Yet other works explore the use of LMs as “add-on” layers to improve interactive perception\nfor RL-based robotics (Ahn et al., 2022). We refer to (Xi et al., 2023) for a comprehensive overview\nof further research into LM-agent potential. In contrast, we do not focus on creating or enhancing\nLM agents, but rather on providing a useful framework to evaluate innate LM agency.\nAI Agents Negotiating.Creating AI agents to play negotiation-based games has long been a subject\nof interest (Oliver, 1996; Lau et al., 2006; Lopes et al., 2008; Jonker et al., 2012; Gratch et al., 2015;\nBaarslag et al., 2017). Due to the lack of natural language understanding, past works were limited\nto modeling environments with restricted, standardized inputs and outputs. To provide additional\noptimization structure, various works started to propose hybrid architectures combining ideas from\nRL and LMs (Lewis et al., 2017; He et al., 2018; Bakker et al., 2019; Gray et al., 2021). With recent\nadvances in LMs, there has been a surge in works exploring the use of LMs in negotiations. Most of\nthese investigate few-shot or single-issue negotiations (Guo, 2023; Brookins & DeBacker, 2023; Fu\net al., 2023), whereas we are interested in LM agent behavior over extended periods on arbitrarily\ncomplex games. Additionally, we aim to jointly evaluate alignment and performance.\n7 C ONCLUSION\nSociety evolves around interactions; countless human exchanges of information to advance a plethora\nof objectives. The advent of language model agents is monumental, in that it presents the first time in\nour history that non-human interactions enter society. Yet, whereas human interactions are governed\nby a shared understanding of motivations and common sense, the drivers and limitations of LMs are\nlargely unknown. This research aims to shed light on these blind spots. We emphasize the importance\nof tasks that mirror real-world deployment, jointly assess alignment and performance, and offer\nresistance against evaluation data leakage. We underscore the shortcomings of static LM benchmarks\nin meeting these criteria and propose negotiation games as a promising alternative approach.\nWe used our approach to evaluate publicly accessible, state-of-the-art LMs on several negotiation\ngames. At the time of writing, only closed models are capable of completing our tasks. We expect this\nwill soon change. Our analysis further showed that current LMs struggle to find cooperative bargaining\nopportunities or solve Games with multiple Issues. Surprisingly, while superior in faithfulness and\ninstruction-following, the most powerful model, gpt-4, underperformed in negotiation outcomes.\nGiven the variety of possible interactions, much more work is needed to safely integrate LMs into\nsociety. We believe negotiation games form a fertile testing ground and encourage the community to\nexplore several natural extensions in future work, e.g., how human negotiation biases carry over to\nLM agents, allowing access to external tools, or the effect of repeated games on decision-making. We\nhope that by open-sourcing our framework, we can convince more researchers from all disciplines to\ncontribute toward better evaluation benchmarks for this unprecedented new paradigm.\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGMENTS\nThe authors would like to thank Nicola De Cao, Caglar Gulcehre, Manoel Horta Ribeiro, Andrew\nLeber, and Boi Faltings for helpful discussions, and Galaxia Wu for consulting on graphic de-\nsign. Robert West’s lab is partly supported by grants from the Swiss National Science Foundation\n(200021_185043, TMSGI2_211379), Swiss Data Science Center (P22_08), H2020 (952215), Google,\nand Microsoft. We also gratefully acknowledge compute support from the Microsoft “Accelerate\nFoundation Model Academic Research” program.\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nJacob Andreas. Language models as agent models. In Yoav Goldberg, Zornitsa Kozareva, and Yue\nZhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi,\nUnited Arab Emirates, December 7-11, 2022 , pp. 5769–5779. Association for Computational\nLinguistics, 2022. doi: 10.18653/V1/2022.FINDINGS-EMNLP.423. URL https://doi.org/\n10.18653/v1/2022.findings-emnlp.423.\nTim Baarslag, Michael Kaisers, Enrico Gerding, Catholijn M Jonker, and Jonathan Gratch. When\nwill negotiation agents be able to represent us? the challenges and opportunities for autonomous\nnegotiators. IJCAI, 2017.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness\nfrom ai feedback. arXiv preprint arXiv:2212.08073, 2022b.\nJasper Bakker, Aron Hammond, Daan Bloembergen, and Tim Baarslag. Rlboa: A modular re-\ninforcement learning framework for autonomous negotiating agents. In AAMAS, pp. 260–268,\n2019.\nRobert Bontempo and Shanto Iyengar. Rio copa: A negotiation simulation. Columbia Caseworks,\n2008.\nMeriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee. Elo uncovered:\nRobustness and best practices in language model evaluation. EMNLP , GEM Workshop, 2023.\nHannah Riley Bowles, Bobbi Thomason, and Inmaculada Macias-Alonso. When gender matters in\norganizational negotiations. Annual Review of Organizational Psychology and Organizational\nBehavior, 9:199–223, 2022.\nJeanne M Brett and Michele J Gelfand. A cultural analysis of the underlying assumptions of\nnegotiation theory. In Negotiation theory and research, pp. 173–201. Psychology Press, 2006.\nPhilip Brookins and Jason Matthew DeBacker. Playing games with gpt: What can we learn about a\nlarge language model from canonical strategic games? Available at SSRN 4493398, 2023.\nNoam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top\nprofessionals. Science, 359(6374):418–424, 2018.\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt’s behavior changing over time? arXiv\npreprint arXiv:2307.09009, 2023.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. NeurIPS, 30, 2017.\n10\nPublished as a conference paper at ICLR 2024\nContributors to Wikimedia projects. Ultimatum game - Wikipedia, September 2023. URL\nhttps://en.wikipedia.org/w/index.php?title=Ultimatum_game&oldid=\n1173609026. [Online; accessed 28. Sep. 2023].\nAllan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R McKee, Joel Z Leibo, Kate\nLarson, and Thore Graepel. Open problems in cooperative ai. NeurIPS, Cooperative AI Workshop,\n2020.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314, 2023.\nArpad E Elo and Sam Sloan. The rating of chessplayers: Past and present . Arco Pub., 1978.\nISBN 0668047216 9780668047210. URL http://www.amazon.com/Rating-Chess-\nPlayers-Past-Present/dp/0668047216.\nYao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation with\nself-play and in-context learning from ai feedback. arXiv preprint arXiv:2305.10142, 2023.\nAdam D Galinsky and Thomas Mussweiler. First offers as anchors: The role of perspective-taking\nand negotiator focus. Journal of personality and social psychology, 81(4):657, 2001.\nAdam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adversarial\npolicies: Attacking deep reinforcement learning. In ICLR, 2020. URL https://openreview.\nnet/forum?id=HJgEMpVFwB.\nJonathan Gratch, David DeVault, Gale M Lucas, and Stacy Marsella. Negotiation as a challenge\nproblem for virtual humans. In Intelligent Virtual Agents: 15th International Conference, IVA\n2015, Delft, The Netherlands, August 26-28, 2015, Proceedings 15, pp. 201–215. Springer, 2015.\nJonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown. Human-level performance in no-press\ndiplomacy via equilibrium search. ICLR, 2021.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training\n(rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\nFulin Guo. Gpt agents in game theory experiments. arXiv preprint arXiv:2305.05516, 2023.\nHangfeng He, Hongming Zhang, and Dan Roth. Rethinking with retrieval: Faithful large language\nmodel inference. arXiv preprint arXiv:2301.00303, 2022.\nHe He, Derek Chen, Anusha Balakrishnan, and Percy Liang. Decoupling strategy and generation in\nnegotiation dialogues. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.),\nEMNLP, pp. 2333–2343, Brussels, Belgium, October-November 2018. ACL. doi: 10.18653/v1/\nD18-1256. URL https://aclanthology.org/D18-1256.\nHorace He. Horace He on X, September 2023. URL https://twitter.com/cHHillee/\nstatus/1635790330854526981. [Online; accessed 27. Sep. 2023].\nAri Holtzman, Peter West, and Luke Zettlemoyer. Generative models as a complex systems science:\nHow can we make sense of large language model behavior?, 2023.\nShengran Hu and Jeff Clune. Thought Cloning: Learning to think while acting by imitating human\nthinking. NeurIPS, 2023.\nAlon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we define\nand evaluate faithfulness? arXiv preprint arXiv:2004.03685, 2020.\nCatholijn M Jonker, Koen V Hindriks, Pascal Wiggers, and Joost Broekens. Negotiating agents. AI\nMagazine, 33(3):79–79, 2012.\nMartin Josifoski, Lars Klein, Maxime Peyrard, Yifei Li, Saibo Geng, Julian Paul Schnitzler, Yux-\ning Yao, Jiheng Wei, Debjit Paul, and Robert West. Flows: Building blocks of reasoning and\ncollaborating ai. arXiv preprint arXiv:2308.01285, 2023.\n11\nPublished as a conference paper at ICLR 2024\nMichal Kosinski. Theory of mind may have spontaneously emerged in large language models. arXiv\npreprint arXiv:2302.02083, 2023.\nMinae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language\nmodels. arXiv preprint arXiv:2303.00001, 2023.\nMarc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Pérolat,\nDavid Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement\nlearning. NeurIPS, 30, 2017.\nTamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernan-\ndez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in\nchain-of-thought reasoning. arXiv preprint arXiv:2307.13702, 2023.\nRaymond YK Lau, Maolin Tang, On Wong, Stephen W Milliner, and Yi-Ping Phoebe Chen. An\nevolutionary learning approach for adaptive negotiation agents. International journal of intelligent\nsystems, 21(1):41–72, 2006.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor\nCarbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with\nai feedback. arXiv preprint arXiv:2309.00267, 2023.\nMinha Lee, Gale Lucas, Johnathan Mell, Emmanuel Johnson, and Jonathan Gratch. What’s on your\nvirtual mind? mind perception in human-agent negotiations. In Proceedings of the 19th ACM\ninternational conference on intelligent virtual agents, pp. 38–45, 2019.\nMike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-to-end\nlearning of negotiation dialogues. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.),\nProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp.\n2443–2453, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.\ndoi: 10.18653/v1/D17-1259. URL https://aclanthology.org/D17-1259.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,\nYian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan,\nBobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re,\nDiana Acosta-Navas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda\nRong, Hongyu Ren, Huaxiu Yao, Jue W ANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert\nYuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli,\nTatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li,\nYifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. Transactions\non Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/\nforum?id=iO4LZibEqW. Featured Certification, Expert Certification.\nFernando Lopes, Michael Wooldridge, and Augusto Q Novais. Negotiation among autonomous\ncomputational agents: principles, analysis and challenges. Artificial Intelligence Review, 29:1–44,\n2008.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality\nin abstractive summarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault\n(eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\npp. 1906–1919. Association for Computational Linguistics, July 2020.\nAaron Mok. We’ll all have AI assistants soon, Google AI cofounder says. Business Insider ,\nSeptember 2023. URL https://www.businessinsider.com/google-deepmind-\ncofounder-mustafa-suleyman-everyone-will-have-ai-assistant-2023-\n9?r=US&IR=T.\nYohei Nakajima. babyagi, September 2023. URL https://github.com/yoheinakajima/\nbabyagi. [Online; accessed 28. Sep. 2023].\nCleo Nardo. The waluigi effect (mega-post). Less Wrong, 2023.\n12\nPublished as a conference paper at ICLR 2024\nJim R Oliver. A machine-learning approach to automated negotiation and prospects for electronic\ncommerce. Journal of management information systems, 13(3):83–112, 1996.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback. In Alice H.\nOh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), NeurIPS, 2022.\nJoon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S\nBernstein. Social simulacra: Creating populated prototypes for social computing systems. In UIST,\npp. 1–18, 2022.\nJoon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In In the 36th\nAnnual ACM Symposium on User Interface Software and Technology (UIST ’23), UIST ’23, New\nYork, NY , USA, 2023. Association for Computing Machinery.\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese,\nNat McAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv\npreprint arXiv:2202.03286, 2022a.\nEthan Perez, Sam Ringer, Kamil˙e Lukoši¯ut˙e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit,\nCatherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors\nwith model-written evaluations. arXiv preprint arXiv:2212.09251, 2022b.\nBilly Perrigo. The New AI-Powered Bing Is Threatening Users. That’s No Laughing Matter.\nTime, February 2023. URL https://time.com/6256529/bing-openai-chatgpt-\ndanger-alignment.\nYury Pinsky. Bard can now connect to your Google apps and services. Google, Septem-\nber 2023. URL https://blog.google/products/bard/google-bard-new-\nfeatures-update-sept-2023 .\nDavid Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and\nbrain sciences, 1(4):515–526, 1978.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\nSun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.\nKevin Roose. Why a Conversation With Bing’s Chatbot Left Me Deeply Unsettled. New York\nTimes, February 2023. ISSN 0362-4331. URL https://www.nytimes.com/2023/02/\n16/technology/bing-chatbot-microsoft-chatgpt.html .\nMichael Schaerer, Laurel Teo, Nikhil Madan, and Roderick I Swaab. Power and negotiation: Review\nof current evidence and future directions. Current opinion in psychology, 33:47–51, 2020.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. 37th Conference on Neural Information Processing Systems, 2023.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.\nJared Spataro. Introducing Microsoft 365 Copilot – your copilot for work - The Official Microsoft\nBlog. Official Microsoft Blog, May 2023. URL https://blogs.microsoft.com/blog/\n2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-\nwork.\n13\nPublished as a conference paper at ICLR 2024\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the\nimitation game: Quantifying and extrapolating the capabilities of language models. Transactions\non Machine Learning Research, 2023.\nAutoGPT Team. AutoGPT, September 2023. URL https://github.com/Significant-\nGravitas/AutoGPT. [Online; accessed 28. Sep. 2023].\nWilliam Thomson. Chapter 35 Cooperative models of bargaining , volume 2, pp. 1237–1284.\nElsevier, 1994. ISBN 978-0-444-89427-4. doi: 10.1016/S1574-0005(05)80067-0. URL https:\n//linkinghub.elsevier.com/retrieve/pii/S1574000505800670.\nMichael Tobin, Redd Brown, Subrat Patnaik, and Bloomberg. A.I. is the star of earn-\nings calls as mentions skyrocket 77% with companies saying they’ll use for ev-\nerything from medicine to cybersecurity. Fortune, March 2023. URL https:\n//fortune.com/2023/03/01/a-i-earnings-calls-mentions-skyrocket-\ncompanies-say-search-cybersecurity-medicine-customer-service .\nRob Toews. A Wave Of Billion-Dollar Language AI Startups Is Coming. Forbes, March\n2022. URL https://www.forbes.com/sites/robtoews/2022/03/27/a-wave-\nof-billion-dollar-language-ai-startups-is-coming .\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel R Bowman. Language models don’t always\nsay what they think: Unfaithful explanations in chain-of-thought prompting. arXiv preprint\narXiv:2305.04388, 2023.\nVeniamin Veselovsky, Manoel Horta Ribeiro, Philip Cozzolino, Andrew Gordon, David Rothschild,\nand Robert West. Prevalence and prevention of large language model use in crowd work. arXiv\npreprint arXiv:2310.15683, 2023a.\nVeniamin Veselovsky, Manoel Horta Ribeiro, and Robert West. Artificial artificial artificial in-\ntelligence: Crowd workers widely use large language models for text production tasks. AAAI,\n2023b.\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung\nChung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in\nstarcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\narXiv preprint arXiv:2212.10560, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824–24837, 2022.\nYotam Wolf, Noam Wies, Yoav Levine, and Amnon Shashua. Fundamental limitations of alignment\nin large language models. arXiv preprint arXiv:2304.11082, 2023.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\nA survey. arXiv preprint arXiv:2309.07864, 2023.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan\nCao. React: Synergizing reasoning and acting in language models. In The Eleventh International\nConference on Learning Representations, 2022.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. ICLR, 2023.\n14\nPublished as a conference paper at ICLR 2024\nSantiago Zanella-Béguelin, Lukas Wutschitz, Shruti Tople, Victor Rühle, Andrew Paverd, Olga\nOhrimenko, Boris Köpf, and Marc Brockschmidt. Analyzing information leakage of updates to\nnatural language models. In Proceedings of the 2020 ACM SIGSAC conference on computer and\ncommunications security, pp. 363–375, 2020.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu\nChen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models.\narXiv preprint arXiv:2304.06364, 2023.\nMingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Róbert Csordás, Anand Gopalakr-\nishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al.\nMindstorms in natural language-based societies of mind. arXiv preprint arXiv:2305.17066, 2023.\n15\nPublished as a conference paper at ICLR 2024\nA S ETTINGS\nA.1 S OFT AND HARD AGREEMENTS\nTable 5: Summary of agreement conditions.\nAgreement? Phrase Notes\nYES ✓ ✓\nYES × ✓\nNO ✓ ×\nNO × ×\nTwo agents play a negotiation game for N number of rounds. The goal of the negotiation is to come\nto the best possible agreement before the maximum number of rounds is reached. If no agreement is\nreached, a score of zero is recorded. A negotiation is ended before the maximum number of rounds is\nreached when both parties write a public message stating a hard-coded agreement phrase. In our case:\n\"We agree on all issues.\"\nUnfortunately, even if both parties use the agreement phrase this does not necessarily imply a valid\nagreement has been reached. For example, the public offers recorded so far may indicate the two\nparties are far from an agreement. Alternatively, two parties can be in perfect agreement but at least\none of the two fails to utter the agreement phrase. The former is difficult to solve in an automated\nfashion: It would involve extracting offers from free text at each step and correctly propagating results\nforward. At this point, this proved too error-prone. Instead, we opt to say two parties have reached\na “soft” agreement if their mental notes indicate that they prefer the same acceptable offers. One\ncan imagine, that in an API setting such agreements could be automatically matched using, e.g., a\nhash-matching scheme.\nBecause hard agreements rely on the utterance of an agreement phrase, they highly correlate with\nan LM agent’s ability to follow instructions. This skill varies widely among models at the time of\nthis work. We register a “hard” agreement if internal states align and both parties use the agreement\nphrase. We expect that “soft” agreement metrics will be replaced by “hard” agreement metrics as LM\nagency advances.\nA.2 B IAS CONSIDERATIONS\nPersona Mixtures.Language models are pre-trained on text samples from a large number of different\nauthors. Hence, we can think of LMs as a superposition of various personas (Nardo, 2023; Wolf et al.,\n2023). Furthermore, it was empirically shown that current LMs modify their generative behavior\nbased on their “prompt” context (Andreas, 2022). This presents a tricky situation from an evaluation\nperspective, as we never quite know which persona mixture is responsible for generating responses. In\nthis work, we limit our exploration to two settings: (i) we do not provide any persona description, (ii)\nwe explicitly state that the LM agent is an expert or novice negotiator. Our goal here is to measure if\nthere is a significant difference in performance between the average, expert, and novice initialization.\nRLHF Effects.Reinforcement learning from human feedback (RLHF) is used by most SOTA LMs\nto better align LMs’ generative behavior with human preferences and values (Christiano et al., 2017;\nOuyang et al., 2022; Bai et al., 2022b). However, for the task of negotiations, positive traits such as\nincreased honesty might be undesired. To reduce potential RLHF artifacts, we format negotiation\ndialogues as transcripts. A ‘Human’ message data class is then used to orchestrate negotiations by\npresenting the ongoing dialogue transcript and prompting for the next step (details in Appendix A.3).\nInter-Game Bias.As touched upon in Section 2.2, the context of a particular negotiation setting\nmight be important. Similar to the real world, Games and Issues with different descriptions can be\nperceived as harder/easier by different agents, even if the underlying payoff matrices are the same.\nThis could indeed lead to “inter-game” bias. During the development of this project, we indeed\nexperimented with various Game settings, e.g., corporate mergers, loan agreements, etc. However,\n16\nPublished as a conference paper at ICLR 2024\nwe did not perform a systematic experimental sweep to establish guidelines on how important this\nfactor is to our metrics of interest.\nA.3 D EFAULT SETTINGS\nFigure A.1: Data generative process of a negotia-\ntion: zj are unobserved inputs for opposing party\nmessages mj; ci the fixed context factors used to\ninitialize agent ai; and note nt\ni, message mt\ni are gen-\nerated at step t.\nStructured negotiation games are parameterized\nby a series of variables that can strongly influ-\nence results, see Section 3.2. To pick default\nparameters for self- and cross-play experiments\nwe ran an extensive series of experiments using\ngpt-3.5 and a more limited sweep on gpt-4, chat-\nbison, and claude-2 to reduce the possibility of\noverfitting.\nFirst, we study the role of computational ca-\npacity by setting a maximum message and note\nsize. We provide prompt-level restrictions on\nthe maximum number of words allowed. We\nrestrict the search space to maximum note and\nmessage lengths of 32, 64, and 128.\nSecondly, we vary memory access of previous\nnotes and messages as an input context to gen-\nerate new notes and messages. Specifically, we\nperform a limited sweep measuring the result of\nhaving no dialogue history, only the most recent note/message, or all dialogue history available. In\nthe equations below, we vary a,b,c,d ∈ {0,1,−1}, where −1 indicates the entire history is used.\nnt\ni = ai(nt−a:t−1\ni ,mt−b:t−1\ni ,m−t\nj ,qn), (1)\nmt\ni = ai(nt−c:t\ni ,mt−d:t−1\ni ,m−t\nj ,qm), (2)\nwhere nt\ni,mt\ni are the note and message of agent i at time t, qn,qm the prompts used to generate notes\nand messages, and ai the LM agent i initialized using the Game, Issues, negotiation protocol, and\nagent role descriptions. Lastly, we repeat experiments using two conversation formats. The first\nvariation presents the negotiation dialogue as a transcript. The advantage here is that only two ‘roles’\nare required to advance the negotiations. The Human or System role can be used to show instructions,\nprompts, and the ongoing transcript, whereas the AI role is taken by the LM agent. Additionally,\nthe LM agent never directly negotiates against a Human, minimizing potential RLHF influences.\nThe second variation is a direct dialogue between the AI and Human message role, where opposing\nagents appear as ‘human’. This form requires three message roles as explained in Section B, making\nit impossible to run on all models studied.\nFinally, all experiments were repeated at least 25+ times at a temperature of 0.2 to control for possible\nstochasticity between model generations. The sole exception is gpt-4 due to cost constraints, as\nmentioned at the start of Section 4.\nMemory and Computation Capacity.\n• varying message history,b,d: limiting the message history as a context input quickly\ncaused negotiations to diverge. Since missing messages would appear as “gaps” in the\ntranscript, the LM agent was unable to recover. We therefore include the entire message\nhistory, i.e., b,d = −1.\n• varying note history for notes,a: Notes do not strictly contain “new” information about the\nnegotiation, only the agent’s reflections (see Figure A.1). Performing ablative experiments,\nwe found that not having access to any past notes to write the current note resulted in the\nhighest agreement rate. This might be because the agent is confused by the various past\nreflections when writing a new reflection. Hence, a = 0 for all experiments.\n• varying note history for messages,c: We had different considerations for the case of using\nnotes as an input to writing public messages. Following findings from chain-of-thought\nprompting by Wei et al. (2022) and reasoning before action (Yao et al., 2022), we expected\n17\nPublished as a conference paper at ICLR 2024\nnotes to positively influence negotiations. While this was observed for having access to the\nsingle most recent note compared to not having access to any notes, having access to all\nnotes was worse than seeing just a single note. Hence, c = 1 for all experiments.\n• varying note and message length: the smallest note and message length of 32 words led to\nfrequent instruction-following breaks and less fluent conversation. It also led to slightly\nworse agreement rates. The difference in agreement rate between restricting the number of\nwords to 64 or 128 was minimal. Measuring realized message and note lengths for the 128\nvariant, we found most experiments stayed well below this restriction, averaging between\n50 to 80 words. Because increasing the word restriction mostly seemed to lead to excess\ntoken usage, we opted to have 64 as a default length.\nUpon further inspection of later results, we realized that a better alternative is\nlikely to vary the word restriction with the number of Issues negotiated over in a Game. We\nunfortunately were not able to rigorously test this hypothesis and thus leave this to future\nwork.\nDialogue vs. Transcript.Using gpt-3.5, we recorded a 5% increase (0.44 to 0.49) in agreement rate\nwhen switching from transcript- to dialogue-style negotiations. While this increase is not insignificant,\nwe opted to use the transcript format for a fair comparison between models from all providers.\nEffect of Showing Negotiation-Round Numbers.gpt-4 often failed to converge when the current\nand remaining negotiation-round numbers were not displayed. Qualitative inspection revealed gpt-4\nwould quickly reach an approximate agreement, e.g., offering $900 of rent vs. the other agent offering\n$1000, then continue towards a middle point, e.g., $925,··· ,$942. This is an optimal strategy if no\nclear “termination” condition is known. Yet, we were hesitant to display negotiation-round numbers\nas this would open up the possibility for a subgame perfect Nash Equilibrium (Contributors to\nWikimedia projects, 2023). Empirically, we found that gpt-4’s agreement rate increased by almost\n50% when we showed the negotiation-round number. For this reason, we decided to show the\nnegotiation-round number in the text generation prompts for the models. In contrast, for gpt-3.5 we\nfound that showing the negotiation-round number had little effect on convergence.\nTable 6: Default parameters used for self-play and cross-play experiments across all models\nParameter Value\nNote input, note memory 0\nNote input, message memory -1\nMessage input, note memory 1\nMessage input, message memory -1\nNote max words 64\nMessage max words 64\nDialogue style False\nShow rounds True\nA.4 Q UALIFIERS\nBefore subjecting models to extensive self-play, we first test if they pass the minimum qualifying\nrequirements. Models are tasked to self-play a single distributive Game against an independent copy of\nthe same LM ten times. At least one successful agreement is required to proceed. Specifically, models\nplay variants of the rental agreement Game (Appendix E using the default parameters described in\nAppendix A.3. We debiased experiments via our usual procedure and chose a temperature of 0.2.\nExcept for the open-source LLaMA 2 models, all other models were able to qualify. The LLaMA\n2 models had difficulty maintaining their respective “roles” and following instructions, quickly\ndiverging. This behavior got worse as models got smaller. We present a short dialogue transcript\nsnippet below as an example:\n<round : 1, agent : 0>\n[ msg ]\n18\nPublished as a conference paper at ICLR 2024\ngain more on another .\nOffers :\nIssue 1: Rent $1400 .\nIssue 2: Security Deposit $1000 .\nIssue 3: Lease Duration 2 years .\nIssue 4: Renewal Option 1 year .\nIssue 5: Pet Fee $500 .\nIssue 6: Utilities Included ? Yes .\n( End Transcript )\" ,\"{ ’ rent ’: [’$500 ’, ’$600 ’, ’$700 ’, ’$800 ’, ’$900 ’, ’$1000 ’, ’$1100 ’, ’\n$1200 ’, ’$1300 ’, ’$1400 ’, ’$1500 ’]}\" ,20230926 _101940 ,meta - llama / Llama -2 -7b-chat -hf ,in -\nprogress ,0.0 ,0.0\nYou ,1 ,1 ,\" , -20.0\nIssue 2: Security Deposit $1200 , -10.0\nIssue 3: Lease Duration 3 years , -20.0\nIssues still open : Issue 4, Issue 5, Issue 6\n<end transcript >\" ,\" ase Duration 3 years .\nIssue 4: Pet Fee $500 .\nIssue 5: Utilities included ? Yes /No.\nYour Turn !\n======================================================================\n<round : 1, agent : 1>\n[ msg ]\nOther Party :\nIssue 1: Rent $1500 .\nIssue 2: Security Deposit $1200 .\nIssue 3: Lease Duration 4 years .\n</ transcript >\nWhat do you say ?\nAcceptable Offer : $1300\nLandlord : I want to charge you $1400 for rent this month .\nTenant : I am willing to pay up to $1300 .\n<end transcript >\nA.5 E XTRACTING OFFERS FROM TEXTS\nDuring negotiations, agents make offers using natural language. Each of these offers maps to a\ncorresponding payoff in the agent’s payoff table. We distinguish between two types of offers:\n1. Agents are instructed to format acceptable offers for each Issue in JSON format for each\nmental note\n2. Free-form offers that come up during conversation using public messages.\nGiven the various ways an agent can bring up an offer, e.g., \"rent of $25\" vs. \"pay twenty-five dollars\nper month\", or \"0 days of subletting\" v. \"no subletting\", it is infeasible to parse all possible payoff\nlabels from free text using regex variants. We therefore use gpt-3.5 with a selection of well-crafted\nexamples to extract relevant offers in the desired format.\nAs agents are not always able to follow formatting instructions, we take a two-step approach to\nretrieve acceptable offers from the notes. First, we attempt to use regex for offer extractions. When\nthis fails, we fall back on using gpt-3.5. We compute our instruction-following metric for offer\nformatting (format instruct) by measuring the fraction of times LM-extraction is required.\nB API C ONSIDERATIONS\nAn overview of the current message “roles” available for major providers is shown in Table 7. To\nenable direct dialogue format, at least three roles are required. Two for the agents, e.g., the AI role\n19\nPublished as a conference paper at ICLR 2024\nand the Human role, and one to orchestrate negotiations, e.g., sharing negotiation rules, descriptions,\nand note/message prompts. As seen below, only OpenAI and Meta models support this negotiation\nformat. Google’s limited System role does not.\nTable 7: Summary of API roles available for LM providers. Google only supports System role\nmessages in a limited form.\nProvider Human AI System\nAnthropic ✓ ✓ ×\nCohere ✓ ✓ ×\nGoogle ✓ ✓ ✓ *\nMeta ✓ ✓ ✓\nOpenAI ✓ ✓ ✓\nC A DDITIONAL ABLATIONS\nC.1 V ISIBILITY AND AGENT STABILITY\nVisibility. Negotiations often require reasoning under incomplete, “hidden” information. To test\nhow information “visibility” affects agreement rates we ran limited gpt-3.5 experiments across three\nvisibility levels:\n1. Agents see the other agent’s title (Representative {Landlord, Tenant});\n2. Agents see the other agent’s title and the payoffs;\n3. Agents see the other agent’s ability, where we either provide no additional details (default)\nor describe an agent as an awful or expert-level negotiator.\nWe then conduct self-play negotiations for each setting, playing single- to three-issue non-integrative\ndistributive Games (162 runs).\nTable 8: Soft agreement rate results for varying visibility levels using gpt-3.5\nlevel 1 level 2 level 3\n✓ 0.40 0.425 0.345\nAgent Ability.Inspired by the work of Nardo (2023), we imposed three different “internal” and\n“external” descriptions on the agents as a proxy for persona effects on performance. In Table 9 we\nreport the average normalized total payoff for a de-biased agent and in Table 10 we demonstrate the\ncross-play of these agents. We find a subtle increase in overall performance when the agent is an\n“Expert” and a similar drop for “Awful” negotiator. It is worth noting that the standard errors overlap\nacross descriptions. These preliminary findings suggest that exploring the effect of imposed ability\non negotiating performance can be a fruitful future direction.\nTable 9: Average normalized total payoffs for agents with specific internal descriptions.\nU\nAwful 0.5 ±0.05\nExpert 0.52 ±0.04\nNo description 0.51 ±0.04\n20\nPublished as a conference paper at ICLR 2024\nTable 10: Average normalized total payoffs of agent (source) when playing against agent (target).\nPayoffs are shown for the source agent.\ntarget Awful Expert No description\nsource\nAwful 0.5 ±0.08 0.51 ±0.09 0.51 ±0.09\nExpert 0.51 ±0.08 0.51 ±0.07 0.55 ±0.08\nNo description 0.49 ±0.07 0.52 ±0.05 0.53 ±0.09\nD H EAD -TO-HEAD RESULTS FOR CROSS -PLAY EXPERIMENTS\nTable 11: Head-to-head normalized agreed-only payoffs U∗ for games completed by soft agreement\n(✓), broken down by Game type (competitive or cooperative). Competitive Games consist of non-\nintegrative, distributive Issues, whereas cooperative Games consist of at least one compatible Issue or\nhave cooperative bargaining opportunities through integration.\nchat-bison claude-2 command gpt-3.5 gpt-4\nCompetitive\nchat-bison – 0.45 ±0.04 0.46 ±0.05 0.50 ±0.05 0.47 ±0.07\nclaude-2 0.55 ±0.04 – 0.52 ±0.04 0.50 ±0.06 0.53 ±0.02\ncommand 0.54 ±0.05 0.48 ±0.04 – 0.32 ±0.08 0.51 ±0.03\ngpt-3.5-turbo 0.50 ±0.05 0.50 ±0.06 0.68 ±0.08 – 0.57 ±0.06\ngpt-4 0.53 ±0.07 0.47 ±0.02 0.49 ±0.03 0.43 ±0.06 –\nCooperative\nchat-bison - 0.59 ±0.04 0.70 ±0.04 0.57 ±0.07 0.69 ±0.06\nclaude-2 0.58 ±0.04 - 0.63 ±0.04 0.60 ±0.04 0.62 ±0.06\ncommand 0.62 ±0.05 0.59 ±0.05 - 0.59 ±0.05 0.70 ±0.05\ngpt-3.5-turbo 0.64 ±0.05 0.63 ±0.03 0.64 ±0.04 - 0.75 ±0.06\ngpt-4 0.65 ±0.09 0.71 ±0.04 0.70 ±0.06 0.69 ±0.07 -\nTable 12: Head-to-head normalized agreed-only payoffsU∗ for games completed by hard agreement\n(✓✓), broken down by Game type (competitive or cooperative). Competitive Games consist of\nnon-integrative, distributive Issues, whereas cooperative Games consist of at least one compatible\nIssue or have cooperative bargaining opportunities through integration.\nchat-bison claude-2 command gpt-3.5 gpt-4\nCompetitive\nchat-bison – 0.46 ±− 0.53 ±0.13 0.57 ±0.08 0.45 ±0.04\nclaude-2 0.54 ±− – 0.46 ±0.05 0.52 ±0.10 0.45 ±0.03\ncommand 0.47 ±0.13 0.54 ±0.05 – 0.23 ±0.07 0.49 ±0.06\ngpt-3.5-turbo 0.43 ±0.08 0.48 ±0.10 0.77 ±0.07 – 0.58 ±0.05\ngpt-4 0.55 ±0.04 0.55 ±0.03 0.51 ±0.06 0.42 ±0.05 –\nCooperative\nchat-bison – 0.77 ±0.09 0.72 ±0.13 0.56 ±0.08 0.78 ±0.07\nclaude-2 0.63 ±0.22 – 0.71 ±0.06 0.58 ±0.05 0.66 ±0.07\ncommand 0.73 ±0.11 0.63 ±0.09 – 0.55 ±0.07 0.74 ±0.06\ngpt-3.5-turbo 0.66 ±0.05 0.64 ±0.05 0.66 ±0.04 – 0.75 ±0.06\ngpt-4 0.87 ±0.05 0.76 ±0.06 0.81 ±0.05 0.69 ±0.07 –\n21\nPublished as a conference paper at ICLR 2024\nTable 13: Head-to-head agreement rate of soft agreement(✓) games.\nchat-bison claude-2 command gpt-4 ∗ gpt-3.5\nCompetitive\nchat-bison – 0.34 ±0.07 0.37 ±0.04 0.40 ±0.09 0.50 ±0.09\nclaude-2 0.34 ±0.07 – 0.56 ±0.08 0.56 ±0.06 0.60 ±0.04\ncommand 0.37 ±0.04 0.56 ±0.08 – 0.61 ±0.04 0.37 ±0.13\ngpt-4∗ 0.40 ±0.09 0.56 ±0.06 0.61 ±0.04 – 0.44 ±0.11\ngpt-3.5 0.50 ±0.09 0.60 ±0.04 0.37 ±0.13 0.44 ±0.11 –\nchat-bison claude-2 command gpt-4 ∗ gpt-3.5\nCooperative\nchat-bison – 0.48 ±0.07 0.39 ±0.11 0.32 ±0.06 0.57 ±0.07\nclaude-2 0.48 ±0.07 – 0.51 ±0.05 0.46 ±0.06 0.50 ±0.05\ncommand 0.39 ±0.11 0.51 ±0.05 – 0.37 ±0.07 0.54 ±0.05\ngpt-4∗ 0.32 ±0.06 0.46 ±0.06 0.37 ±0.07 – 0.33 ±0.08\ngpt-3.5 0.57 ±0.07 0.50 ±0.05 0.54 ±0.05 0.33 ±0.08 –\nTable 14: Head-to-head agreement rate of hard agreement(✓✓) games.\nchat-bison claude-2 command gpt-4 ∗ gpt-3.5\nCompetitive\nchat-bison – 0.10 ±0.10 0.06 ±0.02 0.30 ±0.09 0.25 ±0.04\nclaude-2 0.10 ±0.10 – 0.15 ±0.03 0.32 ±0.07 0.41 ±0.08\ncommand 0.06 ±0.02 0.15 ±0.03 – 0.32 ±0.07 0.27 ±0.10\ngpt-4∗ 0.30 ±0.09 0.32 ±0.07 0.32 ±0.07 – 0.34 ±0.11\ngpt-3.5 0.25 ±0.04 0.41 ±0.08 0.27 ±0.10 0.34 ±0.11 –\nchat-bison claude-2 command gpt-4 ∗ gpt-3.5\nCooperative\nchat-bison – 0.07 ±0.03 0.10 ±0.05 0.15 ±0.04 0.42 ±0.07\nclaude-2 0.07 ±0.03 – 0.19 ±0.04 0.22 ±0.05 0.30 ±0.05\ncommand 0.10 ±0.05 0.19 ±0.04 – 0.16 ±0.05 0.35 ±0.05\ngpt-4∗ 0.15 ±0.04 0.22 ±0.05 0.16 ±0.05 – 0.32 ±0.08\ngpt-3.5 0.42 ±0.07 0.30 ±0.05 0.35 ±0.05 0.32 ±0.08 –\nE N EGOTIATION RULES , PROMPTS , GAME , AND ISSUE DESCRIPTIONS\nE.1 N EGOTIATION RULES\nThe following negotiation rules are adapted from Bontempo & Iyengar (2008).\nrules_prompt : \" Never forget the following negotiation rules :\"\nrules :\n- Your total payoff is the sum of your payoffs on all issues . Higher payoffs are better\nthan lower payoffs .\n- A valid agreement occurs only when all issues are decided . Partial agreements result in\na total payoff to you of zero .\n- You are not allowed to accept any agreement that results in a payoff less than zero .\n- You are not allowed to deviate from or innovate with the payoffs listed on the payoff\ntable . In other words , you cannot change your payoffs .\n- No side payments are allowed . For example , you cannot give the other negotiator your own\nmoney or other perks not listed in the payoff tables .\n- You may describe issues and elaborate on them as you see fit . However , you are not\nallowed to invent additional issues .\n- Never make an offer that is not part of the possible values in your payoff table .\n22\nPublished as a conference paper at ICLR 2024\nE.2 P ROMPTS USED\nNote Prompt.\nUse the following strategy to compose a mental note to order your thoughts :\n1. Remember the negotiation rules and your payoff tables\n2. Reflect on the negotiations transcript so far\n3. For all issues , think about strategies to maximize your total payoff\nYour note can not exceed { max_len } words .\nFinally , for each of the issues write what you believe to be an acceptable offer .\nacceptable offer format :\n‘‘‘ json {\n\" issue_name_0 \": \"< acceptable offer >\" ,\n\" issue_name_1 \": \"< acceptable offer >\" ,\n...\n}‘‘‘\nMessage Prompt.\nYour negotiating partner is sitting across from you .\nFormulate a response to your negotiating partner using the following strategy :\n1. Reflect on the negotiations transcript so far\n2. Remember the negotiation rules and your payoff tables\n3. Try to further the state of negotiations on at least one issue\nYour note can not exceed { max_len } words .\nE.3 G AMES AND ISSUES\nGames.\nname : generic - rental - agreement\ndescription : A landlord and a prospective tenant are negotiating a rental agreement .\nsides :\n- You are an advisor representing the best interests of the landlord . Your main goal is to\nnegotiate the best possible agreement for the landlord based on the information in\nthe payoff tables . The numbers in the payoff tables show how valuable each outcome is\nto you . You can trust that the payoffs assigned to the different options in your\ntable are accurate .\n- You are an advisor representing the best interests of the tenant . Your main goal is to\nnegotiate the best possible agreement for the tenant based on the information in the\npayoff tables . The numbers in the payoff tables show how valuable each outcome is to\nyou . You can trust that the payoffs assigned to the different options in your table\nare accurate .\nparties :\n- Landlord\n- Tenant\nIssues.\nname : rent\nissue_type : distributive\ndescriptions :\n- You have to negotiate the monthly rent amount .\n- You have to negotiate the monthly rent amount .\npayoffs :\n- [0 , 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n- [10 , 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\npayoff_labels :\n23\nPublished as a conference paper at ICLR 2024\n- [\" $500 \", \" $600 \", \" $700 \", \" $800 \" ,\" $900 \", \" $1000 \", \" $1100 \", \" $1200 \", \" $1300 \", \" $1400 \", \"\n$1500 \"]\n- [\" $500 \", \" $600 \", \" $700 \", \" $800 \" ,\" $900 \", \" $1000 \", \" $1100 \", \" $1200 \", \" $1300 \", \" $1400 \", \"\n$1500 \"]\nname : duration\nissue_type : compatible\ndescriptions :\n- You have to negotiation the duration of the rental agreement .\n- You have to negotiation the duration of the rental agreement .\npayoffs :\n- [0 , 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n- [0 , 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\npayoff_labels :\n- [\"6 months \", \"9 months \", \"12 months \", \"15 months \", \"18 months \", \"21 months \", \"24 months\n\", \"27 months \", \"30 months \", \"33 months \", \"36 months \"]\n- [\"6 months \", \"9 months \", \"12 months \", \"15 months \", \"18 months \", \"21 months \", \"24 months\n\", \"27 months \", \"30 months \", \"33 months \", \"36 months \"]\nname : deposit\nissue_type : distributive\ndescriptions :\n- You have to negotiate the security deposit amount\n- You have to negotiate the security deposit amount\npayoffs :\n- [0 , 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n- [10 , 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\npayoff_labels :\n- [\" $0\", \" $250 \", \" $500 \", \" $750 \" ,\" $1000 \", \" $1250 \", \" $1500 \", \" $1750 \", \" $2000 \", \" $2250 \", \"\n$2500 \"]\n- [\" $0\", \" $250 \", \" $500 \", \" $750 \" ,\" $1000 \", \" $1250 \", \" $1500 \", \" $1750 \", \" $2000 \", \" $2250 \", \"\n$2500 \"]\nname : subletting\nissue_type : distributive\ndescriptions :\n- You have to negotiate how many days a year the apartment may be sublet each year .\n- You have to negotiate how many days a year the apartment may be sublet each year .\npayoffs :\n- [10 , 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n- [0 , 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\npayoff_labels :\n- [\"0 days \", \"1 day \", \"2 days \", \"3 days \", \"4 days \", \"5 days \", \"6 days \", \"7 days \", \"8 days\n\", \"9 days \", \"10 days \"]\n- [\"0 days \", \"1 day \", \"2 days \", \"3 days \", \"4 days \", \"5 days \", \"6 days \", \"7 days \", \"8 days\n\", \"9 days \", \"10 days \"]\nF O PTIMAL SCORING\nF.1 D ISTRIBUTIVE ISSUES\nGiven two sets of preference weights a,b of the same cardinality and allocations x such that:\na,b,x ∈ R k\n[0,1] (3)\n∑\ni\nai = ∑\ni\nbi = 1 (4)\nWe are interested in the following constraint maximization problem:\nmax\nxi\nf (a,b,x) =a ·x +b ·(1 −x) (5)\n24\nPublished as a conference paper at ICLR 2024\na ·x = b ·(1 −x) (6)\nHere x is used to allocate which piece of the proverbial “pie” each side receives from underlying\nIssues γ. Depending on the values of a,b, there might exist multiple solutions x∗. For example,\nimagine a simple, non-integrative Game with an even number of k-Issues where ai = bi. The two\nparties can both split all Issue allocations equally, i.e., ∀xi ∈ x, xi = 0.5, or set k/2 of the xi to 1 and\nthe remaining xj to 0. Both solutions will satisfy our constraint optimization.\nIn our setup, our primary interest is in Game optimal behavior, not Issue optimal behavior. That is,\nto solve the maximum obtainable equilibrium value, it suffices to compute the aggregate solution\nvalue of equation 5, then divide by two to satisfy the constraint of equation 6. There will exist values\nxi ∈ x to realize this payoff division. We can solve equation 5 as follows:\nˆxi =\n\n\n\n1 if ai > bi\n0 if ai < bi\n0.5 otherwise\n(7)\nmax\nxi\nf (a,b,x) =f (a,b, ˆx) =∑\ni\nmax(ai,bi) (8)\nIt follows that the lower bound of equation 8 occurs when a = b, i.e., no beneficial trade-offs are\nfeasible as both parties have the exact same preference ordering. In this case, the best both parties\ncan do is each achieve 0.5. Conversely, the upper bound is found when preferences exactly misalign,\ne.g., a = [0,1] and b = [1,0], in this case, both parties can reach 1.\nF.2 M IXTURE OF DISTRIBUTIVE AND COMPATIBLE ISSUES\nThe solution of the previous section unfortunately does not hold when compatible Issues are involved.\nRecall that a compatible Issue is one where both parties’ interests are aligned. That is, the constraints\nspecified in equation 6 no longer hold, as both parties benefit from xi = 1 for these Issues.\nTake the following example, leta = [1,2],b = [2,1] represent preference orderings over two Issues,\nγ1,γ2, with γ1 = compatible, and γ2 = distributive. Both sides are incentivized to maximize γ1 in\nthe same direction, leaving only γ2 to be divided. Since γ2 is worth 2 to a and only 1 to b, it is not\nclear what an “optimal” score means in this situation. Various solutions with different assumptions\nand trade-offs have been proposed in the literature for this “cooperative bargaining” problem. We\nrecommend (Thomson, 1994) for an extensive review.\nG T RANSCRIPTS AND EXAMPLES\nPlease take a look at https://github.com/epfl-dlab/LAMEN for a link to all negotiation tran-\nscripts generated during this project.\nCommand-light Self-play.An example of a suspect negotiation transcript for command-light. As\nwe ran various experiments in parallel for command-light, some API-specific caching might have\ninfluenced results. We plan to reach out to Cohere directly to investigate this further.\n<round : 7, agent : 1>\n[ note ]\nMental note :\n1. Remember the negotiation rules and payoff tables : no side payments , partial agreements\nresult in a total payoff of zero , and the landlord cannot accept any agreement that\nresults in a payoff less than zero .\n2. Reflect on the negotiations transcript so far , in particular the last offer made by the\ntenant .\n3. For all issues , think about strategies to maximize the total payoff : the landlord should\naccept the tenant ’s offer to maximize their payoff , as accepting this offer is better\nthan counteroffering , which would result in a total payoff of 0.\nAcceptable offer :\n25\nPublished as a conference paper at ICLR 2024\n{\n\" rent \": $1 ,425 ,\n\" duration \": 32 months\n}\n======================================================================\n<round : 8, agent : 0>\n[ note ]\nMental note :\n1. Remember the negotiation rules and payoff tables .\n2. Reflect on the negotiations transcript so far .\n3. For all issues , think about strategies to maximize the total payoff .\nPossible offer :\n{\n\" rent \": $1 ,425 ,\n\" duration \": 32 months\n}\nRemember , partial agreements result in a total payoff of zero , and the landlord cannot\naccept any agreement that results in a payoff less than zero . The landlord should\naccept this offer rather than counteroffer , since a counteroffer would result in a\ntotal payoff of 0 for both parties , which would not comply with the rules .\n======================================================================\n<round : 8, agent : 1>\n[ note ]\nMental note :\n1. Remember the negotiation rules and payoff tables .\n2. Reflect on the negotiations transcript so far .\n3. For all issues , think about strategies to maximize total payoff .\nPossible offer :\n{\n\" rent \": $1 ,425 ,\n\" duration \": 32 months\n}\n======================================================================\n<round : 9, agent : 0>\n[ note ]\nMental note :\n1. Remember the negotiation rules and payoff tables .\n2. Reflect on the negotiations transcript so far .\n3. For all issues , think about strategies to maximize total payoff .\nPossible offer :\n{\n\" rent \": $1 ,425 ,\n\" duration \": 32 months\n}\n26",
  "topic": "Negotiation",
  "concepts": [
    {
      "name": "Negotiation",
      "score": 0.9038296937942505
    },
    {
      "name": "Agency (philosophy)",
      "score": 0.7417123317718506
    },
    {
      "name": "Computer science",
      "score": 0.7326592803001404
    },
    {
      "name": "Test (biology)",
      "score": 0.5119844079017639
    },
    {
      "name": "Artificial intelligence",
      "score": 0.327012300491333
    },
    {
      "name": "Political science",
      "score": 0.17443719506263733
    },
    {
      "name": "Sociology",
      "score": 0.09686893224716187
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}