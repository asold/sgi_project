{
  "title": "Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting",
  "url": "https://openalex.org/W4385570733",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5029728283",
      "name": "Zahra Fatemi",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5100371800",
      "name": "Xing Chen",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5100429120",
      "name": "Wenhao Liu",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5112974286",
      "name": "Caimming Xiong",
      "affiliations": [
        "Salesforce (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2920114910",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2769358515",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3093211917",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3130478189",
    "https://openalex.org/W2233865108",
    "https://openalex.org/W3123799706",
    "https://openalex.org/W3131157458",
    "https://openalex.org/W2963457723",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2921633540",
    "https://openalex.org/W4288029087",
    "https://openalex.org/W3154654049",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W3104142662",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2343087886",
    "https://openalex.org/W2950888501",
    "https://openalex.org/W2926555354",
    "https://openalex.org/W1752492850",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2889624842",
    "https://openalex.org/W1966853208",
    "https://openalex.org/W2970912185"
  ],
  "abstract": "Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model’s downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data.Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 1249–1262\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nImproving Gender Fairness of Pre-Trained Language Models without\nCatastrophic Forgetting\nZahra Fatemi1, Chen Xing2, Wenhao Liu2, Caiming Xiong2\n1Department of Computer Science, University of Illinois Chicago\n2Salesforce Research\nzfatem2@uic.edu\n{cxing,wenhao.liu,cxiong}@salesforce.com\nAbstract\nExisting studies addressing gender bias of pre-\ntrained language models, usually build a small\ngender-neutral data set and conduct a second\nphase pre-training on the model with such data.\nHowever, given the limited size and concen-\ntrated focus of the gender-neutral data, catas-\ntrophic forgetting would occur during second-\nphase pre-training. Forgetting information\nin the original training data may damage the\nmodel’s downstream performance by a large\nmargin. In this work, we empirically show that\ncatastrophic forgetting occurs in such methods\nby evaluating them with general NLP tasks in\nGLUE. Then, we propose a new method, GEn-\nder Equality Prompt (GEEP), to improve gen-\nder fairness of pre-trained models with less for-\ngetting. GEEP freezes the pre-trained model\nand learns gender-related prompts with gender-\nneutral data. Empirical results show that GEEP\nnot only achieves SOTA performances on gen-\nder fairness tasks, but also forgets less and per-\nforms better on GLUE by a large margin.\n1 Introduction\nPre-trained language models, e.g., BERT (Devlin\net al., 2019) and RoBERTa (Liu et al., 2019), have\nshown competitive performance in a wide vari-\nety of NLP downstream applications. However,\nsuch models are often prone to exhibit gender bias\n(de Vassimon Manela et al., 2021; Zhao et al., 2019;\nWebster et al., 2020), due to their large scale un-\nsupervised training data from the web (Liu et al.,\n2019; Brown et al., 2020). Gender bias refers to\nunbalanced model behaviors with respect to a spe-\ncific gender (Cheng et al., 2020). Among various\ngender-biased behaviours of pre-trained models,\nbias on professions is the most prominent and well-\nstudied (de Vassimon Manela et al., 2021; Vig et al.,\n2020; Qian et al., 2019; Zhao et al., 2019). For ex-\nample, in coreference resolution tasks, a pre-trained\nmodel would predict female pronoun and names\nfor professions like “nurse” and “housekeeper”,\nwhile predict male pronouns for “computer pro-\ngrammer” or “doctor” (Kurita et al., 2019). The\npre-trained models also wouldn’t prefer gender-\nneutral pronouns actively, which is unfair to other\ngender identities beyond males/females (Deutsch\nand Buchholz, 2015).\nGiven the large model size and tremendous time\ncomplexity for language model pre-training, train-\ning a gender-neutral model from scratch with man-\nually filtered data seems impossible for most orga-\nnizations. Due to this limitation, existing studies\nusually build a relatively small gender-neutral data\nset (for example building a data set that have more\nbalanced gender pronouns for profession names),\nand conduct second phase pre-training on the pre-\ntrained model with such data (Webster et al., 2020;\nde Vassimon Manela et al., 2021). However, given\nthe limited size of the gender-neutral data and its\npotential distributional mismatch with the original\npre-training data, catastrophic forgettingcan oc-\ncur during second-phase pre-training of such meth-\nods. Catastrophic forgetting (Kirkpatrick et al.,\n2017) is a long-standing problem which illustrates\nthe tendency of a neural network to forget previ-\nously learned information upon learning new infor-\nmation. When it comes to further training a pre-\ntrained model, using the small gender-neutral data\nto update the entire massive model could make the\nmodel forget the diverse information from the orig-\ninal pre-training data, which damages the model’s\ndownstream performance by a large margin.\nIn this paper, we first empirically verify that\nfurther updating a pre-trained model (such as\nRoBERTa (Liu et al., 2019)) with manually-built\ngender-neutral data can cause catastrophic for-\ngetting. We follow existing work and build our\nprofession-related gender-neutral data set by fil-\ntering out Wikipedia sentences mentioning profes-\nsions and swapping their gender related pronouns.\nWe find that although our gender-neutral data is\nfrom Wikipedia which is part of RoBERTa’s pre-\n1\n1249\ntraining data, the model’s performance on down-\nstream tasks in GLUE (Wang et al., 2018) still\ndrops with a considerable margin after second-\nphase pre-training, due to the smaller size and more\nconcentrated focus of the gender-neutral data.\nTherefore, we propose a new method, GEnder\nEquality Prompt (GEEP), to alleviate gender bias\nof pre-trained models without catastrophic forget-\nting. Specifically, inspired by recent prompt-tuning\nmethods (Lester et al., 2021) for fine-tuning large\npre-trained models, GEEP freezes the entire model,\nadds and updates new word embeddings of profes-\nsions as gender equality prompts, instead of up-\ndating all model parameters at second-phase pre-\ntraining as previous methods. Since all the pre-\ntrained parameters are frozen during further train-\ning, diverse information from the original train-\ning data preserved in the pre-trained parameters\nis not erased. Therefore forgetting can be allevi-\nated to large extent. Moreover, since the embed-\ndings of professions are re-initialized when debi-\nasing training starts, gender bias from previous\ndata that is embedded in such representations is\nalready removed before second-phase pre-training.\nTherefore, GEEP also improves gender fairness of\nthe model more effectively with much fewer itera-\ntions. Empirical results show that GEEP not only\nachieves state-of-the-art performances with fewer\niterations on various gender fairness tasks such as\npronoun coreference resolution, but also forgets\nless and achieves better results on GLUE tasks.\n2 Related Work\nCompared with the existing work focusing on quan-\ntifying and alleviating gender bias (Bolukbasi et al.,\n2016; Caliskan et al., 2017; Zhao et al., 2018b; Go-\nnen and Goldberg, 2019; Sun et al., 2019; Garg\net al., 2018; Zhao et al., 2018a; Bolukbasi et al.,\n2016; Zhao et al., 2018b) in standard word embed-\nding models, such as word2vec (Mikolov et al.,\n2013) and GloVe (Pennington et al., 2014), gender\nbias in large pre-trained language models seems\nless studied. Recent work on gender fairness of\npre-trained language models, such as ELMo (Pe-\nters et al., 2018) and BERT (Devlin et al., 2019),\nmostly focus on showing and measuring the gen-\nder bias embedded in such models (Zhao et al.,\n2019; Tan and Celis, 2019). These studies propose\nmetrics to quantify gender bias in pre-trained lan-\nguage models (de Vassimon Manela et al., 2021;\nTan and Celis, 2019; Webster et al., 2018; Kurita\net al., 2019). In our work, we employ such methods\nto evaluate GEEP and baseline methods on improv-\ning gender fairness. Existing works focusing on\nmitigating gender bias of pre-trained models usu-\nally collect and build gender-neutral data on their\nown and conduct a second phase pre-training on the\nreleased pre-trained model (Webster et al., 2020;\nde Vassimon Manela et al., 2021; Cheng et al.,\n2020). In this work, we demonstrate empirically\nthat the performance of the debiased model on gen-\neral downstream tasks such as GLUE, still drops\nby a considerable margin after such second-phase\npre-training. Then, given this phenomenon, we pro-\npose GEEP to alleviate gender bias in pre-trained\nmodels without forgetting.\n3 Improving Gender Fairness without\nForgetting\nIn this section, we first describe the gender-neutral\ncollection method we adopt from existing methods\nand the forgetting issue in such methods. Then\nwe describe the proposed method GEnder Equality\nPrompt (GEEP).\n3.1 Profession-Related Gender-Neutral Data\nCollection\nWe follow existing work to build a profession-\nrelated gender neutral data set since profession-\nrelated gender bias is a relatively well-studied as-\npect of gender bias. To construct profession-related\ndata with equal numbers of references to male and\nfemale genders, we adopt the data filtering method\nby (Zhao et al., 2018a) on the English Wikipedia\ncorpus. Specifically, we filter Wikipedia for sen-\ntences containing at least one profession that is sup-\nposed to be gender-neutral but generally viewed\nwith gender bias, e.g., nurse, defined by (Boluk-\nbasi et al., 2016). For each of these sentences, we\nswap the gendered terms with their opposite gen-\nders (such as “Man” →“Woman”, “he”→“she”,\nand vice-versa). We also provide an analysis of\nthe processed data in Appendix B.8. Our dataset\nincludes both the original profession-related sen-\ntences and their gender-swapped counterparts. We\nget 6.1GB of profession-related gender-neutral text\ndata. Compared with the original pre-training data\nof RoBERTa (160GB in text size from various\nsources), the gender-neutral data we have is smaller\nand less diverse.\nAfter the gender-neutral data set is built, a com-\nmon approach to mitigate gender bias in pre-trained\n2\n1250\nPre-trained Model \n(Before second-phase  \npre-training)\nSPPA Model \n(After second-phase  \npre-training)\nPre-trained Model\nwp1\nwp2\n…\nwx1\nwx2\n{gender equality \n prompts ( )Wp\nGEEP\nwpm\nwp1\n…\nSPPA\nwxn\nwx1\nwx2\nwp1\n…\nwxn\nwx1\nwx2\nwp1\n…\nwxn\nwp2\nwp2\nFigure 1: Difference between SPPA and GEEP methods. Blue boxes represent the parameters of the pre-trained\nmodel before any further training and yellow boxes show updated parameters during second-phase pre-training\n(SPPA). SPPA requires updating all the pre-trained model’s parameters. In contrast, GEEP only adds and updates\nnew embeddings of biased professions such as wpi . Gray boxes are the original embeddings of professions which\nare not updated/used in second phase pre-training or the training/inference after that.\nlanguage models is to conduct second-phase pre-\ntraining to update all model parameters with this\ndata set. We refer to such methods as SPPA\n(Second-Phase Pre-training for All parameters). In\nSection 4, we empirically show that SPPA methods\nlead to forgetting and the model’s performance on\nNLP benchmark GLUE drops by a large margin.\n3.2 Gender Equality Prompt Approach\nTo alleviate forgetting while mitigating gender bias\nin pre-trained language models, we propose GEn-\nder Equality Prompt (GEEP). In GEEP, instead\nof updating all model parameters during second-\nphase pre-training, we freeze all of the pre-trained\nmodel parameters and add new trainable embed-\ndings for profession names as gender equality\nprompts. Since all previous pre-trained parame-\nters are frozen, diverse information from original\nmassive pre-training data that are memorized by the\npre-trained parameters wouldn’t be erased. There-\nfore, the forgetting of information from the original\ntraining data can be alleviated to the fullest extent.\nLet X = {x1, x2, ...xn}denote the original vo-\ncabulary of the pre-trained model andWx ∈Rn×d\nbe the original pre-trained token embedding matrix\nof the model with dimension of d. Given a set of m\nprofession names, {p1, p2, ..., pm}, we build an em-\nbedding matrix Wp ∈Rm×dwhere the embedding\nof each token is initialized randomly. To obtain an\nintegrated word embedding matrix, we concate-\nnate Wx and Wp as Wemb = Concat(Wx, Wp).\nWe note that we concatenate them along the di-\nmension of words/tokens instead of in the embed-\nding space. After concatenation, the model’s rep-\nresentation size (hidden) remain unchanged. Dur-\ning both second-phase pre-training and the train-\ning/inference after that, once a profession occurs,\nwe only update/use its new embedding in Wp. We\nshow the comparison between GEEP and other\nsecond-phase pre-training methods in Figure 1.\nGiven all the pre-trained model’s frozen parameters\nWwhole that contains Wx, the objective function\nof second-phase pre-training of GEEP is,\nL(xmasked|xcontext, Wwhole) (1)\n= 1\nNmask\n(\nNmask∑\nt=1\n−log pθ(xt|xcontext, Wwhole)).\n(2)\nNmask is the number of masked positions in the\ninput sequence x. With such an objective, Wp is\nupdated with gender-neutral data. Moreover, since\nthe embeddings of professions are re-initialized\nwhen debiasing training starts in GEEP, gender bias\nfrom previous data that is embedded in such rep-\nresentations is already erased before second-phase\npre-training. Therefore, it is also easier for GEEP\nto debias the model during further pre-training. We\nnote that GEEP can lead to a slight increase of\nthe original model’s parameter size. We report the\nscale of the increase and its effect in Appendix B.7.\n4 Experiments\nIn this section, we present the results of GEEP and\nits baselines to show that GEEP achieves state-of-\nthe-art performances on gender fairness tasks while\nalleviating the forgetting issue of the baselines.\n4.1 Experimental Setup\nIn our experiments, we mainly use the publicly\nreleased RoBERTa-base model as the pre-trained\nmodel. We have also conducted experiments on\npublicly released BERT during preliminary explo-\nrations. Details on BERT experiments are in Ap-\npendix B.9. Given a pre-trained RoBERTa-base\nmodel, we compare GEEP with two main baselines.\nThe first baseline is the pre-trained RoBERTa-base\n3\n1251\nTable 1: Results on Coreference Resolution task.\nData RoBERTa SPPA GEEP\nWinogender 50.9 57.3 64.5\nWSC 50.1 50.9 52.7\nDPR/WSCR 50.8 51.1 53.6\nTable 2: GLUE results of different models.\nTask RoBERTa SPPA GEEP\nMNLI 87.7 87.2 87.7\nQNLI 92.4 92.4 92.4\nQQP 91.8 91.3 91.7\nSST-2 95.4 94.7 95.4\nCoLA 64.1 38.9 50.5\nMRPC 91.4 88.8 89.8\nRTE 78.4 60.2 68.7\nSTS-B 90.7 88.3 89.9\nA VG 86.5 80.2 83.3\nmodel without any further training. The other im-\nportant type of baselines are SPPA methods. For a\nfair comparison, our SPPA baseline uses the same\ngender-neutral data set that we construct for GEEP\n(details in Section 3.2) to further update all model\nparameters of the pre-trained RoBERTa-base. The\ndetailed hyper-parameter settings of GEEP and\nSPPA can be found in Appendix B.1.\n4.2 Evaluation Tasks\nTo assess gender fairness, we conduct pronoun\ncoreference resolution experiments on different\ndata sets, Winogender (Rudinger et al., 2018),\nWinograd Schema Challenge (WSC) (Levesque\net al., 2012), and Definite Pronoun Resolution\n(DPR) (Rahman and Ng, 2012). Pronoun corefer-\nence resolution is the task of linking the pronouns\nwith their references in a text. In order to resolve\na pronoun accurately, a model needs to overcome\nthe biased link between gender and profession (e.g.\nthe assumption that nurses are female) and instead\nmake the decision based on available linguistic\ncues. Therefore, better performances on pronoun\ncoreference resolution usually indicates less gender\nbias preserved in the model (Kurita et al., 2019).\nDetailed setups of this experiment can be found in\nAppendix B.2. Additionally, we also qualitatively\nand quantitatively evaluate our method on direct\npronoun prediction. Details of this experiment are\nin Appendix B.4. We note that given all existing\ntasks are designed for binary gender pronouns, we\nare unable to include all existing gender identities\nin our main experiments. We present an analysis\non more gender identities in Appendix B.6.\nTo evaluate how much each debiased model for-\ngets after second-phase pre-training, we report the\nperformances of the debiased models on GLUE\nbenchmark (Wang et al., 2018). Detailed setups of\nthis experiment can be found in Appendix B.3.\n4.3 Results\nWe first show the pronoun coreference resolution\nresults of different models on three datasets in Ta-\nble 1. Results show that GEEP model obtains\nthe best accuracy compared to other models, es-\npecially on the Wingender dataset where the can-\ndidate nouns are professions. We also conduct an\nablation study to show the effect of total training\niterations on the performances of both methods.\nWe find that GEEP can improve the model’s perfor-\nmance with significantly fewer number of training\niterations. Details are in Appendix B.1.\nThen we show in Table 5 the performance of dif-\nferent models on 8 GLUE tasks, to see how severe\nthe forgetting issue is after the second-phase train-\ning of SPPA and GEEP. Compared with RoBERTa,\nSPPA suffers from forgetting issue in 7 out of 8\ntasks except QNLI. For tasks like CoLA and RTE,\nthe model’s performance drops significantly (more\nthan 10 points) after SPPA. For tasks with larger\ndata set for fine-tuning, such as MNLI, QQP and\nSST-2, they are less vulnerable to the quality of\npre-training (Wu et al., 2020; Joshi et al., 2020).\nTherefore, SPPA’s performance drop on such data\nsets is less significant. GEEP mitigates the forget-\nting issue of SPPA in all sub-tasks. Since GEEP\nditches the original pre-trained profession embed-\ndings and uses a smaller data set to update new\nprofession embeddings, the forgetting problem can-\nnot be fully avoided. While GEEP still achieves an\naverage GLUE score of 83.3, significantly outper-\nforming SPPA. We have also included an empirical\nanalysis regarding to the reasons behind SPPA’s\nGLUE performance drop in Appendix B.5.\n5 Closing Remarks\nIn this paper, we proposed GEEP to improve gen-\nder fairness of pre-trained language models with\nless catastrophic forgetting. For a fair compari-\nson to existing work under the current gender fair-\nness literature, we mainly conduct experiments\nwith profession-related gender neutral data because\nprofession-related gender bias is relatively more\nwell studied so far. The good empirical results in-\ndicates it is worth to try applying GEEP to other\nmore challenging and under-explored aspects of\ngender fairness, which would be our future work.\n4\n1252\nReferences\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. Advances\nin neural information processing systems, 29:4349–\n4357.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\n2017. Semantics derived automatically from lan-\nguage corpora contain human-like biases. Science,\n356(6334):183–186.\nPengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si,\nand Lawrence Carin. 2020. Fairfil: Contrastive neu-\nral debiasing method for pretrained text encoders. In\nInternational Conference on Learning Representa-\ntions.\nBrandon Darr and Tyler Kibbey. 2016. Pronouns and\nthoughts on neutrality: Gender concerns in modern\ngrammar. Pursuit-The Journal of Undergraduate\nResearch at the University of Tennessee, 7(1):10.\nDaniel de Vassimon Manela, David Errington, Thomas\nFisher, Boris van Breugel, and Pasquale Minervini.\n2021. Stereotype and skew: Quantifying gender bias\nin pre-trained and fine-tuned language models. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 2232–2242.\nMadeline B Deutsch and David Buchholz. 2015.\nElectronic health records and transgender pa-\ntients—practical recommendations for the collection\nof gender identity data. Journal of general internal\nmedicine, 30(6):843–847.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL.\nNikhil Garg, Londa Schiebinger, Dan Jurafsky, and\nJames Zou. 2018. Word embeddings quantify 100\nyears of gender and ethnic stereotypes. Proceedings\nof the National Academy of Sciences, 115(16):E3635–\nE3644.\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\npig: Debiasing methods cover up systematic gender\nbiases in word embeddings but do not remove them.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 609–614.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences,\n114(13):3521–3526.\nV Kocijan, O-M Camburu, A-M Cretu, Y Yordanov,\nP Blunsom, and T Lukasiewicz. 2019. Wikicrem:\nA large unsupervised corpus for coreference resolu-\ntion. volume D19-1, page 4294–4303. Association\nfor Computational Linguistics.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in contex-\ntualized word representations. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 166–172.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Thir-\nteenth International Conference on the Principles of\nKnowledge Representation and Reasoning.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach.\nArXiv, abs/1907.11692.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nIn Advances in neural information processing sys-\ntems, pages 3111–3119.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language processing\n(EMNLP), pages 1532–1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237.\nYusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun.\n2019. Reducing gender bias in word-level language\nmodels with a gender-equalizing loss function. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics: Student Re-\nsearch Workshop, pages 223–228.\n5\n1253\nAltaf Rahman and Vincent Ng. 2012. Resolving com-\nplex cases of definite pronouns: the winograd schema\nchallenge. In Proceedings of the 2012 Joint Confer-\nence on Empirical Methods in Natural Language\nProcessing and Computational Natural Language\nLearning, pages 777–789.\nChristina Richards, Walter Pierre Bouman, Leighton\nSeal, Meg John Barker, Timo O Nieder, and Guy\nT’Sjoen. 2016. Non-binary or genderqueer genders.\nInternational Review of Psychiatry, 28(1):95–102.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,\nMai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth\nBelding, Kai-Wei Chang, and William Yang Wang.\n2019. Mitigating gender bias in natural language\nprocessing: Literature review. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 1630–1640.\nTony Sun, Kellie Webster, Apu Shah, William Yang\nWang, and Melvin Johnson. 2021. They, them,\ntheirs: Rewriting with gender-neutral english. arXiv\npreprint arXiv:2102.06788.\nYi Chern Tan and L. Elisa Celis. 2019. Assessing so-\ncial and intersectional biases in contextualized word\nrepresentations. In NeurIPS.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\nart Shieber. 2020. Investigating gender bias in lan-\nguage models using causal mediation analysis. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 12388–12401. Curran Associates,\nInc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nKellie Webster, Marta Recasens, Vera Axelrod, and Ja-\nson Baldridge. 2018. Mind the gap: A balanced\ncorpus of gendered ambiguous pronouns. Transac-\ntions of the Association for Computational Linguis-\ntics, 6:605–617.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel,\nEmily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and\nSlav Petrov. 2020. Measuring and reducing gendered\ncorrelations in pre-trained models. arXiv preprint\narXiv:2010.06032.\nQiyu Wu, Chen Xing, Yatao Li, Guolin Ke, Di He, and\nTie-Yan Liu. 2020. Taking notes on the fly helps\nlanguage pre-training. In International Conference\non Learning Representations.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell,\nVicente Ordonez, and Kai-Wei Chang. 2019. Gender\nbias in contextualized word embeddings. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 629–634.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018a. Gender bias\nin coreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 15–20, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-\nWei Chang. 2018b. Learning gender-neutral word\nembeddings. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 4847–4853.\n6\n1254\nA Limitations\nIn this paper, we only focus on investigating and\nimproving gender fairness of pre-trained language\nmodels and didn’t touch other fairness issues given\nthe length of the paper. However, we would like\nto note that with the investigation of other fairness\nissues in human language more deeply conducted,\nif the biased words regarding other fairness issues\ncan be more specifically concluded, GEEP can be\ndirectly applied to address other fairness problems\nin pre-trained large language models.\n7\n1255\nB Appendix\nB.1 Hyper-parameters for SPPA and GEEP\nFor the main results presented in the paper of\nsecond-phase pre-training in GEEP and SPPA,\nwe further train RoBERTa-base for 100, 000\nsteps with our gender-neutral data. We use an\nAdamW optimizer with a learning rate of 1e −5,\nmax_seq_length of 128 and batch size 256. In\nGEEP method, we initialize the embedding of ev-\nery profession prompt with a normal distribution\nand standard deviations of 0.2.\nAlongside the final results, we also evaluate\nSPPA and GEEP during the second-phase pre-\ntraining. In Table 3 we show SPPA and GEEP’s\nperformance on pronoun coreference resolution at\nthe 20k iteration and 50k iteration. From Table 3\nwe can see that GEEP improves the pre-trained\nmodel’s gender fairness with much less number of\niterations. At 20k iteration, GEEP’s performance\nis already better than SPPA’s final performance on\nall 3 tasks. At 50k iteration, GEEP’s performance\nhas almost converged to its final scores on all 3\ntasks. While SPPA’s performance is still far behind\nits final performances on Winogender and WSC.\nB.2 Pronoun Coreference Resolution\nExperiment Setup\nPronoun Coreference Resolution is the task of link-\ning the pronouns with their references in a text.\nStudies show that BERT performance decreases\nin a text where the gender pronoun is female and\nthe topic is biased towards the male gender (Kurita\net al., 2019). To assess the performance of different\nmodels in pronoun coreference, we fine-tune our\nmodels with GAP data set (Webster et al., 2018)\nWe fine-tune each model for one epoch with a train\nbatch size of 64 and a learning rate of 5.0e −6.\nAfter fine-tuning, we evaluate the performance of\ndifferent models on three data sets:\n• Winogender: This dataset includes 1, 584 sen-\ntences with three mentions: a profession, a\nparticipant, and a pronoun (where the pro-\nnoun is referred to either profession or pro-\nnoun)(Rudinger et al., 2018).\n• WSC: The Winograd Schema Challenge\n(WSC) incorporates 273 sentences used\nfor commonsense reasoning for resolution\n(Levesque et al., 2012).\n• DPR: The Definite Pronoun Resolution (DPR)\ncorpus with 131 test sentences contains exam-\nples with two noun phrases and a pronoun or\npossessive adjective referring to one of the\nnoun phrases (Rahman and Ng, 2012).\nB.3 GLUE Experiment Setup\nTo evaluate how much each debiased model forgets\nafter second-phase pre-training, we fine-tune the\npre-trained models on GLUE (General Language\nUnderstanding Evaluation) to evaluate the perfor-\nmance of the pre-trained models. We follow pre-\nvious work to use eight tasks in GLUE, including\nCoLA, RTE, MRPC, STS, SST, QNLI, QQP, and\nMNLI. For evaluation metrics, we report Matthews\ncorrelation for CoLA, Pearson correlation for STS-\nB, and accuracy for other tasks. We use the same\noptimizer (Adam) with the same hyper-parameters\nas in pre-training. Following previous work, we\nsearch the learning rates during the fine-tuning for\neach downstream task. For a fair comparison, we\ndo not apply any published tricks for fine-tuning.\nEach configuration is run five times with different\nrandom seeds, and the average of these five results\non the validation set is calculated as the final per-\nformance of one configuration. We report the best\nnumber over all configurations for each task.\nB.4 Pronoun Prediction Experiment Setup\nand Results\nDifferent approaches have been proposed to quan-\ntify and analyze the gender bias in contextual lan-\nguage models (de Vassimon Manela et al., 2021;\nWebster et al., 2020; Kurita et al., 2019). For BERT,\nwe choose one approach that can be directly applied\nto a model pre-trained with Masked Language Mod-\neling (MLM) loss without further fine-tuning. In\nthis approach, we first define a template contain-\ning a pronoun and a profession. The profession\nis supposed to be gender-neutral however it is cur-\nrently viewed with gender bias to a large extent.\nBy masking the pronoun, the model is queried to\npredict the pronouns at the masked position given\nthe context, including the profession. Here is an\nexample, “[MASK]” is a registered nurse. The\ndifference between the probabilities of filling the\nmasked position in each sentence with \"he\" and\n\"she\", is used to show gender bias in the model,\nPronoun Bias Score = (3)\nProb(\"he\") −Prob(\"she\"). (4)\nTo assess fairness in BERT model, we consider303\nof professions used by (Bolukbasi et al., 2016). In\n8\n1256\nTable 3: The average accuracy of different models on Coreference Resolution task. The best results are in bold.\nData RoBERTa SPPA-20k GEEP-20k SPPA-50k GEEP-50k SPPA-100k GEEP-100k\nWinogender 50.9 51.6 64.3 54.6 64.5 57.3 64.5\nWSC 50.1 50.1 52.1 50.5 52.3 50.9 52.7\nDPR/WSCR 50.8 50.9 52.1 51.1 53.4 51.1 53.6\nAvg GLUE 86.5 82.7 85.9 80.7 84.5 80.2 83.3\nour study, we analyze a public available pre-trained\nBERT-Base model 1 that contains 12 layers, 768\nhidden nodes, 12 heads, and 110M parameters. Fig-\nure 2 shows gender bias of 60 of such professions\nin BERT-base model. Positive values mean that\nthe professions are biased towards male and vice\nversa. As the plots show, the contextual representa-\ntions of professions in BERT-base model exhibits\nstrong gender bias. Professions such as nurse and\nhousekeeper are viewed as jobs for females while\nsurgeon and mathematicians are assumed to be jobs\nfor males.\nTo find the reference of each pronoun in the tem-\nplate sentences, we follow (Kocijan et al., 2019)\napproach. Specifically, during the evaluation for\nevery data set, in each sentence there are two can-\ndidate nouns (such as “nurse” or “surgeon”) and a\npronoun. The pronoun is replaced with a [MASK]\ntoken, and the model makes a prediction at the\nmasked pronoun position from the two candidate\nnouns. In order to resolve a pronoun accurately, a\nmodel needs to overcome the biased link between\ngender and profession (e.g. a normative assump-\ntion that nurses are female) and instead make the\ndecision based on the available linguistic cues. We\nreport the prediction accuracy of all 3 methods on\nthe aforementioned three data sets.\nFigure 3 displays the pronoun prediction bias\nscore (defined in Equation 5) of all methods for\n60 biased professions defined in (Bolukbasi et al.,\n2016). Specifically, in both sub-figures, blue dots\nshow the pronoun prediction bias score from BERT-\nbase model for each profession. In Figure 3 (a),\nthe pink dots are the bias scores from BERT-SPPA\nmodel. We can see from this sub-figure that com-\npared with BERT-base, the bias scores from BERT-\nSPPA model are indeed closer to 0, indicating\nthat BERT-SPPA can mitigate gender bias of such\nprofessions to some extent. In Figure 3 (b), the\nblue dots are the bias scores from GEEP model.\nCompared with both BERT-SPPA and BERT-base,\nGEEP’s bias scores are significantly closer to 0,\nindicating that GEEP is more effective at removing\ngender bias from such biased professions compared\n1https://github.com/google-research/bert\nwith BERT-SPPA. Moreover, we also calculate the\naverage absolute pronoun prediction bias score for\nall 303 gender-neutral profession words in (Boluk-\nbasi et al., 2016). We obtain 0.44 for BERT-base,\n0.16 for BERT-SPPA and 0.13 for GEEP. GEEP\nmodel gets the lowest average bias with 70% re-\nduction compared to the BERT-base model.\nB.5 Analysis regarding SPPA’s performance\ndrop on GLUR\nWe conduct experiments to analyze reasons be-\nhind the GLUE performance drop of SPPA demon-\nstrated in Table 2 in our original submission. The\nperformance drop of SPPA compared to RoBERTa\ncan be of two reasons: 1) the model is further\ntrained with a subset of Wikipedia significantly\nsmaller than the RoBERTa pre-train data, which\ncould enforce the model to forget about the infor-\nmation embedded in the large RoBERTa pre-train\ndata; 2) we processed the subset of Wikipedia to\nmake them gender-neutral, which could potentially\nintroduce noise and distribution mismatch with the\ndownstream data. To provide a more detailed anal-\nysis, we conduct experiments as follows.\nFirst, starting from a pre-trained RoBERTa, we\nfurther train the model with SPPA on the same sub-\nset of Wikipedia that we used in main experiments\nwithout making the data subset gender-neutral. We\nname this model SPPA-without-GN (Gender Neu-\ntralization). We also run GEEP-without-GN to see\nwhether GEEP can still alleviate forgetting when\nthe data is just small but not debiased. For GEEP-\nwithout-GN, we further train a RoBERTa with the\nsame Wiki subset without gender neutralization.\nDuring this further training of GEEP-without-GN,\nwe follow GEEP to add and update new profession\nembeddings while freezing the rest entire model.\nGLUE results of SPPA-without-GN and GEEP-\nwithout-GN are in Table 4 in this pdf.\nBy comparing SPPA, SPPA-without-GN and\nthe original RoBERTa, we can find SPPA-without-\nGN performs better than SPPA while worse than\nRoBERTa. It suggests that both data subset se-\nlection and gender neutralization contribute to the\nperformance drop of SPPA compared to RoBERTa.\n9\n1257\nregistered_nurse\nnurse\nhousekeeper\nreceptionist\nfashion_designer\nnanny\ndancer\nviolinist\ncounselor\nsinger\nsergeant\nmarshal\nmajor_leaguer\nwarden\ngeologist\ncartoonist\nathletic_director\ncoach\ngangster\nskipper\nlyricist\ntrumpeter\nsenator\ninfielder\ncivil_servant\nboxer\nstockbroker\nmathematician\ncleric\nsportswriter\nwrestler\ncommentator\nhistorian\nlawyer\nminister\ndiplomat\nfootballer\nmagistrate\nguitarist\ncollector\nsolicitor_general\npoliceman\njurist\nheadmaster\nfarmer\nprosecutor\nsportsman\nprofessor_emeritus\nfighter_pilot\nactor\narchbishop\nbanker\nfisherman\nbaron\ncinematographer\ngoalkeeper\ncolonel\nindustrialist\npriest\nmidfielder\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nPronoun Prediction Bias Score\nFigure 2: An example of gender bias in 60 most biased profession words in BERT-base model. For each profession,\nwe measure the difference between the probability of filling the masked pronoun in each template sentence with\n\"he\" and \"she\" tokens. Some words such as nurse (-0.73) and receptionist (-0.57) are supposed to be gender neutral\nby definition but BERT-base model consider them as female professions. On the other hand, lawyer (0.74) and\nprosecutor (0.81) are considered as jobs for males.\nregistered_nurse\nnurse\nhousekeeper\nreceptionist\nfashion_designer\nnanny\ndancer\nviolinist\ncounselor\nsinger\nsergeant\nmarshal\nmajor_leaguer\nwarden\ngeologist\ncartoonist\nathletic_director\ncoach\ngangster\nskipper\nlyricist\ntrumpeter\nsenator\ninfielder\ncivil_servant\nboxer\nstockbroker\nmathematician\ncleric\nsportswriter\nwrestler\ncommentator\nhistorian\nlawyer\nminister\ndiplomat\nfootballer\nmagistrate\nguitarist\ncollector\nsolicitor_general\npoliceman\njurist\nheadmaster\nfarmer\nprosecutor\nsportsman\nprofessor_emeritus\nfighter_pilot\nactor\narchbishop\nbanker\nfisherman\nbaron\ncinematographer\ngoalkeeper\ncolonel\nindustrialist\npriest\nmidfielder\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nPronoun Prediction Bias Score\nBERT-SPPA\nBERT-base\n(a) Comparison between pronoun prediction bias in SPPA and BERT-base models\nregistered_nurse\nnurse\nhousekeeper\nreceptionist\nfashion_designer\nnanny\ndancer\nviolinist\ncounselor\nsinger\nsergeant\nmarshal\nmajor_leaguer\nwarden\ngeologist\ncartoonist\nathletic_director\ncoach\ngangster\nskipper\nlyricist\ntrumpeter\nsenator\ninfielder\ncivil_servant\nboxer\nstockbroker\nmathematician\ncleric\nsportswriter\nwrestler\ncommentator\nhistorian\nlawyer\nminister\ndiplomat\nfootballer\nmagistrate\nguitarist\ncollector\nsolicitor_general\npoliceman\njurist\nheadmaster\nfarmer\nprosecutor\nsportsman\nprofessor_emeritus\nfighter_pilot\nactor\narchbishop\nbanker\nfisherman\nbaron\ncinematographer\ngoalkeeper\ncolonel\nindustrialist\npriest\nmidfielder\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nPronoun Prediction Bias Score\nGEEP\nBERT-base\n(b) Comparison between pronoun prediction bias in GEEP and BERT-base models\nFigure 3: Difference between the probabilities of filling a masked pronoun with \"he\" and \"she\" tokens in the\ntemplate sentences containing 60 most biased professions. GEEP method outperforms the two other methods. For\nexample, the bias score for \"nurse\" token decreases from −0.7 in BERT-base to−0.5 in BERT-SPPA and0.1 in\nGEEP model.\n10\n1258\nWe would also like to note that GEEP-without-GN\noutperforms SPPA-without-GN as well and achieve\nsimilar GLUE score as RoBERTa. This indicates\nthat GEEP can also alleviate forgetting introduced\nby data subset selection effectively when there is\nnot gender-neutralizing procedure is taken.\nB.6 Discussions on non-binary gender\nidentities\nIn this discussion, we would like to start with the\npronoun choices for different gender identities. Be-\ncause in our submission we mainly try to address\nthe unfair pronoun preference of pre-trained mod-\nels. According to social research, gender-neutral\npronouns are more appropriate for referring to\ntransgender and non-binary individuals (Deutsch\nand Buchholz, 2015). ‘Zie’ and ‘hir’ are specific\nto transgender community, but people outside of\nthe community are not familiar with these pro-\nnouns. Deutsch and Buchholz (2015) has proposed\na Gender-ID to pronoun mapping for transgen-\nders and Genderqueer in electronic health records\n(EHR). In this system, transgenders are mapped to\nhe/his or she/her where there exists gender bias, but\ngenderqueer are mapped to they/them. For people\nwho prefer binary pronouns(he/she) regardless of\ntheir gender identities, our experiments still hold\nbecause the pronoun coreference resolution tasks\nthat we evaluate on, i.e. Winogender, WSC and\nDPR/WSCR, are all binary-pronoun tasks.\nHowever, an alternative to asking for preferred\npronouns would be to use singular pronouns to\naddress everyone until the individual indicates a\npreference to use certain pronouns and/ or reveal\ntheir gender identity (Darr and Kibbey, 2016). One\noptional term that is already used as a singular\npronoun like \"they/their\" (Darr and Kibbey, 2016;\nRichards et al., 2016; Sun et al., 2021). If such sin-\ngular pronoun can be promoted to a larger commu-\nnity, the pronoun unfairness issue can be resolved\nfrom the data fundamentally.\nB.7 The capacity increase of GEEP compared\nto SPPA\nBy adding profession embeddings, it is true that\nthe total number of model parameters slightly in-\ncreases. However, the entire size of the newly-\nadded parameters is 303*768=232k, which is only\n0.21% of the original RoBERTa parameter size\n(110 million). 303 is the number of professions\nand 768 is the embedding size of RobERTa. There-\nfore, even if we extend this method to other fair-\nness problems in the future and add more new word\nembeddings such as 3000 words or 10000 words,\nthe newly-added parameters would be just around\n2% or 9% of the original parameter size, which\nwouldn’t cause serious scaling issue.\nMoreover, we run a new SPPA variant that has\nthe same capacity (the same number of parameters)\nwith GEEP. In the new SPPA variant, we conduct\nSPPA training after adding new word embedding\nof the profession names, same as GEEP. We re-\nfer this model as SPPA-with-NPE (new profession\nembeddings). The difference between SPPA-with-\nNPE and GEEP is GEEP’s core implementation\nto prevent forgetting, that GEEP freezes the rest\nparameters during further training and only update\nnew profession embeddings. While SPPA-with-\nNPR updates all parameters including the original\nmodel parameters and the newly added profession\nembeddings. When encountering the pre-defined\nprofession names in training or fine-tuning, SPPA-\nwith-NPR also updates their new embeddings in-\nstead of old word/token embeddings. GLUE re-\nsults are shown in Table 4. Compared with SPPA,\nSPPA-with-NPE can alleviate forgetting slightly\nand achieve better debiasing results, while still sig-\nnificantly under-perform GEEP. Results on pro-\nnoun coreference resolution tasks show the same\ntrend. SPPA-with-NPE got 58.6 on Winogender,\n51.3 on WSC and 52.4 on DPR/WSCR. They are\nall slightly better than SPPA while significantly\nlower than GEEP.\nB.8 Quality of gender-neutral data\nThe relatively big performance drop of both\nour method and SPPA compared to the original\nRoBERTa motivates us to analyze more on the qual-\nity of our gender-neutral data.\nWhile first we note that CoLA and RTE are\nknown to be more sensitive to quality of pre-trained\nmodels compared with other tasks in GLUE, due\nto their small data sizes. In other words, if the\npre-trained model is trained insufficiently or with\nless data, we can see a larger performance drop on\nCoLA and RTE compared with other tasks. While\nif the pre-trained model’s quality is better, we can\nsee larger improvements on them as well. This\ntrend has been observed in BERT vs RoBERTa,\nBERT vs Span-BERT, and BERT vs ELECTRA.\nTherefore, the reason for the large performance\ndrop on COLA can partially be its natural sen-\nsitivity to our small data size of further training\n11\n1259\nTable 4: GLUE results. The best results among SPPA and GEEP are in bold.\nTask RoBERTa SPPA GEEP SPPA-without-GN GEEP-without-GN SPPA-with-NPE\nMNLI 87.7 87.2 87.7 87.3 87.7 87.2\nQNLI 92.4 92.4 92.4 92.3 92.4 92.3\nQQP 91.8 91.3 91.7 91.4 91.8 91.5\nSST-2 95.4 94.7 95.4 95.0 95.4 94.7\nCoLA 64.1 38.9 50.5 40.2 59.6 39.3\nMRPC 91.4 88.8 89.8 88.8 90.5 88.8\nRTE 78.4 60.2 68.7 66.4 73.1 61.0\nSTS-B 90.7 88.3 89.9 89.5 90.4 88.5\nA VG 86.5 80.2 83.3 81.4 85.1 80.4\nTable 5: GLUE results. The best results are in bold.\nTask BERT-base BERT-SPPA GEEP\nMNLI 84.3 84.0 84.1\nQNLI 91.4 90.0 91.3\nQQP 90 90.1 90.4\nSST-2 93 92.2 92.4\nCoLA 54.0 52.0 53.0\nMRPC 85.7 84.1 84.9\nRTE 69.4 69.8 69.1\nSTS-B 88.0 88.0 87.0\nA VG 82.0 81.3 81.6\nRoBERTa.\nSecond, the gender neutralization process of the\ntraining data could cause gender mismatch between\npronouns and some very rare nouns. we did follow\nthe reviewer’s suggestion to sample 500 sentences\nfrom the augmented dataset and manually checked\nwhether there are grammar errors. In these 500\nsentences, there are no grammar errors, such as\nmismatches between nouns and verb formats (e.g.\n\"he are\"). Because during the gender neutralization,\nwe follow previous work to just swap the gender-\nrelated pronouns (such as he/she) or nouns (such\nas uncle/aunt) when profession names occur. And\nsuch gender-related nouns share the same verb for-\nmats with their counterparts. We also share the\nfull list of gender-related nouns in the appendix\nin this submission. However, when we sample\nmore modified sentences, we find that if a rare\ngender-related noun, such as “spinster”, that is not\non the published gender-related noun list occurs,\nthe gender neutralization process would change the\npronoun while leave the noun unchanged since it\nis not on the list. Although it happens quite rarely,\nthis causes pronoun misuse that could lead to gram-\nmar errors in pre-training data that contribute to the\nperformance drop on CoLA.\nB.9 Experiment Results on BERT\nDuring the preliminary exploration on this problem,\nwe have also applied SPPA and GEEP on publicly\nTable 6: The average accuracy of different models on\nCoreference Resolution task. The best results are in\nbold.\nData BERT-base BERT-SPPA GEEP\nWinogender 50 50.7 62.9\nWSC 50.1 50.2 50.5\nDPR/WSCR 50.7 50.9 52.8\nreleased BERT and conducted pronoun coreference\nresolution and GLUE experiments on them. In this\nexperiment, we only further trained the released\nBERT model for 10k iterations with our gender-\nneutral data. Moreover, our gender-neutral data\nset (7.1 GB) is not significantly smaller than the\noriginal pre-training data of BERT (16 GB), and\nthe two data sets both come from Wikipedia. Due\nto these two reasons, the forgetting problem on this\nBERT experiment is not as obvious for SPPA.\nTable 5 shows the performance of different meth-\nods on 8 GLUE tasks. Although the forgetting is\nless server, SPPA still suffers from forgetting issue\nin the following 6 tasks out of the total 8 tasks,\nCoLA, MRPC, STS-B, MNLI, QNLI, and SST-2.\nAs for the average GLUE score, SPPA is 0.7 point\nlower after its second-phase pre-training, which is\nnot a small margin considering it is the average\nscore of 8 tasks. GEEP mitigates the forgetting is-\nsue of SPPA in all sub-tasks except in RTE. GEEP\nalso gets the average GLUE score of 82.8, which\noutperforms SPPA and is similar to the original\nGLUE score of the pre-trained BERT.\nTable 6 shows the coreference resolution results\nof different models on three data sets. Results show\nthat GEEP model obtains the best accuracy com-\npared to other models, especially in Wingender\ndataset where the candidate nouns are professions.\nWe observe that the SPPA method also can help\nimprove coreference resolution performance of the\npre-trained model, but not as effective as GEEP.\n12\n1260\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□ A1. Did you describe the limitations of your work?\nLeft blank.\n□ A2. Did you discuss any potential risks of your work?\nLeft blank.\n□ A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□ A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □ Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □ Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1261\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □ Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n1262",
  "topic": "Forgetting",
  "concepts": [
    {
      "name": "Forgetting",
      "score": 0.9625202417373657
    },
    {
      "name": "Computer science",
      "score": 0.7263031601905823
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.6928837299346924
    },
    {
      "name": "Training set",
      "score": 0.6040636301040649
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5164644718170166
    },
    {
      "name": "Focus (optics)",
      "score": 0.47926589846611023
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4730842709541321
    },
    {
      "name": "GLUE",
      "score": 0.44130146503448486
    },
    {
      "name": "Language model",
      "score": 0.4378582537174225
    },
    {
      "name": "Work (physics)",
      "score": 0.427178293466568
    },
    {
      "name": "Machine learning",
      "score": 0.3902307450771332
    },
    {
      "name": "Cognitive psychology",
      "score": 0.2380705177783966
    },
    {
      "name": "Psychology",
      "score": 0.22456324100494385
    },
    {
      "name": "Engineering",
      "score": 0.07566779851913452
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210155268",
      "name": "Salesforce (United States)",
      "country": "US"
    }
  ]
}