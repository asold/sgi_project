{
  "title": "A CTC Alignment-Based Non-Autoregressive Transformer for End-to-End Automatic Speech Recognition",
  "url": "https://openalex.org/W4361990931",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5080637906",
      "name": "Ruchao Fan",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A5056008057",
      "name": "Wei Chu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5043681842",
      "name": "Chang Peng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5112421995",
      "name": "Abeer Alwan",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3095311338",
    "https://openalex.org/W3211278025",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W6638749077",
    "https://openalex.org/W3016010032",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W6746208923",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W3100753857",
    "https://openalex.org/W6789821389",
    "https://openalex.org/W3112157188",
    "https://openalex.org/W3097882114",
    "https://openalex.org/W3164692279",
    "https://openalex.org/W3097874139",
    "https://openalex.org/W4225644313",
    "https://openalex.org/W3196500669",
    "https://openalex.org/W3169714379",
    "https://openalex.org/W6774835902",
    "https://openalex.org/W3197140813",
    "https://openalex.org/W3147414526",
    "https://openalex.org/W3162431424",
    "https://openalex.org/W3198259287",
    "https://openalex.org/W3160799772",
    "https://openalex.org/W2972389417",
    "https://openalex.org/W6769935647",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W6751097180",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W3095173472",
    "https://openalex.org/W4246193833",
    "https://openalex.org/W3015960524",
    "https://openalex.org/W3162249256",
    "https://openalex.org/W3197148831",
    "https://openalex.org/W2981857663",
    "https://openalex.org/W6780218876",
    "https://openalex.org/W2766219058",
    "https://openalex.org/W3015974384",
    "https://openalex.org/W3028545098",
    "https://openalex.org/W2899423466",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2963242190",
    "https://openalex.org/W6691770337",
    "https://openalex.org/W2107162140",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2407080277",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2972439411",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W3203407300",
    "https://openalex.org/W6769921281",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W3206573929",
    "https://openalex.org/W4293793697",
    "https://openalex.org/W4221151577",
    "https://openalex.org/W4283073456",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287117559",
    "https://openalex.org/W3126267552",
    "https://openalex.org/W2251321385",
    "https://openalex.org/W2973513151",
    "https://openalex.org/W2985287635",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W4295253143",
    "https://openalex.org/W2995999067",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W4291566970",
    "https://openalex.org/W4288072840",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W4287989347",
    "https://openalex.org/W3035445001",
    "https://openalex.org/W2097117768"
  ],
  "abstract": "Recently, end-to-end models have been widely used in automatic speech\\nrecognition (ASR) systems. Two of the most representative approaches are\\nconnectionist temporal classification (CTC) and attention-based encoder-decoder\\n(AED) models. Autoregressive transformers, variants of AED, adopt an\\nautoregressive mechanism for token generation and thus are relatively slow\\nduring inference. In this paper, we present a comprehensive study of a CTC\\nAlignment-based Single-Step Non-Autoregressive Transformer (CASS-NAT) for\\nend-to-end ASR. In CASS-NAT, word embeddings in the autoregressive transformer\\n(AT) are substituted with token-level acoustic embeddings (TAE) that are\\nextracted from encoder outputs with the acoustical boundary information offered\\nby the CTC alignment. TAE can be obtained in parallel, resulting in a parallel\\ngeneration of output tokens. During training, Viterbi-alignment is used for TAE\\ngeneration, and multiple training strategies are further explored to improve\\nthe word error rate (WER) performance. During inference, an error-based\\nalignment sampling method is investigated in depth to reduce the alignment\\nmismatch in the training and testing processes. Experimental results show that\\nthe CASS-NAT has a WER that is close to AT on various ASR tasks, while\\nproviding a ~24x inference speedup. With and without self-supervised learning,\\nwe achieve new state-of-the-art results for non-autoregressive models on\\nseveral datasets. We also analyze the behavior of the CASS-NAT decoder to\\nexplain why it can perform similarly to AT. We find that TAEs have similar\\nfunctionality to word embeddings for grammatical structures, which might\\nindicate the possibility of learning some semantic information from TAEs\\nwithout a language model.\\n",
  "full_text": "IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2023 1\nA CTC Alignment-based Non-autoregressive\nTransformer for End-to-end Automatic Speech\nRecognition\nRuchao Fan, Student Member, IEEE,Wei Chu, Peng Chang, and Abeer Alwan, Fellow, IEEE\nAbstract—Recently, end-to-end models have been widely used\nin automatic speech recognition (ASR) systems. Two of the\nmost representative approaches are connectionist temporal clas-\nsiﬁcation (CTC) and attention-based encoder-decoder (AED)\nmodels. Autoregressive transformers, variants of AED, adopt an\nautoregressive mechanism for token generation and thus are\nrelatively slow during inference. In this paper, we present a\ncomprehensive study of a CTC Alignment-based Single-Step Non-\nAutoregressive Transformer (CASS-NAT) for end-to-end ASR. In\nCASS-NAT, word embeddings in the autoregressive transformer\n(AT) are substituted with token-level acoustic embeddings (TAE)\nthat are extracted from encoder outputs with the acoustical\nboundary information offered by the CTC alignment. TAE can\nbe obtained in parallel, resulting in a parallel generation of\noutput tokens. During training, Viterbi-alignment is used for\nTAE generation, and multiple training strategies are further\nexplored to improve the word error rate (WER) performance.\nDuring inference, an error-based alignment sampling method\nis investigated in depth to reduce the alignment mismatch in\nthe training and testing processes. Experimental results show\nthat the CASS-NAT has a WER that is close to AT on various\nASR tasks, while providing a ∼24x inference speedup. With and\nwithout self-supervised learning, we achieve new state-of-the-art\nresults for non-autoregressive models on several datasets. We also\nanalyze the behavior of the CASS-NAT decoder to explain why\nit can perform similarly to AT. We ﬁnd that TAEs have similar\nfunctionality to word embeddings for grammatical structures,\nwhich might indicate the possibility of learning some semantic\ninformation from TAEs without a language model.\nIndex Terms—CTC alignment, non-autoregressive transformer,\nend-to-end ASR, intermediate loss.\nI. I NTRODUCTION\nE\nND-to-end models have proven successful for speech\nrecognition because of their ability to play the role of\nthe acoustic, pronunciation, and language model in one single\nneural network [1], [2]. Training the above components to-\ngether leads to fewer intermediate errors and thus a lower word\nerror rate (WER) for ASR systems. This training mechanism\nalso requires fewer model parameters, which is suitable for\non-device deployment. Connectionist temporal classiﬁcation\n(CTC) [3], attention-based encoder decoder (AED) [4], and\nRNN-Transducers [5], [6] are the most widely used end-\nto-end models. CTC has a high decoding efﬁciency when\nR. Fan and A. Alwan are with the Department of Electrical and Computer\nEngineering, University of California, Los Angeles, CA, 90095 USA (e-mail:\nfanruchao@g.ucla.edu, alwan@ee.ucla.edu).\nW. Chu and P. Chang are with PAII Inc., CA, USA, (email:\n{chuwei129,changpeng805}@pingan.com.cn).\nThis work is supported in part by the NSF and UCLA-Amazon Science\nHub.\nusing the best path decoding strategy, but it is restricted by\nits assumption of conditionally independent outputs. AED,\nlike the autoregressive transformer (AT) [7], [8], models\noutput dependencies by incorporating a language-model-style\ndecoder. However, the decoding in AT adopts an autoregressive\nmechanism for joint probability factorization, leading to a step-\nby-step generation of output tokens. Such a mechanism lowers\nthe inference speed for ASR, which is an essential factor when\ndesigning an efﬁcient ASR system.\nRecently, non-autoregressive mechanisms have received in-\ncreasing attention for their decoding efﬁciency, enabled by\ngenerating output tokens in parallel [9]–[14]. There are two\nmajor types of Non-Autoregressive methods for Transformers\n(NAT): (i) iterative NATs, and (ii) single-step NATs or one-\nshot NATs. The prevailing iterative NATs relax the strict non-\nautoregressive condition and iteratively generate outputs with\nK decoding passes. Thus, iterative NATs are sometimes called\n“semi-NAT”. Single-step NATs, however, can generate the\noutput sequence in one iteration. Different from the methods\nin neural machine translation that extend encoder input as\nthe decoder input, single-step NATs for speech recognition\nextract high-level acoustic representations as the decoder input,\nassuming that language semantics can be captured by the\nacoustic representations [15]–[17]. These acoustic represen-\ntations, however, are either implicit, extracted by attention\nmechanism [15] or incomplete using only CTC spikes [16],\nwhich make learning language semantics difﬁcult.\nNon-autoregressive methods continue to be proposed based\non CTC because of its efﬁciency [18]. For example, Chi et\nal. proposed to train a reﬁner to iteratively improve CTC\nalignment based on the previous outputs of the reﬁner [19]. In\n[20], CTC alignment is enhanced with a mask token as a prior\ninformation for the decoder in each iteration. Nozaki et al.\nalleviate the output-independent problem of CTC by using in-\ntermediate predictions as additional inputs [21]. Furthermore,\n[22] improves the WER performance of a pure CTC model by\na large margin using Wav2vec2.0 pretraining techniques. The\nperformance of these methods, however, is still worse than\ntheir autoregressive transformer (AT) counterparts.\nIn this paper, we present a comprehensive study of a NAT\nframework by utilizing alignments over the CTC output space.\nThe framework can generate the output sequence within one\niteration, so we refer to it as CTC Alignment-based Single\nStep NAT (CASS-NAT). In CASS-NAT, there are four major\nmodules: encoder, token-level acoustic embedding extractor\n(TAEE), self-attention decoder (SAD), and mixed-attention\narXiv:2304.07611v1  [cs.CL]  15 Apr 2023\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2023 2\ndecoder (MAD). The encoder is used to extract a high-level\nacoustic representation for each frame. The TAEE extracts a\nmore meaningful token-level acoustic embedding (TAE) using\nthe information given by alignments over the CTC output\nspace. The SAD and MAD model the dependencies between\nTAEs, where MAD considers encoder outputs directly for the\npurpose of source-attention while SAD does not. However,\nSAD indirectly uses the information from the encoder through\nthe TAEs for self-attention. Since TAEs can be obtained in\nparallel, no recurrence in output sequence generation exists.\nMeanwhile, the two decoder modules can model the depen-\ndencies between TAEs in the latent space.\nWe summarize the contributions of this work as: 1)\ndetailed experiments to examine the effect of various decoder\nstructures on the WER, while the settings of SAD and MAD in\n[23] were intuitively selected; 2) an investigation of the impact\nof the hyper-parameters on the proposed error-based sampling\nalignment (ESA) method, such as the sampling threshold, the\nnumber of sampled alignments, and the scoring model for\nranking the sampled alignments. The investigation reveals a\ntrade-off between accuracy and inference efﬁciency, which was\nnot covered in [23]; 3) comparisons of the effectiveness of each\nindividual training strategy in [24] and their combinations are\npresented for a better understanding of the proposed training\nstrategies. Knowledge distillation, which is not covered in [24],\nis also included in this work; 4) an investigation of various\nencoder initialization schemes (including AT encoder, CTC\nencoder, and random initialization) for CASS-NAT training\nsince the quality of the CTC alignment is highly relevant\nto model accuracy. The HuBERT encoder [25] is included\nin the comparison as well. Such an investigation was not\ndone in earlier publications; and 5) use the proposed methods\non diverse datasets (LibriSpeech: adult-read, TED2: adult-\nspontaneous, MyST: child-spontaneous, and Aishell1: adult-\nMandarin-read) to validate the generalizability of our algo-\nrithm, and obtain new state-of-the-art ASR results for non-\nautoregressive models. Our earlier publications reported results\non only LibriSpeech and Aishell1.\nThe remainder of the paper is organized as follows. Section\nII introduces the background of end-to-end models and related\nwork. Section III describes the CASS-NAT framework, includ-\ning system architecture and training and inference strategies.\nExperimental setups are described in Section IV, and results\nare shown and discussed in Section V. We conclude the paper\nin Section VI.\nII. B ACKGROUND\nWe ﬁrst review the important concepts behind the pro-\nposed methods and provide basic notations. Let X =\n(x1,...,x t,...,x T) denote the input sequence, where xt con-\ntains speech features of frame t. Y = (y1,...,y u,...,y U) is\nthe output sequence, where yu is a token at position u. The\ngoal of speech recognition tasks is to ﬁnd the best probable\ntranscription Y given acoustic information X, which can be\nformulated as Y∗= argmax\nY\nP(Y|X).\nA. Autoregressive Models\nRecently, transformers are shown to be the best-performing\nautoregressive models (AT) [8], [26]. AT adopts an encoder-\ndecoder structure, where the decoder generates each token\nconditioned on all previous tokens. This architecture de-\nsign achieves sequence modelling by a chain of conditional\nprobabilities, where each conditional probability constructs a\nclassiﬁcation problem. The AT model is then trained through\nan objective function as follows:\nLAT = −log P(Y|X) =−log\nU∏\ni=1\nP(yi|y<i,X)\n= −\nU∑\ni=1\nlog P(yi|y<i,X)\n(1)\nwhere P is the probability distribution of the AT model and\ny<i are all previous tokens before the ith token.\nAT can be trained efﬁciently by using all ground-truth\ntokens as the decoder input (teacher-forcing) or using parallel\nscheduled sampling [27]. During inference, however, a beam\nsearch algorithm is used to obtain the most probable sequences\nover the search space. The beam search and a requirement of\nusing history tokens for generation, destroy the parallelism in\nAT, leading to low inference speed.\nB. Non-autoregressive Models\nNon-autoregressive models have no strict dependencies be-\ntween tokens. For example, CTC has a conditional indepen-\ndence assumption, while Mask-CTC [14] trains the decoder as\na masked language model to build a weak dependency between\nmasked and unmasked tokens. By relaxing the dependency\nassumption, it is possible to generate all tokens in parallel,\nand thus increase inference speed.\n1) CTC and its Alignment:The CTC model has no depen-\ndency assumption between tokens. The posterior probability\nof Y given X can be directly computed by a multiplica-\ntion of the probability for each frame with each other. Let\nZ = {z1,...,z t,...,z T}be the output of the model, where zt\nstands for the output at time step tcorresponding to the input\nxt. The length of Z, however, is always longer than that of Y\nfor a speech recognition task. To compute the loss, a special\nblank token b is added in the vocabulary; b can be predicted\nas an output of Z at any time step. During inference, b and\nrepeated tokens are removed to obtain a shorter sequence Y,\nwhere this process can be deﬁned as a mapping rule β. During\ntraining, there exist multiple Zs that can be mapped to Y using\nthe rule β. As a consequence, CTC loss is a summation of all\nsuch Zs, which is formulated as follows [3]:\nLCTC = −log P(Y|X) =−log\n∑\nZ∈β−1(Y)\nP(Z|X)\n= −log\n∑\nZ∈β−1(Y)\nT∏\nt=1\nP(zt|X)\n(2)\nwhere β−1 is the inverse of the mapping rule.\nAn aligned relationship between all time steps in X and\ntokens in Y is presented in each Z. Thus, Z is also called an\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2023 3\nCTC alignment, which is similar to the alignment in HMM-\nbased ASR systems. In this paper, we consider the beneﬁts of\nthe information offered by the alignment Zto achieve a single-\nstep NAT for end-to-end speech recognition. Z∗is referred to\nas the Viterbi-alignment when it has the maximum probability\nof being mapped to the ground truth Y.\n2) Single-step NAT: Single-step NATs use the encoder-\ndecoder structure, making the output the same length as Y.\nThe output tokens are still conditionally independent of each\nother, just like CTC. But there is an implicit assumption that\nlanguage semantics can be captured by high-level acoustic\nrepresentations, which are similar to word embeddings. The\nmodel is updated using a cross entropy loss for each token,\nwhich can be formulated as:\nLNAT = −log P(Y|X) =−\nU∑\ni=1\nlog P(yi|X) (3)\nC. Other Related Works\n1) Convolution-augmented self-attention: The self-\nattention module in transformers captures global information\nby a weighted summation of the whole sequence. However,\nlocal information is also important for sequence modelling.\nTaking speech features as an example, frequency details\nin each vowel or consonant helps the recognition of these\nsounds. In computer vision, convolution layers have proved\nto be good at capturing local details within a kernel [28].\nRecent works adopt this idea and augment transformers with\na convolution module [29]–[31] in ASR. Relative positional\nencoding is also used in each self-attention module. The\nconvolution-augmented self-attention block can effectively\nimprove performance, but the improvement is only signiﬁcant\nif applied in the AT encoder. The AT decoder, on the\nother hand, adopts a causal structure (upper triangular mask\nmatrix for attention) that captures less local details with a\nconvolution module, and thus the improvement would not\nbe signiﬁcant in that case. In this paper, we explore the use\nof convolution-augmented self-attention layers for the NAT\ndecoder in addition to the encoder.\n2) Intermediate Loss: Deep transformers always suffer\nfrom gradient vanishing, especially for parameters that are\ndistant from the output layers. Intermediate loss has previously\nbeen proposed to add additional loss functions after each layer\nto boost the gradient update [32], [33]. It has been proven\nuseful to use intermediate CTC loss for improving the per-\nformance of the CTC model [34], [35]. In [36], intermediate\nCE loss is used for training deep transformer-based acoustic\nmodels for an HMM-based hybrid ASR system. In this paper,\nwe combine the usage of intermediate CTC and CE loss in\nthe proposed framework.\n3) Self-supervised Learning: Self-supervised learning\n(SSL) has been popular in recent years, especially for\nlow-resource tasks [25], [37]. We use HuBERT in our\nexperiments to further improve the system performance.\nDuring pretraining, the HuBERT model reconstructs the\nmasked input given unmasked areas. We take advantage\nof self-supervised pretraining to improve the modelling\ncapability of the encoder, and thus the quality of the CTC\nConv2D x 2\nNCM\nFilter-bank\nLayerNormPositional\nEncoding\nCTC Loss\nNCM\nToken-level Acoustic Embedding\nLinear + Softmax\nCE Loss\nLinear + Softmax\nAlignment Z\nEncoder\nTM\nPositional\nEncoding\nToken Acoustic\nEmbedding Extractor\nMixed-attn Decoder\nJoint Training\nSelf-attn Decoder\nX\nCM,\nNCM\nTM,\nNCM\nFig. 1: An overview of the proposed CASS-NAT architecture.\nCM and NCM represent a causal and non-causal mask, re-\nspectively. TM stands for trigger mask.\nalignment, which is important for accurate language semantics\nmodelling in the NAT decoder.\nIII. P ROPOSED FRAMEWORK : CASS-NAT\nIn this section, we introduce the proposed CTC Alignment-\nbased Single Step Non-autoregressive Transformer (CASS-\nNAT). We describe the mathematical derivation of the objec-\ntive function, training strategies to improve performance, and\nthe proposed sampling-based decoding strategy.\nA. System Architecture\nThe proposed CASS-NAT system architecture builds upon\nthe CTC/Attention hybrid architecture [38] to be non-\nautoregressive using CTC alignments. Fig.1 shows the four\nmajor modules in CASS-NAT: encoder, token acoustic em-\nbedding extractor (TAEE), self-attention decoder (SAD) and\nmixed-attention decoder (MAD).\n1) Mask in attention : The attention mechanism is im-\nportant for a transformer. The most basic computation in\nthe attention mechanism is scaled dot-product self-attention\nusing a sequence as input. Conventional self-attention utilizes\ninformation across the whole sequence. But each output of\nthe self-attention mechanism is not necessarily dependent on\nall of the input sequence. For example, a neural language\nmodel uses an upper triangular matrix (mask matrix) to gather\ninformation from only past tokens in a sequence. In Fig. 1,\nwe show three mask matrices for different purposes in the\nfour major modules. Since we do not consider a streaming\nASR model at this moment, non-causal mask (NCM) is used\nin the encoder. The NCM is a matrix where the paddings\nare zeros to prevent the padded tokens or padded frames\nfrom attention computation. In TAEE, a trigger mask (TM) is\nused to extract accurate token-level acoustic embeddings. TM\nmarks out the triggered frames, such that the positions of used\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2023 4\nframes are marked as ones, while other positions are marked\nas zeros. Examples of TM can be found in Section III-A3.\nMAD contains both self-attention and cross-attention layers,\nwhich are similar to an AT decoder. In such cases, either a\ncausal mask (CM) or NCM can be used for the self-attention\nlayer, and either a NCM or TM can be used for the cross-\nattention layer. The different choices of the mask matrices in\nMAD are explored experimentally in Section V-A. Eventually,\nthe self-attention computation can be augmented with a mask\nmatrix as follows:\nAttention(Q,K,V,M ) =\n(\nSoftmax\n(QKT\n√dk\n)\n⊗M\n)\nV\n(4)\nwhere Q ∈ Rnq×dq , K ∈ Rnk×dk , V ∈ Rnv×dv , and\nM ∈Rnq×nk are the query, key, value and mask matrices,\nrespectively.\n2) Encoder: The encoder extracts high-level acoustic rep-\nresentations H from speech features X. A linear layer with a\nCTC loss function is added after the encoder as shown in Fig.1.\nThe role of CTC is to obtain an alignment over the CTC output\nspace to offer auxiliary information for the token acoustic em-\nbedding extractor (TAEE). During training, Viterbi-alignment\nis used. During inference, various methods for sampling from\nthe CTC output space are explored experimentally.\n3) Token Acoustic Embedding Extractor : The token\nacoustic embedding extractor is designed to extract token-level\nacoustic embedding (TAE) with the auxiliary information of-\nferred by the CTC alignment. For example, given an alignment\nZ = {z1,...,z t,...,z T}, we can estimate an acoustic segment\nfor each token u as {tu−1 + 1,...,t u}(note that 1 here refers\nto one frame), and the number of tokens in Z.\nFirst, CTC alignments offer an acoustic boundary for each\ntoken yu, which is then transformed into the trigger mask.\nSpeciﬁcally, we deﬁne a mapping rule from alignment to\ntrigger mask, and ﬁx the rule in both the training and inference\nphases. We regard the ﬁrst non-blank index of each token\nin the alignment as its end boundary. The intuition is our\nassumption that the model will not output a token until\nit sees all the acoustic information.of the token. Using the\nﬁrst non-blank index is for simplicity and consistency in\ntraining and decoding. For example, if an alignment is Z =\n{ ,C,C, ,A, , ,T, }for the ground truth Y = {C,A,T },\nwhere is the blank symbol, the end boundary for C and Ais\nZ2 and Z5, respectively, and thus the trigger mask for token\nA is [0,0,1,1,1,0,0,0,0]. The mapping rule might not be\naccurate for acoustic segmentations, but it should be consistent\nduring the training and decoding. The trigger mask here is\ndifferent from that used in [39] for streaming purposes. In\n[39], previous acoustic representations could be reused for\neach token, and thus the trigger mask in the previous example\nfor token A is [1,1,1,1,1,0,0,0,0].\nSecond, CTC alignments provide the number of tokens\nfor the decoder input. After removing blank symbols and\nrepetitions, the number of tokens in an alignment Z is used as\nthe predicted length of sinusoidal positional encoding (decoder\ninput length). As shown in Fig. 1, TAE for each token\nis then extracted with the trigger mask and the sinusoidal\npositional encoding using a one-layer source-attention block.\nThe TAEs replace the word embeddings in AT to achieve\nparallel generation of each sequence.\n4) Self-attention Decoder: TAEs extracted from TAEE (see\nSection III-A3) have the good property of parallel generation\nand thus is used as a substitution of word embeddings for the\ndecoder input. Since there is no need to create recurrence in\nthe decoder, we use a non-causal mask (NCM) in the self-\nattention decoder (SAD) to model the relationships between\nTAEs.\n5) Mixed-attention Decoder: We assume that TAE has a\nsimilar capability of learning language semantics compared\nto the word embedding. Hence, we design a mixed-attention\ndecoder (MAD) to retrace the encoder information for better\ndecision making at the output layer. Similar to an AT decoder,\nMAD has a self-attention layer that uses either CM or NCM,\nand a source-attention layer that uses either TM or NCM. A\nlinear layer is added after MAD, followed by a cross-entropy\nloss. Since we use the Viterbi-alignement during training, the\noutput has the same length as the ground truth Y.\nB. Training Details\nThe training criterion is presented in this section, followed\nby various training strategies used to improve the performance\nof CASS-NAT.\n1) Training Criterion: In our framework, CTC alignments\nZ are introduced as latent variables. Given X and Y in Sec.II,\nthe log-posterior probability can be decomposed into:\nlog P(Y|X) = logEZ|X[P(Y|Z,X)], Z ∈q. (5)\nwhere q is the set of alignments that can be mapped to Y.\nFor those alignments that do not belong to q, we assume that\nP(Y|Z,X) is equal to zero. To reduce computational cost, the\nmaximum approximation [40] is applied:\nlog P(Y|X) ≥EZ|X[log P(Y|Z,X)]\n≈max\nZ\nlog\nU∏\nu=1\nP(yu|ztu−1+1:tu ,x1:T) (6)\nwhere E represents the expectation and tu is the end boundary\nof token u (t0 = 0). All tus can be estimated from the\nalignment Z. We expect that TAEs can capture language\nsemantics to a certain degree (see Section V-E for analysis),\nwhich helps alleviate performance degradation caused by the\nindependence of the output tokens.\nThe framework is trained by jointly optimizing a CTC loss\nfunction on the encoder side (LCTC) and a cross entropy (CE)\nloss function on the decoder side ( Ldec) with a task ratio λ\n[38], and thus the ﬁnal loss function ( Ljoint) is deﬁned as:\nLjoint = −λ·log\n∑\nZ∈q\nT∏\ni=1\nP(zi|X) −log\nU∏\nu=1\nP(yu|z∗\ntu−1+1:tu ,X)\n(7)\nwhere P is the probability distribution, Z∗is the most probable\nalignment (Viterbi-alignment). The second term is a maximum\napproximation for the log-posterior probability as computed by\nEq. 6.\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2023 5\n2) Convolution Augmented Self-attention Block: The self-\nattention computation in Eq.4 considers global information\nacross the sequence, but ignores local details. To alleviate\nthis problem, convolution augmented self-attention blocks are\nproposed to emphasise the modelling of local dependencies\nof the input sequence in the encoder [30], [41]. Different\nfrom previous work, we apply the convolution augmented self-\nattention blocks in the SAD and MAD as well. Speciﬁcally,\nthe feed-forward layer is decomposed into two sub-layers to be\nplaced at the beginning and the end of the block. A convolution\nlayer similar to that in [30] is inserted after the self-attention\nlayer except that we empirically use layer normalization in-\nstead of batch normalization. The ﬁnal computation in the ith\nMAD can be formulated as:\nˆsi = si + 1\n2FFN(si) (8)\ns\n′\ni = ˆsi + LN(Attn( ˆsi,ˆsi,ˆsi,NCM)) (9)\ns\n′′\ni = s\n′\ni + Conv(s\n′\ni) (10)\ns\n′′′\ni = s\n′′\ni + LN(Attn(s\n′′\ni,H,H, NCM)) (11)\noi = LN(s\n′′′\ni + 1\n2FFN(s\n′′′\ni )) (12)\nwhere LN indicates layer normalization and FFN is the feed-\nforward network. NCM is non-causal mask. si and oi are the\ninput and output of block i, respectively. The convolution-\naugmented self-attention block can be used for other NAT\nmodels.\nDifferent from the usage of relative positional encoding\nin [30], [42], we consider a maximum length of the relative\nposition k as in [27]. Therefore, 2k+ 1position embedding\nare learned to represent the relative position between [−k,k].\n3) Intermediate Loss : Since CTC and CE loss func-\ntions are jointly optimized in the CASS-NAT framework,\nwe incorporate intermediate CTC and intermediate CE\nloss functions into Eq.7 so that the parameters in dif-\nferent layers can be updated at the same scale. Let\nLdec = −log ∏U\nu=1 P(yu|z∗\ntu−1+1:tu ,X) and LCTC =\n−log ∑\nZ∈q\n∏T\ni=1 P(zi|X), the objective function is re-\nwritten as:\nLjoint = λCELfinal\ndec + (1−λCE)Lmid\ndec\n+ λCTCLfinal\nCTC + (1−λCTC)Lmid\nCTC\n(13)\nwhere λCE and λCTC are task ratios. midand final indicate\nthe layer position of the inserted loss functions. We found\nintermediate loss to be more effective for CASS-NAT than\nAT models [24]. The intermediate loss can be added to other\nNAT models as well.\n4) Trigger Mask Expansion: The quality of TAE relies on\nthe accuracy of the trigger mask (TM), which is mapped from\nthe CTC alignment. Although the CTC loss function is used to\noptimize the alignment, there are still errors when doing forced\nalignment over the CTC output space, leading to an inaccurate\nTM. To address this issue, we expand TM to include contextual\nframes for each token. For example, suppose the contextual\nframe size is one, the acoustic boundary of token U becomes\n{ztu−1 ,...,z tu+1}. The trigger mask will then be expanded\n1 2 3 4- \nBest Path Alignment (BPA):  \nCTC Output\nError-based Sampling Alignment (ESA):\nCTC Alignments\n \nScoring Model for decoder output\nFig. 2: Illustration of obtaining CTC alignments from CTC\noutputs, including best path alignment (BPA) and error-\nbased alignment sampling alignment (ESA). C(0.90) indicates\nP(zi = C|X) = 0.90. “ ” stands for a blank token. The\nthreshold τ for sampling is set to 0.9.\nby one in the subsequent acoustic embedding extraction. Note\nthat trigger mask expansion is designed speciﬁcally for CASS-\nNAT.\nC. Inference: Error-based Sampling Decoding\nDuring decoding, it is essential to obtain a CTC alignment\nthat is close to the hypothetical Viterbi-alignment used in train-\ning. The transcription, however, is not available. We propose to\nuse three different alignment generation methods for inference:\n(1) best path alignment (BPA); (2) beam search alignment\n(BSA); and (3) error-based sampling alignment (ESA). We\nalso present the results of using Viterbi-alignment as an upper\nbound of WER, assuming that transcriptions are available\nduring decoding, which is referred to as oracle alignment.\nBPA is similar to CTC greedy decoding that selects the token\nwith the highest probability at each time step, but without\nremoving blank and repetitive tokens in the ﬁnal sequence.\nBSA is similar to beam search decoding over the CTC output\nspace, which is the most probable alignment during decoding.\nCompared to BPA, BSA is supposed to generate an alignment\nthat is closer to the oracle alignment, which could lead to a\nlower WER, but the parallelism of the CASS-NAT would be\ndestroyed, resulting in a signiﬁcant increase of the real time\nfactor (RTF).\nConsidering the expectation in Eq. 6, we propose a third\nalignment generation approach by sampling based on the po-\ntential errors in BPA. The method is referred to as error-based\nsampling alignment (ESA). To generate ESA, a threshold τ\ndetermines whether sampling is required at each time step. If\nthe highest probability of the output distribution is larger than\nτ, the rule of BPA holds. Otherwise, we randomly sample from\nthe tokens with the ﬁrst two largest probabilities. As shown in\nshaded blue in Fig. 2, CTC outputs at z3, z5, z6, and z7 need\na sampling because of their low top-1 probability. According\nto the proposed sampling rule, four sampled alignments are\nshown as examples on the right side of Fig. 2. The reason of\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2023 6\nsampling within the top-2 tokens is that the trigger mask is\nsensitive to blank tokens and most mistaken outputs in BPA\ncontain blanks in the top-2 tokens. In addition, sampling within\n2 tokens is efﬁcient because of the small sampling space.\nESA aims at correcting the output which is prone to errors.\nSampling in the decoding stage will not affect much inference\nspeed because it can be done in parallel. It is possible that\nESA-generated alignments are closer to the oracle alignment\nthan BPA. Compared to BSA, ESA can be implemented in\nparallel, avoiding any increase in the RTF. Finally, either an AT\nbaseline or language model can be used to score and identify\nthe best overall alignment. Note that ESA can have different\nlengths for decoder input compared to BPA. As shown in\nFig. 2, the decoder length is 4 for BPA (including an EOS\ntoken), while the lengths are 3, 5, 4, and 5 in the case of\nESA. This ﬂuctuation of the token numbers allows ESA to\npossibly sample an alignment that is of the same length as the\noracle alignment, which is important for the performance of\nNAT models as will be shown experimentally. Note that ESA\ndecoding is proposed speciﬁcally for CASS-NAT.\nIV. E XPERIMENTAL SETUP\nA. Data Preparation\nTo examine the effectiveness of the proposed framework,\nwe conduct several ASR tasks, including read and sponta-\nneous speech, English and Mandarin speech, and adult and\nchild speech. Four datasets are selected: (1) the 960-hour\nLibriSpeech English corpus [43] with read speech, (2) the 178-\nhour Aishell1 Mandarin corpus [44] with adult read speech,\n(3) the 210-hour TEDLIUM2 (TED2) English corpus [45]\nwith TED talk speech, and (4) My Science Tutor (MyST)\nKids English corpus [46] with spontaneous speech. We use\nthe annotated part of MyST, which accounts for 42% (240\nhours) of the corpus.\nAll experiments use 80-dim log-Mel ﬁlter-bank features,\ncomputed every 10ms with a 25ms Hamming window. Fea-\ntures of every 3 consecutive frames are concatenated to form a\n240-dim feature vector as the input. The sets of output labels\nconsist of 5k word pieces for LibriSpeech, and 500 word\npieces for TED2 and for MyST. All sub-words are obtained by\nSentencePiece [47] using the training set of each dataset. 4230\nChinese characters are used as vocabulary for the Aishell1\ndataset. To avoid overﬁtting, we applied speed perturbation\n[48] and SpecAugment [49] to the ﬁlter-bank features from\nTED2, Aishell1 and MyST. Speed perturbation is not used for\nLibriSpeech because of limited computational resources.\nB. Network Architecture\n1) Autoregressive Models: A CTC/Attention AT baseline\nis ﬁrst trained with the architecture ( Ne = 12, Nd = 6,\ndff = 2048, nh = 8, datt = 512) for LibriSpeech, and\n(Ne = 12, Nd = 6, dff = 2048, nh = 4, datt = 256)\nfor the other three datasets, where dff is the dimension of the\nFFN module, datt stands for the dimension of the attention\nmodule, nh is the number of attention heads, Ne and Nd are\nthe number of encoder and decoder blocks, respectively. Prior\nto the encoder, two convolution layers with 64 ﬁlters, a kernel\nsize of 3, and a stride of 2 is adopted, leading to a 4x frame-\nrate reduction. When using a conformer structure to the AT\nencoder, we reduce the dff to be 1024 to keep the number\nof parameters in the model the same as in the transformer\nbaseline, and the maximum length of relative position k is set\nto 20. The kernel size in the convolution module is 31 for\nLibriSpeech and 15 for the other three datasets.\n2) CASS-NAT: During training, CASS-NAT encoder is ini-\ntialized with an AT encoder for faster convergence as in [50].\nThe decoder in AT baseline is replaced by 1-block TAEE,\nm-block SAD and n-block MAD. m and n are investigated\nusing the LibriSpeech dataset, and the best setting is applied to\nthe other three datasets. For convolution-augmented decoder,\nthe dimension of feed-forward layers is also halved and the\nmaximum length of relative position is set to 8 for the tasks\nwith word piece units and 4 for the Aishell1 data. The\ncontextual frame of trigger mask expansion is set to 1 because\nwe do not see further improvements with a larger expansion.\nThe intermediate loss functions are inserted in the middle layer\nof the encoder and MAD with λCE of 0.99 and λCTC of 0.5.\nThe inserted projection layers are discarded during inference.\nThese settings were chosen empirically.\nC. Training and Decoding Setup\nAll experiments are implemented with Pytorch [51] 1. The\nDouble schedule in [49] is adopted as the learning schedule\nfor the LibriSpeech and Aishell datasets, where the learning\nrate is ramped up to and held at 0.001, then be exponentially\ndecayed to 1e-5. The transformer-based scheduler in [7] with\nwarm-up steps of 25k and noam factor of 5 is used for the\nTED2 and MyST datasets. Layer normalization, dropout with\nrate of 0.1 and label smoothing with a penalty of 0.1 are all\napplied as the common strategies for training a transformer.\nWe compute WERs of development sets for early stopping\n(no improvement for 11 epochs). The last 12 epochs are\naveraged for ﬁnal evaluation. Most experiments end within 90\nepochs. In order to investigate the impact of different models\nfor scoring alignments in ESA, a transformer-based language\nmodel is trained with the provided text in LibriSpeech. The\nprovided n-gram models in the dataset are also compared in the\nexperiments. In terms of the pretrained HuBERT encoders, we\nuse the Fairseq model that is trained from LibriSpeech 960-\nhour corpus for ﬁnetuning the English data model and the\nTecent model that is trained from 10000-hour WenetSpeech\n[52] for ﬁnetuning the Mandarin data model.\nDuring AT decoding, the beam size is set to 20 for Lib-\nriSpeech, and 10 for the other three datasets. No external\nlanguage models are used during beam search decoding. The\nevaluation of the real time factor (RTF) is conducted using a\nV100 GPU with a batch size of one. For CASS-NAT decoding\nwith ESA, the threshold, number of sampled alignments and\nscoring models are investigated in Section V-B.\nV. R ESULTS AND ANALYSES\nThe ﬁrst set of ASR experiments used the LibriSpeech\ndataset to explore various decoder structures, impact factors\n1Our code will be available at https://github.com/Diamondfan/cassnat asr\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2023 7\nTABLE I: WERs for different block numbers of the self-\nattention decoder (SAD) and mixed-attention decoder (MAD)\nusing LibriSpeech. Various mask matrices used in MAD are\nalso explored, where CM, NCM and TM represent causal\nmask, non-causal mask and trigger mask, respectively. For\nexample, NCM + TM indicates that NCM is used for self-\nattention in MAD and TM is used for source-attention in\nMAD. Bold numbers represent the best results.\nDecoder\nStructure\nMask in\nMAD\ndev- dev- test- test-\nclean other clean other\n7SAD + 0MAD - 5.3 11.1 5.4 11.1\n5SAD + 2MAD\nNCM + NCM 4.7 10.4 4.8 10.3\nNCM + TM 4.7 10.5 4.9 10.5\nCM + NCM 4.8 10.7 4.9 10.6\n3SAD + 4MAD NCM + NCM 4.7 10.4 4.8 10.4\n1SAD + 6MAD NCM + NCM 4.6 10.5 4.7 10.4\nin ESA decoding, CTC alignment behaviour, and the effec-\ntiveness of the proposed training strategies. Using the best\nsettings found with the LibriSpeech task, experiments are run\nwith the other three datasets.\nA. The Structure of CASS-NAT Decoder\nTo investigate the structure of the CASS-NAT decoder, we\nuse various combinations of m SAD blocks and n MAD\nblocks (m+n= 7) in the decoder to keep the number of model\nparameters similar to that of the AT baseline. We use best\npath alignment during inference and discuss other decoding\nstrategies in the next section. Results are shown in Table I. As\nshown in the table, not using MAD (0 MAD in the ﬁrst row)\ncauses a big performance degradation compared to other con-\nﬁgurations, indicating that token-level speech representations\nmight have to retrace the ﬁne-grained frame-level information\n(encoder outputs) for better contextual modelling. The best\nperformance is achieved when using 5 SADs and 2 MADs\n(WER of 10.3% on the test-other set).\nBecause there are two attention layers (self-attention and\nsource-attention) in each MAD, we try different mask matrices\nfor the two layers. For the self-attention layer, either a causal\nmask (CM) or non-causal mask (NCM) is considered. For the\nsource-attention layer, either a trigger mask (TM) or NCM is\nused. The results in Table I show that using NCM on both\nattention layers results in the best WER. Although TAE is\nregarded as a substitution of word embedding, CM seems not\nnecessarily required in the CASS-NAT decoder for contextual\nmodelling of token-level acoustic embeddings. The settings\nthat achieve the best performance in this section (5 SADs and\n2 MADs with NCM in both attention layers) are selected as\ndefault for subsequent experiments.\nB. Error-based Sampling Alignment (ESA) Decoding\nViterbi-alignment is used in training, but not available\nduring inference. To reduce the alignment mismatch between\ntraining and inference, we propose error-based sampling align-\nment (ESA). There are three important factors that can affect\nthe WER performance of ESA: sampling threshold P, the\nnumber of sampled alignments S and scoring model for\nThreshold P\nWord Error Rate (WER)\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAT Baseline BPA ESA\nWER on test-clean with 10 alignments sampled\n(a) Effect of P using test-clean\nThreshold P\nWord Error Rate (WER)\n6.0\n7.0\n8.0\n9.0\n10.0\n11.0\n12.0\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAT Baseline BPA ESA\nWER on test-other with 10 alignments sampled (b) Effect of P using test-other\nNumber of sampled alignments S\nWord Error Rate (WER)\nReal Time Factor (RTF)\n0.0\n2.0\n4.0\n6.0\n8.0\n10.0\n0.0000\n0.0100\n0.0200\n0.0300\n0.0400\n10 20 50 100 200 300\nWER(test-clean) WER(test-other) RTF-GPU(AVG)\n(c) Effect of the number of sampled alignments S\nFig. 3: WER performance of different values of the threshold\nP and the number of sampled alignments S in error-based\nsampling alignment (ESA) decoding. Real time factor (RTF)\nis evaluated using a V100 GPU with a batch size of one.\nranking alignments. We use the best decoder structure in the\nprevious section (5SAD + 2MAD) to evaluate the performance\nof ESA decoding with different conﬁgurations.\nFigures 3a and 3b show the results when the threshold\nP varies from 0.10 to 0.95 using the LibriSpeech test-clean\nand test-other data, respectively. The number of sampled\nalignments here is 10 and the scoring model is the AT baseline.\nWe also include the WERs of the AT baseline and CASS-\nNAT decoding with best path alignment (BPA) as comparisons.\nAs shown in the ﬁgures, ESA reaches the best performance\nwhen P = 0.9. A higher threshold indicates fewer sampled\nalignments, and thus no further improvement is observed. The\nthreshold P is set to 0.9 in subsequent experiments.\nFig.3c shows the effect of the number of sampled align-\nments S in terms of WER and RTF on the test-clean and\ntest-other data. As observed in the ﬁgure, by increasing the\nnumber of sampled alignments, the WER of ESA decoding\nimproves but the improvement is small when S is greater\nthan 50. Meanwhile, the RTF increases rapidly as S increases.\nOverall, S = 50 might be the most appropriate value to use\nas default in subsequent experiments.\nFinally, various scoring models are compared. We consider\nusing the AT baseline, neural language model (NLM) and\nn-gram language model for ranking the sampled alignments.\nNLM is a transformer-based model trained with the provided\ntext in the LibriSpeech corpus. The n-gram models are also\nthe ones provided in the LibriSpeech corpus. The results of\nESA decoding together with that of BPA and ESA decoding\nare shown in Table II. As analyzed in Sec.III-C, BPA has\nan impressive speedup but the WER is much worse than the\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2023 8\nTABLE II: Comparisons, in terms of WER and GPU speedup,\nof BPA, BSA, and ESA decoding. NLM is the transformer-\nbased neural language model. 3-gram and 4-gram models are\nthe ones offered in LibriSpeech.\nScoring\nModel\ndev- dev- test- test- GPU\nSpeedupclean other clean other\nAT baseline - 3.4 8.1 3.6 8.0 1.00x\nOracle - 2.1 5.5 2.2 5.3 39.6x\nBPA - 4.7 10.4 4.8 10.3 90.0x\nBSA - 3.8 8.8 3.9 8.8 0.90x\nESA\nAT baseline 3.6 8.8 3.8 8.6 28.4x\nNLM 3.6 8.9 3.9 8.7 31.6x\n3-gram 5.4 11.4 5.8 11.4 32.6x\n4-gram 5.4 11.3 5.7 11.3 31.5x\nAT baseline, and BSA can obtain a better WER but it is\neven slower than the AT baseline. By using neural network\nbased scoring models, ESA can retain both the advantages of\nBPA and BSA. For example, using NLM as a scoring model,\nESA can achieve a WER of 8.7% on the LibriSpeech test-\nother data and 31.6x speedup. The degradation in speedup\ncompared to BPA originates from the ranking process. Using\nn-gram models for ranking alignments leads to worse WER\ncompared to NLM in ESA because n-gram models are worse\nthan NLM for language modelling. Another possible reason\nmight be that the probability distribution of n-gram models\nis different from that of CASS-NAT decoder outputs, while\nfor NLM it is similar. The reason why n-gram models have\nsimilar GPU speedup compared to NLM is that the scores\nof the sampled alignments cannot be obtained simultaneously.\nThe best performing scoring model is the AT baseline, and\nthus the AT baseline is used in ESA decoding as default in\nthe following experiments.\nNote that because of the sampling process in ESA decoding,\nWER may be slightly different for different seeds of the ran-\ndom number generator. Fortunately, the randomness is small\nwith a variance of around 0.5% relatively in our experiments.\nC. Why does ESA Work? - An Analysis of the CTC Alignments\nIn this section, we analyze the CTC alignments obtained\nfrom different decoding strategies to understand why ESA\ncan improve the performance of CASS-NAT. Two metrics are\nevaluated on the LibriSpeech test sets: mismatch rate (MR)\nand length prediction error rate (LPER). MR and LPER are\nmeasured between the alignments used in decoding and the\noracle alignment, in which the blank and repetitive tokens are\nremoved. The MR is the ratio of deletion and insertion errors\ncompared to the oracle alignment, and substitution errors are\nnot included because they do not change either the acoustic\nboundary for each token or the number of predicted tokens.\nIf the number of tokens in an alignment is different from\nthat in the oracle alignment, this alignment is considered as a\ncase of length prediction error. LPER is the percentage of the\nutterances with length prediction errors.\nResults of MR and LPER are presented in Table III. Note\nthat the lower bound of WER (using oracle alignment) is\n2.2% on the test-clean data assuming the transcriptions are\navailable during decoding; this indicates that the framework\nis promising. When comparing the two metrics for differ-\nTABLE III: Comparisons of different decoding methods for CASS-\nNAT decoding. Oracle: Viterbi-alignment with ground truth. MR:\nmismatch rate; LPER: length prediction error rate (using word-piece\nas the modeling unit). S: the number of alignments sampled in ESA.\nDecoding S WER (%) MR (%) LPER (%)\nMethod test- test- test- test- test- test-\nclean other clean other clean other\nOracle 1 2.2 5.3 - - - -\nBSA 1 4.0 9.2 2.5 6.0 28.09 48.83\nBPA 1 4.8 10.3 2.4 5.2 34.92 51.68\nESA\n10 4.0 9.0 3.6 6.0 26.41 44.91\n50 3.8 8.6 3.8 6.3 25.46 43.08\n100 3.8 8.5 3.8 6.2 25.61 42.70\n300 3.7 8.5 3.8 6.3 25.53 42.67\nLength difference with the oracle alignment\nNumber of Utterances\nWord Error Rate (WER)\n0\n500\n1000\n1500\n2000\n0.0\n5.0\n10.0\n15.0\n20.0\n ≤ -3 -2 -1 0 1 2 ≥ 3\n ESA # Utterance BPA # Utterance ESA WER BPA WER\nFig. 4: The length prediction error distribution and their\ncorresponding WERs using ESA(s=50) decoding on the test-\nclean dataset.\nent decoding strategies, we observe that the WER is more\ncorrelated with LPER than MR. This suggests that a correct\nestimation of the output length (the length of decoder input) is\nvery important for NAT, which is also mentioned in [13]. The\nreason may be because CASS-NAT decoders have a stronger\nability of correcting mistakes in CTC alignment by contextual\ntoken-level acoustic modelling when the length prediction is\naccurate. To further validate this assumption, we plot the\nlength prediction error distribution and their corresponding\nWERs in Fig. 4. As seen from the ﬁgure, CASS-NAT achieves\nWERs that are lower than 2% when the length of the decoder\ninput is estimated correctly, while the WERs are higher than\n10% for those utterances with an absolute difference in length\nof more than 3 compared to the oracle alignment.\nD. Improving the Training of CASS-NAT\nDespite the use of ESA decoding method, the WER per-\nformance of CASS-NAT (8.6%) is still worse than that of\nthe AT baseline (8.0%). In this section, we attempt to im-\nprove the WER performance of CASS-NAT by using various\ntraining strategies that include convolution-augmented encoder\n(ConvEnc.), intermediate CTC loss (InterCTC), convolution\naugmented decoder (ConvDec.), intermediate CE loss (In-\nterCE) and trigger mask expansion (Mask Exp.). The results\nof various combinations of the training strategies are shown\nin Table IV.\nFirst, we apply ConvEnc. and InterCTC to the autoregres-\nsive transformer to create a new AT baseline for fair compar-\nisons. Note that, we have applied ConvDec. and InterCE to\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2023 9\nTABLE IV: WERs of using the proposed training strategies on the LibriSpeech data. SpecAug is used in all conﬁgurations.\nWERR is the relative WER improvement compared to their corresponding baselines on the test-other data. KD represents using\nknowledge distillation from AT decoder outputs. Other strategies are the ones introduced in Section III-B. Bold faced numbers\nindicate the best WER results.\nModel w/o LM ConvEnc. InterCTC ConvDec. InterCE Mask Exp. KD dev- dev- test- test- WERRclean other clean other\nAT\n3.4 8.1 3.6 8.0 -\n! 2.7 6.9 2.9 6.9 13.8%\n! ! 2.7 6.8 2.8 6.7 16.3%\nCASS-NAT\n3.6 8.8 3.8 8.6 -\n! 3.0 7.3 3.2 7.5 12.8%\n! ! 2.9 7.4 3.1 7.3 15.1%\n! 3.4 8.3 3.6 8.4 2.3%\n! ! 3.3 8.2 3.6 8.3 3.5%\n! ! ! ! 2.8 7.1 2.9 7.1 17.4%\n! 3.6 8.9 3.8 8.7 -1.2%\n! ! ! 3.3 8.3 3.6 8.1 5.8%\n! ! ! ! ! 2.7 7.1 2.9 7.0 18.6%\n! 3.6 8.8 3.8 8.5 1.2%\nthe AT baseline as well, but no improvements are observed,\nand thus their results are not included. When ConvEnc. and\nInterCTC are applied to CASS-NAT, similar improvements\ncompared to their use for AT are observed. The improvements\nstem from their ability of capturing local details in the acoustic\nrepresentations, and thus a stronger encoder is learned and\nit offers an accurate CTC alignment for implicit token-level\nacoustic embedding modelling in the decoder.\nSecond, when using ConvDec. and InterCE, the WER of\nCASS-NAT drops further to 7.1% on the test-other data, which\nis a 17.4% relative WER improvement over the CASS-NAT\nbaseline. The use of ConvDec. and InterCE, however, is not as\neffective as ConvEnc and InterCTC, indicating that a stronger\nmodelling of frame-level acoustic representations is more im-\nportant than that of token-level acoustic representations (TAEs)\nin CASS-NAT. This is reasonable because TAEs are extracted\nfrom frame-level acoustic representations from the encoder.\nInterestingly, we observe a -1.2% relative WER reduction\nwhen the trigger mask expansion method is applied by itself.\nHowever, the method leads to WER improvements when it\nis used together with ConvDec. related strategies. The reason\ncould be that expanding the acoustic boundary for each token\nmight cause confusion for the decoder when the contextual\nmodelling is not strong enough.\nIn addition, as shown in the last line of Table IV, applying\nknowledge distillation, which takes the AT output as a target\nfor NAT training, does not result in WER improvements.\nAlthough knowledge distillation has been proven useful in\nnon-autoregressive neural machine translation in [53], it does\nnot seem to work for speech recognition possibly because there\nare no different outputs corresponding to the same input, while\nmultiple translations of the source language exist in the target\nlanguage for machine translation.\nFinally, using all the training strategies, we achieve an 18%\nrelative WER improvement compared to CASS-NAT baseline;\nthis is only a 3% WER degradation compared to the best\nversion of the AT baseline. In summary, we obtain a non-\nautoregressive speech transformer that performs close to its\n0 50\nT oken position\n0\n20\n40T oken position\n0.00\n0.25\n0.50\n0.75\n1.00\n0 50\nT oken position\n0\n20\n40T oken position\n0.00\n0.25\n0.50\n0.75\n1.00\n0 50\nT oken position\n0\n20\n40T oken position\n0.00\n0.25\n0.50\n0.75\n1.00\n0 50\nT oken position\n0\n20\n40T oken position\n0.00\n0.25\n0.50\n0.75\n1.00\n(a) The 5th ∼8th head of self-attention in the last SAD\n0 50\nT oken position\n0\n20\n40T oken position\n0.00\n0.25\n0.50\n0.75\n1.00\n0 50\nT oken position\n0\n20\n40T oken position\n0.00\n0.25\n0.50\n0.75\n1.00\n0 50\nT oken position\n0\n20\n40T oken position\n0.00\n0.25\n0.50\n0.75\n1.00\n0 50\nT oken position\n0\n20\n40T oken position\n0.00\n0.25\n0.50\n0.75\n1.00\n(b) The 5th ∼8th head of self-attention in the last MAD\nFig. 5: Attention weight distributions of the 5th −8th head\nin the last self-attention decoder (SAD) and mixed-attention\ndecoder (MAD). The ﬁrst utterance in the LiriSpeech train-\nclean-100 subset is used. The y-axis represents output tokens\nautoregressive counterpart with a signiﬁcant GPU speedup.\nE. Why does CASS-NAT Work? - An Analysis of the Decoder\nIn this section, we explain why CASS-NAT can achieve a\ngood performance that is close to its autoregressive counterpart\nby analysing the following elements in CASS-NAT decoder:\n(1) attention weight distribution; and (2) acoustic-level acous-\ntic representations.\nFirst, to analyse the behaviour of the attention mechanism\nin CASS-NAT decoder, we choose the ﬁrst utterance in\nthe LibriSpeech train-clean-100 subset and plot the attention\nweight distribution in the last SAD and MAD blocks. The\nweights of the last 4 heads (8 heads in total) are shown\nin Fig.5. For self-attention weight distributions in SAD, we\nnotice from the ﬁgure that most of the heads learn a mono-\ntonic alignment between token-level acoustic representations,\nindicating that each token relies on adjacent tokens. This is\nsimilar to the idea of word embedding using a continuous\nbag of words (CBOW) and skip-gram [54]. The monotonic\nalignment also shows the usefulness of the relative positional\nencoding because distant tokens with close semantic similarity\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2023 10\n800\n 600\n 400\n 200\n 0 200 400 600 800\n400\n200\n0\n200\n400\n600\n800\nFINALLY\nFINE\nFIND\n FINDING\nBEYOND\nBEHIND\nBENEATH\nBELOW\nBEARING\nBEAR\nBARE\nBURIED\nFINALLY\nBEYOND\nBEARING\n(a) TAEE output\n1500\n 1000\n 500\n 0 500 1000 1500 2000 2500\n500\n0\n500\n1000\nFINALLY\nFINDING\nPLAINLY\nSUDDENLY\nBEYOND\nBEHIND\nBENEATH\nWITHIN\nBEARING\nBENDING\nHEARING\nBRINGING\nFINALLY\nBEYOND\nBEARING\n (b) SAD output\n20\n 15\n 10\n 5\n 0 5 10 15 20\n10\n5\n0\n5\n10\n15\nFINALLY\nPLAINLY\nPRESENTLY\nFIRMLY\nBEYOND\nBEHIND\nWITHIN\nAROUND\nBEARING\nCARRYING\nHEARING\nBURNING\nFINALLY\nBEYOND\nBEARING\n (c) MAD output\nFig. 6: Visualization of token-level acoustic embeddings of three word pieces from outputs of TAEE, SAD and MAD,\nrespectively. The embeddings are plotted using principle component analysis (PCA).\nhave low attention weights. For the attention weights in MAD,\nthere exists a different behaviour in several subspaces of the\nattention computation, where outputs may rely on the same\ninput (vertical line in Fig.5b).\nAs discussed in Section II-B2, we assume that language\nsemantics can be captured by CASS-NAT decoder using token-\nlevel acoustic embeddings. To validate this assumption, we\ncalculate three different token-level acoustic embeddings from\nTAEE, SAD and MAD for each token (word piece in our\ncase). Speciﬁcally, token-level acoustic representations are ﬁrst\nextracted from the ﬁrst 5000 utterances in the train-clean-\n100 Librspeech subset, where each representation has its label\nin the form of a word piece. The embedding of each word\npiece is the average of the corresponding representations. After\nthat, we randomly choose three word pieces and ﬁnd the four\nclosest ones (measured with cosine similarity distance) for\neach word piece. Using the same idea of visualizing word\nembeddings, the 12 selected embeddings are reduced to a 2-\ndimensional space using principal component analyses (PCA)\nand are then plotted. Examples of embeddings at different\nlevels of the CASS-NAT decoder are shown in Fig.6. The\nﬁgure suggests that the token-level acoustic embeddings learn\nnot only acoustic similarities ( BEAR vs. BARE), but also\ntoken-level semantic similarities ( BENEATH vs. BELOW).\nThis occurs, even though embeddings at different levels of the\ndecoder provide different information. For example, higher\nlayer (MAD) embeddings focus more on grammatical sim-\nilarities (e.g. FIRMLY and FINALLY), which is similar\nto word embedding. This ﬁnding suggests the possibility of\nlearning a joint speech and text embeddings in a common\nspace. Even though there is no explicit language modelling, the\nCASS-NAT decoder is able to learn meaningful embeddings,\nwhich may explain why it has a similar performance to its AT\ncounterpart.\nF . Results on other datasets\nFinally, we apply the proposed CASS-NAT to the other\nthree datasets: Aishell (Mandarin speech), TED2 (English\nspontaneous speech), and MyST (child speech), to show the\ngeneralization ability of CASS-NAT. In addition, we explore\nthe effect of different initialization schemes for the CASS-\nNAT encoder and measure the speedup given by CASS-NAT\ncompared to AT baselines. All results are summarized in Table\nV. As can be observed from the table, a randomly initialized\nencoder for CASS-NAT achieves the best performance on the\nAishell1, TED2 and MyST datasets, while the LibriSpeech\ndataset performs better with a pretrained encoder. The reason\ncould be that we force the training iterations of CASS-NAT to\nbe the same as the AT baseline for time considerations, as well\nas for a fair comparison, but LibriSpeech, as a large database\ncompared to the others, needs more training iterations to obtain\naccurate alignments for the decoder. Hence, we suggest using\na pretrained encoder for training CASS-NAT on large datasets.\nWith an appropriate encoder initialization, we now compare\nthe results of CASS-NAT to the AT baselines and SOTA\nresults using NATs. First, when compared to the AT baseline\nwith greedy search decoding, CASS-NAT achieves better (on\nAishell1, TED2, and MyST) or similar (on LibriSpeech)\nWERs performance, and has a 2.89x RTF speedup. When\ncompared to the AT baseline with beam search decoding,\nthe WERs performance on Aisehll and TED2 are still better,\nwhile the WERs on LibriSpeech and MyST are close to the\nbaseline but with a 24x RTF speedup. Second, when compared\nto other NAT methods in the literature, we observe from the\ntable that the proposed CASS-NAT achieves new state-of-\nthe-art results on LibriSpeech, TED2, and MyST when no\npretrained acoustic (e.g. Wav2vec2.0 [37]) or language models\n(e.g. BERT [59]) are used. To compare with the results that\nused extra training data (e.g. SSL with un-annotated data),\nwe use the self-supervised pretrained models as the encoder\nfor CASS-NAT to see whether it can outperform the SOTA\nresults in Table V. Due to time and computational constraints,\nwe obtain only the results on Aishell1 and MyST datasets.\nThe results show that CASS-NAT can achieve comparable\nresults to the best NATs in the literature [57], [58] with SSL\npretraining.\nVI. C ONCLUSIONS\nThis paper presents a comprehensive study of the CASS-\nNAT framework introduced in [23], which utilizes align-\nments obtained from the CTC output space to extract token-\nlevel acoustic embeddings (TAEs), and regards the TAEs\nas substitutions of the word embeddings in autoregressive\ntransformers (AT) to achieve parallel computation. During\ntraining, Viterbi-alignment is used to estimate the posterior\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2023 11\nTABLE V: WER Results of CASS-NAT using different encoders as a starting point, including random initialization (Rand.\nEnc.), initialization from an encoder trained with CTC (CTC enc.), and initialization from a AT encoder (AT Enc.). AT\nbaselines with greedy and beam search decoding are included together with SOTA results using non-autoregressive methods\nin the literature. “-” indicates that WER results have not been reported in the literature. RTF is evaluated with batch size of\none on GPU. Bold-faced numbers are the best results for CASS-NAT and SOTA without self-supervised pretraining.\nModel Conditions\nInference Speed LibriSpeech Aishell1 TED2 MyST\nRTF Speed dev test dev test dev test dev testclean other clean other\nAT greedy search 0.0387 1.00x 2.7 7.0 2.9 7.0 5.1 5.9 9.8 7.8 18.1 17.0\nbeam search 0.3348 0.12x 2.7 6.8 2.8 6.7 4.7 5.1 8.2 7.6 16.5 16.2\nPrevious\nNAT results\nw/o SSL 2.8 7.3 3.1 7.2 [24] 4.5 4.9 [17] 8.7 8.0 [55] 28.0 27.8 [56]\nw/ SSL 1.7 3.6 1.8 3.6 [22] 4.1 4.5 [57] - - 16.8 16.5 [58]\nCASS-NAT\n(this work)\nRand. Enc.\n0.0134 2.89x\n3.3 8.4 3.6 8.3 4.7 5.0 8.4 7.5 16.8 16.4\nCTC Enc. 2.8 7.2 3.1 7.3 5.2 5.6 8.0 7.7 17.1 16.7\nAT Enc. 2.7 7.1 2.9 7.0 5.1 5.4 8.2 7.8 17.1 16.7\nHuBERT Enc. - - - - 4.2 4.5 - - 16.0 15.6\nprobability, and various training strategies are used to improve\nthe WER performance of the CASS-NAT. During inference,\nwe investigate in depth an error-based sampling alignment\n(ESA) method that we introduced recently in [23] to reduce the\nalignment mismatch between training and inference. Detailed\nexperiments show that: (1) a mixed-attention decoder (MAD)\nis important for reducing the WER; (2) the reason why\nESA decoding works well is because it has a lower length\nprediction error rate; (3) convolution augmented encoder and\ndecoder, intermediate loss and mask expansion can improve\nthe WER of CASS-NAT, while knowledge distillation can not;\nand (4) TAEs have similar functionality to word embeddings,\nsuch as representing grammatical structures, indicating the\npossibility of learning semantics without a language model.\nAs a result, CASS-NAT achieves new state-of-the-art results\nfor NAT models on several ASR tasks with and without\nSSL pretraining. The datasets include English and Mandarin\nspeech, read and spontaneous speech, and child and adult\nspeech, showing the generaliztion ability of the proposed\nmethod. The performances of CASS-NAT are comparable, in\nrelative terms, to AT with beam search decoding, but maintain\na ∼24x speed up. Future work includes investigating the\nincorporation of language models using external data.\nREFERENCES\n[1] J. Li, Y . Wu, Y . Gaur, C. Wang, R. Zhao, and S. Liu, “On the comparison\nof popular end-to-end models for large scale speech recognition,” Proc.\nInterspeech 2020, pp. 1–5, 2020.\n[2] J. Li et al., “Recent advances in end-to-end automatic speech recog-\nnition,” APSIPA Transactions on Signal and Information Processing,\nvol. 11, no. 1, 2021.\n[3] A. Graves, S. Fern ´andez, F. Gomez, and J. Schmidhuber, “Connection-\nist temporal classiﬁcation: labelling unsegmented sequence data with\nrecurrent neural networks,” in Proceedings of the 23rd international\nconference on Machine learning (ICML), 2006, pp. 369–376.\n[4] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A\nneural network for large vocabulary conversational speech recognition,”\nin IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2016, pp. 4960–4964.\n[5] A. Graves, “Sequence transduction with recurrent neural networks,”\nInternational Conference of Machine Learning (ICML), 2012.\n[6] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, “Transformer transducer: A streamable speech recognition\nmodel with transformer encoders and rnn-t loss,” in IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2020, pp. 7829–7833.\n[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proceedings\nof the 31st International Conference on Neural Information Processing\nSystems (NIPS), 2017, pp. 6000–6010.\n[8] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-recurrence\nsequence-to-sequence model for speech recognition,” in IEEE In-\nternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2018, pp. 5884–5888.\n[9] J. Gu, J. Bradbury, C. Xiong, V . O. Li, and R. Socher, “Non-\nautoregressive neural machine translation,” in International Conference\non Learning Representations (ICLR), 2018.\n[10] J. Lee, E. Mansimov, and K. Cho, “Deterministic non-autoregressive\nneural sequence modeling by iterative reﬁnement,” in The Conference\non Empirical Methods in Natural Language Processing (EMNLP), 2020,\npp. 1173–1182.\n[11] C. Saharia, W. Chan, S. Saxena, and M. Norouzi, “Non-autoregressive\nmachine translation with latent alignments,” in The Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), 2020,\npp. 1098–1108.\n[12] W. Qi, Y . Gong, J. Jiao, Y . Yan, W. Chen, D. Liu, K. Tang, H. Li, J. Chen,\nR. Zhang et al., “Bang: Bridging autoregressive and non-autoregressive\ngeneration with large scale pretraining,” in International Conference on\nMachine Learning (ICML). PMLR, 2021, pp. 8630–8639.\n[13] N. Chen, S. Watanabe, J. A. Villalba, P. Zelasko, and N. Dehak,\n“Non-autoregressive transformer for speech recognition,” IEEE Signal\nProcessing Letters (SPL), 2020.\n[14] Y . Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi, “Mask\nctc: Non-autoregressive end-to-end asr with ctc and mask predict,” Proc.\nInterspeech 2020, pp. 3655–3659, 2020.\n[15] Y . Bai, J. Yi, J. Tao, Z. Tian, Z. Wen, and S. Zhang, “Fast end-to-\nend speech recognition via non-autoregressive models and cross-modal\nknowledge transferring from bert,” IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing (TASLP), vol. 29, pp. 1897–1911,\n2021.\n[16] Z. Tian, J. Yi, J. Tao, Y . Bai, S. Zhang, and Z. Wen, “Spike-triggered\nnon-autoregressive transformer for end-to-end speech recognition,”Proc.\nInterspeech 2020, pp. 5026–5030, 2020.\n[17] F. Yu, H. Luo, P. Guo, Y . Liang, Z. Yao, L. Xie, Y . Gao, L. Hou,\nand S. Zhang, “Boundary and context aware training for cif-based non-\nautoregressive end-to-end ASR,” in Automatic Speech Recognition and\nUnderstanding Workshop (ASRU). IEEE, 2021, pp. 328–334.\n[18] N. Chen, P. ˙Zelasko, L. Moro-Vel ´azquez, J. Villalba, and N. Dehak,\n“Align-Denoise: Single-Pass Non-Autoregressive Speech Recognition,”\nin Proc. Interspeech 2021, 2021, pp. 3770–3774.\n[19] E. A. Chi, J. Salazar, and K. Kirchhoff, “Align-reﬁne: Non-\nautoregressive speech recognition via iterative realignment,”Proceedings\nof the 2021 Conference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language Technologies,\n(NAACL-HLT), pp. 1920–1927, 2020.\n[20] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly, “Imputer:\nSequence modelling via imputation and dynamic programming,” in\nInternational Conference on Machine Learning (ICML). PMLR, 2020,\npp. 1403–1413.\n[21] J. Nozaki and T. Komatsu, “Relaxing the Conditional Independence\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, 2023 12\nAssumption of CTC-Based ASR by Conditioning on Intermediate Pre-\ndictions,” in Proc. Interspeech 2021, 2021, pp. 3735–3739.\n[22] E. G. Ng, C.-C. Chiu, Y . Zhang, and W. Chan, “Pushing the limits of\nnon-autoregressive speech recognition,” in Proc. Interspeech, pp. 3725–\n3729, 2021.\n[23] R. Fan, W. Chu, P. Chang, and J. Xiao, “Cass-nat: Ctc alignment-\nbased single step non-autoregressive transformer for speech recognition,”\nin IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2021, pp. 5889–5893.\n[24] R. Fan, W. Chu, P. Chang, J. Xiao, and A. Alwan, “An Improved\nSingle Step Non-Autoregressive Transformer for Automatic Speech\nRecognition,” in Proc. Interspeech 2021, 2021, pp. 3715–3719.\n[25] W.-N. Hsu, Y .-H. H. Tsai, B. Bolte, R. Salakhutdinov, and A. Mohamed,\n“Hubert: How much can a bad teacher beneﬁt asr pre-training?” in IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2021, pp. 6533–6537.\n[26] T. Nakatani, “Improving transformer-based end-to-end speech recog-\nnition with connectionist temporal classiﬁcation and language model\nintegration,” in Proc. Interspeech, 2019, pp. 1408–1412.\n[27] P. Zhou, R. Fan, W. Chen, and J. Jia, “Improving generalization of\ntransformer for speech recognition with parallel schedule sampling and\nrelative positional embedding,” arXiv preprint arXiv:1911.00203, 2019.\n[28] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le, “Attention\naugmented convolutional networks,” in Proceedings of the IEEE/CVF\ninternational conference on computer vision (ICCV), 2019, pp. 3286–\n3295.\n[29] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi,\nand Q. V . Le, “Qanet: Combining local convolution with global self-\nattention for reading comprehension,” in International Conference on\nLearning Representations (ICLR), 2018.\n[30] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y . Wu et al., “Conformer: Convolution-augmented\ntransformer for speech recognition,” Proc. Interspeech 2020, pp. 5036–\n5040, 2020.\n[31] W. Han, Z. Zhang, Y . Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati,\nR. Pang, and Y . Wu, “Contextnet: Improving convolutional neural\nnetworks for automatic speech recognition with global context,” Proc.\nInterspeech 2020, pp. 3610–3614, 2020.\n[32] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV . Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition (CVPR), 2015, pp. 1–9.\n[33] A. Tjandra, C. Liu, F. Zhang, X. Zhang, Y . Wang, G. Synnaeve,\nS. Nakamura, and G. Zweig, “Deja-vu: Double feature presentation\nand iterated loss in deep transformer networks,” in IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2020, pp. 6899–6903.\n[34] J. Lee and S. Watanabe, “Intermediate loss regularization for ctc-based\nspeech recognition,” in IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2021, pp. 6224–6228.\n[35] J. Lee, J. Kang, and S. Watanabe, “Layer Pruning on Demand with\nIntermediate CTC,” in Proc. Interspeech 2021, 2021, pp. 3745–3749.\n[36] Y . Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang et al., “Transformer-based\nacoustic modeling for hybrid speech recognition,” in IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2020, pp. 6874–6878.\n[37] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:\nA framework for self-supervised learning of speech representations,”\nNeurIPS 2020, 2020.\n[38] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, “Hybrid\nctc/attention architecture for end-to-end speech recognition,” IEEE Jour-\nnal of Selected Topics in Signal Processing (JSTSP), vol. 11, no. 8, pp.\n1240–1253, 2017.\n[39] N. Moritz, T. Hori, and J. Le, “Streaming automatic speech recognition\nwith the transformer model,” in IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp.\n6074–6078.\n[40] A. Zeyer, A. Merboldt, R. Schl ¨uter, and H. Ney, “A new training pipeline\nfor an improved neural transducer,” Proc. Interspeech 2020, pp. 2812–\n2816, 2020.\n[41] B. Yang, L. Wang, D. F. Wong, L. S. Chao, and Z. Tu, “Convolutional\nself-attention networks,” in Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), Jun. 2019, pp. 4040–4045.\n[42] Z. Dai, Z. Yang, Y . Yang, J. G. Carbonell, Q. Le, and R. Salakhutdi-\nnov, “Transformer-xl: Attentive language models beyond a ﬁxed-length\ncontext,” in Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics (ACL), 2019, pp. 2978–2988.\n[43] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an\nasr corpus based on public domain audio books,” in IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2015, pp. 5206–5210.\n[44] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “Aishell-1: An open-source\nmandarin speech corpus and a speech recognition baseline,” in 20th\nConference of the Oriental Chapter of the International Coordinating\nCommittee on Speech Databases and Speech I/O Systems and Assess-\nment (O-COCOSDA). IEEE, 2017, pp. 1–5.\n[45] A. Rousseau, P. Del ´eglise, Y . Esteve et al., “Enhancing the ted-lium\ncorpus with selected data for language modeling and more ted talks.”\nin Proceedings of the Ninth International Conference on Language\nResources and Evaluation (LREC), 2014, pp. 3935–3939.\n[46] W. Ward, R. Cole, D. Bolanos, C. Buchenroth-Martin, E. Svirsky, S. V .\nVuuren, T. Weston, J. Zheng, and L. Becker, “My science tutor: A\nconversational multimedia virtual tutor for elementary school science,”\nACM Transactions on Speech and Language Processing (TSLP), vol. 7,\nno. 4, pp. 1–29, 2011.\n[47] T. Kudo and J. Richardson, “Sentencepiece: A simple and language inde-\npendent subword tokenizer and detokenizer for neural text processing,”\nThe Conference on Empirical Methods in Natural Language Processing\n(EMNLP), p. 66, 2018.\n[48] T. Ko, V . Peddinti, D. Povey, and S. Khudanpur, “Audio augmentation\nfor speech recognition,” in Sixteenth annual conference of the interna-\ntional speech communication association, 2015, pp. 3586–3589.\n[49] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,\nand Q. V . Le, “Specaugment: A simple data augmentation method for\nautomatic speech recognition,” Proc. Interspeech 2019, pp. 2613–2617,\n2019.\n[50] R. Fan, P. Zhou, W. Chen, J. Jia, and G. Liu, “An online attention-based\nmodel for speech recognition,” Proc. Interspeech 2019, pp. 4390–4394,\n2019.\n[51] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An im-\nperative style, high-performance deep learning library,” in International\nConference on Neural Information Processing Systems (NIPS), 2019,\npp. 8026–8037.\n[52] B. Zhang, H. Lv, P. Guo, Q. Shao, C. Yang, L. Xie, X. Xu, H. Bu,\nX. Chen, C. Zeng et al., “Wenetspeech: A 10000+ hours multi-domain\nmandarin corpus for speech recognition,” in IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp.\n6182–6186.\n[53] C. Zhou, J. Gu, and G. Neubig, “Understanding knowledge distillation\nin non-autoregressive machine translation,” in International Conference\non Learning Representations (ICLR), 2020.\n[54] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation\nof word representations in vector space,” International Conference on\nLearning Representations (ICLR), 2013.\n[55] Y . Higuchi, N. Chen, Y . Fujita, H. Inaguma, T. Komatsu, J. Lee,\nJ. Nozaki, T. Wang, and S. Watanabe, “A comparative study on\nnon-autoregressive modelings for speech-to-text generation,” Automatic\nSpeech Recognition and Understanding (ASRU), pp. 47–54, 2021.\n[56] R. Fan, Y . Zhu, J. Wang, and A. Alwan, “Towards better domain\nadaptation for self-supervised models: A case study of child asr,” IEEE\nJournal of Selected Topics in Signal Processing, vol. 16, no. 6, pp.\n1242–1252, 2022.\n[57] K. Deng, Z. Yang, S. Watanabe, Y . Higuchi, G. Cheng, and P. Zhang,\n“Improving non-autoregressive end-to-end speech recognition with pre-\ntrained acoustic and language models,” IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pp. 8522–8526,\n2022.\n[58] R. Fan and A. Alwan, “DRAFT: A novel framework to reduce domain\nshifting in self-supervised learning and its application to children’s\nASR,” in in Proc. Interspeech 2022, 2022, pp. 4900–4904.\n[59] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” Annual\nConference of the North American Chapter of the Association for\nComputational Linguistics (NAACL-HLT), pp. 4171–4186, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8053498268127441
    },
    {
      "name": "Autoregressive model",
      "score": 0.8025209307670593
    },
    {
      "name": "Inference",
      "score": 0.745013415813446
    },
    {
      "name": "Transformer",
      "score": 0.6393139362335205
    },
    {
      "name": "Speech recognition",
      "score": 0.6317993402481079
    },
    {
      "name": "Security token",
      "score": 0.5945484638214111
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5607448220252991
    },
    {
      "name": "Hidden Markov model",
      "score": 0.5512056946754456
    },
    {
      "name": "Word error rate",
      "score": 0.418425589799881
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41771793365478516
    },
    {
      "name": "Engineering",
      "score": 0.08181354403495789
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    }
  ]
}