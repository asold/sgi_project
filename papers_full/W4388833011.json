{
  "title": "Object detection using convolutional neural networks and transformer-based models: a review",
  "url": "https://openalex.org/W4388833011",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5104200500",
      "name": "Shrishti Shah",
      "affiliations": [
        "Indian Institute of Information Technology, Nagpur"
      ]
    },
    {
      "id": "https://openalex.org/A5018024199",
      "name": "Jitendra V. Tembhurne",
      "affiliations": [
        "Indian Institute of Information Technology, Nagpur"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2102605133",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2193145675",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2109255472",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2964095005",
    "https://openalex.org/W2554867164",
    "https://openalex.org/W607748843",
    "https://openalex.org/W2777050237",
    "https://openalex.org/W4212819644",
    "https://openalex.org/W1932624639",
    "https://openalex.org/W3039368862",
    "https://openalex.org/W2772578059",
    "https://openalex.org/W2519080876",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4214709605",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W6600654476",
    "https://openalex.org/W6600339457",
    "https://openalex.org/W6600755281",
    "https://openalex.org/W6600648412",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6600042225",
    "https://openalex.org/W2043379390",
    "https://openalex.org/W3174480456",
    "https://openalex.org/W4226016066",
    "https://openalex.org/W6600116659",
    "https://openalex.org/W6600612351",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W3199093552",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6600191605",
    "https://openalex.org/W2005750530",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W2963093690",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W4306955484",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W2612624696",
    "https://openalex.org/W227501432",
    "https://openalex.org/W6681931893",
    "https://openalex.org/W4220957668",
    "https://openalex.org/W2954318041",
    "https://openalex.org/W4288083516",
    "https://openalex.org/W3043995050",
    "https://openalex.org/W4390873750",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W3105054740"
  ],
  "abstract": "Abstract Transformer models are evolving rapidly in standard natural language processing tasks; however, their application is drastically proliferating in computer vision (CV) as well. Transformers are either replacing convolution networks or being used in conjunction with them. This paper aims to differentiate the design of convolutional neural networks (CNNs) built models and models based on transformer, particularly in the domain of object detection. CNNs are designed to capture local spatial patterns through convolutional layers, which is well suited for tasks that involve understanding visual hierarchies and features. However, transformers bring a new paradigm to CV by leveraging self-attention mechanisms, which allows to capture both local and global context in images. Here, we target the various aspects such as basic level of understanding, comparative study, application of attention model, and highlighting tremendous growth along with delivering efficiency are presented effectively for object detection task. The main emphasis of this work is to offer basic understanding of architectures for object detection task and motivates to adopt the same in computer vision tasks. In addition, this paper highlights the evolution of transformer-based models in object detection and their growing importance in the field of computer vision, we also identified the open research direction in the same field.",
  "full_text": "Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/.\nREVIEW\nShah and Tembhurne  \nJournal of Electrical Systems and Inf Technol           (2023) 10:54  \nhttps://doi.org/10.1186/s43067-023-00123-z\nJournal of Electrical Systems\nand Information Technology\nObject detection using convolutional neural \nnetworks and transformer-based models: \na review\nShrishti Shah1 and Jitendra Tembhurne1*   \nAbstract \nTransformer models are evolving rapidly in standard natural language processing tasks; \nhowever, their application is drastically proliferating in computer vision (CV) as well. \nTransformers are either replacing convolution networks or being used in conjunction \nwith them. This paper aims to differentiate the design of convolutional neural networks \n(CNNs) built models and models based on transformer, particularly in the domain \nof object detection. CNNs are designed to capture local spatial patterns through con-\nvolutional layers, which is well suited for tasks that involve understanding visual hierar-\nchies and features. However, transformers bring a new paradigm to CV by leveraging \nself-attention mechanisms, which allows to capture both local and global context \nin images. Here, we target the various aspects such as basic level of understanding, \ncomparative study, application of attention model, and highlighting tremendous \ngrowth along with delivering efficiency are presented effectively for object detection \ntask. The main emphasis of this work is to offer basic understanding of architectures \nfor object detection task and motivates to adopt the same in computer vision tasks. \nIn addition, this paper highlights the evolution of transformer-based models in object \ndetection and their growing importance in the field of computer vision, we also identi-\nfied the open research direction in the same field.\nKeywords: Convolutional neural network, Object detection, Transformer-based \nattention, Faster R-CNN, Semantic segmentation, Segmenter, YOLO’s\nIntroduction\nObject detection (OD) is growing rapidly due to the rebirth of convolution neural net -\nworks. The deep CNNs are capable to learn prominent-feature representations of images \ndue to their typical hierarchical architecture, and hence, it offers a fast, rapid, and accu -\nrate way to predict the position of objects within the image. Moreover, Recurrent-CNN \n(R-CNN) [1] accomplished the noteworthy success in CV tasks, as CNN categorizes the \nclass of object only but not capable in determining the position or location of object in \nthe given image.\nDue to the few number of unsolved challenges and slow computing nature of OD, \nR-CNN is revised, and we witnessed the changes in R-CNN for object detection models \nsuch as fast R-CNN [2], faster R-CNN [3], region-based fully convolutional networks \n*Correspondence:   \njtembhurne@iiitn.ac.in\n1 Indian Institute of Information \nTechnology, Nagpur, \nMaharashtra, India\nPage 2 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n(R-FCN) [4], single-shot detector (SSD) [5], you only look once (YOLO) [6], spatial pyra-\nmid pooling network (SPP-Net) [7], and mask R-CNN [8]. These new models are proved \nsuccessful in computing better in terms of results and accuracy. It is noticed that object \ndetectors are categories as—(1) one-stage detector, such as SSD and YOLO, which \nachieves high inference speed, and (2) two-stage detector (such as R-CNN, fast R-CNN, \nfaster R-CNN) that provide high localization and better object recognition accuracy. \nOne-stage detectors compute the prediction boxes directly from the given input images, \nwithout applying region proposal stage, and thus, it becomes time efficient which can be \nemployed for real-time problems. On the other hand, the two-stage detector provides \nbounding boxes by applying region proposal network (RPN), which is followed by fea -\nture extraction stage.\nIn general, these new models ignite a need for their application for real-world prob -\nlems, and led to well-researched domains in the field of image object detection, which \nincludes pose detection, face detection [9], people detection [10], crowd detection [11], \ntraffic sign detection [12], pedestrian detection [13], etc. For the purpose of applying \nthese models in a particular field, there should be a pure understanding of particular \nmodel so that it becomes easy to adopt, along with technique of its application. In addi -\ntion to this, the models also led to new models of object detection and image classifica -\ntion. As a consequence, it helps in improving the applications of multi-region detection \n[14], instance segmentation [15], edge detection [16], salient object detection, action \nrecognition [17], fault detection, text recognition, etc.\nThe new approaches and models are continuously evolving; therefore, in this paper, we \naim to show the comprehensive study on transformer-based detectors that establishes a \npowerful backbone for visual recognition, and proved to achieve competitive results in \ncontrast with convolutional networks. Here, the new architectures along with old para -\ndigm are explored, and similar architectures using different modules that achieve better \ncompetitive results are identified. The CV community adapted transformers extensively \nin this field. The transformers and their variants are to be successful in CV tasks. The \ntransformer-based models are differentiated into two categories such as single-head \nself-attention wherein local or global self-attention is adopted within convolutional net -\nworks. Other is multi-head self-attention that cascades multiple transformer layers.\nTransformers-based modules are utilized for OD as per following sequence: (i) fea -\nture extraction by transformer backbones, along with R-CNN head for OD [18], pyr -\namid vision transformer (ViT) [19], Twins [20], CoaT [21], Swin transformer [22], \nconvolutional vision transformer (CViT) [23], shuffle transformer [24], CrossFormer \n[25], RegionViT [26], and focal transformer models [27], (ii) visual features extraction \nby CNN backbone and a transformer-based decoder for OD, i.e. detection transformer \n(DETR) [28], deformable DETR, [29], and (iii) end-to-end OD by transformer (i.e. \nYOLOS) [30].\nDuring the development of a model, a new module is developed to improve and over -\ncome difficulties and challenges in the domain. In CV, researchers designed improved \nCNN models to overcome the various complications encountered with the existing \nmodels for the real-world applications. The applications are face detection [31], human–\nobject detection [32], traffic signal [33], pedestrian detection [34], etc. With exception \nto this, new models are also designed and integrated with base detection transformer \nPage 3 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \nframeworks such as vision and detection transformer (ViDT) [35], multiple object detec-\ntion [36], instance segmentation [37], one-region multiple objects [38], etc. Moreover, it \nis observed that transformer models show a lot of potential to become a new generic \ndetection pipeline [39].\nIn this work, the review of existing detectors in convolutional networks and trans -\nformer-based architectures is presented, which share the same paradigm but have differ-\nent architectures. This study also enables to understand the adoption of different models \nfor specific application and enhances the scope for further development. The motiva -\ntion for this work is to offer OD understanding and applicability in various domains for \nresearchers started investigating on transformers in vision, object detection tasks, and \napplications. It is noticed that a number of transformer-based models are designed for \nCV task, though there is a scope still exists to deploy more accurate applications having \na highly precise real-time system.\nContributions\nThe contributions targeted in this article are as follows;\n1. To investigate object detection using CNNs and transformer-based architectures, \nand sharing same paradigm containing diverse architectures.\n2. To identify applicability and suitability of different models in object detection for \nspecific application and further improvements for advance development.\n3. To propose the insight and detailed analysis of utilization of transformers in com -\nputer vision, object detection and other similar tasks. Further, examined different \ntransformer-based models handling CV task to offer better accuracy in highly pre -\ncise real-time system.\n4. To identify the issues and propose the future directions in object detection task.\nThe paper organization is as follows: Working of different object detection models \nbased on RCNN, ViT, Faster R-CNN, Mask R-CNN, DETR, and YOLOs is presented \nin “Object detection models” section. “ Tasks and model evolution ” section reveals on \ntask and model evolution. “ Applications of transformers” section highlights the appli -\ncation of transformer in different domains. Further, “Datasets and evaluation metrics” \nsection, proposed the datasets and various evaluation metrics for OD. “Performance \nanalysis and discussion ” section presents the performance analysis on OD. “ Challenges \nin object detection ” section summarizes the challenges in OD. Finally, conclusion and \nfuture scope are discussed in “Conclusion and future scope” section.\nObject detection models\nIt is observed that for the task of OD, number of datasets—PASCAL VOC datasets [40], \nMicrosoft COCO datasets [41], ImageNet datasets, [42], etc., are utilized for model \ntraining and evaluation. At present, variety of datasets being utilized for different tasks. \nFigure 1 shows the emergence of various datasets adopted for OD. In addition, Fig.  2 \nhighlights the improvement in accuracy of various OD algorithms on MS-COCO, \nVOC07, and VOC12 datasets, since 2005 to 2021.\nPage 4 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nThe OD challenges includes—(1) non-accessibility of labelled dataset, (2) multi-scale \nimages with different objects, (3) overlapping objects in videos, (4) low-scale video pro -\ncessing, (5) inherent variant in objects occupying in terms of pixel, i.e. 60–70%, 10–20%, \nand few pixels or less.\nR‑CNN\nIt is generally acknowledged that the progress is slowed down during the year 2010–\n2012, as shown in Fig.  1, with small updates in SIFT [44] and HOG [45] models. In \nFig. 1 Timeline of important datasets\nFig. 2 OD on MS-COCO, VOC07, and VOC12 datasets [43]\nPage 5 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \n[1], a simple and scalable detection algorithm is proposed, which achieved mean aver -\nage precision (mAP) of 58.3% (i.e. 23% higher mAP than the existing detectors). Here, \nhigh-capacity convolutional network is applied to the bottom-up region proposal. The \nidea is really basic, i.e. it acquired the input image, extracted around 2000 bottom-up \nregion proposals (based on selective search), features are computed for every proposal \nusing a large convolutional network, and then linear support vector machine (SVM) as \na classifier is adopted to predict object in each region. Moreover, for recognizing object \ncategories and after scoring each selective search, class-specific bounding box namely \nregressor is created for OD. Figure 3 shows the improvements in mAP on PASCAL VOC \ndataset since 2006–2016.\nThe RCNN model consists of three modules, which is presented in Fig. 4:\n (i) Region proposal R-CNN is not a big fan of particular region proposal method, and \nthus, category-independent region proposals like selective search are utilized.\n (ii) Feature extractor The extracted feature vector from the region proposal is trans -\nformed into 227 × 227 RGB colour plane due to its compatibility with CNNs. Then, \nforward propagation is achieved through five convolutional layers and two dense \nlayers for feature calculation.\n (iii) Test time detection Each class is scored upon extracted features using SVM-trained \nclasses. By giving each score, bounding boxes are derived through greedy non-\nmaximum suppression.\nThe RCNN model achieved boost in performance and a large improvement of mAP \nthrough two comprehensions—(1) applying high-capacity CNNs to bottom-up region \nproposals to localize objects, and (2) train large CNNs when training data is labelled and \nuncommon thus pre-training the model for image classification, later fine-tuning the \nmodel for detection task. Although RCNN made a great progress, feature extractions on \nlarge amount of overlapped proposals (i.e. more than 2000 boxes per image) lead to very \nslow detection (14 s. per image on the GPU). As RCNNs are two-staged object detec -\ntors, the highest obtained detection accuracy is still slower.\nFurther, it is observed that fast R-CNN is developed with the intention to reduce the \ntraining time, as it runs the CNN once on the whole image instead of computing each \n2000 region of interest (RoI) from region proposal, individually. The end layer of deep \nFig. 3 Timeline of mAP on PASCAL VOC dataset\nPage 6 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nCNN is called as RoI pooling layer, which extracts specific features from input region. \nThe CNN output is now realized by dense layer. Now, model produces two outputs—(1) \nclass prediction via softmax function and (2) linear output pertaining to bounding box. \nThis process is repeated recursively for each RoI for a given image. It is successful in \novercoming the drawback of RCNN because it increases the mAP from 58.3 to 70%.\nRCNN marked a remarkable success, as it is among the first to detect position or loca -\ntion of object, and work suitably for multiple objects images. OD in real time is difficult \nto accomplish because it takes testing time of 47 s, approximately for every image, thus, \ntraining system pipeline is tough. Due to these limiting factors, further improvement is \nstill needed in RCNN.\nFaster R‑CNN\nFaster R-CNN was introduced as a first near real-time and end-to-end deep learning \n(DL) detector for object detection. Before the introduction of this network, the region-\nbased CNN models are computationally very costly, where the basic difference is the \nFig. 4 The detailed architecture of RCNN [2]. a RCNN step-by-step process. b Feature extraction from RoI\nPage 7 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \nmodel utilized region proposal method to create sets of regions. The test time, which \nis accelerated to near real-time, exposing region proposal computation as a bottleneck \nwith low computational time, and achieved the mAP of 69.9%. Figure 5 shows the model \nof faster R-CNN, comprising two modules:\ni) Region Proposal Network of CNN: It is present to propose the regions and then \npredicts the object bounds, object scores for every position. This acts like the atten -\ntion model, which is discussed earlier to inform the network where to pay more atten -\ntion. Faster R-CNN network is developed for extracting features, and then working on \nthe region proposal and producing—(1) class labels and (2) bounding box. The RPN \nis applied to generate region proposals; here, the model slides a small network upon \nfeature map generated by convolutional network. This small network accepts spatial \nwindow (n × n) as an input corresponding to feature map generated by convolutional \nnetwork. Every sliding-window mapping is performed with lower-dimensional feature \ndue to mini-network functions in a sliding-window manner, and dense layers sharing is \nconfirmed among all spatial locations. To train RPN, labels assigned under binary class, \ni.e. object or not object to each of the anchors (here negative mining is simply balancing \nby weights), and selecting anchor such that highest intersection upon union overlapping \ncorresponding to ground truth box (for this purpose, greedy selection non-max suppres-\nsion (NMS) method is utilized). This feature further supplied to two sibling dense lay -\ners, i.e. box classification and box regression, also known as REG and CLS. Thus, we \nconclude that this model enables a unified, DL object detection system, which runs at \napproximately in real-time mode. RPN further improved the quality indirectly which in \nturn enhancement in detection accuracy is witnessed.\nAfter faster R-CNN’s bounding box regression wherein through initial proposal, \npredicted bounding box’s location is refined, or anchor box will not assist for post-\nprocessing block. However, it is integrated with detector and training is performed in \nend-to-end manner to achieve better prediction and smooth functioning.\nii) Sharing Features for RPN and Fast R-CNN: This network layer does not consider the \nregion-based OD CNN instead adopt fast R-CNN.\nThe training of fast R-CNN and RPN is accomplished independently, the modification \nin convolutional layers is achieved differently. Therefore, technique needs to be devel -\noped for allowing sharing of convolutional layers amid two networks. The second-stage \ndetector makes predictions relative to some initial guesses made by the RPN, whereas \nFig. 5 Detailed architecture of faster R-CNN [3]\nPage 8 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nsingle-stage methods use anchor boxes. Nevertheless, the recent one demonstrates that \nits final performance heavily depends on initial guesses. This is done by directly setting \nthe losses wherein no post-processing is required. Thereafter, the handcrafted procedure \nis removed and detection is streamlined through the direct prediction via absolute box \nprediction for the set of detections.\nVision transformer (ViT)\nIn [46], a new research direction is offered wherein the model is developed as a com -\npetitor to the CNNs. It also attained an excellent results while requiring substantially a \nfewer computational resources for the training purpose (i.e. almost four times in terms \nof computational efficiency and accuracy). Subsequently, it is witnessed that the trans -\nformer models made remarkable existence in natural language processing (NLP), as they \nare solely based on attention mechanism. It is seen in transformer model [47] that it is \na combination of various attention mechanisms. It consists of following parts, which is \nalso depicted in Fig. 6.\ni) Attention: This is discovered to let the decoder utilize the most relevant part of the \ninput sequence by a weighted combination of all encoded input vectors. The attention \nalso helps the model not to forget the input and the decoder to know where to focus. \nHerein, each vector query, i.e. q = st − 1 , previous decoders output against a database \nof keys to compute a score value, which is computed as a dot vector of specific query \nwith key: e(q , k ) = q .ki (captures each feature to see its relation with other features). The \nabove score is then passed through the softmax α (q ,ki) = softmax (e(q,ki)) , and atten -\ntion is calculated by a weighted sum of vector value v(k i) in order to retain the focus on \nthese words that are relevant to the query.\nhere, the attention function is described as—(1) a query and (2) set of (key, value) pairs \ncorresponding to output, and all the parameters are vectors. The output computation \ncomprises weighted sum wherein weight assigned (value) is calculated using compatibil -\nity function related to query along with the selective key. In a pictorial representation, if \na single object is considered, the attention between patches containing parts of the same \nobject will be high rather than a patch containing background and another containing an \nobject.\nii) Self-Attention: It is proposed due to the challenges faced by the encoder–decoders \nin dealing with long sequence models. In the attention model mechanism, the output of \nAttention (q,K ,V ) =\n∑\nα (q,ki).v(ki)\nFig. 6 The detailed architecture of vision transformer [48]\nPage 9 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \nthe decoder focuses attention mainly on the input, whereas in self-attention models, the \ninput to interact with each other is now allowed.\nThe scaled dot product attention initially computes a dot product for each q and k, it \ndivides each result by \n√\ndk , so that after the dot product cause some self-attention scores \nto become small. Hence,\nIn transformer models, relevance of one item with other items is computed using self-\nattention. Further, self-attention layer updating each computation in sequence by apply-\ning global aggregating on finished input sequence, and hence, it does not forget and \ncapture the interaction among all the entities of input.\n(iii) Self-Attention in Vision: This model allows long-term dependencies while handling \nthe sequence elements, and proves to be better than CNNs (which needs large receptive \nfields). A single head of self-attention works similar to that of the above model, it gives \ninput sequences of image features, i.e. all pixels in the given patch, computes q, K, and V \nvectors, and aggregated spatial information which is identified within the patch. The V \nvectors aggregation is performed after projecting softmax score of q and K, and triplet \n(key (K), query (q), value (V)) is calculated, which is followed by attention computation. \nThereafter, applying it to reweight the values, the output projected is employed to find \noutput features confirming same dimension as that of input.\nThe feature maps input to self-attention, compute their response at a position (i.e. \npositional embedding), and make it possible to capture relations or connections between \nany two locations or positions in the map. This is irrespective of distance; hence, infor -\nmation is integrated across the image for lowest layers as well.\nSelf-attention has two types to implement vector attention which learns weight corre -\nsponding to channel and spatial dimensions—(1) pairwise and (2) patch-wise. The pair -\nwise self-attention computes vector attention as a relationships of feature corresponding \nto neighbours in a particular local neighbourhood. On the other hand, self-attention \nusing patch-wise mechanism offers generalization using convolution operator. Eventu -\nally, the model applied over image regions is found, which are semantically significant \nfor classification. Therefore, as a concluding part, it is computationally intensive and \nallows to capture long-term interaction, also and focus on the importance of particular \nfeature. Explicitly modelling all the pairwise relations between elements in the sequence, \nthus, makes it suitable for specific constraints such as removing duplicates. Moreover, \nself-attention is utilized as one of the layers in the object detection transformer model. \nAttention is applied in some layers and used to connect two modalities like the encoder \nto the decoder, while self-attention is applied within a component.\niv) Multi-headed attention: Fig.  7 shows the flow diagram of multi-headed attention \nwherein combination of n single-head self-attention, each consists of three parameter \nmatrices of their own (i.e. weight matrices {Q i, Ki, Vi}). Concatenation of output in con -\ntext vectors of each head is the output of multi-headed attention layer. It is seen that the \ndifferent learned representations can improve the transformer model. Self-attention is \ninvariant to permutations and modification in the input points occurs when combined \nfor multi-headed attention. It easily operates on irregular input data, unlike conventional \nAttention (Q ,K ,V ) =\n∑ (α (Q ,Kt)√\ndk\n)\nv(ki).\nPage 10 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nconvolution, and requires a specific grid structure. Self-attention offers the capability to \nlearn the features from global and local region, and hence, their experimental results \nconfirm that the multi-headed self-attention (along with sufficient parameters) is a more \ngeneralized procedure.\nThe transformer model consists of encoder–decoder structure wherein inputs and \noutputs sequences are supplied with one element at a time.\nv) Encoder: The encoder is a stack of identical layers (N = 6) wherein every layer con -\nsists of two sub-layers. First layer is a multi-headed self-attention, each of these layers \ncontains all the queries, keys, and values that belongs to same place. Here, previous layer \noutput belongs to encoder, and every position in encoder attends to all the positions in \nprevious layer in encoder. Further, second layer is a position-wise dense layer feed-for -\nward network.\nvi) Decoder: The decoder is also a stack of identical layers (N = 6). Decoder consists \nof three sub-layer, two sub-layers are similar to encoder wherein third layer performs \nmulti-headed attention on output of encoder stack. Similar to encoder, residual connec -\ntion is employed around each of sub-layers (i.e. self-attention layers) which is followed \nby the layer normalization.\nThis transformer model replaced the CNNs backbones in OD models, some models \nimproved their detection accuracy by combining various feature maps [48] in multi-\nheaded self-attention scheme. The target detection performance is drastically improved \nby integrating feature maps output from CNNs, or already existing object detectors with \nmulti-headed attention, or attention modules fusion features. Sometimes, the encoder \nmodule is replaced or integrated, while at some places, the decoder module is replaced; \nhowever, the structures and major idea behind functioning remain the same. Minimal \nconsiderations that are adequate to overcome the aforesaid challenges are required.\nIn [49], it is stated that for years, OD models rely on recognizing the object instances \nindependently without exploring their relation during the learning of the model. Here, \nobject relation component is proposed wherein object set is processed simultaneously \nthrough collaboration between their feature appearance and geometry, and thus, per -\nmitting modelling of their relations, improving object recognition and even removal of \nFig. 7 Multi-headed attention scheme [46]\nPage 11 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \nduplicates is also performed. This perfectly suits like the working of transformers, where \ndemonstration explained that pure transformer employed to the sequence of image \npatches performs well on OD tasks. The image is extracted into patches by looping over \nannotations and image (linear projection of flattened patches), and positional embed -\nding (convolution is a translation and scale equivariant while pooling is a translation and \nscale invariant). Both equi-variance and invariance are important for recognition, object \ndetection, and segmentation purposes. Vision transformer is observed to become invari-\nant to the position of patches, and hence, positional embeddings are added making it \nthe only inductive bias, passed through the ViT model instead of CNN (like in RCNN), \nmulti-headed attention layer is utilized for self-attention, and applied to a sequence of \nimage patches. The encoded patches and self-attention layers outputs are normalized \nand fed into a multilayer perception (MLP) [50]. The MLP is used for classification head \nalong with a hidden layer at the time of pre-training, and fine-tuning is performed by \nsingle linear layer. The MLP mixer comprises classifier head, mixer layers, and per-patch \nlinear embeddings. In addition, channel-mixing and token-mixing MLP are the part of \nmixer layer wherein each consisting of two dense layers and GELU. The unused outputs \ncorrespond to input patches from MLP layer (ViT classifier) that can encode local infor -\nmation, which is beneficial for performing OD. The model outputs the four dimensions \nrepresenting the bounding box coordinates of an object.\nSubsequently, it is found that CNN lacks in global understanding of the image. To \ntrack down long-range dependencies within the image, CNN needs large receptive fields, \nwhereas ViT model for object detection performs better than CNN [51]. The model \nadvantage over RCNN is that it is pre-trained on large textual corpus, further, fine-tuned \non dataset of smaller task, which leads to computational efficiency and scalability. It also \nsupports multi-scalable features, due to densely vision tasks generally involve visual \nobject’s understanding with different size and scale. Transformers can be pre-trained on \ndata of enormous amount, further, applied to specific smaller tasks through fine-tuning. \nThe high computational complexity of self-attention and attention models indicates that \nthere is a limitation of low-resolution inputs. Hence, few applications of CV might con -\ntain some limitations with transformer models [52].\nDetection transformer (DETR)\nThe architecture of DETR shown in Fig.  8 is comparatively simpler than all the previ -\nous transformer-based architectures, which contains all kinds of engineering hurdles, \nthresholds, hyperparameters, and are unable to become competitive with stronger base -\nlines. The design of DETR proposed a direct set prediction problem with unique predic -\ntions via bipartite matching, which uses transformers encoder–decoder model.\nDETR directly predicts set of detections by combining CNN and transformer model. \nCNN utilized to learn 2D representation, flattening and positional encoding is per -\nformed before the transformer encoder. The transformer decoder decodes the input, \ni.e. learned positional embeddings, namely, object queries. Further, decoder output (i.e. \nembedding) is supplied to feed-forward network (FFN) to detect “object” (class and \nbounding box) class or “no object” class. Using a self-attention encoder–decoder model \nupon these embeddings makes the decision that objects together persisting pairwise \nPage 12 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nrelation. Parallelly, whole image is considered as context and also makes these mod -\nels suitable for set prediction under constrained environment, i.e. removal of duplicate \npredictions.\nThe main components are described as follows:\n (i) CNN Backbone: Compact features are extracted (a lower-resolution activation \nmap, and 2D representation of an image), where channel dimension is reduced \nby 1 × 1 convolution for high-level activation map to smaller dimension by con -\nstructing new feature map by zo ∈ Rd×H ×W  . The model shown here is flattening \nand supplementing it with a position encoding before encoding using transformer \nencoder.\n (ii) Transformer Encoder: It consists of six standard encoder layers wherein each layer \nmade up of multi-headed self-attention and FFN. Like, the encoder module of ViT, \nits module also helps to understand the patches with each other globally (which \nis possible through global scene reasoning), and thus, it is important for disen -\ntangling objects. It also seems to separate cases that simplifies the extraction of \nobject and decoder’s localization. The spatial dimension of z o (i.e. output of CNN) \nis also collapsed into one dimension. Here, spatial resolution of encoder plays vital \nrole in determining OD performance, i.e. significant AP gain is achieved by spa -\ntial features resolution increment. This is comparatively better than the CNN out -\nFig. 8 The architectural details of DETR [28]\nPage 13 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \nput feature maps for sliding window. The output of encoder layer is reinterpreting \nthe final transformer states (encoder from ViT), however, eliminating class token \ninstead of outputting spatial feature map.\n (iii) Transformer decoder: It consists of six standard decoder layers, each of which has \nembedding of size “d” with multi-headed, self-encoder–decoder attention at every \nlayer. Decoding performed on positional embeddings using transformer decoder. \nUnlike, the attention model, it is not an autoregressive model and decodes the \nobjects parallelly. Consequently, detection pipeline is simplified by dropping com -\nponents of multiple hand-designed—encoded with prior knowledge, such as non-\nmaximal suppression or spatial anchors. Here, the improvement, which is included \nby NMS, diminishes with increase in depth. Due to self-attention upon activation \nfunction allowing the model to prevent duplicate predictions. Prediction FFNs \nand Hungarian loss after every decoder layer are added. All prediction FFNs share \nparameters, which forecast the detection of an object. The transformer decoder is \nbasically replacing the greedy selection and RoI pooling of faster R-CNN.\nSome of the unique features of DETR are as follows:\n1. DETR model as compared to previous models that work on direct set prediction \ncontains an extra feature, i.e. conjunction of bipartite matching loss.\n2. Unlike, other transformers, it performs non-autoregressive decoding.\n3. It is equivalent to faster R-CNN training, which balances the proposals in positive/\nnegative by subsampling. Matching cost considers both the classes’ predictions and \nsimilarity in prediction and ground truth boxes.\nDETR model thus attains comparable performance against the competitive faster \nR-CNN. The architecture is similar to few modules that is replaced by the transformer \nencoder–decoder. DETR exhibits expressively better results on objects in large size, the \noutcome likely to be achieved by non-local computations of transformer. However, the \ndrawbacks of this model required more training epochs compared to typical detectors to \nconverge, and achieved relatively low performance for small objects.\nDeformable DETR\nIt consists of deformable attention unit learns to attend sampling locations within fea -\nture map, and hence capable in processing high-resolution feature maps. It uses Rela -\ntionNet (RN) [53] and non-local networks (NLN) to form attention amid pixel features \nand bounding box features for the purpose of OD.\nToward transformer‑based object detection\nHowever, DETR models perform encoding of visual features using CNNs, whereas \ntransformers are utilized to decode features into OD outputs. However, transform -\ners adopted for encoding visual features while the RPN model applied for detect -\ning outputs are surveyed successfully. It usually adds a detection network to a ViT. \nThe ViT utilizes merely the state analogous to input class token on final layer and \nis outputted through MLP classifier head. It is resemblance to transformer encoder \nPage 14 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nof DETR. In this model, the remaining tokens which are only utilized as features to \nattend the class token are applied to encode the local information. This corresponds \nto the input patches and are used for detection, as it outputs a visual representation \nof patches if image is considered globally. By reinterpreting outputs in spatial man -\nner, feature map is created that certainly lends itself as an input to detection model \nsimilar to faster R-CNN. Moreover, detection network of faster R-CNN contains RPN \nthat densely predicts the presence of object. The features correspond to top region \nproposals, further, RoI pooled and supplied to detection head, and classification is \nperformed for each region, in addition, coordinates of bounding box are confirmed by \nregresses. Herein, RPN predicts the region with objects by generating several predic -\ntions per location from feature maps, which is outputted by the encoder. Predictions \nare employed to mark the territory in the form of anchor boxes of varying sizes and \naspect ratios. Generating one feature per region proposal is the RoI pooling that is \nmainly achieved and at the end. These pooled features are passed through the pairs \nof heads, one for classification of the object detected and the other is bounding box \nregressor. The detection network works exactly like the transformer decoder in DETR.\nFigure  9 shows the detailed flow diagram of toward transformer-based OD, which \nis a fully trainable end-to-end like DETR with few numbers of added adaptations and \nreplacements. Transformer demonstrates excellent performance in pre-training of a \nlarge dataset and transferred to fewer data points via fine-tuning. This is observed \nbefore as well, however, when training is performed upon strong visual representa -\ntion, it is found that network based on transformer can be developed for certain vis -\nual tasks. Further, resolution of input image limits the performance. Thus, this model \nis quite competitive with the DETR model.\nAuthors in [18] also investigated various methods of using features of intermedi -\nate encoder (input) to detection network, wherein output obtained from last layer of \ntransformer is utilized and concatenation is performed with all state of intermediate \ntransformer. Finally, it is found to become helpful to augment intermediate residual \nblocks among spatial feature map of encoder and detection module, thus, AP gain is \nachieved (it indicates that pre-trained transformer for classification only not adequate \nfor detection task). Here, conclusion is also made that transformer backbone com -\nbined with different modules of CNNs models can possibly make progress on com -\nplex vision tasks because it can pre-train on large data and fine-tune on new data with \nimproved complications. As a consequence, model is capable to handle wide array \nof vision-related problems. However, recent research recommends that OD can be \nFig. 9 Architecture of toward transformer-based object detection [18]\nPage 15 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \nenhanced by adopting semantic segmentation (SS), where object boundaries are well \nencoded along with accurate object localization that learned with segmentations.\nMask R‑CNN\nIt is noticed that SS detects all present objects at the image pixel level. On the other \nhand, instance segmentation identifies every object instance corresponding to each \nobject of image. The instance segmentation is able to improve the visual understanding \nof the surround world and gained huge attention in various CV applications. Its main \nobjective is to perform classification of each pixel into different categories.\nFaster R-CNN is not intended for detecting alignment in pixel-to-pixel; hence, it is \nextended by adding predicting segmentation masks for every RoI (possible by utilizing \nsmall FCN, and segmentation mask is predicted based on pixel-to-pixel basis) in paral -\nlel with classification and bounding box regression. Faster R-CNN after RPN, extracting \nfeatures by RoIPool from every candidate box and perform classification and bounding \nbox regression. Faster R-CNN contains two outcomes for every candidate object, i.e. off-\nset of bounding box offset and class label. Now, third branch that produces object mask \nis added to this model. There are two stages with two different neural networks, first \nstage is faster R-CNN’s basic RPN model, which is applied to extract the RoI. It helps to \nfind the class label and computes the bounding boxes. Second stage contains a neural \nnetwork with a similar procedure to that of RPN. It extracts region from the first stage \nand without anchor box which uses RoIAlign for locating each importance of feature \nmap, generates pixel-wise mask.\nThe model of mask R-CNN is presented in Fig.  10, we also, identified some basics \nfoundations are as follows:\ni) Here, CNN is utilized to generate feature map of image. RPN utilizes CNN to pro -\nduce multiple RoI by lightweight binary classifier to differentiate the existence or nonex-\nistence of object. The output is computed by passing NMS to anchors with high scores. \nThis is analogous to RPN module of faster R-CNN and the transformer’s decoder.\nFig. 10 Work flow diagram of Mask R-CNN [8]\nPage 16 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nWrapped features are passed through two dense layers to make classifications and fed \ninto mask classifiers with two FCN [54] layers for each RoI. After that every mask for \neach class is generated. Now, by evaluating the convolutional network on each extracted \nRoI makes an object detection model segmented.\nii) To make understanding of the main feature: It is observed that extracting the RoI \nis different here. RoIPool is employed, and then passed through RoIAlign for fast speed \nand better accuracy. Mask R-CNN is indirectly a faster R-CNN learning extraction of \nRoIs and attention mechanism using RPN. RPN-generated RoIs which are reduced to \nmake the discrete granularity of feature maps through RoIPool perform coarse spatial \nquantization related to feature extraction. This quantized RoI further partitioned into \nspatial bins that are quantized themselves. Now, addressing misalignment issue, quanti -\nzation-free layer, namely RoIAlign, tries to preserve precise spatial locations with no loss \nof data, and outputs multiple bounding boxes are wrapped into a fixed dimension. It is \nfound that RoIAlign done a great impact by improving mask accuracy relatively 10–50%. \nMoreover, it is concluded to become vital to predict class and decouple mask. Here, \nbinary mask for every class is individually predicted without any competition among the \nclasses. This depends upon network’s RoI classifying branch for predicting category. It is \nseen that stricter localization metrics for extracting features with exact spatial locations \nwill lead to bigger gains.\nTransformer models become successful in replacing the FCN models wherein stacked \nconvolutions are employed to capture semantic information. However, self-attention \nis able to model the rich interactions between pixels and gives competitive outputs in \ncomparison with CNN-based compact prediction tasks (i.e. image segmentation and \nsemantic). Number of segmentation approaches are inserted self-attention along with \nCNNs. However, some recent researches proposed transformers encoder–decoder like \nSEgmentation TRansformer (SETR) [55], which contains ViT encoder, and two decoder \ndesigns pertaining to progressive up sampling, multi-level feature aggregation. The Seg -\nFormer [56] consists of hierarchical pyramid ViT (lacking position encoding) encoder \nand segmentation mask is generated by MLP-based decoder (sampling operation). \nImage features are extracted using segmenter (ViT encoder) and segmentation mask \nprediction is performed by decoder (mask transformer).\nViT segmenter\nIn [57], a segmenter and transformer model for the purpose of semantic segmentation \nis introduced. Here, semantic segmentation is utilized to make partition within image \ninto segments to provide high-level image representations of the target task. This is \naccomplished by assigning each pixel of image to category label pertaining to underlying \nobject. The approach is purely transformer-based, as it is built on ViT and extended up \nto semantic segmentation. The ViTs do not use CNN, however, capture contextual infor-\nmation through designing, and outperform the FCN-based techniques. Further, image \npatches are estimated to sequence of embeddings and transformer is used for encoding. \nThe output from ViT encoder and obtained class labels from the embeddings are taken \nas an input by the decoder, i.e. the mask transformer. A transformer-based decoder is \nproposed for class masks generation, which outperforms the already existing linear base-\nline models, and further extended to accomplish image segmentation tasks, in general. \nPage 17 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \nLater, model is trained in end-to-end and pixel-to-pixel with cross-entropy loss at each \npixel. Therefore, the transformer decoder model can be seen to replace the FCN, but \nthis decoder is unlike the ones who proposed their models for object classification and \ndetection tasks.\nFigure 11 illustrates the ViT segmenter model, whose encoder and decoder modules \nare detailed as follows:\ni) Encoder: It comprises multi-headed self-attention unit wherein point-wise MLP \nunit (two layers with layer norm (LN)) is placed after aforementioned unit, and MLP \nunit employed prior to every block (like a typical encoder module). Here, each of the \nsplit image patches is flattened into 1D form, further, it is linearly projected into patch \nembedding along with positional embeddings. These patches are then applied to the \nencoder for generating sequence of contextual encodings (where specific relation of the \npatch is specified in a special context). These contain rich semantic information and can \nbe utilized by the decoder.\nIt is observed that output embeddings correspond to respective image patches, further \nembeddings are utilized to obtain class labels with point-wise linear decoder or decod -\ning using mask transformer. This decoder when pre-trained for classifying image shows \nthat fine-tune can be done on datasets with moderate-size. Here, decoder is ready for \nsemantic segmentation. The linear decoder then allows to achieve exceptional results; \nhowever, further performance improvement can be accomplished to generate class \nmasks by mask transformer.\nii) Decoder: The mapping between patch-level (PL) encodings to PL class scores is \nachieved by point-wise linear decoder. Subsequently, these PL class scores are up sam -\npled through bilinear interpolation to PL scores, and segmentation map is obtained from \nclass dimension using softmax. Mask transformer, i.e. decoders, inserts set of learnable \nFig. 11 Description of ViT segmenter model [57]\nPage 18 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nclass embeddings, wherein each class embeddings are randomly initialized and allocated \nto single semantic class. The masks are generated by mask transformer (i.e. scalar prod -\nuct among patch embeddings), which are entered to the decoder and class embeddings \nyield by decoder. Here, class masks are computed, further, final segmentation is achieved \nthrough softmax applied on class dimension, followed by LN to obtain class score (pixel-\nwise). Thus, it is verified that patch sizes are a key factor. The sharper boundaries can \nbe obtained through patch size reduction, whereas incrementing patch size results in \ncoarser image representation.\nCNN-based model or CNN along with the transformer-based models basically split, \nclass embeddings and pixel into two different streams due to the computational con -\nstraints. Herein, both are jointly processed during the decoding phase, and hence allow \nthe production of dynamic filters with changing inputs. Subsequently, it is accepted \nas true that their encoder–decoder transformer in end-to-end manner, firstly, offers \nunified technique, for instance, segmentation, panoptic segmentation, and semantic \nsegmentation.\nThis study aims to show that the transformer model in combination with other already \nexisting models to make a great difference in the computational power and accuracy \nleads to a new research work and future directions.\nYou only look once (YOLO) [6]\nIt is noticed that YOLO achieved better results and outperformed other real-time OD \nalgorithms with higher performance. When compared with other R-CNN object detec -\ntion models, paradigm of proposal detection and verification is not followed by them. \nInstead, the proposed architecture uses end-to-end neural network, predicts bounding \nboxes along with class probabilities in one step, similar to fully convolutional networks. \nFigure 12 shows the application of objects detection using YOLO.\nYOLO architecture consists of total 24 convolutional layers with two dense layers at \noutput stage. It divides image into equal dimensional regions called grids. These cor -\nresponding grids predict the bounding boxes with respect to each cell’s coordinates. \nThis means that the model reframed OD as single regression problem, i.e. finding \nimage pixels, obtaining coordinates of bounding box, and producing class proba -\nbilities. In addition, this model also greatly lowers computations, because detection \nfollowed by recognition is performed by the cells of image, unlike techniques such \nas region proposal and sliding window. The entire image scanning is performed by \nYOLO during train and test stage; therefore, it implicitly encoded the contextual \nFig. 12 YOLO [6]\nPage 19 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \ninformation belonging to the classes and corresponding global appearances. YOLO \ndesign offers real-time speeds and training in end-to-end fashion.\nFrom the past reviews about YOLO, it is concluded that YOLO is unified model to \nperform OD, ease in constructing and training can be accomplished on full image. \nContrasting classifier-based techniques, YOLO’s training achieves better performance \ndue to loss function directly linked with detection performance, hence, entire model \ntrained, jointly. Thus, YOLO suited ideal for robust and fast OD applications.\nOver the years, varieties of improvements proposed on YOLO’s v2 and v3 versions \nto enhance detection accuracy and focusing to maintain higher detection speed [30]. \nDespite the improvement in detection speed, we notice drop in localization accuracy, \ncompared with two-stage detectors, especially for small objects. Lower localization \naccuracy witnessed by imposing strong spatial constraints on bounding box predic -\ntion, and hence struggled with small objects in groups. So, bounding boxes prediction \nfrom dataset is also reported by the model. Therefore, it struggles in object gener -\nalization when input object consists unusual aspect ratios or new object. The main \nsource of error is the incorrect localization. When faster R-CNN is combined with \nYOLO, mAP is increased by 3.2%.\nYOLOS [30]\nThis model is a transformer block like the YOLO CNN-based model. In addition to \nthe ViT, the model consists of a detector portion of the network that maps a gener -\nated sequence of detection representations to class and box prediction. The ViT is \ndeveloped to handle long-range dependencies wherein global contextual information \nis stored rather than region-level and local relations. Moreover, ViT does not sup -\nport hierarchical architecture, whereas modern CNNs are capable of handling large \nvariations in scale (input). From the literature, the point is unclear regarding pure ViT \ncapability to transfer pre-trained visual representations belongs to image-level recog -\nnition task to complicated 2D OD task.\nYOLOS is a simple with attention-only network and directly constructed upon ViT, \nit uses object query tokens (i.e. multiple learnable parameters) instead of class token. \nHere, bipartite matching loss is utilized for OD similar to DETR model. YOLOS \nexhibits, flexibility of ViTs in OD in learning features in sequence-to-sequence \nfashion along with optimal 2D inductive biases of image. Although being effective, \nYOLOS fails to remove CNN networks for high CV tasks. The performance of model \nis encouraging and preliminary results show the meaningful information, which \nsuggests the robustness and generalization capability of transformer for varieties of \ndownstream tasks. In comparison with other YOLO models, the accuracy is not best \nin class but has a future scope:\ni) YOLOS needs 150 epochs for transfer learning (TL) to build pre-trained ViT for the \ntask of OD, and performance in terms of detection accuracy is low, thus, scope for \nimprovement is inherent.\nii) Focus is to work upon learning through visual representation for task-agnostic trans -\nformer models than task-oriented design with the fewest possible costs.\nPage 20 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nIt is noted that we can feasibly combine other recent ViTs along with transformer-\nbased detection heads and develop pure ViT network like DETRs, the VITFRCNN, the \nsegmenter, and many more. Mostly, models for CV tasks are seen to replace or compute \nthe same architecture in the transformer sector.\nHere, Table 1 summarizes various models applied for the task of semantic segmenta -\ntion, OD, and image classification based on model highlights and limitations, after inves-\ntigating the studies on image classification, object detection, and image segmentation.\nTasks and model evolution\nIn this section, we discuss the task and model evolution for OD work.\nTechnical evolution of bounding box (BB) regressor\nBounding box (BB) regression is a vital method in OD wherein it refines the location of \npredicted BB through initial proposal or anchor box. The evolution of BB regression is \nas follows: absence of BB regression (before 2008), to from BB to BB (2008—2013), and \nthen to from feature to BB (after 2013) (R-CNN, fast R-CNN, faster R-CNN, and YOLO).\nAfter transformer-based models are proposed, DETR, VITFRCNN, and other object \ndetection-specific models are based upon extracted features from the encoders. For fully \ntransformer-based models, extracted features from the encoders are computed through \ndecoders for predicting the boxes and sent through the prediction head (i.e. FNN).\nFor transformer-based models along with other detectors used, features are extracted \nfrom encoders for predicting boundary boxes, while some models even extract features \nfrom CNN and predict the boxes and select anchors using the decoders. The BB regres -\nsor method remains same “features to BB” , where features are extracted from different \nmodules or predicted through different models, and this depends on the architecture of \nthe detector.\nEvolution of non‑max suppression (NMS)\nNon-maximum suppression is one of the significant group of methods in OD. Due to \nsimilar scores of detection because of neighbouring windows, NMS employed the step \nof post-processing for—1) removal of duplicate BB, and 2) to obtain detection result. The \nNMS [59] is gradually advanced into three groups of techniques, discussed as follows:\n (i)  Greedy Selection (GS): It is applicable to overlapped detection of BB consisting \nmaximum score of detection, whereas others are removed. This selection is used \nby R-CNN, fast R-CNN, faster R-CNN, and YOLO. The GS still considered the \nstrongest baselines for today’s OD purpose.\n (ii)  BB Aggregation: It utilizes full attention of object relationships and corresponding \nspatial layouts.\n (iii)  Learning to NMS: These showed encouraging results in enhancing the occlusion, \ndense OD over the traditional handcrafted NMS techniques.\nIt is noticed that transformer-based models like DETR do not need NMS in their \ndesign because of its set-based loss. It is predicted in [59] that learnable NMS tech -\nniques, relation models explicitly design the relations among different predictions along \nwith attention. By utilizing direct set losses, no post-processing steps are required. Fully \nPage 21 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n Table 1 Summary of state-of-the-arts for semantic segmentation, OD, and image classification\nTask Method Design Highlights Limitations\nImage classification ViT [46] Encoder—NLP transformer for images Transformer (global self-attention) applied on \npatches of image\nLarge-scale dataset training (image size—300 M)\nLinearly embedding—image patches by posi-\ntional embedding\nConvolution-free network Careful TL for new task\nOutperforms ResNet Large model consisting 632 M parameters for \nSOTA results\nIt also attained excellent results while requiring \nsubstantially fewer computational resources for \ntraining purpose (i.e. almost four times in terms \nof computational efficiency and accuracy)\nObject detection RCNN [1] Resized and cropped regions classification by \nCNN\nFrist real-time efficient object detection model \nusing CNNs\nSlow training and detection\nRegion proposal BB refinement by SVM, trained \nby CNN features\nAllows custom region proposal More training time for classification of 2000 region \nproposals each image\nSelective search method is adopted, thus, no \nlearning in the stage\nGenerates bad region proposals\nFast RCNN [2] Fast R-CNN along with edge boxes applied for \nregion proposals generation\nR-CNN must classify every region Fast R-CNN performance is low due to region \nproposals identification\nR-CNN used for cropping and resizing region \nproposals\nFast R-CNN pools features from CNN belongs \nregion proposal\nFast R-CNN is good when not region proposals. \nTherefore, estimating region proposals\nFast R-CNN handles entire image Fast R-CNN efficiently worked compared to \nR-CNN, because, estimations are shared for \noverlapping regions\nFaster RCNN [3] No selective search method Optimal run-time performance RPN training is performed with all anchors (mini-\nbatch—size 256)\nSeparate network for predicting region proposals Improvement over its predecessor with respect \nto run-time speed and raw performance\nExtraction perform on single image\nPage 22 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nTable 1 (continued)\nTask Method Design Highlights Limitations\nRegion proposals prediction reshaped by layer \nof RoI pooling, later used to classify image under \nproposed region, further, prediction of offset for \nBB\nRPN is faster as compared to Selective Search Network convergence is slow\nFixed-length features are extracted from every \nregion proposal by layer of RoI pooling\nDETR [28] Linear projection layer is applied for CNN feature \ndimension reduction\nEnd-to-end pipeline of training using trans-\nformer for OD\nMore time for convergence\nEncoder–decoder consists of spatial positional \nembedding at each layer of multi-head self-\nattention\nNo manual post-processing stage Low detection accuracy for small objects\nOutput positional encoding, i.e. object queries, \nis added to the layer of multi-head self-attention \nin decoder\nHungarian loss is used\nD-DETR [29] Deformable transformer with deformable atten-\ntion layers for sparse priors\nBetter performance for small object compared \nto DETR\nSOTA results using 52.3 AP\nApplied multi-scale attention Converged fast compared to DETR Augmentation in test time\nVITFRCNN [18] Transformers for encoding visual features while \nRPN for detecting outputs\nPre-training capacity is large More training time on large-scale dataset (300 M)\nAdds detection network to ViT Fine-tuning performance is fast Training from scratch is difficult for smaller \ndatasets\nViT used for state related to input class token and \nis outputted through MLP classification head\nInvestigated improvements—superior perfor-\nmance is reported for image in out-of-domain, \nand better performance for large objects\nGPU memory limitation\nAvoiding spurious over detections Self-attention and convolutional layers relation-\nship, and limitations of CNNs\nYOLO [6] YOLO predicts the BB YOLO is faster than other OD algorithms Small objects detection issue\nYOLO estimates the class probabilities for BB Better accuracy of prediction and better IoU in \nBB\nSpatial constraints lower small objects detection\nPage 23 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n Table 1 (continued)\nTask Method Design Highlights Limitations\nYOLOS [59] Transformer block like YOLO CNN-based model 2D OD is accomplished in pure sequence-to-\nsequence way with optimal addition of inductive \nbiases\n150 epochs needed for TL\nIn addition to ViT, this model consists of detector \nportion of the network that maps a generated \nsequence of detection representations to class \nand box prediction\nPerformance is encouraging Learning through visual representation\nPreliminary outcomes are significant\nRank-DETR[70] Rank-based design with SOTA is improve by Rank-DETR Rank-based design needs to be explored more\nprompt engineering Backbone of ResNet-50, Swin-L, and Swin-T is \nused for enhancing localization accuracy\nMore computing time\nRank-based loss calculation, matching cost for \naccurate localization accuracy rank\nMore AP under higher IoU\nSemantic segmentation Mask RCNN [8] Mask R-CNN used for predicting an object mask \n(RoI), also recognized BB\nSimple to train and outperforms state-of-the-arts Process still images, not capable to explore tempo-\nral details of object\nPerform image segmentation, i.e. Semantic and \nInstance\nContains small overhead compared to faster \nR-CNN\nFails in detecting object suffering from low resolu-\ntion\nGeneralization is easy\nViT Segmenter [57] Encoder: projecting image patches in sequential \nembeddings, further, encoding using transformer\nGlobal context captured by transformer Not computationally feasible\nDecoder: mask transformer result from encoder, \nclass embeddings, further, predicts segmenta-\ntion masks\nDecoder: simple point-wise linear is applied to \npatch encodings for better results\nReduction in patch size requires the computation \nof attention along longer sequences\nUnified model for semantic and instance seg-\nmentation, and segmentation of panoptic\nMore computing time\nPage 24 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \ntransformer-based models already consist of inbuilt functionalities needed for imple -\nmenting NMS.\nTechnical evolution of hard‑negative mining (HNM)\nIn [3], imbalanced data issue during training is investigated. Technical evolution related \nto HNM in OD like bootstrap in OD refers to group of training methods wherein small \npart belonging to background samples is considered for training. Further, it iteratively \nadded new misclassified backgrounds during training, without HNM. Later, during the \ndeep learning era with improvement in computational power, faster R-CNN and YOLO \neasily balanced weights among positive and negative windows. These further improve -\nments led to bootstrapping with new loss function.\nDETR transformer models are equivalent to faster R-CNN training process, which \nbalances the obtained proposals of positive or negative by subsampling [60]. Moreover, \nmatching cost is independent object prediction. When transformer models are used in \nconjunction with other R-CNN models, they follow their previous HNMs.\nApplications of transformers\nTransformer-based models are emerged as a competitive alternative to CNN models on \nOD. The use of transformers-based learning for visual representation developed sparked \ninterest in the CV community. In [43], authors reviewed some of the important detec -\ntion applications from the past few years in topics such as face detection, traffic sign \ndetection, and pedestrian detection. Here, discussion is also focused on the difficulties \nand challenges faced in each area along with certain object detectors (which are mostly \nCNN-based) that are able to resolve the challenges.\nIn this section, the same issues are discussed, but transformer-based proposed models \nhelp to overcome the challenges faced by them and then propose a few new signs of pro-\ngress that are achieved in the same areas.\nPedestrian detection\nIn one of the important object detection applications, i.e. pedestrian detection is applied \nin scene perception and object detection in autonomous vehicles, criminal investigation, \nvideo surveillance, etc. In general, DL-based OD methods are greatly progressing in this \nfield and are constantly been improving to face number of challenges, gaining accuracy \nand overcoming issues. In a real-time-based detection application, it is made sure that \nthe model should compute the best results at all times.\nDL ODs such as fast/faster R-CNN presented their best performances for general \ndetection; however, it suffers from limited achievement in detecting small pedestrians \nunder low resolution of convolutional features. Recent solutions are proposed which \nhelps to improve and add specific features to overcome this issue. There is still require -\nment to enhance hard-negative detection because certain background image patches are \nexactly similar to pedestrians in corresponding visual appearance. Features in deeper \nlayers of CNN consist of better semantics, but not qualified to detect dense objects and \nare a reason for occlusion. The CNN utilized successfully in pedestrian detection [61] \nand achieved the promising outcomes.\nPage 25 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \nIt is noted that the fine-tuning process improves generalization and till date, trans -\nformer networks are tested as backbones and found out to be outperforming CNNs in \nterms of generalization and absorbing large-scale datasets for learning robust represen -\ntation. End-to-end detectors, DETR, and deformable DETR accomplish comparatively \nbetter results for common OD. Hence, due to its unique model architecture and com -\npetitive results with fast R-CNN models, it is applied to pedestrian detection.\nTo make DETR practically possible and even for crowded detection, new decoder \nwith dense queries and rectified attention unit, i.e. DQRF, is introduced, which is eas -\nily implementable and benefits to alleviate identified problems of DETR in detection \nof pedestrian. In [34], faster matching algorithm using bipartite scheme is proposed \nwherein improvements are suggested for DETR pertaining to annotations of visible box. \nFurther, Rank-DETR [70] is designed to predict the high-quality OD based on the rank \nof bounding box which accurately identifies the positive prediction and subdue the nega-\ntive predictions. This approach achieved higher localization accuracy by enhancing AP .\nNumber of models can be made in conjunction with attention-based or transformer-\nbased encoders or decoder modules with already existing object detectors. For example, \nin [62], PedesFormer is a swim transformer-based model that focuses on the advance -\nment of research. Segmentation and domain adaptation are constructed using UNet net-\nwork with swim transformer, as it can be applied spatial constraints to the pedestrian \ndetection [13]. Similar to this approach, many models are seen to perform the same task \nwith more improvement. A combination of object detection models with semantic tasks \nis able to put to end the hard-negative samples. Likewise, transformer-based semantic \nmodels can also be applied to achieve same or better results with additional benefits.\nFace detection\nDetection of face is the oldest task in CV and is continuously grown and evolved since \nthen. It is now applied to many other tasks. Due to rapid progress of DL in CV, many \nDL-based frameworks are developed for detecting the face, periodically, which achieved \nimprovements in accuracy. In DL era, mostly, face detection methods follow detection \nidea of general ODs such as faster R-CNN [3] and SSD [5].\nSome of the issues and challenges to detect face are identified as follows:\n (i)  Intra-Class Variation: We know that varieties of skin colours, expressions, move -\nments, and poses are possible from Human faces.\n (ii)  Occlusion: It may happen that faces are partly occluded by another objects.\n (iii)  Multi-Scale Detection: Face detection from range of large variety, especially when \ntiny faces present good encounters.\nCNN is based up on the local ideas for feature expression, which result in low effi -\nciency to capture long-range pixels dependency, thus, poor performance is reported to \nrecognize facial expression. Further, to overcome this problem, self-attention approach \nis added using residual network, due to which recognition is done at global level. CNN \nis not capable to perform the encoding of different features in relative position, while \nattention models can learn different features with focus on the interactive ones. Some \nof the published researches explained about the models, which are experimented with \nPage 26 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nthe combination of attention models with already existing detectors in order to improve \nresults. Here, encoding using transformer is used in capturing relationships amid vari -\nous action units aimed at expressions in wide range in training dataset and thus yields \nhigh classification performance. It is seen that transformers are robust in perturbations, \ndomain shifts, occlusions; hence with this view, TFE model is developed in [63]. Similar \nto this, many transformer-based modules are grouped with already existing object detec-\ntors, where the basic feature and working for the modules remain the same. Though, \narchitecture and improvements vary in the process of detection.\nTraffic signal detection\nIn recent past, detecting lights and traffic signs automatically attracted the researchers \ndue to advancement in self-driving technology. It is also mistakenly seeming to assume \nthe recognition of traffic signs is simple, there are lot of challenges present. This deten -\ntion would be practically difficult on driving under different conditions such as night \nand sun glare. So, image capturing through camera may be blurred due to car in motion. \nDuring bad weather conditions, it becomes even impossible to detect the image. How -\never, there is a need of real-time detection for autonomous vehicles with prominent \naccuracy. Since in this deep learning era, models like faster R-CNN and SSD are also \napplied in traffic sign detection. To overcome certain drawbacks, new techniques are \ncontinuously developed and implemented.\nIt is observed that comparatively higher accuracy is obtained by the two-stage algo -\nrithms, however, recognition speed is slow. Therefore, attention modules can be utilized \nwith other CNN models to overcome the difficulties [64]. Transformer-based models \ncan also be used in conjunction with CNN models, the self-attention module and multi-\nheaded attention have the capabilities to recognize the image, both locally and globally, \nand that leads to further scope of future research.\nGeneral application\nTherefore, transformers showed the promising results on training models with multi-\nmodal input data. It avoided the heavy engineering and inefficiencies during utilization \nof mixed architectures. For the machines to become more useful, algorithms should \nlearn how to aim with multi-sensory inputs. Transformer should be perfect and it is \nexpected to be applied on a large scale for different applications in the near future. It \nis a fascinating cause to advance as a multi-task, and can easily be tailored into already \nexisting models in order to add new features. ViTs are now applied in 3D analysis, video \nprocesses, and generative modelling.\nDatasets and evaluation metrics\nDatasets\nFor the purpose of object detection, various datasets are publically available for the per -\nformance evaluation. There exists a wealth of readily available, open-source datasets that \ncan be harnessed for experimentation and model development. Some datasets are listed \nas follows;\nPage 27 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \n1. CIFAR-10 [65]: CIFAR-10 is a comprehensive dataset that consists of 60,000 colour \nimages in 10 different categories. The dataset holds 10,000 test images and 50,000 \ntraining images split into five training groups. However, the images in CIFAR-10 are \nlow resolution (32 × 32), thus, dataset allows researchers to quickly apply different \nalgorithms.\n2. Open Images [66]: Open Images V4 dataset offers large scale across several dimen -\nsions wherein 30.1 M image-level labels for 19.8 k concepts, 15.4 M bounding boxes \nfor 600 object classes, and 375 k visual relationship annotations involving 57 classes \nare observed. Specifically for object detection, 15 × more bounding boxes than the \nnext largest datasets (15.4  M boxes on 1.9  M images) are provided. The images \noften show complex scenes with several objects (eight annotated objects per image \non average). Open Images V4 dataset competition also uses mean average precision \n(mAP) over the 500 classes to evaluate the object detection task.\n3. COCO Dataset [41]: Common objects in context (COCO) dataset, a seminal \nresource, offers a diverse collection of images with objects situated in complex, real-\nworld contexts. Its wide adoption stems from its capacity to evaluate object detec -\ntion models across intricate scenarios. In COCO, there are more small objects than \nlarge objects. Specifically, approximately 41% of objects are small (area < 322), 34% \nare medium (322 < area < 962), and 24% are large (area > 962). The general average \nprecision (AP) and average recall (AR) are averaged over multiple Intersection over \nUnion (IoU) values. Specifically, we use 10 IoU thresholds of 0.50:0.05:0.95.\n4. PASCAL VOC  [40]: PASCAL VOC, albeit smaller in scale than COCO, has played \na pivotal role in benchmarking object detection algorithms. Its twenty object classes \nhave made it a valuable asset for early stage evaluations. The current metrics used by \nthe current PASCAL VOC object detection challenge are the precision × recall curve \nand average precision.\n5. ImageNet [42]: ImageNet is initially devised for image classification, and then \nextended to encompass object detection challenges. It boasts an extensive array of \nannotated images, offering an invaluable resource for object detection researchers. \nImageNet object localization challenge, which evaluates the performance of object \nlocalization algorithms, uses a specific error metric that considers both the class label \nand overlapping region between the ground truth and detected bounding boxes for \neach image. This metric calculates a \"min error\" for each image, indicating how well \nthe predicted bounding boxes align with the true objects in the image.\nOther datasets such as CIFAR-10, PASCAL VOC, and ImageNet have their signifi -\ncance in computer vision tasks, but COCO’s extensive challenging dataset, its real-\nworld complexity, and rich annotations have gained prominence as a benchmarking \nfor state-of-the-art object detection models.\nExisting datasets provide a strong starting point; moreover, we can always have \nan option to build custom datasets. By creating custom datasets, one can curate a \ncollection of images and annotations that mirror the real-world scenarios and chal -\nlenges encountered in the target application.\nPage 28 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nEvaluation metrics\nMean average precision (mAP), common metric utilized in evaluating the accuracy \nof OD models. It gives a detailed overview of how a model is performing alongside its \ncompetitor models on the same dataset. Here, metrics involved in mAP are discussed, \ni.e. confusion matrix (CM), Intersection over Union (IoU), precision, and recall [67].\nIntersection Over Union: A number that quantifies the degree of overlap between \ntwo boxes. Here, in OD and segmentation, an IoU estimating overlaps related to pre -\ndiction region and ground truth (GT). However, IoU is considered as primary metric \nfor segmentation to report model accuracy.\nUsing IoU threshold value, the prediction of true positive (TP), false negative (FN), \nor false positive (FP) can be decided, collectively known as confusion matrix. Fig -\nure 13 shows IoU for the detection of object in the image.\nConfusion Matrix: A CM is a table that shows performance of a classifier given \nsome truth values/instances.\nTrue positive—classifier predicted positive wherein truth is positive, false positive—\nwrongly predicting positive, i.e. IoU < α (for detection), false negative—no detection \nby classifier, and true negative—correct prediction for negative class.\nSimilarly, in segmentation and OD the exact words are not same (see Fig.  14). In \nOD, correctness of estimate (TP , FN, or FP) is confirmed with IoU threshold, whereas \nin segmentation, this is decided through referring GT pixels.\nPrecision: The total TP from total detections, i.e. TP and FP , this measure helps in \nidentifying the positives prediction that is correct. If you are wondering how to calcu -\nlate precision, it is simply the true positives out of total detections.\nRecall:  The total TP from total detections, i.e. TP and FN, helps in addressing the \nquestion of “What quantity of TP , predicted correctly?” .\nThe average precision (AP) is considered as area under precision–recall curve, and \nit is calculated class-wise. The mAP is averaged over AP for all detected classes.\nFig. 13 Object detection prediction\nPage 29 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \nPerformance analysis and discussion\nThe COCO dataset (used for COCO test-dev and COCO minival benchmarks) and the \nPASCAL VOC 2007 dataset stand out as two pivotal datasets in the field of object detec -\ntion. Over time, object detection models have evolved and improved significantly, with \nthese datasets serving as key driving forces behind the advancements.\nOne comparative analysis on the current scenario of the models with the highest mAP \nalong the varied timeline presented in Fig.  15 uses PASCAL VOC 2007 dataset. This \nshows how transformer model (DETReg) tried to reach the mark better than CNNs. \nFurther, Fig.  16 shows COCO 2017 dataset to evaluate the performance of different \nobject detection models, here, ranked Co-DETR [68] has achieved higher box mAP of \n66.0%. Moreover, the performance of different object detection models on MS-COCO, \nVOC07, and VOC12 datasets being previously indicated in Fig.  2 witnessed OD perfor -\nmance improvement (see Sect. \"Object detection models \"). Moreover, from Fig.  17 we \ncan depict that ranked Co-DETR [68] offers the higher box average precision of 65.9% \nfor OD task.\nOn the MS-COCO dataset which is based on the average precision, the best real-time \nOD algorithm until September 2022 was YOLOv7, followed by vision transformer such \nas Swin and DualSwin, PP-YOLOE, YOLOR, YOLOv4, and EfficientDet. As of July 2023, \nan evolving trend in the realm of real-time OD on the MS-COCO dataset is the ascent \nof Co-DETR, marking a significant shift in the landscape.\nFig. 14 Image segmentation prediction\nFig. 15 OD on PASCAL VOC 2007 at a universal level\nPage 30 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \nTable 2 provides a valuable tool for comparing and assessing various models across \ndifferent datasets and evaluation metrics using quantifiable numerical data. Here, we can \ndetermine and decide which model is more suitable for different computer vision tasks, \ntaking into account the specific requirements and challenges posed by each dataset. The \nimage classification task using ViT-L model on ImageNet achieved a higher accuracy of \n87.76%. Subsequently, OD task reported the highest mAP of 73.20% on PASCAL VOC \nusing fast R-CNN.\nChallenges in object detection\nFollowing are the challenges in object detection;\n1. Small objection detection Large object detection is performed accurately by the vari -\nous object detectors but deprived performance is reported on small objects.\nFig. 16 OD on COCO 2017 test-dev at a universal level [68]\nFig. 17 OD on COCO 2017 minival at a universal level [68]\nPage 31 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \n2. Multi-resolution Object detection algorithms achieve good results under controlled \nenvironment for images in specific resolution. However, these algorithms disappoint \non varied resolution of inputs.\n3. Large dataset Applying CNN and transformer-based algorithms needs big datasets \nwith suitable annotations which is laborious task. Further, various images are gener -\nated by the numerous resources to investigate useful information.\n4. Computational resources To train the object detectors on big datasets requires the \nmore computational power [58].\nTable 2 Comprehensive evaluation metrics and dataset comparative analysis for CV models\nReference Task Model Used Dataset Results\n[1] Object detection RCNN PASCAL VOC mAP—58.50%\n[2] Object detection Fast RCNN PASCAL VOC mAP—70%\nCOCO 2017 test-dev Box mAP—19.7\nmAP—19.70%\n[3] Object detection Faster RCNN PASCAL VOC mAP—73.20%\nCOCO 2017 test-dev Frame per secs—46.7\nmAP—21.90%\nAverage mAP—16.4\n[28] Object detection DETR COCO 2017 Average precision \n(AP)—43.0\nAverage mAP—17.7\n[29] Object detection D-DETR COCO 2017 Average precision \n(AP)—46.9\nAverage mAP—18.5\nmAP—52.30%\n[18] Object detection ViT-B/16-FRCNN COCO 2017 Average precision \n(AP)—37.8\nOBJECTNET-D Average precision \n(AP)—22.9\n[6] Object detection YOLO PASCAL VOC mAP—63.40%\nFrame per secs—46.7\nCOCO 2017 Average mAP—32\nmAP—43.50%\n[30] Object detection YOLOS(VIT-B) COCO 2017 Average mAP—20\n[8] Object detection Mask RCNN COCO 2017 Average mAP—17.6\nBox mAP(Real time)—45.7\n[69] Object detection ResNet-101 Pascal VOC Average mAP—63.7\n[70] Object detection Rank-DETR (ResNet50) COCO 2017 Average mAP—50.2\n[46] Image classification ViT-H ImageNet Accuracy (Top 1)—88.55%\nCIFAR-10 Percentage correct—99.9%\n[46] Image classification ViT-B ImageNet Accuracy(Top1)—85.2%\n[46] Image classification ViT-L ImageNet Accuracy (Top 1)—87.76%\nCIFAR-10 Percentage cor-\nrect—99.42%\nPASCAL VOC mIoU—68\n[57] Semantic segmentation ViT segmenter PASCAL VOC mIoU—59\nPage 32 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n5. Imbalance class Images class imbalance pertaining to background and foreground \nimages leads to lower model performance.\n6. Localization To localize the objects and further perform the prediction, background \npixels restrict the accurate prediction, thus, localization errors need to be reduced.\nConclusion and future scope\nIn the task of object detection, various object detectors using CNN-based model and \ntransformer-based models are proposed in the literature. We have investigated the vari -\nous domains in which object detection in real time is very important and needs sub -\nstantial improvement. CNN-based object detectors lack in generalization and lower \nlocalization accuracy, whereas transformer-based detectors achieve higher detection \naccuracy and more generalization. It is observed that transformer models sparked the \ngreat interest in field of computer vision. One of the great benefits is their inclination \ntowards building universal model architectures that can support any type of input data \nsuch as text, image, audio, and video. In this paper, a suitable explanation of the differ -\nent transformer-based object detectors is demonstrated. In addition, the characteris -\ntics of each feature aims to give an idea of how transformers are able to flexibly fuse \nand conjunct with other DL models to improve on the efficiency. Thus, we suggest the \nneed for new architectures and ideas for future research in particular OD and other CV \ntasks. Here, we offer a meaningful review to depict a line of difference between trans -\nformer-based detectors and CNN models. The utilization of transformer-based detector \nuses the power of existing DL models and various attention mechanisms to achieve the \nhigher generalization which makes it more suitable for real-time objection detection.\nMoreover, we covered the applications of transformers in CV, particularly in tasks \nof recognition such as segmentation, OD, and image classification. However, all these \napplications will be the next step to understand problems in-depth and to be motivated \nfor modelling a new and robust architecture. Further, object detection can be explored \nin more fields such as multi-modal tasks, video processing, video forecasting, image \nsuper-resolution, and 3D analysis.\nAuthor contributions\nS. Shah performed the critical analysis on vision transformer and prepared the initial draft of manuscript. J Tembhurne \nexamined and suggested the changes to review on the applications of vision transformer. All authors reviewed and \nfinalized the manuscript.\nFunding\nNo funding was received for conducting this study.\nData availability\nAll data generated or analysed during this study are included in this published article.\nDeclarations\nCompeting interests\nThe authors have no competing interests to declare that are relevant to the content of this article.\nReceived: 16 May 2023   Accepted: 6 November 2023\n\nPage 33 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \nReferences\n 1. Girshick R, Donahue J, Darrell T, Malik J (2014) Rich feature hierarchies for accurate object detection and semantic \nsegmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 580–587\n 2. Girshick RJCS (2015) Fast R-CNN. arXiv preprint arXiv: 1504. 08083\n 3. Ren S, He K, Girshick R, Sun J (2015) Faster R-CNN: towards real-time object detection with region proposal net-\nworks. In: Advances in neural information processing systems, vol 28\n 4. Dai J, Li Y, He K, Sun J (2016) R-FCN: object detection via region-based fully convolutional networks. In: Advances in \nneural information processing systems, vol 29\n 5. Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY, Berg AC (2016) SSD: single shot multibox detector. In: Euro-\npean conference on computer vision. Springer, Cham, pp 21–37\n 6. Redmon J, Divvala S, Girshick R, Farhadi A (2016) You only look once: unified, real-time object detection. In: Proceed-\nings of the IEEE conference on computer vision and pattern recognition, pp 779–788\n 7. He K, Zhang X, Ren S, Sun J (2015) Spatial pyramid pooling in deep convolutional networks for visual recognition. \nIEEE Trans Pattern Anal Mach Intell 37(9):1904–1916\n 8. He K, Gkioxari G, Dollár P , Girshick R (2017) Mask R-CNN. In: Proceedings of the IEEE international conference on \ncomputer vision, pp 2961–2969\n 9. Jiang H, Learned-Miller E (2017) Face detection with the faster R-CNN. In: 2017 12th IEEE international conference \non automatic face & gesture recognition (FG 2017). IEEE, pp 650–657\n 10. Martinson E, Yalla V (2016) Real-time human detection for robots using CNN with a feature-based layered pre-filter. \nIn: 2016 25th IEEE international symposium on robot and human interactive communication (RO-MAN). IEEE, pp. \n1120–1125\n 11. Stewart R, Andriluka M, Ng AY (2016) End-to-end people detection in crowded scenes. In: Proceedings of the IEEE \nconference on computer vision and pattern recognition, pp 2325–2333\n 12. Boujemaa KS, Berrada I, Bouhoute A, Boubouh K (2017) Traffic sign recognition using convolutional neural networks. \nIn: 2017 International conference on wireless networks and mobile communications (WINCOM). IEEE, pp. 1–6\n 13. Zhang J, Liu C, Wang B, Chen C, He J, Zhou Y, Li J (2022) An infrared pedestrian detection method based on segmen-\ntation and domain adaptation learning. Comput Electr Eng 99:107781\n 14. Gidaris S, Komodakis N (2015) Object detection via a multi-region and semantic segmentation-aware CNN model. \nIn: Proceedings of the IEEE international conference on computer vision, pp 1134–1142\n 15. Hafiz AM, Bhat GM (2020) A survey on instance segmentation: state of the art. Int J Multimed Inf Retr 9(3):171–189\n 16. Ansari MA, Kurchaniya D, Dixit M (2017) A comprehensive analysis of image edge detection techniques. Int J Mul-\ntimed Ubiquitous Eng 12(11):1–12\n 17. Peng X, Schmid C (2016) Multi-region two-stream R-CNN for action detection. In: European conference on com-\nputer vision. Springer, Cham, pp 744–759\n 18. Beal J, Kim E, Tzeng E, Park DH, Zhai A, Kislyuk D (2020) Toward transformer-based object detection. arXiv preprint \narXiv: 2012. 09958\n 19. Wang W, Xie E, Li X, Fan DP , Song K, Liang D, Shao L (2021) Pyramid vision transformer: a versatile backbone for \ndense prediction without convolutions. In: Proceedings of the IEEE/CVF international conference on computer \nvision, pp 568–578\n 20. Chu X, Tian Z, Wang Y, Zhang B, Ren H, Wei X, Shen C (2021) Twins: revisiting the design of spatial attention in vision \ntransformers. Adv Neural Inf Process Syst 34:9355–9366\n 21. Xu W, Xu Y, Chang T, Tu Z (2021) Co-scale conv-attentional image transformers. In: Proceedings of the IEEE/CVF \ninternational conference on computer vision, pp 9981–9990\n 22. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Guo B (2021) Swin transformer: hierarchical vision transformer using shifted \nwindows. In: Proceedings of the IEEE/CVF international conference on computer vision, pp 10012–10022\n 23. Wu H, Xiao B, Codella N, Liu M, Dai X, Yuan L, Zhang L (2021) CVT: Introducing convolutions to vision transformers. \nIn: Proceedings of the IEEE/CVF international conference on computer vision, pp 22–31\n 24. Huang Z, Ben Y, Luo G, Cheng P , Yu G, Fu B (2021) Shuffle transformer: rethinking spatial shuffle for vision trans-\nformer. arXiv preprint arXiv: 2106. 03650\n 25. Wang W, Yao L, Chen L, Lin B, Cai D, He X, Liu W (2021) Crossformer: a versatile vision transformer hinging on cross-\nscale attention. arXiv preprint arXiv: 2108. 00154\n 26. Chen CF, Panda R, Fan Q (2021) Regionvit: regional-to-local attention for vision transformers. arXiv preprint arXiv: \n2106. 02689\n 27. Yang J, Li C, Zhang P , Dai X, Xiao B, Yuan L, Gao J (2021) Focal self-attention for local-global interactions in vision \ntransformers. arXiv preprint arXiv: 2107. 00641\n 28. Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S (2020). End-to-end object detection with trans-\nformers. In: European conference on computer vision. Springer, Cham, pp 213–229\n 29. Zhu X, Su W, Lu L, Li B, Wang X, Dai J (2020) Deformable DETR: deformable transformers for end-to-end object \ndetection. arXiv preprint arXiv: 2010. 04159\n 30. Fang Y, Liao B, Wang X, Fang J, Qi J, Wu R, Liu W (2021) You only look at one sequence: rethinking transformer in \nvision through object detection. Adv Neural Inf Process Syst 34:26183–26197\n 31. Ebrahimpour R, Kabir E, Yousefi MR (2007) Face detection using mixture of MLP experts. Neural Process Lett \n26(1):69–82\n 32. Kim B, Lee J, Kang J, Kim ES, Kim HJ (2021) HOTR: end-to-end human-object interaction detection with transformers. \nIn: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 74–83\n 33. Li M, Han D, Li D, Liu H, Chang CC (2022) MFVT: an anomaly traffic detection method merging feature fusion net-\nwork and vision transformer architecture. EURASIP J Wireless Commun Netw 2022(1):1–22\nPage 34 of 35Shah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n 34. Lin M, Li C, Bu X, Sun M, Lin C, Yan J, Deng Z (2020) DETR for crowd pedestrian detection. arXiv preprint arXiv: 2012. \n06785\n 35. Song H, Sun D, Chun S, Jampani V, Han D, Heo B, Yang MH (2022) An extendable, efficient and effective transformer-\nbased object detector. arXiv preprint arXiv: 2204. 07962\n 36. Meinhardt T, Kirillov A, Leal-Taixe L, Feichtenhofer C (2022) Trackformer: multi-object tracking with transformers. In: \nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 8844–8854\n 37. Wang Y, Xu Z, Wang X, Shen C, Cheng B, Shen H, Xia H (2021) End-to-end video instance segmentation with trans-\nformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 8741–8750\n 38. Wang Y, Zhang X, Yang T, Sun J (2022) Anchor DETR: query design for transformer-based detector. In: Proceedings of \nthe AAAI conference on artificial intelligence, vol 36, No 3, pp 2567–2575\n 39. https:// odsc. medium. com/ vision- trans former- and- its- appli catio ns- 265a6 29c0c f4. Accessed 20 Dec 2022\n 40. Everingham M, Van Gool L, Williams CK, Winn J, Zisserman A (2010) The pascal visual object classes (VOC) challenge. \nInt J Comput Vis 88(2):303–338\n 41. Lin TY, Maire M, Belongie S, Hays J, Perona P , Ramanan D, Zitnick CL (2014). Microsoft coco: common objects in \ncontext. In: European conference on computer vision. Springer, Cham, pp 740–755\n 42. Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009) Imagenet: a large-scale hierarchical image database. In: 2009 \nIEEE conference on computer vision and pattern recognition. IEEE, pp 248–255.\n 43. Zou Z, Shi Z, Guo Y, Ye J (2019) Object detection in 20 years: a survey. arXiv preprint arXiv: 1905. 05055\n 44. Lindeberg T (2012) Scale invariant feature transform, 10491\n 45. Dalal N, Triggs B (2005) Histograms of oriented gradients for human detection. In: 2005 IEEE computer society \nconference on computer vision and pattern recognition (CVPR’05), vol 1. IEEE, pp 886–893\n 46. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Houlsby N (2020) An image is worth \n16 × 16 words: transformers for image recognition at scale. arXiv preprint arXiv: 2010. 11929\n 47. Li J, Wei Y, Liang X, Dong J, Xu T, Feng J, Yan S (2016) Attentive contexts for object detection. IEEE Trans Multimed \n19(5):944–954\n 48. Khan S, Naseer M, Hayat M, Zamir SW, Khan FS, Shah M (2022) Transformers in vision: a survey. ACM Comput Surv \n54(10s):1–41\n 49. Hu H, Gu J, Zhang Z, Dai J, Wei Y (2018) Relation networks for object detection. In: Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, pp 3588–3597\n 50. Tolstikhin IO, Houlsby N, Kolesnikov A, Beyer L, Zhai X, Unterthiner T, Dosovitskiy A (2021) MLP-mixer: an all-MLP \narchitecture for vision. Adv Neural Inf Process Syst 34:24261–24272\n 51. https:// keras. io/ examp les/ vision/ object_ detec tion_ using_ vision_ trans former/. Accessed 22 Dec 2022\n 52. Lin T, Wang Y, Liu X, Qiu X (2022) A survey of transformers. AI Open\n 53. Chi C, Wei F, Hu H (2020) Relationnet++: bridging visual representations for object detection via transformer \ndecoder. Adv Neural Inf Process Syst 33:13564–13574\n 54. Long J, Shelhamer E, Darrell T, Berkeley UC (2014) Fully convolutional networks for semantic segmentation. arXiv \npreprint arXiv: 1411. 4038\n 55. Zheng S, Lu J, Zhao H, Zhu X, Luo Z, Wang Y, Torr PHS (2020) Rethinking semantic segmentation from a sequence-\nto-sequence perspective with transformers. arXiv preprint arXiv: 2012. 15840\n 56. Xie E, Wang W, Yu Z, Anandkumar A, Alvarez JM, Luo P (2021) SegFormer: simple and efficient design for semantic \nsegmentation with transformers. Adv Neural Inf Process Syst 34:12077–12090\n 57. Strudel R, Garcia R, Laptev I, Schmid C (2021) Segmenter: transformer for semantic segmentation. In: Proceedings of \nthe IEEE/CVF international conference on computer vision, pp 7262–7272\n 58. Diwan T, Anirudh G, Tembhurne JV (2022) Object detection using YOLO: challenges, architectural successors, data-\nsets and applications. Multimed Tools Appl 82:1–33\n 59. Hosang J, Benenson R, Schiele B (2017) Learning non-maximum suppression. In: Proceedings of the IEEE conference \non computer vision and pattern recognition, pp 4507–4515\n 60. Canévet O, Fleuret F (2015). Efficient sample mining for object detection. In: Asian conference on machine learning. \nPMLR, pp 48–63\n 61. Xu Z, Li B, Yuan Y, Dang A (2020) Beta R-CNN: looking into pedestrian detection from another perspective. Adv \nNeural Inf Process Syst 33:19953–19963\n 62. Hasan I, Liao S, Li J, Akram SU, Shao L (2022) Pedestrian detection: domain generalization, CNNs, transformers and \nbeyond. arXiv preprint arXiv: 2201. 03176\n 63. Jacob GM, Stenger B (2021) Facial action unit detection with transformers. In: Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp 7680–7689\n 64. Huang H, Liang Q, Luo D, Lee DH (2022) Attention-enhanced one-stage algorithm for traffic sign detection and \nrecognition. J Sens 2022:3705256\n 65. Doon R, Kumar Rawat T, Gautam S (2018) Cifar-10 classification using deep convolutional neural network. In: 2018 \nIEEE Punecon, Pune, India, pp 1–5. https:// doi. org/ 10. 1109/ PUNEC ON. 2018. 87454 28\n 66. Kuznetsova A, Rom H, Alldrin N, Uijlings J, Krasin I, Pont-Tuset J, Ferrari V (2020) The open images dataset v4: unified \nimage classification, object detection, and visual relationship detection at scale. Int J Comput Vis 128(7):1956–1981\n 67. Padilla R, Netto SL, Da Silva EA (2020) A survey on performance metrics for object-detection algorithms. In: 2020 \nInternational conference on systems, signals and image processing (IWSSIP). IEEE, pp 237–242\n 68. Zong Z, Song G, Liu Y (2023) DETRs with collaborative hybrid assignments training. In: Proceedings of the IEEE/CVF \ninternational conference on computer vision, pp 6748–6758\n 69. Krishna O, Ohashi H, Sinha S (2023) MILA: memory-based instance-level adaptation for cross-domain object detec-\ntion. arXiv preprint arXiv: 2309. 01086\nPage 35 of 35\nShah and Tembhurne  Journal of Electrical Systems and Inf Technol           (2023) 10:54 \n \n 70. Pu Y, Liang W, Hao Y, Yuan Y, Yang Y, Zhang C, Huang G (2023) Rank-DETR for high quality object detection. arXiv \npreprint arXiv: 2310. 08854\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7579282522201538
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7146706581115723
    },
    {
      "name": "Transformer",
      "score": 0.6985859274864197
    },
    {
      "name": "Object detection",
      "score": 0.6608357429504395
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6087287664413452
    },
    {
      "name": "Cognitive neuroscience of visual object recognition",
      "score": 0.47526854276657104
    },
    {
      "name": "Machine learning",
      "score": 0.4309004545211792
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3219326138496399
    },
    {
      "name": "Feature extraction",
      "score": 0.2634509801864624
    },
    {
      "name": "Engineering",
      "score": 0.13069984316825867
    },
    {
      "name": "Voltage",
      "score": 0.08996936678886414
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210138251",
      "name": "Indian Institute of Information Technology, Nagpur",
      "country": "IN"
    }
  ]
}