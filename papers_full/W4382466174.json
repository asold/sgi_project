{
    "title": "Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision Transformer Training",
    "url": "https://openalex.org/W4382466174",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3001296363",
            "name": "Zhenglun Kong",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2102336740",
            "name": "Haoyu Ma",
            "affiliations": [
                "University of California, Irvine"
            ]
        },
        {
            "id": "https://openalex.org/A2103499900",
            "name": "Geng Yuan",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2225354445",
            "name": "Mengshu Sun",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A3092024853",
            "name": "Yanyue Xie",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2127745698",
            "name": "Peiyan Dong",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2054824187",
            "name": "Xin Meng",
            "affiliations": [
                "King University"
            ]
        },
        {
            "id": "https://openalex.org/A2105069430",
            "name": "Xuan Shen",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2098377681",
            "name": "Hao Tang",
            "affiliations": [
                "ETH Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2619959539",
            "name": "Minghai Qin",
            "affiliations": [
                "Western Digital (Japan)"
            ]
        },
        {
            "id": "https://openalex.org/A2109104503",
            "name": "Tianlong Chen",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A2103268544",
            "name": "Xiaolong Ma",
            "affiliations": [
                "Clemson University"
            ]
        },
        {
            "id": "https://openalex.org/A2107390275",
            "name": "Xiaohui Xie",
            "affiliations": [
                "University of California, Irvine"
            ]
        },
        {
            "id": "https://openalex.org/A2574108784",
            "name": "Zhang-Yang Wang",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A2116772710",
            "name": "Yanzhi Wang",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2098377681",
            "name": "Hao Tang",
            "affiliations": [
                "ETH Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2574108784",
            "name": "Zhang-Yang Wang",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6778485988",
        "https://openalex.org/W2609701267",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W6786585107",
        "https://openalex.org/W6792881045",
        "https://openalex.org/W3103465009",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W6794920611",
        "https://openalex.org/W6793697131",
        "https://openalex.org/W3139587317",
        "https://openalex.org/W6791447439",
        "https://openalex.org/W3172801447",
        "https://openalex.org/W3157394974",
        "https://openalex.org/W4226315084",
        "https://openalex.org/W3217333840",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W4296556136",
        "https://openalex.org/W6801377519",
        "https://openalex.org/W3174402370",
        "https://openalex.org/W6789425149",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3111747337",
        "https://openalex.org/W6810167572",
        "https://openalex.org/W3195286673",
        "https://openalex.org/W3033492948",
        "https://openalex.org/W3188511781",
        "https://openalex.org/W6810818354",
        "https://openalex.org/W6607171255",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W4214624153",
        "https://openalex.org/W4362500802",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3209017628",
        "https://openalex.org/W2951013084",
        "https://openalex.org/W3169769781",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W4226484461",
        "https://openalex.org/W4287077733",
        "https://openalex.org/W3199245537",
        "https://openalex.org/W3168124404",
        "https://openalex.org/W2953937638",
        "https://openalex.org/W4285541168",
        "https://openalex.org/W3157506437",
        "https://openalex.org/W4294635920",
        "https://openalex.org/W4214636423",
        "https://openalex.org/W3128099838",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W4308536459",
        "https://openalex.org/W4312614783",
        "https://openalex.org/W3207918547",
        "https://openalex.org/W4312249545",
        "https://openalex.org/W3035022492",
        "https://openalex.org/W3194042166",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W4312960790",
        "https://openalex.org/W3165924482",
        "https://openalex.org/W3170863103",
        "https://openalex.org/W3153465022",
        "https://openalex.org/W4214490042",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W4285661751",
        "https://openalex.org/W4239072543",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W4287197288",
        "https://openalex.org/W3159727696",
        "https://openalex.org/W4287115696",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W4287320642",
        "https://openalex.org/W3171125843"
    ],
    "abstract": "Vision transformers (ViTs) have recently obtained success in many applications, but their intensive computation and heavy memory usage at both training and inference time limit their generalization. Previous compression algorithms usually start from the pre-trained dense models and only focus on efficient inference, while time-consuming training is still unavoidable. In contrast, this paper points out that the million-scale training data is redundant, which is the fundamental reason for the tedious training. To address the issue, this paper aims to introduce sparsity into data and proposes an end-to-end efficient training framework from three sparse perspectives, dubbed Tri-Level E-ViT. Specifically, we leverage a hierarchical data redundancy reduction scheme, by exploring the sparsity under three levels: number of training examples in the dataset, number of patches (tokens) in each example, and number of connections between tokens that lie in attention weights. With extensive experiments, we demonstrate that our proposed technique can noticeably accelerate training for various ViT architectures while maintaining accuracy. Remarkably, under certain ratios, we are able to improve the ViT accuracy rather than compromising it. For example, we can achieve 15.2% speedup with 72.6% (+0.4) Top-1 accuracy on Deit-T, and 15.7% speedup with 79.9% (+0.1) Top-1 accuracy on Deit-S. This proves the existence of data redundancy in ViT. Our code is released at https://github.com/ZLKong/Tri-Level-ViT",
    "full_text": "Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision\nTransformer Training\nZhenglun Kong*1, Haoyu Ma*2, Geng Yuan*1, Mengshu Sun1, Yanyue Xie1, Peiyan Dong1,\nXin Meng3, Xuan Shen1, Hao Tang4, Minghai Qin5, Tianlong Chen6, Xiaolong Ma7,\nXiaohui Xie2, Zhangyang Wang6, Yanzhi Wang1\n1Northeastern University\n2University of California, Irvine\n3Peking university\n4CVL, ETH Zurich\n5Western Digital Research\n6Unversity of Texas at Austin\n7Clemson University\n{kong.zhe, yuan.geng, sun.meng, xie.yany, dong.pe, shen.xu, yanz.wang}@northeastern.edu, {haoyum3, xhx}@uci.edu,\n1601214372@pku.edu.cn, hao.tang@vision.ee.ethz.ch, qinminghai@gmail.com,\n{tianlong.chen, atlaswang}@utexas.edu, xiaolom@clemson.edu,\nAbstract\nVision transformers (ViTs) have recently obtained success in\nmany applications, but their intensive computation and heavy\nmemory usage at both training and inference time limit their\ngeneralization. Previous compression algorithms usually start\nfrom the pre-trained dense models and only focus on efficient\ninference, while time-consuming training is still unavoidable.\nIn contrast, this paper points out that the million-scale training\ndata is redundant, which is the fundamental reason for the\ntedious training. To address the issue, this paper aims to intro-\nduce sparsity into data and proposes an end-to-end efficient\ntraining framework from three sparse perspectives, dubbed\nTri-Level E-ViT. Specifically, we leverage a hierarchical data\nredundancy reduction scheme by exploring the sparsity under\nthree levels: the number of training examples in the dataset,\nthe number of patches (tokens) in each example, and the num-\nber of connections between tokens that lie in attention weights.\nWith extensive experiments, we demonstrate that our proposed\ntechnique can noticeably accelerate training for various ViT\narchitectures while maintaining accuracy. Remarkably, under\ncertain ratios, we are able to improve the ViT accuracy rather\nthan compromising it. For example, we can achieve 15.2%\nspeedup with 72.6% (+0.4) Top-1 accuracy on Deit-T, and\n15.7% speedup with 79.9% (+0.1) Top-1 accuracy on Deit-S.\nThis proves the existence of data redundancy in ViT. Our code\nis released at https://github.com/ZLKong/Tri-Level-ViT\nIntroduction\nAfter the convolutional neural networks (CNNs) dominated\nthe computer vision field for more than a decade, the recent\nvision transformer (ViT) (Dosovitskiy et al. 2020) has ush-\nered in a new era in the field of vision (Hudson and Zitnick\n2021; Chen et al. 2021c; Kim et al. 2021; Deng et al. 2021;\nXue, Wang, and Guo 2021; Guo et al. 2021; Srinivas et al.\n*These authors contributed equally.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nTri-level/0.7\nTri-level/0.8\nTri-level/0.9\nEViT/0.5\nEViT/0.6\nEViT/0.7 EViT/0.8 EViT/0.9\nS\"ViTE\nDeiT-S\nEvo-ViT\nForgetting\nDynamicViT-pretrain\nActive Bias\nUVC\nIA-RED\"\nFigure 1: Comparison of different models with various\naccuracy-training efficiency trade-offs. Our Tri-level method\nachieves a better trade-off than the other methods.\n2021; Li et al. 2022; Yang et al. 2021). Existing ViT and vari-\nants, despite the impressive empirical performance, suffer in\ngeneral from large computation effort, and heavy run-time\nmemory usages (Liang et al. 2021; Chen, Fan, and Panda\n2021; Carion et al. 2020; Dai et al. 2021; Amini, Periyasamy,\nand Behnke 2021; Misra, Girdhar, and Joulin 2021; Chen\net al. 2021d; El-Nouby et al. 2021; Yang et al. 2020; Chen\net al. 2021a; Lu et al. 2021). To reduce the computational and\nmemory intensity of the ViT models, many compression algo-\nrithms have been proposed (Ryoo et al. 2021; Pan et al. 2021;\nLiang et al. 2022; Yu et al. 2022b; Kong et al. 2022; Ma et al.\n2022). These methods mainly target efficient inference, and\nthey are either conducted during the fine-tuning phase with\na pre-trained model or require the full over-parameterized\nmodel to be stored and updated during training.\nThe self-attention mechanism in ViTs abandons the in-\nductive bias that is inherent in CNNs, making the training\nof ViTs extremely data-hungry. Specifically, the early ViT\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n8360\nwork (Dosovitskiy et al. 2020) requires to be trained on a\nmuch larger dataset (i.e., JFT-300M with 300 million im-\nages) to claim its superiority compared to CNNs that are\ndirectly trained using the ImageNet dataset. This issue is par-\ntially mitigated by the following works, which incorporate\nknowledge distillation techniques to transfer inductive bias\nfrom a convolutional teacher model during training (Touvron\net al. 2021) or introduce the design of CNNs into transformer\nblocks (Liu et al. 2021; Heo et al. 2021). However, the time-\nconsuming training process still remains computational and\nmemory intensive and even more costly than before, becom-\ning a huge obstacle to a vast number of ViT-based research\nand applications. Therefore, an efficient training paradigm for\nViT models is in demand more than ever. Motivated by such\nneeds, we raise a question: Can we incorporate data sparsity\ninto the training process to amortize the high training costs\ncaused by the data-hungry nature of the ViTs?\nDue to the quadratic image patch-based computation com-\nplexity in the self-attention mechanism of ViT models (Doso-\nvitskiy et al. 2020), it is desirable and reasonable to explore\nthe sparsity by leveraging the redundancy that inherent in\nnatural image data. Towards this end, we propose an effi-\ncient ViT training framework, named Tri-Level E-ViT, which\nhierarchically eliminates the data redundancy in different\nlevels of the ViT training, just like peeling the onion. Specif-\nically, to reduce the training costs and accelerate the training\nprocess, we explore a tri-level data sparsity, which includes\nexample-level sparsity, token-level sparsity, and attention-\nlevel sparsity.\nPrior works have only focused on removing redundant\npatches (tokens) instead of removing the whole image (train-\ning example) (Rao et al. 2021; Liang et al. 2022; Kong et al.\n2022; Xu et al. 2021). This is due to the fact that the tendency\nto train a ViT with a large amount of data has become “deeply\nrooted” and led people to overlook the existence of redun-\ndancy in the example level. Motivated by this, we design a\nViT-specific online example filtering method to assess the\nimportance of each example and to remove less important\nones during training. More specifically, we use both the out-\nput classification logit, and the variance of the attention map\nin the [CLS] token (Dosovitskiy et al. 2020) to evaluate\nthe importance of each example. We train the model with a\nsubset of the full training examples (e.g., 80%) and employ\na remove-and-restore algorithm to continuously update the\ntraining subset, ensuring both the example-level sparsity and\noptimization throughout the entire ViT training process.\nThe ViT decomposes an image into several non-\noverlapping patches. The patch-based representation (Trock-\nman and Kolter 2022) preserves the locality and detailed\ninformation of an image. Thus, the patch can be considered a\ntype of fine-grained data. The ViT conducts the dense self-\nattention among all patches, which leads to quadratic com-\nplexity with respect to the number of patches. Recent works\nsuggest that not all patches are informative (Rao et al. 2021)\nand not all attention connections among patches are necessary\n(Liu et al. 2021; Heo et al. 2021). Thus, we further explore\ntwo levels of data sparsity from the perspective of the patch:\nOne is token-level sparsity, which aims to reduce the number\nof patches (tokens), and the other is attention-level sparsity,\nwhich aims to remove redundant attention connections. We\npropose the Token&Attention selector (TA-selector) to simul-\ntaneously achieve both levels of sparsity during training. In\ndetail, the TA-selector directly selects informative tokens and\ncritical attention connections based on the attention maps\nof early layers. It is a data-dependent scheme and does not\nintroduce any new weights. Thus, TA-selector is flexible to\nbe installed into many ViT variants.\nBy incorporating the tri-level data sparsity, our framework\nsignificantly reduces the training costs and accelerates the\ntraining process while maintaining the original accuracy. To\nthe best of our knowledge, we are the first to explore all levels\nof data sparsity for efficient ViT training. As shown in Figure\n1, our method achieves the best accuracy-efficiency trade-offs\ncompared to existing sparsity methods. Most importantly, our\nTri-Level E-ViT framework is efficient and practical since\nwe do not require to introduce extra networks during training,\nsuch as using a CNN teacher for distillation (Touvron et al.\n2021) or a predictor network for token selection (Rao et al.\n2021). In addition to the training acceleration, the data-sparse\nViT model obtained by our framework can also be directly\nused for efficient inference without further fine-tuning. To\nverify the robustness, we evaluate the training acceleration of\nour framework on general-purpose GPU and FPGA devices.\nOur contributions are summarized as follows:\n•We propose an efficient framework that employs tri-level\ndata sparsity to eliminate data redundancy in the ViT train-\ning process.\n•We propose an attention-aware online example filtering\nmethod specifically for ViT to generate example-level spar-\nsity. We use a remove-and-restore approach to ensure data\nefficiency throughout the entire training process while opti-\nmizing the reduced dataset.\n•We propose a joint attention-based token&attention pruning\nstrategy to simultaneously achieve both token and attention\nconnection sparsity during training. Our method does not re-\nquire a complex token selector module or additional training\nloss or hyper-parameters.\n•We conduct extensive experiments on ImageNet with Deit-T\nand Deit-S, and demonstrate that our method can save up to\n35.6% training time with comparable accuracy. We evaluate\nthe training acceleration on both GPU and FPGA.\nRelated Work\nVision Transformers\nViT (Dosovitskiy et al. 2020) is a pioneering work that uses\na transformer-only structure to solve various vision tasks.\nCompared to traditional CNN structures, ViT allows all the\npositions in an image to interact through transformer blocks,\nwhereas CNNs operated on a fixed-sized window with re-\nstricted spatial interactions, which can have trouble capturing\nrelations at the pixel level in both spatial and time domains.\nSince then, many variants have been proposed. For example,\nDeiT (Touvron et al. 2021), T2T-ViT (Yuan et al. 2021b),\nand Mixer (Tolstikhin et al. 2021) tackle the data-inefficiency\nproblem in ViT by training only with ImageNet. PiT (Heo\net al. 2021) replaces the uniform structure of the transformer\nwith a depth-wise convolution pooling layer to reduce spacial\n8361\nFigure 2: The overall procedure of our proposed Tri-Level E-ViT framework. We explore the training sparsity under three\nlevels: Left: Example-Level Sparsity. Randomly remove a number of examples prior to training. During the training process, the\nexample selector updates the training subset by removing the most unforgettable examples from the training data and restoring\nthe same amount of data from the portion removed before training. Right: Token & Attention level sparsity. Evaluate token\nimportance by the [CLS] token and prune the less informative tokens. We further use the attention vector of each remaining\ntoken to decide critical attention connections.\ndimension and increase channel dimension. SViTE (Chen\net al. 2021b) alleviates training memory bottleneck and im-\nproves inference efficiency by co-exploring input token and\nattention head sparsity. Recent works have also attempted to\ndesign various self-supervised learning schemes for ViTs to\naddress the data-hungry issue. BEiT (Bao, Dong, and Wei\n2021) pre-trains ViT by masked image modeling with an\nimage tokenizer based on dV AE that vectorizes images into\ndiscrete visual tokens. MAE (He et al. 2021) eliminates the\ndV AE pre-training process by reconstructing pixels, in con-\ntrast to predicting tokens.\nData Redundancy Reduction\nThere are many attempts to explore redundancy in data. For\nexample level, (Katharopoulos and Fleuret 2018) proposed\nan importance sampling scheme based on an upper bound\nto the gradient norm, which is added to the loss function.\n(Chang, Learned-Miller, and McCallum 2017) emphasizes\nhigh-variance samples by reweighting all the examples. How-\never, the additional computation overhead makes them less\nefficient for training. (Toneva et al. 2018) used the number of\nforgotten counts of training examples as a metric to reflect\nthe complexity (importance) of the examples. However, it re-\nquires a two-phase training approach that the partial training\nexamples are only removed in the second phase, which limits\nthe overall acceleration. Moreover, all the existing methods\nare applied only to CNN, making them less suitable for ViT,\nbecause their proposed criteria do not consider the ViT ar-\nchitecture for evaluation. For Patch level (token/attention),\nDynamicViT (Rao et al. 2021) removes redundant tokens by\nestimating their importance score with an MLP (Vaswani et al.\n2017) based prediction module. Evo-ViT (Xu et al. 2021) de-\nvelops a slow-fast token evolution method to preserve more\nimage information during pruning. DAT (Xia et al. 2022)\nselects important key and value pairs in self-attention with\na deformable self-attention module. However, all of these\nmethods require additional selectors to guide the removal\nof redundant data, which is not efficient for training. EViT\n(Liang et al. 2022) provides a token reorganization method\nthat reduces and reorganizes the tokens. However, it does not\ntake into account example level or attention level redundancy.\nMethodology\nOverall Framework\nTo reduce the redundant computing caused by the training\ndata, we propose the Tri-Level E-ViT. The overall framework\nis shown in Figure 2. Specifically, we introduce the sparsity\nrelevant to the data from three levels: The training example\nlevel, the token level, and the attention connection level. All\nof these sparse methods are combined together during the\n8362\ntraining process of ViTs.\nSparsity in Training Examples\nPrior works (Toneva et al. 2018; Paul 2021) show that re-\nmoving some less informative and easy examples from the\ntraining dataset will not lesion the performance of the trained\nCNN model. The amount of information (complexity) of\nthe examples can be identified by recording the number of\nforgotten events of examples during CNN training. Exam-\nples with lower forgetting counts are generally considered\neasy to be learned and less informative than others and can\ntherefore be removed in order. More discussions are shown\nin the Appendix. We intend to incorporate this idea into our\nexample-level sparse ViT training. We face two challenges:\n1) How to design such an example evaluation method for\nViT, considering its unique model characteristics? 2) How to\nidentify and remove easy examples from the dataset during\ntraining to reduce the training time without prior knowledge\nof training examples?\nAttention Aware Example Selector We learn the\nconditional probability distribution given a dataset D =\n(xi; yi)i of observation/label pairs. For example xi, the pre-\ndicted label (classification logits) obtained after t steps of\nAdamW is denoted as ˆyt\ni = arg max k p(yik|xi; θt). Let\ncorrt\ni = Bˆyt\ni =ˆyi be a binary variable indicating whether the\nexample is correctly classified at a time step. When example\ni gets misclassified at step t+1 after having been correctly\nclassified at step t, example i undergoes a forgetting event.\nThe binary variable corrt\ni decreases between two consecu-\ntive updates: corrt\ni > corrt+1\ni . On the other hand, a learning\nevent occurs if corrt\ni < corrt+1\ni .\nAfter counting the number of forgetting events for each\nexample, we sort the examples based on the number of forget-\nting events they undergo. However, there exist examples with\nthe same number of forgetting events. Even if the number\nis the same, the complexity of the images is still different.\nTherefore, in addition to the classification logits, we employ\nthe unique self-attention mechanism in the transformer to\nbetter measure the complexity of images. We calculate the\nvariance of the attention map of the [CLS] token (Dosovit-\nskiy et al. 2020) to obtain the similarity of the image patches.\nWe extract the attention map of the [CLS] token from the\nself-attention layer of the model and obtain the variance of\nthe attention map, which is exported along with the classifica-\ntion logits to assist in sorting the images, as shown in Figure\n2. Note that for ViT variants without [CLS] token (Swin),\nwe replace the variance with the cumulative token of the at-\ntention map. After removing the unforgettable examples, the\ncompressed training dataset ˆD is described as:\nˆD = {xi|xi ∈ D, f(xi) ≥ threshold}, (1)\nwhere f() stands for the “sort examples by forgetting counts”\nprocess. Therefore, f(xi) is the list of examples xi sorted\nby counts. “f(xi) ≥ threshold” means choosing examples\nwhose forgetting count is larger than the threshold. These\nselected examples will be our training subset ˆD.\nOnline Example Filtering Existing example sparsity\nworks (Toneva et al. 2018; Yuan et al. 2021a) on CNN split\nthe training process into two phases: 1) Training with the\ncomplete dataset to obtain statistics on forgetting events. 2)\nTraining with the remaining dataset by removing the less\ninformative examples based on the learning statistics. In this\nway, training acceleration can only be achieved in the sec-\nond phase, which only accounts for 40%∼70% of the total\ntraining process (Yuan et al. 2021a). This greatly limits the\noverall acceleration performance.\nUnlike their semi-sparse training approach, we use an end-\nto-end sparse training approach, which keeps the training\ndataset sparse throughout the training process. This is the\nfirst time that the example sparsity is introduced in the ViT\ntraining scenario. Our approach is shown in the left part of\nFigure 2. The main idea is to first remove a random portion\nof training examples from the dataset and then continuously\nupdate the remaining training subset during the training pro-\ncess by using a remove-and-restore mechanism. After several\niterations, only relatively more informative examples remain\nin the training subset.\nSpecifically, before the training starts, we first reduce the\ntraining subset by randomly removing a givenm% of training\nexamples. We define re = 1 − m% as the keep ratio of\ntraining examples. Then, during the training, we periodically\nremove the n% least informative examples from the training\nsubset (n < m) and randomly select n% examples from\nthe removed examples to restore to the training subset. And\nduring the training process, we continuously track the number\nof forgetting events for each training example and identify\nthe least informative examples. In this way, the data set is\noptimized after several iterations during training, and\nm%\nthe data size remains constant throughout the entire training\nprocess, leading to a more consistent training acceleration.\nSparsity in Patches: Token and Attention Level\nThe dense attention of ViTs leads to quadratic complexity\nwith respect to the number of patches. Thus, introducing\nsparsity into patches can significantly reduce the computation.\nWe consider both token-level sparsity and attention-level\nsparsity for patches based on the attention matrix.\nRevisiting ViT The ViT decomposes image I into N non-\noverlapping tokens {Pi}N\ni=1, and apply a linear projection\nto obtain the patch embedding XP ∈ RN×d, where d is\nthe dimension of embedding. The classification token Xcls\nis appended to accumulate information from other tokens\nand predict the class. Thus, the input to the transformer is\nX = [ Xcls, XP ] ∈ RL×d, where L=N+1 for short. The\ntransformer encoder consists of a self-attention layer and a\nfeed-forward network. In self-attention, the input tokens X\ngo through three linear layers to produce the query (Q), key\n(K), and value ( V) matrices respectively, where Q, K, V\n∈ RL×d. The attention operation is conducted as follows:\nAttention(Q, K, V) = Softmax(QKT /\n√\nd)V. (2)\nFor multi-head self-attention (MHSA), H self-attention mod-\nules are applied to X separately, and each of them produces\nan output sequence.\n8363\nToken-level Sparsity Previous works (Rao et al. 2021;\nKong et al. 2022) suggests that pruning less informative\ntokens, such as the background, has little impact on the accu-\nracy. Thus, we can improve training efficiency at the token\nlevel by eliminating redundant tokens. However, these works\nrequire an additional token selector module to evaluate the\nimportance of each token. Moreover, they target fine-tuning,\nwhich is incompatible with training from scratch, as addi-\ntional losses need to be introduced. In this work, we aim\nto prune tokens without introducing additional modules and\ntrain ViTs from scratch with original training recipes.\nIn Eq. 2, Q and K can be regarded as a concatena-\ntion of L token vectors: Q = [ q1, q2, . . . , qL]T and K =\n[k1, k2, . . . , kL]T . For the i-th token, the attention probabil-\nity ai = Softmax(qi · KT /\n√\nd) ∈ RL shows the degree\nof correlation of each key ki with the query qi. The output\nai · V can be considered as a linear combination of all value\nvectors, with ai being the combination coefficients. Thus,\nwe can assume that ai indicates the importance score of all\ntokens. Typically, a large attention value suggests that the\ncorresponding token is important to the query token i.\nFor ViT, the final output only depends on the [CLS] to-\nken. Thus, the attention map of this special tokens acls =\nSoftmax(qcls · KT /\n√\nd) represents the extent to which the\ntoken contributes to the final result. To this end, we utilize\n˜acls ∈ RN , which excludes the first element of acls, as the\ncriterion to select tokens. In MHSA, we use the average atten-\ntion probability of all heads ¯acls = 1\nH\nPH\ni=1 ˜a(h)\ncls . We select\nK(K < N) most important patch tokens based on the value\nof ¯acls and define RT = K\nN as the token keep ratio. Thus,\nonly 1 + RT N tokens are left in the following layers.\nFor ViT variants without the [CLS] token, we calculate\nthe importance scores of tokens by summing each column of\nthe attention map (Wang, Zhang, and Han 2021). In detail,\nwe use the cumulative attention probability P\ni ai ∈ RN to\nselect informative tokens. We hypothesize that tokens that\nare important to many query tokens should be informative.\nAttention-level Sparsity Since attention maps are usually\nsparse, dense attention is unnecessary. Thus, we also intro-\nduce sparsity at the attention level by pruning attention con-\nnections. In detail, given a query token, we only calculate its\nattention connections with a few selected tokens. Previous\nViT variants usually utilize some hand-craft predetermined\nsparse patterns, such as a neighborhood window (Liu et al.\n2021) or dilated sliding window (Beltagy, Peters, and Co-\nhan 2020). However, objects of different sizes and shapes\nmay require different attention connections. Thus, these data-\nagnostic method is limited. Some input-dependent methods\napply the deformable attention (Zhu et al. 2020) to find cor-\nresponding tokens. However, these works also require addi-\ntional modules to learn the selected tokens.\nIn this work, we utilize the attention vector of each patch\ntoken to decide critical connections. Thus, no additional mod-\nules are introduced. In detail, given the image patch token\nPi and its corresponding query qi, we use ¯ai as the saliency\ncriteria. In detail, we take the index of RAN tokens with\nthe largest value of ¯ai, where RA ≪ 1 is the attention-keep\nratio. Thus, each patch token Pi only needs to calculate the\nattention score with the selected RAN tokens for the rest\nlayers.\nToken&Attention Selector (TA-Selector) We further com-\nbine the above token selector and attention connections se-\nlector into a single module, named TA-Selector. As shown in\nthe right part of Figure 2, the TA-Selector is inserted after the\nself-attention module of one transformer encoder layer. Given\nthe attention matrix A = Softmax(QKT /\n√\nd) ∈ RL×L, we\nfirst utilize the attention probability of [CLS] tokens acls\n(the blue row of Figure 2) to locate RT N most informative\ntokens. We denote the index of selected tokens as set T . We\nthen extract the attention map of select tokens AT , whose\nelement is defined by A[i, j] with {i, j} ∈ T ×T(the ticked\nred cell). Then we perform the attention-selector on AT . In\nsummary, 1 +RT N tokens are left after the TA-selector, and\neach token only calculates the attention withRT RAN tokens\nin the following transformer encoder layers.\nWith TA-Selector, we dynamically prune both tokens and\nattention connections at the same time. As the attention map\nat the early layer may be inaccurate (Xu et al. 2021), we\nfollow a hierarchical pruning scheme, which incorporates\nthe TA-Selector between transformer encoder layers several\ntimes. Thus, we gradually prune tokens and attention connec-\ntions with the network, going deeper.\nExperiments\nDatasets and Implementation Details\nOur experiments are conducted on ImageNet-1K (Deng et al.\n2009), with approximately 1.2 million images. We report the\naccuracy on the validation set with 50k images. The image\nresolution is 224 × 224. We also report results on Imagenet-\nV2 matched frequency (Recht et al. 2019) to control over-\nfitting. We apply our framework to different backbones in-\ncluding DeiT (Touvron et al. 2021), Swin (Liu et al. 2021),\nPiT (Heo et al. 2021), and LV-ViT (Jiang et al. 2021) with\nthe corresponding settings. In detail, the network is trained\nfor 300 epochs on 4 NVIDIA V100 GPUs and is optimized\nby AdamW (Loshchilov and Hutter 2019) with weight decay\n0.05. The batch size is set to 512, The learning rate is set to\n5 × 10−4 initially, and is decayed with a cosine annealing\nschedule. For example level sparsity, we apply the remove-\nand-restore approach every 30 epoch with 5% data removal\nfor each iteration. Since this approach takes extra time and\nthe accuracy decreases slightly after each iteration, we only\niterate the number of times in which all the removed data can\nbe covered. Hence, we set the different number of iterations\nfor different keep rates. For 10% removed data, we iterate 3\ntimes (30th, 60th, 90th epoch). For 20% removed data, we\niterate 5 times (30th, 60th, 90th, 120th, 150th epoch). For\ntoken and attention level sparsity, we insert our TA-Selector\nbefore the 4th, 7th, 10th layer for the hierarchical pruning\nscheme as in (Rao et al. 2021). Besides, during the training\nprocess, we adopt a warm-up strategy for the TA-Selector.\nThe token keep ratio RT starts from 1.0 and is gradually\nreduced to the given RT with a cosine annealing schedule.\nFor simplicity and fair comparison, we set the keep ratio\nto\n0.7, 0.8, and 0.9 in our main experiments, denoting as\n8364\nMET\nHOD TRA\nINING TIME REDUCED MACS (G) S AVING 1K ACC(%) V2 A CC(%)\nDEI\nT-T\nDEN\nSE - - 7\n2.2 65.0\nDYNAMIC VIT/0.7 (R AO ET AL . 2021) 0% 3\n0.0% 71.8 (-0.4) 64.6\nS2VITE (C HEN ET AL . 2021 B) 10\n.6% 23.7% 70.1 (-2.1) -\nTRI-\nLEVEL /0.9 15\n.2% 17\n.4% 72\n.6 (+0.4) 65\n.9\nTRI-\nLEVEL /0.8 29\n.4% 31\n.9% 72\n.1 (-0.1) 65\n.0\nTRI-\nLEVEL /0.7 35\n.6% 38\n.5% 71\n.9 (-0.3) 64\n.6\nDEIT\n-S\nDEN\nSE - - 7\n9.8 73.6\nIA-RED 2 (PAN ET AL . 2021) 0% 3\n2.0% 79.1 (-0.7) -\nDYNAMIC VIT/0.7 (R AO ET AL . 2021) 0% 3\n6.7% 79.3 (-0.5) 72.8\nUVC (Y U ET AL . 2022 B) 0% 4\n2.4% 79.4 (-0.4) -\nDYNAMIC VIT/0.7* (R AO ET AL . 2021) 7.\n2% 34.7% 77.6 (-2.2) 71.1\nEVIT/0.8 (L IANG ET AL . 2022) 6.\n0% 13.0% 79.8 (-0.1) 73.4\nEVIT/0.7 (L IANG ET AL . 2022) 20\n.0% 35.0% 79.5 (-0.3) 73.2\nEVIT/0.6 (L IANG ET AL . 2022) 25\n.0% 43.0% 78.9 (-0.9) 72.3\nS2VITE (C HEN ET AL . 2021 B) 22\n.7% 31.6% 79.2 (-0.6) -\nTRI-\nLEVEL /0.9 15\n.7% 17\n.0% 79\n.9 (+0.1) 73\n.8\nTRI-\nLEVEL /0.8 29\n.8% 31\n.3% 79\n.5 (-0.3) 73\n.3\nTRI-\nLEVEL /0.7 34\n.3% 37\n.6% 79\n.3 (-0.5) 72\n.9\nDEIT\n-B\nDEN\nSE - - 8\n1.8 75.8\nIA-RED 2 (PAN ET AL . 2021) 0% 3\n3.0% 80.3 (-1.5) -\nDYNAMIC VIT/0.7 (R AO ET AL . 2021) 0% 3\n7.4% 80.7 (-1.1) 74.2\nEVIT/0.7 (L IANG ET AL . 2022) 15\n.2% 35.0% 81.3 (-0.5) 75.1\nTRI-\nLEVEL /0.9 10\n.2% 15\n.6% 81\n.6 (-0.2) 75\n.3\nTRI-\nLEVEL /0.8 23\n.7% 29\n.1% 81\n.3 (-0.5) 75\n.1\nSWI\nN-T\nDEN\nSE - - 8\n1.2 75.1\nDYNAMIC SWIN /0.9 (R AO ET AL . 2022) 0% 7\n% 81.0 (-0.2) 75.0\nTRI-\nLEVEL /0.9 13\n.1% 15\n.6% 81\n.2 (-0.0) 75\n.1\nTRI-\nLEVEL /0.8 21\n.1% 25\n.3% 81\n.0 (-0.1) 74\n.9\nSWI\nN-S\nDEN\nSE - - 8\n3.2 76.9\nDYNAMIC SWIN /0.7 (R AO ET AL . 2022) 0% 2\n1% 83.2(-0.0) 76.9\nWDP RUNING (YU ET AL . 2022 A) 0% 1\n2.6% 82.4 (-0.6) 76.1\nTRI-\nLEVEL /0.8 20\n.2% 24\n.1% 83\n.2 (-0.0) 76\n.8\nTable 1: The training time reduced, MACs (G) saving and Top-1 accuracy comparison on the ImageNet-1K and Imagenet-V2\nmatched frequency. “*” refers to our reproduced results of DynamicViT training from scratch.\nTri-Level/0.7, Tri-Level/0.8, and Tri-Level/0.9, respectively.\nExperimental Results\nWe report the Top-1 accuracy of each model with different\nkeep ratios. For efficient training metrics, we evaluate the\nreduced running time and MACs (G) saving of the entire\ntraining process. Our main results are shown in Table 1. Re-\nmarkably, our efficient training method can even improve\nthe ViT accuracy rather than compromising it (15.2% time\nreduction with 72.6% Top-1 accuracy on Deit-T, and 15.7%\ntime reduction with 79.9% Top-1 accuracy on Deit-S). This\ndemonstrates the existence of data redundancy in ViT.\nWe also compare our method with other efficient ViTs\naiming at reducing redundancy. Most of the existing efforts\ntarget only efficient inference. They either apply to the fine-\ntuning phase (Rao et al. 2021; Pan et al. 2021) or introduce\nadditional modules with additional training cost (Chen et al.\n2021b; Yu et al. 2022b). In contrast to others, we can accel-\nerate both the training and inference process. Our method\nsignificantly reduces the training time while restricting the\naccuracy drop in a relatively small range. Notably, we are\nable to reduce the training time by 35.6% for DieT-T with a\nnegligible 0.3% accuracy degradation and 4.3% for DieT-S\nwith a 0.5% decrease in accuracy, which outperforms existing\npruning methods in terms of accuracy and efficiency.\nGeneralization\nTo verify the generalizability of our approach, we conduct ex-\nperiments with other vision transformer models and evaluate\nthem on object detection and instance segmentation tasks.\nSwin Transformer We further apply our method to other\nViT variants, such as Swin Transformer (Liu et al. 2021).\nInstead of using the [CLS] token, it applies a global average\npooling on features of all tokens. Thus, we use the cumulative\nattention probability to select tokens. For the example level\nsparsity, we also replace the variance of[CLS] token’s atten-\ntion map with that of the cumulative attention map. Results\n8365\nMethod MACs\n(G) Training Inference\nSaving\nLatency (hour) Latency (ms)\nDeiT-T\nDense - 13.79\n10.91\nTri-Le\nvel/0.9 17.4% 11.49 9.68\nTri-Le\nvel/0.8 31.9% 9.61 8.50\nTri-Le\nvel/0.7 38.5% 8.91 7.36\nDeit-S\nDense - 32.44\n25.68\nTri-Le\nvel/0.9 17.0% 27.28 22.92\nTri-Le\nvel/0.8 31.3% 22.55 20.25\nTri-Le\nvel/0.7 37.6% 21.15 17.61\nTable 2: The inference latency and the average per epoch\ntraining latency on Xilinx ZCU102 FPGA board.\nare shown in Table 1. We are able to reduce the training time\nby 13.1% for Swin-T and 20.2% for Swin-S with no accuracy\ndrop. Thus, our method can be generalized to other ViT archi-\ntectures. More results for other architectures such as LV-ViT\n(Jiang et al. 2021) and other datasets such as ImageNet-Real\nand CIFAR can be found in the Appendix.\nDeployment on Edge Devices\nWe also evaluate our method on an embedded FPGA plat-\nform, namely, Xilinx ZCU102. The platform features a Zynq\nUltraScale + MPSoC device (ZU9EG), which contains em-\nbedded ARM CPUs and 274k LUTs, 2520 DSPs, and 32.1Mb\nBRAMs on the programmable logic fabric. The working fre-\nquency is set to 150MHz for all the designs implemented\nthrough Xilinx Vitis and Vitis HLS 2021.1. The 16-bit fixed-\npoint precision is adopted to represent all the model parame-\nters and activation data. We measure both on-device training\nlatency and inference latency. In detail, the training latency\nis measured on FPGA using a prototype implementation that\nsupports layer-wise forward and backward computations. We\nreport the average per epoch training time. Both inference and\ntraining use the batch size of 1. As in Table 2, our method can\nalso accelerate the training and inference on edge devices.\nAblation Study\nSub-method Effectiveness\nWe evaluate the contribution of each sparse method separately.\nThe results of DeiT-S with different keep ratios at each sparse\nlevel are shown in Table 3. Removing redundant data in\neach sub-method can even lead to improvements in accuracy\nwhile reducing training time. At the same keep ratio, the\nexample level sparsity contributes the most to the training\ntime reduction, as it directly removes the entire image. Thus,\nit can be considered the coarse-grained level of data reduction.\nMeanwhile, the attention level has less reduction in training\ntime even at low keep ratios because it only contributes to\nthe matrix multiplication of the MHSA module. Thus, it can\nbe considered a fine-grained level of data reduction.\nMethod Keep\nTraining 1K V2\nRatio Time Reduced Acc.(%) Acc.(%)\nDense 1.0\n- 79.8 73.6\nExample\nRandom 0.8\n19.9% 78.4 71.7\nActive Bais 0.8 4.3% 79.1 72.5\nForgetting 0.8 16.1% 79.0 72.4\nours 0.9 9.9% 80.1 73.8\nours 0.8 19.8% 79.7 73.5\nours 0.7 29.8% 79.2 72.6\nTok\nen\nStatic 0.7\n20% 75.2 68.3\nDynamicViT 0.7 0% 79.3 72.8\nIA-RED2 0.7 0% 79.1 -\nEvo-ViT 0.7 13.0% 79.4 -\nours 0.9 6.2% 79.9 73.8\nours 0.8 13.3% 79.7 73.4\nours 0.7 20.6% 79.4 73.2\nAttention\nMagnitude 0.5\n2.8% 78.9 72.3\nLongformer 0.2 4.8% 78.8 72.0\nours 0.4 3.3% 79.8 73.5\nours 0.3 3.6% 79.7 73.5\nours 0.2 4.8% 79.7 73.4\nTable 3: Sub-method effectiveness on DeiT-S under different\nkeep ratios. We also compare with existing individual exam-\nple/token/attention sparsity methods.\nComparison of Different Methods\nTo verify the advantages of our method, we compare it with\nother data sparsity strategies on DeiT-S. As shown in Table\n3, Random denotes randomly updating the training data with\nthe same frequency and number of updates as our method.\nUnder the same keep ratio, it shows a significant accuracy\ndegradation (78.4% vs. 79.7% ). We also compare it with\nthe existing work (Toneva et al. 2018), which conducts a\ntwo-phase method for example evaluation and removal. The\naccuracy is lower (79.0% vs. 79.7%), and the training accel-\neration is limited (16.1% vs. 19.8%).\nConclusion\nIn this work, we introduce sparsity into data and propose\nan end-to-end efficient training framework to accelerate ViT\ntraining and inference. We leverage a hierarchical data re-\ndundancy reduction scheme by exploring the sparsity of the\nnumber of training examples in the dataset, the number of\npatches (tokens) in each input image, and the number of con-\nnections between tokens in attention weights. Comprehensive\nexperiments validate the effectiveness of our method. Future\nwork includes extending our framework to other aspects such\nas weight redundancy.\nAcknowledgments\nThe research reported here was funded by the National Sci-\nence Foundation CCF-1919117.\n8366\nReferences\nAmini, A.; Periyasamy, A. S.; and Behnke, S. 2021. T6D-\nDirect: Transformers for Multi-Object 6D Pose Direct Re-\ngression. arXiv preprint arXiv:2109.10948.\nBao, H.; Dong, L.; and Wei, F. 2021. BEiT: BERT\nPre-Training of Image Transformers. arXiv preprint\narXiv:2106.08254.\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision, 213–229. Springer.\nChang, H.-S.; Learned-Miller, E.; and McCallum, A. 2017.\nActive bias: Training more accurate neural networks by em-\nphasizing high variance samples. Advances in Neural Infor-\nmation Processing Systems, 30.\nChen, C.-F. R.; Fan, Q.; and Panda, R. 2021. Crossvit: Cross-\nattention multi-scale vision transformer for image classifica-\ntion. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, 357–366.\nChen, H.; Wang, Y .; Guo, T.; Xu, C.; Deng, Y .; Liu, Z.; Ma,\nS.; Xu, C.; Xu, C.; and Gao, W. 2021a. Pre-trained image\nprocessing transformer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n12299–12310.\nChen, T.; Cheng, Y .; Gan, Z.; Yuan, L.; Zhang, L.; and Wang,\nZ. 2021b. Chasing Sparsity in Vision Transformers: An\nEnd-to-End Exploration. arXiv preprint arXiv:2106.04533.\nChen, T.; Saxena, S.; Li, L.; Fleet, D. J.; and Hinton, G. 2021c.\nPix2seq: A Language Modeling Framework for Object De-\ntection. arXiv preprint arXiv:2109.10852.\nChen, X.; Yan, B.; Zhu, J.; Wang, D.; Yang, X.; and Lu,\nH. 2021d. Transformer tracking. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 8126–8135.\nDai, Z.; Cai, B.; Lin, Y .; and Chen, J. 2021. Up-detr: Unsu-\npervised pre-training for object detection with transformers.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 1601–1610.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDeng, J.; Yang, Z.; Chen, T.; Zhou, W.; and Li, H. 2021.\nTransVG: End-to-End Visual Grounding With Transformers.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), 1769–1779.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nEl-Nouby, A.; Neverova, N.; Laptev, I.; and Jégou, H. 2021.\nTraining vision transformers for image retrieval. arXiv\npreprint arXiv:2102.05644.\nGuo, M.-H.; Cai, J.-X.; Liu, Z.-N.; Mu, T.-J.; Martin, R. R.;\nand Hu, S.-M. 2021. PCT: Point cloud transformer. Compu-\ntational Visual Media, 7(2): 187–199.\nHe, K.; Chen, X.; Xie, S.; Li, Y .; Dollár, P.; and Girshick,\nR. 2021. Masked autoencoders are scalable vision learners.\narXiv preprint arXiv:2111.06377.\nHeo, B.; Yun, S.; Han, D.; Chun, S.; Choe, J.; and Oh, S. J.\n2021. Rethinking Spatial Dimensions of Vision Transformers.\nIn International Conference on Computer Vision (ICCV).\nHudson, D. A.; and Zitnick, C. L. 2021. Generative Adver-\nsarial Transformers. Proceedings of the 38th International\nConference on Machine Learning, ICML 2021.\nJiang, Z.-H.; Hou, Q.; Yuan, L.; Zhou, D.; Shi, Y .; Jin, X.;\nWang, A.; and Feng, J. 2021. All Tokens Matter: Token Label-\ning for Training Better Vision Transformers. In Ranzato, M.;\nBeygelzimer, A.; Dauphin, Y .; Liang, P.; and Vaughan, J. W.,\neds., Advances in Neural Information Processing Systems,\nvolume 34, 18590–18602. Curran Associates, Inc.\nKatharopoulos, A.; and Fleuret, F. 2018. Not all samples are\ncreated equal: Deep learning with importance sampling. In\nInternational conference on machine learning, 2525–2534.\nPMLR.\nKim, B.; Lee, J.; Kang, J.; Kim, E.-S.; and Kim, H. J. 2021.\nHOTR: End-to-End Human-Object Interaction Detection\nwith Transformers. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 74–83.\nKong, Z.; Dong, P.; Ma, X.; Meng, X.; Niu, W.; Sun, M.; Ren,\nB.; Qin, M.; Tang, H.; and Wang, Y . 2022. SPViT: Enabling\nFaster Vision Transformers via Soft Token Pruning. ECCV.\nLi, W.; Liu, H.; Tang, H.; Wang, P.; and Van Gool, L. 2022.\nMhformer: Multi-hypothesis transformer for 3d human pose\nestimation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 13147–13156.\nLiang, J.; Cao, J.; Sun, G.; Zhang, K.; Van Gool, L.; and\nTimofte, R. 2021. SwinIR: Image Restoration Using Swin\nTransformer. arXiv preprint arXiv:2108.10257.\nLiang, Y .; GE, C.; Tong, Z.; Song, Y .; Wang, J.; and Xie,\nP. 2022. EViT: Expediting Vision Transformers via Token\nReorganizations. In International Conference on Learning\nRepresentations.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin, S.;\nand Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n10012–10022.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight Decay\nRegularization. In International Conference on Learning\nRepresentations.\nLu, Z.; Liu, H.; Li, J.; and Zhang, L. 2021. Efficient Trans-\nformer for Single Image Super-Resolution. arXiv preprint\narXiv:2108.11084.\nMa, H.; Wang, Z.; Chen, Y .; Kong, D.; Chen, L.; Liu, X.;\nYan, X.; Tang, H.; and Xie, X. 2022. PPT: token-Pruned\n8367\nPose Transformer for monocular and multi-view human pose\nestimation. In European Conference on Computer Vision,\n424–442. Springer.\nMisra, I.; Girdhar, R.; and Joulin, A. 2021. An End-to-End\nTransformer Model for 3D Object Detection. In ICCV.\nPan, B.; Jiang, Y .; Panda, R.; Wang, Z.; Feris, R.; and Oliva,\nA. 2021. IA-RED2: Interpretability-Aware Redundancy Re-\nduction for Vision Transformers. In Advances in Neural\nInformation Processing Systems.\nPaul, M. e. a. 2021. Deep Learning on a Data Diet: Finding\nImportant Examples Early in Training. InAdvances in Neural\nInformation Processing Systems.\nRao, Y .; Liu, Z.; Zhao, W.; Zhou, J.; and Lu, J. 2022. Dy-\nnamic Spatial Sparsification for Efficient Vision Transform-\ners and Convolutional Neural Networks. arXiv preprint\narXiv:2207.01580.\nRao, Y .; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh,\nC.-J. 2021. DynamicViT: Efficient Vision Transform-\ners with Dynamic Token Sparsification. arXiv preprint\narXiv:2106.02034.\nRecht, B.; Roelofs, R.; Schmidt, L.; and Shankar, V . 2019. Do\nimagenet classifiers generalize to imagenet? In International\nConference on Machine Learning, 5389–5400. PMLR.\nRyoo, M. S.; Piergiovanni, A.; Arnab, A.; Dehghani, M.; and\nAngelova, A. 2021. TokenLearner: What Can 8 Learned\nTokens Do for Images and Videos? In Advances in Neural\nInformation Processing Systems.\nSrinivas, A.; Lin, T.-Y .; Parmar, N.; Shlens, J.; Abbeel, P.;\nand Vaswani, A. 2021. Bottleneck transformers for visual\nrecognition. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 16519–16529.\nTolstikhin, I.; Houlsby, N.; Kolesnikov, A.; Beyer, L.; Zhai,\nX.; Unterthiner, T.; Yung, J.; Keysers, D.; Uszkoreit, J.; Lucic,\nM.; et al. 2021. Mlp-mixer: An all-mlp architecture for vision.\narXiv preprint arXiv:2105.01601.\nToneva, M.; Sordoni, A.; Combes, R. T. d.; Trischler, A.;\nBengio, Y .; and Gordon, G. J. 2018. An empirical study\nof example forgetting during deep neural network learning.\narXiv preprint arXiv:1812.05159.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J’egou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention. In ICML.\nTrockman, A.; and Kolter, J. Z. 2022. Patches Are All You\nNeed? arXiv preprint arXiv:2201.09792.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention\nis all you need. In Advances in neural information processing\nsystems, 5998–6008.\nWang, H.; Zhang, Z.; and Han, S. 2021. Spatten: Efficient\nsparse attention architecture with cascade token and head\npruning. In 2021 IEEE International Symposium on High-\nPerformance Computer Architecture (HPCA), 97–110. IEEE.\nXia, Z.; Pan, X.; Song, S.; Li, L. E.; and Huang, G. 2022.\nVision transformer with deformable attention. InProceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, 4794–4803.\nXu, Y .; Zhang, Z.; Zhang, M.; Sheng, K.; Li, K.; Dong, W.;\nZhang, L.; Xu, C.; and Sun, X. 2021. Evo-ViT: Slow-Fast\nToken Evolution for Dynamic Vision Transformer. arXiv\npreprint arXiv:2108.01390.\nXue, F.; Wang, Q.; and Guo, G. 2021. TransFER: Learn-\ning Relation-aware Facial Expression Representations with\nTransformers. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 3601–3610.\nYang, F.; Yang, H.; Fu, J.; Lu, H.; and Guo, B. 2020. Learn-\ning texture transformer network for image super-resolution.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 5791–5800.\nYang, G.; Tang, H.; Ding, M.; Sebe, N.; and Ricci, E. 2021.\nTransformer-based attention networks for continuous pixel-\nwise prediction. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 16269–16279.\nYu, F.; Huang, K.; Wang, M.; Cheng, Y .; Chu, W.; and Cui, L.\n2022a. Width & Depth Pruning for Vision Transformers. In\nAAAI Conference on Artificial Intelligence (AAAI), volume\n2022.\nYu, S.; Chen, T.; Shen, J.; Yuan, H.; Tan, J.; Yang, S.; Liu, J.;\nand Wang, Z. 2022b. Unified Visual Transformer Compres-\nsion. In International Conference on Learning Representa-\ntions.\nYuan, G.; Ma, X.; Niu, W.; Li, Z.; Kong, Z.; Liu, N.; Gong, Y .;\nZhan, Z.; He, C.; Jin, Q.; et al. 2021a. MEST: Accurate and\nFast Memory-Economic Sparse Training Framework on the\nEdge. Advances in Neural Information Processing Systems,\n34.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.-H.;\nTay, F. E.; Feng, J.; and Yan, S. 2021b. Tokens-to-Token ViT:\nTraining Vision Transformers From Scratch on ImageNet. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 558–567.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020.\nDeformable detr: Deformable transformers for end-to-end\nobject detection. arXiv preprint arXiv:2010.04159.\n8368"
}