{
    "title": "ACT: an Attentive Convolutional Transformer for Efficient Text Classification",
    "url": "https://openalex.org/W3174492648",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2110450295",
            "name": "Pengfei Li",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2230298063",
            "name": "Peixiang Zhong",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2653369183",
            "name": "Kezhi Mao",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2251523364",
            "name": "Wang Dongzhe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097839489",
            "name": "Xuefeng Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106157572",
            "name": "Yun-Feng Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2618086438",
            "name": "Jianxiong Yin",
            "affiliations": [
                "Nvidia (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2148591861",
            "name": "Simon See",
            "affiliations": [
                "Nvidia (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2251523364",
            "name": "Wang Dongzhe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097839489",
            "name": "Xuefeng Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106157572",
            "name": "Yun-Feng Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2618086438",
            "name": "Jianxiong Yin",
            "affiliations": [
                "Nvidia (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2148591861",
            "name": "Simon See",
            "affiliations": [
                "Nvidia (United Kingdom)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2905016804",
        "https://openalex.org/W2181042685",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W6726275242",
        "https://openalex.org/W6786078965",
        "https://openalex.org/W3045979597",
        "https://openalex.org/W2251622960",
        "https://openalex.org/W6691431627",
        "https://openalex.org/W6745742132",
        "https://openalex.org/W2786396726",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2250966211",
        "https://openalex.org/W2799027221",
        "https://openalex.org/W2154359981",
        "https://openalex.org/W2520774990",
        "https://openalex.org/W6754517385",
        "https://openalex.org/W6755411197",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W2593887162",
        "https://openalex.org/W2798858969",
        "https://openalex.org/W2735383278",
        "https://openalex.org/W2892036039",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W2892094955",
        "https://openalex.org/W2759211898",
        "https://openalex.org/W2970431814",
        "https://openalex.org/W2517194566",
        "https://openalex.org/W2814123995",
        "https://openalex.org/W4295253143",
        "https://openalex.org/W2964319599",
        "https://openalex.org/W2962965870",
        "https://openalex.org/W2963912736",
        "https://openalex.org/W2962712961",
        "https://openalex.org/W2413904250",
        "https://openalex.org/W3097777922",
        "https://openalex.org/W2735665712",
        "https://openalex.org/W2963840672",
        "https://openalex.org/W4294238563",
        "https://openalex.org/W2964302946",
        "https://openalex.org/W2941814890",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2899423466",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2767693128",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2964347512",
        "https://openalex.org/W3103513401"
    ],
    "abstract": "Recently, Transformer has been demonstrating promising performance in many NLP tasks and showing a trend of replacing Recurrent Neural Network (RNN). Meanwhile, less attention is drawn to Convolutional Neural Network (CNN) due to its weak ability in capturing sequential and long-distance dependencies, although it has excellent local feature extraction capability. In this paper, we introduce an Attentive Convolutional Transformer (ACT) that takes the advantages of both Transformer and CNN for efficient text classification. Specifically, we propose a novel attentive convolution mechanism that utilizes the semantic meaning of convolutional filters attentively to transform text from complex word space to a more informative convolutional filter space where important n-grams are captured. ACT is able to capture both local and global dependencies effectively while preserving sequential information. Experiments on various text classification tasks and detailed analyses show that ACT is a lightweight, fast, and effective universal text classifier, outperforming CNNs, RNNs, and attentive models including Transformer.",
    "full_text": "ACT: an Attentive Convolutional Transformer for Efﬁcient Text Classiﬁcation\nPengfei Li,1 Peixiang Zhong,1 Kezhi Mao,1*\nDongzhe Wang,2 Xuefeng Yang,2 Yunfeng Liu,2 Jianxiong Yin,3 Simon See3\n1 Nanyang Technological University, Singapore\n2 ZhuiYi Technology, Shenzhen, China,3 NVIDIA AI Tech Center\nfpli006,peixiang001,ekzmaog@ntu.edu.sg, fethanwang,ryan,glenliug@wezhuiyi.com, fjianxiongy,sseeg@nvidia.com\nAbstract\nRecently, Transformer has been demonstrating promising\nperformance in many NLP tasks and showing a trend of re-\nplacing Recurrent Neural Network (RNN). Meanwhile, less\nattention is drawn to Convolutional Neural Network (CNN)\ndue to its weak ability in capturing sequential and long-\ndistance dependencies, although it has excellent local feature\nextraction capability. In this paper, we introduce an Attentive\nConvolutional Transformer (ACT) that takes the advantages\nof both Transformer and CNN for efﬁcient text classiﬁcation.\nSpeciﬁcally, we propose a novel attentive convolution mech-\nanism that utilizes the semantic meaning of convolutional ﬁl-\nters attentively to transform text from complex word space to\na more informative convolutional ﬁlter space where important\nn-grams are captured. ACT is able to capture both local and\nglobal dependencies effectively while preserving sequential\ninformation. Experiments on various text classiﬁcation tasks\nand detailed analyses show that ACT is a lightweight, fast,\nand effective universal text classiﬁer, outperforming CNNs,\nRNNs, and attentive models including Transformer.\n1 Introduction\nText classiﬁcation is a fundamental problem behind many\nresearch topics in Natural Language Processing (NLP), such\nas topic categorization, sentiment analysis, relation extrac-\ntion, etc. The key issue in text classiﬁcation is text represen-\ntation learning, which aims to capture both local and global\ndependencies of texts with respect to class labels. Com-\npared with traditional bag-of-words/n-grams model (Wang\nand Manning 2012), deep neural networks have shown to be\nmore effective since word order information can be utilized\nand more semantic features can be captured. The commonly\nadopted neural architectures in deep neural networks include\nCNN, RNN, and Transformer.\nCNN is a special feed-forward neural network with con-\nvolutional layers interleaved with pooling layers. For NLP,\nthe convolutional kernels/ﬁlters in CNN can be treated as n-\ngram extractors that convert n-gram in each position into a\nvector showing its relevance to the ﬁlters. With the help of\npooling operations, the overall relevance of the text to each\nﬁlter can be captured. Therefore, CNN has advantages in\n*Corresponding author.\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\ncapturing semantic and syntactic information of n-grams for\nmore abstract and discriminative representations (Kim 2014;\nZhang, Zhao, and LeCun 2015; Li and Mao 2019). However,\nCNN is relatively weak in capturing sequential information\nand long-distance dependencies because convolutional ﬁl-\nters normally have small kernel sizes focusing only on local\nn-grams, and the pooling operation results in loss of position\ninformation. Although we could apply dilated CNN (Yu and\nKoltun 2015) or construct deep CNNs with one layer stack\non another to widen the convolution context to some extent,\nthe performance gain is normally marginal with the cost of\nmore data needed (Le, Cerisara, and Denis 2018). Besides,\nthe convolutional ﬁlters in CNN may misﬁt to task-irrelevant\nwords, hence producing non-discriminative features in the\nfeature map (Li et al. 2017, 2020).\nRNN is well-known for processing sequential data recur-\nrently and it is widely used for text classiﬁcation (Tang, Qin,\nand Liu 2015; Yogatama et al. 2017; Zhang et al. 2017a).\nHowever, RNN suffers from two problems due to its re-\ncurrent nature: gradient vanishing and parallel-unfriendly.\nMany works attempt to alleviate the gradient vanishing\nproblem by incorporating attention mechanisms to RNN\n(Zhou et al. 2016; Yang et al. 2016; Zhang et al. 2017b). A\nnovel neural architecture called Transformer (Vaswani et al.\n2017) addresses both problems by relying entirely on self-\nattention to handle long-distance dependencies without re-\ncurrent computations. The emerging of Transformer-based\nneural networks has led to a series of breakthroughs in a\nwide range of NLP tasks (Zhang et al. 2018; Li et al. 2019;\nZhong, Wang, and Miao 2019). Especially, the pre-trained\nlanguage models based on Transformer have achieved state-\nof-the-art performance in many benchmark datasets (Devlin\net al. 2019; Radford et al. 2019; Raffel et al. 2020). However,\nthe heavy architecture of Transformer often requires more\ntraining data, CPU/GPU memory, and computational power,\nespecially for long texts. Besides, since self-attention takes\ninto account all the elements with a weighted averaging op-\neration that disperses the attention distribution, Transformer\nmay overlook the relation of neighboring elements (i.e. n-\ngrams) that are important for text classiﬁcation tasks (Yang\net al. 2018, 2019a; Guo, Zhang, and Liu 2019).\nTo address the above-mentioned limitations of CNN and\nTransformer, we propose an Attentive Convolutional Trans-\nformer (ACT) which takes the advantages of both Trans-\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n13261\nformer and CNN for efﬁcient text classiﬁcation. Similar\nas Transformer, ACT also has a multi-head structure that\njointly performs attention operations in different subspaces.\nHowever, instead of self-attention, a novel attentive convolu-\ntion mechanism is performed in each attention head to better\ncapture local n-gram features. Different from conventional\nCNN, the proposed attentive convolution utilizes the seman-\ntic meaning of convolutional ﬁlters attentively and trans-\nforms texts from complex word space to a more informative\nconvolutional ﬁlter space. This not only simpliﬁes the opti-\nmization of capturing important n-grams for classiﬁcation,\nbut also allows our model to learn meaningful convolutional\nﬁlters since all the ﬁlters contribute to the ﬁnal representa-\ntion directly. Compared with self-attention, the proposed at-\ntentive convolution focuses more on learning important local\nn-gram features globally which are invariant to the speciﬁc\ninputs. These n-gram features are exactly the keywords and\nphrases that are crucial for text classiﬁcation. While majority\nof existing works augment Transformer with conventional\nCNNs to improve locality modeling capability with the cost\nof introducing more parameters (Yu et al. 2018; Mohamed,\nOkhonko, and Zettlemoyer 2019; Yang et al. 2019a; Gulati\net al. 2020), our work is a more lightweight approach and it\nis the ﬁrst to utilize the semantic meaning of convolutional\nﬁlters with attention mechanism.\nThe proposed ACT is also sequence-to-sequence, with an\nadditional global representation output by keeping the max-\npooling functionality of CNN. Therefore, it is able to cap-\nture both local and global features while preserving sequen-\ntial information. Furthermore, we propose a global attention\nmechanism to summarize the outputs of ACT and obtain the\nﬁnal representation by taking local, global, and position in-\nformation into consideration. Experiments are conducted on\ntypical text classiﬁcation tasks including sentiment analysis\nand topic categorization, as well as the more challenging re-\nlation extraction task. We present detailed analyses on ACT,\nresults show that ACT is a lightweight and efﬁcient univer-\nsal text classiﬁer, outperforming existing CNN-based, RNN-\nbased, and attentive models including Transformer.\n2 Attentive Convolutional Transformer\nWe present the proposed ACT in detail in this section. The\nattentive convolution mechanism of ACT is introduced in\nSection 2.1; the multi-head multi-layer structure of ACT is\ndescribed in Section 2.2; the global attention mechanism for\nﬁnal text representation is presented in Section 2.3.\n2.1 Attentive Convolution Mechanism\nAttentive convolution mechanism is the fundamental opera-\ntion of ACT. It ﬁrst performs n-gram convolution over text,\nthen transforms text into convolutional ﬁlter space by com-\nbining the ﬁlters attentively. With different utilization of fea-\nture maps as attention weights, attentive convolution mech-\nanism is able to capture both local and global features of\ntexts. The architecture of the proposed attentive convolution\nis shown in Figure 1 (left).\nLocal feature representation Given a text input t =\n[t1;t2;:::;t l], we ﬁrst represent each word token ti as word\nembedding qi 2 Rdw and obtain the input embeddings\nQ = [q1;q2;:::; ql] by looking up the word embedding\nmatrix Wwrd 2 Rdw\u0002V , where dw is the dimension of\nword embedding and V is vocabulary size. Then, n-gram\nconvolution over input embeddings Q is performed using\nconvolutional ﬁlters F = [f1;f2;:::; fm], where fi 2Rndw\nand nis the convolution kernel size. A feature map matrix\nM 2Rm\u0002l is generated as follows:\nM = Q ~ F (1)\nwhere ~ indicates the convolution operation of fi over Q.\nSpeciﬁcally, the value in the feature map is calculated as\nshown in Equation 2:\nmij = f(fi\nT \u0001Cat(qj;qj+1;:::;q j+n\u00001) +b) (2)\nwhere Cat means concatenation, f is a non-linear activation\nfunction and bis a bias term.\nThe values in the resulted feature map indicate seman-\ntic relevance between n-grams and convolutional ﬁlters. By\ntreating the feature map values as attention weights and ag-\ngregating the semantic convolutional ﬁlters attentively, we\ntransform each n-gram from complex word space to a more\ninformative convolutional ﬁlter space while preserving the\nsequential information of texts. Formally, the attentive con-\nvolution for local feature representation is shown in Equa-\ntion 3.\nO = F \u0001M = F \u0001(Q ~ F) (3)\nwhere O = [o1;o2;:::; ol] 2Rndw\u0002l is the output obtained\nfrom attentive convolution.\nDifferent from self-attention whose output space is still\na complex word space with varying components depending\non the input, the output space in our proposed attentive con-\nvolution mechanism is formed by n-gram convolutional ﬁl-\nters which are learned globally and invariant to the inputs.\nIn such space, important n-grams will be close to the corre-\nsponding ﬁlters and irrelevant n-grams will have small val-\nues. Therefore, the important local features (n-grams) appear\nin the texts can be captured effectively.\nGlobal feature representation Besides local features, at-\ntentive convolution mechanism can also capture global fea-\ntures of texts by applying the max-pooling technique which\nis normally used in conventional CNNs. The max-pooling\nover each row of the feature map M ﬁnds the overall rel-\nevance of the texts to each convolutional ﬁlter. By aggre-\ngating the convolutional ﬁlters attentively using the max-\npooling results, we can ﬁnd the overall semantics of texts\nin the ﬁlter space. Formally, the attentive convolution for\nglobal feature representation is shown in Equation 4.\ng = F \u0001max(M) (4)\nwhere g 2Rndw and maxmeans row-wise max-pooling.\nComparison with existing methods Compared with con-\nventional CNN whose outputs come from feature maps\nonly, our proposed attentive convolution utilizes both feature\nmaps and semantic meaning of convolutional ﬁlters for text\nrepresentation. This allows our model to learn meaningful\nconvolutional ﬁlters effectively since all the ﬁlters contribute\n13262\n…\nt1 t3 t4 tlt2\n…\n…\nf1 f2 fm\n…\nTexts\nInput \nEmbeddings \n(Q)\nN-gram\nConvolution\nLocal \nRepresentation\n…\no1 o3 o4 olo2 …\nFeature Map\n(M)\n<padding>\nConvolutional \nFilters (F)\nGlobal \nRepresentation\ng\n…\n…\n…\n…\n…\n…\nMax Pooling\nInput Embeddings (Q)\nLinear\nAttentive Convolution\n h\nConcat\nLinear\nAdd and Norm\n×  N\nFigure 1: Left: attentive convolution mechanism. Outputs are obtained by combining convolutional ﬁlters attentively utiliz-\ning feature map as attention weights. Right: multi-head multi-layer structure of ACT. h and N indicate number of attentive\nconvolution heads and layers respectively.\nto the ﬁnal representation directly. Moreover, the pooling op-\neration in conventional CNN ignores the sequential informa-\ntion of texts, whereas the local feature representation in our\nmethod preserves the sequential information while capturing\nimportant n-gram features.\nCompared with conventional attention mechanism whose\nattention weights are calculated from vector product of\nqueries (Q) and keys (K), our proposed method calculates\nattention weights through convolution of queries (Q) using\nthe keys (F), where the keys and values in our attention\nmechanism are convolutional ﬁlters learned during end-to-\nend training. The convolution operation involves wider con-\ntext (n-grams) than the vector product of single words, this\nallows our model to capture important n-gram features more\neffectively. These n-gram features are exactly the keywords\nand phrases that are crucial for text classiﬁcation. Besides,\nas mentioned in Section 2.1, the output space is more sim-\npliﬁed and informative since it is formed by convolutional\nﬁlters that are invariant to the inputs.\n2.2 Multi-head Multi-layer Attentive Convolution\nInspired by Transformer (Vaswani et al. 2017), the pro-\nposed ACT also has multi-head and multi-layer structures\nas shown in Figure 1 (right).\nFor h-head ACT, we ﬁrst linearly transform input embed-\ndings Q htimes and perform hattentive convolution simul-\ntaneously. Then the outputs from different attention heads\nare concatenated together and linearly transformed to the\noriginal input dimension, as shown in Equation 5.\nMultiHead(Q) =WOCat(O1;O2;:::; Oh)\nwhere Oi = AttenConv(WQ\ni Q)\n(5)\nHere, AttenConv indicates the proposed attentive convolu-\ntion mechanism, WQ\ni 2R(dw=h)\u0002dw and WO 2Rdw\u0002ndw\nare the weight matrices of linear transformations. Further-\nmore, we adopt the residual connection and layer norm as\nused in Vaswani et al. (2017). For multi-layer ACT, we sim-\nply pass the local representations of lower-layer to the in-\nput of upper-layer to obtain the higher-level local represen-\ntations. The global representation is obtained from the top\nACT layer.\nThe multi-head structure of ACT allows our model to\njointly capture important n-gram features in different sub-\nword spaces, where the n-grams in different spaces have dif-\nferent contributions to the ﬁnal representation. The multi-\nlayer structure allows our model to capture higher-level se-\nmantics effectively. Since the upper-layer involves a wider\ncontext for convolution, it is able to induce more abstract\nand discriminative representations.\n2.3 Global Attention and Classiﬁcation\nTo obtain the ﬁnal representation of texts for classiﬁcation,\nwe propose a global attention mechanism that summarizes\nthe sequential outputs of ACT. As shown in Figure 2, the\nattention weights are calculated by taking both local and\nglobal representations as well as position information of\neach token into consideration.\nThe local representation O 2 Rdw\u0002l and global repre-\nsentation g 2Rdw are obtained from the top-layer of ACT.\nThe position embedding P 2 Rdp\u0002l is obtained by map-\nping each token’s absolute position to dp-dimensional em-\nbeddings based on a trainable position embedding matrix\nWp 2Rdp\u0002P , where P is the total number of positions.\n13263\n…\no1 o3 olo2 …\nGlobal \nRepresentation\nFinal Representation\ng\nα1 α2 α3 αl \nPosition \nEmbedding\np1 p3 plp2 …\nLocal \nRepresentation\nr\nFigure 2: Global attention mechanism. Attention weights \u000bi\nare calculated based on local representation oi, global rep-\nresentation g, and position embedding pi of each token.\nThe ﬁnal text representation is obtained by Equation 6:\nr = O \u0001Softmax(f(WoO + WpP)T c + OT gpdw\n) (6)\nwhere f is a non-linear activation function, Wo 2Rda\u0002dw\nWp 2Rda\u0002dp are linear transformation weight matrices,da\nis attention dimension, c 2Rda is a context vector learned\nby the neural network, pdw is a scaling factor depends on\ninput dimension.\nFor classiﬁcation, we pass the ﬁnal representation r to a\nclassiﬁer consisting of a fully connected layer and a soft-\nmax layer to predict class probabilities. Our model is trained\nby minimizing categorical cross-entropy loss and center loss\n(Wen et al. 2016) using stochastic gradient descent (SGD)\nwith momentum and learning rate decay.\n3 Experiments\nWe evaluate our proposed ACT on three different text clas-\nsiﬁcation tasks, including sentiment analysis, topic catego-\nrization, and relation extraction. Since relation extraction\nis slightly different from traditional text classiﬁcation tasks\nwhere special considerations are needed for target entities,\nwe conduct experiments on it separately.\n3.1 Datasets\nWe use six widely-studied datasets to evaluate our model,\ntwo for each text classiﬁcation task. These datasets are di-\nverse in the aspects of type, size, number of classes, and\ndocument length. Table 1 shows the statistics of the datasets.\nFor sentiment analysis, we use two datasets constructed\nby Zhang et al. (2015) which are obtained from Yelp Dataset\nChallenge 2015. Yelp Review Polarity (Yelp P.) is a binary\nsentiment classiﬁcation dataset whose class is either positive\nor negative; Yelp Review Full (Yelp F.) contains more ﬁne-\ngrained sentiment classes ranging from rating 1 to 5.\nFor topic categorization, we use AG’s News (AGNews)\nand DBPedia datasets created by Zhang et al. (2015). AG-\nNews contains news articles from four categories: world, en-\ntertainment, sports, and business; DBPedia is an ontology\nclassiﬁcation dataset containing 14 non-overlapping cate-\ngories picked from DBpedia 2014.\nFor relation extraction, we use TACRED and\nSemEval2010-task8 (SemEval) datasets which contain\nhand-annotated subject and object entities as well as the\nrelation type between the entities. TACRED is a large-scale\nand complex relation extraction dataset constructed by\nZhang et al. (2017b) which has 41 relation types and a\nno\nrelation class; SemEval2010-task8 (Hendrickx et al.\n2009) is a relatively smaller relation extraction dataset\nwhich has 9 directed relations and 1 other relation.\n3.2 Baseline Models\nA variety of baseline models are used for comparison with\nour model. Different baseline models are used for relation\nextraction since the task is more challenging and normally\nrequires dedicated models.\nText Classiﬁcation Models\n• CNN-based models including Word-level CNN, Char-\nlevel CNN (Zhang, Zhao, and LeCun 2015), and deep\nCNN namely VDCNN (Conneau et al. 2016).\n• RNN-based models including standard LSTM (Zhang,\nZhao, and LeCun 2015), discriminative LSTM (D-LSTM)\nof Yogatama et al. (2017), and Skim-LSTM which dy-\nnamically updates its hidden states (Seo et al. 2018).\n• Attentive models including bi-directional block self-\nattention network (Bi-BloSAN) (Shen et al. 2018), label-\nembedding attentive model (LEAM) (Wang et al. 2018),\nand Transformer encoder (Vaswani et al. 2017) for text\nclassiﬁcation.\nRelation Extraction Models\n• CNN-based models including the standard CNN for sen-\ntence classiﬁcation (Kim 2014), CNN with position em-\nbeddings (CNN-PE) (Nguyen and Grishman 2015), and\ngraph convolutional network (GCN) over pruned depen-\ndency trees of sentences (Zhang, Qi, and Manning 2018).\n• RNN-based models including standard LSTM and\nLSTM with position-aware attention (PA-LSTM) (Zhang\net al. 2017b).\n• CNN-RNN hybrid model including contextualized GCN\n(C-GCN) where the input vectors are obtained using bi-\nLSTM (Zhang, Qi, and Manning 2018).\n• Attentive models including Transformer encoder (Bi-\nlan and Roth 2018), knowledge-attention encoder (Knwl-\nattn) (Li et al. 2019), and knowledge-attention self-\nattention integrated model (Knwl+Self).\n3.3 Experiment Settings\nIn our experiments, word embedding matrix Wwrd is ini-\ntialized with 300-d Glove word embeddings (Pennington,\nSocher, and Manning 2014). The fully connected layer be-\nfore softmax has a dimension of 100. Dropout regulariza-\ntion (Srivastava et al. 2014) with a rate of 0.4 is applied\n13264\nDatasets Types Classes Average\nlengths\nTrain\nsamples\nTest\nsamples\nYelp Review Polarity (Yelp P.) Sentiment 2 156 560,000 38,000\nYelp Review Full (Yelp F.) Sentiment 5 158 650,000 50,000\nAG’s News (AGNews) Topic 4 44 120,000 7,600\nDBPedia Topic 14 55 560,000 70,000\nTACRED Relation 41 36 90,755 15,509\nSemEval2010-task8 (SemEval) Relation 19 19 8,000 2,717\nTable 1: Statistics of the six text classiﬁcation datasets used in our experiments.\nModel Yelp P. Yelp F. AGNews DBPedia\nWord-level CNN 95.40 59.84 91.45 98.58\nChar-level CNN 94.75 61.60 90.15 98.34\nVDCNN 95.72 64.72 91.33 98.71\nLSTM 94.74 58.17 86.06 98.55\nD-LSTM 92.60 59.60 92.10 98.70\nSkim-LSTM / / 93.60 /\nBi-BloSAN 94.56 62.13 93.32 98.77\nLEAM 95.31 64.09 92.45 99.02\nTransformer 96.13* 65.34* 93.89* 98.98*\nACT 97.41 68.16 94.25 99.19\nModel TACRED SemEval\nCNN 59.3* 70.0*\nCNN-PE 61.4* 82.3*\nGCN 64.0 /\nLSTM 61.5* 80.9*\nPA-LSTM 65.1 82.7\nC-GCN 66.4 84.8\nKnwl-attn 66.4 82.3\nKnwl+Self 67.8 84.3\nTransformer 66.5 83.1\nACT 68.0 84.5\nTable 2: Left: classiﬁcation accuracy (%) on sentiment analysis and topic categorization tasks. Right: F 1 scores on relation\nextraction task, ofﬁcial micro-averaged and macro-averaged F1 scores are used for TACRED and SemEval2010-task8 datasets\nrespectively. * means the results are obtained from our implementation. / means not reported. All other results are directly cited\nfrom the respective papers mentioned in Section 3.2.\nduring training. The weight and learning rate for center loss\nare 0.001 and 0.1 respectively. The models are trained using\nSGD with initial learning rate of 0.01 and momentum of 0.9.\nLearning rate is decayed with a rate of 0.9 after 10 epochs\nif the score on the development set does not improve. Batch\nsize is set to 100 and the model is trained for 70 epochs.\nThe dimensions of global attention and position embedding\nare 200 and 60 respectively. We use GeLUs (Hendrycks and\nGimpel 2016) for all the nonlinear activation functions.\nThe hyper-parameters of ACT are selected by grid-search\n(refer to Section 4.2 for details). Speciﬁcally, for senti-\nment analysis and topic categorization, we set aside 10% of\ntraining data as the development set to tune model hyper-\nparameters. We report the average classiﬁcation accuracy on\nthe test set based on 5 independent runs. For ACT, we use\n3-layer encoder with 6 attentive convolution heads in each\nlayer, and m = 100convolutional ﬁlters with a kernel size\nof 3 in the attentive convolution mechanism. For relation ex-\ntraction, we use the same settings as Zhang et al. (2017b) for\na fair comparison with baseline models. Particularly, instead\nof using absolute positions in global attention, we use two\nrelative positions for each token with respect to the two tar-\nget entities. Each relative position embedding has a dimen-\nsion of 30 and they are concatenated together as ﬁnal po-\nsition embedding. For ACT, we use one layer encoder with\n6 attentive convolution heads in each layer, and m = 40\nconvolutional ﬁlters with a kernel size of 3 in the attentive\nconvolution mechanism.\n3.4 Results and Analysis\nExperiment results on the six text classiﬁcation datasets are\nshown in Table 2. Left table shows the classiﬁcation accu-\nracy on sentiment analysis and topic categorization tasks;\nright table shows the F 1 score on relation extraction task.\nOur proposed ACT achieves the best performance among all\nthe baseline models for majority of datasets. For SemEval\ndataset, ACT ranks the 2nd best and has comparable per-\nformance with C-GCN, a sophisticated model for relation\nextraction.\nCompared with CNN-based models, ACT performs bet-\nter than shallow CNN (word/character-level), graph convo-\nlution network (GCN), and deep CNN (VDCNN) with a sig-\nniﬁcant margin. The reason is that ACT is able to capture\nboth local n-gram features and global dependencies effec-\ntively while preserving sequential information. Besides, the\nlearning of convolutional ﬁlters is more efﬁcient using the\nproposed attentive convolution mechanism where semantic\nmeanings of the ﬁlters are utilized for text representation.\nCompared with RNN-based models, ACT consistently\noutperforms standard LSTM and improved variants of\nLSTM (D-LSTM, Skim-LSTM, and PA-LSTM) for all the\ntasks. This credits to the attentive convolution mechanism\nfor better capturing n-gram features, as well as the multi-\nhead multi-layer structure that does not suffer from gradient\nvanishing problem when capturing long-distance dependen-\ncies. The contextualized GCN (C-GCN) using bi-LSTM and\nGCN performs slightly better than ACT on SemEval dataset,\n13265\nFigure 3: Hyper-parameter study on ACT. X-coordinate indicates the hyper-parameters studied, Y-coordinate indicates classiﬁ-\ncation accuracy for Yelp F. dataset and micro-averaged F1 score for TACRED dataset.\nprobably due to the beneﬁts of dependency trees. Our model\ndoes not require any dependency parsing of the sentences.\nIt is observed that attentive models generally outperform\nRNN-based models. This is due to the better ability of at-\ntention mechanisms in capturing long-distance dependen-\ncies, especially the self-attention used in Transformer. The\nproposed ACT outperforms all the attentive models includ-\ning Transformer encoder. The reason is that ACT has bet-\nter local n-gram feature extraction capability by using at-\ntentive convolution mechanism. However, important n-gram\nfeatures may not be captured effectively by Transformer be-\ncause each token will attend to the whole sequence instead\nof n-grams, the output may be affected by irrelevant tokens.\nBesides, ACT also simpliﬁes the optimization because it\ntransforms text representation from complex word space to\nmore informative ﬁlter space, leading to more stable training\nand better keyword extraction capability.\nThe recently proposed knowledge-attention and self-\nattention integrated model (Li et al. 2019) performs as well\nas ACT on relation extraction task, with the aid of external\nlexical resources to better capture the keywords of relations.\nEncouragingly, our proposed ACT is able to capture such\nkeywords effectively without the need of external knowl-\nedge resources, yet achieving better performance.\n4 Discussions\nWe present more in-depth analyses and discussions on ACT\nin this section. Two relatively different datasets are used to\nconduct our experiments: one is Yelp F., a large dataset for\nsentiment analysis; the other is TACRED, a relation extrac-\ntion dataset which is much smaller. We report accuracy and\nmicro-averaged F1 score on the development sets of Yelp F.\nand TACRED respectively.\n4.1 Ablation Study\nWe perform an ablation study on ACT to investigate the con-\ntributions of speciﬁc components of ACT. Results are shown\nin Table 3.\n(1) We replace the proposed attentive convolution mech-\nanism with conventional CNN where feature maps are used\nfor text representation directly, the performance drop by 1.8-\n1.9%. This demonstrates the advantage of utilizing the se-\nmantic meaning of convolutional ﬁlters attentively for text\nModel Yelp F. TACRED\nACT 68.3 67.8\n1. \u0000Attentive Conv. 67.1 66.5\n2. \u0000Multi-head 67.0 65.9\n3. \u0000Global rep. 67.6 67.1\n4. \u0000Position embed. 67.4 63.5\nTable 3: Ablation study on ACT. Accuracy (%) and micro-\naveraged F1 score are reported on the development sets of\nYelp F. and TACRED respectively.\nrepresentation. (2) The proposed multi-head structure out-\nperforms single-head signiﬁcantly, showing the effective-\nness of jointly capturing n-gram features in different sub-\nword spaces in the multi-head structure. (3) Removing the\nglobal representation in global attention degrades the perfor-\nmance by 1%. This demonstrates that incorporating global\nrepresentation into the attention mechanism yields better at-\ntention weights for local representations. (4) After remov-\ning the position embeddings in global attention, the perfor-\nmance drops by 1.3% for Yelp F. and 6.3% for TACRED.\nThis shows that position information is important for text\nclassiﬁcation, especially for relation extraction task.\n4.2 Hyper-parameter Study\nIn this section, we study the inﬂuence of some important\nhyper-parameters on the performance of ACT, including\nnumber of layers, number of attentive convolution heads,\nkernel size, and number of ﬁlters in attentive convolution.\nExperiment results are shown in Figure 3.\nIt is observed that the number of ACT layers affects the\nperformance signiﬁcantly. For small datasets like TACRED,\nsingle-layer ACT achieves the best performance. For large\ndatasets like Yelp F., the optimal number of layers is 3. Fur-\nther increasing the number of layers will increase model\ncomplexity and cause performance drop due to overﬁtting.\nBesides, multiple attentive convolution heads are beneﬁ-\ncial for ACT, the optimal number of heads is 6. For ker-\nnel size, results show that 3-gram convolution is most ef-\nfective for ACT.1 It is also observed that ACT is not very\n1We also tried using multiple kernel sizes simultaneously, re-\nsults show no improvements over single kernel size.\n13266\nSample Sentences True Class Prediction\nOBJ-PERSON returned to Buffalo in 1955 and was a part of a group of black intellectuals who included philosopher\nand poetSUBJ-PERSON SUBJ-PERSON , whom she married in 1958 .\nspouse\nno relation\nOBJ-PERSON returned toBuffalo in1955 and was a part of agroup ofblack intellectualswho included philosopher\nand poet SUBJ-PERSON SUBJ-PERSON ,whom she married in1958 .\nspouse\nWhen I worked at the Renaissance tower , I 'd come here when I was too lazy to walk down the street for something\nbetter .Because , honestly , their pizza just is n't that great . Or good , really . But I 've had the breakfast muffin\ntwice and both times it was beyond awesome ! Just the right amount of grease to let you know it 's good . And super\ncheap !\n3 star\n5 star\nWhen I worked at theRenaissance tower , I'd come here when Iwas too lazy to walkdown thestreet forsomething\nbetter . Because , honestly , their pizza justis n't that great . Or good , really .But I've had thebreakfast muffin\ntwice and both times it wasbeyond awesome ! Just the right amount ofgrease to letyou knowit 's good . And super\ncheap !\n3 star\nTable 4: Attention visualization for Transformer and ACT. For each sample, the visualization of Transformer is presented ﬁrst,\nfollowed by our proposed ACT. Words are highlighted based on the attention weights assigned to them. Best viewed in color.\nsensitive to number of convolutional ﬁlters. However, larger\ndataset (Yelp F.) requires more ﬁlters than smaller dataset\n(TACRED) to achieve the best performance.\n4.3 Attention Visualization\nTo investigate what ACT focuses on, as well as its differ-\nence from Transformer, we conduct visualization of atten-\ntion weights assigned to words. We sample sentences from\nthe development sets of Yelp F. and TACRED. Two of the\nvisualizations are shown in Table 4.\nThe visualization results show that the proposed ACT can\ncapture the keywords and cue phrases more effectively than\nTransformer. It is observed that Transformer attends to a\nwide range of words in the sentence, including stop words\nand punctuations which may be irrelevant for the classiﬁca-\ntion task. On the contrary, ACT pays more attention to the\nimportant n-grams such as “married” for “spouse” relation\nand “is n’t” for ﬁne-grained sentiment classiﬁcation. These\nn-grams are the keywords and cue phrases of certain class\nwhich are crucial for classiﬁcation tasks.\n4.4 Model Size and Inference Speed\nIn this section, we investigate two practical aspects of our\nmodel for real-world applications: model size and inference\nspeed. For model size, we report the number of model pa-\nrameters. For inference speed, we report the average time\nneeded to compute a single batch (batch size of 100) of Yelp\nF. dataset using NVIDIA Tesla P40 GPU with Intel Xeon\nE5-2667 CPU. We also compare our model with Trans-\nformer under the same hyper-parameter settings as described\nin Section 3.3, results are shown in Table 5.\nThe proposed ACT is much smaller and faster compared\nwith Transformer. It has 56% fewer parameters and 2.7 times\nfaster inference speed. Therefore, ACT is a light-weight and\nefﬁcient model for text classiﬁcation, and it is more practical\nfor real-world applications. Although the large pre-trained\nModel # param. Inf. time\nTransformer 3.38M 0.19s\nACT 1.49M 0.07s\nTable 5: Comparison of model parameters and inference\ntime per batch on Yelp F. dataset.\nlanguage models based on Transformer such as BERT (De-\nvlin et al. 2019) and XLNet (Yang et al. 2019b) have\nachieved start-of-the-art performance in many NLP tasks,\nthe memory and speed constraints will become obstacles for\npractical applications.\n5 Conclusion and Future Work\nWe introduce an Attentive Convolutional Transformer\n(ACT) for efﬁcient text classiﬁcation. By taking the advan-\ntages of both Transformer and CNN, ACT is able to capture\nboth local and global dependencies effectively while pre-\nserving sequential information of texts. Particularly, a novel\nattentive convolution mechanism is proposed to better cap-\nture n-gram features in convolutional ﬁlter space. We also\npropose a global attention mechanism to obtain the ﬁnal rep-\nresentation by taking local, global, and position informa-\ntion into consideration. Detailed analyses show that ACT\nis a lightweight and efﬁcient universal text classiﬁer that\nachieves consistently good results over different text classi-\nﬁcation tasks, outperforming CNN-based, RNN-based, and\nattentive models including Transformer.\nAlthough our proposed ACT is dedicated for text classi-\nﬁcation tasks where local feature extraction capability is of\nparticular importance, we will explore the potential applica-\ntions of ACT on other NLP tasks such as machine transla-\ntion, text summarization, and language modeling in future\nwork. Furthermore, we will apply the idea of the proposed\nattentive convolution mechanism to other ﬁelds beyond NLP\ndomain, such as speech recognition and computer vision.\n13267\nReferences\nBilan, I.; and Roth, B. 2018. Position-aware Self-attention\nwith Relative Positional Encodings for Slot Filling. arXiv\npreprint arXiv:1807.03052 .\nConneau, A.; Schwenk, H.; Barrault, L.; and Lecun, Y . 2016.\nVery deep convolutional networks for natural language pro-\ncessing. arXiv preprint arXiv:1606.01781 2.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186.\nGulati, A.; Qin, J.; Chiu, C.-C.; Parmar, N.; Zhang, Y .; Yu,\nJ.; Han, W.; Wang, S.; Zhang, Z.; Wu, Y .; et al. 2020. Con-\nformer: Convolution-augmented Transformer for Speech\nRecognition. arXiv preprint arXiv:2005.08100 .\nGuo, M.; Zhang, Y .; and Liu, T. 2019. Gaussian transformer:\na lightweight approach for natural language inference. In\nProceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 33, 6489–6496.\nHendrickx, I.; Kim, S. N.; Kozareva, Z.; Nakov, P.;\n´O S ´eaghdha, D.; Pad ´o, S.; Pennacchiotti, M.; Romano, L.;\nand Szpakowicz, S. 2009. Semeval-2010 task 8: Multi-way\nclassiﬁcation of semantic relations between pairs of nomi-\nnals. In Proceedings of the Workshop on Semantic Evalu-\nations: Recent Achievements and Future Directions, 94–99.\nAssociation for Computational Linguistics.\nHendrycks, D.; and Gimpel, K. 2016. Gaussian error linear\nunits (gelus). arXiv preprint arXiv:1606.08415 .\nKim, Y . 2014. Convolutional Neural Networks for Sen-\ntence Classiﬁcation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Processing\n(EMNLP), 1746–1751.\nLe, H. T.; Cerisara, C.; and Denis, A. 2018. Do convolu-\ntional networks need to be deep for text classiﬁcation? In\nWorkshops at the Thirty-Second AAAI Conference on Artiﬁ-\ncial Intelligence.\nLi, H.; Kadav, A.; Durdanovic, I.; Samet, H.; and Graf, H. P.\n2017. Pruning Filters for Efﬁcient ConvNets. In 5th In-\nternational Conference on Learning Representations, ICLR\n2017.\nLi, P.; and Mao, K. 2019. Knowledge-oriented convolutional\nneural network for causal relation extraction from natural\nlanguage texts. Expert Systems with Applications 115: 512–\n523.\nLi, P.; Mao, K.; Yang, X.; and Li, Q. 2019. Improving Rela-\ntion Extraction with Knowledge-attention. InProceedings of\nthe 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP) ,\n229–239.\nLi, Q.; Li, P.; Mao, K.; and Lo, E. Y .-M. 2020. Improving\nconvolutional neural network for text classiﬁcation by recur-\nsive data pruning. Neurocomputing 414: 143–152.\nMohamed, A.; Okhonko, D.; and Zettlemoyer, L. 2019.\nTransformers with convolutional context for ASR. arXiv\npreprint arXiv:1904.11660 .\nNguyen, T. H.; and Grishman, R. 2015. Relation extrac-\ntion: Perspective from convolutional neural networks. In\nProceedings of the 1st Workshop on Vector Space Modeling\nfor Natural Language Processing, 39–48.\nPennington, J.; Socher, R.; and Manning, C. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nthe 2014 conference on empirical methods in natural lan-\nguage processing (EMNLP), 1532–1543.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners. OpenAI Blog 1(8): 9.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Ex-\nploring the limits of transfer learning with a uniﬁed text-\nto-text transformer. Journal of Machine Learning Research\n21(140): 1–67.\nSeo, M.; Min, S.; Farhadi, A.; and Hajishirzi, H. 2018. Neu-\nral speed reading via skim-rnn. International Conference on\nLearning Representations .\nShen, T.; Zhou, T.; Long, G.; Jiang, J.; and Zhang, C. 2018.\nBi-directional block self-attention for fast and memory-\nefﬁcient sequence modeling. In International Conference\non Representation Learning.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: a simple way to prevent\nneural networks from overﬁtting. The Journal of Machine\nLearning Research 15(1): 1929–1958.\nTang, D.; Qin, B.; and Liu, T. 2015. Document modeling\nwith gated recurrent neural network for sentiment classiﬁ-\ncation. In Proceedings of the 2015 conference on empirical\nmethods in natural language processing, 1422–1432.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWang, G.; Li, C.; Wang, W.; Zhang, Y .; Shen, D.; Zhang, X.;\nHenao, R.; and Carin, L. 2018. Joint Embedding of Words\nand Labels for Text Classiﬁcation. In Proceedings of the\n56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 2321–2331.\nWang, S.; and Manning, C. D. 2012. Baselines and bigrams:\nSimple, good sentiment and topic classiﬁcation. In Proceed-\nings of the 50th annual meeting of the association for com-\nputational linguistics: Short papers-volume 2, 90–94. Asso-\nciation for Computational Linguistics.\nWen, Y .; Zhang, K.; Li, Z.; and Qiao, Y . 2016. A dis-\ncriminative feature learning approach for deep face recogni-\ntion. In European conference on computer vision, 499–515.\nSpringer.\nYang, B.; Tu, Z.; Wong, D. F.; Meng, F.; Chao, L. S.; and\nZhang, T. 2018. Modeling Localness for Self-Attention Net-\n13268\nworks. In Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, 4449–4458.\nYang, B.; Wang, L.; Wong, D. F.; Chao, L. S.; and Tu, Z.\n2019a. Convolutional Self-Attention Networks. In Proceed-\nings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers),\n4040–4045.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov,\nR.; and Le, Q. V . 2019b. XLNet: Generalized Autoregres-\nsive Pretraining for Language Understanding.arXiv preprint\narXiv:1906.08237 .\nYang, Z.; Yang, D.; Dyer, C.; He, X.; Smola, A.; and Hovy,\nE. 2016. Hierarchical Attention Networks for Document\nClassiﬁcation. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, 1480–\n1489. San Diego, California: Association for Computational\nLinguistics. doi:10.18653/v1/N16-1174. URL https://www.\naclweb.org/anthology/N16-1174.\nYogatama, D.; Dyer, C.; Ling, W.; and Blunsom, P. 2017.\nGenerative and discriminative text classiﬁcation with recur-\nrent neural networks. In Thirty-fourth International Confer-\nence on Machine Learning (ICML 2017). International Ma-\nchine Learning Society.\nYu, A. W.; Dohan, D.; Luong, M.-T.; Zhao, R.; Chen, K.;\nNorouzi, M.; and Le, Q. V . 2018. QANet: Combining Local\nConvolution with Global Self-Attention for Reading Com-\nprehension. In International Conference on Learning Rep-\nresentations.\nYu, F.; and Koltun, V . 2015. Multi-scale context aggregation\nby dilated convolutions. arXiv preprint arXiv:1511.07122 .\nZhang, H.; Xiao, L.; Wang, Y .; and Jin, Y . 2017a. A gen-\neralized recurrent neural architecture for text classiﬁcation\nwith multi-task learning. In Proceedings of the 26th Inter-\nnational Joint Conference on Artiﬁcial Intelligence, 3385–\n3391. AAAI Press.\nZhang, J.; Luan, H.; Sun, M.; Zhai, F.; Xu, J.; Zhang, M.;\nand Liu, Y . 2018. Improving the Transformer Translation\nModel with Document-Level Context. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natural Lan-\nguage Processing, 533–542.\nZhang, X.; Zhao, J.; and LeCun, Y . 2015. Character-level\nconvolutional networks for text classiﬁcation. In Advances\nin neural information processing systems, 649–657.\nZhang, Y .; Qi, P.; and Manning, C. D. 2018. Graph Convolu-\ntion over Pruned Dependency Trees Improves Relation Ex-\ntraction. In Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, 2205–2215.\nZhang, Y .; Zhong, V .; Chen, D.; Angeli, G.; and Manning,\nC. D. 2017b. Position-aware attention and supervised data\nimprove slot ﬁlling. In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing, 35–\n45.\nZhong, P.; Wang, D.; and Miao, C. 2019. Knowledge-\nEnriched Transformer for Emotion Detection in Textual\nConversations. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), 165–176.\nZhou, P.; Shi, W.; Tian, J.; Qi, Z.; Li, B.; Hao, H.; and Xu, B.\n2016. Attention-based bidirectional long short-term mem-\nory networks for relation classiﬁcation. In Proceedings of\nthe 54th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 2: Short Papers), volume 2, 207–\n212.\n13269"
}