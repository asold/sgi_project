{
  "title": "PaLM: A Hybrid Parser and Language Model",
  "url": "https://openalex.org/W2970378492",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2009379665",
      "name": "Hao Peng",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2098207400",
      "name": "Roy Schwartz",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2183947846",
      "name": "Noah A. Smith",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1632114991",
    "https://openalex.org/W3021713638",
    "https://openalex.org/W2963851958",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963411763",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2932376173",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2964030814",
    "https://openalex.org/W4288104054",
    "https://openalex.org/W2888882903",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W2154626406",
    "https://openalex.org/W2287914047",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2964222246",
    "https://openalex.org/W1512626953",
    "https://openalex.org/W1899684160",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2888799392",
    "https://openalex.org/W2932637973",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W4289373464",
    "https://openalex.org/W2962875366",
    "https://openalex.org/W2973723395",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2038324640",
    "https://openalex.org/W2964263959",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W4238633816",
    "https://openalex.org/W2123893795",
    "https://openalex.org/W2889175957",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2889260178",
    "https://openalex.org/W2951699762",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2804228899"
  ],
  "abstract": "Hao Peng, Roy Schwartz, Noah A. Smith. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 3644–3651,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n3644\nPaLM: A Hybrid Parser and Language Model\nHao Peng♠ Roy Schwartz♠♦ Noah A. Smith♠♦\n♠Paul G. Allen School of Computer Science & Engineering, University of Washington\n♦Allen Institute for Artiﬁcial Intelligence\n{hapeng,roysch,nasmith}@cs.washington.edu\nAbstract\nWe present PaLM, a hybrid parser and neu-\nral language model. Building on an RNN lan-\nguage model, PaLM adds an attention layer\nover text spansin the left context. An unsuper-\nvised constituency parser can be derived from\nits attention weights, using a greedy decoding\nalgorithm. We evaluate PaLM on language\nmodeling, and empirically show that it out-\nperforms strong baselines. If syntactic anno-\ntations are available, the attention component\ncan be trained in a supervised manner, pro-\nviding syntactically-informed representations\nof the context, and further improving language\nmodeling performance.\n1 Introduction\nRecent language models have shown very strong\ndata-ﬁtting performance (Jozefowicz et al., 2016;\nMerity et al., 2018). They offer useful products in-\ncluding, most notably, contextual embeddings (Pe-\nters et al., 2018; Radford et al., 2019), which\nbeneﬁt many NLP tasks such as text classiﬁca-\ntion (Howard and Ruder, 2018) and dataset cre-\nation (Zellers et al., 2018).\nLanguage models are typically trained on large\namounts of raw text, and therefore do not ex-\nplicitly encode any notion of structural informa-\ntion. Structures in the form of syntactic trees\nhave been shown to beneﬁt both classical NLP\nmodels (Gildea and Palmer, 2002; Punyakanok\net al., 2008; Das et al., 2012, inter alia) and recent\nstate-of-the-art neural models (Dyer et al., 2016;\nSwayamdipta et al., 2018; Peng et al., 2018b;\nStrubell et al., 2018, inter alia). In this paper\nwe show that LMs can beneﬁt from syntactically-\ninspired encoding of the context.\nWe introduce PaLM ( parser and language\nmodel; Fig. 1), a novel hybrid model combin-\ning an RNN language model with a constituency\nparser. The LM in PaLM attends over spans of\nEMNLP IJCNLP 2019 will be held in xt+1?\n[EMNLP IJCNLP 2019] [will be held] in\n[[EMNLP IJCNLP] [2019]] [[will be] [held]] in\n[[[EMNLP][IJCNLP]][2019]][[[will] [be]] [held]] in\nLM attends\nover spans\nParser\nrecursively\nsplits spans\nby attention\nweights\nFigure 1: An illustration of PaLM. The LM (ﬁrst line)\npredicts the next word ( xt+1, double blue arrow) by\nattending over previous spans ending in time t −1\n(dashed lines). The parser (lines 2–4) splits the pre-\nﬁx into two spans (line 2) by the taking the top scoring\nattended span (red solid line) and the preﬁx leading to\nit. It then recursively splits the two sub-spans using the\nsame procedure (line 3). Finally, spans of length two\nare trivially split into terminal nodes (line 4).\ntokens, implicitly learning which syntactic con-\nstituents are likely. A span-based parser is then de-\nrived from the attention information (Stern et al.,\n2017).\nPaLM has several beneﬁts. First, it is an intu-\nitive and lightweight way of incorporating struc-\ntural information ( §2.1), requiring no marginal\ninference, which can be computationally expen-\nsive (Jelinek and Lafferty, 1991; Chelba and Je-\nlinek, 1998; Roark, 2001; Dyer et al., 2016; Buys\nand Blunsom, 2018; Kim et al., 2019, inter alia).\nSecond, the attention can be syntactically in-\nformed, in the sense that the attention component\ncan optionally be supervised using syntactic an-\nnotations, either through pretraining or by joint\ntraining with the LM ( §2.2). Last, PaLM can de-\nrive an unsupervised constituency parser ( §2.2),\nwhose parameters are estimated purely using the\nlanguage modeling objective.\nTo demonstrate the empirical beneﬁts of PaLM,\nwe experiment with language modeling ( §3).\nPaLM outperforms the AWD-LSTM model (Mer-\nity et al., 2018) on both the Penn Treebank\n3645\n(PTB; Marcus et al., 1993) and WikiText-2 (Mer-\nity et al., 2017) datasets by small but consistent\nmargins in the unsupervised setup. When the\nparser is trained jointly with the language model,\nwe see additional perplexity reductions in both\ncases. Our implementation is available athttps:\n//github.com/Noahs-ARK/PaLM.\n2 PaLM—Parser and Language Model\nWe describe PaLM in detail. At its core is an at-\ntention component, gathering the representations\nof preceding spans at each time step. Similar to\nself-attention, PaLM can be implemented on top\nof RNN encoders (Parikh et al., 2016), or as it\nis (Vaswani et al., 2017). Here we encode the to-\nkens using a left-to-right RNN, denoted with vec-\ntors ht.1\nBelow we describe the span-attention compo-\nnent and the parsing algorithm. We use[i,j],i ≤j\nto denote text span xi ...x j, i.e., inclusive on both\nsides. When i= j, it consists of a single token.\n2.1 Span Attention\nWe want the language model attention to gather\ncontext information aware of syntactic structures.\nA constituency parse can be seen as a collection of\nsyntactic constituents, i.e., token spans. Therefore\nwe attend over preceding spans.2\nAt step t, PaLM attends over the spans end-\ning at t −1, up to a maximum length m, i.e.,\n{[i,t −1]}t−1\ni=t−m.3 Essentially, this can be seen as\nsplitting the preﬁx span [t−m,t−1] into two, and\nattending over the one on the right. Such a span\nattention mechanism is inspired by the top-down\ngreedy span parser of Stern et al. (2017), which\nrecursively divides phrases. In §2.2, we will use\na similar algorithm to derive a constituency parser\nfrom the span attention weights.\nBidirectional span representation with rational\nRNNs. Meaningful span representations are cru-\ncial in span-based tasks (Lee et al., 2017; Peng\net al., 2018c; Swayamdipta et al., 2018, inter\n1We experiment with a strong LSTM implementation for\nlanguage modeling (Merity et al., 2018), see §3.\n2Standard token-based self-attention naturally relates to\ndependency structures through head selection (Strubell et al.,\n2018). In a left-to-right factored language model, dependen-\ncies are less natural if we want to allow a child to precede its\nparent.\n3m is set to 20. This reduces the number of considered\nspans from O(n2) to O(mn). Besides practical concerns, it\nmakes less sense if a phrase goes beyond one single sentence\n(the average sentence length of WSJ training sentences is 21).\nalia). Typical design choices are based on start\nand end token vectors contextualized by bidirec-\ntional RNNs. However, a language model does\nnot have access to future words, and hence run-\nning a backward RNN from right to left is less\nstraightforward: one will have to start an RNN\nrunning at each token, which is computation-\nally daunting (Kong et al., 2016). To compute\nspan representations efﬁciently, we use rational\nRNNs (RRNNs; Peng et al., 2018a).\nRRNNs are a family of RNN models, where the\nrecurrent function can be computed with weighted\nﬁnite-state automata (WFSAs). We use the uni-\ngram WFSA–inspired RRNN (Peng et al., 2018a),\nwhere the cell state update is\nft = σ(Wf ht) , (1a)\nut = (1 −ft) ⊙tanh (Wuht) , (1b)\nct = ft ⊙ct−1 + ut. (1c)\nft is a forget gate implemented with the element-\nwise sigmoid function σ, and ⊙denotes element-\nwise multiplication. Wu and Wf are learned ma-\ntrices. Bias terms are suppressed for clarity.4\nSlightly overloading the notation, let − →c i,j de-\nnote the encoding of span [i,j] by running a for-\nward RRNN in Eq. 1, from left to right. It can\nbe efﬁciently computed by subtracting − →c i−1 from− →c j, weighted by a product of forget gates:\n− →c i,j = − →c j −− →c i−1\nj⨀\nk=i\n− →f k. (2)\n− →f k vectors are the forget gates. See §B for a de-\ntailed derivation.\nUsing this observation, we now derive an efﬁ-\ncient algorithm to calculate the span representa-\ntions based on bidirectional RRNNs. In the inter-\nest of space, Alg. 1 describes the forward span rep-\nresentations. It takes advantage of the distributiv-\nity property of rational RNNs (Peng et al., 2018a),\nand the number of RNN function calls is linear\nin the input length.5 Although overall asymptotic\ntime complexity is still quadratic, Alg. 1 only in-\nvolves elementwise operations, which can be eas-\n4Unlike other RNNs such as LSTM (Hochreiter and\nSchmidhuber, 1997) or GRU (Cho et al., 2014), RRNNs do\nnot apply an afﬁne transformation or a nonlinear dependency\nof ct on ct−1.\n5In contrast, the dynamic program of Kong et al. (2016)\nfor segmental (span-scoring) RNNs requires a quadratic num-\nber of recurrent function calls, since they use LSTMs, where\ndistributivity does not hold.\n3646\nAlgorithm 1RRNN-based span representation.6\n1: procedure SPANREPR ({− →c t, − →f t})\n2: ⊿ Accumulate forward forget gates\n3: for i = 1, . . . , ndo\n4: − →f 1,i = − →f 1,i−1 ⊙− →f i\n5: end for\n6: for j = 1, . . . , ndo\n7: for i = 1, . . . , jdo\n8: − →c i,j=− →c j −− →c i−1 ⊙− →f 1,j/− →f 1,i−1\n9: end for\n10: end for\n11: return − →c i,j vectors\n12: end procedure\nily parallelized on modern GPUs. The backward\none (right to left) is analogous.\nComputing attention. As in standard attention,\nwe use a normalized weighted sum of the span rep-\nresentations. Let g([i,j]) = [ − →c i,j; ← −c i,j] denote\nthe representation of span [i,j], which concate-\nnates the forward and backward representations\ncalculated using Alg. 1. The context vector at is\nat+1 =\nm−1∑\ni=0\nωt,i g([t−i,t]), (3a)\nωt,i = exp st,i\n∑m−1\nj=0 exp st,j\n. (3b)\nHere st,i is implemented as an MLP, taking as in-\nput the concatenation of ht+1 and g([t−i,t]) and\noutputs the attention score. The context vector is\nthen concatenated with the hidden state ¯ht+1 =\n[ht+1; at+1], and fed into onward computation.\nIn summary, given an input sequence, PaLM:\n1. First uses a standard left-to-right RNN to cal-\nculate the hidden states ht.\n2. Feed ht vectors into a one-layer bidirectional\nrational RNN (Eq. 1), using Alg. 1 to com-\npute the span representations.\n3. Attends over spans (Eq. 3b) to predict the\nnext word.\n2.2 Attention-Based Constituency Parsing\nWe next describe the other facet or PaLM: the con-\nstituency parser. Our parsing algorithm is sim-\nilar to the greedy top-down algorithm proposed\nby Stern et al. (2017). It recursively divides a span\ninto two smaller ones, until a single-token span,\ni.e., a leaf, is reached. The order of the partition\n6/ denotes elementwise division. Both elementwise prod-\nuct and division are implemented in log-space.\nspeciﬁes the tree structure. 7 Formally, for a max-\nimum span length m, at each time step j+ 1, we\nsplit the span [j−m+ 1,j] into two smaller parts\n[j−m+ 1,k0] and [k0 + 1,j]. The partitioning\npoint is greedily selected, maximizing the atten-\ntion scores of spans ending at j:8\nk0 = argmax\nk∈{0,...,m−1}\nsj,k. (4)\nThe span is directly returned as a leaf if it contains\na single token. A full parse is derived by running\nthe algorithm recursively, starting with the input as\na single span (with a special end-of-sentence mark\nat the end). The runtime is O(n2), with n −1\npartitioning points. See Fig. 1 for an illustration.\nSupervising the attention. Now that we are\nable to derive phrase structures from attention\nweights, we can further inform the attention if syn-\ntactic annotations are available, using oracle span\nselections. For each token, the gold selection is\na m-dimensional binary vector, and then normal-\nized to sum to one, denoted yt.9 We add a cross-\nentropy loss (averaged across the training data) to\nthe language modeling objective, with λ trading\noff between the two:\nL= LLM + λ\nN\nN∑\nt=1\nH(yt,ωt), (5)\nwith ω being the attention distribution at step t,\nand Nthe length of the training corpus. As we will\nsee in §3, providing syntactically guided span at-\ntention improves language modeling performance.\nDiscussion. PaLM provides an intuitive way to\ninject structural inductive bias into the language\nmodel—by supervising the attention distribution.\nThis setting can be seen as a very lightweight mul-\ntitask learning, where no actual syntactic tree is\npredicted during language modeling training or\nevaluation. The attention weight predictor (i.e.,\nthe s scores in Eq. 3b) can be replaced with an\noff-the-shelf parser, or deterministically set (e.g.,\nto simulate left/right-branching).\n7It is only able to produce binarized unlabeled trees.\n8Another natural choice is to maximize the sum of the\nscores of [i, k0] and [k0+1, j]. The attention score of[i, k0] is\ncomputed at time step k0, and hence does not know anything\nabout the other span on the right. Therefore we consider only\nthe score of the right span.\n9Not necessarily one-hot: multiple spans can end at the\nsame token.\n3647\n3 Experiments\nWe evaluate PaLM on language modeling. We ex-\nperiment with the Penn Treebank corpus (PTB)\nand WikiText-2 (WT2). We follow the preprocess-\ning of Mikolov et al. (2010) for PTB and Mer-\nity et al. (2018) for WT2. More implementation\ndetails are described in Appendix A. We compare\ntwo conﬁgurations of PaLM:\n•PaLM-U builds on top of AWD-LSTM (Merity\net al., 2018), a state-of-the-art of LSTM imple-\nmentation for language modeling. The span at-\ntention is included before the last layer.10\n•PaLM-S is the same model as PaLM-U, but uses\nphrase syntax annotation to provide additional\nsupervision to the attention component (§2.2).11\nWe compare against the AWD-LSTM baseline.\nOn PTB, we also compare to two models us-\ning structural information in language modeling:\nparsing-reading-predict networks (PRPN; Shen\net al., 2018a) predicts syntactic distance as struc-\ntural features for language modeling; ordered-\nneuron LSTM (ON-LSTM; Shen et al., 2018b)\nposits a novel ordering on LSTM gates, simulat-\ning the covering of phrases at different levels in a\nconstituency parse. On PTB we also compare to\nPaLM-RB, a baseline deterministically setting the\nattention scores (Eq. 3b) in decreasing order, such\nthat the derived trees will be right-branching.12\nTables 1 and 2 summarize the language mod-\neling results. On both datasets, the unsuper-\nvised conﬁguration (PaLM-U) outperforms AWD-\nLSTM. On PTB, PaLM-U achieves similar per-\nformance to ON-LSTM and much better perfor-\nmance than PRPN. PaLM-S further reduces the\nperplexity by 1.6–3.4% (relative), showing that in-\ncorporating structural information with supervised\nspan attention helps language modeling. Naively\npromoting right-branching attention (PaLM-RB)\nyields no improvement over the baseline.\nUnsupervised constituency parsing. We eval-\nuate the parser component of PaLM-U on WSJ-\n40. It uses the same data as in language modeling,\nbut ﬁlters out sentences longer than 40 tokens after\n10Preliminary experiments show that including the span at-\ntention after the last layer yields similar empirical results, but\nis more sensitive to hyperparameters.\n11We use the WSJ portion of PTB for parsing annotations.\n12We set scores to m, m−1, . . . ,1, before the softmax.\n13Several recent works report better language modeling\nperplexity (Yang et al., 2019; Takase et al., 2018; Dai et al.,\n2019, inter alia). Their contribution is orthogonal to ours and\nnot head-to-head comparable to the models in the table.\nModel # Params. Dev. Test\nAWD-LSTM 24M 60.0 57.3\nPRPN - - 62.0\nON-LSTM 25M 58.3 56.2\nPaLM-U 24M 58.6 56.4\nPaLM-RB 24M 60.1 57.5\nPaLM-S 24M 57.9 55.5\nTable 1: PTB language modeling perplexity (lower is\nbetter). Bold fonts indicate best performance.13\nModel # Params. Dev. Test\nAWD-LSTM 33M 68.6 65.8\nPaLM-U 36M 68.4 65.4\nPaLM-S 36M 65.5 63.2\nTable 2: WikiText-2 language modeling perplexity\n(lower is better). Bold fonts indicate best performance.\npunctuation removal. The model is selected based\non language modeling validation perplexity.\nIn addition to PRPN, we compare to\nDIORA (Drozdov et al., 2019), which uses\nan inside-outside dynamic program in an au-\ntoencoder. Table 3 shows the F1 results. PaLM\noutperforms the right branching baseline, but\nis not as accurate as the other models. 14 This\nindicates that the type of syntactic trees learned\nby it, albeit useful to the LM component, do not\ncorrespond well to PTB-like syntactic trees.\nDiscussion. Despite its strong performance, the\nparsing algorithm used by Shen et al. (2018a) and\nShen et al. (2018b) suffers from an incomplete\nsupport issue (Dyer et al., 2019). More precisely,\nit fails to produce “close-open-open,” i.e., )((\nstructures. As a result, the parser is intrinsically\nbiased toward right-branching structures. PaLM,\non the other hand, scores all the spans, and there-\nfore can produce any binary tree spanning a given\nsentence: the algorithm recovers any given binary\ntree by letting sj,j−i = 1 if the tree contains non-\nterminal [i,j], and 0 otherwise.15\nIs PaLM empirically biased toward any branch-\ning direction? In greedily selected trees, we mea-\nsure the percentage of left-branching splits (divid-\ning [i,j] into [i,j −1] and j) and right-branching\n14 Evaluation on WSJ-10, which contains sentences with\n10 or less tokens, shows a similar trend.\n15 The maximum span length m is only forced in language\nmodeling training and evaluation.\n3648\nModel Unlabeled F1\nRight Branching 40.7\n†DIORA 60.6\n‡PRPN 52.4\n‡PaLM-U 42.0\nTable 3: Unlabeled unsupervised parsing F1 on WSJ-\n40. ‡trains on the training split of WSJ, while †trains\non AllNLI (Htut et al., 2018). The PRPN result is taken\nfrom Drozdov et al. (2019).\n% Left Splits % Right Splits\nRandom 39.3 ±10.5 41.2 ±8.8\nPaLM-U 1.1 85.6\nGold 6.5 52.7\nTable 4: Percentage of left and right splits. The ﬁrst\nrow shows the numbers averaging over 25 differently\nrandomly initialized PaLM models, without training.\n±indicates standard deviation.\nsplits (dividing [i,j] into iand [i+1,j]).16 Table 4\nsummarizes the results on WSJ-40 test set. The\nﬁrst row shows the results for randomly initialized\nmodels without training. We observe no signiﬁ-\ncant trend of favoring one branching direction over\nthe other. However, after training with the lan-\nguage modeling objective, PaLM-U shows a clear\nright-skewness more than it should: it produces\nmuch more right-branching structures than the\ngold annotation. This means that the span atten-\ntion mechanism has learned to emphasize longer\npreﬁxes, rather than make strong Markov assump-\ntions. More exploration of this effect is left to fu-\nture work.\n4 Conclusion\nWe present PaLM, a hybrid parser and language\nmodel. PaLM attends over the preceding text\nspans. From its attention weights phrase struc-\ntures can be derived. The attention component\ncan be separately trained to provide syntactically-\ninformed context gathering. PaLM outperforms\nstrong baselines on language modeling. In-\ncorporating syntactic supervision during train-\ning leads to further language modeling improve-\nments. Training our unsupervised model on large-\nscale corpora could result in both stronger lan-\n16We exclude trivial splits dividing a length-2 span into\ntwo tokens.\nguage models and, potentially, stronger parsers.\nOur code is publicly available at https://\ngithub.com/Noahs-ARK/PaLM.\nAcknowledgments\nWe thank members of the ARK at the University\nof Washington, and researchers at the Allen In-\nstitute for Artiﬁcial Intelligence for their helpful\ncomments on an earlier version of this work, and\nthe anonymous reviewers for their insightful feed-\nback. This work was supported in part by NSF\ngrant 1562364.\nReferences\nJan Buys and Phil Blunsom. 2018. Neural syntactic\ngenerative models with exact marginalization. In\nProc. of NAACL.\nCiprian Chelba and Frederick Jelinek. 1998. Exploit-\ning syntactic structure for language modeling. In\nProc. of COLING.\nKyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder-decoder\nfor statistical machine translation. In Proc. of\nEMNLP.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G.\nCarbonell, Quoc V . Le, and Ruslan Salakhutdinov.\n2019. Transformer-XL: Attentive language models\nbeyond a ﬁxed-length context. In Proc. of ACL.\nDipanjan Das, Andr´e F. T. Martins, and Noah A. Smith.\n2012. An exact dual decomposition algorithm for\nshallow semantic parsing with constraints. In Proc.\nof *SEM.\nAndrew Drozdov, Pat Verga, Mohit Yadav, Mohit\nIyyer, and Andrew McCallum. 2019. Unsupervised\nlatent tree induction with deep inside-outside recur-\nsive autoencoders. In Proc. of NAACL.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proc. of NAACL.\nChris Dyer, Gbor Melis, and Phil Blunsom. 2019. A\ncritical analysis of biased parsers in unsupervised\nparsing. arXiv:1909.09428.\nDaniel Gildea and Martha Palmer. 2002. The necessity\nof parsing for predicate argument recognition. In\nProc. of ACL.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. Proc. of CVPR.\n3649\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProc. of ACL.\nPhu Mon Htut, Kyunghyun Cho, and Samuel Bow-\nman. 2018. Grammar induction with neural lan-\nguage models: An unusual replication. In Proc. of\nEMNLP.\nFrederick Jelinek and John D. Lafferty. 1991. Compu-\ntation of the probability of initial substring genera-\ntion by stochastic context-free grammars. Computa-\ntional Linguistics, 17(3):315–353.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the lim-\nits of language modeling. arXiv:1602.02410.\nYoon Kim, Alexander M. Rush, Lei Yu, Adhiguna\nKuncoro, Chris Dyer, and G ´abor Melis. 2019. Un-\nsupervised recurrent neural network grammars. In\nProc. of NAACL.\nDiederik P. Kingma and Jimmy Ba. 2014.\nAdam: A method for stochastic optimization.\narXiv:1412.6980.\nLingpeng Kong, Chris Dyer, and Noah A. Smith. 2016.\nSegmental recurrent neural networks. In Proc. of\nICLR.\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\nmoyer. 2017. End-to-end neural coreference resolu-\ntion. In Proc. of EMNLP.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Computa-\ntional Linguistics, 19(2):313–330.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing LSTM\nlanguage models. In Proc. of ICLR.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture\nmodels. In Proc. of ICLR.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In Proc.\nof INTERSPEECH.\nAnkur Parikh, Oscar T ¨ackstr¨om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. In Proc. of\nEMNLP.\nHao Peng, Roy Schwartz, Sam Thomson, and Noah A.\nSmith. 2018a. Rational recurrences. In Proc. of\nEMNLP.\nHao Peng, Sam Thomson, and Noah A. Smith. 2018b.\nBackpropagating through structured argmax using a\nspigot. In Proc. of ACL.\nHao Peng, Sam Thomson, Swabha Swayamdipta, and\nNoah A. Smith. 2018c. Learning joint semantic\nparsers from disjoint data. In Proc. of NAACL.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proc. of NAACL.\nB. T. Polyak and A. B. Juditsky. 1992. Acceleration\nof stochastic approximation by averaging. SIAM J.\nControl Optim., 30(4):838–855.\nVasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.\nThe importance of syntactic parsing and inference in\nsemantic role labeling. American Journal of Com-\nputational Linguistics, 34(2):257–287.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Ope-\nnAI Blog.\nBrian Roark. 2001. Probabilistic top-down parsing\nand language modeling. Computational Linguistics,\n27(2):249–276.\nYikang Shen, Zhouhan Lin, Chin wei Huang, and\nAaron Courville. 2018a. Neural language modeling\nby jointly learning syntax and lexicon. In Proc. of\nICLR.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron C. Courville. 2018b. Ordered neurons: In-\ntegrating tree structures into recurrent neural net-\nworks. In Proc. of ICLR.\nMitchell Stern, Jacob Andreas, and Dan Klein. 2017. A\nminimal span-based neural constituency parser. In\nProc. of ACL.\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for semantic\nrole labeling. In Proc. of EMNLP.\nSwabha Swayamdipta, Sam Thomson, Kenton Lee,\nLuke Zettlemoyer, Chris Dyer, and Noah A. Smith.\n2018. Syntactic scaffolds for semantic structures. In\nProc. of EMNLP.\nSho Takase, Jun Suzuki, and Masaaki Nagata. 2018.\nDirect output connection for a high-rank language\nmodel. In Proc. of EMNLP.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. of NeurIPS.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2019. Breaking the softmax\nbottleneck: A high-rank RNN language model. In\nProc. of ICLR.\n3650\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. SW AG: A large-scale adversar-\nial dataset for grounded commonsense inference. In\nProc. of EMNLP.\n3651\nAppendices\nA Implementation Details\nNeural Network Architecture Our implemen-\ntation is based on AWD-LSTM (Merity et al.,\n2018).17 It uses a three-layer LSTM, with care-\nfully designed regularization techniques. PaLM\nincludes the span attention after the second layer.\nPreliminary results show that it yields similar re-\nsults, but is less sensitive to hyperparameters,\ncompared to adding it to the last layer.\nThe context is concatenated to the hidden state\n(¯ht = [ht; at]), and then fed to a tanh-MLP con-\ntrolled by a residual gate gr (He et al., 2016), be-\nfore fed onward into the next LSTM layer:\nˆht = gr ⊙MLP\n(¯ht\n)\n+ (1 −gr) ⊙ht. (6)\nThe rest of the architecture stays the same as\nAWD-LSTM. We refer the readers to Merity et al.\n(2018) for more details.\nMore details on PaLM-S.PaLM-S uses exactly\nthe same architecture and hyperparameters as its\nunsupervised counterpart. We derive, from PTB\ntraining data, a m-dimensional 0-1 vector for each\ntoken. Each element speciﬁes whether the corre-\nsponding span appears in the gold parse. Trivial\nspans (i.e., the ones over single tokens and full\nsentences) are ignored. The vector are normalized\nto sum to one, in order to facilitate the use of cross-\nentropy loss. λin Eq. 5 is set to 0.01.\nHyperparameters. The regularization and hy-\nperparameters largely follow Merity et al. (2018).\nWe only differ from them by using smaller hidden\nsize (and hence smaller dropout rate) to control for\nthe amount of parameters in the PTB experiments,\nsummarized in Table 5 For the WikiText-2 experi-\nments, we use 200 rational RNN size and 400 di-\nmensional context vectors. Other hyperparameters\nfollow Merity et al. (2018). The max span length\nm is set to 20 for PTB experiments, and 10 for\nWikiText-2.\nMerity et al. (2018) start by using SGD to train\nthe model, and switch to averaged SGD (Polyak\nand Juditsky, 1992) after 5 nonimprovement-\nepochs. We instead use Adam (Kingma and Ba,\n2014) with default PyTorch settings to train the\nmodel for 40 epochs, and then switch to ASGD,\nallowing for faster convergence.\n17https://github.com/salesforce/\nawd-lstm-lm\nType Values\nRational RNN size 200\nContext Vector Size 400\nLSTM Hidden Size 1020\nWeight Dropout 0.45\nVertical Dropout 0.2\nTable 5: The hyperparameters used in the PTB lan-\nguage modeling experiment.\nB Span Representations\nBelow is the derivation for Eq. 2.\n− →c i,j = − →u j +\nj−1∑\nk=i\n− →u k\nj⨀\nℓ=k+1\n− →f ℓ\n= − →u j +\nj−1∑\nk=1\n− →u k\nj⨀\nℓ=k+1\n− →f ℓ −\ni−1∑\nk=1\n− →u k\nj⨀\nℓ=k+1\n− →f ℓ\n= − →c j −\n(\n− →u i−1 +\ni−2∑\nk=1\n− →u k\ni−1⨀\nℓ=k+1\n− →f ℓ\n) j⨀\nℓ=i\n− →f ℓ\n= − →c j −− →c i−1\nj⨀\nk=i\n− →f k",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7862603664398193
    },
    {
      "name": "Parsing",
      "score": 0.7431762218475342
    },
    {
      "name": "Programming language",
      "score": 0.6429024338722229
    },
    {
      "name": "Natural language processing",
      "score": 0.5923060178756714
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4886613190174103
    },
    {
      "name": "Natural language",
      "score": 0.47522857785224915
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ],
  "cited_by": 15
}