{
  "title": "Automatic bat call classification using transformer networks",
  "url": "https://openalex.org/W4386693630",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092859161",
      "name": "Frank Fundel",
      "affiliations": [
        "Universität Ulm"
      ]
    },
    {
      "id": "https://openalex.org/A5036520551",
      "name": "Daniel Braun",
      "affiliations": [
        "Universität Ulm"
      ]
    },
    {
      "id": "https://openalex.org/A5085415063",
      "name": "Sebastian Gottwald",
      "affiliations": [
        "Universität Ulm"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2066459589",
    "https://openalex.org/W4385655584",
    "https://openalex.org/W4223635542",
    "https://openalex.org/W7055168358",
    "https://openalex.org/W2129585530",
    "https://openalex.org/W3095318399",
    "https://openalex.org/W2965503177",
    "https://openalex.org/W6767285179",
    "https://openalex.org/W2806687847",
    "https://openalex.org/W2114829768",
    "https://openalex.org/W2983227958",
    "https://openalex.org/W6811366418",
    "https://openalex.org/W4282827995",
    "https://openalex.org/W6783600611",
    "https://openalex.org/W4206619145",
    "https://openalex.org/W6745254827",
    "https://openalex.org/W4307959803",
    "https://openalex.org/W1988724590",
    "https://openalex.org/W4292836181",
    "https://openalex.org/W4309293828",
    "https://openalex.org/W2116482884",
    "https://openalex.org/W2564314123",
    "https://openalex.org/W2178212854",
    "https://openalex.org/W2109904102",
    "https://openalex.org/W6745678610",
    "https://openalex.org/W2086123829",
    "https://openalex.org/W2754437530",
    "https://openalex.org/W2616247523",
    "https://openalex.org/W6772439654",
    "https://openalex.org/W6732407650",
    "https://openalex.org/W2115796660",
    "https://openalex.org/W6788157442",
    "https://openalex.org/W2963162200",
    "https://openalex.org/W4300822871",
    "https://openalex.org/W2999187172",
    "https://openalex.org/W4312002684",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W3173479031",
    "https://openalex.org/W2994892785",
    "https://openalex.org/W2765532459",
    "https://openalex.org/W3120015336",
    "https://openalex.org/W3204614423",
    "https://openalex.org/W4226371144",
    "https://openalex.org/W2770611316",
    "https://openalex.org/W4225761128",
    "https://openalex.org/W2576656823",
    "https://openalex.org/W4287659519",
    "https://openalex.org/W4226207800",
    "https://openalex.org/W4246374915",
    "https://openalex.org/W4232784937",
    "https://openalex.org/W3181946646",
    "https://openalex.org/W2922355292",
    "https://openalex.org/W4284966855",
    "https://openalex.org/W3102564565",
    "https://openalex.org/W4214673031",
    "https://openalex.org/W2593610980",
    "https://openalex.org/W4313314294",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970226357",
    "https://openalex.org/W4229372511",
    "https://openalex.org/W2732718133",
    "https://openalex.org/W4287646898",
    "https://openalex.org/W4385245566"
  ],
  "abstract": null,
  "full_text": "Automatic Bat Call Classification\nusing Transformer Networks\nFrank Fundel, Daniel A. Braun, Sebastian Gottwald\nInstitute of Neural Information Processing, Ulm University\nFirst submission on: March 22, 2023\nAccepted on: September 1, 2023\nAbstract\nAutomatically identifying bat species from their echolocation calls is a difficult\nbut important task for monitoring bats and the ecosystem they live in. Major chal-\nlenges in automatic bat call identification are high call variability, similarities be-\ntween species, interfering calls and lack of annotated data. Many currently avail-\nable models suffer from relatively poor performance on real-life data due to being\ntrained on single call datasets and, moreover, are often too slow for real-time classi-\nfication. Here, we propose a Transformer architecture for multi-label classification\nwith potential applications in real-time classification scenarios. We train our model\non synthetically generated multi-species recordings by merging multiple bats calls\ninto a single recording with multiple simultaneous calls. Our approach achieves a\nsingle species accuracy of 88.92% (F1-score of 84.23%) and a multi species macro\nF1-score of 74.40% on our test set. In comparison to three other tools on the in-\ndependent and publicly available dataset ChiroV ox, our model achieves at least\n25.82% better accuracy for single species classification and at least 6.9% better\nmacro F1-score for multi species classification.\nKeywords\ncomputational bioacoustics, attention, Transformer, echolocation, species identifi-\ncation, acoustic monitoring, bat calls\n1\narXiv:2309.11218v1  [cs.CV]  20 Sep 2023\n1 Introduction\nBats play a vital role in maintaining ecological balance in various ecosystems\nworldwide. They provide essential pest management for agricultural crops, act as\nprimary predators of mosquitoes and other nocturnal flying insects, pollinate and\ndisperse plant seeds, and even contribute to the formation of certain cave ecosys-\ntems through their guano [1, 2]. Moreover, bats serve as excellent indicators of\nbiodiversity and environmental health [2]. Monitoring bat populations is therefore\ncrucial, particularly considering the decline of species, and some being on the verge\nof extinction, as observed in Germany, for instance [3, 4]. This task is, however,\nincredibly challenging, because bats only hunt at night, travel at high speeds, and\nare audibly silent for human observers, such that the only non-invasive method of\nmonitoring bats is based on recording and categorizing their ultrasonic echoloca-\ntion calls. As classifying hours of recordings manually is tedious, automatic detec-\ntion and classification methods have been studied for many years.\nEarly methods used frequency analysis tools for feature extraction and decision\ntrees for classification [5, 6]. Later, simple machine learning methods like Multi-\nLayer Perceptrons (MLPs) [7, 8, 9, 10, 11, 12, 13], Linear Discriminant Analysis\n(LDA) [9, 14, 10, 11], Support Vector Machines (SVMs) [15, 16, 11, 12], or en-\nsembles of MLPs [17] were used for classifying up to 44 different pre-extracted\nfeatures. More recent methods use simple Convolutional Neural Networks (Con-\nvNets) [18, 19, 20, 21] and Residual Neural Networks (ResNets) [22, 23, 24] to\ndetect and classify single calls, and also Recurrent Neural Networks (RNNs) to\nseparate echolocation calls from social calls [25]. This is in line with neighboring\nresearch fields, where ConvNets and RNNs are used to classify vocalizations of\nwhales [26, 27, 28] or birds [29, 30, 31, 32, 33, 34], for example.\nHowever, there are several challenges associated with automatic bat call clas-\nsification. For instance, distinguishing between closely related bat species such as\nMyotis brandtii and Myotis mystacinus can be difficult due to their similar calls\n[3, 22]. Additionally, bat call recordings exhibit significant variability, influenced\nby factors such as environmental conditions, flying velocity [1, 35] and particular\nacoustic behaviours such as social calls and feeding buzzes [36]. Another issue is\nthe limited availability of annotated data, as manual classification of bat calls re-\nquires expertise and extensive experience. Furthermore, since multiple bats of dif-\nferent species often call simultaneously, the presence of overlapping calls makes\ndetection and classification challenging. These factors contribute to the overall dif-\nficulty in achieving accurate classification performance, resulting in poor general-\nization due to high variability in a relatively small amount of training data. More-\nover, models trained on single, non-overlapping calls may struggle with overlap-\nping calls encountered in real-world scenarios, resulting in subpar performance\n[22].\nMost existing approaches in bat call classification primarily focus on individual\ncalls and disregard the temporal succession within call sequences. However, classi-\nfying sequences of bat calls could potentially address the high variability issue by\n2\ncapturing changes in calls over time, including transitions between different flight\npatterns and speeds. Additionally, analyzing sequence data may alleviate difficul-\nties in classifying overlapping calls when training on data with interfering calls\nfrom different species, where current models struggle—refer to Figure 8. Finally,\nthe use of large models like ResNet-50 can have performance issues, particularly\nin terms of inference time, that typically increases with the number of model pa-\nrameters. This becomes particularly impractical when classifying long recordings,\nespecially when running on a CPU.\nHere, we present BioAcoustic Transformer (BAT), a fast and light-weight end-\nto-end architecture for classifying overlapping multi-species bat call sequences.\nOur approach utilizes a small ConvNet-Transformer hybrid model that operates\non spectrogram representations of bat call recordings. Unlike previous models that\nrely on detecting individual calls and classifying them separately, BAT is trained\non synthetically generated multi-species call sequences. This approach allows us\nto perform multi-label classification, enabling the detection of different bat species\nwithin a single analysis. By leveraging this methodology, our model achieves im-\nproved efficiency and accuracy in handling overlapping calls.\n2 Methods\n2.1 Data acquisition and preprocessing\nOur dataset is based on the Skiba dataset [3] obtained from theMuseum für Naturkunde\nBerlin. This dataset comprises more than 1,500 recordings and over 45,000 indi-\nvidual calls, encompassing 29 bat species. The dataset consists of approximately\n10 GB of W A V audio files. All full spectrum recordings in this dataset are based\non the \"Pettersson D980\" device, with a consistent time expansion ratio of 1:10,\na sample rate of 96,000, and a bit depth of 24. Each recording in the dataset has\nbeen classified by an expert, showcasing a high degree of variability and absence of\noverlap between calls—refer to Figure 1 for an illustrative example. Counting the\nnumber of recordings per species reveals that there are less recordings of the rarer\nspecies—see Figure 2. To supplement the training process, we aimed to incorpo-\nrate a separate model trained on overlapping call sequences, which more closely\nresembles natural conditions. However, due to the lack of a sufficiently large ex-\nisting dataset containing overlapping bat calls, we generated our own synthetic\ndataset of overlapping call sequences.\n3\nFigure 1: Exemplary recording of Eptesicus nilssonii from the Skiba dataset. The\nwaveform shows distinct peaks indicating the occurrence of bat calls, alongside\nbackground noise. Specifically, the calls of Eptesicus nilssonii exhibit significant\nfrequency modulation, typically falling within the range of 24-27 kHz. Notably,\nthe presence of a final buzz at the end of the call can be observed, representing\nthe bat’s approach towards its prey. It is important to note that due to the time\nexpansion factor, the frequency values need to be multiplied by 10. Hence, 2,500\nHz corresponds to 25,000 Hz in the original recording.\n4\nFigure 2: Histogram of recordings per species in the Skiba dataset.\nThe recordings are prefiltered using a 10th-order butterworth high-pass fil-\nter, and downsampled from 96,000 samples per second to 22,050. We experi-\nenced no difference in model performance when using higher sample rates than\n22,050 samples per second. We then split the recordings into randomized training\n(60%, 11,323 recordings), test (25%, 4,980 recordings) and validation (15%, 2,891\nrecordings) sets and stored them in hdf5-files for faster loading (streaming). Impor-\ntantly, we split the recordings before generating sequences, so sequences from the\nsame recording never occur in two different sets. We primarily focus on German\nbat species in our analysis; however, some of them were significantly underrepre-\nsented in the dataset, so we made the decision to exclude them. As a result, the\nfinal dataset comprises 18 species.\nOur dataset only contains recordings where a single bat is calling at any one\ntime, each having a corresponding label. To simulate \"mixed calling\" scenarios, we\nsynthesized these instances by combining multiple randomly sampled sequences\nand their corresponding one-hot encoded labels. At any given time, between one\nto three single bat call sequences are randomly selected for mixing. The mixing\nprocess involves adding the time signals together and dividing the result by the\nnumber of mixed signals. Once mixed, the signals are transformed into their spec-\ntrogram representations. Subsequently, each spectrogram undergoes filtering to re-\nmove constant noise across each frequency band. Due to independent random mix-\n5\ning for each batch, it is highly likely that each batch is unique. The same mixing\napproach was applied to the sequences used for validation and testing.\n2.2 Model architecture\nOur BioAcoustic Transformer (BAT) is a ConvNet-Transformer hybrid model on\nspectrograms. Intuitively, the ConvNet extracts local spatial features of each time\npatch of the spectrogram and the Transformer detects global temporal features of\nthe whole sequence. More precisely, the ConvNet is used to embed each patch in\nthe time domain, where the patch size is chosen to have the average length of a sin-\ngle call. The subsequent attention mechanism in the Transformer can then correlate\neach embedded patch with every other embedded patch of the sequence. The pos-\nsibility of such hybrid architectures were already mentioned by the authors of the\noriginal Transformer [37], anticipating that the linear embeddings to which subse-\nquent self attention layers are applied in the original architecture can be replaced\nby various other embedding networks—see for example [38, 39].\nThe Transformer architecture was first introduced in 2017 by Vaswani et al.\n[37] in the context of language processing (NLP), in particular for translation tasks.\nSoon afterwards it was discovered that its base architecture (Transformer encoder\nblock) is very versatile and nowadays almost every model that tops the state-of-the-\nart charts, especially in sequence processing tasks, contains a Transformer-like part\nsomewhere in its architecture. A basic Transformer encoder block is displayed in\nFigure 3. First, the input sequence is embedded token-wise into a latent represen-\ntation (one vector for each token), usually containing positional information, also\nknown as positional encoding. Every attention unit in the transformer determines\nfor each token three vectors (Query vector, Key vector Value vector) that depend\non the token itself and all the other tokens. From these vectors, attention weights\ncan be calculated between all token pairs simultaneously. These attention weights\nare then used to produce an output that corresponds to a weighted sum of value\nvectors for each token. In order to consider multiple weighting schemes reflecting\nmultiple relevance relationships, there are typically multiple copies of attention\nheads with different Query-Key-Value mappings. The resulting output sequences\nof the attention heads are combined (e.g. concatenated, or discarded except for one\nclassification token in classification tasks as ours) and presented to a final layer that\ntransforms the latent vectors to a specific output, e.g. a softmax over a vocabulary\n(in language tasks), or a softmax over classes, such as the bat species in our case.\nFor the sake of brevity, we have skipped some details of this architecture, such as\nresidual connections, layer normalization, etc., for which we refer the reader to the\noriginal paper [37].\nAs already mentioned, the input to our transformer network is provided by\nConvNet patches. To obtain these patches, each sample from our preprocessed\ndataset of mixed call sequences is sliced into a sequence of 60 overlapping patches\n(with 50% overlap). Each of the resulting patches is embedded using the same\nConvNet consisting of three blocks of convolution, batch normalization, ReLu ac-\n6\ntivation and max pooling. The embedding size for each patch is 64. Similar to\nother Transformer-type classification networks, such as BERT [40], a classifica-\ntion (CLS) token is appended to the token sequence. The resulting sequence of\npatch embeddings and CLS token is then fed into a small Transformer-type en-\ncoder consisting of two self-attention layers with two attention heads each, and a\nfeed-forward dimension of 32. A final linear layer and sigmoid activation applied\nto the transformed CLS token produces the output of the network, predicting the\ndetected bat calls. The model is trained on mixed sequences and multiple labels for\nmulti-label classification, where a class is considered positive whenever the sig-\nmoid of the logits for that class is above 0.5. We manually optimized our model\nusing the validation set. The best results were obtained using Asymmetric Loss\n[41], Sharpness-Aware Minimization [42], cosine scheduler and learning rate of\n5e-4 with 25 epochs. Compare Figure 4 for a visualization of the model architec-\nture.\nFigure 3: The Transformer encoder architecture.\n3 Results\nEvaluation was conducted on two types of samples: single species, where only\none species is present in a sample, and mixed species, where multiple species are\npresent in a sample. The mixed species samples were synthetically generated using\nthe method described in Section 2.1). By conducting these comparative analyses,\nwe gained insights into the model’s performance on single species samples as well\nas its adaptability to mixed species sequences. Two metrics were utilized for eval-\nuation: accuracy and F1-score. Accuracy was employed in the evaluation of single\nspecies samples, as it measures the proportion of correctly classified instances.\nHowever, for multi-label classification, accuracy is not defined, and therefore, it\nwas used exclusively in the single species evaluation. In contrast, the F1-score was\nemployed in both single species and mixed species evaluations. It combines preci-\nsion and recall into a single measure, considering both true positives and false neg-\natives in the dataset. This metric proves particularly effective in situations where\nthe dataset is unbalanced, enabling a comprehensive evaluation of the model’s per-\nformance [43].\"\n7\nFigure 4: The proposed model architecture.\n3.1 Single species\nTo establish a baseline, we initially assess the performance solely on single species\nsamples before proceeding to evaluate sequences with mixed species in the sub-\nsequent section. To this end, we replicate the setting of Schwab et al. [22], which\nutilizes a ResNet architecture. Their model operates on individual calls extracted\nfrom call sequences using peak detection and a secondary ResNet. In addition to\nreplicating their approach, we further explored the baseline model’s capabilities by\nincorporating sequences of individual calls and averaging the predictions (referred\nto as Baseline sequential). This allowed us to assess the model’s performance when\npresented with sequential call data.\nAs one can see from Figure 5, our Transformer-based approach shows better\ntest performance than the baseline regarding accuracy and F1-score, both when\ntrained on single or mixed species recordings. In addition to our regular model\n(BAT), Figure 5 also shows the performance of two variations, where in one case\nwe replace the ConvNet with a larger ResNet, and in the other case we replace\nthe Transformer with a two-layer MLP. Both variations consist of significantly\nmore parameters than BAT. The MLP version showed notably worse performance,\nwhereas the ResNet version was slightly better, but required about 100 times the\nparameters. This shows that the combination of ConvNet embedding and Trans-\nformer is well-suited for the task. We also checked for potential improvements if\n8\nTrained on single species\nModel Accuracy F1-score # Params\nBaseline 76.53% 68.37% 6,148,563\nBaseline sequential 78.49% 68.96% 6,148,563\nBAT ResNet 83.35% 81.51% 6,383,634\nBAT 82.15% 78.42% 69,970\nMLP 79,20% 74,41% 22,198,034\nTrained on mixed species\nBAT 82.18% 77.98% 69,970\nBAT no val 88.92% 84.23% 69,970\nFigure 5: Comparison of different architectures on single species recordings.\nmore data were available, bu adding the validation set to the training dataset—this\nis indicated by the gray results in Figure 5. This type of improvement is of course\nnot expected to be special for our model.\n3.2 Mixed species\nTrained on single species\nModel Micro F1 Macro F1 # Params\nBaseline sequential 30.85% 27.93% 6,148,563\nBAT 46.57% 40.27% 69,970\nTrained on mixed species\nBaseline sequential 64.15% 50.00% 6,148,563\nMLP 72.21% 60.89% 22,198,034\nLSTM 76.82% 68.85% 94,866\nConvNet 77.4% 69.12% 124,162\nSmall ConvNet 74.09% 63.72% 46,114\nBAT 76.62% 69.31% 69,970\nBAT no val 83.02% 77.17% 69,970\nFigure 6: Comparison of different architectures on multi-label classification of\nmixed species recordings. The top two models were trained on single species data,\nwhereas the models in the bottom were trained on mixed species data.\nWhen testing predictions on mixed species recordings (see Figure 6), our Transformer-\nbased model BAT significantly outperforms the baseline model, both when trained\non single species and mixed species recordings. Overall, BAT performs similar\nto other state of the art models such as LSTMs and ConvNets. While it performs\nroughly the same as an LSTM with approximately 95, 000 parameters, BAT re-\nquires less than 70, 000 parameters . BAT’s performance sits in between a small\nand large ConvNet that we tested, with about 65% and 175% the number of param-\n9\neters, respectively. However ConvNets lack the ability of variable input lengths,\nthat is, the size of the input image must be predefined and consistent for all images\nin the dataset. For Transformers, the sequence length can be increased and shorter\ninputs can just be padded.\n3.3 Comparison to available software\nIn this section, we compare our method to other, mostly commercially available\nsoftware, like BatExplorer, batIdent and bdAnalyzer [22]. For comparison, we\nused 704 samples from our test set selected from the Skiba dataset [3], where\neach recording lasts 780 ms, and 167 samples from another smaller bat call dataset\ncalled ChiroV ox [44], where each recording lasts between 1-10 seconds. To make\nthe comparison fairer, we only used species that both BAT and bdAnalyzer did train\non. If 0 calls were detected and thus no classification can be made, the sample clas-\nsification was counted as incorrect. Our model and bdAnalyzer are biased towards\nthe Skiba dataset because both trained on parts of it. The ChiroV ox [44] dataset is\ncompletely independent. BatExplorer [45] could only export two detected species,\nso all mixed sequences with more than 2 were removed when testing BatExplorer.\nWe used default settings for all tools, for bdAnalyzer on the Skiba dataset we used\na manual call detection threshold of 0.3 instead of the automatic threshold, because\notherwise too few calls were detected. From our validation set we could deduce,\nthat a multi-label prediction threshold of 0.33 yields the best results for our model.\nFor all other methods a threshold of 0.5 was used.\nSkiba - Single species\nModel Accuracy Micro F1 Macro F1\nbatIdent 22.8% 35.34% 21.62%\nBatExplorer 38.15% 46.48% 34.36%\nbdAnalyzer 64.13% 71.71% 60.56%\nBAT 84.19% 84.58% 79.52%\nChiroV ox - Single species\nbatIdent 24.03% 38.51% 12.52%\nBatExplorer 16.28% 25.15% 10.15%\nbdAnalyzer 46.27% 56.11% 24.51%\nBAT 72.09% 77.18% 51.05%\nFigure 7: Comparison of different commercially available tools for classification\non single species recordings from Skiba [3] and ChiroV ox [44] database.\n10\nSkiba - Mixed species\nModel Micro F1 Macro F1\nbatIdent 22.48% 14.08%\nBatExplorer 41.84% 33.18%\nbdAnalyzer 65.56% 57.93%\nBAT 75.89% 70.42%\nChiroV ox - Mixed species\nbatIdent 45.14% 12.67%\nBatExplorer 50.51% 22.30%\nbdAnalyzer 52.13% 29.42%\nBAT 69.91% 36.32%\nFigure 8: Comparison of different commercially available tools for classification\non mixed recordings from Skiba [3] and ChiroV ox [44] database.\nOur method outperforms every commercially available tool, and that at a smaller\ncomputational footprint than all the other methods, opening up the possibility for\nreal-time deployment and real-time species classification.\n4 Discussion\nOur study demonstrates the potential applicability of Transformer-based models\nfor efficient classification of bioacoustic signals, such as bat call classification, al-\nlowing for high quality real-time detection based on a light-weight model. Most\nprevious methods for bat call classification were trained on short recordings con-\nsisting of single bat calls [22, 23, 46], which can make identification much more\ndifficult compared to longer recordings with multiple calls [47]. However, longer\nrecordings come with their own difficulties, including the necessity for larger mod-\nels and larger variability of the data. In fact, the presence of multiple species calls\nin longer recordings has been previously pointed out as one of the main challenges\nin bat detection [46, 47].\n11\nModel Detect Classify Call\nsequence\nMulti-\nspecies\nSimple\nannotation\nBat detective [18] Yes No No No No\nSchwab et al. [22] No Yes No No Yes\nTabak et al. [23] No Yes No No Yes\nZualkernan et al. [19] No Yes No No Yes\nChen et al. [24] No Yes No No Yes\nDierckx et al. [46] No Yes No Yes Yes\nAlipek et al. [21] No Yes Yes No Yes\nBatdetect2 [47] Yes Yes Yes (Yes) No\nOurs Yes Yes Yes Yes Yes\nFigure 9: Comparison of multiple related works and their characteristics.\nPreviously, multi-label classification of non-overlapping calls was only possi-\nble by classifying each call individually in a sequence, leaving out temporal in-\nformation of call sequences [22, 46]. A comparison of different models can be\nseen in Figure 9. Here, we compare different model characteristics, for example\nwhether the model is able to detect individual calls or whether the model is capa-\nble of species classification. Most models only focus on species identification [22,\n23, 19, 24, 46, 21], without predicting specific call locations [18, 47]. Interestingly,\nour model is able to predict call locations indirectly as a side effect of creating\npatches and leveraging the attention mechanism of the Transformer. Another char-\nacteristic we compare, is whether the model is able to use temporal information\nfrom sequences of calls, where most models only aim to detect or identify single\ncalls and only two make predictions on sequences of calls [21, 47]. Most mod-\nels were trained on single-species recordings and thus are not capable of detecting\noverlapping calls. Only a few models implemented a multi-label approach [46, 47].\nIn particular, Batdetect2 [47] follows an exceptional approach, where multi-label\nclassification is used, but the detection of overlapping calls is suppressed through\nNon-Maximum-Suppression. Models that are trained on individual calls are inher-\nently capable of multi-species classification within a sequence of non-overlapping\ncalls by classifying each call separately, with the downside of disregarding tempo-\nral information. The last characteristic we compare is whether the model is trained\non data that was extensively annotated. In Batdetective [18], this involved anno-\ntating each call individually with bounding boxes, whereas in Batdetect2 [47], not\nonly bounding boxes but also class labels were annotated. Obtaining the necessary\nresources for such costly annotations remains a challenge in acoustic monitoring\nin most places, thereby limiting its adoption.\nImportantly, in our study, we do not learn features solely from single calls, but\nour model is trained on a synthetically created dataset of multi-species recordings,\nand thus can make use of temporal information and changes between calls. Ran-\ndomly mixing the recordings might also have served as augmentation, resulting in\nmore robust latent representations. Our results show that the combination of the\n12\nConvNet and Transformer architecture borrowed from computer vision [37, 48]\nprovides an efficient model with a moderate number of parameters that can suc-\ncessfully cope with this increased variability of the data. This allows our model to\nimprove on most challenges of previous models that we mentioned in the introduc-\ntion [22, 23], such as the difficulty of overlapping calls, despite being light-weight\nand therefore easy to train and deploy.\nAlthough the Transformer-ConvNet architecture is in principle a black-box\nmodel, the attention mechanism allows to highlight relevant calls for species iden-\ntification through attention maps [49] (compare Appendix 10). The self-attention\nmechanism has recently been reported to improve bat call classification in an-\nother study [47] that segmented multi-call recordings for multi-species classifi-\ncation. Aoadha et al. used a dataset of short bat call recordings that were anno-\ntated by bounding boxes and class labels of individual calls. This costly designed\ndataset was then leveraged to train an encoder-decoder ConvNet with an attention\nmechanism on the latent space to predict bounding boxes and species of individual\ncalls. In contrast, our approach uses a much simpler dataset and longer sequences,\nwhile still being able to visualize the most informative calls for a specific predic-\ntion through attention maps. Their model achieves similar performance on a much\nlarger dataset with comparable classes to our Skiba dataset. Additionally, Aodha et\nal. excluded acoustic behaviours such as feeding buzzes and social calls from their\ndataset. We, on the other hand, intentionally incorporated them to make species\npredictions on those particular acoustic behaviours.\nWhile our model provides a first step towards direct multi-species classifica-\ntion, there is considerable room for improvement. Particularly, for mixed-species\ntraining the main challenge is posed by limited annotated data and imbalanced\nspecies occurrence. The training dataset of multi-species call sequences that we\nartificially created from single-species recordings ideally should be replaced by a\ndataset of actual recordings of mixed species, which might differ quite a bit from\nsimply adding signals. Also, training on more diverse data from multiple different\ndatasets would benefit generalization to unseen data [47]. Moreover, we were able\nto significantly increase the classification performance by including the validation\ndataset into the training data, reflecting the fact the limitation in data availability\nmight actually be the culprit of current model performance. In fact, since classi-\nfying bat calls needs expert knowledge and takes a lot of time, there is very little\nannotated data. Additionally, the occurrence of different species is highly unbal-\nanced, which is reflected in currently available datasets. One possibility to deal\nwith this issue could be the inclusion of unsupervised training.\n5 Conclusion\nIn this work, we propose a new model for bat call classification. We use a ConvNet-\nTransformer hybrid model to classify sequences of bat calls, instead of only clas-\nsifying single bat calls as in previous approaches. Additionally, by synthesizing\n13\nmixed call sequences out of single call sequences, we were able to incorporate\nmulti-label classification for classifying call sequences where multiple species are\ncalling at the same time. Without using multi-stage classification models, we found\nnew state-of-the-art results, that even outperform commercially available tools and\nother methods (Section 3.3). In particular, we were able to achieve a single species\naccuracy of 88.92% (F1-score of 84.23%) and a multi species macro F1-score\nof 74.40% on our test set. On another, independent dataset we achieved a single\nspecies accuracy of 72.09% (F1-score of 51.05%) and a multi-species macro F1-\nscore of 36.32%.\nAs a final remark, we want to note that our model is not tuned in any way\nfor bats specifically. Hence, the same architecture could also be applied to other\ndomains like bird or whale call classification, where a light-weight model like ours\nmight have similar advantages over other approaches.\nReferences\n[1] G. Neuweiler, E. Covey, and D.P.E. Covey. The Biology of Bats . Oxford\nUniversity Press, 2000. ISBN : 9780195099508. URL : https://books.\ngoogle.de/books?id=Gtp4yWnPD9YC.\n[2] Why bats matter - About Bats - Bat Conservation Trust . https://www.\nbats.org.uk/about-bats/why-bats-matter . (Accessed on\n12/16/2021).\n[3] R. Skiba. Europäische Fledermäuse: Kennzeichen, Echoortung und Detek-\ntoranwendung. Die neue Brehm-Bücherei. Westarp-Wiss., 2003.ISBN : 9783894329075.\nURL : https://books.google.de/books?id=04s4nwEACAAJ.\n[4] Säugetiere (Mammalia) - Rote-Liste-Zentrum . https://www.rote-\nliste-zentrum.de/de/Saugetiere-Mammalia-1730.html .\n(Accessed on 12/17/2021).\n[5] Alexander Herr, Nicholas Klomp, and J.S. Atkinson. “Identification of bat\necholocation calls using a decision tree classification system”. In: 4 (Jan.\n1997).\n[6] Maria Adams and Bradley Law. “Reliable Automation of Bat Call Identifi-\ncation for Eastern New South Wales, Australia, Using Classification Trees\nand AnaScheme Software”. In:Acta Chiropterologica12 (June 2010), pp. 231–\n245. DOI : 10.3161/150811010X504725.\n[7] Stuart Parsons and Gareth Jones. “Acoustic identification of twelve species\nof echolocating bat by discriminant function analysis and artificial neu-\nral Networks”. In: The Journal of experimental biology 203 (Oct. 2000),\npp. 2641–56. DOI : 10.1242/jeb.203.17.2641.\n14\n[8] Eric R. Britzke et al. “Acoustic identification of bats in the eastern United\nStates: A comparison of parametric and nonparametric methods”. In: The\nJournal of Wildlife Management 75.3 (2011), pp. 660–667. DOI : https:\n//doi.org/10.1002/jwmg.68 . eprint: https://wildlife.\nonlinelibrary.wiley.com/doi/pdf/10.1002/jwmg.68 .\nURL : https://wildlife.onlinelibrary.wiley.com/doi/\nabs/10.1002/jwmg.68.\n[9] Jorge Ayala-Berdon et al. “Random forest is the best species predictor for\na community of insectivorous bats inhabiting a mountain ecosystem of cen-\ntral Mexico”. In: Bioacoustics 30.5 (2021), pp. 608–628. DOI : 10.1080/\n09524622 . 2020 . 1835539. eprint: https : / / doi . org / 10 .\n1080 / 09524622 . 2020 . 1835539. URL : https : / / doi . org /\n10.1080/09524622.2020.1835539.\n[10] DAMIANO G. PREATONI et al. “IDENTIFYING BATS FROM TIME-\nEXPANDED RECORDINGS OF SEARCH CALLS: COMPARING CLAS-\nSIFICATION METHODS”. In:Journal of Wildlife Management69.4 (2005),\npp. 1601 –1614.DOI : 10.2193/0022-541X(2005)69[1601:IBFTRO]\n2.0.CO;2. URL : https://doi.org/10.2193/0022-541X(2005)\n69[1601:IBFTRO]2.0.CO;2.\n[11] David W. Armitage and Holly K. Ober. “A comparison of supervised learn-\ning techniques in the classification of bat echolocation calls”. In:Ecological\nInformatics 5.6 (2010), pp. 465–473. ISSN : 1574-9541. DOI : https://\ndoi.org/10.1016/j.ecoinf.2010.08.001 . URL : https://\nwww.sciencedirect.com/science/article/pii/S1574954110000919.\n[12] G. Botto Nuñez et al. “The first artificial intelligence algorithm for identi-\nfication of bat species in Uruguay”. In: Ecological Informatics 46 (2018),\npp. 97–102. ISSN : 1574-9541. DOI : https://doi.org/10.1016/j.\necoinf.2018.05.005 . URL : https://www.sciencedirect.\ncom/science/article/pii/S1574954117301127.\n[13] N. Jennings, S. Parsons, and M. J.O. Pocock. “Human vs. machine: iden-\ntification of bat species from their echolocation calls by humans and by\nartificial neural networks”. In: Canadian Journal of Zoology 86.5 (2008),\npp. 371–377. DOI : 10.1139/Z08-009 . eprint: https://doi.org/\n10.1139/Z08-009 . URL : https://doi.org/10.1139/Z08-\n009.\n[14] Danilo Russo and Gareth Jones. “Identification of twenty-two bat species\n(Mammalia: Chiroptera) from Italy by analysis of time-expanded record-\nings of echolocation calls”. In: Journal of Zoology 258.1 (2002), pp. 91–\n103. DOI : https://doi.org/10.1017/S0952836902001231 .\neprint: https://zslpublications.onlinelibrary.wiley.\ncom/doi/pdf/10.1017/S0952836902001231 . URL : https:\n15\n//zslpublications.onlinelibrary.wiley.com/doi/abs/\n10.1017/S0952836902001231.\n[15] Robert Redgwell et al. “Classification of Echolocation Calls from 14 Species\nof Bat by Support Vector Machines and Ensembles of Neural Networks”. In:\nAlgorithms 2 (Sept. 2009). DOI : 10.3390/a2030907.\n[16] Adrian T. Ruiz et al. “Automated Identification Method for Detection and\nClassification of Neotropical Bats”. In: Jan. 2017, 1 (6 .)–1 (6 .) DOI : 10.\n1049/cp.2017.0130.\n[17] Charlotte L. Walters et al. “A continental-scale tool for acoustic identi-\nfication of European bats”. In: Journal of Applied Ecology 49.5 (2012),\npp. 1064–1074. DOI : https://doi.org/10.1111/j.1365-2664.\n2012.02182.x. eprint: https://besjournals.onlinelibrary.\nwiley.com/doi/pdf/10.1111/j.1365-2664.2012.02182.\nx. URL : https://besjournals.onlinelibrary.wiley.com/\ndoi/abs/10.1111/j.1365-2664.2012.02182.x.\n[18] Oisin Mac Aodha et al. “Bat detective—Deep learning tools for bat acoustic\nsignal detection”. In:PLOS Computational Biology14.3 (Mar. 2018), pp. 1–\n19. DOI : 10.1371/journal.pcbi.1005995. URL : https://doi.\norg/10.1371/journal.pcbi.1005995.\n[19] Imran Zualkernan et al. “A Tiny CNN Architecture for Identifying Bat Species\nfrom Echolocation Calls”. In: 2020 IEEE / ITU International Conference on\nArtificial Intelligence for Good (AI4G). 2020, pp. 81–86. DOI : 10.1109/\nAI4G50087.2020.9311084.\n[20] Ali Khalighifar et al. “NABat ML: Utilizing deep learning to enable crowd-\nsourced development of automated, scalable solutions for documenting North\nAmerican bat populations”. In: Journal of Applied Ecology 59.11 (2022),\npp. 2849–2862. DOI : https://doi.org/10.1111/1365-2664.\n14280. eprint: https://besjournals.onlinelibrary.wiley.\ncom / doi / pdf / 10 . 1111 / 1365 - 2664 . 14280. URL : https :\n//besjournals.onlinelibrary.wiley.com/doi/abs/10.\n1111/1365-2664.14280.\n[21] Sercan Alipek et al. “An Efficient Neural Network Design Incorporating\nAutoencoders for the Classification of Bat Echolocation Sounds”. In: An-\nimals 13.16 (2023). ISSN : 2076-2615. DOI : 10.3390/ani13162560 .\nURL : https://www.mdpi.com/2076-2615/13/16/2560.\n[22] E Schwab et al. “Automated Bat Call Classification using Deep Convolu-\ntional Neural Networks”. In: (Apr. 2021).\n[23] Michael Tabak et al. “Automated classification of bat echolocation call record-\nings with artificial intelligence”. In: (June 2021). DOI : 10.1101/2021.\n06.23.449619.\n16\n[24] Xing Chen et al. “Automatic standardized processing and identification of\ntropical bat calls using deep learning approaches”. In: Biological Conser-\nvation 241 (2020), p. 108269. ISSN : 0006-3207. DOI : https://doi.\norg/10.1016/j.biocon.2019.108269 . URL : https://www.\nsciencedirect.com/science/article/pii/S0006320719308961.\n[25] Kangkang Zhang et al. “Separating overlapping bat calls with a bi-directional\nlong short-term memory network”. In: (Dec. 2019).DOI : 10.1101/2019.\n12.15.876656.\n[26] Peter Bermant et al. “Deep Machine Learning Techniques for the Detection\nand Classification of Sperm Whale Bioacoustics”. In: Scientific Reports 9\n(Aug. 2019), pp. 1–10. DOI : 10.1038/s41598-019-48909-4.\n[27] Christian Bergler et al. “ORCA-SPOT: An Automatic Killer Whale Sound\nDetection Toolkit Using Deep Learning”. In: Scientific Reports 9 (2019).\nURL : https://api.semanticscholar.org/CorpusID:198984191.\n[28] Yu Shiu et al. “Deep neural networks for automated detection of marine\nmammal species”. In: Scientific Reports 10 (2020). URL : https://api.\nsemanticscholar.org/CorpusID:210671560.\n[29] EmreÇakır et al. Convolutional Recurrent Neural Networks for Bird Audio\nDetection. 2017. arXiv: 1703.02317 [cs.SD].\n[30] Gabriel Morales et al. “Method for passive acoustic monitoring of bird com-\nmunities using UMAP and a deep neural network”. In: Ecological Infor-\nmatics 72 (2022), p. 101909. ISSN : 1574-9541. DOI : https : / / doi .\norg/10.1016/j.ecoinf.2022.101909 . URL : https://www.\nsciencedirect.com/science/article/pii/S1574954122003594.\n[31] Sharath Adavanne et al. “Stacked Convolutional and Recurrent Neural Net-\nworks for Bird Audio Detection”. In: June 2017.DOI : 10.23919/EUSIPCO.\n2017.8081505.\n[32] Thomas Grill and Jan Schlüter. “Two convolutional neural networks for bird\ndetection in audio signals”. In: 2017 25th European Signal Processing Con-\nference (EUSIPCO). 2017, pp. 1764–1768. DOI : 10.23919/EUSIPCO.\n2017.8081512.\n[33] Emmanuel Dufourq et al. “Passive acoustic monitoring of animal popula-\ntions with transfer learning”. In: Ecol. Informatics 70 (2022), p. 101688.\nURL : https://api.semanticscholar.org/CorpusID:249689029.\n[34] Elias Sprengel et al. “Audio Based Bird Species Identification using Deep\nLearning Techniques”. In: Conference and Labs of the Evaluation Forum .\n2016. URL : https://api.semanticscholar.org/CorpusID:\n460993.\n17\n[35] Danilo Russo, Leonardo Ancillotto, and Gareth Jones. “Bats are still not\nbirds in the digital era: echolocation call variation and why it matters for\nbat species identification”. In: Canadian Journal of Zoology 96.2 (2018),\npp. 63–78. DOI : 10.1139/cjz-2017-0089 . eprint: https://doi.\norg/10.1139/cjz-2017-0089 . URL : https://doi.org/10.\n1139/cjz-2017-0089.\n[36] Yosef Prat, Mor Taub, and Yossi Yovel. “Everyday bat vocalizations con-\ntain information about emitter, addressee, context, and behavior”. In: Sci-\nentific Reports 6.1 (2016), p. 39419. ISSN : 2045-2322. DOI : 10.1038/\nsrep39419. URL : https://doi.org/10.1038/srep39419.\n[37] Ashish Vaswani et al. Attention Is All You Need . 2017. DOI : 10.48550/\nARXIV.1706.03762 . URL : https://arxiv.org/abs/1706.\n03762.\n[38] Yunhe Gao, Mu Zhou, and Dimitris Metaxas. UTNet: A Hybrid Transformer\nArchitecture for Medical Image Segmentation . 2021. DOI : 10 . 48550 /\nARXIV.2107.00781 . URL : https://arxiv.org/abs/2107.\n00781.\n[39] Zihan Li et al. TFCNs: A CNN-Transformer Hybrid Network for Medical\nImage Segmentation. 2022. DOI : 10 . 48550 / ARXIV . 2207 . 03450.\nURL : https://arxiv.org/abs/2207.03450.\n[40] Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers\nfor Language Understanding . 2018. DOI : 10.48550/ARXIV.1810.\n04805. URL : https://arxiv.org/abs/1810.04805.\n[41] Emanuel Ben-Baruch et al. Asymmetric Loss For Multi-Label Classifica-\ntion. 2020. DOI : 10 . 48550 / ARXIV . 2009 . 14119. URL : https :\n//arxiv.org/abs/2009.14119.\n[42] Pierre Foret et al. “Sharpness-Aware Minimization for Efficiently Improving\nGeneralization”. In: CoRR abs/2010.01412 (2020). arXiv: 2010.01412 .\nURL : https://arxiv.org/abs/2010.01412.\n[43] Meng Han et al. “A survey of multi-label classification based on supervised\nand semi-supervised learning”. In: International Journal of Machine Learn-\ning and Cybernetics 14.3 (2023), pp. 697–724. ISSN : 1868-808X. DOI : 10.\n1007/s13042- 022- 01658- 9. URL : https://doi.org/10.\n1007/s13042-022-01658-9.\n[44] Tamás Görföl et al. “ChiroV ox: a public library of bat calls”. In: PeerJ 10\n(Jan. 2022), e12445. DOI : 10 . 7717 / peerj . 12445. URL : https :\n//doi.org/10.7717/peerj.12445.\n[45] BATLOGGER: BatExplorer. https://www.batlogger.com/de/\nproducts/batexplorer/. (Accessed on 07/04/2022).\n18\n[46] Lucile Dierckx, Mélanie Beauvois, and Siegfried Nijssen. “Detection and\nMulti-label Classification of Bats”. In: Advances in Intelligent Data Anal-\nysis XX . Ed. by Tassadit Bouadi, Elisa Fromont, and Eyke Hüllermeier.\nCham: Springer International Publishing, 2022, pp. 53–65. ISBN : 978-3-\n031-01333-1.\n[47] Oisin Mac Aodha et al. “Towards a General Approach for Bat Echolocation\nDetection and Classification”. In: bioRxiv (2022). DOI : 10.1101/2022.\n12.14.520490. eprint: https://www.biorxiv.org/content/\nearly/2022/12/16/2022.12.14.520490.full.pdf . URL :\nhttps://www.biorxiv.org/content/early/2022/12/16/\n2022.12.14.520490.\n[48] Ben Graham et al. LeViT: a Vision Transformer in ConvNet’s Clothing for\nFaster Inference. 2021. arXiv: 2104.01136 [cs.CV].\n[49] Junkang An and Inwhee Joe. “Attention Map-Guided Visual Explanations\nfor Deep Neural Networks”. In: Applied Sciences 12.8 (2022). ISSN : 2076-\n3417. DOI : 10.3390/app12083846 . URL : https://www.mdpi.\ncom/2076-3417/12/8/3846.\n[50] Librosa. https://librosa.org/. (Accessed on 03/25/2022).\n[51] Ramprasaath R. Selvaraju et al. “Grad-CAM: Visual Explanations from Deep\nNetworks via Gradient-Based Localization”. In: International Journal of\nComputer Vision 128.2 (2019), pp. 336–359. DOI : 10.1007/s11263-\n019-01228-7 . URL : https://doi.org/10.1007%2Fs11263-\n019-01228-7.\nAppendix\n5.1 Availability\nWe provide a demo web implementation for the trained model available athttps:\n//bat.hadros.de/. The user is provided with two options: they can either\nselect from a set of example files or upload their own W A V file. If the recording has\nalready been time expanded by 1:10, the user must specify this. The selected audio\nfile is displayed in a minimalistic wave format, allowing playback functionality.\nUpon selecting a desired model and clicking the ’predict’ button, the audio is\nsent to the server. There, the audio undergoes pre-processing, is divided into over-\nlapping patches, and is fed through the chosen model. In addition to the model’s\noutput, a Grad-CAM visualization is generated for each predicted label and sent\nback to the client. This visualization includes the original spectrogram, activation\nmaps, and the prediction.\nThe predicted labels are displayed as tabs, and clicking on a specific tab reveals\nthe corresponding activation map. Due to memory restrictions in this demo, only\nthe first 60 patches (780 ms) are utilized. It is worth noting that the web demo may\n19\nexhibit slower performance due to data transfer to and from the server, but the infer-\nence process itself is fast. For real-world applications, it is recommended to use the\nmodel offline. To facilitate this, we have developed a command-line tool available\non GitHub (https://github.com/FrankFundel/BAT-cli). The tool can\nbe cloned from the repository, and its usage is straightforward, with comprehen-\nsive documentation provided on the GitHub page. By passing a directory as an\nargument to the CLI, all files within that directory will be processed and classified,\nwith the results conveniently saved in a CSV file.\n5.2 Details about the preprocessing of our data\nWe created two functions getIndividuals which extracts individual calls from the\nrecordings and getSequences which extracts patch-sequences from the recordings.\nIn getIndividuals, sound events are detected and, if classified as bat calls, these\nsound events were cut out surrounded by a window-patch of a certain size. Since\nn_fft=512, 23 ms of audio (230 ms time expanded) resulted in 512 samples and\nthe average call length is ca. 25 ms (250 ms time expanded, calculated using data\nfrom Skiba [3]), an appropriate patch length is 44 with an overlap of 22. To detect\nsound events, the mean over each time step was calculated and the built-in function\nfor peak detection from the python audio processing library librosa [50] was used.\nTo differentiate between noise and an actual call, we set up a small ResNet-18 to\nclassify between those two classes and only return patches that were classified as\na bat call (inspired by [22]). For that we manually classified over 2,400 patches as\ncall/no-call and achieved a test accuracy of 94.77% (ADAM, ReduceLROnPlateau,\n0.001 initial learning rate, batch size 128 for 35 epochs). The getIndividuals func-\ntion returns 33,978 labeled and classified call patches.\nThe getSequences slices the whole spectrogram into patches of size 44, and\nthen slices the consecutive patches again, resulting in overlapping sequences of\noverlapping patches. Since the average calls per second is around 9 (calculated\nusing data from Skiba [3]), we selected a sequence length of 60 patches (1 second)\nand a sequence overlap of 15 patches (250 ms). No peak detection or call/no-call\nclassification is needed, since empty patches are important for the preservation of\ntime information.\n5.3 Visualization of the attention mechanism in our model\nWe can use Grad-CAM [51] to visualize the activation of the ConvNet and the\nattention of the Transformer part of our final model (mixed BAT ConvNet). Grad-\nCAM uses the gradients of a specific label during inference, to create a heatmap\nof the most \"important\" parts of an any input with respect to this target. First a\ntarget layer or multiple target layers needs to be specified, then Grad-CAM will\nfollow the gradients that flow into this layer to calculate the activation map. Usually\nthis is some kind of normalization layer, so we chose the first normalization layer\nof the ConvNet and the first normalization layer of the Transformer. A custom\n20\nreshape method is passed to the Grad-CAM algorithm, that transforms the input\nwith respect to each target layer so that the result can be displayed as an image.\nThe activation maps for each target layer are summed up to create a final activation\nmap. The predicted labels of the model are then used to create separate activation\nmaps for each individual label. The activation map is multiplied element-wise with\nthe original input sequence to create a masked output sequence. A few examples\nare shown in the appendix (Fig. 10).\nFigure 10: Ground truth with input sequence is on top, followed by masked input\nsequence for each predicted label.\n21\n22\n23\n24",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7606803178787231
    },
    {
      "name": "Human echolocation",
      "score": 0.5533902645111084
    },
    {
      "name": "Artificial intelligence",
      "score": 0.523991048336029
    },
    {
      "name": "Machine learning",
      "score": 0.5115433931350708
    },
    {
      "name": "Macro",
      "score": 0.42747795581817627
    },
    {
      "name": "Transformer",
      "score": 0.4130024313926697
    },
    {
      "name": "Data mining",
      "score": 0.3692328929901123
    },
    {
      "name": "Biology",
      "score": 0.09166038036346436
    },
    {
      "name": "Engineering",
      "score": 0.07520735263824463
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}