{
    "title": "Faster Transformer-DS: Multiscale Vehicle Detection of Remote-Sensing Images Based on Transformer and Distance-Scale Loss",
    "url": "https://openalex.org/W4389104823",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5102717059",
            "name": "Jiahuan Zhang",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A5102927497",
            "name": "H. B. Liu",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A5100388370",
            "name": "Yi Zhang",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A5100743029",
            "name": "Menghan Li",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A5027395360",
            "name": "Zongqian Zhan",
            "affiliations": [
                "Wuhan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3122799380",
        "https://openalex.org/W6733240366",
        "https://openalex.org/W2964241181",
        "https://openalex.org/W3107867277",
        "https://openalex.org/W6802878479",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W3134487554",
        "https://openalex.org/W3157042932",
        "https://openalex.org/W2963786238",
        "https://openalex.org/W2962834855",
        "https://openalex.org/W3081327309",
        "https://openalex.org/W2478285118",
        "https://openalex.org/W2036329542",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W2962721361",
        "https://openalex.org/W2504335775",
        "https://openalex.org/W2962766617",
        "https://openalex.org/W2997747012",
        "https://openalex.org/W3122173535",
        "https://openalex.org/W3012303644",
        "https://openalex.org/W3175515048",
        "https://openalex.org/W4249736682",
        "https://openalex.org/W6764322716",
        "https://openalex.org/W2962749812",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3171398643",
        "https://openalex.org/W6766183005",
        "https://openalex.org/W6750227808",
        "https://openalex.org/W2963299996",
        "https://openalex.org/W4293584584",
        "https://openalex.org/W3208154594",
        "https://openalex.org/W2586661295",
        "https://openalex.org/W2956902387",
        "https://openalex.org/W4288325606"
    ],
    "abstract": "Vehicle detection (VD) on remote sensing (RS) images has gained impressive achievements these years, mainly thanks to the development of popular learning-based object detection architectures [e.g., faster R-convolutional neural network (CNN), YOLO series, etc.]. However, for RS images, mutiscale VD with tiny size still remains challenging. Particularly, vehicles as tiny objects typically contain only a few pixels with very rare information for model training and validation, which can result in inaccurate localization and intricate classification. In this article, we present a new detection model called faster transformer-DS, in which two improvements are proposed and discussed as follows. 1) Instead of CNN-based ResNet50, a transformer-based backbone&#x2014;pyramid vision transformer v2-b<sub>0</sub> that can extract global context information, is investigated as feature extraction backbone for both object classification and bounding box regression. (2) In contrast to the conventional paradigm based and intersection over union based loss functions, to further fine-tune the localization of predicted bounding box, we proposed a novel distance-scale loss function&#x2014;the distance loss is directly related to the absolute value of the length and width of the bounding box together with its center position, while the scale loss refers to the geometric shape ratio between the bounding box and the ground truth box. To demonstrate the efficacy of our model, comprehensive experiments are conducted to show that both the proposed methods can boost the performance of multiscale VD on RS images and the best result, all of which are confirmed by experimental data indicators and visual detection results. Additionally, several state-of-the-art methods are compared and consequently, our model is notably superior to many common detection models.",
    "full_text": " \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nFaster-Transformer-DS: Multi-scale Vehicle \nDetection of Remote-sensing Images based on \nTransformer and Distance-Scale Loss \n \nJiahuan Zhang, Hengzhen Liu, Yi Zhang, Menghan Li, and Zongqian Zhan* \n \nAbstract—Vehicle Detection (VD) on Remote-Sensing (RS) \nimages has gained impressive achievements  these years , mainly \nthanks to the development of popular learning-based object \ndetection architectures (e.g., Faster R -CNN, YOLO series , etc.). \nHowever, for RS images, muti-scale vehicle detection with tiny size \nstill remains challenging. P articularly, vehicles as tiny objects \ntypically contain only a few pixels with very rare information for \nmodel training and validation, which can result in inaccurate \nlocalization and intricate classification. In this paper, we present a \nnew detection model called Faster-Transformer-DS, in which two \nimprovements are proposed  and discussed: (1) instead of CNN -\nbased ResNet50, a Transformer-based backbone—PVTv2-b0 that \ncan extract global context information, is investigated as feature \nextraction backbone for both object classification and bounding \nbox regression. (2) In contrast to the conventional paradigm-based \nand IoU (Intersection over Union)-based loss functions, to further \nfine-tune the localization of predicted bounding box, we proposed \na novel distance-scale loss (DS Loss) function\n—the distance loss is \ndirectly related to the absolute value of the length and width of the \nbounding box together with its center position, while the scale loss \nrefers to the geometric shape ratio between the bounding box and \nthe ground truth box . To demonstrate the efficacy of our  model, \ncomprehensive experiments are conducted to show that both the \nproposed methods can boost the performance of multi -scale \nvehicle detection on RS images and the best result, all of which are \nconfirmed by experimental data indicators and visual detection \nresults. Additionally, several state -of-the-art methods are \ncompared and consequently, our model  is notably superior  to \nmany common detection models. \n \nIndex Terms —Multi-scale Vehicle Detection, Transformer, \nBounding Box Regression, Distance-Scale Loss. \nI. INTRODUCTION \nEHICLE Detection (VD) [1, 5] pertains to the identification \nof small -sized vehicles that typically occupy only a few \npixels within remote sensing (RS) images, making equivalent \ndetection more challenging than that of typical objects. For RS \nimages, VD task comprises three main components: feature \nextraction, localization, and classification. Feature extraction is \nthe process for backbone to generate feature maps. Localization \nentails marking the location of object vehicles using rectangular \nbounding boxes, while classification involves categorizing the \npredicted bounding boxes based on accurate localization. With \nthe development of smart city transportation, VD in RS images, \nas an important branch of tiny object detection, gradually turns \ninto a hot spot and difficult point in urban planning. In RS \nimage datasets like DOTA an d VOC, both large and small \nvehicles are displayed in the form of small occupancy ratio, \nwhich should all belong to tiny objects. Existing CNN  \n(Convolutional Neural Networks )-based VD models for RS \nimages appear low accuracy , manifesting as omission or false \ndetection and there are difficulties such as background \ninterference, dense objects and small -occupancy-ratio objects \nthat are hard to fine-tune. Therefore, making breakthroughs in \nalgorithms operated in VD tasks is of great significance. \n \nA. CNN or Transformer \nDuring the process of feature extraction, conventional CNN \nfeature extractor s consist of multiple convolutional layers. \nThese layers generate feature maps of different sizes and \nchannels, enabling multi-scale feature fusion related to proposal \ngeneration. Backbones such as VGG16 and ResNet50, are \nfrequently employed to extract high -level feature maps of RS \nimages. Extracted lower -level features encompass perceptual \naspects like texture and shape, while higher -level features \ncapture more sophisticated semantic information like category \nand location . In  the subsequent stage of localization and \nclassification, traditional CNN models typically employ anchor \ngeneration and Non-Maximum Suppression (NMS) to pinpoint \nand identify object vehicles. The confidence of a proposal is \nderived by predicting the vehicle 's location and classification \nresult concerning the anchor box. Afterwards, NMS is utilized \nto eliminate overlapping proposals, thereby obtaining the final \ndetection outcomes. However, CNN have some limitations:  \na) CNN-based feature extraction primarily relies on layer-by-\nlayer and localized computation of features, potentially \nresulting in the loss of correlation between local and global \ncontexts. In the context of RS images, where object  \nvehicles are often small and closely resemble their \nsurroundings, global features become crucial which can aid \nin capturing scene information, enabling the identification \nof various areas, such as streets, car parks, and green spaces. \nb) The internal structure of CNN is highly hierarchical and \nchallenging to interpret due to their heavily non- linear \ncomputations. Predicting the output of a given input and \nfine-tuned model parameters is difficult, limiting the \nmodel's interpretability which  holds significant value in \nnumerous scenarios, such as medical image diagnosis, \nvehicle recognition, and equipment maintenance, where \nunderstanding the model's output is a critical consideration. \nIn contrast, Transformer-based feature extraction backbones \noffer several advantages: \na) As a potent sequence modeling tool, Transformer has \ndemonstrated its efficacy in natural language processing. \nUnlike CNN, Transformer excels in handling long \nV \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nsequences of input and emphasizes contextual interactions, \nmaking it advantageous in feature extraction tasks. \nb) The intra -layer connectivity structure of Transformer \ncontributes to enhanced interpretability. In contrast, CNN \nrely on specific convolutional kernels to learn local \nfeatures that are later combined in subsequent layers, \nmaking the corresponding features difficult to interpret. \nTherefore, Transformer-based feature extraction backbones \nexhibit advantages in capturing global context and providing \nenhanced interpretability. \n \nB. Discussion on Regression Loss Function \nConventional deep learning models are generally solved and \nevaluated by minimizing the value of loss function. The most \ncommonly used Two-stage detectors (e.g., Faster R-CNN [2], \nCascade R-CNN [3], and Dynamic R -CNN [4], etc.) rely on \nBounding Box Regression to localize object vehicle s. Many \ntasks in 2D/3D computer vision tasks,  such as object tracking \nand instance segmentation also depend on accurate Bounding \nBox Regression results. Loss Function as a tool for Bounding \nBox Regression represents the optimization  direction of the \nmodel in deep learning models. The loss function for object \ndetection tasks consists of two parts, Classification Loss and \nRegression Loss, while the work in this paper improves the \naccuracy of VD in RS images by summarizing and studying the \nRegression Loss function. \nCurrently, there are two main aspects for designing \nRegression Loss: paradigm-based (L\n1/L2 /Smooth L1/Balanced \nL1, etc.) and IoU- based (IoU/GIoU/DIoU/CIoU, etc.) loss \nfunctions. Paradigm-based loss functions are computed with the \nhelp of the border points of boxes, whereas the four variables \nof (x, y, w, h) of the predicted bounding box are independent by \ndefault, which is however not consistent with the actual \nsituation (e.g., when the centroid coordinates ( x, y) are located \nin the lower right corner of the whole image , its scale \ninformation (w, h) can only be taken as 0). The commonly used \nevaluation metrics for VD (e.g., Precision, Recall, F 1-score, \nAverage Precision (AP), and mean Average Precision (mAP) ) \nare all calculated based on IoU, whereas its value does  not \nexactly match the variations of (x, y, w, h), so there will be a \nsituation where the loss value is very tiny but the detection \naccuracy is very low, which  thus cannot intuitively reflect the \nrelationship between detection accuracy and IoU.  \nFor example, for L2 Loss, also known as Mean Square Error \n(MSE) Loss, assuming that the result of model prediction  \n(predicted bounding box)  is f(xi) and the true value  (ground \ntruth box) is yi, the loss value is shown in Formula (1): \n( ) [ ]2\n2\n1\n1, ()\nn\nL ii\ni\nLoss x y y f x n =\n= −∑ .                   (1) \nObviously, there is no correlation between the variables xi (in \nthe background of Bounding Box Regression, n=4, x1=x, x2=y, \nx3=w, x4=h). In fact, the four variables of the bounding box (x, \ny, w, h) are correlated with each other by the shape constraints. \nTo address the limitations of paradigm-based loss functions, \nIoU-based loss functions are computed such that (x, y, w, h) are \ncorrelated with each other, and at the same time the metric is \nmade scale invariant. However, IoU -based loss functions also \nhave certain defects, especially when there is a special \npositional relationship between box es (e.g., nesting, boundary \noverlap, center overlap, etc.), it is difficult to objectively reflect \nthe degree of overlap between them.  \nMeanwhile, the absolute offset of the predicted bounding box \noutside a n object vehicle may be  small but will cause large \nfluctuations in IoU-based loss functions; as a comparison, when \nan equal offset is applied to a normal object (house), the change \nin IoU is relatively tiny. Therefore, IoU is very sensitive to the \nbounding box offset of  vehicles [5] , and stronger constraints \nneed to be imposed on the Bounding Box Regression process to \nimprove the accuracy (as quantitative analyzed in Fig. 1(a)(b), \nwhere the squares simulate pixel points). \n \n \n(a) Tiny object (vehicle) \n \n \n(b) Normal object (buildings) \nFig. 1.    Comparison of normal and tiny object detection \n \nTherefore, this paper introduces PVTv2 -b0 [ 22] as a new \nfeature extraction network, which introduces a pyramid \nstructure and makes Transformer suitable for the detection of \nvehicles in RS images by generating a high -resolution multi-\nscale Feature map. On the other hand, this paper proposes a new \nloss function that considers the correlation of the bounding box \nvariables and strengthens the constraints on the bounding box \nshape and position. Through experiments, it is proved that the \nloss function proposed in this paper exhibits higher accuracy \nand better visualization detection results than the existing loss \nfunction in VD in RS images.  \nThe main contributions of this paper are summarized below: \na) Aiming at the lack of semantic information o n local \nfeatures of vehicles extracted by CNN in RS images and \nthe problem of model interpretability, we introduce \nPyramid Vision Transformer v2 -b\n0 (PVTv2-b0) as the \nfeature extraction backbone combined with Feature \nPyramid Networks (FPN) to generate and fuse multi-scale \nfeature maps and use attention mechanism to obtain richer \nglobal feature information , which makes the accuracy \nfurther improved compared with CNN-based methods. \nb) We propose a Distance-Scale Loss (DS Loss) function  \nthat exhibits better convergence performance than existing \nloss functions. Meanwhile, considering that object vehicles \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nin RS images occupy fewer pixels, we introduce the scale \nparameter ki into the  Δf1~Δf3 constraints based on \ngeometric shapes to increase the weight of the related value \nof predicted bounding box in the formula, which further \nimproves the upper limit of convergence accuracy. It is \nexperimentally proved that DS Loss function shows \nsuperior performance in VD. \nc) We demonstrate the higher convergence accuracy of DS \nLoss when Balanced L\n1 is selected as Robust Estimator; \nthen optimal values of DS Loss parameters are determined \nthrough ablation experiments, and the ablation experiments \nfor the constraints prove the indispensability of Δ f1~Δf5; \nfinally, DS Loss is compared with the existing mainstream \nloss functions through a large number of comparison \nexperiments to prove that it generally exhibits higher \nconvergence accuracy in general. \nII. RELATED WORKS \nCurrent research on VD of RS images mainly focuses on \nmulti-scale feature learning, improvement of detection model s \nand design of Regression Loss functions. Through continuous \nimprovement and innovation, researchers have obtained a series \nof excellent results as follows.  \n \nA. Multi-scale Feature Learning \nMulti-scale feature learning is a highly effective approach for \ndetecting tiny objects especially vehicles in RS images. This \nmethod involves decomposing the image into different scales \nand then fusing the features from these various scales to \nenhance the accuracy.  \nFPN [6] is a commonly used method for addressing v ehicle \ndetection in RS images. FPN improves the resolution of the \nbottom layer by employing a top-down deconvolution approach. \nThis ensures that each level has access to a feature map, \nenriching the semantic information available. Bi -Directional \nAttentional Feature Pyramid Network (Bi -AFPN), introduced \nin [7], enhances FPN by learning weights of feature fusion and \nincorporating cross-layer information flow into the feature \ndelivery path of FPN and introduces Attention Mechanism (AM) \nto fuse information across different layers, leading to improved \naccuracy and model robustness.  \nMoreover, feature fusion is a crucial aspect driving \nimprovements and better performance . Hierarchical Attention \nEnhanced Region- based CNN [8] leverages feature fusion \nacross layers. Instead of traditional equal-scale fusion methods, \nthis algorithm fuses features to an appropriate scale, so high-\nresolution features are better utilized to extract information \nabout vehicles, resulting in improved detection performance. In \nsummary, multi-scale feature learning, along with the use of  \nFPN, Bi -Directional Attentional  FPN, and Hierarchical \nAttention Enhanced Region- based CNN, contributes \nsignificantly to the accuracy and robustness of vehicle detection \nin RS images. \n \nB. Improvements to Detection Models \nOptimization of network structure is a vital research direction. \nIt involves fine-tuning various aspects of the network, such as \ndepth or width of a network , feature extractors , activation \nfunctions, and methods of regularization, to achieve better \ndetection performance.  \nOne notable method in this area is the Single -Shot \nRefinement Neural Network (SRR-Net) proposed in [9]. SRR-\nNet effectively reduces false and missed detections of vehicles \nwhile maintaining robust performance even under low \nthreshold conditions. Another  important contribution to  \nnetwork optimization is the introduction of a novel activation \nfunction called Sigmoid -Weighted Linear Units (SiLU) as \nproposed in [10].  SiLU offers improved performance at a \nminimal computational cost, retaining symmetry and non -\nmonotonic properties. Extensive experiments conducted on \nvarious datasets demonstrate the excellent performance of SiLU. \nFurthermore, [11] proposed a new Maximum Correntropy \nCriterion (MCC) for network optimization. MCC employs \nHuber's alternative to exponential squared loss to measure the \nerror between predictions and objects. This helps in mitigating \nthe impact of anomalous samples. Additionally, the theory of \nmaximal correlation is used to define the network regularization \nterm, leading to improved robustness by enhancing the \ncorrelation between inputs and outputs. \n \nC. Design of Regression Loss Functions \nBounding box regression consists of  three main geometric \nfactors: overlap area, centroid distance , and aspect ratio. \nParadigm-based loss functions are usually used to measure the \ndifference between two vectors, taking the coupling between \nthe coordinates into account. In contrast, IoU -based loss \nfunctions do not take the correlation between the coordinates \ninto account but rather regress the box composed of four points \nas a whole. A summary of work s related to Regression Loss is \nshown in Table Ⅰ. \nFor paradigm-based Regression Loss functions, L\n1 Loss and \nL2 Loss are the two most common ones, with their advantages \nand disadvantages. Smooth L1 Loss improves the zero non -\nsmoothing problems compared to L1 Loss and is insensitive to \nthe outliers when x is larger compared to L2 Loss, but the results \nare not exactly equivalent to the actual IoU -based overlap. \nBalanced L1 Loss improves the unreasonable distribution of \npositive and negative sample gradient contributions and reduces \nthe negative effect of noise label on Loss, but also increases the \ncomputational volume. For IoU-based functions, IoU Loss does \nnot reflect the distance between two objects when IoU=0. \nTherefore, the subsequently proposed functions consider the \nminimum outer rectangle of the two boxes , the distance \nbetween the centroid of the two frames,  the true difference \nbetween the aspect ratio, width /height of boxes, and its \nconfidence score, respectively , but still has the drawbacks of \nhigh data labeling requirements and high computational \ncomplexity. A comparison of the advantages and disadvantages \nof regression Loss is shown in Table Ⅰ. \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nTABLE Ⅰ \nSUMMARY OF WORK RELATED TO REGRESSION LOSS \n \n Loss function Advantages Disadvantages \nParadig\nm-based \nL1 Loss [12] Stable gradient; better robustness to \nanomalous outliers \nThe center point is not guided; it is difficult to \nachieve higher accuracy later in the training \nperiod \nL2 Loss [13] No easy missed poles, easy to program and \nimplement Slow convergence; more sensitive to anomalies \nSmoothL1 Loss \n[14] \nImproved zero non-smoothing; more robust to \noutliers \nThe results are not fully equivalent to the actual \nIoU-based overlap \nBalancedL1 Loss \n[15] \nImproving the irrational distribution of \npositive and negative sample gradient \ncontributions \nHigher computational complexity, difficulty in \nhandling long-tailed distributions \nIoU-\nbased \nIoU Loss [16] Scale invariance; non-negative results Inability to accurately reflect the size of the \noverlap \nGIoU Loss [17] Consider the minimum outer rectangle of the \ntwo frames \nRelative positional relationships cannot be \nobjectively described; \nLarge error in a vertical direction, hard to \nconverge \nDIoU Loss [18] Consider the distance between the centers \nof the two frames \nAn aspect ratio of the detection frame not taken \ninto account \nCIoU Loss [18] Consider the aspect ratio of the detection \nframe \nThe true difference between width and \nheight, respectively, and their confidence \nscores are not taken into account \nEIoU Loss [19] \nConsider the true difference between width \nand height respectively and their \nconfidence scores \nHigh data labeling requirements and high \ncomputational complexity \n \nTo sum up, related works of VD in RS images mainly focus \non the improvement and innovation from multiple perspectives, \nsuch as multi-scale feature learning, improvement of detection \nmodels, design of R egression Loss function, etc. On the one \nhand, PVTv2 -b0 is introduced in this paper as a new feature \nextraction backbone to cope with  the limitations exposed by \nCNN. On the other hand, based on existing Regression Loss \nfunctions, we propose a kind of new loss function considering \nthe relevance of the bounding box variables and strengthen \nconstraints on bounding box shapes and positions, frame shape \nand position to strengthen the constraints of the function. \nIII. METHODOLOGY \nA. Network Optimization Based on Faster-Transformer  \nIn the whole process, feature extraction is one of the most \nimportant stages. Traditional models usually conduct CNN to \nextract feature information at different layers and levels. \nOriginal Faster R- CNN (with ResNet50 as backbone) [2, 20] \nusually leave over many  mis-detected/missing-detected \nsamples in the detection of vehicles based on high -resolution \nRS images . A lthough CNN models appear strong feature \nextraction capability, at the same time the global receptive field \nis relatively small, and may only understand the image by local \nfeatures. ResNet50 requires larger convolutional kernels and \ndeeper convolution to expand the receptive field and to obtain \nthe features of tiny objects, but this will lead to a decrease in the \nefficiency of computation and even fail to converge  after \nseveral training epochs. In addition, original CNN, to acquire \nlong-range correlated features of the data (which refers to \nfeatures that are spatially or temporally distant, but correlated, \nand involve a large area, usually covering m ultiple scales and \nlevels of information, and can provide a more comprehensive \nand rich characterization), has to occupy  larger convolution \nkernels together with deeper convolutions to expand the \nreceptive field, which will lead to a rise in model complexity  \nand also a decrease in efficiency or upper limit of convergence. \nWhile Transformer can utilize the learnable global weights \nprovided by the attention mechanism instead of the sliding \nwindow to obtain the long-range features of the global data. \nTherefore, this paper introduces PVTv2 -b\n0 [21] as a new \nfeature extraction network, which introduces a progressive \npyramid structure in Transformer architecture, assisted by FPN \nas sub -network for feature fusion and makes Transformer \nsuitable for VD in RS images by generating high- resolution \nmulti-scale feature maps.  \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \n \nFig. 2.    Network structure diagram of Faster-Transformer (PVTv2-b0).\na) Network Structure of Faster-Transformer (PVTv2-b0) \nThe framework of the VD model based on Faster -\nTransformer (PVTv2-b0) is shown in Fig. 2. It can be seen that \nthe whole network includes feature extraction, feature fusion, \ngeneration of proposals, classification and regression.  \nThe feature extraction part uses PVTv2-b0 as the backbone to \ngenerate muti-scale feature maps.  An input RGB image \n(512×512×3) undergoes feature extraction to yield four \ndifferently sized feature maps: C1, C2, C3, and C4, with different \nchannels, as depicted in the figure. C1, C2, C3, and C4 undergo \nconvolution layers to unify their channel dimension s to 256, \nresulting in C1’, C2’, C3’, and C 4’. Subsequently, C 4’ is \nunsampled bilinearly to obtain C4’’, which shares the same \ndimensions with C3’. C4’’ and C3’ are element-wise added to \nobtain the fused feature map C3’’. Similarly, operations on C3’’ \nand C2’ yield C2’’, and operations on C 2’’ and C1’ yield C1’’. \nC1’’, C2’’, and C 3’’ represent the fused features captured at \ndifferent layers, effectively utilizing multi-scale and multi-layer \nsemantic information to capture intricate details.  Then, P1, P2, \nP3, and P 4 are obtained through 3×3 convolutional processing \nbased on C 1’’, C2’’, C3’’, and C4’’, respectively. P5 is derived \nby bilinear down sampling of P4, reducing its resolution to \nextract coarse features and maintain the receptive field \nsimultaneously, thereby reducing computational complexity. \nBesides, we utilized Region Proposal Network (RPN) from \nFaster R-CNN. RPN adopts a strategy where it simultaneously \npredicts multiple a nchor boxes of different aspect ratios \ncovering various positions across the entire image. Using a \nsliding window approach on feature maps, RPN moves across \ndifferent positions  and performs convolutional operations  at \neach position, mapping the sliding window to a feature vector  \nwith low-dimension. This process allows RPN to predict both \npositive and negative information about the anchors, as well as \nthe coordinates of bound ing boxes. These predictions aid in \nfine-tuning the anchors.  \nLast but not least, we utilize  Cross-Entropy ( CE) Loss as \nClassification Loss to determine if the predicted anchor boxes \ncontain objects  and match them with correct categories  by \nprobability fractions. We also utilize DS Loss as Regression \nLoss to further refine the localization of predicted bounding \nboxes, ensuring they align with the corresponding ground truth \nbounding boxes as much as possible.  \n \nb) Discussion about PVTv2-b0 and ResNet50 \nPVTv2-b0 uses Multi-Head Self-Attention in Transformer as \nthe main means of multi -scale feature extraction, which can \nfully utilize the information of all layers and avoid the \ninformation independent of each other and the poor flow of \ninformation in traditional convolutional layers. In contrast, the \nconvolutional blocks used in ResNet50 are unable to effectively \nutilize the global information compared to Transformer, thus \nmaking it difficult to extract multi -scale features. In addition, \nthe key sampling strategy us ed in the PVTv2 -b\n0 network, the \nScale-Equalizing Pyramid, can control the ratio between feature \nmaps of different scales, thus improving the effectiveness of \nmulti-scale object detection. In contrast, ResNet50 does not set \na similar sampling strategy and fails to make full use of multi -\nscale features. \nDue to the small size of the tiny objects in RS images, more \ndetailed feature extraction is required. The PVTv2 -b0 network \nintroduces Slow Features as a key part, which utilizes the \nDilated Convolution technique for fixed-down sampling, which \neffectively preserves the resolution of the feature maps, thus \nbetter capturing the features of object vehicles. \nB. Principle of DS Loss \nPatch Emb\nEncoder\nStage 1\nC1\n128×128×32\nC1’\n128×128×256\nC1’’\n128×128×256\nC2\n64×64×64\nC2’\n64×64×256\nC2’’ \n64×64×256\nC2’’’\n128×128×256\nC3’’’\n64×64×256\nC3\n32×32×160\nC3’\n32×32×256\nC3’’ \n32×32×256\nC4’’\n32×32×256\nC4\n16×16×160\nC4’ \n16×16×256\nP1\n128×128×256\nP2\n64×64×256\nP3\n32×32×256\nP4\n16×16×256\nP5\n8×8×256\nPatch Emb\nEncoder\nStage 2\nPatch Emb\nEncoder\nStage 3\nPatch Emb\nEncoder\nStage 4\nProposals\nClassification\n(CE Loss)\nRegression\n(DS Loss)\n1×1 Conv\n(256 kernels)\n1×1 Conv\n(256 kernels)\n1×1 Conv\n(256 kernels)\n1×1 Conv\n(256 kernels)\n3×3 Conv 3×3 Conv 3×3 Conv\n1×1 Conv\n1×1 Conv\nclc\nreg\nElement-wise\nAdding\nElement-wise\nAdding\nElement-wise\nAdding\nElement-wise\nAdding\nElement-wise\nAdding\nElement-wise\nAdding\nElement-wise\nAdding\nPosition EmbeddingPosition Embedding Position Embedding\nPosition Embedding\nUp sampling\n(2×)\nUp sampling\n(2×)\nUp sampling\n(2×)\nDown sampling\n(÷2)\nRoI Pooling\nFully \nConnection\nFully \nConnection\nDetection Result\n(512×512×3, RGB Image \nwith Bounding Boxes marked)\nInput(512×512×3, RGB Image)\nFeature Extraction \n(PVTv2-b0)\nFeature Fusion (FPN)\nClassification and Regression\n3×3 Conv\nGeneration of Proposals\n(RPN)\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nIn this paper, we synthesize the advantages and \ndisadvantages of paradigm-based Loss functions and IoU-based \nLoss functions, and therefore propos e DS Loss for Bounding \nBox Regression. Considering the design structure of CIoU Loss, \nEIoU Loss, etc., the loss function designed in this paper adopts \nthe difference form based on length/width difference and area \ndifference and abandons the ratio form based on aspect ratio to \navoid the complex fractional form of loss, simplify the \nderivation process, and reduce the  difficulty of training. DS \nLoss proposes five constraints in terms of the length/width \ndifference, area difference, distance difference from the origin, \nand center distance, which further strengthens the constraints of \nBounding Box Regression in comparison with the single -\ncategory IoU Loss, improves the robustness and accelerates the \nconvergence efficiency. \nThe schematic diagram of the Bounding Box regression stage \nis shown in Fig. 3: the blue/red rectangular boxes in the figure \nrepresent the predicted bounding box/ground truth box, with G \nand B the geometric centroids of each. \n \nFig. 3.    Schematic of the geometric relationship between the \npredicted bounding box and the ground truth box. \nThe coordinate vectors of the predicted bounding box  and \nground truth box are set to be xb= (xb1, xb2, yb1, yb2) ，xg= (xg1, \nxg2, yg1, yg2), respectively. As a result, the width and height of \nthe two can be expressed by Formula (2) , and the coordinates \nof the geometric centers G and B of the predicted bounding box \nand the ground truth box are shown in Formula (3): \n                              \n21\n21\n21\n21\nbb b b\ngt g g\nbb b b\ngt g g\nwxx\nwxx\nhyy\nhyy\n= −\n = − = −\n = −\n                                      \n(2) \n                               \n( )\n( )\n( )\n( )\n12\n12\n12\n12\n0.5\n0.5\n0.5\n0.5\nB gg\nB bb\nG gg\nG gg\nx xx\ny yy\nx xx\ny yy\n = +\n\n = + = +\n = +\n.                                   (3) \nSetting the Euclidean Distance between points G  and B \ndenoted as d GB, the origin square distance and center  square \ndistance are calculated as Formula (4): \n                          \n( ) ( )\n222\n222\n222\nBBB\nGGG\nGB B G B G\ndxy\ndxy\nd xx yy\n = +\n = +\n =− +−\n. (4) \n \na) Constraints Δf1, Δf2 on Length/Width Scale \nThe length constraint Δ f1 and width constraint Δ f2 are \nexpressed as Formula (5) and (6): \n( ) ( )1 12 1,b b gt bb gtf x x w w kw∆= −                            (5) \n                        ( ) ( )2 12 2,b b gt bb gtf y y h h kh∆= − .                         (6) \nwhere k1, k2 are tiny object scale parameters, and w gt, hgt are \nconstant coefficients of balanced magnitude to make the \nconvergence of Loss more sensitive and effective.  \nΔf1, Δf2 constrain the length and width of the predicted \nbounding box, respectively. During the convergence process, \nΔf1 and Δf2 constantly normalize the shape of the predicted \nbounding box to approximate the ground truth box, and this \nnormalization is obtained by feeding the sum of the Losses \nwhich is gained by weighting the values of Δ f1 and Δf2 with \nother constraints back to the model for training. As shown in \nFig. 4. and Fig. 5., the shaded area reflects the desired trend of \nthis constraint for p redicted boundi ng box optimization, and \nthis trend indirectly reflects their contribution. \n \n(a) Before optimization \n \n \n(b) After optimization \nFig. 4.    Schematic diagram of the Δf1 optimization process. \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \n \n(a) Before optimization  \n \n \n (b) After optimization \nFig. 5.    Schematic diagram of the Δf2 optimization process. \n \nb) Constraint Δf3 on Area Scale \nThe area constraint Δf3 can be expressed as Formula (7):  \n \n( ) ( )3 1212 1 3,,,b b b b bb bb gt gtf x x y y w h kw h α∆= −           (7) \n \nwhere k3 is the scale parameter and α 1 is used to regulate the \norder of magnitude of the values. \nThe introduction of the area constraint Δ f3 strengthens the \ngeometric shape constraints and the correlation between the \nlength and width of the bounding box. Facing the experimental \nproblem that Δf1 converges less efficiently than Δ f2 (and vice \nversa) resulting in asynchronous convergence, Δ f3 accelerates \nthe convergence by equating to another slower converging Δ f1 \n(or Δf2). As shown in Fig. 6, the shaded area reflects the desired \ntrend of this constraint for Predicted Bounding Box \noptimization, and this trend indirectly reflects the contribution \nto loss function. \n \n(a) Before optimization \n \n(b) After optimization \nFig. 6.    Schematic diagram of the Δf3 optimization process. \n \nc)  Constraint Δf4 on Difference of Square Distance from the \nOrigin \nThe constraint Δf4 based on the difference of square distance \nfrom the origin can be expressed as Formula (8):  \n     ( ) ( )\n22\n4 1212 2,,,bb bb B Gfxx yy d d α∆= − .              (8) \nThe purpose of using the squared term is to obtain a high \nconvergence accuracy in a finite number of iterations to avoid \nthe emergence of fractional expressions in the Loss which slows \ndown the gradient descent. Δ f\n4 adjusts the predicted bounding \nbox position so that the IoU takes a great value. When the \ncoordinate points of the ground truth box  are all known and \nthere are wbb-wgt=λ1＞0，hbb-hgt=λ2＞0, let the cent er distance \ndGB=d0 be a smaller value of the distance difference after center \ndistance optimization, then the trajectory of point B constitutes \na circle with radius d0 centered on point G. Let all possible B i \npoints on this circular trajectory form a set, then there are: \n( ) ( )\n22 2\n0iiBG BGxx yy d−+− = .            (9)                  \nTherefore, in the optimization process based on Δf4, there \nexists a point \n0BB∈  and a constantλ0 such that when dGB=d0 \nand dB2-dG2=λ0 there is： \n\n0( ) () ,\niB G BG iIoU IoU B B−− ≥∈ .                   (10) \nwhere IoU(B0-G) represents the intersection and concatenation  \nratio of a predicted bounding box centered at B0 with a given \nground truth box, and IoU (Bi-G) represents the intersection and \nconcatenation ratio of a predicted bounding box centered at any \npoint in the set B with a given ground truth box. The process \nabove is shown in Fig. 7. \n \n(a) Before optimization  \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \n \n \n(b) After optimization \nFig. 7.    Schematic diagram of the Δf4 optimization process. \n \nBy symmetry, there are four points B 0i(i=1,2,3,4) that maximize \nIoU, which is symmetric about point G . Δf4 promotes that the \ngeometric centroids of both of them are on the same circular arc \nwith the origin as the center and the radius of d G, so that the \npredicted bounding box is easy to find a point near the circular \narc that maximizes the IoU no matter in which direction the \nconvergence occurs. \n \nd)  Constraint Δf\n5 on Square Distance of Geometric Centers \nΔf5 on center square distance is expressed as Formula (11):  \n( )\n2\n5 1212 3,,,b b b b GBfxx yy d α∆= .                       (11) \nΔf5 reflects the squared value of the distance between the \ncenter of the predicted bounding box and the center of the \nground truth box. Obviously, the smaller the value of the center \ndistance the closer the center of the predicted bounding box is \nto the center of the ground truth box, and the experiments reflect \nthe value of Δf5's contribution to the Loss before weighting by \nthe value of the center distance. It is worth noting that the \nconvergence of the center distance does not guarantee the \noverlap between t he predicted bounding box  and the ground \ntruth box, and it is difficult to reflect the degree of overlap when \nthere are special geometrical relationships (e.g., nesting, etc.) in \nthe predicted bounding box , which does not guarantee the \nimprovement of the IoU and the mAP. The positional \nrelationship between the predicted bounding box  and ground \ntruth box before and after Δf\n5 optimization is shown in Fig. 8. \n   \n(a)Before optimization \n \n \n(b)After optimization \nFig. 8.    Schematic diagram of the Δf5 optimization process. \n \ne) Final Expression of DS Loss and Object Function \nApart from Δf1~Δf5, Robust Estimator [22] F(x) is utilized \nto further improve the convergence accuracy and the immunity \nof the algorithm. The final expression of DS Loss is shown in \nFormula (12): \n( ) ( )\n5\n1212\n1\n,,,bbox DS b b b b i\ni\nLoss x x y y F f−\n=\n= ∆∑ .             (12) \nThe deviation vector between the p redicted bounding box  \nand the ground truth box during training is denoted as t = (tx, ty, \ntw, th). The elements in the deviation vector are calculated as \nshown in Formula (13):  \n( )\n( )\n( )\n( )\nln\nln\nx G B bb\ny G B bb\nw bb gt\nh bb gt\nt xxw\nt y yh\nt ww\nt hh\n= −\n = − =\n =\n.                           (13) \nAfter transforming the independent variables of DS Loss \nfrom (xb1, xb2, yb1, yb2) to ( tx, ty, tw, th), Δf1~Δf5 and the final \noptimization object function are shown in Formula (14) ~ (19): \n( ) ( )\n2\n11\nwt\nw gtft w e k∆= −                           (14) \n ( ) ( )\n2\n22\nht\nh gtft h e k∆= −                          (15) \n( ) ( )3 13, whtt\nw h gt gtf t t wh e k α +∆= −                    (16) \n( ) ( ) ( ) ( )\n22 22\n42 ,x y gt x G gt y G G Gf t t wt x ht y x yα ∆ = ++ +−+  (17) \n( ) ( ) ( )\n22\n53 ,x y gt x gt yf t t wt ht α ∆= +                        (18) \n( )\n** **\n,,,, , , arg min , , ,\nxywhx y w h bbox DS x y w httttt t t t Loss t t t t−\n =  . (19) \nIV. EXPERIMENTS   \nA. Evaluation Metrics, Basic Settings, and Datasets \nIn this paper, we employ the open- source PyTorch -based \nMMDetection [23] as the deep learning framework. During the \nexperiments, the stochastic gradient descent (SGD) solver with \na momentum of 0.9, weight decay of 0.0001, and a base \nlearning rate of 0.25% was used. The training is performed for \n50 epochs. All experiments are conducted on a machine \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nequipped with an i7-8700K CPU, Nvidia GeForce GTX 1080Ti \nGPU, and Cuda version 11.3. \nTABLE Ⅱ \nSUMMARY OF DATASET INFORMATION \n \nMain Information Training Dataset Validating / Testing \nDataset \nTotal images 3459 502 \nImage size 512×512 512×512 \nTotal objects (Small) 42847 6507 \nTotal objects (Large) 13876 7694 \nThe average number of \nobjects (Small) 12.39 12.96 \nThe average number of \nobjects (Large) 4.01 15.33 \nThe average number of \npixels/proportion (Small) 324.66 (0.12%) 375.06 (0.14%) \nThe average number of \npixels/proportion (Large) 1334.54 (0.51%) 1558.20 (0.59%) \n \n   \nFig. 9.    Training Set Image Examples. \n \n   \nFig. 10.    Validation/Test Set Image Examples. \nThe open -source DOTA dataset [ 24] is selected for the \nexperiments, with small/large vehicles being treated as tiny \nobjects for relevant analysis. We specified that both the \nvalidation set and test set consist of the same 502 images, and \nwe selected 3961 original images that were segmented into \n512×512 dimensions.  Basic information about the dataset is \npresented in Table  Ⅱ, and example images with g round truth \nbox annotations are shown in Fig. 9 (training dataset) and Fig. \n10 (validating/testing dataset) (blue/green boxes represent \nlarge/small vehicles, respectively). \nMoreover, the evaluation metrics employed include AP, \nmAP and F\n1-score. Precision and Recall are also considered, \nallowing for a more comprehensive assessment of classifier \nperformance. AP represents the area under the Precision-Recall \ncurve, with higher values indicating better  accuracy. For each \ntested category (small vehicle/large vehicle) , an AP value is \nobtained, reflecting the model's localization as well as \nclassification performance. By computing the average of AP \nvalues across all categories, mAP is derived, which signifies the \nmodel's general performance. The mAP value ranges from 0 to \n1, with higher values indicating better performance. F\n1-score, \non the other hand, as a comprehensive evaluation metric , \ncombines Precision and Recall as their harmonic average , and \nits specific formula is shown as: \n \n1 2 Precision RecallF P recision+Recall\n×= ×  .                      (20) \nThroughout this research, we conduct experiments on Robust \nEstimators, parameter ablation, constraint condition ablation, \nand comparison of Regression Loss functions.  \n \nB. Self-Ablation Studies \na) Ablation Experiments on Robust Estimators \nAccording to (12), when the Robust Estimator is expressed \nusing L\n1, L2, Smooth L1 and Balanced L1, the accuracies of the \nresults are shown in Table Ⅲ . Here, we set k1=k2=k3=1/3, \nα1=0.7, α2=1.2, and α3=2.0. \nTABLE Ⅲ \nEXPERIMENTAL COMPARISON OF ROBUST ESTIMATORS \n(Backbone=PVTv2-b0) \n \nRobust Estimator mAP AP(Small) AP(Large) F1(Small) F1(Large) \nBalanced L1 81.0% 84.6% 77.4% 72.8% 71.8% \nL1 80.6% 83.5% 77.6% 69.2% 70.8% \nL2 79.5% 83.0% 76.0% 65.0% 64.7% \nSmooth L1 80.7% 84.1% 77.3% 70.0% 71.7% \nTo enhance the model's robustness and ensure the resilience \nof the results against disturbances, it is crucial to select a Robust \nEstimator. We conduct a comparative experimental analysis of \nfour different Robust Estimators. Based on the results presented \nin Table Ⅲ, when using Balanced L 1 as the Robust Estimator, \nit outperforms L 1, L 2, and Smooth L 1 in terms of mAP, \nAP(Small), F 1(Small), and F 1(Large) metrics. Although \nBalanced L1 exhibits a slight decrease of 0.02% in AP(Large) \ncompared to L 1, its overall performance on PVTv2-b 0 is \nsignificantly better. Therefore, choosing Balanced  L 1 as the \nRobust Estimator for the DS Loss leads to higher precision in \nobject detection results. \n \nb) Ablation Experiments on Parameters \nBased on Formula (14) ~ (18), this paper requires conducting \nablation experiments to analyze the parameters introduced in \nthe DS Loss formula. The aim is to determine the optimal \nparameter combination that maximizes the comprehensive \nperformance of the experimental results. The conf iguration of \nthe ablation experiments and the corresponding result data are \nshown in Table Ⅳ. F\n1(Small) and F 1(Large) represent the F 1-\nscores of small and large vehicles, respectively. Based on \nprevious results, BalancedL1 is selected as the Robust Estimator \nfor DS Loss. \n \n \n(A)                                                 (B) \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \n \n(C)                                                 (D) \n \n \n(E)                                                   (F) \nFig. 11.    Results of the ablation experiments on DS Loss parameters \n(Backbone=PVTv2-b0). \nAs shown in Fig. 11, this paper conducts a total of six \nablation experiments, denoted as Group -A, B, C, D, E, and F, \nbased on the PVTv2-b0 model, to determine the optimal values \nof parameters α1, α2, α3, k1, k2, and k3 in the DS Loss function. \nTaking Group-A as an example, the curve variations of \nAP(Large) with different values of α 1 indicate that as α 1 \nincreases, the AP(Large) curve first decreases, then rises, and \nfinally undergoes slight fluctuations. The AP(Small) curve \nremains relatively stable throughout the variations, while the \nmAP curve shows minor fluctuations with an upward trend and \nreaches a local maximum when α\n1=0.7, corresponding to a local \nextremum in F 1-score as well. Therefore, choosing α1=0.7 as \nthe parameter value is considered favorable. \nSimilarly, analyzing the curve variations of the other five \nparameters, α2, α3, k1, k2, and k3, under different values, enables \nus to identify the corresponding favorable parameter values. \nAfter discussing and analyzing the values of all parameters, we \nconclude that setting the parameters for handling small -scale \nobjects as k 1=k2=k3=1/3 and simultaneously setting α1=0.7, \nα2=1.2, and α3=2.0, optimizes the detection performance. \n \nc) Ablation Experiments on Δf1 ~ Δf5 \nBased on the results of Experiments a)  and b), we set the \nparameters in the DS Loss formula as follows: α1=0.7, α2=1.2, \nα3=2.0, and k =1/3. Additionally, we choose Balanced L1 as the \nRobust Estimator for DS Loss. F 1(Small) and F 1(Large) \nrepresent the F1-scores of small and large vehicles, respectively. \nΔf1 to Δf5 denote the five constraint conditions in Formula (14) \nto (18). \nAs shown in Table Ⅳ , to demonstrate the interrelationships \namong the five constraint conditions in the DS Loss and their \nimpact on result accuracy, we conduct five sets of constraint \ncondition ablation experiments, with Group -1 as the standard \ngroup while Groups-2 to Groups-6 as the experimental groups.  \nTaking Group-1 and Group-3 as examples, when constraint \ncondition Δf 2 is removed, the values of mAP, AP(Small), \nAP(Large), F1(Small), and F1(Large) all experience a noticeable \ndecrease to varying degrees. Constraint condition Δf2 indirectly \nreflects the loss of width constraints on bounding boxes (as \nshown in Formula (6)), and the reduction in object detection \naccuracy upon removing Δf2 highlights its necessity. \nSimilarly, by comparing Group- 1 with the other four \nexperimental groups, we arrive at similar conclusions. In the \nend, it is evident that when all constraint conditions Δf 1 to Δf5 \nare present, the maximization of mean precision is achieved, \nindicating that each of these constraints is necessary. \nTABLE Ⅳ \nRESULTS OF CONSTRAINT CONDITION ABLATION EXPERIMENTS (Backbone=PVTv2-b0) \n \nGroups Δf1 Δf2 Δf3 Δf4 Δf5 mAP AP(Small) AP(Large) F1(Small) F1(Large) \n1 √ √ √ √ √ 81.0%  84.6% 77.4% 72.8% 71.8% \n2 × √ √ √ √ 80.7% 84.1% 77.4% 70.5% 71.1% \n3 √ × √ √ √ 80.5% 84.0% 77.1% 70.4% 70.7% \n4 √ √ × √ √ 80.5% 83.9% 77.0% 70.1% 70.7% \n5 √ √ √ × √ 80.8% 84.0% 77.5% 69.6% 71.3% \n6 √ √ √ √ × 74.5% 73.8% 75.2% 51.6% 57.0% \nC. Comparative Experiments on Different Loss Functions \nTo evaluate the superiority of DS Loss compared to existing \nloss functions, we conduct comparative studies based on the \nPVTv2-b0 backbone network using different loss functions. \nThe types of Regression Loss functions, namely L 1 Loss, L2 \nLoss, Smooth L1 Loss, and Balanced L1 Loss, along with their \ncorresponding experimental results, are presented in Table Ⅴ . \n(In this context, L1, L2, Smooth L1, and Balanced L1 are forms \nof Regression Loss, while IV. C represents robust estimators, \nwith same expressions but different variables for each.) \nBased on the data analysis, it is evident that when \nconducting experiments on PVTv2 -b0, the detection model \nwith DS Loss achieves higher AP and F 1-scores for small \nvehicles compared to all other participating loss functions. \nWhile for large vehicles, the AP and F 1-scores are slightly \nbelow those of IoU, GIoU, and DIoU loss functions by less \nthan 0.5%. \nNevertheless, DS Loss demonstrates an overall advantage. \nIn terms of more convincing metrics, the mAP and mF 1-score \nfor DS Loss reach 81.0% and 72.3%, respectively. Among all \nfunctions, DS Loss ranks first in mAP, and its mF 1-score is \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nonly 0.1% lower than that of DIoU Loss, ranking second. \nAdditionally, DS Loss also exhibits good performance in \nmIoU, with a value of 56.6%, ranking second among all \nfunctions, only 0.1% behind the top-ranked DIoU Loss. Based \non the comprehensive analysis above, on the PVTv2 -b0 \nbackbone network, DS Loss demonstrates certain superiority \nover other loss functions, particularly in the detection of small \nvehicles.\nTABLE Ⅴ \nRESULTS OF LOSS FUNCTION COMPARATIVE EXPERIMENTS \n \nRegression Loss mAP AP(S) AP(L) mF1-score F1(S) F1(L) mIoU \nDS Loss 81.0% 84.9% 77.3% 72.3% 72.8% 71.8% 56.6% \nL1 Loss 80.2% 83.8% 76.6% 68.5% 68.5% 68.5% 52.1% \nL2 Loss 80.1% 83.4% 76.8% 67.5% 67.5% 67.4% 50.9% \nSmooth L1 Loss 80.8% 84.5% 77.1% 70.8% 71.6% 70.0% 55.1% \nBalanced L1 Loss 80.5% 83.9% 77.0% 69.6% 70.6% 68.5% 53.4% \nIoU Loss 80.9% 84.3% 77.5% 72.0% 71.8% 72.2% 56.3% \nGIoU Loss 80.9% 84.3% 77.5% 71.6% 70.9% 72.2% 55.8% \nCIoU Loss 80.7% 84.2% 77.1% 71.4% 71.5% 71.2% 55.5% \nDIoU Loss 80.6% 84.1% 77.1% 72.4% 72.5% 72.3% 56.7% \n \nThe comparative results of different loss functions on the \nFaster Transformer model are shown in Fig. 12. The red boxes \nindicate cases of missed detection and false detection. Among \nall the loss functions, DS Loss stands out with fewer false \ndetections on the left column of cars (green box), illustrating its \nclear advantage in VD. Additionally, there are noticeable errors \nin the Bounding Box of large cars relative to the ground truth \nboxes in the right column (c) and (d), as evidenced by the \npresence of sev eral red boxes in the result images. Moreover, \ncompared to DS Loss, the results in the right column (e) ~(j) \nexhibit an additional false detection in the sixth -to-last row. In \nsummary, the result images generated by DS Loss show the best \nalignment with g round truth box es and achieve the highest \naccuracy. \n \n                 \n  (a) Ground Truth              (b) DS Loss                     (c) L1 Loss                      (d) L2 Loss              (e) Smooth L1 Loss  \n    \n                 \n                                       (f) Balanced L1                 (g) IoU Loss                            (h) GIoU Loss                 (i) DIoU Loss                 (j) CIoU Loss \nFig. 12.    Result Images of Different Loss Functions on Faster-Transformer Model. \n \nⅤ. DISCUSSION \nBased on the results of previous experiments, extended \ncomparative studies of feature extraction backbones and \ndetection models are also carried out in this section. \n \nA. Comparison of Different Backbones \nTo compare the detection accuracy exhibited by different \nfeature extractors , we select three common backbones here: \nSwin Transformers, ResNet50 , and HRNet, for comparative \nanalysis. We operate DS Loss as the Regression Loss for these \nexperiments, and the results are presented in Table Ⅵ. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nTABLE Ⅵ \nCOMPARATIVE EXPERIMENTAL RESULTS OF DIFFERENT BACKBONES \n \nBackbones mAP AP(S) AP(L) mF1-score F1(S) F1(L) mIoU \nPVTv2-b0 81.0% 84.6% 77.4% 72.3% 72.8% 71.8% 56.6% \nSwin Transformers [25] 73.7% 73.8% 73.7% 53.7% 53.4% 53.9% 36.7% \nResNet50 76.3% 81.0% 71.6% 61.7% 62.8% 60.6% 42.1% \nHRNet [26] 78.1% 77.0% 77.6% 74.3% 73.3% 75.3% 59.1% \n \nIt is evident that the result accuracy on the PVTv2 -b0 \nbackbone network is significantly higher than that of the Swin \nTransformer and ResNet50 backbone networks, demonstrating \na considerable advantage. Furthermore, PVTv2 -b0 achieves \nnotably high precision in mAP and AP for small vehicles, \nsurpassing HRNet by 2.9 and 7.6 percentage points, reaching \n81.0% and 84.6% respectively. Although other precision \nmetrics show a slight decrease compared to HRNet, the \nreduction remains within 4 percentage points. Due to F 1-score \nbeing a comprehensive indicator that avoids reliance on a single \nmaximum value, and IoU being used to reflect the correlation \nbetween ground truth and predictions, the superiority of \nPVTv2-b\n0 in mAP is sufficient to indicate its overall advantage \nover HRNet. \nAdditionally, Swin Transformers' extra local attention \nmechanism still fails to overcome the limitations of CNN, as it \ncannot allow interactions between  boxes. ResNet50's high \ncomputational complexity and spatial memory overhead, as \nwell as its localized convolution, restrict the field of view. \nHRNet requires alignment operations when handling multiple-\nresolution feature maps to ensure consistency in scale and \ngeometry among feature maps. This process can lead to \ninformation loss and error accumulation. These and similar \nreasons limit their accuracy in VD as backbones. \nIn conclusion, DS Loss exhibits the best result accuracy on \nthe PVTv2-b\n0 backbone network. \n \nB. Comparison of Different Models \nIn this paper, we select three  detection models: RetinaNet, \nYOLOv3, and FSAF as comparative models to validate the \nsuperiority of our proposed model —Faster-Transformer-DS. \nThe precision results of each model are presented in Table Ⅶ. \nTABLE Ⅶ \nCOMPARATIVE EXPERIMENTAL RESULTS OF DIFFERENT MODELS \n \nModels mAP AP(S) AP(L) mF1-score F1(S) F1(L) mIoU \nOurs 81.0% 84.6% 77.4% 72.3% 72.8% 71.8% 56.6% \nRetinaNet [27] 65.6% 70.8% 60.4% 20.6% 29.9% 11.2% 12.0% \nYOLOv3 [28] 66.6% 58.7% 74.6% 52.9% 50.9% 54.8% 36.0% \nFSAF [29] 80.6% 79.9% 81.3% 21.6% 24.8% 18.4% 12.1% \n \nBased on the comparative analysis from the figures and \ntables, our model shows significant advantages over RetinaNet \nand YOLOv3, with a substantial improvement of nearly 15 \npercentage points in mAP and AP for both small and large \nvehicles. Furthermore, there is an increase of over 20% in \nprecision metrics such as mF\n1-score, F1-score for both small and \nlarge vehicles, and mIoU. Compared to FSAF, our model \nachieves a remarkable increase of around 50 percentage points \nin F\n1-score and mIoU. However, there is also a decrease of 3.9 \npercentage points in AP for large vehicles, while there are \nimprovements of 0.4 and 4.7 percentage points in mAP and AP \nfor small vehicles, respectively. \nMoreover, RetinaNet exhibits error accumulation, possibly \ndue to the influence of certain samples (distant samples) \ncausing gradient explosion. YOLOv3 suffers from ineffective \nanchor allocation, which hampers the effective utilization of \ncertain corresponding scale feature layers due to the distribution \nof object box sizes. On the other hand, FSAF uses feature \nselection instead of traditional anchor -based object position \nprediction, but it may not capture enough distinct features for \ntiny objects (especiall y overlapping tiny objects), leading to \ninferior detection performance for tiny objects. \nIn conclusion, our model shows outstanding superiority over \nthe other three models, except for a 3.9 percentage points \ndecrease in AP for large vehicles compared to FSAF. For all \nother precision metrics, our model outperforms the other \nmodels significantly. \nVI.\n CONCLUSION \nIn VD tasks for RS images, some loss functions are usually \nused to measure the difference between the modeled predicted \nbounding box and the real ground truth box. In this paper, from \nthe perspective of simple Euclidean distance and IoU, the \ndegree of difference between borders is defined according to \nthe physical meaning of the borders, and the geometric and \ndistributional features of tiny objects in RS images are  fully \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nconsidered and utilized. And based on the five constraints of \nlength/width difference, area difference, origin distance \ndifference, and center distance as the constraints of Regression \nLoss, the DS Loss function is creatively proposed. On the other \nhand, for the problem of poor quality of feature information of \ntiny objects in RS images, this paper optimizes the Faster R-\nCNN model based on Transformers, adopts PVTv2- b0 as the \nfeature extraction network, and generates multi -scale feature \nmaps through its pyramid structure and attention mechanism, \nwhich enhances the feature extraction capability of the network.  \nIn addition, this paper selects Balanced L 1 function with the \nhighest integrated accuracy as Robust Estimator through robust \nestimation experiments and conducts parameter ablation \nexperiments on this basis to find the best parameter \ncombination of DS Loss and prove the indispensability of \nΔf\n1~Δf5. Finally, by comparing the experiments, evaluating the \nmetrics data, and visualization, t he results indicate that when \nconducting experiments on the backbone network PVTv2- b0, \nthe mAP of the DS Loss can reach 81.0%, which is significantly \nhigher than other loss functions used in the experiments. The \nvisual detection performance is also superior. Additionally, DS \nLoss exhibits significantly better overall accuracy on the \nPVTv2-b0 backbone network compared to ResNet50, Swin \nTransformers, and HRNet backbone networks and our whole \ndetection mode—Faster-Transformer-DS performs much better \nthan RetinaNet , YOLOv3 and FSAF models . It is also \ndemonstrated that DS Loss based on the PVTv2 -b0 backbone \nnetwork improves the detect ion accuracy of tiny objects in \nmany ways compared with the existing loss function. The \nexperimental dataset in this paper only detects two categories  \nof objects: large vehicles and small vehicles, and we will \nsubsequently conduct dataset enhancement tests, varying the \nproportion of different categories and their numbers in the \ntraining and test sets, to test the model’s generalization ability, \nand extending other datasets for more extensive experiments. \nAPPENDIX \nBased on  (14) ~ (19),  the constraints Δf 1~Δf3 based on \ngeometric shape are only related to the independent variables tw \nand th, while the constraints Δ f4, Δf5 based on the positional \nrelationship are only related to the independent variables tx and \nty. To prevent Δ f1~Δf5 from having too large an order of \nmagnitude gap during the iterative convergence process, ki, wgt \n/hgt, and α1~α3 are introduced. In addition, the reasons for \nintroducing the scale parameter ki(i=1,2,3) are discussed (as shown \nbelow in the parameter ablation experiments: \n① ki(i=1,2,3) are set to balance the values of Classification Loss \n(loss_cls) and Regression Loss (loss_bbox) to prevent the final \nweighted Loss from becoming unbalanced and thus diminishing \nthe usefulness of the Loss function for model optimization. As \nshown in Formula (21), \n                     \n12 clc bboxLoss w Loss w Loss=⋅ +⋅ .               (21) \n \nwhere w1, w2 represent the given loss weight. \n②Adjusting ki so that the value of the constant term in \nΔf1~Δf3 that is always positive decreases, ensures that the Loss \nis non-negative and that the training set loss ( loss_train) itself \nis not too small at the beginning of the convergence process, \nstrengthening the convergence tendency (because the expected \ntruth values of wbb, hbb are wgt, hgt and not kwgt, khgt.  \n \nGenerally, the goal of training a model with a training set is \nto verify that the smaller the loss (loss_val) of the set, the better. \nBut after loss_train  drops to a certain value, loss_val  starts to \nrise, i.e., overfitting). The ultimate goal of the above regression \ncorrection is to improve the degree of overlap between the \npredicted bounding box and the ground truth box based on IoU. \nTwo specific scenarios are analyzed below:  \n1) When Δf1 and Δf2 based on the geometric shape converge \nto higher accuracy, there are |Δ f1|=μ1 ，|Δf2|=μ2 ，μ1, μ2 ∈ \n(0,0.1). At this point, the length/width values of the predicted \nbounding box and ground truth box are close to each other, and \nthe area constraint  Δf3 can likewise converge to a higher \nprecision. Thus, Δf4 can promote the geometric centroids of the \ntwo to be on the same arc with the origin as the center and a \nradius of dG. Δf5 can promote the absolute distance between the \ngeometric centers of the two to be gradually reduced based on  \nco-circularity, and weaken the positional deviation of the \npredicted bounding box in comparison with the ground truth \nbox (as shown in Fig. 13(a)).  \n2) Similarly, when Δf 4 and Δ f5 based on the positional \nrelationship converge to higher accuracy, there are |Δ f4|=μ4，\n|Δf5|=μ5，μ4, μ5∈ (0,0.1). At this time, the geometric centers of \nthe Predicted bounding box and the ground truth box are closer \nto each other and are approximately co -circular. The \nlength/width of the predicted bounding box  itself has not \nconverged, and there is a large gap in the aspect ratio with the \nground truth box, resulting in a small IoU -based overlap \nbetween the two. Thus, the training  of the model focuses on \noptimizing the constraints Δf 1~Δf3, which drives the predicted \nbounding box to keep approaching the ground truth box in \ngeometric shape and reduces the redundant pixels in the \npredicted bounding box (as shown in Fig. 13(b)). \n   \n(a) Situation (1) \n \n(b) Situation (2)  \nFig. 13.    Schematic diagram of the two cases to be optimized. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nREFERENCES \n[1] Y. Liu, P. Sun, N. Wergeles, and Y. Shang, “A survey and performance \nevaluation of deep learning methods for small object detection ,” Expert \nSystems with Applications, vol.172, pp.114602, Jun. 2021.  \n[2] X. Chen, and A. Gupta, “An implementation of faster rcnn with study \nfor region sampling,” 2017, arXiv:1702.02138. \n[3] Z. Cai, and N. Vasconcelos,  “Cascade R -CNN: Delving into high \nquality object detection , “ in Proc. IEEE Conf. Comput. Vis. Pattern \nRecognit. Workshops, 2018, pp. 6154-6162. \n[4] H. Zhang, H. Chang, B. Ma, N. Wang, and X. Chen, “Dynamic R-CNN: \nTowards high quality object detection via dynamic training ,” \nin Computer Vision–ECCV 2020, vol.12360, 2020, pp. 260-275.  \n[5] J. Wang, C. Xu, W. Yang, and L. Yu, “A normalized gaussian \nwasserstein distance for tiny object detection,” 2021, arXiv: 2110.13389. \n[6] T. Lin al. , “Feature pyramid networks for object detection ,” in Proc. \nIEEE Conf. Comput. Vis. Pattern Recognit. 2017, pp. 936-944. \n[7] Q. Zheng, and Y. Chen , “Feature pyramid of bi- directional stepped \nconcatenation for small object detection ,” Multimedia Tools and \nApplications, vol. 80, no.13, May. 2021. \n[8] J. Lim al., “Small object detection using context and attention ,” 2021 \nInternational Conference on Artificial Intelligence in Information and \nCommunication (ICAIIC). 2021, pp.181-186. \n[9] S. Zhang al. , “Single-shot refinement neural network for object \ndetection,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. 2018, \npp.18-23. \n[10] Elfwing, Stefan, E. Uchibe, and K. Doya,  “Sigmoid-weighted linear \nunits for neural network function approximation in reinforcement \nlearning,” Neural Networks, vol.107, 2018, pp.3-11. \n[11] J. Cao al., “Maximum correntropy criterion -based hierarchical one-\nclass classification ,” in IEEE Transactions on Neural Networks and \nLearning Systems, vol. 32, no. 8, Aug. 2020, pp.3748-3754. \n[12] S. Montrul, “Convergent outcomes in L2 acquisition and L1 loss, “First \nlanguage attrition: Interdisciplinary perspectives on methodological, \n2004, pp.259-279. \n[13] C. P. Lee, and C. J. Lin, “A study on L\n2-loss (squared hinge -loss) \nmulticlass SVM ,” Neural computation , vol. 25, no. 5, 2013, pp.1302-\n1323. \n[14] R. Girshick, “Fast R -CNN,” in Proceedings of the IEEE international \nconference on computer vision, 2015, pp.1440-1448. \n[15] J. Pang, K. Chen, J. Shi, H. Feng, W. Ouyang, and D. Lin, “ Libra R-\nCNN: Towards Balanced Learning for Object Detection,” in Proc. IEEE \nConf. Comput. Vis. Pattern Recognit. 2019, pp. 821-830. \n[16] J. Yu, Y. Jiang, Z. Wang, Z. Cao, and T. Huang, “UnitBox: An \nAdvanced Object Detection Network,” in Proceedings of the 24th ACM \ninternational conference on Multimedia, 2016, pp. 516-520. \n[17] H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese, \n“Generalized Intersection over Union: A Metric and A Loss for \nBounding Box Regression,”  in Proc. IEEE/CVF Conf. Comput. Vis. \nPattern Recognit, 2019, pp. 15-20.  \n[18] Z. Zheng, P. Wang, W. Liu, J. Li, R. Ye, and D. Ren, “Distance-IoU loss: \nFaster and better learning for bounding box regression, “in Proceedings \nof the AAAI conference on artificial intelligence, vol. 34, no.7, 2020, pp. \n12993-13000.    \n[19] Y. F. Zhang, W. Ren, Z. Zhang, Z. Jia, L. Wang, and T. Tan, “Focal and \nEfficient IOU Loss for Accurate Bounding Box Regression ,” \nNeurocomputing, vol. 506, 2022, pp.146-157.    \n[20] D. Theckedath, and R. R. Sedamkar, “Detecting affect states using \nVGG16, ResNet50 and SE -ResNet50 networks ,” SN COMPUT. \nSCI. vol. 1, no. 79, 2020. \n[21] W. Wang al.,  “Pvt v2: Improved baselines with pyramid vision \ntransformer,” Computational Visual Media, vol. 8, no. 3, 2022, pp.415-\n424. \n[22] P. J. Huber, “Robust estimation of a location parameter,” Breakthroughs \nin statistics: Methodology and distribution, 1992, pp.492-518. \n[23] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, and D. Lin, \n“MMDetection: Open mmlab detection toolbox and benchmark ,” in  \nProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019. \n[24] G. Xia al. , “DOTA: A large- scale dataset for object detection in aerial \nimages,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018. \n[25] Z. Liu al., “Swin transformer: Hierarchical vision transformer using \nshifted windows ,” in Proc. IEEE/CVF international conference on \ncomputer vision., 2021. \n[26] C. Yu al., “Lite-HRNet: A lightweight high-resolution network,” in Proc. \nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021. \n[27] H. Zhang al., “Cascade RetinaNet: Maintaining consistency for single-\nstage object detection,” 2019, arXiv:1907.06881.  \n[28] J. Redmon, and A. Farhadi,  “Yolov3: An incremental improvement, ” \n2018, arXiv:1804.02767. \n[29] C. Zhu, Y. He, and M. Savvides, “Feature selective anchor-free module \nfor single-shot object detection,” in Proc. IEEE/CVF Conf. Comput. Vis. \nPattern Recognit., 2019, pp. 840-849. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3335283\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}