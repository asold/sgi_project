{
  "title": "Fine-tuned Language Models for Text Classification.",
  "url": "https://openalex.org/W2784121710",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2117853863",
      "name": "Jeremy Howard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2140581490",
      "name": "Sebastian Ruder",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2087511931",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W2949674892",
    "https://openalex.org/W2595304170",
    "https://openalex.org/W2144415203",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2072715695",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2609130030",
    "https://openalex.org/W2740711318",
    "https://openalex.org/W2740721704",
    "https://openalex.org/W2951670162",
    "https://openalex.org/W1948751323",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2062118960",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2510153535",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W2155541015",
    "https://openalex.org/W1614862348",
    "https://openalex.org/W2964054038",
    "https://openalex.org/W2749581528",
    "https://openalex.org/W189596042",
    "https://openalex.org/W2767434619",
    "https://openalex.org/W2523246573",
    "https://openalex.org/W3104240813",
    "https://openalex.org/W2163302275",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2110798204",
    "https://openalex.org/W2963908579",
    "https://openalex.org/W2606321545",
    "https://openalex.org/W1784932861",
    "https://openalex.org/W2949667497",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2610748790",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2951815801",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2989499211",
    "https://openalex.org/W2611248707",
    "https://openalex.org/W2756381707",
    "https://openalex.org/W2138857742",
    "https://openalex.org/W2556096924",
    "https://openalex.org/W3106003309",
    "https://openalex.org/W2518108298"
  ],
  "abstract": "Transfer learning has revolutionized computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Fine-tuned Language Models (FitLaM), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a state-of-the-art language model. Our method significantly outperforms the state-of-the-art on five text classification tasks, reducing the error by 18-24% on the majority of datasets. We open-source our pretrained models and code to enable adoption by the community.",
  "full_text": "Universal Language Model Fine-tuning for Text Classiﬁcation\nJeremy Howard∗\nfast.ai\nUniversity of San Francisco\nj@fast.ai\nSebastian Ruder∗\nInsight Centre, NUI Galway\nAylien Ltd., Dublin\nsebastian@ruder.io\nAbstract\nInductive transfer learning has greatly im-\npacted computer vision, but existing ap-\nproaches in NLP still require task-speciﬁc\nmodiﬁcations and training from scratch.\nWe propose Universal Language Model\nFine-tuning (ULMFiT), an effective trans-\nfer learning method that can be applied to\nany task in NLP, and introduce techniques\nthat are key for ﬁne-tuning a language\nmodel. Our method signiﬁcantly outper-\nforms the state-of-the-art on six text clas-\nsiﬁcation tasks, reducing the error by 18-\n24% on the majority of datasets. Further-\nmore, with only 100 labeled examples, it\nmatches the performance of training from\nscratch on 100×more data. We open-\nsource our pretrained models and code1.\n1 Introduction\nInductive transfer learning has had a large impact\non computer vision (CV). Applied CV models (in-\ncluding object detection, classiﬁcation, and seg-\nmentation) are rarely trained from scratch, but in-\nstead are ﬁne-tuned from models that have been\npretrained on ImageNet, MS-COCO, and other\ndatasets (Sharif Razavian et al., 2014; Long et al.,\n2015a; He et al., 2016; Huang et al., 2017).\nText classiﬁcation is a category of Natural Lan-\nguage Processing (NLP) tasks with real-world ap-\nplications such as spam, fraud, and bot detection\n(Jindal and Liu, 2007; Ngai et al., 2011; Chu et al.,\n2012), emergency response (Caragea et al., 2011),\nand commercial document classiﬁcation, such as\nfor legal discovery (Roitblat et al., 2010).\n1http://nlp.fast.ai/ulmfit.\n⋆Equal contribution. Jeremy focused on the algorithm de-\nvelopment and implementation, Sebastian focused on the ex-\nperiments and writing.\nWhile Deep Learning models have achieved\nstate-of-the-art on many NLP tasks, these models\nare trained from scratch, requiring large datasets,\nand days to converge. Research in NLP focused\nmostly on transductive transfer (Blitzer et al.,\n2007). For inductive transfer, ﬁne-tuning pre-\ntrained word embeddings (Mikolov et al., 2013),\na simple transfer technique that only targets a\nmodel’s ﬁrst layer, has had a large impact in prac-\ntice and is used in most state-of-the-art models.\nRecent approaches that concatenate embeddings\nderived from other tasks with the input at different\nlayers (Peters et al., 2017; McCann et al., 2017;\nPeters et al., 2018) still train the main task model\nfrom scratch and treat pretrained embeddings as\nﬁxed parameters, limiting their usefulness.\nIn light of the beneﬁts of pretraining (Erhan\net al., 2010), we should be able to do better than\nrandomly initializing the remaining parameters of\nour models. However, inductive transfer via ﬁne-\ntuning has been unsuccessful for NLP (Mou et al.,\n2016). Dai and Le (2015) ﬁrst proposed ﬁne-\ntuning a language model (LM) but require millions\nof in-domain documents to achieve good perfor-\nmance, which severely limits its applicability.\nWe show that not the idea of LM ﬁne-tuning but\nour lack of knowledge of how to train them ef-\nfectively has been hindering wider adoption. LMs\noverﬁt to small datasets and suffered catastrophic\nforgetting when ﬁne-tuned with a classiﬁer. Com-\npared to CV , NLP models are typically more shal-\nlow and thus require different ﬁne-tuning methods.\nWe propose a new method, Universal Language\nModel Fine-tuning (ULMFiT) that addresses these\nissues and enables robust inductive transfer learn-\ning for any NLP task, akin to ﬁne-tuning ImageNet\nmodels: The same 3-layer LSTM architecture—\nwith the same hyperparameters and no addi-\ntions other than tuned dropout hyperparameters—\noutperforms highly engineered models and trans-\narXiv:1801.06146v5  [cs.CL]  23 May 2018\nfer learning approaches on six widely studied text\nclassiﬁcation tasks. On IMDb, with 100 labeled\nexamples, ULMFiT matches the performance of\ntraining from scratch with 10×and—given 50k\nunlabeled examples—with 100×more data.\nContributions Our contributions are the follow-\ning: 1) We propose Universal Language Model\nFine-tuning (ULMFiT), a method that can be used\nto achieve CV-like transfer learning for any task\nfor NLP. 2) We proposediscriminative ﬁne-tuning,\nslanted triangular learning rates , and gradual\nunfreezing, novel techniques to retain previous\nknowledge and avoid catastrophic forgetting dur-\ning ﬁne-tuning. 3) We signiﬁcantly outperform the\nstate-of-the-art on six representative text classiﬁ-\ncation datasets, with an error reduction of 18-24%\non the majority of datasets. 4) We show that our\nmethod enables extremely sample-efﬁcient trans-\nfer learning and perform an extensive ablation\nanalysis. 5) We make the pretrained models and\nour code available to enable wider adoption.\n2 Related work\nTransfer learning in CV Features in deep neu-\nral networks in CV have been observed to tran-\nsition from general to task-speciﬁc from the ﬁrst\nto the last layer (Yosinski et al., 2014). For this\nreason, most work in CV focuses on transferring\nthe ﬁrst layers of the model (Long et al., 2015b).\nSharif Razavian et al. (2014) achieve state-of-the-\nart results using features of an ImageNet model as\ninput to a simple classiﬁer. In recent years, this\napproach has been superseded by ﬁne-tuning ei-\nther the last (Donahue et al., 2014) or several of\nthe last layers of a pretrained model and leaving\nthe remaining layers frozen (Long et al., 2015a).\nHypercolumns In NLP, only recently have\nmethods been proposed that go beyond transfer-\nring word embeddings. The prevailing approach\nis to pretrain embeddings that capture additional\ncontext via other tasks. Embeddings at different\nlevels are then used as features, concatenated ei-\nther with the word embeddings or with the in-\nputs at intermediate layers. This method is known\nas hypercolumns (Hariharan et al., 2015) in CV 2\nand is used by Peters et al. (2017), Peters et al.\n(2018), Wieting and Gimpel (2017), Conneau\n2A hypercolumn at a pixel in CV is the vector of activa-\ntions of all CNN units above that pixel. In analogy, a hyper-\ncolumn for a word or sentence in NLP is the concatenation of\nembeddings at different layers in a pretrained model.\net al. (2017), and McCann et al. (2017) who use\nlanguage modeling, paraphrasing, entailment, and\nMachine Translation (MT) respectively for pre-\ntraining. Speciﬁcally, Peters et al. (2018) require\nengineered custom architectures, while we show\nstate-of-the-art performance with the same basic\narchitecture across a range of tasks. In CV , hyper-\ncolumns have been nearly entirely superseded by\nend-to-end ﬁne-tuning (Long et al., 2015a).\nMulti-task learning A related direction is\nmulti-task learning (MTL) (Caruana, 1993). This\nis the approach taken by Rei (2017) and Liu et al.\n(2018) who add a language modeling objective\nto the model that is trained jointly with the main\ntask model. MTL requires the tasks to be trained\nfrom scratch every time, which makes it inefﬁcient\nand often requires careful weighting of the task-\nspeciﬁc objective functions (Chen et al., 2017).\nFine-tuning Fine-tuning has been used success-\nfully to transfer between similar tasks, e.g. in QA\n(Min et al., 2017), for distantly supervised senti-\nment analysis (Severyn and Moschitti, 2015), or\nMT domains (Sennrich et al., 2015) but has been\nshown to fail between unrelated ones (Mou et al.,\n2016). Dai and Le (2015) also ﬁne-tune a lan-\nguage model, but overﬁt with 10k labeled exam-\nples and require millions of in-domain documents\nfor good performance. In contrast, ULMFiT lever-\nages general-domain pretraining and novel ﬁne-\ntuning techniques to prevent overﬁtting even with\nonly 100 labeled examples and achieves state-of-\nthe-art results also on small datasets.\n3 Universal Language Model Fine-tuning\nWe are interested in the most general inductive\ntransfer learning setting for NLP (Pan and Yang,\n2010): Given a static source task TS and any tar-\nget task TT with TS ̸= TT, we would like to im-\nprove performance on TT. Language modeling\ncan be seen as the ideal source task and a counter-\npart of ImageNet for NLP: It captures many facets\nof language relevant for downstream tasks, such as\nlong-term dependencies (Linzen et al., 2016), hi-\nerarchical relations (Gulordava et al., 2018), and\nsentiment (Radford et al., 2017). In contrast to\ntasks like MT (McCann et al., 2017) and entail-\nment (Conneau et al., 2017), it provides data in\nnear-unlimited quantities for most domains and\nlanguages. Additionally, a pretrained LM can be\neasily adapted to the idiosyncrasies of a target\n13/02/2018 ulmﬁt_pretraining.html\n1/1\ndollarThe gold or\nEmbeddinglayer\nLayer 1\nLayer 2\nLayer 3\nSoftmaxlayer\ngold\n(a) LM pre-training\n13/02/2018 ulmﬁt_lm_ﬁne-tuning.html\n1/1\nsceneThe best ever\nEmbeddinglayer\nLayer 1\nLayer 2\nLayer 3\nSoftmaxlayer (b) LM ﬁne-tuning\n13/02/2018 ulmﬁt_clas_ﬁne-tuning.html\n1/1\nsceneThe best ever\nEmbeddinglayer\nLayer 1\nLayer 2\nLayer 3\nSoftmaxlayer (c) Classiﬁer ﬁne-tuning\nFigure 1: ULMFiT consists of three stages: a) The LM is trained on a general-domain corpus to capture\ngeneral features of the language in different layers. b) The full LM is ﬁne-tuned on target task data using\ndiscriminative ﬁne-tuning (‘Discr’) and slanted triangular learning rates (STLR) to learn task-speciﬁc\nfeatures. c) The classiﬁer is ﬁne-tuned on the target task using gradual unfreezing, ‘Discr’, and STLR to\npreserve low-level representations and adapt high-level ones (shaded: unfreezing stages; black: frozen).\ntask, which we show signiﬁcantly improves per-\nformance (see Section 5). Moreover, language\nmodeling already is a key component of existing\ntasks such as MT and dialogue modeling. For-\nmally, language modeling induces a hypothesis\nspace Hthat should be useful for many other NLP\ntasks (Vapnik and Kotz, 1982; Baxter, 2000).\nWe propose Universal Language Model Fine-\ntuning (ULMFiT), which pretrains a language\nmodel (LM) on a large general-domain corpus and\nﬁne-tunes it on the target task using novel tech-\nniques. The method is universal in the sense that\nit meets these practical criteria: 1) It works across\ntasks varying in document size, number, and label\ntype; 2) it uses a single architecture and training\nprocess; 3) it requires no custom feature engineer-\ning or preprocessing; and 4) it does not require ad-\nditional in-domain documents or labels.\nIn our experiments, we use the state-of-the-\nart language model AWD-LSTM (Merity et al.,\n2017a), a regular LSTM (with no attention,\nshort-cut connections, or other sophisticated ad-\nditions) with various tuned dropout hyperparame-\nters. Analogous to CV , we expect that downstream\nperformance can be improved by using higher-\nperformance language models in the future.\nULMFiT consists of the following steps, which\nwe show in Figure 1: a) General-domain LM\npretraining ( §3.1); b) target task LM ﬁne-tuning\n(§3.2); and c) target task classiﬁer ﬁne-tuning\n(§3.3). We discuss these in the following sections.\n3.1 General-domain LM pretraining\nAn ImageNet-like corpus for language should be\nlarge and capture general properties of language.\nWe pretrain the language model on Wikitext-103\n(Merity et al., 2017b) consisting of 28,595 prepro-\ncessed Wikipedia articles and 103 million words.\nPretraining is most beneﬁcial for tasks with small\ndatasets and enables generalization even with 100\nlabeled examples. We leave the exploration of\nmore diverse pretraining corpora to future work,\nbut expect that they would boost performance.\nWhile this stage is the most expensive, it only\nneeds to be performed once and improves perfor-\nmance and convergence of downstream models.\n3.2 Target task LM ﬁne-tuning\nNo matter how diverse the general-domain data\nused for pretraining is, the data of the target task\nwill likely come from a different distribution. We\nthus ﬁne-tune the LM on data of the target task.\nGiven a pretrained general-domain LM, this stage\nconverges faster as it only needs to adapt to the id-\niosyncrasies of the target data, and it allows us to\ntrain a robust LM even for small datasets. We pro-\npose discriminative ﬁne-tuning and slanted trian-\ngular learning rates for ﬁne-tuning the LM, which\nwe introduce in the following.\nDiscriminative ﬁne-tuning As different layers\ncapture different types of information (Yosinski\net al., 2014), they should be ﬁne-tuned to differ-\nent extents. To this end, we propose a novel ﬁne-\ntuning method, discriminative ﬁne-tuning3.\nInstead of using the same learning rate for all\nlayers of the model, discriminative ﬁne-tuning al-\nlows us to tune each layer with different learning\nrates. For context, the regular stochastic gradient\ndescent (SGD) update of a model’s parametersθat\ntime step tlooks like the following (Ruder, 2016):\nθt = θt−1 −η·∇θJ(θ) (1)\nwhere ηis the learning rate and∇θJ(θ) is the gra-\ndient with regard to the model’s objective func-\ntion. For discriminative ﬁne-tuning, we split the\nparameters θinto {θ1,...,θ L}where θl contains\nthe parameters of the model at the l-th layer and\nLis the number of layers of the model. Similarly,\nwe obtain {η1,...,η L}where ηl is the learning\nrate of the l-th layer.\nThe SGD update with discriminative ﬁne-\ntuning is then the following:\nθl\nt = θl\nt−1 −ηl ·∇θlJ(θ) (2)\nWe empirically found it to work well to ﬁrst\nchoose the learning rate ηL of the last layer by\nﬁne-tuning only the last layer and using ηl−1 =\nηl/2.6 as the learning rate for lower layers.\nSlanted triangular learning rates For adapting\nits parameters to task-speciﬁc features, we would\nlike the model to quickly converge to a suitable\nregion of the parameter space in the beginning\nof training and then reﬁne its parameters. Using\nthe same learning rate (LR) or an annealed learn-\ning rate throughout training is not the best way\nto achieve this behaviour. Instead, we propose\nslanted triangular learning rates (STLR), which\nﬁrst linearly increases the learning rate and then\nlinearly decays it according to the following up-\ndate schedule, which can be seen in Figure 2:\ncut= ⌊T ·cut frac⌋\np=\n{\nt/cut, if t<cut\n1 − t−cut\ncut·(1/cut frac−1) , otherwise\nηt = ηmax ·1 +p·(ratio−1)\nratio\n(3)\nwhere T is the number of training iterations 4,\ncut frac is the fraction of iterations we increase\n3 An unrelated method of the same name exists for deep\nBoltzmann machines (Salakhutdinov and Hinton, 2009).\n4In other words, the number of epochs times the number\nof updates per epoch.\nthe LR, cutis the iteration when we switch from\nincreasing to decreasing the LR,pis the fraction of\nthe number of iterations we have increased or will\ndecrease the LR respectively, ratiospeciﬁes how\nmuch smaller the lowest LR is from the maximum\nLR ηmax, and ηt is the learning rate at iteration t.\nWe generally usecut frac = 0.1, ratio= 32and\nηmax = 0.01.\nSTLR modiﬁes triangular learning rates (Smith,\n2017) with a short increase and a long decay pe-\nriod, which we found key for good performance. 5\nIn Section 5, we compare against aggressive co-\nsine annealing, a similar schedule that has recently\nbeen used to achieve state-of-the-art performance\nin CV (Loshchilov and Hutter, 2017).6\nFigure 2: The slanted triangular learning rate\nschedule used for ULMFiT as a function of the\nnumber of training iterations.\n3.3 Target task classiﬁer ﬁne-tuning\nFinally, for ﬁne-tuning the classiﬁer, we augment\nthe pretrained language model with two additional\nlinear blocks. Following standard practice for\nCV classiﬁers, each block uses batch normaliza-\ntion (Ioffe and Szegedy, 2015) and dropout, with\nReLU activations for the intermediate layer and a\nsoftmax activation that outputs a probability dis-\ntribution over target classes at the last layer. Note\nthat the parameters in these task-speciﬁc classi-\nﬁer layers are the only ones that are learned from\nscratch. The ﬁrst linear layer takes as the input the\npooled last hidden layer states.\nConcat pooling The signal in text classiﬁcation\ntasks is often contained in a few words, which may\n5We also credit personal communication with the author.\n6While Loshchilov and Hutter (2017) use multiple anneal-\ning cycles, we generally found one cycle to work best.\noccur anywhere in the document. As input docu-\nments can consist of hundreds of words, informa-\ntion may get lost if we only consider the last hid-\nden state of the model. For this reason, we con-\ncatenate the hidden state at the last time step hT\nof the document with both the max-pooled and the\nmean-pooled representation of the hidden states\nover as many time steps as ﬁt in GPU memory\nH = {h1,..., hT}:\nhc = [hT,maxpool(H),meanpool(H)] (4)\nwhere [] is concatenation.\nFine-tuning the target classiﬁer is the most crit-\nical part of the transfer learning method. Overly\naggressive ﬁne-tuning will cause catastrophic for-\ngetting, eliminating the beneﬁt of the information\ncaptured through language modeling; too cautious\nﬁne-tuning will lead to slow convergence (and re-\nsultant overﬁtting). Besides discriminative ﬁne-\ntuning and triangular learning rates, we propose\ngradual unfreezing for ﬁne-tuning the classiﬁer.\nGradual unfreezing Rather than ﬁne-tuning all\nlayers at once, which risks catastrophic forgetting,\nwe propose to gradually unfreeze the model start-\ning from the last layer as this contains the least\ngeneral knowledge (Yosinski et al., 2014): We\nﬁrst unfreeze the last layer and ﬁne-tune all un-\nfrozen layers for one epoch. We then unfreeze the\nnext lower frozen layer and repeat, until we ﬁne-\ntune all layers until convergence at the last itera-\ntion. This is similar to ‘ chain-thaw’ (Felbo et al.,\n2017), except that we add a layer at a time to the\nset of ‘thawed’ layers, rather than only training a\nsingle layer at a time.\nWhile discriminative ﬁne-tuning, slanted trian-\ngular learning rates, and gradual unfreezing all\nare beneﬁcial on their own, we show in Section\n5 that they complement each other and enable our\nmethod to perform well across diverse datasets.\nBPTT for Text Classiﬁcation (BPT3C) Lan-\nguage models are trained with backpropagation\nthrough time (BPTT) to enable gradient propa-\ngation for large input sequences. In order to\nmake ﬁne-tuning a classiﬁer for large documents\nfeasible, we propose BPTT for Text Classiﬁca-\ntion (BPT3C): We divide the document into ﬁxed-\nlength batches of size b. At the beginning of each\nbatch, the model is initialized with the ﬁnal state\nof the previous batch; we keep track of the hid-\nden states for mean and max-pooling; gradients\nDataset Type # classes # examples\nTREC-6 Question 6 5.5k\nIMDb Sentiment 2 25k\nYelp-bi Sentiment 2 560k\nYelp-full Sentiment 5 650k\nAG Topic 4 120k\nDBpedia Topic 14 560k\nTable 1: Text classiﬁcation datasets and tasks with\nnumber of classes and training examples.\nare back-propagated to the batches whose hidden\nstates contributed to the ﬁnal prediction. In prac-\ntice, we use variable length backpropagation se-\nquences (Merity et al., 2017a).\nBidirectional language model Similar to exist-\ning work (Peters et al., 2017, 2018), we are not\nlimited to ﬁne-tuning a unidirectional language\nmodel. For all our experiments, we pretrain both a\nforward and a backward LM. We ﬁne-tune a clas-\nsiﬁer for each LM independently using BPT3C\nand average the classiﬁer predictions.\n4 Experiments\nWhile our approach is equally applicable to se-\nquence labeling tasks, we focus on text classiﬁca-\ntion tasks in this work due to their important real-\nworld applications.\n4.1 Experimental setup\nDatasets and tasks We evaluate our method on\nsix widely-studied datasets, with varying numbers\nof documents and varying document length, used\nby state-of-the-art text classiﬁcation and transfer\nlearning approaches (Johnson and Zhang, 2017;\nMcCann et al., 2017) as instances of three com-\nmon text classiﬁcation tasks: sentiment analy-\nsis, question classiﬁcation, and topic classiﬁca-\ntion. We show the statistics for each dataset and\ntask in Table 1.\nSentiment Analysis For sentiment analysis, we\nevaluate our approach on the binary movie review\nIMDb dataset (Maas et al., 2011) and on the binary\nand ﬁve-class version of the Yelp review dataset\ncompiled by Zhang et al. (2015).\nQuestion Classiﬁcation We use the six-class\nversion of the small TREC dataset (V oorhees and\nTice, 1999) dataset of open-domain, fact-based\nquestions divided into broad semantic categories.\nModel Test Model TestIMDb\nCoVe (McCann et al., 2017) 8.2\nTREC-6\nCoVe (McCann et al., 2017) 4.2\noh-LSTM (Johnson and Zhang, 2016) 5.9 TBCNN (Mou et al., 2015) 4.0\nVirtual (Miyato et al., 2016) 5.9 LSTM-CNN (Zhou et al., 2016) 3.9\nULMFiT (ours) 4.6 ULMFiT (ours) 3.6\nTable 2: Test error rates (%) on two text classiﬁcation datasets used by McCann et al. (2017).\nAG DBpedia Yelp-bi Yelp-full\nChar-level CNN (Zhang et al., 2015) 9.51 1.55 4.88 37.95\nCNN (Johnson and Zhang, 2016) 6.57 0.84 2.90 32.39\nDPCNN (Johnson and Zhang, 2017) 6.87 0.88 2.64 30.58\nULMFiT (ours) 5.01 0.80 2.16 29.98\nTable 3: Test error rates (%) on text classiﬁcation datasets used by Johnson and Zhang (2017).\nTopic classiﬁcation For topic classiﬁcation, we\nevaluate on the large-scale AG news and DBpedia\nontology datasets created by Zhang et al. (2015).\nPre-processing We use the same pre-processing\nas in earlier work (Johnson and Zhang, 2017; Mc-\nCann et al., 2017). In addition, to allow the lan-\nguage model to capture aspects that might be rel-\nevant for classiﬁcation, we add special tokens for\nupper-case words, elongation, and repetition.\nHyperparameters We are interested in a model\nthat performs robustly across a diverse set of tasks.\nTo this end, if not mentioned otherwise, we use the\nsame set of hyperparameters across tasks, which\nwe tune on the IMDb validation set. We use\nthe AWD-LSTM language model (Merity et al.,\n2017a) with an embedding size of 400, 3 layers,\n1150 hidden activations per layer, and a BPTT\nbatch size of 70. We apply dropout of 0.4 to\nlayers, 0.3 to RNN layers, 0.4 to input embed-\nding layers, 0.05 to embedding layers, and weight\ndropout of 0.5 to the RNN hidden-to-hidden ma-\ntrix. The classiﬁer has a hidden layer of size 50.\nWe use Adam with β1 = 0.7 instead of the de-\nfault β1 = 0.9 and β2 = 0.99, similar to (Dozat\nand Manning, 2017). We use a batch size of 64,\na base learning rate of 0.004 and 0.01 for ﬁne-\ntuning the LM and the classiﬁer respectively, and\ntune the number of epochs on the validation set of\neach task7. We otherwise use the same practices\n7On small datasets such as TREC-6, we ﬁne-tune the LM\nonly for 15 epochs without overﬁtting, while we can ﬁne-tune\nlonger on larger datasets. We found 50 epochs to be a good\ndefault for ﬁne-tuning the classiﬁer.\nused in (Merity et al., 2017a).\nBaselines and comparison models For each\ntask, we compare against the current state-of-the-\nart. For the IMDb and TREC-6 datasets, we com-\npare against CoVe (McCann et al., 2017), a state-\nof-the-art transfer learning method for NLP. For\nthe AG, Yelp, and DBpedia datasets, we com-\npare against the state-of-the-art text categorization\nmethod by Johnson and Zhang (2017).\n4.2 Results\nFor consistency, we report all results as error rates\n(lower is better). We show the test error rates\non the IMDb and TREC-6 datasets used by Mc-\nCann et al. (2017) in Table 2. Our method outper-\nforms both CoVe, a state-of-the-art transfer learn-\ning method based on hypercolumns, as well as the\nstate-of-the-art on both datasets. On IMDb, we\nreduce the error dramatically by 43.9% and 22%\nwith regard to CoVe and the state-of-the-art re-\nspectively. This is promising as the existing state-\nof-the-art requires complex architectures (Peters\net al., 2018), multiple forms of attention (McCann\net al., 2017) and sophisticated embedding schemes\n(Johnson and Zhang, 2016), while our method em-\nploys a regular LSTM with dropout. We note\nthat the language model ﬁne-tuning approach of\nDai and Le (2015) only achieves an error of 7.64\nvs. 4.6 for our method on IMDb, demonstrating\nthe beneﬁt of transferring knowledge from a large\nImageNet-like corpus using our ﬁne-tuning tech-\nniques. IMDb in particular is reﬂective of real-\nworld datasets: Its documents are generally a few\nFigure 3: Validation error rates for supervised and semi-supervised ULMFiT vs. training from scratch\nwith different numbers of training examples on IMDb, TREC-6, and AG (from left to right).\nparagraphs long—similar to emails (e.g for legal\ndiscovery) and online comments (e.g for commu-\nnity management); and sentiment analysis is simi-\nlar to many commercial applications, e.g. product\nresponse tracking and support email routing.\nOn TREC-6, our improvement—similar as the\nimprovements of state-of-the-art approaches—is\nnot statistically signiﬁcant, due to the small size of\nthe 500-examples test set. Nevertheless, the com-\npetitive performance on TREC-6 demonstrates\nthat our model performs well across different\ndataset sizes and can deal with examples that range\nfrom single sentences—in the case of TREC-6—\nto several paragraphs for IMDb. Note that despite\npretraining on more than two orders of magnitude\nless data than the 7 million sentence pairs used by\nMcCann et al. (2017), we consistently outperform\ntheir approach on both datasets.\nWe show the test error rates on the larger AG,\nDBpedia, Yelp-bi, and Yelp-full datasets in Table\n3. Our method again outperforms the state-of-\nthe-art signiﬁcantly. On AG, we observe a simi-\nlarly dramatic error reduction by 23.7% compared\nto the state-of-the-art. On DBpedia, Yelp-bi, and\nYelp-full, we reduce the error by 4.8%, 18.2%,\n2.0% respectively.\n5 Analysis\nIn order to assess the impact of each contribution,\nwe perform a series of analyses and ablations. We\nrun experiments on three corpora, IMDb, TREC-\n6, and AG that are representative of different tasks,\ngenres, and sizes. For all experiments, we split off\n10% of the training set and report error rates on\nthis validation set with unidirectional LMs. We\nﬁne-tune the classiﬁer for 50 epochs and train all\nmethods but ULMFiT with early stopping.\nLow-shot learning One of the main beneﬁts of\ntransfer learning is being able to train a model for\nPretraining IMDb TREC-6 AG\nWithout pretraining 5.63 10.67 5.52\nWith pretraining 5.00 5.69 5.38\nTable 4: Validation error rates for ULMFiT with\nand without pretraining.\na task with a small number of labels. We evalu-\nate ULMFiT on different numbers of labeled ex-\namples in two settings: only labeled examples are\nused for LM ﬁne-tuning (‘ supervised’); and all\ntask data is available and can be used to ﬁne-tune\nthe LM (‘semi-supervised’). We compare ULM-\nFiT to training from scratch—which is necessary\nfor hypercolumn-based approaches. We split off\nbalanced fractions of the training data, keep the\nvalidation set ﬁxed, and use the same hyperparam-\neters as before. We show the results in Figure 3.\nOn IMDb and AG, supervised ULMFiT with\nonly 100 labeled examples matches the perfor-\nmance of training from scratch with 10×and 20×\nmore data respectively, clearly demonstrating the\nbeneﬁt of general-domain LM pretraining. If we\nallow ULMFiT to also utilize unlabeled exam-\nples (50k for IMDb, 100k for AG), at 100 labeled\nexamples, we match the performance of training\nfrom scratch with50×and 100×more data on AG\nand IMDb respectively. On TREC-6, ULMFiT\nsigniﬁcantly improves upon training from scratch;\nas examples are shorter and fewer, supervised and\nsemi-supervised ULMFiT achieve similar results.\nImpact of pretraining We compare using no\npretraining with pretraining on WikiText-103\n(Merity et al., 2017b) in Table 4. Pretraining is\nmost useful for small and medium-sized datasets,\nwhich are most common in commercial applica-\ntions. However, even for large datasets, pretrain-\ning improves performance.\nLM IMDb TREC-6 AG\nVanilla LM 5.98 7.41 5.76\nAWD-LSTM LM 5.00 5.69 5.38\nTable 5: Validation error rates for ULMFiT with a\nvanilla LM and the AWD-LSTM LM.\nLM ﬁne-tuning IMDb TREC-6 AG\nNo LM ﬁne-tuning 6.99 6.38 6.09\nFull 5.86 6.54 5.61\nFull + discr 5.55 6.36 5.47\nFull + discr + stlr 5.00 5.69 5.38\nTable 6: Validation error rates for ULMFiT with\ndifferent variations of LM ﬁne-tuning.\nImpact of LM quality In order to gauge the im-\nportance of choosing an appropriate LM, we com-\npare a vanilla LM with the same hyperparame-\nters without any dropout 8 with the AWD-LSTM\nLM with tuned dropout parameters in Table 5.\nUsing our ﬁne-tuning techniques, even a regular\nLM reaches surprisingly good performance on the\nlarger datasets. On the smaller TREC-6, a vanilla\nLM without dropout runs the risk of overﬁtting,\nwhich decreases performance.\nImpact of LM ﬁne-tuning We compare no ﬁne-\ntuning against ﬁne-tuning the full model (Erhan\net al., 2010) (‘ Full’), the most commonly used\nﬁne-tuning method, with and without discrimi-\nnative ﬁne-tuning (‘Discr’) and slanted triangular\nlearning rates (‘Stlr’) in Table 6. Fine-tuning the\nLM is most beneﬁcial for larger datasets. ‘ Discr’\nand ‘Stlr’ improve performance across all three\ndatasets and are necessary on the smaller TREC-6,\nwhere regular ﬁne-tuning is not beneﬁcial.\nImpact of classiﬁer ﬁne-tuning We compare\ntraining from scratch, ﬁne-tuning the full model\n(‘Full’), only ﬁne-tuning the last layer (‘ Last’)\n(Donahue et al., 2014), ‘Chain-thaw’ (Felbo et al.,\n2017), and gradual unfreezing (‘Freez’). We fur-\nthermore assess the importance of discriminative\nﬁne-tuning (‘Discr’) and slanted triangular learn-\ning rates (‘ Stlr’). We compare the latter to an\nalternative, aggressive cosine annealing schedule\n(‘Cos’) (Loshchilov and Hutter, 2017). We use a\nlearning rate ηL = 0.01 for ‘Discr’, learning rates\n8To avoid overﬁtting, we only train the vanilla LM classi-\nﬁer for 5 epochs and keep dropout of 0.4 in the classiﬁer.\nClassiﬁer ﬁne-tuning IMDb TREC-6 AG\nFrom scratch 9.93 13.36 6.81\nFull 6.87 6.86 5.81\nFull + discr 5.57 6.21 5.62\nLast 6.49 16.09 8.38\nChain-thaw 5.39 6.71 5.90\nFreez 6.37 6.86 5.81\nFreez + discr 5.39 5.86 6.04\nFreez + stlr 5.04 6.02 5.35\nFreez + cos 5.70 6.38 5.29\nFreez + discr + stlr 5.00 5.69 5.38\nTable 7: Validation error rates for ULMFiT with\ndifferent methods to ﬁne-tune the classiﬁer.\nof 0.001 and 0.0001 for the last and all other layers\nrespectively for ‘Chain-thaw’ as in (Felbo et al.,\n2017), and a learning rate of 0.001 otherwise. We\nshow the results in Table 7.\nFine-tuning the classiﬁer signiﬁcantly improves\nover training from scratch, particularly on the\nsmall TREC-6. ‘ Last’, the standard ﬁne-tuning\nmethod in CV , severely underﬁts and is never\nable to lower the training error to 0. ‘ Chain-\nthaw’ achieves competitive performance on the\nsmaller datasets, but is outperformed signiﬁcantly\non the large AG. ‘ Freez’ provides similar per-\nformance as ‘ Full’. ‘ Discr’ consistently boosts\nthe performance of ‘ Full’ and ‘ Freez’, except\nfor the large AG. Cosine annealing is competi-\ntive with slanted triangular learning rates on large\ndata, but under-performs on smaller datasets. Fi-\nnally, full ULMFiT classiﬁer ﬁne-tuning (bottom\nrow) achieves the best performance on IMDB and\nTREC-6 and competitive performance on AG. Im-\nportantly, ULMFiT is the only method that shows\nexcellent performance across the board—and is\ntherefore the only universal method.\nClassiﬁer ﬁne-tuning behavior While our re-\nsults demonstrate that how we ﬁne-tune the clas-\nsiﬁer makes a signiﬁcant difference, ﬁne-tuning\nfor inductive transfer is currently under-explored\nin NLP as it mostly has been thought to be un-\nhelpful (Mou et al., 2016). To better understand\nthe ﬁne-tuning behavior of our model, we compare\nthe validation error of the classiﬁer ﬁne-tuned with\nULMFiT and ‘Full’ during training in Figure 4.\nOn all datasets, ﬁne-tuning the full model leads\nto the lowest error comparatively early in train-\ning, e.g. already after the ﬁrst epoch on IMDb.\nFigure 4: Validation error rate curves for ﬁne-\ntuning the classiﬁer with ULMFiT and ‘ Full’ on\nIMDb, TREC-6, and AG (top to bottom).\nThe error then increases as the model starts to\noverﬁt and knowledge captured through pretrain-\ning is lost. In contrast, ULMFiT is more sta-\nble and suffers from no such catastrophic forget-\nting; performance remains similar or improves un-\ntil late epochs, which shows the positive effect of\nthe learning rate schedule.\nImpact of bidirectionality At the cost of train-\ning a second model, ensembling the predictions of\na forward and backwards LM-classiﬁer brings a\nperformance boost of around 0.5–0.7. On IMDb\nwe lower the test error from5.30 of a single model\nto 4.58 for the bidirectional model.\n6 Discussion and future directions\nWhile we have shown that ULMFiT can achieve\nstate-of-the-art performance on widely used text\nclassiﬁcation tasks, we believe that language\nmodel ﬁne-tuning will be particularly useful in the\nfollowing settings compared to existing transfer\nlearning approaches (Conneau et al., 2017; Mc-\nCann et al., 2017; Peters et al., 2018): a) NLP for\nnon-English languages, where training data for su-\npervised pretraining tasks is scarce; b) new NLP\ntasks where no state-of-the-art architecture exists;\nand c) tasks with limited amounts of labeled data\n(and some amounts of unlabeled data).\nGiven that transfer learning and particularly\nﬁne-tuning for NLP is under-explored, many fu-\nture directions are possible. One possible direc-\ntion is to improve language model pretraining and\nﬁne-tuning and make them more scalable: for\nImageNet, predicting far fewer classes only in-\ncurs a small performance drop (Huh et al., 2016),\nwhile recent work shows that an alignment be-\ntween source and target task label sets is impor-\ntant (Mahajan et al., 2018)—focusing on predict-\ning a subset of words such as the most frequent\nones might retain most of the performance while\nspeeding up training. Language modeling can also\nbe augmented with additional tasks in a multi-task\nlearning fashion (Caruana, 1993) or enriched with\nadditional supervision, e.g. syntax-sensitive de-\npendencies (Linzen et al., 2016) to create a model\nthat is more general or better suited for certain\ndownstream tasks, ideally in a weakly-supervised\nmanner to retain its universal properties.\nAnother direction is to apply the method to\nnovel tasks and models. While an extension to\nsequence labeling is straightforward, other tasks\nwith more complex interactions such as entailment\nor question answering may require novel ways to\npretrain and ﬁne-tune. Finally, while we have\nprovided a series of analyses and ablations, more\nstudies are required to better understand what\nknowledge a pretrained language model captures,\nhow this changes during ﬁne-tuning, and what in-\nformation different tasks require.\n7 Conclusion\nWe have proposed ULMFiT, an effective and ex-\ntremely sample-efﬁcient transfer learning method\nthat can be applied to any NLP task. We have also\nproposed several novel ﬁne-tuning techniques that\nin conjunction prevent catastrophic forgetting and\nenable robust learning across a diverse range of\ntasks. Our method signiﬁcantly outperformed ex-\nisting transfer learning techniques and the state-\nof-the-art on six representative text classiﬁcation\ntasks. We hope that our results will catalyze new\ndevelopments in transfer learning for NLP.\nAcknowledgments\nWe thank the anonymous reviewers for their valu-\nable feedback. Sebastian is supported by Irish\nResearch Council Grant Number EBPPG/2014/30\nand Science Foundation Ireland Grant Number\nSFI/12/RC/2289.\nReferences\nJonathan Baxter. 2000. A Model of Inductive Bias\nLearning. Journal of Artiﬁcial Intelligence Research\n12:149–198.\nJohn Blitzer, Mark Dredze, and Fernando Pereira.\n2007. Biographies, bollywood, boom-boxes\nand blenders: Domain adaptation for sentiment\nclassiﬁcation. Annual Meeting-Association\nfor Computational Linguistics 45(1):440.\nhttps://doi.org/10.1109/IRPS.2011.5784441.\nCornelia Caragea, Nathan McNeese, Anuj Jaiswal,\nGreg Traylor, Hyun-Woo Kim, Prasenjit Mitra,\nDinghao Wu, Andrea H Tapia, Lee Giles, Bernard J\nJansen, et al. 2011. Classifying text messages for the\nhaiti earthquake. In Proceedings of the 8th interna-\ntional conference on information systems for crisis\nresponse and management (ISCRAM2011). Citeseer.\nRich Caruana. 1993. Multitask learning: A\nknowledge-based source of inductive bias. In Pro-\nceedings of the Tenth International Conference on\nMachine Learning.\nZhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and\nAndrew Rabinovich. 2017. GradNorm: Gradient\nNormalization for Adaptive Loss Balancing in Deep\nMultitask Networks pages 1–10.\nZi Chu, Steven Gianvecchio, Haining Wang, and Sushil\nJajodia. 2012. Detecting automation of twitter ac-\ncounts: Are you a human, bot, or cyborg? IEEE\nTransactions on Dependable and Secure Computing\n9(6):811–824.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc\nBarrault, and Antoine Bordes. 2017. Supervised\nLearning of Universal Sentence Representations\nfrom Natural Language Inference Data. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing.\nAndrew M. Dai and Quoc V . Le. 2015. Semi-\nsupervised Sequence Learning. Advances in Neu-\nral Information Processing Systems (NIPS ’15)\nhttp://arxiv.org/abs/1511.01432.\nJeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-\nman, Ning Zhang, Eric Tzeng, and Trevor Darrell.\n2014. Decaf: A deep convolutional activation fea-\nture for generic visual recognition. In International\nconference on machine learning. pages 647–655.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep Biafﬁne Attention for Neural Dependency\nParsing. In Proceedings of ICLR 2017.\nDumitru Erhan, Yoshua Bengio, Aaron Courville,\nPierre-Antoine Manzagol, Pascal Vincent, and Samy\nBengio. 2010. Why does unsupervised pre-training\nhelp deep learning? Journal of Machine Learning\nResearch 11(Feb):625–660.\nBjarke Felbo, Alan Mislove, Anders Søgaard, Iyad\nRahwan, and Sune Lehmann. 2017. Using millions\nof emoji occurrences to learn any-domain represen-\ntations for detecting sentiment, emotion and sar-\ncasm. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Process-\ning.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of NAACL-HLT 2018.\nBharath Hariharan, Pablo Arbel´aez, Ross Girshick, and\nJitendra Malik. 2015. Hypercolumns for object seg-\nmentation and ﬁne-grained localization. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition. pages 447–456.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep Residual Learning for Image\nRecognition. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition.\nGao Huang, Zhuang Liu, Kilian Q. Weinberger, and\nLaurens van der Maaten. 2017. Densely Connected\nConvolutional Networks. In Proceedings of CVPR\n2017.\nMinyoung Huh, Pulkit Agrawal, and Alexei A Efros.\n2016. What makes ImageNet good for transfer\nlearning? arXiv preprint arXiv:1608.08614 .\nSergey Ioffe and Christian Szegedy. 2015. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. In International\nConference on Machine Learning. pages 448–456.\nNitin Jindal and Bing Liu. 2007. Review spam de-\ntection. In Proceedings of the 16th international\nconference on World Wide Web. ACM, pages 1189–\n1190.\nRie Johnson and Tong Zhang. 2016. Supervised and\nsemi-supervised text categorization using lstm for\nregion embeddings. In International Conference on\nMachine Learning. pages 526–534.\nRie Johnson and Tong Zhang. 2017. Deep pyramid\nconvolutional neural networks for text categoriza-\ntion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers). volume 1, pages 562–570.\nTal Linzen, Emmanuel Dupoux, and Yoav Gold-\nberg. 2016. Assessing the ability of lstms to\nlearn syntax-sensitive dependencies. arXiv preprint\narXiv:1611.01368 .\nLiyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan\nGui, Jian Peng, and Jiawei Han. 2018. Empower\nsequence labeling with task-aware neural language\nmodel. In Proceedings of AAAI 2018.\nJonathan Long, Evan Shelhamer, and Trevor Darrell.\n2015a. Fully convolutional networks for semantic\nsegmentation. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition .\npages 3431–3440.\nMingsheng Long, Yue Cao, Jianmin Wang, and\nMichael I. Jordan. 2015b. Learning Transferable\nFeatures with Deep Adaptation Networks. In Pro-\nceedings of the 32nd International Conference on\nMachine learning (ICML ’15). volume 37.\nIlya Loshchilov and Frank Hutter. 2017. SGDR:\nStochastic Gradient Descent with Warm Restarts. In\nProceedings of the Internal Conference on Learning\nRepresentations 2017.\nAndrew L Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies-Volume 1. Association for Com-\nputational Linguistics, pages 142–150.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan,\nKaiming He, Manohar Paluri, Yixuan Li, Ashwin\nBharambe, and Laurens van der Maaten. 2018. Ex-\nploring the Limits of Weakly Supervised Pretraining\n.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in Translation: Con-\ntextualized Word Vectors. In Advances in Neural\nInformation Processing Systems.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017a. Regularizing and Optimiz-\ning LSTM Language Models. arXiv preprint\narXiv:1708.02182 .\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017b. Pointer Sentinel Mixture\nModels. In Proceedings of the International Con-\nference on Learning Representations 2017.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Distributed Representations of Words\nand Phrases and their Compositionality. In Ad-\nvances in Neural Information Processing Systems.\nSewon Min, Minjoon Seo, and Hannaneh Hajishirzi.\n2017. Question Answering through Transfer Learn-\ning from Large Fine-grained Supervision Data. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Short Pa-\npers).\nTakeru Miyato, Andrew M Dai, and Ian Good-\nfellow. 2016. Adversarial training methods for\nsemi-supervised text classiﬁcation. arXiv preprint\narXiv:1605.07725 .\nLili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,\nLu Zhang, and Zhi Jin. 2016. How Transferable are\nNeural Networks in NLP Applications? Proceed-\nings of 2016 Conference on Empirical Methods in\nNatural Language Processing .\nLili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and\nZhi Jin. 2015. Discriminative neural sentence mod-\neling by tree-based convolution. In Proceedings of\nthe 2015 Conference on Empirical Methods in Nat-\nural Language Processing.\nEWT Ngai, Yong Hu, YH Wong, Yijun Chen, and Xin\nSun. 2011. The application of data mining tech-\nniques in ﬁnancial fraud detection: A classiﬁca-\ntion framework and an academic review of literature.\nDecision Support Systems 50(3):559–569.\nSinno Jialin Pan and Qiang Yang. 2010. A survey on\ntransfer learning. IEEE Transactions on Knowledge\nand Data Engineering 22(10):1345–1359.\nMatthew E Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn Proceedings of ACL 2017.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL 2018.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv:1704.01444 .\nMarek Rei. 2017. Semi-supervised multitask learning\nfor sequence labeling. In Proceedings of ACL 2017.\nHerbert L Roitblat, Anne Kershaw, and Patrick Oot.\n2010. Document categorization in legal electronic\ndiscovery: computer classiﬁcation vs. manual re-\nview. Journal of the Association for Information\nScience and Technology 61(1):70–80.\nSebastian Ruder. 2016. An overview of gradient\ndescent optimization algorithms. arXiv preprint\narXiv:1609.04747 .\nRuslan Salakhutdinov and Geoffrey Hinton. 2009.\nDeep boltzmann machines. In Artiﬁcial Intelligence\nand Statistics. pages 448–455.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Improving neural machine translation\nmodels with monolingual data. arXiv preprint\narXiv:1511.06709 .\nAliaksei Severyn and Alessandro Moschitti. 2015.\nUNITN: Training Deep Convolutional Neural Net-\nwork for Twitter Sentiment Classiﬁcation. Proceed-\nings of the 9th International Workshop on Semantic\nEvaluation (SemEval 2015) pages 464–469.\nAli Sharif Razavian, Hossein Azizpour, Josephine Sul-\nlivan, and Stefan Carlsson. 2014. Cnn features off-\nthe-shelf: an astounding baseline for recognition. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition. pages 806–813.\nLeslie N Smith. 2017. Cyclical learning rates for train-\ning neural networks. In Applications of Computer\nVision (WACV), 2017 IEEE Winter Conference on .\nIEEE, pages 464–472.\nVladimir Naumovich Vapnik and Samuel Kotz. 1982.\nEstimation of dependences based on empirical data,\nvolume 40. Springer-Verlag New York.\nEllen M V oorhees and Dawn M Tice. 1999. The trec-8\nquestion answering track evaluation. In TREC. vol-\nume 1999, page 82.\nJohn Wieting and Kevin Gimpel. 2017. Revisiting Re-\ncurrent Networks for Paraphrastic Sentence Embed-\ndings. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (ACL\n2017).\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in neural information\nprocessing systems. pages 3320–3328.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in neural information pro-\ncessing systems. pages 649–657.\nPeng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu,\nHongyun Bao, and Bo Xu. 2016. Text classiﬁcation\nimproved by integrating bidirectional lstm with two-\ndimensional max pooling. In Proceedings of COL-\nING 2016.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.846267580986023
    },
    {
      "name": "Scratch",
      "score": 0.7897850275039673
    },
    {
      "name": "Task (project management)",
      "score": 0.7180511951446533
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6823422908782959
    },
    {
      "name": "Language model",
      "score": 0.6624506711959839
    },
    {
      "name": "Transfer of learning",
      "score": 0.6548058986663818
    },
    {
      "name": "Key (lock)",
      "score": 0.5986831784248352
    },
    {
      "name": "Natural language processing",
      "score": 0.597490668296814
    },
    {
      "name": "Code (set theory)",
      "score": 0.5529164671897888
    },
    {
      "name": "State (computer science)",
      "score": 0.4442238211631775
    },
    {
      "name": "Transfer (computing)",
      "score": 0.42027518153190613
    },
    {
      "name": "Machine learning",
      "score": 0.3852396011352539
    },
    {
      "name": "Programming language",
      "score": 0.11873596906661987
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.05781453847885132
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Parallel computing",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 340
}