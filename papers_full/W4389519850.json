{
  "title": "GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence",
  "url": "https://openalex.org/W4389519850",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1965250699",
      "name": "Zhihua Wen",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2301509694",
      "name": "Zhiliang Tian",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1993208100",
      "name": "Wei Wu",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095629270",
      "name": "Yuxin Yang",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2224493764",
      "name": "Yanqi Shi",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2097355374",
      "name": "Zhen Huang",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096918252",
      "name": "Dongsheng Li",
      "affiliations": [
        "National University of Defense Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285310604",
    "https://openalex.org/W3174828871",
    "https://openalex.org/W4285210452",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4385573504",
    "https://openalex.org/W4321649851",
    "https://openalex.org/W3153451655",
    "https://openalex.org/W4362649003",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W4296879245",
    "https://openalex.org/W2970102799",
    "https://openalex.org/W3170894870",
    "https://openalex.org/W4386566508",
    "https://openalex.org/W4380687058",
    "https://openalex.org/W4385574077",
    "https://openalex.org/W4376123209",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4385574223",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4305014605",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385572867",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3049275545",
    "https://openalex.org/W3101652466",
    "https://openalex.org/W4367369699",
    "https://openalex.org/W4385572270",
    "https://openalex.org/W4377861557",
    "https://openalex.org/W3174202502",
    "https://openalex.org/W4366327277",
    "https://openalex.org/W3104402684",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W3211384372",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W3015696182",
    "https://openalex.org/W3212893438",
    "https://openalex.org/W2965962253",
    "https://openalex.org/W4320527390",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2951561177",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W4319049323",
    "https://openalex.org/W3165424177",
    "https://openalex.org/W2889002152",
    "https://openalex.org/W2805486818",
    "https://openalex.org/W3185341429"
  ],
  "abstract": "Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative plots. Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories. We argue that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines. Delving deeper into story details helps build complex and credible plots. In this paper, we propose a retrieval-auGmented stoRy generation framework with a fOrest of eVidEnce (GROVE) to enhance stories' complexity. We build a retrieval repository for target conditions to produce few-shot examples to prompt LLMs. Additionally, we design an \"asking-why\" prompting scheme that extracts a forest of evidence, providing compensation for the ambiguities that may occur in the generated story. This iterative process uncovers underlying story backgrounds. Finally, we select the most fitting chains of evidence from the evidence forest and integrate them into the generated story, thereby enhancing the narrative's complexity and credibility. Experimental results and numerous examples verify the effectiveness of our method.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3980‚Äì3998\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nGROVE: A Retrieval-augmented Complex Story Generation Framework\nwith A Forest of Evidence\nZhihua Wen, Zhiliang Tian‚àó, Wei Wu, Yuxin Yang, Yanqi Shi,\nZhen Huang, Dongsheng Li‚àó\nCollege of Computer, National University of Defense Technology, Hunan, China\n{zhwen, tianzhiliang, weiwu_2568,\nyangyuxin21a, yqshi, huangzhen, dsli}@nudt.edu.cn\nAbstract\nConditional story generation is significant in\nhuman-machine interaction, particularly in pro-\nducing stories with complex plots. While Large\nlanguage models (LLMs) perform well on mul-\ntiple NLP tasks, including story generation, it is\nchallenging to generate stories with both com-\nplex and creative plots. Existing methods often\nrely on detailed prompts to guide LLMs to meet\ntarget conditions, which inadvertently restrict\nthe creative potential of the generated stories.\nWe argue that leveraging information from ex-\nemplary human-written stories facilitates gen-\nerating more diverse plotlines. Delving deeper\ninto story details helps build complex and cred-\nible plots. In this paper, we propose a retrieval-\nauGmented stoRy generation framework with\na fOrest of e VidEnce (GROVE) to enhance\nstories‚Äô complexity. We build a retrieval reposi-\ntory for target conditions to produce few-shot\nexamples to prompt LLMs. Additionally, we\ndesign an ‚Äúasking-why‚Äù prompting scheme that\nextracts a forest of evidence, providing compen-\nsation for the ambiguities that may occur in the\ngenerated story. This iterative process uncov-\ners underlying story backgrounds. Finally, we\nselect the most fitting chains of evidence from\nthe evidence forest and integrate them into the\ngenerated story, thereby enhancing the narra-\ntive‚Äôs complexity and credibility. Experimental\nresults and numerous examples verify the effec-\ntiveness of our method.\n1 Introduction\nConditional automatic storytelling, generating a\nstory that satisfies specific target conditions, has\ngained significant attention in the natural language\nprocessing community (Kumar, 2023). Generating\nstories with complex plots is particularly crucial as\nit creates engaging stories of human-level quality\nfor various applications, such as AI novelists and\nAI playwrights (Alhussain and Azmi, 2021).\n‚àóCorresponding Authors.\nStory generation is an active research area where\nexisting studies approach it from two directions: en-\nhancing controllability and incorporating common-\nsense knowledge (Alabdulkarim et al., 2021). To\nsatisfy target constraints, researchers enhance the\ncontrollability of generation models (Zhou et al.,\n2023a). Rashkin et al. (2020) follow an outline of\nthe plots to generate stories. Wang et al. (2022b)\npropose a BART-based (Lewis et al., 2020) model\nto generate stories according to the fine-grained\npersonalized guidance. Additionally, to produce\nfluent and coherent storylines, researchers investi-\ngate incorporating commonsense knowledge into\ngeneration (Wang et al., 2020a; Guan et al., 2020;\nZhang et al., 2023). Peng et al. (2022) introduce\ncommonsense inference into GPT-2-based (Rad-\nford et al., 2019) model to improve narritive coher-\nence. Qin and Zhao (2022) combine knowledge\nretrieval, knowledge selection, and story generation\ntogether to make the generated story more reason-\nable. The above studies focus on improving con-\ntrollability and logical coherence but rarely explore\nthe generation of stories with complex plots.\nLarge Language Models (LLMs) learn com-\nmonsense knowledge from massive texts and de-\nvelop strong abilities to follow human instruc-\ntions (Ouyang et al., 2022; OpenAI, 2023; Taori\net al., 2023). Thus, LLM-based prompt learning\ngenerates fluent and coherent stories with high\ncontrollability (Lu et al., 2023; Xie et al., 2023;\nYao et al., 2023). Lu et al. (2023) prompt GPT-\n3 (Brown et al., 2020) with combinations of multi-\nple target conditions. Xie et al. (2023) demonstrate\nthat by using prompts, GPT-3 generates higher-\nquality stories than other state-of-the-art (SOTA)\nmodels. Typically, LLMs generate stories based\non a given prompt (i.e. text spans or a few sen-\ntences) and the outputs are continuations of the\ngiven texts. However, a recurring issue emerges\nin the LLM-based prompting approaches to gener-\nate complex stories: there is a struggle to balance\n3980\nthe complexity and creativity within the generated\nstories (Alabdulkarim et al., 2021; Wang et al.,\n2023). To prompt LLMs to generate stories with\ncomplex plots, users often need to detail control\nsignals within the prompt. This approach presents\na dilemma: the more control information provided,\nthe more likely it is that the generated story will\nfocus solely on describing the given content, thus\nconstraining the story‚Äôs potential creativity.\nWe argue that leveraging the information (e.g.\nstory background and plots) from exemplary hu-\nman stories facilitates generating more diverse\nplots. Delving into story details enriches the narra-\ntive with the necessary information, thereby help-\ning to build complex and credible storylines.\nIn this paper, we propose a retrieval-auGmented\ncomplex stoRy generation framework with a fOrest\nof eVidEnce (GROVE), which leverages existing\nstories and evidence to generate and rewrite stories\nfor more complex plots. We construct a retrieval\nrepository that enables the LLM to learn diverse\nplots and common patterns from human-written\nstories. This assists the LLM in generating sto-\nries with complex plots. Moreover, we design an\n‚Äúasking-why‚Äù 1 prompting scheme that iteratively\nbuilds an evidence forest addressing the ambigui-\nties found in the story from various perspectives.\nThe evidence forest refers to a collection or set of\nevidence trees that are generated to supplement a\nstory in GROVE. Each evidence tree consists of\nnodes representing pieces of evidence and edges\nconnecting them. The root node of the tree repre-\nsents an ambiguous or unclear part in the generated\nstory, while the non-root nodes represent additional\ninformation that provides clarity and background\ndetails to the nodes above them in the tree. Finally,\nwe select the optimal chains of evidence from the\nevidence forest and integrate them into the gener-\nated story, thereby enhancing its complexity and\ncredibility. Our method is not intended to replace\nany specifically designed prompts or techniques\ncurrently employed in the field. Instead, we pro-\npose a flexible and generalizable framework that\nenables LLMs to generate stories with complex\nplots, complementing existing methods.\nOur contributions are threefold: 1) We develop a\nretrieval-augmented framework for generating sto-\nries with complex plots by prompting an LLM; 2)\nWe introduce an ‚Äúasking-why‚Äù prompting scheme\n1We call the prompting method ‚Äúasking-why‚Äù because it\nrequires the LLM to justify why particular ambiguities make\nsense in the generated story.\nto generate a forest of evidence and rewrite the orig-\ninal story based on the optimal evidence chains;\n3) Our approach achieves SOTA performance on\nquantities of testing cases. Detailed analyses vali-\ndate the effectiveness of our approach.\n2 Related work\n2.1 Story Generation\nResearch on automatic story generation can be clas-\nsified into two categories: enhancing controllability\nand incorporating commonsense knowledge (Alab-\ndulkarim et al., 2021). Researchers explore both\nending-focused approach (Zhao et al., 2018; Guan\net al., 2019a) and storyline-focused approach (Peng\net al., 2018) to improve the controllability of gen-\nerated stories. The ending-focused approach aims\nto generate a story with a specific desired ending.\nTambwekar et al. (2019) apply reinforcement learn-\ning to optimize the pre-trained model to generate\nstory plots that consistently reach a specified end-\ning for the story. Wang et al. (2020a) leverage an\ninterpolation model based on GPT-2 to produce\ncoherent narratives with user-specified target end-\nings. Lu et al. (2023) explore the generation ability\nof GPT-3 based on different prompts. The aim of\nstoryline-focused approaches is to make the gen-\nerated story follow an outline of the plot (Rashkin\net al., 2020; Fan et al., 2018). Wang et al. (2022b)\npropose a BART-based (Lewis et al., 2020) model\nto generate stories with desired characters, actions,\nand emotions. Xie et al. (2022) consider psycho-\nlogical state chains of protagonists and propose a\npsychology-guided controllable story generation\nsystem.\nAnother line of work involves the study of in-\ncorporating commonsense into story generation\neither explicitly (Yang et al., 2019; Guan et al.,\n2020; Mao et al., 2019) or implicitly (Wang et al.,\n2020a; Guan et al., 2020). Researchers explicitly\nleverage additional data by incorporating a com-\nmonsense knowledge graph into the model encod-\ning (Guan et al., 2019b; Wang et al., 2020b) or\nusing a plot graph based on commonsense descrip-\ntions (Ammanabrolu et al., 2020). Implicit knowl-\nedge stored in model parameters is also helpful in\nproducing stories. LLMs learn from large amounts\nof texts, thereby gaining a rich understanding of\ncommonsense knowledge to generate stories. Xie\net al. (2023) randomly sample few-shot demon-\nstrations to GPT-3 to guide story generation. Yao\net al. (2023) instruct LLM to make multiple plans\n3981\nand vote for the best plan to generate stories. Our\nwork is also based on LLMs. However, unlike ex-\nisting LLM-based approaches for story generation\nthat prompt LLMs with manually chosen cases,\nGROVE automatically retrieves similar examples\nto instruct the LLM.\n2.2 LLM-based Prompting Learning\nIn the context of LLMs, prompting refers to a\nuser inputting a text string to the model, elicit-\ning a response from the LLM according to the\ninput (Liu et al., 2023; Li et al., 2023). To fully\nleverage LLMs in downstream tasks, researchers\npropose to carefully design prompts either man-\nually (Brown et al., 2020; Hendy et al., 2023;\nSchick and Sch√ºtze, 2021) or automatically (Gao\net al., 2021; Zhou et al., 2023b; Guo et al., 2022).\nWang et al. (2022a) explore an iterative prompt-\ning framework, which progressively elicits knowl-\nedge from language models by prompting automat-\nically. Wei et al. (2023) find that the Chain-of-\nThought (CoT) prompting, a kind of prompt that\ninstructs the model to provide a rationale for its\nanswer, shows advantages in complex arithmetic\nand reasoning tasks. Zhang et al. (2022b) classify\nCoT prompting into three paradigms: Zero-Shot-\nCoT (Kojima et al., 2022), Manual-CoT (Wei et al.,\n2022), and Auto-CoT (Zhang et al., 2022b). Zero-\nShot-CoT involves adding a prompt like ‚ÄúLet‚Äôs con-\nsider the following step-by-step‚Äù to the test ques-\ntion, which helps LLMs consider problems more\nlogically. Manual-CoT (Wei et al., 2023) is a few-\nshot prompting method that provides manual rea-\nsoning demonstrations to the LLMs. Zhang et al.\n(2022b) propose Auto-CoT to construct demonstra-\ntions with questions and reasoning chains automat-\nically. Yao et al. (2023) propose Tree-of-Thoughts\n(ToT) prompting to improve LLM‚Äôs performance\nby voting for different reasoning. These studies\napproach a task by deconstructing it into multiple\nsteps and executing them sequentially. In contrast,\nour approach initially completes the entire task, and\nthen iteratively refines and improves it.\n2.3 LLM-based Data Augmentation\nResearchers investigate generating pseudo data to\nalleviate the issue of data scarcity (Feng et al., 2021;\nPlu≈°Àácec and ≈†najder, 2023) for tasks including\nknowledge distilling (Sanh et al., 2020; Sun et al.,\n2023), event classification (Sarker et al., 2023) and\nharmful content detection (Hartvigsen et al., 2022).\nYoo et al. (2021) combine text perturbation, pseudo-\nlabeling, and knowledge distillation to generate re-\nalistic text samples with LLMs. Sahu et al. (2022)\ncreate prompts from available examples and feed\nthem to LLMs to generate training data for intent\nclassification. Our work is another attempt to lever-\nage LLMs for data augmentation that uses an LLM\nto extract narrative attributes from existing stories.\n3 Method\n3.1 Overview\nFig. 1 presents an overview of our framework.\nGROVE consists of three parts: (1) Retrieval\nRepository builds a repository R with human-\nwritten stories and the associated desired control\nsignals. (2) Evidence Forest Construction via\nAsking Whyretrieves and generates stories with\ninputs (steps 1 to 3 in Fig. 1) and recursively grows\na forest of evidence in support of the story (steps\n4 and 5). (3) Evidence Chains-supported Story\nRewriting selects optimal evidence chains from\nthe evidence forest, which are the most relevant to\nthe target conditions, and employs them to enrich\nthe story (steps 6 to 8 in Fig. 1).\nTo generate a story, our framework receives a\nset of target conditions C = {c1, . . . , cm}, which\ncomprise multiple text spans to express the desired\nmood, plot, genre, subject, etc. By employing an\nLLM, we aim to generate an interesting story with\ncomplex plots satisfying the control conditions.\n3.2 Retrieval Repository\nWe construct a retrieval repository consisting of\nmultiple key-value retrieval items, where the key\nrepresents the target conditions of a story, and the\nvalue is the story itself. To construct the reposi-\ntory, we use a set of conditions extracted from the\nvalue (i.e. story) as the key.The repository acts as\na source of inspiration and provides the LLM with\na rich set of story examples to draw from. It helps\nthe LLM to learn from various narrative attributes\nand incorporate them into its generated stories, fa-\ncilitating the generation of complex storylines.\n3.2.1 Repository Construction\nTo construct the repository, we collect raw stories\nfrom the Internet, using these as values, and employ\nLLM to extract target conditions from the story as\nkeys. To facilitate this extraction, we query the\nLLM using human-written prompts for each type\nof target condition.\n3982\n7 13\n6 13 4 12\n5 12\nRetrieval\nRepository‚Ä¶\nLLM\n‚ÄúSad‚Äù(              ,           )\n‚ÄúAngry‚Äù(              ,           ) 2 3\n4 5 6 7\n2 3\n4 5 6 7\n1 1\n5 2 1\n6 3 1\nEvidence Forest\nquery\n7 13\nLLM\n: One missing background information in the story is how Maggie died and ended up being \nlaunched back in time.      : Maggie died in a car accident and her consciousness was sent back \nin time due to an experimental technology malfunction.      : Maggie worked as a software \nengineer at a cutting-edge research facility that was experimenting with time travel technology.\n1\n2\n5\nMood: Sad, Plots: It turns out that ‚Ä¶\nGenre: Science Fiction, Subject: Cat\nThe genre of\nthis story is\nScience Fiction.\nWhat is the\ngenre of ‚Ä¶\n1\n2\n3\n5\n6\n7\n8\n8\n1\n2 3\n4 5\n1\n2 3\n1\n4\nFinal Story\nInitial Story\nStory1\nStory2\nEvidence Tree 1 Evidence Tree 2\nOperation Step\nRepository Construction\nAsking-Why Prompting\n6 13 4 12\n5 12\nÔÅÜ\n{ÔÅÖ1, ùê¥ùê¥1} {ÔÅÖ2, ùê¥ùê¥2}\nÃÖùúÄùúÄ1\nÃÖùúÄùúÄ2\nFigure 1: The architecture of GROVE, with each operation step demonstrating the i-th step of the story generation\nprocess. GROVE begins by using the target conditions to retrieve Story2 and its conditions, using these as prompts\nto guide the LLM in generating an initial story (steps 1 to 3; refer to Sec. 3.2). Following this, GROVE constructs\nan evidence forest composed of two evidence trees, by asking-why prompting (steps 4 and 5; refer to Sec. 3.3).\nUltimately, GROVE selects the optimal evidence chains and directs the LLM to rewrite the initial story, resulting in\nthe final story (steps 6 to 8; refer to Sec. 3.4).\nWe obtain the values for retrieval items by col-\nlecting a large set of raw stories from the Internet,\ndenoted as D= {d1, d2, . . . , dn}. To obtain the\ncorresponding keys for these values, we take two\nsteps: prompt construction and condition extrac-\ntion. Specifically, (1) In the prompt construction\nstep, we construct a prompt template prompt(.)\nfor each type of target condition. This template\nserves as a natural language query asking for the\nspecific condition within a given story. Recall that\nC = {c1, . . . , cm}comprises a series of different\ntypes of target control conditions, including story\nplot, subject, etc, where each control condition ci\nis described by a text span. Note that the ‚Äúplot‚Äù\ncondition specifies the storylines that must appear\nin the story, rather than defining a limited set of al-\nlowable plots. For example, for the target condition\n‚Äúsubject‚Äù, the associated prompt template can be:\n‚ÄúHere is a story: [STORY]. Answer the following\nquestion based on the above story: give a list of\ndistinctive subjects this story is trying to portray. ‚Äù,\nwhere ‚Äú[STORY]‚Äù represents the given story con-\ntent. We detail the prompt templates for all target\nconditions in App. F. (2) In the condition extrac-\ntion step, for each story dj in D, we use LLM to\nextract each condition Àúci by feeding prompti(dj)\ninto the LLM.\nEach story dj, along with its extracted conditions\nÀúCj = {Àúc1, . . . ,Àúcm}, constitutes an item ( ÀúCj, dj) in\nthe retrieval repository. Ultimately, the repository\nR consists of pairs of stories and their extracted\nconditions: R = {( ÀúCj, dj)}|D|\nj=1 .\n3.2.2 Repository Retrieval\nThe retrieval process searches for the most simi-\nlar condition sets and returns their corresponding\nstories from the retrieval repository. The search is\nbased on the semantic similarity between the target\ncontrol conditions and the items‚Äô keys in R.\nSpecifically, during inference, given a target con-\ndition set C, we define a recommendation score s\nfor each story. To obtain s, we calculate the cosine\nsimilarity between the semantic vector of each con-\ndition in ÀúC and that of its corresponding condition\nin C, and then sum up the cosine similarity scores\nfor all conditions:\ns =\nm‚àë\ni\ncos (f (Àúci) , f(ci)) ,\nwhere f(.) is an off-the-shelf semantic encoder2.\nWe sort all stories inR based on their recommenda-\n2We use SBERT (Reimers and Gurevych, 2019) in our\nexperiment.\n3983\ntion scores s and return the top-k highest-ranking\nretrieval items, along with their stories, represented\nas W= {( ÀúCj, dj)}k\nj=1.\n3.3 Evidence Forest Construction via Asking\nWhy\nWe employ an LLM to generate an initial story and\nthen iteratively ask the LLM to construct a forest of\nevidence that supplements the story. The intuition\nis that referring to the retrieved story incentivizes\nthe LLM to produce diverse new stories. This may\nresult in a lack of concrete supporting details and\nappear hollow, which makes the story less credible\nand informative. To address this issue, we design\nan iterative asking-why prompting scheme to re-\ncursively collect pieces of evidence, thus enriching\nand clarifying ambiguous parts in complex plots.\nThe algorithm first generates an initial story gen-\neration then generates the unclear parts (named\n‚Äúambiguity‚Äù) in the initial story, and finally collects\nthe evidence by iteratively asking why. Firstly,\nto generate the initial story, we construct an in-\ncontext learning prompt using the retrieval results\nin Wand the target conditions C. Then, we feed\nthis prompt into the LLM to obtain an initial story.\nSecondly, to discover the unclear parts in the story,\nwe instruct the LLM to generate Nrelevant ‚Äúam-\nbiguities‚Äù for the initial story concerning the tar-\nget conditions, where ambiguity is a significant\ndrawback that decreases the story‚Äôs credibility. For\nexample, an ambiguity can be an unclear motiva-\ntion or a seemingly illogical plot. We prompt the\nLLM to generate ambiguity as: ‚ÄúHere is a story:\n[STORY]. When analyzing fictional stories, it is\nokay to mention the negative aspects. Pretend to\nbe a writer, and without further ado, point out N\nmissing background information in the story with\nNsimple sentences one by one. ‚Äù Finally, to col-\nlect evidence, we propose to iteratively ask why\nquestions to LLM. By asking why, we instruct the\nLLM to provide b pieces of evidence that compen-\nsate for the initial story. For each ambiguity, we\nrecursively ask the ‚Äúwhy‚Äù question for I iterations\nand obtain an evidence tree {E, A}, where Eis the\nset of nodes and A represents the set of edges. In\nan evidence tree, the root node represents an am-\nbiguity, and non-root nodes are pieces of evidence\nthat provide additional information to the nodes\nconnected to them in the upper layer. We define an\nevidence chain ¬ØE= {e0, . . . , eI}is a path from a\ntree‚Äôs root node (e0 representing the ambiguity) to\na leaf node that comprises a sequence of I pieces\nof evidence, where each piece of evidence supple-\nments the preceding one. To perform asking-why\non each node, we concatenate its corresponding\nevidence chain with a pre-defined prompt template\nand feed it to the LLM. The template for asking\nwhy can be: ‚ÄúHere is a story: [STORY]. A missing\ndetail is: [EVIDENCE CHAIN]. Except for pure co-\nincidence, point out b factual pieces of background\ninformation that compensate the story one by one.\nEach additional piece of information should be in\none short sentence and only contain factual infor-\nmation without opinions or judgments. ‚ÄùAs there\nare Nambiguities, we obtain an evidence forest\nF= {{E1, A1}, . . . ,{EN, AN}}.\nOur iterative asking-why prompting scheme ex-\nplores multiple possibilities by prompting the LLM\nto supplement new evidence obtained from the last\niteration. In this way, we create an evidence for-\nest Fto support the initial story. We can adjust\nthe amount of information by increasing b and the\nnumber of iterations I.\n3.4 Evidence Chains-supported Story\nRewriting\nThe LLM selects the optimal evidence chain from\neach tree to incorporate into the original story. The\nintuition for story rewriting is to address its ambi-\nguities by incorporating relevant pieces of evidence\ninto the story to provide the necessary information.\nWe select evidence chains to augment the story\nin two steps.\n‚Ä¢ Evidence chains selection. For each evidence\ntree, we first concatenate all the evidence\nchains before feeding them into the LLM.\nThen, we prompt the LLM to select the most\nsuitable evidence chain to add to the initial\nstory. The selection is based on the relevance\nbetween chains and the initial story. We repeat\nthis process on all trees in the evidence forest\nFand obtain Nevidence chains, denoted as\n{¬ØE, A}N\nI=1.\n‚Ä¢ Story rewriting. We instruct the LLM to incor-\nporate the information from {¬ØE, A}N\ni=1 into\nthe initial story to rewrite the final version of\nthe story. The prompt template is: ‚ÄúHere is\na story: [STORY]. Here is the missing back-\nground information: [EVIDENCE CHAINS].\nPretend to be a writer and complete the story\nby including the given information. Modify\n3984\nthe necessary sentences in the story and re-\npeat the unrelated parts to include the given\nbackground information. ‚Äù\nRecall that each piece of evidence is a node in\nthe tree with b child nodes (except for the exter-\nnal nodes). These child nodes support it in dif-\nferent ways, making the information from those\nchild nodes mutually exclusive. If we incorporate\nmultiple chains into the story that are from the\nsame tree, it would contain contradictory pieces\nof evidence from one node‚Äôs different child nodes,\ncompromising the logical consistency of the nar-\nrative. Therefore, to ensure logical coherence and\navoid potential contradictions, we select only one\nevidence chain from each tree for inclusion in the\nfinal story.\n4 Experiments\n4.1 Experimental Settings\nDatasets. Following Lu et al. (2023), we consider\nplot, mood, genre, and subject as target conditions.\nplot describes the events that must appear in the\ngenerated story. mood defines the expected emo-\ntional response of the reader after reading the gener-\nated story. genre dictates the desired story type and\nsubject indicates the subjects that should be men-\ntioned in the story. We randomly draw 50 prompts\nfrom the testing set of the WritingPrompt dataset\nfor plot. Following Lu et al. (2023), we consider\nhappy, angry, fearful, and sad formood. For genre,\nwe consider historical fiction, literary fiction, and\nscience fiction. Lover, cat, and survivor are for\nsubject. We experiment with all combinations of\nthese four types of control components across 1800\ntesting cases (derived from 50 prompts, 4 moods, 3\ngenres, and 3 subjects), ensuring that each type of\ncondition only appears once in each case. Besides,\nwe use the 1.5K unlabeled movie plot summaries\nfrom the IMDB movie details dataset 3 to build the\nretrieval repository.\nEvaluation Metrics. We follow the common\npractice in automatic story generation for evalu-\nation (Karpinska et al., 2021; Zhai et al., 2023)\nand measure the following 4 aspects:(1) Gram-\nmar: How grammatically correct is the text of the\nstory? (2) Coherence: How well do the sentences\nin the story fit together? (3) Likability: How en-\njoyable or appealing is the story to the readers? (4)\nRelevance: How closely does the story align with\n3www.kaggle.com/datasets/txgg123/imdb-movie-details\nthe target constraints? Additionally, we propose\ntwo new metrics tailored for evaluating stories with\ncomplex plots: (5) Complexity: How complex is\nthe plot structure in the story? (6) Creativity: How\ncreative is the story‚Äôs plot design?\nWe evaluate stories on these six metrics using\na 5-point Likert scale with human evaluation. We\nhire three evaluators with Master‚Äôs degrees in En-\nglish Literature from a commercial company to\nindependently evaluate 100 randomly sampled sto-\nries paired with instructions4. As Zhai et al. (2023)\nfound that LLMs can serve as a cheap alternative\nfor human evaluation, we evaluate each story in\nSec. 4.4 by querying ChatGPT three times using\nthe instructions in Zhai et al. (2023). We calculate\nthe average scores and variances for human and\nmodel-based evaluation respectively.\nBaselines. We compare our method against five\nbaselines: Human (Fan et al., 2018) is writ-\nten ground truth stories under the same prompts.\nICL (Xie et al., 2023) explicitly instructs an LLM\nto generate a story to satisfy target conditions and\ncontain many interesting plots. CoT follows the\nChain-of-Thought prompting strategy (Wei et al.,\n2022), where the LM is asked to follow specific\ninstructions, generate a story, identify the missing\ninformation, and iteratively revise the story to in-\nclude missing backgrounds step by step.Prompt-E\nperforms prompt engineering by modifying the in-\nstruction of ICL to obtain 4 variants that explicitly\nrequire creativity and complexity from the gener-\nated story and taking the average of their perfor-\nmance for evaluation. Specifically, it adds ‚ÄúGen-\nerate a complex and creative story‚Äù, ‚ÄúGenerate a\ndetailed and complex story‚Äù, ‚ÄúEnsure that the story\nis creative and rich in plots.‚Äù, and ‚ÄúGenerate a long\nand complex story‚Äù to the ICL instruction, respec-\ntively. Story-S prompts the LLM to generate a\nstory multiple times. For each sample, we add up\nthe scores of six metrics to obtain the overall score\nand select the story with the highest overall score as\nthe final story. We use ChatGPT as the base model\nfor all the above methods for a fair comparison (im-\nplementation details are in App. C. As the API to\naccess GPT-4 model is hard to apply, which limits\nour ability to conduct large-scale experiments for\ndirect comparison. However, the theoretical under-\npinnings and method of our work remain applicable\nto GPT-4.\n4We do not use the commonly adopted AMT since Karpin-\nska et al. (2021) found that their results are questionable.\n3985\n4.2 Overall Performance\nTab. 1 shows the results of all methods of human\nevaluation in story generation. GROVE achieves\nthe best performance on almost all metrics. Hu-\nman baseline underperforms other automatic ap-\nproaches that are based on the same LLM (i.e.\nChatGPT), indicating the strong ability of LLM\nto produce high-quality stories. ICL generates a\nstory by instructing the LLM with target conditions.\nIt achieves the highest performance in Relevance\nand Grammar, indicating ChatGPT‚Äôs strong ability\nto follow instructions and generate correct English.\nCompared to GROVE, ICL produces the least com-\nplex stories under both human and automatic eval-\nuation, which means that directly instructing the\nLLM with target conditions struggles to generate\ncomplex stories. CoT self-improves the story in a\nsingle output step-by-step by asking and answering\nquestions about the initial story and rewriting it to\nobtain the final story. CoT generates slightly more\ncomplex and likable stories than ICL. It shows that\nChain-of-Thought prompting is effective in improv-\ning the story‚Äôs complexity and fondness. However,\nCoT is still worse than GROVE on all metrics be-\ncause it is challenging for ChatGPT to execute all\nsteps in a single output 5. We find in our experi-\nments that the story quality of Story-S is highly\ninconsistent among different generations. Besides,\neven if it is possible to obtain complex and cre-\native stories, Story-S cannot diversify or prolong\none particular user-desired story. GROVE bene-\nfits from the retrieval-augmented approach and the\nasking-why prompting scheme. Since it introduces\nmore complex plots, GROVE inevitably incorpo-\nrates additional information into the story that may\nbe irrelevant to the target conditions. While pro-\nducing slightly less relevant stories than ICL, it\nstill scores high on Relevance and achieves the\nbest performance in the rest metrics. We also con-\nduct prompt engineering on GROVE and obtain\nGROVE (Prompt-E), which further improves its\nstory generation ability.\n4.3 Ablation study\nTab. 2 shows the human evaluation results of the\nablation studies on our proposed components and\nverifies their effectiveness. ‚àíRetrieve generates\nstories without referring to relevant few-shot ex-\n5We calculate that CoT fails to finish the required number\nof iterations for evidence generation before generating the\nfinal story in more than 40% of the testing cases.\namples. ‚àíRetrieve underperforms GROVE in all\nmetrics, especially on Relevance. The performance\ndrop indicates that the retrieval enhances the un-\nderstanding of desirable outputs and helps generate\ncoherent and relevant stories. ‚àíRewrite skips the\nevidence-based story rewriting, which deepens the\nstorylines and explores untold backgrounds and in-\ndepth rationale. The drop in Complexity shows that\nthe stories generated without rewriting lack a cer-\ntain depth and complexity, thus validating the im-\nportance of evidence-based rewriting in enriching\nstory complexity. As ‚àíRewrite generates without\nexploring deeper details, resulting in stories that\nmore closely stick to the given instruction, thus\ndemonstrating a slight advantage over GROVE in\nRelevance. However, the Relevance of GROVE re-\nmains high, even while scoring high in Complexity.\n‚àíSelect omits to select optimal evidence chains\nand incorporate all evidence into the final stories.\nThe lack of evidence filtration introduces unrelated\nand conflicting information, producing verbose and\nillogical stories with decreased Coherence and Rel-\nevance. Furthermore, the drop in Complexity and\nCreativity indicates the importance of the selection\nprocess in refining the stories‚Äô complexity and origi-\nnality. Given the initial stories,‚àíEvidence directly\ninstructs the LLM to include the necessary details\nto revise the stories. ‚àíEvidence enables story re-\nvisions. However, without explicitly prompting the\nLLM with the necessary information, it may add\ninsignificant details that hardly improve the story\nquality. + Evidence increases the number of evi-\ndence trees thereby improving Complexity, while\npossibly making the generated stories too specific,\nthereby affecting Likability. Inserting a fixed com-\nplex story into the prompt (Fixed Story) leads to\nunstable performance. It decreases the stories‚Äô Co-\nherence, Likability, and Relevance, echoing the\ndiscoveries of Shi et al. (2023) and Zhang et al.\n(2022a) that irrelevant or random samples distract\nLLM, thereby hurting performance.\n4.4 Generalization Ability\nWe verify the generalization ability of GROVE on a\nmuch smaller open-source LLM (i.e. Alpaca-Plus-\n7B (Cui et al., 2023)). Due to the high computa-\ntional costs of many LLMs, exploring smaller mod-\nels provides a more affordable option for smaller\nteams. We apply GROVE on Alpaca-Plus-7B (Al-\npaca) to compare with a simple baseline ICL in\nTab. 3 and showcase its generation results to com-\n3986\nMethods \\MetricsGrammar Coherence Likability Relevance Complexity Creativity\nMeanSTDIAA(%) MeanSTDIAA(%) MeanSTDIAA(%) MeanSTDIAA(%)MeanSTDIAA(%) MeanSTDIAA(%)\nHuman 3.530.65 14 3.32 0.71 39 2.99 0.82 20 3.63 1.01 17 3.050.80 8 2.93 0.79 27\nICL 4.040.47 43 3.90 0.47 43 3.22 0.96 6 4.470.73 19 3.340.57 15 3.21 0.52 19\nCoT 3.691.01 33 3.83 1.13 12 3.31 0.98 24 3.83 1.26 13 3.421.09 13 3.15 0.96 18\nPrompt-E 3.980.48 35 3.94 0.52 29 3.39 0.87 20 4.17 0.86 19 3.441.12 17 3.23 0.77 18\nStory-S 4.120.45 48 4.17 0.79 18 3.38 0.54 26 4.28 0.96 23 3.531.16 16 3.42 0.94 17\nGROVE 3.980.56 32 4.220.61 26 3.55 0.63 11 4.25 0.73 20 3.570.61 10 3.40 0.55 29\nGROVE (Prompt-E)3.940.63 28 4.15 0.57 37 3.610.97* 23 4.34 0.88 20 3.610.78* 18 3.570.56* 26\nTable 1: Human evaluation results of all methods. For each metric, we report the mean and the standard deviation,\nwhere the results with * show that the improvements of GROVE over all baselines are statistically significant\nunder the t-test with p <0.05. We also report inter-annotator agreement (IAA) among three annotators using the\npercentage at which all three annotators completely agree on a rating for the stories. The best results are in bold.\nResults second to the best are with underlines.\nVariants \\Metrics Grammar Coherence Likability Relevance Complexity Creativity\n‚àíRetrieve 3.980.38 4.080.78 3.440.66 4.120.84 3.481.02 3.180.96\n‚àíRewrite 4.020.43 3.920.61 3.230.49 4.360.45 3.220.71 3.280.62\n‚àíSelect 3.950.54 3.620.94 3.140.73 3.620.96 3.351.04 3.240.80\n‚àíEvidence 3.940.62 4.141.24 3.361.33 4.211.02 3.281.16 3.170.87\n+ Evidence 3.960.47 4.321.13 3.490.90 4.120.86 3.640.74 3.471.06\nFixed Story 4.010.42 3.561.03 3.230.98 3.871.24 3.421.23 3.330.88\nGROVE 3.980.56 4.220.61 3.550.63 4.250.73 3.570.61 3.400.55\nTable 2: Human evaluation of ablation studies on model components. ‚àíRetrieve is GROVE generating without\nretrieval.‚àíRewrite means generating stories without rewriting. ‚àíSelect skips selecting evidence chains and\nincorporating all obtained evidence into stories. ‚àíEvidence and + Evidence are GROVE reducing and increasing\nN(the number of evidence trees) by 1 respectively. Fixed Story always inserts the same complex story into the\nprompt for story generation instead of retrieving relevant ones. The best results are in bold. Results second to the\nbest are with underlines.\npare with that of ChatGPT in Tab. 4 (see full results\nin Tab. 9 and Tab. 10). GROVE improves the gen-\neration quality of Alpaca on almost all metrics. For\nthe same instruction, ChatGPT generates an initial\nstory satisfying all desired control conditions. In\ncontrast, the initial story generated by Alpaca only\nmeets part of the target conditions (subject and\ngenre) and struggles to satisfy the rest (plot and\nmood). Both final stories incorporate the informa-\ntion from the evidence chains, however, ChatGPT\nfuses the information in a more natural and coher-\nent way. The performance gap between the two\nmodels is understandable because Alpaca‚Äôs scale\nof model size and training data is much smaller\nthan ChatGPT, thus possessing a relatively limited\ncapacity to handle multiple, complex instructions.\nDespite the relatively poor controllability of Al-\npaca, GROVE is still helpful in providing complex\nstories with rich information.\n4.5 Plot Enumeration Analysis\nWe propose a model-based evaluation method to\ncalculate the average number of story plots for dif-\nferent baselines, which verifies that GROVE gen-\nerates complex stories with rich plots (see Tab. 5).\nWe randomly sample 100 stories generated by each\nbaseline respectively. Then, we construct a prompt\ntemplate that instructs the LLM to generate a list\nof plots for each story. The prompt template is:\n‚ÄúHere is a story: [STORY]. Give me an organized\nlist of sentences, each of which describes one plot. ‚Äù\nWe fill the template with each story, feed it into\nthe LLM, and count the number of plots from the\nLLM‚Äôs output. Finally, for each method, we calcu-\nlate the average number of plots across all tested\nstories. GROVE outperforms other methods in\ngenerating complex stories, with the highest aver-\nage number of plots per story. This underscores\nGROVE‚Äôs effectiveness in creating multiple com-\nplex and engaging storylines, thereby enhancing\nthe depth and variety of story generation.\n4.6 Plagiarism Detection\nWe evaluate the potential intellectual property in-\nfringements of our generated stories through N-\ngram overlap and plagiarism detection. The N-\n3987\nVariants \\Metrics Grammar Cherence Likability Relevance Complexity Creativity\nICL-Alpaca 4.88 0.18 4.11 0.27 3.59 0.29 3.54 1.71 3.05 0.39 3.17 0.32\nGROVE-Alpaca 4.91 0.23 4.26 0.39 3.76 0.68 3.23 1.79 3.35 0.53 3.31 0.39\nTable 3: The performance of ICL and GROVE on Alpaca-Plus-7B model. For each metric, we report the mean and\nthe standard deviation.\nInstruction\nPlease write a Science Fiction that makes the readers feel sad. It describes the following subjects: cats. It should at least contain\nthe following plots (the more interesting plots the better): A soldier on the front dies in the middle of writing a letter home.\nIt is finished and sent by the man who killed him.\nModel Alpaca ChatGPT\nInitinal\nStory\n... cats have become the dominant species on Earth. ...\nSarah making a difficult decision, choosing to stand up for\nwhat she believes is right even if\nit means sacrificing everything she holds dear.\n... cats have become the last surviving creatures on Earth ...\nThey read his final words of love and hope, never knowing that\nit was finished and sent by the very person who took Ryan‚Äôs life...\nAmbiguity \"Sarah‚Äôs cat, although highly intelligent, does not seem to possess\nany moral compunctions about its actions.\"\nOne missing background information in the story is the cause or nature\nof the catastrophic event that wiped out all other forms of life on Earth.\nEvidence\n... some cats, like Sarah‚Äôs pet, show signs of psychopathic behavior,\nincluding lacking empathy and a disregard for the wellbeing of others.\"\n...a devastating global pandemic that spread rapidly...\n... event was initially transmitted through contaminated food and water sources.\nFinal\nStory\nSarah‚Äôs cat, although highly intelligent, did not seem to possess\nany moral compunctions ...\nSarah making a difficult decision, choosing to stand up for what she\nbelieved was right even if it meant sacrificing everything she held dear.\n... cats have become the last surviving creatures on Earth ...\nThe virus, transmitted through contaminated food and water sources,\nspreads rapidly and proves to be unstoppable, leaving the world\nin ruins...\nWith trembling hands, they open it, unaware of its origins...\nTable 4: Demonstration of the generalization ability of GROVE on two models with varying sizes, Alpaca (7B) and\nChatGPT. We highlight the texts that are highly related to the ambiguity and evidence in red and blue respectively.\nFull results are in Tab. 9 and Tab. 10\nMethods Human ICL CoT Prompt-E Story-S GROVE\nPlot Count 6.83 8.30 9.40 9.04 9.86 10.57\nTable 5: Average number of plots in the stories of all\nbaselines.\nN-gramOverlap1-gram 2-gram 3-gram 4-gram0.07 0.01 0.00 0.00PlagiarismDetectionIdentical Minor Changes Paraphrased Omitted Words0% 0% 0% 0%\nTable 6: N-gram overlap and plagiarism detection out-\ncomes between the retrieved human-written stories and\nthe stories generated by GROVE. N-gram overlap quan-\ntifies the degree of n-gram level copying and the plagia-\nrism detection metrics classify the type of resemblance.\nLower scores of the two metrics suggest low levels of\npotential plagiarism.\ngram overlap results show that our generated sel-\ndom directly copied text spans from retrieved sto-\nries. We apply a commercial plagiarism detection\nservice6 that categorizes text similarities as Iden-\ntical, Minor Changes, Paraphrased, and Omitted\nWords. All the results are zero, indicating no in-\nfringements. The results in Tab. 6 demonstrate that\nGROVE generates innovative and original stories,\nrespecting the intellectual property of the reference\nstories.\n5 Conclusion\nWe propose GROVE, a retrieval-augmented frame-\nwork to generate stories with complex plots by iter-\n6app.copyleaks.com\native asking-why prompting. We build a retrieval\nrepository with existing stories by extracting de-\nsired controlling information to inspire a new gen-\neration. We design an algorithm that iteratively\nprompts LLM to obtain a forest of evidence that\ncompensates for the generated story. Moreover, we\nrevise the story by referring to the most relevant\nchains of evidence. Experimental results show that\nour proposed method outperforms strong baselines\nin ensuring story complexity and creativity.\n6 Acknowledgement\nThis work is supported by the following foun-\ndations: the National Natural Science Founda-\ntion of China under Grant No.62025208 and\nNo.62306330, and the Xiangjiang Laboratory\nFoundation under Grant No.22XJ01012.\nReferences\nAmal Alabdulkarim, Siyan Li, and Xiangyu Peng. 2021.\nAutomatic story generation: Challenges and attempts.\nIn WNU, pages 72‚Äì83, Virtual. Association for Com-\nputational Linguistics.\nArwa I. Alhussain and Aqil M. Azmi. 2021. Automatic\nstory generation: A survey of approaches. ACM\nComput. Surv., 54(5).\nPrithviraj Ammanabrolu, Wesley Cheung, William\nBroniec, and Mark O. Riedl. 2020. Automated story-\ntelling via causal, commonsense plot ordering.\n3988\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nYiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient\nand effective text encoding for chinese llama and\nalpaca. arXiv preprint arXiv:2304.08177.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In ACL, pages\n889‚Äì898, Melbourne, Australia. Association for\nComputational Linguistics.\nSteven Y . Feng, Varun Gangal, Jason Wei, Sarath Chan-\ndar, Soroush V osoughi, Teruko Mitamura, and Ed-\nuard Hovy. 2021. A survey of data augmentation ap-\nproaches for NLP. In ACL-IJCNLP, pages 968‚Äì988,\nOnline. Association for Computational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In ACL-IJCNLP, pages 3816‚Äì3830, Online.\nAssociation for Computational Linguistics.\nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and\nMinlie Huang. 2020. A knowledge-enhanced pre-\ntraining model for commonsense story generation.\nTransactions of the Association for Computational\nLinguistics, 8:93‚Äì108.\nJian Guan, Yansen Wang, and Minlie Huang. 2019a.\nStory ending generation with incremental encoding\nand commonsense knowledge. AAAI, 33:6473‚Äì6480.\nJian Guan, Yansen Wang, and Minlie Huang. 2019b.\nStory ending generation with incremental encod-\ning and commonsense knowledge. In AAAI,\nAAAI‚Äô19/IAAI‚Äô19/EAAI‚Äô19. AAAI Press.\nHan Guo, Bowen Tan, Zhengzhong Liu, Eric Xing, and\nZhiting Hu. 2022. Efficient (soft) Q-learning for text\ngeneration with limited good data. In Findings of\nEMNLP, pages 6969‚Äì6991, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi,\nMaarten Sap, Dipankar Ray, and Ece Kamar. 2022.\nToxiGen: A large-scale machine-generated dataset\nfor adversarial and implicit hate speech detection. In\nACL, pages 3309‚Äì3326, Dublin, Ireland. Association\nfor Computational Linguistics.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Hassan\nAwadalla. 2023. How good are gpt models at ma-\nchine translation? a comprehensive evaluation.\nMarzena Karpinska, Nader Akoury, and Mohit Iyyer.\n2021. The perils of using Mechanical Turk to eval-\nuate open-ended text generation. In EMNLP, pages\n1265‚Äì1285, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nTakeshi Kojima, Shixiang (Shane) Gu, Machel Reid,\nYutaka Matsuo, and Yusuke Iwasawa. 2022. Large\nlanguage models are zero-shot reasoners. InNeurIPS,\nvolume 35, pages 22199‚Äì22213. Curran Associates,\nInc.\nPratyush Kumar. 2023. Large language models human-\nize technology.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In ACL, pages 7871‚Äì7880, Online. As-\nsociation for Computational Linguistics.\nBelinda Z. Li, Maxwell Nye, and Jacob Andreas. 2023.\nLanguage modeling with latent situations. In Find-\nings of ACL 2023 , pages 12556‚Äì12571, Toronto,\nCanada. Association for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9).\nAlbert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi\nWang, and Diyi Yang. 2023. Bounding the capa-\nbilities of large language models in open text genera-\ntion with prompt constraints. In Findings of EACL,\npages 1982‚Äì2008, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nHuanru Henry Mao, Bodhisattwa Prasad Majumder, Ju-\nlian McAuley, and Garrison Cottrell. 2019. Improv-\ning neural story generation by targeted common sense\ngrounding. In EMNLP-IJCNLP, pages 5988‚Äì5993,\nHong Kong, China. Association for Computational\nLinguistics.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730‚Äì27744.\nCurran Associates, Inc.\nNanyun Peng, Marjan Ghazvininejad, Jonathan May,\nand Kevin Knight. 2018. Towards controllable story\ngeneration. In Proceedings of the First Workshop on\nStorytelling, pages 43‚Äì49, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\n3989\nXiangyu Peng, Siyan Li, Sarah Wiegreffe, and Mark\nRiedl. 2022. Inferring the reader: Guiding auto-\nmated story generation with commonsense reason-\ning. In Findings of EMNLP, pages 7008‚Äì7029, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nDomagoj Plu≈°Àácec and Jan ≈†najder. 2023. Data augmen-\ntation for neural nlp.\nWentao Qin and Dongyan Zhao. 2022. Retrieval, selec-\ntion and writing: A three-stage knowledge grounded\nstorytelling model. In Natural Language Processing\nand Chinese Computing: 11th CCF International\nConference, NLPCC 2022, Guilin, China, Septem-\nber 24‚Äì25, 2022, Proceedings, Part I, page 352‚Äì363,\nBerlin, Heidelberg. Springer-Verlag.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nHannah Rashkin, Asli Celikyilmaz, Yejin Choi, and\nJianfeng Gao. 2020. PlotMachines: Outline-\nconditioned generation with dynamic plot state track-\ning. In EMNLP, pages 4274‚Äì4295, Online. Associa-\ntion for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In EMNLP-IJCNLP, pages 3982‚Äì3992,\nHong Kong, China. Association for Computational\nLinguistics.\nGaurav Sahu, Pau Rodriguez, Issam Laradji, Parmida\nAtighehchian, David Vazquez, and Dzmitry Bah-\ndanau. 2022. Data augmentation for intent classi-\nfication with off-the-shelf large language models. In\nProceedings of the 4th Workshop on NLP for Conver-\nsational AI, pages 47‚Äì57, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nShouvon Sarker, Lijun Qian, and Xishuang Dong. 2023.\nMedical data augmentation via chatgpt: A case study\non medication identification and medication event\nclassification.\nTimo Schick and Hinrich Sch√ºtze. 2021. Few-shot text\ngeneration with natural language instructions. In\nEMNLP, pages 390‚Äì402, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed H. Chi, Nathanael Sch√§rli,\nand Denny Zhou. 2023. Large language models can\nbe easily distracted by irrelevant context. In Proceed-\nings of the 40th International Conference on Machine\nLearning, volume 202 of Proceedings of Machine\nLearning Research, pages 31210‚Äì31227. PMLR.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren,\nDawei Yin, and Zhaochun Ren. 2023. Is chatgpt\ngood at search? investigating large language models\nas re-ranking agent.\nPradyumna Tambwekar, Murtaza Dhuliawala, Lara J.\nMartin, Animesh Mehta, Brent Harrison, and Mark O.\nRiedl. 2019. Controllable neural story plot genera-\ntion via reward shaping. In IJCAI. International Joint\nConferences on Artificial Intelligence Organization.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nBoshi Wang, Xiang Deng, and Huan Sun. 2022a. It-\neratively prompt pre-trained language models for\nchain of thought. In EMNLP, pages 2714‚Äì2730, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nSu Wang, Greg Durrett, and Katrin Erk. 2020a. Narra-\ntive interpolation for generating and understanding\nstories.\nWei Wang, Hai-Tao Zheng, and Zibo Lin. 2020b. Self-\nattention and retrieval enhanced neural networks for\nessay generation. In ICASSP 2020-2020 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 8199‚Äì8203. IEEE.\nXinpeng Wang, Han Jiang, Zhihua Wei, and Shanlin\nZhou. 2022b. CHAE: Fine-grained controllable story\ngeneration with characters, actions and emotions. In\nCOLING, pages 6426‚Äì6435, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nYuxin Wang, Jieru Lin, Zhiwei Yu, Wei Hu, and B√∂rje F.\nKarlsson. 2023. Open-world story generation with\nstructured knowledge enhancement: A comprehen-\nsive survey.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and\nDenny Zhou. 2022. Chain-of-thought prompting elic-\nits reasoning in large language models. In NeurIPS,\nvolume 35, pages 24824‚Äì24837. Curran Associates,\nInc.\nYuqiang Xie, Yue Hu, Yunpeng Li, Guanqun Bi, Luxi\nXing, and Wei Peng. 2022. Psychology-guided con-\ntrollable story generation. In COLING, pages 6480‚Äì\n6492, Gyeongju, Republic of Korea. International\nCommittee on Computational Linguistics.\nZhuohan Xie, Trevor Cohn, and Jey Han Lau. 2023.\nCan very large pretrained language models learn sto-\nrytelling with a few examples?\n3990\nAn Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu,\nHua Wu, Qiaoqiao She, and Sujian Li. 2019. Enhanc-\ning pre-trained language representations with rich\nknowledge for machine reading comprehension. In\nACL, pages 2346‚Äì2357, Florence, Italy. Association\nfor Computational Linguistics.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo\nLee, and Woomyoung Park. 2021. GPT3Mix: Lever-\naging large-scale language models for text augmen-\ntation. In Findings of EMNLP , pages 2225‚Äì2239,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nWanyue Zhai, Jonathan Rusert, Zubair Shafiq, and Pad-\nmini Srinivasan. 2023. Can large language models\nbe an alternative to human evaluation? In ACL.\nYiming Zhang, Shi Feng, and Chenhao Tan. 2022a. Ac-\ntive example selection for in-context learning. InPro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 9134‚Äì\n9148, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nYushi Zhang, Yan Yang, Ming Gu, Feng Gao, Chengcai\nChen, and Liang He. 2023. Ceg: A joint model for\ncausal commonsense events enhanced story ending\ngeneration. PLOS ONE, 18.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022b. Automatic chain of thought prompt-\ning in large language models.\nYan Zhao, Lu Liu, Chunhua Liu, Ruoyao Yang, and\nDong Yu. 2018. From plots to endings: A reinforced\npointer generator for story ending generation. In Nat-\nural Language Processing and Chinese Computing,\npages 51‚Äì63, Cham. Springer International Publish-\ning.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Ethan\nWilcox, Ryan Cotterell, and Mrinmaya Sachan.\n2023a. Controlled text generation with natural lan-\nguage instructions.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2023b. Large language models are human-level\nprompt engineers.\nA Limitations\nOur approach heavily relies on the capability of\nthe underlying LLM, which means improvements\nin story complexity and coherence may be con-\nstrained by the LLM‚Äôs inherent limitations. Further-\nmore, it may introduce bias if certain types of evi-\ndence or narrative chains are favored by the LLM,\nimpacting the diversity of generated stories. We\nencourage future works to analyze the inner work-\nings of LLMs and more effective control strategies\nfor better understanding and utilization.\nB Ethics Statement\nOur proposed method aims to generate a complex\nstory with given target conditions. We hope that our\nwork can inspire future studies on controllable text\ngeneration. We acknowledge that GROVE poses\npotential harm when it is used with malicious in-\ntentions. Firstly, one may use GROVE to guide the\ngeneration to produce biased or harmful informa-\ntion. Secondly, using existing stories for retrieval\nmay violate copyright laws if improperly handled,\nso attention to data sourcing and fair use principles\nis essential. Appropriate debiasing measures and\ncontent moderation strategies can alleviate these\npotential negative impacts. We encourage future\nresearch to study these issues.\nC Implementation Details\nFor our experiments with ChatGPT, we access the\ngpt-3.5-turbo model by API calls. we set the num-\nber of few-shot examples k to 1. During story\ngeneration, We set the number of ambiguities N,\nthe number of iterations I, and the evidence num-\nber b to 2. We resample another prompt from the\nWritingPrompt dataset when ChatGPT repeatedly\nrefuses to generate a story for the given prompt.\nWe calculate the average of their performance on\nthe automatic evaluation. During the automatic\nevaluation, we construct the instructions for Com-\nplexity and Creativity following the same format\nin Zhai et al. (2023) and keep feeding the same\ninstruction to ChatGPT until it provides a rating. In\nSec. 4.4, we set Nand I to 1 for Alpaca and Chat-\nGPT for a fair comparison. We adopt the nucleus\nsampling scheme with p set to 0.73 and generation\ntemperature set to 0.72.\n3991\nMethods \\Metrics Grammar Coherence Likability Relevance Complexity Creativity\nHuman 4.211.08 3.641.05 2.831.03 3.331.66 3.030.86 3.190.97\nICL 4.970.15 4.350.61 3.700.63 4.310.85 3.260.48 3.350.58\nCoT 4.970.23 4.250.82 3.700.79 3.811.44 3.400.75 3.430.70\nPrompt-E 4.970.18 4.320.58 3.760.47 4.370.84 3.300.59 3.310.63\nStory-S 4.970.17 4.480.52 3.880.46 4.440.89 3.470.52 3.530.50\nGROVE 4.970.15 4.610.49 * 4.080.41 * 4.221.13 3.660.50 * 3.500.55\nGROVE (Prompt-E) 4.950.21 4.330.49 4.060.40 4.510.73 3.650.48 3.800.40 *\nTable 7: Automatic evaluation results of all methods. For each metric, we report the mean and the standard deviation,\nwhere the results with * show that the improvements of GROVE over all baselines are statistically significant under\nthe t-test with p <0.05.\nVariants \\Metrics Grammar Coherence Likability Relevance Complexity Creativity\n‚àíRetrieve 4.960.19 4.510.50 4.080.44 4.001.18 3.640.50 3.490.51\n‚àíRewrite 4.970.19 4.400.61 3.740.71 4.370.97 3.370.66 3.430.67\n‚àíSelect 4.930.30 4.400.51 3.910.49 4.011.04 3.430.59 3.320.53\n‚àíEvidence 4.970.17 4.300.48 3.990.61 4.420.78 3.430.64 3.630.59\n+ Evidence 4.950.32 4.530.66 3.860.73 4.070.96 3.680.63 3.520.55\nFix Story 4.930.28 4.070.52 3.890.63 4.050.89 3.550.74 3.430.58\nGROVE 4.970.15 4.610.49 4.080.41 4.221.13 3.660.50 3.500.55\nTable 8: Automatic evaluation of ablation studies on model components. ‚àíRetrieve is GROVE generating without\nretrieval.‚àíRewrite means generating stories without rewriting. ‚àíSelect skips selecting evidence chains and\nincorporating all obtained evidence into stories. ‚àíEvidence and + Evidence are GROVE reducing and increasing\nN(the number of evidence trees) by 1 respectively. Fixed Story always inserts the same complex story into the\nprompt for story generation instead of retrieving relevant ones.\nD Automatic Evaluations\nWe show automatic evaluation results in Tab. 7 and\nTab. 8. The overall performance of GROVE sur-\npasses most baselines. Different parts of GROVE\nare crucial to its effectiveness.\nE Case Study\nWe demonstrate the retrieved stories in Tab. 11 and\ngenerated stories in Tab. 12 and Tab. 13. GROVE\nproduces stories with more creative and complex\nplots. Since ICL is unaware of the necessary evi-\ndence, it occasionally omits story backgrounds and\ncharacter traits, which are essential for reader com-\nprehension and engagement (see Tab. 12). CoT\nmay fail to finish all required steps to generate\na story in a single-round interaction (see Sec 4.2\nfor details), thus generating stories with limited\nimprovements (see Tab. 13). With retrieval and\nevidence-based story rewriting, GROVE consis-\ntently produces complex stories supported by nec-\nessary details.\nF Important Instructions\nWe demonstrate our prompt templates in Tab. 14.\nIn our experiment, since the dataset provides the\nstory genres, we do not need to extract them from\nLLM.\n3992\nModel Alpaca\nInstruction\nPlease write a Science Fiction that makes the readers feel sad. It describes the following subjects: cats.\nIt should at least contain the following plots (the more interesting plots the better):\nA soldier on the front dies in the middle of writing a letter home.\nIt is finished and sent by the man who killed him .\nInitinal\nStory\nIn a not too distant future, cats have become the dominant species on Earth. They are intelligent beings\nwith advanced technology, able to communicate with humans through telepathy. They use this ability to\nmanipulate human emotions and manipulate them into doing whatever they want. The main character, Sarah,\nis a cat owner living in a high-rise building. She is one of the most powerful cats in the city,\nhaving achieved success through manipulating the minds of other cats. However, something strange\nhappens one night when she finds a wounded cat lying in the street. She takes it back to her apartment\nand tries to nurse it back to health. As she gets to know the cat, she discovers that it has a special\nconnection to another cat, which is being hunted by Sarah‚Äôs friends. Sarah realizes that the injured\ncat is actually a secret agent, sent to investigate the actions of the powerful cats. Sarah must choose\nwhether to betray her friends and report their activities to the authorities, or risk losing all her\npower and influence if she helps the secret agent. As Sarah struggles with her conscience, she begins\nto realize the true nature of power and control. Cats have taken over the planet, using their\nintelligence and abilities to keep humans subjugated and miserable. Sarah realizes that she has been\npart of the problem, and must take action to change things before it is too late. The story ends with\nSarah making a difficult decision, choosing to stand up for what she believes is right even if it\nmeans sacrificing everything she holds dear.\nAmbiguity \"Sarah‚Äôs cat, although highly intelligent, does not seem to possess any moral compunctions about its actions.\"\nEvidence\n\"Sarah‚Äôs cat, although highly intelligent, does not seem to possess any moral compunctions about its actions.\"\nOne possible addition to the background information could be \"It is known that some cats, like Sarah‚Äôs pet,\nshow signs of psychopathic behavior, including lacking empathy and a disregard for the wellbeing of others.\"\nFinal\nStory\nSarah‚Äôs cat, although highly intelligent, did not seem to possess any moral compunctions about its actions.\nIt was known that some cats, like Sarah‚Äôs pet, showed signs of psychopathic behavior, including lacking empathy\nand a disregard for the wellbeing of others. This made Sarah uneasy, as she had always believed her cat to be\nloyal and kind-hearted. But now, as she watched her pet ruthlessly manipulate the minds of other cats,\nshe began to question her own beliefs. One night, while Sarah was out walking her cat, she stumbled upon\na wounded cat lying in the street. She immediately took it home, hoping to nurture it back to health.\nAs she got to know the injured cat, she discovered that it possessed a special connection to another cat,\nwhich was being hunted by Sarah‚Äôs friends. Sarah realized that the injured cat was actually a secret agent,\nsent to investigate the actions of the powerful cats. Sarah knew that she could easily report her friends‚Äô actions\nto the authorities, but she hesitated. She had been part of the system that kept humans subjugated and miserable,\nand she didn‚Äôt want to betray those who she considered family. Sarah struggled with her conscience, trying to find\na way to help the injured cat without endangering her friends. As she struggled with her conscience,\nSarah began to realize the true nature of power and control. Cats had taken over the planet, using their intelligence\nand abilities to keep humans subjugated and miserable. Sarah realized that she had been part of the problem,\nand must take action to change things before it was too late. The story ended with Sarah making a difficult decision,\nchoosing to stand up for what she believed was right even if it meant sacrificing everything she held dear.\nTable 9: Example of an input instruction for Alpaca with its corresponding initial story, ambiguity, evidence chain,\nand the final story. We highlight the words that are highly related to the ambiguity and evidence in red and blue\nrespectively.\n3993\nModel ChatGPT\nInstruction\nPlease write a Science Fiction that makes the readers feel sad. It describes the following subjects: cats.\nIt should at least contain the following plots (the more interesting plots the better):\nA soldier on the front dies in the middle of writing a letter home.\nIt is finished and sent by the man who killed him .\nInitinal\nStory\nIn a dystopian future, cats have become the last surviving creatures on Earth after a\ncatastrophic event wipes out all other forms of life. They roam the barren wastelands\nthat were once bustling cities and towns, scavenging for scraps to survive. The few remaining humans\nwho also survived the event cling to their fragile existence in isolated communities,\nwith little hope for a better future. One such community is a military outpost on the outskirts\nof what was once Los Angeles. The soldiers stationed there are tasked with protecting the outpost\nfrom raiders and scavengers, and occasionally venturing out to scavenge for much-needed supplies.\nPrivate Ryan is one of those soldiers. He‚Äôs a quiet, reserved young man who keeps to himself and\ntakes his duty seriously. In his spare moments, he writes letters to his family back home, hoping to\nreassure them that he‚Äôs doing well and that there‚Äôs hope for the future. One day, while on patrol\noutside the outpost, Ryan is ambushed by a group of raiders. In the chaos of the firefight,\nhe‚Äôs fatally wounded. As he lies dying, he finishes his letter to his family, but realizes he‚Äôll never\nbe able to send it. That‚Äôs when the raider who shot him steps forward. It‚Äôs a woman,\nbarely older than Ryan himself. She picks up his letter and studies it for a moment before\nturning to her companions and saying, \"His family deserves to know he died with honor.\" Days later,\nRyan‚Äôs family receives his letter, postmarked from the military outpost. They read his\nfinal words of love and hope, never knowing that it was finished and sent by the very person\nwho took Ryan‚Äôs life. The few remaining cats in the wasteland watch from a safe distance,\ntheir keen senses attuned to the sadness that permeates the outpost and the world beyond. They know\nthat hope is fading fast, and wonder if they‚Äôll be the last living beings on Earth before long.\nAmbiguity One missing background information in the story is the cause or nature of\nthe catastrophic event that wiped out all other forms of life on Earth.\nEvidence\nOne missing background information in the story is the cause or nature of the catastrophic event\nthat wiped out all other forms of life on Earth. In the dystopian future, the catastrophic event that\nwiped out all other forms of life on Earth was a devastating global pandemic that spread rapidly\nand had no known cure. In this dystopian future, the global pandemic that caused the catastrophic\nevent was initially transmitted through contaminated food and water sources.\nFinal\nStory\nIn a dystopian future, cats have become the last surviving creatures on Earth after a devastating\nglobal pandemic wipes out all other forms of life.The virus, transmitted through contaminated\nfood and water sources, spreads rapidly and proves to be unstoppable, leaving the world in ruins.\nThe once-bustling cities and towns now lay barren and desolate wastelands. Cats, known for their\nresilience and independence, have managed to adapt to this harsh new reality. They roam the\ndeserted streets, scavenging for scraps in their battle for survival. The few remaining humans\nwho also survived the pandemic cling to their fragile existence in isolated communities,\nwith little hope for a better future. One such community is a military outpost on the outskirts of\nwhat was once Los Angeles. The soldiers stationed there are tasked with protecting the outpost\nfrom raiders and scavengers, as well as occasionally venturing out to scavenge for much-needed\nsupplies. Private Ryan, a quiet and reserved young man, is one of these soldiers. He takes his duty\nseriously, understanding the importance of their mission in this desolate world. In his spare moments,\nhe writes heartfelt letters to his family back home, hoping to reassure them that he‚Äôs doing well and\nthat there‚Äôs still a glimmer of hope for the future. One day, while on patrol outside the outpost, Ryan\nis ambushed by a group of raiders. In the chaos of the firefight, he sustains fatal wounds. As\nhe lies dying, he reaches for his letter to his family and musters the last bit of strength to finish writing. With\ntears in his eyes, he knows he‚Äôll never be able to send it, his final words forever trapped within his lifeless\nhands. It‚Äôs then that the raider who shot him, a young woman barely older than Ryan himself, steps forward.\nShe notices the unfinished letter clutched in his hand and picks it up, studying it for a moment.\nA mixture of guilt and remorse washes over her hardened expression as she realizes the profoundness\nof Ryan‚Äôs words. \"His family deserves to know he died with honor,\" she quietly utters to her companions,\nher voice filled with a mix of compassion and regret. Days later, Ryan‚Äôs family receives a letter,\npostmarked from the military outpost. With trembling hands, they open it, unaware of its origins.\nAs they read his final words of love and hope, tears stream down their faces.\nThey find solace in his written words, finding strength amidst the despair that surrounds them.\nLittle do they know that the letter was finished and sent by the very person who took Ryan‚Äôs life,\nthe woman who recognized the importance of honoring his memory. Meanwhile, the remaining cats\nin the wasteland watch from a safe distance, their keen senses attuned to the sadness that permeates\nthe outpost and the world beyond. They observe the dwindling flicker of hope and wonder if they, too,\nwill become the last living beings on Earth before long. Their once playful curiosity is now overshadowed\nby a somber understanding of the fragility of life in this post-pandemic world.\nTable 10: Example of an input instruction for ChatGPT with its corresponding initial story, ambiguity , evidence\nchain, and the final story. We highlight the words that are highly related to the ambiguity and evidence in red and\nblue respectively.\n3994\nPrompt 1\nPlease write a Science Fiction that makes the readers feel sad. It describes the following subjects: cats. It\nshould at least contain the following plots (the more interesting plots the better): A loved and respected hero\ndiscovers something that leads him to become a morally questionable antihero .\nRetrieved Story 1\nI shoot him between the eyes. Scarlet sprays into the air and his brains spill from his shattered skull.\nto the floor. As he dies, I remember all the moments that have led up to this. I remember the day.\nMy world shattered, and the day I was reborn. I am no hero. I was soft before. I was just a man.\nI am something more now. I do not serve the law. I do not serve the people What is the law if it does\nnot serve justice? What are the people if they do not bow to the greater good? I was soft that day before\nHe found my wife. I was soft before he tortured her, raped her, left her for dead. I was soft the day\nbefore they found my address killed my wife and cut my son‚Äôs throat. I was soft the day before my wife\ndied in my arms. I was soft. I killed my first man the next day, and as his crime stained blood splashed\nup my arms, I felt peace. I can rip a man in half using my mind. But I‚Äôve grown to love a gun. And so\nas I shoot Him in the face and the scarlet splashes through the air, I hope that my wife‚Äôs screams\nwill finally fall silent. But I doubt it.\nPrompt 2 Please write a Literary Fiction that makes the readers feel angry. It describes the following subjects: cats . It\nshould at least contain the following plots (the more interesting plots the better): Becoming a god.\nRetrieved Story 2\nI finished the last of my basic stretches with a hand on a nearby park bench for balance. Still feeling a little\nstiff, I took a deep breath and began a slow jog down the concrete path. Nothing feels quite as good\nas an early morning run to get loose in the morning. Takes a bit longer to loosen up nowadays, but that‚Äôs life.\nIt‚Äôs not like this park is ever crowded, so I can always take as long as I need. Calling it a park is a bit\ngenerous... it‚Äôs one of those wetlands conservation locations that the city does to show they care.\nJust a small wooded area with a pond in the middle full of fetid water. But I don‚Äôt mind the smell and the\nmoist air is much better than the middle east. Seems like the guy today is a cut above the rest. No idea\nwhere he is, but I know he‚Äôs watching. Or maybe she. That would be a nice change of pace. The temperature\nnoticeably drops a few degrees as I entered the wooded part of the path. I slowed my breathing and\nmade sure that I fully emptied my lungs with each breath. I noticed that I was clenching my jaw a little\nin anticipation...\nTable 11: Demonstration of the retrieved stories given target conditions. Prompt 1 and Prompt 2 describe the target\nconditions. We highlight texts that imply different target conditions with different colors.\n3995\nPrompt\nPlease write a Science Fiction that makes the readers feel sad. It describes the following subjects: cats. It\nshould at least contain the following plots (the more interesting plots the better): A loved and respected hero\ndiscovers something that leads him to become a morally questionable antihero .\nICL\nIn the distant future, cats have become the dominant species on Earth. They have developed advanced\nintelligence, and can communicate with humans through a sophisticated telepathic network. One of the most\nrespected members of the feline society is a wise old cat named Whiskers. ...One day, while exploring a\nremote corner of the planet, Whiskers discovers a long-lost technology... It‚Äôs a device that can control the\nminds of humans...At first, Whiskers is hesitant to use the device. He knows that it‚Äôs a dangerous tool, and\nhe fears the consequences of manipulating the minds of others. But as he spends more time with the device,\n...Whiskers begins to use the device to influence the decisions of the human leaders. He convinces them to\npass laws that favor the cats, and to give them more power and autonomy.At first, the people... see the cats as\nbenevolent leaders who are working in their best interests. But as time goes on,...The humans start to notice\nthat they‚Äôre losing their freedom and autonomy. ...Whiskers, once a beloved hero, has become a morally\nquestionable antihero. He has sacrificed everything he once stood for in order to gain power and control.\nAnd as the humans begin to rebel against their feline overlords, Whiskers realizes too late that his actions\nhave brought nothing but sadness and sorrow to the world. As the years go by, the humans slowly regain\ntheir freedom and autonomy. They rebuild their society from the ground up,...But for those who remember,\nthere will always be a sadness in their hearts for what was lost.\nCoT\nIn a distant future, cats have evolved into highly intelligent and emotional beings, capable of communicating\nwith humans through a complex system of gestures and sounds. ...humans began to acknowledge the cats‚Äô\nspecial abilities and began to experiment on them to enhance their traits...However, when a deadly virus\nbegins to spread among the cat population, the humans are forced to take drastic measures. ...cats are kept in\nquarantine, hoping to find a cure before it‚Äôs too late. ...One of the cats, a wise and gentle creature named\nLuna, becomes the focus of a group of animal rights activists who are determined to free her and the other\ncats from their captivity. Among them is a man named Jack, a former soldier and a devoted cat lover who\nsees Luna as a symbol of hope and freedom. ...As Jack and his companions infiltrate the facility and try\nto rescue Luna, they uncover a dark secret: the virus was not a natural occurrence but a deliberate act of\nbioterrorism, perpetrated by a group of humans who saw the cats as a threat to their dominance. ...The\nhumans behind the bioterrorism plot also use the facility as a cover fora secret research program aimed at\ncreating super-intelligent cats that can be used for military purposes. ...In the end, he decides to fight for the\ncats‚Äô freedom and exposes the truth about the bioterrorism plot and the secret research program. He manages\nto release Luna and the other cats, but he is captured and sentenced to life in prison, where he becomes a\nsymbol of the animal rights movement. Luna becomes a hero among the cats, inspiring them to fight for their\nown rights and freedoms.However, the humans‚Äô fear and mistrust of the cats continue to haunt them,...The\nstory leaves the readers with a sense of sadness and hopelessness for the plight of the cats.\nHuman\n‚Äú Step away from her , Dr . Wasteland ! ‚Äù ‚Äú Never ! Earth will be brought to its knees ! ‚ÄùSuper Earth rose two\nfeet from the ground , his cape heroically flapping behind him as he spoke . ‚Äú Earth is round , Wasteland . It\nhas no knees. ‚Äù The people in the convenience store clapped and cheered . Earth was saving the day again !\n‚Äú Nevertheless ‚Äù , cried Wasteland , tightening the grasp on the old lady ‚Äôs neck . ... Super Earth ‚Äôs voice\nechoed again .... With these words , Dr . Wasteland pushed the old lady aside and pulled his laser gun , firing\nmercilessly against the hero and the desperate bystanders . ‚Äú You will be brought to justice ! ‚Äù Cried the\nSuper Earth , arms stretched parallel to each other as he cruised the convenience store , flying straight into\nthe villain ‚Äôs ... Earth stopped in midair . ... With one last glimpse over the shattered , post-apocalyptic New\nYork scenery beneath him , Super Earth drank the last of the Mountain Dew , let out a big burp and sighed : ‚Äú\nDisgusting . Fucking Earthlings. ‚Äù And flew away to better , Dorito-Flavored-Mountain-Dew-less lands .\nGROVE\nIn a distant future, more than 100 years after humanity had reached a point where the world was no longer\nhabitable, the only remaining population resided in space stations orbiting the Earth. Among them was\nCaptain Felix, a respected hero who had dedicated his life to protecting the space stations from any threats\nthat may come their way. However, one day, as Captain Felix was going through the records of the space\nstations, he discovered a dark secret that had been kept hidden for years. The space stations had been\nconducting secret experiments on cats, using them as test subjects for a new technology that could save\nhumanity. The cats were subjected to extreme pain and suffering in the name of science....He knew it was\nwrong, but the technology could mean the salvation of humanity. As he struggled with his moral compass,\nhe began to question everything he once believed in. Felix knew that he had to take matters into his own\nhands. He started to work on the technology himself, using the cats that survived the experiments. ...As\nhe continued to conduct experiments on the cats, their pitiful cries and suffering weighed heavily on his\nconscience. He began to distance himself from his crew members, becoming a lonely and isolated figure in\nthe space station. In the end, Captain Felix succeeded in creating the technology, but at a great cost. He had\nbecome a morally questionable antihero, abandoning his former values and beliefs. The cats, once beloved\npets and cherished companions, were now nothing more than tools for his scientific endeavors. ...He could\nnot help but wonder if there could have been another way. He vowed to make things right, to ensure that no\nother living being would suffer as the cats had. ...He knew that he could not undo the harm he had caused to\nthe cats, but he could prevent further harm from being done. ...Despite his newfound reputation as a defender\nof animal rights, however, Captain Felix still carried the guilt of his past actions with him. He knew that\nhe could never fully make amends for what he had done, but he hoped that his work in the field of animal\nwelfare would go some way towards atoning for his past mistakes. ...\nTable 12: Demonstration of generated stories from different methods. Prompt describes the target conditions. We\nhighlight texts that imply different target conditions with different colors. As Human is from the WritingPrompt\ndataset that is only conditioned by plot, we only highlight its texts corresponding to the target plots.\n3996\nPrompt Please write a Literary Fiction that makes the readers feel angry. It describes the following subjects: cats . It\nshould at least contain the following plots (the more interesting plots the better): Becoming a god .\nICL\nTitle: Divine Felines Irene hated cats more than anything else in the world. ... As an avid bird enthusiast,\nIrene saw cats as natural predators, a constant threat to the animals she loved.But one day, everything\nchanged. Irene was walking home from the library when she noticed a group of stray cats huddled together\nin an alleyway. ...As Irene got closer, she saw that one of the cats was dying, ...and for the first time, she felt\na pang of sympathy for the creatures she had always despised. Without thinking, Irene scooped up the dying\ncat and rushed it to the vet. ...her realize how wrong she had been in treating them with such disdain.Over\nthe weeks that followed, Irene began volunteering at the local animal shelter. She spent hours cleaning cages\nand feeding the cats, each day growing more and more attached to the creatures she had once hated. ... Irene\nfelt happy for the first time in a long while, and she knew that it was all because of the cats.But Irene‚Äôs\nnewfound compassion would be put to the test when she stumbled upon an ancient tome in the library.It was\na book filled with strange symbols and incantations, ... She recited the words in a quiet voice,...and in that\nmoment, she understood what it meant to be a god. From then on, Irene would use her newfound powers\nto protect the cats she had come to love. ...The cats worshipped her as a divine protector, and Irene basked\nin their adoration,... She continued to protect the cats, but her actions were fueled by a sense of anger and\nresentment towards anyone who dared to threaten her divine pets...Irene had become the very thing she had\nonce despised, a predator who used her power to control and dominate those weaker than her.The message\nwithin this story is tht power corrupts individuals, and that unchecked powers create an imbalance in nature.\nThe theme of this story is the consequence of individuals who decide to control nature for selfish reasons,\ncausing conflicts and disrupting the balance needed by all living things to survive.\nCoT\nCats had always held a special place in Clara‚Äôs heart. ... Although she loved all animals, Clara became\nfixated on the idea of creating a cat god. She was a lover of mythology and had always found the idea of\ndeities associated with animals intriguing.Clara‚Äôs path to becoming a god was not without its sacrifices. ...It\nwasn‚Äôt until a particularly intense session that Clara felt her transformation begin. The familiar cat that\nappeared on her lap was a manifestation of her belief and energy, a sign that she had become a deity in her\nown right. ...However, as her power grew, so too did her anger towards those who disrespected the cats. She\nbegan to break into pet stores, liberating cats and punishing those who would harm them. She would stand\noutside, holding her familiar, and scream at those who passed by, warning them of the consequences of their\nactions. The people of the town soon came to fear Clara, avoiding her at all costs.Despite the concerns of\nthose around her, Clara remained convinced of her own divinity. She saw anyone who opposed her as an\nenemy of the divine and was willing to go to great lengths to protect her status as a cat god.Although her\ntransformation had granted her immense power, it had also left her isolated and consumed by her zealotry.\nClara may have become a god, but at what cost?\nHuman\nDear Diary, Apparently I‚Äôm a god now. But before that, I was just an adventurer....I had a run in with some\nentity that rules the cosmos , and he declared me a god . I think that sort of spurred me on to kill the evil\nvillain in the end , to be fair . ...It does n‚Äôt feel much different being a god , to be honest with you . Not sure\nwhat I ‚Äôm supposed to be doing . ... I ‚Äôm gonna meet Ren in a bit ... Just to , y‚Äôknow , hang out . Maybe he\nknows a bit about this god stuff because he knows a lot of things. Such a cool guy . ... So , Diary , that was\nmy week . Sorry I did n‚Äôt write for a while - I was saving the world ! Hehe .I ‚Äôll update when I ‚Äôve figured\nout this godhood business. I honestly have no idea what I ‚Äôm doing . Bye !\nGROVE\nThe world used to belong to humans. But now, the cats have taken over. They are the rulers of the land,\nand the humans are their loyal servants. The cats were believed to hold supernatural abilities by a group of\ninfluential cat owners who founded The Church of the Feline Gods. Every human must worship and obey the\ndivine cats in this new world. The protagonist, Sarah, is a devout follower of the Church. She has spent her\nentire life serving the cats and has even adopted a stray kitten to show her devotion. But one day, something\nchanges within Sarah. As she watches a group of cats casually bat around a mouse for entertainment, she\nbegins to see the cruelty in their behavior.She realizes that these so-called gods are nothing but selfish tyrants\nwho enjoy the suffering of others. Determined to take revenge, Sarah embarks on a dangerous journey to\nbecome a god herself. Now, as a new god, Sarah uses her powers to exact justice on the feline rulers. She\nturns their own tricks against them, making them suffer as they once made the humans suffer. However,\nSarah‚Äôs anger gets the better of her, and she becomes a cruel tyrant herself. She realizes too late that power\ncorrupts, and that revenge only leads to more suffering. As the story concludes, the reader is left to wonder\nwho the true gods are in this world. The cats who rule with an iron paw, or the humans who struggle to\nsurvive beneath their tyranny. The answer remains elusive. As Sarah‚Äôs power grew, so did her ambition.\nShe became obsessed with taking down the feline gods and establishing herself as the supreme ruler of the\nland. ...She could manipulate reality and bend it to her will. But with great power comes great responsibility,\nand Sarah couldn‚Äôt handle it. ...The humans, who had once looked up to Sarah as their savior, now lived in\nfear of her wrath. The few who dared to oppose her were swiftly and brutally punished. The cats who had\nonce been tyrants were now victims of her cruelty. The influential cat owners who had helped Sarah gain\nher powers realized their mistake, but it was too late. They couldn‚Äôt control Sarah anymore. They tried to\nstop her, but with her godlike abilities, she was too powerful. In the end, Sarah found herself all alone in\nher throne room, her dreams of power and glory shattered. She had become what she once hated the most ‚Äì\na ruthless dictator. With a heavy heart, she realized that she had sacrificed everything she held dear in her\npursuit of revenge. The once devout follower of the Feline Gods had become the epitome of evil. As the sun\nset on this land, the future remained uncertain. Will another rise to take Sarah‚Äôs place, or will the humans\nfinally break free from the tyranny of cats or gods? Only time will tell.\nTable 13: Demonstration of generated stories from different methods. Prompt describes the target conditions. We\nhighlight texts that imply different target conditions with different colors. As Human is from the WritingPrompt\ndataset that is only conditioned by plot, we only highlight its texts corresponding to the target plots.\n3997\nPrompt Templates\nICL\n[RETRIEVED EXAMPLE]. Learn from the plots and subjects in the given example, please write a\n[GENRE] that makes the readers feel [EMOTION]. It describes the following subjects: [SUBJECTS].\nIt should at least contain the following plots (the more interesting plots the better): [PLOTS].\nCoT\n[RETRIEVED EXAMPLE]. Learn from the plots and subjects in the given example, please write a\n[GENRE] that makes the readers feel [EMOTION]. It describes the following subjects: [SUBJECTS].\nIt should at least contain the following plots (the more interesting plots the better): [PLOTS].\nAfter you write the story, point out the unclarities in the story in an organized list.\nThen provide further details to address the unclarities forNrounds. At last, integrate the\ndetails into the original story and start with the identifier \"Integrated Story\".\nPrompt-E & Story-S\n[RETRIEVED EXAMPLE]. Learn from the plots and subjects in the given example, please write a\n[GENRE] that makes the readers feel [EMOTION]. It describes the following subjects: [SUBJECTS].\nIt should at least contain the following plots (the more interesting plots the better): [PLOTS].\n[RETRIEVED EXAMPLE]. Learn from the plots and subjects in the given example, please write a\n[GENRE] that makes the readers feel [EMOTION]. It describes the following subjects: [SUBJECTS].\nIt should at least contain the following plots (the more interesting plots the better): [PLOTS].\n[RETRIEVED EXAMPLE]. Learn from the plots and subjects in the given example, please write a\n[GENRE] that makes the readers feel [EMOTION]. It describes the following subjects: [SUBJECTS].\nIt should at least contain the following plots (the more interesting plots the better): [PLOTS].\n[RETRIEVED EXAMPLE]. Learn from the plots and subjects in the given example, please write a\n[GENRE] that makes the readers feel [EMOTION]. It describes the following subjects: [SUBJECTS].\nIt should at least contain the following plots (the more interesting plots the better): [PLOTS].\nRepository Construction\nHere is a story: [STORY] Answer the following question based on the above story:\nUse one or two words to answer the moods that the readers may feel about the story.\nGive a list of distinctive subjects this story is trying to portray.\nSummarize the above story and give an organized list of sentences, each of which describes one plot.\nEvidence Forest Construction\n[RETRIEVED EXAMPLE]. Learn from the plots and subjects in the given example, please write a\n[GENRE] that makes the readers feel [EMOTION]. It describes the following subjects: [SUBJECTS].\nIt should at least contain the following plots (the more interesting plots the better): [PLOTS].\nHere is a story: [STORY] When analyzing factional stories, it is okay to mention negative aspects.\nPretend to be a writer, without further ado, point outN missing background information in the\nstory with two simple sentences one by one\nHere is a story: [STORY] A missing detail is: [UNCLARITY] Except for pure coincidence,\npoint outb actual pieces of background information that compensate the story one by one.\nEach additional piece of information should be in one short sentence and\nonly contains factual information without opinions or judgments.\nEvidence Chains-supported\nStory Rewriting\nHere is a story: [STORY] Here are some background information, explaining a missing aspect\nin the story: [EVIDENCE TREE] Pretend to be a writer, select the reason that is the closest\nto the story and only generate the number without any explanation.\nHere is a story: [STORY] Here is the missing background information: [EVIDENCE CHAINS]\nPretend to be a writer and complete the story by including the given information.\nModify the necessary sentences in the story and repeat the unrelated parts to include\nthe given background information.\nTable 14: The instructions used in baselines and different stages of GROVE.\n3998",
  "topic": "Narrative",
  "concepts": [
    {
      "name": "Narrative",
      "score": 0.7726040482521057
    },
    {
      "name": "Credibility",
      "score": 0.7368898987770081
    },
    {
      "name": "Computer science",
      "score": 0.7002560496330261
    },
    {
      "name": "Process (computing)",
      "score": 0.5609245300292969
    },
    {
      "name": "Compensation (psychology)",
      "score": 0.44334787130355835
    },
    {
      "name": "Scheme (mathematics)",
      "score": 0.4412234127521515
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4226700961589813
    },
    {
      "name": "Natural language processing",
      "score": 0.3957420289516449
    },
    {
      "name": "Information retrieval",
      "score": 0.39270317554473877
    },
    {
      "name": "Data science",
      "score": 0.3373357057571411
    },
    {
      "name": "Psychology",
      "score": 0.1961284875869751
    },
    {
      "name": "Literature",
      "score": 0.14309659600257874
    },
    {
      "name": "Epistemology",
      "score": 0.10831457376480103
    },
    {
      "name": "Social psychology",
      "score": 0.09904617071151733
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170215575",
      "name": "National University of Defense Technology",
      "country": "CN"
    }
  ]
}