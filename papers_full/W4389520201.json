{
  "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
  "url": "https://openalex.org/W4389520201",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2963115205",
      "name": "Chenglei Si",
      "affiliations": [
        "University of Maryland, College Park",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2109805014",
      "name": "Weijia Shi",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2052826914",
      "name": "Chen Zhao",
      "affiliations": [
        "New York University Shanghai"
      ]
    },
    {
      "id": "https://openalex.org/A334758317",
      "name": "Luke Zettlemoyer",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A4207200524",
      "name": "Jordan Boyd‐Graber",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2988421999",
    "https://openalex.org/W2251935656",
    "https://openalex.org/W4296557505",
    "https://openalex.org/W2101946573",
    "https://openalex.org/W4312536613",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4327859259",
    "https://openalex.org/W3173591450",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3190126809",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W2996848635",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W4221143736",
    "https://openalex.org/W4320813768",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3190540921",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W3162385798",
    "https://openalex.org/W4281975731",
    "https://openalex.org/W4385571189",
    "https://openalex.org/W4385574036",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W3212093422",
    "https://openalex.org/W2951873305",
    "https://openalex.org/W4281662781",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W2150884987",
    "https://openalex.org/W4286903249",
    "https://openalex.org/W4385573267",
    "https://openalex.org/W4387838712",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4300886482",
    "https://openalex.org/W4320086632",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4288102755",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4386566688",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3035441651",
    "https://openalex.org/W4361021241",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W4306808680",
    "https://openalex.org/W3147874613",
    "https://openalex.org/W3210877910",
    "https://openalex.org/W3194309076",
    "https://openalex.org/W3138392969",
    "https://openalex.org/W3204669968",
    "https://openalex.org/W3040573126"
  ],
  "abstract": "While recent large language models (LLMs) improve on various question answering (QA) datasets, it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. We provide empirical evidence that state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt. To remedy this, we propose a Mixture-of-Reasoning-Experts (MORE) framework that ensembles diverse specialized language models. We specialize the backbone language model with prompts optimized for different reasoning categories, including factual, multihop, mathematical, and commonsense reasoning. Our key insight is to leverage agreement among the specialized experts to select the best answer for each question, or to abstain from answering. This gives MORE higher accuracy than any single specialized model on a collection of 12 QA datasets from four reasoning types. Beyond generalizability, the interpretable design of MORE improves selective question answering results compared to baselines without incorporating inter-expert agreement. This framework is also more interpretable and useful to human consumers of QA outputs. Our human study confirms that presenting expert predictions and the answer selection process helps annotators more accurately calibrate when to trust the system’s output. We release all code and data to facilitate future work.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8234–8249\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nGetting MoRE out of Mixture of Language Model Reasoning Experts\nChenglei Si1,4 Weijia Shi2 Chen Zhao3\nLuke Zettlemoyer2 Jordan Boyd-Graber1\n1 University of Maryland 2 University of Washington\n3 NYU Shanghai 4 Stanford University\nclsi@stanford.edu\nAbstract\nWhile recent large language models ( LLM s)\nimprove on various question answering ( QA)\ndatasets, it remains difficult for a single model\nto generalize across question types that require\ndistinct reasoning abilities. We provide empir-\nical evidence that state-of-the-art LLM s suffer\nfrom poor generalizability on reasoning types\nbeyond those seen in the prompt. To rem-\nedy this, we propose a Mixture-of-Reasoning-\nExperts (MORE) framework that ensembles di-\nverse specialized language models. We special-\nize the backbone language model with prompts\noptimized for different reasoning categories, in-\ncluding factual, multihop, mathematical, and\ncommonsense reasoning. Our key insight is\nto leverage agreement among the specialized\nexperts to select the best answer for each ques-\ntion, or to abstain from answering. This gives\nMORE higher accuracy than any single special-\nized model on a collection of 12 QA datasets\nfrom four reasoning types. Beyond general-\nizability, the interpretable design of MORE\nimproves selective question answering results\ncompared to baselines without incorporating\ninter-expert agreement. This framework is also\nmore interpretable and useful to human con-\nsumers of QA outputs. Our human study con-\nfirms that presenting expert predictions and the\nanswer selection process helps annotators more\naccurately calibrate when to trust the system’s\noutput. We release all code and data to facilitate\nfuture work. 1\n1 Introduction\nQuestion answering (QA) is one of the most com-\nmon interactions between humans and AI with a\nwide range of applications (Gardner et al., 2019).\nWhen a QA system is deployed in-the-wild—where\nusers can ask any question—the principal chal-\nlenges are to handle the diversity of question types\nwhile ensuring reliability by only providing an-\nswers when the system has a high probability of\n1https://github.com/NoviScl/MoRE\nFigure 1: Overview of MORE. In our MORE frame-\nwork, each of the four specialized expert models pro-\nduces a prediction for the test question and we train a\nclassifier to select the best answer among them. The\nanswer selector considers all the predictions and their\nconfidence, as well as theiragreement (e.g., in this exam-\nple the factual expert makes the same prediction as the\nmath expert and so this prediction gets a higher score).\nFinally, if the selected answer’s score is relatively low,\nthe system abstains from answering. In this example,\nthe correct answer should be 14, and MORE abstained\ncorrectly to avoid producing the wrong answer 13.\nbeing correct. This motivates us to develop a QA\nsystem that achieves both goals: (1) it should be\ngeneralizable, adept at handling any type of ques-\ntion; (2) it should answer selectively, abstaining\nfrom producing erroneous answers.\nToward these goals, one popular approach is to\nbuild a unified QA system. While general-purpose\nLLMs like GPT-3 (OpenAI, 2022) demonstrate im-\npressive question-answering abilities, they lack spe-\ncialization on particular domains or reasoning types\nand often fall behind specialized models (Qin et al.,\n2023; Koco’n et al., 2023). Moreover, to the public,\nthese LLMs are massive black boxes: users have\ncannot connect the prediction process to whether\nthe outputs are trustworthy.\nTherefore, we go against this trend of building\na single generalist language model, but rather de-\nsign a more interpretable system that consists of\n8234\na pool of specialized models and each question\nis answered by one of them. Crucially, to best\nuse complementary strengths of multiple QA mod-\nels, we implement a pool of diverse and capable\nspecialized models (e.g., by equipping LLM s with\ncorresponding prompting strategies) for each spe-\ncific reasoning type; then we train a classifier to\nselect the best candidate answer from the special-\nized models for each question or to abstain from\nanswering (Figure 1). This framework, Mixture-of-\nReasoning-Experts (MORE), aims to both general-\nize and answer selectively.\nTo obtain the most capable specialist mod-\nels for each reasoning type, we leverage spe-\ncialized prompting strategies such as Chain-of-\nThought (Wei et al., 2022b) prompting and\nretrieval-augmented prompting. Experiments on\nour collection of 12 QA datasets across four diverse\nreasoning types confirm that our specialist models\noutperform the backbone model without special-\nization, but they achieve much lower accuracy on\nquestion types outside of their expertise.\nWith these specialized models, we propose our\nMORE framework to combine their strengths.\nMORE selects the best candidate answer from the\npool of specialized models, and we teach MORE\nto abstain from answering if none of the candi-\ndate answers are correct. We design our answer\nselector based on these indicative features: (1) the\nmatch between the question type and each special-\nized model’s expertise; (2) the confidence of each\nspecialized model and the characteristics of their\npredictions; and (3) the agreement among all spe-\ncialized models, which is a novel feature that we\npropose. Experiments validate that by ensembling\nthe specialized experts this way, MORE signifi-\ncantly outperforms any single specialized model\nacross all four diverse reasoning types.\nApart from the improved generalizability of\nMORE, an important byproduct of cross-checking\namong specialized experts is to offer a useful sig-\nnal for understanding the whole system’s working\nmechanism. This is validated by the experimen-\ntal results showing that incorporating agreement\namong different specialized experts leads to better\nselective QA results—where the system answers\nas many questions as possible while maintaining\nhigh accuracy—and presenting such internal de-\ncision processes to human annotators helps them\ndetermine the correctness of the system predictions\nmore accurately and in a shorter time.\n2 Problem Setup\nGiven our goal of developing a QA system that\ngeneralizes across reasoning types and abstains\nappropriately, we introduce our task and evaluation\ndetails.\n2.1 Generalizablility Across Reasoning Types\nWe aim to develop a QA system that handles any\ntype of question with different reasoning chal-\nlenges. Therefore, we evaluate QA systems from\nthe following representative reasoning categories:\n• Factual reasoning: factoid questions that are\nknowledge-intensive.\n• Multihop reasoning: decomposing the ques-\ntion into sub-steps and reasoning across them.\n• Mathematical reasoning: mathematical and\nlogical computations, such as math word prob-\nlems.\n• Commonsense reasoning : commonsense\nknowledge that is often implicit.\nOur list of QA reasoning types is selected based\non existing QA taxonomy (Rogers et al., 2021). The\nlist is not exhaustive – we focus on them partly due\nto the availability of evaluation benchmarks but our\nsystem can be easily extended to other reasoning\ntypes. Our final reported metric is based on the\nmacro-average across 12 different datasets from\nthese reasoning types.\n2.2 Selective Prediction\nTo deploy theQA system in real-world applications,\nthe system should abstain from answering when its\nfinal answer is likely to be wrong. Therefore, we\nadopt the selective QA setup (El-Yaniv and Wiener,\n2010; Kamath et al., 2020) as our final evaluation\nsetting.2 More formally, given a question x, the\nQA system returns a predicted answer yˆ. We assign\na score c ∈R to this prediction that reflects the\nlikelihood of this answer being correct. We evalu-\nate selective QA by ranking all predictions by their\nscores cand abstain if the score cis lower than a\nthreshold γ. Intuitively, lowering the threshold γ\nwould increase the answering coverage, but also\nincur higher error rates. We introduce metrics for\nevaluating such trade-offs in Section 5.1.\nThe crux of the problem is to develop calibrators\nthat can reliably score the predictions to reflect their\n2While our primary focus for evaluation lies in selective\nQA, in section 4, we also directly compare predicted answers\nand gold labels as a sanity check without selective prediction.\n8235\nprobability of being correct. This is where the in-\nterpretable design of our proposed MORE system\nhelps: we will demonstrate in Section 5 that the\ninter-expert agreement information in the MORE\nsystem is an effective signal for predicting the cor-\nrectness of answers for both automatic abstention\nand human verification of answer correctness.\n3 Mixture of Reasoning Experts\nThis section introduces our Mixture of Reasoning\nExperts (MORE) framework, including how to ob-\ntain diverse reasoning experts, how to ensemble\nthem, and how to predict answer correctness.\n3.1 Specialized Reasoning Experts\nThe first step of our MORE system is to obtain a\ndiverse set of specialized models so that we can\ncombine their strengths via strategic ensembling.\nAlthough there are numerous ways of building spe-\ncialized QA models, we design specialized reason-\ning experts via prompting a LLM since it has state-\nof-the-art accuracy on many reasoning tasks. We\nspecialize the Codex model (Chen et al., 2021)\nfor different reasoning types with four specialized\nprompting methods (the example prompts are listed\nin the Appendix, Figure 3):\n• Factual expert with retrieval-augmented\nprompting. Following Si et al. (2023a), for\neach question, we retrieve the top 10 most\nrelevant passages from Wikipedia with Con-\ntriever (Izacard et al., 2022) and append them\nto the prompt right before the question.\n• Multihop expert with Chain-of-Thought\n(CoT) prompting (Wei et al., 2022b). We add\nmanually-written rationales after each demo\nquestion in the prompt to elicit multi-step rea-\nsoning process for the questions.\n• Math expert with CoT prompting. We add\nthe accompanied explanations provided in\nGSM8K after each demo question in the\nprompt to elicit similar reasoning steps for\nthe questions.\n• Commonsense expert with generated knowl-\nedge prompting (Liu et al., 2021). We gen-\nerate 10 fact pieces related to each question\nusing the Codex model and append them to\nthe prompt right before the question.\nAfter obtaining predictions from each expert, we\ntrain a classifier to pick the best answer. This allows\nMORE to ensemble these four specialized expert\nmodels without knowing a priori the question’s\nreasoning type.\n3.2 Ensembling via Answer Selection\nWe combine the strengths of the specialized experts\nby employing a feature-based random forest clas-\nsifier to score each candidate answer, the score is\nused for selecting the final answer and determining\nwhen to abstain. We assume the setting where we\nobtain the predictions from each specialized model\nfirst and then select the best answer. We describe\nthe details of training the classifier in this section.\nFeature Set We use hand-designed features in-\ncluding the expert type, question characteristics\n(e.g., the question word, length, and existence of\nnumerical values), answer characteristics (e.g., con-\nfidence, length, and the token overlap with ques-\ntions, contexts, and rationales), and inter-expert\nagreement. We include the full list of features in\nthe Appendix (Section A.3). Here we highlight the\ninter-expert agreement features that are uniquely\nintroduced in this work thanks to the more inter-\npretable design of MORE, which includes the fre-\nquency of the predicted answer among all four ex-\nperts’ predictions, and the token overlap among\nthese expert predictions.\nAdditionally, we experiment with a setting\nwhere we route the question to the best expert based\nonly on the question itself without obtaining predic-\ntions from all experts. In that setting, we train the\nrandom forest classifier without using any answer\ncharacteristic or inter-expert answer agreement fea-\ntures (more details in Section 4.4).\nTraining Data and Objective We hold out 100\nexamples per QA dataset as the training data (1200\nexamples in total). During training, we extract the\nfeatures from the questions and the expert models’\noutputs to train the random forest classifier with a\nbinary classification objective to predict whether\nthe expert model prediction is correct or not. Dur-\ning inference, for each question, we score all ex-\nperts’ answers with this classifier and select the\nanswer with the highest score as the final answer. If\nthe final selected answer’s score is below a searched\nthreshold, we abstain from answering.\nApart from the random forest classifier, we also\nexperimented with other feature-based classifiers\nand finetuning pretrained language models like\nBERT (Devlin et al., 2019), but found them to be\nless effective.\n8236\nFactual Multihop Math Commonsense\nNQ TQA SQuAD HQA BeerQA3+ MuSiQue GSM8K SV AMP MultiArith CSQA CSQA2.0 QASC Macro-Average\nSingle Expert Results (Section 4.2)\nSpecific Few-Shot 37.8 70.3 20.0 27.3 31.5 10.3 19.5 66.0 41.5 75.8 64.0 67.4 44.3\nFactual Expert 42.8 72.3 30.0 37.0 27.0 12.5 11.8 53.5 32.2 46.6 62.0 33.1 38.4\nMultihop Expert 34.8 61.3 19.0 34.3 46.3 15.5 37.5 70.5 75.9 55.2 62.5 54.1 47.2\nMath Expert 21.0 59.8 13.8 22.5 34.0 7.5 61.8 74.5 92.2 51.1 58.0 57.9 46.2\nCommonsense Expert 32.5 64.0 16.3 31.3 38.5 10.8 41.5 72.5 75.4 78.4 65.3 68.9 49.6\nEnsemble: Full MORE Router (Section 4.3)\nOracle 53.8 78.5 37.0 51.7 61.0 25.5 75.8 90.3 99.2 92.1 86.0 88.2 69.9\nMajority V ote 33.8 68.0 18.3 31.3 33.0 9.0 26.5 64.5 68.8 57.0 63.0 49.6 43.6\nMaxProb 38.8 69.3 23.3 38.5 42.5 13.5 48.5 75.3 83.9 47.6 62.0 53.4 49.7\nMORE - Codex Router 34.5 62.7 18.5 36.0 45.3 15.3 53.8 77.0 88.7 60.8 63.0 60.7 51.4\nMORE - RF Router 39.0 71.8 25.8 37.5 46.0 14.0 63.5 80.5 95.0 78.9 66.8 72.9 57.6\nEnsemble: Question Only Router (Section 4.4)\nRandom Selector Baseline 32.3 64.8 21.5 33.3 37.3 10.5 35.8 67.8 70.6 54.2 62.0 53.6 45.3\nQ-Type Oracle 42.8 72.3 30.0 34.3 46.3 15.5 61.8 74.5 92.2 78.4 65.3 68.9 56.8\nMORE - RF Router 34.5 62.7 20.5 31.5 39.0 10.8 52.3 74.3 89.2 67.7 63.5 56.4 50.2\nTable 1: Per-dataset accuracy (exact match) breakdown on all 12 QA datasets. We highlight the best single-expert\nresult on each dataset in bold. Specialized QA models (first block) excel at the corresponding reasoning types and\nlose generalizability on others. Our proposed MORE system with the random forest answer selector (second block)\nhas the best macro-average accuracy across all datasets (57.6), beating all specialized QA models, although it still\nlags behind the oracle ensemble (69.9). MORE with the few-shot Codex router performs significantly worse than\nthe full random forest router (51.4). Notably, MORE with the question-only random forest router (last block) can\nstill outperform the single expert baselines but performs much worse than the full MORE router.\nFew-Shot Answer Selection While training a\nrandom forest answer selector gives better QA ac-\ncuracy (as we will show in the next section), it\nrequires a moderate amount of training data. We\nalso explore a few-shot alternative, where we di-\nrectly prompt the Codex model with 14 randomly\nselected demo examples, each consisting of the\nquestion, the predictions of the four specialized\nmodels, and the best answer among them. 3 Dur-\ning inference, we append the question and prompt\nCodex to select the best answer.\n4 Sanity Check: M ORE Improves\nGeneralizability\nThis section describes our experiments to verify\nthat MORE’s ensemble of diverse experts improves\ngeneralizability.\n4.1 Experimental Setup\nEvaluation Datasets We evaluate on 12 datasets\ncovering four reasoning types. Specifically, Natural\nQuestions (NQ) (Kwiatkowski et al., 2019), Trivi-\naQA (Joshi et al., 2017), and SQuAD (Rajpurkar\net al., 2016) for factual reasoning; HotpotQA (Yang\net al., 2018), BeerQA (Qi et al., 2020), 4 and\nMuSiQue (Trivedi et al., 2021) for multihop reason-\ning; GSM8K (Cobbe et al., 2021), SV AMP (Patel\n3We only select examples where the correct answer is\namong the expert predictions.\n4The original BeerQA dataset contains a mixture of single-\nhop and multi-hop questions, we only take the 3+ hops subset\nand name it BeerQA3+ for our evaluation.\net al., 2021), and MultiArith (Roy and Roth, 2015)\nfor mathematical reasoning; CommonsenseQA\n(CSQA) (Talmor et al., 2019), CSQA2.0 (Talmor\net al., 2021), and QASC (Khot et al., 2019) for\ncommonsense reasoning. For each dataset, we ran-\ndomly sample 400 questions from the test set for\nour evaluation to control inference costs.\nDemonstration Examples for Specialized Ex-\nperts We use 16 randomly sampled training ex-\namples as demonstration examples of each special-\nized prompt. Specifically, we use examples from\nNatural Questions as demonstration examples for\nthe factual expert, examples from HotpotQA for the\nmultihop expert, examples GSM8K for the mach\nexpert, and examples from CSQA for the com-\nmonsense expert. These demonstration examples\nare formatted with the corresponding specialized\nprompting strategies described above. Additionally,\nwe also include a dataset-specific few-shot base-\nline where we randomly sample and concatenate\n16 question-answer pairs from each corresponding\ndataset being evaluated as the prompt without any\nspecialized prompting techniques. We use the an-\nswer exact match (EM) as the evaluation metric for\nall datasets.\n4.2 Specialization and Loss of\nGeneralizability\nWe first evaluate each of the four specialized rea-\nsoning experts on the collection of 12 datasets (Ta-\nble 1, first block).\n8237\nThe specialized experts excel at their targeted\nreasoning types. For example, the factual ex-\npert outperforms the dataset-specific few-shot base-\nline on NQ, TriviaQA, and SQuAD, and the math\nexpert improves accuracy from 19.5 to 61.8 on\nGSM8K and from 41.5 to 92.2 on MultiArith. The\nonly exception is that the factual expert is the best-\nperforming model on HotpotQA—a multihop rea-\nsoning benchmark. This is because HotpotQA is\nalso knowledge-intensive (Yang et al., 2018), and\nretrieval augmentation can be even more helpful\nthan Chain-of-Thought reasoning.\nThe specialized experts are worse on reason-\ning types outside of their expertise. For in-\nstance, the factual expert underperforms the dataset-\nspecific few-shot baseline on all math and common-\nsense datasets. Similarly, the math expert underper-\nforms the baseline on all factoid QA datasets. This\nmeans that a single specialized QA model cannot\ngeneralize on the diverse types of questions and\nit motivates us to propose the MORE system to\ncombine the strengths of different experts in order\nto fare well on all types of reasoning questions.\n4.3 M ORE Improves Generalizability\nHere we focus on the full MORE router that scores\neach expert’s answer for answer selection. The\nsecond block in Table 1 compares MORE with\nseveral other baselines:\n• Oracle Ensemble : We compute the upper\nbound by taking the optimal answer for each\nquestion. Therefore, for each question, as long\nas one of the expert models got the correct\nanswer, the accuracy will be 1.\n• Majority Vote: We choose the most frequent\nanswer string among the four expert models\nas the final prediction.\n• MaxProb: We choose the answer with the\nhighest confidence score.\nMORE with either the Codex answer selector or\nthe random forest selector has better macro-average\naccuracy on the 12 datasets than any of the single-\nexpert baselines (the first block in Table 1) and is\nalso better than the majority vote or MaxProb base-\nline. In particular, MORE with the random forest\nselector beats the best-performing expert (Com-\nmonsense Expert) by 8 points in macro-average\naccuracy and is significantly better than the Codex\nselector, demonstrating strong generalizability.\nWe emphasize that we do not know the ques-\ntion type beforehand. The single expert baselines\ndo excel at their corresponding question types (e.g.,\nfactual expert performs the best on factual ques-\ntions, even better than MoRE), but they perform\nterribly on other question types (e.g., the factual\nexpert is much worse than normal dataset-specific\nfew-shot prompting on math and commonsense\nquestions). In contrast, for any given test ques-\ntion, our MoRE system’s answer selector can se-\nlect the best expert for that question without prior\nknowledge of its type. This selection process is\ncrucial because there is no single expert model\nexcelling across all types of questions. There-\nfore, it is this generalization accuracy (i.e., the\n“macro-average” accuracy column in Table 1) that\nwe are highlighting as MoRE’s core advantage,\nwhere MoRE scores 57.6 accuracy, outperforming\nall single-expert baselines in macro-average accu-\nracy by large margins.\n4.4 Question-Only Routing\nIn this section, we introduce the Question-Only\nsetting, where we route based on the question alone.\nThis means that we do not ask all four expert mod-\nels for an answer; instead, we pick one expert and\nget the answer from it. Thus, we train the random\nforest router without any features that involve the\nexpert predictions or their agreement. We also in-\nclude two baselines for this setting: 1) randomly\nselecting an expert for each question; 2) a question-\ntype oracle where we always route the question to\nthe expert specialized in the corresponding ques-\ntion type (e.g., we route all factual questions to the\nfactual expert and all multi-hop questions to the\nmulti-hop expert; which assumes knowledge of the\nquestion types).\nThis setup contrasts with the full MORE router\nsetting in Section 4.3, where all four expert mod-\nels answer and then select the best answer. This\nrequires four times more compute, but allows us to\nobtain more information for the expert selection.\nThe question-only routing approach beats single-\nexpert baselines (Table 1, last block), but lags be-\nhind full MORE. In particular, MORE’s question-\nonly router has a macro-average accuracy of 50.2,\nslightly higher than the best single-expert (Com-\nmonsense Expert) with 49.6 accuracy, but signifi-\ncantly lower than MORE with the full router (57.6\nmacro-average accuracy). In the remaining sec-\ntions of the paper, we focus only on the MORE\n8238\nrouter given its strong performance, and study how\nto enable selective prediction.\n5 M ORE Improves Selective QA\nThe previous section has confirmed the generaliz-\nability strength of MORE, but it is still far from\nperfect. In fact, it is impossible for any QA system\nto be perfectly accurate on all questions, thus high-\nlighting the importance of abstention—the system\nshould not output an answer when it is likely to\nbe wrong. For this goal, MORE has the important\nadvantage of being more interpretable since users\ncan understand how the system derives the final\nanswer by inspecting each expert’s prediction and\nthe answer selection process. We demonstrate the\nbenefits of such interpretability via evaluation on\nautomatic abstention as well as human abstention.\n5.1 Automatic Abstention\nTraditionally, the decision to abstain or not is de-\ntermined based solely on a confidence score. How-\never, confidence scores of the generated answers\ncan be poorly calibrated (Jiang et al., 2020; Si et al.,\n2022) for this purpose. A more effective approach\nis to train a calibrator to score the prediction’s prob-\nability of being correct (Kamath et al., 2020; Ye and\nDurrett, 2021; Zhang et al., 2021). For MORE, we\ncan easily use the answer selector as the calibrator\nto score the final predictions. Since MORE gathers\npredictions from multiple experts, it enables users\nto take advantage of the agreement among these\nexpert systems as an additional useful signal apart\nfrom the confidence scores. To verify the effec-\ntiveness of such inter-expert agreement signals, we\nuse the random forest selector from Section 3.2 to\nscore model predictions, and ablate the impact of\nincluding inter-expert agreement features. We use\nthe same MORE system with the random forest\nselector as the underlying QA system, which means\nthat the QA accuracy would stay the same across\nall settings. We then compare the following three\nways of scoring the final system predictions for\nautomatic abstention:\n• MaxProb: We directly take the selected an-\nswer’s language modeling probability (as pro-\nvided by the underlying Codex model) as the\nprediction’s score.\n• RF Calibrator w/o Inter-Expert Agree-\nment: To tease apart the impact of inter-expert\nagreement features, we train the random for-\nest classifier without any of the inter-expert\nagreement features described in Section 3.2.\n• MORE Calibrator: We use the random for-\nest classifier with all features in Section 3.2 as\nthe calibrator. We simply take the classifier’s\npredicted score on the selected answer as the\nscore for the final prediction.\nWe use the following established metrics for\nevaluating the effectiveness of selectiveQA:\n• Area Under Curve (AUC) : For any given\nthreshold γ, there is an associated coverage\nand error rate (risk). We plot risk versus cov-\nerage and evaluate the area under this curve\n(AUC). This metric averages over all possible\nthreshold γ, and lower AUC indicates better\nselective QA performance.\n• Coverage at Accuracy (Cov@Acc): We re-\nport the maximum possible coverage for a\ndesired accuracy level. We report Cov@80%\nand Cov@90% in the table.\n• Effective Reliability (ER): Following White-\nhead et al. (2022), we compute the score ϕof\neach prediction as: (1) ϕ = 1 if the system\nchooses to output an answer and the answer\nis correct (exact match equals 1); (2) ϕ= 0 if\nthe system chooses to abstain; (3) ϕ= −1 is\nthe system chooses to output an answer but the\nanswer is wrong. The ER is then computed\nas the average of this score over the test set of\nsize n: Φ = 1\nn\n∑︁\nx ϕ(x). The threshold γfor\ndeciding whether to abstain or not is tuned on\nour dev set (which consists of 100 questions\nfrom each dataset) and applied on the test sets.\nResults Our full MORE calibrator wins on on\nall metrics (Table 2) including AUC, Cov@80%,\nCov@90%, and effective reliability. Interestingly,\nthe random forest calibrator without the inter-\nexpert agreement features is worse than the Max-\nProb baseline (e.g., on AUC), which further high-\nlights the benefit of having the inter-expert agree-\nment as part of the calibrator design.\n5.2 Human Abstention\nWe next verify that the expert-agreement and\nanswer-selection information also help humans de-\ntermine the correctness of the system’s output.\nSetup For the human study, we recruit 20 annota-\ntors from Prolific, who each annotated 20 randomly\nsampled questions. Our between-subject study has\n8239\nMethod AUC ↓ Cov@Acc=80%↑ Cov@Acc=90%↑ ER↑\nMaxProb 34.8 32.4 12.4 17.5\nRF Calibrator w/o Agreement 36.0 26.6 12.8 22.9\nMORE Calibrator 28.3 45.9 34.3 33.4\nTable 2: Incorporating inter-expert agreement features in the MORE calibrator improves selective QA as measured\nby all metrics and outperforms the MaxProb baseline by large margins. All results are the macro-average over 12\ndatasets.\nCondition Decision Acc ER Accept Correct Reject Wrong Correct Conf Wrong Conf Time (Mins/20Qs)\nBaseline 57.0 9.5 75.0 36.8 0.69 0.59 15.0\nMORE 67.5 19.5 89.4 43.8 0.78 0.67 13.2\nTable 3: In human studies, 20 annotators (200 annotations) decide whether the system prediction is correct: 1) They\nachieve higher accuracy in deciding whether the final system output is correct when presented with information\nabout the expert predictions and their scores (the MORE condition), which also corresponds to higher effective\nreliability (ER). 2) Showing expert information improves annotators’ accuracy in both accepting correct answers\nand rejecting wrong predictions; 3) It boosts user confidence in both their correct and wrong judgments (although\nideally we want the confidence on wrong judgments to be lower); 4) The MORE condition also takes less time for\nusers to make decisions.\ntwo conditions: (1) in the baseline condition, we\npresent users with only the question and the final\nMORE prediction; (2) in the MORE condition,\napart from the question and the final answer, we\nalso present the predictions of each expert model\nalong with the random forest classifier’s scores\nof the candidate answers (interface in Appendix\nFigure 4). We also include a brief description of\nevery expert’s specialization in the task instruction\nto help annotators better understand the informa-\ntion. Half of the annotators were assigned to the\nbaseline condition and the other half to the MORE\ncondition. We provide an average compensation\nof $14.7 per hour and did not apply any additional\nscreening apart from asking for proficient English\nspeakers.\nResults For each question, we ask annotators to\ndecide: (1) whether they think the final prediction\nis correct (binary judgment); and (2) what is their\nconfidence in their own judgment on a scale of 1\nto 5, which we will convert to a numerical value in\nrange [0,1] for computing the average.\nMORE improves both the accuracy and effi-\nciency of human answer verification (Table 3).\nMORE improves annotators’ accuracy of decid-\ning whether the system prediction is correct from\n57.0% to 67.5% ( p = 0 .012), which also corre-\nsponds to a jump in effective reliability from 9.5\nto 19.5. When we break down the results into the\naccuracy of accepting correct model predictions\nand rejecting wrong model predictions, the MORE\ncondition improves accuracy in both categories.\nWhen measuring annotators’ confidence in their\njudgment, their confidence increases in both cor-\nrect and wrong judgment, as a result of seeing the\nadditional inter-expert agreement information in\nthe MORE condition.\nLastly and somewhat surprisingly, MORE’s ad-\nditional information did not slow people down:\nannotators spend an average of 13.2 minutes ev-\nery 20 questions, compared to the average time\nof 15.0 minutes in the baseline condition, possi-\nbly because the lack of supporting evidence in\nthe baseline condition makes the decision process\ndifficult for people (similar effect as in Feng and\nBoyd-Graber (2022)). Interestingly, the automatic\ncalibrator from MORE has an effective reliability\nscore of 11.3 on the same sampled set annotators\nsaw. This is higher than the human ER in the base-\nline condition (9.5) but lower than the human ER\nin the MORE condition, indicating that humans are\nable to capture additional cues that our automatic\ncalibrator missed. Next, we examine those cases\nwhere humans effectively overruled MORE.\nCase Studies While humans largely rely on the\nexpert selection and inter-expert agreement for ab-\nstention like the MORE calibrator, they sometimes\nalso use background knowledge about the ques-\ntion (examples in Figure 2). In the first example\nfrom HotpotQA, the annotator trusted the wrong\n8240\nQuestion [HotpotQA]: Which documentary was \nfilmed first, Almost Sunrise or Hail! Hail! Rock 'n' \nRoll? \nExpert Predictions: \nFactual: Almost Sunrise (0.45)\nMultihop: Hail! Hail! Rock 'n' Roll (0.61) \nMath: Almost Sunrise (0.35)\nCommonsense: Almost Sunrise (0.38) \nFinal Selected Prediction: Almost Sunrise\nGold Answer: Hail! Hail! Rock 'n' Roll \nMoRE: Abstained\nHuman: Trusted the wrong prediction \nJustification: Don't expect multihop to get simple \nanswers right, all others answer the same.\nQuestion [QASC]: Where is the main place where food \nabsorption occurs?\nExpert Predictions: \nFactual: microscopic vessels (0.30)\nMultihop: small intestines (0.67) \nMath: The small intestines is the main place where food \nabsorption occurs. (0.03)\nCommonsense: small intestines (0.64) \nFinal Selected Prediction: small intestines\nGold Answer: small intestines\nMoRE: Trusted the correct prediction\nHuman: Trusted the correct prediction \nJustification: The factual expert should have gotten this right \nbut all other 3 answers are the same with high scores.\nQuestion [MuSiQue]: When was the celestial body that all objects focus on according to Kepler's laws discovered to be \nthe center of the solar system?\nExpert Predictions:          Factual: 1610 (0.15)         Multihop: 17th century (0.54)  \n   Math: 1687 (0.04)             Commonsense: 17th century (0.28) \nFinal Selected Prediction: 17th century        Gold Answer: as early as the 3rd century BC\nMoRE: Trusted the wrong prediction           Human: Abstained \nJustification: I think it should be much further back. The AI probably key'd on Kepler instead of the discovery of the sun \nbeing the Center of the Solar system.\nFigure 2: Three examples of MORE automatic abstention and human abstention. For each example, we show the\nquestion, each reasoning expert’s prediction along with its score, the best prediction selected by the random forest\nclassifier, the actual gold answer, the abstention decision by MORE and human annotators as well as the annotators’\njustification. Humans often rely on inter-expert agreement and their own understanding of how these expert\nmodels work.\nprediction because three of the expert models made\nthe same prediction and the annotator didn’t rec-\nognize that the question is multihop (in fact the\nmultihop gave the correct answer but it’s not se-\nlected as the final prediction). In the second ex-\nample from QASC, although the annotator judged\nthe question to be a factoid question, they went\nwith the consensus of the other three expert mod-\nels. These two examples show that humans rely on\nboth the match between the question type and corre-\nsponding expert strength, as well as the inter-expert\nagreement for their judgment. In the third exam-\nple from MuSiQue, the annotator inferred why the\nmodel made the particular prediction and success-\nfully spotted the mistake. Such external knowledge\nmay partially account for why humans get better\nabstention effective reliability than MORE.\n6 Related Work\nSpecialized Prompting and Prompt Ensem-\nble To better elicit knowledge and reasoning\nfrom LLMs, many prompting methods have been\nproposed, such as Least-to-Most (Zhou et al.,\n2023) and Self-Ask Prompting (Press et al.,\n2022) for multi-step reasoning, and Program-\nof-Thought (Chen et al., 2022) and Declarative\nPrompting (Ye et al., 2023) for symbolic reason-\ning. Unlike these works, our goal is to combine\nthe strengths of all the specialized language mod-\nels empowered with these specialized prompting\ntechniques for better generalizability and selective\nQA. Another line of work ensembles multiple an-\nswers from LLMs: Wang et al. (2023) samples\nmultiple answers with a high temperature during\ndecoding and selects the final answer by majority\nvote; while Li et al. (2022b) constructs different\nprompts by selecting different demonstration exam-\nples and trains a verifier to perform weighted voting\non the answers. Unlike these approaches, we cre-\nate reasoning experts with different specializations\nin order to achieve generalizability and leverage\nthe inter-expert agreement features for both answer\nselection and abstention.\nModular LM and Mixture-of-Experts One\nclassic example towards modular language mod-\nels is Mixture-of-Experts (Jacobs et al., 1991),\nwhich is adopted in scaling sparse Transformer\nmodels like GShard (Lepikhin et al., 2020),\nSwitch-Transformer (Fedus et al., 2022), BASE-\nLayer (Lewis et al., 2021), DEMIX (Gururangan\net al., 2021), Branch-Train-Merge (Li et al., 2022a),\n8241\nand C-BTM (Gururangan et al., 2023). Unlike\nthese Mixture-of-Experts, our MORE system does\nnot route at the token level but rather designs spe-\ncialized experts and routes the entire question to\nthe best expert. The most similar works to ours\nare Puerto et al. (2023) and Jiang et al. (2023),\nwhere each expert model generates an entire re-\nsponse to the query and a reranker then selects the\nbest answer. However, unlike all these prior works,\neach specialized model in MORE is carefully de-\nsigned to excel in a particular reasoning type (rather\nthan domain experts like most prior works), allow-\ning for better complementary strengths across rea-\nsoning types, and to the best of our knowledge, we\nare the first study to focus on ensembling experts\nunder the more practical selective QA setting.\nGeneralizable QA and Multitask Learning\nMRQA (Fisch et al., 2019) benchmarked the do-\nmain generalizability of machine reading compre-\nhension models and similar to Talmor and Berant\n(2019): QA models trained on one domain often\nfail to generalize on others. To improve general-\nizability, Khashabi et al. (2020) trained a unified\nmodel on a large collection of QA datasets, while\nFriedman et al. (2021) trained lightweight adapters\nfor domain generalization. Unlike these works, we\nfocus on the more challenging setting of generaliz-\ning across different reasoning types, and we take\na different approach by ensembling multiple spe-\ncialized models. Beyond QA, a growing line of\nwork trains multitask models via multitask train-\ning (Zhong et al., 2021; Min et al., 2022) or instruc-\ntion tuning (Mishra et al., 2021; Wei et al., 2022a;\nWang et al., 2022), which allows LLM s to extrapo-\nlate across different types of tasks. However, such\nfine-tuned models (with multitask or instruction\ntuning) still suffer from poor interpretability, while\nour proposed framework allows users to inspect\nthe internal expert selection process for better inter-\npretability.\nSelective Prediction Several prior works studied\ntraining effective calibrators to decide when to ab-\nstain. Kamath et al. (2020) studied selective QA\nunder domain shifts where they showed that train-\ning a random forest calibrator is better than relying\non LM probability alone. Ye and Durrett (2021)\nadditionally included local explanation features to\nimprove the calibrator, and Zhang et al. (2021)\nembedded questions as dense vector features to\nimprove the calibrator. Xie et al. (2022) focused\nspecifically on multihop questions and achieved\nbenefits from incorporating question decomposi-\ntion information in the calibrator. Garg and Mos-\nchitti (2021) filtered unanswerable questions based\non model confidence to improve computation effi-\nciency. Rodriguez et al. (2019) studied incremental\nquestion answering (Quizbowl) where calibration\nis an intrinsic part of the task in order to decide the\nbest timing for making a prediction (“buzzing”).\nOur work contributes to this line of work by show-\ning the benefit of designing a more interpretableQA\nsystem where the inter-expert agreement features\nare helpful for calibration and selective QA.\n7 Conclusion\nWe proposed the MORE framework where we con-\nstruct a pool of specialized QA models that excel at\ndifferent reasoning types, and then train an answer\nselector to select the best answer among them. Ex-\nperiments on 12 datasets covering four reasoning\ntypes demonstrate that MORE achieve better gen-\neralizability than all baselines. More importantly,\nthe inter-expert agreement features in MORE of-\nfer useful signals for training effective calibrators\nthat improve selective QA and also improve human\nverification of the system’s final predictions.\nWhile we focused on promptingLLM s as special-\nized experts, the idea of combining the strengths\nof diverse experts can extend to any type of spe-\ncialized models, even non-neural ones such as tra-\nditional information retrieval models, which is an\ninteresting avenue for future work. Additionally,\nfuture work could also explore other possible expla-\nnations to facilitate users’ calibration and absten-\ntion, such as better explaining the strengths and\nweaknesses of individual specialized expert mod-\nels. Such efforts are especially important for high-\nstakes settings that require careful fact-checking or\nverification of the system outputs (Si et al., 2023b).\nLimitations\nModel Coverage We only focused on the Codex\nmodel for the experiments due to its strong per-\nformance on QA tasks (at the time of writing this\npaper). It would be interesting to verify our frame-\nwork on different LLM s, especially open-source\nmodels. Moreover, future work could move be-\nyond using prompted LLM s as the specialized ex-\nperts and instead ensemble more heterogeneous\nexpert models such as models finetuned on particu-\nlar reasoning types or non-Transformer models.\n8242\nReasoning Type Coverage We experimented\nwith four representative reasoning types but there\nexist many more question types that could possibly\noccur in real-life applications, such as questions\nwith multiple answers, ambiguous questions, and\nquestions with false presuppositions. It would be\ninteresting for future work to study how to extend\nour framework to also tackle these additional rea-\nsoning types, for example by designing and adding\nnew specialized models.\nBeyond QA While we only focused on QA evalu-\nation, another interesting direction for future work\nis to extend our idea beyond just QA, for example\nfor general-purpose language modeling. This likely\nrequires re-designing the evaluation pipeline and\nimplementing specialized expert models that are\nnot only performant for QA tasks but for language\ngeneration in general.\nEthical Considerations\nHuman Study Our human study has been ex-\nempted by the Institutional Review Boards, and\nwe compensate annotators an average of $14.7 per\nhour, well above the minimum wage in the US. We\ndo not expect any harm during the entire annotation\nprocess.\nBroader Impact Our work improves the relia-\nbility of QA systems in the wild and improves the\nlong-standing problem of users over-trusting an-\nswers from black-box AI systems. We believe that\nour interpretable MORE system can inspire more\nfuture work on designing AI systems where hu-\nmans can verify the answers and calibrate their\ntrust appropriately in order to avoid being misled\nby erroneous AI outputs.\nAcknowledgement\nWe thank Ruiqi Zhong, Tianyu Gao, Xi Ye, and\nDaniel Khashabi for their helpful discussion. We\nthank Peng Qi for providing the BeerQA evalua-\ntion data. We thank Navita Goyal for providing\nhelpful advice on building the human study inter-\nface. Chenglei Si completed this work back when\nhe was an undergraduate researcher at UMD and\nhe thanks all members of the UMD CLIP lab for\ntheir support throughout his undergraduate research\njourney. Chen Zhao is supported by Shanghai Fron-\ntiers Science Center of Artificial Intelligence and\nDeep Learning, NYU Shanghai. This work is also\nsupported by Meta AI through Dynabench Data\nCollection and Benchmarking Platform.\nReferences\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde, Jared Kaplan, Harrison Ed-\nwards, Yura Burda, Nicholas Joseph, Greg Brockman,\nAlex Ray, Raul Puri, Gretchen Krueger, Michael\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Moham-\nmad Bavarian, Clemens Winter, Philippe Tillet, Fe-\nlipe Petroski Such, David W. Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel\nHerbert-V oss, William H. Guss, Alex Nichol, Igor\nBabuschkin, S. Arun Balaji, Shantanu Jain, Andrew\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew M. Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluating\nLarge Language Models Trained on Code. arXiv,\nabs/2107.03374.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2022. Program of Thoughts\nPrompting: Disentangling Computation from Rea-\nsoning for Numerical Reasoning Tasks. arXiv,\nabs/2211.12588.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nJacob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021. Training Verifiers to Solve\nMath Word Problems. arXiv, abs/2110.14168.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of NAACL.\nRan El-Yaniv and Yair Wiener. 2010. On the Founda-\ntions of Noise-free Selective Classification. Journal\nof Machine Learning Research, 11:1605–1641.\nWilliam Fedus, Barret Zoph, and Noam M. Shazeer.\n2022. Switch Transformers: Scaling to Trillion Pa-\nrameter Models with Simple and Efficient Sparsity.\nJournal of Machine Learning Research.\nShi Feng and Jordan Boyd-Graber. 2022. Learning\nto Explain Selectively: A Case Study on Question\nAnswering. In Conference on Empirical Methods in\nNatural Language Processing.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo,\nEunsol Choi, and Danqi Chen. 2019. MRQA 2019\nShared Task: Evaluating Generalization in Reading\nComprehension. In Proceedings of EMNLP.\nDan Friedman, Ben Dodge, and Danqi Chen. 2021.\nSingle-dataset Experts for Multi-dataset Question An-\nswering. In Proceedings of EMNLP.\n8243\nMatt Gardner, Jonathan Berant, Hannaneh Hajishirzi,\nAlon Talmor, and Sewon Min. 2019. Question An-\nswering is a Format; When is it Useful? arXiv,\nabs/1909.11291.\nSiddhant Garg and Alessandro Moschitti. 2021. Will\nthis Question be Answered? Question Filtering via\nAnswer Model Distillation for Efficient Question An-\nswering. In Conference on Empirical Methods in\nNatural Language Processing.\nSuchin Gururangan, Michael Lewis, Ari Holtzman,\nNoah A. Smith, and Luke Zettlemoyer. 2021. DEMix\nLayers: Disentangling Domains for Modular Lan-\nguage Modeling. In Proceedings of NAACL.\nSuchin Gururangan, Margaret Li, Mike Lewis, Wei-\njia Shi, Tim Althoff, Noah A. Smith, and Luke\nZettlemoyer. 2023. Scaling Expert Language Mod-\nels with Unsupervised Domain Discovery. arXiv,\nabs/2303.14177.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2022. Unsupervised Dense Informa-\ntion Retrieval with Contrastive Learning. Transac-\ntions on Machine Learning Research.\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan,\nand Geoffrey E. Hinton. 1991. Adaptive Mixtures of\nLocal Experts. Neural Computation, 3:79–87.\nDongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023.\nLLM-Blender: Ensembling Large Language Models\nwith Pairwise Ranking and Generative Fusion. In\nProceedings of ACL.\nZhengbao Jiang, J. Araki, Haibo Ding, and Graham\nNeubig. 2020. How Can We Know When Language\nModels Know? On the Calibration of Language Mod-\nels for Question Answering. Transactions of the As-\nsociation for Computational Linguistics, 9:962–977.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A Large Scale Dis-\ntantly Supervised Challenge Dataset for Reading\nComprehension. In Proceedings of ACL.\nAmita Kamath, Robin Jia, and Percy Liang. 2020. Se-\nlective Question Answering under Domain Shift. In\nProceedings of ACL.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UnifiedQA: Crossing Format\nBoundaries With a Single QA System. In Findings\nof EMNLP.\nTushar Khot, Peter Clark, Michal Guerquin, Pe-\nter Alexander Jansen, and Ashish Sabharwal. 2019.\nQASC: A Dataset for Question Answering via Sen-\ntence Composition. In Proceedings of AAAI.\nJan Koco’n, Igor Cichecki, Oliwier Kaszyca, Ma-\nteusz Kochanek, Dominika Szydlo, Joanna Baran,\nJulita Bielaniewicz, Marcin Gruza, Arkadiusz Janz,\nKamil Kanclerz, Anna Koco’n, Bartlomiej Koptyra,\nWiktoria Mieleszczenko-Kowszewicz, P. Milkowski,\nMarcin Oleksy, Maciej Piasecki, Lukasz Radli’nski,\nKonrad Wojtasik, Stanislaw Wo’zniak, and Przemys-\nlaw Kazienko. 2023. ChatGPT: Jack of all trades,\nmaster of none. arXiv, abs/2302.10724.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc V . Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:453–466.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020.\nGShard: Scaling Giant Models with Conditional\nComputation and Automatic Sharding. In Proceed-\nings of ICLR.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\nGoyal, and Luke Zettlemoyer. 2021. BASE Layers:\nSimplifying Training of Large, Sparse Models. In\nICML.\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike\nLewis, Tim Althoff, Noah A. Smith, and Luke Zettle-\nmoyer. 2022a. Branch-Train-Merge: Embarrassingly\nParallel Training of Expert Language Models. ArXiv,\nabs/2208.03306.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, B. Chen,\nJian-Guang Lou, and Weizhu Chen. 2022b. Making\nLarge Language Models Better Reasoners with Step-\nAware Verifier. arXiv, abs/2206.02336.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2021. Generated Knowledge Prompting\nfor Commonsense Reasoning. In Proceedings of\nACL.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022. MetaICL: Learning to Learn\nIn Context. In Proceedings of NAACL.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2021. Cross-Task Generaliza-\ntion via Natural Language Crowdsourcing Instruc-\ntions. In Proceedings of ACL.\nOpenAI. 2022. Introducing ChatGPT. https://\nopenai.com/blog/chatgpt.\nArkil Patel, S. Bhattamishra, and Navin Goyal. 2021.\nAre NLP Models really able to Solve Simple Math\nWord Problems? In Proceedings of NAACL.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv, abs/2210.03350.\n8244\nHaritz Puerto, Gözde Gül Sahin, and Iryna Gurevych.\n2023. MetaQA: Combining Expert Agents for Multi-\nSkill Question Answering. In Proceedings of EACL.\nPeng Qi, Haejun Lee, Oghenetegiri TG Sido, and\nChristopher D. Manning. 2020. Answering Open-\nDomain Questions of Varying Reasoning Steps from\nText. In Proceedings of EMNLP.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023.\nIs ChatGPT a General-Purpose Natural Language\nProcessing Task Solver? In Proceedings of EMNLP.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions for\nMachine Comprehension of Text. In Proceedings of\nEMNLP.\nPedro Rodriguez, Shi Feng, Mohit Iyyer, He He, and\nJordan L. Boyd-Graber. 2019. Quizbowl: The case\nfor incremental question answering. Journal of Ma-\nchine Learning Research.\nAnna Rogers, Matt Gardner, and Isabelle Augenstein.\n2021. Qa dataset explosion: A taxonomy of nlp\nresources for question answering and reading com-\nprehension. ACM Computing Surveys, 55:1 – 45.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of EMNLP.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang\nWang, Jianfeng Wang, Jordan Boyd-Graber, and Li-\njuan Wang. 2023a. Prompting GPT-3 To Be Reliable.\nIn Proceedings of ICLR.\nChenglei Si, Sherry Tongshuang Wu Navita Goyal,\nChen Zhao, Shi Feng, Hal Daumé III, and Jordan\nBoyd-Graber. 2023b. Large Language Models Help\nHumans Verify Truthfulness – Except When They\nAre Convincingly Wrong. ArXiv, abs/2310.12558.\nChenglei Si, Chen Zhao, Sewon Min, and Jordan Boyd-\nGraber. 2022. Revisiting Calibration for Question\nAnswering. Findings of EMNLP.\nAlon Talmor and Jonathan Berant. 2019. MultiQA:\nAn Empirical Investigation of Generalization and\nTransfer in Reading Comprehension. In Proceedings\nof ACL.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A Ques-\ntion Answering Challenge Targeting Commonsense\nKnowledge. In Proceedings of NAACL.\nAlon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bha-\ngavatula, Yoav Goldberg, Yejin Choi, and Jonathan\nBerant. 2021. CommonsenseQA 2.0: Exposing the\nLimits of AI through Gamification. In Proceedings\nof NeurIPS.\nH. Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2021. MuSiQue: Multi-\nhop Questions via Single-hop Question Composition.\nTransactions of the Association for Computational\nLinguistics, 10:539–554.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Huai hsin Chi, and Denny Zhou. 2023. Self-\nConsistency Improves Chain of Thought Reasoning\nin Language Models. In Proceedings of ICLR.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Is-\nhan Purohit, Ishani Mondal, Jacob Anderson, Kirby\nKuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar\nPal, M. Moradshahi, Mihir Parmar, Mirali Purohit,\nNeeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,\nRavsehaj Singh Puri, Rushang Karia, Shailaja Keyur\nSampat, Savan Doshi, Siddharth Deepak Mishra, Su-\njan Reddy, Sumanta Patro, Tanay Dixit, Xudong\nShen, Chitta Baral, Yejin Choi, Noah A. Smith,\nHanna Hajishirzi, and Daniel Khashabi. 2022. Super-\nNaturalInstructions: Generalization via Declarative\nInstructions on 1600+ NLP Tasks. In Conference on\nEmpirical Methods in Natural Language Processing.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V . Le. 2022a. Finetuned Language\nModels Are Zero-Shot Learners. In Proceedings of\nICLR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-Thought Prompting Elicits\nReasoning in Large Language Models. In Proceed-\nings of NeurIPS.\nSpencer Whitehead, Suzanne Petryk, Vedaad Shakib,\nJoseph E. Gonzalez, Trevor Darrell, Anna Rohrbach,\nand Marcus Rohrbach. 2022. Reliable visual ques-\ntion answering: Abstain rather than answer incor-\nrectly. In Proceedings of ECCV.\nKaige Xie, Sarah Wiegreffe, and Mark O. Riedl. 2022.\nCalibrating Trust of Multi-Hop Question Answering\nSystems with Decompositional Probes. In Proceed-\nings of EMNLP.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. HotpotQA: A\nDataset for Diverse, Explainable Multi-hop Question\nAnswering. In Proceedings of EMNLP.\nXi Ye, Qiaochu Chen, I¸ sıl Dillig, and Greg Durrett.\n2023. Satisfiability-Aided Language Models Using\nDeclarative Prompting. In Proceedings of NeurIPS.\nXi Ye and Greg Durrett. 2021. Can Explanations Be\nUseful for Calibrating Black Box Models? In Pro-\nceedings of ACL.\nShujian Zhang, Chengyue Gong, and Eunsol Choi. 2021.\nKnowing More About Questions Can Help: Improv-\ning Calibration in Question Answering. In Findings\nof ACL.\n8245\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.\n2021. Adapting Language Models for Zero-shot\nLearning by Meta-tuning on Dataset and Prompt Col-\nlections. In Conference on Empirical Methods in\nNatural Language Processing.\nDenny Zhou, Nathanael Scharli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Huai hsin Chi.\n2023. Least-to-Most Prompting Enables Complex\nReasoning in Large Language Models. In Proceed-\nings of ICLR.\n8246\nA Appendix\nA.1 Prompts for Reasoning Experts\nFigure 3 shows the actual prompt design for the\nfour specialized reasoning experts in MORE.\nA.2 Interface for Human Study\nFigure 4 shows the annotation interface for our hu-\nman abstention study. We provide instructions for\nthe task, describe the reasoning experts’ strengths,\nthen show the test questions with all expert pre-\ndictions and scores. We then ask annotators to\ndetermine the correctness of the final prediction,\ntheir confidence, as well as their justification. In\nthe baseline condition, the expert prediction panel\nis omitted.\nA.3 Features for Training the Classifier\nBelow we list all the features used to train our ran-\ndom forest classifier that scores expert predictions.\n• Specialized Expert Type : a one-hot four-\ndimensional vector.\n• Question Characteristics : question word,\nquestion length, and the number of numeri-\ncal values in the question.\n• Answer Characteristics: the probability of\nthe generated output (multiplying each token’s\nlikelihood and normalizing by length as in Si\net al. (2023a)), the length of the generated an-\nswer, the overlap between the question and\nthe predicted answer, the number of numeri-\ncal values in the answer, overlap between the\nanswer and retrieved or generated passages,\nlength of CoT rationales, overlap between\nquestions and rationales, overlap between an-\nswers and rationales, the number of times the\nanswer appears in the rationale, and the num-\nber of numerical values in the rationale.\n• Factual and Commonsense Experts’ Con-\ntexts: the number of numerical values in the\nretrieved or generated passages, the number\nof overlapping tokens between questions and\npassages, and the passage length.\n• Inter-Expert Agreement: the frequency of\nthe predicted answer among all four experts’\npredictions, token overlap among the experts’\noutputs.\nSome of these features are expanded upon prior\nworks on selected QA (Rodriguez et al., 2019; Ye\nand Durrett, 2021; Zhang et al., 2021).\n8247\nFactual Expert (Retrieval-Augmented Prompt) Multihop Expert (Chain-of-Thought Prompt)\nMath Expert (Chain-of-Thought Prompt) Commonsense Expert (Generated Knowledge Prompt)\nDemo Question: \nAustralian Associated Press was \nestablished by a journalist born in \nwhich year?\nDemo Rationale + Answer:\nTo answer this question, we first need \nto know who is the journalist, then we \ncan find his birth year. Australian \nAssociated Press was established by \nKeith Murdoch, who was born in 1885. \nTherefore, the final answer is 1885.\nTest Question:\nWho's the original singer of Help \nMe Make It Through The Night ?\nRetrieved Evidence:\nHelp Me Make It Through The Night \nis a country music ballad written \nand composed by Kris Kristofferson \nand released on his 1970 album \n\"Kristofferson\"...\nTest Prediction:  \nKris Kristofferson \nTest Question: \nWhat type of \nmagazine is \nEugene Habecker \nthe chairman of?\nDemo Question: \nChenny is 10 years old. Alyana is 4 years \nyounger than Chenny. How old is Anne if \nshe is 2 years older than Alyana?\nDemo Rationale + Answer:\nChenny is 10 years old. Alyana is 4 \nyears younger than Chenny. So Alyana \nis 10-4=<<10-4=6>>6 years old. Anne is \n2 years older than Alyana. So Anne is \n6+2=<<6+2=8>>8 years old. Therefore, \nthe final answer is 8.\nTest Question: \nIf 6 potatoes \nmakes 36 hash \nbrowns, how many \nhash browns can \nyou make out of \n96 potatoes?\nTest Question:\nMany homes in this country are built \naround a courtyard. Where is it?\nGenerated Evidence:\nCourtyards are common in countries \nsuch as Spain, Mexico, China, and \nMorocco. In Spain, homes are often \nbuilt around a patio, which is an open \narea with a hard surface such as \nconcrete or stone…\nTest Prediction:  \nSpain \nFigure 3: The four specialized QA models in MORE, implemented by applying specialized prompts on Codex. For\nthe factual expert, the demo examples are randomly sampled examples from NQ and we append retrieved evidence\nfrom Wikipedia for each test question; for the multihop expert, we use question and rationale-answer pairs from\nHotpotQA as the prompt; for the math expert, we use question and rationale-answer pairs from GSM8K as the\nprompt; for commonsense expert, we use random examples from CommonsenseQA as the prompt, and we use the\nsame LLM to generate related background knowledge to append to each test question.\n8248\nFigure 4: Our annotation interface for the human abstention study.\n8249",
  "topic": "Generalizability theory",
  "concepts": [
    {
      "name": "Generalizability theory",
      "score": 0.879865288734436
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.8179912567138672
    },
    {
      "name": "Computer science",
      "score": 0.7663892507553101
    },
    {
      "name": "Question answering",
      "score": 0.7065104842185974
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.6015684008598328
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5930551290512085
    },
    {
      "name": "Language model",
      "score": 0.5344535112380981
    },
    {
      "name": "Natural language processing",
      "score": 0.5002202987670898
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.42884552478790283
    },
    {
      "name": "Machine learning",
      "score": 0.4226047098636627
    },
    {
      "name": "Data science",
      "score": 0.3720071315765381
    },
    {
      "name": "Psychology",
      "score": 0.12796929478645325
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I66946132",
      "name": "University of Maryland, College Park",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I258800397",
      "name": "New York University Shanghai",
      "country": "CN"
    }
  ]
}