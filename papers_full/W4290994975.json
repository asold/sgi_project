{
  "title": "Saturated Transformers are Constant-Depth Threshold Circuits",
  "url": "https://openalex.org/W4290994975",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2123573390",
      "name": "William Merrill",
      "affiliations": [
        "New York University",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1898665253",
      "name": "Ashish Sabharwal",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2183947846",
      "name": "Noah A. Smith",
      "affiliations": [
        "Allen Institute",
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4298227433",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W3098666169",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W134510267",
    "https://openalex.org/W2014850486",
    "https://openalex.org/W4234348737",
    "https://openalex.org/W2952744660",
    "https://openalex.org/W4224874866",
    "https://openalex.org/W3104298168",
    "https://openalex.org/W1506378440",
    "https://openalex.org/W6713024538",
    "https://openalex.org/W2972515356",
    "https://openalex.org/W6791675639",
    "https://openalex.org/W3214210379",
    "https://openalex.org/W2889175957",
    "https://openalex.org/W2908802752",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963059228",
    "https://openalex.org/W6788417511",
    "https://openalex.org/W6677900116",
    "https://openalex.org/W6795898621",
    "https://openalex.org/W6771550025",
    "https://openalex.org/W3121462755",
    "https://openalex.org/W4239263507",
    "https://openalex.org/W3014096773",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W3133204183",
    "https://openalex.org/W4214566146",
    "https://openalex.org/W2401138554",
    "https://openalex.org/W4238846128"
  ],
  "abstract": "Abstract Transformers have become a standard neural network architecture for many NLP problems, motivating theoretical analysis of their power in terms of formal languages. Recent work has shown that transformers with hard attention are quite limited in power (Hahn, 2020), as they can be simulated by constant-depth AND/OR circuits (Hao et al., 2022). However, hard attention is a strong assumption, which may complicate the relevance of these results in practice. In this work, we analyze the circuit complexity of transformers with saturated attention: a generalization of hard attention that more closely captures the attention patterns learnable in practical transformers. We first show that saturated transformers transcend the known limitations of hard-attention transformers. We then prove saturated transformers with floating-point values can be simulated by constant-depth threshold circuits, giving the class TC0 as an upper bound on the formal languages they recognize.",
  "full_text": null,
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7976453304290771
    },
    {
      "name": "Computer science",
      "score": 0.7048431038856506
    },
    {
      "name": "Electronic circuit",
      "score": 0.5327767133712769
    },
    {
      "name": "Computer engineering",
      "score": 0.33000248670578003
    },
    {
      "name": "Algorithm",
      "score": 0.32476407289505005
    },
    {
      "name": "Electrical engineering",
      "score": 0.3153771758079529
    },
    {
      "name": "Engineering",
      "score": 0.09074446558952332
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}