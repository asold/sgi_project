{
  "title": "DA-Transformer: Distance-aware Transformer",
  "url": "https://openalex.org/W3170261818",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2108092137",
      "name": "Chuhan Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2142281011",
      "name": "Fangzhao Wu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2117581150",
      "name": "Yongfeng Huang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4288375838",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2983180560",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2931198394",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3092302658",
    "https://openalex.org/W2963652649",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970793364",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2925618549",
    "https://openalex.org/W2903728819",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3034503922",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2970550739",
    "https://openalex.org/W2156387975",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3098649723",
    "https://openalex.org/W3099143320",
    "https://openalex.org/W3117150731",
    "https://openalex.org/W2952357537"
  ],
  "abstract": "Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the relevance between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 2059–2068\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n2059\nDA-Transformer: Distance-aware Transformer\nChuhan Wu† Fangzhao Wu‡ Yongfeng Huang†\n†Department of Electronic Engineering & BNRist, Tsinghua University, Beijing 100084, China\n‡Microsoft Research Asia, Beijing 100080, China\n{wuchuhan15,wufangzhao}@gmail.com, yfhuang@tsinghua.edu.cn\nAbstract\nTransformer has achieved great success in the\nNLP ﬁeld by composing various advanced\nmodels like BERT and GPT. However, Trans-\nformer and its existing variants may not be\noptimal in capturing token distances because\nthe position or distance embeddings used by\nthese methods usually cannot keep the precise\ninformation of real distances, which may not\nbe beneﬁcial for modeling the orders and re-\nlations of contexts. In this paper, we pro-\npose DA-Transformer, which is a distance-\naware Transformer that can exploit the real\ndistance. We propose to incorporate the real\ndistances between tokens to re-scale the raw\nself-attention weights, which are computed by\nthe relevance between attention query and key.\nConcretely, in different self-attention heads\nthe relative distance between each pair of to-\nkens is weighted by different learnable pa-\nrameters, which control the different prefer-\nences on long- or short-term information of\nthese heads. Since the raw weighted real dis-\ntances may not be optimal for adjusting self-\nattention weights, we propose a learnable sig-\nmoid function to map them into re-scaled coef-\nﬁcients that have proper ranges. We ﬁrst clip\nthe raw self-attention weights via the ReLU\nfunction to keep non-negativity and introduce\nsparsity, and then multiply them with the re-\nscaled coefﬁcients to encode real distance in-\nformation into self-attention. Extensive exper-\niments on ﬁve benchmark datasets show that\nDA-Transformer can effectively improve the\nperformance of many tasks and outperform the\nvanilla Transformer and its several variants.\n1 Introduction\nTransformer (Vaswani et al., 2017) has achieved\nhuge success in the NLP ﬁeld in recent\nyears (Kobayashi et al., 2020). It serves as the ba-\nsic architecture of various state-of-the-art models\nlike BERT (Devlin et al., 2019) and GPT (Rad-\nford et al., 2019), and boosts the performance of\nmany tasks like text generation (Koncel-Kedziorski\net al., 2019), machine translation (Vaswani et al.,\n2017), and reading comprehension (Xu et al., 2019).\nThus, the improvement on the Transformer archi-\ntecture would be beneﬁcial for many NLP-related\nﬁelds (Wu et al., 2020a).\nA core component of Transformer is multi-head\nself-attention, which is responsible for modeling\nthe relations between contexts (Yang et al., 2019;\nGuo et al., 2019). However, self-attention is\nposition-agnostic since it does not distinguish the\norders of inputs. Thus, in the vanilla Transformer,\nposition encoding is applied to the input to help\nTransformer capture position information. How-\never, in contrast to recurrent and convolutional neu-\nral networks, it is difﬁcult for vanilla Transform-\ners to be aware of the token distances (Shaw et al.,\n2018), which are usually important cues for context\nmodeling. Thus, several works explored to incor-\nporate token distance information into Transformer.\nFor example, Shaw et al. (2018) proposed to com-\nbine the embeddings of relative positions with at-\ntention key and value in the self-attention network.\nThey restricted the maximum relative distance to\nonly keep the precise relative position information\nwithin a certain distance. Yan et al. (2019) pro-\nposed a variant of self-attention network for named\nentity recognition, which incorporates sinusoidal\nembeddings of relative position to compute atten-\ntion weights in a direction- and distance-aware way.\nHowever, the distance or relative position embed-\ndings used by these methods usually cannot keep\nthe precise information of the real distance, which\nmay not be beneﬁcial for the Transformer to cap-\nture word orders and the context relations.\nIn this paper, we propose adistance-aware Trans-\nformer (DA-Transformer), which can explicitly ex-\nploit real token distance information to enhance\ncontext modeling by leveraging the relative dis-\ntances between different tokens to re-scale the raw\nattention weights before softmax normalization.\n2060\nMore speciﬁcally, since global and local context\nmodeling usually have different distance prefer-\nences, we propose to learn a different parameter in\ndifferent attention heads to weight the token dis-\ntances, which control the preferences of attention\nheads on long or short distances. In addition, since\nthe weighted distances may not have been restricted\nto a proper range, we propose a learnable sigmoid\nfunction to map the weighted distances into re-\nscaled coefﬁcients. They are further multiplied\nwith the raw attention weights that are clipped by\nthe ReLU function for keeping the non-negativity\nand introducing sparsity. We conduct extensive\nexperiments on ﬁve benchmark datasets for dif-\nferent tasks, and the results demonstrate that our\napproach can effectively enhance the performance\nof Transformer and outperform its several variants\nwith distance modeling.\nThe main contributions of this paper include:\n•We propose a distance-aware Transformer that\nuses the real token distances to keep precise\ndistance information in adjusting attention\nweights for accurate context modeling.\n•We propose to use different parameters to\nweight real distances in different attention\nheads to control their diverse preferences on\nshort-term or long-term information.\n•We propose a learnable sigmoid function to\nmap the weighted distances into re-scaled co-\nefﬁcients with proper ranges for better adjust-\ning the attention weights.\n•We conduct extensive experiments on ﬁve\nbenchmark datasets and the results validate\nthe effectiveness of our proposed method.\n2 Related Work\n2.1 Transformer\nTo make this paper self-contained, we ﬁrst brieﬂy\nintroduce the architecture of Transformer, which\nwas initially introduced to the machine translation\ntask (Vaswani et al., 2017). It has become an im-\nportant basic neural architecture of various state-of-\nthe-art NLP models like BERT (Devlin et al., 2019)\nand GPT (Radford et al., 2019). The core compo-\nnent of Transformer is multi-head self-attention. It\nhas h attention heads, where the parameters in each\nhead are independent. For the i-th attention head,\nit takes a matrix H as the input. It ﬁrst uses three\nindependent parameter matrices W(i)\nQ , W(i)\nK , and\nW(i)\nV to respectively transform the input matrix H\ninto the input query Q(i), key K(i) and value V(i),\nwhich is formulated as follows:\nQ(i), K(i), V(i) = HW(i)\nQ , HW(i)\nK , HW(i)\nV . (1)\nThen, it uses a scaled dot-product attention head to\nprocess its query, key and value, which is formu-\nlated as follows:\nAttention(Q(i), K(i), V(i)) = softmax(Q(i)K(i)⊤\n√\nd\n)V(i),\n(2)\nwhere d is the dimension of the vectors in the query\nand key. The outputs of the h attention heads are\nconcatenated together and the ﬁnal output is a lin-\near projection of the concatenated representations,\nwhich is formulated as follows:\nMultihead(Q, K, V) = Concat(head1, ...,headh)WO,\nwhere headi = Attention(Q(i), K(i), V(i)),\n(3)\nwhere WO is an output projection matrix. In the\nstandard Transformer, a position-wise feed-forward\nneural network is further applied to the output of\nmulti-head self-attention network. Its function is\nformulated as follows:\nFFN (x) =max(0, xW1 + b1)W2 + b2, (4)\nwhere W1, W2, b1, b2 are kernel and bias param-\neters. Transformer also employs layer normaliza-\ntion (Ba et al., 2016) and residual connection (He\net al., 2016) techniques after the multi-head self-\nattention and feed-forward neural networks, which\nare also kept in our method.\nSince self-attention network does not distinguish\nthe order and position of input tokens, Transformer\nadds the sinusoidal embeddings of positions to the\ninput embeddings to capture position information.\nHowever, position embeddings may not be opti-\nmal for distance modeling in Transformer because\ndistances cannot be precisely recovered from the\ndot-product between two position embeddings.\n2.2 Distance-aware Transformer\nInstead of directly using the sinusoidal position\nembedding (Vaswani et al., 2017) or the absolute\nposition embedding (Devlin et al., 2019), several\nvariants of the Transformer explore to use the rela-\ntive positions to better model the distance between\ncontexts (Shaw et al., 2018; Wang et al., 2019; Dai\net al., 2019; Yan et al., 2019). For example, Shaw\n2061\net al. (2018) proposed to add the embeddings of\nrelative positions to the attention key and value to\ncapture the relative distance between two tokens.\nThey only kept the precise distance within a certain\nrange by using a threshold to clip the maximum\ndistance to help generalize to long sequences. Dai\net al. (2019) proposed Transformer-XL, which uses\nanother form of relative positional encodings that\nintegrate content-dependent positional scores and\na global positional score into the attention weights.\nYan et al. (2019) proposed direction-aware sinu-\nsoidal relative position embeddings and used them\nin a similar way with Transformer-XL. In addition,\nthey proposed to use the un-scaled attention to bet-\nter ﬁt the NER task. However, relative position\nembeddings may not be optimal for modeling dis-\ntance information because they usually cannot keep\nthe precise information of real token distances. Dif-\nferent from these methods, we propose to directly\nre-scale the attention weights based on the mapped\nrelative distances instead of using sinusoidal po-\nsition embeddings, which can explicitly encode\nreal distance information to achieve more accurate\ndistance modeling.\n3 DA-Transformer\nIn this section, we introduce our proposeddistance-\naware Transformer (DA-Transformer) approach,\nwhich can effectively exploit real token distance\ninformation to enhance context modeling. It uses\na learnable parameter to weight the real distances\nbetween tokens in each attention head, and uses\na learnable sigmoid function to map the weighted\ndistances into re-scaled coefﬁcients with proper\nranges, which are further used to adjust the raw at-\ntention weights before softmax normalization. The\ndetails of DA-Transformer are introduced in the\nfollowing sections.\n3.1 Head-wise Distance Weighting\nSimilar with the standard Transformer, the input\nof our model is also a matrix that contains the\nrepresentation of each token, which is denoted as\nH = [h1, h2, ...,hN ], where N is the length of\nthe sequence. We denote the real relative distance\nbetween the i-th and j-th positions as Ri,j, which\nis computed by Ri,j = |i −j|. We can then ob-\ntain the relative distance matrix R ∈RN×N that\ndescribes the relative distance between each pair of\npositions. In each attention head, we use a learn-\nable parameter wi to weight the relative distance\nFigure 1: The curves of our learnable sigmoid function\nunder different vi.\nby R(i) = wiR, which will be further used to\nadjust the self-attention weights. In our method,\nwe stipulate that a more positive R(i) will amplify\nthe attention weights more strongly while a more\nnegative R(i) will diminish them more intensively.\nThus, a positive wi means that this attention head\nprefers to capture long-distance information, while\na negative wi means that it focuses more on lo-\ncal contexts. By learning different values of wi,\ndifferent attention heads may have different prefer-\nences on capturing either short-term or long-term\ncontextual information with different intensity.\n3.2 Weighted Distance Mapping\nSince the raw weighted distances may not be in the\nproper range for adjusting the attention weights,\nwe need to map them into the re-scaled coefﬁcients\nvia a function ˆR(i) = f(R(i)) that is suitable for\nadjusting the self-attention weights. However, it is\nnot a trivial task to design the functionf(·) because\nit needs to satisfy the following requirements: (1)\nf(0) = 1. We stipulate that zero distances do not\ninﬂuence the self-attention weights. (2) The value\nof f(R(i)) should be zero when R(i) →−∞. This\nrequirement is to guarantee that if an attention head\nprefers to capture local information (wi < 0), the\nlong-distance information should be surpassed. 1\n(3) The value of f(R(i)) should be limited when\nR(i) →+∞. This requirement is to ensure that\nthe model is able to process long sequences with-\nout over-emphasize distant contexts. (4) The scale\nof f(·) needs to be tunable. This aims to help the\nmodel better adjust the intensity of distance infor-\nmation. (5) The function f(·) needs to be mono-\n1Although the raw negative attention weights may be\nraised to 0 by f(·), the model can still surpass these atten-\ntion weights after softmax by increasing the scale of other\nattention weights.\n2062\ntone. To satisfy the ﬁve requirements above, we\npropose a learnable sigmoid function to map the\nweighted relative distances R(i), which is formu-\nlated as follows:\nf(R(i); vi) = 1 + exp(vi)\n1 + exp(vi −R(i)), (5)\nwhere vi is a learnable parameter in this head that\ncontrols the upperbound and ascending steepness\nof this function. The curves of our learnable sig-\nmoid function under several different values of vi\nare plotted in Fig. 1. We can see that the proposed\nfunction satisﬁes all the requirements above. In\naddition, from this ﬁgure we ﬁnd that if vi is larger,\nthe upperbound of the curve is higher, which means\nthat distance information is more intensive. When\nvi = 0, it is in fact identical to the standard sigmoid\nfunction except for the scaling factor of 2. By map-\nping the weighted distances R(i) via the function\nf(·), we can obtain the ﬁnal re-scaled coefﬁcients\nˆR(i) in a learnable way. Several illustrative exam-\nples of the re-scaled coefﬁcients under wi = ±1\nand vi = ±1 are respectively shown in Figs. 2(a)-\n2(d). We can see that ifwi is positive, long-distance\ncontexts are preferred while short-term contexts are\nsurpassed. The situation is reversed if wi turns to\nnegative. In addition, the coefﬁcients in Fig. 2(c)\nhave larger dynamic ranges than the coefﬁcients in\nFig. 2(a), indicating that long-distance information\nis more dominant in Fig. 2(c). Moreover, the co-\nefﬁcients in Fig. 2(d) are “sharper” than those in\nFig. 2(b), which indicates that the model tends to\ncapture shorter distances.\n3.3 Attention Adjustment\nThen, we use the re-scaled coefﬁcients to adjust the\nraw attention weights that are computed by the dot-\nproduct between the query and key, i.e., Q(i)K(i)⊤\n√\nd .\nDifferent from existing methods that add the query-\nkey dot-product with position or distance repre-\nsentations, in our approach we propose to multi-\nply the re-scaled coefﬁcients with the query-key\ndot-product. This is because for the tokens whose\nrelations are very weak, if their re-scaled coefﬁ-\ncients are large, their ﬁnal attention weights will\nbe over-ampliﬁed if we simply add the re-scaled\ncoefﬁcients to their raw attention weights. This\nis not optimal for modeling contextual informa-\ntion because the attention weights of irrelevant con-\ntexts cannot be fully surpassed. However, there\nare also some problems if we directly multiply the\n0 2 4 6 8 10\n0\n2\n4\n6\n8\n10\n 1.0\n1.1\n1.2\n1.3\n(a) wi = 1, vi = −1.\n0 2 4 6 8 10\n0\n2\n4\n6\n8\n10\n0.2\n0.4\n0.6\n0.8\n1.0 (b) wi = −1, vi = −1.\n0 2 4 6 8 10\n0\n2\n4\n6\n8\n10\n 1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n(c) wi = 1, vi = 1.\n0 2 4 6 8 10\n0\n2\n4\n6\n8\n10\n0.2\n0.4\n0.6\n0.8\n1.0 (d) wi = −1, vi = 1.\nFigure 2: The re-scaled coefﬁcient matrices under dif-\nferent values of wi and vi. Dark regions indicate that\nthe corresponding attention weights are promoted.\nre-scaled coefﬁcients ˆR(i) and the raw attention\nweights Q(i)K(i)⊤\n√\nd . This is because the sign of atten-\ntion weights Q(i)K(i)⊤\n√\nd is indeﬁnite and the multi-\nplied results cannot accurately reﬂect the inﬂuence\nof distance information. Thus, we propose to add\na ReLU (Glorot et al., 2011) activation function to\nthe raw attention weights to keep non-negativity. In\nthis way, the ﬁnal output O(i) of an attention head\ncan be formulated as follows:\nO(i) = softmax(ReLU(Q(i)K(i)⊤) ∗ˆR(i)\n√\nd\n)V(i), (6)\nwhere ∗represents element-wise product. The\nReLU function can also introduce sparsity to the\nself-attention because only the positive attention\nweights can be ampliﬁed by the re-scaled coefﬁ-\ncients, which makes the attention weights in our\nmethod sharper. We concatenate the output from\nthe h independent attention heads, and project it\ninto a uniﬁed output. In addition, we keep the\nsame layer normalization and residual connection\nstrategy as the standard Transformer.\n3.4 Computational Complexity Analysis\nCompared with the standard Transformer, the ma-\njor additional time cost is brought by computing\nthe re-scaled coefﬁcients ˆR(i) and using them to\nadjust the attention weights. The theoretical time\ncomplexity of the two operations in each head is\nO(N2), which is much smaller than the time com-\nplexity of computing the attention weights, i.e.,\nO(N2 ×d). In addition, both Eq. (5) and Eq. (6)\nin our approach can be computed in a vectorized\n2063\nmanner. Thus, the additional time consumption of\nour method is very light. Besides, the increase of\nparameters is also minimal because we only intro-\nduce 2h additional parameters, which are usually\nignorable compared with the projection matrices\nlike W(i)\nQ . Thus, our approach inherits the efﬁ-\nciency of the Transformer architecture.\n4 Experiments\n4.1 Datasets and Experimental Settings\nOur experiments are conducted on ﬁve benchmark\ndatasets for different tasks. Four of them are bench-\nmark NLP datasets. The ﬁrst one is AG’s News2\n(denoted as AG), which is a news topic classiﬁca-\ntion dataset. The second one is Amazon Electron-\nics (He and McAuley, 2016) (denoted as Amazon),\nwhich is a dataset for review rating prediction. The\nthird one is Stanford Sentiment Treebank (Socher\net al., 2013) (denoted as SST). We use the binary\nclassiﬁcation version of this dataset. The fourth one\nis Stanford Natural Language Inference (Bowman\net al., 2015) (SNLI) dataset, which is a widely used\nnatural language inference dataset. The detailed\nstatistics of these datasets are summarized in Ta-\nble 1. In addition, we also conduct experiments on\na benchmark news recommendation dataset named\nMIND (Wu et al., 2020c), aiming to validate the\neffectiveness of our approach in both text and user\nmodeling. It contains the news impression logs of 1\nmillion users from Microsoft News3 from October\n12 to November 22, 2019. The training set contains\nthe logs in the ﬁrst ﬁve weeks except those on the\nlast day which are used for validation. The rest logs\nare used for test. The key statistics of this dataset\nare summarized in Table 2.\nDataset # Train # Dev. # Test # Classes Avg. len.\nAG 108k 12k 7.6k 4 44\nAmazon 40k 5k 5k 5 133\nSST 8k 1k 2k 2 19\nSNLI 55k 10k 10k 2 22\nTable 1: Statistics of AG, Amazon, SST and SNLI\ndatasets.\n# Users 1,000,000Avg. title len. 11.52\n# News 161,013# Click samples 5,597,979\n# Impressions500,000# Non-click samples136,162,621\nTable 2: Statistics of the MIND dataset.\n2https://www.di.unipi.it/en/\n3https://www.msn.com/en-us\nIn our experiments, we use the 300-dimensional\nGlove (Pennington et al., 2014) embeddings for\nword embedding initialization.4 The number of at-\ntention head is 16, and the output dimension of each\nattention is 16. We use one Transformer layer in all\nexperiments. On the AG, SST and SNLI datasets,\nwe directly apply Transformer-based methods to\nthe sentences. On the Amazon dataset, since re-\nviews are usually long documents, we use Trans-\nformers in a hierarchical way by learning sentence\nrepresentations from words via a word-level Trans-\nformer ﬁrst and then learning document represen-\ntations from sentences via a sentence-level Trans-\nformer. On the MIND dataset, following (Wu et al.,\n2019, 2020b) we also use a hierarchical model ar-\nchitecture that ﬁrst learns representations of histor-\nical clicked news and candidate news from their\ntitles with a word-level Transformer, then learns\nuser representations from the representations of\nclicked news with a news-level Transformer, and\nﬁnal matches user and candidate news representa-\ntions to compute click scores. 5 We use the same\nmodel training strategy with negative sampling\ntechniques as NRMS (Wu et al., 2019). On all\ndatasets we use Adam (Kingma and Ba, 2015) as\nthe optimization algorithm and the learning rate is\n1e-3. On the AG, Amazon, SST and SNLI datasets,\naccuracy and macro-Fscore are used as the per-\nformance metric. On the MIND dataset, follow-\ning (Wu et al., 2019) we use the average AUC,\nMRR, nDCG@5 and nDCG@10 scores of all ses-\nsions as the metrics. Each experiment is repeated\n5 times independently and the average results with\nstandard deviations are reported.\n4.2 Performance Evaluation\nWe compare our proposedDA-Transformer method\nwith several baseline methods, including: (1)Trans-\nformer (Vaswani et al., 2017), the vanilla Trans-\nformer architecture, where sinusoidal positional\nembeddings are used. (2) Transformer-RPR (Shaw\net al., 2018), a variant of Transformer with relative\nposition representations. (3) Transformer-XL (Dai\net al., 2019), a variant of Transformer that consists\nof a segment-level recurrence mechanism and a\nsinusoidal relative position encoding scheme. (4)\nAdapted Transformer(Yan et al., 2019), a variant\n4We do not use contextualized embeddings generated by\nlanguage models like BERT because we mainly focus on\nvalidating the effectiveness of our Transformer architecture.\n5Both the word-level and news-level Transformers contain\none self-attention layer.\n2064\nMethods AG Amazon\nAccuracy Macro-F Accuracy Macro-F\nTransformer 93.01 ±0.13 93.00 ±0.13 65.15 ±0.40 42.14 ±0.41\nTransformer-RPR 93.14 ±0.12 93.13 ±0.13 65.29 ±0.38 42.40 ±0.40\nTransformer-XL 93.35 ±0.10 93.34 ±0.11 65.50 ±0.40 42.88 ±0.43\nAdapted Transformer 93.28 ±0.13 93.27 ±0.14 65.47 ±0.39 42.69 ±0.42\n*DA-Transformer 93.72±0.11 93.70±0.12 66.38±0.39 44.29±0.40\nTable 3: Results on AG and Amazon. *Improvement over the underlined second best results is signiﬁcant at\np <0.05.\nMethods SST SNLI\nAccuracy Macro-F Accuracy Macro-F\nTransformer 89.67 ±0.22 89.59 ±0.24 81.45 ±0.30 81.42 ±0.31\nTransformer-RPR 89.94 ±0.19 89.90 ±0.20 82.20 ±0.31 82.18 ±0.31\nTransformer-XL 90.06 ±0.20 90.02 ±0.21 83.19 ±0.29 83.15 ±0.30\nAdapted Transformer 90.15 ±0.19 90.10 ±0.1 82.35 ±0.28 82.31 ±0.30\n*DA-Transformer 90.49±0.17 90.43±0.19 84.18±0.27 84.16±0.29\nTable 4: Results onSST and SNLI. *Improvement over the underlined second best results is signiﬁcant atp <0.05.\nMethods AUC MRR nDCG@5 nDCG@10\nTransformer 67.76 ±0.18 33.05 ±0.16 35.94 ±0.19 41.63 ±0.20\nTransformer-RPR 67.81 ±0.16 33.10 ±0.17 35.98 ±0.20 41.65 ±0.21\nTransformer-XL 67.92 ±0.16 33.15 ±0.16 36.04 ±0.20 41.70 ±0.19\nAdapted Transformer 67.70 ±0.22 33.01 ±0.20 35.89 ±0.17 41.58 ±0.23\n*DA-Transformer 68.32±0.15 33.36±0.16 36.34±0.14 42.07±0.17\nTable 5: Results on the MIND dataset. *Improvement over the underlined second best results is signiﬁcant at\np <0.05.\nof Transformer that uses direction- and distance-\naware position encoding. The results of our ap-\nproach and these methods on the ﬁve datasets are\nrespectively shown in Tables 4 and 5. From the\nresults, we have several observations.\nFirst, compared with the vanilla Transformer,\nthe compared methods that consider distance infor-\nmation consistently achieve better performance. It\nshows that distance information is very important\nin context modeling. Second, among the meth-\nods with distance information, the performance of\nTransformer-RPR is lower than the others. This\nmay be because Transformer-RPR does not keep\nthe precise long-distance information. Third, by\ncomparing Transformer-XL and Adapted Trans-\nformer, we ﬁnd that the performance of Adapted\nTransformer is better on the SST dataset, while\nTransformer-XL is better on other datasets. This\nis probably because Adapted Transformeris more\nsuitable for modeling local contexts and the sen-\ntences in the SST dataset are usually short, while\nTransformer-XL may be more appropriate for mod-\neling long sequences. Fourth, our method con-\nsistently achieves better performance on the ﬁve\ndatasets, and its improvement over the second best\nmethod is statistically signiﬁcant (t-test p<0.05).\nThis is because our method can explicitly encode\nreal distance information rather than using posi-\ntional encoding, making the modeling of distance\nmore accurate.\nWe further compare the performance of different\nmethods in a rating regression task on the Amazon\ndataset. The results are shown in Fig. 3. From\nFig. 3 we observe similar patterns with the results\nin classiﬁcation tasks, which validate the generality\nof our DA-Transformer in different genres of tasks.\n4.3 Inﬂuence of Different Mapping Functions\nNext, we study the inﬂuence of using different map-\nping functions f(·) for computing the re-scaled\ncoefﬁcients. We compare the performance of our\nmethod w.r.t. several differentf(·), including: (1)\n2065\nRMSE MAE0.5\n0.7\n0.9\n1.1\n1.3\n1.5\nTransformer\nTransformer-RPR\nTransformer-XL\nAdapted Transformer\nDA-Transformer\nFigure 3: Performance comparison of rating regression\non Amazon. Lower scores indicate better performance.\nf(x) = min(x, T) (clip), using a threshold T to\nclip the weighted distance; (2)f(x) =kix+bi (lin-\near), using a linear transformation to the weighted\ndistance; (3) f(x) = exp(x) (exponent), using an\nexponent function to map the weighted distance;\n(4) f(x) = 1\n1+exp(−x) (sigmoid), using the sig-\nmoid function to activate the weighted distance;\nand (5) f(x; vi) = 1+exp(vi)\n1+exp(vi−x) , our learnable sig-\nmoid function. Due to space limitation, we only\npresent the results on the AG, Amazon and MIND\ndatasets in Fig. 4. From these results, we ﬁnd that\nclip is not optimal for mapping the weighted dis-\ntance. This is because it cannot keep the precise\ndistance information beyond a certain range. In\naddition, simply using the linear transformation is\nalso insufﬁcient. This may be because our attention\nadjustment method requires f(·) to be positive, but\nlinear transformation cannot guarantee. Besides,\nwe ﬁnd that the sigmoid function and our proposed\nfunction are better than the exponential function.\nThis may be because long sequences will lead to\nthe problem of exponent explosion, which is harm-\nful to context modeling. Moreover, our proposed\nlearnable sigmoid function is better than the stan-\ndard sigmoid function. It shows that adjusting the\nactivation function in a learnable way can better\nmap the raw distances into re-scaled coefﬁcients.\n4.4 Inﬂuence of Different Attention\nAdjusting Methods\nThen, we explore the inﬂuence of different meth-\nods for adjusting the raw attention weights. We\nconsider four different kinds of methods, includ-\ning: (1) adding the re-scaled coefﬁcients to the at-\ntention weights normalized by softmax (late add);\n(2) multiplying the re-scaled coefﬁcients with the\nattention weights normalized by softmax (late mul-\ntiply); (3) adding the re-scaled coefﬁcients to the\nraw attention weights before normalization (early\nadd), which is widely used in existing methods like\nTransformer-XL; (4) multiplying the re-scaled coef-\nﬁcients with the raw attention weights activated by\nReLU, which is the method used in our approach\n(early multiply). The results on the AG, Amazon\nand MIND datasets are shown in Fig. 5. According\nto these results, we ﬁnd that early adjustment is\nbetter than late adjustment. This may be because\nthe late adjustment methods will change the total\namount of attention, which may not be optimal.\nIn addition, we ﬁnd that multiplying is better than\nadding for both early and late adjustment. This\nmay be because adding large re-scaled coefﬁcients\nmay over-amplify some attention weights. For ex-\nample, if a raw attention weight is relatively small,\nit is not suitable to add large re-scaled coefﬁcients\nto it because the corresponding contexts may not\nhave close relations. In contrast, multiplying the\nre-scaled coefﬁcients will not over-amplify the low\nattention weights. Moreover, in our early multi-\nply method we further propose to use the ReLU\nfunction to introduce sparsity to make the Trans-\nformer more “focused”. Thus, our method is better\nthan the existing early add method in adjusting the\nattention weights.\n4.5 Model Interpretation\nFinally, we interpret our proposed method by visu-\nalizing its key parameters and the attention weights.\nwe ﬁrst visualize the parameters wi and vi in our\nmethod, which control the preferences of attention\nheads on long-term or short-term information and\nthe shape of the learnable sigmoid function, re-\nspectively. The visualization results on the AG and\nMIND datasets are respectively shown in Figs. 6\nand 7.6 From Fig. 6, we ﬁnd it is very interest-\ning that half of the parameters wi are positive and\nthe rest of them are negative. It indicates that half\nof the attention heads mainly aim to capture local\ncontexts, while the rest ones are responsible for\nmodeling long-distance contexts. It may be be-\ncause both short-term and long-term contexts are\nuseful for understanding news topics. In addition,\nwe ﬁnd that most attention heads have negative vi\nwhile the rest are positive. It shows that on the AG\ndataset the intensity of attention adjustment is mild\nin most attention heads. From Fig. 7(a), we ﬁnd\nlong-term information is somewhat more important\nthan local information in modeling news texts for\n6We show the average results of 5 runs. The values of wi\nand vi in these ﬁgures are sorted and are not corresponding to\nthe head orders.\n2066\nAccuracy Macro-F92.6\n92.8\n93.0\n93.2\n93.4\n93.6\n93.8\nClip\nLinear\nExponent\nSigmoid\nLearnable Sigmoid\n(a) AG.\nAccuracy Macro-F64.0\n64.5\n65.0\n65.5\n66.0\n66.5Accuracy\n40.0\n41.0\n42.0\n43.0\n44.0\n45.0\nMacro-FClip\nLinear\nExponent\nSigmoid\nLearnable Sigmoid (b) Amazon.\nAUC nDCG@1067.0\n67.3\n67.6\n67.9\n68.2\n68.5AUC\n41.0\n41.3\n41.6\n41.9\n42.2\n42.5\nnDCG@10\nClip\nLinear\nExponent\nSigmoid\nLearnable Sigmoid (c) MIND.\nFigure 4: Inﬂuence of using different mapping functions.\nAccuracy Macro-F92.8\n93.0\n93.2\n93.4\n93.6\n93.8\nLate Add\nLate Multiply\nEarly Add\nEarly Multiply\n(a) AG.\nAccuracy Macro-F64.0\n64.5\n65.0\n65.5\n66.0\n66.5Accuracy\n40.0\n41.0\n42.0\n43.0\n44.0\n45.0\nMacro-FLate Add\nLate Multiply\nEarly Add\nEarly Multiply (b) Amazon.\nAUC nDCG@1067.0\n67.3\n67.6\n67.9\n68.2\n68.5AUC\n41.0\n41.3\n41.6\n41.9\n42.2\n42.5\nnDCG@10Late Add\nLate Multiply\nEarly Add\nEarly Multiply (c) MIND.\nFigure 5: Inﬂuence of using different attention adjusting methods.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nAttention Head i\n-0.02\n-0.01\n0.00\n0.01\n0.02\n0.03\n0.04\n(a) wi.\n (b) vi.\nFigure 6: The weights learned by different attention\nheads on the AG dataset.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nAttention Head i\n-0.03\n-0.02\n-0.01\n0.00\n0.01\n0.02\n0.03\n(a) Word-level wi.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nAttention Head i\n-0.40\n-0.30\n-0.20\n-0.10\n-0.00\n0.10\n0.20\n0.30 (b) News-level wi.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nAttention Head i\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n(c) Word-level vi.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nAttention Head i\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06 (d) News-level vi.\nFigure 7: The distance weights learned by different at-\ntention heads on the MIND dataset.\nnews recommendation. However, from Fig. 7(b)\nwe ﬁnd an interesting phenomenon that only one\nhead has a strong negative wi while the values of\nwi in all the rest heads are positive. It means that\nonly one attention head tends to capture short-term\nuser interests while all the other heads prefer to\ncapture long-term user interests. This is intuitive\nbecause users usually tend not to intensively click\nvery similar news and their long-term interests may\nhave more decisive inﬂuence on their news clicks.\nIn addition, we ﬁnd it is interesting that on MIND\nall values of vi are positive. It may indicate that\ndistance information has a strong impact on the\nattention weights. These visualization results show\nthat DA-Transformer can ﬂexibly adjust its prefer-\nence on short-term or long-term information and\nthe intensity of attention adjustment by learning\ndifferent values of wi and vi according to the task\ncharacteristics.7\nWe then visualize the attention weights produced\nby the vanilla Transformer and the distance-aware\nattention weights in our DA-Transformer method.\nThe attention weights of a sentence in the AG\ndataset computed by four different attention heads\nare respectively shown in Figs. 8(a) and 8(b). From\nFig. 8(a), we ﬁnd it is difﬁcult to interpret the self-\nattention weights because they are too “soft”. In\naddition, it is difﬁcult for us to understand the dif-\n7We do not observe signiﬁcant correlations between the\nsequence length and the signs of wi. This may indicate that\nthe values of wi depend more on the task characteristics rather\nthan text lengths.\n2067\n(a) Vanilla Transformer.\n(b) DA-Transformer. The ﬁrst two heatmaps are produced by heads with wi < 0 and others are produced by heads with wi > 0.\nFigure 8: The self-attention weights learned by the vanilla Transformer and our proposed DA-Transformer method.\nferences between the information captured by dif-\nferent attention heads. Different from the vanilla\nTransformer, from Fig. 8(b) we ﬁnd that the at-\ntention weights obtained by our method are more\nsparse, indicating that the attention mechanism in\nour method is more focused. In addition, it is\neasier for us to interpret the results by observing\nthe attention heatmap. For example, the ﬁrst two\nheatmaps in Fig. 8(b) are produced by the two atten-\ntion heads with preferences on short-term contexts.\nWe can see that they mainly capture the relations\namong local contexts, such as the relations between\n“biotech” and “sector”. Differently, in the latter\ntwo heatmaps obtained by the two attention heads\nthat prefer long-term contexts, we can observe that\nthe model tends to capture the relations between\na word (e.g., “biotech”) with the global contexts.\nThese results show that different attention heads in\nour method are responsible for capturing different\nkinds of information, and their differences can be\ndirectly observed from the self-attention weights.\nThus, our method can be better interpreted than\nvanilla Transformers.\n5 Conclusion\nIn this paper, we propose a distance-aware Trans-\nformer, which can leverage the real distance be-\ntween contexts to adjust the self-attention weights\nfor better context modeling. We propose to ﬁrst\nuse different learnable parameters in different at-\ntention heads to weight the real relative distance\nbetween tokens. Then, we propose a learnable sig-\nmoid function to map the weighted distances into\nre-scaled coefﬁcients with proper ranges. They are\nfurther multiplied with the raw attention weights\nthat are activated by the ReLU function to keep\nnon-negativity and produce sharper attention. Ex-\ntensive experiments on ﬁve benchmark datasets\nshow that our approach can effectively improve the\nperformance of Transformer by introducing real\ndistance information to facilitate context modeling.\nAcknowledgments\nThis work was supported by the National Natural\nScience Foundation of China under Grant numbers\nU1936208 and U1936216.\n2068\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nSamuel Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. In\nEMNLP, pages 632–642.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. In ACL, pages 2978–2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT, pages 4171–4186.\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n2011. Deep sparse rectiﬁer neural networks. In AIS-\nTATS, pages 315–323.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,\nXiangyang Xue, and Zheng Zhang. 2019. Star-\ntransformer. In NAACL-HLT, pages 1315–1325.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In CVPR, pages 770–778.\nRuining He and Julian McAuley. 2016. Ups and downs:\nModeling the visual evolution of fashion trends with\none-class collaborative ﬁltering. In WWW, pages\n507–517.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2020. Attention is not only a weight:\nAnalyzing transformers with vector norms. In\nEMNLP, pages 7057–7075.\nRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan,\nMirella Lapata, and Hannaneh Hajishirzi. 2019.\nText generation from knowledge graphs with graph\ntransformers. In NAACL-HLT, pages 2284–2293.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In EMNLP, pages 1532–1543.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In NAACL-HLT, pages 464–468.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In EMNLP, pages 1631–1642.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nXing Wang, Zhaopeng Tu, Longyue Wang, and Shum-\ning Shi. 2019. Self-attention with structural position\nrepresentations. In EMNLP-IJCNLP, pages 1403–\n1409.\nChuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi,\nYongfeng Huang, and Xing Xie. 2019. Neural news\nrecommendation with multi-head self-attention. In\nEMNLP-IJCNLP, pages 6390–6395.\nChuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng\nHuang. 2020a. Improving attention mechanism\nwith query-value interaction. arXiv preprint\narXiv:2010.03766.\nChuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng\nHuang. 2020b. Sentirec: Sentiment diversity-aware\nneural news recommendation. In AACL-IJCNLP,\npages 44–53.\nFangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan\nWu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie,\nJianfeng Gao, Winnie Wu, et al. 2020c. Mind: A\nlarge-scale dataset for news recommendation. In\nACL, pages 3597–3606.\nHu Xu, Bing Liu, Lei Shu, and S Yu Philip. 2019. Bert\npost-training for review reading comprehension and\naspect-based sentiment analysis. In NAACL-HLT,\npages 2324–2335.\nHang Yan, Bocao Deng, Xiaonan Li, and Xipeng\nQiu. 2019. Tener: Adapting transformer en-\ncoder for name entity recognition. arXiv preprint\narXiv:1911.04474.\nBaosong Yang, Jian Li, Derek F Wong, Lidia S Chao,\nXing Wang, and Zhaopeng Tu. 2019. Context-aware\nself-attention networks. In AAAI, volume 33, pages\n387–394.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7001545429229736
    },
    {
      "name": "Transformer",
      "score": 0.6940993666648865
    },
    {
      "name": "Security token",
      "score": 0.5752483010292053
    },
    {
      "name": "Exploit",
      "score": 0.5508954524993896
    },
    {
      "name": "ENCODE",
      "score": 0.4747013449668884
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42292076349258423
    },
    {
      "name": "Sigmoid function",
      "score": 0.41664645075798035
    },
    {
      "name": "Engineering",
      "score": 0.0918077826499939
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Artificial neural network",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}