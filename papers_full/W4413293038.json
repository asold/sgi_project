{
  "title": "A survey on privacy risks and protection in large language models",
  "url": "https://openalex.org/W4413293038",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A1995635696",
      "name": "Kang Chen",
      "affiliations": [
        "Jimei University",
        "Wenzhou-Kean University"
      ]
    },
    {
      "id": "https://openalex.org/A2288866134",
      "name": "Xiuze Zhou",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A3157443011",
      "name": "Lin Yuanguo",
      "affiliations": [
        "Jimei University"
      ]
    },
    {
      "id": "https://openalex.org/A2602966720",
      "name": "Shibo Feng",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A1898662817",
      "name": "Li Shen",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2108925152",
      "name": "Pengcheng Wu",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A1995635696",
      "name": "Kang Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2288866134",
      "name": "Xiuze Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3157443011",
      "name": "Lin Yuanguo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2602966720",
      "name": "Shibo Feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1898662817",
      "name": "Li Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108925152",
      "name": "Pengcheng Wu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2606882085",
    "https://openalex.org/W2804268810",
    "https://openalex.org/W6605475740",
    "https://openalex.org/W4392669753",
    "https://openalex.org/W6780161852",
    "https://openalex.org/W4378527334",
    "https://openalex.org/W4323669724",
    "https://openalex.org/W6600647914",
    "https://openalex.org/W4213025374",
    "https://openalex.org/W4400118952",
    "https://openalex.org/W4281806276",
    "https://openalex.org/W6600755281",
    "https://openalex.org/W4404006793",
    "https://openalex.org/W4402387171",
    "https://openalex.org/W4406302454",
    "https://openalex.org/W4324007070",
    "https://openalex.org/W4297796148",
    "https://openalex.org/W4392720167",
    "https://openalex.org/W4404783115",
    "https://openalex.org/W4401042737",
    "https://openalex.org/W2051267297",
    "https://openalex.org/W2781896769",
    "https://openalex.org/W4393145569",
    "https://openalex.org/W4400806526",
    "https://openalex.org/W4392903998",
    "https://openalex.org/W6600730691",
    "https://openalex.org/W2995022099",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4402634371",
    "https://openalex.org/W4383334323",
    "https://openalex.org/W3035367371",
    "https://openalex.org/W4402667088",
    "https://openalex.org/W4402042869",
    "https://openalex.org/W6600175266",
    "https://openalex.org/W3196832521",
    "https://openalex.org/W6600100092",
    "https://openalex.org/W2807363941",
    "https://openalex.org/W4402684292",
    "https://openalex.org/W4399291916",
    "https://openalex.org/W4406292872",
    "https://openalex.org/W4365794479",
    "https://openalex.org/W6782463312",
    "https://openalex.org/W4236786653",
    "https://openalex.org/W6833208449",
    "https://openalex.org/W6608648704",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W6604583463",
    "https://openalex.org/W4406152279",
    "https://openalex.org/W3194918904",
    "https://openalex.org/W6600617465",
    "https://openalex.org/W4400526908",
    "https://openalex.org/W6604709423",
    "https://openalex.org/W3178659068",
    "https://openalex.org/W4404455614",
    "https://openalex.org/W2744999500",
    "https://openalex.org/W6600137863",
    "https://openalex.org/W6600109629",
    "https://openalex.org/W6600248585",
    "https://openalex.org/W4408749928",
    "https://openalex.org/W4405812282",
    "https://openalex.org/W4404783046",
    "https://openalex.org/W4386072179",
    "https://openalex.org/W4412130408",
    "https://openalex.org/W4392353733",
    "https://openalex.org/W6602641848",
    "https://openalex.org/W6712347134",
    "https://openalex.org/W3167500796",
    "https://openalex.org/W6602127427",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W6604801084",
    "https://openalex.org/W3035616549",
    "https://openalex.org/W3171523434",
    "https://openalex.org/W3213185541",
    "https://openalex.org/W4372260089",
    "https://openalex.org/W4404918643",
    "https://openalex.org/W3015584356",
    "https://openalex.org/W4390871934",
    "https://openalex.org/W644599125",
    "https://openalex.org/W3125518682"
  ],
  "abstract": "Abstract Although Large Language Models (LLMs) have become increasingly integral to diverse applications, their capabilities raise significant privacy concerns. This survey offers a comprehensive overview of privacy risks associated with LLMs and examines current solutions to mitigate these challenges. First, we analyze privacy leakage and attacks in LLMs, focusing on how these models unintentionally expose sensitive information through techniques such as model inversion, training data extraction, and membership inference. We investigate the mechanisms of privacy leakage, including the unauthorized extraction of training data and the potential exploitation of these vulnerabilities by malicious actors. Next, we review existing privacy protection against such risks, such as inference detection, federated learning, backdoor mitigation, and confidential computing, and assess their effectiveness in preventing privacy leakage. Furthermore, we highlight key practical challenges and propose future research directions to develop secure and privacy-preserving LLMs, emphasizing privacy risk assessment, secure knowledge transfer between models, practical deployment and trade-offs, and interdisciplinary frameworks for privacy governance. Ultimately, this survey aims to establish a roadmap for addressing escalating privacy challenges in the LLMs domain.",
  "full_text": "Journal of King Saud University Computer and Information Sciences (2025) 37:163\nhttps://doi.org/10.1007/s44443-025-00177-1\nREVIEW PAPER\nA survey on privacy risks and protection in large language models\nKang Chen 1,2 · Xiuze Zhou 3 · Yuanguo Lin 1 · Shibo Feng 4 · Li Shen 5 · Pengcheng Wu 6\nReceived: 9 April 2025 / Accepted: 11 July 2025 / Published online: 18 August 2025\n© The Author(s) 2025\nAbstract\nAlthough Large Language Models (LLMs) have become increasingly integral to diverse applications, their capabilities\nraise signiﬁcant privacy concerns. This survey offers a comprehensive overview of privacy risks associated with LLMs and\nexamines current solutions to mitigate these challenges. First, we analyze privacy leakage and attacks in LLMs, focusing\non how these models unintentionally expose sensitive information through techniques such as model inversion, training\ndata extraction, and membership inference. We investigate the mechanisms of privacy leakage, including the unauthorized\nextraction of training data and the potential exploitation of these vulnerabilities by malicious actors. Next, we review existing\nprivacy protection against such risks, such as inference detection, federated learning, backdoor mitigation, and conﬁdential\ncomputing, and assess their effectiveness in preventing privacy leakage. Furthermore, we highlight key practical challenges\nand propose future research directions to develop secure and privacy-preserving LLMs, emphasizing privacy risk assessment,\nsecure knowledge transfer between models, practical deployment and trade-offs, and interdisciplinary frameworks for privacy\ngovernance. Ultimately, this survey aims to establish a roadmap for addressing escalating privacy challenges in the LLMs\ndomain.\nKeywords Large language models (LLMs) · Privacy protection · LLM vulnerabilities · Privacy leakage\n1 Introduction\nLarge Language Models (LLMs) are powerful tools in Natu-\nral Language Processing (NLP), employing deep learning\nalgorithms to interpret and produce text that resembles\nhuman language. They exhibit robust capabilities in instruc-\ntion adherence and text task execution, such as writing and\ncoding (Achiam et al. 2023; Bubeck et al. 2023; Touvron\net al. 2023). In recent years, LLMs have shown important\nimplications for the development of general-purpose artiﬁ-\ncial intelligence systems, which represents a signiﬁcant leap\nin the ﬁeld (Kasneci et al. 2023; Chen et al. 2022a). Simulta-\nneously, LLMs demonstrate strong capabilities in contextual\nlearning (Duan et al. 2024a). The large-scale adoption of\nLLMs has introduced a new shift toward more efﬁcient trans-\nfer learning across NLP tasks (Plant et al. 2022).\nAn LLM exempliﬁed by ChatGPT is widely used for solv-\ning various NLP-related tasks in daily personal life (Li et al.\n2024b;O k e ye ta l . 2023). Recent studies have increasingly\nfocused on the impact of LLMs on privacy. With the contin-\nual improvement in the reasoning abilities of LLMs, current\nExtended author information available on the last page of the article\nresearch on privacy primarily focuses on the extraction of\ntraining data (Staab et al. 2023). LLMs supplement limited\nempirical knowledge with domain-speciﬁc insights, although\nthe reliability of this generated knowledge remains uncer-\ntain. Combining LLMs with input from multiple stakeholders\nimproves knowledge quality and scalability; however, it may\nalso raise privacy concerns (Xia et al. 2024). The training data\nfor LLMs is extracted typically from a wide range of Internet\ntexts, which may contain personal, sensitive, or privacy-\nrelated information. An undesirable side effect of using the\nextensive Internet for training is that the model may retain\npotentially sensitive information, which could be leaked to a\nthird party (Staab et al. 2023).\nA key concern in current privacy research involves how\nLLMs may memorize and inadvertently reveal sensitive\ntraining data (Staab et al. 2023). While storing user informa-\ntion to enable personalized responses, these models introduce\nsigniﬁcant privacy and cybersecurity risks (Dhungana et al.\n2025). The personalized deployment of LLMs in split learn-\ning also carries privacy risks, necessitating strong security\nmeasures to protect raw data and intermediate representa-\ntions, particularly in sensitive areas like healthcare (Shu et al.\n2025). LLMs face challenges during inference and training.\n123\n163 Page 2 of 23 Journal of King Saud University Computer and Information Sciences (2025) 37 :163\nThe memory of the model stores vast amounts of data, includ-\ning sensitive information, which can lead to the unintentional\ngeneration of content resembling the training data, poten-\ntially leaking personal or proprietary details. Additionally,\nthe unpredictability of the output of the model complicates\nsecurity, especially in dynamic or multi-round scenarios. The\nvariety of training data sources makes it difﬁcult to assess\nthe sensitivity of each data piece. With continual improve-\nments in LLMs’ reasoning, these models can infer personal\nattributes from text, reaching new levels of capability (Staab\net al. 2023).\nIn LLMs operations, privacy protection technologies are\ngaining strategic relevance in deployment contexts, espe-\ncially in the digital age, where safeguarding personal and\nsensitive data is critical. These technologies help legal profes-\nsionals navigate complex data protection regulations, while\nimproving compliance with data processing and storage\nrequirements. Privacy protection methods, including data\ncleaning, differential privacy (Yan et al. 2024), and con-\nﬁdential computing (Mo et al. 2024), ensure the secure\nhandling of user information, thereby preserving privacy\nand reducing the risk of accidental data exposure. To main-\ntain user privacy throughout the data lifecycle, a frame-\nwork for securing Retrieval-Augmented Generation (RAG)\npipelines incorporates encryption, zero-trust principles, and\nguardrails (Nandagopal 2025). A conceptual solution has\nalso been proposed to enhance user privacy by detecting\nand anonymizing sensitive named entity categories, while\nmaintaining context by substituting original entities with\nfunctionally equivalent ones ( ˙Zarski and Janicki 2025). These\nmethods signiﬁcantly improve the privacy protection of\nLLMs.\nMotivation The primary motivation for investigating privacy\nissues in LLMs is to ensure the accuracy and reliability of\nmodel outputs. In critical areas such as education, healthcare,\nand law, incorrect information can lead to misleading conclu-\nsions and serious social consequences, such as misdiagnosis\nor legal errors. Additionally, as growing emphasis on privacy\nregulations in digital systems, safeguarding users’ personal\ninformation has become essential. The improper use or leak-\nage of sensitive data during training can lead to legal liability\nand a signiﬁcant erosion of user trust, negatively impact-\ning both businesses and users. While signiﬁcant research has\nbeen conducted on privacy in machine learning, the speciﬁc\nchallenges of LLMs still remain underexplored. These chal-\nlenges include privacy management, model complexity, and\nthe practical implementation of privacy protection technolo-\ngies. This paper aims to support the development of privacy\nprotection in LLMs through systematic review and research,\nensuring their security and reliability in real-world applica-\ntions, thus enhancing compliance with ethical AI standards\nand fostering responsible model deployment.\nExisting surveys have explored various aspects of the\nsecurity and privacy of LLMs. For instance, Das et al. ( 2025)\nprovide a comprehensive overview of the vulnerabilities of\nLLMs, categorizing them into security attacks and privacy\nattacks. They further discuss the speciﬁc challenges associ-\nated with each category and outline corresponding defense\nmechanisms; Yao et al. ( 2024) organize their ﬁndings into\nbeneﬁcial applications, offensive uses, and inherent vulner-\nabilities. They further divide these threats into two distinct\ngroups: AI model-inherent vulnerabilities and non-AI model-\ninherent vulnerabilities; Esmradi et al. ( 2023)r e v i e waw i d e\nrange of attack techniques, implementation methods, and\nmitigation strategies in LLMs, broadly categorizing attacks\ninto those targeting the model itself and those targeting model\napplications.\nThese surveys have provided foundational perspectives\nto understanding the risks associated with LLMs and the\nvarious defenses that can be employed. However, these sur-\nveys often address privacy issues independently or without\na systematic framework. In contrast, our survey presents a\nmore extensive classiﬁcation that integrates more speciﬁed\nprivacy concerns. We further classify these issues based on\ntheir unique characteristics, offering a more granular ana-\nlytical framework. This ﬁne-grained classiﬁcation approach\nemphasizes the interconnectedness of these domains. Focus-\ning on privacy highlights its critical importance in protecting\nuser privacy and meeting regulatory standards. By system-\natically analyzing privacy concerns, our survey underscores\ntheir signiﬁcance and provides practical recommendations\nfor enhancing the ethical use of LLMs.\nContributions Our work provides a comprehensive clas-\nsiﬁcation of the privacy risks and attack types faced by\nLLMs. We systematically reviewed and reorganized eleven\nrepresentative risks and attacks, offering clear deﬁnitions\nand corresponding mitigation strategies for each category.\nBy comparing and reﬁning previous survey efforts, we\ndeveloped a more structured and complete taxonomy of\nprivacy-related challenges. In addition, after critically ana-\nlyzing the advantages and disadvantages of existing tech-\nnologies, including data cleaning, federated learning, among\nothers, we explored how to effectively apply these tech-\nnologies to enhance the security and user privacy protection\nof LLMs. These contributions not only address under-\nexplored areas in current research and propose potential\nimprovements or new approaches to privacy protection in the\ncontext of LLMs, but also offer valuable guidance for future\nwork.\nThe remainder of this paper is organized as follows. The\narchitecture and vulnerabilities of LLMs are introduced in\nSection 2. The privacy threats to LLMs are discussed in Sec-\ntion 3. Popular mitigation techniques for different types of\nattacks are discussed in Section 4. Future research directions\n123\nJournal of King Saud University Computer and Information Sciences (2025) 37 :163 Page 3 of 23 163\nare presented in Section 5. Finally, the conclusion is given in\nSection 6.\n2 Background\n2.1 Architecture and vulnerabilities of LLMs\nAs a deep learning-based NLP model, LLMs have a com-\nplex and multi-stage workﬂow to transform collected data\ninto useful text responses. From a data perspective, the entire\nprocess begins with the collection of the dataset, which\nincludes users’ natural language data. Recent studies have\nshown that the quality of this initial data signiﬁcantly impacts\ndownstream performance (Wang et al. 2025; Taori et al.\n2023). The data is then preprocessed for conversion into a\nformat compatible with the model, with any non-essential\nor duplicate information being removed to enhance data\nintegrity and model performance. In the core stages of pre-\ntraining and ﬁne-tuning, the system learns language rules\nusing large-scale text data to acquire comprehensive lin-\nguistic representations that capture syntax, semantics, and\ncontextual relationships. Subsequently, the model is ﬁne-\ntuned on speciﬁc task data to adapted to domain-speciﬁc\ndownstream tasks (Ouyang et al. 2022). Throughout this pro-\ncess, the integrity and quality of the data are vital for both\npre-training and ﬁne-tuning.\nFrom a deployment perspective, the process also intro-\nduces privacy risks, particularly during the data collection\nand model deployment phases. The collection of large\namounts of textual data, which may contain personal or sensi-\ntive information (Carlini et al. 2021), along with the real-time\ninteraction between users and the model, increases the risk\nof personal data exposure. Sensitive information provided by\nusers may be processed and stored by the model, making it\nsusceptible to exploitation by attackers who can exploit vul-\nnerabilities to access this information. Ultimately, the model\ndeployment phase integrates the trained and ﬁne-tuned mod-\nels into practical applications. This process is illustrated in\nFig. 1.\nDuring user interaction with LLMs, when users input sen-\nsitive information as part of their prompts (Kshetri 2023),\nprivacy issues originate from this data provision process.\nRecent research demonstrates that even anonymized prompts\ncan be reverse-engineered to recover private information (Liu\net al. 2025). During input, users may unintentionally provide\npersonal information, conﬁdential data, or sensitive content.\nIf this information is handled incorrectly, it may lead to pri-\nvacy leakage or attacks from malicious actors.\n2.2 LLMs vulnerabilities\nAccording to recent studies, privacy vulnerabilities in LLMs\nare structurally embedded and difﬁcult to mitigate (Yao et al.\n2024; Yan et al. 2024). These vulnerabilities can be classiﬁed\ninto different categories according to their characteristics,\nincluding the following: privacy attacks, privacy leakage,\ncontextual leakage (Mireshghallah et al. 2023), and back-\ndoor attacks (Das et al. 2025). The privacy risks discussed\nin this paper are typically categorized into target-based or\nmethod-based approaches. Privacy in LLMs entails safe-\nguarding identiﬁable user data and minimizing exposure to\nunintended data disclosure.\nFrom the perspective of LLM deployers, deeply embedded\nprivacy vulnerabilities present a major operational chal-\nlenge across the model lifecycle - from pre-training and\nﬁne-tuning to public deployment. These risks are magni-\nﬁed by the open-ended nature of user interactions, which\nmay evoke memorized or inferential disclosures of sensi-\ntive data. Comprehensive surveys highlight that deployers\nbear primary responsibility for implementing crucial safe-\nguards - such as input/output ﬁltering, differential privacy,\nFig. 1 LLM Privacy Risks: Data Flow Analysis Based on LLMs Life-\ncycle. This diagram illustrates the privacy risks across different stages\nof LLMs development. During data collection and preprocessing, sen-\nsitive information may be exposed. The training phase, including both\npre-training and ﬁne-tuning, introduces additional privacy vulnerabil-\nities. The model undergoes pre-training and ﬁne-tuning, followed by\nevaluation using various metrics, while deployment extends these risks\nto real-world applications. The framework highlights how privacy con-\ncerns persist throughout the entire model lifecycle, with each phase\ncontributing to potential data exposure\n123\n163 Page 4 of 23 Journal of King Saud University Computer and Information Sciences (2025) 37 :163\naudit pipelines, and real-time monitoring during inference -\nas well as ensuring robustness against jailbreaks, data poison-\ning, and contextual leakage (Das et al. 2025; Wang et al. 2024;\nNeel and Chang 2023). Emerging deployment protocols now\nadditionally recommend conducting risk audits, red-teaming\nexercises, and employing dynamic alignment and minimal\nexposure principles to detect and neutralize privacy breaches\nbefore public release. These layered defenses are essential\nfor maintaining user trust, complying with evolving regu-\nlations, and safeguarding data conﬁdentiality in real-world\nLLM applications.\nThe vulnerabilities of LLMs, particularly privacy-related\nthreats, are examined in this paper. Speciﬁcally, we examine\nprivacy leakage and three types of attacks targeting the fol-\nlowing: models, data, and users themselves. Notably, various\nattacks, such as data poisoning and backdoor insertion, often\nshare overlapping mechanisms - particularly by introducing\nmalicious samples during training to alter model behav-\nior (Das et al. 2025; Yan et al. 2024). All existing privacy\nattack methods in the literature have the potential to compro-\nmise LLMs, posing critical challenges to the conﬁdentiality\nand integrity of user-related data.\n3 PrivacyissuesofLLMs\nIn the context of LLMs, privacy remains a key vulnerabil-\nity in the design and deployment of LLMs. We have divided\nprivacy issues into two categories based on how attackers\ncan access sensitive information: privacy leakage and privacy\nattacks. Privacy leakage denotes the exploitation of LLM\nvulnerabilities by attackers to collect sensitive information;\nwhereas, privacy attacks involve exploiting model behaviors\nor system-level vulnerabilities to extract sensitive informa-\ntion. A detailed classiﬁcation of these types is provided in\nTables 1 and 2. Next, we brieﬂy introduce these two types of\nprivacy threats and their impacts.\n3.1 Privacy leakage\nThese risks encompass various forms of data exposure that\nundermine user conﬁdentiality. Understanding these risks\nenables the development of effective protective strategies\n3.1.1 Sensitive information leakage\nWhen interacting with LLMs, users may enter personal sen-\nsitive details, including their name, phone number, address,\nID card number, and bank account information. Once stored\nor processed by the model, this information may be exposed\nto unintended third parties or used without consent. Below\nis a classiﬁcation of sensitive information leakage caused by\ndifferent methods.\nSensitive query Privacy leakage in LLMs often results from\nusers mishandling sensitive information. For example, when\ninteracting with LLMs, user disclosure of personally identi-\nﬁable information has been identiﬁed as a potential source\nof leakage (Shokri et al. 2017). User input can be incor-\nporated into the knowledge base for training these models\nand improving tools; however, this caution has not pre-\nvented some LLM users from including sensitive data in\ntheir prompts (Kshetri 2023). Kshetri ( 2023) notes that there\nis some confusion regarding the speciﬁc mechanisms and\npotential impact of privacy risks when users include sensitive\ndetails in their input. Some users believe that the information\nthey provide is stored in the ChatGPT database, which could\nlead to the potential leakage of this data to others in response\nto different queries (Carlini et al. 2021).\nLLM-based interactions have been widely adopted for\ntasks such as programming, academic writing, and med-\nical diagnosis (Zamﬁrescu-Pereira et al. 2023). However,\ndespite their usefulness, LLMs present signiﬁcant privacy\nand security risks. Although user input is not automatically\nused for training, it is often stored by LLM providers for\nmodel improvement, raising concerns about potential leak-\nage (Carlini et al. 2021). Previous work has demonstrated\nthat sensitive queries can result in private information being\nleaked, either through direct access to model parameters or\nthrough adversarial probing of the model (Fredrikson et al.\n2015). Additionally, although LLMs, such as ChatGPT gen-\nerate responses based on pre-trained models, which do not\ninherently merge sensitive information into the model or\nshare it with other users, the risk of leakage remains sig-\nniﬁcant. Studies have shown that even pre-trained models,\nwhen exposed to sensitive queries, can unintentionally recall\nor expose personal data, due to the nature of their training pro-\ncesses and the large-scale datasets (Bommasani et al. 2021).\nIn summary, even though the risks associated with LLMs\nmay not always align with user expectations, they are still\nsigniﬁcant and must be carefully monitored.\nSensitive information exposed by ﬁne-tuning Currently,\nLLMs have demonstrated strong performance across a\nrange of NLP tasks (Wang et al. 2023; Chen and Zhou\n2024). However, when LLMs are applied to specialized\nﬁelds, they often face challenges including hallucinations,\ndomain knowledge gaps (Bang et al. 2023; Chan et al.\n2023) in speciﬁc areas (Singhal et al. 2025), and a failure\nto integrate the latest knowledge into constantly evolving\nindustry scenarios (Kasneci et al. 2023). Fine-tuning with\nhigh-quality domain-speciﬁc knowledge improves model\naccuracy and relevance in specialized applications. By incor-\nporating advanced knowledge and data from particular ﬁelds\ninto LLMs, the models better understand and generate text\ncontent relevant to those ﬁelds, enhancing their domain-\nspeciﬁc performance. However, when ﬁne-tuning an LLM,\n123\nJournal of King Saud University Computer and Information Sciences (2025) 37 :163 Page 5 of 23 163\nTable 1 Overview of Privacy Leakage Categories\nCategory Work Method Evaluated Model Dataset Evaluation Metric\nZamﬁrescu-Pereira et al. ( 2023) Design probe GPT-3 / Performance\nWang et al. ( 2023) Zero-shot robustness DeBERTa-L, BART-L, SST-2, QQP , ASR\nevaluation etc MNLI, etc\nSensitive Information Bang et al. ( 2023) Multi-turn GPT-4, GPT-3, National Flag- ROUGE-1, ChrF++,\nLeakage approach ChatGPT Drawing, etc etc\nSinghal et al. ( 2025) Overlap analysis Flan-PaLM, Med-PaLM2, MedQA (USMLE), PubMedQA, Acc\netc MedMCQA, etc\nShen et al. ( 2025) Query-unrelated BiLSTM-CRF, spaCy, PII-Bench dataset Precision, Recall,\nPII masking GPT-3.5, GPT-4o, etc F1-score\nFan et al. ( 2024) Zero-shot, Law Recitation, MPT-7B, GOLDCOIN-HIPAA Acc, Prec, Rec,\nDirect Prompt, LLM API Llama2-7B, etc etc\nMireshghallah et al. ( 2023) Differential Privacy GPT-4, ChatGPT, / Sensitivity Score,\nInstructGPT, etc Rate, Error Rate\nContextual Leakage Staab et al. ( 2023) Anonymization PaLM2-Chat, Enron-Email, Top-k accuracies,\nAlignment GPT-4, etc PAN competition, etc Jaro-Winkler, etc\nThomas et al. ( 2024) Web search GPT-3.5, GPT-4 / Acc\nLi et al. ( 2025) Judge-Student Pair GPT-4o, Gemini-1.5-ﬂash, AlpacaEval 2.0 Preference Leakage\nDesign LLaMA-3.3-70B Score\nLi et al. ( 2023) Private ﬁne-tuning BERT, RoBERTa MNLI, SST-2, QQP AUC, Acc, ASR\nexposure score, etc\nThis table summarizes representative studies under each privacy leakage category, including their methods, evaluated models, datasets, and evalu ation metrics\n123\n163 Page 6 of 23 Journal of King Saud University Computer and Information Sciences (2025) 37 :163\nTable 2 Overview of Privacy Attack Categories\nCategory Work Method Evaluated Model Dataset Evaluation Metric\nLi et al. ( 2021) Layer weight poisoning training PTMs SST-2, IMDB, etc LFR, Clean Acc\nKurita et al. ( 2020) Restricted Inner Product BERT, XLNet SST-2, OffensEval, etc LFR, Clean Acc\nPoison Learning\nBackdoor Attacks Zhang et al. ( 2021) Dynamic Surgery ResNet-18, etc IMDB, SST-2 distinct, BLEU, etc\nLi et al. ( 2024b) Model-Editing Techniques GPT-2-XL, GPT-J SST-2,AGNews, etc ASR,CACC\nFredrikson et al. ( 2015) Big machine learning Softmax, MLP , DAE FiveThirtyEight, GSS Correct rate, acc, etc\nModel Stealing Truong et al. ( 2021) Data-free Resnet-34-8x, etc SVHN, CIFAR-10. Acc\nAttacks model extraction\nSha and Zhang ( 2024) Prompt engineering ChatGPT, LLaMA RetrievalQA, Alpaca-GPT4 Acc, recall, etc\nModel Inversion Zhang et al. ( 2020) Generative adversarial VGG16, ResNet-152, etc MNIST, ChestX-ray8, etc PSNR, Attack Acc, etc\nAttacks network\nZhang et al. ( 2022) Word embedding Tiny-BERT, BERT Emotion/Yelp Dataset RR, Acc, PLL\nperturbation\nData Stealing He et al. ( 2024) Fine-Tuning GPT-3.5-turbo, Mistral-7B Do, D’o ASR\nAttacks\nGao and Zhang ( 2023) Spilt learning LeNet-5, VGG16, etc MNIST, CIFAR-10, etc Complexity\nTraining Data Bai et al. ( 2024) Special Characters Llama-2-Chat, etc / ASR, Count\nExtraction Attacks Attack\nCarlini et al. ( 2021) Proof-of-concept GPT-2 Top-n, Temperature, Internet Perplexity, Small, etc\nAttack\nMembership Inference Fu et al. ( 2023) Multiple regularization GPT-2, GPT-J, Wikitext-103, AUC\nAttacks generation, self-prompt Falcon-7B, LLaMA-7B XSum, etc\nDuan et al. ( 2024b) overlap analysis GPT-2-SMALL, etc Pile-CC, Wikipedia, etc AUC, ROC, etc\nZhao et al. ( 2021) Membership inference Logistic Regression, etc Loc-30, Pur-100, etc AUC, Acc\nAttribute Inference Gong and Liu ( 2018) Attribute inference SAN, SBA Google+ Precision, Recall, F-Score\nAttacks\nChen et al. ( 2021) Model Extraction BERT-based API / /\nThis table provides a structured overview of existing attack methods targeting different types of privacy leakage, along with experimental details from the referenced works\n123\nJournal of King Saud University Computer and Information Sciences (2025) 37 :163 Page 7 of 23 163\nit is often necessary to train with domain-speciﬁc datasets\nthat may contain personal sensitive information, including\npersonally identifying information and health records (Xiao\net al. 2023). If the data is not properly processed, desensi-\ntized, or encrypted, the model may learn patterns related to\nsensitive information during training, potentially leading to\nsensitive information exposure.\nTo systematically evaluate the risk of PII leakage in ﬁne-\ntuned LLMs, Shen et al. ( 2025) recently proposed PII-Bench,\na comprehensive benchmark designed to assess query-\naware privacy protection mechanisms. PII-Bench comprises\n2,842 test samples spanning 55 ﬁne-grained PII categories\nand simulates a variety of realistic scenarios ranging from\nsingle-entity descriptions to complex multi-party interac-\ntions. Experimental results indicate that while current LLMs\nperform adequately on fundamental PII detection tasks, they\nexhibit signiﬁcant limitations in discerning the relevance of\nPII to user queries, particularly in complex, multi-entity sce-\nnarios.\n3.1.2 Contextual leakage\nPrivacy is not a standalone concept conﬁned to conventional\nconﬁdential information (such as identiﬁcation numbers);\ninstead, it is closely connected to complex societal frame-\nworks, thereby complicating the identiﬁcation and analysis\nof potential privacy violations (Fan et al. 2024). Recently,\nthe rise of LLMs has led to concerns about data mem-\nory and leakage, highlighting the importance of secure\ninformation ﬂow. This is particularly critical in interactive\nsettings, where LLMs retrieve data from various sources,\nincluding past email exchanges, and produce responses\nusing contextual details. When information ﬂows in vio-\nlation of contextual norms, privacy leakage occurs. For\ninstance, if your healthcare provider discloses your health\nrecords, including sensitive health details, with an insur-\nance company for promotional reasons, this would violate\ncontextual integrity (Mireshghallah et al. 2023). Contextual\nintegrity refers to the appropriateness of information ﬂows\nwithin speciﬁc social contexts, deﬁned by roles, expectations,\nand transmission principles. Apthorpe et al. ( 2018)p r o -\nposed employing ﬁve parameters - sender, recipient, subject,\nattribute, and transmission principle as key factors to describe\nthe information ﬂow and associated contexts. Among these,\nthe theory of contextual integrity deﬁnes privacy norms in\nterms of the appropriateness of a universally accepted spe-\nciﬁc information exchange or “information ﬂow.\"\nA comprehensive study, carried out on the capacity of pre-\ntrained LLMs to extract personal attributes from text, reveals\nthat current LLMs can identify these attributes in various con-\ntexts. Using the PersonalReddit dataset to evaluate the most\nadvanced LLMs (Staab et al. 2023), it was found that GPT-4\nreached an accuracy rate of 84% in the top-1 and 95.1% in\nthe top-3. With recent advances, LLMs can automatically\ninfer a wide range of personal authorship attributes from\nlarge amounts of unstructured text (such as public forums or\nsocial media posts) based on context during inference. This\ncapability increases the likelihood of re-identifying users or\nexposing inferred sensitive traits.\nPersonal preferences leakage Personal preference leakage\nshould be considered a form of context leakage because such\npreferences are typically inferred from a user’s prior inter-\nactions with the model rather than being explicitly provided.\nWhen these preferences are revealed or utilized without user\nintent, it indicates that the model is drawing on previously\nstored contextual information (Li et al. 2025). In today’s\ntechnology-driven world, personalization plays a crucial role\nin enhancing user engagement with language models and\nplatforms (Chen et al. 2024). However, this personalization\nmay involve the use of private user information, potentially\nresulting in privacy leakage. Individuals may unintentionally\nexpose their preferences through personalized recommenda-\ntions and targeted content, leading to both direct and indirect\nprivacy risks. Service providers can infer complex user pro-\nﬁles and preferences from interaction data, even without\ndirect access to sensitive information (Yan et al. 2024). Fur-\nthermore, studies have shown that LLMs are effective at\ngenerating outputs that align with users’ preferences, some-\ntimes even more accurately than humans (Thomas et al.\n2024), which heightens the risk of unintended preference\nand privacy leakage.\nIn order to thoroughly assess privacy risks in question-\nanswering contexts, PrivacyQA has been proposed as a\nbenchmark designed to assess the propensity of LLMs to\ninadvertently disclose sensitive personal information during\nresponse generation. PrivacyQA comprises realistic queries\ninvolving personal and sensitive data, enabling rigorous test-\ning of whether LLMs leak private information in the course\nof answering (Mireshghallah et al. 2023). Recent studies\nleveraging the PrivacyQA benchmark have demonstrated\nthat despite advancements in privacy-preserving techniques,\nLLMs remain vulnerable to subtle privacy attacks that\nexploit contextual and preference-based information (Li\net al. 2023). These ﬁndings underscore the critical need to\nintegrate robust privacy safeguards, such as context-aware\nﬁltering and response sanitization, in practical deployments\nto mitigate the risk of sensitive information leakage effec-\ntively.\n3.1.3 Summary\nThis subsection discusses different types of privacy leakage\nin LLMs, with a focus on their interconnectedness. Privacy\nleakage is usually divided into two types: Sensitive Informa-\ntion Leakage and Contextual Leakage. Sensitive Information\n123\n163 Page 8 of 23 Journal of King Saud University Computer and Information Sciences (2025) 37 :163\nLeakage includes direct or indirect exposure of private data,\nsuch as sensitive user queries and personal information unin-\ntentionally remembered and revealed by ﬁne-tuned models.\nContextual Leakage refers to private details revealed through\nthe interaction process, like background context or personal\npreferences that can be guessed without being clearly stated.\nThese two types are not separate; they are closely linked.\nFor example, a sensitive query may contain hints about user\npreferences, and ﬁne-tuned models may memorize both clear\nfacts and hidden traits from context-rich data. This intercon-\nnectedness increases the overall risk, as one form of leakage\ncan lead to another, exposing more personal information\nthan expected. Recognizing these connections is essential for\nbuilding stronger privacy protections across the entire LLM\nlifecycle.\n3.2 Privacy attacks\nStudies on privacy attacks targeting LLMs are examined in\nthis section. These attacks are classiﬁed into three groups:\nmodel-based, data-based, and user-based depending on the\nnature of the exploited component and the adversary’s\napproach. Furthermore, each category is further divided\nbased on the speciﬁc characteristics of the approaches used.\n3.2.1 Model-based attacks\nBackdoor attacks Backdoor attacks - where poisoned sam-\nples are inserted into training data - allow adversaries to later\ntrigger hidden behaviors that leak sensitive user data (Das\net al. 2025;L ie ta l . 2021), as well as manipulate the out-\nput of a model by triggering speciﬁc keywords in the input\nsequence (Kurita et al. 2020). Whether introduced during\npre-training or ﬁne-tuning, poisoned data can embed hid-\nden behaviors into the model (Yan et al. 2024). Among the\ntechniques used to introduce backdoors, weight poisoning is\nprevalent; it modiﬁes pre-trained weights via ﬁne-tuning on\ndatasets deliberately embedded with triggers and mislabeled\ntargets in speciﬁc tasks (Kurita et al. 2020;L ie ta l . 2021;\nZhang et al. 2021).\nLi et al. ( 2024b) identiﬁed several shortcomings related\nto weight poisoning, including the compromise of the gen-\neral functionality of the model and the inability to construct\nan extensive dataset for each attack task. Consequently,\nthey inject backdoors into basic LLMs, minimizing the data\nrequirements for each attack target while ensuring that clean\ndata remains unaffected when applied to various tasks. The\noriginal lightweight backdoor injection (Li et al. 2024b)i s\ndeﬁned as follows:\n/Delta1\nl ≜ arg min\n/Delta1l\n(∥(W l +/Delta1l )K l −V l ∥+∥ (W l +/Delta1l )K b\nl −V b\nl ∥),\n(1)\nwhere K l and V l represent the original knowledge pair in the\ntarget model. The objective is to identify a (Kb, Vb) pair to\nmodify the model parameters and introduce backdoor knowl-\nedge, where Kb =[ kb1,kb2,···] , Vb =[ vb1,vb2,···] .T h e\nspeciﬁc layers l and the original parameters in Multilayer\nPerceptron (MLP), denoted as W l , are used for editing.\nThere are several challenges associated with this opti-\nmization through ( 1). Representing triggers and targets as\nkey-value pairs K b\nl , V b\nl for editing is not straightforward.\nIn instances with limited data, ﬁnding sufﬁcient and repre-\nsentative K l and V l to maintain the model’s understanding\nof benign sentences is challenging. To overcome these\nchallenges, a new framework, Li et al. ( 2024b), has been pro-\nposed, which employs model editing techniques to implant\nbackdoors into pre-trained LLMs targeting various attack\ngoals.\nIn the duplex model parameter editing, given the pres-\nence of backdoor key-value pairs ( K\nb, Vb) and task-related\nknowledge ( Kc, Vc) on a specialized, clean dataset ( D), /Delta1l\nis deﬁned as follows:\n/Delta1l =/Delta1l\nb+/Delta1l\nc = Rl\nb K T\nb (Cl+Kb K T\nb )−1+Rl\nc K T\nc (Cl+Kc K T\nc )−1,\n(2)\nwhere Cl = K l K lT denotes the covariance of the knowledge\npre-learned in the model, preserving its memory. This covari-\nance can be approximated by empirically sampling the input\nknowledge representation to W\nl . Rl\nb is computed as follows:\nV l\nb − W l K l\nb\nMA X (L) − l + 1 . (3)\nThe residual error between the target value representation\nV l\nb and the current output representation at the l-th MLP is\nquantiﬁed by this term. Additionally, for a given set of con-\nsecutive layers L (e.g., L = [5, 6, 7]), the residual error across\nthe lower layers l ∈ L is distributed to enhance stability.\nModel stealing attacks In a model stealing attack, an attacker\nseeks to duplicate or replicate models ﬁne-tuned on sensi-\ntive datasets by observing their responses through querying.\nBy extracting parameters and internal information about the\nmodel, it is possible to reconstruct or duplicate the model\nwithout direct access to the dataset, thereby obtaining access\nto conﬁdential details about the model (Yan et al. 2024).\nDue to the nature of this attack, query complexity has\nalways been a signiﬁcant challenge in model stealing. To\ntackle this problem, Truong et al. ( 2021) proposed a tech-\nnique, Data Free Model Extraction (DFME), to extract\nmachine learning models using only the victim’s black box\npredictions, without requiring access to private or proprietary\ntraining data. Subsequently, Sha and Zhang ( 2024) intro-\nduced a new type of model stealing attack. Prompt stealing\n123\nJournal of King Saud University Computer and Information Sciences (2025) 37 :163 Page 9 of 23 163\ntypically consists of two stages, response-based parame-\nter extraction and inverse prompt reconstruction: the user\nemploys prompt engineering to obtain the desired response\nfrom LLMs, while the adversary attempts to reverse-engineer\nthe original prompt through the parameter extractor and\nprompt reconstructor.\nModel inversion attacks Model inversion attacks involve\nanalyzing the output content of the model, along with\nits parameters and gradients, and using reverse engineer-\ning to reconstruct or invert training samples from private\ndatasets (Yan et al. 2024; Fredrikson et al. 2015). Attackers\nfrequently attempt to use this method to recover sensitive\ninformation from training data, increasing the risk of re-\nidentiﬁcation or unauthorized data reconstruction.\nBased on image data, Zhang et al. ( 2020) proposed an\nefﬁcient attack method called Generative Model Inversion\n(GMI), which reverses Deep Neural Networks (DNNs) and\nreconstructs private training data with great precision. They\nalso highlighted that this weakness is inevitable for highly\npredictive systems, as these systems can create a strong cor-\nrelation between features and labels, which aligns with what\nan attacker leverages to carry out model inversion attacks.\nNotably, the ﬁrst model inversion attack (Text Revealer) was\ndemonstrated on text reconstruction using transformers for\ntext classiﬁcation (Zhang et al. 2022). In such a novel attack,\nassuming the adversary has domain knowledge and black-\nbox access to the target model. The attack consists of two\nphases: collection and continuous disturbance, based on tar-\nget model feedback.\nIn the stage of word embedding perturbation, the adver-\nsary generates perturbations /Delta1H\nt for Ht by solving the\nfollowing optimization problem:\nmin\n/Delta1Ht\nLad v(G(Ht + /Delta1Ht ), D pr i ,a ), (4)\nwhere Ht denotes the current hidden state of the text genera-\ntor G, and Lad v signiﬁes an adversarial loss used to assess the\ndifference between the generated text G(Ht ) and the private\ndataset D pr i ,a of the target label a.\n3.2.2 Data-based attacks\nData stealing attacks Adversaries attempt to inject a back-\ndoor into the pre-trained LLM by strategically injecting a\nlimited number of poisoned samples into the training data.\nSubsequently, they can extract memorized sensitive infor-\nmation from the model upon trigger activation by combining\npredeﬁned backdoor triggers, thus achieving data stealing\nattacks (He et al. 2024).\nData privacy attacks can be broadly categorized into model\nstealing and data stealing. Unlike model stealing attacks,\nwhich involve extracting model architecture and parameters\nthrough queries and responses, the purpose of data steal-\ning attacks is to retrieve the training data from pre-trained\nmodels (He et al. 2024). For a given victim model, the\nattacker generates and carefully modiﬁes adversarial prompts\nto obtain private data. The stealing prompt can be an “adver-\nsarial\" prompt, where the attacker directly inputs the model\nfor optimization without malicious training. To enhance the\neffectiveness of the attack, attackers can introduce a small\nsubset of poisoned data into the training set. Third-party plat-\nforms may utilize these modiﬁed training sets to ﬁne-tune the\nbase model. After publicly uploading the model, attackers\ninput query prompts containing predeﬁned text triggers. The\nmodel deviates from its intended output behavior and reveals\nmemorized training data. Conversely, if the user lacks prior\nknowledge of the predeﬁned triggers, the model will reject\ndirect query prompts. The overall optimization objective can\nbe expressed as follows He et al. ( 2024):\nL =− 1\nTpr e\n&\nTpr e∑\nti\ncPθ (I pr i vate | Sy ,( Xb ⊕ ti )), (5)\nwhere ti represents a ﬁxed trigger predeﬁned by the attacker\n(only known to the attacker), and I pr i vate represents private\ninformation stolen from the model.\nGiven that client privacy data can be easily extracted by\nserver models and that multiple intermediate server models\nin Split Learning (SL) can lead to even more leaks, Gao\nand Zhang ( 2023) proposed a novel attack on SL called\nthe Pseudo-Client Attack (PCA T). The only requirement for\nthe server in the same learning task is a very small dataset\n(approximately 0.1%-5% of the private training set). This\nattack is particularly transparent to the client, allowing the\nserver to obtain the client’s privacy without the risk of detec-\ntion, thereby posing serious data and privacy threats.\nTraining data extraction attacks Data extraction attack\nextracts memorized training data directly from the model,\nresulting in a high degree of privacy leakage (Bai et al.\n2024). Training data extraction attacks are somewhat similar\nto model inversion attacks, as both have the ability to recon-\nstruct training data points. In contrast, the purpose of training\ndata extraction attacks is to reconstruct verbatim training\nexamples, rather than just representative “fuzzy\" examples,\nwhich increases the risk of exposing verbatim conﬁdential\nrecords. For instance, they can extract sensitive information\nword for word, such as social security numbers or pass-\nwords (Carlini et al. 2021).\nBased on the characteristics of this attack, a train-\ning data extraction attack was employed against GPT-2,\ndemonstrating that this attack is applicable to any language\nmodel (Carlini et al. 2021). GPT-2 poses various privacy\nrisks, including but not limited to disrupting data secrecy in\nLLMs, causing direct privacy leakage, and violating contex-\n123\n163 Page 10 of 23 Journal of King Saud University Computer and Information Sciences (2025) 37 :163\ntual norms around expected information use (i.e., contextual\nintegrity). Bai et al. ( 2024) introduced a simple but effec-\ntive data extraction attack, Special Characters Attack (SCA),\nwhich uses two sets of special characters and one set of\nEnglish letters to trigger the output of raw training data\nfrom the memorization capabilities of LLMs. They revealed\na possible mechanism in LLMs: if the model generates\nuncontrolled or incoherent responses, increasing the chance\nof revealing memorized text. This ﬁnding prompted the\nenhancement of SCA to extract more raw data, thereby rais-\ning greater privacy concerns.\n3.2.3 User-based attacks\nMembership inference attacks A Membership Inference\nAttack (MIA) is an attack that allows attackers to infer user\ndata information from sample data of the target machine\nlearning model (Yang 2022). This involves inferring infor-\nmation about training data, model parameters, and other\nattributes by examining the output of the model or its\nresponses to queries (Yan et al. 2024). Since machine learn-\ning models are typically trained on conﬁdential information,\nsuch attacks can lead to user data disclosure and compromis-\ning privacy protections. Moreover, inference attacks may also\njeopardize the intellectual property of the model owner (Yang\n2022).\nCurrently, there are two types of MIAs designed for LLMs,\nboth of which share the common issue of heavily relying on\nthe overﬁtting of the target model. To tackle this issue, Fu\net al. ( 2023) introduced a specialized membership inference\nattack, Self-calibrated Probabilistic V ariation membership\ninference attack (SPV-MIA). In this attack, they designed\na self-promoting method to extract a reference dataset by\nprompting the target LLM and collecting the generated text.\nInstead of using probabilities as membership signals, they\nopted to identify member records based on memorization,\nwhich poses higher privacy risks. A study conducted by Duan\net al. ( 2024b) discovered that in large-scale LLMs, the use\nof extensive training data and near-one epoch training sig-\nniﬁcantly reduces the attack performance of MIAs. This\nindicates that, due to the lack of memorization of member\ndata, MIAs cannot effectively attack pre-trained LLMs. It has\nbeen shown that the attack performance of MIAs on LLMs\nand their training data is still largely unexplored and that the\nperformance of MIAs is unstable.\nAttribute inference attacks Attribute Inference Attacks aim\nto deduce missing attributes from partially known records\nin the training dataset by interacting with machine learn-\ning models via an Application Programming Interface\n(API) (Zhao et al. 2021). In today’s internet, attackers use\nseemingly innocent user information published on online\nsocial platforms to deduce the missing attributes of users,\nmeaning that privacy attributes can be deduced from pub-\nlicly available user data (Gong and Liu 2018).\nNotably, due to improved contextual understanding and\ntext generation capabilities, LLMs demonstrate the ability to\nautonomously infer a wide range of personal attributes from\nlarge volumes of unstructured text provided during infer-\nence (Staab et al. 2023). Chen et al. ( 2021) developed an\neffective attribute inference attack that can infer sensitive\nattributes from APIs based on BERT training data. Their\nexperiments have shown that such attacks can compromise\nproprietary information or expose user data hosted by the API\nowners and lead to privacy leakage. Additionally, many of\nthese attacks circumvent existing defense mechanisms. Fine-\ntuned LLMs further exacerbate the risk, as they can reveal\ninferred sensitive attributes - such as personal IDs, health\ninformation, and location data - through contextual reason-\ning.\n3.2.4 Summary\nThis subsection summarizes various privacy attacks on\nLLMs, which are categorized into model-based, data-based,\nand user-based attacks. Model-based attacks, such as back-\ndoor attacks, model inversion attacks, and model stealing\nattacks, aim to manipulate model behavior or extract internal\nrepresentations. Data-based attacks, including data stealing\nand training data extraction, focus on recovering original\ntraining data or inputs. User-based attacks, such as mem-\nbership inference and attribute inference attacks, target the\nidentiﬁcation of individuals in the training set or the inference\nof sensitive user traits. Although these attacks are grouped\nby focus, they are closely interconnected. For example, a\nstolen model may be used to launch inversion or membership\ninference attacks more effectively, while data extracted from\ntraining may enable accurate attribute inference. Backdoored\nmodels may increase the model’s vulnerability to user-based\nor data-based exploitation. This interconnectedness reveals\nthat these attacks often reinforce each other, forming a chain\nof escalating risks. Understanding how these attack types\ninteract is essential for developing comprehensive defenses\nthat go beyond isolated threat models.\n3.3 Lessons learned\nThis section highlights key insights gained from the analy-\nsis of privacy leakage and privacy attacks in LLMs. First,\nprivacy risks in LLMs are not limited to isolated incidents;\nrather, they emerge from complex and often subtle interac-\ntions across different layers of model behavior, training data,\nand user interaction. Sensitive information leakage - whether\nthrough queries, ﬁne-tuning, or contextual inference - often\nlays the groundwork for more targeted privacy attacks. Like-\nwise, model, data, and user-based attacks are not mutually\n123\nJournal of King Saud University Computer and Information Sciences (2025) 37 :163 Page 11 of 23 163\nexclusive but can reinforce each other, creating cascading\nrisks. These forms of interconnectedness demonstrate that\naddressing privacy concerns in LLMs requires a compre-\nhensive, system-level perspective. Piecemeal defenses are\nunlikely to succeed unless the dependencies between leakage\nand attack vectors are well understood and jointly mitigated.\nThe lessons learned underscore the importance of designing\nLLM architectures and deployment strategies with privacy as\na ﬁrst-class consideration, integrating protective mechanisms\nat all stages of the model lifecycle.\n4 PrivacymitigationinLLMs\nAs LLMs are increasingly integrated into critical AI appli-\ncations, their associated vulnerabilities have garnered sub-\nstantial research focus. Therefore, safeguarding LLMs from\nprivacy risks is essential to preserve their reliability and\noperational integrity. It is imperative to develop robust defen-\nsive measures to secure LLMs. In this section, we review\nresearch on mitigating LLM vulnerabilities to address emerg-\ning privacy issues. Speciﬁcally, we analyze current defense\nstrategies, outlining their strengths, limitations, and unique\ntraits to inform future enhancements in LLMs security.\nBased on the classiﬁcation of privacy issues into privacy\nleakage and privacy attacks, depending on how attackers\naccess sensitive information, we categorize the correspond-\ning defense strategies into two main types: defense against\nprivacy leakage and defense against privacy attacks. As illus-\ntrated in Fig. 2, each type of privacy risk is matched with\nspeciﬁc mitigation strategies. Tables 3 and 4 further summa-\nrize these defense mechanisms in detail.\n4.1 Defense against privacy leakage\nData cleaning Data cleaning, which entails detecting and\nrectifying errors, handling missing values, and resolving\ninconsistencies in the dataset to enhance its quality, pro-\ntecting sensitive information through anonymization, data\nminimization, and security practices, is essential for ensur-\ning privacy protection. More speciﬁcally, data cleaning can\nremove or anonymize Personally Identiﬁable Information\n(PII), including name, address, social security number, etc.,\nto reduce the likelihood of individual re-identiﬁcation in the\ndataset. This approach can consolidate data at a higher level\nto lessen the chances of re-identiﬁcation. For instance, rather\nthan keeping track of each inference query, the queries can\nbe summarized by day or week (Yan et al. 2024).\nGiven the ease with which private data leakage can\noccur, V enditti et al. ( 2024) introduced Private Association\nEditing (PAE), a new defense strategy to reduce private data\nleakage, to eliminate stored private information by modifying\nFig. 2 Taxonomy of LLM’s privacy in this survey. This diagram\npresents a structured mapping between privacy issues and correspond-\ning defense strategies in LLMs. Privacy issues are divided into Privacy\nLeakage and Privacy Attacks. Leakage arises from sensitive queries or\nﬁne-tuning and is mitigated by data cleaning, inference detection, and\nfederated learning. Attacks - such as backdoor, model inversion, model\nstealing, and membership inference - are countered by techniques like\nbackdoor removal, conﬁdential computing, cryptography, and differen-\ntial privacy. The diagram highlights a clear one-to-one correspondence\nbetween each type of privacy risk and its defense method\n123\n163 Page 12 of 23 Journal of King Saud University Computer and Information Sciences (2025) 37 :163\nTable 3 Classiﬁcation of Countermeasures for Privacy Leakage\nCategory Work Method Evaluated Model Dataset Evaluation Metric\nV enditti et al. ( 2024) Private Association Editing GPT-J Book3, The Enron Emails BLEU, METEOR, Acc,\nFleiss’ K\nData Cleaning Ullah et al. ( 2024) Reinforcement Learning GPT-3, ChatGPT / Model Performance, Acc,\nScalability, etc\nCarvalho et al. ( 2023) Data Perturbation Logistic Regression, Adult Income dataset, ACC, F1-score,\nRandom Forest, etc German Credit dataset, etc AUC-ROC, etc\nGoyal et al. ( 2024) Ensemble of FLAN-T5, GPT-2 Test sets of detectors Acc, AUC, PII NER F1\nsafety detectors (racial bias,PII, etc)\nSun et al. ( 2023) Federated Learning, RoBERTa, Llama 2 SST-2, Yelp, AG’s News Acc\nblack box optimization\nY uan et al. ( 2021) Federated Learning LSTM, FL model Penn Treebank, WikiText-2, Top-K Accuracy,\nEnwik8 Top-K Smallest-\nEdit Distance\nFederated Learning Sun et al. ( 2024) TrustLLM benchmark GPT-4, ChatGPT, SQuAD2.0, TruthfulQA, Acc, F1-Score\nacross multi-dimensions LLaMA-2, etc AdvGLUE, etc\nZhao et al. ( 2023) Soft Prompt Tuning GPT, BERT AG News, IMDB, Acc, ASR\nYelp, etc\nWu et al. ( 2024) Black Box Discrete GPT-3 / Acc\nPrompt Tuning\nTong et al. ( 2023) Differential privacy GPT-4, BERT CNN/Daily Mail, Diversity, MAUVE,\nWikitext-103-v1 Coherence\nInference Detection Mireshghallah et al. ( 2023) Differential Privacy GPT-4, ChatGPT, InstructGPT, / Sensitivity Score/Error Rate\nLlama-2 Chat, Llama-2 Chat, etc\nYao et al. ( 2024) Instance Obfuscation LMaaS SST-2, SST-5, Acc, F1\nMRPC, QNLI\nKim et al. ( 2024) Black-box Probing, OPT-350M, OPT-1.3B, Pile dataset string match\nWhite-box Probing OPT-2.7B\nThis table organizes defense strategies according to the leakage types they aim to mitigate, based on experimental studies, including implementati on details and evaluation settings\n123\nJournal of King Saud University Computer and Information Sciences (2025) 37 :163 Page 13 of 23 163\nTable 4 Defense Measures Against Privacy Attacks\nCategory Work Method Evaluated Model Dataset Evaluation Metric\nSha et al. ( 2022) Fine-tuning ML model CIFAR10, CIFAR100, STL10, ASR, Clean Accuracy,\nGTSRB, SVHN Computational Cost\nBackdoor Removal Zhu et al. ( 2023) Fine-tuning PreAct-ResNet18, CIFAR-10, Tiny ImageNet,\nVGG19-BN GTSRB Acc, ASR, DER\nLiu et al. ( 2018) Deep learning, / Dtrain , Dvalid ,A S R , A c c\nFine-Pruning Face dataset\nXu et al. ( 2023) Importance Driven ResNet-18, VGG-16, CIFAR-10, GTSRB ASR,CA\nCloning DenseNet-121\nChen et al. ( 2022b) Approximation method Raw, ReLU, ReLU-S, SST-2, MRPC, STS-B, Acc, F1, P/S corr.\nReLU-S-L, HE etc m/mm, Precision,\nRecall, Perf\nCryptography Dong et al. ( 2022) Neural Network ResNet50 BC-TCGA, GSE2034, etc Non-replicability,\nInference PneumoniaMNIST, Utility\nDermaMNIST,\nYao et al. ( 2024) Instance Obfuscation LMaaS SST-2, SST-5, MRPC, Acc, F1\nQNLI\nLuo et al. ( 2024) Machine learning based BERTBASE, BERTLARGEE RTE, MRPC, CoLA, Acc\non secret sharing STS-B, QNLI\nConﬁdential Computing Chen et al. ( 2023a) Trusted Execution / NA TIVE X, Latency, time\nEnvironments P W/O T X, etc\nHuang et al. ( 2024) Fine-tuning, Federal LLM, SWMT CHIP-CTC, LoRA,P-Tuning v2\nlightweight encryption KUAKE-IR, etc\nZhu et al. ( 2020) AI workloads VGG16, GoogLeNet, / Performance Overhead\nResNet50, ResNet101,\nResNet152\nP l a n te ta l .(2022) Differential privacy Baseline (FX), Gl, Trustpilot dataset, Acc, F1\nKo, Be, MB, Ro, etc OSCAR dataset\nDifferential Privacy Flemings et al. ( 2024) Private Mixing of GPT-2 small / Perplexity, Memory Usage,\nEnsemble Distributions Wall-clock Training Time\nMeisenbacher et al. ( 2024) Word-Level LSTM-based model AG News, IMDb Utility Metrics, Privacy Metrics,\nMetric DP Composite Score\nThis table outlines practical defense techniques against privacy attacks, with reference to experimental papers, covering key methodological and evaluation aspects\n123\n163 Page 14 of 23 Journal of King Saud University Computer and Information Sciences (2025) 37 :163\nthe parameters of LLMs, eliminating the need for pre-\ntraining. Generative Artiﬁcial Intelligence (AI) tools based\non LLMs use a large number of parameters to extensively\nanalyze vast datasets and extract key information. However,\nthe extracted data may contain sensitive information that rep-\nresents a signiﬁcant risk to user privacy, leading users to be\nreluctant to use such tools. To tackle this problem, Ullah et al.\n(2024) designed a conceptual model (PrivChatGPT), which\nprotects user privacy through two main components: data\ncuration and preprocessing. This model safeguards private\ncontext and large-scale data during the training process to\navoid privacy leakage. Data curation primarily involves replac-\ning training data with forged or randomly generated data.\nIn addition, recent work has introduced LLMGuard, a\nmodular toolkit designed to detect and sanitize sensitive\ncontent in real-time during LLM interactions. It enhances\ndata cleaning pipelines by preventing the exposure of private\ninformation at inference time through effective PII detection\nand response ﬁltering (Goyal et al. 2024). Even though it is\nnot a part of traditional data cleaning, LLMGuard comple-\nments it by providing real-time sanitization during inference,\nforming a more complete privacy protection pipeline.\nWhile data cleaning offers a lightweight and practi-\ncal defense, it primarily operates at the data level and\nmay be limited in handling deeper model memorization.\nIt enhances privacy without requiring model retraining, but\noverly aggressive cleaning may affect data utility or model\nperformance. Research on data privacy and predictive per-\nformance found that higher levels of privacy protection\ncan lead to decreased model accuracy, demonstrating the\ntrade-off between privacy preservation and predictive per-\nformance (Carvalho et al. 2023).\nFederated learning The development of LLMs has encoun-\ntered challenges in practical applications, primarily due to\nlimited public access to domain-speciﬁc data and the need\nto preserve its privacy. To address these issues, Federated\nLearning (FL) has become a promising approach that enables\nprivacy-preserving collaborative training on distributed data,\nintegrating privacy protection measures into collaborative\nmodeling (Chen et al. 2023b). Through decentralized train-\ning, models are trained across multiple edge devices or\nservers while safeguarding data privacy (Yan et al. 2024). A\nfederated learning framework (FedBPT), introduced by Sun\net al. ( 2023), is designed to preserve privacy while tuning\nlanguage models. This framework optimizes prompts locally\nand transmits only prompt updates to minimize communica-\ntion costs and mitigate privacy risks. Compared to white-box\nmethods such as FedPrompt (Zhao et al. 2023), which\nrequire access to model gradients and parameters, FedBPT\noperates in a purely black-box setting, further enhancing\nprivacy and reducing transmission overhead. Furthermore,\ncomparing FedBPT with other black-box approaches like\nFedAvg-BBT and FedBiOT (Wu et al. 2024), demonstrat-\ning competitive performance in accuracy, communication\nefﬁciency, and robustness under non-IID data distributions.\nBy integrating federated learning with black-box optimiza-\ntion algorithms, this approach facilitates secure, collaborative\nmodel enhancement without disclosing sensitive data. How-\never, FedBPT cannot completely prevent privacy leakage, as\nmalicious servers may extract private user data from shared\ngradients.\nRecent researches have identiﬁed privacy leakage in FL,\nparticularly in tasks like image categorization, including\nclass representative reconstruction (Y uan et al. 2021). The\ncombination of these methods not only improves the privacy\npreservation capability of the model, but also enhances multi-\nparty collaboration in privacy-sensitive domains, especially\nin applications involving sensitive data. Currently, FL still\nfaces challenges including gradient leakage, heterogeneous\nsystems, and performance drop under non-IID distributions.\nThese limitations highlight the need for continued reﬁnement\nin privacy mechanisms and optimization strategies (Kairouz\net al. 2021). To address these gaps, TRUSTLLM - a recent\nbenchmark suite introduced by Sun et al. ( 2024) - incor-\nporates privacy evaluation protocols speciﬁcally tailored to\nfederated LLM training, enabling systematic assessment of\nprivacy robustness in decentralized environments. Therefore,\nin the federated learning of LLMs, precise and stage-speciﬁc\noptimization and design are crucial for improving the effec-\ntiveness and efﬁciency of privacy protection at different\nstages. However, FL represents a notable technical advance-\nment in enabling LLMs to be trained on decentralized\ndata without compromising privacy. Its strengths include\nimproved data protection, reduced communication overhead,\nand the ability to leverage diverse data sources.\nInference detection Existing defense schemes for LLMs\nhave been ineffective in safeguarding the privacy of docu-\nments within prompts during the inference process in actual\ntext generation tasks (Tong et al. 2023). Considering the\npotential privacy risks in the text generated by the model,\ndetection and inference-based methods can identify and mit-\nigate such risks.\nCONFAIDE, proposed by Mireshghallah et al. ( 2023), is a\nbenchmark based on context integrity theory. It aims to iden-\ntify critical ﬂaws in the privacy reasoning ability of LLMs\nduring instruction optimization while demonstrating through\nexperiments the broader issue of the lack of reasoning abil-\nity of the model. The Instance-Obfuscated Inference (IOI)\nmethod was developed to address privacy issues of deci-\nsions in natural language understanding tasks throughout\ntheir entire lifecycle (Yao et al.\n2024). The IOI workﬂow\nfor privacy protection of decisions is depicted in Fig. 3.\nTo preserve the privacy of the whole document in the\nblack-box LLM inference process and address the informa-\n123\nJournal of King Saud University Computer and Information Sciences (2025) 37 :163 Page 15 of 23 163\nFig. 3 Demonstration of IOI workﬂow for privacy protection of deci-\nsions. When a user submits a health-related query, the system does\nnot return a direct plaintext response. Instead, the query is com-\nbined with a textual obfuscator, and both are separately encoded by a\nprivacy-preserving module. This module generates non-invertible and\nunlinkable representations. The pretrained language model (PLM) pro-\ncesses these encoded inputs without knowing their connection, the\noriginal query, or the expected output. Only the user can interpret the\nﬁnal diagnosis. To enhance privacy, each query is paired with multiple\nobfuscators and sent to the PLM in random order (Yao et al. 2024)\ntion bias caused by differential privacy, Tong et al. ( 2023)\nintroduced a framework (InferDPt), which not only protects\nthe privacy of prompts but also enhances the capabilities of\nremote LLMs, improving the quality of text generated by\nlocal models. However, detection methods aim at identify-\ning privacy leakage by directly examining the text generated\nby LLMs. Based on this principle, research has demonstrated\na novel detection tool (ProPILE), aimed at making data sub-\njects or PII owners aware of potential PII leakage through\nLLM-based services. In contrast, it allows data subjects to\ndevelop prompts using their own PII to assess the degree of\nprivacy infringement in LLMs (Kim et al. 2024).\nInference detection has advanced the interpretability of\nprivacy leakage and improved user awareness. However, it\nstill faces challenges. These methods often depend on pre-\ndeﬁned deﬁnitions of sensitive information, limiting their\nability to detect diverse forms of privacy leakage. Their effec-\ntiveness may also decline when models infer sensitive data\nthrough reasoning rather than direct memorization (Staab\net al. 2023). Nevertheless, recent work like ProPILE signals\na shift toward active user participation and controllable pri-\nvacy assessment, moving defense strategies toward greater\ninteractivity and practicality (Kim et al. 2024).\n4.1.1 Summary\nThis subsection summarizes key defense strategies against\nprivacy leakage in LLMs, including data cleaning, federated\nlearning, and inference detection. Data cleaning reduces the\npresence of sensitive information before training and serves\nas a foundational safeguard. Inference detection monitors\nmodel outputs to identify potential leaks, while federated\nlearning enables decentralized training to keep raw user\ndata local. Though each method targets a different stage,\ntheir effectiveness is closely interconnected. For example,\ndata cleaning strengthens the privacy guarantees of feder-\nated learning, and inference detection can catch leaks that\noccur despite decentralized training. Insights from detection\ncan also inform future data sanitization.\n4.2 Defense against privacy attacks\nBackdoor removal Backdoor attacks, one of the main threats\ncurrently faced by LLMs, manifest in several ways: secu-\nrity threats, decreased model performance, and signiﬁcant\ndata privacy issues. Defense strategies designed to counter\nbackdoor attacks include effective and secure measures, with\nbackdoor removal being a key approach to protect LLMs.\nSha et al. ( 2022) demonstrated that ﬁne-tuning is one\nof the most common and easily adopted machine learning\ntraining operations that effectively removes backdoors from\nmachine learning models while maintaining high model prac-\nticality. Building on this, they proposed super ﬁne-tuning,\nnoting that ﬁne-tuning models in independent scenarios may\npose higher risks to member privacy. However, experimen-\ntal results demonstrate that after super ﬁne-tuning, the risk of\nmember leakage is further diminished. Therefore, from a pri-\nvacy leakage standpoint, ﬁne-tuning has negligible negative\nconsequences on the target model. Fine-tuning using benign\ndata naturally serves as a defense to remove backdoor effects\nfrom compromised models. To improve the defense effective-\nness of basic ﬁne-tuning with limited benign data, Zhu et al.\n(2023) introduced Fine-Tuning Sharpness-Aware Minimiza-\ntion (FT-SAM), which promotes the learning of backdoor\n123\n163 Page 16 of 23 Journal of King Saud University Computer and Information Sciences (2025) 37 :163\nneurons and alleviates backdoor effects. FT-SAM is deﬁned\nas follows:\nTw = diag (|w1| ,|w2| , ...,|wd |) ∈ Rd×d , (6)\nwhere wi is the i -th entry of w, to set an adaptive pertur-\nbation budget for different neurons and encourage larger\nperturbations for neurons with larger weight norms, which\nare more likely related to the backdoor effect. Additionally,\nstudies (Liu et al. 2018) suggest combining pruning and ﬁne-\ntuning as promising defense measures. Evaluations of their\neffectiveness have shown that these methods can effectively\nweaken or even eliminate backdoors in the model.\nWhile ﬁne-tuning-based methods are practical and widely\napplicable, they address threats at the model behavior\nlevel rather than solely focusing on data protection. Recent\nadvances incorporate adaptive techniques like neuron attribu-\ntion and weight sensitivity to precisely identify and neutralize\nmalicious components within the model. This shift from\ngeneric data masking to targeted model-level interventions\nenhances defense effectiveness, especially against stealthy\nbackdoors and in scenarios with limited clean data, which\nremain challenging to handle (Xu et al. 2023).\nCryptography To safeguard the privacy of LLMs, cryptogra-\nphy-based techniques play a critical role. These methods pri-\nmarily prevent sensitive information from being leaked to\nunauthorized third parties by ensuring the protection and reli-\nability of data. Homomorphic encryption (Acar et al. 2018)\nis one of the advanced encryption techniques that allow spe-\nciﬁc computational operations to be executed on encrypted\ndata without the necessity of decrypting it initially. This\nfeature enables homomorphic encryption to perform useful\ncomputations while protecting data privacy, thus enabling\nprivacy-preserving computation across a range of domains.\nGiven the complex calculations of Transformer blocks, it\nis difﬁcult for pre-trained models to infer ciphertext data,\nand currently, homomorphic encryption tools do not support\nthis. To address this limitation, Chen et al. ( 2022b) introduced\nTHE-X, an approximation method for Transformers that pro-\nvides privacy protection for pre-trained models developed by\npopular frameworks.\nMulti-party computation (Dong et al. 2022) ensures that\nmultiple participants jointly complete model training or\ninference tasks without leaking their respective data via\nprotocols such as secret sharing and secure aggregation,\nthereby effectively protecting the privacy of LLMs. Nonethe-\nless, the application of secure multi-party computing in\nPrivacy-Preserving Inference (PPI) for Transformer models\nfrequently results in signiﬁcant performance degradation or\nslowdowns. PPI is deﬁned as follows:\nM(E (x)) → y\n′, (7)\nwhere the encoding function E (·) serves two purposes: (1)\nencode the original x into privacy-preserving representations\nthat M can interpret; (2) transition the inference results from\nthe actual prediction y to the privacy-protected output y′ (Yao\net al. 2024). Luo et al. ( 2024) introduced a comprehensive\nframework, SecFormer, to effectively remove the high-cost\nindex and maximum operations in PPI without compromis-\ning effectiveness.\nIn cryptography, functional secret sharing (Boyle et al.\n2015) is a unique encryption technique that revolves around\nthe core idea of dividing a secret or data into multiple\nparts. These parts alone cannot reveal the original data, but\ncan only be reconstructed or used for computation when a\ndeﬁned quorum of shares is combined. Defense measures\nbased on homomorphic encryption, multi-party computa-\ntion, and functional secret sharing provide provable security\nguarantees in LLMs threatened by privacy attacks. Despite\nthe advancements in efﬁciency for key components, exper-\nimental ﬁndings suggest that their implementation could\ncause performance deterioration. Alternative methods often\nleverage the concept of obfuscation; however, their unpre-\ndictability and protection capabilities are lower compared to\nencryption-based solutions, with most focusing on mitigat-\ning speciﬁc attacks (Yan et al. 2024).\nCryptography-based techniques, such as homomorphic\nencryption and secure multi-party computation, provide\nstrong theoretical privacy guarantees for LLMs. Recent\nadvances have improved efﬁciency by approximating com-\nplex operations and optimizing protocols, enabling more\npractical privacy-preserving inference. However, these meth-\nods still face signiﬁcant challenges including high com-\nputational overhead, performance degradation, and limited\nsupport for complex models like Transformers. Balancing\nsecurity and efﬁciency remains a key obstacle for their\nwidespread adoption in real-world LLM applications.\nConﬁdential computing In the context of LLMs, conﬁden-\ntial computing safeguards privacy across training, inference,\nand deployment stages. For example, it protects sensitive\ndata during model training, enforces secure execution envi-\nronments during inference to prevent tampering or leakage,\nand leverages techniques such as remote attestation and data\nsealing during deployment to ensure integrity and trust (Sabt\net al. 2015;H ue ta l . 2024). Conﬁdential computing has been\napplied in both research and industry to address privacy and\nsecurity challenges across different contexts (Mo et al. 2024).\nConﬁdential computing employs a hardware-Trusted Exe-\ncution Environment (TEE) to protect data in use. TEEs have\nemerged as a solution to privacy issues, providing a hidden\nenvironment for computing and data analysis. They ensure\nprivacy through isolation, encryption, and attestation. The\nworkﬂow of conﬁdential computing as a service is illustrated\nin the diagram Fig. 4 (Chen et al. 2023a).\n123\nJournal of King Saud University Computer and Information Sciences (2025) 37 :163 Page 17 of 23 163\nFig. 4 CCaaS workﬂow. After remote attestation, the user establishes\na secure channel with the platform. The data provider then sends input\ndata to the Conﬁdential Computing as a Service (CCaaS) framework.\nThis framework decrypts the data, performs the conﬁdential computing\ntask, re-encrypts the result, and sends the output back to the user. CCaaS\ncan serve multiple data providers as a shared service (Chen et al. 2023a)\nTo deploy TEEs on both ends, a method was proposed to\nensure secure communication and enable partitioned model\ntuning while preserving accuracy (Huang et al. 2024). Nev-\nertheless, current TEEs still cannot support the extensive\npractical requirements of large-scale conﬁdential computing\nin LLMs. In response, Zhu et al. ( 2020) proposed the ﬁrst het-\nerogeneous TEE framework that truly supports large-scale\nor data-intensive computing without any chip-level modiﬁ-\ncations.\nConﬁdential computing offers hardware-enforced protec-\ntion for LLMs by securing data during training, inference,\nand deployment. Unlike traditional software-based isola-\ntion methods, it enables computation on sensitive data\nwithin trusted execution environments (TEEs), signiﬁcantly\nreducing exposure risks. However, current TEEs still face\nchallenges in memory limitations, limited hardware support,\nand integration complexity, which constrain their applicabil-\nity in large-scale LLMs scenarios.\nDifferential privacy LLMs typically need a substantial vol-\nume of data for training, including users’ personal infor-\nmation, conversation records, behavioral habits, and more.\nAttackers often infer and extract sensitive data from the\ntraining data. To address this issue, a technique known as\ndifferential privacy is frequently employed to safeguard data\nprivacy, especially in ﬁelds like statistical publishing and data\nanalysis (Y u et al. 2021). The goal is to enable researchers\nto derive valuable insights from the entire dataset without\ndisclosing any speciﬁc individual data (Yan et al. 2024).\nAdditionally, differential privacy introduces mathematical\nmechanisms that add random noise during data processing\nand model training, making it challenging for attackers to\ndeduce particular personal details, even if they obtain the\ntraining data of the model (Y u et al. 2021). This approach\nhelps protect user privacy and reduces the risk of data leak-\nage. Given that larger and more complex models are more\nprone to leaking private information, differential privacy may\nhave signiﬁcant effects on model utility. Plant et al. ( 2022)\nproposed using hybrid or metric differential privacy tech-\nniques to mitigate these effects. “Hybrid\" means the com-\nbination of adversarial and local differential privacy, which\nseeks to combine the generalization beneﬁts of differential\nprivacy with the protection of sensitive variables exposed\nthrough adversarial analysis. In their evaluation, hybrid and\nmetric-DP methods were shown to outperform standard local\ndifferential privacy by offering stronger privacy protection\nwith less degradation in model utility, achieving a better pri-\nvacy–utility trade-off (Meisenbacher et al. 2024)\nAs a technical advancement in privacy-preserving machine\nlearning, differential privacy offers formal mathematical\nguarantees, making it widely adopted in LLM-related research.\nHowever, implementing differential privacy in large-scale\nlanguage model scenarios poses challenges such as increased\ntraining time, higher memory consumption, and degraded\nutility. Even with moderate privacy budgets (e.g., ε = 8),\nstandard methods like DP-SGD struggle to maintain genera-\ntion quality, necessitating novel approaches such as PMixED\nto mitigate utility loss (Flemings et al. 2024).\n4.2.1 Summary\nThis subsection outlines key defense strategies against\nprivacy attacks in LLMs, including differential privacy,\nbackdoor removal, cryptographic methods, and conﬁdential\ncomputing. Differential privacy introduces noise to pro-\ntect individual data points, while backdoor removal detects\nand neutralizes malicious model behaviors. Cryptographic\ntechniques, such as secure computation and homomorphic\nencryption, safeguard data during processing. Conﬁdential\ncomputing secures model execution within trusted envi-\nronments. These defenses are closely interconnected. For\ninstance, differential privacy can reduce the impact of data-\nbased attacks even when cryptographic safeguards are lim-\nited. Conﬁdential computing can enhance backdoor removal\nby protecting the model at runtime. Meanwhile, cryptogra-\nphy and conﬁdential computing work hand in hand-securing\nboth data and the execution environment. This intercon-\nnectedness underscores the value of layered defenses that\ncollectively address diverse and overlapping threats.\n4.3 Lessons learned\nThis section highlights key lessons from current strategies\nfor mitigating privacy risks in LLMs. Effective privacy pro-\n123\n163 Page 18 of 23 Journal of King Saud University Computer and Information Sciences (2025) 37 :163\ntection requires more than isolated techniques - it depends\non understanding the interconnectedness among risks and\ndefenses. For example, data cleaning is lightweight and\nproactive but limited by the accuracy of data labeling. Infer-\nence detection is useful during deployment but may suffer\nfrom high false positives. Federated learning enhances user-\nlevel data protection but is vulnerable to poisoning and\nrequires signiﬁcant coordination. Differential privacy offers\nstrong theoretical guarantees but can degrade model perfor-\nmance. Conﬁdential computing and cryptographic methods\nprovide robust safeguards during training and inference, yet\noften introduce high computational costs and hardware con-\nstraints. These trade-offs suggest that no single solution is\nsufﬁcient. Empirical studies further support this view: most\ndefenses are validated through simulated attacks - such as\nmembership inference or model inversion - where metrics\nlike attack success rate, exposure scores, and precision-\nrecall are used to demonstrate effectiveness (Kim et al.\n2024). These experiments often involve standard benchmark\ndatasets and comparative baselines to ensure reproducibil-\nity and practical relevance. The observed results reveal that\ncombining complementary techniques - such as using infer-\nence detection to monitor outputs in federated settings, or\nintegrating cryptography with trusted hardware - can address\nmultiple threats simultaneously. Ultimately, privacy mitiga-\ntion in LLMs should follow a layered, system-level design\nthat balances protection strength, practicality, and adaptabil-\nity.\n5 Practicalchallengesandfuturedirections\nDespite signiﬁcant advancements in privacy protection for\nLLMs, some practical challenges remain unaddressed. Future\nresearch should focus on the following directions.\n5.1 Privacy-preserving model compression\nReducing the size of LLMs through compression tech-\nniques, such as pruning, quantization, and knowledge dis-\ntillation (Zhu et al. 2024), is a common practice aimed at\nimproving computational efﬁciency and reducing storage and\nlatency requirements for deployment. While these techniques\nare essential for making LLMs more accessible and scalable,\nthey often come with a critical trade-off: a potential loss of\nprivacy. During compression, sensitive information embed-\nded in the model weights or activations may inadvertently be\nexposed. For example, when knowledge distillation is used,\nthe student model acquires knowledge from the outputs of the\nteacher model, which may carry indirect traces of sensitive\ndata from the training process (Qin et al. 2024).\nFederated learning offers an important avenue for securely\ncompressing LLMs. By training the model in a decentral-\nized manner across multiple clients and only aggregating\nmodel updates, federated learning prevents direct exposure\nto sensitive data, making it a natural ﬁt for privacy-conscious\nmodel compression. Applying federated learning techniques\nto model compression could enable collaborative, privacy-\npreserving compression of large models without centralizing\ndata (McMahan et al. 2017). This would allow organizations\nto share model improvements and compress models without\ndirectly accessing the underlying sensitive data.\nAdditionally, the development of privacy-aware prun-\ning techniques (Chu et al. 2024), where individual model\nparameters or neurons are selectively pruned based on their\ncontribution to overall privacy risk, could further reduce the\nleakage of sensitive information. By designing pruning algo-\nrithms that consider privacy concerns, it is possible to prune\nmodels in a way that minimizes the risk of data leakage.\n5.2 Privacy risk assessment\nAccurately assessing privacy risks in LLMs presents a fun-\ndamental challenge due to the complexity and scale of these\nmodels (Ye et al. 2023), as well as the variety of sensitive data\nthey may encounter during training, ﬁne-tuning, and infer-\nence. LLMs trained on vast and diverse datasets inadvertently\nmemorize sensitive information embedded within the data,\nmaking it necessary to establish comprehensive frameworks\nfor privacy risk evaluation. These frameworks must account\nfor multiple factors, including the potential for data leakage,\nadversarial vulnerabilities, and compliance with legal and\nregulatory standards governing data protection.\nIn future work, we need to build robust privacy risk evalua-\ntion frameworks that assess the full spectrum of privacy risks\nassociated with LLMs. These frameworks should include\nmethods for evaluating data leakage risks, such as member-\nship inference (Fu et al. 2023) and attribute inference (Gong\nand Liu 2018) attacks, where an LLM might inadvertently\nreveal private, sensitive information about individuals or\norganizations through model outputs or gradients. Future\nframeworks should also incorporate tools for model auditing,\nwhich can systematically assess how an LLM processes and\nstores sensitive information. Such audits can identify whether\nthe model retains PII or conﬁdential details that might be\nreconstructed through attacks (Carlini et al. 2021). Moreover,\nauditing tools should examine whether the model’s design\nand training procedures align with privacy guidelines deﬁned\nby regulatory bodies, ensuring that LLMs remain compliant\nwith privacy laws throughout their lifecycle.\n5.3 Secure knowledge sharing across LLMs\nAs LLMs are increasingly ﬁne-tuned and shared across orga-\nnizations, ensuring secure and efﬁcient knowledge transfer\nwithout exposing proprietary or sensitive data has become\n123\nJournal of King Saud University Computer and Information Sciences (2025) 37 :163 Page 19 of 23 163\na critical concern. Collaborative model training, such as\ncross-organizational model sharing (Su et al. 2025), has the\npotential to foster progress in natural language processing\nwhile safeguarding the privacy of the underlying datasets.\nHowever, these methods introduce new challenges related to\ndata leakage, model inversion, and unauthorized exposure of\nconﬁdential data. Safeguarding the privacy of the data used\nfor training, as well as the knowledge embedded within the\ntrained models, requires innovative cryptographic techniques\nthat enable secure knowledge transfer.\nIn this context, methods like Secure Multi-Party Compu-\ntation (SMPC) (Feng and Yang 2022) and Zero-Knowledge\nProofs (ZKPs) (Sun et al. 2021) offer some promising\nsolutions. SMPC allows each participant to perform com-\nputations on their combined data while maintaining the data\nitself conﬁdential. This technique is particularly useful for\nLLM training in federated environments, where data pri-\nvacy is a concern but collaboration among different parties\nis still necessary. Additionally, ZKPs enable one party to\ndemonstrate to another that they possess certain knowledge\n(e.g., a model’s parameter updates or the correctness of a\ncomputation) without revealing the knowledge itself (Daf-\ntardar et al. 2024). The application of ZKPs to LLMs,\nparticularly in settings where multiple organizations wish\nto collaboratively train a model without sharing their sen-\nsitive datasets, represents a key area for future exploration.\nHybrid cryptographic protocols that combine SMPC, ZKPs,\nand other privacy-preserving techniques could provide even\nmore secure and efﬁcient solutions for cross-organizational\nknowledge sharing. For instance, SMPC can be used for col-\nlaborative training, while ZKPs can verify that the shared\ncomputations are correct without disclosing any private\ndata.\n5.4 Practical deployment and trade-offs\nThe deployment of LLMs in industry settings involves nav-\nigating complex privacy challenges, and leading companies\nadopt distinct strategies. For instance, OpenAI emphasizes\ndata minimization and differential privacy during ﬁne-tuning,\nwhile Anthropic employs “Constitutional AI\" to align out-\nputs with ethical guidelines. Both companies implement\nstrict data anonymization and access controls, yet face\ntrade-offs between usability and privacy. Industrial practices\nprioritize scalable solutions, often favoring data minimiza-\ntion and access controls over academic-focused methods like\nfederated learning or rigorous differential privacy guarantees,\nwhich face hurdles in real-world latency and complexity.\nFuture directions of industrial practices highlight primar-\nily the need for architectures such as SecGPT (Wu et al.\n2024), which isolates third-party app execution to prevent\ndata leaks, and synthetic data generation (Endres et al. 2022)\nto reduce the reliance on sensitive training corpora. In addi-\ntion, we would leverage hybrid privacy-preserving methods\nto mitigate leakage risks while maintaining utility. For exam-\nple, the encrypted inference method (Peng et al. 2023) can\nbe used to protect sensitive client data, where a Graph\nConvolution Network (GCN) achieves enhanced conver-\ngence through a hierarchical two-level distillation framework\nderived from an all-ReLU teacher model. Nonetheless,\nindustry adoption of these methods remains limited due to\ncomputational overhead, highlighting a gap between aca-\ndemic advances and practical deployment in LLMs.\n5.5 Interdisciplinary approaches to privacy\ngovernance\nEffective privacy protection for LLMs is an inherently inter-\ndisciplinary challenge that necessitates collaboration among\nAI researchers, legal experts, and policymakers. As LLMs\nbecome more ubiquitous in applications across various indus-\ntries - from healthcare and ﬁnance to customer service and\ncontent moderation - the risk of privacy violations escalates,\nmaking it essential to establish a robust framework that bal-\nances the technological potential of LLMs with the protection\nof sensitive data (Kibriya et al. 2024). Developing such a\nframework requires the integration of technical, ethical, and\nlegal perspectives, ensuring that privacy protection strategies\nare both scientiﬁcally sound and compliant with relevant reg-\nulations.\nA crucial component of this effort is ensuring compliance\nwith data protection laws. Researchers must explore ways to\nintegrate privacy-preserving technologies within the frame-\nwork of these regulations. For example, while the General\nData Protection Regulation (GDPR) emphasizes the right to\ndata erasure (the “right to be forgotten\") (V oigt and V on dem\nBussche 2017), ensuring compliance is a complex challenge.\nLLMs must be designed to prevent them from retaining pri-\nvate information that could violate this principle. Current\nLLMs are typically trained in massive web corpora that may\ninadvertently include personal or sensitive data. Once mem-\norized, such information can resurface in model outputs,\nraising concerns about compliance and data subject rights.\nTo this end, selective unlearning and data attribution tracing\nhave emerged as promising technical remedies. For instance,\nthe unlearning process can be formulated as a min-max opti-\nmization problem and addressed in two stages. In the attack\nstage, perturbation vectors are trained and injected into the\nlatent space of the LLM to recover the targeted, unlearned\nknowledge. In the defense stage, these perturbation vectors\nare leveraged to improve the robustness of the unlearned\nmodel against such recovery attempts (Y uan et al. 2025).\nMoreover, collaborative efforts should focus on creating\nopen-source privacy benchmarks that assess the privacy risks\nof LLMs (Li et al. 2024a) in a standardized and transpar-\nent manner. The development of these benchmarks will help\n123\n163 Page 20 of 23 Journal of King Saud University Computer and Information Sciences (2025) 37 :163\nimprove accountability and transparency in the deployment\nof LLMs, providing both the AI community and regulators\nwith tools to measure how well data security and privacy\nprotections are implemented in practice. Ultimately, by facil-\nitating interdisciplinary collaboration and ongoing research,\nwe can ensure that LLMs are deployed in ways that prioritize\nprivacy, transparency, and accountability.\n6 Conclusion\nThis survey provided a comprehensive overview of the\nprivacy risks associated with LLMs, focusing on privacy\nleakage and privacy attacks, as well as the defenses avail-\nable to mitigate these risks. We systematically discussed\nthe various ways in which LLMs can inadvertently expose\nsensitive information through mechanisms such as model\ninversion, training data extraction, and membership infer-\nence. Additionally, we categorized and reviewed existing\nprivacy preservation techniques, including inference detec-\ntion, federated learning, and conﬁdential computing, evaluat-\ning their strengths and limitations. Another key contribution\nof this survey is the identiﬁcation of practical challenges in\nimplementing effective privacy protections. Furthermore, we\noutlined future research directions, emphasizing the need for\nmore scalable, transparent, and efﬁcient privacy solutions.\nBy synthesizing current research, we aim to provide a clearer\nunderstanding of the privacy landscape in LLMs and guide\nfuture efforts to develop privacy-conscious AI systems.\nAcknowledgements This research was supported in part by the\nNational Natural Science Foundation of China [No. 62107022], and\nin part by the Startup Fund of Jimei University [No. ZQ2024014].\nAuthor Contributions Y .Lin: Conceptualization, Project administra-\ntion, Funding acquisition, Writing—original draft. K. Chen: V alidation,\nInvestigation, Writing—original draft. X. Zhou: Data curation, Formal\nanalysis, Writing—review and editing. L. Shen: Supervision, Writ-\ning—review and editing. S. Feng: Methodology. P . Wu: Visualization,\nResources.\nData Availability The datasets used and/or analyzed during the cur-\nrent study are available from the corresponding author upon reasonable\nrequest.\nDeclarations\nCompeting interests The authors declare that they have no competing\ninterests.\nOpen Access This article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and repro-\nduction in any medium or format, as long as you give appropriate credit\nto the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed mate-\nrial. Y ou do not have permission under this licence to share adapted\nmaterial derived from this article or parts of it. The images or other\nthird party material in this article are included in the article’s Creative\nCommons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article’s Creative Commons\nlicence and your intended use is not permitted by statutory regula-\ntion or exceeds the permitted use, you will need to obtain permission\ndirectly from the copyright holder. To view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by-nc-nd/4.0/.\nReferences\nAcar A, Aksu H, Uluagac AS, Conti M (2018) A survey on homo-\nmorphic encryption schemes: Theory and implementation. ACM\nComput Surv (Csur) 51(4):1–35\nAchiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL,\nAlmeida D, Altenschmidt J, Altman S, Anadkat S et al (2023)\nGpt-4 technical report. arXiv preprint arXiv:2303.08774\nApthorpe N, Shvartzshnaider Y , Mathur A, Reisman D, Feamster N\n(2018) Discovering smart home internet of things privacy norms\nusing contextual integrity. Proc ACM Interact Mob Wearable Ubiq-\nuit Technol 2(2):1–23\nBai Y , Pei G, Gu J, Yang Y , Ma X (2024) Special characters attack:\nToward scalable training data extraction from large language mod-\nels. arXiv preprint arXiv:2405.05990\nBang Y , Cahyawijaya S, Lee N, Dai W, Su D, Wilie B, Lovenia H,Ji Z,\nY u T, Chung W et al (2023) A multitask multilingual multimodal\nevaluation of chatgpt on reasoning hallucination and interactivity.\narXiv preprint arXiv:2302.04023\nBommasani R, Hudson DA, Adeli E, Altman R, Arora S, Arx S,\nBernstein M.S, Bohg J, Bosselut A, Brunskill E et al (2021) On\nthe opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258\nBoyle E, Gilboa N, Ishai Y (2015) Function secret sharing. In: Annual\ninternational conference on the theory and applications of crypto-\ngraphic techniques, Springer, pp 337–367\nBubeck S, Chadrasekaran V , Eldan R, Gehrke J, Horvitz E, Kamar E,\nLee P , Lee YT, Li Y , Lundberg S et al (2023) Sparks of artiﬁcial\ngeneral intelligence: Early experiments with gpt-4. ArXiv\nCarlini N, Tramer F, Wallace E, Jagielski M, Herbert-V oss A, Lee K,\nRoberts A, Brown T, Song D, Erlingsson U et al (2021) Extract-\ning training data from large language models. In: 30th USENIX\nsecurity symposium (USENIX Security 21), pp 2633–2650\nCarvalho T, Moniz N, Faria P , Antunes L (2023) Towards a data privacy-\npredictive performance trade-off. Expert Syst Appl 223:119785\nChan C, Cheng J, Wang W, Jiang Y , Fang T, Liu X, Song Y (2023)\nChatgpt evaluation on sentence level relations: A focus on temporal\ncausal and discourse relations. arXiv preprint arXiv:2304.14827\nChen D, Zhou X (2024) Attmoe: Attention with mixture of experts for\nremaining useful life prediction of lithium-ion batteries. J Energy\nStorage 84:110780\nChen D, Hong W, Zhou X (2022a) Transformer network for remain-\ning useful life prediction of lithium-ion batteries. IEEE Access\n10:19621–19628\nChen J, Liu Z, Huang X, Wu C, Liu Q, Jiang G, Pu Y , Lei Y , Chen X,\nWang X et al (2024) When large language models meet personal-\nization: Perspectives of challenges and opportunities. World Wide\nWeb 27(4):42\nChen T, Bao H, Huang S, Dong L, Jiao B, Jiang D, Zhou H, Li J, Wei\nF (2022b) The-x: Privacy-preserving transformer inference with\nhomomorphic encryption. arXiv preprint arXiv:2206.00216\nChen H, Chen HH, Sun M, Li K, Chen Z, Wang X (2023a) A veriﬁed\nconﬁdential computing as a service framework for privacy preser-\nvation. In: 32nd USENIX security symposium (USENIX Security\n23), pp 4733–4750 (2023)\nChen C, Feng X, Zhou J, Yin J, Zheng X (2023b) Federated large lan-\nguage model: A position paper. arXiv preprint arXiv:2307.08925\n123\nJournal of King Saud University Computer and Information Sciences (2025) 37 :163 Page 21 of 23 163\nChen C, He X, Lyu L, Wu F (2021) Killing one bird with two stones:\nModel extraction and attribute inference attacks against bert-based\napis. arXiv preprint arXiv:2105.10909\nChu T, Yang M, Laoutaris N, Markopoulou A (2024) Priprune: Quan-\ntifying and preserving privacy in pruned federated learning. ACM\nTrans Model Perform Eval Comput Syst\nDaftardar A, Reagen B, Garg S (2024) Szkp: A scalable accelerator\narchitecture for zero-knowledge proofs. In: Proceedings of the\n2024 international conference on parallel architectures and com-\npilation techniques, pp 271–283\nDas BC, Amini MH, Wu Y (2025) Security and privacy challenges of\nlarge language models: A survey. ACM Comput Surv 57(6):1–39\nDhungana B, Ghimire V , Shrestha Lama J, Sadat N, Caporusso N, Doan\nM (2025) Assessing cybersecurity awareness of ChatGPT’s new\nmemory feature. Presented at posters-at-the-capitol Northern Ken-\ntucky University\nDong C, Weng J, Liu JN, Zhang Y , Tong Y , Yang A, Cheng Y , Hu S\n(2022) Fusion: Efﬁcient and secure inference resilient to malicious\nservers. arXiv preprint arXiv:2205.03040\nDuan H, Dziedzic A, Papernot N, Boenisch F (2024a) Flocks of\nstochastic parrots: Differentially private prompt learning for large\nlanguage models. Adv Neural Inf Process Syst 36\nDuan M, Suri A, Mireshghallah N, Min S, Shi W, Zettlemoyer L,\nTsvetkov Y , Choi Y , Evans D, Hajishirzi H (2024b) Do mem-\nbership inference attacks work on large language models? arXiv\npreprint arXiv:2402.07841\nEndres M, Mannarapotta V enugopal A, Tran TS (2022) Synthetic data\ngeneration: A comparative study. In: Proceedings of the 26th inter-\nnational database engineered applications symposium, pp 94–102\nEsmradi A, Yip DW, Chan CF (2023) A comprehensive survey of attack\ntechniques implementation and mitigation strategies in large lan-\nguage models. In: International conference on ubiquitous security,\nSpringer, pp 76–95\nFan W, Li H, Deng Z, Wang W, Song Y (2024) Goldcoin: Ground-\ning large language models in privacy laws via contextual integrity\ntheory. arXiv preprint arXiv:2406.11149\nFeng D, Yang K (2022) Concretely efﬁcient secure multi-party compu-\ntation protocols: survey and more. Secur Saf 1:2021001\nFlemings J, Razaviyayn M, Annavaram M (2024) Differentially private\nnext-token prediction of large language models. arXiv preprint\narXiv:2403.15638\nFredrikson M, Jha S, Ristenpart T (2015) Model inversion attacks that\nexploit conﬁdence information and basic countermeasures. In: Pro-\nceedings of the 22nd ACM SIGSAC conference on computer and\ncommunications security, pp 1322–1333\nFu W, Wang H, Gao C, Liu G, Li Y , Jiang T (2023) Practical membership\ninference attacks against ﬁne-tuned large language models via self-\nprompt calibration. arXiv preprint arXiv:2311.06062\nGao X, Zhang L (2023) {PCA T}: Functionality and data stealing from\nsplit learning by {Pseudo-Client} attack. In: 32nd USENIX secu-\nrity symposium (USENIX Security 23), pp 5271–5288\nGong NZ, Liu B (2018) Attribute inference attacks in online social\nnetworks. ACM Trans Priv Secur (TOPS) 21(1):1–30\nGoyal S, Hira M, Mishra S, Goyal S, Goel A, Dadu N, DB K, Mehta S,\nMadaan N (2024) Llmguard: Guarding against unsafe llm behav-\nior. Proceedings of the AAAI conference on artiﬁcial intelligence\n38:23790–23792\nHe J, Hou G, Jia X, Chen Y , Liao W, Zhou Y , Zhou R (2024) Data\nstealing attacks against large language models via backdooring.\nElectron 13(14):2858\nHuang W, Wang Y , Cheng A, Zhou A, Y u C, Wang L (2024) A fast per-\nformant, secure distributed training framework for large language\nmodel. arXiv preprint arXiv:2401.09796\nHu G, Wu Y , Chen G, Dinh TTA, Ooi BC (2024) Sesemi: Secure\nserverless model inference on sensitive data. arXiv preprint\narXiv:2412.11640\nKairouz P , McMahan HB, Avent B, Bellet A, Bennis M, Bhagoji\nAN, Bonawitz K, Charles Z, Cormode G, Cummings R et al\n(2021) Advances and open problems in federated learning. Found\nTrends® Mach Learn 14(1–2):1–210\nKasneci E, Seßler K, Küchemann S, Bannert M, Dementieva D, Fischer\nF, Gasser U, Groh G, Günnemann S, Hüllermeier E et al (2023)\nChatgpt for good? on opportunities and challenges of large lan-\nguage models for education. Learn Individ Differ 103:102274\nKibriya H, Khan WZ, Siddiqa A, Khan MK (2024) Privacy issues in\nlarge language models: a survey. Comput Electr Eng 120:109698\nKim S, Y un S, Lee H, Gubri M, Y oon S, Oh SJ (2024) Propile: Probing\nprivacy leakage in large language models. Adv Neural Inf Process\nSyst 36\nKshetri N (2023) Cybercrime and privacy threats of large language\nmodels. IT Prof 25(3):9–13\nKurita K, Michel P , Neubig G (2020) Weight poisoning attacks on pre-\ntrained models. arXiv preprint arXiv:2004.06660\nLi H, Guo D, Li D, Fan W, Hu Q, Liu X, Chan C, Yao D, Yao Y , Song Y\n(2023) Privlm-bench: A multi-level privacy evaluation benchmark\nfor language models. arXiv preprint arXiv:2311.04044\nLi Q, Hong J, Xie C, Tan J, Xin R, Hou J, Yin X, Wang Z, Hendrycks\nD, Wang Z et al (2024a) Llm-pbe: Assessing data privacy in large\nlanguage models. arXiv preprint arXiv:2408.12787\nLi Y , Li T, Chen K, Zhang J, Liu S, Wang W, Zhang T, Liu Y (2024b)\nBadedit: Backdooring large language models by model editing.\narXiv preprint arXiv:2403.13355\nLi L, Song D, Li X, Zeng J, Ma R, Qiu X (2021) Backdoor attacks on\npre-trained models by layerwise weight poisoning. arXiv preprint\narXiv:2108.13888\nLi D, Sun R, Huang Y , Zhong M, Jiang B, Han J, Zhang X, Wang\nW, Liu H (2025) Preference leakage: A contamination problem in\nllm-as-a-judge. arXiv preprint arXiv:2502.01534\nLiu Y , Huang J, Li Y , Wang D, Xiao B (2025) Generative AI model\nprivacy: A survey. Artif Intell Rev 58(1):1–47\nLiu K, Dolan-Gavitt B, Garg S (2018) Fine-pruning: Defending\nagainst backdooring attacks on deep neural networks. In: Interna-\ntional symposium on research in attacks intrusions and defenses,\nSpringer, pp 273–294\nLuo J, Zhang Y , Zhang Z, Zhang J, Mu X, Wang H, Y u Y ,\nXu Z (2024) Secformer: Fast and accurate privacy-preserving\ninference for transformer models via smpc. Findings of the\nassociation for computational linguistics ACL 2024:13333–\n13348\nMcMahan B, Moore E, Ramage D, Hampson S, Arcas BA (2017)\nCommunication-efﬁcient learning of deep networks from decen-\ntralized data. In: Artiﬁcial intelligence and statistics, PMLR, pp\n1273–1282\nMeisenbacher S, Nandakumar N, Klymenko A, Matthes F (2024)\nA comparative analysis of word-level metric differential pri-\nvacy: Benchmarking the privacy-utility trade-off. arXiv preprint\narXiv:2404.03324\nMireshghallah N, Kim H, Zhou X, Tsvetkov Y , Sap M, Shokri R, Choi\nY (2023) Can llms keep a secret? testing privacy implications of\nlanguage models via contextual integrity theory. arXiv preprint\narXiv:2310.17884\nMo F, Tarkhani Z, Haddadi H (2024) Machine learning with conﬁdential\ncomputing: A systematization of knowledge. ACM Comput Surv\n56(11):1–40\nNandagopal S (2025) Securing retrieval-augmented generation\npipelines: A comprehensive framework. J Comput Sci Technol\nStud 7(1):17–29\nNeel S, Chang P (2023) Privacy issues in large language models: A\nsurvey. arXiv preprint arXiv:2312.06717\nOkey OD, Udo EU, Rosa RL, Rodríguez DZ, Kleinschmidt JH (2023)\nInvestigating chatgpt and cybersecurity: A perspective on topic\nmodeling and sentiment analysis. Comput Secur 135:103476\n123\n163 Page 22 of 23 Journal of King Saud University Computer and Information Sciences (2025) 37 :163\nOuyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P , Zhang\nC, Agarwal S, Slama K, Ray A et al (2022) Training language\nmodels to follow instructions with human feedback. Adv Neural\nInf Process Syst 35:27730–27744\nPeng H, Ran R, Luo Y , Zhao J, Huang S, Thorat K, Geng T, Wang\nC, Xu X, Wen W et al (2023) Lingcn: Structural linearized graph\nconvolutional network for homomorphically encrypted inference.\nAdv Neural Inf Process Syst 36:20679–20694\nPlant R, Giuffrida V , Gkatzia D (2022) Y ou are what you write: Pre-\nserving privacy in the era of large language models. arXiv preprint\narXiv:2204.09391\nQin L, Zhu T, Zhou W, Y u PS (2024) Knowledge distillation in federated\nlearning: A survey on long lasting challenges and new solutions.\narXiv preprint arXiv:2406.10861\nSabt M, Achemlal M, Bouabdallah A (2015) Trusted execution envi-\nronment: What it is and what it is not. In: 2015 IEEE Trust-\ncom/BigDataSE/Ispa, vol 1. IEEE, pp 57–64\nSha Z, He X, Berrang P , Humbert M, Zhang Y (2022) Fine-tuning\nis all you need to mitigate backdoor attacks. arXiv preprint\narXiv:2212.09067\nSha Z, Zhang Y (2024) Prompt stealing attacks against large language\nmodels. arXiv preprint arXiv:2402.12959\nShen H, Gu Z, Hong H, Han W (2025) Pii-bench: Evaluat-\ning query-aware privacy protection systems. arXiv preprint\narXiv:2502.18545\nShokri R, Stronati M, Song C, Shmatikov V (2017) Membership\ninference attacks against machine learning models. In: 2017\nIEEE symposium on security and privacy (SP), IEEE, pp 3–\n18\nShu Y , Li S, Dong T, Meng Y , Zhu H (2025) Model inversion in split\nlearning for personalized llms: New insights from information bot-\ntleneck theory. arXiv preprint arXiv:2501.05965\nSinghal K, Tu T, Gottweis J, Sayres R, Wulczyn E, Amin M, Hou L,\nClark K, Pfohl SR, Cole-Lewis H et al (2025) Toward expert-level\nmedical question answering with large language models. Nat Med\n1–8\nStaab R, V ero M, Balunovi´ c M, V echev M (2023) Beyond memoriza-\ntion: Violating privacy via inference with large language models.\narXiv preprint arXiv:2310.07298\nSu J, Xu B, Jiang L, Liu H, Chen Y , Li Y et al (2025) Cross-\norganizational knowledge sharing partner selection based on fogg\nbehavioral model in probabilistic hesitant fuzzy environment.\nExpert Syst Appl 260:125348\nSun X, Y u FR, Zhang P , Sun Z, Xie W, Peng X (2021) A survey on\nzero-knowledge proof in blockchain. IEEE Netw 35(4):198–205\nSun L, Huang Y , Wang H, Wu S, Zhang Q, Gao C, Huang Y , Lyu\nW, Zhang Y , Li X et al (2024) Trustllm: Trustworthiness in large\nlanguage models. 3. arXiv preprint arXiv:2401.05561\nSun J, Xu Z, Yin H, Yang D, Xu D, Chen Y , Roth HR (2023) Fedbpt:\nEfﬁcient federated black-box prompt tuning for large language\nmodels. arXiv preprint arXiv:2310.01467\nTaori R, Gulrajani I, Zhang T, Dubois Y , Li X, Guestrin C, Liang P ,\nHashimoto TB (2023) Alpaca: A strong replicable instruction-\nfollowing model. Stanf Center Res Found Model 3(6):7. https://\ncrfm.stanford.edu/2023/03/13/alpaca.html\nThomas P , Spielman S, Craswell N, Mitra B (2024) Large language\nmodels can accurately predict searcher preferences. In: Proceed-\nings of the 47th international acm sigir conference on research and\ndevelopment in information retrieval, pp 1930–1940\nTong M, Chen K, Qi Y , Zhang J, Zhang W, Y u N (2023) Privinfer:\nPrivacy-preserving inference for black-box large language model.\narXiv preprint arXiv:2310.12214\nTouvron H, Lavril T, Izacard G, Martinet X, Lachaux MA, Lacroix,\nT, Rozière B, Goyal N, Hambro E, Azhar F et al (2023) Llama:\nOpen and efﬁcient foundation language models. arXiv preprint\narXiv:2302.13971\nTruong JB, Maini P , Walls RJ, Papernot N (2021) Data-free model\nextraction. In: Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pp 4771–4780\nUllah I, Hassan N, Gill SS, Suleiman B, Ahanger TA, Shah Z, Qadir\nJ, Kanhere SS (2024) Privacy preserving large language models:\nChatgpt case study based vision and framework. IET Blockchain\n4:706–724\nV enditti D, Ruzzetti ES, Xompero GA, Giannone C, Favalli A,\nRomagnoli R, Zanzotto FM (2024) Enhancing data privacy in\nlarge language models through private association editing. arXiv\npreprint arXiv:2406.18221\nV oigt P , Bussche A (2017) The eu general data protection regulation\n(gdpr). A Practical Guide 1st Ed, Cham: Springer International\nPublishing. 10(3152676):10–5555\nWang JT, Wu T, Song D, Mittal P , Jia R (2025) Greats: Online selection\nof high-quality data for llm training in every iteration. Adv Neural\nInf Process Syst 37:131197–131223\nWang J, Hu X, Hou W, Chen H, Zheng R, Wang Y , Yang L, Huang\nH, Ye W, Geng X et al (2023) On the robustness of chatgpt:\nAn adversarial and out-of-distribution perspective. arXiv preprint\narXiv:2302.12095\nWang S, Zhu T, Liu B, Ding M, Guo X, Ye D, Zhou W, Y u PS (2024)\nUnique security and privacy threats of large language model: A\ncomprehensive survey. arXiv preprint arXiv:2406.07973\nWu J , C h e n S , Ya n g Y, L i Y, H o u S , J i n g R , Wa n g Z , C h e n\nW, Tian Z (2024) Feddtpt: Federated discrete and transferable\nprompt tuning for black-box large language models. arXiv preprint\narXiv:2411.00985\nWu Y , Roesner F, Kohno T, Zhang N, Iqbal U (2024) Secgpt: An exe-\ncution isolation architecture for llm-based systems. arXiv e-prints\n2403\nXia L, Fan J, Parlikad A, Huang X, Zheng P (2024) Unlocking large lan-\nguage model power in industry: Privacy-preserving collaborative\ncreation of knowledge graph. IEEE Trans Big Data\nX i a o Y, J i n Y, B a i Y, Wu Y, Ya n g X , L u o X , Yu W, Z h a o X , L i u Y, G u Q\net al (2023) Privacymind: large language models can be contextual\nprivacy protection learners. arXiv preprint arXiv:2310.02469\nXu Q, Tao G, Honorio J, Liu Y , An S, Shen G, Cheng S, Zhang X (2023)\nMedic: Remove model backdoors via importance driven cloning.\nIn: Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp 20485–20494\nYang Y (2022) Holistic risk assessment of inference attacks in machine\nlearning. arXiv preprint arXiv:2212.10628\nYan B, Li K, Xu M, Dong Y , Zhang Y , Ren Z, Cheng X (2024) On\nprotecting the data privacy of large language models (llms): A\nsurvey. arXiv preprint arXiv:2403.05156\nYao Y , Duan J, Xu K, Cai Y , Sun Z, Zhang Y (2024) A survey on large\nlanguage model (llm) security and privacy: The good the bad and\nthe ugly. High-Conﬁdence Comput 100211\nYao Y , Wang F, Ravi S, Chen M (2024) Privacy-preserving lan-\nguage model inference with instance obfuscation. arXiv preprint\narXiv:2402.08227\nYe W, Ou M, Li T, Ma X, Yanggong Y , Wu S, Fu J, Chen G, Wang\nH, Zhao J et al (2023) Assessing hidden risks of llms: an empiri-\ncal study on robustness consistency and credibility. arXiv preprint\narXiv:2305.10235\nY uan X, Ma X, Zhang L, Fang Y , Wu D (2021) Beyond class-level pri-\nvacy leakage: Breaking record-level privacy in federated learning.\nIEEE Internet Things J 9(4):2555–2565\nY uan H, Jin Z, Cao P , Chen Y , Liu K, Zhao J (2025) Towards robust\nknowledge unlearning: An adversarial framework for assessing\nand improving unlearning robustness in large language models.\nProc AAAI Conf Artif Intell 39:25769–25777\nY u D, Naik S, Backurs A, Gopi S, Inan HA, Kamath G, Kulkarni J,\nLee YT, Manoel A, Wutschitz L et al (2021) Differentially private\nﬁne-tuning of language models. arXiv preprint arXiv:2110.06500\n123\nJournal of King Saud University Computer and Information Sciences (2025) 37 :163 Page 23 of 23 163\nZamﬁrescu-Pereira J, Wong RY , Hartmann B, Yang Q (2023) Why\njohnny can’t prompt: how non-ai experts try (and fail) to design llm\nprompts. In: Proceedings of the 2023 CHI conference on human\nfactors in computing systems, pp 1–21\n˙Zarski TL, Janicki A (2025) Enhancing privacy while preserving context\nin text transformations by large language models. Inf 16(1):49\nZhang R, Hidano S, Koushanfar F (2022) Text revealer: Private text\nreconstruction via model inversion attacks against transformers.\narXiv preprint arXiv:2209.10505\nZhang Y , Jia R, Pei H, Wang W, Li B, Song D (2020) The secret revealer:\nGenerative model-inversion attacks against deep neural networks.\nIn: Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp 253–261\nZhang Z, Ren X, Su Q, Sun X, He B (2021) Neural network surgery:\nInjecting data patterns into pre-trained models with minimal\ninstance-wise side effects. In: Proceedings of the 2021 conference\nof the north american chapter of the association for computational\nlinguistics: human language technologies, pp 5453–5466\nZhao BZH, Agrawal A, Coburn C, Asghar HJ, Bhaskar R, Kaafar MA,\nWebb D, Dickinson P (2021) On the (in) feasibility of attribute\ninference attacks on machine learning models. In: 2021 IEEE euro-\npean symposium on security and privacy (EuroS&P), IEEE, pp\n232–251\nZhao H, Du W, Li F, Li P , Liu G (2023) Fedprompt: Communication-\nefﬁcient and privacy-preserving prompt tuning in federated learn-\ning. In: ICASSP 2023-2023 IEEE international conference on\nacoustics speech and signal processing (ICASSP), IEEE, pp 1–\n5\nZhu X, Li J, Liu Y , Ma C, Wang W (2024) A survey on model com-\npression for large language models. Trans Assoc Comput Linguist\n12:1556–1577\nZhu J, Hou R, Wang X, Wang W, Cao J, Zhao B, Wang Z, Zhang Y , Ying\nJ, Zhang L et al (2020) Enabling rack-scale conﬁdential computing\nusing heterogeneous trusted execution environment. In: 2020 IEEE\nsymposium on security and privacy (SP), IEEE, pp 1450–1465\nZhu M, Wei S, Shen L, Fan Y , Wu B (2023) Enhancing ﬁne-tuning\nbased backdoor defense with sharpness-aware minimization. In:\nProceedings of the IEEE/CVF international conference on com-\nputer vision, pp 4466–4477\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\nAuthorsandAﬃliations\nKang Chen 1,2 · Xiuze Zhou 3 · Yuanguo Lin 1 · Shibo Feng 4 · Li Shen 5 · Pengcheng Wu 6\nB Y uanguo Lin\nxdlyg@jmu.edu.cn\nB Li Shen\nls6743@nyu.edu\nKang Chen\nchenkang@kean.edu\nXiuze Zhou\nxz.zhou@connect.hkust-gz.edu.cn\nShibo Feng\nshibo001@ntu.edu.sg\nPengcheng Wu\npengchengwu@ntu.edu.sg\n1 School of Computer Engineering, Jimei University, 361021\nXiamen, China\n2 College of Science, Mathematics and Technology,\nWenzhou-Kean University, 325060 Wenzhou, China\n3 The Hong Kong University of Science and Technology\n(Guangzhou), 511453 Guangzhou, China\n4 College of Computing and Data Science, Nanyang\nTechnological University, 639798 Singapore, Singapore\n5 School of Professional Studies, New Y ork University, 10003\nNew Y ork, United States\n6 Webank-NTU Joint Research Institute on Fintech, Nanyang\nTechnological University, 639798 Singapore, Singapore\n123",
  "topic": "Internet privacy",
  "concepts": [
    {
      "name": "Internet privacy",
      "score": 0.6308879256248474
    },
    {
      "name": "Privacy protection",
      "score": 0.5096559524536133
    },
    {
      "name": "Information privacy",
      "score": 0.4768173098564148
    },
    {
      "name": "Computer security",
      "score": 0.4460236132144928
    },
    {
      "name": "Data Protection Act 1998",
      "score": 0.44594845175743103
    },
    {
      "name": "Computer science",
      "score": 0.44054940342903137
    },
    {
      "name": "Business",
      "score": 0.3423978090286255
    }
  ]
}