{
  "title": "An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models",
  "url": "https://openalex.org/W3043024363",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227125079",
      "name": "Tu, Lifu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286993047",
      "name": "Lalwani, Garima",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226903437",
      "name": "Gella, Spandana",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105465932",
      "name": "He He",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2952989203",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963383094",
    "https://openalex.org/W2237537322",
    "https://openalex.org/W2963655793",
    "https://openalex.org/W2964125718",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2986366538",
    "https://openalex.org/W3034584102",
    "https://openalex.org/W3034381157",
    "https://openalex.org/W3035331128",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W2963420481",
    "https://openalex.org/W2970019270",
    "https://openalex.org/W3034709122",
    "https://openalex.org/W3035139434",
    "https://openalex.org/W2963542100",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2962972504",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2970789589",
    "https://openalex.org/W2994934025",
    "https://openalex.org/W2556468274",
    "https://openalex.org/W2963372062",
    "https://openalex.org/W3104240813",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2991497298",
    "https://openalex.org/W2970680991",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3035352537",
    "https://openalex.org/W2953494151",
    "https://openalex.org/W2798665661",
    "https://openalex.org/W3098843277",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W2912237282",
    "https://openalex.org/W2987241137",
    "https://openalex.org/W2984256198",
    "https://openalex.org/W2932893307"
  ],
  "abstract": "Recent work has shown that pre-trained language models such as BERT improve robustness to spurious correlations in the dataset. Intrigued by these results, we find that the key to their success is generalization from a small amount of counterexamples where the spurious correlations do not hold. When such minority examples are scarce, pre-trained models perform as poorly as models trained from scratch. In the case of extreme minority, we propose to use multi-task learning (MTL) to improve generalization. Our experiments on natural language inference and paraphrase identification show that MTL with the right auxiliary tasks significantly improves performance on challenging examples without hurting the in-distribution performance. Further, we show that the gain from MTL mainly comes from improved generalization from the minority examples. Our results highlight the importance of data diversity for overcoming spurious correlations.",
  "full_text": "An Empirical Study on Robustness to Spurious Correlations using\nPre-trained Language Models\nLifu Tu1 ∗ Garima Lalwani2 Spandana Gella2 He He3 ∗\n1Toyota Technological Institute at Chicago,2Amazon AI, 3New York University\nlifu@ttic.edu, {glalwani, sgella}@amazon.com, hehe@cs.nyu.edu\nAbstract\nRecent work has shown that pre-trained lan-\nguage models such as BERT improve robust-\nness to spurious correlations in the dataset.\nIntrigued by these results, we ﬁnd that the\nkey to their success is generalization from a\nsmall amount of counterexamples where the\nspurious correlations do not hold. When such\nminority examples are scarce, pre-trained\nmodels perform as poorly as models trained\nfrom scratch. In the case of extreme minority,\nwe propose to use multi-task learning (MTL)\nto improve generalization. Our experiments\non natural language inference and paraphrase\nidentiﬁcation show that MTL with the right\nauxiliary tasks signiﬁcantly improves perfor-\nmance on challenging examples without hurt-\ning the in-distribution performance. Further,\nwe show that the gain from MTL mainly\ncomes from improved generalization from\nthe minority examples. Our results highlight\nthe importance of data diversity for overcom-\ning spurious correlations.1\n1 Introduction\nA key challenge in building robust NLP models\nis the gap between limited linguistic variations in\nthe training data and the diversity in real-world lan-\nguages. Thus models trained on a speciﬁc dataset\nare likely to rely on spurious correlations: pre-\ndiction rules that work for the majority examples\nbut do not hold in general. For example, in natu-\nral language inference (NLI) tasks, previous work\nhas found that models learned on notable bench-\nmarks achieve high accuracy by associating high\nword overlap between the premise and the hypothe-\nsis with entailment (Dasgupta et al., 2018; McCoy\net al., 2019). Consequently, these models perform\n∗ Most work was done during ﬁrst author’s internship and\nlast author’s work at Amazon AI.\n1Code is available at https://github.com/\nlifu-tu/Study-NLP-Robustness\npoorly on the so-called challenging or adversarial\ndatasets where such correlations no longer hold\n(Glockner et al., 2018; McCoy et al., 2019; Nie\net al., 2019; Zhang et al., 2019). This issue has\nalso been referred to as annotation artifacts (Guru-\nrangan et al., 2018), dataset bias (He et al., 2019;\nClark et al., 2019), and group shift (Oren et al.,\n2019; Sagawa et al., 2020) in the literature.\nMost current methods rely on prior knowledge\nof spurious correlations in the dataset and tend\nto suffer from a trade-off between in-distribution\naccuracy on the independent and identically dis-\ntributed (i.i.d.) test set and robust accuracy2 on\nthe challenging dataset. Nevertheless, recent em-\npirical results have suggested that self-supervised\npre-training improves robust accuracy, while not\nusing any task-speciﬁc knowledge nor incurring in-\ndistribution accuracy drop (Hendrycks et al., 2019,\n2020).\nIn this paper, we aim to investigate how and\nwhen pre-trained language models such as BERT\nimprove performance on challenging datasets. Our\nkey ﬁnding is that pre-trained models are more ro-\nbust to spurious correlations because they can gen-\neralize from a minority of training examples that\ncounter the spurious pattern, e.g., non-entailment\nexamples with high premise-hypothesis word over-\nlap. Speciﬁcally, removing these counterexamples\nfrom the training set signiﬁcantly hurts their per-\nformance on the challenging datasets. In addi-\ntion, larger model size, more pre-training data, and\nlonger ﬁne-tuning further improve robust accuracy.\nNevertheless, pre-trained models still suffer from\nspurious correlations when there are too few coun-\nterexamples. In the case of extreme minority, we\nempirically show that multi-task learning (MTL)\nimproves robust accuracy by improving general-\nization from the minority examples, even though\npreivous work has suggested that MTL has limited\n2We use the term “robust accuracy” from now on to refer\nto the accuracy on challenging datasets.\nDataset Size Heuristic Input Label\nNatural language inference\nTrain MNLI 393k high word overlap P: The doctor mentioned the manager who ran. entailment⇒entailment H: The doctor mentioned the manager.\nhigh word overlap P: The actors who advised the manager saw the tourists.Test HANS 30k ⇏entailment H: The manager saw the tourists. non-entailment\nParaphrase Identiﬁcation\nTrain QQP 364k same bag-of-words S 1: Bangkok vs Shanghai? paraphrase⇒paraphrase S 2: Shanghai vs Bangkok?\nsame bag-of-words S1: Are all dogs smart or can some be dumb?Test PAWSQQP 677 ⇏paraphrase S2: Are all dogs dumb or can some be smart? non-paraphrase\nTable 1: Representative examples from the training datasets (MNLI and QQP) and the challenging/test\ndatasets (HANS and PAWSQQP). Overlaping text spans are highlighted for NLI examples and swapped\nwords are highlighted for paraphrase identiﬁcation examples. The word overlap-based heuristic that works\nfor typical training examples fails on the test data.\nadvantage in i.i.d. settings (Søgaard and Goldberg,\n2016; Hashimoto et al., 2017).\nThis work sheds light on the effectiveness of\npre-training on robustness to spurious correlations.\nOur results highlight the importance of data diver-\nsity (even if the variations are imbalanced). The\nimprovement from MTL also suggests that tradi-\ntional techniques that improve generalization in the\ni.i.d. setting can also improve out-of-distribution\ngeneralization through the minority examples.\n2 Challenging Datasets\nIn a typical supervised learning setting, we test\nthe model on held-out examples drawn from the\nsame distribution as the training data, i.e. the in-\ndistribution or i.i.d. test set. To evaluate if the\nmodel latches onto known spurious correlations,\nchallenging examples are drawn from a different\ndistribution where such correlations do not hold. In\npractice, these examples are usually adapted from\nthe in-distribution examples to counter known spu-\nrious correlations on notable benchmarks. Poor\nperformance on the challenging dataset is consid-\nered an indicator of a problematic model that relies\non spurious correlations between inputs and labels.\nOur goal is to develop robust models that have\ngood performance on both the i.i.d. test set and the\nchallenging test set.\n2.1 Datasets\nWe focus on two natural language understand-\ning tasks, NLI and paraphrase identiﬁcation (PI).\nBoth have large-scale benchmarking datasets with\naround 400k examples. While recent models have\nachieved near-human performance on these bench-\nmarks,3 the challenging datasets exploiting spuri-\nous correlations bring down the performance of\nstate-of-the-art models below random guessing.\nWe summarize the datasets used for our analysis in\nTable 1.\nNLI. Given a premise sentence and a hypothesis\nsentence, the task is to predict whether the hypoth-\nesis is entailed by, neutral with, or contradicts the\npremise. MultiNLI (MNLI) (Williams et al., 2017)\nis the most widely used benchmark for NLI, and it\nis also the most thoroughly studied in terms of spu-\nrious correlations. It was collected using the same\ncrowdsourcing protocol as its predecessor SNLI\n(Bowman et al., 2015) but covers more domains.\nRecently, McCoy et al. (2019) exploit high word\noverlap between the premise and the hypothesis\nfor entailment examples to construct a challenging\ndataset called HANS. They use syntactic rules to\ngenerate non-entailment (neutral or contradicting)\nexamples with high premise-hypothesis overlap.\nThe dataset is further split into three categories de-\npending on the rules used: lexical overlap,\nsubsequence, and constituent.\nPI. Given two sentences, the task is to predict\nwhether they are paraphrases or not. On Quora\nQuestion Pairs (QQP) (Iyer et al., 2017), one of the\nlargest PI dataset, Zhang et al. (2019) show that\nvery few non-paraphrase pairs have high word over-\nlap. They then created a challenging dataset called\nPAWS that contains sentence pairs with high word\n3See the leaderboard at https://\ngluebenchmark.com.\nTrained on MNLI Trained on QQP\nIn-distribution Challenging In-distribution Challenging\nModel MNLI-m HANS QQP PA WS QQP\nNon pre-trained baselines\nBERTscratch 67.9 (0.5) 49.9 (0.2) 83.0 (0.7) 40.6 (1.9)\nESIM 78.1 a 49.1a 85.3b 38.9b\npre-trained models\nBERTBASE (prior) 84.0 c 53.8c 90.5d 33.5d\nBERTBASE (ours) 84.5 (0.1) 62.5 (3.4) 90.8 (0.3) 36.1 (0.8)\nBERTLARGE 86.2 (0.2) 71.4 (0.6) 91.3 (0.3) 40.1 (1.8)\nRoBERTaBASE 87.4 (0.2) 74.1 (0.9) 91.5 (0.2) 42.6 (1.9)\nRoBERTaLARGE 89.1 (0.1) 77.1 (1.6) 89.0 (3.1) 39.5 (4.8)\nTable 2: Accuracies (with standard deviation) on the in-distribution datasets, MNLI-matched (MNLI-m)\nand QQP dev sets, as well as the challenging datasets, HANS and PAWSQQP. Pre-trained transformers\nimprove accuracies on both the in-distribution and challenging datasets over non pre-trained models,\nexcept on PAWSQQP. Our models ﬁne-tuned for more epochs further improve prior results on the\nchallenging data. Results taken from prior work: a He et al. (2019), b Zhang et al. (2019), c McCoy et al.\n(2019), d Zhang et al. (2019).\noverlap but different meanings through word swap-\nping and back-translation. In addition to PAWSQQP\nwhich is created from sentences in QQP, they also\nreleased PAWSWiki created from Wikipedia sen-\ntences.\n3 Pre-training Improve Robust Accuracy\nRecent results have shown that pre-trained mod-\nels appear to improve performance on challeng-\ning examples over models trained from scratch\n(Yaghoobzadeh et al., 2019; He et al., 2019;\nKaushik et al., 2020). In this section, we conﬁrm\nthis observation by thorough experiments on differ-\nent pre-trained models and motivate our inquiries.\nModels. We compare pre-trained models of dif-\nferent sizes and using different amounts of pre-\ntraining data. Speciﬁcally, we use the BERTBASE\n(110M parameters) and BERT LARGE (340M pa-\nrameters) models implemented in GluonNLP (Guo\net al., 2020) pre-trained on 16GB of text (Devlin\net al., 2019).4 To investigate the effect of size of\nthe pre-training data, we also experiment with the\nRoBERTaBASE and RoBERTaLARGE models (Liu\net al., 2019d),5 which have the same architecture as\n4 The book_corpus_wiki_en_uncased model\nfrom https://gluon-nlp.mxnet.io/model_\nzoo/bert/index.html.\n5 The openwebtext_ccnews_stories_books_cased\nmodel from https://gluon-nlp.mxnet.io/\nBERT but were trained on ten times as much text\n(about 160GB). To ablate the effect of pre-training,\nwe also include a BERTBASE model with random\ninitialization, BERTscratch.\nFine-tuning. We ﬁne-tuned all models for 20\nepochs and selected the best model based on the\nin-distribution dev set. We used the Adam opti-\nmizer with a learning rate of 2e-5, L2 weight de-\ncay of 0.01, batch sizes of 32 and 16 for base\nand large models respectively. Weights of\nBERTscratch and the last layer (classiﬁer) of pre-\ntrained models are initialized from a normal dis-\ntribution with zero mean and 0.02 variance. All\nexperiments are run with 5 random seeds and the\naverage values are reported.\nObservations and inquiries. In Table 2, we\nshow results for NLI and PI respectively. As\nexpected, they improve performance on in-\ndistribution test sets signiﬁcantly. 6 On the chal-\nlenging datasets, we make two key observations.\nFirst, while pre-trained models improve the per-\nformance on challenging datasets, the improvement\nis not consistent across datasets. Speciﬁcally, the\nimprovement on PAWSQQP are less promising than\nmodel_zoo/bert/index.html.\n6 The lower performance of RoBERTaLARGE compared to\nRoBERTaBASE is partly due to its high variance in our experi-\nments.\nEpochs\n0 5 10 15 20\nAccuracy(%)\n50\n55\n60\n65\n70\n75\n80\n85\nMNLI Dev\nHANS\n(a) NLI/HANS\nEpochs\n0 5 10 15 20\nAccuracy(%)\n20\n40\n60\n80\n100\nQQP Dev\nPAWS\nQQP (b) QQP/PAWS\nFigure 1: Accuracy on the in-distribution data (MNLI dev and QQP dev) and the challenging data\n(HANS and PAWSQQP) after each ﬁne-tuning epoch using BERTBASE. The performance plateaus on the\nin-distribution data quickly around epoch 3, however, accuracy on the challenging data keeps increasing.\nHANS. While larger models ( large vs. base)\nand more training data (RoBERTa vs. BERT) yield\na further improvement of 5 to 10 accuracy points on\nHANS, the improvement on PAWSQQP is marginal.\nSecond, even though three to four epochs of\nﬁne-tuning is typically sufﬁcient for in-distribution\ndata, we observe that longer ﬁne-tuning improves\nresults on challenging examples signiﬁcantly (see\nBERTBASE ours vs. prior in Table 2). As shown in\nFigure 1, while the accuracy on MNLI and QQP\ndev sets saturate after three epochs, the perfor-\nmance on the corresponding challenging datasets\nkeeps increasing until around the tenth epoch, with\nmore than 30% improvement.\nThe above observations motivate us to ask the\nfollowing questions:\n1. How do pre-trained models generalize to out-\nof-distribution data?\n2. When do they generalize well given the incon-\nsistent improvements?\n3. What role does longer ﬁne-tuning play?\nWe provide empirical answers to these questions\nin the next section and show that the answers are\nall related to a small amount of counterexamples in\nthe training data.\n4 Generalization from Minority\nExamples\n4.1 Pre-training Improves Robustness to\nData Imbalance\nOne common impression is that the diversity in\nlarge amounts of pre-training data allows pretrained\nmodels to generalize better to out-of-distribution\ndata. Here we show that while pre-training im-\nproves generalization, they do not enable extrapo-\nlation to unseen patterns. Instead, they generalize\nbetter from minority patterns in the training set.\nImportantly, we notice that examples in HANS\nand PAWS are not completely uncovered by the\ntraining data, but belong to the minority groups.7\nFor example, in MNLI, there are 727 HANS-like\nnon-entailment examples where all words in the\nhypothesis also occur in the premise; in QQP,\nthere are 247 PAWS-like non-paraphrase exam-\nples where the two sentences have the same bag of\nwords. We refer to these examples that counter the\nspurious correlations as minority examples. We hy-\npothesize that pre-trained models are more robust\nto group imbalance, thus generalizing well from\nthe minority groups.\nTo verify our hypothesis, we remove minority\nexamples during training and observe its effect on\nrobust accuracy. Speciﬁcally, for NLI we sort non-\nentailment (contradiction and neutral) examples in\nMNLI by their premise-hypothesis overlap, which\nis deﬁned as the percentage of hypothesis words\nthat also appear in the premise. We then remove\nincreasing amounts of these examples in the sorted\norder.\nAs shown in Figure 2, all models have signif-\nicantly worse accuracy on HANS as more coun-\nterexamples are removed, while maintaining the\noriginal accuracy when the same amounts of ran-\ndom training examples are removed. With 6.4%\n7 Following Sagawa et al. (2020), we loosely deﬁne group\nas a distribution of examples with similar patterns, e.g., high\npremise-hypothesis overlap and non-entailment.\n0.0 0.1 0.4 1.6 6.4\n% training data removed\n50\n60\n70accuracy (%)\nmodel = BERTbase\nstrategy\noverlap\nrandom\n0.0 0.1 0.4 1.6 6.4\n% training data removed\nmodel = BERTlarge\n0.0 0.1 0.4 1.6 6.4\n% training data removed\nmodel = RoBERTabase\n0.0 0.1 0.4 1.6 6.4\n% training data removed\nmodel = RoBERTalarge\nFigure 2: Accuracy on HANS when a small fraction of training data is removed. Removing non-entailment\nexamples with high premise-hypothesis overlap signiﬁcantly hurt performance compared to removing\nexamples uniformly at random.\ncounterexamples removed, the performance of\nmost pretrained models is near-random, as poor as\nnon-pretrained models. Interestingly, larger mod-\nels with more pre-training data (RoBERTaLARGE)\nappear to be slightly more robust with increased\nlevel of imbalance.\nTakeaway. These results reveal that pre-training\nimprove robust accuracy by improving the i.i.d.\naccuracy on minority groups, highlighting the im-\nportance of increasing data diversity when creating\nbenchmarks. Further, pre-trained models still suf-\nfer from suprious correlations when the minority\nexamples are scarce. To enable extrapolation, we\nmight need additional inductive bias (Nye et al.,\n2019) or new learning algorithms (Arjovsky et al.,\n2019).\n4.2 Minority Patterns Require Varying\nAmounts of Training Data\nGiven that pre-trained models generalize better\nfrom minority examples, why do we not see similar\nimprovement on PAWSQQP even though QQP also\ncontains counterexamples? Unlike HANS exam-\nples that are generated from a handful of templates,\nPAWS examples are generated by swapping words\nin a sentence followed by human inspection. They\noften require recognizing nuance syntactic differ-\nences between two sentences with a small edit\ndistance. For example, compare “ What’s classy\nif you’re poor , but trashy if you’re rich? ” and\n“What’s classy if you’re rich , but trashy if you’re\npoor?”. Therefore, we posit that more samples are\nneeded to reach good performance on PAWS-like\nexamples.\nTo test the hypothesis, we plot learning curves by\nﬁne-tuning pre-trained models on the challenging\ndatasets directly (Liu et al., 2019b). Speciﬁcally,\nwe take 11,990 training examples from PAWSQQP,\n20 40 60 80 100\n% training data\n60\n80\n100accuracy (%)\ndataset = HANS\nmodel\nBERTbase\nBERTlarge\nRoBERTabase\nRoBERTalarge\n20 40 60 80 100\n% training data\ndataset = PAWSQQP\nFigure 3: Learning curves of models trained on\nHANS and PAWSQQP. Accuracy on PAWS QQP\nincreases slowly, whereas all models quickly reach\n100% accuracy on HANS.\nand randomly sample the same number of training\nexamples from HANS;8 the rest is used as dev/test\nset for evaluation. In Figure 3, we see that all\nmodels reach 100% accuracy rapidly on HANS.\nHowever, on PAWS, accuracy increases slowly and\nthe models struggle to reach around 90% accuracy\neven with the full training set. This suggests that\nthe amount of minority examples in QQP might\nnot be sufﬁcient for reliably estimating the model\nparameters.\nTo have a qualitative understanding on why\nPAWS examples are difﬁcult to learn, we com-\npare sentence length and constituency parse tree\nheight of examples in HANS and PAWS. 9 We\nﬁnd that PAWS contains longer and syntactically\nmore complex sentences, with an average length\nof 20.7 words and parse tree height of 11.4, com-\npared to 9.2 and 7.5 on HANS. Figure 4 shows that\nthe accuracy of BERTBASE and RoBERTaBASE on\n8 HANS has more examples in total (30,000), therefore we\nsub-sample it to control for the data size.\n9 We use the off-the-shelf constituency parser from Stan-\nford CoreNLP (Manning et al., 2014). For each example, we\ncompute the maximum length (number of words) and parse\ntree height of the two sentences.\n0\n10\n20\n30\n40\n50\n[1,10) [10,15) [15,20) [20,25]\nAccuracy(%) \nConstituency Parser Tree Height \nRoBERTa BERT\n0\n10\n20\n30\n40\n50\n60\n[1,10) [10,15) [15,20) [20,25]\nAccuracy(%) \nSentence Length \nRoBERTa BERT\nFigure 4: Accuracy of BERTBASE and RoBERTaBASE on PAWSQQP decreases with increasing sentence\nlength and parse tree height.\nEpochs\n0 5 10 15 20\nLoss\n0\n0.5\n1\n1.5\n2\n2.5\ntrain(all)\ntrain(minority)\ndev(all)\ndev(minority)\n(a) Loss\nEpochs\n0 5 10 15 20\nAccuracy(%)\n40\n50\n60\n70\n80\n90\n100\ntrain(all)\ntrain(minority)\ndev(all)\ndev(minority) (b) Accuracy\nFigure 5: The average losses and accuracies of the examples in the training and dev set when ﬁne-tuning\nBERTBASE on MNLI. We show plots for the whole training set and the minority examples separately. The\nminority examples are non-entailment examples with at least 80% premise-hypothesis overlap. Accuracy\nof minority examples takes longer to plateau.\nPAWSQQP decreases as the example length and the\nparse tree height increase.\nTakeaway. We have shown that the inconsistent\nimprovement on different challenging datasets are\nresulted from the same mechanism: pre-trained\nmodels improve robust accuracy by generalizing\nfrom minority examples, however, perhaps unsur-\nprisingly, different minority patterns may require\nvarying amounts of training data. This also poses a\npotential challenge in using data augmentation to\ntackle spurious correlations.\n4.3 Minority Examples Require Longer\nﬁne-tuning\nIn the previos section, we have shown in Figure 1\nthat longer ﬁne-tuning improves accuracy on chal-\nlenging examples, even though the in-distribution\naccuracy saturates pretty quickly. To understand\nthe result from the perspective of minority exam-\nples, we compare the loss on all examples and the\nminority examples during ﬁne-tuning. Figure 5\nshows the loss and accuracy at each epoch on all\nexamples and HANS-like examples in MNLI sepa-\nrately.\nFirst, we see that the training loss of minority\nexamples decreases more slowly than the average\nloss, taking more than 15 epochs to reach near-zero\nloss. Second, the dev accuracy curves show that\nthe accuracy of minority examples plateaus later,\naround epoch 10, whereas the average accuracy\nstops to increaste around epoch 5. In addition, it\nappears that BERT does not overﬁt with additional\nﬁne-tuning based on the accuracy curves. 10 Sim-\nilary, a concurrent work (Zhang et al., 2020) has\nfound that longer ﬁne-tuning improve few-sample\nperformance.\n10 We ﬁnd that the average accuracy stays almost the same\nwhile the dev loss is increasing. Guo et al. (2017) had similar\nobservations. One possible explanation is that the model\nprediction becomes less conﬁdent (hence larger log loss), but\nthe argmax prediction is correct.\nTakeaway. While longer ﬁne-tuning does not\nhelp in-distribution accuracy, we ﬁnd that it im-\nproves performance on the minority groups. This\nsuggests that selecting models or early stopping\nbased on the i.i.d. dev set performance is insufﬁ-\ncient, and we need new model selection criteria for\nrobustness.\n5 Improve Generalization through\nMulti-task Learning\nOur results on minority examples show that increas-\ning the amount of counterexamples to spurious cor-\nrelations helps to improve model robustness. Then,\nan obvious solution is data augmentation; in fact,\nboth McCoy et al. (2019) and Zhang et al. (2019)\nshow that adding a small amount of challenging\nexamples to the training set signiﬁcantly improves\nperformance on HANS and PAWS. However, these\nmethods often require task-speciﬁc knowledge on\nspurious correlations and heavy use of rules to gen-\nerate the counterexamples. Instead of adding exam-\nples with speciﬁc patterns, we investigate the effect\nof aggregating generic data from various sources\nthrough multi-task learning (MTL). It has been\nshown that MTL reduces the sample complexity\nof individual tasks compared to single-task learn-\ning (Caruana, 1997; Baxter, 2000; Maurer et al.,\n2016), thus may further improve the generalization\ncapability of pre-trained models, especially on the\nminority groups.\n5.1 Multi-task Learning\nWe learn from datasets from different sources\njointly, where one is the target dataset to be eval-\nuated on, and the rest are auxiliary datasets. The\ntarget dataset and the auxiliary dataset can belong\nto either the same task, e.g., MNLI and SNLI, or\ndifferent but related tasks, e.g., MNLI and QQP.\nAll datasets share the representation given by the\npre-trained model, and we use separate linear clas-\nsiﬁcation layers for each dataset. The learning ob-\njective is a weighted sum of average losses on each\ndataset. We set the weight to be 1 for all datasets,\nequivalent to sampling examples from each dataset\nproportional to its size.11 During training, we sam-\nple mini-batches from each dataset sequentially\n11 Prior work has shown that the mixing weights may im-\npact the ﬁnal results in MTL, especially when there is a risk of\noverﬁtting to low-resource tasks (Raffel et al., 2019). Given\nthe relatively large dataset sizes in our experiments (Table 4),\nwe did not see signiﬁcant change in the results when varying\nthe mixing weights.\nand use the same optimization hyperparameters as\nin single-task ﬁne-tuning (Section 3) except for\nsmaller batch sizes due to memory constraints.12.\nAuxiliary datasets. We consider NLI and PI as\nrelated tasks since they both require understand-\ning and comparing the meaning of two sentences.\nTherefore, we use both benchmark datasets and\nchallenging datasets for NLI and PI as our auxil-\niary datasets. The hope is that benchmark data from\nrelated tasks helps transfer useful knowledge across\ntasks, thus improving generalization on minority\nexamples, and the challenging datasets counter-\ning speciﬁc spurious correlations further improve\ngeneralization on the corresponding minority ex-\namples. We analyze the contribution of the two\ntypes of auxiliary data in Section 5.2. The MTL\ntraining set up is shown in Table 4. 13 Details on\nthe auxiliary datasets are described in Section 2.1.\n5.2 Results\nMTL improves robust accuracy. Our main\nMTL results are shown in Table 3. MTL increases\naccuracies on the challenging datasets across tasks\nwithout hurting the in-distribution performance, es-\npecially when the minority examples in the target\ndataset is scarce (e.g., PAWS). While prior work\nhas shown limited success of MTL when tested on\nin-distribution data(Søgaard and Goldberg, 2016;\nHashimoto et al., 2017; Raffel et al., 2019), our\nresults demonstrate its value for out-of-distribution\ngeneralization.\nOn HANS, MTL improves the accuracy signif-\nicantly for BERTBASE but not for RoBERTaBASE.\nTo conﬁrm the result, we additionally experimented\nwith RoBERTaLARGE and obtained consistent re-\nsults: MTL achieves an accuracy of 75.7 (2.1) on\nHANS, similar to the STL result, 77.1 (1.6). One\npotential explanation is that RoBERTa is already\nsufﬁcient for providing good generalization from\nminority examples in MNLI.\nIn addition, both MTL and RoBERTaBASE yiedls\nbiggest improvement on lexical overlap, as\nshown in the results on HANS by category (Ta-\nble 5), We believe the reason is that lexical\n12The minibatch size of the target dataset is 16. For the\nauxiliary dataset, it is proportional to the dataset size and not\nlarger than 16, such that the total number of examples in a\nbatch is at most 32.\n13 For MNLI, we did not include other PI datasets such\nas STS-B (Cer et al., 2017) and MPRC (Dolan and Brockett,\n2005) since their sizes (3.7k and 7k) are too small compared\nto QQP and other auxiliary tasks.\nTask = MNLI Task = QQP\nIn-distribution Challenging In-distribution Challenging Challenging\nModel Algo. MNLI-m HANS QQP PA WS QQP PA WSWiki\nBERTBASE\nSTL 84.5 (0.1) 62.5 (0.2) 90.8 (0.3) 36.1 (0.8) 46.9 (0.3)\nMTL 83.7 (0.3) 68.2 (1.8) 91.3 (.07) 45.9 (2.1) 52.0 (1.9)\nRoBERTaBASE\nSTL 87.4 (0.2) 74.1 (0.9) 91.5 (0.2) 42.6 (1.9) 49.6 (1.9)\nMTL 86.4 (0.2) 72.8 (2.4) 91.7 (.04) 51.7 (1.2) 57.7 (1.5)\nTable 3: Comparison between models ﬁne-tuned with multi-task (MTL) and single-task (STL) learning.\nMTL improves robust accuracy on challenging datasets. We ran t-tests for the mean accuracies of STL\nand MTL on ﬁve runs and the larger number is bolded when they differ signiﬁcantly with a p <0.001.\nAuxiliary Datasets Size Target\nNLI PI\nMNLI 393k ✓\nSNLI 549k ✓ ✓\nQQP 364k ✓\nPAWSQQP+Wiki 60k ✓\nHANS 30k ✓\nTable 4: Auxiliary dataset sizes for the different\ntarget datasets from two tasks: NLI and PI.\nModel Algo. HANS-O HANS-C HANS-S\nBERTBASE STL 75.8 (4.9) 59.1 (4.8) 52.7 (1.2)\nBERTBASE MTL 89.5 (1.9) 61.9 (2.3) 53.1 (1.1)\nRoBERTaBASE STL 88.5 (2.0) 70.0 (2.3) 63.9 (1.4)\nRoBERTaBASE MTL 90.3 (1.2) 64.8 (3.1) 63.5 (4.9)\nTable 5: MTL Results on different cate-\ngories on HANS: lexical overlap (O),\nconstituent (C), and subsequence (S).\nBoth auxiliary data (MTL) and larger pre-training\ndata (RoBERTa) improve accuracies mainly on\nlexical overlap.\noverlap is the most representative pattern among\nhigh-overlap and non-entailment training examples.\nIn fact, 85% of the 727 HANS-like examples be-\nlongs to lexical overlap. This suggests that\nfurther improvement on HANS may require better\ndata coverage on other categories.\nOn PAWS, MTL consistently yields large im-\nprovement across pre-trained models. Given that\nQQP has fewer minority examples resembling the\npatterns in PAWS, which is also harder to learn\n(Section 4.2), the results show that MTL is an ef-\nfective way to improve generalization when the\nRemoved MNLI-m HANS ∆\nNone 83.7 (0.3) 68.2 (1.8) -\nPAWSQQP+Wiki 83.5 (0.3) 64.6 (3.5) -3.6\nQQP 83.2 (0.3) 63.2 (3.7) -5.0\nSNLI 84.3 (0.2) 66.9 (1.5) -1.3\nTable 6: Results of the ablation study on auxil-\niary datasets using BERTBASE on MNLI (the tar-\nget task). While the in-distribution performance is\nhardly affected when a speciﬁc auxiliary dataset\nis excluded, performance on the challenging data\nvaries (difference shown in ∆).\nminority examples are scarce. Next, we investigate\nwhy MTL is helpful.\nImproved generalization from minority exam-\nples. We are interested in ﬁnding how MTL helps\ngeneralization from minority examples. One possi-\nble explanation is that the challenging data in the\nauxiliary datasets prevent the model from learning\nsuprious patterns. However, the ablation studies on\nauxiliary datasets in Table 6 and Table 7 show that\nthe challenging datasets are not much more helpful\nthan benchmark datasets. The other possible expla-\nnation is that MTL reduces sample complexity for\nlearning from the minority examples in the target\ndataset. To verify this, we remove minority exam-\nples from both the auxiliary and the target datasets,\nand compare their effect on the robust accuracy.\nWe focus on PI because MTL shows largest\nimprovement there. In Table 8, we show the re-\nsults after removing minority examples in the tar-\nget dataset, QQP, and the auxiliary dataset, MNLI,\nrespectively. We also add a control baseline where\nthe same amounts of randomly sampled examples\nRemoved QQP PA WS QQP ∆\nNone 91.3 (.07) 45.9 (2.1) -\nHANS 91.5 (.06) 45.3 (1.8) -0.6\nMNLI 91.2 (.11) 42.3 (1.8) -3.6\nSNLI 91.3 (.09) 44.2 (1.3) -1.7\nTable 7: Results of the ablation study on auxiliary\ndatasets using BERTBASE on QQP (the target task).\nWhile the in-distribution performance is hardly af-\nfected when a speciﬁc auxiliary dataset is excluded,\nperformance on the challenging data varies (differ-\nence shown in ∆).\nare removed. The results conﬁrm our hypothe-\nsis: without the minority examples in the target\ndataset, MTL is only marginally better than STL on\nPAWSQQP. In contrast, removing minority exam-\nples in the auxiliary dataset has a similar effect to\nremoving random examples; both do not cause sig-\nniﬁcant performance drop. Therefore, we conclude\nthat MTL improves robust accuracy by improving\ngeneralization from minority examples in the target\ndataset.\nTakeaway. These results suggest that both pre-\ntraining and MTL do not enable extrapolation, in-\nstead, they improve generalization from minority\nexamples in the (target) training set. Thus it is im-\nportant to increase coverage of diverse patterns in\nthe data to improve robustness to spurious correla-\ntions.\n6 Related Work\nPre-training and robustness. Recently, there is\nan increasing amount of interest in studying the\neffect of pre-training on robustness. Hendrycks\net al. (2019, 2020) show that pre-training improves\nmodel robustness to label noise, class imbalance,\nand out-of-distribution detection. In cross-domain\nquestion-answering, Li et al. (2019) show that the\nensemble of different pre-trained models signif-\nicantly improves performance on out-of-domain\ndata. In this work, we answers why pre-trained\nmodels appear to improve out-of-distribution ro-\nbustness and point out the importance of minority\nexamples in the training data.\nData augmentation. The most straightforward\nway to improve model robustness to out-of-\ndistribution data is to augment the training set with\nRemoved QQP PA WS QQP ∆\nNone 91.3 (.07) 45.9 (2.1) -\nrandom examples\nQQP 91.3 (.03) 44.3 (.31 ) -1.6\nMNLI 91.4 (.02) 45.0 (1.5 ) -0.9\nminority examples\nQQP 91.3 (.09) 38.2 (.73) -7.7\nMNLI 91.3 (.08) 44.3 (2.0) -1.6\nTable 8: Ablation study on the effect of minor-\nity examples in the auxiliary (MNLI) and the tar-\nget (QQP) datasets in MTL with BERTBASE. For\nMNLI, we removed 727 non-entailment examples\nwith 100% overlap. For QQP, we removed 228\nnon-paraphrase examples with 100% overlap. We\nalso removed equal amounts of random examples\nin the control experiments. We ran t-tests for the\nmean accuracies after minority removal and ran-\ndom removal based on ﬁve runs, and numbers with\na signiﬁcant difference ( p < 0.001) are bolded.\nThe improvement from MTL mainly comes from\nbetter generalization from minority examples in the\ntarget dataset.\nexamples from the target distribution. Recent work\nhas shown that augmenting syntactically-rich ex-\namples improves robust accuracy on NLI (Min\net al., 2020). Similarly, counterfactual augmen-\ntation aims to identify parts of the input that im-\npact the label when intervened upon, thus avoid-\ning learning spurious features (Goyal et al., 2019;\nKaushik et al., 2020). Finally, data recombination\nhas been used to achieve compositional generaliza-\ntion (Jia and Liang, 2016; Andreas, 2020). How-\never, data augmentation techniques largely rely on\nprior knowledge of the spurious correlations or hu-\nman efforts. In addition, as shown in Section 4.2\nand a concurrent work (Jha et al., 2020), it is of-\nten unclear how much augmented data is needed\nfor learning a pattern. Our work shows promise in\nadding generic pre-training data or related auxiliary\ndata (through MTL) without assumptions on the\ntarget distribution.\nRobust learning algorithms. Serveral recent\nwork proposes new learning algorithms that are\nrobust to spurious correlations in NLI datasets (He\net al., 2019; Clark et al., 2019; Yaghoobzadeh et al.,\n2019; Zhou and Bansal, 2020; Sagawa et al., 2020;\nMahabadi et al., 2020; Utama et al., 2020). They\nrely on prior knowledge to focus on “harder” ex-\namples that do not enable shortcuts during training.\nOne weakness of these methods is their arguably\nstrong assumption on knowing the spurious corre-\nlations a priori. Our work provides evidence that\nlarge amounts of generic data can be used to im-\nprove out-of-distribution generalization. Similarly,\nrecent work has shown that semi-supervised learn-\ning with generic auxiliary data improves model\nrobustness to adversarial examples (Schmidt et al.,\n2018; Carmon et al., 2019).\nTransfer learning. Robust learning is also re-\nlated to domain adaptation or transfer learning\nsince both aim to learn from one distribution and\nachieve good performance on a different but related\ntarget distribution. Data selection and reweighting\nare common techniques used in domain adapta-\ntion. Similar to our ﬁndings on minority examples,\nsource examples similar to the target data have\nbeen found to be helpful to transfer (Ruder and\nPlank, 2017; Liu et al., 2019a). In addition, many\nworks have shown that MTL improves model per-\nformance on out-of-domain datasets (Ruder, 2017;\nLi et al., 2019; Liu et al., 2019c). A concurrent\nwork (Akula et al., 2020) shows that MTL im-\nproves robustness on advesarial examples in visual\ngrounding. In this work, we further connect the ef-\nfectiveness of MTL to generalization from minority\nexamples.\n7 Conclusion and Discussion\nOur study is motivated by recent observations on\nthe robustness of large-scale pre-trained transform-\ners. Speciﬁcally, we focus on robust accuracy on\nthe challenging datasets which are designed to ex-\npose spurious correlations learned by the model.\nOur analysis reveals that pre-training improves ro-\nbustness by better generalizing from a minority of\nexamples that counter dominant spurious patterns\nin the training set. In addition, we show that more\npre-training data, larger model size, and additional\nauxiliary data through MTL further improve ro-\nbustness, especially when the amount of minority\nexamples is scarce.\nOur work suggests that it is possible to go be-\nyond the robustness-accuracy trade-off with more\ndata. However, the amount of improvement is still\nlimited by the coverage of the training data be-\ncause current models do not extrapolate to unseen\npatterns. Thus an important future direction is to\nincrease data diversity through new crowdsourcing\nprotocols or efﬁcient human-in-the-loop augmenta-\ntion.\nWhile our work provides new perspectives on\npre-training and robustness, it only scratches the\nsurface of the effectiveness of pre-trained models\nand leaves many questions open. For example, why\npre-trained models do not overﬁt to the minority\nexamples; how different initialization (from dif-\nferent pre-trained models) inﬂuences optimization\nand generalization. Understanding these questions\nare key to designing better pre-training methods for\nrobust models.\nFinally, the difference between results on HANS\nand PAWS calls for more careful thinking on the\nformulation and evaluation of out-of-distribution\ngeneralization. Semi-manually constructed chal-\nlenging data often covers only a speciﬁc type of dis-\ntribution shift, thus the results may not generalize\nto other types. A more comprehensive evaluation\nwill drive the development of principled methods\nfor out-of-distribution generalization.\nAcknowledgments\nWe would like to thank the Lex and Comprehend\ngroups at Amazon Web Services AI for helpful\ndiscussions, and the reviewers for their insightful\ncomments. We would also like to thank the Glu-\nonNLP team for the infrastructure support.\nReferences\nArjun R Akula, Spandana Gella, Yaser Al-Onaizan,\nSong-Chun Zhu, and Siva Reddy. 2020. Words\naren’t enough, their order matters: On the robust-\nness of grounding visual referring expressions.\nIn Association for Computational Linguistics\n(ACL).\nJ. Andreas. 2020. Good-enough compositional\ndata augmentation. In Association for Computa-\ntional Linguistics (ACL).\nM. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-\nPaz. 2019. Invariant risk minimization. arXiv\npreprint arXiv:1907.02893v2.\nJ. Baxter. 2000. A model of inductive bias learn-\ning. Journal of Artiﬁcial Intelligence Research\n(JAIR), 12:149–198.\nS. Bowman, G. Angeli, C. Potts, and C. D. Man-\nning. 2015. A large annotated corpus for learn-\ning natural language inference. In Empiri-\ncal Methods in Natural Language Processing\n(EMNLP).\nY . Carmon, A. Raghunathan, L. Schmidt, P. Liang,\nand J. C. Duchi. 2019. Unlabeled data improves\nadversarial robustness. In Advances in Neural\nInformation Processing Systems (NeurIPS).\nRich Caruana. 1997. Multitask learning. Machine\nlearning, 28(1):41–75.\nD. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and\nL. Specia. 2017. SemEval-2017 task 1: Seman-\ntic textual similarity - multilingual and cross-\nlingual focused evaluation. In Proceedings of\nthe Eleventh International Workshop on Seman-\ntic Evaluations.\nC. Clark, M. Yatskar, and L. Zettlemoyer. 2019.\nDon’t take the easy way out: Ensemble based\nmethods for avoiding known dataset biases. In\nEmpirical Methods in Natural Language Pro-\ncessing (EMNLP).\nIshita Dasgupta, Demi Guo, Andreas Stuhlmüller,\nSamuel Gershman, and Noah Goodman. 2018.\nEvaluating compositionality in sentence embed-\ndings. In Annual Meeting of the Cognitive Sci-\nence Society, CogSci 2018.\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova.\n2019. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In\nNorth American Association for Computational\nLinguistics (NAACL).\nW. B. Dolan and C. Brockett. 2005. Automatically\nconstructing a corpus of sentential paraphrases.\nIn Proceedings of the International Workshop on\nParaphrasing.\nM. Glockner, V . Shwartz, and Y . Goldberg. 2018.\nBreaking NLI systems with sentences that re-\nquire simple lexical inferences. In Association\nfor Computational Linguistics (ACL).\nY . Goyal, Z. Wu, J. Ernst, D. Batra, D. Parikh, and\nS. Lee. 2019. Counterfactual visual explana-\ntions. In International Conference on Machine\nLearning (ICML).\nC. Guo, G. Pleiss, Y . Sun, and K. Q. Weinberger.\n2017. On calibration of modern neural networks.\nIn International Conference on Machine Learn-\ning (ICML).\nJ. Guo, H. He, T. He, L. Lausen, M. Li, H. Lin,\nX. Shi, C. Wang, J. Xie, S. Zha, A. Zhang,\nH. Zhang, Z. Zhang, Z. Zhang, S. Zheng, and\nY . Zhu. 2020. Gluoncv and gluonnlp: Deep\nlearning in computer vision and natural language\nprocessing. Journal of Machine Learning Re-\nsearch (JMLR), 21.\nS. Gururangan, S. Swayamdipta, O. Levy,\nR. Schwartz, S. R. Bowman, and N. A. Smith.\n2018. Annotation artifacts in natural language\ninference data. In North American Association\nfor Computational Linguistics (NAACL).\nK. Hashimoto, C. Xiong, Y . Tsuruoka, and\nR. Socher. 2017. A joint many-task model:\nGrowing a neural network for multiple NLP\ntasks. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nH. He, S. Zha, and H. Wang. 2019. Unlearn dataset\nbias for natural language inference by ﬁtting the\nresidual. In Proceedings of the EMNLP Work-\nshop on Deep Learning for Low-Resource NLP.\nD. Hendrycks, K. Lee, and M. Mazeika. 2019. Us-\ning pre-training can improve model robustness\nand uncertainty. In International Conference on\nMachine Learning (ICML).\nD. Hendrycks, X. Liu, E. Wallace, A. Dziedzic,\nR. Krishnan, and D. Song. 2020. Pretrained\ntransformers improve out-of-distribution robust-\nness. In Association for Computational Linguis-\ntics (ACL).\nS. Iyer, N. Dandekar, and K. Csernai.\n2017. First quora dataset release:\nQuestion pairs. Accessed online at\nhttps://www.quora.com/q/quoradata/First-\nQuora-Dataset-Release-Question-Pairs.\nR. Jha, C. Lovering, and E. Pavlick. 2020. When\ndoes data augmentation help generalization in\nNLP? In Association for Computational Linguis-\ntics (ACL).\nR. Jia and P. Liang. 2016. Data recombination\nfor neural semantic parsing. In Association for\nComputational Linguistics (ACL).\nD. Kaushik, E. Hovy, and Z. C. Lipton. 2020.\nLearning the difference that makes a difference\nwith counterfactually-augmented data. In Inter-\nnational Conference on Learning Representa-\ntions (ICLR).\nHongyu Li, Xiyuan Zhang, Yibing Liu, Yiming\nZhang, Quan Wang, Xiangyang Zhou, Jing Liu,\nHua Wu, and Haifeng Wang. 2019. D-net: A\npre-training and ﬁne-tuning framework for im-\nproving the generalization of machine reading\ncomprehension. In Proceedings of the 2nd Work-\nshop on Machine Reading for Question Answer-\ning, pages 212–219.\nMiaofeng Liu, Yan Song, Hongbin Zou, and Tong\nZhang. 2019a. Reinforced training data selec-\ntion for domain adaptation. In Proceedings of\nthe 57th Annual Meeting of the Association for\nComputational Linguistics, pages 1957–1968.\nN. F. Liu, R. Schwartz, and N. A. Smith. 2019b.\nInoculation by ﬁne-tuning: A method for analyz-\ning challenge datasets. In North American Asso-\nciation for Computational Linguistics (NAACL).\nXiaodong Liu, Pengcheng He, Weizhu Chen, and\nJianfeng Gao. 2019c. Multi-task deep neural net-\nworks for natural language understanding. In As-\nsociation for Computational Linguistics (ACL).\nY . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V . Stoy-\nanov. 2019d. RoBERTa: A robustly optimized\nBERT pretraining approach. arXiv preprint\narXiv:1907.11692.\nR. K. Mahabadi, Y . Belinkov, and J. Henderson.\n2020. End-to-end bias mitigation by modelling\nbiases in corpora. In Association for Computa-\ntional Linguistics (ACL).\nChristopher D. Manning, Mihai Surdeanu, John\nBauer, Jenny Finkel, Steven J. Bethard, and\nDavid McClosky. 2014. The Stanford CoreNLP\nnatural language processing toolkit. In Associa-\ntion for Computational Linguistics (ACL) System\nDemonstrations.\nA. Maurer, M. Pontil, and B. Romera-Paredes.\n2016. The beneﬁt of multitask representation\nlearning. Journal of Machine Learning Research\n(JMLR), 17:1–32.\nR. T. McCoy, E. Pavlick, and T. Linzen. 2019.\nRight for the wrong reasons: Diagnosing syn-\ntactic heuristics in natural language inference.\narXiv preprint arXiv:1902.01007.\nJ. Min, R. T. McCoy, D. Das, E. Pitler, and\nT. Linzen. 2020. Syntactic data augmentation in-\ncreases robustness to inference heuristics. In As-\nsociation for Computational Linguistics (ACL).\nYixin Nie, Yicheng Wang, and Mohit Bansal. 2019.\nAnalyzing compositionality-sensitivity of nli\nmodels. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, volume 33, pages\n6867–6874.\nM. I. Nye, A. Solar-Lezama, J. B. Tenenbaum,\nand B. M. Lake. 2019. Learning composi-\ntional rules via neural program synthesis. In\nAdvances in Neural Information Processing Sys-\ntems (NeurIPS).\nY . Oren, S. Sagawa, T. B. Hashimoto, and P. Liang.\n2019. Distributionally robust language model-\ning. In Empirical Methods in Natural Language\nProcessing (EMNLP).\nC. Raffel, N. Shazeer, A. Roberts, K. Lee,\nS. Narang, M. Matena, Y . Zhou, W. Li, and P. J.\nLiu. 2019. Exploring the limits of transfer learn-\ning with a uniﬁed text-to-text transformer. arXiv\npreprint arXiv:1910.10683.\nS. Ruder. 2017. An overview of multi-task learn-\ning in deep neural networks. arXiv preprint\narXiv:1706.05098.\nSebastian Ruder and Barbara Plank. 2017.\nLearning to select data for transfer learning\nwith bayesian optimization. arXiv preprint\narXiv:1707.05246.\nS. Sagawa, P. W. Koh, T. B. Hashimoto, and\nP. Liang. 2020. Distributionally robust neural\nnetworks for group shifts: On the importance of\nregularization for worst-case generalization. In\nInternational Conference on Learning Represen-\ntations (ICLR).\nL. Schmidt, S. Santurkar, D. Tsipras, K. Talwar,\nand A. Madry. 2018. Adversarially robust gener-\nalization requires more data. In Advances in Neu-\nral Information Processing Systems (NeurIPS),\npages 5014–5026.\nA. Søgaard and Y . Goldberg. 2016. Deep multi-\ntask learning with low level tasks supervised at\nlower layers. In Association for Computational\nLinguistics (ACL).\nPrasetya Ajie Utama, Naﬁse Sadat Moosavi, and\nIryna Gurevych. 2020. Mind the trade-off: De-\nbiasing NLU models without degrading the in-\ndistribution performance. In Proceedings of the\n58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 8717–8729, Online.\nAssociation for Computational Linguistics.\nA. Williams, N. Nangia, and S. R. Bowman. 2017.\nA broad-coverage challenge corpus for sentence\nunderstanding through inference. arXiv preprint\narXiv:1704.05426.\nYadollah Yaghoobzadeh, Remi Tachet des Combes,\nTimothy J. Hazen, and Alessandro Sordoni.\n2019. Robust natural language inference\nmodels with example forgetting. CoRR,\nabs/1911.03861.\nT. Zhang, F. Wu, A. Katiyar, K. Q. Weinberger, and\nY . Artzi. 2020. Revisiting few-sample BERT\nﬁne-tuning. arXiv preprint arXiv:2006.05987.\nY . Zhang, J. Baldridge, and L. He. 2019. PAWS:\nParaphrase adversaries from word scrambling.\nIn North American Association for Computa-\ntional Linguistics (NAACL).\nX. Zhou and M. Bansal. 2020. Towards robustify-\ning NLI models against lexical dataset biases.\nIn Association for Computational Linguistics\n(ACL).",
  "topic": "Spurious relationship",
  "concepts": [
    {
      "name": "Spurious relationship",
      "score": 0.9571675062179565
    },
    {
      "name": "Computer science",
      "score": 0.6877162456512451
    },
    {
      "name": "Inference",
      "score": 0.671270489692688
    },
    {
      "name": "Paraphrase",
      "score": 0.6545519232749939
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6488888263702393
    },
    {
      "name": "Generalization",
      "score": 0.6418256163597107
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5802438259124756
    },
    {
      "name": "Machine learning",
      "score": 0.5786907076835632
    },
    {
      "name": "Counterexample",
      "score": 0.5497258305549622
    },
    {
      "name": "Natural language processing",
      "score": 0.3896583616733551
    },
    {
      "name": "Mathematics",
      "score": 0.17394033074378967
    },
    {
      "name": "Discrete mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I160992636",
      "name": "Toyota Technological Institute at Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 9
}