{
  "title": "PepCNN deep learning tool for predicting peptide binding residues in proteins using sequence, structural, and language model features",
  "url": "https://openalex.org/W4389085025",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4277283202",
      "name": "Abel Chandra",
      "affiliations": [
        "Griffith University"
      ]
    },
    {
      "id": "https://openalex.org/A2131476663",
      "name": "Alok Sharma",
      "affiliations": [
        "RIKEN Center for Integrative Medical Sciences",
        "Griffith University",
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2609567017",
      "name": "Iman Dehzangi",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2099238532",
      "name": "Tatsuhiko Tsunoda",
      "affiliations": [
        "The University of Tokyo",
        "RIKEN Center for Integrative Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1945286381",
      "name": "Abdul Sattar",
      "affiliations": [
        "Griffith University"
      ]
    },
    {
      "id": "https://openalex.org/A4277283202",
      "name": "Abel Chandra",
      "affiliations": [
        "Griffith University"
      ]
    },
    {
      "id": "https://openalex.org/A2131476663",
      "name": "Alok Sharma",
      "affiliations": [
        "RIKEN Center for Integrative Medical Sciences",
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2609567017",
      "name": "Iman Dehzangi",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2099238532",
      "name": "Tatsuhiko Tsunoda",
      "affiliations": [
        "The University of Tokyo",
        "RIKEN Center for Integrative Medical Sciences",
        "Griffith University"
      ]
    },
    {
      "id": "https://openalex.org/A1945286381",
      "name": "Abdul Sattar",
      "affiliations": [
        "Griffith University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2145080853",
    "https://openalex.org/W2059942519",
    "https://openalex.org/W1768303176",
    "https://openalex.org/W2052033556",
    "https://openalex.org/W2904418950",
    "https://openalex.org/W2050000500",
    "https://openalex.org/W2006192061",
    "https://openalex.org/W2033726949",
    "https://openalex.org/W1983142511",
    "https://openalex.org/W2757108520",
    "https://openalex.org/W2109444605",
    "https://openalex.org/W2264362083",
    "https://openalex.org/W2808043011",
    "https://openalex.org/W3016691329",
    "https://openalex.org/W4281557145",
    "https://openalex.org/W4281294164",
    "https://openalex.org/W2032087577",
    "https://openalex.org/W4312840063",
    "https://openalex.org/W2964962196",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4320040097",
    "https://openalex.org/W3087184316",
    "https://openalex.org/W3046220160",
    "https://openalex.org/W2910096450",
    "https://openalex.org/W2979866130",
    "https://openalex.org/W3161702043",
    "https://openalex.org/W4319293599",
    "https://openalex.org/W2101926813",
    "https://openalex.org/W2583907533",
    "https://openalex.org/W2433743436",
    "https://openalex.org/W3133458480",
    "https://openalex.org/W4317212783",
    "https://openalex.org/W2018661561",
    "https://openalex.org/W2158714788",
    "https://openalex.org/W2112832917",
    "https://openalex.org/W2986368198",
    "https://openalex.org/W2115538424",
    "https://openalex.org/W2145126338",
    "https://openalex.org/W2343614482",
    "https://openalex.org/W4205134630",
    "https://openalex.org/W2054954101",
    "https://openalex.org/W2953997005",
    "https://openalex.org/W3016271240",
    "https://openalex.org/W2724823461",
    "https://openalex.org/W2973756194",
    "https://openalex.org/W2023272942",
    "https://openalex.org/W2087518504",
    "https://openalex.org/W4303645584",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2953008890",
    "https://openalex.org/W2102461176",
    "https://openalex.org/W4308687190",
    "https://openalex.org/W2166701319",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W3209435229",
    "https://openalex.org/W3044778276",
    "https://openalex.org/W2777941847",
    "https://openalex.org/W2890285536",
    "https://openalex.org/W3084524758",
    "https://openalex.org/W2100908214",
    "https://openalex.org/W3168997536",
    "https://openalex.org/W3104705366",
    "https://openalex.org/W1514205146",
    "https://openalex.org/W2311607323"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports\nPepCNN deep learning tool \nfor predicting peptide binding \nresidues in proteins using \nsequence, structural, and language \nmodel features\nAbel Chandra 1*, Alok Sharma 1,2,3*, Iman Dehzangi 4,5, Tatsuhiko Tsunoda 2,3,6 & \nAbdul Sattar 1\nProtein–peptide interactions play a crucial role in various cellular processes and are implicated in \nabnormal cellular behaviors leading to diseases such as cancer. Therefore, understanding these \ninteractions is vital for both functional genomics and drug discovery efforts. Despite a significant \nincrease in the availability of protein–peptide complexes, experimental methods for studying \nthese interactions remain laborious, time-consuming, and expensive. Computational methods \noffer a complementary approach but often fall short in terms of prediction accuracy. To address \nthese challenges, we introduce PepCNN, a deep learning-based prediction model that incorporates \nstructural and sequence-based information from primary protein sequences. By utilizing a \ncombination of half-sphere exposure, position specific scoring matrices from multiple-sequence \nalignment tool, and embedding from a pre-trained protein language model, PepCNN outperforms \nstate-of-the-art methods in terms of specificity, precision, and AUC. The PepCNN software and \ndatasets are publicly available at https:// github. com/ abela vit/ PepCNN. git.\nProtein–peptide interactions are pivotal for a myriad of cellular functions including metabolism, gene expres -\nsion, and DNA  replication1,2. These interactions are essential to cellular health but can also be implicated in \npathological conditions like viral infections and  cancer3. Understanding these interactions at a molecular level \nholds the potential for breakthroughs in therapeutic interventions and diagnostic methods. Remarkably, small \npeptides mediate approximately 40% of these crucial  interactions4.\nTraditional experimental approaches to study protein–peptide interactions, despite advances in structural \nbiology, have significant  limitations5. They are often costly, time-consuming, and technically challenging due to \nfactors such as small peptide  sizes6, weak binding  affinities7, and peptide  flexibility8. On the other hand, com -\nputational methods offer a complementary approach but are also encumbered by issues related to prediction \naccuracy and computational efficiency. This is often due to the limitations of current algorithms for the inherently \ncomplex nature of protein–peptide interactions.\nComputational methods aimed at predicting protein–peptide interactions primarily belong to two distinct \ncategories: structure-based and sequence-based. In the realm of structure-based models like  PepSite9, SPRINT-\nStr10, and  Peptimap11 leverage an array of structural attributes, such as Accessible Surface Area (ASA), Secondary \nStructure (SS), and Half-Sphere Exposure (HSE), to make their predictions. Conversely, sequence-based methods \nlike SPRINT-Seq12,  PepBind13,  Visual14, PepNN-Seq15, and  PepBCL16, utilize machine learning algorithms and \nvarious features, including amino acid sequences, physicochemical properties, and evolutionary information. \nNotably,  PepBind13 was the first to incorporate intrinsic disorder into feature design, acknowledging its relevance \nOPEN\n1Institute for Integrated and Intelligent Systems, Griffith University, Brisbane, Australia. 2Laboratory for Medical \nScience Mathematics, Department of Biological Sciences, School of Science, The University of Tokyo, Tokyo, \nJapan. 3Laboratory for Medical Science Mathematics, RIKEN Center for Integrative Medical Sciences, Yokohama, \nJapan. 4Department of Computer Science, Rutgers University, Camden, NJ, USA. 5Center for Computational and \nIntegrative Biology, Rutgers University, Camden, USA. 6Laboratory for Medical Science Mathematics, Department \nof Computational Biology and Medical Sciences, Graduate School of Frontier Sciences, The University of Tokyo, \nTokyo, Japan. *email: abel.chandra@griffithuni.edu.au; alok.fj@gmail.com\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\nto protein–peptide  interactions17. Most recently,  SPPPred18 employed both structure-based and sequence-based \nattributes such as HSE, SS, ASA, PSSM, and physicochemical properties to predict protein–peptide binding \nresidues.\nThe rise of deep learning technologies has added another dimension to the computational proteomics land-\nscape. Various algorithms now facilitate the conversion of protein features into image-like formats, making them \ncompatible with deep learning architectures such as Convolutional Neural Network (CNN)19. Transformer-based \nmodels have also emerged as powerful tools for sequence  representation 20, often outperforming traditional \nmodels by capturing long-range interactions within the sequence. For example, Wardah et al. 14 introduced a \nCNN-based method called Visual, which encodes protein sequences as image-like representations to predict \npeptide-binding residues in proteins. Abdin et al.15 unveiled PepNN-Seq, a method leveraging the capabilities of \na pre-trained contextualized language model named  ProtBert20 for protein sequence embedding. Most recently, \nWang et al.16 used  ProtBert20 in a contrastive learning framework for predicting protein–peptide binding residues.\nDeep learning algorithms, a specialized subset of machine learning, have shown considerable promise in \naddressing complex challenges in protein science and structural  biology22,23. These algorithms, inspired by human \ncognitive processes, employ artificial neural networks to learn complex data  representations24,25. Compared to \nthe traditional machine learning framework like Random Forest (RF) and Support Vector Machines (SVM), \ndeep learning models excel in autonomously discovering patterns and features from  data26. Initially popular -\nized in fields like medical imaging, speech recognition, computer vision, and natural language processing, these \nalgorithms have marked milestones such as predicting folding of proteins with remarkable accuracy, making \nthem particularly effective when applied to large and complex  data27. Given the data-intensive nature of mod -\nern biotechnological research, proteomics is increasingly becoming a fertile ground for the application of deep \nlearning  technologies28–30.\nCNNs31 have demonstrated exceptional prowess in image classification tasks, thereby suggesting their appli-\ncability to other forms of spatial data, including protein  structures32,33. Their ability to preserve spatial hierar -\nchies within the data makes them uniquely suited for applications in proteomics. Concurrently, advancements \nin natural language processing have facilitated the development of pre-trained contextualized language models \nspecifically designed for protein biology, further enriching computational tools available for the  field34,35.\nMotivated by these technological leaps, we designed PepCNN, an innovative model that synergistically inte-\ngrates protein sequence embeddings from protein language model (pLM) with CNN. Our method represents \na groundbreaking, consensus-based approach by amalgamating sequence-based features derived from ProtT5-\nXL-UniRef50, transformer language model by Elnaggar et al.20 (herein called ProtT5) with traditional sequence-\nbased (Position Specific Scoring Matrices (PSSMs)) and structure-based attributes to train a one-dimensional \n(1D) CNN, as shown in Fig. 1. Rigorous evaluations underscore that PepCNN sets a new benchmark, outclassing \nexisting methods such as the recent sequence-based PepBCL, PepNN-Seq that utilizes a pre-trained language \nmodel, PepBind with intrinsic disorder features, SPRINT-Str with its emphasis on structural features like ASA, \nSS, and HSE, and most lately the SPPPred method that incoporates both structural and sequence-based features. \nThe marked superiority of PepCNN over these methodologies, in both input requirements and predictive per -\nformance, promises not only to redefine computational methods but also to accelerate drug discovery, enhance \nour understanding of disease mechanisms, and pioneer new computational approaches in bioinformatics.\nResults\nExperimental setup\nWe used two widely used benchmark datasets in this study to fairly assess and compare our proposed method \nwith the existing approaches. These datasets are commonly used by recent state-of-the-art methods for model \ntraining and test in order to carry out evaluation and  comparisons16. We also followed the same process for \na fair comparison. The two datasets were initially obtained from the BioLiP  database36 and sequences with a \nredundancy of > 30% sequence identity were removed using ‘blastclust’ in the BLAST  package37. We addressed \nthe issue of class imbalance in the training set of our datasets by employing random under-sampling 38,39. This \nensures that our model is not biased towards any particular class and can generalize well during evaluation. A \nresidue in a protein sequence is said to be binding if any of its heavy atom is within 3.5 Å (angstrom) from a \nheavy atom in the  peptide12 found during lab experimentation. The resulting 1279 peptide-binding proteins \ncontain 290,943 non-binding residues (experimental label = 0) and 16,749 binding residues (experimental label \n= 1). We designated the two datasets as Datasets 1 and 2, respectively, to make the discussions easier. Table  1 \ndisplays the datasets’ executive summary. The following subsections describe the specifics of the datasets for \nmodel training and evaluation.\nDataset 1\nIn Dataset 1, the test set (TE125) was proposed by Taherzadeh et al. 10 in their structure-based approach called \nSPRINT-Str. To create this set, they firstly selected proteins which were thirty amino acids or more in length \nand contained three or more binding residues. TE125 was then constructed by randomly selecting 10% of the \nproteins and the remaining were assigned to the training set. There are 29,154 non-binding residues and 1716 \nbinding residues in the 125 proteins that make up the TE125 set. In this work, we followed a similar procedure \nas Taherzadeh et al. 10 to construct our training set, i.e. selecting proteins if they had more than thirty amino \nacids and contained three or more binding residues. As a result, 1,115 proteins were obtained for training which \nconstituted of 251,770 non-binding residues and 14,942 binding residues. These numbers clearly show that there \nis an imbalance ratio of around 1:17 between the binding and non-binding residues. This can bias any model \ntowards the classification of non-binding residues over the classification of binding residues if trained directly \non this training set. Therefore, random under-sampling technique was applied to the train set which resulted in \n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\nFigure 1.  Flow diagram of the proposed work for the prediction of binding and non-binding residues. (A) \nFeature extraction component is where the features for each proteins are generated. (B) Residue extraction \ncomponent is where the feature set pertaining to each residue is extracted. (C) The model training block \ncontains the CNN model training step using 80% of the training set to train the network, and the remaining \n20% for validation. (D) The model evaluation component is where the residues in the test set are predicted to be \nbinding or non-binding using the trained CNN model. Figure created using Inkscape  software21.\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\na total of 37,355 residues. From this training set, 80% of the residues were actually used for training the model, \nand the remaining 20% of the residues were used as the validation set during the training stage.\nDataset 2\nIn Dataset 2, the test set (TE639) was proposed by Zhao et al.13 in their sequence-based approach called PepBind. \nThey constructed their train and test sets by randomly dividing the 1279 proteins into two equal subsets. There \nwere 141,840 non-binding residues and 8490 binding residues in the 639 proteins that make up the TE639 set. \nIn the training set, there were 640 proteins, but to save training time, 20% of the proteins were selected to train \ntheir model. The training set in this work was however created by keeping all of the 640 proteins and this resulted \nin 149,103 non-binding residues and 8259 binding residues. It is evident that this training set is also highly \nimbalanced, with an imbalance ratio of 1:18 between the binding and non-binding residues. After the random \nunder-sampling technique, the final number of residues in the training set was therefore 20,647. This final set \nthen underwent a split with 80:20 ratio for the final training and validation set during the model training stage.\nComparison with existing methods\nTo show the performance of our PepCNN model, we compared the results with nine existing methods. These are: \n Pepsite9,  Peptimap11, SPRINT-Seq12, SPRINT-Str10,  PepBind13,  Visual14, PepNN-Seq15,  PepBCL16, and  SPPPred18. \nWe employed sensitivity, specificity, precision, mathews correlation coefficient (MCC), and area under the \nreceiver operating characteristic (ROC) curve (popularly known as AUC) as our evaluation metrics. Sensitivity \nmeasures the true positive rate, specificity indicates the true negative rate, precision signifies the positive pre-\ndictive value, MCC measures the contrast between the predicted labels and the experimental labels, and AUC \nrepresents the model’s overall classification ability. Note that all the metrics, except AUC, rely on the probability \nthreshold where varying the threshold would also alter the metric values. AUC metric therefore provides more \nconfidence for evaluating a model’s performance.\nThe results on TE125 and TE639 test sets are shown in Tables  2 and 3, respectively. In the result tables, a \nthreshold value of 0.877 is used in Table  2 and a value of 0.885 is used in Table  3. Since the test sets were also \nemployed by the previous methods, their results in the tables below are taken directly from their work. As seen \nfrom the results on TE125 and TE639 test sets, PepCNN (our proposed method) achieves higher performance \ncompared to all of the previous methods.\nFor TE125 (Table 2), PepCNN achieves 0.254 sensitivity, 0.988 specificity, 0.55 precision, 0.350 MCC, and \n0.843 AUC. In comparison to all the previous methods, including the PepBCL method (the best performing \nmethod so far), specificity, precision, and AUC have been improved by our method. The biggest improvement \nwas seen on the AUC metric (3.4%), which is a valuable measure for the overall discriminatory capacity of the \n classifiers40,41.\nThe results on TE639 test set is shown in Table 3 where the sensitivity, specificity, precision, MCC, and AUC \nvalues obtained by our method are 0.217, 0.986, 0.479, 0.297, and 0.826, respectively. Similar results as TE125 \nTable 1.  Breakdowns of Dataset 1 and Dataset 2.\nDataset 1 Dataset 2\nTE125 TR1115 TE639 TR640\n(test set) (train set) (test set) (train set)\nNo. of proteins 125 1115 639 640\nNo. of residues 30,870 266,712 150,330 157,362\nNo. of non-binding residues 29,154 251,770 141,840 149,103\nNo. of binding residues 1716 14,942 8490 8259\nTable 2.  Performances of the proposed PepCNN model and the previous methods on the TE125 test set. The \nhighest values in each column are highlighted in bold.\nMethods Sensitivity Specificity Precision MCC AUC \nPepsite9 0.180 0.970 – 0.200 0.610\nPeptimap11 0.320 0.950 – 0.270 0.630\nSPRINT-Seq12 0.210 0.960 – 0.200 0.680\nSPRINT-Str10 0.240 0.980 – 0.290 0.780\nPepBind13 0.344 – 0.469 0.372 0.793\nVisual14 0.670 0.680 – 0.170 0.730\nPepNN-Seq15 – – – 0.278 0.805\nPepBCL16 0.315 0.984 0.540 0.385 0.815\nSPPPred18 0.315 0.959 – 0.230 0.710\nPepCNN (ours) 0.254 0.988 0.55 0.350 0.843\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\nare observed on the TE639 test set, whereby, the specificity, precision, and AUC have been increased compared \nto the previous methods. Again, the biggest improvement was achieved on the AUC metric (by 2.7%) compared \nto the previous best performing method, PepBCL. Even though our method did not perform the best on all the \nmetrics in the two test sets, it surpassed the other methods on majority of the metrics, including AUC. These \nimprovements portray the importance of feature sets from pLM, PSI-BLAST (a multiple-sequence alignment \n(MSA) tool), and structural information pertaining to half-sphere exposure and the use of this feature set with \nCNN to learn robust features for the prediction of binding and non-binding residues in protein sequences.\nCase study\nTo elaborate on the output prediction of our proposed method, we randomly selected three protein sequences \nfrom the TE125 test set after they had been predicted by our model. These proteins were pdbID: 1dpuA, pdbID: \n2bugA, and pdbID: 1uj0A and are visualized as 3D structures in Fig.  2A–F , respectively. The magenta color in \nthe figure shows the binding residues and the gray color shows the non-binding residues. The top visualization \nin the figure illustrates the experimental output (the true binding residues) of the proteins, while the bottom \nvisualization shows the binding residues of the proteins predicted by our model. The protein structures B, D, \nand F of Fig.  2 show that the predicted binding residues by our PepCNN model closely resembles the actual \nbinding residues in the corresponding proteins detected by the lab experiment (structures A, C, and E of Fig. 2).\nTo further quantify the prediction of the amino acids in these three proteins in relation to the actual binding \nsites, we unrolled the sequences into a one-dimensional representation (see Fig.  3). The amino acids in the top \nand bottom sequences show the experimental labels and the predicted labels by our proposed method, respec -\ntively. The experimental and predicted labels are further distinguished by the use of blue and red colors, respec-\ntively. From the figure, it can be seen that in terms of the binding sites, our model correctly predicted 6 out of \nthe 11 sites in 1dpuA, 6 out of the 9 sites in 2bugA, and 8 out of the 9 sites in 1uj0A, which results in a sensitivity \nvalue of 0.545, 0.667, and 0.889, respectively, for the proteins. Furthermore, in terms of the non-binding sites, \nour model correctly predicted 56 out of the 58 sites in 1dpuA, 119 out of the 122 sites in 2bugA, and 42 out of \nTable 3.  Performances of the proposed PepCNN model and the previous methods on the TE639 test set. The \nhighest values in each column are highlighted in bold..\nMethods Sensitivity Specificity Precision MCC AUC \nPepBind13 0.317 – 0.450 0.348 0.767\nPepNN-Seq15 – – – 0.251 0.792\nPepBCL16 0.252 0.983 0.470 0.312 0.804\nPepCNN (ours) 0.217 0.986 0.479 0.297 0.826\nFigure 2.  3D structure visualization of three proteins (pdbID: 1dpuA, pdbID: 2bugA, and pdbID: 1uj0A) \nillustrating the binding (in magenta) and non-binding (in gray) residues using the PyMol  software42. The \nexperimental output (true binding residues) of the proteins are located in the top part (A, C, and E) and its \ncorresponding predicted binding residues by our method PepCNN are located in the bottom part (B, D, and F).\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\nthe 49 sites in 1uj0A, which results in a specificity value of 0.966, 0.975, and 0.857, respectively, for the proteins. \nIt can be seen that even though the sensitivity measure is not high for all the proteins, the ability to attain a high \nnumber of correctly predicted non-binding sites and low number of false positive sites allow the model to predict \nthe binding sites in the same regions as the experimental findings for all the three protein sequences. The close \ndetection of the binding sites in the sequences by our proposed method can therefore greatly assist the efforts of \nthe experimental procedures by narrowing down the regions for further investigations, thereby tremendously \nreducing the time, effort, and cost needed to confirm and understand the protein–peptide binding sites in new \nproteins. The observations from Figs. 2 and 3 indicate a high degree of similarity between predicted and actual \nbinding residues which validates that our algorithm effectively leverages information from primary protein \nsequences for the residue prediction task.\nInsights into the residue features\nBefore embarking on the deep learning algorithm, we had built initial models in this work in which the perfor-\nmance of each of the feature sets and their combinations had been evaluated. In the initial models, we employed \nan ensemble of RF classifiers to have diverse training sets for Dataset 1 for a thorough evaluation. Moreover, it \nallowed for us to have less computational complexity compared to using a deep learning model. The ensemble \nconsisted of 15 individual RF classifiers with different training sets by randomly selecting different non-binding \nresidues during the data balancing stage. The hyper-parameters of the classifiers were tuned using the Hyperopt \n algorithm43 with 5-fold cross-validation scheme. The ensemble’s final predictions on the test set were determined \nby averaging the individual RF classifiers’ probabilities, ensuring a robust and generalized performance.\nFigure 4 shows the ROC curves obtained for the individual feature sets and the different feature set com-\nbinations on TE125. It can be seen that the embedding from the ProtT5 pLM attains a significantly high AUC \nvalue (0.81) in comparison to the PSSM feature set (AUC of 0.642), the HSE feature set (AUC of 0.56), and even \nthe PSSM+HSE feature combination (AUC of 0.697). As the bindings are dependent on the conformations of \n proteins44, this affirms that the embedding from the pre-trained transformer model captures essential information \nconcealed in the primary protein sequences which relates to the structure and function of proteins and therefore \nFigure 3.  Unrolled protein sequences pdbID: 1dpuA (A), pdbID: 2bugA (B), and pdbID: 1uj0A (C) presented \nas a one-dimensional representation. The top sequence of each protein showcases experimentally confirmed \nbinding residues (in blue), while the bottom sequence depicts the predicted binding residues by our proposed \nmethod, PepCNN (in red).\nFigure 4.  ROC curves for the individual feature sets and the different feature set combinations using the \nensemble of RF classifiers on TE125.\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\ncontributes immensely to the binding prediction. The incorporation of PSSM and HSE feature sets to the embed-\nding saw a further increase in the performances, with the most increase coming from the Embedding+HSE \nfeature combination (0.821) and a slight increase in the Embedding + PSSM feature combination (0.812) when \ncompared to just the performance of the embedding. Moreover, the feature combination Embedding + HSE + \nPSSM achieved the overall best AUC value of 0.823. The result obtained by combining the three features sug -\ngests that PSSMs from sequence alignment and the structural properties from half-sphere exposure add valuable \ninformation in terms of the evolutionary properties and protein surface attributes to the protein sequence rep -\nresentations of the transformer model. This final feature combination was then used to build our deep learning \nmodel to further improve the performance.\nDiscussion\nWe have demonstrated that PepCNN can effectively predict binding and non-binding residues in the protein \nsequences. It established the possibility of the pLM embedding, PSSM, and HSE feature combination with CNN \nas feature extractor to predict interaction sites and explore the mechanisms of protein–peptide binding. The \nthree proteins were randomly selected for visualization so that the similarity of the predicted and experimental \nbinding residues could be deciphered. The strong correlation observed suggests that our approach holds promise \nfor identifying prospective binding sites in a broad array of proteins.\nWhen evaluating a predictor, the most ideal model would be the one which has the sensitivity and specific -\nity measures equal to 1, however, this incidence is not prevalent in clinical and computational biology research \nsince the measures increase when either of them  decreases45. The ROC curve, which is an analytical method \nrepresented as a graph, is therefore mainly used for evaluating the performance of a binary classification model \nand to also compare the test result of two or more models. Essentially, the curve plots the coordinate points using \nthe false positive rate (1-specificity) as the x-axis and the true positive rate (sensitivity) as the y-axis. The closer \nthe plot is to the upper left corner of the graph, the higher the model’s performance is since the upper left corner \nhas sensitivity equal to 1 and the false positive rate equal to 0 (specificity is equal to 1). The desired ROC curve \nhence has an AUC (area under the ROC curve) equal to 1.\nThe study of protein–peptide binding is desired since the peptides exhibit low toxicity and posses small inter-\nface areas (as peptides are mostly 5–15 residues  long46), making them good targets for efficacious therapeutic \ndesigns and drug discovery  process47. In addition, peptide-like inhibitors are used for treating diabetes, cancer, \nand autoimmune  diseases48. In the past, search for peptides as therapeutics was discouraged due to their short \nhalf-life and slow  absorption49, however, these short amino acid chains are considered drug candidates once \nagain due to the emergence of synthetic approaches which allow for changes to its biophysical and biochemical \n properties50.\nUnderstanding the structure of protein–peptide complexes is often a prerequisite for the design of peptide-\nbased drugs. The challenges of studying these complexes are unique compared to other interactions such as \nprotein–protein and protein–ligand. In protein–protein interactions, complexes are usually formed based on \nwell-defined 3D structures, and in the protein–ligand interactions, small ligands typically bind in deeply buried \nregions of proteins. Conversely, peptides often lack stable structures and usually bind with weak affinity to large, \nshallow pockets on protein  surfaces51. Given these complexities, and the limitations of current experimental \nmethods like X-ray crystallography and nuclear magnetic resonance, there is a compelling need for robust \ncomputational methods.\nIn summary, our work contributes to addressing these challenges by offering a highly accurate and compu-\ntationally efficient method for predicting protein–peptide interaction sites. Such advances are crucial for both \nfundamental biological research and practical applications in drug design.\nConclusion\nIn this work, we have developed a new deep learning-based protein–peptide binding residue predictor called \nPepCNN. The model leverages sequence-based features, which are extracted from a pre-trained pLM, as well as \nfrom a MSA tool. In addition to these, we incorporated a structure-based feature known as half-sphere exposure. \nUtilizing these diverse properties of protein sequences as input, our convolutional neural network was effective \nin learning essential features. As a result, PepCNN was able to outperform existing methods that also rely on \nprimary protein sequence information, as demonstrated by tests on two distinct datasets.\nLooking ahead, our future research aims to further enhance the model’s performance. One innovative avenue \nfor exploration will involve integrating DeepInsight  technology19. This technology converts feature vectors into \ntheir corresponding image representations, thus enabling the application of 2D CNN architectures. This change \nopens up the possibility of implementing transfer learning techniques to boost the model’s predictive power.\nMethods\nEvaluation metrics\nThe proposed model in this work was evaluated using the residues in the test sets TE125 and TE639 after being \ntrained on their respective training sets. These test sets are highly imbalanced, and for this reason, suitable \nmetrics were chosen to effectively evaluate our model for the classification task. These metrics were Sensitivity, \nSpecificity, Precision, and MCC. The formulation of these metrics are given below.\n(1)Sensitivity= TP\nTP + FN\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\nIn the above formulas, TP stands for True Positives, TN stands for True Negatives, FP stands for False Posi-\ntives, and FN stands for False Negatives. TP is the number of actual binding residues correctly classified by the \nmodel, TN is the number of actual non-binding residues correctly classified by the model, FP is the number of \nactual non-binding residues incorrectly classified by the model, and finally FN is the number of actual binding \nresidues incorrectly classified by the model. For the given model, the Sensitivity metric [given by Eq. ( 1)] and \nthe Specificity metric [given by Eq. (2 )] calculate the fraction of binding residues and non-binding residues \ncorrectly predicted, respectively, the Precision metric [given by Eq. (3 )] calculates the proportion of binding \nresidues correctly classified out of all the residues classified as binding, and the MCC metric [given by Eq. (4 )] \ncalculates the prediction ability for both the binding and non-binding residues. The values range from 0 to 1 \nfor the Sensitivity, Specificity, and Precision metrics and the higher the value, the better the prediction model \nis. The MCC metric takes on values ranging from − 1 to + 1 where + 1 indicates a highly positive correlation, \nwhile − 1 indicates a highly negative correlation. It should be noted that the above metrics are dependent of \nthe probability threshold of the classifier and varying the threshold would also vary the metric values. For this \nreason, these metrics cannot be heavily relied upon for the model evaluation. Therefore, in addition to the above \nmetrics, we have also included the AUC metric which is calculated based on the classification probability values \nand is independent of the threshold setting. The metric therefore gives more confidence in the evaluation of a \nmodel’s performance. AUC is also a very useful metric since it measures the overall performance of the clas-\nsification model by calculating its separability between the predicted binding and non-binding residues. The \nrange of values for the AUC metric is from 0 to 1, with 0 being the worst measure of separability and 1 being a \nvery good measure of separability.\nFeature extraction\nThe features chosen in this study are the representations from a pre-trained pLM, evolutionary relationships \nin the protein sequences using a MSA tool, and the structural attributes in terms of the solvent exposure of the \nresidues in the sequences. In the feature extraction stage of our proposed method (Fig.  1A), the three differ -\nent feature-types were obtained by submitting the 1,279 proteins to the three tools: pre-trained ProtT5  pLM20, \nPSI-BLAST37, and  HSEpred52 to acquire the Embedding, PSSM, and HSE values, respectively. The following \nsubsections elucidates each of these features in detail.\nTransformer embedding\nTransformer models from natural language processing employ latest DL algorithms and such architectures have \nshown huge potential in proteomics field due to its ability to leverage on the growing databases of protein \nsequences. These models offer transfer learning where the knowledge acquired from data-rich tasks can be \ntransferred to similar data-limited tasks. Several pLMs have been developed by Elnaggar et al.20 and out of those \nmodels, ProtT5 is amongst the most widely used pre-trained models in the literature to tackle various  tasks53. It \nis based on the T5  architecture54, which is akin to the originally proposed architecture for language translation \n task55 as depicted in Fig.  5. It consists of the encoder and decoder blocks, where the encoder projects the input \nsequence to an embedding space and the decoder generates the output embedding based on the embedding of \nthe encoder. To do this, firstly the input sequence tokens (  x1 , ..., xn ) are mapped by the encoder to generate \nrepresentation z ( z1 , ..., zn ). The decoder then uses the representation z to produce output sequence ( y1 , ..., yn ), \nelement by element. Both the encoder and decoder have the main components known as the multi-head atten-\ntion and the feed-forward layer. The multi-head attention is a result of combining multiple self-attention modules \n(heads), where the self-attention is an attention mechanism that relates different positions in the input sequence \nto compute its representation. The attention function maps a position’s query vector and a set of key-value vectors \nfor all the positions to an output vector. In order to carry out this operation for all the positions simultaneously, \nthe query, key and value vectors are packed together into matrices Q , K, and V , respectively, and the output \nmatrix is computed as: head = Attention(Q, K, V) = softmax(QK T\n√\ndk)V, where 1√\ndk is the scaling factor. It is much \nbeneficial to have multi-head attention instead of a single self-attention module since it allows for the capturing \nof information from different representations at the different positions. This is done by linearly projecting the \nqueries, keys and values n  times. The multi-head attention is therefore given by: MultiHead(Q , K, V) = \nConcat(head1 , ..., headn)W O , where headi  = Attention(QW Q\ni  , K W K\ni  , V W V\ni  ); W Q\ni  , W K\ni  , W V\ni  and W O are projection \nmatrices. The ProtT5 transformer used in this work is a 3 billion parameter model which was trained on the Big \nFantastic  Database56 and fine-tuned on the  UniRef5057 database. Even though ProtT5 has both encoder and \ndecoder blocks in its architecture, the authors found that the encoder embedding outperformed the decoder \nembedding on all tasks, hence the pre-trained model extracts the embedding from its encoder side. The output \nembedding of the ProtT5 model is a matrix of dimension L × 1024 (where L represents the protein’s length and \n1024 the values of the network’s last hidden layer). This matrix captures relationships between amino acid residues \n(2)Speciﬁcity= TN\nTN + FP\n(3)Precision= TP\nTP + FP\n(4)MCC = TP× TN − FP× FN√(TP+ FN)(TP+ FP)(TN + FP)(TN + FN)\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\nin the input protein sequence based on the attention mechanism and produces a rich set of features that encom-\npasses relevant protein structural and functional information.\nPosition specific scoring matrices\nIn protein engineering, MSAs are a popularly used technique for aligning sequences to determine their evolu -\ntionary relationships and structural/functional constraints within families of proteins to aid diverse prediction \n pipelines58. For instance, it has been a vital component for contact and structure  predictions59,60, as well as other \nprediction tasks such as functional effects of  mutations61 and rational protein  design62. To incorporate the potency \nof the information held in MSA, PSI-BLAST tool was employed in this work to obtained the sequence-profiles. \nIt was run using the E-value threshold of 0.001 in three iterations which resulted in two matrices, log odds and \nlinear probabilities of the amino acids, with dimensions L × 20 (where 20 represents the 20 different amino acids \nof the genetic code). The matrix with linear probabilities was used in this work in which each of the elements \nin the row represent the substitution probabilities of the amino acid with all the 20 amino acids in the genetic \ncode. PSSM can therefore be formulated as P = {P ij : i = 1...L and j = 1...20}, where P ij is the probability for the jth \namino acid in the ith position of the input sequence and has a high value for a highly conserved position, while \na low value indicates a weakly conserved position.\nHalf‑sphere exposure\nThe information about a protein’s surface is valuable for the prediction of protein–peptide binding sites as the \npeptides often bind to the shallow surface  regions51. HSE is an effective property that measures the solvent expo-\nsure for distinguishing buried, partially buried and exposed  residues63. It has been widely used in protein–peptide \nand other binding prediction  tasks10,18,64,65. In this work, the HSE values of the proteins were obtained from \nthe HSEpred server, which gives a measure of how buried an amino acid is in the protein’s three-dimensional \nstructure. HSE for a residue is measured by firstly setting a sphere of radius rd = 13 Å at the residue’s C α atom. \nSecondly, this sphere is divided into two halves by constructing a plane perpendicular to a given C α-Cβ vector \nthat goes through the residue’s C α atom resulting in two HSE measures: HSE-up and HSE-down. HSE-up refers \nto the upper sphere in the direction of the side chain and HSE-down refers to the lower sphere which is in the \nopposite direction to the side chain. Finally, the number of C α atoms in the upper and lower half of the sphere \nare measured,  respectively52. Refer to Fig. 6 for the illustration of the HSE-up and HSE-down measures. Contact \nnumber is another important measure and it indicates the total number of C  α atoms in the sphere of the C  α \nFigure 5.  The original encoder-decoder  Transformer55 which was proposed for language translation task. The \nnetwork can have layers of these encoder-decoder modules, denoted by Nx. The input sequence is fed to the \nencoder and the decoder produces a new output sequence. At each timestep, an output is predicted, which is \nthen fed back to the network (decoder), including all the previous outputs, to predict the output for the next \ntimestep and so on until the output sequence (translation) is produced.\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\natom of a  residue66. The output of HSEpred is therefore a feature matrix of dimension L × 3 where 3 represents \nthe values of HSE-up, HSE-down, and the contact number for each residue.\nConvolutional neural network\nFrom the deep learning area, CNN is one of the most widely used network in the recent  times67. It is a type of \nfeed-forward neural network that uses convolutional structures to extract features from data. A CNN has three \nmain components: convolutional layer, pooling layer, and fully connected layer. The convolutional layer consists \nof several convolution filters. It produces what are known as feature maps by convolving the input with a filter \nand then applying nonlinear activation function to each of the resulting elements. The border information can \nbe lost during the convolution process, so to mitigate this, padding is introduced to increase the input with a zero \nvalue, which can indirectly change its size. Additionally, the stride is used to control the convolving density. The \ndensity is lower for longer strides. The pooling layer down-samples an image, which reduces the amount of data \nand at the same time preserves useful information. Moreover, by eliminating superfluous features, it can also \nlower the number of model parameters. One or more fully connected layers are added after several convolutional \nand pooling layers. In the fully connected layers, all the previous layer neurons are connected to every neurons \nin the current layer and this results in the generation of global semantic information. The network can more \naccurately approximate the target function by increasing its depth, however, this also makes the network more \ncomplex, which makes it harder to optimize and are more likely to overfit.\nFigure 6.  Depiction of the HSE-up and HSE-down measures. The dotted lines indicate the plane’s position \nwhich divides the sphere of the residue’s C α atom (in orange) with radius rd into two equal half spheres. The \nother C α atoms (in green) represents parts of other residues in the protein sequence.\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\nCNN has made some outstanding advancements in a variety of fields, including, but not limited to, computer \nvision and natural language processing, which has garnered significant interest from researchers in various fields. \nA CNN can also be applied to 1D and multidimensional input data in addition to the processing of 2D images. \nIn order to process 1D data, CNN typically uses 1D convolutional filters (as portrayed in Fig. 7).\nBuilding the deep learning model\nIn order to build a classifier that carries out per residue binding/non-binding prediction, it is important to extract \ninformation pertaining to each residue. In the residue extraction stage of our proposed method (Fig.  1B), we \nrepresented each residue with its sequence based (pre-trained pLM embedding and PSSM) and structure (HSE) \nbased information. This was done by extracting the values corresponding to each residue from the three feature \nmatrices obtained when the proteins were submitted to the three feature extraction tools. Tensor sum was applied \nto the resulting vectors, i.e. 1 × 1024 Embedding vector, 1 × 20 PSSM vector, and 1 × 3 HSE vector, which formed \na feature vector of dimension 1 × 1,047 to represent each residue. These residues were kept in their respective \nsets (i.e. train and test) to effectively train and evaluate the model without bias.\nIn the model training stage (Fig. 1C), we trained a 1D CNN to build our predictor based on the Tensorflow \n framework68. The model has 8.7 million trainable parameters which were trained using 80% of the training set, \nand the remaining 20% were used for network validation. The model is composed of three 1D convolutional \nlayers and two fully connected (dense) layers. For the convolutional layers, the first layer contains 128 filters of \nsize 5, the second layer contains 128 filters of size 3, and the third layer contains 64 filters of size 3. The stride for \neach layer was kept as 1 and the padding was used such that the output size of each layer was equal to the input \nsize to the layer. Dropouts were used after each convolutional layer. In the fully connected layers, the first layer \nand the second layer contains 128 and 32 neurons, respectively. Finally, the output was made of a single neuron \nfor binary classification. The ReLU activation function was used in each of the layers, while a sigmoid activation \nfunction was used in the output neuron. The model was trained using Adam optimizer with a learning rate of 1 × \n 10−6, loss using binary crossentropy, and metric as AUC. Moreover, early stopping was employed with a patience \nof 3. The network was optimized using the Bayesian Optimization algorithm in the Keras Tuner  library69. The \nplots of the training progress of the model for the training sets TR1115 and TR640 are shown in Fig. 8.\nFigure 7.  A sample 1D CNN depiction which shows the flow of information from the input to the output \nthrough its three main layers: convolutional, pooling, and fully connected.\n12\nVol:.(1234567890)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\nData availability\nThe datasets used in this paper can be downloaded from the GitHub link https:// github. com/ abela vit/ PepCNN. \ngit.\nCode availability\nThe PepCNN codes (in Python) are available at the GitHub link https:// github. com/ abela vit/ PepCNN. git.\nReceived: 7 September 2023; Accepted: 16 November 2023\nReferences\n 1. Pawson, T. & Nash, P . Assembly of cell regulatory systems through protein interaction domains. Science300, 445–452 (2003).\n 2. Rubinstein, M. & Niv, M. Y . Peptidic modulators of protein–protein interactions: Progress and challenges in computational design. \nBiopolym. Origi. Res. Biomol. 91, 505–513 (2009).\n 3. Lee, H., Heo, L., Lee, M. S. & Seok, C. Galaxypepdock: A protein–peptide docking tool based on interaction similarity and energy \noptimization. Nucl. Acids Res. 43, W431–W435 (2015).\n 4. Neduva, V . et al. Systematic discovery of new recognition peptides mediating protein interaction networks. PLoS Biol. 3, e405 \n(2005).\n 5. Chandra, A. et al. Phoglystruct: Prediction of phosphoglycerylated lysine residues using structural properties of amino acids. Sci. \nRep. 8, 17923 (2018).\n 6. Vlieghe, P ., Lisowski, V ., Martinez, J. & Khrestchatisky, M. Synthetic therapeutic peptides: Science and market. Drug Discov. Today \n15, 40–56 (2010).\n 7. Dyson, H. J. & Wright, P . E. Intrinsically unstructured proteins and their functions. Nat. Rev. Mol. Cell Biol. 6, 197–208 (2005).\n 8. Bertolazzi, P ., Guerra, C. & Liuzzi, G. Predicting protein-ligand and protein–peptide interfaces. Eur. Phys. J. Plus 129, 1–10 (2014).\n 9. Petsalaki, E., Stark, A., García-Urdiales, E. & Russell, R. B. Accurate prediction of peptide binding sites on protein surfaces. PLoS \nComput. Biol. 5, e1000335 (2009).\n 10. Taherzadeh, G., Zhou, Y ., Liew, A.W .-C. & Y ang, Y . Structure-based prediction of protein–peptide binding regions using random \nforest. Bioinformatics 34, 477–484 (2018).\n 11. Lavi, A. et al. Detection of peptide-binding sites on protein surfaces: The first step toward the modeling and targeting of peptide-\nmediated interactions. Proteins Struct. Funct. Bioinform. 81, 2096–2105 (2013).\nFigure 8.  Plots of AUC and loss for the training progress of the proposed model on the two training sets: (A) \nTR1115 train set (achieving an AUC of 0.8521 on the validation set), and (B) TR640 train set (achieving an \nAUC of 0.8301 on the validation set).\n13\nVol.:(0123456789)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\n 12. Taherzadeh, G., Y ang, Y ., Zhang, T., Liew, A.W .-C. & Zhou, Y . Sequence-based prediction of protein–peptide binding sites using \nsupport vector machine. J. Comput. Chem. 37, 1223–1229 (2016).\n 13. Zhao, Z., Peng, Z. & Y ang, J. Improving sequence-based prediction of protein–peptide binding residues by introducing intrinsic \ndisorder and a consensus method. J. Chem. Inf. Model. 58, 1459–1468 (2018).\n 14. Wardah, W . et al. Predicting protein–peptide binding sites with a deep convolutional neural network. J. Theor. Biol. 496, 110278 \n(2020).\n 15. Abdin, O., Nim, S., Wen, H. & Kim, P . M. Pepnn: A deep attention model for the identification of peptide binding sites. Commun. \nbiology 5, 503 (2022).\n 16. Wang, R., Jin, J., Zou, Q., Nakai, K. & Wei, L. Predicting protein–peptide binding residues via interpretable deep learning. Bioin‑\nformatics 38, 3351–3360 (2022).\n 17. Weatheritt, R. J. & Gibson, T. J. Linear motifs: Lost in (pre) translation. Trends Biochem. Sci. 37, 333–341 (2012).\n 18. Shafiee, S., Fathi, A. & Taherzadeh, G. Spppred: Sequence-based protein-peptide binding residue prediction using genetic program-\nming and ensemble learning. IEEE/ACM Transactions on Comput. Biol. Bioinforma. 20, 2029–2040 (2022).\n 19. Sharma, A., Vans, E., Shigemizu, D., Boroevich, K. A. & Tsunoda, T. Deepinsight: A methodology to transform a non-image data \nto an image for convolution neural network architecture. Sci. Rep. 9, 11399 (2019).\n 20. Elnaggar, A. et al. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE Trans. Pattern \nAnal. Mach. Intell. 44, 7112–7127 (2021).\n 21. Inkscape, version 1.2.2. Software available from http:// inksc ape. org.\n 22. Min, S., Lee, B. & Y oon, S. Deep learning in bioinformatics. Briefings Bioinform. 18, 851–869 (2017).\n 23. Sharma, A., Lysenko, A., Boroevich, K. A. & Tsunoda, T. DeepInsight-3D architecture for anti-cancer drug response prediction \nwith deep-learning on multi-omics. Sci. Rep. 13, 2483 (2023).\n 24. Rojas, R. Neural Networks: A Systematic Introduction (Springer, New Y ork, 2013).\n 25. Wen, B. et al. Deep learning in proteomics. Proteomics 20, 1900335 (2020).\n 26. Wang, P ., Fan, E. & Wang, P . Comparative analysis of image classification algorithms based on traditional machine learning and \ndeep learning. Pattern Recogn. Lett. 141, 61–67 (2021).\n 27. Nguyen, G. et al. Machine learning and deep learning frameworks and libraries for large-scale data mining: A survey. Artif. Intell. \nRev. 52, 77–124 (2019).\n 28. Kandathil, S. M., Greener, J. G. & Jones, D. T. Recent developments in deep learning applied to protein structure prediction. Proteins \nStruct. Funct. Bioinform. 87, 1179–1189 (2019).\n 29. Meyer, J. G. Deep learning neural network tools for proteomics. Cell Rep. Methods 1, 1–10 (2021).\n 30. Neely, B. A. et al. Toward an integrated machine learning model of a proteomics experiment. J. Proteome Res. 22, 681–696 (2023).\n 31. Fukushima, K. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift  \nin position. Biol. Cybern. 36, 193–202 (1980).\n 32. Ragoza, M., Hochuli, J., Idrobo, E., Sunseri, J. & Koes, D. R. Protein–ligand scoring with convolutional neural networks. J. Chem. \nInf. Model. 57, 942–957 (2017).\n 33. Zeng, H., Edwards, M. D., Liu, G. & Gifford, D. K. Convolutional neural network architectures for predicting DNA-protein bind-\ning. Bioinformatics 32, i121–i127 (2016).\n 34. Rao, R. M. et al. Msa transformer. In International Conference on Machine Learning, 8844–8856 (PMLR, 2021).\n 35. Chandra, A., Tünnermann, L., Löfstedt, T. & Gratz, R. Transformer-based deep learning for predicting protein properties in the \nlife sciences. Elife 12, e82819 (2023).\n 36. Y ang, J., Roy, A. & Zhang, Y . Biolip: A semi-manually curated database for biologically relevant ligand–protein interactions. Nucleic \nAcids Res. 41, D1096–D1103 (2012).\n 37. Altschul, S. F . et al. Gapped blast and psi-blast: A new generation of protein database search programs. Nucleic Acids Res. 25, \n3389–3402 (1997).\n 38. Yu, D.-J. et al. Improving protein-ATP binding residues prediction by boosting SVMs with random under-sampling. Neurocomput‑\ning 104, 180–190 (2013).\n 39. Mahmud, S. H. et al. Prediction of drug–target interaction based on protein features using undersampling and feature selection \ntechniques with boosting. Anal. Biochem. 589, 113507 (2020).\n 40. Jiménez-Valverde, A. Insights into the area under the receiver operating characteristic curve (AUC) as a discrimination measure \nin species distribution modelling. Glob. Ecol. Biogeogr. 21, 498–507 (2012).\n 41. Sing, T., Sander, O., Beerenwinkel, N. & Lengauer, T. Rocr: Visualizing classifier performance in r. Bioinformatics 21, 3940–3941 \n(2005).\n 42. Schrödinger, LLC. The PyMOL molecular graphics system, version 2.5.5 (2015). Software available from https:// pymol. org/2/.\n 43. Bergstra, J., Y amins, D. & Cox, D. Making a science of model search: Hyperparameter optimization in hundreds of dimensions \nfor vision architectures. In International Conference on Machine Learning, 115–123 (PMLR, 2013).\n 44. Stank, A., Kokh, D. B., Fuller, J. C. & Wade, R. C. Protein binding pocket dynamics. Acc. Chem. Res. 49, 809–815 (2016).\n 45. Nahm, F . S. Receiver operating characteristic curve: Overview and practical use for clinicians. Korean J. Anesthesiol.  75, 25–36 \n(2022).\n 46. London, N., Movshovitz-Attias, D. & Schueler-Furman, O. The structural basis of peptide–protein binding strategies. Structure  \n18, 188–199 (2010).\n 47. Liu, D. et al. Self-assembly of mitochondria-specific peptide amphiphiles amplifying lung cancer cell death through targeting the \nVDAC1–hexokinase-II complex. J. Mater. Chem. B 7, 4706–4716 (2019).\n 48. Pant, S., Singh, M., Ravichandiran, V ., Murty, U. & Srivastava, H. K. Peptide-like and small-molecule inhibitors against covid-19. \nJ. Biomol. Struct. Dyn. 39, 2904–2913 (2021).\n 49. Lau, J. L. & Dunn, M. K. Therapeutic peptides: Historical perspectives, current development trends, and future directions. Bioorg. \nMed. Chem. 26, 2700–2707 (2018).\n 50. Angelova, A., Drechsler, M., Garamus, V . M. & Angelov, B. Pep-lipid cubosomes and vesicles compartmentalized by micelles from \nself-assembly of multiple neuroprotective building blocks including a large peptide hormone PACAP-DHA. ChemNanoMat 5, \n1381–1389 (2019).\n 51. Petsalaki, E. & Russell, R. B. Peptide-mediated interactions in biological systems: New discoveries and applications. Curr. Opin. \nBiotechnol. 19, 344–350 (2008).\n 52. Song, J., Tan, H., Takemoto, K. & Akutsu, T. Hsepred: Predict half-sphere exposure from protein sequences. Bioinformatics  24, \n1489–1497 (2008).\n 53. Pokharel, S., Pratyush, P ., Heinzinger, M., Newman, R. H. & Kc, D. B. Improving protein succinylation sites prediction using \nembeddings from protein language model. Sci. Rep. 12, 16933 (2022).\n 54. Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 5485–5551 \n(2020).\n 55. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst.30, 5998–6008 (2017).\n 56. Steinegger, M. & Söding, J. Clustering huge protein sequence sets in linear time. Nat. Commun. 9, 2542 (2018).\n 57. Suzek, B. E. et al. Uniref clusters: A comprehensive and scalable alternative for improving sequence similarity searches. Bioinfor‑\nmatics 31, 926–932 (2015).\n14\nVol:.(1234567890)Scientific Reports |        (2023) 13:20882  | https://doi.org/10.1038/s41598-023-47624-5\nwww.nature.com/scientificreports/\n 58. Petti, S. et al. End-to-end learning of multiple sequence alignments with differentiable Smith–Waterman. Bioinformatics 39, btac724 \n(2023).\n 59. Jones, D. T., Buchan, D. W ., Cozzetto, D. & Pontil, M. Psicov: Precise structural contact prediction using sparse inverse covariance \nestimation on large multiple sequence alignments. Bioinformatics 28, 184–190 (2012).\n 60. Jumper, J. et al. Highly accurate protein structure prediction with alphafold. Nature 596, 583–589 (2021).\n 61. Frazer, J. et al. Disease variant prediction with deep generative models of evolutionary data. Nature 599, 91–95 (2021).\n 62. Russ, W . P . et al. An evolution-based model for designing chorismate mutase enzymes. Science 369, 440–445 (2020).\n 63. Pan, Y ., Wang, Z., Zhan, W . & Deng, L. Computational identification of binding energy hot spots in protein-RNA complexes using \nan ensemble approach. Bioinformatics 34, 1473–1480 (2018).\n 64. Wang, H., Liu, C. & Deng, L. Enhanced prediction of hot spots at protein–protein interfaces using extreme gradient boosting. Sci. \nRep. 8, 14285 (2018).\n 65. Pan, Y ., Zhou, S. & Guan, J. Computationally identifying hot spots in protein-DNA binding interfaces using an ensemble approach. \nBMC Bioinform. 21, 1–16 (2020).\n 66. Hamelryck, T. An amino acid has two sides: A new 2d measure provides a different view of solvent exposure. Proteins Struct. Funct. \nBioinform. 59, 38–48 (2005).\n 67. Li, Z., Liu, F ., Y ang, W ., Peng, S. & Zhou, J. A survey of convolutional neural networks: analysis, applications, and prospects. IEEE \nTrans. Neural Netw. Learn. Syst. 33, 6999–7019 (2021).\n 68. Abadi, M., et al. TensorFlow: Large-scale machine learning on heterogeneous systems (2015). Software available from tensorflow.\norg.\n 69. O’Malley, T. et al. Keras Tuner. https:// github. com/ keras- team/ keras- tuner (2019).\nAcknowledgements\nThis research is partially supported by Australian Research Council Grant DP180102727. We are grateful to \nthe Griffith University eResearch Service & Specialised Platforms team for their High Performance Computing \nCluster to complete this research.\nAuthor contributions\nA.C. performed analysis and experiments. A.C and A.Sharma conceived and wrote the first manuscript. \nA.Sharma and I.D curated the data and T.T. and A.Sattar contributed in manuscript write-up. All authors read \nand approved the final manuscript.\nCompeting interest \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to A.C. or A.S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Sequence (biology)",
  "concepts": [
    {
      "name": "Sequence (biology)",
      "score": 0.619750440120697
    },
    {
      "name": "Computational biology",
      "score": 0.5682307481765747
    },
    {
      "name": "Computer science",
      "score": 0.5302314758300781
    },
    {
      "name": "Natural language processing",
      "score": 0.47309592366218567
    },
    {
      "name": "Peptide",
      "score": 0.4620117247104645
    },
    {
      "name": "Peptide sequence",
      "score": 0.4475489556789398
    },
    {
      "name": "Deep learning",
      "score": 0.4275110960006714
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4217839241027832
    },
    {
      "name": "Bioinformatics",
      "score": 0.3902643322944641
    },
    {
      "name": "Biochemistry",
      "score": 0.2795187830924988
    },
    {
      "name": "Chemistry",
      "score": 0.2685658633708954
    },
    {
      "name": "Biology",
      "score": 0.22980719804763794
    },
    {
      "name": "Gene",
      "score": 0.10384717583656311
    }
  ]
}