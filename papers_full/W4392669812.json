{
    "title": "My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks",
    "url": "https://openalex.org/W4392669812",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2790642444",
            "name": "Tanmay Chavan",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4320554404",
            "name": "Omkar Gokhale",
            "affiliations": [
                "Indian Institute of Technology Madras"
            ]
        },
        {
            "id": "https://openalex.org/A4320548338",
            "name": "Aditya Kane",
            "affiliations": [
                "Indian Institute of Technology Madras"
            ]
        },
        {
            "id": "https://openalex.org/A4320548339",
            "name": "Shantanu Patankar",
            "affiliations": [
                "Indian Institute of Technology Madras"
            ]
        },
        {
            "id": "https://openalex.org/A2899880915",
            "name": "Raviraj Joshi",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2130313487",
        "https://openalex.org/W3099919888",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W4224040999",
        "https://openalex.org/W4287262272",
        "https://openalex.org/W4283155872",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4281675963",
        "https://openalex.org/W4224267299",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3182993912",
        "https://openalex.org/W4304701448",
        "https://openalex.org/W3136221257",
        "https://openalex.org/W4221152720",
        "https://openalex.org/W3119228675",
        "https://openalex.org/W4221152721",
        "https://openalex.org/W3153303820",
        "https://openalex.org/W2965373594"
    ],
    "abstract": "The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models.In this work, we focus on the low-resource Indian language Marathi which lacks any prior work in code-mixing.We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 10 million social media sentences for pretraining.We also release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models pre-trained on MeCorpus.Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like codemixed Mr-En hate speech detection, sentiment analysis, and language identification respectively.These evaluation datasets individually consist of manually annotated ~12,000 Marathi-English code-mixed tweets.Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models.This is the first work that presents artifacts for code-mixed Marathi research.",
    "full_text": "Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023, pages 242–249\nNovember 1–4, 2023. ©2023 Asian Federation of Natural Language Processing\n242\nMy Boli: Code-mixed Marathi-English Corpora, Pretrained Language\nModels and Evaluation Benchmarks\nTanmay Chavan1∗ , Omkar Gokhale2∗ , Aditya Kane2∗ , Shantanu Patankar2∗ ,\nand Raviraj Joshi3\nPune Institute of Computer Technology, L3Cube1\nGeorgia Institute of Technology, L3Cube2\nIndian Institute of Technology Madras, L3Cube3\nchavantanmay1402@gmail.com, ogokhale3@gatech.edu, adityakane1@gmail.com,\nspatankar34@gatech.edu, ravirajoshi@gmail.com\nAbstract\nThe research on code-mixed data is limited due\nto the unavailability of dedicated code-mixed\ndatasets and pre-trained language models. In\nthis work, we focus on the low-resource Indian\nlanguage Marathi which lacks any prior work in\ncode-mixing. We present L3Cube-MeCorpus, a\nlarge code-mixed Marathi-English (Mr-En) cor-\npus with 10 million social media sentences for\npretraining. We also release L3Cube-MeBERT\nand MeRoBERTa, code-mixed BERT-based\ntransformer models pre-trained on MeCorpus.\nFurthermore, for benchmarking, we present\nthree supervised datasets MeHate, MeSent,\nand MeLID for downstream tasks like code-\nmixed Mr-En hate speech detection, sentiment\nanalysis, and language identification respec-\ntively. These evaluation datasets individu-\nally consist of manually annotated ~12,000\nMarathi-English code-mixed tweets. Ablations\nshow that the models trained on this novel\ncorpus significantly outperform the existing\nstate-of-the-art BERT models. This is the first\nwork that presents artifacts for code-mixed\nMarathi research. All datasets and models\nare publicly released athttps://github.com/\nl3cube-pune/MarathiNLP.\n1 Introduction\nThe modern world has been engulfed by the pres-\nence of social media platforms like Twitter and\nFacebook (Salem and Mourtada, 2011). Moreover,\nwebsites like YouTube have witnessed considerable\nuser interaction in the comments section of videos\n(Siersdorfer et al., 2010). These posts and com-\nments closely reflect the thoughts of the general\npublic. It is common for users from different lin-\nguistic backgrounds to discuss social, political, and\nother topics over such social media. This leads to\nusers using a mixed language for communicating\nover social media platforms.\nCode-mixing is known as the mixing of words\nfrom multiple languages while retaining the script\n∗First author, equal contribution\nof a single language. The Latin script is often\nused to encapsulate the terms of some languages.\nFor example, a given text can be of the Marathi\nlanguage written in Latin script, as opposed to the\nDevanagari script, which is the original script of the\nMarathi language (Joshi, 2022a). Code-mixed data\nis inherently difficult to process and analyze due to\nits linguistic complexity, variance in spelling and\ngrammar, and long-tailed distribution of uncom-\nmon terms and phrases, which are often specific to\nthe geography and demographics of the source lo-\ncation. It is observed that a large number of tweets,\ncomments, and posts on social media are code-\nmixed in nature. Thus, with the advent of social\nmedia analytics, effectively analyzing code-mixed\ndata has gained the utmost importance.\nMarathi is a language which has its origins in\nMaharashtra, a state in India. Due to the state’s\ngeographic and demographic expanse, Marathi has\nevolved into a language with multiple varieties and\ndialects. Recently, there has been some focus on\nMarathi NLP based on the Devanagari script (Joshi,\n2022b,a; Kulkarni et al., 2021; Patil et al., 2022;\nLitake et al., 2022). However, a large chunk of\ntweets, posts, and comments in Marathi are in code-\nmixed form. In spite of this, no efforts have been\nmade to curate models and datasets pertaining to\nMarathi code-mixed data in the past. This work\npresents the following.\n1. We release three supervised datasets (L3Cube-\nMeHate, MeLID, and MeSent) and one unsu-\npervised dataset (L3Cube-MeCorpus)1. The\nunsupervised corpus comprises 5 million\n(70.9M tokens) Roman script code-mixed\nMarathi-English samples compiled from vari-\nous sources. We further include 5M Devana-\ngari sentences based on original text making\nit a 10 million (139.5M tokens) mixed-script\nMeCorpus.\n1MarathiNLP\n243\n2. The supervised dataset contains labels for\ncode-mixed Marathi-English (Roman script)\nhate classification, sentiment detection, and\nlanguage identification. These datasets were\nmanually annotated by native Marathi speak-\ners.\n3. Finally, we release a plethora of code-\nmixed MeBERT-based pre-trained and fine-\ntuned models for downstream tasks trained\non these novel corpora. These mod-\nels include MeBERT 2, MeBERT-Mixed 3,\nMeBERT-Mixed-v24, MeRoBERTa-Mixed5,\nand MeRoBERTa6. The models suffixed as\n’Mixed’ were trained on full 10M MeCor-\npus while others were trained on 5M Roman\nMeCorpus. The supervised models include\nMeSent-RoBERTa7, MeHate-RoBERTa8, and\nMeLID-RoBERTa9.\nThis work is a major milestone towards democ-\nratizing NLP for the Marathi language. Addition-\nally, we present several ablations with fine-tuned\nmodels. This is the first work to present a large\nunsupervised corpus, multiple pre-trained models,\nand high-quality supervised datasets. This work is\na strong foundation in the domain of Marathi and\ncode-mixed Marathi NLP.\n2 Related Works\nThe use of regional scripts, such as Devanagari,\nGurmukhi, Bengali, etc., presents a significant chal-\nlenge in India due to keyboards primarily designed\nfor the Roman script and the population’s familiar-\nity with it. The demand for code-mix datasets and\nmodels tailored to regional languages has increased\nexponentially. These resources play a crucial role\nin enabling enhanced analysis and moderation of\nsocial media content that is code-mixed.\nIn the realm of language models, BERT-based\narchitectures (Vaswani et al., 2017), including vari-\nations such as RoBERTa (Liu et al., 2019) and\nALBERT (Lan et al., 2019), have gained popu-\nlarity due to their application in pre-training and\n2MeBERT\n3MeBERT-Mixed\n4MeBERT-Mixed-v2\n5MeRoBERT-Mixed\n6MeRoBERTa\n7MeSent-RoBERTa\n8MeHate-RoBERTa\n9MeLID-RoBERTa\nfine-tuning on various tasks. Multilingual mod-\nels like multilingual-BERT, XLM-RoBERTa (Con-\nneau et al., 2019), and MuRIL(Khanuja et al., 2021)\nhave specifically focused on data representations\nthat are multilingual and cross-lingual in nature, of-\nfering improvements in accuracy and latency. How-\never, these models are pre-trained on less than a\nhundred thousand real code-mix texts.\nWhile previous research efforts have addressed\ncode mixing in other Indian languages, the specific\ndomain of code-mix Marathi remains largely unex-\nplored. Notably, there is a scarcity of prior work\nand an absence of a dedicated code-mix Marathi\ndataset. However, other Indian languages have seen\nsome notable contributions. For instance, Hande\net al. (2020) presents KanCMD a code-mixed Kan-\nnada dataset for sentiment analysis and offensive\nlanguage detection. Chakravarthi et al. (2021)\nand have released datasets encompassing Tamil-\nEnglish and Malayalam-English code-mixed texts.\nNayak and Joshi (2022) have made available Hing-\nCorpus, a Hindi-English code-mix dataset, and also\nopen-sourced pre-trained models trained on code-\nmix corpora. Srivastava and Singh (2021) provide\nHinGE, a dataset for the generation and evaluation\nof code-mixed Hinglish text, and demonstrate tech-\nniques for algorithmically creating synthetic Hindi\ncode-mixed texts.\nIn the realm of transliteration, there have been\nattempts to pre-train language models using translit-\nerated texts. However, these models often underper-\nform due to the rule-based nature of most translit-\neration techniques, which struggle to account for\nthe diverse spelling variations present in real-life\ncode-mixed texts (Santy et al., 2021).\n3 MeCorpus - Pretraining Data Creation\nWe introduce MeCorpus, a new pre-training cor-\npus of 10 million code-mixed Marathi sentences.\nThis consists of 5M Roman and 5M Devanagari\nsentences. These sentences are extracted from the\nsocial media platforms YouTube and Twitter. We\nalso used synthetic data obtained by transliterating\nTweets written in the Devanagari script. The com-\nplete data collection process is illustrated in Figure\n1.\n3.1 Twitter data\nA part of the pretraining corpus is obtained from\nthe social networking site Twitter. We utilize\nsnscrape, a scraper for social networking sites\n244\nYoutube\nTwitter\nComments (Top 200 \nYoutube Channels)\nPreprocessing\nYoutube mr-en\n2.28M\nscrape_by = lang(mr)\nscrape_by = keyword\nscrape_by = video_id\nMarathi Tweets\n(Devanagari) \nTwitter Transliterated\n1.69 M\nTwitter mr-en \n1.04M\nRemove English\nRemove Devanagari\nRemove mr-hi codemix\nSource Scraping Functions\nTweets (Romanized \nmarathi keywords 75) Language filtering\nTransliteration\nRaw Datasets mePreTraining Dataset\nFigure 1: Dataset creation process for our 5 million code-mixed corpora. These 5 million examples are further\ntransliterated to the Devanagari script, which results in a combined corpora of 10 million examples.\nto scrape the data from Twitter. We use a keyword-\nbased approach to curate the data. We use fre-\nquently used Marathi words as keywords and fetch\nall the tweets containing the given word. The list of\nseed keywords is generated by selecting a few com-\nmon Marathi words and scraping tweets containing\nthese words. Then we identify the most frequently\noccurring Marathi words in these tweets and add\nthem to our list. The seed words are properly vetted\nbefore being added to the list. Proper care is taken\nto manually check that the word is predominantly\nexclusive to the Marathi language and doesn’t oc-\ncur in texts from other languages. We fetch a fixed\nnumber of tweets belonging to a certain keyword\nand discard the keyword if the tweets fail to satis-\nfactorily meet the aforementioned conditions after\nmanual verification. Otherwise, we scrape all the\ntweets containing the keyword and add them to our\ncorpus. This manual verification process ensures\nthat the curated data largely contains Marathi text.\nThe Twitter data amounts to over a million\ntweets. All of the tweets at least partly contain\ncode-mixed Marathi. A significant number of the\ntweets exhibit code-switching between Marathi and\nEnglish. A small portion of the tweets also contain\ncode-switched Hindi-Marathi text. We anonymize\nthe data before using it for pre-training. The user-\nname mentions are replaced with the ’@USER’\ntext. We also remove links and hashtags from the\ntweets. The Twitter corpus contains 50M tokens.\n3.2 YouTube data\nYouTube comments are an excellent source of\nCode-mixed Marathi data. We scrape all the com-\nments from 200 Marathi YouTube channels us-\ning the youtube-comments-scraper library. This\nSource Number of\nSentences\nTwitter tweets 1,037,659\nYoutube comments 2,277,108\nTransliterated-tweets 1,685,233\nTotal 5,000,000\nTable 1: Source-wise split of the Roman script pretrain-\ning corpus (5M).\ngave us a mix of English, Devanagari, and code-\nmixed Marathi sentences. We then removed the\nDevanagari and English comments to obtain the\ncode-mixed Marathi data. This data was then pre-\nprocessed and used in our pre-training dataset. De-\nvanagari words were identified and removed by\nchecking their utf-8 encoding. We remove all com-\nments that have more than 80% Devanagari words.\nThis gives us comments that are either English or\nMarathi-English code-mixed. We used a fast text\nclassifier to identify sentences that are English. We\nremoved these sentences and were left with code-\nmixed Marathi sentences. Thus at the end of both\nfiltering steps, we are left with 2,278,097 of the\noriginal 7,599,588 comments.\n3.3 Transliteration\nWe scraped around 1.7 million Marathi Devanagari\ntweets from Twitter using the snscrape library. We\nthen used the indic-trans Python library to translit-\nerate 1,685,233 of these Devanagari tweets to Code-\nmixed Marathi and added them to our dataset.\n4 Me Corpus - Transliteration\nWe created an additional 5 million Devanagari sen-\ntences and added them to the corpus. This was\n245\ndone by transliterating the Tweets and YouTube\ncomments mentioned in sections 3.1 and 3.2 re-\nspectively to the Devanagari script. We also used\nthe scraped Devanagari tweets mentioned in sec-\ntion 3.3. This gave us a total of 5M Devanagari sen-\ntences. These 5M sentences were used to pre-train\nthe multilingual models mentioned in the future\nsections.\n5 MeEval - Downstream dataset creation\nWe aim to create a large dataset of code-mix\nMarathi-English data, annotated with sentiment\nand hatefulness. In this study, we selected a set of\ntweets from a larger corpus of 1,037,659 tweets ob-\ntained from the social media platform Twitter. Half\nof the tweets chosen were posted on Twitter before\n2013, and the other half were posted after 2013.\nThis helped to provide a more diachronic distribu-\ntion of tweets, as the number of tweets posted in the\npast few years far outnumber the old tweets. The\ntweets were selected randomly apart from this cri-\nteria. This ensures a realistic representation of the\nsentiment, hate, and profanity distributions present\nin the real-world data across the past several years.\nFor data annotation, we selected four annotators\nwho are proficient in Marathi, Hindi, and English\nlanguages. All four annotators are native Marathi\nspeakers and hold undergraduate-level proficiency\nin English. Any discrepancies discovered within\nthe dataset were systematically resolved through\ncollaborative consensus among the annotators.\nThe collected data was labeled according to three\ndistinct categories: sentiment, hate, and language\nidentification. We followed a set of guidelines\nwhile labeling the data to ensure the veracity of the\nannotation. We annotated the data after anonymiz-\ning it. This helped remove any bias or knowledge\nof the entity posting it. We also disregarded any\nadditional information that could be inferred by\nus based on external context but is not apparent\nby reading the text by itself. Here, we outline the\ndataset statistics and annotation procedure. The\ndataset statistics are described in Table 3. A few\nannotated examples are presented in Table 2.\n5.1 MeSent Dataset\nThe code-mixed Marathi-English sentiment data\nis termed the MeSent Dataset. Tweets express-\ning good or heartening emotions such as thank-\nfulness, happiness, applause, and appreciation are\nlabeled positive. Tweets expressing negative or\ndisheartening emotions like strong dissent, disap-\npointment, sorrow, derision, and hate are labeled\nnegative. Plain facts, statements, and simple re-\nsponses are labeled neutral. If a tweet contains\nconflicting emotions, the stronger emotion is cho-\nsen.\n• +1 indicating a positive sentiment,\n• -1 indicating a negative sentiment, and\n• 0 indicating a neutral sentiment.\nWhile annotating the data, we removed unsuitable\nand ambiguous tweets. Finally, we selected 4,000\ntweets from each sentiment category, leading to the\ndataset containing a total of 12,000 tweets.\n5.2 MeHate Dataset\nFor the hatefulness annotation, we labeled any\ntweets expressing strongly negative feelings such\nas insults, mockery, abuse, intimidation, and threats\nas hateful. Any tweet not containing such hateful\ncontent is labeled as non-hateful. We use 1 for\nhateful content and 0 for non-hateful content. The\nMeHate dataset contains 1384 hateful and 1384\nnon-hateful tweets, totaling 2768 tweets. We also\nrelease the full 12k labeled tweets with the majority\nof non-hate labels.\n5.3 MeLID dataset\nAdditionally, a Language Identification (LID)\ndataset is created. Each word within the selected\ntweets is labeled based on its language as Marathi,\nEnglish, or Other. The Other category contains\ninvalid words, words from languages other than\nEnglish or Marathi, and literals such as numbers\nand proper nouns. The MeLID dataset contains\n11,814 tweets. For all three supervised datasets,\nwe provide a pre-defined train, test, and validation\nsplit of 80:10:10.\n6 Models trained on code-mixed\nMeCorpus\nWe train several well-known models on our novel\npretraining corpora. In this section, we outline\nthese models and their training details.\nWe used pre-trained BERT, RoBERTa, mBERT,\nMuRIL, and XLM Roberta as the base models\nand trained them on the novel MeCorpus using\nthe Masked Language Modelling (MLM) objective.\nFor MLM training, we train the models for two\n246\nExample Text Hate\n(MeHate Dataset)\nSentiment\n(MeSent Dataset)\nLID\n(MeLID Dataset)\nGood morning sir mast tumhala \nbhetayche ahe\n0\n(Non-hateful)\n1 \n(Positive)\n[\"Good\" : ENG, \"morning\" : ENG, \"sir\" : \nENG, \"mast\" : MAR, \"tumhala\" : MAR, \n\"bhetayche\" : MAR, \"ahe\" : MAR ]\nnalayak jalavu nakos amhala 1 \n(Hateful)\n-1\n(Negative)\n[\"nalayak\" : MAR, \"jalavu\" : MAR, \n\"nakos\" : MAR, \"amhala\" : MAR ]\nSsc cha decision lavkar ghya 0\n(Non-hateful)\n0 \n(Neutral)\n[\"Ssc\" : OTH, \"cha\" : MAR, \"decision\" : \nENG, \"lavkar\" : MAR, \"ghya\" : MAR ]\nFigure 2: Table containing code-mixed examples with their annotations.\nepochs at a learning rate of 2e − 5, with a weight\ndecay of 0.01 and a mask probability of 0.15.\nThe monolingual models were pre-trained on the\nRoman 5M codemixed data mentioned in section\n3. While the multilingual models were trained on\nthe full 10M corpus (5M Roman + 5M Devanagari\nsentences) mentioned in section 4. The models\npre-trained on mixed-script corpus are suffixed as\n’Mixed’.\nThe resulting models were named similarly to\nthe original models, prefixed with \" me\", which\nstands for Marathi-English. Therefore, the mod-\nels MeBERT, MeBERT-Mixed, MeBERT-Mixed-\nv2, MeRoBERTa-Mixed, and MeRoBERTa are\nthe BERT, mBERT, MuRIL, XLM-RoBERTa, and\nRoBERTa models trained on the MeCorpus respec-\ntively. Note that these models are further \"fine-\ntuned\" on the MeCorpus using the MLM training\nobjective.\n7 Results\nWe fine-tune our MeBERT models on the MeSent,\nMeHate, and MeLID datasets as mentioned in sec-\ntion 5 and test them on the respective test data. The\nsame process is repeated for their base models and\na few state-of-the-art Marathi models like Indic-\nBERT (Kakwani et al., 2020), Marathi-Tweets-\nBERT (Gokhale et al., 2022), and Marathi Code-\nmixed Abusive MuRIL (Das et al., 2022). The\nresults obtained from this are showcased in Table 2.\nIt is observed that MeBERT-Mixed-v2 outperforms\nall other models on the MeHate evaluation set with\nan F1 score of 78.3%. For the sentiment analysis\ncorpus MeSent, MeRoBERTa outperforms the oth-\ners by obtaining an F1 score of 67.27%. Testing the\nmodels on the MeLID dataset, MeBERT-Mixed-v2\noutperforms the other models by obtaining an F1\nModel MeHate MeSent MeLID\nIndic-BERT 61.62 55.29 87.37\nMahaTweets-BERT 63.18 57.59 87.38\nAbusive-MuRIL 67.69 - -\nBERT 61.98 59.06 87.89\nMeBERT 73.78 61.92 88.01\nmBERT 66.80 60.25 87.64\nMeBERT-Mixed 77.39 65.73 88.25\nMuRIL 67.93 63.38 87.55\nMeBERT-Mixed-v2 78.3 64.23 88.6\nXLM-RoBERTa 64.66 61.06 87.54\nMeRoBERTa-Mixed 78.07 67.17 87.42\nRoBERTa 66.10 58.86 86.46\nMeRoBERTa 77.85 67.27 88.41\nTable 2: Macro F1 scores (in %) of models on the\nMeHate, MeSent, and MeLID datasets.\nscore of 88.6%. The newly pre-trained code-mixed\nMeBERT-based models consistently outperform\ntheir base models as well as the state-of-the-art\nMarathi models.\n8 Conclusion\nThis work lays the necessary groundwork for fu-\nture work on code-mixed Marathi. We introduce a\nnovel pretraining corpus of 5 million code-mixed\ntext examples. In addition to that, we present five\nnew models trained on this code-mixed corpus.\nFurthermore, we present three supervised datasets\nof 12,000 tweets for hate classification, sentiment\nanalysis, and language identification annotated by\nnative Marathi speakers. We also present thorough\nablations and show that our code-mixed MeBERT\nmodels outperform the previous state-of-the-art\nmodels by a considerable margin.\n247\nLimitations\nA major problem while dealing with Romanized\nMarathi is the lack of a singular correct spelling\nof words. A Marathi word can be written in sev-\neral ways in Marathi, all of which are equally\nvalid and correctly convey meaning despite hav-\ning significantly different spellings. Developing\nefficient approaches to tackle this issue will lead to\na significant increase in performance on NLP tasks\ndealing with code-mixed languages. Our keyword-\nbased scraping method uses words primarily from\nthe western Maharashtra dialect of Marathi, which\nmight not sufficiently represent samples from other\nMarathi dialects. Efforts to increase the dataset to\ninclude examples from other dialects will make the\ndataset more diverse and robust.\nEthics Statement\nAll of the data used in our experiments has been\nscraped by legal and valid means, adhering to the\nprovided guidelines. We anonymized the data be-\nfore usage to protect the privacy of the original\nauthors of the data. This data might contain biases\nand thus must be used with care. This data also con-\ntains strong language which might be unsuitable\nfor some applications. This data should be used\nonly for research purposes and not for training any\nmodel for deployment.\nAcknowledgments\nThis work was done under the L3Cube Pune men-\ntorship program. We would like to express our\ngratitude towards our mentors at L3Cube for their\ncontinuous support and encouragement. This work\nis a part of the L3Cube-MahaNLP project (Joshi,\n2022b).\nReferences\nBharathi Raja Chakravarthi, Prasanna Kumar Ku-\nmaresan, Ratnasingam Sakuntharaj, Anand Kumar\nMadasamy, Sajeetha Thavareesan, Premjith B, Sub-\nalalitha Chinnaudayar Navaneethakrishnan, John P.\nMcCrae, and Thomas Mandl. 2021. Overview of the\nHASOC-DravidianCodeMix Shared Task on Offen-\nsive Language Detection in Tamil and Malayalam. In\nWorking Notes of FIRE 2021 - Forum for Information\nRetrieval Evaluation. CEUR.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nMithun Das, Somnath Banerjee, and Animesh Mukher-\njee. 2022. Data bootstrapping approaches to improve\nlow resource abusive language detection for indic lan-\nguages. In Proceedings of the 33rd ACM Conference\non Hypertext and Social Media, pages 32–42.\nOmkar Gokhale, Aditya Kane, Tanmay Chavan, Shan-\ntanu Patankar, and Raviraj Joshi. 2022. Spread love\nnot hate: Undermining the importance of hateful pre-\ntraining for hate speech detection. arXiv preprint\narXiv:2210.04267.\nAdeep Hande, Ruba Priyadharshini, and Bharathi Raja\nChakravarthi. 2020. Kancmd: Kannada codemixed\ndataset for sentiment analysis and offensive language\ndetection. In Proceedings of the Third Workshop\non Computational Modeling of People’s Opinions,\nPersonality, and Emotion’s in Social Media, pages\n54–63.\nRaviraj Joshi. 2022a. L3cube-mahacorpus and ma-\nhabert: Marathi monolingual corpus, marathi bert\nlanguage models, and resources. In Proceedings\nof the WILDRE-6 Workshop within the 13th Lan-\nguage Resources and Evaluation Conference, pages\n97–101.\nRaviraj Joshi. 2022b. L3cube-mahanlp: Marathi natural\nlanguage processing datasets, models, and library.\narXiv preprint arXiv:2205.14728.\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish\nGolla, NC Gokul, Avik Bhattacharyya, Mitesh M\nKhapra, and Pratyush Kumar. 2020. Indicnlpsuite:\nMonolingual corpora, evaluation benchmarks and\npre-trained multilingual language models for indian\nlanguages. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 4948–\n4961.\nSimran Khanuja, Diksha Bansal, Sarvesh Mehtani,\nSavya Khosla, Atreyee Dey, Balaji Gopalan,\nDilip Kumar Margam, Pooja Aggarwal, Rajiv Teja\nNagipogu, Shachi Dave, Shruti Gupta, Subhash\nChandra Bose Gali, Vish Subramanian, and Partha\nTalukdar. 2021. Muril: Multilingual representations\nfor indian languages.\nAtharva Kulkarni, Meet Mandhane, Manali Likhitkar,\nGayatri Kshirsagar, and Raviraj Joshi. 2021.\nL3cubemahasent: A marathi tweet-based sentiment\nanalysis dataset. In Proceedings of the Eleventh\nWorkshop on Computational Approaches to Subjec-\ntivity, Sentiment and Social Media Analysis, pages\n213–220.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\n248\nOnkar Litake, Maithili Ravindra Sabane, Parth Sachin\nPatil, Aparna Abhijeet Ranade, and Raviraj Joshi.\n2022. L3cube-mahaner: A marathi named entity\nrecognition dataset and bert models. In Proceedings\nof the WILDRE-6 Workshop within the 13th Lan-\nguage Resources and Evaluation Conference, pages\n29–34.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nRavindra Nayak and Raviraj Joshi. 2022. L3cube-\nhingcorpus and hingbert: A code mixed hindi-english\ndataset and bert language models. arXiv preprint\narXiv:2204.08398.\nHrushikesh Patil, Abhishek Velankar, and Raviraj Joshi.\n2022. L3cube-mahahate: A tweet-based marathi hate\nspeech detection dataset and bert models. In Proceed-\nings of the Third Workshop on Threat, Aggression and\nCyberbullying (TRAC 2022), pages 1–9.\nFadi Salem and Racha Mourtada. 2011. Civil move-\nments: The impact of facebook and twitter. Arab\nSocial Media Report, 1.\nSebastin Santy, Anirudh Srinivasan, and Monojit Choud-\nhury. 2021. BERTologiCoMix: How does code-\nmixing interact with multilingual BERT? In Proceed-\nings of the Second Workshop on Domain Adaptation\nfor NLP, pages 111–121, Kyiv, Ukraine. Association\nfor Computational Linguistics.\nStefan Siersdorfer, Sergiu Chelaru, Wolfgang Nejdl,\nand Jose San Pedro. 2010. How useful are your com-\nments? analyzing and predicting youtube comments\nand comment ratings. pages 891–900.\nVivek Srivastava and Mayank Singh. 2021. Hinge: A\ndataset for generation and evaluation of code-mixed\nhinglish text. In Proceedings of the 2nd Workshop on\nEvaluation and Comparison of NLP Systems, pages\n200–208.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nA Appendix\n249\nDataset Sample\nCount\nAverage\nword\ncount\nLabels\nCount\nper\nlabel\nMeLID 11,813 12\nMarathi 99,230\nEnglish 26,290\nOther 10,870\nMeSent 12,000 16\nPositive 4,000\nNeutral 4,000\nNegative 4,000\nMeHate 2,768 17 Non Hate 1,384\nHate 1,384\nTable 3: Statistics for benchmark datasets"
}