{
  "title": "Recent advances in deep learning and language models for studying the microbiome",
  "url": "https://openalex.org/W4406128378",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2507107688",
      "name": "BingHao Yan",
      "affiliations": [
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A4229091923",
      "name": "Yunbi Nam",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2096161695",
      "name": "Lingyao Li",
      "affiliations": [
        "University of South Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2734853497",
      "name": "Rebecca A. Deek",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2119815381",
      "name": "Hongzhe Li",
      "affiliations": [
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2121527847",
      "name": "Siyuan Ma",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2507107688",
      "name": "BingHao Yan",
      "affiliations": [
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A4229091923",
      "name": "Yunbi Nam",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2096161695",
      "name": "Lingyao Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2734853497",
      "name": "Rebecca A. Deek",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2119815381",
      "name": "Hongzhe Li",
      "affiliations": [
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2121527847",
      "name": "Siyuan Ma",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2972021296",
    "https://openalex.org/W2899779000",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W2009257824",
    "https://openalex.org/W4404821554",
    "https://openalex.org/W4392884201",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3134954922",
    "https://openalex.org/W4392270525",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W4391316987",
    "https://openalex.org/W4388923523",
    "https://openalex.org/W2967688728",
    "https://openalex.org/W4302426080",
    "https://openalex.org/W3033077186",
    "https://openalex.org/W2047032063",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4393870943",
    "https://openalex.org/W6747701563",
    "https://openalex.org/W2624135745",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W4396912882",
    "https://openalex.org/W4384819947",
    "https://openalex.org/W4393433029",
    "https://openalex.org/W6858838901",
    "https://openalex.org/W4396738493",
    "https://openalex.org/W4390790220",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4362736313",
    "https://openalex.org/W4229073727",
    "https://openalex.org/W4385988359",
    "https://openalex.org/W4214943162",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W4387840657",
    "https://openalex.org/W6859050812",
    "https://openalex.org/W6683033130",
    "https://openalex.org/W2794407684",
    "https://openalex.org/W2135639274",
    "https://openalex.org/W2187341651",
    "https://openalex.org/W4388024559",
    "https://openalex.org/W4390692489",
    "https://openalex.org/W3200103613",
    "https://openalex.org/W4404644013",
    "https://openalex.org/W4310957810",
    "https://openalex.org/W4377861946",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2119859604",
    "https://openalex.org/W4283321006",
    "https://openalex.org/W2608736608",
    "https://openalex.org/W1448965840",
    "https://openalex.org/W2978901899",
    "https://openalex.org/W2076048958",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4406302098",
    "https://openalex.org/W3170383918",
    "https://openalex.org/W6863126066",
    "https://openalex.org/W4393299232",
    "https://openalex.org/W4389258756",
    "https://openalex.org/W4212774754"
  ],
  "abstract": "Recent advancements in deep learning, particularly large language models (LLMs), made a significant impact on how researchers study microbiome and metagenomics data. Microbial protein and genomic sequences, like natural languages, form a language of life , enabling the adoption of LLMs to extract useful insights from complex microbial ecologies. In this paper, we review applications of deep learning and language models in analyzing microbiome and metagenomics data. We focus on problem formulations, necessary datasets, and the integration of language modeling techniques. We provide an extensive overview of protein/genomic language modeling and their contributions to microbiome studies. We also discuss applications such as novel viromics language modeling, biosynthetic gene cluster prediction, and knowledge integration for metagenomics studies.",
  "full_text": "Recent advances in deep learning\nand language models for studying\nthe microbiome\nBinghao Yan1† , Yunbi Nam2† , Lingyao Li3, Rebecca A. Deek4,\nHongzhe Li1* and Siyuan Ma2*\n1Department of Biostatistics, Epidemiology, and Informatics, Perelman School of Medicine, University of\nPennsylvania, Philadelphia, PA, United States,2Department of Biostatistics, Vanderbilt University Medical\nCenter, Nashville, TN, United States,3School of Information, University of South Florida, Tampa, FL,\nUnited States,4Department of Biostatistics and Health Data Science, University of Pittsburgh, Pittsburgh,\nPA, United States\nRecent advancements in deep learning, particularly large language models\n(LLMs), made a signiﬁcant impact on how researchers study microbiome and\nmetagenomics data. Microbial protein and genomic sequences, like natural\nlanguages, form a language of life, enabling the adoption of LLMs to extract\nuseful insights from complex microbial ecologies. In this paper, we review\napplications of deep learning and language models in analyzing microbiome\nand metagenomics data. We focus on problem formulations, necessary datasets,\nand the integration of language modeling techniques. We provide an extensive\noverview of protein/genomic language modeling and their contributions to\nmicrobiome studies. We also discuss applications such as novel viromics\nlanguage modeling, biosynthetic gene cluster prediction, and knowledge\nintegration for metagenomics studies.\nKEYWORDS\nmicrobiome, virome, artiﬁcial intelligence, large language models, transformer, attention\n1 Introduction\nThe study of microbiomes and metagenomics has signi ﬁcantly advanced our\nunderstanding of microbial communities and their complex interactions within\nhosts and environments. The microbiome refers to the collective genomes of\nmicroorganisms residing in a speci ﬁc habitat, such as human body sites (e.g., gut,\nskin, airway) and environments (e.g., air, soil, water). Metagenomics research involves\nthe direct proﬁling and analysis of these microbial communities’genomic sequences,\nbypassing the need for isolating and cultu ring individual members. This approach\nallows for a comprehensive assessment of microbial diversity, functions, and dynamics\nwithin their natural contexts.\nThe complex dependency encoded in metagenomic sequences represents gene/protein-,\norganism-, and community-level biological structures and functions. Examples include\nresidue-residue contact patterns for protein 3D structures, functional relationship between\ngenes and their regulatory, non-coding counterparts (e.g., promoters, enhancers), mobility\nfor horizontal gene transfers, and genome-scale organization of functional modules (e.g.,\noperons and biosynthetic gene clusters). Such dependency patterns, when interrogated at\nthe revolutionary scale (i.e., encompassing many diverse organisms and environments), can\ncapture fundamental biological properties as shaped over time by evolutionary processes,\nthus representing a meaningful“language of life”. On the other hand, the availability of\nOPEN ACCESS\nEDITED BY\nHuilin Li,\nNew York University, United States\nREVIEWED BY\nTao He,\nSan Francisco State University, United States\nLiangliang Zhang,\nCase Western Reserve University, United States\n*CORRESPONDENCE\nHongzhe Li,\nhongzhe@upenn.edu\nSiyuan Ma,\nsiyuan.ma@vumc.org\n†These authors have contributed equally to\nthis work\nRECEIVED 10 September 2024\nACCEPTED 13 December 2024\nPUBLISHED 07 January 2025\nCITATION\nYan B, Nam Y, Li L, Deek RA, Li H and Ma S (2025)\nRecent advances in deep learning and language\nmodels for studying the microbiome.\nFront. Genet. 15:1494474.\ndoi: 10.3389/fgene.2024.1494474\nCOPYRIGHT\n© 2025 Yan, Nam, Li, Deek, Li and Ma. This is an\nopen-access article distributed under the terms\nof theCreative Commons Attribution License\n(CC BY). The use, distribution or reproduction in\nother forums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in this\njournal is cited, in accordance with accepted\nacademic practice. No use, distribution or\nreproduction is permitted which does not\ncomply with these terms.\nFrontiers inGenetics frontiersin.org01\nTYPE Review\nPUBLISHED 07 January 2025\nDOI 10.3389/fgene.2024.1494474\nmicrobial genomic sequences, both annotated and unannotated for\nbiological properties [e.g., UniRef, Suzek et al. (2007); MGnify;\nRichardson et al. (2023) ], drastically increased over the past\ndecade due to advancements in next-generation sequencing\nprotocols, bioinformatics, and computational capacities. The\navailability of these metagenomic “big data” suggests that, given\ncapable modeling architecture and capacity, microbiomes ’\nevolutionary and functional dependency structures can be\ncomputationally learned, represented, and utilized for studying\nthe microbiome.\nTo this end, advances in powerful artiﬁcial intelligence (AI)\nmethods regarding the design and training of highly complex, large-\nscale deep learning models have been adopted to characterize\nmicrobial genes and genomes from large-scale metagenomic\nsequences, offering powerful tools for extracting, interpreting,\nand integrating complex microbiome data ( Hernández Medina\net al., 2022). In particular, inspired by the recent breakthrough of\nlarge language models (LLMs) in dealing with natural language\ntasks, similar methods have been developed and applied for\nmodeling protein and genomic languages of life. To avoid\nconﬂating nomenclature, we reserve “LLM” in this review\nexclusively for large language models [e.g., ChatGPT ( Liu Y.\net al., 2023)] and instead use terms such as “protein language\nmodel” and “DNA language model” to more explicitly refer to\ngenomic sequence models. Indeed, whereby natural languages are\norganized in sequential words and phrases which form the basic\nunits of modeling ( “tokens”), microbial genomic elements are\nsimilarly organized as sequences of nucleotide base pairs (for\ngenomic DNA) or amino acids (AA, for proteins). Given the\ncomplexity of genomic dependency structures, metagenomic\nresearch was fast to adopt advanced language modeling\ntechniques for studying microbial community sequences, with\nmodels spanning different genomic scales (microbial proteins,\ncontigs, genomes, and communities) and designed for a variety\nof tasks (Figure 1), yielding promising performance improvement\nand novel applications.\nThis review aims to provide a survey of recent developments in\ndeep learning and language modeling for analyzing microbiome and\nmetagenomics data. We focus on problem formulation, the datasets\nrequired to address these questions, and how sequence-based\nlanguage models are integrated into deep learning algorithms. In\nSection 2, we brieﬂy discuss the typical language model architecture\nfrom recent LMM breakthroughs and how they can be applied\ntowards genomic sequence modeling. We discuss inSection 3 two\nbroad classes of language models for microbiome studies, namely,\nprotein language models and DNA/genomic language models,\ndistinguished by their drastically different range of genomic\n“contexts” for sequence dependency structures. We then review\nthree speciﬁc applications of high interests to theﬁeld in Sections\n4–6, namely, novel viromics language modeling, models for\nFIGURE 1\nReview of protein/DNA/genomic language models as applied to metagenomic studies.(A) Protein and genomic sequences share similar properties\nas natural language sequences, with amino acids or neucleotides as units of sequences (“tokens”). The complex dependency structure of protein/gene-\nlevel or genomic-scale sequences can then be modeled by language model techniques, such as transformer-based attention mechanism for various\ndownstream tasks.(B) Review of encoder- and decoder-style transformer attention mechanisms and their applications in metagenomic studies.\nDecoder-style model architecture (similar to that of BERT) aims to provide a meaningful representation of genomic sequences and is useful for\ndownstream predictive tasks. Encoder-style model architecture (similar to that of ChatGPT) generates new sequences given past tokens and is most\nuseful for generative tasks such as novel protein design.\nFrontiers inGenetics frontiersin.org02\nYan et al. 10.3389/fgene.2024.1494474\npredicting biosynthetic gene clusters (BGCs), and knowledge\nintegration in metagnomics studies aided by natural language\nLLMs. We conclude with prospective remarks and discussions\nin Section 7.\n2 Brief review of LLMs and their\nextension towards modeling the\nlanguage of life\nLLMs are advanced foundation models speciﬁcally designed to\nunderstand and generate human language. They can perform a wide\nrange of natural language processing (NLP) tasks, such as question-\nanswering, information extraction, and text summarization (Chang\net al., 2024; Li et al., 2024). The scalability, versatility, and contextual\nunderstanding of LLMs can be attributed to two key factors. First,\nLLMs are trained on massive datasets that encompass diverse\nlinguistic patterns, enabling them to learn complexities and\nnuances in languages as sequences of tokens (i.e., words and\nphrases). Second, LLMs are built on the transformer architecture,\nwhich consists of an encoder and a decoder and uses self-attention\nmechanisms to process input sequences. The attention mechanism\nefﬁciently encodes dependency structures of sequential tokens,\nvastly increases the learnable lengths of long-range dependencies,\nand encodes tokens and sequences accounting for their upstream\nand downstream neighboring “contexts”. This allows for efﬁcient\nprocessing of sequential data, enabling LLMs to provide meaningful\nrepresentation of input text and generate coherent and contextually\nrelevant output text based on input prompts (Vaswani et al., 2017).\nInspired by LLMs, language models in microbiome research\noften employ a similar architectural design ( Figure 1). These\nlanguage models of genomic sequences (Ligeti et al., 2024; Shang\net al., 2023; Mardikoraem et al., 2023) thus provide an improved\nrepresentation of sequences with richer context and can be scaled up\nto and trained at impressive complexities (up to billions of model\nparameters). Such models often include a transformer encoder\ncomponent that processes input sequences— such as protein or\nDNA sequences — and converts them into high-dimensional\nrepresentations that capture essential features of input sequences\nin their contexts. The attention mechanism in these models assigns\ndifferent weights to various parts of the sequence, characterizing\ndependency structures shaped over evolution and allowing the\nmodel to focus on relevant regions. This focus ensures that the\nmodel prioritizes areas within a sequence that are most signiﬁcant\nfor biological interpretation. Similar to the application of BERT for\nvarious natural language tasks ( Devlin, 2018 ), the encoded\nrepresentation can then be used for tasks such as contextualizing\nmicrobial genes and genomic sequences in their broader genomic\nneighborhoods (Hwang et al., 2024), predicting the structure and\nfunctions of protein given their sequences (Lin et al., 2023), and\nsegmentation and identi ﬁcation of speci ﬁc regulatory elements\nacross microbial genomes ( Zhou et al., 2023 ). In comparison,\ndecoder-style models focus on generating output sequences, given\nthe encoded representation of past sequence tokens. This is more\nsimilar to GPT-style LLMs (Brown, 2020), whereby the task towards\nmicrobiome application often involves the generation of new,\nfunctional, and viable protein sequences ( Ferruz et al., 2022 ;\nMadani et al., 2023; Jin et al., 2024).\n3 Language modeling of proteins,\ncontigs, and genomes of the\nmicrobiome\nTo facilitate the survey, we categorize existing language models\nfor metagenomic sequences into two classes: (1) models on the\nprotein/gene scale and (2) those on the genome scale. Theﬁrst,\nwhich we termprotein language models (Table 1), ﬁts well within the\ncontext length for transformers since microbial proteins are\ngenerally under 1,000 AAs (tokens). In contrast,DNA or genomic\nlanguage models (Table 2) often require additional techniques to\nextend their operating ranges due to the large scale of microbial\ncontigs or whole genomes. For example, the bacterial genome\ntypically ranges from 0.5 to 10 million base pairs, a scale that\noften far exceeds the context window of transformers. In\naddition, the two classes target different applications: protein\nlanguage models are used for designing and predicting individual\nproteins, while DNA/genomic language models examine genes and\nproteins within their broader genomic contexts as well as\nintergenic regions.\n3.1 Protein language models for novel\nprotein generation\nExisting protein language models applied towards microbiome\nstudies are summarized in Table 1. We highlight two speci ﬁc\napplications, namely, the generation of novel proteins and the\nprediction of their functions and structures. The dependency\nstructure of amino acids across known microbial proteins is\nlearned and utilized to generate arti ﬁcial, potentially novel\nprotein sequences by protein language models such as ProGen\n(Madani et al., 2023) and ProtGPT2 (Ferruz et al., 2022). This is\nperformed in an autoregressive fashion, often with decoder-only\narchitecture similar to that of the GPT language models, whereby the\nlikely AA at the next position is predicted given the sequence of\npreceding residues. If trained across a sufﬁciently large variation of\nraw occurring microbial protein spaces (millions or more protein\nsequences), models with enoughﬂexibility can learn the inherent\nevolutionary patterns that natural protein sequences harbor and\nthus generate artiﬁcial proteins that are functionally viable like\nnatural proteins.\nTo this end, ProtGPT2 was based on the GPT-2 architecture and\ntrained on 50 million sequences spanning the entire protein space.\nProteins generated by the model in return displayed propensities of\namino acid sequences akin to those of natural proteins, but can still\ncover under-explored protein sequence regions. ProGen and its\niteration ( Nijkamp et al., 2023 ) performed similar modeling\ntasks, and additionally (1) allowed the inclusion of “tags” to\nspecify protein properties for generating proteins in a more\ncontrollable fashion, and (2) experimentally veriﬁed that model-\ngenerated de novoprotein sequences were sufﬁciently distinct from\nnatural proteins but demonstrated functional viability comparable\nto them. Of note, while these models were typically trained to cover\nthe universal protein space (e.g., UniRef-50), both models highlight\ngood coverage of microbial protein properties. ProGen speciﬁcally\nvalidated the antibacterial functional property of its generated novel\nproteins that were comparable to natural lysozymes.\nFrontiers inGenetics frontiersin.org03\nYan et al. 10.3389/fgene.2024.1494474\n3.2 Protein language models for function\nand structure prediction\nRelated to, but different from the task of generating novel\nprotein sequences, prediction-focused protein language models\nare primarily concerned with predicting proteins ’ biological\nproperties (e.g., 3D structures, functions) based on their AA\nresidue sequences. Encoder-style language model architectures\nsuch as that of BERT are of particular relevance, as these models\naim to learn the best representation of each token (i.e., AA) given the\nbroader sequence context and thus can represent entire sequences in\na meaningful, efﬁcient manner. For example,Elnaggar et al. (2022)\ndeveloped several LMs for protein sequences, including two auto-\nregressive models (Transformer-XL, XLNet) and four auto-encoder\nmodels (BERT, Albert, Electra, T5) on data from UniRef and BFD\ncontaining up to 393 billion amino acids. Transformer Uniref90 MT\nfrom the Protein BERT project can be downloaded from the project\nGitHub repository (https://github.com/nadavbra/protein_bert) and\nprotein sequences are embedded using the function in the protein\nBert python package. Such representations can then be fed as the\ninput to downstream predictive models, often also realized with\nneural networks (NN), for various tasks.\nFor predicting protein structures, with scaling language models\nfrom 8 million to 15 billion parameters, the ESM-2 model (Lin et al.,\n2023) effectively internalizes evolutionary patterns directly from\nprotein sequences. The learned attention patterns provided a low-\nresolution protein structure, corresponding to residue-residue\ncontact maps. This was further combined with a downstream\npredictive module to form the ESMFold model, which offers\ndirect inference from sequence to protein 3D structures and\nachieved comparable performances as SOTA protein structure\nprediction models (e.g., AlphaFold2). Of relevance to the\nmicrobiome, the authors applied their model to construct an\natlas of predicted structures of over 600 million metagenomic\nprotein sequences. Another group of predictive tasks aims to\nmine biological functions based on protein sequences. As a\nrepresentative, Ma et al. (2022) focused on predicting\nantimicrobial peptides (AMPs) as products of the gut\nmicrobiome. They constructed the best combination over several\nlanguage models, including one of the BERT architecture, to\ncomputationally mine AMP candidates from gut metagenomic\nstudies. With additional computationalﬁltering and experimental\nvalidation, they demonstrated that identi ﬁed candidates were\neffective against multi-drug-resistant bacteria and demonstrated\nmicrobial membrane disruption in mechanistic studies. Such\nstudies represent the potential of language model-aided\ncomputational efforts toward human and environmental\nmicrobiome studies for high-throughput mining of microbial\nstructural and functional properties.\n3.3 DNA language models at the\ngenomic scale\nThe full review of DNA/genomic language models is provided in\nTable 2. As discussed above, microbial genomes have drastically\nincreased scales compared to single genes or proteins. Genomic\nsequences also possess much sparser biological information than\nproteins, containing intergenic regions with both functional and\njunk DNA elements. The DNA sequence vocabulary also only\nconsists of four different types of nucleotides, less than the\n20 different AAs that typically constitute protein sequences. As\nsuch, language models that operate on the genome scale require\nadditional considerations than protein models and can be further\ndivided into two categories. Theﬁrst type, often termed in literature\nas DNA language models, focuses on modeling DNA sequences\nTABLE 1 Protein language models.\nModel Model architecture Usage Relevance to the\nmicrobiome\nAdditional notes\nAntimicrobial peptide\n(AMP) prediction Ma\net al. (2022)\nThe best combination of LSTM,\nattention-based and BERT models\nIdentifying candidate AMPs\nfrom human microbiome data\nthat were further validated\nexperimentally\nTraining/validation data and\napplication focus on bacterial\npeptides\nAMPs predicted through best\nensemble of different prediction\nmodels, suggesting robustness of\nﬁndings\nProtGPT2 Ferruz et al.\n(2022)\nTransformer decoder model that\nmatches that of GPT2\nGenerating novel proteins Universal training and\nvalidation sequences include\nmicrobial proteins\nByte Pair Encoding (BPE)\ntokenization improves model\nperformance\nProGen Madani et al.\n(2023);\nProgen2 (Nijkamp et al.,\n2023)\nStandard transformer decoder with\nleft-to-right causal masking\nGenerating viable and novel\nproteins with controlled\nfunctions\nUniversal training and\nvalidation sequences includes\nmicrobial proteins\nGenerated viable and\nexperimentally validated\nantibacterial proteins\nModel size study suggests even huge\nmodels (>6 billion parameters) are\nfar from overﬁtting. Suggested\nmodel can traverse protein space\nunderexplored in naturally observed\nsequences\nESM-1b Rives et al.\n(2021);\nESM-2, ESMFold (Lin\net al., 2023)\nESM-1b/ESM-2: BERT-like masked\ntoken architecture;\nESMFold: Folding NN (based on\nESM-2 representations) composed of\nfolding blocks + structure prediction\nmodule\nESM-1b/ESM-2 provides\nmeaningful sequence\nrepresentations\nESMFold provides fast and\naccurate prediction of protein\n3D structure based on\nsequences\nUniversal training and\nvalidation sequences include\nmicrobial proteins\nProvides database with\nprediction of large number of\nmetagenomic protein\nsequences\nProtein sequence attention pattern\ncan predict structural residue\ncontact probability in a zero-shot\nfashion\nLanguage model architecture allows\nfaster structure prediction compared\nto SOTA models based on multiple\nsequence alignment\nFrontiers inGenetics frontiersin.org04\nYan et al. 10.3389/fgene.2024.1494474\ntruly on the full genome scale of organisms, e.g., DNABERT (Zhou\net al., 2023), Nucleotide Transformer [NT,Dalla-Torre et al. (2024)].\nAs such, they adopt techniques such as specialized tokenization,\nalternative attention patterns, and hierarchical modeling\narchitecture to drastically extend model contextual lengths. An\nimportant advantage of this approach is that it allows for the\nrepresentation and identi ﬁcation of non-coding functional\nelements on the DNA (e.g., promoters).\nTasked with the ambitious goal of providing a generic,\n“foundation” model for genomes, models such as the DNABERT\nand NT aim to provide meaningful, contextualized representations\nof genome-scale DNA sequences that can be used to predict their\nfunctional properties and molecular phenotypes. Trained on\ngenomes spanning hundreds of organisms (including genomes\nfrom microbial species) and based on encoder-style model\narchitectures, these models are then utilized towards tasks such\nas predicting genomic elements (promoter, enhancer, transcription\nfactor, epigenetic marks) and differentiating microbial species.\nWhile studies of human genomes are still the focus, they do\ndemonstrate transferrable of learned representations across\nspecies to metagenomes, as well as improved model performance\nwhen the combination of diverse genomes was included during\nTABLE 2 DNA/genomic language models.\nModel Model architecture Usage Relevance to the\nmicrobiome\nAdditional notes\nDNABERT Ji et al.\n(2021);\nDNABERT-2\n(Zhou et al., 2023)\nEncoder-only transformer\narchitecture with masked modeling\nDNABERT-2 incorporates new\ntechniques such as Attention with\nLinear Biases to increase context\nlength and Flash Attention to\nincrease computation and memory\nefﬁciency\nHuman (DNABERT) and multi-\nspecies (DNABERT-2) for\nrepresenting genomic nucleotide\nsequences applicable for\ndownstream tasks\nDemonstrated utilities in various\ntasks: genomic element prediction\n(promoter, enhancer,\ntranscription factor, epigenetic\nmarks), microbial species\nclassiﬁcation\nDNABERT-2 training/validation\nbased on multi-species genomes\nincluding bacteria and fungi\nEvaluated for microbial gnomic\nelement prediction and species\nclassiﬁcation\nAlso provides system of\nbenchmarking tasks for evaluating\nDNA language models\nBPE tokenization improves model\nperformance\ngLM Hwang et al.\n(2024)\nRoBERTa-based transformer\narchitecture\nGenes (embedding from ESM-2)\nare tokens and microbial contigs\n(15–30 genes) are sequences\nEnriching microbial genes’\nrepresentations with longer-range\ngenomic context. Providing\ncontextualized gene function\nprediction and characterizing\nhigher-order genomic features\nTraining/validation data and\napplication focus on microbial\ngenomes\n“Contextualization” learns\nrepresentations of microbial genes in\ntheir longer-range genomic contexts,\nencoding enriched genomic\ninformation such as mobility for\nhorizontal gene transfer and operon\nmembership\nNT Dalla-Torre\net al. (2024);\nSegmentNT (de\nAlmeida et al.,\n2024)\nNT: encoder-only transformer\narchitecture with masked modeling\nSegmentNT: a segmentation NN\nhead based on NT embedding\nNT provides human-focused and\nmulti-species (DNABERT-2)\nrepresentation for genomic\nnucleotide sequences applicable\nfor downstream tasks\nSegmentNT specializes in\npredicting genomic elements\nbased NT representation\nNT has multi-species version\nincorporating bacterial and fungal\ngenomes\nSegmentNT does not incorporate\nmicrobial genomes\nPerformance of multi-species model\nwas demonstrated to match or\noutperform human-only model on\ntasks speciﬁc for human genomes\nSpecies-aware DNA\nlanguage models\nKarollus et al.\n(2024)\nStandard DNABERT architecture\nwas adopted\n(a) to learn meaningful species-\nspeciﬁc and shared regulatory\nfeatures across evolution (b) to\ntransfer these features to unseen\nspecies\nTrained on non-coding regions\nfrom >800 fungal species spanning\nover 500 million years of evolution\nFocus on non-coding DNA and\nregulatory elements\nFGBERT Duan et al.\n(2024)\nJoint objectives of (a) masked gene\nmodeling with a context-aware\ntokenizer and (b) contrastive\nlearning with data augmentation\nand negative sampling to capture\nthe functional relationships\nbetween genes\nDownstream tasks include gene\noperons, functional genes,\ngenome pathogenes, and nitrogen\ncycle prediction\nPre-trained on 100 million\nmetagenomic sequences\nFirst metagenomic pre-trained\nmodel encoding (a) context-aware\nand (b) function-relevant\nrepresentations of metagenomic\nsequences. Protein-based gene\nrepresentations converted from the\nDNA sequence from metagenomic\nsequences, to protein sequence using\nENA, and then to ESM-2\nrepresentations\nProkBERT family\nLigeti et al. (2024)\nEncoder-only masked language\nmodeling with the newly\nintroduced Local Context-Aware\n(LCA) tokenization\nGenerate nucleotide sequence\nrepresentation. Applied\ndownstream tasks include (a)\nbacterial promoter prediction and\n(b) bacteriophage identiﬁcation\nBacteriophages have a signiﬁcant\nrole in the microbiome, inﬂuencing\nhost dynamics and serving as\nessential agents for horizontal gene\ntransfer\nThe implementation of masked\nlanguage modeling (MLM) with LCA\nrequires slight variations in masking\ntokens: to prevent trivial restoration\nfrom locality, the model needs to\nensure neighboring tokens to be\nmasked as well\nFrontiers inGenetics frontiersin.org05\nYan et al. 10.3389/fgene.2024.1494474\nmodel training. On more reduced scales, microbial DNA\nlanguage models are focused on learning the genomic pattern\nof speciﬁc organisms and speci ﬁc genomic elements. Karollus\net al. (2024) for example trained a DNABERT-like model\nspeciﬁcally for non-coding regi ons up- and down-stream of\ngene sequences from fungal species, and demonstrated that the\nlearned sequence representations can capture motifs and\nregulatory properties of these elements, in contrast to the\nbackground and non-coding sequences. While current DNA\nlanguage models are still limi ted in training data and model\ncapacity (the NT at its largest scale was trained with 2.5 billion\nparameters on 850 species) to truly operate as the foundational\nrepresentation of diverse ge nomes, we anticipate signi ﬁcant\nprogress in the near future aided by rapid development on\nlanguage model scales and computational power.\n3.4 Genomic language models contextualize\ngenes and gene clusters\nAlternatively, another group of metagenome language models\nexamines medium-to long-range contexts between genes, often\noperating on the contig scale and excluding intergenic sequences.\nWe term these as genomic models as an intermediate approach\nbetween protein and DNA language models. These models often\nadopt hierarchical scaffolding across genes (genes themselves are\nembedded by protein language models), to provide a\ncontextualized and richer representation of genes in their\nbroader genomic neighborhood. Gene properties such as their\ndifferential functions across microbes and genome/community-\nscale organization (horizontal gene transfer, operon membership)\ncan then be further interrogated, which is not possible in protein\nlanguage models where they are modeled in isolation from\neach other.\nIn comparison to full-scale DNA language models, genomic\nlanguage models such as the gLM (Hwang et al., 2024) and FGBERT\n(Duan et al., 2024 ) instead focus on contig-to genome-scale\norganization of microbial genes (seeTable 2). gLM, for example,\nadopts EMS-2 protein embeddings for each gene and models their\ngenomic dependency structures on the contig (15 –30 genes in\nlength) scale. This enables, ﬁrst, the enrichment of each gene’s\nembedding in its broader genomic texts. Genes’“higher-order”,\ngenome- and community-level functional properties can be further\ndelineated that are indistinguishable from protein-scale language\nmodeling alone, such as differential gene functions in different\nbiomes and microbial species, as well as their self-mobility in\nhorizontal gene transfer events across genomes. Secondly, the\norganization of gene clusters in linkage with each other on the\ngenome can also be represented, whereby subsets of model attention\npatterns from gLM and FGBERT both demonstrated\ncorrespondence with operon memberships. The longer-scale\norganization of biosynthetic gene clusters is also relevant and\ndiscussed in a dedicated section as a specialized task. As\npopulation-scale studies of the microbiome often focus on gene-\nor pathway-level sample proﬁles, such genomic language models\nprovide practical intermediate solutions to enrich microbiome\nstudies using recent language model advancement with microbial\ngene elements’broader genomic contexts.\n4 Language models for virome\nannotation and virome-host\ninteractions\nThe human virome consists of eukaryotic viruses that infect\neukaryotic cells and prokaryotic viruses, also known as\nbacteriophages, that infects and replicates within bacteria and\narchaea. The gut virome is a vital component of the human gut\nmicrobiome, consisting mainly of viruses that infect bacteria\n(bacteriophages or phages), along with other viral species that\nmay infect eukaryotic cells. The virome plays a crucial role in\nmaintaining gut health by in ﬂuencing the bacterial population\ndynamics, shaping immune responses, and potentially affecting\nthe overall metabolic environment of the gut.\nMetagenomic sequencing of the gut microbiome provides a\nwealth of information for identifying viruses, especially\nbacteriophages, which are key players in viral-bacterial\ninteractions. One important method for studying these\ninteractions is through CRISPR spacers, which serve as a\nmolecular record of past viral infections in bacterial genomes.\nCRISPR-Cas systems are a bacterial immune defense mechanism\nthat targets invading bacteriophages (Dion et al., 2021). There has\nbeen signiﬁcant interest in applying recently developed protein or\nDNA sequence language models in virome sequence identiﬁcation\nand annotation, as well as in building predictive models for virus-\nbacterium interactions based on sequence data.\n4.1 Virome sequence annotation and\nidentiﬁcation\nAnnotation of viral genomes in metagenomic samples is a\ncrucial ﬁrst step in understanding viral diversity and function.\nCurrent annotation approaches primarily rely on sequence\nhomology methods, such as pro ﬁle Hidden Markov Model\n(pHMM)-based approaches. However, these methods are limited\nby the scarcity of characterized viral proteins and the signiﬁcant\ndivergence among viral sequences. To address these challenges,\nFlamholz et al. (2024) applied curated virome protein family\n(VPF) databases alongside recently developed protein language\nmodels (PLMs). They demonstrated that PLM-based\nrepresentations of viral protein sequences can capture functional\nhomology beyond the reach of traditional sequence homology\nmethods. Their reference annotations were derived from the\nProkaryotic Virus Remote Homologous Groups (PHROGs)\ndatabase, a curated library of VPFs designed to detect remote\nsequence homology. PHROGs are manually annotated into high-\nlevel functional categories and contains 868,340 protein sequences\nclustered into 38,880 families, of which 5,088 are assigned to\n9 functional classes. Using these data, Flamholz et al. (2024)\nshowed that PLM-based representations of viral proteins can\neffectively predict their functions, even in the absence of close\nsequence homologs.\nPeng et al. (2024)developed a viral language model (ViraLM)\nthat adapts the genome foundation model DNABERT-2 (Zhou et al.,\n2023) for virus detection by ﬁne-tuning the model for a binary\nclassiﬁcation of novel viral contigs in metagenomic data.\nDNABERT-2 is pre-trained on a vast array of organisms,\nFrontiers inGenetics frontiersin.org06\nYan et al. 10.3389/fgene.2024.1494474\nacquiring valuable representations of DNA sequences, which is\nparticularly useful for distinguishing viral sequences from those\nof other species. To adapt the genome foundation model for virus\ndetection, theyﬁne-tuned this model for a binary classiﬁcation task\nwith two labels: viral sequences vs. others, where they constructed a\nsubstantial viral dataset comprising 49,929 high-quality viral\ngenomes downloaded from the NCBI RefSeq, spanning diverse\ntaxonomic groups as positive samples. The negative data\n(245,734 non-viral sequences) are complete assemblies of\nbacteria, archaea, fungi, and protozoa, also downloaded from the\nNCBI RefSeq. The genomes are randomly cut into short contigs\nranging from 300 to 2000 bp to minic variable-length contigs in the\nmetagenomic data. They observed that the model initialized using\nthe pre-trained foundation model converges faster and performs\nbetter in virus contig identiﬁcation.\n4.2 Deep learning and LLM methods for\nvirome-host interaction\nOne important problem in virome research is to predict which\nviruses can infect which hosts, a crucial step for understanding how\nviruses interact with hosts and cause diseases. Virome-host\ninteractions also play a crucial role in understanding and\ndeﬁning phage therapy, which uses bacteriophages to treat\nbacterial infections.\nCurrently, there are no high-throughput experimental methods\nthat can deﬁnitively assign a host to the uncultivated viruses. A\nnumber of computational approaches have been developed to\npredict unknown virus-host associations. The coevolution of a\nvirus and its host left signals in their genomes, which have been\nexploited for computational prediction of virus-host associations.\nThe alignment-based approaches search for homology such as\nprophage (Roux et al., 2015) or CRISPR-cas spacers (Staals and\nBrouns, 2013; Horvath and Barrangou, 2010). Algorithms like the\nBasic Local Alignment Search Tool (BLAST) are commonly used to\nalign viral sequences with host genome sequences to detect\nhomology. This can reveal conserved regions in viral and host\nproteins, such as receptor-binding domains that allow viruses to\nenter host cells. In contrast, alignment-free methods use features\nsuch as k-mer composition, codon usage, or GC content to measure\nthe similarity between viral and host sequences or to other viruses\nwith a known host. By identifying which viral genomes contain\nsequences matching a bacterium’s CRISPR spacers, researchers can\ninfer potential virus-host interactions. However, this approach is\nlimited by the set of known CRISPR spacers.\nAs a comparison, predicting virus-host interactions based on\nk-mer matching and codon usage analysis is another powerful\napproach for identifying novel viral-bacterial interactions. Codon\nusage refers to the frequency with which different codons are used to\nencode amino acids in a genome. When a virus’s codon usage\nmatches that of its host, it suggests that the virus has evolved to\nefﬁciently exploit the host’s translational machinery, enhancing its\nability to replicate within that host. This provides critical\ninformation in predicting potential virus-host interactions. By\nperforming joint analysis of codon usage and other genomic\nfeatures, researchers can achieve more accurate predictions\nregarding which host species are susceptible to particular viruses.\nSince these genomic features are embedded in the viral or\nbacterial genomes, it is possible to learn these features\nautomatically using machine learning and AI methods. Liu D.\net al. (2023) developed evoMIL for predicting virus-host\nassociation at the species level from viral sequence only. They\nused datasets that were collected from the Virus-Host database\nVHDB, (https://www.genome.jp/virushostdb/), which contains a\nmanually curated set of known species-level virus-host\nassociations collated from a variety of sources, including public\ndatabases such as RefSeq, GenBank, UniProt, and ViralZone and\nevidence from the literature surveys (Liu D. et al., 2023). For each\nknown interaction, this database provides NCBI taxonomic ID for\nthe virus and host and the Refseq IDs for the virus genomes. The\nﬁnal data set includes 17,733 associations between 12,650 viruses\nand 3,740 hosts that were used to construct binary datasets for both\nprokaryotic and eukaryotic hosts. For each of the hosts, an evoMIL\nmodel is built to predict the possible interacting viruses.\nLiu D. et al. (2023)then applied the pre-trained ESM-1b model\nto transform protein sequences intoﬁxed-length embedding vectors,\nwhich serve as features for downstream binary and multi-class\nclassiﬁcation. Additionally, they applied multiple instance\nlearning (MIL) (Maron and Lozano-Pérez, 1997), where multiple\ninstances are grouped together with a single label, and are classiﬁed\nas a whole. They employed attention-based MIL (Ilse et al., 2018) for\neach host. Speciﬁcally, for each host, they collected the same number\nof positive and negative viruses, and then obtained embeddings of\nprotein sequences from viruses obtained by the pre-trained\ntransformer model ESM-1b. To handle the input length of the\nPLMs, they split the protein sequences of viruses to sub-\nsequences for generating embeddings. An attention-based MIL\nwas applied to train the model for each host dataset using the\nprotein feature matrices of viruses. The resulting models can be used\nto predict whether a new virus interacts with a host for which a\ncorresponding predictive model has been developed.\nIn addition to species-level virus-bacterium interaction\nprediction, Gaborieau et al. (2023) introduced a novel dataset\nand prediction model that focuses on phage-bacteria interactions\nat the strain level, utilizing genomic data of 403 natural,\nphylogenetically diverse, Escherichia strains and\n96 bacteriophages. Their ﬁndings highlight that bacterial surface\nstructures, such as lipopolysaccharides (LPS) and capsules, play a\ncritical role in determining these interactions. Speci ﬁcally, they\nidentiﬁed bacterial surface polysaccharides as key adsorption\nfactors that signi ﬁcantly enhance the accuracy of interaction\npredictions. This offers a valuable dataset for developing phage\ncocktails to combat emerging bacterial pathogens.\n5 Deep learning and language models\nfor prediction of biosynthetic\ngene clusters\nMicrobial secondary metabolites are chemical compounds that\nexhibit a broad range of functions and have great potential in\npharmaceutical applications, such as antimicrobial agents and\nanticancer therapies. These bioactive small molecules are usually\nencoded by clusters of genes along the bacterial genome known as\nBiosynthetic Gene Clusters (BGCs). Although accurate,\nFrontiers inGenetics frontiersin.org07\nYan et al. 10.3389/fgene.2024.1494474\nexperimental validation of BGCs is laborious and costly. High-\nthrougput sequencing techniques, alongside advanced genome\nassembly algorithms, have enabled people to access the vast\namount of bacterial genomic data. The genomic sequence data\nserves as a rich resource for BGCs mining, allowing researchers\nto better understand the functional potential of bacteria and discover\nnew secondary metabolites or natural products.\nMachine learning-based algorithms have been developed for the\ndetection of BGCs in microbial genomes. antiSMASH (Medema\net al., 2011) identiﬁes candidate BGCs through multiple sequence\nalignment based on the proﬁle hidden Markov model (pHMM)\nlibrary constructed from experimentally characterized signature\nprotein or protein domains, subsequently ﬁltering these\ncandidates using curated rules based on expert knowledge.\nPRISM (Skinnider et al., 2017) employs a similar approach by\nsearching through an HMM library. ClusterFinder (Cimermancic\net al., 2014) utilizes a hidden Markov-based probabilistic algorithm\nto identify known and unknown BGCs. Extending beyond these\nmethods, MetaBGC (Sugimoto et al., 2019) integrates segmented\npHMM with clustering strategies, making it possible to detect BGCs\ndirectly from metagenomic reads.\nDespite the success of existing machine learning-based\nalgorithms, traditional machine learning models cannot handle\nthe long-range dependencies between genome sequences and\ncannot transfer knowledge from other datasets, thereby resulting\nin a lower power of detecting the new BGCs. Several machine\nlearning frameworks, including those with transformer-type\nlanguage modeling architecture, have been developed speciﬁcally\nfor predicting bacterial BGCs. These models leverage advanced\ncomputational techniques to analyze genomic data and identify\nregions that encode for biosynthetic pathways. Many existing methods\nuse sequences of the protein family domains (Pfams) to characterize\nthe BGCs and bacterial genomics. Proteins are generally composed of\none or more functional regions, commonly termed domains. Different\ncombinations of domains give rise to the diverse range of proteins\nfound in nature. The identiﬁcation of domains that occur within\nproteins can therefore provide insights into their function.\n5.1 Deep learning methods for BGC\nprediction\nDeepBGC is a deep learning-based tool that uses a combination\nof convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) to predict and classify BGCs in bacterial\ngenomes. It processes raw genomic sequences to identify BGCs\nand provides detailed annotations of their functional components\n(Hannigan et al., 2019). e-DeepBGC further extended DeepBGC to\nincorporate functional description of protein family domains and to\nutilize the Pfam similarity database in data augmentation (Liu et al.,\n2022). Pfam also generates higher-level groupings of related entries,\nknown as clans. A clan is a collection of Pfam entries which are\nrelated by similarity of sequence, structure or proﬁle-HMM. Rios-\nMartinez et al. (2023) developed a deep learning model that\nleverages self-supervised learning to detect and classify BGCs in\nmicrobial genomes. This approach aims to improve the accuracy\nand efﬁciency of BGC identiﬁcation and predict the types of natural\nproducts they produce.\n5.2 BGC prediction based on\nlanguage models\nLai et al. (2023) introduced BGC-Prophet, a neural network\nmodel that leverages natural language processing (NLP) techniques\nto analyze genomic sequences as linguistic data, identifying patterns\nindicative of biosynthetic gene clusters (BGCs). This innovative\napproach enables the model to grasp the complex syntax and\nsemantics inherent in genetic sequences. The input to BGC-\nProphet consists of embeddings represented by 320-dimensional\nvectors, generated through ESM-2 (Lin et al., 2023). The model\narchitecture integrates convolutional neural networks (CNNs) with\ntransformer-based models, a hybrid design that effectively manages\nthe sequential nature of DNA data, thereby enhancing the accuracy\nof BGC detection and classi ﬁcation. Table 3 compares these\nmethods, highlighting the deep learning models and primary data\nsources used for training.\nFigure 2 illustrates the differences in BGC prediction when\nperformed at the Pfam levelversus the amino acid level. Positive\nsamples can be derived from segmenting amino acid sequences\nwithin biosynthetic gene clusters (BGCs) in the MIBiG database,\nwhile negative samples can be generated by randomly segmenting\nbacterial genomes, excluding sequences similar to known BGCs.\nProtein sequences can be obtained directly from genome datasets or\nannotated from genome sequences. For Pfam-level prediction,\nPfams are ﬁrst identi ﬁed along the protein sequences using\nbioinformatics tools, and embeddings for each Pfam are\ngenerated using Pfam2vec. For amino acid-level prediction, pre-\ntrained protein language models such as ProtBert-BFD embeddings\n(Elnaggar et al., 2022) are employed to embed the segmented amino\nacid sequences. Once these embeddings are obtained, deep learning\nmodels are applied to assign scores, indicating the probability that\neach embedding corresponds to a BGC.\n6 Public knowledge integration in\nmicrobiome studies with LLMs\nDue largely to the rapid development and growth of\nmetagenomics research in the last 2 decades, it is well established\nthat the human microbiome is associated with overall human-host\nhealth. Many of the ﬁndings that link the gut microbiome to\ncomplex diseases, such as IBD and Crohn ’s Disease, can be\nfound within individual scienti ﬁc publications. Manual\naggregation of these results, available in the public domain, into\nan organized and searchable repository would be time-prohibitive\nand limited to only a small subject of microbes and diseases (Badal\net al., 2019). Such knowledge bases can be used for downstream\nanalysis and discovery. NLP and text mining approaches can be used\nto automate this process.\nAutomated extraction of microbiome-disease associations from\nscientiﬁc text requires three steps. First is to identify the disease and\nmicrobe(s) mentioned in the text. This is known as entity extraction,\nwhere the entity is either the disease or microbe. Well-established\nalgorithms such as Named entity Recognizers (NERs) and linguistic\ntaggers can be used for this process. The second step is relationship\nextraction which aims to establish the existence of a relationship\nbetween a pair of entities (i.e., microbe-disease pair). Theﬁnal step is\nFrontiers inGenetics frontiersin.org08\nYan et al. 10.3389/fgene.2024.1494474\nto reﬁne the categorization of identiﬁed relationships into positive or\nnegative associations. Several statistical models have been developed\nfor relationship extraction. While each step requires the use of NLP\nalgorithms, the integration of deep learning and LMMs into steps\ntwo and three are of particular interest recently.\nAn early example of using deep learning in relationship\nextraction comes from Wu et al. (2021) . In this work, the\nauthors apply a pretrained BERE model to identify microbe-\ndisease associations. BERE is a deep learning model initially\ndeveloped for extracting drug-related associations (Hong et al.,\n2020). The model is pretrained using a biomedical corpus. The\nmodel converts the text into vector representation using word\nembeddings with sentences represented as 200-dimensional\nconcatenations. Then the recurrent neural network encodes\nshort- and long-range dependencies, as well as semantic features\nusing gated recurrent units (GRUs). Finally, a classiﬁer performs\nprediction. The prediction task has four possible labels: positive in\nwhich the microbe’s presence will increase when disease occurs,\nnegative in which the microbe’s presence will decrease when the\ndisease occurs, relate when the microbe-disease pair occurs together\nbut the relationship cannot be determined, and NA when there is no\nrelationship description in the text. The model requires a large\namount of training data. Although, the gold standard of manual\ncuration is difﬁcult and costly. The authors implement a transfer\nlearning silver standard corpus, learned with automated tools but\npotentially with error,ﬁrst and thenﬁne tune with the gold standard\nmanually curated corpus. This transfer learning approach results in\na reduction in the error rate.\nDeep learning models like the one just described have been\nrecently reﬁned to use LLMs like GPT-3 and BERT (Karkera et al.,\n2023). The principal advantage of using LLMs in this setting is that\nthey reduce the requirement for large amounts of training data,\ngiven that they are already pretrained with large amounts of text.\nThe setting where noﬁne-tuning or training data is used is known as\nTABLE 3 Deep learning methods for BGC prediction.\nAlgorithm Model Pretraining Level Primary source data\nDeepBGC BiLSTM No Pfam BGCs from Cimermancic et al. (2014)+ MIBiG\ne-DeepBGC BiLSTM No Pfam MIBiG Medema et al. (2015)\nBiGCARP ByteNet Yes Pfam antiSMASH Blin et al. (2019)+ MIBiG\nBGC-Prophet Transformer Yes Amino acids GTDB Parks et al. (2022)+ MIBiG\nFIGURE 2\nA comparison of BGC prediction based on pfam2vec embedding for Pfam level prediction and embedding based on PLMs for amino acid level\nprediction.\nFrontiers inGenetics frontiersin.org09\nYan et al. 10.3389/fgene.2024.1494474\nzero-shot learning. Karkera et al. (2023) uses the same positive,\nnegative, relate, and NA labels asWu et al. (2021)with their LLMs\nand ﬁnd that zero- and few-shot learners do not perform very well,\nparticularly with the NA label. Thus indicating that out-of-the-box\nimplementation of LLMs for identifying microbe-disease\nassociations is limited. The performance of generative (e.g., GPT-\n3) and discriminative (e.g., BERT) models improve withﬁne-tuning.\nThe amount of improvement is strongly dependent on the quality of\ntraining data.\n7 Discussion\nThe recent development of deep learning methods, and large\nlanguage models in particular, has led to many novel applications\nthat address signiﬁcant challenges in microbiome and metagenomic\nresearch. In this paper, we have reviewed the latest applications of\nthese methods in microbial function analysis, including the\nidentiﬁcation of biosynthetic gene clusters in bacterial genomes,\nannotation of virome genomes, and prediction of virus-bacteria\ninteractions. We have also explored the use of generic LLMs, such as\nChatGPT, for extracting microbe-disease associations from public\nknowledge. We discuss challenges and future directions below.\n7.1 Data underrepresentation, scarcity, and\nquality issues\nThere still remain signi ﬁcant portions of microbial taxa,\nfunctional elements, ecologies, and environments that are poorly\ncharacterized, annotated, or cultured. These will necessarily lack\nrepresentations in the“training data” databases for AI-based model.\nTo further advance this promising research area, it is essential to\nfocus on both the collection and annotation of datasets from\nmultiple sources. The integration of diverse datasets — ranging\nfrom genomic sequences to environmental metadata — will\nprovide a more comprehensive understanding of microbial\ncommunities and their interactions. However, this requires\nmeticulous data curation, standardization, and the creation of\nlarge, well-annotated datasets that can serve as benchmarks for\ntraining and evaluating deep learning models. Speciﬁcally, for each\nresearch area as covered in this review:\n DNA, protein, and genomic language models. Models will\nnaturally prioritize microbes and microbial genetic elements\nfrom well-studied environments and conditions (e.g., the\nhuman gut). On the other hand, current research has also\ndemonstrated microbiome genome language models are\ncapable of traversing genomic and protein spaces that are\nentirely unexplored by existing microbiome research. For\nexample, ProtGPT2 shows that it produces not only\nchallenging targets but also previously unreported\ntopologies ( Ferruz et al., 2022 ). As such, microbiome\nlanguage models hold the promise to at least partially\ncover under-characterized microbes and genes.\n BGC identiﬁcation. While the models have demonstrated\nsigniﬁcant potential, this area still faces notable challenges\nfrom data limitation. The largest experimentally validated\nBGCs database, MiBIG 3.0, contains approximately\n2,500 entries, which is relatively small for training the AI\nmodels. To address this issue,Rios-Martinez et al. (2023)\nexpanded the dataset by using the predicted BGCs from\nantismash. However, the model ’s performance may be\naffected by the accuracy of the predicting algorithm and\nis subject to prediction bias. Moreover, most validated BGCs\nbelong to Polyketide and Nonribosomal peptide classes,\nleaving the rest of BGC classes underrepresented. The\nimbalance in the training set may lead to less prediction\npower for less-characterized BGC types. Lastly, there is no\nuniversally accepted approach for constructing negative\nsamples (non-BGC sequences) for training. Ideally, the\nnegative samples should resemble true BGCs while\navoiding false positives. The arbitrarily constructed\nnegative samples may also affect the model performance.\nAddressing these issues of data limitation is crucial for\nadvancing AI-driven BGC discovery and ensuring more\naccurate and robust predictions across diverse BGC classes.\n Virome. Unlike bacterial or eukaryotic genomes, viral\ngenome annotations are limited by the lack of\ncomprehensive and high-quality reference databases. This\nhampers the ability of language models to learn meaningful\nrepresentations for virome data. In addition, viruses evolve\nrapidly, leading to highly divergent sequences even within\nclosely related taxa. This makes it challenging for language\nmodels to effectively model and predict conserved\nfunctional elements or interactions. The NIH Human\nVirome program is expected to generate a large set of\nvirome sequences to characterize the human virome in\nlongitudinal, diverse cohorts across the lifespan, which\ncan be used to develop virome-speciﬁc models. It is also\npossible to leverage protein structure predictions (e.g.,\nAlphaFold) alongside sequence-based language models to\nimprove virome functional annotations and virome-bacteria\ninteraction predictions.\n Public knowledge integration. The automated extraction is\nheavily inﬂuenced by the quality of training data. BothWu\net al. (2021)and Karkera et al. (2023)note that predictive\naccuracy of such language models is tied to the quality of\ntraining data. Therefore, the existence of high quality gold\nstandard corpus for training and ﬁne-tuning are key to\nmodel performance.\n7.2 Evaluation, interpretation, and validation\nof ﬁndings\nDownstream interpretation and validation ofﬁndings are vital\nto translate the progress made with microbiome AI research into\nbiological and clinical progress. This can be facilitated by, ﬁrst,\nbenchmarking AI models against existing data resources. For\nexample, publicly available, high-quality microbiome cohorts,\nsuch as those constructed under the Human Microbiome Project\n(Integrative, 2014) and the American Gut Project (McDonald et al.,\n2018), should serve as real-world “silver standards” to compare\nrecent AI models and gauge their capabilities to generate novel\ninsights in a meaningful application setting. There have also been\nFrontiers inGenetics frontiersin.org10\nYan et al. 10.3389/fgene.2024.1494474\npreliminary efforts in assembling and curating computational\nbenchmarking tasks, such as those fromZhou et al. (2023) and\nMarin et al. (2023) . These resources aim to compile realistic\nbiological tasks for analysis of genomes and metagenomes with\nknown ground truth, thus facilitating fair and meaningful\ncomparison between AI models. However, given the nascent\nnature of the ﬁeld, such efforts mostly focus on tasks related to\nthe human genome. In the future, we anticipate the development of\nsimilar benchmarking resources specialized for metagenomics\nlanguage models.\nSecond, wet-lab-based validation of new AI model discoveries is\nnecessary, which can be realized through biochemistry-based or\nmodel-system-based evaluation approaches. For example, ProGen\ntested 100 AI-generated novel gene sequences (sufﬁciently deviating\nfrom known protein space) with cell-free synthesis and validated\ntheir bioactivity via substrate binding andﬂuorescence responses\n(Madani et al., 2023). MetaBGC puriﬁed and solved the structures of\nﬁve new type II polyketide molecules as the products of\ncharacterized BGCs, two of which exhibited strong antibacterial\nactivities (Sugimoto et al., 2019 ). In the future, we anticipate\nincreasing cross-disciplinary collaborations to facilitate such\npractices, and in particular, the development of standardized\nprotocols for validating AI-generated ﬁndings in real\nbiological systems.\n7.3 Other future directions\n7.3.1 Multi-domain integration\nThe natural large language model research has made striking\nprogress towards multi-domain data integration, spanning data\nmodalities such as texts, images, video, and audio (Wang et al.,\n2024). For metagenomic AI research, we anticipate the integration\nacross data domains will become a similar crucial area for future\nresearch. This involves both integrating across multi-omics data\nmodalities (e.g., metatranscriptomics, proteomics, metabolomics,\nhost genetics) with metagenomics data, and the integration of\ncomplex biological nuances of microbiome data based on existing\nknowledge (interaction of microbes with host genetics, the\nenvironment, and among themselves). Successful LLM techniques\nsuch as knowledge graph integration (Pan et al., 2024) and retrieval-\naugmented generation ( Zhao et al., 2024 ) can be potentially\ntransferred for integration tasks in metagenomics AI models.\nRegardless, the capability of large-scale AI models promise their\npotential to integrate across diverse microbiome data types and\nexisting knowledge, providing a more holistic understanding of\nmicrobial functions and interactions.\n7.3.2 Computation and model development\nGiven the model scales and the size of their training data,\ncomputations related to the development of new genomic\nlanguage models can become prohibitive in order to achieve\ndesirable model accuracy. As an example, ESM2 reported that\ntheir largest sized model (15 billion parameters) took 60 days to\ntrain over 512 NVIDIA V100 GPUs (Lin et al., 2023). We anticipate\nfuture research will develop ef ﬁcient techniques to improve\ncomputational performance, especially for adapting pre-trained\nmodels towards domain-speciﬁc tasks (i.e., ﬁne-tuning). To this\nend, recently efﬁcient parameter update techniques such as adapter\ntuning (Houlsby et al., 2019) and Low-Rank Adaption [LoRA,Hu\net al. (2021)] hold promises. For example, DNABERT-2 adopted\nLoRA to efﬁciently update the model parameters during itsﬁne-\ntuning stage (Zhou et al., 2023). On the architectural front, there is a\nneed to design models that can handle the unique challenges posed\nby microbiome and metagenomic data, such as high dimensionality,\nsparsity, and complex relationships between microbial species.\nInnovations in model architectures, such as graph neural\nnetworks, attention mechanisms, and hierarchical models, could\nplay a crucial role in capturing the intricate dependencies within the\ndata. Moreover, these models should be adaptable to the evolving\nnature of the datasets, allowing for continuous learning and\nreﬁnement as new data becomes available.\nAuthor contributions\nBY: Writing–original draft, Writing–review and editing. YN:\nWriting–original draft, Writing –review and editing. LL:\nWriting–original draft, Writing –review and editing. RD:\nWriting–original draft, Writing –review and editing. HL:\nWriting–original draft, Writing –review and editing. SM:\nWriting–original draft, Writing–review and editing.\nFunding\nThe author(s) declare thatﬁnancial support was received for the\nresearch, authorship, and/or publication of this article. This project\nis supported in part by R01GM123056 (HL) and\nU24OD035523 (SM).\nAcknowledgments\nChatGPT-4o was used to proofread the manuscript.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nFrontiers inGenetics frontiersin.org11\nYan et al. 10.3389/fgene.2024.1494474\nReferences\nBadal, V. D., Wright, D., Katsis, Y., Kim, H. C., Swafford, A. D., Knight, R., et al.\n(2019). Challenges in the construction of knowledge bases for human microbiome-\ndisease associations. Microbiome 7 (1), 129. doi:10.1186/s40168-019-0742-2\nBlin, K., Pascal Andreu, V., de los Santos, E. L. C., Del Carratore, F., Lee, S. Y.,\nMedema, M. H., et al. (2019). The antismash database version 2: a comprehensive\nresource on secondary metabolite biosynthetic gene clusters. Nucleic acids Res. 47,\nD625-D630–D630. doi:10.1093/nar/gky1060\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., et al. (2020).\nLanguage models are few-shot learners.arXiv Prepr. arXiv:2005.14165. doi:10.48550/\narXiv.2005.14165\nChang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., et al. (2024). A survey on\nevaluation of large language models.ACM Trans. Intelligent Syst. Technol.15, 1–45.\ndoi:10.1145/3641289\nCimermancic, P., Medema, M. H., Claesen, J., Kurita, K., Brown, L. C. W.,\nMavrommatis, K., et al. (2014). Insights into secondary metabolism from a global\nanalysis of prokaryotic biosynthetic gene clusters.Cell 158, 412–421. doi:10.1016/j.cell.\n2014.06.034\nDalla-Torre, H., Gonzalez, L., Mendoza-Revilla, J., Lopez Carranza, N.,\nGrzywaczewski, A. H., Oteri, F., et al. (2024). Nucleotide transformer: building and\nevaluating robust foundation models for human genomics.Nat. Methods. doi:10.1038/\ns41592-024-02523-z\nde Almeida, B. P., Dalla-Torre, H., Richard, G., Blum, C., Hexemer, L., Gélard, M.,\net al. (2024). Segmentnt: annotating the genome at single-nucleotide resolution with dna\nfoundation models. bioRxiv. 03. doi:10.1101/2024.03.14.584712\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: pre-training of deep\nbidirectional transformers for language understanding.arXiv Prepr. arXiv:1810.04805.\ndoi:10.48550/arXiv.1810.04805\nDion, M.-B., Plante, P.-L., Zufferey, E., Shah, S. A., Corbeil, J., and Moineau, S. (2021).\nStreamlining crispr spacer-based bacterial host predictions to decipher the viral dark\nmatter. Nucleic Acids Res.49, 3127–3138. doi:10.1093/nar/gkab133\nDuan, C., Zang, Z., Xu, Y., He, H., Liu, Z., Song, Z., et al. (2024). Fgbert: function-\ndriven pre-trained gene language model for metagenomics.arXiv Prepr.doi:10.48550/\narXiv.2402.16901\nElnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G., Wang, Y., Jones, L., et al. (2022).\nProttrans: toward understanding the language of life through self-supervised learning.\nIEEE Trans. Pattern Analysis Mach. Intell.44, 7112–7127. Epub 2022 Sep 14. doi:10.\n1109/TPAMI.2021.3095381\nFerruz, N., Schmidt, S., and Höcker, B. (2022). Protgpt2 is a deep unsupervised\nlanguage model for protein design.Nat. Commun. 13, 4348. doi:10.1038/s41467-022-\n32007-7\nFlamholz, Z. N., Biller, S. J., and Kelly, L. (2024). Large language models improve\nannotation of prokaryotic viral proteins. Nat. Microbiol. 9, 537–549. doi:10.1038/\ns41564-023-01584-8\nGaborieau, B., Vaysset, H., Tesson, F., Charachon, I., Dib, N., Bernier, J., et al. (2023).\nPredicting phage-bacteria interactions at the strain level from genomes.bioRxiv. doi:10.\n1101/2023.11.22.567924\nHannigan, G. D., Prihoda, D., Palicka, A., Soukup, J., Klempir, O., Rampula, L., et al.\n(2019). A deep learning genome-mining strategy for biosynthetic gene cluster\nprediction. Nucleic acids Res.47, e110. doi:10.1093/nar/gkz654\nHernández Medina, R., Kutuzova, S., Nielsen, K. N., Johansen, J., Hansen, L. H.,\nNielsen, M., et al. (2022). Machine learning and deep learning applications in\nmicrobiome research. ISME Commun. 2, 98. doi:10.1038/s43705-022-00182-9\nHong, L., Lin, J., Li, S., Wan, F., Yang, H., Jiang, T., et al. (2020). A novel machine\nlearning framework for automated biomedical relation extraction from large-scale\nliterature repositories. Nat. Mach. Intell.2, 347–355. doi:10.1038/s42256-020-0189-y\nHorvath, P., and Barrangou, R. (2010). Crispr/cas, the immune system of bacteria and\narchaea. Science 327, 167–170. doi:10.1126/science.1179555\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo,\nA., et al. (2019). “Parameter-efﬁcient transfer learning for NLP,” in International\nconference on machine learning (PMLR), 2790 –2799. Available at: https://\nproceedings.mlr.press/v97/houlsby19a.html.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., et al. (2021). Lora: low-\nrank adaptation of large language models.arXiv Prepr. arXiv:2106.09685. doi:10.48550/\narXiv.2106.09685\nHwang, Y., Cornman, A. L., Kellogg, E. H., Ovchinnikov, S., and Girguis, P. R. (2024).\nGenomic language model predicts protein co-regulation and function.Nat. Commun.\n15, 2880. doi:10.1038/s41467-024-46947-9\nIlse, M., Tomczak, J., and Welling, M. (2018). “Attention-based deep multiple\ninstance learning, ” in International Conference on machine learning (PMLR),\nproceedings of machine learning research,2 1 2 7–2136.\nIntegrative, H. (2014). The integrative human microbiome project: dynamic analysis\nof microbiome-host omics proﬁles during periods of human health and disease.Cell\nhost and microbe16, 276–289. doi:10.1016/j.chom.2014.08.014\nJi, Y., Zhou, Z., Liu, H., and Davuluri, R. V. (2021). Dnabert: pre-trained bidirectional\nencoder representations from transformers model for dna-language in genome.\nBioinformatics 37, 2112–2120. doi:10.1093/bioinformatics/btab083\nJin, M., Xue, H., Wang, Z., Kang, B., Ye, R., Zhou, K., et al. (2024). Prollm: protein\nchain-of-thoughts enhanced llm for protein-protein interaction prediction. bioRxiv\n(2024–04). doi:10.48550/arXiv.2405.06649\nKarkera, N., Acharya, S., and Palaniappan, S. K. (2023). Leveraging pre-trained\nlanguage models for mining microbiome-disease relationships.BMC Bioinforma. 24,\n290. doi:10.1186/s12859-023-05411-z\nKarollus, A., Hingerl, J., Gankin, D., Grosshauser, M., Klemon, K., and Gagneur, J.\n(2024). Species-aware dna language models capture regulatory elements and their\nevolution. Genome Biol. 25, 83. doi:10.1186/s13059-024-03221-x\nLai, Q., Yao, S., Zha, Y., Zhang, H., Ye, Y., Zhang, Y., et al. (2023). Deciphering the\nbiosynthetic potential of microbial genomes using a bgc language processing neural\nnetwork model. bioRxiv, 2023–2111. biorxiv preprints.\nLi, L., Zhou, J., Gao, Z., Hua, W., Fan, L., Yu, H., et al. (2024). A scoping review of\nusing large language models (llms) to investigate electronic health records (ehrs).arXiv\nPrepr. arXiv:2405.03066. doi:10.48550/arXiv.2405.03066\nLigeti, B., Szepesi-Nagy, I., Bodnár, B., Ligeti-Nagy, N., and Juhász, J. (2024). Prokbert\nfamily: genomic language models for microbiome applications.Front. Microbiol. 14,\n1331233. doi:10.3389/fmicb.2023.1331233\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., et al. (2023). Evolutionary-scale\nprediction of atomic-level protein structure with a language model. Science 379,\n1123–1130. doi:10.1126/science.ade2574\nLiu, D., Young, F., Robertson, D. L., and Yuan, K. (2023a). Prediction of virus-host\nassociation using protein language models and multiple instance learning.bioRxiv 2023,\n04. doi:10.1101/2023.04.07.536023\nLiu, M., Li, Y., and Li, H. (2022). Deep learning to predict the biosynthetic gene\nclusters in bacterial genomes.J. Mol. Biol.434, 167597. doi:10.1016/j.jmb.2022.167597\nLiu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., et al. (2023b). Summary of\nchatgpt-related research and perspective towards the future of large language models.\nMeta-Radiology.100017. doi:10.1016/j.metrad.2023.100017\nMa, Y., Guo, Z., Xia, B., Zhang, Y., Liu, X., Yu, Y., et al. (2022). Identiﬁcation of\nantimicrobial peptides from the human gut microbiome using deep learning.Nat.\nBiotechnol. 40, 921–931. doi:10.1038/s41587-022-01226-0\nMadani, A., Krause, B., Greene, E. R., Subramanian, S., Mohr, B. P., Holton, J. M., et al.\n(2023). Large language models generate functional protein sequences across diverse\nfamilies. Nat. Biotechnol. 41, 1099–1106. doi:10.1038/s41587-022-01618-2\nMardikoraem, M., Wang, Z., Pascual, N., and Woldring, D. (2023). Generative models\nfor protein sequence modeling: recent advances and future directions. Brieﬁngs\nBioinforma. 24, bbad358. doi:10.1093/bib/bbad358\nMarin, F. I., Teufel, F., Horlacher, M., Madsen, D., Pultz, D., Winther, O., et al. (2023).\n“Bend: benchmarking dna language models on biologically meaningful tasks,” in The\ntwelfth international conference on learning representations.\nMaron, O., and Lozano-Pérez, T. (1997). “A framework for multiple-instance\nlearning,” in Advances in neural information processing systems,1 0 .\nMcDonald, D., Hyde, E., Debelius, J. W., Morton, J. T., Gonzalez, A., Ackermann, G.,\net al. (2018). American gut: an open platform for citizen science microbiome research.\nMsystems 3, e00031-18–e01128. doi:10.1128/mSystems.00031-18\nMedema, M. H., Blin, K., Cimermancic, P., De Jager, V., Zakrzewski, P., Fischbach, M.\nA., et al. (2011). antismash: rapid identiﬁcation, annotation and analysis of secondary\nmetabolite biosynthesis gene clusters in bacterial and fungal genome sequences.Nucleic\nacids Res. 39, W339–W346. doi:10.1093/nar/gkr466\nMedema, M. H., Kottmann, R., Yilmaz, P., Cummings, M., Biggins, J. B., Blin, K., et al.\n(2015). Minimum information about a biosynthetic gene cluster.Nat. Chem. Biol.11,\n625–631. doi:10.1038/nchembio.1890\nNijkamp, E., Ruffolo, J. A., Weinstein, E. N., Naik, N., and Madani, A. (2023).\nProgen2: exploring the boundaries of protein language models.Cell Syst.14, 968–978.e3.\ndoi:10.1016/j.cels.2023.10.002\nPan, S., Luo, L., Wang, Y., Chen, C., Wang, J., and Wu, X. (2024). Unifying large\nlanguage models and knowledge graphs: a roadmap.IEEE Trans. Knowl. Data Eng.36,\n3580–3599. doi:10.1109/tkde.2024.3352100\nParks, D. H., Chuvochina, M., Rinke, C., Mussig, A. J., Chaumeil, P.-A., and\nHugenholtz, P. (2022). Gtdb: an ongoing census of bacterial and archaeal diversity\nthrough a phylogenetically consistent, rank normalized and complete genome-based\ntaxonomy. Nucleic acids Res.50, D785–D794. doi:10.1093/nar/gkab776\nPeng, C., Shang, J., Guan, J., Wang, D., and Sun, Y. (2024). Viralm: empowering virus\ndiscovery through the genome foundation model.bioRxiv 40, btae704. 2024–01. doi:10.\n1093/bioinformatics/btae704\nRichardson, L., Allen, B., Baldi, G., Beracochea, M., Bileschi, M. L., Burdett, T., et al.\n(2023). Mgnify: the microbiome sequence data analysis resource in 2023.Nucleic Acids\nRes. 51, D753–D759. doi:10.1093/nar/gkac1080\nFrontiers inGenetics frontiersin.org12\nYan et al. 10.3389/fgene.2024.1494474\nRios-Martinez, C., Bhattacharya, N., Amini, A. P., Crawford, L., and Yang, K. K. (2023).\nDeep self-supervised learning for biosynthetic gene cluster detection and product\nclassiﬁcation. PLOS Comput. Biol.19, e1011162. doi:10.1371/journal.pcbi.1011162\nRives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., et al. (2021). Biological structure\nand function emerge from scaling unsupervised learning to 250 million protein\nsequences. Proc. Natl. Acad. Sci.118, e2016239118. doi:10.1073/pnas.2016239118\nRoux, S., Enault, F., Hurwitz, B. L., and Sullivan, M. B. (2015). Virsorter: mining viral\nsignal from microbial genomic data.PeerJ 3, e985. doi:10.7717/peerj.985\nShang, J., Tang, X., and Sun, Y. (2023). Phatyp: predicting the lifestyle for\nbacteriophages using bert.Brieﬁngs Bioinforma.24, bbac487. doi:10.1093/bib/bbac487\nSkinnider, M. A., Merwin, N. J., Johnston, C. W., and Magarvey, N. A. (2017). Prism 3:\nexpanded prediction of natural product chemical structures from microbial genomes.\nNucleic acids Res.45, W49-W54–W54. doi:10.1093/nar/gkx320\nS t a a l s ,R .H .J . ,a n dB r o u n s ,S .J .J .( 2 0 1 3 ) .“Distribution and mechanism of the type i crispr-\ncas systems,” in CRISPR-cas systems(Springer), 145–169. doi:10.1007/978-3-642-34657-6_8\nSugimoto, Y., Camacho, F. R., Wang, S., Chankhamjon, P., Odabas, A., Biswas, A.,\net al. (2019). A metagenomic strategy for harnessing the chemical repertoire of the\nhuman microbiome. Science 366, eaax9176. doi:10.1126/science.aax9176\nSuzek, B. E., Huang, H., McGarvey, P., Mazumder, R., and Wu, C. H. (2007). Uniref:\ncomprehensive and non-redundant uniprot reference clusters. Bioinformatics 23,\n1282–1288. doi:10.1093/bioinformatics/btm098\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need.Adv. neural Inf. Process. Syst.30. doi:10.48550/arXiv.\n1706.03762\nW a n g ,J . ,J i a n g ,H . ,L i u ,Y . ,M a ,C . ,Z h a n g ,X . ,P a n ,Y . ,e ta l .( 2 0 2 4 ) .A\ncomprehensive review of m ultimodal large language models: performance and\nchallenges across different tasks. arXiv Prepr. arXiv:2408.01319 . doi:10.48550/\narXiv.2408.01319\nWu, C., Xiao, X., Yang, C., Chen, J., Yi, J., and Qiu, Y. (2021). Mining microbe–disease\ninteractions from literature via a transfer learning model.BMC Bioinforma.22 (1), 432.\ndoi:10.1186/s12859-021-04346-7\nZhao, P., Zhang, H., Yu, Q., Wang, Z., Geng, Y., Fu, F., et al. (2024). Retrieval-\naugmented generation for ai-generated content: a survey. arXiv preprint arXiv:\n2402.19473\nZhou, Z., Ji, Y., Li, W., Dutta, P., Davuluri, R. V., and Liu, H. (2023). DNABERT-2:\nefﬁcient and effective foundation model for multi-species genome.arXiv preprint arXiv:\n2306.15006. doi:10.48550/arXiv.2306.15006\nFrontiers inGenetics frontiersin.org13\nYan et al. 10.3389/fgene.2024.1494474",
  "topic": "Microbiome",
  "concepts": [
    {
      "name": "Microbiome",
      "score": 0.7018260359764099
    },
    {
      "name": "Computational biology",
      "score": 0.4958744943141937
    },
    {
      "name": "Computer science",
      "score": 0.47134682536125183
    },
    {
      "name": "Deep learning",
      "score": 0.4536705017089844
    },
    {
      "name": "Data science",
      "score": 0.35094496607780457
    },
    {
      "name": "Biology",
      "score": 0.33528923988342285
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3226243555545807
    },
    {
      "name": "Bioinformatics",
      "score": 0.30496490001678467
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I79576946",
      "name": "University of Pennsylvania",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I901861585",
      "name": "Vanderbilt University Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2613432",
      "name": "University of South Florida",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I170201317",
      "name": "University of Pittsburgh",
      "country": "US"
    }
  ]
}