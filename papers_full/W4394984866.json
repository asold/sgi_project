{
  "title": "AlpaPICO: Extraction of PICO frames from clinical trial documents using LLMs",
  "url": "https://openalex.org/W4394984866",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4226846113",
      "name": "Ghosh, Madhusudan",
      "affiliations": [
        "Indian Association for the Cultivation of Science"
      ]
    },
    {
      "id": "https://openalex.org/A4226846112",
      "name": "Mukherjee, Shrimon",
      "affiliations": [
        "Indian Association for the Cultivation of Science"
      ]
    },
    {
      "id": null,
      "name": "Ganguly, Asmit",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": "https://openalex.org/A2752624873",
      "name": "Basuchowdhuri, Partha",
      "affiliations": [
        "Indian Association for the Cultivation of Science"
      ]
    },
    {
      "id": "https://openalex.org/A2747230134",
      "name": "Naskar Sudip Kumar",
      "affiliations": [
        "Jadavpur University"
      ]
    },
    {
      "id": "https://openalex.org/A3211370773",
      "name": "Ganguly, Debasis",
      "affiliations": [
        "University of Glasgow"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2088212219",
    "https://openalex.org/W2097110957",
    "https://openalex.org/W1629765770",
    "https://openalex.org/W6638703957",
    "https://openalex.org/W6735285355",
    "https://openalex.org/W6753768548",
    "https://openalex.org/W6682205165",
    "https://openalex.org/W6601952396",
    "https://openalex.org/W6767171284",
    "https://openalex.org/W1034374084",
    "https://openalex.org/W2160987310",
    "https://openalex.org/W6767306326",
    "https://openalex.org/W6796340768",
    "https://openalex.org/W6771994473",
    "https://openalex.org/W6770200002",
    "https://openalex.org/W6757609451",
    "https://openalex.org/W6796992055",
    "https://openalex.org/W6773104867",
    "https://openalex.org/W6757479430",
    "https://openalex.org/W6771065168",
    "https://openalex.org/W6770858627",
    "https://openalex.org/W6772300667",
    "https://openalex.org/W6797839492",
    "https://openalex.org/W3199733524",
    "https://openalex.org/W6797316239",
    "https://openalex.org/W2572043993",
    "https://openalex.org/W6854743113",
    "https://openalex.org/W3189623885",
    "https://openalex.org/W4364374320",
    "https://openalex.org/W2889198729",
    "https://openalex.org/W6874686548",
    "https://openalex.org/W6857807729",
    "https://openalex.org/W6797567487",
    "https://openalex.org/W6633714834",
    "https://openalex.org/W6752788575",
    "https://openalex.org/W2067284187",
    "https://openalex.org/W6642349764",
    "https://openalex.org/W6748634344",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W6801307165",
    "https://openalex.org/W2992824360",
    "https://openalex.org/W6859691304",
    "https://openalex.org/W6744568511",
    "https://openalex.org/W6748139664",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6796159650",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6803129872",
    "https://openalex.org/W6850471959",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6769263558",
    "https://openalex.org/W6752554729",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6847574335",
    "https://openalex.org/W2147994374",
    "https://openalex.org/W4238846128",
    "https://openalex.org/W2801356492",
    "https://openalex.org/W2148437670",
    "https://openalex.org/W4385570519",
    "https://openalex.org/W4247661885",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4285116827",
    "https://openalex.org/W4301409532",
    "https://openalex.org/W4387841511",
    "https://openalex.org/W3155340931",
    "https://openalex.org/W4365800096",
    "https://openalex.org/W4235810635",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1554540371",
    "https://openalex.org/W3211983836",
    "https://openalex.org/W49437421",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W2613995098",
    "https://openalex.org/W2964179635"
  ],
  "abstract": null,
  "full_text": "AlpaPICO: Extraction of PICO Frames from\nClinical Trial Documents Using LLMs\nMadhusudan Ghosh†⋆a, Shrimon Mukherjee†⋆a, Asmit Ganguly∗∗b, Partha\nBasuchowdhuria, Sudip Kumar Naskard, Debasis Gangulyc\n† The authors equally contributed to this paper.\n∗ Corresponding authors.\n∗∗ Work done during an internship at IACS.\naSchool of Mathematical and Computational Sciences, Indian Association for the Cultivation of\nScience\nbComputer Science and Engineering, Indian Institute of Technology, Patna\ncSchool of Computing Science, University of Glasgow\ndDepartment of Computer Science and Engineering, Jadavpur University\nAbstract\nIn recent years, there has been a surge in the publication of clinical trial reports,\nmaking it challenging to conduct systematic reviews. Automatically extracting\nPopulation, Intervention, Comparator, and Outcome (PICO) from clinical trial\nstudies can alleviate the traditionally time-consuming process of manually scru-\ntinizing systematic reviews. Existing approaches of PICO frame extraction in-\nvolves supervised approach that relies on the existence of manually annotated\nEmail addresses:madhusuda.iacs@gmail.com (Madhusudan Ghosh†⋆),\niacsshrimon@gmail.com (Shrimon Mukherjee†⋆),\nasmitganguly.personal@gmail.com (Asmit Ganguly∗∗),\npartha.basuchowdhuri@iacs.res.in (Partha Basuchowdhuri),\nsudip.naskar@gmail.com (Sudip Kumar Naskar),\nDebasis.Ganguly@glasgow.ac.uk (Debasis Ganguly)\nPreprint submitted to Methods Tuesday 17 th September, 2024\narXiv:2409.09704v1  [cs.CL]  15 Sep 2024\ndata points in the form of BIO label tagging. Recent approaches, such as In-\nContext Learning (ICL), which has been shown to be effective for a number of\ndownstream NLP tasks, require the use of labeled examples. In this work, we\nadopt ICL strategy by employing the pretrained knowledge of Large Language\nModels (LLMs), gathered during the pretraining phase of an LLM, to automati-\ncally extract the PICO-related terminologies from clinical trial documents in un-\nsupervised set up to bypass the availability of large number of annotated data\ninstances. Additionally, to showcase the highest effectiveness of LLM in oracle\nscenario where large number of annotated samples are available, we adopt the\ninstruction tuning strategy by employing Low Rank Adaptation (LORA) to con-\nduct the training of gigantic model in low resource environment for the PICO\nframe extraction task. More specifically, both of the proposed frameworks utilize\nAlpaCare as base LLM which employs both few-shot in-context learning and in-\nstruction tuning techniques to extract PICO-related terms from the clinical trial\nreports. We applied these approaches to the widely used coarse-grained datasets\nsuch as EBM-NLP, EBM-COMET and fine-grained datasets such as EBM-NLPrev\nand EBM-NLPh. Our empirical results show that our proposed ICL-based frame-\nwork produces comparable results on all the version of EBM-NLP datasets and\nthe proposed instruction tuned version of our framework produces state-of-the-\nart results on all the different EBM-NLP datasets. Our project is available at\nhttps://github.com/shrimonmuke0202/AlpaPICO.git.\nKeywords:\nLLM, Llama, Bio-Medical NER, In-Context Learning, Instruction Tuning, PICO\nframe extraction\n2\n1. Introduction\nIn the last few decades, the concept of Evidence-Based Medicine (EBM) has\ngarnered significant interest within the healthcare community. More specifically,\nEBM is a technique used by medical practitioners and healthcare professionals to\nguide their clinical decision-making regarding patient care by utilizing the high-\nest quality and most up-to-date research evidence available [1]. Additionally, meta\nanalysis is one of the necessary statistical technique in evidence synthesis litera-\nture to provide a sufficient number of necessary medical evidences by combining\nthe results of different research studies to determine the necessary action [2]. Meta\nanalysis is highly labor intensive and time-consuming process, due to the necessity\nfor manually scrutinizing an extensive number of research articles and extracting\npertinent information from them [3]. Recently, a general trend observed in the\nscientific literature of any discipline is that it grows at a rapid rate embracing\nnew theories. The steep rise of scientific publications makes it difficult to manu-\nally conduct evidence synthesis. The process of systematically reviewing clinical\ndata, including prescriptions and electronic health records, can be made simpler\nby automatically extracting relevant outcome terms. Previous research has not\nextensively explored the natural language processing (NLP)-based evidence syn-\nthesis literature due to the scarcity of annotated data [4, 5] to employ the relevant\nmachine learning approaches for extracting the important components such as\nParticipants/Populations (P), Interventions (I)/ Comparators (C) 1 and Outcomes\n(O) [9], also popularly known as PICO. To alleviate this challenge, Nye et. al. [6]\ndeveloped an EBM-NLP dataset where the dataset uses an arbitrary selection pro-\n1It is to be noted that I and C are very often merged together into just I [6, 7, 8].\n3\ncess for outcome label annotations [10], and later on, revised datasets such as\nEBM-COMET and EBM-NLP-revised (EBM-NLP rev) have been introduced in\nthis literature [11]. Additionally, to enhance the overall performance of the clini-\ncal trial task, the biomedical literature has witnessed a significant number of state-\nof-the-art (SOTA) pretrained models (PLMs) such as BNER [12, 13] to conduct\nthe biomedical named entity recognition task. However, such language models\noften struggle due to the scarcity of extensive annotated data instances. Moreover,\nthe biomedical literature has seen a proliferation of publications aiming to lever-\nage the pretrained knowledge of large language models (LLMs), gathered during\nthe pretraining phase, such as PMC-Llama [14], BioMedGPT-LM [15], etc., by\nfine-tuning them on domain specific tasks.\nRecently, AlpaCare [16] applied finetuning strategy on all the layers of gigan-\ntic Llama 2 chat model using a newly constructed medical instruction response\ndataset MedInstruct-52k. However, this technique is highly resource intensive and\ntime consuming due to the update process of larger number of parameters than that\nof in Llama 2 chat model. The main problem with this approach is that conducting\ndomain specific training of these generative models is highly resource intensive\nand time consuming. To bridge this gap, we apply a novel in-context learning\n(ICL) framework where an additional annotated context is provided to a LLM is\nshown to be effective for downstream PICO frame extraction task by bypassing\nthe additional training process required in supervised setup. The context supplied\nis a set of annotated sentences, extracted from a relevant training corpus avail-\nable for this downstream PICO frame extraction task. Additionally, in contrast to\nthe existing supervised approaches [17, 18], by considering the PICO frame ex-\ntraction as sequence classification task, we utilize parameter efficient finetuning\n4\nPopulation\nMet with my\npsychiatrist... I\nsuffer from\ndepression we\ndiscussed\nabout...\nMet with my\npsychiatrist... I\nsuffer from\ndepression we\ndiscussed\nabout...\nI was put on\nduloxetine\n.....\nI was put on\nduloxetine\n..... Intervention\nI don't feel happy\nor calm either.\nShe said it is a\nside effect of the\nduloxetine.   \nI don't feel happy\nor calm either.\nShe said it is a\nside effect of the\nduloxetine.   \nOutcome\nAlpaPICO\nFigure 1: Example of PICO frame extraction using our proposed framework AlpaPICO.\nHere we pass documents to our proposed framework for extracting PICO frames from the\nclinical trial documents.\nstrategy (PEFT) specifically by utilizing the low rank adaptation module (LoRa)\nto finetune AlpaCare on PICO frame generation task in low resource scenario.\nTo the best of our knowledge, we are the first one to investigate the feasibility of\napplying both incontext learning (ICL) and instruction tuning strategies for PICO\nframe extraction task. The overall workflow of our framework has been shown in\nFigure 1.\n1.1. Our Contributions\nTo summarize, the following are our contributions in this paper.\n1. To the best of our knowledge, we are first in exploring the potential of em-\nploying an ICL-based framework to perform the downstream PICO frame\nextraction task from biomedical literature by utilizing the pretrained knowl-\nedge of LLM, which effectively omits the entire training process of a super-\n5\nvised setup.\n2. Our empirical results clearly demonstrate that our k-shot contexts in ICL-\nbased framework significantly enhances the performance ofk-shot ICL frame-\nwork, in compare to the zero-shot scenario where the need for training is\ncompletely eliminated.\n3. We also employ instruction tuning based approach to conduct the PICO\nframe extraction task on both the fine-grained and coarse-grained datasets\navailable in EBM-NLP literature.\n2. Related Work\nOur research focuses on examining the deep learning-based entity extrac-\ntion approaches from the scientific literature. In this section, we breifly describe\nthe state-of-the-art methodologies employed for named-entity recognition (NER)\ntasks in scientific domain.\nGeneral NER Techniques. NER is a very popular conventional subtask of in-\nformation extraction in the NLP domain. Many techniques have been developed\nand are still being used in this field over the years. Some of the most impor-\ntant and recent ones are contextual embedding based model [19], BiLSTM-CRF\n[20], CNN-based [21], Cross-BiLSTM-CNN and Att-BiLSTM [22], gated rela-\ntion network by capturing global context [23]. Xu et al. and Li et al. examined\nthe performance of nested NER [24, 25]. Numerous studies are conducted on the\njoint entities and relations extraction [26, 27, 28, 29, 30, 31], named entity nor-\nmalization. [32], and NER for low-resource languages [33, 34]. Researchers have\nworked on important variants of NER task, such as, document level NER using\n6\na multitask learning approach [35], NER with a multi-level topic-aware attention\nmechanism [36] and information extraction using a multi-modal approach [37].\nA popular application of NER is to extract scientific concept names from scien-\ntific literature [38, 39, 40]. Some researchers have extracted AI methodology,\nand components from AI domain [41, 42]. Recently, NER has been applied in\nbio-NLP and biomedical domains [43, 24, 25, 44]. Additionally, FLAIR [45], a\nneural framework, helps significantly to finetune existing PLMs on downstream\nNER task to yield SOTA results.\nEvidence-based Medical Entity Extraction. Extraction of PICO related termi-\nnologies from clinical trial text is an important area of research. According to\nearlier research, the entity extraction task from bio-medical literature was per-\nformed at the sentence level when a large number of annotated data instances were\nunavailable [46, 47]. Recent inventions of pre-trained language models (PLMs)\nsuch as ELMo [48], GPT [49], BERT [50], XLM [51] and XLMNet [52] etc.\nhelp significantly to mitigate the problem of limited annotated data samples. Such\nmodels also help to achieve state-of-the-art results on different NLP tasks includ-\ning named entity recognition [53, 54]. Some recent studies [18, 55, 11, 17] uti-\nlized PLMs for biological entity extraction task on the available datasets such\nas EBM-NLP [6] corpus and EBP-NLP rev [11]. The prior state-of-the-art mod-\nels [56, 17, 57, 58] on PICO entity recognition task performed poorly on the\nEBM-NLP corpus because it contains pharmaceutical intervention classes over\nnon-pharmaceutical ones. Small-scale annotation resulted poorly in PICO span\nextraction task from clinical trial literature. Later researchers used distantly su-\npervised datasets to overcome the problem of small annotated datasets [59, 60].\nAdditionally, the PICO-related entity extraction task was performed by breaking\n7\ndown the available entity classes into different binary classes [61].\nLarge Language Models and In-context Learning. Recently LLMs [62, 63, 64,\n65, 66] have obtained significant improvement on a variety of NLP tasks [67, 68,\n69, 70, 71]. The use of LLMs for downstream tasks can be divided into two cate-\ngories: firstly finetuning, and secondly in-context learning (ICL). In the finetuning\nstrategy, a pretrained model is initialized, and additional epochs are executed on\nthe downstream supervised data[72, 73, 74, 75]. In contrast to that, ICL based\nstrategy involves instructing LLMs to generate text based on few-shot demonstra-\ntions. Reformulating the first step of the downstream task involves incorporating\nprompts with demonstrations [76]. A systematic analysis of in-context learning\nframework was performed on various tasks by the GPT-3 model [62]. Chowdh-\nery et. al. [66] performed analysis for the NMT task on PaLM. Researchers have\nshown that better prompts and demonstrations lead to a performance boost for\nin-context learning [69, 77, 78]. Recently, in-context learning based techniques\nhave been used for NER task [79, 80, 81, 82].\nInstruction tuning. As a successful approach for customizing language models\nto handle diverse tasks, instruction tuning has garnered growing attention and en-\ngagement from the community. FLAN [83], T0 [84], and Tk-Instruct [85] trans-\nform extensive sets of pre-existing supervised learning datasets into an instruction-\nfollowing format. Subsequently, they finetune encoder-decoder models, demon-\nstrating robust zero-shot and few-shot performance across various NLP bench-\nmarks. Researchers utilized crowd-sourced high-quality instructional data to fine-\ntune GPT-3, transforming it into InstructGPT and improving its capacity to com-\nprehend user intentions and adhere to instructions [86]. Notably, recent progress\nin smaller models [87, 88, 89] has demonstrated task-following capabilities, achieved\n8\nthrough finetuning on instruction data generated by language models like Chat-\nGPT or GPT-4. Nevertheless, smaller models frequently encounter difficulties in\nproducing top-notch responses across various tasks [90]. A more detailed analysis\nof specific benchmarks exposes a significant disparity between these models and\nChatGPT [91]. The study conducted by [92] investigates instruction-tuning for\ninformation extraction tasks. Nevertheless, their approach heavily depends on su-\npervised datasets and demonstrates inferior performance compared to ChatGPT.\nSome works emphasized tuning models to excel at a specific type of task [93]. The\ndiversity in the instruction-tuning method is derived from task labels (e.g., relation\ntypes for relation extraction, entity types for NER), rather than instructions. By\nconcentrating on task-level capabilities and employing NER as a case study, it is\nshown that a tuning recipe can be devised, not only closing down the performance\ngap but also surpassing the performance of ChatGPT. In this study [16], the sig-\nnificance of task diversity in instruction tuning for the medical domain is demon-\nstrated. Comprehensive experiments are conducted to evaluate free-form instruc-\ntion in both medical and general domains. The results show that tuning AlpaCare\nwith a diverse medical self-instruct dataset can simultaneously improve its med-\nical capacity and generalization ability. Additionally, we introduce MedInstruct-\n52K, a diverse medical task dataset containing 52,000 instruction-response pairs,\nand MedInstruct-test, a set of novel medical tasks crafted by clinicians. These\ndatasets aim to facilitate the development and evaluation of future domain-specific\ninstruction-following models.\nIn this work, we are exploring an ICL-based framework to perform down-\nstream PICO framework extraction tasks from clinical trial documents by using\nthe knowledge of pretrained LLM, which omits the entire training process of a\n9\nsupervised setup. Additionally, we also perform an instruction tuning strategy to\nconduct the said task.\n3. Methodology\nIn this section, we discuss the task definition followed by our proposed frame-\nworks to conduct the downstream PICO frame extraction task.\n3.1. Overview of Sequence Labeling task\nGiven a clinical trial document ( D), our goal is to identify the entity spans\n(Es) as well as the corresponding categories (EPI/CO ) of the identified entities\nfrom the sentences D = (s1, s2, ...., sN ) of that particular document. In general,\nthis task is considered as a traditional sequence labeling problem such that, given\na sequence of words in any sentence si = ( w0, w1, ...., wN ) from a sentence si,\na supervised neural network model learns its parameters ( fθ) to map an input se-\nquence. Formally speaking, let s be a sentence of maximum lengthN in a training\nset T , i.e., s = {w0, . . . , wN−1}, where each wj is a token of that sentence. Let\nthe set of mentions of entities occurring in s be e(s) = {e0, . . . , en−1} ⊂ E,\nwhere E contains the collection of PICO spans annotated in the dataset. Each\nmention, e(s), is a subsequence of x, which we denote by an indicator sequence\nof positions {I(w0), . . . ,I(wN−1)}, where I(wi) = 1 if wi is a part of some en-\ntity e ∈ e(x). A continuous span {j, . . . , j+ n − 1} (where j ∈ ZM ) such that\nI(xi) = 1 , ∀j ≤ i ≤ j + n − 1 denotes an entity comprised of n tokens. To\ndistinguish the start of a span from its continuity and also its end, it is a com-\nmon practice to denote the label of the first element of such an index set with\nB (denoting Beginning of a span), the subsequent elements as I (denoting that\nthese are Inside a span) and the first index after the span ends. Thus, each token\n10\nQuery\nSentence\nTraining\nCorpus\nRetrieval Model Prompt\nConstruction\nLarge Language\nModelParameter freeze\nInput\n{'INT': ['budesonide Turbuhaler', 'budesonide aqua'],\n'OUT': [],\n'PAR': ['seasonal allergic rhinitis .']}\nOutput\nExtracted PICO relted\nterms \nComparison of budesonide Turbuhaler with budesonide\naqua in the treatment of seasonal allergic rhinitis .\nFigure 2: Visuaization of our proposed ICL-based framework for PICO frame extraction\nsequence s ∈ Tof length N is mapped to a label sequence of the same length,\ni.e., s = {s0, . . . , sN−1} 7→y = {y0, . . . yN−1} where each yi ∈ {B, I, O}. Fi-\nnally, given a set of examples (s,y) of such sequence pairs, the parameters θ of a\nsequence classification model are learned by optimizing\nz = argmin\nθ\nX\n(x,y)∈D\nL(y, f(s, θ)), (1)\nwhere L is a standard loss function, e.g., the cross-entropy.\n3.2. Overview of In-Context Learning\nNow we describe one of our proposed methodology to extract the entities\nfrom clinical trial text. In contrast to supervised learning, in-context learning\n(ICL) does not necessitate the training of a specified set of parameters θ on la-\nbeled instances. Instead, the posterior probabilities are now dependent on various\nfactors, including a collection of k labeled input examples, the decoder parame-\nters of a pre-trained LLM. Our work investigates the feasibility of applying in-\ncontext learning (ICL) framework to extract the named entity recognition (NER)\n11\nfrom the clinical literature to utilize the pretrained knowledge of LLM. The input\nX = [T; D; I] incorporates the task description T, demonstrations D, and input\nsample I while the generated output is a set of extracted entities of different cat-\negories Y = {E1type : [ e1, ..., en]..., E2type : [ e1, .., en]}. Figure 2 presents an\nexample from clinical trial dataset, where an ICL-based framework generates the\nbiomedical entities of different types such as “seasonal allergic rhinitis” as Inter-\nvention, “budesonide” as Participation and Outcome, by utilizing the knowledge\nfrom the training instances. The overall ICL framework is depicted in Figure 2.\noutput:{'INT': ['ranitidine', 'lansoprazole'], 'OUT': ['Comparison'], 'PAR': []}\ninput:Comparison of ranitidine and lansoprazole in short-term low-dose triple therapy for Helicobacter pylori infection .\ninput:Comparison of inhaled albuterol powder and aerosol in asthma .\ninput:Comparison of budesonide Turbuhaler with budesonide aqua in the treatment of seasonal allergic rhinitis .\noutput:{'INT': ['budesonide', 'budesonide aqua'], 'PAR': ['seasonal allergic rhinitis'], 'OUT': []}\nYou are tasked with solving the Named Entity Recognition (NER) problem. Extract words from the text that are\nrelated to each of the following entities: INT, PAR, OUT.\noutput:{'INT': ['inhaled albuterol powder and aerosol'], 'OUT': [], 'PAR': ['asthma']}\nFew Shots\nDemonstrations\nTask Description\nInput\nSentence\nFigure 3: Formats of input and output of our in-context learning based framework for name\nentity recognition task. The input is formed by the Task Description, Demonstrations, and\nthe Input Sentence.\nTask Description. The task description provides an overview of entity recogni-\ntion problem, with a specific focus on identifying ‘PICO’ frames from clinical\ntrial literature. This framework can be employed in the context of evidence-based\nmedicine research, encompassing four key components: ‘Patient/Population’, ‘In-\ntervention’, ‘Comparison’, and ‘Outcome’. To perform ‘PICO’ frame extraction\ntask by utilizing the pretrained knowledge of LLM, it is important to guide LLM\nas discussed in the work of Min et al. [94]. Additionally, Figure 3 visually demon-\nstrates the importance of the task description in helping the LLM to generate the\n12\nthree distinct types of entities: ‘Partition’, ‘Intervention’, and ‘Outcome’. By fol-\nlowing the outlined task description, the model acquires a comprehensive grasp\nof the intricacies associated with pinpointing and classifying the distinct entities\nencapsulated within the ’PICO’ framework. This profound comprehension is piv-\notal for the model’s ability to precisely decode and handle information pertinent\nto the specified task.\nDemonstrations. Demonstrations play a vital role in ICL framework by convey-\ning intra-class knowledge related to target entity types. This includes insights\ninto entity semantics and contextual patterns, resulting in a comprehensive un-\nderstanding of the subject matter. The essence of demonstrations is captured in\nFigure 3, where the demonstration instance within the illustrative set follows a\nspecific template such as: “input: {text} output: {extractions}.” In this context,\n{text} demonstrates the semantically similar context relevant to the target sen-\ntence from where we need to identify the to the textual content being considered,\nwhile {extractions} represents the extracted entities present in the given text. This\ndistinction facilitates a clear and detailed representation of the output format, en-\nabling a deeper comprehension of the target entity types and their contextual rep-\nresentations.\nExtractions. The outcome of the extraction procedure yields an array of entities,\nwith each distinct extraction articulated as ”E NTITY is type”. For instance, as\nillustrated in Figure 3, the extraction of ”treatment of seasonal allergic rhinitis”\ndenotes ”allergic rhinitis” as an identified entity classified under ”Participation”.\nThis representation style, akin to natural language, facilitates the efficient leverage\nof the inherent text generation proficiencies of large language models by tapping\ninto the extensive knowledge they have amassed during the pretraining phase.\n13\nArchitecture. In line with the task formulation, we have chosen to employ LLaMA\nbased models, namely, AlpaCare [95]. AlpaCare is specifically developed for\nmedical applications while the primary goal of AlpaCare is to enhance the model’s\nproficiency in the medical domain while maintaining strong generalization capa-\nbilities across various tasks. The decoder of AlpaCare framework is tasked with\nprocessing diverse inputs, such as instructions, demonstrations, and textual data.\nThese inputs are represented as demonstrations. Subsequently, the decoder func-\ntions to generate a comprehensive set of extractions, which take the form of a\ntokenized text sequence denoted as Y = [y1, . . . , yn].\nThe efficacy of an ICL-based framework primarily depends upon two crucial\ncapabilities, namely, the ability to learn contextually and extract suitable infor-\nmation. In this manner, we can perceive a Large Language Model (LLM) as a\nmeta-function, that is, a function that employs extractors as its input (in the form\nof instructions and demonstrations) and produces the requisite entity extractor as\nits output. To choose the appropriate text portion, we employ dense vector repre-\nsentations from BioBERT to calculate the cosine similarity.\nRetrieval-based approach to obtain relevant context for Entity Recognition\nTask. To obtain the set of text units for our downstream PICO frame extraction\ntask, we first execute a keyword query formulated from the input sentences of\ntraining data on a dense index constructed from the document collection. As the\ngranularity of retrievable units, we work at the sentence level to match the input\nsentence’s length. A dense index retriever, which we used in our experiments\nsimilar to [96], outputs a list of top- k candidate sentences retrieved from an in-\ndex. Our proposed ICL-based framework for the PICO frame extraction task is\ndepicted in Figure 2.\n14\nx\nPretrained \n  Weights\nA\nB\n⊕\nh\nFigure 4: LoRA block of our AlpaPICO framework.\n4. Downstream Task focused Instruction Tuning\nAdditionally, to the best of our knowledge, we are the first to propose an in-\nstruction tuning approach along with an ICL-based framework which helps to\nimprove the zero-shot and few-shot performance of LLMs such as Alpaca [87]\nand Vicuna [88].\nOverview of Parameter Efficient Finetuning. In contrast, we introduce a gen-\neral framework of task-focused instruction tuning strategy, where the pretrained\nmodel AlpaCare [95] is further finetuned for biomedical entity extraction task\nfrom clinical trial dataset. In addition, the storage and deployment of finetuned\nmodels independently for each downstream task can incur significant expenses,\n15\ngiven that fine-tuned models are of the same size as the original pretrained model.\nNevertheless, as the parameter sizes of large language models are huge, conduct-\ning comprehensive fine-tuning becomes infeasible due to the requirement of high\nresources. Also, it is infeasible to store them individually for different versions\nof frameworks produced after the task-specific finetuning [97] as the parameter\nsizes of the finetuned versions are the same as the non-finetuned version of frame-\nworks. To alleviate this problem, we employ the parameter efficient finetuning\n(PEFT) strategy by utilizing the low rank adaptation (LoRA) [98] technique to\nfinetune a small number of (extra) model parameters while freezing most param-\neters of the pretrained LLMs, thereby greatly decreasing the computational and\nstorage costs. LoRA applies a simple and efficient methodology to update the pa-\nrameters of a weight matrix. This approach breaks down the higher dimensional\nmatrix into a multiplication of two matrices with a low rank using the Kronecker\nproduct [99]. We consider LoRA as a reparametrization-based learning which can\nbe formulated as follows:\nMo = MiW0 + Mi∆W = MiW0 + MiBA, (2)\nwhere W0 ∈ Rd×d is the pretrained weight matrix, including weights in the MLP\nor Attention layer. B ∈ Rr×d and A ∈ Rr×d are lower-rank matrix intended for\ncovering ∆W as depicted in Figure 4. r ≪ d is an important hyper-parameter\nfor LoRA. To conduct the instruction based finetuning for our downstream PICO\nframe extraction task, we prepare our own dataset in conversation style dataset.\nAs depicted in Figure 5, we prepare our own conversation style dataset to con-\nduct the instruction tuning approach for our downstream PICO frame extraction\ntask. In general, most of the recent literature [93] utilized state-of-the-art (SOTA)\nLLM such as ChatGPT [76] based framework to generate the task specific diverse\n16\nInstruction Tuning Template\nSystem Prompt: You are tasked with solving the Named Entity Recognition (NER) problem. Extract words     \n from the text that are related to each of the following entities: INT, PAR, OUT.\nComparison of budesonide Turbuhaler with budesonide aqua in the treatment of seasonal allergic rhinitis .\n{'INT': ['budesonide', 'Turbuhaler', 'budesonide', 'aqua'], 'PAR': ['seasonal', 'allergic', 'rhinitis', '.'], 'OUT': []}\nSETTING Private practices and hospital clinics in Ontario , Quebec and Manitoba .\n{'INT': [], 'PAR': ['Ontario', ',', 'Quebec', 'and', 'Manitoba', '.'], 'OUT': []}\n{'INT': [], 'PAR': [], 'OUT': ['eye', 'symptoms', '.']}\nThere were no statistically significant changes from baseline for eye symptoms .\nFigure 5: The data construction prompt is utilized to produce entity mentions and their\ncorresponding types for a specific passage.\nsamples to perform the instruction-tuning approach for our downstream task. In\ncontrast to that, we utilize the existing annotated training dataset to prepare the\nconversation style dataset to conduct the downstream PICO frame extraction due\nto the limited resources. From Figure 5, we can observe that our conversion style\ntemplate which has three parts i.e., instruction, input, and output. In\nFigure 5 the ‘System Prompt’ is used as the instruction whereas the input\nis used as sentences from the training data, and the output is the annotated\nentities present in that particular sentence. After obtaining conversation style an-\nnotated dataset, we propose a novel framework namely AlpaPICO by finetuning\nthe AlpaCare [16] in instruction-tuning [100] based framework in an end-to-end\nfashion for our PICO extraction task by employing LoRA technique.\nImplementation Details. All the models were trained on Nvidia A100 80 GB\nGPU. In terms of the common neural network settings, we used AdamW [101] as\n17\nCoarse-grained Dataset Fine-grained Dataset\nEBM-NLP EBM-COMET EBM-NLP rev EBM-NLP h\n# total sentences 53,397 5,193 40,092 53,404\nTraining # sentences 40,935 3,895 30,069 40,942\nValidation # sentences 10,386 779 6,014 1,864\nTest # sentences 2,076 519 4,009 2,076\nTable 1: Dataset Statistics\nthe optimizer with a learning rate of 0.0003 and a stopping criterion as mentioned\nin [102]; the training batch size used was 8.\n5. Experiment Setup\nIn this section, we first describe the details of the datasets which have been\nused for our PICO frame extraction task from clinical trial literature and then we\nfollow it up with the research questions to the task of PICO frame extraction, and\nthe methods investigated towards addressing those questions.\n5.1. Dataset Description\nTo carry out our experimental investigations utilizing both our proposed ICL-\nbased framework and the instruction-tuning-based method, we employ on four\nopenly accessible datasets: EBMNLP, EBM-COMET, EBM-NLP rev, and EBM-\nNLP-hierarchical (EBM-NLP h). We have categorized all the datasets into two\n18\nDataset NameCoarse-grained LabelsFine-grained Labels\nParticipants Age, Sex, Sample size, Condition\nEBM-NLPh Interventions Surgical, Physical, Drug, Educational, Psychological, Control, Other\nOutcomes Physical, Pain, Mortality, Adverse effects, Mental, Other\nEBM-NLPrev Outcomes Physical, Pain, Mortality, Adverse effects, Mental, Other\nTable 2: Description of fine-grained entity types present in EBM-NLPrev and EBM-NLP-\nhierarchical (EBM-NLPh).\ncategories such as coarse-grained version and fine-grained version, according to\nits available annotations. More specifically speaking, entities belong to the given\ncategories such as ‘Participation,’ ‘Intervention,’ or ‘Outcome’ categories are con-\nsidered as coarse-grained annotations. In contrast to that, the subdivision of a spe-\ncific entity label into more detailed categories is considered as fine-grained anno-\ntation. It is important to note that, for fair comparison, during the evaluation phase,\nthe fine-grained labels are mapped to coarse-grained labels. The fine-grained an-\nnotation is only utilized during the training phase. The Table 1 demonstrates the\noverall dataset statistics. The Table 2 demonstrates that in the EBM-NLPh dataset,\neach coarse-grained category is further divided into fine-grained labels, while the\nEBM-NLPrev contains finer labels only for one broader label.\n5.2. Research Questions\nIn this section we pose a few important research questions, which are central\nto our research work.\n19\nRQ-1. What is the feasibility of applying in-context learning based framework for\nour downstream PICO frame extraction task from clinical trial text using AlpaCare\nlanguage model?\nRQ-2. How effectively can we apply instruction tuning-based techniques using\nan LLM in extracting the PICO related terminologies for both coarse-grained and\nfine-grained variants of the dataset?\n5.3. Methods Investigated\n5.3.1. Baselines for Instruction Tuning Setup\nThe PICO frame extraction task in clinical trial literature is commonly viewed\nas a sequence labeling task. Unlike traditional supervised frameworks, our pro-\nposed novel AlpaPICO considers the PICO frame extraction as a sequence gen-\neration task. By evaluating its performance against state-of-the-art supervised\nframeworks, we demonstrate the efficacy of AlpaPICO, emphasizing its effective-\nness in addressing the challenges of natural language generation tasks compared\nto sequence classification tasks [103].\n• BioBERT [104]: In particular, Lee et.al. [104] employed Bio-BERT to fine-\ntune it on the downstream PICO frame extraction to leverage the pretrained\nbiomedical knowledge of the Bio-BERT, acquired during the pretraining\nphase by the BioBERT language model.\n• SciBERT [17]: Similar to BioBERT work, we finetune the SciBERT lan-\nguage model for the downstream the PICO frame extraction task by consid-\nering it as sequence labeling task.\n20\n• BioLinkBERT-Large [18]: Additionally, in order to show the efficacy of our\nproposed novel AlpaPICO framework, we also apply finetuning strategy on\nBioLinkBERT language for our downstream PICO frame extraction task.\n• Llama-2-Inst [105]: We finetune the Llama-2 large language model by con-\nverting our PICO frame extraction sequence classification dataset to con-\nversation style format as described in Section 4.\n5.3.2. Baselines for In-context Learning Setup\nTo show the effectiveness of our proposed in-context learning based frame-\nwork we compare the performance with the following baselines. Although ICL-\nbased framework eliminates the conventional training process, we compare its\nperformance with the purely supervised frameworks to leverage the effectiveness\nof pretrained knowledge of LLM, gathered during the pretraining phase.\n• Zero-Shot: In this configuration, we instruct our language model AlpaCare\nwithout supplying any additional annotated context i.e., annotated demon-\nstrations as described in Section 3.2 to perform the PICO frame extraction\ntask by considering the sequence generation task.\n• BioLinkBERT-Large [18]: In order to achieve optimal performance, partic-\nularly when an annotated training dataset is accessible for training purposes,\nwe undertake fine-tuning of the BioLinkBERT language model specifically\nfor the downstream PICO frame extraction task.\n5.3.3. Result & Analysis\nTo investigate RQ-1, Table 3 shows a comparison between the methods inves-\ntigated for PICO frame extraction task. Table 4 shows performance of k-shot\n21\nModel\nEBM-NLP EBM-NLPh EBM-NLPrev EBM-COMET\nF-score F-score F-score F-score\nBaselines\nZero-shot (ICL0) 0.020 0.048 0.00 0.00\nBioLinkBERT-Large [18] 74.19 43.95 80.41 62.52\nICL k-shot ICL 47.21 63.10 40.59 43.74\nTable 3: A comparison between F-scores obtained from supervised PLMs, their respective\nin-domain (biomedical) versions, and our instruction tuning based approach AlpaPICO.\nThe best results have been shown in bold and the second best results have been under-\nlined. Values ofk are 3, 4, 9, 9 for EBM-NLP, EBM-NLPh, EBM-NLPrev, EBM-COMET\nrespectively.\nDataset Precision Recall F-Score Accuracy\nEBM-NLPrev 48.08 48.35 47.21 31.15\nEBM-COMET 45.06 36.93 40.59 25.47\nTable 4: The overall performance of our k-shot ICL based technique using a pre-trained LLM\nAlpaPICO. Values of k are 3, 4, 9, 9 for EBM-NLPrev, and EBM-COMET respectively.\nICL based technique for EBM-NLP rev, and EBM-COMET. Also, Table 5 shows\nthe overall results for k-shot ICL for EBM-NLP and EBM-NLP h respectively.\nIt can be seen that task-specific finetuning of any language model produces the\n22\nEBM-NLP EBM-NLPh\nPrecision Recall F-score Accuracy Precision Recall F-score Accuracy\nOUT 49.46 35.59 41.40 26.10 62.02 49.26 54.91 37.84\nINT 36.47 54.71 43.76 28.01 67.76 52.56 59.20 42.05\nPAR 58.32 54.74 56.47 39.35 79.39 71.42 75.19 60.25\nTable 5: Class-wise performace of our k-shot ICL based technique using a pre-trained LLM Al-\npaPICO. Values of k are 3,4,9,9 for EBM-NLP and EBM-NLPh respectively.\nPC 1\nPC 2\nFigure 6: Dimensionally-reduced feature vector representation of the randomly sampled\n1000 training instances from EBM-NLP. Later on, these vectors underwent additional\nreduction to only two dimensions using PCA for the purpose of visualization. Note that\nthe diversity in the training instances of EBM-NLP dataset is less.\nbest results (as seen for BioLinkBERT-Large). However, a supervised framework\nrequires the existence of a training set of labeled examples of sentences and the\n23\nannotated entities present in the sentence. In contrast to that, even a completely\nunsupervised zero-shot scenario ICL 0 and without any training set yields results\nthat are substantially worse in terms of F-score than the supervised frameworks.\nThis observation leads to further investigation of whether the addition of annotated\ncontext with the system prompt to instruct the AlpaCare framework for the down-\nstream PICO frame extraction task. We can observe from Table 3 that the addition\nof annotated context along with the task description as described in Figure 3 helps\nsignificantly to improve the performance of AlpaCare 1 LLM for our downstream\nPICO frame extraction task in an unsupervised setup. More specifically speak-\ning, our proposed k-shot ICL-based framework, wherek represents the number of\nannotated instances formally referred to as demonstrations (cf. Section 3.2), per-\nforms well across various datasets. The probable explanation for this phenomenon\nis that when AlpaCare, finetuned on biomedical data instances, encounters the k\nnumber of semantically similar instances from the training dataset, it effectively\nleverages its pretrained knowledge, acquired during the pretraining phase. This\nenables AlpaCare to generate relevant entities more adeptly in comparison to the\nzero-shot scenario, where no additional demonstrations are provided alongside\nthe task description. Additionally, we can also observe from Table 3 that the per-\nformance of our proposed k-shot ICL framework is relatively comparable to the\nsupervised framework in most of the datasets such as EBM-NLP, EBM-NLP rev\nand EBM-COMET whereas our k-shot ICL based framework outperforms all the\nsupervised frameworks on EBM-NLPh dataset. The rationale underlying this ob-\nserved performance is rooted in the fact that the ICL-based framework treats our\n1xz97/AlpaCare-llama2-7b\n24\ndownstream PICO frame extraction task as a natural language generation task, di-\nverging from the conventional sequence classification task employed, for instance,\nfinetuning BioLinkBERT-Large on the PICO frame extraction from the clinical\ntrial dataset. Natural language generation inherently poses greater challenges, as\nhighlighted in the work by Yang et al. [103], compared to classification problems.\nAdditionally, another key insight is that the ICL framework tends to yield opti-\nmal performance when presented with a diverse set of annotated instances. In our\nspecific case, analysis of the embedding space, as depicted in Figure 6, reveals\nthat a substantial portion of training instances from the EBM-NLP dataset forms\na cohesive cluster, suggesting the importance of diversity in annotated instances\nfor achieving satisfactory performance. An interesting observation concerning the\nfine-grained EBM-NLPh dataset, as mentioned in Table 3, is that our k-shot ICL-\nbased framework outperforms both the zero-shot and supervised BioLink-BERT-\nLarge framework in the PICO frame extraction task. The likely reason is that the\nclean and fine-grained annotations present in the EBM-NLPh dataset, as compared\nto the EBM-NLP, EBM-COMET, and EBM-NLP rev datasets where clean anno-\ntated demonstrations help our ICL framework to outperform the baseline models\nin terms of F-score.\nTo explore RQ-2, we perform an instruction tuning based technique using Al-\npaCare LLM on both coarse-grained and fine-grained datasets. We evaluate the\nperformance of our proposed AlpaPICO framework in terms of F-score. Table 6\nshows an interesting sets of observations, which presents that our proposed in-\nstruction tuning chat model AlpaPICO produces comparable results on coarse-\ngrained datasets such as EBM-NLP and EBM-COMET datasets. Table 7 and\nTable 8 show detailed observations ofAlpaPICO using our instruction based tun-\n25\nModel EBM-NLPEBM-NLPrev EBM-COMETEBM-NLPh\nBioBERT [104] 73.18 53.10 81.50 –\nSciBERT [17] 73.06 52.80 77.60 –\nBioLinkBERT-Large [18]74.19 43.95 80.41 62.52\nLlama-2-Inst. 60.51 59.36 64.48 69.86\nAlpaPICO 64.81 62.33 70.90 70.12\nTable 6: A comparison between F-scores obtained from supervised PLMs, their respective\nin-domain (biomedical) versions, and our instruction tuning based approach AlpaPICO.\nThe best results have been shown inbold and the second best results have been underlined.\nExample 1:\nExtracted:\nTarget:\nExample 2:\nExtracted:\nTarget:\nExample 3:\nExtracted:\nTarget:\n{'OUT': ['implantation', 'clinical pregnancy']}\nExample 1:\n{'OUT': ['implantation rate', 'clinical pregnancy rate']}Extracted:\nTarget:\nExample 2:\nExtracted:\nTarget:\n{'OUT': ['marginal bone level (MBL)']}\n {'OUT': ['marginal bone']}\nExample 3:\nExtracted:\n{'OUT': ['accurate lifestyle factors']}\nTarget: {'OUT': ['lifestyle factors']}\nExample 4:\nExtracted:\nTarget:\n{'OUT': ['moderate/severe exacerbations']}\n{'OUT': ['exacerbations']}\n{'INT': ['Systemic', 'oxytetracycline'], 'OUT': ['rate',\n'of', 'cure', 'for', 'S.', 'aureus', 'mastitis', '.'], 'PAR': []}\n{'INT': ['oxytetracycline'], 'PAR': [], 'OUT': ['rate', 'of',\n'cure']}\n{'INT': ['amoxicillin', 'penicillin', 'V'], 'OUT': [], 'PAR':\n['children', 'with', 'pharyngitis', '.']}\n{'INT': ['amoxicillin', 'penicillin'], 'PAR': ['GABHS',\n'in', 'children', 'with', 'pharyngitis', '.'], 'OUT': ['not',\n'inferior']}\n{'INT': ['nicotine', 'nicotine', 'gum'], 'OUT':\n['cardiovascular', 'or', 'lipid', 'stress', 'reactivity'],\n'PAR': ['women', '.']}\n{'INT': ['nicotine', 'nicotine'], 'PAR': ['women', '.'],\n'OUT': ['cardiovascular', 'or', 'lipid', 'stress',\n'reactivity']}\n(a) (b) \nFigure 7: Comparison of PICO frame entities generated by our proposed instruction\ntuned AlpaPICO framework and the actual annoated entities on coarse-grained (a) EBM-\nCOMET and (b) EBM-NLP datasets.\ning based approach EBM-NLP, EBM-NLP h, EBM-NLP rev and EBM-COMET\ndatasets respectively. The rationale behind this performance is that the annota-\ntions of both the coarse-grained EBM-NLP and EBM-COMET datasets are noisy\nin nature. Additionally, from Figure 7, we can observe that the generated entities\n26\nDataset Precision Recall F-Score Accuracy\nEBM-NLPrev 85.15 49.16 62.33 45.27\nEBM-COMET 81.40 62.80 70.90 54.90\nTable 7: Performance of our instruction based approach AlpaPICO\nEBM-NLP EBM-NLPh\nPrecision Recall F-score Accuracy Precision Recall F-score Accuracy\nOUT 85.88 49.03 62.42 45.37 65.87 63.66 64.75 47.87\nINT 64.21 49.91 56.17 39.05 78.16 59.54 67.59 51.05\nPAR 82.38 70.28 75.85 61.09 81.4 74.91 78.02 63.97\nTable 8: Class-wise performance of our instruction tuning based approach AlpaPICO on EBM-\nNLP and EBM-NLPh datasets.\nfor the first test instance of EBM-COMET dataset is ‘implantation rate’ and ‘clin-\nical pregnancy rate’ whereas the actual annotation contains the ‘implantation’ and\n‘clinical pregnancy’ respectively. Another example of the generated entities of the\ntest instance of the EBM-NLP dataset is ‘systematic oxytretracycline’ whereas the\nactual annotation is ‘oxytretracycline’. Hence, from both example snippets, it is\nevident that our proposed AlpaPICO framework can successfully generate cen-\ntral entity tokens, while also generating additional tokens due to the presence of\nnoisy annotations in the training instances. However, since we employ a token-\n27\nlevel strict matching technique to calculate the F-score, our AlpaPICO framework\ndoes not outperform the performance of existing supervised baselines on these\ntwo datasets. In contrast to the aforementioned observation, another noteworthy\nfinding is that our instruction-tuned AlpaPICO framework exhibits a significant\nperformance advantage over existing supervised baselines in terms of F-score on\nboth the fine-grained EBM-NLP rev and EBM-NLPh datasets. The likely reason\nbehind this observation is that the combination of fine-grained and clear annota-\ntions, as detailed in Section 5.1, contributes substantially to enhancing the perfor-\nmance of our instruction-tuned AlpaPICO framework.\n6. Ablation Study\nThroughout the experiments with our k-shot ICL framework, we employ two\nstrategies to select the relevant annotated contexts, which are considered as demon-\nstrations, aiming to enhance the performance of our proposed ICL framework. In\nselecting the relevant annotated context, we employ a strategy based on dense\nindex and cosine similarity. Specifically, we acquire the BioLinkBERT embed-\nding of a particular test instance from the EBM-NLP dataset, where the dense\nembedding of the training instances is pre-indexed using the FAISS framework 2.\nThe test instance embedding serves as the necessary query embedding, which is\npassed to the FAISS dense indexer. This indexer utilizes the hierarchical navi-\ngable small worlds (HNSW) [106] based nearest neighbor searching strategy to\nretrieve semantically relevant instances. In contrast, we also implement a random\nselection strategy to retrieve the annotated context from the training dataset cor-\n2https://github.com/facebookresearch/faiss\n28\n 0\n 10\n 20\n 30\n 40\n 50\n 60 Random\nICL\n1 2 3 4 5 6 5\nF Score\nk\nFigure 8: Performance (F-score on Y-axis) comparison based on random context selection,\nby changing the number of samples (k on X-axis), in the ICL-based framework.\nresponding to a specific test instance. It is noteworthy as depicted in Figure 8,\nthat our randomly selected 5 examples from EBM-NLP training dataset achieves\nhighest F-score but it cannot outperform the best-performing 5-shot ICL, where\nthe required context is selected using the HNSW algorithm. So, we perform addi-\ntional experiments by employing ICL framework, selecting semantically similar\ncontexts corresponding to specific test instances across the remaining datasets.\nHowever, we carry out more ablation experiments for our k-shot ICL frame-\nwork to select the optimized k value for our downstream PICO frame extraction\ntask from the clinical trial literature. From Figure 9, we observe that the op-\ntimized k-values are not the same for all the datasets. For instance, we attain\nthe highest F-score on the EBM-NLP dataset within the ICL framework for k =\n3. Conversely, for the EBM-NLP h, EBM-NLPrev, and EBM-COMET datasets,\nthe k values have been set to 4, 9, and 9, respectively. The probable reason for\nthis phenomenon is that the ICL-based framework is inherently responsive and\n29\n 0\n 5\n 10\n 15\n 20\n 25\n 30\n 35\n 40\n 45\n1 2 3 4 5 6 7 8 9 10  0\n 5\n 10\n 15\n 20\n 25\n 30\n 35\n 40\n 45\n1 2 3 4 5 6 7 8 9 10\n(a) (b)\n(c) (d)\nF Score F Score\nF Score\nF Score\nk k\nk k\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\n1 2 3 4 5 0\n 10\n 20\n 30\n 40\n 50\n1 2 3 4\nFigure 9: Performance (F-score on Y-axis) comparison of our k-shot ICL framework on\ndifferent datasets (a) EBM-NLP, (b) EBM-NLPh, (c) EBM-NLPrev, (d) EBM-COMET by\nchanging the number of context (k).\ncontext-sensitive [107].\n7. Concluding Remarks\nIn this work, we investigate the feasibility of applying In-context learning\nframework as well as instruction tuning approach for PICO frame extraction task\nfrom clinical trial documents. Our novel k-shot ICL based framework for PICO\nframe extraction task in the evidence-based medicine literature perform signifi-\n30\ncantly well, without applying any training operation. Additionally, we propose\na supervised instruction tuning based framework in low resource environment,\nnamely AlpaPICO, which produces the state-of-the-art (SOTA) performance on\nboth EBM-NLPrev and EBM-NLPh datasets. It also produces comparable results\non the two remaining datasets. As both of our approaches consider the PICO\nframe extraction as a natural language generation task instead of a sequence clas-\nsification task, our smaller case-study, as depicted in Figure 7 shows the effec-\ntiveness of LLM towards generating the PICO frame extraction task. A limitation\nof our work is that the computation is memory-intensive due to the use of LLMs,\nthus making it difficult to execute our approach on a terminal with limited mem-\nory capacity. Additionally, due to the limitations of our own resources, we could\nnot use a larger or a commercially accessible LLM. In future, we plan to improve\nthe performance of ICL framework by selecting the relevant context from exter-\nnal corpus, such as Cochrane database. We can also leverage commercial Large\nLanguage Models (LLMs) to generate state-of-the-art data instances for evidence-\nbased medicine literature. These instances can be used to apply knowledge distil-\nlation processes on smaller versions of LLMs for the PICO frame extraction task\nfrom clinical trial documents.\nAuthor Contribution:\nMG, SM, PBC, SKN and DG conceptualized the study. MG, SM and AG\nperformed experimental work. MG and SM contributed to analysis the results and\nprepare the figures. MG, SM and AG wrote the initial draft of the manuscript.\nMG, SM, PBC, SKN, DG edited the manuscript.\n31\nConflicts of interest:\nThe authors declare that there are no conflicts of interest.\nFunding information:\nThis study was funded by Indian Association for the Cultivation of Science\n(IACS), Kolkata, India.\nReferences\n[1] D. L. Sackett, Evidence-based medicine, in: Seminars in perinatology,\nV ol. 21, Elsevier, 1997, pp. 3–5.\n[2] D. J. Cook, C. D. Mulrow, R. B. Haynes, Systematic reviews: synthesis\nof best evidence for clinical decisions, Annals of internal medicine 126 (5)\n(1997) 376–380.\n[3] S. R. Jonnalagadda, P. Goyal, M. D. Huffman, Automating data extraction\nin systematic reviews: a systematic review, Systematic reviews 4 (1) (2015)\n1–16.\n[4] F. Boudin, J.-Y . Nie, M. Dawes, Positional language models for clinical\ninformation retrieval, in: Proceedings of the 2010 Conference on Empirical\nMethods in Natural Language Processing, 2010, pp. 108–115.\n[5] I. Marshall, J. Kuiper, E. Banner, B. C. Wallace, Automating biomedical\nevidence synthesis: RobotReviewer, in: M. Bansal, H. Ji (Eds.), Proceed-\nings of ACL 2017, System Demonstrations, Association for Computational\n32\nLinguistics, Vancouver, Canada, 2017, pp. 7–12.\nURL https://aclanthology.org/P17-4002\n[6] B. Nye, J. J. Li, R. Patel, Y . Yang, I. Marshall, A. Nenkova, B. Wallace, A\ncorpus with multi-level annotations of patients, interventions and outcomes\nto support language processing for medical literature, ACL, Melbourne,\nAustralia, 2018.\n[7] D. Jin, P. Szolovits, Pico element detection in medical text via long short-\nterm memory neural networks, in: Proceedings of the BioNLP 2018 work-\nshop, 2018, pp. 67–75.\n[8] S. N. Kim, D. Martinez, L. Cavedon, L. Yencken, Automatic classification\nof sentences to support evidence based medicine, in: BMC bioinformatics,\nV ol. 12, BioMed Central, 2011, pp. 1–10.\n[9] X. Huang, J. Lin, D. Demner-Fushman, Evaluation of pico as a knowledge\nrepresentation for clinical questions, in: AMIA, V ol. 2006, American Med-\nical Informatics Association, 2006, p. 359.\n[10] M. Abaho, D. Bollegala, P. Williamson, S. Dodd, Correcting crowdsourced\nannotations to improve detection of outcome types in evidence based\nmedicine, in: CEUR Workshop Proceedings, V ol. 2429, 2019, pp. 1–5.\n[11] M. Abaho, D. Bollegala, P. R. Williamson, S. Dodd, Assessment of con-\ntextualised representations in detecting outcome phrases in clinical trials,\narXiv preprint arXiv:2203.03547 (2022).\n[12] A. Stubbs, C. Kotfila, ¨O. Uzuner, Automated systems for the de-\nidentification of longitudinal clinical narratives: Overview of 2014\n33\ni2b2/uthealth shared task track 1, Journal of biomedical informatics 58\n(2015) S11–S19.\n[13] ¨O. Uzuner, Y . Luo, P. Szolovits, Evaluating the state-of-the-art in automatic\nde-identification, Journal of the American Medical Informatics Association\n14 (5) (2007) 550–563.\n[14] C. Wu, W. Lin, X. Zhang, Y . Zhang, Y . Wang, W. Xie, Pmc-llama: Towards\nbuilding open-source language models for medicine (2023). arXiv:\n2304.14454.\n[15] Y . Luo, J. Zhang, S. Fan, K. Yang, Y . Wu, M. Qiao, Z. Nie, Biomedgpt:\nOpen multimodal generative pre-trained transformer for biomedicine\n(2023). arXiv:2308.09442.\n[16] X. Zhang, C. Tian, X. Yang, L. Chen, Z. Li, L. R. Petzold, Alpacare:\nInstruction-tuned large language models for medical application, arXiv\npreprint arXiv:2310.14558 (2023).\n[17] I. Beltagy, K. Lo, A. Cohan, SciBERT: A pretrained language model for\nscientific text, in: Proceedings of the 2019 EMNLP, ACL, Hong Kong,\nChina, 2019, pp. 3615–3620.\n[18] M. Yasunaga, J. Leskovec, P. Liang, LinkBERT: Pretraining language mod-\nels with document links, in: Proceedings of the 60th Annual Meeting of\nthe ACL (V olume 1: Long Papers), ACL, Dublin, Ireland, 2022, pp. 8003–\n8016.\n[19] S. Hoory, A. Feder, A. Tendler, A. Cohen, S. Erell, I. Laish, H. Nakhost,\n34\nU. Stemmer, A. Benjamini, A. Hassidim, Y . Matias, Learning and evalu-\nating a differentially private pre-trained language model, in: Proceedings\nof the Third Workshop on Privacy in Natural Language Processing, ACL,\nOnline, 2021, pp. 21–29.\n[20] S. Mayhew, G. Nitish, D. Roth, Robust named entity recognition with true-\ncasing pretraining, in: Proceedings of the AAAI Conference on Artificial\nIntelligence, V ol. 34, 2020, pp. 8480–8487.\n[21] C. Sung, V . Goel, E. Marcheret, S. Rennie, D. Nahamoo, CNNBiF: CNN-\nbased bigram features for named entity recognition, ACL, 2021.\n[22] P.-H. Li, T.-J. Fu, W.-Y . Ma, Why attention? analyze bilstm deficiency and\nits remedies in the case of ner, in: Proceedings of the AAAI Conference on\nArtificial Intelligence, V ol. 34, 2020, pp. 8236–8244.\n[23] H. Chen, Z. Lin, G. Ding, J. Lou, Y . Zhang, B. Karlsson, Grn: Gated re-\nlation network to enhance convolutional neural network for named entity\nrecognition, in: Proceedings of the AAAI Conference on Artificial Intelli-\ngence, V ol. 33, 2019, pp. 6236–6243.\n[24] Y . Xu, H. Huang, C. Feng, Y . Hu, A supervised multi-head self-attention\nnetwork for nested named entity recognition, in: Proceedings of the AAAI\nConference on Artificial Intelligence, V ol. 35, 2021, pp. 14185–14193.\n[25] B. Li, S. Liu, Y . Sun, W. Wang, X. Zhao, Recursively binary modification\nmodel for nested named entity recognition, in: Proceedings of the AAAI\nConference on Artificial Intelligence, V ol. 34, 2020, pp. 8164–8171.\n35\n[26] D. Dai, X. Xiao, Y . Lyu, S. Dou, Q. She, H. Wang, Joint extraction of en-\ntities and overlapping relations using position-attentive sequence labeling,\nin: Proceedings of the AAAI conference on artificial intelligence, V ol. 33,\n2019, pp. 6300–6308.\n[27] D. Zeng, H. Zhang, Q. Liu, Copymtl: Copy mechanism for joint extraction\nof entities and relations with multi-task learning, in: Proceedings of the\nAAAI conference on artificial intelligence, V ol. 34, 2020, pp. 9507–9514.\n[28] T. Nayak, H. T. Ng, Effective modeling of encoder-decoder architecture for\njoint entity and relation extraction, in: Proceedings of the AAAI conference\non artificial intelligence, V ol. 34, 2020, pp. 8528–8535.\n[29] Y . Xiao, C. Tan, Z. Fan, Q. Xu, W. Zhu, Joint entity and relation extraction\nwith a hybrid transformer and reinforcement learning based model, in: Pro-\nceedings of the AAAI Conference on Artificial Intelligence, V ol. 34, 2020,\npp. 9314–9321.\n[30] K. Sun, R. Zhang, S. Mensah, Y . Mao, X. Liu, Progressive multitask learn-\ning with controlled information flow for joint entity and relation extraction,\nAssociation for the Advancement of Artificial Intelligence (AAAI) (2021).\n[31] R. Li, D. Li, J. Yang, F. Xiang, H. Ren, S. Jiang, L. Zhang, Joint extrac-\ntion of entities and relations via an entity correlated attention neural model,\nInformation Sciences 581 (2021) 179–193.\n[32] Z. Ji, T. Xia, M. Han, J. Xiao, A neural transition-based joint model for\ndisease named entity recognition and normalization, in: Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and\n36\nthe 11th International Joint Conference on Natural Language Processing\n(V olume 1: Long Papers), Association for Computational Linguistics, On-\nline, 2021, pp. 2819–2827. doi:10.18653/v1/2021.acl-long.\n219.\nURL https://aclanthology.org/2021.acl-long.219\n[33] A. Das, D. Ganguly, U. Garain, Named entity recognition with word\nembeddings and wikipedia categories for a low-resource language, ACM\nTrans. Asian Low Resour. Lang. Inf. Process. 16 (3) (2017) 18:1–18:19.\ndoi:10.1145/3015467.\nURL https://doi.org/10.1145/3015467\n[34] S. Mukherjee, M. Ghosh, Girish, P. Basuchowdhuri, MLlab4CS at\nSemEval-2023 task 2: Named entity recognition in low-resource lan-\nguage Bangla using multilingual language models, in: A. K. Ojha, A. S.\nDo˘gru¨oz, G. Da San Martino, H. Tayyar Madabushi, R. Kumar, E. Sar-\ntori (Eds.), Proceedings of the 17th International Workshop on Semantic\nEvaluation (SemEval-2023), Association for Computational Linguistics,\nToronto, Canada, 2023, pp. 1388–1394. doi:10.18653/v1/2023.\nsemeval-1.192.\nURL https://aclanthology.org/2023.semeval-1.192\n[35] D. Wang, H. Fan, J. Liu, Learning with joint cross-document information\nvia multi-task learning for named entity recognition, Information Sciences\n579 (2021) 454–467.\n[36] Q. Ma, L. Yu, H. Chen, J. Yan, Z. Lin, Sequence labeling with mlta: Multi-\nlevel topic-aware mechanism, Information Sciences 637 (2023) 118934.\n37\n[37] J. I. Toledo, M. Carbonell, A. Forn ´es, J. Llad ´os, Information extraction\nfrom historical handwritten document images with a context-aware neural\nmodel, Pattern Recognition 86 (2019) 27–36.\n[38] M. Ghosh, P. Santra, S. A. Iqbal, P. Basuchowdhuri, Astro-mT5: En-\ntity extraction from astrophysics literature using mT5 language model, in:\nT. Ghosal, S. Blanco-Cuaresma, A. Accomazzi, R. M. Patton, F. Grezes,\nT. Allen (Eds.), Proceedings of the first Workshop on Information Extrac-\ntion from Scientific Publications, Association for Computational Linguis-\ntics, Online, 2022, pp. 100–104.\nURL https://aclanthology.org/2022.wiesp-1.12\n[39] Y . Luan, L. He, M. Ostendorf, H. Hajishirzi, Multi-task identification of\nentities, relations, and coreference for scientific knowledge graph construc-\ntion, arXiv preprint arXiv:1808.09602 (2018).\n[40] S. Jain, M. van Zuylen, H. Hajishirzi, I. Beltagy, Scirex: A chal-\nlenge dataset for document-level information extraction, arXiv preprint\narXiv:2005.00512 (2020).\n[41] M. Ghosh, D. Ganguly, P. Basuchowdhuri, S. K. Naskar, Extracting\nmethodology components from ai research papers: A data-driven factored\nsequence labeling approach, in: Proceedings of the 32nd ACM Interna-\ntional Conference on Information and Knowledge Management, 2023, pp.\n3897–3901.\n[42] M. Ghosh, D. Ganguly, P. Basuchowdhuri, S. K. Naskar, Enhancing\nai research paper analysis: Methodology component extraction using\n38\nfactored transformer-based sequence modeling approach, arXiv preprint\narXiv:2311.03401 (2023).\n[43] Y . Tong, Y . Chen, X. Shi, A multi-task approach for improving biomed-\nical named entity recognition by incorporating multi-granularity informa-\ntion, in: Findings of the ACL: ACL-IJCNLP 2021, ACL, Online, 2021, pp.\n4804–4813.\n[44] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,\nS. Fidler, Aligning books and movies: Towards story-like visual explana-\ntions by watching movies and reading books, in: Proceedings of the IEEE\ninternational conference on computer vision, 2015, pp. 19–27.\n[45] A. Akbik, D. Blythe, R. V ollgraf, Contextual string embeddings for se-\nquence labeling, in: Proceedings of the 27th International Conference\non Computational Linguistics, Association for Computational Linguistics,\nSanta Fe, New Mexico, USA, 2018, pp. 1638–1649.\nURL https://aclanthology.org/C18-1139\n[46] F. Boudin, J.-Y . Nie, J. C. Bartlett, R. Grad, P. Pluye, M. Dawes, Combining\nclassifiers for robust pico element detection, BMC medical informatics and\ndecision making 10 (1) (2010) 1–6.\n[47] K.-C. Huang, C. C.-H. Liu, S.-S. Yang, F. Xiao, J.-M. Wong, C.-C. Liao,\nI.-J. Chiang, Classification of pico elements by text features systematically\nextracted from pubmed abstracts, in: 2011 IEEE International Conference\non Granular Computing, IEEE, 2011, pp. 279–283.\n39\n[48] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nL. Zettlemoyer, Deep contextualized word representations, in: Proceedings\nof the 2018 Conference of the NAACL, ACL, New Orleans, Louisiana,\n2018, pp. 2227–2237.\n[49] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., Improving\nlanguage understanding by generative pre-training (2018).\n[50] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep\nbidirectional transformers for language understanding, in: Proceedings of\nthe 2019 ACL, V olume 1 (Long and Short Papers), ACL, Minneapolis,\nMinnesota, 2019, pp. 4171–4186.\n[51] G. Lample, A. Conneau, Cross-lingual language model pretraining, arXiv\npreprint arXiv:1901.07291 (2019).\n[52] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, Q. V . Le,\nXlnet: Generalized autoregressive pretraining for language understanding,\nAdvances in neural information processing systems 32 (2019).\n[53] E. F. Sang, F. De Meulder, Introduction to the conll-2003 shared\ntask: Language-independent named entity recognition, arXiv preprint\ncs/0306050 (2003).\n[54] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad: 100,000+ ques-\ntions for machine comprehension of text, arXiv preprint arXiv:1606.05250\n(2016).\n[55] S. Liu, Y . Sun, B. Li, W. Wang, F. T. Bourgeois, A. G. Dunn, Sent2Span:\nSpan detection for PICO extraction in the biomedical text without span\n40\nannotations, in: Findings of the ACL: EMNLP 2021, ACL, Punta Cana,\nDominican Republic, 2021, pp. 1705–1715.\n[56] A. J. Brockmeier, M. Ju, P. Przybyła, S. Ananiadou, Improving reference\nprioritisation with pico recognition, BMC medical informatics and decision\nmaking 19 (1) (2019) 1–14.\n[57] T. Zhang, Y . Yu, J. Mei, Z. Tang, X. Zhang, S. Li, Unlocking the power of\ndeep pico extraction: Step-wise medical ner identification, arXiv preprint\narXiv:2005.06601 (2020).\n[58] M. Ghosh, S. Mukherjee, P. Santra, G. Na, P. Basuchowdhuri, Blink-\ntextsubscriptlstm: Biolinkbert and lstm based approach for extraction of\npico frame from clinical trial text, in: Proceedings of the 7th Joint Inter-\nnational Conference on Data Science & Management of Data (11th ACM\nIKDD CODS and 29th COMAD), 2024, pp. 227–231.\n[59] A. Dhrangadhariya, H. M ¨uller, DISTANT-CTO: A zero cost, distantly su-\npervised approach to improve low-resource entity extraction using clinical\ntrials literature, in: Proceedings of the 21st Workshop on Biomedical Lan-\nguage Processing, ACL, Dublin, Ireland, 2022, pp. 345–358.\n[60] A. Giannakopoulos, C. Musat, A. Hossmann, M. Baeriswyl, Unsuper-\nvised aspect term extraction with B-LSTM & CRF using automatically\nlabelled datasets, in: Proceedings of the 8th Workshop on Computational\nApproaches to Subjectivity, Sentiment and Social Media Analysis, ACL,\nCopenhagen, Denmark, 2017, pp. 180–188.\n41\n[61] J. Mullenbach, S. Wiegreffe, J. Duke, J. Sun, J. Eisenstein, Explainable\nprediction of medical codes from clinical text, in: Proceedings of the 2018\nConference of NAACL, ACL, New Orleans, Louisiana, 2018, pp. 1101–\n1111.\n[62] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models\nare few-shot learners, Advances in neural information processing systems\n33 (2020) 1877–1901.\n[63] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\nJ. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling language\nmodels: Methods, analysis & insights from training gopher, arXiv preprint\narXiv:2112.11446 (2021).\n[64] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper,\nZ. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti, et al., Using deepspeed\nand megatron to train megatron-turing nlg 530b, a large-scale generative\nlanguage model, arXiv preprint arXiv:2201.11990 (2022).\n[65] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Ruther-\nford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al., Training\ncompute-optimal large language models, arXiv preprint arXiv:2203.15556\n(2022).\n[66] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scaling lan-\nguage modeling with pathways, arXiv preprint arXiv:2204.02311 (2022).\n42\n[67] S. Hegselmann, A. Buendia, H. Lang, M. Agrawal, X. Jiang, D. Sontag,\nTabllm: Few-shot classification of tabular data with large language models,\narXiv preprint arXiv:2210.10723 (2022).\n[68] D. Vilar, M. Freitag, C. Cherry, J. Luo, V . Ratnakar, G. Foster, Prompting\npalm for translation: Assessing strategies and performance, arXiv preprint\narXiv:2211.09102 (2022).\n[69] E. Perez, D. Kiela, K. Cho, True few-shot learning with language mod-\nels, Advances in neural information processing systems 34 (2021) 11054–\n11070.\n[70] B. Pietrzak, B. Swanson, K. Mathewson, M. Dinculescu, S. Chen, Story\ncentaur: Large language model few shot learning as a creative writing tool\n(2021).\n[71] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M.\nDai, Q. V . Le, Finetuned language models are zero-shot learners, arXiv\npreprint arXiv:2109.01652 (2021).\n[72] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou,\nW. Li, P. J. Liu, Exploring the limits of transfer learning with a unified\ntext-to-text transformer, The Journal of Machine Learning Research 21 (1)\n(2020) 5485–5551.\n[73] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman,\nN. A. Smith, Annotation artifacts in natural language inference data, arXiv\npreprint arXiv:1803.02324 (2018).\n43\n[74] A. Roberts, C. Raffel, N. Shazeer, How much knowledge can you pack\ninto the parameters of a language model?, arXiv preprint arXiv:2002.08910\n(2020).\n[75] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented lan-\nguage model pre-training (2020) 3929–3938.\n[76] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\nLanguage models are unsupervised multitask learners, OpenAI blog 1 (8)\n(2019) 9.\n[77] Y . Lu, M. Bartolo, A. Moore, S. Riedel, P. Stenetorp, Fantastically ordered\nprompts and where to find them: Overcoming few-shot prompt order sen-\nsitivity, arXiv preprint arXiv:2104.08786 (2021).\n[78] O. Rubin, J. Herzig, J. Berant, Learning to retrieve prompts for in-context\nlearning, arXiv preprint arXiv:2112.08633 (2021).\n[79] S. Wang, X. Sun, X. Li, R. Ouyang, F. Wu, T. Zhang, J. Li, G. Wang, Gpt-\nner: Named entity recognition via large language models, arXiv preprint\narXiv:2304.10428 (2023).\n[80] M. Zhang, H. Yan, Y . Zhou, X. Qiu, Promptner: A prompting method\nfor few-shot named entity recognition via k nearest neighbor search, arXiv\npreprint arXiv:2305.12217 (2023).\n[81] K. Pakhale, Comprehensive overview of named entity recognition:\nModels, domain-specific applications and challenges, arXiv preprint\narXiv:2309.14084 (2023).\n44\n[82] D. Ashok, Z. C. Lipton, Promptner: Prompting for named entity recogni-\ntion, arXiv preprint arXiv:2305.15444 (2023).\n[83] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li,\nX. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned\nlanguage models, arXiv preprint arXiv:2210.11416 (2022).\n[84] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja, M. Dey, M. S. Bari, C. Xu,\nU. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. Nayak,\nD. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X.\nYong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma,\nA. Santilli, T. Fevry, J. A. Fries, R. Teehan, S. Biderman, L. Gao, T. Bers,\nT. Wolf, A. M. Rush, Multitask prompted training enables zero-shot task\ngeneralization (2021). arXiv:2110.08207.\n[85] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Arunk-\numar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al., Super-\nnaturalinstructions: Generalization via declarative instructions on 1600+\nnlp tasks, arXiv preprint arXiv:2204.07705 (2022).\n[86] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kel-\nton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike,\nR. Lowe, Training language models to follow instructions with human feed-\nback (2022). arXiv:2203.02155.\n[87] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang,\n45\nT. B. Hashimoto, Stanford alpaca: An instruction-following llama model,\nhttps://github.com/tatsu-lab/stanford_alpaca (2023).\n[88] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng,\nS. Zhuang, Y . Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality (March\n2023).\nURL https://vicuna.lmsys.org\n[89] B. Peng, C. Li, P. He, M. Galley, J. Gao, Instruction tuning with gpt-4,\narXiv preprint arXiv:2304.03277 (2023).\n[90] Y . Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu, D. Wadden,\nK. MacMillan, N. A. Smith, I. Beltagy, H. Hajishirzi, How far can camels\ngo? exploring the state of instruction tuning on open resources (2023).\narXiv:2306.04751.\n[91] A. Gudibande, E. Wallace, C. Snell, X. Geng, H. Liu, P. Abbeel, S. Levine,\nD. Song, The false promise of imitating proprietary llms (2023). arXiv:\n2305.15717.\n[92] X. Wang, W. Zhou, C. Zu, H. Xia, T. Chen, Y . Zhang, R. Zheng, J. Ye,\nQ. Zhang, T. Gui, J. Kang, J. Yang, S. Li, C. Du, Instructuie: Multi-\ntask instruction tuning for unified information extraction (2023). arXiv:\n2304.08085.\n[93] W. Zhou, S. Zhang, Y . Gu, M. Chen, H. Poon, Universalner: Targeted dis-\ntillation from large language models for open named entity recognition,\narXiv preprint arXiv:2308.03279 (2023).\n46\n[94] S. Min, M. Lewis, L. Zettlemoyer, H. Hajishirzi, MetaICL: Learning to\nlearn in context, in: Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Association for Computational Linguistics,\nSeattle, United States, 2022, pp. 2791–2809. doi:10.18653/v1/\n2022.naacl-main.201.\nURL https://aclanthology.org/2022.naacl-main.201\n[95] X. Zhang, C. Tian, X. Yang, L. Chen, Z. Li, L. R. Petzold,\nAlpacare:instruction-tuned large language models for medical application\n(2023). arXiv:2310.14558.\n[96] Z. Wu, Y . Wang, J. Ye, Z. Wu, J. Feng, J. Xu, Y . Qiao, OpenICL: An\nopen-source framework for in-context learning, in: D. Bollegala, R. Huang,\nA. Ritter (Eds.), Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (V olume 3: System Demonstrations), Asso-\nciation for Computational Linguistics, Toronto, Canada, 2023, pp. 489–\n498. doi:10.18653/v1/2023.acl-demo.47.\nURL https://aclanthology.org/2023.acl-demo.47\n[97] Z. Fu, H. Yang, A. M.-C. So, W. Lam, L. Bing, N. Collier, On the effective-\nness of parameter-efficient fine-tuning (2022). arXiv:2211.15583.\n[98] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, W. Chen, Lora:\nLow-rank adaptation of large language models, ArXiv abs/2106.09685\n(2021).\n[99] A. Edalati, M. S. Tahaei, I. Kobyzev, V . Nia, J. J. Clark, M. Reza-\n47\ngholizadeh, Krona: Parameter efficient tuning with kronecker adapter,\nArXiv abs/2212.10650 (2022).\n[100] Y . Fang, X. Liang, N. Zhang, K. Liu, R. Huang, Z. Chen, X. Fan, H. Chen,\nMol-instructions: A large-scale biomolecular instruction dataset for large\nlanguage models, arXiv preprint arXiv:2306.08018 (2023).\n[101] I. Loshchilov, F. Hutter, Decoupled weight decay regularization, in: Inter-\nnational Conference on Learning Representations, 2019.\n[102] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,\nF. Guzm´an, E. Grave, M. Ott, L. Zettlemoyer, V . Stoyanov, Unsupervised\ncross-lingual representation learning at scale, in: Proceedings of the 58th\nAnnual Meeting of the ACL, ACL, Online, 2020, pp. 8440–8451.\n[103] P. Yang, X. Sun, W. Li, S. Ma, W. Wu, H. Wang, SGM: Sequence gener-\nation model for multi-label classification, in: E. M. Bender, L. Derczyn-\nski, P. Isabelle (Eds.), Proceedings of the 27th International Conference\non Computational Linguistics, Association for Computational Linguistics,\nSanta Fe, New Mexico, USA, 2018, pp. 3915–3926.\nURL https://aclanthology.org/C18-1330\n[104] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, J. Kang, Biobert: a\npre-trained biomedical language representation model for biomedical text\nmining, Bioinformatics 36 (4) (2020) 1234–1240.\n[105] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,\nB. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, et al., Llama: Open and\n48\nefficient foundation language models, arXiv preprint arXiv:2302.13971\n(2023).\n[106] Y . A. Malkov, D. A. Yashunin, Efficient and robust approximate nearest\nneighbor search using hierarchical navigable small world graphs (2018).\narXiv:1603.09320.\n[107] Z. Wu, Y . Wang, J. Ye, L. Kong, Self-adaptive in-context learning: An\ninformation compression perspective for in-context example selection and\nordering.\n49",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8283010721206665
    },
    {
      "name": "Context (archaeology)",
      "score": 0.599247395992279
    },
    {
      "name": "Named-entity recognition",
      "score": 0.5731374025344849
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5434026122093201
    },
    {
      "name": "Machine learning",
      "score": 0.5000102519989014
    },
    {
      "name": "Task (project management)",
      "score": 0.4676622450351715
    },
    {
      "name": "Natural language processing",
      "score": 0.4596945345401764
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4537295997142792
    },
    {
      "name": "Oracle",
      "score": 0.4330725073814392
    },
    {
      "name": "Information retrieval",
      "score": 0.3932141363620758
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Software engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}