{
  "title": "Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection",
  "url": "https://openalex.org/W3184357402",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2744100290",
      "name": "Feng Xinyang",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2362372901",
      "name": "Song Dong-jin",
      "affiliations": [
        "University of Connecticut"
      ]
    },
    {
      "id": "https://openalex.org/A2384332295",
      "name": "Chen, Yuncong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287435947",
      "name": "Chen, Zhengzhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748068332",
      "name": "Ni Jingchao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1690476359",
      "name": "Chen Hai-feng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1867429401",
    "https://openalex.org/W2579718262",
    "https://openalex.org/W3022606336",
    "https://openalex.org/W2341058432",
    "https://openalex.org/W2560474170",
    "https://openalex.org/W1967456674",
    "https://openalex.org/W2963610939",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2163612318",
    "https://openalex.org/W6601575680",
    "https://openalex.org/W2777342313",
    "https://openalex.org/W2122361470",
    "https://openalex.org/W2964232409",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2136637876",
    "https://openalex.org/W2987146532",
    "https://openalex.org/W2540481276",
    "https://openalex.org/W6600281463",
    "https://openalex.org/W2766042539"
  ],
  "abstract": "Detecting abnormal activities in real-world surveillance videos is an\\nimportant yet challenging task as the prior knowledge about video anomalies is\\nusually limited or unavailable. Despite that many approaches have been\\ndeveloped to resolve this problem, few of them can capture the normal\\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\\nseldom explicitly consider the local consistency at frame level and global\\ncoherence of temporal dynamics in video sequences. To this end, we propose\\nConvolutional Transformer based Dual Discriminator Generative Adversarial\\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\\nSpecifically, we first present a convolutional transformer to perform future\\nframe prediction. It contains three key components, i.e., a convolutional\\nencoder to capture the spatial information of the input video clips, a temporal\\nself-attention module to encode the temporal dynamics, and a convolutional\\ndecoder to integrate spatio-temporal features and predict the future frame.\\nNext, a dual discriminator based adversarial training procedure, which jointly\\nconsiders an image discriminator that can maintain the local consistency at\\nframe-level and a video discriminator that can enforce the global coherence of\\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\\nthe prediction error is used to identify abnormal video frames. Thoroughly\\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\\nthe proposed adversarial spatio-temporal modeling framework.\\n",
  "full_text": "Convolutional Transformer based Dual Discriminator\nGenerative Adversarial Networks for Video Anomaly Detection\nXinyang Feng\nColumbia University\nNew York, New York, USA\nxf2143@columbia.edu\nDongjin Songâˆ—\nUniversity of Connecticut\nStorrs, Connecticut, USA\ndongjin.song@uconn.edu\nYuncong Chen\nNEC Laboratories America, Inc.\nPrinceton, New Jersey, USA\nyuncong@nec-labs.com\nZhengzhang Chen\nNEC Laboratories America, Inc.\nPrinceton, New Jersey, USA\nzchen@nec-labs.com\nJingchao Ni\nNEC Laboratories America, Inc.\nPrinceton, New Jersey, USA\njni@nec-labs.com\nHaifeng Chen\nNEC Laboratories America, Inc.\nPrinceton, New Jersey, USA\nhaifeng@nec-labs.com\nABSTRACT\nDetecting abnormal activities in real-world surveillance videos is\nan important yet challenging task as the prior knowledge about\nvideo anomalies is usually limited or unavailable. Despite that many\napproaches have been developed to resolve this problem, few of\nthem can capture the normal spatio-temporal patterns effectively\nand efficiently. Moreover, existing works seldom explicitly con-\nsider the local consistency at frame level and global coherence of\ntemporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative\nAdversarial Networks (CT-D2GAN) to perform unsupervised video\nanomaly detection. Specifically, we first present a convolutional\ntransformer to perform future frame prediction. It contains three\nkey components, i.e., a convolutional encoder to capture the spatial\ninformation of the input video clips, a temporal self-attention mod-\nule to encode the temporal dynamics, and a convolutional decoder\nto integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure,\nwhich jointly considers an image discriminator that can maintain\nthe local consistency at frame-level and a video discriminator that\ncan enforce the global coherence of temporal dynamics, is employed\nto enhance the future frame prediction. Finally, the prediction error\nis used to identify abnormal video frames. Thoroughly empirical\nstudies on three public video anomaly detection datasets,i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the\neffectiveness of the proposed adversarial spatio-temporal modeling\nframework.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Scene anomaly detection ;\nAdversarial learning; Anomaly detection; Neural networks.\nâˆ—Corresponding author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nMM â€™21, October 20â€“24, 2021, Virtual Event, China\nÂ© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8651-7/21/10. . . $15.00\nhttps://doi.org/10.1145/3474085.3475693\nKEYWORDS\nVideo anomaly detection; Generative adversarial networks; Trans-\nformer model; Convolutional neural network; Spatio-temporal mod-\neling\nACM Reference Format:\nXinyang Feng, Dongjin Song, Yuncong Chen, Zhengzhang Chen, Jingchao\nNi, and Haifeng Chen. 2021. Convolutional Transformer based Dual Dis-\ncriminator Generative Adversarial Networks for Video Anomaly Detection.\nIn Proceedings of the 29th ACM International Conference on Multimedia (MM\nâ€™21), October 20â€“24, 2021, Virtual Event, China. ACM, New York, NY, USA,\n9 pages. https://doi.org/10.1145/3474085.3475693\n1 INTRODUCTION\nWith the rapid growth of video surveillance data, there is an in-\ncreasing demand to automatically detect abnormal video sequences\nin the context of large-scale normal (regular) video data. Despite\na substantial amount of research effort has been devoted to this\nproblem [3, 8, 13, 14, 16, 19, 22, 31, 34], video anomaly detection,\nwhich aims to identify the activities that do not conform to reg-\nular patterns in a video sequence, is still a challenging task. This\nis because real-world abnormal video activities can be extremely\ndiverse while the prior knowledge about these anomalies is usually\nlimited or even unavailable.\nWith the assumption that a model can only generalize to data\nfrom the same distribution as the training set, abnormal activities\nin the test set will manifest as deviance from regular patterns. A\ncommon approach to resolve this problem is to learn a model that\ncan capture regular patterns in the normal video clips during the\ntraining stage, and check whether there exists any irregular pattern\nthat diverges from regular patterns in the test video clips. Within\nthis framework, it is crucial to not only represent the regular ap-\npearances but also capture the normal spatio-temporal dynamics to\ndifferentiate abnormal activities from normal activities in a video\nsequence. This serves as an important motivation for our proposed\nmethods.\nEarly studies have used handcrafted features to represent video\npatterns [13, 16, 19, 29]. For instance, Li et al. [13] introduced mix-\ntures of dynamic textures and defined outliers under this model\nas anomalies. These approaches, however, are usually not optimal\nfor video anomaly detection since the features are extracted based\nupon a different objective.\narXiv:2107.13720v1  [cs.CV]  29 Jul 2021\nRecently, deep neural networks are becoming prevalent in video\nanomaly detection, showing superior performance over handcrafted\nfeature based methods. For instance, Hasan et al. [8] developed a\nconvolutional autoencoder (Conv-AE) to model the spatio-temporal\npatterns in a video sequence simultaneously with a 2D CNN. The\ntemporal dynamics, however, are not explicitly considered. To\nbetter cope with the spatio-temporal information in a video se-\nquence, convolutional long short-term memory (LSTM) autoen-\ncoder (ConvLSTM-AE) [17, 27] was proposed to model the spatial\npatterns with fully convolutional networks and encode the tempo-\nral dynamics using convolutional LSTM (ConvLSTM). ConvLSTM,\nhowever, suffers from computational and interpretation issues. A\npowerful alternative for sequence modeling is the self-attention\nmechanism [33]. It has demonstrated superior performance and\nefficiency in many different tasks, e.g., sequence-to-sequence ma-\nchine translation [33], time series prediction [24], autoregressive\nmodel based image generation [23], and GAN-based image synthe-\nsis [39]. However, it has seldom been employed to capture regular\nspatio-temporal patterns in the surveillance videos.\nMore recently, adversarial learning has shown impressive\nprogress on video anomaly detection. For instance, Ravanbakhsh\net al. [ 25] developed a GAN based anomaly detection approach\nfollowing conditional GAN framework [10]. Liu et al. [14] proposed\nan anomaly detection approach based on future frame prediction.\nTang et al. [31] extended this framework by adding a reconstruc-\ntion task. The generative models in these two works were based\non U-Net [26]. Similar to Conv-AE, the temporal dynamics in the\nvideo clip were not explicitly encoded and the temporal coherence\nwas enforced by a loss term on the optical flow. Moreover, the\npotential discriminative information in the form of consistency at\nframe-level and global coherence of temporal dynamics in video\nsequences were not fully considered in previous works.\nIn this paper, to better capture the regular spatio-temporal pat-\nterns and cope with the potential discriminative information at\nframe-level and in video sequences, we propose Convolutional\nTransformer based Dual Discriminator Generative Adversarial Net-\nworks (CT-D2GAN) to perform unsupervised video anomaly detec-\ntion. We first present a convolutional transformer to perform future\nframe prediction. The convolutional transformer is essentially a\nencoder-decoder framework consisting of three key components,\ni.e., a convolutional encoder to capture the spatial patterns of the\ninput video clip, a novel temporal self-attention module adapted for\nvideo temporal modeling that can explicitly encode the temporal\ndynamics, and a convolutional decoder to integrate spatio-temporal\nfeatures and predict the future frame. Because of the temporal\nself-attention module, convolutional transformer can capture the\nunderlying temporal dynamics efficiently and effectively. Next, in\norder to maintain the local consistency of the predicted frame and\nthe global coherence conditioned on the previous frames, we adapt\ndual discriminator GAN to deal with video frames and employ an\nadversarial training procedure to further enhance the prediction\nperformance. Finally, the prediction error is adopted to identify ab-\nnormal video frames. Thoroughly empirical studies on three public\nvideo anomaly detection datasets, i.e., UCSD Ped2, CUHK Avenue,\nand Shanghai Tech Campus, demonstrate the effectiveness of the\nproposed framework and techniques.\n2 RELATED WORK\nThe proposed Convolutional Transformer based Dual Discriminator\nGenerative Adversarial Networks (CT-D2GAN) is closely related\nto deep learning based video anomaly detection and self-attention\nmechanism [33].\nNote that we focus our discussions on methods based on unsu-\npervised settings, which are efficient in generalization without the\ntime-consuming and error-prone process of manual labeling. We\nare aware that there are numerous works on weakly supervised\nor supervised video anomaly detection, e.g., Sultani et al. (2018)\nproposed a deep multiple instance ranking framework using video-\nlevel labels and achieves better performance than convolutional\nauto-encoder (Conv-AE) based method [ 8], but it employs both\nnormal and abnormal video clips for training which is different\nfrom our setting.\nDeep neural networks based video anomaly detection methods\ndemonstrate superior performance over traditional methods based\non handcrafted features. Hasan et al. (2016) developed Conv-AE\nmethod to simultaneously learn the spatio-temporal patterns in a\nvideo with 2D convolutional neural networks by concatenating the\nvideo frames in the channel dimension. The temporal information\nis mixed with the spatial information in the first convolutional layer,\nthus not explicitly encoded. Xu et al. (2017) proposed appearance\nand motion DeepNet (AMDN) to learn video feature representations,\nwhich however still requires a decoupled one-class SVM classifier\napplied on learned representation to generate anomaly score. Dong\net al. (2019) proposed a memory-augmented autoencoder (MemAE)\nthat uses a memory module to constrain the reconstruction.\nMore recently, adversarial learning has demonstrated flexibility\nand impressive performance in multiple video anomaly detection\nstudies. A generative adversarial networks (GANs) based anomaly\ndetection approach [ 25] was developed following cGAN frame-\nwork of image-to-image translation [10]. Specifically, it employs\nimage and optical flow as source domain and target domain, and\nvice versa, and trains cross-channel generation through adversarial\nlearning. The reconstruction error is used to compute anomaly\nscore. The only temporal constraint is imposed by the optical flow\ncalculation. Liu et al. (2018) proposed an anomaly detection ap-\nproach based on future frame prediction in GAN framework and\nU-Net [26]. Similar to Conv-AE, the temporal information is not\nexplicitly encoded and the temporal coherence between neighbor-\ning frames is enforced by a loss term on the optical flow. Tang et al.\n(2020) extended the future frame prediction framework by adding\na reconstruction task. One way to alleviate the temporal encoding\nissue in video spatio-temporal modeling is to use convolutional\nLSTM autoencoder (ConvLSTM-AE) based methods [4, 17, 27, 38],\nwhere the spatial and temporal patterns are encoded with fully\nconvolutional networks and convolutional LSTM, respectively. De-\nspite its popularity, ConvLSTM suffers from issues such as large\nmemory consumption. The complex gating operations add to the\ncomputational cost and complicate the information flow, making\ninterpretation difficult.\nA more effective and efficient alternative for sequence modeling\nis the self-attention mechanism [33], which is essentially an atten-\ntion mechanism relating different positions of a single sequence to\ncompute a representation of the sequence, in which the keys, values,\nand queries are from the same set of features. Some related appli-\ncations include autoregressive model based image generation [23],\nGAN-based image synthesis [39].\nIn this work, based on related works, we introduce the convolu-\ntional transformer by extending the self-attention mechanism to\nvideo sequence modeling and develop a novel self-attention mod-\nule specialized for spatio-temporal modeling in video sequences.\nCompared to existing approaches for video anomaly detection, the\nproposed convolutional transformer model has the advantage of\nbeing able to explicitly and efficiently encode the temporal infor-\nmation in a sequence of feature maps, where the computation of\nattentions can be fully parallelized via matrix multiplications. Based\non the convolutional transformer, a dual discriminator generative\nadversarial networks (D2GAN) approach is developed to further\nenhance the future frame prediction through enforcing local consis-\ntency of the predicted frame and the global coherence conditioned\non the previous frames. Note that the proposed D2GAN differs from\nexisting works on dual discriminator based GAN which have been\napplied to different scenarios [5, 21, 35, 37].\n3 CT-D2GAN\nIn this section, we first introduce the problem formulation and\ninput to our framework. Then, we present the motivation and tech-\nnical details of the proposed CT-D2GAN framework including con-\nvolutional transformer, dual discriminator GAN, the overall loss\nfunction, and lastly the regularity score calculation. An overview\nof the framework is illustrated in Figure 1.\nIn CT-D2GAN, a convolutional transformer is employed to gen-\nerate future frame prediction based on past frames, an image dis-\ncriminator and a video discriminator are used to maintain the local\nconsistency and global coherence.\n3.1 Problem Statement\nGiven an input representation of video clip of length ğ‘‡, i.e., ğ¼ =\n(ğ¼ğ‘¡âˆ’ğ‘‡+1,...,ğ¼ ğ‘¡) âˆˆRâ„Ã—ğ‘¤Ã—ğ‘Ã—ğ‘‡, where â„, ğ‘¤, ğ‘ are the height, width\nand number of channels, we aim to predict the (ğ‘¡ +1)-th frame\nas ^ğ¼ğ‘¡+1 âˆˆ Râ„Ã—ğ‘¤Ã—ğ‘ and identify abnormal activities based upon\nthe prediction error, i.e., ğ‘’MSE,ğ‘¡ = 1\nâ„Â·ğ‘¤Â·ğ‘\nÃğ‘\nğ‘–=1 âˆ¥^ğ¼:,:,ğ‘–,ğ‘¡+1 âˆ’ğ¼:,:,ğ‘–,ğ‘¡+1âˆ¥2\nğ¹,\nwhere ğ¼:,:,ğ‘–,ğ‘¡+1 âˆˆRâ„Ã—ğ‘¤.\n3.2 Input\nAs appearance and motion are two characteristics of video data, it is\ncommon to explicitly incorporate optical flow together with the still\nimages to describe a video sequence [28], e.g. optical flow has been\nemployed to represent video sequences in the cGAN framework [25]\nand used as a motion constraint [14].\nIn this work, we stack image with pre-computed optical flow\nmaps [2, 9] in channel dimension as inputs similar to Simonyan et\nal. [28] for video action recognition and Ravanbakhsh et al. [25] for\nvideo anomaly detection. The optical flow maps consist of a horizon-\ntal component, a vertical component and a magnitude component.\nTo be noted that, the optical flow map is computed from the previ-\nous image and current image, thus does not contain future frame\ninformation. Therefore, the input can be given as ğ¼ âˆˆRâ„Ã—ğ‘¤Ã—4Ã—ğ‘‡,\nand we used 5 consecutive frames as inputs, i.e., ğ‘‡ = 5, similar to\nLiu et al. [14].\n3.3 Convolutional Transformer\nConvolutional transformer is developed to obtain a future frame\nprediction based on past frames. It consists of three key components:\na convolutional encoder to encode spatial information, a temporal\nself-attention module to capture the temporal dynamics, and a\nconvolutional decoder to integrate spatio-temporal features and\npredict future frame.\n3.3.1 Convolutional Encoder. The convolutional encoder [15] is\nemployed to extract spatial features from each frame of the video.\nEach frame of the video is first resized to 256 Ã—256 and then\nfed into the convolutional encoder. The convolutional encoder\nconsists of 5 convolutional blocks. And the convolutional block\nfollows common structure in CNN. All the convolutional kernels\nare set as 3 Ã—3 pixels. For brevity, we denote a convolutional\nlayer with stride ğ‘  and number of filters ğ‘› as Convğ‘ ,ğ‘›, a batch\nnormalization layer as BN, a scaled exponential linear unit [12] as\nSELU, and a dropout operation with dropout ratio ğ‘Ÿ as dropoutğ‘Ÿ.\nThe structure of the convolutional encoder is: [Conv 1,64-SELU-\nBN]-[Conv2,64-SELU-BN-Conv1,64-SELU]-[Conv2,128-SELU-BN-\nConv1,128-SELU]-[Conv2,256-SELU-BN-dropout0.25-Conv1,256-\nSELU]-[Conv2,256-SELU-BN-dropout0.25-Conv1,256-SELU] ,\nwhere each [Â·] represents a convolutional block.\nAt the ğ‘™-th convolutional block conv ğ‘™, ğ¹ğ‘™\nğ‘¡âˆ’ğ‘– âˆˆ Râ„ğ‘™Ã—ğ‘¤ğ‘™Ã—ğ‘ğ‘™,ğ‘– âˆˆ\n[0,...,ğ‘‡ âˆ’1]denotes the input feature maps to the self-attention\nmodule with â„ğ‘™, ğ‘¤ğ‘™, ğ‘ğ‘™ as the height, width, and number of channels,\nrespectively. The temporal dynamics among the spatial feature\nmaps of different time steps will be encoded with temporal self-\nattention module.\n3.3.2 Temporal Self-attention Module. To explicitly encode the tem-\nporal information in the video sequence, we extend self-attention\nmechanism in the transformer model [33] and develop a novel tem-\nporal self-attention module to capture the temporal dynamics of\nthe multi-scale spatial feature maps generated from the convolu-\ntional encoder. This section applies to all layers, thus we omit the\nlayer for clarity. An illustration of the multi-head temporal self-\nattention module is shown in the upper panel of Figure 1. Spatial\nFeature Vector. We first use global average pooling (GAP) to ex-\ntract a feature vector fğ‘¡ from the feature map ğ¹ğ‘¡ extracted in the\nconvolutional encoder. The feature vector in current time step fğ‘¡\nwill be used as part of the query and each historical feature vector\nfğ‘¡âˆ’ğ‘–, ğ‘– âˆˆ[1,ğ‘‡ âˆ’1]will be used as part of the key to index spatial\nfeature maps.\nPositional Encoding. Different from sequence models such as\nLSTM, self-attention does not model sequential information inher-\nently, therefore it is necessary to incorporate temporal positional\ninformation into the model. We generate a positional encoding\nvector PE âˆˆRğ‘‘ğ‘ following [33]:\nPEğ‘,2ğ‘– = sin(ğ‘/100002ğ‘–/ğ‘‘ğ‘)\nPEğ‘,2ğ‘–+1 = cos(ğ‘/100002ğ‘–/ğ‘‘ğ‘)\n(1)\nwhere ğ‘‘ğ‘ denotes the dimension of PE, ğ‘denotes the temporal po-\nsition andğ‘– âˆˆ[0,..., (ğ‘‘ğ‘/2âˆ’1)]denotes the index of the dimension.\nEmpirically, we fix ğ‘‘ğ‘ = 8 in our study.\nTemporal Self-Attention. We concatenate the positional encod-\ning vector with the spatial feature vector for each time step and\n~t+1t+1\nt+1\n~\nt+1t+11,2,â€¦t, 1,2,â€¦t,\n1,2,â€¦t\nDiscriminator2DConv Discriminator3DConvRealorFake RealorFake\nEncoder DecoderTemporal Self-attention Module~\nGenerator: Conv Transformer\nÏƒ\nConvâ€¦\nâ€¦\nConvConv Dcos\nDcos\nHead-1\nHead-h\nâ€¦\nSSG\nPE\nâ€¦\nGAP!\"($)\n&\"'$($)\n&$($)\n()(*)\n()'*(*)\n(*(*)\n()\n()'*\n(*\n+)\n,)(*)\n,)(-)\n()./\n,)./Concat() +)\nFigure 1: The architecture of the proposed CT-D2GAN framework. (Upper panel) The convolutional transformer generator is\nconsisted of a convolutional encoder, a temporal self-attention module, and a convolutional decoder. Multi-head self-attention\nis applied on the feature maps ğ¹ğ‘¡ extracted from convolutional encoder: ğ¹ğ‘¡ is transformed to multi-head feature maps ğ¹(k)\nğ‘¡ via\na convolutional operation; within each head, we apply a global average pooling (GAP) operation on ğ¹(ğ‘˜)\nğ‘¡ to generate a spatial\nfeature vector by aggregating over spatial dimension, and concatenate the positional encoding (PE) vector; we then compare\nthe similarity ğ·cos between query q(ğ‘˜)\nğ‘¡ and memory m(ğ‘˜)\nğ‘¡ feature vectors and generate the attention weights by normalizing\nacross time steps using softmax ğœ; the attended feature map ğ»(â„)\nğ‘¡ is a weighted average of the feature maps at different time\nsteps; the final attended map ğ»MH\nğ‘¡ is the concatenation over all the heads; the final integrated map ğ‘†ğ‘¡ is a weighted average of\nthe query ğ¹MH\nğ‘¡ and the attended feature maps according to a spatial selective gate (SSG). ğ‘†ğ‘¡ is decoded to the predicted future\nframe with the convolutional decoder. (Lower panels) The image discriminator (left) and video discriminator (right) used in\nour dual discriminator GAN framework.\nuse the concatenated vectors as the queries and keys, and the fea-\nture maps as the values in the setting of self-attention mechanism.\nFor each query frame at time ğ‘¡, the current concatenated feature\nvector qğ‘¡ = [fğ‘¡; PE] âˆˆRğ‘ğ‘™+ğ‘‘ğ‘ is used as query, and compared\nto the feature vector of each frame from the input video clip i.e.\nmemory mğ‘¡âˆ’ğ‘– = [fğ‘¡âˆ’ğ‘–; PE]âˆˆ Rğ‘ğ‘™+ğ‘‘ğ‘,ğ‘– âˆˆ[1,...,ğ‘‡ âˆ’1]using cosine\nsimilarity:\nğ·(qğ‘¡,mğ‘¡âˆ’ğ‘–)= qğ‘¡ Â·mğ‘¡âˆ’ğ‘–\nâˆ¥qğ‘¡âˆ¥âˆ¥mğ‘¡âˆ’ğ‘–âˆ¥. (2)\nBased on the similarity between qğ‘¡ and mğ‘¡âˆ’ğ‘–, we can generate\nthe normalized attention weights ğ‘ğ‘¡,ğ‘– âˆˆ R across the temporal\ndimension using a softmax function:\nğ‘ğ‘¡,ğ‘¡âˆ’ğ‘– = exp(ğ›½ğ·(qğ‘¡,mğ‘¡âˆ’ğ‘–))Ã\nğ‘—âˆˆ[1...ğ‘‡âˆ’1]exp(ğ›½ğ·(qğ‘¡,mğ‘¡âˆ’ğ‘—)), (3)\nwhere a positive temperature variableğ›½is introduced to sharpen the\nlevel of focus in the softmax function and is automatically learned\nin the model through a single hidden densely-connected layer with\nthe query as the input.\nThe final attended feature maps ğ»ğ‘¡ are a weighted sum of all\nfeature maps ğ¹ using the attention weights in Eq. (3):\nğ»ğ‘¡ =\nâˆ‘ï¸\nğ‘–âˆˆ[1,...,ğ‘‡âˆ’1]\nğ‘ğ‘¡,ğ‘¡âˆ’ğ‘– Â·ğ¹ğ‘¡âˆ’ğ‘–. (4)\nMulti-head Temporal Self-Attention. Multi-head self-\nattention [33] enables the model to jointly attend to information\nfrom different representation subspaces at different positions. We\nadapt it to spatio-temporal modeling by first mapping the spatial\nfeature maps to ğ‘›â„ = 8 groups, each using 32 1 Ã—1 convolutional\nkernels. For each group of feature maps with dimension ğ‘â„ = 32,\nwe then perform the single head self-attention as described in the\nprevious subsection and generate attended feature maps for head ğ‘˜\nas ğ»(ğ‘˜)\nğ‘¡ :\nğ»(ğ‘˜)\nğ‘¡ =\nâˆ‘ï¸\nğ‘–âˆˆ[1,...,ğ‘‡âˆ’1]\nğ‘(ğ‘˜)\nğ‘¡,ğ‘¡âˆ’ğ‘– Â·ğ¹(ğ‘˜)\nğ‘¡âˆ’ğ‘–, (5)\nwhere ğ¹(ğ‘˜)\nğ‘¡âˆ’ğ‘– âˆˆRâ„ğ‘™Ã—ğ‘¤ğ‘™Ã—ğ‘â„ is the transformed feature map at frame\nğ‘¡âˆ’ğ‘– for head ğ‘˜, ğ‘(ğ‘˜)\nğ‘¡,ğ‘¡âˆ’ğ‘– is the corresponding attention weight. The\nfinal multi-head attended feature map ğ»MH\nğ‘¡ âˆˆRâ„ğ‘™Ã—ğ‘¤ğ‘™Ã—(ğ‘â„Â·ğ‘›â„)is\nthe concatenation of the attended feature maps from all the heads\nalong the channel dimension:\nğ»MH\nğ‘¡ = Concat(ğ»(1)\nğ‘¡ , ...,ğ» (ğ‘›â„)\nğ‘¡ ). (6)\nIn this way, the final attended feature maps not only integrate\nspatial information from the convolutional encoder, but also cap-\nture temporal information from multi-head temporal self-attention\nmechanism.\nSpatial Selective Gate. The aforementioned module extends the\nself-attention mechanism to the temporal modeling of 2D image\nfeature maps, however, it comes with the loss of fine-grained spa-\ntial resolution due to the GAP operation. To compensate this, we\nintroduce spatial selective gate (SSG), which is a spatial attention\nmechanism to integrate the current and historical information.\nThe attended feature maps from the temporal self-attention mod-\nule and the feature maps of the current query are concatenated,\non which we learn a spatial selective gate using a sub-network\nNSSG with structure: Conv1,256-BN-SELU-Conv1,256-BN-SELU-\nConv1,256-BN-SELU-Conv1,256-Conv1,256-Sigmoid. The final out-\nput is a pixel-wise weighted average of the attended maps ğ»MH\nğ‘¡\nand the current queryâ€™s multi-head transformed feature maps\nğ¹MH\nğ‘¡ âˆˆRâ„ğ‘™Ã—ğ‘¤ğ‘™Ã—(ğ‘â„Â·ğ‘›â„), according to ğ‘†ğ‘†ğº:\nğ‘†ğ‘¡ = ğ‘†ğ‘†ğº â—¦ğ¹MH\nğ‘¡ +(1 âˆ’ğ‘†ğ‘†ğº)â—¦ğ»MH\nğ‘¡ (7)\nwhere â—¦denotes element-wise multiplication.\nWe add SSG at each level of temporal self-attention module. As\nthe spatial dimensions are larger at shallow layers and we want\nto include contextual information while preserving the spatial res-\nolution, we use dilated convolution [36] with different dilatation\nfactors at the 4 convolutional blocks in the sub-network NSSG,\nspecifically from conv2 to conv5, the dilation factors are (1,2,4,1),\n(1,2,2,1), (1,1,2,1), (1,1,1,1). Note that SSG is computationally more\nefficient than directly forwarding the concatenated feature maps to\nthe convolutional decoder.\n3.3.3 Convolutional Decoder. The outputs of the temporal self-\nattention module ğ‘†ğ‘¡ are fed into the convolutional decoder. The\nconvolutional decoder predicts the video frame using 4 transposed\nconvolutional layers with stride 2 on the feature maps in a reverse\norder of the convolutional encoder. The fully-scaled feature maps\nthen go through one convolutional layer with 32 filters and one\nconvolutional layer withğ‘filters of size 1 Ã—1 that maps to the same\nsize of channels ğ‘ in the input. In order to predict finer details, we\nutilize the skip connection [26] to connect the spatio-temporally\nintegrated maps at each level of the convolutional encoder to the\ncorresponding level of the convolutional decoder, which allows the\nmodel to further fine-tune the predicted frames.\n3.4 Dual Discriminator GAN\nWe propose a dual discriminator GAN using both an image dis-\ncriminator and a video discriminator to further enhance the future\nframe prediction of convolutional transformer via adversarial train-\ning. The image discriminator ğ·ğ¼ critiques on whether the current\nframe is generated or real just on the basis of one single frame to\nassess the local consistency. The video discriminator ğ·ğ‘‰ performs\ncritique on the prediction conditioned on the past frames to assess\nthe global coherence. Specifically, we stack the past frames with\ncurrent generated or real frame in the temporal dimension and\nthe video discriminator is essentially a video classifier. This idea of\ncombining local and global (contextual) discriminator is similar to\nadversarial image inpainting [37] but is used in a totally different\ncontext.\nThe network structures of the two discriminators are kept the\nsame except that we use 2D operations in image discriminator and\nthe corresponding 3D operations in the video discriminator. We\nuse PatchGAN architecture as described in [10] and use spectral\nnormalization [20] in each convolutional layer. In the 3D version,\nthe stride and kernel size in the temporal dimension were set at 1\nand 2 respectively.\nThe method in Liu et al. [14] is similar to using the image dis-\ncriminator only. Different from the video discriminator in Tulyakov\net al. [32], which applies on the whole synthetic video clip, our\nproposed video discriminator conditions on the real frames.\n3.5 Loss\nFor the adversarial training, we use the Wasserstein GAN with\ngradient penalty (WGAN-GP) setting [ 1, 7]. The generator ğº is\nthe mapping : ğ¼ â†’eğ¼ğ‘¡+1. For discriminators, ğ·ğ‘‰ : (ğ¼,^ğ¼ğ‘¡+1) â†’\nğ‘[(ğ¼,^ğ¼ğ‘¡+1)is real]and ğ·ğ¼ : ^ğ¼ğ‘¡+1 â†’ğ‘[^ğ¼ğ‘¡+1 is real]are video and\nimage discriminators respectively. The GAN loss is:\nğ¿ğ‘ğ‘‘ğ‘£(ğº,ğ·ğ¼,ğ·ğ‘‰)= Eğ¼,eğ¼ğ‘¡+1\n[ğ·ğ‘‰(ğ¼,eğ¼ğ‘¡+1)]âˆ’ Eğ¼,ğ¼ğ‘¡+1 [ğ·ğ‘‰(ğ¼,ğ¼ğ‘¡+1)]\n+ğœ†Eğ¼,^ğ¼ğ‘¡+1\n[(âˆ¥âˆ‡ğ·ğ‘‰(ğ¼,^ğ¼ğ‘¡+1)âˆ¥2 âˆ’1)2]\n+Eeğ¼ğ‘¡+1\n[ğ·ğ¼(eğ¼ğ‘¡+1)]âˆ’ Eğ¼ğ‘¡+1 [ğ·ğ¼(ğ¼ğ‘¡+1)]\n+ğœ†E^ğ¼ğ‘¡+1\n[(âˆ¥âˆ‡ğ·ğ¼(^ğ¼ğ‘¡+1)âˆ¥2 âˆ’1)2]\n(8)\nwhere ^ğ¼ğ‘¡+1 = ğœ–ğ¼ğ‘¡+1 +(1âˆ’ğœ–)eğ¼ğ‘¡+1, ğœ– âˆ¼ğ‘ˆ[0,1]. The penalty coefficient\nğœ†is fixed as 10 in all our experiments.\nIn addition, we consider the pixel-wise ğ¿1 loss of the prediction.\nTherefore the total loss ğ¿is:\nğ¿= ğ¿ğ‘ğ‘‘ğ‘£ +âˆ¥ğ¼ğ‘¡+1 âˆ’eğ¼ğ‘¡+1âˆ¥1 (9)\nWe trained our models on each dataset separately by minimizing\nthe loss above using ADAM [11] algorithm with learning rate 0.0002\nand a batch size of 5.\n3.6 Regularity Score\nA regularity score based on the prediction error ğ‘’ğ‘¡ is calculated for\neach video frame:\nğ‘Ÿğ‘’ğ‘¡ = 1 âˆ’ ğ‘’ğ‘¡ âˆ’minğœğ‘’ğœ\nmaxğœğ‘’ğœ âˆ’minğœğ‘’ğœ\n(10)\nIn Hasan et al. [8], ğ‘’ğ‘¡ is the frame-wise reconstruction ğ‘’MSE,ğ‘¡. In\nLiu et al. [ 14], ğ‘’ğ‘¡ is equivalently negative frame-wise prediction\nTable 1: Video anomaly detection datasets details\nDataset Total # frames/clips Training # frames/clips Testing # frames/clips Anomaly Types\nUCSD Ped2 4,560/28 2,550/16 2,010/12 biker, skater, vehicle\nCUHK Avenue 30,652/37 15,328/16 15,324/21 running, loitering, object throwing\nShanghaiTech 315,307/437 274,516/330 40,791/107 biker, skater, vehicle, sudden motion\nPSNR (Peak Signal to Noise Ratio): PSNR = 10log10\nmax(eğ¼ğ‘¡)\nğ‘’MSE,ğ‘¡ . In\nthis study, we use similar setting to the two methods above with:\nğ‘’ğ‘¡ = log10ğ‘’MSE,ğ‘¡.\n4 EXPERIMENTS\nIn this section, we first introduce the three public datasets used\nin our experiments, which follow the same setup as other similar\nunsupervised video anomaly detection studies. Then, we report the\nvideo anomaly detection performance and comparison with other\nmethods. Finally, we perform ablation studies to demonstrate the\ncontribution of each component and interpret the results based on\nthe proposed CT-D2GAN.\n4.1 Datasets\nWe evaluate our framework on three widely used public video anom-\naly detection datasets, i.e., UCSD Ped2 dataset [13] 1, CUHK Avenue\ndataset [16] 2, and ShanghaiTech Campus (SH-Tech) dataset [18] 3.\nWe describe the dataset-specific characteristics and the effects on\nvideo anomaly detection performance, some details can be found\nin Table 1:\n4.1.1 UCSD Ped2. UCSD Ped2 includes pedestrians, vehicles\nlargely moving in parallel to the camera plane.\n4.1.2 CUHK Avenue. CUHK Avenue includes pedestrians and ob-\njects both moving parallel to or toward/away from the camera.\nSlight camera motion is present in the dataset. Some of the anom-\nalies are staged actions.\n4.1.3 ShanghaiTech. Different from the other datasets, the Shang-\nhaiTech dataset is a multi-scene dataset (13 scenes), and includes\npedestrians, vehicles, and sudden motions, and the ratios of each\nscene in the training set and test set can be different.\n4.2 Evaluation\nThe model was trained and evaluated on a system with an NVIDIA\nGeForce 1080 Ti GPU and implemented with PyTorch. To measure\nthe effectiveness of our proposed CT-D2GAN framework for video\nanomaly detection, we report the area under the receiver operating\ncharacteristics (ROC) curvei.e., AUC. Specifically, AUC is calculated\nby comparing the frame-level regularity scores with frame-level\nground truth labels.\n1http://www.svcl.ucsd.edu/projects/anomaly/dataset.html\n2http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html\n3https://github.com/StevenLiuWen/sRNN_TSC_Anomaly_Detection#\nshanghaitechcampus-anomaly-detection-dataset\nTable 2: Frame-level video anomaly detection performance\n(AUC).\nMethod UCSD Ped2 CUHK SH-Tech\nMPPCA+SF [19] 61.3 - -\nMDT [13, 19] 82.9 - -\nConv-AE [8] 85.0 â€  80.0 â€  60.9 â€ \n3D Conv [40] 91.2 80.9 -\nStacked RNN [18] 92.2 81.7 68.0\nConvLSTM-AE [17] 88.1 77.0 -\nmemAE [6] 94.1 83.3 71.2\nmemNormality [22] 97.0 88.5 70.5\nClusterAE [3] 96.5 86.0 73.3\nAbnormalGAN [25] 93.5 - -\nFrame prediction [14] 95.4 85.1 72.8\nPred+Recon [31] 96.3 85.1 73.0\nCT-D2GAN 97.2 85.9 77.7\nâ€ Evaluated in [14];\n-: Not evaluated in the study.\nOrdered in publication year. The best performance in\neach dataset is highlighted in boldface.\n4.3 Video Anomaly Detection\nTo demonstrate the effectiveness of our proposed CT-D2GAN frame-\nwork for video anomaly detection, we compare it against 12 dif-\nferent baseline methods. Among those, MPPCA (mixture of prob-\nabilistic principal component analyzers) + SF (social force) [ 19],\nMDT (mixture of dynamic textures) [13, 19] are handcrafted fea-\nture based methods; Conv-AE [8], 3D Conv [40], Stacked RNN [18],\nand ConvLSTM-AE [17] are encoder-decoder based approaches;\nMemAE [ 6], MemNormality [ 22] and ClusterAE [ 3] are recent\nencoder-decoder based methods enhanced with memory mod-\nule or clustering; AbnormalGAN [25], Frame prediction [14], and\nPred+Recon [31] are methods based on adversarial training.\nTable 2 shows the frame-level video anomaly detection perfor-\nmance (AUC) of various approaches. We observed that encoder-\ndecoder based approaches in general outperform handcrafted fea-\nture based methods. This is because the handcrafted features\nare usually extracted based upon a different objective and thus\ncan be sub-optimal. Within encoder-decoder based approaches,\nConvLSTM-AE outperforms Conv-AE since it can better capture\ntemporal information. We also notice that adversarial training based\nmethods perform better than most baseline methods. Finally, our\nFigure 2: Examples of video anomaly detection. The blue lines in the line graphs delineate frame-level regularity scores. The\ngreen and red shaded segments in the line graphs indicate the ground truth normal and abnormal video segments respectively.\nThe frames in the green boxes are regular frames from the regular video segments, the frames in the red boxes are abnormal\nframes from abnormal video segments. The abnormal objects are annotated.\nproposed CT-D2GAN framework achieves the best performance\non UCSD Ped2 and SH-Tech, and close to the best performance in\nCUHK [22]. This is because our proposed model can not only cap-\nture the spatio-temporal patterns explicitly and effectively through\nconvolutional transformer but also leverage the dual discrimina-\ntor GAN based adversarial training to maintain local consistency\nat frame-level and global coherence in video sequences. Recent\nmemory or clustering enhanced methods [3, 6, 22] show good per-\nformance and is orthogonal to our proposed framework and can\nintegrate with our proposed framework in future work to further\nimprove performance. Examples of video anomaly detection results\noverlaid on the abnormal activity ground truth of all three datasets\nare shown in Figure 2, along with example video frames from the\nregular and abnormal video segments.\nDue to the multi-scene nature of SH-Tech dataset, we also an-\nalyzed the most ample single scene that constitutes 25% (83/330\nclips) of training set and 32% (34/107 clips) of test set, the AUC\nis 87.5 which is much better than the overall dataset and reach\nsimilar level with other single-scene datasets. This could imply that\ngeneralizing to less ample scenes is still a challenging task given\nunbalanced training set.\nThanks to the convolutional transformer architecture and opti-\nmizations including spatial selective gate, our model is computa-\ntionally efficient. At inference time, our model runs at 45 FPS on\none NVIDIA GeForce 1080 Ti GPU.\nTable 3: Video anomaly detection performance under differ-\nent ablation settings on UCSD Ped2 dataset.\nAblation setting AUC\nConv Transformer 94.2\nConv Transformer + image discriminator 95.7\nConv Transformer + video discriminator 96.9\nU-Net + dual discriminator 95.5\nCT-D2GAN 97.2\n4.4 Ablation Studies\nTo understand how each component contributes to the anomaly\ndetection task, we conducted ablation studies with different set-\ntings: (1) convolutional transformer only without the adversarial\ntraining (Conv Transformer), (2) Conv Transformer with image\ndiscriminator only, (3) Conv Transformer with video discriminator\nonly, (4) U-Net based generator (as has been utilized in image-to-\nimage translation [10] and video anomaly detection [14]) with dual\ndiscriminator, and compare with our full CT-D2GAN model. The\nperformance comparison can be found in Table 3. We observed\nthat adversarial training can enhance the performance for anom-\naly detection, either with the image discriminator or the video\ndiscriminator. Video discriminator alone achieves nearly similar\nperformance as using dual discriminator, but we observed the loss\ndecreased faster when combined with image discriminator. Using\nimage discriminator alone was not as effective, and the loss was\nless stable. Finally, we observed that CT-D2GAN achieved superior\nperformance than U-Net with dual discriminator, suggesting that\nconvolutional transformer can better capture the spatio-temporal\ndynamics and thus can make a more accurate detection.\n4.5 Interpretation\nWe illustrate an example of predicted future frame gğ‘¡+1 and com-\npare it with the previous frameğ‘¡ and the ground truth future frame\nğ‘¡+1 in Figure 3. The prediction performance is poor for the anomaly\n(red box). And also we noted that the model is able to capture the\ntemporal dynamics by predicting the future behavior in normal\npart of the image (green box).\nSelf-attention weights under perturbation. It is not straightfor-\nward to directly interpret the temporal self-attention weight vector,\nas temporal self-attention is applied to an abstract representation\nof the video. Therefore, to further investigate the effectiveness of\ntemporal self-attention, we perturb two frames of the video and\nrun the inference on this perturbed video segment. For one frame\n(Figure 4, red), we added a random Gaussian noise with zero mean\nFigure 3: An example showing the future frame prediction\nin the normal part of the image (green box, pedestrian in this\ncase) where we observe the model capturing the dynamics of\nthe behavior, and abnormal part of the image (red box, bicy-\ncle in this case) where there is large prediction error. From\nleft to right, we show the last frame in the input video clip\n(ğ‘¡), the predicted future frame gğ‘¡+1, and the ground truth fu-\nture frame ğ‘¡+1.\nFigure 4: Temporal self-attention weights in perturbed video\nclip.\nand 0.1 standard deviation to the image to simulate the deteriora-\ntion in video quality; for another frame (Figure 4, purple), we scaled\nthe optical flow maps by 0.9 to simulate the frame rate distortion.\nWe plot the temporal attention weights for the frame right after\nthe two perturbed frames in Figure 4. The weights assigned to the\nperturbed frames are clearly lower than the others, implying less\ncontribution to the attended map. This suggests that self-attention\nmodule can adaptively select relevant feature maps and is robust\nto input noise.\n5 CONCLUSIONS\nIn this paper, we developed Convolutional Transformer based Dual\nDiscriminator Generative Adversarial Networks (CT-D2GAN) to\nperform unsupervised video anomaly detection. The convolutional\ntransformer which consists of three components, i.e., a convolu-\ntional encoder to capture the spatial patterns of the input video clip,\na temporal self-attention module to encode the temporal dynamics,\nand a convolutional decoder to integrate spatio-temporal features,\nwas employed to perform future frame prediction. A dual discrim-\ninator based adversarial training approach was used to maintain\nthe local consistency of the predicted frame and the global coher-\nence conditioned on the previous frames. Thorough experiments on\nthree widely used video anomaly detection datasets demonstrate\nthat our proposed CT-D2GAN is able to detect anomaly frames\nwith superior performance.\nREFERENCES\n[1] Martin Arjovsky, Soumith Chintala, and LÃ©on Bottou. 2017. Wasserstein gen-\nerative adversarial networks. In International Conference on Machine Learning\n(ICML). PMLR, 214â€“223.\n[2] Thomas Brox, AndrÃ©s Bruhn, Nils Papenberg, and Joachim Weickert. 2004. High\naccuracy optical flow estimation based on a theory for warping. In European\nConference on Computer Vision (ECCV) . Springer, 25â€“36.\n[3] Yunpeng Chang, Zhigang Tu, Wei Xie, and Junsong Yuan. 2020. Clustering\nDriven Deep Autoencoder for Video Anomaly Detection. In European Conference\non Computer Vision (ECCV) . Springer, 329â€“345.\n[4] Yong Shean Chong and Yong Haur Tay. 2017. Abnormal event detection in\nvideos using spatiotemporal autoencoder. In International Symposium on Neural\nNetworks (ISNN) . Springer, 189â€“196.\n[5] Fei Dong, Yu Zhang, and Xiushan Nie. 2020. Dual Discriminator Generative\nAdversarial Network for Video Anomaly Detection. IEEE Access 8 (2020), 88170â€“\n88176.\n[6] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour,\nSvetha Venkatesh, and Anton van den Hengel. 2019. Memorizing normality\nto detect anomaly: Memory-augmented deep autoencoder for unsupervised\nanomaly detection. In IEEE International Conference on Computer Vision (ICCV) .\nIEEE, 1705â€“1714.\n[7] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and\nAaron C Courville. 2017. Improved training of Wasserstein GANs. In Advances\nin Neural Information Processing Systems (NIPS) . 5767â€“5777.\n[8] Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K Roy-Chowdhury, and\nLarry S Davis. 2016. Learning temporal regularity in video sequences. In IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) . IEEE, 733â€“742.\n[9] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy,\nand Thomas Brox. 2017. Flownet 2.0: Evolution of optical flow estimation with\ndeep networks. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR). IEEE, 2462â€“2470.\n[10] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-\nImage Translation with Conditional Adversarial Networks. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) . IEEE, 5967â€“5976.\n[11] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-\nmization. International Conference on Learning Representations (ICLR) (2015).\n[12] GÃ¼nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter.\n2017. Self-normalizing neural networks. In Advances in Neural Information\nProcessing Systems (NIPS) . 971â€“980.\n[13] Weixin Li, Vijay Mahadevan, and Nuno Vasconcelos. 2014. Anomaly detection\nand localization in crowded scenes. IEEE Transactions on Pattern Analysis and\nMachine Intelligence 36, 1 (2014), 18â€“32.\n[14] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. 2018. Future Frame\nPrediction for Anomaly Detection â€“ A New Baseline. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) . IEEE, 6536â€“6545.\n[15] Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully convolutional\nnetworks for semantic segmentation. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) . IEEE, 3431â€“3440.\n[16] Cewu Lu, Jianping Shi, and Jiaya Jia. 2013. Abnormal event detection at 150 FPS\nin MATLAB. In IEEE International Conference on Computer Vision (ICCV) . IEEE,\n2720â€“2727.\n[17] Weixin Luo, Wen Liu, and Shenghua Gao. 2017. Remembering history with\nconvolutional LSTM for anomaly detection. In IEEE International Conference on\nMultimedia and Expo (ICME) . IEEE, 439â€“444.\n[18] Weixin Luo, Wen Liu, and Shenghua Gao. 2017. A revisit of sparse coding based\nanomaly detection in stacked RNN framework. IEEE International Conference on\nComputer Vision (ICCV) 1, 2 (2017), 3.\n[19] Vijay Mahadevan, Weixin Li, Viral Bhalodia, and Nuno Vasconcelos. 2010. Anom-\naly detection in crowded scenes. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) . IEEE, 1975â€“1981.\n[20] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. 2018.\nSpectral Normalization for Generative Adversarial Networks. In International\nConference on Learning Representations (ICLR) .\n[21] Tu Nguyen, Trung Le, Hung Vu, and Dinh Phung. 2017. Dual discriminator\ngenerative adversarial nets. In Advances in neural information processing systems\n(NIPS). 2670â€“2680.\n[22] Hyunjong Park, Jongyoun Noh, and Bumsub Ham. 2020. Learning Memory-\nguided Normality for Anomaly Detection. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) . 14372â€“14381.\n[23] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,\nAlexander Ku, and Dustin Tran. 2018. Image Transformer. In International\nConference on Machine Learning (ICML) . PMLR, 4052â€“4061.\n[24] Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison W.\nCottrell. 2017. A Dual-Stage Attention-Based Recurrent Neural Network for\nTime Series Prediction. In International Joint Conference on Artificial Intelligence\n(IJCAI). 2627â€“26332.\n[25] Mahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lucio Marcenaro, Carlo\nRegazzoni, and Nicu Sebe. 2017. Abnormal Event Detection in Videos using\nGenerative Adversarial Nets. IEEE International Conference on Image Processing\n(ICIP) (2017), 1577â€“1581.\n[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional\nnetworks for biomedical image segmentation. In International Conference on\nMedical Image Computing and Computer-Assisted Intervention (MICCAI) . Springer,\n234â€“241.\n[27] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and\nWang-chun Woo. 2015. Convolutional LSTM network: A machine learning ap-\nproach for precipitation nowcasting. InAdvances in Neural Information Processing\nSystems (NIPS) . 802â€“810.\n[28] Karen Simonyan and Andrew Zisserman. 2014. Two-stream convolutional net-\nworks for action recognition in videos. In Advances in Neural Information Pro-\ncessing Systems (NIPS) . 568â€“576.\n[29] Dongjin Song and Dacheng Tao. 2010. Biologically Inspired Feature Manifold for\nScene Classification. IEEE Transactions on Image Processing 19, 1 (2010), 174â€“184.\n[30] Waqas Sultani, Chen Chen, and Mubarak Shah. 2018. Real-World Anomaly\nDetection in Surveillance Videos. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) . IEEE, 6479â€“6488.\n[31] Yao Tang, Lin Zhao, Shanshan Zhang, Chen Gong, Guangyu Li, and Jian Yang.\n2020. Integrating prediction and reconstruction for anomaly detection. Pattern\nRecognition Letters 129 (2020), 123â€“130.\n[32] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. 2018. MoCoGAN:\nDecomposing Motion and Content for Video Generation. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) . IEEE, 1526â€“1535.\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you\nneed. In Advances in Neural Information Processing Systems (NIPS) . 6000â€“6010.\n[34] Dan Xu, Yan Yan, Elisa Ricci, and Nicu Sebe. 2017. Detecting anomalous events\nin videos by learning deep representations of appearance and motion. Computer\nVision and Image Understanding 156 (2017), 117â€“127.\n[35] Han Xu, Pengwei Liang, Wei Yu, Junjun Jiang, and Jiayi Ma. 2019. Learning\na Generative Model for Fusing Infrared and Visible Images via Conditional\nGenerative Adversarial Network with Dual Discriminators.. In International\nJoint Conference on Artificial Intelligence (IJCAI) . 3954â€“3960.\n[36] Fisher Yu and Vladlen Koltun. 2016. Multi-scale context aggregation by dilated\nconvolutions. International Conference on Learning Representations (ICLR) (2016).\n[37] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang. 2018.\nGenerative Image Inpainting With Contextual Attention. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) . IEEE, 5505â€“5514.\n[38] Chuxu Zhang, Dongjin Song, Yuncong Chen, Xinyang Feng, Cristian Lumezanu,\nWei Cheng, Jingchao Ni, Bo Zong, Haifeng Chen, and V. Nitesh Chawla. 2019. A\nDeep Neural Network for Unsupervised Anomaly Detection and Diagnosis in\nMultivariate Time Series Data. In Association for the Advancement of Artificial\nIntelligence (AAAI). AAAI, 1409â€“1416.\n[39] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. 2019. Self-\nattention generative adversarial networks. InInternational Conference on Machine\nLearning (ICML) . PMLR, 7354â€“7363.\n[40] Yiru Zhao, Bing Deng, Chen Shen, Yao Liu, Hongtao Lu, and Xian-Sheng Hua.\n2017. Spatio-Temporal AutoEncoder for Video Anomaly Detection. In ACM\nInternational Conference on Multimedia (ACM MM) . ACM, 1933â€“1941.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I140172145",
      "name": "University of Connecticut",
      "country": "US"
    }
  ],
  "cited_by": 82
}