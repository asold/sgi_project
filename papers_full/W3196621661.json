{
  "title": "Towards Transferable Adversarial Attacks on Vision Transformers",
  "url": "https://openalex.org/W3196621661",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2103019599",
      "name": "Zhipeng Wei",
      "affiliations": [
        "Intelligent Health (United Kingdom)",
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2108246486",
      "name": "Jingjing Chen",
      "affiliations": [
        "Fudan University",
        "Intelligent Health (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2946506291",
      "name": "Micah Goldblum",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2169345886",
      "name": "Zuxuan Wu",
      "affiliations": [
        "Intelligent Health (United Kingdom)",
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2122996091",
      "name": "Tom Goldstein",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2693570096",
      "name": "Yu-Gang Jiang",
      "affiliations": [
        "Intelligent Health (United Kingdom)",
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2103019599",
      "name": "Zhipeng Wei",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2108246486",
      "name": "Jingjing Chen",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2946506291",
      "name": "Micah Goldblum",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2169345886",
      "name": "Zuxuan Wu",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2122996091",
      "name": "Tom Goldstein",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2693570096",
      "name": "Yu-Gang Jiang",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6747220948",
    "https://openalex.org/W2926400157",
    "https://openalex.org/W6698183232",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W6719080892",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W6722226382",
    "https://openalex.org/W4200632883",
    "https://openalex.org/W2274287116",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3134155560",
    "https://openalex.org/W2991057431",
    "https://openalex.org/W3034176567",
    "https://openalex.org/W2791683151",
    "https://openalex.org/W6755430541",
    "https://openalex.org/W4214669216",
    "https://openalex.org/W4376983087",
    "https://openalex.org/W3173053527",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W3200643978",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4300427962",
    "https://openalex.org/W1945616565",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W2896078964",
    "https://openalex.org/W3137894200",
    "https://openalex.org/W4226146561",
    "https://openalex.org/W3185095134",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W4312414395",
    "https://openalex.org/W2998254302",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W4311304289",
    "https://openalex.org/W2460937040",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W3006076803",
    "https://openalex.org/W2950864148",
    "https://openalex.org/W2976752987",
    "https://openalex.org/W2963542245",
    "https://openalex.org/W2774644650",
    "https://openalex.org/W2570685808",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W1562353621",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W3130943264",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W4287905175",
    "https://openalex.org/W4287122830",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2996140774",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2969542116",
    "https://openalex.org/W3161120562",
    "https://openalex.org/W2962847335",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W2963389226",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3159732141",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3137963805"
  ],
  "abstract": "Vision transformers (ViTs) have demonstrated impressive performance on a series of computer vision tasks, yet they still suffer from adversarial examples. In this paper, we posit that adversarial attacks on transformers should be specially tailored for their architecture, jointly considering both patches and self-attention, in order to achieve high transferability. More specifically, we introduce a dual attack framework, which contains a Pay No Attention (PNA) attack and a PatchOut attack, to improve the transferability of adversarial samples across different ViTs. We show that skipping the gradients of attention during backpropagation can generate adversarial examples with high transferability. In addition, adversarial perturbations generated by optimizing randomly sampled subsets of patches at each iteration achieve higher attack success rates than attacks using all patches. We evaluate the transferability of attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The results of these experiments demonstrate that the proposed dual attack can greatly boost transferability between ViTs and from ViTs to CNNs. In addition, the proposed method can easily be combined with existing transfer methods to boost performance.",
  "full_text": "Towards Transferable Adversarial Attacks on Vision Transformers\nZhipeng Wei1,2*, Jingjing Chen1,2*, Micah Goldblum3, Zuxuan Wu1,2†\nTom Goldstein3, Yu-Gang Jiang1,2†\n1Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University\n2Shanghai Collaborative Innovation Center on Intelligent Visual Computing\n3Department of Computer Science, University of Maryland\nzpwei21@m.fudan.edu.cn, chenjingjing@fudan.edu.cn, goldblumcello@gmail.com,\nzxwu@fudan.edu.cn, tomg@cs.umd.edu, ygj@fudan.edu.cn\nAbstract\nVision transformers (ViTs) have demonstrated impressive\nperformance on a series of computer vision tasks, yet they\nstill suffer from adversarial examples. In this paper, we\nposit that adversarial attacks on transformers should be spe-\ncially tailored for their architecture, jointly considering both\npatches and self-attention, in order to achieve high transfer-\nability. More specifically, we introduce a dual attack frame-\nwork, which contains a Pay No Attention (PNA) attack and\na PatchOut attack, to improve the transferability of adversar-\nial samples across different ViTs. We show that skipping the\ngradients of attention during backpropagation can generate\nadversarial examples with high transferability. In addition,\nadversarial perturbations generated by optimizing randomly\nsampled subsets of patches at each iteration achieve higher at-\ntack success rates than attacks using all patches. We evaluate\nthe transferability of attacks on state-of-the-art ViTs, CNNs\nand robustly trained CNNs. The results of these experiments\ndemonstrate that the proposed dual attack can greatly boost\ntransferability between ViTs and from ViTs to CNNs. In ad-\ndition, the proposed method can easily be combined with ex-\nisting transfer methods to boost performance.\nIntroduction\nVision Transformers (ViTs) (Dosovitskiy et al. 2020) have\nattained excellent performance compared to state-of-the-art\nCNNs on standard image recognition tasks. However, ViTs\nare still vulnerable to security threats via adversarial exam-\nples (Goodfellow, Shlens, and Szegedy 2014; Wang et al.\n2021a; Chen et al. 2021a; Wei et al. 2020), which are nearly\nindistinguishable from natural images while containing per-\nturbations that result in incorrect predictions. It is known that\na model of unknown structure can be attacked using adver-\nsarial images crafted with a different “surrogate” model (Liu\net al. 2016). This cross-model transferability property of ad-\nversarial examples makes it feasible to attack a “black-box”\nmodel without knowing its architecture or other properties.\nCross-model transferability is a well-studied phenomenon\nfor CNNs (Xie et al. 2019; Dong et al. 2018; Wei et al.\n2021). High-performance transfer attacks typically employ\n*These authors contributed equally.\n†Correspondence to: Zuxuan Wu, Yu-Gang Jiang.\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ndata augmentation and advanced gradient calculations to\nprevent perturbations from over-fitting to the model, as this\nwould diminish their success when transferring to black-box\nmodels. In contrast, relatively little is known about the trans-\nferability properties of attacks crafted on ViTs, and extend-\ning existing approaches that work well on CNNs to trans-\nformers is non-trivial due to significant structural differ-\nences. More specifically, ViTs receive a sequence of flat-\ntened patches from images as inputs and use a series of\nmulti-headed self-attention (MSA) layers to learn relation-\nships between patches. The use of standard attacks without\nconsidering these unique architectural features will result in\nsuboptimal results, and give the user an inaccurate sense of\nthe adversarial vulnerability of the transformer model class.\nIn light of these structural differences, we aim to gener-\nate highly transferable adversarial examples using white-box\nViTs as proxy models to attack different black-box ViTs,\nnormally trained CNNs, and robustly trained CNNs. In par-\nticular, we introduce a dual attack method tailored for the ar-\nchitecture of ViTs—attacking the attention mechanism and\npatches simultaneously, both of which are the core compo-\nnents of popular transformer architectures. In particular, we\nuse a Pay No Attention (PNA) attack and a PatchOut attack\nto manipulate the attention mechanism and image features in\nparallel. The PNA attack improves adversarial transferabil-\nity by treating the attention weights computed on the for-\nward pass as constants. In other words, it does not propagate\nthrough the branch of the computation graph that produces\nthe attention weights, as illustrated in Figure 1(a). This pre-\nvents patches from strongly interacting, as measured by the\nShapley interaction index (Shapley 1988), which is known\nto help boost adversarial transferability (Wang et al. 2020).\nIn addition, the PatchOut attack randomly samples a sub-\nset of patches to receive updates on each iteration of the at-\ntack crafting process. This is akin to using dropout (Hinton\net al. 2012) on perturbation patches, and helps to combat\nover-fitting. It also shares similarities with random feature\nselection in Random Forests (Breiman 2004), Dropout and\nDiversity Input (DI) (Xie et al. 2019). Besides, we integrate\nthe L2 norm (Chen et al. 2021b) into our dual attack to prefer\na large distance from the benign sample.\nTo validate the effectiveness of our approach, and to il-\nlustrate how the gradients of attention weights impair ad-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2668\nEmbedded \nPatches\nAttention\nMap\n⊗\n⊗\nBackpropagation\nWQ WK WV\n(a) Self-attention Block\n42.47\n36.69\n38.09\n32.30\n39.96\n34.49\n35.34\n29.92 MSA 1-4MSA 5-8MSA 9-12 (b) Bypass Attention\nFigure 1: (a): An illustration of the self-attention block. The\nattention map calculated by Q and K is dropped in gradi-\nent backpropagation (red dashed box). (b): Adversarial ex-\namples are generated by using gradients that bypass atten-\ntion (no gradients pass through the dotted red box in Figure\n1(a)). We split the 12 attention blocks of ViT-B/16 into three\nchunks: MSA 1-4, 5-8 and 9-12. We consider the 8 differ-\nent propagation paths that include/exclude every combina-\ntion of these three blocks. The rightmost path skips all at-\ntention units and achieves the best black-box attack success\nrate (labeled in green) while the red path through all atten-\ntion blocks achieves the worst black-box attack success rate\n(labeled in red). The attacks are crafted by BIM on 2,000\nimages under perturbations with L∞-norm bounded above\nby ϵ = 16. The number of iterations is10.\nversarial transferability, we conduct two toy experiments us-\ning the BIM (Kurakin, Goodfellow, and Bengio 2016) attack\non the white-box model ViT-B/16 (Dosovitskiy et al. 2020)\nusing the ImageNet validation dataset. Figure 1(b) shows\nthe results of PNA. We observe that the attack success rate\n(ASR) decreases as more attention gradients are used during\nbackpropagation. Bypassing all gradients of attention (the\ngreen path) improves ASR from 29.92% to 42.47%. Figure 2\nshows the results of PatchOut, where we randomly select ten\npatches as one input pattern. We call such a sparse perturba-\ntion a “ten-patch.” We see that stacking multiple ten-patches\nachieves a higher ASR than using perturbations produced\nby optimizing on the whole image at once. This observation\ndemonstrates that stacking perturbations from diverse input\npatterns can help alleviate the over-fitting problem.\nWe further conduct extensive experiments on the Ima-\ngeNet dataset (Russakovsky et al. 2015). We demonstrate\nthat the proposed dual attack method significantly improves\nASR against black-box ViTs, normally trained CNNs, and\nrobustly trained CNNs. Compared to MI (Dong et al. 2018)\nand SGM (Wu et al. 2020a), the dual attack improves ASR\n1 2 3 4 5 6 7 8 9 10\nNumber of ten-patches\n0\n20\n40\n60\n80\n100Attack success rate (%)\nOptimize ten-patches \nOptimize the image\nFigure 2: Attack success rate against black-box models with\nvarying number of perturbation updates. We consider us-\ning either ten-patch updates or whole-image updates. The\nblue line generates adversarial perturbations by accumulat-\ning multiple ten-patch perturbations, which are generated\nby optimizing a single ten-patch at each stage. The orange\nline represents perturbations generated using whole-image\nupdates. We average the attack success rate over numerous\nblack-box models: DeiT-B, LeViT-256, CaiT-S-24, PiT-B,\nConViT-B, TNT-S and Visformer-S.\nby 15.86%, 27.68%, 23.52% on average for ViTs, normally\ntrained CNNs and robustly trained CNNs respectively based\non the adversarial transferability. Our results demonstrate\nthe feasibility of using white-box ViTs to attack other black-\nbox models, and they further demonstrate the vulnerability\nof seemingly robustly trained CNNs. We briefly summarize\nour primary contributions as follows:\n• We study the attention mechanism in ViTs and propose\nthe Pay No Attention (PNA) attack to craft adversarial\nexamples without backpropping through attention. PNA\nis applicable to any gradient-based attack method and\nany attention-based neural network.\n• We study how stacking perturbations using random sub-\nsets of patches can improve attack transferability, and we\npropose the PatchOut Attack to generate adversarial ex-\namples using different patches as input at each iteration.\n• We conduct comprehensive transfer attack experiments\nusing 4 different white-box ViTs against 8 black-box\nViTs, 4 CNNs, and 3 robustly trained CNNs, showing\nthat the proposed dual attack can greatly improve adver-\nsarial transferability and indicating that it can be com-\nbined with existing methods to further improve perfor-\nmance.\nRelated Work\nAdversarial attacks (Yan, Wei, and Li 2020) are often stud-\nied under white- and black-box threat models. The white-\nbox setting allows the adversary fully access to victim mod-\nels, while the black-box setting only permits access to the\noutput of a victim. Hence, the black-box setting is substan-\ntially more challenging. Transfer-based attacks generate ad-\nversarial examples on white-box models with the intent that\nthe attacked samples will also be effective against black-box\nmodels. In this section, we review existing work on transfer-\nbased attacks as well as on ViTs.\n2669\nTransfer-based Attacks on CNNs\nFast Gradient Sign Method (FGSM) (Goodfellow, Shlens,\nand Szegedy 2014) and Basic Iterative Method (BIM)\n(Kurakin, Goodfellow, and Bengio 2017) are fundamental\nwhite-box attack methods which are commonly used for\ntransfer-based attacks. FGSM performs a one-step update in\nthe direction of the sign of the gradient to maximizes loss.\nBIM applies FGSM several times iteratively with a small\nstep size. Due to the fine-grained updates, BIM often gen-\nerates more powerful adversarial examples than FGSM for\nwhite-box attacks, but BIM simultaneously achieves lower\ntransferability than FGSM because of the over-fitting prob-\nlem (Kurakin, Goodfellow, and Bengio 2016). To overcome\nthis problem, several efforts have been made to improve ad-\nversarial transferability. These efforts can be roughly cate-\ngorized into two groups: data augmentation and advanced\ngradient calculation. Data augmentation ensures that the at-\ntack’s effectiveness is invariant to input transformations.\nFor example, the Diversity Input (DI) attack (Xie et al.\n2019) applies random resizing and padding to the inputs\nwith a fixed probability at each iteration. The Translation-\nInvariant (TI) attack (Dong et al. 2019) optimizes a pertur-\nbation over an ensemble of translated images by convolv-\ning the gradient with a linear or Guassian kernel, motivated\nby the near translation-invariance of CNNs. Scale-Invariant\nMethod (SIM) (Lin et al. 2019) optimizes a perturbation\nover an ensemble of scaled images at each iteration.\nAdvanced gradient calculations stabilize the update di-\nrection or add new terms to the loss function in order to\nbetter optimize adversarial perturbations or to reduce over-\nfitting. For example, Momentum Iterative (MI) attack (Dong\net al. 2018) stabilizes update directions and escapes from\npoor local maxima by integrating a momentum term. Nes-\nterov Accelerated Gradient (Lin et al. 2019) can also be\nviewed as another momentum term for improving transfer-\nability. Transferable Adversarial Perturbations (TAP) (Zhou\net al. 2018) maximize the distance between natural images\nand their adversarial examples in intermediate feature maps\nand also uses regularization to smooth the resulting pertur-\nbations. The Attention-Guided Transfer Attack (ATA) (Wu\net al. 2020b) also maximizes distance in the attention maps\nobtained by Grad-CAM (Selvaraju et al. 2017) for critical\nfeature destruction. Skip Gradient Method (SGM) (Wu et al.\n2020a) utilizes a decay factor to reduce gradients from the\nresidual modules and encourages the attack to focus on more\ntransferable low-level information. The Interaction-Reduced\n(IR) attack (Wang et al. 2020) discovers the negative correla-\ntion between adversarial transferability and the interactions\ninside perturbations. Based on the findings of IR, Wang et al.\npropose to decrease interactions inside perturbations, which\ncan be implicitly represented by Shapley values. Neverthe-\nless, the above approaches are designed for CNNs, and the\nperformance degrades significantly when the attacks are di-\nrectly applied to ViTs due to key architectural differences.\nVision Transformers\nInspired by the great success of transformers on Natural\nLanguage Processing, the vision transformer (ViT) was first\nintroduced by (Dosovitskiy et al. 2020). This model re-\nceives raw image patches as input and is pre-trained with\na large image dataset. Subsequently, many works have pro-\nposed to further improve the accuracy and efficiency of\nViTs. In order to overcome the necessity of pre-training\nViTs on massive datasets, DeiT (Touvron et al. 2021a) in-\ntroduces a teacher-student strategy specific to transformers\nwhich utilizes a new distillation token to learn knowledge\nfrom CNNs. T2T-ViT (Yuan et al. 2021) introduces a T2T\nmodule to model the local structure of the image and adopts\na deep-narrow structure for the backbone of Transformers.\nTNT (Han et al. 2021) utilizes an outer transformer block\nand an inner transformer block to learn relationships be-\ntween patches and within patches separately. CaiT (Touvron\net al. 2021b) builds deeper transformers and inserts class to-\nkens only in later layers. CvT (Wu et al. 2021) introduces\nconvolutions into ViTs with the benefits of CNNs, which in-\nserts the class token only in the last layer. Other ViTs such as\nLeViT (Graham et al. 2021), PiT (Heo et al. 2021), ConViT\n(d’Ascoli et al. 2021), Visformer (Chen et al. 2021c), M2TR\n(Wang et al. 2021b) and Deepvit (Zhou et al. 2021) further\nimprove ViTs from different angles as well. Apart from the\nabove works, there are also some other papers (Tang et al.\n2021; Paul and Chen 2021; Shi and Han 2021) that discuss\nthe robustness of ViTs.\nTransfer-based Attacks on ViTs\nCompared to transfer-based attacks on CNNs, less effort has\nbeen made in investigating the transferability of adversarial\nexamples across different ViTs. One related work (Naseer\net al. 2021) proposes the Self-Ensemble (SE) method, which\nboosts adversarial transferability by optimizing perturba-\ntions on an ensemble of models. This method utilizes the\nclass token at each layer with a shared classification head to\nbuild an ensemble of models. As the perturbations are op-\ntimized over an ensemble of models, the generated exam-\nple remains adversarial with respect to multiple models, and\nthis property may encourage generalization to other black-\nbox models. Additionally, (Naseer et al. 2021) also intro-\nduces a Token Refinement (TR) module to fine-tune the\nclass tokens for further enhancing transferability. Despite\nexhibiting promising performance, the broad applicability\nof this method is limited since many ViT models do not\nhave enough class tokens to build an ensemble. Furthermore,\nTR needs to access the ImageNet training set during a fine-\ntuning process, which is time-consuming. In contrast, our\nmethod is generalizable across different ViTs models and is\neasy to implement.\nMethodology\nPreliminary\nConsider an image sample x ∈ X ⊂RH×W×C and its\nground-truth label y ∈ Y= {1, ..., K}, where H, W,\nC denote height, width, and the number of channels re-\nspectively, and K represents the number of classes. We\nreshape the image x into a sequence of flattened patches\nxp = {x1\np, ..., xi\np, ..., xN\np } ∈RN×(P2·C), where xi\np de-\nnotes the i-th patch of x, (P, P) is the resolution of xi\np, and\n2670\nN = H ·W/P 2 is the number of patches. Then, xp is the\ninput to a vision transformer. We use f(x) :X →Yto de-\nnote the prediction function for a white-box ViT surrogate\nmodel. We use g to denote a black-box victim model, which\ncan be a vanilla CNN, robustly trained CNN, and ViT. Fol-\nlowing (Dong et al. 2018; Xie et al. 2019; Wu et al. 2020a),\nwe focus on untargeted adversarial attacks and enforce an\nL∞-norm constraint on perturbations. Hence, the goal of the\ntransfer-based attack is to add a perturbation δ to x yielding\nan adversarial example xadv using information from f in or-\nder to maximize the probability that g(xadv) ̸= y subject\nto the constraint that ||δ||∞≤ϵ. The optimization problem\nfor generating adversarial examples on white-box models is\nformulated as follows:\narg max\nδ\nJ(f(x + δ), y), s.t.||δ||∞< ϵ, (1)\nwhere J(·, ·) is the loss function (e.g. cross-entropy).\nPay No Attention (PNA)\nIn ViTs, a single MSA uses multiple self-attention struc-\ntures, where each such structure, i.e. a head, learns its own\nunique features. By projecting concatenated outputs from\nmultiple heads, MSA combines information from different\nrepresentation subspaces (Vaswani et al. 2017). Based on\nthe observation that the gradients of attention in each head\nimpair the generation of highly transferable adversarial ex-\namples, we drop the attention gradients to alleviate the inter-\naction between patches.\nFigure 1(a) illustrates the proposed Pay No Attention\n(PNA) attack. Given an input patch embeddingZ ∈RN×D,\nquery, key, and value weights WQ, WK, WV ∈RD×Dh,\nthe attention can be formulated as follows:\nA = softmax(ZW Q(ZW K)T /\np\nDh), (2)\nwhere A ∈ RN×N denotes the attention weight. Then the\noutput of this head can be defined as follows:\nZ\n′\n= A(ZW V ). (3)\nThus the gradient of the output Z\n′\nwith respect to input Z\ncan be decomposed as:\n∂Z\n′\n∂Z = (I\nO\nA)∂(ZW V )\n∂Z + ((ZWV )T O\nI)∂A\n∂Z . (4)\nOur proposed PNA attack ignores backpropagation\nthrough the attention branch, i.e., it sets ∂A\n∂Z = 0. This is\nequivalent to treating the attention weights as a constant, or\nusing a “stop gradient” on the attention weights. This results\nin the approximation\n∂Z\n′\n∂Z ≈(I\nO\nA)∂(ZW V )\n∂Z = (I\nO\nA)((WV )T O\nI),\n(5)\nwhere N denotes the Kronecker product. PNA forces the\nperturbation to exploit the network only by using feature\nrepresentations, and not by exploited highly model-specific\nproperties of attention. This results in adversarial examples\nwith high transferability. Skipping attention also allows gra-\ndients to focus on each patch individually, rather than re-\nlying on complex interactions between patches. IR (Wang\nAlgorithm 1: The dual attack on ViTs\nInput: The loss functionJ of Equation 7, a white-box model\nf, a clean image x with its ground-truth class y.\nParameter: The perturbation budget ϵ, iteration number I,\nused patch number T.\nOutput: The adversarial example.\n1: δ0 ←0\n2: α ←ϵ\nI\n3: for i = 0to I −1 do\n4: xs ←P atchOut(xp, T)\n5: M ←Equation 6\n6: g ←P NA(∇δJ with the L2 norm)\n7: δi ←clipϵ(δi−1 + α ·g)\n8: end for\n9: xadv = x + δI\n10: return xadv\net al. 2020) showed a negative correlation between adver-\nsarial transferability and these multi-patch interactions. We\nbelieve this is another reason why PNA improves transfer-\nability.\nPatchOut Attack\nTo motivate our work, we consider the findings of (Xie et al.\n2019), which suggests that diverse input patterns can im-\nprove the transferability of adversarial examples by allevi-\nating the over-fitting phenomenon. Therefore, we introduce\nthe PatchOut Attack, which randomly attacks a subset of\npatches at each iteration to alleviate over-fitting.\nWe useT to control the number of used patches at each it-\neration, and xs = {x1\ns, ..., xt\ns, ..., xT\ns }to denote the selected\npatches. Thus, the attack mask M ∈{0, 1}H×W×C can be\nformulated as:\nMi\np =\n\u001a1, if xi\np in xs\n0, otherwise (6)\nwhere Mi\np ∈{0, 1}P×P×C is the region corresponding to\nxi\np in the image. Therefore, PatchOut replaces Equation 1\nwith:\narg max\nδ\nJ(f(x+M ⊙δ), y)+ λ||δ||2, s.t.||δ||∞< ϵ,(7)\nwhere ⊙denotes element-wise multiplication. The added\nsecond term encourages perturbations to have a large L2\nnorm, preferring a large distance from x. λ controls the bal-\nance between the loss function and the regularization term.\nWe finally summarize the proposed dual attack to craft\nadversarial examples in Algorithm 1, where the function\nP atchOut(·, ·) randomly select T patches from xp to gen-\nerate xs, the function P NA(·)bypasses the gradients of at-\ntention, and clipϵ(·) restricts each entry of generated pertur-\nbations to be within [−ϵ, ϵ].\nExperiments\nExperimental Settings\nDataset Following the setting from Dong et al. (2018,\n2019); Xie et al. (2019); Lin et al. (2019), we randomly sam-\nple one image, which is correctly classified by all models,\n2671\nMethod ViT-B/16 PiT-B CaiT-S-24 Visformer-S DeiT-B TNT-S LeViT-256 ConViT-B\nFGSM 15.57 19.80 20.43 19.37 22.08 22.78 18.80 25.58\nBIM 20.77 22.17 22.63 22.70 33.53 32.13 20.45 35.30\nMI 41.23 45.23 47.13 45.97 56.03 55.23 43.75 58.25\nDI 32.57 45.13 43.07 47.77 48.08 55.18 43.25 49.35\nTI 19.33 17.67 16.50 19.00 25.13 28.18 13.70 27.53\nSIM 34.97 32.73 35.17 31.13 44.13 46.73 36.43 45.68\nSGM 38.87 41.60 52.30 48.80 60.53 64.33 51.13 60.68\nIR 21.33 22.70 24.00 23.43 34.00 33.43 21.30 36.38\nTAP 25.27 24.73 33.40 32.20 43.20 39.78 30.03 42.20\nATA 3.47 1.13 0.97 2.67 3.68 3.37 2.02 3.72\nSE 29.05 21.25 31.40 24.90 45.23 37.87 21.73 46.03\nOurs 46.10 52.40 59.87 58.60 63.85 67.25 57.62 63.70\nTable 1: MASR (%) against ViTs with various attack methods. We use ViT-B/16, PiT-B, CaiT-S-24 and Visformer-S as white-\nbox models respectively to generate adversarial examples. Each model is evaluated on adversarial examples generated by\nwhite-box ViTs, and the ASRs are averaged over surrogate models to obtain MASR. The best results are in bold.\nfrom each class from the ImageNet 2012 validation dataset\n(Russakovsky et al. 2015), to conduct our experiments.\nModels We evaluate the performance and transferability\nof the proposed method under two different settings: 1) The\nsurrogate and victim models are both ViTs; 2) The surro-\ngate is a ViT, and the victim model is a CNN. In these ex-\nperiments, we evaluate performance on multiple ViT vari-\nants as well as multiple CNNs architectures, including both\nnormally trained models and robustly trained models. For\nViTs, we conduct experiments on ViT-B/16 (Dosovitskiy\net al. 2020), DeiT-B (Touvron et al. 2021a), TNT-S (Han\net al. 2021), LeViT-256 (Graham et al. 2021), PiT-B (Heo\net al. 2021), CaiT-S-24 (Touvron et al. 2021b), ConViT-B\n(d’Ascoli et al. 2021), and Visformer-S (Chen et al. 2021c).\nWhile for CNNs, the experiments are conducted on Incep-\ntion v3 (Inc-v3) (Szegedy et al. 2016), Inception v4 (Inc-\nv4), Inception ResNet v2 (IncRes-v2) (Szegedy et al. 2017),\nand ResNet v2-152 (Res-v2) (He et al. 2016). We chose\nthese models to conduct our experiments since they are pub-\nlicly available in the timm library (Wightman 2019). We\nrandomly select ViT-B/16, PiT-B, CaiT-S-24, Visformer-S\nas the white-box models to generate adversarial examples.\nWhen selecting one model as the white-box surrogate, we\nuse the remaining models as black-box victims when evalu-\nating performance.\nEvaluation We use Attack Success Rate (ASR) to quan-\ntify performance on black-box models. Here, ASR denotes\nthe proportion of generated adversarial examples from the\nsurrogate model that are successfully misclassified by the\nblack-box victim model. A higher success rate means better\nadversarial transferability. We also use Mean ASR (MASR)\nto denote the mean value of ASR for one black-box victim,\naverages across different white-box surrogate models. Fol-\nlowing Dong et al. (2018, 2019); Xie et al. (2019), we set\nthe norm constraint ϵ = 16and J as the cross-entropy loss\nfunction. For the iterative attack, we set I = 10and thus the\nstep size α = 1.6. We resize all images to224 ×224 to con-\nduct experiments. For the inputs of ViTs, we set the patch\nsize P = 16, thus the number of the patches isN = 196.\nPerformance Comparison\nWe compare our method with several baseline attacks, in-\ncluding MI (Dong et al. 2018), DI (Xie et al. 2019), TI\n(Dong et al. 2019), SIM (Lin et al. 2019), SGM (Wu et al.\n2020a), IR (Wang et al. 2020), TAP (Zhou et al. 2018),\nATA (Wu et al. 2020b), and SE (Naseer et al. 2021), which\nare integrated into BIM (Kurakin, Goodfellow, and Bengio\n2016). Note that we do not compare with TR (Naseer et al.\n2021) because it requires fine-turning on ImageNet, and thus\nwould yield an unfair comparison in terms of both data ac-\ncess and compute cost. For each baseline method, we fol-\nlow their original settings in our experiments. We set our\npatch number T = 130, and we set the balancing parameter\nλ = 0.1in PatchOut according to the experimental results.\nFor performance comparison, we report the MASR obtained\nfrom white-box models including ViT-B/16, PiT-B, CaiT-S-\n24 and Visformer-S.\nPerformance on ViTs We first evaluate the adversarial\ntransferability of our method across different ViTs. Table\n1 summarizes the results on different black-box ViT mod-\nels. From the results, we have the following observations.\nFirst, TI and ATA, which improve the adversarial transfer-\nability significantly on CNNs, exhibit poor performances on\nViTs. As TI is proposed based on the property of translation\ninvariance in CNNs, it is not applicable to ViTs since the\narchitecture of ViTs leads to much less image-specific in-\nductive bias than CNNs (e.g., translation invariance) (Yuan\net al. 2021). Furthermore, while the key idea of ATA is to\ncorrupt the features of discriminative regions shared among\ndifferent CNNs, it is also not applicable to ViTs for the rea-\nson that the discriminative region varies significantly across\ndifferent ViTs. Second, compared to other baseline methods\ndesigned for CNNs, SGM performs much better on ViTs.\nThis is because the key idea of SGM is to boost transfer-\nability by encouraging the model to focus more on low-level\ninformation, which is also applicable to ViTs since SGM is\n2672\nMethod Inc-v3 Inc-v4 IncRes-v2 Res-v2 Inc-v3 ens3 Inc-v3ens4 IncRes-v2ens\nFGSM 20.78 18.80 16.38 19.05 14.62 13.98 10.38\nBIM 17.88 14.77 12.40 11.82 8.02 6.20 4.45\nMI 39.65 37.43 32.17 33.28 24.80 22.10 17.68\nDI 32.78 31.75 26.40 25.00 17.73 15.22 11.12\nTI 23.27 23.60 15.28 20.88 18.80 20.80 13.92\nSIM 30.55 27.63 24.17 24.48 19.45 17.67 13.40\nSGM 38.42 34.00 27.25 27.25 18.70 16.53 11.68\nIR 17.65 15.83 12.08 12.48 8.05 6.50 4.78\nTAP 29.58 26.10 20.67 19.23 12.58 10.62 6.85\nATA 3.25 2.53 2.03 2.07 1.20 0.85 0.92\nSE 18.40 16.47 12.33 12.63 9.13 6.73 5.10\nOurs 47.95 45.12 38.45 38.93 26.20 22.85 18.10\nTable 2: MASR (%) against CNNs with various attack methods. We use ViT-B/16, PiT-B, CaiT-S-24 and Visformer-S as white-\nbox models to generate adversarial examples. Each CNN is evaluated on adversarial examples generated by white-box ViTs,\nand the ASRs are averaged over surrogate models to obtain MASR. The best results are in bold.\nnot designed based on the unique characteristics of CNNs.\nThird, SE, which is proposed for improving the transferabil-\nity of ViTs, does not work well across different structures of\nViTs. This is because the performance of SE is closely re-\nlated to the number of class tokens for building the ensem-\nble models. As there are few or even no class tokens in some\nViTs (e.g, PiT-B, Visformer-S), SE works poorly for such\nsituations. Lastly, our dual attack consistently outperforms\nthe baseline attacking methods and achieves a 58.67 % av-\nerage MASR by averaging MASR across different black-\nbox models. Our method, which considers the core compo-\nnents of ViTs, achieves the best results in ViTs with differ-\nent structures. These experiments validate the effectiveness\nof the proposed dual attack.\nPerformance on CNNs We further evaluate the perfor-\nmance of transferring the adversarial examples generated on\nwhite-box ViTs to attack black-box CNNs. The results are\nsummarized in Table 2. We have the following observations.\nFirst, the attack success rates of all methods decrease signif-\nicantly, which indicates that adversarial samples generated\non ViTs are less transferable on CNNs than ViTs, presum-\nably due to structural differences between ViTs and CNNs.\nSecond, MI improves the transferability of the generated ad-\nversarial examples by utilizing the momentum term to sta-\nbilize update directions. Third, compared to other baseline\nmethods, our proposed dual attack significantly improves\nMASR despite the different structures between ViTs and\nCNNs. The dual attack has a 42.61% average MASR on\nnormally trained CNNs and a 22.38% average MASR on\nrobustly trained CNNs. These results indicate the feasibility\nof using ViTs to attack robustly trained CNNs.\nCombining with Existing Methods The proposed dual\nattack tailored for ViTs can be easily combined with exist-\ning methods to further improve performance. We demon-\nstrate this by combining our method with MI and SGM.\nThe reason we choose MI and SGM is that these two meth-\nods perform much better than other baseline methods. Using\nthese combined methods, we conduct experiments with ViT-\nclean adversarial clean adversarial\nFigure 3: Visualization of randomly picked clean images and\ntheir corresponding adversarial images, crafted on the ViT-\nB/16 model using our dual attack.\nB/16 as the white-box model. Specifically, SGM + Ours uses\nfewer gradients than MultiLayer Perceptron (MLP) modules\naccording to a decay factor that is set as 0.5. Table 3 shows\nthe results of the integrated methods. We observe that the\nproposed dual attack significantly improves the transferabil-\nity of MI and SGM across ViTs, normally trained CNNs,\nand robustly trained CNNs. In particular, the dual attack\nincreases the average MASR of MI and SGM by 28.54 %\nand 102.49 %, respectively, when attacking robustly trained\nCNNs. The results demonstrate that the dual attack can eas-\nily be combined with existing methods to further boost per-\nformance.\nVisualization of Adversarial Examples Figure 3 depicts\n8 randomly selected clean images and their corresponding\nadversarial examples. Here, we observe that the adversarial\nperturbations are hardly perceptible.\n2673\nMethod ViTs Normal CNNs Robust CNNs\nMI 49.10 35.63 21.53\nMI + Ours 59.70 48.79 30.13\nSGM 52.28 31.73 15.64\nSGM + Ours 63.48 52.31 31.37\nTable 3: Average ASR (%) of models under attacks that\ncombine the proposed method and existing algorithms. The\nexperiment is conducted by using ViT-B/16 as the white-\nbox model. Average ASR is obtained by averaging ASRs of\ndifferent types of black-box models. ”Normal CNNs” and\n”Robust CNNs” denote normally trained CNNs and robustly\ntrained CNNs respectively.\nPatchOut L2 PNA ViTs Normal Robust\n- - - 26.97 10.63 5.50\n✓ - - 34.92 12.55 7.07\n- ✓ - 37.79 16.55 10.00\n- - ✓ 42.47 18.75 10.97\n✓ ✓ - 48.82 24.55 14.67\n- ✓ ✓ 49.69 22.36 13.54\n✓ ✓ ✓ 59.15 36.23 23.87\nTable 4: Average ASR (%) for our proposed method with\ndifferent component combinations. PatchOut indicates the\noperation of sampling a subset of patches, L2 denotes the\nregularization term in Equation 7, PNA is the operation that\nbypasses the gradients of attention. ‘✓’ indicates that the\ncomponent is used while ‘-’ indicates that it is not used.\n“Normal” and “Robust” denote normally trained CNNs and\nrobustly trained CNNs respectively.\nAblation Studies\nTo validate the effect of each component proposed in our\nmethod, we evaluate the performance under various combi-\nnations of the components: PatchOut, the regularization term\nL2 in Equation 7, and Pay No Attention (PNA). We also con-\nduct experiments to analyze the effect of hyper-parameters,\nincluding the used patch number T and the balance parame-\nter λ. For the ablation study, we use ViT-B/16 as the white-\nbox model and generate adversarial examples on 2,000 ran-\ndomly sampled images to attack other black-box models.\nEffect of Components Table 4 shows the results of differ-\nent component combinations. It is observed that using any\nof the components leads to performance improvements on\nboth ViTs and CNNs. Among these components, PNA im-\nproves transferability most significantly on both ViTs and\nCNNs, demonstrating that gradients from ViTs, except for\nthose from attention modules, are most vulnerable. The best\nperformance is achieved when combining all components\ntogether, which suggests that three components contribute to\nthe improvement of transferability in complementary ways.\nEffect of Parameters We investigate the effect of pa-\nrameters T and λ and summarize the results in Figure 4.\nThe used patch number T determines how many patches\n10 40 160 19670 100 130 \n T\n0\n10\n20\n30\n40Attack success rate (%)\n(a) The used patch number T\n0.001 0.01 0.1 1.0 10.0\n20\n30\n40\n50\n60Attack success rate (%)\n(b) λ\nFigure 4: Average ASR (%) of ViTs under attacks using var-\nious values of (a) patch number T and (b) λ in Equation 7.\nare used in each attack iteration. When all patches are in-\ncluded (T = N), PatchOut degenerates into BIM. When\nT is small, PatchOut is not able to generate strong adver-\nsarial examples in a limited number of iterations (I = 10).\nThus, it is vital to study optimal settings of T. We experi-\nment by tuning T without L2. Figure 4(a) shows the results,\nwith T ∈[10, 40, 70, 100, 130, 160, 196]. When T = 130,\nPatchOut achieves optimal results. With this value ofT, each\npatch can be attacked multiple times in the context of dif-\nferent patch subsets. For the balance parameter λ, we set\nλ ∈[0.001, 0.01, 0.1, 1, 10] to conduct the experiments with\nT = 130. Figure 4(b) illustrates the effect of various λ.\nWhen λ = 0.1, PatchOut achieves the best result by bal-\nancing the contribution of each term in Equation 7.\nConclusion\nIn this paper, we identify several properties of ViTs that can\nbe leveraged to produce more transferable adversarial ex-\namples. Specifically, we find that ignoring the gradients of\nattention units and only perturbing a subset of the patches\nat each iteration prevents overfitting and creates diverse in-\nput patterns, thus increasing transferability. We conduct two\ntoy experiments to validate these assumptions and propose\nthe dual attack for ViTs consisting of the Pay No Attention\n(PNA) attack and the PatchOut attack. We conduct a series\nof experiments to show that the proposed method can greatly\nimprove transferability. Combining our attack with other ex-\nisting methods, our techniques consistently enhance trans-\nferability. The results of this study show that cross-model\ntransferability happens even across very wide architectural\ngaps between models. The transferability observed in this\nwork further suggests that the feature extractors and implicit\nbiases of transformer architectures and CNNs are not as dif-\nferent as one might suspect, and we think adversarial exam-\nples might serve as the basis for future studies of the simi-\nlarities and differences between the two. Code is available at\nhttps://github.com/zhipeng-wei/PNA-PatchOut.\n2674\nAcknowledgments\nThe authors would like to thank the anonymous referees for\ntheir valuable comments and helpful suggestions. This work\nwas supported in part by NSFC project (#62032006), Sci-\nence and Technology Commission of Shanghai Municipal-\nity Project (20511101000), and in part by Shanghai Pujiang\nProgram (20PJ1401900).\nReferences\nBreiman, L. 2004. Random Forests. Machine Learning, 45:\n5–32.\nChen, K.; Wei, Z.; Chen, J.; Wu, Z.; and Jiang, Y .-G. 2021a.\nAttacking Video Recognition Models with Bullet-Screen\nComments. arXiv preprint arXiv:2110.15629.\nChen, S.; Tao, Q.; Ye, Z.; and Huang, X. 2021b. Going\nFar Boosts Attack Transferability, but Do Not Do It. arXiv\npreprint arXiv:2102.10343.\nChen, Z.; Xie, L.; Niu, J.; Liu, X.; Wei, L.; and Tian, Q.\n2021c. Visformer: The vision-friendly transformer. arXiv\npreprint arXiv:2104.12533.\nd’Ascoli, S.; Touvron, H.; Leavitt, M.; Morcos, A.; Biroli,\nG.; and Sagun, L. 2021. Convit: Improving vision trans-\nformers with soft convolutional inductive biases. arXiv\npreprint arXiv:2103.10697.\nDong, Y .; Liao, F.; Pang, T.; Su, H.; Zhu, J.; Hu, X.; and Li,\nJ. 2018. Boosting adversarial attacks with momentum. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 9185–9193.\nDong, Y .; Pang, T.; Su, H.; and Zhu, J. 2019. Evading de-\nfenses to transferable adversarial examples by translation-\ninvariant attacks. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 4312–\n4321.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nGoodfellow, I. J.; Shlens, J.; and Szegedy, C. 2014. Explain-\ning and harnessing adversarial examples. arXiv preprint\narXiv:1412.6572.\nGraham, B.; El-Nouby, A.; Touvron, H.; Stock, P.; Joulin,\nA.; J´egou, H.; and Douze, M. 2021. LeViT: a Vision Trans-\nformer in ConvNet’s Clothing for Faster Inference. arXiv\npreprint arXiv:2104.01136.\nHan, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.; and Wang,\nY . 2021. Transformer in transformer. arXiv preprint\narXiv:2103.00112.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Identity map-\npings in deep residual networks. In European conference on\ncomputer vision, 630–645. Springer.\nHeo, B.; Yun, S.; Han, D.; Chun, S.; Choe, J.; and Oh, S. J.\n2021. Rethinking spatial dimensions of vision transformers.\narXiv preprint arXiv:2103.16302.\nHinton, G. E.; Srivastava, N.; Krizhevsky, A.; Sutskever, I.;\nand Salakhutdinov, R. 2012. Improving neural networks\nby preventing co-adaptation of feature detectors. ArXiv,\nabs/1207.0580.\nKurakin, A.; Goodfellow, I.; and Bengio, S. 2016. Ad-\nversarial machine learning at scale. arXiv preprint\narXiv:1611.01236.\nKurakin, A.; Goodfellow, I. J.; and Bengio, S. 2017.\nAdversarial examples in the physical world. ArXiv,\nabs/1607.02533.\nLin, J.; Song, C.; He, K.; Wang, L.; and Hopcroft, J. E. 2019.\nNesterov accelerated gradient and scale invariance for adver-\nsarial attacks. arXiv preprint arXiv:1908.06281.\nLiu, Y .; Chen, X.; Liu, C.; and Song, D. 2016. Delv-\ning into transferable adversarial examples and black-box at-\ntacks. arXiv preprint arXiv:1611.02770.\nNaseer, M.; Ranasinghe, K.; Khan, S.; Khan, F. S.; and\nPorikli, F. 2021. On Improving Adversarial Transferability\nof Vision Transformers. arXiv preprint arXiv:2106.04169.\nPaul, S.; and Chen, P.-Y . 2021. Vision transformers are ro-\nbust learners. arXiv preprint arXiv:2105.07581.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\net al. 2015. Imagenet large scale visual recognition chal-\nlenge. International journal of computer vision, 115(3):\n211–252.\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\nParikh, D.; and Batra, D. 2017. Grad-cam: Visual explana-\ntions from deep networks via gradient-based localization. In\nProceedings of the IEEE international conference on com-\nputer vision, 618–626.\nShapley, L. S. 1988. A value for n-person games, 31–40.\nCambridge University Press.\nShi, Y .; and Han, Y . 2021. Decision-based Black-box Attack\nAgainst Vision Transformers via Patch-wise Adversarial Re-\nmoval. ArXiv, abs/2112.03492.\nSzegedy, C.; Ioffe, S.; Vanhoucke, V .; and Alemi, A. A.\n2017. Inception-v4, inception-resnet and the impact of resid-\nual connections on learning. In Thirty-first AAAI conference\non artificial intelligence.\nSzegedy, C.; Vanhoucke, V .; Ioffe, S.; Shlens, J.; and Wojna,\nZ. 2016. Rethinking the inception architecture for computer\nvision. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2818–2826.\nTang, S.; Gong, R.; Wang, Y .; Liu, A.; Wang, J.; Chen, X.;\nYu, F.; Liu, X.; Song, D.; Yuille, A.; et al. 2021. Robustart:\nBenchmarking robustness on architecture design and train-\ning techniques. arXiv preprint arXiv:2109.05211.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021a. Training data-efficient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning, 10347–10357. PMLR.\nTouvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; and\nJ´egou, H. 2021b. Going deeper with image transformers.\narXiv preprint arXiv:2103.17239.\n2675\nVaswani, A.; Shazeer, N. M.; Parmar, N.; Uszkoreit, J.;\nJones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017.\nAttention is All you Need. ArXiv, abs/1706.03762.\nWang, J.; Liu, A.; Yin, Z.; Liu, S.; Tang, S.; and Liu, X.\n2021a. Dual Attention Suppression Attack: Generate Adver-\nsarial Camouflage in Physical World. In IEEE Conference\non Computer Vision and Pattern Recognition.\nWang, J.; Wu, Z.; Chen, J.; and Jiang, Y .-G. 2021b. M2TR:\nMulti-modal Multi-scale Transformers for Deepfake Detec-\ntion. arXiv preprint arXiv:2104.09770.\nWang, X.; Ren, J.; Lin, S.; Zhu, X.; Wang, Y .; and Zhang, Q.\n2020. A unified approach to interpreting and boosting adver-\nsarial transferability. arXiv preprint arXiv:2010.04055.\nWei, Z.; Chen, J.; Wei, X.; Jiang, L.; Chua, T.-S.; Zhou,\nF.; and Jiang, Y .-G. 2020. Heuristic black-box adversar-\nial attacks on video recognition models. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 34,\n12338–12345.\nWei, Z.; Chen, J.; Wu, Z.; and Jiang, Y .-G. 2021. Cross-\nModal Transferable Adversarial Attacks from Images to\nVideos. arXiv:2112.05379.\nWightman, R. 2019. PyTorch Image Models. https://github.\ncom/rwightman/pytorch-image-models. Accessed: 2021-\n08-20.\nWu, D.; Wang, Y .; Xia, S.-T.; Bailey, J.; and Ma, X. 2020a.\nSkip connections matter: On the transferability of adver-\nsarial examples generated with resnets. arXiv preprint\narXiv:2002.05990.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. Cvt: Introducing convolutions to vision\ntransformers. arXiv preprint arXiv:2103.15808.\nWu, W.; Su, Y .; Chen, X.; Zhao, S.; King, I.; Lyu, M. R.; and\nTai, Y .-W. 2020b. Boosting the transferability of adversar-\nial samples via attention. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n1161–1170.\nXie, C.; Zhang, Z.; Zhou, Y .; Bai, S.; Wang, J.; Ren, Z.;\nand Yuille, A. L. 2019. Improving transferability of ad-\nversarial examples with input diversity. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2730–2739.\nYan, H.; Wei, X.; and Li, B. 2020. Sparse black-box\nvideo attack with reinforcement learning. arXiv preprint\narXiv:2001.03754.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.; Tay,\nF. E.; Feng, J.; and Yan, S. 2021. Tokens-to-token vit: Train-\ning vision transformers from scratch on imagenet. arXiv\npreprint arXiv:2101.11986.\nZhou, D.; Kang, B.; Jin, X.; Yang, L.; Lian, X.; Jiang, Z.;\nHou, Q.; and Feng, J. 2021. Deepvit: Towards deeper vision\ntransformer. arXiv preprint arXiv:2103.11886.\nZhou, W.; Hou, X.; Chen, Y .; Tang, M.; Huang, X.; Gan, X.;\nand Yang, Y . 2018. Transferable adversarial perturbations.\nIn Proceedings of the European Conference on Computer\nVision (ECCV), 452–467.\n2676",
  "topic": "Transferability",
  "concepts": [
    {
      "name": "Transferability",
      "score": 0.9719923138618469
    },
    {
      "name": "Adversarial system",
      "score": 0.9141942858695984
    },
    {
      "name": "Computer science",
      "score": 0.794273853302002
    },
    {
      "name": "Transformer",
      "score": 0.5952889919281006
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5674415826797485
    },
    {
      "name": "Machine learning",
      "score": 0.4879152476787567
    },
    {
      "name": "Engineering",
      "score": 0.08271798491477966
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Logit",
      "score": 0.0
    }
  ]
}