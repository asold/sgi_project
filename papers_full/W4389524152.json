{
  "title": "Pretrained Language Models v. Court Ruling Predictions: A Case Study on a Small Dataset of French Court of Appeal Rulings",
  "url": "https://openalex.org/W4389524152",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093458292",
      "name": "Olivia Vaudaux",
      "affiliations": [
        "Laboratoire d'Informatique de Grenoble",
        "Translational Innovation in Medicine and Complexity"
      ]
    },
    {
      "id": "https://openalex.org/A5052803368",
      "name": "Caroline Bazzoli",
      "affiliations": [
        "Translational Innovation in Medicine and Complexity"
      ]
    },
    {
      "id": "https://openalex.org/A5006675504",
      "name": "Maximin Coavoux",
      "affiliations": [
        "GIPSA-Lab",
        "Laboratoire d'Informatique de Grenoble"
      ]
    },
    {
      "id": "https://openalex.org/A5078244320",
      "name": "Géraldine Vial",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5015115884",
      "name": "Étienne Vergès",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2765440119",
    "https://openalex.org/W124542521",
    "https://openalex.org/W2980376477",
    "https://openalex.org/W4207022113",
    "https://openalex.org/W2954257417",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3036862134",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W4287993252",
    "https://openalex.org/W2511234952",
    "https://openalex.org/W3215431778",
    "https://openalex.org/W2963387630",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2786435373",
    "https://openalex.org/W2536769020",
    "https://openalex.org/W2962854673",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3103187652"
  ],
  "abstract": "International audience",
  "full_text": "Proceedings of the Natural Legal Language Processing Workshop 2023, pages 38–43\nDecember 7, 2023 ©2023 Association for Computational Linguistics\nPretrained Language Models v. Court Ruling Predictions:\nA Case Study on a Small Dataset of French Court of Appeal Rulings\nOlivia Vaudaux,1,2,3 Caroline Bazzoli,1 Maximin Coavoux,2 Géraldine Vial3 and Étienne Vergès3\n1Univ. Grenoble Alpes, CNRS, UMR 5525, VetAgro Sup, Grenoble INP, TIMC, 38000 Grenoble, France\n2Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France\n3Univ. Grenoble Alpes, CRJ, EA 1965\nolivia.vaudaux@gmail.com, first.last@univ-grenoble-alpes.fr\nAbstract\nNLP systems are increasingly used in the law\ndomain, either by legal institutions or by the\nindustry. As a result there is a pressing need to\ncharacterize their strengths and weaknesses and\nunderstand their inner workings. This article\npresents a case study on the task of judicial deci-\nsion prediction, on a small dataset from French\nCourts of Appeal. Specifically, our dataset of\naround 1000 decisions is about the habitual\nplace of residency of children from divorced\nparents. The task consists in predicting, from\nthe facts and reasons of the documents, whether\nthe court rules that children should live with\ntheir mother or their father. Instead of feeding\nthe whole document to a classifier, we care-\nfully construct the dataset to make sure that\nthe input to the classifier does not contain any\n‘spoilers’ (it is often the case in court rulings\nthat information all along the document men-\ntions the final decision). Our results are mostly\nnegative: even classifiers based on French pre-\ntrained language models (Flaubert, JuriBERT)\ndo not classify the decisions with a reasonable\naccuracy. However, they can extract the deci-\nsion when it is part of the input. With regards\nto these results, we argue that there is a strong\ncaveat when constructing legal NLP datasets\nautomatically.\n1 Introduction\nNatural language processing (NLP) has now many\napplications in the legal domain, and is used in\npractice to provide tools for law practitioners, such\nas lawyers or judges (e.g. for information retrieval,\ndocument classification), to quantify judicial risks\nin specific cases. Judicial decision prediction\nthrough NLP tools is also important for law re-\nsearchers to identify the factors that most influence\nthe final decision of a case, characterize judge’s\nreasoning process, and help data annotation proce-\ndures.\nIn this paper, we present a case study on the\nprediction of the verdict ruling on the habitual res-\nidency of children in appeal cases from French\ncourts. These documents follow a typical structure\n(facts, judgement of the first court whose ruling\nwas the object of an appeal, decision itself). How-\never, the structure is only implicit (sections are not\ntitled), and judges’ writing is not standardised and\ndiffers a lot from one court of appeal to another.\nFinally, it is often the case that information of each\ntype (facts, reasons, decision) are scattered through\nthe document. As a result, the document might\ncontain ‘spoilers’ for the decision itself at various\nplaces in the document.\nIn order to assess the ability of BERT-style lan-\nguage models to infer a verdict from the legal con-\ntent itself, rather than the full text of the document,\nwe implemented an annotation campaign aiming\nat assigning a semantic type to every segment in\neach document as: (i) facts (ii) information about\nthe first judgment (iii) reasons for the decision (iv)\ndecision itself (containing explicitly the variable\nto predict) (v) none of these types. Thanks to\nthis annotation procedure, lead by law experts, we\ncompare the usefulness of various types of inputs\nfor several types of classifiers, including bag-of-n-\ngrams classifiers and BERT-based classifiers. In\nparticular, we used 3 French pretrained language\nmodels: FlauBERT (Le et al., 2020) and Camem-\nBERT (Martin et al., 2020) that were trained on\ngeneric-domain data, and JuriBERT (Douka et al.,\n2021), that has been specifically trained on French\nlegal texts.\nIn summary, our contributions are as follows:\n• We construct a dataset of 1k judicial decisions\nannotated at the segment level by law experts\n(law professors and PhD students);\n• A set of empirical results on a judicial decision\nprediction task on this dataset;\n• In regards to our (mostly negative) results, we\nargue for the need for a careful curation of\n38\nautomatically constructed datasets for legal\nNLP.\n2 Related Work\nPredicting the outcome of a judicial decisions based\non a textual input is rather recent and has first\nbeen applied to English decisions from the Eu-\nropean Court of Human Rights (ECtHR Aletras\net al., 2016). In particular, Aletras et al. (2016)\nconstructed a dataset of ECtHR decisions focus-\ning on cases about a few articles of the European\nConvention on Human Rights, and evaluated bag-\nof-n-grams approaches.\nSubsequently, several studies were made on the\nECHR corpus, consisting of approximately 11 500\nrulings from 1959 to our days. Medvedeva et al.\n(2020) and Liu and Chen (2017) evaluated various\nlinear classifiers. Kaur and Bozic (2019) used deep\nneural networks based on convolutional networks.\nO’Sullivan and Beel (2019) used word embeddings\npretrained on documents from the ECtHR data.\nChalkidis et al. (2019) introduced Legal-BERT, a\nBERT-style model pretrained on legal data (legisla-\ntive texts, ECtHR cases and American contracts),\nand evaluated it on the ECtHR dataset. Other com-\nmon dataset include decisions from the US supreme\ncourt, e.g. Katz et al. (2014) use decision trees to\npredict the votes of Judges. We refer the reader to\nMedvedeva et al. (2023) for a general survey on\njudicial decision prediction. In contrast to these\npublications, we focus on French, and on a very\nspecific type of litigation: the habitual residency of\nchildren of divorced parents. Moreover we use both\nlinear classifiers and deep nets based on fine-tuning\npretrained language models.\nAs regards legal NLP applied to French, ¸ Sulea\net al. (2017) and Sulea et al. (2017) evaluated SVM\nclassifiers on the prediction of the area of a case,\nas well as its ruling, on decisions from the French\nSupreme Court (Cour de Cassation). Salaün et al.\n(2020) used pretrained language models such as\nFlaubert (Le et al., 2020) and Camembert (Mar-\ntin et al., 2020) to predict the outcome of cases\nabout conflicts between a tenant and a landlord in\nQuébec. They observed that pretrained language\nmodels outperformed linear classifiers. Our dataset\nis on another type of litigation (habitual residency\nof children) and we focus on the manual construc-\ntion of our dataset, instead of automatically con-\nstructing it. Finally, Douka et al. (2021) introduced\nJuriBERT, trained on Légifrance, an official web-\nsite publishing all French law and evaluated it on\ntopic classification tasks for documents from the\nCour de Cassation (highest court in France).\n3 Data\nIn France, when parents of minor children sepa-\nrate, they must decide on their children’s place of\nresidency. If the parents are unable to agree on a\nplace of residency, the matter is referred to a court\nwhich, after hearing each parent, issues a ruling.\nWe study the three principal outcomes: children\nlive with their mother, their father, or in alternating\nresidency. If the outcome of the lower court is not\nsuitable for at least one parent, they can make an\nappeal to retry the case. New judges re-examine the\nfacts and confirm or overturn the judgment handed\ndown by the first court. These are the rulings we’ll\nbe using for the classification experiment.\nWe collected 987 rulings from French Courts of\nAppeal from the Jurica1 databasis, that we accessed\nthrough a legal publisher (LexisNexis). The rulings\nhave the following structure:\n1. The metadata of the trial (date, place of appeal\ncourt, trial number, etc.);\n2. The facts: the parties (the parents) are intro-\nduced along with their lawyers, children, and\nthe composition of the court.2;\n3. The prior judgment under appeal is described,\nand the parties claims’ and arguments are set\nout;\n4. The reasons of the Court of Appeal’s decision:\nthe judge explains the decision to come based\non sections of the law and the facts provided\nby the parties;\n5. The reasons lead to the decision taken by the\ncourt: the reversal or confirmation of the first-\ninstance judgment, from which we can infer\nthe label we want to predict.\nAlthough the structure of court rulings in sec-\ntions (facts, judgment, reasons, decision) is conven-\ntional, judges’ practices vary a lot in the length of\nthese sections. For example, the facts section might\nbe extremely short or very verbose, and its content\n1Jurica is no longer maintained due to a recent change in\npolicy about legal open data.\n2The publisher replaced names by their initial in an effort to\ncomply with French personal data laws (1978 Data Protection\nAct).\n39\nmay not be restricted to what lawyers define as\nfacts. Moreover information about facts or reasons\nmay be scattered through the document and not\nonly occur in their dedicated section. Therefore,\ninstead of relying on the structure of the documents\nto extract information (and let the model be ex-\nposed to potential ‘spoilers’ about the decision),\nwe rely on the manual annotation of every segment\nof the document as either (i) fact (ii) judgement\n(iii) reason (iv) decision, or (v) not relevant. The\nmanual annotations were carried out by two law\nprofessors (who are coauthors of the paper and de-\nsigned the annotation guidelines), one law post-doc\nresearcher and one law graduate students who were\ntrained for the task.\nDue to the time required to carry out annotations,\neach document was annotated by a single annotator,\nso we did not compute interannotator agreement\nscores.\nAnnotating the rulings was quite challenging, es-\npecially to differentiate the facts and the reasons,\nbecause depending on the judge’s method, the par-\nties’ claims and arguments can be set out in the\nreasons to justify the verdict. Therefore, in several\nrulings, the facts category is limited to the names\nand birth dates of parents and their children.\nPrediction Task Our goal was to predict the out-\ncome of appeal cases ruling on the residency of chil-\ndren among three possible outcomes: they live with\ntheir mother (label M, 50%), their father (F, 33%)\nor in alternate residency (B for both, 17%).3\nWe trained and evaluated classifiers settings in\nseveral settings, depending on the information in-\nput to the model: (i) F+R: Facts+Reasons (ii)\nF+R+J: Facts+Reasons+Judgment (iii) Decision\n(iv) Decision+Judgment. The last two settings are\nmeant to assess the model’s ability to extract the\ninformation about the decision,4 whereas the first\ntwo assess the model’s ability to make inferences\nabout the judges’ reasoning.\n3An alternative way of framing the task with two labels\nwould be to predict whether the first judgment is (i) confirmed\n(ii) reversed. However, these two labels do not provide the\nsame information as the three-label task we chose, since labels\nhave to be interpreted with regards to the first judgment, and\nthe second label (reversed judgement) is not as informative\n(2 possible outcomes).\n4Sometimes the decision text only mentions that the first\njudgment is confirmed or overturned, making it necessary to\nknow what the first judgment is to infer the label correctly.\n4 Models\nBaseline We use various classical classifiers as\nbaselines, namely:\n• a multinomial naive Bayes (MNB);\n• a k-Nearest Neighbors (k-NN) with Manhat-\ntan distance;\n• a multiclass Support Vector Machine (SVM);\n• a forest of decision trees, with a predefined\nnumber of trees created, where the final de-\ncision is made on the basis of the label most\npredicted by the trees;\n• a Multi-Layer Perceptron (MLP) with 1 hid-\nden layer of 100 units, with a ReLU activation\nfunction.\nWe vectorize the texts using bag-of- n-grams\nmethods: either unweigthed, or weighted by TF-\nIDF. We use both unigrams, trigrams or both (based\non preliminary experiments). For each algorithm,\nwe made a Cross Validation to determine the best\nhyperparameters of each classifier. All baselines\nwere implemented with Scikit-Learn library (Pe-\ndregosa et al., 2011).\nLanguage Models As regards pretrained lan-\nguage models, we used FlauBERT (Le et al.,\n2020) and CamemBERT (Martin et al., 2020), two\ngeneric-purpose pretrained models for French, as\nwell as JuriBERT (Douka et al., 2021), a language\nmodel trained only on data from the legal domain.\nBecause of the 512 tokens limitation of language\nmodels, the classifiers could not read the entire\ndata in input, and thus did not make the difference\nbetween F+R and F+R+J, or didn’t have all the\nargumentation of the judge. Therefore, we chunked\neach document based on line breaks, encode each\nchunk separately with the language model, and run\na bi-LSTM (2 layers of 256 units) on the chunk\nrepresentations to obtain a fixed-size vector for the\nwhole document. We use dropout before feeding\nthe representation to a classification layer.\nFinally, to tokenize the data, we used the tokeniz-\ners associated with each language models, provided\nby the platform Hugging Face, and the associated\nlibrary transformers (Wolf et al., 2020).\n5 Experiments\nBefore starting data processing, we split the corpus\ninto three stratified subsets: training (68%), testing\n(20%) and validation (12%).\n40\nInput Language Model Accuracy F1-Score\nFacts + Reasons\nMNB+ bag-of-unigrams without TF-IDF0.57 0.54\nFlauBERT 0.51 0.39\nCamemBERT 0.52 0.44\nJuriBERT 0.48 0.42\nFacts + Reasons + Judgment\nMLP + bag of-trigrams without TF-IDF0.56 0.50\nFlauBERT 0.48 0.36\nCamemBERT 0.49 0.43\nJuriBERT 0.55 0.42\nDecision\nMLP + bag of-trigrams without TF-IDF 0.89 0.88\nFlauBERT 0.90 0.89\nCamemBERT 0.88 0.88\nJuriBERT 0.85 0.86\nDecision + Judgment\nMLP + bag of-trigrams without TF-IDF 0.74 0.69\nFlauBERT 0.95 0.94\nCamemBERT 0.92 0.92\nJuriBERT 0.96 0.96\nTable 1: Accuracy and F1-Score score for the best baseline in each setting and for the three classifiers based on\nlanguage models: FlauBERT, CamemBERT & JuriBERT. MLP : Multi-Layer Perceptron ; MNB : multinomial\nnaive Bayes.\nThe BERT-based classifiers were trained for 80\nepochs, with a batch size of 1, on a GPU. We\nused the AdamW optimizer (Loshchilov and Hutter,\n2019) to minimize the loss function (negative log\nlikelihood), and a scheduler with warmup, to vary\nthe learning rate for a few warm-up periods. We\ntuned the learning rate ({10−6, 2 ·10−6, 5 ·10−6}\nfor FlauBERT, {10−5, 10−6, 2 ·10−6, 5 ·10−6},\n{10−5, 2 ·10−5, 2 ·10−6, 5 ·10−6}for JuriBERT)\nand the dropout value (0.15, 0.25)\nFor each classifier, we selected the best models\nbased on validation F1 score (macro-average over\nclasses. We also report accuracy in results.\nResults and Discussion The test results for the\nbest baseline (as selected on the validation set) in\neach setting and for the three classifiers based on\nlanguage models are presented in Table 1. As re-\ngards the best baseline, we note that the best results\nwere obtained on the decision and the judgment\n+ decision, i.e. the parts of the rulings with the\noutcome explicitly stated. We found out that the\nMLP with bag-of-trigrams without TF-IDF weight-\ning performed the best on all categories, except for\nthe facts + reasons, where the multinomial naive\nBayes algorithm with bag-of-unigrams without TF-\nIDF weighting reached the highest score. Finally,\nthe results for F+R and F+R+J input are below\n60%, slighlty above the most-frequent-label base-\nline which scores 50%.\nFor the language models, the categories with the\nhighest results were those with the verdict stated.\nThese results are even better than those of the base-\nline, exceeding 90% accuracy. The F1-score is also\nvery high, showing it has nothing to do with the\nimbalance of the corpus.\nOverall, we conclude that the classifiers based\non pretrained language models have no difficulty\nreading over the decision and inferring the label\n(judgment+decision setting), however they struggle\nto infer what the decision will be when they are\nfed only the facts, reasons and judgment informa-\ntion. We hypothesize that the difficulty stems from\nthe length of documents (on average 1095 tokens\nfor F+R+J input). Moreover the key information\nusable to make a reasonable prediction is often\nscarce: most of the reasons cite the relevant articles\nand laws (that are not predictive of the output but\nmake counfounding factors), which makes it hard\nto distinguish information that is important for the\ndecision from irrelevant information. Finally, the\ndocuments do not follow standardized writing and\ndiffer considerably between each other (depending\non the writer and the tradition of the local court).\nThe mismatch in results between settings with\nand without decision is an argument in our opinion\nin favour of the need to manually curate datasets\nfor legal NLP, rather than only relying on automatic\nconstruction.\nHuman performance on the taskAs a control\nexperiment, we assessed the performance of hu-\n41\nmans (either law experts or laypeople) on the task,\non a sample of 100 documents, where humans were\nshown the F+R input, and had to make a guess at\nthe output. Both law experts and laypeople had\naccuracies between 89 and 96%, which shows that\nthe inputs contain enough information to predict\nthe label.\n6 Conclusion\nOur experiments have shown the limitations of lan-\nguage models on judicial documents, since classi-\nfiers based on these models fail to understand the\ntextual input data. They perform well when the ver-\ndict is already given, but fail when the text is more\nimportant and requires a deeper analytical reading,\nduring the court’s argumentation for example.\nOn the question of children’s habitual residency,\nthe subject is far too sensitive to entrust the task\nof deciding to an AI, and we can only imagine\ncomplementary tools, making it possible to reduce\nthe duration of procedures, to support citizens in\nputting together their case or to help the parties’\nlawyers. By successfully processing data in such a\nway that a classifier can identify the most relevant\nterms and phrases, it would be possible to find the\n‘winning arguments,’ those that convince the judge\nor that can tip a case over the edge. This could then\nhelp lawyers to build their arguments, by giving\nthem access to these elements in cases similar to\ntheir own.\nEthics Statement\nNLP applied to legal decisions raises several key\nethical questions. Processing legal decision is\nimportant for understanding how legal reasoning\nwork, and might also be used to analyse judges’\nbiases in their decisions. However, as all techno-\nlogical tools, it is prone to dual use (Hovy and\nSpruit, 2016). The main risk is that institutions\nimplement such models to replace judges, with\nharmful consequences, since machine learning sys-\ntems are known to amplify biases they are exposed\nto in the training data, and are oftentimes not easily\ninterpretable in their predictions, which makes un-\nusable in a context where they may have an impact\non humans.\nRegarding personal data, the legal publisher who\nprovided the data did not anonymize the decisions\nto a GDPR compliant standard. Indeed, they only\nreplaced surnames with initials, but kept firstnames\nand town names unchanged. As a result, we are un-\nable to release the annotated data. We acknowledge\nthat not doing so is a hindrance for reproducibil-\nity and open science. In this case, we judge that\npersonal data safety should be prioritized.\nAcknowledgements\nWe thank Chen Chen for working on a pilot study\nthat led to this paper, as well as Nour Safa (PhD\nstudent at Université Grenoble Alpes) and Djo-\nhar Sidhoum-Rahal (then a post-doc at Université\nGrenoble Alpes) for their work on the annotation of\nthe dataset. We acknowledge the financial support\nof MIAI@Grenoble Alpes (ANR-19-P3IA-0003).\nReferences\nNikolaos Aletras, Dimitrios Tsarapatsanis, Daniel\nPreo¸ tiuc-Pietro, and Vasileios Lampos. 2016. Pre-\ndicting judicial decisions of the european court of\nhuman rights: A natural language processing per-\nspective. PeerJ computer science, 2:e93.\nIlias Chalkidis, Ion Androutsopoulos, and Nikolaos Ale-\ntras. 2019. Neural legal judgment prediction in En-\nglish. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4317–4323, Florence, Italy. Association for Compu-\ntational Linguistics.\nStella Douka, Hadi Abdine, Michalis Vazirgiannis, Ra-\njaa El Hamdani, and David Restrepo Amariles. 2021.\nJuriBERT: A masked-language model adaptation for\nFrench legal text. In Proceedings of the Natural Le-\ngal Language Processing Workshop 2021, pages 95–\n101, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nDirk Hovy and Shannon L. Spruit. 2016. The social\nimpact of natural language processing. In Proceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 591–598, Berlin, Germany. Association\nfor Computational Linguistics.\nDaniel Martin Katz, Michael J Bommarito II au2, and\nJosh Blackman. 2014. Predicting the behavior of\nthe supreme court of the united states: A general\napproach.\nArshdeep Kaur and Bojan Bozic. 2019. Convolutional\nneural network-based automatic prediction of judg-\nments of the european court of human rights. In\nAICS, pages 458–469.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoit Crabbé, Laurent Besacier, and Didier\nSchwab. 2020. FlauBERT: Unsupervised language\nmodel pre-training for French. In Proceedings of\nthe 12th Language Resources and Evaluation Confer-\nence, pages 2479–2490, Marseille, France. European\nLanguage Resources Association.\n42\nZhenyu Liu and Huanhuan Chen. 2017. A predictive\nperformance comparison of machine learning models\nfor judicial cases. In 2017 IEEE Symposium series on\ncomputational intelligence (SSCI), pages 1–6. IEEE.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 7203–\n7219, Online. Association for Computational Lin-\nguistics.\nMasha Medvedeva, Michel V ols, and Martijn Wieling.\n2020. Using machine learning to predict decisions of\nthe european court of human rights. Articial Intelli-\ngence and Law, 28(2):237–266.\nMasha Medvedeva, Martijn Wieling, and Michel V ols.\n2023. Rethinking the field of automatic prediction\nof court decisions. Artificial Intelligence and Law,\n31(1):195–212.\nConor O’Sullivan and Joeran Beel. 2019. Predicting the\noutcome of judicial decisions made by the european\ncourt of human rights. CoRR, abs/1912.10819.\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,\nMathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-\ncent Dubourg, et al. 2011. Scikit-learn: Machine\nlearning in Python. Journal of Machine Learning\nResearch, 12(Oct):2825–2830.\nOlivier Salaün, Philippe Langlais, Andrés Lou, Hannes\nWestermann, and Karim Benyekhlef. 2020. Anal-\nysis and multilabel classification of quebec court\ndecisions in the domain of housing law. Natu-\nral Language Processing and Information Systems,\n12089:135 – 143.\nOctavia-Maria Sulea, Marcos Zampieri, Shervin Mal-\nmasi, Mihaela Vela, Liviu P. Dinu, and Josef van\nGenabith. 2017. Exploring the use of text classifica-\ntion in the legal domain. CoRR, abs/1710.09306.\nOctavia-Maria ¸ Sulea, Marcos Zampieri, Mihaela Vela,\nand Josef van Genabith. 2017. Predicting the law\narea and decisions of French Supreme Court cases. In\nProceedings of the International Conference Recent\nAdvances in Natural Language Processing, RANLP\n2017, pages 716–722, Varna, Bulgaria. INCOMA\nLtd.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\n43",
  "topic": "Appeal",
  "concepts": [
    {
      "name": "Appeal",
      "score": 0.8918122053146362
    },
    {
      "name": "Classifier (UML)",
      "score": 0.7327386140823364
    },
    {
      "name": "Computer science",
      "score": 0.660596489906311
    },
    {
      "name": "Natural language processing",
      "score": 0.5275489687919617
    },
    {
      "name": "Task (project management)",
      "score": 0.5201533436775208
    },
    {
      "name": "Strengths and weaknesses",
      "score": 0.5010626316070557
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4923502206802368
    },
    {
      "name": "Judicial opinion",
      "score": 0.43021655082702637
    },
    {
      "name": "Construct (python library)",
      "score": 0.4298778474330902
    },
    {
      "name": "Law",
      "score": 0.36920005083084106
    },
    {
      "name": "Political science",
      "score": 0.22567468881607056
    },
    {
      "name": "Psychology",
      "score": 0.15930229425430298
    },
    {
      "name": "Engineering",
      "score": 0.07223615050315857
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210104430",
      "name": "Laboratoire d'Informatique de Grenoble",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210136308",
      "name": "Translational Innovation in Medicine and Complexity",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210124956",
      "name": "GIPSA-Lab",
      "country": "FR"
    }
  ]
}