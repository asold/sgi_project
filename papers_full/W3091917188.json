{
  "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages",
  "url": "https://openalex.org/W3091917188",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4221697252",
      "name": "Bhattamishra, Satwik",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A4225789619",
      "name": "Ahuja, Kabir",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221697254",
      "name": "Goyal, Navin",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963059228",
    "https://openalex.org/W3035314023",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2970738355",
    "https://openalex.org/W2972515356",
    "https://openalex.org/W2963723151",
    "https://openalex.org/W2997645120",
    "https://openalex.org/W200063938",
    "https://openalex.org/W3006881356",
    "https://openalex.org/W2962749806",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1581242383",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2056647002",
    "https://openalex.org/W2949292434",
    "https://openalex.org/W3017116062",
    "https://openalex.org/W2973122905",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2995273672",
    "https://openalex.org/W2106878575",
    "https://openalex.org/W2951977278",
    "https://openalex.org/W2964189376",
    "https://openalex.org/W3012835873",
    "https://openalex.org/W2963753324",
    "https://openalex.org/W2607666415",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W3023252952",
    "https://openalex.org/W2115912164",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2901264502",
    "https://openalex.org/W2983902802",
    "https://openalex.org/W2039934838",
    "https://openalex.org/W2914557243"
  ],
  "abstract": "Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7096–7116,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n7096\nOn the Ability and Limitations of Transformers to\nRecognize Formal Languages\nSatwik Bhattamishra♠ Kabir Ahuja♦∗ Navin Goyal♠\n♠Microsoft Research India\n♦Udaan.com\n{t-satbh,navingo}@microsoft.com\nkabir.ahuja@udaan.com\nAbstract\nTransformers have supplanted recurrent mod-\nels in a large number of NLP tasks. However,\nthe differences in their abilities to model dif-\nferent syntactic properties remain largely un-\nknown. Past works suggest that LSTMs gener-\nalize very well on regular languages and have\nclose connections with counter languages. In\nthis work, we systematically study the ability\nof Transformers to model such languages as\nwell as the role of its individual components\nin doing so. We ﬁrst provide a construction\nof Transformers for a subclass of counter lan-\nguages, including well-studied languages such\nas n-ary Boolean Expressions, Dyck-1, and\nits generalizations. In experiments, we ﬁnd\nthat Transformers do well on this subclass,\nand their learned mechanism strongly corre-\nlates with our construction. Perhaps surpris-\ningly, in contrast to LSTMs, Transformers do\nwell only on a subset of regular languages with\ndegrading performance as we make languages\nmore complex according to a well-known mea-\nsure of complexity. Our analysis also provides\ninsights on the role of self-attention mecha-\nnism in modeling certain behaviors and the\ninﬂuence of positional encoding schemes on\nthe learning and generalization abilities of the\nmodel.\n1 Introduction\nTransformer (Vaswani et al., 2017) is a self-\nattention based architecture which has led to state-\nof-the-art results across various NLP tasks (Devlin\net al., 2019; Liu et al., 2019; Radford et al., 2018).\nMuch effort has been devoted to understand the\ninner workings and intermediate representations of\npre-trained models; Rogers et al. (2020) is a recent\nsurvey. However, our understanding of their practi-\ncal ability to model different behaviors relevant to\nsequence modeling is still nascent.\n∗This research was conducted during the author’s intern-\nship at Microsoft Research.\nRegular   \nContext-Free \nContext-Sensitive \nRecursively\tEnumerable \nCounter \nFigure 1: Counter languages form a strict superset of\nregular languages, and are a strict subset of context-\nsensitive languages. Counter and context-free lan-\nguages have a nonempty intersection and neither set is\ncontained in the other.\nOn the other hand, a long line of research has\nsought to understand the capabilities of recurrent\nneural models such as the LSTMs (Hochreiter and\nSchmidhuber, 1997) . Recently, Weiss et al. (2018),\nSuzgun et al. (2019a) showed that LSTMs are ca-\npable of recognizing counter languages such as\nDyck-1 and anbn by learning to perform counting\nlike behavior. Suzgun et al. (2019a) showed that\nLSTMs can recognize shufﬂes of multiple Dyck-\n1 languages, also known as Shufﬂe-Dyck. Since\nTransformer based models (e.g., GPT-2 and BERT)\nare not equipped with recurrence and start compu-\ntation from scratch at each step, they are incapable\nof directly maintaining a counter. Moreover, it is\nknown that theoretically RNNs can recognize any\nregular language in ﬁnite precision, and LSTMs\nwork well for this task in practical settings. How-\never, Transformer’s ability to model such properties\nin practical settings remains an open question.\nPrior to the current dominance of Transformers\nfor NLP tasks, recurrent models like RNN-based\nmodels such as LSTMs were the most common\nchoice, and their computational capabilities have\n7097\nbeen studied for decades, e.g., (Kolen and Kremer,\n2001). In this work, we investigate the ability of\nTransformers to express, learn, and generalize on\ncertain counter and regular languages. Formal lan-\nguages provide us a controlled setting to study a\nnetwork’s ability to model different syntactic prop-\nerties in isolation and the role of its individual com-\nponents in doing so.\nRecent work has demonstrated close connections\nbetween LSTMs and counter automata. Hence, we\nseek to understand the capabilities of Transform-\ners to model languages for which the abilities of\nLSTMs are well understood. We ﬁrst show that\nTransformers are expressive enough to recognize\ncertain counter languages like Shufﬂe-Dyck and\nn-ary Boolean Expressions by using self-attention\nmechanism to implement the relevant counter oper-\nations in an indirect manner. We then extensively\nevaluate the model’s learning and generalization\nabilities on such counter languages and ﬁnd that\nmodels generalize well on such languages. Visu-\nalizing the intermediate representations of these\nmodels shows strong correlations with our pro-\nposed construction. Although Transformers can\ngeneralize well on some popularly used counter\nlanguages, we observe that they are limited in their\nability to recognize others. We ﬁnd a clear con-\ntrast between the performance of Transformers\nand LSTMs on regular languages (a subclass of\ncounter languages). Our results indicate that, in\ncontrast to LSTMs, Transformers achieve limited\nperformance on languages that involve modeling\nperiodicity, modular counting, and even simpler\nstar-free variants of Dyck-1, which they were able\nto recognize effortlessly. Our analysis provides\ninsights about the signiﬁcance of different compo-\nnents, namely self-attention, positional encoding,\nand the number of layers. Our results also show\nthat positional masking and positional encoding\ncan both aid in generalization and training, but in\ndifferent ways. We conduct extensive experiments\non over 25 carefully chosen formal languages. Our\nresults are perhaps the ﬁrst indication of the limita-\ntions of Transformers for practical-sized problems\nthat are, in a precise sense, very simple, and in\nparticular, easy for recurrent models.\n2 Related Work\nNumerous works, e.g., Suzgun et al. (2019b);\nSennhauser and Berwick (2018); Skachkova et al.\n(2018), have attempted to understand the capabil-\nities and inner workings of recurrent models by\nempirically analyzing them on formal languages.\nWeiss et al. (2018) showed that LSTMs are capa-\nble of simulating counter operations and explored\ntheir practical ability to recognize languages like\nanbn and anbncn. Suzgun et al. (2019a) further\nshowed that LSTMs can learn to recognize Dyck-1\nand Shufﬂe-Dyck and can simulate the behavior of\nk-counter machines. Theoretical connections of re-\ncurrent models have been established with counter\nlanguages (Merrill, 2019; Merrill et al., 2020; Mer-\nrill, 2020). It has also been shown that RNN based\nmodels can recognize regular languages (Kolen and\nKremer, 2001; Korsky and Berwick, 2019) and ef-\nforts have been made to extract DFAs from RNNs\ntrained to recognize regular languages (Weiss et al.,\n2019; Wang et al., 2018b; Michalenko et al., 2019).\nWe are not aware of such studies for Transformers.\nRecently, researchers have sought to empirically\nanalyze different aspects of the Transformer trained\non practical NLP tasks such as the information con-\ntained in intermediate layers (Rogers et al., 2020;\nReif et al., 2019; Warstadt et al., 2019). V oita et al.\n(2019) studied the role of different types of atten-\ntion heads. Yang et al. (2019); Tsai et al. (2019) ex-\namined the ability of the model to learn order infor-\nmation via different positional encoding schemes.\nComplementary to these, our work is focused on\nanalyzing Transformer’s ability to model particu-\nlar behaviors that could be relevant to modeling\nlinguistic structure. Recently, it has been shown\nthat Transformers are Turing-complete (P´erez et al.,\n2019; Bhattamishra et al., 2020) and are univer-\nsal approximators of sequence-to-sequence func-\ntions given arbitrary precision (Yun et al., 2020).\nHahn (2020) shows that Transformers cannot recog-\nnize languages Parity and Dyck-2. However, these\nresults only apply to very long words, and their\napplicability to practical-sized inputs is not clear\n(indeed, we will see different behavior for practical-\nsized input). Moreover, these results concern the\nexpressive power of Transformers and do not ap-\nply to learning and generalization abilities. Thus\nTransformers’ ability to model formal languages\nrequires further investigation.\n3 Deﬁnitions\nWe consider the Transformer as used in popular pre-\ntrained LM models such as BERT and GPT, which\nis the encoder-only model of the original seq-to-seq\narchitecture (Vaswani et al., 2017). The encoder\n7098\nconsists of multiple layers with two blocks each:\n(1) self-attention block, (2) a feed-forward network\n(FFN). For 1 ≤i≤n, at the i-th step, the model\ntakes as input the sequence s1,s2,...,s i where\ns ∈Σ and generates the output vector yi. Each\ninput si is ﬁrst converted into an embedding vector\nusing the function fe : Σ →Rdmodel and usually\nsome form of positional encoding is added to yield\nthe ﬁnal input vectorxi. The embedding dimension\ndmodel is also the dimension of intermediate vectors\nof the network. Let Xi := (x1,..., xi) for i≥1.\nIn the self-attention block, the input vectors un-\ndergo linear transformations Q(·),K(·), and V(·)\nyielding the corresponding query, key and value\nvectors, respectively. The self-attention mechanism\ntakes as input a query vector Q(xi), key vectors\nK(Xi), and value vectors V(Xi). An attention-\nhead denoted by Att(Q(xi),K(Xi),V (Xi)), is\na vector ai = ∑i\nj=1 αjvj, where (α1,...,α i) =\nsoftmax(⟨Q(xi),K(x1)⟩,..., ⟨Q(xi),K(xi)⟩).\nThe output of a layer denoted by zi is computed\nby zi = O(ai) where 1 ≤i ≤nand O(·) typi-\ncally denotes an FFN with ReLU activation. The\ncomplete L-layer model is a repeated application\nof the single-layer model described above, which\nproduces a vector zL\ni at its ﬁnal layer where Lde-\nnotes the last layer. The ﬁnal output is obtained by\napplying a projection layer with some normaliza-\ntion or an FFN over the vectorszL\ni ’s and is denoted\nby yi = F(zL\ni ). Residual connections and layer\nnormalization are also applied to aid the learning\nprocess of the network.\nIn an LM setting, when the Transformer pro-\ncesses the input sequentially, each input symbol\ncan only attend over itself and the previous inputs,\nmasking is applied over the inputs following it.\nNote that, providing positional information in this\nform via masked self-attention is also referred to\nas positional masking (Vaswani et al., 2017; Shen\net al., 2018). A Transformer model without posi-\ntional encoding and positional masking is order-\ninsensitive.\n3.1 Formal Languages\nFormal languages are abstract models of the syn-\ntax of programming and natural languages; they\nalso relate to cognitive linguistics, e.g., J¨ager and\nRogers (2012); Hahn (2020) and references therein.\nCounter Languages. These are languages recog-\nnized by a deterministic counter automaton (DCA),\nthat is, a DFA with a ﬁnite number of unbounded\ncounters (Fischer et al., 1968). The counters can be\nincremented/decremented by constant values and\ncan be reset to 0 (details in App. B.1). The com-\nmonly used counter languages to study sequence\nmodels are Dyck-1, anbn, and anbncn. Several\nworks have explored the ability of recurrent models\nto recognize these languages as well as their under-\nlying mechanism to do so. We include them in our\nanalysis as well as some general form of counter\nlanguages such as Shufﬂe-Dyck (as used in Suzgun\net al. (2019a)) and n-ary Boolean Expressions. The\nlanguage Dyck-1 over alphabet Σ = {[,]}consists\nof balanced parentheses deﬁned by derivation rules\nS →[ S ] |SS |ϵ. Shufﬂe-Dyck is a family\nof languages containing shufﬂes of Dyck-1 lan-\nguages. Shufﬂe-kdenotes the shufﬂe of kDyck-1\nlanguages: it contains kdifferent types of brackets,\nwhere each type of bracket is required to be well-\nbalanced, but their relative order is unconstrained.\nFor instance, a Shufﬂe-2 language over alphabet\nΣ = {[,],(,)}contains the words ([)] and [((]))\nbut not ])[(. We also consider n-ary Boolean Ex-\npressions (hereby BoolExp-n), which are a family\nof languages of valid Boolean expressions (in the\npreﬁx notation) parameterized by the number of\noperators and their individual arities. For instance,\nan expression with unary operator ∼and binary op-\nerator ∧contains the word ‘∧∼ 01’ but not ‘∼10’\n(formal deﬁnitions in App. B).\nNote that, although languages such as Dyck-1\nand anbn are context-free, a DCA with a single\ncounter is sufﬁcient to recognize Dyck-1 and anbn.\nSimilarly, a DCA with two single-turn counters\ncan recognize anbncn. On the other hand, recog-\nnizing Shufﬂe-Dyck requires multiple multi-turn\ncounters, where for a given type of bracket, its cor-\nresponding counter is incremented or decremented\nby 1. Hence, it represents a more general form of\ncounter languages. Similarly, recognizing BoolExp\nrequires a 1-counter DCA with the counter updates\ndepending on the operator: a ternary operator will\nincrement the counter by 2 (= arity −1) whereas\na unary operator will increment it by 0. Figure 1\nshows the relationship between counter languages\nand other classes of formal languages.\nRegular Languages. Regular languages, perhaps\nthe best studied class of formal languages, form a\nsubclass of counter languages1. They neatly divide\n1For simplicity, from now on, we will refer to a particular\nlanguage as a counter language if a DCA with a nonzero\nnumber of counters is necessary to recognize it, else we will\nrefer to it as a regular language.\n7099\ninto two subclasses: star-free and non-star-free.\nStar-free languages can be described by regular\nexpressions formed by union, intersection, comple-\nmentation, and concatenation operators but not the\nKleene star (∗). Like regular languages, star-free\nlanguages are surprisingly rich with algebraic, log-\nical, and multiple other characterizations and con-\ntinue to be actively researched, e.g., (McNaughton\nand Papert, 1971; J¨ager and Rogers, 2012). They\nform a simpler subclass of regular languages where\nthe notion of simplicity can be made precise in\nvarious ways, e.g. they are ﬁrst-order logic deﬁn-\nable and cannot represent languages that require\nmodular counting.\nWe ﬁrst consider Tomita grammars containing 7\nregular languages representable by DFAs of small\nsizes, a popular benchmark for evaluating recur-\nrent models and extracting DFA from trained re-\ncurrent models (see, e.g., Wang et al. (2018a)).\nTomita grammars contain both star-free and non-\nstar-free languages. We further investigate some\nnon-star-free languages such as (aa)∗, Parity and\n(abab)∗. Parity contains words over {0,1}with an\neven number of 1’s. Similarly (aa)∗and (abab)∗\nrequire modeling periodicity.\nOn the other hand, the seemingly similar looking\nlanguage (ab)∗is star-free: (ab)∗= (b∅c + ∅ca+\n∅caa∅c + ∅cbb∅c)c, where ·c denotes set comple-\nmentation, and thus ∅c = Σ∗. The dot-depth of a\nstar-free language is a measure of nested concatena-\ntion or sequentiality required in a star-free regular\nexpression (formal deﬁnition in App. B.2). We\ndeﬁne a family D0,D1,... of star-free languages.\nFor n ≥0, the language Dn over Σ = {a,b}is\ndeﬁned inductively as follows: Dn = (aDn−1b)∗\nwhere D0 = ϵ, the empty word. Thus D1 = (ab)∗\nand D2 = (a(ab)∗b)∗. Language Dn is known to\nhave dot-depth n.\nThe list of all considered languages and their\ndeﬁnitions are provided in the App. B.\n4 Expressiveness Results\nProposition 4.1. There exists a Transformer as\ndeﬁned in Section 3 that can recognize the family\nof languages Shufﬂe-Dyck.\nProof. Let s1,s2,...,s n denote a sequence\nw ∈ Shufﬂe-k over the alphabet Σ =\n{[0,..., [k−1,]0,..., ]k−1}. The language Shufﬂe-\n1 is equivalent to Dyck-1. For any Shufﬂe- klan-\nguage, consider a model with dmodel = 2k, where\nthe embedding function fe is deﬁned as follows.\nFor each type of open bracket [j where,0 ≤j <k,\nthe vector fe([j) has the value +1 and −1 at the in-\ndices 2jand 2j+ 1, respectively. It has the value 0\nat the rest of the indices. Similarly for each closing\nbracket, the vector fe(]j) has the value −1 and +1\nat the indices 2jand 2j+ 1, and it has the value 0\nat the rest of the indices. For Dyck-1, this would\nlead to fe([) = [+1,−1]T and fe(]) = [−1,+1]T\n(with dmodel = 2). We use a single-layer Trans-\nformer where we set the matrix corresponding to\nlinear transformation for key vectors to be null ma-\ntrix, that is K(x) = 0 for all x. This will lead to\nequal attention weights for all inputs. The matrices\ncorresponding to Q(·) and V(·) are set to Identity.\nThus, Att(Q(xi),K(Xi),V (Xi)) = 1\ni\n∑i\nj=1 vj\nfor 1 ≤i ≤n. Hence, at the i-th step, the self-\nattention block produces a vector ai which has\nthe values σ([j)−σ(]j)\ni at indices 2j and the values\nσ(]j)−σ([j)\ni at indices 2j+ 1, where σ(s) denotes\nthe number of occurrence of the symbol s. For\ninstance, in Dyck-1, if in the ﬁrst iinputs, there are\nσ([) open brackets and σ(]) closing brackets, then\nai = [σ([)−σ(])\ni ,σ(])−σ([)\ni ]T, where i= σ([) +σ(]).\nIn ai, the value σ([) −σ(]) represents the depth\n(difference between the number of open and clos-\ning brackets) of the Dyck-1 word at indexi. Hence,\nthe ﬁrst coordinate is the ratio of the depth of the\nDyck-1 word and its length at that index, while the\nother coordinate is its negative.\nWe then apply a simple FFN with ReLU ac-\ntivation over the vector ai. The vector zi =\nReLU(Iai). The even indices of the vector zi will\nbe nonzero if the number of open brackets of the\ncorresponding type is greater than the number of\nclosing brackets. A similar statement holds for the\nodd indices. Thus, for a given word to be in Shufﬂe-\nk, the values at odd indices of the vector zi must\nnever be nonzero, and the values of all coordinates\nmust be zero at the last step to ensure the number\nof open and closing brackets are the same.\nFor an input sequence s1,s2,...,s n, the model\nwill produce z1,..., zn based on the construction\nspeciﬁed above. A word w belongs to language\nShufﬂe-kif zi,2j+1 = 0 for all 1 ≤i≤n, 0 ≤j <\nkand zn = 0 and does not belong to the language\notherwise. This can be easily implemented by an\nadditional layer of self-attention and feedforward\nnetwork to classify a given sequence.\nThe bottleneck for precision in the construc-\n7100\nLanguage Model Bin-1 Accuracy\n[1, 50]↑\nBin-2 Accuracy\n[51, 100]↑\nBin-3 Accuracy\n[101, 150]↑\nShufﬂe-2\nLSTM (Baseline) 100.0 100.0 100.0\nTransformer (Absolute Positional Encodings) 100.0 85.2 63.3\nTransformer (Relative Positional Encodings) 100.0 51.6 3.8\nTransformer (Only Positional Masking) 100.0 100.0 93.0\nBoolExp-3\nLSTM (Baseline) 100.0 100.0 99.7\nTransformer (Absolute Positional Encodings) 100.0 90.6 51.3\nTransformer (Relative Positional Encodings) 100.0 96.0 68.4\nTransformer (Only Positional Masking) 100.0 100.0 99.8\nanbncn\nLSTM (Baseline) 100.0 100.0 97.8\nTransformer (Absolute Positional Encodings) 100.0 62.1 5.3\nTransformer (Relative Positional Encodings) 100.0 31.3 22.0\nTransformer (Only Positional Masking) 100.0 100.0 100.0\nTable 1: The performance of Transformers and LSTMs on the respective counter languages. Refer to section 6 for\ndetails. Performance on other counter languages such as Shufﬂe-4 and Shufﬂe-6 are listed in Table 8 in appendix.\ntion above is the calculation of values of the form\nσ([)−σ(])\ni in the vector ai. Since in a ﬁnite precision\nsetting with r bits, this can be computed up to a\nvalue exponential in r, our proof entails that Trans-\nformers can recognize languages in Shufﬂe-Dyck\nfor lengths exponential in the number of bits.\nUsing a similar logic, one can also show that\nTransformers can recognize the family of languages\nBoolExp-n(refer to Lemma C.2). By setting the\nvalue vectors according to the arities of the opera-\ntors, the model can obtain the ratio of the counter\nvalue of the underlying automata and the length of\nthe input at each step via self-attention. Although\nthe above construction is speciﬁc to these language\nfamilies, we provide a proof for a more general\nbut restricted subclass of Counter Languages in the\nappendix (refer to Lemma C.1). The above con-\nstruction serves to illustrate how Transformers can\nrecognize such languages by indirectly doing rele-\nvant computations. As we will later see, this will\nalso help us interpret how trained models recognize\nsuch languages.\n5 Experimental Setup\nIn our experiments, we consider 27 formal lan-\nguages belonging to different parts in the hierarchy\nof counter and regular languages. For each lan-\nguage, we generate samples within a ﬁxed-length\nwindow for our training set and generate multiple\nvalidation sets with different windows of length to\nevaluate the model’s generalization ability.\nFor most of the languages, we generate 10k sam-\nples for our training sets within lengths 1 to 50 and\ncreate different validation sets containing samples\nwith distinct but contiguous windows of length.\nThe number of samples in each validation set is\n2k, and the width of each window is about 50. For\nlanguages that have very few positive examples\nin a given window of length, such as (ab)∗ and\nanbncn, we train on all positive examples within\nthe training window. Similarly, each validation\nset contains all possible strings of the language\nfor a particular range. Table 6 in appendix lists\nthe dataset statistics of all 27 formal languages we\nconsider.2. We have made our source code avail-\nable at https://github.com/satwik77/Transformer-\nFormal-Languages.\n5.1 Training details\nWe train the model on character prediction task as\nintroduced in Gers and Schmidhuber (2001) and\nas used in Suzgun et al. (2019b,a). Similar to an\nLM setup, the model is only presented with posi-\ntive samples from the given language. For an input\nsequence s1,s2,...,s n, the model receives the se-\nquence s1,...,s i for 1 ≤i≤nat each step iand\nthe goal of the model is to predict the next set of\nlegal/valid characters in the (i+ 1)th step. From\nhere onwards, we say a model can recognize a lan-\nguage if it can perform the character prediction task\nperfectly.\nThe model assigns a probability to each charac-\nter in the vocabulary of the language corresponding\nto its validity in the next time-step. The output\ncan be represented by a k-hot vector where each\ncoordinate corresponds to a character in the vocab-\nulary of the language. The output is computed by\napplying a sigmoid activation over the scores as-\nsigned by the model for each character. Following\nSuzgun et al. (2019b,a), the learning objective of\n2Our experimental setup closely follows the setup of Suz-\ngun et al. (2019a,b) for RNNs\n7101\nthe model is to minimize the mean-squared error\nbetween the predicted probabilities and the k-hot\nlabels.3 During inference, we use a threshold of\n0.5 to obtain the predictions of the model. For a\ntest sample, the model’s prediction is considered\nto be correct if and only if its output at every step\nis correct. Note that, this is a relatively stringent\nmetric as a correct prediction is obtained only when\nthe output is correct at every step. The accuracy of\nthe model over test samples is the fraction of total\nsamples predicted correctly4. Similar to Suzgun\net al. (2019a) we consider models of small sizes to\nprevent them from memorizing the training set and\nmake it feasible to visualize the model. In our ex-\nperiments, we consider Transformers with up to 4\nlayers, 4 heads and the dimension of the intermedi-\nate vectors within 2 to 32. We extensively tune the\nmodel across various hyperparameter settings. We\nalso examine the inﬂuence of providing positional\ninformation in different ways such as absolute en-\ncodings, relative encodings (Dai et al., 2019) and\nusing only positional masking without any explicit\nencodings.\n6 Results on Counter Languages\nWe evaluated the performance of the model on 9\ncounter languages. Table 1 shows the performance\nof different models described above on some rep-\nresentative languages. We also include the perfor-\nmance of LSTMs as a baseline. We found that\nTransformers of small size (single head and single\nlayer) can generalize well on some general form\nof counter languages such as Shufﬂe-Dyck and\nBoolExp-n. Surprisingly, we observed this behav-\nior when the network was not provided any form of\nexplicit positional encodings, and positional infor-\nmation was only available in the form of masking.\nFor models with positional encoding, the lack of\nthe ability to generalize to higher lengths could be\nattributed to the fact that the model has never been\ntrained on some of the positional encodings that it\nreceives at test time. On the other hand, the model\nwithout any explicit form of positional encoding\nis less susceptible to such issues if it is capable of\nperforming the task and was found to generalize\nwell across various hyperparameter settings.\n3We also tried BCE loss in our initial experiments and\nfound similar results for languages such as Parity, Tomita\ngrammars and certain counter languages.\n4A discussion on the choice of character prediction task\nand its relation to other tasks such as standard classiﬁcation\nand LM is provided in section D.1 in the appendix.\n( [ ( ) ] ) ( ) [ [ [ [ ( ) ] ] ] ]\n20\n10\n0\nOutput of Self-Attention Block\nCoordinate-0\nc1 * '[' DL Ratio\nCoordinate-2\nc2 * '(' DL Ratio\n(a)\n+ ~ + > 0 0 > ~ 1 ~ ~ 1 1 1 0\n0.3\n0.2\n0.1\n0.0\nOutput of Self-Attention Block\nCoordinate-0\nk * CL ratio\n(b)\nFigure 2: Values of different coordinates of the output\nof self-attention block of the models trained on Shufﬂe-\n2 and BoolExp-3. The dotted lines are the scaled depth\nto length ratios for Shufﬂe-2 and scaled counter value\nto length ratios for BoolExp-3. We observe a near per-\nfect Pearson correlation coefﬁcent of 0.99 between out-\nputs of self attention block and the DL and CL ratios.\n6.1 Role of Self-Attention\nIn order to check our hypothesis in Sec. 4, we\nvisualize certain attributes of trained models that\ngeneralize well on Shufﬂe-2 and BoolExp-3.5 Our\nconstruction in Sec. 4 recognizes sequences in\nShufﬂe-Dyck by computing the depth to length\nratio of the input at each step via self-attention\nmechanism. For BoolExp-n, the model can achieve\nthe task similarly by computing the corresponding\ncounter value divided by length (refer to Lemma\nC.2). Interestingly, upon visualizing the outputs\nof the self-attention block for a model trained on\nShufﬂe-2, we found a strong correlation of its ele-\nments with the depth to length ratio. As shown in\nFig. 2a, different coordinates of the output vector\nof the self-attention block contain computations\ncorresponding to different counters of the Shufﬂe-2\nlanguage. We observe the same behavior for mod-\nels trained on Shufﬂe-4 language (refer to Figure 5\nin appendix). Similarly, upon visualizing a model\ntrained on Boolean Expressions with 3 operators,\nwe found strong correlation6 between its elements\nand the ratio of the counter value and length of\nthe input (refer to Figure 2b). This indicates that\nthe model learns to recognize inputs by carrying\nout the required computation in an indirect manner,\n5We take the model with the smallest number of parameters\nthat generalized well making it feasible for us to visualize it.\n6The Pearson correlation of values were ∼0.99\n7102\n1-Layer 2-Layer\nModel Type Bin 0 Bin 1 Bin 0 Bin 1\nPositional Masking 45.1 38.9 100.0 99.2\nPositional Encoding 55.8 37.9 100.0 99.6\nLSTM 100.0 100.0 100.0 100.0\nTable 2: Results on language Reset-Dyck-1 with differ-\nent number of layers.\nas described in our construction. Additionally, for\nboth models, we found that the attention weights of\nthe self-attention block were uniformly distributed\n(refer to Figure 4 in appendix). Further, on inspect-\ning the embedding and value vectors of the open\nand closing brackets, we found that their respec-\ntive coordinates were opposite in sign and similar\nin magnitude. As opposed to Shufﬂe-Dyck, for\nBoolExp-n, the magnitudes of the elements in the\nvalue vectors were according to their corresponding\narity. For instance, the magnitude for a ternary oper-\nator was (almost) thrice the magnitude for a unary\noperator (refer to Figure 3 in appendix). These\nobservations are consistent with our construction,\nindicating that the model uses its value vectors to\ndetermine the counter updates and then at each step,\naggregates all the values to obtain a form of the ﬁ-\nnal counter value in an indirect manner. This is\ncomplementary to LSTMs, which can simulate the\nbehavior of k-counters more directly by making\nrespective updates to its cell states upon receiving\neach input (Suzgun et al., 2019a).\n6.2 Limitations of the Single-Layer\nTransformer\nAlthough we observed that single-layer Transform-\ners are easily able to recognize some of the popu-\nlarly studied counter languages, at the same time,\nit is not necessarily true for counter languages\nthat require reset operations. We deﬁne a vari-\nant of the Dyck-1 language. Let Reset-Dyck- 1\nbe the language deﬁned over the alphabet Σ =\n{[,],#}, where # denotes a symbol that resets\nthe counter. Words in Reset-Dyck- 1 have the\nform Σ∗#v, where the string vbelongs to Dyck-\n1. When the machine encounters the reset sym-\nbol #, it must ignore all the previous input, reset\nthe counter to 0 and go to start state. It is easy\nto show that this cannot be directly implemented\nwith a single layer self-attention network with po-\nsitional masking (Lemma C.3 in Appendix). The\nkey limitation for both with and without encodings\nis the fact that for a single layer network the scor-\nTransformer LSTM\nLanguage Star-\nFree\nBin 0 Bin 1 Bin 0 Bin 1\nTomita 1 \u0013 100.0 100.0 100.0 100.0\nTomita 2 \u0013 100.0 100.0 100.0 100.0\nTomita 3 \u0017 75.4 10.8 100.0 100.0\nTomita 4 \u0013 100.0 92.4 100.0 100.0\nTomita 5 \u0017 29.3 0.0 100.0 100.0\nTomita 6 \u0017 88.8 0.0 100.0 100.0\nTomita 7 \u0013 100.0 100.0 100.0 100.0\nTable 3: Results on Tomita grammar\ning function ⟨Q(xn),K(x#)⟩and the value vector\ncorresponding to the reset symbol is independent of\nthe preceding inputs which it is supposed to negate\n(reset). The same limitation does not hold for multi-\nlayer networks where the value vector, as well as\nthe scoring function for the reset symbol, are de-\npendent on its preceding inputs. On evaluating the\nmodel on data generated from such a language, we\nfound that single-layer networks are unable to per-\nform well in contrast to networks with two layers\n(Table 2)7. LSTMs, on the other hand, can emulate\nthe reset operation using forget gate.\n7 Results on Regular Languages\nWe ﬁrst examine the popular benchmark of Tomita\ngrammars. While the LSTMs generalize perfectly\non all 7 languages, Transformers are unable to gen-\neralize on 3 languages, all of which are non-star-\nfree. Note that, all star-free languages in Tomita\ngrammar have dot-depth 1. Recognizing non-star-\nfree languages requires modeling properties such\nas periodicity and modular counting. Consequently,\nwe evaluate the model on some of the simplest non-\nstar-free languages such as the languages(aa)∗and\nParity. We ﬁnd that they consistently fail to learn\nor generalize on such languages, whereas LSTMs\nof very small sizes perform ﬂawlessly. Table 4 lists\nthe performance on some non-star-free languages.\nNote that LSTMs can easily recognize such simple\nnon-star-free languages considered here by using\nits internal memory and recurrence 8. However,\ndoing the same task via self-attention mechanism\nwithout using any internal memory could be highly\nnon-trivial and potentially impossible. Languages\nsuch as (aa)∗and Parity are among the simplest\n7The results and limitations of single-layer Transformers\nare conﬁned to this subsection. The rest of the results in\nthe paper are not speciﬁc to single-layer Transformers unless\nexplicitly mentioned.\n8For tasks such as Parity, LSTMs can simply ﬂip between\ntwo values in its hidden state upon receiving 1’s as input and\nignore when it receives 0’s as input.\n7103\nTransformer LSTM\nLanguage Property Bin 0 Bin 1 Bin 0 Bin 1\nParity non-SF 68.7 (23.0) 0.0 (0.0) 100.0 100.0\n(aa)∗ non-SF 100 (1.3) 0.0 (0.0) 100.0 100.0\n(abab)∗ non-SF 100.0 (9.9) 5.4 (0.0) 100.0 100.0\nD1 depth-1 100.0 100.0 100.0 100.0\nD2 depth-2 74.6 3.1 100.0 100.0\nD4 depth -4 90.2 3.3 100.0 100.0\nTable 4: Results on non-star-free languages (non-SF)\nand the language Dn. The values in parenthesis corre-\nspond to the scores obtained for a model without resid-\nual connections. This is to prevent the model from\nsolving the task by memorizing the positional encod-\nings and study the ability of self-attention mechanism\nto solve the task.\nnon-star-free languages, and hence limitations in\nrecognizing such languages carry over to a larger\nclass of languages. The results above may suggest\nthat the star-free languages are precisely the regular\nlanguages recognizable by Transformers. As we\nwill see in the next section, this is not so.\n7.1 Necessity of Positional Encodings\nThe architecture of Transformer imposes limita-\ntions for recognizing certain types of languages.\nAlthough Transformers seem to generalize well\nwhen they are capable of performing a task with\nonly positional masking, they are incapable of rec-\nognizing certain types of languages without ex-\nplicit positional encodings. We consider the fam-\nily of star-free languages Dn deﬁned in Sec. 3.1.\nNote that the task of recognizing Dn is equiva-\nlent to recognizing Dyck-1 with maximum depth n,\nwhere the symbols aand bin Dn are analogous to\nopen and closing brackets in Dyck-1 respectively.\nThe primary difference between recognizing Dn\nand Dyck-1 is that in case of Dn, when the input\nreaches the maximum depth n, the model must\npredict a(the open bracket) as invalid for the next\ncharacter, whereas in Dyck-1, open brackets are al-\nways allowed. We show that although Transformers\nwith only positional masking can generalize well\non Dyck-1, they are incapable of recognizing the\nlanguage Dn for n> 1. The limitation arises from\nthe fact that when the model receives a sequence\nof only a’s, then due to the softmax based aggrega-\ntion, the output of the self-attention block ai will\nbe a constant vector, implying that the output of\nthe feed-forward will also be a constant vector, that\nis, z1 = z2 = ... = zn. In case of languages such\nas Dn, if the input begins with nconsecutive as,\nthen, since the model cannot distinguish between\n(aa)∗ (aaaa)∗\nEncoding Scheme Bin 0 Bin 1 Bin 0 Bin 1\nPositional Masking 0.0 0.0 0.0 0.0\nAbsolute Encoding 1.3 0.0 6.7 0.0\nRelative Encoding 0.6 0.0 1.7 0.0\ncos(nπ) 100.0 100.0 0.0 0.0\nTrainable Embedding 100.0 0.0 100.0 0.0\nTable 5: Performance of transformer based models on\n(aa)∗ and (aaaa)∗, for different types of position en-\ncoding schemes. To separately study the effect of differ-\nent position encodings on the self attention mechanism,\nwe do not include residual connections in the models\nstudied here.\nthe n-th aand the preceding a’s, the model cannot\nrecognize the language Dn. This limitation does\nnot exist if the model is provided explicit positional\nencoding. Upon evaluating Transformers with po-\nsitional encodings on instances of the language Dn,\nwe found that the models are able to generalize to a\ncertain extent on strings within the same lengths as\nseen during training but fail to generalize on higher\nlengths (Table 4). It is perhaps surprising that small\nand simpler self-attention networks can generalize\nvery well on languages such as Dyck-1 but achieve\nlimited performance on a language that belongs to\na much simpler class such as star-free.\nSimilarly, since (aa)∗, is a unary language (al-\nphabet size is 1), the model will always receive the\nsame character at each step. Hence, for a model\nwith only positional masking, the output vector\nwill be the same at every step, making it incapable\nof recognizing the language (aa)∗. For the lan-\nguage Parity, when the input word contains only\n1’s, the task reduces to recognizing(11)∗and hence\na model without positional encodings is incapable\nof recognizing Parity even for very small lengths re-\ngardless of the size of the network (refer to Lemma\nC.4). We ﬁnd it surprising that for Parity, which\nis permutation invariant, positional encodings are\nnecessary for transformers to recognize them even\nfor very small lengths.\n7.2 Inﬂuence of Custom Positional Encodings\nThe capability and complexity of the network could\nsigniﬁcantly depend on the positional encoding\nscheme. For instance, for language (aa)∗, the abil-\nity of a self-attention network to recognize it de-\npends solely on the positional encoding. Upon eval-\nuating with standard absolute and relative encoding\nschemes, we observe that the model is unable to\nlearn or generalize well. At the same time, it is easy\nto show that if cos(nπ), which has a period of two\n7104\nis used as positional encoding, the self-attention\nmechanism can easily achieve the task which we\nalso observe when we empirically evaluated with\nsuch an encoding. However, the same encoding\nwould not work for a language such as (aaaa)∗,\nwhich has a periodicity of four. Table 5 shows the\nperformance of the model with different types of\nencodings. When we used ﬁxed-length trainable\npositional embeddings, the obtained learned em-\nbeddings were very similar to the cos(nπ) form;\nhowever, such embeddings cannot be used for se-\nquences of higher lengths. This also raises the need\nfor better learnable encodings schemes that can\nextrapolate to variable lengths of inputs not seen\nduring training data such as (Liu et al., 2020).\nOur experiments on over 15 regular languages\nseem to indicate that Transformers are able to gen-\neralize on star-free languages within dot-depth 1\nbut have difﬁculty with higher dot-depths or more\ncomplex classes like non-star-free languages. Ta-\nble 9 in Appendix lists results on all considered\nregular languages.\n8 Discussion\nWe showed that Transformers can easily generalize\non certain counter languages such as Shufﬂe-Dyck\nand Boolean Expressions in a manner similar to\nour proposed construction. Our visualizations im-\nply that Transformers do so with a generalizable\nmechanism instead of overﬁtting on some statistical\nregularities. Similar to natural languages, Boolean\nExpressions consist of recursively nested hierar-\nchical constituents. Recently, Papadimitriou and\nJurafsky (2020) showed that pretraining LSTMs\non formal languages like Shufﬂe-Dyck transfers\nto LM performance on natural languages. At the\nsame time, our results show clear limitations of\nTransformers compared to LSTMs on a large class\nof regular languages. Evidently, the performance\nand capabilities of Transformers heavily depend on\narchitectural constituents e.g., the positional encod-\ning schemes and the number of layers. Recurrent\nmodels have a more automata-like structure well-\nsuited for counter and regular languages, whereas\nself-attention networks’ structure is very different,\nwhich seems to limit their abilities for the consid-\nered tasks.\nOur work poses a number of open questions. Our\nresults are consistent with the hypothesis that Trans-\nformers generalize well for star-free languages with\ndot-depth 1, but not for higher depths. Clarifying\nthis hypothesis theoretically and empirically is an\nattractive challenge. What does the disparity be-\ntween the performance of Transformers on natural\nand formal languages indicate about the complexity\nof natural languages and their relation to linguistic\nanalysis? (See also Hahn (2020)). Another interest-\ning direction would be to understand whether cer-\ntain modiﬁcations or recently proposed variants of\nTransformers improve their performance on formal\nlanguages. Regular and counter languages model\nsome aspects of natural language while context-\nfree languages model other aspects such as hier-\narchical dependencies. Although our results have\nsome implications on them, we leave a detailed\nstudy on context-free languages for future work.\nAcknowledgements\nWe thank the anonymous reviewers for their con-\nstructive comments and suggestions. We would\nalso like to thank our colleagues at Microsoft Re-\nsearch and Michael Hahn for their valuable feed-\nback and helpful discussions.\nReferences\nSatwik Bhattamishra, Arkil Patel, and Navin Goyal.\n2020. On the computational power of transformers\nand its implications in sequence modeling. arXiv\npreprint arXiv:2006.09286.\nRina Cohen and Janusz Brzozowski. 1971. Dot-depth\nof star-free events. Journal of Computer and System\nSciences, 5:1–16.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nV olker Diekert and Paul Gastin. 2008. First-order de-\nﬁnable languages. In Logic and Automata: History\nand Perspectives [in Honor of Wolfgang Thomas] ,\nvolume 2 of Texts in Logic and Games , pages 261–\n306. Amsterdam University Press.\n7105\nPatrick C Fischer, Albert R Meyer, and Arnold L\nRosenberg. 1968. Counter machines and counter\nlanguages. Mathematical systems theory, 2(3):265–\n283.\nFelix A Gers and E Schmidhuber. 2001. Lstm recur-\nrent networks learn simple context-free and context-\nsensitive languages. IEEE Transactions on Neural\nNetworks, 12(6):1333–1340.\nMichael Hahn. 2020. Theoretical limitations of self-\nattention in neural sequence models. Transactions\nof the Association for Computational Linguistics ,\n8:156–171.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nGerhard J ¨ager and James Rogers. 2012. Formal\nlanguage theory: reﬁning the chomsky hierarchy.\nPhilosophical Transactions of the Royal Society B:\nBiological Sciences, 367(1598):1956–1970.\nJohn F Kolen and Stefan C Kremer. 2001. A ﬁeld\nguide to dynamical recurrent networks. John Wiley\n& Sons.\nSamuel A Korsky and Robert C Berwick. 2019. On\nthe computational power of rnns. arXiv preprint\narXiv:1906.06349.\nXuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and\nCho-Jui Hsieh. 2020. Learning to encode position\nfor transformer with continuous dynamical model.\narXiv preprint arXiv:2003.09229.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nRobert McNaughton and Seymour A. Papert. 1971.\nCounter-Free Automata (M.I.T. Research Mono-\ngraph No. 65). The MIT Press.\nWilliam Merrill. 2019. Sequential neural networks\nas automata. In Proceedings of the Workshop on\nDeep Learning and Formal Languages: Building\nBridges, pages 1–13, Florence. Association for Com-\nputational Linguistics.\nWilliam Merrill. 2020. On the linguistic capacity\nof real-time counter automata. arXiv preprint\narXiv:2004.06866.\nWilliam Merrill, Gail Weiss, Yoav Goldberg, Roy\nSchwartz, Noah A Smith, and Eran Yahav. 2020. A\nformal hierarchy of rnn architectures. arXiv preprint\narXiv:2004.08500.\nJoshua J. Michalenko, Ameesh Shah, Abhinav Verma,\nSwarat Chaudhuri, and Ankit B. Patel. 2019. Finite\nautomata can be linearly decoded from language-\nrecognizing RNNs. In International Conference on\nLearning Representations.\nIsabel Papadimitriou and Dan Jurafsky. 2020. Pretrain-\ning on non-linguistic structure as a tool for analyz-\ning learning bias in language models. arXiv preprint\narXiv:2004.14601.\nJorge P ´erez, Javier Marinkovi ´c, and Pablo Barcel ´o.\n2019. On the turing completeness of modern neural\nnetwork architectures. In International Conference\non Learning Representations.\nJean-ric Pin. 2017. The dot-depth hierarchy, 45 years\nlater. In The Role of Theory in Computer Science ,\npages 177–201.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nbert. In Advances in Neural Information Processing\nSystems, pages 8592–8600.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works. arXiv preprint arXiv:2002.12327.\nLuzi Sennhauser and Robert Berwick. 2018. Evaluat-\ning the ability of LSTMs to learn context-free gram-\nmars. In Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 115–124, Brussels, Bel-\ngium. Association for Computational Linguistics.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,\nShirui Pan, and C. Zhang. 2018. Disan: Directional\nself-attention network for rnn/cnn-free language un-\nderstanding. In AAAI.\nNatalia Skachkova, Thomas Trost, and Dietrich\nKlakow. 2018. Closing brackets with recurrent neu-\nral networks. In Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpret-\ning Neural Networks for NLP, pages 232–239, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nHoward Straubing. 1994. Finite Automata, Formal\nLogic, and Circuit Complexity . Birkhauser Verlag,\nCHE.\nMirac Suzgun, Yonatan Belinkov, Stuart Shieber, and\nSebastian Gehrmann. 2019a. LSTM networks can\nperform dynamic counting. In Proceedings of the\nWorkshop on Deep Learning and Formal Languages:\nBuilding Bridges, pages 44–54, Florence. Associa-\ntion for Computational Linguistics.\nMirac Suzgun, Yonatan Belinkov, and Stuart M.\nShieber. 2019b. On evaluating the generalization of\nLSTM models in formal languages. In Proceedings\nof the Society for Computation in Linguistics (SCiL)\n2019, pages 277–286.\n7106\nYao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada,\nLouis-Philippe Morency, and Ruslan Salakhutdinov.\n2019. Transformer dissection: An uniﬁed under-\nstanding for transformer’s attention via the lens of\nkernel. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4344–4353, Hong Kong, China. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nQinglong Wang, Kaixuan Zhang, Alexander G. Oror-\nbia II, Xinyu Xing, Xue Liu, and C. Lee Giles.\n2018a. A comparative study of rule extraction for re-\ncurrent neural networks. CoRR, abs/1801.05420v2.\nQinglong Wang, Kaixuan Zhang, II Ororbia, G Alexan-\nder, Xinyu Xing, Xue Liu, and C Lee Giles. 2018b.\nA comparative study of rule extraction for recurrent\nneural networks. arXiv preprint arXiv:1801.05420.\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason\nPhang, Anhad Mohananey, Phu Mon Htut, Paloma\nJeretic, and Samuel R. Bowman. 2019. Investi-\ngating BERT’s knowledge of language: Five anal-\nysis methods with NPIs. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2877–2887, Hong Kong,\nChina. Association for Computational Linguistics.\nGail Weiss, Yoav Goldberg, and Eran Yahav. 2018.\nOn the practical computational power of ﬁnite pre-\ncision RNNs for language recognition. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 740–745, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nGail Weiss, Yoav Goldberg, and Eran Yahav. 2019.\nLearning deterministic weighted automata with\nqueries and counterexamples. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. AlcheBuc, E. Fox,\nand R. Garnett, editors,Advances in Neural Informa-\ntion Processing Systems 32, pages 8560–8571. Cur-\nran Associates, Inc.\nBaosong Yang, Longyue Wang, Derek F. Wong,\nLidia S. Chao, and Zhaopeng Tu. 2019. Assessing\nthe ability of self-attention networks to learn word\norder. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 3635–3644, Florence, Italy. Association\nfor Computational Linguistics.\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh\nRawat, Sashank Reddi, and Sanjiv Kumar. 2020.\nAre transformers universal approximators of\nsequence-to-sequence functions? In International\nConference on Learning Representations.\n7107\nA Roadmap\nThe appendix is organized as follows. In section\nB we ﬁrst provide formal deﬁnitions of the key\nlanguages used in our investigation in the main pa-\nper. In sections B.1 and B.2, we also provide the\nformal deﬁnitions of automata, star-free languages\nand the dot-depth hierarchy. In section C, we pro-\nvide the details of all our expressiveness results.\nSection D contains the details of our experimental\nsetup which could be relevant for reproducibility\nof the results and includes a thorough discussion of\nthe choice of character prediction task. The list of\nall the formal languages we have considered, their\ndataset statistics as well as the results are provided\nin section D.\nB Deﬁnitions\nIn this section, we provide formal deﬁnitions of\nsome of the languages used in our analysis. In\ncounter languages, we ﬁrst deﬁne the family of\nshufﬂed Dyck-1 languages. The language Dyck-1\nis a simple context-free language that can also be\nrecognized by a counter automaton with a single\ncounter. We generate the data for Dyck-1 based on\nthe following PCFG,\nS →\n\n\n\n(S) with probability p\nSS with probability q\nε with probability 1 −(p+ q)\nwhere 0 < p,q <1 and (p+ q) <1. We use 0.5\nas the value of pand 0.25 as the value for q.\nShufﬂe-Dyck. We now deﬁne the Shufﬂe-Dyck\nlanguage introduced and described in (Suzgun\net al., 2019a). We ﬁrst deﬁne the shufﬂing op-\neration formally. The shufﬂing operation || :\nΣ∗×Σ∗ → P(Σ∗) can be inductively deﬁned\nas follows:9\n•u||ε= ε||u= {u}\n•αu||βv = α(u||βv) ∪β(αu||v)\nfor any α,β ∈Σ and u,v ∈Σ∗. For instance, the\nshufﬂe of aband cdis\nab||cd= {abcd,acbd,acdb,cabd,cadb,cdab }.\nThere is a natural extension of the shufﬂing opera-\ntion ||to languages. The shufﬂe of two languages\n9We abuse notation by allowing a string to stand for the\nsingleton containing that string. ϵis the empty string.\nL1 and L2, denoted L1||L2, is the set of all pos-\nsible interleavings of the elements of L1 and L2,\nrespectively, that is:\nL1||L2 =\n⋃\nu∈L1, v∈L2\nu||v\nGiven a language L, we deﬁne its self-shufﬂing\nL||2 to be L||σ(L), where σis an isomorphism on\nthe vocabulary of Lto a disjoint vocabulary. More\ngenerally, we deﬁne the k-self-shufﬂe\nL||k =\n{\n{ε} if k= 0\nL||σ(L||k−1) otherwise .\nWe use Shufﬂe-kto denote the shufﬂe ofkDyck-\n1 languages (Dyck-1||k) each with its own brackets.\nShufﬂe-1 is the same as Dyck-1. For instance the\nlanguage Shufﬂe-2 is the shufﬂe of Dyck- 1 over\nalphabet Σ = {(,)}and another Dyck-1 over the\nalphabet Σ = {[,]}. Hence the resulting Shufﬂe-2\nlanguage is deﬁned over alphabet Σ = {[,],(,)}\nand contains words such as ([)] and [((])) but not\n])[(. This is different from the context-free lan-\nguage Dyck-2 in which ([]) belongs to the lan-\nguage but ([)] does not. Similar to (Suzgun et al.,\n2019a) we generate the training data by generating\nsequence for Dyck-nbut by providing the correct\ntarget values for the character prediction task.\nn-ary Boolean Expressions. We now deﬁne the\nfamily of languages n-ary Boolean Expressions\nparameterized by the number and arities of its op-\nerators. An instance of the language contains oper-\nators of different arities and as shown in (Fischer\net al., 1968), these languages can be recognized by\ncounter-machines with a single counter. However\nas opposed to Dyck-1 the values with which the\ncounters will be incremented or decremented will\ndepend on the arity of its operator. A language\nwith noperators can be deﬁned by the following\nderivation rules\n<exp> -> <VALUE>\n<exp> -> <UNARY> <exp>\n<exp> -> <BINARY> <exp> <exp>\n..\n<exp> -> <n-ARY> <exp> .. <exp>\nTomita Grammars Tomita Grammars are 7 regu-\nlar langauges deﬁned on the alphabet Σ = {0,1}.\nTomita-1 has the regular expression 1∗ i.e. the\nstrings containing only 1’s and no 0s are allowed.\nTomita-2 is deﬁned by the regular expression(10)∗.\nTomita-3 accepts the strings where odd number\n7108\nof consecutive 1s are always followed by an even\nnumber of 0s. Tomita-4 accepts the strings that\ndo not contain 3 consecutive 0s. In Tomita-5 only\nthe strings containing an even number of 0s and\neven number of 1s are allowed. In Tomita-6 the\ndifference in the number of 1s and 0s should be\ndivisible by 3 and ﬁnally, Tomita-7 has the regular\nexpression 0∗1∗0∗1∗.\nWe note that Tomita 2 = D1 = (01)∗and that\nthe very simple language {0,1,2}∗02∗ has dot-\ndepth 2 (Cohen and Brzozowski, 1971).\nB.1 Counter Automata\nWe deﬁne the general counter machine following\n(Fischer et al., 1968). We are concerned with real-\ntime counter machines here in which the number\nof computation steps is bounded by the number\nof inputs similar to how we use sequence models\nin practice. The machine has a ﬁnite number of\nunbounded counters and it modiﬁes it by adding\nor subtracting values or resetting the counter value\nto 0. For m ∈ Z, let +m denote the function\nx ↦→ x+ m. Let ×0 denote the constant zero\nfunction x↦→0.\nDeﬁnition B.1 (General counter machine (Fischer\net al., 1968)) . A k-counter machine is a tuple\n⟨Σ,Q,q 0,u,δ,F ⟩with\n1. A ﬁnite alphabet Σ\n2. A ﬁnite set of states Q\n3. An initial state q0\n4. A counter update function\nu: Σ ×Q×{0,1}k →\n(\n{+m: m∈Z}∪{×0}\n)k\n5. A state transition function\nδ: Σ ×Q×{0,1}k →Q\n6. An acceptance mask\nF ⊆Q×{0,1}k\nA machine processes an input string xone token\nat a time. For each token, we use uto update the\ncounters and δto update the state according to the\ncurrent input token, the current state, and a ﬁnite\nmask of the current counter values.\nFor a vector v, let z(v) denote the broadcasted\n“zero-check” function, i.e. z(v)i is 0 if vi = 0 or 1\notherwise. Let ⟨q,c⟩∈ Q×Zk be a conﬁguration\nof machine M. Upon reading input xt ∈Σ, we\ndeﬁne the transition\n⟨q,c⟩→xt ⟨δ(xt,q,z (c)),u(xt,q,z (c))(c)⟩.\nFor any string x∈Σ∗with length n, a counter\nmachine accepts xif there exist states q1,..,q n and\ncounter conﬁgurations c1,.., cn such that\n⟨q0,0⟩→x1 ⟨q1,c1⟩→x2 ..→xn ⟨qn,cn⟩∈ F.\nA counter machines accepts a language Lif, for\neach x∈Σ∗, it accepts xiff x∈L. Refer to (Mer-\nrill, 2020) for more details on counter machines,\nvariants and their properties.\nB.2 Star-free regular languages and the\ndot-depth hierarchy\nStar-free regular languages (deﬁned in the main\npaper) are a simpler subclass of regular languages;\nthey have regular expressions without Kleene star\n(but use set complementation). The set of star-free\nlanguages is further stratiﬁed by the dot-depth hier-\narchy, which is a hierarchy of families of languages\nwhose union is the family of star-free languages.\nInformally, the position of a language in this hier-\narchy is a measure of the number of nested con-\ncatenations or sequentiality required to express the\nlanguage in a star-free regular expression. Both the\nstar-free regular languages as well as the dot-depth\nhierarchy are well-studied with rich connections\nand multiple (equivalent) deﬁnitions. For more in-\nformation, see e.g. (McNaughton and Papert, 1971;\nCohen and Brzozowski, 1971; Straubing, 1994;\nDiekert and Gastin, 2008; J¨ager and Rogers, 2012;\nPin, 2017).\nTo deﬁne the dot-depth hierarchy, we ﬁrst de-\nﬁne Boolean and concatenation closures of lan-\nguage families. For a language family Lover a\nﬁnite alphabet Σ = {a1,...,a k}, its Boolean clo-\nsure BLis the set of languages obtained by ap-\nplying Boolean operators (union, intersection and\nset complementation w.r.t. Σ∗) to the languages\nin L. In other words, BLis the smallest family of\nlanguages containing Land closed under Boolean\noperations: if L1,L2 ∈L then L1 ∩L2 ∈BL and\nL1 ∪L2 ∈BL and Lc\n1,Lc\n2 ∈BL. Similarly, de-\nﬁne the concatenation closure of Las the smallest\nfamily of languages containing Land closed under\nconcatenation: if L1,L2 ∈L then L1L2 ∈ML.\n7109\nWe begin with the class Eof basic languages\nconsisting of {a1},... {ak},{ϵ},∅. By alternately\napplying the operators Band Mto Ewe can deﬁne\nthe hierarchy\nE⊆ME⊆BME⊆MBME⊆ ....\nLet B0 = BME. The dot-depth hierarchy is the\nsequence of families of languages B0 ⊆B1 ⊆...\ndeﬁned inductively by Bn+1 = BMBn for n≥0.\nIt is known that all the inclusions inB0 ⊆B1 ⊆...\nare strict and is exempliﬁed by the languages Dn\n(see Pin (2017)). Minor variations in the deﬁnition\nexist in the literature; in particular, we could have\napplied the operator Bﬁrst, but these have only\nminor effects on the overall concept and results.\nC Expressiveness Results\nWe deﬁne a weaker version of counter automata\nwhich are restricted in a certain sense. Then, we\nshow that Transformers are at least as powerful as\nsuch automata.\nDeﬁnition C.1 (Simpliﬁed and Stateless counter\nmachine). We deﬁne a counter machine to be sim-\npliﬁed and stateless if uand δhave the following\nform,\nu: Σ →{+m: m∈Z}k,\nδ: Σ →Q\nThis implies that the machine can have kcoun-\nters. The counters can be incremented or decre-\nmented by any values but it will only depend on the\ninput symbol. Similarly, the state transition will\nalso depend on the current input. A string x∈Σ∗\nwill be accepted if ⟨qn,z(cn)⟩∈ F. We use LRCL\nto denote the class of languages recognized by such\na counter machine. The above language is similar\nto Σ-restricted counter machine deﬁned in (Merrill\net al., 2020).\nLemma C.1. Transformers can recognizeLRCL.\nProof. Let s1,s2,...,s n denote a sequence w ∈\nΣ∗. If the counter machine has k counters, then\nlet the dimension of intermediate vectors dmodel =\n2k+ |Σ|. The ﬁrst 2kdimensions will be reserved\nfor counter related operations and then |Q|dimen-\nsions will be reserved to obtain the state vector.\nThe embedding vector xi of each symbol will have\n0s in the ﬁrst 2kdimensions and the last |Σ|dimen-\nsions will have the one-hot encoding representation\nof the symbol. For a kcounter machine the value\nvectors would have a subvector of dimension 2 re-\nserved for computations pertaining to each of the\ncounter. That is, x2j:2j+1 will be reserved for the\njth counter where 0 ≤j <k. For any given input\nsymbol s, if u(s) has counter operation of +mat\nthe jth counter, then the value will be such that\nv will contain +mat index 2j and −mat index\n2j + 1 upto index 2k. The last |Σ|dimensions\nwill have the value 0 in the value vectors. This\ncan be easily obtained by a linear transformation\nV(.) over one-hot encodings. The linear transfor-\nmation K(.) to obtain the key vectors will lead to\nzero vectors and hence all inputs will have equal\nattention weights. The linear transformation V(.)\nto obtain the value vectors vi will be identity func-\ntion. Hence the output of the self-attention block\nalong with residual connection will be of the form\nai = 1\ni\n∑i\nt=1 vt + xi.\nThe last |Σ|dimensions of the vector ai will\nhave one-hot encoding of the input vector at i-th\nstep. The one-hot encoding of the input can be\neasily mapped to the one-hot encoding for the cor-\nresponding state using a simple FFN. Additionally,\nthis will ensure that, at the i-th step, the output of\nthe self-attention block ai will have the value cj\ni\nat indices 2j, where cj denotes the counter value\nof the counter automata representing the language.\nSimilarly, the odd indices 2j + 1 will have the\nvalue −cj\ni . After applying a simple feed-forward\nnetwork with ReLU activation, we obtain the out-\nput vector zi. It is easy to implement the zero check\nfunction with a simple linear layer over the output\nvector. The network accepts an input sequence w\nwhen the values in the output vector corresponding\nto each counter and state at the n-th correspond to\nthat required for the ﬁnal state.\nWe next show that n-ary Boolean Expressions\ncan be recognized by Transformers with a similar\nconstruction.\nLemma C.2. Transformers can recognize n-ary\nBoolean Expressions.\nProof. Let Lm denote a language of type n-ary\nBoolean Expressions with m operators deﬁned\nover the alphabet Σ. Consider a single layer\nTransformer network with dmodel = 2 . Let\ns0,s1,...,s n be sequence w where w ∈ Σ∗.\nLet s0 be a special start symbol with embedding\nfe = [+1 ,−1]. The embeddings of each input\nsymbol s ∈ Σ are deﬁned as follows, fe(s) =\n7110\n[+(r−1),−(r−1)] where rdenotes the arity of\nthe symbol. The arity of values such as 0 and 1\nis taken as 0. Similar to the previous construc-\ntion, the key values are null and hence attention\nweights are uniform leading to ai = 1\ni\n∑i\nt=1 vt.\nHence the output of the self-attention block will\nbe ai = [ cj\ni ,−cj\ni ], where cj denotes the counter\nvalue of the automata representing the language.\nEssentially, for each operator, the value added to\nthe counter is equal to its arity subtracted by 1. For\neach value such as 0 and 1, the counter value is\ndecremented by 1. We then apply a simple FFN\nwith ReLU activation to obtain the output vector\nzi = ReLU(Iai).\nAn input sequence w belongs to the language\nLm if the second coordinate of the output is zero\nat every step, that is, zi,2 = 0 for 0 ≤i ≤nand\nzn = 0.\nLet Reset-Dyck-1 be a language deﬁned over al-\nphabet Σ = {[,],1}, where 1 denotes a symbol that\nrequires a reset operation. Words in Reset-Dyck-1\nhave the form Σ∗1v, where the string v belongs\nto Dyck-1. So essentially, when the machine en-\ncounters the reset symbol 1, it has to ignore all the\nprevious inputs, reset the counter to 0 and go to\nstart state.\nLemma C.3. A single-layer Transformer with only\npositional masking cannot recognize the language\nReset-Dyck-1.\nProof. The proof is straightforward. Let\ns1,s2,...,s n be an input sequence w. Let sr de-\nnote the r-th symbol where the reset symbol oc-\ncurs. It is easy to see that the scoring function\n⟨qn,K(vr)⟩is independent of the position as well\nas the inputs before the reset symbol which are\nrelevant for the reset operation. Consider the case\nwhere the ﬁrst half of the input contains a sequence\nof open and closing brackets such that it does not\nbelong to Dyck-1 and the second half contains a\nsequence that belongs to Dyck-1. If the reset sym-\nbol occurs after the ﬁrst half of the sequence, then\nthe word belongs to Dyck-1 and if it occurs in the\nbeginning then it does not belong to the language\nDyck-1. However, by construction, the output of\nthe model zn will remain the same regardless of\nthe position of the reset symbol and hence by con-\ntradiction, it cannot recognize such a language.\nThe above limitation does not exist if there is a\ntwo layer network. The scoring function as well as\nvalue vector of the reset symbol will be dependent\nof the inputs that precede it. Hence it is not nec-\nessary that a two layer network will not be able to\nrecognize such a language. Indeed, as shown in the\nmain paper, the 2-layer Transformer performs well\non Reset-Dyck-1.\nLemma C.4. Transformers with only positional\nmasking cannot recognize the language (aa)∗.\nProof. Let s1,s2,...,s n be an input sequence w\nwhere w∈a∗. Since it is a unary language, the in-\nput at each step will be the same symbol and hence\nthe embedding as well as query, key and value vec-\ntors will be the same. Since all the value vectors\nare the same, regardless of the attention weights,\nthe output of the self-attention vector ai will be a\nconstant vector at each timestep. This implies that\nthe output vectors z1 = z2 = ... = zn. Induc-\ntively, it is easy to see that regardless of the number\nof layers this phenomenon will carry forward and\nhence the output vector at each timestep will be the\nsame. Thus, the network cannot distinguish output\nat even steps and odd steps which is necessary to\nrecognize the language (aa)∗.\nFor parity, in the case where the input consists of\nonly 1s, the problem reduces to recognizing (11)∗.\nHence it follows from the above result that a net-\nwork without positional encoding cannot recognize\nparity even for minimal lengths.\nD Experiments\nD.1 Discussion on Character Prediction Task\nAs described in section 5.1, we use character predic-\ntion task in our experiments to evaluate the model’s\nability to recognize a language. In character predic-\ntion task the model is only presented with positive\nsamples from a given language and its goal is to\npredict the next set of valid characters. During in-\nference, the model predicts the next set of legal\ncharacters at each step and a prediction is consid-\nered to be correct if and only if the model’s output\nat every step is correct. The character prediction\ntask is similar to predicting which of the input char-\nacters are allowed to make a transition in a given\nautomaton such that it leads to a non-dead state. If\nan input character is not among the legal characters,\nthat implies the underlying automaton will transi-\ntion to a dead state and regardless of the following\ncharacters, the input word will never be accepted.\nWhen the end-of-sequence symbol is allowed as\n7111\none of the next set of legal characters, it implies\nthat the underlying automaton is in the ﬁnal state\nand the input can be accepted.\nCharacter prediction and classiﬁcation. If a\nmodel can perform character prediction task per-\nfectly, then it can also perform classiﬁcation\nin the following way. For an input sequence\ns1,s2,...,s n, the model receives the sequence\ns1,...,s i for 1 ≤i≤nat each step iand model\npredicts the set of valid characters in the (i+ 1)th\nposition. If the next character is among the model’s\npredicted set of valid characters at each step iand\nthe end of symbol character is allowed at the n-th\nstep, then the word is accepted and if any char-\nacter is not within the model’s predicted set of\nvalid characters, then the word is rejected. One\nof the primary reason for the choice of character\nprediction task is that it is arguably more robust\nthan the standard classiﬁcation task. The metric\nfor character prediction task is relatively stringent\nand the model is required to model the underlying\nmechanism as opposed to just one label in standard\nclassiﬁcation. Note that the null accuracy (accuracy\nwhen all the predictions are replaced by a single la-\nbel) is 50% if the distribution of labels is balanced\n(higher otherwise), on the other hand the null ac-\ncuracy of character prediction task is close to 0.\nAdditionally, in case of classiﬁcation, depending\non how the positive or negative data are generated,\nthe model may also be biased to predict based on\nsome statistical regularites instead of modeling the\nactual mechanism. In (Weiss et al., 2019), they ﬁnd\nthat LSTMs trained to recognize Dyck-1 via clas-\nsiﬁcation on randomly sampled data do not learn\nthe correct mechanism and fail on adversarially\ngenerated samples. On the other hand, Suzgun\net al. (2019a) show that LSTMs trained to recog-\nnize Dyck-1 via character prediction task learn to\nperform the correct mechanism required to do the\ntask.\nCharacter prediction and language modelling.\nThe character prediction task has clear connections\nwith Language modelling. If a model can per-\nform language modelling perfectly, then it can per-\nform character prediction task in the following way.\nFor an input sequence s1,s2,...,s n, the model\nreceives the sequence s1,...,s i, for 1 ≤i ≤n\nat each step iand predicts a distribution over the\nvocabulary. Mapping all the characters for which\nthe model assigns a nonzero probability to 1 and\nmapping to 0 for all characters that are assigned\nzero probability will reduce it to character predic-\ntion task. However, there are a few issues with\nusing language modelling in our formal language\nsetting. Firstly, as mentioned in (Suzgun et al.,\n2019a), the task of recognizing a language is not\ninherently probabilistic. Our goal here is to un-\nderstand whether a network can or cannot model a\nparticular language. Using language modelling will\nrequire us to impose a distribution arbitrarily for the\ngiven setting. More importantly, in character pre-\ndiction task, some signals are explicitly provided.\nIn the case of language modelling, we may just\nhave to rely on the model to pick up those nuanced\nsignals. For instance, in the language Dn, when\nthe input reaches the maximum depth n, in char-\nacter prediction task it is explicitly provided the\ntarget value that ais not allowed anymore whereas\nin language modelling the model is expected to\nassign zero probability to aat the maximum depth\nbased on the fact that it will never see a word depth\nmore than nin the training data. This phenomenon\nhas major issues. For instance, when we consider\nDyck-1 in practical setting, we can only provide\nit with limited data which implies there will be\na sequence with a maximum ﬁnite depth. In this\nscenario, a language model trained on such data\nmay learn the Dyck-1 language or the language\nDn with that particular maximum depth. This lim-\nitation does not exist in the character prediction\ntask where the signal is explicitly provided during\ntraining.\nD.2 Experimental Details\nWe use 4 NVIDIA Tesla P100 GPUs each with 16\nGB memory to run our experiments, and train and\nevaluate our models on about 9 counter languages\nand 18 regular languages. The important details of\nall of these languages like the training and test sizes\nand the lengths of the strings considered, have been\nsummarized in Table 6. In all of our experiments,\nthe ﬁrst bin always has the same length range as the\ntraining set, i.e. if the training set contains strings\nwith lengths in range [2,50], then the strings in\nthe ﬁrst test bin will also lie in the same range.\nWidth of bin is the difference between upper and\nlower limits of the string lengths that lie in that bin.\nAll the test bins are taken to be disjoint from each\nother. Hence, if we have 3 bins with a width of\n50 and the training range is [2,50], then the length\nranges for the test bins will be[2,50], [52,100] and\n[102,150].\n7112\nTraining Data Test Data\nLanguage Size Length\nRange\nSize per\nBin\nLength\nRange\nNumber\nof Bins\nBin\nWidth\nCounter Languages\nShufﬂe-2 10000 [2, 50] 2000 [2, 150] 3 50\nShufﬂe-4 10000 [2, 100] 2000 [2, 200] 3 50\nShufﬂe-6 10000 [2, 1000] 2000 [2, 200] 3 50\nBoolean-3 10000 [2, 50] 2000 [2, 150] 3 50\nBoolean-5 10000 [2, 50] 2000 [2, 150] 3 50\nanbn 50 [2, 100] 50 [2, 300] 3 100\nanbncn 50 [3, 150] 50 [3, 450] 3 150\nanbncndn 50 [4, 200] 50 [4, 600] 3 200\nDyck-1 10000 [2, 50] 2000 [2, 150] 3 50\nRegular Languages\nTomita 1 50 [2, 50] 100 [2, 100] 2 50\nTomita 4 10000 [2, 50] 2000 [2, 100] 2 50\nTomita 7 10000 [2, 50] 2000 [2, 100] 2 50\nTomita 2 25 [2, 50] 50 [2, 100] 2 50\naa∗bb∗cc∗dd∗ee∗ 10000 [5, 200] 1000 [5, 300] 2 100\n{a,b}∗d{b,c}∗ 10000 [1, 50] 2000 [1, 100] 2 50\n{0,1,2}∗02∗ 10000 [2, 50] 2000 [2, 100] 2 50\nD2 10000 [2, 100] 2000 [2, 200] 2 100\nD3 10000 [2, 100] 2000 [2, 200] 2 100\nD4 10000 [2, 100] 2000 [2, 200] 2 100\nD12 10000 [2, 100] 2000 [2, 200] 2 100\nParity 10000 [2,50] 2000 [2, 100] 2 50\n(aa)∗ 250 [2, 500] 50 [2, 600] 2 100\n(aaaa)∗ 125 [4, 500] 25 [4, 600] 2 100\n(abab)∗ 125 [4, 500] 25 [4, 600] 2 100\nTomita 3 10000 [2, 50] 2000 [2, 100] 2 50\nTomita 5 10000 [2, 50] 2000 [2, 100] 2 50\nTomita 6 10000 [2, 50] 2000 [2, 100] 2 50\nTable 6: Statistics of different datasets used in the experiments. Note that the width of the ﬁrst bin is always deﬁned\nby the training set (see D), and hence can be different from the widths of other bins reported in Bin Width column.\nAs an example, for (aa)∗, the ﬁrst bin will have a length range of [2, 500] and [502, 600] for the second bin.\nFor each of these languages, we extensively tune\non a bunch of different architectural and optimiza-\ntion related hyperparameters. Table 7 lists the hy-\nperparameters considered in our experiments and\nthe bounds for each of them. This corresponds\nto about 162 different conﬁgurations for tuning\ntransformers (for a hidden size of 3, 4 heads are\nnot allowed) and 40 conﬁgurations for LSTMs .\nOver all the languages and hyperparameters there\nwere a minimum of 117 parameters and a max-\nimum of 17,888 parameters for the models that\nwe considered. We use a grid search procedure\nto tune the hyperparameters. While reporting the\naccuracy scores for a given language, we compute\nthe mean of the top 5 accuracies, corresponding to\nall hyperparameter conﬁgurations. For some ex-\nperiments we had to consider the hyperparameters\nlying outside of the values speciﬁed in Table 7. As\nan instance, we considered 4 layer transformers in\nthe cases where the training accuracies obtained\nwere low for single and two layered networks and\nreported the results accordingly.\nFor training our models we used RMSProp op-\ntimizer with the smoothing constant α= 0.99. In\nour initial few experiments we also tried Stochas-\ntic Gradient Descent with learning rate decay and\nAdam Optimizer, but decided to go ahead with RM-\nSProp as it outperformed SGD in majority of exper-\niments and gave similar performance as Adam but\nneeded fewer hyperparameters. For each language\nwe train models corresponding to each language\nfor 100 epochs and a batch size of 32. In case of\nconvergence, i.e. perfect accuracies for all the bins,\nbefore completion of all epochs, we stop the train-\ning process early. The results of our experiments\non counter and regular languages are provided in\nTables 8 and 9 respectively.\n7113\nHyperparameter Bounds\nHidden Size [3, 32]\nHeads [1, 4]\nNumber of Layers [1, 2] — [1, 4]\nLearning Rate [1e-2, 1e-3]\nPosition Encoding\nScheme\n[Absolute, Relative, Positional\nMasking]\nTable 7: Different hyperparameters and the values considered for each of them. Note that certain parameters like\nHeads and Position Encoding Scheme are only relevant for Transformer based models and not for LSTMs. We\nconsidered upto 4 layers transformers in the cases where the training accuracies obtained were low for single and\ntwo layered networks and reported the results accordingly.\nE Plots\nWe visualize different aspects of the trained mod-\nels to understand how they achieve a particular\ntask and if the learned behaviour resembles our\nconstructions. Figure 3 shows the value vectors\ncorresponding to the trained models on Shufﬂe-2\nand Boolean-3 Language. We also visualize the\nattention weights corresponding to these two mod-\nels in Figure 4. Similar to the self-attention output\nvisualizations for Shufﬂe-2 and Boolean-3 in the\nmain paper, we visualize these values for a model\ntrained on Shufﬂe-4 in Figure 5 and again, ﬁnd\nclose correlations with the depth to length ratios\nof different types of brackets in the language. Fi-\nnally, in Figure 6, we visualize a component of\nthe learned position embeddings vectors and found\na similar behaviour to cos(nπ) agreeing with our\nhypothesis.\n0 1 2 3 4 5 6 7 8\n()[]\n0.74 -2.5 -24 1.8 9 -2.8 -0.95 10 -28\n-0.78 2.7 24 -2.1 -9.4 2 0.9 -10 27\n-21 -17 1.4 25 -3.3 3 -19 4.6 1.4\n22 18 -1.2 -26 2.7 -4.1 20 -4.2 -1.3\nValue Vectors\n20\n0\n20\n(a)\n0 1 2\n~+>\n-0.1 -0.052 -0.14\n-0.31 -0.1 -0.31\n-0.51 -0.15 -0.5\nValue Vectors\n0.4\n0.2\n(b)\nFigure 3: Plot of value vectors of transformer based\nmodels trained on Shufﬂe-2 3a and Boolean-3 language\n3b. The Shufﬂe-2 model had a hidden size of 8 and\nboolean-3 model had a hidden size of 3. The x-axis\ncorresponds to different components of the value vec-\ntors for both models. Shufﬂe-2 language consisted of\nsquare and round brackets, while for Boolean-3 we con-\nsidered 3 operators namely: ∼a unary operator, + a\nbinary operator and ﬁnally, >which is a ternary opera-\ntor..\n7114\nLanguage Model Bin-1 Accuracy\n[1, 50]↑\nBin-2 Accuracy\n[51, 100]↑\nBin-3 Accuracy\n[101, 150]↑\nDyck-1\nLSTM (Baseline) 100.0 100.0 100.0\nTransformer (Absolute Positional Encodings) 100.0 100.0 100.0\nTransformer (Relative Positional Encodings) 100.0 91.0 60.7\nTransformer (Only Positional Masking) 100.0 100.0 100.0\nShufﬂe-2\nLSTM (Baseline) 100.0 100.0 100.0\nTransformer (Absolute Positional Encodings) 100.0 85.2 63.3\nTransformer (Relative Positional Encodings) 100.0 51.6 3.8\nTransformer (Only Positional Masking) 100.0 100.0 93.0\nShufﬂe-4\nLSTM (Baseline) 100.0 100.0 99.6\nTransformer (Absolute Positional Encodings) 100.0 46.6 20.8\nTransformer (Relative Positional Encodings) 100.0 57.2 5.5\nTransformer (Only Positional Masking) 100.0 100.0 98.8\nShufﬂe-6\nLSTM (Baseline) 100.0 99.9 99.5\nTransformer (Absolute Positional Encodings) 100.0 50.4 16.6\nTransformer (Relative Positional Encodings) 100.0 59.1 5.7\nTransformer (Only Positional Masking) 100.0 99.9 94.0\nBoolean Expressions (3)\nLSTM (Baseline) 100.0 100.0 99.7\nTransformer (Absolute Positional Encodings) 100.0 90.6 51.3\nTransformer (Relative Positional Encodings) 100.0 96.0 68.4\nTransformer (Only Positional Masking) 100.0 100.0 99.8\nBoolean Expressions (5)\nLSTM (Baseline) 100.0 99.5 96.0\nTransformer (Absolute Positional Encodings) 100.0 84.3 40.8\nTransformer (Relative Positional Encodings) 100.0 72.3 32.3\nTransformer (Only Positional Masking) 100.0 99.8 99.0\nanbn\nLSTM (Baseline) 100.0 100.0 99.9\nTransformer (Absolute Positional Encodings) 100.0 100.0 100.0\nTransformer (Relative Positional Encodings) 100.0 100.0 100.0\nTransformer (Only Positional Masking) 100.0 100.0 100.0\nanbncn\nLSTM (Baseline) 100.0 100.0 97.8\nTransformer (Absolute Positional Encodings) 100.0 62.1 5.3\nTransformer (Relative Positional Encodings) 100.0 31.3 22.0\nTransformer (Only Positional Masking) 100.0 100.0 100.0\nanbncndn\nLSTM (Baseline) 100.0 100.0 99.9\nTransformer (Absolute Positional Encodings) 88.45 0.0 0.0\nTransformer (Relative Positional Encodings) 41.1 0.0 0.0\nTransformer (Only Positional Masking) 100.0 100.0 99.4\nTable 8: The performance of Transformers and LSTMs on the respective counter languages. Refer to section 6 in\nthe main paper for details.\n( [ ( ) ] ) ( ) [ [ [ [ ( ) ] ] ] ]\n(\n[\n(\n)\n]\n)\n(\n)\n[\n[\n[\n[\n(\n)\n]\n]\n]\n]\n1\n0.5 0.5\n0.33 0.34 0.33\n0.25 0.25 0.25 0.25\n0.2 0.2 0.2 0.2 0.2\n0.17 0.17 0.17 0.17 0.16 0.17\n0.14 0.14 0.14 0.14 0.14 0.14 0.14\n0.12 0.13 0.12 0.13 0.12 0.13 0.12 0.13\n0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11\n0.1 0.1 0.1 0.1 0.098 0.1 0.1 0.1 0.1 0.1\n0.09 0.092 0.09 0.091 0.089 0.091 0.09 0.091 0.092 0.092 0.092\n0.083 0.084 0.083 0.084 0.081 0.084 0.083 0.084 0.084 0.084 0.084 0.084\n0.077 0.078 0.077 0.077 0.075 0.077 0.077 0.077 0.078 0.078 0.078 0.078 0.077\n0.071 0.072 0.071 0.071 0.07 0.071 0.071 0.071 0.072 0.072 0.072 0.072 0.071 0.071\n0.066 0.068 0.066 0.067 0.065 0.067 0.066 0.067 0.068 0.068 0.068 0.068 0.066 0.067 0.065\n0.062 0.064 0.062 0.062 0.061 0.062 0.062 0.062 0.064 0.064 0.064 0.064 0.062 0.062 0.061 0.061\n0.059 0.06 0.059 0.059 0.058 0.059 0.059 0.059 0.06 0.06 0.06 0.06 0.059 0.059 0.058 0.058 0.058\n0.055 0.057 0.055 0.056 0.054 0.056 0.055 0.056 0.057 0.057 0.057 0.057 0.055 0.056 0.054 0.054 0.054 0.054\nAttention Weights Layer: 1 Head: 1\n(a)\n+ ~ + > 0 0 > ~ 1 ~ ~ 1 1 1 0\n+\n~\n+\n>\n0\n0\n>\n~\n1\n~\n~\n1\n1\n1\n0\n1\n0.5 0.5\n0.33 0.33 0.33\n0.25 0.25 0.25 0.25\n0.2 0.2 0.2 0.2 0.2\n0.17 0.17 0.17 0.17 0.17 0.17\n0.14 0.14 0.14 0.14 0.14 0.14 0.14\n0.13 0.12 0.13 0.13 0.12 0.12 0.13 0.12\n0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11 0.11\n0.1 0.1 0.1 0.1 0.099 0.099 0.1 0.1 0.099 0.1\n0.091 0.091 0.091 0.091 0.09 0.09 0.091 0.091 0.09 0.091 0.091\n0.084 0.083 0.084 0.084 0.083 0.083 0.084 0.083 0.083 0.083 0.083 0.083\n0.077 0.077 0.077 0.078 0.076 0.076 0.078 0.077 0.076 0.077 0.077 0.076 0.076\n0.072 0.071 0.072 0.072 0.071 0.071 0.072 0.071 0.071 0.071 0.071 0.071 0.071 0.071\n0.067 0.067 0.067 0.068 0.066 0.066 0.068 0.067 0.066 0.067 0.067 0.066 0.066 0.066 0.066\nAttention Weights Layer: 1 Head: 1 (b)\nFigure 4: Attention maps for models trained on Shufﬂe-2 and Boolean-3 languages. Similar to our constructions\nfor recognizing these languages, we observe nearly uniform attention weights in both cases\n7115\nTransformer\n(Only Positional Masking)\nTransformer\n(w Position Encodings) LSTM\nLanguage Property Dot-\nDepth\nBin 0 Bin 1 Bin 0 Bin 1 Bin 0 Bin 1\nTomita 1 SF 1 100.0 100.0 100.0 100.0 100.0 100.0\nTomita 4 SF\n(LT-k)\n1 24.1 0.2 100.0 92.4 100.0 100.0\nTomita 7 SF 1 100.0 100.0 99.9 99.8 100.0 100.0\nTomita 2 =\nD1 = (01)∗\nSF 1 100.0 100.0 100.0 100.0 100.0 100.0\naa∗bb∗cc∗dd∗ee∗ SF 1 100.0 100.0 100.0 100.0 100.0 100.0\n{a,b}∗d{b,c}∗ SF 1 100.0 100.0 100.0 100.0 100.0 100.0\n{0,1,2}∗02∗ SF 2 74.2 35.6 100.0 68.7 100.0 100.0\nD2 SF 2 7.8 0.4 74.6 3.1 100.0 100.0\nD3 SF 3 16.2 4.2 80.9 8.5 100.0 100.0\nD4 SF 4 36.9 15.6 90.2 3.3 100.0 100.0\nD12 SF 12 16.5 0.0 95.8 1.5 100.0 100.0\nParity non-SF − 22.0 0.0 68.7 0.0 100.0 100.0\n(aa)∗ non-SF − 0.0 0.0 100.0 0.0 100.0 100.0\n(aaaa)∗ non-SF − 0.0 0.0 100.0 0.0 100.0 100.0\n(abab)∗ non-SF − 0.0 0.0 100.0 2.5 100.0 100.0\nTomita 3 non-SF − 9.8 9.8 75.4 10.8 100.0 100.0\nTomita 5 non-SF − 4.9 0.0 29.3 0.0 100.0 100.0\nTomita 6 non-SF − 9.1 0.0 88.8 0.0 100.0 100.0\nTable 9: Summary of results on Regular Languages. The languages are arranged in an increasing order of their\ncomplexities.\n( { [ < ( { [ < ( { [ < ( { [ < ( { [ <> ] } ) > ] } ) > ] } ) > ] } ) > ] } )\n12.5\n10.0\n7.5\n5.0\n2.5\n0.0\nOutput of Self-Attention Block\nCoordinate-0\nc3 * '{' DL Ratio\n(a)\n( { [ < ( { [ < ( { [ < ( { [ < ( { [ <> ] } ) > ] } ) > ] } ) > ] } ) > ] } )\n0\n5\n10\n15\n20\n25\n30\n35\nOutput of Self-Attention Block\nCoordinate-12\nc1 * '(' DL Ratio (b)\n( { [ < ( { [ < ( { [ < ( { [ < ( { [ <> ] } ) > ] } ) > ] } ) > ] } ) > ] } )7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nOutput of Self-Attention Block\nCoordinate-15\nc2 * '[' DL Ratio\n(c)\n( { [ < ( { [ < ( { [ < ( { [ < ( { [ <> ] } ) > ] } ) > ] } ) > ] } ) > ] } )\n2\n0\n2\n4\n6\n8\n10\nOutput of Self-Attention Block\nCoordinate-25\nc4 * '<' DL Ratio (d)\nFigure 5: Values of four different coordinates of the output of self-attention block. The model is trained to\nrecognize Shufﬂe-4. The dotted lines are the scaled depth to length ratio for the four types of bracket provided for\nreference.\n7116\n0 10 20 30 40 50\nTime Steps\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\nCoordinate 3 of Learned Position Encodings for (aa)*\nCoordinate-3\ny = 0\nFigure 6: The values of coordiante 3 of the learned po-\nsition encodings on the language (aa)∗. The variation\nin the encodings resemble a periodic behaviour similar\nto cos(nπ)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7111766934394836
    },
    {
      "name": "Transformer",
      "score": 0.6928126811981201
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5330575704574585
    },
    {
      "name": "Subclass",
      "score": 0.5084577798843384
    },
    {
      "name": "Natural language processing",
      "score": 0.49560558795928955
    },
    {
      "name": "Generalization",
      "score": 0.4405609369277954
    },
    {
      "name": "Formal language",
      "score": 0.42204582691192627
    },
    {
      "name": "Programming language",
      "score": 0.2269197702407837
    },
    {
      "name": "Mathematics",
      "score": 0.20716288685798645
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Antibody",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Immunology",
      "score": 0.0
    }
  ]
}