{
    "title": "HybridPrompt: Bridging Language Models and Human Priors in Prompt Tuning for Visual Question Answering",
    "url": "https://openalex.org/W4382202719",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2108652900",
            "name": "Zhiyuan Ma",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4382228958",
            "name": "Zhihuan Yu",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2102724350",
            "name": "Jianjun Li",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2110234391",
            "name": "Guohui Li",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2108652900",
            "name": "Zhiyuan Ma",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4382228958",
            "name": "Zhihuan Yu",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2102724350",
            "name": "Jianjun Li",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2110234391",
            "name": "Guohui Li",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2745461083",
        "https://openalex.org/W2616125804",
        "https://openalex.org/W2904742344",
        "https://openalex.org/W2916723116",
        "https://openalex.org/W3020257313",
        "https://openalex.org/W3196267904",
        "https://openalex.org/W2412400526",
        "https://openalex.org/W2964067226",
        "https://openalex.org/W3126960149",
        "https://openalex.org/W2560730294",
        "https://openalex.org/W3205632231",
        "https://openalex.org/W3126337491",
        "https://openalex.org/W3034472388",
        "https://openalex.org/W6728881024",
        "https://openalex.org/W6752083267",
        "https://openalex.org/W3126792443",
        "https://openalex.org/W3205502670",
        "https://openalex.org/W3095117529",
        "https://openalex.org/W6793601707",
        "https://openalex.org/W3184735396",
        "https://openalex.org/W2933067512",
        "https://openalex.org/W3118641406",
        "https://openalex.org/W3119438769",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W4285119160",
        "https://openalex.org/W4304098627",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W3096331697",
        "https://openalex.org/W2969862959",
        "https://openalex.org/W3171915994",
        "https://openalex.org/W2171810632",
        "https://openalex.org/W2954861308",
        "https://openalex.org/W2744822616",
        "https://openalex.org/W2747623286",
        "https://openalex.org/W3134873017",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W3173220247",
        "https://openalex.org/W2963954913",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3016211260",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3176824248",
        "https://openalex.org/W3034787499",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W2964072591",
        "https://openalex.org/W2963150162",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3004349648",
        "https://openalex.org/W4306755431",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2963383024",
        "https://openalex.org/W4287547182",
        "https://openalex.org/W4312877428",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W2963717374",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2966683369",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W3166986030",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W2949197413",
        "https://openalex.org/W3207798279",
        "https://openalex.org/W2963521239",
        "https://openalex.org/W3166396011"
    ],
    "abstract": "Visual Question Answering (VQA) aims to answer the natural language question about a given image by understanding multimodal content. However, the answer quality of most existing visual-language pre-training (VLP) methods is still limited, mainly due to: (1) Incompatibility. Upstream pre-training tasks are generally incompatible with downstream question answering tasks, which makes the knowledge from the language model not well transferable to downstream tasks, and greatly limits their performance in few-shot scenarios; (2) Under-fitting. They generally do not integrate human priors to compensate for universal knowledge from language models, so as to fit the challenging VQA problem and generate reliable answers. To address these issues, we propose HybridPrompt, a cloze- and verify-style hybrid prompt framework with bridging language models and human priors in prompt tuning for VQA. Specifically, we first modify the input questions into the cloze-style prompts to narrow the gap between upstream pre-training tasks and downstream VQA task, which ensures that the universal knowledge in the language model can be better transferred to subsequent human prior-guided prompt tuning. Then, we imitate the cognitive process of human brain to introduce topic and sample related priors to construct a dynamic learnable prompt template for human prior-guided prompt learning. Finally, we add fixed-length learnable free-parameters to further enhance the generalizability and scalability of prompt learning in the VQA model. Experimental results verify the effectiveness of HybridPrompt, showing that it achieves competitive performance against previous methods on widely-used VQAv2 dataset and obtains new state-of-the-art results. Our code is released at: https://github.com/zhizhi111/hybrid.",
    "full_text": "HybridPrompt: Bridging Language Models and Human Priors in Prompt Tuning\nfor Visual Question Answering\nZhiyuan Ma, Zhihuan Yu, Jianjun Li*, Guohui Li\nHuazhong University of Science and Technology (HUST), China\n{zhiyuanma,zhihuanyu,jianjunli,guohuili}@hust.edu.cn\nAbstract\nVisual Question Answering (VQA) aims to answer the natu-\nral language question about a given image by understanding\nmultimodal content. However, the answering quality of most\nexisting visual-language pre-training (VLP) methods is still\nlimited, mainly due to: (1) Incompatibility. Upstream pre-\ntraining tasks are generally incompatible with downstream\nquestion answering tasks, which makes the knowledge from\nthe language model not well transferable to downstream tasks,\nand greatly limits their performance in few-shot scenarios;\n(2) Under-ﬁtting. They generally do not integrate human pri-\nors to compensate for universal knowledge from language\nmodels, so as to ﬁt the challenging VQA problem and gen-\nerate reliable answers. To address these issues, we propose\nHybridPrompt, a cloze- and verify-style hybrid prompt frame-\nwork with bridging language models and human priors in\nprompt tuning for VQA. Speciﬁcally, we ﬁrst modify the in-\nput questions into the cloze-style prompts to narrow the gap\nbetween upstream pre-training tasks and downstream VQA\ntask, which ensures that the universal knowledge in the lan-\nguage model can be better transferred to subsequent human\nprior-guided prompt tuning. Then, we imitate the cognitive\nprocess of human brain to introduce topic and sample related\npriors to construct a dynamically learnable prompt template\nfor human prior-guided prompt learning. Finally, we add ﬁxed-\nlength learnable free-parameters to further enhance the gen-\neralizability and scalability of prompt learning in the VQA\nmodel. Experimental results verify the effectiveness of Hy-\nbridPrompt, showing that it achieves competitive performance\nagainst previous methods on widely-used VQAv2 dataset and\nobtains new state-of-the-art results. Our code is released at:\nhttps://github.com/zhizhi111/hybrid.\nIntroduction\nVisual Question Answering (VQA) (Yu et al. 2017, 2018) is\na classic and challenging multimodal comprehension task,\nwhich aims to learn cross-modal semantic content from\nimage-text pair to answer a given natural language ques-\ntion. Inspired by the success of vision-language pre-training\n(VLP), we have recently witnessed a boosting number of\nresearch works on VQA (Chen et al. 2020; Li et al. 2020;\nXu et al. 2021; Li et al. 2021b; Radford et al. 2021; Li et al.\n*Corresponding author.\nCopyright © 2023, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nQ: What is the little girl doing?\nA: Swimming \ngoggles\ngirl\nswimming lap\nVQA Task\nVLP Methods\nOurs\n0 / 1 little\n(a) Pre-training Stage\nPre-training  \nLoss\nITM MLM\n[CLS] What is the [MASK] doing\n[SEP]\ngirl\nSwimming\nVQA\n(b) Fine-tuning Stage\n(c) Topic, Sample and Attribute-aware Prompt-tuning Stage\n[CLS] What is the little doing\n[SEP]\ngirl\nyes / no Swimming\nAFV AMC\nPrompt-tuning  \nLoss\n[HARD PROMPT]\nfood\nsport\ncountry\nbaseball  taekwondo  yoga\nINSERT\nTopic\n[CLS] The girl is [ANS] which\n[SEP]\nlittle is a [TOP] such as [S1] [S2] [S3]\nand we can notice that [SOFT PROMPT]\n[CLS]\n[SEP]\nQuestion Answering \nLoss\nWhat is the [MASK] doinggirl\nFigure 1: Example of VLP methods and our proposed Hy-\nbridPrompt method for VQA task.\n2019b; Lu et al. 2019; Kim, Son, and Kim 2021; Cui et al.\n2021; Ma et al. 2022b; Li et al. 2021a; Jia et al. 2021; Ma\net al. 2022a; Liu et al. 2022).\nThough achieving remarkable progress, existing VLP-\nbased methods still suffer from the following two limitations.\n(1) Incompatibility. Firstly, upstream pre-training tasks are\ngenerally incompatible with downstream question answering\ntask. Taking Figure 1 as an example, the VLP-based methods\nmainly exploit the “Pre-training + Fine-tuning” paradigm\nto train a vision-language model, aiming at obtaining a cor-\nrect answer such as “Swimming” for VQA. However, in the\nupstream pre-training task, i.e., in the pre-training stage in\nFigure 1 (a), previous VLP methods generally only carry\nout simple mask-language-modeling (MLM), image-text-\nmatching (ITM) or some extended tasks to respectively learn\nmasked contextual word representations (e.g., the word“little”\nhas been replaced by a special token [MASK]) and coarse-\ngrained image-text matching (e.g., “What is the little girl\ndoing” and its corresponding description image), which are\nobviously different from the downstream VQA task depicted\nin the next ﬁne-tuning stage in Figure 1 (b). In the VQA\ntask, the pre-trained model needs to deeply understand the\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n13371\nnature of the question, so as to obtain a reliable answer such\nas “Swimming” for accurate visual question answering. Es-\npecially in the new few-shot domains, accurate and reliable\nanswers are more difﬁcult to obtain due to the lack of a large\namount of annotated VQA data to support model ﬁne-tuning.\n(2) Under-ﬁtting. Secondly, prior models generally do not in-\ntegrate human priors (e.g., topic, sample and attribute-related\npriors) to compensate for universal knowledge from language\nmodels to ﬁt the challenging VQA problem and generate reli-\nable answers. For example, in Figure 1, when the question\nasks “What is the little girl doing?”, the VQA system is\nexpected to ﬁrstly explore the topic of the answer such as\n“country”, “sport” or “food” as in Figure 1 (c). There is no\ndoubt that clarity on the topic of the answer will greatly con-\ntribute to the success of VQA. Moreover, the listing of similar\nentities (e.g., “baseball”, “taekwondo” and “yoga”) in the\nsame topic will obviously help the VQA system to learn a\nsample-aware answer, since similar entities tend to have sim-\nilar topic-shared features, which will make the prediction of\nthe answer (i.e., “Swimming”) easier. Last but not the least,\nprior models also fail to learn scenario-speciﬁc features. For\nexample, we can notice that in Figure 1, the image shows that\nthe little girl is wearing a swimming lap. If such an scenario-\nor attribute-aware representation can be captured by the VQA\nmodel, it will help reach a more accurate answer.\nTo address the aforementioned limitations, we propose\nHybridPrompt, a cloze- and verify-style hybrid prompt frame-\nwork with bridging language models and human priors in\nprompt tuning for VQA. Speciﬁcally, to address the ﬁrst\nlimitation, we stand on the shoulder of prompt learning\nparadigm (Liu et al. 2021) to propose a “pre-training +\nprompt-tuning” based approach, which modiﬁes the input\nquestions into cloze- and verify-style prompts to mitigate\nthe gap between upstream MLM & ITM tasks and down-\nstream VQA task. Based on such a framework, we further ad-\ndress the second limitation by designing a topic, sample and\nattribute-aware hybrid prompt template to dynamically inte-\ngrate human priors and perform prompt-tuning for achieving\nmore accurate and reliable answer prediction. Experiments\non widely-used VQA v2 dataset demonstrate the effective-\nness of HybridPrompt, showing that it achieves competitive\nperformance and obtains new state-of-the-art results.\nRelated Work\nGeneral VQA Methods (General-VQA). As a vital yet\nchallenging multimodal task, VQA recently has drawn\nmore and more attention. In most general VQA methods,\nattention mechanism and multi-modal fusion are two\nfundamental techniques. The attention mechanism has been\nwidely explored in both computer vision (CV) and natural\nlanguage processing (NLP), and has also been jointly applied\nin many VQA models such as SAN (Yang et al. 2016),\nUpDn (Anderson et al. 2018), BAN (Kim, Jun, and Zhang\n2018), DFAF (Gao et al. 2019) and MCAN (Yu et al. 2019),\nwhich effectively build a crucial bridge for joint reasoning\nbetween multimodal features and signiﬁcantly enhance the\naccuracy of VQA models. Moreover, to achieve further\ninteractions between visual and textual features for answer\nprediction, a series of the VQA models have been proposed\nby employing various advanced bilinear pooling strategies,\nsuch as MLB (Kim et al. 2017), MCB (Fukui et al. 2016),\nMUTAN (Ben-Younes et al. 2017), BLOCK (Ben-Younes\net al. 2019), and MHEF (Lao et al. 2021b). Recently, by\ntaking advantages of graph networks (GN) in relational\nreasoning, some classical GN-based VQA models have been\nproposed (e.g., MuRel (Cadene et al. 2019), ReGAT (Li et al.\n2019a), MN-GMN (Khademi 2020) and DC-GCN (Huang\net al. 2020)) and shown promising results.\nVLP-Based VQA Methods (VLP-VQA). Despite signif-\nicant improvements, previous general VQA methods still\ncannot align visual and textual features well enough for\njoint training. To this end, a series of vision-language\npre-training (VLP)-based VQA models have been proposed\nto mitigate the predicament. Speciﬁcally, the VLP-VQA\nmodels are ﬁrstly pretrained on large-scale paired image-text\ncorpus to obtain uniﬁed-modality representations and then\ntransferred to downstream task to beneﬁt visual question\nanswering. ViLBERT (Lu et al. 2019) and LXMERT (Tan\nand Bansal 2019) are two pioneering researches by adopting\na dual-stream architecture to separately encode visual and\ntextual features and then perform multimodal fusion via a\nuniﬁed encoder. But they are computationally expensive. In\nview of this, some recent works tend to use the single-stream\narchitecture to fuse them and show promising performance.\nFor example, UNITER (Chen et al. 2020) adopts Faster\nR-CNN (Ren et al. 2015) to extract feature sequences of\nRegion of Interest (RoI) from images, and then concatenates\nthem with textual sequence into a uniﬁed Transformer\nencoder, which is mainly supervised by MLM, MRM and\nITM tasks for pre-training. Simultaneously,\nOSCAR (Li\net al. 2020) introduces the object tags into image-text\nsequence and constructs a binary contrastive loss to learn\nimage-text alignments. Later,\nVinVL (Zhang et al. 2021)\nextends the binary contrastive loss into 3-way contrastive\nloss to effectively transfer to VQA task for ﬁne-tuning.\nUnlike them,\nViLT (Kim, Son, and Kim 2021) is the ﬁrst\nto use pre-trained ViT (Dosovitskiy et al. 2020) to extract\nvisual features and directly employ linear mapping method\nto embed image blocks for faster VQA prediction. Likewise,\nsubsequent ALBEF (Li et al. 2021a) and UNIMO (Li et al.\n2021b) also adopt such a uniﬁed architecture and leverage\ncross-modal contrastive learning to align the image and text\nbefore fusing them through cross-modal attention. Recently,\nE2E-VLP (Xu et al. 2021) builds a new uniﬁed Transformer\nframework to jointly learn visual representations and\nsemantic alignments between image and text for end-to-end\nvision-language pre-training. ROSITA (Cui et al. 2021)\nfurther introduces intra-modal and cross-modal knowledge\ngraph into the model and uses a novel SKM strategy to\npretrain, which effectively enhance the semantic alignments.\nVery recently, TCL (Yang et al. 2022) takes advantage of\nlocalized and structural information and proposes triple con-\ntrastive learning for VLP by leveraging both cross-modal and\nintra-modal contrastive objectives to provide complementary\nbeneﬁts in representation learning. Generally speaking,\nthese VLP-VQA methods ﬁrstly use MLM, ITM and their\nextended tasks to pre-train a large-scale language model,\n13372\n[CLS] The bowl is [ANS] which\n[SEP]\nmaterial is a [TOP] such as [S1] [S2] [S3]\nand we can notice that [SOFT PROMPT]\nin … girl\nbowl\ngirl\nbreadfresh fruit\nQ: What / material / is / the / bowl / in / \n      front / of / this / little / girl / ?\nA: Glass\nTransformer-Layer\nC\nMLP\nAdaptive Topic Classification Contrastive Sample Filling\nmaterial\nsport\n[S1] [S2] [S3]Mapping Filling\nTransformer-Layers\nVQA Task\nyes / no glass\nAFV AMC\nPrompt-Tuning  \nLoss\nPos. Prompt\nNeg. Prompt\nHybridPrompt\nPos.\nNeg.food\nTopic Sample\nmaterial ...\n[TOP]\nFigure 2: The framework of proposed HybridPrompt.\nand then directly transfer the pre-trained language model to\ndownstream VQA task for answer prediction but leveraging\na brand new QA objective, which is completely different\nfrom the previous pre-training objectives. This is also the\nreason why the effect of VQA has not been signiﬁcantly\nimproved with the increasing scale of model parameters and\npre-training corpus, that is, the lack of bridging language\nmodel and human prior knowledge in the ﬁne-tuning of VQA.\nPrompt Learning Techniques (PLT).Enhancing the prior\ncapabilities of models by incorporating more knowledge into\nlanguage models (LMs) via prompts has recently sparked\nthe interest of NLP researchers (Petroni et al. 2019; Shin\net al. 2020; Jiang et al. 2020; Li and Liang 2021; Zhong,\nFriedman, and Chen 2021; Lester, Al-Rfou, and Constant\n2021; Gao, Fisch, and Chen 2021; Liu et al. 2021), which\nis another line of research relevant to our work. In this new\ndubbed “pre-train, prompt and predict” paradigm, instead of\nadapting pre-trained LMs to downstream tasks via objective\nengineering, downstream tasks are reorganized to look more\nlike those solved during the original LM training with the\nhelp of a textual prompt. For example, when recognizing the\nemotion of a social media post, “I missed the bus today.”, we\nmay continue with a prompt “I felt so __”. The above exam-\nple is a preﬁx-style prompt, that is, “I missed the bus today.\nI felt so __” as in Preﬁx-Tuning (Li and Liang 2021) and\nPromptTuning (Lester, Al-Rfou, and Constant 2021). How-\never, these approaches require manually constructing context\nprompts for each data sample, which is practically infeasi-\nble. Different from existing prompt learning techniques, in\nthis work, we introduce a topic, sample and attribute-guided\nlearnable hybrid template into VLP models to bridge the uni-\nversal knowledge from LMs and specialized knowledge from\nhuman priors in prompt-tuning for reliable VQA.\nMethodology\nWe ﬁrst present the problem deﬁnition, and then brieﬂy in-\ntroduce our observations, based on which we propose our\nHybridPrompt framework.\nProblem Deﬁnition. Following prior work (Lao et al. 2021a),\nwe deﬁne the VQA task as predicting the most likely an-\nswer a from an answer dictionary A, giving an image I\nfrom image set Iand a question Q from question set Q.\nA VQA dataset with N training instances is denoted as\nD= f(Ii;Qi);aigN\ni=1, where Ii 2I and Qi 2Q are the\nimage and question input of the i-th instance, while ai 2A\ndenotes the correct answer of the i-th instance. Consider-\ning that a question to an image may have multiple correct\nanswers (e.g., “carpet” and “rug”) in widely-used VQA\ndatasets (Goyal et al. 2017), the VQA model can be formally\ndeﬁned as learning a mapping function f : Q\u0002I! [0;1]jAj\nfrom the multimodal inputs Q\u0002I to the answer space A,\nthen producing a probability distribution over Aand select-\ning the answer with the highest probability as output. The\nprobability distribution can be formally deﬁned as,\nP(Aj Ii;Qi) =softmax(f\u0012(Ii;Qi)) (1)\nwhere \u0012denotes the trainable parameters of the VQA model.\nWe can employ a cross-entropy loss for the answer mapping\nclassiﬁcation task. Formally,\nLAMC = \u00001\nN\nNX\ni\njAjX\nj\na\u0003\nijlog (P(aij jIi;Qi)) (2)\nwhere aij denotes the jth answer candidate in Afor the ith\ntraining instance, and a\u0003\nij is its ground truth label.\nIntuitive Observations. We make some intuitive yet mean-\ningful observations about VQA: Firstly, to answer a question,\nit is helpful to narrow down the search range for an answer\nby giving the topic category of the answer. Secondly, inspired\nby the success of contrastive learning, the accuracy of\nvisual question answering can be further improved if several\ntopic-related samples (positive samples) are given to prompt\nanswer generation or several topic-unrelated samples (nega-\ntive samples) are given to exclude topic-unrelated answers.\nThirdly, when searching for answers under the same topic,\nusing a soft-prompting strategy (Li and Liang 2021; Lester,\n13373\nAl-Rfou, and Constant 2021) to train a set of attribute-related\nrepresentations of answers for prompt-tuning is beneﬁcial\nfor improving the VQA model’s ability to distinguish similar\nanswers, which can further improve the model’s performance.\nFramework Overview.Based on the above observations, we\npropose a hybrid prompt framework HybridPrompt, which\naims to introduce a dynamically trainable template for afore-\nmentioned topic, sample and attribute-aware downstream\nprompt-tuning on VQA task. The proposed HybridPrompt\nframework mainly comprises four parts, as shown in Figure 2.\nSpeciﬁcally, a Hybrid Weak Prompt (HWP) layer (Sec. ) is\nﬁrst used to construct a cloze-style weak prompt with [ANS],\n[TOP] and [Si] slots for answer prediction, topic classiﬁ-\ncation and sample ﬁlling respectively. This layer makes the\nVQA task be more close to the upstream MLM pre-training\ntask, which signiﬁcantly reduces its difﬁculty. These tem-\nplates with unﬁlled slots are then fed to the next Dynamic\nHard Prompt (DHP) layer (Sec. ) for dynamic adaptive topic\nclassiﬁcation. By fusing hidden information from [TOP]\nand [CLS] slots, their topic categories can be obtained by\nfeeding the fused representations into an MLP layer followed\nby a softmax function. Then, by searching a topic-sample\nlibrary based on human knowledge and sampling randomly,\nwe can obtain the topic-related samples as positive samples,\nwhereas topic-unrelated ones as negative samples for con-\ntrastive learning. Subsequently, these hard templates ﬁlled\nwith topic and samples are fed into the next Trainable Soft\nPrompt (TSP) layer (Sec. ) for training and updating of soft\nprompts by employing multi-layer Transformers. Finally, an\nAnswer Mapping Classiﬁcation (AMC) layer (Sec. ) is de-\nsigned to conduct the ﬁnal answer mapping classiﬁcation\nby predicting a probability distribution over answer dictio-\nnary\nAunder the obtained hybrid prompts. Note the AMC\nloss is a contrastive loss based on positive and negative sam-\nples, which not only guides the VQA model to give correct\npredictions, but also guides the model to give correct predic-\ntions only if the given prompts are correct. Moreover, a fresh\nanswer ﬁlling veriﬁcation (AFV) loss is designed to super-\nvise the answer prediction process from a global perspective,\nwhich imitates the commonly-used ITM pretraining task and\nmakes the VQA model beneﬁt more from pre-training stage\nof the language models.\nHybrid Weak Prompt (HWP) Layer\nThe HWP layer is designed to modify the questions of\nVQA tasks into the cloze-style hybrid prompt templates for\nadapting to upstream MLM task and facilitating subsequent\nprompt-tuning process. Following Liu et al. (2021), we deﬁne\nour weak prompt operation as a prompting functionfprompt(\u0001),\nwhich aims to map the question Q into a weak prompt tem-\nplate T,\nT = fprompt(Q) (3)\nthe weak prompting operation mainly comprises two steps:\nlinguistic reorganization and weak prompt concatenation.\nLinguistic Reorganization This step is used to modify\neach batch of questions into declarative statements, which\ninclude general questions and special questions. For general\nquestions (i.e., the answer type isyes or no), we simply search\nfor typical words in the sentence and reorganize the sentence\nby placing them after the subject. The typical words we use\ninclude be verbs (e.g., is, am, are, was or were), auxiliary\nverbs (e.g., do, does, did, have, has or had) and modal verbs\n(e.g., may, can, need, might, must, dare, will, shall, would,\nor should). For more complex special questions, we use the\nfollowing rules to reorganize the questions into initial weak\nprompt template Tinitial:\n• what / who / why / how1+ be + [x]: The initial weak\nprompt template is: [x] + be + [ANS];\n• when / where+ be + [x]: The initial weak prompt tem-\nplate is: [x] + be + at / in the + [ANS];\n• what / whose / which+ [x1] + be + [x2]: The initial weak\nprompt template is: [x2] + be + [ANS] + [x1];\n• interrogative + auxiliary / modal + [x]:The initial weak\nprompt template in this case is similar to the previous\nrules, but different from them in that it needs to replace the\nbe verb with the corresponding auxiliary or modal verb,\nbased on the type of the speciﬁc interrogative pronoun.\nWeak Prompt Concatenation This step is used to intro-\nduce human knowledge and concatenate it with the above\ninitial weak prompt template to obtain the ﬁnal weak prompt\ntemplate T. Speciﬁcally, T = [CLS] + Tinitial + ‘which is a’\n+ [TOP] + ‘such as’ +[S1] + [S2] + [S3] + ‘and we can\nnotice that’ +[SOFT PROMPT] + [SEP] + v1 + v2 + \u0001\u0001\u0001\n+ vm, where [CLS], [SEP], [TOP] and [Si] respectively\ndenote classiﬁcation token, separate token, topic token and\nsample token, while [SOFT PROMPT] represents for a set\nof continuous randomly initialized tensors with ﬁxed length.\nMoreover, v1, v2, \u0001\u0001\u0001 , vm respectively denotes visual tokens\nfrom an image detected by the Faster-RCNN (Ren et al. 2015)\nas in UNITER (Chen et al. 2020), and mis the number of\ndetected visual tokens from the image.\nDynamic Hard Prompt (DHP) Layer\nThe DHP layer is the key to implementing topic and sample-\naware prompt tuning in our work, which aims to dynamically\nlearn the possible position of the answer in the latent topic\nspace by exploiting a uniﬁed Transformer layer, and give out\npoints near the answer as positive samples whereas points\nfar away from it as negative samples for contrastive learning.\nSpeciﬁcally, we ﬁrst deﬁne the Transformer layer used for\ndynamic topic classiﬁcation as function ftopic(\u0001),\nH = ftopic(T) (4)\nNote H0 is the hidden state of [CLS], and Hn+4 is the\nrepresentation of [TOP], where n denotes the length of\nTinitial. The DHP layer mainly comprises two steps: adaptive\ntopic classiﬁcation and contrastive sample ﬁlling.\nAdaptive Topic Classiﬁcation To obtain topic-aware\nsentence-level representation to narrow down the search\nrange for an answer, we fuse the hidden state of [CLS] and\n1The special questions guided by how include how long, how\nfar, how much, how old, how heavy, how tall, etc.\n13374\n[TOP] [S 1] [S 2] [S 3] [TOP] [S 1] [S 2] [S 3] [TOP] [S 1] [S 2] [S 3]\nJudge yes no unknown Brand nike apple dell Material plastic wood metal\nColor white blue red Number -10 3.14 58 Pattern strips solid plaid\nType sarcasm humor sorrow Animal cat dog horse Gender male female woman\nTime 7.00 am afternoon night Country usa uk china Weather rainy sunny windy\nSport baseball taekwondo yoga Fruit banana orange apple Food bread milk cake\nAge 1 10 young Name big ben united jack Reason safety fast stability\nOrientation left right north Location beach outside street Person man mom boy\nComment poor clear good Object umbrella kite frisbee Others unknown unknown unknown\nTable 1: The categories of topics and their possible corresponding samples.\n[TOP] by leveraging an MLP layer and a softmax function\nas follows,\nP(cjT) =softmax (MLP (H0;Hn+4)) (5)\nwhere cdenotes the latent topic categories of the answer, and\nP(cjT) is a probability distribution over all categories. The\ncategories labeled by us on VQA v2 dataset are summarized\nin Table 1, which includes K = 24categories in total. We\nthen employ a cross-entropy loss to supervise the adaptive\ntopic classiﬁcation process as follows,\nLATC = \u00001\nN\nNX\ni\nKX\nj\nc\u0003\nijlog(P(cij jT)) (6)\nwhere cij denotes the j-th topic category for the i-th training\ninstance, and c\u0003\nij is its ground truth label annotated by us.\nFinally, a topic word wtop can be obtained by selecting the\ncategory with the highest probability over all labeled topic\ncategories, which will be ﬁlled at the [TOP] slot of the T.\nContrastive Sample Filling To obtain sample-aware\nentity-level representation for further ﬁne-grained locating of\nthe answer, we design the contrastive sample ﬁlling module.\nSpeciﬁcally, we search the topic-sample pairs in Table 1 to\nmatch the topic word wtop obtained in the previous classi-\nﬁcation module and obtain three topic-related items as the\npositive samples for ﬁlling. Note that the three samples shown\nin Table 1 are randomly selected from the sample library with\na maximum size of 10. On the other hand, we also search for\ntopic words that are not related to wtop and their correspond-\ning three random samples as negative samples for ﬁlling,\nwhich are used for pushing away the distance between the\nanswer and the negative samples. Finally, the template ﬁlled\nwith a topic word and several samples can be represented by\nTﬁlled = fT+\nﬁlled;T\u0000\nﬁlledg.\nTrainable Soft Prompt (TSP) Layer\nThe TSP layer is the key to implementing attribute-aware\nprompt for predicting a more ﬁne-grained answer, which\naims to employ the multi-layer Transformers to adaptively\ntrain a set of continuous randomly initialized tensors to ﬁt a\nspeciﬁc yet challenging answer and improve the personalized\nrepresentation ability of the VQA model. Speciﬁcally, we\nadopt 12 layers of Transformer as backbone, which is used\nfor adaptive attribute-aware training and can be deﬁned as\nfunction fattri(\u0001),\nH(l)\nattri = f(l)\nattri(H(l\u00001)\nattri ) (7)\nH(0)\nattri = Tﬁlled (8)\nwhere ldenotes layer number, and H(12)\nattri is the output of the\nlast layer. The TSP layer retrieves the hidden featureH(12)\nans of\nthe [ANS] state and the hidden feature H(12)\ncls of the [CLS]\nstate for prediction and veriﬁcation of the next AMC layer.\nAnswer Mapping Classiﬁcation (AMC)\nThe AMC layer is designed to predict a ﬁnal answer based on\nthe training of a hybrid prompt template. Speciﬁcally, given\nhidden features H(12)\nans and H(12)\ncls , AMC is trained to map the\nhidden feature of the answer into an answer space Ato select\na most likely answer and verify whether the answer is the\ncorrect one under given the correct prompt (i.e., T+\nﬁlled).\nFor answer mapping classiﬁcation, HybridPrompt ﬁrst\nfeeds hidden feature H(12)\nans into an MLP, and then uses a\nsoftmax function to obtain the ﬁnal answer prediction prob-\nability distribution Panswer(Aj T). Note T here refers to the\naforementioned hybrid prompt template from (Ii;Qi) of the\ni-th training instance. Formally,\nPanswer(Aj T) =softmax(MLP(H(12)\nans )) (9)\nThe answer mapping classiﬁcation is a contrastive learning\nprocess over positive and negative prompt templates, which\ncan be constrained by a cross-entropy loss LAMC,\nL+\nAMC = \u00001\nN\nXN\ni\nXjAj\nj\na\u0003\nij log\n\u0000\nP(aij jT+\nﬁlled)\n\u0001\n(10)\nL\u0000\nAMC = 1\nN\nXN\ni\nXjAj\nj\na\u0003\nij log\n\u0000\nP(aij jT\u0000\nﬁlled)\n\u0001\n(11)\nLAMC = L+\nAMC + L\u0000\nAMC (12)\nwhere aij denotes the j-th answer candidate in Afor the i-th\ntraining instance, and a\u0003\nij is its ground truth label.\nFor answer ﬁlling veriﬁcation, HybridPrompt ﬁrst feeds\nhidden feature H(12)\ncls into an MLP, and then further utilizes\na sigmoid function to obtain the ﬁnal answer veriﬁcation\nprobability distribution Pverify(Aj T). Formally,\nPverify(Aj T) =sigmoid(MLP(H(12)\ncls )) (13)\nThe answer ﬁlling veriﬁcation process can also be constrained\nby a cross-entropy loss. Formally,\nLAFV = \u00001\nN\nXN\ni\nXjAj\nj\n`\u0003\nijlog (P(`ij jT)) (14)\n13375\nMethods test-dev (%) test-std (%)\nAll Y/N Num. Other All\nUpDn (2018) 65.32 81.82 44.21 56.05 65.67\nMuRel (2019) 68.03 84.77 49.84 57.85 68.41\nDFAF (2019) 70.22 86.09 53.32 60.49 70.34\nReGAT (2019a) 70.27 86.08 54.42 60.33 70.58\nMCAN (2019) 70.63 86.82 53.26 60.72 70.90\nDC-GCN (2020) 71.21 87.32 53.75 61.45 71.54\nLENA (2021) 69.39 85.87 49.97 59.52 69.70\nMHEF (2021b) 69.91 86.80 45.52 59.90 69.94\nHybridPrompt 76.12 91.65 59.54 66.62 76.30\nTable 2: Comparison results with General-VQA methods.\nBold indicates the winner.\nwhere `ij is a binary value that denotes the veriﬁcation result\nin the j-th answer candidate inAfor the i-th training instance,\nwhile `\u0003\nij is a soft pseudo-label computed by the formula:\n`\u0003\nij = a\u0003\nijlog\n\u0000\nP(aij jT+\nﬁlled)\n\u0001\n(15)\nSimilar to the above AMC loss, answer ﬁlling veriﬁcation\nis also a contrastive learning process, and its loss can be\ncomputed by,\nLAFV = L+\nAFV + L\u0000\nAFV (16)\nIn sum, our model is trained to minimize the following total\nobjective:\nLTotal = LATC + LAMC + LAFV (17)\nExperiments\nExperimental Setup\nDatasets.\nWe evaluate our model on VQA v2 dataset (Goyal\net al. 2017), which is the most commonly-used VQA\nbenchmark dataset and is manually built on the images\nfrom MSCOCO (Lin et al. 2014). The dataset is split into\ntraining (83K images and 444K questions), validation (41K\nimages and 214K questions), and test (81K images and 448K\nquestions) sets.\nImplementation Details. HybridPrompt adopts 12-layer\nTransformers as the backbone. The initial learning rate is set\nto 8e\u00005, and the weight decay is set as 0:01. The batch size\nis set to 128 and the number of iterative steps in training is\nset to 20000. The length of soft prompts and the number of\nnegative templates for each instance are all set to 4. Note\nthe hyperparameters are all tuned with grid-search over the\nvalidation set. We adopt the AdamW (Loshchilov and Hutter\n2018) optimizer to optimize the model and all experiments\nare performed on 4 NVIDIA RTX3090 GPUs with PyTorch.\nBaselines. For a more holistic and objective comparison,\nwe compare HybridPrompt versus the latest “8+11” SOTA\nmodels, and classify them into two groups:\n• General-VQA methods without pre-training, including\nUpDn (Anderson et al. 2018), MuRel (Cadene et al.\n2019), DFAF (Gao et al. 2019), ReGAT (Li et al. 2019a),\nMCAN (Yu et al. 2019), DC-GCN (Huang et al. 2020),\nLENA (Han et al. 2021) and MHEF (Lao et al. 2021b);\nMethods test-dev (%) test-std (%)\nViLBERT (Lu et al. 2019) 70.55 70.92\nLXMERT (Tan and Bansal 2019) 72.42 72.54\nUNITER (Chen et al. 2020) 72.70 72.91\nOSCAR (Li et al. 2020) 73.16 73.44\nViLT(Kim, Son, and Kim 2021) 71.26 -\nE2E-VLP (Xu et al. 2021) 73.25 73.67\nUNIMO (Li et al. 2021b) 73.79 74.02\nROSITA (Cui et al. 2021) 73.91 73.97\nALBEF (Li et al. 2021a) 74.54 74.70\nVinVL (Zhang et al. 2021) 75.95 76.12\nTCL (Yang et al. 2022) 74.90 74.92\nHybridPrompt (Ours) 76.12 76.30\nTable 3: Comparison results with VLP-VQA methods. Bold\nindicates the winner.\n• VLP-VQA methods based on pretraining-then-tuning,\nincluding ViLBERT (Lu et al. 2019), LXMERT (Tan\nand Bansal 2019), UNITER (Chen et al. 2020), OS-\nCAR (Li et al. 2020), ViLT (Kim, Son, and Kim 2021),\nE2E-VLP (Xu et al. 2021), UNIMO (Li et al. 2021b),\nROSITA (Cui et al. 2021), ALBEF (Li et al. 2021a),\nVinVL (Zhang et al. 2021) and TCL (Yang et al. 2022).\nAll baselines use the base model for a fair comparison.\nOverall Performance Comparison\nAs shown in Table 2, HybridPrompt signiﬁcantly outper-\nforms the General-VQA methods and demonstrates excel-\nlent performance on the test-dev and test-std sets. Speciﬁ-\ncally, compared with the current best attention-based method\nMCAN, the multimodal fusion method MHEF and the graph-\nbased method DC-GCN, HybridPrompt respectively exceeds\n7:62%, 9:09% and 6:65% on accuracy score of test-std, in-\ndicating the strong application prospect of the “pre-training\n+ prompt-tuning ” paradigm in multimodal comprehension.\nFurther, from the perspective of visual language pre-training,\nHybridPrompt also achieves competitive performance against\nexisting VLP-VQA models and obtains the new state-of-\nthe-art results on VQA v2 dataset, as shown in Table 3.\nSpeciﬁcally, HybridPrompt outperforms the latest model TCL\nby 1:63% and 1:84% on test-dev and test-std, respectively.\nCompared with the best model VinVL, HybridPrompt also\nachieves a certain amount of improvement, which demon-\nstrates its effectiveness.\nAblation Studies\nWe conduct ablation studies to evaluate the effectiveness of\neach component, as shown in Table 4. Speciﬁcally, #1 de-\nnotes the complete model; #2 w/o answer ﬁlling veriﬁcation\nmeans we train the model without AFV loss; #3 w/o train-\nable soft prompts means we remove the ﬁxed-length soft\nprompts in our hybrid template to train the model; #4 w/o\ncontrastive sample ﬁlling means we remove all the negative\ntemplates and only adopt L+\nAMC and L+\nAFV\nlosses; #5 w/o\nadaptive topic classiﬁcation means ATC loss is removed and\n[TOP] state is implicitly updated; #6 w/o weak prompt\nconcatenation means the topic, sample and attribute-aware\n13376\n# Model Accuracy (%)\ndev \u0001 std \u0001\n1 Complete model 76.12 - 76.30 -\n2 w/o answer ﬁlling veriﬁcation 75.42 0.70 75.68 0.62\n3 w/o trainable soft prompts 75.06 1.06 75.18 1.12\n4 w/o contrastive sample ﬁlling 74.35 1.77 74.49 1.81\n5 w/o adaptive topic classiﬁcation 71.08 5.04 71.15 5.15\n6 w/o weak prompt concatenation 68.21 7.91 68.35 7.95\nTable 4: Ablation study on VQA v2 dataset.\nWhat color is the ball that\nthe little boy is playing?\nbee\nboy \nDirect-tuning  \n(VinVL)\nPrompt-tuning  \n(Ours)\nball\nView: Global\n[TOP]: Color\n[TOP]: Sport\n[CLS]: All\nView 1: Global\nView 2: Local\nFigure 3: Case study.\nhybrid weak prompt is not added and we only adopt linguistic\nreorganization module to modify the inputs and predict the\nanswer at the [ANS] position. From Table 4, we can observe\nthat removing each component will result in a performance\ndegradation. Particularly, w/o weak prompt adding and w/o\nadaptive topic classiﬁcation respectively cause 7:95% and\n5:15% absolute drops in accuracy for test-std, which further\nveriﬁes the effectiveness of our hybrid prompting for VQA.\nFurther Analysis\nCase Study. To verify whether our model can gradually nar-\nrow the view range over the latent answer space and perform\nmore accurate classiﬁcation, we visualize the probability\ndistribution from our AMC layer and VinVL’s classiﬁcation\nlayer. From the visualized distributions in Figure 3, we\ncan see that compared with VinVL, HybridPrompt can\nmore precisely predict the answer “yellow” by gradually\nnarrowing the search range from global view to topic-related\nlocal view (i.e., View 1 !View 2), demonstrating its\nsuperiority in obtaining reliable answer for VQA.\nAttention Visualization. To more clearly illustrate what the\nhard prompts and soft prompts have learned, we visualize\nthe attention weights from the last Transformer-layer in\nTSP, as shown in Figure 4. From Figure 4a, we can observe\nthat HybridPrompt’s [ANS] slot is very good at gaining\nknowledge from [TOP] prompt as well as cross-modal\nvisual prompt (i.e., “swimming lap”), which demonstrates\nthe effectiveness of our hard prompting. From Figure 4b, we\ncan see that soft prompts receive most of the attention from\nother tokens, which validates its latent ability in learning\npersonalized semantics of the whole sentence.\nThe Length of Soft Prompts. To further explore the inﬂu-\nence of different lengths of the soft prompts on model train-\ning, we conduct this set of hyperparameter experiment. From\nFigure 5a, we can see that with the increase of training steps,\nall models can effectively converge. In particular, the longer\ngirl\nis\n[ANS]\nwhich\nis\na\n[TOP]\n[girl]\n[goggles]\n[swimm\ning lap]\ngirl\nis\n[ANS]\nwhich\nis\na\n[TOP]\n[girl]\n[goggles]\n[swimm\ning lap]\n 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) Hard Prompting.\ngirl\nis\n[ANS]\nsoft1\nsoft2\nsoft3\nsoft4\n[girl]\n[goggles]\n[swimm\ning lap]\ngirl\nis\n[ANS]\nsoft1\nsoft2\nsoft3\nsoft4\n[girl]\n[goggles]\n[swimm\ning lap]\n 0.0\n0.2\n0.4\n0.6\n0.8\n1.0 (b) Soft Prompting.\nFigure 4: Attention visualization.\n0 25 50 75 100 125 150 175 200\nsteps(1e2)\n1\n2\n3\n4\n5\n6loss\n2 tokens\n4 tokens\n8 tokens\n16 tokens\n(a) Loss decrease curve.\n0 25 50 75 100 125 150 175 200\nsteps(1e2)\n50\n55\n60\n65\n70\n75score\n 2 tokens\n4 tokens\n8 tokens\n16 tokens (b) Accuracy increase curve.\nFigure 5: Loss decrease and accuracy increase curves under\ndifferent soft prompt lengths.\nthe soft-prompt length is, the more steps are needed for con-\nvergence. For example, the soft prompts with 4 tokens will\nconverge after 10;000 steps, while the soft prompts with 16\ntokens requires 17500 steps for convergence. It also suggests\nthat there may be more implicit knowledge to be learned.\nFrom Figure 5b, we can further see that for the VQA task,\nit is not the longer the soft prompt length, the higher the\naccuracy. Especially for the soft prompts with\n16 tokens, it\nonly achieves about70% accuracy when converges. We guess\nthe reason might be that too-long soft prompts will interfere\nwith the attention among other tokens, making it difﬁcult to\naccurately predict answer from the [ANS] hidden state.\nConclusion\nIn this paper, we propose HybridPrompt, a cloze- and verify-\nstyle hybrid prompt framework with bridging language mod-\nels and human priors in prompt tuning for VQA. Speciﬁ-\ncally, we ﬁrst modify the input questions into the cloze-style\nprompts to mitigate the gap between upstream pre-training\ntasks and downstream VQA task. Then, we further propose a\ndynamically learnable hybrid prompt template for accurate\nand reliable answer prediction. Experiments on commonly-\nused VQA v2 dataset demonstrate the effectiveness of Hy-\nbridPrompt, showing that it outperforms previous VQA meth-\nods and obtains new state-of-the-art results. For future work,\nwe intend to explore human prior-guided prompt-tuning ap-\nproaches from a visual perspective. We also plan to develop\nnon-template prompt generation techniques to see if better\nperformance can be achieved.\n13377\nAcknowledgements\nWe would like to thank all anonymous reviewers for their\nvaluable comments. The work was partially supported by the\nNational Natural Science Foundation of China under Grant\nNo. 62272176 and 61672252.\nReferences\nAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;\nGould, S.; and Zhang, L. 2018. Bottom-up and top-down\nattention for image captioning and visual question answering.\nIn Proceedings of CVPR, 6077–6086.\nBen-Younes, H.; Cadene, R.; Cord, M.; and Thome, N. 2017.\nMutan: Multimodal tucker fusion for visual question answer-\ning. In Proceedings of ICCV, 2612–2620.\nBen-Younes, H.; Cadene, R.; Thome, N.; and Cord, M. 2019.\nBlock: Bilinear superdiagonal fusion for visual question an-\nswering and visual relationship detection. In Proceedings of\nthe AAAI, volume 33, 8102–8109.\nCadene, R.; Ben-Younes, H.; Cord, M.; and Thome, N. 2019.\nMurel: Multimodal relational reasoning for visual question\nanswering. In Proceedings of CVPR, 1989–1998.\nChen, Y .-C.; Li, L.; Yu, L.; El Kholy, A.; Ahmed, F.; Gan,\nZ.; Cheng, Y .; and Liu, J. 2020. Uniter: Universal image-text\nrepresentation learning. In Proceedings of ECCV, 104–120.\nCui, Y .; Yu, Z.; Wang, C.; Zhao, Z.; Zhang, J.; Wang, M.;\nand Yu, J. 2021. ROSITA: Enhancing Vision-and-Language\nSemantic Alignments via Cross-and Intra-modal Knowledge\nIntegration. In Proceedings of ACM MM, 797–806.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An Image is Worth 16x16\nWords: Transformers for Image Recognition at Scale. In\nProceedings of ICLR.\nFukui, A.; Park, D. H.; Yang, D.; Rohrbach, A.; Darrell, T.;\nand Rohrbach, M. 2016. Multimodal Compact Bilinear Pool-\ning for Visual Question Answering and Visual Grounding. In\nProceedings of EMNLP, 457–468.\nGao, P.; Jiang, Z.; You, H.; Lu, P.; Hoi, S. C.; Wang, X.; and\nLi, H. 2019. Dynamic fusion with intra-and inter-modality\nattention ﬂow for visual question answering. In Proceedings\nof CVPR, 6639–6648.\nGao, T.; Fisch, A.; and Chen, D. 2021. Making Pre-trained\nLanguage Models Better Few-shot Learners. In Proceedings\nof the ACL, 3816–3830.\nGoyal, Y .; Khot, T.; Summers-Stay, D.; Batra, D.; and Parikh,\nD. 2017. Making the v in vqa matter: Elevating the role\nof image understanding in visual question answering. In\nProceedings of CVPR, 6904–6913.\nHan, Y .; Guo, Y .; Yin, J.; Liu, M.; Hu, Y .; and Nie, L. 2021.\nFocal and Composed Vision-semantic Modeling for Visual\nQuestion Answering. In Proceedings of ACM MM, 4528–\n4536.\nHuang, Q.; Wei, J.; Cai, Y .; Zheng, C.; Chen, J.; Leung, H.-f.;\nand Li, Q. 2020. Aligned dual channel graph convolutional\nnetwork for visual question answering. In Proceedings of\nACL, 7166–7176.\nJia, C.; Yang, Y .; Xia, Y .; Chen, Y .-T.; Parekh, Z.; Pham, H.;\nLe, Q.; Sung, Y .-H.; Li, Z.; and Duerig, T. 2021. Scaling up\nvisual and vision-language representation learning with noisy\ntext supervision. In Proceedings of ICML, 4904–4916.\nJiang, Z.; Xu, F. F.; Araki, J.; and Neubig, G. 2020. How can\nwe know what language models know? In Journals of TACL,\n8: 423–438.\nKhademi, M. 2020. Multimodal neural graph memory net-\nworks for visual question answering. In Proceedings of ACL,\n7177–7188.\nKim, J.; On, K. W.; Lim, W.; Kim, J.; Ha, J.; and Zhang, B.\n2017. Hadamard Product for Low-rank Bilinear Pooling. In\nICLR 2017.\nKim, J.-H.; Jun, J.; and Zhang, B.-T. 2018. Bilinear attention\nnetworks. In Proceedings of NeurIPS, 31.\nKim, W.; Son, B.; and Kim, I. 2021. Vilt: Vision-and-\nlanguage transformer without convolution or region supervi-\nsion. In Proceedings of ICML, 5583–5594.\nLao, M.; Guo, Y .; Liu, Y .; Chen, W.; Pu, N.; and Lew, M. S.\n2021a. From Superﬁcial to Deep: Language Bias driven\nCurriculum Learning for Visual Question Answering. In\nProceedings of ACM MM, 3370–3379.\nLao, M.; Guo, Y .; Pu, N.; Chen, W.; Liu, Y .; and Lew, M. S.\n2021b. Multi-stage hybrid embedding fusion network for\nvisual question answering. Neurocomputing, 423: 541–550.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The Power of\nScale for Parameter-Efﬁcient Prompt Tuning. In Proceedings\nof the EMNLP 2021, 3045–3059.\nLi, J.; Selvaraju, R.; Gotmare, A.; Joty, S.; Xiong, C.; and\nHoi, S. C. H. 2021a. Align before fuse: Vision and language\nrepresentation learning with momentum distillation. In Pro-\nceedings of NeurIPS, 34.\nLi, L.; Gan, Z.; Cheng, Y .; and Liu, J. 2019a. Relation-aware\ngraph attention network for visual question answering. In\nProceedings of ICCV, 10313–10322.\nLi, L. H.; Yatskar, M.; Yin, D.; Hsieh, C.-J.; and Chang, K.-\nW. 2019b. Visualbert: A simple and performant baseline for\nvision and language. arXiv preprint arXiv:1908.03557.\nLi, W.; Gao, C.; Niu, G.; Xiao, X.; Liu, H.; Liu, J.; Wu,\nH.; and Wang, H. 2021b. UNIMO: Towards Uniﬁed-Modal\nUnderstanding and Generation via Cross-Modal Contrastive\nLearning. In Proceedings of ACL, 2592–2607.\nLi, X.; Yin, X.; Li, C.; Zhang, P.; Hu, X.; Zhang, L.; Wang,\nL.; Hu, H.; Dong, L.; Wei, F.; et al. 2020. Oscar: Object-\nsemantics aligned pre-training for vision-language tasks. In\nProceedings of ECCV, 121–137.\nLi, X. L.; and Liang, P. 2021. Preﬁx-Tuning: Optimizing\nContinuous Prompts for Generation. In Proceedings of the\nACL, 4582–4597.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.;\nRamanan, D.; Dollár, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In Proceedings of ECCV,\n740–755.\nLiu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,\nG. 2021. Pre-train, prompt, and predict: A systematic survey\n13378\nof prompting methods in natural language processing. arXiv\npreprint arXiv:2107.13586.\nLiu, Y .; Wei, W.; Peng, D.; and Zhu, F. 2022. Declaration-\nbased Prompt Tuning for Visual Question Answering. arXiv\npreprint arXiv:2205.02456.\nLoshchilov, I.; and Hutter, F. 2018. Decoupled Weight Decay\nRegularization. In Proceedings of ICLR 2018.\nLu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. ViLBERT:\nPretraining Task-Agnostic Visiolinguistic Representations\nfor Vision-and-Language Tasks. In Proceedings of NeurIPS,\n13–23.\nMa, Z.; Li, J.; Li, G.; and Cheng, Y . 2022a. UniTranSeR: A\nUniﬁed Transformer Semantic Representation Framework for\nMultimodal Task-Oriented Dialog System. In Proceedings\nof the 60th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), 103–114.\nMa, Z.; Li, J.; Li, G.; and Huang, K. 2022b. CMAL: A Novel\nCross-Modal Associative Learning Framework for Vision-\nLanguage Pre-Training. In Proceedings of the 30th ACM\nInternational Conference on Multimedia, 4515–4524.\nPetroni, F.; Rocktäschel, T.; Riedel, S.; Lewis, P.; Bakhtin, A.;\nWu, Y .; and Miller, A. 2019. Language Models as Knowledge\nBases? In Proceedings of EMNLP, 2463–2473.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from natural\nlanguage supervision. In Proceedings of ICML, 8748–8763.\nRen, S.; He, K.; Girshick, R. B.; and Sun, J. 2015. Faster\nR-CNN: Towards Real-Time Object Detection with Region\nProposal Networks. In Proceedings of NeurIPS, 91–99.\nShin, T.; Razeghi, Y .; Logan IV , R. L.; Wallace, E.; and\nSingh, S. 2020. AutoPrompt: Eliciting Knowledge from\nLanguage Models with Automatically Generated Prompts.\nIn Proceedings of EMNLP, 4222–4235.\nTan, H.; and Bansal, M. 2019. LXMERT: Learning Cross-\nModality Encoder Representations from Transformers. In\nProceedings of EMNLP, 5099–5110.\nXu, H.; Yan, M.; Li, C.; Bi, B.; Huang, S.; Xiao, W.; and\nHuang, F. 2021. E2E-VLP: End-to-End Vision-Language\nPre-training Enhanced by Visual Learning. In Proceedings\nof ACL, 503–513.\nYang, J.; Duan, J.; Tran, S.; Xu, Y .; Chanda, S.; Chen, L.;\nZeng, B.; Chilimbi, T.; and Huang, J. 2022. Vision-Language\nPre-Training with Triple Contrastive Learning.arXiv preprint\narXiv:2202.10401.\nYang, Z.; He, X.; Gao, J.; Deng, L.; and Smola, A. 2016.\nStacked attention networks for image question answering. In\nProceedings of CVPR, 21–29.\nYu, Z.; Yu, J.; Cui, Y .; Tao, D.; and Tian, Q. 2019. Deep\nmodular co-attention networks for visual question answering.\nIn Proceedings of CVPR, 6281–6290.\nYu, Z.; Yu, J.; Fan, J.; and Tao, D. 2017. Multi-modal fac-\ntorized bilinear pooling with co-attention learning for visual\nquestion answering. In Proceedings of ICCV, 1821–1830.\nYu, Z.; Yu, J.; Xiang, C.; Fan, J.; and Tao, D. 2018. Beyond\nbilinear: Generalized multimodal factorized high-order pool-\ning for visual question answering. IEEE TNNLS, 29(12):\n5947–5959.\nZhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.; Choi,\nY .; and Gao, J. 2021. Vinvl: Revisiting visual representations\nin vision-language models. In Proceedings of CVPR, 5579–\n5588.\nZhong, Z.; Friedman, D.; and Chen, D. 2021. Factual Probing\nIs [MASK]: Learning vs. Learning to Recall. In Proceedings\nof NAACL, 5017–5033.\n13379"
}