{
  "title": "Arabic Grammatical Error Detection Using Transformers-based Pretrained Language Models",
  "url": "https://openalex.org/W4385705364",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092620183",
      "name": "Sarah AlOyaynaa",
      "affiliations": [
        "Imam Mohammad ibn Saud Islamic University"
      ]
    },
    {
      "id": "https://openalex.org/A2088076765",
      "name": "Yasser Kotb",
      "affiliations": [
        "Imam Mohammad ibn Saud Islamic University",
        "Ain Shams University"
      ]
    },
    {
      "id": "https://openalex.org/A5092620183",
      "name": "Sarah AlOyaynaa",
      "affiliations": [
        "Imam Mohammad ibn Saud Islamic University"
      ]
    },
    {
      "id": "https://openalex.org/A2088076765",
      "name": "Yasser Kotb",
      "affiliations": [
        "Imam Mohammad ibn Saud Islamic University",
        "Ain Shams University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4200101290",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W4309988928",
    "https://openalex.org/W2901510365",
    "https://openalex.org/W3118802803",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2972834553",
    "https://openalex.org/W3081459453",
    "https://openalex.org/W2061443933",
    "https://openalex.org/W3163832451",
    "https://openalex.org/W3018086583",
    "https://openalex.org/W3116890009",
    "https://openalex.org/W2251944986",
    "https://openalex.org/W6739901393"
  ],
  "abstract": "This paper presents a new study to use pre-trained language models based on the transformers for Arabic grammatical error detection (GED). We proposed fine-tuned language models based on pre-trained language models called AraBERT and M-BERT to perform Arabic GED on two approaches, which are the token level and sentence level. Fine-tuning was done with different publicly available Arabic datasets. The proposed models outperform similar studies with F1 value of 0.87, recall of 0.90, precision of 0.83 at the token level, and F1 of 0.98, recall of 0.99, and precision of 0.97 at the sentence level. Whereas the other studies in the same field (i.e., GED) results less than the current study (e.g., F0.5 of 69.21). Moreover, the current study shows that the fine-tuned language models that were built on the monolingual pre-trained language models result in better performance than the multilingual pre-trained language models in Arabic.",
  "full_text": " \nArabic Grammatical Error Detection Using \nTransformers-based Pretrained Language \nModels \nSarah AlOyaynaa 1*, Yasser Kotb1,2 \n1Information Systems Department, College of Computer and Information Sciences, Imam \nMohammad Ibn Saud Islamic University (IMSIU), Riyadh, Saudi Arabia. \n2 Computer Science Division, Department of Mathematics, Faculty of Science, Ain Shams University, \nCairo, Egypt. \nAbstract. This paper presents a new study to use pre -trained language \nmodels based on the transformers for Arabic grammatical error detection \n(GED). We proposed fine -tuned language models based on pre -trained \nlanguage models called AraBERT and M-BERT to perform Arabic GED on \ntwo approaches, which are the token level and sentence level. Fine -tuning \nwas done with different publicly available Arabic datasets. The proposed \nmodels outperform similar studies with F1 value of 0.87, recall of 0.90, \nprecision of 0.83 at the token level, and F1 of 0.98, recall of 0.99, and \nprecision of 0.97 at the sentence level. Whereas the other studies in the same \nfield (i.e., GED) results less than the current study (e.g., F0.5 of 69.21). \nMoreover, the current study shows that the fine-tuned language models that \nwere built on the monolingual pre -trained language models result in better \nperformance than the multilingual pre-trained language models in Arabic. \n1 Introduction \nLanguage is often considered the hallmark of human intelligence. Thus, developing systems \nthat can understand human language are one of the main obstacl es in the quest for artificial \ngeneral intelligence. \nMachine learning (ML) and Deep Learning (DL) which are subdomains of Artificial \nIntelligence (AI) have succeeded extensively and outperformed other technologies in many \nAI aspects (Q. Zhang et al., 2018)  such as speech recognition, image analysis, and natural \nlanguage processing (NLP). NLP is “ a theory-motivated range of computational techniques \nfor the automatic analysis and representation of human language” (Young et al., 2017). It is \na field of ML deal ing with linguistics that builds and develops Language Models (Singh & \nMahmood, 2021) . There are a lot of NLP tasks such as text classification, name entity \nrecognition, question and answering, and machine translation (Montejo-Ráez & Jiménez -\nZafra, 2022). \nPreviously, there was a misunderstanding that machine learning tasks can only achiev e \ngood results if there is a huge amount of data that have similar distribution and feature space. \n \n* Corresponding author: saloyiana@sm.imamu.edu.sa \n© The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative Commons \nAttribution License 4.0 (https://creativecommons.org/licenses/by/4.0/).\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\nHence, that making implementing machine learning tasks and their collection of labeled \ntraining data are expensive and difficult. Transfer learning came to t ackle these challenges \nby reducing the need for gathering labeled training data for new tasks as well as reemploying \nthe knowledge gained from a different task (Sarhan & Spruit, 2020). \nThe transformers model is one of the most famous ML architectures that was introduced \nby Google (Vaswani, 2017) which is a sequence -to-sequence (S2S) architecture and mainl y \nproposed to perform neural machine translation (NMT) (Karita et al., 2019). Its architecture \nconsists of the encoder and decoder layers, and one is coupled to the other through layers of \nthe feed -forward network and multi -head attention. The cosine and s ine functions, which \nproduce positional encoding, assist the model, and recall the order and position of words. \nSelf-attention is used as a main method by the encoder and decoder layer’s multi -head \nattention layer (Chouikhi & Alsuhaibani, 2022). \nThe recent launches of improved Transformer (Vaswani, 2017) based models like BERT \n(Devlin et al., 2019) turned out to be a climacteric year for the NLP world. These models \nwere trained on large datasets to create pre-trained language models. Then, transfer learning \nwas used to fine -tune these models on a small, labeled dataset for  task-specific features \nresulting in significant performance enhancement on several NLP tasks (Elmadani et al., \n2020)(Sun et al., 2020). \nGrammatical Error Detection and Correction (GED&C) systems are commonly used to \nidentify grammatical writing errors and then correct them automatically. Grammatical Error \nDetection (GED) is one of the key components of the grammatical error correction (GEC) \ncommunity (Q. Wang & Tan, 2020). Even earlier, it was the first step of GEC, especially, \nthe GEC approaches that are based on hand-crafted rules. Those rules are different for various \ngrammatical error types, thus, detecting errors in the given text is the basis of the correction \nsystem (Q. Wang & Tan, 2020). Thereafter, such systems are handy for both natives and \nlearners. In terms of language learners, an effective and automatic GED system is useful for \nthem to find whether the texts written themselves have errors or not. \nMoreover, the implementations of the GED&C systems are not new in the field and there \nare many efforts to master these tasks. Furthermore, some of them gained great success such \nas implementing such a system using statistical machine translation techniques (Brockett et \nal., 2006).  However, in terms of the Arabic language, there are a few and still not those \nefficient efforts, and implementing such systems requires many resources which could be \nchallenging. \nArabic is one of the world’s five major languages with over 290 million native speakers \nand a total of 422 million world speakers (UNESCO, 2019) (Ethnologue, 2020). It is a \nSemitic, highly structured, and derivational language where morphology and syntax play a \nvery important role (Shaalan, 2005). However, as it is a widely spoken language but in terms \nof NLP applications and studies especially GED&C is still new in the research field compared \nwith other prominent languages (e.g., English). Hence, that could lead to hardening the \ncontribution process. \nThe current study tackles the low -resource problem by taking the pre -trained language \nmodels that were built based on transformers architecture (Vaswani, 2017) to perform the \nGED task based on two approaches which are: the token level classification and sentence \nlevel classification, both in Arabic Language. \nTable 1  represents the covered errors of the token level classification task in the current \nstudy (Shaalan, 2005) (Zaghouani et al., 2014): \n  \n2\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\nHence, that making implementing machine learning tasks and their collection of labeled \ntraining data are expensive and difficult. Transfer learning came to t ackle these challenges \nby reducing the need for gathering labeled training data for new tasks as well as reemploying \nthe knowledge gained from a different task (Sarhan & Spruit, 2020). \nThe transformers model is one of the most famous ML architectures that was introduced \nby Google (Vaswani, 2017) which is a sequence -to-sequence (S2S) architecture and mainl y \nproposed to perform neural machine translation (NMT) (Karita et al., 2019). Its architecture \nconsists of the encoder and decoder layers, and one is coupled to the other through layers of \nthe feed -forward network and multi -head attention. The cosine and s ine functions, which \nproduce positional encoding, assist the model, and recall the order and position of words. \nSelf-attention is used as a main method by the encoder and decoder layer’s multi -head \nattention layer (Chouikhi & Alsuhaibani, 2022). \nThe recent launches of improved Transformer (Vaswani, 2017) based models like BERT \n(Devlin et al., 2019) turned out to be a climacteric year for the NLP world. These models \nwere trained on large datasets to create pre-trained language models. Then, transfer learning \nwas used to fine -tune these models on a small, labeled dataset for  task-specific features \nresulting in significant performance enhancement on several NLP tasks (Elmadani et al., \n2020)(Sun et al., 2020). \nGrammatical Error Detection and Correction (GED&C) systems are commonly used to \nidentify grammatical writing errors and then correct them automatically. Grammatical Error \nDetection (GED) is one of the key components of the grammatical error correction (GEC) \ncommunity (Q. Wang & Tan, 2020). Even earlier, it was the first step of GEC, especially, \nthe GEC approaches that are based on hand-crafted rules. Those rules are different for various \ngrammatical error types, thus, detecting errors in the given text is the basis of the correction \nsystem (Q. Wang & Tan, 2020). Thereafter, such systems are handy for both natives and \nlearners. In terms of language learners, an effective and automatic GED system is useful for \nthem to find whether the texts written themselves have errors or not. \nMoreover, the implementations of the GED&C systems are not new in the field and there \nare many efforts to master these tasks. Furthermore, some of them gained great success such \nas implementing such a system using statistical machine translation techniques (Brockett et \nal., 2006).  However, in terms of the Arabic language, there are a few and still not those \nefficient efforts, and implementing such systems requires many resources which could be \nchallenging. \nArabic is one of the world’s five major languages with over 290 million native speakers \nand a total of 422 million world speakers (UNESCO, 2019) (Ethnologue, 2020). It is a \nSemitic, highly structured, and derivational language where morphology and syntax play a \nvery important role (Shaalan, 2005). However, as it is a widely spoken language but in terms \nof NLP applications and studies especially GED&C is still new in the research field compared \nwith other prominent languages (e.g., English). Hence, that could lead to hardening the \ncontribution process. \nThe current study tackles the low -resource problem by taking the pre -trained language \nmodels that were built based on transformers architecture (Vaswani, 2017) to perform the \nGED task based on two approaches which are: the token level classification and sentence \nlevel classification, both in Arabic Language. \nTable 1  represents the covered errors of the token level classification task in the current \nstudy (Shaalan, 2005) (Zaghouani et al., 2014): \n  \n \nTable 1. Covered errors in token level task. \nType of Error Definition Example \nSpelling \nDeleting at least one character \nof a word or inserting an extra \ncharacter of a word. \n  سموه ودعى \nMorphology Incorrect derivation or \ninflection. \nالطموحـةالتنمية  \n  حديقاتذهب علي إلى  \nجميلة. \nSyntactic \nWrong agreement in gender, \nnumber, or case, wrong case \nassignment, wrong order of \nwords, wrong tense use, \nmissing or redundant words. \nالجديدأنا أدرس في الجامعة  .\n عدد المواقف تقلأن لا  \n \nWhereas Table  2 shows the covered errors of the sentence level classification as follows: \n \nTable 2. Covered errors in sentence level. \nType of Error Definition Example \nSpelling \nMissing words Deleting at least one word لعب الولد القدم \nDuplicate words Duplicate at least one word \nwithout any meaning \nتفرح بكونك تفرحلا \nمسيطرا  \nMissing character Deleting at least one \ncharacter of a word \nمشتركة قضياهناك \nبينهما  \nDuplicate character \nDuplicate at least one \ncharacter without any \nmeaning \nهناك قضايا مشتركة\nبينهههههما  \n \nMoreover, a comparison between the pre -trained language model that was originally \ntrained on many different languages such as English, Chinese, etc., (i.e., multilingual \nlanguage model) and the pre -trained language model which was originally trained on one  \nlanguage only (i.e., monolingual language model) (Pires et al., 2020) will be provided. \nThe paper structure is as follows: in the next section, we will provide a review of the \nliterature. After that, the following methodology will be presented. The third section will \ncover the main results of the study  as well as the discussion part. Finally, we will conclude \nand provide the future direction and some study limitations.\n \n2 Literature Review \nWith advancements in industry and information technology, large volumes of electronic \ndocuments such as newspapers, em ails, weblogs, and these are produced daily. Producing \nelectronic documents has considerable benefits. Therefore, the existence of automatic \nsystems such as spell and grammar -checker/correctors can help to improve their quality \n(Ehsan, 2014).  \nThe GED&C ta sk is not a new application in the field of NLP. In the literature, NLP \nresearchers put the effort to achieve the GED&C task with high performance at different \ntimes, via different approaches and architectures, and diverse languages.  \n3\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\nParnow et al. (2020) i ntroduced a transformer -based model with formulating the GEC \ntask as a semi-supervised learning problem. The model was trained using three labeled GEC \ncorpora and a couple as plain text corpora. It tested on the CoNLL-2014 test benchmark and \nachieved F0.5 of 60.2 without a fully pre -trained encoder. The proposed method showed a \ncompetitive result with the present leading models. \nA study done by Grammarly (Alikaniotis et al., 2019), highlighted the effectiveness of \nthe transformer architecture approach in th e GEC task. It used three different pre -trained \nlanguage models: BERT, GPT, and GPT-2 with no annotated data to be trained. To evaluate \nthe proposed model, it employed two standard publicly available datasets: CoNLL-2014 and \nFCE. The result showed that this approach leverages the state-of-the-art against the previous \nstate-of-the-art trained on large corpora with transformer architecture. However, the study \ndiscouraged following the same employed methodology and invited new improvements.  \nQiu & Qu (2019) ado pted a two stage -model to incrementally correct Chinese \ngrammatical errors. In the first stage, it checked for spelling mistakes and correct them based \non the language model. It used a language model to select the best probable word from \ncandidates. The se cond stage focused on correcting the grammatical errors and remaining \nspelling mistakes based on the sequence -to-sequence transformer model. It treated the \ncorrection task as a translation task.   The model training was done on NLPCC 2018 shared \ntask 2. The proposed system outperformed the compared state -of-the-art systems with F0.5 \nof 32.01. \nLanguage grammars are complex in nature, and when we talk about Arabic it became \nmore complex.  Even Arabic -speaking people nowadays are not fully familiar with the \ngrammar of their language. Thus, Arabic grammatical checking is considered a difficult task. \nThe difficulty comes from several sources (K. Shaalan et al., 1993) such as the length of the \nsentence and the complex Arabic syntax; the omission of diacritics (vowels) in written Arabic \n‘at-ta.sk¯il’; the free word  order nature of Arabic sentence; and the presence of an elliptic \npersonal pronoun ‘ad.-dam¯iral-mustatir’(K. F. Shaalan, 2005). \nArabic language in nature has many forms. The modern form of Arabic and most used by \nhumans is called Modern Standard Arabic (M SA). MSA is a simplified form of Classical \nArabic and follows the same grammar of it (K. Shaalan, 2010). The main differences between \nClassical and MSA are that MSA has a larger and more modern vocabulary and does not use \nsome of the more complicated forms of grammar found in Classical Arabic. \nThe authors in (Solyman et al., 2019) proposed a hybrid model for Arabic GEC based on \ndeep encoder -decoder architecture. It used Multi convolutional layers (i.e., nine \nconvolutional encoder and nine decoder layers), with an attention mechanism. Also, it was \ntested using the testing set of a larg e Arabic corpus. The model gains precision of 70.23, \nrecall of 72.10, and F1 of 71.14.  \nTherefore, the findings showed high statical results out of the proposed model. (Madi & \nAl-Khalifa, 2018) proposed a model for the Arabic GED tool based on DL. It employed a \nsimple recurrent neural network (RNN) architecture. Nonetheless, this study used a \nhandcrafted and very small corpus. The results showed that the tool was good to identify the \nerror to the user, but, with no suggested corrections. As an improvement of the previous work, \n(Madi & Al -Khalifa, 2020)  developed a set of systems ba sed on RNN, Long short -term \nmemory (LSTM), and A Bidirectional LSTM (BiLSTM) architectures, all the work done on \na large Arabic corpus, unlike the previous work. The result revealed that the BiLSTM \nperformed the best of all systems in terms of F0.5 of 81.5 5% in the training set and LSTM \nwas the worst performance. However, on the test set, the results show the opposite overall \nmeasures. \nAl-qaraghuli (2021) developed a vanilla transformer to correct soft spelling mistakes in \nArabic. The model handled four typ es of soft spelling mistakes, namely, confusion among \nshapes, both shapes at the end of the word, hamza alef ( همزة الألفinsertion and omission of \n4\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\nParnow et al. (2020) i ntroduced a transformer -based model with formulating the GEC \ntask as a semi-supervised learning problem. The model was trained using three labeled GEC \ncorpora and a couple as plain text corpora. It tested on the CoNLL-2014 test benchmark and \nachieved F0.5 of 60.2 without a fully pre -trained encoder. The proposed method showed a \ncompetitive result with the present leading models. \nA study done by Grammarly (Alikaniotis et al., 2019), highlighted the effectiveness of \nthe transformer architecture approach in th e GEC task. It used three different pre -trained \nlanguage models: BERT, GPT, and GPT-2 with no annotated data to be trained. To evaluate \nthe proposed model, it employed two standard publicly available datasets: CoNLL-2014 and \nFCE. The result showed that this approach leverages the state-of-the-art against the previous \nstate-of-the-art trained on large corpora with transformer architecture. However, the study \ndiscouraged following the same employed methodology and invited new improvements.  \nQiu & Qu (2019) ado pted a two stage -model to incrementally correct Chinese \ngrammatical errors. In the first stage, it checked for spelling mistakes and correct them based \non the language model. It used a language model to select the best probable word from \ncandidates. The se cond stage focused on correcting the grammatical errors and remaining \nspelling mistakes based on the sequence -to-sequence transformer model. It treated the \ncorrection task as a translation task.   The model training was done on NLPCC 2018 shared \ntask 2. The proposed system outperformed the compared state -of-the-art systems with F0.5 \nof 32.01. \nLanguage grammars are complex in nature, and when we talk about Arabic it became \nmore complex.  Even Arabic -speaking people nowadays are not fully familiar with the \ngrammar of their language. Thus, Arabic grammatical checking is considered a difficult task. \nThe difficulty comes from several sources (K. Shaalan et al., 1993) such as the length of the \nsentence and the complex Arabic syntax; the omission of diacritics (vowels) in written Arabic \n‘at-ta.sk¯il’; the free word  order nature of Arabic sentence; and the presence of an elliptic \npersonal pronoun ‘ad.-dam¯iral-mustatir’(K. F. Shaalan, 2005). \nArabic language in nature has many forms. The modern form of Arabic and most used by \nhumans is called Modern Standard Arabic (M SA). MSA is a simplified form of Classical \nArabic and follows the same grammar of it (K. Shaalan, 2010). The main differences between \nClassical and MSA are that MSA has a larger and more modern vocabulary and does not use \nsome of the more complicated forms of grammar found in Classical Arabic. \nThe authors in (Solyman et al., 2019) proposed a hybrid model for Arabic GEC based on \ndeep encoder -decoder architecture. It used Multi convolutional layers (i.e., nine \nconvolutional encoder and nine decoder layers), with an attention mechanism. Also, it was \ntested using the testing set of a larg e Arabic corpus. The model gains precision of 70.23, \nrecall of 72.10, and F1 of 71.14.  \nTherefore, the findings showed high statical results out of the proposed model. (Madi & \nAl-Khalifa, 2018) proposed a model for the Arabic GED tool based on DL. It employed a \nsimple recurrent neural network (RNN) architecture. Nonetheless, this study used a \nhandcrafted and very small corpus. The results showed that the tool was good to identify the \nerror to the user, but, with no suggested corrections. As an improvement of the previous work, \n(Madi & Al -Khalifa, 2020)  developed a set of systems ba sed on RNN, Long short -term \nmemory (LSTM), and A Bidirectional LSTM (BiLSTM) architectures, all the work done on \na large Arabic corpus, unlike the previous work. The result revealed that the BiLSTM \nperformed the best of all systems in terms of F0.5 of 81.5 5% in the training set and LSTM \nwas the worst performance. However, on the test set, the results show the opposite overall \nmeasures. \nAl-qaraghuli (2021) developed a vanilla transformer to correct soft spelling mistakes in \nArabic. The model handled four typ es of soft spelling mistakes, namely, confusion among \nshapes, both shapes at the end of the word, hamza alef ( همزة الألف insertion and omission of \nafter aljamaea (الجماعةand alef waw confusion among teh,marbuta (تاء مربوطةand at the \nend of the teh heh (تاءword. Moreover, to train the model, they used Tashkeela set to train \none model and the Wiki -40B to train another model. Moreover, they developed a dataset \ncontaining artificial errors generated following a stochastic error injection approach that was \nproposed (Khedher, 2021) to use in the model testing phase. In terms of the Tashkeela model \nperformance, the best result achieved is accuracy of 99.62% whereas, the wiki model \nrevealed the best accuracy of 99.77%. \nSCUT AGEC is a model that is introduced by (Solyman et al., 2021), it is a sequence-to-\nsequence model that was built on Convolutional Neural Network (CNN) architecture which \nconsists of nine encoder-decoder layers with an attention mechanism. It was pre-trained and \nfinetuned on a data set that was developed during the study called SCUT which is synthetic \ndata consisting of 18,061,610 words divided into training and development sets. The model \nachieved state -of-the-art results against the neural AGEC models with the strong \ncompetitiveness of the hybrid AGEC systems on the Qatar Arabic Language Bank (QALB) \ntest set with F1 of 70.64%. Table 3 provides a summary of the reviewed studies: \n \nTable 3. Summary of the literature review. \nStudy Year Language Scope Result \n(Solyman et al., 2021) 2021 Arabic Correction \nPrecision of 70.23 \nRecall of 72.10, \nF1 of 71.14 \n(Al-qaraghuli, 2021) 2021 Arabic Correction Best accuracy of: 99.77 \n(Parnow et al., 2020) 2020 English Correction F05 of 60.2 \n(Madi & Al-Khalifa, 2020) 2020 Arabic Detection \nF0.5 of 69.21 \nRecall of 41.61 \nPrecision of 82.97 \n(Qiu & Qu, 2019) 2019 Chinese Detection and \nCorrection F05 of 32.01 \n \nAfter all, the efforts in applying GED&C models to Arabic are low compared with other \nlanguages (i.e., English, Indian, and Chines). Therefore, this study aims to provide a new \ncontribution to the Arabic language field. Also, help native and non -native Arabic speakers \nimprove their writing skills and become more confident regarding their documentation.  \nMore recently, the method of pre -training language models on an amount of unlabeled \ndata and fine -tuning in downstream tasks has made a breakthrough in seve ral natural \nlanguage understanding tasks (Antoun et al., 2020)(Devlin et al., 2019). To the best of our \nknowledge, this method has not been utilized to serve Arabic GED&C yet despite its \nnoticeable results in the literature. Therefore, the current study wi ll use the popular pre -\ntrained language models that were built based on the transformers (Vaswani, 2017) \narchitecture to help improve the Arabic GED task. Moreover, there will be an evaluation of \nthe existing pre -trained language models in terms of perform ing the GED task to highlight \nthe possibility of using such a method in that regard.  \n5\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\n3 Methodology \nThe proposed work followed two different tasks which are the token-level GED task and the \nsentence-level GED task. The upcoming section explains both in deta il. Fig 1  provides a \nhigh-level overview of the study's methodology: \n \n \nFig. 1. Methodology Overview. \n3.1 Token-level grammatical error detection \nIn this task, the grammatical correctness of the word level (i.e., token) will be checked.  \n3.1.1 Datasets \nFirst, we used A7'ta (أخطاءdataset which is a dataset that was published to rich the GED \ndomain. A7'ta (Madi & Al -Khalifa, 2019) is a collection of modern Arabic sentences. It \ncontained 470 erroneous sentences and their 470 error -free counterparts, and they are \ncategorized into eight main categories: syntax errors, morphological errors, semantic errors, \nlinguistic errors, stylistic errors, spelling errors, punctuation errors, and the use of informal \nas well as borrowed words. This dataset was developed mainly to se rve the grammar -\nchecking model training. However, there is a data imbalance issue declared by a dataset \nconsumer (Madi & Al-Khalifa, 2020).  \nDue to the lack of prepared datasets to be consumed in the targeted field, and despite the \nimbalance issue, A7'ta is a highly prepared dataset for the current task. Therefore, we tried \nto tackle the dataset issue and improve it by taking a portion of other publicly available \ndatasets in the field (i.e., QALB (Mohit et al., 2015)) and SCUT (Solyman et al., 2021)) and \nreformat them to match A7'ta format. Then, merging all of them into one dataset. Finally, the \nfinal dataset was split into two datasets which are the training dataset to be used in the fine-\ntuning step and the test dataset for testing the model performance. The splitting was applied \nrandomly with a percentage of 70%-20% for train-test datasets with a random state of 200. \n3.1.2 Data Preprocessing \nTo be able to start our experiments, first, we started with the tokenization process which is a \nstep where the sentences decompose into tokens (i.e., splitting the sentences into words, each \nword called a \"token\". As we aim to use BERT based model, we had to add the special [CLS] \nsymbol at the start of every text and the token [SEP] at the end of each sentence as that is the \nsupported format by the model.  We utilized a tokenizer class in the hugging face library \nnamed \"BertTokenizerFast\" with its default values except the keys in Table 4: \n  \n6\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\n3 Methodology \nThe proposed work followed two different tasks which are the token-level GED task and the \nsentence-level GED task. The upcoming section explains both in deta il. Fig 1  provides a \nhigh-level overview of the study's methodology: \n \n \nFig. 1. Methodology Overview. \n3.1 Token-level grammatical error detection \nIn this task, the grammatical correctness of the word level (i.e., token) will be checked.  \n3.1.1 Datasets \nFirst, we used A7'ta (أخطاءdataset which is a dataset that was published to rich the GED \ndomain. A7'ta (Madi & Al -Khalifa, 2019) is a collection of modern Arabic sentences. It \ncontained 470 erroneous sentences and their 470 error -free counterparts, and they are \ncategorized into eight main categories: syntax errors, morphological errors, semantic errors, \nlinguistic errors, stylistic errors, spelling errors, punctuation errors, and the use of informal \nas well as borrowed words. This dataset was developed mainly to se rve the grammar -\nchecking model training. However, there is a data imbalance issue declared by a dataset \nconsumer (Madi & Al-Khalifa, 2020).  \nDue to the lack of prepared datasets to be consumed in the targeted field, and despite the \nimbalance issue, A7'ta is a highly prepared dataset for the current task. Therefore, we tried \nto tackle the dataset issue and improve it by taking a portion of other publicly available \ndatasets in the field (i.e., QALB (Mohit et al., 2015)) and SCUT (Solyman et al., 2021)) and \nreformat them to match A7'ta format. Then, merging all of them into one dataset. Finally, the \nfinal dataset was split into two datasets which are the training dataset to be used in the fine-\ntuning step and the test dataset for testing the model performance. The splitting was applied \nrandomly with a percentage of 70%-20% for train-test datasets with a random state of 200. \n3.1.2 Data Preprocessing \nTo be able to start our experiments, first, we started with the tokenization process which is a \nstep where the sentences decompose into tokens (i.e., splitting the sentences into words, each \nword called a \"token\". As we aim to use BERT based model, we had to add the special [CLS] \nsymbol at the start of every text and the token [SEP] at the end of each sentence as that is the \nsupported format by the model.  We utilized a tokenizer class in the hugging face library \nnamed \"BertTokenizerFast\" with its default values except the keys in Table 4: \n  \nTable 4. Tokenizer settings in token level. \nKey Value \nReturn offsets mapping true \nPadding 128 \nTruncation true \nMax length 128 \n \nAfter we had individual tokens in the required format, we needed to convert them to \nnumbers (i.e., IDs), and each number represents a token of the data. Lastly, we converted \nthose numbers into tensors to be understood by the model. For that, we used a clas s in \nPyTorch called: \"as_tensor\" with its default values. \n \n3.2 Sentence-level grammatical error detection \nIn this task, the grammatical correctness of the sentence level will be checked. \n3.2.1 Datasets:  \nFirst, we used SCUT which is a publicly available dataset that was published to rich the GEC \ndomain, so it has two columns with the names: source (i.e., incorrect erroneous sentences) \nand targets (i.e., correct sentences). It is a dataset developed by (Solyman et al., 2021) to \novercome the lack of resources in the Ar abic language in terms of the GEC task. It was \ndeveloped in an unsupervised manner to generate a large-scale parallel synthetic corpus based \non the confusion function. The source of the data was the Al-Watan corpus which is an open-\naccess consisting of 10, 000,000 (ten million) words written in Modern Standard Arabic \n(MSA) by professional journalists. A portion of this data was taken (i.e., 18,061,610 million \nwords) and the confusion function has been used to randomly generated errors. The dataset \nfocuses mainly on the errors related to spelling errors and is categorized as follows:  missing \nwords, duplicate words, a missing character, and duplicate character.   \nSince there is not that much data related to the targeted task, we reformat the dataset to \nbe suitable for such a task. The reformatting was in two steps, the first one is adding a new \ncolumn named \"label\", then, classifying the target as correct (i.e., 1), and the source as \nincorrect (i.e., 0). And that was done on a portion of the main dataset. Then,  it was saved in \na new file and used while the finetuning process. Finally, the final dataset was split into two \ndatasets which are the training dataset to be used in the fine -tuning step and the test dataset \nfor testing the model performance. The splittin g was applied randomly with a percentage of \n90%-10% for train-validation datasets. Also, to generate the test dataset, following the same \nreformat approach done on another portion of the same main dataset.  \n \n3.2.2 Data Preprocessing \nThe data preprocessing was the same as the token-level experiment as we used the same pre-\ntrained model starting with the tokenization until converting the token into tensors. However, \nthe used tokenizer class here was \" BertTokenizer\". Again, we used the default values except \nthe keys in Table 5: \n  \n7\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\nTable 5. Tokenizer settings in sentence level. \nKey Value \nreturn_attention_mask true \npad_to_max_length true \nadd_special_tokens true \nMax length 64 \n \n4 Experiments \nThe current study's experiments mainly scoped on the BERT -based pre-trained models and \nused the hugging face library to pull the models from it. Since we target the Arabic language, \nwe had to filter the models to only the Arabic -supported ones which are no t that many. We \nconducted multiple experiments to evaluate the performance of a different set of available \nmodels. After the observation, we concluded with two main models to be focused on during \nthis study which are the M -BERT (Pires et al., 2020): multil ingual language model, and \nAraBERT (Antoun et al., 2020): monolingual language model with the same settings.  \nBidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) \nhad been introduced by Google to tackle the unidirectional problem of the standard language \nmodel by using a “masked language model” (MLM) objective which randomly masks tokens \nfrom the input to predict the original vocabulary id of the masked word based on its context \nonly. Moreover, it aims to master the next sentence prediction task. It was built based on the \ntransformer encoder architecture provided by (Vaswani, 2017). It showed its effectiveness in \ndifferent tasks of NLP such as question answering (Devlin et al., 2019), text classification \n(Garrido-Merchán & Gonz ález-Carvajal, 2020), sentiment classification (Gao et al., 2019), \nand text summarization (Elmadani et al., 2020).  \nM-BERT (Pires et al., 2020) is like the original English BERT model but instead of being \ntrained only on monolingual English data (Pires et al., 2020), it is trained on the Wikipedia \npages of 104 languages with a shared word piece vocabulary thus multilingual language \nmodel.  \nWhereas AraBERT (Antoun et al., 2020) is an extension of BERT but it is pre -trained in \nthe Arabic language only. It follows BERT architecture, and it aims to improve the state -of-\nthe-art Arabic NLP tasks. It was pre -trained on more than a billion Arabic words and had \nbeen applied to perform many NLP tasks in Arabic such as Sentiment Analysis, and Question \nAnswering, and showed new results that were not achieved before (Antoun et al., 2020). \nBoth models were fine -tuned with an initial learning rate of 1e -5 with a batch size of 8 \nand went over 4 training epochs. Whereas all the rest hyperparameters were kept at their \ndefault values. The fine-tuning process was done on the Google Colab with a GPU runtime \nusing Python programming language and PyTorch framework.  \n5 Results \nThe main finding of this study illustrates in Table 6:  \n  \n8\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\nTable 5. Tokenizer settings in sentence level. \nKey Value \nreturn_attention_mask true \npad_to_max_length true \nadd_special_tokens true \nMax length 64 \n \n4 Experiments \nThe current study's experiments mainly scoped on the BERT -based pre-trained models and \nused the hugging face library to pull the models from it. Since we target the Arabic language, \nwe had to filter the models to only the Arabic -supported ones which are no t that many. We \nconducted multiple experiments to evaluate the performance of a different set of available \nmodels. After the observation, we concluded with two main models to be focused on during \nthis study which are the M -BERT (Pires et al., 2020): multil ingual language model, and \nAraBERT (Antoun et al., 2020): monolingual language model with the same settings.  \nBidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) \nhad been introduced by Google to tackle the unidirectional problem of the standard language \nmodel by using a “masked language model” (MLM) objective which randomly masks tokens \nfrom the input to predict the original vocabulary id of the masked word based on its context \nonly. Moreover, it aims to master the next sentence prediction task. It was built based on the \ntransformer encoder architecture provided by (Vaswani, 2017). It showed its effectiveness in \ndifferent tasks of NLP such as question answering (Devlin et al., 2019), text classification \n(Garrido-Merchán & Gonz ález-Carvajal, 2020), sentiment classification (Gao et al., 2019), \nand text summarization (Elmadani et al., 2020).  \nM-BERT (Pires et al., 2020) is like the original English BERT model but instead of being \ntrained only on monolingual English data (Pires et al., 2020), it is trained on the Wikipedia \npages of 104 languages with a shared word piece vocabulary thus multilingual language \nmodel.  \nWhereas AraBERT (Antoun et al., 2020) is an extension of BERT but it is pre -trained in \nthe Arabic language only. It follows BERT architecture, and it aims to improve the state -of-\nthe-art Arabic NLP tasks. It was pre -trained on more than a billion Arabic words and had \nbeen applied to perform many NLP tasks in Arabic such as Sentiment Analysis, and Question \nAnswering, and showed new results that were not achieved before (Antoun et al., 2020). \nBoth models were fine -tuned with an initial learning rate of 1e -5 with a batch size of 8 \nand went over 4 training epochs. Whereas all the rest hyperparameters were kept at their \ndefault values. The fine-tuning process was done on the Google Colab with a GPU runtime \nusing Python programming language and PyTorch framework.  \n5 Results \nThe main finding of this study illustrates in Table 6:  \n  \n \nTable 6. Study Results. \n \nApproach \nModel \nAccuracy \nF1 \nF0.5 \nPrecision \nRecall \nToken Level \n(A) 0.86 0.87 0.85 0.83 0.90 \n(B) 0.85 0.85 0.83 0.81 0.91 \nSentence Level \n(A) 0.98 0.98 0.97 0.97 0.99 \n(B) 0.96 0.96 0.95 0.94 0.98 \n \nIn the GED on the token level classification, the model that was built on the monolingual \nlanguage model (i.e., model A), achieved a higher result than the multilingual language \nmodel (i.e., model B). As you see in Table 6, model A has F1 of 87% and accuracy of 86%. \nWhereas model B results in F1 of 85% and accuracy of 85% as well. It can be concluded that \nthe monolingual models can perform better in such tasks. \nMoreover, the conducted experiments on GED were also done on the sentences level \nclassification with the same approach performed on the token -level model that was built on \nthe monolingual language model (i.e., model A), achieved a higher result than the \nmultilingual language model (i.e., model B). As you see in the Table 6, model A has F1 of \n98% and accuracy of 98% whereas model B result in F1 of 96% and accuracy o f 96% as \nwell. \nDespite the relatively low -resource fine-tuning, the results suggest that the pre -trained \nmodels might tremendously be beneficial, especially in GED tasks. Thus, the results can \nsummarize that the popular approach (i.e., fine -tuning pre-trained language) is applicable in \nthe Arabic language generally, and grammar checking in particular. The charts in Fig 2 and \nFig 3 represent the accuracy results of both models: \n \n \n9\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\n \nFig. 2. Token Level Results. \n \nFig. 3. Sentence Level Results. \n \nThe following examples show the performance of our fine -tuned models whether it is \nmultilingual or monolingual and both on token-level and sentence level comparing them with \nthe original pre -trained model without any fine -tuning. Note that the golden column is  the \nreal value in the datasets: \n \n5.1 Model (A) Example \nTable 7 shows examples of the Token-Level and Sentence level outputs of the model (A). \n  \n10\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\n \nFig. 2. Token Level Results. \n \nFig. 3. Sentence Level Results. \n \nThe following examples show the performance of our fine -tuned models whether it is \nmultilingual or monolingual and both on token-level and sentence level comparing them with \nthe original pre -trained model without any fine -tuning. Note that the golden column is  the \nreal value in the datasets: \n \n5.1 Model (A) Example \nTable 7 shows examples of the Token-Level and Sentence level outputs of the model (A). \n  \nTable 7. Model (A) Output. \n \nFull Sentence هناك قضيا مشتركة بينهههههما \nInput Golden \n(correct) \nOutput \nArabert Without \nFinetuning \nFine-tuned \nArabert \nToken-level \nهناكCorrect Incorrect Correct \nقضيا Incorrect Correct Incorrect \nمشتركةCorrect Incorrect Correct \nبينهههههماIncorrect Correct Incorrect \nSentence-level هناك قضيا مشتركة\nبينهههههما  Incorrect Incorrect Incorrect \n \n5.2 Model (B) Example  \nTable 8 shows examples of the Token-Level and Sentence level outputs of the model (B). \n \nTable 8. Model (B) Output. \nFull Sentence هناك قضيا مشتركة بينهههههما \nInput Golden \n(correct) \nOutput \nMbert Without \nFinetuning \nFine-tuned \nMbert \nToken-level \nهناكCorrect Incorrect Correct \nقضيا Incorrect Incorrect Incorrect \nمشتركةCorrect Correct Correct \nبينهههههماIncorrect Incorrect Incorrect \nSentence-level هناك قضيا مشتركة\nبينهههههما  Incorrect Incorrect Incorrect \n \n \n6 Discussion \nAs Table 6 shows, the fine-tuned models produced very well results in terms of GED despite \nthe fact that the fine-tuning was done on relatively small data and super few training rounds. \nGenerally, this proves the effective ness of the pre -trained models in performing NLP tasks \nwhich GED is one of them and agrees with the literature.  \nThe models that were built on the monolingual pre -trained models resulted in better \nperformance than the ones trained in multiple languages. Th us, it can be concluded that the \nmore pre-trained models are dedicated to one language training, the better the results in that \nlanguage generally, and in Arabic in particular. However, the multilingual pre-trained models \n11\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\nare still effective, especially in low-resource languages, and do not have a monolingual pre -\ntrained language model yet. \nAgain, as you see in the above section, the experiments were conducted on two different \nmethods to detect grammar errors in Arabic which are on the token level and the s entence \nlevel. The results on classifying the sentences as correct or incorrect were higher in terms of \nstatics, it may be due to the fact that the model fine-tuned on a relatively large amount of data \nin comparison with the amount of the token level model . And the work to optimize these \ndifferences is left to the future. \nEven though (Al -qaraghuli, 2021) focused on Arabic grammatical correction, not \ndetection only, the current study's result agrees with it. It agrees with the promising \nperformance of the models that are built on the transformers in terms of NLP tasks, especially \nin Arabic. The (Al-qaraghuli, 2021) results in Arabic grammatical correction with accuracy \nof 99.77% in correcting the artificial grammar errors. However, the following approach in \nthe study was super time -consuming and required a lot of effort as well as costs since they \nstarted to build a transformer from scratch (i.e., vanilla transformers) then train it, and fine -\ntune it. Compared with our result which is relatively close to the ( Al-qaraghuli, 2021), we \ncan say that the possibility of utilizing the popular pre -trained models and fine -tuning it \ndirectly may result in the same result with much less time, effort, and even resources.    \nMadi & Al-Khalifa, 2020, conducted an error detection task in Arabic following a neural \nnetwork approach but with different architecture than the one used in the current study. It \nused SimpleRNN, LSTM, and BiLSTM-based models and trained them on the same dataset \nused in the current study. The best result  was the BiLSTM with precision of 82.97%, recall \nof 41.61%, and F0.5 of 69.21 %. In comparison with our result, our approach outperforms \nall the (Madi & Al -Khalifa, 2020) results with F0.5 of 0.85 in the token level and F0.5 of \n0.97 in the sentence level. This might be an indication that the GED is more suitable to be \nexecuted on pre -trained models that were built on the transformer's architecture -based \nmodels. It might be due to that fact the self-attention mechanism. \nFurthermore, agree with the current approach, but in other language settings like English, \nGerman, Czech, and Russian, (Katsumata & Komachi, 2020). Katsumata & Komachi, 2020 \nconducted a study following a similar approach to the current study. However, it employed \nanother pre-trained model both called BART and the mBART (i.e., the multilingual version \nof Bart), and the focus of the study was on the GEC at the end. The performance was \nremarkable with a minimum training cycle (i.e., fine-tuning). The best results achieved after \nthe fine-tuning on a publicly available dataset was F0.5 of 65.6% in terms of the English \nlanguage. \nThe noticeable difference between the token level, and the sentence level statics results, \nis due to the remarkable difference in the used data mount during the finetuning pro cess and \nthat agrees with (Katsumata & Komachi, 2020) in terms of the Russian language. As the \ntoken level dataset is not available in terms of the Arabic language. Also, agreeing with (Madi \n& Al -Khalifa, 2020), the imbalance issue that was described in th e methodology section \nmight lead to this as well, even after trying to tackle it. These can be considered tiny \nlimitations and call for improvements in future works as the token level detection can be \nmore accurate in terms of grammar-checking tasks. Moreover, the imbalance issue led to an \ninconsistency in the accuracy, F0.5, and other metrics results, with that, however, it had never \ngone below 80% across all metrics which is still high and considerable results.  \nThe results prove that the employment of transfer learning and the pre-trained models that \nwere built based on the transformers can improve the performance in terms of GED on many \nlevels such as the needed resources to fine -tune the models whether it is the data, or the \nhardware req uired to be used while conducting the experiments (Pajak, 2021). Thus, this \napproach can benefit the applications in low -resources languages and is a plus to the widely \n12\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\nare still effective, especially in low-resource languages, and do not have a monolingual pre -\ntrained language model yet. \nAgain, as you see in the above section, the experiments were conducted on two different \nmethods to detect grammar errors in Arabic which are on the token level and the s entence \nlevel. The results on classifying the sentences as correct or incorrect were higher in terms of \nstatics, it may be due to the fact that the model fine-tuned on a relatively large amount of data \nin comparison with the amount of the token level model . And the work to optimize these \ndifferences is left to the future. \nEven though (Al -qaraghuli, 2021) focused on Arabic grammatical correction, not \ndetection only, the current study's result agrees with it. It agrees with the promising \nperformance of the models that are built on the transformers in terms of NLP tasks, especially \nin Arabic. The (Al-qaraghuli, 2021) results in Arabic grammatical correction with accuracy \nof 99.77% in correcting the artificial grammar errors. However, the following approach in \nthe study was super time -consuming and required a lot of effort as well as costs since they \nstarted to build a transformer from scratch (i.e., vanilla transformers) then train it, and fine -\ntune it. Compared with our result which is relatively close to the ( Al-qaraghuli, 2021), we \ncan say that the possibility of utilizing the popular pre -trained models and fine -tuning it \ndirectly may result in the same result with much less time, effort, and even resources.    \nMadi & Al-Khalifa, 2020, conducted an error detection task in Arabic following a neural \nnetwork approach but with different architecture than the one used in the current study. It \nused SimpleRNN, LSTM, and BiLSTM-based models and trained them on the same dataset \nused in the current study. The best result  was the BiLSTM with precision of 82.97%, recall \nof 41.61%, and F0.5 of 69.21 %. In comparison with our result, our approach outperforms \nall the (Madi & Al -Khalifa, 2020) results with F0.5 of 0.85 in the token level and F0.5 of \n0.97 in the sentence level. This might be an indication that the GED is more suitable to be \nexecuted on pre -trained models that were built on the transformer's architecture -based \nmodels. It might be due to that fact the self-attention mechanism. \nFurthermore, agree with the current approach, but in other language settings like English, \nGerman, Czech, and Russian, (Katsumata & Komachi, 2020). Katsumata & Komachi, 2020 \nconducted a study following a similar approach to the current study. However, it employed \nanother pre-trained model both called BART and the mBART (i.e., the multilingual version \nof Bart), and the focus of the study was on the GEC at the end. The performance was \nremarkable with a minimum training cycle (i.e., fine-tuning). The best results achieved after \nthe fine-tuning on a publicly available dataset was F0.5 of 65.6% in terms of the English \nlanguage. \nThe noticeable difference between the token level, and the sentence level statics results, \nis due to the remarkable difference in the used data mount during the finetuning pro cess and \nthat agrees with (Katsumata & Komachi, 2020) in terms of the Russian language. As the \ntoken level dataset is not available in terms of the Arabic language. Also, agreeing with (Madi \n& Al -Khalifa, 2020), the imbalance issue that was described in th e methodology section \nmight lead to this as well, even after trying to tackle it. These can be considered tiny \nlimitations and call for improvements in future works as the token level detection can be \nmore accurate in terms of grammar-checking tasks. Moreover, the imbalance issue led to an \ninconsistency in the accuracy, F0.5, and other metrics results, with that, however, it had never \ngone below 80% across all metrics which is still high and considerable results.  \nThe results prove that the employment of transfer learning and the pre-trained models that \nwere built based on the transformers can improve the performance in terms of GED on many \nlevels such as the needed resources to fine -tune the models whether it is the data, or the \nhardware req uired to be used while conducting the experiments (Pajak, 2021). Thus, this \napproach can benefit the applications in low -resources languages and is a plus to the widely \nused languages. Moreover, it can be considered a fast approach which leads to saving mo re \ntime and increasing overall productivity.  \nWith all that said, a good and balanced dataset, despite it is size, is required. As we \nmentioned earlier, the difference in the result between the sentence -level and token -level \nexperiments is slightly high, even though the main difference in the application itself was in \nthe dataset. The one used in the sentence level was balanced (i.e., (i) class items equal (c) \nclass items) in both the training set and test set. On the other hand, at the token level, even \nthough we tried to make it balanced as much as possible when splitting the dataset for training \nand testing, the imbalance issue appeared again since the sentence tokens class (i.e., i and, c) \ndiffer from one to another. And that can be a call to future work. \n7 Conclusions \nThe conducted study investigated and concluded that fine -tuning the pre -trained language \nmodels (i.e., mBERT & AraBERT) can be applied successfully to perform GED tasks in \nArabic. Our experiments show a remarkable result in performing both aimed NLP tasks. The \nproposed models related to the token level classification task achieved F1 of 87% and \naccuracy of 86% in the monolingual language model, and F1 of 85% and accuracy of 85% \nin the multilingual language model. Moreover, the proposed models related to the sentence-\nlevel classification task achieved F1 of 98% and accuracy of 98% in the monolingual \nlanguage model, and F1 of 96% and accuracy of 96% in the multilingual language model. \nWith taking the effectiveness of the following method, one of the main tasks to be tested in \nthe recent future is the Arabic GEC. \nReferences \n1. Al-qaraghuli, M. (2021). Correcting Arabic Soft Spelling Mistakes Using \nTransformers. 146–151. \n2. Antoun, W., Baly, F., & Hajj, H. (2020). AraBERT: Transformer-based Model for \nArabic Language Understanding. ArXiv, May, 9–15. \n3. Chouikhi, H., & Alsuhaibani, M. (2022). Deep Transformer Language Models for \nArabic Text Summarization: A Comparison Study. Applied Sciences (Switzerland), \n12(23). https://doi.org/10.3390/app122311944 \n4. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of \ndeep bidirectional transformers for language understanding. NAACL HLT 2019 - 2019 \nConference of the North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies - Proceedings of the Conference, 1(Mlm), \n4171–4186. \n5. Ethnologue. Arabic language statistics, 2020. \n6. Karita, S., Wang, X., Watanabe, S., Yoshimura, T., Zhang, W., Chen, N., Hayashi, T., \nHori, T., Inaguma, H., Jiang, Z., Someki, M., Soplin, N. E. Y., & Yamamoto, R. \n(2019). A Comparative Study on Transformer vs RNN in Speech Applications. 2019 \nIEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019 - \nProceedings, 9(4), 449–456. https://doi.org/10.1109/ASRU46091.2019.9003750 \n7. Madi, N., & Al-Khalifa, H. (2020). Error detection for Arabic text using neural \nsequence labeling. Applied Sciences (Switzerland), 10(15), 1–14. \nhttps://doi.org/10.3390/APP10155279 \n13\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023\n8. Madi, N., & Al-Khalifa, H. S. (2018). A Proposed Arabic Grammatical Error Detection \nTool Based on Deep Learning. Procedia Computer Science, 142, 352–355. \nhttps://doi.org/10.1016/j.procs.2018.10.482 \n9. Montejo-Ráez, A., & Jiménez-Zafra, S. M. (2022). Current Approaches and \nApplications in Natural Language Processing. Applied Sciences (Switzerland), 12(10), \n10–15. https://doi.org/10.3390/app12104859 \n10. Parnow, K., Li, Z., & Zhao, H. (2020). Grammatical Error Correction : More Data with \nMore Context. 24–29. https://doi.org/10.1109/IALP51396.2020.9310498 \n11. Pires, T., Schlinger, E., & Garrette, D. (2020). How multilingual is multilingual \nBERT? ACL 2019 - 57th Annual Meeting of the Association for Computational \nLinguistics, Proceedings of the Conference, 4996–5001. \nhttps://doi.org/10.18653/v1/p19-1493 \n12. Qiu, Z., & Qu, Y. (2019). A Two-Stage Model for Chinese Grammatical Error \nCorrection. IEEE Access, 7, 146772–146777. \nhttps://doi.org/10.1109/ACCESS.2019.2940607 \n13. Sarhan, I., & Spruit, M. (2020). Can we survive without labelled data in NLP? Transfer \nlearning for open information extraction. Applied Sciences (Switzerland), 10(17). \nhttps://doi.org/10.3390/APP10175758 \n14. Shaalan, K. F. (2005). Arabic GramCheck : a grammar checker for Arabic. September \n2004, 643–665. https://doi.org/10.1002/spe.653 \n15. Singh, S., & Mahmood, A. (2021). The NLP Cookbook : Modern Recipes for \nTransformer Based Deep Learning Architectures. 68675–68702. \nhttps://doi.org/10.1109/ACCESS.2021.3077350 \n16. Solyman, A., Wang, Z., & Tao, Q. (2019). Proposed model for arabic grammar error \ncorrection based on convolutional neural network. Proceedings of the International \nConference on Computer, Control, Electrical, and Electronics Engineering 2019, \nICCCEEE 2019. https://doi.org/10.1109/ICCCEEE46830.2019.9071310 \n17. Solyman, A., Zhenyu, W., Qian, T., Abdulgader, A., Elhag, M., & Toseef, M. (2021). \nSynthetic data with neural machine translation for automatic correction in arabic \ngrammar. Egyptian Informatics Journal, 22(3), 303–315. \nhttps://doi.org/10.1016/j.eij.2020.12.001 \n18. Zaghouani, W., Mohit, B., Habash, N., Obeid, O., Tomeh, N., Rozovskaya, A., Farra, \nN., Alkuhlani, S., & Oflazer, K. (2014). Large scale Arabic error annotation: \nGuidelines and framework. Proceedings of the 9th International Conference on \nLanguage Resources and Evaluation, LREC 2014, 2362–2369. \nhttps://doi.org/10.1184/R1/6373136.v1 \n19. UNESCO. World Arabic language day, Dec 2019. \nhttps://www.unesco.org/ar/days/world-arabic-language \n20. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & \nPolosukhin, I. (2017). Attention is all you need. Advances in neural information \nprocessing systems, 30. \n \n14\nITM Web of Conferences 56, 04009 (2023) https://doi.org/10.1051/itmconf/20235604009\nICDSAC 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.773318886756897
    },
    {
      "name": "Transformer",
      "score": 0.7063182592391968
    },
    {
      "name": "Natural language processing",
      "score": 0.6640387773513794
    },
    {
      "name": "Sentence",
      "score": 0.6560445427894592
    },
    {
      "name": "Artificial intelligence",
      "score": 0.630698025226593
    },
    {
      "name": "Security token",
      "score": 0.6196297407150269
    },
    {
      "name": "Arabic",
      "score": 0.5833628177642822
    },
    {
      "name": "Language model",
      "score": 0.5731196999549866
    },
    {
      "name": "Recall",
      "score": 0.48244887590408325
    },
    {
      "name": "Speech recognition",
      "score": 0.3482242524623871
    },
    {
      "name": "Linguistics",
      "score": 0.24078544974327087
    },
    {
      "name": "Voltage",
      "score": 0.12968671321868896
    },
    {
      "name": "Engineering",
      "score": 0.06678837537765503
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}