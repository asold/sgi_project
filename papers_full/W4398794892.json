{
  "title": "Crowdsourcing with Enhanced Data Quality Assurance: An Efficient Approach to Mitigate Resource Scarcity Challenges in Training Large Language Models for Healthcare",
  "url": "https://openalex.org/W4398794892",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101366333",
      "name": "Prosanta Barai",
      "affiliations": [
        "University of Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A5109724212",
      "name": "Gondy Leroy",
      "affiliations": [
        "University of Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A5059751607",
      "name": "P. S. Bisht",
      "affiliations": [
        "University of Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A5111211829",
      "name": "Joshua Rothman",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A5101977405",
      "name": "Sumi Lee",
      "affiliations": [
        "University of Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A5109724213",
      "name": "Jennifer Andrews",
      "affiliations": [
        "University of Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A5088418286",
      "name": "Sydney Rice",
      "affiliations": [
        "University of Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A5109724214",
      "name": "Arif Ahmed",
      "affiliations": [
        "University of Arizona"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4367668587",
    "https://openalex.org/W3216317617",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4362522726",
    "https://openalex.org/W4295077729",
    "https://openalex.org/W2560647685"
  ],
  "abstract": "Large Language Models (LLMs) have demonstrated immense potential in artificial intelligence across various domains, including healthcare. However, their efficacy is hindered by the need for high-quality labeled data, which is often expensive and time-consuming to create, particularly in low-resource domains like healthcare. To address these challenges, we propose a crowdsourcing (CS) framework enriched with quality control measures at the pre-, real-time-, and post-data gathering stages. Our study evaluated the effectiveness of enhancing data quality through its impact on LLMs (Bio-BERT) for predicting autism-related symptoms. The results show that real-time quality control improves data quality by 19 percent compared to pre-quality control. Fine-tuning Bio-BERT using crowdsourced data generally increased recall compared to the Bio-BERT baseline but lowered precision. Our findings highlighted the potential of crowdsourcing and quality control in resource-constrained environments and offered insights into optimizing healthcare LLMs for informed decision-making and improved patient care.",
  "full_text": "arXiv:2405.13030v1  [cs.CL]  16 May 2024\nCrowdsourcing with Enhanced Data Quality Assurance: An\nEﬃcient Approach to Mitigate Resource Scarcity Challenges in\nT raining Large Language Models for Healthcare\nProsanta Barai, MS 1, Gondy Leroy , PhD 1, Prakash Bisht 1, Joshua M.\nRothman, MD 2, Sumi Lee, MS 1, Jennifer Andrews, PhD 1, Sydney A.\nRice, MD, MSc 1, Arif Ahmed, MS 1\n1The University of Arizona, T ucson 85721, U.S.A;\n2UC San Diego Division of Academic General Pediatrics, USA\nAbstract\nLarge Language Models (LLMs) have demonstrated immense pot ential in artiﬁcial intelligence across various\ndomains, including healthcare. However, their eﬃcacy is hi ndered by the need for high-quality labeled data,\nwhich is often expensive and time-consuming to create, part icularly in low-resource domains like healthcare.\nT o address these challenges, we propose a crowdsourcing (CS ) framework enriched with quality control\nmeasures at the pre-, real-time-, and post-data gathering s tages. Our study evaluated the eﬀectiveness of\nenhancing data quality through its impact on LLMs (Bio-BER T ) for predicting autism-related symptoms.\nThe results show that real-time quality control improves da ta quality by 19% compared to pre-quality control.\nFine-tuning Bio-BER T using crowdsourced data generally in creased recall compared to the Bio-BER T\nbaseline but lowered precision. Our ﬁndings highlighted th e potential of crowdsourcing and quality control\nin resource-constrained environments and oﬀered insights into optimizing healthcare LLMs for informed\ndecision-making and improved patient care.\nIntroduction\nWith recent advancements in artiﬁcial intelligence (AI) an d machine learning (ML), Large Language Models\n(LLMs) have emerged as powerful tools that can tap into unpre cedented capabilities in natural language\nunderstanding (identifying fake news, conversational res ponse generation) 1,2 and healthcare decision support\nsystems (medical question answering, summarizing electro nic health records, pathology text mining) 3–6 .\nHowever, the remarkable performance and eﬃciency of LLMs of ten face a challenge: the scarcity of resources\nrequired for training these models eﬀectively 7. Models like GPT-3 (175B parameters, 45TB training data) 8\nand BARD (540B parameters, 80B tokens) 9 rely on large corpora of text data. While they provide reason able\nout-of-the-box performance, they often require ﬁne-tunin g. The required data for training or tuning can be\ncost-prohibitive or practically impossible in ﬁelds such a s healthcare, where securing balanced labeled training\ndata remains a signiﬁcant hurdle 10,11. This results in challenges like class imbalance, spurious correlation,\nand data bias. Moreover, experts warn that we may soon run out of new text for training due to the low\nprojected availability of unlabeled text data and the memor izing eﬀect of LLM 12. However, healthcare\npractitioners require highly sophisticated and well-trai ned models, underscoring the urgency of addressing\nthis challenge 3.\nAlgorithmic Solutions for ML in Low Resource Domains\nT o address the resource limitations, researchers across va rious domains have proposed various solutions. F or\nexample, transfer learning (TL) 13 , as introduced by Bozinovski and F ulgosi (1976) 14, involves leveraging the\nknowledge of a pretrained model from a richer domain to boost the performance of a further downstream\ntask. Even though TL has been useful in several low-resource domain scenarios 15,16, challenges like domain\nmismatch 17, catastrophic forgetting 18, and negative transfer 19 persist. If we ﬁnd a reasonable match\nbetween the source and target domain, the computational and data resources necessary to tune billions of\nparameters are challenging, making it an undesirable choic e to adapt 8. Another alternative well-established\ntechnique is resampling 20, as documented in Estabrooks’s work from 2004. This method, characterized\nas an external approach, allows for the oversampling of mino rity classes or the undersampling of majority\nclasses 21. However, it comes with several associated challenges, inc luding reliance on classiﬁcation type 22\nand the potential for information loss attributable to data redundancy 23. Another noteworthy approach\nto the low resource domain problem is data augmentation, whi ch includes synthetic data augmentation.\n(SDA)24 and crowdsourcing 25. In SDA, data can be augmented using either data warping 26 (transformations\napplied in the data space) or a pretrained generative model 24 . Literature shows improvement in model\nperformance where the amount of real data bounds improvemen t 26. Additionally , issues such as target\ndomain representation 27 and self-consuming generative process 28 continue to challenge the eﬀectiveness of\nSDA.\nCrowdsourcing Solutions for ML in Low Resource Domains\nWith crowdsourcing, humans, usually laypersons and not dom ain experts, are recruited to generate data.\nDomain relevance (alignment of a crowd worker’s task with a ﬁ eld of interest) and creating diverse and\nrealistic data are the primary strengths of crowdsourcing 29,30. LLMs like GPT and BARD are trained on\nconventional text data from web and book corpora with no know n established mechanism to validate the\ninformation. This may lead to errors or misinformation, whi ch should be avoided, especially in healthcare.\nUsing crowdsourced data, researchers can more easily take c are of the required quality control. Additionally ,\nadaptability in crowdsourcing enables iterative data coll ection based on model performance tailored to the\nresearcher’s need. F or example, in the case of training a pre dictive healthcare language model, if one label\nperforms poorly because of inferior data quality in the ﬁrst iteration, the researcher can iteratively collect\nadditional data to improve that speciﬁc label’s performanc e.\nHowever, crowdsourcing requires data quality control, whi ch is often overlooked. F or example, Geiger enlisted\nfour types of crowd work: crowd creation (crowd creates cont ent), solving (crowd seeks solution to a speciﬁc\nproblem), rating (crowd creates Ratings and reviews), and p rocessing (collective eﬀort of \"the crowd\" in\ncompleting tasks) 31. In the case of crowd creation, which involves content gener ation, like responding to\nspeciﬁc questions or writing reviews and blog posts, the eva luation criteria are often vague and poorly\ndeﬁned 31 , resulting in subpar data quality . Also, Allahbakhsh menti oned quality control in crowdsourcing\nas a critical issue 32 in their work. This quality control challenge often results in inferior quality data, creating\nthe loop of the \"garbage in, garbage out\" issue, as described by the father of modern computing, Charles\nBabbage 33. Hence, there is an increasing focus on the data-centric app roach in modern artiﬁcial intelligence\n(AI) research, where data is methodically engineered as an e ssential step in creating reliable AI systems 34.\nF ollowing this data-centric approach, several researcher s suggested various cures for low quality in\ncrowdsourced data, broadly classiﬁed as pre-quality contr ol schema (Pre QC) and post-quality control\nschema (Post QC). F or instance, Allahbakhsh laid out the Pre QC schema among two essential dimensions:\nworker proﬁle and task design 32. It mainly focuses on creating a suitable worker pool couple d with detailed\ntask design at a granular level 32,35. Reputation and credential (e.g., education, approval rat e) based worker\nselection and defensive task design are essential parts of a Pre QC schema. In their work, Enrique showed\nthat using adequate incentives (also a part of Pre QC) as an ex trinsic motivator can make crowdsourcing\nsuccessful 36. However, Malone argued that adequate incentives could be s olely due to monetary incentives\nharming the crowdsourcing eﬀort 37. Additionally , most of this Pre QC schema implementation is a\nhard-wired quality control technique typically embedded i n the host website, and researchers cannot often\ncustomize it based on their speciﬁc requirements 32.\nIn the Post QC schema, the QC mechanism is applied once the dat a has been collected. Here, enhancement\ncan be achieved through post-processing by implementing ri gorous attention checks, setting quality thresholds\nfor text data, and some generic text cleanup 35. Besides that, expert review (domain expert assesses the da ta\nquality), majority consensus, contributor evaluation (ev aluates a contribution based on the contributor’s\nquality) 32, and workﬂow management 38,39 are a few other ways Post QC can be implemented. However, it\nis acknowledged that Post QC can be resource-intensive, con suming valuable time and ﬁnancial resources,\nthereby creating a signiﬁcant gap in the quality control pro cess.\nT o address this challenge, we developed a crowdsourcing (CS ) framework integrating real-time quality control\n(real-time QC), acting as a discreet background ﬁlter that c omplements Pre QC eﬀorts. Our framework\nstrategically addresses the inherent data quality and inte grity challenges associated with crowdsourced data,\nthus making it a more reliable and eﬃcient solution. Through experimentation, expert, statistical, and ML\nevaluation, we demonstrate that crowdsourced data, as impr oved through our data quality control, can help\nincrease the performance of LLMs, even when faced with limit ed resources.\nMaterials and Methods\nW e collect text data to ﬁne-tune a transformer-based langua ge model (Bio-BER T 40) to predict signs and\nsymptoms of autism from text data. W e compare three stages fo r crowdsourcing quality control: Pre Quality\nControl (Pre QC), Real-time QC, and Post Quality Control (Po st QC). W e work with Amazon Mechanical\nT urk (AMT) and Qualtrics surveys.\nPre QC\nDuring the pre-QC phase, we use a qualiﬁcation task that requ ires AMT workers who reside in the United\nStates, with a minimum work approval rate of 98%. These AMT mi nimum requirements are a common\napproach and not domain speciﬁc. The qualiﬁcation task focu ses on creating additional ﬁlters for the workers.\nWithin the qualiﬁcation task, we asked for demographic info rmation and inquired about any personal or\nfamilial connections with autism or autism-related concer ns, i.e., we posed 29 questions related to signs and\nsymptoms of autism. While A TM oﬀers a wide array of default qu aliﬁcations, we customize qualiﬁcations\nto align with our domain speciﬁc objectives using the respon ses on the qualiﬁcation task: AMT masters (\nMM) qualiﬁcation for those with AMT a master’s degree, healt hcare (HC) qualiﬁcation for workers who are\nin the healthcare profession, graduate degree (GD) qualiﬁc ation for people with a minimum US graduate\ndegree, and education (ED) for participants who are educati on professionals.\nReal-time QC\nThe real-time Quality Control (publicly available on GitHu b 1) phase is executed simultaneously with the\nsubmission of responses by workers. Its primary objective i s to identify and mitigate copy-and-paste answers.\nW orkers generate responses once the Human Intelligence T as k (HIT) is live on AMT. Then, we detect\ncoherent text and nonsensical gibberish by HTML’s built-in spellcheck feature. F or example, if a respondent\ntyped nonsensical words during the Google search for that sp eciﬁc response, the search engine will return a\nblank page and a null value for the retrieved text vector. Usi ng that information, we trigger an alert message\nto re-enter the response.\nWhen coherent text is identiﬁed, we proceed to initiate an XH R API call, leveraging the Google Custom\nSearch Engine API. F ollowing the API call, the framework ext racts and retrieves the surface text from the\nGoogle search results. Subsequently , it computes n-gram re presentations for the input text and the retrieved\ncontent. If at least one congruent n-gram is detected during this process, the response is assumed to be copied\nand fails the automated evaluation. F or example, say text 1 ( A dietary restriction implies restrictions on\nspeciﬁc foods that an individual cannot or will not consume .) is the response text, and text 2 ( A dietary\nrestriction means restrictions on certain foods that a person will not consume .) is the retrieved text,\nthen this will be recognized as copied response due to 2 bolde d 3-gram matches. W e provide an alert message,\nprompting the user to re-enter their response. This evaluat ion process is executed behind the scenes with\nlittle to no obtrusion to the worker. Our framework can also k eep the metadata on the number of attempts\nbefore a successful submission for any necessary post-hoc a nalysis.\n1 https://github.com/porosantabarai/Crowdsourcing-Real-time-QC.git\nPost QC\nAfter data collection, we carry out the Post QC of the data. Ou r primary goal in this phase is to detect\nand eliminate low-quality responses. One or more strategie s can be used, such as deﬁning strict performance\ncutoﬀ criteria on attention and quality check questions, ev aluating response completion times, and using\nhuman evaluations. F or instance, a respondent who repeats a n authentic response, approved by Real-time\nQC, for multiple questions unrelated to the speciﬁc symptom the question is asking can be eliminated in this\nphase of QC. W e used multiple expert assessments to evaluate the applicability and quality of the collected\ndata.\nStudy Design\nW e compare the three types of quality control using four sequ ential studies. First, Experiment 1, or the\n\"Pre QC\" study , investigates the inﬂuence of pre-QC measure s on the data. Then, the subsequent three\nexperiments incorporate real-time QC features using the be st quality control setting from Experiment 1.\nExperiment 2, denoted as \"Paste,\" introduces a copy-paste r estriction for respondents, preventing them from\ncopying and pasting responses. Experiment 3, \"GS,\" integra tes custom Google search features (as outlined\nin the Real-time QC section). Finally , Experiment 4, \"Paste +GS,\" combines the features from experiments\ntwo and three. W e also collected baseline data (No QC) withou t custom qualiﬁcation or CS implementation.\nProcedure\nW e collected all the data, including four experimental cond itions and No QC on the AMT platform. In\nthe Pre QC experiment, we ﬁrst posted the qualiﬁcation task, which collected data regarding respondent\ndemographics and their connections to individuals diagnos ed with or displaying symptoms of autism.\nThen, a HIT comprising one of the 29 questions related to the s igns and symptoms of autism was made\navailable. F or instance, \"Describe an interaction with you or others where this person uses sounds that\nyou don’t expect in a typical interaction\" is an example ques tion related to the A1 symptoms prepared by\npediatricians. After completing our qualiﬁcation tasks, A MT workers could complete up to 29 HIT s (all\nquestions). In all experimental conditions, we requested t en responses per question on AMT. W orkers were\ncompensated with $0.10 for completing the voluntary qualiﬁ cation task and $0.40 for each question answered.\nThere was an average delay of 12 days between the experiments .\nEvaluation\nW e conduct our evaluation in the context of classifying text as ASD criteria. The American Psychiatric\nAssociation’s Diagnostic and Statistical Manual (DSM) out lines speciﬁc diagnostic criteria for autism\nspectrum disorder (ASD), consisting of 12 criteria classiﬁ ed into domains A, B, C, D, and E. Domain A\nfocuses on \"Persistent Deﬁcits in Social Communication and Social Interaction\" (A1-A3), while Domain B\naddresses \"Restricted, repetitive patterns of behavior, i nterests, or activities\" (B1-B4). T o receive an ASD\ndiagnosis per DSM-5 guidelines, an individual must meet thr ee criteria from Domain A and at least two\ncriteria from Domain B. W e used Bio-BER T 40 to label sentences in EHR with A1-A3 and B1-B4 labels.\nOur methodology involved retraining this model (Bio-BER T) on a dataset of 34,313 sentences extracted\nfrom 150 CDC-trained clinician reports. These reports were generated as part of the Autism Developmental\nDisabilities Monitoring (ADDM) surveillance by the Center s for Disease Control (CDC). Among 34,313\nsentences, only 3,570 were labeled. W e use 10-fold cross-va lidation. W e report precision and recall for the\nbaseline model Bio-BER T (no data augmentation) and Experim ents 1 to 4. Our goal is to augment this\ndataset with the newly collected data.\nBasic Quality Evaluation T wo independent evaluators blindly assessed the overall da ta quality of the four\nexperimental datasets. They categorized responses as \"Ove rall Good\" or \"Overall Bad.\" Responses were\nmarked as \"Overall Good\" if they were relevant and/or contai ned example behavior, while responses were\nmarked as \"Overall Bad\" if irrelevant, clearly copied from a nother source, incoherent, or if the example\nbehavior pertained to a diﬀerent question. F or example, thi s response (\"The autistic person that I know\ngroans a lot in conversations, even when the topic is lighthe arted and does not warrant any annoyance or\nanything that should normally elicit a groan.\") was given to a question related to A1 but actually it aligns\nwith DSM criteria B1 hence marked as \"Overall Bad. All the rat ings were binary (1 or 0). Using the rating of\nthe response from two evaluators, we calculated Cohen’s kap pa (quantitative measure of agreement between\ntwo raters) for the “Overall Good” metric. W e also perform a o ne-way ANOV A test among the ratings from\ntwo evaluators to check if there is any signiﬁcant diﬀerence between the two evaluators.\nDomain Expert Evaluation A domain expert evaluated a random subset of the 175 response s from the Post\nQC data set. All examples were in random order. Five categori es were used for this evaluation: Typical, Not\nTypical, Normal, EHR Match, and Exact Match, which assess re sponse closeness to autism behavior. The\nﬁrst, Typical, assesses how well the behavior aligns with ty pical characteristics; not Typical asses atypical\nbehavior but not commonly seen in autism, and Normal assesse s the regular behavior; EHR Match assesses\nif the response behavior is typically encountered in HER. Fi nally , Exact Match assesses if the provided\nresponse matches exactly with the actual labels. Expert eva luated the ﬁrst four categories on a ﬁve-point\nLikert scale from strong relevance (5) to o relevance (1) for the respective category and “Exact Match” as\nyes or no.\nResults\nCrowdsourced Dataset\nA total of 1200 workers completed our qualiﬁcation tasks, an d upon inspection, 982 were assigned four\ndistinct qualiﬁcations (MM, HC, GD, and ED): 264 MM workers, 307 HC workers, 318 GD workers, and 93\nED workers. Subsequently , we published the HIT s containing the 29 questions exclusively for the selected four\ncohorts (MM, HC, GD, and ED) to observe the eﬀect of custom qua liﬁcation on data quality . W e requested\nten random responses per question (implemented by AMT), res ulting in 29*10*4 = 1,160 requested responses.\nW e selected the highest-performing qualiﬁcation group fro m the Pre QC experiments for the subsequent\nthree experiments. The GD group was the group with the highes t quality data (see T able 3). F ollowing\nHIT s were made accessible solely to this pool of 318 GD worker s. T able 1 shows training data size in diﬀerent\nexperimental conditions by each DSM criteria. Our Post QC da taset encompasses 866 evaluated responses\nfrom the four experimental datasets. Additionally , we accu mulated a baseline dataset of 1551 sentences for\nthe same set of questions, labeled \"NO QC,\" where no qualiﬁca tion controls were implemented, serving as a\nvaluable point of comparison.\nT able 1. Number of training sentences by diﬀerent DSM criteria for al l experimental conditions.\nDSM Criterion Bio-BER T T raining No QC Pre QC\nExperiment-1 Real-time QC Post QC\nExperiment-2: Paste Experiment-3: GS Experiment-4: Paste +GS\nA1 855 200 192 50 50 50 124\nA2 471 216 129 30 30 30 83\nA3 524 204 160 40 40 40 91\nB1 539 228 160 40 40 40 144\nB2 338 203 160 40 40 40 92\nB3 146 215 119 30 30 30 124\nB4 697 284 240 60 60 60 208\nT otal 3570 1550 1160 290 290 290 866\nT able 2. Participants’ characteristics on four and no experimental conditions [% (count)].\nV ariable Choice No QC (n=160) 2\nPre QC\nExperiment-1\n(n=168)\nReal-time QC\nExperiment-2: Paste\n(n=34)\nExperiment-3: GS\n(n=54)\nExperiment-4: Paste+GS\n(n=26)\nSex F emale 38.1 (61) 37.5 (63) 47.1 (16) 37.0 (20) 53.9 (14)\nMale 61.9 (99) 62.5 (105) 52.9 (18) 63.0 (34) 46.2 (12)\nRace\nAmerican Indian or Alaska Native 1.9 (3) 0.6 (1) 0.0 (0) 0.0 (0 ) 0.0 (0)\nAsian 13.1 (21) 4.2 (7) 5.9 (2) 3.7 (2) 7.7 (2)\nBlack or African-American 5 (8) 1.8 (3) 2.9 (1) 1.9 (1) 11.5 (3 )\nWhite 80 (128) 91.7 (154) 91.2 (31) 94.4 (51) 76.9 (20)\nEthnicity Hispanic or Latino 15 (24) 18.5 (31) 5.9 (2) 3.7 (2) 3.9 (1)\nNot Hispanic or Latino 85 (136) 81.5 (137) 94.1 (32) 96.3 (52) 96.1 (25)\nEducation\nLess than high school degree NA 0.0 (0) 0.0 (0) 0.0 (0) 0.0 (0)\nHigh school diploma NA 3 (5) 0.0 (0) 3.7 (2) 11.5 (3)\nAssociate degree NA 1.2 (2) 0.0 (0) 1.9 (1) 11.5 (3)\nBachelor’s degree NA 53.6 (90) 38.2 (13) 57.4 (31) 46.2 (12)\nMaster’s degree NA 41.7 (70) 61.8 (21) 37.0 (20) 30.8 (8)\nDoctoral degree (PhD, MD,...) NA 0.6 (1) 0.6 (1) 0.0 (0) 0.0 (0 )\nAge\nMean 39.8 38.8 46.9 38.0 42.5\nSD 13.7 12.1 13.1 11.5 13.4\nRange 22-70 21-70 27-70 25-70 23-70\nParticipants and Demographics\nThe demographic characteristics across the ﬁve experiment al conditions reveal several variations (see T able\n2). F emales were more predominant in Experiment 4, \"Paste+G S\" (54%), compared to the other conditions,\nwhile males were more prominent across the other experiment al conditions, with the highest being 63% in\nExperiment 3. When considering race, White individuals dom inated all conditions, especially in Experiment 1\n(91.7%) and Experiment 4 (76.9%). Regarding ethnicity , non -Hispanic individuals were the majority across\nall experimental conditions, with higher percentages in Ex periment 3 (96.3%) and 4 (96.1%). Bachelor’s\ndegree was the most prominent in all conditions but notably l ower in Experiment 2 (38.2%). Lastly , the age\ncharacteristics displayed minor variations among experim ents, with the mean age ranging from 38 to 46.9\nyears, but no signiﬁcant distinctions.\nBasic Quality Evaluation\nAgreement between two evaluators was high (see T able 4), so w e merged ratings and reported the average\nvalue. Highlighted rows from T able 3 show the quality of Pre Q C data for four qualiﬁcations (MM, GD,\nHC, and ED). The GD cohort’s data showed the best data quality (68%), while the ED cohort showed the\nhighest percentage of data that evaluators classiﬁed as \"Ov erall Bad\" (70%). In the HC group, evaluators\nmarked 20% as bad. A substantial amount of the data remained u nusable even with pre-quality control in\nplace, highlighting the possible need for Real-time QC.\nT able 3. A verage Percentage V alues of Measures Overall Good, Overal l Bad, and Duplicate Across F our\nExperimental Conditions\nExperiment Description Qualiﬁcation T otal Count (N) Overa ll Good (%) Overall Bad (%) Duplicate (%)\nED 290 29.65 70.35 53.75\nGD 290 68.30 31.70 52.20\nHC 290 34.65 65.35 18.901 Pre QC\nMM 290 45.00 55.00 36.40\n2 Paste GD 290 36.55 63.45\n3 GS GD 290 53.65 46.35\n4 Paste+ GS GD 290 63.10 37.05\n2 Education information is not available for No QC data becaus e this property was collected as a part of Pre QC experiment.\nCompared to only Pre QC data, we notice a consistent improvem ent in the data quality (approximately 20%).\nData collected with experimental conditions 2 and 3 combine d achieved the highest quality (63%). Comparing\nexperiment 2 (Paste) with experiment 3 (GS), we see a 12% to 22 % improvement in overall data quality .\nOverall, the \"Combined\" condition, which merged \"Paste Res triction\" and \"Google Search,\" demonstrated\nboth improved data quality ratings, underscoring the poten tial beneﬁts of implementing Real-time QC in\ncrowdsourcing.\nT able 4. Cohen’s Kappa and one-way ANOV A results for Overall Good acr oss four experimental conditions.\nExperiment Description Cohen’s Kappa F-V alue/p-value\n1 Pre QC 0.95 1.57/ 0.210\n2 Paste 0.87 2.41/ 0.121\n3 GS 0.94 0.56/ 0.454\n4 Paste+GS 0.95 0.27/ 0.606\nDomain Expert Evaluation\nT able 5 shows the expert evaluation result of 175 randomly se lected responses from Experiment 4 (Paste=GS)\ndata. The result shows that 55% (96) responses were intellig ible, and 45% (79) were unintelligible. Among 96\nintelligible responses, the expert found that 75% (72) resp onses exactly matched relevant diagnostic criteria.\nThe rest of the 12 responses described multiple behaviors or other behaviors compared to the question asked.\nAmong intelligible responses, we see a high agreement (aver age Likert scale rating of 4.4) of the expert on\ncrowdsourced responses’ alignment with clinical EHR repor ts, indicating better data quality . Agreement on\ncrowdsourced data describing normal behavior rather than a utism is low, with an average Likert scale rating\nof 2.12. A higher Likert scale rating on Typical and EHR Match indicates better quality crowdsourced data.\nT able 5. Expert evaluation of a random subset of the Experiment-4 dat a.\nDSM Criterion Counts Unintelligible Intelligible Exact Ma tch Typical Normal Not Typical EHR Match\nA1 25 9 16 9 4.31 1.94 1.47 4.19\nA2 25 13 12 7 4.33 1.17 1.67 5.00\nA3 25 13 12 10 4.33 2.08 1.00 4.08\nB1 25 8 17 10 4.29 1.59 1.88 4.56\nB2 25 10 15 14 4.20 2.00 1.00 4.13\nB3 25 13 12 11 4.33 1.67 1.75 4.42\nB4 25 13 12 11 1.33 4.42 1.33 4.42\nT otal 175 79 96 72 NA NA NA NA\nA verage NA NA NA NA 3.88 2.12 1.44 4.40\nLLM Evaluation\nF or the LLM evaluation, we used data from four experiments an d tested the impact of adding this to the\nbaseline Bio-BER T data. T able 6 shows precision and recall. The table shows that transitioning from baseline\n(Bio-BER T) to \"No-QC\" reduced the model performance for pre cision but increased recall: an average 23%\ndrop in precision and an average 13% increase in recall.\nThere are notable diﬀerences between the diﬀerent types of q uality control. Moving from \"No QC\" to\nExperiment 1(\"Pre QC\") generally results in improved preci sion across most labels, ranging from 3% to 21%\nimprovement. On the other hand, the model’s recall value has dropped by as much as 10%, indicating a\nhigher chance of false negatives. Within the \"Real-time QC\" phase, diﬀerent experiments (\"Paste,\" \"GS,\"\nand \"Paste+GS\") exhibit improved precision in most cases an d, in general, similar recall values. F or models\nwith little baseline data, such as (B2 and B3), we see a gradua l improvement in precision, with the highest\n(B2: 0.6471, B3: 0.6316) being in Experiment 4 data. The \"Pos t QC\" phase generally maintains precision\nat levels slightly better than \"Pre QC.\" However, compared t o Real-time QC, the performance gain of Post\nQC in terms of precision is 2% to 4%, with some labels having sl ightly inferior performance. Regarding the\nrecall, most of the recall values decreased, ranging from a 3 % to 12% reduction. Therefore, we can conclude\nthat with proper real-time QC implemented, adding another l ayer of manual labor (in post-QC) did not help\nimprove model performance a lot.\nT able 6. Precision and recall measure of the Bio-BER T model on diﬀere nt experimental data.\nReal-time QC\nDSM Criterion Bio-Bert\n(Baseline) No QC Pre QC\nExperiment-1 Experiment-2:\nPaste\nExperiment-3:\nGS\nExperiment-4:\nPaste+GS\nPost QC\nPrecision Recall Precision Recall Precision Recall Precision Recall Precision Recall Precision Recall Precision Recall\nA1 0.7338 0.3269 0.5363 0.452 0.4758 0.5994 0.6218 0.4584 0.6234 0.4456 0.6267 0.452 0.5654 0.5129\nA2 0.83 0.5929 0.5856 0.6929 0.785 0.6 0.7579 0.6929 0.7742 0.6858 0.7616 0.7072 0.804 0.5858\nA3 0.7379 0.5 0.4046 0.6823 0.5107 0.6636 0.572 0.687 0.5868 0.6636 0.5917 0.6636 0.5488 0.6309\nB1 0.6438 0.6732 0.4793 0.7648 0.5591 0.6798 0.5498 0.7582 0.565 0.7386 0.5693 0.7255 0.6122 0.6602\nB2 0.7615 0.5646 0.4661 0.7279 0.5898 0.7143 0.6491 0.6667 0.6173 0.6803 0.6471 0.6735 0.6716 0.6259\nB3 0.6512 0.35 0.44 0.5125 0.625 0.5 0.6316 0.45 0.6207 0.45 0.6316 0.45 0.5257 0.5125\nB4 0.6425 0.6455 0.45 0.7591 0.5221 0.75 0.5257 0.791 0.5134 0.7864 0.4958 0.8046 0.5289 0.75\nA verage 0.7144 0.5219 0.4803 0.6559 0.5811 0.6439 0.6154 0.6435 0.6144 0.6358 0.6177 0.6395 0.6081 0.6112\nReplication Evaluation of the Proposed Mechanism\nA comprehensive evaluation study was conducted to evaluate the eﬀectiveness of the mechanisms designed\nto detect copied and Google-searched (GS) responses. Using the same set of 29 questions employed for\ndata collection, new responses were collected and categori zed into three groups: \"Authentic,\" obtained\nfrom parental data collected through Qualtrics and veriﬁed by the human expert; \"Copied,\" generated by\nperforming a Google search for each question; and \"Paraphra sed,\" derived from the Copied responses using\nthe ChatGPT paraphrasing mechanism. Subsequently , all res ponses were subjected to our detection tool to\nassess its robustness. T able 7 shows that the tool detected a ll 29 Authentic and 29 Copied responses. However,\nin the case of Paraphrased responses, the tool detected 11 ou t of 29, leaving 18 undetected instances.\nT able 7. Robustness assessment of the proposed mechanism.\nAuthentic Copied Paraphrased\nDetected 29 29 11\nUndetected 0 0 18\nT otal 29 29 29\nDiscussion and Conclusion\nW e provided a comprehensive QC outline for improving the per formance of LLMs in healthcare through the\nstrategic use of crowdsourced data and quality control meas ures. W e observed signiﬁcantly improved recall\nin models with crowdsourced data ﬁltered by data control com pared to the Bio-BER T baseline model trained\non EHR data, but precision dropped. However, improved preci sion and minor improvement in recall were\nobserved in four experiments compared to no QC-implemented model. Experimenting with a larger data set\nis needed to investigate this issue further. W e showed that o nly Pre QC is insuﬃcient, and Post QC might\nbe redundant and resource-intensive while Real-time QC is i n place. In this study , we experimented with\ndata quality and how LLM’s performance can be improved with c rowdsourcing. In forthcoming studies, we\nplan to juxtapose synthetic data augmentation and resampli ng techniques while also addressing the current\nlimitation in paraphrased response detection by integrati ng a semantic understanding mechanism between\nresponses.\nAcknowledgements\nThis project was supported in part by grant number R01MH1249 35 from the National Institute of Mental\nHealth. Part of the data presented was collected by the Cente rs for Disease Control (CDC) and Prevention\nAutism and Developmental Disabilities Monitoring (ADDM) N etwork supported by CDC Cooperative\nAgreement Number 5UR3/DD000680 and by the University of Ari zona FY23 BIO5 Rapid Grant.\nReferences\n1. Zellers R, Holtzman A, Rashkin H, Bisk Y, F arhadi A, Roesne r F, et al. Defending against neural fake\nnews. Advances in neural information processing systems. 2 019;32.\n2. Zhang Y, Sun S, Galley M, Chen YC, Brockett C, Gao X, et al. Di alogpt: Large-scale generative\npre-training for conversational response generation. arX iv preprint arXiv:191100536. 2019.\n3. Thirunavukarasu AJ, Ting DSJ, Elangovan Kea. Large langu age models in medicine. Nat Med.\n2023;29:1930-40.\n4. Santos T, T ariq A, Das S, V ayalpati K, Smith GH, T rivedi H, e t al. PathologyBER T – pre-trained\nvs. a new transformer language model for pathology domain. a rXiv. 2022. A vailable from:\nhttps://arxiv.org/abs/2205.06885.\n5. Singhal K, T u T, Gottweis J, Sayres R, W ulczyn E, Hou L, et al . T owards expert-level medical question\nanswering with large language models. arXiv preprint arXiv :230509617. 2023.\n6. Adams LC, T ruhn D, Busch F, Kader A, Niehues SM, Makowski MR , et al. Leveraging GPT-4 for post\nhoc transformation of free-text radiology reports into str uctured reporting: a multilingual feasibility\nstudy . Radiology . 2023;307(4):e230725.\n7. Masud MM, et al. F acing the reality of data stream classiﬁc ation: coping with scarcity of labeled data.\nKnowledge and Information Systems. 2012;33:213-44.\n8. Brown T, et al. Language models are few-shot learners. In: Advances in neural information processing\nsystems 33; 2020. .\n9. Chowdhery A, et al. PaLM: Scaling language modeling with P athways. arXiv preprint arXiv:220402311.\n2022. A vailable from: https://arxiv.org/abs/2204.02311 .\n10. A ydin F, et al. Medical multimodal classiﬁers under scar ce data condition. arXiv preprint\narXiv:190208888. 2019.\n11. Ondov B, Attal K, Demner-F ushman D. A survey of automated methods for biomedical text\nsimpliﬁcation. Journal of the American Medical Informatic s Association. 2022;29(11):1976-88.\n12. Villalobos P , Sevilla J, Heim L, Besiroglu T, Hobbhahn M, Ho A. Will we run out of data? An analysis\nof the limits of scaling datasets in Machine Learning. arXiv preprint arXiv:221104325. 2022.\n13. Pan SJ, Y ang Q. A Survey on T ransfer Learning. IEEE T ransa ctions on Knowledge and Data Engineering.\n2010;22(10):1345-59.\n14. Bozinovski S, F ulgosi A. The inﬂuence of pattern similar ity and transfer learning upon training of a\nbase perceptron b2. In: Proceedings of Symposium Informati ca. vol. 3; 1976. .\n15. Peng Y, Y an S, Lu Z. T ransfer learning in biomedical natur al language processing: an evaluation of\nBER T and ELMo on ten benchmarking datasets. arXiv preprint a rXiv:190605474. 2019.\n16. Abad A, et al. Cross lingual transfer learning for zero-r esource domain adaptation. In: ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and Sig nal Processing (ICASSP); 2020. .\n17. Niu S, et al. A decade survey of transfer learning (2010–2 020). IEEE T ransactions on Artiﬁcial\nIntelligence. 2020;1(2):151-66.\n18. Kirkpatrick J, et al. Overcoming catastrophic forgetti ng in neural networks. Proceedings of the National\nAcademy of Sciences. 2017;114(13):3521-6.\n19. Rosenstein MT, Marx Z, Kaelbling LP . T o T ransfer or Not to T ransfer. In: Proc. Conf. Neural\nInformation Processing Systems (NIPS ’05) W orkshop Induct ive T ransfer: 10 Y ears Later; 2005. .\n20. Estabrooks A, Jo T, Japkowicz N. A multiple resampling me thod for learning from imbalanced data\nsets. Computational Intelligence. 2004;20(1):18-36.\n21. Chawla NV, et al. SMOTE: synthetic minority over-sampli ng technique. Journal of artiﬁcial intelligence\nresearch. 2002;16:321-57.\n22. V arotto G, et al. Comparison of Resampling T echniques fo r Imbalanced Datasets in Machine Learning:\nApplication to Epileptogenic Zone Localization F rom Inter ictal Intracranial EEG Recordings in Patients\nWith F ocal Epilepsy . F rontiers in Neuroinformatics. 2021; 15.\n23. Susan S, Kumar A. The balancing trick: Optimized samplin g of imbalanced datasets—A brief survey of\nthe recent State of the Art. Engineering Reports. 2021;3(4) .\n24. Kumar V, Choudhary A, Cho E. Data augmentation using pre- trained transformer models. arXiv\npreprint arXiv:200302245. 2020.\n25. Doan A, Ramakrishnan R, Halevy A Y. Crowdsourcing system s on the world-wide web. Communications\nof the ACM. 2011;54(4):86-96.\n26. W ong SC, et al. Understanding data augmentation for clas siﬁcation: when to warp? In: 2016\nInternational Conference on Digital Image Computing: T ech niques and Applications (DICT A); 2016.\n.\n27. Dou ZY, Hu J, Anastasopoulos A, Neubig G. Unsupervised do main adaptation for neural machine\ntranslation with domain-aware feature embeddings. arXiv p reprint arXiv:190810430. 2019.\n28. Alemohammad S, et al. Self-consuming generative models go mad. arXiv preprint arXiv:230701850.\n2023.\n29. Rea SC, Kleeman H, Zhu Q, Gilbert B, Y ue C. Crowdsourcing a s a tool for research: Methodological,\nfair, and political considerations. Bulletin of Science, T echnology & Society . 2020;40(3-4):40-53.\n30. V aughan JW. Making better use of the crowd: How crowdsour cing can advance machine learning\nresearch. The Journal of Machine Learning Research. 2017;1 8(1):7026-71.\n31. Geiger D, Rosemann M, Fielt E. Crowdsourcing Informatio n Systems–A Systems Theory Perspective.\nIn: ACIS 2011 Proceedings; 2011. A vailable from: https://a isel.aisnet.org/acis2011/33.\n32. Allahbakhsh M, Benatallah B, Ignjatovic A, Motahari-Ne zhad HR, Bertino E, Dustdar S. Quality control\nin crowdsourcing systems: Issues and directions. IEEE Inte rnet Computing. 2013;17(2):76-81.\n33. Babbage C. Passages from the Life of a Philosopher; 1864.\n34. Zha D, Bhat ZP , Lai KH, Y ang F, Jiang Z, Zhong S, et al. Data- centric artiﬁcial intelligence: A survey .\narXiv preprint arXiv:230310158. 2023.\n35. Nilforoshan H, W ang J, W u E. Precog: Improving crowdsour ced data quality before acquisition. arXiv\npreprint arXiv:170402384. 2017.\n36. Estellés-Arolas E. The need of co-utility for successfu l crowdsourcing. Co-utility: Theory and\nApplications. 2018:189-200.\n37. Malone TW, Laubacher R, Dellarocas C. The collective int elligence genome. MIT Sloan management\nreview. 2010.\n38. Kittur A, Smus B, Khamkar S, Kraut RE. Crowdforge: Crowds ourcing complex work. In: Proceedings\nof the 24th annual ACM symposium on User interface software a nd technology; 2011. p. 43-52.\n39. Kulkarni A, Can M, Hartmann B. Collaboratively crowdsou rcing workﬂows with turkomatic. In:\nProceedings of the acm 2012 conference on computer supporte d cooperative work; 2012. p. 1003-12.\n40. Lee J, Y oon W, Kim S, Kim D, Kim S, So CH, et al. BioBER T: a pre -trained biomedical language\nrepresentation model for biomedical text mining. Bioinfor matics. 2020;36(4):1234-40.",
  "topic": "Crowdsourcing",
  "concepts": [
    {
      "name": "Crowdsourcing",
      "score": 0.8896183371543884
    },
    {
      "name": "Health care",
      "score": 0.655881404876709
    },
    {
      "name": "Scarcity",
      "score": 0.63970547914505
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.6227737069129944
    },
    {
      "name": "Control (management)",
      "score": 0.5545316934585571
    },
    {
      "name": "Computer science",
      "score": 0.5139960646629333
    },
    {
      "name": "Quality assurance",
      "score": 0.49950122833251953
    },
    {
      "name": "Data quality",
      "score": 0.4754205048084259
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.457640141248703
    },
    {
      "name": "Data science",
      "score": 0.4384470582008362
    },
    {
      "name": "Knowledge management",
      "score": 0.34057366847991943
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2507203221321106
    },
    {
      "name": "Business",
      "score": 0.24959957599639893
    },
    {
      "name": "Marketing",
      "score": 0.09128814935684204
    },
    {
      "name": "World Wide Web",
      "score": 0.0872529149055481
    },
    {
      "name": "Metric (unit)",
      "score": 0.08367112278938293
    },
    {
      "name": "Political science",
      "score": 0.0771055817604065
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    },
    {
      "name": "Service (business)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I138006243",
      "name": "University of Arizona",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ],
  "cited_by": 1
}