{
  "title": "BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model",
  "url": "https://openalex.org/W2913129712",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2327300082",
      "name": "Wang, Alex",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753220617",
      "name": "Cho, Kyunghyun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W195465510",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W1889945169",
    "https://openalex.org/W2963456134",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W193851967",
    "https://openalex.org/W2963643701",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W2145094598",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2053476892",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2099939455",
    "https://openalex.org/W183625566",
    "https://openalex.org/W2103819961",
    "https://openalex.org/W2013145865",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2013035813",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from BERT. We generate from BERT and find that it can produce high-quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.",
  "full_text": "BERT has a Mouth, and It Must Speak:\nBERT as a Markov Random Field Language Model\nAlex Wang\nNew York University\nalexwang@nyu.edu\nKyunghyun Cho\nNew York University\nFacebook AI Research\nCIFAR Azrieli Global Scholar\nkyunghyun.cho@nyu.edu\nAbstract\nWe show that BERT (Devlin et al., 2018) is\na Markov random ﬁeld language model. This\nformulation gives way to a natural procedure\nto sample sentences from BERT. We generate\nfrom BERT and ﬁnd that it can produce high-\nquality, ﬂuent generations. Compared to the\ngenerations of a traditional left-to-right lan-\nguage model, BERT generates sentences that\nare more diverse but of slightly worse quality.\n1 Introduction\nBERT (Devlin et al., 2018) is a recently released\nsequence model used to achieve state-of-art results\non a wide range of natural language understanding\ntasks, including constituency parsing (Kitaev and\nKlein, 2018) and machine translation (Lample and\nConneau, 2019). Early work probing BERT’s lin-\nguistic capabilities has found it surprisingly robust\n(Goldberg, 2019).\nBERT is trained on a masked language model-\ning objective. Unlike a traditional language mod-\neling objective of predicting the next word in a se-\nquence given the history, masked language model-\ning predicts a word given its left and right context.\nBecause the model expects context from both di-\nrections, it is not immediately obvious how BERT\ncan be used as a traditional language model (i.e.,\nto evaluate the probability of a text sequence) or\nhow to sample from it.\nWe attempt to answer these questions by show-\ning that BERT is a combination of a Markov\nrandom ﬁeld language model (MRF-LM, Jernite\net al., 2015; Mikolov et al., 2013) with pseudo log-\nlikelihood (Besag, 1977) training. This formula-\ntion automatically leads to a sampling procedure\nbased on Gibbs sampling.\n2 BERT as a Markov Random Field\nLet X = (x1,...,x T) be a sequence of random\nvariables xi, each of which is categorical in that\nit can take one of M items from a vocabulary\nV = {v1,...,v M}. These random variables form\na fully-connected graph with undirected edges, in-\ndicating that each variable xi is dependent on all\nthe other variables.\nJoint Distribution To deﬁne a Markov random\nﬁeld (MRF), we start by deﬁning a potential over\ncliques. Among all possible cliques, we only con-\nsider the clique corresponding to the full graph.\nAll other cliques will be assigned a potential of\n1 (i.e. exp(0)). The potential for this full-graph\nclique decomposes into a sum of T log-potential\nterms:\nφ(X) =\nT∏\nt=1\nφt(X) = exp\n( T∑\nt=1\nlog φt(X)\n)\n,\nwhere we use X to denote the fully-connected\ngraph created from the original sequence. Each\nlog-potential φt(X) is deﬁned as\nlog φt(X) =\n\n\n\n1h(xt)⊤fθ(X\\t), if [MASK] /∈\nX1:t−1 ∪Xt+1:T\n0, otherwise,\n(1)\nwhere fθ(X\\t) ∈RM, 1h(xt) is a one-hot vector\nwith index xt set to 1, and\nX\\t = (x1,...,x t−1,[MASK] ,xt+1,...,x T)\nFrom this log-potential, we can deﬁne a probabil-\nity of a given sequence X as\npθ(X) = 1\nZ(θ)\nT∏\nt=1\nφt(X), (2)\narXiv:1902.04094v2  [cs.CL]  9 Apr 2019\nwhere\nZ(θ) =\n∑\nX′\nT∏\nt=1\nφt(X′),\nfor all X′. This normalization constant is unfor-\ntunately impractical to compute exactly, rendering\nexact maximum log-likelihood intractable.\nConditional Distribution Given a ﬁxed X\\t,\nthe conditional probability of xt is derived to be\np(xt|X\\t) = 1\nZ(X\\t) exp(1h(xt)⊤fθ(X\\t)),\n(3)\nwhere\nZ(X\\t) =\nM∑\nm=1\nexp(1h(m)⊤fθ(X\\t)).\nThis derivation follows from the peculiar formula-\ntion of the log-potential in Eq. (1). It is relatively\nstraightforward to compute, as it is simply softmax\nnormalization over M terms (Bridle, 1990).\n(Stochastic) Pseudo Log-Likelihood Learning\nOne way to avoid the issue of intractability in com-\nputing the normalization constant Z(θ) above1\nis to resort to an approximate learning strategy.\nBERT uses pseudo log-likelihood learning, where\nthe pseudo log-likelihood is deﬁned as:\nPLL(θ; D) = 1\n|D|\n∑\nX∈D\n|X|∑\nt=1\nlog p(xt|X\\t),. (4)\nwhere D is a set of training examples. We maxi-\nmize the predictability of each token in a sequence\ngiven all the other tokens, instead of the joint prob-\nability of the entire sequence.\nIt is still expensive to compute the pseudo log-\nlikelihood in Eq. (4) for even one example, espe-\ncially when fθ is not linear. This is because we\nmust compute |X|forward passes of fθ for each\nsequence, when |X|can be long and fθ be compu-\ntationally heavy. Instead we could stochastically\n1 In BERT it is not intractable in the strictest sense, since\nthe amount of computation is bounded (by T = 500) each\niteration. It however requires computation up to exp(500)\nwhich is in practice impossible to compute exactly.\nestimate it by\n1\n|X|\n|X|∑\nt=1\nlog p(xt|X\\t)\n=Et∼U({1,...,|X|})\n[\nlog p(xt|X\\t)\n]\n≈1\nK\nK∑\nk=1\nlog p(x˜tk |X\\˜tk ),\nwhere ˜tk ∼U({1,..., |X|}. Let us refer to this as\nstochastic pseudo log-likelihood learning.\nIn Reality The stochastic pseudo log-likelihood\nlearning above states that we “mask out” one to-\nken in a sequence at a time and let fθ predict it\nbased on all the other “observed” tokens in the se-\nquence. Devlin et al. (2018) however proposed to\n“mask out” multiple tokens at a time and predict\nall of them given both all “observed” and “masked\nout” tokens in the sequence. This brings the origi-\nnal BERT closer to a denoising autoencoder (Vin-\ncent et al., 2010), which could still be considered\nas training a Markov random ﬁeld with (approxi-\nmate) score matching (Vincent, 2011).\n3 Using BERT as an MRF-LM\nThe discussion so far implies that BERT is a\nMarkov random ﬁeld language model (MRF-LM)\nand that it learns a distribution over sentences (of\nsome given length). This framing suggests that we\ncan use BERT not only as parameter initialization\nfor ﬁnetuning but as a generative model of sen-\ntences to either score a sentence or sample a sen-\ntence.\nRanking Let us ﬁx the length T. Then, we can\nuse BERT to rank a set of sentences. We can-\nnot compute the exact probabilities of these sen-\ntences, but we can compute their unnormalized\nlog-probabilities according to Eq. (2):\nT∑\nt=1\nlog φt(X).\nThese unnormalized probabilities can be used to\nﬁnd the most likely sentence within the set or to\nsort the sentences according to their probabilities.\nSampling Sampling from a Markov random\nﬁeld is less trivial than is from a directed graph-\nical model which naturally admits ancestral sam-\npling. One of the most widely used approaches\nthe nearest regional centre is alemanno , with another connec-\ntion to potenza and maradona , and the nearest railway station\nis in bergamo , where the line terminates on its northern end\nfor all of thirty seconds , she was n’t going to speak . maybe\nthis time , she ’d actually agree to go . thirty seconds later ,\nshe ’d been speaking to him in her head every\n’ let him get away , mrs . nightingale . you could do it again\n. ’ ’ he - ’ ’ no , please . i have to touch him . and when you\ndo , you run .\n“ oh , i ’m sure they would be of a good service , ” she assured\nme . “ how are things going in the morning ? is your husband\nwell ? ” “ yes , very well\nhe also “ turned the tale [ of ] the marriage into a book ” as\nhe wanted it to “ be elegiac ” . both sagas contain stories of\nboth couple and their wedding night ;\n“ i know . ” she paused .“ did he touch you ? ” “ no . ” “ ah .\n” “ oh , no , ” i said , confused , not sure why\n“ i had a bad dream . ” “ about an alien ship ? who was it ?\n” i check the text message that ’s been only partially restored\nyet, the one that says love .\ni watched him through the glass , wondering if he was going\nto attempt to break in on our meeting . but he did n’t seem to\neven bother to knock when he entered the room . i was n’t\nreplaced chris hall ( st . louis area manager ) . june 9 : mike\nhoward ( syndicated “ good morning ” , replaced steve koval\n, replaced dan nickolas , and replaced phil smith ) ;\n“ how long has it been since you have made yourself an offer\nlike that ? ” asked planner . “ oh ” was the reply . planner\nhad heard of some of his other business associates who had\nTable 1: Random sample generations from BERT base (left) and GPT (right).\nis Markov-chain Monte-Carlo (MCMC) sam-\npling (Neal, 1993; Swendsen and Wang, 1986;\nSalakhutdinov, 2009; Desjardins et al., 2010; Cho\net al., 2010). In this report, we only consider\nGibbs sampling which ﬁts naturally with (stochas-\ntic) pseudo log-likelihood learning.\nIn Gibbs sampling, we start with a random ini-\ntial state X0, which we initialize to be an all-mask\nsequence, i.e., ([MASK] ,..., [MASK]), though\nwe could with a sentence consisting of randomly\nsampled words or by retrieving a sentence from\ndata. At each iteration i, we sample the position\nti uniformly at random from {1,...,T }and mask\nout the selected location, i.e., xi\nti = [MASK], re-\nsulting in Xi\n\\ti . We now compute p(xti |Xi\n\\ti ) ac-\ncording to Eq. (3), sample ˜xti from it2, and con-\nstruct the next sequence by\nXi+1 = (xi\n1,...,x i\nti−1,˜xti ,xi\nti+1,...,x i\nT).\nWe repeat this procedure many times, preferably\nwith thinning.3 Because Gibbs sampling, as well\nas any MCMC sampler with a local proposal dis-\ntribution, tends to get stuck in a mode of the dis-\ntribution, we advise running multiple chains of\nGibbs sampling or using different sentence initial-\nizations.\nSequential Sampling The undirectedness of the\nMRF-LM and the bidirectional nature of BERT do\nnot naturally admit sequential sampling, but given\nthat the dominant approach to text generation is\n2 In practice, one can imagine sampling from the k-most\nprobable words (Fan et al., 2018). We ﬁnd k = 100 to be\neffective in early experiments.\n3 Thinning refers to the procedure of selecting a sample\nonly once a while during MCMC sampling.\nleft-to-right, we experiment with generating from\nBERT in such a manner.\nAs with our non-sequential sampling scheme,\nwe can begin with a seed sentence of either all\nmasks or a random sentence. Whereas previously\nwe sampled a position t∈{1,...,T }to mask out\nand generate for at each time step, in the sequen-\ntial setting, at each time step t, we mask out xt\nt,\ngenerate a word for that position, and substitute\nit into the sequence. After T timesteps, we have\na sampled a token at each position, at which we\npoint we can terminate or repeat the process from\nthe current sentence.\n4 Experiments\nOur experiments demonstrate the potential of us-\ning BERT as a standalone language model rather\nthan as a parameter initializer for transfer learn-\ning (Devlin et al., 2018; Lample and Conneau,\n2019; Nogueira and Cho, 2019). We show that\nsentences sampled from BERT are well-formed\nand are assigned high probabilities by an off-the-\nshelf language model. We take pretrained BERT\nmodels trained on a mix of Toronto Book Corpus\n(TBC, Zhu et al., 2015) and Wikipedia provided\nby Devlin et al. (2018) and its PyTorch implemen-\ntation4 provided by HuggingFace. We experiment\nwith both the base and large BERT conﬁguations.\n4.1 Evaluation\nWe consider several evaluation metrics to estimate\nthe quality and diversity of the generations.\n4 https://github.com/huggingface/\npytorch-pretrained-BERT\nModel Self-BLEU ( ↓)\n% Unique n-grams (↑)\nSelf WT103 TBC\nn=2 n=3 n=4 n=2 n=3 n=4 n=2 n=3 n=4\nBERT (large) 9.43 63.15 92.38 98.01 59.91 91.86 98.43 64.59 93.27 98.59\nBERT (base) 10.06 60.76 91.76 98.14 57.90 91.72 98.55 60.94 92.04 98.56\nGPT 40.02 31.13 67.01 87.28 33.71 72.86 91.12 25.74 65.04 88.42\nWT103 9.80 70.29 94.36 99.05 56.19 88.05 97.44 68.35 94.20 99.23\nTBC 12.51 62.19 92.70 98.73 55.30 91.08 98.81 44.75 82.06 96.31\nTable 2: Self-BLEU and percent of generated n-grams that are unique relative to own generations (left) WikiText-\n103 test set (middle) a sample of 5000 sentences from Toronto Book Corpus (right). For the WT103 and TBC\nrows, we sample 1000 sentences from the respective datasets.\nQuality To automatically measure the quality of\nthe generations, we follow Yu et al. (2017) by\ncomputing BLEU (Papineni et al., 2002) between\nthe generations and the original data distributions\nto measure how similar the generations are. We\nuse a random sample of 5000 sentences from the\ntest set of WikiText-103 (WT103, Merity et al.,\n2016) and a random sample of 5000 sentences\nfrom TBC as references.\nWe also use the perplexity of a trained language\nmodel evaluated on the generations as a rough\nproxy for ﬂuency. Speciﬁcally, we use the Gated\nConvolutional Language Model (Dauphin et al.,\n2016) pretrained on WikiText-1035.\nDiversity To measure the diversity of each\nmodel’s generations, we compute self-BLEU (Zhu\net al., 2018): for each generated sentence, we com-\npute BLEU treating the rest of the sentences as ref-\nerences, and average across sentences. Self-BLEU\nmeasures how similar each generated sentence is\nto the other generations; high self-BLEU indicates\nthat the model has low sample diversity.\nWe also evaluate the percentage ofn-grams that\nare unique, when compared to the original data\ndistribution and within the corpus of generations.\nWe note that this metric is somewhat in opposition\nto BLEU between generations and data, as fewer\nunique n-grams implies higher BLEU.\nMethodology We use the non-sequential sam-\npling scheme with sampling from the top k= 100\nmost frequent words at each time step, as empir-\nically this led to the most coherent generations.\nWe show generations from the sequential sam-\npler in Table 4 in the appendix. We compare\nagainst generations from a high-quality neural lan-\nguage model, the OpenAI Generative Pre-Training\n5https://github.com/pytorch/fairseq/\ntree/master/examples/conv_lm\nModel Corpus-BLEU (↑) PPL (↓)\nWT103 TBC\nBERT (large) 5.05 7.60 331.47\nBERT (base) 7.80 7.06 279.10\nGPT 10.81 30.75 154.29\nWT103 17.48 6.57 54.00\nTBC 10.05 23.05 314.28\nTable 3: Quality metrics of model generations. Per-\nplexity (PPL) is measured using an additional language\nmodel (Dauphin et al., 2016). For the WT103 and TBC\nrows, we sample 1000 sentences from the respective\ndatasets.\nTransformer (Radford et al., 2018, GPT), which\nwas trained on TBC and has approximately the\nsame number of parameters as the base conﬁgura-\ntion of BERT. For BERT, we pad each input with\nspecial symbols [CLS] and [SEP]. For GPT, we\nstart with a start of sentence token and generate\nleft to right. For all models, we generate 1000 un-\ncased sequences of length 40. Finally, as a triv-\nial baseline, we sample 1000 sentences from TBC\nand the training split of WT103 and compute all\nautomatic metrics against these samples.\n5 Results\nWe present sample generations, quality results,\nand diversity results respectively in Tables 1, 2, 3.\nWe ﬁnd that, compared to GPT, the BERT gen-\nerations are of worse quality, but are more diverse.\nSurprisingly, the outside language model, which\nwas trained on Wikipedia, is less perplexed by\nthe GPT generations than the BERT generations,\neven though GPT was only trained on romance\nnovels and BERT was trained on romance nov-\nels and Wikipedia. On actual data from TBC, the\noutside language model is about as perplexed as\non the BERT generations, which suggests that do-\nmain shift is an issue in using a trained language\nFigure 1: Fluency scores for 100 sentences samples from each of BERT large, BERT base, and GPT, as judged by\nhuman annotators according to a four-point Likert scale.\nmodel for evaluating generations and that the GPT\ngenerations might have collapsed to fairly generic\nand simple sentences. This observation is further\nbolstered by the fact that the GPT generations have\na higher corpus-BLEU with TBC than TBC has\nwith itself. The perplexity on BERT samples is\nnot absurdly high, and in reading the samples, we\nﬁnd that many are fairly coherent. The corpus-\nBLEU between BERT models and the datasets is\nlow, particularly with WT103.\nWe ﬁnd that BERT generations are more di-\nverse than GPT generations. GPT has highn-gram\noverlap (smaller percent of unique n-grams) with\nTBC, but surprisingly also with WikiText-103, de-\nspite being trained on different data. Furthermore,\nGPT generations have greatern-gram overlap with\nthese datasets than these datasets have with them-\nselves, further suggesting that GPT is relying sig-\nniﬁcantly on generic sentences. BERT has lower\nn-gram overlap with both corpora, with similar de-\ngrees of n-gram overlap as the samples of the data.\nFor a more rigorous evaluation of generation\nquality, we collect human judgments on sentence\nﬂuency for 100 samples from BERT large, BERT\nbase, and GPT using a four point Likert scale.\nFor each sample we ask three annotators to rate\nthe sentence on its ﬂuency and take the average\nof the three judgments as the sentence’s ﬂuency\nscore. We present a histogram of the results in\nFigure 1. For BERT large, BERT base, and GPT\nwe respectively get mean scores over the samples\nof 2.37 ( σ = 0.83), 2.65 (σ = 0.65), and 2.80\n(σ= 0.51). All means are within a standard devia-\ntion of each other. BERT base and GPT have simi-\nlar unimodal distributions with BERT base having\na slightly more non-ﬂuent samples. BERT large\nhas a bimodal distribution.\n6 Conclusion\nWe show that BERT is a Markov random ﬁeld lan-\nguage model. Formulating BERT in this way gives\nrise to a practical algorithm for generating from\nBERT based on Gibbs sampling that does not re-\nquire any additional parameters or training. We\nverify in experiments that the algorithm produces\ndiverse and fairly ﬂuent generations. The power\nof this framework is in allowing the principled ap-\nplication of Gibbs sampling, and potentially other\nMCMC algorithms, for generating from BERT.\nFuture work might explore these improved sam-\npling methods, especially those that do not need\nto run the model over the entire sequence each\niteration and that more robustly handle variable-\nlength sequences. To facilitate such investigation,\nwe release our code on GitHub at https:\n//github.com/nyu-dl/bert-gen and\na demo as a Colab notebook at https://\ncolab.research.google.com/drive/\n1MxKZGtQ9SSBjTK5ArsZ5LKhkztzg52RV.\nAcknowledgements\nWe thank Ilya Kulikov and Nikita Nangia for their\nhelp, as well as reviewers for insightful comments.\nAW is supported by an NSF Fellowship. KC\nis partly supported by Samsung Advanced Insti-\ntute of Technology (Next Generation Deep Learn-\ning: from Pattern Recognition to AI) and Samsung\nElectronics (Improving Deep Learning using La-\ntent Structure).\nReferences\nJulian Besag. 1977. Efﬁciency of pseudolikelihood\nestimation for simple gaussian ﬁelds. Biometrika,\npages 616–618.\nJohn S Bridle. 1990. Probabilistic interpretation of\nfeedforward classiﬁcation network outputs, with re-\nlationships to statistical pattern recognition. In Neu-\nrocomputing, pages 227–236. Springer.\nKyungHyun Cho, Tapani Raiko, and Alexander Ilin.\n2010. Parallel tempering is efﬁcient for learning re-\nstricted boltzmann machines. In Neural Networks\n(IJCNN), The 2010 International Joint Conference\non, pages 1–8. IEEE.\nYann N Dauphin, Angela Fan, Michael Auli, and David\nGrangier. 2016. Language modeling with gated con-\nvolutional networks. arXiv preprint:1612.08083.\nGuillaume Desjardins, Aaron Courville, Yoshua Ben-\ngio, Pascal Vincent, and Olivier Delalleau. 2010.\nTempered markov chain monte carlo for training of\nrestricted boltzmann machines. In Proceedings of\nthe thirteenth international conference on artiﬁcial\nintelligence and statistics, pages 145–152.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint:1810.04805.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. arXiv\npreprint:1805.04833.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties.\nYacine Jernite, Alexander Rush, and David Sontag.\n2015. A fast variational approach for learning\nmarkov random ﬁeld language models. In Inter-\nnational Conference on Machine Learning , pages\n2209–2217.\nNikita Kitaev and Dan Klein. 2018. Multilingual\nconstituency parsing with self-attention and pre-\ntraining.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual Language Model Pretraining. arXiv e-prints,\npage arXiv:1901.07291.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture\nmodels.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space. arXiv preprint:1301.3781.\nRadford M Neal. 1993. Probabilistic inference using\nmarkov chain monte carlo methods.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage\nre-ranking with bert. arXiv preprint:1901.04085.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language under-\nstanding paper. pdf.\nRuslan R Salakhutdinov. 2009. Learning in markov\nrandom ﬁelds using tempered transitions. In Ad-\nvances in neural information processing systems ,\npages 1598–1606.\nRobert H Swendsen and Jian-Sheng Wang. 1986.\nReplica monte carlo simulation of spin-glasses.\nPhysical review letters, 57(21):2607.\nPascal Vincent. 2011. A connection between score\nmatching and denoising autoencoders. Neural com-\nputation, 23(7):1661–1674.\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie,\nYoshua Bengio, and Pierre-Antoine Manzagol.\n2010. Stacked denoising autoencoders: Learning\nuseful representations in a deep network with a lo-\ncal denoising criterion. Journal of machine learning\nresearch, 11(Dec):3371–3408.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n2017. Seqgan: Sequence generative adversarial nets\nwith policy gradient. In AAAI, pages 2852–2858.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,\nWeinan Zhang, Jun Wang, and Yong Yu. 2018.\nTexygen: A benchmarking platform for text genera-\ntion models. arXiv preprint:1802.01886.\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and\nmovies: Towards story-like visual explanations by\nwatching movies and reading books. In arXiv\npreprint:1506.06724.\nA Other Sampling Strategies\nWe explored two other sampling strategies: left-\nto-right and generating for all positions at each\ntime step. See Section 3 for an explanation of\nthe former. For the latter, we start with an ini-\ntial sequence of all masks, and at each time step,\nwe would not mask any positions but would gen-\nerate for all positions. This strategy is designed\nto save on computation. However, we found that\nthis tended to get stuck in non-ﬂuent sentences that\ncould not be recovered from. We present sample\ngenerations for the left-to-right strategy in Table 4.\nall the good people , no more , no less . no more . for ... the kind of better people ... for ... for ... for ... for ... for ... for ...\nas they must become again .\nsometimes in these rooms , here , back in the castle . but then : and then , again , as if they were turning , and then slowly\n, and and then and then , and then suddenly .\nother available songs for example are the second and ﬁnal two complete music albums among the highest played artists ,\nincluding : the one the greatest ... and the last recorded album , ” this sad heart ” respectively .\n6 that is i ? ? and the house is not of the lord . i am well ... the lord is ... ? , which perhaps i should be addressing : ya is\nthen , of ye ? ?\nfour - cornered rap . big screen with huge screen two of his friend of old age . from happy , happy , happy . left ? left ?\nleft ? right . left ? right . right ? ?\nTable 4: Random sample generations from BERT base using a sequential, left-to-right sampling strategy.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6775098443031311
    },
    {
      "name": "Markov random field",
      "score": 0.6305493712425232
    },
    {
      "name": "Markov chain",
      "score": 0.5596073269844055
    },
    {
      "name": "Language model",
      "score": 0.526817798614502
    },
    {
      "name": "Field (mathematics)",
      "score": 0.472907155752182
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46724677085876465
    },
    {
      "name": "Sample (material)",
      "score": 0.4624972343444824
    },
    {
      "name": "Natural language processing",
      "score": 0.3840944468975067
    },
    {
      "name": "Mathematics",
      "score": 0.21061185002326965
    },
    {
      "name": "Machine learning",
      "score": 0.18315163254737854
    },
    {
      "name": "Pure mathematics",
      "score": 0.06722313165664673
    },
    {
      "name": "Image (mathematics)",
      "score": 0.05865961313247681
    },
    {
      "name": "Physics",
      "score": 0.05702930688858032
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Image segmentation",
      "score": 0.0
    }
  ]
}