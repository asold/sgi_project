{
  "title": "TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech",
  "url": "https://openalex.org/W3041561163",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3033002800",
      "name": "Andy T. Liu",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A4287859061",
      "name": "Shang-Wen Li",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2514219681",
      "name": "Hung-Yi Lee",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A3033002800",
      "name": "Andy T. Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287859061",
      "name": "Shang-Wen Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2514219681",
      "name": "Hung-Yi Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3096171739",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W2927746189",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6607333740",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6784776607",
    "https://openalex.org/W6763238093",
    "https://openalex.org/W2973049979",
    "https://openalex.org/W6844194202",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W3003875258",
    "https://openalex.org/W6777232839",
    "https://openalex.org/W3096626135",
    "https://openalex.org/W3096485810",
    "https://openalex.org/W6778265221",
    "https://openalex.org/W6777859476",
    "https://openalex.org/W2794209590",
    "https://openalex.org/W2125496931",
    "https://openalex.org/W2962901777",
    "https://openalex.org/W2995181338",
    "https://openalex.org/W2002342963",
    "https://openalex.org/W2964227577",
    "https://openalex.org/W2077804127",
    "https://openalex.org/W3035202887",
    "https://openalex.org/W3097286738",
    "https://openalex.org/W3096017728",
    "https://openalex.org/W3015265920",
    "https://openalex.org/W6840487619",
    "https://openalex.org/W3015949486",
    "https://openalex.org/W3100270690",
    "https://openalex.org/W2947445680",
    "https://openalex.org/W2982039329",
    "https://openalex.org/W2973157397",
    "https://openalex.org/W3033038061",
    "https://openalex.org/W3015356564",
    "https://openalex.org/W6769196770",
    "https://openalex.org/W6773205534",
    "https://openalex.org/W2972943112",
    "https://openalex.org/W3016181583",
    "https://openalex.org/W6631362777",
    "https://openalex.org/W3016011332",
    "https://openalex.org/W2972451902",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W3015412890",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W3024182269",
    "https://openalex.org/W3026957705",
    "https://openalex.org/W2979476256",
    "https://openalex.org/W2996383576",
    "https://openalex.org/W2947454875",
    "https://openalex.org/W3125709657",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2943493972",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2988736778",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W4288348042",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W3148040514",
    "https://openalex.org/W1635512741",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3160345865",
    "https://openalex.org/W3015412285",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2949667497",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3198858531",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3102342027",
    "https://openalex.org/W3095292526",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2981991061",
    "https://openalex.org/W3030987249",
    "https://openalex.org/W3026842484",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1608367484",
    "https://openalex.org/W3104896896"
  ],
  "abstract": "We introduce a self-supervised speech pre-training method called TERA, which\\nstands for Transformer Encoder Representations from Alteration. Recent\\napproaches often learn by using a single auxiliary task like contrastive\\nprediction, autoregressive prediction, or masked reconstruction. Unlike\\nprevious methods, we use alteration along three orthogonal axes to pre-train\\nTransformer Encoders on a large amount of unlabeled speech. The model learns\\nthrough the reconstruction of acoustic frames from their altered counterpart,\\nwhere we use a stochastic policy to alter along various dimensions: time,\\nfrequency, and magnitude. TERA can be used for speech representations\\nextraction or fine-tuning with downstream models. We evaluate TERA on several\\ndownstream tasks, including phoneme classification, keyword spotting, speaker\\nrecognition, and speech recognition. We present a large-scale comparison of\\nvarious self-supervised models. TERA achieves strong performance in the\\ncomparison by improving upon surface features and outperforming previous\\nmodels. In our experiments, we study the effect of applying different\\nalteration techniques, pre-training on more data, and pre-training on various\\nfeatures. We analyze different model sizes and find that smaller models are\\nstrong representation learners than larger models, while larger models are more\\neffective for downstream fine-tuning than smaller models. Furthermore, we show\\nthe proposed method is transferable to downstream datasets not used in\\npre-training.\\n",
  "full_text": "1\nTERA: Self-Supervised Learning of\nTransformer Encoder Representation for Speech\nAndy T. Liu, Shang-Wen Li, and Hung-yi Lee\nAbstract—We introduce a self-supervised speech pre-training\nmethod called TERA, which stands for Transformer Encoder\nRepresentations from Alteration. Recent approaches often learn\nby using a single auxiliary task like contrastive prediction, autore-\ngressive prediction, or masked reconstruction. Unlike previous\nmethods, we use alteration along three orthogonal axes to pre-\ntrain Transformer Encoders on a large amount of unlabeled\nspeech. The model learns through the reconstruction of acoustic\nframes from their altered counterpart, where we use a stochastic\npolicy to alter along various dimensions: time, frequency, and\nmagnitude. TERA can be used for speech representations extrac-\ntion or ﬁne-tuning with downstream models. We evaluate TERA\non several downstream tasks, including phoneme classiﬁcation,\nkeyword spotting, speaker recognition, and speech recognition.\nWe present a large-scale comparison of various self-supervised\nmodels. TERA achieves strong performance in the comparison\nby improving upon surface features and outperforming previous\nmodels. In our experiments, we study the effect of applying\ndifferent alteration techniques, pre-training on more data, and\npre-training on various features. We analyze different model sizes\nand ﬁnd that smaller models are strong representation learners\nthan larger models, while larger models are more effective for\ndownstream ﬁne-tuning than smaller models. Furthermore, we\nshow the proposed method is transferable to downstream datasets\nnot used in pre-training.\nIndex Terms—self-supervised, pre-training, representation\nI. I NTRODUCTION\nU\nNLIKE humans, capable of self-learning through experi-\nences and interactions, current real-world speech applica-\ntions like automatic speech recognition (ASR) rely heavily on\nlarge amounts of human annotations. For the next generation\nof speech processing systems to exhibit similar cognitive\n© 2021 IEEE. Personal use of this material is permitted. Permission from\nIEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to servers\nor lists, or reuse of any copyrighted component of this work in other works.\nAccepted Version. Manuscript received July 12, 2020; revised February 25,\n2021 and June 1, 2021; accepted June 29, 2021. Date of publication July 8,\n2021; date of current version July 30, 2021. This work was supported in\npart by the National Science Council, Taiwan, under Contract 110-2628-E-\n002-001. The work of A. Liu was supported in part by the Frontier Speech\nTechnology Scholarship of National Taiwan University and in part by ASUS\nAICS. The associate editor coordinating the review of this manuscript and\napproving it for publication was Dr. Ozlem Kalinli. (Corresponding author:\nHung-yi Lee.).\nAndy T. Liu and Hung-yi Lee are with the Graduate Institute of\nCommunication Engineering, College of Electrical Engineering and Com-\nputer Science, National Taiwan University, Taipei 10617, Taiwan (e-mail:\nf07942089@ntu.edu.tw; hungyilee@ntu.edu.tw).\nShang-Wen Li is with Amazon AI, New York, NY 10001 USA (e-mail:\nswdanielli@gmail.com).\nA. Liu was supported by Frontier Speech Technology Scholarship of\nNational Taiwan University. A. Liu was also supported by ASUS AICS.\nDigital Object Identiﬁer: http://dx.doi.org/10.1109/TASLP.2021.3095662\nintelligence levels as humans, machines should be designed to\nlearn from unlabeled data as humans do. In the era of big data,\nself-supervised learning has emerged as an attractive approach\nto leverage knowledge from many unlabeled data, and are\nshown effective for improving downstream systems [1]–[27].\nIn self-supervised learning, an auxiliary task (or pre-training\ntask) is formulated, and models are trained to solve it. While\nsolving the auxiliary task, the network is learning a func-\ntion that maps input to desired representations. Hence the\nfundamental tenet of self-supervised learning is the design\nof an auxiliary task, which allows the model to leverage\nknowledge from unlabeled data. As such, the formulation\nof the auxiliary task should be carefully chosen. The task\nshould be challenging enough for the model to learn high-\nlevel semantic properties and not be too amiable to exploit\nlow-level shortcuts.\nAfter self-supervised pre-training, learned models could be\napplied to downstream Speech and Language Processing (SLP)\ntasks through feature-based speech representation extraction,\nor ﬁne-tuning as part of the downstream model. Speech\nrepresentations are compact vectors which aim to capture\nhigh-level semantic information from raw speech [1]–[3], [6]–\n[21], [27]. Thus, the goal of speech representation learning\nis to ﬁnd a transform that maps the input acoustic features\ninto such vectors. When the pre-trained networks are re-used\nas features, it provides a useful speech representation to re-\nduce classiﬁer complexity, makes high-level information more\naccessible, and ultimately improves downstream SLP tasks.\nBesides, speech representations also help transfer learning and\nadaptation across different data distributions [6], [7], [9], [20],\n[21]. On the other hand, the ﬁne-tuning approach uses the pre-\ntrained model to initialize a downstream model for supervised\ntraining. The parameters of self-supervised learned models are\nfound to be good initialization for ASR [4], [5], [20]–[26].\nIn this work, we propose TERA: Transformer Encoder\nRepresentations from Alteration, where we use alteration on\ndata to pre-train Transformer Encoders [28]. We introduce a\ntotal of three types of alteration to form the self-supervised\npre-training scheme: 1) time alteration: reconstructing from\ncorrupted blocks of time steps. 2) frequency alteration: recon-\nstructing from missing blocks of frequency bins. 3) magnitude\nalteration: reconstructing from altered feature magnitudes.\nThese alterations can be applied together or separately in the\npre-training process. We apply alteration on data by dynami-\ncally sampling through a probabilistic policy to create random\nalterations. The model acquires information about the content\naround the corrupted or altered portions, and by reconstructing\nthem, the model learns a more contextualized representation.\narXiv:2007.06028v3  [eess.AS]  4 Aug 2021\n2\nWe illustrated the framework in Fig. 1.\nWe use the following downstream tasks to evaluate TERA:\nphoneme classiﬁcation, keyword spotting, speaker recognition,\nand automatic speech recognition (ASR). Also, we compare\nthe effectiveness of each alteration method separately and in\ncombination. As a result, we conﬁrm that each of the proposed\nalteration methods guides the model to learn a distinct aspect\nof speech: 1) The time alteration effectively enforces a more\naccurate phoneme prediction, keyword detection, and speech\nrecognition, as it leads the model to learn richer phonetic con-\ntent. 2) The frequency alteration effectively improves speaker\nprediction accuracy, as it leads the model to encode speaker\nidentity. 3) The magnitude alteration effectively improves\nperformance for all tasks, as it potentially increases data\ndiversity for pre-training.\nDifferent self-supervised frameworks have been widely\nstudied, in Section II we provide a thorough review. Previous\nwork explored mostly for reconstruction on the temporal axis,\nfor example unidirectional (or autoregressive) reconstruction\nof magnitude or phase from past frames [8]–[10], [15]–[18],\n[23], or bidirectional reconstruction of a temporal frame from\nboth past and future slices [12]–[14], [20]–[22], [24]–[27].\nOur work contrasts with prior work in several ways. Firstly,\nunlike previous work that only employs reconstruction on the\ntemporal axis, we use reconstruction loss and apply alteration\non data along three orthogonal axes, including time, frequency,\nand magnitude axis. Secondly, most works evaluated their\napproach with classiﬁcation tasks only [1], [7], [8], [11], [13]–\n[17], [20], [21]. In contrast, we moved beyond classiﬁcation\nand applied our model to ASR. For a comprehensive investi-\ngation, we evaluate our method with four downstream tasks.\nThirdly, we explore knowledge transfer between pre-trained\nmodels and downstream tasks, an under-investigated problem\nin speech compared to NLP [29], [30]. We leverage two\nways to incorporate the pre-trained model with downstream\ntasks, where most of the previous work only explored one\nway of transferring their pre-trained models. Fourthly, we\nstudy how self-supervised models behave when pre-trained on\na different amount of unlabeled data. Surprisingly, we ﬁnd\nthat methods that learn from time-only masked reconstruction\nmethods can sometimes not beneﬁt from more unlabeled data.\nThis is because of the reconstruction nature, memorizing all\nthe details, including the unnecessary noise. Additionally, we\nstudy the effect of pre-training on various features. We explore\nfour different acoustic features in this work, including log Mel,\nfMLLR, MFCC, and FBANK. We report results primarily on\nlog Mel and fMLLR. The usage of fMLLR, is not explored\nbefore for reconstruction-based methods. Also, none of the\nprevious work explores more than one acoustic feature for their\nmethod. Our study ﬁnds that using different acoustic features\nin reconstruction-based learning has a signiﬁcant effect on\npre-trained models and is a parameter choice for researchers.\nFurthermore, we ﬁnd smaller pre-trained models perform well\nfor feature extraction over larger models, and larger models\ntend to be more effective for ﬁne-tuning. Finally, we show\nexplicitly that our approach continues to work well in the face\nof domain mismatch between pre-training and downstream\ndatasets. For reproducibility of our results, we provide our\nFig. 1: The illustration of the proposed TERA self-supervised\nspeech representation approach.\nimplementation with pre-trained models and evaluation scripts\nin the S3PRL Toolkit 1 [31].\nII. R ELATED WORK\nThere are two major branches of speech pre-training meth-\nods: Contrastive Predictive Coding (CPC) and Reconstruction.\nA. Contrastive Losses\n1) Contrastive Predictive Coding: The CPC paper [1]\ndescribes a form of unidirectional modeling in the feature\nspace, where the model learns to predict the near future\nframes in an acoustic sequence while contrasting with frames\nfrom other sequences or frames from a more distant time.\nIn wav2vec [2], the CPC [1] loss is used to pre-train speech\nrepresentations for speech recognition, and experiment results\nshow self-supervised pre-training improves supervised speech\nrecognition.\n2) CPC with Quantization: In the work of vq-wav2vec [3],\nthe wav2vec [2] approach is incorporated with the well-\nperforming Natural Language Processing (NLP) algorithm\n– Bidirectional Encoder Representations from Transformers\n(BERT) [32], [33]. The vq-wav2vec [3] approach learns\nBERT-like speech representations through a two-stage training\npipeline. In a follow-up work of BERT + vq-wav2vec [4], the\npre-trained vq-wav2vec [3] model is directly ﬁne-tuned on\ntranscribed speech using a Connectionist Temporal Classiﬁ-\ncation (CTC) [34] loss instead of feeding the representations\ninto a task-speciﬁc model. In wav2vec 2.0 [5], the vq-wav2vec\nmethod is improved to a single-stage training scheme through\ntime masking in the latent space.\n1The S3PRL Toolkit: https://github.com/s3prl/s3prl\n3\nFig. 2: The illustration of different inputs with various alteration applied for the proposed auxiliary objective. The altered part\nis highlighted in yellow.\n3) Improving CPC: In recent research, the CPC loss\nhas also been extended and applied to bidirectional context\nnetworks. Bidirectional CPC (Bidir-CPC) [6] representations\nare learned from bidirectional predictive models. The multi-\nlayer CNN encoder network is shared for both directions,\nbut two autoregressive GRU context networks read encoded\nobservations from the forward and backward contexts. The\nModiﬁed CPC [7] focuses on improving the CPC approach\nby introducing several modiﬁcations to the original. These\nmodiﬁcations include changing the batch normalization to\nchannel-wise normalization, replacing the linear prediction\nlayer with a Transformer layer [28], and replacing the GRU\ncontext network with LSTM cells.\nIn this work, TERA is compared with several contrastive\nlearning methods, including CPC [1], wav2vec [2], vq-\nwav2vec [3], BERT + vq-wav2vec [4], wav2vec 2.0 [5], Bidir-\nCPC [6], and Modiﬁed CPC [7].\nB. Reconstruction Losses\nAnother recently emerged branch of speech pre-training\napproach devotes its attention on reconstruction losses.\n1) Autoregressive reconstruction: Primarily inspired by lan-\nguage models (LM) for text, the Autoregressive Predictive\nCoding (APC) [8], [9] model can be seen as a speech version\nof LM. The APC approach uses an autoregressive model\nto encode temporal information of past acoustic sequence;\nthe model then predicts future frames like a recurrent-based\nLM [35] while conditioning on past frames. In [10], the\nAPC objective is extended to multi-target training. The new\nobjective predicts not only the future frame conditioning on\nprevious context but also past memories. In VQ-APC [11],\na vector quantization (VQ) layer is used with the APC\nobjective, which imposes a bottleneck and forces the model to\nlearn better representations. In DeCoAR [12], combining the\nbidirectionality of ELMo [36] and the reconstruction objective\nof APC [8], [9], models were able to learn deep contextualized\nacoustic representations.\n2) Time-only Masked Reconstruction: Largely inspired by\nthe Masked Language Model (MLM) task from BERT [32],\n[33], [37] and Permutation Language Modeling (PLM) from\nXLNet [38], recent work [20]–[27] have explored using BERT-\nstyle tasks to pre-train speech encoders. These approaches\nadopt the NLP pre-training technique to continuous speech. In\nMockingjay [20], input frames of speech are masked to zero\nto pre-train Transformer Encoders. In Audio ALBERT [21],\nMockingjay is modiﬁed to have shared parameters across\nTransformer layers. In [39], Mockingjay is shown to be effec-\ntive in defending adversarial black-box attacks. And in [40],\nthe self-attention of Mockingjay is shown to be meaningful\nand explainable. On the other hand, TERA can be seen as\nan improved version of Mockingjay [20]. Using the time\nalteration alone as pre-training objective reduces TERA to\nMockingjay.\nIn [24], [26], time-only masked reconstruction following\nthe standard BERT masking policy is employed to pre-train\nASR encoders. In [25], a simpler masking policy is employed,\nwhere input features are divided into chunks of four frames,\nand masking on chunks are applied with a probability of\n15%. In Speech-XLNet [38], models learn by reconstructing\nfrom shufﬂed input speech frame orders rather than masked\nframes. In [22], SpecAugment [41] is applied on input frames\nto pre-train ASR encoders (bi-GRUs). In wav2vec 2.0 [5], time\nmasking is applied in the latent space. In Non-autoregressive\nPredictive Coding (NPC) [27], time masking is introduced\nthrough Masked Convolution Blocks, rather than on the input\ndata. In [42], phoneme posterior vectors are used to train\n4\na standard BERT [32], [38] model. The phoneme posterior\nvectors are output from a supervised acoustic model, which\nrequires CTC loss training over the ground-truth phonemes.\nAlso, in [43], CTC loss is used along with time-only masked\nreconstruction training to learn phonetic representations. As\n[42] and [43] both use phoneme labels for CTC training, they\ndiverge from other works that are fully self-supervised.\n3) Learning from Other Reconstruction Losses: Other than\nautoregressive and time-only masked reconstruction losses,\nprevious works have also explored the reconstruction of differ-\nent targets or frameworks, including temporal slice estimation,\ngap estimation, autoencoders, phase prediction, and Markov\nModels. In Audio2Vec [13], [14], the model learns through\nreconstructing a spectrogram slice from past and future slices;\nthis can be seen as a speech version of the NLP Word2Vec [44]\nvariants CBoW (continuous bag-of-words) and skip-gram. The\nTemporalGap [13], [14] approach learns through estimating\nthe temporal gap between two short audio segments extracted\nat random from the same audio clip. In [15], speech represen-\ntations are learned by applying autoencoding neural networks\nto speech waveform. Apart from reconstructing spectrograms,\nin [17], representations are learned through reconstructing the\nphase of the short-time Fourier transform from its magnitude.\nIn PASE [18], a single neural encoder learns to solve mul-\ntiple self-supervised tasks at once, including reconstruction\nof waveform, Log power spectrum, MFCC, prosody, and\nother binary discrimination tasks. The ConvDMM [19] ap-\nproach learns speech representations with convolutional neural\nnetworks and Markov Models. Although The design of the\nauxiliary task fundamentally decides what the model learns\nthrough its reconstruction.\nIn this work, TERA is compared with several reconstruction\nlearning methods, including APC [8], [9], VQ-APC [11], De-\nCoAR [12], Mockingjay [20], Audio ALBERT [21], SpecAug-\nment [22], [41], and NPC [27].\nIII. P ROPOSED METHODOLOGY\nA. Alteration on Data\nAs illustrated in Fig. 1A, the input acoustic frames (out-\nlined in the red box) and target predicted frames (outlined\nin the green box) could be any acoustic features, such as\nMFCC, FBANK, fMLLR, or log Mel. We show a sample\nof 80-dimensional log Mel feature sequence from the Lib-\nriSpeech [45] train-clean-100 subset in Fig. 2A. We denote\nthe entire speech corpus as Xand the acoustic features of the\nutterance sampled from X as − →x. The length (the number of\nframes) and the height (the number of frequency bins) of − →x\nis denoted as Lx and Hx, respectively. Below, we introduce\nhow we use different methods to alter the input − →x.\n1) Time Alteration: Our model learns bidirectional repre-\nsentations from past and future contexts by altering contiguous\nsegments along the time axis. In time alteration, a certain\npercentage of input frames are altered during training, and\nthe model attempts to reconstruct the corrupted span from\nneighboring frames. We randomly select Tnum amount of\nstarting indexes IT without replacement to alter the input\nutterance. The amount Tnum is given as the maximum time\nalteration percentage PT normalized by the time alteration\nwidth WT :\nTnum = ⌊PT ×Lx ÷WT ⌉ (1)\nNote that if time alteration width WT = 1, then Tnum =\nPT ×Lx. For each starting index location it in IT , we alter\nWT consecutive frames from it according to the following\nstochastic alteration policy: 1) 80% of the time, we mask\nall the selected frames to zero. 2) 10% of the time, we\nreplace all with random segments of frames. 3) For the rest\n10% of the time, we do nothing and leave the frames in − →x\nunchanged. The design of Case 3) is to allow the model to\nreceive real inputs during training and addresses the train-\ntest inconsistency problem. This inconsistency problem comes\nfrom the absence of alteration during inference time, and the\nmodel will only receive acoustic features without alteration.\nWe illustrate the masking and replacing of frames in\nFig. 2B and 2C, respectively. Our time alteration policy is\nmore sophisticated than other time-only masked reconstruction\napproaches [22], [25], where they simply mask a percentage\nwith zeroed-out spans, unlike ours that have random and real\nframes. We set the time alteration width WT to 7 frames,\nwhich corresponds to 85ms of speech. The time alteration\nwidth WT then lies in the average phoneme duration range\n(average phone duration is around 50 ms to 100 ms at\nusual rates of 10 to 20 phones per second). We set the PT\npercentage of total altered frames to 15%, as suggested in\n[20], [32], [33]. We allow time alteration blocks to overlap\neach other, hence resulting in the larger highlighted yellow\nbox in the left of Fig. 2B and 2C. With overlapping, we\ngenerate a longer altered span ( > WT ) and force the model\nto infer on more global structure rather than a ﬁxed local\nspan ( WT ). The idea behind time alteration is that a model\nthat can predict the partial loss of small speech segments\nshould provide a contextualized understanding of previous\nand future content. Our experiments show that the proposed\ntime alteration is the essential element that drives models to\nlearn bidirectional understanding, resulting in a substantial\nimprovement compared to models that are not using the time\nalteration method.\n2) Frequency Alteration: Our second pre-training method\nis frequency alteration. It is largely inspired by SpecAugment\nproposed for ASR augmentation [41], and the ASR pre-\ntraining scheme proposed in [22]. We randomly mask the\nvalues of a block of consecutive frequency bins to zero for\nall time steps across the input sequence for this alteration.\nThe block of masked frequency is selected by ﬁrst sampling\nthe width of block WC from {0,1,...,W C}uniformly. Then,\nwe sample a frequency index IC from {0,1,...,H x −Wc −1},\nwhere Hx is the number of frequency bins in input sequence− →x. The frequency bins from IC to IC +Wc −1 are those to be\nmasked. Note that the policy will mask none of the frequencies\nfor 1/(WC +1) of the time. Thus, from time to time, the model\nwill receive inputs with all of the frequency information. By\nallowing the model to receive real inputs again addresses the\ninconsistency between training and inference time.\nWe illustrate this alteration’s effect on input sequence in\nFig. 2D. Unlike the time alteration case, where we sample\n5\nmany blocks for alteration as visualized in Fig. 2B and 2C,\nwe only sample a single block for frequency alteration in\neach utterance. The reason is that acoustic sequences can be\narbitrarily long and temporally smooth [46], while there are\nonly a limited and ﬁxed number of frequencies Hx. Hence, we\nselect multiple blocks for time alteration, but only one block\nfor the frequency axis. Following the work of [22], we set\nthe maximum frequency alteration width WC to 16 frequency\nbins (20% of the 80-dimension feature). The intuition behind\nfrequency alteration is that a model that can predict the\npartial loss of frequency information should learn a high-level\nunderstanding along the frequency axis.\nAs we will show in our experiments, we ﬁnd that using fre-\nquency alteration provides a more linearly spreadable speaker\nrepresentation and a stronger speaker recognizer. Surprisingly,\nencoding speaker information through this objective does not\ncompromise phoneme classiﬁcation or ASR performance but\ninstead increases performance. This makes TERA not only\nsuitable for tasks that only require speaker information (e.g.\nspeaker recognition) and tasks that only require phonetic\ninformation (e.g. speech recognition), but also beneﬁcial for\ntasks that requires both speaker and phonetic information at\nthe same time (e.g. voice conversion [16]).\n3) Magnitude Alteration: We introduce the third method,\nmagnitude alteration, by applying sampled Gaussian noise to\naugment input sequences’ magnitude with a probability PN .\nFor PN of the time, we sample a random magnitude matrix − →z\nof dimensions Lx and Hx, which has the same shape as − →x.\nEach element in − →z is sampled from the normal distribution\nN with zero mean and 0.2 variance. We then add − →z on top\nof the real frames of − →x. We show the effect of magnitude\nalteration in Fig. 2E, where we apply magnitude alteration to\nthe original utterances − →x. By altering input magnitude, we\npotentially increase the amount of pre-training data (which is\nsimilar to the idea of data augmentation [41]). Additionally,\nmagnitude alteration offers another variation to all the ‘mask\nto zero‘ cases described in Section III-A1 (time alteration)\nand Section III-A2 (frequency alteration). We illustrate this\nnew ‘mask to noise‘ variation in Fig. 2F, where the selected\nblocks of time and frequency are now with random magnitudes\ninstead of zeros. Empirically, altering magnitude provides a\nperformance beneﬁt for all downstream tasks, thanks to the\nincreased input data variety. Also, the gain from magnitude\nalteration is additive to that of other alteration techniques.\nB. Pre-training TERA\nWe use the three alteration techniques to formulate the\nTERA self-supervised pre-training task, where the model is\nrequired to minimize the reconstruction error of acoustic\nfeatures given altered frames as input. The proposed three\nalteration methods can be used separately or used together\nas a mixture, as shown in Fig. 2. The complete pre-training\nprocess of TERA is described in Algorithm 1, where we\nshow how three alteration on data are deployed through the\nproposed stochastic policy. The stochastic policy dynamically\nsamples random pattern based on the applied alterations every\ntime we feed an input sequence to the model. We denote\nAlgorithm 1 The TERA self-supervised pre-training algorithm\nInput: Dataset X, total training steps Tsteps, time alteration\npercentage PT , time alteration width WT , frequency al-\nteration width WC, magnitude alteration percentage PN .\nOutput: Transformer Encoders parameters θenc\n1: Initialize Transformer Encoders Tenc parameters θtenc\n2: Initialize Prediction Network Pnet parameters θpnet\n3: for t= 1to Tsteps do\n4: Sample speech utterance − →x from X\n5: if apply time alteration then\n6: Lx = number of frames in − →x\n7: Tnum = ⌊PT ×Lx ÷WT ⌉\n8: IT = Sample Tnum amount of starting indexes\n9: Sample value v from uniform distribution U(0,1)\n10: if v <0.8 then\n11: for all it in IT do\n12: ˆx ←mask WT number of consecutive frames\nin − →x to zero\n13: end for\n14: else if v >= 0.8 and v <0.9 then\n15: for all it in IT do\n16: ˆx ← replace WT number of consecutive\nframes in − →x with randomly sampled consecu-\ntive frames from the same utterance\n17: end for\n18: else\n19: Do nothing, ˆx←− →x\n20: end if\n21: end if\n22: if apply frequency alteration then\n23: if apply time alteration then\n24: Apply on top of previous alteration, − →x ←ˆx\n25: end if\n26: Wc = Sample a frequency width from U(0,WC)\n27: Hx = number of frequencies in − →x\n28: IC = Sample a frequency index from U(0,Hx −Wc)\n29: ˆx ←starting from IC, mask Wc consecutive fre-\nquency bins in − →x to zero\n30: end if\n31: if apply magnitude alteration then\n32: if apply time or frequency alteration then\n33: Apply on top of previous alterations, − →x ←ˆx\n34: end if\n35: Sample value v from uniform distribution U(0,1)\n36: if v <PN then\n37: Sample magnitude vector − →z from N(0,0.2)\n38: Apply noise ˆx←− →x + − →z to alter magnitude\n39: else\n40: Do nothing, ˆx←− →x\n41: end if\n42: end if\n43: Compute L1 reconstruction loss to update:\nLrec(θtenc,θpnet) =\n− →x −Pnet(Tenc(ˆx))\n\n1 (2)\n44: end for\n6\nthe altered input as ˆx. In the case where multiple alterations\nare applied on the same input, − →x is used again to represent\ncarried-on alterations. After input alteration, we feed ˆx into\nthe Transformer Encoders Tenc and the prediction network\nPnet. The architecture of Pnet is consist of a 2-layer feed-\nforward network with a hidden size of 768. For Tenc, we\nuse Transformer Encoders with a hidden size of 768, number\nof self-attention heads as 12, dropout rate of 0.1, and the\nintermediate feed-forward layer hidden size as 3072. We\nprimarily report results on three model sizes: base (Layer=3,\nParameters=21.3M), medium (Layer=6, Parameters=42.6M),\nand large (Layer=12, Parameters=85.1M). The Transformer\nEncoders Tenc and the prediction network Pnet are connected\nto reconstruct − →x from ˆx.\nL1 reconstruction loss is then computed between input− →x and network output from Pnet to update the network\nparameters θtenc and θpnet. We use gradient descent training\nwith mini-batches of size 32 to ﬁnd model parameters that\nminimize the L1 loss under the self-supervised pre-training\ntask. We employ the AdamW optimizer [47] for updating\nmodel parameters, where the learning rate is warmed up over\nthe ﬁrst 7% of total training steps Tsteps to a peak value\nof 2e−4 and then linearly decayed. Our pre-training setup\ncan be accommodated in a single 1080Ti GPU with 11GB\nof memory. Computational efﬁciency allows interested parties\nto easily train our model with their data without massive\ncomputational resources. The models are trained with ﬁxed\ntotal training steps Tsteps (details in Section IV-A). After pre-\ntraining, the parameters θtenc of the Transformer Encoders\nTenc are retained for downstream tasks, while the prediction\nnetwork Pnet is discarded, as illustrated in Figure 1.\nIn this work, our models’ input is an 80-dimensional\nlog Mel spectrogram if not speciﬁed otherwise. We also\nexplore pre-training with other acoustic features, including\n39-dimensional MFCC, 80-dimensional FBANK, and 40-\ndimensional fMLLR. We extract all of the features with a\nwindow of 25 ms and a stride of 10 ms. We apply per-\nutterance CMVN (cepstral mean and variance normalization)\nto the features. We set the total pre-training steps Tsteps of\nTERA as 200k and 1M for 100 hours and 960 hours of pre-\ntraining data, respectively.\nC. Incorporating with Downstream Tasks\nWe investigate different ways to incorporate the learned\nTERA model to downstream tasks.\n1) Representation Extraction: We ﬁrst use the weighted\nsum technique to investigate which layer is best for represen-\ntation extraction. We use a learnable weighted sum to integrate\nhidden states from all layers of the self-supervised model when\ntraining with downstream tasks, similar to the ELMO [36]\napproach. We then analyze the learned weights and ﬁnd that\nfor TERA, APC [8], [9], and vq-wav2vec [3], the weighted\nsum always favors the last layer the most (we investigate with\nphoneme and speaker classiﬁcation tasks). Hence in this work,\nwe extract representations from TERA’s deepest layer, which\nis essentially the hidden states of the last Transformer En-\ncoder layer. The extracted representation is fed to downstream\nclassiﬁers as input and replacing surface features. Parameters\nof TERA are frozen when training downstream tasks in this\napproach. In later experiments, we use this approach if not\nspeciﬁed otherwise.\n2) Fine-tuning: The second approach is to ﬁne-tune the\nTERA model with downstream models. Here the output of\nTERA is connected to a downstream model of any kind, as\nillustrated in Fig. 1. We then update the pre-trained TERA to-\ngether with random initialized downstream models. We denote\nthis approach as ﬁne-tune to distinguish from representation\nextraction in later experimental tables and ﬁgures.\nIV. E XPERIMENTAL SETUP\nWe use four downstream tasks for evaluation: phoneme\nclassiﬁcation, speaker recognition, keyword spotting, and\nspeech recognition. We use publicly known and available set-\ntings for our downstream tasks to link our works with previous\nones and allow easier comparison. For all experiments, we\ntrain with ﬁxed random seeds for consistency.\nA. Datasets\nWe use a total of three datasets. We consider three subsets\nof LibriSpeech for the pre-training of TERA: the train-clean-\n100, the train-clean-360, and the train-other-500 subset. The\nthree subsets add up to a total of 960 hours of data. We\nalso use the LibriSpeech [45] dataset for downstream tasks of\nphone classiﬁcation, speaker recognition, and speech recog-\nnition. The train-clean-100 subset is used in the phoneme\nclassiﬁcation and speaker recognition task. For ASR, we use\nthe train-clean-100 as labeled data, and the dev-clean and test-\nclean subset are used for evaluation. We use the TIMIT [48]\ndataset to evaluate the transferability of pre-trained models.\nWe do not use the TIMIT dataset for pre-training, but we\nuse it for downstream phoneme classiﬁcation and ASR tasks.\nWe consider two subsets of TIMIT: the training set and\nthe complete test set. As is usually done, we derive the\ndevelopment set and the core test set from the complete test\nset, according to the Kaldi [49] TIMIT recipe and [50]. We\nreport results by training downstream tasks on the training set\nand evaluating with the development and core test sets. For\nkeyword spotting, we use the Speech Commands [51] dataset.\nWe consider two subsets of Speech Commands: the training\nset and the test set. We derive the development set from the\nvalidation list provided in Speech Commands [51]. We report\nresults by training keyword spotting models on the training\nset, and evaluating on the development set and test set. The\nSpeech Commands dataset is also not used for pre-training.\nB. Phoneme Classiﬁcation Setup\n1) Phoneme Classiﬁcation on LibriSpeech: We measure the\nframe-wise phoneme prediction performance with classiﬁers\ntrained on top of representations. Following previous work [1],\n[7], we adapt the common setup using 41 possible phoneme\nclasses and the train-clean-100 subset of LibriSpeech [45].\nTo make the comparison as fair as possible, we use aligned\nphoneme labels and train/test split provided in the CPC [1]\n7\nand Modiﬁed CPC [7] literature. We derive 10% of the data\nfrom the training split and use it as the development set.\nFollowing the previous work [1], we utilize linear classiﬁers\nto measure the linear separability of phonemes. We denote\nthese classiﬁers as linear. Additionally, following the work\nof Modiﬁed CPC [7], we also report results from performing\nlinear classiﬁcation with a concatenation of 8 windows, which\nmatches the average length of a phoneme. We denote this type\nof classiﬁer setting as linear concat. As not all the information\nencoded is linearly accessible, in addition to measuring linear\nseparability, we also use classiﬁers with one single hidden\nlayer, following the same settings as in [1]. We denote such\nsetting as 1-hidden.\n2) Phoneme Classiﬁcation on TIMIT: For TIMIT, we ob-\ntain a phoneme label for each frame from the manual phoneme\ntranscriptions provided in TIMIT. We measure accuracy after\nmapping the 48 phonemes to the smaller set of 39 phoneme\nclasses, as is customary for TIMIT since [52] (the same\napplies to ASR on TIMIT). Following the classiﬁer settings\nfor LibriSpeech, we also use the linear, linear concat, and 1-\nhidden classiﬁers on TIMIT. For both LibriSpeech and TIMIT,\nwe report phoneme classiﬁcation accuracy (%) on the test set.\nWe investigate the domain shift issue with this setup, where\nmodels are pre-trained on LibriSpeech then used on TIMIT\nfor the downstream task.\nC. Keyword Spotting Setup\nWe evaluate representations on the keyword spotting task.\nWe follow the standard setup [51], where the keyword spotting\ntask is a 12-class classiﬁcation problem. The test set provides\nequal numbers of instances for each class; hence class accounts\nfor approximately 8.3%. To probe representations, we did not\nuse complex models for this task, as performance may saturate.\nInstead, we use a two hidden layer feed-forward classiﬁer, with\nmean pooling overtime applied just before the output layer.\nWe report keyword classiﬁcation accuracy (%) on the test set.\nThis setup allows us to study the domain shift issue, where\nmodels are pre-trained on LibriSpeech then used on Speech\nCommands for the detection task.\nD. Speaker Classiﬁcation Setup\nWe evaluate representations on speaker prediction tasks.\nFollowing the common experiment setting in [1], [20], [21],\nwe adopt the LibriSpeech [45] train-clean-100 subset, which\nconsists of 251 speakers. We use the same train/test split as\nprovided in [1]. We evaluate the pre-trained models with two\ntypes of speaker classiﬁcation tasks, frame-wise linear classiﬁ-\ncation, and utterance-wise linear classiﬁcation. For frame-wise\nspeaker classiﬁcation, the classiﬁer predicts the speaker for\neach input frame. We denote this experiment setting as speaker\nframe. As for utterance-wise classiﬁcation, we average the\nrepresentation of each utterance over time. Then the classiﬁer\npredicts speaker identity conditioning on the averaged vector.\nWe denote this setting as speaker utter . For both settings,\nwe employ a simple linear classiﬁcation model. In [1], [21],\nthey also investigate these two speaker classiﬁcation settings.\nIn general, the speaker frame classiﬁcation task is more\ndifﬁcult than speaker utter , however speaker utter is a more\ncommon scenario for speaker classiﬁcation. Also, good results\nin speaker frame always imply good results in speaker utter,\nbut not the other way around. Hence we report both speaker\nframe and speaker utter for completeness. For both tasks, we\nreport speaker classiﬁcation accuracy (%) on the test set.\nE. Hybrid DNN/HMM ASR Setup\nWe evaluate the performance of ASR models on top of\nrepresentations. We employ the Hybrid DNN/HMM ASR\nmodeling implemented with the PyTorch-Kaldi [53] toolkit.\nWe investigate two types of DNN settings, MLP and liGRU.\nMLP is a simple single-layer multilayer perceptron model.\nliGRU is a 5-layer light-gated recurrent unit followed by 2-\nlayers of fully-connected network. In ﬁrst-pass decoding, we\nuse a 4-gram language model with a beam-search algorithm.\nThe WER score is computed with the NIST SCTK scoring\ntoolkit [53]. For lattice rescoring, we use the Kaldi Con-\nstArpaLm format language model. The decoding and rescoring\nscripts are inherited from the Kaldi s5 recipe [49].\nWhen we utilize TERA as a representation extractor, we\nfeed the output of TERA to liGRU or MLP and freeze the\nparameters of TERA during training. As for ﬁne-tuning TERA,\nwe update the TERA model with liGRU or MLP as part of\nthe DNN component in the hybrid ASR framework. For our\nASR experiments, we pre-train TERA on 40-dimensional fM-\nLLR [54] features instead of 80-dimensional log Mel features\nsince the PyTorch-Kaldi ASR toolkit [53] works best with\nfMLLR inputs. To highlight the effect of pre-training, we use a\nlimited amount of labeled data, i.e., the train-clean-100 subset,\nfor supervised ASR training. Hyperparameters are tuned on the\ndev-clean subset, and testing results measured from the test-\nclean subset are reported. We report ASR modeling results of\nLibriSpeech in terms of WER. In addition to evaluating with\nLibriSpeech, we also benchmark ASR results with TIMIT,\nwhere we measure PER. With this setup, we investigate the\ndomain shift issue, where models are pre-trained using data in\nthe domains different than the downstream tasks.\nF . Training Downstream Tasks\nFor both representation extraction and ﬁne-tuning, we use\nthe AdamW [47] optimizer to update models when training\nwith downstream tasks. We use a learning rate of 2e−4 with a\nbatch size of 16. When applying representation extraction for\nASR tasks, we use the RMSPROP optimizer with a learning\nrate of 2e−4. We half the learning rate every epoch if the\ndevelopment set error does not drop more than a threshold of\n0.001. We use a batch size of 16, and update for 24 epochs.\nWe use the same setting as above for ﬁne-tuning on ASR,\nexcept we use a different learning rate for each TERA model.\nLearning rate is set to 2e−4 when we ﬁne-tune base, 1e−4\nfor medium, and 5e−5 for large. Empirically, we ﬁnd that\nlarger models require a lower learning rate during ﬁne-tuning.\nHence, we half the learning rate every time the model depth\nis doubled; this helps stabilize the entire ﬁne-tuning process.\nDuring downstream ﬁne-tuning, we also apply the SpecAug-\nment [41] LD policy. During SpecAugment, the chosen (and\n8\nlinear linear concat 1-hidden\n40\n60\n80\nAccuracy (%)\n45.8\n56.5 53.7\n63.5\n71.0 71.7\n63.0\n70.6 71.770.4 75.0 80.2\n69.6 74.7 79.6\n70.8 75.8 79.8\n72.1 76.9 80.8\nPhoneme classiﬁcation on seen data - LibriSpeech (41-classes)\nlinear linear concat 1-hidden\n50\n60\n70\n80\nAccuracy (%)\n50.3\n58.2\n52.5\n63.3\n68.3 64.963.3\n68.3 65.4\n70.8 72.2 73.370.1 72.3 72.570.5 72.7 73.071.5 73.4 73.9\nPhoneme classiﬁcation on unseen data - TIMIT (39-classes)\nkeyword spotting (unseen) speaker frame (seen) speaker utter (seen)\n60\n80\n100Accuracy (%)\n82.2\n53.8\n79.386.6\n99.8 99.9\n88.4\n99.9 99.9\n91.3 98.1 99.0\n90.5\n99.9 99.9\n89.0\n99.8 99.7\n90.5\n99.7 99.7\nClassiﬁcation on more speech downstream tasks\nmag freq freq+mag time time+mag time+freq time+freq+mag\nFig. 3: The effect of different alterations. We use phoneme classiﬁcation, keyword spotting, and speaker recognition on both seen and\nunseen data to study the effect of different alteration methods. All results are base models pre-trained on the LibriSpeech train-clean-100\nsubset, testing accuracy on different types of classiﬁers are reported. Methods with unidirectional context (without time alteration) are in\ndotted color, methods with bidirectional context (with time alteration) are in solid color.\npossibly overlapping) time and frequency blocks are zeroed\nout. We omit the use of time warping as masking provides\nenough regularization. We ﬁnd SpecAugment is additive to\nthe proposed pre-training approach, as it delays overﬁtting and\nimproves the ﬁnal accuracy numbers in the downstream tasks.\nV. R ESULTS\nIn Section V-A, we study the effect of applying different\ncombinations of time, frequency, and magnitude alteration. In\nSection V-B, we compare TERA with recent self-supervised\nrepresentation methods on downstream tasks. In Section V-C,\nwe study multiple aspects of the TERA pre-training process,\nincluding ﬁne-tuning with downstream models, the amount\nof unlabeled data, the effect of pre-training on various fea-\ntures, various model sizes, and different masking policies.\nIn Section V-D, we compare TERA with approaches where\nwe train ASR models on frozen speech representations. In\nSection V-E, TERA is compared with approaches that ﬁne-\ntune their pre-trained models. In Section V-F, we demonstrate\ntransferring TERA representations from one dataset to another\nby presenting ASR PER on TIMIT [48].\nA. The Effect of Different Alterations\nWith three types of alterations, there are a total of seven\ncombinations, namely ”time”, ”freq”, ”mag”, ”time+freq”,\n”time+mag”, ”freq+mag”, and ”time+freq+mag”. We pre-\ntrain TERA with all seven combinations of alterations on 100\nhours of LibriSpeech, then measure the learned representations\nwith downstream tasks. The results are presented in Figure 3.\nWe use TERA for representation extraction (from the last\nlayer), i.e., TERA parameters were frozen when adapting\ndownstream models. We split our methods into two categories:\none with the time alteration that allows the model to encode\nbidirectional information (denoted in a solid color). The sec-\nond category is the models that don’t include the time alter-\nation method (denoted in dotted color). The conclusions made\nin the following sections are statistically signiﬁcant, where we\nmeasured p-values with statistical signiﬁcance tests (paired\nsamples t-test or Fisher’s exact test). Overall, applying all\nalterations ”time+freq+mag” achieves the best performance\non average (with an averaged p-value of 0.0492 across nine\ntasks when compared to ”time”). Now, we discuss the effect\nof each alteration in detail.\n1) The Effect of Time Alteration: We discuss the effect\nof time alteration for each downstream task. For phoneme\nclassiﬁcation, the models with time alteration (in solid color)\nperform reasonably well, while the models without time\nalteration (in dotted color) perform relatively poorly. The\nresults of seen data (LibriSpeech) and unseen data (TIMIT)\nagree with each other and yield similar trends. For keyword\nspotting, the ”time” model achieves the best performance.\nAdding other alterations compromise the performance. The\nmodels with time alteration perform better than the models\nthat do not have time alteration. This observation suggests\nthat bidirectional understanding is an essential aspect of de-\ntection tasks. For speaker recognition, the existence of time\nalteration does not compromise the performance. With time\nalteration alone, the ”time” model achieves slightly lower but\n9\nRepresentation Network #Params Input Feature Pre-train Learning Style\nMFCC - 0 13-dim + delta 2 - -\nFBANK - 0 80-dim + delta 2 - -\nlog Mel - 0 80-dim - -\nAPC [8], [9] 3-GRU 4,064,256 80-dim log Mel LibriSpeech 360 hr autoregressive reconstruction\nVQ-APC [11] 3-GRU 4,064,256 80-dim log Mel LibriSpeech 360 hr autoregressive reconstruction + VQ\nCPC [1], [7] 5-Conv 1-Trans 1,843,456 waveform LibriLight 60k hr contrastive\nvq-wav2vec [3] 20-Conv 6,042,624 waveform LibriSpeech 960 hr contrastive + VQ\nwav2vec 2.0 [5] 7-Conv 24-Trans 214,658,176 waveform LibriSpeech 960 hr contrastive + latent masking\nMockingjay [20] 3-Trans 21,327,360 80-dim log Mel LibriSpeech 960 hr time-only masked reconstruction\nAudio ALBERT [21] 3-Trans 7,151,616 80-dim log Mel LibriSpeech 960 hr time-only masked reconstruction\nNPC [27] 3-Conv 19,273,728 80-dim log Mel LibriSpeech 360 hr time-only masked reconstruction\nTERA (Ours) 3-Trans 21,327,360 80-dim log Mel LibriSpeech 960 hr ”time+freq+mag” alteration\nTABLE I: Details of baseline features and recent speech representation approaches. We evaluate their performance on downstream\ntasks and present the results in Table II.\nsatisfactory performance. From the above discussion, we thus\ndeem the time alteration to be indispensable. We speculate that\nalthough Transformer Encoders [28] are bidirectional due to\ntheir multi-head self-attention, without the time alteration, the\nnetwork fails to encode proper context and yield sub-optimal\nperformance. However, through alteration on the time axis, the\nmodel establishes a bidirectional understanding of the audio\nand gives better results.\n2) The Effect of Frequency Alteration: For the phoneme\nclassiﬁcation tasks, adding the frequency alteration during pre-\ntraining is effective in boosting performance. The ”time+freq”\nand ”time+freq+mag” model improves over the ”time” model\nfor both seen and unseen data. For keyword spotting, adding\nthe frequency alteration does not help. For speaker recognition,\nwe ﬁnd that the model can achieve strong speaker classiﬁ-\ncation performance by employing the frequency alteration. The\n”freq” model, with frequency alteration alone, is enough to\nachieve high performance. The ”mag” and ”time” models,\nwithout frequency alteration, have lower performance. By\nemploying the frequency alteration, models beneﬁt in the\nphoneme classiﬁcation and speaker recognition performance,\nwhich is surprising and counter-intuitive. It is well known that\na robust phonetic system should be speaker invariant [16].\nWe surmise this phenomenon results from that TERA repre-\nsentations provide more accessibility to phonetic and speaker\ninformation and maintain the separability between the two\ntypes of information. Downstream models can efﬁciently learn\nto extract task-speciﬁc information. To sum up this section, the\nfrequency alteration effectively encodes speaker identity while\nalso ﬁne grinding the phonetic quality.\n3) The Effect of Magnitude Alteration: For the phoneme\nclassiﬁcation tasks, adding the magnitude alteration is effec-\ntive in boosting performance. The ”time+freq+mag” model\nimproves over the ”time+freq” model for both seen and\nunseen data. For keyword spotting, adding the magnitude\nalteration improves most cases but does not improve over time\nalteration alone. For speaker recognition, adding the magnitude\nalteration improves performance for all cases. We observe\nthat by applying the magnitude alteration during pre-training,\nthe model learns to be robust to various inputs. As a result,\nwe improve downstream tasks performance by adding the\nmagnitude alteration.\nB. Comparison of Recent Speech Representation Approaches\nIn this section, we compare TERA with recent speech rep-\nresentation learning methods through evaluating with down-\nstream tasks. We select eight different methods, which cover\na wide range of representation learning techniques. We list\nthese methods and their details in Table I. We organize Table I\nby ﬁrst grouping methods with similar learning styles, then\nby their publication date in the order of ﬁrst to last. We use\npublicly available pre-trained weights or pre-training scripts\nprovided by their original authors for all of the listed models.\nNote that all models are pre-trained on LibriSpeech [45] except\nfor CPC [1], [7], which is pre-trained on LibriLight [55]. All of\nthe models and baseline features have a stride of 10 ms, with\nonly one exception where wav2vec 2.0 [5] has a downsample\nrate of 320, which generates a representation every 20 ms.\nInterestingly, all of the methods that employ reconstruction\nloss in their learning objective use an 80-dim log Mel input\nfeature. We also use 80-dim log Mel for consistency and\nfair comparison. On the other hand, all methods that employ\ncontrastive loss in their learning objective use raw waveform\nas an input feature. In Table I, we also list the details of\nbaseline features. We present the performance of different\nrepresentations on downstream tasks in Table II. The pre-\ntrained models are all used for representation extraction (from\nthe last layer), i.e., we freeze parameters when adopting the\ndownstream model and feed representations to the downstream\nmodel as input. In the last column, we average the accuracy of\nall tasks across each row. The above results can be reproduced\nwith the S3PRL toolkit 1, where all the self-supervised models\nand downstream tasks are available with easy-to-use setups.\n1) Phoneme Classiﬁcation Results: For phoneme classiﬁ-\ncation on seen data (LibriSpeech), TERA outperforms other\nmethods. For phoneme classiﬁcation on unseen data (TIMIT),\nTERA outperforms other methods except for CPC on the\nlinear concat classiﬁer. Some representations encounter a\nmore signiﬁcant degradation in performance for unseen data.\nOn the other hand, TERA is not affected by the domain\nchange. In particular, we ﬁnd that adding the TERA magnitude\nalteration helps performance for unseen data. As a result,\nby adding magnitude alteration, we increase accuracy for\nphoneme classiﬁcation on TIMIT. We observe that all rep-\nresentations beneﬁt from concatenating neighboring frames,\nas their performance on linear concat is better than linear.\nThe linear concat classiﬁer provided neighboring informa-\ntion through the concatenated frames. Several representations\n10\nRepresentation Phoneme - LibriSpeech Phoneme - TIMIT Keyword Speaker Averagelinear linear concat 1-hidden linear linear concat 1-hidden spotting frame utter\nMFCC 47.88 51.46 62.47 54.74 58.55 65.52 87.99 13.18 90.61 59.16\nFBANK 48.01 52.16 63.38 55.00 58.35 64.52 86.01 31.35 94.70 61.50\nlog Mel 41.93 49.94 48.55 47.37 56.04 51.05 72.35 22.38 95.33 53.88\nAPC [8], [9] 72.76 79.62 78.06 72.90 76.74 74.51 91.04 79.08 99.63 80.48\nVQ-APC [11] 71.92 79.08 77.62 72.06 75.98 74.11 92.92 68.56 99.21 79.05\nCPC [1], [7] 71.31 78.52 77.40 73.10 77.71 75.57 94.16 75.47 99.32 80.28\nvq-wav2vec [3] 63.38 76.53 66.18 67.52 76.42 68.68 94.55 24.76 84.09 69.12\nwav2vec 2.0 [5] 56.39 72.72 73.61 59.32 71.76 69.14 68.87 82.53 95.40 72.19\nMockingjay [20] 67.51 72.07 78.70 69.21 71.68 72.53 88.09 97.22 98.35 79.48\nAudio ALBERT [21] 68.13 72.62 78.65 69.62 71.37 72.68 90.59 96.65 98.37 79.85\nNPC [27] 67.32 76.08 75.35 65.76 72.96 68.42 87.41 27.72 96.16 70.80\nTERA time+freq 74.45 78.84 82.14 73.92 75.80 75.98 91.89 99.57 99.59 83.58\n+mag 74.07 78.67 81.90 74.14 75.92 76.23 92.60 99.47 99.48 83.61\nTABLE II: Performance of baseline features and recent speech representation approaches. Representations listed in Table I are evaluated\non the four downstream tasks. We report testing accuracy (%) and highlight the highest score for each column in bold font.\nMethod Phoneme - LibriSpeech Phoneme - TIMIT Keyword Speaker Averagelinear linear concat 1-hidden linear linear concat 1-hidden spotting frame utter\nTERA 74.07 78.67 81.90 74.14 75.92 76.23 92.60 99.47 99.48 83.61\n+ ﬁne-tune 89.09 89.07 89.53 78.81 78.86 78.37 94.03 99.50 99.62 88.54\n+ SpecAug 90.36 90.07 90.88 79.50 78.69 78.82 93.74 99.81 99.86 89.08\nrandom init + SpecAug 88.70 89.49 89.08 70.10 68.09 71.57 8.34 0.424 1.67 54.16\nTABLE III: Performance of ﬁne-tuning TERA and baseline approaches. Here we use the TERA-base ”time+freq+mag” model, row\none is identical to the last row of Table II. We report testing accuracy (%) and highlight the highest score for each column in bold font.\nbeneﬁt more from linear concat than others, depending on\nhow much temporal information the single feature frame\nalready encodes. We also observe that all representations\nbeneﬁt from a deeper downstream model, as their performance\non 1-hidden is better than linear. The 1-hidden classiﬁer is\nable to extract more information than linear classiﬁers. The\nwav2vec 2.0 method achieves the lowest accuracy for phoneme\nclassiﬁcation, which contrasts its high performance when used\nas an initialization for ASR encoder ﬁne-tuning [5]. The under-\nperforming of wav2vec 2.0 here suggests that although large\nand deep models are suitable for downstream initialization\nand ﬁne-tuning, they may not be a good choice for feature\nextraction. Our later experiment shows that TERA-large under-\nperform TERA-base for feature extraction, but TERA-large\noutperforms TERA-base for ASR ﬁne-tuning.\n2) Keyword Spotting Results: For keyword spotting, all\nrepresentations achieve a good score. However, wav2vec 2.0\nand NPC are not able to surpass the performance of baseline\nfeatures. The wav2vec 2.0 method achieves the lowest score,\nwhich again suggests that pre-trained models may work well\nfor ﬁne-tuning, but it does not always imply that they can\ngenerate meaningful representations. The NPC method and\nMockingjay achieve comparable performance, where Mock-\ningjay barely exceeds the performance of MFCC. By adding\nfrequency alteration and magnitude alteration to Mockingjay,\nTERA can boost the performance by 4.51% over Mockingjay.\nThe best performing representation on keyword spotting is\nvq-wav2vec, which is surprising as it does not perform as\nwell as the others on phoneme classiﬁcation and speaker\nrecognition. CPC achieves comparable performance with vq-\nwav2vec, suggesting that contrastive learning over time may be\nthe key for detection tasks. Other than CPC and vq-wav2vec,\nVQ-APC and TERA achieve high scores on this task too.\n3) Speaker Recognition Results: For the speaker recogni-\ntion task, TERA achieves the highest speaker classiﬁcation\naccuracy for both the frame and utter setting. We can observe\na discriminative comparison under the frame setting. Methods\nthat use autoregressive, contrastive, vector-quantization have\na lower frame-wise speaker recognition accuracy. The autore-\ngressive prediction method focuses more on the local depen-\ndencies between time steps. Also, contrastive learning discrim-\ninates input from a distant time, and vector-quantization is\na bottleneck that limits information ﬂow over the network.\nThe above techniques seem to have encouraged the model\nnot to encode speaker information in each frame, resulting\nin poor frame classiﬁcation performance. Although the NPC\ntechnique uses time-masked reconstruction like Mockingjay\nand Audio ALBERT, it performs poorly for the frame setting.\nWe speculate that this is because Masked Convolution Blocks’\nmasking is ﬁxed and designed to focus on local dependencies.\nAs a result, each representation doesn’t preserve speaker\ninformation. On the other hand, the time mask of Mockingjay,\nAudio ALBERT, and TERA is applied through dynamic\nmasking [20], [21], where masks may overlap each other,\ncreating various time mask lengths. The overlapped masks\nencourage the model to also focus on global information. As\nspeaker characteristics tend to persist across time, this thus\npreserves the speaker information. For the utter setting of\nspeaker recognition, all of the representations yield satisfactory\nresults except for vq-wav2vec. However, remember that vq-\nwav2vec achieves the highest score on keyword spotting. We\nconclude that there is a trade-off for some representation\nlearning methods, vq-wav2vec is good on keyword spotting\nbut lacks speaker information.\nC. Analysis on TERA\nTo better understand how TERA derives representation from\nspeech, we study various aspects of TERA’s pre-training.\nWe ﬁrst investigate how TERA can beneﬁt from ﬁne-tuning,\nwhere we present results in Table III. Next, we investigate\n11\nlinear linear concat 1-hidden\n40\n60\n80\nAccuracy (%)\n63.5 69.6\n76.7\n64.0\n70.5\n77.5\n70.8 75.8 79.8\n65.2\n72.0 76.271.0 76.9 80.074.5 78.8 82.1\n66.2\n73.5 73.9\nPhoneme classiﬁcation on seen data - LibriSpeech (41-classes)\nlinear linear concat 1-hidden\n50\n60\n70\n80\nAccuracy (%)\n66.8 70.1 72.9\n67.9 70.5 72.470.5 72.7 73.0\n66.7 70.3 70.671.7 74.8 73.673.9 75.8 76.0\n67.6 72.2 69.3\nPhoneme classiﬁcation on unseen data - TIMIT (39-classes)\nkeyword spotting (unseen) speaker frame (seen) speaker utter (seen)\n60\n80\n100Accuracy (%)\n90.6 95.4 98.3\n90.3 96.7 98.7\n89.0\n99.8 99.7\n90.8\n98.8 99.1\n91.5\n99.5 99.6\n91.9\n99.6 99.6\n88.6\n98.5 99.0\nClassiﬁcation on more speech downstream tasks\nbase MFCC base FBANK base 100hr large medium base 960hr SpecAug\nFig. 4: Analysis on TERA. We pre-train TERA with different amount of unlabeled data, with different features, and with different model\ndepth. Moreover, we directly apply the SpecAugment [41] LD Policy for pre-training instead of TERA’s alteration, we get performance\ndegrade. The models trained with 100 hours are denoted in red, and the models trained with 960 hours are in denoted blue.\nthe effect of increasing the amount of pre-training data. Also,\nwe study the effect of learning with various acoustic features.\nFurthermore, we investigate how the network depth (number\nof layers) affects TERA’s performance. Finally, we study the\ndifference between directly applying SpecAugment and the\nTERA time and frequency alteration. We intentionally did not\napply the magnitude alteration here to rule out the variation.\nWe visualize results in Figure 4. We denote models trained on\n100 hours of LibriSpeech in red and denote models trained\non 960 hours of LibriSpeech in blue. All TERA models are\npre-trained with time and frequency alteration. We freeze all\nmodels for representation extraction from the last layer.\n1) Fine-tuning TERA: In Table III, we show the effect\nof ﬁne-tuning the pre-trained TERA with downstream tasks.\nIn the last column, we average the accuracy of all tasks\nacross each row. We ﬁne-tune TERA ”time+freq+mag”, the\nbest performing TERA-base model, which is pre-trained on\n960 hours of LibriSpeech. In the ﬁrst row of Table III,\nwe include the results from the last row of Table II of\nthe frozen TERA ”time+freq+mag” model to link the two\ntables. With ﬁne-tuning, we see considerable performance\nincreases for all tasks. By adding SpecAugment [41] during\nﬁne-tune, we improve performance for some tasks. However,\nthe improvement of adding SpecAugment during ﬁne-tuning is\nlimited because the pre-trained weights are good initialization\nfor the tasks. Empirically, we ﬁnd that when compared to\ntraining downstream classiﬁers on frozen TERA, ﬁne-tuning\nTERA with downstream tasks dramatically reduces the number\nof training steps for the downstream models to converge.\nWe also observe that ﬁne-tuning for phoneme classiﬁcation\non LibriSpeech brings a more signiﬁcant improvement than\non TIMIT. The reason is that there are more labeled data\nfor LibriSpeech. It is worth noticing that ﬁne-tuning with\nlimited label data (TIMIT) still beneﬁts performance, where\nwe observe no sign of overﬁtting.\nWe train the same TERA network (with randomly initial-\nized parameters) on downstream tasks, where we distort the\ninput features with SpecAugment to prevent overﬁtting. The\nabove setting serves as a baseline for ﬁne-tuning pre-trained\nTERA, and we denote it as random init + SpecAug in Ta-\nble III. Overall, the downstream models trained with randomly\ninitialized TERA and SpecAugment either underperform or\nperform poorly. The baseline random init + SpecAug results\nof phoneme classiﬁcation on LibriSpeech are close to the\npre-trained models because the amount of label is sufﬁcient.\nHowever, there is a performance gap between baseline results\nand pre-trained models on phoneme classiﬁcation for TIMIT.\nThe reason is the limited amount of label data. The baseline\nrandom init + SpecAug overﬁts for the keyword spotting and\nspeaker recognition task. Note that the reported accuracy is\nfrom the test set; therefore, a low testing score means severe\noverﬁtting on the training set. It does not indicate that the\nnetwork is untrainable. We conclude that without pre-training,\nmodel architecture alone provides no beneﬁt.\n2) Pre-training on More Data: Unsurprisingly, pre-training\nTERA on more data increases the performance of all down-\nstream tasks. In Figure 4, base 960hr improves signiﬁcantly\nover base 100hr . Presumably, all pre-trained models should\nbeneﬁt from more unlabeled data. However, we ﬁnd that this is\nnot the case for all methods. In particular, we ﬁnd that models\n12\nlinear linear concat 1-hidden speaker frame speaker utter\n40\n60\n80\n100Accuracy (%)\n67 76 75\n28\n96\n66 75 74\n28\n95\n70 75 79\n98 99\n68 72 79\n97 98\n71 76 80\n100 100\n74 79 82\n100 100\nClassiﬁcation on seen data - LibriSpeech\nlinear linear concat 1-hidden keyword spotting\n60\n70\n80\n90\n100Accuracy (%)\n66\n73 68\n87\n65\n73\n68\n93\n71 72 73\n91\n69 72 73\n88\n71 73 73\n89\n74 76 76\n92\nClassiﬁcation on unseen data - TIMIT / Speech Commands\nNPC 360hr NPC 960hr Mockingjay 100hr Mockingjay 960hr TERA 100hr TERA 960hr\nFig. 5: Pre-training on more data. We pre-train different masked reconstruction models on a different amount of unlabeled data. We\nobserve that NPC and Mockingjay got worse performance as we add noisy data (LibriSpeech train-other-500) during pre-train, mainly due\nto the reconstruction nature that remembers everything. TERA, on the other hand, does not suffer from this issue.\ntrained from time-only masked reconstruction (See Table I)\nmay not improve by training on more data, especially when\nthe added data is unclean. To be more speciﬁc, we ﬁnd that\nMockingjay [20] and NPC [27] got worse performance when\nwe add train-other-500 to the pre-training data. The Mock-\ningjay method uses dynamic masking on data along the time-\naxis. The NPC method uses a mask in its convolution module\nto achieve masking on the time-axis. These methods use the\nidea of reconstructing a masked time span from surrounding\ncontext, which is unlike other pre-training methods such as\nthe line of work of APC [8]–[11] that uses autoregressive\nprediction and the line of work of CPC [1]–[7] that uses\ncontrastive learning. The time-only masked reconstruction\nprocess forces the model to remember all the information\nof speech, including speaker characteristics and other noise.\nHowever, with the regularization of frequency masking, the\nTERA representations do not exhibit this weakness.\nThis observation is presented in Figure 5, where NPC [27],\nMockingjay [20], and TERA ”time+freq” are pre-trained with\nincreasing amounts of unlabeled data. We evaluate the learned\nrepresentations on downstream tasks. For all tasks and all\ntypes of classiﬁers, both NPC and Mockingjay got worse\nperformance when we add the train-other-500 subset for pre-\ntrain. There is only one exception with NPC, where it improves\non the keyword spotting task by pre-training on more data. On\nthe other hand, TERA can beneﬁt from training on noisy data.\nAs a result, TERA improves for all tasks by pre-training on\nmore data.\nWe want to point out that pre-training on more data has not\nbeen explored in NPC and Mockingjay, where they only report\nresults with models pre-trained on 360 hours of LibriSpeech.\nWe believe this is a unique phenomenon for methods that learn\nfrom the time-only masked reconstruction. To the best of our\nknowledge, only NPC and Mockingjay examine this kind of\nbehavior. Note that we pre-train Mockingjay and TERA with\nthe same implementation 1 and setting. The only difference is\nby adding the frequency masking.\n3) Learning with Different Acoustic Features: In Figure 4,\nwe pre-trained TERA with MFCC and FBANK as both input\nand output targets, instead of log Mel. The setting of these\nacoustic features is identical to the ones listed in Table I.\nFor phoneme classiﬁcation on both LibriSpeech and TIMIT,\npre-training on log Mel outperforms the other two, while\npre-training on FBANK is better than MFCC. By using a\nmore primitive feature, the model can preserve more phoneme\ninformation. However, the results are opposite for keyword\nspotting, where models pre-trained on MFCC slightly outper-\nforms FBANK, and models pre-trained on FBANK slightly\nexceeds log Mel. Learning from all features successfully\npreserves the speaker information, with the model pre-trained\non log Mel achieving the highest score for speaker recognition.\nIn general, pre-training with log Mel features results in the best\nperforming model. This aligns with previous work [8]–[11],\n[20], [21], [27] (See Table I) that also uses 80-dim log Mel.\n4) Effect of Different Network Depth: We present classiﬁ-\ncation accuracy of three TERA models, including base (3-\nlayer), medium (6-layer), large (12-layer) in Figure 4. We\nobserve that performance decay for all tasks as model depth\nincreases. We conclude that a small model is sufﬁcient to\nsolve the proposed pre-training task, which is reasonable as\na lot of work also uses 3-layer models (See Table I). Note\nthat here TERA is used for representation extraction and\nnot ﬁne-tune. In our later ASR experiments, we ﬁnd that\nlarge models outperform small models during ﬁne-tuning. The\nabove discovery aligns with our previous ﬁnding with wav2vec\n2.0, that large models are excellent for ﬁne-tuning but not\nrepresentation extraction. On the other hand, small models are\nadequate for feature extraction than large models.\n5) Comparing TERA with SpecAugment: SpecAugment is\na regularization technique for improving ASR training [41].\nIt shares a similar ideology with our time and frequency\nalteration. We consider the LD Policy of SpecAugment, which\nis the best performing policy described in the paper. Following\nthe notations in SpecAugment [41], the LD Policy has the\n13\nfollowing parameters: time mask parameter T=100 (maximum\nlength of the consecutive time mask), frequency mask pa-\nrameter F=9 (maximum length of the consecutive frequency\nmask), number of time masks mT=2 (amount of consecutive\nmask blocks in time), and number of frequency masks mF=2\n(amount of consecutive mask blocks in frequency). Here we\nﬁrst point out some key differences between the proposed\nmethod and SpecAugment [41], then we present the exper-\niment results of comparing TERA with SpecAugment.\nThe difference in time mask length. SpecAugment uses a\nlonger time mask length of up to 100 compared to TERA’s\nlength of 7. In SpecAugment, the time mask length applied is\nchosen from a uniform distribution from 0 to the maximum\nconsecutive length ( T) of 100. The considerable variation\nof time mask length introduces some problems during pre-\ntraining. Empirically, we ﬁnd that pre-training loss of recon-\nstruction from SpecAugment is volatile and not stable. Also,\nin our experiments, we ﬁnd that the large variation of time\nmask length makes it hard for the pre-trained model to encode\nphonetic information.\nThe difference in number of time masks. SpecAugment uses\ntwo consecutive mask blocks ( mT=2) for each input. A ﬁxed\namount of consecutive mask blocks is employed. Whereas\nTERA determines the number of mask blocks by the maximum\ntime alteration percentage PT = 15%. The amount of mask\nblocks Tnum is then given as Tnum = ⌊PT ×Lx÷WT ⌉, where\nLx is the length of input. Simply put, the number of masking\nblocks will change according to different input lengths. The\nnumber of time masks will vastly affect the model’s training\nfor long utterances. By ﬁxing the number of consecutive mask\nblocks, SpecAugment does not utilize the advantage of longer\ntraining samples.\nThe difference in masking policy. The SpecAugment masks\nselected time blocks to zero. In TERA, following the idea of\nBERT [32], a more sophisticated policy is applied. There are\nthree cases for the selected time blocks, 1) mask to zero, 2)\nreplace with random time blocks, and 3) do nothing. Case\n2) helps TERA learn the order of speech and not always\nreconstruct it from zero, case 3) helps TERA ease the training\nand testing inconsistency problem. Overall, TERA uses a more\nadvanced time alteration policy, and SpecAugment uses a\nsimple mask-to-zero method.\nComparing experimentally. To compare with SpecAugment\nexperimentally, we apply the SpecAugment LD Policy to\nself-supervised speech training. We use the same pre-training\nsettings (960 hours) and network architecture for both TERA\nand SpecAugment (3-layer Transformer Encoders Tenc and the\n2-layer prediction network Pnet), with only the difference be-\ntween masking policies discussed above. We show the results\nin dark blue and blue dotted color in Figure 4. TERA base\n960hr largely outperforms SpecAug on phoneme classiﬁcation\nfor both LibriSpeech and TIMIT, and on the keyword spotting\ntask. Also, TERA wins over SpecAugment on the speaker\nrecognition task. We conclude that SpecAugment is suitable\nfor regularizing ASR training but not self-supervised learning.\nOn the other hand, TERA is more effective for self-supervised\nrepresentation learning.\nModels Pre-train Labels WER Rescore\nliGRU + MFCC None 100 hr 8.66 6.42\nliGRU + FBANK None 100 hr 8.64 6.34\nliGRU + fMLLR None 100 hr 8.63 6.25\nBidir-CPC [6] 960 hr 96 hr 14.96 9.41\nBidir-CPC [6] 8000 hr 96 hr 13.69 8.70\nvq-wav2vec [3] 960 hr 960 hr 6.2 -\nwav2vec-large [12] 960 hr 100 hr 6.92 -\nDeCoAR [12] 960 hr 100 hr 6.10 -\nliGRU + TERA-base 960 hr 100 hr 8.31 6.01\nliGRU + TERA-medium 960 hr 100 hr 8.37 6.05\nliGRU + TERA-large 960 hr 100 hr 8.35 6.01\nTABLE IV: Comparison of recent speech representation ap-\nproaches for ASR. All results are from training an ASR system\non top of frozen representations, without ﬁne-tuning the pre-trained\nmodel. We report WER on the LibriSpeech [45] test-clean subset.\nD. Speech Representations for ASR\nWe further apply TERA to speech recognition tasks. In\nTable IV, we list results of TERA and recent work in terms\nof WER, where all ASR models are trained on top of frozen\nrepresentations without ﬁne-tuning. All TERA models use a\ncombination of ”time+freq+mag” alteration as the auxiliary\nobjective. All of the works report WER and LM rescored\nWER (denoted as Rescore) on the test-clean subset of Lib-\nriSpeech [45]. All methods are pre-trained with 960 hours of\nLibriSpeech, except for one experiment setup in [6] where\nit uses 8000 hours of data for pre-training. All of the work\nuse only the train-clean-100 for downstream adaption, except\nfor the setup in [6] and [3] that use 96 hours and 960 hours\nof label, respectively. We use the decoding and rescoring\nsetup described in Section IV-E for all liGRU + TERA and\nits variation, as well as liGRU with baseline features. Beam-\nsearch decoding with a 4-gram language model is used for\nBidir-CPC [6]. For wav2vec-large and DeCoAR [12], a 4-gram\nLM is used in the ﬁrst-pass decoding.\nFirst, we observe that the model sizes of TERA (i.e.,\nbase, medium and large) have little inﬂuence on the ASR\nperformance when TERA is used as an extractor for speech\nrepresentation. This observation aligns with our previous dis-\ncovery on other downstream tasks. The representations from\nthe base model are sufﬁcient to improve supervised ASR.\nSince all the cited models use different LM setups, it is\nhard to conclude the WER comparison in Table IV. However,\nwe cite other work’s performance to show that the WER\nachieved in this work is well within the expected range. We\nalso investigate three baseline features of MFCC, FBANK,\nand fMLLR. We use an identical ASR framework and setting\nof TERA representations for the three features. Our results\nsuggest that TERA yields constant improvement over surface\nfeatures in the same ASR framework.\nE. Speech Pre-training for ASR Comparison\nIn this section, we compare the results of ﬁne-tuning various\npre-trained models for ASR. All TERA models use a combina-\ntion of ”time+freq+mag” alteration as the auxiliary objective.\nWe summarize the results from previous literature as well as\nﬁne-tuning TERA with liGRU or MLP framework in Table V.\nWe also list results from recent literature, where all results are\nfrom ﬁne-tuning the pre-trained model as an ASR encoder.\n14\nModels Labels WER Rescore\nwav2vec 2.0 - large [5] 100 hr 2.3 -\nDiscrete BERT + vq-wav2vec [4] 100 hr 4.5 -\nContinuous BERT + wav2vec [4] 100 hr 11.8 -\nMasked Pre-trained Encoders [26] 100 hr 9.68 -\nMasked Pre-trained Encoders [26] 360 hr 7.83 -\nliGRU + TERA-base (ﬁne-tune) 100 hr 8.23 5.84\nliGRU + TERA-medium (ﬁne-tune) 100 hr 8.22 5.90\nliGRU + TERA-large (ﬁne-tune) 100 hr 8.00 5.80\nMLP + TERA-base (ﬁne-tune) 100 hr 8.47 6.24\nMLP + TERA-medium (ﬁne-tune) 100 hr 8.02 5.86\nMLP + TERA-large (ﬁne-tune) 100 hr 7.96 5.84\nTABLE V: Comparison of recent pre-training approaches for\nASR. All results are from ﬁne-tuning the pre-trained model as speech\nencoders as part of the ASR system. ASR WER and WER after LM\nrescoring on the LibriSpeech [45] test-clean subset are reported.\nSimilar to the previous section, we report WER and LM\nrescored WER on the test-clean subset of LibriSpeech [45].\nThe ﬁrst-pass decoding and LM rescoring setting are described\nin Section IV-E. All the methods investigated here were pre-\ntrained with 960 hours and use 100 hours of labels, except for\nMasked Pre-trained Encoders [26] trained with 360 hours of\nlabels. The wav2vec 2.0 [5] uses a Transformer [28] language\nmodel with beam search size of 500 for decoding. The vq-\nwav2vec [4] uses a 4-gram LM during ﬁrst-pass decoding,\nand Masked Pre-trained Encoders [26] adopt beam search\nand RNN LM with CTC decoding. When ﬁne-tuning TERA\nwith liGRU models, performance roughly correlates with the\ndepth of TERA, and the large TERA achieved the best WER.\nRemember that large models (wav2vec 2.0, TERA-large, etc)\ndid not perform well for feature extraction. However, they\nare effective for ASR ﬁne-tuning. By comparing the liGRU\nresults in Table IV and Table V, we see that ﬁne-tuning TERA\nconsistently outperforms the case when TERA is simply\nused for extraction of speech representation. The ASR model\nadopting base TERA improves from 6.01% to 5.84%, the\nmedium TERA improves from 6.05% to 5.90%, and the large\nTERA from 6.01% to 5.80%.\nHere we also cite other work’s performance to show that the\nWER achieved for the proposed method is within the expected\nrange. The wav2vec 2.0 [5] large model achieves a high\nscore of 2.3%. However, we argue that it is consists of seven\nconvolution blocks plus 24 transformer blocks. In contrast,\nour base model contains only a 3-layer Transformer Encoder\nlayers, which brings substantial low-footprint beneﬁts. The\nmassive wav2vec 2.0 model needs to be trained on 128 V100\nGPUs, where the TERA model can be trained on a single GPU.\nThe discrete BERT + vq-wav2vec [4] achieves a high score\nof 4.5%. However, we argue that the two-step pre-training is\ncomputation-intensive during model training. First, a discrete\nvocabulary of the data is learned from vq-wav2vec [3], and\nthen in the second step a standard BERT [32] is trained on\nthese discrete units. Also, the discrete BERT + vq-wav2vec [4]\nis built by stacking a standard BERT model [32] of 12\nTransformer Encoder layers [28] on top of vq-wav2vec [3],\nwhich consists of an 8-layer encoder network and a 12-\nlayer aggregator network (or context network, as described\nin [1]). Our small encoder architecture (3-layer) beneﬁts from\nless computational cost and can run on edge devices during\ninference for downstream tasks.\nModels Pre-train PER\nCNN + TD-ﬁlterbanks [56] None 18.0\nCNN + HMM [57] None 16.5\nliGRU + MFCC [58] None 16.7\nliGRU + FBANK [58] None 15.8\nliGRU + fMLLR [58] None 14.9\nwav2vec [2] 80 hr 17.6\nwav2vec [2] 960 hr 15.6\nwav2vec [2] 960 + WSJ 81 hr 14.7\nliGRU + TERA-base 100 hr 15.2\nliGRU + TERA-base 360 hr 14.9\nliGRU + TERA-base 460 hr 14.9\nliGRU + TERA-base 960 hr 14.5\nliGRU + TERA-base (ﬁne-tune) 960 hr 15.2\nMLP + TERA-base (ﬁne-tune) 960 hr 16.6\nliGRU + TERA-medium 960 hr 14.9\nTABLE VI: Comparison of pre-training approaches between\nrecent work and the proposed approach on TIMIT [48]. All\nthe pre-training data are from LibriSpeech [45], if not speciﬁed\notherwise. We report testing results in terms of PER. All of the TERA\nmodels use the combined auxiliary objective of ”time+freq+mag”\nalteration.\nWe also ﬁne-tune TERA with MLP models, and we ﬁnd\na similar trend but sometimes higher WER compared to\nTERA with liGRU. Using a deeper model with MLP gives\nperformance beneﬁt, and large achieves the best WER among\nthe MLP models. The reason is that the simple architecture\nof MLP can beneﬁt from a deeper TERA model. Comparing\nMLP with liGRU, MLP achieved superior performance than\nliGRU on the medium model, and similar performance for the\nrest of the model size. Although in general MLP outperforms\nliGRU, however MLP has the advantage of a fast training\nand inference time, thanks to the absence of recurrent units.\nAdditionally, the parameters of the 1-layerMLP is signiﬁcantly\nless than the 5-layer liGRU models. To conclude this section\nfor our proposed method, using a deeper model increases ASR\nperformance during ﬁne-tuning.\nF . Transferring to TIMIT\nWe then explore how the mismatch of domains between pre-\ntraining and downstream tasks affects performance. For the ex-\nploration, we pre-train TERA with LibriSpeech [45], and apply\nthe resulting networks to the supervised TIMIT [48] ASR task.\nThe same Hybrid ASR setting and framework described above\nfor LibriSpeech ASR are used, except that we use a learning\nrate of 4e−4 and a batch size of 8. Testing results of TERA\nand another self-supervised learning technique, wav2vec [2],\nare summarized in Table VI in terms of PER. We also list\nthe results of strong supervised systems [56]–[58]. All of\nthe TERA models use a combination of ”time+freq+mag”\nalteration as the auxiliary objective, and are pre-trained with\nvarious amounts of data. As expected, pre-training on a larger\namount of data gives performance beneﬁt, and we achieved\nthe best WER (14.5%) with 960 hours of pre-training data.\nWe ﬁnd that for TIMIT ASR as the downstream task, ﬁne-\ntuning is not helpful, and extracting speech representations\nfrom the last layer provides the best performance. The reason\nis likely because there is not enough labeled data in TIMIT,\nwhich aligns with our discovery in Section V-C1 for other\ndownstream tasks. Also, there is no signiﬁcant gain when\nextracting features from a larger model medium, which aligns\n15\nwith our previous discussion that smaller models are better for\nfeature extraction.\nVI. C ONCLUSION\nWe propose a novel self-supervised training scheme called\nTERA, where the model learns from the reconstruction of\naltered input. We pre-train TERA using a large amount of unla-\nbeled data, and adapt TERA to downstream SLP tasks using a\nlimited amount of labeled data. We demonstrate strong results\nin phone classiﬁcation, keyword spotting, speaker recognition,\nand speech recognition. We conduct a complete ablation study\nand a thorough comparison of recent representation learning\nand pre-training approaches. We show that TERA pre-trained\non one dataset can be easily transferred to another downstream\ndataset. We study how self-supervised models behave on more\npre-training data and ﬁnd that time-only masked reconstruction\nmethods cannot beneﬁt from extensive data. We also study\nthe choice of acoustic features for pre-training. We show that\nit plays a crucial role in reconstruction-based self-supervised\nlearning, as various surface features will lead to signiﬁcantly\ndifferent downstream performance. We investigate networks\nwith different depths and ﬁnd that small models are more\nsuitable for feature extraction than large models. On the other\nhand, large models are more effective for ﬁne-tuning than\nsmall models.\nACKNOWLEDGMENT\nThe authors are grateful to the National Center for High-\nperformance Computing for computer time and facilities. They\nthank Shu-wen Yang for implementing a signiﬁcant part of the\nS3PRL toolkit and pre-training the APC, VQ-APC, and NPC\nmodels; and Yist Y . Lin for implementing the keyword spotting\ntask.\nREFERENCES\n[1] A. van den Oord, Y . Li, and O. Vinyals, “Representation learning\nwith contrastive predictive coding,” CoRR, vol. abs/1807.03748, 2018.\n[Online]. Available: http://arxiv.org/abs/1807.03748\n[2] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec: Unsu-\npervised pre-training for speech recognition,” Interspeech, 2019.\n[3] A. Baevski, S. Schneider, and M. Auli, “vq-wav2vec: Self-supervised\nlearning of discrete speech representations,” in International Conference\non Learning Representations , 2020. [Online]. Available: https:\n//openreview.net/forum?id=rylwJxrYDS\n[4] A. Baevski and A. Mohamed, “Effectiveness of self-supervised pre-\ntraining for asr,” in ICASSP 2020 - 2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) , 2020, pp. 7694–\n7698.\n[5] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:\nA framework for self-supervised learning of speech representations,”\nin NeurIPS 2020, December 6-12, 2020, virtual , H. Larochelle,\nM. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds.,\n2020. [Online]. Available: https://proceedings.neurips.cc/paper/2020/\nhash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html\n[6] K. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. van den\nOord, “Learning robust and multilingual speech representations,” in\nFindings of the Association for Computational Linguistics: EMNLP\n2020. Online: Association for Computational Linguistics, Nov. 2020,\npp. 1182–1192. [Online]. Available: https://www.aclweb.org/anthology/\n2020.ﬁndings-emnlp.106\n[7] M. Rivi `ere, A. Joulin, P.-E. Mazar ´e, and E. Dupoux, “Unsupervised\npretraining transfers well across languages,” in ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2020, pp. 7414–7418.\n[8] Y .-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An Unsupervised\nAutoregressive Model for Speech Representation Learning,” in\nProc. Interspeech 2019 , 2019, pp. 146–150. [Online]. Available:\nhttp://dx.doi.org/10.21437/Interspeech.2019-1473\n[9] Y . Chung and J. Glass, “Generative pre-training for speech with autore-\ngressive predictive coding,” in ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2020,\npp. 3497–3501.\n[10] Y .-A. Chung and J. Glass, “Improved speech representations with\nmulti-target autoregressive predictive coding,” in Proceedings of\nthe 58th Annual Meeting of the Association for Computational\nLinguistics. Online: Association for Computational Linguistics, Jul.\n2020, pp. 2353–2358. [Online]. Available: https://www.aclweb.org/\nanthology/2020.acl-main.213\n[11] Y .-A. Chung, H. Tang, and J. Glass, “Vector-quantized autoregressive\npredictive coding,” Interspeech 2020, pp. 3760–3764, 2020.\n[12] S. Ling, Y . Liu, J. Salazar, and K. Kirchhoff, “Deep contextualized\nacoustic representations for semi-supervised speech recognition,” in\nICASSP 2020 - 2020 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , 2020, pp. 6429–6433.\n[13] M. Tagliasacchi, B. Gfeller, F. de Chaumont Quitry, and D. Roblek,\n“Self-supervised audio representation learning for mobile devices,”\nCoRR, vol. abs/1905.11796, 2019. [Online]. Available: http://arxiv.org/\nabs/1905.11796\n[14] M. Tagliasacchi, B. Gfeller, F. d. C. Quitry, and D. Roblek, “Pre-training\naudio representations with self-supervision,” IEEE Signal Processing\nLetters, vol. 27, pp. 600–604, 2020.\n[15] J. Chorowski, R. J. Weiss, S. Bengio, and A. van den Oord, “Unsu-\npervised speech representation learning using wavenet autoencoders,”\nIEEE/ACM Transactions on Audio, Speech, and Language Processing ,\nvol. 27, no. 12, p. 2041–2053, Dec 2019.\n[16] A. T. Liu, P.-c. Hsu, and H.-Y . Lee, “Unsupervised end-to-end learning\nof discrete linguistic units for voice conversion,” Interspeech, Sep 2019.\n[17] F. de Chaumont Quitry, M. Tagliasacchi, and D. Roblek, “Learning audio\nrepresentations via phase prediction,” 2019.\n[18] S. Pascual, M. Ravanelli, J. Serr `a, A. Bonafonte, and Y . Bengio,\n“Learning problem-agnostic speech representations from multiple self-\nsupervised tasks,” Interspeech 2019, Sep 2019.\n[19] S. Khurana, A. Laurent, W.-N. Hsu, J. Chorowski, A. Ła ´ncucki,\nR. Marxer, and J. Glass, “A Convolutional Deep Markov Model\nfor Unsupervised Speech Representation Learning,” in Interspeech\n2020, Shanghai, China, Oct. 2020. [Online]. Available: https:\n//hal.archives-ouvertes.fr/hal-02912029\n[20] A. T. Liu, S.-w. Yang, P.-H. Chi, P.-c. Hsu, and H.-y. Lee, “Mockingjay:\nUnsupervised speech representation learning with deep bidirectional\ntransformer encoders,” ICASSP 2020, May 2020.\n[21] P.-H. Chi, P.-H. Chung, T.-H. Wu, C.-C. Hsieh, S.-W. Li, and H. yi Lee,\n“Audio albert: A lite bert for self-supervised learning of audio represen-\ntation,” in SLT 2020, 2020.\n[22] W. Wang, Q. Tang, and K. Livescu, “Unsupervised pre-training of\nbidirectional speech encoders via masked reconstruction,” ICASSP 2020,\nMay 2020.\n[23] X. Song, G. Wang, Y . Huang, Z. Wu, D. Su, and H. Meng, “Speech-\nXLNet: Unsupervised Acoustic Model Pretraining for Self-Attention\nNetworks,” in Interspeech 2020 , 2020, pp. 3765–3769. [Online].\nAvailable: http://dx.doi.org/10.21437/Interspeech.2020-1511\n[24] S. Li, L. Li, Q. Hong, and L. Liu, “Improving Transformer-Based Speech\nRecognition with Unsupervised Pre-Training and Multi-Task Semantic\nKnowledge Learning,” in Interspeech 2020 , 2020, pp. 5006–5010.\n[Online]. Available: http://dx.doi.org/10.21437/Interspeech.2020-2007\n[25] D. Jiang, W. Li, R. Zhang, M. Cao, N. Luo, Y . Han, W. Zou, and\nX. Li, “A further study of unsupervised pre-training for transformer\nbased speech recognition,” in Submitted to International Conference\non Learning Representations , 2021, under review. [Online]. Available:\nhttps://openreview.net/forum?id=hrpSB rzQTU\n[26] L. Liu and Y . Huang, “Masked pre-trained encoder base on joint ctc-\ntransformer,” 2020.\n[27] A. H. Liu, Y .-A. Chung, and J. Glass, “Non-autoregressive predictive\ncoding for learning speech representations from local dependencies,”\n2020.\n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nu. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proceedings\nof the 31st International Conference on Neural Information Processing\nSystems, ser. NIPS’17. Red Hook, NY , USA: Curran Associates Inc.,\n2017, p. 6000–6010.\n[29] C. Sun, X. Qiu, Y . Xu, and X. Huang, “How to ﬁne-tune bert for text\nclassiﬁcation?” Chinese Computational Linguistics , p. 194–206, 2019.\n16\n[30] A. Chronopoulou, C. Baziotis, and A. Potamianos, “An embarrassingly\nsimple approach for transfer learning from pretrained language models,”\nProceedings of the 2019 Conference of the North , 2019.\n[31] A. T. Liu and Y . Shu-wen, “The S3PRL toolkit: Self-supervised speech\npre-training and representation learning,” 2020. [Online]. Available:\nhttps://github.com/s3prl/s3prl\n[32] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) . Minneapolis,\nMinnesota: Association for Computational Linguistics, Jun. 2019,\npp. 4171–4186. [Online]. Available: https://www.aclweb.org/anthology/\nN19-1423\n[33] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly\noptimized BERT pretraining approach,” CoRR, vol. abs/1907.11692,\n2019. [Online]. Available: http://arxiv.org/abs/1907.11692\n[34] A. Graves, S. Fern ´andez, F. Gomez, and J. Schmidhuber, “Connection-\nist temporal classiﬁcation: labelling unsegmented sequence data with\nrecurrent neural networks,” in Proceedings of the 23rd international\nconference on Machine learning , 2006, pp. 369–376.\n[35] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. ˇCernock`y, and S. Khudanpur,\n“Recurrent neural network based language model,” in Eleventh annual\nconference of the international speech communication association, 2010.\n[36] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, “Deep contextualized word representations,” Pro-\nceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), 2018.\n[37] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and\nR. Soricut, “Albert: A lite bert for self-supervised learning of\nlanguage representations,” in International Conference on Learning\nRepresentations, 2020. [Online]. Available: https://openreview.net/\nforum?id=H1eA7AEtvS\n[38] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le,\n“XLNet: Generalized Autoregressive Pretraining for Language Under-\nstanding,” arXiv e-prints, p. arXiv:1906.08237, Jun. 2019.\n[39] H. Wu, A. T. Liu, and H. yi Lee, “Defense for Black-Box\nAttacks on Anti-Spooﬁng Models by Self-Supervised Learning,” in\nProc. Interspeech 2020 , 2020, pp. 3780–3784. [Online]. Available:\nhttp://dx.doi.org/10.21437/Interspeech.2020-2026\n[40] S. wen Yang, A. T. Liu, and H. yi Lee, “Understanding Self-Attention\nof Self-Supervised Audio Transformers,” in Proc. Interspeech 2020 ,\n2020, pp. 3785–3789. [Online]. Available: http://dx.doi.org/10.21437/\nInterspeech.2020-2231\n[41] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,\nand Q. V . Le, “Specaugment: A simple data augmentation method for\nautomatic speech recognition,” Interspeech 2019, Sep 2019.\n[42] P. Wang, L. Wei, Y . Cao, J. Xie, and Z. Nie, “Large-scale unsupervised\npre-training for end-to-end spoken language understanding,” in ICASSP,\n2020.\n[43] S. Ling, J. Salazar, Y . Liu, and K. Kirchhoff, “BERTphone: Phonetically-\naware Encoder Representations for Utterance-level Speaker and Lan-\nguage Recognition,” in Odyssey 2020 The Speaker and Language\nRecognition Workshop, 2020.\n[44] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of\nword representations in vector space,” in 1st International Conference\non Learning Representations, ICLR 2013, Scottsdale, Arizona, USA,\nMay 2-4, 2013, Workshop Track Proceedings, Y . Bengio and Y . LeCun,\nEds., 2013. [Online]. Available: http://arxiv.org/abs/1301.3781\n[45] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An\nASR corpus based on public domain audio books,” in ICASSP, 2015.\n[46] N.-Q. Pham, T.-S. Nguyen, J. Niehues, M. M ¨uller, and A. Waibel, “Very\nDeep Self-Attention Networks for End-to-End Speech Recognition,”\nin Interspeech 2019 , 2019, pp. 66–70. [Online]. Available: http:\n//dx.doi.org/10.21437/Interspeech.2019-2702\n[47] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” in\nInternational Conference on Learning Representations , 2019. [Online].\nAvailable: https://openreview.net/forum?id=Bkg6RiCqY7\n[48] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D. S.\nPallett, “DARPA TIMIT acoustic-phonetic continous speech corpus CD-\nROM. NIST speech disc 1-1.1,” NASA STI/Recon Technical Report N,\np. 27403, Feb. 1993.\n[49] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,\nM. Hannemann, P. Motlicek, Y . Qian, P. Schwarz, J. Silovsky, G. Stem-\nmer, and K. Vesely, “The kaldi speech recognition toolkit,” in ASRU,\n2011.\n[50] C. Lopes and F. Perdigao, “Phone recognition on the timit database,”\nSpeech Technologies/Book, vol. 1, pp. 285–302, 2011.\n[51] P. Warden, “Speech commands: A public dataset for single-word speech\nrecognition.” Dataset available online , 2017. [Online]. Available:\nhttp://download.tensorﬂow.org/data/speech commands v0.01.tar.gz\n[52] K.-F. Lee and H.-W. Hon, “Speaker-independent phone recognition\nusing hidden markov models,” IEEE Transactions on Acoustics, Speech,\nand Signal Processing , vol. 37, no. 11, pp. 1641–1648, 1989.\n[53] M. Ravanelli, T. Parcollet, and Y . Bengio, “The pytorch-kaldi speech\nrecognition toolkit,” in ICASSP 2019 - 2019 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP) , 2019,\npp. 6465–6469.\n[54] M. J. Gales, “Maximum likelihood linear transformations for hmm-based\nspeech recognition,” Computer speech & language , vol. 12, no. 2, pp.\n75–98, 1998.\n[55] J. Kahn, M. Rivi `ere, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazar ´e,\nJ. Karadayi, V . Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko,\nG. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, “Libri-light: A\nbenchmark for asr with limited or no supervision,” in ICASSP 2020 ,\n2020, pp. 7669–7673, https://github.com/facebookresearch/libri-light.\n[56] N. Zeghidour, N. Usunier, I. Kokkinos, T. Schaiz, G. Synnaeve, and\nE. Dupoux, “Learning ﬁlterbanks from raw speech for phone recogni-\ntion,” ICASSP 2018, Apr 2018.\n[57] L. T ´oth, “Phone recognition with hierarchical convolutional deep maxout\nnetworks,” EURASIP Journal on Audio, Speech, and Music Processing ,\nvol. 2015, no. 1, pp. 1–13, 2015.\n[58] M. Ravanelli, P. Brakel, M. Omologo, and Y . Bengio, “Light gated\nrecurrent units for speech recognition,” IEEE Transactions on Emerging\nTopics in Computational Intelligence, vol. 2, no. 2, p. 92–102, Apr 2018.\nAndy T. Liu received the bachelor’s degree in elec-\ntrical engineering from National Taiwan University\n(NTU), Taipei, Taiwan, in 2018. He is currently\nworking toward the Ph.D. degree with the Graduate\nInstitute of Communication Engineering, NTU, su-\npervised by Professor Hung-yi Lee. His research in-\nterests include self-supervised learning, pre-training,\nand representation learning in the speech and NLP\ndomain.\nShang-Wen Li received the Ph.D. degree from MIT\nComputer Science and Artiﬁcial Intelligence Labo-\nratory, Cambridge, MA, USA, in 2016 supervised\nby Professor Victor Zue. Since 2019, he has been\na Senior Applied Scientist with Amazon AWS AI.\nHe was with Apple Siri and Amazon Alexa before\njoining AWS. His research interests include spoken\nlanguage understanding, natural language genera-\ntion, dialog management, and low-resource speech\nprocessing.\nHung-yi Lee received the M.S. and Ph.D. degrees\nfrom National Taiwan University, Taipei, Taiwan, in\n2010 and 2012, respectively. From September 2012\nto August 2013, he was a Postdoctoral Fellow with\nthe Research Center for Information Technology\nInnovation, Academia Sinica, Taipei, Taiwan. From\nSeptember 2013 to July 2014, he was a Visiting\nScientist with Spoken Language Systems Group,\nMIT Computer Science and Artiﬁcial Intelligence\nLaboratory, Cambridge, MA, USA. He is currently\nan Assistant Professor with the Department of Elec-\ntrical Engineering, National Taiwan University, with a joint appointment\nwith the Department of Computer Science and Information Engineering of\nthe university. His research interests include spoken language understanding,\nspeech recognition, and machine learning.",
  "topic": "Tera-",
  "concepts": [
    {
      "name": "Tera-",
      "score": 0.8084686398506165
    },
    {
      "name": "Computer science",
      "score": 0.7599316239356995
    },
    {
      "name": "Transformer",
      "score": 0.7176772356033325
    },
    {
      "name": "Encoder",
      "score": 0.6038061380386353
    },
    {
      "name": "Autoregressive model",
      "score": 0.545745849609375
    },
    {
      "name": "Speech recognition",
      "score": 0.5378431677818298
    },
    {
      "name": "Keyword spotting",
      "score": 0.45243173837661743
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44467711448669434
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38794416189193726
    },
    {
      "name": "Machine learning",
      "score": 0.3387153148651123
    },
    {
      "name": "Mathematics",
      "score": 0.10701432824134827
    },
    {
      "name": "Engineering",
      "score": 0.07844612002372742
    },
    {
      "name": "Voltage",
      "score": 0.07382982969284058
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    }
  ]
}