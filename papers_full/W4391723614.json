{
  "title": "TLC-XML: Transformer with Label Correlation for Extreme Multi-label Text Classification",
  "url": "https://openalex.org/W4391723614",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5088080667",
      "name": "Fei Zhao",
      "affiliations": [
        "University of Science and Technology Liaoning"
      ]
    },
    {
      "id": "https://openalex.org/A5032765483",
      "name": "Qing Ai",
      "affiliations": [
        "University of Science and Technology Liaoning"
      ]
    },
    {
      "id": "https://openalex.org/A5072327118",
      "name": "Xiangna Li",
      "affiliations": [
        "State Grid Corporation of China (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100370075",
      "name": "Wenhui Wang",
      "affiliations": [
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5064441410",
      "name": "Qingyun Gao",
      "affiliations": [
        "University of Science and Technology Liaoning"
      ]
    },
    {
      "id": "https://openalex.org/A5100658559",
      "name": "Yichun Liu",
      "affiliations": [
        "University of Science and Technology Liaoning"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1991418309",
    "https://openalex.org/W4281657308",
    "https://openalex.org/W2906963924",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W3037422790",
    "https://openalex.org/W3177232285",
    "https://openalex.org/W3080994088",
    "https://openalex.org/W4205902690",
    "https://openalex.org/W2743021690",
    "https://openalex.org/W2520348554",
    "https://openalex.org/W4306750934",
    "https://openalex.org/W2744136723",
    "https://openalex.org/W2788125153",
    "https://openalex.org/W3080802002",
    "https://openalex.org/W2739996966",
    "https://openalex.org/W2047940964",
    "https://openalex.org/W2932399282",
    "https://openalex.org/W3120220640",
    "https://openalex.org/W2998486497",
    "https://openalex.org/W4308112365",
    "https://openalex.org/W4200055642",
    "https://openalex.org/W3035665101",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1781770377",
    "https://openalex.org/W2061873838",
    "https://openalex.org/W2068074736",
    "https://openalex.org/W4224307896"
  ],
  "abstract": "Abstract Extreme multi-label text classification (XMTC) annotates related labels for unknown text from large-scale label sets. Transformer-based methods have become the dominant approach for solving the XMTC task due to their effective text representation capabilities. However, the existing Transformer-based methods fail to effectively exploit the correlation between labels in the XMTC task. To address this shortcoming, we propose a novel model called TLC-XML, i.e., a Transformer with label correlation for extreme multi-label text classification. TLC-XML comprises three modules: Partition, Matcher and Ranker. In the Partition module, we exploit the semantic and co-occurrence information of labels to construct the label correlation graph, and further partition the strongly correlated labels into the same cluster. In the Matcher module, we propose cluster correlation learning, which uses the graph convolutional network (GCN) to extract the correlation between clusters. We then introduce these valuable correlations into the classifier to match related clusters. In the Ranker module, we propose label interaction learning, which aggregates the raw label prediction with the information of the neighboring labels. The experimental results on benchmark datasets show that TLC-XML significantly outperforms state-of-the-art XMTC methods.",
  "full_text": "Neural Processing Letters (2024) 56:25\nhttps://doi.org/10.1007/s11063-024-11460-z\nTLC-XML: Transformer with Label Correlation for Extreme\nMulti-label Text Classiﬁcation\nFei Zhao 1 · Qing Ai 1 · Xiangna Li 2 · Wenhui Wang 3,4 · Qingyun Gao 1 · Yichun Liu 1\nAccepted: 25 October 2023 / Published online: 10 February 2024\n© The Author(s) 2024\nAbstract\nExtreme multi-label text classiﬁcation (XMTC) annotates related labels for unknown text\nfrom large-scale label sets. Transformer-based methods have become the dominant approach\nfor solving the XMTC task due to their effective text representation capabilities. However,\nthe existing Transformer-based methods fail to effectively exploit the correlation between\nlabels in the XMTC task. To address this shortcoming, we propose a novel model called\nTLC-XML, i.e., a Transformer with label correlation for extreme multi-label text classiﬁca-\ntion. TLC-XML comprises three modules: Partition, Matcher and Ranker. In the Partition\nmodule, we exploit the semantic and co-occurrence information of labels to construct the\nlabel correlation graph, and further partition the strongly correlated labels into the same clus-\nter. In the Matcher module, we propose cluster correlation learning, which uses the graph\nconvolutional network (GCN) to extract the correlation between clusters. We then introduce\nthese valuable correlations into the classiﬁer to match related clusters. In the Ranker module,\nwe propose label interaction learning, which aggregates the raw label prediction with the\nB Qing Ai\nlyaiqing@126.com\nFei Zhao\nustlzhaofei@163.com\nXiangna Li\n289491049@qq.com\nWenhui Wang\nangleboy@foxmail.com\nQingyun Gao\nustlgqy@163.com\nYichun Liu\nustlliuyichun@163.com\n1 School of Computer Science and Software Engineering, University of Science and Technology\nLiaoning, Anshan 114051, China\n2 State Grid Information and Telecommunication Group Company Ltd., State Grid Corporation of\nChina, Beijing 100053, China\n3 Beijing Synchrotron Radiation Facility, Chinese Academy of Sciences, Beijing 100049, China\n4 Chinese Spallation Neutron Source Science Center, Chinese Academy of Sciences, Dongguan 523808,\nChina\n123\n25 Page 2 of 21 F. Zhao et al.\ninformation of the neighboring labels. The experimental results on benchmark datasets show\nthat TLC-XML signiﬁcantly outperforms state-of-the-art XMTC methods.\nKeywords Extreme multi-label text classiﬁcation · Label correlation · Graph convolutional\nnetwork · Transformer model\n1 Introduction\nExtreme multi-label text classiﬁcation (XMTC) is a task that assigns unknown text with\nrelated labels from extremely large-scale label sets. XMTC is widely applied to recommen-\ndation systems [ 1], patent classiﬁcation [ 2] and search engines [ 3]. Different from traditional\nmulti-label classiﬁcation tasks, both the number of labels and the number of instances in the\nXMTC task are extremely large. Therefore, the XMTC task suffers from the following two\nchallenges: (1) because of the extremely large output space, the training time and memory\nconsumption of the model are excessive; and (2) a high proportion of tail labels (the label\nhas few relevant instances) leads to severe label sparsity.\nTraditional methods for solving the XMTC task can be roughly divided into the following\nthree main categories: one-versus-all methods, embedding-based methods and tree-based\nmethods. Similar to the straightforward strategy for solving multi-label classiﬁcation,\none-versus-all methods learn a subclassiﬁer for each label. However, the computational\ncomplexity of such methods is very high due to the large-scale label sets. Embedding-based\nmethods assume that the label matrix is low-rank and project the original label space into a\nlow-dimensional subspace to reduce the complexity of the problem. Due to the large propor-\ntion of tail labels in the XMTC setting, the low-rank assumption is violated, leading to lower\naccuracy. The tree-based methods decompose the original problem into multiple subprob-\nlems by partitioning the label space. Existing tree-based methods only exploit the semantic\nlabel information, easily leading to error propagation.\nDeep learning methods are currently the most effective techniques for solving XMTC\ntasks. These methods integrate feature extraction and classiﬁcation into an end-to-end frame-\nwork, thereby achieving superior performance. However, powerful text embeddings require\ndeep network architectures, resulting in a large number of parameters. In XMTC scenarios,\nnumerous labels have fewer positive instances, so training massive parameter models is a\nformidable task. The pre-trained Transformer models [ 4–6], which have a large number of\nparameters, were pre-trained unsupervised on a large-scale corpus, thereby resulting in bet-\nter initialization parameters. X-Transformer [ 7] ﬁrst applied the Transformer model to solve\nthe XMTC task and achieved excellent performance, so many variants [ 8–10] have been\nproposed in recent years. The correlation between labels is ubiquitous in the XMTC task.\nXun et al. [ 11] introduced the CorNet block, which uses label correlation to enhance label\npredictions and can be easily integrated with various other XMTC methods. However, the\nCorNet framework ignores the co-occurrence information between labels. BGNN-XML [ 12]\nemploys co-occurrence correlations between labels to partition the label space, but fails to\nutilize label correlation information in the classiﬁcation model.\nExisting deep learning methods have the following shortcomings. First, these methods fail\nto fully consider the combination of semantic and cooccurrence correlations between labels.\nSecond, these methods ignore the rich cooccurrence information between label clusters. In\norder to fully consider the correlations between labels and to exploit these label correlations\nin both the label space partitioning and the classiﬁcation model, we propose TLC-XML,\n123\nTLC-XML: Transformer with Label Correlation for Extreme… Page 3 of 21 25\nTable 1 Comparison of traditional and deep learning methods\nMethod Advantages Drawbacks\nTraditional methods Traditional methods\nare easier to\ninterpret\nTraditional methods\ncannot capture the\ncontextual semantic\ninformation of the\ntext\nDeep learning methods Deep learning\nmethods can extract\npowerful text\nembeddings, and\ncan achieve superior\nperformance\nComplex models\nrequire long training\ntimes and large\namounts of\ncomputing\nresources\nwhich comprises three modules: partitioning the label space (Partition), matching related\nclusters (Matcher) and ranking candidate labels (Ranker). In Partition, we use label seman-\ntics and co-occurrence information to extract correlations between labels. Then, the label\ncorrelation graph is constructed by using labels as nodes and correlations between labels\nas edges. Furthermore, we propose the label graph partition (LGP) algorithm to partition\nstrongly correlated labels into the same cluster. In Matcher, we propose cluster correlation\nlearning (CCL) algorithm, which uses the graph convolutional network (GCN) to extract\nthe correlation between clusters. Then, these valuable correlations are introduced into the\nclassiﬁer to match related clusters. In Ranker, we propose the label interaction learning (LIL)\nalgorithm, which aggregates the raw label prediction with the information of the neighboring\nlabels. In addition, we use residual mapping to alleviate the over-smoothing problem.\nWe summarize the three main contributions of this paper as follows:\n1. We propose a novel TLC-XML model based on a pre-trained Transformer for the XMTC\ntask. TLC-XML accounts for the semantic and co-occurrence correlations between labels\nand uses the label correlations in the label space partitioning and classiﬁcation model.\n2. The CCL algorithm and LIL algorithm are proposed to extract different levels of cor-\nrelation between labels, and these valuable information and Transformer-based feature\nextraction networks are integrated into an end-to-end training framework.\n3. We conduct extensive experiments on ﬁve benchmark datasets of XMTC, and the exper-\nimental results show that TLC-XML outperforms state-of-the-art methods.\nThe rest of this paper is sequentially organized as follows. Section 2 reviews related work\non extreme multi-label text classiﬁcation. In Sect. 3, we present TLC-XML in detail, including\nthe Partition module, Matcher module and Ranker module. Section 4 shows the experimental\nconﬁguration, experimental results and ablation studies. Finally, the conclusion of this paper\ni sg i v e ni nS e c t .5.\n2 Related Works\nThere are two approaches for solving the XMTC task, including traditional methods and\ndeep learning methods. Their advantages and drawbacks are summarized in Table 1.\n123\n25 Page 4 of 21 F. Zhao et al.\n2.1 Traditional Methods\nOne-versus-all methods are a classical strategy for solving the XMTC task, which trains\na subclassiﬁer for each label. However, one-versus-all methods suffer from a large model\nsize and excessive training time. To reduce the complexity, PD-Sparse [ 13] exploits both\nprimal and dual sparsity for sublinear time costs. DiSMEC [ 14] uses a double parallelization\nlayer to improve training and prediction speed. In addition, DiSMEC prunes model weight\ncoefﬁcients to reduce the model size, thus requiring fewer computational resources. To further\nimprove the DiSMEC training speed, Schultheis and Babbar [ 15] proposed a novel weight\ninitialization strategy, signiﬁcantly speeding up classiﬁer training by setting the initial vector.\nEmbedding-based methods assume that the label matrix is low-rank and train a classiﬁer\non a low-dimensional embedding subspace. Due to the high proportion of tail labels in\nthe XMTC setting, the low-rank assumption is broken. To address this drawback, SLEEC\n[16] partitions the training samples into multiple clusters and preserves the distance of the\nnearest label to learn local embeddings. SLEEC further uses the k-nearest neighbor classiﬁer\nfor prediction in each subspace. However, SLEEC partitions training samples without label\ninformation. AnnexML [ 17] extends SLEEC by constructing the k-nearest neighbor graph on\nthe label embedding subspace and uses the approximate nearest neighbor search algorithm\nto efﬁciently predict.\nTree-based methods, which partitions the extreme problem into multiple subproblems,\nare an effective strategy for solving the XMTC task. Parabel [ 18] uses the k-means clustering\nalgorithm to recursively partition the label space into two label clusters, which can construct\na deep and balanced label tree. To prevent propagation error in the deep tree, Bonsai [ 19]\ncombines the advantages of shallow trees and unbalanced partitioning, where tail labels are\nassigned to different partitions. Therefore, Bonsai achieves better prediction performance\non tail labels. To accelerate inference for tree-based methods, XR-LINEAR [ 20]u s e st h e\nmasked sparse chunk multiplication (MSCM) algorithm to avoid unnecessary traversal and\noptimize memory locality.\n2.2 Deep Learning Methods\nDeep learning methods use various networks to learn semantic context representations from\nthe original text and achieve superior performance. XML-CNN [ 21] ﬁrst applies deep learning\nmethods to the XMTC task. XML-CNN extracts the text representation by dynamic max-\npooling and 1D convolution [ 22] and introduces a low-dimensional hidden layer for efﬁcient\ncomputation. AttentionXML [ 23] builds a wide and shallow label tree and trains a sepa-\nrate classiﬁer for each layer. In addition, AttentionXML uses bidirectional long short-term\nmemory (BiLSTM) with an attention mechanism to capture a speciﬁc text representation\nfor each label. X-Transformer [ 7] employs Transformer encoders to match relevant clus-\nters and then uses sparse TF-IDF features and neural embeddings to rank the labels. Since\nthe X-Transformer suffers from the drawbacks of a large model size and excessive text\ntruncation, LightXML [ 9] improves the text embedding and input sequence length. In addi-\ntion, LightXML combines the Transformer model with the generative cooperative network,\nenabling the Transformer model to learn a better text representation.\n123\nTLC-XML: Transformer with Label Correlation for Extreme… Page 5 of 21 25\nFig. 1 The framework of the proposed TLC-XML model for XMTC\n3 TLC-XML\nIn this section, we propose the TLC-XML model to solve the XMTC task. The proposed model\ncomprises three modules: Partition, Matcher and Ranker. As shown in Fig. 1, in Partition,\nwe construct the label correlation graph according to the label co-occurrence matrix and the\nlabel embedding matrix and partition the strongly correlated labels into the same cluster. In\nMatcher, the correlation between clusters extracted from the CCL algorithm is combined\nwith the text representation extracted from the Transformer model, which is used to match\nrelated label clusters from the partitioned label clusters. In Ranker, we use the LIL algorithm\nto aggregate the label prediction with the information of the neighboring labels. The ﬁnal\nscores of the candidate labels are determined by combining the outputs of the CCL and LIL\nalgorithms.\n3.1 Problem Formulation\nFormally, we assume that\n{(\nxi ,yi\n)} N\ni =1 is a training set, where x i denotes the raw text of the\nith instance and y i ∈ {0,1}L denotes the label vector for the ith instance. y il = 1 if instance\nxi is related to the lth label and 0 otherwise. The objective of the XMTC task aims to learn a\nmap f that assigns a score to each instance, and y il = f (xi ) obtains a higher score if label l\nis related to instance x i . The main mathematical symbols used in this paper are summarized\nin Table 2.\n3.2 Partition\nTo solve the extremely large-scale label space problem, we partition the original label space\nY = {1,..., l,..., L} into K label clusters {Sk }K\nk=1,w h e r eSk represents the set of labels on\nthe kth cluster.\n123\n25 Page 6 of 21 F. Zhao et al.\nTable 2 Main mathematical\nsymbols used in this paper Notation Description\nxi Raw text of instances\nyi L dimensional binary label vectors\ny′\ni K dimensional instance-to-cluster vectors\nN Number of instances\nL Number of labels\nK Number of label clusters\nG Label correlation graph\nH Cluster correlation graph\nV The set of nodes in the graph\nE The set of edges in the graph\nA The graph adjacency matrix\n3.2.1 Label Correlation Graph\nThe label co-occurrence information can reﬂect dependencies between labels. Therefore, we\nﬁrst use the conditional probability to extract the co-occurrence between labels. Since the\nconditional probability is directional, we employ a symmetric conditional probability matrix\nA\np =\n{\na p\nij\n}\nto represent the co-occurrence strength between labels.\na p\nij = 1\n2 [P (j | i ) + P (i | j )] , (1)\nwhere P (j | i ) is the occurrence probability of the jth label if the ith label appears in\nan instance. The large fraction of tail labels in the XMTC task results in sparse label co-\noccurrence information. We further introduce the semantic information of labels to enhance\nthe correlation between labels. The correlation matrix ˆA p =\n{\nˆa p\nij\n}\ncombines co-occurrence\nand semantic correlations.\nˆa p\nij = a p\nij + λ · σ\n(\ncos\n(\nzi , z j\n))\n, (2)\nwhere cos(·,·) returns the cosine similarity between two vectors, σ(·) is the sigmoid function,\nλ is a trade-off parameter between semantic and co-occurrence correlations, and zi and z j\nare the word embeddings of the label text. We can further obtain the label correlation graph\nG = (V p , ˆA p ),w h e r eV p and ˆA p are the label node set and the adjacency matrix that stores\nthe correlated edge weights.\n3.2.2 Partition the Label Graph\nIn this subsection, to ﬁnd clusters of strongly correlated labels in G, we propose the label\ngraph partition (LGP) algorithm to partition the label graph. The quality of the label graph\npartition is measured by modularity Q [23].\nQ = 1\n2m\nL∑\ni ,j =1\n[\n˜a p\nij − ki k j\n2m\n]\nδ (i , j ), (3)\n123\nTLC-XML: Transformer with Label Correlation for Extreme… Page 7 of 21 25\nwhere ˜a p\nij =\n{\n1i f ˆa p\nij >τ p\n0 otherwise , δ\n(\nli ,l j\n)\n=\n{\n1i f i and j are in the same cluster\n0 otherwise , ki =\n∑ L\nj =0 ˜a p\nij is the degree of node i, m = 1/2∑ L\ni ,j =1 ˜a p\nij is the number of edges, and τp is\na noise threshold that is used to convert a weighted graph to an unweighted graph.\nAlgorithm 1 Label Graph Partition\nInput:\n{\nyi\n} N\ni =1 and {zi }L\ni =1.\nOutput: {Sk }K\nk=1.\n1: Initialize V p , Q′ = 0, /Delta1Q = 0a n d S = ∅.\n2: Construct the correlated edges of label graph G according to Eq. ( 2).\n3: Initialize each label node as a separate cluster with K = L.\n4: while K > 1 do\n5: for i = 1,..., L do\n6: Merge the node i into the cluster of its neighbors sequentially.\n7: Calculate the modularity gain /Delta1Q after merging according to Eq. ( 3).\n8: if max (/Delta1Q)> 0 then\n9: Move node i to the cluster of the node j .\n10: Q is the current modularity value.\n11: else\n12: The node i stays in its original cluster.\n13: end if\n14: if Q > Q′ then\n15: Update the optimal partition {Sk }K\nk=1 and Q′ = Q.\n16: end if\n17: Reset K = number of clusters after merging.\n18: Reset all nodes of each cluster to a node.\n19: end for\n20: end while\n21: return {S\nk }K\nk=1.\nThe LGP algorithm is shown in Algorithm 1.T h e L labels on the original label space are\npartitioned into K label clusters. Then, a three-level label tree is constructed, where the root\nnode is the entire label set, the nodes of the second level are label clusters, and the leaf nodes\nare the original labels, where each label can only appear in one cluster.\n3.3 Matcher\nAfter partitioning the label space, the original set of labels is partitioned into K label clusters.\nIn Matcher, we aim to match the relevant K\n′\nclusters\n{\nS\n′\nk\n} K\n′\nk=1\nfrom the label clusters {Sk }K\nk=1\nfor a given instance xi .\nPrevious works [ 24–29] have shown that exploiting the correlation between labels in\nmulti-label classiﬁcation can signiﬁcantly enhance the classiﬁcation performance. The GCN\nis an effective algorithm for extracting the correlations between labels. However, the sparse\nlabel co-occurrence matrix in the XMTC task results in many label nodes without correlated\nedges, which further limits the information propagation of GCN. After partitioning the label\nspace, the co-occurrence information between label clusters is remarkably rich, as shown in\nFig. 2. Therefore, classiﬁcation performance can be improved by exploiting the correlation\nbetween label clusters.\n123\n25 Page 8 of 21 F. Zhao et al.\nFig. 2 The co-occurrence of labels and label clusters on EURLex-4K\n3.3.1 Cluster Correlation Graph\nThe cluster correlation graph can be represented as H = (V m , E m ),w h e r e V m and E m\nare the nodes corresponding to the cluster and the connected edges. The adjacency matrix\nAm =\n{\nam\nij\n}\ndenotes the co-occurrence correlation between label clusters.\nWe extract the representation of clusters V m =\n{\nvm\n1 ,...,v m\nk ,...,v m\nK\n}\nby aggregating\nthe label embedding.\nvm\nk =\n∑\nl∈Sk zl\n|Sk | , (4)\nwhere zl is the embedding of label l and |Sk | denotes the number of labels in label cluster\nSk . To represent the co-occurrence relationship between clusters, we use the conditional\nprobability am\nij to calculate the weight matrix between clusters, i.e.,\nam\nij =\n{\nP\n(\nSi |S j\n)\nif i ̸= j\n1 otherwise . (5)\nTo reduce the computational cost, we ﬁlter the noise of lower values for am\nij . In addition, to\nalleviate the excessive information aggregation from neighboring nodes, we further employ\nthe re-weighted scheme [ 24]f o r ˆa\nm\nij to balance the relationship between nodes and their\nneighborhoods.\nˆam\nij =\n{\nam\nij if am\nij >τ m\n0 otherwise , (6)\n˜am\nij =\n⎧\n⎪⎨\n⎪⎩\np\nL∑\nj =1\nˆam\nij\nif i ̸= j\n1 − p otherwise\n, (7)\nwhere τm is the noise threshold and p is a trade-off parameter in which nodes tend to aggregate\nneighbor information if p is close to 1; otherwise, they focus on their own information.\nTherefore, ˜Am =\n{\n˜am\nij\n}\nis the enhanced adjacency matrix, storing the correlation strength\nbetween clusters.\n123\nTLC-XML: Transformer with Label Correlation for Extreme… Page 9 of 21 25\n3.3.2 Cluster Correlation Learning\nIn this subsection, we propose cluster correlation learning (CCL) to capture the correlations\nbetween cluster nodes in the cluster correlation graph. CCL is based on an extended variant\nof the graph convolutional network (GCN), which is an effective neural network model for\nnode-level prediction tasks. GCN can learn node embeddings by aggregating information\nfrom their neighbors using convolutional operations. For the node representation matrix\nH\n(t ) ∈ RL×D at the t th layer, we use the convolution operation of GCN [ 30], and the node\nrepresentation of the next layer is updated by\nH (t +1) = h\n(\n˜Am H (t )Wt\n)\n, (8)\nwhere H (0) = V m is the initial embedding matrix of labels, Wt ∈ RD×D is a matrix of\nlearnable parameters, and h(·) is a nonlinear activation function. After stacking the T -layer\nconvolution, we take H (T ) ∈ RL×D as the ﬁnal cluster embedding, where each node can\nintegrate information from T -order neighbors in the cluster correlation graph.\nWe employ the classical multi-label classiﬁcation loss Lmatcher to train an end-to-end\nclassiﬁer that considers dependencies between label clusters.\nLmatcher = 1\nN\nN∑\ni =1\nK∑\nk=1\n[\ny′\nik log\n(\nˆy′\nik\n)\n+\n(\n1 − y′\nik\n)\nlog\n(\n1 −ˆy′\nik\n)]\n, (9)\nwhere y ′\nik =\n{\n1i f y il = 1a n d l ∈ Sk\n0 otherwise is the instance-to-cluster assignment, ˆy′\nik =\nσ\n(\nH (T )\nk /Phi1(xi )\n)\nis the predicted value, /Phi1(·) is the Transformer encoder that represents\nthe raw text as the feature vector, and σ (·) is the sigmoid function that is used at the last\nlayer to perform the binary classiﬁcation for each label.\nFor an instance x i , the relevant candidate clusters\n{\nS\n′\nk\n} K\n′\nk=1\ncan be obtained by the Matcher\nmodule. {\nS\n′\nk\n} K\n′\nk=1\n=\n{\nSk | k ∈ rank K ′\n(\nˆy\n′\ni\n)}\n, (10)\nwhere rank k (·) returns the k largest indices.\n3.4 Ranker\n3.4.1 Label Interaction Learning\nA common approach for solving the XMTC task uses a fully connected layer as the last\nlayer of the network and employs a sigmoid function to obtain the label prediction, such as\nXML-CNN [21], Attention-XML [ 22] and LightXML [ 9]. However, such methods ignore the\ncorrelation information between labels, which can be used to improve prediction accuracy.\nTo exploit the correlation information between labels, we propose label interaction learning\n(LIL), which is an effective network block that can learn the importance of different labels\nand adaptively adjust label prediction. However, the deeper network could cause performance\ndegradation and over-smoothing. Speciﬁcally, we ﬁrst employ a fully connected layer to\npredict the raw output LIL (0) and use residual mapping [ 31] to aggregate the raw output\n123\n25 Page 10 of 21 F. Zhao et al.\nwith the information of the neighboring labels.\n{\nLIL (0) = W ⊤\nr · /Phi1(xi )\nLIL (c) = LIL (c − 1) + /Phi1(xi ) Wc Ar , (11)\nwhere Wr ∈ RD×L is the matrix of learnable parameters on the fully connected layer, c\nis the number of aggregated layers, Wc ∈ RD×L is the matrix of learnable parameters to\nadjust the neighboring label information, and Ar ∈ RL×L is transformed from the label\ncorrelation matrix A p according to Eqs. ( 6)a n d( 7). We employ the loss Lranke r to train the\nLIL algorithm.\nLranke r = 1\nN\nN∑\ni =1\nL∑\nl=1\n[\nyil log\n(\nˆyil\n)\n+\n(\n1 − yil\n)\nlog\n(\n1 −ˆyil\n)]\n, (12)\nwhere ˆyil = σ (LIL (c)) is the predicted value of the labels.\n3.4.2 Ranking Candidate Labels\nIn this subsection, we aim to rank each candidate label. The scores of the candidate labels are\ndetermined according to the combination output of the CCL algorithm and LIL algorithm.\nFor an instance x i , the score of the candidate label is deﬁned as score i =\n{\nˆyil | l ∈\n{\nS\n′\nk\n}}\n.\n3.5 Training Algorithm\nThe TLC-XML training algorithm is provided in Algorithm 2.\nAlgorithm 2 TLC-XML training algorithm\nInput: Train set\n{(\nxi ,yi\n)} N\ni =1, label clusters {Sk }K\nk=1 and pre-trained Transformer parameters {θ 0}.\nOutput: {Wt }T\nt =1, Wr , {Wc }C\nc=1, {θm } and {θr }.\n1: Initialize the Transformer parameters {θ m\n}\nand {θ r } according to {θ 0}.\n2: Construct instance-to-cluster assignment matrix y ′ according to\n{(\nxi ,yi\n)} N\ni =1 and {Sk }K\nk=1.\n3: Initialize nodes V m and adjacency matrix ˜Am of H according to Eq. ( 4)a n d( 7).\n4: while Matcher not converge do\n5: Combine the CCL with the Transformer text representation.\n6: Calculate the loss L\nmatcher according to Eq. ( 9).\n7: Update {θm } and {Wt }T\nt =1 by back propagation.\n8: end while\n9: while Ranker not converge do\n10: Calculate the raw output of the label by {θr } and Wr .\n11: Aggregate information from neighboring labels according to LIL.\n12: Calculate the loss L\nranke r according to Eq. ( 12).\n13: Update {θr }, Wr and {Wc }C\nc=1 by back propagation.\n14: end while\n15: return {θm }, {Wt }T\nt =1, {θr }, Wr and {Wc }C\nc=1.\n123\nTLC-XML: Transformer with Label Correlation for Extreme… Page 11 of 21 25\nTable 3 Datasets statistics\nDataset NM L L ˆL\nRCV1 23 ,149 781 ,265 103 3 .18 729.67\nEURLex-4K 15 ,449 3865 3956 5 .3 20.79\nAAPD 54 ,840 1000 54 2 .41 2444\nWiki10-31K 14 ,146 6616 30 ,938 18 .64 8.52\nAmazonCat-13K 1 ,186,239 306 ,782 13 ,330 5 .04 448.57\n4 Experiment\n4.1 Experimental Configuration\n4.1.1 Datasets\nWe evaluate TLC-XML on ﬁve publicly available XMTC benchmark datasets including\nRCV1 [ 32], EURLex-4K [ 33], AAPD [ 34], Wiki10-31K and AmazonCat-13K [ 35]. The\ncharacteristics of the benchmark datasets are summarized in Table 3,w h e r e N denotes the\ntotal number of training instances, M denotes the total number of test instances, L denotes\nthe total number of labels, and L and ˆL denote the mean number of labels per instance and\nthe mean number of instances per label, respectively.\n4.1.2 Evaluation Metrics\nRanking-based evaluation metrics are widely used to compare the performance of XMTC\nmodels, such as P@ k and nDCG@k with k=1,3 and 5. P@ k represents the probability that the\ntop-k predicted labels are the ground truth labels. The predicted label vector and the ground\ntruth label vector are denoted as ˆy and y, respectively.\nP@k = 1\nk\n∑\nl∈rank k (ˆy)\nyl , (13)\nwhere rank k\n(\nˆy\n)\nreturns the k largest indices. The normalized discounted cumulative gain\n(nDCG) [ 36] is another widely used evaluation metric in the XMTC task, which measures\nthe relevance and ranking of the predicted labels. The nDCG@ k is deﬁned as\nnDCG@k = DCG@k\n∑ min(k,∥y∥0)\nl=1 1/log (l + 1)\n, (14)\nwhere ∥y∥0 returns the count of nonzero values in y, DCG@ k = ∑ k\nl=1\nytop (rank k (ˆy),l)\nlog(l+1) is a\ncumulative gain value based on the relevance and ranking of the predicted labels, and top (·,l)\nreturns the lth largest item.\n4.1.3 Comparing Methods and Implementation Details\nTo evaluate the TLC-XML model, we compare it with eight state-of-the-art methods for\nsolving the XMTC task, including the embedding-based method SLEEC [ 16]; the tree-\n123\n25 Page 12 of 21 F. Zhao et al.\nTable 4 Dataset-speciﬁc\nhyperparameters Dataset EP B IL p c\nRCV1 25 12 384 0.15 1\nEURLex-4K 25 8 500 0.1 1\nAAPD 3 12 384 0.15 2\nWiki10-31K 15 6 350 0.15 2\nAmazonCat-13K 10 8 264 0.15 2\nTable 5 Comparison results on RCV1\nEvaluation method P@1 P@3 P@5 nDCG@3 nDCG@5\nSLEEC 95.36 79.32 54.71 90.22 90.89\nParabel 96.15 80.79 55.23 91.31 91.57\nXR-LINEAR 96.30 80.92 55.17 91.48 91.59\nXML-CNN 96.82 81.06 56.02 92.17 92.58\nAttentionXML 97.15 83.63 57.93 94.48 95.30\nCorNetAttentionXML 97.64 83.57 57.92 94.57 95.39\nX-Transformer 96.67 82.36 57.03 93.88 94.85\nLightXML 96.98 82.90 57.26 94.18 95.07\nTLC-XML 97.75 84 .00 58 .08 94 .97 95 .63\nbased methods Parabel [ 18] and XR-LINEAR [ 20]; the Transformers-based methods X-\nTransformer [ 7] and LightXML [ 9]; and other neural network variants XML-CNN [ 21],\nAttentionXML [ 22] and CorNetAttentionXML [ 11]. For fair comparisons, all methods are\nrun on our machine using released code, where the hyperparameters follow the settings given\nin their papers. Traditional methods use bag-of-word (BOW) features to train classiﬁers,\nTransformer-based methods uniformly use a single Transformer model to extract the features\nof the original text, and RNN methods also use a single label tree for prediction.\nThe TLC-XML uses the pre-trained BERT model [ 4] as the text encoder, and combines\nthe [CLS] token at the ﬁnal ﬁve hidden layers to represent text, where the dropout rate of\ntext representation is 0.2. The label semantic embedding matrix Z is represented by FastText\nembeddings of raw label text. The noise thresholds τ\np , τm and τr are set to 0.1, 0.05 and 0.005,\nrespectively, the trade-off parameter λ is set to 0.5, and h (·) uses ReLU activation functions.\nWe use AdamW with 100 warm-up steps as the training optimizer, where the learning rate is\n3e−5 on the ﬁne-tuned Transformer, the learning rate on the remaining layers is 6e −5a n d\nthe weight decay is set to 0.01. In addition, dataset-speciﬁc hyperparameters are shown in\nTable 4,w h e r eEP denotes the number of learning epochs, B denotes the training batch size,\nIL denotes the input token length of the Transformer model, p is a trade-off parameter, and\nc is the number of aggregated layers. All experiments are implemented using the PyTorch\nframework on a single Nvidia 3080 Ti GPU.\n4.2 Experimental Results\nWe compare eight state-of-the-art XMTC methods on ﬁve extremely large-scale datasets,\nand the detailed results are presented in Tables 5, 6, 7, 8 and 9, where the best performance\n123\nTLC-XML: Transformer with Label Correlation for Extreme… Page 13 of 21 25\nTable 6 Comparison results on EURLex-4K\nEvaluation method P@1 P@3 P@5 nDCG@3 nDCG@5\nSLEEC 79.28 64.31 52.32 68.14 61.60\nParabel 82.04 69.36 57.94 72.65 66.37\nXR-LINEAR 82.02 69.58 58.11 72.80 66.43\nXML-CNN 76.32 62.79 51.38 66.23 60.27\nAttentionXML 85.46 73.03 60.97 76.35 70.43\nCorNetAttentionXML 85.56 73.21 61.84 76.53 71.08\nX-Transformer 84.45 72.87 60.78 75.89 69.61\nLightXML 85.07 73.62 61.78 76.47 70.83\nTLC-XML 85.25 73.73 61 .87 76 .83 71 .20\nTable 7 Comparison results on AAPD\nEvaluation method P@1 P@3 P@5 nDCG@3 nDCG@5\nSLEEC 81.92 57.59 38.82 77.68 81.52\nParabel 82.24 57.81 38.46 77.93 81.47\nXR-LINEAR 82.18 57.82 38.79 77.88 81.65\nXML-CNN 74.36 53.85 37.81 71.13 75.94\nAttentionXML 83.70 60.70 41.96 80.10 84.52\nCorNetAttentionXML 85.10 61.47 42.10 81.16 85.52\nX-Transformer 84.74 61.53 42.16 81.12 85.07\nLightXML 83.70 60.43 41.40 79.96 84.06\nTLC-XML 86.50 62 .40 42 .24 82 .32 85 .59\nTable 8 Comparison results on Wiki10-31K\nEvaluation method P@1 P@3 P@5 nDCG@3 nDCG@5\nSLEEC 85.62 72.79 62.58 75.73 67.95\nParabel 84.25 72.44 63.34 74.92 68.36\nXR-LINEAR 84.55 73.00 64.07 75.53 68.72\nXML-CNN 81.39 66.17 56.04 69.77 61.81\nAttentionXML 86.37 77.69 68.80 79.71 73.07\nCorNetAttentionXML 87.14 77.64 68.57 79.87 73.06\nX-Transformer 87.70 77.73 67.08 79.95 72.84\nLightXML 88.45 77.64 68.82 80.22 73.21\nTLC-XML 88.83 78 .41 68 .98 80 .83 73 .70\nis shown in boldface. TLC-XML outperforms the other comparative methods in all metrics\nexcept P@1. However, the P@1 evaluation results of TLC-XML were slightly lower than\nthose of CorNetAttentionXML on EURex-4K and LightXML on AmazonCat-13K. Since\nTLC-XML exploits the correlation between labels, the label co-occurrence in the training\nset can signiﬁcantly affect the prediction performance.\nL or ˆL are larger in the Wiki10-31K\n123\n25 Page 14 of 21 F. Zhao et al.\nTable 9 Comparison results on AmazonCat-13K\nEvaluation method P@1 P@3 P@5 nDCG@3 nDCG@5\nSLEEC 90.48 76.13 61.47 84.86 82.72\nParabel 93.00 79.12 64.55 87.53 85.08\nXR-LINEAR 92.97 78.94 64.28 87.39 84.76\nXML-CNN 93.26 77.07 61.41 86.21 83.43\nAttentionXML 94.07 79.03 63.74 88.01 85.78\nCorNetAttentionXML 95.99 82.38 67.13 91.15 89.31\nX-Transformer 94.90 80.89 65.38 89.68 87.12\nLightXML 96.54 83.64 68.39 92.10 89.97\nTLC-XML 96.23 84.12 68 .63 92 .15 90 .06\nTable 10 Summary of the\nFriedman statistics FF and the\ncritical value in terms of ﬁve\nevaluation metrics\nEvaluation metric FF Critical value ( α = 0.05)\nP@1 14.462\nP@3 19.166\nP@5 20.194 2.244\nnDCG@3 30.884\nnDCG@5 36.541\nand AAPD datasets, as shown in Table 3, so TLC-XML achieves better performance than\nother datasets. In addition, the prediction performance of TLC-XML is signiﬁcantly better\nfor top-3 and top-5 metrics than for top-1 metrics due to the over-smoothing problem. We\ndiscuss the optimal number of aggregation layers in Sect. 4.3.3. To systematically compare\nsuch algorithms, the Friedman test [ 37] is used to evaluate whether there are statistical\nperformance gaps. For each evaluation metric, the average rank of the j th algorithm is\ncomputed by R\nj = 1\nT\n∑ T\ni =1 r j\ni ,w h e r eT = 5 is the number of datasets, and r j\ni denotes the\nranking of the j th algorithm in the i th benchmark dataset. Friedman statistics FF follow the\nF -distribution and can be computed by:\nFF = (T − 1) X 2\nF\nT (P − 1) − X 2\nF\n, (15)\nwhere X 2\nF = 12T\nP(P+1)\n[∑ P\nj =1 R2\nj − P(P+1)2\n4\n]\n,a n d P = 9 is the number of comparison\nalgorithms in our experiment.\nTable 10 shows the Friedman statistics FF and the corresponding critical values are sum-\nmarized for each evaluation metric at a signiﬁcance level α = 0.05. The FF values for\neach evaluation metric are higher than the critical values, so the performance between the\nalgorithms is signiﬁcantly different.\nTo further validate the classiﬁcation performance of TLC-XML against other comparison\nmethods, we employ the Nemenyi test [ 37] to analyze the performance gap among the\ncompared methods. The critical difference (CD) is introduced to compute the difference in the\naverage ranks among the algorithms, where TLC-XML is treated as the control algorithm and\nCD = q\nα\n√P(P + 1)/6T (qα = 3.102 at the signiﬁcance level α = 0.05 and the number of\ncomparison algorithms P = 9). Figure 3 shows CD diagrams of all evaluation metrics, where\n123\nTLC-XML: Transformer with Label Correlation for Extreme… Page 15 of 21 25\nFig. 3 Comparison of TLC-XML with eight comparison algorithms using the Nemenyi test\nthe algorithm not connected with TLC-XML is considered to have signiﬁcantly different\nperformance at the signiﬁcance level α = 0.05. As shown in Fig. 3, TLC-XML outperforms\nother comparative methods in the average rank of each evaluation metric on all datasets.\nIn addition, TLC-XML signiﬁcantly outperforms XML-CNN, SLEEC, Parabel and XR-\nLINEAR at the signiﬁcance level α = 0.05.\nOn the RCV1 and Wiki10-31K datasets, we compare the training time and performance\nof XML-CNN, AttentionXML, LightXML, CorNetAttentionXML, and TLC-XML. Figure 4\nshows the training time and classiﬁcation performance of these methods. The following con-\nclusions can be summarized: (1) TLC-XML achieves the best performance with less training\ntime; (2) the average ranking of TLC-XML and CorNetAttentionXML outperforms other\ncomparison methods by utilizing label correlation, as shown in Fig. 3; and (3) compared with\nCorNetAttentionXML, TLC-XML outperforms CorNetAttentionXML in terms of classiﬁ-\ncation performance and training time.\n4.3 Ablation Studies\n4.3.1 Effect of Partitioning the Label Space\nTo further explore the effect of the label space partitioning method on the classiﬁcation\nperformance, we compare the proposed LGP algorithm to random partitioning (Random)\nand cluster-based partitioning (Cluster) with 512 clusters on the Wiki10-31K dataset. In\naddition, we implement two variant models named LGP-Se and LGP-Co according to LGP\nalgorithm. LGP-Se is implemented by exploiting only the label semantic information in\nPartition. LGP-Co is implemented by exploiting only the label co-occurrence information in\nPartition. Figure 5 shows that the LGP partitioning method outperforms the other partitioning\n123\n25 Page 16 of 21 F. Zhao et al.\nFig. 4 Training time and classiﬁcation performance on RCV1 and Wiki10-31K\nFig. 5 Effect of the partitioning method on the classiﬁcation performance\nmethods, which veriﬁes that using the proposed LGP algorithm to partition the label space\nworks well on the XMTC task.\n4.3.2 Effect of Label Correlation\nWe further investigate the effectiveness of utilizing label correlation information. On the\nWiki10-31K dataset, we compare TLC-XML with three other classiﬁcation models (named\nFC, CorNet [ 11] and MaCor), where FC is implemented by using a fully connected layer,\nCorNet considers label correlation by using a CorNet block, and MaCor uses only the pro-\nposed CCL algorithm to exploit the correlation between clusters. Figure 6 shows that utilizing\nthe label correlation information signiﬁcantly improves the classiﬁcation performance. Dif-\nferent from the CorNet, TLC-XML can extract different levels of correlation between labels,\nand this valuable information is further integrated with feature extraction networks. There-\nfore, TLC-XML signiﬁcantly outperforms other classiﬁcation models on the top-3 and top-5\nevaluation metrics.\n123\nTLC-XML: Transformer with Label Correlation for Extreme… Page 17 of 21 25\nFig. 6 Effect of utilizing label correlation on classiﬁcation performance\n4.3.3 Effect of Aggregated Layers\nTo further verify the effect of aggregated layers in the LIL algorithm, we set different numbers\nof aggregated layers to evaluate the model performance on the RCV1 and Wiki10-31K\ndatasets. Figures 7 and 8 show that the deeper layers can achieve better performance in terms\nof top-3 and top-5 evaluation metrics, mainly due to the deeper aggregated layer capturing\nricher correlation information. However, labels ignore their own information due to over-\naggregating information from neighboring labels, especially in the case of higher accuracy\ntop-1 predictions. For the evaluation metric P@1, the best results are obtained with the\naggregated layer of 1. For the above results, we set the aggregated layer to 1 for smaller\ndatasets and 2 for larger datasets.\n5 Conclusion\nIn this paper, we propose a Transformer-based TLC-XML for XMTC tasks and exploit\nlabel correlations in both label space partitioning and the classiﬁcation model. TLC-XML\ncomprises three modules: Partition, Matcher, and Ranker. In Partition, we utilize semantic and\nco-occurrence correlations between labels to partition the label space. In Matcher, we combine\nthe correlation between clusters and the text representation to match related label clusters. In\nRanker, we aggregate the raw label prediction with neighboring label information and further\nuse residual mapping to avoid over-smoothing. The experimental results demonstrate that\nTLC-XML is signiﬁcantly superior to state-of-the-art XMTC methods. In many practical\nscenarios, it can be expensive and time-consuming to annotate all labels of the sample,\nespecially when the number of possible labels is extremely large. Therefore, it is valuable to\n123\n25 Page 18 of 21 F. Zhao et al.\nFig. 7 Effect of the number of aggregated layers in RCV1\nFig. 8 Effect of the number of aggregated layers in Wiki10-31K\n123\nTLC-XML: Transformer with Label Correlation for Extreme… Page 19 of 21 25\nhandle missing label problems with a robust and efﬁcient strategy. In future work, we plan\nto exploit the correlation between labels for datasets with missing labels.\nAuthor Contributions The authors conﬁrm contribution to the paper as follows: study conception and design:\nFZ, QA; data collection: FZ; analysis and interpretation of results: FZ, QA; draft manuscript preparation: FZ,\nQA, QG, YL. Supervision: QA, XL, WW. All authors reviewed the results and approved the ﬁnal version of\nthe manuscript.\nFunding This research was funded in part by the Natural Science Foundation of Liaoning Province in China\n(2020-MS-281).\nDeclarations\nConﬂict of interest The authors declare that they have no conﬂict of interest.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is\nnot included in the article’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\n1. McAuley JJ, Pandey R, Leskovec J (2015) Inferring networks of substitutable and complementary prod-\nucts. In: Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and\ndata mining, pp 785–794\n2. Jung G, Shin J, Lee S (2023) Impact of preprocessing and word embedding on extreme multi-label patent\nclassiﬁcation tasks. Appl Intell 53(4):4047–4062\n3. Jain H, Balasubramanian V , Chunduri B, V arma M (2019) Slice: scalable linear extreme classiﬁers trained\non 100 million labels for related searches. In: Proceedings of the twelfth ACM international conference\non web search and data mining, pp 528–536\n4. Devlin J, Chang M-W, Lee K, Toutanova K (2018) BERT: pre-training of deep bidirectional transformers\nfor Language understanding. arXiv:1810.04805\n5. Lan Z, Chen M, Goodman S, Gimpel K, Sharma P , Soricut R (2020) ALBERT: a lite BERT for self-\nsupervised learning of language representations. In: International conference on learning representations,\npp 25–32\n6. Yang Z, Dai Z, Yang Y , Carbonell JG, Salakhutdinov R, Le QV (2019) Xlnet: generalized autoregressive\npretraining for language understanding. In: Advances in neural information processing systems, pp 5754–\n5764\n7. Chang W-C, Y u H-F, Zhong K, Yang Y , Dhillon IS (2020) Taming pretrained transformers for extreme\nmulti-label text classiﬁcation. In: 26th ACM SIGKDD conference on knowledge discovery and data\nmining, pp 3163–3171\n8. Ye H, Chen Z, Wang D-H, Davison B (2020) Pretrained generalized autoregressive model with adaptive\nprobabilistic label clusters for extreme multi-label text classiﬁcation. In: International conference on\nmachine learning, pp 10809–10819\n9. Jiang T, Wang D, Sun L, Yang H, Zhao Z, Zhuang F (2021) Lightxml: transformer with dynamic negative\nsampling for high-performance extreme multi-label text classiﬁcation. In: Proceedings of the AAAI\nconference on artiﬁcial intelligence, pp 7987–7994\n10. Zhang J, Chang W-C, Y u H-F, Dhillon I (2021) Fast multi-resolution transformer ﬁne-tuning for extreme\nmulti-label text classiﬁcation. In: Advances in neural information processing systems, pp 7267–7280\n11. Xun G, Jha K, Sun J, Zhang A (2020) Correlation networks for extreme multi-label text classiﬁcation.\nIn: Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery and data\nmining, pp 1074–1082\n123\n25 Page 20 of 21 F. Zhao et al.\n12. Zong D, Sun S (2023) BGNN-XML: bilateral graph neural networks for extreme multi-label text classi-\nﬁcation. IEEE Trans Knowl Data Eng 35(7):6698–6709\n13. Yen IE-H, Huang X, Ravikumar P , Zhong K, Dhillon I (2016) Pd-sparse: a primal and dual sparse approach\nto extreme multiclass and multilabel classiﬁcation. In: International conference on machine learning, pp\n3069–3077\n14. Babbar R, Schölkopf B (2017) Dismec: distributed sparse machines for extreme multi-label classiﬁcation.\nIn: Proceedings of the tenth ACM international conference on web search and data mining, pp 721–729\n(2017)\n15. Schultheis E, Babbar R (2022) Speeding-up one-versus-all training for extreme classiﬁcation via mean-\nseparating initialization. Mach Learn 111(11):3953–3976\n16. Bhatia K, Jain H, Kar P , V arma M, Jain P (2015) Sparse local embeddings for extreme multi-label\nclassiﬁcation. In: Advances in neural information processing systems, pp 730–738\n17. Tagami Y (2017) Annexml: approximate nearest neighbor search for extreme multi-label classiﬁcation.\nIn: Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data\nmining, pp 455–464\n18. Prabhu Y , Kag A, Harsola S, Agrawal R, V arma M (2018) Parabel: partitioned label trees for extreme\nclassiﬁcation with application to dynamic search advertising. In: Proceedings of the 2018 world wide\nweb conference, pp 993–1002\n19. Khandagale S, Xiao H, Babbar R (2020) Bonsai: diverse and shallow trees for extreme multi-label\nclassiﬁcation, machine learning. Mach Learn 109(11):2099–2119\n20. Etter PA, Zhong K, Y u H-F, Ying L, Dhillon I (2022) Enterprise-scale search: accelerating inference for\nsparse extreme multi-label ranking trees. In: Proceedings of the ACM web conference 2022, pp 452–461\n21. Liu J, Chang W-C, Wu Y , Yang Y (2017) Deep learning for extreme multi-label text classiﬁcation. In:\nProceedings of the 40th international ACM SIGIR conference on research and development in information\nretrieval, pp 115–124\n22. Y ou R, Zhang Z, Wang Z, Dai S, Mamitsuka H, Zhu S (2019) AttentionXML: label tree-based attention-\naware deep model for high-performance extreme multi-label text classiﬁcation. In: Advances in neural\ninformation processing systems, pp 5820–5830\n23. Clauset A, Newman ME, Moore C (2004) Finding community structure in very large networks. Phys Rev\nE 70(6):066111\n24. Chen Z-M, Wei X-S, Wang P , Guo Y (2019) Multi-label image recognition with graph convolutional\nnetworks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp\n5177–5186\n25. Xie K, Wei Z, Huang L, Qin Q, Zhang W (2021) Graph convolutional networks with attention for multi-\nlabel weather recognition. Neural Comput Appl 33(17):11107–11123\n26. Tang P , Jiang M, Xia BN, Pitera JW, Welser J, Chawla NV (2020) Multi-label patent categorization\nwith non-local attention-based graph convolutional network. In: Proceedings of the AAAI conference on\nartiﬁcial intelligence, pp 9024–9031\n27. Vu H-T, Nguyen M-T, Nguyen V-C, Pham M-H, Nguyen V-Q, Nguyen V-H (2023) Label-representative\ngraph convolutional network for multi-label text classiﬁcation. Appl Intell 53(12):14759–14774\n28. Hang J-Y , Zhang M-L (2021) Collaborative learning of label semantics and deep label-speciﬁc features\nfor multi-label classiﬁcation. IEEE Trans Pattern Anal Mach Intell 44(12):9860–9871\n29. Xu J, Tian H, Wang Z, Wang Y , Kang W, Chen F (2021) Joint input and output space learning for\nmulti-label image classiﬁcation. IEEE Trans Multimed 23:1696–1707\n30. Kipf TN, Welling M (2016) Semi-supervised classiﬁcation with graph convolutional networks.\narXiv:1609.02907\n31. He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In: Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pp 770–778\n32. Lewis DD, Yang Y , Russell-Rose T, Li F (2004) Rcv1: a new benchmark collection for text categorization\nresearch. J Mach Learn Res 5(4):361–397\n33. Loza Mencía E, Fürnkranz J (2008) Efﬁcient pairwise multilabel classiﬁcation for large-scale problems\nin the legal domain. In: Joint European conference on machine learning and knowledge discovery in\ndatabases, pp 50–65\n34. Yang P , Sun X, Li W, Ma S, Wu W, Wang H (2018) SGM: sequence generation model for multi-label\nclassiﬁcation. In: Proceedings of the 27th international conference on computational linguistics, pp 3915–\n3926\n35. McAuley J, Leskovec J (2013) Hidden factors and hidden topics: understanding rating dimensions with\nreview text. In: Proceedings of the 7th ACM conference on recommender systems, pp 165–172\n123\nTLC-XML: Transformer with Label Correlation for Extreme… Page 21 of 21 25\n36. Prabhu Y , V arma M (2014) Fastxml: a fast, accurate and stable tree-classiﬁer for extreme multi-label\nlearning. In: Proceedings of the 20th ACM SIGKDD international conference on knowledge discovery\nand data mining, pp 263–272\n37. Demšar J (2006) Statistical comparisons of classiﬁers over multiple data sets. J Mach Learn Res 7:1–30\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional afﬁliations.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7678700685501099
    },
    {
      "name": "Exploit",
      "score": 0.6295973658561707
    },
    {
      "name": "XML",
      "score": 0.5923013687133789
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5435590147972107
    },
    {
      "name": "Correlation",
      "score": 0.5305248498916626
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5197712779045105
    },
    {
      "name": "Classifier (UML)",
      "score": 0.484867662191391
    },
    {
      "name": "Transformer",
      "score": 0.41872408986091614
    },
    {
      "name": "Data mining",
      "score": 0.36560046672821045
    },
    {
      "name": "Machine learning",
      "score": 0.32937973737716675
    },
    {
      "name": "Mathematics",
      "score": 0.11591553688049316
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}