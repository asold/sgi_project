{
  "title": "Block-Skim: Efficient Question Answering for Transformer",
  "url": "https://openalex.org/W4226109952",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2108427779",
      "name": "Yue Guan",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2131985026",
      "name": "Zhengyi Li",
      "affiliations": [
        "Institute of Natural Science",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2584500735",
      "name": "Zhouhan Lin",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2236082114",
      "name": "Yuhao Zhu",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2226896803",
      "name": "Jingwen Leng",
      "affiliations": [
        "Institute of Natural Science",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2162841773",
      "name": "Minyi Guo",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2108427779",
      "name": "Yue Guan",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "ShangHai JiAi Genetics & IVF Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2131985026",
      "name": "Zhengyi Li",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "ShangHai JiAi Genetics & IVF Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2226896803",
      "name": "Jingwen Leng",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2162841773",
      "name": "Minyi Guo",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Shanghai Jiao Tong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2749590927",
    "https://openalex.org/W6764724796",
    "https://openalex.org/W6753640285",
    "https://openalex.org/W6781662413",
    "https://openalex.org/W4221162983",
    "https://openalex.org/W2115761837",
    "https://openalex.org/W6760727467",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W3020206637",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2612431505",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2937189735",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W6745742132",
    "https://openalex.org/W6729638187",
    "https://openalex.org/W3022331012",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W6774054309",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W6736522727",
    "https://openalex.org/W2964182247",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W3033188311",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3003074159",
    "https://openalex.org/W2950199911",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3035408713",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3115555382",
    "https://openalex.org/W2557764419",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W3132616766",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3174708387",
    "https://openalex.org/W2963148663",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W2767693128",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2609826708",
    "https://openalex.org/W4287812455",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W3101882350"
  ],
  "abstract": "Transformer models have achieved promising results on natural language processing (NLP) tasks including extractive question answering (QA). Common Transformer encoders used in NLP tasks process the hidden states of all input tokens in the context paragraph throughout all layers. However, different from other tasks such as sequence classification, answering the raised question does not necessarily need all the tokens in the context paragraph. Following this motivation, we propose Block-skim, which learns to skim unnecessary context in higher hidden layers to improve and accelerate the Transformer performance. The key idea of Block-Skim is to identify the context that must be further processed and those that could be safely discarded early on during inference. Critically, we find that such information could be sufficiently derived from the self-attention weights inside the Transformer model. We further prune the hidden states corresponding to the unnecessary positions early in lower layers, achieving significant inference-time speedup. To our surprise, we observe that models pruned in this way outperform their full-size counterparts. Block-Skim improves QA models' accuracy on different datasets and achieves 3 times speedup on BERT-base model.",
  "full_text": "Block-Skim: Efﬁcient Question Answering for Transformer\nYue Guan1,2, Zhengyi Li1,2, Zhouhan Lin1, Yuhao Zhu3, Jingwen Leng1,2, Minyi Guo1,2,\n1 Shanghai Jiao Tong University\n2 Shanghai Qi Zhi Institute\n3 University of Rochester\n{bonboru,hobbit,leng-jw}@sjtu.edu.cn, guo-my@cs.sjtu.edu.cn, lin.zhouhan@gmail.com, yzhu@rochester.edu\nAbstract\nTransformer models have achieved promising results on nat-\nural language processing (NLP) tasks including extractive\nquestion answering (QA). Common Transformer encoders\nused in NLP tasks process the hidden states of all input to-\nkens in the context paragraph throughout all layers. However,\ndifferent from other tasks such as sequence classiﬁcation, an-\nswering the raised question does not necessarily need all the\ntokens in the context paragraph. Following this motivation,\nwe propose Block-Skim, which learns to skim unnecessary\ncontext in higher hidden layers to improve and accelerate\nthe Transformer performance. The key idea of Block-Skim\nis to identify the context that must be further processed and\nthose that could be safely discarded early on during infer-\nence. Critically, we ﬁnd that such information could be suf-\nﬁciently derived from the self-attention weights inside the\nTransformer model. We further prune the hidden states cor-\nresponding to the unnecessary positions early in lower lay-\ners, achieving signiﬁcant inference-time speedup. To our sur-\nprise, we observe that models pruned in this way outperform\ntheir full-size counterparts. Block-Skim improves QA mod-\nels’ accuracy on different datasets and achieves 3\u0002 speedup\non BERTbase model.\nIntroduction\nThe Transformer model (Vaswani et al. 2017) has pushed\nmodel performance on various NLP applications to a new\nstage by introducing multi-head attention (MHA) mech-\nanism (Lin et al. 2017). Further, the Transformer-based\nBERT (Devlin et al. 2018) model advances its performances\nby introducing self-supervised pre-training and has reached\nstate-of-the-art accuracy on many NLP tasks. This has made\nit at the core of many state-of-the-art models, especially in\nrecent question answering (QA) models (Huang et al. 2020).\nOur key insight for QA is that when human beings are\nanswering a question with a passage as a context, they do\nnot spend the same level of comprehension for each of the\nsentences equally across the paragraph. Most of the con-\ntents are quickly skimmed over with little attention on it,\nwhich means that for a speciﬁc question most of the con-\ntents are semantically redundant. However, in the Trans-\nformer architecture, all tokens go through the same amount\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n[CLS] Who played quarterback for the Broncos after Peyton Manning was benched  ? [SEP] \nFollowing their loss in the divisional round of the previous season  's playoffs , the Denver    Broncos \nunderwent numerous coaching changes, including a mutual parting with head coach John Fox ( who \nhad won four divisional championships in his four years as Broncos head coach ),    and the hiring of \nGary Kubiak as the new head coach. under Kubiak, the Broncos planned to install a run - oriented \noffense with zone blocking    to blend in with quarterback Peyton Manning's shotgun passing skills, \nbut struggled with numerous changes and injuries to the offensive line, as well as manning having \nhis worst    statistical season since his rookie year with the Indianapolis Colts in 1998, due to a \nplantar fasciitis injury in his heel that he had suffered since the    summer, and the simple fact that \nManning was getting old, as he turned 39 in the 2015 off - season. Although the team had a 7 – 0 \nstart    , Manning led the NFL in interceptions. In week 10, Manning suffered a partial tear of the \nplantar fasciitis in his left foot. He set     the NFL's all - time record for career passing yards in this \ngame, but was benched after throwing four interceptions in favor of backup quarterback Brock \nOsweiler    , who took over as the starter for most of the remainder of the regular season. Osweiler \nwas injured, however, leading to Manning's return during the    week 17 regular season finale, \nwhere the Broncos were losing 13 – 7 against the 4 – 11 San Diego Chargers, resulting in Manning \nre - claiming the starting quarterback    position for the playoffs by leading the team to a key 27 – \n20 win that enabled the team to clinch the number one overall AFC seed. Under defensive \ncoordinator Wade    Phillips, the Broncos'defense ranked number one in total yards allowed, \npassing yards allowed and sacks, and like the previous three seasons, the team has continued. [SEP]\nFigure 1: Example of Block-Skim method on a query from\nthe SQuAD dataset. The question and answer tokens are an-\nnotated in red. Only question and few evidence blocks are\nfully processed (annotated by yellow). And other blocks are\nskimmed for acceleration with the knowledge from atten-\ntion weights (annotated by grey). Here the block size is 32\ntokens.\nof computation, which suggests that we can take advantage\nof that by discarding many of the tokens early in the lower\nlayers of the Transformer. This semantic level redundancy\nsheds light on effectively reducing the sequence lengths at\nhigher layers. Since the execution overhead of self-attention\nincreases quadratically w.r.t. sequence length, this seman-\ntic level pruning could signiﬁcantly reduce the computation\ntime for long contexts.\nTo excavate the efﬁciency from this insight, we propose to\nﬁrst chop up the context into blocks, and then learn a classi-\nﬁer to terminate those less relevant ones early in lower lay-\ners by looking at the attention weights as shown in Fig. 1.\nMoreover, with the supervision of ground truth answer posi-\ntions, a model that jointly learns to discard context blocks as\nwell as answering questions exhibits signiﬁcantly better per-\nformance over its full-size counterpart. Unfortunately, this\nalso makes the proposed Block-Skim method dedicated for\nextractive QA downstream task. However, QA task is sig-\nniﬁcant in real work production scenarios. Moreover, our\nmethod lies in the trade-off space between generality, us-\nability, and efﬁciency. While sacriﬁcing generality on appli-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n10710\ncable tasks, our proposed method is easy for adoption as it\nworks as a plug-in for existing models. Similarly, leverag-\ning the QA-speciﬁc attention weight patterns makes Block-\nSkim achieves better speedup results than other methods.\nIn this paper, we provide the ﬁrst empirical study on at-\ntention feature maps to show that an attention map could\ncarry enough information to locate the answer scope. We\nthen propose Block-Skim, a plug-and-play module to the\ntransformer-based models, to accelerate transformer-based\nmodels on QA tasks. By handling the attention weight ma-\ntrices as feature maps, the CNN-based Block-Skim module\nextracts information from the attention mechanism to make a\nskim decision. With the predicted block mask, Block-Skim\nskips irrelevant context blocks, which do not enter subse-\nquent layers’ computation. Besides, we devise a new train-\ning paradigm that jointly trains the Block-Skim objective\nwith the native QA objective, where extra optimization sig-\nnals regarding the question position are given to the attention\nmechanism directly.\nIn our evaluation, we show Block-Skim improves the QA\naccuracy and F1 score on all the datasets and models we\nevaluated. Speciﬁcally, BERTbase is accelerated for 3×with-\nout any accuracy loss.\nThis paper contributes to the following 3 aspects.\n• We for the ﬁrst time show that an attention map is effec-\ntive for locating the answer position in the input.\n• We propose Block-Skim, which leverages the attention\nmechanism to improve and accelerate Transformer mod-\nels on QA tasks. The key is to extract information from\nthe attention mechanism during processing and intelli-\ngently predict what blocks to skim.\n• We evaluate Block-Skim on several Transformer-based\nmodel architectures and QA datasets and demonstrate its\nefﬁciency and generality.\nRelated Work\nRecurrent Models with Skimming. The idea to skip or\nskim irrelevant sections or tokens of input sequence has\nbeen studied in NLP models, especially recurrent neural net-\nworks (RNN) (Rumelhart, Hinton, and Williams 1986) and\nlong short-term memory network (LSTM) (Hochreiter and\nSchmidhuber 1997). LSTM-Jump (Yu, Lee, and Le 2017)\nuses the policy-gradient reinforcement learning method to\ntrain an LSTM model that decides how many time steps to\njump at each state. They also use hyper-parameters to con-\ntrol the tokens before a jump, maximum tokens to jump, and\nthe maximum number of jumping. Skim-RNN (Seo et al.\n2018) dynamically decides the dimensionality and RNN\nmodel size to be used at the next time step. In speciﬁc, they\nadopt two ”big” and ”small” RNN models and select the\n”small” one for skimming. Structural-Jump-LSTM (Hansen\net al. 2018) uses two agents to decide whether to jump by a\nsmall step to the next token or structurally to the next punc-\ntuation. Skip-RNN (Campos et al. 2017) learns to skip state\nupdates and thus results in the reduced computation graph\nsize. The difference of Block-Skim to these works is two-\nfold. First, the previous works make the skimming decision\nbased on the hidden states or embeddings during processing.\nHowever, we are the ﬁrst to analyze and utilize the atten-\ntion mechanism for skimming. Secondly, our work is based\non the Transformer model (Vaswani et al. 2017), which has\noutperformed the recurrent type models on most NLP tasks.\nTransformer with Input Reduction. Unlike the sequen-\ntial processing of the recurrent models, the Transformer\nmodel calculates all the input sequence tokens in parallel.\nAs such, skimming can be regarded as a reduction in se-\nquence dimension. Power-BERT (Goyal et al. 2020) extracts\ninput sequence at a token level while processing. During\nthe ﬁne-tuning process for downstream tasks, Goyal et al.\nproposes a soft-extraction layer to train the model jointly.\nLength-Adaptive Transformer (Kim and Cho 2020) further\nextends Power-BERT by forwarding the rejected tokens to\nthe ﬁnal linear layer. Funnel-Transformer (Dai et al. 2020)\nproposes a novel pyramid architecture with input sequence\nlength dimension reduced gradually regardless of seman-\ntic clues. For tasks requiring full sequence length output,\nsuch as masked language modeling and extractive question\nanswering, Funnel-Transformer up-samples at the input di-\nmension to recover. DeFormer (Cao et al. 2020) propose\nto pre-process and cache the paragraphs at shallow layers\nand only concatenate with the question parts at deep lay-\ners. Universal Transformer (Dehghani et al. 2018) proposes\na dynamic halting mechanism that determines the reﬁnement\nsteps for each token. Different from these works, Block-\nSkim utilizes attention information between question and to-\nken pairs and skims the input sequence at the block granu-\nlarity accordingly. Moreover, Block-Skim does not modify\nthe vanilla Transformer model, making it more applicable.\nEfﬁcient Transformer. There are also many efforts for de-\nsigning efﬁcient Transformers (Zhou et al. 2020; Wu et al.\n2019; Tay et al. 2020). For example, researchers have ap-\nplied well-studied compression methods to Transformers,\nsuch as pruning (Guo et al. 2020), quantization (Wang and\nZhang 2020; Guo et al. 2022), distillation (Sanh et al. 2019),\nand weight sharing. Other efforts focus on dedicated efﬁ-\ncient attention mechanism considering its quadratic com-\nplexity of sequence length (Kitaev, Kaiser, and Levskaya\n2019; Beltagy, Peters, and Cohan 2020; Zaheer et al. 2020).\nBlock-Skim is orthogonal to these techniques on the input\ndimension reduction. We demonstrate that Block-Skim is\ncompatible with efﬁcient Transformers with experimental\nresults.\nAttention-based Block Relevance Prediction\nToken-Level Relevance Analysis\nTransformer.The Transformer model adopts the multi-head\nself-attention mechanism and calculates hidden states for\neach position as an attention-based weighted sum of input\nhidden states. The weight vector is calculated by parame-\nterized linear projection query Q and key K as Equation 1.\nGiven a sequence of input embeddings, the output contextual\nembedding is composed by the input sequence with different\nattention at each position,\nAttention(Q; K) =Softmax (QKT =\np\ndk); (1)\n10711\nFigure 2: Attention weight value distribution comparison on\nthe answer and irrelevant tokens. The attention heatmaps are\nproﬁled on the development set of SQuAD dataset with a\nBERTbase model with 12 layers and 12 attention heads per\nlayer. The full results are shown in appendix.\nwhere Q;K are query and key matrix of input embeddings,\ndk is the length of a query or key vector. As such, the at-\ntention weight feature map is often visualized as a heatmap\ndemonstrating the information gathering relationship along\ntokens (Kovaleva et al. 2019). The model exploits multi-\nple parallel groups of such attention weights, a.k.a. attention\nheads, for attending to information at different positions.\nExtractive QA is one of the ultimate downstream tasks in\nthe NLP. Given a text document and a question about the\ncontext, the answer is a contiguous span of the text. To pre-\ndict the start and end position of the input context given a\nquestion, the embedding of each certain token is processed\nfor all the layers in the Transformer encoder model. In many\nend-to-end open-domain QA systems, information retrieval\nis the preceding step at the coarse-grained passage or para-\ngraph level for ﬁltering out irrelevant passages. With the\ncharacteristic of the extractive QA problem where answer\nspans are part of the passage, our question is that whether we\ncan apply a similar ﬁltering technique at ﬁne-grained granu-\nlarity during the Transformer model inference.\nIn this work, we propose to augment the attention mech-\nanism with the ability to predict the relevance of contextual\ntokens without modifying the original Transformer model.\nPrior work (Goyal et al. 2020) shows that attention strength\nis a good indicator for answer tokens. However, we analyze\nthe attention weight distribution of a trained BERTbase model\ntrained with SQuAD (Rajpurkar et al. 2016) dataset and ﬁnd\nthat the attention weights of multi-head attention only have\nnoticeably patterns at the late layers.\nFig. 2 compares the attention weights at Layer 4 and 9\nin the trained BERTbase model. The tokens are classiﬁed to\nanswer tokens or irrelevant tokens with the labels from the\nFigure 3: Accuracy of CNN model predicting whether a\nblock contains answer with attention weight as input. The\nCNN is feed with either an all attention weight heatmap or\nonly the diagonal block region.\ndataset. At late layers like Layer 9, the attention weights of\nanswer tokens are signiﬁcantly larger than those of irrelevant\ntokens. However, at early layers like Layer 4, the attention\nweight strength is indistinguishable for answer tokens and\nirrelevant tokens. For a better latency reduction, it is desir-\nable to ﬁnd irrelevant tokens as early as possible. However,\nusing the attention weight value as the relevance criterion\ncould be problematic at early layers.\nCNN Based Block Relevance Prediction\nGiven the complex pattern of attention weights, we propose\nto use a CNN-based feature extractor to process the attention\nheatmaps as input image channels and predict the relevance\nof each token. To amortize the processing overhead, we split\nthe input sequence X = (x0;x1;:::;x i) into i=k exclusive\nblocks blockj = (xj×k;xj×k+1:::;xj×k+(k−1)), where kis\nthe block size, i.e. tokens included in the continuous input\nspan. The relevance of a block is deﬁned as whether it con-\ntains the exact ﬁnal answer. As such, our goal is to ﬁgure\nout the blocks’ relevance and skim the irrelevant ones dur-\ning Transformer inference.\nFig. 4 shows the details of how we extract the attention in-\nformation from the Transformer and feed them into the CNN\nmodel. In the CNN module, we use two 3 ×3 convolution\nand one 1 ×1 convolution, all of which use theReLU opera-\ntion (Hahnloser and Seung 2001) as the activation function.\nWe insert a2×2 average pooling layer for the ﬁrst two3×3\nconvolutional layers to reduce the feature map size. In addi-\ntion, we also use two batch normalization layers (Ioffe and\nSzegedy 2015) to improve the prediction accuracy. To lo-\ncate the answer context blocks, we use a linear classiﬁcation\nlayer to calculate the score for each block. The module out-\nputs a block-level prediction mask that corresponds to the\nrelevance of a block of input tokens to the question.\nThis model is trained with all attention heatmap proﬁled\nfrom the same set of heatmap data as described before. The\nprediction accuracy is shown as Fig. 3. In general, the model\nachieves decent accuracy demonstrating that a CNN model\nis capable to extract the attending behavior information and\nlocate the answer. Intuitively, the CNN models with higher\nlayer attention heatmaps have better performance. It sug-\ngests that the backbone model becomes more convinced on\nquestion answering when it gets deeper.\n10712\nLinear: Q\nLinear: K\nLinear: V\nConv\nFFN\nMulti-head Attention\nPooling\nConv\nPooling\nConv\nFFN\nSoftmax\nTransformer Layer\nFFN\nBlock-Skim Module\n0 1 ... 31Block0:Xi\n32 33 ... 63Block1:Xi\n64 65 ... 95Block2:Xi\n...\n32K ...BlockK:Xi\nBlock0\nBlock1\nBlock2\n...\nBlockK\nAttention Weight \nHeatmaps\nHeatmap Diagonal \nBlock Reigions\nM1\nM2\nM3\nMK\nBlock Relevance\nPrediction Mask\nInput Sequence\nTensors\nOps\nMul\n32K+1 32K+31\n0 1 ... 31Block0:Xi\n...\n32K ...BlockK:Xi 32K+1 32K+31\nFigure 4: The overall schematic of Block-Skim and the architecture of the CNN model. Here we take block size 32 as example.\nThe total number of blocks and attention heads are K and H. We only show the main operations for simplicity.\nSimplifying CNN Predictor with Diagonal\nAttention\nThe above method of feeding the whole attention feature\nmap to the CNN predictor has a major problem, which is the\npredictor needs to deal with the variable size of the atten-\ntion feature map. As such, we simplify the input to the CNN\nmodel with only attention from its diagonal region. In spe-\nciﬁc, we only feed the diagonal heat-map region as the input\nrepresentation for each input sequence block, as expressed\nin Fig. 4.\nOur hypothesis is that the diagonal region of the atten-\ntion heat map contains sufﬁcient information to identify the\nblock relevance. Because previous works (Clark et al. 2019;\nGuan et al. 2020) show that the attention mechanism has sev-\neral ﬁxed patterns, that is, diagonal, stride, block, or dense\ntypes. And all of these patterns can be easily recognized with\nonly the diagonal region.\nSimilarly, we optimize CNN models with reduced\nheatmap and the result is shown as Fig. 3. As we can see, the\nmodels achieve similar prediction accuracy compared with\nusing a whole attention weight heatmap. The result justiﬁes\nour hypothesis that it is possible to use the diagonal infor-\nmation from attention heatmaps to predict the answer rele-\nvance. By doing so, the computation complexity is also re-\nduced dramatically as the input size is much smaller.\nThe above ﬁnding conﬁrms our hypothesis that the diag-\nonal attention weight indeed carries information for ﬁgur-\ning out answer positions. This motivates us to utilize such\nattention information to narrow the possible answer posi-\ntion along with the processing of the input sequence. In the\nnext section, we introduce our design that uses a plug-and-\nplay end-to-end learning module to extract useful informa-\ntion from the attention weights for skimming decisions.\nTransformer with Block-Skim\nThe previous section shows the feasibility of using the at-\ntention weights to predict the relevance of token blocks.\nHowever, naively using the predictor can lead to signiﬁ-\ncant degradation of the QA task accuracy. Because the block\nrelevance predictor is only trained with the answer labels,\nit could fail in the multi-hop QA task, which requires in-\nformation beyond the answer labels. To solve this prob-\nlem, we propose an end-to-end multi-objective joint train-\ning paradigm. Then during inference time, the prediction of\nthe Block-Skim model is augmented to ﬁlter the input se-\nquence for acceleration. This causes a mismatch between\ntraining and inference models. However, skimming blocks\nduring training makes joint training unstable. And our ex-\nperimental results demonstrate that this mismatch is negligi-\nble. We give a detailed demonstration of the proposed joint\ntraining paradigm and inference process as follows.\nSingle-Task Multi-Objective Joint Training\nFollowing the previous experiments, we append the afore-\nmentioned CNN models to each layer to predict the blocks’\nrelevance and optimize them together with the backbone\nTransformer model. As such, there are two types of classi-\nﬁers in the model augmented with Block-Skim module. The\nﬁrst is the original QA classiﬁer at the last layer and the\nsecond is the block-level relevance classiﬁer at each layer.\nThese two classiﬁers optimize the same downstream task of\npredicting the answer position with an identical target label.\nHowever, they are fed with a different type of loss objectives,\nthat is, the QA objective with Transformer output embed-\ndings and the Block-Skim objective with attention weights.\nWe jointly train these classiﬁers so that the training objective\nis to minimize the sum of all classiﬁers’ losses.\n10713\nThe loss function of each block-level classiﬁer is calcu-\nlated as the cross-entropy loss against the ground truth label\nwhether a block contains answer tokens or not. Equation 2\ngives the formal deﬁnition. The total loss of the block-level\nclassiﬁer LBlockSkim is the sum of all blocks that only con-\ntain passage tokens. The reason is that we only want to throw\naway blocks with irrelevant passage tokens instead of ques-\ntions. Blocks that have question tokens are not used in the\ntraining process.\nLBlockSkim =\nX\nmi∈{passage blocks}\nCELoss(mi;yi)\nyi =\n\u001a1 , block i has answer tokens\n0 , block i has no answer tokens\n(2)\nFor the calculation of the ﬁnal total loss Ltotal, we in-\ntroduce two hyper-parameters in Equation 3. We ﬁrst use a\nharmony coefﬁcient \u000bso that different models and settings\ncould adjust the ratio between the QA loss and block-level\nrelevance classiﬁer loss. It is decided by grid search on the\ndevelopment set. We then use the balance factor \fto adjust\nthe loss from positive and negative relevance blocks because\nthere are typically many more blocks that contain no answer\ntokens (i.e., negative bocks) than the blocks that do contain\nanswer tokens (i.e., positive bocks). This hyper-parameter\nselection will be explained in detail in experiments setup.\nLtotal = LQA + \u000b\nX\nith layer\n(\fLi;y=1\nBlockSkim + Li;y=0\nBlockSkim )\n(3)\nOur Block-Skim is a convenient plugin module owing to\nthe following two reasons. First, it does not affect the back-\nbone model calculation, because it only regularizes the at-\ntention value distribution with extra parameters to the back-\nbone model. In other words, a model trained with Block-\nSkim can be used with it removed. Second, the introduced\nBlock-Skim objective neither needs an extra training signal\nnor reduces the QA accuracy. In fact, we will show that the\nextra gradient signal feeding to the attention improves the\noriginal QA accuracy.\nMulti-hop QA. Our joint training approach can also ad-\ndress the challenge in the multi-hop QA tasks (Yang et al.\n2018), where deriving answers requires multiple pieces of\nevidence and reasoning. Although the block relevance pre-\ndiction only uses the answer label signal, the original QA\ntask ensures that evidence needs to be kept. In other words,\nthe evidence reasoning information is encoded implicitly in\nthe contextual embeddings. To illustrate such a point, we\nperform an ablation study that incorporates the evidence la-\nbel in the Block-Skim predictor training. The predictor ac-\ncuracy does not improve with the additional evidence label,\nwhich conﬁrms the effectiveness of our single-task multi-\nobjective joint training.\nInference with Block-Skim\nWe now describe how to use the Block-Skim to accelerate\nthe QA task inference. Although we add the block-level rel-\nevance classiﬁcation loss in the joint training process, we\ndo not actually throw away any blocks because it can skip\nanswer blocks and the QA task training becomes unsta-\nble. However, we only augment block reduction with the\nBlock-Skim module during the inference for saving compu-\ntation and avoiding heavy changes to the underlying Trans-\nformer. During inference computation, we split the input se-\nquence by the block granularity, which is a hyper-parameter\nin our model. The model skips a set of blocks according to\nthe skimming module results for the following layers. With\nthose design features, Block-Skim works as an add-on com-\nponent to the original Transformer model and is compati-\nble with many Transformer variant models as well as model\ncompression methods.\nWe provide an analytical model to demonstrate the la-\ntency speedup potential of Block-Skim. Suppose that we in-\nsert the Block-Skim module to a vanilla model with the total\nLlayers, and a portion of mi blocks remains for the follow-\ning layers after layer i. The ideal processing complexity of\none token for one Transformer layer is noted asTlayer. Here\nwe make an approximation that the computation complex-\nity is linear to the sequence length N. This is a conservative\napproximation because the attention mechanism is O(N2).\nThe performance speedup is formulated by Equation 4 if\nwe ignore the computation overhead of Block-Skim. In fact,\nthe computation of a single Block-Skim module is smaller\nthan Transformer layers for 100 times. For example, whenP\nmk∈{passage blocks}mk = 0:9, it means 10% of tokens\nare skimmed each layer. This skimming decision will result\nin a total speedup ratio of 1:86×.\nSpeedup = TV anilla\nTBlock−Skim\n= L \u0001 N \u0001 Tlayer\nPL\ni=0(Qi\nj=0\nP\nmj;k∈{layerj}mj;k \u0001 N) \u0001 Tlayer\n= L\nPL\ni=0\nQi\nj=0\nP\nmj;k∈{layerj}mj;k\n(4)\nEvaluation\nExperimental Setup\nDataset. We evaluate our method on 6 extractive QA\ndatasets, including SQuAD 1.1 (Rajpurkar et al. 2016),\nNatural Questions (Kwiatkowski et al. 2019), Trivi-\naQA (Joshi et al. 2017), NewsQA (Trischler et al. 2016),\nSearchQA(Dunn et al. 2017) and HotpotQA (Yang et al.\n2018). HotpotQA provides questions that require multi-hop\nreasoning to answer with supporting facts. The diversity of\nthese datasets such as various passage lengths and different\ndocument sources lets us evaluate the general applicability\nof the proposed method.\nModel. We follow the setting of the BERT model to use\nthe structure of the Transformer encoder and a linear classi-\nﬁcation layer for all the datasets. As previously explained,\nBlock-Skim works as an add-on module to the vanilla\nTransformer, and therefore is generally applicable to all\nTransformer-based models, as well as model compression\nmethods. To illustrate this point, we apply the Block-Skim\nmethod to two BERT models with different size settings. We\nevaluate the base setting with 12 heads and 12 layers, as well\n10714\nDatasets SQuAD HotpotQA NewsQA\nNaturalQuestions Tri\nviaQA SearchQA Avg.\nF1 Speedup F1 Speedup F1 Speedup F1 Speedup F1 Speedup F1 Speedup F1 Speedup\nBalance F\nactor 20 20 30 30 100 150 -\nVanilla\nBERT 88.32 1× 74.39 1× 66.57 1× 78.85 1× 72.61 1× 79.93 1× 76.78 1×\nBlock-Skim T\nraining 88.92 1× 74.88 1× 67.76 1× 78.98 1× 73.29 1× 80.32 1× 77.36 1×\nBlock-Skim Inference 88.52 3:01× 74.47 2:28× 65.14 2:53× 78.48 2:56× 72.80 1:81× 79.84 3:17× 76.54 2:56×\nDeformer 87.2 3:1× - - - - - - - - - - - -\nLength-Adapti\nve Transformer 88.7 2:22× - - - - - - - - - - - -\nTable 1: Validation F1 score and FLOPs speedup of BERTbase model evaluated on different QA datasets. The balance factor is\ndetermined by calculating the block number distribution on training set.\nas the large setting with 24 layers and 16 heads as described\nin prior work (Devlin et al. 2018).\nModel Compression Methods. We conduct the following\nmodel compression methods on BERTbase models to demon-\nstrate the compatibility of our Block-Skim.\n• Distillation. Knowledge distillation uses a teacher model\nto transfer the knowledge to a smaller student model.\nHere we adopt DistilBERT (Sanh et al. 2019) setting to\ndistill a 6-layer model from the BERTbase model.\n• Weight Sharing. By sharing weight parameters among\nlayers, the amount of weight parameters reduces. Note\nthat weight sharing does not impact the computation\nFLOPs (ﬂoating-point operations). We evaluate Block-\nSkim on ALBERT (Lan et al. 2019) that shares weight\nparameters among all layers.\n• Pruning. Instead of conventional weight pruning tech-\nniques, we evaluate head pruning (Michel, Levy, and\nNeubig 2019) that is speciﬁc to the attention mecha-\nnism in Transformer models. The pruning of attention\nheads reduces the input feature size to the Block-Skim\nmodule. We prune 50% of attention heads based on the\nattention head importance criterion introduced in prior\nwork (Michel, Levy, and Neubig 2019).\nInput Dimension Reduction Baselines. We also com-\npare with input dimension reduction methods Deformer and\nLength-Adaptive Transformer. Deformer(Cao et al. 2020)\npre-process and caches the context paragraphs at early lay-\ners to reduce the actual inference sequence length. Length-\nAdaptive Transformer (Kim and Cho 2020) is a successive\nversion of Power-BERT which forwards the tokens rejected\nto the ﬁnal layer by attention strength.\nTraining Setting. We implement the proposed method\nbased on open-sourced library from Wolf et al. (2019)1. For\neach baseline model, we use the released pre-trained check-\npoints 2. We follow the training setting used by Devlin et al.\n(2018) and Liu et al. (2019) to perform the ﬁne-tuning on\nthe above extractive QA datasets. We initialize the learning\nrate to 3e−5 for BERT models and 5e−5 for ALBERT\nwith a linear learning rate scheduler. For SQuAD dataset,\nwe apply batch size 16 and maximum sequence length 384.\n1The source code is available at https://github.com/\nChandlerGuan/blockskim.\n2We use pre-trained language model checkpoints released from\nhttps://huggingface.co/models.\nSeed 0\n1 2 3 4 Avg. Std.\nVanilla EM\n80.95 81.08 80.98 81.06 80.96 81.00 0.06\nF1 88.32 88.44 88.59 88.44 88.42 88.44 0.10\nBlock-Skim EM 81.52\n81.25 81.24 81.51 81.84 81.47 0.25\nF1 88.92 88.48 88.66 88.76 88.99 88.76 0.20\nTable 2: Results of multiple runs under same training and\nhyper-parameter setting with different random seeds.\nAnd for the other datasets, we apply batch size 32 and max-\nimum sequence length 512. We perform all the experiments\nreported with random seed 42. We train a baseline model\nand Block-Skim model with the same setting for two epochs\nand report accuracies from MRQA task benchmark for com-\nparison. We use four V100 GPUs with 32 GB memory for\nthe training experiments.\nThe balance factor \fis determined by block sample num-\nbers and reported in Tbl. 1. The harmony factor \u000b is 0:01\nfor ALBERT and 0:1 for all the other models we used. It is\ndetermined by hyper-parameter grid search from 1e−3 to\n10 with a step of ×10.\nWe use the inference FLOPs as a general measurement\nof the model computational complexity on all platforms. We\nuse TorchProﬁle(Liu 2020) to calculate the FLOPs for each\nmodel and normalize the results as a ratio to BERTbase.\nJoint Training Results\nWe ﬁrst evaluate Block-Skim joint training effect to the QA\ntask by comparing BERTbase models and their variants with\nBlock-Skim augmented. In their Block-Skim versions, the\nBlock-Skim modules only participate in the training process\nand are removed in the inference task. Tbl. 1 shows the re-\nsult on multiple QA datasets. Block-Skim outperforms the\nbaseline training objective on all datasets evaluated and ex-\nceeds with 0:58% F1 score on average. This suggests that\nthe Block-Skim objective is consistent with the QA objec-\ntive and even improves its accuracy. The results show the\nwide applicability of our method to different datasets with\nvarying difﬁculty and complexity.\nWe further show the robustness when using the Block-\nSkim joint training as an add-on module. Tbl. 2 shows the re-\nsult of multiple runs using the identical optimization setting\nwith different random seeds. By introducing the Block-Skim\nloss in Training, the QA accuracy of the backbone model\nis improved for 0:4 on exact match and 0:32 on F1 score.\n10715\nFigure 5: FLOPs speedup of different models and model\ncompression methods with Block-Skim on SQuAD and Hot-\npotQA datasets. The FLOPs are normalized to BERTbase re-\nsult of 48:32G FLOPs result. The results of vanilla mod-\nels with different size, model compression algorithms and\nBlock-Skim augmented methods are grouped together.\nAnd triggering Block-Skim always surpasses the backbone\nmodel with the same setting. This is because the extra train-\ning objective provides direct gradient signals to the attention\nmechanism and regularizes its value distribution.\nInference Speedup Results\nResults on Various Datasets. The FLOPs speedup result\nnormalized to BERT base model is demonstrated in Tbl. 1.\nBlock-Skim achieves 2:59×speedup on average with a mi-\nnor accuracy degradation of 0:23 on different datasets eval-\nuated. On the multi-hop QA dataset HotpotQA, our method\nalso achieves 2:28 times speedup. The results show that the\nproposed Block-Skim method is capable to identify the se-\nmantic redundancy with attention information.\nComparison to Vanilla BERT Baseline. Block-Skim im-\nproves the BERTbase model inference latency by 3:1×and\n2:4×respectively on SQuAD and HotpotQA datasets. When\ntreating the model size settings of vanilla BERT model as\na trade-off between accuracy and complexity, Block-Skim\nimproves this trade-off by a margin. As shown in Fig. 5 our\nmethod accelerate BERTlarge as fast as the vanilla BERTbase\nmodel but with a much higher accuracy. In speciﬁc, the la-\ntency of vanilla BERTlarge model is 3:47×of BERTbase, and\nour method reduces the gap to 1:09×on SQuAD dataset,\nwhich translates to the 3:18×speedup.\nCompatibility to Model Compression Methods. We com-\npare the Block-Skim’s compatibility to other model com-\npression methods with Fig. 5. These model compression\nmethods trade accuracy for computation complexity to dif-\nferent extents. For example, distilling 12-layer BERT base\nmodel to 6 layers results in a 2% accuracy decrease and\nID Description Update\nT\nransformer\nSkim\nTraining\nBlock-Skim\nModule\nBlock\nSize\nQA\nEM F1\nSQuAD\n1 Baseline X - -\n- 80.92 88.32\n2\nBlock-Skim X X 32 81.52 88.92\n3\nFreeze Transformer X 32 80.92 88.32\n4\nSkim Traning X X\nX 32 79.27 86.83\n5 Block\nSize 1 X X 1 81.22 88.60\n6\nBlock Size 8 X X 8 81.25 88.63\n7\nBlock Size 16 X X 16 81.35 88.75\n8\nBlock Size 64 X X 64 81.39 88.65\n9\nBlock Size 128 X X 128 80.90 88.33\nHotpotQA\n10 Baseline X - -\n- 60.37 74.39\n11\nBlock-Skim X X 32 60.54 74.88\n12\nEvidence Loss X X 32 60.78 74.85\nTable 3: Ablation studies of the Block-Skim components\nwith BERTbase backbone model on SQuAD and HotpotQA\ndatasets.\n2 times speedup. With Block-Skim method appended to\nthis model, the methods can be further accelerated with\nno or minor accuracy loss. Speciﬁcally, using Block-Skim\nwith DistilBERT achieves 5× speedup compared to the\nvanilla BERT model. And even with head-pruning reduc-\ning the attention information, Block-Skim is also compatible\nand achieves over 2×speedup. Even though still compati-\nble, Block-Skim gets less acceleration on ALBERT models.\nWe suggest that sharing parameters of attention mechanism\nmakes it harder to optimize with extra Block-Skim objective.\nAs the proposed Block-Skim method aims to reduce the in-\nput sequence dimension semantic redundancy, it is compati-\nble to these model compression methods focusing on model\nredundancy theoretically. By designing Block-Skim not to\nmodify the backbone model, our method is generally ap-\nplicable to these algorithms as well as other model pruning\nmethods (Guo et al. 2020; Qiu et al. 2019; Gan et al. 2020).\nBlock-Skim achieves close speedup with less accuracy\ndegradation compared to Deformer and more speedup with\nsimilar accuracy degradation compared to Length-Adaptive\nTransformer on SQuAD-1.1 dataset. This suggests Block-\nSkim captures the runtime semantic redundancy better. Al-\nthough this also makes it only applicable to QA tasks.\nAblation Study\nWe design a series ablation experiment of components in\nBlock-Skim to study their individual effect. The experiments\nare performed based on the same setting. We report the de-\ntailed results in Tbl. 3, and summarize the key ﬁndings as\nfollows.\nID-3. Instead of joint training, we perform a two-step train-\ning. We ﬁrst perform the ﬁne-tuning for the QA task. We\nthen perform the Block-Skim training with the baseline QA\nmodel frozen. In other words, we only use the Block-Skim\nobjective and only update the weights in the Block-Skim\nmodules. Therefore, the QA accuracy remains the same as\nthe baseline model, which is lower than the joint training\n(ID-3). Meanwhile, the Block-Skim classiﬁer also has a\nlower accuracy than the joint training especially at layer 6.\nID-4 We skim blocks during the joint Block-Skim QA train-\ning process. Because the mis-skimmed blocks may confuse\n10716\nthe QA optimization, it leads to a considerable accuracy loss.\nID-5-ID-9. We study the impact of different block sizes.\nSpeciﬁcally, when the block size is 1, it is equivalent to\nskim at the token granularity. Our experimental result shows\nthat the accuracy of Block-Skim classiﬁer is better when the\nblock size is larger. On the other hand, a larger block size\nalso leads to less number of blocks and therefore the perfor-\nmance speedup becomes limited. To this end, we choose the\nblock size of 32 as a design sweet spot.\nID-11-ID-12. We evaluate the applicability of Block-Skim\nto multi-hop QA task with HotpotQA dataset. As introduced\nin , we add supporting facts (i.e., evidence) for each question\nto the Block-Skim objective in the ID-12 experiment by la-\nbelling evidence blocks to 1 in Eq.2 for the skim predictor\nmodules. This leads to a higher QA accuracy. But the aver-\nage accuracy of skim predictors at all layers is worse, which\nis 86:08% compared to 92:67%. This ablation experiment\nshows that our single-task multi-objective joint training is\nalready able to capture the evidence information, rendering\nexplicitly adding it to the training unnecessary.\nConclusion\nIn this work, we propose a plug-and-play Block-Skim mod-\nule to Transformer and its variants for efﬁcient QA process-\ning. We empirically demonstrate that the attention mecha-\nnism can provide instructive information for locating the an-\nswer span. Leveraging this insight, we propose to learn the\nattention in a supervised manner, which terminates irrelevant\nblocks at early layers, signiﬁcantly reducing the computa-\ntions. Besides, the proposed Block-Skim training objective\nprovides attention mechanism with extra learning signal and\nimproves QA accuracy on all datasets and models we evalu-\nated. With the use of Block-Skim module, such distinction is\nstrengthened in a supervised fashion. This idea may be also\napplicable to other tasks and architectures.\nAppendix Attention Distribution\nWe show the full results of attention weight value distribu-\ntion discussed in Fig.2. Fig. 6 shows that deeper layers have\nmore distinguishable patterns.\nAcknowledgements\nThis work was supported by the National Key R&D Pro-\ngram of China under Grant 2019YFF0302600, and the Na-\ntional Natural Science Foundation of China (NSFC) grant\n(62072297, 62106143, and 61832006). We would like to\nthank the anonymous reviewers for their thoughtful com-\nments and constructive suggestions. Zhouhan Lin is also\nsupported by Shanghai Pujiang Program. We also thank\nYuxian Qiu and Kexin Li with whom we have inspiring dis-\ncussions on the evaluation experiment design. Finally, we\nthank Zhihui Zhang for helping the presentation and visual-\nization of experimental results. Jingwen Leng and Zhouhan\nLin are the corresponding authors of this paper.\nFigure 6: Attention weight value distribution comparison on\nthe answer and irrelevant tokens. The attention heatmaps are\nproﬁled on the development set of SQuAD dataset with a\nBERTbase model with 12 layers and 12 attention heads per\nlayer.\n10717\nReferences\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150.\nCampos, V .; Jou, B.; Gir´o-i-Nieto, X.; Torres, J.; and Chang,\nS. 2017. Skip RNN: Learning to Skip State Updates in Re-\ncurrent Neural Networks. CoRR, abs/1708.06834.\nCao, Q.; Trivedi, H.; Balasubramanian, A.; and Balasub-\nramanian, N. 2020. Deformer: Decomposing pre-trained\ntransformers for faster question answering. arXiv preprint\narXiv:2005.00697.\nClark, K.; Khandelwal, U.; Levy, O.; and Manning, C. D.\n2019. What Does BERT Look at? An Analysis of BERT’s\nAttention. In Proceedings of the 2019 ACL Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Networks for\nNLP, 276–286.\nDai, Z.; Lai, G.; Yang, Y .; and Le, Q. V . 2020.\nFunnel-Transformer: Filtering out Sequential Redundancy\nfor Efﬁcient Language Processing. arXiv preprint\narXiv:2006.03236.\nDehghani, M.; Gouws, S.; Vinyals, O.; Uszkoreit, J.; and\nKaiser, L. 2018. Universal Transformers. In International\nConference on Learning Representations.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDunn, M.; Sagun, L.; Higgins, M.; Guney, V . U.; Cirik,\nV .; and Cho, K. 2017. Searchqa: A new q&a dataset aug-\nmented with context from a search engine. arXiv preprint\narXiv:1704.05179.\nGan, Y .; Qiu, Y .; Leng, J.; Guo, M.; and Zhu, Y . 2020.\nPtolemy: Architecture Support for Robust Deep Learning.\nIn 2020 53rd Annual IEEE/ACM International Symposium\non Microarchitecture (MICRO).\nGoyal, S.; Choudhary, A. R.; Chakaravarthy, V .; Man-\nishRaje, S.; Sabharwal, Y .; and Verma, A. 2020. PoWER-\nBERT: Accelerating BERT inference for Classiﬁcation\nTasks. arXiv preprint arXiv:2001.08950.\nGuan, Y .; Leng, J.; Li, C.; Chen, Q.; and Guo, M. 2020.\nHow Far Does BERT Look At: Distance-based Cluster-\ning and Analysis of BERT ′s Attention. arXiv preprint\narXiv:2011.00943.\nGuo, C.; Hsueh, B. Y .; Leng, J.; Qiu, Y .; Guan, Y .; Wang,\nZ.; Jia, X.; Li, X.; Guo, M.; and Zhu, Y . 2020. Acceler-\nating Sparse DNN Models without Hardware-Support via\nTile-Wise Sparsity. arXiv preprint arXiv:2008.13006.\nGuo, C.; Qiu, Y .; Leng, J.; Gao, X.; Zhang, C.; Liu, Y .; Yang,\nF.; Zhu, Y .; and Guo, M. 2022. SQuant: On-the-Fly Data-\nFree Quantization via Diagonal Hessian Approximation. In\nInternational Conference on Learning Representations.\nHahnloser, R. H.; and Seung, H. S. 2001. Permitted and for-\nbidden sets in symmetric threshold-linear networks. In Ad-\nvances in neural information processing systems, 217–223.\nHansen, C.; Hansen, C.; Alstrup, S.; Simonsen, J. G.; and Li-\noma, C. 2018. Neural Speed Reading with Structural-Jump-\nLSTM. In International Conference on Learning Represen-\ntations.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation, 9(8): 1735–1780.\nHuang, Z.; Xu, S.; Hu, M.; Wang, X.; Qiu, J.; Fu, Y .; Zhao,\nY .; Peng, Y .; and Wang, C. 2020. Recent Trends in Deep\nLearning Based Open-Domain Textual Question Answering\nSystems. IEEE Access, 8: 94341–94356.\nIoffe, S.; and Szegedy, C. 2015. Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal\nCovariate Shift. In International Conference on Machine\nLearning, 448–456.\nJoshi, M.; Choi, E.; Weld, D. S.; and Zettlemoyer, L. 2017.\nTriviaQA: A Large Scale Distantly Supervised Challenge\nDataset for Reading Comprehension. In Proceedings of the\n55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 1601–1611.\nKim, G.; and Cho, K. 2020. Length-Adaptive Transformer:\nTrain Once with Length Drop, Use Anytime with Search.\narXiv preprint arXiv:2010.07003.\nKitaev, N.; Kaiser, L.; and Levskaya, A. 2019. Reformer:\nThe Efﬁcient Transformer. In International Conference on\nLearning Representations.\nKovaleva, O.; Romanov, A.; Rogers, A.; and Rumshisky, A.\n2019. Revealing the dark secrets of BERT. arXiv preprint\narXiv:1908.08593.\nKwiatkowski, T.; Palomaki, J.; Redﬁeld, O.; Collins, M.;\nParikh, A.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin,\nJ.; Lee, K.; et al. 2019. Natural questions: a benchmark for\nquestion answering research. Transactions of the Associa-\ntion for Computational Linguistics, 7: 453–466.\nLan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.;\nand Soricut, R. 2019. ALBERT: A Lite BERT for Self-\nsupervised Learning of Language Representations. In In-\nternational Conference on Learning Representations.\nLin, Z.; Feng, M.; Santos, C. N. d.; Yu, M.; Xiang, B.; Zhou,\nB.; and Bengio, Y . 2017. A structured self-attentive sentence\nembedding. arXiv preprint arXiv:1703.03130.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach.\nLiu, Z. 2020. Torchproﬁle. https://github.com/zhijian-liu/\ntorchproﬁle/.\nMichel, P.; Levy, O.; and Neubig, G. 2019. Are Sixteen\nHeads Really Better than One? Advances in Neural Infor-\nmation Processing Systems, 32: 14014–14024.\nQiu, Y .; Leng, J.; Guo, C.; Chen, Q.; Li, C.; Guo, M.; and\nZhu, Y . 2019. Adversarial Defense Through Network Proﬁl-\ning Based Path Extraction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR).\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSQuAD: 100, 000+ Questions for Machine Comprehension\nof Text. In EMNLP.\nRumelhart, D. E.; Hinton, G. E.; and Williams, R. J. 1986.\nLearning representations by back-propagating errors. na-\nture, 323(6088): 533–536.\n10718\nSanh, V .; Debut, L.; Chaumond, J.; and Wolf, T. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter. arXiv preprint arXiv:1910.01108.\nSeo, M.; Min, S.; Farhadi, A.; and Hajishirzi, H. 2018. Neu-\nral Speed Reading via Skim-RNN. In International Confer-\nence on Learning Representations.\nTay, Y .; Dehghani, M.; Bahri, D.; and Metzler, D. 2020. Ef-\nﬁcient Transformers: A Survey. arXiv e-prints, arXiv–2009.\nTrischler, A.; Wang, T.; Yuan, X.; Harris, J.; Sordoni, A.;\nBachman, P.; and Suleman, K. 2016. NEWSQA: A MA-\nCHINE COMPREHENSION DATASET.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWang, C.; and Zhang, X. 2020. Q-BERT: A BERT-based\nFramework for Computing SPARQL Similarity in Natural\nLanguage. In Companion Proceedings of the Web Confer-\nence 2020, 65–66.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-\nson, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y .; Plu,\nJ.; Xu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest,\nQ.; and Rush, A. M. 2019. HuggingFace’s Transform-\ners: State-of-the-art Natural Language Processing. ArXiv,\nabs/1910.03771.\nWu, Z.; Liu, Z.; Lin, J.; Lin, Y .; and Han, S. 2019. Lite\nTransformer with Long-Short Range Attention. In Interna-\ntional Conference on Learning Representations.\nYang, Z.; Qi, P.; Zhang, S.; Bengio, Y .; Cohen, W. W.;\nSalakhutdinov, R.; and Manning, C. D. 2018. HotpotQA:\nA Dataset for Diverse, Explainable Multi-hop Question An-\nswering. In EMNLP.\nYu, A. W.; Lee, H.; and Le, Q. 2017. Learning to Skim Text.\nIn Proceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers).\nZaheer, M.; Guruganesh, G.; Dubey, A.; Ainslie, J.; Alberti,\nC.; Ontanon, S.; Pham, P.; Ravula, A.; Wang, Q.; Yang, L.;\net al. 2020. Big bird: Transformers for longer sequences.\narXiv preprint arXiv:2007.14062.\nZhou, W.; Xu, C.; Ge, T.; McAuley, J.; Xu, K.; and Wei, F.\n2020. BERT Loses Patience: Fast and Robust Inference with\nEarly Exit. arXiv, arXiv–2006.\n10719",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7586495876312256
    },
    {
      "name": "Transformer",
      "score": 0.6654680967330933
    },
    {
      "name": "Speedup",
      "score": 0.5928958654403687
    },
    {
      "name": "Inference",
      "score": 0.5682229995727539
    },
    {
      "name": "Paragraph",
      "score": 0.544424831867218
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5369242429733276
    },
    {
      "name": "Natural language processing",
      "score": 0.47957223653793335
    },
    {
      "name": "Surprise",
      "score": 0.4582293629646301
    },
    {
      "name": "Encoder",
      "score": 0.43373364210128784
    },
    {
      "name": "Question answering",
      "score": 0.43174678087234497
    },
    {
      "name": "Communication",
      "score": 0.06496590375900269
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I5388228",
      "name": "University of Rochester",
      "country": "US"
    }
  ],
  "cited_by": 21
}