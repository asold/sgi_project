{
  "title": "Hybrid Autoregressive and Non-Autoregressive Transformer Models for Speech Recognition",
  "url": "https://openalex.org/W4212772209",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2958005215",
      "name": "Zhengkun Tian",
      "affiliations": [
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2539317246",
      "name": "Jiangyan Yi",
      "affiliations": [
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2104925143",
      "name": "Jianhua Tao",
      "affiliations": [
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2107936128",
      "name": "Shuai Zhang",
      "affiliations": [
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2141554513",
      "name": "Zhengqi Wen",
      "affiliations": [
        "Institute of Automation"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W854541894",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2526425061",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2144499799",
    "https://openalex.org/W2962760690",
    "https://openalex.org/W2976556660",
    "https://openalex.org/W3016010032",
    "https://openalex.org/W6769806307",
    "https://openalex.org/W3112157188",
    "https://openalex.org/W3097882114",
    "https://openalex.org/W3162899666",
    "https://openalex.org/W3160622492",
    "https://openalex.org/W3097874139",
    "https://openalex.org/W3097625183",
    "https://openalex.org/W6790121257",
    "https://openalex.org/W2973122799",
    "https://openalex.org/W3011339933",
    "https://openalex.org/W6763832098",
    "https://openalex.org/W2936078256",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6731370813",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W3096160024",
    "https://openalex.org/W4287628715",
    "https://openalex.org/W3093941495",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2989134874",
    "https://openalex.org/W4285789737",
    "https://openalex.org/W2946200149",
    "https://openalex.org/W3162431424",
    "https://openalex.org/W3125815078",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W4288088457",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "The autoregressive (AR) models, such as attention-based encoder-decoder\\nmodels and RNN-Transducer, have achieved great success in speech recognition.\\nThey predict the output sequence conditioned on the previous tokens and\\nacoustic encoded states, which is inefficient on GPUs. The non-autoregressive\\n(NAR) models can get rid of the temporal dependency between the output tokens\\nand predict the entire output tokens in at least one step. However, the NAR\\nmodel still faces two major problems. On the one hand, there is still a great\\ngap in performance between the NAR models and the advanced AR models. On the\\nother hand, it's difficult for most of the NAR models to train and converge. To\\naddress these two problems, we propose a new model named the two-step\\nnon-autoregressive transformer(TSNAT), which improves the performance and\\naccelerating the convergence of the NAR model by learning prior knowledge from\\na parameters-sharing AR model. Furthermore, we introduce the two-stage method\\ninto the inference process, which improves the model performance greatly. All\\nthe experiments are conducted on a public Chinese mandarin dataset ASIEHLL-1.\\nThe results show that the TSNAT can achieve a competitive performance with the\\nAR model and outperform many complicated NAR models.\\n",
  "full_text": "TSNAT: Two-Step Non-Autoregressvie Transformer Models\nfor Speech Recognition\nZhengkun Tian1,2, Jiangyan Yi1, Jianhua Tao1,2,3, Ye Bai1,2, Shuai Zhang1,2,\nZhengqi Wen1, Xuefei Liu1\n1NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China\n2School of ArtiÔ¨Åcial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n3 CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China\n{zhengkun.tian, jiangyan.yi, jhtao, ye.bai, shuai.zhang, zqwen, xuefei.liu}@nlpr.ia.ac.cn\nAbstract\nThe autoregressive (AR) models, such as attention-based\nencoder-decoder models and RNN-Transducer, have achieved\ngreat success in speech recognition. They predict the out-\nput sequence conditioned on the previous tokens and acous-\ntic encoded states, which is inefÔ¨Åcient on GPUs. The non-\nautoregressive (NAR) models can get rid of the temporal de-\npendency between the output tokens and predict the entire out-\nput tokens in at least one step. However, the NAR model still\nfaces two major problems. On the one hand, there is still a\ngreat gap in performance between the NAR models and the\nadvanced AR models. On the other hand, it‚Äôs difÔ¨Åcult for\nmost of the NAR models to train and converge. To address\nthese two problems, we propose a new model named the two-\nstep non-autoregressive transformer(TSNAT), which improves\nthe performance and accelerating the convergence of the NAR\nmodel by learning prior knowledge from a parameters-sharing\nAR model. Furthermore, we introduce the two-stage method\ninto the inference process, which improves the model perfor-\nmance greatly. All the experiments are conducted on a public\nChinese mandarin dataset ASIEHLL-1. The results show that\nthe TSNAT can achieve a competitive performance with the AR\nmodel and outperform many complicated NAR models.\nIndex Terms: Autoregressive, Non-Autoregressive, Trans-\nformer, Two-Step, Speech Recognition\n1. Introduction\nEnd-to-end models have achieved great success in speech\nrecognition, especially attention-based encoder-decoder models\n[1, 2, 3, 4] and transducer-based models [5, 6, 7, 8, 9]. Most of\nthese end-to-end models generate the target sequence in an au-\ntoregressive fashion, which predicts the next token conditioned\non the previously generated tokens and the acoustic encoded se-\nquence. The autoregressive characteristic makes the inference\nprocess must be carried out step by step, which cannot be im-\nplemented in parallel and results in a large latency. By contrast,\nthe non-autoregressive model (NAR) can get rid of the temporal\ndependency and directly generate the target sequences based on\nthe encoded acoustic states in at least one step.\nAlthough the Non-autoregressive models can perform in-\nference very efÔ¨Åciently, it still faces two signiÔ¨Åcant problems.\nOn the one hand, there is still a great gap in performance be-\ntween the advanced autoregressive (AR) model and the non-\nautoregressive model. Chen et.al proposed two kinds of itera-\ntive inference methods to alleviate the problem [10]. Too many\niterations have a negative impact on the speed of inference. Be-\nsides, some researchers also utilized a CTC Model to generate\nthe preliminary predictions and then correct the previous pre-\ndiction by a non-autoregressive decoder [11, 12, 13]. However,\nit will introduce some new problems. It is hard to eliminate\naccumulated error caused by the CTC Models and carry out\nthe frame-wise operation of the CTC module in parallel. On\nthe other hand, it is difÔ¨Åcult for most of the non-autoregressive\ntransformer models to train and converge. To our knowledge,\nthere are three major ways to alleviate this problem. Firstly,\nsome works try to introduce an auxiliary CTC loss to acceler-\nate the training and convergence [12, 14]. Secondly, more it-\nerations in the training process also bring the improvement on\nthe performance [10, 15], which is very simple and straightfor-\nward. Thirdly, instead of learning from a sequence partially or\nfully Ô¨Ålled with <MASK> [10, 15], some works try to improve\ntraining efÔ¨Åciency by providing a preliminary sequence with the\ninformation of target sequence to the non-autoregressive trans-\nformer decoder [11, 12, 13, 14]. These methods are either time-\nconsuming or difÔ¨Åcult to implement.\nThe traditional NAR model predicts the output tokens con-\nditional on the acoustic encoded states and a sequence that is\ncompletely empty and does not contain any prior knowledge. In\norder to make the training match the inference, the NAR model\ntries to learn from the sequence fully Ô¨Ålled with <MASK> in\nthe training process, which hinders the improvement of train-\ning speed and performance. To address this problem, inspired\nby the dual-mode ASR (streaming and non-streaming) [16] and\ntwo-pass end-to-end models [17, 18, 19], we propose a model\nnamed two-step non-autoregressive transformer (TSNAT). The\nTSNAT utilizes an AR model to accelerate the NAR model\nconvergence and improve the performance. Our model con-\nsists of an optional frond-end block, an acoustic encoder, and\na dual-mode transformer decoder. The dual-mode transformer\ndecoder can model the context in both an autoregressive and a\nnon-autoregressive way. Different from the traditional method\nthat transfer knowledge into the NAR model from a pre-trained\nAR model [20], we train an AR model and a NAR model\nfrom scratch simultaneously. The NAR model can learn some\nlinguistic prior knowledge from the AR model depending on\nthe dual-mode transformer decoder. During the inference, our\nmodel can perform two-step decoding without depending on the\nexternal attention decoder. All the experiments are conducted\non a public Chinese mandarin dataset ASIEHLL-1. The results\nshow that the TSNAT can achieve a competitive performance\nwith the AR model and outperform other NAR models.\nThe remainder of this paper is organized as follows. Sec-\ntion 2 describes our proposed model. Section 3 presents our\nexperimental setup and results. The conclusions will be given\nin Section 4.\narXiv:2104.01522v1  [eess.AS]  4 Apr 2021\nRescoringPre-SelectionAcousticEncoderConv Front EndÔºàoptionalÔºâN-Best Hypothesises[MASK]  [MASK]  [MASK]  [MASK]  [MASK]\nBest Predicted Sequence\nAcoustic Feature SequenceParameters SharingAutoregressive DecoderNon-Autoregressive Decoder\n(a) The Structure of Two-Step Non-Autoregressive Transformer\n‚àô‚àô‚àô\n‚àô‚àô‚àô\n‚àô‚àô‚àô\n‚àô‚àô‚àô\n12 ‚àô‚àô‚àô T‚Äê1T\n<EOS>\nùëâ‡¨µ\nùëâ‡ØÑ‡¨ø‡¨µ\nùëâ‡ØÑ (b) The First-Step Inference Graph\nFigure 1: (a) illustrates the structure of our proposed two-step non-autoregressive transformer (TSNAT) and the inference process. The\nTSNAT model consists of an optional front-end block, an acoustic encoder, and a dual-mode transformer decoder. The inference can be\ndivided into two-step, pre-selection and rescoring. The dual-mode decoder Ô¨Årst generates n-best hypothesizes in a non-autoregressive\nand then rescores these hypotheses in an autoregressive way. (b) illustrates an output probability graph of the non-autoregressive\nmodel. The Ô¨Årst red circle of each column means the end-of-sentence token <EOS>. The other black circle represents different tokens\nof vocabulary. Each path that starts with the blank circle and ends with a red circle in the graph represents a possible hypothesis.\n2. Two-Step Non-Autoregressive\nTransformer\nOur proposed two-step non-autoregressive transformer, as\nshown in Fig.1(a), consists of three components, an optional\nconvolutional front end block, an acoustic encoder, and a dual-\nmode transformer decoder.\n2.1. The Convolutional Front End and Acoustic Encoder\nThe convolutional front end block generally consists of two 2D-\nConvolution layers and an output project layer [4]. The convo-\nlution layer is mainly responsible for the processing and down-\nsampling of the low-level acoustic features. The last output\nproject layer will project the processed acoustic feature into the\ndimension that the encoder required. The convolutional front\nend block also can be replaced with the linear project layers\nwith splicing frame operation [21].\nWe utilize the transformer encoder [22, 4] as the acoustic\nencoder, which contains a positional embedding, N repetitive\nmulti-head self-attention layers (MHA), and feed-forward net-\nwork layer (FFN). The sine-cosine positional embedding pro-\nposed by [22] is applied for all the experiments in this paper.\nBesides, the model also applies residual connection and layer\nnormalization.\nThe multi-head self-attention layers depend on all contexts\nto compute the attention weights from different perspectives,\nwhich makes it powerful to model long-range temporal infor-\nmation.\nSAi(Q,K,V ) =Softmax((Qt)K‚ä§/\n‚àö\ndk)V\nMHA(Q,K,V ) =Concat(SA0,SA1,...,SA H)\n(1)\nwhere Q, K and V indicate the query, key, and value matrix re-\nspectively. dk is the last dimension ofK. The multi-head atten-\ntion concatenates Houtput vectors of the self-attention mecha-\nnism. Besides, both the input of self-attention and the output of\nmulti-head attention should equip with an independent weight\nmatrix respectively. The feed-forward network (FFN) contains\ntwo linear layers and a gated liner unit (GLU) [23] activation\nfunction [7].\nFFN(x) =GLU(xW1 + b1)W2 + b2 (2)\nwhere parameters W‚àóand b‚àóare learnable weight matrix and\nbias respectively.\n2.2. The Dual-Mode Transformer Decoder\nThe major difference between the autoregressive model and the\nnon-autoregressive model focus on the structure of the decoder.\nThe AR models force themselves to learn the linguistic depen-\ndencies by blocking out the future information. This character-\nistic makes the AR model predict the output sequence step by\nstep. The conditional probability PAR(Y|X) can be expressed\nas:\nPAR(Y|X) =P(y1|X)\nL‚àè\ni=2\nP(yi|y<i,X) (3)\nwhere X is the acoustic encoded sequence with length T and\nyi denotes the i-th token of predicted sequence with length L.\nHowever, the non-autoregressive models get rid of the depen-\ndencies on the previously predicted tokens. Each step in the\ninference process is independent of each other, which makes\nthe inference step can be implemented in parallel and greatly\nimproves the reasoning speed of the model. The conditional\nprobability PNAR (Y|X) can be rewritten as:\nPNAR (Y|X) =\nL‚àè\ni=1\nP(yi|X) (4)\nBoth the AR decoder and the NAR decoder can apply the\ntransformer as the basic structure. There are two signiÔ¨Åcant dif-\nferences between these two kinds of decoders. On the one hand,\nthe AR transformer needs to apply the masking operation to the\nprevious output to model the linguistic dependencies, but the\nNAR transformer needs to model the bidirectional dependen-\ncies between the output tokens without any masking operations.\nOn the other hand, the AR decoder adopts all the tokens of vo-\ncabulary and some special tokens as the modeling units, while\nthe NAR decoder only requires one<MASK> token. Their mod-\neling units are not coincident.\nInspired by the dual-mode ASR [16], which models the\nstreaming ASR and non-streaming ASR by sharing an encoder,\nwe propose a novel method that uniÔ¨Åes the AR and NAR de-\ncoder into one dual-mode transformer decoder (DMTD). We\nassume that the AR decoder will accelerate the training and\nconvergence of the NAR decoder. The linguistic dependencies\nlearned by the AR decoder might be able to provide some prior\nknowledge for the NAR decoder, which will make the train-\ning of the NAR decoder more efÔ¨Åcient than guessing based on\nthe empty sequence. The self-attention of DMTD applies to\nthe masking-optional operation. During training, the encoder\nforwards once, and the decoder forwards twice, once in autore-\ngressive mode and once in non-autoregressive mode. During\nthe AR forward, the DMTD will adopt the truth token sequences\nthat begin with the begin-of-sentence token<BOS> as the input,\nand then mask future information of self-attention and calculate\nthe cross-entropy (CE) loss LAR. For the NAR forward, the\ndecoder will utilize a sequence Ô¨Ålled with <MASK> as the input\nand calculate the CE loss LNAR . The Ô¨Ånal loss is equal to the\nweighted sum of LAR and LNAR .\nL= (1‚àíŒ±)LNAR + Œ±LAR (5)\nwhere Œ±dictates the weight of AR loss. The model is optimized\nby these two losses jointly.\n2.3. The Two Step Inference\nMost of the non-autoregressive models just select the token\nwhich has the highest probability at each position and con-\ncatenates them from left to right as the Ô¨Ånal predicted result\n[10, 14, 15]. This inference method can be regarded as a greedy\nsearch. As shown in Fig.1(b), we consider the conditional prob-\nability matrix generated by the decoder as a graph, which con-\ntains numerous possible hypothesize. Each hypothesis starts\nwith any blank circle in the Ô¨Årst column and ends with a red\ncircle (end-of-sentence token <EOS>). We could select a bet-\nter hypothesis than the one predicted by greedy search by de-\npending on the external language model or the second attention\ndecoder as [17, 19, 18]. Now that the dual-mode transformer\ndecoder is able to be trained in the AR and NAR fashion simul-\ntaneously, the decoder can perform dual-mode inference natu-\nrally.\nThe inference process can be divided into two steps, pre-\nselection and rescoring. During the Ô¨Årst pre-selection, the dual-\nmode decoder will generate the conditional probability matrix\nfrom a full-mask sequence in a NAR way and then select the\nN-best hypothesizes. In order to avoid that the model tends\nto choose shorter sentences, we utilize the length-normalized\nscores. The selection of N-best hypothesizes from the proba-\nbility graph contains only simple addition operations, and does\nnot take much time and computing resources. During the sec-\nond step, the dual-mode decoder will insert begin-of-sentence\ntoken <BOS> into the head of the N-best hypothesizes. Then it\nutilizes the processed candidates as the input and predict them\nin an AR fashion. Due to the transformer can carry out efÔ¨Å-\ncient computing in parallel, the entire inference process can be\nÔ¨Ånished in two step.\n3. Experiments and Results\n3.1. Dataset\nIn this work, all experiments are conducted on a public Man-\ndarin speech corpus AISHELL-1 1.The training set contains\nabout 150 hours of speech (120,098 utterances) recorded by 340\nspeakers. The development set contains about 20 hours (14,326\nutterances) recorded by 40 speakers. And about 10 hours (7,176\nutterances / 36109 seconds) of speech is used as the test set. The\nspeakers of different sets are not overlapped.\n1https://openslr.org/33/\n3.2. Experimental Setup\nFor all experiments, we use 80-dimensional FBANK features\ncomputed on a 25ms window with a 10ms shift. We choose\n4234 characters (including a padding symbol <PAD>, an un-\nknown token <UNK>, a begin-of-sentence token <BOS>) and\nan end-of-sentence token <EOS> as modeling units.\nOur model consists of 12 encoder blocks and 6 decoder\nblocks. There are 4 heads in multi-head attention. The 2D con-\nvolution front end utilizes two-layer time-axis CNN with ReLU\nactivation, stride size 2, channels 384, and kernel size 3. Both\nthe output size of the multi-head attention and the feed-forward\nlayers are 384. The hidden size of the feed-forward layers is\n768. We adopt an Adam optimizer with warmup steps 12000\nand the learning rate scheduler reported in [22]. After 100\nepochs, we average the parameters saved in the last 20 epochs.\nWe also use the time mask and frequency mask method pro-\nposed in [24] instead of speed perturbation.\nWe use the character error rate (CER) to evaluate the perfor-\nmance of different models. For evaluating the inference speed\nof different models, we decode utterances one by one to com-\npute real-time factor (RTF) on the test set. The RTF is the time\ntaken to decode one second of speech. All experiments are con-\nducted on a GeForce GTX TITAN X 12G GPU.\n3.3. Results\n3.3.1. Comparison of The Model With Different AR Weights Œ±\nThis section compares the models with the different AR weights\nŒ±. When the weight Œ±is equal to 0, the NAR model is equiva-\nlent to the one that adopts an empty sequence as the input with-\nout any prior knowledge. If Œ±is equal to 1, the model will be\ncompletely transformed into an AR model. When the weight is\nbetween 0 and 1, the model can perform the two-step inference.\nFor all one-step inference in this section (marked asOneStep),\nthe NAR models ( Œ± ‚àà [0,1)) apply greedy search and the\nAR (Œ± = 1.0) models apply the beam search with width 10.\nThe two-step inference of this section (marked as TwoStep)\nrescores the 10-best candidate sequences. As shown in Table 1,\nit‚Äôs obvious that the hybrid AR and NAR training (Œ± ‚àà(0,1))\ncan improve the performance of the NAR model. We guess that\nthe NAR decoder can get some prior knowledge from the AR\ndecoder by sharing the parameters, which reduces the difÔ¨Åculty\nof training the NAR model from scratch. In the case of one-step\ninference, there is still a great performance gap between the AR\nmodel(Œ± = 1.0) and the NAR model ( Œ± ‚àà[0,1)). However,\nthe introduced two-step inference method can improve the per-\nformance of the NAR model greatly and achieve comparable\nresults with the AR model.\nTable 1: Comparison of the model with different AR weights Œ±\n(CER %).\nWeight Œ± 0.0 0.3 0.5 0.7 0.9 1.0\nDev OneStep 6.5 5.9 5.8 5.8 6.5 5.3\nTwoStep - 5.6 5.5 5.4 5.6 -\nTest OneStep 7.2 6.5 6.5 6.4 6.5 6.0\nTwoStep - 6.2 6.1 6.0 6.2 -\n3.3.2. The inÔ¨Çuence of the N-best Sequences on The Model\nPerformance\nThis section pays attention to the inference of the N-best se-\nquence on the NAR model performance. We select the different\nnumber of candidate sequences. Under the condition that N\nis equal to 1, the two-step inference makes no sense. There-\nfore, we replace it with the result of the greedy search. The\nresults in Table 2 indicate that the more candidate sequences,\nthe better performance it archives, which is consistent with the\nresults previously reported [19]. When the number of candidate\nsequences is greater than 10, the performance tends to be sta-\nble. However, with the increase of N, the model requires more\ncomputing resources, which leads to an increase in latency and\nreal-time-factor (RTF). Taken together, we will select the 10-\nbest candidates for the second-step rescoring by balancing the\nmodel performance and the inference speed.\nTable 2: Comparison of the inÔ¨Çuence of the N-best sequences\non the model performance (CER %).\nN 1 5 10 20 50\nDev 5.8 5.4 5.4 5.4 5.3\nTest 6.4 6.0 5.9 5.9 5.9\nRTF 0.0054 0.0167 0.0173 0.0181 0.0220\n3.3.3. Comparison with Other Non-Autoregressive Models\nTo further study the upper limit of model performance,\nwe readjust the parameter conÔ¨Åguration of the model and\nnamed three models of different sizes, named TSNAT-Small,\nTSNAT-Middle and TSNAT-Big. The three models have the\nsame depth (12 encoder blocks and 6 decoder blocks) and are\ntrained under the same conditions. Their differences focus on\nthe dimensions of the model (dmodel) and the hidden size of the\nfeed-forward network (dff ), as shown in Table 3.\nTable 3: The Parameter ConÔ¨Åguration of The Model.\nMode TSNAT-Small TSNAT-Middle TSNAT-Big\ndmodel 384 512 512\ndff 384 512 512\n#Params 34M 59M 87M\nTable 4: Comparison with other non-autoregressive models\n(CER %). All models in this table use SpecAugment to improve\nthe performance.\nModel LM Dev Test RTF\nA-FMLM(K=1) [10] w/o 6.2 6.7 -\nInsertion-NAT [25] w/o 6.1 6.7 -\nLASO-big [15] ‚ô¶ w/o 5.8 6.4 -\nCASS-NAT [26]‚ô¶ w 5.3 5.8 -\nCTC-enhanced NAR [13] ‚ô¶ w/o 5.3 5.9 -\nST-NAT [14] w/o 6.9 7.7 0.0056\nST-NAT [14] w 6.4 7.0 0.0292\nTSNAT-Small (34M) w/o 5.8 6.4 0.0054\n+ Two Step Inference w/o 5.4 5.9 0.0173\nTSNAT-Middle (59M) w/o 5.4 6.0 0.0063\n+ Two Step Inference w/o 5.2 5.7 0.0176\nTSNAT-Big (87M) w/o 5.3 6.0 0.0077\n+ Two Step Inference w/o 5.1 5.6 0.0185\n‚ô¶ These models additionally use speed-perturb to augment the\nspeech data.\nCompared with the other non-autoregressive models, we\nproposed TSNAT model can achieve quite competitive perfor-\nmance. To our best knowledge, the TSNAT-Big with two-step\ninference achieves the state-of-the-art (SOTA) performance\nwithout depending on any external language models. The re-\nsults also show that the two-step inference is faster than ST-NAT\nwith a neural language model because the two-step inference of\nthe dual-mode transformer decoder can be implemented in par-\nallel without iteration step by step.\n(a) Self-Attention of NAR Mode\n (b) Self-Attention of AR Mode\n(c) EncDec-Attention of NAR Mode\n(d) EncDec-Attention of AR Mode\nFigure 2: The Visualization of Attention Weights in The\nDual-Mode Decoder. We extract the self-attention weights\nand encoder-decoder attention weights from the last block\nof the dual-mode transformer decoder. The pictures come\nfrom the test set, and its corresponding sentence ID is\n‚ÄôBAC009S0764W0121‚Äô.\n3.3.4. The Analysis of Attention Weights in The Dual-Mode De-\ncoder\nWe want to Ô¨Ånd out the difference between the AR and NAR\nmodes of the dual-mode transformer decoder. Therefore, we\nextract the attention weights of these two modes from the last\ndecoder block. It‚Äôs clear that the self-attention mechanism of\nthe NAR model focuses on the whole input sequence, while\nthe self-attention mechanism of the AR mode pays attention to\nthe previous sequence. This is consistent with their own char-\nacteristics. Besides, the encoder-decoder attention weights of\nthe NAR mode are more dispersed than the one under the AR\nmode, which also shows the autoregressive model has stronger\nmodeling ability.\n4. Conclusions\nCompared with the autoregressive model for speech recogni-\ntion, the non-autoregressive model can get rid of the temporal\ndependency and predict the entire tokens in at least one step.\nHowever, the non-autoregressive model still faces two prob-\nlems. On the one hand, there is still a great performance gap\nbetween the advanced autoregressive (AR) model and the non-\nautoregressive (NAR) transformer model. On the other hand,\nit‚Äôs difÔ¨Åcult for most of the non-autoregressive models to train\nand converge. In this paper, we try to address these problems\nfrom two aspects. Firstly, we propose a parameters-sharing\ntraining method to improve the performance of the NAR model\nby learning some valuable knowledge from the AR model. Then\nwe make full use of the dual-mode decoder by introducing the\ntwo-step inference method. We conduct our experiments on a\npublic Chinese mandarin dataset ASIEHLL-1. The results show\nthat our proposed model can achieve comparable performance\nwith the AR model and outperform other typical NAR models.\n5. References\n[1] C. J. K, B. Dzmitry, S. Dmitriy, C. Kyunghyun, and B. Yoshua,\n‚ÄúAttention-based models for speech recognition,‚Äù in Advances in\nneural information processing systems, 2015, pp. 577‚Äì585.\n[2] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, ‚ÄúListen, attend\nand spell: A neural network for large vocabulary conversational\nspeech recognition,‚Äù in 2016 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2016,\npp. 4960‚Äì4964.\n[3] K. Suyoun, H. Takaaki, and W. Shinji, ‚ÄúJoint ctc-attention based\nend-to-end speech recognition using multi-task learning,‚Äù in2017\nIEEE international conference on acoustics, speech and signal\nprocessing (ICASSP). IEEE, 2017, pp. 4835‚Äì4839.\n[4] D. Linhao, X. Shuang, and X. Bo, ‚ÄúSpeech-transformer: a no-\nrecurrence sequence-to-sequence model for speech recognition,‚Äù\nin 2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2018, pp. 5884‚Äì5888.\n[5] A. Graves, ‚ÄúSequence transduction with recurrent neural net-\nworks,‚ÄùarXiv preprint arXiv:1211.3711, 2012.\n[6] Y . He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez,\nD. Zhao, D. Rybach, A. Kannan, Y . Wu, R. Panget al., ‚ÄúStream-\ning end-to-end speech recognition for mobile devices,‚Äù inICASSP\n2019-2019 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2019, pp. 6381‚Äì6385.\n[7] Z. Tian, J. Yi, J. Tao, Y . Bai, and Z. Wen, ‚ÄúSelf-Attention Trans-\nducers for End-to-End Speech Recognition,‚Äù in Proc. Interspeech\n2019, 2019, pp. 4395‚Äì4399.\n[8] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, ‚ÄúTransformer transducer: A streamable speech recog-\nnition model with transformer encoders and rnn-t loss,‚Äù inICASSP\n2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2020, pp. 7829‚Äì7833.\n[9] C.-F. Yeh, J. Mahadeokar, K. Kalgaonkar, Y . Wang, D. Le,\nM. Jain, K. Schubert, C. Fuegen, and M. L. Seltzer, ‚ÄúTransformer-\ntransducer: End-to-end speech recognition with self-attention,‚Äù\narXiv preprint arXiv:1910.12977, 2019.\n[10] N. Chen, S. Watanabe, J. Villalba, and N. Dehak, ‚ÄúNon-\nautoregressive transformer automatic speech recognition,‚Äù arXiv\npreprint arXiv:1911.04908, 2019.\n[11] Y . Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi,\n‚ÄúMask ctc: Non-autoregressive end-to-end asr with ctc and mask\npredict,‚ÄùarXiv preprint arXiv:2005.08700, 2020.\n[12] Y . Higuchi, H. Inaguma, S. Watanabe, T. Ogawa, and\nT. Kobayashi, ‚ÄúImproved mask-ctc for non-autoregressive end-to-\nend asr,‚ÄùarXiv preprint arXiv:2010.13270, 2020.\n[13] X. Song, Z. Wu, Y . Huang, C. Weng, D. Su, and H. Meng, ‚ÄúNon-\nautoregressive transformer asr with ctc-enhanced decoder input,‚Äù\narXiv preprint arXiv:2010.15025, 2020.\n[14] Z. Tian, J. Yi, J. Tao, Y . Bai, S. Zhang, and Z. Wen,\n‚ÄúSpike-Triggered Non-Autoregressive Transformer for End-to-\nEnd Speech Recognition,‚Äù in Proc. Interspeech 2020 , 2020,\npp. 5026‚Äì5030. [Online]. Available: http://dx.doi.org/10.21437/\nInterspeech.2020-2086\n[15] Y . Bai, J. Yi, J. Tao, Z. Tian, Z. Wen, and S. Zhang, ‚ÄúListen At-\ntentively, and Spell Once: Whole Sentence Generation via a Non-\nAutoregressive Architecture for Low-Latency Speech Recogni-\ntion,‚Äù in Proc. Interspeech 2020, 2020, pp. 3381‚Äì3385. [Online].\nAvailable: http://dx.doi.org/10.21437/Interspeech.2020-1600\n[16] J. Yu, W. Han, A. Gulati, C.-C. Chiu, B. Li, T. N. Sainath, Y . Wu,\nand R. Pang, ‚ÄúDual-mode asr: Unify and improve streaming asr\nwith full-context modeling,‚ÄùProceedings of ICLR, 2021.\n[17] T. N. Sainath, R. Pang, D. Rybach, Y . He, R. Prabhavalkar, W. Li,\nM. Visontai, Q. Liang, T. Strohman, Y . Wuet al., ‚ÄúTwo-pass end-\nto-end speech recognition,‚Äù arXiv preprint arXiv:1908.10992 ,\n2019.\n[18] K. Hu, T. N. Sainath, R. Pang, and R. Prabhavalkar, ‚ÄúDeliberation\nmodel based two-pass end-to-end speech recognition,‚Äù inICASSP\n2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2020, pp. 7799‚Äì7803.\n[19] Z. Tian, J. Yi, Y . Bai, J. Tao, S. Zhang, and Z. Wen, ‚ÄúOne\nin a hundred: Select the best predicted sequence from numer-\nous candidates for streaming speech recognition,‚Äù arXiv preprint\narXiv:2010.14791, 2020.\n[20] Y . Ren, Y . Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y .\nLiu, ‚ÄúFastspeech: Fast, robust and controllable text to speech,‚Äù\nin Advances in Neural Information Processing Systems, 2019, pp.\n3171‚Äì3180.\n[21] Y . zhao, J. Li, X. Wang, and Y . Li, ‚ÄúThe speechtransformer\nfor large-scale mandarin chinese speech recognition,‚Äù in ICASSP\n2019 - 2019 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2019, pp. 7095‚Äì7099.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\nin Advances in neural information processing systems , 2017, pp.\n5998‚Äì6008.\n[23] Y . N. Dauphin, A. Fan, M. Auli, and D. Grangier, ‚ÄúLanguage\nmodeling with gated convolutional networks,‚Äù in Proceedings of\nthe 34th International Conference on Machine Learning-Volume\n70. JMLR. org, 2017, pp. 933‚Äì941.\n[24] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V . Le, ‚ÄúSpecaugment: A simple data augmen-\ntation method for automatic speech recognition,‚Äù arXiv preprint\narXiv:1904.08779, 2019.\n[25] Y . Fujita, S. Watanabe, M. Omachi, and X. Chan, ‚ÄúInsertion-based\nmodeling for end-to-end automatic speech recognition,‚Äù arXiv\npreprint arXiv:2005.13211, 2020.\n[26] R. Fan, W. Chu, P. Chang, and J. Xiao, ‚ÄúCass-nat: Ctc\nalignment-based single step non-autoregressive transformer for\nspeech recognition,‚ÄùarXiv preprint arXiv:2010.14725, 2020.",
  "topic": "Autoregressive model",
  "concepts": [
    {
      "name": "Autoregressive model",
      "score": 0.9253115057945251
    },
    {
      "name": "Computer science",
      "score": 0.7733163833618164
    },
    {
      "name": "Inference",
      "score": 0.6134964227676392
    },
    {
      "name": "Speech recognition",
      "score": 0.59070885181427
    },
    {
      "name": "Transformer",
      "score": 0.5875474810600281
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4518943130970001
    },
    {
      "name": "Hidden Markov model",
      "score": 0.45076972246170044
    },
    {
      "name": "Machine learning",
      "score": 0.35524696111679077
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3549196124076843
    },
    {
      "name": "Mathematics",
      "score": 0.1159837543964386
    },
    {
      "name": "Econometrics",
      "score": 0.09700170159339905
    },
    {
      "name": "Engineering",
      "score": 0.08051064610481262
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210112150",
      "name": "Institute of Automation",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210097554",
      "name": "Center for Excellence in Brain Science and Intelligence Technology",
      "country": "CN"
    }
  ]
}