{
  "title": "Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting",
  "url": "https://openalex.org/W3104215796",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2104698494",
      "name": "Sanyuan Chen",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2173671235",
      "name": "Yu-Tai Hou",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2103132936",
      "name": "Yiming Cui",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096786427",
      "name": "Wanxiang Che",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098738246",
      "name": "Ting Liu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2132627111",
      "name": "Xiangzhan Yu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2554863749",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W3003289092",
    "https://openalex.org/W2473930607",
    "https://openalex.org/W2964352358",
    "https://openalex.org/W2474280151",
    "https://openalex.org/W2974317861",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2975185270",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2979736514",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W3106003309",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2737492962",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2924984511",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2962863357",
    "https://openalex.org/W2949995560",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2994415862",
    "https://openalex.org/W4301163820",
    "https://openalex.org/W4319988532",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W1607198972",
    "https://openalex.org/W2774373350",
    "https://openalex.org/W2806984819",
    "https://openalex.org/W2963850662",
    "https://openalex.org/W2963072899",
    "https://openalex.org/W3091322135",
    "https://openalex.org/W4295883599",
    "https://openalex.org/W2963813679",
    "https://openalex.org/W2971339032",
    "https://openalex.org/W2060277733",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3098511564",
    "https://openalex.org/W3034709122",
    "https://openalex.org/W1581755290",
    "https://openalex.org/W2804175194",
    "https://openalex.org/W2964067969",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W2945383715",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2962982474",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3103800629",
    "https://openalex.org/W2964186069",
    "https://openalex.org/W2962707369",
    "https://openalex.org/W2963588172",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W3013325675",
    "https://openalex.org/W2939911019",
    "https://openalex.org/W2963788399",
    "https://openalex.org/W2113839990",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2927746189",
    "https://openalex.org/W3127504686",
    "https://openalex.org/W2995998574",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2964189064",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3086499488",
    "https://openalex.org/W2963559848",
    "https://openalex.org/W3140968660",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3120490999",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2998496267"
  ],
  "abstract": "Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we introduce a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better average performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7870–7881,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n7870\nRecall and Learn: Fine-tuning Deep Pretrained Language Models with\nLess Forgetting\nSanyuan Chen1, Yutai Hou1, Yiming Cui1,2, Wanxiang Che1, Ting Liu1, Xiangzhan Yu1\n1School of Computer Science and Technology, Harbin Institute of Technology, China\n2Joint Laboratory of HIT and iFLYTEK Research (HFL), Beijing, China\n{sychen, ythou, ymcui, car, tliu}@ir.hit.edu.cn, yxz@hit.edu.cn\nAbstract\nDeep pretrained language models have\nachieved great success in the way of pretrain-\ning ﬁrst and then ﬁne-tuning. But such a\nsequential transfer learning paradigm often\nconfronts the catastrophic forgetting problem\nand leads to sub-optimal performance. To\nﬁne-tune with less forgetting, we propose a\nrecall and learn mechanism, which adopts\nthe idea of multi-task learning and jointly\nlearns pretraining tasks and downstream tasks.\nSpeciﬁcally, we introduce a Pretraining Sim-\nulation mechanism to recall the knowledge\nfrom pretraining tasks without data, and\nan Objective Shifting mechanism to focus\nthe learning on downstream tasks gradually.\nExperiments show that our method achieves\nstate-of-the-art performance on the GLUE\nbenchmark. Our method also enables BERT-\nbase to achieve better average performance\nthan directly ﬁne-tuning of BERT-large. Fur-\nther, we provide the open-source R ECADAM\noptimizer, which integrates the proposed\nmechanisms into Adam optimizer, to facility\nthe NLP community.1\n1 Introduction\nDeep Pretrained Language Models (LMs), such\nas ELMo (Peters et al., 2018) and BERT (Devlin\net al., 2019), have signiﬁcantly altered the land-\nscape of Natural Language Processing (NLP), and\na wide range of NLP tasks has been promoted by\nthese pretrained language models. These successes\nare mainly achieved through Sequential Transfer\nLearning (Ruder, 2019): pretrain a language model\non large-scale unlabeled data and then adapt it to\ndownstream tasks. The adaptation step is usually\nconducted in two manners: ﬁne-tuning or freez-\ning pretrained weights. In practice, ﬁne-tuning is\nadopted more widely due to its ﬂexibility (Phang\net al., 2018; Peters et al., 2019; Lan et al., 2020).\n1https://github.com/Sanyuan-Chen/RecAdam\nDespite the great success, sequential transfer\nlearning of deep pretrained LMs is prone to catas-\ntrophic forgetting during the adaptation step.Catas-\ntrophic forgetting is a common problem for se-\nquential transfer learning, and it happens when a\nmodel forgets previously learned knowledge and\noverﬁts to target domains (McCloskey and Co-\nhen, 1989; Kirkpatrick et al., 2017). To remedy\nthe catastrophic forgetting in transferring deep pre-\ntrained LMs, existing efforts mainly explore ﬁne-\ntuning tricks to forget less. ULMFiT (Howard and\nRuder, 2018) introduced discriminative ﬁne-tuning,\nslanted triangular learning rates, and gradual un-\nfreezing for LMs ﬁne-tuning. Lee et al. (2020)\nreduced forgetting in BERT ﬁne-tuning by ran-\ndomly mixing pretrained parameters to a down-\nstream model in a dropout-style.\nInstead of learning pretraining tasks and down-\nstream tasks in sequence, Multi-task Learning\nlearns both of them simultaneously, thus can in-\nherently avoid the catastrophic forgetting prob-\nlem. Xue et al. (2019) tackled forgetting in au-\ntomatic speech recognition by jointly training the\nmodel with previous and target tasks. Kirkpatrick\net al. (2017) proposed Elastic Weight Consolidation\n(EWC) to overcome catastrophic forgetting when\ncontinuous learning multiple tasks by adopting the\nmulti-task learning paradigm. EWC regularizes\nnew task training by constraining the parameters\nwhich are important for previous tasks and adapt\nmore aggressively on other parameters. Thanks\nto the appealing effects on catastrophic forgetting,\nEWC has been widely applied in various domains,\nsuch as game playing (Ribeiro et al., 2019), neural\nmachine translation (Thompson et al., 2019) and\nreading comprehension (Xu et al., 2019).\nHowever, these multi-task learning methods can-\nnot be directly applied to the sequential transferring\nregime of deep pretrained LMs. Firstly, multi-task\nlearning methods require to use the data of pre-\n7871\ntraining tasks during adaptation, but the pretraining\ndata of LMs is often inaccessible or too large for\nthe adaptation. Secondly, we only care about the\ndownstream task’s performance, while multi-task\nlearning also aims to promote performance on pre-\ntraining tasks.\nIn this paper, we propose a recall and learn\nmechanism to cope with the forgetting problem\nof ﬁne-tuning the deep pretrained LMs. To achieve\nthis, we take advantage of multi-task learning by\nadopting LMs pretraining as an auxiliary learning\ntask during ﬁne-tuning. Speciﬁcally, we introduce\ntwo mechanisms for the two challenges mentioned\nabove, respectively. As for the challenge of data\nobstacles, we introduce the Pretraining Simulation\nto achieve multi-task learning without accessing\nto pretraining data. It helps the model recall previ-\nously learned knowledge by simulating the pretrain-\ning objective using only the pretrained parameters.\nAs for the challenge of learning objective differ-\nence, we introduce the Objective Shifting to bal-\nance new task learning and pretrained knowledge\nrecalling. It allows the model to focus gradually\non the new task by shifting the multi-task learning\nobjective to the new task learning.\nWe also provide Recall Adam (RECADAM) opti-\nmizer to integrate the proposed recall and learn\nmechanism into Adam optimizer (Kingma and\nBa, 2015). We release the source code of the\nRECADAM optimizer implemented in PyTorch\n(Paszke et al., 2019). It is easy to use and can\nfacilitate the NLP community for better ﬁne-tuning\nof deep pretrained LMs. Experiments on the GLUE\nbenchmark with the BERT-base model show that\nthe proposed method can signiﬁcantly outperform\nthe vanilla ﬁne-tuning method. Our method with\nthe BERT-base model can even achieve better av-\nerage results than directly ﬁne-tuning the BERT-\nlarge model. In addition, thanks to the effectiveness\nof pretrained knowledge recalling, we can initial-\nize the model with random parameters and gain\nbetter performance with larger parameter search\nspace than the pretrained initialization. Finally, we\nachieve state-of-the-art performance on the GLUE\nbenchmark with the ALBERT-xxlarge model.\nOur contributions can be summarized as follows:\n(1) We propose to tackle the catastrophic forgetting\nproblem of ﬁne-tuning the deep pretrained LMs\nby adopting the idea of multi-task learning and\nobtain state-of-the-art results on the GLUE bench-\nmark. (2) We propose a recall and learn mechanism\nwith Pretraining Simulation and Objective Shifting\nto achieve multi-task ﬁne-tuning without data of\npretraining tasks. (3) We provide the open-source\nRECADAM optimizer to facilitate deep pretrained\nLMs ﬁne-tuning with less forgetting.\n2 Background\nIn this section, we present two transfer learning\nsettings: sequential transfer learning and multi-task\nlearning. They both aim to improve the learning\nperformance by transferring knowledge across mul-\ntiple tasks, but apply to different scenarios.\n2.1 Sequential Transfer Learning\nSequential transfer learninglearns source tasks and\ntarget tasks in sequence, and transfers knowledge\nfrom source tasks to improve the models’ perfor-\nmance on target tasks.\nIt typically consists of two stages: pretraining\nand adaptation. During pretraining, the model\nis trained on source tasks with the loss function\nLossS. During adaptation, the pretrained model is\nfurther trained on target tasks with the loss func-\ntion LossT. The standard adaptation methods in-\ncludes ﬁne-tuning and feature extraction. Fine-\ntuning updates all the parameters of the pretrained\nmodel, while feature extraction regards the pre-\ntrained model as a feature extractor and keeps it\nﬁxed during the adaptation phase.\nSequential transfer learning has been widely\nused recently, and the released deep pretrained LMs\nhave achieved great successes on various NLP tasks\n(Peters et al., 2018; Devlin et al., 2019; Lan et al.,\n2020). While the adaptation of the deep pretrained\nLMs is very efﬁcient, it is prone tocatastrophic for-\ngetting, where the model forgets previously learned\nknowledge from source tasks when learning new\nknowledge from target tasks.\n2.2 Multi-task Learning\nMulti-task Learning learns multiple tasks simulta-\nneously, and improves the models’ performance\non all of them by sharing knowledge across these\ntasks (Caruana, 1997; Ruder, 2017).\nUnder the multi-task learning paradigm, the\nmodel is trained on both source tasks and target\ntasks with the loss function:\nLossM = λLossT + (1−λ)LossS (1)\nwhere λ ∈(0,1) is a hyperparameter balancing\nthese two tasks. It can inherently avoid catastrophic\n7872\nforgetting because the loss on source tasks LossS\nis always part of the optimization objective.\nTo overcome catastrophic forgetting (discussed\nin §2.1), can we apply the idea of multi-task learn-\ning to the adaptation of the deep pretrained LMs?\nThere are two challenges in practice:\n1) We cannot get access to the pretraining data to\ncalculate LossS during adaptation.\n2) The optimization objective of adaptation is\nLossT, while multi-task learning aims to op-\ntimize LossM, i.e., the weighted sum of LossT\nand LossS.\n3 Methodology\nIn this section, we introduce Pretraining Simulation\n(§3.1) and Objective Shifting (§3.2) to overcome\nthe two challenges (discussed in §2.2) respectively.\nPretraining Simulation allows the model to learn\nsource tasks without pretraining data, and Objec-\ntive Shifting allows the model to focus on target\ntasks gradually. We also introduce the RECADAM\noptimizer (§3.3) to integrate these two mechanisms\ninto the common-used Adam optimizer.\n3.1 Pretraining Simulation\nAs for the ﬁrst challenge that pretraining data is\nunavailable, we introduce Pretraining Simulation\nto approximate the optimization objective of source\ntasks as a quadratic penalty, which keeps the model\nparameters close to the pretrained parameters.\nFollowing Elastic Weight Consolidation (EWC;\nKirkpatrick et al. 2017; Husz´ar 2017), we approx-\nimate the optimization objective of source tasks\nwith Laplace’s Method and assumption of indepen-\ndence among the model parameters. Since EWC\nrequires pretraining data, we further introduce a\nstronger independence assumption and derive a\nquadratic penalty, which is independent of the pre-\ntraining data. We introduce the detailed derivation\nprocess as follows.\nFrom the probabilistic perspective, the learning\nobjective on the source tasks LossS would be opti-\nmizing the negative log posterior probability of the\nmodel parameters θgiven data of source tasks DS:\nLossS = −log p(θ|DS)\nThe pretrained parameters θ∗can be assumed\nas a local minimum of the parameter space, and it\nsatisﬁes the equation:\nθ∗= arg minθ{−logp(θ|DS)}\nDue to the intractability, the optimization objec-\ntive −log p(θ|DS) is locally approximated with\nthe Laplace’s Method (MacKay, 2003):\n−logp(θ|DS) ≈−logp(θ∗|DS)\n+ 1\n2(θ−θ∗)⊤H(θ∗)(θ−θ∗)\nwhere H(θ∗) is the Hessian matrix of the opti-\nmization objective w.r.t. θ and evaluated at θ∗.\n−log p(θ∗|DS) is a constant term w.r.t. θ, and\nit can be ignored during optimization.\nSince the pretrained model convergences on the\nsource tasks, H(θ∗) can be approximated with\nthe empirical Fisher information matrix F(θ∗)\n(Martens, 2014):\nF(θ∗) =Ex∼DS[∇θlogpθ(x)∇θlogpθ(x)⊤|θ=θ∗]\nH(θ∗) ≈NF(θ∗) +Hprior(θ∗)\nwhere N is the number of i. i. d. observations in\nDS, Hprior(θ∗) is the Hessian matrix of the nega-\ntive log prior probability −log p(θ).\nBecause of the computational intractability,\nEWC approximate H(θ∗) by using the diagonal\nof F(θ∗) and ignoring the prior Hessian matrix\nHprior(θ∗):\n(θ−θ∗)⊤H(θ∗)(θ−θ∗) ≈N∑\niFi(θi−θ∗i)2\nwhere Fi is the corresponding diagonal Fisher in-\nformation value of the model parameter θi.\nSince the pretraining data is unavailable, we fur-\nther approximate H(θ∗) with a stronger assump-\ntion that each diagonal Fisher information value Fi\nis independent of the corresponding parameter θi:\n(θ−θ∗)⊤H(θ∗)(θ−θ∗) ≈NF∑\ni(θi−θ∗i)2\nThe ﬁnal approximated optimization objective\nof the source tasks is the quadratic penalty between\nthe model parameters and the pretrained parame-\nters:\nLossS =−logp(θ|DS)\n≈1\n2(θ−θ∗)⊤H(θ∗)(θ−θ∗)\n≈1\n2(θ−θ∗)⊤(NF(θ∗) +Hprior(θ∗))(θ−θ∗)\n≈1\n2N\n∑\ni\nFi(θi−θ∗i)2\n≈1\n2NF\n∑\ni\n(θi−θ∗i)2\n= 1\n2γ\n∑\ni\n(θi−θ∗i)2\nwhere 1\n2 γis the coefﬁcient of the quadratic penalty.\n7873\nt0\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000018\n/uni00000014/uni00000011/uni00000013(t)\n0 < k <\nk/uni00000003/uni00000003\nk/uni00000003/uni000000030\nFigure 1: Objective Shifting: we replace the coefﬁcient\nλ with the annealing function λ(t). Fine-tuning and\nmulti-task learning can be regarded as the special cases\n(k→∞ and k→0) of our method.\n3.2 Objective Shifting\nAs for the second challenge that the optimization\nobjective of multi-task learning is inconsistent with\nadaptation, we introduce Objective Shifting to al-\nlow the objective function to gradually shift to\nLossT with the annealing coefﬁcient.\nWe replace the coefﬁcient λin the optimization\nobjective of multi-task learning (as shown in Eq. 1)\nwith the annealing function λ(t), where trefers to\nthe update timesteps during ﬁne-tuning. The loss\nfunction of our method is set to multi-task learning\nwith annealing coefﬁcient:\nLoss = λ(t)LossT + (1−λ(t))LossS\nSpeciﬁcally, to better balance the multi-task\nlearning and ﬁne-tuning, λ(t) is calculated as the\nsigmoid annealing function (Bowman et al., 2016;\nKiperwasser and Ballesteros, 2018):\nλ(t) = 1\n1 + exp(−k·(t−t0))\nwhere kand t0 are the hyperparameters controlling\nthe annealing rate and timesteps.\nAs shown in Figure 1, at the beginning of the\ntraining process, the model mainly learns gen-\neral knowledge by focusing more on pretraining\ntasks. As training progress, the model gradually fo-\ncuses on target tasks and learns more target-speciﬁc\nknowledge while recalling the knowledge of pre-\ntraining tasks. At the end of the training process,\nthe model completely focuses on target tasks, and\nthe ﬁnal optimization objective is LossT.\nFine-tuning and multi-task learning can be re-\ngarded as special cases of our method. When\nk→∞, our method can be regarded as ﬁne-tuning.\nThe model ﬁrstly gets pretrained on source tasks\nwith the LossS, then learns the target tasks with\nthe LossT. When k →0, λ(t) is a constant func-\ntion, then our method can be regarded as the multi-\ntask learning. The model learns source tasks and\ntarget tasks simultaneously with the loss function\n1\n2 (LossT + LossS).\n3.3 RecAdam Optimizer\nAdam optimizer (Kingma and Ba, 2015) is com-\nmonly used for ﬁne-tuning the deep pretrained\nLMs. We introduce Recall Adam ( RECADAM)\noptimizer to integrate the quadratic penalty and the\nannealing coefﬁcient, which are the core factors of\nthe Pretraining Simulation ( §3.1) and Objective\nShifting (§3.2) mechanisms respectively, by de-\ncoupling them from the gradient updates in Adam\noptimizer.\nLoshchilov and Hutter (2019) observed that L2\nregularization and weight decay are not identical\nfor adaptive gradient algorithms such as Adam, and\nconﬁrmed the proposed AdamW optimizer based\non decoupled weight decay could substantially im-\nprove Adam’s performance in both theoretical and\nempirical way.\nSimilarly, it is necessary to decouple the\nquadratic penalty and the annealing coefﬁcient\nwhen ﬁne-tuning the pretrained LMs with Adam\noptimizer. Otherwise, both the quadratic penalty\nand annealing coefﬁcient would be adapted by the\ngradient update rules, resulting in different magni-\ntudes of the quadratic penalty among the model’s\nweights.\nThe comparison between Adam and\nRECADAM are shown in Algorithm 1, where\nSetScheduleMultiplier(t) (Line 11) refers to the\nprocedure (e.g. warm-up technique) to get the\nscaling factor of the step size.\nLine 6 of Algorithm 1 shows how we implement\nthe quadratic penalty and annealing coefﬁcient with\nthe vanilla Adam optimizer. The weighted sum\nof the gradient of target task objective function\n∇f(θ) and the gradient of the quadratic penalty\nγ(θ−θ∗) get adapted by the gradient update rules,\nwhich derives to inequivalent magnitudes of the\nquadratic penalty among the model’s weights, e.g.\nthe weights that tend to have larger gradients∇f(θ)\nwould have the larger second moment v and be\n7874\nAlgorithm 1 Adam and RecAdam\n1: given initial learning rate α ∈R, momentum factors β1 = 0.9,β2 = 0.999,ϵ = 10−8, pretrained parameter vector\nθ∗ ∈Rn, coefﬁcient of quadratic penalty γ ∈R, annealing coefﬁcient in objective function λ(t) = 1/(1 + exp(−k·(t−\nt0)),k ∈R,t0 ∈N\n2: initialize timestep t←0, parameter vector θt=0 ∈Rn, ﬁrst moment vector mt=0 ←0, second moment vector vt=0 ←0,\nschedule multiplier ηt=0 ∈R\n3: repeat\n4: t←t+ 1\n5: ∇ft(θt−1) ←SelectBatch(θt−1) ⊿ select batch and return the corresponding gradient\n6: gt ← λ(t) ∇ft(θt−1) +(1 −λ(t))γ(θt−1 −θ∗)\n7: mt ←β1mt−1 + (1−β1)gt ⊿ here and below all operations are element-wise\n8: vt ←β2vt−1 + (1−β2)g2\nt\n9: ˆmt ←mt/(1 −βt\n1) ⊿ β1 is taken to the power of t\n10: ˆvt ←vt/(1 −βt\n2) ⊿ β2 is taken to the power of t\n11: ηt ←SetScheduleMultiplier(t) ⊿ can be ﬁxed, decay, or also be used for warm restarts\n12: θt ←θt−1 −ηt\n(\nλ(t) αˆmt/(√ˆvt + ϵ) +(1 −λ(t))γ(θt−1 −θ∗)\n)\n13: until stopping criterion is met\n14: return optimized parameters θt\npenalized by the relatively smaller amount than\nother weights.\nWith RECADAM optimizer, we decouple the gra-\ndient of the quadratic penalty γ(θ−θ∗) and the an-\nnealing coefﬁcient λ(t) in Line 12 of Algorithm 1.\nIn this way, only the gradient of target task objec-\ntive function ∇f(θ) get adapted during the opti-\nmization steps, and all the weights of the training\nmodel would be more effectively penalized with\nthe same rate (1 −λ(t))γ.\nSince the RECADAM optimizer is only one line\nmodiﬁcation from Adam optimizer, it can be eas-\nily used by feeding the additional parameters, in-\ncluding the pretrained parameters and a few hy-\nperparameters of the Pretraining Simulation and\nObjective Shifting mechanisms.\n4 Experiments\n4.1 Setup\nModel: We conduct the experiments with the\ndeep pretrained language model BERT-base (De-\nvlin et al., 2019) and ALBERT-xxlarge (Lan et al.,\n2020).\nBERT is a deep bi-directional pretrained model\nbased on multi-layer Transformer encoders. It is\npretrained on the large-scale corpus with two unsu-\npervised tasks: Masked LM and Next Sentence Pre-\ndiction, and has achieved signiﬁcant improvements\non a wide range of NLP tasks. We use the BERT-\nbase model with 12 layers, 12 attention heads and\n768 hidden dimensions (total 108M parameters).\nALBERT is the latest deep pretrained LM that\nachieves state-of-the-art performance on several\nbenchmarks. It improves BERT by the parameter\nreduction techniques and self-supervised loss for\nsentence-order prediction (SOP). The ALBERT-\nxxlarge model with 12 layers, 64 attention heads,\n128 embedding dimensions and 4,096 hidden di-\nmensions (total 235M parameters) is the current\nstate-of-the-art model released by Lan et al. (2020).\nData: We evaluate our methods on the Gen-\neral Language Understanding Evaluation (GLUE)\nbenchmark (Wang et al., 2019).\nGLUE is a well-known benchmark evaluating\nmodel capabilities for natural language understand-\ning. It includes 9 tasks: Corpus of Linguistic\nAcceptability (CoLA; Warstadt et al. 2018), Stan-\nford Sentiment Treebank (SST; Socher et al. 2013),\nMicrosoft Research Paraphrase Corpus (MRPC;\nDolan and Brockett 2005), Semantic Textual Sim-\nilarity Benchmark (STS; Cer et al. 2017), Quora\nQuestion Pairs (QQP),2 Multi-Genre NLI (MNLI;\nWilliams et al. 2018), Question NLI (QNLI; Ra-\njpurkar et al. 2016), Recognizing Textual Entail-\nment (RTE; Dagan et al. 2006; Bar Haim et al.\n2006; Giampiccolo et al. 2007; Bentivogli et al.\n2009) and Winograd NLI (WNLI; Levesque et al.\n2011).\nFollowing previous works (Yang et al., 2019;\nLiu et al., 2019; Lan et al., 2020), we report our\nsingle-task single-model results on the dev set of\n8 GLUE tasks, excluding the problematic WNLI\ndataset.3 We report Pearson correlations for STS,\nMatthew’s correlations for CoLA, the “match” con-\ndition (MNLI-m) for MNLI, and accuracy scores\n2https://www.quora.com/q/quoradata/First-Quora-\nDataset-Release-Question-Pairs\n3https://gluebenchmark.com/faq\n7875\nfor other tasks.\nImplementation: As discussed in §3.3, we im-\nplement the Pretraining Simulation and Objective\nShifting techniques with the proposed RECADAM\noptimizer. We ﬁne-tune the additional output layer\nwith the vanilla Adam optimizer, since it is ex-\ncluded in the parameters of pretrained LMs. Our\nmethods use random initialization because of the\npretrained knowledge recalling implementation,\nwhile vanilla ﬁne-tuning initializes the ﬁne-tuning\nmodel with the pretrained parameters.\nWe use the data processing and evaluation script\nimplemented by HuggingFace Transformers li-\nbrary.4 We ﬁne-tune BERT-base and ALBERT-\nxxlarge model with the same hyperparameters fol-\nlowing Devlin et al. (2019) and Lan et al. (2020),\nexcept for the maximum sequence length which\nwe set to 128 rather than 512. For the BERT-\nbase model, we set the learning rate to 2e-5, use\nthe gradient bias correction and select the training\nstep (61,360 on MNLI, 56,855 on QQP, 33,890 on\nQNLI, 21,050 on SST, 13,400 on CoLA, 9,000 on\nSTS, 11,500 on MRPC, 7,800 on RTE) to improve\nthe ﬁne-tuning stability on each task (Mosbach\net al., 2020; Zhang et al., 2020b). We note that we\nﬁne-tune on RTE, STS, and MRPC directly using\nthe pretrained LM while the previous works are\nusing an MNLI checkpoint for further performance\nimprovement. As for the hyperparameters of our\nmethods, we set γin the quadratic penalty to 5,000,\nand select the best t0 and k in {100, 250, 500,\n1,000}and {0.05, 0.1, 0.2, 0.5, 1}respectively for\nthe annealing coefﬁcient λ(t) on each dev set. Fol-\nlowing previous works (Liu et al., 2019; Lan et al.,\n2020), we report the score of 5 differently-seeded\nruns for each result.\n4.2 Results on GLUE\nTable 1 shows the single-task single-model results\nof our RECADAM ﬁne-tuning method comparing\nto the vanilla ﬁne-tuning method with BERT-base\nand ALBERT-xxlarge model on the dev set of the\nGLUE benchmark. We also present the single-task\nsingle-model results with the BERT-base model\non the test set of the GLUE benchmark in Ap-\npendix A.1, where we achieve 1.0% improvement\non average.\nResults with BERT-base: With the BERT-base\nmodel, we outperform the vanilla ﬁne-tuning\n4https://github.com/huggingface/transformers\nmethod on 7 out of 8 tasks of the GLUE benchmark\nand achieve 1.0% improvement on the average me-\ndian performance.\nEspecially for the tasks with smaller training\ndata (<10k), our method can achieve signiﬁcant\nimprovements (+1.7% on average) compared to the\nvanilla ﬁne-tuning method. Because of the data\nscarcity, vanilla ﬁne-tuning on these tasks is po-\ntentially brittle and prone to overﬁtting and catas-\ntrophic forgetting problems (Phang et al., 2018;\nJiang et al., 2019). With the proposed RECADAM\nmethod, we successfully achieve better ﬁne-tuning\nby learning target tasks while recalling the knowl-\nedge of pretraining tasks.\nIt is interesting to ﬁnd that compared to the me-\ndian results with the BERT-large model, we can\nalso achieve better results on more than half of the\ntasks (e.g., +4.0% on RTE, +0.4% on STS, +1.8%\non CoLA, +0.4% on SST, +0.1% on QQP) and\nbetter average results (+0.5%) of all the GLUE\ntasks. Thanks to the less catastrophic forgetting\nrealized by RECADAM, we can get better overall\nperformance with much fewer parameters of the\npretrained model.\nResults with ALBERT-xxlarge: With the state-\nof-the-art model ALBERT-xxlarge, we outperform\nthe vanilla ﬁne-tuning method on 5 out of 8 tasks of\nthe GLUE benchmark and achieve the state-of-the-\nart single-task single-model average median result\n90.2% on the dev set of the GLUE benchmark.\nSimilar to the results with the BERT-base model,\nWe ﬁnd that our improvements mostly come from\nthe tasks with smaller training data (<10k), and we\ncan improve the ALBERT-xxlarge model’s median\nperformance on these tasks by +1.5% on average.\nAlso, compared to the reported results by Lan et al.\n(2020), we can achieve similar or better median\nresults on RTE (+0.1%), STS (-0.1%), and MRPC\n(+1.0%) tasks without pretraining on the MNLI\ntask.\nOverall, we outperform the average median re-\nsults of the baseline with the ALBERT-xxlarge\nmodel by 0.7%, which is lower than the im-\nprovement we gain with the BERT-base model\n(+1.0%). With advanced model design and pretrain-\ning techniques, ALBERT-xxlarge achieves signiﬁ-\ncantly better performance on the GLUE benchmark,\nwhich would be harder to be further improved.\n7876\nModel MNLI QQP QNLI SST Avg CoLA STS MRPC RTE Avg Avg392k 363k 108k 67k >10k 8.5k 5.7k 3.5k 2.5k <10k\nBERT-base (Devlin et al., 2019) 84.4 - 88.4 92.7 - - - 86.7 - - -\nBERT-base (rerun)Median 84.8 91.4 91.6 93.0 90.2 60.6 89.8 86.5 71.1 77.0 83.6\nBERT-base + RecAdamMedian 85.0 91.4 91.9 93.6 90.5 62.4 90.4 87.7 74.4 78.7 84.6\nBERT-base (rerun)Max 84.9 91.4 92.0 93.3 90.4 61.6 89.9 88.7 71.5 77.9 84.2\nBERT-base + RecAdamMax 85.3 91.6 92.1 94.0 90.8 62.6 90.6 88.7 77.3 79.8 85.3\nBERT-large (Devlin et al., 2019) 86.6 91.3 92.3 93.2 90.9 60.6 90.0 88.0 70.4 77.3 84.1\nXLNet-large (Yang et al., 2019) 89.8 91.8 93.9 95.6 92.8 63.6 91.8 89.2 83.8 82.1 87.4\nRoBERTa-large (Liu et al., 2019) 90.2 92.2 94.7 96.4 93.4 68.0 92.4 90.9 86.6 84.5 88.9\nALBERT-xxlarge (Lan et al., 2020) 90.8 92.2 95.3 96.9 93.8 71.4 93.0 90.9 89.2 86.1 90.0\nALBERT-xxlarge (rerun)Median 90.6 92.2 95.4 96.7 93.7 69.5 93.0 91.2 87.4 85.3 89.5\nALBERT-xxlarge + RecAdamMedian 90.5 92.3 95.3 96.8 93.7 72.9 92.9 91.9 89.3 86.8 90.2\nALBERT-xxlarge (rerun)Max 90.7 92.2 95.4 96.8 93.8 72.1 93.2 91.4 89.9 86.7 90.2\nALBERT-xxlarge + RecAdamMax 90.6 92.4 95.5 97.0 93.9 75.1 93.0 93.1 91.7 88.2 91.1\nTable 1: State-of-the-art single-task single-model results on the dev set of the GLUE benchmark. The number\nbelow each task refers to the number of training data. The average scores of the tasks with large training data\n(>10k), the tasks with small training data (<10k), and all the tasks are reported separately. We rerun the baseline\nof vanilla ﬁne-tuning without further pretraining on MNLI. We report median and maximum over 5 runs.\nMethod CoLA STS MRPC RTE Avg\nvanilla ﬁne-tuning 60.6 89.8 86.5 71.1 77.0\nRecAdam + PI 62.0 90.4 87.3 73.6 78.3\nRecAdam + RI 62.4 90.4 87.7 74.4 78.7\nTable 2: Comparison of different model initialization\nstrategies: pretrained initialization (PI) and Random\nInitialization (RI). We report median over 5 runs.\n4.3 Analysis\nModel Initialization: With our RECADAM\nmethod, the model can be initialized with random\nvalues, and recall the knowledge of pretraining\ntasks while learning the new tasks.\nIt is interesting to see whether the choice of ini-\ntialization strategies would impact the performance\nof our RECADAM method. Table 2 shows the\nperformance comparison of different initialization\nstrategies for RECADAM obtained by the BERT-\nbase model. It shows that RECADAM with both\ninitialization strategies can outperform the vanilla\nﬁne-tuning method on all four tasks. For the target\ntask STS, the model with pretrained initialization\ncan achieve the same result as random initializa-\ntion. For the other tasks (e.g., CoLA, MRPC, RTE),\nthe models with random initialization can achieve\nbetter performance. It is because the randomly\ninitialized model can beneﬁt from a larger param-\neter search space. By contrast, with pretrained\ninitialization, the search space would be limited to\naround the pretraining model, making it harder for\nthe model to escape poor local minima and gain\nbetter performance on the new tasks.\nForgetting Analysis: As introduced in §3.2, we\nrealize multi-task ﬁne-tuning with the Objective\nShifting technique, which allows the model’s learn-\ning objective to shift from the source tasks to the\ntarget tasks gradually. The hyperparameter kcon-\ntrols the rate of the objective shifting.\nFigure 2 shows the learning curves of our ﬁne-\ntuning methods with different kvalue obtained by\nBERT-base model trained on CoLA dataset. As dis-\ncussed in §3.2, Fine-tuning and multi-task learning\ncan be regarded as the special cases (k→∞ and\nk→0) of our method.\nAs shown in Figure 2a, with the larger shifting\nrate k, the model can converge quickly on the target\ntask. As kdecreases, it takes a longer time for the\nmodel to converge on the target task because of\nthe slower shifting from the pretrained knowledge\nrecalling to target task learning.\nFigure 2b shows the pretrained knowledge for-\ngetting during the ﬁne-tuning process. We mea-\nsure the pretrained knowledge forgetting by the\nEuclidean distance between the weights of the\nﬁne-tuning model and the pretrained model. With\nvanilla ﬁne-tuning ( k →∞), the Euclidean dis-\ntance begins at zero and increases as the model\nlearns the target task. With a modest shifting ratek,\nat the very early timesteps, the Euclidean distance\ndrops sharply because of the random initialization\nand pretrained knowledge recalling. Then the curve\nrises with the growth rate slowing down because of\nthe target task learning. As kdecreases, thanks to\nmore iterations of pretrained knowledge recalling,\nthe model can achieve less forgetting at the end of\n7877\n/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni0000000b/uni00000014/uni00000048/uni00000016/uni0000000c\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a/uni00000037/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000057/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056\n(a) Training loss on the target task\n/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000013\n/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000048/uni00000010/uni00000017\n/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000048/uni00000010/uni00000017\n/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000048/uni00000010/uni00000016\n/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000048/uni00000010/uni00000016\n/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000048/uni00000010/uni00000015\n/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000048/uni00000010/uni00000015\n/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000048/uni00000010/uni00000014\n/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000048/uni00000010/uni00000014\n/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014\n/uni0000004e/uni00000003/uni00000020/uni00000003\n/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000003/uni0000000b/uni00000014/uni00000048/uni00000016/uni0000000c\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015/uni00000028/uni00000058/uni00000046/uni0000004f/uni0000004c/uni00000047/uni00000048/uni00000044/uni00000051/uni00000003/uni00000047/uni0000004c/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048 (b) Knowledge forgetting from the source tasks\nFigure 2: Learning curves obtained by BERT-base model trained with different objective shifting ratekon CoLA.\nthe ﬁne-tuning.\nOverall, our methods provide a bridge between\nﬁne-tuning and multi-task learning. With smaller\nk, the model achieves less knowledge forgetting\nfrom the source tasks but risks not converging com-\npletely on the target task. With a good balance\nbetween the pretrained knowledge recalling and\nnew task learning, our methods can consistently\noutperform the vanilla ﬁne-tuning by not only con-\nverging on target tasks but also less forgetting from\nsource tasks.\n5 Related Works\nCatastrophic forgetting has been observed as a\ngreat challenge issue in sequential transfer learn-\ning, especially in the continuous learning paradigm\n(McCloskey and Cohen, 1989; French, 1999; Good-\nfellow et al., 2013; De Lange et al., 2019). Many\nmethods have been proposed to avoid catastrophic\nforgetting (Kirkpatrick et al., 2017; Li and Hoiem,\n2017; Rebufﬁ et al., 2017; Mallya and Lazebnik,\n2018). We focus on regularization-based meth-\nods (Kirkpatrick et al., 2017; Li and Hoiem, 2017)\nwhich recall the previous knowledge with an regu-\nlarization term, because they don’t require the stor-\nage of the pretraining data, and are ﬂexible on the\nnew tasks. Regularization-based methods can be\nfurther divided into data-focused and prior-focused\nmethods. Data-focused methods regularize the\nnew task learning by knowledge distillation from\nthe pretrained model (Hinton et al., 2015; Li and\nHoiem, 2017; Zhang et al., 2020a). Prior-focused\nmethods regard the distribution of the pretrained pa-\nrameters as prior when learning the new task (Kirk-\npatrick et al., 2017; Zenke et al., 2017; Xuhong\net al., 2018; Aljundi et al., 2018). We adopted the\nidea of prior-focused methods because they enable\nthe model to learn more general knowledge from\nthe pretrained parameters more efﬁciently. While\nthe prior-focused methods, such as EWC (Kirk-\npatrick et al., 2017) and its variants (Schwarz et al.,\n2018; Liu et al., 2018), don’t directly access to\nthe pretraining data, they need some pretraining\nknowledge which is not available in our setting.\nTherefore, we further approximate to a quadratic\npenalty which is independent with the pretraining\ndata given the pretrained parameters.\nCatastrophic forgetting in NLP has raised in-\ncreased attention recently (Mou et al., 2016; Arora\net al., 2019; Chronopoulou et al., 2019). Many\napproaches have been proposed to overcome the\nforgetting problem in various domains, such as\nmachine translation (Miceli-Barone et al., 2017;\nThompson et al., 2019) and reading comprehension\n(Xu et al., 2019). As sequential transfer learning\nwidely used for NLP tasks (Howard and Ruder,\n2018; Devlin et al., 2019; Liu et al., 2019; Lan\net al., 2020), previous works explore many ﬁne-\ntuning tricks to reduce catastrophic forgetting for\nadaptation of the deep pretrained LMs (Howard and\nRuder, 2018; Sun et al., 2019; Zhang et al., 2019;\nChen et al., 2019; Jiang et al., 2019; Lee et al.,\n2020). In this paper, we bring the idea of multi-task\nlearning which can inherently avoid catastrophic\nforgetting, and achieve consistent improvement\nwith the proposed RECADAM optimizer.\n6 Conclusion\nIn this paper, we propose to tackle the catas-\ntrophic forgetting in transferring deep pretrained\nlanguage models by bridging two transfer learning\nparadigms: sequential ﬁne-tuning and multi-task\n7878\nlearning. To cope with the absence of pretraining\ndata during the joint learning of the pretraining task,\nwe introduce a Pretraining Simulation mechanism\nto learn the pretraining task without data. Then\nwe introduce the Objective Shifting mechanism\nto better balance the learning of the pretraining\nand downstream tasks. Experiments demonstrate\nthe superiority of our method in transferring deep\npretrained language models, and we provide the\nopen-source RECADAM optimizer by integrating\nthe proposed mechanisms into Adam optimizer to\nfacilitate better usage of deep pretrained language\nmodels.\nAcknowledgements\nWe are grateful for the helpful comments and sug-\ngestions from the anonymous reviewers. This work\nwas supported by the National Natural Science\nFoundation of China (NSFC) via grant 61976072,\n61632011 and 61772153.\nReferences\nRahaf Aljundi, Francesca Babiloni, Mohamed Elho-\nseiny, Marcus Rohrbach, and Tinne Tuytelaars.\n2018. Memory aware synapses: Learning what (not)\nto forget. In Proceedings of the European Confer-\nence on Computer Vision, pages 139–154.\nGaurav Arora, Afshin Rahimi, and Timothy Baldwin.\n2019. Does an lstm forget more than a cnn? an\nempirical study of catastrophic forgetting in nlp. In\nProceedings of the Australasian Language Technol-\nogy Association, pages 77–86.\nRoy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The second PASCAL recognising\ntextual entailment challenge. Proceedings of the\nSecond PASCAL Challenges Workshop on Recognis-\ning Textual Entailment.\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009. The\nﬁfth PASCAL recognizing textual entailment chal-\nlenge. In Proceedings of Text Analysis Conference.\nSamuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew\nDai, Rafal Jozefowicz, and Samy Bengio. 2016.\nGenerating sentences from a continuous space. In\nProceedings of The SIGNLL Conference on Compu-\ntational Natural Language Learning, pages 10–21.\nRich Caruana. 1997. Multitask learning. Machine\nlearning, 28(1):41–75.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings of\nthe International Workshop on Semantic Evaluation,\npages 1–14. Association for Computational Linguis-\ntics.\nXinyang Chen, Sinan Wang, Bo Fu, Mingsheng Long,\nand Jianmin Wang. 2019. Catastrophic forgetting\nmeets negative transfer: Batch spectral shrinkage for\nsafe transfer learning. In Proceedings of the Ad-\nvances in Neural Information Processing Systems ,\npages 1906–1916.\nAlexandra Chronopoulou, Christos Baziotis, and\nAlexandros Potamianos. 2019. An embarrassingly\nsimple approach for transfer learning from pre-\ntrained language models. In Proceedings of the\nNorth American Chapter of the Association for Com-\nputational Linguistics, pages 2089–2095.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges. Eval-\nuating Predictive Uncertainty, Visual Object Classi-\nﬁcation, and Recognising Tectual Entailment, pages\n177–190. Springer.\nMatthias De Lange, Rahaf Aljundi, Marc Masana,\nSarah Parisot, Xu Jia, Ales Leonardis, Gregory\nSlabaugh, and Tinne Tuytelaars. 2019. Contin-\nual learning: A comparative study on how to defy\nforgetting in classiﬁcation tasks. arXiv preprint\narXiv:1909.08383.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the North American\nChapter of the Association for Computational Lin-\nguistics, pages 4171–4186. Association for Compu-\ntational Linguistics.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the International Workshop on\nParaphrasing.\nRobert M. French. 1999. Catastrophic forgetting in\nconnectionist networks. Trends in Cognitive Sci-\nences, 3(4):128 – 135.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing, pages 1–9. Association for Com-\nputational Linguistics.\nIan J Goodfellow, Mehdi Mirza, Da Xiao, Aaron\nCourville, and Yoshua Bengio. 2013. An em-\npirical investigation of catastrophic forgetting in\ngradient-based neural networks. arXiv preprint\narXiv:1312.6211.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\n7879\nJeremy Howard and Sebastian Ruder. 2018. Univer-\nsal language model ﬁne-tuning for text classiﬁcation.\nIn Proceedings of the Association for Computational\nLinguistics, pages 328–339.\nFerenc Husz ´ar. 2017. On quadratic penalties in\nelastic weight consolidation. arXiv preprint\narXiv:1712.03847.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xi-\naodong Liu, Jianfeng Gao, and Tuo Zhao. 2019.\nSmart: Robust and efﬁcient ﬁne-tuning for pre-\ntrained natural language models through princi-\npled regularized optimization. arXiv preprint\narXiv:1911.03437.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof the International Conference on Learning Repre-\nsentations.\nEliyahu Kiperwasser and Miguel Ballesteros. 2018.\nScheduled multi-task learning: From syntax to trans-\nlation. Transactions of the Association for Computa-\ntional Linguistics, 6:225–240.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the National Academy of Sciences ,\n114(13):3521–3526.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learn-\ning of language representations. In Proceedings of\nthe International Conference on Learning Represen-\ntations.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2020. Mixout: Effective regularization to ﬁnetune\nlarge-scale pretrained language models. In Proceed-\nings of the International Conference on Learning\nRepresentations. OpenReview.net.\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The Winograd schema challenge. In\nProceedings of the Association for the Advancement\nof Artiﬁcial Intelligence Spring Symposium, page 47.\nZhizhong Li and Derek Hoiem. 2017. Learning with-\nout forgetting. IEEE Transactions on Pattern Analy-\nsis and Machine Intelligence, 40(12):2935–2947.\nXialei Liu, Marc Masana, Luis Herranz, Joost Van de\nWeijer, Antonio M Lopez, and Andrew D Bagdanov.\n2018. Rotate your networks: Better weight con-\nsolidation and less catastrophic forgetting. In Pro-\nceedings of the International Conference on Pattern\nRecognition, pages 2262–2268. IEEE.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In Proceedings of the\nInternational Conference on Learning Representa-\ntions. OpenReview.net.\nDavid JC MacKay. 2003. Information theory, infer-\nence and learning algorithms. Cambridge university\npress.\nArun Mallya and Svetlana Lazebnik. 2018. Packnet:\nAdding multiple tasks to a single network by itera-\ntive pruning. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition ,\npages 7765–7773.\nJames Martens. 2014. New insights and perspectives\non the natural gradient method. arXiv preprint\narXiv:1412.1193.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of\nLearning and Motivation , volume 24, pages 109–\n165. Elsevier.\nAntonio Valerio Miceli-Barone, Barry Haddow, Ulrich\nGermann, and Rico Sennrich. 2017. Regularization\ntechniques for ﬁne-tuning in neural machine trans-\nlation. In Proceedings of the Empirical Methods in\nNatural Language Processing, pages 1489–1494.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2020. On the stability of ﬁne-tuning\nbert: Misconceptions, explanations, and strong base-\nlines. arXiv preprint arXiv:2006.04884.\nLili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,\nLu Zhang, and Zhi Jin. 2016. How transferable are\nneural networks in nlp applications? In Proceedings\nof the Empirical Methods in Natural Language Pro-\ncessing, pages 479–489.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Proceedings of the Advances in Neu-\nral Information Processing Systems , pages 8024–\n8035. Curran Associates, Inc.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the North American\nChapter of the Association for Computational Lin-\nguistics, pages 2227–2237.\n7880\nMatthew E. Peters, Sebastian Ruder, and Noah A.\nSmith. 2019. To tune or not to tune? adapting pre-\ntrained representations to diverse tasks. In Proceed-\nings of the Workshop on Representation Learning\nfor NLP, pages 7–14. Association for Computational\nLinguistics.\nJason Phang, Thibault F ´evry, and Samuel R Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Proceedings\nof the Empirical Methods in Natural Language Pro-\ncessing, pages 2383–2392. Association for Compu-\ntational Linguistics.\nSylvestre-Alvise Rebufﬁ, Alexander Kolesnikov,\nGeorg Sperl, and Christoph H Lampert. 2017. icarl:\nIncremental classiﬁer and representation learning.\nIn Proceedings of the IEEE conference on Computer\nVision and Pattern Recognition, pages 2001–2010.\nJo˜ao Ribeiro, Francisco Melo, and Jo ˜ao Dias. 2019.\nMulti-task learning and catastrophic forgetting in\ncontinual reinforcement learning. EPiC Series in\nComputing, 65:163–175.\nSebastian Ruder. 2017. An overview of multi-task\nlearning in deep neural networks. arXiv preprint\narXiv:1706.05098.\nSebastian Ruder. 2019. Neural transfer learning for\nnatural language processing. Ph.D. thesis, NUI Gal-\nway.\nJonathan Schwarz, Wojciech Czarnecki, Je-\nlena Luketina, Agnieszka Grabska-Barwinska,\nYee Whye Teh, Razvan Pascanu, and Raia Hadsell.\n2018. Progress & compress: A scalable framework\nfor continual learning. In Proceedings of the\nInternational Conference on Machine Learning ,\npages 4535–4544. PMLR.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the Empirical Methods in\nNatural Language Processing, pages 1631–1642.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune bert for text classiﬁcation?\nIn Proceedings of the China National Conference on\nChinese Computational Linguistics, pages 194–206.\nSpringer.\nBrian Thompson, Jeremy Gwinnup, Huda Khayrallah,\nKevin Duh, and Philipp Koehn. 2019. Overcoming\ncatastrophic forgetting during domain adaptation of\nneural machine translation. In Proceedings of the\nNorth American Chapter of the Association for Com-\nputational Linguistics, pages 2062–2068.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the International Conference on Learn-\ning Representations. OpenReview.net.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint 1805.12471.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus for\nsentence understanding through inference. In Pro-\nceedings of the North American Chapter of the As-\nsociation for Computational Linguistics.\nYing Xu, Xu Zhong, Antonio Jose Jimeno Yepes, and\nJey Han Lau. 2019. Forget me not: Reducing catas-\ntrophic forgetting for domain adaptation in reading\ncomprehension. arXiv preprint arXiv:1911.00202.\nJiabin Xue, Jiqing Han, Tieran Zheng, Xiang Gao,\nand Jiaxing Guo. 2019. A multi-task learning\nframework for overcoming the catastrophic forget-\nting in automatic speech recognition. arXiv preprint\narXiv:1904.08039.\nLI Xuhong, Yves Grandvalet, and Franck Davoine.\n2018. Explicit inductive bias for transfer learning\nwith convolutional networks. In Proceedings of\nthe International Conference on Machine Learning ,\npages 2825–2834.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Proceedings of the Ad-\nvances in Neural Information Processing Systems ,\npages 5754–5764.\nFriedemann Zenke, Ben Poole, and Surya Ganguli.\n2017. Continual learning through synaptic intelli-\ngence. In Proceedings of the International Con-\nference on Machine Learning , pages 3987–3995.\nJMLR. org.\nJeffrey O Zhang, Alexander Sax, Amir Zamir,\nLeonidas Guibas, and Jitendra Malik. 2019. Side-\ntuning: Network adaptation via additive side net-\nworks. arXiv preprint arXiv:1912.13503.\nJunting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li,\nSerafettin Tasci, Larry Heck, Heming Zhang, and C-\nC Jay Kuo. 2020a. Class-incremental learning via\ndeep model consolidation. In Proceedings of the\nIEEE Winter Conference on Applications of Com-\nputer Vision, pages 1131–1140.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q\nWeinberger, and Yoav Artzi. 2020b. Revisit-\ning few-sample bert ﬁne-tuning. arXiv preprint\narXiv:2006.05987.\n7881\nModel MNLI QQP QNLI SST Avg CoLA STS MRPC RTE Avg Avg392k 363k 108k 67k >10k 8.5k 5.7k 3.5k 2.5k <10k\nBERT-base (Devlin et al., 2019) 84.6 71.2 90.5 93.5 85.0 52.1 85.8 88.9 66.4 73.3 79.1\nBERT-base + RecAdam 85.0 71.2 91.0 94.0 85.3 55.4 85.8 88.6 70.0 75.0 80.1\nTable 3: Results on the test set of the GLUE benchmark, scored by the evaluation server.5 The number below each\ntask refers to the number of training data. The average scores of the tasks with large training data (>10k), the tasks\nwith small training data (<10k), and all the tasks are reported separately. Following Devlin et al. (2019), we report\nF1 scores for QQP and MRPC, Spearman correlations for STS-B, Matthew’s correlations for CoLA, and accuracy\nscores for the other tasks. We submitted the best model on each dev set.\nA Appendices\nA.1 Test Results on GLUE Tasks\nAs shown in §4.2, we report both the median and\nthe maximum scores over ﬁve runs for the vanilla\nﬁne-tuning method and our RECADAM ﬁne-tuning\nmethod on the dev set of the GLUE benchmark.\nThe results with the BERT-base model show that\nwe outperform the baseline method by 1.0% on\nthe average median performance and 1.1% on the\naverage maximum performance.\nTo conﬁrm our best model’s generalization on\nthe dev set, we present the single-task single-model\nresults with the BERT-base model on the test set\nof the GLUE benchmark in Table 3. Similar to the\nperformance on the dev set, we achieve the same\nor better results on 7 out of 8 tasks of the GLUE\nbenchmark and achieves 1.0% improvement on\naverage.\nCompared to the results (+0.3% on average) on\nthe tasks with larger training data (>10k), we ob-\ntain more signiﬁcant improvement (+1.7% on aver-\nage) on the tasks with smaller training data (<10k).\nIt is consistent with our ﬁndings on the dev results\n(discussed in §4.2), which shows the generaliza-\ntion and effectiveness of the proposed RECADAM\nmethod.\n5https://gluebenchmark.com/leaderboard",
  "topic": "Forgetting",
  "concepts": [
    {
      "name": "Forgetting",
      "score": 0.9031287431716919
    },
    {
      "name": "Computer science",
      "score": 0.8273242712020874
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7609742879867554
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6438689231872559
    },
    {
      "name": "Recall",
      "score": 0.6310397982597351
    },
    {
      "name": "Task (project management)",
      "score": 0.5908522605895996
    },
    {
      "name": "Mechanism (biology)",
      "score": 0.5313482880592346
    },
    {
      "name": "Language model",
      "score": 0.5302664041519165
    },
    {
      "name": "Transfer of learning",
      "score": 0.5060276389122009
    },
    {
      "name": "Deep learning",
      "score": 0.4851430356502533
    },
    {
      "name": "Focus (optics)",
      "score": 0.4647008776664734
    },
    {
      "name": "Machine learning",
      "score": 0.34364885091781616
    },
    {
      "name": "Natural language processing",
      "score": 0.32718759775161743
    },
    {
      "name": "Cognitive psychology",
      "score": 0.09430238604545593
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    }
  ]
}