{
  "title": "Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning",
  "url": "https://openalex.org/W3103616906",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4202063432",
      "name": "Lin, Zhaojiang",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A4222272762",
      "name": "Madotto, Andrea",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2743188307",
      "name": "Fung, Pascale",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2108325777",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2964352247",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W4322588812",
    "https://openalex.org/W2949193663",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2913443447",
    "https://openalex.org/W99485931",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2785093437",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W2102409316",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2914397182",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W3019410392",
    "https://openalex.org/W2996882407",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2890964657",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2979478117",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W2487501366",
    "https://openalex.org/W2953830716",
    "https://openalex.org/W2755806193",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2914204778",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2945978556",
    "https://openalex.org/W2911300548",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Fine-tuning pre-trained generative language models to down-stream language generation tasks has shown promising results. However, this comes with the cost of having a single, large model for each task, which is not ideal in low-memory/power scenarios (e.g., mobile). In this paper, we propose an effective way to fine-tune multiple down-stream generation tasks simultaneously using a single, large pretrained model. The experiments on five diverse language generation tasks show that by just using an additional 2-3% parameters for each task, our model can maintain or even improve the performance of fine-tuning the whole model.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 441–459\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n441\nExploring Versatile Generative Language Model Via Parameter-Efﬁcient\nTransfer Learning\nZhaojiang Lin∗, Andrea Madotto∗, Pascale Fung\nCenter for Artiﬁcial Intelligence Research (CAiRE)\nDepartment of Electronic and Computer Engineering\nThe Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong\n{zlinao,amadotto}@connect.ust.hk,\npascale@ece.ust.hk\nAbstract\nFine-tuning pre-trained generative language\nmodels to down-stream language generation\ntasks has shown promising results. However,\nthis comes with the cost of having a single,\nlarge model for each task, which is not ideal\nin low-memory/power scenarios (e.g., mobile).\nIn this paper, we propose an effective way\nto ﬁne-tune multiple down-stream generation\ntasks simultaneously using a single, large pre-\ntrained model. The experiments on ﬁve di-\nverse language generation tasks show that by\njust using an additional 2-3% parameters for\neach task, our model can maintain or even\nimprove the performance of ﬁne-tuning the\nwhole model 1.\n1 Introduction\nLarge-scale language models (Radford et al., 2019;\nDai et al., 2019) have shown to be effective in learn-\ning highly transferable embedding, which can be\nused in several down-stream tasks. For instance,\nbidirectional models (Peters et al., 2018; Devlin\net al., 2019) are ﬁne-tuned to improve classiﬁca-\ntion tasks (Wang et al., 2019), while, unidirectional\nlanguage models (Radford et al., 2019) are more\neffective in language generation tasks. In this work,\nwe focus on the latter, and show that it is possible to\ndynamically steer the output of a language model\n(e.g., GPT-2) towards a speciﬁc task (e.g., sum-\nmarization) without modifying the original model\nparameters.\nFeature-based transfer (Howard and Ruder,\n2018; Fan et al., 2020a,b) and ﬁne-tuning (Devlin\net al., 2019) are the most commonly used methods\nfor transfer learning of a language. The former\nfreezes the pre-trained model and uses it as a fea-\nture extractor for training a new classiﬁer, and the\n∗ ∗Equal contributions.\n1Code available in https://github.com/zlinao/\nVGLM\nlatter uses the pre-trained weight as a weight initial-\nization for the model to be trained for downstream\ntasks. The feature-based transfer strategy has not\nshown promising results (Devlin et al., 2019), while\nﬁne-tuning, on the other hand, can achieve state of\nthe art performance in multiple tasks (Dong et al.,\n2019). However, the downside of the latter is the\nneed for a seperate model for each of the ﬁne-tuned\ntasks. This is especially relevant for on-device\napplications, where a limited amount of computa-\ntion/memory is available.\nTherefore, we study how to effectively use a\nsingle pre-trained model as the backbone for mul-\ntiple language generation tasks, such as conver-\nsational question answering, summarization, ma-\nchine translation, multi-turn chit-chat dialogue, and\ntask-oriented natural language generation. This is\na particular parameter-sharing schema, where we\nconstrain the shared parameters to be the ones in\nthe pre-trained model, and we learn task-speciﬁc\nparameters for each of the considered datasets.\nIn this paper, we propose to use residual adapter\nlayers (Houlsby et al., 2019) and task embeddings\nfor modelling the aforementioned task-speciﬁc pa-\nrameters, and we explore different training strate-\ngies such as distillation (Hinton et al., 2015; Kim\nand Rush, 2016). We also analyse the trade-off be-\ntween freezing or not freezing the language model\nparameters by leveraging two learning settings,\nmulti-task (MT) (Caruana, 1997) and continual\nlearning (CL) (Thrun and Pratt, 2012). With our\nexperiments, we empirically demonstrate that by\nadding less than 3% task-speciﬁc parameters, our\nmodel can maintain or even achieve better perfor-\nmance than ﬁne-tuning the whole model.\n2 Related work\nPre-trained generative language models (Radford\net al., 2019, 2018; Dai et al., 2019; Yang et al.,\n442\n2019; Peters et al., 2018) have shown to be very\neffective in language generation, whereas, bidirec-\ntional pre-trained models (Devlin et al., 2019; Liu\net al., 2019; Sanh et al., 2019) signiﬁcantly improve\nthe performance of several down-stream classiﬁ-\ncation tasks. Fine-tuning large pre-trained models\nhas shown positive results in dialogue tasks (Wolf\net al., 2019b; Budzianowski and Vuli´c, 2019) and\nother language generation tasks (Dong et al., 2019).\nHowever, all of the previous works only consider\nﬁne-tuning on each generation task individually,\nwhich requires a separate model for each task. In\nthis work, we use only a single model, for multiple\ngeneration tasks.\nResidual adapters, derived from residual net-\nworks (He et al., 2016), were ﬁrst introduced by\nRebufﬁ et al. (2017) for multiple visual domain\nlearning. Houlsby et al. (2019) proposed low-rank\nresidual adapters to improve the scalability of the\nadapter module, and effectively transfer BERT (De-\nvlin et al., 2019) to multiple text classiﬁcation tasks\nsimultaneously, while Bapna and Firat (2019) ap-\nplied an adapter layer to language/domain adapta-\ntion for neural machine translation. On the other\nhand, Dathathri et al. (2019) proposed a plug and\nplay method to control the language model genera-\ntion without ﬁnetuning the model. Differently, in\nthis paper, we extend the idea of adapters to a large\nvariety of language generation tasks, which has not\nbeen considered before, and we compare the idea\nof a ﬁxed pre-trained back-bone for continual learn-\ning with multi-task training (Stickland and Murray,\n2019).\n3 Methodology\nThe Versatile Language Model ( VLM) is com-\nposed of three components: a pre-trained language\nmodel back-bone (e.g., GPT-2), and two kinds of\nspecialized parameters for each generation tasks\nsuch as low-rank residual adapters and task embed-\nding. Figure 1 shows the VLM architecture with\nthe specialized parameters in different colours.\nResidual Adapters These are trainable modules\nwhich steer the pre-trained model to different down-\nstream tasks. We adapt the design of the feed-\nforward Transformer sub-layer following Bapna\nand Firat (2019). To elaborate, the adapter block\nconsists of 1) a layer normalization (Ba et al., 2016)\nfor an efﬁcient adaptation and 2) a following au-\ntoencoder (Hinton and Zemel, 1994), with a resid-\nual connection. Formally, given the hidden rep-\nCQA NLG NMT DLGSUM\nN×\nWord\nEmbedding\nPositional\nEmbedding\nDocument Q1 A1\nAtt 1 Att 2 Att n Response· · ·\nArticle Summary\nSource-Lang Target-Lang\nPersona Sys Usr\nAdapter Layers\n+\nSegment Embedding\nTask Embedding\nGPT-2 Layer\nUsr· · · Sys\n· · · Qn An\nLM Head\nFigure 1: Simpliﬁed illustration of the Versatile Lan-\nguage Model. A detailed illustration is reported in Ap-\npendix A1.\nresentation Hi ∈Rt×d from the language model\nlayer i, where d is the hidden dimension and t is\nthe current generation step, the residual adapter\ncomputes the following:\nAdapter(Hi) = (ReLU(LN(Hi)WE\ni ))WD\ni + Hi\nwhere WE\ni and WD\ni are parameters of dimension\nd ×m and m ×d respectively, and LN(·) denotes\nlayer normalization. The bottleneck dimension m\nis tunable and it allows to adjust the capacity of the\nadapter according to complexity of the target task.\nTask Embedding. To adapt unconditional gener-\native language models to different conditional lan-\nguage generation tasks (e.g., CoQA, Summariza-\ntion), we construct a set of task-speciﬁc segment\nembeddings. For example, in multi-turn dialogue,\nwe alternate between System and User embeddings\nto help the model to capture the hierarchical struc-\nture of dialogues. Figure 1 shows the task embed-\nding for each task, and more details are available\nin Appendix A2.\nKnowledge Distillation In tasks with a large\ndistributional shift from the original pre-trained\nlanguage model (e.g., Machine Translation), we\nexpect a larger performance gap between VLM\nand full ﬁne-tuning. To cope with this issue, we\npropose to use sentence-level knowledge distilla-\ntion (Kim and Rush, 2016), to help the task-speciﬁc\nparameters to better adapt to the task. Speciﬁcally,\n443\nPersona (DLG) NMT SUM CoQA NLGParam. ppl. ↓ BLEU ↑ BLEU ↑ ROUGE 2 ↑ F1 ↑ BLEU ↑ AVG↑\nGPT-2 Finetune 5× 13.13 2.17 25.45 18.1 67.7 66.4 57.77\nw/o Pre-Train 5× 37.77 0.99 16.52 17.0 15.1 60.5 53.51\nw/o Task Emb. 5× 13.24 0.00 0.61 15.0 35.2 53.1 47.25\nLM Head 2.55× 17.58 1.34 12.05 15.8 47.0 65.2 55.25\nVLM MT 1.13× 13.15 0.84 22.49 17.7 69.3 65.6 57.08\nVLM 1.13× 14.06 1.99 24.19* 18.0* 66.2 67.1 57.97\nw/o Task Emb. 1.13× 14.31 0.00 0.95 15.0 32.2 58.3 50.99\nReference - 38.08¶ - 29.2§ 17.20 ¶¶ 45.4†† 65.9‡‡ 57.54\nSOTA - 17.51† - 35.2‡ 21.53 §§ 82.5∥ 66.2‡‡ 57.44\nTable 1: Results of VLM versus other ﬁne-tuning techniques on the ﬁve evaluated datasets. Param. refers\nto the number of parameters that need to be stored after training. We use the adapter with distillation* for\ntranslation and summarization. The Reference and SOTA results are: Proﬁle Memory ¶(Zhang et al., 2018),\nTransferTransfo† (Wolf et al., 2019b), DynamicConv‡(Wu et al., 2019), Transformer§(Vaswani et al., 2017), PG¶¶\n(See et al., 2017), T5-11B§§(Raffel et al., 2019), UniLM∥(Dong et al., 2019), PG†† (Reddy et al., 2019) and SOTA\nsystem‡‡ in Duˇsek et al. (2019)\nwe ﬁrst fully ﬁne-tune a GPT-2 model on the train-\ning set of a task (e.g., Machine Translation). Then\nwe replace the gold target (e.g., gold translation)\nin the training set with the greedy decoded output\nfrom the full ﬁne-tuned model. Finally, the new\nconstructed training set is used to ﬁne-tune the stu-\ndent VLM.\n4 Experiments\n4.1 Datasets & Evaluation Metrics\nWe conduct our experiment on ﬁve diverse datasets\ncovering multiple generation tasks: Persona-\nChat (Zhang et al., 2018; Dinan et al., 2019) for\nchit-chat based dialogue (DLG), IWSLT (Cettolo\net al., 2016) German-English neural machine trans-\nlation (NMT), CNN/Daily-Mail (Hermann et al.,\n2015; Nallapati et al., 2016) for text-summarization\n(SUM), CoQA (Reddy et al., 2019) for genera-\ntive conversational question answering (CQA), and\nE2E NLG-challenge (Duˇsek et al., 2019) for task-\noriented natural language generation (NLG).\nWe use a large variety of evaluation metrics,\nsuch as perplexity, F1 score, BLEU (Papineni et al.,\n2002), ROUGE (Lin, 2004), NIST (Lin and Och,\n2004), METEOR (Denkowski and Lavie, 2014)\nand CiDER (Vedantam et al., 2015). Each task\nuses the appropriate measure, as reported in Table\n1, where in NLG we report the normalized average\nscore of multiple metrics, as in Duˇsek et al. (2019).\nMore information about task description and the\nmetrics used in each task are reported in Appendix\nA2.\n4.2 Implementation and model comparison\nWe implement VLM based on GPT-2-small\n(124M) (Wolf et al., 2019a), and experiment\nwith varying adapter bottleneck dimensions in\n{10, 50, 100, 300}and pick the best one in each\ntask to trade-off the performance with the param-\neter efﬁciency. Speciﬁcally, we choose bottleneck\nsizes 100, 300, 100, 300 and 10 for DLG, NMT,\nSUM, QA, and NLG, respectively, which results\nin 13% additional parameters in total. We ablate\nthe adapter training with and without knowledge\ndistillation and task embeddings. We also test the\nperformance of a frozen back-bone (VLM) to show\nthe ability to continuously learn tasks, and multi-\ntask ﬁne-tune (VLM MT) with a trainable backbone\nto show possible positive transferring among tasks\nas in Stickland and Murray (2019). More training\ndetails and the dataset pre-processing are reported\nin Appendix A2.\nTo show the effectiveness of the proposed\nmethodology of learning a versatile generative\nmodel, we compare (i) ﬁne-tuneing the whole GPT-\n2 model for each task separately (GPT-2 Finetune),\n(ii) ﬁne-tuning the language model head of GPT-\n2 for each task (LM-Head), (iii) existing baseline\nmodels reported (Reference), and (iv) the state-of-\nthe-art models for all the tasks (SOTA).\n4.3 Results and Analysis\nTable 1 shows the experimental results of the afore-\nmentioned models. Appendix A3 and A4 report\ndetailed results and generated samples for all the\n444\ndatasets. Our ﬁndings can be summarized as fol-\nlow:\nFine-tuning GPT-2 vs Baseline & SOTA. Fine-\ntuneing the whole GPT-2-small in each task can\ngenerally improve on the performance of competi-\ntive baselines such as Pointer-Generator (See et al.,\n2017) in summarization (SUM) and CoQA. In\nboth the Persona-Chat and the NLG tasks GPT-2\nﬁne-tuning slightly outperforms the current SOTA,\nwhereas, we observe a performance gap between\nGPT-2 and SOTA in NMT and SUM. Notably,\nthe advantage of GPT-2 pre-training is limited in\nNMT: 1) no or little German text is present in the\npretraining corpus; 2) the GPT-2 BPE (Sennrich\net al., 2016) tokenizer is optimized for English text,\nand not for multiple languages. Finally, in SUM\nand CoQA, the SOTA models use 100 ×bigger\nmodels (Raffel et al., 2019) and bidirectional atten-\ntion (Dong et al., 2019), where instead, GPT-2 uses\nunidirectional attention.\nAdapter vs Fine-tuning GPT-2 & LM Head.\nFine-tuning only the adapter layers introduces 13%\nadditional parameters to the model with a mini-\nmal loss in performance ( 0.4%) compared to ﬁne-\ntuning a seperate GPT-2 model. Moreover, the\nadapter layers are more effective, both in terms of\nperformance and number of additional parameters,\ncompared to ﬁne-tuning LM-Head.\nKnowledge Distillation (KD) Using KD in the\ntraining procedure is especially useful in tasks such\nas NMT and SUM, where the gap between ﬁne-\ntuning the whole model and adapter is large. This\nis because KD reduces the complexity of training\ntargets (by replacing the gold training target with\na teacher model generated target), which helps\nwith low-capacity adapter (with 4% parameter)\nby providing an easier translation/summarization\ntask (Zhou et al., 2019). Figure 2 shows the ef-\nfect of using distillation training when the gap with\nthe full ﬁne-tuning is more substantial. On the\nother hand, when the adapter performance is very\nclose to that of the ﬁne-tuning baseline, or better\n(i.e. NLG), distillation has no impact on the ﬁnal\nperformance.\nTask Embedding The specialized segment em-\nbedding (a.k.a. task embedding) is very important\nfor achieving competitive performance, indepen-\ndently of the adapter. In Table 1, we can observe\na substantial drop in performance when the task\n0.0% 1.0% 2.0% 3.0% 4.0%\nRatio Param.\n16.0\n16.5\n17.0\n17.5\n18.0Rouge-2\nSUM\nVLM\nVLM+Dist.\nGPT-2 Finetune\nLM-Head\n0.0% 1.0% 2.0% 3.0% 4.0%\nRatio Param.\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0BLEU\nNMT\nVLM\nVLM+Dist.\nGPT-2 Finetune\nLM-Head\nFigure 2: Performance comparison among different ra-\ntios of additional parameters for the SUM and NMT\ntasks.\nembedding is not deployed. Indeed, without a task\nembedding the model struggles to learn when the\ninput sequence ends, and how to distinguish the dif-\nferent parts of the input sequence (e.g., attributes\nin NLG, document and question in CoQA etc.).\nFrozen Backbone vs Trainable Backbone As\npreviously mentioned, VLM can be trained either\nby freezing the weight of the GPT-2 model, i.e.,\nindependently and continually learning one task at\na time, or by multitasking all the tasks and thus\nﬁne-tuning both the GPT-2 model and the adapters.\nThe latter model has the advantage of being able\nto transfer knowledge among tasks, as we can ob-\nserve in Table 1 for the CoQA task, where VLM\nMulti-Task improve the F1 score by 3%. On the\nother hand, the frozen back-bone model has the big\nadvantage of learning tasks sequentially, since the\noriginal GPT-2 weights remain untouched.\n5 Conclusion\nIn this paper, we have presented a Versatile Lan-\nguage Model which learns ﬁve diverse natural lan-\nguage generation tasks in a single model. We found\nthat a residual adapter is more effective than ﬁne-\ntuning other parts of the model (e.g., LM-Head),\nand that distillation helps in reducing the gap in\n445\nperformance in hard to ﬁne-tune tasks, such as sum-\nmarization and translation. Finally, we show the\ntrade-off between a frozen and trainable back-bone,\nshowing that the former has a competitive perfor-\nmance, with the advantage of being extendable to\nfuture tasks without full re-training.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu,\nAnirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron\nCourville, and Yoshua Bengio. 2016. An actor-critic\nalgorithm for sequence prediction. arXiv preprint\narXiv:1607.07086.\nAnkur Bapna and Orhan Firat. 2019. Simple, scalable\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 1538–1548.\nPaweł Budzianowski and Ivan Vuli´c. 2019. Hello, it’s\ngpt-2–how can i help you? towards the use of pre-\ntrained language models for task-oriented dialogue\nsystems. arXiv preprint arXiv:1907.05774.\nRich Caruana. 1997. Multitask learning. Machine\nlearning, 28(1):41–75.\nMauro Cettolo, Niehues Jan, St ¨uker Sebastian, Luisa\nBentivogli, Roldano Cattoni, and Marcello Federico.\n2016. The iwslt 2016 evaluation campaign. In In-\nternational Workshop on Spoken Language Transla-\ntion.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language mod-\nels: a simple approach to controlled text generation.\narXiv preprint arXiv:1912.02164.\nMichael Denkowski and Alon Lavie. 2014. Meteor uni-\nversal: Language speciﬁc translation evaluation for\nany target language. In Proceedings of the EACL\n2014 Workshop on Statistical Machine Translation.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\nAlexander Miller, Kurt Shuster, Jack Urbanek,\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\nLowe, et al. 2019. The second conversational\nintelligence challenge (convai2). arXiv preprint\narXiv:1902.00098.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. 2019. Uniﬁed\nlanguage model pre-training for natural language\nunderstanding and generation. arXiv preprint\narXiv:1905.03197.\nOndˇrej Duˇsek, Jekaterina Novikova, and Verena Rieser.\n2019. Evaluating the state-of-the-art of end-to-end\nnatural language generation: The E2E NLG Chal-\nlenge. arXiv preprint arXiv:1901.11528.\nYingruo Fan, Jacqueline CK Lam, and Victor On Kwok\nLi. 2020a. Facial action unit intensity estimation\nvia semantic correspondence learning with dynamic\ngraph convolution. In AAAI, pages 12701–12708.\nYingruo Fan, Victor Li, and Jacqueline CK Lam.\n2020b. Facial expression recognition with deeply-\nsupervised attention network. IEEE Transactions on\nAffective Computing.\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summarization.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4098–4109.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nTianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo\nChen, and Tie-Yan Liu. 2018. Layer-wise coordi-\nnation between encoder and decoder for neural ma-\nchine translation. In Advances in Neural Informa-\ntion Processing Systems, pages 7944–7954.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in neural information\nprocessing systems, pages 1693–1701.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nGeoffrey E Hinton and Richard S Zemel. 1994.\nAutoencoders, minimum description length and\nhelmholtz free energy. In Advances in neural infor-\nmation processing systems, pages 3–10.\n446\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for nlp.\nIn International Conference on Machine Learning ,\npages 2790–2799.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339.\nPo-Sen Huang, Chong Wang, Sitao Huang, Dengy-\nong Zhou, and Li Deng. 2017. Towards neural\nphrase-based machine translation. arXiv preprint\narXiv:1706.05565.\nYoon Kim and Alexander M Rush. 2016. Sequence-\nlevel knowledge distillation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1317–1327.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nChin-Yew Lin and Franz Josef Och. 2004. Auto-\nmatic evaluation of machine translation quality us-\ning longest common subsequence and skip-bigram\nstatistics. In Proceedings of the 42nd Annual Meet-\ning on Association for Computational Linguistics ,\npage 605. Association for Computational Linguis-\ntics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, and\nPascale Fung. 2019. Personalizing dialogue agents\nvia meta-learning. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5454–5459.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nCaglar Gulcehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nrnns and beyond. In Proceedings of The 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage models are unsupervised multitask learners.\nIn ArXiv.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2015. Sequence level train-\ning with recurrent neural networks. arXiv preprint\narXiv:1511.06732.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Advances in Neural In-\nformation Processing Systems, pages 506–516.\nSiva Reddy, Danqi Chen, and Christopher D Manning.\n2019. Coqa: A conversational question answering\nchallenge. Transactions of the Association for Com-\nputational Linguistics, 7:249–266.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725.\nAsa Cooper Stickland and Iain Murray. 2019. Bert and\npals: Projected attention layers for efﬁcient adapta-\ntion in multi-task learning. In International Confer-\nence on Machine Learning, pages 5986–5995.\nSebastian Thrun and Lorien Pratt. 2012. Learning to\nlearn. Springer Science & Business Media.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\n447\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4566–4575.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\nYijun Wang, Yingce Xia, Li Zhao, Jiang Bian, Tao\nQin, Guiquan Liu, and Tie-Yan Liu. 2018. Dual\ntransfer learning for neural machine translation with\nmarginal distribution regularization. In Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019a. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019b. Transfertransfo: A trans-\nfer learning approach for neural network based con-\nversational agents. CoRR, abs/1901.08149.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019c. Transfertransfo: A\ntransfer learning approach for neural network\nbased conversational agents. arXiv preprint\narXiv:1901.08149.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N\nDauphin, and Michael Auli. 2019. Pay less attention\nwith lightweight and dynamic convolutions. arXiv\npreprint arXiv:1901.10430.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nSemih Yavuz, Abhinav Rastogi, Guan-Lin Chao, and\nDilek Hakkani-Tur. 2019. Deepcopy: Grounded\nresponse generation with hierarchical pointer net-\nworks. arXiv preprint arXiv:1908.10731.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 2204–\n2213.\nChunting Zhou, Graham Neubig, and Jiatao Gu.\n2019. Understanding knowledge distillation in non-\nautoregressive machine translation. arXiv preprint\narXiv:1911.02727.\n448\nA Appendices\nA.1 Model details\nFigure 3 illustrates a detailed version of VLM.\nVLM shares a GPT-2 back-bone and for each task,\nthe model looks up a set of task embeddings for\nmodeling different input structures and chooses the\ncorresponding adapter.\nA.2 Experiment details\nIn this section, we will describe the dataset, eval-\nuation metrics, dataset preprocessing and training\ndetails for each task.\nConversational Question Answering (CQA)\nCoQA (Reddy et al., 2019) is a free-form conver-\nsational question answering dataset. The task is to\nanswer the questions in a conversation. Each turn\nin the conversation contains a question, and we\nneed to answer the questions based on conversation\nhistories and documents. We use document, ques-\ntion, and answer segment embedding to help the\nmodel to distinguish the document and alternating\nquestions and answers in the input sequence. We\nﬁne-tune the full GPT2-small or VLM (trainable\nadapter with a ﬁxed GPT2-small) for ﬁve epochs\nwith the Adam optimizer. For distillation we only\nﬁne-tune VLM for three epochs. We set the batch\nsize to 16 and limit the maximum length of the\ndocument to 400 tokens and only retain the last\ntwo turns of questions and answers in the dialogue\nhistory. Following Reddy et al. (2019) we use the\nF1 score as evaluation metrics.\nSummarization (SUM) CNN/Daily-Mail is a\nbenchmark (Hermann et al., 2015; Nallapati et al.,\n2016) for text summarization. We use article, sum-\nmary segment embedding to divide the article and\nthe summary. We ﬁne-tune the full GPT2-small\nand VLM for 10 epochs with the Adam optimizer.\nFor distillation, we only ﬁne-tune VLM for ﬁve\nepochs. We set the batch size to 32 and limit the\nmaximum length of the article to 400 tokens and\nthat of the summary to 130 tokens. We use the\nROUGE-1, ROUGE-2, and ROUGE-L scores (Lin,\n2004) as evaluation metrics.\nNeural Machine Translation (NMT) We use\nthe spoken German-English translation dataset\nIWSLT (Cettolo et al., 2016) as our NMT bench-\nmark. We use source, target segment embedding to\ndivide the source language and the target language.\nWe ﬁne-tune the full GPT2-small, VLM and distil-\nlated VLM for 8 epochs with the Adam optimizer.\nWe set the batch size to 32 and limit the maximum\nlength of the source and target sequence to 100\ntokens. We use BLEU (Papineni et al., 2002) as the\nevaluation metric.\nPersona Dialogue (DLG) The Persona-Chat\ndataset (Zhang et al., 2018) is a persona-grounded\nmulti-turn converstion dataset. We use persona,\nsystem, user segment embedding to help the model\nto distinguish the persona, alternating system utter-\nance and user utterance in an input sequence. We\nﬁne-tune the full GPT2-small or VLM for three\nepochs with the Adam optimizer. We set the batch\nsize to 16 and only retain the last ﬁve utterances\nin the dialogue history. We use perplexity, BLEU,\nand Consistency score (Madotto et al., 2019) as\nevaluation metrics.\nNatural Language Generation (NLG) The nat-\nural language generation challenge (Du ˇsek et al.,\n2019) is a dataset for building a response generation\nmodule for task-oriented dialogue systems. Given\na set of response attributes, the model needs to gen-\nerate responses. For example, when the input at-\ntribute is name[The Wrestlers], priceRange[cheap],\ncustomerRating[low], the output should be The\nwrestlers offers competitive prices, but is not highly\nrated by customers. We use a set of attribute seg-\nment embedding to segment the input attributes.\nWe ﬁne-tune the full GPT2-small and VLM for\n10 epochs with the Adam optimizer. We set the\nbatch size to 32 and use BLUE (Papineni et al.,\n2002) , ROUGE (Lin, 2004), NIST (Lin and Och,\n2004), METEOR (Denkowski and Lavie, 2014)\nand CiDER (Vedantam et al., 2015) as evaluation\nmetrics.\nComputational Cost Fine-tuning VLM requires\naround 80%-90% GPU memory compared to full-\nﬁnetune the whole GPT-2 model, as it only updates\nthe small ratio of parameters. And both models\nhave similar training cost, we report the training\nspeed with single GTX 1080 Ti:\nTask Training Speed Training set size\nSUM 7.5h/epoch 300, 000\nNMT 1.6h/epoch 200, 000\nDLG 1.5h/epoch 130, 000\nQA 5.0h/epoch 100, 000\nNLG 0.2h/epoch 42, 000\n449\nA.3 Detailed Results\nIn this section, we report the detailed results for\neach task in Tables 2-6. We use a greedy decoding\nstrategy for all the tasks.\nA.4 Example\n450\nCQA NLG NMT DLGSUM\nAdapter Layers\nAdapter Layers\nSUM DLGNMTNLGCQA\nAdapter Layers\nSUM DLGNMTNLGCQA\nAdapter Layers\nSUM DLGNMTNLGCQA\nAdapter Layers\nSUM DLGNMTNLGCQA\nHero\n1 2 3 4 5+ + + + +\n+ + + + +\n6 7 8 9 10 11 12+ + + + + + +\n+ + + + + + +\nDoc Doc Doc Doc Q1 Q1 Q1 Q1 Q1\nAda is 20 Who is the hero ? Ada How old\nA1 Q2 Q2\nGPT-2 Layer\n14 15 16 17+ + + +\n+ + + +\nQ2 Q2\nshe ? SOS 20\nA2 A2\n13+\n+is\nQ2\n20 EOS\nN×\nWord\nPositional\nTask\nPer Per Per Sys Sys Usr Usr Usr Usr Usr Sys Sys Sys Sys Sys Sys Sys\nI\n1 2 3 4 5+ + + + +\n+ + + + +\n6 7 8 9 10 11 12+ + + + + + +\n+ + + + + + +love cats Hi ! Hi how are you ? SOS I\nGPT-2 Layer\n14 15 16 17+ + + +\n+ + + +ﬁne and you ?\n13+\n+\nam\nN×\nI ﬁne and you ?am EOS\nWord\nPositional\nTask\nName\n1 2 3 4 5+ + + + +\n+ + + + +\n6 7 8 9 10 11 12+ + + + + + +\n+ + + + + + +\nEagleType Ita lian Area cent er SOS The Eagle is\nGPT-2 Layer\n14 15 16 17+ + + +\n+ + + +Ita lian restu rant\n13+\n+\nan\nN×\nAtt1 Att1 Att2 Att2 Att2 Att3 Att3 Att3 Resp Resp Resp Resp Resp Resp Resp Resp Resp\nThe Eagle is Ita lian restu rantan EOS\nWord\nPositional\nTask\nJoe\n1 2 3+ + +\n+ + +\n56 57 58 59 60 61 62+ + + + + + +\n+ + + + + + +and Mary eve nt in NY SOS Mary and\nGPT-2 Layer\n64 65 66+ + +\n+ + +went to NY\n63+\n+Joe\nMary and Joe to NY EOSwent\nArt Art Art Art Art Art Art Sum Sum Sum Sum Sum Sum Sum\n···\nWord\nPositional\nTask\nN×\nIch\n1 2 3 4 5+ + + + +\n+ + + + +\n6 7 8 9 10 11 12+ + + + + + +\n+ + + + + + +lie be sch warzeKat zen ! EOS I love bla\nGPT-2 Layer\n14+\n+cats\n13+\n+ck\nN×\nI love bla cats EOSck\nDe De De De De De De De En En En En En En\nWord\nPositional\nTask\nFigure 3: A detailed version of VLM. VLM shares a GPT-2 back-bone and for each task, the model looks up a set\nof task embeddings and chooses the corresponding adapter.\n451\n0.0% 1.0% 2.0% 3.0% 4.0%\nRatio Param.\n13\n14\n15\n16\n17Perplexity\nPersona-Chat\nVLM\nGPT-2 Finetune\nLM-Head\n0.0% 1.0% 2.0% 3.0% 4.0%\n55.5\n56.0\n56.5\n57.0\n57.5\n58.0Avg. Metric\nNLG\nVLM\nVLM+Dist.\nGPT-2 Finetune\nLM-Head\n0.0% 1.0% 2.0% 3.0% 4.0%\nRatio Param.\n50\n55\n60\n65F1\nCQA\nVLM\nVLM+Dist.\nGPT-2 Finetune\nLM-Head\nFigure 4: Performance comparison among different ratios of additional parameters. Here we can see that knowl-\nedge distillation does not improve the performance of the NLG task because of the small gap between VLM and\nthe full ﬁne-tuned GPT-2. Instead for the dialogue and QA tasks, the gold target is always better than the distillated\ntarget.\n452\nCNN / Daily Mail\nModels ROUGE 1 ROUGE 2 ROUGE L\nGPT Finetune 37.4 18.1 27.7\nw/o Pre-Train 35.5 17 26.2\nVLM mutli-task 36.6 17.7 27\nVLM-10 (+ DIst.) 35.0 (36.2) 16.5 (17.3) 25.0 (25.7)\nVLM-50 (+ DIst.) 36.4 (36.8) 17.5 (17.9) 26.6 (26.8)\nVLM-100 (+ DIst.) 36.5 (37.0) 17.6 (18.0) 27.0 (27.0)\nVLM-300 (+ DIst.) 36.6 (36.7) 17.6 (17.7) 26.6 (26.7)\nPGNet (See et al., 2017) 39.53 17.28 36.38\nBottom-Up (Gehrmann et al., 2018) 41.22 18.68 38.34\nUniLM (Dong et al., 2019) 43.33 20.21 40.51\nT5-11B (Raffel et al., 2019) 43.52 21.55 40.69\nTable 2: Summarization results.\nPersona\nModels Perplexity BLEU Consistency (C)\nGPT Finetune 13.13 2.17 0.71\nw/o Pre-Train 37.77 0.99 0.12\nVLM mutli-task 13.15 0.84 0.27\nVLM-10 15.76 1.63 0.86\nVLM-50 14.54 1.84 0.72\nVLM-100 (+ DIst.) 14.06 (89.34) 1.99 (2.15) 0.76 (0.72)\nVLM-300 13.73 1.98 0.74\nDeep Copy (Yavuz et al., 2019) 54.58 4.09 -\nPAML-TRS (Madotto et al., 2019) 30.42 1.0 0.07\nADAPT Centre (ConvAI2) (Dinan et al., 2019) 29.85 - -\nPersona-Chat (Zhang et al., 2018) 35.07 - -\nTransferTransfero (Wolf et al., 2019c) 17.51 - -\nTable 3: Persona Chat results.\nCoQA\nModels F1\nGPT Finetune 67.7\nw/o Pre-Train 15.1\nVLM mutli-task 69.3\nVLM-50 (+ DIst.) 55.8 (56.2)\nVLM-100 (+ DIst.) 64.3 (62.9)\nVLM-300 (+ DIst.) 66.2 (64.8)\nSeq2Seq (Reddy et al., 2019) 27.5\nPGNet (Reddy et al., 2019) 45.4\nDrQA (Reddy et al., 2019) 54.7\nUNILM (Dong et al., 2019) 82.5\nHuman (Reddy et al., 2019) 89.8\nTable 4: CoQA results.\n453\nNMT\nModels BLUE\nGPT Finetune 25.45\nw/o Pre-Train 16.52\nVLM mutli-task 22.49\nVLM-10 (+ DIst.) 6.27(12.57)\nVLM-50 (+ DIst.) 14.79(20.09)\nVLM-100 (+ DIst.) 19.89(22.39)\nVLM-300 (+ DIst.) 23.77(24.19)\nTransformer (Vaswani et al., 2017) 29.2\nDynamicConv (Wu et al., 2019) 35\nMIXER (Ranzato et al., 2015) 21.83\nAC+LL (Bahdanau et al., 2016) 28.53\nNPMT (Huang et al., 2017) 28.96\nDual Transfer Learning (Wang et al., 2018) 32.35\nLYC Transforemer (He et al., 2018) 35.07\nTable 5: NMT results.\nNLG\nModels BLEU NIST/10 METEOR ROUGE L CIDEr/10 norm. avg.\nGPT Finetune 66.44 0.85279 0.4548 0.6911 0.22546 57.771\nw/o Pre-Train 60.54 0.81697 0.4152 0.6471 0.19086 53.5106\nVLM mutli-task 65.63 0.8342 0.4525 0.6889 0.22213 57.0806\nVLM-10 67.1 0.85046 0.4545 0.6935 0.229 57.9692\nVLM-50 66.01 0.84124 0.4568 0.6876 0.22128 57.3404\nVLM-100 65.38 0.83922 0.4564 0.6893 0.21972 57.1688\nVLM-300 66.18 0.84876 0.4539 0.6897 0.22387 57.5606\nVLM-10 + DIst. 65.03 0.83199 0.456 0.6849 0.21286 56.721\nVLM-50 + DIst. 65.23 0.83326 0.4576 0.6866 0.21287 56.8526\nVLM-100 + DIst. 64.35 0.82485 0.4584 0.6852 0.20989 56.4368\nVLM-300 + DIst. 65.19 0.83481 0.4575 0.6878 0.21182 56.8766\nTGEN baseline (Duˇsek et al., 2019) 65.93 0.86094 0.4483 0.685 0.22338 57.5384\nSLUG (Duˇsek et al., 2019) 66.19 0.8613 0.4454 0.6772 0.22615 57.44\nTable 6: NLG results.\n454\nNMT IWSLT 2014\nSource\nWenn ihr mit jemanden in den 20ern arbeitet,\neinen liebt, wegen einem in den 20ern Schlaf\nverliert, ich m¨ochte euch seh en... O.k. Großartig.\nLeute in den 20ern sind wirklich wichtig.\nGPT-2 Finetune\nIf you work with somebody in the ’20s, you love\nthem because you lost a loved one in the ’20s,\nI want to see you – – great. People in the ’20s\nare really important.\nVLM\nIf you work with somebody in the ’20s, because\nof a love lost in the ’20s, I want to see you – OK.\nGreat. People in the ’20s are really important.\nLM-Head\nIf you work with someone in the 20ern, you love,\nyou love, you love, you love, you love, you love,\nyou love, you love, you love, you love, you love,\nyou love, you love, you love, you love, you love,\nyou love, you love, you love, you love, you love,\nyou love, you love, you love, you love, you love,\nyou love, you love, you love, you love, you love,\nyou love, you love, you love, you love, you love,\nyou love,\nTarget\nIf you work with twentysomethings, you love a\ntwentysomething, you’re losing sleep over\ntwentysomethings, I want to see — Okay. Awesome,\ntwentysomethings really matter.\nSource\nJa, die Leute lassen sich sp¨ater h¨auslich nieder als\nfr¨uher, aber das machte Alex’ 20er nicht zum\nEntwicklungsausfall.\nGPT-2 Finetune\nYes, people will be more domestic in the future\nthan they used to be, but that didn’t make Alex’\n20s for failure.\nVLM Yes, people would come up later than they used\nto, but that didn’t make Alex’ 20s a disaster.\nLM-Head Yes, people are later going to come back as former\nformer, but that doesn’t make Alex’ 20s anymore.\nTarget\nYes, people settle down later than they used to,\nbut that didn’t make Alex’s 20s a developmental\ndowntime.\nSource Leute in den 20ern wie Alex und ich hatten nichts als Zeit.\nGPT-2 Finetune People in the ’20s like Alex and I didn’t have time for time.\nVLM People in the ’20s like Alex and I had nothing but time.\nLM-Head People like Alex and I had nothing as a time.\nTarget Twentysomethings like Alex and I had nothing but time.\nTable 7: NMT examples.\n455\nSummurization CNN Daily Mail\nSource\nlos angeles -lrb- cnn -rrb- it’s more than just one state’s internal problem.\nthe historic california drought hurts the,rest of the union, too. that’s\nbecause california is a breadbasket to the nation, growing more\nthan a third of its vegetables and nearly two-thirds of its fruits and nuts.\nhere’s why we should heed the ongoing drought in the most populous\nstate, a slowly expanding natural disaster now in its fourth year that\nthis week prompted gov. jerry brown to announce a,mandatory 25 % ....\nGPT-2 Finetune\nthe drought hurts the rest of the union, too, because california is a\nbreadbasket to the nation, the drought is, causing the nation’s price\nto rise, thanks to a strong dollar.\nVLM\ncalifornia drought hurts rest of the union, too, says jerry brown.\nbrown: prices rose last year for these items on your kitchen table. he\nsays prices expected to rise this year because of inﬂation.\nLM-Head\nnew : the drought has hurt the rest of the union, the drought has hurt\nthe rest of the union, too, the drought has hurt the rest of the union,\ntoo, the drought has hurt the rest of the union, too.\nTarget\namericans paid more for some fruits and vegetables last year because\nof the drought, tourists will now have to ask for a glass of water at a\ncalifornia restaurant, perhaps the only good thing is another “ great ”\nwine grape harvest last year.\nSource\n-lrb- cnn -rrb- governments around the world are using the threat of terrorism\n– real or perceived – to advance executions, amnesty international alleges\nin its annual report on the death penalty. “ the dark trend of governments using\nthe death penalty in a futile attempt to tackle real or imaginary threats to state\nsecurity and public safety was stark last year, ” said salil shetty, amnesty’s\nsecretary general in a release. “ it is shameful that so many states around the\nworld are essentially playing with people’s lives – putting people to death for\n‘ terrorism’or to quell internal instability on the ill-conceived premise of deterrence.\nGPT-2 Finetune\nreport: governments are using the threat of terrorism to advance executions,\nchina is mentioned as having used, the death penalty as a tool in its “ strike hard ”\ncampaign., the number of executions worldwide has gone down by almost\n22 % on the previous year.\nVLM\namnesty international releases its annual report on the death penalty, the report\ncites the example of pakistan, lifting a six-year moratorium on the execution\nof civilians.\nLM-Head\namnesty international : death penalty is a tool in state-sanctioned killing\ncampaigns, china is also mentioned, as having used the death penalty as a\ntool in its “ strike hard ” campaign against terrorism.,china is also mentioned,\nas having used the death penalty as a tool in its “ strike hard ” campaign\nagainst terrorism\nTarget\namnesty’s annual death penalty report catalogs encouraging signs, but setbacks\nin numbers of those sentenced to death.,organization claims that governments\naround the world are using the threat of terrorism to advance executions.,\nthe number of executions worldwide has gone down by almost 22 % compared\nwith 2013, but death sentences up by 28 %.\nTable 8: SUM examples.\n456\nConversational QA (CoQA)\nSource\n(CNN) – Dennis Farina, the dapper, mustachioed cop turned-actor best\nknown for his tough-as-nails work in such TV series as ”Law Order,”\n”Crime Story,” and ”Miami Vice,” has died. He was 69.\n”We are deeply saddened by the loss,of a great actor and a wonderful man,”\nsaid his publicist, Lori De Waal, in a statement Monday. ”\nDennis Farina was always warmhearted and professional, with a great\nsense of humor and passion for his profession. He will be greatly missed\nby his family, friends and colleagues.” Farina, who had a long career\nas a police ofﬁcer in Chicago, got into acting through director Michael Mann,\nwho used him as a consultant and cast him in his 1981 movie,”Thief.”\nThat role led to others in such Mann-created shows as ”Miami Vice”\n(in which Farina played a mobster) and ”Crime Story” (in which he\nstarred as Lt. Mike Torello). Farina also had roles, generally as\neither cops or gangsters, in a number of movies, including ”Midnight\nRun” (1988), ”Get Shorty” (1995), ”The Mod Squad” (1999) and\n”Snatch” (2000). In 2004, he joined the cast of the long-running\n”Law Order” after Jerry Orbach’s departure, playing Detective\nJoe Fontana, a role he reprised on the spinoff ”Trial by Jury.”\nFontana was known for ﬂashy clothes and an expensive car, a distinct\ncounterpoint to Orbach’s rumpled Lennie Briscoe. Farina was on ”Law Order”\nfor two years, partnered with Jesse L. Martin’s Ed Green.\nMartin’s character became a senior detective after Farina left the show.\nQ1: Is someone in showbiz?\nA1: yes\nQ2: Whom?\nGPT-2 Finetune Dennis Farina\nVLM Dennis Farina\nLM-Head Michael Mann\nTarget Dennis Farina\nTable 9: CQA examples.\n457\nConversational QA (CoQA)\nSource\nDocument:\nOnce upon a time, in a barn near a farm house, there\nlived a little white kitten named Cotton. Cotton lived high up in\na nice warm place above the barn where all of the farmer’s horses\nslept. But Cotton wasn’t alone in her little,home above the barn,\noh no. She shared her hay bed with her mommy and 5 other sisters.\nAll of her sisters were cute and,ﬂuffy, like Cotton. But she was the\nonly white one in the bunch. The rest of her sisters were all orange\nwith beautiful white tiger stripes like Cotton’s mommy. Being\ndifferent made Cotton quite sad. She often wished she looked like\nthe rest of her family. So one day, when Cotton found a can of the\nold farmer’s orange paint, she used it to paint herself like them.\nWhen her mommy and sisters found her they started laughing.\n”What are you doing, Cotton?!” ”I only wanted to be,more like you”.\nCotton’s mommy rubbed her face on Cotton’s and said ”Oh Cotton,\nbut your fur is so pretty and special, like you. We would never want\nyou to be any other way”. And with that, Cotton’s mommy picked\nher up and dropped her into,a big bucket of water. When Cotton\ncame out she was herself again. Her sisters licked her face until\nCotton’s fur was all all dry. ”Don’t ever do that again, Cotton!”\nthey all cried. ”Next time you might mess up that pretty white fur\nof yours and we wouldn’t want that!” Then Cotton thought,\n”I change my mind. I like being special”.\nQ1: Whose paint was it?\nA1: the farmer\nQ2: What did Cotton’s mother and siblings do when they saw\nher painted orange?\nGPT-2 Finetune laughed\nVLM laughed\nLM-Head They put her in a big bucket of water\nTarget laughed\nTable 10: CQA examples.\n458\nDialogue Persona-Chat\nSource\nPersona:\ni read twenty books a year\ni’m a stunt double as my second job.\ni only eat kosher.\ni was raised in, a single parent household.\nContext:\nUSR: hello what are doing today?\nSYS: i am good, i just got off work and tired, i have two jobs.\nUSR: i just got done watching a horror movie\nGPT-2 Finetune i love horror movies, i’m a stunt double.\nVLM i love horror movies, i read twenty books a year.\nLM-Head i love horror movies, i love them.\nTarget i rather read, i’ve read about 20 books this year.\nSource\nPersona:\ni’m a little girl.\ni’ve superpowers.\ni like to make time stop.\ni live in the cloud.\nContext:\nUSR: hi i dye my hair 4 times a year\nGPT-2 Finetune that is cool. i like to dye my hair.\nVLM wow that’s a lot of dyeing. i love to dye.\nLM-Head i love to make time stop\nTarget do you live in the sky? i do. in the clouds.\nTable 11: DLG examples.\n459\nNatural Language Generation NLG\nSource\nname = Blue Spice\neatType = coffee shop\nnear = Crowne Plaza Hotel\ncustomer rating = 5 out of 5\nGPT-2 Finetune Blue Spice is a coffee shop near Crowne Plaza Hotel with a\n5 out of 5 customer rating.\nVLM Blue Spice is a coffee shop near Crowne Plaza Hotel. It has\na customer rating of 5 out of 5.\nLM-Head Blue Spice is a coffee shop near Crowne Plaza Hotel with\na customer rating of 5 out of 5.\nTarget\n- The coffee shop Blue Spice is based near Crowne Plaza Hotel and\nhas a high customer rating of 5 out of 5.\n- The Blue Spice coffee shop, near Crowne Plaza Hotel, has a\ncustomer rating of 5 out of 5.\n- If you want a coffee shop rated 5 out of 5 pick Blue Spice. It is\nlocated near Crowne Plaza Hotel.\nSource\nname = Giraffe\neatType = pub\nfamilyFriendly = yes\nfood = English\nnear = Rainbow Vegetarian Caf ´e’\narea = riverside\nGPT-2 Finetune\nGiraffe is a family friendly pub located near Rainbow Vegetarian Caf ´e.\nIt serves English food and is located in t\nhe riverside area\nVLM Giraffe is a family friendly pub located near Rainbow Vegetarian Caf ´e\nin the riverside area.\nLM-Head Giraffe is a pub near Rainbow Vegetarian Caf ´e that serves English food.\nIt is family friendly and located in the,riverside area.\nTarget\n- The Giraffe,,a child-friendly, English pub, is,located in riverside area,\nnear Rainbow Vegetarian Caf ´e.\n- Giraffe is a pub that is children friendly and,provides English food.It is\nlocated on the riverside near Rainbow Vegetarian Caf ´e.\n- There is an English based pub called Giraffe. It is located in the riverside\narea near the Rainbow Vegetarian Caf ´e and, yes, it is kid friendly.\nTable 12: NLG examples.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.84795081615448
    },
    {
      "name": "Language model",
      "score": 0.7424941062927246
    },
    {
      "name": "Task (project management)",
      "score": 0.7144336700439453
    },
    {
      "name": "Generative grammar",
      "score": 0.6595462560653687
    },
    {
      "name": "Generative model",
      "score": 0.6250291466712952
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5368998050689697
    },
    {
      "name": "Ideal (ethics)",
      "score": 0.5051923394203186
    },
    {
      "name": "Transfer of learning",
      "score": 0.48519042134284973
    },
    {
      "name": "Transfer (computing)",
      "score": 0.46522092819213867
    },
    {
      "name": "Machine learning",
      "score": 0.34358325600624084
    },
    {
      "name": "Natural language processing",
      "score": 0.3291429877281189
    },
    {
      "name": "Parallel computing",
      "score": 0.08742949366569519
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}