{
  "title": "Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation",
  "url": "https://openalex.org/W4226052928",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2140085755",
      "name": "Shizhe Chen",
      "affiliations": [
        "Universit√© Paris Sciences et Lettres",
        "√âcole Normale Sup√©rieure"
      ]
    },
    {
      "id": "https://openalex.org/A4227793068",
      "name": "Pierre-Louis Guhur",
      "affiliations": [
        "Universit√© Paris Sciences et Lettres",
        "√âcole Normale Sup√©rieure"
      ]
    },
    {
      "id": "https://openalex.org/A2098096191",
      "name": "Makarand Tapaswi",
      "affiliations": [
        "Indian Institute of Technology Hyderabad"
      ]
    },
    {
      "id": "https://openalex.org/A2111851554",
      "name": "Cordelia Schmid",
      "affiliations": [
        "Universit√© Paris Sciences et Lettres",
        "√âcole Normale Sup√©rieure"
      ]
    },
    {
      "id": "https://openalex.org/A2170872680",
      "name": "Ivan Laptev",
      "affiliations": [
        "√âcole Normale Sup√©rieure",
        "Universit√© Paris Sciences et Lettres"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962789679",
    "https://openalex.org/W6748848655",
    "https://openalex.org/W2509493982",
    "https://openalex.org/W2030021468",
    "https://openalex.org/W2336416123",
    "https://openalex.org/W3195026654",
    "https://openalex.org/W6765108809",
    "https://openalex.org/W6774815639",
    "https://openalex.org/W2593841437",
    "https://openalex.org/W6746700414",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2987914945",
    "https://openalex.org/W6776907606",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W6753516098",
    "https://openalex.org/W3174887918",
    "https://openalex.org/W6746518932",
    "https://openalex.org/W2951973805",
    "https://openalex.org/W2962744691",
    "https://openalex.org/W2964487155",
    "https://openalex.org/W6774605722",
    "https://openalex.org/W3003391301",
    "https://openalex.org/W3035232877",
    "https://openalex.org/W6770783600",
    "https://openalex.org/W6770775884",
    "https://openalex.org/W2964339842",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6630875275",
    "https://openalex.org/W6784287907",
    "https://openalex.org/W6751885507",
    "https://openalex.org/W6757724268",
    "https://openalex.org/W3176974620",
    "https://openalex.org/W2964935470",
    "https://openalex.org/W2926977875",
    "https://openalex.org/W3172675210",
    "https://openalex.org/W6803039797",
    "https://openalex.org/W4214700710",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6778299498",
    "https://openalex.org/W6780451406",
    "https://openalex.org/W3100923070",
    "https://openalex.org/W2979727876",
    "https://openalex.org/W3034758614",
    "https://openalex.org/W6774959054",
    "https://openalex.org/W3192009892",
    "https://openalex.org/W3034578524",
    "https://openalex.org/W3176495323",
    "https://openalex.org/W3205676116",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W3109380382",
    "https://openalex.org/W6766904570",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W6640174482",
    "https://openalex.org/W6621543089",
    "https://openalex.org/W6692846177",
    "https://openalex.org/W2954003188",
    "https://openalex.org/W4288283492",
    "https://openalex.org/W41554520",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964043796",
    "https://openalex.org/W2772545238",
    "https://openalex.org/W2909303996",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W1931877416",
    "https://openalex.org/W648786980",
    "https://openalex.org/W3034500398",
    "https://openalex.org/W4287282775",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W3105274166",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287256269",
    "https://openalex.org/W2950697717",
    "https://openalex.org/W3109085430",
    "https://openalex.org/W3109097593",
    "https://openalex.org/W3098358101",
    "https://openalex.org/W4294225490",
    "https://openalex.org/W2991186560",
    "https://openalex.org/W3034728521",
    "https://openalex.org/W3214642103",
    "https://openalex.org/W4226163860",
    "https://openalex.org/W3011144238",
    "https://openalex.org/W2805984364",
    "https://openalex.org/W2884565639"
  ],
  "abstract": "Following language instructions to navigate in unseen environments is a challenging problem for autonomous embodied agents. The agent not only needs to ground languages in visual scenes, but also should explore the environment to reach its target. In this work, we propose a dual-scale graph transformer (DUET) for joint long-term action planning and fine-grained cross-modal understanding. We build a topological map on-the-fly to enable efficient exploration in global action space. To balance the complexity of large action space reasoning and fine-grained language grounding, we dynamically combine a fine-scale encoding over local observations and a coarse-scale encoding on a global map via graph transformers. The proposed approach, DUET, significantly outperforms state-of-the-art methods on goal-oriented vision-and-language navigation (VLN) benchmarks REVERIE and SOON. It also improves the success rate on the fine-grained VLN benchmark R2R.",
  "full_text": "Think Global, Act Local: Dual-scale Graph Transformer for\nVision-and-Language Navigation\nShizhe Chen‚Ä†, Pierre-Louis Guhur‚Ä†, Makarand Tapaswi‚Ä°, Cordelia Schmid‚Ä† and Ivan Laptev‚Ä†\n‚Ä†Inria, ¬¥Ecole normale sup¬¥erieure, CNRS, PSL Research University ‚Ä°IIIT Hyderabad\nhttps://cshizhe.github.io/projects/vln_duet.html\nEnvironment\nObservations\nTopologicalMapping Global Action PlanningInstruction: ‚Äúgo into the \nliving room and water the \nplant on the table.‚Äù\nhShortest Route Planning\nNext\nLocationLocal Actions\nPanorama\nEncoding\nGraph\nUpdate\nInstruction Dynamic\nFusion\nstep ùë°+1: panorama + GPS location\nCoarse-scale\nEncoding\nFine-scale\nEncoding\na\nb\nc\nd\ne\nf\nh\ni\ngeh\nstep ùë°: panorama + GPS location\nmap ùë°-1 map ùë°\ng\nh\nj\ngg\nFigure 1. An agent is required to navigate in unseen environments to reach target locations according to language instructions. It only\nobtains local observations of the environment and is allowed to make local actions, i.e., moving to neighboring locations. In this work,\nwe propose to build topological maps on-the-Ô¨Çy to enable long-term action planning. The map contains visited nodes\n and navigable\nnodes\n that can be reached from the previously visited nodes. Our method predicts global actions, i.e., all navigable nodes in the map,\nand trades off complexity by combining a coarse-scale graph encoding with a Ô¨Åne-scale encoding\n of observations at the current node\n .\nAbstract\nFollowing language instructions to navigate in unseen\nenvironments is a challenging problem for autonomous em-\nbodied agents. The agent not only needs to ground lan-\nguages in visual scenes, but also should explore the envi-\nronment to reach its target. In this work, we propose a\ndual-scale graph transformer (DUET) for joint long-term\naction planning and Ô¨Åne-grained cross-modal understand-\ning. We build a topological map on-the-Ô¨Çy to enable ef-\nÔ¨Åcient exploration in global action space. To balance the\ncomplexity of large action space reasoning and Ô¨Åne-grained\nlanguage grounding, we dynamically combine a Ô¨Åne-scale\nencoding over local observations and a coarse-scale encod-\ning on a global map via graph transformers. The proposed\napproach, DUET, signiÔ¨Åcantly outperforms state-of-the-art\nmethods on goal-oriented vision-and-language navigation\n(VLN) benchmarks REVERIE and SOON. It also improves\nthe success rate on the Ô¨Åne-grained VLN benchmark R2R.\n1. Introduction\nAutonomous navigation is an essential ability for intel-\nligent embodied agents. Given the convenience of natu-\nral language for human-machine interaction, autonomous\nagents should also be able to understand and act accord-\ning to human instructions. Towards this goal, Vision-and-\nLanguage Navigation (VLN) [1] is a challenging problem\nthat has attracted a lot of recent research [2‚Äì9]. VLN re-\nquires an agent to follow language instructions and to navi-\ngate in unseen environments to reach a target location. Ini-\ntial approaches to VLN [2‚Äì4] use Ô¨Åne-grained instructions\nproviding step-by-step navigation guidance such as ‚Äú Walk\nout of the bedroom. Turn right and walk down the hallway.\nAt the end of the hallway turn left. Walk in front of the couch\nand stop‚Äù. This Ô¨Åne-grained VLN task enables grounding of\ndetailed instructions but is less practical due to the need of\nstep-by-step guidance. A more convenient interaction with\nagents can be achieved by goal-oriented instructions [7, 8]\nsuch as ‚Äú Go into the living room and water the plant on\nthe table‚Äù. This task, however, is more challenging as it re-\nquires both the grounding of rooms and objects as well as\nthe efÔ¨Åcient exploration of environments to reach the target.\nIn order to efÔ¨Åciently explore new areas, or correct pre-\nvious decisions, an agent should keep track of already\nexecuted instructions and visited locations in its mem-\nory. Many existing VLN approaches [2, 10‚Äì14] implement\n1\narXiv:2202.11742v1  [cs.CV]  23 Feb 2022\nmemory using recurrent architectures, e.g. LSTM, and con-\ndense navigation history in a Ô¨Åxed-size vector. Arguably,\nsuch an implicit memory mechanism can be inefÔ¨Åcient to\nstore and utilize previous experience with a rich space-\ntime structure. A few recent approaches [15, 16] propose\nto explicitly store previous observations and actions, and\nto model long-range dependencies for action prediction via\ntransformers [17]. However, these models only allow for\nlocal actions, i.e., moving to neighboring locations. As a\nresult, an agent has to run its navigation model N times to\nbacktrack Nsteps, which increases instability and compute.\nA potential solution is to build a map [18] that explicitly\nkeeps track of all visited and navigable locations observed\nso far. The map allows an agent to make efÔ¨Åcient long-\nterm navigation plans. For example, the agent is able to\nselect a long-term goal from all navigable locations in the\nmap, and then uses the map to calculate a shortest path to\nthe goal. Topological maps have been explored by previous\nVLN works [8, 19, 20]. These methods, however, still fall\nshort in two aspects. Firstly, they rely on recurrent architec-\ntures to track the navigation state as shown in the middle of\nFigure 2, which can greatly hinder the long-term reasoning\nability for exploration. Secondly, each node in topologi-\ncal maps is typically represented by condensed visual fea-\ntures. Such coarse representations reduce complexity but\nmay lack details to ground Ô¨Åne-grained object and scene\ndescriptions in instructions.\nOur approach addresses both of these shortcomings, the\nÔ¨Årst one based on a transformer architecture and the second\none with a dual-scale action planning approach. We propose\na Dual-scale graph Transformer (DUET) with topological\nmaps. As illustrated in Figure 1, our model consists of\ntwo modules: topological mapping and global action plan-\nning. In topological mapping, we construct a topological\nmap over time by adding newly observed locations to the\nmap and updating visual representations of nodes. Then at\neach step, the global action planning module predicts a next\nlocation in the map or a stop action. To balance Ô¨Åne-grained\nlanguage grounding and reasoning over large graphs, we\npropose to dynamically fuse action predictions from dual\nscales: a Ô¨Åne-scale representation of the current location\nand a coarse-scale representation of the map. In particu-\nlar, we use transformers to capture cross-modal vision-and-\nlanguage relations, and improve the map encoding by in-\ntroducing the knowledge of graph topology into transform-\ners. We pretrain the model with behavior cloning and aux-\niliary tasks, and propose a pseudo interactive demonstra-\ntor to further improve policy learning. DUET signiÔ¨Åcantly\noutperforms state-of-the-art methods on goal-oriented VLN\nbenchmarks REVERIE and SOON. It also improves success\nrate on Ô¨Åne-grained VLN benchmark R2R. In summary, the\ncontributions of our work are three-fold:\n‚Ä¢ We propose a dual-scale graph transformer (DUET)\nHAMT\nVisited locationsCurrent location Navigable locations\nGraph-basedDUET\n(Ours)\nVisual Memory Action Space\nR Recurrent \nstate\nAction\nNavigation \nMemory\nFine-grained\nrepresentation\nR\n[15][8, 19, 20]\nFigure 2. Method comparison. HAMT [15] stores navigation and\nvisual memories to capture long-range dependency in action pre-\ndiction, but is limited to a local action space. Graph-based ap-\nproaches [8, 19, 20] use topological maps to support a global ac-\ntion space, but suffer from a recurrent navigation memory and a\ncoarse-scale visual representation. Our DUET model overcomes\nprevious limitations with a dual-scale encoding over the map.\nwith topological maps for VLN. It combines coarse-\nscale map encoding and Ô¨Åne-scale encoding of the cur-\nrent location for efÔ¨Åcient planning of global actions.\n‚Ä¢ We employ graph transformers to encode the topolog-\nical map and to learn cross-modal relations with the\ninstruction, so that action prediction can rely on a long-\nrange navigation memory.\n‚Ä¢ DUET achieves state of the art on goal-oriented VLN\nbenchmarks, with more than 20% improvement on\nsuccess rate (SR) on the challenging REVERIE and\nSOON datasets. It also generalizes to Ô¨Åne-grained\nVLN task, i.e., increasing SR on R2R dataset by 4%.\n2. Related work\nVision-and-language navigation (VLN). Navigation tasks\ninvolving instruction following [2‚Äì6, 9, 21‚Äì23] have be-\ncome increasingly popular. Initial VLN methods mainly\nadopt recurrent neural networks with cross-modal atten-\ntion [2, 10, 13, 24, 25]. More recently, transformer-based ar-\nchitectures have been shown successful in VLN tasks [26],\nnotably by leveraging pre-trained architectures. For exam-\nple, PRESS [27] adopts BERT [28] for instruction encod-\ning. Different variants of ViLBERT are used in [29, 30]\nto measure compatibility between instructions and visual\npaths, but cannot be used for sequential action prediction.\nRecurrent VLN-BERT [14] addresses the limitation by in-\njecting a recurrent unit in transformer architectures for ac-\ntion prediction. Instead of relying on one recurrent state,\n2\nE.T. [16] and HAMT [15] directly use transformers to cap-\nture long-range dependency to all past observations and ac-\ntions (see Ô¨Årst row in Figure 2).\nMaps for navigation. The work on visual navigation has\na long tradition of using SLAM [31] to construct metric\nmaps [32] of the environment, using non-parametric meth-\nods [33], neural networks [34,35], or a mixture of both [36].\nAnderson et al . [37] employ such metric maps for VLN\ntasks. However, it is challenging and requires accurate de-\ntermination to construct metric map in real-time naviga-\ntion. Therefore, several works [38, 39] propose to represent\nthe map as topological structures for pre-exploring environ-\nments [40], or for back-tracking to other locations, trading-\noff navigation accuracy with the path length [10, 24]. A\nfew recent VLN works [8, 19, 20] used topological maps\nto support global action planning, but they suffer from us-\ning recurrent architectures for state tracking and also lack a\nÔ¨Åne-scale representation for language grounding as shown\nin Figure 2. We address the above limitations via a dual-\nscale graph transformer with topological maps.\nTraining algorithms for sequential prediction. Behav-\nior cloning is the most widely used training algorithm for\nsequential prediction. Nevertheless, it suffers from distri-\nbution shifts between training and testing. To address the\nlimitation, different training algorithms have been proposed\nsuch as scheduled sampling [41], DAgger [42], reinforce-\nment learning (RL) [43]. Most VLN works [13, 14] com-\nbine behavior cloning and A3C RL [44]. Wang et al. [45]\npropose to learn rewards via soft expert distillation. Due to\nthe difÔ¨Åculty of using RL in tasks with sparse rewards, we\ninstead use an interactive demonstrator to mimic an expert\nand provide supervision in sequential training.\n3. Method\nProblem formulation. In the standard VLN setup for dis-\ncrete environments [2, 7, 8], the environment is an undi-\nrected graph G = {V,E}, where V = {Vi}K\ni=1 denotes\nK navigable nodes, and Edenotes connectivity edges. An\nagent is equipped with an RGB camera and a GPS sensor,\nand is initialized at a starting node in a previously unseen\nenvironment. The goal of the agent is to interpret natural\nlanguage instructions and to traverse the graph to the tar-\nget location and Ô¨Ånd the object speciÔ¨Åed by the instruction.\nW= {wi}L\ni=1 are word embeddings of the instruction with\nLwords. At each time stept, the agent receives a panoramic\nview and position coordinates of its current node Vt. The\npanorama is split into n images Rt = {ri}n\ni=1, each rep-\nresented by an image feature vector ri and a unique orien-\ntation. To enable Ô¨Åne-grained visual perception, m object\nfeatures Ot = {oi}m\ni=1 are extracted in the panorama using\nannotated object bounding boxes or automatic object detec-\ntors [46]. In addition, the agent is aware of a few naviga-\na\nc\nb\nd e\nf\ng\na\nc\nb\nd e\n\"!\"# \"!\nd\ne\nf\ng\nNew Observation\nd eAction\nFigure 3. Illustration of graph updating at time step t. Given a\nnew action d ‚Üíe, an agent receives new observations at node e.\nIt then adds new nodes and updates node representations.\nble views corresponding to its neighboring nodes N(Vt) as\nwell as their coordinates. The navigable views ofN(Vt) are\na subset of Rt. The possible local action space At at step t\ncontains navigating to Vi ‚ààN(Vt) and stopping at Vt. Af-\nter the agent decides to stop at a location, it needs to predict\nthe location of the target object in the panorama.\nExploration and language grounding are two essential\nabilities for VLN agents. However, existing works ei-\nther only allow for local actions At [13‚Äì15] which hinders\nlong-range action planning, or lack object representations\nOt [8, 19, 20] which might be insufÔ¨Åcient for Ô¨Åne-grained\ngrounding. Our work addresses both issues with a dual-\nscale representation and global action planning.\nOverview. As illustrated in Figure 1, our model consists\nof two learnable modules, namely topological mapping and\nglobal action planning. The topological mapping module\ngradually constructs a topological map over time. The\nglobal action planning module then performs dual-scale\nreasoning based on coarse-scale global observations and\nÔ¨Åne-scale local observations. In the following, we introduce\ntopological mapping in Sec. 3.1 and global action planning\nin Sec. 3.2. We end this section by presenting our approach\nto train our model and use it for inference in Sec. 3.3.\n3.1. Topological Mapping\nThe environment graph G is initially unknown to the\nagent, hence, our model gradually builds its own map us-\ning observations along the path. Let Gt = {Vt,Et}with Kt\nnodes, Gt ‚äÇG be the map of the environment observed after\ntnavigation steps. There are three types of nodes in Vt (see\nFigure 1): (i) visited nodes\n ; (ii) navigable nodes\n ; and\n(iii) the current node\n . The agent has access to panoramic\nviews for visited nodes and the current node. Navigable\nnodes are unexplored and are only partially observed from\nalready visited locations, hence, they have different visual\nrepresentations. At each step t, we add the current node Vt\nand its neighboring unvisited nodesN(Vt) to Vt‚àí1, and up-\ndate Et‚àí1 accordingly as illustrated in Figure 3. Given the\nnew observation atVt, we also update visual representations\nof the current node and navigable nodes as follows.\nVisual representations for nodes. At time step t, the agent\n3\nreceives image features Rt and object features Ot of node\nVt. We use a multi-layer transformer [17] to model spatial\nrelations among images and objects. The core of the trans-\nformer is the self-attention block:\n[R‚Ä≤\nt,O‚Ä≤\nt] =SelfAttn ([Rt,Ot]) , (1)\nSelfAttn(X) =Softmax\n(XWq(XWk)T\n‚àö\nd\n)\nXWv, (2)\nwhere W‚àó ‚ààRd√ód are parameters and biases are omitted.\nFor ease of notation, we still use Rt,Ot in the following\ninstead of R‚Ä≤\nt,O‚Ä≤\nt to denote the encoded embeddings.\nThen we update visual representation of the current\nnode\n by average pooling ofRt and Ot. As the agent also\npartially observes N(Vt) at Vt, we accumulate visual rep-\nresentations of these navigable nodes\n based on the cor-\nresponding view embedding in Rt. If a navigable node has\nbeen seen from multiple locations, we average all the par-\ntial view embeddings as its visual representation. We usevi\nto denote the pooled visual representation for each node Vi.\nSuch a coarse-scale representation enables efÔ¨Åcient reason-\ning over large graphs, but may not provide sufÔ¨Åcient infor-\nmation for Ô¨Åne-grained language grounding especially for\nobjects. Therefore, we keep Rt,Ot as a Ô¨Åne-grained visual\nrepresentation\n for the current node Vt to support detailed\nreasoning at a Ô¨Åne-scale.\n3.2. Global Action Planning\nFigure 4 illustrates the global action planning module.\nThe coarse-scale encoder makes predictions over all previ-\nously visited nodes, but uses a coarse-scale visual represen-\ntation. The Ô¨Åne-scale encoder instead predicts local actions\ngiven Ô¨Åne-grained visual representations of the current lo-\ncation. The dynamic fusion of both encoders combines pre-\ndictions of global and local actions.\n3.2.1 Text Encoder\nTo each word embedding in Wis added a positional em-\nbedding [28] corresponding to the position of the word\nin the sentence and a type embedding for text [47]. All\nword tokens are then fed into a multi-layer transformer\nto obtain contextual word representations, denoted here as\nÀÜW= {ÀÜw1,¬∑¬∑¬∑ , ÀÜwL}.\n3.2.2 Coarse-scale Cross-modal Encoder\nThe module takes the coarse-scale map Gt and encoded in-\nstruction ÀÜWto make navigation predictions over a global\naction space (‚à™t\ni=1Ai).\nNode embedding. To the node visual feature vi is added a\nlocation encoding and a navigation step encoding. The lo-\ncation encoding embeds the location of a node in the map\nin an egocentric view, which is the orientation and distance\nrelative to the current node. The navigation step encoding\nembeds the latest visited time step for visited nodes and 0\nfor unexplored nodes. In this way, visited nodes are en-\ncoded with a different navigation history to improve align-\nment with the instruction. We add a ‚Äòstop‚Äô node v0 in the\ngraph to denote a stop action and connect it with all other\nnodes.\nGraph-aware cross-modal encoding. The encoded node\nand word embeddings are fed into a multi-layer graph-\naware cross-modal transformer. Each transformer layer\nconsists of a cross-attention layer [47] to model relations\nbetween nodes and instructions, and a graph-aware self-\nattention layer to encode environment layout. The standard\nattention in Eq. (2) only considers visual similarity among\nnodes, and thus it might overlook nearby nodes which are\nmore relevant than distant nodes. To address the problem,\nwe propose the graph-aware self-attention (GASA) which\nfurther takes into account the structure of the graph to com-\npute attention as follows:\nGASA(X) =Softmax\n(XWq(XWk)T\n‚àö\nd\n+ M\n)\nXWv, (3)\nM = EWe + be, (4)\nwhere X denotes node representations, E is the pair-wise\ndistance matrix obtained from Et, and We,be are two learn-\nable parameters. We stack N layers in the encoder and de-\nnote the output embedding of node Vi as ÀÜvi.\nGlobal action prediction. We predict a navigation score\nfor each node Vi in Gt as below:\nsc\ni = FFN(ÀÜvi), (5)\nwhere FFN denotes a two-layer feed-forward network. To\nbe noted, sc\n0 is the stop score. In most VLN tasks, it is not\nnecessary for an agent to revisit a node, and thus we mask\nthe score for visited nodes if not specially mentioned.\n3.2.3 Fine-scale Cross-modal Encoder\nThis part attends to the current location Vt in the map\nto enable Ô¨Åne-scale cross-modal reasoning. The input is\nthe instruction ÀÜWt and Ô¨Åne-grained visual representations\n{Rt,Ot}of the current node. The module predicts navi-\ngation actions in a local action space (At), and grounds the\nobject at the Ô¨Ånal time step.\nVisual Embedding. We add two types of location embed-\ndings to Rt,Ot. The Ô¨Årst type is the current location in the\nmap relative to the start node. This embedding helps un-\nderstand absolute locations in instruction such as ‚Äúgo to the\nliving room in Ô¨Årst Ô¨Çoor ‚Äù. Then for Vi ‚ààN (Vt), we add\na second location embedding, the relative position of each\nneighboring node to the current node. It helps the encoder\nto realize egocentric directions such as ‚Äúturn right‚Äù. A spe-\ncial ‚Äòstop‚Äô tokenr0 is added for stop action.\nFine-grained cross-modal reasoning. We concatenate\n[r0; Rt; Ot] as visual tokens and exploit a standard multi-\n4\nGraph-aware\nSelf-Attention\n‚Ñá!\nText\nCross-\nAttention FFN\n\"\"\nLocal √† Global\nLocal Action\nPrediction\n!!\nMulti-layer\nTransformer ## , ‚ãØ , #$\n‚ãØ\n\"!\n\"\" ‚ãØ\n\"\"‚ãØ\nSelf-\nAttention\nCross-\nAttention FFN\nCoarse-scale Cross-modal Encoder\nFine-scale Cross-modal Encoder\nTopological Mapping\n!#\n!\"\n# !!‚ãØ\n# \"!‚ãØ\n# \"\"\n# $$\nÃÇ \"\"\nÃÇ \"#‚ãØ\nNode\nEmbedding\nText\nEmbedding\nImage\nEmbedding\nDynamic\nFusion\nFFN\n# !#\n# !\"\nObject Prediction\nGlobal Action\nPrediction\nText Encoder\nimagesobjects\nMulti-layer Transformer\n‚ãØ ‚ãØ\n##\n%, ‚ãØ , #$\n%\n&\"\n%, ‚ãØ , &&\n%\n\"#\n$$\n\"#‚ãØ\n\"&\n$#\n$$ ‚ãØ\nPanorama Encoding\nGraph Update\nfull pooling\npartial pooling\n$ nodes\nFigure 4. DUET consists of topological mapping (left) and global action planning (right). The mapping module outputs a graph with K\nnode features {vi}K\ni=1, and the current panorama encoding with image features {ri}n\ni=1 and object features {oi}m\ni=1. Node feature v0 and\nimage feature r0 are used to indicate the ‚Äòstop‚Äô action. The global action planning uses transformers for coarse- and Ô¨Åne-scale cross-modal\nencoding and fuses the two scales to obtain a global action score si for each node.\nlayer cross-modal transformer [47] to model vision and lan-\nguage relations. The output embeddings of visual tokens\nare represented as ÀÜr0, ÀÜRt, ÀÜOt respectively.\nLocal action prediction and object grounding. We pre-\ndict a navigation score sf\ni in local action space At similar\nto Eq. (5). Moreover, as the goal-oriented VLN task re-\nquires object grounding, we further use a FFN to generate\nobject scores based on ÀÜOt.\n3.2.4 Dynamic Fusion\nWe propose to dynamically fuse coarse- and Ô¨Åne-scale ac-\ntion predictions for better global action prediction. How-\never, the Ô¨Åne-scale encoder predicts actions in a local ac-\ntion space which does not match with the coarse-scale en-\ncoder. Therefore, we Ô¨Årst convert local action scores sf\ni ‚àà\n{stop,N(Vt)}into the global action space. In order to nav-\nigate to other unexplored nodes that are not connected with\nthe current node, the agent needs to backtrack through its\nneighboring visited nodes. Therefore, we sum over scores\nof visited nodes in N(Vt) as an overall backtrack score\nsback. We keep the values for sf\ni ‚àà{stop,N(Vt)}and use\nthe constant sback for the others. Hence, the converted global\naction scores are:\nsf‚Ä≤\ni =\n{\nsback, if Vi ‚ààVt ‚àíN(Vt),\nsf\ni, otherwise.\n(6)\nAt each step, we concatenate ÀÜv0 from coarse-scale encoder\nand ÀÜr0 from Ô¨Åne-scale encoder to predict a scalar for fusion:\nœÉt = Sigmoid(FFN([ÀÜv0; ÀÜr0])). (7)\nThe Ô¨Ånal navigation score for Vi is:\nsi = œÉtsc\ni + (1‚àíœÉt)sf‚Ä≤\ni . (8)\n3.3. Training and Inference\nPretraining. As shown in [15,16,26], it is beneÔ¨Åcial to pre-\ntrain transformer-based VLN models with auxiliary tasks as\ninitialization. Therefore, we Ô¨Årst pretrain our model based\non off-line expert demonstrations with behavior cloning\nand other common vision-and-language proxy tasks. We\nuse masked language modeling (MLM) [28], masked re-\ngion classiÔ¨Åcation (MRC) [48], single-step action predic-\ntion (SAP) [15] and object grounding (OG) [49] if object\nannotations are available. The SAP and OG loss in behav-\nior cloning given a demonstration path P‚àóis as follows:\nLSAP =\n‚àëT\nt=1\n‚àílog p(a‚àó\nt|W,P‚àó\n<t) (9)\nLOG = ‚àílog p(o‚àó|W,PT) (10)\nwhere a‚àó\nt is the expert action of a partial demonstration path\nP‚àó\n<t, and o‚àóis the groundtruth object at the last locationPT.\nMore details are presented in the supplementary material.\nPolicy learning via an interactive demonstrator. Behav-\nior cloning suffers from distribution shifts between training\nand testing. Therefore, we propose to further train the pol-\nicy with the supervision from a pseudo interactive demon-\nstrator (PID) œÄ‚àósimilar to the DAgger algorithm [42]. Dur-\ning training we have access to the environment graph G,\nhence œÄ‚àócan utilize Gto select the next target node, i.e., a\nnavigable node with the overall shortest distance from the\ncurrent node and to the Ô¨Ånal destination. In each iteration,\nwe use the current policy to sample a trajectory Pand use\nœÄ‚àóto obtain pseudo supervision:\nLPID =\n‚àëT\nt=1\n‚àílog p(aœÄ‚àó\nt |W,P<t) (11)\n5\nwhere aœÄ‚àó\nt is our pseudo target at step t. We combine the\noriginal expert demonstrations with our pseudo demonstra-\ntions in policy learning with a balance factor Œª:\nL= ŒªLSAP + LPID + LOG. (12)\nInference. At each time step during testing, we update the\ntopological map as introduced in Sec. 3.1 and then predict\na global action as explained in Sec. 3.2. If it is a naviga-\ntion action, the shortest route planning module employs the\nFloyd algorithm to obtain a shortest path from the current\nnode to the predicted node given the map, otherwise the\nagent stops at the current location. The agent is forced to\nstop if it exceeds the maximum action steps. In such case, it\nwill return to a node with maximum stop probability as its\nÔ¨Ånal prediction. At the stopped location, the agent selects\nan object with maximum object prediction score.\n4. Experiments\n4.1. Datasets\nWe focus our evaluation on goal-oriented VLN bench-\nmarks REVERIE [7] and SOON [8], which require Ô¨Åne-\ngrained object grounding and advanced exploration capa-\nbilities to Ô¨Ånd a remote object. We also evaluate our model\non the widely used VLN benchmark R2R [2], which has\nstep-by-step instructions and no object localization.\nREVERIE contains high-level instructions mainly describ-\ning target locations and objects. Instructions contain 21\nwords on average. Given predeÔ¨Åned object bounding boxes\nprovided for each panorama, the agent should select the cor-\nrect object bounding box at the end of the navigation path.\nThe length of expert paths ranges from 4 to 7 steps.\nSOON also provides instructions describing target rooms\nand objects. The average length of instructions is 47 words.\nSOON does not provide object boxes and requires the agent\nto predict object center locations in the panorama. Hence,\nwe use an automatic object detector [46] to obtain candidate\nobject boxes. The length of expert paths ranges from 2 to\n21 steps with 9.5 steps on average.\nR2R contains step-by-step navigation instructions. The av-\nerage length of instructions is 32 words. The average length\nof expert paths is 6 steps.\nExamples from REVERIE and R2R are illustrated in Fig-\nure 5. Further details are in the supplementary material.\n4.2. Evaluation Metrics\nNavigation metrics. We use standard metrics [1] to mea-\nsure navigation performance, i.e., Trajectory Length (TL):\naverage path length in meters; Navigation Error (NE): av-\nerage distance in meters between agent‚Äôs Ô¨Ånal location and\nthe target; Success Rate (SR): the ratio of paths with NE\nTable 1. Comparison of different scales and dual-scale fusion strat-\negy on REVERIE val unseen split.\nscale fusion OSR‚Üë SR‚Üë SR\nOSR ‚Üë SPL‚Üë RGS‚Üë RGSPL‚Üë\nÔ¨Åne - 30.96 28.86 93.22 23.57 20.39 16.64\ncoarse - 46.44 36.52 78.64 25.98 - -\nmulti average 51.86 45.81 88.33 31.94 32.49 22.78\ndynamic 51.07 46.98 91.40 33.73 32.15 23.03\nless than 3 meters; Oracle SR (OSR): SR given oracle stop\npolicy; and SR penalized by Path Length (SPL).\nObject grounding metrics. To evaluate both the naviga-\ntion and object grounding, we follow [7] and adopt Remote\nGrounding Success (RGS): the proportion of successfully\nexecuted instructions. We also use RGS penalized by Path\nLength (RGSPL). All the metrics are the higher the better\nexcept for TL and NE.\n4.3. Implementation Details\nFeatures. For images, we adopt ViT-B/16 [50] pretrained\non ImageNet to extract features. For objects, we use the\nsame ViT on the REVERIE dataset as it provides bounding\nboxes, while we use the BUTD object detector [46] on the\nSOON dataset. The orientation feature [11] contains sin(¬∑)\nand cos(¬∑) values for heading and elevation angles.\nModel architecture. We use 9, 2, 4 and 4 transformer\nlayers in the text encoder, panorama encoder, coarse-scale\ncross-modal encoder and Ô¨Åne-scale cross-modal encoder,\nrespectively. Other hyper-parameters are set the same as in\nLXMERT [47], e.g., the hidden layer size is 768. We utilize\nthe pretrained LXMERT for initialization.\nTraining details. On the REVERIE dataset, we Ô¨Årst pre-\ntrain DUET with the batch size of 32 for 100k iterations\nusing 2 Nvidia Tesla P100 GPUs. We automatically gener-\nate synthetic instructions to augment the dataset [10]. Then\nwe use Eq. (12) to Ô¨Åne-tune the policy with the batch size of\n8 for 20k iterations on a single Tesla P100. The best epoch\nis selected by SPL on val unseen split. More details are\nprovided in supplementary material.\n4.4. Ablation Study\nWe ablated our approach on the REVERIE dataset. All\nresults in this section are reported on the val unseen split.\n1) Coarse-scale vs. Ô¨Åne-scale encoders. We Ô¨Årst evalu-\nate coarse-scale and Ô¨Åne-scale encoders separately for the\nREVERIE navigation task in the upper part of Table 1. As\nthe coarse-scale encoder is not fed with object representa-\ntions, it is unable to select target objects for the REVERIE\ntask. However, it outperforms the Ô¨Åne-scale version except\nfor SR\nOSR , for which the Ô¨Åne-scale encoder achieves much\nhigher performance. This ratio estimates the performance\nof the stop action (the OSR is the success rate under oracle\n6\nTable 2. Ablation of graph-aware self-attention (GASA) for graph\nencoding on REVERIE val unseen split.\nFusion GASA OSR‚Üë SR‚Üë SPL‚Üë RGS‚Üë RGSPL‚Üë\naverage √ó 49.22 44.50 30.90 29.88 20.73\n‚úì 51.86 45.81 31.94 32.49 22.78\ndynamic √ó 49.25 45.24 32.88 29.91 21.57\n‚úì 51.07 46.98 33.73 32.15 23.03\nstop policy) and indicates that Ô¨Åne-grained visual represen-\ntations are essential to determine the target location speci-\nÔ¨Åed in the instruction. However, the Ô¨Åne-scale encoder ob-\ntains a low OSR score, suggesting it lacks exploration due\nto a limited action space. The coarse-scale encoder instead\nbeneÔ¨Åts from the constructed map and is able to efÔ¨Åciently\nexplore more areas with high OSR and SPL metrics.\n2) Dual-scale fusion strategy. As the Ô¨Åne- and coarse-\nscale encoders are complementary, we compare different\napproaches to fuse the two encoders in the bottom part of\nTable 1. Both fusion methods outperform the Ô¨Åne-scale and\ncoarse-scale encoder by a large margin. Our proposed dy-\nnamic fusion achieves more efÔ¨Åcient exploration compared\nto the average fusion with 1.79% improvement on SPL.\n3) Graph-aware self-attention. Table 2 ablates models\nwith or without graph topology encoded in the transformer\nas in Eq. (3). It shows that the awareness of the graph struc-\ntures is more beneÔ¨Åcial to improve the SPL score, which\nemphasizes navigating to the target with shorter distance.\n4) Training losses. In Table 3, we compare different train-\ning losses for DUET. The Ô¨Årst row only uses LSAP in be-\nhavior cloning. As it is not trained for object grounding, we\ncan ignore RGS and RGSPL metrics. The second row adds\nthe object supervision in training. It also improves navi-\ngation performance, which suggests that additional cross-\nmodal supervisions such as association between words and\nobjects can be beneÔ¨Åcial to VLN tasks. In the third row,\nwe add common auxiliary proxy tasks MLM and MRC\nin training, which are more helpful for object grounding.\nAs instructions in REVERIE mainly describe the Ô¨Ånal tar-\nget, these two losses are more relevant to object grounding.\nWe further Ô¨Åne-tune the model with reinforcement learning\n(RL) [14,15] or our PID in the last two rows to address dis-\ntribution shift issue in behavior cloning. Both RL and PID\nachieve signiÔ¨Åcant improvement and PID outperforms RL.\n5) Data augmentation with synthetic instructions. We\nevaluate contributions of augmenting training data with syn-\nthetic instructions. The upper block of Table 4 presents re-\nsults of pretraining with or without the augmented data. We\ncan see that the synthetic data is beneÔ¨Åcial in the pretraining\nstage and improves SPL and RGSPL by 1.63% and 1.76%\nrespectively. Based on the initialization of the model in row\n2, we use PID to further improve the policy. The results are\nTable 3. Ablation of training losses on REVERIE val unseen split.\nPretrain Finetune OSR‚Üë SR‚Üë SPL‚ÜëRGS‚ÜëRGSPL‚ÜëSAP OG Aux RL PID\n‚úì √ó √ó √ó √ó 38.45 35.30 24.55 - -\n‚úì ‚úì √ó √ó √ó 40.24 37.80 26.40 23.89 16.36\n‚úì ‚úì ‚úì √ó √ó 37.63 36.81 27.19 25.05 18.40\n‚úì ‚úì ‚úì ‚úì √ó 47.51 42.35 32.97 29.91 23.53\n‚úì ‚úì ‚úì √ó ‚úì 51.07 46.98 33.73 32.15 23.03\nTable 4. Ablation of augmented speaker data in training on\nREVERIE val unseen split.\nPID Aug OSR‚Üë SR‚Üë SPL‚Üë RGS‚Üë RGSPL‚Üë\n√ó √ó 37.29 34.56 25.56 23.00 16.64\n‚úì 37.63 36.81 27.19 25.05 18.40\n‚úì √ó 51.07 46.98 33.73 32.15 23.03\n‚úì 52.09 46.58 32.72 31.75 22.18\nTable 5. Comparison with the state of the art on SOON dataset.\nSplit Methods TL OSR ‚Üë SR‚Üë SPL‚Üë RGSPL‚Üë\nVal\nUnseen\nGBE [8] 28.96 28.54 19.52 13.34 1.16\nDUET (Ours) 36.20 50.91 36.28 22.58 3.75\nTest\nUnseen\nGBE [8] 27.88 21.45 12.90 9.23 0.45\nDUET (Ours) 41.83 43.00 33.44 21.42 4.17\nshown in the bottom block of Table 4. The synthetic data\nhowever does not bring improvements to the performance.\nWe hypothesize that auxiliary proxy tasks in pretraining\nhelp to take advantage from the noisy synthetic data, but\nthe policy learning still requires cleaner data.\n4.5. Comparison with State of the Art\nREVERIE. Table 6 compares our Ô¨Ånal model with state-\nof-the-art models on the REVERIE dataset. Our model sig-\nniÔ¨Åcantly beats the state of the arts on all evaluation metrics\non the three splits. For example, on the val unseen split, our\nmodel outperforms the previous best model HAMT [15] by\n14.03% on SR, 3.53% on SPL and 5.75% on RGSPL. Our\nmodel also generalizes better on the test unseen split, where\nwe improve over HAMT by 22.11% on SR, 9.39% on SPL\nand 8.98% on RGSPL. This clearly demonstrates the effec-\ntiveness of our dual-scale action planning model with topo-\nlogical maps. Note that none of the previous methods has\nemployed a map for navigation on this dataset.\nSOON. Table 5 presents the results on the SOON dataset.\nOur model also achieves signiÔ¨Åcant better performance than\nthe previous graph-based approach GBE [8], with 20.54%\ngains on SR and 12.19% on SPL on test unseen split. The\nresults, however, are much lower than those on REVERIE.\n7\nTable 6. Comparison with the state-of-the-art methods on REVERIE dataset.\nMethods\nVal Seen Val Unseen Test Unseen\nNavigation Grounding Navigation Grounding Navigation Grounding\nTL OSR ‚Üë SR‚Üë SPL‚Üë RGS‚Üë RGSPL‚Üë TL OSR ‚Üë SR‚Üë SPL‚Üë RGS‚Üë RGSPL‚Üë TL OSR ‚Üë SR‚Üë SPL‚Üë RGS‚Üë RGSPL‚Üë\nHuman - - - - - - - - - - - - 21.18 86.83 81.51 53.66 77.84 51.44\nSeq2Seq [2] 12.88 35.70 29.59 24.01 18.97 14.96 11.07 8.07 4.20 2.84 2.16 1.63 10.89 6.88 3.99 3.09 2.00 1.58\nRCM [12] 10.70 29.44 23.33 21.82 16.23 15.36 11.98 14.23 9.29 6.97 4.89 3.89 10.60 11.68 7.84 6.67 3.67 3.14\nSMNA [11] 7.54 43.29 41.25 39.61 30.07 28.98 9.07 11.28 8.15 6.44 4.54 3.61 9.23 8.39 5.80 4.53 3.10 2.39\nFAST-MATTN [7] 16.35 55.17 50.53 45.50 31.97 29.66 45.28 28.20 14.40 7.19 7.84 4.67 39.05 30.63 19.88 11.61 11.28 6.08\nSIA [49] 13.61 65.85 61.91 57.08 45.96 42.65 41.53 44.67 31.53 16.28 22.41 11.56 48.61 44.56 30.80 14.85 19.02 9.20\nRecBERT [14] 13.44 53.90 51.79 47.96 38.23 35.61 16.78 35.02 30.67 24.90 18.77 15.27 15.86 32.91 29.61 23.99 16.50 13.51\nAirbert [30] 15.16 48.98 47.01 42.34 32.75 30.01 18.71 34.51 27.89 21.88 18.23 14.18 17.91 34.20 30.28 23.61 16.83 13.28\nHAMT [15] 12.79 47.65 43.29 40.19 27.20 25.18 14.08 36.84 32.95 30.20 18.92 17.28 13.62 33.41 30.40 26.67 14.88 13.08\nDUET (Ours) 13.86 73.86 71.75 63.94 57.41 51.14 22.11 51.07 46.98 33.73 32.15 23.03 21.30 56.91 52.51 36.06 31.88 22.06\nREVERIE: Go to the living room and wipe down the end table.\ns\ns\ns\n s\ns\n s\ns\ns\nR2R: Walk from telephone down hall and turn left down hall just before \nvase. Walk through archway into bedroom. Stop between bed and chair.\nHAMT [30]\nDueT\n HAMT [30]DueT HAMT [15]\nDueTDueT HAMT [15]\nFigure 5. Predicted trajectories of DUET and the state-of-the-art\nHAMT [15]. The green and checkered Ô¨Çags denote start and tar-\nget locations respectively. The dashed lines denote global actions.\nDUET is able to make more efÔ¨Åcient explorations and correct its\nprevious decisions, while HAMT is limited by its local actions.\nThis is because SOON contains fewer and more challenging\ntraining data (see supplementary material for analysis).\nR2R. As shown in Table 7, DUET beats state-of-the-art ap-\nproaches on success rate (SR) by 6% and 4% on val un-\nseen and test unseen split respectively. However, it achieves\ncomparable performances on SPL. This can be explained by\nthe fact that for map-based approaches backtracking is en-\ncouraged which makes the trajectory length longer. We fur-\nther compare a coarse-scale DUET for fair comparison with\nprevious graph-based approaches [8, 19, 20] which do not\nuse a Ô¨Åne-scale encoder. Even without using the Ô¨Åne-scale\nrepresentation, DUET still outperform them by a margin,\nshowing the effectiveness of our graph transformer. It also\ndemonstrates DUET is able to backtrack more efÔ¨Åciently.\nTable 7. Comparison with the state of the art on R2R dataset.\nMethods are grouped according to the used memories: ‚ÄòRec‚Äô for\nrecurrent state, ‚ÄòSeq‚Äô for sequence and ‚ÄòMap‚Äô for topological map.\nMem Methods Val Unseen Test Unseen\nTL‚Üì NE‚Üì SR‚Üë SPL‚Üë TL‚Üì NE‚Üì SR‚Üë SPL‚Üë\nRec\nSeq2Seq [2] 8.39 7.81 22 - 8.13 7.85 20 18\nSF [10] - 6.62 35 - 14.82 6.62 35 28\nPRESS [27] 10.36 5.28 49 45 10.77 5.49 49 45\nEnvDrop [13] 10.70 5.22 52 48 11.66 5.23 51 47\nAuxRN [51] - 5.28 55 50 - 5.15 55 51\nPREV ALENT[26] 10.19 4.71 58 53 10.51 5.30 54 51\nRelGraph [52] 9.99 4.73 57 53 10.29 4.75 55 52\nRecBERT [14] 12.01 3.93 63 57 12.35 4.09 63 57\nSeq HAMT [15] 11.87 3.65 65 59 12.65 4.11 63 58\nHAMT-e2e [15] 11.46 2.29 66 61 12.27 3.93 65 60\nMap\nEGP [19] - 4.83 56 44 - 5.34 53 42\nGBE [8] - 5.20 54 43 - 5.18 53 43\nSSM [20] 20.7 4.32 62 45 20.4 4.57 61 46\nDUET-coarse 12.96 3.67 68 59 13.08 3.93 67 58\nDUET (Ours) 13.94 3.31 72 60 14.73 3.65 69 59\nFigure 5 visualizes some qualitative examples.\n5. Conclusion\nWe propose DUET (dual-scale graph transformer) for\nvision-and-language navigation (VLN) based on online\nconstructed topological maps. It uses graph transform-\ners to reason over a coarse-scale map representation for\nlong-term action planning and a Ô¨Åne-scale local repre-\nsentation for Ô¨Åne-grained language grounding. The two\nscales are dynamically combined in the navigation pol-\nicy. DUET achieves state-of-the-art performance on VLN\nbenchmarks REVERIE, SOON and R2R. However, our ap-\nproach is not always successful as demonstrated by the gap\nbetween seen and unseen environments, and is restricted\nto discrete environments. Future work will address these\npoints. Applications of our work should take security and\nprivacy risks into account.\n8\nAcknowledgement. This work was granted access to the\nHPC resources of IDRIS under the allocation 101002 made\nby GENCI. This work is funded in part by the French\ngovernment under management of Agence Nationale de la\nRecherche as part of the ‚ÄúInvestissements d‚Äôavenir‚Äù pro-\ngram, reference ANR19-P3IA-0001 (PRAIRIE 3IA Insti-\ntute) and by Louis Vuitton ENS Chair on ArtiÔ¨Åcial Intelli-\ngence.\nReferences\n[1] Peter Anderson, Angel Chang, Devendra Singh Chap-\nlot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen\nKoltun, Jana Kosecka, Jitendra Malik, Roozbeh\nMottaghi, Manolis Savva, et al. On evaluation\nof embodied navigation agents. arXiv preprint\narXiv:1807.06757, 2018. 1, 6\n[2] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,\nMark Johnson, Niko S ¬®underhauf, Ian Reid, Stephen\nGould, and Anton Van Den Hengel. Vision-and-\nlanguage navigation: Interpreting visually-grounded\nnavigation instructions in real environments. In\nCVPR, pages 3674‚Äì3683, 2018. 1, 2, 3, 6, 8, 12, 13\n[3] Howard Chen, Alane Suhr, Dipendra Misra, Noah\nSnavely, and Yoav Artzi. Touchdown: Natural lan-\nguage navigation and spatial reasoning in visual street\nenvironments. In CVPR, pages 12538‚Äì12547, 2019.\n1, 2\n[4] Alexander Ku, Peter Anderson, Roma Patel, Eugene\nIe, and Jason Baldridge. Room-across-room: Mul-\ntilingual vision-and-language navigation with dense\nspatiotemporal grounding. In EMNLP, pages 4392‚Äì\n4412, 2020. 1, 2\n[5] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv\nBatra, and Stefan Lee. Beyond the nav-graph: Vision-\nand-language navigation in continuous environments.\nIn ECCV, pages 104‚Äì120. Springer, 2020. 1, 2\n[6] Mohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke\nZettlemoyer, and Dieter Fox. Alfred: A benchmark for\ninterpreting grounded instructions for everyday tasks.\nIn CVPR, pages 10740‚Äì10749, 2020. 1, 2\n[7] Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang,\nWilliam Yang Wang, Chunhua Shen, and Anton\nvan den Hengel. Reverie: Remote embodied visual\nreferring expression in real indoor environments. In\nCVPR, pages 9982‚Äì9991, 2020. 1, 3, 6, 8, 12, 13\n[8] Fengda Zhu, Xiwen Liang, Yi Zhu, Qizhi Yu, Xiao-\njun Chang, and Xiaodan Liang. Soon: Scenario ori-\nented object navigation with graph-based exploration.\nIn CVPR, pages 12689‚Äì12699, 2021. 1, 2, 3, 6, 7, 8,\n12, 13\n[9] Muhammad Zubair Irshad, Chih-Yao Ma, and Zsolt\nKira. Hierarchical cross-modal agent for robotics\nvision-and-language navigation. In ICRA, pages\n13238‚Äì13246, 2021. 1, 2\n[10] Daniel Fried, Ronghang Hu, V olkan Cirik, Anna\nRohrbach, Jacob Andreas, Louis-Philippe Morency,\nTaylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and\nTrevor Darrell. Speaker-follower models for vision-\nand-language navigation. In NeurIPS, pages 3318‚Äì\n3329, 2018. 1, 2, 3, 6, 8\n[11] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Al-\nRegib, Zsolt Kira, Richard Socher, and Caiming\nXiong. Self-monitoring navigation agent via auxiliary\nprogress estimation. In ICLR, 2019. 1, 6, 8\n[12] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz,\nJianfeng Gao, Dinghan Shen, Yuan-Fang Wang,\nWilliam Yang Wang, and Lei Zhang. Reinforced\ncross-modal matching and self-supervised imitation\nlearning for vision-language navigation. In CVPR,\npages 6629‚Äì6638, 2019. 1, 8\n[13] Hao Tan, Licheng Yu, and Mohit Bansal. Learning to\nnavigate unseen environments: Back translation with\nenvironmental dropout. In NAACL, pages 2610‚Äì2621,\n2019. 1, 2, 3, 8\n[14] Yicong Hong, Qi Wu, Yuankai Qi, Cristian\nRodriguez-Opazo, and Stephen Gould. Vln BERT: A\nrecurrent vision-and-language BERT for navigation.\nIn CVPR, pages 1643‚Äì1653, 2021. 1, 2, 3, 7, 8, 13\n[15] Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid,\nand Ivan Laptev. History aware multimodal trans-\nformer for vision-and-language navigation. In\nNeurIPS, 2021. 2, 3, 5, 7, 8, 13, 14\n[16] Alexander Pashevich, Cordelia Schmid, and Chen\nSun. Episodic transformer for vision-and-language\nnavigation. In ICCV, pages 15942‚Äì15952, 2021. 2,\n3, 5\n[17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\nIn NeurIPS, pages 5998‚Äì6008, 2017. 2, 4\n[18] Devendra Singh Chaplot, Ruslan Salakhutdinov, Ab-\nhinav Gupta, and Saurabh Gupta. Neural topological\nSLAM for visual navigation. In CVPR, pages 12875‚Äì\n12884, 2020. 2\n9\n[19] Zhiwei Deng, Karthik Narasimhan, and Olga Rus-\nsakovsky. Evolving graphical planner: Contextual\nglobal planning for vision-and-language navigation.\nIn NeurIPS, volume 33, 2020. 2, 3, 8\n[20] Hanqing Wang, Wenguan Wang, Wei Liang, Caiming\nXiong, and Jianbing Shen. Structured scene memory\nfor vision-language navigation. InCVPR, pages 8455‚Äì\n8464, 2021. 2, 3, 8\n[21] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish\nVaswani, Eugene Ie, and Jason Baldridge. Stay on the\npath: Instruction Ô¨Ådelity in vision-and-language navi-\ngation. In ACL, pages 1862‚Äì1872, 2019. 2, 13\n[22] Abhishek Das, Samyak Datta, Georgia Gkioxari, Ste-\nfan Lee, Devi Parikh, and Dhruv Batra. Embodied\nquestion answering. In CVPR, pages 1‚Äì10, 2018. 2\n[23] Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit\nBansal, Tamara L Berg, and Dhruv Batra. Multi-target\nembodied question answering. In CVPR, pages 6309‚Äì\n6318, 2019. 2\n[24] Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caim-\ning Xiong, and Zsolt Kira. The regretful agent:\nHeuristic-aided navigation through progress estima-\ntion. In CVPR, pages 6732‚Äì6740, 2019. 2, 3\n[25] Arun Balajee Vasudevan, Dengxin Dai, and Luc\nVan Gool. Talk2nav: Long-range vision-and-language\nnavigation with dual attention and spatial memory.In-\nternational Journal of Computer Vision , 129(1):246‚Äì\n266, 2021. 2\n[26] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence\nCarin, and Jianfeng Gao. Towards learning a generic\nagent for vision-and-language navigation via pre-\ntraining. In CVPR, pages 13137‚Äì13146, 2020. 2, 5, 8,\n13\n[27] Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk,\nAsli Celikyilmaz, Jianfeng Gao, Noah A Smith, and\nYejin Choi. Robust navigation with language pretrain-\ning and stochastic sampling. In EMNLP, pages 1494‚Äì\n1499, 2019. 2, 8\n[28] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. BERT: Pre-training of deep bidi-\nrectional transformers for language understanding. In\nNAACL, pages 4171‚Äì4186, 2019. 2, 4, 5, 11\n[29] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Pe-\nter Anderson, Devi Parikh, and Dhruv Batra. Im-\nproving vision-and-language navigation with image-\ntext pairs from the web. In ECCV, pages 259‚Äì274.\nSpringer, 2020. 2\n[30] Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen,\nIvan Laptev, and Cordelia Schmid. Airbert: In-domain\npretraining for vision-and-language navigation. In\nICCV, pages 1634‚Äì1643, 2021. 2, 8\n[31] Sebastian Thrun. Probabilistic robotics. Communica-\ntions of the ACM, 45(3):52‚Äì57, 2002. 3\n[32] Sebastian Thrun. Learning metric-topological maps\nfor indoor mobile robot navigation. ArtiÔ¨Åcial Intelli-\ngence, 99(1):21‚Äì71, 1998. 3\n[33] Albert S Huang, Abraham Bachrach, Peter Henry,\nMichael Krainin, Daniel Maturana, Dieter Fox, and\nNicholas Roy. Visual odometry and mapping for au-\ntonomous Ô¨Çight using an rgb-d camera. In Robotics\nResearch, pages 235‚Äì252. Springer, 2017. 3\n[34] Jingwei Zhang, Lei Tai, Ming Liu, Joschka\nBoedecker, and Wolfram Burgard. Neural SLAM:\nLearning to explore with external memory. arXiv\npreprint arXiv:1706.09520, 2017. 3\n[35] Saurabh Gupta, James Davidson, Sergey Levine,\nRahul Sukthankar, and Jitendra Malik. Cognitive\nmapping and planning for visual navigation. InCVPR,\npages 2616‚Äì2625, 2017. 3\n[36] Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh\nGupta, Abhinav Gupta, and Ruslan Salakhutdinov.\nLearning to explore using active neural SLAM. In\nICLR, 2020. 3\n[37] Peter Anderson, Ayush Shrivastava, Devi Parikh,\nDhruv Batra, and Stefan Lee. Chasing ghosts: Instruc-\ntion following as bayesian state tracking. NeurIPS,\n32:371‚Äì381, 2019. 3\n[38] Nikolay Savinov, Alexey Dosovitskiy, and Vladlen\nKoltun. Semi-parametric topological memory for nav-\nigation. ICLR, 2018. 3\n[39] Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio\nSavarese. Scene memory transformer for embodied\nagents in long-horizon tasks. In CVPR, pages 538‚Äì\n547, 2019. 3\n[40] Kevin Chen, Junshen K Chen, Jo Chuang, Marynel\nV¬¥azquez, and Silvio Savarese. Topological planning\nwith transformers for vision-and-language navigation.\nIn CVPR, pages 11276‚Äì11286, 2021. 3\n[41] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. Scheduled sampling for sequence pre-\ndiction with recurrent neural networks. In NeurIPS,\nvolume 28, 2015. 3\n10\n[42] St ¬¥ephane Ross, Geoffrey Gordon, and Drew Bagnell.\nA reduction of imitation learning and structured pre-\ndiction to no-regret online learning. InAISTATS, pages\n627‚Äì635. JMLR Workshop and Conference Proceed-\nings, 2011. 3, 5\n[43] Richard S Sutton and Andrew G Barto. Reinforcement\nlearning: An introduction. MIT press, 2018. 3\n[44] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi\nMirza, Alex Graves, Timothy Lillicrap, Tim Harley,\nDavid Silver, and Koray Kavukcuoglu. Asynchronous\nmethods for deep reinforcement learning. In ICML,\npages 1928‚Äì1937. PMLR, 2016. 3\n[45] Hu Wang, Qi Wu, and Chunhua Shen. Soft expert re-\nward learning for vision-and-language navigation. In\nECCV, pages 126‚Äì141. Springer, 2020. 3\n[46] Peter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\nBottom-up and top-down attention for image caption-\ning and visual question answering. In CVPR, pages\n6077‚Äì6086, 2018. 3, 6, 12\n[47] Hao Tan and Mohit Bansal. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In EMNLP, pages 5103‚Äì5114, 2019. 4, 5,\n6\n[48] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. ViLBERT: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. In\nNeurIPS, volume 32, 2019. 5\n[49] Xiangru Lin, Guanbin Li, and Yizhou Yu. Scene-\nintuitive agent for remote embodied visual grounding.\nIn CVPR, pages 7036‚Äì7045, 2021. 5, 8\n[50] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16 √ó 16 words: Transformers for\nimage recognition at scale. ICLR, 2020. 6, 12\n[51] Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiao-\ndan Liang. Vision-language navigation with self-\nsupervised auxiliary reasoning tasks. In CVPR, pages\n10012‚Äì10022, 2020. 8\n[52] Yicong Hong, Cristian Rodriguez, Yuankai Qi, Qi Wu,\nand Stephen Gould. Language and visual entity\nrelationship graph for agent navigation. NeurIPS,\n33:7685‚Äì7696, 2020. 8\n[53] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. Show, attend and tell: Neural im-\nage caption generation with visual attention. InICML,\npages 2048‚Äì2057. PMLR, 2015. 12\n[54] Jeffrey Pennington, Richard Socher, and Christo-\npher D Manning. Glove: Global vectors for word rep-\nresentation. In EMNLP, pages 1532‚Äì1543, 2014. 12\n[55] Angel Chang, Angela Dai, Thomas Funkhouser, Ma-\nciej Halber, Matthias Niebner, Manolis Savva, Shuran\nSong, Andy Zeng, and Yinda Zhang. Matterport3d:\nLearning from rgb-d data in indoor environments. In\n3DV, pages 667‚Äì676. IEEE, 2017. 12\nAppendix\nSection A provides additional details for the model. The\nexperimental setup is described in Section B, including\ndatasets, metrics and implementation details. Section C\npresents more ablation studies of our DUET model. Sec-\ntion D shows more qualitative examples.\nA. Model Details\nA.1. Pretraining Objectives\nAs introduced in Sec 3.3, we employ two auxiliary\nproxy tasks in pretraining in addition to behavior cloning\ntasks SAP (single-step action prediction) and OG (object\ngrounding). In the following, we describe the two auxiliary\ntasks: masked language modeling (MLM) and masked re-\ngion classiÔ¨Åcation (MRC). The inputs for the two tasks are\npairs of instruction Wand demonstration path P.\nMasked Language Modeling (MLM) task aims to learn\ngrounded language representations and cross-modal align-\nment by predicting masked words given contextual words\nand demonstration path. We randomly replace tokens in\nW by a special token [mask] with the probability of\n15% [28]. Both the coarse-scale encoder and Ô¨Åne-scale en-\ncoder can generate contextual word embeddings for masked\nwords as introduced in Sec 3.2.2 and 3.2.3 respectively.\nThe coarse-scale encoder utilizes visual information from\nan encoded graph at the Ô¨Ånal step as contexts, while the\nÔ¨Åne-scale encoder utilizes the last panoramic observation\nas visual contexts. We average output embeddings of\nthe two encoders for masked words, and employ a two-\nlayer fully-connected network to predict word distributions\np(wi|W\\i,P) where W\\i is the masked instruction and wi\nis the label of masked word. The objective of the task is\nminimizing the negative log-likelihood of original words:\nLMLM = ‚àílog p(wi|W\\i,P).\nMasked Region ClassiÔ¨Åcation (MRC) aims to predict se-\nmantic labels of masked image regions in an observation\n11\ngiven an instruction and neighboring regions. As instruc-\ntions in goal-oriented VLN tasks mainly describe the last\nobservation in the demonstration path, we only apply the\nMRC task on the Ô¨Åne-scale encoder. We randomly zero out\nview images and objects in the last observation of Pwith\nthe probability of 15%. The target semantic labels for view\nimages are class probability predicted by an image classi-\nÔ¨Åcation model [50] pretrained on ImageNet, while the la-\nbels for objects are class probability predicted by an object\ndetector [46] pretrained on VisualGenome. We use a two-\nlayer fully-connected network to predict semantic labels for\neach masked visual token, and minimize the KL divergence\nbetween the predicted and target probability distribution.\nA.2. Speaker Model for Data Augmentation\nWe train a speaker model to synthesize instructions\nbased on visual observations for REVERIE dataset. As\nREVERIE provides annotated object classes and Matter-\nport3D also contains annotated room classes, we utilize\nthese semantic labels to alleviate the gap between vision\nand language. Our speaker model consists of a panorama\nencoder and a sentence decoder. The panorama encoder is\nfed with image features of the panorama, semantic labels\nof target object and target room as well as the level of the\nroom. We project all the input features into the same dimen-\nsion, and utilize a transformer with self-attention to capture\nrelations of each token. The sentence decoder then sequen-\ntially generates words conditioning on the encoded tokens.\nWe use LSTM as the decoder and follow the architecture in\nshow-attend-tell image captioning model [53].\nPlease note that we only employ data in REVERIE train-\ning split to learn the speaker model. We initialize the word\nembeddings in encoder and decoder with pretrained GloVe\nembeddings [54] and train the speaker model for 50 epochs.\nWe employ the trained speaker model to synthesize instruc-\ntions for every annotated object in the REVERIE training\nsplit, leading to 19,636 instructions in total. We extend the\nsize of the training set from 10,466 instruction-path pairs to\n30,102 pairs.\nB. Experimental Setups\nB.1. Dataset\nWe primarily focus our evaluation on goal-oriented VLN\nbenchmarks REVERIE [7] and SOON [8]. To localize tar-\nget objects in these benchmarks, the agent requires Ô¨Åne-\ngrained object grounding and advanced exploration capa-\nbilities. We also test our model on less demanding VLN\nbenchmarks R2R [2] with step-by-step instructions and no\nobject localization. All the benchmarks build upon the Mat-\nterport3D [55] environment and contain 90 photo-realistic\nhouses. Each house is deÔ¨Åned by a set of navigable loca-\ntions. Each location is represented by the corresponding\npanorama image, GPS coordinates and a set of possible ac-\ntions. We adopt the standard split of houses into training,\nval seen, val unseen, and test subsets. Houses in the val\nseen split are the same as in training, while houses in val\nunseen and test splits are different from training.\nTable 8 presents statistics of the three datasets. To be\nnoted, we follow the released challenge split on SOON\ndataset instead of the split in the original paper [8]1.\nB.2. Data Processing for SOON Dataset\nThe SOON dataset does not provide annotated object\nbounding boxes per panorama. It only annotates the loca-\ntion of target object bounding boxes for each instruction,\nincluding the orientation of object‚Äôs center point as well\nas orientation of top left, top right, bottom left, and bot-\ntom right corners. The object grounding setting in SOON\ndataset is to predict the orientation of object‚Äôs center point.\nHowever, we observe that though the annotated objects‚Äô\ncenter points are of good quality, their annotations of the\nfour corners are quite noisy 2. Therefore, we propose to\nclean the object bounding boxes in training and also pro-\nvide more automatically detected objects as Ô¨Åne-grained vi-\nsual contexts to represent each panorama.\nSpeciÔ¨Åcally, we employ the BUTD detector [46] pre-\ntrained on VisualGenome to detect objects per panorama,\nwhich covers 1600 object and scene classes. We Ô¨Ålter\nsome unimportant classes for SOON dataset such as ‚Äòback-\nground‚Äô, ‚ÄòÔ¨Çoor‚Äô, ‚Äòceiling‚Äô, ‚Äòwall‚Äô, ‚Äòroof‚Äô and so on. We then\nselect one of the detected objects as our pseudo target ac-\ncording to the semantic similarity of object classes and the\nEuclidean distances of the objects‚Äô center points compared\nto annotated target object. In this way, we convert the object\ngrounding setting in SOON datset similar to the setting in\nREVERIE dataset, whose goal is to select one object from\nall candidate objects. In inference, we utilize the orientation\nof the selected object as our object grounding prediction.\nB.3. Evaluation Metrics\nDue to the different settings for object grounding in\nREVERIE and SOON datasets, deÔ¨Ånitions of success in the\ntwo datasets are different. In REVERIE dataset, the success\nis deÔ¨Åned as arriving at a location where the target object is\nvisible and selecting the target object among all annotated\ncandidate objects in the panorama of the location. In SOON\ndataset, an agent succeeded in carrying out an instruction if\nit arrives 3 meters near to one of the target locations and the\n1As shown in https://github.com/ZhuFengdaaa/SOON/\nissues/1, Zhu et al . [8] do not release the split in their original pa-\nper. Therefore, performance comparisons on SOON dataset are based on\ntheir challenge report https://scenario-oriented-object-\nnavigation.github.io/.\n2As shown in https://github.com/ZhuFengdaaa/SOON/\nissues/2, about 50% polygons constructed by the annotated four cor-\nners do not contain the objects‚Äô center point.\n12\nTable 8. Dataset statistics. #house, #instr denote the number of houses and instructions respectively.\nVLN Task Dataset Train Val Seen Val Unseen Test Unseen\n#house #instr #house #instr #house #instr #house #instr\nObject-oriented REVERIE [7] 60 10,466 46 1,423 10 3,521 16 6,292\nSOON [8] 34 2,780 2 113 5 339 14 1,411\nFine-grained R2R [2] 61 14,039 56 1,021 11 2,349 18 4,173\nR4R [21] 59 233,532 40 1,035 11 45,234 - -\nTable 9. Ablation of balance factor Œªin the Ô¨Åne-tuning loss.\nNavigation Object Grounding\nOSR SR SPL RGS RGSPL\n0 53.00 48.22 33.00 32.12 22.04\n0.2 51.07 46.98 33.73 32.15 23.03\n0.5 52.06 46.98 32.38 32.43 22.72\n1 50.33 45.64 32.54 30.19 21.50\npredicted orientation of target object‚Äôs center point is inside\nof the annotated polygon of the object in the location.\nB.4. Training Details\nREVERIE: In pretraining, we combine the original dataset\nwith augmented data synthesized by our speaker model. We\npretrain DUET with the batch size of 32 for 100k iterations\nusing 2 Nvidia Tesla P100 GPUs. Then we use Eq. (12)\npresented in the main paper to Ô¨Åne-tune the policy with the\nbatch size of 8 for 20k iterations on a single Tesla P100.\nThe best epoch is selected by SPL on val unseen split.\nSOON: As the size of SOON dataset is much smaller\nthan REVERIE dataset and the instructions are much more\ncomplicated, we do not synthesize instructions for SOON\ndataset. We pretrain model using the original instructions\nand our automatically cleaned object bounding boxes for\n40k iterations with batch size of 32. We Ô¨Åne-tune the model\nfor 40k iterations with batch size of 2 on a single Tesla P100\nand select the best model by SPL on val unseen split.\nR2R: Following previous works [14, 15, 26], we adopt aug-\nmented R2R data [26] in pretraining. We pretrain the model\nfor 200k interations with batch size of 64. We Ô¨Åne-tune the\nmodel for 20k iterations with batch size of 8.\nC. Additional Ablations\nC.1. Balance factor Œªin Ô¨Åne-tuning objective\nTable 9 presents the performance of using different Œªin\nthe Ô¨Åne-tuning objective in Eq. (12) of the main paper. The\nlarger Œª, the more important of the behavior cloning. We\ncan see that over-emphasizing behavior cloning is harmful\nto the exploration ability. The model with Œª = 1achieves\nGo to second level hallway next to the kitchen and clean the photo above the \nblack bench and that is closest to the kitchen.\ns\ns\nDueTDUET\nGo to the brown bedroom on level 2 at the end of the hall and open the left \nwindow.\nDueTHAMT\nDueTDUET DueTHAMT\nFigure 6. Predicted trajectories of DUET and the state-of-the-art\nHAMT [15] on REVERIE val unseen split. The green and check-\nered Ô¨Çags denote start and target locations respectively.\nthe worst OSR and SR. Removing behavior cloning (Œª= 0)\nachieves good navigation performance such as in OSR, SR\nand SPL, but it is less competitive in object grounding. We\nthink this is because the agent fails to navigate to target lo-\ncations in its sampled trajectories, and is unable to train the\nobject grounding module. However, the agent is guaranteed\nto arrive at target locations in behavior cloning.\nC.2. Backtrack ratio in inference\nThe backtrack action indicates that the agent does not\nselect a neighboring node from the local action space but\njumps to a previously partially observed node through the\nglobal action space. We compute the backtrack ratio for\nDUET. On the REVERIE val seen split, DUET only back-\ntracks in 13.7% of the predicted trajectories; while on the\nREVERIE val unseen split, DUET backtracks in 48.6% of\n13\nexit the roped off hall, follow the red carpet, turn right, continue straight \ndown the red carpet, enter room at the end, stop once inside the room.\ns\ns\nHAMT [30]DueTHAMT HAMT [15]\nDueTDUET\nWalk all the way forward passing all the picture frames on the wall on your \nleft. Enter the corner on your left with the arch layout, and stop there.\nDueTDUET\nDueTHAMT\nDueTHAMT\nFigure 7. Predicted trajectories of DUET and the state-of-the-art\nHAMT [15] on R2R val unseen split. The green and checkered\nÔ¨Çags denote start and target locations respectively.\nits predicted trajectories. As the agent has the capacity to\nmemorize house structures in seen environments, it can di-\nrectly Ô¨Ånd the target location without much exploration in\nseen environments. However, when the agent is deployed in\nunseen environments, it has to explore more to Ô¨Ånd the tar-\nget location speciÔ¨Åed by high-level instructions. When step-\nby-step instructions are given such as in R2R dataset, we\nobserve the backtrack ratio signiÔ¨Åcantly decreases to 23.2%\non val unseen split, which matches our expectation.\nC.3. Fusion weights of coarse and Ô¨Åne scales\nWe observe that the agent typically puts more weights\non the Ô¨Åne-scale module in the beginning and at the end of\nthe navigation, and on the coarse-scale module in the mid-\ndle. Quantitatively, the average weight of the coarse-scale\nmodule is 0.36 in the beginning, 0.45 in the middle, and\n0.42 at the end. The agent may not need to backtrack at\nearly steps, so it relies more on the local Ô¨Åne-scale module.\nThen, the agent needs to explore so the global coarse-scale\nmodule gets more attention. When deciding where to stop,\nthe agent should identify the target object and the Ô¨Åne-scale\nmodule is emphasized again.\nC.4. Failure analysis\nWe perform an additional quantitative evaluation on the\nREVERIE dataset. For navigation, we measure whether an\nagent stops at the target room type ( e.g. a bathroom) or at\nthe correct location. We obtain the following results: (a) in-\ncorrect room type: 29.82%; (b) correct room type + incor-\nrect location: 23.20%; (c) correct location: 46.98%. This\nshows that Ô¨Åne-grained scene understanding remains chal-\nlenging. With respect to object grounding, once an agent\nreaches the correct location, the object can be correctly lo-\ncalized 68.43% of the time.\nD. Qualitative Examples\nFigure 6 visualizes some examples of our DUET and the\nstate-of-the-art HAMT [15] model on REVERIE dataset. In\nboth the cases, the agents explore an incorrect direction in\nthe Ô¨Årst attempt. However, DUET is able to efÔ¨Åciently ex-\nplore another direction towards the goal. Figure 7 shows\nsome examples on R2R dataset. Though step-by-step in-\nstructions are provided, the instruction can still be ambigu-\nous. For example, both directions of the start point in the top\nexample of Figure 7 can ‚Äúexit the rope off hall‚Äù. DUET is\nalso better at correcting its previous decisions when it Ô¨Ånds\nthat the followup instructions do not match with the visual\nobservations.\nWe further provide some failure cases in REVERIE and\nR2R datasets in Figure 8. In the top example of Fig-\nure 8, there are several bathrooms in the house and our\nDUET model arrives at one of bathroom. However, the\narrived bathroom does not contain the Ô¨Åne-grained objects\nspeciÔ¨Åed in the instruction. It suggests that our model still\nneeds to improve the Ô¨Åne-grained object grounding capa-\nbility. The bottom example presents three different instruc-\ntions for the same trajectory on R2R dataset. The agent\nsucceeds in following the Ô¨Årst instruction, but fails for the\nother two instructions. We observe that the predictions are\nnot very robust across different language instructions.\n14\nREVERIE: Go to the living room and ipe \ndown the end table.\ngo to the bathroom on the second floor \ninside of the room with teddy bear on the \nchair and clean the the first picture close to \nthe door. \n(‚úó)\nR2R: Leave the closet and bedroom. In the hall, go down the stairs. \nStop on the third stair from the top.\ns\ns\nREVERIE: Go to the living room and ipe \ndown the end table.\nGo to the bathroom on level 2 that has a \ngrounded towel rack with two red towels on it \nwhere above said rack is a depiction of an \nangel praying and bring me one of the photos \nclosest to the entrance of the room. \n(‚úó) \nREVERIE: Go to the living room and ipe \ndown the end table.\nGo to the bathroom that has red towels \nwith white stripes and dust off the photo \nthat is closest to the doorway. \n(‚úó)\nREVERIE: Go to the living room and ipe \ndown the end table.\nWalk past the counters and exit the kitchen. \nWait next to the landscape painting on the \nwall. \n(‚úî)\nDueTHAMT\nREVERIE: Go to the living room and ipe \ndown the end table.\nWalk between the two kitchen islands and \nthen turn right. Pass through the stone \narchway and stop just after you pass \nthrough it. Wait there.\n (‚úó)\nREVERIE: Go to the living room and ipe \ndown the end table.\nWalk through the kitchen. Walk through the \narchway to the left of the stove. Wait at the \nframed landscape painting. \n(‚úó)\nFigure 8. Predicted trajectories of DUET on REVERIE val unseen split (top) and R2R val unseen split (bottom). The green and checkered\nÔ¨Çags denote start and target locations respectively.\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6440445184707642
    },
    {
      "name": "Transformer",
      "score": 0.5536167621612549
    },
    {
      "name": "Dual (grammatical number)",
      "score": 0.46796661615371704
    },
    {
      "name": "Graph",
      "score": 0.4446547329425812
    },
    {
      "name": "Computer vision",
      "score": 0.41400572657585144
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3996768593788147
    },
    {
      "name": "Theoretical computer science",
      "score": 0.21545904874801636
    },
    {
      "name": "Engineering",
      "score": 0.16306182742118835
    },
    {
      "name": "Electrical engineering",
      "score": 0.1462155282497406
    },
    {
      "name": "Linguistics",
      "score": 0.13337883353233337
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210103330",
      "name": "√âcole Normale Sup√©rieure",
      "country": "BI"
    },
    {
      "id": "https://openalex.org/I2746051580",
      "name": "Universit√© Paris Sciences et Lettres",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I65181880",
      "name": "Indian Institute of Technology Hyderabad",
      "country": "IN"
    }
  ],
  "cited_by": 126
}