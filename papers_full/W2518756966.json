{
  "title": "Character-Level Language Modeling with Hierarchical Recurrent Neural Networks",
  "url": "https://openalex.org/W2518756966",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5057914051",
      "name": "Kyuyeon Hwang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5113491293",
      "name": "Wonyong Sung",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2949563612",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W2263232528",
    "https://openalex.org/W2159505618",
    "https://openalex.org/W6908809",
    "https://openalex.org/W2136848157",
    "https://openalex.org/W2293997542",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2147568880",
    "https://openalex.org/W2150355110",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2962819663",
    "https://openalex.org/W2057653135",
    "https://openalex.org/W1951742130",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2024490156",
    "https://openalex.org/W2546915671",
    "https://openalex.org/W183625566",
    "https://openalex.org/W2969945254",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2220350356",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W1899794420"
  ],
  "abstract": "Recurrent neural network (RNN) based character-level language models (CLMs) are extremely useful for modeling out-of-vocabulary words by nature. However, their performance is generally much worse than the word-level language models (WLMs), since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules with different timescales. Despite the multi-timescale structures, the input and output layers operate with the character-level clock, which allows the existing RNN CLM training approaches to be directly applicable without any modifications. Our CLM models show better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word Benchmark with only 2% of parameters. Also, we present real-time character-level end-to-end speech recognition examples on the Wall Street Journal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30%.",
  "full_text": "arXiv:1609.03777v2  [cs.LG]  2 Feb 2017\nCHARACTER-LEVEL LANGUAGE MODELING\nWITH HIERARCHICAL RECURRENT NEURAL NETWORKS\nKyuyeon Hwang and Wonyong Sung\nDepartment of Electrical and Computer Engineering\nSeoul National University\n1, Gwanak-ro, Gwanak-gu, Seoul, 08826 Korea\nkyuyeon.hwang@gmail.com; wysung@snu.ac.kr\nABSTRACT\nRecurrent neural network (RNN) based character-level language\nmodels (CLMs) are extremely useful for modeling out-of-vocabulary\nwords by nature. However, their performance is generally much\nworse than the word-level language models (WLMs), since CLMs\nneed to consider longer history of tokens to properly predict the\nnext one. We address this problem by proposing hierarchicalRNN\narchitectures, which consist of multiple modules with different\ntimescales. Despite the multi-timescale structures, the input and\noutput layers operate with the character-level clock, which allows\nthe existing RNN CLM training approaches to be directly applicable\nwithout any modiﬁcations. Our CLM models show better perplexity\nthan Kneser-Ney (KN) 5-gram WLMs on the One Billion Word\nBenchmark with only 2% of parameters. Also, we present real-time\ncharacter-level end-to-end speech recognition examples on the Wall\nStreet Journal (WSJ) corpus, where replacing traditional mono-clock\nRNN CLMs with the proposed models results in better recognition\naccuracies even though the number of parameters are reducedto\n30%.\nIndex Terms— Character-level language model, hierarchical re-\ncurrent neural network, long short-term memory\n1. INTRODUCTION\nLanguage models (LMs) show the probability distribution over\nsequences of words or characters, and they are very important\nfor many speech and document processing applications including\nspeech recognition, text generation, and machine translation [1, 2, 3].\nLMs can be classiﬁed into character-, word-, and context-levels ac-\ncording to the unit of the input and output. In the character-level\nLM (CLM) [2], the probability distribution of the next characters are\ngenerated based on the past character sequences. Since the number\nof alphabets is small in English, for example, the input and output\nof the CLM is quite simple. However, the word-level LM (WLM)\nis usually needed because the character-level modeling is disadvan-\ntaged in utilizing the long period of past sequences. However, the\nproblem of the word-level model is the complexity of the input and\noutput because the vocabulary size to be supported can be bigger\nthan 1 million.\nLMs have long been developed by analyzing a large amount of\ntexts and storing the probability distribution of word sequences into\nThis work was supported in part by the Brain Korea 21 Plus Project and\nthe National Research Foundation of Korea (NRF) grant funded by the Korea\ngovernment (MSIP) (No. 2015R1A2A1A10056051).\nthe memory. The statistical language model demands a large mem-\nory space, often exceeding 1 GB, not only because the vocabulary\nsize is large but also their combinations needs to be considered. In\nrecent years, the language modeling based on recurrent neural net-\nworks (RNNs) have been actively investigated [4, 5]. However, the\nRNN based WLMs still demand billions of parameters because of\nthe large vocabulary size.\nIn this work, we propose hierarchical RNN based LMs that com-\nbine the advantageous characteristics of both character- and word-\nlevel LMs. The proposed network consists of a low-level and ahigh-\nlevel RNNs. The low-level RNN employs the character-level input\nand output, and provides the short-term embedding to the high-level\nRNN that operates as the word-level RNN. The high-level RNN do\nnot need complex input and output because it receives the character-\nembedding information from the low-level network, and sends the\nword-prediction information back to the low-level in a compressed\nform. Thus, when considering the input and output, the proposed\nnetwork is a CLM, although it contains a word-level model inside.\nThe low-level module operates with the character input clock, while\nthe high-level one runs with the space (<w>) and sentence boundary\ntokens (<s>) that separates words. We expect this hierarchical LM\ncan be extended for processing a longer period of information, such\nas sentences, topics, or other contexts.\n2. RELATED WORK\n2.1. Character-level language modeling with RNNs\nCLMs need to consider longer sequence of history tokens to pre-\ndict the next token than the WLMs, due to the smaller unit of to-\nkens. Therefore, traditionalN-gram models cannot be employed for\nCLMs. Thanks to the recent advances in RNNs, RNN-based CLMs\nhas begun to show satisfactory performances [2, 6]. Especially, deep\nlong short-term memory (LSTM) [7] based CLMs show excellent\nperformance and successfully applied to end-to-end speechrecogni-\ntion system [8].\nFor training RNN CLMs, training data should be ﬁrst converted\nto the sequence of one-hot encoded character vectors,xt, where the\ncharacters include word boundary symbols,<w>or space, and op-\ntionally sentence boundary symbols,<s>. Then, as shown in Fig-\nure 1, the RNN is trained to predict the next characterxt+1 by min-\nimizing the cross-entropy loss of the softmax output [9] that repre-\nsents the probability distributions of the next character.\nCurrent character \nTarget character \nTime \nT H E <w> Q U\nH E <w> Q U I\nRNN \nFig. 1: Training an RNN-based CLM.\n2.2. Character-aware word-level language modeling\nThere has been many attempts to make WLMs understand character-\nlevel inputs. One of the most successful approaches is to encode the\narbitrary character sequence to ﬁxed dimensional vector, which is\ncalled word embedding, and feed this vector to the word-level RNN\nLMs. In [10], convolutional neural networks (CNNs) are usedto\ngenerate word embeddings, and achieved the state of the art results\non English Penn Treebank corpus [11]. The similar CNN-basedem-\nbedding approach is used by [5] with very large LSTM networkson\nthe One Billion Word Benchmark [12], also achieving the state of\nthe art perplexity. In [13, 14], bidirectional LSTMs are employed\ninstead of CNNs for word embedding. However, in all of these\napproaches, LMs still generate the output probabilities atthe word-\nlevel. Although the character-level modeling approach of the output\nword probability is introduced using CNN softmax in [5], thebase\nLSTM network still runs with a word-level clock.\nOur approach is different from the above ones in many ways.\nFirst, our base model is the character-level RNN LMs, instead of\nWLMs, and we extend this model to enhance the model to consider\nlong-term contexts. Therefore, the output probabilities are generated\nwith a character-level clocks. This property is extremely useful for\ncharacter-level beam search for end-to-end speech recognition [8].\nAlso, the input and output of our model are the same as those ofthe\ntraditional character-level RNNs, thus the same training algorithm\nand recipe can be used without any modiﬁcations. Furthermore, the\nproposed models have signiﬁcantly less number of parameters com-\npared to WLM-based ones, since the size of our model does not di-\nrectly depend on the vocabulary size of the training set. Note that\na similar hierarchical concept has been used for character-level ma-\nchine translation [15]. However, we propose more general hierarchi-\ncal unidirectional RNN architecture that can be applied forvarious\napplications.\n3. RNNS WITH EXTERNAL CLOCK AND RESET\nSIGNALS\nIn this section, we generalize the existing RNN structures and extend\nthem with external clocks and reset signals. The extended models\nbecome the basic building blocks of the hierarchical RNNs.\nMost types of RNNs or recurrent layers can be generalized as\nst = f(xt, st− 1) , yt = g(st) (1)\nwhere xt is the input,st is the state,yt is the output at time stept,\nf(·) is the recurrence function, andg(·) is the output function. For\nexample, a hidden layer of Elman networks [16] can be writtenas\nyt = st = ht = σ (Whxxt + Whhht− 1 + bh) (2)\nwhere ht is the activation of the hidden layer,σ (·) is the activation\nfunction,Whx and Whh are the weight matrices andbh is the bias\nvector.\nRNN RNN \nLevel 1 module Level 2 module \nShort-term \nembedding \nLong-term \ncontext \nInput \nOutput \nInput \nRNN \nLevel 3 module \nShort-term \nembedding \nLong-term \ncontext \nInput \nSlower clock rate \nFig. 2: Hierarchical RNN (HRNN).\nLSTMs [7] with forget gates [17] and peephole connections [18]\ncan also be converted to the generalized form. The forward equations\nof the LSTM layer are as follows:\nit = σ (Wixxt + Wihht− 1 + Wimmt− 1 + bi) (3)\nft = σ (Wf xxt + Wf hht− 1 + Wf mmt− 1 + bf ) (4)\nmt = ft ◦ mt− 1 + it ◦ tanh(Wmxxt + Wmhht− 1 + bm) (5)\not = σ (Woxxt + Wohht− 1 + Wommt + bo) (6)\nht = ot ◦ tanh(mt) (7)\nwhere it,ft, andot are the input, forget, and output gate values, re-\nspectively,mt is the memory cell state,ht is the output activation of\nthe layer,σ (·) is the logistic sigmoid function, and◦ is the element-\nwise multiplication operator. These equations can be generalized by\nsettingst = [mt, ht] and yt = ht.\nAny generalized RNNs can be converted to the ones that incor-\nporate an external clock signal,ct, as\nst = (1− ct)st− 1 + ctf(xt, st− 1) , yt = g(st) (8)\nwhere ct is 0 or 1. The RNN updates its state and output only when\nct = 1. Otherwise, whenct = 0, the state and output values remain\nthe same as those of the previous step.\nThe reset of RNNs is performed by settingst− 1 to 0. Speciﬁ-\ncally, (8) becomes\nst = (1− ct)(1 − rt)st− 1 + ctf(xt, (1 − rt)st− 1) (9)\nwhere the reset signalrt = 0 or1. When rt = 1, the RNN forgets\nthe previous contexts.\nIf the original RNN equations are differentiable, the extended\nequations with clock and reset signals are also differentiable. There-\nfore, the existing gradient-based training algorithms forRNNs, such\nas backpropagation through time (BPTT), can be employed fortrain-\ning the extended versions without any modiﬁcations.\n4. CHARACTER-LEVEL LANGUAGE MODELING WITH\nA HIERARCHICAL RNN\nThe proposed hierarchical RNN (HRNN) architectures have several\nRNN modules with different clock rates as depicted in Figure2. The\nhigher level module employs a slower clock rate than the lower mod-\nule, and the lower level module is reset at every clock of the higher\nlevel module. Speciﬁcally, if there areL hierarchy levels, then the\nRNN consists ofL submodules. Each submodulel operates with an\nexternal clockcl,t and a reset signalrl,t, wherel = 1, · · ·, L . The\nlowest level module,l = 1, has the fastest clock rate, that is,c1,t = 1\nfor allt. On the other hand, the higher level modules,l > 1, have\nslower clock rates andcl,t can be 1 only whencl− 1,t is 1. Also, the\nlower level modulesl < L are reset by the higher level clock signals,\nthat is,rl,t = cl+1,t.\nLSTM \nLSTM LSTM \nLSTM \nCharacter RNN Word RNN \nWord \nembedding \nContext \nCurrent character \nNext character \n(softmax) \nCurrent character \n(<w> or <s>) \nCurrent character \n(a) HLSTM-A\nLSTM \nLSTM LSTM \nLSTM \nCharacter RNN Word RNN \nWord \nembedding \nContext \nCurrent character \nNext character \n(softmax) \nCurrent character \n(<w> or <s>) \n(b) HLSTM-B\nFig. 3: Two-level hierarchical LSTM (HLSTM) structures for\nCLMs.\nThe hidden activations of a module,l < L , are fed to the next\nhigher level module,l + 1, delayed by one time stepto avoid un-\nwanted reset byrl,t = cl+1,t = 1. This hidden activation vector,\nor embedding vector, contains compressed short-term context infor-\nmation. The reset of the module by the higher level clock signals\nhelps the module to concentrate on compressing only the short term\ninformation, rather than considering longer dependencies. The next\nhigher level module,l + 1, process this short-term information to\ngenerate the long-term context vector, which is fed back to the lower\nlevel module,l. There is no delay for this context propagation.\nFor character-level language modeling, we use a two-level (L =\n2) HRNN with lettingl = 1be a character-level module andl = 2be\na word-level module. The word-level module is clocked at theword\nboundary input,<w>, which is usually a whitespace character. The\ninput and softmax output layer is connected to the character-level\nmodule, and the current word boundary token (e.g.<w>or <s>)\ninformation is given to the word-level module. Since this HRNNs\nhave a scalable architecture, we expect this HRNN CLM can be ex-\ntended for modeling sentence-level contexts by adding an additional\nsentence-level module,l = 3. In this case, the sentence-level clock,\nc3,t becomes 1 when the input character is a sentence boundary to-\nken <s>. Also, the word-level module should be clocked at both the\nword boundary input,<w>, and the sentence boundary input,<s>.\nIn this paper, the experiments are performed only with the two-level\nHRNN CLMs.\nWe propose two types of two-level HRNN CLM architectures.\nAs shown in Figure 3, both models have two LSTM layers per sub-\nmodule. Note that each connection has a weight matrix. In the\nHLSTM-A architecture, both LSTM layers in the character-level\nmodule receives one-hot encoded character input. Therefore, the\nsecond layer of the character-level module is a generative model\nconditioned by the context vector. On the other hand, in HLSTM-\nB, the second LSTM layer of the character-level module does not\nhave direct connection from the character inputs. Instead,a word\nembedding from the ﬁrst LSTM layer is fed to the second LSTM\nlayer, which makes the ﬁrst and second layers of the character-level\nmodule work together to estimate the next character probabilities\nwhen the context vector is given. The experimental results show that\nHLSTM-B is more efﬁcient for CLM applications.\nSince the character-level modules are reset by the word-boundary\ntoken (i.e.<w>or whitespace), the context vector from the word-\nlevel module is the only source for the inter-word context informa-\ntion. Therefore, the model is trained to generate the context vector\nthat contains useful information about the probability distribution of\nthe next word. From this perspective, the word-level modulein both\nHRNN CLM architectures can be considered as a word-level RNN\nTable 1: Perplexities of CLMs on the WSJ corpus\nModel Size # Params BPC Word PPL\nDeep LSTM 2x512 3.23 M 1.148 99.5\nDeep LSTM 4x512 7.43 M 1.132 93.3\nDeep LSTM 4x1024 29.54 M 1.101 82.4\nHLSTM-A 4x512 7.50 M 1.089 78.5\nHLSTM-B (no reset) 4x512 8.48 M 1.080 75.7\nHLSTM-B 4x512 8.48 M 1.073 73.6\nHLSTM-B 4x1024 33.74 M 1.058 69.2\nTable 2: Perplexities of WLMs on the WSJ corpus in the literature\nModel # Params PPL\nKN 5-gram (no count cutoffs) [26] - 80\nRNN-640 + ME 4-gram feature [26] 2 G 59\nLM, where the input is a word embedding vector and the output is\na compressed descriptor of the next word probabilities. Although\nthe proposed model consists of several RNN modules with different\ntimescales, these can be jointly trained by BPTT as described in\nSection 3.\n5. EXPERIMENTS\nThe proposed HRNN based CLMs are evaluated with two text\ndatasets: the Wall Street Journal (WSJ) corpus [19] and One Bil-\nlion Word Benchmark [12]. Also, we present an end-to-end speech\nrecognition example, where HLSTM CLMs are employed for preﬁx\ntree-based beam search decoding.\nThe RNNs are trained with truncated backpropagation through\ntime (BPTT) [20, 21]. Also, ADADELTA [22] and Nesterov mo-\nmentum [23] is applied for weight update. No regularizationmethod,\nsuch as dropout [24], is employed. The training is accelerated using\nGPUs by training multiple sequences in parallel [25].\n5.1. Perplexity\nIn this section, our models are compared with other WLMs in the lit-\nerature in terms of word-level perplexity (PPL). The word-level PPL\nof our models is directly converted from bits-per-character (BPC),\nwhich is the standard performance measure for CLMs, as follows:\nP P L = 2BP C× Nc\nNw (10)\nwhere Nc and Nw are the number of characters and words in a test\nset, respectively. Note that sentence boundary symbols (<s>) are\nalso regarded as characters and words.\n5.1.1. Wall Street Journal (WSJ) corpus\nThe Wall Street Journal (WSJ) corpus [19] is designed for train-\ning and benchmarking automatic speech recognition systems. For\nthe perplexity experiments, we used the non-verbalized punctuation\n(NVP) version of the LM training data inside the corpus. The dataset\nconsists of about 37 million words, where one percent of the total\ndata is held out for the ﬁnal evaluation and does not participate in\ntraining. All alphabets are converted to the uppercases.\nTable 3: Perplexities of the HRNN CLMs on the One Billion Word\nBenchmark\nModel Size # Params BPC Word PPL\nHLSTM-B 4x512 9.06 M 1.228 83.3\nHLSTM-B 4x1024 34.90 M 1.140 60.7\nTable 4: Perplexities of WLMs on the One Billion Word Benchmark\nin the literature\nModel # Params PPL\nSigmoid RNN-2048 [29] 4.1 G 68.3\nInterpolated KN-5, 1.1B n-grams [12] 1.76 G 67.6\nLightRNN [30] 41 M 66\nSparse non-negative matrix LM [31] 33 G 52.9\nRNN-1024 + ME 9-gram feature [12] 20 G 51.3\nCNN input + 2xLSTM-8192-1024 [5] 1.04 G 30.0\nTable 1 shows the perplexities of traditional mono-clock deep\nLSTM and HLSTM based CLMs on the held-out set. Note that\nthe sizeNxM means that the network consists ofN LSTM layers,\nwhere each layer containsM memory cells. The HLSTM models\nshow better perplexity performanes even when the number of LSTM\ncells or parameters is much smaller than that of the deep LSTMnet-\nworks. Especially, HLSTM-B network with the size of 4x512 has\nabout 9% lower perplexity than deep LSTM (4x1024) model, even\nwith only 29% of parameters.\nIt is important to reset the character-level modules at the word-\nlevel clocks for helping the character-level modules to better con-\ncentrate on the short-term information. As observed in Table 1, re-\nmoving the reset functionality of the character-level module of the\nHLSTM-B model results in degraded performance.\nThe non-ensemble perplexities of WLMs in the literature arepre-\nsented in Table 2. The Kneser-Ney (KN) smoothed 5-gram model\n(KN-5) [27] is a strong non-neural WLM baseline. With the standard\ndeep RNN based CLMs, it is very hard to beat KN-5 in terms of per-\nplexity. However, it is surprising that all HLSTM models in Table 1\nshows better perplexities than KN-5 does. The RNN based WLM\nmodel combined with the maximum entropy 4-gram feature [28,26]\nshows much better results than the proposed HLSTM based CLM\nmodels. However, like most of the WLMs, it also needs a very large\nnumber (2 G) of parameters and cannot handle out-of-vocabulary\n(OOV) words.\n5.1.2. One Billion Word Benchmark\nThe One Billion Word Benchmark [12] dataset contains about 0.8\nbillion words and roughly 800 thousand words of vocabulary.We\nfollowed the standard way of splitting the training and testdata as\nin [12]. Each byte of UTF-8 encoded text is regarded as a character.\nTherefore, the size of the character set is 256.\nDue to the large amount of training data and weeks of training\ntime, only two HLSTM-B experiments are conducted with the size\nof 4x512 and 4x1024. As shown in Table 3, there are large gap (22.5)\nin word-level perplexity between the two models. Therefore, further\nimprovement in perplexity can be expected with bigger networks.\nThe perplexities of other WLMs are summarized in Table 4.\nThe proposed HLSTM-B model (4x1024) shows better perplexities\nthan the interpolated KN-5 model with 1.1 billion n-grams [12] even\nTable 5: End-to-end ASR results on the WSJ Nov’92 20K evaluation\nset (eval92)\nModel Size # Params Word PPL WER\nDeep LSTM 4x512 7.43 M 93.3 8.36%\nDeep LSTM 4x1024 29.54 M 82.4 7.85%\nHLSTM-B 4x512 8.48 M 73.6 7.79%\nHLSTM-B 4x1024 33.74 M 69.2 7.78%\nthough the number of parameters of our model is only 2% of thatof\nthe KN-5 model. Also, our model performs better than LightRNN\n[30], which is a word-level RNN LM that has about 17% more pa-\nrameters than ours. However, much lower perplexities are reported\nwith sparse non-negative matrix LM and the maximum entropy fea-\nture based RNN model [12], where the number of parameters are33\nG and 20 G, respectively. Recently, the state of the art perplexity of\n30.0 was reported in [5] with a single model that has 1 G parame-\nters. The model is basically a very large LSTM LM. However, the\nconvolutional neural network (CNN) is used to generate wordem-\nbedding of arbitrary character sequences as the input of theLSTM\nLM. Therefore, this model can handle OOV word inputs, however,\nstill the model runs with a word-level clock.\n5.2. End-to-end automatic speech recognition (ASR)\nIn this section, we apply the proposed CLMs to the end-to-endau-\ntomatic speech recognition (ASR) system to evaluate the models in\nmore practical situation than just measuring perplexities. The CLMs\nare trained with WSJ LM training data as in Section 5.1.1. Unlike\nWLMs, the proposed CLMs have very small number of parameters,\nso they can be employed for real-time character-level beam search.\nThe incremental speech recognition system proposed in [8] is\nused for the evaluation. The acoustic model is 4x512 unidirectional\nLSTM and end-to-end trained with connectionist temporal classiﬁ-\ncation (CTC) loss [32] using the non-verbalized punctuation (NVP)\nportion of WSJ SI-284 training set. The acoustic features are 40-\ndimensional log-mel ﬁlterbank coefﬁcients, energy and their delta\nand double-delta values, which are extracted every 10 ms with 25\nms Hamming window. The beam-search decoding is performed ona\npreﬁx-tree with depth-pruning and width-pruning [8]. The insertion\nbonus is 1.6, the LM weight is 2.0, and the beam width is 512.\nThe results are summarized in Table 5. It is observed that the\nperplexity of LM and the word error rate (WER) have strong cor-\nrelation. As shown in the table, we can achieve a better WER by\nreplacing the traditional deep LSTM (4x1024) CLM with the pro-\nposed HLSTM-B (4x512) CLM, while reducing the number of LM\nparameters to 30%.\n6. CONCLUDING REMARKS\nIn this paper, hierarchical RNN (HRNN) based CLMs are proposed.\nThe HRNN consists of several submodules with different clock\nrates. Therefore, it is capable of learning long-term dependencies as\nwell as short-term details. The experimental results on OneBillion\nBenchmark show that HLSTM-B networks signiﬁcantly outperform\nKneser-Ney 5-gram LMs with only 2% of parameters. Although\nother RNN-based WLMs show better performance than our models,\nthey have impractically many parameters. On the other hand,as\nshown in the WSJ speech recognition example, the proposed model\ncan be employed for the real-time speech recognition with less than\n10 million parameters. Also, CLMs can handle OOV words by\nnature, which is a great advantage for the end-to-end speechrecog-\nnition and many NLP tasks. One of the interesting future workis to\ntrain the clock signals, instead of using manually designedones.\n7. REFERENCES\n[1] Lawrence Rabiner and Biing-Hwang Juang,Fundamentals of\nspeech recognition, Prentice Hall, 1993.\n[2] Ilya Sutskever, James Martens, and Geoffrey E Hinton, “Gen-\nerating text with recurrent neural networks,” inProceedings\nof the 28th International Conference on Machine Learning\n(ICML-11), 2011, pp. 1017–1024.\n[3] Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent\nJ Della Pietra, Fredrick Jelinek, John D Lafferty, Robert L Mer-\ncer, and Paul S Roossin, “A statistical approach to machine\ntranslation,”Computational linguistics, vol. 16, no. 2, pp. 79–\n85, 1990.\n[4] Tomas Mikolov, Martin Karaﬁ´ at, Lukas Burget, Jan Cernock` y,\nand Sanjeev Khudanpur, “Recurrent neural network based lan-\nguage model.,” inProc. Interspeech, 2010, vol. 2, p. 3.\n[5] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu, “Exploring the limits of language\nmodeling,”arXiv preprint arXiv:1602.02410, 2016.\n[6] Michiel Hermans and Benjamin Schrauwen, “Training and\nanalysing deep recurrent neural networks,” inAdvances in Neu-\nral Information Processing Systems, 2013, pp. 190–198.\n[7] Sepp Hochreiter and J¨ urgen Schmidhuber, “Long short-term\nmemory,” Neural computation, vol. 9, no. 8, pp. 1735–1780,\n1997.\n[8] Kyuyeon Hwang and Wonyong Sung, “Character-level incre-\nmental speech recognition with recurrent neural networks,” in\n2016 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2016.\n[9] John S Bridle, “Probabilistic interpretation of feedforward clas-\nsiﬁcation network outputs, with relationships to statistical pat-\ntern recognition,” inNeurocomputing, pp. 227–236. Springer,\n1990.\n[10] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M\nRush, “Character-aware neural language models,” inThirtieth\nAAAI Conference on Artiﬁcial Intelligence, 2016.\n[11] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice\nSantorini, “Building a large annotated corpus of english: The\npenn treebank,”Computational linguistics, vol. 19, no. 2, pp.\n313–330, 1993.\n[12] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robinson, “One\nbillion word benchmark for measuring progress in statistical\nlanguage modeling,”arXiv preprint arXiv:1312.3005, 2013.\n[13] Wang Ling, Tiago Lu´ ıs, Lu´ ıs Marujo, Ram´ on FernandezAs-\ntudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel\nTrancoso, “Finding function in form: Compositional charac-\nter models for open vocabulary word representation,” in2015\nConference on Empirical Methods in Natural Language Pro-\ncessing, 2015, pp. 1520–1530.\n[14] Yasumasa Miyamoto and Kyunghyun Cho, “Gated word-\ncharacter recurrent language model,” in2016 Conference on\nEmpirical Methods in Natural Language Processing, 2016, pp.\n1992–1997.\n[15] Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W Black,\n“Character-based neural machine translation,” inProceedings\nof the 54th Annual Meeting of the Association for Computa-\ntional Linguistics, 2016, vol. 357–361.\n[16] Jeffrey L Elman, “Finding structure in time,”Cognitive Sci-\nence, vol. 14, no. 2, pp. 179–211, 1990.\n[17] Felix A Gers, J¨ urgen Schmidhuber, and Fred Cummins,\n“Learning to forget: Continual prediction with LSTM,”Neu-\nral Computation, vol. 12, no. 10, pp. 2451–2471, 2000.\n[18] Felix A Gers, Nicol N Schraudolph, and J¨ urgen Schmidhuber,\n“Learning precise timing with LSTM recurrent networks,”The\nJournal of Machine Learning Research, vol. 3, pp. 115–143,\n2003.\n[19] Douglas B Paul and Janet M Baker, “The design for the Wall\nStreet Journal-based CSR corpus,” inProceedings of the work-\nshop on Speech and Natural Language. Association for Com-\nputational Linguistics, 1992, pp. 357–362.\n[20] Paul J Werbos, “Backpropagation through time: what it does\nand how to do it,”Proceedings of the IEEE, vol. 78, no. 10, pp.\n1550–1560, 1990.\n[21] Ronald J Williams and Jing Peng, “An efﬁcient gradient-based\nalgorithm for on-line training of recurrent network trajectories,”\nNeural Computation, vol. 2, no. 4, pp. 490–501, 1990.\n[22] Matthew D Zeiler, “ADADELTA: An adaptive learning rate\nmethod,”arXiv preprint arXiv:1212.5701, 2012.\n[23] Yurii Nesterov, “A method of solving a convex programming\nproblem with convergence rate O (1/k2),”Soviet Mathematics\nDoklady, vol. 27, no. 2, pp. 372–376, 1983.\n[24] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya\nSutskever, and Ruslan R Salakhutdinov, “Improving neural net-\nworks by preventing co-adaptation of feature detectors,”arXiv\npreprint arXiv:1207.0580, 2012.\n[25] Kyuyeon Hwang and Wonyong Sung, “Single stream paral-\nlelization of generalized LSTM-like RNNs on a GPU,” in2015\nIEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP). IEEE, 2015, pp. 1047–1051.\n[26] Tom´ aˇ s Mikolov,Statistical language models based on neural\nnetworks, Ph.D. thesis, Brno University of Technology, 2012.\n[27] Reinhard Kneser and Hermann Ney, “Improved backing-off\nfor m-gram language modeling,” in1995 International Confer-\nence on Acoustics, Speech, and Signal Processing (ICASSP).\nIEEE, 1995, vol. 1, pp. 181–184.\n[28] Tomas Mikolov and Geoffrey Zweig, “Context dependent re-\ncurrent neural network language model,” in2012 IEEE Spoken\nLanguage Technology Workshop, 2012, pp. 234–239.\n[29] Shihao Ji, SVN Vishwanathan, Nadathur Satish, MichaelJ An-\nderson, and Pradeep Dubey, “BlackOut: Speeding up recurrent\nneural network language models with very large vocabularies,”\nin4th International Conference on Learning Representations,\n2016.\n[30] Xiang Li, Tao Qin, Jian Yang, Xiaolin Hu, and Tieyan Liu,\n“LightRNN: Memory and computation-efﬁcient recurrent neu-\nral networks,” inAdvances in Neural Information Processing\nSystems, 2016, pp. 4385–4393.\n[31] Noam Shazeer, Joris Pelemans, and Ciprian Chelba, “Sparse\nnon-negative matrix language modeling for skip-grams,” in\nProc. Interspeech, 2015, pp. 1428–1432.\n[32] Alex Graves, Santiago Fern´ andez, Faustino Gomez, andJ¨ urgen\nSchmidhuber, “Connectionist temporal classiﬁcation: la-\nbelling unsegmented sequence data with recurrent neural net-\nworks,” inProceedings of the 23rd International Conference\non Machine Learning. ACM, 2006, pp. 369–376.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9667402505874634
    },
    {
      "name": "Language model",
      "score": 0.8121011853218079
    },
    {
      "name": "Recurrent neural network",
      "score": 0.7865586280822754
    },
    {
      "name": "Character (mathematics)",
      "score": 0.7820650339126587
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7617385983467102
    },
    {
      "name": "Computer science",
      "score": 0.7557730674743652
    },
    {
      "name": "Vocabulary",
      "score": 0.5705103278160095
    },
    {
      "name": "Word (group theory)",
      "score": 0.5314936637878418
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5187118649482727
    },
    {
      "name": "Speech recognition",
      "score": 0.4324149191379547
    },
    {
      "name": "Artificial neural network",
      "score": 0.42739158868789673
    },
    {
      "name": "Natural language processing",
      "score": 0.386410653591156
    },
    {
      "name": "Linguistics",
      "score": 0.11239954829216003
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 7
}