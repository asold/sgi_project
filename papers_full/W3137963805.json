{
  "title": "DeepViT: Towards Deeper Vision Transformer",
  "url": "https://openalex.org/W3137963805",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2563060429",
      "name": "Zhou, Daquan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2963182374",
      "name": "Kang, Bingyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350479366",
      "name": "Jin, Xiaojie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350596567",
      "name": "Yang, Linjie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2473987630",
      "name": "Lian, Xiaochen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202207709",
      "name": "Jiang, Zihang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3174905547",
      "name": "Hou, Qibin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2594576826",
      "name": "Feng, Jiashi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3047171714",
    "https://openalex.org/W3133954504",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963037463",
    "https://openalex.org/W3122317902",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2964189064",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W3034752215",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2401231614",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963512530",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2163922914",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code is publicly available at https://github.com/zhoudaquan/dvit_repo.",
  "full_text": "DeepViT: Towards Deeper Vision Transformer\nDaquan Zhou1, Bingyi Kang1, Xiaojie Jin2, Linjie Yang2,\nXiaochen Lian2, Zihang Jiang1, Qibin Hou1, Jiashi Feng1\n1National University of Singapore, 2ByteDance US AI Lab\n{zhoudaquan21, xjjin0731, lianxiaochen, yljatthu, andrewhoux}@gmail.com\njzihang, kang, elefjia@nus.edu.sg\nAbstract\nVision transformers (ViTs) have been successfully ap-\nplied in image classiﬁcation tasks recently. In this paper,\nwe show that, unlike convolution neural networks (CNNs)\nthat can be improved by stacking more convolutional lay-\ners, the performance of ViTs saturate fast when scaled to\nbe deeper. More speciﬁcally, we empirically observe that\nsuch scaling difﬁculty is caused by the attention collapse\nissue: as the transformer goes deeper, the attention maps\ngradually become similar and even much the same after\ncertain layers. In other words, the feature maps tend to\nbe identical in the top layers of deep ViT models. This\nfact demonstrates that in deeper layers of ViTs, the self-\nattention mechanism fails to learn effective concepts for\nrepresentation learning and hinders the model from get-\nting expected performance gain. Based on above obser-\nvation, we propose a simple yet effective method, named\nRe-attention, to re-generate the attention maps to increase\ntheir diversity at different layers with negligible computa-\ntion and memory cost. The proposed method makes it feasi-\nble to train deeper ViT models with consistent performance\nimprovements via minor modiﬁcation to existing ViT mod-\nels. Notably, when training a deep ViT model with 32 trans-\nformer blocks, the Top-1 classiﬁcation accuracy can be im-\nproved by 1.6% on ImageNet. Code is publicly available at\nhttps://github.com/zhoudaquan/dvit_repo.\n1. Introduction\nRecent studies [7, 37] have demonstrated that transform-\ners [38] can be successfully applied to vision tasks [18] with\ncompetitive performance compared with convolutional neu-\nral networks (CNNs) [9, 35]. Different from CNNs that\naggregate global information by stacking multiple convo-\nlutions ( e.g., 3 ×3) [9, 10], vision transformers (ViTs)\n[7] take advantages of the self-attention (SA) mechanism\n[38] to capture spatial patterns and non-local dependen-\ncies. This allows ViTs to aggregate rich global information\n[7]\nFigure 1: Top-1 classiﬁcation performance of vision transform-\ners (ViTs) [7] on ImageNet with different network depth {12, 16,\n24, 32}. Directly scaling the depth of ViT by stacking more trans-\nformer blocks cannot monotonically increase the performance. In-\nstead, the model performance saturates when going deeper. In con-\ntrast, with the proposed Re-attention, our DeepViT model success-\nfully achieves better performance when it goes deeper.\nwithout handcrafting layer-wise local feature extractions as\nCNNs and thus achieves better performance. For example,\nas shown in [37], a 12-block ViT model with 22M learn-\nable parameters achieves better results than the ResNet-101\nmodel which has more than 30 bottleneck convolutional\nblocks in ImageNet classiﬁcation.\nThe recent progress of deep CNN models is largely\ndriven by training very deep models with a large number\nof layers which is enabled by novel model architecture de-\nsigns [9, 41, 31, 20, 47]. This is because a deeper CNN can\nlearn richer and more complex representations for the in-\nput images and provide better performance on vision tasks\n[1, 45, 29]. Thus, how to effectively scale CNNs to be\ndeeper is an important theme in recent deep learning ﬁelds,\nwhich stimulates the techniques like residual learning [9].\nConsidering the remarkable performance of shallow\narXiv:2103.11886v4  [cs.CV]  19 Apr 2021\nViTs, a natural question arises: can we further improve\nperformance of ViTs by making it deeper, just like CNNs?\nThough it seems to be straightforward at the ﬁrst glance,\nthe answer may not be trivial since ViT is essentially dif-\nferent from CNNs in its heavy reliance on the self-attention\nmechanism. To settle the question, we investigate in detail\nthe scalability of ViTs along depth in this work.\nWe start with a pilot study on ImageNet to investigate\nhow the performance of ViT changes with increased model\ndepth. In Fig. 1, we show the performance of ViTs [7] with\ndifferent block numbers (green line), ranging from 12 to 32.\nAs shown, as the number of transformer blocks increases,\nthe model performance does not improve accordingly. To\nour surprise, the ViT model with 32 transformer blocks per-\nforms even worse than the one with 24 blocks. This means\nthat directly stacking more transformer blocks as performed\nin CNNs [9] is inefﬁcient at enhancing ViT models. We\nthen dig into the cause of this phenomenon. We empirically\nobserved that as the depth of ViTs increases, the attention\nmaps, used for aggregating the features for each transformer\nblock, tend to be overly similar after certain layers, which\nmakes the representations stop evolving after certain layers.\nWe name this speciﬁc issue as attention collapse. This in-\ndicates that as the ViT goes deeper, the self-attention mech-\nanism becomes less effective for generating diverse atten-\ntions to capture rich representations.\nTo resolve the attention collapse issue and effectively\nscale the vision transformer to be deeper, We present a sim-\nple yet effective self-attention mechanism, named as Re-\nattention. Our Re-attention takes advantage of the multi-\nhead self-attention(MHSA) structure and regenerates atten-\ntion maps by exchanging the information from different at-\ntention heads in a learnable manner. Experiments show that,\nWithout any extra augmentation and regularization poli-\ncies, simply replacing the MHSA module in ViTs with Re-\nattention allows us to train very deep vision transformers\nwith even 32 transformer blocks with consistent improve-\nments as shown in Fig. 1. In addition, we also provide ab-\nlation analysis to help better understand of the role of Re-\nattention in scaling vision transformers.\nTo sum up, our contributions are as follows:\n• We deeply study the behaviour of vision transformers\nand observe that they cannot continuously beneﬁt from\nstacking more layers as CNNs. We further identify\nthe underlying reasons behind such a counter-intuitive\nphenomenon and conclude it as attention collapse for\nthe ﬁrst time.\n• We present Re-attention, a simple yet effective atten-\ntion mechanism that considers information exchange\namong different attention heads.\n• To the best of our knowledge, we are the ﬁrst to suc-\ncessfully train a 32-block ViT on ImageNet-1k from\nscratch with consistent performance improvement. We\nshow that by replacing the self-attention module with\nour Re-attention, new state-of-the-art results can be\nachieved on the ImageNet-1k dataset without any pre-\ntraining on larger datasets.\n2. Related Work\n2.1. Transformers for Vision Tasks\nTransformers [38] are initially used for machine transla-\ntion which replace the recurrence and convolutions entirely\nwith self-attention mechanisms [28, 14, 48, 11, 39, 17] and\nachieve outstanding performance. Later, transformers be-\ncome the dominant models for various natural language pro-\ncessing (NLP) tasks [2, 26, 6, 21]. Motivated by their suc-\ncess on the NLP tasks, recent researchers attempted to com-\nbine the self-attention mechanism into CNNs for computer\nvision tasks [40, 3, 4, 32, 24, 50, 49].. Those achievements\nalso stimulate interests of the community in building purely\ntransformer-based models (without convolutions and induc-\ntive bias) for vision tasks. The vision transformer (ViT) [7]\nis among the ﬁrst attempt that uses the pure transformer ar-\nchitecture to achieve competitive performance with CNNs\non the image classiﬁcation task. However, due to the large\nmodel complexity, ViT needs to be pre-trained on larger-\nscale datasets (e.g., JFT300M) for performing well on the\nImageNet-1k dataset. To solve the data efﬁciency issue,\nDeiT [37] deploys knowledge distillation to train the model\nwith a larger pre-trained teacher model. In this manner, vi-\nsion transformer can perform well on ImageNet-1k without\nthe need of pre-training on larger dataset. Differently, in this\nwork, we target at a different problem with ViT,i.e., how to\neffectively scale ViT to be deeper. We propose a new de-\nsign for the self-attention mechanism so that it can perform\nwell on vision tasks without the need of extra data, teacher\nnetworks, and the domain speciﬁc inductive bias.\n2.2. Depth Scaling of CNNs\nIncreasing the network depth of a CNN model is deemed\nto be an effective way to improve the model performance\n[30, 33, 34, 10, 15]. However, very deep CNNs are gener-\nally harder to train to perform signiﬁcantly better than the\nshallow ones in the past [8, 44]. How to effectively scale\nup the CNNs in depth was a long-standing and challeng-\ning problem [16]. The recent progress of CNNs largely\nbeneﬁts from novel architecture design strategies that make\ntraining deep CNNs more effective [9, 35, 13, 12, 36, 52].\nTransformer-alike models have modularized architectures\nand thus can be easily made deeper by repeating the basic\ntransformer blocks or using larger embedding dimensions\n[2, 19]. However, those straightforward scaling strategies\nonly work well with larger datasets and stronger augmen-\ntation policies [51, 46, 43] to alleviate the brought training\nSelf-Attention\nNorm\nFeed Foward\nNorm\nAdd\nAdd\nPos. Embed\nPatch Embed\nInput\nLinear + Loss\n×N\nRe-Attention \nNorm\nFeed Foward\nNorm\nAdd\nAdd\nPos. Embed\nPatch Embed\nInput\nLinear + Loss\n×N\n(a) (b)\nTransformer block with self-attention\nTransformer block with Re-attention\nFigure 2: Comparison between the (a) original ViT with N trans-\nformer blocks and (b) our proposed DeepViT model. Different\nfrom ViT, DeepViT replaces the self-attention layer within the\ntransformer block with the proposed Re-attention which effec-\ntively addresses the attention collapse issue and enables training\ndeeper ViTs. More details are given in Sec. 4.2.\ndifﬁculties. In this paper, we observed that with the same\ndataset, the performance of vision transformers do saturate\nas the network depth rises. We rethink the self-attention\nmechanism and present a simple but effective approach to\naddress the difﬁculties in scaling vision transformers.\n3. Revisiting Vision Transformer\nA vision transformer (ViT) model [37, 7], as depicted\nin Fig. 2(a), is composed of three main components: a\nlinear layer for patch embedding ( i.e., mapping the high-\nresolution input image to a low-resolution feature map), a\nstack of transformer blocks with multi-head self-attention\nand feed-forward layers for feature encoding, and a linear\nlayer for classiﬁcation score prediction. In this section, we\nﬁrst review its unique transformer blocks, in particular the\nself-attention mechanism, and then we provide studies on\nthe collapse problem of self-attention.\n3.1. Multi-Head Self-Attention\nTransformers [38] were extensively used in natural lan-\nguage for encoding a sequence of input word tokens into a\nsequence of embeddings. To comply with such sequence-\nto-sequence learning structure when processing images,\nViTs ﬁrst divide an input image into multiple patches uni-\nformly and encode each patch into a token embedding.\nThen, all these tokens, together with a class token, are fed\ninto a stack of transformer blocks.\nEach transformer block consists of a multi-head self-\nattention (MHSA) layer and a feed-forward multi-layer per-\nceptron (MLP). The MHSA generates a trainable associate\nmemory with a query ( Q) and a pair of key ( K)-value (V)\npairs to an output via linearly transforming the input. Math-\nematically, the output of a MHSA is calculated by:\nAttention(Q,K,V ) = Softmax(QK⊤/\n√\nd)V, (1)\nwhere\n√\ndis a scaling factor based on the depth of the net-\nwork. The output of the MHSA is then normalized and fed\ninto the MLP to generate the input to the next block. In\nthe above self-attention, Q and K are multiplied to gen-\nerate the attention map, which represents the correlation\nbetween all the tokens within each layer. It is used to re-\ntrieve and combine the embeddings in the value V. In\nthe following, we particularly analyze the role of the at-\ntention map in scaling the ViT. For convenience, we use\nA ∈ RH×T×T to denote the attention map, with H be-\ning the number of SA heads and T the number of tokens.\nFor the h-th SA head, the attention map is computed as\nAh,:,: = Softmax(QhKh\n⊤/\n√\nd) with Qh and Kh from the\ncorresponding head. When the context is clear, we omit the\nsubscript h.\n3.2. Attention Collapse\nMotivated by the success of deep CNNs [9, 30, 35, 36],\nwe conduct systematic study in the changes of the perfor-\nmance of ViTs as depth increases. Without loss of gen-\nerality, we ﬁrst ﬁx the hidden dimension and the number\nof heads to 384 and 12 respectively 1, following the com-\nmon practice in [37]. Then we stack different number of\ntransformer blocks (varying from 12 to 32) to build multiple\nViT models corresponding to different depths. The overall\nperformances for image classiﬁcation are evaluated on Im-\nageNet [18] and summarized in Fig. 1. As evidenced by\nthe performance curve, we surprisingly ﬁnd that the clas-\nsiﬁcation accuracy improves slowly and saturates fast as\nthe model goes deeper. More speciﬁcally, we can observe\nthat the improvement stops after employing 24 transformer\nblocks. This phenomenon demonstrates that existing ViTs\nhave difﬁculty in gaining beneﬁts from deeper architectures.\nSuch a problem is quite counter-intuitive and worth ex-\nploration, as similar issues ( i.e., how to effectively train a\ndeeper model) have also been observed for CNNs at its early\ndevelopment stage [9], but properly solved later [9, 10]. By\ntaking a deeper look into the transfromer architecture, we\nwould like to highlight that the self-attention mechanism\nplays a key role in ViTs, which makes it signiﬁcantly differ-\nent from CNNs. Therefore, we start with investigating how\nthe self-attention, or more concretely, the generated atten-\ntion map A varies as the model goes deeper.\n1Similar phenomenon can also be found when we vary the hidden di-\nmension size according to our experiments.\nFigure 3: (a) The similarity ratio of the generated self-attention maps across different layers. The visualization is based on ViT models with\n32 blocks pre-trained on ImageNet. For visualization purpose, we plot the ratio of token-wise attention vectors with similarity in Eqn. (2)\nlarger than the average similarity within nearest ktransformer blocks. As can be seen, the similarity ratio is larger than 90% for blocks\nafter the 17th one. (b) The ratio of similar blocks to the total number of blocks increases when the depth of the ViT model increases. (c)\nSimilarity of attention maps from different heads within the same block. The similarity between different heads within the blocks is all\nlower than 30% and they present sufﬁcient diversity.\nTo measure the evolution of the attention maps over lay-\ners, we compute the following cross-layer similarity be-\ntween the attention maps from different layers:\nMp,q\nh,t =\nAp\nh,:,t\n⊤Aq\nh,:,t\n∥Ap\nh,:,t∥∥Aq\nh,:,t∥, (2)\nwhere Mp,q is the cosine similarity matrix between the at-\ntention map of layerspand q. Each element Mp,q\nh,t measures\nthe similarity of attention for head hand token t. Consider\none speciﬁc self-attention layer and its h-th head, A∗\nh,:,t is\na T-dimensional vector representing how much the input\ntoken t contributes to each of the T output tokens. Mp,q\nh,t ,\nthus, provides an appropriate measurement on how the con-\ntribution of one token varies from layer pto q. When Mp,q\nh,t\nequals one, it means that tokentplays exactly the same role\nfor self-attention in layers pand q.\nGiven Eqn. (2), we then train a ViT model with 32 trans-\nformer blocks on ImageNet-1k and investigate the above\nsimilarity among all the attention maps. As shown in\nFig. 3(a), the ratio of similar attention maps in M after\nthe 17th block is larger than 90% . This indicates that the\nlearned attention maps afterwards are similar and the trans-\nformer block may degenerate to an MLP. As a result, fur-\nther stacking such degenerated MHSA may introduce the\nmodel rank degeneration issue ( i.e., the rank of the model\nparameter tensor from multiplying the layer-wise parame-\nters together will decrease) and limits the model learning\ncapacity. This is also validated by our analysis on the degen-\neration of learned features as shown below. Such observed\nattention collapse could be one of the reasons for the ob-\nserved performance saturation of ViTs. To further validate\nthe existence of this phenomenon for ViTs with different\ndepths, we conduct the same experiments on ViTs with 12,\n16, 24 and 32 transformer blocks respectively and calculate\nthe number of blocks with similar attention maps. The re-\nFigure 4: ( Left): Cross layer similarity of attention map and fea-\ntures for ViTs. The black dotted line shows the cosine similarity\nbetween feature maps of the last block and each of the previous\nblocks. The red dotted line shows the ratio of similar attention\nmaps of adjacent blocks. The visualization is based on a 32-block\nViT model pre-trained on ImageNet-1k. ( Right): Feature map\ncross layer cosine similarity for both the original ViT model and\nours. As can be seen, replacing the original self-attention with\nRe-attention could reduce the feature map similarity signiﬁcantly.\nsults shown in Fig. 3(b) clearly demonstrate the ratio of the\nnumber of similar attention map blocks to the total number\nof blocks increases when adding more transformer blocks.\nTo understand how the attention collapse may hurt the\nViT model performance, we further study how it affects fea-\nture learning of the deeper layers. For a speciﬁc 32-block\nViT model, we compare the ﬁnal output features with the\noutputs of each intermediate transformer block by investi-\ngating their cosine similarity. The results in Fig. 4 demon-\nstrate that the similarity is quite high and the learned fea-\ntures stop evolving after the 20th block. There is a close\ncorrelation between the increase of attention similarity and\nfeature similarity. This observation indicates that attention\ncollapse is responsible for the non-scalable issue of ViTs.\n4. Re-attention for Deep ViT\nAs revealed above, one major obstacle in scaling up ViT\nto a deeper one is the attention collapse problem. In this sec-\nFigure 5: Impacts of embedding dimension on the similarity\nof generated self-attention map across layers. As can be seen,\nthe number of similar attention maps decreases with increasing\nembedding dimension. However, the model size also increases\nrapidly.\nTable 1: Top-1 accuracy on ImageNet-1k dataset of vision trans-\nformer with different embedding dimensions. The number of\nmodel parameters increase quadratically with the embedding di-\nmension. The number of similar attention map blocks with differ-\nent embedding dimensions are shown in Figure 5\n.\nModel #Blocks Embed Dim. #Param. (M) Top-1 Acc.(%)\nViT 12 256 8.15 74.6\nViT 12 384 18.51 77.86\nViT 12 512 33.04 78.8\nViT 12 768 86 79.3\ntion, we present two solution approaches, one is to increase\nthe hidden dimension for computing self-attention and the\nother one is a novel re-attention mechanism.\n4.1. Self-Attention in Higher Dimension Space\nOne intuitive solution to conquer attention collapse is to\nincrease the embedding dimension of each token. This will\naugment the representation capability of each token embed-\nding to encode more information. As such, the resultant\nattention maps can be more diverse and the similarity be-\ntween each block’s attention map could be reduced. With-\nout loss of generality, we verify this approach empirically\nby conducting a set of experiments based on ViT models\nwith 12 blocks for quick experiments. Following previous\ntransformer based works [38, 7], four embedding dimen-\nsions are selected, ranging from 256 to 768. The detailed\nconﬁgurations and the results are shown in Tab. 1.\nFrom Fig. 5 and Tab. 1, one can see that the number of\nblocks with similar attention maps is reduced and the at-\ntention collapse is alleviated by increasing the embedding\ndimension. Consequently, the model performance is also\nincreased accordingly. This validates our core hypothe-\nsis—the attention collapse is the main bottleneck for scaling\nViT. Despite its effectiveness, increasing the embedding di-\nmension also increases the computation cost signiﬁcantly\nand the brought performance improvement tends to dimin-\nish. Besides, a larger model (with higher embedding di-\nmension) typically needs more data for training, suffering\nthe over-ﬁtting risk and decreased efﬁciency.\n4.2. Re-attention\nIt has been demonstrated in Sec. 3 that the similarity be-\ntween attention maps across different transformer blocks is\nhigh, especially for deep layers. However, we ﬁnd the sim-\nilarity of attention maps from different heads of the same\ntransformer block is quite small, as shown in Fig. 3(c).\nClearly, different heads from the same self-attention layer\nfocus on different aspects of the input tokens. Based on this\nobservation, we propose to establish cross-head communi-\ncation to re-generate the attention maps and train deep ViTs\nto perform better.\nConcretely, we use the attention maps from the heads\nas basis and generate a new set of attention maps by dy-\nnamically aggregating them. To achieve this, we deﬁne a\nlearnable transformation matrix Θ ∈RH×H and then use it\nto mix the multi-head attention maps into re-generated new\nones, before being multiplied with V. Speciﬁcally, the Re-\nattention is implemented by:\nRe-Attention(Q,K,V ) = Norm(Θ⊤(Softmax(QK⊤\n√\nd\n)))V,\n(3)\nwhere transformation matrix Θ is multiplied to the self-\nattention map A along the head dimension. Here Norm\nis a normalization function used to reduced the layer-wise\nvariance. Θ is end-to-end learnable.\nAdvantages: The advantages of the proposed Re-attention\nare two-fold. First of all, compared with other possible at-\ntention augmentation methods, such as randomly dropping\nsome elements of the attention map or tuning SoftMax tem-\nperature, our Re-attention exploits the interactions among\ndifferent attention heads to collect their complementary in-\nformation and better improves the attention map diversity.\nThis is also veriﬁed by our following experiments. Further-\nmore, our Re-attention is effective and easy to implement. It\nneeds only a few lines of code and negligible computational\noverhead compared to the original self-attention. Thus it is\nmuch more efﬁcient than the approach of increasing embed-\nding dimension.\n5. Experiments\nIn this section, we ﬁrst conduct experiments to further\ndemonstrate the attention collapse problem. Then, we give\nextensive ablation analysis to show the advantages of the\nproposed Re-attention. By incorporating Re-attention into\nthe transformers, we design two modiﬁed version of vi-\nsion transformers and name them as deep vision transform-\ners (DeepViT). Finally, we compare the proposed DeepViT\nmodels against the latest state-of-the-arts (SOTA).\n5.1. Experiment Details\nTo make a fair comparison, we ﬁrst tuned a set of pa-\nrameters for training the ViT base model and then use the\nsame set of hyper-parameters for all the ablation experi-\nments. Speciﬁcally, we use AdamW optimizer [23] and co-\nsine learning rate decay policy with an initial learning rate\nof 0.0005. We use 8 Telsa-V100 GPUs and train the model\nfor 300 epochs using Pytorch [25] library. The batch size is\nset to 256. We use 3 epochs for learning rate warm-up [22].\nWe also use some augmentation techniques such as mixup\n[46] and random augmentation [5] to boost the performance\nof baseline models following [47]. When comparing with\nother methods, we adopt the same set of hyper-parameters\nas used by the target models. We report results on the Im-\nageNet dataset [18]. For all experiments, the image size is\nset to be 224 ×224. To study the scaling capability of cur-\nrent transformer blocks, we set the embedding dimension to\n384 and the expansion ratio 3 for the MLP layers. We use\n12 heads for all the models. More detailed conﬁgurations\nare shown in Tab. 2.\nTable 2: Baseline model speciﬁcations. All ablation experiments\nare based on the ViT models with different number of blocks. The\n‘#B’ in ‘ViT-#B’ denotes the number of transformer blocks in the\nmodel.\nModel #Blocks #Embeddings MLP Size Params. (M)\nViT-16B 16 384 1152 24.46\nViT-24B 24 384 1152 36.26\nViT-32B 32 384 1152 48.09\n5.2. More Analysis on Attention Collapse\nIn this section, we show more analysis on the attention\nmap similarity and study how the collapsed attention maps\naffect the model performance.\nAttention reuse: As discussed above, when the model goes\ndeeper, the attention maps of the deeper blocks become\nhighly similar. This implies that adding more blocks on a\ndeep ViT model may not improve the model performance.\nTo further verify this claim, we design an experiment to\nreuse the attention maps computed at an early block of ViT\nto replace the ones after it. Speciﬁcally, we run experi-\nments on the ViT models with 24 blocks and 32 blocks\nbut share the Q and K values (and the resulted attention\nmaps) of the last “unique” block to all the blocks afterwards.\nThe “unique” block is deﬁned as the block whose attention\nmap’s similarity ratio with adjacent layers is smaller than\n90%. More implementation details can be found in the sup-\nplementary material. The results are shown in Tab. 3. Sur-\nprisingly, for a ViT model with 32 transformer blocks, when\nwe use the same Qand K values for the last 15 blocks, the\nperformance degradation is negligible. This implies the at-\ntention collapse problem indeed exists and reveals the inef-\nﬁcacy in adding more blocks when the model is deep.\nTable 3: ImageNet top-1 accuracy of the ViT models with shared\nself-attention maps. ‘#Shared blocks’ denotes the number of the\ntransformer blocks that share the same attention map.\n#Blocks #Embeddings #Shared blocks Top-1 Acc. (%)\n24 384 0 79.3\n24 384 11 78.7\n32 384 0 79.2\n32 384 15 79.2\nVisualization: To more intuitively understand the attention\nmap collapse across layers, we visualize the learned atten-\ntion maps from different blocks of the original ViT [7]. We\ntake a 32-block ViT model as an example and pre-train it\non ImageNet. The visualization of the attention maps with\noriginal MHSA and Re-attention are shown in Fig. 6. It can\nbe observed that the original MHSA learns the local rela-\ntionship among the adjacent patches in the shallow blocks\nand the attention maps tend to expand to cover more patches\ngradually. In the deep blocks, the MHSA learns nearly uni-\nform global attention maps with high similarity. Differently,\nafter implementing Re-attention, the attention maps at deep\nblocks keep the diversity and have small similarities from\nadjacent blocks.\n5.3. Analysis on Re-attention\nIn this subsection, we present two straightforward mod-\niﬁcations to the current self-attention mechanism as base-\nlines. We then conduct a series of comparison experiments\nto show the advantages of our proposed Re-attention.\nRe-attention v.s. Self-attention: We ﬁrst evaluate the ef-\nfectiveness of Re-attention by comparing to the pure ViT\nmodels using the same set of training hyper-parameters. We\ndirectly replace the self-attention module in ViT with Re-\nattention and show the results in Tab. 4 with different num-\nber of transformer blocks. As can be seen, the vanilla ViT\narchitecture suffers performance saturation when adding\nmore transformer blocks. This phenomenon coincides with\nour observations that the number of blocks with similar\nattention maps increases with the depth as shown in Fig.\n3(b). Interestingly, when replacing the self-attention with\nour proposed Re-attention, the number of similar blocks are\nall reduced to be zero and the performance rises consistently\nas the model depth increases. The performance gain is es-\npecially signiﬁcant for deep ViT with 32 blocks. This might\nbe explained by the fact that the 32 block ViT model has the\nlargest number of blocks with similar attention maps and the\nimprovements should be proportional to the number sim-\nSelf-attention\nRe-attention\nBlock 1 Block 4 Block 11 Block 18 Block 23 Block 29 Block 30\nFigure 6: Attention map visualization of the selected blocks of the baseline ViT model with 32 transformer blocks. The ﬁrst row is based\non original Self-attention module and the second is based on Re-attention. As can be seen, the model only learns local patch relationship\nat its shallow blocks with the rest of attention values near to zero. Though their the scope increases gradually as the block goes deeper, the\nattention maps tend to become nearly uniform and thus lose diversity. After adding Re-attention, the originally similar attention maps are\nchanged to be diverse as shown in the second row. Only at the last block’s attention map, a nearly uniform attention map is learned.\nθ11 \nθH1 ...\nθ1H ...\nθHH ...\n...\nNormalization\nMulti-head attention\nk q\nValue projection v\nMatrix\nTransformation\nMulti-head attention\nk q\nValue projection v\nFigure 7: ( Left): The original self-attention mechanism; ( Right):\nOur proposed re-attention mechanism. As shown, the original at-\ntention map is mixed via a learnable matrix Θ before multiplied\nwith values.\nilar blocks in the model. These experiments demonstrate\nthat the proposed Re-attention can indeed solve the atten-\ntion collapse problem and thus enables training a very deep\nvision transformer without extra datasets or augmentation\npolicies.\nTable 4: ImageNet Top-1 accuracy of deep ViT (DeepViT) models\nwith Re-attention and different number of transformer blocks.\nModel #Similar Blocks Param. (M) Top-1 Acc. (%)\nViT-16B [7] 5 24.5 78.9\nDeepViT-16B 0 24.5 79.1 (+0.2)\nViT-24B [7] 11 36.3 79.4\nDeepViT-24B 0 36.3 80.1 (+0.7)\nViT-32B [7] 16 48.1 79.3\nDeepViT-32B 0 48.1 80.9 (+1.6)\nComparison to adding temperature in self-attention:\nThe most intuitive way to mitigate the over-smoothing phe-\nnomenon is to sharpen the distribution of the elements in\nthe attention map of MHSA. We could achieve this by as-\nsigning a temperature τ to the Softmax layer of MHSA:\nAttention(Q,K,V ) = Softmax\n(QK⊤\nτ\n√\nd\n)\nV. (4)\nAs the attention collapse is observed to be severe on deep\nlayers (as shown in Fig. 3), we design two sets of experi-\nments on a ViT model with 32 transformer blocks: (a) lin-\nearly decaying the temperature τ in each block such that\nthe attention map distribution is sharpened and (b) mak-\ning the temperature learnable and optimized together with\nthe model training. We ﬁrst check the impact of the Soft-\nMax temperature on reducing the attention map similarity.\nAs shown in Fig. 8(a), the number of similar blocks are\nstill large. Correspondingly, the feature similarity among\nblocks are also large as shown in Fig. 8(b). Thus, adding a\ntemperature to the SoftMax only reduces the attention map\nsimilarity by a small margin. The classiﬁcation results on\nImageNet are shown in Tab. 5. As shown, using a learn-\nable temperature could improve the performance but the im-\nprovement is marginal.\nTable 5: ImageNet Top-1 accuracy of the ViT models with Soft-\nMax temperature τ and the drop attention. The embedding dimen-\nsion of all the models is set as 384.\n# Blocks # Similar Blocks Model Acc. (%)\n32 16 Vanilla 79.3\n32 13 Linearly decayed τ 79.0\n32 10 Learnable τ 79.5\n32 8 drop attention 79.5\n32 0 Re-attention 80.9\nComparison to dropping attentions: Another baseline\nwe have attempted to differentiate the self-attention maps\nacross layers is to use random dropout on the attention maps\nA. As the dropout will mask out different positions on\nFigure 8: (a) Adjacent block attention map similarity with different methods. As can be seen, our proposed Re-attention achieves low\ncross layer attention map similarity. (b) Cosine similarity between the feature map of the last block and each of the previous block. (c)\nVisualization of transformation matrix of the last block.\nthe attention maps for different blocks, the similarity be-\ntween attention maps could be reduced. The impacts on\nthe attention maps and the output features of each block are\nshown in Fig. 8(a-b). It is observed that dropping atten-\ntion does reduce the cross layer similarity of the attention\nmaps. However, the similarity among features are not re-\nduced by much. This is because the difference between at-\ntention maps comes from the zero positions in the generated\nmask. Those zero values do reduce the similarity between\nattention maps but not contribute to the features. Thus, the\nimprovement is still not signiﬁcant as shown in Tab. 5.\nAdvantages of Re-attention: Our proposed Re-attention\nbrings more signiﬁcant improvements over the temperature-\ntuning and the attention-dropping methods. This is because\nboth adding temperature and dropping attention are regular-\nizing the distribution of the originally over-smoothed self-\nattention maps, without explicitly encouraging them to be\ndiverse. However, our proposed Re-attention mechanism\nuses different heads (whose attention maps are dissimilar)\nas basis and re-generate the attention maps via the trans-\nformation matrix Θ. This process incorporates the inter-\nhead information communication and the generated atten-\ntion maps can encode richer information. It is worth noting\nthat the original MHSA design can be thought as a special\ncase of Re-attention with an identity transformation matrix.\nBy making Θ learnable for each block, an optimized pat-\ntern could be learned end to end. As shown in Fig. 8(c),\nthe learned transformation matrix assigns a diverse set of\nweights for each newly generated head. It clearly shows that\nthe combination for each new heads takes different weights\nfrom the original heads in the re-attention process and thus\nreduces the similarity between their attention maps. As\nshown in Fig. 8(a), our proposed Re-attention achieves the\nlowest cross layer attention map similarity. Consequently,\nit also reduces the feature map similarity across layers as\nshown in Fig. 8(b).\n5.4. Comparison with other SOTA models\nWith Re-attention, we design two ViT variants, i.e.,\nDeepViT-S and DeepViT-L, based on the ViT with 16 and\n32 transformer blocks respectively. For both models, we use\nRe-attention to replace the self-attention. To have a similar\nnumber of parameters with other ViT models, we adjust the\nembedding dimension accordingly. The hidden dimensions\nof DeepViT-S and DeepViT-L models are set as 396 and 408\nrespectively. More details on the model conﬁguration are\ngiven in the supplementary material. Besides, motivated by\n[42], we add three CNN layers for extracting the token em-\nbeddings, using the same conﬁgurations as [42]. It is worth\nnoting that we do not use the optimized training recipes\nand the repeated augmentation as [37] for training our mod-\nels. The results are shown in Tab. 6. Clearly, our DeepViT\nmodel achieves higher accuracy with less parameters than\nthe recent CNN and ViT based models. Notably, without\nany complicated architecture change as made by T2T-ViT\n[42] (adopting a deep-narrow architecture) or DeiT [37] (in-\ntroducing token distillation), simply using the Re-attention\nmakes our DeepViT-L outperforms them by 0.4 points with\neven smaller model size (55M vs. 64M & 86 M).\n6. Conclusion\nIn this work, we found the attention collapse problem of\nvision transformers as they go deeper and propose a novel\nRe-attention mechanism to solve it with minimum amount\nof computation and memory overhead. With our proposed\nRe-attention, we are able to maintain an increasing perfor-\nmance when increasing the depth of ViTs. We hope our\nobservations and methods could facilitate the development\nof vision transformers in future.\nTable 6: Top-1 accuracy comparison with other SOTA models on\nImageNet. * denotes our reproduced results. ⋆ denotes our model\ntrained with training recipes used in DeiT [37].\nModel Params. (M) MAdds (G) Acc. (%)\nResNet50 [9] 25 4.0 76.2\nResNet50* 25 4.0 79.0\nRegNetY-8GF [27] 40 8.0 79.3\nVit-B/16 [7] 86 17.7 77.9\nVit-B/16* 86 17.7 79.3\nT2T-ViT-16 [42] 21 4.8 80.6\nDeiT-S [37] 22 – 79.8\nDeepVit-S (Ours) 27 6.2 81.4\nDeepVit-S⋆ (Ours) 27 6.2 82.3\nResNet152 [9] 60 11.6 78.3\nResNet152* 60 11.6 80.6\nRegNetY-16GF [27] 54 15.9 80.0\nVit-L/16 [7] 307 – 76.5\nT2T-ViT-24 [42] 64 12.6 81.8\nDeiT-B [37] 86 – 81.8\nDeiT-B* 86 17.7 81.5\nDeepVit-L (Ours) 55 12.5 82.2\nDeepVit-L⋆ (Ours) 58 12.8 83.1\nDeepVit-L⋆ ↑ 384 (Ours) 58 12.8 84.3\nReferences\n[1] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Rep-\nresentation learning: A review and new perspectives. IEEE\ntransactions on pattern analysis and machine intelligence ,\n35(8):1798–1828, 2013.\n[2] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, pages 213–229. Springer, 2020.\n[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. arXiv\npreprint arXiv:2012.00364, 2020.\n[5] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmenta-\ntion with a reduced search space. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, pages 702–703, 2020.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[8] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-\nculty of training deep feedforward neural networks. In Pro-\nceedings of the thirteenth international conference on artiﬁ-\ncial intelligence and statistics, pages 249–256. JMLR Work-\nshop and Conference Proceedings, 2010.\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. In European\nconference on computer vision , pages 630–645. Springer,\n2016.\n[11] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim\nSalimans. Axial attention in multidimensional transformers.\narXiv preprint arXiv:1912.12180, 2019.\n[12] Qibin Hou, Daquan Zhou, and Jiashi Feng. Coordinate at-\ntention for efﬁcient mobile network design. arXiv preprint\narXiv:2103.02907, 2021.\n[13] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\nbilenetv3. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1314–1324, 2019.\n[14] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Lo-\ncal relation networks for image recognition. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 3464–3473, 2019.\n[15] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional net-\nworks. pages 4700–4708, 2017.\n[16] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Fi-\nrat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan\nNgiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efﬁcient\ntraining of giant neural networks using pipeline parallelism.\narXiv preprint arXiv:1811.06965, 2018.\n[17] Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen,\nJiashi Feng, and Shuicheng Yan. Convbert: Improving\nbert with span-based dynamic convolution. arXiv preprint\narXiv:2008.02496, 2020.\n[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In Advances in neural information processing sys-\ntems, pages 1097–1105, 2012.\n[19] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao\nChen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam\nShazeer, and Zhifeng Chen. Gshard: Scaling giant models\nwith conditional computation and automatic sharding. arXiv\npreprint arXiv:2006.16668, 2020.\n[20] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Changhu\nWang, and Jiashi Feng. Improving convolutional networks\nwith self-calibrated convolutions. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10096–10105, 2020.\n[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019.\n[22] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas-\ntic gradient descent with warm restarts. arXiv preprint\narXiv:1608.03983, 2016.\n[23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017.\n[24] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\nVilbert: Pretraining task-agnostic visiolinguistic represen-\ntations for vision-and-language tasks. arXiv preprint\narXiv:1908.02265, 2019.\n[25] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library.arXiv\npreprint arXiv:1912.01703, 2019.\n[26] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. OpenAI blog, 1(8):9, 2019.\n[27] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll ´ar. Designing network design\nspaces. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 10428–\n10436, 2020.\n[28] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv:1906.05909, 2019.\n[29] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg\nSperl, and Christoph H Lampert. icarl: Incremental classiﬁer\nand representation learning. In Proceedings of the IEEE con-\nference on Computer Vision and Pattern Recognition, pages\n2001–2010, 2017.\n[30] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[31] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottle-\nneck transformers for visual recognition. arXiv preprint\narXiv:2101.11605, 2021.\n[32] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy,\nand Cordelia Schmid. Videobert: A joint model for video\nand language representation learning. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 7464–7473, 2019.\n[33] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1–9, 2015.\n[34] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\ntecture for computer vision. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n2818–2826, 2016.\n[35] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105–6114. PMLR,\n2019.\n[36] Mingxing Tan and Quoc V Le. Mixconv: Mixed depthwise\nconvolutional kernels. arXiv preprint arXiv:1907.09595 ,\n2019.\n[37] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020.\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017.\n[39] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-\nalone axial-attention for panoptic segmentation. InEuropean\nConference on Computer Vision , pages 108–126. Springer,\n2020.\n[40] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794–7803, 2018.\n[41] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1492–1500,\n2017.\n[42] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021.\n[43] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-\nization strategy to train strong classiﬁers with localizable fea-\ntures. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 6023–6032, 2019.\n[44] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\nworks. arXiv preprint arXiv:1605.07146, 2016.\n[45] Daokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang.\nNetwork representation learning: A survey. IEEE transac-\ntions on Big Data, 6(1):3–28, 2018.\n[46] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017.\n[47] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi\nZhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R\nManmatha, et al. Resnest: Split-attention networks. arXiv\npreprint arXiv:2004.08955, 2020.\n[48] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor-\ning self-attention for image recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10076–10085, 2020.\n[49] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and\nVladlen Koltun. Point transformer. arXiv preprint\narXiv:2012.09164, 2020.\n[50] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. arXiv preprint arXiv:2012.15840, 2020.\n[51] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. InProceedings\nof the AAAI Conference on Artiﬁcial Intelligence, volume 34,\npages 13001–13008, 2020.\n[52] Daquan Zhou, Qibin Hou, Yunpeng Chen, Jiashi Feng, and\nShuicheng Yan. Rethinking bottleneck structure for efﬁcient\nmobile network design. ECCV , August, 2, 2020.\nA. Experiment Implementation Details\nAttention reuse: As shown in Fig. 3(b) and Tab. 3 in the\nmain paper, the vision transformers with 24 blocks and 32\nblocks have 11 and 15 blocks with similar attention maps,\nrespectively. To verify the effectiveness of the attention\nmaps from those blocks, we directly force those blocks to\nshare the same attention map as the last ‘unique’ block as\ndeﬁned in Sec. 5.2. Speciﬁcally, we take the attention map\nof the last ‘unique’ block and denote it as Aunique. For all\nthe following blocks, the attention output is calculated by:\nAttention(Q,K,V ) = Norm(Θ⊤Aunique)V, (5)\nwhere Θ is used to simulate the small variance between\nattention maps across layers since they are not identical.\nNorm is batch normalization used to adjust the variance\nacross layers. As shown in Tab. 3, for a ViT with 32 blocks,\nforcing the top 15 blocks to share the same attention map\ncauses negligible degradation on the classiﬁcation accuracy\non ImageNet. This proves that adding those blocks do not\ncontribute to the accuracy improvement.\nTraining loss: We use the cross-entropy (CE) loss as the\ntraining loss for all experiments. To minimize the similarity\nof the attention maps across layers, we add the cosine sim-\nilarity between layers into the loss function when training\nthe model.\nLosstrain = LossCE + λ\nB∑\nl=0\nSimilarity(Al,Al+1) (6)\nwhere Similarity (Al,Al+1) denotes the cosine similarity\nbetween layer land l+ 1 and Al denotes the attention map\nof layer l. B denotes the number of bottom blocks used for\nregularization and is a hyper-parameter. We set B to 4, 8\nand 12 for training ViT models with 16, 24 and 32 blocks\nrespectively.\nTable 7: Structural hyper-parameter of DeepViT-S and DeepViT-\nL. Note that the embedding dimension is slightly larger than the\nbaseline models. This is to adjust the size of the model to have a\ncomparable size with other methods for a fair comparison.\nModel #Blocks #Embedding MLP size Split ratio\nDeepViT-S 16 396 1188 11-5\nDeepViT-L 32 420 1260 20-12\nB. DeepViT architecture design\nAs observed in Fig. 3(a), the attention maps of the trans-\nformer blocks become similar only at the top blocks. Thus,\nFigure 9: ViT classiﬁcation accuracy with Re-attention applied\non different number of blocks. The black dotted line denotes\nthe cosine similarity ratio between adjacent blocks of the original\nViT model with 16 blocks. The red dotted line denotes the top-1\nclassiﬁcation accuracy on ImageNet. The accuracy of the model\nwith blocks index k denotes that the Re-attention is applied on top\n(16 −k) blocks.\nit is not necessary to apply re-attention to all blocks. To\nstudy the optimal number of blocks with re-attention, we\nconduct a set of experiments on a ViT model with 16 trans-\nformer blocks. For each experiment, we only apply re-\nattention on the top K blocks where K ranges from 5 to\n15. The rest of the blocks are using the original transformer\nblock structure. We train each model on ImageNet with the\nsame set of training hyper-parameters as those for baseline\nmodels as detailed in Sec. 5 in the main paper. The results\nare shown in Fig. 9.\nIt is observed that, as the number of re-attention blocks\nvaries, the top-1 classiﬁcation accuracy changes corre-\nspondingly. The highest accuracy appears at the position\nwhere the number of re-attention blocks is the same as the\nnumber of similar attention map blocks. Based on this\nobservation, we deﬁne the architecture of DeepViT-S and\nDeepViT-L with 5 and 12 re-attention blocks respectively.\nDetailed conﬁgurations are shown in Tab. 7. Note that we\nadjust the embedding dimension to have a comparable size\nwith other methods.\nC. Impacts of hyper-parameters\nIn the main paper, all experiments are run with the same\nset of training hyper-parameters as the one used for repro-\nducing ViT models. However, as shown in [37], an im-\nproved training recipe could improve the performance of\nViT models signiﬁcantly. In Tab. 8, we present the per-\nformance of DeepViT-S and DeepViT-L with the same set\nof training recipes as DeiT except that we do not use re-\npeated augmentation. In Tab. 8, it is clearly shown that the\nperformance of DeepViT could be further improved with\noptimized training hyper-parameters.\nTable 8: DeepViT model with different training recipes. ⋆ denotes\nthe model trained with DeiT [37] training recipes.\nModel Params. (M) MAdds (G) Acc. (%)\nDeiT-S [37] 22 – 79.8\nDeiT-S (KD) [37] 22 – 81.2\nDeepVit-S (Ours) 27 6.2 81.4\nDeepVit-S⋆ (Ours) 27 6.2 82.3\nDeiT-B [37] 86 17.7 81.8\nDeiT-B (KD) [37] 86 17.7 83.4\nDeepViT-L (Ours) 55 12.5 82.2\nDeepViT-L⋆ (Ours) 58 12.8 83.1\nD. Similarity calculation\nCosine similarity between layers To measure the similar-\nity between the attention maps, we deﬁne the similaritySp,q\nbetween the attention maps of two layers,pand q, as the ra-\ntio of the number of similar vector pairs to the total number\nof pairs between two attention maps:\nS(p,q) =\n∑I\n|Mp,q|, I i,j =\n{\n1, if Mp,q\ni,j >τ\n0, otherwise (7)\nwhere τ is a hyper-parameter and used as a threshold for\ndeciding similar vectors2.\nDeﬁnition of similar blocks A block is counted as a similar\nblock if the similarity between it’s attention map and the ad-\njacent block’s attention map is larger than 80%. To measure\nthe block similarity for a ViT model withBblocks, we take\nthe ratio of the number of similar blocks to the total number\nof blocks as a measurement.\n20.5 is selected as a threshold for visualization purpose in this paper",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.772331953048706
    },
    {
      "name": "Transformer",
      "score": 0.7145050764083862
    },
    {
      "name": "Computation",
      "score": 0.6582263708114624
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6100263595581055
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5866766571998596
    },
    {
      "name": "Stacking",
      "score": 0.5657913088798523
    },
    {
      "name": "Scaling",
      "score": 0.5320551991462708
    },
    {
      "name": "Deep learning",
      "score": 0.4817397892475128
    },
    {
      "name": "Feature learning",
      "score": 0.45219501852989197
    },
    {
      "name": "FLOPS",
      "score": 0.4348198473453522
    },
    {
      "name": "Code (set theory)",
      "score": 0.43160220980644226
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40738430619239807
    },
    {
      "name": "Machine learning",
      "score": 0.3369115889072418
    },
    {
      "name": "Algorithm",
      "score": 0.29495418071746826
    },
    {
      "name": "Parallel computing",
      "score": 0.12317577004432678
    },
    {
      "name": "Engineering",
      "score": 0.08334031701087952
    },
    {
      "name": "Voltage",
      "score": 0.08146664500236511
    },
    {
      "name": "Mathematics",
      "score": 0.0635460615158081
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Nuclear magnetic resonance",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": []
}