{
  "title": "Plansformer Tool: Demonstrating Generation of Symbolic Plans Using Transformers",
  "url": "https://openalex.org/W4385764285",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2811212743",
      "name": "Vishal Pallagani",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A4382325714",
      "name": "Bharath Muppasani",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A2137423774",
      "name": "Biplav Srivastava",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A2079166072",
      "name": "Francesca Rossi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2775914336",
      "name": "Lior Horesh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2410162818",
      "name": "Keerthiram Murugesan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A939433112",
      "name": "Andrea Loreggia",
      "affiliations": [
        "University of Brescia",
        "Brescia University"
      ]
    },
    {
      "id": "https://openalex.org/A2556369748",
      "name": "Francesco Fabiano",
      "affiliations": [
        "University of Udine"
      ]
    },
    {
      "id": "https://openalex.org/A3081921619",
      "name": "Rony Joseph",
      "affiliations": [
        "Indian Institute of Information Technology, Nagpur",
        "International Institute of Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5092549992",
      "name": "Yathin Kethepalli",
      "affiliations": [
        "International Institute of Information Technology",
        "Indian Institute of Information Technology, Nagpur"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4382202796",
    "https://openalex.org/W2291853497",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2001680357",
    "https://openalex.org/W2126902640",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4312045619",
    "https://openalex.org/W2106086642",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3101355526",
    "https://openalex.org/W2024256451",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4283330306",
    "https://openalex.org/W2577270548",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2035078485",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W1555696599",
    "https://openalex.org/W3161457214",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W2149361370",
    "https://openalex.org/W3111885891"
  ],
  "abstract": "Plansformer is a novel tool that utilizes a fine-tuned language model based on transformer architecture to generate symbolic plans. Transformers are a type of neural network architecture that have been shown to be highly effective in a range of natural language processing tasks. Unlike traditional planning systems that use heuristic-based search strategies, Plansformer is fine-tuned on specific classical planning domains to generate high-quality plans that are both fluent and feasible. Plansformer takes the domain and problem files as input (in PDDL) and outputs a sequence of actions that can be executed to solve the problem. We demonstrate the effectiveness of Plansformer on a variety of benchmark problems and provide both qualitative and quantitative results obtained during our evaluation, including its limitations. Plansformer has the potential to significantly improve the efficiency and effectiveness of planning in various domains, from logistics and scheduling to natural language processing and human-computer interaction. In addition, we provide public access to Plansformer via a website as well as an API endpoint; this enables other researchers to utilize our tool for planning and execution. The demo video is available at https://youtu.be/_1rlctCGsrk",
  "full_text": "Plansformer Tool: Demonstrating Generation of Symbolic Plans Using\nTransformers\nVishal Pallagani1 , Bharath Muppasani1 , Biplav Srivastava1 , Francesca Rossi2 ,\nLior Horesh2 , Keerthiram Murugesan2 , Andrea Loreggia3 ,\nFrancesco Fabiano4 , Rony Joseph5 , Yathin Kethepalli5\n1University of South Carolina - USA\n2IBM Research - USA\n3University of Brescia - Italy\n4University of Udine - Italy\n5IIIT Naya Raipur - India\nAbstract\nPlansformer is a novel tool that utilizes a fine-tuned\nlanguage model based on transformer architecture\nto generate symbolic plans. Transformers are a\ntype of neural network architecture that have been\nshown to be highly effective in a range of natural\nlanguage processing tasks. Unlike traditional plan-\nning systems that use heuristic-based search strate-\ngies, Plansformer is fine-tuned on specific classi-\ncal planning domains to generate high-quality plans\nthat are both fluent and feasible. Plansformer takes\nthe domain and problem files as input (in PDDL )\nand outputs a sequence of actions that can be ex-\necuted to solve the problem. We demonstrate the\neffectiveness of Plansformer on a variety of bench-\nmark problems and provide both qualitative and\nquantitative results obtained during our evaluation,\nincluding its limitations. Plansformer has the po-\ntential to significantly improve the efficiency and\neffectiveness of planning in various domains, from\nlogistics and scheduling to natural language pro-\ncessing and human-computer interaction. In ad-\ndition, we provide public access to Plansformer\nvia a website as well as an API endpoint; this en-\nables other researchers to utilize our tool for plan-\nning and execution. The demo video is available at\nhttps://youtu.be/\n1rlctCGsrk.\n1 Introduction\nLarge Language Models (LLMs) have revolutionized the field\nof Natural Language Processing (NLP), outperforming hu-\nmans in various natural language tasks [Vaswaniet al., 2017;\nDevlin et al., 2018; Brown et al., 2020; Scao et al., 2022;\nChowdhery et al., 2022; Li, 2022]. However, their use in do-\nmains involving symbols, such as mathematics [Hendrycks\net al., 2021b; Cobbe et al., 2021], coding [Hendrycks et al.,\n2021a; Chen et al., 2021], and automated planning [Lamanna\net al., 2023; Jim ´enez et al., 2012 ], has been limited due to\ntheir inability to reason with symbolic data. In this paper, we\npropose using LLM trained for code generation to generate\nvalid plans for automated planning domains.\nTo accomplish this, we create a training and test set for four\nclassical planning domains and use CodeT5 (base) [Wang\net al., 2021 ], a pre-trained code generation model, as the\nLLM. We then present Plansformer, which is obtained by\nfine-tuning CodeT5 on planning problems, making it capable\nof generating symbolic plans of high quality. Our experimen-\ntal results indicate that the syntactic and symbolic knowledge\nlearned from different programming languages in the CodeT5\nmodel can be useful for the PDDL-based automated planning\ntask, achieving promising results in generating valid and op-\ntimal plans.\nPlansformer is not intended to replace traditional auto-\nmated planners, which are capable of generating valid or\noptimal plans, but rather complement them. A Plansformer\ncan play to its benefit as a fast solver and has relaxation\nin terms of correctness, while the traditional planner can\nbe used as a sound and complete solver, which is delib-\nerative and always generates a correct output. This work\nalso explores LLMs’ capabilities in dealing with symbolic\nlanguage, revealing a promising direction to harness LLMs\nfor symbolic tasks such as planning. This is a signifi-\ncant contribution as prior work [Valmeekam et al., 2022;\nSilver et al., 2022] has shown that even state-of-the-art LLMs,\nsuch as GPT-3 [Brown et al., 2020], cannot reason with sym-\nbolic data.\n2 Background and Methodology\nAutomated planning is a field of AI concerned with gener-\nating plans to achieve goals [Ghallab et al., 2004a; Ghal-\nlab et al., 2004b ]. Traditional approaches use search algo-\nrithms but have scalability and uncertainty limitations [Ghal-\nlab et al., 2014]. Learning-based approaches, leveraging ma-\nchine learning, can overcome these limitations, learn from\ndata, generalize to new domains, and improve performance\n[Veloso et al., 1995; Zimmerman and Kambhampati, 2003 ].\nWe present a comparison of traditional and learning-based\nplanning approaches in Table 2.\nPlansformer, a learning-based planner is generated and\ntested in two phases: modeling and evaluation. Fine-tuning\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nDemonstrations Track\n7158\nModels V\nalid Plans(%) Invalid Plans Optimal Plans (%) Avg. Time (sec)\nFailed (%) Incomplete/Wrong (%)\nFastDo\nwnward (Ground\nTruth)\n100% - - 100% 10.28s\nGPT-2 0% 0% 100% 0% 0.05s\nT5-base 0.25% 17.3% 82.7% 0.25% 0.47s\nCodex 0.15% 99.85% 0% 0.15% 1s\nCodeT5-base 0.6% 0% 99.4% 0.6% 0.68s\nPlansformer 83.64% 16.18% 0.19% 73.27% 0.06s\nPlansformer-bw 90.04% 9.94% 0.02% 88.44% 0.05s\nPlansformer-hn 84.97% 14.72% 0.31% 82.58% 0.05s\nPlansformer-gr 82.97% 16.61% 0.42% 69.47% 0.06s\nPlansformer-dl 76.56% 23.44% 0% 52.61% 0.09s\nTable 1: Results of plan validation.\nCriteria Traditional Planners Learning-based Planning\nRepresentation Symbolic representation, logical reasoning Neural network-based, data-driven\nScalability Limited scalability, exponential growth of\nstate space\nScalable to large state spaces, can learn from large data\nsets\nAccuracy Can guarantee correctness, optimal solutions Prone to errors, suboptimal solutions\nGeneralization Limited ability to generalize to unseen do-\nmains\nCan generalize to unseen domains with sufficient training\ndata\nInterpretability Human-understandable, explainable Lack of interpretability, black-box models\nEfficiency Inefficient for large state spaces, computa-\ntionally expensive\nCan be more efficient than traditional planners, especially\nfor large state spaces\nTable 2: Comparison of traditional and learning-based planning\nthe CodeT5 to address planning syntax and semantics for the\nfirst phase, and evaluating the competency of Plansformer as\na language model and planner for the second phase. The se-\nquence of actions generated by Plansformer are validates us-\ning both language based (e.g. ROUGE, BLEU ) and planning\nbased metrics (e.g. validity, optimality).\n2.1 Modeling Phase\nIn the modeling phase, we fine-tune CodeT5 by creating a\nplanning-based dataset. We focus on four classical plan-\nning benchmark domains from International Planning Com-\npetitions [ICAPS, 2022; Younes et al., 2005; Long and Fox,\n2003]: Blocksworld [Gupta and Nau, 1991], Towers of Hanoi\n[Gerety and Cull, 1986 ], Grippers [Seipp et al., 2016 ], and\nDriverlog [Roberts et al., 2014], each with multiple problem\ninstances. We generate optimal plans [Helmert and Domsh-\nlak, 2011 ] for each problem instance using the FastDown-\nward planner [Helmert, 2006]. The generated dataset for each\ndomain contains 18,000 plans with different problem con-\nfigurations, and we use 5-fold cross-validation for training.\nWe use a Byte-level BPE tokenizer with a vocabulary size\nof 32,005 and add PDDL-specific tokens ([GOAL], [INIT],\n[ACTION], [PRE], [EFFECT]) to simplify the input to Plans-\nformer, which represents the goal state, initial state, possible\nactions with their associated preconditions and effects caused\nby the actions in the environment. CodeT5 is well-suited\nfor planning tasks as it can generate goal-directed, structured\ncode with semantic meaning. We fine-tune it with 80% of the\n18,000 generated samples for each of the four domains in the\nplanning dataset.\n2.2 Evaluation Phase\nIn the evaluation phase of Plansformer, the model is tested\nfor both plan validation and language model competency.\nFor plan validation, the sequence of actions generated by\nPlansformer must guide an agent from the initial state to\nthe goal state for a given problem instance, and we evalu-\nate for optimality and validity using a plan validation tool\ncalled V AL[Howey et al., 2004]. For language model com-\npetency, we use metrics such as BLEU[Papineni et al., 2002]\nand ROUGE-L [Lin, 2004] to measure precision and recall,\nrespectively. Although these metrics have no direct intuition\nin automated planning, we use them to evaluate the task of\nplan generation from the perspective of LLMs. The results of\nthese evaluations provide conclusive evidence on how well\nPlansformer generates plans and its performance as a lan-\nguage model.\n3 System Demonstration\nThe website [Pallagani, 2023b] provides an easy-to-use inter-\nface to exploit all the functionalities of Plansformer. It also\nprovides all the information to guide the user through the dif-\nferent features made available, for instance how to use and\nleverage the various features and tools for editing and gener-\nating plans. On the landing page, the tool is introduced to the\nuser, along with instructions on how to use it. By clicking\non the editor button (in the menu bar), the user is directed to\nthe tool usage page. The website provides an intuitive PDDL\n[Aeronautiques et al., 1998] editor, where the user can add or\nedit code files as per their requirements. Figure 1 provides an\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nDemonstrations Track\n7159\nFigure 1: Screenshot of the editor page.\nexample of the editor page with the Blocksworld domain in\nPDDL.\nFor the user’s convenience, the website offers a reference to\nsome problem instances from the four domains under study.\nFurthermore, the user can create new files on the website or\nupload files from their local system. To facilitate the user’s\nexperience, the website also offers the functionality of saving\nfiles from the Plansformer tool to their local file storage.\nThe solve functionality is the next step in the user jour-\nney. To initiate the plan generation process, the user needs\nto choose a domain and its corresponding problem file. The\nwebsite provides an optional plan validity checker, which en-\nables the user to know the validity and optimality character-\nistics of the generated plan. Once the solution is obtained,\nthe output is displayed, and the user is shown the generated\nplan, a summary of the selected domain and problem, along\nwith the validation results if requested. Researchers can also\nleverage the API to use Plansformer for their work[Pallagani,\n2023a], in order to use the API endpoint domain and problem\nfiles must be specified as input, as reported in Listing 1\nListing 1: Example API Request\nc u r l −X POST \\\n−H ’ Content −Type : m u l t i p a r t / form − d a t a ’ \\\n−F ’ domain= @/ p a t h / t o / domain . pddl ’ \\\n−F ’ problem = @/ p a t h / t o / problem . pddl ’ \\\nh t t p : / / 1 2 9 . 2 5 2 . 1 3 1 . 1 3 / p l a n s f o r m e r /\nSometimes users may experience some delays in the com-\nputation of a solution. The website is implemented for\ndemonstration purposes and it is based on a basic server.\nThis is the reason for the increased latency of plan genera-\ntion in the demo website which can leverage only on CPUs\nfor the computation of a solution. The website is inspired by\nthe well-known online planning tool [Muise, 2015 ], which\nshould help users in getting familiar with our solution.\n4 System Evaluation\nPlansformer is evaluated on multiple planning domains of\nvarying complexities using both quantitative and qualitative\nmeasures. For model evaluation, Plansformer is compared\nModels R\nOUGE-Lrecall ROUGE-Lprecision ROUGE-Lfmeasure BLEU\nGPT-2 0.04 0.14 0.06\n0.07\nT5-base 0.16 0.70 0.26 0.02\nCodex 0.72 0.52 0.60 0.36\nCodeT5-base 0.41 0.28 0.33 0.02\nPlansformer 0.93 0.93 0.93 0.89\nPlansformer-bw 0.97 0.99 0.98 0.90\nPlansformer-hn 0.99 0.96 0.97 0.95\nPlansformer-gr 0.94 0.94 0.94 0.92\nPlansformer-dl 0.82 0.83 0.82 0.79\nTable 3: Results of model testing (best performance in bold).\nwith other language models using the model evaluation met-\nrics (ROUGE and BLEU) and the results as seen in Table 3,\nshow that Plansformer outperforms all other models, includ-\ning Codex [Chen et al., 2021]. However, for plan validation,\nFastDownward, a traditional classical planning system is also\nadded to the test-bed. Plans generated by Plansformer are\nevaluated for validity and optimality, and the results as seen\nin Table 1, show that Plansformer performs best in simple\nplanning domains (such as blocksworld) but generates fewer\noptimal plans in complex domains (such as driverlog). The\npaper reports that the average time taken by Plansformer to\nsolve the test-bed of problems is approximately 200 times\nfaster than the FastDownward planner1. The study by [Palla-\ngani et al., 2022] provides a comprehensive account of the ex-\nperimental methodology and outcomes achieved through the\napplication of Plansformer, offering an in-depth analysis of\nthe results obtained.\n5 Conclusion\nIn this demonstration, we have showcased a novel tool that\nleverages an LLM for generating and validating plans for\nclassical problems in four selected domains. As a next step,\nwe are actively expanding the tool’s applicability to various\nother planning domains, aiming to enhance its generalizabil-\nity and versatility. In future iterations, we plan to extend the\ntool’s capabilities to address more complex planning scenar-\nios, including epistemic and hierarchical planning. Addition-\nally, we aim to enable the tool to automatically repair invalid\nplans, and to enhance its visual output to facilitate easier plan\ninterpretation and analysis.\n1when used with GPU capabilities\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nDemonstrations Track\n7160\nReferences\n[Aeronautiques et al., 1998] Constructions Aeronautiques,\nAdele Howe, Craig Knoblock, ISI Drew McDermott, Ash-\nwin Ram, Manuela Veloso, Daniel Weld, David Wilkins\nSRI, Anthony Barrett, Dave Christianson, et al. Pddl— the\nplanning domain definition language. Technical Report,\nTech. Rep., 1998.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. Language models are few-shot\nlearners. Advances in neural information processing sys-\ntems, 33:1877–1901, 2020.\n[Chen et al., 2021] Mark Chen, Jerry Tworek, Heewoo Jun,\nQiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. Evaluating large language models\ntrained on code. arXiv preprint arXiv:2107.03374, 2021.\n[Chowdhery et al., 2022] Aakanksha Chowdhery, Sharan\nNarang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles\nSutton, Sebastian Gehrmann, et al. Palm: Scal-\ning language modeling with pathways. arXiv preprint\narXiv:2204.02311, 2022.\n[Cobbe et al., 2021] Karl Cobbe, Vineet Kosaraju, Moham-\nmad Bavarian, Jacob Hilton, Reiichiro Nakano, Christo-\npher Hesse, and John Schulman. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168,\n2021.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Gerety and Cull, 1986] C Gerety and P Cull. Time com-\nplexity of the towers of hanoi problem. SIGACT News,\n18(1):80–87, mar 1986.\n[Ghallab et al., 2004a] Malik Ghallab, Dana Nau, and Paolo\nTraverso. Automated Planning: theory and practice. El-\nsevier, 2004.\n[Ghallab et al., 2004b] Malik Ghallab, Dana Nau, and Paolo\nTraverso. Automated Planning: Theory and Practice. The\nMorgan Kaufmann Series in Artificial Intelligence. Mor-\ngan Kaufmann, Amsterdam, 2004.\n[Ghallab et al., 2014] Malik Ghallab, Dana Nau, and Paolo\nTraverso. The actors view of automated planning and act-\ning: A position paper. Artificial Intelligence, 208:1–17,\n2014.\n[Gupta and Nau, 1991] Naresh Gupta and Dana S. Nau.\nComplexity results for blocks-world planning. In In Pro-\nceedings of AAAI-91, pages 629–633, 1991.\n[Helmert and Domshlak, 2011] Malte Helmert and Carmel\nDomshlak. Lm-cut: Optimal planning with the landmark-\ncut heuristic. Seventh international planning competition\n(IPC 2011), deterministic part, pages 103–105, 2011.\n[Helmert, 2006] Malte Helmert. The fast downward plan-\nning system. Journal of Artificial Intelligence Research,\n26:191–246, 2006.\n[Hendrycks et al., 2021a] Dan Hendrycks, Steven Basart,\nSaurav Kadavath, Mantas Mazeika, Akul Arora, Ethan\nGuo, Collin Burns, Samir Puranik, Horace He, Dawn\nSong, et al. Measuring coding challenge competence with\napps. arXiv preprint arXiv:2105.09938, 2021.\n[Hendrycks et al., 2021b] Dan Hendrycks, Collin Burns,\nSaurav Kadavath, Akul Arora, Steven Basart, Eric Tang,\nDawn Song, and Jacob Steinhardt. Measuring mathemati-\ncal problem solving with the math dataset. arXiv preprint\narXiv:2103.03874, 2021.\n[Howey et al., 2004] Richard Howey, Derek Long, and\nMaria Fox. Val: Automatic plan validation, continuous\neffects and mixed initiative planning using pddl. In 16th\nIEEE International Conference on Tools with Artificial In-\ntelligence, pages 294–301. IEEE, 2004.\n[ICAPS, 2022] ICAPS. International planning competi-\ntions at international conference on automated plan-\nning and scheduling (icaps). In https://www.icaps-\nconference.org/competitions/, 2022.\n[Jim´enez et al., 2012] Sergio Jim ´enez, Tom ´as De La Rosa,\nSusana Fern´andez, Fernando Fern ´andez, and Daniel Bor-\nrajo. A review of machine learning for automated plan-\nning. The Knowledge Engineering Review, 27(4):433–\n467, 2012.\n[Lamanna et al., 2023] Leonardo Lamanna, Luciano Ser-\nafini, Mohamadreza Faridghasemnia, Alessandro Saffiotti,\nAlessandro Saetti, Alfonso Gerevini, and Paolo Traverso.\nPlanning for learning object properties. arXiv preprint\narXiv:2301.06054, 2023.\n[Li, 2022] Hang Li. Language models: Past, present, and\nfuture. Commun. ACM, 65(7):56–63, jun 2022.\n[Lin, 2004] Chin-Yew Lin. Rouge: A package for automatic\nevaluation of summaries. In Text summarization branches\nout, pages 74–81, 2004.\n[Long and Fox, 2003] Derek Long and Maria Fox. The 3rd\ninternational planning competition: Results and analysis.\nJournal of Artificial Intelligence Research, 20:1–59, 2003.\n[Muise, 2015] Christian Muise. Planning domains. http://\nplanning.domains/, 2015. Accessed: 2023-02-20.\n[Pallagani et al., 2022] Vishal Pallagani, Bharath Mup-\npasani, Keerthiram Murugesan, Francesca Rossi, Lior\nHoresh, Biplav Srivastava, Francesco Fabiano, and An-\ndrea Loreggia. Plansformer: Generating symbolic plans\nusing transformers. arXiv preprint arXiv:2212.08681 ,\n2022.\n[Pallagani, 2023a] Vishal Pallagani. Plansformer api. http:\n//129.252.131.13/plansformer/, 2023. Accessed: 2023-02-\n20.\n[Pallagani, 2023b] Vishal Pallagani. Plansformer website.\nhttps://uo-sc-plansformer.vercel.app/, 2023. Accessed:\n2023-02-20.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nDemonstrations Track\n7161\n[Papineni et al., 2002] Kishore Papineni, Salim Roukos,\nTodd Ward, and Wei-Jing Zhu. Bleu: a method for au-\ntomatic evaluation of machine translation. In Proceedings\nof the 40th annual meeting of the Association for Compu-\ntational Linguistics, pages 311–318, 2002.\n[Roberts et al., 2014] Mark Roberts, Adele Howe, and In-\ndrajit Ray. Evaluating diversity in classical planning.\nIn Proceedings of the International Conference on Auto-\nmated Planning and Scheduling, volume 24, pages 253–\n261, 2014.\n[Scao et al., 2022] Teven Le Scao, Angela Fan, Christo-\npher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow,\nRoman Castagn ´e, Alexandra Sasha Luccioni, Franc ¸ois\nYvon, Matthias Gall ´e, et al. Bloom: A 176b-parameter\nopen-access multilingual language model. arXiv preprint\narXiv:2211.05100, 2022.\n[Seipp et al., 2016] Jendrik Seipp, Florian Pommerening,\nGabriele R¨oger, and Malte Helmert. Correlation complex-\nity of classical planning domains. 2016.\n[Silver et al., 2022] Tom Silver, Varun Hariprasad, Reece S\nShuttleworth, Nishanth Kumar, Tom´as Lozano-P´erez, and\nLeslie Pack Kaelbling. Pddl planning with pretrained large\nlanguage models. In NeurIPS 2022 Foundation Models for\nDecision Making Workshop, Oct 2022.\n[Valmeekam et al., 2022] Karthik Valmeekam, Alberto\nOlmo, Sarath Sreedharan, and Subbarao Kambhampati.\nLarge language models still can’t plan (a benchmark for\nllms on planning and reasoning about change). arXiv\npreprint arXiv:2206.10498, 2022.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. Advances in neural information processing systems,\n30, 2017.\n[Veloso et al., 1995] Manuela Veloso, Jaime Carbonell, Ali-\ncia Perez, Daniel Borrajo, Eugene Fink, and Jim Blythe.\nIntegrating planning and learning: The prodigy architec-\nture. Journal of Experimental & Theoretical Artificial In-\ntelligence, 7(1):81–120, 1995.\n[Wang et al., 2021] Yue Wang, Weishi Wang, Shafiq Joty,\nand Steven CH Hoi. Codet5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understanding\nand generation. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing,\npages 8696–8708, 2021.\n[Younes et al., 2005] H˚akan LS Younes, Michael L Littman,\nDavid Weissman, and John Asmuth. The first probabilistic\ntrack of the international planning competition. Journal of\nArtificial Intelligence Research, 24:851–887, 2005.\n[Zimmerman and Kambhampati, 2003] Terry Zimmerman\nand Subbarao Kambhampati. Learning-assisted automated\nplanning: Looking back, taking stock, going forward. AI\nMagazine, 24(2):73–73, 2003.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nDemonstrations Track\n7162",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.855352520942688
    },
    {
      "name": "Transformer",
      "score": 0.7310678958892822
    },
    {
      "name": "Architecture",
      "score": 0.5715484619140625
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4910874366760254
    },
    {
      "name": "Natural language",
      "score": 0.48190099000930786
    },
    {
      "name": "Scheduling (production processes)",
      "score": 0.4635236859321594
    },
    {
      "name": "Heuristic",
      "score": 0.43621626496315
    },
    {
      "name": "Automated planning and scheduling",
      "score": 0.4290885031223297
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3832659125328064
    },
    {
      "name": "Software engineering",
      "score": 0.3746805489063263
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I155781252",
      "name": "University of South Carolina",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210156936",
      "name": "IBM Research - Austin",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I79940851",
      "name": "University of Brescia",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I129043915",
      "name": "University of Udine",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I4387155265",
      "name": "International Institute of Information Technology",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210138251",
      "name": "Indian Institute of Information Technology, Nagpur",
      "country": "IN"
    }
  ],
  "cited_by": 12
}