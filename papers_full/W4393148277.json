{
  "title": "Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework",
  "url": "https://openalex.org/W4393148277",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5113151498",
      "name": "Weixi Weng",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5101456902",
      "name": "Chun Yuan",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2739748921",
    "https://openalex.org/W3102057471",
    "https://openalex.org/W2709553318",
    "https://openalex.org/W6676033746",
    "https://openalex.org/W6676587327",
    "https://openalex.org/W6725448924",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3012101827",
    "https://openalex.org/W6749414244",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W6680880821",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W3006800054",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6631782140",
    "https://openalex.org/W4225322345",
    "https://openalex.org/W6637568146",
    "https://openalex.org/W2970929725",
    "https://openalex.org/W6760900574",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W4229078651",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2886642929",
    "https://openalex.org/W6785652829",
    "https://openalex.org/W6610754620",
    "https://openalex.org/W1483870316",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2905357333",
    "https://openalex.org/W2748021867",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W6734871034",
    "https://openalex.org/W3206713300",
    "https://openalex.org/W3012704771",
    "https://openalex.org/W3013647346",
    "https://openalex.org/W3172507542",
    "https://openalex.org/W4292199959",
    "https://openalex.org/W4360837890",
    "https://openalex.org/W4221163994",
    "https://openalex.org/W2605287558",
    "https://openalex.org/W3035564946",
    "https://openalex.org/W2115403315",
    "https://openalex.org/W3034937575",
    "https://openalex.org/W4386071764",
    "https://openalex.org/W2110091014",
    "https://openalex.org/W3198359486",
    "https://openalex.org/W4313160574",
    "https://openalex.org/W3034998639",
    "https://openalex.org/W3034779842",
    "https://openalex.org/W2964115968",
    "https://openalex.org/W1542886316",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3118508671",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4313160378",
    "https://openalex.org/W3035175896",
    "https://openalex.org/W4366400469",
    "https://openalex.org/W3004591935",
    "https://openalex.org/W2968634921",
    "https://openalex.org/W2593768305",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2104094955",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W4294294767",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2989236540",
    "https://openalex.org/W2953127297",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W4304091663",
    "https://openalex.org/W2963664410",
    "https://openalex.org/W3021542222",
    "https://openalex.org/W2953106684",
    "https://openalex.org/W2148440006",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W4313160863",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2963730616",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3166409449",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W4282945772"
  ],
  "abstract": "Unsupervised domain adaptation object detection(UDAOD) research on Detection Transformer(DETR) mainly focuses on feature alignment and existing methods can be divided into two kinds, each of which has its unresolved issues. One-stage feature alignment methods can easily lead to performance fluctuation and training stagnation. Two-stage feature alignment method based on mean teacher comprises a pretraining stage followed by a self-training stage, each facing problems in obtaining reliable pretrained model and achieving consistent performance gains. Methods mentioned above have not yet explore how to utilize the third related domain such as target-like domain to assist adaptation. To address these issues, we propose a two-stage framework named MTM, i.e. Mean Teacher-DETR with Masked Feature Alignment. In the pretraining stage, we utilize labeled target-like images produced by image style transfer to avoid performance fluctuation. In the self-training stage, we leverage unlabeled target images by pseudo labels based on mean teacher and propose a module called Object Queries Knowledge Transfer(OQKT) to ensure consistent performance gains of the student model. Most importantly, we propose masked feature alignment methods including Masked Domain Query-based Feature Alignment(MDQFA) and Masked Token-wise Feature Alignment(MTWFA) to alleviate domain shift in a more robust way, which not only prevent training stagnation and lead to a robust pretrained model in the pretraining stage, but also enhance the model's target performance in the self-training stage. Experiments on three challenging scenarios and a theoretical analysis verify the effectiveness of MTM.",
  "full_text": "Mean Teacher DETR with Masked Feature Alignment:\nA Robust Domain Adaptive Detection Transformer Framework\nWeixi Weng, Chun Yuan\nTsinghua Shenzhen International Graduate School, Tsinghua University\nwengwx22@mails.tsinghua.edu.cn, yuanc@sz.tsinghua.edu.cn\nAbstract\nUnsupervised domain adaptive object detection (UDAOD)\nresearch on Detection Transformer (DETR) mainly focuses\non feature alignment and existing methods can be divided into\ntwo kinds, each of which has its unresolved issues. One-stage\nfeature alignment methods can easily lead to performance\nfluctuation and training stagnation. Two-stage feature align-\nment method based on mean teacher comprises a pretraining\nstage followed by a self-training stage, each facing problems\nin obtaining a reliable pretrained model and achieving consis-\ntent performance gains. Methods mentioned above have not\nyet explored how to utilize the third related domain such as\nthe target-like domain to assist adaptation. To address these\nissues, we propose a two-stage framework named MTM, i.e.\nMean Teacher-DETR with Masked Feature Alignment. In\nthe pretraining stage, we utilize labeled target-like images\nproduced by image style transfer to avoid performance fluc-\ntuation. In the self-training stage, we leverage unlabeled tar-\nget images by pseudo labels based on mean teacher and pro-\npose a module called Object Queries Knowledge Transfer\n(OQKT) to ensure consistent performance gains of the stu-\ndent model. Most importantly, we propose masked feature\nalignment methods including Masked Domain Query-based\nFeature Alignment (MDQFA) and Masked Token-Wise Fea-\nture Alignment (MTWFA) to alleviate domain shift in a more\nrobust way, which not only prevent training stagnation and\nlead to a robust pretrained model in the pretraining stage\nbut also enhance the model’s target performance in the self-\ntraining stage. Experiments on three challenging scenarios\nand a theoretical analysis verify the effectiveness of MTM.\nIntroduction\nObject detection is always recognized as an important task\nin the field of computer vision. Recently, Detection Trans-\nformer (DETR) (Carion et al. 2020) redefines object predic-\ntion by departing from the traditional anchor-based method-\nology (Girshick 2015) and embracing the concept of ob-\nject queries which serve as learnable embeddings interacting\nwith the image features, allowing the model to predict ob-\nject classes and bounding box coordinates. While DETR has\ndemonstrated remarkable performance on various datasets,\nits application to real-world environments still presents chal-\nlenges, particularly when training data and testing data are\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ncollected from different distributions, i.e. domain shift.\nIn order to enable object detectors trained on a labeled\nsource domain to be effectively deployed on a completely\nunlabeled target domain, unsupervised domain adaptive ob-\nject detection(UDAOD) emerged as a solution. UDAOD re-\nsearch on DETR is gradually gaining increasing attention,\nwith particular emphasis on feature alignment (Wang et al.\n2021; Gong et al. 2022; Yu et al. 2022; Zhang et al. 2023).\nFeature alignment is commonly implemented through ad-\nversarial training, with the objective of extracting domain-\ninvariant global-level, local-level and instance-level features\nfrom source domain and target domain, thus reducing do-\nmain shift. Existing methods on DETR can be categorized\ninto one-stage feature alignment methods (Wang et al. 2021;\nGong et al. 2022; Zhang et al. 2023) and two-stage feature\nalignment method based on mean teacher (Yu et al. 2022).\nWe deeply studied the above two methods and identified sev-\neral inherent issues, which will be elaborated below.\nOne-stage feature alignment methods (Xu et al. 2020a;\nGong et al. 2022; Zhang et al. 2023) generally train DETR\nfrom scratch with labeled source images and unlabeled tar-\nget images, conducting feature alignment on the output of\nthe backbone, encoder, and decoder. However, training in\nthis manner has inherent issues: (I1) performance fluctua-\ntion: the lack of labels for target images restricts the model’s\nability to extract features from target images, and conduct-\ning feature alignment with low-quality target features eas-\nily results in performance fluctuation. Furthermore, it’s very\nlikely to end up with an underperforming model as shown\nby the red line in Figure 1; (I2) training stagnation: at the\nearly training stage, if there is a significant difference in the\ndistribution between the two types of training data, domain\ndiscriminators converge quickly and become proficient at\ndistinguishing features in the later period of training. How-\never, the backbone struggles to produce domain-invariant\nfeatures at this point, which results in marginal performance\nimprovement as shown by the yellow line in Figure 1.\nTwo-stage feature alignment method (Yu et al. 2022)\nbased on mean teacher (Sohn et al. 2020; Xu et al. 2021)\naims to leverage unlabeled target images by self-training.\nTake MTTrans (Yu et al. 2022) for example, it comprises a\npretraining stage followed by a self-training stage. It firstly\npretrains a model merely on labeled source images and only\nconducts feature alignment in the self-training stage, shar-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5912\nMasked Feature Alignment +\nsource images & target-like images\nNon-masked Feature Alignment +\nsource images & target-like images\nSource images & target-like images\nNon-masked Feature Alignment +\nsource images & target images\nOnly source images\nMasked Feature Alignment +\nsource images & target-like images\nNon-masked Feature Alignment +\nsource images & target-like images\nSource images & target-like images\nNon-masked Feature Alignment +\nsource images & target images\nOnly source images\nFigure 1: We train Deformable DETR from scratch with different pretraining strategies in the weather adaptation scenario. It\nturns out that pretraining with masked feature alignment on source images and target-like images brings the best performance.\ning the same issues with the one-stage methods. Although\nMTTrans significantly improves target performance through\nmean teacher, it also faces other issues: (I3) unreliable pre-\ntrained model: its pretrained model is obtained by training\non labeled source images without any feature alignment,\nresulting in the teacher model failing to generate accurate\npseudo-labels; (I4) unstable performance gains: it uses the\nsame the object queries between the teacher and the stu-\ndent model in the self-training stage, resulting in the student\nmodel struggling to attain consistent performance gains.\nFurthermore, the methods mentioned above only consider\nadapting between two domains, while how to use the third\nrelated domain such as a target-like domain generated by\nimage style transfer to assist adaptation is still unexplored.\nBased on the facts above, we propose MTM, i.e. Mean\nTeacher-DETR with Masked Feature Alignment consisting\nof a pretraining stage and a self-training stage. In the pre-\ntraining stage, we utilize labeled source images and labeled\ntarget-like images produced by cycleGAN (Zhu et al. 2017)\ninstead of unlabeld target images to avoid the problem of\nperformance fluctuation (I1). In the self-training stage, we\nadopt the mean teacher framework to leverage target images\nby pseudo labels. We further propose Object Queries Knowl-\nedge Transfer (OQKT) which enhances the semantic infor-\nmation of the student model’s object queries by multi-head\nattention to prompt consistent performance gains of the stu-\ndent model (I4). Most importantly, we propose masked fea-\nture alignment methods including Masked Domain Query-\nbased Feature Alignment (MDQFA) and Masked Token-\nWise Feature Alignment (MTWFA) to alleviate domain shift\nin a more robust way, which not only prevent training stag-\nnation (I2) and lead to robust pretrained model (I3) in the\npretraining stage but also enhance the model’s final target\nperformance in the self-training stage. The contributions of\nthis paper are as follows:\n• Through experiments we find that utilizing labeled\ntarget-like images produced by CycleGAN (Zhu et al.\n2017) to participate in training DETR from scratch\navoids performance fluctuation.\n• We propose Object Queries Knowledge Transfer\n(OQKT) based on mean teacher DETR to guarantee con-\nsistent performance gains of the student model.\n• Most importantly, we propose masked feature alignment\nmethods including Masked Domain Query-based Feature\nAlignment (MDQFA) and Masked Token-Wise Feature\nAlignment (MTWFA) to reduce domain shift in a more\nrobust way. They benefit both stages of MTM.\n• Extensive experiments on three challenging domain\nadaptation scenarios have demonstrated that MTM has\noutperformed existing state-of-the-art (SOTA) methods\nin this field. A theoretical analysis is also presented to\nverify the effectiveness of MTM.\nRelated Work\nDetection Transformer\nObject detection is a challenging task in the field of com-\nputer vision. Recently, the Detection Transformer(DETR)\nhas garnered significant attention by presenting a novel ob-\nject detection pipeline based on Transformer. Deformable\nDETR (Zhu et al. 2021), as an important advancement of\nDETR, introduces deformable attention modules that only\nattend to a small set of key sampling points around a refer-\nence. Deformable DETR achieves better performance than\nDETR with much less convergence time and computational\nrequirements. Deformable DETR has several variants and\nthe two-stage variant achieves the best performance. In the\ntwo-stage Deformable DETR, the encoder generates ob-\nject queries from multi-scale features extracted by the CNN\nbackbone, while the decoder refines object queries and em-\nploys them for predicting object classes and generating\nbounding box coordinates. Following (Wang et al. 2021), we\nmainly investigate UDAOD on Deformable DETR, specifi-\ncally the two-stage Deformable DETR.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5913\nDomain Adaptive Object Detection\nExtensive research has been conducted on unsupervised do-\nmain adaptive object detection (UDAOD) based on differ-\nent architectures. Proposed solutions can be categorized into\nthree mainstreams: feature alignment, mean teacher and do-\nmain transfer. These kinds of solutions often work together\nto form a more robust framework. However, research on\nDETR so far has mainly focused on feature alignment.\nDAF (Chen et al. 2018) is the first work to apply feature\nalignment on Faster RCNN by means of adversarial train-\ning, performing feature alignment at the image-level and\ninstance-level respectively. Recently, there has been a con-\ntinuous surge of research based on DETR. SFA (Wang et al.\n2021) makes a pioneering attempt to conduct sequence fea-\nture alignment on different levels based on DETR. O 2Net\n(Gong et al. 2022) introduces an Object-Aware Alignment\nmodule to align the multi-scale features of the CNN back-\nbone. DA-DETR (Zhang et al. 2023) proposes a CNN-\nTransformer Blender which fuses the output of the CNN\nbackbone and the encoder to better align them.\nMTOR (Cai et al. 2019) firstly exploits mean teacher\non Faster RCNN, building a consistency loss between the\nteacher and the student model to learn more about the ob-\nject relation. MTTrans (Yu et al. 2022) firstly applies mean\nteacher on DETR, proposing multi-level feature alignment\nto improve the quality of pseudo labels.\nMethods of domain transfer are normally built on Cycle-\nGAN (Zhu et al. 2017) which conducts image style trans-\nfer between domains. Given source images and target im-\nages, CycleGAN generates target-like images by transfer-\nring source images into target style, as well as source-\nlike images. UMT (Deng et al. 2021) additionally utilizes\nsource-like and target-like images for training, and proposes\nseveral strategies on mean teacher to address model bias.\nAW ADA (Menke, Wenzel, and Schwung 2022) uses the pro-\nposals generated by the object detector to aid in more effi-\ncient image style transfer. While domain transfer has been\nextensively studied on other object detectors (Ren et al.\n2015), to the best of our knowledge, how to use it to assist\ndomain adaptation on DETR remains unexplored.\nMethods\nThis section introduces MTM consisting of a robust pre-\ntraining stage and a performance-enhancing self-training\nstage.\nMTM Framework Overview\nIn the pretraining stage, we utilize labeled source images and\nlabeled target-like images for training. A target-like image\nproduced by CycleGAN (Zhu et al. 2017) retains the con-\ntent of the source image while exhibiting the style of target\nimages, thereby sharing identical annotations with its cor-\nresponding source images. Pretraining with labeled target-\nlike images not only avoids performance fluctuation (I1) as\nshown by the green line in Figure 1 but also contributes to\nobtaining a robust pretrained model (I3).\nIn the self-training stage, we utilize labeled source images\nand unlabeled target images for training. Given the absence\nof target images during pretraining, we leverage them by\npseudo labels based on mean teacher in this stage. We fur-\nther propose Object Queries Knowledge Transfer (OQKT)\nbased on mean teacher DETR to ensure consistent perfor-\nmance gains of the student model (I4).\nMost importantly, we propose masked feature align-\nment methods including Masked Domain Query-based Fea-\nture Alignment (MDQFA) and Masked Token-Wise Feature\nAlignment (MTWFA) which not only prevent training stag-\nnation (I2) and help to obtain a robust pretrained model (I3)\nin the pretraining stage but also contribute to enhancing the\nmodel’s final target performance in the self-training stage.\nObject Queries Knowledge Transfer\nIn the mean teacher framework, the teacher model takes in\nweakly augmented target images as input while the student\nmodel processes strongly augmented counterparts. As they\nundergo different data augmentation strategies, it is highly\nlikely to result in significantly different object queries be-\ntween them. Therefore, enforcing the student to use the same\nobject queries as the teacher (Yu et al. 2022) tends to re-\nstrain consistent performance gains of the student. However,\nwe can’t ignore that the teacher model is likely to generate\nvaluable object queries from weakly augmented target im-\nages. In order to transfer the knowledge within the object\nqueries from the teacher model to the student model and ad-\ndress the issue of unstable performance gains (I4), we pro-\npose a simple yet effective module named Object Queries\nKnowledge Transfer(OQKT).\nQEt represents query embeddings of the teacher model’s\nobject queries and PEt is the corresponding position em-\nbeddings. The teacher model’s object queries are obtained\nby adding QEt and PEt together. In a similar way, QEs\nand PEs are defined. We further define:\nQuery = QEs + P Es\nKey = QEt + P Et\nV alue= QEt\n(1)\nThe knowledge transfer mechanism of this module is\nbased on the multi-head attention mechanism in (Vaswani\net al. 2017), which is shown as follows:\nATTENTION (Q, K, V) =softmax(QKT\n√dk\nV ) (2)\nMULTI HEAD (Q, K, V) =Concat(head1, ··· headn)WO\nwhere headi = ATTENTION (QWQ\ni , KWK\ni , V WV\ni )\n(3)\nWhere WQ\ni ,WK\ni , WV\ni and WO\ni are weight matrices and\nn = 16, dk = 16 in OQKT. The teacher’s object queries\nand the student’s object queries interact via the multi-head\nattention mechanism to obtain additional features. The ad-\nditional features are multiplied by a parameter and added\nto the student’s query embeddings, enhancing the semantic\ninformation of the student’s object queries for better predic-\ntion. QES is updated as follows:\nQES = QES + α × MULTI HEAD (Query, Key, V alue)(4)\nIn the self-training stage, with the increasing of epochs,α\ndecreases linearly from 1 to 0, promoting the student model\nto generate high-quality object queries by itself.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5914\nFigure 2: MTM framework in the self-training stage. In the mean teacher structure, the student model is updated by back-\npropagation while the teacher model is updated by the Exponential Moving Average (EMA) of the student model. OQKT\nenhances the student’s object queries with the teacher’s object queries by the multi-head attention mechanism. Masked feature\nalignment is also conducted in this stage to alleviate the domain shift between source data and target data.\nMasked Feature Alignment\nProposed by (Wang et al. 2021), Domain Query-based Fea-\nture Alignment (DQFA) alleviates global-level domain shift\nfrom image style, weather etc., while Token-Wise Feature\nAlignment (TWFA) deals with local-level and instance-level\ndomain shift caused by object appearance, scale, texture\netc. However, when conducting them on source images and\ntarget-like images in the pretraining stage, training stagna-\ntion is observed as shown by the yellow line in Figure 1.\nTo prevent training stagnation and elevate the model’s\ntarget performance, we innovatively propose masked fea-\nture alignment methods including Masked Domain Query-\nbased Feature Alignment (MDQFA) and Masked Token-\nWise Feature Alignment (MTWFA). MDQFA and MTWFA\nrandomly mask sequence features before feeding them into\ndomain discriminators, making it harder to classify their do-\nmains. Our masked feature alignment methods not only hin-\nder the domain discriminators from converging too quickly\nthus preventing training stagnation, but also improve the\ndomain discriminators’ robustness through a diverse range\nof masked sequence features, consequently further elevat-\ning the model’s target performance. Details of MDQFA and\nMTWFA are described below. The domain discriminators’\nstructures for both masked feature alignment methods ex-\nhibit identical architectures, as depicted in Figure 2.\nMasked Domain Query-based Feature Alignment On\nthe encoder side, a domain query qenc ∈ R1×C is concate-\nnated with the multi-scale feature extracted from the CNN\nbackbone to form the input for the encoder:\nZ0 = [qenc, z1, z2, ..., zNenc] +Epos + Elevel (5)\nNenc stands for the length of the encoder input embeddings.\nEpos and Elevel represents corresponding position embed-\ndings and feature level embeddings. The sequence features\nderived from the l-th layer are denoted as Zl. During the en-\ncoding process, the domain query gathers domain-specific\nfeatures from the sequence feature. We further set a random\nmask which can be formulated as:\nMDQFA\nencl,i =\n\u001a\n0, if Rl,i < θMask\n1, Otherwise (6)\nwhere l = 1...Lenc indexes the layer of the encoder, i ∈\n[0, C) represents the element coordinate of domain query,\nRl,i is a random floating-point number that falls within the\nrange of 0 to 1 and θMask is a hyperparameter controlling\nthe masking rate. Then we compute the element-wise prod-\nuct of the domain query and the mask to get the masked\ndomain query ZM\nl,0 that only contains partial global domain\ninformation. The domain discriminatorDMDQFA\nenc needs to de-\ntermine which domain the masked domain query belongs to.\nWe utilize the binary cross-entropy loss as the domain clas-\nsification loss. The whole procedure is formulated as below:\nZM\nl,0 = MDQFA\nencl ⊙ Zl,0 (7)\nLMDQFA\nencl = −\n\u0002\ndlogDMDQFA\nenc (ZM\nl,0)\n+ (1− d)log\n\u0000\n1 − DMDQFA\nenc (ZM\nl,0)\n\u0001\u0003 (8)\nd represents the domain label with 0 denoting the source\ndomain and 1 denoting the target or target-like domain.\nOn the decoder side, we concatenate a domain query\nqdec ∈ R1×C with the object queries to form the input:\nQ0 = [qdec; q1, q2, ..., qNdec] +E\n′\npos (9)\nNdec stands for the length of the decoder input embeddings.\nE\n′\npos represents position embeddings. The refined object\nqueries derived from the l-th layer are denoted as Ql. The\ngeneration of the random mask of the domain queries in the\ndecoder MDQFA\ndecl\nfollows a similar procedure as equation 7.\nWe feed the masked domain query into the domain discrim-\ninator DMDQFA\ndec and calculate the binary cross-entropy loss.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5915\nMasked Token-Wise Feature AlignmentOn the encoder\nside, each token embedding in the sequence feature stands\nfor a particular-scale feature of the image and aggregates\nlocal-level information from nearby key points. We set a ran-\ndom mask for the whole sequence feature as follows:\nMTWFA\nencl,i,j =\n\u001a\n0, if Rl,i,j < η· θMask\n1, Otherwise (10)\nwhere l = 1...Ldec indexes the layers of the encoder, i ∈\n[1, Nenc] and j ∈ [0, C) respectively represent the horizon-\ntal and vertical coordinates of the sequence feature. Since\ndomain discriminators of MTWFA deal with more diverse\nlocal-level features which are more difficult to classify their\ndomains, we set the hyperparameter η to a decimal between\n0 and 1 indicating that MTWFA will use a smaller mask\nthreshold compared with MDQFA. Then we compute the\nelement-wise product of the sequence feature and the cor-\nresponding mask as the masked sequence feature, which is\nfed into the domain discriminator DMTWFA\nenc . We adopt the\nbinary cross-entropy loss as the domain classification loss.\nThe whole procedure is formulated as below:\nZM\nl = MTWFA\nencl ⊙ Zl (11)\nLMTWFA\nencl = − 1\nNenc\nNencX\ni=1\n\u0002\ndlogDMTWFA\nenc (ZM\nl,i )\n+(1 − d)log\n\u0000\n1 − DMTWFA\nenc (ZM\nl,i )\n\u0001\u0003\n(12)\nd signifies the domain label, where 0 denotes the source\ndomain, and 1 denotes the target or target-like domain.\nOn the decoder side, each object query that corresponds to\na predicted object’s class and position aggregates instance-\nlevel features in the decoding process. Similarly, the se-\nquence features of the decoder are randomly masked and\nthen fed into the domain discriminator DMTWFA\ndec to calculate\nthe corresponding binary cross-entropy loss.\nOverall Training Strategy\nOur MTM framework consists of two training stages. In the\npretraining stage, we aim to train a robust pretrained model,\nwhich will serve as the teacher model later. The loss function\nof this stage Lpre combines the object detection loss and the\nfeature alignment loss, which is defined as follows:\nLpre = Ldet(Is, Bs) +Ldet(Itl, Bs) − Ladv(Is) − Ladv(Itl)\n(13)\nWhere Is and Itl represent source images and target-like\nimages, and they share the same annotations Bs. The fea-\nture alignment loss Ladv consists of four parts, which are\nMDQFA loss on the encoder, MTWFA loss on the encoder,\nMDQFA loss on the decoder and MTWFA on the decoder:\nLadv =\nLencX\nl=1\n(λMDQFA · LMDQFA\nencl + λMTWFA · LMTWFA\nencl )+\nLdecX\nl=1\n(λMDQFA · LMDQFA\ndecl + λMTWFA · LMTWFA\ndecl )\n(14)\nAfter obtaining a robust pretrained model in the pretrain-\ning stage, we then proceed to conduct self-training. The loss\nfunction of this stage Lst can be formulated as:\nLst = Ldet(Is, Bs)+ Ldet(It, ˆBt)−Ladv(Is)−Ladv(It) (15)\nWhere It represents target images and ˆBt refers to pseudo\nlabels produced by the teacher model for It. To summarize,\nthe overall training objective of MTM is defined as:\nmin\nG\nmax\nD\nLdet(G) − Ladv(G, D) (16)\nwhere G is the object detector and D represents the domain\ndiscriminators.\nExperiment\nIn this section, we conduct extensive experiments to testify\nour contributions: (1) pretraining with labeled target-like im-\nages avoids performance fluctuation (I1) and improves target\nperformance; (2) OQKT helps the student model earn con-\nsistent performance gains in the self-training stage (I4); (3)\nour masked feature alignment methods including MDQFA\nand MTWFA prevent training stagnation (I2) and provide\nsignificant performance improvements in both stages.\nDatasets and Settings\nDatasets We evaluate MTM on three challenging domain\nadaptation scenarios. i.e. weather adaptation, scene adapta-\ntion, and synthetic to real adaptation. A detailed introduction\nof the three scenarios is as below:\n• Weather Adaptation: Cityscapes (Cordts et al. 2016) is a\nlandscape dataset containing 2975 training and 500 val-\nidation images. Foggy Cityscapes (Sakaridis, Dai, and\nVan Gool 2018) is generated from Cityscapes by a fog\nsynthesis algorithm. In this scenario, Cityscapes is the\nsource dataset and Foggy Cityscapes is the target dataset.\n• Scene Adaptation: In this scenario, Cityscapes also\nserves as the source dataset. BDD100K (Yu et al. 2020)\nis an autonomous driving dataset consisting of 100k HD\nvideo clips. We utilize the daytime subset of BDD100K\nwhich contains 36728 training images and 5258 valida-\ntion images as the target dataset in this scenario.\n• Synthetic to Real Adaptation: Sim10K (Johnson-\nRoberson et al. 2017) is generated by the Grand Theft\nAuto game engine, containing 10,000 synthetic game\nimages. In this scenario, Sim10K serves as the source\ndataset and Cityscapes serves as the target dataset.\nComparative Benchmarks We compare MTM with five\nbaselines built on Deformable DETR to validate the ef-\nfectiveness of our proposed framework: Deformable DETR\nonly trained on source data (Zhu et al. 2021), SFA (Wang\net al. 2021), O 2Net (Gong et al. 2022), MTTrans (Yu et al.\n2022) and DA-DETR (Zhang et al. 2023).\nEvaluation Metric We report the Average Precision on\nthe car category with a threshold of 0.50 in the synthetic to\nreal adaptation scenario. We adopt the mean Average Preci-\nsion (mAP) of a threshold of 0.50 in the other two scenarios.\nImplementation Details We train CycleGAN (Zhu et al.\n2017) for 100 epochs with a batch size of 8 and a learn-\ning rate of 2 × 10−4 which linearly decreases to 0 after 50\nepochs. We adopt an ImageNet (Deng et al. 2009)-pretrained\nResNet50 network as the CNN backbone. In the pretraining\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5916\nMethod person rider car truck bus train motorcycle bicycle mAP\nDeformable DETR(source) 38.0 38.7 45.3 16.3 26.7 4.2 22.9 36.7 28.5\nSFA(Wang et al. 2021) 46.5 48.6 62.6 25.1 46.2 29.4 28.3 44.0 41.3\nMTTrans(Yu et al. 2022) 47.7 49.9 65.2 25.8 45.9 33.8 32.6 46.5 43.4\nDA-DETR(Zhang et al. 2023) 49.9 50.0 63.1 25.8 45.9 33.8 32.6 46.5 43.5\nO2Net(Gong et al. 2022) 48.7 51.5 63.6 31.1 47.6 47.8 38.0 45.9 46.8\nMTM(ours) 51.0 53.4 67.2 37.2 54.4 41.6 38.4 47.7 48.9\nTable 1: Results(%) in the weather adaptation scenario, i.e. Cityscapes → Foggy Cityscapes.\nMethod person rider car truck bus motorcycle bicycle mAP\nDeformable DETR(source) 38.9 26.7 55.2 15.7 19.7 10.8 16.2 26.2\nSFA(Wang et al. 2021) 40.4 27.6 57.5 19.1 23.4 15.4 19.2 28.9\nO2Net(Gong et al. 2022) 40.4 31.2 58.6 20.4 25.0 14.9 22.7 30.5\nMTTrans(Yu et al. 2022) 44.1 30.1 61.5 25.1 26.9 17.7 23.0 32.6\nMTM(ours) 53.7 35.1 68.8 23.0 28.8 23.8 28.0 37.3\nTable 2: Results(%) in the scene adaptation scenario, i.e. Cityscapes → BDD100K.\nMethod AP on car\nDeformable DETR(source) 47.4\nSFA(Wang et al. 2021) 52.6\nO2Net(Gong et al. 2022) 54.1\nDA-DETR(Zhang et al. 2023) 54.7\nMTTrans(Yu et al. 2022) 57.9\nMTM(ours) 58.1\nTable 3: Results(%) in the synthetic to real adaptation sce-\nnario, i.e. Sim10k → Cityscapes.\nstage of 80 epochs, we use an Adam optimizer with an ini-\ntial learning rate 2 × 10−4 decayed by 0.1 every 40 epochs.\nIn the self-training stage of 20 epochs, the learning rate is\n2 × 10−6. The filtering threshold of pseudo labels is 0.50.\nFollowing (Wang et al. 2021), in the weather adaptation sce-\nnario, λMTWFA is 1, and λMDQFA is 0.1. In other scenarios,\nthey are set to 0.01 and 0.001 respectively. θMask and η are\nset to 0.40 and 0.50. We use one 24GB GeForce RTX 3090\nGPU in all experiments. Each batch includes 1 image from\nthe source domain and 1 image from either the target-like\ndomain or the target domain.\nComparative Study\nThe evaluations of our MTM are conducted on three domain\nadaptation scenarios. In the weather adaptation scenario (Ta-\nble 1), MTM outperforms the SOTA by 2.1 mAP. In the\nscene adaptation scenario (Table 2), MTM shows a signifi-\ncant performance improvement of 4.7 mAP over the current\nSOTA method. In the synthetic to real adaptation scenario\n(Table 3), MTM demonstrates a performance improvement\nof 0.2 mAP compared to MTTrans (Yu et al. 2022), but still\nhas a large improvement compared to the other baselines.\nAblation Studies\nGiven that our framework is made up of two stages, the abla-\ntion studies are conducted in each stage in the weather adap-\ntation scenario to prove the effectiveness of each component.\nResults of the ablation studies are presented in Table 4-6.\nPretraining Stage Several observations can be drawn\nfrom Table 4: (1) pretraining with target-like images signifi-\ncantly enhances the model’s target performance, resulting in\na 13.8 mAP improvement; (2) MDQFA and MTWFA further\nimprove performance compared to their non-masked coun-\nterparts. MDQFA and MTWFA further improve the model’s\ntarget performance by 0.8 mAP and 0.9 mAP compared to\ntheir non-masked counterpart respectively. Combining both\nof them culminates in a 1.9 mAP improvement over their\nnon-masked counterparts combination.\nAs depicted in Figure 1, we can observe that: (1) training\nwith target-like images indeed avoids performance fluctu-\nation (I1); (2) when conducting non-masked feature align-\nment on source images and target-like images, target perfor-\nmance plateaus after 40 epochs, indicating training stagna-\ntion (I2). Yet, masked feature alignment prevents this issue\nand maintains performance growth even after 40 epochs.\nIn Table 5, we present the target performance of pre-\ntrained models under different mask settings controlled by\nhyperparameters θMask and η. The best pretraining perfor-\nmance is attained when θMask = 0.40 and η = 0.50.\nFrom Table 5, we find that setting the mask thresholdθMask\nof MDQFA between 0.30 and 0.50, and the mask thresh-\nold η · θMask of MTWFA between 0.10 and 0.30, further\nenhances the target performance of the pretrained model.\nThe disparate optimal threshold ranges of the two meth-\nods are likely due to the fact that they handle features of\ndifferent levels. MDQFA deals with global domain queries,\nso its domain discriminators are capable of distinguishing\ndomains even when a significant portion of the informa-\ntion is masked. However, MTWFA handles local-level and\ninstance-level sequence features, thus it will easily cause\nconfusion to the domain discriminators of MTWFA if too\nmuch information is dropped from the mask.\nSelf-training Stage Table 6 reveals notable observations\nas below: (1) MT helps increase target performance; (2)\nOQKT helps MT framework further improve target perfor-\nmance. With a pretrained model of 47.2 mAP, MT with\nOQKT outperforms standard MT by 0.2 mAP. MT achieves\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5917\nMethod Target-like MTWFA MDQFA mAP\nD-DETR 28.5\n✓ 42.3\nProposed\n✓ ⃝ 42.7\n✓ ✓ 43.6\n✓ ⃝ 44.9\n✓ ✓ 45.7\n✓ ⃝ ⃝ 45.3\n✓ ✓ ✓ 47.2\nTable 4: Ablation study of the pretraining stage in\nthe weather adaptation scenario. D-DETR stands for\nDeformable-DETR trained on source data. ⃝ refers to\nadopting the non-masked feature alignment method.\nθMask 0.20 0.30 0.40 0.50 0.60 0.70 0.80\nη\n1/3 45.3 45.5 46.1 46.8 45.2 45.0 44.5\n1/2 45.6 46.2 47.2 45.8 44.9 44.3 44.2\n1 45.6 45.8 46.6 45.5 44.9 44.6 43.4\nTable 5: Performance of pretrained models in the weather\nadaptation scenario under different mask settings.\nMethod MT OQKT MTWFA MDQFA mAP\nD-DETR\n28.5\n✓ 35.8\n✓ ✓ 38.3\nProposed\n47.2\n✓ 47.7\n✓ ✓ 47.9\n✓ ✓ ✓ 48.0\n✓ ✓ ✓ 48.1\n✓ ✓ ✓ 48.4\n✓ ✓ ✓ ✓ 48.9\nTable 6: Ablation study of the self-training stage in the\nweather adaptation scenario. MT refers to utilizing the mean\nteacher framework for self-training.\na further performance gain of 2.5 mAP if incorporated with\nOQKT; (3) MDQFA and MTWFA improve target perfor-\nmance by 0.1 mAP and 0.2 mAP respectively, and their com-\nbination brings a performance gain of 0.4 mAP.\nTheoretical Analysis\nThis section analyzes our framework from a theoretical as-\npect based on (Blitzer et al. 2007; Ben-David et al. 2010).\nLet H be a hypothesis space of VC dimensiond. For each\ni ∈ {1, ..., N}, let Si be a labeled sample of size βjm gen-\nerated by drawing βjm points from domain Di and labeling\nthem according to fi, and αi represents domain weight of\nDi. N is set to 2 in our work because we only use two types\nof labeled images including source images and target-like\nimages. If ˆh ∈ His an empirical hypothesis of the empirical\nα-weighted error ˆϵα(h) on these multi-source samples, we\nhave the following theorem to bound the target error of the\nempirical hypothesis ˆh:\nTheorem 1 For anyδ ∈ (0, 1), with probability1 − δ,\nϵT (ˆh) ≤ ˆϵα(ˆh) +1\n2dH∆H(Dα, DT ) +γα\n+\nvuu\nt\n(\n2X\nj=1\nα2\nj\nβj\n)(dln(2m) − ln(δ)\n2m )\n(17)\nwhere γα = minh{ϵT (h) + ϵα(h)} represents the er-\nror of the joint ideal hypothesis that is correlated with the\nability of the detector and the discriminability of features.\ndH∆H(Dα, DT ) is the domain divergence that is associated\nwith the domain-invariance of features.\nIn Theorem 1, the target error ϵT (ˆh) can be bounded by\nfour factors: (1) the empiricalα-weighted error on the multi-\nsource samples ˆϵα(ˆh), which can be minimized by applying\nsupervised loss on the multi-source samples; (2) the domain\ndivergence between the multi-source and the target domain\ndH∆H(Dα, DT ). Our masked feature alignment methods al-\nleviate domain shift to minimize the domain divergence; (3)\nthe error of the joint ideal hypothesis γα. Both our pretrain-\ning stage and self-training stage help enhance the model’s\nability to reduce this value; (4) a complexity term whose\nminimum value is obtained when ∀j ∈ {1,2}, αj = βj.\nThis can be achieved by assigning equal weight to each sam-\nple point in the multi-source data. Based on the analysis\nabove, we can minimize the target error ϵT (ˆh) by MTM.\nConclusion\nIn this work, we deeply investigate UDAOD methods based\non DETR and uncover several issues. Previous one-stage\nfeature alignment methods overlook their inherent issues:\nperformance fluctuation and training stagnation, while the\ntwo-stage feature alignment method based on mean teacher\nintroduces new challenges like unreliable pretrained model\nand unstable performance gains. Besides, how to utilize the\nthird related domain such as the target-like domain to as-\nsist domain adaptation remains unexplored in existing meth-\nods. To address the issues and build a robust domain adap-\ntive Detection Transformer framework, we propose a two-\nstage framework named MTM, i.e. Mean Teacher DETR\nwith Masked Feature Alignment.\nIn the pretraining stage, CycleGAN is used to generate\ntarget-like images. We incorporate generated target-like im-\nages in pretraining to avoid performance fluctuation. In the\nself-training stage, we leverage unlabeled target images by\npseudo labels based on mean teacher. We propose Object\nQueries Knowledge Transfer (OQKT) to achieve consis-\ntent performance gains. Above all, we propose masked fea-\nture alignment including Masked Domain Query-based Fea-\nture Alignment (MDQFA) and Masked Token-Wise Feature\nAlignment (MTWFA) to alleviate domain shift in a more ro-\nbust way. Our masked feature alignment methods not only\nprevent training stagnation and lead to a robust pretrained\nmodel in the pretraining stage but also enhance the model’s\nfinal target performance in the self-training stage.\nExperimental results and a theoretical analysis have\nproven the effectiveness of MTM. We expect that our re-\nsearch will inspire future work in this area.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5918\nAcknowledgements\nThis work was supported by the National Key R&D\nProgram of China (2022YFB4701400/4701402), SSTIC\nGrant(KJZD20230923115106012), Shenzhen Key Labora-\ntory (ZDSYS20210623092001004), and Beijing Key Lab of\nNetworked Multimedia.\nReferences\nAnthony, M.; Bartlett, P. L.; Bartlett, P. L.; et al. 1999.Neu-\nral network learning: Theoretical foundations, volume 9.\ncambridge university press Cambridge.\nArjovsky, M.; Chintala, S.; and Bottou, L. 2017. Wasserstein\ngenerative adversarial networks. InInternational conference\non machine learning, 214–223. PMLR.\nArruda, V . F.; Paixao, T. M.; Berriel, R. F.; De Souza, A. F.;\nBadue, C.; Sebe, N.; and Oliveira-Santos, T. 2019. Cross-\ndomain car detection using unsupervised image-to-image\ntranslation: From day to night. In 2019 International Joint\nConference on Neural Networks (IJCNN), 1–8. IEEE.\nBartlett, P. L.; Foster, D. J.; and Telgarsky, M. J. 2017.\nSpectrally-normalized margin bounds for neural networks.\nAdvances in neural information processing systems, 30.\nBen-David, S.; Blitzer, J.; Crammer, K.; Kulesza, A.;\nPereira, F.; and Vaughan, J. W. 2010. A theory of learning\nfrom different domains. Machine learning, 79: 151–175.\nBlitzer, J.; Crammer, K.; Kulesza, A.; Pereira, F.; and Wort-\nman, J. 2007. Learning bounds for domain adaptation. Ad-\nvances in neural information processing systems, 20.\nBousmalis, K.; Trigeorgis, G.; Silberman, N.; Krishnan, D.;\nand Erhan, D. 2016. Domain separation networks.Advances\nin neural information processing systems, 29.\nCai, Q.; Pan, Y .; Ngo, C.-W.; Tian, X.; Duan, L.; and Yao, T.\n2019. Exploring object relation in mean teacher for cross-\ndomain detection. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 11457–\n11466.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part I 16, 213–229. Springer.\nChen, C.; Zheng, Z.; Ding, X.; Huang, Y .; and Dou, Q. 2020.\nHarmonizing transferability and discriminability for adapt-\ning object detectors. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 8869–\n8878.\nChen, M.; Chen, W.; Yang, S.; Song, J.; Wang, X.; Zhang,\nL.; Yan, Y .; Qi, D.; Zhuang, Y .; Xie, D.; and Pu, S. 2022.\nLearning Domain Adaptive Object Detection with Proba-\nbilistic Teacher. arXiv:2206.06293.\nChen, Y .; Li, W.; Sakaridis, C.; Dai, D.; and Van Gool, L.\n2018. Domain adaptive faster r-cnn for object detection in\nthe wild. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 3339–3348.\nCordts, M.; Omran, M.; Ramos, S.; Rehfeld, T.; Enzweiler,\nM.; Benenson, R.; Franke, U.; Roth, S.; and Schiele, B.\n2016. The cityscapes dataset for semantic urban scene un-\nderstanding. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 3213–3223.\nCrammer, K.; Kearns, M.; and Wortman, J. 2008. Learn-\ning from Multiple Sources. Journal of Machine Learning\nResearch, 9(8).\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDeng, J.; Li, W.; Chen, Y .; and Duan, L. 2021. Unbiased\nmean teacher for cross-domain object detection. InProceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 4091–4101.\nDing, L.; Wang, L.; and Tao, D. 2020. Self-Attention with\nCross-Lingual Position Representation. arXiv:2004.13310.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nGanin, Y .; Ustinova, E.; Ajakan, H.; Germain, P.; Larochelle,\nH.; Laviolette, F.; Marchand, M.; and Lempitsky, V . 2016.\nDomain-adversarial training of neural networks.The journal\nof machine learning research, 17(1): 2096–2030.\nGirshick, R. 2015. Fast r-cnn. In Proceedings of the IEEE\ninternational conference on computer vision, 1440–1448.\nGong, K.; Li, S.; Li, S.; Zhang, R.; Liu, C. H.; and Chen,\nQ. 2022. Improving Transferability for Domain Adaptive\nDetection Transformers. In Proceedings of the 30th ACM\nInternational Conference on Multimedia, 1543–1551.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2020. Generative adversarial networks. Communications of\nthe ACM, 63(11): 139–144.\nGuo, T.; Huynh, C. P.; and Solh, M. 2019. Domain-adaptive\npedestrian detection in thermal images. In 2019 IEEE in-\nternational conference on image processing (ICIP), 1660–\n1664. IEEE.\nHe, F.; Liu, T.; and Tao, D. 2020. Why resnet works? resid-\nuals generalize. IEEE transactions on neural networks and\nlearning systems, 31(12): 5349–5362.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHe, M.; Wang, Y .; Wu, J.; Wang, Y .; Li, H.; Li, B.; Gan, W.;\nWu, W.; and Qiao, Y . 2022. Cross domain object detection\nby target-perceived dual branch distillation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 9570–9580.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5919\nJohnson-Roberson, M.; Barto, C.; Mehta, R.; Sridhar, S. N.;\nRosaen, K.; and Vasudevan, R. 2017. Driving in the Matrix:\nCan Virtual Worlds Replace Human-Generated Annotations\nfor Real World Tasks? arXiv:1610.01983.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2017. Im-\nagenet classification with deep convolutional neural net-\nworks. Communications of the ACM, 60(6): 84–90.\nLi, W.; Li, F.; Luo, Y .; Wang, P.; et al. 2020. Deep domain\nadaptive object detection: a survey. In 2020 IEEE Sympo-\nsium Series on Computational Intelligence (SSCI), 1808–\n1813. IEEE.\nLiu, S.; John, V .; Blasch, E.; Liu, Z.; and Huang, Y . 2018.\nIR2VI: Enhanced night environmental perception by unsu-\npervised thermal image translation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion Workshops, 1153–1160.\nLiu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.;\nFu, C.-Y .; and Berg, A. C. 2016. Ssd: Single shot multibox\ndetector. In Computer Vision–ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11–14,\n2016, Proceedings, Part I 14, 21–37. Springer.\nLv, W.; Zhao, Y .; Xu, S.; Wei, J.; Wang, G.; Cui, C.; Du,\nY .; Dang, Q.; and Liu, Y . 2023. DETRs Beat YOLOs on\nReal-time Object Detection. arXiv:2304.08069.\nMenke, M.; Wenzel, T.; and Schwung, A. 2022. AW ADA:\nAttention-Weighted Adversarial Domain Adaptation for Ob-\nject Detection. arXiv:2208.14662.\nPan, S. J.; Tsang, I. W.; Kwok, J. T.; and Yang, Q. 2010.\nDomain adaptation via transfer component analysis. IEEE\ntransactions on neural networks, 22(2): 199–210.\nRedmon, J.; Divvala, S.; Girshick, R.; and Farhadi, A. 2016.\nYou only look once: Unified, real-time object detection. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 779–788.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn:\nTowards real-time object detection with region proposal net-\nworks. Advances in neural information processing systems,\n28.\nSaito, K.; Ushiku, Y .; Harada, T.; and Saenko, K. 2019.\nStrong-weak distribution alignment for adaptive object de-\ntection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 6956–6965.\nSakaridis, C.; Dai, D.; and Van Gool, L. 2018. Semantic\nfoggy scene understanding with synthetic data. Interna-\ntional Journal of Computer Vision, 126: 973–992.\nShen, Z.; Maheshwari, H.; Yao, W.; and Savvides, M. 2019.\nScl: Towards accurate domain adaptive object detection via\ngradient detach based stacked complementary losses. arXiv\npreprint arXiv:1911.02559.\nSimonyan, K.; and Zisserman, A. 2015. Very Deep Convo-\nlutional Networks for Large-Scale Image Recognition. In\nBengio, Y .; and LeCun, Y ., eds.,3rd International Confer-\nence on Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceedings.\nSohn, K.; Zhang, Z.; Li, C.-L.; Zhang, H.; Lee, C.-Y .;\nand Pfister, T. 2020. A Simple Semi-Supervised Learning\nFramework for Object Detection. arXiv:2005.04757.\nTzeng, E.; Hoffman, J.; Saenko, K.; and Darrell, T. 2017.\nAdversarial discriminative domain adaptation. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, 7167–7176.\nVan der Maaten, L.; and Hinton, G. 2008. Visualizing data\nusing t-SNE. Journal of machine learning research, 9(11).\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, W.; Cao, Y .; Zhang, J.; He, F.; Zha, Z.-J.; Wen, Y .;\nand Tao, D. 2021. Exploring sequence feature alignment\nfor domain adaptive detection transformers. In Proceedings\nof the 29th ACM International Conference on Multimedia,\n1730–1738.\nXu, C.-D.; Zhao, X.-R.; Jin, X.; and Wei, X.-S. 2020a. Ex-\nploring categorical regularization for domain adaptive object\ndetection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 11724–11733.\nXu, M.; Wang, H.; Ni, B.; Tian, Q.; and Zhang, W. 2020b.\nCross-domain detection via graph-induced prototype align-\nment. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 12355–12364.\nXu, M.; Zhang, Z.; Hu, H.; Wang, J.; Wang, L.; Wei, F.; Bai,\nX.; and Liu, Z. 2021. End-to-end semi-supervised object\ndetection with soft teacher. InProceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 3060–3069.\nYu, F.; Chen, H.; Wang, X.; Xian, W.; Chen, Y .; Liu, F.;\nMadhavan, V .; and Darrell, T. 2020. BDD100K: A Di-\nverse Driving Dataset for Heterogeneous Multitask Learn-\ning. arXiv:1805.04687.\nYu, J.; Liu, J.; Wei, X.; Zhou, H.; Nakata, Y .; Gudovskiy,\nD. A.; Okuno, T.; Li, J.; Keutzer, K.; and Zhang, S.\n2022. MTTrans: Cross-domain Object Detection with Mean\nTeacher Transformer. In Avidan, S.; Brostow, G. J.; Ciss ´e,\nM.; Farinella, G. M.; and Hassner, T., eds.,Computer Vision\n- ECCV 2022 - 17th European Conference, Tel Aviv, Israel,\nOctober 23-27, 2022, Proceedings, Part IX, volume 13669\nof Lecture Notes in Computer Science, 629–645. Springer.\nZhang, J.; Huang, J.; Luo, Z.; Zhang, G.; Zhang, X.; and\nLu, S. 2023. DA-DETR: Domain Adaptive Detection Trans-\nformer With Information Fusion. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 23787–23798.\nZhao, L.; and Wang, L. 2022. Task-specific inconsistency\nalignment for domain adaptive object detection. InProceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 14217–14226.\nZhu, J.-Y .; Park, T.; Isola, P.; and Efros, A. A. 2017. Un-\npaired image-to-image translation using cycle-consistent ad-\nversarial networks. InProceedings of the IEEE international\nconference on computer vision, 2223–2232.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2021.\nDeformable DETR: Deformable Transformers for End-to-\nEnd Object Detection. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5920",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5578448176383972
    },
    {
      "name": "Transformer",
      "score": 0.5028521418571472
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4633861482143402
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40692275762557983
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.35755571722984314
    },
    {
      "name": "Data mining",
      "score": 0.32984018325805664
    },
    {
      "name": "Engineering",
      "score": 0.14474543929100037
    },
    {
      "name": "Electrical engineering",
      "score": 0.06133627891540527
    },
    {
      "name": "Linguistics",
      "score": 0.05388614535331726
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}