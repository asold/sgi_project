{
  "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
  "url": "https://openalex.org/W2981852735",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222913930",
      "name": "Raffel, Colin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224379215",
      "name": "Shazeer, Noam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224145415",
      "name": "Roberts, Adam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2330663980",
      "name": "Lee, Katherine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224379199",
      "name": "Narang, Sharan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286870704",
      "name": "Matena, Michael",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2378589247",
      "name": "Zhou, Yanqi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1894711984",
      "name": "Li Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288824284",
      "name": "Liu, Peter J.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2510153535",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W2786685006",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2535838896",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2989033444",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2170716095",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2120615054",
    "https://openalex.org/W2978708643",
    "https://openalex.org/W2921848006",
    "https://openalex.org/W2966989210",
    "https://openalex.org/W2257408573",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2971531230",
    "https://openalex.org/W2267186426",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2798858969",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2949776890",
    "https://openalex.org/W2804236178",
    "https://openalex.org/W2963121782",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W1752492850",
    "https://openalex.org/W2913946806",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2251994258",
    "https://openalex.org/W2963644595",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2947813521",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2945290257",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2963703197",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2775461895",
    "https://openalex.org/W2612953412",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W2149933564",
    "https://openalex.org/W2924690340",
    "https://openalex.org/W2116612304",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2953958347",
    "https://openalex.org/W2974875810",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2889326796",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W1598866093",
    "https://openalex.org/W2901394229",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W2797328513",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2940024477",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2809324505",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W2900167092",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2964165804",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W1486649854",
    "https://openalex.org/W2787214294",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2161381512",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963804993",
    "https://openalex.org/W2979736514",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2578673564",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2912018747",
    "https://openalex.org/W2950613642",
    "https://openalex.org/W2963667932",
    "https://openalex.org/W2341401723",
    "https://openalex.org/W2958953787",
    "https://openalex.org/W3036267641",
    "https://openalex.org/W2804935296",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2155893237",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2966892770",
    "https://openalex.org/W2612431505",
    "https://openalex.org/W2295072214",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2250653840",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2130942839"
  ],
  "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
  "full_text": "Journal of Machine Learning Research 21 (2020) 1-67 Submitted 1/20; Revised 6/20; Published 6/20\nExploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer\nColin Raffel∗ craffel@gmail.com\nNoam Shazeer∗ noam@google.com\nAdam Roberts∗ adarob@google.com\nKatherine Lee∗ katherinelee@google.com\nSharan Narang sharannarang@google.com\nMichael Matena mmatena@google.com\nYanqi Zhou yanqiz@google.com\nWei Li mweili@google.com\nPeter J. Liu peterjliu@google.com\nGoogle, Mountain View, CA 94043, USA\nEditor: Ivan Titov\nAbstract\nTransfer learning, where a model is first pre-trained on a data-rich task before being fine-\ntuned on a downstream task, has emerged as a powerful technique in natural language\nprocessing (NLP). The effectiveness of transfer learning has given rise to a diversity of\napproaches, methodology, and practice. In this paper, we explore the landscape of transfer\nlearning techniques for NLP by introducing a unified framework that converts all text-based\nlanguage problems into a text-to-text format. Our systematic study compares pre-training\nobjectives, architectures, unlabeled data sets, transfer approaches, and other factors on\ndozens of language understanding tasks. By combining the insights from our exploration\nwith scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results\non many benchmarks covering summarization, question answering, text classification, and\nmore. To facilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.1\nKeywords: transfer learning, natural language processing, multi-task learning, attention-\nbased models, deep learning\n1. Introduction\nTraining a machine learning model to perform natural language processing (NLP) tasks\noften requires that the model can process text in a way that is amenable to downstream\nlearning. This can be loosely viewed as developing general-purpose knowledge that allows\nthe model to “understand” text. This knowledge can range from low-level (e.g. the spelling\n∗. Equal contribution. A description of each author’s contribution is available in Appendix A. Correspondence\nto craffel@gmail.com.\n1. https://github.com/google-research/text-to-text-transfer-transformer\n©2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J. Liu.\nLicense: CC-BY 4.0, seehttps://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at\nhttp://jmlr.org/papers/v21/20-074.html.\narXiv:1910.10683v4  [cs.LG]  19 Sep 2023\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nor meaning of words) to high-level (e.g. that a tuba is too large to fit in most backpacks).\nIn modern machine learning practice, providing this knowledge is rarely done explicitly;\ninstead, it is often learned as part of an auxiliary task. For example, a historically common\napproach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map\nword identities to a continuous representation where, ideally, similar words map to similar\nvectors. These vectors are often learned through an objective that, for example, encourages\nco-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b).\nRecently, it has become increasingly common to pre-train the entire model on a data-rich\ntask. Ideally, this pre-training causes the model to develop general-purpose abilities and\nknowledge that can then be transferred to downstream tasks. In applications of transfer\nlearning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski\net al., 2014), pre-training is typically done via supervised learning on a large labeled data set\nlike ImageNet (Russakovsky et al., 2015; Deng et al., 2009). In contrast, modern techniques\nfor transfer learning in NLP often pre-train using unsupervised learning on unlabeled data.\nThis approach has recently been used to obtain state-of-the-art results in many of the most\ncommon NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu\net al., 2019c; Lan et al., 2019). Beyond its empirical strength, unsupervised pre-training\nfor NLP is particularly attractive because unlabeled text data is available en masse thanks\nto the Internet—for example, the Common Crawl project2 produces about 20TB of text\ndata extracted from web pages each month. This is a natural fit for neural networks, which\nhave been shown to exhibit remarkable scalability, i.e. it is often possible to achieve better\nperformance simply by training a larger model on a larger data set (Hestness et al., 2017;\nShazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019;\nShazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a).\nThis synergy has resulted in a great deal of recent work developing transfer learning\nmethodology for NLP, which has produced a wide landscape of pre-training objectives\n(Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlabeled\ndata sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), benchmarks (Wang et al.,\n2019b, 2018; Conneau and Kiela, 2018), fine-tuning methods (Howard and Ruder, 2018;\nHoulsby et al., 2019; Peters et al., 2019), and more. The rapid rate of progress and diversity\nof techniques in this burgeoning field can make it difficult to compare different algorithms,\ntease apart the effects of new contributions, and understand the space of existing methods for\ntransfer learning. Motivated by a need for more rigorous understanding, we leverage a unified\napproach to transfer learning that allows us to systematically study different approaches\nand push the current limits of the field.\nThe basic idea underlying our work is to treat every text processing problem as a\n“text-to-text” problem, i.e. taking text as input and producing new text as output. This\napproach is inspired by previous unifying frameworks for NLP tasks, including casting all text\nproblems as question answering (McCann et al., 2018), language modeling (Radford et al.,\n2019), or span extraction Keskar et al. (2019b) tasks. Crucially, the text-to-text framework\nallows us to directly apply the same model, objective, training procedure, and decoding\nprocess to every task we consider. We leverage this flexibility by evaluating performance\non a wide variety of English-based NLP problems, including question answering, document\n2. http://commoncrawl.org\n2\nExploring the Limits of Transfer Learning\n\"translate English to German: That is good.\"\n\"cola sentence: The \ncourse is jumping well.\"\n\"summarize: state authorities \ndispatched emergency crews tuesday to \nsurvey the damage after an onslaught \nof severe weather in mississippi…\"\n\"stsb sentence1: The rhino grazed \non the grass. sentence2: A rhino \nis grazing in a field.\"\nT5\n\"Das ist gut.\"\n\"not acceptable\"\n\"six people hospitalized after \na storm in attala county.\"\n\"3.8\"\nFigure 1: A diagram of our text-to-text framework. Every task we consider—including\ntranslation, question answering, and classification—is cast as feeding our model\ntext as input and training it to generate some target text. This allows us to use the\nsame model, loss function, hyperparameters, etc. across our diverse set of tasks. It\nalso provides a standard testbed for the methods included in our empirical survey.\n“T5” refers to our model, which we dub the “Text-to-Text Transfer Transformer”.\nsummarization, and sentiment classification, to name a few. With this unified approach,\nwe can compare the effectiveness of different transfer learning objectives, unlabeled data\nsets, and other factors, while exploring the limits of transfer learning for NLP by scaling up\nmodels and data sets beyond what has previously been considered.\nWe emphasize that our goal is not to propose new methods but instead to provide a\ncomprehensive perspective on where the field stands. As such, our work primarily comprises\na survey, exploration, and empirical comparison of existing techniques. We also explore the\nlimits of current approaches by scaling up the insights from our systematic study (training\nmodels up to11 billion parameters) to obtain state-of-the-art results in many of the tasks\nwe consider. In order to perform experiments at this scale, we introduce the “Colossal Clean\nCrawled Corpus” (C4), a data set consisting of hundreds of gigabytes of clean English text\nscraped from the web. Recognizing that the main utility of transfer learning is the possibility\nof leveraging pre-trained models in data-scarce settings, we release our code, data sets, and\npre-trained models.1\nThe remainder of the paper is structured as follows: In the following section, we discuss\nour base model and its implementation, our procedure for formulating every text processing\nproblem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a\nlarge set of experiments that explore the field of transfer learning for NLP. At the end of the\nsection (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art\nresults on a wide variety of benchmarks. Finally, we provide a summary of our results and\nwrap up with a look towards the future in Section 4.\n3\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n2. Setup\nBefore presenting the results from our large-scale empirical study, we review the necessary\nbackground topics required to understand our results, including the Transformer model\narchitecture and the downstream tasks we evaluate on. We also introduce our approach\nfor treating every problem as a text-to-text task and describe our “Colossal Clean Crawled\nCorpus” (C4), the Common Crawl-based data set we created as a source of unlabeled text\ndata. We refer to our model and framework as the “Text-to-Text Transfer Transformer”\n(T5).\n2.1 Model\nEarly results on transfer learning for NLP leveraged recurrent neural networks (Peters\net al., 2018; Howard and Ruder, 2018), but it has recently become more common to use\nmodels based on the “Transformer” architecture (Vaswani et al., 2017). The Transformer\nwas initially shown to be effective for machine translation, but it has subsequently been\nused in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann\net al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are\nbased on the Transformer architecture. Apart from the details mentioned below and the\nvariants we explore in Section 3.2, we do not deviate significantly from this architecture as\noriginally proposed. Instead of providing a comprehensive definition of this model, we refer\nthe interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for\na more detailed introduction.\nThe primary building block of the Transformer is self-attention (Cheng et al., 2016).\nSelf-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes\na sequence by replacing each element by a weighted average of the rest of the sequence.\nThe original Transformer consisted of an encoder-decoder architecture and was intended\nfor sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has\nrecently also become common to use models consisting of a single Transformer layer stack,\nwith varying forms of self-attention used to produce architectures appropriate for language\nmodeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction\ntasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural\nvariants in Section 3.2.\nOverall, our encoder-decoder Transformer implementation closely follows its originally-\nproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to\na sequence of embeddings, which is then passed into the encoder. The encoder consists\nof a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer\nfollowed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to\nthe input of each subcomponent. We use a simplified version of layer normalization where\nthe activations are only rescaled and no additive bias is applied. After layer normalization,\na residual skip connection (He et al., 2016) adds each subcomponent’s input to its output.\nDropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip\nconnection, on the attention weights, and at the input and output of the entire stack. The\ndecoder is similar in structure to the encoder except that it includes a standard attention\n3. http://nlp.seas.harvard.edu/2018/04/03/attention.html\n4. http://jalammar.github.io/illustrated-transformer/\n4\nExploring the Limits of Transfer Learning\nmechanism after each self-attention layer that attends to the output of the encoder. The\nself-attention mechanism in the decoder also uses a form of autoregressive or causal self-\nattention, which only allows the model to attend to past outputs. The output of the final\ndecoder block is fed into a dense layer with a softmax output, whose weights are shared with\nthe input embedding matrix. All attention mechanisms in the Transformer are split up into\nindependent “heads” whose outputs are concatenated before being further processed.\nSince self-attention is order-independent (i.e. it is an operation on sets), it is common\nto provide an explicit position signal to the Transformer. While the original Transformer\nused a sinusoidal position signal or learned position embeddings, it has recently become\nmore common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a).\nInstead of using a fixed embedding for each position, relative position embeddings produce\na different learned embedding according to the offset between the “key” and “query” being\ncompared in the self-attention mechanism. We use a simplified form of position embeddings\nwhere each “embedding” is simply a scalar that is added to the corresponding logit used\nfor computing the attention weights. For efficiency, we also share the position embedding\nparameters across all layers in our model, though within a given layer each attention head\nuses a different learned position embedding. Typically, a fixed number of embeddings are\nlearned, each corresponding to a range of possible key-query offsets. In this work, we use32\nembeddings for all of our models with ranges that increase in size logarithmically up to an\noffset of128 beyond which we assign all relative positions to the same embedding. Note\nthat a given layer is insensitive to relative position beyond128 tokens, but subsequent layers\ncan build a sensitivity to larger offsets by combining local information from previous layers.\nTo summarize, our model is roughly equivalent to the original Transformer proposed by\nVaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer\nnormalization outside the residual path, and using a different position embedding scheme.\nSince these architectural changes are orthogonal to the experimental factors we consider in\nour empirical survey of transfer learning, we leave the ablation of their impact for future\nwork.\nAs part of our study, we experiment with the scalability of these models, i.e. how their\nperformance changes as they are made to have more parameters or layers. Training large\nmodels can be non-trivial since they might not fit on a single machine and require a great deal\nof computation. As a result, we use a combination of model and data parallelism and train\nmodels on “slices” of Cloud TPU Pods.5 TPU pods are are multi-rack ML supercomputers\nthat contain1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with\nsupporting CPU host machines. We leverage the Mesh TensorFlow library (Shazeer et al.,\n2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky,\n2014).\n2.2 The Colossal Clean Crawled Corpus\nMuch of the previous work on transfer learning for NLP makes use of large unlabeled data\nsets for unsupervised learning. In this paper, we are interested in measuring the effect of the\nquality, characteristics, and size of this unlabeled data. To generate data sets that satisfy\nour needs, we leverage Common Crawl as a source of text scraped from the web. Common\n5. https://cloud.google.com/tpu/\n5\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nCrawl has previously been used as a source of text data for NLP, for example to train an\nn-gram language model (Buck et al., 2014), as training data for commonsense reasoning\n(Trinh and Le, 2018), for mining parallel texts for machine translation (Smith et al., 2013),\nas a pre-training data set (Grave et al., 2018; Zellers et al., 2019; Liu et al., 2019c), and\neven simply as a giant text corpus for testing optimizers (Anil et al., 2019).\nCommon Crawl is a publicly-available web archive that provides “web extracted text”\nby removing markup and other non-text content from the scraped HTML files. This process\nproduces around 20TB of scraped text data each month. Unfortunately, the majority of the\nresulting text is not natural language. Instead, it largely comprises gibberish or boiler-plate\ntext like menus, error messages, or duplicate text. Furthermore, a good deal of the scraped\ntext contains content that is unlikely to be helpful for any of the tasks we consider (offensive\nlanguage, placeholder text, source code, etc.). To address these issues, we used the following\nheuristics for cleaning up Common Crawl’s web extracted text:\n• We only retained lines that ended in a terminal punctuation mark (i.e. a period,\nexclamation mark, question mark, or end quotation mark).\n• We discarded any page with fewer than 3 sentences and only retained lines that\ncontained at least 5 words.\n• We removed any page that contained any word on the “List of Dirty, Naughty, Obscene\nor Otherwise Bad Words”.6\n• Many of the scraped pages contained warnings stating that Javascript should be\nenabled so we removed any line with the word Javascript.\n• Some pages had placeholder “lorem ipsum” text; we removed any page where the\nphrase “lorem ipsum” appeared.\n• Some pages inadvertently contained code. Since the curly bracket “{” appears in\nmany programming languages (such as Javascript, widely used on the web) but not in\nnatural text, we removed any pages that contained a curly bracket.\n• Since some of the scraped pages were sourced from Wikipedia and had citation markers\n(e.g. [1], [citation needed], etc.), we removed any such markers.\n• Many pages had boilerplate policy notices, so we removed any lines containing the\nstrings “terms of use”, “privacy policy”, “cookie policy”, “uses cookies”, “use of\ncookies”, or “use cookies”.\n• To deduplicate the data set, we discarded all but one of any three-sentence span\noccurring more than once in the data set.\nAdditionally, since most of our downstream tasks are focused on English-language text,\nwe used langdetect7 to filter out any pages that were not classified as English with a\nprobability of at least 0.99. Our heuristics are inspired by past work on using Common\n6. https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\n7. https://pypi.org/project/langdetect/\n6\nExploring the Limits of Transfer Learning\nCrawl as a source of data for NLP: For example, Grave et al. (2018) also filter text using an\nautomatic language detector and discard short lines and Smith et al. (2013); Grave et al.\n(2018) both perform line-level deduplication. However, we opted to create a new data set\nbecause prior data sets use a more limited set of filtering heuristics, are not publicly available,\nand/or are different in scope (e.g. are limited to News data (Zellers et al., 2019; Liu et al.,\n2019c), comprise only Creative Commons content (Habernal et al., 2016), or are focused on\nparallel training data for machine translation (Smith et al., 2013)).\nTo assemble our base data set, we downloaded the web extracted text from April 2019\nand applied the aforementioned filtering. This produces a collection of text that is not only\norders of magnitude larger than most data sets used for pre-training (about 750 GB) but also\ncomprises reasonably clean and natural English text. We dub this data set the “Colossal\nClean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets.8\nWe consider the impact of using various alternative versions of this data set in Section 3.4.\n2.3 Downstream Tasks\nOur goal in this paper is to measure general language learning abilities. As such, we study\ndownstream performance on a diverse set of benchmarks, including machine translation,\nquestion answering, abstractive summarization, and text classification. Specifically, we\nmeasure performance on the GLUE and SuperGLUE text classification meta-benchmarks;\nCNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English\nto German, French, and Romanian translation. All data was sourced from TensorFlow\nDatasets.9\nGLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019b) each comprise a\ncollection of text classification tasks meant to test general language understanding abilities:\n• Sentence acceptability judgment (CoLA (Warstadt et al., 2018))\n• Sentiment analysis (SST-2 (Socher et al., 2013))\n• Paraphrasing/sentence similarity (MRPC (Dolan and Brockett, 2005), STS-B (Cer\net al., 2017), QQP (Iyer et al., 2017))\n• Natural language inference (MNLI (Williams et al., 2017), QNLI (Rajpurkar et al.,\n2016), RTE (Dagan et al., 2005), CB (De Marneff et al., 2019))\n• Coreference resolution (WNLI and WSC (Levesque et al., 2012))\n• Sentence completion (COPA (Roemmele et al., 2011))\n• Word sense disambiguation (WIC (Pilehvar and Camacho-Collados, 2018))\n• Question answering (MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018),\nBoolQ (Clark et al., 2019))\n8. https://www.tensorflow.org/datasets/catalog/c4\n9. https://www.tensorflow.org/datasets\n7\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nWe use the data sets as distributed by the GLUE and SuperGLUE benchmarks. For\nsimplicity, when fine-tuning we treat all of the tasks in the GLUE benchmark (and similarly\nfor SuperGLUE) as a single task by concatenating all of the constituent data sets. As\nsuggested by Kocijan et al. (2019) we also include the Definite Pronoun Resolution (DPR)\ndata set (Rahman and Ng, 2012) in the combined SuperGLUE task.\nThe CNN/Daily Mail (Hermann et al., 2015) data set was introduced as a question-\nanswering task but was adapted for text summarization by Nallapati et al. (2016); we\nuse the non-anonymized version from See et al. (2017) as an abstractive summarization\ntask. SQuAD (Rajpurkar et al., 2016) is a common question-answering benchmark. In our\nexperiments, the model is fed the question and its context and asked to generate the answer\ntoken-by-token. For WMT English to German, we use the same training data as (Vaswani\net al., 2017) (i.e. News Commentary v13, Common Crawl, Europarl v7) andnewstest2013\nas a validation set (Bojar et al., 2014). For English to French, we use the standard training\ndata from 2015 andnewstest2014 as a validation set (Bojar et al., 2015). For English to\nRomanian, which is a standard lower-resource machine translation benchmark, we use the\ntrain and validation sets from WMT 2016 (Bojar et al., 2016). Note that we only pre-train\non English data, so in order to learn to translate a given model will need to learn to generate\ntext in a new language.\n2.4 Input and Output Format\nIn order to train a single model on the diverse set of tasks described above, we cast all of\nthe tasks we consider into a “text-to-text” format—that is, a task where the model is fed\nsome text for context or conditioning and is then asked to produce some output text. This\nframework provides a consistent training objective both for pre-training and fine-tuning.\nSpecifically, the model is trained with a maximum likelihood objective (using “teacher forcing”\n(Williams and Zipser, 1989)) regardless of the task. To specify which task the model should\nperform, we add a task-specific (text) prefix to the original input sequence before feeding it\nto the model.\nAs an example, to ask the model to translate the sentence “That is good.” from English\nto German, the model would be fed the sequence “translate English to German: That is\ngood.” and would be trained to output “Das ist gut.” For text classification tasks, the\nmodel simply predicts a single word corresponding to the target label. For example, on the\nMNLI benchmark (Williams et al., 2017) the goal is to predict whether a premise implies\n(“entailment”), contradicts (“contradiction”), or neither (“neutral”) a hypothesis. With\nour preprocessing, the input sequence becomes “mnli premise: I hate pigeons. hypothesis:\nMy feelings towards pigeons are filled with animosity.” with the corresponding target word\n“entailment”. Note that an issue arises if our model outputs text on a text classification\ntask that does not correspond to any of the possible labels (for example if the model\noutputs “hamburger” when the only possible labels for a task were “entailment”, “neutral”,\nor “contradiction”). In this case, we always count the model’s output as wrong, though we\nnever observed this behavior in any of our trained models. Note that the choice of text prefix\nused for a given task is essentially a hyperparameter; we found that changing the exact\nwording of the prefix had limited impact and so did not perform extensive experiments into\ndifferent prefix choices. A diagram of our text-to-text framework with a few input/output\n8\nExploring the Limits of Transfer Learning\nexamples is shown in Figure 1. We provide full examples of preprocessed inputs for every\ntask we studied in Appendix D.\nOur text-to-text framework follows previous work that casts multiple NLP tasks into\na common format: McCann et al. (2018) propose the “Natural Language Decathlon”, a\nbenchmark that uses a consistent question-answering format for a suite of ten NLP tasks.\nThe Natural Language Decathlon also stipulates that all models must be multi-task, i.e.\nare able to simultaneously tackle all of the tasks at once. We instead allow for separately\nfine-tuning the model on each individual task and use short task prefixes instead of an explicit\nquestion-answer format. Radford et al. (2019) evaluate the zero-shot learning capabilities of\nlanguage models by feeding some input to the model as a prefix and then autoregressively\nsampling an output. For example, automatic summarization is done by feeding in a document\nfollowed by the text “TL;DR:” (short for “too long, didn’t read”, a common abbreviation)\nand then the summary is predicted via autoregressive decoding. We mainly consider models\nthat explicitly process an input with an encoder before generating an output with a separate\ndecoder and we focus on transfer learning rather than zero-shot learning. Finally, Keskar\net al. (2019b) unify many NLP tasks as “span extraction”, where text corresponding to\npossible output choices are appended to the input and the model is trained to extract the\ninput span corresponding to the correct choice. In contrast, our framework also allows for\ngenerative tasks like machine translation and abstractive summarization where it is not\npossible to enumerate all possible output choices.\nWe were able to straightforwardly cast all of the tasks we considered into a text-to-text\nformat with the exception of STS-B, which is a regression task where the goal is to predict\na similarity score between1 and 5. We found thatmost of these scores were annotated\nin increments of0.2, so we simply rounded any score to the nearest increment of0.2 and\nconverted the result to a literal string representation of the number (e.g. the floating-point\nvalue 2.57 would be mapped to the string “2.6”). At test time, if the model outputs a\nstring corresponding to a number between1 and 5, we convert it to a floating-point value;\notherwise, we treat the model’s prediction as incorrect. This effectively recasts the STS-B\nregression problem as a 21-class classification problem.\nSeparately, we also convert the Winograd tasks (WNLI from GLUE, WSC from Super-\nGLUE, and the DPR data set we add to SuperGLUE) into a simpler format that is more\namenable to the text-to-text framework. Examples from the Winograd tasks consist of a\ntext passage containing an ambiguous pronoun that could refer to more than one of the noun\nphrases in the passage. For example, the passage might be “The city councilmen refused\nthe demonstrators a permit because they feared violence.”, which contains the ambiguous\npronoun “they” that could refer to “city councilmen” or “demonstrators”. We cast the WNLI,\nWSC, and DPR tasks as text-to-text problems by highlighting the ambiguous pronoun in\nthe text passage and asking the model to predict the noun that it refers to. The example\nmentioned above would be transformed to the input “The city councilmen refused the\ndemonstrators a permit because *they* feared violence.” and the model would be trained to\npredict the target text “The city councilmen”.\nFor WSC, examples contain the passage, the ambiguous pronoun, a candidate noun,\nand a True/False label reflecting whether the candidate matches the pronoun (ignoring any\narticles). We only train on examples with a “True” label since we do not know the correct\nnoun targets for examples with a “False” label. For evaluation, we assign a “True” label if\n9\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nthe words in the model’s output are a subset of the words in the candidate noun phrase\n(or vice versa) and assign a “False” label otherwise. This removes roughly half of the WSC\ntraining set, but the DPR data set adds about1,000 pronoun resolution examples. Examples\nfrom DPR are annotated with the correct referent noun, making it easy to use this data set\nin the format listed above.\nThe WNLI training and validation sets have a significant overlap with the WSC training\nset. To avoid leaking validation examples into our training data (a particular issue in the\nmulti-task experiments of Section 3.5.2), we therefore never train on WNLI and never report\nresults on the WNLI validation set. Omitting results on the WNLI validation set is standard\npractice (Devlin et al., 2018) due to the fact that it is “adversarial” with respect to the\ntraining set, i.e. validation examples are all slightly-perturbed versions of training examples\nwith the opposite label. As such, we do not include WNLI in the average GLUE score\nwhenever we report on the validation set (all sections except Section 3.7 where results\nare presented on the test sets). Converting examples from WNLI to the “referent noun\nprediction” variant described above is a little more involved; we describe this process in\nAppendix B.\n3. Experiments\nRecent advances in transfer learning for NLP have come from a wide variety of developments,\nsuch as new pre-training objectives, model architectures, unlabeled data sets, and more.\nIn this section, we carry out an empirical survey of these techniques in hopes of teasing\napart their contribution and significance. We then combine the insights gained to attain\nstate-of-the-art in many of the tasks we consider. Since transfer learning for NLP is a rapidly\ngrowing area of research, it is not feasible for us to cover every possible technique or idea\nin our empirical study. For a broader literature review, we recommend a recent survey by\nRuder et al. (2019).\nWe systematically study these contributions by taking a reasonable baseline (described\nin Section 3.1) and altering one aspect of the setup at a time. For example, in Section 3.3\nwe measure the performance of different unsupervised objectives while keeping the rest of\nour experimental pipeline fixed. This “coordinate ascent” approach might miss second-order\neffects (for example, some particular unsupervised objective may work best on a model\nlarger than our baseline setting), but performing a combinatorial exploration of all of the\nfactors in our study would be prohibitively expensive. In future work, we expect it could be\nfruitful to more thoroughly consider combinations of the approaches we study.\nOur goal is to compare a variety of different approaches on a diverse set of tasks while\nkeeping as many factors fixed as possible. In order to satisfy this aim, in some cases we do\nnot exactly replicate existing approaches. For example, “encoder-only” models like BERT\n(Devlin et al., 2018) are designed to produce a single prediction per input token or a single\nprediction for an entire input sequence. This makes them applicable for classification or span\nprediction tasks but not for generative tasks like translation or abstractive summarization.\nAs such, none of the model architectures we consider are identical to BERT or consist of an\nencoder-only structure. Instead, we test approaches that are similar in spirit—for example,\nwe consider an analogous objective to BERT’s “masked language modeling” objective in\n10\nExploring the Limits of Transfer Learning\nSection 3.3 and we consider a model architecture that behaves similarly to BERT on text\nclassification tasks in Section 3.2.\nAfter outlining our baseline experimental setup in the following subsection, we undertake\nan empirical comparison of model architectures (Section 3.2), unsupervised objectives\n(Section 3.3), pre-training data sets (Section 3.4), transfer approaches (Section 3.5), and\nscaling (Section 3.6). At the culmination of this section, we combine insights from our study\nwith scale to obtain state-of-the-art results in many tasks we consider (Section 3.7).\n3.1 Baseline\nOur goal for our baseline is to reflect typical, modern practice. We pre-train a standard\nTransformer (described in Section 2.1) using a simple denoising objective and then separately\nfine-tune on each of our downstream tasks. We describe the details of this experimental\nsetup in the following subsections.\n3.1.1 Model\nFor our model, we use a standard encoder-decoder Transformer as proposed by Vaswani et al.\n(2017). While many modern approaches to transfer learning for NLP use a Transformer\narchitecture consisting of only a single “stack” (e.g. for language modeling (Radford et al.,\n2018; Dong et al., 2019) or classification and span prediction (Devlin et al., 2018; Yang et al.,\n2019)), we found that using a standard encoder-decoder structure achieved good results\non both generative and classification tasks. We explore the performance of different model\narchitectures in Section 3.2.\nOur baseline model is designed so that the encoder and decoder are each similar in\nsize and configuration to a “BERTBASE” (Devlin et al., 2018) stack. Specifically, both the\nencoder and decoder consist of12 blocks (each block comprising self-attention, optional\nencoder-decoder attention, and a feed-forward network). The feed-forward networks in each\nblock consist of a dense layer with an output dimensionality ofdff = 3072 followed by a\nReLU nonlinearity and another dense layer. The “key” and “value” matrices of all attention\nmechanisms have an inner dimensionality ofdkv = 64and all attention mechanisms have12\nheads. All other sub-layers and embeddings have a dimensionality ofdmodel = 768. In total,\nthis results in a model with about220 million parameters. This is roughly twice the number\nof parameters of BERTBASE since our baseline model contains two layer stacks instead of\none. For regularization, we use a dropout probability of0.1 everywhere dropout is applied\nin the model.\n3.1.2 Training\nAs described in Section 2.4, all tasks are formulated as text-to-text tasks. This allows us to\nalways train using standard maximum likelihood, i.e. using teacher forcing (Williams and\nZipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (Shazeer and\nStern, 2018). At test time, we use greedy decoding (i.e. choosing the highest-probability\nlogit at every timestep).\nWe pre-train each model for219 = 524,288 steps on C4 before fine-tuning. We use a\nmaximum sequence length of512 and a batch size of128 sequences. Whenever possible,\n11\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nwe “pack” multiple sequences into each entry of the batch10 so that our batches contain\nroughly 216 = 65,536 tokens. In total, this batch size and number of steps corresponds\nto pre-training on235 ≈34B tokens. This is considerably less than BERT (Devlin et al.,\n2018), which used roughly137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly\n2.2T tokens. Using only235 tokens results in a reasonable computational budget while still\nproviding a sufficient amount of pre-training for acceptable performance. We consider the\neffect of pre-training for more steps in Sections 3.6 and 3.7. Note that235 tokens only covers\na fraction of the entire C4 data set, so we never repeat any data during pre-training.\nDuringpre-training, weusean“inversesquareroot”learningrateschedule: 1\n/√\nmax(n,k)\nwhere n is the current training iteration andk is the number of warm-up steps (set to104\nin all of our experiments). This sets a constant learning rate of0.01 for the first104 steps,\nthen exponentially decays the learning rate until pre-training is over. We also experimented\nwith using a triangular learning rate (Howard and Ruder, 2018), which produced slightly\nbetter results but requires knowing the total number of training steps ahead of time. Since\nwe will be varying the number of training steps in some of our experiments, we opt for the\nmore generic inverse square root schedule.\nOur models are fine-tuned for218 = 262,144 steps on all tasks. This value was chosen\nas a trade-off between the high-resource tasks (i.e. those with large data sets), which\nbenefit from additional fine-tuning, and low-resource tasks (smaller data sets), which overfit\nquickly. During fine-tuning, we continue using batches with128 length-512 sequences (i.e.\n216 tokens per batch). We use a constant learning rate of0.001 when fine-tuning. We save\na checkpoint every5,000 steps and report results on the model checkpoint corresponding\nto the highest validation performance. For models fine-tuned on multiple tasks, we choose\nthe best checkpoint for each task independently. For all of the experiments except those in\nSection 3.7, we report results in the validation set to avoid performing model selection on\nthe test set.\n3.1.3 Vocabulary\nWe use SentencePiece (Kudo and Richardson, 2018) to encode text as WordPiece tokens\n(Sennrich et al., 2015; Kudo, 2018). For all experiments, we use a vocabulary of32,000\nwordpieces. Since we ultimately fine-tune our model on English to German, French, and\nRomanian translation, we also require that our vocabulary covers these non-English languages.\nTo address this, we classified pages from the Common Crawl scrape used in C4 as German,\nFrench, and Romanian. Then, we trained our SentencePiece model on a mixture of10 parts\nof English C4 data with1 part each of data classified as German, French or Romanian.\nThis vocabulary was shared across both the input and output of our model. Note that\nour vocabulary makes it so that our model can only process a predetermined, fixed set of\nlanguages.\n3.1.4 Unsupervised Objective\nLeveraging unlabeled data to pre-train our model necessitates an objective that does not\nrequire labels but (loosely speaking) teaches the model generalizable knowledge that will be\n10. https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/\nindex.html#data_generators.generator_utils.pack_examples\n12\nExploring the Limits of Transfer Learning\n<X> <Y>\n<X> <Y> <Z>\nFigure 2: Schematic of the objective we use in our baseline model. In this example, we\nprocess the sentence “Thank you for inviting me to your party last week.” The\nwords “for”, “inviting” and “last” (marked with an×) are randomly chosen for\ncorruption. Each consecutive span of corrupted tokens is replaced by a sentinel\ntoken (shown as<X> and <Y>) that is unique over the example. Since “for” and\n“inviting” occur consecutively, they are replaced by a single sentinel<X>. The\noutput sequence then consists of the dropped-out spans, delimited by the sentinel\ntokens used to replace them in the input plus a final sentinel token<Z>.\nuseful in downstream tasks. Preliminary work that applied the transfer learning paradigm\nof pre-training and fine-tuning all of the model’s parameters to NLP problems used a\ncausal language modeling objective for pre-training (Dai and Le, 2015; Peters et al., 2018;\nRadford et al., 2018; Howard and Ruder, 2018). However, it has recently been shown that\n“denoising” objectives (Devlin et al., 2018; Taylor, 1953) (also called “masked language\nmodeling”) produce better performance and as a result they have quickly become standard.\nIn a denoising objective, the model is trained to predict missing or otherwise corrupted\ntokens in the input. Inspired by BERT’s “masked language modeling” objective and the\n“word dropout” regularization technique (Bowman et al., 2015), we design an objective that\nrandomly samples and then drops out15% of tokens in the input sequence. All consecutive\nspans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token\nis assigned a token ID that is unique to the sequence. The sentinel IDs are special tokens\nwhich are added to our vocabulary and do not correspond to any wordpiece. The target\nthen corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel\ntokens used in the input sequence plus a final sentinel token to mark the end of the target\nsequence. Our choices to mask consecutive spans of tokens and only predict dropped-out\ntokens were made to reduce the computational cost of pre-training. We perform thorough\ninvestigation into pre-training objectives in Section 3.3. An example of the transformation\nresulting from applying this objective is shown in Figure 2. We empirically compare this\nobjective to many other variants in Section 3.3.\n3.1.5 Baseline Performance\nIn this section, we present results using the baseline experimental procedure described above\nto get a sense of what kind of performance to expect on our suite of downstream tasks.\nIdeally, we would repeat every experiment in our study multiple times to get a confidence\ninterval on our results. Unfortunately, this would be prohibitively expensive due to the large\n13\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nGLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n⋆Baseline average 83.28 19 .24 80 .88 71 .36 26 .98 39 .82 27 .65\nBaseline standard deviation 0.235 0 .065 0 .343 0 .416 0 .112 0 .090 0 .108\nNo pre-training 66.22 17 .60 50 .31 53 .04 25 .86 39.77 24.04\nTable 1:Average and standard deviation of scores achieved by our baseline model and\ntraining procedure. For comparison, we also report performance when training on\neach task from scratch (i.e. without any pre-training) for the same number of steps\nused to fine-tune the baseline model. All scores in this table (and every table in\nour paper except Table 14) are reported on the validation sets of each data set.\nnumber of experiments we run. As a cheaper alternative, we train our baseline model10\ntimes from scratch (i.e. with different random initializations and data set shuffling) and\nassume that the variance over these runs of the base model also applies to each experimental\nvariant. We don’t expect most of the changes we make to have a dramatic effect on the\ninter-run variance, so this should provide a reasonable indication of the significance of\ndifferent changes. Separately, we also measure the performance of training our model for218\nsteps (the same number we use for fine-tuning) on all downstream tasks without pre-training.\nThis gives us an idea of how much pre-training benefits our model in the baseline setting.\nWhen reporting results in the main text, we only report a subset of the scores across all\nthe benchmarks to conserve space and ease interpretation. For GLUE and SuperGLUE, we\nreport the average score across all subtasks (as stipulated by the official benchmarks) under\nthe headings “GLUE” and “SGLUE”. For all translation tasks, we report the BLEU score\n(Papineni et al., 2002) as provided by SacreBLEU v1.3.0 (Post, 2018) with “exp” smoothing\nand “intl” tokenization. We refer to scores for WMT English to German, English to French,\nand English to Romanian as EnDe, EnFr, and EnRo, respectively. For CNN/Daily Mail,\nwe find the performance of models on the ROUGE-1-F, ROUGE-2-F, and ROUGE-L-F\nmetrics (Lin, 2004) to be highly correlated so we report the ROUGE-2-F score alone under\nthe heading “CNNDM”. Similarly, for SQuAD we find the performance of the “exact match”\nand “F1” scores to be highly correlated so we report the “exact match” score alone. We\nprovide every score achieved on every task for all experiments in Table 16, Appendix E.\nOur results tables are all formatted so that each row corresponds to a particular experi-\nmental configuration with columns giving the scores for each benchmark. We will include\nthe mean performance of the baseline configuration in most tables. Wherever a baseline\nconfiguration appears, we will mark it with a⋆ (as in the first row of Table 1). We also\nwill boldface any score that is within two standard deviations of the maximum (best) in a\ngiven experiment.\nOur baseline results are shown in Table 1. Overall, our results are comparable to existing\nmodels of similar size. For example, BERTBASE achieved an exact match score of80.8\non SQuAD and an accuracy of84.4 on MNLI-matched, whereas we achieve80.88 and\n84.24, respectively (see Table 16). Note that we cannot directly compare our baseline to\nBERTBASE because ours is an encoder-decoder model and was pre-trained for roughly1⁄4\nas many steps. Unsurprisingly, we find that pre-training provides significant gains across\nalmost all benchmarks. The only exception is WMT English to French, which is a large\n14\nExploring the Limits of Transfer Learning\nenough data set that gains from pre-training tend to be marginal. We include this task in\nour experiments to test the behavior of transfer learning in the high-resource regime. Since\nwe perform early stopping by selecting the best-performing checkpoint, the large disparity\nbetween our baseline and “no pre-training” emphasize how much pre-training improves\nperformance on tasks with limited data. While we do not explicitly measure improvements\nin data efficiency in this paper, we emphasize that this is one of the primary benefits of the\ntransfer learning paradigm.\nAs for inter-run variance, we find that for most tasks the standard deviation across runs\nis smaller than1% of the task’s baseline score. Exceptions to this rule include CoLA, CB,\nand COPA, which are all low-resource tasks from the GLUE and SuperGLUE benchmarks.\nFor example, on CB our baseline model had an average F1 score of91.22 with a standard\ndeviation of3.237 (see Table 16), which may be partly due to the fact that CB’s validation\nset contains only56 examples. Note that the GLUE and SuperGLUE scores are computed\nas the average of scores across the tasks comprising each benchmark. As a result, we caution\nthat the high inter-run variance of CoLA, CB, and COPA can make it harder to compare\nmodels using the GLUE and SuperGLUE scores alone.\n3.2 Architectures\nWhile the Transformer was originally introduced with an encoder-decoder architecture, much\nmodern work on transfer learning for NLP uses alternative architectures. In this section, we\nreview and compare these architectural variants.\n3.2.1 Model Structures\nA major distinguishing factor for different architectures is the “mask” used by different\nattention mechanisms in the model. Recall that the self-attention operation in a Transformer\ntakes a sequence as input and outputs a new sequence of the same length. Each entry of\nthe output sequence is produced by computing a weighted average of entries of the input\nsequence. Specifically, letyi refer to theith element of the output sequence andxj refer to\nthe jth entry of the input sequence.yi is computed as∑\nj wi,jxj, wherewi,j is the scalar\nweight produced by the self-attention mechanism as a function ofxi and xj. The attention\nmask is then used to zero out certain weights in order to constrain which entries of the input\ncan be attended to at a given output timestep. Diagrams of the masks we will consider are\nshown in Figure 3. For example, the causal mask (Figure 3, middle) sets anywi,j to zero if\nj >i.\nThe first model structure we consider is an an encoder-decoder Transformer, which\nconsists of two layer stacks: The encoder, which is fed an input sequence, and the decoder,\nwhich produces a new output sequence. A schematic of this architectural variant is shown\nin the left panel of Figure 4.\nThe encoder uses a “fully-visible” attention mask. Fully-visible masking allows a self-\nattention mechanism to attend to any entry of the input when producing each entry of\nits output. We visualize this masking pattern in Figure 3, left. This form of masking is\nappropriate when attending over a “prefix”, i.e. some context provided to the model that\nis later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible\nmasking pattern and appends a special “classification” token to the input. BERT’s output\n15\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nx1 x2 x3 x4 x5\ny5\ny4\ny3\ny2\ny1\nx1 x2 x3 x4 x5\ny5\ny4\ny3\ny2\ny1\nx1 x2 x3 x4 x5\ny5\ny4\ny3\ny2\ny1\nFigure 3: Matrices representing different attention mask patterns. The input and output\nof the self-attention mechanism are denotedx and y respectively. A dark cell\nat rowi and columnj indicates that the self-attention mechanism is allowed to\nattend to input elementj at output timestepi. A light cell indicates that the\nself-attention mechanism isnot allowed to attend to the correspondingi and j\ncombination. Left: A fully-visible mask allows the self-attention mechanism to\nattend to the full input at every output timestep. Middle: A causal mask prevents\nthe ith output element from depending on any input elements from “the future”.\nRight: Causal masking with a prefix allows the self-attention mechanism to use\nfully-visible masking on a portion of the input sequence.\nat the timestep corresponding to the classification token is then used to make a prediction\nfor classifying the input sequence.\nThe self-attention operations in the Transformer’s decoder use a “causal” masking pattern.\nWhen producing theith entry of the output sequence, causal masking prevents the model\nfrom attending to thejth entry of the input sequence forj >i. This is used during training\nso that the model can’t “see into the future” as it produces its output. An attention matrix\nfor this masking pattern is shown in Figure 3, middle.\nThe decoder in an encoder-decoder Transformer is used to autoregressively produce an\noutput sequence. That is, at each output timestep, a token is sampled from the model’s\npredicted distribution and the sample is fed back into the model to produce a prediction for\nthe next output timestep, and so on. As such, a Transformer decoder (without an encoder)\ncan be used as a language model (LM), i.e. a model trained solely for next-step prediction\n(Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). This constitutes the second\nmodel structure we consider. A schematic of this architecture is shown in Figure 4, middle.\nIn fact, early work on transfer learning for NLP used this architecture with a language\nmodeling objective as a pre-training method (Radford et al., 2018).\nLanguage models are typically used for compression or sequence generation (Graves,\n2013). However, they can also be used in the text-to-text framework simply by concatenating\nthe inputs and targets. As an example, consider the case of English to German translation:\nIf we have a training datapoint with input sentence “That is good.” and target “Das ist\ngut.”, we would simply train the model on next-step prediction over the concatenated input\nsequence “translate English to German: That is good. target: Das ist gut.” If we wanted to\n16\nExploring the Limits of Transfer Learning\nx1 x2 x3 x4\ny1 y2 .\nEncoder Decoder\nx1 x2 x3 y1 y2\nx2 x3 y1 y2 .\nLanguage model\nx1 x2 x3 y1 y2\nx2 x3 y1 y2 .\nPreﬁx LM\nFigure 4: Schematics of the Transformer architecture variants we consider. In this diagram,\nblocks represent elements of a sequence and lines represent attention visibility.\nDifferent colored groups of blocks indicate different Transformer layer stacks. Dark\ngrey lines correspond to fully-visible masking and light grey lines correspond\nto causal masking. We use “.” to denote a special end-of-sequence token that\nrepresents the end of a prediction. The input and output sequences are represented\nas x and y respectively. Left: A standard encoder-decoder architecture uses fully-\nvisible masking in the encoder and the encoder-decoder attention, with causal\nmasking in the decoder. Middle: A language model consists of a single Transformer\nlayer stack and is fed the concatenation of the input and target, using a causal\nmask throughout. Right: Adding a prefix to a language model corresponds to\nallowing fully-visible masking over the input.\nobtain the model’s prediction for this example, the model would be fed the prefix “translate\nEnglish to German: That is good. target:” and would be asked to generate the remainder\nof the sequence autoregressively. In this way, the model can predict an output sequence\ngiven an input, which satisfies the needs of text-to-text tasks. This approach was recently\nused to show that language models can learn to perform some text-to-text tasks without\nsupervision (Radford et al., 2019).\nA fundamental and frequently cited drawback of using a language model in the text-\nto-text setting is that causal masking forces the model’s representation of theith entry of\nthe input sequence to only depend on the entries up untili. To see why this is potentially\ndisadvantageous, consider the text-to-text framework where the model is provided with a\nprefix/context before being asked to make predictions (e.g., the prefix is an English sentence\nand the model is asked to predict the German translation). With fully causal masking, the\nmodel’s representation of a prefix state can only depend on prior entries of the prefix. So,\nwhen predicting an entry of the output, the model will attend to a representation of the\nprefix that is unnecessarily limited. Similar arguments have been made against using a\nunidirectional recurrent neural network encoder in sequence-to-sequence models (Bahdanau\net al., 2015).\n17\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nThis issue can be avoided in a Transformer-based language model simply by changing\nthe masking pattern. Instead of using a causal mask, we use fully-visible masking during\nthe prefix portion of the sequence. This masking pattern and a schematic of the resulting\n“prefix LM” (the third model structure we consider) are illustrated in the rightmost panels of\nFigures 3 and 4, respectively. In the English to German translation example mentioned above,\nfully-visible masking would be applied to the prefix “translate English to German: That is\ngood. target:” and causal masking would be used during training for predicting the target\n“Das ist gut.” Using a prefix LM in the text-to-text framework was originally proposed by\nLiu et al. (2018). More recently, Dong et al. (2019) showed that this architecture is effective\non a wide variety of text-to-text tasks. This architecture is similar to an encoder-decoder\nmodel with parameters shared across the encoder and decoder and with the encoder-decoder\nattention replaced with full attention across the input and target sequence.\nWe note that when following our text-to-text framework, the prefix LM architecture\nclosely resembles BERT (Devlin et al., 2018) for classification tasks. To see why, consider an\nexample from the MNLI benchmark where the premise is “I hate pigeons.”, the hypothesis is\n“My feelings towards pigeons are filled with animosity.” and the correct label is “entailment”.\nTo feed this example into a language model, we would transform it into the sequence “mnli\npremise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.\ntarget: entailment”. In this case, the fully-visible prefix would correspond to the entire input\nsequence up to the word “target:”, which can be seen as being analogous to the “classification”\ntoken used in BERT. So, our model would have full visibility over the entire input, and then\nwould be tasked with making a classification by outputting the word “entailment”. It is easy\nfor the model to learn to output one of the valid class labels given the task prefix (“mnli” in\nthis case). As such, the main difference between a prefix LM and the BERT architecture is\nthat the classifier is simply integrated into the output layer of the Transformer decoder in\nthe prefix LM.\n3.2.2 Comparing Different Model Structures\nIn the interest of experimentally comparing these architectural variants, we would like each\nmodel we consider to be equivalent in some meaningful way. We might say that two models\nare equivalent if they either have the same number of parameters or they require roughly\nthe same amount of computation to process a given (input-sequence, target-sequence) pair.\nUnfortunately, it is not possible to compare an encoder-decoder model to a language model\narchitecture (comprising a single Transformer stack) according to both of these criteria\nat the same time. To see why, first note an encoder-decoder model withL layers in the\nencoder andL layers in the decoder has approximately the same number of parameters as a\nlanguage model with2L layers. However, the sameL+ L encoder-decoder model will have\napproximately the same computational cost as a language model withonly L layers. This\nis a consequence of the fact that theL layers in the language model must be applied to\nboth the input and output sequence, while the encoder is only applied to the input sequence\nand the decoder is only applied to the output sequence. Note that these equivalences are\napproximate—there are some extra parameters in the decoder due to the encoder-decoder\nattention and there are also some computational costs in the attention layers that are\nquadratic in the sequence lengths. In practice, however, we observed nearly identical step\n18\nExploring the Limits of Transfer Learning\ntimes forL-layer language models versusL+ L-layer encoder-decoder models, suggesting a\nroughly equivalent computational cost. Further, for the model sizes we consider, the number\nof parameters in the encoder-decoder attention layers is about 10% of the total parameter\ncount, so we make the simplifying assumption that anL+ L-layer encoder-decoder model\nhas the same number of parameters as an2L-layer language model.\nTo provide a reasonable means of comparison, we consider multiple configurations for\nour encoder-decoder model. We will refer to the number of layers and parameters in a\nBERTBASE-sized layer stack asLand P, respectively. We will useM to refer to the number\nof FLOPs required for anL+ L-layer encoder-decoder model orL-layer decoder-only model\nto process a given input-target pair. In total, we will compare:\n• An encoder-decoder model withL layers in the encoder andL layers in the decoder.\nThis model has2P parameters and a computation cost ofM FLOPs.\n• An equivalent model, but with parameters shared across the encoder and decoder,\nresulting inP parameters and anM-FLOP computational cost.\n• An encoder-decoder model withL/2 layers each in the encoder and decoder, givingP\nparameters and anM/2-FLOP cost.\n• A decoder-only language model with L layers and P parameters and a resulting\ncomputational cost ofM FLOPs.\n• A decoder-only prefix LM with the same architecture (and thus the same number\nof parameters and computational cost), but with fully-visible self-attention over the\ninput.\n3.2.3 Objectives\nAs an unsupervised objective, we will consider both a basic language modeling objective as\nwell as our baseline denoising objective described in Section 3.1.4. We include the language\nmodeling objective due to its historic use as a pre-training objective (Dai and Le, 2015;\nRamachandran et al., 2016; Howard and Ruder, 2018; Radford et al., 2018; Peters et al.,\n2018) as well as its natural fit for the language model architectures we consider. For models\nthat ingest a prefix before making predictions (the encoder-decoder model and prefix LM),\nwe sample a span of text from our unlabeled data set and choose a random point to split\nit into prefix and target portions. For the standard language model, we train the model\nto predict the entire span from beginning to end. Our unsupervised denoising objective is\ndesigned for text-to-text models; to adapt it for use with a language model we concatenate\nthe inputs and targets as described in Section 3.2.1.\n3.2.4 Results\nThe scores achieved by each of the architectures we compare are shown in Table 2. For\nall tasks, the encoder-decoder architecture with the denoising objective performed best.\nThis variant has the highest parameter count (2P) but the same computational cost as the\nP-parameter decoder-only models. Surprisingly, we found that sharing parameters across the\nencoder and decoder performed nearly as well. In contrast, halving the number of layers in\n19\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nArchitecture Objective Params Cost GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n⋆Encoder-decoder Denoising 2P M 83.28 19 .24 80 .88 71 .36 26 .98 39 .82 27 .65\nEnc-dec, shared Denoising P M 82.81 18 .78 80.63 70 .73 26.72 39 .03 27.46\nEnc-dec, 6 layers Denoising P M/ 2 80 .88 18 .97 77 .59 68 .42 26 .38 38 .40 26 .95\nLanguage model Denoising P M 74.70 17 .93 61 .14 55 .02 25 .09 35 .28 25 .86\nPrefix LM Denoising P M 81.82 18 .61 78 .94 68 .11 26 .43 37 .98 27 .39\nEncoder-decoder LM 2P M 79.56 18 .59 76 .02 64 .29 26 .27 39 .17 26 .86\nEnc-dec, shared LM P M 79.60 18 .13 76 .35 63 .50 26 .62 39 .17 27 .05\nEnc-dec, 6 layers LM P M/ 2 78 .67 18 .26 75 .32 64 .06 26 .13 38 .42 26 .89\nLanguage model LM P M 73.78 17 .54 53 .81 56 .51 25 .23 34 .31 25 .38\nPrefix LM LM P M 79.68 17 .84 76 .87 64 .86 26 .28 37 .51 26 .76\nTable 2:Performance of the different architectural variants described in Section 3.2.2. We\nuse P to refer to the number of parameters in a 12-layer base Transformer layer\nstack andM to refer to the FLOPs required to process a sequence using the encoder-\ndecoder model. We evaluate each architectural variant using a denoising objective\n(described in Section 3.1.4) and an autoregressive objective (as is commonly used\nto train language models).\nthe encoder and decoder stacks significantly hurt performance. Concurrent work (Lan et al.,\n2019) also found that sharing parameters across Transformer blocks can be an effective means\nof lowering the total parameter count without sacrificing much performance. XLNet also\nbears some resemblance to the shared encoder-decoder approach with a denoising objective\n(Yang et al., 2019). We also note that the shared parameter encoder-decoder outperforms\nthe decoder-only prefix LM, suggesting that the addition of an explicit encoder-decoder\nattention is beneficial. Finally, we confirm the widely-held conception that using a denoising\nobjective always results in better downstream task performance compared to a language\nmodeling objective. This observation has been previously made by Devlin et al. (2018),\nVoita et al. (2019), and Lample and Conneau (2019) among others. We undertake a more\ndetailed exploration of unsupervised objectives in the following section.\n3.3 Unsupervised Objectives\nThe choice of unsupervised objective is of central importance as it provides the mechanism\nthrough which the model gains general-purpose knowledge to apply to downstream tasks.\nThis has led to the development of a wide variety of pre-training objectives (Dai and Le,\n2015; Ramachandran et al., 2016; Radford et al., 2018; Devlin et al., 2018; Yang et al., 2019;\nLiu et al., 2019b; Wang et al., 2019a; Song et al., 2019; Dong et al., 2019; Joshi et al., 2019).\nIn this section, we perform a procedural exploration of the space of unsupervised objectives.\nIn many cases, we will not replicate an existing objective exactly—some will be modified to\nfit our text-to-text encoder-decoder framework and, in other cases, we will use objectives\nthat combine concepts from multiple common approaches.\nOverall, all of our objectives ingest a sequence of token IDs corresponding to a tokenized\nspan of text from our unlabeled text data set. The token sequence is processed to produce a\n(corrupted) input sequence and a corresponding target. Then, the model is trained as usual\n20\nExploring the Limits of Transfer Learning\nObjective Inputs Targets\nPrefix language modeling Thank you for inviting me to your party last week .\nBERT-style Devlin et al. (2018) Thank you<M> <M> me to your party apple week . (original text)\nDeshuffling party me for your to . last fun you inviting week Thank (original text)\nMASS-style Song et al. (2019) Thank you <M> <M> me to your party<M> week . (original text)\nI.i.d. noise, replace spans Thank you <X> me to your party<Y> week . <X> for inviting<Y> last <Z>\nI.i.d. noise, drop tokens Thank you me to your party week . for inviting last\nRandom spans Thank you <X> to <Y> week . <X> for inviting me<Y> your party last<Z>\nTable 3:Examples of inputs and targets produced by some of the unsupervised objectives\nwe consider applied to the input text “Thank you for inviting me to your party last\nweek .” Note that all of our objectives processtokenized text. For this particular\nsentence, all words were mapped to a single token by our vocabulary. We write\n(original text)as a target to denote that the model is tasked with reconstructing the\nentire input text.<M> denotes a shared mask token and<X>, <Y>, and<Z> denote\nsentinel tokens that are assigned unique token IDs. The BERT-style objective\n(second row) includes a corruption where some tokens are replaced by a random\ntoken ID; we show this via the greyed-out word apple.\nwith maximum likelihood to predict the target sequence. We provide illustrative examples\nof many of the objectives we consider in Table 3.\n3.3.1 Disparate High-Level Approaches\nTo begin with, we compare three techniques that are inspired by commonly-used objectives\nbut differ significantly in their approach. First, we include a basic “prefix language modeling”\nobjective as was used in Section 3.2.3. This technique splits a span of text into two\ncomponents, one to use as inputs to the encoder and the other to use as a target sequence\nto be predicted by the decoder. Second, we consider an objective inspired by the “masked\nlanguage modeling” (MLM) objective used in BERT (Devlin et al., 2018). MLM takes a\nspan of text and corrupts15% of the tokens. 90% of the corrupted tokens are replaced\nwith a special mask token and10% are replaced with a random token. Since BERT is an\nencoder-only model, its goal during pre-training is to reconstruct masked tokens at the\noutput of the encoder. In the encoder-decoder case, we simply use the entire uncorrupted\nsequence as the target. Note that this differs from our baseline objective, which uses only\nthe corrupted tokens as targets; we compare these two approaches in Section 3.3.2. Finally,\nwe also consider a basic deshuffling objective as used e.g. in (Liu et al., 2019a) where it was\napplied to a denoising sequential autoencoder. This approach takes a sequence of tokens,\nshuffles it, and then uses the original deshuffled sequence as a target. We provide examples\nof the inputs and targets for these three methods in the first three rows of Table 3.\nThe performance of these three objectives is shown in Table 4. Overall, we find that the\nBERT-style objective performs best, though the prefix language modeling objective attains\nsimilar performance on the translation tasks. Indeed, the motivation for the BERT objective\nwas to outperform language model-based pre-training. The deshuffling objective performs\nconsiderably worse than both prefix language modeling and the BERT-style objective.\n21\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nObjective GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\nPrefix language modeling 80.69 18 .94 77 .99 65 .27 26.86 39.73 27.49\nBERT-style (Devlin et al., 2018) 82.96 19 .17 80 .65 69 .85 26 .78 40 .03 27 .41\nDeshuffling 73.17 18 .59 67 .61 58 .47 26 .11 39 .30 25 .62\nTable 4:Performance of the three disparate pre-training objectives described in Section 3.3.1.\nObjective GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\nBERT-style (Devlin et al., 2018) 82.96 19 .17 80.65 69.85 26 .78 40.03 27.41\nMASS-style (Song et al., 2019) 82.32 19 .16 80 .10 69 .28 26 .79 39.89 27.55\n⋆Replace corrupted spans 83.28 19.24 80 .88 71 .36 26 .98 39.82 27.65\nDrop corrupted tokens 84.44 19 .31 80 .52 68.67 27.07 39.76 27.82\nTable 5:Comparison of variants of the BERT-style pre-training objective. In the first two\nvariants, the model is trained to reconstruct the original uncorrupted text segment.\nIn the latter two, the model only predicts the sequence of corrupted tokens.\n3.3.2 Simplifying the BERT Objective\nBased on the results in the prior section, we will now focus on exploring modifications to\nthe BERT-style denoising objective. This objective was originally proposed as a pre-training\ntechnique for an encoder-only model trained for classification and span prediction. As\nsuch, it may be possible to modify it so that it performs better or is more efficient in our\nencoder-decoder text-to-text setup.\nFirst, we consider a simple variant of the BERT-style objective where we don’t include the\nrandom token swapping step. The resulting objective simply replaces15% of the tokens in\nthe input with a mask token and the model is trained to reconstruct the original uncorrupted\nsequence. A similar masking objective was used by Song et al. (2019) where it was referred to\nas “MASS”, so we call this variant the “MASS-style” objective. Second, we were interested\nto see if it was possible to avoid predicting the entire uncorrupted text span since this\nrequires self-attention over long sequences in the decoder. We consider two strategies to\nachieve this: First, instead of replacing each corrupted token with a mask token, we replace\nthe entirety of each consecutive span of corrupted tokens with a unique mask token. Then,\nthe target sequence becomes the concatenation of the “corrupted” spans, each prefixed by\nthe mask token used to replace it in the input. This is the pre-training objective we use in\nour baseline, described in Section 3.1.4. Second, we also consider a variant where we simply\ndrop the corrupted tokens from the input sequence completely and task the model with\nreconstructing the dropped tokens in order. Examples of these approaches are shown in the\nfifth and sixth rows of Table 3.\nAn empirical comparison of the original BERT-style objective to these three alternatives\nis shown in Table 5. We find that in our setting, all of these variants perform similarly. The\nonly exception was that dropping corrupted tokens completely produced a small improvement\nin the GLUE score thanks to a significantly higher score on CoLA (60.04, compared to our\n22\nExploring the Limits of Transfer Learning\nCorruption rate GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n10% 82.82 19.00 80.38 69.55 26.87 39.28 27.44\n⋆15% 83.28 19.24 80.88 71 .36 26 .98 39 .82 27 .65\n25% 83.00 19 .54 80 .96 70.48 27.04 39 .83 27 .47\n50% 81 .27 19 .32 79 .80 70 .33 27.01 39 .90 27 .49\nTable 6: Performance of the i.i.d. corruption objective with different corruption rates.\nbaseline average of53.84, see Table 16). This may be due to the fact that CoLA involves\nclassifying whether a given sentence is grammatically and syntactically acceptable, and\nbeing able to determine when tokens are missing is closely related to detecting acceptability.\nHowever, dropping tokens completely performed worse than replacing them with sentinel\ntokens on SuperGLUE. The two variants that do not require predicting the full original\nsequence (“replace corrupted spans” and “drop corrupted spans”) are both potentially\nattractive since they make the target sequences shorter and consequently make training\nfaster. Going forward, we will explore variants where we replace corrupted spans with\nsentinel tokens and only predict the corrupted tokens (as in our baseline objective).\n3.3.3 Varying the Corruption Rate\nSo far, we have been corrupting 15% of the tokens, the value used in BERT (Devlin et al.,\n2018). Again, since our text-to-text framework differs from BERT’s, we are interested to\nsee if a different corruption rate works better for us. We compare corruption rates of10%,\n15%, 25%, and 50% in Table 6. Overall, we find that the corruption rate had a limited\neffect on the model’s performance. The only exception is that the largest corruption rate we\nconsider (50%) results in a significant degradation of performance on GLUE and SQuAD.\nUsing a larger corruption rate also results in longer targets, which can potentially slow down\ntraining. Based on these results and the historical precedent set by BERT, we will use a\ncorruption rate of15% going forward.\n3.3.4 Corrupting Spans\nWe now turn towards the goal of speeding up training by predicting shorter targets. The\napproach we have used so far makes an i.i.d. decision for each input token as to whether\nto corrupt it or not. When multiple consecutive tokens have been corrupted, they are\ntreated as a “span” and a single unique mask token is used to replace the entire span.\nReplacing entire spans with a single token results in unlabeled text data being processed into\nshorter sequences. Since we are using an i.i.d. corruption strategy, it is not always the case\nthat a significant number of corrupted tokens appear consecutively. As a result, we might\nobtain additional speedup by specifically corrupting spans of tokens rather than corrupting\nindividual tokens in an i.i.d. manner. Corrupting spans was also previously considered as a\npre-training objective for BERT, where it was found to improve performance (Joshi et al.,\n2019).\nTo test this idea, we consider an objective that specifically corrupts contiguous, randomly-\nspaced spans of tokens. This objective can be parametrized by the proportion of tokens to\nbe corrupted and the total number of corrupted spans. The span lengths are then chosen\n23\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nSpan length GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n⋆Baseline (i.i.d.) 83.28 19.24 80 .88 71 .36 26.98 39 .82 27 .65\n2 83.54 19.39 82.09 72 .20 26 .76 39 .99 27 .63\n3 83.49 19 .62 81 .84 72 .53 26 .86 39.65 27.62\n5 83.40 19.24 82.05 72 .23 26 .88 39.40 27.53\n10 82 .85 19 .33 81.84 70.44 26.79 39.49 27.69\nTable 7:Performance of the span-corruption objective (inspired by Joshi et al. (2019)) for\ndifferent average span lengths. In all cases, we corrupt 15% of the original text\nsequence.\nrandomly to satisfy these specified parameters. For example, if we are processing a sequence\nof 500 tokens and we have specified that15% of tokens should be corrupted and that there\nshould be25 total spans, then the total number of corrupted tokens would be500×0.15 = 75\nand the average span length would be75/25 = 3. Note that given the original sequence\nlength and corruption rate, we can equivalently parametrize this objective either by the\naverage span length or the total number of spans.\nWe compare the span-corruption objective to the i.i.d-corruption objective in Table 7.\nWe use a corruption rate of15% in all cases and compare using average span lengths of2, 3,\n5 and 10. Again, we find a limited difference between these objectives, though the version\nwith an average span length of10 slightly underperforms the other values in some cases.\nWe also find in particular that using an average span length of3 slightly (but significantly)\noutperforms the i.i.d. objective on most non-translation benchmarks. Fortunately, the\nspan-corruption objective also provides some speedup during training compared to the i.i.d.\nnoise approach because span corruption produces shorter sequences on average.\n3.3.5 Discussion\nFigure 5 shows a flow chart of the choices made during our exploration of unsupervised\nobjectives. Overall, the most significant difference in performance we observed was that\ndenoising objectives outperformed language modeling and deshuffling for pre-training. We\ndid not observe a remarkable difference across the many variants of the denoising objectives\nwe explored. However, different objectives (or parameterizations of objectives) can lead to\ndifferent sequence lengths and thus different training speeds. This implies that choosing\namong the denoising objectives we considered here should mainly be done according to\ntheir computational cost. Our results also suggest that additional exploration of objectives\nsimilar to the ones we consider here may not lead to significant gains for the tasks and model\nwe consider. Instead, it may be fortuitous to explore entirely different ways of leveraging\nunlabeled data.\n3.4 Pre-training Data set\nLike the unsupervised objective, the pre-training data set itself is a crucial component of\nthe transfer learning pipeline. However, unlike objectives and benchmarks, new pre-training\ndata sets are usually not treated as significant contributions on their own and are often not\n24\nExploring the Limits of Transfer Learning\nFigure 5: A flow chart of our exploration of unsupervised objectives. We first consider a\nfew disparate approaches in Section 3.3.1 and find that a BERT-style denoising\nobjective performs best. Then, we consider various methods for simplifying the\nBERT objective so that it produces shorter target sequences in Section 3.3.2.\nGiven that replacing dropped-out spans with sentinel tokens performs well and\nresults in short target sequences, in Section 3.3.3 we experiment with different\ncorruption rates. Finally, we evaluate an objective that intentionally corrupts\ncontiguous spans of tokens in Section 3.3.4.\nreleased alongside pre-trained models and code. Instead, they are typically introduced in\nthe course of presenting a new method or model. As a result, there has been relatively little\ncomparison of different pre-training data sets as well as a lack of a “standard” data set used\nfor pre-training. Some recent notable exceptions (Baevski et al., 2019; Liu et al., 2019c;\nYang et al., 2019) have compared pre-training on a new large (often Common Crawl-sourced)\ndata set to using a smaller preexisting data set (often Wikipedia). To probe more deeply\ninto the impact of the pre-training data set on performance, in this section we compare\nvariants of our C4 data set and other potential sources of pre-training data. We release all\nof the C4 data set variants we consider as part of TensorFlow Datasets.11\n3.4.1 Unlabeled Data Sets\nIn creating C4, we developed various heuristics to filter the web-extracted text from Common\nCrawl (see Section 2.2 for a description). We are interested in measuring whether this\nfiltering results in improved performance on downstream tasks, in addition to comparing\nit to other filtering approaches and common pre-training data sets. Towards this end, we\ncompare the performance of our baseline model after pre-training on the following data sets:\nC4 As a baseline, we first consider pre-training on our proposed unlabeled data set as\ndescribed in Section 2.2.\nUnfiltered C4To measure the effect of the heuristic filtering we used in creating C4\n(deduplication, removing bad words, only retaining sentences, etc.), we also generate\nan alternate version of C4 that forgoes this filtering. Note that we still uselangdetect\n11. https://www.tensorflow.org/datasets/catalog/c4\n25\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nto extract English text. As a result, our “unfiltered” variant still includes some filtering\nbecause langdetect sometimes assigns a low probability to non-natural English text.\nRealNews-like Recent work has used text data extracted from news websites (Zellers\net al., 2019; Baevski et al., 2019). To compare to this approach, we generate another\nunlabeled data set by additionally filtering C4 to only include content from one of the\ndomains used in the “RealNews” data set (Zellers et al., 2019). Note that for ease of\ncomparison, we retain the heuristic filtering methods used in C4; the only difference is\nthat we have ostensibly omitted any non-news content.\nWebText-likeSimilarly, the WebText data set (Radford et al., 2019) only uses content\nfrom webpages that were submitted to the content aggregation website Reddit and\nreceived a “score” of at least 3. The score for a webpage submitted to Reddit is\ncomputed based on the proportion of users who endorse (upvote) or oppose (downvote)\nthe webpage. The idea behind using the Reddit score as a quality signal is that users\nof the site would only upvote high-quality text content. To generate a comparable data\nset, we first tried removing all content from C4 that did not originate from a URL that\nappeared in the list prepared by the OpenWebText effort.12 However, this resulted in\ncomparatively little content—only about 2 GB—because most pages never appear on\nReddit. Recall that C4 was created based on a single month of Common Crawl data.\nTo avoid using a prohibitively small data set, we therefore downloaded 12 months\nof data from Common Crawl from August 2018 to July 2019, applied our heuristic\nfiltering for C4, then applied the Reddit filter. This produced a 17 GB WebText-like\ndata set, which is of comparable size to the original 40GB WebText data set (Radford\net al., 2019).\nWikipedia The website Wikipedia consists of millions of encyclopedia articles written\ncollaboratively. The content on the site is subject to strict quality guidelines and\ntherefore has been used as a reliable source of clean and natural text. We use the\nEnglish Wikipedia text data from TensorFlow Datasets,13 which omits any markup or\nreference sections from the articles.\nWikipedia + Toronto Books CorpusAdrawbackofusingpre-trainingdatafromWikipedia\nis that it represents only one possible domain of natural text (encyclopedia articles).\nTo mitigate this, BERT (Devlin et al., 2018) combined data from Wikipedia with the\nToronto Books Corpus (TBC) (Zhu et al., 2015). TBC contains text extracted from\neBooks, which represents a different domain of natural language. BERT’s popularity\nhas led to the Wikipedia + TBC combination being used in many subsequent works.\nThe results achieved after pre-training on each of these data sets is shown in Table 8. A\nfirst obvious takeaway is that removing the heuristic filtering from C4 uniformly degrades\nperformance and makes the unfiltered variant perform the worst in every task. Beyond\nthis, we found that in some cases a pre-training data set with a more constrained domain\noutperformed the diverse C4 data set. For example, using the Wikipedia + TBC corpus\n12. https://github.com/jcpeterson/openwebtext\n13. https://www.tensorflow.org/datasets/catalog/wikipedia\n26\nExploring the Limits of Transfer Learning\nData set Size GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n⋆C4 745GB 83.28 19.24 80.88 71 .36 26.98 39 .82 27 .65\nC4, unfiltered 6.1TB 81.46 19 .14 78 .78 68 .04 26 .55 39 .34 27 .21\nRealNews-like 35GB 83.83 19 .23 80.39 72 .38 26.75 39 .90 27 .48\nWebText-like 17GB 84.03 19 .31 81 .42 71.40 26.80 39 .74 27 .59\nWikipedia 16GB 81.85 19.31 81.29 68 .01 26.94 39.69 27.67\nWikipedia + TBC 20GB 83.65 19.28 82 .08 73 .24 26 .77 39.63 27.57\nTable 8:Performance resulting from pre-training on different data sets. The first four\nvariants are based on our new C4 data set.\nproduced a SuperGLUE score of73.24, beating our baseline’s score (using C4) of71.36.\nThis is almost entirely attributable to a boost in performance from25.78 (baseline, C4) to\n50.93 (Wikipedia + TBC) on the Exact Match score for MultiRC (see Table 16). MultiRC\nis a reading comprehension data set whose largest source of data comes from fiction books,\nwhich is exactly the domain covered by TBC. Similarly, using the RealNews-like data set\nfor pre-training conferred an increase from68.16 to 73.72 on the Exact Match score for\nReCoRD, a data set that measures reading comprehension on news articles. As a final\nexample, using data from Wikipedia produced significant (but less dramatic) gains on\nSQuAD, which is a question-answering data set with passages sourced from Wikipedia.\nSimilar observations have been made in prior work, e.g. Beltagy et al. (2019) found that\npre-training BERT on text from research papers improved its performance on scientific tasks.\nThe main lesson behind these findings is thatpre-training on in-domain unlabeled data can\nimprove performance on downstream tasks. This is unsurprising but also unsatisfying if\nour goal is to pre-train a model that can rapidly adapt to language tasks from arbitrary\ndomains. Liu et al. (2019c) also observed that pre-training on a more diverse data set yielded\nimprovements on downstream tasks. This observation also motivates the parallel line of\nresearch on domain adaptation for natural language processing; for surveys of this field see\ne.g. Ruder (2019); Li (2012).\nA drawback to only pre-training on a single domain is that the resulting data sets are\noften substantially smaller. Similarly, while the WebText-like variant performed as well or\nbetter than the C4 data set in our baseline setting, the Reddit-based filtering produced a\ndata set that was about40×smaller than C4 despite being based on12×more data from\nCommon Crawl. Note, however, that in our baseline setup we only pre-train on235 ≈34B\ntokens, which is only about8 times larger than the smallest pre-training data set we consider.\nWe investigate at what point using a smaller pre-training data sets poses an issue in the\nfollowing section.\n3.4.2 Pre-training Data set Size\nThe pipeline we use to create C4 was designed to be able to create extremely large pre-\ntraining data sets. The access to so much data allows us to pre-train our models without\nrepeating examples. It is not clear whether repeating examples during pre-training would\nbe helpful or harmful to downstream performance because our pre-training objective is itself\nstochastic and can help prevent the model from seeing the same exact data multiple times.\n27\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nNumber of tokens Repeats GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n⋆Full data set 0 83.28 19 .24 80 .88 71 .36 26 .98 39 .82 27 .65\n229 64 82.87 19 .19 80 .97 72 .03 26 .83 39 .74 27 .63\n227 256 82 .62 19.20 79.78 69 .97 27.02 39 .71 27.33\n225 1,024 79 .55 18 .57 76 .27 64 .76 26 .38 39 .56 26 .80\n223 4,096 76 .34 18 .33 70 .92 59 .29 26 .37 38 .84 25 .81\nTable 9:Measuring the effect of repeating data during pre-training. In these experiments,\nwe only use the firstN tokens from C4 (with varying values ofN shown in the\nfirst column) but still pre-train over235 tokens. This results in the data set being\nrepeated over the course of pre-training (with the number of repeats for each\nexperiment shown in the second column), which may result in memorization (see\nFigure 6).\nTo test the effect of limited unlabeled data set sizes, we pre-trained our baseline model\non artificially truncated versions of C4. Recall that we pre-train our baseline model on\n235 ≈34B tokens (a small fraction of the total size of C4). We consider training on truncated\nvariants of C4 consisting of229, 227, 225 and 223 tokens. These sizes correspond to repeating\nthe data set64, 256, 1,024, and4,096 times respectively over the course of pre-training.\nThe resulting downstream performance is shown in Table 9. As expected, performance\ndegrades as the data set size shrinks. We suspect this may be due to the fact that the model\nbegins to memorize the pre-training data set. To measure if this is true, we plot the training\nloss for each of these data set sizes in Figure 6. Indeed, the model attains significantly\nsmaller training losses as the size of the pre-training data set shrinks, suggesting possible\nmemorization. Baevski et al. (2019) similarly observed that truncating the pre-training data\nset size can degrade downstream task performance.\nWe note that these effects are limited when the pre-training data set is repeated only\n64 times. This suggests that some amount of repetition of pre-training data might not be\nharmful. However, given that additional pre-training can be beneficial (as we will show in\nSection 3.6) and that obtaining additional unlabeled data is cheap and easy, we suggest\nusing large pre-training data sets whenever possible. We also note that this effect may be\nmore pronounced for larger model sizes, i.e. a bigger model may be more prone to overfitting\nto a smaller pre-training data set.\n3.5 Training Strategy\nSo far we have considered the setting where all parameters of a model are pre-trained on\nan unsupervised task before being fine-tuned on individual supervised tasks. While this\napproach is straightforward, various alternative methods for training the model on down-\nstream/supervised tasks have been proposed. In this section, we compare different schemes\nfor fine-tuning the model in addition to the approach of training the model simultaneously\non multiple tasks.\n28\nExploring the Limits of Transfer Learning\n/uni00000014/uni00000015/uni00000014/uni00000014/uni00000016/uni00000014/uni00000014/uni00000017/uni00000014/uni00000014/uni00000018/uni00000014/uni00000014/uni00000019/uni00000014/uni00000014\n/uni00000037/uni00000058/uni00000049/uni00000054/uni00000004/uni00000082/uni00000004/uni00000015/uni00000010/uni00000014/uni00000014/uni00000014\n/uni00000014/uni00000012/uni00000014\n/uni00000014/uni00000012/uni00000016\n/uni00000014/uni00000012/uni00000018\n/uni00000014/uni00000012/uni0000001a\n/uni00000014/uni00000012/uni0000001c\n/uni00000015/uni00000012/uni00000014\n/uni00000038/uni00000056/uni00000045/uni0000004d/uni00000052/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000050/uni00000053/uni00000057/uni00000057\n/uni00000028/uni00000045/uni00000058/uni00000045/uni00000057/uni00000049/uni00000058/uni00000004/uni00000057/uni0000004d/uni0000005e/uni00000049\n/uni0000002a/uni00000059/uni00000050/uni00000050/uni00000004/uni00000048/uni00000045/uni00000058/uni00000045/uni00000057/uni00000049/uni00000058\n/uni00000016/uni00000016/uni0000001d\n/uni00000016/uni00000016/uni0000001b\n/uni00000016/uni00000016/uni00000019\n/uni00000016/uni00000016/uni00000017\nFigure 6: Pre-training loss for our original C4 data set as well as4 artificially truncated\nversions. The sizes listed refer to the number of tokens in each data set. The four\nsizes considered correspond to repeating the data set between64 and 4,096 times\nover the course of pre-training. Using a smaller data set size results in smaller\ntraining loss values, which may suggest some memorization of the unlabeled data\nset.\n3.5.1 Fine-tuning Methods\nIt has been argued that fine-tuning all of the model’s parameters can lead to suboptimal\nresults, particularly on low-resource tasks (Peters et al., 2019). Early results on transfer\nlearning for text classification tasks advocated fine-tuning only the parameters of a small\nclassifier that was fed sentence embeddings produced by a fixed pre-trained model (Subra-\nmanian et al., 2018; Kiros et al., 2015; Logeswaran and Lee, 2018; Hill et al., 2016; Conneau\net al., 2017). This approach is less applicable to our encoder-decoder model because the\nentire decoder must be trained to output the target sequences for a given task. Instead, we\nfocus on two alternative fine-tuning approaches that update only a subset of the parameters\nof our encoder-decoder model.\nThe first, “adapter layers” (Houlsby et al., 2019; Bapna et al., 2019), is motivated by\nthe goal of keeping most of the original model fixed while fine-tuning. Adapter layers are\nadditional dense-ReLU-dense blocks that are added after each of the preexisting feed-forward\nnetworks in each block of the Transformer. These new feed-forward networks are designed\nso that their output dimensionality matches their input. This allows them to be inserted\ninto the network with no additional changes to the structure or parameters. When fine-\ntuning, only the adapter layer and layer normalization parameters are updated. The main\nhyperparameter of this approach is the inner dimensionalityd of the feed-forward network,\nwhich changes the number of new parameters added to the model. We experiment with\nvarious values ford.\nThe second alternative fine-tuning method we consider is “gradual unfreezing” (Howard\nand Ruder, 2018). In gradual unfreezing, more and more of the model’s parameters are fine-\ntuned over time. Gradual unfreezing was originally applied to a language model architecture\nconsisting of a single stack of layers. In this setting, at the start of fine-tuning only the\n29\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nFine-tuning method GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n⋆All parameters 83.28 19 .24 80 .88 71 .36 26 .98 39 .82 27 .65\nAdapter layers,d = 32 80 .52 15 .08 79 .32 60 .40 13 .84 17 .88 15 .54\nAdapter layers,d = 128 81 .51 16 .62 79 .47 63 .03 19 .83 27 .50 22 .63\nAdapter layers,d = 512 81 .54 17 .78 79 .18 64 .30 23 .45 33 .98 25 .81\nAdapter layers,d = 2048 81 .51 16 .62 79 .47 63 .03 19 .83 27 .50 22 .63\nGradual unfreezing 82.50 18 .95 79 .17 70.79 26.71 39 .02 26 .93\nTable 10:Comparison of different alternative fine-tuning methods that only update a subset\nof the model’s parameters. For adapter layers,drefers to the inner dimensionality\nof the adapters.\nparameters of the final layer are updated, then after training for a certain number of updates\nthe parameters of the second-to-last layer are also included, and so on until the entire\nnetwork’s parameters are being fine-tuned. To adapt this approach to our encoder-decoder\nmodel, we gradually unfreeze layers in the encoder and decoder in parallel, starting from\nthe top in both cases. Since the parameters of our input embedding matrix and output\nclassification matrix are shared, we update them throughout fine-tuning. Recall that our\nbaseline model consists of12 layers each in the encoder and decoder and is fine-tuned for\n218 steps. As such, we subdivide the fine-tuning process into12 episodes of218\n/12 steps each\nand train from layers12 −n to 12 in thenth episode. We note that Howard and Ruder\n(2018) suggested fine-tuning an additional layer after each epoch of training. However, since\nour supervised data sets vary so much in size and since some of our downstream tasks are\nactually mixtures of many tasks (GLUE and SuperGLUE), we instead adopt the simpler\nstrategy of fine-tuning an additional layer after every218\n/12 steps.\nA comparison of the performance of these fine-tuning approaches is shown in Table 10.\nFor adapter layers, we report the performance using an inner dimensionalityd of 32, 128,\n512, 2048. Pursuant with past results (Houlsby et al., 2019; Bapna et al., 2019) we find that\nlower-resource tasks like SQuAD work well with a small value ofd whereas higher resource\ntasks require a large dimensionality to achieve reasonable performance. This suggests that\nadapter layers could be a promising technique for fine-tuning on fewer parameters as long as\nthe dimensionality is scaled appropriately to the task size. Note that in our case we treat\nGLUE and SuperGLUE each as a single “task” by concatenating their constituent data\nsets, so although they comprise some low-resource data sets the combined data set is large\nenough that it necessitates a large value ofd. We found that gradual unfreezing caused\na minor degradation in performance across all tasks, though it did provide some speedup\nduring fine-tuning. Better results may be attainable by more carefully tuning the unfreezing\nschedule.\n3.5.2 Multi-task Learning\nSo far, we have been pre-training our model on a single unsupervised learning task before\nfine-tuning it individually on each downstream task. An alternative approach, called “multi-\ntask learning” (Ruder, 2017; Caruana, 1997), is to train the model on multiple tasks at a\ntime. This approach typically has the goal of training a single model that can simultaneously\n30\nExploring the Limits of Transfer Learning\nperform many tasks at once, i.e. the model and most of its parameters are shared across all\ntasks. We relax this goal somewhat and instead investigate methods for training on multiple\ntasks at once in order to eventually produce separate parameter settings that perform well\non each individual task. For example, we might train a single model on many tasks, but\nwhen reporting performance we are allowed to select a different checkpoint for each task.\nThis loosens the multi-task learning framework and puts it on more even footing compared\nto the pre-train-then-fine-tune approach we have considered so far. We also note that in our\nunified text-to-text framework, “multi-task learning” simply corresponds to mixing data sets\ntogether. It follows that we can still train on unlabeled data when using multi-task learning\nby treating the unsupervised task as one of the tasks being mixed together. In contrast,\nmost applications of multi-task learning to NLP add task-specific classification networks or\nuse different loss functions for each task (Liu et al., 2019b).\nAs pointed out by Arivazhagan et al. (2019), an extremely important factor in multi-task\nlearning is how much data from each task the model should be trained on. Our goal is to not\nunder- or over-train the model—that is, we want the model to see enough data from a given\ntask that it can perform the task well, but not to see so much data that it memorizes the\ntraining set. How exactly to set the proportion of data coming from each task can depend on\nvarious factors including data set sizes, the “difficulty” of learning the task (i.e. how much\ndata the model must see before being able to perform the task effectively), regularization,\netc. An additional issue is the potential for “task interference” or “negative transfer”, where\nachieving good performance on one task can hinder performance on another. Given these\nconcerns, we begin by exploring various strategies for setting the proportion of data coming\nfrom each task. A similar exploration was performed by Wang et al. (2019a).\nExamples-proportional mixing A major factor in how quickly a model will overfit to\na given task is the task’s data set size. As such, a natural way to set the mixing\nproportions is to sample in proportion to the size of each task’s data set. This is\nequivalent to concatenating the data sets for all tasks and randomly sampling examples\nfrom the combined data set. Note, however, that we are including our unsupervised\ndenoising task, which uses a data set that is orders of magnitude larger than every\nother task’s. It follows that if we simply sample in proportion to each data set’s size,\nthe vast majority of the data the model sees will be unlabeled, and it will undertrain\non all of the supervised tasks. Even without the unsupervised task, some tasks (e.g.\nWMT English to French) are so large that they would similarly crowd out most of\nthe batches. To get around this issue, we set an artificial “limit” on the data set sizes\nbefore computing the proportions. Specifically, if the number of examples in each of\nour N task’s data sets isen,n ∈{1,...,N }then we set probability of sampling an\nexample from themth task during training torm = min(em,K)/∑ min(en,K) where\nK is the artificial data set size limit.\nTemperature-scaled mixingAn alternative way of mitigating the huge disparity between\ndata set sizes is to adjust the “temperature” of the mixing rates. This approach was\nused by multilingual BERT to ensure that the model was sufficiently trained on low-\nresource languages.14 To implement temperature scaling with temperatureT, we raise\n14. https://github.com/google-research/bert/blob/master/multilingual.md\n31\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\neach task’s mixing raterm to the power of1⁄T and renormalize the rates so that they\nsum to 1. WhenT = 1, this approach is equivalent to examples-proportional mixing\nand asT increases the proportions become closer to equal mixing. We retain the data\nset size limitK (applied to obtainrm before temperature scaling) but set it to a large\nvalue ofK = 221. We use a large value ofK because increasing the temperature will\ndecrease the mixing rate of the largest data sets.\nEqual mixing In this case, we sample examples from each task with equal probability.\nSpecifically, each example in each batch is sampled uniformly at random from one of\nthe data sets we train on. This is most likely a suboptimal strategy, as the model will\noverfit quickly on low-resource tasks and underfit on high-resource tasks. We mainly\ninclude it as a point of reference of what might go wrong when the proportions are set\nsuboptimally.\nTo compare these mixing strategies on equal footing with our baseline pre-train-then-\nfine-tune results, we train multi-task models for the same total number of steps:219 + 218 =\n786,432. The results are shown in Table 11.\nIn general, we find that multi-task training underperforms pre-training followed by\nfine-tuning on most tasks. The “equal” mixing strategy in particular results in dramatically\ndegraded performance, which may be because the low-resource tasks have overfit, the high-\nresource tasks have not seen enough data, or the model has not seen enough unlabeled data to\nlearn general-purpose language capabilities. For examples-proportional mixing, we find that\nfor most tasks there is a “sweet spot” forK where the model obtains the best performance,\nand larger or smaller values ofK tend to result in worse performance. The exception (for the\nrange ofK values we considered) was WMT English to French translation, which is such a\nhigh-resource task that it always benefits from a higher mixing proportion. Finally, we note\nthat temperature-scaled mixing also provides a means of obtaining reasonable performance\nfrom most tasks, withT = 2performing the best in most cases. The finding that a multi-task\nmodel is outperformed by separate models trained on each individual task has previously\nbeen observed e.g. by Arivazhagan et al. (2019) and McCann et al. (2018), though it has\nbeen shown that the multi-task setup can confer benefits across very similar tasks Liu et al.\n(2019b); Ratner et al. (2018). In the following section, we explore ways to close the gap\nbetween multi-task training and the pre-train-then-fine-tune approach.\n3.5.3 Combining Multi-Task Learning with Fine-Tuning\nRecall that we are studying a relaxed version of multi-task learning where we train a single\nmodel on a mixture of tasks but are allowed to evaluate performance using different parameter\nsettings (checkpoints) for the model. We can extend this approach by considering the case\nwhere the model is pre-trained on all tasks at once but is then fine-tuned on the individual\nsupervised tasks. This is the method used by the “MT-DNN” (Liu et al., 2015, 2019b),\nwhich achieved state-of-the-art performance on GLUE and other benchmarks when it was\nintroduced. We consider three variants of this approach: In the first, we simply pre-train the\nmodel on an examples-proportional mixture with an artificial data set size limit ofK = 219\nbefore fine-tuning it on each individual downstream task. This helps us measure whether\nincluding the supervised tasks alongside the unsupervised objective during pre-training\n32\nExploring the Limits of Transfer Learning\nMixing strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n⋆Baseline (pre-train/fine-tune) 83.28 19 .24 80 .88 71 .36 26 .98 39 .82 27 .65\nEqual 76.13 19 .02 76 .51 63 .37 23 .89 34 .31 26 .78\nExamples-proportional, K = 216 80.45 19 .04 77 .25 69 .95 24 .35 34 .99 27 .10\nExamples-proportional, K = 217 81.56 19 .12 77 .00 67 .91 24 .36 35 .00 27 .25\nExamples-proportional, K = 218 81.67 19 .07 78 .17 67 .94 24 .57 35 .19 27 .39\nExamples-proportional, K = 219 81.42 19.24 79.78 67 .30 25 .21 36 .30 27.76\nExamples-proportional, K = 220 80.80 19.24 80 .36 67.38 25 .66 36 .93 27.68\nExamples-proportional, K = 221 79.83 18 .79 79 .50 65 .10 25 .82 37 .22 27 .13\nTemperature-scaled, T = 2 81 .90 19.28 79.42 69 .92 25 .42 36 .72 27 .20\nTemperature-scaled, T = 4 80 .56 19.22 77.99 69 .54 25 .04 35 .82 27 .45\nTemperature-scaled, T = 8 77 .21 19 .10 77 .14 66 .07 24 .55 35 .35 27 .17\nTable 11:Comparison of multi-task training using different mixing strategies. Examples-\nproportional mixing refers to sampling examples from each data set according to\nthe total size of each data set, with an artificial limit (K) on the maximum data set\nsize. Temperature-scaled mixing re-scales the sampling rates by a temperatureT.\nFor temperature-scaled mixing, we use an artificial data set size limit ofK = 221.\ngives the model some beneficial early exposure to the downstream tasks. We might also\nhope that mixing in many sources of supervision could help the pre-trained model obtain a\nmore general set of “skills” (loosely speaking) before it is adapted to an individual task. To\nmeasure this directly, we consider a second variant where we pre-train the model on the same\nexamples-proportional mixture (withK = 219) except that we omit one of the downstream\ntasks from this pre-training mixture. Then, we fine-tune the model on the task that was\nleft out during pre-training. We repeat this for each of the downstream tasks we consider.\nWe call this approach “leave-one-out” multi-task training. This simulates the real-world\nsetting where a pre-trained model is fine-tuned on a task it had not seen during pre-training.\nNote that multi-task pre-training provides a diverse mixture of supervised tasks. Since other\nfields (e.g. computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski\net al., 2014)) use a supervised data set for pre-training, we were interested to see whether\nomitting the unsupervised task from the multi-task pre-training mixture still produced good\nresults. For our third variant we therefore pre-train on an examples-proportional mixture of\nall of the supervised tasks we consider withK = 219. In all of these variants, we follow our\nstandard procedure of pre-training for219 steps before fine-tuning for218 steps.\nWe compare the results of these approaches in Table 12. For comparison, we also include\nresults for our baseline (pre-train then fine-tune) and for standard multi-task learning\n(without fine-tuning) on an examples-proportional mixture withK = 219. We find that\nfine-tuning after multi-task pre-training results in comparable performance to our baseline.\nThis suggests that using fine-tuning after multi-task learning can help mitigate some of\nthe trade-offs between different mixing rates described in Section 3.5.2. Interestingly, the\nperformance of “leave-one-out” training was only slightly worse, suggesting that a model\nthat was trained on a variety of tasks can still adapt to new tasks (i.e. multi-task pre-\ntraining might not result in a dramatic task interference). Finally, supervised multi-task\npre-training performed significantly worse in every case except for the translation tasks. This\n33\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nTraining strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n⋆Unsupervised pre-training + fine-tuning 83.28 19 .24 80 .88 71 .36 26 .98 39.82 27 .65\nMulti-task training 81.42 19.24 79.78 67 .30 25 .21 36 .30 27 .76\nMulti-task pre-training + fine-tuning 83.11 19 .12 80 .26 71 .03 27 .08 39.80 28.07\nLeave-one-out multi-task training 81.98 19 .05 79 .97 71.68 26 .93 39.79 27.87\nSupervised multi-task pre-training 79.93 18 .96 77 .38 65 .36 26 .81 40.13 28 .04\nTable 12:Comparison of unsupervised pre-training, multi-task learning, and various forms\nof multi-task pre-training.\ncould suggest that the translation tasks benefit less from (English) pre-training, whereas\nunsupervised pre-training is an important factor in the other tasks.\n3.6 Scaling\nThe “bitter lesson” of machine learning research argues that general methods that can\nleverage additional computation ultimately win out against methods that rely on human\nexpertise (Sutton, 2019; Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016;\nMahajan et al., 2018; Shazeer et al., 2018, 2017; Huang et al., 2018b; Keskar et al., 2019a).\nRecent results suggest that this may hold true for transfer learning in NLP (Liu et al., 2019c;\nRadford et al., 2019; Yang et al., 2019; Lan et al., 2019), i.e. it has repeatedly been shown\nthat scaling up produces improved performance compared to more carefully-engineered\nmethods. However, there are a variety of possible ways to scale, including using a bigger\nmodel, training the model for more steps, and ensembling. In this section, we compare these\ndifferent approaches by addressing the following premise: “You were just given4×more\ncompute. How should you use it?”\nWe start with our baseline model, which has220M parameters and is pre-trained and\nfine-tuned for 219 and 218 steps respectively. The encoder and decoder are both sized\nsimilarly to “BERTBASE”. To experiment with increased model size, we follow the guidelines\nof “BERTLARGE” Devlin et al. (2018) and usedff = 4096, dmodel = 1024, dkv = 64 and\n16-head attention mechanisms. We then generate two variants with16 and 32 layers each in\nthe encoder and decoder, producing models with2×and 4×as many parameters as our\noriginal model. These two variants also have a roughly2×and 4×the computational cost.\nUsing our baseline and these two larger models, we consider three ways of using4×as much\ncomputation: Training for4×as many steps, training for2×as many steps with the2×\nbigger model, and training the4×bigger model for the “baseline” number of training steps.\nWhen we increase the training steps, we scale both the pre-train and fine-tune steps for\nsimplicity. Note that when increasing the number of pre-training steps, we are effectively\nincluding more pre-training data as C4 is so large that we do not complete one pass over\nthe data even when training for223 steps.\nAn alternative way for the model to see4×as much data is to increase the batch size by a\nfactor of4. This can potentially result in faster training due to more efficient parallelization.\nHowever, training with a4×larger batch size can yield a different outcome than training\n34\nExploring the Limits of Transfer Learning\nScaling strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n⋆Baseline 83.28 19 .24 80 .88 71 .36 26 .98 39 .82 27 .65\n1× size, 4× training steps 85.33 19 .33 82 .45 74 .72 27 .08 40 .66 27 .93\n1× size, 4× batch size 84.60 19 .42 82 .52 74 .64 27 .07 40 .60 27 .84\n2× size, 2× training steps 86.18 19.66 84.18 77.18 27 .52 41.03 28.19\n4× size, 1× training steps 85.91 19.73 83.86 78 .04 27.47 40 .71 28 .10\n4× ensembled 84.77 20.10 83.09 71 .74 28.05 40.53 28.57\n4× ensembled, fine-tune only 84.05 19 .57 82 .36 71 .55 27 .55 40 .22 28 .09\nTable 13:Comparison of different methods of scaling up our baseline model. All methods\nexcept ensembling fine-tuned models use4×the computation as the baseline.\n“Size” refers to the number of parameters in the model and “training time” refers\nto the number of steps used for both pre-training and fine-tuning.\nfor 4×as many steps (Shallue et al., 2018). We include an additional experiment where we\ntrain our baseline model with a4×larger batch size to compare these two cases.\nIt is common practice on many of the benchmarks we consider to eke out additional\nperformance by training and evaluating using an ensemble of models. This provides an\northogonal way of using additional computation. To compare other scaling methods to\nensembling, we also measure the performance of an ensemble of4 separately pre-trained and\nfine-tuned models. We average the logits across the ensemble before feeding them into the\noutput softmax nonlinearity to obtain an aggregate prediction. Instead of pre-training4\nseparate models, a cheaper alternative is to take a single pre-trained model and produce4\nseparate fine-tuned versions. While this does not use our entire4×computational budget,\nwe also include this method to see if it produces competitive performance to the other scaling\nmethods.\nThe performance achieved after applying these various scaling methods is shown in\nTable 13. Unsurprisingly, increasing the training time and/or model size consistently\nimproves the baseline. There was no clear winner between training for4×as many steps\nor using a4×larger batch size, though both were beneficial. In general, increasing the\nmodel size resulted in an additional bump in performance compared to solely increasing\nthe training time or batch size. We did not observe a large difference between training a\n2×bigger model for2×as long and training a4×bigger model on any of the tasks we\nstudied. This suggests that increasing the training time and increasing the model size can be\ncomplementary means of improving performance. Our results also suggest that ensembling\nprovides an orthogonal and effective means of improving performance through scale. In some\ntasks (CNN/DM, WMT English to German, and WMT English to Romanian), ensembling4\ncompletely separately trained models significantly outperformed every other scaling approach.\nEnsembling models that were pre-trained together but fine-tuned separately also gave a\nsubstantial performance increase over the baseline, which suggests a cheaper means of\nimproving performance. The only exception was SuperGLUE, where neither ensembling\napproach significantly improved over the baseline.\nWe note that different scaling methods have different trade-offs that are separate from\ntheir performance. For example, using a larger model can make downstream fine-tuning and\n35\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\ninference more expensive. In contrast, the cost of pre-training a small model for longer is\neffectively amortized if it is applied to many downstream tasks. Separately, we note that\nensembling N separate models has a similar cost to using a model that has anN×higher\ncomputational cost. As a result, some consideration for the eventual use of the model is\nimportant when choosing between scaling methods.\n3.7 Putting It All Together\nWe now leverage the insights from our systematic study to determine how far we can push\nperformance on popular NLP benchmarks. We are also interested in exploring the current\nlimits of transfer learning for NLP by training larger models on large amounts of data. We\nstart with our baseline training approach and make the following changes:\nObjective We swap out the i.i.d. denoising objective in our baseline for the span-corruption\nobjective described in Section 3.3.4, which was loosely inspired by SpanBERT (Joshi\net al., 2019). Specifically, we use a mean span length of3 and corrupt15% of the\noriginal sequence. We found that this objective produced marginally better performance\n(Table 7) while being slightly more computationally efficient due to shorter target\nsequence lengths.\nLonger training Our baseline model uses a relatively small amount of pre-training (1⁄4 as\nmuch as BERT (Devlin et al., 2018),1⁄16 as much as XLNet (Yang et al., 2019),1⁄64 as\nmuch as RoBERTa (Liu et al., 2019c), etc.). Fortunately, C4 is big enough that we\ncan train for substantially longer without repeating data (which can be detrimental,\nas shown in Section 3.4.2). We found in Section 3.6 that additional pre-training can\nindeed be helpful, and that both increasing the batch size and increasing the number of\ntraining steps can confer this benefit. We therefore pre-train our models for1 million\nsteps on a batch size of211 sequences of length512, corresponding to a total of about\n1 trillion pre-training tokens (about32×as many as our baseline). In Section 3.4.1, we\nshowed that pre-training on the RealNews-like, WebText-like, and Wikipedia + TBC\ndata sets outperformed pre-training on C4 on a few downstream tasks. However, these\ndata set variants are sufficiently small that they would be repeated hundreds of times\nover the course of pre-training on1 trillion tokens. Since we showed in Section 3.4.2\nthat this repetition could be harmful, we opted instead to continue using the C4 data\nset.\nModel sizes In Section 3.6 we also showed how scaling up the baseline model size improved\nperformance. However, using smaller models can be helpful in settings where limited\ncomputational resources are available for fine-tuning or inference. Based on these\nfactors, we train models with a wide range of sizes:\n• Base. This is our baseline model, whose hyperparameters are described in\nSection 3.1.1. It has roughly220 million parameters.\n• Small. We consider a smaller model, which scales the baseline down by using\ndmodel = 512, dff = 2,048, 8-headed attention, and only6 layers each in the\nencoder and decoder. This variant has about60 million parameters.\n36\nExploring the Limits of Transfer Learning\n• Large. Since our baseline uses a BERTBASE-sized encoder and decoder, we\nalso consider a variant where the encoder and decoder are both similar in size\nand structure to BERTLARGE. Specifically, this variant uses dmodel = 1,024,\ndff = 4,096, dkv = 64, 16-headed attention, and24 layers each in the encoder and\ndecoder, resulting in around770 million parameters.\n• 3B and 11B.To further explore what kind of performance is possible when\nusing larger models, we consider two additional variants. In both cases, we use\ndmodel = 1024, a 24 layer encoder and decoder, anddkv = 128. For the “3B”\nvariant, we usedff = 16,384 with 32-headed attention, which results in around\n2.8 billion parameters; for “11B” we usedff = 65,536 with 128-headed attention\nproducing a model with about11 billion parameters. We chose to scale updff\nspecifically because modern accelerators (such as the TPUs we train our models\non) are most efficient for large dense matrix multiplications like those in the\nTransformer’s feed-forward networks.\nMulti-task pre-training In Section 3.5.3, we showed that pre-training on a multi-task\nmixture of unsupervised and supervised tasks before fine-tuning worked as well as\npre-training on the unsupervised task alone. This is the approach advocated by the\n“MT-DNN” (Liu et al., 2015, 2019b). It also has the practical benefit of being able to\nmonitor “downstream” performance for the entire duration of training, rather than\njust during fine-tuning. We therefore used multi-task pre-training in our final set of\nexperiments. We hypothesize that larger models trained for longer might benefit from\na larger proportion of unlabeled data because they are more likely to overfit to smaller\ntraining data sets. However, we also note that the results of Section 3.5.3 suggest that\nfine-tuning after multi-task pre-training can mitigate some of the issues that might\narise from choosing a suboptimal proportion of unlabeled data. Based on these ideas,\nwe substitute the following artificial data set sizes for our unlabeled data before using\nstandard example-proportional mixing (described in Section 3.5.2):710,000 for Small,\n2,620,000 for Base,8,660,000 for Large,33,500,000 for 3B, and133,000,000 for 11B.\nFor all model variants, we also capped the effective data set size of the WMT English\nto French and WMT English to German data sets to1M examples during pre-training.\nFine-tuning on individual GLUE and SuperGLUE tasksSo far, when fine-tuning\non GLUE and SuperGLUE, we have concatenated all of the data sets in each benchmark\nso that we only fine-tune models once for GLUE and once for SuperGLUE. This\napproach makes our study logistically simpler, but we found that this sacrifices a small\namount of performance on some tasks compared to fine-tuning on the task separately. A\npotential issue with fine-tuning on individual tasks, which would otherwise be mitigated\nby training on all tasks at once, is that we might overfit quickly to low-resource tasks.\nFor example, our large batch size of211 length-512 sequences would result in the entire\ndata set appearing multiple times in each batch for many of the low-resource GLUE\nand SuperGLUE tasks. We therefore use a smaller batch size of8 length-512 sequences\nduring fine-tuning for each GLUE and SuperGLUE task. We also save checkpoints\nevery 1,000 steps rather than every5,000 steps to ensure we have access to the model’s\nparameters before it overfits.\n37\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nBeam searchAll of our previous results were reported using greedy decoding. For tasks\nwith long output sequences, we found improved performance from using beam search\n(Sutskever et al., 2014). Specifically, we use a beam width of4 and a length penalty\nof α= 0.6 (Wu et al., 2016) for the WMT translation and CNN/DM summarization\ntasks.\nTest setSince this is our final set of experiments, we report results on the test set rather\nthan the validation set. For CNN/Daily Mail, we use the standard test set distributed\nwith the data set. For the WMT tasks, this corresponds to usingnewstest2014 for\nEnglish-German, newstest2015 for English-French, andnewstest2016 for English-\nRomanian. For GLUE and SuperGLUE, we used the benchmark evaluation servers to\ncompute official test set scores.15,16 For SQuAD, evaluating on the test set requires\nrunning inference on a benchmark server. Unfortunately, the computational resources\non this server are insufficient for obtaining predictions from our largest models. As\na result, we instead continue to report performance on the SQuAD validation set.\nFortunately, the model with the highest performance on the SQuAD test set also\nreported results on the validation set, so we can still compare to what is ostensibly\nthe state-of-the-art.\nApart from those changes mentioned above, we use the same training procedure and\nhyperparameters as our baseline (AdaFactor optimizer, inverse square root learning rate\nschedule for pre-training, constant learning rate for fine-tuning, dropout regularization,\nvocabulary, etc.). For reference, these details are described in Section 2.\nThe results of this final set of experiments are shown in Table 14. Overall, we achieved\nstate-of-the-art performance on18 out of the24 tasks we consider. As expected, our largest\n(11 billion parameter) model performed best among our model size variants across all tasks.\nOur T5-3B model variant did beat the previous state of the art in a few tasks, but scaling\nthe model size to11 billion parameters was the most important ingredient for achieving our\nbest performance. We now analyze the results for each individual benchmark.\nWe achieved a state-of-the-art average GLUE score of90.3. Notably, our performance was\nsubstantially better than the previous state-of-the-art for the natural language inference tasks\nMNLI, RTE, and WNLI. RTE and WNLI are two of the tasks where machine performance\nhas historically lagged behind human performance, which is93.6 and 95.9 respectively (Wang\net al., 2018). In terms of parameter count, our 11B model variant is the largest model that\nhas been submitted to the GLUE benchmark. However, most of the best-scoring submissions\nuse a large amount of ensembling and computation to produce predictions. For example,\nthe best-performing variant of ALBERT (Lan et al., 2019) uses a model similar in size and\narchitecture to our 3B variant (though it has dramatically fewer parameters due to clever\nparameter sharing). To produce its impressive performance on GLUE, the ALBERT authors\nensembled “from 6 to 17” models depending on the task. This likely results in it being more\ncomputationally expensive to produce predictions with the ALBERT ensemble than it is\nwith T5-11B.\nFor SQuAD, we outperformed the previous state-of-the-art (ALBERT (Lan et al., 2019))\nby over one point on the Exact Match score. SQuAD is a long-standing benchmark that\n15. http://gluebenchmark.com\n16. http://super.gluebenchmark.com\n38\nExploring the Limits of Transfer Learning\nGLUE CoLA SST-2 MRPC MRPC STS-B STS-B\nModel Average Matthew’s Accuracy F1 Accuracy Pearson Spearman\nPrevious best 89.4a 69.2b 97.1a 93.6b 91.5b 92.7b 92.3b\nT5-Small 77.4 41 .0 91 .8 89 .7 86 .6 85 .6 85 .0\nT5-Base 82.7 51 .1 95 .2 90 .7 87 .5 89 .4 88 .6\nT5-Large 86.4 61 .2 96 .3 92 .4 89 .9 89 .9 89 .2\nT5-3B 88.5 67 .1 97 .4 92 .5 90 .0 90 .6 89 .8\nT5-11B 90.3 71 .6 97 .5 92.8 90 .4 93.1 92 .8\nQQP QQP MNLI-m MNLI-mm QNLI RTE WNLI\nModel F1 Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy\nPrevious best 74.8c 90.7b 91.3a 91.0a 99.2a 89.2a 91.8a\nT5-Small 70.0 88 .0 82 .4 82 .3 90 .3 69 .9 69 .2\nT5-Base 72.6 89 .4 87 .1 86 .2 93 .7 80 .1 78 .8\nT5-Large 73.9 89 .9 89 .9 89 .6 94 .8 87 .2 85 .6\nT5-3B 74.4 89 .7 91 .4 91 .2 96 .3 91 .1 89 .7\nT5-11B 75.1 90.6 92.2 91 .9 96.9 92.8 94 .5\nSQuAD SQuAD SuperGLUE BoolQ CB CB COPA\nModel EM F1 Average Accuracy F1 Accuracy Accuracy\nPrevious best 90.1a 95.5a 84.6d 87.1d 90.5d 95.2d 90.6d\nT5-Small 79.10 87 .24 63 .3 76 .4 56 .9 81 .6 46 .0\nT5-Base 85.44 92 .08 76 .2 81 .4 86 .2 94 .0 71 .2\nT5-Large 86.66 93 .79 82 .3 85 .4 91 .6 94 .8 83 .4\nT5-3B 88.53 94 .95 86 .4 89 .9 90 .3 94 .4 92 .0\nT5-11B 91.26 96 .22 88 .9 91 .2 93 .9 96 .8 94 .8\nMultiRC MultiRC ReCoRD ReCoRD RTE WiC WSC\nModel F1a EM F1 Accuracy Accuracy Accuracy Accuracy\nPrevious best 84.4d 52.5d 90.6d 90.0d 88.2d 69.9d 89.0d\nT5-Small 69.3 26 .3 56 .3 55 .4 73 .3 66 .9 70 .5\nT5-Base 79.7 43 .1 75 .0 74 .2 81 .5 68 .3 80 .8\nT5-Large 83.3 50 .7 86 .8 85 .9 87 .8 69 .3 86 .3\nT5-3B 86.8 58 .3 91 .2 90 .4 90 .7 72 .1 90 .4\nT5-11B 88.1 63 .3 94 .1 93 .4 92 .5 76 .9 93 .8\nWMT EnDe WMT EnFr WMT EnRo CNN/DM CNN/DM CNN/DM\nModel BLEU BLEU BLEU ROUGE-1 ROUGE-2 ROUGE-L\nPrevious best 33.8e 43.8e 38.5f 43.47g 20.30g 40.63g\nT5-Small 26.7 36 .0 26 .8 41 .12 19 .56 38 .35\nT5-Base 30.9 41 .2 28 .0 42 .05 20 .34 39 .40\nT5-Large 32.0 41 .5 28 .1 42 .50 20 .68 39 .75\nT5-3B 31.8 42 .6 28 .2 42 .72 21 .02 39 .94\nT5-11B 32.1 43 .4 28 .1 43.52 21 .55 40 .69\nTable 14:Performance of our T5 variants on every task we study. Small, Base, Large, 3B,\nand 11B refer to model configurations with60 million, 220 million, 770 million,\n3 billion, and11 billion parameters, respectively. In the first row of each table,\nwe report the state-of-the-art for the task (as of October 24th, 2019), with the\nsuperscript denoting its source with references listed at the end of this caption. All\nresults are reported on the test set except for SQuAD where we use the validation\nset. a(Lan et al., 2019)b(Wang et al., 2019c)c(Zhu et al., 2019)d(Liu et al.,\n2019c) e(Edunov et al., 2018)f (Lample and Conneau, 2019)g(Dong et al., 2019)\n39\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nwas created over three years ago, and most recent improvements have only increased the\nstate-of-the-art by a fraction of a percentage point. We note that when results are reported\non the test set, they are typically based on an ensemble of models and/or leverage external\ndata sets (e.g. TriviaQA (Joshi et al., 2017) or NewsQA (Trischler et al., 2016)) to augment\nthe small SQuAD training set. Human performance on SQuAD is estimated at82.30 and\n91.22 for the Exact Match and F1 metric respectively (Rajpurkar et al., 2016), so it is not\nclear if further improvements on this benchmark are meaningful.\nFor SuperGLUE, we improved upon the state-of-the-art by a large margin (from an\naverage score of84.6 (Liu et al., 2019c) to88.9). SuperGLUE was designed to include\ntasks that were “beyond the scope of current state-of-the-art systems, but solvable by most\ncollege-educated English speakers” (Wang et al., 2019b). We nearly match the human\nperformance of89.8 (Wang et al., 2019b). Interestingly, on the reading comprehension tasks\n(MultiRC and ReCoRD) we exceed human performance by a large margin, suggesting the\nevaluation metrics used for these tasks may be biased towards machine-made predictions.\nOn the other hand, humans achieve100% accuracy on both COPA and WSC, which is\nsignificantly better than our model’s performance. This suggests that there remain linguistic\ntasks that are hard for our model to perfect, particularly in the low-resource setting.\nWe did not achieve state-of-the-art performance on any of the WMT translation tasks.\nThis may be in part due to our use of an English-only unlabeled data set. We also note that\nmost of the best results on these tasks use backtranslation (Edunov et al., 2018; Lample and\nConneau, 2019), which is a sophisticated data augmentation scheme. The state of the art on\nthe low-resource English to Romanian benchmark also uses additional forms of cross-lingual\nunsupervised training (Lample and Conneau, 2019). Our results suggest that scale and\nEnglish-language pre-training may be insufficient to match the performance of these more\nsophisticated methods. On a more specific note, the best results on English to German\nnewstest2014 set use the much larger training set from WMT 2018 (Edunov et al., 2018),\nmaking direct comparison to our results difficult.\nFinally, on CNN/Daily Mail we attain state-of-the-art performance, though only by\na significant amount on the ROUGE-2-F score. It has been shown that improvements\nto the ROUGE score do not necessarily correspond to more coherent summaries (Paulus\net al., 2017). Furthermore, while CNN/Daily Mail is posed as an abstractive summarization\nbenchmark, purely extractive approaches have been shown to work well (Liu, 2019). It has\nalso been argued that generative models trained with maximum likelihood are prone to\nproducing repetitive summaries (See et al., 2017). Despite these potential issues, we find\nthat our models do generate coherent and largely correct summaries. We provide some\nnon-cherry-picked validation set examples in Appendix C.\nTo achieve its strong results, T5 combines insights from our experimental study with\nunprecedented scale. Note that in Section 3.6 we found that scaling up the pre-training\namount or size of our baseline model produced substantial gains. Given this, we were\ninterested to measure how much the “non-scaling” changes we introduced into T5 contributed\nto its strong performance. We therefore carried out a final experiment where we compared\nthe following three configurations: First, the standard baseline model, which was pre-trained\non 235 ≈34B tokens; second, the baseline trained instead for about 1 trillion tokens (i.e.\nthe same amount of pre-training used for T5), which we refer to as “baseline-1T”; and\nthird, T5-Base. Note that the differences between baseline-1T and T5-Base comprise the\n40\nExploring the Limits of Transfer Learning\nModel GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n⋆Baseline 83.28 19 .24 80 .88 71 .36 26 .98 39 .82 27 .65\nBaseline-1T 84.80 19 .62 83 .01 73 .90 27 .46 40 .30 28 .34\nT5-Base 85.97 20 .90 85 .44 75 .64 28 .37 41 .37 28 .98\nTable 15:Performance comparison of T5-Base to our baseline experimental setup used in\nthe rest of the paper. Results are reported on the validation set. “Baseline-1T”\nrefers to the performance achieved by pre-training the baseline model on 1 trillion\ntokens (the same number used for the T5 model variants) instead of235 ≈34B\ntokens (as was used for the baseline).\n“non-scaling” changes we made when designing T5. As such, comparing the performance of\nthese two models gives us a concrete measurement of the impact of the insights from our\nsystematic study.\nThe performance of these three model configurations is shown in Table 15. Consistent\nwith the findings in Section 3.6, we find that additional pre-training improves performance\nover the baseline. Nevertheless, T5-Base substantially outperforms baseline-1T on all\ndownstream tasks. This suggests that scale is not the only factor that contributes to T5’s\nsuccess. We hypothesize that the larger models benefit not only from their increased size\nbut also from these non-scaling factors.\n4. Reflection\nHaving completed our systematic study, we wrap up by first recapping some of our most\nsignificant findings. Our results provide some high-level perspective on which avenues of\nresearch might be more or less promising. To conclude, we outline some topics we think\nmight provide effective approaches for further progressing the field.\n4.1 Takeaways\nText-to-text Our text-to-text framework provides a simple way to train a single model\non a wide variety of text tasks using the same loss function and decoding procedure.\nWe showed how this approach can be successfully applied to generative tasks like\nabstractive summarization, classification tasks like natural language inference, and\neven regression tasks like STS-B. In spite of its simplicity, we found the text-to-\ntext framework obtained comparable performance to task-specific architectures and\nultimately produced state-of-the-art results when combined with scale.\nArchitectures While some work on transfer learning for NLP has considered architectural\nvariants of the Transformer, we found the original encoder-decoder form worked\nbest in our text-to-text framework. Though an encoder-decoder model uses twice as\nmany parameters as “encoder-only” (e.g. BERT) or “decoder-only” (language model)\narchitectures, it has a similar computational cost. We also showed that sharing the\nparameters in the encoder and decoder did not result in a substantial performance\ndrop while halving the total parameter count.\n41\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nUnsupervised objectives Overall, we found that most “denoising” objectives, which train\nthe model to reconstruct randomly corrupted text, performed similarly in the text-to-\ntext setup. As a result, we suggest using objectives that produce short target sequences\nso that unsupervised pre-training is more computationally efficient.\nData setsWe introduced the “Colossal Clean Crawled Corpus” (C4), which comprises\nheuristically-cleaned text from the Common Crawl web dump. When comparing C4 to\ndata sets that use additional filtering, we found that training on in-domain unlabeled\ndata could boost performance in a few downstream tasks. However, constraining to\na single domain typically results in a smaller data set. We separately showed that\nperformance can degrade when an unlabeled data set is small enough that it is repeated\nmany times over the course of pre-training. This motivates the use of a large and\ndiverse data set like C4 for generic language understanding tasks.\nTraining strategiesWe found that the basic approach of updating all of a pre-trained\nmodel’s parameters during fine-tuning outperformed methods that are designed to\nupdate fewer parameters, although updating all parameters is most expensive. We also\nexperimented with various approaches for training the model on multiple tasks at once,\nwhich in our text-to-text setting simply corresponds to mixing examples from different\ndata sets when constructing batches. The primary concern in multi-task learning is\nsetting the proportion of each task to train on. We ultimately did not find a strategy\nfor setting mixing proportions that matched the performance of the basic approach of\nunsupervised pre-training followed by supervised fine-tuning. However, we found that\nfine-tuning after pre-training on a mixture of tasks produced comparable performance\nto unsupervised pre-training.\nScaling We compared various strategies for taking advantage of additional compute, includ-\ning training the model on more data, training a larger model, and using an ensemble\nof models. We found each approach conferred a significant boost in performance,\nthough training a smaller model on more data was often outperformed by training\na larger model for fewer steps. We also showed an ensemble of models can provide\nsubstantially better results than a single model, which provides an orthogonal means\nof leveraging additional computation. Ensembling models that were fine-tuned from\nthe same base pre-trained model performed worse than pre-training and fine-tuning\nall models completely separately, though fine-tune-only ensembling still substantially\noutperformed a single model.\nPushing the limitsWe combined our above insights and trained substantially larger\nmodels (up to11 billion parameters) to achieve state-of-the-art results across many of\nthe benchmarks we considered. For unsupervised training, we extracted text from our\nC4 data set and applied a denoising objective that corrupts contiguous spans of tokens.\nWe pre-trained on a multi-task mixture before fine-tuning on individual tasks. Overall,\nour models were trained on over1 trillion tokens. In the interest of facilitating the\nreplication, extension, and application of our results, we release our code, the C4 data\nset, and pre-trained model weights for each T5 variant.1\n42\nExploring the Limits of Transfer Learning\n4.2 Outlook\nThe inconvenience of large modelsAn unsurprising but important result from our\nstudy is that larger models tend to perform better. The fact that the hardware used for\nrunning these models is continually getting cheaper and more powerful suggests that\nscaling up may continue to be a promising way to achieve better performance (Sutton,\n2019). However, it will always be the case that there are applications and scenarios\nwhere using a smaller or less expensive model is helpful, for example when performing\nclient-side inference or federated learning (Konečn` y et al., 2015, 2016). Relatedly, one\nbeneficial use of transfer learning is the possibility of attaining good performance on\nlow-resource tasks. Low-resource tasks often occur (by definition) in settings where\none lacks the assets to label more data. It follows that low-resource applications often\nalso have limited access to computational resources which can incur additional costs.\nAs a result, we advocate for research on methods that achieve stronger performance\nwith cheaper models so that transfer learning can be applied where it will have the\nmost impact. Some current work along these lines include distillation (Hinton et al.,\n2015; Sanh et al., 2019; Jiao et al., 2019), parameter sharing (Lan et al., 2019), and\nconditional computation (Shazeer et al., 2017).\nMore efficient knowledge extractionRecall that one of the goals of pre-training is\n(looselyspeaking)toprovidethemodelwithgeneral-purpose“knowledge”thatimproves\nits performance on downstream tasks. The method we use in this work, which is\ncurrently common practice, is to train the model to denoise corrupted spans of text.\nWe suspect that this simplistic technique may not be a very efficient way to teach the\nmodel general-purpose knowledge. More concretely, it would be useful to be able to\nattain good fine-tuning performance without needing to train our models on1 trillion\ntokens of text first. Some concurrent work along these lines improves efficiency by\npre-training a model to distinguish between real and machine-generated text (Clark\net al., 2020).\nFormalizing the similarity between tasksWe observed that pre-training on unlabeled\nin-domain data can improve performance on downstream tasks (Section 3.4). This\nfinding mostly relies on basic observations like the fact that SQuAD was created using\ndata from Wikipedia. It would be useful to formulate a more rigorous notion of the\n“similarity” between the pre-training and downstream tasks, so that we could make\nmore principled choices about what source of unlabeled data to use. There is some\nearly empirical work along these lines in the field of computer vision (Huh et al., 2016;\nKornblith et al., 2018; He et al., 2018). A better notion of the relatedness of tasks could\nalso help choosesupervised pre-training tasks, which has been shown to be helpful for\nthe GLUE benchmark (Phang et al., 2018).\nLanguage-agnostic models We were disappointed to find that English-only pre-training\ndid not achieve state-of-the-art results on the translation tasks we studied. We also\nare interested in avoiding the logistical difficulty of needing to specify which languages\na vocabulary can encode ahead of time. To address these issues, we are interested in\nfurther investigating language-agnostic models, i.e. models that can perform a given\nNLP task with good performance regardless of the text’s language. This is an especially\n43\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\npertinent issue given that English is not the native language for the majority of the\nworld’s population.\nThe motivation for this paper was the flurry of recent work on transfer learning for\nNLP. Before we began this work, these advances had already enabled breakthroughs in\nsettings where learning-based methods had not yet been shown to be effective. We are\nhappy to be able to continue this trend, for example by nearly matching human-level\nperformance on the SuperGLUE benchmark, a task specifically designed to be difficult\nfor modern transfer-learning pipelines. Our results stem from the combination of a\nstraightforward and unified text-to-text framework, our new C4 data set, and insights\nfrom our systematic study. Additionally, we provided an empirical overview of the\nfield and a perspective on where it stands. We are excited to see continued work using\ntransfer learning towards the goal of general language understanding.\nAcknowledgments\nWe thank Grady Simon, Noah Fiedel, Samuel R. Bowman, Augustus Odena, Daphne Ippolito,\nNoah Constant, Orhan Firat, Ankur Bapna, and Sebastian Ruder for their comments on\nthis manuscript; Zak Stone and the TFRC team for their support; Austin Tarango for\nhis guidance on data set creation; Melvin Johnson, Dima Lepikhin, Katrin Tomanek, Jeff\nKlingner, and Naveen Arivazhagan for insight into multi-task machine translation; Neil\nHoulsby for comments on adapter layers; Olga Wichowska, Ola Spyra, Michael Banfield,\nYi Lin, and Frank Chen for assistance with infrastructure; Etienne Pot, Ryan Sepassi, and\nPierre Ruyssen for collaboration on TensorFlow Datasets; Rohan Anil for help with our\ndownload pipeline for Common Crawl; Robby Neale and Taku Kudo for their work on\nSentencePiece; Jeffrey Li for pointing out missing details about the creation of C4; and\nmany other members of the Google Brain team for their discussion and insight.\n44\nExploring the Limits of Transfer Learning\nAppendix A. Contributions\nColin designed the scope of this project and wrote this paper, ran all the experiments in\nSections 3.1 to 3.6, and contributed a large portion of our codebase. Noam contributed\nmany of the ideas, including the text-to-text framework, unsupervised objectives, and\ndata set mixing strategies; implemented our base Transformer model and its architectural\nvariants; and ran the experiments in Section 3.7. Adam oversaw all engineering aspects\nfor this project, created the C4 data set, implemented our data set pipeline, and added\nvarious benchmark data sets. Katherine coordinated experiments, wrote and updated\ndocumentation, ran experiments to help design our baseline, and contributed to many parts\nof our codebase. Sharan contributed some of the required data sets and preprocessors, and\nran assorted preliminary experiments, in addition to co-leading the open-sourcing of our\ncodebase. Michael owned all aspects of the Winograd data sets, ingested many of the data\nsets we used, contributed various improvements and fixes to our infrastructure, and ran some\npreliminary experiments. Yanqi ran experiments and implemented methods to help settle on\na reasonable baseline and helped with the final fine-tuning of the models in Section 3.7. Wei\nalso helped with final fine-tuning and improved some of our preprocessors. Peter prototyped\nan early version of the pre-training data set and resolved issues pertaining to the SQuAD\nand CNN/DM tasks. All authors helped set the scope and research direction we followed in\nthis work.\nAppendix B. Converting WNLI to Our Text-to-Text Format\nNote that as discussed in Section 2.4, we do not train on any of the data from WNLI. Instead,\nwhen evaluating on the WNLI test set (for the results in Section 3.7), we convert the WNLI\ntest set to the “referent noun prediction” text-to-text format so that we can evaluate using a\nmodel trained on WSC and DPR. Our WNLI preprocessor is inspired by the one proposed\nby He et al. (2019). Recall that examples from WNLI consist of a premise, a hypothesis,\nand a label that indicates whether the hypothesis is True or False. Using the example from\nSection 2.4, the hypothesis would be “The city councilmen refused the demonstrators a\npermit because they feared violence.” with the premise “The demonstrators feared violence.”\nand the label False. We first find the location of all pronouns in the premise (“they” in\nour example). Then, we find the maximum number of words that precede or follow each\npronoun that are a substring in the hypothesis (“feared violence” in our example), ignoring\ncase and punctuation. When the premise contains multiple candidate pronouns, we choose\nthe pronoun that is preceded or followed by the largest substring of the hypothesis. We then\nhighlight the pronoun in the premise by surrounding it with asterisks. For the candidate\nnoun (which is compared to our model’s prediction to obtain a True or False label), we\nremove the matching substring from the hypothesis and optionally make it non-possessive\n(resulting in “the demonstrators”).\nAppendix C. Example Predictions on CNN/Daily Mail\nTo show that our model is generating fluent summaries, we include a few example decodes\nfrom our best model (T5-11B) on the validation set along with the ground-truth summaries.\nThese examples selected at random and were not cherry-picked.\n45\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n1. Ground-truth: leopard gave up after spiky creature refused to back down\nin fight in kruger national park, south africa . wildlife enthusiast lisl\nmoolman, 41, caught the bizarre battle while out on the road and armed with\nher camera . the leopard and porcupine tumbled out of the bushes and began\nto fight by roadside - watched by ms moolman .\nPrediction: leopard tried to make lunch out of a plucky porcupine in kruger\nnational park, south africa . but the predator was put firmly in its place\nafter the spiky creature refused to back down during a fight . wildlife\nenthusiast lisl moolman, 41, caught the bizarre battle while out on the\nroad and armed with her camera .\n2. Ground-truth: researchers say homes are a hotbed of undiscovered species\n. study of 40 north carolina homes found 8,000 bacterial and archaeal taxa\n. 11 houses in california revealed a great variety of fungus among us .\nPrediction: researchers estimate that the indoor biome makes up about 0.5\npercent of ice-free land, or about 247,000 square miles, almost the size\nof texas . a study of 40 houses in north carolina used genomic technologies\nto document more than 8,000 bacterial and archaeal taxa . another study\nof 50 houses in the same state described more than 750 types of arthropods,\nincluding insects and spiders . an examination of 11 houses in california\nfound a great variety of fungus among us .\n3. Ground-truth: executions have been temporarily halted in the state after\nthe drug used for lethal injections appeared faulty on monday night . kelly\ngissendaner had her death delayed for a second time when it was found that\nthe lethal pentobarbital dose appeared cloudy . state officials dithered\nover the decision , changing their mind three times before deciding not\nto take the risk that it wouldn’t work . it is ’essential that executions\nare carried out in a constitutional manner,’ said georgia attorney general\nsam olens .\nPrediction: the execution of kelly gissendaner, the only woman on georgia’s\ndeath row, was delayed for a second time on monday . the execution team\nfound that the lethal pentobarbital dose appeared cloudy . the cloudy drug\nbolstered death penalty opponents, who have been vocal in their opposition\nafter three botched executions in other parts of the country .\n4. Ground-truth: dani alves was not selected for the brazil squad to face\nfrance and chile . barcelona defender was filmed serving up hot dogs to\npeople on saturday . this week alves released a charity single with former\nteam-mat jose pinto . alves looks set to leave barcelona on a free transfer\nthis summer . reports in spanish press claim he will sign for paris saint-germain\n.\nPrediction: dani alves was not selected for brazil’s friendlies against\nfrance and chile . the barcelona right back has released a charity single\nwith jose pinto . alves was filmed serving up snacks from behind the counter\n46\nExploring the Limits of Transfer Learning\nof a hot dog bar on saturday . alves is yet to be offered a new deal at\nthe nou camp . click here for all the latest barcelona news .\nAppendix D. Preprocessed Examples\nIn this section, we provide examples of our preprocessing for each of the data sets we consider.\nD.1 CoLA\nOriginal input:\nSentence: John made Bill master of himself.\nProcessed input: cola sentence: John made Bill master of himself.\nOriginal target:1\nProcessed target: acceptable\nD.2 RTE\nOriginal input:\nSentence 1:A smaller proportion of Yugoslavia’s Italians were settled in Slovenia\n(at the 1991 national census, some 3000 inhabitants of Slovenia declared\nthemselves as ethnic Italians).\nSentence 2:Slovenia has 3,000 inhabitants.\nProcessed input: rte sentence1: A smaller proportion of Yugoslavia’s Italians\nwere settled in Slovenia (at the 1991 national census, some 3000 inhabitants\nof Slovenia declared themselves as ethnic Italians). sentence2: Slovenia\nhas 3,000 inhabitants.\nOriginal target:1\nProcessed target: not_entailment\nD.3 MNLI\nOriginal input:\nHypothesis: The St. Louis Cardinals have always won.\nPremise: yeah well losing is i mean i’m i’m originally from Saint Louis and\nSaint Louis Cardinals when they were there were uh a mostly a losing team\nbut\nProcessed input: mnli hypothesis: The St. Louis Cardinals have always won. premise:\nyeah well losing is i mean i’m i’m originally from Saint Louis and Saint Louis\nCardinals when they were there were uh a mostly a losing team but\n47\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nOriginal target: 2\nProcessed target: contradiction\nD.4 MRPC\nOriginal input:\nSentence 1:We acted because we saw the existing evidence in a new light ,\nthrough the prism of our experience on 11 September , \" Rumsfeld said .\nSentence 2:Rather , the US acted because the administration saw \" existing\nevidence in a new light , through the prism of our experience on September\n11 \" .\nProcessed input: mrpc sentence1: We acted because we saw the existing evidence\nin a new light , through the prism of our experience on 11 September , \" Rumsfeld\nsaid . sentence2: Rather , the US acted because the administration saw \"\nexisting evidence in a new light , through the prism of our experience on\nSeptember 11 \" .\nOriginal target:1\nProcessed target: equivalent\nD.5 QNLI\nOriginal input:\nQuestion: Where did Jebe die?\nSentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and\nJebe died on the road back to Samarkand.\nProcessed input: qnli question: Where did Jebe die? sentence: Genghis Khan recalled\nSubutai back to Mongolia soon afterwards, and Jebe died on the road back to\nSamarkand.\nOriginal target: 0\nProcessed target: entailment\nD.6 QQP\nOriginal input:\nQuestion 1:What attributes would have made you highly desirable in ancient\nRome?\nQuestion 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\nProcessed input: qqp question1: What attributes would have made you highly desirable\nin ancient Rome? question2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A\nFRESHER?\n48\nExploring the Limits of Transfer Learning\nOriginal target: 0\nProcessed target: not_duplicate\nD.7 SST2\nOriginal input:\nSentence: it confirms fincher ’s status as a film maker who artfully bends\ntechnical know-how to the service of psychological insight .\nProcessed input: sst2 sentence: it confirms fincher ’s status as a film maker\nwho artfully bends technical know-how to the service of psychological insight\n.\nOriginal target:1\nProcessed target: positive\nD.8 STSB\nOriginal input:\nSentence 1:Representatives for Puretunes could not immediately be reached\nfor comment Wednesday.\nSentence 2:Puretunes representatives could not be located Thursday to comment\non the suit.\nProcessed input: stsb sentence1: Representatives for Puretunes could not immediately\nbe reached for comment Wednesday. sentence2: Puretunes representatives could\nnot be located Thursday to comment on the suit.\nOriginal target: 3.25\nProcessed target: 3.2\nD.9 CB\nOriginal input:\nHypothesis: Valence was helping\nPremise: Valence the void-brain, Valence the virtuous valet. Why couldn’t\nthe figger choose his own portion of titanic anatomy to shaft? Did he think\nhe was helping?\nProcessed input: cb hypothesis: Valence was helping premise: Valence the void-brain,\nValence the virtuous valet. Why couldn’t the figger choose his own portion\nof titanic anatomy to shaft? Did he think he was helping?\nOriginal target:1\nProcessed target: contradiction\n49\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nD.10 COPA\nOriginal input:\nQuestion: effect\nPremise: Political violence broke out in the nation.\nChoice 1: Many citizens relocated to the capitol.\nChoice 2: Many citizens took refuge in other territories.\nProcessed input: copa choice1: Many citizens relocated to the capitol. choice2:\nMany citizens took refuge in other territories. premise: Political violence\nbroke out in the nation. question: effect\nOriginal target:1\nProcessed target: True\nD.11 MultiRC\nOriginal input:\nAnswer: There was only pie to eat, rather than traditional breakfast foods\nParagraph: <b>Sent 1: </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent\n2: </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent\n3: </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent\n4: </b>One day, Joey and Jimmy went swimming together at their Aunt Julie’s\npond.<br><b>Sent 5: </b>Joey woke up early in the morning to eat some food\nbefore they left.<br><b>Sent 6: </b>He couldn’t find anything to eat except\nfor pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit (a pear),\nor oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and Jimmy went\nto the pond.<br><b>Sent 9: </b>On their way there they saw their friend\nJack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for several\nhours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent\n12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent\n13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When\nthey got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent\n15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16:\n</b>The two squirrels ate some food that Joey’s mom, Jasmine, made and went\noff to bed.<br>\nQuestion: Why was Joey surprised the morning he woke up for breakfast?\nProcessed input: multirc question: Why was Joey surprised the morning he woke\nup for breakfast? answer: There was only pie to eat, rather than traditional\nbreakfast foods paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel\nnamed Joey.<br><b>Sent 2: </b>Joey loved to go outside and play with his cousin\nJimmy.<br><b>Sent 3: </b>Joey and Jimmy played silly games together, and were\nalways laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together\n50\nExploring the Limits of Transfer Learning\nat their Aunt Julie’s pond.<br><b>Sent 5: </b>Joey woke up early in the morning\nto eat some food before they left.<br><b>Sent 6: </b>He couldn’t find anything\nto eat except for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit\n(a pear), or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and\nJimmy went to the pond.<br><b>Sent 9: </b>On their way there they saw their\nfriend Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for\nseveral hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent\n12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent\n13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When\nthey got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent\n15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16: </b>The\ntwo squirrels ate some food that Joey’s mom, Jasmine, made and went off to\nbed.<br>\nOriginal target:1\nProcessed target: True\nD.12 WiC\nOriginal input:\nPOS: N\nSentence 1:It was the deliberation of his act that was insulting .\nSentence 2:The deliberations of the jury .\nWord: deliberation\nProcessed input: wic pos: N sentence1: It was the deliberation of his act that\nwas insulting . sentence2: The deliberations of the jury . word: deliberation\nOriginal target: 0\nProcessed target: False\nD.13 WSC and DPR\nOriginal input:\nSpan 2 text:it\nSpan 1 text:stable\nSpan 2 index:20\nSpan 1 index:1\nText: The stable was very roomy, with four good stalls; a large swinging window\nopened into the yard , which made it pleasant and airy.\nProcessed input: wsc: The stable was very roomy, with four good stalls; a large\nswinging window opened into the yard , which made *it* pleasant and airy.\n51\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nOriginal target:1\nProcessed target: stable\nD.14 CNN/Daily Mail\nOriginal input: marouane fellaini and adnan januzaj continue to show the world\nthey are not just teammates but also best mates. the manchester united and\nbelgium duo both posted pictures of themselves out at a restaurant on monday\nnight ahead of their game against newcastle on wednesday . januzaj poses\nin the middle of fellaini and a friend looking like somebody who failed to\nreceive the memo about it being a jackson 5 themed night. premier league\nduo adnan januzaj and marouane fellaini pose with a friend on the dance floor\n. manchester united and belgium duo fellaini and januzaj are good friends\nboth on and off the pitch . manchester united ace fellaini runs over to the\nbench to celebrate his goal against qpr with friend januzaj . the disco effect\nin the background adds to the theory, but januzaj doesn’t seem to mind as\nthey later pose on the dance floor with other friends. united haven’t had\ntoo many reasons to have a song and dance this season so it seems they may\nbe hitting the discotheques as another form of release. however, victory against\nnewcastle on wednesday would leave manager louis van gaal at least tapping\nhis toes as they continue to fight for a champions league spot this season.\njanuzaj and robin van persie join fellaini in celebrating in front of the\nmanchester united fans at west brom . januzaj receives some words of wisdom\nfrom manchester united’s dutch manager louis van gaal . januzaj and fellaini\nare joined by some friends as they take to the dance floor ahead of the newcastle\ngame .\nProcessed input: summarize: marouane fellaini and adnan januzaj continue to show\nthe world they are not just teammates but also best mates. the manchester\nunited and belgium duo both posted pictures of themselves out at a restaurant\non monday night ahead of their game against newcastle on wednesday . januzaj\nposes in the middle of fellaini and a friend looking like somebody who failed\nto receive the memo about it being a jackson 5 themed night. premier league\nduo adnan januzaj and marouane fellaini pose with a friend on the dance floor\n. manchester united and belgium duo fellaini and januzaj are good friends\nboth on and off the pitch . manchester united ace fellaini runs over to the\nbench to celebrate his goal against qpr with friend januzaj . the disco effect\nin the background adds to the theory, but januzaj doesn’t seem to mind as\nthey later pose on the dance floor with other friends. united haven’t had\ntoo many reasons to have a song and dance this season so it seems they may\nbe hitting the discotheques as another form of release. however, victory against\nnewcastle on wednesday would leave manager louis van gaal at least tapping\nhis toes as they continue to fight for a champions league spot this season.\njanuzaj and robin van persie join fellaini in celebrating in front of the\nmanchester united fans at west brom . januzaj receives some words of wisdom\n52\nExploring the Limits of Transfer Learning\nfrom manchester united’s dutch manager louis van gaal . januzaj and fellaini\nare joined by some friends as they take to the dance floor ahead of the newcastle\ngame .\nOriginal target: the belgian duo took to the dance floor on monday night with\nsome friends . manchester united face newcastle in the premier league on\nwednesday . red devils will be looking for just their second league away win\nin seven . louis van gaal’s side currently sit two points clear of liverpool\nin fourth .\nProcessed target: the belgian duo took to the dance floor on monday night with\nsome friends . manchester united face newcastle in the premier league on\nwednesday . red devils will be looking for just their second league away win\nin seven . louis van gaal’s side currently sit two points clear of liverpool\nin fourth .\nD.15 SQuAD\nOriginal input:\nQuestion: What does increased oxygen concentrations in the patient’s lungs\ndisplace?\nContext: Hyperbaric (high-pressure) medicine uses special oxygen chambers\nto increase the partial pressure of O 2 around the patient and, when needed,\nthe medical staff. Carbon monoxide poisoning, gas gangrene, and decompression\nsickness (the ’bends’) are sometimes treated using these devices. Increased\nO 2 concentration in the lungs helps to displace carbon monoxide from the\nheme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria\nthat cause gas gangrene, so increasing its partial pressure helps kill them.\nDecompression sickness occurs in divers who decompress too quickly after\na dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming\nin their blood. Increasing the pressure of O 2 as soon as possible is part\nof the treatment.\nProcessed input: question: What does increased oxygen concentrations in the patient’s\nlungs displace? context: Hyperbaric (high-pressure) medicine uses special\noxygen chambers to increase the partial pressure of O 2 around the patient\nand, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene,\nand decompression sickness (the ’bends’) are sometimes treated using these\ndevices. Increased O 2 concentration in the lungs helps to displace carbon\nmonoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the\nanaerobic bacteria that cause gas gangrene, so increasing its partial pressure\nhelps kill them. Decompression sickness occurs in divers who decompress too\nquickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and\nhelium, forming in their blood. Increasing the pressure of O 2 as soon as\npossible is part of the treatment.\n53\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nOriginal target: carbon monoxide\nProcessed target: carbon monoxide\nD.16 WMT English to German\nOriginal input: \"Luigi often said to me that he never wanted the brothers to end\nup in court,\" she wrote.\nProcessed input: translate English to German: \"Luigi often said to me that he\nnever wanted the brothers to end up in court,\" she wrote.\nOriginal target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder\nvor Gericht landen\", schrieb sie.\nProcessed target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder\nvor Gericht landen\", schrieb sie.\nD.17 WMT English to French\nOriginal input:This image section from an infrared recording by the Spitzer telescope\nshows a \"family portrait\" of countless generations of stars: the oldest stars\nare seen as blue dots, while more difficult to identify are the pink-coloured\n\"new-borns\" in the star delivery room.\nProcessed input: translate English to French: This image section from an infrared\nrecording by the Spitzer telescope shows a \"family portrait\" of countless\ngenerations of stars: the oldest stars are seen as blue dots, while more difficult\nto identify are the pink-coloured \"new-borns\" in the star delivery room.\nOriginal target: Ce détail d’une photographie infrarouge prise par le télescope\nSpitzer montre un \"portrait de famille\" des innombrables générations d’étoiles:\nles plus vieilles étoiles sont en bleu et les points roses, plus difficiles\nà identifier, sont les \"nouveau-nés\" dans la salle d’accouchement de l’univers.\nProcessed target: Ce détail d’une photographie infrarouge prise par le télescope\nSpitzer montre un \"portrait de famille\" des innombrables générations d’étoiles:\nles plus vieilles étoiles sont en bleu et les points roses, plus difficiles\nà identifier, sont les \"nouveau-nés\" dans la salle d’accouchement de l’univers.\nD.18 WMT English to Romanian\nOriginal input:Taco Bell said it plans to add 2,000 locations in the US by 2022.\nProcessed input: translate English to Romanian: Taco Bell said it plans to add\n2,000 locations in the US by 2022.\nOriginal target:Taco Bell a afirmat că, până în 2022, intent, ionează să deschidă\n2000 de restaurante în SUA.\n54\nExploring the Limits of Transfer Learning\nProcessed target: Taco Bell a afirmat că, până în 2022, intent, ionează să deschidă\n2000 de restaurante în SUA.\n55\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nAppendix E. Scores on Every Task for All Experiments\nThe following table lists the scores achieved on every task in the experiments described in\nSections 3.2 to 3.6.\n56\nGLUE SuperGLUE WMT\nScore CoLA SST-2 MRPC MRPC STSB STSB QQP QQP MNLI m MNLImm QNLI RTE CNN/DM SQuAD Score BoolQ CB CB COPA MultiRC MultiRC ReCoRD ReCoRD RTE WiC WSC EnDe EnFr EnRo\nTable Experiment Average MCC Acc F1 Acc PCC SCC F1 Acc Acc Acc Acc Acc R-1-F R-2-F R-L-F EM F1 Average Acc F1 Acc Acc F1 EM F1 EM Acc Acc Acc BLEU BLEU BLEU\n1 ⋆Baseline average 83.28 53 .84 92 .68 92 .07 88 .92 88 .02 87 .94 88 .67 91 .56 84 .24 84 .57 90 .48 76 .28 41 .33 19 .24 38 .77 80 .88 88 .81 71 .36 76 .62 91 .22 91 .96 66 .20 66 .13 25 .78 69 .05 68 .16 75 .34 68 .04 78 .56 26 .98 39 .82 27 .65\n1 Baseline standard deviation 0.235 1 .111 0 .569 0 .729 1 .019 0 .374 0 .418 0 .108 0 .070 0 .291 0 .231 0 .361 1 .393 0 .065 0 .065 0 .058 0 .343 0 .226 0 .416 0 .365 3 .237 2 .560 2 .741 0 .716 1 .011 0 .370 0 .379 1 .228 0 .850 2 .029 0 .112 0 .090 0 .108\n1 No pre-training 66.22 12 .29 80 .62 81 .42 73 .04 72 .58 72 .97 81 .94 86 .62 68 .02 67 .98 75 .69 58 .84 39 .19 17 .60 36 .69 50 .31 61 .97 53 .04 65 .38 71 .61 76 .79 62 .00 59 .10 0 .84 20 .33 17 .95 54 .15 54 .08 65 .38 25 .86 39 .77 24 .04\n2 ⋆Enc/dec, denoising 83.28 53 .84 92 .68 92 .07 88 .92 88 .02 87 .94 88 .67 91 .56 84 .24 84 .57 90 .48 76 .28 41 .33 19 .24 38 .77 80 .88 88 .81 71 .36 76 .62 91 .22 91 .96 66 .20 66 .13 25 .78 69 .05 68 .16 75 .34 68 .04 78 .56 26 .98 39 .82 27 .65\n2 Enc/dec, shared, denoising 82.81 55 .24 91 .86 91 .58 88 .24 87 .43 87 .58 88 .69 91 .60 83 .88 84 .01 90 .23 73 .65 41 .11 18 .78 38 .48 80 .63 88 .49 70 .73 77 .13 95 .04 96 .43 65 .00 66 .16 22 .98 68 .95 68 .09 70 .76 68 .18 75 .96 26 .72 39 .03 27 .46\n2 Enc/dec, 6 layers, denoising 80.88 46 .26 92 .09 91 .51 87 .99 87 .01 86 .76 87 .93 90 .97 82 .20 82 .41 88 .83 71 .48 40 .83 18 .97 38 .31 77 .59 86 .07 68 .42 73 .79 91 .70 92 .86 67 .00 61 .02 19 .62 61 .26 60 .33 72 .20 65 .99 75 .00 26 .38 38 .40 26 .95\n2 Language model, denoising 74.70 24 .50 90 .60 86 .08 78 .92 85 .22 85 .42 85 .40 88 .99 76 .72 77 .05 86 .02 64 .62 39 .49 17 .93 36 .91 61 .14 71 .37 55 .02 65 .47 60 .08 71 .43 58 .00 43 .03 2 .94 53 .35 52 .31 53 .07 58 .62 63 .46 25 .09 35 .28 25 .86\n2 Prefix LM, denoising 81.82 49 .99 92 .43 91 .43 88 .24 87 .20 86 .98 88 .41 91 .39 82 .32 82 .93 88 .71 74 .01 40 .46 18 .61 37 .90 78 .94 87 .31 68 .11 75 .50 93 .37 91 .07 60 .00 63 .43 21 .20 65 .03 64 .11 71 .48 65 .67 73 .08 26 .43 37 .98 27 .39\n2 Enc/dec, LM 79.56 42 .03 91 .86 91 .64 88 .24 87 .13 87 .00 88 .21 91 .15 81 .68 81 .66 88 .54 65 .70 40 .67 18 .59 38 .13 76 .02 84 .85 64 .29 72 .23 85 .74 89 .29 57 .00 60 .53 16 .26 59 .28 58 .30 65 .34 64 .89 70 .19 26 .27 39 .17 26 .86\n2 Enc/dec, shared, LM 79.60 44 .83 92 .09 90 .20 85 .78 86 .03 85 .87 87 .77 91 .02 81 .74 82 .29 89 .16 65 .34 40 .16 18 .13 37 .59 76 .35 84 .86 63 .50 70 .49 91 .41 87 .50 55 .00 60 .21 16 .89 57 .83 56 .73 63 .54 63 .48 70 .19 26 .62 39 .17 27 .05\n2 Enc/dec, 6 layers, LM 78.67 38 .72 91 .40 90 .40 86 .52 86 .82 86 .49 87 .87 91 .03 80 .99 80 .92 88 .05 65 .70 40 .29 18 .26 37 .70 75 .32 84 .06 64 .06 71 .38 85 .25 89 .29 60 .00 57 .56 16 .79 55 .22 54 .30 66 .79 63 .95 71 .15 26 .13 38 .42 26 .89\n2 Language model, LM 73.78 28 .53 89 .79 85 .23 78 .68 84 .22 84 .00 84 .88 88 .70 74 .94 75 .77 84 .84 58 .84 38 .97 17 .54 36 .37 53 .81 64 .55 56 .51 64 .22 59 .92 71 .43 64 .00 53 .04 1 .05 46 .81 45 .78 58 .84 56 .74 69 .23 25 .23 34 .31 25 .38\n2 Prefix LM, LM 79.68 41 .26 92 .09 90 .11 86 .27 86 .82 86 .32 88 .35 91 .35 81 .71 82 .02 89 .04 68 .59 39 .66 17 .84 37 .13 76 .87 85 .39 64 .86 71 .47 93 .37 91 .07 57 .00 58 .67 16 .89 59 .25 58 .16 64 .26 66 .30 71 .15 26 .28 37 .51 26 .76\n4 Language modeling with prefix 80.69 44 .22 93 .00 91 .68 88 .48 87 .20 87 .18 88 .39 91 .41 82 .66 83 .09 89 .29 68 .95 40 .71 18 .94 38 .15 77 .99 86 .43 65 .27 73 .55 83 .95 87 .50 55 .00 59 .65 18 .89 61 .76 60 .76 68 .59 65 .67 73 .08 26 .86 39 .73 27 .49\n4 BERT-style (Devlin et al., 2018) 82.96 52 .49 92 .55 92 .79 89 .95 87 .68 87 .66 88 .47 91 .44 83 .60 84 .05 90 .33 75 .45 41 .27 19 .17 38 .72 80 .65 88 .24 69 .85 76 .48 94 .37 94 .64 61 .00 63 .29 25 .08 66 .76 65 .85 72 .20 69 .12 75 .00 26 .78 40 .03 27 .41\n4 Deshuffling 73.17 22 .82 87 .16 86 .88 81 .13 84 .03 83 .82 86 .38 89 .90 76 .30 76 .34 84 .18 58 .84 40 .75 18 .59 38 .10 67 .61 76 .76 58 .47 69 .17 63 .70 78 .57 56 .00 59 .85 12 .70 45 .52 44 .36 57 .04 64 .89 68 .27 26 .11 39 .30 25 .62\n5 BERT-style (Devlin et al., 2018) 82.96 52 .49 92 .55 92 .79 89 .95 87 .68 87 .66 88 .47 91 .44 83 .60 84 .05 90 .33 75 .45 41 .27 19 .17 38 .72 80 .65 88 .24 69 .85 76 .48 94 .37 94 .64 61 .00 63 .29 25 .08 66 .76 65 .85 72 .20 69 .12 75 .00 26 .78 40 .03 27 .41\n5 MASS-style (Song et al., 2019) 82.32 47 .01 91 .63 92 .53 89 .71 88 .21 88 .18 88 .58 91 .44 82 .96 83 .67 90 .02 77 .26 41 .16 19 .16 38 .55 80 .10 88 .07 69 .28 75 .08 84 .98 89 .29 63 .00 64 .46 23 .50 66 .71 65 .91 72 .20 67 .71 78 .85 26 .79 39 .89 27 .55\n5 ⋆Replace corrupted spans 83.28 53 .84 92 .68 92 .07 88 .92 88 .02 87 .94 88 .67 91 .56 84 .24 84 .57 90 .48 76 .28 41 .33 19 .24 38 .77 80 .88 88 .81 71 .36 76 .62 91 .22 91 .96 66 .20 66 .13 25 .78 69 .05 68 .16 75 .34 68 .04 78 .56 26 .98 39 .82 27 .65\n5 Drop corrupted tokens 84.44 60 .04 92 .89 92 .79 89 .95 87 .28 86 .85 88 .56 91 .54 83 .94 83 .92 90 .74 79 .42 41 .27 19 .31 38 .70 80 .52 88 .28 68 .67 75 .90 96 .02 94 .64 56 .00 65 .06 23 .92 65 .54 64 .60 71 .12 67 .40 74 .04 27 .07 39 .76 27 .82\n6 Corruption rate = 10% 82 .82 52 .71 92 .09 91 .55 88 .24 88 .19 88 .15 88 .47 91 .40 83 .50 84 .51 90 .33 75 .45 41 .05 19 .00 38 .53 80 .38 88 .36 69 .55 74 .98 92 .37 92 .86 62 .00 66 .04 24 .66 67 .93 67 .09 70 .76 67 .24 75 .96 26 .87 39 .28 27 .44\n6 ⋆Corruption rate =15% 83 .28 53 .84 92 .68 92 .07 88 .92 88 .02 87 .94 88 .67 91 .56 84 .24 84 .57 90 .48 76 .28 41 .33 19 .24 38 .77 80 .88 88 .81 71 .36 76 .62 91 .22 91 .96 66 .20 66 .13 25 .78 69 .05 68 .16 75 .34 68 .04 78 .56 26 .98 39 .82 27 .65\n6 Corruption rate = 25% 83 .00 53 .47 93 .00 92 .44 89 .46 87 .36 87 .36 88 .68 91 .53 84 .44 84 .15 90 .77 74 .01 41 .69 19 .54 39 .14 80 .96 88 .61 70 .48 76 .39 93 .02 92 .86 68 .00 65 .46 24 .66 68 .20 67 .39 73 .65 67 .87 72 .12 27 .04 39 .83 27 .47\n6 Corruption rate = 50% 81 .27 46 .26 91 .63 91 .11 87 .99 87 .87 87 .64 88 .70 91 .57 83 .64 84 .10 90 .24 70 .76 41 .51 19 .32 38 .89 79 .80 87 .76 70 .33 75 .02 93 .05 92 .86 68 .00 62 .97 24 .13 64 .94 64 .13 72 .20 68 .50 77 .88 27 .01 39 .90 27 .49\n7 ⋆Baseline (i.i.d.) 83.28 53 .84 92 .68 92 .07 88 .92 88 .02 87 .94 88 .67 91 .56 84 .24 84 .57 90 .48 76 .28 41 .33 19 .24 38 .77 80 .88 88 .81 71 .36 76 .62 91 .22 91 .96 66 .20 66 .13 25 .78 69 .05 68 .16 75 .34 68 .04 78 .56 26 .98 39 .82 27 .65\n7 Average span length = 2 83 .54 53 .82 92 .20 93 .05 90 .44 87 .85 87 .71 88 .42 91 .40 84 .28 84 .46 90 .88 77 .62 41 .23 19 .39 38 .69 82 .09 89 .69 72 .20 77 .06 90 .43 91 .07 70 .00 66 .28 26 .13 71 .34 70 .61 75 .45 68 .34 78 .85 26 .76 39 .99 27 .63\n7 Average span length = 3 83 .49 53 .90 92 .43 92 .25 89 .46 87 .49 87 .53 88 .72 91 .51 84 .85 84 .84 90 .99 77 .26 41 .50 19 .62 38 .94 81 .84 89 .66 72 .53 76 .85 94 .37 94 .64 70 .00 67 .64 28 .75 70 .84 69 .90 74 .73 67 .71 77 .88 26 .86 39 .65 27 .62\n7 Average span length = 5 83 .40 52 .12 93 .12 92 .63 89 .71 88 .70 88 .47 88 .84 91 .64 84 .32 84 .29 90 .79 76 .90 41 .39 19 .24 38 .82 82 .05 89 .79 72 .23 77 .06 83 .06 89 .29 69 .00 68 .16 30 .12 71 .36 70 .53 75 .81 69 .91 79 .81 26 .88 39 .40 27 .53\n7 Average span length = 10 82 .85 50 .11 92 .09 91 .95 88 .97 88 .45 88 .22 88 .86 91 .63 84 .34 84 .28 91 .07 76 .17 41 .38 19 .33 38 .80 81 .84 89 .39 70 .44 76 .45 87 .40 89 .29 65 .00 66 .87 29 .59 69 .82 68 .94 72 .56 67 .55 75 .96 26 .79 39 .49 27 .69\n8 ⋆C4 83.28 53 .84 92 .68 92 .07 88 .92 88 .02 87 .94 88 .67 91 .56 84 .24 84 .57 90 .48 76 .28 41 .33 19 .24 38 .77 80 .88 88 .81 71 .36 76 .62 91 .22 91 .96 66 .20 66 .13 25 .78 69 .05 68 .16 75 .34 68 .04 78 .56 26 .98 39 .82 27 .65\n8 C4, unfiltered 81.46 48 .01 91 .63 92 .72 89 .95 87 .79 87 .60 88 .31 91 .27 82 .30 82 .34 88 .71 72 .20 41 .09 19 .14 38 .54 78 .78 87 .04 68 .04 75 .75 89 .17 91 .07 62 .00 65 .52 25 .60 62 .42 61 .58 69 .68 67 .08 72 .12 26 .55 39 .34 27 .21\n8 RealNews-like 83.83 56 .55 92 .66 92 .06 88 .97 87 .71 87 .37 88 .51 91 .49 84 .35 84 .46 90 .61 78 .34 41 .38 19 .23 38 .84 80 .39 88 .50 72 .38 77 .00 93 .09 94 .64 66 .00 65 .92 23 .82 74 .56 73 .72 75 .81 66 .61 80 .77 26 .75 39 .90 27 .48\n8 WebText-like 84.03 56 .38 93 .12 92 .31 89 .22 88 .69 88 .68 88 .65 91 .56 84 .70 84 .84 90 .83 77 .62 41 .23 19 .31 38 .70 81 .42 89 .15 71 .40 76 .88 83 .08 89 .29 66 .00 64 .10 24 .24 72 .24 71 .36 75 .45 68 .03 82 .69 26 .80 39 .74 27 .59\n8 Wikipedia 81.85 45 .53 92 .32 91 .67 88 .24 85 .62 86 .40 88 .37 91 .34 82 .61 83 .25 90 .96 77 .26 41 .39 19 .31 38 .81 81 .29 89 .18 68 .01 76 .12 56 .03 80 .36 67 .00 65 .01 25 .92 69 .03 68 .06 74 .73 67 .08 76 .92 26 .94 39 .69 27 .67\n8 Wikipedia + TBC 83.65 55 .53 92 .78 92 .41 89 .22 86 .67 86 .27 89 .47 92 .29 84 .38 83 .45 91 .94 76 .90 41 .22 19 .28 38 .67 82 .08 89 .70 73 .24 76 .22 95 .40 92 .86 69 .00 51 .59 50 .93 69 .53 68 .51 77 .62 66 .93 81 .73 26 .77 39 .63 27 .57\n9 ⋆Full data set 83.28 53 .84 92 .68 92 .07 88 .92 88 .02 87 .94 88 .67 91 .56 84 .24 84 .57 90 .48 76 .28 41 .33 19 .24 38 .77 80 .88 88 .81 71 .36 76 .62 91 .22 91 .96 66 .20 66 .13 25 .78 69 .05 68 .16 75 .34 68 .04 78 .56 26 .98 39 .82 27 .65\n9 229 (64 repeats) 82.87 53 .82 92 .78 91 .79 88 .73 87 .56 87 .58 88 .73 91 .54 84 .07 84 .21 90 .59 73 .65 41 .18 19 .19 38 .67 80 .97 88 .90 72 .03 76 .76 92 .96 92 .86 66 .00 65 .11 26 .76 69 .35 68 .49 75 .81 67 .24 82 .69 26 .83 39 .74 27 .63\n9 227 (256 repeats) 82.62 50 .60 92 .32 92 .07 88 .73 87 .83 87 .60 88 .65 91 .54 83 .43 84 .37 90 .12 75 .81 41 .24 19 .20 38 .70 79 .78 87 .63 69 .97 75 .29 93 .42 91 .07 63 .00 61 .82 23 .61 66 .27 65 .39 73 .65 66 .30 80 .77 27 .02 39 .71 27 .33\n9 225 (1,024 repeats) 79.55 43 .84 91 .28 89 .32 85 .05 85 .92 85 .74 88 .05 91 .09 81 .29 81 .72 87 .90 69 .31 40 .66 18 .57 38 .13 76 .27 84 .58 64 .76 72 .63 83 .97 82 .14 64 .00 59 .39 17 .94 56 .94 56 .04 64 .98 65 .20 73 .08 26 .38 39 .56 26 .80\n9 223 (4,096 repeats) 76.34 32 .68 89 .45 89 .84 86 .03 83 .49 83 .42 87 .18 90 .61 77 .80 78 .69 85 .47 64 .62 40 .16 18 .33 37 .66 70 .92 80 .20 59 .29 69 .85 73 .48 73 .21 56 .00 57 .66 14 .38 46 .69 45 .79 59 .57 65 .05 68 .27 26 .37 38 .84 25 .81\n10 ⋆All parameters 83.28 53 .84 92 .68 92 .07 88 .92 88 .02 87 .94 88 .67 91 .56 84 .24 84 .57 90 .48 76 .28 41 .33 19 .24 38 .77 80 .88 88 .81 71 .36 76 .62 91 .22 91 .96 66 .20 66 .13 25 .78 69 .05 68 .16 75 .34 68 .04 78 .56 26 .98 39 .82 27 .65\n10 Adapter layers, d = 32 80 .52 45 .33 91 .63 90 .59 86 .76 88 .38 88 .06 86 .99 90 .26 83 .63 83 .94 90 .72 67 .15 34 .50 15 .08 32 .15 79 .32 87 .70 60 .40 65 .32 50 .87 73 .21 52 .00 58 .61 19 .41 65 .50 64 .58 62 .09 64 .58 73 .08 13 .84 17 .88 15 .54\n10 Adapter layers, d = 128 81 .51 45 .35 92 .89 91 .49 88 .24 87 .73 87 .65 87 .73 90 .93 83 .64 84 .09 90 .52 72 .56 36 .71 16 .62 34 .37 79 .47 87 .61 63 .03 69 .20 52 .21 75 .00 56 .00 61 .08 18 .05 67 .94 66 .97 68 .59 66 .77 73 .08 19 .83 27 .50 22 .63\n10 Adapter layers, d = 512 81 .54 44 .25 93 .35 91 .00 87 .25 88 .74 88 .44 88 .02 91 .15 83 .08 83 .80 89 .62 74 .37 38 .63 17 .78 36 .25 79 .18 87 .32 64 .30 73 .18 59 .86 71 .43 56 .00 62 .94 18 .57 66 .56 65 .74 70 .76 67 .87 74 .04 23 .45 33 .98 25 .81\n10 Adapter layers, d = 2048 82 .62 49 .86 92 .55 91 .30 87 .99 88 .46 88 .35 88 .36 91 .40 83 .63 83 .18 90 .66 76 .53 39 .44 18 .30 37 .06 79 .40 87 .36 68 .61 74 .53 88 .00 91 .07 58 .00 61 .10 18 .89 66 .73 66 .06 73 .29 71 .16 75 .96 25 .64 36 .92 26 .93\n10 Gradual Unfreezing 82.50 51 .74 91 .97 92 .61 89 .71 87 .27 86 .90 88 .26 91 .35 83 .42 83 .49 89 .71 75 .09 40 .88 18 .95 38 .40 79 .17 87 .30 70 .79 75 .51 93 .09 94 .64 70 .00 62 .03 21 .51 65 .69 64 .79 72 .92 69 .12 77 .89 26 .71 39 .02 26 .93\n11 ⋆Baseline (pre-train/fine-tune) 83.28 53 .84 92 .68 92 .07 88 .92 88 .02 87 .94 88 .67 91 .56 84 .24 84 .57 90 .48 76 .28 41 .33 19 .24 38 .77 80 .88 88 .81 71 .36 76 .62 91 .22 91 .96 66 .20 66 .13 25 .78 69 .05 68 .16 75 .34 68 .04 78 .56 26 .98 39 .82 27 .65\n11 Equal 76.13 39 .47 90 .94 82 .90 75 .74 78 .83 78 .44 86 .45 89 .71 82 .08 82 .92 90 .13 59 .93 40 .95 19 .02 38 .39 76 .51 85 .61 63 .37 73 .06 82 .37 83 .93 65 .00 60 .89 17 .52 60 .51 59 .70 61 .01 60 .03 65 .38 23 .89 34 .31 26 .78\n11 Examples-proportional, K = 216 80.45 42 .07 91 .97 90 .97 87 .50 85 .41 85 .04 86 .89 90 .10 83 .01 83 .66 90 .74 72 .56 41 .16 19 .04 38 .59 77 .25 85 .72 69 .95 76 .67 86 .38 89 .29 70 .00 65 .93 27 .91 62 .78 61 .95 76 .90 65 .83 73 .08 24 .35 34 .99 27 .10\n11 Examples-proportional, K = 217 81.56 47 .35 91 .40 91 .55 88 .24 86 .15 85 .93 86 .94 90 .06 82 .76 84 .12 90 .79 75 .09 41 .06 19 .12 38 .47 77 .00 85 .87 67 .91 77 .89 77 .54 85 .71 57 .00 67 .78 27 .07 61 .51 60 .54 79 .06 65 .20 74 .04 24 .36 35 .00 27 .25\n11 Examples-proportional, K = 218 81.67 46 .85 91 .63 91 .99 88 .73 87 .68 87 .20 86 .93 90 .35 83 .30 84 .01 91 .47 73 .29 40 .96 19 .07 38 .43 78 .17 86 .74 67 .94 76 .57 78 .88 87 .50 62 .00 67 .70 30 .85 63 .43 62 .54 76 .53 65 .67 67 .31 24 .57 35 .19 27 .39\n11 Examples-proportional, K = 219 81.42 45 .94 91 .63 92 .20 89 .22 88 .44 88 .32 86 .84 90 .10 83 .73 84 .29 91 .84 70 .40 41 .26 19 .24 38 .71 79 .78 88 .15 67 .30 75 .66 75 .59 87 .50 59 .00 68 .22 30 .64 65 .32 64 .29 73 .65 65 .05 69 .23 25 .21 36 .30 27 .76\n11 Examples-proportional, K = 220 80.80 42 .55 92 .78 91 .27 87 .99 88 .36 88 .10 86 .10 89 .62 84 .15 84 .26 92 .20 68 .95 41 .05 19 .24 38 .46 80 .36 88 .27 67 .38 73 .21 76 .18 83 .93 62 .00 67 .57 26 .86 66 .12 65 .22 76 .90 64 .73 69 .23 25 .66 36 .93 27 .68\n11 Examples-proportional, K = 221 79.83 44 .45 91 .28 89 .00 84 .31 87 .54 87 .40 84 .93 88 .53 82 .54 84 .16 90 .85 67 .87 40 .51 18 .79 37 .92 79 .50 87 .48 65 .10 71 .16 68 .88 85 .71 57 .00 62 .75 23 .40 64 .50 63 .65 72 .92 64 .11 71 .15 25 .82 37 .22 27 .13\n11 Temperature-scaled, T = 2 81 .90 54 .00 91 .74 90 .56 86 .76 85 .11 84 .60 86 .40 89 .74 83 .47 84 .15 91 .51 72 .56 41 .09 19 .28 38 .54 79 .42 87 .77 69 .92 76 .73 92 .37 92 .86 57 .00 69 .80 31 .90 66 .65 65 .74 72 .92 67 .08 75 .96 25 .42 36 .72 27 .20\n11 Temperature-scaled, T = 4 80 .56 45 .38 91 .97 89 .68 85 .78 83 .13 82 .76 86 .39 90 .00 82 .78 84 .19 91 .16 73 .65 41 .09 19 .22 38 .51 77 .99 86 .81 69 .54 76 .76 97 .36 96 .43 59 .00 68 .10 31 .48 64 .26 63 .27 74 .73 64 .26 71 .15 25 .04 35 .82 27 .45\n11 Temperature-scaled, T = 8 77 .21 40 .07 91 .06 88 .11 83 .33 79 .20 79 .06 86 .60 89 .90 83 .05 83 .56 90 .21 59 .93 41 .01 19 .10 38 .40 77 .14 85 .99 66 .07 73 .94 93 .70 94 .64 60 .00 66 .36 26 .86 63 .46 62 .60 62 .09 63 .32 65 .38 24 .55 35 .35 27 .17\n12 ⋆Unsupervised pre-training + fine-tuning 83.28 53 .84 92 .68 92 .07 88 .92 88 .02 87 .94 88 .67 91 .56 84 .24 84 .57 90 .48 76 .28 41 .33 19 .24 38 .77 80 .88 88 .81 71 .36 76 .62 91 .22 91 .96 66 .20 66 .13 25 .78 69 .05 68 .16 75 .34 68 .04 78 .56 26 .98 39 .82 27 .65\n12 Multi-task training 81.42 45 .94 91 .63 92 .20 89 .22 88 .44 88 .32 86 .84 90 .10 83 .73 84 .29 91 .84 70 .40 41 .26 19 .24 38 .71 79 .78 88 .15 67 .30 75 .66 75 .59 87 .50 59 .00 68 .22 30 .64 65 .32 64 .29 73 .65 65 .05 69 .23 25 .21 36 .30 27 .76\n12 Multi-task pre-training + fine-tuning 83.11 51 .42 92 .66 91 .73 88 .73 88 .06 87 .70 88 .61 91 .61 84 .09 84 .31 91 .85 76 .53 41 .15 19 .12 38 .59 80 .26 88 .50 71 .03 79 .54 81 .69 87 .50 65 .00 70 .72 31 .48 65 .94 65 .03 81 .23 68 .18 73 .08 27 .08 39 .80 28 .07\n12 Leave-one-out multi-task training 81.98 48 .00 93 .23 91 .72 88 .24 87 .76 87 .32 88 .61 91 .44 84 .00 84 .11 90 .79 72 .20 41 .34 19 .05 38 .77 79 .97 88 .10 71 .68 78 .35 86 .76 89 .29 66 .00 68 .09 29 .49 66 .23 65 .27 79 .06 68 .65 78 .85 26 .93 39 .79 27 .87\n12 Supervised multi-task pre-training 79.93 36 .60 92 .43 91 .58 88 .24 87 .03 86 .78 88 .15 91 .20 82 .87 83 .16 90 .13 70 .76 41 .12 18 .96 38 .49 77 .38 85 .65 65 .36 75 .66 68 .87 83 .93 58 .00 64 .81 21 .93 55 .37 54 .61 71 .12 67 .40 75 .96 26 .81 40 .13 28 .04\n13 ⋆Baseline 83.28 53 .84 92 .68 92 .07 88 .92 88 .02 87 .94 88 .67 91 .56 84 .24 84 .57 90 .48 76 .28 41 .33 19 .24 38 .77 80 .88 88 .81 71 .36 76 .62 91 .22 91 .96 66 .20 66 .13 25 .78 69 .05 68 .16 75 .34 68 .04 78 .56 26 .98 39 .82 27 .65\n13 1× size, 4× training steps 85.33 60 .29 93 .81 94 .06 91 .67 89 .42 89 .25 89 .15 91 .87 86 .01 85 .70 91 .63 78 .34 41 .52 19 .33 38 .96 82 .45 90 .19 74 .72 79 .17 94 .75 92 .86 71 .00 67 .34 29 .70 72 .63 71 .59 78 .34 72 .10 82 .69 27 .08 40 .66 27 .93\n13 1× size, 4× batch size 84.60 56 .08 93 .12 92 .31 89 .22 88 .85 88 .84 89 .35 92 .07 85 .98 86 .13 91 .07 80 .14 41 .70 19 .42 39 .08 82 .52 90 .21 74 .64 78 .78 93 .69 94 .64 72 .00 68 .09 30 .95 74 .73 73 .90 76 .53 70 .06 81 .73 27 .07 40 .60 27 .84\n13 2× size, 2× training steps 86.18 62 .04 93 .69 93 .36 90 .69 89 .18 89 .23 89 .35 92 .05 87 .23 87 .05 92 .68 81 .95 41 .74 19 .66 39 .14 84 .18 91 .29 77 .18 80 .98 97 .36 96 .43 74 .00 71 .34 35 .68 77 .11 76 .34 80 .51 69 .28 85 .58 27 .52 41 .03 28 .19\n13 4× size, 1× training steps 85.91 57 .58 94 .38 92 .67 89 .95 89 .60 89 .60 89 .44 92 .14 87 .05 87 .12 93 .12 83 .39 41 .60 19 .73 39 .08 83 .86 91 .32 78 .04 81 .38 89 .09 94 .64 73 .00 73 .74 40 .40 78 .25 77 .40 81 .59 70 .22 91 .35 27 .47 40 .71 28 .10\n13 4× ensembled 84.77 56 .14 93 .46 93 .31 90 .67 89 .71 89 .60 89 .62 92 .24 86 .22 86 .53 91 .60 77 .98 42 .10 20 .10 39 .56 83 .09 90 .40 71 .74 77 .58 89 .85 91 .07 66 .00 69 .32 29 .49 72 .67 71 .94 76 .90 69 .12 72 .12 28 .05 40 .53 28 .09\n13 4× ensembled, fine-tune only 84.05 54 .78 92 .78 93 .15 90 .44 88 .34 88 .12 89 .27 91 .97 85 .33 85 .88 90 .98 77 .62 41 .66 19 .57 39 .12 82 .36 89 .86 71 .56 77 .43 90 .07 92 .86 69 .00 67 .31 26 .34 70 .47 69 .64 75 .45 68 .18 74 .04 27 .55 40 .22 28 .09\nTable 16:Score achieved on every task we consider for all of the experiments in this paper. In the first column, we list the table where the condensed results were presented for a given experiment. As in the main text, a row marked with⋆ denotes our baseline model (described in Section 3.1).\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level\nlanguage modeling with deeper self-attention. InProceedings of the AAAI Conference on\nArtificial Intelligence, 2019.\nRohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory-efficient adaptive\noptimization for large-scale learning.arXiv preprint arXiv:1901.11150, 2019.\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim\nKrikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multi-\nlingual neural machine translation in the wild: Findings and challenges.arXiv preprint\narXiv:1907.05019, 2019.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization.arXiv\npreprint arXiv:1607.06450, 2016.\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-\ndriven pretraining of self-attention networks.arXiv preprint arXiv:1903.07785, 2019.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. InThird International Conference on Learning\nRepresentations, 2015.\nAnkur Bapna, Naveen Arivazhagan, and Orhan Firat. Simple, scalable adaptation for neural\nmachine translation.arXiv preprint arXiv:1909.08478, 2019.\nIz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific\ntext. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019.\nOndřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Jo-\nhannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al.\nFindings of the 2014 workshop on statistical machine translation. InProceedings of the\nNinth Workshop on Statistical Machine Translation, 2014.\nOndřej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck,\nChris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, et al.\nFindings of the 2015 workshop on statistical machine translation. InProceedings of the\nTenth Workshop on Statistical Machine Translation, 2015.\nOndřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow,\nMatthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz,\net al. Findings of the 2016 conference on machine translation. InProceedings of the First\nConference on Machine Translation, 2016.\nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, and Samy\nBengio. Generating sentences from a continuous space.arXiv preprint arXiv:1511.06349,\n2015.\n58\nExploring the Limits of Transfer Learning\nChristian Buck, Kenneth Heafield, and Bas Van Ooyen. N-gram counts and language models\nfrom the common crawl. InLREC, 2014.\nRich Caruana. Multitask learning.Machine learning, 28(1), 1997.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation.\narXiv preprint arXiv:1708.00055, 2017.\nJianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for\nmachine reading.arXiv preprint arXiv:1601.06733, 2016.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions.\narXiv preprint arXiv:1905.10044, 2019.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra:\nPre-training text encoders as discriminators rather than generators. arXiv preprint\narXiv:2003.10555, 2020.\nAlexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence\nrepresentations. arXiv preprint arXiv:1803.05449, 2018.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Super-\nvised learning of universal sentence representations from natural language inference data.\narXiv preprint arXiv:1705.02364, 2017.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual\nentailment challenge. InMachine Learning Challenges Workshop, 2005.\nAndrew M. Dai and Quoc V. Le. Semi-supervised sequence learning. InAdvances in neural\ninformation processing systems, 2015.\nMarie-Catherine De Marneff, Mandy Simons, and Judith Tonhauser. The CommitmentBank:\nInvestigating projection in naturally occurring discourse. InSinn und Bedeutung 23, 2019.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A\nlarge-scale hierarchical image database. In2009 IEEE conference on computer vision and\npattern recognition, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-\ntraining of deep bidirectional transformers for language understanding.arXiv preprint\narXiv:1810.04805, 2018.\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential para-\nphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005),\n2005.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language\nunderstanding and generation.arXiv preprint arXiv:1905.03197, 2019.\n59\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nSergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation\nat scale. arXiv preprint arXiv:1808.09381, 2018.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov.\nLearning word vectors for 157 languages.arXiv preprint arXiv:1802.06893, 2018.\nAlex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\nIvan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: Multilingual web-size corpus\nwith free license. In Proceedings of the Tenth International Conference on Language\nResources and Evaluation (LREC’16), pages 914–922, 2016.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\nimage recognition. InProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016.\nKaiming He, Ross Girshick, and Piotr Dollár. Rethinking ImageNet pre-training.arXiv\npreprint arXiv:1811.08883, 2018.\nPengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network\nmodel for commonsense reasoning.arXiv preprint arXiv:1907.11983, 2019.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\nMustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In\nAdvances in neural information processing systems, 2015.\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan\nKianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling\nis predictable, empirically.arXiv preprint arXiv:1712.00409, 2017.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of\nsentences from unlabelled data.arXiv preprint arXiv:1602.03483, 2016.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531, 2015.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer\nlearning for NLP.arXiv preprint arXiv:1902.00751, 2019.\nJeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classifi-\ncation. arXiv preprint arXiv:1801.06146, 2018.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne,\nNoam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Dou-\nglas Eck. Music transformer: Generating music with long-term structure. InSeventh\nInternational Conference on Learning Representations, 2018a.\n60\nExploring the Limits of Transfer Learning\nYanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V\nLe, and Zhifeng Chen. GPipe: Efficient training of giant neural networks using pipeline\nparallelism. arXiv preprint arXiv:1811.06965, 2018b.\nMinyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes ImageNet good for\ntransfer learning? arXiv preprint arXiv:1608.08614, 2016.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai. First Quora dataset release: Question\npairs. https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs ,\n2017.\nYangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,\nSergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature\nembedding. In Proceedings of the 22nd ACM international conference on Multimedia,\n2014.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and\nQun Liu. TinyBERT: Distilling BERT for natural language understanding.arXiv preprint\narXiv:1909.10351, 2019.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large\nscale distantly supervised challenge dataset for reading comprehension.arXiv preprint\narXiv:1705.03551, 2017.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy.\nSpanBERT: Improving pre-training by representing and predicting spans.arXiv preprint\narXiv:1907.10529, 2019.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling.arXiv preprint arXiv:1602.02410, 2016.\nNal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network\nfor modelling sentences. InProceedings of the 52nd Annual Meeting of the Association for\nComputational Linguistics, 2014.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard\nSocher. CTRL: A conditional transformer language model for controllable generation.\narXiv preprint arXiv:1909.05858, 2019a.\nNitishShirishKeskar, BryanMcCann, CaimingXiong, andRichardSocher. Unifyingquestion\nanswering and text classification via span extraction.arXiv preprint arXiv:1904.09286,\n2019b.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.\nLooking beyond the surface: A challenge set for reading comprehension over multiple\nsentences. InProceedings of North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n61\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nRyan Kiros, Yukun Zhu, Ruslan R. Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio\nTorralba, and Sanja Fidler. Skip-thought vectors. InAdvances in neural information\nprocessing systems, 2015.\nVid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas\nLukasiewicz. A surprisingly robust trick for Winograd schema challenge.arXiv preprint\narXiv:1905.06290, 2019.\nJakub Konečn` y, Brendan McMahan, and Daniel Ramage. Federated optimization: Dis-\ntributed optimization beyond the datacenter.arXiv preprint arXiv:1511.03575, 2015.\nJakub Konečn` y, H. Brendan McMahan, Felix X. Yu, Peter Richtárik, Ananda Theertha\nSuresh, and Dave Bacon. Federated learning: Strategies for improving communication\nefficiency. arXiv preprint arXiv:1610.05492, 2016.\nSimon Kornblith, Jonathon Shlens, and Quoc V. Le. Do better ImageNet models transfer\nbetter? arXiv preprint arXiv:1805.08974, 2018.\nAlex Krizhevsky. One weird trick for parallelizing convolutional neural networks.arXiv\npreprint arXiv:1404.5997, 2014.\nTaku Kudo. Subword regularization: Improving neural network translation models with\nmultiple subword candidates.arXiv preprint arXiv:1804.10959, 2018.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing.arXiv preprint arXiv:1808.06226,\n2018.\nGuillaume Lample and Alexis Conneau. Cross-lingual language model pretraining.arXiv\npreprint arXiv:1901.07291, 2019.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and\nRadu Soricut. ALBERT: A lite BERT for self-supervised learning of language representa-\ntions. arXiv preprint arXiv:1909.11942, 2019.\nHector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge.\nIn Thirteenth International Conference on the Principles of Knowledge Representation\nand Reasoning, 2012.\nQi Li. Literature survey: domain adaptation algorithms for natural language processing.\n2012.\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text\nsummarization branches out, 2004.\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\nand Noam Shazeer. Generating Wikipedia by summarizing long sequences.arXiv preprint\narXiv:1801.10198, 2018.\nPeter J. Liu, Yu-An Chung, and Jie Ren. SummAE: Zero-shot abstractive text summarization\nusing length-agnostic auto-encoders.arXiv preprint arXiv:1910.00998, 2019a.\n62\nExploring the Limits of Transfer Learning\nXiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Rep-\nresentation learning using multi-task deep neural networks for semantic classification\nand information retrieval. InProceedings of the 2015 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\n2015.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural\nnetworks for natural language understanding.arXiv preprint arXiv:1901.11504, 2019b.\nYang Liu. Fine-tune BERT for extractive summarization.arXiv preprint arXiv:1903.10318,\n2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized\nBERT pretraining approach.arXiv preprint arXiv:1907.11692, 2019c.\nLajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence\nrepresentations. arXiv preprint arXiv:1803.02893, 2018.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan\nLi, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly\nsupervised pretraining. InProceedings of the European Conference on Computer Vision\n(ECCV), 2018.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The nat-\nural language decathlon: Multitask learning as question answering. arXiv preprint\narXiv:1806.08730, 2018.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word\nrepresentations in vector space.arXiv preprint arXiv:1301.3781, 2013a.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed\nrepresentations of words and phrases and their compositionality. InAdvances in neural\ninformation processing systems, 2013b.\nRamesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, and Bing\nXiang. Abstractive text summarization using sequence-to-sequence RNNs and beyond.\narXiv preprint arXiv:1602.06023, 2016.\nMaxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring\nmid-level image representations using convolutional neural networks. InProceedings of\nthe IEEE conference on computer vision and pattern recognition, 2014.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for\nautomatic evaluation of machine translation. InProceedings of the 40th annual meeting on\nassociation for computational linguistics. Association for Computational Linguistics, 2002.\nRomain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n63\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nJeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors\nfor word representation. InProceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), 2014.\nMatthew Peters, Sebastian Ruder, and Noah A. Smith. To tune or not to tune? adapting\npretrained representations to diverse tasks.arXiv preprint arXiv:1903.05987, 2019.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\nLee, and Luke Zettlemoyer. Deep contextualized word representations.arXiv preprint\narXiv:1802.05365, 2018.\nJason Phang, Thibault Févry, and Samuel R. Bowman. Sentence encoders on STILTs: Sup-\nplementary training on intermediate labeled-data tasks.arXiv preprint arXiv:1811.01088,\n2018.\nMohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 10,000 example pairs for\nevaluating context-sensitive representations.arXiv preprint arXiv:1808.09121, 2018.\nMatt Post. A call for clarity in reporting BLEU scores.arXiv preprint arXiv:1804.08771,\n2018.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners, 2019.\nAltaf Rahman and Vincent Ng. Resolving complex cases of definite pronouns: the Winograd\nschema challenge. InProceedings of the 2012 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natural Language Learning. Association\nfor Computational Linguistics, 2012.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+\nquestions for machine comprehension of text.arXiv preprint arXiv:1606.05250, 2016.\nPrajit Ramachandran, Peter J. Liu, and Quoc V. Le. Unsupervised pretraining for sequence\nto sequence learning.arXiv preprint arXiv:1611.02683, 2016.\nAlex Ratner, Braden Hancock, Jared Dunnmon, Roger Goldman, and Christopher Ré.\nSnorkel MeTaL: Weak supervision for multi-task learning. InProceedings of the Second\nWorkshop on Data Management for End-To-End Machine Learning, 2018.\nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible\nalternatives: An evaluation of commonsense causal reasoning. In2011 AAAI Spring\nSymposium Series, 2011.\nSebastian Ruder. An overview of multi-task learning in deep neural networks.arXiv preprint\narXiv:1706.05098, 2017.\nSebastian Ruder. Neural transfer learning for natural language processing. PhD thesis, NUI\nGalway, 2019.\n64\nExploring the Limits of Transfer Learning\nSebastian Ruder, Matthew E. Peters, Swabha Swayamdipta, and Thomas Wolf. Transfer\nlearning in natural language processing. InProceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Tutorials,\npages 15–18, 2019.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale\nvisual recognition challenge.International journal of computer vision, 2015.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled\nversion of BERT: smaller, faster, cheaper and lighter.arXiv preprint arXiv:1910.01108,\n2019.\nAbigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization\nwith pointer-generator networks.arXiv preprint arXiv:1704.04368, 2017.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare\nwords with subword units.arXiv preprint arXiv:1508.07909, 2015.\nChristopher J Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and\nGeorge E. Dahl. Measuring the effects of data parallelism on neural network training.\narXiv preprint arXiv:1811.03600, 2018.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position\nrepresentations. arXiv preprint arXiv:1803.02155, 2018.\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\ncost. arXiv preprint arXiv:1804.04235, 2018.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn\nKoanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan\nSepassi, and Blake Hechtman. Mesh-tensorflow: Deep learning for supercomputers. In\nAdvances in Neural Information Processing Systems, 2018.\nJason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-\nBurch, and Adam Lopez. Dirt cheap web-scale parallel text from the common crawl. In\nProceedings of the 51st Annual Meeting of the Association for Computational Linguistics,\n2013.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a\nsentiment treebank. InProceedings of the 2013 conference on empirical methods in natural\nlanguage processing, 2013.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to\nsequence pre-training for language generation.arXiv preprint arXiv:1905.02450, 2019.\n65\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting.The Journal of\nMachine Learning Research, 2014.\nSandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J. Pal. Learning\ngeneral purpose distributed sentence representations via large scale multi-task learning.\narXiv preprint arXiv:1804.00079, 2018.\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural\nnetworks. InAdvances in neural information processing systems, 2014.\nRichard S. Sutton. The bitter lesson. http://www.incompleteideas.net/IncIdeas/\nBitterLesson.html, 2019.\nWilson L. Taylor. “Cloze procedure”: A new tool for measuring readability.Journalism\nBulletin, 1953.\nTrieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning.arXiv preprint\narXiv:1806.02847, 2018.\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip\nBachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset.arXiv\npreprint arXiv:1611.09830, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural\ninformation processing systems, 2017.\nElena Voita, Rico Sennrich, and Ivan Titov. The bottom-up evolution of representations\nin the transformer: A study with machine translation and language modeling objectives.\narXiv preprint arXiv:1909.01380, 2019.\nAlex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\narXiv preprint arXiv:1804.07461, 2018.\nAlex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma\nPatel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, et al. Can you tell me\nhow to get past Sesame Street? Sentence-level pretraining beyond language modeling. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\n2019a.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for general-\npurpose language understanding systems.arXiv preprint arXiv:1905.00537, 2019b.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, and Luo Si. StructBERT:\nIncorporating language structures into pre-training for deep language understanding.\narXiv preprint arXiv:1908.04577, 2019c.\n66\nExploring the Limits of Transfer Learning\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability\njudgments. arXiv preprint arXiv:1805.12471, 2018.\nAdina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus\nfor sentence understanding through inference.arXiv preprint arXiv:1704.05426, 2017.\nRonald J. Williams and David Zipser. A learning algorithm for continually running fully\nrecurrent neural networks.Neural computation, 1989.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural\nmachine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144, 2016.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V.\nLe. XLNet: Generalized autoregressive pretraining for language understanding.arXiv\npreprint arXiv:1906.08237, 2019.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features\nin deep neural networks? InAdvances in neural information processing systems, 2014.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad\nNorouzi, and Quoc V. Le. QAnet: Combining local convolution with global self-attention\nfor reading comprehension.arXiv preprint arXiv:1804.09541, 2018.\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roes-\nner, and Yejin Choi. Defending against neural fake news.arXiv preprint arXiv:1905.12616,\n2019.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin\nVan Durme. ReCoRD: Bridging the gap between human and machine commonsense\nreading comprehension. arXiv preprint arXiv:1810.12885, 2018.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Thomas Goldstein, and Jingjing Liu. Freelb: En-\nhanced adversarial training for language understanding.arXiv preprint arXiv:1909.11764,\n2019.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio\nTorralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual expla-\nnations by watching movies and reading books. InProceedings of the IEEE international\nconference on computer vision, 2015.\n67",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8534925580024719
    },
    {
      "name": "Computer science",
      "score": 0.821079671382904
    },
    {
      "name": "Transfer of learning",
      "score": 0.7402752041816711
    },
    {
      "name": "Natural language processing",
      "score": 0.6627885699272156
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6433743834495544
    },
    {
      "name": "Transformer",
      "score": 0.6014506816864014
    },
    {
      "name": "Question answering",
      "score": 0.5769933462142944
    },
    {
      "name": "Task (project management)",
      "score": 0.4889870882034302
    },
    {
      "name": "Natural language understanding",
      "score": 0.4631468951702118
    },
    {
      "name": "Language model",
      "score": 0.4630351960659027
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4478144645690918
    },
    {
      "name": "Training set",
      "score": 0.4217499792575836
    },
    {
      "name": "Natural language",
      "score": 0.3951631188392639
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 3691
}