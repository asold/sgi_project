{
  "title": "HDFormer: High-order Directed Transformer for 3D Human Pose Estimation",
  "url": "https://openalex.org/W4385767582",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2585568996",
      "name": "Hanyuan Chen",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2248337351",
      "name": "Jun-Yan He",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2798463504",
      "name": "Wangmeng Xiang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4304081856",
      "name": "Zhi-Qi Cheng",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A1973321923",
      "name": "Wei Liu",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2120400312",
      "name": "Hanbing Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1897219996",
      "name": "Bin Luo",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2097758085",
      "name": "Geng Yifeng",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2970811884",
      "name": "Xuansong Xie",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2972662547",
    "https://openalex.org/W6863994431",
    "https://openalex.org/W2769331938",
    "https://openalex.org/W3124503747",
    "https://openalex.org/W4292433237",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W3157895073",
    "https://openalex.org/W3107320732",
    "https://openalex.org/W3186796015",
    "https://openalex.org/W3034854791",
    "https://openalex.org/W2945071622",
    "https://openalex.org/W6803771590",
    "https://openalex.org/W3034217102",
    "https://openalex.org/W3185478863",
    "https://openalex.org/W6767176642",
    "https://openalex.org/W3034448411",
    "https://openalex.org/W3103977930",
    "https://openalex.org/W2903549000",
    "https://openalex.org/W4304080525",
    "https://openalex.org/W2968510372",
    "https://openalex.org/W2948058585",
    "https://openalex.org/W2948246283",
    "https://openalex.org/W6759363029",
    "https://openalex.org/W4366736206",
    "https://openalex.org/W6864014924",
    "https://openalex.org/W3021645648",
    "https://openalex.org/W4287062814",
    "https://openalex.org/W3034581612",
    "https://openalex.org/W2983796203",
    "https://openalex.org/W4288043310",
    "https://openalex.org/W3195639294",
    "https://openalex.org/W6791858558",
    "https://openalex.org/W4221154956",
    "https://openalex.org/W6755811877",
    "https://openalex.org/W2605498834",
    "https://openalex.org/W2964318832",
    "https://openalex.org/W4313068951",
    "https://openalex.org/W6868564194",
    "https://openalex.org/W3136525061",
    "https://openalex.org/W4309396530",
    "https://openalex.org/W3188906027",
    "https://openalex.org/W3202716970",
    "https://openalex.org/W3035416506",
    "https://openalex.org/W3098612954",
    "https://openalex.org/W3009246422",
    "https://openalex.org/W3022928074",
    "https://openalex.org/W4287265181",
    "https://openalex.org/W4312417903",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W2964221239",
    "https://openalex.org/W3106838237",
    "https://openalex.org/W3126541466",
    "https://openalex.org/W2797184202",
    "https://openalex.org/W3169891778",
    "https://openalex.org/W3205327953",
    "https://openalex.org/W4385767754",
    "https://openalex.org/W2962896489",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4288088610",
    "https://openalex.org/W4225557002",
    "https://openalex.org/W2963097270",
    "https://openalex.org/W2984313141",
    "https://openalex.org/W4313164251",
    "https://openalex.org/W4290856675",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W4377371545"
  ],
  "abstract": "Human pose estimation is a challenging task due to its structured data sequence nature. Existing methods primarily focus on pair-wise interaction of body joints, which is insufficient for scenarios involving overlapping joints and rapidly changing poses. To overcome these issues, we introduce a novel approach, the High-order Directed Transformer (HDFormer), which leverages high-order bone and joint relationships for improved pose estimation. Specifically, HDFormer incorporates both self-attention and high-order attention to formulate a multi-order attention module. This module facilitates first-order \"joint-joint\", second-order \"bone-joint\", and high-order \"hyperbone-joint\" interactions, effectively addressing issues in complex and occlusion-heavy situations. In addition, modern CNN techniques are integrated into the transformer-based architecture, balancing the trade-off between performance and efficiency. HDFormer significantly outperforms state-of-the-art (SOTA) models on Human3.6M and MPI-INF-3DHP datasets, requiring only 1/10 of the parameters and significantly lower computational costs. Moreover, HDFormer demonstrates broad real-world applicability, enabling real-time, accurate 3D pose estimation. The source code is in https://github.com/hyer/HDFormer.",
  "full_text": "HDFormer: High-order Directed Transformer for 3D Human Pose Estimation\nHanyuan Chen1 , Jun-Yan He1 , Wangmeng Xiang1 , Zhi-Qi Cheng2 ,\nWei Liu1 , Hanbing Liu3 , Bin Luo1 , Yifeng Geng1 and Xuansong Xie1\n1DAMO Academy, Alibaba Group\n2Carnegie Mellon University, Pittsburgh, PA, USA\n3Tsinghua University, Beijing, China\nhanyuan.chy, leyuan.hjy, wangmeng.xwm, luwu.lb, cangyu.gyf@alibaba-inc.com\nzhiqic@cs.cmu.edu, ustclwwx@gmail.com, liuhb21@mails.tsinghua.edu.cn, xingtong.xxs@taobao.com\nAbstract\nHuman pose estimation is a challenging task due\nto its structured data sequence nature. Existing\nmethods primarily focus on pair-wise interaction\nof body joints, which is insufficient for scenarios\ninvolving overlapping joints and rapidly changing\nposes. To overcome these issues, we introduce a\nnovel approach, the High-order Directed Trans-\nformer (HDFormer), which leverages high-order\nbone and joint relationships for improved pose esti-\nmation. Specifically, HDFormer incorporates both\nself-attention and high-order attention to formu-\nlate a multi-order attention module. This module\nfacilitates first-order ‚Äùjoint‚Üîjoint‚Äù, second-order\n‚Äùbone‚Üîjoint‚Äù, and high-order ‚Äùhyperbone‚Üîjoint‚Äù\ninteractions, effectively addressing issues in com-\nplex and occlusion-heavy situations. In addi-\ntion, modern CNN techniques are integrated into\nthe transformer-based architecture, balancing the\ntrade-off between performance and efficiency. HD-\nFormer significantly outperforms state-of-the-art\n(SOTA) models on Human3.6M and MPI-INF-\n3DHP datasets, requiring only 1/10 of the pa-\nrameters and significantly lower computational\ncosts. Moreover, HDFormer demonstrates broad\nreal-world applicability, enabling real-time, accu-\nrate 3D pose estimation. The source code is at\nhttps://github.com/hyer/HDFormer.\n1 Introduction\nDespite significant strides in deep learning-based 3D pose es-\ntimation [Iskakov et al., 2019; Qiu et al., 2019; Pavllo et al.,\n2018; Li et al., 2020; Zhu et al., 2021; Gong et al., 2021;\nYe et al., 2022 ], achieving stable, accurate pose sequences\nremains elusive. The prevalent 3D pose estimation frame-\nwork takes 2D pose detection results [Chen et al., 2017;\nSun et al., 2019 ] as inputs and estimates depth information\nvia end-to-end Graph Convolutional Networks (GCNs) [Cai\net al., 2019; Pavllo et al., 2018 ] or Transformers[Zhang et\nal., 2022 ]. However, complex scenarios involving overlap-\nping keypoints, rapid pose changes, and varying scales pose\nchallenges to the depth estimation of 3D keypoints.\n(a) joint <-> joint (b) bone <-> joint (c) hyperbone <-> joint\nFigure 1: Illustration of first-order (joint‚Üî joint) attention, second-\norder (bone‚Üî joint) and high-order (hyperbone‚Üî joint) atten-\ntion. First-order attention models connections between joints, while\nsecond-order attention focuses on the relationship between joints\nand bones. High-order attention delves into intricate relationships\nbetween joints and hyperbones.\nFace with these challenges, existing methods mainly uti-\nlize first-order ‚Äùjoint ‚Üî joint‚Äù and second-order ‚Äùbone ‚Üî\njoint‚Äù connections, often overlooking high-order interactions\namong joint sets (referred to as hyperbones). Different\nfrom first-order (‚Äújoint ‚Üî joint‚Äù) and second-order (‚Äúbone\n‚Üî joint‚Äù) that focus on pair-wise joints/bones connections,\nhigh-order relations could describe complex motion dynam-\nics. The high-order interactions contain rich semantic infor-\nmation in motions, as skeletons often move in specific pat-\nterns and involve multiple joints and bones simultaneously.\nTo Learn high-order information without expensive costs,\nwe propose a novel framework named High-order Directed\nTransformer (HDFormer), which coherently exploits the\nmulti-order information aggregation of skeleton structure for\n3D pose estimation. Specifically, HDFormer leverages the\nfirst-order attention to learn the spatial semantics among\n‚Äújoint ‚Üî joint‚Äù relationships. Additionally, it integrates a ro-\nbust high-order attention module to enhance 3D pose estima-\ntion accuracy by capturing both second-order and high-order\ninformation. To encode the hyperbone features, the hyper-\nbone representation encoding module is employed under the\nconstraints of a pre-defined directed human skeleton graph.\nHDFormer strikes a commendable balance between efficacy\nand efficiency. In summary, the key contributions of this pa-\nper are summarized as follows:\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n581\n‚Ä¢ We investigate high-order attention module to learn both\nthe ‚Äúbone‚Üîjoint‚Äù and ‚Äúhyperbone‚Üîjoint‚Äù with an effec-\ntive and efficient cross-attention mechanism. To the best\nof our knowledge, it is the first end-to-end model to utilize\nhigh-order information on a directed skeleton graph for 3D\npose estimation.\n‚Ä¢ We propose a novel High-order Directed Transformer (HD-\nFormer) for 3D pose estimation. It utilizes ‚Äújoint‚Üîjoint‚Äù,\n‚Äúbone‚Üîjoint‚Äù and ‚Äúhyperbone‚Üîjoint‚Äù information with a\nthree-stage U-shape architecture design, which endows the\nnetwork with the ability to handle more complex scenarios.\n‚Ä¢ HDFormer is evaluated on popular 3D pose estimation\nbenchmarks Human3.6M and MPI-INF-3DHP with anal-\nysis of quantitative and qualitative results. Specifically, it\nachieves 21.6% (96 frames) on Human3.6M without using\nany extra data, which outperforms the existing SOTA work\nMixSTE [Zhang et al., 2022 ] with only 1/10 parameters\nand a fraction of computational cost.\n2 Related Work\n2.1 3D Human Pose Estimation\nDespite progress in 2D human pose estimation, 3D ver-\nsions face challenges due to depth ambiguity. While some\nmethods leverage multi-view images/videos [Iskakov et al.,\n2019; Qiu et al., 2019; Ye et al., 2022; He et al., 2021;\nZhao et al., 2018; Huang et al., 2021; Qiao et al., 2022;\nHe et al., 2023], these setups are costly and complex. Hence,\nestimating 3D poses with monocular images/videos is more\npractical, with recent studies showcasing the efficacy of\nconverting 2D joint locations to 3D [Pavllo et al., 2018;\nZhu et al., 2021]. Strategies such as data augmentation have\nbeen used to generate diverse 2D-3D pose pairs, improving\ngeneralization [Gong et al., 2021; Li et al., 2020].\n2.2 Graph ConvNet Based Methods\nGCNs [Scarselli et al., 2009; Gilmer et al., 2017 ], extend-\ning conventional convolution operators to graphs, have been\nutilized in several human-related tasks like action recogni-\ntion [Shi et al., 2019; Shi et al., 2018; Xiang et al., 2022 ],\naction synthesis [Yan et al., 2019], and 3D human pose esti-\nmation [Zhou et al., 2022; Zhou et al., 2023]. [Wang et al.,\n2020] created a lightweight, efficient U-shaped model to cap-\nture temporal dependencies. Inspired by [Shi et al., 2019 ],\n[Hu et al., 2021] proposed a conditional directed graph con-\nvolution for adaptive graph topology, enhancing non-local\ndependence. Other methods, such as Semantic Graph Con-\nvolution (SemGConv) [Zhao et al., 2019 ] and Modulated-\nGCN [Zou and Tang, 2021 ], also prioritize spatial joint re-\nlationships. However, they often overlook edge information,\nparticularly high-order relationships. Unlike Skeletal-GNN\n[Zeng et al., 2021 ] which utilizes GCN to capture action-\nspecific poses, we establish joint and hyperbone relationships\nvia cross-attention in the directed graph skeleton.\n2.3 Transformer Based Methods\nTransformers, first introduced by [Vaswani et al., 2017 ],\nhave been widely used in visual tasks [Zhou et al., 2022;\nTu et al., 2023; Cheng et al., 2022 ]. In the domain of\n3D pose estimation, [Li et al., 2021 ] proposed the Stride\nTransformer to lift a long 2D pose sequence to a single 3D\npose. PoseFormer [Zheng et al., 2021 ] created a model\ncomprising Spatial-Temporal Transformer blocks to capture\nspatial and global dependencies across frames. Similarly,\nMixSTE [Zhang et al., 2022 ] captured the temporal motion\nof body joints over long sequences. While CrossFormer\n[Wang et al., 2021 ] and GraFormer [Zhao et al., 2022 ] en-\ncoded dependencies between body joints, our method applies\na cross-attention mechanism to integrate high-order relation-\nships and explore the hyperbone-joint relationship. Although\ntransformer-based methods effectively capture global spatial-\ntemporal joint relationships, their computational costs signif-\nicantly exceed those of GCN-based methods.\n3 Proposed Method\n3.1 Preliminaries\nDirected Skeleton Graph\nThe directed skeleton graphG shown in the right part of Fig. 2\nrepresents the human skeleton structure, where the nodes are\nhuman skeleton joints and the arrows are human skeleton\nbones. Generally, the human skeleton can be represented as\na graph G = ( V; E), where the vertices are human skele-\nton joints and edges are physical connections between two\njoints. Here V is the set of N joints and E is characterized by\nthe adjacency matrix A ‚àà RN√óN . The raw pose data, i.e.,\nthe joint keypoints vector, is a set of 2D coordinates. In this\nway, the pose data is transformed into a graph sequence and\nspecifically represented as a tensor X ‚àà RT√óN√óC , where T,\nN, and C denote the temporal length, numbers of joints and\nchannels, respectively. Directed graph is adopted, as it allows\na convenient hyperbone definition.\nTransformer\nOur model‚Äôs attention mechanism is built upon the original\nimplementation of the classic Transformer [Vaswani et al.,\n2017]. The attention computing with query, key and value\nmatrix Q, K, V in each head are depicted as:\nAttn(Q, K, V) = Softmax((QKT + A + Œ®)/\np\ndm)V, (1)\nwhere Q, K, V‚àà RN√ódm , N is the number of tokens, and\ndm indicates the dimension of each token. The multi-head\nattention of S heads is defined as follows:\n‚Ñèi = Attn(Qi, Ki, Vi), i ‚àà {1, . . . ,S}, (2)\nMSA = Concat(‚Ñè1, ...,‚ÑèS)Wo, (3)\nwhere ‚Ñè is the attention calculation result for a single head,\nWo ‚àà Rdm √ódm is the linear projection weight. A is the ad-\njacency matrix and Œ® is a learnable adjacency matrix. The\nmatrix A is fixed and represents the predetermined connec-\ntions between joints, while the learnable adjacency matrix Œ®\nadjusts the connection weights based on the input data, im-\nproving the capturing of spatial relationships between differ-\nent joints. The ablation study on the impact of Œ® is presented\nin line 1 of Tab. 7.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n582\nLinear\nLinearLinearLinear\nHyperboneRepresentation\nxScalexSoftmax\nJoint FeatureHigh-order Directed Transformer\nLinearx\nSoftmax\nLinear\nLinearx\n+ A\n+\nres(1x1)\nFirst-orderAttention\nFirst-order attention block\nNormNormJoint Feature\nHyperboneRepresentation Learning\nNorm+MLP+Joint Feature\nDistanceofShortestPathÔºàSPDÔºâ\nLinearProjection\n...\n......\nNormJoint Feature\nInstantiation Function ùùì(#)\nJoint Feature Selection\nÔºöarethehyperbonefeaturesgeneratedby‚Ñã!,#\nSkeleton Directed Graph ùìñ‚Ñ¨!,#=SPD(ùí¢,ùëñ,ùëó)\n‚Ñã!,#\n‚Ä¶‚Ä¶‚Ä¶ùùì(‚Ñã!,#) Concatenate\nScale\nFigure 2: The illustration of High-order Direction Transformer (HDFormer) block. HDFormer block consists of three major parts: (a)\nFirst-order attention block to capture ‚Äújoint‚Üî joint‚Äù spatial relationship; (b) Hyperbone representation learning module to encode hyperbone\nfeatures; (c) High-order attention block to capture both second-order ‚Äúbone ‚Üî joint‚Äù and high-order ‚Äúhyperbone‚Üî joint‚Äù interactions.\n3.2 High-order Directed Transformer\nThe spatial connections between ‚Äújoint ‚Üîjoint‚Äù and\n‚Äújoint‚Üîbone‚Äù are referred to as first-order and second-\norder information in the 3D pose estimation, which is widely\nstudied in the previous works [Zhang et al., 2022]. Neverthe-\nless, pairwise first-order and second-order information alone\ncannot fully describe the complex human skeleton dynamics\nin the 2D to 3D mappings. For example, human skeletons\noften move in specific patterns and involve multiple joints\nand bones at the same time. This observation leads us to\nfurther investigate the high-order information interaction of\nthe human skeleton by integrating the high-order attention\nlearning with directed graph and propose a High-order Direct\nTransformer (HDFormer) for 3D pose estimation.\nFirst-order Attention Modeling\nThe joint sets of the skeleton describe the rough posture of\nthe human body, and the global multi-head attention [Zhang\net al., 2022 ] has demonstrated its effectiveness in 3D pose\nestimation. Therefore, the multi-head attention scheme is\nadopted in this work for first-order attention modeling. As\nillustrated in Fig. 2(a), A ‚àà RN√óN is the adjacency ma-\ntrix and Œ® is the learnable adjacency matrix which has the\nsame dimension as A. Specifically, given the joint token set\nZ = {z1 . . . zi} where i ‚àà N denotes the index of skele-\nton joints, zi ‚àà RC, C represents the feature channel. The\nfirst-order self-attention modeling and feature of joints can\nbe obtained by following Eq. 1, where query, key, and value\nmatrices are generated by three linear layers, Q = WqZ,\nK = WkZ, and V = WvZ, respectively. S represents the\nnumber of heads and‚Ñèk is the output of each head. Unlike the\ntraditional multi-head attention fuses the output of the atten-\ntion module with concatenating (Eq. 3), we revamp the fusion\nscheme with a simple accumulation:\nÀÜZ =\nSX\nk=1\n‚Ñèk, (4)\nwhere ÀÜZ denote the final output of first-order attention.\nHyperbone Representation\nA hyperbone is a series of joints and bones that are connected\nsequentially. The human skeleton can be represented as a\nspecial type of graph without loops, allowing for the unique\ndetermination of the shortest path between two joints. Given\na starting and ending joint, the corresponding hyperbone can\nbe identified using the distance of the shortest path (SPD).\nSpecifically, as shown in Fig. 2, the human skeleton can be\ndescribed as a directed graph G. The ‚Äúhip‚Äù joint is defined\nas the directed graph‚Äôs root node. Given two joint nodes on\nthe directed graph, we could follow the direction of edges to\nfind the shortest path from starting joint i to j. For example,\nthere is a shortest path from joint index 0 to joint index 3 by\n[0, 1, 2, 3], which is done by moving from index 0 to index\n3 following the edges of the directed graph (bone for human\nskeleton). Formally, given the human skeleton-directed graph\nG and the (start, end) joint indices ( i, j), we could utilize\nthe SPD algorithm to discover the joint set belonging to the\nhyperbone Bi,j = {vhi, vhi+1 , . . . , vhj }, where v‚àó represents\nthe human joint, |Bi,j| = n represents the number of joints in\nhyperbone, and we call this the order of hyperbone, h‚àó is the\njoint index:\nBi,j = SPD(G, i, j), (5)\nTo encode hyperbone features, we propose a novel hyperbone\nencoding method. Specifically, the feature of hyperbone can\nbe obtained by a function œï(¬∑) that takes hyperbone joint set\nBi,j‚Äôs corresponding features Hi,j = {zhi, zhi+1 , . . . , zhj }\nas input and generate hyperbone features, where zhi is the\nfeature of joint vhi.\nInstantiation\nPrevious works, e.g. Anatomy [Chen et al., 2021 ], used a\nsimple subtraction of joint features to construct bone fea-\ntures. In contrast, we propose a general process for construct-\ning both bone and hyperbone features, and offer instantiation\nmethods. Specifically, we investigate several instantiations of\nthe function œï(¬∑).\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n583\nSubtraction. œï(¬∑) can be defined as a subtraction operation.\nAs we use a directed graph to represent the human skeleton\nwhen adopting subtraction for hyperbone representation, it is\nequivalent to the subtraction of start and end joints:\nœï(Hi,j) = f(zhi ‚àí zhj ), (6)\nwhere z is the joint feature, f is a linear mapping. This repre-\nsentation is easy to calculate and works fine for second-order\nbone representation, however, it loses information on bone\nsequence for hyperbone with higher order.\nSummation/Multiplication. œï(¬∑) can also be defined as\nelement-wise summation or multiplication for joints:\nœï(Hi,j) =\nX\nz‚ààHi,j\nf(z)/n, (7)\nœï(Hi,j) =\nY\nz‚ààHi,j\nf(z), (8)\nwhere n is the number of joints, f is a linear mapping.\nConcatenation. œï(¬∑) can be defined with concatenation and\nlinear mapping:\nœï(Hi,j) = f([zh1 , . . . , zhn]), (9)\nwhere the operator [¬∑] represents the concatenation of features\nin the shortest path, f maps the concatenated feature to the\nsame dimension as the joint feature.\nSub-Concat. To overcome the sequence information loss\nissue in subtraction, we combine subtraction and concatena-\ntion for a mixed function for œï(¬∑):\nœï(Hi,j) = f([zh1 ‚àí zh2 , . . . , zhn‚àí1 ‚àí zhn]), (10)\nwhere the second-order bone feature is calculated with sub-\ntraction and the high-order hyperbone feature is obtained with\nconcatenation and linear mapping.\nHigh-order Directed Transformer\nFig. 2(b) illustrates the architecture of our proposed High-\norder Directed Transformer block, which consists of three\ncomponents: first-order attention block, hyperbone encoding\nblock, and high-order attention block. The cross-attention\nfusion involves joint features ÀÜZ from first-order attention\nmodeling block and hyperbone feature H = [Y2, ..Yo, ..Yn],\nwhere Yo represent hyperbone features with order o from hy-\nperbone encoding block. Formally, the cross-attention fusion\ncan be expressed as:\nYo = [œï(Hi,j)], |Hi,j| = o,\nH = [Y2, . . . , Yn],\nQh = Wqh\nÀÜZ, Kh = WkhH, Vh = WvhH,\nCrossAttn(Qh, Kh, Vh) = Softmax(QhKT\nh /\np\ndm)Vh,\n(11)\nwhere Wqh, Wkh, Wvh are learnable parameters. Since we\nonly use the joint feature in the query, the computation and\nmemory complexity of generating the cross-attention map\nin cross-attention are linear rather than quadratic as in all-\nattention, making the entire process more efficient. Similar to\nMHSA [Vaswaniet al., 2017], we also adopt a multi-head at-\ntention design and add an MLP layer after the attention layer.\nLoss Function\nWe adopted the loss function similar to UGCN [Wang et al.,\n2020], which was formulated as follow:\nL = Lp + ŒªLm, (12)\nwhere Lp is the 3D joint coordinates loss, which is defined as\nthe mean per joint position error (MPJPE), and Lm is motion\nloss introduced by [Wang et al., 2020], Œª is a hyperparameter\nfor balancing two objectives and is set to 0.1. Motion loss\nallows our model to capture more natural movement patterns\nof the keypoints in the prediction, since Minkowski Distance\nloss does not consider the similarity of temporal structure be-\ntween the estimated pose sequence and ground truth.\n3.3 Network Architecture\nAs illustrated in Fig. 3, the proposed network architecture\ncontains three stages: 1) Downsampling stage collects long-\ntime range information by temporal pooling. The temporal\ndownsampling block has an inside temporal convolution‚Äôs\nstride set to 2 and kernel size set to 5. It is used to down-\nsample the temporal resolution; 2) Upsampling stage recov-\ners the temporal resolution, and skip connections are adopted\nbetween the downsampling stage and the upsampling stage\nto integrate the low-level features. The temporal upsampling\nblock is the conventional bilinear interpolation along the tem-\nporal axis to recover higher temporal resolution. 3) Merging\nstage transforms the feature maps at different temporal scales\nin the upsampling stage, and fuses them to obtain the final\nembedding. Finally, the 3D coordinate for each keypoint is\nregressed by a fully connected layer.\n4 Experiment\n4.1 Datasets and Metric\nExperiments are conducted on the 3D pose estimation bench-\nmark dataset Human3.6M [Ionescu et al., 2014 ] and MPI-\nINF-3DHP [Mehta et al., 2016a ]. Human3.6M is the most\nwidely used evaluation benchmark, containing 3.6 million\nvideo frames captured from four synchronized cameras with\ndifferent locations and poses at 50 Hz. 11 subjects are per-\nforming 15 kinds of actions. MPI-INF-3DHP is a 3D human\nbody pose estimation dataset consisting of both constrained\nindoor and complex outdoor scenes. It consists of 1.3M\nframes captured from the 14 cameras. For fair comparisons,\nthe evaluation metric MPJPE is adopted in this work, which\nfollows the setting of the previous works [Hu et al., 2021;\nCai et al., 2019; Zhang et al., 2022; Zhao et al., 2022].\n4.2 Implementation Details\nWe optimized the model by the AdaMod optimizer [Ding et\nal., 2019] for 110 epochs with a batch size of 256, and the\nbase learning rate is 5 √ó 10‚àí3 with decayed by 0.1 at 80, 90,\nand 100 epochs. To avoid over-fitting, we set the weight de-\ncay factor to 10‚àí5 for parameters of convolution layers and\nthe dropout rate to 0.3 at part of the layers. Besides, we fol-\nlowed UGCN [Wanget al., 2020] to apply the sliding window\nalgorithm with a step length of 5 to estimate a variable-length\npose sequence with fixed input length at inference time.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n584\nX\nY\nZ\n2D\tpose\nsequence\n+\n+\n+\n+\n+\nHDFormer\tBlockFirst-order\tAttention\nBlock\nTemporal\nUpsampling\nTemporal\nDownsampling FC\nDownSampling\tStage UpSampling\tStage Merging\tStage\n[B,\t2,\tJ,\tT]\n[B,\t16,\tJ,\tT]\n[B,\t32,\tJ,\tT/2]\n[B,\t64,\tJ,\tT/4]\n[B,\t128,\tJ,\tT/8]\n[B,\t128,\tJ,\tT/8]\n[B,\t64,\tJ,\tT/4]\n[B,\t32,\tJ,\tT/2]\n[B,\t16,\tJ,\tT]\n[B,\t16,\tJ,\tT]\n[B,\t16,\tJ,\tT]\n[B,\t16,\tJ,\tT]\n[B,\t3,\tJ,\tT]\n[B,\t16,\tJ,\tT]\n[B,\t32,\tJ,\tT/2]\n[B,\t64,\tJ,\tT/4]\n[B,\t128,\tJ,\tT/8]\n[B,\t16,\tJ,\tT]\n3D\tpose\nsequence\nFigure 3: Overview of our framework: A High-order Directed Transformer with a U-shaped design for 3D human pose estimation. The\nframework includes downsampling, upsampling, and merging stages, incorporating high-order attention and multi-scale temporal information.\nThe batch size, the number of nodes, and the sequence length are represented by symbols B, J, and T, respectively.\n(a)\tFirst-order\tAttention (b)\tHigh-order\tAttention\t\n0:\t\tHip\t\n1:\t\tR.Hip\t\n2:\t\tR.Knee\t\n3:\t\tR.Ankle\t\n4:\t\tL.Hip\t\n5:\t\tL.Knee\t\n6:\t\tL.Ankle\t\n7:\t\tSpine\t\n8:\t\tThorax\t\n9:\t\tNose\t\n10:\tHead\t\n11:\tL.Shoulder\t\n12:\tL.Elbow\t\n13:\tL.Wrist\t\n14:\tR.Shoulder\t\n15:\tR.Elbow\t\n16:\tR.Wrist\nFigure 4: Visualization of first-order attentions and high-order atten-\ntion between body joints and hyperbone.\n4.3 Quantitative Evaluation\nResults on Human3.6M. The proposed approach is com-\npared with the state-of-the-art methods to evaluate the per-\nformance. In this subsection, the reported performance in\ntheir original paper is directly copied as their results. The per-\nformance comparison with the state-of-the-art works on Hu-\nman3.6M [Ionescu et al., 2014] is listed in Tab. 1, including\ngraph ConvNet-based and Transformer-based methods. For\nfair comparisons with other SOTA methods, we consider not\nonly the effectiveness of the model but also the scale of pa-\nrameters and latency in Tab. 2, which can comprehensively\ndemonstrate the real-world performance of the model. To\nour knowledge, this is the first comprehensive comparison ex-\nperiment on the benchmark dataset Human3.6M. The current\nSOTA method MixSTE [Zhang et al., 2022 ], a transformer-\nbased model, achieves 25.9% and 21.6% MPJPE with the\ninput of 81 and 243-frame sequences, respectively. Com-\npared to MixSTE, the proposed HDFormer achieves 21.6%\nMPJPE with the input of only 96 frames. More importantly,\nour model has only a 1/10 scale of 3.7 M vs. 33.8 M and six\ntimes the speed. The graph ConvNet-based SOTA method\nU-CondDGCN has a very small scale of parameters, latency,\nand ideal performance. However, the proposed HDFormer\nis a transformer-based method that achieves significant im-\nprovement compared to U-CondDGCN with a very close\nscale of parameters and same-level latency.\nResults on MPI-INF-3DHP. In Tab. 3, we compared our\nmethod with state-of-the-art methods on the MPI-INF-3DHP\nbenchmark to evaluate the generalization ability of the pro-\nposed HDFormer. We take the ground-truth 2D poses as\nmodel input. Our method achieves the same trends as the\nresults on Human3.6M, which is also the SOTA performance\nunder the metric of PCK, AUC, and MPJPE.\n4.4 Qualitative Results\nAs shown in Fig. 4, we further conduct visualization on the\nFirst-order attention and High-order attention. The selected\naction (Eating of test set S9) is applied for visualization.\nFor the First-order attention map in Fig. 4(a), the horizon-\ntal and vertical axes are all joint indexes, and it can be eas-\nily observed that the dependency between the spine node\nand left/right elbow nodes are significant for the ‚Äúeating‚Äù se-\nquence. Besides, the left shoulder node also plays an impor-\ntant role in the spatial relationship with the left ankle node\nwhen eating in the sitting pose. Furthermore, to demonstrate\nthe effect of the proposed high-order attention block, we fur-\nther visualize the high-order attention map for the action of\neating from the test set S9 in Fig. 4(b), where the vertical\naxes were the index of the joint while the horizontal axes\nwere the index of hyperbones. In our experiments, the maxi-\nmum SPD length was 4. As a result, the hyperbone sequence\nhas 42 bone features in the horizontal axes. From the atten-\ntion map, we can find that the hyperbone feature has an im-\npact on different joints. The left/right elbows and left/right\nwrist have a relatively large response to hyperbone sequence\nindex from 38 to 41, which corresponds to the higher order\nbone feature. The hyperbone 38 to 41 corresponding to joint\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n585\nProtocol #1 Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.\nCai [Cai et al., 2019] (CPN, T=7) 44.6 47.4 45.6 48.8 50.8 59.0 47.2 43.9 57.9 61.9 49.7 46.6 51.3 37.1 39.4 48.8\nPavllo [Pavllo et al., 2018] (CPN, T=243) 45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9 46.8\nXu [Xu et al., 2020] (CPN, T=9) 37.4 43.5 42.7 42.7 46.6 59.7 41.3 45.1 52.7 60.2 45.8 43.1 47.7 33.7 37.1 45.6\nLiu [Liu et al., 2020](CPN, T=243) 41.8 44.8 41.1 44.9 47.4 54.1 43.4 42.2 56.2 63.6 45.3 43.5 45.3 31.3 32.2 45.1\nWang [Wang et al., 2020] (CPN, T=96) 41.3 43.9 44.0 42.2 48.0 57.1 42.2 43.2 57.3 61.3 47.0 43.5 47.0 32.6 31.8 45.6\nHu [Hu et al., 2021] (CPN, T=96) 38.0 43.3 39.1 39.4 45.8 53.6 41.4 41.4 55.5 61.9 44.6 41.9 44.5 31.6 29.4 43.4\nZhang [Zhang et al., 2022] (CPN, T=81) 39.8 43.0 38.6 40.1 43.4 50.6 40.6 41.4 52.2 56.7 43.8 40.8 43.9 29.4 30.3 42.4\nWang [Wang et al., 2020] (HR-Net, T=96) 38.2 41.0 45.9 39.7 41.4 51.4 41.6 41.4 52.0 57.4 41.8 44.4 41.6 33.1 30.0 42.6\nHu [Hu et al., 2021] (HR-Net, T=96) 35.5 41.3 36.6 39.1 42.4 49.0 39.9 37.0 51.9 63.3 40.9 41.3 40.3 29.8 28.9 41.1\nZhang [Zhang et al., 2022] (HR-Net, T=243) 36.7 39.0 36.5 39.4 40.2 44.9 39.8 36.9 47.9 54.8 39.6 37.8 39.3 29.7 30.6 39.8\nHDFormer(CPN, T=96) 38.1 43.1 39.3 39.4 44.3 49.1 41.3 40.8 53.1 62.1 43.3 41.8 43.1 31.0 29.7 42.6\nHDFormer (HR-Net, T=96) 34.7 41.7 36.0 38.4 41.1 45.3 39.6 37.4 49.0 63.1 39.8 38.9 40.2 29.3 29.1 40.3\nProtocol #2 Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.\nCai [Cai et al., 2019] (CPN, T=7) 35.7 37.8 36.9 40.7 39.6 45.2 37.4 34.5 46.9 50.1 40.5 36.1 41.0 29.6 33.2 39.0\nPavllo [Pavllo et al., 2018] (CPN, T=243) 34.1 36.1 34.4 37.2 36.4 42.2 34.4 33.6 45.0 52.5 37.4 33.8 37.8 25.6 27.3 36.5\nXu [Xu et al., 2020] (CPN, T=9) 31.0 34.8 34.7 34.4 36.2 43.9 31.6 33.5 42.3 49.0 37.1 33.0 39.1 26.9 31.9 36.2\nLiu [Liu et al., 2020](CPN, T=243) 32.3 35.2 33.3 35.8 35.9 41.5 33.2 32.7 44.6 50.9 37.0 32.4 37.0 25.2 27.2 35.6\nWang [Wang et al., 2020] (CPN, T=96) 32.9 35.2 35.6 34.4 36.4 42.7 31.2 32.5 45.6 50.2 37.3 32.8 36.3 26.0 23.9 35.5\nHu [Hu et al., 2021] (CPN, T=96) 29.8 34.4 31.9 31.5 35.1 40.0 30.3 30.8 42.6 49.0 35.9 31.8 35.0 25.7 23.6 33.8\nZhang [Zhang et al., 2022] (CPN, T=81) 32.0 34.2 31.7 33.7 34.4 39.2 32.0 31.8 42.9 46.9 35.5 32.0 34.4 23.6 25.2 33.9\nWang [Wang et al., 2020] (HR-Net, T=96) 28.4 32.5 34.4 32.3 32.5 40.9 30.4 29.3 42.6 45.2 33.0 32.0 33.2 24.2 22.9 32.7\nHu [Hu et al., 2021] (HR-Net, T=96) 27.7 32.7 29.4 31.3 32.5 37.2 29.3 28.5 39.2 50.9 32.9 31.4 32.1 23.6 22.8 32.1\nZhang [Zhang et al., 2022] (HR-Net, T=243) 28.0 30.9 28.6 30.7 30.4 34.6 28.6 28.1 37.1 47.3 30.5 29.7 30.5 21.6 20.0 30.6\nHDFormer (CPN, T=96) 29.6 33.8 31.7 31.3 33.7 37.7 30.6 31.0 41.4 47.6 35.0 30.9 33.7 25.3 23.6 33.1\nHDFormer (HR-Net, T=96) 27.9 32.8 29.7 30.6 32.5 35.0 28.9 29.2 38.3 50.0 32.9 30.1 31.8 23.6 22.8 31.7\nTable 1: Quantitative comparisons with state-of-the-art methods on Human3.6M under protocol #1 and protocol #2, where methods marked\nwith ‚Ä† are video-based; T denotes the number of input frames; and CPN and HR-Net denote the input 2D poses are estimated by[Chen et al.,\n2017] and [Sun et al., 2019], respectively. The best results of CPN and HR-Net are marked in red and blue, respectively.\nMethods MPJPE[‚Üì] Params Latency Frames\nU-CondDGCN [Hu et al., 2021] 22.7 3.4 M 0.6 ms 96\nCai [Cai et al., 2019] 37.2 5.04M 11.6ms 7\nPavllo [Pavllo et al., 2018] 37.2 17.0M - 243\nLiu [Liu et al., 2020] 34.7 11.25M 9.9ms 243\nWang [Wang et al., 2020] 25.6 1.69M - 96\nMixSTE [Zhang et al., 2022] 25.9 33.7M 2.6ms 81\nMixSTE ‚Ä°[Zhang et al., 2022] 21.6 33.8M 8.0 ms 243\nHDFormer* 22.1 2.8 M 0.9 ms 96\nHDFormer 21.6 3.7 M 1.3 ms 96\nTable 2: Results on Human3.6M with ground-truth 2D poses as in-\nput. Our method with subtraction feature representation is marked\nwith *. The latency is measured with batch size = 1.\nsets (0,7,8,9,10), (0,7,8,11,12), (0,7,8,14,15), (7,8,11,12,13),\nwhich maps to the upper body parts and head. Besides, the\nknee joints are crooked when eating in a sitting pose as Fig. 4\n(b). We also evaluate the visual result of estimated poses in\nFig. 5. It can be seen that HDFormer estimates more accurate\nposes in cluttered and self-occlusion hands and feet compared\nto MixSTE [Zhang et al., 2022].\n4.5 Ablation Study\nImpact of Multi-Order Attention. We evaluated the in-\nfluence of our multi-order attention schema by conducting\nablation studies on various order combinations, as shown in\nTab. 4. The experiments, with Human3.6M 2D ground truth\nkey points as input, indicated an improvement in the Mean\nPer Joint Position Error (MPJPE) as the order number in-\ncreased, with the best performance observed at order = 4 .\nThis affirms the value of high-order attention in capturing\ncomplex skeletal information through ‚Äùhyperbone‚Üîjoint‚Äù\nMethods PCK[‚Üë] AUC[‚Üë] MPJPE[‚Üì]\nCNN [Mehta et al., 2016b] 75.7 39.3 -\nVNect(ResNet101) [Mehta et al., 2017] 79.4 41.6 -\nTrajectoryPose3D [Lin and Lee, 2019] 83.6 51.4 79.8\nUGCN [Wang et al., 2020] 86.9 62.1 68.1\nU-CondDGCN [Hu et al., 2021] 97.9 69.5 42.5\nMixSTE [Zhang et al., 2022](T=27) 94.4 66.5 54.9\nHDFormer(T=32) 96.8 64.0 51.5\nHDFormer(T=96) 98.7 72.9 37.2\nTable 3: Results on MPI-INF-3DHP with three metrics.\nMethods Order MPJPE[‚Üì]\nHDFormer\n1 25.0\n2 23.6\n3 22.8\n4 21.6\n5 22.7\nTable 4: Ablation study of the order number. We compared the re-\nsults of different orders involved in the high-order attention trans-\nformer block in Human3.6M with ground truth as input.\nfeature interaction.\nEffectiveness of HDformer Block at Different Stages.\nWe conducted experiments by adopting HDFormer block at\nvarious stages in Tab. 5. Compared with adopting HDFormer\nat the downsampling stage (yields 6.9mm decrease), adopt-\ning it at all stages (yields 3.3mm decrease) and adopting it\nat the merge stage get the best performance (yields 4.0 mm\nimprovement). We found that adopting HDFormer block at\nthe merging stage achieves a better result than other stages.\nThis could be due to the complex skeleton dynamics of hy-\nperbones can not be learned at early stages (i.e., downsam-\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n586\nGT MixSTEOurs\nazim=-90,\telev\t=-20\nazim=-90,\telev\t=-84\nazim=-90,\telev\t=-10\nFigure 5: Qualitative comparison between our method (HDFormer)\nand [Zhang et al., 2022] with the Eating (first and second row) and\nPhoto (third row) actions on Human3.6M. The green circle high-\nlights locations where our method has better results.\nMethods High-order Attention MPJPE[‚Üì] ‚àÜ\nBaseline - 25.6 -\nHDFormer\nUpsampling stage 24.1 1.5\nDownsampling stage 32.5 -6.9\nMerging stage 21.6 4.0\nAll stage 29.9 -3.3\nTable 5: Ablation study of the effectiveness of HDFormer at differ-\nent stages. We compared the results of the baseline (UGCN [Wang\net al., 2020 ]), our HDFormer, and different configurations for our\nHDFormer on Human3.6M. The ‚àÜ denotes the improvements com-\npared with the baseline.\npling stage), therefore, leads to inferior performance. While\nin the merging stage, HDFormer block can fuse complex in-\nformation from previous stages and by high-order attention.\nExploration of Hyperbone Representation. The hyper-\nbone representation is a vital factor for the graph skeleton\nstructure, and we exploit the way of the hyperbone feature\nrepresentation. We conducted experiments by adopting 4th\norder HDFormer block with different instantiations modes,\nwhich can be seen in Tab. 6. We found that all the hyperbone\nrepresentation methods outperform the baseline, as they uti-\nlize high-order information. Among them, subtraction + con-\ncatenation boosts the performance over baseline by 4.0mm.\nIt shows that bone feature concatenation with shortest path\naggregation is effective for hyperbone feature representation.\nRole of Position Encoding and Multi-Head Attention.\nWe observed from our experimental results (line 2 of Tab. 7)\nthat incorporating absolute positional encoding led to a de-\ncrease in performance. Besides, we have conducted an abla-\nMethods hyperbone representation MPJPE[‚Üì] ‚àÜ\nBaseline - 25.6 -\nHDFormer\nsummation 23.5 2.1\nmultiplication 23.1 2.5\nconcatenation 23.5 2.1\nsubtraction + concatenation 21.6 4.0\nTable 6: Ablation study of hyperbone representation. We compared\nthe results of different edge feature representations for hyperbone\nencoding in Human3.6M with ground truth as input.\nLine Methods MPJPE[‚Üì] Params Frames\n1 HDFormer (w/o Œ®) 27.4 3.7 M 96\n2 HDFormer (with pos encoding) 22.1 3.8 M 96\n3 HDFormer (multi-head concat) 22.9 3.7 M 96\n4 HDFormer (T=243) 21.8 4.7 M 96\n5 HDFormer (proposed) 21.6 3.7 M 96\nTable 7: More results on Human3.6M with GT 2D poses as input.\ntion study on the use of concatenation and summation in the\nmulti-head attention module, and we found that summation\nresulted in better performance, which can be shown in line 3\nof Tab. 7. Consequently, we adopted the summation in the\nmulti-head attention in our proposed model. We also found\nextending the input frame numbers to 243 led to a slight de-\ncline compared to 96 frames as shown in line 4 of 7. The\nreason might be the small scale of our model (1/10 compared\nto [Zhang et al., 2022]) can not well capture temporal redun-\ndancy and noise in dense sequence.\n5 Conclusion\nIn this work, we propose a novel model named High-order\nDirected Transformer (HDFormer), which considers both\n‚Äújoint‚Üîjoint‚Äù, ‚Äúbone‚Üîjoint ‚Äù and ‚Äúhyperbone‚Üîjoint ‚Äù con-\nnections. Specifically, we propose a hyperbone represen-\ntation learning module and a high-order attention module\nto model complicated semantic relations between hyperbone\nand joint. We conduct extensive experiments to provide both\nquantitative and qualitative analysis. Our proposed method\nachieves state-of-the-art performances with only 1/10 param-\neters and a fraction of computational cost compared to re-\ncently published SOTA.\nAcknowledgments\nZhi-Qi Cheng‚Äôs research in this project was supported by\nthe US Department of Transportation, Office of the Assistant\nSecretary for Research and Technology, under the Univer-\nsity Transportation Center Program (Federal Grant Number\n69A3551747111), as well as Intel and IBM Fellowships.\nContribution Statement\nHanyuan Chen, Jun-Yan He, Wangmeng Xiang, and Zhi-Qi\nCheng contributed equally to this work as co-first authors,\nwith random ordering. They were involved in study design,\nexperiment execution, manuscript writing, and discussions.\nWangmeng Xiang is the corresponding author.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n587\nReferences\n[Cai et al., 2019] Yujun Cai, Liuhao Ge, Jun Liu, Jianfei\nCai, Tat-Jen Cham, Junsong Yuan, and Nadia Magnenat-\nThalmann. Exploiting spatial-temporal relationships for\n3d pose estimation via graph convolutional networks.\nIEEE/CVF International Conference on Computer Vision,\npages 2272‚Äì2281, 2019.\n[Chen et al., 2017] Yilun Chen, Zhicheng Wang, Yuxiang\nPeng, Zhiqiang Zhang, Gang Yu, and Jian Sun. Cas-\ncaded pyramid network for multi-person pose estimation.\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7103‚Äì7112, 2017.\n[Chen et al., 2021] Tianlang Chen, Chengjie Fang, Xiaohui\nShen, Yiheng Zhu, Zhili Chen, and Jiebo Luo. Anatomy-\naware 3d human pose estimation with bone-based pose de-\ncomposition. IEEE Transactions on Circuits and Systems\nfor Video Technology, 32:198‚Äì209, 2021.\n[Cheng et al., 2022] Zhi-Qi Cheng, Qi Dai, Siyao Li, Teruko\nMitamura, and Alexander Hauptmann. Gsrformer:\nGrounded situation recognition transformer with alternate\nsemantic attention refinement. In Proceedings of the\n30th ACM International Conference on Multimedia, pages\n3272‚Äì3281, 2022.\n[Ding et al., 2019] Jianbang Ding, Xuancheng Ren, Ruixuan\nLuo, and Xu Sun. An adaptive and momental bound\nmethod for stochastic learning. ArXiv, abs/1910.12249,\n2019.\n[Gilmer et al., 2017] Justin Gilmer, Samuel S. Schoenholz,\nPatrick F. Riley, Oriol Vinyals, and George E. Dahl.\nNeural message passing for quantum chemistry. ArXiv,\nabs/1704.01212, 2017.\n[Gong et al., 2021] Kehong Gong, Jianfeng Zhang, and Ji-\nashi Feng. Poseaug: A differentiable pose augmenta-\ntion framework for 3d human pose estimation. IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8571‚Äì8580, 2021.\n[He et al., 2021] Jun-Yan He, Xiao Wu, Zhi-Qi Cheng,\nZhaoquan Yuan, and Yu-Gang Jiang. Db-lstm: Densely-\nconnected bi-directional lstm for human action recogni-\ntion. Neurocomputing, 444:319‚Äì331, 2021.\n[He et al., 2023] Jun-Yan He, Zhi-Qi Cheng, Chenyang Li,\nWangmeng Xiang, Binghui Chen, Bin Luo, Yifeng Geng,\nand Xuansong Xie. Damo-streamnet: Optimizing stream-\ning perception in autonomous driving. arXiv preprint\narXiv:2303.17144, 2023.\n[Hu et al., 2021] Wenbo Hu, Changgong Zhang, Fangneng\nZhan, Lei Zhang, and Tien-Tsin Wong. Conditional di-\nrected graph convolution for 3d human pose estimation.\nProceedings of the 29th ACM International Conference on\nMultimedia, 2021.\n[Huang et al., 2021] Siyu Huang, Haoyi Xiong, Zhi-Qi\nCheng, Qingzhong Wang, Xingran Zhou, Bihan Wen, Jun\nHuang, and Dejing Dou. Generating person images with\nappearance-aware pose stylizer. In International Joint\nConference on Artificial Intelligence, pages 623‚Äì629. In-\nternational Joint Conferences on Artificial Intelligence,\n2021.\n[Ionescu et al., 2014] Catalin Ionescu, Dragos Papava, Vlad\nOlaru, and Cristian Sminchisescu. Human3.6m: Large\nscale datasets and predictive methods for 3d human sens-\ning in natural environments.IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 36:1325‚Äì1339, 2014.\n[Iskakov et al., 2019] Karim Iskakov, Egor Burkov, Victor S.\nLempitsky, and Yury Malkov. Learnable triangulation\nof human pose. IEEE/CVF International Conference on\nComputer Vision, pages 7717‚Äì7726, 2019.\n[Li et al., 2020] Shichao Li, Lei Ke, Kevin Pratama, Yu-\nWing Tai, Chi-Keung Tang, and Kwang-Ting Cheng. Cas-\ncaded deep monocular 3d human pose estimation with\nevolutionary training data. IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 6172‚Äì\n6182, 2020.\n[Li et al., 2021] Wenhao Li, Hong Liu, Runwei Ding,\nMengyuan Liu, Pichao Wang, and Wenming Yang. Ex-\nploiting temporal contexts with strided transformer for 3d\nhuman pose estimation. IEEE Transactions on Multime-\ndia, 25:1282‚Äì1293, 2021.\n[Lin and Lee, 2019] Jiahao Lin and Gim Hee Lee. Trajectory\nspace factorization for deep video-based 3d human pose\nestimation. ArXiv, abs/1908.08289, 2019.\n[Liu et al., 2020] Ruixu Liu, Ju Shen, He Wang, Chen Chen,\nSen ching S. Cheung, and Vijayan K. Asari. Attention\nmechanism exploits temporal contexts: Real-time 3d hu-\nman pose reconstruction. IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 5063‚Äì5072,\n2020.\n[Mehta et al., 2016a] Dushyant Mehta, Helge Rhodin, Dan\nCasas, Pascal V . Fua, Oleksandr Sotnychenko, Weipeng\nXu, and Christian Theobalt. Monocular 3d human pose es-\ntimation in the wild using improved cnn supervision.Inter-\nnational Conference on 3D Vision, pages 506‚Äì516, 2016.\n[Mehta et al., 2016b] Dushyant Mehta, Helge Rhodin, Dan\nCasas, Pascal V . Fua, Oleksandr Sotnychenko, Weipeng\nXu, and Christian Theobalt. Monocular 3d human pose es-\ntimation in the wild using improved cnn supervision.Inter-\nnational Conference on 3D Vision, pages 506‚Äì516, 2016.\n[Mehta et al., 2017] Dushyant Mehta, Srinath Sridhar, Olek-\nsandr Sotnychenko, Helge Rhodin, Mohammad Shafiei,\nHans-Peter Seidel, Weipeng Xu, Dan Casas, and Chris-\ntian Theobalt. Vnect: Real-time 3d human pose estimation\nwith a single rgb camera. ACM Trans. Graph., 36:44:1‚Äì\n44:14, 2017.\n[Pavllo et al., 2018] Dario Pavllo, Christoph Feichtenhofer,\nDavid Grangier, and Michael Auli. 3d human pose es-\ntimation in video with temporal convolutions and semi-\nsupervised training. IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 7745‚Äì7754, 2018.\n[Qiao et al., 2022] Jian-Jun Qiao, Zhi-Qi Cheng, Xiao Wu,\nWei Li, and Ji Zhang. Real-time semantic segmentation\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n588\nwith parallel multiple views feature augmentation. In Pro-\nceedings of the 30th ACM International Conference on\nMultimedia, pages 6300‚Äì6308, 2022.\n[Qiu et al., 2019] Haibo Qiu, Chunyu Wang, Jingdong\nWang, Naiyan Wang, and Wenjun Zeng. Cross view fu-\nsion for 3d human pose estimation. IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 4341‚Äì4350,\n2019.\n[Scarselli et al., 2009] Franco Scarselli, Marco Gori,\nAh Chung Tsoi, Markus Hagenbuchner, and Gabriele\nMonfardini. The graph neural network model. IEEE\nTransactions on Neural Networks, 20:61‚Äì80, 2009.\n[Shi et al., 2018] Lei Shi, Yifan Zhang, Jian Cheng, and\nHanqing Lu. Two-stream adaptive graph convolu-\ntional networks for skeleton-based action recognition.\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12018‚Äì12027, 2018.\n[Shi et al., 2019] Lei Shi, Yifan Zhang, Jian Cheng, and\nHanqing Lu. Skeleton-based action recognition with di-\nrected graph neural networks. IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 7904‚Äì\n7913, 2019.\n[Sun et al., 2019] Ke Sun, Bin Xiao, Dong Liu, and Jing-\ndong Wang. Deep high-resolution representation learn-\ning for human pose estimation. IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5686‚Äì\n5696, 2019.\n[Tu et al., 2023] Shuyuan Tu, Qi Dai, Zuxuan Wu, Zhi-Qi\nCheng, Han Hu, and Yu-Gang Jiang. Implicit temporal\nmodeling with learnable alignment for video recognition.\nArXiv, abs/2304.10465, 2023.\n[Vaswani et al., 2017] Ashish Vaswani, Noam M. Shazeer,\nNiki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. In NIPS, 2017.\n[Wang et al., 2020] Jingbo Wang, Sijie Yan, Yuanjun Xiong,\nand Dahua Lin. Motion guided 3d pose estimation from\nvideos. ArXiv, abs/2004.13985, 2020.\n[Wang et al., 2021] Wenxiao Wang, Lulian Yao, Long Chen,\nBinbin Lin, Deng Cai, Xiaofei He, and Wei Liu. Cross-\nformer: A versatile vision transformer hinging on cross-\nscale attention. ArXiv, abs/2303.06908, 2021.\n[Xiang et al., 2022] Wangmeng Xiang, Chao Li, Yuxuan\nZhou, Biao Wang, and Lei Zhang. Language supervised\ntraining for skeleton-based action recognition. ArXiv,\nabs/2208.05318, 2022.\n[Xu et al., 2020] Jingwei Xu, Zhenbo Yu, Bingbing Ni,\nJiancheng Yang, Xiaokang Yang, and Wenjun Zhang.\nDeep kinematics analysis for monocular 3d human pose\nestimation. IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 896‚Äì905, 2020.\n[Yan et al., 2019] Sijie Yan, Zhizhong Li, Yuanjun Xiong,\nHuahan Yan, and Dahua Lin. Convolutional sequence gen-\neration for skeleton-based action synthesis. IEEE/CVF In-\nternational Conference on Computer Vision, pages 4393‚Äì\n4401, 2019.\n[Ye et al., 2022] Hang Ye, Wentao Zhu, Chun yu Wang, Ru-\njie Wu, and Yizhou Wang. Faster voxelpose: Real-time\n3d human pose estimation by orthographic projection. In\nEuropean Conference on Computer Vision, 2022.\n[Zeng et al., 2021] Ailing Zeng, Xiao Sun, Lei Yang, Nanx-\nuan Zhao, Minhao Liu, and Qiang Xu. Learning skele-\ntal graph neural networks for hard 3d pose estimation.\nIEEE/CVF International Conference on Computer Vision,\npages 11416‚Äì11425, 2021.\n[Zhang et al., 2022] Jinlu Zhang, Zhigang Tu, Jianyu Yang,\nYujin Chen, and Junsong Yuan. Mixste: Seq2seq mixed\nspatio-temporal encoder for 3d human pose estimation in\nvideo. IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 13222‚Äì13232, 2022.\n[Zhao et al., 2018] Bo Zhao, Xiao Wu, Zhi-Qi Cheng, Hao\nLiu, Zequn Jie, and Jiashi Feng. Multi-view image gener-\nation from a single-view. In Proceedings of the 26th ACM\ninternational conference on Multimedia, pages 383‚Äì391,\n2018.\n[Zhao et al., 2019] Long Zhao, Xi Peng, Yu Tian, Mub-\nbasir Kapadia, and Dimitris N. Metaxas. Semantic graph\nconvolutional networks for 3d human pose regression.\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3420‚Äì3430, 2019.\n[Zhao et al., 2022] Weixi Zhao, Weiqiang Wang, and Yunjie\nTian. Graformer: Graph-oriented transformer for 3d pose\nestimation. IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 20406‚Äì20415, 2022.\n[Zheng et al., 2021] Ce Zheng, Sijie Zhu, Mat‚Äôias Mendieta,\nTaojiannan Yang, Chen Chen, and Zhengming Ding. 3d\nhuman pose estimation with spatial and temporal trans-\nformers. IEEE/CVF International Conference on Com-\nputer Vision, pages 11636‚Äì11645, 2021.\n[Zhou et al., 2022] Yuxuan Zhou, Zhi-Qi Cheng, Chao Li,\nYifeng Geng, Xuansong Xie, and Margret Keuper. Hyper-\ngraph transformer for skeleton-based action recognition.\nArXiv, abs/2211.09590, 2022.\n[Zhou et al., 2023] Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan\nHe, Bin Luo, Yifeng Geng, Xuansong Xie, and Margret\nKeuper. Overcoming topology agnosticism: Enhancing\nskeleton-based action recognition through redefined skele-\ntal topology awareness. arXiv preprint arXiv:2305.11468,\n2023.\n[Zhu et al., 2021] Yiran Zhu, Xing Xu, Fumin Shen, Yanli Ji,\nLianli Gao, and Heng Tao Shen. Posegtac: Graph trans-\nformer encoder-decoder with atrous convolution for 3d hu-\nman pose estimation. In International Joint Conference on\nArtificial Intelligence, 2021.\n[Zou and Tang, 2021] Zhiming Zou and Wei Tang. Modu-\nlated graph convolutional network for 3d human pose esti-\nmation. IEEE/CVF International Conference on Computer\nVision, pages 11457‚Äì11467, 2021.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n589",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7798478603363037
    },
    {
      "name": "Transformer",
      "score": 0.7751560211181641
    },
    {
      "name": "Pose",
      "score": 0.6902298927307129
    },
    {
      "name": "Joint (building)",
      "score": 0.6213098168373108
    },
    {
      "name": "Architecture",
      "score": 0.42385154962539673
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4237775206565857
    },
    {
      "name": "Machine learning",
      "score": 0.3260488510131836
    },
    {
      "name": "Engineering",
      "score": 0.09969830513000488
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210086143",
      "name": "Alibaba Group (Cayman Islands)",
      "country": "KY"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ],
  "cited_by": 47
}