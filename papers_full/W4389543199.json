{
  "title": "A Full Transformer-based Framework for Automatic Pain Estimation using Videos",
  "url": "https://openalex.org/W4389543199",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5035772048",
      "name": "Stefanos Gkikas",
      "affiliations": [
        "Hellenic Mediterranean University"
      ]
    },
    {
      "id": "https://openalex.org/A5087123353",
      "name": "Manolis Tsiknakis",
      "affiliations": [
        "Foundation for Research and Technology Hellas",
        "Hellenic Mediterranean University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2399802263",
    "https://openalex.org/W2763559981",
    "https://openalex.org/W2912654919",
    "https://openalex.org/W4319455197",
    "https://openalex.org/W2536346144",
    "https://openalex.org/W2343651547",
    "https://openalex.org/W2116126344",
    "https://openalex.org/W2154798479",
    "https://openalex.org/W4226478960",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W6755477022",
    "https://openalex.org/W6790830454",
    "https://openalex.org/W2965861839",
    "https://openalex.org/W2954739124",
    "https://openalex.org/W3180146251",
    "https://openalex.org/W3004782605",
    "https://openalex.org/W3087023375",
    "https://openalex.org/W2330996833",
    "https://openalex.org/W4229001484",
    "https://openalex.org/W3175059179",
    "https://openalex.org/W3199961353",
    "https://openalex.org/W2341528187",
    "https://openalex.org/W2949662773",
    "https://openalex.org/W2963839617",
    "https://openalex.org/W1985941926",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W2785066428",
    "https://openalex.org/W3104792420",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3101998545"
  ],
  "abstract": "The automatic estimation of pain is essential in designing an optimal pain management system offering reliable assessment and reducing the suffering of patients. In this study, we present a novel full transformer-based framework consisting of a Transformer in Transformer (TNT) model and a Transformer leveraging cross-attention and self-attention blocks. Elaborating on videos from the BioVid database, we demonstrate state-of-the-art performances, showing the efficacy, efficiency, and generalization capability across all the primary pain estimation tasks.",
  "full_text": "A Full Transformer-based Framework for Automatic Pain Estimation\nusing Videos\nStefanos Gkikas 1 and Manolis Tsiknakis 2\nAbstract— The automatic estimation of pain is essential in\ndesigning an optimal pain management system offering reliable\nassessment and reducing the suffering of patients. In this\nstudy, we present a novel full transformer-based framework\nconsisting of a Transformer in Transformer (TNT) model and\na Transformer leveraging cross-attention and self-attention\nblocks. Elaborating on videos from theBioViddatabase,\nwe demonstrate state-of-the-art performances, showing the\nefcacy, efciency, and generalization capability across all the\nprimary pain estimation tasks.\nI. INTRODUCTION\nPain, according to Williams and Craig [1], is “a distressing\nexperience associated with actual or potential tissue damage\nwith sensory, emotional, cognitive and social components”.\nFrom a biological perspective, pain is an unfavorable sensa-\ntion that originates from the peripheral nervous system. Its\nprimary function is to activate sensory neurons and alert the\norganism to potentially harmful situations, thus serving as a\nvital mechanism for identifying and responding to threats [2].\nThe Global Burden of Disease (GBD) study refers that pain\nis the number one cause of years lived with disability (YLD),\nconcerning not only individuals but also society as a whole,\nconstituting clinical, economic, and social constraints [3].\nThe primary types of pain are acute and chronic. The major\ndifference between them is related to the duration; when it\nis present for less than three months, the pain is considered\nacute and probably accompanied by physical damage, while\nchronic perseveres the recovery process [4]. People of all\nages experience painful situations due to an accident, illness,\nor even during treatment, provoking a plethora of daily life\nchallenges. Especially in chronic pain conditions, additional\nmental health problems,e.g.,anxiety, depression, and sleep-\nrelated problems, commonly occur [5]. Furthermore, inad-\nequate pain management often leads to negative collateral\nconsequences associated with drug overuse, opioids, and\naddiction [6]. A crucial matter that needs focused attention\nis the welfare of vulnerable groups who may not be able to\ncommunicate directly or objectively. Their pain assessment\nis usually based on observing behavioral or physiological\nresponses from caregivers or family members. This specic\nsetting often leads to wrong or insufcient assessment for\n1Stefanos Gkikas is a Ph.D. candidate in Affective Com-\nputing at Hellenic Mediterranean University, Heraklion, Greece\ngkikas@ics.forth.gr\n2Manolis Tsiknakis is a Professor of Biomedical Informatics at Hellenic\nMediterranean University and Afliated Researcher at the Computational\nBiomedicine Laboratory of the Foundation for Research and Technology\n(FORTH), Heraklion, Greece tsiknaki@ics.forth.gr\ntwo main reasons: continuous monitoring is challenging\nwithout adopting technology-based solutions, and the es-\ntimation’s precision is often minimal due to inadequate\ntraining or prejudices [7]. Further challenges arise with\nthe elderly where either diminished manifestation ability or\neven unwilling communication behavior are presented [8].\nIn addition, an essential body of research [9][10] indicates\nsignicant variations of pain manifestation among people of\ndifferent gender and age, suggesting that the pain assess-\nment is an even more intricate process requiring increased\nconsideration. The automatic pain estimation procedure is\nfounded on utilizing behavioral and physiological modalities.\nThe primary behavioral modalities include facial expressions,\nbody-head movements, gestures, and vocalizations, while\nthe physiological include electrocardiography, electromyog-\nraphy, and skin conductance responses.\nThe mainstream deep neural architectures in computer\nvision (CV) are the Convolutional Neural Networks (CNN).\nEspecially in the researcheld of automatic pain assessment\nelaborating images/videos, CNNs are the fundamental com-\nponent of every approach [5]. The domination of transformer\narchitecture [11] in natural language processing (NLP),\nwhere their core element is the self-attention mechanism,\ninspired researchers to develop equivalent models for visual\napplications. The introduction of Vision Transformers (ViT)\n[12] led to the creation of a new paradigm of architecture in\nthe computer vision domain. A plethora of new approaches\nhas developed on the basis of ViT. Such an approach is\nthe Transformer in Transformer (TNT) [13], which enhances\nthe local feature representation by the further division of\nthe patches into sub-patches. Despite the impressive results\nandexibility of the transformer-based models, they scale\npoorly with the input size and increase the computational\ncost because of the self-attention layers which compare the\ninput to every other input. Several efforts have been made\nto reduce the complexity and improve the efciency of such\narchitectures. The primary approach is the replacement of\nself-attention with cross-attention [14] or the incorporation\nof both [15].\nIn this study, we develop a framework consisting of a TNT\nmodel, which is utilized as the“spatial feature extraction\nmodule”applied to each video frame, and a transformer-\nbased model with cross and self-attention blocks as the\n“temporal feature extraction module”applied to each feature\nsequence of videos. In this way, we can exploit the temporal\ndimension of videos and offer more reliable estimations\nabout the continuous nature of the pain sensation. The\nremaining of this study is organized as follows: in Section\n979-8-3503-2447-1/23/$31.00 ©2023 IEEE\n2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) | 979-8-3503-2447-1/23/$31.00 ©2023 IEEE | DOI: 10.1109/EMBC40787.2023.10340872\nThis work is licensed under a Creative Commons Attribution 3.0 License. For more information, see htt p://creativecommons.org/licenses/by/3.0/\nII, we present the related work, and Section III describes the\ndevelopment process of our framework. Section IV presents\nthe conducted experiments and results, andnally, Section\nV concludes the paper.\nII. RELATED WORK\nNumerous research efforts have been made to estimate\nthe pain level in humans utilizing videos. Zhi et Wan [16]\ntrying to capture pain’s dynamic nature, they developed Long\nShort-term Memory Networks with sparse coding (SLSTM).\nTavakolianet al.[17] developed 3D CNNs with kernels of\nvarious temporal depths capturing short, mid, and long-range\nfacial expressions. Similarly, in [18], the authors proposed\na 3D CNN but combined it with self-attention structures\nto increase the importance of specic input dimensions.\nThiamet al.[19] adopted two strategies to exploit video’s\ntemporal dimension. Initially, they encoded the video frames\ninto motion history and opticalow images, and after,\nthey designed a framework incorporating a CNN and a\nbidirectional LSTM (biLSTM). In a similar manner, the\nauthors in [20] also encoded videos into single RGB images\nemploying statistical spatio-temporal distillation (SSD) and\nfollowed by a Siamese network trained in a self-supervised\nsetting. Werneret al.[21] followed a domain-specic feature\napproach, proposing a set of markers describing facial actions\nand their dynamics and classifying them with a deep random\nforest (RF) classier, while Pataniaet al.[22] adopted Deep\nGraph Neural Networks (GNN) architectures and dense maps\nofducial points in order to detect pain. Finally, Xinet\nal.[23] presented a multi-task framework, estimating the\nperson’s identity beyond the pain level, comprising a CNN\nwith an autoencoder attention module.\nRegarding transformer-based methods, the only study pro-\nposed in [24] where the authors developed a deep attention\ntransformer framework that consists of a ResNet subnetwork\nextracting frame-based features and a transformer model\ncapturing the temporal relationship among the frames.\nIII. METHODOLOGY\nThis section describes the employed database, the prepro-\ncessing methods, the design of our framework, as well as\nimplementation details regarding the training procedure.\nA. Preprocessing\nBefore feeding videos into our model for the pain esti-\nmation procedure, it was necessary to apply face detection\nand alignment for performance and computational efciency\nimprovements. We combined the well-known face detector\nMTCNN [25] with the Face Alignment Network (FAN) [26],\nwhich utilizes 3D landmarks. The 3D approach is essential\nto our problem since the head movements in several cases,\nespecially in high-intensity pain, are increased, leading to\nerroneous alignment from 2D approaches. We also note\nthat all the experiments were conducted utilizing frames\nof resolution224×224pixels. Figure 1 depicts the facial\nalignment method applied in a video frame.\nFig. 1: Application of the face alignment. Illustration of the\nlandmarks in 2D space (left) and 3D space (right).\nB. Transformer-based Framework\nOur framework consists of two main components; the\n“spatial feature extraction module”,i.e.,a TNT model, and\nthe“temporal feature extraction module”,i.e.,a transformer\nwith cross and self-attention blocks. In Figure 2, we illus-\ntrate our proposed framework, which consists of24million\nparameters and4.2gigaoating point operations (GFLOPS).\n1) Spatial feature extraction module:Similarly to stan-\ndard ViT, every given frame is initially split intonpatches\nFk = [F k,1, Fk,2, ...F k,n]∈R n×p×p×3, wherep×pis\nthe resolution of each patch (i.e.,16×16) and3is the\nnumber of color channels. Afterward, the patches are further\ndivided intomsub-patches for the model to learn both global\nand local feature representations of the frame. Consequently,\nevery input frame of a video is transformed into a sequence\nof patches and sub-patches:\nFk →[F k,n,1, Fk,n,2..., F k,n,m],(1)\nwhereF k,n,m ∈R s×s×3 is them-th sub-patch ofn-th\npatch ofk-th frame of each video, whiles×sis resolution\nof each sub-patch (i.e.,4×4). Next, the patches and the\nsub-patches with a linear projection are transformed into\nembeddingsZandY. The following step is the position\nembedding, where the spatial information of each patch\nand sub-patch is retained. This procedure is based on the\n1D learnable position encoding, where for each patch, the\nfollowing position encodings is assigned:\nZ0 ←Z 0 +E patch,(2)\nwhereE patch are the patch position encodings. Respectively,\nfor each sub-patch within a patch, a position encoding is\nadded:\nY i\n0 ←Y i\n0 +E sub−patch,(3)\nwhereE sub−patch are the sub-patch position encodings and\ni= 1,2, ...mis the index of a sub-patch within a patch.\nNext, the sub-patches are led to a transformer encoder called\nan “Inner Transformer Encoder”, consisting of2multi-\nhead self-attention blocks, which are essentially dot product\nattention. The attention is expressed as follows:\nAttention(Q, K, V) =softmax\n(QKT\n√dk\nV\n)\n,(4)\nwhereQ∈R M×D ,K∈R M×C andV∈R M×C (M\nis input dimension,CandDare channel dimensions) are\nprojections of the corresponding input and represent the\nQuery, Key and Value matrices respectively. Specically,\nQ=XW Q,K=XW K andV=XW V whereWare\nthe learnable weight matrices andXis the input. The output\nembedding of the “Inner Transformer Encoder” is added\nto the patch embedding, which subsequently is led to the\n“Outter Transformer Encoder”. This encoder consists of3\nmulti-head self-attention blocks, and the output embedding\nof it is a feature vectord= 192. The entire“spatial feature\nextraction module”has a depth of12blocks.\n2) Temporal feature extraction module:The extracted fea-\nture embedding of each frame within a video is concatenated\ninto a larger vectorDwhich essentially is a feature repre-\nsentation of the entire videoV⇒D= (d 1_\nd2_\n, ...dk).\nAfterward, the D feature vector is driven into our temporal\nmodule, a transformer model consisting of1cross-attention\nand2self-attention components with a fully connected\nneural network (FCN) after each one. The cross-attention\nintroducing asymmetry into the attention operation reduces\nthe computational complexity and makes our approach more\nefcient. Specically, instead of a projection of the input\nwith dimensions ofM×D, theQin cross-attention is a\nlearned matrix with dimensionsN×D, whereN < M.\nThe self-attention components of this module are identical as\ndescribed in equation 4. The cross and self-attention blocks\nare1and8multi-heads, respectively. Furthermore, regarding\nposition encoding, we adopted the Fourier feature position\nencoding [15].\n3) Training Details:Initially, before the automatic pain\nestimation training procedure, we pre-trained our“spatial\nfeature extraction module”with theVGGFace2dataset [27],\nconsisting of more than three million face images from over\nnine thousand people. In Table I, we list the hyper-parameters\nof our method and the augmentation techniques applied.\nC. Database Details\nIn this study, we employed the publicly availableBioVid\nHeat Pain Database[28], which incorporates facial videos,\nelectrocardiograms, electromyograms, and skin conductance\nlevels of87healthy participants (subjects). They were sub-\njected to experimentally induced heat pain atve different\nintensity levels; No pain (NP), mild pain (P 1), moderate\npain (P 2), severe pain (P 3), and very severe pain (P 4). The\nparticipants were stimulated20times for each intensity, thus\ngenerating100samples for every modality. In this work, we\nutilized the videos(87×100 = 8700)from Part A ofBioVid.\nIV. EXPERIMENTS & RESULTS\nIn this section, we present the conducted experiments\nregarding pain estimation. We note that the experiments were\nperformed in binary and multi-level classication settings.\nSpecically, (1) NP vs. P 1, (2) NP vs. P 2, (3) NP vs. P 3, (4)\nNP vs. P 4 respecting the binary classication tasks, and-\nnally, (5) multi-level pain classication, utilizing all the avail-\nable pain classes of the database. The evaluation protocol\nthat we followed is the leave-one-subject-out (LOSO) cross-\nvalidation. Furthermore, the classication metrics adopted\nTABLE I: T RAINING HYPER -PARAMETERS USED IN OUR\nMETHOD .\nEpochs Optimizer Learning\nrate\nLR\ndecay\nWeight\ndecay\nWarmup\nepochs\n200 AdamW 1e-4 cosine 0.1 5\nLabel\nsmoothing\nDropPath Attention\nDropout\nLoss\nFunction\nAugmentation\nMethods\n0.1 0.1 0.1 Cross\nEntropy\nAugMix&\nTrivialAugment\nDropPath applied to the“spatial feature extraction module”, Attention Dropout\napplied to the“temporal feature extraction module”\nin this study are the following: micro-average accuracy,\nmacro-average precision, macro-average recall (sensitivity),\nand macro-average F1 score.\nA. Pain Estimation\nRegarding the classication results of the pain estimation\ntasks, we observe the following: on NP vs. P 1, we achieved\n65.95%accuracy, while the precision is close to it with\n65.90%. Similarly, the F1 score is65.04, and interestingly\nthe recall (sensitivity) is 67.85%. On NP vs. P 2, the accuracy\nincreased to66.87%as also the other performance metrics,\nespecially the F1 score, which increased over1.15%showing\nthe improvement in the detection of true positive samples.\nOn NP vs. P 3, the increase in the performances is particularly\nnoticeable. We attained69.22%accuracy, while the sensitiv-\nity improved to70.84%. The classication improvement is\nreasonable since the pain is characterized as severe at the P 3\nlevel, and the subjects’ manifestations become more intense.\nOn the task with the higher level of pain,i.e.,NP vs. P 4, the\nrecall is74.75%, while in terms of accuracy, we achieved\n73.28%. It is evident that recognizing very severe pain is the\nmost straightforward identication task considering that the\npain threshold is on the tolerance limits, and most subjects\ndemonstrate it clearly with their facial expressions. Finally,\nthe range of performances is diminished in the last task,\ni.e.,the multi-level classication, since estimating all levels\nsimultaneously is a more challenging procedure. We attained\n31.52%accuracy and recall of29.94%, indicating that the\nability to detect true positive samples in this task has more\nchallenges.\nAt this point, we want to highlight that our framework\nregarding both the architecture and the training procedure\nremained identical across all tasks, binary and multi-level\nclassication tasks. Our purpose was to study the general-\nization capabilities of our method for every possible scenario\n(within the limits of the database) similar to clinical settings.\nTable II presents the classication results.\nB. Video Sampling\nIn this section, we study the effect of video sampling on\nautomatic pain estimation. The experiments in IV-A were\nconducted utilizing all the available frames (i.e.,138) from\neach video. In the following experiments, we sample frames\nwith a stride of2,3, and4. Initially, utilizing all138frames\nleads to a video feature representationDwith a size of\nVideo\nFramek\nPatch1 Patch2 PatchnSub-patch1,m Sub-patch2,m\n...\nSub-patchn,m\nLinear Projection\nPatch + Potition Embedding\n...\n...\nInner\nTransformer\nEncoder\nInner\nTransformer\nEncoder\nOuter Transformer Encoder\nInner\nTransformer\nEncoder\nSpatial Feature\nExtraction\nModule\n...\nConcatenation\n...\nTemporal Feature Extraction\nModule\n1x\nSelf-Attention\nFCN\nFCN\nFCN\nSelf-Attention\nCross-Attention\nPain Estimation\nat Video Level\nFig. 2: An overview of our proposed framework for automatic pain estimation\nTABLE II: C LASSIFICATION RESULTS ON THE PAIN ESTI -\nMATION TASKS .\nMetric\nTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAcc. 65.95 66.87 69.22 73.28 31.52\nPre. 65.90 66.89 69.18 73.31 31.48\nRec. 67.85 68.34 70.84 74.75 29.94\nF1 65.04 66.19 68.54 72.75 27.82\nAcc.: accuracy Pre.: precision Rec.: recall NP: no pain P 1: mild pain P 2: moderate\npain P 3: severe pain P 4: very severe pain MC: multi-level classication\n138×192 = 26496. Likewise, a stride of2leads to69\nframes, and a size ofDequals69×192 = 13248. For\nstrides3and4, we have 46 and 35 frames, respectively,\nand sizes ofDequal8832and6720. Table III presents the\nclassication accuracy using the different number of input\nframes for the corresponding pain estimation tasks. At the\nsame time, Figure 3 illustrates the impact of the number of\nframes on the mean accuracy across theve tasks and the\nmean runtime during inference. We observe an increase in\nperformance of about1.38%, utilizing138frames compared\nto35frames. Respectively, the runtime is increased by a\nfactor of3. Despite the fact of the multiply of runtime, every\nsampling rate choice can achieve real-time automatic pain\nestimation in situations where it is needed.\nTABLE III: C LASSIFICATION RESULTS ON THE PAIN ES -\nTIMATION TASKS UTILIZING A DIFFERENT NUMBER OF\nINPUT FRAMES , REPORTED ON ACCURACY %.\nNumber of\nFrames\nTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\n138 65.95 66.87 69.22 73.28 31.52\n69 65.76 66.74 69.15 73.25 31.29\n46 65.66 66.70 68.50 71.78 31.20\n35 65.40 66.12 68.32 72.01 30.80\nC. Interpretation\nAn important area of research, especially in the deep\nlearning-relatedelds, is the interpretability of the models\nto provide explanations for the decisions making. This is\nespecially true regarding healthcare topics since the trans-\nparency improvement of these models is essential for their\nacceptance and adoption in the clinical domain. In this study,\nwe adopted the method of [29] to create relevance maps\ndisplaying in which facial areas our model,i.e.,the“spatial\nfeature extraction module”, pays attention. Examples of the\nrelevance maps are shown in Figure 4. We notice that in the\ninitiation of a facial expression sequence, the model attends\nin “arbitrary” areas. As the pain progression continues, the\nattention becomes more precise to regions that manifest the\npainful occurrence. We want to point out that according to\n60,40\n60,60\n60,80\n61,00\n61,20\n61,40\n35 46 69 13 8\n16,00\n22,80\n29,60\n36,40\n43,20\n50,00\n35 46 69 13 8\nMean Accuracy %\nRuntime (ms)\nNumber of In put Frames Number of In put Frames\nFig. 3: The effect of the number of input frames on the\naccuracy (left) and the effect on the runtime in milliseconds\n(right). Runtime is during inference on a Nvidia RTX-3090.\nthe relevance maps, no universal expressions describe pain\nexclusively. Nevertheless, we recognize a tendency toward\ngeneral facial regions such as the mouth and eyes.\nD. Comparison with existing methods\nFinally, in this section, we compare our accomplished\nresults employing the transformer-based framework (using\nall available frames per video) with studies that utilize the\nPart A of theBioViddatabase with all87subjects and follow\nthe identical evaluation protocol,i.e.,leave-one-subject-out\n(LOSO) cross-validation, in order to perform objective and\naccurate comparisons. Table IV shows the corresponding re-\nsults, where there are three main groups of studies;i)studies\nconducting exclusive pain detection (NP vs. P 4),ii)studies\nexaming pain detection and multi-level pain estimation, and\nnally,iii)studies exploring all the major pain-related tasks.\nOur approach, comparing it with the studies that conducted\nexperiments on every task, achieved the highest perfor-\nmances on both binary and multi-level pain estimations.\nRegarding the studies that were performed solely on pain\ndetection or/and multi-level pain estimation, our method\nattained comparable or even better results,e.g.,[19][20][22].\nWe observe that the performances come from the restricted\nin terms of experiments studies tend to be higher. In our\nview, however, the importance of researching and developing\nsystems capable of performing adequately in every scenario\nis greater.\nV. C ONCLUSIONS\nThis study explored the application of the transformer-\nbased architecture for automatic pain estimation using\nvideos. We developed a framework that consisted exclu-\nsively of transformer models, exploiting both the spatial\nand temporal dimensions of the frame sequences. The con-\nducted experiments revealed the efcacy of our framework\nin assessing pain and demonstrating the generalization ca-\npabilities to accomplish every pain estimation task with\nsatisfactory classication results, especially in low-intensity\npain where the facial expressions are subtle. Furthermore,\nwe showed that our proposed framework is characterized by\nhigh efciency and is able to perform in real-time settings.\nAnother important aspect of our study is the creation of\nrelevance maps demonstrating to which facial areas the\nmodel pays attention. We believe that more efforts from\nthe affective-computing community are needed to improve\nthe interpretability of the adopted deep-learning approaches.\nIn addition, we suggest that future studies include details\nregarding the computational cost of their approaches,e.g.,\nthroughput measurements, number of model parameters, or\nnumber of FLOPS, to assess their real-time application.\nAlthough it may not be the primary focus of the studies, it\nis still relevant. The comparison with other related methods\ndemonstrated comparable or improved results depending on\nthe corresponding training scenario. We believe that future\nresearch regarding the automatic pain estimationeld needs\nto investigate all available tasks since they clinically provide\nessential information for pain management.\nIt is worth noting that our current approach may benet\nfrom the utilization of more complex modules. Specically,\nincreasing the number of attention heads and blocks in\nthe transformer models, or enlarging the extracted feature\nvectors, would result in a more comprehensive representa-\ntion of the data. However, it should be acknowledged that\nimplementing such modications would also come with a\nsignicant increase in computational cost and time require-\nments.\nACKNOWLEDGMENT\nThis work has received funding from the European\nUnion’s Horizon 2020 research and innovation program\nunder grant agreement No945175(Project:CARDIOCARE).\nThis paper reects only the author’s view and the Commis-\nsion is not responsible for any use that may be made of the\ninformation it contains.\nREFERENCES\n[1] A. C. de C Williams and K. D. Craig, “Updating the denition of\npain.,”Pain, vol. 157, pp. 2420–2423, 11 2016.\n[2] K. S and T. RS, “Neuroanatomy and neuropsychology of pain,”\nCureus, vol. 9, 10 2017.\n[3] “Global, regional, and national incidence, prevalence, and years lived\nwith disability for 354 Diseases and Injuries for 195 countries and\nterritories, 1990-2017: A systematic analysis for the Global Burden of\nDisease Study 2017,”The Lancet, vol. 392, pp. 1789–1858, nov 2018.\n[4] D. C. Turk and R. Melzack, “The measurement of pain and the\nassessment of people experiencing pain.,” pp. 3–16, 2011.\n[5] S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\ndeep learning methods: A systematic review,”Computer Methods and\nPrograms in Biomedicine, vol. 231, p. 107365, 2023.\n[6] P. Dinakar and A. M. Stillman, “Pathogenesis of Pain,”Seminars in\nPediatric Neurology, vol. 23, pp. 201–208, aug 2016.\n[7] B. G. S. Dekel, A. Gori, A. Vasarri, M. C. Sorella, G. Di Nino, and\nR. M. Melotti, “Medical evidence inuence on inpatients and nurses\npain ratings agreement,”Pain Research and Management, vol. 2016,\n2016.\n[8] H. H. Yong, S. J. Gibson, D. J. Horne, and R. D. Helme, “Development\nof a pain attitudes questionnaire to assess stoicism and cautiousness\nfor possible age differences.,”The journals of gerontology. Series B,\nPsychological sciences and social sciences, vol. 56, pp. P279–84, sep\n2001.\n[9] E. J. Bartley and R. B. Fillingim, “Sex differences in pain: a brief\nreview of clinical and experimentalndings,”British journal of\nanaesthesia, vol. 111, pp. 52–58, jul 2013.\n[10] S. Gkikas., C. Chatzaki., E. Pavlidou., F. Verigou., K. Kalkanis.,\nand M. Tsiknakis., “Automatic pain intensity estimation based on\nelectrocardiogram and demographic factors,” pp. 155–162, SciTePress,\n2022.\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\ninProceedings of the 31st International Conference on Neural In-\nformation Processing Systems, NIPS’17, (Red Hook, NY , USA),\np. 5998–6008, Curran Associates Inc., 2017.\nFig. 4: Relevance Maps\nTABLE IV: C OMPARISON OF STUDIES THAT UTILIZED BIOVID, VIDEOS ,\nAND LOSO CROSS -VALIDATION\nStudy\nTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nTavakolian & Hadid. [17] - - - 86.02 -\nThiam et al. [19] - - - 69.25 -\nTavakolian et al. [20] - - - 71.02 -\nPatania et al. [22] - - - 73.20 -\nHuang et al. [18] - - - 77.50 34.30\nXin et al. [23] - - - 86.65 40.40\nZhi & Wan [16] 56.50 57.10 59.60 61.70 29.70\nWerner et al. [21] 53.30 56.00 64.00 72.40 30.80\nOur approach 65.95 66.87 69.22 73.28 31.52\n[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,”arXiv preprint arXiv:2010.11929, 2020.\n[13] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, “Transformer\nin transformer,”Advances in Neural Information Processing Systems,\nvol. 34, pp. 15908–15919, 2021.\n[14] J. Lee, Y . Lee, J. Kim, A. Kosiorek, S. Choi, and Y . W. Teh, “Set\ntransformer: A framework for attention-based permutation-invariant\nneural networks,” inProceedings of the 36th International Conference\non Machine Learning(K. Chaudhuri and R. Salakhutdinov, eds.),\nvol. 97 ofProceedings of Machine Learning Research, pp. 3744–3753,\nPMLR, 09–15 Jun 2019.\n[15] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and\nJ. Carreira, “Perceiver: General perception with iterative attention,” in\nInternational conference on machine learning, pp. 4651–4664, PMLR,\n2021.\n[16] R. Zhi and M. Wan, “Dynamic facial expression feature learning based\non sparse RNN,” inProceedings of 2019 IEEE 8th Joint International\nInformation Technology and Articial Intelligence Conference, ITAIC\n2019, pp. 1373–1377, Institute of Electrical and Electronics Engineers\nInc., may 2019.\n[17] M. Tavakolian and A. Hadid, “A Spatiotemporal Convolutional Neural\nNetwork for Automatic Pain Intensity Estimation from Facial Dynam-\nics,”International Journal of Computer Vision, vol. 127, pp. 1413–\n1425, oct 2019.\n[18] D. Huang, X. Feng, H. Zhang, Z. Yu, J. Peng, G. Zhao, and Z. Xia,\n“Spatio-temporal pain estimation network with measuring pseudo\nheart rate gain,”IEEE Transactions on Multimedia, vol. 24, pp. 3300–\n3313, 2022.\n[19] P. Thiam, H. A. Kestler, and F. Schwenker, “Two-stream attention\nnetwork for pain recognition from video sequences,”Sensors (Switzer-\nland), vol. 20, p. 839, feb 2020.\n[20] M. Tavakolian, M. Bordallo Lopez, and L. Liu, “Self-supervised pain\nintensity estimation from facial videos via statistical spatiotemporal\ndistillation,”Pattern Recognition Letters, vol. 140, pp. 26–33, 2020.\n[21] P. Werner, A. Al-Hamadi, K. Limbrecht-Ecklundt, S. Walter, S. Gruss,\nand H. C. Traue, “Automatic pain assessment with facial activity\ndescriptors,”IEEE Transactions on Affective Computing, vol. 8, no. 3,\npp. 286–299, 2016.\n[22] S. Patania, G. Boccignone, S. Bur ˇsi´c, A. D’Amelio, and R. Lanzarotti,\n“Deep graph neural network for video-based facial pain expression\nassessment,” inProceedings of the 37th ACM/SIGAPP Symposium on\nApplied Computing, pp. 585–591, 2022.\n[23] X. Xin, X. Li, S. Yang, X. Lin, and X. Zheng, “Pain expression\nassessment based on a locality and identity aware network,”IET Image\nProcessing, vol. 15, no. 12, pp. 2948–2958, 2021.\n[24] H. Xu and M. Liu, “A deep attention transformer network for pain\nestimation with facial expression video,” inChinese Conference on\nBiometric Recognition, pp. 112–119, Springer, 2021.\n[25] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao, “Joint face detection and\nalignment using multitask cascaded convolutional networks,”IEEE\nsignal processing letters, vol. 23, no. 10, pp. 1499–1503, 2016.\n[26] A. Bulat and G. Tzimiropoulos, “How far are we from solving the\n2d & 3d face alignment problem?(and a dataset of 230,000 3d facial\nlandmarks),” inProceedings of the IEEE International Conference on\nComputer Vision, pp. 1021–1030, 2017.\n[27] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, “Vggface2:\nA dataset for recognising faces across pose and age,” in2018 13th\nIEEE international conference on automatic face & gesture recogni-\ntion (FG 2018), pp. 67–74, IEEE, 2018.\n[28] S. Walter, S. Gruss, H. Ehleiter, J. Tan, H. C. Traue, S. Crawcour,\nP. Werner, A. Al-Hamadi, A. O. Andrade, and G. M. D. Silva, “The\nbiovid heat pain database: Data for the advancement and systematic\nvalidation of an automated pain recognition,” pp. 128–131, 2013.\n[29] H. Chefer, S. Gur, and L. Wolf, “Transformer interpretability beyond\nattention visualization,” inProceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 782–791, 2021.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7895830869674683
    },
    {
      "name": "Computer science",
      "score": 0.6357402801513672
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4216425120830536
    },
    {
      "name": "Machine learning",
      "score": 0.35416319966316223
    },
    {
      "name": "Engineering",
      "score": 0.21830657124519348
    },
    {
      "name": "Electrical engineering",
      "score": 0.10325375199317932
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I28710699",
      "name": "Hellenic Mediterranean University",
      "country": "GR"
    },
    {
      "id": "https://openalex.org/I8901234",
      "name": "Foundation for Research and Technology Hellas",
      "country": "GR"
    }
  ]
}