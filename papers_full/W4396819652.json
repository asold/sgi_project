{
  "title": "Contextual Spelling Correction with Language Model for Low-Resource Setting",
  "url": "https://openalex.org/W4396819652",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5097372571",
      "name": "Nishant Luitel",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5097372572",
      "name": "Nirajan Bekoju",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5111222839",
      "name": "Anand Kumar Sah",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102716470",
      "name": "Subarna Shakya",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2016871293",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6855531465",
    "https://openalex.org/W6854442353",
    "https://openalex.org/W4386543119",
    "https://openalex.org/W2006642610",
    "https://openalex.org/W2741908291",
    "https://openalex.org/W6787019166",
    "https://openalex.org/W6843566661",
    "https://openalex.org/W2057900969",
    "https://openalex.org/W2151504427",
    "https://openalex.org/W3103161287",
    "https://openalex.org/W6852894773",
    "https://openalex.org/W2885301267",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4394828356",
    "https://openalex.org/W6963512413",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W6779068807",
    "https://openalex.org/W4225383331",
    "https://openalex.org/W2916997151"
  ],
  "abstract": "The task of Spell Correction(SC) in low-resource languages presents a\\nsignificant challenge due to the availability of only a limited corpus of data\\nand no annotated spelling correction datasets. To tackle these challenges a\\nsmall-scale word-based transformer LM is trained to provide the SC model with\\ncontextual understanding. Further, the probabilistic error rules are extracted\\nfrom the corpus in an unsupervised way to model the tendency of error\\nhappening(error model). Then the combination of LM and error model is used to\\ndevelop the SC model through the well-known noisy channel framework. The\\neffectiveness of this approach is demonstrated through experiments on the\\nNepali language where there is access to just an unprocessed corpus of textual\\ndata.\\n",
  "full_text": "Contextual Spelling Correction with Language\nModel for Low-resource Setting\nNishant Luitel 1, Nirajan Bekoju 2, Anand Kumar Sah 3 and Subarna Shakya 4\nDept. of Electronics and Computer Engineering,\nPulchowk Campus, Tribhuwan University,\nLalitpur, Nepal\n076bct041.nishant@pcampus.edu.np1, 076bct039.nirajan@pcampus.edu.np 2, anand.sah@pcampus.edu.np 3, drss@ioe.edu.np 4\nAbstract—The task of Spell Correction(SC) in low-resource\nlanguages presents a significant challenge due to the availability\nof only a limited corpus of data and no annotated spelling\ncorrection datasets. To tackle these challenges a small-scale\nword-based transformer LM is trained to provide the SC model\nwith contextual understanding. Further, the probabilistic error\nrules are extracted from the corpus in an unsupervised way\nto model the tendency of error happening(error model). Then\nthe combination of LM and error model is used to develop the\nSC model through the well-known noisy channel framework.\nThe effectiveness of this approach is demonstrated through\nexperiments on the Nepali language where there is access to\njust an unprocessed corpus of textual data.\nIndex Terms—Noisy channel, Language model, Error model,\nUnsupervised\nI. Introduction\nAccurate and reliable written communication is fundamental\nin various domains, from academia to professional settings,\nand its importance cannot be overstated. Spelling mistakes\nprovide a serious problem since they can make communi-\ncation and comprehension difficult. The task of spelling\ncorrection(SC), therefore, stands as an important component\nin natural language processing (NLP) systems.\nSpelling errors generally manifest in two primary cate-\ngories: real-word errors and non-word errors [1]. Real-word\nerrors refer to instances where a misspelled word exists as\na valid word in the language. These errors often result\nin the unintentional usage of a correctly spelled word that,\nunfortunately, does not convey the intended meaning in the\ngiven context. For example, ‘हात धुनुहो् र सवा जीवन जउनुहो्\n।’ translates to wash your hands to stay healthy. However if\nthe word ‘हात’ is changed to ‘हार’ which is within a single\nedit distance of the original word meaning ‘hand’, we get a\nreal word that translates to ‘a piece of jewelry which doesn’t\nmake sense contextually. Conversely, non-word errors involve\nmisspellings that result in sequences of characters that do not\nconstitute valid words in the language. A decent spelling\ncorrection model should be able to handle both types of errors.\nIn this paper, the statistical technique of noisy channels\nto perform spelling correction is focused. The term ‘noisy\nchannel’ originates from information theory, where it refers\nto a communication channel that introduces random noise into\ntransmitted signals. Instead of attempting to directly identify\nand correct errors in the observed text, this approach involves\nestimating the most likely intended message given the ob-\nserved text and the probabilities of different error sources [2].\nKey components of noisy channel spelling correction include\nlanguage models, error models, and candidate generation.\nAs pointed out already, the correction of real word errors\nrequires contextual understanding. Contextual understanding\nin a text can be captured using a Language model. Recent\nlanguage models that are trained on a huge corpus of data\nare shown to have the ability to do tasks in a few-shot setting\n[3]. One could try using such LLMs to build context-sensitive\nspell checkers [4], [5]. However, training such huge LMs\nis expensive, and for low-resource languages, enough quality\ndatasets are a bottleneck. A method of spelling correction\nthat requires a minimal amount of resources is proposed and\na decent small transformer-based LM on a corpus of only 1.2\nGB data is trained. LMs provide probability that a given\nword occurs in the presence of its context. Hence, the list\nof candidate words can be ranked based on the contextual\nprobability given by LMs.\nAlthough language models provide good contextual un-\nderstanding, error model is required that provides a higher\nweighting to candidate words that are likely to generate the\nerror words. In other words, though some candidate words can\nbe contextually appropriate they can still not be the intended\nword because they are unlikely to be typed as the observed\nnoisy word. Hence, balanced weighting from both LMs and\nthe error model is necessary. This is equivalent to performing\nBayesian inference to determine the posterior distribution as\ndescribed in section III.\nA simple way to generate candidates for words includes\nsearching a fixed vocabulary to find real words that are within\na given edit distance. An edit distance of 2 is a decent choice.\nHowever, for smaller words, even with an edit distance of\n1 can result in a large number of candidates which can be\ndifficult to handle computationally. Hence, to reduce the\nnumber of candidates for every word, a simple method is\ndeveloped as described in section III-A.\nDue to the intrinsic difficulty of Nepali script as well as\nthe influence of English medium communication, the quality\nof published Nepali text in the media has been decreasing.\nHence, the Nepali language can benefit greatly from the use of\nautomatic spelling correction systems. The main contributions\nof this paper are as follows:\n1) An autoregressive language model based on word-based\nvocabulary with around 350,000 tokens is trained.\n2) An error model is trained from a corpus of text in an\nunsupervised way and proposed a simple algorithm to\nadd noise that resembles human-like errors. This can be\nused to create SC datasets.\n3) Comparative study is performed based on character and\nword level accuracy using 2 error models and 5 variants\nof language models. Ablation study is performed to\nshow the level of corrective behavior using only the\nerror model. We have made our code open via GitHub 1.\nII. Related Works\nSpelling correction (SC) systems in the Nepali language\nhave predominantly been dictionary-based, relying on manual\nrules to generate comprehensive lists of words [6], [7]. These\nsystems utilize the Hunspell framework. A different approach\nused includes using deep learning to perform end-to-end\ntraining [8]. However, other successful approaches such as\nthe noisy channel model haven’t been tried for Nepali which\nis explored in this paper.\nSC generally involves the detection of error words, find-\ning candidates for correction, ranking the candidates, and\ngenerating noisy datasets either for end-to-end training or\nevaluation. Detection of error is done mostly by recognizing\nout-of-vocabulary(OOV) words. However, recognizing only\nOOV words as errors don’t lead to spell checking on real\nword errors [9]. Hence, to correct real word errors, every\nword is considered to be a likely noisy word and candidates\nare generated for each word. These candidates are generated\neither using Damerau-Levenshtein edit distance directly to\nerror word [10] or by using edit distance on metaphones\n[9]. Authors in [11] use both DL edit distance of up to 2\nand double metaphone to generate candidates of edit distance\n3. Besides using edit distance, using language models to\nproduce top probability candidates has been another popular\nchoice [12], [13]. Nonetheless, this may not be an ideal\nchoice if the language model itself has a noisy or misspelled\nvocabulary. Once the candidates are generated they have to be\nranked. Noisy channel models rank the candidates based on\nfull Bayesian inference i.e. combined weighting of the error\nmodel(likelihood) and language model(prior) [1], [2]. Error\nmodels that have been used include equal distribution over\ncandidates [2], the inverse of edit-distance [9] and partition\nbased approach developed in [14]. Similarly, language models\nused typically involve probabilistic n-grams [9], [15]. Though\nthe revolution has been seen in language models trained using\nneural networks, direct probability estimates from such models\nhaven’t been used together with error models to calculate\nweightings for the candidates. We intend to do just that. One\napproach that also uses probability estimates from Neural LM\nis taken in [12], where BERT-based LM is used to determine\nthe candidates for each word. They then use edit distance to\nfind words with minimum edit distance among the candidates\n1https://github.com/NishantLuitel/Nepali-Spell-Checker\nto perform the correction. The approach used here contrasts\nwith this, as candidates are initially generated using edit\ndistance and subsequently ranked using Neural LM and the\nchannel model.\nFor low-resource languages, datasets for evaluating spelling\ncorrection models generally don’t exist. Hence, a noisy dataset\nis generated using different methodologies. The simplest\napproach to generating errors is by introducing random ed-\nits (deletion, addition, substitution, and replacement) with\nsome probability to characters but it doesn’t generate realistic\nhuman-like errors [16]. Other approaches include replacing\nsome of the words with their frequent misspellings [12],\n[17] and, finding and applying rules that generally occur in\nmisspellings [16], [18]. Authors in [16] suggest translating\nconfusion matrix in high resource language to low resource\nlanguage via keyboard mapping, but it wouldn’t be relevant for\nlanguage like Nepali where misspellings occur due to intrinsic\nhardness in recognizing spellings rather than the action of\nmistyping. A large dataset of misspelled and corresponding\ncorrect sentences can also facilitate training end-to-end net-\nworks for SC systems [18], [19].\nIII. Methodolody\nA. Candidate Generation\nConsidering all the candidates of observed words within a\ndecent edit distance such as 2 can be quite expensive. To\nreduce the number of possible candidates, A simple heuristic\nis designed. Initially, candidates for words with a length of 3\nor less is generated using an edit distance of 1 and for words\nwith a length larger than 3 using an edit distance of 2 or less.\nIf the candidate list has less than 5 words then we return the\nlist. Otherwise, both the observed word and the words in the\ncandidate list are transliterated to English and, then sort the\npair in ascending order using edit distance calculated on the\ndouble metaphone as the key. Finally, only the candidates\nthat have minimum edit distance with the observed word are\nreturned. Damerau-Levenshtein edit distance is used in this\napproach.\nWhat can be done with words only, can also be done for\nentire sentences. For this [1] suggests combining permutation\nof candidates of each word to form a complete list of candidate\nsentences. Similar to ranking words one can also rank entire\ncandidate sentences. However, this is computationally more\nexpensive than using words only. This is explained in further\nsubsections.\nB. Training an Error Model\nTraining an error model amounts to learningP(x|w)i.e.\nthe probability that the observed noisy word ‘x’ is generated\nfrom the intended word ‘w’. Authors in [14] approximate\nthis likelihood by finding the partitions ‘R’ in observed\nwords and partitions ‘T’ in candidate words that maximize\nit. Mathematically,\nP(x|w)≈max\nR,T s.t.|R|=|T|\n|R|∏\ni=1\n[P(T i|Ri)](1)\nAs seen in eq.1, observed words are partitioned into sub-\nstrings and the substrings are aligned with the partitions of\ncandidate words. To estimate,P(T i|Ri)simple counting-\nbased probability can be calculated for all such substring\nalignments using triples of the correct word, misspelled word,\nand frequency extracted from the corpus in an unsupervised\nway. The creation of these triples, as demonstrated in [15],\ninvolves the extraction of terms with close edit distances.\nThe underlying assumption is that correct words manifest\napproximately 10 times more frequently than their closely\nmisspelled counterparts. In alignment with this methodology,\nthe used approach involves the extraction of terms within\nan edit distance of 2, only considering those that exhibit a\nfrequency fivefold greater than their misspelled counterparts\nwithin the analyzed corpus. We let the maximum length of\neach partition be a single character to keep things simple and,\nallow partitions to include empty string to allow deletion or\naddition.\nFigure 1. is the heat map representing the edit probabilities\nof some devnagari characters(all not included due to space\nlimitation) learned from training the error model. It captures\na larger proportion of the probability for character substitutions\nthat is likely for a human. For example'ि◌' -> '◌ी'and'श' ->\n''substitutions have redder color in the heatmap representing\nlikely human errors. This heatmap of edit probabilities is used\nfor generating errors. The one used for correction is shown in\nthe appendix.\nAdditionally, to build an error model for candidate sen-\ntences, independence is assumed and hence naively multiply\nthe error probability of individual words. For observed\nsentence ‘X’ composed of wordsx 1, ..., xl with candidate cor-\nrection sentence ‘ W ’ composed of wordsw 1, ..., wl,P(X| W )\nis given by,\nP(X| W ) =\nl∏\ni=1\n[P(x i|wi)](2)\nC. Training a Neural LM\nRecent State of the art LMs are trained either using bidirec-\ntional transformers [20], or with autoregressive transformers\n[21], [22]. Similar to probabilistic LMs, autoregressive neural\nLMs also provide the probability value of the next token given\nthe context. However, neural models can more efficiently\nremember contexts than their probabilistic counterparts. In the\nscenario of contextual spelling correction, these models can\nprovide us with a probability of occurring of each candidate\nword for the given context. Hence, the probabilityP(w), of\ncandidate word ‘w ′ to occur for contextual observed words\n(xi−1, xi−2, ...) is given by,\nP(w) =P(w|x i−1, xi−2, ...)(3)\nBERT-based LMs use future context alongside with past\ncontext. Hence, the probabilityP(w)for such bidirectional\ntransformers is given by,\nP(w) =P(w|..., x i+1, xi−1, ...)(4)\nFigure 1: Heat map showing edit probability from the trained\nBM model for error generation\nSimilarly, to calculate the probability of candidate sentence\n‘W ’ composed of individual words (w 1, ..., wl) to occur, we\nagain assume independence and naively multiply the proba-\nbility of occurrence of individual words. Hence,\nP( W ) =\nl∏\ni=1\n[p(wi)](5)\nFor this research, word vocabulary-based autoregressive\nneural transformer is trained using cross-entropy loss which\nallows to work directly with word-level tokenization. It is also\ncompared with BERT-based language models that are openly\navailable [23], [24].\nD. Combining Error Model and LM\nTo rank the candidates, the posterior distribution ‘P(w|x)’\nis calculated by performing full Bayesian inference using\nBayes theorem:\nP(w|x) = P(x|w)P(w)\nP(x) (6)\nHere, the likelihood term corresponds to the error model as\ndefined before and prior as the language model accordingly.\nThe goal is to find the word ‘w’ that maximizes eq.6. Since\nP(x)is constant for any ‘w’, it is safe to ignore the denomi-\nnator and find the most likely intended word as:\nˆw=arg max\nw\nP(x|w)P(w)(7)\nFurther, a hyperparameter ‘λ’ is defined to provide a greater\ndegree of freedom in eq. 7. The value of ‘λ’ is estimated\nby inspecting its result in a few sentences. Hence the final\nequation becomes,\nˆw=arg max\nw\nP(x|w)P(w) λ (8)\nAccordingly, Bayesian inference is applied to rank the candi-\ndate sentences as well.\nˆW =arg maxW\nP(X| W )P(W) λ (9)\nE. Evaluation Dataset\nAround 320 Nepali sentences is gathered and then used the\nerror model trained in sec. III-B to generate errors in them.\nAlgorithm 1. shows the pseudocode for this process. First,\nNonetoken ‘n’ is added between each character in every\nword but only if both preceding and successive character\nisn’t a character modifier. Character modefiers in Nepali\nincludes characters'◌े','◌ै','◌ो','◌ौ','◌्','ॎ◌','ि◌','◌ी','◌ु','◌ू','◌ृ'\nand'◌ा'. Then, for each character in aNoneadded word,\nwith_probabilityfunction is used to return eitherTrueorFalse\nwith probability proportional to the frequency of that character\nin our error model as returned by the functioncount_probab.\nFinally, if the corresponding character has returnedTrue,\nwe look at that character’s dictionary with probabilities to\nget replaced with other characters(returned bydict_selector\nfunction) and sample a character using that distribution.\nAlgorithm 1:Pseudocode for error generating function\nwith python-like syntax\nData:tokens (set of words in a sentence)\n(1) fori←0tolen(tokens)−1do\n(2) forj, ttoenumerate(tokens[i])do\n(3) temp←temp + t\n(4) ifj==len(tokens[i])−1then\n(5) continue;\n(6) ift/∈NepaliCharacter Modifiersand\ntokens[i][j+ 1] /∈\nNepaliCharacter Modifiersthen\n(7) temp←temp + ‘n’\n(8) result←[with_probability(count_probab[t])if\nt∈dict_selectorelse False fortintemp]\n(9) li←[chif(b̸=True orch/∈dict_selector)else\nsample_from_probability(dict_selector[ch])for\nch, binzip(temp,result)]\n(10) tokens[i]←‘’.join([ ‘’if\nchis None orch==‘n’ elsechforchinli])\n(11) returntokens\nTo make a fair comparison of this method, two error models\nare trained on two different corpora. Error model trained from\nthe dataset ‘A large scale Nepali text Corpus’ [25] was used for\ngenerating the evaluation dataset whereas, the model trained\nfrom ‘Oscar’ dataset [26] was used during actual evaluation.\nIV . Experimental Setup\nWe try out the combination of two error models and five\nlanguage models to evaluate the generated dataset.\nA. Error Models\n1)Constant distributive(CD): In this error model, ‘α’\nparameter is set which represents the probability that\nthe observed word is correct i.e.P(w|w)then, we\nuniformly distribute ‘1−α’ over all the candidates of\n‘x’ except ‘w’ [2]. Mathematically,\nThe absolute value ofxis defined\nP(x|w) =\n\n\n\nαifx=w\n1−α\n|C(x)|−1 ifx∈C(x)\n0ifx/∈C(x)\nWe choose the value ofαto be 0.65.\n2)Brill and Moore(BM): BM error model was trained\nas described in section III-B with two corpus of data.\nTraining performed in the ‘Oscar’ dataset [26] was used\nfor evaluation.\nB. Language Models\n1)KnLM-2: Probabilistic bigram language model is\ntrained using [26] dataset with Kneser-Ney smoothing\nwhich allows backing off to unigrams when bigram\nisn’t available. Inference using n-gram language models\nare fast because they involve dictionary lookup. How-\never, the memory requirement increases drastically with\nincreasing ‘n’ so it is not efficient to capture longer\ncontexts. We use candidate sentence ranking with this\nmodel.\n2)NepaliBert: NepaliBert was trained by [23] in Masked\nLanguage modeling fashion using the BERT architecture\ndescribed in [20]. Candidate sentence ranking is used\nwith this model.\n3)deBerta: deBerta is the Bert-based Nepali LM trained\nby [24] following the approach used in [27]. Basically,\ninstead of using the sum of positional encoding with\nthe embedding of tokens, deBerta model uses a separate\nembedding to positional encoding as well and uses\nthe disentangled attention calculated from both relative\nposition and content. Like NepaliBert, we again use\ncandidate sentence ranking.\n4)Trans: We have trained an autoregressive LM with 4\ntransformer encoders and the same number of decoders\non the word-based vocabulary of around 350,000 using\nthe Oscar corpus [26]. An embedding size of 300 was\nused with four attention heads in the transformer. The\ndropout value was 0.05. It was trained with cross-\nentropy loss. The trans model also uses candidate\nsentence correction.\n5)Trans-word: This is the same language model as de-\nscribed in 4. However, it uses candidate word correction\nrather than candidate sentences.\nC. Evaluation Metrics\nSC models are evaluated and compared based on the fol-\nlowing metrics:\n1)Word Error Rate(WER): WER is the ratio of the total\nnumber of word-level errors to the total number of words\nin the reference text. A lower WER indicates better\nperformance in terms of word-level accuracy.\n2)Word Accuracy: Word accuracy is computed by di-\nviding the number of accurately corrected words by the\ntotal number of error words present in the text. A higher\naccuracy indicates a better SC model.\n3)Character Accuracy: To calculate character accuracy,\nalignment is created between each word in the generated\ndataset and the corresponding ground truth word. Then\nthe ratio taken between all the corrected characters to\nthe total error character represents character accuracy.\nV . Result\nTables I, II, and III show WER, word accuracy, and\ncharacter accuracy respectively for two error models and 5\nvariants of language models used as described in section IV.\nWER calculation is found to be 0.25 when no correction\nwas applied. When this score is compared after correction\nwith different models, it is observed that the Trans version\ndecreases the WER to lowest among other models. Bert-based\nmodels introduce more error after correction suggesting over-\ncorrection. WER score is lower for BM models compared to\nCD models except for NepaliBert. The correct label dataset is\nitself noisy, hence even the accurate corrections for some cases\nhave increased the WER. The only way to get rid of this noise\nis to manualy correct the misspellings(not performed here) in\nthe original dataset considered as the correct label dataset.\nIn terms of word accuracy, Trans with BM is best per-\nforming with an accuracy of about 69.1%. The performance\nof probabilistic LM(Knlm-2) and Trans-word are comparable.\nAgain the Bert-based models are heavily outperformed by\nother models. CD models tend to be better than BM models in\nterms of character accuracy. We note that character accuracy\nis higher than word accuracy for corresponding models. This\nmeans that there are some error words where few but not all\nof the characters are corrected.\nAlthough Bert-based language models don’t match the\nperformance of other models, it doesn’t mean that they are\nless capable as a language model. It probably means that\nour method of correction doesn’t work well with BERT-like\nmodels.\nTable I: Word error rate(WER) after correction.\nKnlm-2 deBerta NepaliBert Trans Trans-word\nCD 0.201 0.372 0.321 0.145 0.168\nBM 0.187 0.299 0.248 0.119 0.166\nVI. Ablation study\nAblation study is performed by removing the language\nmodel entirely from the probability calculations. The objective\nTable II: Word accuracy\nKnlm-2 deBerta NepaliBert Trans Trans-word\nCD 0.601 0.403 0.384 0.665 0.599\nBM 0.647 0.439 0.422 0.691 0.647\nTable III: Character accuracy\nKnlm-2 deBerta NepaliBert Trans Trans-word\nCD 0.671 0.535 0.492 0.744 0.694\nBM 0.683 0.495 0.443 0.690 0.695\nwas to observe and compare whether the error model by itself\nhas some corrective abilities. From the Bayesian perspective,\nthe prior is constant which amounts to performing a maximum\nlikelihood estimate with possible candidates. Hence, the\ncandidate word, that is most likely to result in the given\nerror ignoring how likely it is to occur within the context, is\nchosen. However, the error model will always return a larger\nprobability score for actually observed words even if they are\nnot intended so we have chosen to use a correctly spelled\nvocabulary list of around 125000 words and to provide a larger\nnegative score if the observed word isn’t in the vocabulary.\nThis way error model compares the words that are in the vocab\nto find a maximum likelihood candidate. The vocabulary\nlist was formed by extracting Nepali words from the Nepali\ndictionary using OCR.\nTable IV: Word Candidate based(Ablation study)\nWER Word acc. Char acc.\nCD 0.286 0.395 0.532\nBM 0.186 0.457 0.485\nTable V: Sentence Candidate based(Ablation study)\nWER Word acc. Char acc.\nCD 0.286 0.395 0.532\nBM 0.188 0.371 0.358\nThe result shown in table IV is the performance of the\nSC model where word-based correction is used. It can be\nobserved that the WER has dropped from 0.25 to 0.186 for\nthe BM model. However, for the CD model, the WER has\nincreased to 0.286 showing an overcorrecting nature. Hence,\nit can be concluded from this analysis that the BM model has\na better correcting ability than the CD model. However, the\ncharacter accuracy for CD is higher than that of BM. Similarly,\nthe result shown in table V uses sentence-based correction.\nThis was analyzed only for 200 sentences. Again, it can be\nobserved that the BM model better captures the errors than CD\nin terms of word accuracy and WER but fails to outperform\non character accuracy.\nVII. Scope and Limitations\nSC systems find many applications both in assisting humans\nto write correctly as well as within the context of Machine\nLearning. SC can also be employed to preprocess the input\ntext to tackle adversarial attacks with misspellings [17], [28].\nIt can be used to create higher-quality NLP datasets. More-\nover, it can also be applied to correct misspelled tokens by\npost-processing outputs from applications such as OCR and\nASR [29].\nOne limitation of our method is that it only handles the\ncases where input tokens are the same number as the correct\nlabel tokens. Hence, two tokens ‘म लाई’ is not corrected to\nthe correct token ‘मलाई’. Additionally, an error model have\nbeen employed that learns human-like errors from the corpus,\nwhich is not suitable for usage in situations where the error-\ngenerating distribution is much different, such as correcting\nOCR predictions. Similar can be said about the post-correction\nof ASR system outputs. Nonetheless, it is hypothesized that\nthe method proposed might exhibit some corrective behavior\neven in those situations due to reasonable candidate creation.\nVIII. Future Work\nFuture direction that can be taken is to train the model end\nto end using seq2seq architectures in subword or word level\nusing error generation process to create the larger datasets. In\nthe context of the noisy channel method, this is equivalent to\nlearning both the error model and prior jointly with the same\nnetwork. Additionally, introducing errors by splitting a correct\nword or joining two consecutive separate words can be used\nto handle the cases of different length input and correct label\ntokens.\nFurther, enhancement can be done to the model by intro-\nducing NER to correctly recognize and ignore correcting name\nentities in the sentence.\nIX. Conclusion\nAlthough neural architectures have been shown to have\ngreat generalizing ability, they require large amounts of data.\nFor low-resource languages such as Nepali, one has to rely\non unsupervised methods to tackle problems such as SC. In\nthis paper, it is shown with just a single noisy corpus of text,\none can train both an error model and, an LM using SOTA\ntechniques to create a decent SC system using a noisy channel\napproach. Furthermore, this approach can be adapted to any\nlanguage.\nAcknowledgment\nWe are extremely thankful to all the researchers for making\nopen the language models [23], [24] and the datasets [25],\n[26] used in this paper.\nReferences\n[1] D. Jurafsky and J. H. Martin, “Spelling correction and the noisy\nchannel,” inSpeech and Language Processing (3rd ed. draft), 2024.\nChapter on the Stanford University website.\n[2] E. Mays, F. J. Damerau, and R. L. Mercer, “Context based spelling\ncorrection,”Information Processing and Management, vol. 27, no. 5,\npp. 517–522, 1991.\n[3] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nV oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei, “Language models are few-shot learners,” 2020.\n[4] X. Zhang, X. Zhang, C. Yang, H. Yan, and X. Qiu, “Does correction\nremain a problem for large language models?,” 2023.\n[5] M. C. Penteado and F. Perez, “Evaluating gpt-3.5 and gpt-4 on gram-\nmatical error correction for brazilian portuguese,” 2023.\n[6] B. K. Bal, B. Karki, L. P. Khatiwada, and P. Shretha, “Nepali\nspellchecker 1 . 1 and the thesaurus , research and development,” PAN\nLocalization Working Papers, 2007.\n[7] K. B. Bal and P. Shrestha, “Nepali spellchecker,”P AN Localization\nWorking Papers 2004-2007, pp. 316–318, 2007.\n[8] B. Prasain, N. Lamichhane, N. Pandey, P. Adhikari, and P. Mudbhari,\n“Nepali spelling checker,”Journal of Engineering and Sciences, vol. 1,\np. 128–130, Dec. 2022.\n[9] K. H. Lai, M. Topaz, F. R. Goss, and L. Zhou, “Automated mis-\nspelling detection and correction in clinical free-text records,”Journal\nof Biomedical Informatics, vol. 55, pp. 188–195, 2015.\n[10] P. Norvig, “How to write a spelling corrector,” 2007. [Online].\nAvailable: http://norvig.com/spell-correct.html.\n[11] P. Fivez, S. Šuster, and W. Daelemans, “Unsupervised context-sensitive\nspelling correction of english and dutch clinical free-text with word and\ncharacter n-gram embeddings,” 2017.\n[12] A. Pal and A. Mustafi, “Vartani spellcheck – automatic context-sensitive\nspelling correction of ocr-generated hindi text using bert and levenshtein\ndistance,” 2020.\n[13] P. Li, “uChecker: Masked pretrained language models as unsupervised\nChinese spelling checkers,” inProceedings of the 29th International\nConference on Computational Linguistics(N. Calzolari, C.-R. Huang,\nH. Kim, J. Pustejovsky, L. Wanner, K.-S. Choi, P.-M. Ryu, H.-H.\nChen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue, S. Kim,\nY . Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S.-H. Na,\neds.), (Gyeongju, Republic of Korea), pp. 2812–2822, International\nCommittee on Computational Linguistics, Oct. 2022.\n[14] E. Brill and R. C. Moore, “An improved error model for noisy channel\nspelling correction,” inProceedings of the 38th Annual Meeting of the\nAssociation for Computational Linguistics, (Hong Kong), pp. 286–293,\nAssociation for Computational Linguistics, Oct. 2000.\n[15] C. Whitelaw, B. Hutchinson, G. Chung, and G. Ellis, “Using the web for\nlanguage independent spellchecking and autocorrection.,” pp. 890–899,\n01 2009.\n[16] A. Kuznetsov and H. Urdiales, “Spelling correction with denoising\ntransformer,”arXiv preprint arXiv:2105.05977, 2021.\n[17] S. M. Jayanthi, D. Pruthi, and G. Neubig, “NeuSpell: A neural spelling\ncorrection toolkit,” inProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations\n(Q. Liu and D. Schlangen, eds.), (Online), pp. 158–164, Association\nfor Computational Linguistics, Oct. 2020.\n[18] D. Mbaye and M. Diallo, “Beqi: Revitalize the senegalese wolof\nlanguage with a robust spelling corrector,”CoRR, vol. abs/2305.08518,\n2023.\n[19] P. Etoori, M. Chinnakotla, and R. Mamidi, “Automatic spelling correc-\ntion for resource-scarce languages using deep learning,” inProceedings\nof ACL 2018, Student Research Workshop(V . Shwartz, J. Tabassum,\nR. V oigt, W. Che, M.-C. de Marneffe, and M. Nissim, eds.), (Melbourne,\nAustralia), pp. 146–152, Association for Computational Linguistics, July\n2018.\n[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language understanding,”\ninProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers)(J. Burstein, C. Do-\nran, and T. Solorio, eds.), (Minneapolis, Minnesota), pp. 4171–4186,\nAssociation for Computational Linguistics, June 2019.\n[21] G. Yenduri, R. M, C. S. G, S. Y , G. Srivastava, P. K. R. Maddikunta,\nD. R. G, R. H. Jhaveri, P. B, W. Wang, A. V . Vasilakos, and\nT. R. Gadekallu, “Generative pre-trained transformer: A comprehen-\nsive review on enabling technologies, potential applications, emerging\nchallenges, and future directions,” 2023.\n[22] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever,et al.,\n“Language models are unsupervised multitask learners,”OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[23] Rajan, “Nepalibert,” 2021. [Online]. Available: https://huggingface.co/\nRajan/NepaliBERT.\n[24] U. Maskey, M. Bhatta, S. Bhatt, S. Dhungel, and B. K. Bal, “Nepali en-\ncoder transformers: An analysis of auto encoding transformer language\nmodels for Nepali text classification,” inProceedings of the 1st Annual\nMeeting of the ELRA/ISCA Special Interest Group on Under-Resourced\nLanguages(M. Melero, S. Sakti, and C. Soria, eds.), (Marseille, France),\npp. 106–111, European Language Resources Association, June 2022.\n[25] R. Lamsal, “A large scale nepali text corpus.” , IEEE Dataport, doi:\nhttps://dx.doi.org/10.21227/jxrd-d245, 2020.\n[26] P. J. Ortiz Suárez, B. Sagot, and L. Romary, “Asynchronous Pipeline for\nProcessing Huge Corpora on Medium to Low Resource Infrastructures,”\nin7th Workshop on the Challenges in the Management of Large\nCorpora (CMLC-7)(P. Bański, A. Barbaresi, H. Biber, E. Breiteneder,\nS. Clematide, M. Kupietz, H. Lüngen, and C. Iliadi, eds.), (Cardiff,\nUnited Kingdom), Leibniz-Institut für Deutsche Sprache, July 2019.\n[27] P. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced bert\nwith disentangled attention,” inInternational Conference on Learning\nRepresentations, 2021.\n[28] B. M. GARLAPATI, A. K. Singh, and S. R. Chalamala, “A robust\nmethod to protect text classification models against adversarial attacks,”\nThe International FLAIRS Conference Proceedings, vol. 35, May 2022.\n[29] J. Guo, T. N. Sainath, and R. J. Weiss, “A spelling correction model\nfor end-to-end speech recognition,” inICASSP 2019 - 2019 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 5651–5655, 2019.\nAppendix\nA. Error Model Edit Probability\nFigure 2: Heat map showing edit probability from the trained\nBM model used for correction\nThe heatmap in Figure 2 shows edit probabilities of some\ndevanagari tokens(all not shown due to space limitation)\ntrained from the Oscar data corpus. This model is used for\nperforming correction.\nB. Training Data Analysis\nTwo different corpora have been used to train two separate\nBM models. Hence, comparative analysis of these two corpus\nusing histograms was performed as shown in Figure 3-5.\nIn Figure 3, for better visualization of the distribution plot,\nfor the IEEE dataset, the distribution plot is shown for only\nthose paragraphs that have no. of words less than 80, and for\nthe Oscar dataset, the number of words is less than 150.\nTable VI: words per paragraph summary\nDataset 1st Quartile 2nd Quartile 3rd Quartile Mean Maximum\nIEEE 2 19 47 34.18 11501\nOscar 31 59 210 232.78 17904\nIn Figure 4, for better visualization of the distribution plot,\nfor the IEEE dataset, the distribution plot is shown for only\nthose paragraphs with no. of sentences less than 20, and for\nthe Oscar dataset, the number of sentences less than 50.\nTable VII: sentence per paragraph summary\nDataset 1st Quartile 2nd Quartile 3rd Quartile Mean Maximum\nIEEE 1 2 4 3.0796 606\nOscar 2 4 13 14.38 1060\nFrom the descriptive statistic shown in table VI, we can see\nthat the Oscar dataset has a large no. of words per paragraph\ncompared to the IEEE dataset. Finally, for different cut-off\nfrequencies, the no. of vocab obtained is shown for the Oscar\nand IEEE datasets in Figure 5.\nFigure 3: IEEE(top) vs Oscar(bottom) corpus words per\nparagraph analysis\nFigure 4: IEEE(top) vs Oscar(bottom) corpus sentence per\nparagraph analysis\nFigure 5: IEEE(top) vs Oscar(bottom) corpus vocab length for\ngiven cut-off frequency analysis",
  "topic": "Spelling",
  "concepts": [
    {
      "name": "Spelling",
      "score": 0.8032478094100952
    },
    {
      "name": "Computer science",
      "score": 0.7797281742095947
    },
    {
      "name": "Natural language processing",
      "score": 0.6501920223236084
    },
    {
      "name": "Language model",
      "score": 0.6316109299659729
    },
    {
      "name": "Artificial intelligence",
      "score": 0.577811062335968
    },
    {
      "name": "Probabilistic logic",
      "score": 0.5599761009216309
    },
    {
      "name": "Error detection and correction",
      "score": 0.5402166247367859
    },
    {
      "name": "Transformer",
      "score": 0.517266571521759
    },
    {
      "name": "Spell",
      "score": 0.5062965154647827
    },
    {
      "name": "Task (project management)",
      "score": 0.4423871338367462
    },
    {
      "name": "Word (group theory)",
      "score": 0.424469530582428
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4216960072517395
    },
    {
      "name": "Nepali",
      "score": 0.4178074896335602
    },
    {
      "name": "Linguistics",
      "score": 0.2507067322731018
    },
    {
      "name": "Algorithm",
      "score": 0.07535040378570557
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": []
}