{
  "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
  "url": "https://openalex.org/W4389518760",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3016646846",
      "name": "Joshua Ainslie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5002725719",
      "name": "James Lee-Thorp",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109999465",
      "name": "Michiel de Jong",
      "affiliations": [
        "University of Southern California",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3156685601",
      "name": "Yury Zemlyanskiy",
      "affiliations": [
        "University of Southern California",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5092005408",
      "name": "Federico Lebrón",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A632247528",
      "name": "Sumit Sanghai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4281758439",
    "https://openalex.org/W4385570209",
    "https://openalex.org/W4308760184",
    "https://openalex.org/W4310561894",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4288028629",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4311252752",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W4319166707",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2002555321",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2963204221",
    "https://openalex.org/W3138154797",
    "https://openalex.org/W3169942382",
    "https://openalex.org/W4307934016"
  ],
  "abstract": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4895–4901\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nGQA: Training Generalized Multi-Query Transformer Models from\nMulti-Head Checkpoints\nJoshua Ainslie∗, James Lee-Thorp ∗, Michiel de Jong ∗††\nYury Zemlyanskiy, Federico Lebrón, Sumit Sanghai\nGoogle Research\nAbstract\nMulti-query attention (MQA), which only\nuses a single key-value head, drastically\nspeeds up decoder inference. However, MQA\ncan lead to quality degradation, and moreover\nit may not be desirable to train a separate\nmodel just for faster inference. We (1) propose\na recipe for uptraining existing multi-head lan-\nguage model checkpoints into models with\nMQA using 5% of original pre-training com-\npute, and (2) introduce grouped-query atten-\ntion (GQA), a generalization of multi-query at-\ntention which uses an intermediate (more than\none, less than number of query heads) number\nof key-value heads. We show that uptrained\nGQA achieves quality close to multi-head at-\ntention with comparable speed to MQA.\n1 Introduction\nAutoregressive decoder inference is a severe bottle-\nneck for Transformer models due to the memory\nbandwidth overhead from loading decoder weights\nand all attention keys and values at every decod-\ning step (Shazeer, 2019; Pope et al., 2022; de Jong\net al., 2022). The memory bandwidth from loading\nkeys and values can be sharply reduced through\nmulti-query attention(Shazeer, 2019), which uses\nmultiple query heads but single key and value\nheads.\nHowever, multi-query attention (MQA) can lead\nto quality degradation and training instability, and\nit may not be feasible to train separate models\noptimized for quality and inference. Moreover,\nwhile some language models already use multi-\nquery attention, such as PaLM (Chowdhery et al.,\n2022), many do not, including publicly available\nlanguage models such as T5 (Raffel et al., 2020)\nand LLaMA (Touvron et al., 2023).\nThis work contains two contributions for faster\ninference with large language models. First, we\n∗Equal contribution.\n†University of Southern California. Work done at Google\nResearch.\nshow that language model checkpoints with multi-\nhead attention (MHA ) can be uptrained (Komat-\nsuzaki et al., 2022) to use MQA with a small frac-\ntion of original training compute. This presents a\ncost-effective method to obtain fast multi-query as\nwell as high-quality MHA checkpoints.\nSecond, we propose grouped-query attention\n(GQA ), an interpolation between multi-head and\nmulti-query attention with single key and value\nheads per subgroup of query heads. We show that\nuptrained GQA achieves quality close to multi-\nhead attention while being almost as fast as multi-\nquery attention.\n2 Method\n2.1 Uptraining\nGenerating a multi-query model from a multi-head\nmodel takes place in two steps: ﬁrst, converting the\ncheckpoint, and second, additional pre-training to\nallow the model to adapt to its new structure. Fig-\nure 1 shows the process for converting a multi-head\ncheckpoint into a multi-query checkpoint. The pro-\njection matrices for key and value heads are mean\npooled into single projection matrices, which we\nﬁnd works better than selecting a single key and\nvalue head or randomly initializing new key and\nvalue heads from scratch.\nFigure 1: Overview of conversion from multi-head to\nmulti-query attention. Key and value projection matri-\nces from all heads are mean pooled into a single head.\nThe converted checkpoint is then pre-trained for\n4895\nFigure 2: Overview of grouped-query method. Multi-head attention has H query, key, and value heads. Multi-query\nattention shares single key and value heads across all query heads. Grouped-query attention instead shares single\nkey and value heads for each group of query heads, interpolating between multi-head and multi-query attention.\na small proportion αof its original training steps\non the same pre-training recipe.\n2.2 Grouped-query attention\nGrouped-query attention divides query heads into\nGgroups, each of which shares a single key head\nand value head. GQA -G refers to grouped-query\nwith Ggroups. GQA -1, with a single group and\ntherefore single key and value head, is equivalent to\nMQA, while GQA-H, with groups equal to number\nof heads, is equivalent to MHA . Figure 2 shows a\ncomparison of grouped-query attention and multi-\nhead/multi-query attention. When converting a\nmulti-head checkpoint to a GQA checkpoint, we\nconstruct each group key and value head by mean-\npooling all the original heads within that group.\nAn intermediate number of groups leads to an\ninterpolated model that is higher quality thanMQA\nbut faster than MHA , and, as we will show, rep-\nresents a favorable trade-off. Going from MHA\nto MQA reduces H key and value heads to a sin-\ngle key and value head, reducing the size of the\nkey-value cache and therefore amount of data that\nneeds to be loaded by a factor of H. However,\nlarger models generally scale the number of heads,\nsuch that multi-query attention represents a more\naggressive cut in both memory bandwidth and ca-\npacity. GQA lets us keep the same proportional\ndecrease in bandwidth and capacity as model size\nincreases.\nMoreover, larger models suffer relatively less\nfrom memory bandwidth overhead from attention,\nas the KV-cache scales with model dimension\nwhile model FLOPs and parameters scale with the\nsquare of model dimension. Finally, standard shard-\ning for large models replicates the single key and\nvalue head by the number of model partitions (Pope\net al., 2022); GQA removes the waste from such\npartitioning. Therefore, we expect GQA to present\na particularly good trade-off for larger models.\nWe note that GQA is not applied to the encoder\nself-attention layers; encoder representations are\ncomputed in parallel, and memory bandwidth is\ntherefore generally not the primary bottleneck.\n3 Experiments\n3.1 Experimental setup\nConﬁgurations All models are based on the\nT5.1.1 architecture (Raffel et al., 2020), im-\nplemented with JAX (Bradbury et al., 2018),\nFlax (Heek et al., 2020), and Flaxformer 1. For\nour main experiments we consider T5 Large and\nXXL with multi-head attention, as well as up-\ntrained versions of T5 XXL with multi-query and\ngrouped-query attention. We use the Adafactor op-\ntimizer with the same hyperparameters and learn-\ning rate schedule as T5 (Raffel et al., 2020). We\napply MQA and GQA to decoder self-attention\nand cross-attention, but not encoder self-attention.\nUptraining Uptrained models are initialized\nfrom public T5.1.1 checkpoints. The key and value\nheads are mean-pooled to the appropriate MQA or\nGQA structure, and then pre-trained for a further\nαproportion of original pre-training steps with the\noriginal pre-training setup and dataset from (Raffel\net al., 2020). For α= 0.05, training took approxi-\nmately 600 TPUv3 chip-days.\nData We evaluate on summarization datasets\nCNN/Daily Mail (Nallapati et al., 2016), arXiv\nand PubMed (Cohan et al., 2018), MediaSum (Zhu\net al., 2021), and Multi-News (Fabbri et al., 2019);\n1https://github.com/google/flaxformer\n4896\nModel Tinfer Average CNN arXiv PubMed MediaSum MultiNews WMT TriviaQA\ns R1 R1 R1 R1 R1 BLEU F1\nMHA-Large 0.37 46.0 42.9 44.6 46.2 35.5 46.6 27.7 78.2\nMHA-XXL 1.51 47.2 43.8 45.6 47.5 36.4 46.9 28.4 81.9\nMQA-XXL 0.24 46.6 43.0 45.0 46.9 36.1 46.5 28.5 81.3\nGQA-8-XXL 0.28 47.1 43.5 45.4 47.7 36.3 47.2 28.4 81.6\nTable 1: Inference time and average dev set performance comparison of T5 Large and XXL models with multi-\nhead attention, and 5% uptrained T5-XXL models with multi-query and grouped-query attention on summarization\ndatasets CNN/Daily Mail, arXiv, PubMed, MediaSum, and MultiNews, translation dataset WMT, and question-\nanswering dataset TriviaQA.\ntranslation dataset WMT 2014 English-to-German;\nand question answering dataset TriviaQA (Joshi\net al., 2017). We do not evaluate on popular clas-\nsiﬁcation benchmarks such as GLUE (Wang et al.,\n2019) as autoregressive inference is less applicable\nfor those tasks.\nFine-tuning For ﬁne-tuning, we use a constant\nlearning rate of 0.001, batch size 128, and dropout\nrate 0.1 for all tasks. CNN/Daily Mail and WMT\nuse input length of 512 and output length 256.\nOther summarization datasets use input length\n2048 and output length 512. Finally, TriviaQA\nuses input length 2048 and output length 32. We\ntrain until convergence and select the checkpoint\nwith the highest dev performance. We use greedy\ndecoding for inference.\nTiming We report time per sample per TPUv4\nchip, as measured by xprof (Google, 2020). For\ntiming experiments we use 8 TPUs with the largest\nbatch size that ﬁts up to 32 per TPU, and paral-\nlelization optimized separately for each model.\n3.2 Main results\nFigure 3 shows average performance over all\ndatasets as a function of average inference time\nfor MHA T5-Large and T5-XXL, and uptrained\nMQA and GQA -8 XXL models with uptraining\nproportion α = 0.05. We see that a larger up-\ntrained MQA model provides a favorable trade-\noff relative to MHA models, with higher quality\nand faster inference than MHA -Large. Moreover,\nGQA achieves signiﬁcant additional quality gains,\nachieving performance close to MHA -XXL with\nspeed close to MQA . Table 1 contains full results\nfor all datasets.\n3.3 Ablations\nThis section presents experiments to investigate\nthe effect of different modeling choices. We eval-\n0 0.5 1 1.5\n46\n46.5\n47\nMHA-Large\nMHA-XXL\nMQA-XXL\nGQA-XXL\nTime per sample (ms)\nPerformance\nFigure 3: Uptrained MQA yields a favorable trade-\noff compared to MHA with higher quality and\nfaster speed than MHA-Large, and GQA achieves\neven better performance with similar speed gains\nand comparable quality to MHA-XXL. Average per-\nformance on all tasks as a function of average inference\ntime per sample for T5-Large and T5-XXL with multi-\nhead attention, and 5% uptrained T5-XXL with MQA\nand GQA-8 attention.\nuate performance on a representive subsample of\ntasks: CNN/Daily Mail, (short-form summariza-\ntion), MultiNews (long-form summarization), and\nTriviaQA (question-answering).\nCheckpoint conversion Figure 4 compares the\nperformance of different methods for checkpoint\nconversion. Mean pooling appears to work best,\nfollowed by selecting a single head and then ran-\ndom initialization. Intuitively, results are ordered\nby the degree to which information is preserved\nfrom the pre-trained model.\nUptraining steps Figure 5 shows how perfor-\nmance varies with uptraining proportion for T5\nXXL with MQA and GQA . First, we note that\nGQA already achieves reasonable performance af-\nter conversion while MQA requires uptraining to\n4897\n54.4 54.6 54.8 55 55.2 55.4 55.6\nMean\nFirst\nRandom\nFigure 4: Performance comparison of different check-\npoint conversion methods for T5-Large uptrained to\nMQA with proportion α = 0.05. ‘Mean’ mean-pools\nkey and value heads, ‘First’ selects the ﬁrst head and\n‘Random’ initializes heads from scratch.\nbe useful. Both MQA and GQA gain from 5%\nuptraining with diminishing returns from 10%.\n0 0.02 0.04 0.06 0.08 0.1\n54\n55\n56\n57\nUptraining proportion α\nPerformance\nMHA\nGQA\nMQA\nFigure 5: Performance as a function of uptraining pro-\nportion for T5 XXL models with MQA and GQA-8.\nNumber of groups Figure 6 demonstrates the\neffect of the number of GQA groups on infer-\nence speed. For larger models the memory band-\nwidth overhead from the KV cache is less con-\nstraining (Shazeer, 2019), while the reduction in\nkey-value size is sharper due to the increased num-\nber of heads. As a result, increasing the number\nof groups from MQA only results in modest slow-\ndowns initially, with increasing cost as we move\ncloser to MHA . We selected 8 groups as a favor-\nable middle ground.\n4 Related Work\nThis work is focused on achieving a better trade-\noff between decoder quality and inference time\nthrough reducing the memory bandwidth over-\nhead (Williams et al., 2009) from loading keys and\nvalues. Shazeer (2019) ﬁrst proposed reducing this\noverhead through multi-query attention. Follow-up\nwork showed that multi-query attention is espe-\n1 4 8 16 32 64\n1\n2\nGQA groups\nTime per sample (s)\nMHA\nGQA\nMQA\nFigure 6: Time per sample for GQA-XXL as a func-\ntion of the number of GQA groups with input length\n2048 and output length 512. Going from 1 (MQA) to\n8 groups adds modest inference overhead, with increas-\ning cost to adding more groups.\ncially helpful for long inputs (Pope et al., 2022;\nde Jong et al., 2022). Rabe (2023) independently\ndeveloped GQA with public implementation.\nA number of other methods have been proposed\nto reduce memory bandwidth overhead from keys\nand values, as well as parameters. Flash atten-\ntion (Dao et al., 2022) structures the attention com-\nputation to avoid materializing the quadratic at-\ntention scores, reducing memory and speeding up\ntraining. Quantization (Dettmers et al., 2022; Fran-\ntar et al., 2022) reduces the size of weights and\nactivations, including keys and values, by lowering\nprecision. Model distillation (Hinton et al., 2015;\nGou et al., 2021) instead reduces model size at\na given precision, using data generated from the\nlarger model to ﬁnetune the smaller model. Layer-\nsparse cross-attention (de Jong et al., 2022) elim-\ninates most cross-attention layers which make up\nthe primary expense for longer inputs. Speculative\nsampling (Chen et al., 2023; Leviathan et al., 2022)\nameliorates the memory bandwidth bottleneck by\nproposing multiple tokens with a smaller model\nwhich are then scored in parallel by a larger model.\nFinally, the uptraining procedure we propose\nis inspired by Komatsuzaki et al. (2022), which\nuptrains standard T5 checkpoints into sparsely acti-\nvated Mixture-of-Experts models.\n5 Conclusion\nLanguage models are expensive for inference pri-\nmarily due to the memory bandwidth overhead\nfrom loading keys and values. Multi-query atten-\ntion reduces this overhead at the cost of decreased\nmodel capacity and quality. We propose to convert\nmulti-head attention models to multi-query models\n4898\nwith a small fraction of original pre-training com-\npute. Moreover, we introduce grouped-query atten-\ntion, an interpolation of multi-query and multi-head\nattention that achieves quality close to multi-head\nat comparable speed to multi-query attention.\nLimitations\nThis paper focuses on ameliorating the memory\nbandwidth overhead from loading keys and values.\nThis overhead is most important when generating\nlonger sequences, for which quality is inherently\ndifﬁcult to evaluate. For summarization we employ\nRouge score, which we know is a ﬂawed evaluation\nthat does not tell the whole story; for that reason,\nit is difﬁcult to be certain our trade-offs are cor-\nrect. Due to limited computation, we also do not\ncompare our XXL GQA model to a comparitive\nmodel trained from scratch, so we do not know the\nrelative performance of uptraining vs training from\nscratch. Finally, we evaluate the impact of uptrain-\ning and GQA only on encoder-decoder models.\nRecently, decoder-only models are extremely pop-\nular, and since these models do not have separate\nself-attention and cross-attention, we expect GQA\nto have a stronger advantage over MQA.\nAcknowlegements\nWe thank Santiago Ontañón, Afroz Mohiuddin,\nWilliam Cohen and others at Google Research for\ninsightful advice and discussion.\nReferences\nJames Bradbury, Roy Frostig, Peter Hawkins,\nMatthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake\nVanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. 2018. JAX: composable transformations of\nPython+NumPy programs.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irv-\ning, Jean-Baptiste Lespiau, Laurent Sifre, and\nJohn Jumper. 2023. Accelerating large language\nmodel decoding with speculative sampling. CoRR,\nabs/2302.01318.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Na-\nzli Goharian. 2018. A discourse-aware attention\nmodel for abstractive summarization of long docu-\nments. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 615–621,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,\nand Christopher Ré. 2022. Flashattention: Fast and\nmemory-efﬁcient exact attention with io-awareness.\nCoRR, abs/2205.14135.\nMichiel de Jong, Yury Zemlyanskiy, Joshua Ainslie,\nNicholas FitzGerald, Sumit Sanghai, Fei Sha, and\nWilliam Cohen. 2022. FiDO: Fusion-in-decoder op-\ntimized for stronger performance and faster infer-\nence. arXiv preprint arXiv:2212.08153.\nTim Dettmers, Mike Lewis, Younes Belkada, and\nLuke Zettlemoyer. 2022. Llm.int8(): 8-bit ma-\ntrix multiplication for transformers at scale. CoRR,\nabs/2208.07339.\nAlexander R. Fabbri, Irene Li, Tianwei She, Suyi Li,\nand Dragomir R. Radev. 2019. Multi-news: A large-\nscale multi-document summarization dataset and ab-\nstractive hierarchical model. In Proceedings of the\n57th Conference of the Association for Computa-\ntional Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n1074–1084. Association for Computational Linguis-\ntics.\nElias Frantar, Saleh Ashkboos, Torsten Hoeﬂer, and\nDan Alistarh. 2022. GPTQ: accurate post-training\nquantization for generative pre-trained transformers.\nCoRR, abs/2210.17323.\nGoogle. 2020. Proﬁle your model with cloud tpu\ntools. https://cloud.google.com/tpu/docs/\ncloud-tpu-tools. Accessed: 2022-11-11.\nJianping Gou, Baosheng Yu, Stephen J. Maybank, and\nDacheng Tao. 2021. Knowledge distillation: A sur-\nvey. Int. J. Comput. Vis., 129(6):1789–1819.\n4899\nJonathan Heek, Anselm Levskaya, Avital Oliver, Mar-\nvin Ritter, Bertrand Rondepierre, Andreas Steiner,\nand Marc van Zee. 2020. Flax: A neural network\nlibrary and ecosystem for JAX.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nCoRR, abs/1503.02531.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp,\nCarlos Riquelme Ruiz, Basil Mustafa, Joshua\nAinslie, Yi Tay, Mostafa Dehghani, and Neil\nHoulsby. 2022. Sparse upcycling: Training mixture-\nof-experts from dense checkpoints.\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2022. Fast inference from transformers via specu-\nlative decoding. CoRR, abs/2211.17192.\nRamesh Nallapati, Bowen Zhou, Cícero Nogueira dos\nSantos, Çaglar Gülçehre, and Bing Xiang. 2016.\nAbstractive text summarization using sequence-to-\nsequence rnns and beyond. In Proceedings of the\n20th SIGNLL Conference on Computational Natural\nLanguage Learning, CoNLL 2016, Berlin, Germany,\nAugust 11-12, 2016, pages 280–290. ACL.\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery,\nJacob Devlin, James Bradbury, Anselm Levskaya,\nJonathan Heek, Kefan Xiao, Shivani Agrawal, and\nJeff Dean. 2022. Efﬁciently scaling transformer in-\nference. arXiv preprint arXiv:2211.05102.\nMarkus Rabe. 2023. Memory-efﬁcient attention.\nhttps://github.com/google/flaxformer/\nblob/main/flaxformer/components/\nattention/memory_efficient_attention.py.\nAccessed: 2023-05-23.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nNoam Shazeer. 2019. Fast transformer decoding:\nOne write-head is all you need. arXiv preprint\narXiv:1911.02150.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. 2023.\nLlama: Open and efﬁcient foundation language mod-\nels.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nSamuel Williams, Andrew Waterman, and David A.\nPatterson. 2009. Rooﬂine: an insightful visual per-\nformance model for multicore architectures. Com-\nmun. ACM, 52(4):65–76.\nChenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng.\n2021. Mediasum: A large-scale media interview\ndataset for dialogue summarization. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 5927–5934. Associ-\nation for Computational Linguistics.\n4900\nA Training Stability\nWe ﬁnd that multi-query attention can lead to train-\ning instability during ﬁne-tuning, in particular com-\nbined with long input tasks. We trained multiple\nT5-Large models with multi-query attention from\nscratch. In each case, pre-training suffered from\nfrequent loss spikes and the ﬁnal models diverged\nimmediately when ﬁne-tuning on long-input tasks.\nUptrained multi-query attention models are more\nstable but still display high variance, so for multi-\nquery models on unstable tasks we report average\nperformance over three ﬁne-tuning runs. Uptrained\ngrouped-query attention models, however, appear\nto be stable, so we did not investigate futher on the\nroot causes of multi-query instability.\n4901",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8215606212615967
    },
    {
      "name": "Inference",
      "score": 0.7266016006469727
    },
    {
      "name": "Generalization",
      "score": 0.4506067931652069
    },
    {
      "name": "Query optimization",
      "score": 0.4322908818721771
    },
    {
      "name": "Query expansion",
      "score": 0.42559635639190674
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3847692608833313
    },
    {
      "name": "Data mining",
      "score": 0.321644127368927
    },
    {
      "name": "Information retrieval",
      "score": 0.13659173250198364
    },
    {
      "name": "Mathematics",
      "score": 0.08126333355903625
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ],
  "cited_by": 217
}