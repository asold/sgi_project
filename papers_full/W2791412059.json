{
    "title": "Neural Lattice Language Models",
    "url": "https://openalex.org/W2791412059",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A5067416581",
            "name": "Jacob Buckman",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A5068811427",
            "name": "Graham Neubig",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2114790177",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2138747152",
        "https://openalex.org/W2290910789",
        "https://openalex.org/W48178473",
        "https://openalex.org/W2577255746",
        "https://openalex.org/W2963201387",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2727642071",
        "https://openalex.org/W2950448199",
        "https://openalex.org/W2220350356",
        "https://openalex.org/W2530486890",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W2963266340",
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2962826786",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W2162456950",
        "https://openalex.org/W2110485445",
        "https://openalex.org/W1508405452",
        "https://openalex.org/W2741986357",
        "https://openalex.org/W2625092622",
        "https://openalex.org/W1689711448",
        "https://openalex.org/W2610780044",
        "https://openalex.org/W1727996342",
        "https://openalex.org/W2143612262",
        "https://openalex.org/W2166270474",
        "https://openalex.org/W2962964385",
        "https://openalex.org/W2271595638",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W2952276042"
    ],
    "abstract": "In this work, we propose a new language modeling paradigm that has the ability to perform both prediction and moderation of information flow at multiple granularities: neural lattice language models. These models construct a lattice of possible paths through a sentence and marginalize across this lattice to calculate sequence probabilities or optimize parameters. This approach allows us to seamlessly incorporate linguistic intuitions — including polysemy and the existence of multiword lexical items — into our language model. Experiments on multiple language modeling tasks show that English neural lattice language models that utilize polysemous embeddings are able to improve perplexity by 9.95% relative to a word-level baseline, and that a Chinese model that handles multi-character tokens is able to improve perplexity by 20.94% relative to a character-level baseline.",
    "full_text": "Neural Lattice Language Models\nJacob Buckman\nLanguage Technologies Institute\nCarnegie Mellon University\njacobbuckman@gmail.com\nGraham Neubig\nLanguage Technologies Institute\nCarnegie Mellon University\ngneubig@cs.cmu.edu\nAbstract\nIn this work, we propose a new language mod-\neling paradigm that has the ability to perform\nboth prediction and moderation of informa-\ntion ﬂow at multiple granularities: neural lat-\ntice language models . These models con-\nstruct a lattice of possible paths through a sen-\ntence and marginalize across this lattice to cal-\nculate sequence probabilities or optimize pa-\nrameters. This approach allows us to seam-\nlessly incorporate linguistic intuitions – in-\ncluding polysemy and the existence of multi-\nword lexical items – into our language model.\nExperiments on multiple language modeling\ntasks show that English neural lattice language\nmodels that utilize polysemous embeddings\nare able to improve perplexity by 9.95% rela-\ntive to a word-level baseline, and that a Chi-\nnese model that handles multi-character to-\nkens is able to improve perplexity by 20.94%\nrelative to a character-level baseline.\n1 Introduction\nNeural network models have recently contributed to-\nwards a great amount of progress in natural language\nprocessing. These models typically share a common\nbackbone: recurrent neural networks (RNN), which\nhave proven themselves to be capable of tackling\na variety of core natural language processing tasks\n(Hochreiter and Schmidhuber, 1997; Elman, 1990).\nOne such task is language modeling, in which we\nestimate a probability distribution over sequences of\ntokens that corresponds to observed sentences ( §2).\nNeural language models, particularly models con-\nditioned on a particular input, have many applica-\ntions including in machine translation (Bahdanau et\nal., 2016), abstractive summarization (Chopra et al.,\n2016), and speech processing (Graves et al., 2013).\ndogs chased the small cat \ndogs chased the small\ncat\ndogs chased the\nsmall\ndogs chased\nthe\nthe_small\nthe_small_cat      small_cat\ndogs_chasedchased\nchased_the dogs_chased_the\nchased_the_small\nFigure 1: Lattice decomposition of a sentence and its cor-\nresponding lattice language model probability calculation\nSimilarly, state-of-the-art language models are al-\nmost universally based on RNNs, particularly long\nshort-term memory (LSTM) networks (Jozefowicz\net al., 2016; Inan et al., 2017; Merity et al., 2016).\nWhile powerful, LSTM language models usually\ndo not explicitly model many commonly-accepted\nlinguistic phenomena. As a result, standard mod-\nels lack linguistically informed inductive biases, po-\ntentially limiting their accuracy, particularly in low-\ndata scenarios (Adams et al., 2017; Koehn and\nKnowles, 2017). In this work, we present a novel\nmodiﬁcation to the standard LSTM language mod-\neling framework that allows us to incorporate some\nvarieties of these linguistic intuitions seamlessly:\nneural lattice language models (§3.1). Neural lat-\ntice language models deﬁne a lattice over possi-\nble paths through a sentence, and maximize the\nmarginal probability over all paths that lead to gen-\nerating the reference sentence, as shown in Fig. 1.\nDepending on how we deﬁne these paths, we can in-\ncorporate different assumptions about how language\nshould be modeled.\nIn the particular instantiations of neural lattice\nlanguage models covered by this paper, we focus on\ntwo properties of language that could potentially be\nof use in language modeling: the existence of multi-\nword lexical units (Zgusta, 1967) ( §4.1) and poly-\n529\nTransactions of the Association for Computational Linguistics, vol. 6, pp. 529–541, 2018. Action Editor: Holger Schwenk.\nSubmission batch: 8/2017; Revision batch: 1/2018; Published 8/2018.\nc⃝2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\nsemy (Ravin and Leacock, 2000) (§4.2). Neural lat-\ntice language models allow the model to incorporate\nthese aspects in an end-to-end fashion by simply ad-\njusting the structure of the underlying lattices.\nWe run experiments to explore whether these\nmodiﬁcations improve the performance of the model\n(§5). Additionally, we provide qualitative visualiza-\ntions of the model to attempt to understand what\ntypes of multi-token phrases and polysemous em-\nbeddings have been learned.\n2 Background\n2.1 Language Models\nConsider a sequence X for which we want to cal-\nculate its probability. Assume we have a vocabulary\nfrom which we can select a unique list of|X|tokens\nx1,x2,...,x |X| such that X = [x1; x2; ... ; x|X|],\ni.e. the concatenation of the tokens (with an appro-\npriate delimiter). These tokens can be either on the\ncharacter level (Hwang and Sung, 2017; Ling et al.,\n2015) or word level (Inan et al., 2017; Merity et al.,\n2016). Using the chain rule, language models gen-\nerally factorize p(X) in the following way:\np(X) =p(x1,x2,...,x |X|)\n=\n|X|∏\nt=1\np(xt |x1,x2,...,x t−1). (1)\nNote that this factorization is exact only in the\ncase where the segmentation is unique. In character-\nlevel models, it is easy to see that this property is\nmaintained, because each token is unique and non-\noverlapping. In word-level models, this also holds,\nbecause tokens are delimited by spaces, and no word\ncontains a space.\n2.2 Recurrent Neural Networks\nRecurrent neural networks have emerged as the\nstate-of-the-art approach to approximating p(X). In\nparticular, the LSTM cell (Hochreiter and Schmid-\nhuber, 1997) is a speciﬁc RNN architecture which\nhas been shown to be effective on many tasks, in-\ncluding language modeling (Press and Wolf, 2017;\nJozefowicz et al., 2016; Merity et al., 2016; Inan et\nal., 2017).1 LSTM language models recursively cal-\n1In this work, we utilize an LSTM with linked input and\nforget gates, as proposed by Greff et al. (2016).\nculate the hidden and cell states ( ht and ct respec-\ntively) given the input embedding et−1 correspond-\ning to token xt−1:\nht,ct = LSTM(ht−1,ct−1,et−1,θ), (2)\nthen calculate the probability of the next token given\nthe hidden state, generally by performing an afﬁne\ntransform parameterized by W and b, followed by a\nsoftmax:\np(xt |ht) :=softmax(W ∗ht + b). (3)\n3 Neural Lattice Language Models\n3.1 Language Models with Ambiguous\nSegmentations\nTo reiterate, the standard formulation of language\nmodeling in the previous section requires splitting\nsentence Xinto a unique set of tokens x1,...,x |X|.\nOur proposed method generalizes the previous for-\nmulation to remove the requirement of uniqueness\nof segmentation, similar to that used in non-neural\nn-gram language models such as Dupont and Rosen-\nfeld (1997) and Goldwater et al. (2007).\nFirst, we deﬁne some terminology. We use the\nterm “token”, designated by xi, to describe any in-\ndivisible item in our vocabulary that has no other\nvocabulary item as its constituent part. We use the\nterm “chunk”, designated by ki or xj\ni , to describe\na sequence of one or more tokens that represents a\nportion of the full string X, containing the unit to-\nkens xi through xj: xj\ni = [xi,xi+1; ... ; xj]. We\nalso refer to the “token vocabulary”, which is the\nsubset of the vocabulary containing only tokens, and\nto the “chunk vocabulary”, which similarly contains\nall chunks.\nNote that we can factorize the probability of any\nsequence of chunks K using the chain rule, in pre-\ncisely the same way as sequences of tokens:\np(K) =p(k1,k2,...,k |K|)\n=\n|K|∏\nt=1\np(kt |k1,k2,...,k t−1). (4)\nWe can factorize the overall probability of a to-\nken list X in terms of its chunks by using the chain\nrule, and marginalizing over all segmentations. For\nany particular token list X, we deﬁne a set of valid\n530\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\nsegmentations S(X), such that for every sequence\ns ∈ S(X), X = [xs1−1\ns0 ; xs2−1\ns1 ; ... ; x\ns|s|\ns|s|−1 ]. The\nfactorization is:\np(X) =\n∑\nS\np(X,S) =\n∑\nS\np(X|S)p(S) =\n∑\nS∈S(X)\np(S)\n=\n∑\nS∈S(X)\n|S|∏\nt=1\np(xst−1\nst−1 |xs1−1\ns0 ,xs2−1\ns1 ,...,x st−1−1\nst−2 ).\n(5)\nNote that, by deﬁnition, there exists a unique seg-\nmentation of X such that x1,x2,... are all tokens,\nin which case|S|= |X|. When only that one unique\nsegmentation is allowed per X, Scontains only that\none element, so summation drops out, and therefore\nfor standard character-level and word-level models,\nEq. (5) reduces to Eq. (4), as desired. However,\nfor models that license multiple segmentations per\nX, computing this marginalization directly is gener-\nally intractable. For example, consider segmenting\na sentence using a vocabulary containing all words\nand all 2-word expressions. The size of Swould\ngrow exponentially with the number of words in X,\nmeaning we would have to marginalize over tril-\nlions of unique segmentations for even modestly-\nsized sentences.\n3.2 Lattice Language Models\nTo avoid this, it is possible to re-organize the com-\nputations in a lattice, which allows us to dramati-\ncally reduce the number of computations required\n(Dupont and Rosenfeld, 1997; Neubig et al., 2010).\nAll segmentations of X can be expressed as the\nedges of paths through a lattice over token-level pre-\nﬁxes of X: x<1,x<2,...,X . The inﬁmum is the\nempty preﬁx x<1; the supremum is X; an edge from\npreﬁx x<i to preﬁx x<j exists if and only if there\nexists a chunk xj\ni in our chunk vocabulary such that\n[x<i; xj\ni ] =x<j. Each path through the lattice from\nx<1 to X is a segmentation of X into the list of to-\nkens on the traversed edges, as seen in Fig. 1.\nThe probability of a speciﬁc preﬁx p(x<j) is\ncalculated by marginalizing over all segmentations\nleading up to xj−1\np(x<j) =\n∑\nS∈S(x<j)\n|S|∏\nt=1\np(xst−1\nst−1 |x<st−1 ), (6)\nwhere by deﬁnition s|S| = j. The key insight here\nthat allows us to calculate this efﬁciently is that this\nis a recursive formula and that instead of marginaliz-\ning over all segmentations, we can marginalize over\nimmediate predecessor edges in the lattice,Aj. Each\nitem in Aj is a location i(= st−1), which indicates\nthat the edge between preﬁxx<i and preﬁx x<j, cor-\nresponding to token xj\ni , exists in the lattice. We can\nthus calculate p(x<j) as\np(x<j) =\n∑\ni∈Aj\np(x<i)p(xj\ni |x<i). (7)\nSince Xis the supremum preﬁx node, we can use\nthis formula to calculate p(X) by setting j = |X|.\nIn order to do this, we need to calculate the proba-\nbility of each of its |X|predecessors. Each of those\ntakes up to |X|calculations, meaning that the com-\nputation for p(X) can be done in O( |X|2) time. If\nwe can guarantee that each node will have a maxi-\nmum number of incoming edgesDso that |Aj|≤ D\nfor all j, then this bound can be reduced to O(D|X|)\ntime.2\nThe proposed technique is completely agnostic to\nthe shape of the lattice, and Fig. 2 illustrates several\npotential varieties of lattices. Depending on how the\nlattice is constructed, this approach can be useful in\na variety of different contexts, two of which we dis-\ncuss in §4.\n3.3 Neural Lattice Language Models\nThere is still one missing piece in our attempt to ap-\nply neural language models to lattices. Within our\noverall probability in Eq. (7), we must calculate the\nprobability p(xj\ni |x<i) of the next segment given\nthe history. However, given that there are potentially\nan exponential number of paths through the lattice\nleading to xi, this is not as straightforward as in the\ncase where only one segmentation is possible. Pre-\nvious work on lattice-based language models (Neu-\nbig et al., 2010; Dupont and Rosenfeld, 1997) uti-\nlized count-based n-gram models, which depend on\nonly a limited historical context at each step mak-\ning it possible to compute the marginal probabilities\nin an exact and efﬁcient manner through dynamic\nprogramming. On the other hand, recurrent neural\n2Thus, the standard token-level language model whereD =\n1 takes O(|X|) computations.\n531\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\nthe dog barked .\nthe dog barked .\nthe dog dog barked .\nthe dog barked .\nthe dog\ndog barked\nbarked .\nthe\ndog1 barked1\n.\ndog2 barked2\n(a)\n(b)\n(c)\n(d)\nFigure 2: Example of (a) a single-path lattice, (b) a sparse\nlattice, (c) a dense lattice with D= 2, and (d) a multilat-\ntice with D= 2, for sentence “the dog barked .”\nmodels depend on the entire context, causing them\nto lack this ability. Our primary technical contribu-\ntion is therefore to describe several techniques for\nincorporating lattices into a neural framework with\ninﬁnite context, by providing ways to approximate\nthe hidden state of the recurrent neural net.\n3.3.1 Direct Approximation\nOne approach to approximating the hidden state\nis the TreeLSTM framework described by Tai et al.\n(2015).3 In the TreeLSTM formulation, new states\nare derived from multiple predecessors by simply\nsumming the individual hidden and cell state vec-\ntors of each of them. For each predecessor location\ni ∈Aj, we ﬁrst calculate the local hidden state ˜h\nand local cell state ˜c by combining the embedding\nej\ni with the hidden state of the LSTM at x<i using\nthe standard LSTM update function as in Eq. (2):\n˜hi,˜ci = LSTM(hi,ci,ej\ni ,θ) for i∈Aj.\nWe then sum the local hidden and cell states:\nhj =\n∑\ni∈Aj\n˜hi cj =\n∑\ni∈Aj\n˜ci.\n3This framework has been used before for calculating neural\nsentence representations involving lattices by Su et al. (2016)\nand Sperber et al. (2017), but not for the language models that\nare the target of this paper.\nThis formulation is powerful, but comes at the\ncost of sacriﬁcing the probabilistic interpretation of\nwhich paths are likely. Therefore, even if almost\nall of the probability mass comes through the “true”\nsegmentation, the hidden state may still be heavily\ninﬂuenced by all of the “bad” segmentations as well.\n3.3.2 Monte-Carlo Approximation\nAnother approximation that has been proposed is\nto sample one predecessor state from all possible\npredecessors, as seen in Chan et al. (2017). We can\ncalculate the total probability that we reach some\npreﬁx x<j, and we know how much of this prob-\nability comes from each of its predecessors in the\nlattice, so we can construct a probability distribution\nover predecessors in the lattice:\nM(x<i |θ) =p(x<i |θ)p(xj\ni |x<i; θ)\np(x<j |θ) . (8)\nTherefore, one way to update the LSTM is to sam-\nple one predecessorx<i from the distributionMand\nsimply set hj = ˜hi and cj = ˜ci. However, sampling\nis unstable and difﬁcult to train: we found that the\nmodel tended to over-sample short tokens early on\nduring training, and thus segmented every sentence\ninto unigrams. This is similar to the outcome re-\nported by Chan et al. (2017), who accounted for it\nby incorporating an ϵencouraging exploration.\n3.3.3 Marginal Approximation\nIn another approach, which allows us to incorpo-\nrate information from all predecessors while main-\ntaining a probabilistic interpretation, we can utilize\nthe probability distribution M to instead calculate\nthe expected value of the hidden state:\nhj = Ex<i∼M [ ˜hi] =\n∑\ni∈Aj\nM(x<i |θ)˜hi\ncj = Ex<i∼M [˜ci] =\n∑\ni∈Aj\nM(x<i |θ)˜ci.\n3.3.4 Gumbel-Softmax Interpolation\nThe Gumbel-Softmax trick, or concrete distribu-\ntion, described by Jang et al. (2017) and Maddi-\nson et al. (2017), is a technique for incorporating\ndiscrete choices into differentiable neural computa-\ntions. In this case, we can use it to select a prede-\ncessor. The Gumbel-Softmax trick works by taking\nadvantage of the fact that adding Gumbel noise to\n532\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\nthe pre-softmax predecessor scores and then taking\nthe argmax is equivalent to sampling from the prob-\nability distribution. By replacing the argmax with a\nsoftmax function scaled by a temperature τ, we can\nget this pseudo-sampled distribution through a fully\ndifferentiable computation:\nN(x<i |θ) = exp((log(M(x<i |θ)) +gi)/τ)∑\nk∈Aj exp((log(M(x<k |θ)) +gk)/τ).\nThis new distribution can then be used to calculate\nthe hidden state by taking a weighted average of the\nstates of possible predecessors:\nhj =\nj−1∑\ni∈Aj\nN(x<i |θ)˜hi cj =\nj−1∑\ni=j−L\nN(x<i |θ)˜ci.\nWhen τ is large, the values of N(x<i | θ) are\nﬂattened out; therefore, all the predecessor hidden\nstates are summed with approximately equal weight,\nequivalent to the direct approximation ( §3.3.1). On\nthe other hand, when τ is small, the output distri-\nbution becomes extremely peaky, and one predeces-\nsor receives almost all of the weight. Each prede-\ncessor x<i has a chance of being selected equal to\nM(x<i |θ), which makes it identical to ancestral\nsampling (§3.3.2). By slowly annealing the value of\nτ, we can smoothly interpolate between these two\napproaches, and end up with a probabilistic interpre-\ntation that avoids the instability of pure sampling-\nbased approaches.\n4 Instantiations of Neural Lattice LMs\nIn this section, we introduce two instantiations of\nneural lattice languages models aiming to capture\nfeatures of language: the existence of coherent\nmulti-token chunks, and the existence of polysemy.\n4.1 Incorporating Multi-Token Phrases\n4.1.1 Motivation\nNatural language phrases often demonstrate sig-\nniﬁcant non-compositionality: for example, in En-\nglish, the phrase “rock and roll” is a genre of mu-\nsic, but this meaning is not obtained by viewing the\nwords in isolation. In word-level language model-\ning, the network is given each of these words as in-\nput, one at a time; this means it must capture the id-\niomaticity in its hidden states, which is quite round-\nabout and potentially a waste of the limited param-\neters in a neural network model. A straightforward\nsolution is to have an embedding for the entire multi-\ntoken phrase, and use this to input the entire phrase\nto the LSTM in a single timestep. However, it is also\nimportant that the model is able to decide whether\nthe non-compositional representation is appropriate\ngiven the context: sometimes, “rock” is just a rock.\nAdditionally, by predicting multiple tokens in a\nsingle timestep, we are able to decrease the num-\nber of timesteps across which the gradient must\ntravel, making it easier for information to be prop-\nagated across the sentence. This is even more useful\nin non-space-delimited languages such as Chinese,\nin which segmentation is non-trivial, but character-\nlevel modeling leads to many sentences being hun-\ndreds of tokens long.\nThere is also psycho-linguistic evidence which\nsupports the fact that humans incorporate multi-\ntoken phrases into their mental lexicon. Siyanova-\nChanturia et al. (2011) show that native speakers of\na language have signiﬁcantly reduced response time\nwhen processing idiomatic phrases, whether they are\nused in an idiomatic sense or not, while Bannard and\nMatthews (2008) show that children learning a lan-\nguage are better at speaking common phrases than\nuncommon ones. This evidence lends credence to\nthe idea that multi-token lexical units are a useful\ntool for language modeling in humans, and so may\nalso be useful in computational models.\n4.1.2 Modeling Strategy\nThe underlying lattices utilized in our multi-token\nphrase experiments are “dense” lattices: lattices\nwhere every edge (below a certain length L) is\npresent (Fig. 2, c). This is for two reasons. First,\nsince every sequence of tokens is given an oppor-\ntunity to be included in the path, all segmentations\nare candidates, which will potentially allow us to\ndiscover arbitrary types of segmentations without a\nprejudice towards a particular theory of which multi-\ntoken units we should be using. Second, using a\ndense lattice makes minibatching very straightfor-\nward by ensuring that the computation graphs for\neach sentence are identical. If the lattices were not\ndense, the lattices of various sentences in a mini-\nbatch could be different; it then becomes necessary\nto either calculate a differently-shaped graph for ev-\nery sentence, preventing minibatching and hurting\ntraining efﬁciency, or calculate and then mask out\n533\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\nthe missing edges, leading to wasted computation.\nSince only edges of length L or less are present,\nthe maximum in-degree of any node in the lattice\nD is no greater than L, giving us the time bound\nO(L|X|).\n4.1.3 Token Vocabularies\nStoring an embedding for every possible multi-\ntoken chunk would require |V|L unique embed-\ndings, which is intractable. Therefore, we construct\nour multi-token embeddings by merging composi-\ntional and non-compositional representations.\nNon-compositional Representation We ﬁrst es-\ntablish a priori set of “core” chunk-level tokens, each\nhave a dense embedding. In order to guarantee full\ncoverage of sentences, we ﬁrst add every unit-level\ntoken to this vocabulary, e.g. every word in the cor-\npus for a word-level model. Following this, we also\nadd the most frequent n-grams (where 1 < n≤L).\nThis ensures that the vast majority of sentences will\nhave several longer chunks appear within them, and\nso will be able to take advantage of tokens at larger\ngranularities.\nCompositional Representation However, the\nnon-compositional embeddings above only account\nfor a subset of all n-grams, so we additionally\nconstruct compositional embeddings for each chunk\nby running a BiLSTM encoder over the individual\nembeddings of each unit-level token within it (Dyer\net al., 2016). In this way, we can create a unique\nembedding for every sequence of unit-level tokens.\nWe use this composition function on chunks\nregardless of whether they are assigned non-\ncompositional embeddings or not, as even high-\nfrequency chunks may display compositional prop-\nerties. Thus, for every chunk, we compute the chunk\nembedding vector xj\ni by concatenating the compo-\nsitional embedding with the non-compositional em-\nbedding if it exists, or otherwise with an <UNK>\nembedding.\nSentinel Mixture Model for Predictions At each\ntimestep, we want to use our LSTM hidden state ht\nto assign some probability mass to every chunk with\na length less than L. To do this, we follow Merity\net al. (2016) in creating a new “sentinel” token<s>\nand adding it to our vocabulary. At each timestep,\nwe ﬁrst use our neural network to calculate a score\nfor each chunk C in our vocabulary, including the\nsentinel token. We do a softmax across these scores\nto assign a probability pmain(Ct+1 |ht; θ) to every\nchunk in our vocabulary, and also to<s>. For token\nsequences not represented in our chunk vocabulary,\nthis probability pmain(Ct+1 |ht; θ) = 0.\nNext, the probability mass assigned to the sentinel\nvalue, pmain(<s> |ht; θ), is distributed across all\npossible tokens sequences of length less than L, us-\ning another LSTM with parameters θsub. Similar to\nJozefowicz et al. (2016), this sub-LSTM is initial-\nized by passing in the hidden state of the main lattice\nLSTM at that timestep. This gives us a probability\nfor each sequence psub(c1,c2,...,c L |ht; θsub).\nThe ﬁnal formula for calculating the probability\nmass assigned to a speciﬁc chunk Cis:\np(C |ht; θ) =pmain(C |ht; θ)+\npmain(<s>|ht; θ)psub(C |ht; θsub).\n4.2 Incorporating Polysemous Tokens\n4.2.1 Motivation\nA second shortcoming of current language mod-\neling approaches is that each word is associated with\nonly one embedding. For highly polysemous words,\na single embedding may be unable to represent all\nmeanings effectively.\nThere has been past work in word embeddings\nwhich has shown that using multiple embeddings for\neach word is helpful in constructing a useful repre-\nsentation. Athiwaratkun and Wilson (2017) repre-\nsented each word with a multimodal Gaussian dis-\ntribution and demonstrated that embeddings of this\nform were able to outperform more standard skip-\ngram embeddings on word similarity and entailment\ntasks. Similarly, Chen et al. (2015) incorporate\nstandard skip-gram training into a Gaussian mixture\nframework and show that this improves performance\non several word similarity benchmarks.\nWhen a polysemous word is represented using\nonly a single embedding in a language modeling\ntask, the multimodal nature of the true embedding\ndistribution may causes the resulting embedding to\nbe both high-variance and skewed from the positions\nof each of the true modes. Thus, it is likely useful\nto represent each token with multiple embeddings\nwhen doing language modeling.\n534\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\n4.2.2 Modeling Strategy\nFor our polysemy experiments, the underlying lat-\ntices are multi-lattices: lattices which are also multi-\ngraphs, and can have any number of edges between\nany given pair of nodes (Fig. 2, d). Lattices set up\nin this manner allow us to incorporate multiple em-\nbeddings for each word. Within a single sentence,\nany pair of nodes corresponds to the start and end\nof a particular subsequence of the full sentence, and\nis thus associated with a speciﬁc token. Each edge\nbetween them is a unique embedding for that to-\nken. While many strategies for choosing the num-\nber of embeddings exist in the literature (Neelakan-\ntan et al., 2014), in this work, we choose a number\nof embeddings Eand assign that many embeddings\nto each word. This ensures that the maximum in-\ndegree of any node in the lattice D, is no greater\nthan E, giving us the time bound O(E|X|).\nIn this work, we do not explore models that in-\nclude both chunk vocabularies and multiple embed-\ndings. However, combining these two techniques, as\nwell as exploring other, more complex lattice struc-\ntures, is an interesting avenue for future work.\n5 Experiments\n5.1 Data\nWe perform experiments on two languages: English\nand Chinese, which provide an interesting contrast\nin linguistic features.4\nIn English, the most common benchmark for\nlanguage modeling recently is the Penn Tree-\nbank, speciﬁcally the version preprocessed by\nTom´aˇs Mikolov (2010). However, this corpus is lim-\nited by being relatively small, only containing ap-\nproximately 45,000 sentences, which we found to be\ninsufﬁcient to effectively train lattice language mod-\nels.5 Thus, we instead used the Billion Word Corpus\n(Chelba et al., 2014). Past experiments on the BWC\ntypically modeled every word without restricting the\nvocabulary, which results in a number of challenges\nregarding the modeling of open vocabularies that are\northogonal to this work. Thus, we create a pre-\n4Code to reproduce datasets and experiments is\navailable at: http://github.com/jbuckman/\nneural-lattice-language-models\n5Experiments using multi-word units resulted in overﬁtting,\nregardless of normalization and hyperparameter settings.\nprocessed version of the data in the same manner\nas Mikolov, lowercasing the words, replacing num-\nbers with <N> tokens, and <UNK>ing all words\nbeyond the ten thousand most common. Addition-\nally, we restricted the data set to only include sen-\ntences of length 50 or less, ensuring that large mini-\nbatches could ﬁt in GPU memory. Our subsampled\nEnglish corpus contained 29,869,166 sentences, of\nwhich 29,276,669 were used for training, 5,000 for\nvalidation, and 587,497 for testing. To validate that\nour methods scale up to larger language modeling\nscenarios, we also report a smaller set of large-scale\nexperiments on the full billion word benchmark in\nAppendix A.\nIn Chinese, we ran experiments on a subset of\nthe Chinese GigaWord corpus. Chinese is also par-\nticularly interesting because unlike English, it does\nnot use spaces to delimit words, so segmentation is\nnon-trivial. Therefore, we used a character-level lan-\nguage model for the baseline, and our lattice was\ncomposed of multi-character chunks. We used sen-\ntences from Guangming Daily , again <UNK>ing\nall but the 10,000 most common tokens and restrict-\ning the selected sentences to only include sentences\nof length 150 or less. Our subsampled Chinese cor-\npus included 934,101 sentences for training, 5,000\nfor validation, and 30,547 for testing.\n5.2 Main Experiments\nWe compare a baseline LSTM model, dense lattices\nof size 1, 2, and 3, and a multilattice with 2 and 3\nembeddings per word.\nThe implementation of our networks was done in\nDyNet (Neubig et al., 2017). All LSTMs had 2 lay-\ners, each with a hidden dimension of 200. Vari-\national dropout (Gal and Ghahramani, 2016) of .2\nwas used on the Chinese experiments, but hurt per-\nformance on the English data, so it was not used.\nThe 10,000 word embeddings each had dimension\n256. For lattice models, chunk vocabularies were se-\nlected by taking the 10,000 words in the vocabulary\nand adding the most common 10,000 n-grams with\n1 <n ≤L. The weights on the ﬁnal layer of the net-\nwork were tied with the input embeddings, as done\nby Press and Wolf (2017) and Inan et al. (2017).\nIn all lattice models, hidden states were computed\nusing a weighted expectation ( §3.3.3) unless men-\ntioned otherwise. In multi-embedding models, em-\n535\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\nTable 1: Results on English language modeling task\nModel Valid. Perp. Test Perp.\nBaseline 47.64 48.62\nMulti-Token (L= 1) 45.69 47.21\nMulti-Token (L= 2) 44.15 46.12\nMulti-Token (L= 3) 45.19 46.84\nMulti-Emb (E = 2) 44.80 46.32\nMulti-Emb (E = 3) 42.76 43.78\nTable 2: Results on Chinese language modeling task\nModel Valid. Perp. Test Perp.\nBaseline 41.46 40.72\nMulti-Token (L= 1) 49.86 50.99\nMulti-Token (L= 2) 38.61 37.22\nMulti-Token (L= 3) 33.01 32.19\nMulti-Emb (E = 2) 40.30 39.28\nMulti-Emb (E = 3) 45.72 44.40\nbedding sizes were decreased so as to maintain the\nsame total number of parameters. All models were\ntrained using the Adam optimizer with a learning\nrate of .01 on a NVIDIA K80 GPU. The results can\nbe seen in Table 1 and Table 2.\nIn the multi-token phrase experiments, many ad-\nditional parameters are accrued by the BiLSTM\nencoder and sub-LSTM predictive model, making\nthem not strictly comparable to the baseline. To ac-\ncount for this, we include results for L = 1, which,\nlike the baseline LSTM approach, fails to leverage\nmulti-token phrases, but includes the same number\nof parameters as L= 2and L= 3.\nIn both the English and Chinese experiments, we\nsee the same trend: increasing the maximum lat-\ntice size decreases the perplexity, and for L = 2\nand above, the neural lattice language model outper-\nforms the baseline. Similarly, increasing the number\nof embeddings per word decreases the perplexity,\nand for E = 2 and above, the multiple-embedding\nmodel outperforms the baseline.\n5.3 Hidden State Calculation Experiments\nWe compare the various hidden-state calculation ap-\nproaches discussed in Section 3.3 on the English\ndata using a lattice of size L = 2 and dropout of\n.2. These results can be seen in Table 3.\nTable 3: Hidden state calculation comparison results\nModel Valid. Perp. Test Perp.\nBaseline 64.18 60.67\nDirect (§3.3.1) 59.74 55.98\nMonte Carlo (§3.3.2) 62.97 59.08\nMarginalization (§3.3.3) 58.62 55.06\nGS Interpolation (§3.3.4) 59.19 55.73\nFor all hidden state calculation techniques, the\nneural lattice language models outperform the\nLSTM baseline. The ancestral sampling technique\nused by Chan et al. (2017) is worse than the others,\nwhich we found to be due to it getting stuck in a lo-\ncal minimum which represents almost everything as\nunigrams. There is only a small difference between\nthe perplexities of the other techniques.\n5.4 Discussion and Analysis\nNeural lattice language models convincingly out-\nperform an LSTM baseline on the task of lan-\nguage modeling. One interesting note is that in En-\nglish, which is already tokenized into words and\nhighly polysemous, utilizing multiple embeddings\nper word is more effective than including multi-\nword tokens. In contrast, in the experiments on the\nChinese data, increasing the lattice size of the multi-\ncharacter tokens is more important than increasing\nthe number of embeddings per character. This cor-\nresponds to our intuition; since Chinese is not tok-\nenized to begin with, utilizing models that incorpo-\nrate segmentation and compositionality of elemen-\ntary units is very important for effective language\nmodeling.\nTo calculate the probability of a sentence, the\nneural lattice language model implicitly marginal-\nizes across latent segmentations. By inspecting the\nprobabilities assigned to various edges of the lattice,\nwe can visualize these segmentations, as is done\nin Fig. 3. The model successfully identiﬁes bi-\ngrams which correspond to non-compositional com-\npounds, like “prime minister”, and bigrams which\ncorrespond to compositional compounds, such as “a\nquarter”. Interestingly, this does not occur for all\nhigh-frequency bigrams; it ignores those that are not\ninherently meaningful, such as “<UNK>in”, yield-\ning qualitatively good phrases.\nIn the multiple-embedding experiments, it is pos-\n536\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\nFigure 3: Segmentation of three sentences randomly sampled from the test corpus, usingL= 2. Green numbers show\nprobability assigned to token sizes. For example, the ﬁrst three words in the ﬁrst sentence have a 59% and 41% chance\nof being “please let me” or “please let me” respectively. Boxes around words show greedy segmentation.\nTable 4: Comparison of randomly-selected contexts of several words selected from the vocabulary of the Billion Word\nCorpus, in which the model preferred one embedding over the other.\nrock1 rock2\n...at the <unk>pop , rock and jazz... ...including hsbc , northern rock and...\n...a little bit <unk>rock ,... ...pakistan has a <unk>rock music scene...\n...on light rock and <unk>stations... ...spokesman for round rock , <unk>...\nbank1 bank2\n...being a bank holiday in... ...the bank of england has...\n...all the us bank runs and... ...with the royal bank of scotland...\n...by getting the bank ’s interests... ...development bank of japan and the...\npage1 page2\n...on page <unk>of the... ...was it front page news...\n...a source told page six .... ...himself , tony page , the former ...\n...on page <unk>of the... ...sections of the page that discuss...\nproﬁle1 proﬁle2\n...( <unk>: quote , proﬁle , research )... ...so <unk>the proﬁle of the city...\n...( <unk>: quote , proﬁle , research )... ...the highest proﬁle <unk>held by...\n...( <unk>: quote , proﬁle , research )... ...from high i , elite schools ,...\nedition1 edition2\n... of the second edition of windows... ...of the new york edition . ...\n... this month ’sedition of<unk>, the ... ...of the new york edition . ...\n...forthcoming d.c. edition of the hit... ...of the new york edition . ...\nrodham1 rodham2\n...senators hillary rodham clinton and...\n...making hillary rodham clinton his...\n...hillary rodham clinton ’s campaign has...\nsible to see which of the two embeddings of a word\nwas assigned the higher probability for any speciﬁc\ntest-set sentence. In order to visualize what types of\nmeanings are assigned to each embedding, we select\nsentences in which one embedding is preferred, and\nlook at the context in which the word is used. Sev-\neral examples of this can be seen in Table 4; it is\nclear from looking at these examples that the system\ndoes learn distinct embeddings for different senses\nof the word. What is interesting, however, is that it\ndoes not necessarily learn intuitive semantic mean-\nings; instead it tends to group the words by the con-\n537\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\ntext in which they appear. In some cases, likeproﬁle\nand edition, one of the two embeddings simply cap-\ntures an idiosyncrasy of the training data.\nAdditionally, for some words, such as rodham in\nTable 4, the system always prefers one embedding.\nThis is promising, because it means that in future\nwork it may be possible to further improve accu-\nracy and training efﬁciency by assigning more em-\nbeddings to polysemous words, instead of assigning\nthe same number of embeddings to all words.\n6 Related Work\nPast work that utilized lattices in neural models\nfor natural language processing centers around us-\ning these lattices in the encoder portion of machine\ntranslation. Su et al. (2016) utilized a variation\nof the Gated Recurrent Unit (GRU) that operated\nover lattices, and preprocessed lattices over Chi-\nnese characters that allowed it to effectively encode\nmultiple segmentations. Additionally, Sperber et al.\n(2017) proposed a variation of the TreeLSTM with\nthe goal of creating an encoder over speech lattices\nin speech-to-text. Our work tackles language mod-\neling rather than encoding, and thus addresses the\nissue of marginalization over the lattice.\nAnother recent work which marginalized over\nmultiple paths through a sentence is Ling et al.\n(2016). The authors tackle the problem of code gen-\neration, where some components of the code can be\ncopied from the input, via a neural network. Our\nwork expands on this by handling multi-word tokens\nas input to the neural network, rather than passing in\none token at a time.\nNeural lattice language models improve accuracy\nby helping the gradient ﬂow over smaller paths, pre-\nventing vanishing gradients. Many hierarchical neu-\nral language models have been proposed with a sim-\nilar objective (Koutnik et al., 2014; Zhou et al.,\n2017). Our work is distinguished from these by\nthe use of latent token-level segmentations that cap-\nture meaning directly, rather than simply being high-\nlevel mechanisms to encourage gradient ﬂow.\nChan et al. (2017) propose a model for predict-\ning characters at multiple granularities in the de-\ncoder segment of a machine translation system. Our\nwork expands on theirs by considering the entire lat-\ntice at once, rather than considering a only a sin-\ngle path through the lattice via ancestral sampling.\nThis allows us to train end-to-end without the model\ncollapsing to a local minimum, with no exploration\nbonus needed. Additionally, we propose a more\nbroad class of models, including those incorporat-\ning polysemous words, and apply our model to the\ntask of word-level language modeling, rather than\ncharacter-level transcription.\nConcurrently to this work, van Merri¨enboer et al.\n(2017) have proposed a neural language model that\ncan similarly handle multiple scales. Our work is\ndifferentiated in that it is more general: utilizing\nan open multi-token vocabulary, proposing multiple\ntechniques for hidden state calculation, and handling\npolysemy using multi-embedding lattices.\n7 Future Work\nIn the future, we would like to experiment with uti-\nlizing neural lattice language models in extrinsic\nevaluation, such as machine translation and speech\nrecognition. Additionally, in the current model, the\nnon-compositional embeddings must be selected a\npriori, and may be suboptimal. We are exploring\ntechniques to store ﬁxed embeddings dynamically,\nso that the non-compositional phrases can be se-\nlected as part of the end-to-end training.\n8 Conclusion\nIn this work, we have introduced the idea of a neural\nlattice language model, which allows us to marginal-\nize over all segmentations of a sentence in an end-\nto-end fashion. In our experiments on the Billion\nWord Corpus and Chinese GigaWord corpus, we\ndemonstrated that the neural lattice language model\nbeats an LSTM-based baseline at the task of lan-\nguage modeling, both when it is used to incorpo-\nrate multiple-word phrases and multiple-embedding\nwords. Qualitatively, we observed that the latent\nsegmentations generated by the model correspond\nwell to human intuition about multi-word phrases,\nand that the varying usage of words with multiple\nembeddings seems to also be sensible.\nAcknowledgements\nThe authors would like to thank Holger Schwenk,\nKristina Toutanova, Cindy Robinson, and all the re-\nviewers of this work for their invaluable feedback.\n538\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\nReferences\nOliver Adams, Adam Makarucha, Graham Neubig,\nSteven Bird, and Trevor Cohn. 2017. Cross-lingual\nword embeddings for low-resource language model-\ning. In Proceedings of the 15th Conference of the Eu-\nropean Chapter of the Association for Computational\nLinguistics: Volume 1, Long Papers, volume 1, pages\n937–947.\nBen Athiwaratkun and Andrew Wilson. 2017. Multi-\nmodal word distributions. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), volume 1, pages\n1645–1656.\nDzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk,\nPhilemon Brakel, and Yoshua Bengio. 2016. End-\nto-end attention-based large-vocabulary speech recog-\nnition. In IEEE International Conference on Acous-\ntics, Speech and Signal Processing, pages 4945–4949.\nIEEE.\nColin Bannard and Danielle Matthews. 2008. Stored\nword sequences in language learning: The effect of\nfamiliarity on children’s repetition of four-word com-\nbinations. Psychological Science, 19(3):241–248.\nWilliam Chan, Yu Zhang, Quoc Le, and Navdeep Jaitly.\n2017. Latent sequence decompositions. 5th Interna-\ntional Conference on Learning Representations.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robinson.\n2014. One billion word benchmark for measuring\nprogress in statistical language modeling. Interspeech.\nXinchi Chen, Xipeng Qiu, Jingxiang Jiang, and Xuanjing\nHuang. 2015. Gaussian mixture embeddings for mul-\ntiple word prototypes. CoRR, abs/1511.06246.\nSumit Chopra, Michael Auli, Alexander M Rush, and\nSEAS Harvard. 2016. Abstractive sentence sum-\nmarization with attentive recurrent neural networks.\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 93–98.\nPierre Dupont and Ronald Rosenfeld. 1997. Lattice\nbased language models. Technical report, DTIC Doc-\nument.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and\nNoah A Smith. 2016. Recurrent neural network gram-\nmars. North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 199–209.\nJeffrey L. Elman. 1990. Finding structure in time. Cog-\nnitive science, 14(2):179–211.\nYarin Gal and Zoubin Ghahramani. 2016. A theoreti-\ncally grounded application of dropout in recurrent neu-\nral networks. In Advances in Neural Information Pro-\ncessing Systems, pages 1019–1027.\nSharon Goldwater, Thomas L. Grifﬁths, Mark Johnson,\net al. 2007. Distributional cues to word boundaries:\nContext is important. In H. Caunt-Nulton, S. Kilati-\nlate, and I. Woo, editors, BUCLD 31: Proceedings of\nthe 31st Annual Boston University Conference on Lan-\nguage Development, pages 239–250. Somerville, Mas-\nsachusetts: Cascadilla Press.\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey\nHinton. 2013. Speech recognition with deep recurrent\nneural networks. In IEEE International Conference on\nAcoustics, Speech and Signal Processing, pages 6645–\n6649. IEEE.\nKlaus Greff, Rupesh K. Srivastava, Jan Koutn ´ık, Bas R.\nSteunebrink, and J ¨urgen Schmidhuber. 2016. LSTM:\nA search space odyssey. IEEE Transactions on Neural\nNetworks and Learning Systems.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9(8):1735–\n1780.\nKyuyeon Hwang and Wonyong Sung. 2017. Character-\nlevel language modeling with hierarchical recurrent\nneural networks. In IEEE International Conference on\nAcoustics, Speech and Signal Processing, pages 5720–\n5724. IEEE.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A loss\nframework for language modeling. 5th International\nConference on Learning Representations.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Categori-\ncal reparameterization with Gumbel-Softmax. 5th In-\nternational Conference on Learning Representations.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the limits\nof language modeling. arXiv:1602.02410.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceedings\nof the First Workshop on Neural Machine Translation,\npages 28–39.\nJan Koutnik, Klaus Greff, Faustino Gomez, and Juergen\nSchmidhuber. 2014. A clockwork RNN. Proceedings\nof Machine Learning Research.\nWang Ling, Isabel Trancoso, Chris Dyer, and Alan W.\nBlack. 2015. Character-based neural machine transla-\ntion. CoRR, abs/1511.04586.\nWang Ling, Edward Grefenstette, Karl Moritz Hermann,\nTom´aˇs Ko ˇcisk`y, Andrew Senior, Fumin Wang, and\nPhil Blunsom. 2016. Latent predictor networks for\ncode generation. Association for Computational Lin-\nguistics.\nChris J Maddison, Andriy Mnih, and Yee Whye Teh.\n2017. The concrete distribution: A continuous relax-\nation of discrete random variables. 5th International\nConference on Learning Representations.\n539\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. 4th International Conference on Learning Repre-\nsentations.\nArvind Neelakantan, Jeevan Shankar, Re Passos, and An-\ndrew Mccallum. 2014. Efﬁcient nonparametric es-\ntimation of multiple embeddings per word in vector\nspace. In Proceedings of EMNLP. Citeseer.\nGraham Neubig, Masato Mimura, Shinsuke Mori, and\nTatsuya Kawahara. 2010. Learning a language model\nfrom continuous speech. In INTERSPEECH, pages\n1053–1056.\nGraham Neubig, Chris Dyer, Yoav Goldberg, Austin\nMatthews, Waleed Ammar, Antonios Anastasopoulos,\nMiguel Ballesteros, David Chiang, Daniel Clothiaux,\nTrevor Cohn, et al. 2017. DyNet: The dynamic neural\nnetwork toolkit. arXiv preprint arXiv:1701.03980.\nOﬁr Press and Lior Wolf. 2017. Using the output embed-\nding to improve language models. 5th International\nConference on Learning Representations.\nYael Ravin and Claudia Leacock. 2000. Polysemy: The-\noretical and Computational Approaches . OUP Ox-\nford.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. Association for Computational Lin-\nguistics.\nAnna Siyanova-Chanturia, Kathy Conklin, and Norbert\nSchmitt. 2011. Adding more fuel to the ﬁre: An\neye-tracking study of idiom processing by native and\nnon-native speakers. Second Language Research ,\n27(2):251–272.\nMatthias Sperber, Graham Neubig, Jan Niehues, and\nAlex Waibel. 2017. Neural lattice-to-sequence mod-\nels for uncertain inputs. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1380–1389.\nJinsong Su, Zhixing Tan, Deyi Xiong, and Yang\nLiu. 2016. Lattice-based recurrent neural net-\nwork encoders for neural machine translation. CoRR,\nabs/1609.07730, ver. 2.\nKai Sheng Tai, Richard Socher, and Christopher D. Man-\nning. 2015. Improved semantic representations from\ntree-structured long short-term memory networks. As-\nsociation for Computational Linguistics.\nLuk´aˇs Burget Jan Honza Cernock Sanjeev Khudanpur\nTom´aˇs Mikolov, Martin Karaﬁ ´at. 2010. Recur-\nrent neural network based language model. Pro-\nceedings of the 11th Annual Conference of the Inter-\nnational Speech Communication Association , pages\n1045–1048.\nBart van Merri¨enboer, Amartya Sanyal, Hugo Larochelle,\nand Yoshua Bengio. 2017. Multiscale sequence\nmodeling with a learned dictionary. arXiv preprint\narXiv:1707.00762.\nLadislav Zgusta. 1967. Multiword lexical units. Word,\n23(1-3):578–587.\nHao Zhou, Zhaopeng Tu, Shujian Huang, Xiaohua Liu,\nHang Li, and Jiajun Chen. 2017. Chunk-based bi-\nscale decoder for neural machine translation. Associa-\ntion for Computational Linguistics.\nA Large-Scale Experiments\nTo verify that our ﬁndings scale to state-of-the-\nart language models, we also compared a baseline\nmodel, dense lattices of size 1 and 2, and a multi-\nlattice with 2 embeddings per word on the full byte-\npair encoded Billion Word Corpus.\nIn this set of experiments, we take the full Bil-\nlion Word Corpus, and apply byte-pair encoding as\ndescribed by Sennrich et al. (2015) to construct a\nvocabulary of 10,000 sub-word tokens. Our model\nconsists of three LSTM layers, each with 1500 hid-\nden units. We train the model for a single epoch over\nthe corpus, using the Adam optimizer with learning\nrate .0001 on a P100 GPU. We use a batch size of 40,\nand variational dropout of 0.1. The 10,000 sub-word\nembeddings each had dimension 600. For lattice\nmodels, chunk vocabularies were selected by taking\nthe 10,000 sub-words in the vocabulary and adding\nthe most common 10,000 n-grams with 1 <n ≤L.\nThe weights on the ﬁnal layer of the network were\ntied with the input embeddings, as done by Press\nand Wolf (2017) and Inan et al. (2017). In all\nlattice models, hidden states were computed using\nweighted expectation (§3.3.3). In multi-embedding\nmodels, embedding sizes were decreased so as to\nmaintain the same total number of parameters.\nResults of these experiments are in Table 5. The\nperformance of the baseline model is roughly on par\nwith that of state-of-the-art models on this database;\ndifferences can be explained by model size and hy-\nperparameter tuning. The results show the same\ntrend as the results of our main experiments, in-\ndicating that the performance gains shown by our\nsmaller neural lattice language models generalize to\nthe much larger datasets used in state-of-the-art sys-\ntems.\n540\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\nTable 5: Results on large-scale Billion Word Corpus\nModel Valid. Test Sec./\nPerp. Perp. Batch\nBaseline 54.1 37.7 .45\nMulti-Token (L= 1) 54.2 37.4 .82\nMulti-Token (L= 2) 53.9 36.4 4.85\nMulti-Emb (E = 2) 53.8 35.2 2.53\nTable 6: V ocabulary size comparison\nModel Valid. Perp. Test Perp.\nBaseline 64.18 60.67\n10000-chunk vocab 58.62 55.06\n20000-chunk vocab 57.40 54.15\nB Chunk Vocabulary Size\nWe compare a 2-lattice with a non-compositional\nchunk vocabulary of 10,000 phrases with a 2-\nlattice with a non-compositional chunk vocabulary\nof 20,000 phrases. The results can be seen in Table\n6. Doubling the number of non-compositional em-\nbeddings present decreases the perplexity, but only\nby a small amount. This is perhaps to be expected,\ngiven that doubling the number of embeddings cor-\nresponds to a large increase in the number of model\nparameters for phrases that may have less data with\nwhich to train them.\n541\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025\n542\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036 by guest on 05 November 2025"
}