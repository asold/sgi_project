{
  "title": "Finding Skill Neurons in Pre-trained Transformer-based Language Models",
  "url": "https://openalex.org/W4385573694",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2102849134",
      "name": "Xiao-Zhi Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287895564",
      "name": "Kaiyue Wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104573188",
      "name": "Zhengyan Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1982465838",
      "name": "Lei Hou",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2129156004",
      "name": "Juanzi Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2105406322",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2101295242",
    "https://openalex.org/W3102812725",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W3104350794",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W4302028573",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W2160921898",
    "https://openalex.org/W3104136798",
    "https://openalex.org/W2963400886",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W4285662474",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3212706150",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W2963503967",
    "https://openalex.org/W2963236897",
    "https://openalex.org/W2013239224",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W1899185266",
    "https://openalex.org/W3024936740",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2120480077",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W3154028478",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W3172099915",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4292402161",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W102708294",
    "https://openalex.org/W3082665562",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3035691519",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W2276892413",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W3203366473",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3205717164",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2785648721",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3037626499",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W2963749936",
    "https://openalex.org/W1970719523"
  ],
  "abstract": "Transformer-based pre-trained language models have demonstrated superior performance on various natural language processing tasks. However, it remains unclear how the skills required to handle these tasks distribute among model parameters. In this paper, we find that after prompt tuning for specific tasks, the activations of some neurons within pre-trained Transformers are highly predictive of the task labels. We dub these neurons skill neurons and confirm they encode task-specific skills by finding that: (1) Skill neurons are crucial for handling tasks. Performances of pre-trained Transformers on a task significantly drop when corresponding skill neurons are perturbed. (2) Skill neurons are task-specific. Similar tasks tend to have similar distributions of skill neurons. Furthermore, we demonstrate the skill neurons are most likely generated in pre-training rather than fine-tuning by showing that the skill neurons found with prompt tuning are also crucial for other fine-tuning methods freezing neuron weights, such as the adapter-based tuning and BitFit. We also explore the applications of skill neurons, including accelerating Transformers with network pruning and building better transferability indicators. These findings may promote further research on understanding Transformers. The source code can be obtained from https://github.com/THU-KEG/Skill-Neuron.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11132–11152\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nFinding Skill Neurons in Pre-trained Transformer-based Language Models\nXiaozhi Wang1∗, Kaiyue Wen2∗, Zhengyan Zhang1,\nLei Hou1,3†, Zhiyuan Liu1,3†, Juanzi Li1,3\n1Department of Computer Science and Technology, BNRist;\n2Institute for Interdisciplinary Information Sciences;\n3KIRC, Institute for Artificial Intelligence,\nTsinghua University, Beijing, 100084, China\n{wangxz20,wenky20}@mails.tsinghua.edu.cn\nAbstract\nTransformer-based pre-trained language mod-\nels have demonstrated superior performance\non various natural language processing tasks.\nHowever, it remains unclear how the skills re-\nquired to handle these tasks distribute among\nmodel parameters. In this paper, we find that\nafter prompt tuning for specific tasks, the activa-\ntions of some neurons within pre-trained Trans-\nformers1 are highly predictive of the task labels.\nWe dub these neuronsskill neuronsand confirm\nthey encode task-specific skills by finding that:\n(1) Skill neurons are crucial for handling tasks.\nPerformances of pre-trained Transformers on\na task significantly drop when corresponding\nskill neurons are perturbed. (2) Skill neurons\nare task-specific. Similar tasks tend to have sim-\nilar distributions of skill neurons. Furthermore,\nwe demonstrate the skill neurons are most\nlikely generated in pre-training rather than fine-\ntuning by showing that the skill neurons found\nwith prompt tuning are also crucial for other\nfine-tuning methods freezing neuron weights,\nsuch as the adapter-based tuning and BitFit. We\nalso explore the applications of skill neurons,\nincluding accelerating Transformers with net-\nwork pruning and building better transferability\nindicators. These findings may promote fur-\nther research on understanding Transformers.\nThe source code can be obtained from https:\n//github.com/THU-KEG/Skill-Neuron.\n1 Introduction\nPre-trained language models (PLMs), mostly based\non Transformer architecture (Vaswani et al., 2017),\nhave achieved remarkable performance on broad\nand diverse natural language processing (NLP)\ntasks (Han et al., 2021). However, it remains un-\nclear how the skills required to handle these tasks\ndistribute among model parameters. Are there\n∗ indicates equal contribution.\n† Corresponding author: Z.Liu and L.Hou.\n1For brevity,Transformer-based language modelsare often\nreferred to as Transformers in this paper.\n0.16\n 0.14\n 0.12\n 0.10\n 0.08\n 0.06\n 0.04\nActivation\n100\n101\n102\n#Sentences\nPositive\nNegative\nFigure 1: Histogram of activation of a neuron within\nRoBERTaBASE on positive-label (blue) and negative-\nlabel (orange) sentences in SST-2 validation set.\nspecific neurons within pre-trained Transformers\nencoding these skills? Progress on this problem\nmay help to understand the working mechanisms\nof pre-trained Transformers (Zeiler and Fergus,\n2014; Karpathy et al., 2015; Bau et al., 2020; Suau\net al., 2020), intervene model behaviors (Bau et al.,\n2018; Mitchell et al., 2021), and improve model\nefficiency (Dalvi et al., 2020; Zhang et al., 2021).\nPrompt tuning (Li and Liang, 2021; Lester et al.,\n2021) prepends some trainable embeddings, i.e.,\nsoft prompts, into the inputs and adapts PLMs to\nhandle tasks by only tuning the soft prompts while\nfreezing all the PLM parameters. It has attracted\nwide attention recently as a promising parameter-\nefficient fine-tuning methods (Su et al., 2021; Liu\net al., 2022). In this paper, we find that after prompt\ntuning for a task, the activations on soft prompts of\nsome neurons within pre-trained Transformers are\nhighly predictive for the task. For instance, Fig-\nure 1 shows the activation distribution of a specific\nneuron within RoBERTaBASE (Liu et al., 2019b).\nThis neuron’s activation is highly predictive of the\nlabels of SST-2 (Socher et al., 2013), an established\nsentiment analysis dataset. When the input sen-\ntences express positive sentiments, the activations\non soft prompts of this neuron tend to be much\n11132\nhigher than when they express negative sentiments.\nIt suggests that this neuron may encode the skill of\ndistinguishing sentiments.\nWe dub these special neurons skill neuronsand\ndevelop a simple and effective method to find them\nfor classification tasks via prompt tuning. For a\nbinary classification task, we first calculate the em-\npirical mean activation on a soft prompt token over\nthe training set for each neuron and use it as this\nneuron’s baseline activation. If this neuron’s activa-\ntion for an input sample is higher than the baseline,\nwe regard it as predicting one label and vice versa.\nWe aggregate the prediction accuracies on the vali-\ndation set of multiple soft prompts as the neuron’s\npredictivity score. The neurons with the highest\npredictivity scores are identified as skill neurons.\nFor multi-class classification tasks, we decompose\nthem into multiple binary classification subtasks\nand aggregate the skill neurons of subtasks as the\nskill neurons of the multi-class task.\nWe confirm the skill neurons encode task-\nspecific skills with a series of experimental find-\nings: (1) Skill neurons generally and stably emerge.\nFor all the 7 investigated tasks and 5 random trials,\nwe can consistently find skill neurons with high pre-\ndictivities close to prompt tuning. (2) Skill neurons\nare crucial for handling tasks. When we perturb\nskill neurons by adding random noises to their acti-\nvations, the performances on corresponding tasks\ndrop much more significantly than when random\nneurons are perturbed. (3) Skill neurons are task-\nspecific. Similar tasks exhibit similar predictivity\nrankings of skill neurons, and skill neurons of same-\ntype tasks are more important for handling a task\nthan those of different-type tasks. (4) Skill neurons\nare not from shallow word selectivity. The skill\nneurons typically do not selectively activate on key-\nwords relating to the task, and their predictivities\nare not significantly influenced by the label words\nused in prompt tuning.\nAfter showing that skill neurons encode skills,\nwe further demonstrate that skill neurons are most\nlikely generated in pre-training rather than manu-\nfactured by the fine-tuning process of prompt tun-\ning. This is concluded from: (1) Even for randomly\ngenerated prompts and untuned hard prompts, the\nskill neurons still exhibit much better predictivity\nperformance than random guesses. (2) Skill neu-\nrons are also crucial for other fine-tuning methods\nfreezing neuron weights. Performance of models\ntrained with adapter-based tuning (Houlsby et al.,\n2019) and BitFit (Ben-Zaken et al., 2022) signif-\nicantly drops when the skill neurons found with\nprompt tuning are perturbed.\nMoreover, we explore the practical applications\nof skill neurons. First, we apply skill neurons to\nnetwork pruning (Anwar et al., 2017; Dalvi et al.,\n2020), which aims at removing redundant param-\neters to reduce memory cost and accelerate infer-\nence. Experiments show that by only keeping top\nskill neurons active, we can reduce the pre-trained\nTransformer to 66.6% of its original parameters\nand achieve about 1.4 inference speedup. Then\nwe explore building better prompt transferability\nindicators following Su et al. (2021). We improve\ntheir overlapping rate of activated neuronsmetric\nby only taking skill neurons into account, and this\nachieves significantly better performance.\nTo summarize, our contributions are four-fold:\n(1) We observe the existence of skill neurons, the\nspecial neurons within pre-trained Transformers,\nwhich are highly predictive for specific tasks, and\ndevelop a method to find them via prompt tuning.\n(2) We empirically confirm that skill neurons do\nencode the skills required to handle tasks. (3) We\nshow skill neurons are generated in pre-training\nrather than fine-tuning. (4) We preliminarily ex-\nplore the applications of skill neurons. We hope\nthese findings could facilitate future research on\nunderstanding the mechanism of PLMs.\n2 Preliminary\nWe introduce the basic knowledge about prompt\ntuning (§ 2.1), the definition of investigated neu-\nrons (§ 2.2), and the investigation setup (§ 2.3).\n2.1 Prompt Tuning\nPrompt tuning (PT), or soft prompting, is a recently-\ndeveloped parameter-efficient fine-tuning method,\nwhich has attracted wide attention with its capa-\nbility to effectively adapt PLMs to downstream\ntasks (Li and Liang, 2021; Lester et al., 2021) and\nquery inner knowledge of PLMs (Qin and Eisner,\n2021; Zhong et al., 2021). PT prepends some soft\nprompts into the input sequences to prompt the\nPLM to decode the desired label wordsof the train-\ning task in the same way as the pre-training objec-\ntive. For each task, a verbalizer function (Schick\nand Schütze, 2021) is used to map the specific label\nwords to the labels of the task. Each soft prompt\nis a virtual token, which is essentially a trainable\nembedding. During prompt tuning, only the param-\n11133\neters in soft prompts are tuned, and all the PLM’s\noriginal parameters are frozen.\nFormally, given an input sequence with n to-\nkens X = {w1,w2,...,w n}, prompt tuning\nprepends lrandomly initialized soft prompts P =\n{p1,p2,...,p l}before them, where pi ∈Rd and\ndis the input dimension of the PLM. Taking the\nPLMs pre-trained with the masked language model-\ning objective (Devlin et al., 2019) as an example, a\nspecial [MASK] token is prepended, and the prompt\ntuning objective is to maximize the likelihood of\nfilling desired label word yinto it:\nL= p(y|[MASK], P, x1, . . . , xn). (1)\nSome initial prompt tuning works (Qin and Eis-\nner, 2021; Zhong et al., 2021) regard soft prompts\nas the relaxation of natural language hard prompts,\nwhich are initially designed to query inner factual\nknowledge of PLMs (Petroni et al., 2019; Jiang\net al., 2020). Su et al. (2021) hypothesize that soft\nprompts work by stimulating PLMs’ inner abilities.\nInspired by these, we observe the inner activations\nof PLMs and find skill neurons.\n2.2 Neurons in Transformers\nTransformer (Vaswani et al., 2017) is the state-of-\nthe-art NLP model architecture, which is used by\nthe majority of PLMs (Devlin et al., 2019; Liu et al.,\n2019b; Brown et al., 2020; Raffel et al., 2020). A\npre-trained Transformer model is typically stacked\nwith multiple identical Transformer layers. Each\nTransformer layer consists of a self-attention mod-\nule and a feed-forward network (FFN), among\nwhich the FFN carries two-thirds of the param-\neters. Previous work has highlighted the impor-\ntance of FFN (Press et al., 2020; Dong et al., 2021)\nand found FFN encodes rich information (Suau\net al., 2020; Geva et al., 2021; Dai et al., 2021).\nInspired by these, we study the neurons and activa-\ntions within FFN.\nFormally, the FFN in a Transformer layer is:\nFFN(x) =f(xK⊤+ b1)V + b2, (2)\nwhere x ∈Rd is the hidden embedding of a token,\nf(·) is the activation function, K,V ∈Rdm×d are\ntrainable matrices, and b1,b2 are biases.\nFor simplicity, let a = f(xK⊤+ b1) ∈Rdm.\nWe regardai, the i-th element ofa, as the activation\nof the i-th neuron on input x. It represents the\nimportance of Ki and Vi, the i-th column vectors\nof K and V, respectively. Hence we define Ki and\nVi as the weights of the i-th neuron in this layer.\nAlthough they study essentially the same param-\neters as us, Dai et al. (2021) and Zhang et al. (2021)\nuse the term neuron to denote activations in our def-\ninition. Some other works (Dalvi et al., 2019; Dur-\nrani et al., 2020; Hennigen et al., 2020; Antverg\nand Belinkov, 2022) define a dimension in con-\ntextualized representations as a neuron. Since we\nstudy how the skills distribute among model param-\neters rather than input-dependent representations,\nwe study the neurons defined in this section.\n2.3 Investigation Setup\nTo comprehensively investigate the skill neuron\nphenomenon, we use RoBERTa BASE (Liu et al.,\n2019b), a widely-used Transformer model pre-\ntrained with the masked language modeling ob-\njective (Devlin et al., 2019), and conduct experi-\nments on 7 tasks of 3 types, including: (1) Sen-\ntiment Analysis, including SST-2 (Socher et al.,\n2013), IMDB (Maas et al., 2011), and TweetEval\n(Tweet) (Barbieri et al., 2020); (2) Natural Lan-\nguage Inference, including MNLI (Williams et al.,\n2018) and QNLI (Wang et al., 2019); (3)Topic Clas-\nsification, including AG News and DBpedia (Zhang\net al., 2015). Details about the tasks and prompt\ntuning implementations are shown in appendices A\nand B, respectively.\n3 Finding Skill Neurons\nWe use a simple and effective method to find skill\nneurons for a given pre-trained Transformer M.\n3.1 Binary Classification Task\nWe first introduce how to find skill neurons\nfor binary classification tasks. Let T be a bi-\nnary classification task and its dataset be D =\n{(x1,y1) ,(x2,y2) ,...,\n(\nx|D|,y|D|\n)\n}, which is di-\nvided into training set Dtrain, development set\nDdev, and test set Dtest. The i-th sample (xi,yi)\ncontains an input xi and its label yi ∈{0,1}.\nFor a specific neuronNwithin M, let a(N,t,x )\nbe the activation of it on token t given the in-\nput sentence x. We firstly do prompt tuning on\nMwith Dtrain and get a group of lsoft prompts\nP = {p1,p2,...,p l}. Given a soft prompt pi, we\ncalculate the baseline activation of Non pi over\nthe training set as follows:\nabsl(N, pi) = 1\n|Dtrain|\n∑\nxj,yj∈Dtrain\na(N, pi, xj). (3)\n11134\nIntuitively, we can regard that the neuron Npre-\ndicts positive label 1 for the input sentence xwhen\na(N,pi,x) > absl(N,pi). Hence the prediction\naccuracy over the development set is as follows:\nAcc(N, pi) =\n∑\nxj,yj∈Ddev\n1[1[a(N,pi,xj)>absl(N,pi)]=yj]\n|Ddev| ,\n(4)\nwhere 1[condition] ∈{0,1}is the indicator function\nevaluating to 1 iff the condition holds.\nThe above way only considers the positive corre-\nlations between the labels and neuronal activations,\nwhich is also the case of previous work (Geva et al.,\n2021; Dai et al., 2021). However, strong negative\ncorrelations also suggest that the information about\nskills is encoded in this neuron. Conceptually,\nthis is similar to the fact that inhibitory neurons\nin brains also contribute to certain functions (Rudy\net al., 2011). Hence we define the predictivity of\nNon soft prompt token pi as:\nPred(N, pi) = max(Acc(N, pi), 1 −Acc(N, pi)). (5)\nFor each group of soft prompts P, the predictiv-\nity of Non it is defined as the predictivity of the\nbest soft prompt token. Considering the skill neu-\nrons shall be consistently predictive, we conduct\n5 random trials of prompt tuning and get 5 groups\nof prompts: P= {P1,P2,...,P 5}. The overall\npredictivity of neuron Nis defined as:\nPred(N) = 1\n|P|\n∑\nPi∈P\nmaxpj∈Pi(Pred(N, pj)). (6)\nThen we sort all the neurons within model M\nby the descending order of their predictivities and\nuse the top neurons as the skill neurons in experi-\nments. Appendix G discusses some potential de-\nsign choices considered in finding skill neurons.\n3.2 Multi-class Classification Task\nTo find skill neurons for a multi-class classifica-\ntion task, we first decompose it into multiple bi-\nnary classification subtasks. Then we find skill\nneurons by ranking the neurons with their predic-\ntivities of the decomposed subtasks in a similar\nway as introduced in § 3.1 but use the soft prompts\nof the original task instead of subtasks. Skill neu-\nrons of the multi-class classification task consist\nof equal numbers of subtask skill neurons. For in-\nstance, MNLI (Williams et al., 2018) task requires\n0.16\n 0.14\n 0.12\n 0.10\n 0.08\n 0.06\nActivation of Neuron #1\n0.16\n0.14\n0.12\n0.10\n0.08\n0.06\n0.04\nActivation of Neuron #2\nEntailment\nNeutral\nContradiction\nFigure 2: Distribution of activations of two neurons on a\nsoft prompt for samples in MNLI validation set. Dashed\nlines indicate baseline activations of the two neurons.\nTask Prompt\nTuning\nSkill\nNeuron\nSST-2 91.8±0.5 91.6±0.3\nIMDB 91.6±0.5 92.0±0.3\nTweet 70.0±0.2 56.0±3.2\nMNLI 76.8±1.8 74.7±2.5\nQNLI 85.7±0.7 86.0±0.4\nAG News 98.8±0.1 98.9±0.1\nDBpedia 99.7±0.1 99.8±0.1\nTable 1: Accuracies (%) on various tasks of prompt\ntuning and skill neurons, along with standard deviations\nover 5 random trials. For the binary classification tasks,\nthe skill neuron performance is the predictivity of the\ntop-1 skill neuron. For multi-class classification tasks,\nthe skill neuron performance is obtained by training a\nlogistic regression model taking only the activations of\nthe top-1 neurons of decomposed subtasks as inputs.\nto classify the relationships between sentence pairs\ninto ENTAILMENT , NEUTRAL and CONTRADIC -\nTION . We decompose it into two subtasks: the\nfirst one is to classify ENTAILMENT and CONTRA -\nDICTION samples, and the second one is to clas-\nsify NEUTRAL and NON-NEUTRAL samples. If\nwe need top- 100 skill neurons of MNLI, we will\nretrieve top-50 unique skill neurons for the two\nsubtasks, respectively. Figure 2 shows the acti-\nvation distribution of the two top skill neurons\nwithin RoBERTaBASE of the two subtasks, respec-\ntively. The samples of three labels form three distin-\nguishable clusters, which suggests the effectiveness\nof this skill-neuron-finding method. More details\nabout how we decompose the investigated tasks are\nshown in appendix A.\n4 Do Skill Neurons Encode Skills?\nWe explore whether skill neurons really encode\ntask-specific skills with a series of experiments.\n11135\n60 65 70 75 80 85 90 95\nPredictivity (%)\n0\n1000\n2000\n3000\n4000\n5000#Neuron\n Ours\nMax.\nAvg.\nFigure 3: Histogram of neuron’s predictivity for IMDB.\nError bars indicate ±1 s.e.m. over 5 random trials.\n4.1 Skill Neurons Generally and Stably\nEmerge\nWe first confirm that the skill neuron phenomenon\nis general and stable for various NLP tasks.\nGenerality. To explore whether we can gener-\nally find highly-predictive skill neurons for various\ntasks, we apply the skill-neuron-finding method in\n§ 3 to 7 NLP tasks introduced in § 2.3. The per-\nformances of the top-predictivity found skill neu-\nrons and prompt tuning are shown in Table 1. For\nall the tasks, we can find skill neurons achieving\ncomparable performance to prompt tuning, which\ndemonstrates specific skill neurons generally exist\nin pre-trained Transformers for various tasks.\nStability. To rule out the possibility that the skill\nneurons are just from randomness and confirm the\nstability of this phenomenon, we conduct5 random\ntrails (with different data orders and prompt initial-\nizations) to find skill neurons for all the tasks. Fig-\nure 3 shows the distributions of neuron predictivi-\nties within RoBERTaBASE for SST-2 task. Distribu-\ntions for the other tasks are left in appendix C. We\ncan see that our method can stably find substantial\nskill neurons with high predictivities via prompts.\nPrevious methods use average (Dai et al., 2021)\nand maximum (Suau et al., 2020) activations on in-\nput tokens instead of activations on prompts to find\nselective neurons, which are shown as the “Avg.”\nand “Max.” results in Figure 3, respectively. The\nexperimental results indicate that previous methods\nhardly find highly-predictive neurons, which sug-\ngests that prompt tuning is crucial for finding skill\nneurons. We encourage future work to explore the\nreason why prompt tuning can help in this.\n0 5 10 15\nPertubation Rate (%)\n60\n65\n70Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom\nFigure 4: Accuracy on Tweet drops along with the neu-\nron perturbation rate. Error bars indicate ±1 s.e.m. over\n5 random trials. The perturbations are conducted in de-\nscending orders of neurons’ predictivities for different\ntasks or in random order (the “Random” curve).\n4.2 Skill Neurons are Crucial for Handling\nTasks\nA natural hypothesis is that if the skill neurons\nreally encode skills, they shall be more important\nfor PLMs to handle various tasks. To verify this,\nwe perturb the skill neurons and see whether PLM’s\nperformance drops more than perturbing random\nneurons. Specifically, the perturbation is to add\na Gaussian noise ( µ = 0 and σ = 0.1) into the\nneurons’ activations (Arora et al., 2018), so that\nthe neurons cannot function properly, and then we\nobserve the PLM’s prompt tuning performances.\nThe perturbation results onTweet task are shown\nin Figure 4, from which we observe that when we\nperturb top skill neurons of this task, the PLM’s\nperformance drops much more significantly than\nwhen we perturb neurons in random order. It in-\ndicates that the highly-predictive skill neurons are\nindeed crucial for handling tasks and supports that\nskill neurons encode skills. Perturbation results on\nthe other tasks are shown in appendix D.1, and they\nall exhibit similar phenomena.\n4.3 Skill Neurons are Task-specific\nWe further study whether skill neurons are task-\nspecific, i.e., do skill neurons encode task-specific\nhigh-level skills like distinguishing sentiments for\nsentiment analysis, or do they just encode some\ntask-general low-level skills like recognizing parts\nof speech, which are also helpful for handling tasks.\nFirst, if skill neurons are task-specific, we shall\nfind similar skill neurons for similar tasks. To ver-\nify this, we rank neurons in descending orders of\ntheir predictivities for different tasks and see Spear-\n11136\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nFigure 5: Spearman’s rank correlations between the\nneuron predictivity orders of different tasks. Results are\naveraged over all the layers.\nman’s rank correlations (Spearman, 1987) between\nthe orders of different tasks. The average results\nover all the 12 layers of RoBERTaBASE are shown\nin Figure 5. We can see that the correlations be-\ntween similar tasks of the same type are obviously\nhigher, which confirms that similar tasks have sim-\nilar skill neurons. The layer-wise correlations are\nshown in appendix C, from which we can see skill\nneurons tend to be more task-specific in higher\nlayers, which is consistent with previous probing\nfindings (Liu et al., 2019a).\nMoreover, if skill neurons are task-specific, the\nskill neurons of same-type tasks shall be more im-\nportant for handling a specific task. This has been\nsupported by Figure 4, which shows that the ac-\ncuracy on Tweet drops much more significantly\nwhen we perturb neurons in the predictivity orders\nof same-type tasks (SST-2, IMDB). To qualify this\neffect and comprehensively show this phenomenon\nin all tasks, we define the neuronal importanceof\na source task to an evaluation task as the area be-\ntween the accuracy curves obtained by perturbing\nneurons in the predictivity order of the source task\nand in random order. For instance, in Figure 4, the\nneuronal importance of SST-2 to Tweet is the area\nbetween the blue curve and the gray curve. The\noverall neuronal importance is shown in Figure 6,\nfrom which we can see the skill neurons of same-\ntype tasks are obviously more important, which\nstrongly supports that the found skill neurons en-\ncode task-specific skills again.\n4.4 Skill Neurons are not from Word\nSelectivity\nPrevious works (Dai et al., 2021; Suau et al., 2020)\nshow that neurons in Transformers may selectively\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nEvaluation T ask\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\nSource T ask\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nFigure 6: Neuronal importances of different task pairs.\nResults are averaged over 5 random trials. For an evalu-\nation task, the neuronal importances of different source\ntasks are normalized as z-scores.\nCosine Similarity\nTop AGES, GES, ITIES, ause, UNCH,\nAGE, ORK, STE, TING, FE\nBottom sham, Nicol, bogus, Rox, Nay, contro,\nguy, uneven, arbitrarily, unnatural\nAverage Activation\nTop starters, village, oster, iddled, af,\nmafia, aley, tired, dep, ophobic\nBottom\nofficial, repression, illegal,\ncalled, ensible, regime, abusers,\nshould, creation, refuse\nTable 2: Related words for SST-2’s top skill neuron.\nactivate on some words or concepts. To confirm\nthat skill neurons encode skills, we show that skill\nneurons are not from these selectivities.\nWe first do case studies on the related words of\nthe top skill neurons, including the words with top\nand bottom cosine similarities between their input\nembeddings and the neuron weight vectors (Dai\net al., 2021), and the words with top and bottom av-\nerage activations (Suau et al., 2020). The results of\nSST-2 are shown in Table 2. We can see these\nrelated words do not convey sentiments, which\ndemonstrates the skill neurons are not from key-\nword selectivities. Results of the other tasks are\nshown in appendix F.\nFurthermore, considering the prompt tuning\nmethod does predictions by decoding label tokens,\nwe need to check whether skill neurons depend on\nthe label words used. If so, it indicates that the skill\nneurons do not encode the skills for handling tasks\nbut encode the skills for selectively decoding some\nwords. We rule out this possibility by finding that if\nwe use different random words as label words, the\n11137\nTask Random\nGuess\nRandom\nModel\nRandom\nPrompt\nHard\nPrompt\nSST-2 50.0 52 .8±0.4 78.1±0.4 83.3\nIMDB 50.0 58 .0±0.7 76.7±2.0 75.1\nTweet 33.3 48 .3±0.0 48.2±1.8 48.6\nMNLI 33.3 32 .2±0.4 39.8±1.1 40.5\nQNLI 50.0 54 .3±0.8 69.5±0.5 65.2\nAG News 50.0 62 .7±0.3 96.0±0.3 95.9\nDBpedia 50.0 60 .9±0.4 98.8±0.1 99.2\nTable 3: Accuracies (%) on various tasks of top skill\nneurons found with random prompts and untuned hard\nprompts, compared to random guess and random model.\nWe also report standard deviations over 5 random trials.\nachieved predictivity orders of neurons are pretty\nconsistent. Specifically, for all the tasks, the av-\nerage Spearman’s correlation between the neuron\npredictivity orders of 5 random label words is 0.87.\n5 Where do Skill Neurons Come from?\nIn § 4, we confirm that skill neurons do encode task-\nspecific skills. Then a natural question is where\nskill neurons come from, i.e., do skill neurons ac-\nquire these skills in pre-training or prompt tuning?\nWe find that skill neurons are most likely gener-\nated in pre-training with empirical evidence.\nWe first try to find skill neurons with tuning-free\nprompts, including random prompts, which are ran-\ndomly generated embeddings, and human-written\nhard prompts. The predictivities of the found neu-\nrons are shown in Table 3. We can see that even\nwithout tuning, we can still find neurons with non-\ntrivial predictivities. Malach et al. (2020) shows\nthat randomly initialized neural networks may have\npredictive subnetworks. Hence we also compare\nwith randomly initialized models using random\nprompts. It can be observed that the neurons in\nrandom models are predictive to some extent, but\ntheir predictivities are far below the neurons in pre-\ntrained models. These results imply that the skill\nneurons are generated in pre-training, and prompt\ntuning only serves as an effective tool to observe\nthe specificity of these neurons.\nTo provide stronger evidence, we explore\nwhether the skill neurons found with prompt tun-\ning are also important for other fine-tuning meth-\nods with different dynamics. We explore two\nparameter-efficient fine-tuning methods, including\nadapter-based tuning (Houlsby et al., 2019), which\nonly tunes the additional adapter layers plugged in\nTransformers, and BitFit (Ben-Zaken et al., 2022),\nwhich only tunes the bias vectors. The two tuning\n0 5 10 15\nPertubation Rate (%)\n70\n75\n80\n85\n90Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom\nFigure 7: BitFit accuracy on IMDB drops along with the\nneuron perturbation rate. Error bars indicate ±1 s.e.m.\nover 5 random trials. The perturbations are conducted\nin predictivity orders obtained with prompt tuning.\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nEvaluation T ask\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\nSource T ask\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nFigure 8: Average neuronal importance over models\ntrained with adapter-based tuning and BitFit.\nmethods both keep neuron weights fixed, which\nensures that the skill neurons are unchanged during\ntuning. BitFit model’s performances on IMDB when\nneurons are perturbed in the descending orders of\npredictivities obtained with prompts are shown in\nFigure 7, and the results for other tasks and adapter\nmodels are shown in appendix D. We can see the\nhighly-predictive skill neurons found with prompts\nare still crucial for models fine-tuned with other\nmethods. To comprehensively show this effect,\nsimilar to § 4.3, we visualize the average neuronal\nimportance over models trained with adapter-based\ntuning and BitFit in Figure 8. The skill neurons\nfound with prompt tuning also exhibit task-specific\nimportance, which again supports that skill neurons\nare generated in pre-training rather than manufac-\ntured by prompt tuning.\n6 Application\nWe further explore the applications of our skill neu-\nron finding. We show two preliminary use cases:\n11138\nTask Prompt\nTuning\nPruned\nModel Speedup\nSST-2 91.8±0.5 89.3±2.0 1.34\nIMDB 91.6±0.5 87.6±3.0 1.34\nTweet 70.0±0.2 69.0±0.9 1.34\nMNLI 76.8±1.8 70.0±1.1 1.38\nQNLI 85.7±0.7 81.0±1.0 1.36\nAG News 98.8±0.1 99.8±0.1 1.32\nDBpedia 99.7±0.1 99.0±0.1 1.33\nTable 4: Accuracies (%) on various tasks of vanilla\nprompt tuning and prompt tuning on pruned models,\nalong with standard deviations over 5 random trials.\nWe also report the achieved inference speedups on the\ntasks. Speedups are evaluated on a single CPU since it\nis widely used for model inference (Mittal et al., 2021).\nnetwork pruning and transferability indicator.\n6.1 Network Pruning\nFirst, we apply our skill neuron finding to network\npruning (Anwar et al., 2017; Dalvi et al., 2020),\nwhich is to reduce memory cost and accelerate\ninference by removing redundant parameters in\nneural networks. Existing works have explored\nprune PLMs with weight magnitude (Han et al.,\n2015; Gordon et al., 2020) and loss attribu-\ntion (Michel et al., 2019). Here we explore prune\nPLMs by only keeping the top 2% skill neurons\nactive for each task and set the activations of\nthe 98% frozen neurons always as their baseline\nactivations. Considering that the frozen neurons\nare fixed, we merge them into bias terms. We\napply this pruning method to the top 9 layers\nof RoBERTaBASE and reduce it to 66.6% of its\noriginal parameters. The performances of prompt\ntuning on pruned models and vanilla prompt tuning\non the original model are shown in Table 4. Our\npruning based on skill neurons generally performs\ncomparably to vanilla prompt tuning and can\nachieve about 1.4 inference speedup.\n6.2 Transferability Indicator\nPrevious works (Su et al., 2021; Vu et al., 2021)\nexplore improving prompt tuning with cross-task\nprompt transfer. Su et al. (2021) propose that\nthe overlapping rate of activated neurons(ON)\nbetween soft prompts can serve as a prompt trans-\nferability indicator, which has good correlations\nwith zero-shot prompt transferability and can help\nto qualify task similarities and improve prompt\ntransfer. Su et al. (2021) take all neurons into ON\ncalculation, but the redundant neurons without task-\nspecific skills may bring noisy signals. Here we\nonly take the top 20% skill neurons of target tasks\ninto the calculation. This improves the average\nSpearman’s correlation between ON and prompt\ntransferability over our tasks from 0.53 to 0.71.\n7 Related Work\nSelective Neurons in Artificial Neural Networks\nThere have long been findings about selective neu-\nrons in artificial neural networks. Many computer\nvision works (Coates et al., 2012; Le et al., 2013;\nZeiler and Fergus, 2014; Agrawal et al., 2014; Zhou\net al., 2015; Bau et al., 2020) find that both super-\nvised and unsupervised models can have units se-\nlectively respond to specific visual objects and con-\ncepts. Radford et al. (2017) also find neurons corre-\nsponding to sentiments in unsupervised long short-\nterm memory networks. Interestingly, there are\nsimilar selective neurons in human brains (Barlow,\n1972; Quiroga et al., 2005). The widespread emer-\ngence of these neuronal selectivities implies that\nthere may be common learning mechanisms among\nintelligent systems, which is extremely worthwhile\nto explore in the future.\nBau et al. (2017) and Mu and Andreas (2020)\nfind that selective neurons are more important,\nwhich is consistent with our findings. However,\nMorcos et al. (2018) draw opposite conclusions.\nWe discuss this with experiments in appendix H.\nAnalyzing Pre-trained Transformers After the\nsuccess of Transformer-based PLMs (Devlin et al.,\n2019; Yang et al., 2019; Raffel et al., 2020), many\nefforts have been devoted to analyzing how PLMs\nwork, such as probing the knowledge of PLMs (Liu\net al., 2019a; Hewitt and Manning, 2019; Petroni\net al., 2019) and understanding the behaviors of\nPLMs’ parameters (V oita et al., 2019; Clark et al.,\n2019). Among these, some works (Dalvi et al.,\n2019; Durrani et al., 2020; Antverg and Belinkov,\n2022) find that individual neurons capture linguistic\nproperties, but they define neurons as dimensions in\ncontextualized representations. Other works (Suau\net al., 2020; Geva et al., 2021; Dai et al., 2021)\nstudy the same group of neurons as us and find that\nsome neurons encode specific information like con-\ncepts, facts, and word patterns. Inspired by them,\nwe study whether neurons encode high-level skills\nfor handling tasks in this work and demonstrate\nthat we can observe skill neurons with the help\nof prompts. We believe it is promising to explore\nwhether and how skill neurons collaborate with the\nneurons encoding information in future works.\n11139\n8 Conclusion and Future Work\nIn this paper, we find some special neurons in\npre-trained Transformers whose activations on soft\nprompts are highly predictive of the task labels of\ninputs. We dub these neurons skill neurons and\ndevelop a method to find them via prompt tun-\ning. With extensive experiments, we confirm that\nskill neurons encode task-specific skills required\nto handle these tasks and find empirical evidence\nshowing that skill neurons are most likely gener-\nated in pre-training rather than fine-tuning. We also\ndemonstrate some practical applications of our skill\nneuron finding. In the future, we will extend our\nprompt-based skill neuron finding method to more\nscenarios, such as covering non-classification tasks\nand other parameters in Transformers like atten-\ntion heads. We will also explore more fundamen-\ntal problems about skill neurons and the working\nmechanisms of PLMs, including how the skill neu-\nrons emerge in pre-training, as well as the relation-\nships between skill neurons and neurons encoding\nspecific information found in previous works.\nLimitations\nAlthough we conducted extensive experiments,\nthe exploration scope of this work has some lim-\nitations: (1) The experimental analyses are all\nbased on RoBERTaBASE . Whether the skill neuron\nphenomenon widely exists for other Transformer-\nbased pre-trained language models is unclear and\nmore explorations are needed to verify it. (2) The\ndatasets used in our experiments are all English,\nwhich limits the linguistic features covered in our\nanalyses, and the evaluation tasks are limited to\nclassification tasks. We choose English just be-\ncause of its rich resource. Although we intuitively\nbelieve the observed phenomena are not dependent\non the English language, experiments on more di-\nverse languages are needed in future works. (3)\nFollowing previous works (Geva et al., 2021; Dai\net al., 2021), the analyzed neurons in our work all\ndistribute in the feed-forward layers of Transform-\ners. Deeper analyses may require considering other\nparameters like the attention heads. We encourage\nfuture works to address these limitations and get\nmore comprehensive analysis results.\nAcknowledgements\nThis work is supported by the New Generation Ar-\ntificial Intelligence of China (2020AAA0106501),\nthe Institute for Guo Qiang, Tsinghua University\n(2019GQB0003), and Huawei Noah’s Ark Lab. We\nthank anonymous reviewers for their suggestions.\nReferences\nPulkit Agrawal, Ross B. Girshick, and Jitendra Malik.\n2014. Analyzing the performance of multilayer neu-\nral networks for object recognition. In Proceedings\nof ECCV, pages 329–344.\nOmer Antverg and Yonatan Belinkov. 2022. On the\npitfalls of analyzing individual neurons in language\nmodels. In Proceedings of ICLR.\nSajid Anwar, Kyuyeon Hwang, and Wonyong Sung.\n2017. Structured pruning of deep convolutional neu-\nral networks. ACM Journal on Emerging Technolo-\ngies in Computing Systems (JETC), 13(3):1–18.\nSanjeev Arora, Rong Ge, Behnam Neyshabur, and\nYi Zhang. 2018. Stronger generalization bounds for\ndeep nets via a compression approach. In Proceed-\nings of ICML, pages 254–263.\nSören Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives. 2007.\nDBpedia: A nucleus for a web of open data. In\nProceedings of ISWC/ASWC, pages 722–735.\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020. TweetEval:\nUnified benchmark and comparative evaluation for\ntweet classification. In Findings of EMNLP, pages\n1644–1650.\nHorace B Barlow. 1972. Single units and sensation: A\nneuron doctrine for perceptual psychology? Percep-\ntion, 1(4):371–394.\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir\nDurrani, Fahim Dalvi, and James Glass. 2018. Iden-\ntifying and controlling important neurons in neural\nmachine translation. In Proceedings of ICLR.\nDavid Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and\nAntonio Torralba. 2017. Network dissection: Quanti-\nfying interpretability of deep visual representations.\nProceedings of CVPR, pages 3319–3327.\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata\nLapedriza, Bolei Zhou, and Antonio Torralba. 2020.\nUnderstanding the role of individual units in a\ndeep neural network. Proceedings of the National\nAcademy of Sciences, 117(48):30071–30078.\nElad Ben-Zaken, Shauli Ravfogel, and Yoav Goldberg.\n2022. BitFit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In\nProceedings of ACL.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\n11140\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nProceedings of NeurIPS, pages 1877–1901.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286.\nAdam Coates, Andrej Karpathy, and A. Ng. 2012. Emer-\ngence of object-selective features in unsupervised\nfeature learning. In Proceedings of NeurIPS, pages\n2681–2689.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu\nWei. 2021. Knowledge neurons in pretrained trans-\nformers. arXiv preprint, arXiv:2104.08696.\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Be-\nlinkov, Anthony Bau, and James Glass. 2019. What\nis one grain of sand in the desert? analyzing individ-\nual neurons in deep nlp models. In Proceedings of\nAAAI, pages 6309–6317.\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and\nYonatan Belinkov. 2020. Analyzing redundancy in\npretrained transformer models. In Proceedings of\nEMNLP, pages 4908–4926.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT, pages\n4171–4186.\nYihe Dong, Jean-Baptiste Cordonnier, and Andreas\nLoukas. 2021. Attention is not all you need: pure\nattention loses rank doubly exponentially with depth.\nIn Proceedings of ICML, pages 2793–2803.\nNadir Durrani, Hassan Sajjad, Fahim Dalvi, and\nYonatan Belinkov. 2020. Analyzing individual neu-\nrons in pre-trained language models. In Proceedings\nof EMNLP, pages 4865–4880.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of EMNLP, pages\n5484–5495.\nMitchell A Gordon, Kevin Duh, and Nicholas Andrews.\n2020. Compressing bert: Studying the effects of\nweight pruning on transfer learning. arXiv preprint\narXiv:2002.08307.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015. Learning both weights and connections for\nefficient neural network. In Proceedings of NeurIPS,\npages 1135–1143.\nXu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao\nLiu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao\nHan, Minlie Huang, et al. 2021. Pre-trained models:\nPast, present and future. AI Open, pages 225–250.\nLucas Torroba Hennigen, Adina Williams, and Ryan\nCotterell. 2020. Intrinsic probing through dimension\nselection. In Proceedings of EMNLP, pages 197–\n216.\nJohn Hewitt and Christopher D Manning. 2019. A struc-\ntural probe for finding syntax in word representations.\nIn Proceedings of NACCL-HLT, pages 4129–4138.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of ICML, pages 2790–2799.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.\nVisualizing and understanding recurrent networks.\narXiv preprint arXiv:1506.02078, pages 818–833.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nQuoc V . Le, Marc’Aurelio Ranzato, Rajat Monga,\nMatthieu Devin, Gregory S. Corrado, Kai Chen, Jef-\nfrey Dean, and A. Ng. 2013. Building high-level\nfeatures using large scale unsupervised learning. In\nProceedings of ICASSP, pages 8595–8598.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of EMNLP, pages 3045–\n3059.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Clément Delangue, Théo Matus-\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, François\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In Proceedings of EMNLP, pages 175–\n184.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of ACL, pages 4582–4597.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\n11141\nrepresentations. In Proceedings of NAACL-HLT,\npages 1073–1094.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin\nYang, and Jie Tang. 2022. P-Tuning v2: Prompt\ntuning can be comparable to fine-tuning universally\nacross scales and tasks. In Proceedings of ACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907,11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of ACL-HLT, pages 142–150.\nEran Malach, Gilad Yehudai, Shai Shalev-shwartz, and\nOhad Shamir. 2020. Proving the lottery ticket hy-\npothesis: Pruning is all you need. In Proceedings of\nICML.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Proceed-\nings of NeurIPS, pages 14014–14024.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2021. Fast model\nediting at scale. In Proceedings of ICLR.\nSparsh Mittal, Poonam Rajput, and Sreenivas Subra-\nmoney. 2021. A survey of deep learning on CPUs:\nopportunities and co-optimizations. IEEE Transac-\ntions on Neural Networks and Learning Systems,\npages 1–21.\nAri S Morcos, David GT Barrett, Neil C Rabinowitz,\nand Matthew Botvinick. 2018. On the importance of\nsingle directions for generalization. In Proceedings\nof ICLR.\nJesse Mu and Jacob Andreas. 2020. Compositional\nexplanations of neurons. In Proceedings of NeurIPS,\npages 17153–17163.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of EMNLP-IJCNLP,\npages 2463–2473.\nOfir Press, Noah A. Smith, and Omer Levy. 2020. Im-\nproving transformer models by reordering their sub-\nlayers. In Proceedings of ACL, pages 2996–3005.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of NAACL-HLT, pages 5203–5212.\nR Quian Quiroga, Leila Reddy, Gabriel Kreiman,\nChristof Koch, and Itzhak Fried. 2005. Invariant\nvisual representation by single neurons in the human\nbrain. Nature, 435(7045):1102–1107.\nAlec Radford, Rafal Józefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv:1704.01444.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1–\n67.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 task 4: Sentiment analysis in Twitter.\nIn Proceedings of SemEval, pages 502–518.\nBernardo Rudy, Gordon Fishell, SooHyun Lee, and Jens\nHjerling-Leffler. 2011. Three groups of interneurons\naccount for nearly 100% of neocortical gabaergic\nneurons. Developmental neurobiology, 71(1):45–61.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of EACL,\npages 255–269.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of EMNLP, pages 1631–1642.\nCharles Spearman. 1987. The proof and measurement\nof association between two things. In Proceedings of\nAJP, 3/4, pages 441–471.\nYusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan,\nYankai Lin, Zhiyuan Liu, Peng Li, Juanzi Li, Lei\nHou, Maosong Sun, et al. 2021. On transferability\nof prompt tuning for natural language understanding.\narXiv preprint arXiv:2111.06719.\nXavier Suau, Luca Zappella, and Nicholas Apostoloff.\n2020. Finding experts in transformer models. arXiv\npreprint arXiv:2005.07647.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NeurIPS, pages 5998–\n6008.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting,\nthe rest can be pruned. In Proceedings of NAACL,\npages 5797–5808.\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and\nDaniel Cer. 2021. Spot: Better frozen model adap-\ntation through soft prompt transfer. arXiv preprint\narxiv:2110.07904.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Proceed-\nings of ICLR.\n11142\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of NAACL-HLT, pages 1112–1122.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of EMNLP,\npages 38–45.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Proceedings of NeurIPS,\npages 5754–5764.\nMatthew D. Zeiler and Rob Fergus. 2014. Visualiz-\ning and understanding convolutional networks. In\nProceedings of ECCV.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text classi-\nfication. In Proceedings of NeurIPS, pages 649–657.\nZhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li,\nMaosong Sun, and Jie Zhou. 2021. Moefication:\nConditional computation of transformer models for\nefficient inference. arXiv preprint arXiv:2110.01786.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning to\nrecall. In Proceedings of NAACL, pages 5017–5033.\nBolei Zhou, Aditya Khosla, Àgata Lapedriza, Aude\nOliva, and Antonio Torralba. 2015. Object detectors\nemerge in deep scene cnns. In Proceedings of ICLR.\n11143\nAppendices\nA Details about Investigated Tasks\nIn experiments, we use7 established public English\nNLP datasets, which are licensed and intended for\nresearch use. These datasets are all created with\npublic texts, and we believe they do not involve per-\nsonal information and are well anonymized. The\ndetails about the datasets are as follows:\nA.1 Sentiment Analysis\nSST-2 (Socher et al., 2013) requires to classify\nthe sentiments expressed in movie reviews into\nPOSITIVE and NEGATIVE sentiments.\nIMDB (Maas et al., 2011) requires to classify the\nsentiments expressed in reviews from the Internet\nMovie Database2 into POSITIVE and NEGATIVE\nsentiments.\nTweetEval (Barbieri et al., 2020) is a collection of\n7 Twitter-specific classification tasks. Here we use\nits sentiment analysis subtask, which is originally\nfrom SemEval 2017 Task 4 (Rosenthal et al., 2017).\nIt requires to recognize if a tweet is POSITIVE ,\nNEGATIVE or NEUTRAL . We decompose it to two\nsubtasks: POSITIVE vs. NEGATIVE , and NEURAL\nvs. N ON-NEUTRAL .\nA.2 Natural Language Inference\nMNLI (Williams et al., 2018) requires to recog-\nnize the relationship between sentence pairs as\nENTAILMENT , NEUTRAL and CONTRADICTION .\nWe decompose it to two subtasks: ENTAILMENT\nvs. CONTRADICTION , and NEURAL vs. NON-\nNEUTRAL .\nQNLI (Wang et al., 2019) requires to classify\nwhether a context sentence contains the answer\nto a question.\nA.3 Topic Classification\nAG News (Zhang et al., 2015) requires to classify\nthe 4 topics of news articles in the AG’s corpus3.\nDBpedia (Zhang et al., 2015) requires to classify\nthe 14 topics of articles in DBpedia (Auer et al.,\n2007).\nSince recognizing different topics requires essen-\ntially different skills, we use the only two similar\nlabels of the two tasks. They are BUSINESS and\nSPORTS in AG News, and COMPANY and ATHLETE\nin DBpedia.\n2https://www.imdb.com\n3http://groups.di.unipi.it/~gulli/AG_corpus_\nof_news_articles.html\nTask Training Validation Test\nSST-2 53, 879 13 , 470 872\nIMDB 20, 000 5 , 000 25 , 000\nTweet 45, 615 2 , 000 12 , 284\nMNLI 314, 161 78 , 541 9 , 815\nQNLI 83, 794 20 , 949 5 , 463\nAG News 47, 966 12 , 034 3 , 800\nDBpedia 63, 899 16 , 100 9 , 999\nTable 5: Data statistics of the 7 used datasets.\nWe obtain the datasets from Huggingface’s\ndataset platform (Lhoest et al., 2021). For the\ndatasets included in the GLUE collection (Wang\net al., 2019), since we cannot get their test set,\nwe use the released validation set as our test set,\n80% random samples from the original training set\nas our training set, and the other 20% samples as\nour validation set. The detailed data statistics are\nshown in Table 5.\nB Implementations Details\nWe implement the prompt tuning method intro-\nduced in § 2.1 with l = 127 soft prompts. We\nrandomly initialize each soft prompt using a nor-\nmal distribution with the standard deviation as0.03.\nWe then train the model using Adam (Kingma and\nBa, 2015) as the optimizer. We set the learning\nrate as 0.001 and the batch size as 8. We do the\nevaluation on the validation set every2,000 itera-\ntions and early stop the training if the validation\naccuracy does not rise for 6 times. We use label\nwords Negative, Positive for binary classifica-\ntion tasks and Negative, Neutral, Positive\nfor multi-class classification tasks. For the ran-\ndom label words experiment in § 4.4, we uniformly\nsample the label words from the vocabulary of\nRoBERTa (Liu et al., 2019b).\nWe conduct all experiments on RoBERTaBASE\nmodel, which has 110M parameters, and we use\nHuggingface’s Transformers library (Wolf et al.,\n2020) to implement the experiments. We run the\nexperiments on NVIDIA GeForce RTX 2080 Ti\nand NVIDIA GeForce RTX 3090 GPUs, and it\ntakes about 1000 GPU hours.\nC More Predictivity Distributions\nWe report the predictivity distribution for IMDB\nin § 4.1 and show the distributions for the other 4\nbinary classification tasks in Figure 9. We can see\nour method can stably find many highly-predictive\nskill neurons for all the tasks. For the multi-class\n11144\nclassification tasks, since the predictivities are for\ndecomposed subtasks, we cannot draw distribu-\ntions for the original tasks and do not include them\nin the results here.\nD More Neuron Perturbation Results\nHere we demonstrate more neuron perturbation\nexperimental results.\nD.1 Performance Dropping Trends for\nPrompt Tuning\nIn Figure 4, we show the performance dropping\ntrend on Tweet task. The results on the other tasks\nare shown in Figure 11.\nD.2 Performance Dropping Trends for\nAdapter-based Tuning\nThe performance dropping trends of adapter-based\ntuning models on various tasks are shown in Fig-\nure 12.\nD.3 Performance Dropping Trends for BitFit\nThe performance dropping trends of BitFit models\non various tasks are shown in Figure 13.\nE Layer-wise Correlations between\nNeuron Predictivity Orders of Different\nTasks\nFigure 5 shows the overall Spearman’s rank corre-\nlations between the neuron predictivity orders of\ndifferent tasks, which is averaged over the12 lay-\ners of RoBERTaBASE . Here we further present the\nlayer-wise correlations in Figure 14, from which\nwe can see the skill neurons are more and more\ntask-specific from the bottom layer to the top layer,\nwhich is consistent with the probing findings (Liu\net al., 2019a) showing that PLMs tend to learn gen-\neral skills in the lower layers and learn specific\nskills in the higher layers. These results suggest\nthat our neuron-finding method can find both neu-\nrons encoding general skills in the lower layers and\nneurons encoding specific skills in the lower layers,\nbut the found top skill neurons are task-specific\nin general (Figure 5). In this work, we focus on\nthe task-specific top skill neurons and leave careful\nstudy for the neurons encoding general skills in\nfuture work.\nF More Word Selectivity Results\nIn Table 2, we show the related words for SST-2.\nHere we further show the results for the other tasks\nin Table 6. We can see these related words gener-\nally do not convey clues about solving the tasks.\nG Discussions on Neuron-Finding Design\nChoices\nIn this section, we discuss some potential other de-\nsign choices that may be used in finding important\nskill neurons to provide more background about\nwhy we choose the method described in § 3 finally\nand inspire future works.\nPerturbation-based neuron finding. A natural\nway to define the importance of a neuron (to a task)\nis to perturb the neurons and see how they influence\nthe predictions. The perturbation-based method has\nbeen used in previous analysis works (Michel et al.,\n2019), and we also adopt them in our analytical\nexperiments. But we and many other neuron-level\nanalysis works (Dalvi et al., 2019; Durrani et al.,\n2020; Antverg and Belinkov, 2022; Suau et al.,\n2020; Geva et al., 2021; Dai et al., 2021) cannot di-\nrectly use this method to locate important neurons.\nThis is because of the efficiency issue. Perturbing\nevery individual neuron is unaffordable.\nIs prompt tuning necessary? This work starts\nfrom an interesting empirical finding, i.e., the skill\nneuron phenomenon. This finding is based on\nprompt tuning. In § 4 and Figure 3, we show that\nprevious methods without prompt tuning cannot\nwell locate the skill neurons. Since we focus on\nconfirming the finding and exploring the properties\nof skill neurons, we conduct all the experiments\nbased on prompt tuning and do not explore whether\nit is necessary. Intuitively, as our experiments sug-\ngest that the emergence of skill neurons does not\ndepend on prompt tuning but is mostly an intrin-\nsic property for pre-trained Transformer-based lan-\nguage models, we believe prompt tuning may not\nbe the only way to locate skill neurons. We will\nexplore other methods without prompt tuning in\nfuture works, which may bring some benefits, like\nimproving overall efficiency.\nOther ways to define neuron’s predictivity. In\n§ 3.1, we define the predictivity of a neuron (1)\nusing the maximum over prompt tokens and (2)\nconsidering both the positive and negative correla-\ntions. These two choices are made with preliminary\nexperiments. Figure 10 shows an example, from\nwhich we can see that when defining neuron’s pre-\ndictivity using the mean values over prompt tokens\n11145\n60 65 70 75 80 85 90\nPredictivity (%)\n0\n1000\n2000\n3000\n4000\n5000\n6000#Neuron Ours\nAvg.\nMax.\n(a) SST-2\n60 65 70 75 80 85\nPredictivity (%)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000#Neuron Ours\nMax.\nAvg. (b) QNLI\n80 85 90 95 100\nPredictivity (%)\n0\n5000\n10000\n15000\n20000#Neuron Ours\nMax.\nAvg.\n(c) DBpedia\n80 85 90 95 100\nPredictivity (%)\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000#Neuron Ours\nMax.\nAvg. (d) AG News\nFigure 9: Histograms of predictivity for various tasks on neurons within RoBERTa BASE . Error bars indicate ±1\ns.e.m. over 5 random trials.\n60 65 70 75 80 85 90\nPredictivity (%)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000#Neuron\nDefault\nOnly Positive Correlation\nMean Over Prompt T okens\nFigure 10: Histogram of neuron’s predictivity in differ-\nent definitions for SST-2. Error bars indicate ±1 s.e.m.\nover 5 random trials.\nor only considering the positive correlations, the\npredictivities will be significantly under-estimated\nthan the default definition in § 3.1.\nH Experiments following Morcos et al.\n(2018)\nSome previous works (Bau et al., 2017; Mu and\nAndreas, 2020) suggest that selective neurons con-\ntribute more to model accuracies. In § 4, we also\nfind that perturbing selective skill neurons leads to\nmore performance drop. However, Morcos et al.\n(2018) draw opposite conclusions and find that se-\nlective and non-selective neurons are similarly im-\nportant. These pose questions about why these\nconclusions are inconsistent.\nWe find that except for experimental setups, the\nmain difference between Morcos et al. (2018) and\nours lies in the definition of neuronal selectivity.\nMorcos et al. (2018) define a \"selectivity index\"\nand we use the predictivity score introduced in § 3.\nTo check whether these different definitions lead to\ninconsistent results, we do experiments under our\nsetup and also try to perturb neurons in descending\norders of their “selectivity index”. The results are\nshown in Figure 15. We can see that when using the\n“selectivity index”, the found neurons are surely not\n11146\n0 5 10 15\nPertubation Rate (%)\n75\n80\n85\n90Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom\n(a) On SST-2\n0 5 10 15\nPertubation Rate (%)\n70\n75\n80\n85\n90Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom (b) On IMDB\n0 5 10 15\nPertubation Rate (%)\n65\n70\n75Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom\n(c) On MNLI\n0 5 10 15\nPertubation Rate (%)\n75\n80\n85Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom (d) On QNLI\n0 5 10 15\nPertubation Rate (%)\n97.5\n98.0\n98.5\n99.0Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom\n(e) On AG News\n0 5 10 15\nPertubation Rate (%)\n98.5\n99.0\n99.5Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom (f) On DBpedia\nFigure 11: Accuracies on various tasks drop along with the neuron perturbation rates. Error bars indicate ±1 s.e.m.\nover 5 random trials. The perturbations are conducted in descending orders of neurons’ predictivities for different\ntasks or in random order (the “Random” curve).\nmore important than random neurons as reported\nby Morcos et al. (2018). But our predictivity metric\ncan find significantly more important neurons for\nall the tasks.\n11147\nIMDB\nCosine Similarity\nTop legged, turnout, ladder, heid, flexible, Quite, contrary, runs, Reference, enqu\nBottom qq, qa, Capture, Import, Tripoli, hereby, eus, ,, rip, Lima\nAverage Activation\nTop success, Kund, Sanctuary, Lim, Wave, dele, Crystal, flung, Kerala, .............\nBottom vation, goodbye, concludes, bye, Congratulations,\nCongratulations, Fare, farewell, BY, ceremony,\nTweet\nCosine Similarity\nTop atican, uras, isman, anan, Luck, Merit, Character, alth, atching, character,\nBottom Register, enzymes, elsen, Registrar, tasting, regist, soils, µ, Chambers, LINE,\nAverage Activation\nTop dh, Titan, utable, exited, iOS, chel, loophole, acious, 520, Harmony,\nBottom spike, unbelievably, Toxic, prov, RIS, resulting, risks, rising, ues, reapp,\nMNLI\nCosine Similarity\nTop trigger, Pis, deadlines, Launch, mares,\nPROGRAM, Congratulations, Success, Congratulations, Gig,\nBottom minim, xt, spoof, dism, avoid, asive, WN, offset, inter, antiqu,\nAverage Activation\nTop nickel, grun, cluded, 91, handled, secure, very, dairy, gent, Roses,\nBottom ayed, disl, ect, wipes, screwed, resistance, aw, ruin, shrinking, spite,\nQNLI\nCosine Similarity\nTop otyp, disemb, sidel, melanch, unint, outwe, umbnails, precedence, unfl, Sym,\nBottom 314, 223, 313, 234, ,, 316, 341, 463, 238, 261,\nAverage Activation\nTop eds, adding, apocalypse, strawberry, apopt, Kid, leaf, Silent, technical,\nBottom entrepreneurial, Econom, Columb, prime, roleum, Trade, rounded, isner, enz, 158,\nAG News\nCosine Similarity\nTop aukee, erity, lambda, ropolitan, roxy, LAN, ylon, incinn, oslav, coni,\nBottom Gross, Villa, Uri, ende, Summary, Gallup, Temp, Rog, RP, Ram,\nAverage Activation\nTop fight, desert, Merge, Mail, Mid, Rankings, istic, **, berries, Pen,\nBottom ETS, 107, Line, 106, observers, Ranked, EB, ido, Bass, alf,\nDBpedia\nCosine Similarity\nTop ming, umbered, hind, utter, pepper, scr, increment, usher, empt, atmospheric,\nBottom Chron, kan, Div, Case, Thread, Role, Crash, Mode, Tank, Apps,\nAverage Activation\nTop Bubble, mailed, Ari, razen, Perspective, ogical, Gin, Disney, icons, Huang,\nBottom Jacob, Boss, Dad, trough, Shiny, carn, Gravity, toolbar, Sword, temple,\nTable 6: Related words for various tasks’ top skill neurons.\n11148\n0 5 10 15\nPertubation Rate (%)\n70\n75\n80\n85\n90Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom\n(a) On SST-2\n0 5 10 15\nPertubation Rate (%)\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom (b) On IMDB\n0 5 10 15\nPertubation Rate (%)\n55\n60\n65\n70Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom\n(c) On Tweet\n0 5 10 15\nPertubation Rate (%)\n50\n55\n60\n65\n70\n75\n80\n85Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom (d) On MNLI\n0 5 10 15\nPertubation Rate (%)\n60\n65\n70\n75\n80\n85\n90Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom\n(e) On QNLI\n0 5 10 15\nPertubation Rate (%)\n98.0\n98.5\n99.0Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom (f) On AG News\n0 5 10 15\nPertubation Rate (%)\n99.8\n99.9Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom\n(g) On DBpedia\nFigure 12: Adapter-based tuning accuracies on various tasks drop along with the neuron perturbation rates. Error\nbars indicate ±1 s.e.m. over 5 random trials. The perturbations are conducted in predictivity orders obtained with\nprompt tuning.\n11149\n0 5 10 15\nPertubation Rate (%)\n55\n60\n65\n70\n75\n80\n85\n90\n95Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom\n(a) On SST-2\n0 5 10 15\nPertubation Rate (%)\n55\n60\n65\n70Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom (b) On Tweet\n0 5 10 15\nPertubation Rate (%)\n40\n45\n50\n55\n60\n65\n70\n75\n80Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom\n(c) On MNLI\n0 5 10 15\nPertubation Rate (%)\n55\n60\n65\n70\n75\n80\n85\n90Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom (d) On QNLI\n0 5 10 15\nPertubation Rate (%)\n93.0\n93.5\n94.0\n94.5\n95.0\n95.5\n96.0\n96.5\n97.0\n97.5\n98.0\n98.5\n99.0Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom\n(e) On AG News\n0 5 10 15\nPertubation Rate (%)\n99.7\n99.8\n99.9Accuracy (%)\nSST-2\nIMDB\nT weet\nQNLI\nMNLI\nAG News\nDBpedia\nRandom (f) On DBpedia\nFigure 13: BitFit accuracies on various tasks drop along with the neuron perturbation rates. Error bars indicate ±1\ns.e.m. over 5 random trials. The perturbations are conducted in predictivity orders obtained with prompt tuning.\n11150\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia 0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(a) Layer 1\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n (b) Layer 2\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n (c) Layer 3\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n(d) Layer 4\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n (e) Layer 5\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n (f) Layer 6\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n(g) Layer 7\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n (h) Layer 8\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n (i) Layer 9\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n(j) Layer 10\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n (k) Layer 11\nSST-2 IMDB T weet MNLI QNLI\nAG NewsDBpedia\nSST-2\nIMDB\nT weet\nMNLI\nQNLI\nAG News\nDBpedia 0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n (l) Layer 12\nFigure 14: Spearman’s rank correlations between the neuron predictivity orders of different tasks on different layers.\nLayer 1 is the bottom layer near the inputs, and layer 12 is the top layer near the outputs.\n11151\n0 5 10 15\nPertubation Rate (%)\n75\n80\n85\n90Accuracy (%)\nSelectivity Index\nOurs\nRandom\n(a) On SST-2\n0 5 10 15\nPertubation Rate (%)\n70\n75\n80\n85\n90Accuracy (%)\nSelectivity Index\nOurs\nRandom (b) On IMDB\n0 5 10 15\nPertubation Rate (%)\n60\n65\n70Accuracy (%)\nSelectivity Index\nOurs\nRandom\n(c) On Tweet\n0 5 10 15\nPertubation Rate (%)\n65\n70\n75Accuracy (%)\nSelectivity Index\nOurs\nRandom (d) On MNLI\n0 5 10 15\nPertubation Rate (%)\n75\n80\n85Accuracy (%)\nSelectivity Index\nOurs\nRandom\n(e) On QNLI\n0 5 10 15\nPertubation Rate (%)\n97.5\n98.0\n98.5Accuracy (%)\nSelectivity Index\nOurs\nRandom (f) On AG News\n0 5 10 15\nPertubation Rate (%)\n98.5\n99.0\n99.5Accuracy (%)\nSelectivity Index\nOurs\nRandom\n(g) On DBpedia\nFigure 15: Prompt tuning accuracies on various tasks drop along with the neuron perturbation rates. Error bars\nindicate ±1 s.e.m. over 5 random trials. The perturbations are conducted in descending predictivity orders (Ours),\nrandom orders (Random) and descending \"selectivity index\" (Morcos et al., 2018) orders (Selectivity Index).\n11152",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7554221153259277
    },
    {
      "name": "Computer science",
      "score": 0.7426266670227051
    },
    {
      "name": "ENCODE",
      "score": 0.5549978613853455
    },
    {
      "name": "Transferability",
      "score": 0.422616571187973
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41775017976760864
    },
    {
      "name": "Machine learning",
      "score": 0.40064379572868347
    },
    {
      "name": "Voltage",
      "score": 0.14966845512390137
    },
    {
      "name": "Biology",
      "score": 0.0734967291355133
    },
    {
      "name": "Engineering",
      "score": 0.07259392738342285
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Logit",
      "score": 0.0
    }
  ]
}