{
  "title": "Corpus Complexity Matters in Pretraining Language Models",
  "url": "https://openalex.org/W4385571141",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2155196028",
      "name": "Ameeta Agrawal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104716290",
      "name": "Suresh Singh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4288087322",
    "https://openalex.org/W2096333253",
    "https://openalex.org/W2950419928",
    "https://openalex.org/W4367016270",
    "https://openalex.org/W3090789254",
    "https://openalex.org/W1967390364",
    "https://openalex.org/W2146950091",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2993383518",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3137010024",
    "https://openalex.org/W2964125718",
    "https://openalex.org/W4289259401",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2550587029",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3212940836",
    "https://openalex.org/W4225156065",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4297801177",
    "https://openalex.org/W3193344850",
    "https://openalex.org/W3153617151",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W2611598898",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4205411913",
    "https://openalex.org/W2911489562"
  ],
  "abstract": "It is well known that filtering low-quality data before pretraining language models or selecting suitable data from domains similar to downstream task datasets generally leads to improved downstream performance.However, the extent to which the quality of a corpus, in particular its complexity, affects its downstream performance remains less explored.In this work, we address the problem of creating a suitable pretraining corpus given a fixed corpus budget.Using metrics of text complexity we propose a simple yet effective approach for constructing a corpus with rich lexical variation.Our extensive set of empirical analyses reveal that such a diverse and complex corpus yields significant improvements over baselines consisting of less diverse and less complex corpora when evaluated in the context of general language understanding tasks.",
  "full_text": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP), pages 257–263\nJuly 13, 2023 ©2023 Association for Computational Linguistics\nCorpus Complexity Matters in Pretraining Language Models\nAmeeta Agrawal and Suresh Singh\nDepartment of Computer Science\nPortland State University\n{ameeta,singhsp}@pdx.edu\nAbstract\nIt is well known that filtering low-quality data\nbefore pretraining language models or selecting\nsuitable data from domains similar to down-\nstream task datasets generally leads to im-\nproved downstream performance. However,\nthe extent to which the quality of a corpus,\nin particular its complexity, affects its down-\nstream performance remains less explored. In\nthis work, we address the problem of creating\na suitable pretraining corpus given a fixed cor-\npus budget. Using metrics of text complexity\nwe propose a simple yet effective approach for\nconstructing a corpus with rich lexical varia-\ntion. Our extensive set of empirical analyses\nreveal that such a diverse and complex corpus\nyields significant improvements over baselines\nconsisting of less diverse and less complex cor-\npora when evaluated in the context of general\nlanguage understanding tasks.\n1 Introduction\nThe recent trend in training language models (LM)\nhas been to use increasingly larger text corpora\n(Khandelwal et al., 2019; Kaplan et al., 2020;\nBorgeaud et al., 2021). While this approach gen-\nerally does improve downstream performance, it\ncomes at a substantial computational cost. An-\nother line of research has found that increasing the\npretraining data does not always improve the per-\nformance on downstream tasks (Martin et al., 2019;\nDai et al., 2019; Shin et al., 2022). In response,\nnumerous studies have explored approaches such\nas utilizing pretraining corpora that are domain\nspecific or using data filtering to reduce the size\nof the pretraining corpus, while improving down-\nstream task performance (Beltagy et al., 2019; Lee\net al., 2020; Grave et al., 2018; Raffel et al., 2019;\nBrown et al., 2020). The shortcoming of these\nmethods is that the pretrained LM may be very\nspecific to the selected tasks, and therefore, show\nlimited generalizability to other downstream tasks,\nor require heuristic filtering techniques. In this re-\nsearch, we explore a complementary approach and\ninvestigate whether improving the complexity of\nthe pretraining corpus can yield improved model\nperformance. The implication is that rather than\narbitrarily increasing the size of a corpus as is done\ntoday, increasing its complexity might yield higher\nperformance but at a lower computational cost.\nIntuitively it is easy to compare a children’s book\nwith a college textbook and state that the latter is\nmore complex. Unfortunately, providing a general\nformal definition is fraught because books of dif-\nferent genres are complex in different ways (e.g.,\npost-modern novel vs. biography). However, at-\ntempts have been made to characterize text com-\nplexity using reasonable measures such as vocabu-\nlary size, syntactic complexity, and semantic rich-\nness (Jensen, 2009). In this paper we use metrics\nthat derive from these linguistic measures includ-\ning types, type-token ratio, entropy, and Flesch\nreading-ease to estimate corpus complexity.\nFirst we construct five distinct corpora of equal\nsize but varying complexityto pretrain LMs. The re-\nsulting models are then fine-tuned and evaluated on\ndownstream tasks from the GLUE benchmark. Our\nresults suggest that a corpus containing a breadth\nof complexity from easy to hard but one that is\nskewed towards hard makes an effective corpus as\nevaluated in general language understanding tasks.\nThe key contributions of our paper are as follows:\n(i) We propose a simple approach for constructing\na lexically rich and complex corpus for pretraining\nof language models; (ii) We conduct an extensive\nset of experiments by pretraining several language\nmodels from scratch on corpora of differing com-\nplexity, and then evaluating these models on a di-\nverse set of downstream tasks; (iii) We analyze our\nresults to estimate the correlation between the com-\nplexity of a corpus, its similarity to downstream\ndata, and its performance on various downstream\ntasks.\n257\n2 Related Work\nBelow, we briefly review two broadly related\nthreads of research.\nData selection. Ruder and Plank (2017) pro-\nposed several similarity and diversity measures for\nassessing the suitability of data for transfer learning.\nDai et al. (2019) studied the problem of selecting\nappropriate corpus for pretraining in the context\nof Named Entity Recognition (NER) downstream\ntasks, and found that language models pretrained\non source text similar to the target task outperform\nthe ones pretrained on other sources (with one ex-\nception). Gururangan et al. (2020) compared the\nvocabulary overlap between pretraining sources\nand target domain corpora, and found that the pre-\ntrained model performs slightly better when target\ndomain is less distant than source domain, but not\nin all the cases. Lange et al. (2021) studied the\nselection of source data for transfer learning.\nSelecting data from similar domains as down-\nstream tasks for pretraining of domain-specific\nlanguage models has generally been shown to be\nbeneficial, e.g., SciBERT (Beltagy et al., 2019),\nBioBERT (Lee et al., 2020). However, prior work\nhas also observed that this trend does not always\nhold true (Martin et al., 2019; Shin et al., 2022).\nDai et al. (2020) found that models pretrained on\nforums corpus (0.6B tokens) outperformed those\ntrained on tweets corpus (0.9B tokens) on both\nforums- and tweets-related downstream tasks, as\nwell as a significantly larger generic BERT model\n(3.3B tokens), highlighting the importance of do-\nmain similarity of corpus over its size.\nData engineering. A complementary line of\nresearch suggests that engineering the corpus be-\nfore pretraining through reordering (Agrawal et al.,\n2021; Nagatsuka et al., 2021; Li et al., 2021; Wang\net al., 2023), preprocessing (Babanejad et al., 2023),\nand filtering (Grave et al., 2018; Raffel et al., 2019;\nBrown et al., 2020; Rae et al., 2021; Kreutzer et al.,\n2022) can potentially enhance both the overall per-\nformance and efficiency of language models.\nDiverging from previous studies, our research fo-\ncuses on examining the influence of the complexity\nof a pretraining corpus on downstream tasks related\nto general language understanding. To accomplish\nthis, we introduce a straightforward methodology\nfor constructing a corpus that embodies richness\nand complexity.\n3 Method\nLet Cbe an unlabeled pretraining corpus of |C|to-\ntal tokens, consisting of a vocabulary set VC, i.e.,\nthe unique tokens or types in C. Similarly, let Dbe\na labeled downstream dataset with total number of\ntokens |D|and a vocabulary set VD. Given a fixed\ncorpus budget (e.g., number of tokens), we first\naim to construct distinct corpora of various com-\nplexity. Then, the goal is to measure the similarity\nbetween these corpora and downstream datasets,\nand estimate the correlation between complexity,\nsimilarity, and performance.\nWe present some metrics for assessing the com-\nplexity of a corpus and for computing the similarity\nbetween two collections of text – the pretraining\ncorpus and the downstream datasets in subsections\n3.1 and 3.2, before describing the procedure for cre-\nating corpora of varying complexity in subsection\n3.3.\n3.1 Corpus Complexity\nWe consider three metrics for estimating the com-\nplexity of a text corpus.\nTypes. This is the number of types or unique tokens\nin a corpus (i.e., its vocabulary).\nType-Token Ratio (TTR).Lexical complexity can\nalso be indexed via TTR – the higher the ratio, the\ngreater the lexical diversity in the sample (Johnson,\n1944). Although TTR is often sensitive to length\nof the texts, for analyzing corpora of comparable\nsizes, it can serve as a useful metric (Johansson,\n2008), and is computed as TTR(C) =|VC|\n|C| .\nEntropy. Broadly speaking, entropy is a mea-\nsure of randomness or disorder (Shannon, 1948;\nFano, 1961), and the greater the number of differ-\nent words in a text, the higher its entropy, or, con-\nceptually, its complexity. We calculate the unigram\nentropy of Cas follows:\nH(C) =−\n|VC|∑\ni=1\np(wi) log2 p(wi)\nwhere p(wi) is the probability of type wi in C.\n3.2 Text Similarity\nWe adopt two well-defined measures to estimate\nthe similarity between two pieces of text, such as\nthe pretraining corpus Cand a downstream dataset\nD.\n258\nVocabulary Overlap Ratio (VOR).This computes\nthe percentage of word types that appear in both\nthe texts (VC and VD) where a higher ratio indicates\nhigher similarity, and is calculated as:\nVOR(C,D) =|VC ∩VD|\n|VD| .\nJensen-Shannon divergence (JSD).This metric\nmeasures the distance between two texts (Lin,\n1991), and D(JS) is defined as:\nD(JS)(P||Q) =α1D(KL)(P||M)\n+ α2D(KL)(Q||M)\nwhere M = α1P+α2Q, and P and Qare the prob-\nability distributions of two texts (e.g., a pretraining\ncorpus Cand a downstream dataset D, in our case).\nThe values of α1 and α2 are set as 0.5 each. D(KL)\nis Kullback-Leibler divergence, a measure for com-\nparing the differences in two texts, and is defined\nas, D(KL)(P||Q) =∑\ni pi log pi\nqi\n.\n3.3 Constructing Corpora with Varying\nComplexity\nThe complexity of a corpus can be summarized by\nusing metrics like number of types, type-token ra-\ntio, and entropy (section 3.1). However, in order to\ncreate a corpus according to varying complexity we\nneed a more fine-grained metric that can compute\ncomplexity at document (or even paragraph) level.\nOne such metric is the Flesch reading ease (FRE)\nscore, commonly used to assess the difficulty of a\npiece of text (Flesch, 1948).\nFor a document di ∈C, its FRE score is com-\nputed as:\nFRE(di) = 206.835−1.015\n(#w\n#s\n)\n−84.6\n(#l\n#w\n)\nwhere #w, #s, and #l denote the number of\nwords, sentences, and syllables in di, respectively.\nThe word and sentence length serve as proxies for\nsemantic and syntactic complexity, respectively.\nNote that texts with high FRE scores tend to display\nlower complexity (e.g., children’s books), while an\neditorial in the New York Times which has a much\ngreater complexity, shows lower FRE scores. Thus,\nour approach for creating a more complex corpus is\nto combine pieces (paragraphs or documents) from\nexisting corpora based on their FRE score.\nOur method starts by adopting two text corpora\nwidely used for pretraining of language models:\nFigure 1: FRE distribution of the corpora. Lower FRE\nindicates higher complexity. wikibooks spans the\nfull spectrum of complexity, consisting of both low and\nhigh complexity, but mostly skewed towards the latter.\nwiki-103, a subset of English Wikipedia (Merity\net al., 2016) and BookCorpus, a large collection of\nbooks (Zhu et al., 2015). From these, we construct\nthe following five corpora:\n• wiki: This is the original wiki-103 corpus\nconsisting of around 100 million tokens.\n• books-small, books-easy,\nbooks-hard: Next, we create a\ncomparably-sized corpus of ∼100M to-\nkens, called books-small, by randomly\nsampling books from BookCorpus. Then, for\neach book in BookCorpus, we compute its\nFRE score and create two relevant baselines:\nbooks-easy by combining books of lowest\ncomplexity (i.e., the highest FRE scores), and\nconversely, books-hard by using books\nwith the highest complexity (i.e., the lowest\nFRE scores).\n• wikibooks: Finally, we hypothesize that a\ncomplex and diverse corpus contains a blend\nof texts with different levels of complexity,\nalbeit with a focus on more complex ones. We\nspeculate that this composition would allow\nit to capture the nuanced linguistic aspects\npresent in a wide range of texts. To create\nsuch a corpus, which we call wikibooks,\nwe first sample some articles from wiki-103\nand books from BookCorpus of varying com-\nplexity (i.e., FRE scores ranging from high\nto low), and then use up the remaining cor-\npus quota by sampling texts of mostly high\ncomplexity (low FRE scores).\nFigure 1 plots the FRE distribution of each of\nthe five corpora. As we can see, books-easy,\n259\nCorpus Tokens Types TTR (%) Entropy\nwiki 104M 267K 0.26 7.375\nbooks-easy 120M 258K 0.22 6.294\nbooks-hard 111M 417K 0.38 6.826\nbooks-small 116M 346K 0.29 6.483\nwikibooks 109M 436K 0.40 7.179\nTable 1: Characteristics of different pretraining corpora.\nbooks-hard, and books-small span a nar-\nrow range of complexity all skewing towards less\ncomplex; wiki has moderate to high complex-\nity; and wikibooks is the only one to show the\nbroadest range of complexity, with most of the\nmass concentrated in the highest complexity range,\nbut also some in the lowest complexity range.\n3.4 Downstream Datasets and\nImplementation\nWe use eight datasets from the General Language\nUnderstanding Evaluation (GLUE) benchmark in\nour experiments, which includes CoLA, MNLI,\nMRPC, QNLI, QQP, RTE, SST-2 and STS-B\n(Wang et al., 2018).\nText tokenization is done using NLTK 1, and\nFRE scores are computed using Readability pack-\nage2. Using the different corpora, we pretrain from\nscratch different versions of BERT-base model3\n(Devlin et al., 2019). The training continues for\nat most 30K steps. Checkpoints saved after 10K,\n20K, and 30K steps are then fine-tuned over the\ndownstream datasets for two epochs each.\n4 Discussion\nOur work investigates: (i) whether document-level\nmetric such as FRE can be used to construct cor-\npora of varying complexity, (ii) whether corpora of\nhigher complexity lead to improvements in down-\nstream performance, (iii) whether a complex cor-\npus is more similar to downstream data, and (iv)\nthe correlation between complexity, similarity, and\nperformance.\n1We use NLTK tokenizer: https://www.nltk.org/\napi/nltk.tokenize.html.\n2We use Readability package: https://pypi.org/\nproject/readability/ To account for the length-\nbased differences in Wikipedia articles and Books, we ran-\ndomly but sequentially select a subset of 1000 sentences for\neach book when computing its FRE.\n3We use the uncased version, with 12 transformer layers,\nbatch size set to 8, maximum length of the input sequence set\nto 512, and all other settings set as default. All pretraining and\nfine-tuning experiments are performed using HuggingFace\nlibrary (Wolf et al., 2019).\nFigure 2: Comparison of (unweighted) average GLUE\nscore, across five different pretraining corpora under\nvarying number of training steps (10K, 20K, 30K).\nWhether FRE can help create suitably complex\ncorpus. Table 1 summarizes the details of the five\ndistinct corpora, where we find that wikibooks,\nwhich contains a mix of low and high complex-\nity text, has the highest number of types and TTR,\nand second highest entropy. This demonstrates the\neffectiveness of using a computationally simple\nmetric such as FRE in creating corpora of a wide\nrange of complexity. Moreover, we also notice that\nthere is no corpus in our sample with a unigram\nentropy of less than six bits/word, which is in line\nwith information-theoretic models of communica-\ntion (Bentz et al., 2017).\nAnalyzing corpus complexity and downstream\nperformance. Figure 2 plots the average scores\nacross eight downstream tasks obtained using mod-\nels pretrained with the five different corpora un-\nder varying number of training steps. Three out\nof five corpora yield increasingly better results as\nthe training progresses, except books-easy and\nbooks-small which show the opposite trend.\nOn the one hand, this suggests that simply training\nfor longer time does not always guarantee a mono-\ntonically increasing performance score. On the\nother hand, this also indicates that pretraining on\nfairly less complex corpora (cf. Fig. 1) is generally\nless effective.\nIn connecting the results of Figure 2 with com-\nplexity metrics reported in Table 1, we observe\nthat wikibooks, a corpus with a comparatively\nhigher degree of complexity characterized by a\nlarger number of word types and a higher TTR,\nconsistently outperforms all other corpora across\nthe three model checkpoints. On the opposite end is\nthe poorest performing corpus books-easy with\nthe fewest types, lowest TTR, and lowest entropy.\nAnalyzing similarity between pretraining corpus\nand downstream datasets. Now, we assess the\nsimilarity between these corpora and downstream\n260\n(a) Similarity (VOR) between pretraining corpus and down-\nstream dataset (darker shades indicate higher similarity)\n(b) Correlation between similarity (VOR) and performance (pos-\nitive correlation is better)\nFigure 3: (top) Similarity (VOR) between pretraining\ncorpora and downstream datasets (train). (bottom) Pear-\nson’s correlation analysis (similarity and performance).\ndatasets to examine whether a more complex cor-\npus provides greater alignment with the down-\nstream data. Figure 3a shows that wikibooks\nis more similar to all the downstream datasets in\ncomparison to the other corpora, aligning with the\nintuition that a corpus with richer vocabulary subse-\nquently has increased similarity with downstream\ndata. As a further analysis, Figure 3b shows a mod-\nerate to high correlation between the similarity of\nthe corpus to downstream datasets and the corre-\nsponding performance across most datasets, which\nstrengthens as training progress. Similar trends\nhold for JSD (included in Appendix A). These find-\nings indicate that pretraining using a corpus that\nis similar to the downstream datasets is generally\nbeneficial, and VOR provides a computationally\nsimple way of estimating this similarity.\nAnalyzing complexity, similarity, and perfor-\nmance. Figure 4 presents Kendall’s Tau correlation\nanalysis for all three factors: complexity, similar-\nity, and performance. In looking at the last row in\nparticular (i.e., performance of the ‘30K’ model)\nwe observe that performance is strongly correlated\nwith VOR, which in turn is strongly correlated with\nFigure 4: Kendall’s Tau analysis comparing perfor-\nmance, complexity, and similarity. Darker shades indi-\ncate better correlation except for JSD, where a lighter\nshade (negative correlation) is desirable.\nmetrics of complexity (types, TTR, and entropy).\nTaken together, these results suggest that a more\ncomplex corpus leads to better downstream evalua-\ntion performance.\n5 Conclusions\nWe investigate whether pretraining on a corpus with\nhigher complexity subsequently yields improved\nperformance in downstream evaluations. Within\nthis study, we construct corpora of diverse complex-\nities by using straightforward metrics like Flesch\nreading ease, and estimate corpus-level complex-\nity using metrics such as unique word types, type-\ntoken ratio, or unigram entropy. The results of\nour extensive empirical analysis, which involves\ntraining language models from scratch using five\ndistinct corpora of varying text complexity and eval-\nuating their performance across eight downstream\ntasks, suggest a strong correlation between cor-\npus complexity, its similarity to downstream data,\nand the resulting performance on these tasks. One\ninteresting direction for future research involves\nexploring the findings of this study in the context\nof generative language models.\nLimitations\nOne limitation of our study is that, due to computa-\ntional constraints, we use what are now considered\nas relatively “small-sized” models and corpora, ex-\nclusively focusing on the English language and\ngeneric domains such as Wikipedia articles and\nbooks. The generalizability of our findings to larger\ncorpora, other languages, or specific domains such\nas medical texts warrants further investigation.\n261\nAcknowledgments\nWe thank the anonymous reviewers for their insight-\nful comments. This work was partially supported\nby NSF grants 2246174 and 1910655.\nReferences\nAmeeta Agrawal, Suresh Singh, Lauren Schneider, and\nMichael Samuels. 2021. On the role of corpus or-\ndering in language modeling. In Proceedings of the\nSecond Workshop on Simple and Efficient Natural\nLanguage Processing, pages 142–154.\nNastaran Babanejad, Heidar Davoudi, Ameeta Agrawal,\nAijun An, and Manos Papagelis. 2023. The role\nof preprocessing for word representation learning\nin affective tasks. IEEE Transactions on Affective\nComputing, pages 1–18.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientific text. arXiv\npreprint arXiv:1903.10676.\nChristian Bentz, Dimitrios Alikaniotis, Michael\nCysouw, and Ramon Ferrer-i Cancho. 2017. The en-\ntropy of words—learnability and expressivity across\nmore than 1000 languages. Entropy, 19(6):275.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, et al. 2021. Improving lan-\nguage models by retrieving from trillions of tokens.\narXiv preprint arXiv:2112.04426.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nXiang Dai, Sarvnaz Karimi, Ben Hachey, and Ce-\ncile Paris. 2019. Using similarity measures to\nselect pretraining data for NER. arXiv preprint\narXiv:1904.00585.\nXiang Dai, Sarvnaz Karimi, Ben Hachey, and Cecile\nParis. 2020. Cost-effective selection of pretraining\ndata: A case study of pretraining bert on social media.\narXiv preprint arXiv:2010.01150.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRobert M Fano. 1961. Transmission of information:\nA statistical theory of communications. American\nJournal of Physics, 29(11):793–794.\nRudolph Flesch. 1948. A new readability yardstick.\nJournal of applied psychology, 32(3):221.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learn-\ning word vectors for 157 languages. arXiv preprint\narXiv:1802.06893.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nadapt language models to domains and tasks. arXiv\npreprint arXiv:2004.10964.\nKristian TH Jensen. 2009. Indicators of text complexity.\nMees, IM; F . Alves & S. Göpferich (eds.), pages 61–\n80.\nVictoria Johansson. 2008. Lexical diversity and lexi-\ncal density in speech and writing: A developmental\nperspective. Working papers/Lund University, De-\npartment of Linguistics and Phonetics, 53:61–79.\nWendell Johnson. 1944. Studies in language behavior:\nA program of research. Psychological Monographs,\n56(2):1–15.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. arXiv preprint arXiv:1911.00172.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab,\nDaan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera\nTapo, Nishant Subramani, Artem Sokolov, Claytone\nSikasote, et al. 2022. Quality at a glance: An audit of\nweb-crawled multilingual datasets. Transactions of\nthe Association for Computational Linguistics, 10:50–\n72.\nLukas Lange, Jannik Strötgen, Heike Adel, and Dietrich\nKlakow. 2021. To share or not to share: Predicting\nsets of sources for model transfer learning. arXiv\npreprint arXiv:2104.08078.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nConglong Li, Minjia Zhang, and Yuxiong He. 2021.\nCurriculum learning: A regularization method for ef-\nficient and stable billion-scale gpt model pre-training.\narXiv preprint arXiv:2108.06084.\n262\nJianhua Lin. 1991. Divergence measures based on the\nshannon entropy. IEEE Transactions on Information\ntheory, 37(1):145–151.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSuárez, Yoann Dupont, Laurent Romary, Éric Ville-\nmonte de La Clergerie, Djamé Seddah, and Benoît\nSagot. 2019. Camembert: a tasty french language\nmodel. arXiv preprint arXiv:1911.03894.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nKoichi Nagatsuka, Clifford Broni-Bediako, and\nMasayasu Atsumi. 2021. Pre-training a bert with\ncurriculum learning by increasing block-size of input\ntext. In Proceedings of the International Conference\non Recent Advances in Natural Language Processing\n(RANLP 2021), pages 989–996.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nSebastian Ruder and Barbara Plank. 2017. Learning to\nselect data for transfer learning with Bayesian opti-\nmization. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 372–382, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nClaude Elwood Shannon. 1948. A mathematical theory\nof communication. The Bell system technical journal,\n27(3):379–423.\nSeongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong\nKim, HyoungSeok Kim, Boseop Kim, Kyunghyun\nCho, Gichang Lee, Woomyoung Park, Jung-Woo Ha,\net al. 2022. On the effect of pretraining corpora on\nin-context learning by a large-scale language model.\narXiv preprint arXiv:2204.13509.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nYile Wang, Yue Zhang, Peng Li, and Yang Liu. 2023.\nLanguage model pre-training with linguistically mo-\ntivated curriculum learning.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE in-\nternational conference on computer vision , pages\n19–27.\nA Similarity Analysis\nFigure 5 presents the results of similarity analysis\nand Pearson’s correlation analysis using Jensen-\nShannon divergence.\n(a) JSD (lighter shades indicate higher similarity)\n(b) JSD (negative correlation is better)\nFigure 5: (top) Similarity between pretraining corpora\nand downstream datasets (train set) using JSD. The last\ncolumn ‘average’ presents the average results of all\nthe datasets. ( bottom) Pearson’s correlation analysis\nbetween JSD and performance at 10K, 20K, and 30K\nstep checkpoints.\n263",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.734656572341919
    },
    {
      "name": "Natural language processing",
      "score": 0.5844096541404724
    },
    {
      "name": "Language model",
      "score": 0.5222150087356567
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5094531178474426
    },
    {
      "name": "Linguistics",
      "score": 0.35213127732276917
    },
    {
      "name": "Philosophy",
      "score": 0.0717284083366394
    }
  ],
  "institutions": []
}