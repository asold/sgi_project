{
  "title": "Language Models are Few-shot Multilingual Learners",
  "url": "https://openalex.org/W3199377785",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4223199573",
      "name": "Winata, Genta Indra",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A4222272762",
      "name": "Madotto, Andrea",
      "affiliations": [
        null,
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A4202063432",
      "name": "Lin, Zhaojiang",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A4209981129",
      "name": "Liu, Rosanne",
      "affiliations": [
        null,
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4282320216",
      "name": "Yosinski, Jason",
      "affiliations": [
        null
      ]
    },
    {
      "id": null,
      "name": "Fung, Pascale Ngan",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3170632963",
    "https://openalex.org/W3023911605",
    "https://openalex.org/W2971167298",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3106255016",
    "https://openalex.org/W3047738520",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W3162296828",
    "https://openalex.org/W3105096579",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3156414406",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W3021016503",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3032532958",
    "https://openalex.org/W3094467593",
    "https://openalex.org/W3100128199",
    "https://openalex.org/W3015564377",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2997214274",
    "https://openalex.org/W3012291345",
    "https://openalex.org/W3153266325",
    "https://openalex.org/W3049346316",
    "https://openalex.org/W3086966320",
    "https://openalex.org/W3109959371",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2898856000",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3165910725",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3156064004",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W3132736064",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W3186578552",
    "https://openalex.org/W2950733326",
    "https://openalex.org/W2989143494",
    "https://openalex.org/W3098637735",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3110524561",
    "https://openalex.org/W3099453223",
    "https://openalex.org/W2971160427",
    "https://openalex.org/W3168491067",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2803609229",
    "https://openalex.org/W3100532709",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3214298066",
    "https://openalex.org/W3154046873",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3118580427",
    "https://openalex.org/W3185293939",
    "https://openalex.org/W3126960149",
    "https://openalex.org/W2996035354"
  ],
  "abstract": "General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models.",
  "full_text": "Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 1–15\nNovember 11, 2021. ©2021 Association for Computational Linguistics\n1\nLanguage Models are Few-shot Multilingual Learners\nGenta Indra Winata1∗, Andrea Madotto1,3∗, Zhaojiang Lin1,\nRosanne Liu2,3, Jason Yosinski3, Pascale Fung1\n1The Hong Kong University of Science and Technology\n2Google Brain 3ML Collective\n{giwinata, amadotto, zlinao}@connect.ust.hk\nAbstract\nGeneral-purpose language models have\ndemonstrated impressive capabilities, perform-\ning on par with state-of-the-art approaches\non a range of downstream natural language\nprocessing (NLP) tasks and benchmarks when\ninferring instructions from very few examples.\nHere, we evaluate the multilingual skills of the\nGPT and T5 models in conducting multi-class\nclassiﬁcation on non-English languages\nwithout any parameter updates. We show\nthat, given a few English examples as context,\npre-trained language models can predict not\nonly English test samples but also non-English\nones. Finally, we ﬁnd the in-context few-shot\ncross-lingual prediction results of language\nmodels are signiﬁcantly better than random\nprediction, and they are competitive compared\nto the existing state-of-the-art cross-lingual\nmodels and translation models.\n1 Introduction\nThe progress in language model (LM) pre-\ntraining (Peters et al., 2018; Devlin et al., 2019;\nRadford et al., 2019; Yang et al., 2019; Liu et al.,\n2019a; Brown et al., 2020; Liu et al., 2020a; Lewis\net al., 2020; Raffel et al., 2020; Gao et al., 2020a)\nhas led to the possibility of conducting few-shot\nlearning, that is, learning a new task using a small\nnumber of examples without any further training\nor gradient computation. Few-shot learning alle-\nviates the cost for extensive labeled data, which\nis beneﬁcial since collecting high-quality labeled\ndata is resource-intensive and expensive. It also\nreduces the cost for model ﬁne-tuning, which re-\nquires tremendous GPU or TPU resources. Few-\nshot learning can be seen as a one-for-all plug-\nand-play computational model that can be applied\nto various natural language tasks, from sentiment\nanalysis for text classiﬁcation to story generation,\nprovided only a small context (Brown et al., 2020).\n∗ Equal contribution\nFigure 1: Accuracy vs. model size on English-Spanish\nMNLU dataset. Cross-lingual in-context learning with\nLMs (i.e., context with few English examples tested on\nSpanish sentences) performs as well as models trained\nin cross-lingual setting (Liu et al., 2020b) and transla-\ntion baselines.\nThe idea of few-shot learning is also relevant\nto address the low-resource issue in non-English\nlanguages. Few-shot learning has been applied\nto NLP tasks (Brown et al., 2020; Madotto et al.,\n2020b; Lu et al., 2021; Perez et al., 2021; Liu et al.,\n2021a,b; Cahyawijaya et al., 2021a). Common ap-\nproaches to solve the low-resource issue are to pre-\ntrain models with self-supervised learning using un-\nlabelled monolingual text data collected from vari-\nous resources available online (Wilie et al., 2020;\nLe et al., 2020; Martin et al., 2020; Eddine et al.,\n2020; Nguyen and Nguyen, 2020; Scheible et al.,\n2020; Bhattacharjee et al., 2021; Lee et al., 2020;\nCahyawijaya et al., 2021b; Park et al., 2021) and\nthen apply pre-training on the source language and\nﬁne-tune on the target languages (Schuster et al.,\n2019; Lin et al., 2019; Winata et al., 2019, 2021;\nPfeiffer et al., 2020; Zheng et al., 2021; Lin et al.,\n2021b). Conversely, the few-shot learning does\nnot need any training from the source and target\nlanguages. Figure 1 shows how it is possible to uti-\nlize pre-trained models on non-English languages,\nsuch as Spanish, as the performance is not random,\n2\nFigure 2: Example of the inference and query generation on the few-shot learning, where the source language and\ntarget language are German and English, respectively.\nand the performance increases as the models are\ngiven more samples. We conjecture that pre-trained\nmodels may be able to adapt to languages that are\nsimilar to English. However, for many language\ntasks, it is difﬁcult to collect a large supervised\ntraining dataset as language experts (e.g., linguists\nor native speakers) are required to annotate the\ndata.\nAnother line of work is to apply cross-lingual\ntransfer on English with the same task as the target\nlanguages (Ponti et al., 2018; Artetxe and Schwenk,\n2019; Liu et al., 2019b; Lauscher et al., 2020; Liu\net al., 2020b, 2021c; Chen et al., 2021). How-\never, such methods still need to apply a ﬁne-tuning\nstep to update the model for fast adaptation, which\ncan be challenging for large pre-trained models –\nsome models require substantial memory capac-\nity – since the models have to be trained on high-\nperforming machines. Different from the afore-\nmentioned method, in-context learning using an\nLM does not allow any parameter updates. Thus,\nthe process does not need to compute and store the\ngradients for backward propagation.\nIn this work, we investigate the practicality of\napplying few-shot learning in the multilingual set-\nting for four languages, English, French, German,\nand Spanish, on natural language understanding in-\ntent prediction tasks using publicly available LMs\nthat are mainly trained on English data. We show\nthat, given a few English examples as context, pre-\ntrained LMs can predict not only English test sam-\nples, but also non-English ones (Figure 2). To\nthe best of our knowledge, no existing works have\nstudied these tasks in multilingual settings. We\nconjecture that the English LMs can still produce\ngood results on languages that are closely related\nto English. We construct the inference for the\nmulti-class prediction setup by extending the idea\nfrom Madotto et al. (2020b) of applying multiple\nbinary predictions on each class. Instead of guid-\ning the model to generate true or false like in their\nwork, which is not consistent and sometimes gen-\nerates other words –, we introduce maximum conﬁ-\ndence prediction. This method considers the con-\nﬁdence of predicting a certain label to provide a\nprediction. We design this as a multiple-choice task\nin which the conﬁdence of the prediction for all pos-\nsible classes is compared. Each class’s conﬁdence\nscore is computed by normalizing the logits of gen-\nerating the next boolean token given the prompt as\nthe context. This method is considered to be more\nscalable than the simple k-way few-shot learning,\nwhere we need to put all data in a single prompt,\nsince we only have a ﬁxed maximum sequence\nlength and, in the deployment, each forward step\ncan be run in parallel to speed up the process. To\nincrease the difﬁculty of the challenge, we also pro-\npose a cross-lingual task, where the context and\nquery are in different languages.\nOverall, we ﬁnd that conditional generative LMs,\nsuch as the GPT-2 (Radford et al., 2019), GPTNEO\nmodels (Gao et al., 2020a), and T5 models (Raffel\n3\net al., 2020) have the capability to predict non-\nEnglish languages, and adding more shots and us-\ning larger models achieves a substantial increment\nin performance, making it signiﬁcantly better than\nrandom, which indicates the models are able to un-\nderstand the prompt. We only focus on GPT and T5\nmodels. T5 models do not perform as well as GPT\nmodels, which might be caused by the pre-training\nstrategy. Experimental results in the cross-lingual\nsetting demonstrate that pre-trained LMs make cor-\nrect predictions. To summarize, our contributions\nare as follows:\n• We study few-shot learning in the multilingual\nsetting on four languages without any gradi-\nent updates. We use the publicly available\nGPT and T5 LMs, and compare the results\nto those from the zero-shot and ﬁne-tuning\napproaches.\n• We propose a simple and straightforward ap-\nproach to perform few-shot learning on multi-\nclass classiﬁcation by applying binary predic-\ntion and considering the conﬁdence of predict-\ning the boolean tokens.\n• We display the zero-shot, one-shot, and many-\nshot proﬁciency of the LMs in the cross-\nlingual setting when the language of the\nprompt is different from the target language.\n2 Few-shot Multilingual Learners\nFirst, we brieﬂy deﬁne the notation of the input\nand output of the task, and then we introduce our\nmethod to design prompts for few-shot in-context\nlearning. 1\n2.1 Notation and Tasks\nLet us deﬁne Das the distribution over the dataset\nand P as the prompt that we use as the input of\nthe LM θ. The prompt P = [Dpos,Dneg,Q] is a\nconcatenation of few-shot samples: positive sam-\nples Dpos, negative samples Dneg, and the query\nQ, where Dpos, Dneg ∼D. Dpos is a sample with\na label that is the same as the query, and Dneg is a\nsample that is taken from the datasetDwith a label\nother than the query. θtakes P as the input of the\nmodel, and the LM generates a word y. We deﬁne\nthe task Ts→t, where sis the source language and\ntis the target language.\n1The code is released at https://github.com/\ngentaiscool/few-shot-lm.\nIn this paper, we focus on the intent detection\ntask in the monolingual and cross-lingual settings.\nIn the monolingual setting, the source language is\nthe same as the target language, and in the cross-\nlingual setting, we take the source language as dif-\nferent from the target language (s̸= t). We design\nour task as a multiple-choice problem, in which\neach sample has a label l ∈L, where Lis the set\nof possible labels. We predict the boolean (true or\nfalse) for each sample and take the highest predic-\ntion conﬁdence.\n2.2 Prompt Generation\nWe deﬁne the task by designing prompts to perform\nfew-shot learning. We design our task as a binary\nclassiﬁcation for multi-class prediction by follow-\ning Madotto et al. (2020b). The idea is to guide\nthe model to predict the boolean tokens, true and\nfalse. We examine the usage of two types of LMs,\nGPT and T5 models, and we construct prompts\nspeciﬁc to each model. We use a speciﬁc way to\nprobe the LMs to perform the few-shot prediction\nsince they are trained with different learning objec-\ntives. Table 1 shows the format of the preﬁx we\nuse for the GPT and T5 models. Xi is one of the\nModel Prompt\nGPT [SAMPLES]Q→\nT5 [SAMPLES]Q→[MASK]\n[SAMPLES]\nFormat Example\nX1→true\\nzeige mir meine wecker=>get_alarm=true\\n\nX∗1 →false\\nentferne alle wecker=>get_alarm=false\\n\n··· ···\nXk→true\\nkann ich meine wecker sehen?=>get_alarm=true\\n\nX∗k→false\\nkeinen sound bitte=>get_alarm=false\\n\nTable 1: Prompt format given a few German examples\nas context.\nfew-shot samples, and X∗\ni is the sample from other\nclasses. For the GPT models, we only input the\npreﬁx by concatenating positive and negative sam-\nples with the query. Speciﬁcally for the T5 models,\nwe add an additional token after the query and let\nthe model predict that particular token during the\ngeneration step.\nFigure 2 shows an example of how we generate\nthe prompt in k-shot settings. We create Lprompts\nand apply Lforward steps for each sample. For\neach prompt, kpositive and negative samples are\nrandomly drawn from the dataset. It is worthwhile\nto note that the sampling method is similar tok-way\nfew-shot learning, but the samples are not merged\n4\ninto a single prompt. We do this because we want\nto give more shots as the prompt to the LMs as\nthey have a limitation on the number of tokens they\ncan accept as input (1,024 tokens in GPT-2XL and\n2,048 tokens in GPTNEO). We add a special token\n\\n as a separator between each sample, as shown\nin Table 1.\n2.3 Maximum Conﬁdence Prediction\nTo get the ﬁnal prediction of each sample, ﬁrst, we\ncompute the score of predicting the next boolean\n(true or false 2) given the prompt Xi for label i:\nPθ(y = true|Xi) and Pθ(y = false|Xi) from\nthe prediction distribution. Then, we normalize the\nscore to get the probability of generating the true\ntoken to measure how much conﬁdence the LM\nhas to predict label i. We collect all the conﬁdence\nscores over all label options and choose the highest\nconﬁdence score among them, as follows:\nMC(X,L) = argmax\ni∈L\nPθ(y= true|Xi)∑\nbPθ(y= b|Xi) , (1)\nwhere b ∈ {true,false}. We take the label\nwith the highest conﬁdence score as MC(X,L).\n2.4 Choices of Samples\nFor in-context learning, choosing the order of sam-\nples is essential (Lu et al., 2021). Here, we examine\nthe impact of the order of the samples. We con-\nstruct the probing set in two ways: (1) shufﬂe the\nfew-shot samples and measure the variance in per-\nformance after changing their order, and (2) arrange\nthe positive samples before the negative samples.\nWe ﬁnd that the latter works well, speciﬁcally on\nthe T5 models.\n3 Baselines\nIn this work, we compare the few-shot learning\nperformance with other common approaches: zero-\nshot, zero-shot cross-task, and ﬁne-tuning.\n3.1 Zero-shot Cross-Task\nOne way to solve zero-shot prediction is by us-\ning entailment models to calculate the entailment\nscore between sequences and labels. Given a pre-\ntrained LM ψ with an entailment head, a set of\nhypotheses H, and possible labels L, the model\n2Notice that some tokenizers (e.g., T5) splits \"true\" in two\nsub-tokens. We compute the score of the ﬁrst sub-token only\nsince it is signiﬁcantly different for the two label (i.e. \"tr\" and\n\"fal\").\naccepts two inputs, the hypothesis h ∈ H and\nlabel l ∈L, and generates the entailment score\ngiven any combinations of the hypothesis and label\nPψ(y= entail|h,l):\nES(H,L) = argmax\nh,l∈{H,L}\nPψ(y= entail|h,l). (2)\n3.2 Zero-shot In-Context Learning\nThis approach is very similar to our few-shot ap-\nproach. It does not need any samples, and the\nmodel is only given natural language instruction.\nHowever, instead of using the prompt like in the\nfew-shot setting, we can set up the prompt in a\nquestion-and-answer (Q&A) format as follows:\nQ: Is ‘<INTENT>’ the intent of ‘<TEXT>’? A:. (3)\n3.3 Fine-tuning\nFine-tuning is the most common approach to up-\ndating a pre-trained model’s weights when training\nwith a labeled dataset. The advantage of this ap-\nproach is strong performance since we give super-\nvised signals with the correct labels to the model.\nFor ﬁne-tuning, we use the same sets of few-shot\nsamples as in the in-context learning. In Sec-\ntion 4.2, we provide the hyper-parameters used\nin the experiments.\n4 Experiments\n4.1 Datasets and Metrics\nWe use an English natural language understanding\n(NLU) dataset, SNIPS (Coucke et al., 2018), and\ntwo multilingual NLU datasets, MTOP (Li et al.,\n2021) and Multilingual NLU (MultiNLU) (Schus-\nter et al., 2019). MTOP includes four languages,\nEnglish ( en), French ( fr), German ( de), and\nSpanish (es), and Multilingual NLU includes two\nlanguages, English ( en) and Spanish ( es). We\nmeasure the model performance by calculating the\naverage and standard deviation of the accuracy with\nthree runs.\n4.2 Experiment Settings\nWe set up the experiment in two settings: mono-\nlingual and cross-lingual. In the monolingual set-\nting, we test the ability of the model to conduct\nfew-shot in-context learning on four languages: En-\nglish (en), French (fr), German (de), and Spanish\n(es). In the cross-lingual setting, we test its abil-\nity to predict a query from a non-English language\n5\nModels SNIPS MTOP MultiNLU\nen de en es fr en es\nRandom 14.29 15.07 15.25 15.55 14.36 8.33 8.33\nFull-training SOTA 99.00‡ 88.80† 94.00† 90.10† 89.60† 99.11∗ 98.90∗\nZero-shot Cross-Task Prediction\nBARTLARGE0.4B 74.43 24.80 43.41 36.06 24.77 65.60 34.77\nXLM-RLARGE0.6B 68.00 54.30 53.37 51.67 51.99 77.79 66.35\nFew-shot Learning (K-shot)\nGPT-20.1B 39.33±8.58 40.03±6.34 35.46±0.92 36.18±2.12 41.16±5.65 51.59±12.83 37.56±7.14\nGPT-2MEDIUM0.3B 65.71±2.80 52.94±5.12 63.35±3.01 54.33±4.75 50.6±2.44 72.21±14.88 50.25±4.99\nGPT-2LARGE0.8B 71.43±10.27 50.94±6.63 59.70±4.50 52.38±2.65 44.75±1.11 62.36±13.82 58.04±5.28\nGPT-2XL1.6B 78.43±3.16 78.43±3.16 73.93±1.21 56.61±2.02 45.21±2.54 79.04±5.05 64.74±7.64\nGPTNEO1.3B 84.19±2.78 67.17±2.50 82.40±1.90 73.51±0.95 66.3±1.29 89.70±1.28 85.77±2.53\nGPTNEO2.7B 91.24±0.68 71.57±5.94 81.51±0.39 76.94±0.83 70.31±1.99 83.76±3.14 87.82±1.55\nGPTNEO-J6B 93.38±0.76 80.97±3.21 89.66±0.50 84.18±0.32 85.04±1.18 94.32±1.14 88.54±6.18\nT5LARGE0.8B 23.57±8.93 41.84±7.63 36.02±5.26 49.49±6.32 40.41±5.97 37.57±15.23 21.20±6.51\nT53B3B 46.52±6.69 50.81±6.45 46.17±4.06 46.45±4.39 44.38±0.22 31.46±18.18 31.60±14.90\nGPTNEO2.7B(ordered)86.71±1.62 55.69±3.45 55.12±4.01 50.77±4.41 50.70±2.47 63.33±7.14 61.51±1.63\nT5LARGE0.8B(ordered)25.90±18.51 63.06±4.56 51.92±3.90 62.71±6.30 55.91±3.82 38.97±14.80 63.10±4.46\nT53B3B(ordered) 93.00±3.00 74.11±2.69 65.03±1.87 66.97±1.35 68.89±2.51 80.12±3.95 86.60±2.40\nFine-tuning (40-shot)\nmBERT0.2B 88.57±3.14 25.21±2.31 41.44±5.59 33.82±10.08 16.54±5.54 84.88±1.59 87.87±3.29\nXLM-RBASE0.3B 87.95±1.39 27.47±11.90 37.03±5.11 27.16±5.51 13.8±6.50 77.06±3.16 74.85±1.53\nTable 2: Zero-shot and few-shot results in the monolingual setting. The SOTA results are taken from †Li et al.\n(2021), ‡Qin et al. (2019), and ∗Schuster et al. (2019).\nwith the English context (en→XX). In the few-shot\nin-context learning, we use k-way-few-shot clas-\nsiﬁcation, taking ksamples. For each model, we\ntake k ∈[0,5,K], where K ≤40 is the largest\nnumber of few-shot samples that can be passed to\nthe model as input and is divisible by 10 without\nexceeding the maximum input token limit. We uti-\nlize an NVIDIA Tesla V100 16GB GPU to run the\ninference so that the model is ensured to ﬁt in a\nsingle GPU, and we use 16-bit precision.\nModel details We run experiments on a variety\nof publicly available models:3 four sizes of GPT-2\nmodels (0.1B, 0.3B, 0.8B and 1.6B), three sizes\nof GPTNEO models (1.3B, 2.7B, and 6B), and two\nsizes of T5 models (0.8B and 3B). Table 3 shows\nthe details of each pre-trained model.\nBaselines We use the same sets of few-shot sam-\nples for the baselines. We run ﬁne-tuning on the\npre-trained models mBERT (Devlin et al., 2019)\nand XLM-R (Conneau et al., 2020), and also com-\npare our models with the zero-shot cross-task mod-\nels using pre-trained models XLM-R, ﬁne-tuned\non XNLI (Conneau et al., 2018), and BART, ﬁne-\n3The models except GPT NEO-J are taken from\nhttps://huggingface.co/. The GPT NEO-J model\nis taken from https://github.com/kingoflolz/\nmesh-transformer-jax/\ntuned on MNLI (Williams et al., 2018);4 a random\nbaseline; and state-of-the-art results reported on\neach dataset. For the ﬁnetuning, we use a learning\nrate of 5e-5 with a decay of 0.9 for every epoch,\nand a batch size of 32. We apply an early stopping\nafter 5 epochs without any improvement on the\nvalidation set.\nModel Name n params nlayers nhidden nffn\nGPT-2 0.1B 12 768\nGPT-2MEDIUM 0.3B 24 768 -\nGPT-2LARGE 0.8B 36 1,280 -\nGPT-2XL 1.6B 48 1,600 -\nGPTNEO 1.3B 24 2,048 -\nGPTNEO 2.7B 32 2,560 -\nGPTNEO-J 6B 28 4096 16,384\nT5LARGE 0.8B 24 1,024 4,096\nT53B 3B 24 1,024 16,384\nTable 3: Model architecture.\n4The XLM-R model ﬁne-tuned with XNLI data can\nbe accessed at https://huggingface.co/joeddav/\nxlm-roberta-large-xnli . The BART model ﬁne-\ntuned with MNLI data can be accessed at https://\nhuggingface.co/facebook/bart-large-mnli\n6\nModels MTOP MultiNLU\nen→de en →es en →fr en →es\nFine-tuning (all-shot on source language, zero-shot on target language)\nSeq2Seq w/ CRISS (Li et al., 2021) 36.10 48.60 46.60 -\nSeq2Seq w/ XLM-R (Li et al., 2021) 42.30 50.30 43.90 -\nNLM (Liu et al., 2021d) 54.91 59.99 58.16 -\nX2Parser (Liu et al., 2021d) 56.16 60.30 58.34 -\nMulti CoVe (Schuster et al., 2019) - - - 53.89\nTranslate-Train (Liu et al., 2020b) - - - 85.39\nMTL (Liu et al., 2020b) - - - 87.88\nFew-shot Learning (K-shot)\nGPT-20.1B 23.89 ±1.52 27.10 ±3.19 26.14 ±0.54 38.60 ±3.54\nGPT-2MEDIUM 0.3B 39.61 ±5.42 41.81 ±4.66 42.40 ±3.84 40.40 ±10.48\nGPT-2LARGE 0.8B 30.94 ±4.45 34.69 ±6.50 33.04 ±4.56 23.99 ±14.02\nGPT-2XL 1.6B 42.88 ±4.94 48.43 ±4.42 50.67 ±4.50 51.31 ±9.87\nGPTNEO 1.3B 56.14 ±2.75 63.14 ±2.52 60.25 ±3.32 64.82 ±5.94\nGPTNEO 2.7B 58.27 ±1.28 64.79 ±1.69 62.30 ±1.60 65.91 ±6.42\nGPTNEO-J 6B 79.41 ±1.18 81.57 ±0.83 77.85 ±1.63 82.66 ±4.19\nT5LARGE 0.8B 37.14 ±5.44 38.14 ±3.20 33.53 ±4.85 14.95 ±16.34\nT53B 3B 35.35 ±7.07 34.64 ±6.21 37.26 ±8.68 14.11 ±14.01\nGPTNEO 2.7B(ordered)0.8B 42.23 ±3.24 48.62 ±2.60 46.30 ±3.02 47.83 ±5.73\nT53B (ordered)3B 52.23 ±4.29 52.74 ±3.20 49.72 ±5.37 50.42 ±6.01\nTable 4: Few-shot results in the cross-lingual setting on MTOP and MultiNLU datasets.\n5 Results and Analysis\n5.1 Model Performance\nTables 2 and 4 show the results in the monolingual\nand cross-lingual settings, respectively. The tables\nshow that the performance improvement is highly\nrelated to the size of the pre-trained model, and the\nperformance gap between the fully trained state-\nof-the-art model and the few-shot learning models\ndecreases when we use larger models, indicating\nthe usefulness of utilizing models of bigger sizes.\nThe performance of the models with few-shot learn-\ning is considered promising as they are not trained\nat all, and the best model’s performance gap with\nthe ﬁne-tuned model is less than 10%.\nFew-shot vs. Fine-tuning. Comparing the per-\nformance of generative models to ﬁne-tuning, it\nis clear that we can achieve higher accuracy with-\nout any training. However, in this experiment, we\nacknowledge GPT and T5 models we use for in-\ncontext learning are larger than the models we ﬁne-\ntune, and few-shot learning is much more efﬁcient\nsince the models are not required to store the in-\ntermediate memory. In terms of inference speed,\nthe few-shot models require more time to run an\ninference step, which may cause a bottleneck when\nthe number of few-shot samples is relatively large.\nThis is the limitation of this method, and reduc-\ning the inference time is an open research area to\nimprove the efﬁciency of in-context learning.\nZero-shot cross-task baselines. Surprisingly,\nthe zero-shot cross-task models are able to pre-\ndict the samples much better than the random\nbaseline, particularly on English tasks. Overall,\nthe XLM-RLARGE model performs better than the\nBARTLARGE models in all tasks except SNIPS.\nGPT vs. T5 models. In general, the GPT mod-\nels outperform the T5 models in all language pairs\nand datasets in a head-to-head comparison: Both\nGPT-2LARGE and T5LARGE have a similar number\nof parameters (0.8B), but they have a signiﬁcant\nperformance difference. A similar pattern can also\nbe observed on larger models, such as GPT NEO\n2.7B and T53B 3B. Although the T5 models per-\nform worse than the GPT models, they do not have\na maximum token size for the input, as the GPT\nmodels do, which is one of the advantages of using\n7\nthem. On the other hand, we ﬁnd that changing the\nsample order tremendously affects the performance\nof the T5 models. As shown in Tables 2 and 4, the\nperformance increases substantially when we sort\nthe few-shot samples based on their label (i.e., ﬁrst\nall positive and then all negative examples). Con-\nversely, the GPT models suffer loss in performance.\nThus, we can make the conclusion that changing\nthe sample order may produce high variance in the\nresults, as also shown in (Lu et al., 2021).\nEffectiveness on non-English languages.\nBased on the results, the performance of the\nmodels is lower in the non-English languages than\nin English. These results are expected since the\npre-trained models are mostly trained on English\ndata. However, the differences in performance\nare marginal. This ﬁnding may indicate that\nour few-shot learning method can be effectively\nutilized for languages that are in the same language\nfamily as English, such as French, German, and\nSpanish, but this will require further investigation\nin the future.\nCross-lingual results. Based on the results in Ta-\nble 4, we can see that the generative models are\nable to use the context from English to predict\nthe sample in non-English languages. The cross-\nlingual setting is considered harder than the mono-\nlingual one since the models need to contextualize\nand understand the source and target languages to\npredict the test samples correctly. In general, the\ntrend of the results in the cross-lingual setting is\nsimilar to the monolingual setting. In the MTOP\ndataset, we ﬁnd that the models generally achieve\nhigher performance for en→es than for the other\ntwo target languages ( de and fr). In MultiNLU,\nour GPTNEO-J closes the gap with the existing state-\nof-the-art baseline with ﬁne-tuning from Liu et al.\n(2020b) underperforming it only by a close margin\nof around 4.2%, and the GPTNEO-J performance is\nonly less than 3% worse than that of the Translate-\nTrain model. These results show a promising new\ndirection in the zero-shot cross-lingual research\nthat can be applied to other datasets and language\npairs.\n5.2 Ablation Study\nTo further understand how much data we need for\nthe in-context learning, we conduct experiments\nwith different numbers of few-shot samples, in-\ncluding zero-shot experiments on the MTOP and\nMultiNLU datasets.\nMTOP dataset. Figures 3, 4, 5, and 6 illustrate\nthe results with different numbers of samples on\nthe MTOP dataset in the monolingual setting. We\nshow a different set of k-shot results for each model\naccording to the maximum samples that can be\nused in the model as input. The results consistently\nimproved as the number of shots increases. In-\nterestingly, the QA style’s zero-shot strategy can\noutperform random prediction only on two or three\nmodels in each language, and the others are worse.\nThe ﬁne-tuning results on MTOP are thus far worse\nthan those of few-shot learning.\nMultiNLU dataset. Figures 7 and 8 illustrate\nthe results with different numbers of samples on\nthe MultiNLU dataset in the monolingual setting.\nThe results on MultiNLU for the models with ﬁne-\ntuning are closer to those of few-shot learning than\nthose on the MTOP dataset. The reason may be the\nnumber of labels that the MTOP dataset has com-\npared to MultiNLU. As a result, the zero-shot per-\nformance on the GPT models is sometimes worse\nthan that of the random baseline.\n6 Related Work\n6.1 Few-shot In-Context Learning\nRecent work on few-shot in-context learning uses\nLMs to solve NLP tasks (Petroni et al., 2019;\nBrown et al., 2020; Gao et al., 2020b; Madotto\net al., 2020b; Zhao et al., 2021; Schick and Schütze,\n2021; Lin et al., 2021a). In this approach, we select\nthe appropriate prompts to trigger the LMs to be-\nhave so that they can predict the desired output (Liu\net al., 2021b). However, the prompts have to be\nengineered to allow the LM to generate a text ap-\npropriate to solve the task. Learning to calibrate\nthe few-shot results is also essential to reduce the\nmodel’s performance variance (Zhao et al., 2021),\nand the selection criteria in choosing the prompts\nare also important (Perez et al., 2021). In another\nstream of work, Shin et al. (2020); Li and Liang\n(2021) proposed an automated method to create\nprompts for a diverse set of tasks by gradient-based\ntuning instead of manually searching for a good\nprompt. Using such a method, may allow us to\nﬁnd an optimal prompt easier, it is very difﬁcult\nto discover the optimal prompts for complicated\nnatural language processing tasks, such as semantic\nparsing (Liu et al., 2021b).\n8\nFigure 3: The results on German (de) MTOP dataset\nwith GPT models.\nFigure 4: The results on English (en) MTOP dataset\nwith GPT models.\nFigure 5: The results on Spanish (es) MTOP dataset\nwith GPT models.\nFigure 6: The results on French (fr) MTOP dataset with\nGPT models.\nFigure 7: The results on English (en) multilingual NLU\ndataset with GPT models.\nFigure 8: The results on Spanish (es) multilingual NLU\ndataset with GPT models.\n6.2 Pre-trained Language Models\nRecent advances in pre-trained LMs have been\nfocused on building pre-trained encoders, such\nas BERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019a), ELMO (Peters et al., 2018), ULM-\nFiT (Howard and Ruder, 2018), ELECTRA (Clark\net al., 2019), XLM (Conneau and Lample, 2019),\nand XLM-R (Conneau et al., 2020; Goyal et al.,\n2021), decoder-only models, such as GPT mod-\nels (Radford et al., 2019; Brown et al., 2020) and\nencoder-decoder models, such as T5 (Raffel et al.,\n2020), BART (Lewis et al., 2020), and their mul-\n9\ntilingual versions, mT5 (Xue et al., 2021) and\nmBART (Liu et al., 2020a).\nPre-trained encoders have been used to improve\nthe contextualized representations of multilingual\nsystems in various NLP tasks, for example, dia-\nlogue systems (Liu et al., 2020b, 2021d; Li et al.,\n2021), code-switching sequence labeling (Aguilar\net al., 2020; Winata et al., 2021; Winata, 2021), and\nmultilingual speech recognition (Datta et al., 2020;\nWinata et al., 2020). Meanwhile, the pre-trained\nencoder-decoder models, have been used for vari-\nous sequence generation tasks, such as summariza-\ntion (Raffel et al., 2020), conversational agents (Lin\net al., 2020b,a; Madotto et al., 2020a; Wu and\nXiong, 2020; Hosseini-Asl et al., 2020; Lin et al.,\n2021b), and knowledge grounding (Chen et al.,\n2020; Zhao et al., 2020).\n7 Limitation and Future Work\nMore Languages In this paper, we explored only\ncross-lingual transfer learning from and to Latin-\nbased language (e.g., English to Spanish / French\n/ German). Extending our approach to non-Latin\nlanguages (e.g., Thai, Chinese, etc.) is challeng-\ning for two reasons: 1) we are currently using En-\nglish tokenizers which are known to fails, or they\nassign UNK tokens when prompt with non-Latin\ncharacters, and 2) a possible little, or absent, the\nnamed entity overlap between the source and target\nlanguage, which could make the English prompt\ncompletely irrelevant. The latter suggests an in-\nteresting future work, where we could study the\ncorrelation between performance and word (or to-\nken) overlapping of the source (en) and the target\nlanguage samples.\nMore Datasets and Models Intent recognition is\nan important task, especially in multiple language\nscenarios. In future work, we plan to include the\nmissing languages of MTOP and MultiNLU, and\nto add more languages from the MultiATIS++ (Xu\net al., 2020) which consists of a total of 9 languages,\nthat is, English, Spanish, German, French, Por-\ntuguese, Chinese, Japanese, Hindi, and Turkish.\nMoreover, to cope with the tokenization issues, we\nwould like to explore multilingual LMs such as\nMT5 (Xue et al., 2021).\n8 Conclusion\nThis paper demonstrates the multilingual skills of\npre-trained LMs, GPT and T5, in conducting in-\ncontext learning without parameter updates. This\nwork is our initial attempt to show the effectiveness\nof in-context learning in the multilingual and cross-\nlingual setting. It covers four different languages\nand explores the possibility of conducting efﬁcient\ninference on low-resource tasks. We ﬁnd that LMs\ncan predict samples correctly, signiﬁcantly better\nthan the random prediction, in cross-lingual tasks\nwith no training examples of the target languages.\nWe would like to investigate further the applicabil-\nity of this method to other tasks and languages in\nfuture work.\nAcknowledgment\nWe want to thank Bryan Wilie and Samuel Cahyaw-\nijaya for their support in accessing the cloud ser-\nvice. We also sincerely thank Zihan Liu and ML\nCollective members for helping with the discussion\nabout this project. Finally, we want to thanks the\nreviewer of the paper for their meaningful com-\nments and suggestions. Given the short time for\nthe camera-ready, we tried our best to improve the\nﬁnal version of the paper.\nReferences\nGustavo Aguilar, Bryan McCann, Tong Niu,\nNazneen Rajani, N. Keskar, and T. Solorio.\n2020. Char2subword: Extending the subword em-\nbedding space from pre-trained models using robust\ncharacter compositionality. ArXiv, abs/2010.12730.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nAbhik Bhattacharjee, Tahmid Hasan, Kazi Samin,\nM Sohel Rahman, Anindya Iqbal, and Rifat Shahri-\nyar. 2021. Banglabert: Combating embedding bar-\nrier for low-resource language understanding. arXiv\npreprint arXiv:2101.00204.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nSamuel Cahyawijaya, Genta Indra Winata, Holy Love-\nnia, Bryan Wilie, Wenliang Dai, Etsuko Ishii, and\nPascale Fung. 2021a. Greenformer: Factorization\ntoolkit for efﬁcient deep neural networks.\nSamuel Cahyawijaya, Genta Indra Winata, Bryan\nWilie, Karissa Vincentio, Xiaohong Li, Adhiguna\nKuncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Ba-\nhar, Masayu Leylia Khodra, et al. 2021b. Indonlg:\n10\nBenchmark and resources for evaluating indone-\nsian natural language generation. arXiv preprint\narXiv:2104.08200.\nGuanhua Chen, Shuming Ma, Yun Chen, Li Dong,\nDongdong Zhang, Jia Pan, Wenping Wang, and Furu\nWei. 2021. Zero-shot cross-lingual transfer of neu-\nral machine translation with multilingual pretrained\nencoders. arXiv preprint arXiv:2104.08757.\nWenhu Chen, Yu Su, Xifeng Yan, and William Yang\nWang. 2020. KGPT: Knowledge-grounded pre-\ntraining for data-to-text generation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n8635–8648, Online. Association for Computational\nLinguistics.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. In International Conference on Learning Rep-\nresentations.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Édouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. Advances in\nNeural Information Processing Systems , 32:7059–\n7069.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2475–2485.\nAlice Coucke, Alaa Saade, Adrien Ball, Théodore\nBluche, Alexandre Caulier, David Leroy, Clément\nDoumouro, Thibault Gisselbrecht, Francesco Calta-\ngirone, Thibaut Lavril, et al. 2018. Snips voice plat-\nform: an embedded spoken language understanding\nsystem for private-by-design voice interfaces. arXiv\npreprint arXiv:1805.10190.\nArindrima Datta, Bhuvana Ramabhadran, Jesse\nEmond, Anjuli Kannan, and Brian Roark. 2020.\nLanguage-agnostic multilingual modeling. In\nICASSP 2020-2020 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 8239–8243. IEEE.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nMoussa Kamal Eddine, Antoine J-P Tixier, and\nMichalis Vazirgiannis. 2020. Barthez: a skilled pre-\ntrained french sequence-to-sequence model. arXiv\npreprint arXiv:2010.12321.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020a.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020b.\nMaking pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nNaman Goyal, Jingfei Du, Myle Ott, Giri Ananthara-\nman, and Alexis Conneau. 2021. Larger-scale trans-\nformers for multilingual masked language modeling.\narXiv preprint arXiv:2105.00572.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,\nSemih Yavuz, and Richard Socher. 2020. A simple\nlanguage model for task-oriented dialogue. arXiv\npreprint arXiv:2005.00796.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaš. 2020. From zero to hero: On the lim-\nitations of zero-shot language transfer with multilin-\ngual transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4483–4499.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoit Crabbé, Laurent Besacier, and Didier\nSchwab. 2020. Flaubert: Unsupervised language\nmodel pre-training for french. In Proceedings of\nthe 12th Language Resources and Evaluation Con-\nference, pages 2479–2490.\nSangah Lee, Hansol Jang, Yunmee Baik, Suzi Park,\nand Hyopil Shin. 2020. Kr-bert: A small-scale\nkorean-speciﬁc language model. arXiv preprint\narXiv:2008.03979.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880.\n11\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit\nGupta, Sonal Gupta, and Yashar Mehdad. 2021.\nMtop: A comprehensive multilingual task-oriented\nsemantic parsing benchmark. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 2950–2962.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-\ntuning: Optimizing continuous prompts for genera-\ntion. arXiv preprint arXiv:2101.00190.\nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li,\nYuyan Zhang, Mengzhou Xia, Shruti Rijhwani,\nJunxian He, Zhisong Zhang, Xuezhe Ma, et al. 2019.\nChoosing transfer languages for cross-lingual learn-\ning. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 3125–3135.\nZhaojiang Lin, Bing Liu, Seungwhan Moon, Paul A\nCrook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu,\nAndrea Madotto, Eunjoon Cho, and Rajen Subba.\n2021a. Leveraging slot descriptions for zero-shot\ncross-domain dialogue statetracking. In Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n5640–5648.\nZhaojiang Lin, Zihan Liu, Genta Indra Winata, Samuel\nCahyawijaya, Andrea Madotto, Yejin Bang, Etsuko\nIshii, and Pascale Fung. 2020a. Xpersona: Eval-\nuating multilingual personalized chatbot. arXiv\npreprint arXiv:2003.07568.\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata,\nPeng Xu, Feijun Jiang, Yuxiang Hu, Chen Shi, and\nPascale Fung. 2021b. Bitod: A bilingual multi-\ndomain dataset for task-oriented dialogue modeling.\narXiv e-prints, pages arXiv–2106.\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata,\nand Pascale Fung. 2020b. Mintl: Minimalist trans-\nfer learning for task-oriented dialogue systems. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3391–3405.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021a. What\nmakes good in-context examples for gpt- 3? arXiv\npreprint arXiv:2101.06804.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021b. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020a. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019a.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZihan Liu, Jamin Shin, Yan Xu, Genta Indra Winata,\nPeng Xu, Andrea Madotto, and Pascale Fung. 2019b.\nZero-shot cross-lingual dialogue systems with trans-\nferable latent variables. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1297–1303.\nZihan Liu, Genta Indra Winata, Zhaojiang Lin, Peng\nXu, and Pascale Fung. 2020b. Attention-informed\nmixed-language training for zero-shot cross-lingual\ntask-oriented dialogue systems. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 34, pages 8433–8440.\nZihan Liu, Genta Indra Winata, Peng Xu, and Pascale\nFung. 2021c. X2parser: Cross-lingual and cross-\ndomain framework for task-oriented compositional\nsemantic parsing. arXiv preprint arXiv:2106.03777.\nZihan Liu, Genta Indra Winata, Peng Xu, and Pas-\ncale Fung. 2021d. X2Parser: Cross-lingual and\ncross-domain framework for task-oriented compo-\nsitional semantic parsing. In Proceedings of the\n6th Workshop on Representation Learning for NLP\n(RepL4NLP-2021), pages 112–127, Online. Associ-\nation for Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\nRiedel, and Pontus Stenetorp. 2021. Fantastically\nordered prompts and where to ﬁnd them: Overcom-\ning few-shot prompt order sensitivity.arXiv preprint\narXiv:2104.08786.\nAndrea Madotto, Samuel Cahyawijaya, Genta Indra\nWinata, Yan Xu, Zihan Liu, Zhaojiang Lin, and Pas-\ncale Fung. 2020a. Learning knowledge bases with\nparameters for task-oriented dialogue systems. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: Findings,\npages 2372–2394.\nAndrea Madotto, Zihan Liu, Zhaojiang Lin, and Pas-\ncale Fung. 2020b. Language models as few-shot\nlearner for task-oriented dialogue systems. arXiv\npreprint arXiv:2008.06239.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSuárez, Yoann Dupont, Laurent Romary, Éric Ville-\nmonte de la Clergerie, Djamé Seddah, and Benoît\nSagot. 2020. Camembert: a tasty french language\nmodel. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7203–7219.\nDat Quoc Nguyen and Anh Tuan Nguyen. 2020.\nPhobert: Pre-trained language models for viet-\nnamese. In Proceedings of the 2020 Conference on\n12\nEmpirical Methods in Natural Language Processing:\nFindings, pages 1037–1042.\nSungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik\nCho, Jiyoon Han, Jangwon Park, Chisung Song, Jun-\nseong Kim, Yongsook Song, Taehwan Oh, et al.\n2021. Klue: Korean language understanding eval-\nuation. arXiv preprint arXiv:2105.09680.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. arXiv\npreprint arXiv:2105.11447.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–\n2237.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2020. Mad-x: An adapter-based frame-\nwork for multi-task cross-lingual transfer. arXiv\npreprint arXiv:2005.00052.\nEdoardo Maria Ponti, Ivan Vuli´c, Goran Glavaš, Nikola\nMrkši´c, and Anna Korhonen. 2018. Adversarial\npropagation and zero-shot cross-lingual transfer of\nword vector specialization. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 282–293.\nLibo Qin, Wanxiang Che, Yangming Li, Haoyang Wen,\nand Ting Liu. 2019. A stack-propagation frame-\nwork with token-level intent detection for spoken\nlanguage understanding. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2078–2087.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21:1–67.\nRaphael Scheible, Fabian Thomczyk, Patric Tippmann,\nVictor Jaravine, and Martin Boeker. 2020. Got-\ntbert: a pure german language model. arXiv preprint\narXiv:2012.02110.\nTimo Schick and Hinrich Schütze. 2021. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2339–2352.\nSebastian Schuster, Sonal Gupta, Rushin Shah, and\nMike Lewis. 2019. Cross-lingual transfer learning\nfor multilingual task oriented dialog. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 3795–3805.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Eliciting\nknowledge from language models using automati-\ncally generated prompts. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4222–4235.\nBryan Wilie, Karissa Vincentio, Genta Indra Winata,\nSamuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim,\nSidik Soleman, Rahmad Mahendra, Pascale Fung,\nSyafri Bahar, et al. 2020. Indonlu: Benchmark and\nresources for evaluating indonesian natural language\nunderstanding. In Proceedings of the 1st Confer-\nence of the Asia-Paciﬁc Chapter of the Association\nfor Computational Linguistics and the 10th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 843–857.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122.\nGenta Indra Winata. 2021. Multilingual transfer learn-\ning for code-switched language and speech neural\nmodeling. arXiv preprint arXiv:2104.06268.\nGenta Indra Winata, Samuel Cahyawijaya, Zihan Liu,\nZhaojiang Lin, Andrea Madotto, and Pascale Fung.\n2021. Are multilingual models effective in code-\nswitching? In Proceedings of the Fifth Workshop\non Computational Approaches to Linguistic Code-\nSwitching, pages 142–153.\nGenta Indra Winata, Andrea Madotto, Chien-Sheng\nWu, and Pascale Fung. 2019. Code-switched lan-\nguage models using neural based synthetic data from\nparallel sentences. In Proceedings of the 23rd Con-\nference on Computational Natural Language Learn-\ning (CoNLL), pages 271–280.\n13\nGenta Indra Winata, Guangsen Wang, Caiming Xiong,\nand Steven Hoi. 2020. Adapt-and-adjust: Over-\ncoming the long-tail problem of multilingual speech\nrecognition. arXiv preprint arXiv:2012.01687.\nChien-Sheng Wu and Caiming Xiong. 2020. Probing\ntask-oriented dialogue representation from language\nmodels. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 5036–5051.\nWeijia Xu, Batool Haider, and Saab Mansour. 2020.\nEnd-to-end slot alignment and recognition for cross-\nlingual nlu. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 5052–5063.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mt5: A massively\nmultilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 483–498.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. Advances in Neural Infor-\nmation Processing Systems, 32:5753–5763.\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Im-\nproving few-shot performance of language models.\narXiv preprint arXiv:2102.09690.\nXueliang Zhao, Wei Wu, Can Xu, Chongyang Tao,\nDongyan Zhao, and Rui Yan. 2020. Knowledge-\ngrounded dialogue generation with pre-trained lan-\nguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 3377–3390, Online. As-\nsociation for Computational Linguistics.\nBo Zheng, Li Dong, Shaohan Huang, Wenhui Wang,\nZewen Chi, Saksham Singhal, Wanxiang Che, Ting\nLiu, Xia Song, and Furu Wei. 2021. Consistency\nregularization for cross-lingual ﬁne-tuning. arXiv\npreprint arXiv:2106.08226.\nA Full k-shot Results\nThis appendix shows the results on few-shot mono-\nlingual and cross-lingual settings on SNIPS, MTOP,\nand multilingual NLU datasets over a different\nnumber of samples.\n14\nFigure 9: The acc results on English (en) SNIPS with\nGPT models.\nFigure 10: The f1 results on English (en) SNIPS with\nGPT models.\nFigure 11: The acc results on the cross-lingual setting,\nEnglish-German (de) MTOP dataset with GPT models.\nFigure 12: The f1 results on the cross-lingual setting,\nEnglish-German (de) MTOP dataset with GPT models.\nFigure 13: The acc results on the cross-lingual setting,\nEnglish-Spanish (es) MTOP dataset with GPT models.\nFigure 14: The f1 results on the cross-lingual setting,\nEnglish-Spanish (es) MTOP dataset with GPT models.\n15\nFigure 15: The acc results on the cross-lingual setting,\nEnglish-French (fr) MTOP dataset with GPT models.\nFigure 16: The f1 results on the cross-lingual setting,\nEnglish-French (fr) MTOP dataset with GPT models.\nFigure 17: The acc results on the cross-lingual setting,\nEnglish-Spanish (es) multilingual NLU dataset with\nGPT models.\nFigure 18: The f1 results on the cross-lingual setting,\nEnglish-Spanish (es) multilingual NLU dataset with\nGPT models.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8373055458068848
    },
    {
      "name": "Natural language processing",
      "score": 0.7106994390487671
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6531385779380798
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6395002007484436
    },
    {
      "name": "Class (philosophy)",
      "score": 0.5586604475975037
    },
    {
      "name": "Language model",
      "score": 0.5207152962684631
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4749310612678528
    },
    {
      "name": "English language",
      "score": 0.45303356647491455
    },
    {
      "name": "Shot (pellet)",
      "score": 0.42270126938819885
    },
    {
      "name": "Linguistics",
      "score": 0.21101754903793335
    },
    {
      "name": "Engineering",
      "score": 0.055732667446136475
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 15
}