{
  "title": "A Latent Variable Recurrent Neural Network for Discourse-Driven Language Models",
  "url": "https://openalex.org/W2964036636",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2153992776",
      "name": "Yangfeng Ji",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1432492132",
      "name": "Gholamreza Haffari",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2142862529",
      "name": "Jacob Eisenstein",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2128909582",
    "https://openalex.org/W29849901",
    "https://openalex.org/W2180877453",
    "https://openalex.org/W2951570923",
    "https://openalex.org/W2131861279",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2083195487",
    "https://openalex.org/W108071511",
    "https://openalex.org/W2164567676",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2207587218",
    "https://openalex.org/W2197913429",
    "https://openalex.org/W2964106094",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2166637769",
    "https://openalex.org/W1824523713",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2294607529",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2111535382",
    "https://openalex.org/W2250767751",
    "https://openalex.org/W1526096287",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2949952998",
    "https://openalex.org/W1779483307",
    "https://openalex.org/W2950616554",
    "https://openalex.org/W1855867616",
    "https://openalex.org/W2251293245",
    "https://openalex.org/W2128970689",
    "https://openalex.org/W2438667436",
    "https://openalex.org/W2964325005",
    "https://openalex.org/W2950067852",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2251849926",
    "https://openalex.org/W1612268991",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W3018009202",
    "https://openalex.org/W2963275229",
    "https://openalex.org/W2109318894",
    "https://openalex.org/W2120262106",
    "https://openalex.org/W2166957049",
    "https://openalex.org/W2106925883",
    "https://openalex.org/W2251753273",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W1806891645",
    "https://openalex.org/W2251826083",
    "https://openalex.org/W2134036914",
    "https://openalex.org/W2109462987",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W592244745",
    "https://openalex.org/W2158139315",
    "https://openalex.org/W2250201115"
  ],
  "abstract": "This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences.A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations.The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task.The resulting model can therefore employ a training objective that includes not only discourse relation classification, but also word prediction.As a result, it outperforms state-ofthe-art alternatives for two tasks: implicit discourse relation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus.Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline.",
  "full_text": "Proceedings of NAACL-HLT 2016, pages 332–342,\nSan Diego, California, June 12-17, 2016.c⃝2016 Association for Computational Linguistics\nA Latent Variable Recurrent Neural Network\nfor Discourse Relation Language Models\nYangfeng Ji\nGeorgia Institute of Technology\nAtlanta, GA 30308, USA\njiyfeng@gatech.edu\nGholamreza Haffari\nMonash University\nClayton, VIC, Australia\ngholamreza.haffari\n@monash.edu\nJacob Eisenstein\nGeorgia Institute of Technology\nAtlanta, GA 30308, USA\njacobe@gatech.edu\nAbstract\nThis paper presents a novel latent variable re-\ncurrent neural network architecture for jointly\nmodeling sequences of words and (possibly\nlatent) discourse relations between adjacent\nsentences. A recurrent neural network gen-\nerates individual words, thus reaping the ben-\neﬁts of discriminatively-trained vector repre-\nsentations. The discourse relations are rep-\nresented with a latent variable, which can be\npredicted or marginalized, depending on the\ntask. The resulting model can therefore em-\nploy a training objective that includes not only\ndiscourse relation classiﬁcation, but also word\nprediction. As a result, it outperforms state-of-\nthe-art alternatives for two tasks: implicit dis-\ncourse relation classiﬁcation in the Penn Dis-\ncourse Treebank, and dialog act classiﬁcation\nin the Switchboard corpus. Furthermore, by\nmarginalizing over latent discourse relations\nat test time, we obtain a discourse informed\nlanguage model, which improves over a strong\nLSTM baseline.\n1 Introduction\nNatural language processing (NLP) has recently ex-\nperienced a neural network “tsunami” (Manning,\n2016). A key advantage of these neural architec-\ntures is that they employ discriminatively-trained\ndistributed representations, which can capture the\nmeaning of linguistic phenomena ranging from in-\ndividual words (Turian et al., 2010) to longer-range\nlinguistic contexts at the sentence level (Socher et\nal., 2013) and beyond (Le and Mikolov, 2014). Be-\ncause they are discriminatively trained, these meth-\nods can learn representations that yield very accurate\npredictive models (e.g., Dyer et al, 2015).\nHowever, in comparison with the probabilistic\ngraphical models that were previously the dominant\nmachine learning approach for NLP, neural archi-\ntectures lack ﬂexibility. By treating linguistic an-\nnotations as random variables, probabilistic graphi-\ncal models can marginalize over annotations that are\nunavailable at test or training time, elegantly model-\ning multiple linguistic phenomena in a joint frame-\nwork (Finkel et al., 2006). But because these graph-\nical models represent uncertainty for every element\nin the model, adding too many layers of latent vari-\nables makes them difﬁcult to train.\nIn this paper, we present a hybrid architecture\nthat combines a recurrent neural network language\nmodel with a latent variable model over shallow\ndiscourse structure. In this way, the model learns\na discriminatively-trained distributed representation\nof the local contextual features that drive word\nchoice at the intra-sentence level, using techniques\nthat are now state-of-the-art in language model-\ning (Mikolov et al., 2010). However, the model\ntreats shallow discourse structure — speciﬁcally, the\nrelationships between pairs of adjacent sentences —\nas a latent variable. As a result, the model can act\nas both a discourse relation classiﬁer and a language\nmodel. Speciﬁcally:\n•If trained to maximize the conditional like-\nlihood of the discourse relations, it outper-\nforms state-of-the-art methods for both im-\nplicit discourse relation classiﬁcation in the\nPenn Discourse Treebank (Rutherford and Xue,\n2015) and dialog act classiﬁcation in Switch-\n332\nboard (Kalchbrenner and Blunsom, 2013). The\nmodel learns from both the discourse annota-\ntions as well as the language modeling objec-\ntive, unlike previous recursive neural architec-\ntures that learn only from annotated discourse\nrelations (Ji and Eisenstein, 2015).\n•If the model is trained to maximize the joint\nlikelihood of the discourse relations and the\ntext, it is possible to marginalize over discourse\nrelations at test time, outperforming language\nmodels that do not account for discourse struc-\nture.\nIn contrast to recent work on continuous latent\nvariables in recurrent neural networks (Chung et al.,\n2015), which require complex variational autoen-\ncoders to represent uncertainty over the latent vari-\nables, our model is simple to implement and train,\nrequiring only minimal modiﬁcations to existing re-\ncurrent neural network architectures that are imple-\nmented in commonly-used toolkits such as Theano,\nTorch, and CNN.\nWe focus on a class of shallow discourse rela-\ntions, which hold between pairs of adjacent sen-\ntences (or utterances). These relations describe how\nthe adjacent sentences are related: for example, they\nmay be in CONTRAST , or the latter sentence may of-\nfer an answer to a question posed by the previous\nsentence. Shallow relations do not capture the full\nrange of discourse phenomena (Webber et al., 2012),\nbut they account for two well-known problems: im-\nplicit discourse relation classiﬁcation in the Penn\nDiscourse Treebank, which was the 2015 CoNLL\nshared task (Xue et al., 2015); and dialog act clas-\nsiﬁcation, which characterizes the structure of in-\nterpersonal communication in the Switchboard cor-\npus (Stolcke et al., 2000), and is a key component of\ncontemporary dialog systems (Williams and Young,\n2007). Our model outperforms state-of-the-art alter-\nnatives for implicit discourse relation classiﬁcation\nin the Penn Discourse Treebank, and for dialog act\nclassiﬁcation in the Switchboard corpus.\n2 Background\nOur model scaffolds on recurrent neural network\n(RNN) language models (Mikolov et al., 2010), and\nrecent variants that exploit multiple levels of linguis-\ntic detail (Ji et al., 2015; Lin et al., 2015).\nRNN Language Models Let us denote token nin\na sentence t by yt,n ∈ {1 ...V }, and write yt =\n{yt,n}n∈{1...Nt}to indicate the sequence of words in\nsentence t. In an RNN language model, the proba-\nbility of the sentence is decomposed as,\np(yt) =\nNt∏\nn\np(yt,n |yt,<n), (1)\nwhere the probability of each word yt,n is condi-\ntioned on the entire preceding sequence of words\nyt,<n through the summary vector ht,n−1. This vec-\ntor is computed recurrently from ht,n−2 and from\nthe embedding of the current word, Xyt,n−1 , where\nX ∈RK×V and Kis the dimensionality of the word\nembeddings. The language model can then be sum-\nmarized as,\nht,n =f(Xyt,n,ht,n−1) (2)\np(yt,n |yt,<n) =softmax (Woht,n−1 + bo) , (3)\nwhere the matrix Wo ∈RV ×K deﬁnes the output\nembeddings, and bo ∈RV is an offset. The function\nf(·) is a deterministic non-linear transition function.\nIt typically takes an element-wise non-linear trans-\nformation (e.g., tanh) of a vector resulting from the\nsum of the word embedding and a linear transforma-\ntion of the previous hidden state.\nThe model as described thus far is identical to the\nrecurrent neural network language model (RNNLM)\nof Mikolov et al. (2010). In this paper, we replace\nthe above simple hidden state units with the more\ncomplex Long Short-Term Memory units (Hochre-\niter and Schmidhuber, 1997), which have consis-\ntently been shown to yield much stronger perfor-\nmance in language modeling (Pham et al., 2014).\nFor simplicity, we still use the term RNNLM in re-\nferring to this model.\nDocument Context Language Model One draw-\nback of the RNNLM is that it cannot propagate long-\nrange information between the sentences. Even if\nwe remove sentence boundaries, long-range infor-\nmation will be attenuated by repeated application of\nthe non-linear transition function. Ji et al. (2015)\npropose the Document Context Language Model\n(DCLM) to address this issue. The core idea is to\nrepresent context with two vectors: ht,n, represent-\ning intra-sentence word-level context, and ct, rep-\nresenting inter-sentence context. These two vectors\n333\nyt −1, Nt−1−2 yt −1, Nt−1−1 yt −1, Nt−1\nyt ,1 yt ,2\nzt\nyt ,3\nFigure 1:A fragment of our model with latent variable zt, which only illustrates discourse information ﬂow from sentence (t −1)\nto t. The information from sentence (t − 1) affects the distribution of zt and then the words prediction within sentence t.\np(yt,n+1 |zt, yt,<n, yt−1) =g\n(\nW(zt)\no ht,n  \nrelation-speciﬁc\nintra-sentential context\n+ W(zt)\nc ct−1  \nrelation-speciﬁc\ninter-sentential context\n+ b(zt)\no\nrelation-speciﬁc\nbias\n)\n(4)\nFigure 2:Per-token generative probabilities in the discourse relation language model\nare then linearly combined in the generation func-\ntion for word yt,n,\np(yt,n |yt,<n,y<t)\n= softmax (Woht,n−1 + Wcct−1 + bo) , (5)\nwhere ct−1 is set to the last hidden state of the pre-\nvious sentence. Ji et al. (2015) show that this model\ncan improve language model perplexity.\n3 Discourse Relation Language Models\nWe now present a probabilistic neural model over\nsequences of words and shallow discourse relations.\nDiscourse relations zt are treated as latent variables,\nwhich are linked with a recurrent neural network\nover words in a latent variable recurrent neural net-\nwork (Chung et al., 2015).\n3.1 The Model\nOur model (see Figure 1) is formulated as a two-step\ngenerative story. In the ﬁrst step, context informa-\ntion from the sentence (t−1) is used to generate the\ndiscourse relation between sentences (t−1) and t,\np(zt |yt−1) =softmax (Uct−1 + b) , (6)\nwhere zt is a random variable capturing the dis-\ncourse relation between the two sentences, and ct−1\nis a vector summary of the contextual information\nfrom sentence (t−1), just as in the DCLM (Equa-\ntion 5). The model maintains a default context vec-\ntor c0 for the ﬁrst sentences of documents, and treats\nit as a parameter learned with other model parame-\nters during training.\nIn the second step, the sentence yt is generated,\nconditioning on the preceding sentenceyt−1 and the\ndiscourse relation zt:\np(yt |zt,yt−1) =\nNt∏\nn\np(yt,n |yt,<n,yt−1,zt), (7)\nThe generative probability for the sentence yt de-\ncomposes across tokens as usual (Equation 7). The\nper-token probabilities are shown in Equation 4, in\nFigure 2. Discourse relations are incorporated by pa-\nrameterizing the output matrices W(zt)\no and W(zt)\nc ;\ndepending on the discourse relation that holds be-\ntween (t−1) and t, these matrices will favor dif-\nferent parts of the embedding space. The bias term\nb(zt)\no is also parametrized by the discourse relation,\nso that each relation can favor speciﬁc words.\nOverall, the joint probability of the text and dis-\ncourse relations is,\np(y1:T ,z1:T ) =\nT∏\nt\np(zt |yt−1) ×p(yt |zt,yt−1).\n(8)\nIf the discourse relations zt are not observed, then\nour model is a form of latent variable recurrent neu-\nral network (LVRNN). Connections to recent work\non LVRNNs are discussed in §6; the key difference\nis that the latent variables here correspond to linguis-\ntically meaningful elements, which we may wish to\npredict or marginalize, depending on the situation.\nParameter Tying As proposed, the Discourse Re-\nlation Language Model has a large number of pa-\nrameters. Let K, H and V be the input dimension,\n334\nhidden dimension and the size of vocabulary in lan-\nguage modeling. The size of each prediction matrix\nW(z)\no and W(z)\nc is V ×H; there are two such matri-\nces for each possible discourse relation. We reduce\nthe number of parameters by factoring each of these\nmatrices into two components:\nW(z)\no = Wo ·V(z), W(z)\nc = Wc ·M(z), (9)\nwhere V(z) and M(z) are relation-speciﬁc compo-\nnents for intra-sentential and inter-sentential con-\ntexts; the size of these matrices is H ×H, with\nH ≪V. The larger V ×H matrices Wo and Wc\nare shared across all relations.\n3.2 Inference\nThere are two possible inference scenarios: in-\nference over discourse relations, conditioning on\nwords; and inference over words, marginalizing over\ndiscourse relations.\nInference over Discourse Relations The prob-\nability of discourse relations given the sentences\np(z1:T | y1:T ) is decomposed into the product of\nprobabilities of individual discourse relations condi-\ntioned on the adjacent sentences∏\nt p(zt |yt,yt−1).\nThese probabilities are computed by Bayes’ rule:\np(zt |yt,yt−1) = p(yt |zt,yt−1) ×p(zt |yt−1)∑\nz′ p(yt |z′,yt−1) ×p(z′|yt−1).\n(10)\nThe terms in each product are given in Equations 6\nand 7. Normalizing involves only a sum over a small\nﬁnite number of discourse relations. Note that infer-\nence is easy in our case because all words are ob-\nserved and there is no probabilistic coupling of the\ndiscourse relations.\nInference over Words In discourse-informed lan-\nguage modeling, we marginalize over discourse re-\nlations to compute the probability of a sequence of\nsentence y1:T , which can be written as,\np(y1:T ) =\nT∏\nt\n∑\nzt\np(zt |yt−1) ×p(yt |zt,yt−1),\n(11)\nbecause the word sequences are observed, decou-\npling each zt from its neighbors zt+1 and zt−1.\nThis decoupling ensures that we can compute the\noverall marginal likelihood as a product over local\nmarginals.\n3.3 Learning\nThe model can be trained in two ways: to maximize\nthe joint probability p(y1:T ,z1:T ), or to maximize\nthe conditional probability p(z1:T |y1:T ). The joint\ntraining objective is more suitable for language mod-\neling scenarios, and the conditional objective is bet-\nter for discourse relation prediction. We now de-\nscribe each objective in detail.\nJoint likelihood objective The joint likelihood\nobjective function is directly adopted from the joint\nprobability deﬁned in Equation 8. The objective\nfunction for a single document with T sentences or\nutterances is,\nℓ(θ) =\nT∑\nt\nlog p(zt |yt−1)\n+\nNt∑\nn\nlog p(yt,n |yt,<n,yt−1,zt), (12)\nwhere θrepresents the collection of all model pa-\nrameters, including the parameters in the LSTM\nunits and the word embeddings.\nMaximizing the objective function ℓ(θ) will\njointly optimize the model on both language lan-\nguage and discourse relation prediction. As such,\nit can be viewed as a form of multi-task learn-\ning (Caruana, 1997), where we learn a shared rep-\nresentation that works well for discourse relation\nprediction and for language modeling. However, in\npractice, the large vocabulary size and number of to-\nkens means that the language modeling part of the\nobjective function tends to dominate.\nConditional objective This training objective is\nspeciﬁc to the discourse relation prediction task, and\nbased on Equation 10 can be written as:\nℓr(θ) =\nT∑\nt\nlog p(zt |yt−1) + logp(yt |zt,yt−1)\n−log\n∑\nz′\np(z′|yt−1) ×p(yt |z′,yt−1)\n(13)\nThe ﬁrst line in Equation 13 is the same as ℓ(θ),\nbut the second line reﬂects the normalization over all\n335\npossible values of zt. This forces the objective func-\ntion to attend speciﬁcally to the problem of maxi-\nmizing the conditional likelihood of the discourse\nrelations and treat language modeling as an auxil-\niary task (Collobert et al., 2011).\n3.4 Modeling limitations\nThe discourse relation language model is carefully\ndesigned to decouple the discourse relations from\neach other, after conditioning on the words. It\nis clear that text documents and spoken dialogues\nhave sequential discourse structures, and it seems\nlikely that modeling this structure could improve\nperformance. In a traditional hidden Markov model\n(HMM) generative approach (Stolcke et al., 2000),\nmodeling sequential dependencies is not difﬁcult,\nbecause training reduces to relative frequency es-\ntimation. However, in the hybrid probabilistic-\nneural architecture proposed here, training is al-\nready expensive, due to the large number of param-\neters that must be estimated. Adding probabilis-\ntic couplings between adjacent discourse relations\n⟨zt−1,zt⟩would require the use of dynamic pro-\ngramming for both training and inference, increas-\ning time complexity by a factor that is quadratic in\nthe number of discourse relations. We did not at-\ntempt this in this paper; we do compare against a\nconventional HMM on the dialogue act prediction\ntask in §5.\nJi et al. (2015) propose an alternative form of\nthe document context language model, in which the\ncontextual information ct impacts the hidden state\nht+1, rather than going directly to the outputs yt+1.\nThey obtain slightly better perplexity with this ap-\nproach, which has fewer trainable parameters. How-\never, this model would couplezt with all subsequent\nsentences y>t, making prediction and marginaliza-\ntion of discourse relations considerably more chal-\nlenging. Sequential Monte Carlo algorithms offer a\npossible solution (de Freitas et al., ; Gu et al., 2015),\nwhich may be considered in future work.\n4 Data and Implementation\nWe evaluate our model on two benchmark datasets:\n(1) the Penn Discourse Treebank (Prasad et al.,\n2008, PDTB), which is annotated on a corpus of\nWall Street Journal acticles; (2) the Switchboard di-\nalogue act corpus (Stolcke et al., 2000, SWDA),\nwhich is annotated on a collections of phone con-\nversations. Both corpora contain annotations of dis-\ncourse relations and dialogue relations that hold be-\ntween adjacent spans of text.\nThe Penn Discourse Treebank (PDTB)provides\na low-level discourse annotation on written texts. In\nthe PDTB, each discourse relation is annotated be-\ntween two argument spans, Arg1 and Arg2. There\nare two types of relations: explicit and implicit.\nExplicit relations are signalled by discourse mark-\ners (e.g., “ however”, “moreover”), and the span of\nArg1 is almost totally unconstrained: it can range\nfrom a single clause to an entire paragraph, and\nneed not be adjacent to either Arg2 nor the dis-\ncourse marker. However, automatically classifying\nthese relations is considered to be relatively easy,\ndue to the constraints from the discourse marker it-\nself (Pitler et al., 2008). In addition, explicit rela-\ntions are difﬁcult to incorporate into language mod-\nels which must generate each word exactly once. On\nthe contrary, implicit discourse relations are anno-\ntated only between adjacent sentences, based on a\nsemantic understanding of the discourse arguments.\nAutomatically classifying these discourse relations\nis a challenging task (Lin et al., 2009; Pitler et al.,\n2009; Rutherford and Xue, 2015; Ji and Eisenstein,\n2015). We therefore focus on implicit discourse re-\nlations, leaving to the future work the question of\nhow to apply our modeling framework to explicit\ndiscourse relations. During training, we collapse all\nrelation types other than implicit (explicit, ENTREL,\nand N OREL) into a single dummy relation type,\nwhich holds between all adjacent sentence pairs that\ndo not share an implicit relation.\nAs in the prior work on ﬁrst-level discourse re-\nlation identiﬁcation (e.g., Park and Cardie, 2012),\nwe use sections 2-20 of the PDTB as the training\nset, sections 0-1 as the development set for param-\neter tuning, and sections 21-22 for testing. For pre-\nprocessing, we lower-cased all tokens, and substi-\ntuted all numbers with a special token “ NUM”. To\nbuild the vocabulary, we kept the 10,000 most fre-\nquent words from the training set, and replaced low-\nfrequency words with a special token “ UNK”. In\nprior work that focuses on detecting individual rela-\ntions, balanced training sets are constructed so that\n336\nthere are an equal number of instances with and\nwithout each relation type (Park and Cardie, ; Biran\nand McKeown, 2013; Rutherford and Xue, 2014).\nIn this paper, we target the more challenging multi-\nway classiﬁcation problem, so this strategy is not ap-\nplicable; in any case, since our method deals with\nentire documents, it is not possible to balance the\ntraining set in this way.\nThe Switchboard Dialog Act Corpus (SWDA)\nis annotated on the Switchboard Corpus of human-\nhuman conversational telephone speech (Godfrey et\nal., 1992). The annotations label each utterance\nwith one of 42 possible speech acts, such as AGREE ,\nHEDGE , and WH-QUESTION . Because these speech\nacts form the structure of the dialogue, most of them\npertain to both the preceding and succeeding utter-\nances (e.g., AGREE ). The SWDA corpus includes\n1155 ﬁve-minute conversations. We adopted the\nstandard split from Stolcke et al. (2000), using 1,115\nconversations for training and nineteen conversa-\ntions for test. For parameter tuning, we randomly\nselect nineteen conversations from the training set\nas the development set. After parameter tuning, we\ntrain the model on the full training set with the se-\nlected conﬁguration. We use the same preprocessing\ntechniques here as in the PDTB.\n4.1 Implementation\nWe use a single-layer LSTM to build the recur-\nrent architecture of our models, which we im-\nplement in the CNN package.1 Our implemen-\ntation is available on https://github.com/\njiyfeng/drlm. Some additional details follow.\nInitialization Following prior work on RNN ini-\ntialization (Bengio, 2012), all parameters except\nthe relation prediction parameters U and bare ini-\ntialized with random values drawn from the range\n[−\n√\n6/(d1 + d2),\n√\n6/(d1 + d2)], where d1 and d2\nare the input and output dimensions of the parame-\nter matrix respectively. The matrix U is initialized\nwith random numbers from [−10−5,10−5] and bis\ninitialized to 0.\nLearning Online learning was performed using\nAdaGrad (Duchi et al., 2011) with initial learning\n1https://github.com/clab/cnn\nrate λ= 0.1. To avoid the exploding gradient prob-\nlem, we used norm clipping trick with a threshold of\nτ = 5.0 (Pascanu et al., 2012). In addition, we used\nvalue dropout (Srivastava et al., 2014) with rate 0.5,\non the input X, context vector cand hidden state h,\nsimilar to the architecture proposed by Pham et al.\n(2014). The training procedure is monitored by the\nperformance on the development set. In our experi-\nments, 4 to 5 epochs were enough.\nHyper-parameters Our model includes two tun-\nable hyper-parameters: the dimension of word rep-\nresentation K, the hidden dimension of LSTM unit\nH. We consider the values {32,48,64,96,128}for\nboth K and H. For each corpus in experiments, the\nbest combination of K and H is selected via grid\nsearch on the development set.\n5 Experiments\nOur main evaluation is discourse relation prediction,\nusing the PDTB and SWDA corpora. We also eval-\nuate on language modeling, to determine whether\nincorporating discourse annotations at training time\nand then marginalizing them at test time can im-\nprove performance.\n5.1 Implicit discourse relation prediction on\nthe PDTB\nWe ﬁrst evaluate our model with implicit discourse\nrelation prediction on the PDTB dataset. Most of the\nprior work on ﬁrst-level discourse relation predic-\ntion focuses on the “one-versus-all” binary classiﬁ-\ncation setting, but we attack the more general four-\nway classiﬁcation problem, as performed by Ruther-\nford and Xue (2015). We compare against the fol-\nlowing methods:\nRutherford and Xue (2015)build a set of feature-\nrich classiﬁers on the PDTB, and then augment\nthese classiﬁers with additional automatically-\nlabeled training instances. We compare against\ntheir published results, which are state-of-the-art.\nJi and Eisenstein (2015)employ a recursive neural\nnetwork architecture. Their experimental setting\nis different, so we re-run their system using the\nsame setting as described in §4.\n337\nModel Accuracy Macro F1\nBaseline\n1. Most common class 54.7 —\nPrior work\n2. (Rutherford and Xue, 2015) 55.0 38.4\n3. (Rutherford and Xue, 2015) 57.1 40.5\nwith extra training data\n4. (Ji and Eisenstein, 2015) 56.4 40.0\nOur work -DRLM\n5. Joint training 57.1 40.5\n6. Conditional training 59.5∗ 42.3\n∗ signiﬁcantly better than lines 2 and 4 with p <0.05\nTable 1: Multiclass relation identiﬁcation on the ﬁrst-level\nPDTB relations.\nResults As shown in Table 1, the conditionally-\ntrained discourse relation language models (DRLM)\noutperforms all alternatives, on both metrics. While\nthe jointly-trained DRLM is at the same level as the\nprevious state-of-the-art, conditional training on the\nsame model provides a signiﬁcant additional advan-\ntage, indicated by a binomial test.\n5.2 Dialogue Act tagging\nDialogue act tagging has been widely studied in both\nNLP and speech communities. We follow the setup\nused by Stolcke et al. (2000) to conduct experiments,\nand adopt the following systems for comparison:\nStolcke et al. (2000) employ a hidden Markov\nmodel, with each HMM state corresponding to a\ndialogue act.\nKalchbrenner and Blunsom (2013) employ a\ncomplex neural architecture, with a convolutional\nnetwork at each utterance and a recurrent net-\nwork over the length of the dialog. To our knowl-\nedge, this model attains state-of-the-art accuracy\non this task, outperforming other prior work such\nas (Webb et al., 2005; Milajevs and Purver, 2014).\nResults As shown in Table 2, the conditionally-\ntrained discourse relation language model (D RLM)\noutperforms all competitive systems on this task. A\nbinomial test shows the result in line 6 is signiﬁ-\ncantly better than the previous state-of-the-art (line\n4). All comparisons are against published results,\nand Macro- F1 scores are not available. Accuracy\n1. Model Accuracy\nBaseline\n2. Most common class 31.5\nPrior work\n3. (Stolcke et al., 2000) 71.0\n4. (Kalchbrenner and Blunsom,\n2013)\n73.9\nOur work -DRLM\n5. Joint training 74.0\n6. Conditional training 77.0∗\n∗ signiﬁcantly better than line 4 with p <0.01\nTable 2:The results of dialogue act tagging.\nis more reliable on this evaluation, since no single\nclass dominates, unlike the PDTB task.\n5.3 Discourse-aware language modeling\nAs a joint model for discourse and language model-\ning, D RLM can also function as a language model,\nassigning probabilities to sequences of words while\nmarginalizing over discourse relations. To deter-\nmine whether discourse-aware language modeling\ncan improve performance, we compare against the\nfollowing systems:\nRNNLM+LSTM This is the same basic architec-\nture as the RNNLM proposed by (Mikolov et al.,\n2010), which was shown to outperform a Kneser-\nNey smoothed 5-gram model on modeling Wall\nStreet Journal text. Following Pham et al. (2014),\nwe replace the Sigmoid nonlinearity with a long\nshort-term memory (LSTM).\nDCLM We compare against the Document Context\nLanguage Model (DCLM) of Ji et al. (2015). We\nuse the “context-to-output” variant, which is iden-\ntical to the current modeling approach, except that\nit is not parametrized by discourse relations. This\nmodel achieves strong results on language model-\ning for small and medium-sized corpora, outper-\nforming RNNLM+LSTM.\nResults The perplexities of language modeling on\nthe PDTB and the SWDA are summarized in Ta-\nble 3. The comparison between line 1 and line\n2 shows the beneﬁt of considering multi-sentence\ncontext information on language modeling. Line\n3 shows that adding discourse relation information\n338\nPDTB SWDA\nModel K H PPLX K H PPLX\nBaseline\n1. RNNLM 96 128 117.8 128 96 56.0\n2. DCLM 96 96 112.2 96 96 45.3\nOur work\n3. D RLM 64 96 108.3 128 64 39.6\nTable 3:Language model perplexities ( PPLX ), lower is better.\nThe model dimensions K and H that gave best performance on\nthe dev set are also shown.\nyields further improvements for both datasets. We\nemphasize that discourse relations in the test doc-\numents are marginalized out, so no annotations are\nrequired for the test set; the improvements are due\nto the disambiguating power of discourse relations\nin the training set.\nBecause our training procedure requires discourse\nannotations, this approach does not scale to the large\ndatasets typically used in language modeling. As a\nconsequence, the results obtained here are somewhat\nacademic, from the perspective of practical language\nmodeling. Nonetheless, the positive results here mo-\ntivate the investigation of training procedures that\nare also capable of marginalizing over discourse re-\nlations at training time.\n6 Related Work\nThis paper draws on previous work in both discourse\nmodeling and language modeling.\nDiscourse and dialog modeling Early work on\ndiscourse relation classiﬁcation utilizes rich, hand-\ncrafted feature sets (Joty et al., 2012; Lin et al.,\n2009; Sagae, 2009). Recent representation learn-\ning approaches attempt to learn good representations\njointly with discourse relation classiﬁers and dis-\ncourse parsers (Ji and Eisenstein, 2014; Li et al.,\n2014). Of particular relevance are applications of\nneural architectures to PDTB implicit discourse re-\nlation classiﬁcation (Ji and Eisenstein, 2015; Zhang\net al., 2015; Braud and Denis, 2015). All of these\napproaches are essentially classiﬁers, and take su-\npervision only from the 16,000 annotated discourse\nrelations in the PDTB training set. In contrast, our\napproach is a probabilistic model over the entire text.\nProbabilistic models are frequently used in dia-\nlog act tagging, where hidden Markov models have\nbeen a dominant approach (Stolcke et al., 2000). In\nthis work, the emission distribution is an n-gram\nlanguage model for each dialogue act; we use a\nconditionally-trained recurrent neural network lan-\nguage model. An alternative neural approach for di-\nalogue act tagging is the combined convolutional-\nrecurrent architecture of Kalchbrenner and Blunsom\n(2013). Our modeling framework is simpler, rely-\ning on a latent variable parametrization of a purely\nrecurrent architecture.\nLanguage modeling There are an increasing\nnumber of attempts to incorporate document-level\ncontext information into language modeling. For ex-\nample, Mikolov and Zweig (2012) introduce LDA-\nstyle topics into RNN based language modeling.\nSordoni et al. (2015) use a convolutional structure\nto summarize the context from previous two utter-\nances as context vector for RNN based language\nmodeling. Our models in this paper provide a uni-\nﬁed framework to model the context and current sen-\ntence. Wang and Cho (2015) and Lin et al. (2015)\nconstruct bag-of-words representations of previous\nsentences, which are then used to inform the RNN\nlanguage model that generates the current sentence.\nThe most relevant work is the Document Context\nLanguage Model (Ji et al., 2015, DCLM); we de-\nscribe the connection to this model in§2. By adding\ndiscourse information as a latent variable, we attain\nbetter perplexity on held-out data.\nLatent variable neural networksIntroducing la-\ntent variables to a neural network model increases\nits representational capacity, which is the main goal\nof prior efforts in this space (Kingma and Welling,\n2014; Chung et al., 2015). From this perspective,\nour model with discourse relations as latent vari-\nables shares the same merit. Unlike this prior work,\nin our approach, the latent variables carry a lin-\nguistic interpretation, and are at least partially ob-\nserved. Also, these prior models employ continuous\nlatent variables, requiring complex inference tech-\nniques such as variational autoencoders (Kingma\nand Welling, 2014; Burda et al., 2016; Chung et al.,\n2015). In contrast, the discrete latent variables in our\nmodel are easy to sum and maximize over.\n339\n7 Conclusion\nWe have presented a probabilistic neural model\nover sequences of words and shallow discourse re-\nlations between adjacent sequences. This model\ncombines positive aspects of neural network ar-\nchitectures with probabilistic graphical models: it\ncan learn discriminatively-trained vector representa-\ntions, while maintaining a probabilistic representa-\ntion of the targeted linguistic element: in this case,\nshallow discourse relations. This method outper-\nforms state-of-the-art systems in two discourse rela-\ntion detection tasks, and can also be applied as a lan-\nguage model, marginalizing over discourse relations\non the test data. Future work will investigate the\npossibility of learning from partially-labeled train-\ning data, which would have at least two potential ad-\nvantages. First, it would enable the model to scale up\nto the large datasets needed for competitive language\nmodeling. Second, by training on more data, the\nresulting vector representations might support even\nmore accurate discourse relation prediction.\nAcknowledgments\nThanks to Trevor Cohn, Chris Dyer, Lingpeng Kong,\nand Quoc V . Le for helpful discussions, and to the\nanonymous reviewers for their feedback. This work\nwas supported by a Google Faculty Research award\nto the third author. It was partially performed dur-\ning the 2015 Jelinek Memorial Summer Workshop\non Speech and Language Technologies at the Uni-\nversity of Washington, Seattle, and was supported\nby Johns Hopkins University via NSF Grant No IIS\n1005411, DARPA LORELEI Contract No HR0011-\n15-2-0027, and gifts from Google, Microsoft Re-\nsearch, Amazon and Mitsubishi Electric Research\nLaboratory.\nReferences\nYoshua Bengio. 2012. Practical recommendations for\ngradient-based training of deep architectures. In Neu-\nral Networks: Tricks of the Trade, pages 437–478.\nSpringer.\nOr Biran and Kathleen McKeown. 2013. In Proceed-\nings of the Association for Computational Linguistics\n(ACL), pages 69–73, Sophia, Bulgaria.\nChlo´e Braud and Pascal Denis. 2015. Comparing word\nrepresentations for implicit discourse relation classi-\nﬁcation. In Proceedings of Empirical Methods for\nNatural Language Processing (EMNLP), pages 2201–\n2211, Lisbon, September.\nYuri Burda, Roger Grosse, and Ruslan Salakhutdinov.\n2016. Importance weighted autoencoders. In Pro-\nceedings of the International Conference on Learning\nRepresentations (ICLR).\nRich Caruana. 1997. Multitask learning. Machine learn-\ning, 28(1):41–75.\nJunyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth\nGoel, Aaron Courville, and Yoshua Bengio. 2015.\nA recurrent latent variable model for sequential data.\nIn Neural Information Processing Systems (NIPS),\nMontr´eal.\nR. Collobert, J. Weston, L. Bottou, M. Karlen,\nK. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-\nguage processing (almost) from scratch. Journal of\nMachine Learning Research, 12:2493–2537.\nJo˜ao FG de Freitas, Mahesan Niranjan, Andrew H. Gee,\nand Arnaud Doucet. Sequential monte carlo methods\nto train neural network models. Neural computation,\n12(4):955–993.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\nAdaptive subgradient methods for online learning and\nstochastic optimization. The Journal of Machine\nLearning Research, 12:2121–2159.\nChris Dyer, Miguel Ballesteros, Wang Ling, Austin\nMatthews, and Noah A. Smith. 2015. Transition-\nbased dependency parsing with stack long short-term\nmemory. In Proceedings of the Association for Com-\nputational Linguistics (ACL), pages 334–343, Beijing,\nChina.\nJenny Rose Finkel, Christopher D Manning, and An-\ndrew Y Ng. 2006. Solving the problem of cascading\nerrors: Approximate bayesian inference for linguis-\ntic annotation pipelines. In Proceedings of Empirical\nMethods for Natural Language Processing (EMNLP),\npages 618–626.\nJohn J Godfrey, Edward C Holliman, and Jane McDaniel.\n1992. Switchboard: Telephone speech corpus for re-\nsearch and development. In ICASSP, volume 1, pages\n517–520. IEEE.\nShixiang Gu, Zoubin Ghahramani, and Richard E Turner.\n2015. Neural adaptive sequential monte carlo. In Neu-\nral Information Processing Systems (NIPS), Montr´eal.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780.\nYangfeng Ji and Jacob Eisenstein. 2014. Representation\nlearning for text-level discourse parsing. In Proceed-\nings of the Association for Computational Linguistics\n(ACL), Baltimore, MD.\n340\nYangfeng Ji and Jacob Eisenstein. 2015. One vector is\nnot enough: Entity-augmented distributional seman-\ntics for discourse relations. Transactions of the Asso-\nciation for Computational Linguistics (TACL), June.\nYangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,\nand Jacob Eisenstein. 2015. Document con-\ntext language models. In International Conference\non Learning Representations, Poster Paper, volume\nabs/1511.03962.\nShaﬁq Joty, Giuseppe Carenini, and Raymond Ng. 2012.\nA novel discriminative framework for sentence-level\ndiscourse analysis. In Proceedings of Empirical Meth-\nods for Natural Language Processing (EMNLP).\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent\nconvolutional neural networks for discourse composi-\ntionality. In Proceedings of the Workshop on Continu-\nous Vector Space Models and their Compositionality,\npages 119–126, Soﬁa, Bulgaria, August. Association\nfor Computational Linguistics.\nDiederik P Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. In Proceedings of the In-\nternational Conference on Learning Representations\n(ICLR).\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Pro-\nceedings of the International Conference on Machine\nLearning (ICML).\nJiwei Li, Rumeng Li, and Eduard Hovy. 2014. Recursive\ndeep models for discourse parsing. In Proceedings of\nEmpirical Methods for Natural Language Processing\n(EMNLP).\nZiheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.\nRecognizing implicit discourse relations in the penn\ndiscourse treebank. In Proceedings of Empirical\nMethods for Natural Language Processing (EMNLP),\npages 343–351, Singapore.\nRui Lin, Shujie Liu, Muyun Yang, Mu Li, Ming Zhou,\nand Sheng Li. 2015. Hierarchical recurrent neural\nnetwork for document modeling. In Proceedings of\nEmpirical Methods for Natural Language Processing\n(EMNLP), pages 899–907, Lisbon, September.\nChristopher D. Manning. 2016. Computational linguis-\ntics and deep learning. Computational Linguistics,\n41(4).\nTomas Mikolov and Geoffrey Zweig. 2012. Context de-\npendent recurrent neural network language model. In\nProceedings of Spoken Language Technology (SLT),\npages 234–239.\nTomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan Cer-\nnock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In INTER-\nSPEECH, pages 1045–1048.\nDmitrijs Milajevs and Matthew Purver. 2014. Investi-\ngating the contribution of distributional semantic in-\nformation for dialogue act classiﬁcation. In Proceed-\nings of the 2nd Workshop on Continuous Vector Space\nModels and their Compositionality (CVSC), pages 40–\n47.\nJoonsuk Park and Claire Cardie. Improving implicit dis-\ncourse relation recognition through feature set opti-\nmization. In Proceedings of the 13th Annual Meeting\nof the Special Interest Group on Discourse and Dia-\nlogue, pages 108–112, Seoul, South Korea, July. As-\nsociation for Computational Linguistics.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2012. On the difﬁculty of training recurrent neural\nnetworks. arXiv preprint arXiv:1211.5063.\nVu Pham, Th ´eodore Bluche, Christopher Kermorvant,\nand J ´erˆome Louradour. 2014. Dropout improves re-\ncurrent neural networks for handwriting recognition.\nIn Frontiers in Handwriting Recognition (ICFHR),\n2014 14th International Conference on, pages 285–\n290. IEEE.\nEmily Pitler, Mridhula Raghupathy, Hena Mehta, Ani\nNenkova, Alan Lee, and Aravind Joshi. 2008. Eas-\nily identiﬁable discourse relations. In Proceedings of\nthe International Conference on Computational Lin-\nguistics (COLING), pages 87–90, Manchester, UK.\nEmily Pitler, Annie Louis, and Ani Nenkova. 2009. Au-\ntomatic sense prediction for implicit discourse rela-\ntions in text. In Proceedings of the Association for\nComputational Linguistics (ACL), Suntec, Singapore.\nRashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-\nsakaki, Livio Robaldo, Aravind Joshi, and Bonnie\nWebber. 2008. The Penn Discourse Treebank 2.0. In\nProceedings of LREC.\nAttapol T Rutherford and Nianwen Xue. 2014. Discov-\nering implicit discourse relations through brown clus-\nter pair representation and coreference patterns. In\nProceedings of the European Chapter of the Associ-\nation for Computational Linguistics (EACL).\nAttapol Rutherford and Nianwen Xue. 2015. Improving\nthe inference of implicit discourse relations via classi-\nfying explicit discourse connectives. pages 799–808,\nMay–June.\nKenji Sagae. 2009. Analysis of discourse structure with\nsyntactic dependencies and data-driven shift-reduce\nparsing. In Proceedings of the 11th International Con-\nference on Parsing Technologies (IWPT’09), pages\n81–84, Paris, France, October. Association for Com-\nputational Linguistics.\nRichard Socher, Alex Perelygin, Jean Y Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\n341\nIn Proceedings of Empirical Methods for Natural Lan-\nguage Processing (EMNLP), Seattle, W A.\nAlessandro Sordoni, Michel Galley, Michael Auli, Chris\nBrockett, Yangfeng Ji, Meg Mitchell, Jian-Yun Nie,\nJianfeng Gao, and Bill Dolan. 2015. A neural network\napproach to context-sensitive generation of conversa-\ntional responses. In Proceedings of the North Ameri-\ncan Chapter of the Association for Computational Lin-\nguistics (NAACL), Denver, CO, May.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. The Journal of Machine Learning\nResearch, 15(1):1929–1958.\nAndreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-\nbeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul\nTaylor, Rachel Martin, Carol Van Ess-Dykema, and\nMarie Meteer. 2000. Dialogue act modeling for\nautomatic tagging and recognition of conversational\nspeech. Computational linguistics, 26(3):339–373.\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: A simple and general method\nfor semi-supervised learning. pages 384–394.\nTian Wang and Kyunghyun Cho. 2015. Larger-\ncontext language modelling. arXiv preprint\narXiv:1511.03729.\nNick Webb, Mark Hepple, and Yorick Wilks. 2005. Di-\nalogue act classiﬁcation based on intra-utterance fea-\ntures. In Proceedings of the AAAI Workshop on Spo-\nken Language Understanding.\nBonnie Webber, Markus Egg, and Valia Kordoni. 2012.\nDiscourse structure and language technology. Journal\nof Natural Language Engineering, 1.\nJason D Williams and Steve Young. 2007. Partially ob-\nservable markov decision processes for spoken dialog\nsystems. Computer Speech & Language, 21(2):393–\n422.\nNianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi\nPrasad, Christopher Bryant, and Attapol T Rutherford.\n2015. The CoNLL-2015 shared task on shallow dis-\ncourse parsing. In Proceedings of the Conference on\nNatural Language Learning (CoNLL).\nBiao Zhang, Jinsong Su, Deyi Xiong, Yaojie Lu, Hong\nDuan, and Junfeng Yao. 2015. Shallow convolu-\ntional neural network for implicit discourse relation\nrecognition. In Proceedings of Empirical Methods for\nNatural Language Processing (EMNLP), pages 2230–\n2235, Lisbon, September.\n342",
  "topic": "Latent variable",
  "concepts": [
    {
      "name": "Latent variable",
      "score": 0.7690759897232056
    },
    {
      "name": "Computer science",
      "score": 0.7161881923675537
    },
    {
      "name": "Variable (mathematics)",
      "score": 0.587291419506073
    },
    {
      "name": "Artificial intelligence",
      "score": 0.497388631105423
    },
    {
      "name": "Artificial neural network",
      "score": 0.48070937395095825
    },
    {
      "name": "Natural language processing",
      "score": 0.4631898105144501
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4273523688316345
    },
    {
      "name": "Linguistics",
      "score": 0.3204490542411804
    },
    {
      "name": "Mathematics",
      "score": 0.11265462636947632
    },
    {
      "name": "Philosophy",
      "score": 0.0641060471534729
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    }
  ]
}