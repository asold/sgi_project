{
    "title": "Generalization potential of large language models",
    "url": "https://openalex.org/W4405463625",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2793470391",
            "name": "Mikhail Budnikov",
            "affiliations": [
                "Constructor University"
            ]
        },
        {
            "id": "https://openalex.org/A1980704684",
            "name": "Anna Bykova",
            "affiliations": [
                "National Research University Higher School of Economics"
            ]
        },
        {
            "id": "https://openalex.org/A1601369361",
            "name": "Ivan P. Yamshchikov",
            "affiliations": [
                "Constructor University"
            ]
        },
        {
            "id": "https://openalex.org/A2793470391",
            "name": "Mikhail Budnikov",
            "affiliations": [
                "Constructor University"
            ]
        },
        {
            "id": "https://openalex.org/A1980704684",
            "name": "Anna Bykova",
            "affiliations": [
                "National Research University Higher School of Economics"
            ]
        },
        {
            "id": "https://openalex.org/A1601369361",
            "name": "Ivan P. Yamshchikov",
            "affiliations": [
                "Constructor University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1677182931",
        "https://openalex.org/W4404906291",
        "https://openalex.org/W3198659451",
        "https://openalex.org/W3104570641",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W4283794074",
        "https://openalex.org/W3152698349",
        "https://openalex.org/W3201490875",
        "https://openalex.org/W6603884005",
        "https://openalex.org/W6601897980",
        "https://openalex.org/W6611608239",
        "https://openalex.org/W6603433950",
        "https://openalex.org/W6601080257",
        "https://openalex.org/W4402671810",
        "https://openalex.org/W6606084162",
        "https://openalex.org/W2996044589",
        "https://openalex.org/W6602430550",
        "https://openalex.org/W6603717528",
        "https://openalex.org/W6600882715",
        "https://openalex.org/W4385572845",
        "https://openalex.org/W6725704494",
        "https://openalex.org/W2912811302",
        "https://openalex.org/W3207523779",
        "https://openalex.org/W6605898172",
        "https://openalex.org/W2100483895",
        "https://openalex.org/W2167267841",
        "https://openalex.org/W2161278885",
        "https://openalex.org/W3008311476",
        "https://openalex.org/W6601955380",
        "https://openalex.org/W2116341502",
        "https://openalex.org/W4237591687",
        "https://openalex.org/W6600234944",
        "https://openalex.org/W6601211009",
        "https://openalex.org/W6635305009",
        "https://openalex.org/W6743198006",
        "https://openalex.org/W3158330599",
        "https://openalex.org/W6635717201",
        "https://openalex.org/W6601731210",
        "https://openalex.org/W2552194003",
        "https://openalex.org/W6681105374",
        "https://openalex.org/W4285232587",
        "https://openalex.org/W2086161653",
        "https://openalex.org/W4385574336",
        "https://openalex.org/W6600120041",
        "https://openalex.org/W6605118383",
        "https://openalex.org/W4389523706",
        "https://openalex.org/W6600225014",
        "https://openalex.org/W4385572634",
        "https://openalex.org/W2766447205",
        "https://openalex.org/W6850207665",
        "https://openalex.org/W6814003322",
        "https://openalex.org/W3102259594",
        "https://openalex.org/W4285233740",
        "https://openalex.org/W6630459717",
        "https://openalex.org/W3160359250",
        "https://openalex.org/W2984256198",
        "https://openalex.org/W2970019270",
        "https://openalex.org/W4312273141",
        "https://openalex.org/W6822005569",
        "https://openalex.org/W4393065402",
        "https://openalex.org/W3210277894",
        "https://openalex.org/W3186138538",
        "https://openalex.org/W6600798642",
        "https://openalex.org/W6600212061",
        "https://openalex.org/W6602503091",
        "https://openalex.org/W6600688380",
        "https://openalex.org/W6600577311",
        "https://openalex.org/W6600696048",
        "https://openalex.org/W4393160302",
        "https://openalex.org/W6635750829",
        "https://openalex.org/W4393160747"
    ],
    "abstract": "Abstract The rise of deep learning techniques and especially the advent of large language models (LLMs) intensified the discussions around possibilities that artificial intelligence with higher generalization capability entails. The range of opinions on the capabilities of LLMs is extremely broad: from equating language models with stochastic parrots to stating that they are already conscious. This paper represents an attempt to review LLM landscape in the context of their generalization capacity as an information theoretic property of those complex systems. We discuss the suggested theoretical explanations for generalization in LLMs and highlight possible mechanisms responsible for these generalization properties. Through an examination of existing literature and theoretical frameworks, we endeavor to provide insights into the mechanisms driving the generalization capacity of LLMs, thus contributing to a deeper understanding of their capabilities and limitations in natural language processing tasks.",
    "full_text": "REVIEW\nGeneralization potential of large language models\nMikhail Budnikov 1 • Anna Bykova 2 • Ivan P. Yamshchikov1,3\nReceived: 29 April 2024 / Accepted: 20 November 2024 / Published online: 17 December 2024\n/C211 The Author(s) 2024\nAbstract\nThe rise of deep learning techniques and especially the advent of large language models (LLMs) intensiﬁed the discussions\naround possibilities that artiﬁcial intelligence with higher generalization capability entails. The range of opinions on the\ncapabilities of LLMs is extremely broad: from equating language models with stochastic parrots to stating that they are\nalready conscious. This paper represents an attempt to review LLM landscape in the context of their generalization\ncapacity as an information theoretic property of those complex systems. We discuss the suggested theoretical explanations\nfor generalization in LLMs and highlight possible mechanisms responsible for these generalization properties. Through an\nexamination of existing literature and theoretical frameworks, we endeavor to provide insights into the mechanisms driving\nthe generalization capacity of LLMs, thus contributing to a deeper understanding of their capabilities and limitations in\nnatural language processing tasks.\nKeywords Large language models /C1 Generalization /C1 Semantic information\n1 Introduction\nMachine learning systems are experiencing a rapid surge in\npower and capability. These systems, driven by data-cen-\ntric methodologies, exhibit the capacity to model increas-\ningly intricate domains, whether it is an image classiﬁer [1]\nor a language model [ 2], showcasing performance levels\nthat surpass human abilities on designated benchmarks.\nSuch advancements owe their existence to extensive scal-\ning, encompassing both the augmentation of the model’s\nparameters and the ampliﬁcation of training data volumes.\nHowever, the growth in data availability faces a critical\nbottleneck, particularly concerning unlabeled data, which\nis forecasted to become scarce by the end of this decade\n[3]. This scarcity poses a formidable challenge, particularly\nfor tasks necessitating human supervision or those that are\nseldom encountered in web data sources. Mere aggregation\nof diverse datasets no longer sufﬁces to propel machine\nlearning systems to greater heights; rather, it necessitates\nthe development of methodologies capable of efﬁcient\ngeneralization. Furthermore, as researchers endeavor to\nscale AI to superhuman capacities safely, it becomes\nimperative to comprehend the precise mechanisms through\nwhich these systems generalize. This understanding is\nessential to mitigate the emergence of unforeseen capa-\nbilities or undesirable outcomes [ 4]. Hence, advancing\nbeyond the era of brute-force data accumulation, the focus\nshifts toward reﬁning the algorithms’ ability to generalize\nacross varied domains while ensuring their safety and\nreliability in real-world applications.\nIn this work we undertake a comprehensive examination\nof the phenomenon of generalization and explore strategies\nfor its control. Our primary focus centers on natural lan-\nguage processing (NLP), with particular emphasis on large\nlanguage models (LLMs). LLMs currently stand as the\napex of universal machine learning systems, rendering\nthem a ﬁtting subject for analysis. Nonetheless, many of\nthe concepts and methodologies discussed herein hold\nrelevance across the broader spectrum of machine learning.\nTo provide a lucid exposition of the subject matter, we\norganize our discussion around the various stages of the\nmachine learning pipeline. By structuring our analysis in\nthis manner, we aim to offer a systematic understanding of\nthe factors inﬂuencing generalization. At the conclusion of\n& Ivan P. Yamshchikov\nivan.yamshchikov@thws.de\n1 Constructor University, 28759 Bremen, Germany\n2 LEYA, Higher School of Economics, St. Petersburg 28759,\nRussia\n3 THWS, CAIRO, 97082 Wu¨ rzburg, Germany\n123\nNeural Computing and Applications (2025) 37:1973–1997\nhttps://doi.org/10.1007/s00521-024-10827-6(0123456789().,-volV)(0123456789().,- volV)\neach section, we list key insights from the literature under\nexamination.\nOur work encompasses a wide array of questions per-\ntinent to the study of generalization:\n• Data\n– What qualities of data are important for pre-\ntraining?\n– Which datasets lead to better out-of-distribution\ngeneralization?\n– How the data can be used to control the inductive\nbias of the model?\n• Model architecture\n– What properties of a trained model are correlated\nwith better generalization?\n– How can model architecture explain these\nproperties?\n– How can model architecture be used to control the\ninductive bias?\n• Training process\n– Which optimizers lead to better generalization and\nhow are they related to each other?\n– Can one combine or mitigate inductive biases of\ndifferent models by changing the training process?\n• Inference\n– What kinds of learning and generalization can\nhappen at inference time?\n– How can one amplify model’s capabilities by\naugmenting it with tools, memory and prompting\nschemes?\n2 Methodology\nWe have formed this review based on the different\nhypotheses about what inﬂuences generalization, especially\nin the NLP tasks. Our review is organized around the\nvarious stages of the machine learning pipeline that inﬂu-\nences generalization results: data, model architecture,\ntraining process and inference. We tried to make the most\ndetailed review within of each stages.\n3 Data\nMachine learning algorithms are fundamentally designed\nto acquire knowledge from data but the precise nature of\nthis learning process often remains not clear. In our work,\nwe explore various methodologies employed to shape the\nlearning process and the resultant impact on the acquired\nknowledge.\nWe delve into a multitude of approaches, including:\n• Data Selection: High-quality datasets for a model with\nrelevant and informative examples, thus steering its\nlearning trajectory toward desired outcomes.\n• Demonstrations and Instruction Following: Leveraging\ndemonstrations and explicit instructions to guide the\nlearning process, facilitating the acquisition of speciﬁc\nproblem-solving behaviors or task-oriented strategies.\n• Data Generation from Models or Algorithms: Using\ndata generation techniques wherein data are synthesized\nfrom existing models or algorithms, thereby expanding\nthe diversity and richness of the training dataset.\n• Using Data from Different Tasks or Modalities: Data\nsourced from disparate tasks or modalities can foster a\nmore comprehensive understanding of underlying pat-\nterns and relationships, leading to enhanced model\ngeneralization.\n• Integration of Random Data: Introducing elements of\nrandomness into the training dataset, thereby encour-\naging adaptability and resilience in the model’s learning\nprocess.\nBy scrutinizing these diverse methodologies, we aim to\nelucidate the multifaceted ways in which the information\nused for training can shape the ﬁnal model.\n3.1 Irrelevant data\nPre-training language models entail transferring linguistic\nknowledge collected from a data-rich task to a downstream\ntask [5]. This process serves as a foundational step in the\ndevelopment of robust and high-performing models.\nHowever, the ramiﬁcations of pre-training extend far\nbeyond this initial transfer of knowledge, encompassing a\nmyriad of nuanced effects that warrant closer examination.\nPapadimitriou and Jurafsky [ 6] demonstrate an intrigu-\ning ﬁnding regarding pre-training long short-term memory\n(LSTM) networks [ 7] on structured, non-linguistic data\nsuch as MIDI music, Java code, or nested parentheses.\nRemarkably, this pre-training approach leads to a reduction\nin perplexity when the LSTM model is tested on Spanish\ntext. This observation underscores the versatility of pre-\ntraining methodologies and highlights the potential for\nleveraging diverse datasets to enhance model performance\nacross linguistic domains.\nMoreover, the efﬁcacy of pre-training extends beyond\nlanguage-speciﬁc tasks. Lu et al. [ 8] showcase this phe-\nnomenon by achieving performance levels comparable to\ntraining from scratch across various modalities. Their\napproach involves ﬁne-tuning only the input and output\nembeddings of a pre-trained Generative Pre-trained\n1974 Neural Computing and Applications (2025) 37:1973–1997\n123\nTransformer 2 (GPT-2) model [ 9]. This ﬁnding not only\nunderscores the transferability of pre-trained models but\nalso underscores the importance of efﬁcient ﬁne-tuning\nstrategies in optimizing model performance across diverse\ndomains.\nSinha et al. [ 10] conducted a notable investigation\nwherein they explored the impact of removing all infor-\nmation pertaining to word order during the pre-training\nphase of a model. Their ﬁndings indicate that despite the\nabsence of word order information during pre-training, the\nﬁnal performance of the model is minimally affected,\nprovided that a ﬁne-tuning phase has the correct word\norder.\nKrishna et al. [ 11] conducted a study in which they\nsampled the training data from a completely artiﬁcial lan-\nguage, comprising random n-grams. Their investigation\naimed to explore the impact of pre-training objectives that\nnecessitate processing such information, such as tasks\ninvolving copying sentences in the correct order. Despite\nthe synthetic nature of the training data, authors observed\nthat pre-training objectives requiring the processing of this\nartiﬁcial language still led to performance improvements in\nthe model on summarization tasks when compared to a\nrandomly initialized version.\nMaennel et al. [ 12] conducted a study wherein they\ntrained neural networks with entirely random labels,\nrevealing an unexpected phenomenon: in certain cases, this\ntraining approach still facilitated improved performance\nduring subsequent ﬁne-tuning stages. Investigating the\nunderlying mechanisms of this intriguing effect, they dis-\ncovered that the ﬁrst layer of the network exhibited\nremarkable adaptability to the data. Speciﬁcally, authors\nfound that the weights connecting the inputs to randomly\nselected neurons in the ﬁrst hidden layer behaved as ran-\ndom variables during pre-training. Remarkably, they\nobserved that, over the course of pre-training, the eigen-\nvectors of the covariance matrix associated with these\nweights aligned with those of the covariance matrix\nderived from the data itself. This alignment indicated that\nthe ﬁrst layer of the network was effectively adapting to\ncapture statistical properties of the input data, despite the\nrandom nature of the labels. Building upon this discovery,\nChowers and Weiss [ 13] further elucidated the behavior of\nthe ﬁrst layer in neural networks. Their study revealed that,\nduring pre-training, the ﬁrst layer progressively converged\ntoward a whitening transformation for the training dataset.\nThis transformation involves decorrelating and normalizing\nthe input features, effectively enhancing the network’s\nability to extract meaningful patterns from the data.\nResearch suggests that pre-training guides optimization\ntoward a ﬂatter basin within the loss landscape, a phe-\nnomenon with signiﬁcant implications for model perfor-\nmance and adaptability. Mehta et al. [14] demonstrated this\nﬁnding through empirical analysis and proposed it as a key\nfactor contributing to the reduced susceptibility of pre-\ntrained models to catastrophic forgetting during ﬁne-\ntuning.\nFurthermore, Neyshabur et al. [ 15] observed a similar\nphenomenon and provided additional insights into its\nimplications. They demonstrated that models ﬁne-tuned\nfrom the same pre-trained checkpoint tend to remain within\nthe same basin of the loss landscape.\n3.2 Internal representations\nBeyond their proﬁciency in language processing, contem-\nporary language models have demonstrated a remarkable\ncapacity to learn and encode higher-level information\nspanning diverse domains. Gurnee and Tegmark [ 16]\nrecently showed the emergence of detailed world models\nwithin pre-trained language models. These world models\nare presented in terms of both time and space, allowing for\nthe extraction of nuanced information regarding the timing\nand location of events. Remarkably, through linear pro-\njection from activations in layer 50 corresponding to\nspeciﬁc tokens, it becomes feasible to discern the temporal\nand spatial contexts associated with various events encoded\nwithin the model. Moreover, Li et al. [ 17] conducted a\nstudy wherein they trained a language model to predict\nmoves in a simple board game, Othello. They discovered\nthat the model inherently represented the state of the\nOthello board internally. This ﬁnding underscores the\nmodel’s capacity to capture complex relational structures\nand dynamic states within diverse problem domains, tran-\nscending its original training objectives in natural language\nprocessing. Furthermore, Jin and Rinard [ 18] provide\ncompelling evidence that language models trained on code\nexhibit sophisticated representations of program states.\nTheir study reveals a strong correlation between the quality\nof these representations and the model’s proﬁciency in\ngenerating correct programs.\nAnother discovery in the ﬁeld of language models is\ntheir innate tendency to learn linear representations, even\nin the absence of explicit direction to do so. This phe-\nnomenon mirrors the insights offered by Word2vec [ 19],\nwhich demonstrated that simple vector arithmetic applied\nto word embeddings yields semantically meaningful\nresults. Building upon this foundational work, recent\nstudies have further illuminated the linear nature of rep-\nresentations within advanced language models. For\ninstance, Turner et al. [ 20] revealed that the vector arith-\nmetic manipulation also gives insightful results when\napplied to activations within the GPT-2 model. Moreover,\nNanda [ 21] delved into game-based language models,\ndemonstrating that Othello-GPT exhibits a linear world\nmodel. Further corroborating these ﬁndings, Jin and Rinard\nNeural Computing and Applications (2025) 37:1973–1997 1975\n123\n[18] provided compelling evidence that representations\nwithin language models trained on code also exhibit\nlinearity.\n3.3 High-quality data\nState-of-the-art language models represent a pinnacle of\nnatural language understanding, fueled by training on vast\ncorpora of textual data. However, the sheer scale of data\nrequired for training these models comes with signiﬁcant\ncosts and challenges. As the volume of available data\napproaches its ﬁnite limits, the feasibility and scalability of\ntraining such models become increasingly constrained,\npotentially posing a bottleneck to further advancements in\nNLP research and development.\nIn the context of applications, a critical inquiry revolves\naround determining the optimal composition and volume of\ndata necessary for endowing language models with speciﬁc\ncapabilities. For instance, understanding the minimum data\nrequirements for achieving competencies such as coherent\nEnglish generation or zero-shot reasoning is pivotal for\nstreamlining model training and deployment pipelines.\nThis entails delving into questions regarding the diversity,\nquality, and relevance of training data, as well as exploring\nstrategies for dataset curation and augmentation.\nEldan and Li [22] conducted a study that underscores the\npotential of training language models on stories with highly\nrestricted vocabularies. Their ﬁndings demonstrate that\neven with stringent constraints on vocabulary size, it is\nfeasible to develop language models with less than 10\nmillion parameters, capable of generating coherent and\ngrammatically correct stories. This research clariﬁes on the\nadaptability of language models to operate effectively\nwithin constrained linguistic environments.\nBuilding upon prior research, Gunasekar et al. [ 23] have\nextended the exploration into dataset selection strategies,\nparticularly within the programming domain. Their study\nreveals that through data with maximal educational value,\nit is feasible to achieve substantial reductions in the size of\nlanguage models for code-related tasks. Similarly, Li et al.\n[24] have investigated the impact of dataset selection\nstrategies on models designed for commonsense reasoning.\nRecently, Surkov and Yamshchikov [ 25] introduced\n‘‘Vygotsky distance’’, a tool for benchmark similarity\nassessment that could potentially structure further choices\nof training data and benchmarks to maximize generaliza-\ntion potential of the models.\n3.4 Learning inductive bias from data\nInductive biases play a crucial role in shaping the learning\nbehavior and generalization capabilities of machine learn-\ning models. These biases can be effectively instilled into\nmodels through pre-training on datasets that exemplify\nspeciﬁc properties or tasks. Several recent studies have\nshowcased the effectiveness of leveraging pre-training\nmodels with inductive biases across various domains. For\ninstance, McCoy et al. [ 26] adopted a model-agnostic\nmeta-learning approach [ 27] to identify and incorporate\ninductive biases essential for rapid language acquisition.\nSimilarly, Wu et al. [ 28] devised synthetic datasets that\nnecessitate deductive, inductive, and abductive reasoning\nskills, reﬂecting fundamental principles of mathematical\nreasoning. Meanwhile, Lindemann et al. [ 29] explored the\nutility of pre-training models to simulate ﬁnite state\ntransducers (FSTs) based on their descriptions. By pre-\ntraining models on tasks involving FST emulation, they\ninjected speciﬁc inductive biases into the models, enabling\nthem to better generalize to NLP tasks characterized by\nsimilar structural properties.\nMukherjee et al. [ 30] have introduced an approach to\nknowledge distillation adapted speciﬁcally for language\nmodels. In this methodology, a smaller model is trained on\nthe detailed explanations generated by a larger, more\ncomplex model. By leveraging these rich training data\nderived from the explanations of the larger model, the\nsmaller model is equipped to achieve enhanced perfor-\nmance, leading to improved generalization across various\ntasks. This transfer of knowledge can be compared with\ntransferring a superior inductive bias from the teacher\nmodel to the student model.\n3.5 Demonstrations of instruction following\nand problem solving\nA predominant trend within the domain of language\nmodels centers around the paradigm of pre-training fol-\nlowed by ﬁne-tuning, particularly adapted toward instruc-\ntion following tasks [ 31]. For instance, Wei et al. [ 32]\nshowed that ﬁne-tuning language models on instruction\nfollowing demonstrations gives better zero-shot perfor-\nmance than models of comparable size trained solely\nthrough zero-shot or few-shot learning approaches. More-\nover, Ouyang et al. [ 33] introduced reinforcement learning\ntechniques alongside supervised ﬁne-tuning, showcasing\nthat models trained using this hybrid approach exhibit\nimproved generalization to previously unseen instructions.\nFurthermore, Ye et al. [ 34] explored a training methodol-\nogy wherein models are trained to infer instructions based\non task inputs and correct answers. This instruction\nguessing approach has been shown to signiﬁcantly enhance\ngeneralization performance, particularly for tasks featuring\nunseen answer formats.\nChen et al. [ 35] highlight a connection between agents\nand ﬁne-tuning within the ﬁeld of language models. Their\nresearch clariﬁes on the potential beneﬁts of ﬁne-tuning the\n1976 Neural Computing and Applications (2025) 37:1973–1997\n123\nunderlying language model using demonstrations of agent\nbehavior across diverse tasks and various prompting\nmethods. By ﬁne-tuning language models on such agent-\nbased demonstrations, authors observed notable improve-\nments in both performance and generalization of the\nresulting agents. In Sect. 6, we will discuss language\nagents in more details, exploring their role and capabilities.\nWang et al. [ 36] conducted a comprehensive study on\nthe performance of pre-trained models trained on instruc-\ntion-following datasets, revealing insights into the rela-\ntionship between model performance, task diversity, model\nsize, and data quantity. Their ﬁndings indicate that the\nperformance of pre-trained models exhibits a log-linear\nincrease with the number of tasks and the size of the model\narchitecture. The authors observed that model performance\ndoes not signiﬁcantly depend on the quantity of data\navailable for each individual task. Moreover, Chan et al.\n[37] highlighted additional crucial features of data distri-\nbution that impact in-context learning, particularly in sce-\nnarios where models are required to comprehend and\ngenerate text within speciﬁc contexts.\n3.6 Summary\nSimple structural patterns, such as matching tokens in\nbracket sequences, word co-occurrence statistics are\nimportant elements that signiﬁcantly inﬂuence the perfor-\nmance of pre-trained language models. However, beyond\nmerely capturing superﬁcial statistics from data, large\nlanguage models also develop internal representations that\nreﬂect a broader understanding of the world, often char-\nacterized by linear structures.\nEfforts to optimize the efﬁciency and capabilities of\nlanguage models often involve training on datasets that\noffer greater interpretability and educational value. Fur-\nthermore, the distribution of model capabilities can be\nadjusted by training on datasets that align with desired\ninductive biases. For instance, datasets featuring exercises\non reasoning primitives can promote the development of\nmodels with enhanced reasoning abilities.\nTo enhance the effectiveness of language models in few-\nshot and zero-shot learning scenarios, incorporating train-\ning or ﬁne-tuning with demonstrations of instruction fol-\nlowing is crucial. Similarly, to bolster the model’s\nproﬁciency in complex reasoning tasks, datasets should\nencompass applications of problem-solving strategies,\nenabling models to acquire nuanced reasoning skills.\nAnother noteworthy observation is that in-context\nlearning tends to accelerate when it signiﬁcantly facilitates\ntask-solving efﬁciency. This phenomenon becomes more\npronounced with an increasing number of tasks, as the\nlearning process becomes more conducive to meta-learn-\ning. With a greater diversity of tasks, the model learns not\nonly how to perform individual tasks but also how to\neffectively generalize across them, thereby enhancing its\noverall learning efﬁciency and adaptability.\n4 Model architecture\nTo understand how model architecture affects generaliza-\ntion we ﬁrst review some properties that are usually found\nin models that demonstrate comparatively high general-\nization potential or that can provide lower bounds for it,\nand then we discuss explanation proposed to explain why\nmodels have those properties and achieve superior gener-\nalization. Finally, we review architecture modiﬁcations,\nmostly speciﬁc to language models, which help model to\ngeneralize better in certain directions.\n4.1 Factors related to generalization\nOne common setting for studying generalization is the i.i.d.\n(independent and identically distributed) case, where both\ntraining and testing samples are drawn from the same\ndistribution and are independent of each other. This setup\nenables the quantiﬁcation of generalization and the estab-\nlishment of lower bounds for it. Numerous properties have\nbeen identiﬁed to provide such lower bounds, with many of\nthem focusing on describing model complexity in various\nways:\n• Compressibility: Arora et al. [ 38], Lotﬁ et al. [ 39]\n• Weight norm: Bartlett [ 40], Wei and Ma [ 41],\nKawaguchi et al. [ 42]\n• Flatness: Hochreiter and Schmidhuber [ 43], Bahri et al.\n[44], Orvieto et al. [ 45]\n• Algorithmic stability: Chatterjee and Zielinski [ 46],\nBousquet and Elisseeff [ 47]\nWhen deploying a model in real-world scenarios,\nencountering a distribution shift is almost inevitable. In\npractice, training data can only fully represent the actual\ndistribution of inputs under the most straightforward cir-\ncumstances. Thus, a more pertinent, albeit challenging,\nconsideration is out-of-distribution generalization. As elu-\ncidated by Wolpert [48] in no free lunch theorems, without\nadditional assumptions, all learning algorithms perform\nequally poorly. Even widely employed techniques such as\ncross-validation will lose to anti-cross-validation in half of\nthe cases. This underscores the signiﬁcance of addressing\nout-of-distribution scenarios and highlights the limitations\nof relying solely on traditional methodologies in complex\nreal-world contexts.\nIf one assumes that simpler hypotheses are preferable to\ncomplex ones, it lays the foundation for deriving a general\nmethod of inductive inference known as Solomonoff\nNeural Computing and Applications (2025) 37:1973–1997 1977\n123\ninduction [49]. This method involves averaging predictions\nfrom all possible models, weighted by their Kolmogorov\ncomplexity, resulting in a ﬁnite number of errors while\npredicting any computable sequence. Goldblum et al. [ 50]\nhighlight that many real-world data sources exhibit a bias\ntoward simplicity, meaning they can be effectively com-\npressed compared to the uniform distribution suggested by\nthe no free lunch theorems. An interesting observation is\nthat modern deep learning models, including large lan-\nguage models, whether trained or randomly initialized, also\ndemonstrate a propensity for simpler solutions [ 50, 51].\nThis alignment with simplicity suggests a fundamental\nprinciple guiding both human and artiﬁcial intelligence in\nlearning and problem-solving tasks.\n4.2 Role of depth\nOne advantage of deep models lies in their enhanced\nexpressiveness. According to Cover’s theorem [52], simply\nprojecting data into a higher-dimensional space with non-\nlinear transformations is likely to render it linearly sepa-\nrable. If this transformation occurs before the\ndimensionality surpasses the number of samples, the\nresultant linear separation gives a simple hypothesis with\npromising potential for generalization. From a theoretical\nstandpoint, Eldan and Shamir [ 53] demonstrate that adding\nan extra layer to a neural network can exponentially reduce\nthe number of parameters for certain functions. Empiri-\ncally, deep randomized neural networks [ 54] conﬁrm this\nnotion, indicating that the addition of more non-linear\nlayers, even without training, can offer utility in improving\nmodel performance.\nMoreover, Hinton et al. [ 55] argues that neural net-\nworks’ ability to learn distributed representations enables\nthem to capture relevant information in a more efﬁcient\nand organic manner. As the network learns various con-\ncepts in one layer, it can then use arbitrary linear combi-\nnations of these concepts in subsequent layers, thereby\nrepresenting compositions of the original concepts. Con-\nsequently, in datasets exhibiting hierarchical structures,\nwhere higher-level concepts are formed by combinations of\nconcepts from lower levels, deep models are better suited\nto represent such data.\nSome model architectures possess the capability to\ninternally execute algorithms, with each non-linear layer\nserving as a computation step. For instance, Von Oswald\net al. [ 56] demonstrate that in-context learning in Trans-\nformers bears resemblance to gradient descent. Xu et al.\n[57] explore the alignment of certain neural architectures\nwith common classes of algorithms and illustrate that better\nalignment correlates with improved sample efﬁciency and\ngeneralization. For instance, graph neural networks\n(GNNs) [ 58] excel at learning algorithms resembling\ndynamic programming [ 59], they struggle with problems\nrequiring exhaustive search. These latter problems can be\nswiftly learned by more versatile architectures capable of\niterating over all subsets. If there existed an architecture\ncapable of learning arbitrary algorithms from data without\na strong bias toward speciﬁc algorithmic classes, it could\nserve as a robust approximation of Solomonoff’s induction.\nSuch an architecture might offer superior generalization\nperformance compared to less algorithmically oriented\nmodels.\nChatterjee and Zielinski [46] highlight a phenomenon in\ndeep architectures trained with gradient methods: positive\nfeedback loops can emerge due to the paths in the network\nhaving gradients proportional to the product of weights\nalong the path. Consequently, paths through edges with\nalready high weights receive higher gradients and thus\ngrow even faster. This ampliﬁcation effect of depth serves\nto magnify gradient directions that frequently occur in the\ndataset. The authors argue that such common gradient\ndirections play a crucial role in generalization and empir-\nically demonstrate that robust estimators of gradients, such\nas the mean of clipped gradients or gradient median, lead to\nimproved generalization.\n4.3 Model architecture for inductive bias\nWhile data-driven methods for controlling inductive bias\nshow promise, certain types of bias may be more effec-\ntively managed by explicitly altering the model\narchitecture.\nAn effective change known to enhance generalization is\nthe incorporation of a world model. For instance, Efﬁ-\ncientZero [60] demonstrates the utility of explicitly learn-\ning a model of the environment to achieve sample-efﬁcient\nlearning across various games. By leveraging this world\nmodel for planning, EfﬁcientZero exhibits impressive\nperformance. Similarly, Li et al. [ 61] introduce a language\nmodel augmented with a value function, enabling control\nover properties that necessitate planning, such as generated\nsequence length or the probability of the prompt given the\ngeneration. This approach illustrates the potential of inte-\ngrating world models to enhance the capabilities and\nﬂexibility of language models.\nIn addition to a world model, incorporating a bias for\noptimization can also prove beneﬁcial. For instance, Amos\nand Kolter [ 62] introduce a quadratic programming solver\nas one of the layers in their model, showcasing its ability to\nlearn Sudoku solely from observing solution examples\nwithout explicit rule instructions. Similarly, von Oswald\net al. [ 63] discover that Transformers can internalize gra-\ndient descent procedures without architectural modiﬁca-\ntions. They propose augmenting models with layers\ncapable of solving least squares regression, further\n1978 Neural Computing and Applications (2025) 37:1973–1997\n123\nillustrating the potential for integrating optimization biases\nto enhance model capabilities.\nCertain problems may demand more computation than\ncan be accommodated within a single forward pass of the\nmodel. In such scenarios, having an adaptive amount of\ncomputation at each step proves advantageous. Graves [64]\naddress this challenge by allowing for multiple intermedi-\nate predictions before reaching the ﬁnal output. Banino\net al. [ 65] extend a similar approach to Transformers,\nproposing a differentiable training method to facilitate\nadaptability. Alternatively, Dehghani et al. [ 66] avoid\nmultiple forward passes and instead apply the same set of\nlayers iteratively, thereby enabling the model’s depth to\nadapt dynamically.\nCertain types of information are best memorized by the\nmodel, and in such cases, the addition of an explicit\nexternal memory can be beneﬁcial. Khandelwal et al. [ 67]\npropose linearly interpolating the outputs of a large lan-\nguage model with predictions from k-nearest neighbors,\naiding in the prediction of rare patterns such as factual\nknowledge. Guu et al. [ 68] take a different approach by\nfeeding retrieved documents to another neural network,\nenabling a more ﬂexible combination with model predic-\ntions. Borgeaud et al. [ 69] demonstrate that this approach\nremains effective even at the scale of trillions of tokens,\nallowing for equivalent performance with a 25x smaller\nmodel. Wu et al. [ 70] incorporate kNN lookup at the layer\nlevel, seamlessly integrating retrieval and attention mech-\nanisms. They also advocate for writing facts to external\nmemory during inference, enabling the model to access\ndeﬁnitions of functions or other relevant mathematical\nobjects. Sukhbaatar et al. [ 71] replace MLP blocks in\nTransformers with extra tokens added to self-attention,\nsimplifying the architecture and paving the way for the\napplication of additional attention-related techniques to\nevery layer in such models.\nH3 [72] serves as another compelling demonstration of\ndeliberate architecture design. The authors meticulously\nidentify the speciﬁc capabilities that render Transformers\nsuperior to state space models and subsequently craft a new\nlayer designed to precisely enhance these abilities. The\nresulting model surpasses Transformer baselines in zero-\nand few-shot performance, showcasing the efﬁcacy of this\ntargeted design approach.\nAnother research direction emphasizes architectures that\npromote sparsity. The rationale behind this approach lies in\nthe notion that if different components of the model operate\nindependently, alterations affecting one component will not\nimpact others, resulting in increased robustness to\ndistribution shifts and enhanced interpretability. To this\nend, Elhage et al. [ 73] introduce the SoLU activation\nfunction, deﬁned as f ðxÞ¼ x /C1 softmaxðxÞ, which leads to\nmore efﬁcient activations in MLP layers within Trans-\nformers. Additionally, Lamb et al. [ 74] partition the\nparameters and representations into groups that communi-\ncate solely through attention mechanisms, fostering com-\npetition among these groups. They demonstrate that\nimplementing this approach for all Transformer layers\nexcept the ﬁrst and last ones leads performance\nimprovements.\nExtensive research has been dedicated to designing\nneural architectures capable of learning and executing\nalgorithms. The neural turing machine [ 75], for instance,\nintegrates a recursive network with an external memory in\na differentiable manner, resulting in qualitatively improved\nperformance on algorithmic tasks. Velicˇkovic´ and Blundell\n[76] propose leveraging graph neural networks [ 58\n], as\ntheir architecture is inherently well-suited to operations\ncharacteristic of dynamic programming Bellman [ 59],\nallowing many algorithms to be formulated in this\nframework.\nIn general, if there exists a known symmetry within the\ndata that can be represented as a matrix group, Finzi et al.\n[77] demonstrate a method to construct a layer that exhibits\nequivariance to it. Furthermore, Finzi et al. [ 78] illustrate\nthat for a model architecture incorporating speciﬁc biases,\nit is possible to construct a ﬂexible yet still biased model by\nensembling the biased model with a more generalized one,\nsubsequently regularizing the weight assigned to the gen-\neral model. Similarly, Goldblum et al. [ 50] advocate for\nensembling models with diverse architectures and applying\nappropriate regularization as a ﬂexible approach to learning\nwhich bias to apply based on the data.\n4.4 Summary\nSimple models tend to exhibit relatively higher general-\nization capacity, as evidenced by both empirical observa-\ntions and theoretical estimates. Simplicity, in this context,\ncan be construed in terms of compressibility (or low Kol-\nmogorov complexity), small weight norms, ﬂatness of the\nloss landscape, or minimal reliance on individual training\nexamples.\nDepth plays a crucial role in this phenomenon, with\nseveral explanations of its impact. Firstly, the incorporation\nof nonlinearities enhances model expressiveness, often\nexponentially, thereby necessitating fewer parameters and\nreducing complexity relative to the training data. Secondly,\nNeural Computing and Applications (2025) 37:1973–1997 1979\n123\ndeep architectures naturally capture hierarchical or com-\nputational structures inherent in the data. Finally, when\ntrained via gradient descent, deep models naturally priori-\ntize common cases in terms of gradient directions while\ndisregarding outliers.\n5 Training process\nThis section is by far the most difﬁcult one to write since,\nunfortunately, there are a lot of choices that are made\nduring training of large language models, yet most of the\npapers on the topic disclose only a fraction of those deci-\nsions. Initially, we delve into works that analyze the indi-\nvidual inﬂuence of each factor. Subsequently, we explore\nthree major categories of modiﬁcations to mainstream\ngradient-based optimizers that are adapted to enhance\ngeneralization. These include methods explicitly targeting\nsolutions within ﬂat basins of the loss landscape, parameter\naveraging across multiple models, and training models to\nexhibit resilience against input perturbations.\nAn appealing property of human generalization lies in\nits capacity to generate novel insights even without expo-\nsure to new data, e.g., by theoretical research. As the\navailability of training data gradually emerges as a bot-\ntleneck, methodologies capable of extracting more\nknowledge from a ﬁxed dataset through bootstrapping\nmechanisms assume greater signiﬁcance.\nThe outcomes of model training endeavors, encom-\npassing generalization performance, are signiﬁcantly con-\ntingent upon hyperparameters. Consequently, techniques\nthat facilitate automatic hyperparameter adjustment hold\nconsiderable promise.\nLastly, we delve into three distinct groups of methods\naimed at controlling the model’s generalization tendencies.\nThese encompass mechanisms for regulating the simplicity\nof learned solutions, fostering diversity among solutions,\nand introducing biases to steer the model’s generalization\ntrajectory in a desired direction.\n5.1 Role of the optimizer\nThe extent to which the choice of optimizer, often mani-\nfesting as a form of stochastic gradient descent (SGD),\ninﬂuences generalization performance prompts inquiry.\nMingard et al. [ 79] provide insights by demonstrating that\nrandomly sampling parameters as opposed to using SGD\nhas negligible effects. Speciﬁcally, their study investigates\nthe probability of a deep neural network trained with SGD\nconverging to a speciﬁc input–output mapping on a given\ndataset. Their ﬁndings reveal that this probability aligns\nclosely with the Bayesian posterior probability of the\nmapping given the dataset. Similarly, Chiang et al. [ 80]\nconduct an experiment employing uniform sampling in lieu\nof Gaussian processes, achieving comparable generaliza-\ntion success rates to those obtained with SGD. Concur-\nrently, Barak et al. [ 81] explore the task of learning k-\nsparse parity of n bits. Their study underscores that the\nperformance of SGD, particularly in terms of training\ndynamics cannot be explained by random sampling. They\ndelineate the following reasons:\n1. The distribution of convergence times deviates signif-\nicantly from geometric patterns, a ﬁnding that aligns\nwith the observations of Kalimeris et al. [ 82]. Their\nstudy highlights that models trained with SGD exhibit\na tendency to increase in complexity throughout the\ntraining process.\n2. Deep models trained with SGD demonstrate an ability\nto master the sparse parity task in a number of\niterations approaching asymptotic optimality, a phe-\nnomenon challenging to clarify without introducing an\nexplicit sparsity prior. However, it’s noteworthy that\nthe inherent sparsity bias may stem from the deep\nmodel architecture itself. Mingard et al. [ 83] provide\ninsights into this aspect, demonstrating that such\nmodels exhibit a bias toward simpler functions.\nSpeciﬁcally, if the prior assigns equal probability mass\nto k-sparse functions for all k, the probability of a\nsingle k-sparse function will be Xðn\n/C0 kÞ.\n3. A signiﬁcant portion of the variance in convergence\ntimes can be attributed to the initialization method used\nin SGD.\n4. Phase transitions, exempliﬁed by phenomenon such as\ngrokking [ 84], showcase distinct qualitative shifts in\ntraining behavior across different stages. Hu et al. [ 85]\noffer an explicit modeling approach for this phe-\nnomenon using a hidden Markov model, where distinct\nstates correspond to various phases in the training\nprocess.\n5.2 Flatness\nSince at least Hochreiter and Schmidhuber [ 43], there has\nbeen a hypothesis suggesting that ﬂat minima in the loss\nlandscape correspond to better generalizing solutions.\nVarious techniques have been developed to identify such\nminima, often involving modiﬁcations to the loss function.\n1980 Neural Computing and Applications (2025) 37:1973–1997\n123\nLet’s denote the data distribution as p(x, y), and the mod-\nel’s loss with parameters h as Lðy j x; hÞ, typically repre-\nsenting the negative log-likelihood. The standard\noptimization process can then be expressed as:\narg min\nh\nEx;y /C24 pðx;yÞ½Lðy j x; hÞ/C138 þ khkk 2\n/C16/C17\nð1Þ\nForet et al. [ 86] try to identify minima that exhibit ﬂatness\nin all directions, drawing motivation from PAC Bayesian\ngeneralization bounds:\narg min\nh\nEx;y /C24 pðx;yÞ max\n/C15kk /C20 q\nLðy j x; h þ /C15Þ\n/C20/C21\nþ khkk 2\n/C18/C19\nð2Þ\nOrvieto et al. [ 45] concentrate on expected sharpness,\nconstructing their loss function as a smoothed version of\nthe original:\narg min\nh\nEx;y /C24 pðx;yÞ E/C15 /C24 N ð0;r2IÞLðy j x; h þ /C15Þ\n/C2/C3/C0/C1\nð3Þ\nChaudhari et al. [ 87] propose a similar approach:\narg min\nh\nEx;y /C24 pðx;yÞ /C0 log\nZ\nRn\nexpð/C0 Lðy j x; h þ /C15Þ\n/C20/C18\n/C0 c\n2 /C15kk 2Þd/C15\ni/C17 ð4Þ\nAdditionally, Ishida et al. [ 88] suggest a heuristic solution\nempirically shown to improve generalization:\narg min\nh\nEx;y /C24 pðx;yÞ\nh\nLðy j x; hÞ/C0 bjj\ni/C16/C17\nð5Þ\nBoth 2 and 3 optimize for large loss basins, just in different\nways. Regarding the connection with 4, Yang et al. [ 89]\ndemonstrate that some monotonically increasing function\nof negative local entropy serves as an upper bound for the\nHessian norm, which is directly optimized in 3. As for 5,\nLiu et al. [ 90] show that when the loss surpasses the\nﬂooding threshold b and the model alternates between\ngradient descent and gradient ascent, the dynamics become\nequivalent to minimizing the norm of the gradient. After\nSGD converges close to a local minimum, the expected\ngradient norm becomes proportional to the Hessian norm\nand the amount of stochasticity in SGD updates, implying\noptimization for ﬂatness.\n5.3 Averaging\nThe concept of using trajectory averaging for faster con-\nvergence is not novel, as evidenced by Polyak and Juditsky\n[91] and the literature referenced therein. However, recent\nresearch has proposed various approaches to leverage such\naveraging and has also clariﬁed its correlation with\ngeneralization.\nIzmailov et al. [92] discovered that SGD with cyclical or\nconstant learning rate schedules tends to converge to the\nvicinity of high-performing networks but often remains at\nthe boundary rather than reaching the center. To address\nthis, they proposed averaging the model weights from\nmultiple points during training. This method was found to\ngive solutions closer to the center of the corresponding loss\nbasin, resulting in enhanced generalization without adding\noverhead to either training or inference. Building upon this,\nLu et al. [93] applied the technique to ﬁne-tune pre-trained\nmodels and noted improvements in generalization.\nZhang et al. [ 94] proposed a two-level optimizer where\nthe outer loop advances in the direction of the average\nweight updates computed in the inner loop. While their\nprimary focus was on enhancing convergence, they also\nnoted improved generalization. A slight variation of this\napproach is presented in Zhang et al. [ 95], where the inner\nloop trains multiple copies of the model from the same\nstarting point and subsequently averages their parameters.\nThey suggest that this method can be combined with other\ntechniques, such as sharpness-aware minimization [ 86]o r\nensembling, to achieve superior performance.\nMandt et al. [ 96] examine SGD samples as a Markov\nchain, offering a concise proof that Polyak averaging [ 91]\nwith a constant learning rate is optimal among all\nstochastic gradient methods. They demonstrate that\nasymptotically, its efﬁcacy remains unaffected by batch\nsize or learning rate adjustments.\n5.4 Adversarial training\nWhile methodologies discussed in the previous section\nconcentrate on the ﬂatness of the loss concerning model\nparameters, adversarial training can be interpreted as\nstriving for ﬂatness of the loss concerning the inputs.\nHowever, the relationship between adversarial training and\ngeneralization is complex, and it can have both positive\nand negative effects.\nGoodfellow et al. [ 97] clarify that adversarial examples\narise due to the overly linear behavior of the model, which\nattempts to generalize in directions irrelevant to the task at\nhand. Essentially, if the model behaves linearly in the\nvicinity of a particular input and the input space is high-\ndimensional, perturbations in the direction of the gradient\ncan signiﬁcantly inﬂuence the model’s output. While the\nimpact from each dimension may be small, their cumula-\ntive effect grows linearly with the number of dimensions.\nNeural Computing and Applications (2025) 37:1973–1997 1981\n123\nConsequently, this introduces the ﬁrst source of the\nrobustness-generalization trade-off: linear extrapolation\nfacilitates easy generalization but compromises robustness.\nRaghunathan et al. [ 98] demonstrate this trade-off using a\ntoy example and observe that access to ample unlabeled\ndata points can help mitigate it. On the theoretical front,\nBubeck et al. [ 99] demonstrate that achieving robust\nlearning can be exponentially challenging computationally,\nwhile Schmidt et al. [ 100] illustrate that it can also be hard\nfrom an information theoretic sense.\nMadry et al. [ 101] highlight another challenge in\nbuilding robust models: as the perturbation size increases,\nthe decision boundary becomes more complex, necessi-\ntating a higher-capacity model to accommodate it, as\nillustrated in Fig. 1. Paradoxically, if there exists a less\ngeneral but more robust way to predict the data, the model\nmight transition to it, thereby compromising generalization\nonce again. Additionally, they observe that augmenting\nmodel capacity, such as increasing depth, inherently\nenhances robustness, even in the absence of adversarial\ntraining. Conversely, models with insufﬁcient capacity fail\nto derive beneﬁts from adversarial training and struggle to\nlearn effectively under such conditions.\nAdversarial training can also serve as a form of regu-\nlarization, as demonstrated by Goodfellow et al. [ 97], who\nobserved improved generalization on the MNIST dataset\nafter its application. This phenomenon arises because a\nnetwork exhibiting large gradients with respect to many\ninput features at certain training examples will lack\nrobustness; perturbing any of these features will result in\nsigniﬁcant shifts in the outputs. Consequently, the network\nis compelled to either rely on linear combinations with\nsmall coefﬁcients (in terms of L1 norm) or adopt highly\nnon-linear behaviors, ensuring that only a few features are\npivotal for each example. The former effect promotes\nsparsity, a beneﬁcial form of simplicity, while the latter\nreduces linearity, which, in certain scenarios, helps prevent\noverly simplistic solutions (similar in spirit to Sect. 5.7).\n5.5 Bootstrapping\nIf one aims to train a model for a task with limited or\nunavailable data, some form of bootstrapping becomes\nnecessary.\nAmini et al. [102] provide a comprehensive overview of\nmethods that leverage a trained classiﬁer and unlabeled\ndata to enhance performance further. Similarly, Huang\net al. [ 103] propose an analogous approach adapted for\nlarge language models, employing prompting techniques\nand aggregating multiple predictions to derive more reli-\nable answers from the model. Expanding on this, Jung et al.\n[104] demonstrate the feasibility of sampling unlabeled\ndata from a language model itself. Furthermore, Wang\net al. [105] use such an approach to generate an instruction\ntuning dataset from only 175 seed examples annotated by\nhumans, showcasing that a model ﬁne-tuned on this dataset\noutperforms strong baseline models.\nIn certain scenarios, the intrinsic nature of the problem\npermits the model to enhance itself solely through self-\ninteraction. A notable instance is AlphaGo Zero [ 106], a\nreinforcement learning system that attained superhuman\nperformance solely via self-play. Extending this concept,\nBricman and Feeney [ 107] propose training language\nmodels to advocate for various positions in arguments\nagainst other language models. They argue that such a\ndynamic will foster automated truth-seeking capabilities\nand substantially augment the language model’s abilities\nfar beyond its initial state.\n5.6 Meta-optimization\nThe emergent abilities of Transformers highlight the\npotential of optimizing a sufﬁciently ﬂexible model on\ndiverse datasets to achieve various beneﬁcial generaliza-\ntions. Therefore, it stands to reason that enhancing the\nﬂexibility of the optimization process itself can similarly\ngive gains in generalization.\nOne category of approaches revolves around optimizing\nthe hyperparameters of existing optimizers. For instance,\nChandra et al. [ 108] calculate the gradients of the training\nloss with respect to hyperparameters and subsequently\nutilize gradient-based optimizers to automatically adjust\nFig. 1 Decision boundary in\nadversarial training. Figure from\nMadry et al. [ 101]\n1982 Neural Computing and Applications (2025) 37:1973–1997\n123\nthese hyperparameters during training. However, as high-\nlighted by Wu et al. [ 109], there exists a trade-off between\nshort-term and long-term performance. For instance, a high\nlearning rate may consistently overshoot in each iteration\nbut achieve greater overall progress by moving faster.\nConsequently, when such meta-gradients are computed\nwith a short horizon, it often results in hyperparameters\nbiased toward short-term performance, leading to learning\nrates signiﬁcantly lower than optimal by orders of\nmagnitude.\nOne potential approach to address this credit assignment\nchallenge is by leveraging multiple training runs across\nvarious models to train a model capable of optimizing\nhyperparameters optimally. Almeida et al. [ 110] approach\nthis as a reinforcement learning problem, employing\nProximal Policy Optimization (PPO) [ 111] to train an\nLSTM model. This LSTM model takes unitless metrics\nregarding the current training state, such as the log-ratio of\nvalidation and training performance or the cosine similarity\nbetween gradient and momentum, as input and outputs\nadjustments to be made to the hyperparameters.\nYang et al. [ 89] take a direct approach to meta-opti-\nmization by optimizing the model parameters themselves.\nThey leverage local entropy [ 87] and Hessian norm to\nidentify meta-optimizers that lead to enhanced general-\nization performance for the optimized models. Expanding\non this idea, Kirsch and Schmidhuber [ 112] push the\nboundaries further by replacing each model weight with an\nLSTM and meta-learning the entire training dynamics.\nTheir study demonstrates that such a system can learn to\nimitate back-propagation, and the optimization algorithm\nlearned through meta-learning on one dataset generalizes\neffectively to training on another dataset.\nPeebles et al. [ 113] adopt an approach to optimization\nby learning a generative model for the joint distribution of\nmodel parameters and various metrics. This strategy\nenables them to sample models exhibiting low loss or other\ndesirable properties. While this approach may not be\nentirely practical at present, the concept of sampling\nmodels with good generalization, rather than solely\nfocusing on training methodologies to achieve such mod-\nels, presents an interesting avenue for further research. For\ninstance, Decision Transformer [ 114] demonstrated\nimpressive performance in model-free ofﬂine reinforce-\nment learning by leveraging a Transformer to learn the\njoint distribution of policies and their associated rewards.\n5.7 Avoiding solutions that are too simple\nOne crucial inductive bias in machine learning is sim-\nplicity, a principle often encapsulated by Occam’s razor.\nThis principle suggests that, given two features with equal\npredictive power, the simpler one is preferred. Numerous\nstudies have demonstrated that deep neural networks\nexhibit some form of Occam’s razor, particularly when\ntrained using gradient descent methods [ 51, 83]. While\nsimplicity can serve as a useful tool to prevent overﬁtting\nin many cases, there exists an inherent tension between\nsimplicity and complexity in achieving generalizable\nsolutions. In scenarios where overly simplistic solutions\nmay exist, it becomes imperative to avoid them, as they\ntend to rely on spurious correlations from the training data\nand may consequently fail to generalize well to unseen test\ndata.\nClark et al. [ 115] address this challenge by employing\nan ensemble approach, where a smaller capacity model is\ntrained alongside the main model. The smaller model pri-\nmarily captures superﬁcial patterns, while the larger one\nlearns features that generalize better. Similarly, Nam et al.\n[116] observe that superﬁcial features are often learned\ninitially. To mitigate this, they propose training a biased\nmodel by gradually increasing the weight assigned to\ncorrectly predicted examples. Concurrently, they train the\nmain, unbiased model on the more challenging instances.\nThe concept of simplicity can be customized for speciﬁc\narchitectures or tasks. In the ﬁeld of language modeling, a\ncrucial aspect is the model’s ability to capture long-dis-\ntance dependencies. For instance, [ 117] augment the\neffective context length of a language model by computing\nthe difference between output logits of a short-context\nmodel and those of the main model. Another approach by\nChuang et al. [118] delves into the disparities among layers\nin a deep model. They observe that outputs from the ﬁnal\nlayers tend to exhibit greater factual correctness. By\nstrategically selecting one of the earlier layers for com-\nparison, they enhance factuality even further. Speciﬁcally,\nthey identify the layer with the highest Jensen-Shannon\ndivergence of next-word distributions from the ﬁnal layer\nand then subtract their logits.\n5.8 Learning diverse solutions\nIf a model generalizes in a wrong manner, but superﬁcial\nfeatures are challenging to differentiate in terms of sim-\nplicity, it may prove beneﬁcial to train a diverse set of\nmodels. Each model could rely on different features,\nincreasing the likelihood that at least one of them captures\nthe essential ones. Teney et al. [ 119] achieved such\ndiversity by approximating feature importance through loss\ngradients and penalizing the dot product of these gradients\nfor all pairs of models. Consequently, they obtained models\nthat were all reasonably accurate, but concurrently made\npredictions based on different input characteristics.\nAnother approach involves modifying the training pro-\ncedure of a single model to incentivize the utilization of\nmultiple features. For instance, Pezeshki et al. [ 120]\nNeural Computing and Applications (2025) 37:1973–1997 1983\n123\nidentiﬁed that ridge regression introduces undesired cou-\npling between features. They introduced spectral decou-\npling, which entails applying L2 regularization to the\noutput logits instead of the network weights. The authors\nderived this approach by analyzing the model’s behavior in\nthe NTK [ 121] regime. However, an intuitive analogy can\nbe drawn to methods from the previous section. Consider\nthe model as an ensemble of two: one that leverages\nsuperﬁcial features and another based on more critical\nones. After training the biased part, regularization on out-\nputs will compel the unbiased model to focus on more\nchallenging examples. This is because increasing activa-\ntions for easy answers will no longer lead to a reduction in\nloss.\n5.9 Controlling a known bias\nTo mitigate biases similar to those induced by simplicity,\none approach involves training a distinct model speciﬁcally\ndesigned to capture the biased features and patterns. Sub-\nsequently, the residuals of this specialized model can be\nused to train the primary model [ 122]. Alternatively, the\nprimary model can be trained as part of an ensemble\nalongside the biased model [ 123], thereby reducing the\nimpact of the bias on the ﬁnal predictions.\nIn the domain of computer vision, Touvron et al. [ 124]\ndemonstrated enhanced data efﬁciency for vision Trans-\nformers by training them not only to predict the correct\nanswers but also to predict the answers provided by a\nconvolutional network. Building upon this work, Ren et al.\n[125] illustrated that employing two distinct teacher\narchitectures gives even greater beneﬁts.\nNaturally, for a model to effectively adopt a particular\ninductive bias, it must possess the ﬂexibility to learn it. For\ninstance, the LSTM architecture tends to derive fewer\nadvantages from LIME pre-training [ 28] compared to the\nTransformer architecture [ 126].\n5.10 Summary\nThe inﬂuence of gradient-based optimizers on successful\ngeneralization is moderate, with the model architecture\nplaying a more pivotal role by imposing an appropriate\nsimplicity prior. Understanding the behavior of models\nduring training-including the increasing complexity of\nrepresented functions, initialization dependence, and phase\ntransitions-is crucial.\nVarious approaches aim to incentivize convergence to\nﬂatter solutions, despite being formulated in different\nways, they optimize for similar properties.\nAveraging models in parameter space proves effective,\nespecially when models stem from the same training\ntrajectory or have a common ancestor, such as in ﬁne-\ntuning scenarios.\nWhile adversarial training may enhance generalization,\nit demands increased model capacity to learn a more non-\nlinear solution, potentially compromising performance for\nﬁxed parameter counts.\nBootstrapping methods encompass generating training\ndata from scratch or from seed examples, leveraging lan-\nguage model reasoning to enhance data quality, and\nemploying model outputs in self-interaction to serve as\nprimary training data sources.\nMeta-optimizers offer avenues for improved general-\nization within the same training budget, through hyperpa-\nrameter selection for existing optimizers and direct\noptimization of models. Basic notions like back-propaga-\ntion can even be parametrized and learned. Moreover, it\nmay be feasible to generate model parameters in a single\npass using another model, allowing control over different\nproperties, including generalization.\nWhen simple solutions are deemed inadequate for a\ntask, training a less capable model alongside the main one\ncan capture overly simplistic features and patterns, pre-\nventing the main model from relying on them. Alterna-\ntively, if undesired bias capture via an auxiliary model is\nuncertain, the training process can incentivize the model to\ndiversify its feature usage for predictions. Finally, known\nuseful biases represented by speciﬁc model architectures or\nalgorithms can be imparted onto models with different\narchitectures, provided they exhibit sufﬁcient ﬂexibility.\n6 Inference\nEven without updating model parameters, it is possible to\ninﬂuence performance by adjusting how the model is uti-\nlized. A prominent example of this is zero-shot and few-\nshot learning, where simply adding examples to the prompt\ncan enable the model to grasp the task contextually. Further\nadvancements can be made by augmenting the model with\nmemory, additional tools, and sophisticated prompting\nschemes.\nThese systems, often referred to as language agents,\nhave garnered signiﬁcant research attention recently. For a\ncomprehensive overview, one may refer to [ 127, 128]. In\nthis section, we aim to underscore the pivotal role of such\nsystems in enhancing the capabilities of raw language\nmodels, particularly in terms of their generalization.\n6.1 In-context learning\nIn-context learning [ 129], as exempliﬁed by techniques\nsuch as chain-of-thought prompting, offers means to instill\ndesired biases into models without explicit training. For\n1984 Neural Computing and Applications (2025) 37:1973–1997\n123\ninstance, presenting the model with examples of reasoning\nin similar tasks can prompt it to employ similar reasoning\nin its responses [ 130]. Even the simple addition of phrases\nlike ‘‘let’s think step by step’’ before providing answers can\ngive signiﬁcant effects [ 131]. In a demonstration by Jojic\net al. [ 132], models executing algorithms step-by-step\nwithin prompts showcase the ability to solve speciﬁed\ntasks.\nIn their work Min et al. [ 133] integrate pre-training with\nin-context learning. Speciﬁcally, they train the model using\nin-context learning demonstrations, enhancing its ability to\nadapt and learn in-context during inference. This approach\ncan be interpreted as an inductive bias toward acquiring\nfurther inductive biases.\nGarg et al. [ 134] demonstrate that Transformers exhibit\nthe capacity to learn functions that generate data, including\nlinear functions, two-layer neural networks, or decision\ntrees, following pre-training with corresponding in-context\nlearning examples. This ﬁnding suggests a meta-level\ninductive bias, focusing on the acquisition of speciﬁc\nclasses of inductive biases, but ones less prevalent in nat-\nural language tasks.\nKirsch et al. [ 135] have shown that various models can\nbe trained as in-context learners provided they possess a\nsufﬁciently large capacity and are exposed to a diverse\narray of tasks during pre-training. Interestingly, they note\nthat the critical dimension for effectiveness in this context\nis not the number of parameters, but rather the size of\nmodel state (memory).\n6.2 Tool use\nA popular way to augment agent capabilities in some\ndomain is to connect it to relevant tools. Parisi et al. [ 136]\nallow models to invoke tools and show how to bootstrap\nmodel to use tools effectively from a few demonstrations of\ntool use. Komeili et al. [ 137] allow the model access to\ninternet search and show that it leads to better performance\nthan retrieval-based methods. Gao et al. [ 138] augment\nlanguage models with the access to Python interpreter.\nSchick et al. [ 139] use question answering, calculator,\nWikipedia search, machine translation and calendar as\ntools. Patil et al. [ 140] ﬁne-tune a language model to\neffectively use a vast array of tools via their APIs,\nincluding pre-trained models from HuggingFace, Torch-\nHub, and TensorHub. Liu et al. [ 141] use language model\nto convert a given task into a formal language and then\nemploy a specialized solver for planning tasks.\n6.3 Memory\nMemory is a critical aspect of a learning system, particu-\nlarly inﬂuencing its generalization performance, as\ndemonstrated in [ 135]. Furthermore, a model with\nenhanced memory capabilities can generate longer outputs\nwithout sacriﬁcing coherence, offering advantages for\napproaches that extend a model’s abilities at the expense of\nlonger inference times, as discussed in the next sections.\nHowever, numerous models either possess a ﬁnite context\nwindow or struggle to access information from distant time\nsteps, prompting various efforts to augment them with\nexternal memory mechanisms.\nZhou et al. [142] propose RecurrentGPT, which operates\nsimilarly to LSTM but utilizes natural language processing\nfor all operations instead of linear algebra. In another\napproach, Packer et al. [ 143] conceptualize memory as a\ntool accessible to the system, enabling functions such as\nkeyword-based search. Additionally, Chen et al. [ 144]\nassist the system in handling long documents by con-\nstructing a tree of summaries ﬁrst, allowing the system to\nnavigate it to locate relevant information.\n6.4 Mutli-step reasoning\nWang et al. [145] use a technique where multiple solutions\nare sampled from a model, and the most frequent answer is\nselected. Since all correct solutions to a problem should\ngive the same correct answer, while incorrect solutions\nmay diverge, this aggregation method tends to enhance the\naccuracy of proposed solutions. Building upon this idea,\nYao et al. [146] introduce the concept of a tree of thoughts,\nwhere the model can branch into different lines of rea-\nsoning at any point, instead of sampling multiple chains of\nreasoning and aggregating them afterward. Extending fur-\nther, Besta et al. [ 147] advance from trees to arbitrary\ngraphs, enabling the aggregation of ideas even before\nreaching the ﬁnal output or reﬁning an idea rather than\nderiving conclusions from it. Lastly, Sel et al. [ 148] dis-\ncover that presenting in-context examples of tree search\nover ideas is sufﬁcient to encourage a language model to\nuse a similar technique. Furthermore, because the model is\nnot bound to strictly follow the algorithm and can apply\nheuristics, it performs even better than the original algo-\nrithm. For a visual comparison of these approaches, refer to\nFig. 2.\n6.5 Reflection\nMadaan et al. [ 149] demonstrate that a simple iterative\nprocess of generation and self-reﬂection can enhance the\nquality of outputs from the same underlying language\nmodel. Meanwhile, Shinn et al. [ 150] employ language\nmodels to simulate reinforcement learning algorithms.\nInstead of relying on gradient descent, they solve the credit\nassignment problem through self-reﬂection. To preserve\nthe insights gained, they use an external memory. These\nNeural Computing and Applications (2025) 37:1973–1997 1985\n123\napproaches allow for a trade-off between longer inference\ntimes and improved output quality, where quality encom-\npasses generalization. By engaging in more extensive\n‘‘thinking,’’ a model may uncover deeper structures within\na task, even if said task lies outside the distribution origi-\nnally encountered by the model.\nZhao et al. [ 151] integrate external memory storage,\nreﬂective analysis of failed attempts, reuse of previous\nsuccessful strategies, and examination of commonalities\namong successive attempts. This integrated approach\nresults in an agent capable of experiential learning and\nadept at generalizing across tasks.\nZelikman et al. [ 152] present an approach where an\nagent is represented as a Python function. This function\ntakes an initial solution to the task, a utility function\nspeciﬁc to the task, and a language model as inputs, and\niteratively reﬁnes the solution. Through recursive applica-\ntion of this agent to optimize its own code, they provide a\nproof-of-concept for self-improvement in language model-\nbased agents.\n6.6 Summary\nThe upper limit of what Transformers can achieve through\nin-context learning remains uncertain. However, a\nnotable achievement lies in their ability to emulate rea-\nsoning strategies and even basic machine learning algo-\nrithms. Furthermore, reﬁning in-context learning\ncapabilities is feasible through effective pre-training\nstrategies.\nTo address the limitations of modern language models,\nleveraging external tools and memory proves beneﬁcial.\nAdditionally, employing sophisticated prompting\ntechniques enhances robustness against inadvertent rea-\nsoning errors and enables learning from experience without\nre-training.\nThe comparative table presents the key results of various\nstudies that were considered in this review (Appendix A).\n7 Discussion\nThis paper provides a review of the expanding ﬁeld of\nresearch focused on understanding the generalization\ncapabilities of neural networks. Within this domain,\nnumerous approaches have emerged, operating at various\nlevels of abstraction and targeting different components of\nthe training pipeline. These components encompass diverse\naspects such as model architecture, data distribution, opti-\nmization techniques, and modiﬁcations made during\ninference. A wide array of strategies has been proposed,\neach offering insights into enhancing generalization per-\nformance. These strategies span from structural modiﬁca-\ntions to the neural network architecture to adjustments in\nthe distribution of training data.\nA critical need exists for the development of theories\nthat clarify the causal mechanisms underlying observed\nphenomenon. Establishing such theoretical foundations\ncould pave the way for principled approaches aimed at\nsystematically improving generalization across diverse\nneural network architectures and tasks. As the ﬁeld con-\ntinues to evolve, the pursuit of such theories becomes\nincreasingly imperative.\nFig. 2 Prompting strategies for multi-step reasoning. Figure from Besta et al. [ 147]\n1986 Neural Computing and Applications (2025) 37:1973–1997\n123\nAppendix A Comparison table of different studies\nAuthors Models Data Results\nIrrelevant data\nPapadimitriou\nand Jurafsky\n[6]\nPre-training Long Short-Term Memory\n(LSTM) networks\nStructured, non-linguistic data such as\nMIDI music, Java code, or nested\nparentheses\nTransfer learning as a method for\nanalyzing the encoding of\ngrammatical structure in neural\nlanguage models; improve test\nperformance on natural language,\ndespite no overlap in surface form or\nvocabulary\nLu et al. [ 8] Fine-tuning only the input and output\nembeddings of a pre-trained\nGenerative Pre-trained Transformer 2\n(GPT-2) model; Frozen Pre-trained\nTransformer (FPT)\nNatural language data; the datasets\nprovided by TAPE\nInvestigate the capability of a\ntransformer pre-trained on natural\nlanguage to generalize to other\nmodalities with minimal ﬁne-tuning;\nlanguage pre-trained Transformers can\nobtain strong performance on a variety\nof non-language tasks\nSinha et al. [ 10] Masked language model (MLM) Sentences with randomly shufﬂed word\norder\nMLM’s success can largely be explained\nby it having learned higher-order\ndistributional statistics that make for a\nuseful prior for subsequent ﬁne-tuning\nIrrelevant data\nKrishna et al.\n[11]\nDifferent T5 models; PG models The training data from a completely\nartiﬁcial language, comprising\nrandom n-grams\nPre-training on documents consisting of\ncharacter n-grams selected at random\nnearly matches the performance of\nmodels pre-trained on real corpora\nMaennel et al.\n[12]\nSeveral network architectures, including\nVGG16 and ResNet-18, on CIFAR-10\nand ImageNet\nNatural image data with entirely\nrandom labels\nAn alignment between the principal\ncomponents of network parameters\nand data takes place when training\nwith random labels\nChowers and\nWeiss [13]\nDeep Convolutional Neural Networks\n(CNNs)\nDifferent dtasets, for example,\nImageNet, CIFAR-10\nThe energy distribution is very different\nfrom that of the initial weights and is\nremarkably consistent across random\ninitializations, datasets, architectures\nand even when the CNNs are trained\nwith random labels\nMehta et al.\n[14]\nDifferent pre-trained Transformer\nmodels (D-BERT: DistilBERT,\nBERT-b: BERT- base, RoBERTa:\nRoBERTa-base, BERT-L: BERT-\nLarge)\nDifferent datasets, for example,\nMNIST, CIFAR-10\nInvestigate existing methods in the\ncontext of large, pre-trained models\nand evaluate their performance on a\nvariety of text and image classiﬁcation\ntasks\nIrrelevant data\nNeyshabur et al.\n[15]\nRI (random initialization), P (pre-trained\nmodel), RI-T (model trained on target\ndomain from random initialization),\nP-T (model trained/ﬁne-tuned on\ntarget domain starting from pre-trained\nweights)\nIMAGENET pre-training;\nCHEXPERT; three sets from\nDOMAINNET as downstream\ntransfer learning tasks\nNew tools and analyses to address\nfundamental questions what enables a\nsuccessful transfer and which part of\nthe network is responsible for that\nInternal representations\nGurnee and\nTegmark [\n16]\nThe Llama-2 family of models Datasets: world, US, NYC places; three\ntemporal datasets (historical ﬁgures,\nartworks, news headlines)\nIdentify individual ‘‘space neurons’’ and\n‘‘time neurons’’ that reliably encode\nspatial and temporal coordinates\nNeural Computing and Applications (2025) 37:1973–1997 1987\n123\nAuthors Models Data Results\nLi et al. [ 17] GPT model A simple board game, Othello Evidence of an emergent nonlinear\ninternal representation of the board\nstate; produce ‘ ‘latent saliency maps’’\nthat help explain predictions\nJin and Rinard\n[18]\nTransformer model A synthetic corpus of programs written\nin a domain-speciﬁc language for\nnavigating 2D grid world\nenvironments\nEvidence that language models of code\ncan learn to represent the formal\nsemantics of programs\nTurner et al.\n[20]\nLLMs (LLaMA-3, OPT, GPT-2, and\nGPT-J)\nThe Stanford IMDb Large Movie\nReview Dataset;\nRealToxicityPrompts; LAMA\nConceptNet; OpenWebText\nInvestigate activation engineering:\nmodifying activations at inference\ntime to predictably alter model\nbehavior\nHigh-quality data\nEldan and Li\n[22]\nGPT-3.5 and GPT-4 TinyStories—a synthetic dataset of\nshort stories that only contain words\nthat a typical 3 to 4-year-olds usually\nunderstand\nA framework which uses GPT-4 to\ngrade the content generated by these\nmodels as if those were stories written\nby students and graded by a (human)\nteacher\nGunasekar et al.\n[23]\nphi-1—a Transformer-based language\nmodel for code\nCodeTextbook; CodeExercises dataset phi-1 attains accuracy 50.6% on\nHumanEval and 55.5% on MBPP\nLi et al. [ 24] phi-1.5—a 1.3 billion parameter model A combination of phi-1’s training data\n(7B tokens) and newly created\nsynthetic, ‘‘textbook-like’’ data\n(roughly 20B tokens)\nphi-1.5 exhibits the ability to ‘‘think step\nby step’’ or perform some rudimentary\nin-context learning\nSurkov and\nYamshchikov\n[25]\nGLUE, SuperGLUE, CLUE, and\nRussianSuperGLUE\nThe data of GLUE, SuperGLUE,\nCLUE, RussianSuperGLUE; all the\nbenchmarks in Papers With Code\nwhich belong to the NLP ﬁeld\nA theoretical instrument and a practical\nalgorithm to calculate similarity\nbetween benchmark tasks\nLearning inductive bias from data\nMcCoy et al.\n[26]\nModel in framework Different data A framework for giving particular\nlinguistic inductive biases to a neural\nnetwork model\nWu et al. [ 28] LIME models, HAT Synthetic datasets Hypothesis that Transformer networks\nare ﬂexible enough to learn inductive\nbias from suitable generic tasks\nLearning inductive bias from data\nLindemann\net al. [ 29]\nPre-training models Synthetic data Show how a structural inductive bias\ncan be efﬁciently injected into a\nseq2seq model by pre-training it to\nsimulate structural transformations on\nsynthetic data\nMukherjee et al.\n[30]\nOrca, a 13-billion parameter model that\nlearns to imitate the reasoning process\nof LFMs (large foundation models);\nGPT-4\nLarge-scale and diverse imitation data\nwith judicious sampling and selection\nShow that learning from step-by-step\nexplanations, whether these are\ngenerated by humans or more\nadvanced AI models, is a promising\ndirection to improve model\ncapabilities and skills\nDemonstrations of instruction following and problem solving\nWei et al. [ 32] 137B parameter pre-trained language\nmodel FLAN\nOver 60 NLP datasets verbalized via\nnatural language instruction templates\nNumber of ﬁne-tuning datasets, model\nscale, and natural language\ninstructions are key to the success of\ninstruction tuning\nOuyang et al.\n[33]\nFine-tune GPT-3 using supervised\nlearning; InstructGPT\nCollect a dataset of labeler\ndemonstrations\nInstructGPT models show\nimprovements in truthfulness and\nreductions in toxic output generation\nwhile having minimal performance\nregressions on public NLP datasets\n1988 Neural Computing and Applications (2025) 37:1973–1997\n123\nAuthors Models Data Results\nDemonstrations of instruction following and problem solving\nYe et al. [ 34] FLIPPED model The subset of T0 meta-training datasets FLIPPED gives particularly large\nimprovements on tasks with unseen\nlabels, outperforming T0-11B by up to\n?20% average F1 score\nChen et al. [ 35] A variety of base LMs A setup of question answering (QA)\nwith a Google search API\nPropose FireAct, a novel approach to\nﬁne-tuning LMs with trajectories from\nmultiple tasks and prompting methods,\nand show having more diverse ﬁne-\ntuning data can further improve agents\nWang et al. [36] Tk-INSTRUCT—a Transformer model\ntrained to follow a variety of in-\ncontext instructions\nInstruction-following datasets; SUPER-\nNATURAL INSTRUCTIONS, a\nbenchmark of 1,616 diverse NLP\ntasks and their expert-written\ninstructions\nTk-INSTRUCT outperforms existing\ninstruction-following models such as\nInstructGPT by over 9% on our\nbenchmark despite being an order of\nmagnitude smaller\nChan et al. [ 37] An embedder; a causal Transformer\nmodel\nData consist of a mix of ‘bursty’ and\n‘non-bursty’ sequences\nIn-context learning traded off against\nmore conventional weight-based\nlearning, and models were unable to\nachieve both simultaneously\nFactors related to generalization\nChatterjee and\nZielinski [46]\nDeep architectures trained with gradient\nmethods\nDifferent data Exploration of the interaction of the\ngradients of different examples during\ntraining\nGoldblum et al.\n[50]\nGPT-2, GoogLeNet models CIFAR-10; CIFAR-100; ImageNet Architectures designed for a particular\ndomain, such as computer vision, can\ncompress datasets on a variety of\nseemingly unrelated domains\nRole of depth\nVon Oswald\net al. [ 56]\nTransformers Linear regression datasets Trained Transformers become meta-\noptimizers, i.e., learn models by\ngradient descent in their forward pass\nXu et al. [ 57] Graph neural networks (GNNs) Dataset generation GNNs align with DP (dynamic\nprogramming) and to solve different\ntasks\nModel architecture for inductive bias\nLi et al. [ 61] A language model augmented with a\nvalue function; BLEU; AdverSuc\nThe OpenSubtitles (OSDb) dataset Strategy to manipulate the behavior of a\nneural decoder that enables it to\ngenerate outputs that have speciﬁc\nproperties of interest\nAmos and\nKolter [62]\nOptNet—a network architecture Dataset of puzzles OptNet—a network architecture that\nintegrates optimization problems\n(speciﬁcally in the form of quadratic\nprograms) as individual layers in\nlarger end-to-end trainable deep\nnetworks\nModel architecture for inductive bias\nvon Oswald\net al. [ 63]\nTransformers A large compilation of various English\ntext datasets including parts of\nWikipedia, arXiv, and code\nStandard next-token prediction error\nminimization gives rise to a subsidiary\nlearning algorithm, this process\ncorresponds to gradient-based\noptimization of a principled objective\nfunction, which leads to strong\ngeneralization performance on unseen\nsequences\nNeural Computing and Applications (2025) 37:1973–1997 1989\n123\nAuthors Models Data Results\nGraves [64] Recurrent neural networks The Hutter prize Wikipedia dataset Adaptive Computation Time (ACT)—\nan algorithm that allows recurrent\nneural networks to learn how many\ncomputational steps to take between\nreceiving an input and emitting an\noutput\nBanino et al.\n[65]\nTransformers The bAbI question answering dataset;\nthe English Question Answer dataset\nPonderNet—a new algorithm that learns\nto adapt the amount of computation\nbased on the complexity of the\nproblem at hand\nDehghani et al.\n[66]\nThe Universal Transformer The WMT14 En-De dataset The Universal Transformer (UT)—a\nparallel-in-time self-attentive recurrent\nsequence model which can be cast as a\ngeneralization of the Transformer\nmodel\nModel architecture for inductive bias\nKhandelwal\net al. [ 67]\nkNN-LMs WIKITEXT-103 LM; BOOKS is the\nToronto Books Corpus; WIKI-3B is\nEnglish Wikipedia\nkNN-LMs—an extension a pre-trained\nneural language model (LM) by\nlinearly interpolating it with a\nk-nearest neighbors (kNN) model;\nkNN-LM achieves a new state-of-the-\nart perplexity of 15.79—a 2.9 point\nimprovement with no additional\ntraining\nGuu et al. [ 68] Retrieval-Augmented Language Model\npre-training (REALM)\nCoNLL-2003 data; Open-QA datasets Augmentation of language model pre-\ntraining with a latent knowledge\nretriever, which allows the model to\nretrieve and attend over documents\nfrom a large corpus such as Wikipedia\nBorgeaud et al.\n[69]\nRetrieval-Enhanced Transformer\n(Retro)\nA 2 trillion token database Enhancement of auto-regressive\nlanguage models by conditioning on\ndocument chunks retrieved from a\nlarge corpus, based on local similarity\nwith preceding tokens\nWu et al. [ 70] Extension to the Transformer\narchitecture, called kNN-augmented\nattention; Transformer XL;\nMemorizing Transformer\nGeneric webtext (C4), math papers\n(arXiv), books (PG-19), code\n(Github), formal theorems (Isabelle)\nExtension of language models with the\nability to memorize the internal\nrepresentations of past inputs\nRole of the optimizer\nMingard et al.\n[79]\nDeep neural networks (DNNs) MNIST, Fashion-MNIST, IMDb movie\nreview dataset, Ionosphere Dataset\nRandomly sampling parameters as\nopposed to using SGD has negligible\neffects\nKalimeris et al.\n[82]\nModels trained with SGD Binary MNIST; CIFAR-10 Key to this work is a new measure of\nhow well one classiﬁer explains the\nperformance of another, based on\nconditional mutual information\nFlatness\nYang et al. [ 89] Neural networks ResNet-20 CIFAR-10 An implicit connection between the\nlocal entropy and the Hessian;\nﬂatness-aware regularizers to\nincorporate two metrics into the L2O\nframework for meta-training\noptimizers to learn to generalize\nAveraging\nIzmailov et al.\n[92]\nState-of-the-art residual networks,\nPyramidNets, DenseNets, and\nShakeShake networks\nCIFAR-10, CIFAR-100 and ImageNet Simple averaging of multiple points\nalong the trajectory of SGD, with a\ncyclical or constant learning rate, leads\nto better generalization than\nconventional training\n1990 Neural Computing and Applications (2025) 37:1973–1997\n123\nAuthors Models Data Results\nAveraging\nLu et al. [ 93] The pre-trained DistilRoBERTa;\nRoBERTa-Large\nThe GLUE benchmark Adaptation of Stochastic Weight\nAveraging (SWA), a method\nencouraging convergence to a ﬂatter\nminimum, to ﬁne-tuning PLMs; this\nsimple optimization technique is able\nto outperform the state-of-the-art KD\nmethods for compact models\nZhang et al.\n[94]\nTransformer-based neural machine\ntranslation models\nImageNet, CIFAR-10/100; the Penn\nTreebank dataset; the WMT 2014\nEnglish-to-German dataset\nLookahead—a new optimization\nalgorithm, that is orthogonal to\nprevious approaches and iteratively\nupdates two sets of weights\nZhang et al.\n[95]\nCNNs and ViTs CIFAR and ImageNet Lookaround - a straightforward yet\neffective SGD-based optimizer\nleading to ﬂatter minima with better\ngeneralization\nAdversarial training\nGoodfellow\net al. [ 97]\nDeep neural networks The MNIST dataset The primary cause of neural networks’\nvulnerability to adversarial\nperturbation is their linear nature\nRaghunathan\net al. [ 98]\nWide ResNet 40-2 models CIFAR-10; MNIST The robustness-generalization trade-off:\nlinear extrapolation facilitates easy\ngeneralization but compromises\nrobustness\nBootstrapping\nHuang et al.\n[103]\nPre-trained large language models GSM8K; DROP; OpenBookQA;\nANLI-A3\nLLM is capable of self-improving with\nonly unlabeled datasets\nJung et al. [104] Impossible-T5; GPT-3.5; ChatGPT Distilled dataset from 1.5B LMs IMPOSSIBLE DISTILLATION—a\nframework for paraphrasing and\nsentence summarization, that distills a\nhigh-quality dataset and model from a\nlow-quality teacher that itself cannot\nperform these tasks\nMeta-optimization\nChandra et al.\n[108]\nMLPs, CNNs, RNNs CIFAR-10 Showing how to automatically compute\nhypergradients with a simple and\nelegant modiﬁcation to back-\npropagation\nWu et al. [ 109] MLP network with two hidden layers of\n100 units, with ReLU activations;\nCNN network\nMNIST; CIFAR-10 Introduction of a toy problem, a noisy\nquadratic cost function, on which the\nauthors analyze short-horizon bias by\nderiving and comparing the optimal\nschedules for short- and long-time\nhorizons\nAvoiding solutions that are too simple\nClark et al.\n[115]\nLXMERT—a transformer-based model\nthat has been pre-trained on image-\ncaption data; the pre-trained uncased\nBERT-Base model; ResNet-18\nSynthetic MNIST; VQA-CP v2 dataset;\nthe MNLI Hard sentence pair\nclassiﬁcation dataset; ImageNet\nA method that can automatically detect\nand ignore the dataset-speciﬁc patterns\n(dataset biases)\nNam et al. [116] MLP with three hidden layers, ResNet-\n20, and ResNet-18\nColored MNIST; Corrupted CIFAR-10 A failure-based debiasing scheme by\ntraining a pair of neural networks\nsimultaneously\nChuang et al.\n[118]\nLLaMA family models TruthfulQA; FACTOR (News/Wiki) Decoding by Contrasting Layers (DoLa)\napproach obtains the next-token\ndistribution by contrasting the\ndifferences in logits obtained from\nprojecting the later layers versus\nearlier layers to the vocabulary space\nNeural Computing and Applications (2025) 37:1973–1997 1991\n123\nAuthors Models Data Results\nLearning diverse solutions\nTeney et al.\n[119]\nA fully connected two-hidden layer\nMLP classiﬁer\nImageNet-9; MNIST/CIFAR The simplicity bias can be mitigated and\nout-of-distribution generalization\nimproved\nPezeshki et al.\n[120]\nA convolutional network with ReLU\nnonlinearity\nCIFAR-10, CIFAR-100, and CIFAR-2;\nColored MNIST\nGradient Starvation arises when cross-\nentropy loss is minimized by capturing\nonly a subset of features relevant for\nthe task, despite the presence of other\npredictive features that fail to be\ndiscovered\nControlling a known bias\nTouvron et al.\n[124]\nVision Transformer Imagenet Competitive convolution-free\ntransformers by training on Imagenet\nonly; a teacher-student strategy\nspeciﬁc to Transformers\nRen et al. [ 125] Vision Transformer; CivT (cross\ninductive bias transformer); ResNet;\nRedNet\nImageNet A distillation-based method to train\nvision Transformers; lightweight\nteachers with different architectural\ninductive biases (e.g., convolution and\ninvolution) to co-advise the student\nTransformer\nIn-context learning\nJojic et al. [132] GPT-3 family of models A dataset of 100 random nonrepeating\ndigit sequences of length 5\nExecution and description of iterations\nby regimenting self-attention (IRSA)\nin one (or a combination) of three\nways: 1) Using strong repetitive\nstructure in an example of an\nexecution path of a target program for\none particular input, 2) Prompting\nwith fragments of execution paths, and\n3) Explicitly forbidding (skipping)\nself-attention to parts of the generated\ntext\nIn-context learning\nMin et al. [ 133] The pre-trained LM 142 NLP datasets MetaICL (Meta-training for InContext\nLearning) - a meta-training framework\nfor few-shot learning where a pre-\ntrained language model is tuned to do\nin-context learning on a large set of\ntraining tasks\nTool use\nParisi et al.\n[136]\nTransformer-based language models;\npre-trained T5 models\nNatural Questions (NQ); MathQA Tool Augmented Language Models\n(TALM)—combining a text-only\napproach to augment language models\nwith non-differentiable tools, and an\niterative ‘‘self-play’’ technique to\nbootstrap performance starting from\nfew tool demonstrations\nKomeili et al.\n[137]\nT5; BART-Large; BlenderBot variants;\nTransformer\nCollected dataset of human-human\nconversations\nAn approach that learns to generate an\ninternet search query based on the\ncontext, and then conditions on the\nsearch results to ﬁnally generate a\nresponse\nTool use\nGao et al. [ 138] ProgramAided Language models (PAL) BIG-Bench Hard ProgramAided Language models (PAL)\n- an approach that uses the LLM to\nread natural language problems and\ngenerate programs as the intermediate\nreasoning steps\n1992 Neural Computing and Applications (2025) 37:1973–1997\n123\nAuthors Models Data Results\nMemory\nZhou et al.\n[142]\nRECURRENTGPT; RNN A diverse set of genres of novels\nincluding science ﬁction, romance,\nfantasy, horror, mystery, and thriller\nnovels\nRECURRENTGPT—a languagebased\nsimulacrum of the recurrence\nmechanism in RNNs\nPacker et al.\n[143]\nMemGPT (MemoryGPT) Multi-Session Chat dataset; augmented\nMSC dataset, nested KV retrieval\ndataset, and a dataset of embeddings\nfor 20M Wikipedia articles\nVirtual context management—a\ntechnique drawing inspiration from\nhierarchical memory systems in\ntraditional operating systems which\nprovide the illusion of an extended\nvirtual memory via paging between\nphysical memory and disk\nChen et al.\n[144]\nStable Beluga 2; MEMWALKER;\nLLaMA 2 Chat\nQuALITY, SummScreenFD, and\nGovReport from the SCROLLS\nbenchmark\nMEMWALKER—a method that ﬁrst\nprocesses the long context into a tree\nof summary nodes\nMutli-step reasoning\nWang et al.\n[145]\nUL2; GPT-3; LaMDA-137B; PaLM-\n540B\nThe Math Word Problem Repository;\nCommonsenseQA\nA decoding strategy, self-consistency, to\nreplace the naive greedy decoding\nused in chain-of-thought prompting\nYao et al. [ 146] GPT-4 Game of 24; for a creative writing task\nwhere the input is 4 random\nsentences; 5/C2 5 mini crosswords\n‘ ‘Tree of Thoughts’’ (ToT) - a\nframework for language model\ninference which generalizes over the\npopular ‘‘Chain of Thought’’ approach\nto prompting language models, and\nenables exploration over coherent\nunits of text (‘‘thoughts’’) that serve as\nintermediate steps toward problem\nsolving\nBesta et al.\n[147]\nGPT-3.5; Llama-2 Different prompts Graph of Thoughts (GoT) - a framework\nthat advances prompting capabilities\nin large language models (LLMs)\nbeyond those offered by paradigms\nsuch as Chain of Thought or Tree of\nThoughts (ToT)\nReﬂection\nMadaan et al.\n[149]\nGPT-3.5 and GPT-4 A single LLM as the generator SELF-REFINE - an approach for\nimproving initial outputs from LLMs\nthrough iterative feedback and\nreﬁnement\nReﬂection\nShinn et al.\n[150]\nCoT (GT); ReAct HotPotQA; multi-step tasks in common\nhousehold environments in AlfWorld;\ncode writing tasks in competition-like\nenvironments with interpreters and\ncompilers in HumanEval; MBPP;\nLeetcodeHard\nReﬂexion—a framework to reinforce\nlanguage agents not by updating\nweights, but instead through linguistic\nfeedback; Reﬂexion achieves a 91%\npass@1 accuracy on the HumanEval\ncoding benchmark\nZhao et al.\n[151]\nReAct and Act as main baselines\nplanning LLM agents\nHotpotQA; ALFWorld and WebShop;\nFEVER\nExperiential Learning (ExpeL) agent\nthat autonomously gathers experiences\nand extracts knowledge using natural\nlanguage from a collection of training\ntasks\nZelikman et al.\n[152]\nGPT-4 Code generation using LMs as meta-\noptimizers\nDemonstration that LMs like GPT-4 are\ncapable of improving code that\nleverages the LM itself\nNeural Computing and Applications (2025) 37:1973–1997 1993\n123\nFunding Open Access funding enabled and organized by Projekt\nDEAL. Mrs. Bykova work was supported by the grant for research\ncenters in the ﬁeld of AI provided by the Analytical Center for the\nGovernment of the Russian Federation (ACRF) in accordance with\nthe agreement on the provision of subsidies (identiﬁer of the Agree-\nment 000000D730321P5Q0002) and the agreement with HSE Uni-\nversity No. 70-2021-00139.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nData availability This is a review paper; thus, it has no accompanying\ndata.\nReferences\n1. He K, Zhang X, Ren S, Sun J (2015) Delving deep into recti-\nﬁers: surpassing human-level performance on imagenet classi-\nﬁcation. In: Proceedings of the IEEE international conference on\ncomputer vision, pp 1026–1034\n2. Shlegeris B, Roger F, Chan L (2022) Language models seem to\nbe much better than humans at next-token prediction. https://\nwww.alignmentforum.org/posts/htrZrxduciZ5QaCjw/language-\nmodels-seem-to-be-much-better-than-humans-at-next\n3. Villalobos P, Sevilla J, Heim L, Besiroglu T, Hobbhahn M, Ho\nA (2022) Will we run out of data? an analysis of the limits of\nscaling datasets in Machine learning. arXiv preprint arXiv:2211.\n04325\n4. Hendrycks D, Mazeika M, Woodside T (2023) An overview of\ncatastrophic AI risks. arXiv preprint arXiv:2306.12001\n5. Han X, Zhang Z, Ding N, Gu Y, Liu X, Huo Y, Qiu J, Yao Y,\nZhang A, Zhang L et al (2021) Pre-trained models: past, present\nand future. AI Open 2:225–250\n6. Papadimitriou I, Jurafsky D (2020) Learning music helps you\nread: Using transfer to study linguistic structure in language\nmodels. arXiv preprint arXiv:2004.14601\n7. Hochreiter S, Schmidhuber J (1997) Long short-term memory.\nNeural Comput 9(8):1735–1780\n8. Lu K, Grover A, Abbeel P, Mordatch I (2021) Pretrained\nTransformers as universal computation engines. arXiv preprint.\narXiv:2103.05247\n9. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I et al\n(2019) Language models are unsupervised multitask learners.\nOpenAI blog 1(8):9\n10. Sinha K, Jia R, Hupkes D, Pineau J, Williams A, Kiela D (2021)\nMasked language modeling and the distributional hypothesis:\norder word matters pre-training for little. arXiv preprint arXiv:\n2104.06644\n11. Krishna K, Bigham J, Lipton ZC (2021) Does pretraining for\nsummarization require knowledge transfer? arXiv preprint\narXiv:2109.04953\n12. Maennel H, Alabdulmohsin IM, Tolstikhin IO, Baldock R,\nBousquet O, Gelly S, Keysers D (2020) What do neural net-\nworks learn when trained with random labels? Adv Neural Inf\nProcess Syst 33:19693–19704\n13. Chowers R, Weiss Y (2023) What do CNNs learn in the ﬁrst\nlayer and why? A linear systems perspective\n14. Mehta SV, Patil D, Chandar S, Strubell E (2021) An empirical\ninvestigation of the role of pre-training in lifelong learning.\narXiv preprint arXiv:2112.09153\n15. Neyshabur B, Sedghi H, Zhang C (2020) What is being trans-\nferred in transfer learning? Adv Neural Inf Process Syst\n33:512–523\n16. Gurnee W, Tegmark M (2023) Language models represent\nspace and time. arXiv preprint arXiv:2310.02207\n17. Li K, Hopkins AK, Bau D, Vie ´gas F, Pﬁster H, Wattenberg M\n(2022) Emergent world representations: exploring a sequence\nmodel trained on a synthetic task. arXiv preprint arXiv:2210.\n13382\n18. Jin C, Rinard M (2023) Evidence of meaning in language\nmodels trained on programs. arXiv preprint arXiv:2305.11169\n19. Mikolov T, Chen K, Corrado G, Dean J (2013) Efﬁcient esti-\nmation of word representations in vector space. arXiv preprint\narXiv:1301.3781\n20. Turner A, Thiergart L, Udell D, Leech G, Mini U, MacDiarmid\nM (2023) Activation addition: steering language models without\noptimization. arXiv preprint arXiv:2308.10248\n21. Nanda N (2023) Actually, othello-GPT has a linear emergent\nworld representation. https://www.neelnanda.io/mechanistic-\ninterpretability/othello\n22. Eldan R, Li Y (2023) Tinystories: How small can language\nmodels be and still speak coherent English? arXiv preprint\narXiv:2305.07759\n23. Gunasekar S, Zhang Y, Aneja J, Mendes CCT, Del Giorno A,\nGopi S, Javaheripi M, Kauffmann P, Rosa G, Saarikivi O, et al\n(2023) Textbooks are all you need. arXiv preprint arXiv:2306.\n11644\n24. Li Y, Bubeck S, Eldan R, Del Giorno A, Gunasekar S, Lee YT\n(2023) Textbooks are all you need ii: phi-1.5 technical report.\narXiv preprint arXiv:2309.05463\n25. Surkov MK, Yamshchikov IP (2024) Vygotsky distance: mea-\nsure for benchmark task similarity. arXiv preprint arXiv:2402.\n14890\n26. McCoy RT, Grant E, Smolensky P, Grifﬁths TL, Linzen T\n(2020) Universal linguistic inductive biases via meta-learning.\narXiv preprint arXiv:2006.16324\n27. Finn C, Abbeel P, Levine S (2017) Model-agnostic meta-\nlearning for fast adaptation of deep networks. In: International\nconference on machine learning. PMLR, pp 1126–1135\n28. Wu Y, Rabe MN, Li W, Ba J, Grosse RB, Szegedy C (2021)\nLIME: learning inductive bias for primitives of mathematical\nreasoning. In: International conference on machine learning.\nPMLR, pp 11251–11262\n29. Lindemann M, Koller A, Titov I (2023) Injecting a structural\ninductive bias into a seq2seq model by simulation. arXiv pre-\nprint arXiv:2310.00796\n30. Mukherjee S, Mitra A, Jawahar G, Agarwal S, Palangi H,\nAwadallah A (2023) Orca: progressive learning from complex\nexplanation traces of GPT-4. arXiv preprint arXiv:2306.02707\n31. Yi S, Goel R, Khatri C, Cervone A, Chung T, Hedayatnia B,\nVenkatesh A, Gabriel R, Hakkani-Tur D (2019) Towards\ncoherent and engaging spoken dialog response generation using\nautomatic conversation evaluators. arXiv preprint arXiv:1904.\n13015\n32. Wei J, Bosma M, Zhao VY, Guu K, Yu AW, Lester B, Du N,\nDai AM, Le QV (2021) Finetuned language models are zero-\nshot learners. arXiv preprint arXiv:2109.01652\n1994 Neural Computing and Applications (2025) 37:1973–1997\n123\n33. Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin\nP, Zhang C, Agarwal S, Slama K, Ray A et al (2022) Training\nlanguage models to follow instructions with human feedback.\nAdv Neural Inf Process Syst 35:27730–27744\n34. Ye S, Kim D, Jang J, Shin J, Seo M (2022) Guess the instruc-\ntion! making language models stronger zero-shot learners. arXiv\npreprint arXiv:2210.02969\n35. Chen B, Shu C, Shareghi E, Collier N, Narasimhan K, Yao S\n(2023) Fireact: toward language agent ﬁne-tuning. arXiv pre-\nprint arXiv:2310.05915\n36. Wang Y, Mishra S, Alipoormolabashi P, Kordi Y, Mirzaei A,\nArunkumar A, Ashok A, Dhanasekaran AS, Naik A, Stap D,\net al (2022) Super-naturalinstructions: generalization via\ndeclarative instructions on 1600 ? NLP tasks. arXiv preprint\narXiv:2204.07705\n37. Chan S, Santoro A, Lampinen A, Wang J, Singh A, Richemond\nP, McClelland J, Hill F (2022) Data distributional properties\ndrive emergent in-context learning in Transformers. Adv Neural\nInf Process Syst 35:18878–18891\n38. Arora S, Ge R, Neyshabur B, Zhang Y (2018) Stronger gener-\nalization bounds for deep nets via a compression approach. In:\nInternational conference on machine learning. PMLR,\npp 254–263\n39. Lotﬁ S, Finzi M, Kapoor S, Potapczynski A, Goldblum M,\nWilson AG (2022) PAC-bayes compression bounds so tight that\nthey can explain generalization. Adv Neural Inf Process Syst\n35:31459–31473\n40. Bartlett P (1996) For valid generalization the size of the weights\nis more important than the size of the network. In: Advances in\nneural information processing systems, vol 9\n41. Wei C, Ma T (2019) Data-dependent sample complexity of deep\nneural networks via Lipschitz augmentation. In: Advances in\nneural information processing systems, vol 32\n42. Kawaguchi K, Kaelbling LP, Bengio Y (2017) Generalization in\ndeep learning. arXiv preprint arXiv:1710.05468\n43. Hochreiter S, Schmidhuber J (1997) Flat minima. Neural\nComput 9(1):1–42\n44. Bahri D, Mobahi H, Tay Y (2021) Sharpness-aware minimiza-\ntion improves language model generalization. arXiv preprint\narXiv:2110.08529\n45. Orvieto A, Kersting H, Proske F, Bach F, Lucchi A (2022)\nAnticorrelated noise injection for improved generalization. In:\nInternational conference on machine learning. PMLR,\npp 17094–17116\n46. Chatterjee S, Zielinski P (2022) On the generalization mystery\nin deep learning. arXiv preprint arXiv:2203.10036\n47. Bousquet O, Elisseeff A (2000) Algorithmic stability and gen-\neralization performance. In: Advances in neural information\nprocessing systems, vol 13\n48. Wolpert DH (1996) The lack of a priori distinctions between\nlearning algorithms. Neural Comput 8(7):1341–1390\n49. Solomonoff RJ (2009) Algorithmic probability: theory and\napplications. In: Information theory and statistical learning,\npp 1–23\n50. Goldblum M, Finzi M, Rowan K, Wilson AG (2023) The no free\nlunch theorem, Kolmogorov complexity, and the role of\ninductive biases in machine learning. arXiv preprint arXiv:2304.\n05366\n51. Valle-Perez G, Camargo CQ, Louis AA (2018) Deep learning\ngeneralizes because the parameter-function map is biased\ntowards simple functions. arXiv preprint arXiv:1805.08522\n52. Cover TM (1965) Geometrical and statistical properties of\nsystems of linear inequalities with applications in pattern\nrecognition. IEEE Trans Electron Comput 3:326–334\n53. Eldan R, Shamir O (2016) The power of depth for feedforward\nneural networks. In: Conference on learning theory. PMLR,\npp 907–940\n54. Gallicchio C, Scardapane S (2020) Deep randomized neural\nnetworks. In: Recent trends in learning from data: tutorials from\nthe INNS big data and deep learning conference\n(INNSBDDL2019). Springer, pp 43–68\n55. Hinton GE, et al (1986) Learning distributed representations of\nconcepts. In: Proceedings of the 8th annual conference of the\ncognitive science society, vol 1. Amherst, MA, p 12\n56. Von Oswald J, Niklasson E, Randazzo E, Sacramento J,\nMordvintsev A, Zhmoginov A., Vladymyrov M (2023) Trans-\nformers learn in-context by gradient descent. In: International\nconference on machine learning. PMLR, pp 35151–35174\n57. Xu K, Li J, Zhang M, Du SS, Kawarabayashi K-i, Jegelka S\n(2019) What can neural networks reason about? arXiv preprint\narXiv:1905.13211\n58. Scarselli F, Gori M, Tsoi AC, Hagenbuchner M, Monfardini G\n(2008) The graph neural network model. IEEE Trans Neural\nNetw 20(1):61–80\n59. Bellman R (1966) Dynamic programming. Science\n153(3731):34–37\n60. Ye W, Liu S, Kurutach T, Abbeel P, Gao Y (2021) Mastering\natari games with limited data. Adv Neural Inf Process Syst\n34:25476–25488\n61. Li J, Monroe W, Jurafsky D (2017) Learning to decode for\nfuture success. arXiv preprint arXiv:1701.06549\n62. Amos B, Kolter JZ (2017) Optnet: differentiable optimization as\na layer in neural networks. In: International conference on\nmachine learning. PMLR, pp 136–145\n63. Oswald J, Niklasson E, Schlegel M, Kobayashi S, Zucchet N,\nScherrer N, Miller N, Sandler M, Vladymyrov M, Pascanu R,\net al (2023) Uncovering mesa-optimization algorithms in\ntransformers. arXiv preprint arXiv:2309.05858\n64. Graves A (2016) Adaptive computation time for recurrent neural\nnetworks. arXiv preprint arXiv:1603.08983\n65. Banino A, Balaguer J, Blundell C (2021) Pondernet: learning to\nponder. arXiv preprint arXiv:2107.05407\n66. Dehghani M, Gouws S, Vinyals O, Uszkoreit J, Kaiser Ł (2018)\nUniversal transformers. arXiv preprint\narXiv:1807.03819\n67. Khandelwal U, Levy O, Jurafsky D, Zettlemoyer L, Lewis M\n(2019) Generalization through memorization: nearest neighbor\nlanguage models. arXiv preprint arXiv:1911.00172\n68. Guu K, Lee K, Tung Z, Pasupat P, Chang M (2020) Retrieval\naugmented language model pre-training. In: International con-\nference on machine learning. PMLR, pp 3929–3938\n69. Borgeaud S, Mensch A, Hoffmann J, Cai T, Rutherford E,\nMillican K, Van Den Driessche GB, Lespiau J-B, Damoc B,\nClark A, et al (2022) Improving language models by retrieving\nfrom trillions of tokens. In: International conference on machine\nlearning. PMLR, pp 2206–2240\n70. Wu Y, Rabe MN, Hutchins D, Szegedy C (2022) Memorizing\ntransformers. arXiv preprint arXiv:2203.08913\n71. Sukhbaatar S, Grave E, Lample G, Jegou H, Joulin A (2019)\nAugmenting self-attention with persistent memory. arXiv pre-\nprint arXiv:1907.01470\n72. Fu DY, Dao T, Saab KK, Thomas AW, Rudra A, Re ´ C (2022)\nHungry hungry hippos: towards language modeling with state\nspace models. arXiv preprint arXiv:2212.14052\n73. Elhage N, Hume T, Olsson C, Nanda N, Henighan T, Johnston\nS, ElShowk S, Joseph N, DasSarma N, Mann B, Hernandez D,\nAskell A, Ndousse K, Jones A, Drain D, Chen A, Bai Y, Ganguli\nD, Lovitt L, Hatﬁeld-Dodds Z, Kernion J, Conerly T, Kravec S,\nFort S, Kadavath S, Jacobson J, Tran-Johnson E, Kaplan J, Clark\nJ, Brown T, McCandlish S, Amodei D, Olah C (2022) Softmax\nNeural Computing and Applications (2025) 37:1973–1997 1995\n123\nlinear units. transformer circuits thread. https://transformer-cir\ncuits.pub/2022/solu/index.html\n74. Lamb A, He D, Goyal A, Ke G, Liao C-F, Ravanelli M, Bengio\nY (2021) Transformers with competitive ensembles of inde-\npendent mechanisms. arXiv preprint arXiv:2103.00336\n75. Graves A, Wayne G, Danihelka I (2014) Neural turing\nmachines. arXiv preprint arXiv:1410.5401\n76. Velicˇkovic´ P, Blundell C (2021) Neural algorithmic reasoning.\narXiv preprint arXiv:2105.02761\n77. Finzi M, Welling M, Wilson AG (2021) A practical method for\nconstructing equivariant multilayer perceptrons for arbitrary\nmatrix groups. In: International conference on machine learning,\npp 3318–3328. PMLR\n78. Finzi M, Benton G, Wilson AG (2021) Residual pathway priors\nfor soft equivariance constraints. Adv Neural Inf Process Syst\n34:30037–30049\n79. Mingard C, Valle-Pe´rez G, Skalse J, Louis AA (2021) Is SGD a\nBayesian sampler? Well, almost. J Mach Learn Res\n22(1):3579–3642\n80. Chiang P-y, Ni R, Miller DY, Bansal A, Geiping J, Goldblum\nM, Goldstein T (2022) Loss landscapes are all you need: neural\nnetwork generalization can be explained without the implicit\nbias of gradient descent. In: The 11th international conference\non learning representations\n81. Barak B, Edelman B, Goel S, Kakade S, Malach E, Zhang C\n(2022) Hidden progress in deep learning: SGD learns parities\nnear the computational limit. Adv Neural Inf Process Syst\n35:21750–21764\n82. Kalimeris D, Kaplun G, Nakkiran P, Edelman B, Yang T, Barak\nB, Zhang H (2019) SGD on neural networks learns functions of\nincreasing complexity. In: Advances in neural information\nprocessing systems, vol 32\n83. Mingard C, Skalse J, Valle-Pe´rez G, Martı´nez-Rubio D, Mikulik\nV, Louis AA (2019) Neural networks are a priori biased towards\nBoolean functions with low entropy. arXiv preprint arXiv:1909.\n11522\n84. Power A, Burda Y, Edwards H, Babuschkin I, Misra V (2022)\nGrokking: generalization beyond overﬁtting on small algorith-\nmic datasets. arXiv preprint arXiv:2201.02177\n85. Hu MY, Chen A, Saphra N, Cho K (2023) Latent state models of\ntraining dynamics. arXiv preprint arXiv:2308.09543\n86. Foret P, Kleiner A, Mobahi H, Neyshabur B (2020) Sharpness-\naware minimization for efﬁciently improving generalization.\narXiv preprint arXiv:2010.01412\n87. Chaudhari P, Choromanska A, Soatto S, LeCun Y, Baldassi C,\nBorgs C, Chayes J, Sagun L, Zecchina R (2019) Entropy-SGD:\nbiasing gradient descent into wide valleys. J Stat Mech Theory\nExp 2019(12):124018\n88. Ishida T, Yamane I, Sakai T, Niu G, Sugiyama M (2020) Do we\nneed zero training loss after achieving zero training error? arXiv\npreprint arXiv:2002.08709\n89. Yang J, Chen T, Zhu M, He F, Tao D, Liang Y, Wang Z (2023)\nLearning to generalize provably in learning to optimize. In:\nInternational conference on artiﬁcial intelligence and statistics,\npp 9807–9825. PMLR\n90. Liu Q, Zheng R, Rong B, Liu J, Liu Z, Cheng Z, Qiao L, Gui T,\nZhang Q, Huang X-J (2022) Flooding-X: improving BERT’s\nresistance to adversarial attacks via loss-restricted ﬁne-tuning.\nIn: Proceedings of the 60th annual meeting of the association for\ncomputational linguistics (Vol 1: Long Papers), pp 5634–5644\n91. Polyak BT, Juditsky AB (1992) Acceleration of stochastic\napproximation by averaging. SIAM J Control Optim\n30(4):838–855\n92. Izmailov P, Podoprikhin D, Garipov T, Vetrov D, Wilson AG\n(2018) Averaging weights leads to wider optima and better\ngeneralization. arXiv preprint arXiv:1803.05407\n93. Lu P, Kobyzev I, Rezagholizadeh M, Rashid A, Ghodsi A,\nLanglais P (2022) Improving generalization of pre-trained lan-\nguage models via stochastic weight averaging. arXiv preprint\narXiv:2212.05956\n94. Zhang M, Lucas J, Ba J, Hinton GE (2019) Lookahead opti-\nmizer: k steps forward, 1 step back. In: Advances in neural\ninformation processing systems, vol 32\n95. Zhang J, Liu S, Song J, Zhu T, Xu Z, Song M (2023) Look-\naround optimizer: k steps around, 1 step average. arXiv preprint\narXiv:2306.07684\n96. Mandt S, Hoffman MD, Blei DM (2017) Stochastic gradient\ndescent as approximate Bayesian inference. arXiv preprint\narXiv:1704.04289\n97. Goodfellow IJ, Shlens J, Szegedy C (2014) Explaining and\nharnessing adversarial examples. arXiv preprint arXiv:1412.\n6572\n98. Raghunathan A, Xie SM, Yang F, Duchi JC, Liang P (2019)\nAdversarial training can hurt generalization. arXiv preprint\narXiv:1906.06032\n99. Bubeck S, Lee YT, Price E, Razenshteyn I (2019) Adversarial\nexamples from computational constraints. In: International\nconference on machine learning. PMLR, pp 831–840\n100. Schmidt L, Santurkar S, Tsipras D, Talwar K, Madry A (2018)\nAdversarially robust generalization requires more data. In:\nAdvances in neural information processing systems, vol 31\n101. Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A (2017)\nTowards deep learning models resistant to adversarial attacks.\narXiv preprint arXiv:1706.06083\n102. Amini M-R, Feofanov V, Pauletto L, Devijver E, Maximov Y\n(2022) Self-training: a survey. arXiv preprint arXiv:2202.12040\n103. Huang J, Gu SS, Hou L, Wu Y, Wang X, Yu H, Han J (2022)\nLarge language models can self-improve. arXiv preprint arXiv:\n2210.11610\n104. Jung J, West P, Jiang L, Brahman F, Lu X, Fisher J, Sorensen T,\nChoi Y (2023) Impossible distillation: from low-quality model\nto high-quality dataset & model for summarization and para-\nphrasing. arXiv preprint arXiv:2305.16635\n105. Wang Y, Kordi Y, Mishra S, Liu A, Smith NA, Khashabi D,\nHajishirzi H (2022) Self-instruct: aligning language model with\nself generated instructions. arXiv preprint arXiv:2212.10560\n106. Silver D, Schrittwieser J, Simonyan K, Antonoglou I, Huang A,\nGuez A, Hubert T, Baker L, Lai M, Bolton A et al (2017)\nMastering the game of go without human knowledge. Nature\n550(7676):354–359\n107. Bricman P, Feeney T (2023) Elements of computational phi-\nlosophy. https://compphil.github.io/. Accessed 01 Nov 2023\n108. Chandra K, Xie A, Ragan-Kelley J, Meijer E (2022) Gradient\ndescent: the ultimate optimizer. NeurIPS\n109. Wu Y, Ren M, Liao R, Grosse R (2018) Understanding short-\nhorizon bias in stochastic meta-optimization. arXiv preprint\narXiv:1803.02021\n110. Almeida D, Winter C, Tang J, Zaremba W (2021) A general-\nizable approach to learning optimizers. arXiv preprint arXiv:\n2106.00958\n111. Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O (2017)\nProximal policy optimization algorithms. arXiv preprint arXiv:\n1707.06347\n112. Kirsch L, Schmidhuber J (2021) Meta learning backpropagation\nand improving it. Adv Neural Inf Process Syst 34:14122–14134\n113. Peebles W, Radosavovic I, Brooks T, Efros AA, Malik J (2022)\nLearning to learn with generative models of neural network\ncheckpoints. arXiv preprint arXiv:2209.12892\n114. Chen L, Lu K, Rajeswaran A, Lee K, Grover A, Laskin M,\nAbbeel P, Srinivas A, Mordatch I (2021) Decision transformer:\nreinforcement learning via sequence modeling. Adv Neural Inf\nProcess Syst 34:15084–15097\n1996 Neural Computing and Applications (2025) 37:1973–1997\n123\n115. Clark C, Yatskar M, Zettlemoyer L (2020) Learning to model\nand ignore dataset bias with mixed capacity ensembles. arXiv\npreprint arXiv:2011.03856\n116. Nam J, Cha H, Ahn S, Lee J, Shin J (2020) Learning from\nfailure: de-biasing classiﬁer from biased classiﬁer. Adv Neural\nInf Process Syst 33:20673–20684\n117. Malkin N, Wang Z, Jojic N (2021) Coherence boosting: when\nyour pretrained language model is not paying enough attention.\narXiv preprint arXiv:2110.08294\n118. Chuang Y-S, Xie Y, Luo H, Kim Y, Glass J, He P (2023) DoLa:\ndecoding by contrasting layers improves factuality in large\nlanguage models. arXiv preprint arXiv:2309.03883\n119. Teney D, Abbasnejad E, Lucey S, Hengel A (2022) Evading the\nsimplicity bias: training a diverse set of models discovers\nsolutions with superior OOD generalization. In: Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pp 16761–16772\n120. Pezeshki M, Kaba O, Bengio Y, Courville AC, Precup D, Lajoie\nG (2021) Gradient starvation: a learning proclivity in neural\nnetworks. Adv Neural Inf Process Syst 34:1256–1272\n121. Jacot A, Gabriel F, Hongler C (2018) Neural tangent kernel:\nconvergence and generalization in neural networks. In Advances\nin neural information processing systems, vol 31\n122. He H, Zha S, Wang H (2019) Unlearn dataset bias in natural\nlanguage inference by ﬁtting the residual. arXiv preprint arXiv:\n1908.10763\n123. Clark C, Yatskar M, Zettlemoyer L (2019) Don’t take the easy\nway out: ensemble based methods for avoiding known dataset\nbiases. arXiv preprint arXiv:1909.03683\n124. Touvron H, Cord M, Douze M, Massa F, Sablayrolles A, Je ´gou\nH (2021) Training data-efﬁcient image transformers & distilla-\ntion through attention. In: International conference on machine\nlearning. PMLR, pp 10347–10357\n125. Ren S, Gao Z, Hua T, Xue Z, Tian Y, He S, Zhao H (2022) Co-\nadvise: cross inductive bias distillation. In: Proceedings of the\nIEEE/CVF conference on computer vision and pattern recog-\nnition, pp 16773–16782\n126. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez\nAN, Kaiser Ł, Polosukhin I (2017) Attention is all you need. In:\nAdvances in neural information processing systems, vol 30\n127. Xi Z, Chen W, Guo X, He W, Ding Y, Hong B, Zhang M, Wang\nJ, Jin S, Zhou E, et al (2023) The rise and potential of large\nlanguage model based agents: a survey. arXiv preprint arXiv:\n2309.07864\n128. Wang L, Ma C, Feng X, Zhang Z, Yang H, Zhang J, Chen Z,\nTang J, Chen X, Lin Y, et al (2023) A survey on large language\nmodel based autonomous agents. arXiv preprint arXiv:2308.\n11432\n129. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal\nP, Neelakantan A, Shyam P, Sastry G, Askell A et al (2020)\nLanguage models are few-shot learners. Adv Neural Inf Process\nSyst 33:1877–1901\n130. Wei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, Le QV,\nZhou D et al (2022) Chain-of-thought prompting elicits rea-\nsoning in large language models. Adv Neural Inf Process Syst\n35:24824–24837\n131. Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y (2022) Large\nlanguage models are zero-shot reasoners. Adv Neural Inf Pro-\ncess Syst 35:22199–22213\n132. Jojic A, Wang Z, Jojic N (2023) GPT is becoming a Turing\nmachine: here are some ways to program it. arXiv preprint\narXiv:2303.14310\n133. Min S, Lewis M, Zettlemoyer L, Hajishirzi H (2021) MetaICL:\nlearning to learn in context. arXiv preprint arXiv:2110.15943\n134. Garg S, Tsipras D, Liang PS, Valiant G (2022) What can\nTransformers learn in-context? A case study of simple function\nclasses. Adv Neural Inf Process Syst 35:30583–30598\n135. Kirsch L, Harrison J, Sohl-Dickstein J, Metz L (2022) General-\npurpose in-context learning by meta-learning Transformers.\narXiv preprint arXiv:2212.04458\n136. Parisi A, Zhao Y, Fiedel N (2022) Talm: Tool augmented lan-\nguage models. arXiv preprint arXiv:2205.12255\n137. Komeili M, Shuster K, Weston J (2021) Internet-augmented\ndialogue generation. arXiv preprint arXiv:2107.07566\n138. Gao L, Madaan A, Zhou S, Alon U, Liu P, Yang Y, Callan J,\nNeubig G (2023) Pal: program-aided language models. In:\nInternational conference on machine learning. PMLR,\npp 10764–10799\n139. Schick T, Dwivedi-Yu J, Dessı` R, Raileanu R, Lomeli M, Zet-\ntlemoyer L, Cancedda N, Scialom T (2023) Toolformer: Lan-\nguage models can teach themselves to use tools. arXiv preprint\narXiv:2302.04761\n140. Patil SG, Zhang T, Wang X, Gonzalez JE (2023) Gorilla: Large\nlanguage model connected with massive apis. arXiv preprint\narXiv:2305.15334\n141. Liu B, Jiang Y, Zhang X, Liu Q, Zhang S, Biswas J, Stone P\n(2023) LLM ?P: Empowering large language models with\noptimal planning proﬁciency. arXiv preprint arXiv:2304.11477\n142. Zhou W, Jiang YE, Cui P, Wang T, Xiao Z, Hou Y, Cotterell R,\nSachan M (2023) RecurrentGPT: interactive generation of (ar-\nbitrarily) long text. arXiv preprint arXiv:2305.13304\n143. Packer C, Fang V, Patil SG, Lin K, Wooders S, Gonzalez JE\n(2023) MemGPT: towards LLMs as operating systems. arXiv\npreprint arXiv:2310.08560\n144. Chen H, Pasunuru R, Weston J, Celikyilmaz A (2023) Walking\ndown the memory maze: beyond context limit through interac-\ntive reading. arXiv preprint arXiv:2310.05029\n145. Wang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S,\nChowdhery A, Zhou D (2022) Self-consistency improves chain\nof thought reasoning in language models. arXiv preprint arXiv:\n2203.11171\n146. Yao S, Yu D, Zhao J, Shafran I, Grifﬁths TL, Cao Y, Nar-\nasimhan K (2023) Tree of thoughts: deliberate problem solving\nwith large language models. arXiv preprint arXiv:2305.10601\n147. Besta M, Blach N, Kubicek A, Gerstenberger R, Gianinazzi L,\nGajda J, Lehmann T, Podstawski M, Niewiadomski H, Nyczyk\nP, et al (2023) Graph of thoughts: Solving elaborate problems\nwith large language models. arXiv preprint arXiv:2308.09687\n148. Sel B, Al-Tawaha A, Khattar V, Wang L, Jia R, Jin M (2023)\nAlgorithm of thoughts: enhancing exploration of ideas in large\nlanguage models. arXiv preprint arXiv:2308.10379\n149. Madaan A, Tandon N, Gupta P, Hallinan S, Gao L, Wiegreffe S,\nAlon U, Dziri N, Prabhumoye, S, Yang Y, et al (2023) Self-\nreﬁne: iterative reﬁnement with self-feedback. arXiv preprint\narXiv:2303.17651\n150. Shinn N, Cassano F, Labash B, Gopinath A, Narasimhan K, Yao\nS (2023) Reﬂexion: language agents with verbal reinforcement\nlearning. arXiv preprint arXiv:2303.11366\n151. Zhao A, Huang D, Xu Q, Lin M, Liu Y-J, Huang G (2023)\nExpeL: LLM agents are experiential learners. arXiv preprint\narXiv:2308.10144\n152. Zelikman E, Lorch E, Mackey L, Kalai AT (2023) Self-taught\noptimizer (STOP): recursively self-improving code generation.\narXiv preprint arXiv:2310.02304\nPublisher’s Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nNeural Computing and Applications (2025) 37:1973–1997 1997\n123"
}