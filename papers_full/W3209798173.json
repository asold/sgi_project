{
  "title": "Facial Expression Recognition With Visual Transformers and Attentional Selective Fusion",
  "url": "https://openalex.org/W3209798173",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2941172193",
      "name": "Fuyan Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1996260172",
      "name": "Bin Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126219569",
      "name": "Shutao Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2787524669",
    "https://openalex.org/W2103943262",
    "https://openalex.org/W3112113890",
    "https://openalex.org/W2481681431",
    "https://openalex.org/W3089911443",
    "https://openalex.org/W2035372623",
    "https://openalex.org/W2948217907",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W4214669216",
    "https://openalex.org/W3035336958",
    "https://openalex.org/W6755106128",
    "https://openalex.org/W3003720578",
    "https://openalex.org/W6786361841",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W2745497104",
    "https://openalex.org/W2341528187",
    "https://openalex.org/W2799041689",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6787283782",
    "https://openalex.org/W6787906798",
    "https://openalex.org/W6786690992",
    "https://openalex.org/W6786708909",
    "https://openalex.org/W2944523338",
    "https://openalex.org/W2479639417",
    "https://openalex.org/W6787826751",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2889978276",
    "https://openalex.org/W1990207480",
    "https://openalex.org/W2904483377",
    "https://openalex.org/W6676019839",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W4238539449",
    "https://openalex.org/W6675224631",
    "https://openalex.org/W2563705555",
    "https://openalex.org/W3127463063",
    "https://openalex.org/W6632670727",
    "https://openalex.org/W2540801879",
    "https://openalex.org/W1986803802",
    "https://openalex.org/W3091862369",
    "https://openalex.org/W3006012949",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2106390385",
    "https://openalex.org/W4388315058",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2093422693",
    "https://openalex.org/W3019410392",
    "https://openalex.org/W3186105911",
    "https://openalex.org/W2902598059",
    "https://openalex.org/W3129458182",
    "https://openalex.org/W2060488580",
    "https://openalex.org/W3035565904",
    "https://openalex.org/W6725923168",
    "https://openalex.org/W3189369550",
    "https://openalex.org/W3088827635",
    "https://openalex.org/W2014185685",
    "https://openalex.org/W3134941379",
    "https://openalex.org/W2038317184",
    "https://openalex.org/W3154541964",
    "https://openalex.org/W2798553619",
    "https://openalex.org/W2806927661",
    "https://openalex.org/W2805613995",
    "https://openalex.org/W2896277673",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3106547440",
    "https://openalex.org/W3101998545",
    "https://openalex.org/W3109635183",
    "https://openalex.org/W3124054989",
    "https://openalex.org/W2894217452",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3122081138",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3117707723",
    "https://openalex.org/W1546411676",
    "https://openalex.org/W3114896399",
    "https://openalex.org/W2513140567",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W2101176070",
    "https://openalex.org/W2108113956",
    "https://openalex.org/W3180355996"
  ],
  "abstract": "Facial Expression Recognition (FER) in the wild is extremely challenging due\\nto occlusions, variant head poses, face deformation and motion blur under\\nunconstrained conditions. Although substantial progresses have been made in\\nautomatic FER in the past few decades, previous studies were mainly designed\\nfor lab-controlled FER. Real-world occlusions, variant head poses and other\\nissues definitely increase the difficulty of FER on account of these\\ninformation-deficient regions and complex backgrounds. Different from previous\\npure CNNs based methods, we argue that it is feasible and practical to\\ntranslate facial images into sequences of visual words and perform expression\\nrecognition from a global perspective. Therefore, we propose the Visual\\nTransformers with Feature Fusion (VTFF) to tackle FER in the wild by two main\\nsteps. First, we propose the attentional selective fusion (ASF) for leveraging\\ntwo kinds of feature maps generated by two-branch CNNs. The ASF captures\\ndiscriminative information by fusing multiple features with the global-local\\nattention. The fused feature maps are then flattened and projected into\\nsequences of visual words. Second, inspired by the success of Transformers in\\nnatural language processing, we propose to model relationships between these\\nvisual words with the global self-attention. The proposed method is evaluated\\non three public in-the-wild facial expression datasets (RAF-DB, FERPlus and\\nAffectNet). Under the same settings, extensive experiments demonstrate that our\\nmethod shows superior performance over other methods, setting new state of the\\nart on RAF-DB with 88.14%, FERPlus with 88.81% and AffectNet with 61.85%. The\\ncross-dataset evaluation on CK+ shows the promising generalization capability\\nof the proposed method.\\n",
  "full_text": "1\nFacial Expression Recognition with Visual\nTransformers and Attentional Selective Fusion\nFuyan Ma, Bin Sun, Member, IEEE,and Shutao Li, Fellow, IEEE\nAbstractâ€”Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face\ndeformation and motion blur under unconstrained conditions. Although substantial progresses have been made in automatic FER\nin the past few decades, previous studies were mainly designed for lab-controlled FER. Real-world occlusions, variant head poses\nand other issues deï¬nitely increase the difï¬culty of FER on account of these information-deï¬cient regions and complex backgrounds.\nDifferent from previous pure CNNs based methods, we argue that it is feasible and practical to translate facial images into sequences of\nvisual words and perform expression recognition from a global perspective. Therefore, we propose the Visual Transformers with Feature\nFusion (VTFF) to tackle FER in the wild by two main steps. First, we propose the attentional selective fusion (ASF) for leveraging two\nkinds of feature maps generated by two-branch CNNs. The ASF captures discriminative information by fusing multiple features with\nthe global-local attention. The fused feature maps are then ï¬‚attened and projected into sequences of visual words. Second, inspired\nby the success of Transformers in natural language processing, we propose to model relationships between these visual words with\nthe global self-attention. The proposed method is evaluated on three public in-the-wild facial expression datasets (RAF-DB, FERPlus\nand AffectNet). Under the same settings, extensive experiments demonstrate that our method shows superior performance over other\nmethods, setting new state of the art on RAF-DB with 88.14%, FERPlus with 88.81% and AffectNet with 61.85%. The cross-dataset\nevaluation on CK+ shows the promising generalization capability of the proposed method.\nIndex Termsâ€”Facial expression recognition in the wild, global-local attention, Transformers, global self-attention.\nF\n1 I NTRODUCTION\nU\nNDERSTANDING human emotional states is the funda-\nmental task to develop emotional intelligence, which\nis an interdisciplinary ï¬eld spanning from different research\nareas, such as psychology and computer science. Facial ex-\npression is one of the most natural, powerful and universal\nsignals for human beings to convey their emotional states\nand intentions [1], [2]. Facial expression recognition (FER)\nsystems have various applications, including human-robot\ninteraction (HRI), mental health assessment and driver fa-\ntigue monitoring. Therefore, numerous research endeavours\nhave been invested for promoting the development of FER.\nAs shown in Fig. 1, the challenges of FER in the wild\nmainly come from occlusions, variant head poses, face\ndeformation, motion blur, insufï¬cient qualitative data, etc.,\nwhich lead to signiï¬cant changes of the facial appearance.\nThese unexpected issues deï¬nitely increase the difï¬culty\nof expression recognition. With the advance of machine\nlearning, especially deep learning, researchers have made\ngreat progress on FER in the past decades. Before deep\nlearning era, traditional FER methods have mainly used\n\u000f This work is supported by the National Key Research and Development\nProject (2018YFB1305200), the National Natural Science Fund of China\n(61801178) and the Key-Area Research and Development Plan of Guang-\ndong Province (2018B010107001).\n\u000f Fuyan Ma and Bin Sun are with College of Electrical and Information\nEngineering, and with the Key Laboratory of Visual Perception and\nArtiï¬cial Intelligence of Hunan Province, Hunan University, Changsha,\n410082, China. (mafuyan@hnu.edu.cn; sunbin611@hnu.edu.cn)\n\u000f Shutao Li is with College of Electrical and Information Engineering,\nwith the State Key Laboratory of Advanced Design and Manufacturing\nfor Vehicle Body and with the Key Laboratory of Visual Perception and\nArtiï¬cial Intelligence of Hunan Province, Hunan University, Changsha,\n410082,China.(shutao_li@hnu.edu.cn)\nHappyNeuralSurpriseSadAngryDisgustFearContempt\nRAF-DBFERPlusAffectNetCK+\nFig. 1. The samples from RAF-DB, FERPlus, AffectNet and CK+. Variant\nhead poses, occlusions and other unconstrained conditions can be seen\nin above images. Note that RAF-DB is annotated with seven basic\nexpressions and FERPlus, AffectNet, CK+ are with eight expression\nlabels, including the contempt category.\nhandcrafted features and shallow learning (e.g., Histograms\nof Oriented Gradients (HOGs) [3], Local Binary Patterns\n(LBP) [4], [5], non-negative matrix factorization (NMF) [6]\nand sparse representation [7]). With the popularity of data-\ndriven techniques, deep learning based methods have ex-\nceeded traditional methods by a large margin and achieved\nstate-of-the-art FER performance (e.g., [8], [9], [10], [11]).\nDespite the signiï¬cant success of learned representation\nfor constrained FER, the performance of FER in the wild\nis still far from satisfactory. Majority of these proposed al-\ngorithms are implemented on lab-collected datasets, such as\nCK+ [12], MMI [13] and Oulu-CASIA [14]. These algorithms\nperform perfectly on these lab-collected FER datasets, be-\narXiv:2103.16854v3  [cs.CV]  22 Feb 2022\n2\ncause the controlled images are frontal with minimal il-\nlumination changes and limited occlusions. However, the\nperformance degrades dramatically on the real-world FER\ndatasets, such as RAF-DB [8], FERPlus [15] and Affect-\nNet [16]. Compared with the real-world FER datasets, the\nnumber of images from the lab-collected FER datasets is\nrelatively small. Especially, convolutional neural networks\n(CNNs) [17] for the FER in the wild usually require suf-\nï¬cient training face images to ensure generalizability for\nreal applications. Most publicly available datasets for FER\ndo not have a sufï¬cient quantity of images for training.\nTherefore, the performance of FER in the wild is limited by\nnot only the unconstrained conditions but also the available\ndata volume.\nDifferent backgrounds, illuminations and head poses are\nfairly common under the unconstrained conditions, which\nare irrelevant to facial expressions [18]. Directly recognizing\nexpression on such images is a big challenge. To remove\ncomplex backgrounds and non-face areas, it is indispensable\nto detect faces before training the deep neural network\nto learn meaningful features. Several face detectors like\nMTCNN [19] and Dlib [20] are used to detect faces in\ncomplex scenarios. After obtaining relatively accurate face\nregions, various methods have been proposed to improve\nthe FER accuracy and enhance the generalization ability of\nexpression recognition algorithms. Most of the researchers\ndesign their methods for occlusion-aware [21], [22] and\nmulti-view FER, which are two main obstacles for FER in\nreal-world scenarios. Very recently, attention models have\nbeen successfully applied for FER to explore meaningful\nregions. Li et al. [23] proposed a patch-gated CNN that in-\ntegrates path-level attention for expression recognition with\nocclusion. Besides, attention models have been successfully\napplied for FER to explore meaningful regions. Similar to\n[23], several methods, such as [10], [24], [25], also used\nattention-like mechanisms to focus on the most discrimina-\ntive features to improve FER accuracy. Non-frontal face im-\nages are overwhelmingly common in real-world scenarios.\nPrevious methods often treat expression recognition in non-\nfrontal face images as the multi-view FER problem. Zheng\n[26] proposed to select the optimal sub-regions of a face\nthat contribute most to the expression recognition based\non a group sparse reduced-rank regression. Liu et al. [27]\nproposed to tackle multi-view FER by three parts: the multi-\nchannel feature extraction, the multi-scale feature fusion and\nthe pose-aware recognition. In addition, generative adver-\nsarial networks (GAN) have also been applied for multi-\nview FER. GAN-based methods [28] [29] can synthesize\nface images with large head pose variations to enlarge the\ntraining set for FER. Especially, Zhang et al. [30] proposed\nan end to end model based on GAN for facial images\nsynthesize with a set of facial landmarks and pose-invariant\nfacial expression recognition by exploiting the geometry in-\nformation. Sun et al. [31] proposed cyclic image generation\nfor unsupervised cross-view facial expression recognition\nbased on GAN.\nTaking variant poses and occlusions in face images for\nexample, we illustrate the motivation of our method. Variant\nposes and the resize operation lead to face deformation\ncompared with frontal faces, which can be seen as out-\nof-order subregions. Occlusions in face images result in\ninformation-deï¬cient subregions. By analogy with natural\nlanguage, FER in the wild can be tackled in a different way.\nThere is an interesting phenomenon that we can understand\nthe sentences, i.e., â€œ I cdnâ€™uolt blveiee taht I cluod aulaclty un-\ndresatnd what I was rdanieg: the phaonmneel pweor of the hmuan\nmnid.â€ One of the most possible reasons is that the cog-\nnitive mode of us human beings usually impels ourselves\nto look and think about this sentence globally. Also like\nthe image description task in Natural Language Processing\n(NLP), we may use several visual words to describe the\nemotional state for an image. Inspired by this cognitive\nmode, we hypothesize that it is feasible and effective to\nrecognize the facial expressions by a sequence of visual\nwords from a global perspective. Therefore, we propose\nthe Visual Transformers with Feature Fusion (VTFF) for\nrobust facial expression recognition in the wild. Our method\ncombines LBP features and CNN features to further enrich\nthe representation of the visual words, referring to the\nhybrid feature extraction. The reason we use LBP features\nis that it can catch the small movements of the faces and\nextract image texture information. We design the attentional\nfeature fusion (ASF) to adaptively integrate LBP features\nand CNN features. The ASF aggregates both global and\nlocal relationships between two kinds of features, which can\neffectively improve the recognition performance. We simply\nconvert the fused feature maps into a sequence of visual\nwords by ï¬‚attening and projecting the features maps. After\nobtaining these visual words, we then exploit the multi-\nlayer Transformer encoder to boost the performance. The\nglobal self-attention in the multi-layer Transformer encoder\nallows the network to model the contextual information\nof the representative visual words and focus on the most\ndiscriminative features.\nOverall, the main contributions of our work can be\nsummarized as follows:\n1) We propose the Visual Transformers with feature\nfusion for FER in the wild, which integrates LBP\nfeatures and CNN features with the global-local\nattention and the global self-attention for improving\nexpression recognition accuracy.\n2) We design a simple but effective feature fusion\nmodule named ASF to aggregate both global and\nlocal facial information. Moreover, the ASF guides\nthe backbones to extract the required information\nwhile squeeze the useless information in an end-to-\nend manner.\n3) To the best of our knowledge, we are the ï¬rst to\napply Transformers for the FER. The global self-\nattention enables the whole network to learn the\nrelationships between elements of visual feature\nsequences and ignore the information-deï¬cient re-\ngions.\n4) Extensive experimental results on three publicly\navailable FER in the wild datasets, i.e., RAF-DB,\nFERPlus and AffectNet, demonstrate that our VTFF\nachieves state-of-the-art expression recognition per-\nformance. Especially, we also conduct experiments\non occlusion and variant pose subsets of these three\ndatasets and cross-dataset evaluation on CK+ to\nshow the promising generalization ability of our\n3\nmethod.\nThe rest of this article is organized as follows: Section\n2 brieï¬‚y reviews related works for FER in the wild and\nprovides a comprehensive review of recent advances in FER.\nThe core idea of our proposed VTFF is presented in Section\n3. Section 4 presents an extensive performance evaluation\nfor the proposed method and state-of-the-art approaches.\nWe conclude our work in Section 5.\n2 R ELATED WORK\n2.1 Facial Expression Recognition in the Wild\nFacial expression in the wild is signiï¬cantly challenging\nwhen facing unexpected conditions mentioned in Section 1.\nResearchers start to shift their attention from hand-crafted\nfeatures [3], [4], [5], [6], [7] to deep features [32], [33] for\naccurately extracting discriminative features. For example,\nTang [34] utilized the CNNs for feature extraction and\nreplaced the softmax layer with the linear SVMs, which gave\nsigniï¬cant gains and won the ICML 2013 Representation\nLearning Workshopâ€™s face expression recognition challenge.\nLater on, Li et al. [8] proposed to enhance the discriminative\npower of deep features by a deep locality-preserving CNN\n(DLP-CNN) method. These two approaches [34], [8] are\nremarkable, indicating the effectiveness of deep features\nfor FER. Meanwhile, region-based attention networks are\nespecially suitable for FER in the wild, because they allow\nfor salient face features to dynamically come to forefront\nwhen some occlusions or clutters occur in an image. In [10],\nWang et al. proposed region attention networks (RAN) to\ncapture the importance of facial regions for occlusion and\npose variant FER. Likewise, Li [9] et al. proposed a CNN\nwith attention mechanism for Occlusion Aware FER, which\nfocused on the most discriminative face regions. Similar\nwith [9], [10], our method utilizes the global self-attention\nmechanism to recognize facial expressions from a global\nperspective, which emphasizes the discriminative regions\nand generalizes well in case of occlusions or variant poses.\nApart from these attribute-unrelated methods, some FER\nalgorithms integrated demographic features (i.e., gender,\nrace, age, etc.) for improving the expression recognition\nperformance. It is worth mentioning that Fan [24] ï¬rstly\nproposed a deeply-supervised attention network, which\ntakes facial attributes into consideration. In addition, Xu\net al. [35] proposed to use the attribute information as\ninput to address bias and conducted a comparative study\nof the bias and fairness for FER. To further suppress the\nuncertainties containing in the datasets, Wang et al. [11]\nproposed a self-cure Network (SCN) for FER in the wild,\nwhich took full advantage of attention mechanism to weight\neach training face sample. These mainstream deep-learning\nbased FER works focus on directly adopting the CNNs\nfor extracting deep features, ignoring the carefully-designed\nfeatures (such as LBP) for FER and the inter-relationships\nwithin deep features.\n2.2 Feature Fusion for FER\nMost research works only use deep features to distinguish\nfacial expressions. As we mentioned in Section 1, there\nare various hand-crafted features, such as LBP [36] and\nHOGs [37], which have been well-developed for facial ex-\npression feature representation. In addition, feature fusion\ncan enrich the representative ability of the whole networks,\nwhich effectively improves the generalization ability and the\nrecognition performance [38], [39], [40]. Therefore, various\nfeature fusion based FER methods have been proposed to\nfurther improve the performance of expression recognition.\nChen et al. [41] proposed to fuse dynamic textures, geo-\nmetric features and acoustic features to tackle FER in the\nwild. The dynamic texture descriptor of visual information\nis an extension of HOGs, named Histogram of Oriented\nGradients from Three Orthogonal Planes (HOG-TOP). In\naddition, Shao and Qian [42] proposed a dual branch CNN\nto extracts LBP features and deep features parallel. They\nutilized the concatenation operation to fuse these feature\nmaps. Likewise, Li et al. [25] combined LBP features and\nCNN features with dense connection to improve expression\nrecognition accuracy. They did not well utilize comple-\nmentary information and abandon redundant information.\nAdditionally, these feature fusion methods were mainly con-\nducted on lab-controlled FER datasets, such as CK+ [12] and\nOulu-CASIA [14]. Base on these, We propose the attentional\nselective fusion (ASF) for integrating the LBP features and\nthe CNN features, which squeezes the useless information\nand generates correlated weight maps from both local and\nglobal perspectives.\n2.3 Transformers in Computer Vision\nApart from the CNNs-based feature representation tech-\nnique, Transformers [43] have shown dominant perfor-\nmance in NLP . Inspired by the success of Transformers,\nseveral researchers have tried to invest Transformers on\ncomputer vision tasks, such as object detection [44], pose\nestimation [45], high-resolution image synthesis [46], video\ninstance segmentation [47], trajectory prediction [48], etc.\nTransformer-based approaches have shown superior per-\nformance compared with CNN-based methods, when fully\ntrained on large-scale datasets. Vision Transformer (ViT) [49]\nwas the ï¬rst work to apply a vanilla Transformer to images\nwith few modiï¬cations. According to [49], ViT yielded lower\naccuracy compared with ResNet when trained on ImageNet\n[50]. ViT was ï¬rstly trained on large datasets, and then ï¬ne-\ntuned for downstream tasks, because Transformers need\namounts of data to generalize well on computer vision tasks.\nThe feature pyramid structure in CNNs has also been ap-\nplied with Transformers. Wang et al. [51] proposed Pyramid\nVision Transformer (PVT) for pixel-level dense prediction,\nwhich can work as the feature extraction backbone without\nconvolutions. In addition to these Transformers without\nconvolution operation, several works [52]â€“[54] proposed to\nblend convolutional layers into Transformers, which further\nimproved the performance of pure Transformers. Inspired\nby the vanilla Transformer and these brilliant Transformer-\nvariants, we ï¬rstly propose to directly apply Transformers\nfor FER. As far as we know, no works attempt to capture\nthe relationships among deep features for facial expression\nrecognition. We utilize Transformers to model long depen-\ndencies between input sequences by the global self-attention\nmechanism. Such global self-attention enables the model to\nignore the information-deï¬cient regions and recognize the\n4\nAttentional Selective Fusionâ€¦\nâ€¦\nâ€¦+Position EmbeddingNorm\nMulti-headSelf Attention\nNormMLPÃ—ğ‘ğ‘™\nFlat\nFCSoftmax\ny! y\" y#â€¦\ncls\nResNet18\nMulti-layer Transformer Encoder\nLinear\nğ¼ğ¿ğµğ‘ƒğ¼ğ‘…ğºğµ\nFig. 2. An overview of our proposed VTFF . It can be divided into three parts, visual words extraction, relationship modeling and expression\nclassiï¬cation. The pre-trained ResNet18 is used as the backbone to extract feature maps. All the extracted features are fused by our attentional\nselective fusion to get representative visual words. The input visual words are obtained by simply ï¬‚attening the spatial dimensions of the feature\nmaps and projecting to the speciï¬c dimension. And then apply the multi-layer Transformer encoder to model the relationships between different\nvisual features components. The network ï¬nally calculates the expression probabilities by a simple softmax function.\nConvLayer\nAveragePooling\n BatchNormalization\nReLULayer\nElement-wiseAdditionSigmoidBroadcastingElement-wiseAdditionElement-wiseMultiplication\n1-\nLocalAttention\nGlobalAttention\nFig. 3. The attentional selective fusion module.\nexpressions from a global perspective in case of occlusions\nor variant poses for FER.\n3 M ETHOD\n3.1 Overview\nFig. 2 illustrates the overall diagram of our Visual Trans-\nformers with feature fusion for facial expression recognition.\nOur VTFF is built upon on two pre-trained ResNet18 [32]\nnetworks, and consists of two crucial components: i) atten-\ntional selective fusion, ii) multi-Layer Transformer encoder.\nFor a given face image ğ¼ğ‘…ğºğµ with the size of ğ»\u0002ğ‘Š \u00023,\nwe ï¬rst get its LBP feature image with the size of ğ»\u0002ğ‘Š\u00021\nand concatenate it to a feature image ğ¼ğ¿ğµğ‘ƒ with the size of\nğ»\u0002ğ‘Š\u00023. The feature extraction backbones are composed of\ntwo ResNet18 networks: one is for the RGB image and the\nother is for its LBP feature image. Particularly, we employ\nthe ï¬rst ï¬ve stages of ResNet18 as the backbone to extract\nfeature maps ğ‘‹ğ¿ğµğ‘ƒ and ğ‘‹ğ‘…ğºğµ with the size of ğ»\nğ‘… \u0002ğ‘Š\nğ‘… \u0002ğ¶ğ‘“ ,\nwhere R is the downsampling rate of ResNet18, ğ¶ğ‘“ is the\nchannel number of the output of the stage 5. For simplicity,\nwe denote ğ»ğ‘‘ = ğ»\nğ‘… and ğ‘Šğ‘‘ = ğ‘Š\nğ‘… . In this paper, ğ‘… = 32 and\nğ» = ğ‘Š = 224. We initialize the whole network weights by\nthe pre-trained weights on MS-Celeb-1M face recognition\ndataset. Without loss of generalization, the ASF is utilized\nto combine the features extracted from the RGB image and\nthe features extracted from its LBP feature image, which\nwill be introduced in Section 3.2 in detail. The ASF mod-\nule dynamically adjusts the weights of these features and\nguides the networks focus more on discriminative features\nthat are vital for improving expression recognition. The\nfusion weights of the ASF are generated via the global-local\nattention, which aggregates global and local context for fur-\nther expression recognition. The size of fused feature maps\nğ‘‹ğ‘“ğ‘¢ğ‘ ğ‘’ğ‘‘ is also ğ»ğ‘‘\u0002ğ‘Šğ‘‘\u0002ğ¶ğ‘“ . Afterwards, we feed the ï¬‚attened\nfeatures to a linear projection and a learnable classiï¬cation\ntoken is added. We get embedded visual words with size of\nÂ¹ğ»ğ‘‘ğ‘Šğ‘‘Â¸1Âº\u0002ğ¶ğ‘, where ğ¶ğ‘ is the channel of ï¬‚attened features\nafter projection. We also add position embeddings to the\nembeddings to retain positional information, as [43] and [49]\ndo. The input embeddings are further fed to the Transformer\nencoder, which is composed of ğ‘ğ‘™ encoder blocks. Finally,\nthe probabilities of facial expressions are generated by a\nfully connected layer and the softmax function.\n3.2 Attentional Selective Fusion\nOur attentional selective fusion consists of global attention\nand local attention, as can be seen in Fig. 3, which can\nprovide additional ï¬‚exibility in fusing different types of\ninformation. As mentioned in the Section 3.1, given two\nfeature maps ğ‘‹ğ¿ğµğ‘ƒÂ–ğ‘‹ğ‘…ğºğµ 2Rğ»ğ‘‘\u0002ğ‘Šğ‘‘\u0002ğ¶ğ‘“ extracted from the\nbackbones, we ï¬rst fuse the LBP featuresğ‘‹ğ¿ğµğ‘ƒ and the CNN\nfeatures ğ‘‹ğ‘…ğºğµ for capturing the subsequent information\ninteraction:\nğ‘ˆ = ğ‘Šğ¿ğ‘‹ğ¿ğµğ‘ƒ Â¸ğ‘Šğ¶ğ‘‹ğ‘…ğºğµÂ– (1)\n5\nwhere ğ‘ˆ is the integrated feature maps after summation\nbetween ğ‘‹ğ¿ğµğ‘ƒ and ğ‘‹ğ‘…ğºğµ, and Â¸denotes element-wise sum-\nmation. ğ‘Šğ¿ and ğ‘Šğ¶ are the weights for initial integration\nand simply implemented by two 1 \u00021 convolutions.\nTo perform both global and local selective fusion, we\nthen choose global average pooling and the pixel-wise\nconvolution as global context and local context aggregator,\nrespectively. The global context progressively squeezes each\nfeature map of size ğ»ğ‘‘ \u0002ğ‘Šğ‘‘ into a scalar, and exploits\nthe inter-channel relationship of features. Different from the\nglobal context, the local context preserves and highlights the\nsubtle details of the input features, which is complementary\nto the global context. Aggregating local and global contexts\nfacilitates the network to beneï¬t from different types of\nfeatures and recognize ambiguous facial expressions more\naccurately. The global context and local context are com-\nputed as follows:\nğºÂ¹ğ‘ˆÂº= ğœÂ¹BNÂ¹ğ¶ğ‘œğ‘›ğ‘£2\nğºÂ¹ğ›¿Â¹BNÂ¹ğ¶ğ‘œğ‘›ğ‘£1\nğºÂ¹APÂ¹ğ‘ˆÂºÂºÂºÂºÂºÂºÂºÂ– (2)\nğ¿Â¹ğ‘ˆÂº= ğœÂ¹BNÂ¹ğ¶ğ‘œğ‘›ğ‘£2\nğ¿Â¹ğ›¿Â¹BNÂ¹ğ¶ğ‘œğ‘›ğ‘£1\nğ¿Â¹ğ‘ˆÂºÂºÂºÂºÂºÂºÂ• (3)\nAs Eq. (2) shows, the global adaptive average pooling AP is\nï¬rst applied to squeeze each feature map of ğ‘ˆ 2Rğ»ğ‘‘\u0002ğ‘Šğ‘‘\u0002ğ¶ğ‘“\ninto a scalar on the spatial dimension, which is carried out\nusing Eq. (4).\nAPÂ¹ğ‘ˆÂº= 1\nğ»ğ‘‘ğ‘Šğ‘‘\nğ»ğ‘‘âˆ‘ï¸\nğ‘–=1\nğ‘Šğ‘‘âˆ‘ï¸\nğ‘–=1\nğ‘¥ğ‘Â¹ğ‘–Â–ğ‘— ÂºÂ–ğ‘ = 1Â–2Â–Â•Â•Â•Â–ğ¶ ğ‘“ Â– (4)\nwhere ğ»ğ‘‘ and ğ‘Šğ‘‘ are the height and width of the input\nfeature map ğ‘ˆ, and ğ¶ğ‘“ is the number of channels of ğ‘ˆ. BN is\nthe Batch Normalization. ğ›¿denotes the ReLU function, and\nğœ denotes the Sigmoid function. The kernel sizes of ğ¶ğ‘œğ‘›ğ‘£1\nğº\nand ğ¶ğ‘œğ‘›ğ‘£2\nğº are ğ¶ğ‘“\nğ‘Ÿ \u0002ğ¶ğ‘“ \u00021\u00021, ğ¶ğ‘“ \u0002ğ¶ğ‘“\nğ‘Ÿ \u00021\u00021. And the kernel\nsizes of ğ¶ğ‘œğ‘›ğ‘£1\nğ¿, ğ¶ğ‘œğ‘›ğ‘£2\nğ¿ are ğ¶ğ‘“\nğ‘Ÿ \u0002ğ¶ğ‘“ \u00021 \u00021, 1 \u0002ğ¶ğ‘“\nğ‘Ÿ \u00021 \u00021.\nAlthough ğ¶ğ‘œğ‘›ğ‘£1\nğº and ğ¶ğ‘œğ‘›ğ‘£1\nğ¿ have the same kernel size, their\nfunctions on the inputs are signiï¬cantly different, especially\nwhen coupled with ğ¶ğ‘œğ‘›ğ‘£2\nğº and ğ¶ğ‘œğ‘›ğ‘£2\nğ¿. Speciï¬cally, ğ¶ğ‘œğ‘›ğ‘£1\nğº\nwith the size of ğ¶ğ‘“\nğ‘Ÿ \u0002ğ¶ğ‘“ \u00021 \u00021 squeezes its input with the\nreduction ratio ğ‘Ÿ. We set ğ‘Ÿ to 8 in this paper. And ğ¶ğ‘œğ‘›ğ‘£2\nğº\nincreases the channel dimensionality of its input and later\nresults in the global fusion weights ğºÂ¹ğ‘ˆÂº2 R1\u00021\u0002ğ¶ğ‘“ . There-\nfore, ğ¶ğ‘œğ‘›ğ‘£1\nğº and ğ¶ğ‘œğ‘›ğ‘£2\nğº are applied to capture the global\nrelationships of these features along the channel dimension.\nMeanwhile, ğ¶ğ‘œğ‘›ğ‘£1\nğ¿ squeezes ğ‘ˆ 2Rğ»ğ‘‘\u0002ğ‘Šğ‘‘\u0002ğ¶ğ‘“ into the shape\nof ğ»ğ‘‘\u0002ğ‘Šğ‘‘\u0002ğ¶ğ‘“\nğ‘Ÿ . And ğ¶ğ‘œğ‘›ğ‘£2\nğ¿ also squeezes the input along the\nchannel dimension while keeping the spatial dimensionality\nof its input constant. The shape of the ğ¶ğ‘œğ‘›ğ‘£2\nğ¿â€™s output is\nğ»ğ‘‘\u0002ğ‘Šğ‘‘\u00021. In other words, ğ¶ğ‘œğ‘›ğ‘£1\nğ¿ and ğ¶ğ‘œğ‘›ğ‘£2\nğ¿ progressively\nreduce the channel dimensionality of features maps to focus\non the discriminative information scattered locally.\nGiven the global fusion weights ğºÂ¹ğ‘ˆÂºand the local fu-\nsion weights ğ¿Â¹ğ‘ˆÂº, the reï¬ned global-local attention weights\ncan be obtained by Eq. (5).\nğºğ¿Â¹ğ‘ˆÂº= ğºÂ¹ğ‘ˆÂº\b ğ¿Â¹ğ‘ˆÂºÂ– (5)\nwhere \b represents the broadcasting addition. Then, the\nfused feature map ğ‘‹ğ‘“ğ‘¢ğ‘ ğ‘’ğ‘‘ is calculated by as follows:\nğ‘‹ğ‘“ğ‘¢ğ‘ ğ‘’ğ‘‘ =ğ‘‹ğ¿ğµğ‘ƒ \nğœÂ¹ğºğ¿Â¹ğ‘ˆÂºÂºÂ¸\nğ‘‹ğ‘…ğºğµ \nğœÂ¹1 \u0000ğºğ¿Â¹ğ‘ˆÂºÂºÂ– (6)\nwhere \nis the element-wise multiplication.\n3.3 Multi-Layer Transformer Encoder\nThe fused 2D feature map ğ‘‹ğ‘“ğ‘¢ğ‘ ğ‘’ğ‘‘ need to be ï¬‚attened into\na 1D visual embedding sequence, and further can be fed\nfor the multi-layer Transformer encoder as input. Therefore,\nwe reshape ğ‘‹ğ‘“ğ‘¢ğ‘ ğ‘’ğ‘‘ 2Rğ»ğ‘‘\u0002ğ‘Šğ‘‘\u0002ğ¶ğ‘“ into a ï¬‚attened sequence\nand feed it to a linear projection to get ğ‘‹ğ‘“ 2 Rğ»ğ‘‘ğ‘Šğ‘‘\u0002ğ¶ğ‘,\nwhere ğ»ğ‘‘ğ‘Šğ‘‘ is the sequence length and ğ¶ğ‘“ , ğ¶ğ‘ are set to\n512 and 768 respectively. As in [49], a classiï¬cation token\n[cls] is appended at the beginning of the input sequence\nğ‘‹ğ‘“ . The learnable state of the [cls] token at the output of\nthe Transformer encoder is utilized to represent the whole\nfeature sequence, which serves for the ï¬nal prediction. To\nincorporate the positional information in the multi-layer\nTransformer encoder, the 1D learnable positional embed-\ndings are added to the feature embeddings:\nğ‘0 = Â»ğ‘¥ğ‘ğ‘™ğ‘ ; ğ‘¥1\nğ‘“ ; ğ‘¥2\nğ‘“ ; ğ‘¥3\nğ‘“ ; Â•Â•Â• ; ğ‘¥ğ»ğ‘‘ğ‘Šğ‘‘\nğ‘“ Â¼Â¸ ğ‘ƒğ¸Â¹ğ»ğ‘‘ğ‘Šğ‘‘ Â¸1; ğ¶ğ‘ÂºÂ– (7)\nwhere ğ‘ƒğ¸Â¹ğ»ğ‘‘ğ‘Šğ‘‘ Â¸1; ğ¶ğ‘Âº2 RÂ¹ğ»ğ‘‘ğ‘Šğ‘‘Â¸1Âº\u0002ğ¶ğ‘ learns the embed-\ndings for each position index, [cls] token included, and ğ‘0\nrepresents the resulting position-aware feature sequence.\nTo model the complex interactions among all elements\nof the facial feature embeddings, we input ğ‘0 to the stan-\ndard multi-Layer Transformer encoder. The Transformer\nencoder calculates the weights of embeddings ğ‘0 through\nmulti-head self-attention (MHSA). This is done by learnable\nqueries ğ‘„, keys ğ¾, and values ğ‘‰. We compute the single-\nhead global self-attention(SHSA) using Eq. (8). Details of\nSHSA in the ï¬rst layer can be formulated as follows:\nâ„ğ‘’ğ‘ğ‘‘ğ‘— = ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› Â¹ğ‘„ğ‘—Â–ğ¾ğ‘—Â–ğ‘‰ğ‘—Âº\n= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ Â¹\nğ‘„ğ‘—ğ¾ğ‘‡\nğ‘—\np\nğ‘‘\nÂºğ‘‰ğ‘—\n= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ Â¹\nğ‘0ğ‘Šğ‘„\nğ‘— Â¹ğ‘0ğ‘Šğ¾\nğ‘— Âºğ‘‡\np\nğ‘‘\nÂºğ‘0ğ‘Šğ‘‰\nğ‘— Â–\n(8)\nwhere ğ‘„ğ‘— = ğ‘0ğ‘Šğ‘„\nğ‘— , ğ¾ğ‘— = ğ‘0ğ‘Šğ¾\nğ‘— , ğ‘‰ğ‘— = ğ‘0ğ‘Šğ‘‰\nğ‘— and ğ‘Šğ‘„\nğ‘— 2\nRğ¶ğ‘\u0002ğ‘‘, ğ‘Šğ¾\nğ‘— 2 Rğ¶ğ‘\u0002ğ‘‘, ğ‘Šğ‘‰\nğ‘— 2 Rğ¶ğ‘\u0002ğ‘‘ are the parameters of\nthese linear projections. Speciï¬cally, multi queries, keys and\nvalues project ğ‘0 into ğ‘â„ different representation subspaces.\nMulti-head self-attention (MHSA) can be described as:\nğ‘€ğ»ğ‘†ğ´ Â¹ğ‘0Âº= ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡Â¹â„ğ‘’ğ‘ğ‘‘1Â–Â•Â•Â•Â–â„ğ‘’ğ‘ğ‘‘ ğ‘â„Âºğ‘Šğ‘‚Â– (9)\nwhere ğ‘â„ is the number of different heads, and ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡\ndenotes the concatenation operation. ğ‘Šğ‘‚ 2 Râ„1\u0002ğ‘‘ are the\nparameters of a linear projection, where the dimension of\neach head ğ‘‘ is equal to ğ¶ğ‘\nğ‘â„\nand â„1 is the hidden size of\nthe ï¬rst layer. Each Transformer encoder consists of ğ‘ğ‘™\nlayers of MHSA blocks. Formally, the standard multi-layer\nTransformer encoder computes forwardly for ğ‘– = 1Â–Â•Â•Â•Â–ğ‘ ğ‘™\nlayers:\nË†ğ‘ğ‘– = ğ‘€ğ»ğ‘†ğ´ Â¹ğ¿ğ‘Â¹ğ‘ğ‘–\u00001ÂºÂºÂ¸ ğ‘ğ‘–\u00001 (10)\nğ‘ğ‘– = ğ‘€ğ¿ğ‘ƒÂ¹ğ¿ğ‘Â¹Ë†ğ‘ğ‘–ÂºÂºÂ¸ Ë†ğ‘ğ‘– (11)\nwhere Ë†ğ‘ğ‘– and ğ‘ğ‘– are intermediate output and ï¬nal output at\nlayer ğ‘–. The ğ‘€ğ¿ğ‘ƒ consists of two position-wise feed-forward\nlayers and a GELU non-linearity activation function. The\nğ¿ğ‘ denotes the layer normalization, which is applied before\nevery attention block and the ğ‘€ğ¿ğ‘ƒ. The hidden dimension\nof the ğ‘€ğ¿ğ‘ƒ is set to 3,072 in this paper. The outputğ‘ğ‘ğ‘™\n0 of the\n6\nlayer ğ‘ is also normalized by the ğ¿ğ‘. It is notable that we\njust apply the fully-connected layer for the ï¬nal [cls] token,\nwhich is further for calculating the expression probability\nscores. Mathematically, the probability scores are generated\nas follows:\nğ‘Œ = ğ¿ğ‘Â¹ğ‘ğ‘ğ‘™\n0 ÂºÂ– (12)\nğ‘¦ğ‘– = ğ‘’ğœƒğ‘‡\nğ‘– ğ‘Œ\nÃğ‘€\nğ‘–=1 ğ‘’ğœƒğ‘‡\nğ‘– ğ‘Œ Â– (13)\nwhere ğ‘ğ‘ğ‘™\n0 is the ï¬rst [cls] token of the whole sequence ğ‘ğ‘\nğ‘™\nand ğ‘Œ is the output of the Multi-layer Transformer encoder.\nğœƒrepresents the parameters of the fully-connected layer and\nğœƒğ‘– is the ğ‘–-th column of ğœƒ. The number of expression classes\nis ğ‘€. ğ‘¦ğ‘– is the probability score of the ğ‘–-th facial expression,\nand the ï¬nal predicted expressions can be easily obtained\nby arg max function during inference.\n4 E XPERIMENTS\nIn order to demonstrate the effectiveness of our proposed\nmethod, we carry out extensive experiments on three in-the-\nwild FER datasets (i.e., RAF-DB, FERPlus and AffectNet)\nand cross-dataset evaluation on CK+. In this section, we\nï¬rst introduce the FER datasets used in our experiments\nand implementation details. Then, the proposed method\nis compared with several state-of-the-art approaches. Sub-\nsequently, the impact of each component of the proposed\nVTFF model is investigated with experiments on these\ndatasets.\n4.1 Datasets\nWe evaluate our approach on three frequently used fa-\ncial expression datasets (RAF-DB, FERPlus and Affect-\nNet). These datasets are all collected in the wild, which\nmay suffer from different illuminations and occlusions.\nTo demonstrate the effectiveness of our method when\nhandling occlusion and variant pose issues in real-word\nconditions, we also conduct experiments on Occlusion-\nRAF-DB, Pose-RAF-DB, Occlusion-FERPlus, Pose-FERPlus,\nOcclusion-AffectNet, Pose-AffectNet, which are subsets of\nRAF-DB, FERPlus and AffectNet, respectively. The cross-\ndataset evaluation experiments are also conducted on CK+\nto verify the superior generalization ability of our method.\nThe details of the datasets used in the experiments are\nintroduced as follows.\nRAF-DB contains 29,672 real-world facial images col-\nlected from Flickr. The whole images of RAF-DB are labeled\nby 315 well-trained annotators and each image is labeled by\nabout 40 independent annotators. RAF-DB contains two dif-\nferent subsets: single-label subset and multi-label subset. In\nour experiments, we only use single-label subset, including\nseven basic emotions (neutral, happy, surprise, sad, angry,\ndisgust, fear). The images from the single-label subset are\nsplit into 12,271 training samples and 3,068 testing samples.\nThe expressions in both training images and test images\nhave imbalanced distribution. The overall sample accuracy\nis used for performance measurement.\nFERPlus is extended from the original FER2013 dataset,\nwhich was for the ICML 2013 Challenges in Representation\nLearning. FERPlus contains 28,709 training images and 3,589\ntest images, all of which are collected by the Google search\nengine. The original size of the images in FERPlus is 48 \u0002\n48. Each image in FERPlus is annotated by 10 annotators\nand FERPlus provides better quality labels than the original\nFER2013 labels. Apart from seven basic emotions as RAF-\nDB, the contempt category is included in the labels. We\nmainly report overall sample accuracy under the supervi-\nsion of majority voting for performance measurement.\nAffectNet is the largest facial expression datasets with\nmore than 1,000,000 facial images collected from the Inter-\nnet. AffectNet provides both discrete categorical and con-\ntinuous dimensional (i.e., valence and arousal) annotations.\nIt should be noted that AffectNet has imbalanced training,\nvalidation and test sets, of which 450,000 images have been\nannotated manually. In our experiment, we utilize images\nannotated with eight basic expressions as FERPlus, 287,652\nimages for training and 4,000 images for testing. Since the\ntest set is not available to the public, we mainly report\nmean class accuracy on the validation set for performance\nmeasurement and fair comparison with other methods.\nOcclusion and Pose Variant Datasets(i.e., Occlusion-\nRAF-DB, Pose-RAF-DB, Occlusion-FERPlus, Pose-FERPlus,\nOcclusion-AffectNet, Pose-AffectNet) are occlusion and\npose subsets of RAF-DB, FERPlus, AffectNet and manu-\nally collected by [10]. There are various occlusion types\noccurring in samples of these datasets, such as wearing\nmask, wearing glasses and objects in upper/bottom face.\nIn addition, variant pose issues can be divided into two\ncategories, poses larger than 30 degrees and poses larger\nthan 45 degrees. The sample distribution can be found\nin [10]. We report overall sample accuracy or mean class\naccuracy according to corresponding original datasets.\nCK+ is the extended Cohn-Kandade(CK) dataset for fa-\ncial action unit and expression recognition, collected under\na lab-collected environment. The original data of CK+ has\n593 video sequences from 123 subjects, 327 of which are\nannotated with seven basic emotions and contempt. Each of\nthe video sequences consists of images from onset (the ï¬rst\nframe) to peak expression (the last frame). We follow previ-\nous work to utilize the ï¬rst frame of a video sequence as a\nneutral sample and the last frame with the target expression.\nIn total, we obtain 618 images with seven emotions and 654\nimages with eight emotions for testing. We use the overall\nsample accuracy to evaluate the generalization capacity of\nour method.\n4.2 Implementation Details\nIn our experiments, the face images are detected by MTCNN\n[19] and further resized to the size of 224 \u0002224. For fair\ncomparison with previous state-of-the-art methods, we use\nthe same backbone ResNet18 pre-trained on the MS-Celeb-\n1M face recognition dataset. The facial features are extracted\nfrom the last convolutional stage of ResNet18. The learning\nrate of our methods is initialized as 0.005. We use a linear\nlearning rate warmup of 1,000 steps and cosine learning\nrate decay. The Adam optimizer [61] is used to optimize\nthe whole networks with a batch size of 32 and train the\nmodel for 20,000 steps on RAF and FERPlus, 40,000 steps\non AffectNet, respectively. The standard cross-entropy loss\nis utilized to supervise the model to generalize well for\n7\nTABLE 1\nComparison with state-of-the-art methods on RAF-DB. The best results are in bold.\nmethod Year Angry Disgust Fear Happy Sad Surprise Neutral Accuracy\nVGG [8] 2018 66.05 25.00 37.84 73.08 51.46 53.49 47.21 69.34\nbaseDCNN [8] 2018 70.99 52.50 50.00 92.91 77.82 79.64 83.09 82.66\nCenter Loss [8] 2018 68.52 53.13 54.05 93.08 78.45 79.63 83.24 82.86\nDLP-CNN [8] 2018 71.60 52.15 62.16 92.83 80.13 81.16 80.29 82.74\nFSN [55] 2018 72.80 46.90 56.80 90.50 81.60 81.80 76.90 81.14\ngACNN [9] 2018 - - - - - - - 85.07\nRAN [10] 2020 - - - - - - - 86.90\nSCN [11] 2020 - - - - - - - 87.03\nDSAN-VGG-RACE [24] 2020 82.71 56.25 58.11 94.01 83.89 89.06 80.00 85.37\nSPWFA-SE [56] 2020 80.00 59.00 59.00 93.00 84.00 88.00 86.00 86.31\nOurs 2021 85.80 68.12 64.86 94.09 87.24 85.41 87.50 88.14\nTABLE 2\nComparison with state-of-the-art methods\non FERPlus and AffectNet. The best results are in bold.\n(a) Results on FERPlus.\nMethod Year Accuracy\nCSLD [15] 2016 83.85\nResNet+VGG [57] 2017 87.4\nSHCNN [58] 2019 86.54\nLDR [59] 2020 87.6\nRAN\u0005[10] 2020 88.55\nRAN [10] 2020 87.85\nSCN [11] 2020 88.01\nOurs 2021 88.81\n(b) Results on AffectNet.\nMethod Year Accuracy\nIPA2LT [60] 2018 55.11\ngACNN [9] 2018 58.78\nSPWFA-SE [56] 2020 59.23\nRANy[10] 2020 52.97\nRAN [10] 2020 59.50\nSCN [11] 2020 60.23\nOursy 2021 56.13\nOurs 2021 61.85\nexpression recognition. We implement our method with\nPytorch [62] toolbox and conduct all the experiments on a\nsingle NVIDIA GTX 1080Ti GPU card.\n4.3 Comparison with State-of-the-art Methods\nThe proposed VTFF model is compared with several state-\nof-the-art methods on RAF-DB, FERPlus and AffectNet. We\nachieve new state-of-the-art results on these datasets to\nour knowledge. The better performance demonstrates the\nsuperiority of our proposed method.\nResults on RAF-DB: Comparison with other state-of-\nthe-art methods can be found in Table 1. The methods in [8]\npresented their performance using mean accuracy. For fair\ncomparison, we convert their results to accuracy, following\n[56]. Since gACNN [9], RAN [10] and SCN [11] did not\nreport the speciï¬c expression recognition accuracy or the\nconfusion matrices, the corresponding places are marked\nwith â€™-â€™ in Table 1. Although SPWFA-SE [56] also did not\nreport speciï¬c expression recognition accuracy, it provided\nthe confusion matrix on RAF-DB. Therefore, we borrow the\naccuracy results from its confusion matrix for comparison.\nOverall, our proposed method achieves 88.14% on RAF-\nDB. As shown in Table 1, our method achieves all the best\nresults among all methods except for the surprise category.\nIn detail, our VTFF has obtained gains of 18.8% and 1.11%\nover VGG and SCN, which are the baseline method and the\nprevious state-of-the-art method, respectively. DSAN-VGG-\nRACE integrated deeply-supervised blocks and attention\nblocks with race labels, which were additional data com-\npared with our purely used expression labels. Since RAF-\nDB also has extremely imbalanced distribution, the slight\nperformance decline of the surprise category is reasonable\nand acceptable. The accuracy of the disgust expression\nrecognition of our method has recorded an increase of\n9.12% compared with the previous best result of [56], which\ndemonstrates the effectiveness and the superiority of our\nmethod in feature learning.\nResults on FERPlus:Table 2(a) presents the comparison\nresults on FERPlus. We compare our models with CNN-\nbased methods including CSLD [15], ResNet+VGG [57],\nSHCNN [58], LDR [59], as well as two recent state-of-the-art\nmethods(RAN [10], SCN [11]). As we can see in Table 2(a),\nour proposed VTFF achieves 88.81% on FERPlus. Under the\nsame experiment settings, total improvements of our VTFF\non FERPlus are 0.96% and 0.80% when compared with RAN\nand SCN. Especially, RAN\u0005means the improved RAN with\nextra face alignment. Although face alignment is crucial for\nface recognition and facial expression recognition, it is a\npreprocessing step and CNN-based methods tend to rec-\nognize expression in an end-to-end manner. Even without\nface alignment in our experiment settings, our VTFF also\nachieves much better results over RAN \u0005.\nResults on AffectNet: We compare our method with\nseveral methods on AffectNet and report the results in Table\n2(b). We obtain 61.85% with oversampling on AffectNet,\nwithout bells and whistles. IPA2LT [60], gACNN [9] and\nSPWFA-SE [56] are trained for seven classes on AffectNet\nwithout the contempt category. As mentioned above, Affect-\nNet has imbalanced distribution. SPWFA-SE utilizes focal\nloss function to handle the imbalance problem. To deal\nwith the imbalance issue, as RAN [10] and SCN [11] do,\nwe adopt the oversampling strategy in our experiments.\nEspecially, method marked with y represents it is trained\nwithout the oversampling strategy. Our method without\nusing oversampling exceeds RAN y by 3.16%. In addition,\n8\nTABLE 3\nComparison with lately published methods on AffectNet-7. It is noted that the accuracy\nresults are borrowed from their confusion matrices. The best results are in bold.\nmethod Year Angry Disgust Fear Happy Sad Surprise Neutral Accuracy\nMFMP+ [63] 2021 55.00 46.00 53.00 88.00 55.00 55.00 64.00 58.86\nIDFL [64] 2021 31.00 65.00 49.00 95.00 59.00 43.00 73.00 59.20\nWSFER [65] 2021 58.54 30.28 50.42 88.01 67.56 51.90 73.55 60.04\nT21DST [66] 2021 18.00 40.00 53.00 96.00 62.00 62.00 79.00 60.12\nSDW [67] 2021 53.00 56.00 61.00 86.00 58.00 53.00 59.00 61.11\nReCNN [68] 2021 59.00 54.40 65.60 87.60 59.40 60.00 62.40 64.06\nOurs 2021 61.20 53.00 60.40 88.40 60.80 64.80 65.00 64.80\nTABLE 4\nComparison with other methods\non Occlusion and Pose Variant Datasets.\n(a) Results on Occlusion-RAF-DB, Pose-RAF-DB.\nMethod Occlusion Pose(30) Pose(45)\nBaseline [10] 80.19 84.04 83.15\nRAN [10] 82.72 86.74 85.20\nOurs 83.95 87.97 88.35\n(b) Results on Occlusion-FERPlus, Pose-FERPlus.\nMethod Occlusion Pose(30) Pose(45)\nBaseline [10] 73.33 78.11 75.50\nRAN [10] 83.63 82.23 80.40\nOurs 84.79 88.29 87.20\n(c) Results on Occlusion-AffectNet, Pose-AffectNet.\nMethod Occlusion Pose(30) Pose(45)\nBaseline [10] 49.48 50.10 48.50\nRAN [10] 58.50 53.90 53.19\nOurs 62.98 60.61 61.00\nTABLE 5\nCross-dataset evaluation results on CK+.\nMethod Train Test Accuracy\ngACNN [9] RAF-DB CK+ 81.07\nSPWFA-SE [56] RAF-DB CK+ 81.72\nSPWFA-SE [56] AffectNet-7 CK+ 85.44\nOurs RAF-DB CK+ 81.88\nOurs FERPlus CK+ 83.79\nOurs AffectNet-8 CK+ 86.24\nthe oversampling technique enhances RAN y as well as our\nmethod by 6.53% and 5.72% respectively, which demon-\nstrates the oversampling technique greatly eliminates the\nnegative effect of imbalanced class distribution. Our VTFF\nwith only AffectNet for training outperforms SCN by 1.62%,\nwhich applied extra dataset WebEmotion for pre-training\nand then ï¬ne-tuned SCN on AffectNet. The improvements\nof our VTFF over previous methods suggest that the VTFF\nindeed has better generalization ability even on large-scale\nexpression recognition datasets like AffectNet.\nTo further investigate our proposed VTFF on Affect-\nNet, we compare the performance of VTFF with the lately\npublished approaches including [63]â€“[68]. To make fair\ncomparison, we also train our model excluding the con-\ntempt category. Table 3 illustrates the detail comparison\nresults on AffectNet-7. It can be seen that our proposed\nmethod achieves superior performance in terms of mean\nclass accuracy compared with other methods with a 5.94%\nto 0.74% improvement. Additionally, the proposed method\nalso achieves the highest accuracies for the two facial expres-\nsions (Angry and Surprise) among these methods, which\nare 61.2% and 64.80% respectively. Xia et al. [68] propose\nthe Relation Convolutional Neural Network to focus on the\nmost discriminative facial regions, which achieves an accu-\nracy of 64.6% on average and the highest accuracy in Fear\n(64.6%). The IDFL [64] and the T21DST [66] achieve higher\naccuracies in Neural (79.00%, 73.00%) and Happy (96.00%,\n95.00%), but their performances in Angry (31.00%, 18.00%)\nand Fear (49.00%, 53.00%) are far from satisfactory. This is\nbecause they do not take the imbalanced class distribution\ninto consideration. Different from [64], [66], Hayale et al.\n[67] propose to set different penalization weights for classes\nbased on the proportions in the training set, achieving the\nhighest accuracy on Disgust (56.00%). Zhang et al. [65]\nleverage noisy data to boost the performance of FER and\nobtain the best result in Sad (67.56%). We can conclude from\nTable 3 that our proposed VTFF achieves competitive results\nand even outperforms the lately published methods, which\nshows the effectiveness of our VTFF.\nResults on Occlusion and Pose Variant Datasets:To\nexamine our method in case of occlusion and variant pose\nin real scenarios, we also conduct several experiments\non Occlusion-RAF-DB, Pose-RAF-DB, Occlusion-FERPlus,\nPose-FERPlus, Occlusion-AffectNet and Pose-AffectNet. Ta-\nble 4 shows the accuracy of the experimental results under\ncorresponding subsets. RAN [10] proposed to divide a face\nimage into subregions and introduced a region biased loss\nfor capturing the importance of different regions for occlu-\nsion and pose variant expression recognition. Although our\nVTFF is not speciï¬cally designed for occlusion and variant\npose FER issues, our method outperforms RAN with a large\nmargin in each case, which shows the superiority of our\nmethod. Speciï¬cally, our method exceeds RAN by 1.23%,\n1.16% and 4.48% on Occlusion-RAF-DB, Occlusion-FERPlus\nand Occlusion-AffectNet. Our method also outperforms\nRAN on Pose-RAF-DB, Pose-FERPlus and Pose-AffectNet.\nThe gains are 1.23%, 6.06% and 6.71% with pose larger than\n30 degrees. On Pose-FERPlus, Pose-AffectNet and Pose-\nRAF-DB with pose larger than 45 degrees, our method\nsigniï¬cantly outperforms RAN with the gains of 3.15%,\n6.8% and 7.81%, respectively. Overall, these results reliably\n9\nTABLE 6\nAblation study w.r.t. LBP features, ASF and MTE, performed on RAF-DB, FERPlus and AffectNet.\nIt is noted that we set ğ‘ğ‘™ = 4 and ğ‘â„ = 8 to explore the effects of other modules and conduct a fair ablation study.\nWe run the experiment three times over multiple random seeds, and then compute the mean and standard deviation of the recognition accuracy.\nSetting LBP ASF MTE RAF-DB FERPlus AffectNet\na % % % 86.37 \u00060.35 86.70 \u00060.24 58.45 \u00060.10\nb \" % % 86.53 \u00060.27 87.17 \u00060.30 58.65 \u00060.14\nc \" \" % 87.14 \u00060.39 87.55 \u00060.18 59.88 \u00060.21\nd % % \" 87.60 \u00060.12 87.65 \u00060.15 60.50 \u00060.09\ne \" % \" 87.83 \u00060.04 88.15 \u00060.23 60.90 \u00060.12\nf \" \" \" 88.19 \u00060.21 88.70 \u00060.17 61.52 \u00060.07\nverify the effectiveness of our method on occlusion and\nvariant pose issues. In addition, the superior performance\nconsists with our hypothesis that it is feasible and effective\nto recognize the facial expressions by a sequence of visual\nwords from a global perspective.\nResults on CK+: We also conduct cross-dataset eval-\nuation to verify the superior generalization ability of our\nmethod. Speciï¬cally, we ï¬rst train the network individu-\nally on RAF-DB, FERPlus as well as AffectNet, and then\nevaluate the model directly on CK+. Table 5 shows that\nour method achieves better performance than previous ap-\nproaches. Note that gACNN and SPWFA-SE are trained\nfor predicting seven basic emotions. Although our model\npredicts one more expression (contempt), which generally\nlowers down the ï¬nal results as shown in Table 3, the\nmodel trained on AffectNet gets an accuracy of 86.54%\non CK+. Compared with the gACNN and the SPWFA-\nSE, our method has better performance and increases by\n0.81% and 0.16%. The model trained on AffectNet achieves\nhigher accuracy than the ones trained on RAF-DB and\nFERPlus, because AffectNet is a relatively large scale dataset\nand contains more facial images. Table 5 demonstrates that\nour method has better generalization capacity and achieves\nbetter performance without exceptions.\n4.4 Ablation Study\nAs shown in Fig 2, our proposed method VTFF consists\nof LBP features with the attentional selective fusion (ASF)\nmodule and the multi-layer Transformer encoder (MTE).\nTo validate the effectiveness of these modules, we conduct\ncomparative experiments on RAF, FERPlus and AffectNet\nby discarding some parts of our VTFF. The detail settings\nof these experiments can are found in Table 6, where the\nsetting (a) represents the baseline method. Since the ASF is\nused to integrate the LBP features and the CNN features, it\ncan not be retained without the LBP module.\nEffectiveness of the LBP features for FER.Since our\nmethod begins with extracting the LBP features, we design\nthe ablation study to investigate the impact of LBP features\nfor FER. Taking results on RAF-DB for example, the baseline\nwithout any modiï¬cations on RAF-DB achieves 86.37% in\nterms of mean accuracy. Adding the LBP features on RAF-\nDB improves the baseline by 0.16%. Settings ( a, b) and\nsettings (d, e) demonstrate that integrating the LBP features\nimproves the baselines on FERPlus and AffectNet by 0.47%\nand 0.20%, which also suggests the LBP features are beneï¬-\ncial in improving expression recognition performance. This\nTABLE 7\nAblation study w.r.t ASF and the concatenation\nfusion on RAF-DB, FERPlus, and AffectNet(mean \u0006standard\ndeviation). \u0003denotes the method using pretrained weights, otherwise\nthe method is trained from scratch.\nMethod RAF-DB FERPlus AffectNet\nOurs w/ Concat 81.12 \u00060.25 83.97 \u00060.37 58.07 \u00060.15\nOurs\u0003w/ Concat 87.35 \u00060.39 86.86 \u00060.16 59.85 \u00060.10\nOurs w/ ASF 82.30 \u00060.31 84.71 \u00060.20 58.77 \u00060.09\nOurs\u0003w/ ASF 87.83 \u00060.04 88.15 \u00060.23 60.90 \u00060.12\ncan be explained by that the LBP features can extract texture\ninformation and reï¬‚ect ï¬ne facial changes, which show\nthe subtle differences of expressions. Nevertheless, directly\nusing additional the LBP features for FER is of limited use,\nbecause the simple addition fusion strategy is unsatisfactory\nfor combining the LBP features and the CNN features.\nEvaluation of the attentional selective fusion (ASF).To\nverify the effectiveness of the ASF for fusing LBP features\nand CNN features, we conduct experiments by replacing\nthe ASF with the simple element-wise addition. According\nto settings ( b, c) and settings ( e, f), the ASF leads to an\nincrease in recognition accuracy when fusing the LBP fea-\ntures and the CNN features, showing the effectiveness of\nthe proposed ASF. Speciï¬cally, we can see from settings ( b,\nc) that the designed ASF further improves the performance\nby 0.61%, 0.38% and 1.23%. According to the settings ( a, c)\nthe ASF improves the baseline setting ( a) by 0.76%, 0.85%\nand 1.43% with additional LBP features, respectively. The\nASF aggregates global and local contexts for fusing the LBP\nfeatures and the CNN features, which further improves the\nrecognition performance.\nIn addition, we compare our ASF with the concatenation\noperation. Concatenating ğ¿ğ¿ğµğ‘ƒ and ğ¿ğ‘…ğºğµ is actually a data-\nlevel fusion method while our attentional selective fusion\nworks at the feature-level. We conduct corresponding ex-\nperiments to verify whether the attentional selective fusion\nsurpasses the concatenation operation. To be speciï¬c, we\nadd an extra layer to reduce the shape ğ» \u0002ğ‘Š \u00024 into\nğ» \u0002ğ‘Š \u00023 to ï¬t the input for the pretrained ResNet. We\nï¬ne-tune the whole model and also train the model from\nscratch. From Table 7, we can conclude that our method\nusing the ASF signiï¬cantly improves the performance on all\ndatasets over that with the simple concatenation operation,\nwhich indicates that the feature-level fusion technique (i.e.,\n10\nTABLE 8\nAblation study w.r.t. the number of heads, the number of layers,\nperformed on RAF-DB, FERPlus and AffectNet. Bold values\ncorrespond to the best performance.\nSetting ğ‘ğ‘™ ğ‘â„ Params(M) RAF-DB FERPlus AffectNet\ni 4 4 51.8 87.45 88.46 61.08\nii 4 8 51.8 88.14 88.69 61.55\niii 4 12 51.8 87.52 88.65 61.85\niv 8 4 80.1 87.22 88.14 60.82\nv 8 8 80.1 87.61 88.81 61.23\nvi 8 12 80.1 87.48 88.52 61.30\nvii 12 4 108.5 87.23 88.20 60.85\nviii 12 8 108.5 87.29 88.52 60.45\nix 12 12 108.5 87.09 88.21 61.50\nTABLE 9\nComparison between ViT and our VTFF . The p-values of these\nmethods for RAF-DB, FERPlus and AffectNet are provided in the last\ntwo rows. The method marked with \u0003denotes it inherits the pretrained\nweights, otherwise the method is trained from scratch.\nSetting ğ‘ğ‘™ ğ‘â„ Params(M) RAF-DB FERPlus AffectNet\nViT 12 12 85.8 47.55 47.72 27.87\nViT\u0003 12 12 85.8 85.14 88.07 58.77\nOurs 4 8 51.8 82.27 84.80 58.75\nOurs\u0003 4 8 51.8 88.14 88.69 61.55\np-values - - - 3 Â•47 \u0002ğ‘’\u00006 7Â•30 \u0002ğ‘’\u00006 3Â•39 \u0002ğ‘’\u00006\np-values\u0003 - - - 6 Â•68 \u0002ğ‘’\u00005 1Â•18 \u0002ğ‘’\u00004 2Â•57 \u0002ğ‘’\u00005\nASF) surpasses the data-level fusion technique (i.e., concate-\nnation) under this condition.\nEffectiveness of the multi-layer Transformer encoder\n(MTE). To explore the impact of the MTE, we evaluate the\nperformance of the MTE in this part. We also compare the\neffects of the MTE on RAF-DB, FERPlus and AffectNet. It\nis worth to explain that we set the number of layers ğ‘ğ‘™ = 4\nand the number of heads ğ‘â„ = 8 in Table 6 to explore the\neffects of other modules and conduct a fair ablation study.\nFrom settings (a, d), settings (b, e) and settings (c, f), we can\nclearly see that the MTE greatly improves the performance.\nSpeciï¬cally, compared with the baseline setting ( a), the\nsetting (d) shows that integrating the MTE outperforms the\nbaseline by 1.23%, 0.85% and 2.05% on RAF-DB, FERPlus\nand AffectNet. Settings (a, b) and settings (b, c) demonstrate\nthat integrating the LBP features and the ASF improves\nthe baselines on RAF by 0.26% and 0.61% in terms of\nmean accuracy, respectively. According to the settings (a,\nd), employing the MTE outperforms the baseline method\nResNet18 by 1.23%. When keeping ğ‘ğ‘™ and ğ‘â„ constant in\nTable 6, we can conclude that the MTE contributes most to\nthe accuracy improvements over the LBP features and the\nASF. In addition, we infer that employing the MTE increases\nthe ability of learning discriminative features, outperform-\ning corresponding baselines.\nImpact of the number of layers and heads of the\nMTE. The multi-layer Transformer encoder consists of ğ‘ğ‘™\nidentical layers. The multi-head self-attention in each layer\nenables the model decompose the information into ğ‘â„\nrepresentation subspaces and jointly capture discrimina-\ntive information at different positions. The effect of MTE\nmodule is dependent on its hyperparameters, i.e., ğ‘ğ‘™ and\nTABLE 10\nTraining cost (GPU-days) of our\nmethod for RAF-DB, FERPlus and AffectNet.\nMethod RAF-DB FERPlus AffectNet\nOurs 0.06 0.07 0.36\nOurs\u0003 0.06 0.07 0.35\nğ‘â„. Therefore, we investigate the effect of MTE with dif-\nferent hyperparameter setttings from 4 to 12 on RAF-DB,\nFERPlus and AffectNet in Table 8. Table 8 compares the\nperformance of our method in terms of accuracy/mean\naccuracy and parameters. We observe that increasing the\nnumber of layers ğ‘ğ‘™ greatly burdens the parameters of the\nwhole network. Generally, smaller ğ‘ğ‘™ values tend to achieve\nbetter recognition performance, and larger ğ‘ğ‘™ values result\nin excessive parameters at the risk of overï¬tting. Note that\nthe smaller the value of ğ‘â„, the worse performance we may\nget, because there are not enough subspaces to learn latent\nrepresentations. Especially, it can be seen that the model\nproduces the best performance on RAF dataset with ğ‘ğ‘™ = 4\nand ğ‘â„ = 8. However, when ğ‘ğ‘™ = 12 and ğ‘â„ = 12, the MTE\nmay even bring negative effect and the model produces\nworse recognition accuracy on RAF dataset than the model\nwithout MTE as shown in Table 6. The reason is that the\nnumber of model parameters grows dramatically with the\ngrowth of ğ‘ğ‘™ and ğ‘â„ and the 108.5M parameters of the\nmodel with ğ‘ğ‘™ = 12 and ğ‘â„ = 12 may be too many to be\neffectively optimized by learning from the small dataset like\nRAF. On the larger datasets like AffectNet, such negative\neffect of MTE is not observed.\nImpact of the pre-trained weights. To ï¬nd out the\nimpact of pre-trained weights on our method, we train\nour model from scratch or ï¬ne-tune the model from pre-\ntrained ResNet18 weights. We also give a comparison of\nViT and our VTFF in Table 9. We implement ViT-Base [49]\non these three FER datasets and conduct experiments based\non default settings as [49] described. The ViT denotes that it\nis trained from scratch and ViT \u0003 represents we ï¬ne-tune it\nwith weights pre-trained on ImageNet-21k and ImageNet.\nFrom settings ViT\u0003and Ours\u0003, we can infer that our method\nachieves better performance but with fewer parameters.\nThe performance of ViT greatly drops when trained from\nscratch instead of ï¬ne-tuning, because the feature extraction\ncapacity of ViT is relatively limited without the guidance of\nlarge-scale datasets. Table 9 shows that ï¬ne-tuning models\nfrom pre-trained weights usually results in better perfor-\nmance. The training cost of our method for each dataset\ncan be seen in Table 10. Theoretically, the training cost for\nthe same method should be equal. However, in practice,\nthere exist slight differences between the experiments of the\nsame method, because some other programs may allocate\nthe GPU resource and the data I/O devices may work\ndifferently under various conditions.\nWe also provide a statistical evidence of the results to\nverify that the differences between them are statistically\nsigniï¬cant. The McNemarâ€™s Test is used to compare the\nrecognition accuracy of our proposed method and ViT. We\nformulate the null hypothesis that the performance of the\ntwo models are equal. Thus, the alternative hypothesis is\n11\nTABLE 11\nImpact of the ASF on ViT and our method. We run the experiment\nthree times on each dataset with random seeds and compute the mean\nand standard deviation of the recognition accuracy.\nMethod RAF-DB FERPlus AffectNet\nViT\u0003w/o ASF 85.07 \u00060.30 87.93 \u00060.52 58.70 \u00060.09\nViT\u0003w/ ASF 65.51 \u00060.39 69.03 \u00060.46 27.31 \u00060.18\nOurs\u0003w/o ASF 87.83 \u00060.04 88.15 \u00060.23 60.90 \u00060.12\nOurs\u0003w/ ASF 88.19 \u00060.21 88.70 \u00060.17 61.52 \u00060.07\ndeï¬ned as: one of the two models performs better than\nthe other. For the assessment of the statistical signiï¬cance,\nwe set the signiï¬cance threshold ğ›¼ = 0Â•05 to compute the\np-values for all experiments. The resulted p-values for all\ndatasets are reported in Table 9. According to these results,\nwe conclude that for all datasets the null hypothesis is\nrejected as the p-values were less than the signiï¬cance level\nof 0.05, and thus, the differences between ViT and our\nmethod are statistically signiï¬cant. It also can be inferred\nfrom Table 9 that the improvements obtained by our model\nare not due to chance.\nImpact of the ASF on ViT and our VTFF.To investigate\nthe impact of the ASF on ViT and our VTFF, we evaluate\nthe methods with and without the ASF. The experimental\nresults are shown in Table 11. Under the same data settings,\nthe vanilla ViT (ViT w/o ASF) does not work so well\ncompared with our method (Ours w/ ASF) on all datasets.\nWe think the reason can be summarized as follows: a)\nLarge numbers of parameters of ViT bring heavy burden\non relatively small FER datasets when ï¬ne-tuing. To some\nextent, this is consistent with the ablation study on different\nlayer values ğ‘ğ‘™ and the number of heads ğ‘â„. Larger ğ‘ğ‘™ and\nğ‘â„ may result in worse performance. b) The vanilla ViT is\ndesigned for FER-invariant tasks and takes nothing about\nFER into consideration, while our model integrates the\nResNet pretrained on MS-Celeb-1M face recognition dataset\nand designs the ASF module for the feature fusion. Our\nmethod not only inherits the global representation capacity\nof Transformers, but also consists of task-speciï¬c modules\nfor FER.\nHowever, directly applying the ASF to fuse the inputs for\nViT seems infeasible, because the ASF is a kind of feature-\nlevel fusion method. To be speciï¬c, the ASF performs global\nand local feature fusion along the channel dimension. The\nchannel number of our method is 512, while the channel\nnumber of the inputs of ViT is 3. The ASF may not be\neffective when fusing feature maps with such small channel\nnumber. We train the ViT with the attentional selective\nfusion on the inputs to verify our hypothesis. As we can see\nfrom the methods ViT w/o ASF and ViT w/ ASF in Table\n11, the performances of ViT with the ASF drop dramatically\ncompared with ViT without the ASF. In addition, it can be\nconcluded from the last two rows in Table 11 that our model\nembedded with the ASF further improves the recognition\nperformance, which demonstrates the effectiveness of the\nASF for feature-level fusion.\nAngry0.999Neural0.998Sad0.999\nNeural0.911SadAngry0.485Happy\nHappy1.000\nNeural0.999FearHappy0.907Happy0.999Happy0.966 Surprise0.444Angry\nHappy1.000Happy1.000 Happy0.999\nAngry0.531\nNeural0.991\nDisgust0.9483\nFear1.000\nFig. 4. An illustration of the learned attention maps. The ï¬rst row is to\nshow the raw images, the second row is the attention maps of the MTE,\nand the corresponding predictions or the labels are shown in the third\nrow. The annotations marked in green are the predicted expressions and\ntheir conï¬dence scores, while the others in orange are false predictions\nwith the ground-truth labels annotated.\n4.5 Visualization\nOur method computes relationships between visual words\nand captures discriminative features for expression recog-\nnition. Table 4 provides an empirical evidence to support\nthe effectiveness of our method. To visually demonstrate\nthe superior capacity of our method, the raw images and\ntheir corresponding attention weight maps are visualized in\nFig. 4. We randomly select several images from RAF-DB,\nFERPlus and AffectNet, and compute the attention weight\nmaps across all heads. The weights of all layers are then\nmultiplied recursively and projected into the input image\nspace.\nAs shown in Fig. 4, the weight maps shows that our\nmethod can identify occlusion regions and highlight dis-\ncriminative features. For example, the third raw image is oc-\ncluded with a ï¬nger on the left part, while our method pro-\nvides high attention weights on the right part. The weight\nmap of the eighth raw image contains high attention in the\nbottom, covering the mouth area. We also provide some\nfalse predictions in Fig. 4. The ï¬rst false-positive sample\nindicates that a man is blowing up a balloon about to burst\nand his eyes close out of fear. It shows that the detection pro-\ncess is a double-edged sword, which provides the precious\nface locations but excludes the background knowledge. The\ncartoon face is unfortunately predicted incorrectly because\nof limited cartoon faces in these datasets. The other two\nfalse-positive samples also indicate the external knowledge\nlike the social norms should be taken into consideration for\nthe future research work. From Fig. 4, We can conclude that\nour method can dynamically model the relationships among\nthese visual words and highlight discriminative regions to\nboost recognition performance.\n5 C ONCLUSION\nIn this paper, we present the Visual Transformers with\nFeature Fusion (VTFF) for facial expression recognition in\nthe wild. We propose to tackle the expression recognition\nproblem by translating facial images into sequences of vi-\nsual words and performing recognition from a global per-\nspective. To achieve these goals, we design the attentional\n12\nselective fusion to dynamically and adaptively combine\nLBP features and CNN features for improving recognition\naccuracy. The visual words are generated by ï¬‚attening and\nprojecting the fused feature maps. The multi-layer Trans-\nformer encoder utilizes the global self-attention mechanism\nto shift attention to discriminative visual words from a\nglobal perspective. The experimental results demonstrate\nthat the VTFF exceeds other state-of-the-art methods on\nthree frequently used facial expression datasets, i.e., RAF-\nDB, FERPlus, AffectNet. The cross-dataset evaluation on\nCK+ also demonstrates the promising generalization ability\nof our method.\nREFERENCES\n[1] C. Darwin and P . Prodger, The expression of the emotions in man and\nanimals. Oxford University Press, USA, 1998.\n[2] Y.-I. Tian, T. Kanade, and J. F. Cohn, â€œRecognizing action units for\nfacial expression analysis,â€ IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 23, no. 2, pp. 97â€“115, 2001.\n[3] N. Dalal and B. Triggs, â€œHistograms of oriented gradients for\nhuman detection,â€ in IEEE Conference on Computer Vision and\nPattern Recognition, vol. 1, 2005, pp. 886â€“893.\n[4] C. Shan, S. Gong, and P . W. McOwan, â€œRobust facial expression\nrecognition using local binary patterns,â€ in IEEE International\nConference on Image Processing, vol. 2, 2005, pp. IIâ€“370.\n[5] X. Feng, M. Pietikainen, and A. Hadid, â€œFacial expression recogni-\ntion with local binary patterns and linear programming,â€ Pattern\nRecognition And Image Analysis C/C of Raspoznavaniye Obrazov I\nAnaliz Izobrazhenii, vol. 15, no. 2, p. 546, 2005.\n[6] I. Buciu and I. Pitas, â€œApplication of non-negative and local non\nnegative matrix factorization to facial expression recognition,â€ in\nProceedings of the 17th International Conference on Pattern Recognition,\nvol. 1, 2004, pp. 288â€“291.\n[7] S. H. Lee, K. N. Plataniotis, and Y. M. Ro, â€œIntra-class variation re-\nduction using training expression images for sparse representation\nbased facial expression recognition,â€ IEEE Transactions on Affective\nComputing, vol. 5, no. 3, pp. 340â€“351, 2014.\n[8] S. Li and W. Deng, â€œReliable crowdsourcing and deep locality-\npreserving learning for unconstrained facial expression recogni-\ntion,â€ IEEE Transactions on Image Processing, vol. 28, no. 1, pp. 356â€“\n370, 2018.\n[9] Y. Li, J. Zeng, S. Shan, and X. Chen, â€œOcclusion aware facial ex-\npression recognition using cnn with attention mechanism,â€ IEEE\nTransactions on Image Processing, vol. 28, no. 5, pp. 2439â€“2450, 2018.\n[10] K. Wang, X. Peng, J. Yang, D. Meng, and Y. Qiao, â€œRegion attention\nnetworks for pose and occlusion robust facial expression recogni-\ntion,â€ IEEE Transactions on Image Processing, vol. 29, pp. 4057â€“4069,\n2020.\n[11] K. Wang, X. Peng, J. Yang, S. Lu, and Y. Qiao, â€œSuppressing uncer-\ntainties for large-scale facial expression recognition,â€ inProceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 6897â€“6906.\n[12] P . Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and\nI. Matthews, â€œThe extended cohn-kanade dataset (ck+): A com-\nplete dataset for action unit and emotion-speciï¬ed expression,â€\nin IEEE Conference on Computer Vision and Pattern Recognition-\nworkshops, 2010, pp. 94â€“101.\n[13] M. Valstar and M. Pantic, â€œInduced disgust, happiness and sur-\nprise: an addition to the mmi facial expression database,â€ in Proc.\n3rd Intern. Workshop on EMOTION (satellite of LREC): Corpora for\nResearch on Emotion and Affect, 2010, pp. 65â€“70.\n[14] G. Zhao, X. Huang, M. Taini, S. Z. Li, and M. PietikÃ¤Inen, â€œFa-\ncial expression recognition from near-infrared videos,â€ Image and\nVision Computing, vol. 29, no. 9, pp. 607â€“619, 2011.\n[15] E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang, â€œTraining deep\nnetworks for facial expression recognition with crowd-sourced\nlabel distribution,â€ in Proceedings of the 18th ACM International\nConference on Multimodal Interaction, 2016, pp. 279â€“283.\n[16] A. Mollahosseini, B. Hasani, and M. H. Mahoor, â€œAffectnet: A\ndatabase for facial expression, valence, and arousal computing in\nthe wild,â€ IEEE Transactions on Affective Computing , vol. 10, no. 1,\npp. 18â€“31, 2017.\n[17] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\nW. Hubbard, and L. D. Jackel, â€œBackpropagation applied to hand-\nwritten zip code recognition,â€ Neural Computation, vol. 1, no. 4, pp.\n541â€“551, 1989.\n[18] S. Li and W. Deng, â€œDeep facial expression recognition: A\nsurvey,â€ IEEE Transactions on Affective Computing , 2020, doi:\n10.1109/TAFFC.2020.2981446.\n[19] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, â€œJoint face detection\nand alignment using multitask cascaded convolutional networks,â€\nIEEE Signal Processing Letters, vol. 23, no. 10, pp. 1499â€“1503, 2016.\n[20] B. Amos, B. Ludwiczuk, M. Satyanarayanan et al. , â€œOpenface:\nA general-purpose face recognition library with mobile applica-\ntions,â€ CMU School of Computer Science, vol. 6, no. 2, 2016.\n[21] F. Bourel, C. C. Chibelushi, and A. A. Low, â€œRecognition of facial\nexpressions in the presence of occlusion.â€ inProcedings of the British\nMachine Vision Conference, 2001, pp. 1â€“10.\n[22] S. Happy and A. Routray, â€œAutomatic facial expression recogni-\ntion using features of salient facial patches,â€ IEEE Transactions on\nAffective Computing, vol. 6, no. 1, pp. 1â€“12, 2014.\n[23] Y. Li, J. Zeng, S. Shan, and X. Chen, â€œPatch-gated cnn for occlusion-\naware facial expression recognition,â€ in 24th International Confer-\nence on Pattern Recognition, 2018, pp. 2209â€“2214.\n[24] Y. Fan, V . Li, and J. C. Lam, â€œFacial expression recognition with\ndeeply-supervised attention network,â€ IEEE Transactions on Affec-\ntive Computing, 2020, doi: 10.1109/TAFFC.2020.2988264.\n[25] J. Li, K. Jin, D. Zhou, N. Kubota, and Z. Ju, â€œAttention mechanism-\nbased cnn for facial expression recognition,â€ Neurocomputing, vol.\n411, pp. 340â€“350, 2020.\n[26] W. Zheng, â€œMulti-view facial expression recognition based on\ngroup sparse reduced-rank regression,â€ IEEE Transactions on Af-\nfective Computing, vol. 5, no. 1, pp. 71â€“85, 2014.\n[27] Y. Liu, J. Zeng, S. Shan, and Z. Zheng, â€œMulti-channel pose-\naware convolution neural networks for multi-view facial expres-\nsion recognition,â€ in 2018 13th IEEE International Conference on\nAutomatic Face & Gesture Recognition (FG 2018) . IEEE, 2018, pp.\n458â€“465.\n[28] F. Zhang, T. Zhang, Q. Mao, and C. Xu, â€œJoint pose and expression\nmodeling for facial expression recognition,â€ in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition , June\n2018.\n[29] Y.-H. Lai and S.-H. Lai, â€œEmotion-preserving representation learn-\ning via generative adversarial network for multi-view facial ex-\npression recognition,â€ in 2018 13th IEEE International Conference on\nAutomatic Face & Gesture Recognition (FG 2018) . IEEE, 2018, pp.\n263â€“270.\n[30] F. Zhang, T. Zhang, Q. Mao, and C. Xu, â€œGeometry guided pose-\ninvariant facial expression recognition,â€IEEE Transactions on Image\nProcessing, vol. 29, pp. 4445â€“4460, 2020.\n[31] N. Sun, Q. Lu, W. Zheng, J. Liu, and G. Han, â€œUnsupervised cross-\nview facial expression image generation and recognition,â€ IEEE\nTransactions on Affective Computing, 2020.\n[32] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning\nfor image recognition,â€ in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2016, pp. 770â€“778.\n[33] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger,\nâ€œDensely connected convolutional networks,â€ in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition , 2017,\npp. 4700â€“4708.\n[34] Y. Tang, â€œDeep learning using linear support vector machines,â€\narXiv preprint arXiv:1306.0239, 2013.\n[35] T. Xu, J. White, S. Kalkan, and H. Gunes, â€œInvestigating bias and\nfairness in facial expression recognition,â€ inEuropean Conference on\nComputer Vision, 2020, pp. 506â€“523.\n[36] S. Moore and R. Bowden, â€œLocal binary patterns for multi-view\nfacial expression recognition,â€ Computer Vision and Image Under-\nstanding, vol. 115, no. 4, pp. 541â€“558, 2011.\n[37] Y. Hu, Z. Zeng, L. Yin, X. Wei, X. Zhou, and T. S. Huang, â€œMulti-\nview facial expression recognition,â€ in 2008 8th IEEE International\nConference on Automatic Face & Gesture Recognition , 2008, pp. 1â€“6.\n[38] K.-H. Pong and K.-M. Lam, â€œMulti-resolution feature fusion for\nface recognition,â€ Pattern Recognition, vol. 47, no. 2, pp. 556â€“567,\n2014.\n[39] T.-Y. Lin, P . DollÃ¡r, R. Girshick, K. He, B. Hariharan, and S. Be-\nlongie, â€œFeature pyramid networks for object detection,â€ in Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2017, pp. 2117â€“2125.\n13\n[40] G. Lin, A. Milan, C. Shen, and I. Reid, â€œReï¬nenet: Multi-path\nreï¬nement networks for high-resolution semantic segmentation,â€\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2017, pp. 1925â€“1934.\n[41] J. Chen, Z. Chen, Z. Chi, and H. Fu, â€œFacial expression recogni-\ntion in video with multiple feature fusion,â€ IEEE Transactions on\nAffective Computing, vol. 9, no. 1, pp. 38â€“50, 2016.\n[42] J. Shao and Y. Qian, â€œThree convolutional neural network models\nfor facial expression recognition in the wild,â€ Neurocomputing, vol.\n355, pp. 82â€“92, 2019.\n[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€\narXiv preprint arXiv:1706.03762, 2017.\n[44] J. Beal, E. Kim, E. Tzeng, D. H. Park, A. Zhai, and D. Kislyuk,\nâ€œToward transformer-based object detection,â€ arXiv preprint\narXiv:2012.09958, 2020.\n[45] S. Yang, Z. Quan, M. Nie, and W. Yang, â€œTranspose: Towards\nexplainable human pose estimation by transformer,â€arXiv preprint\narXiv:2012.14214, 2020.\n[46] P . Esser, R. Rombach, and B. Ommer, â€œTaming transformers for\nhigh-resolution image synthesis,â€ arXiv preprint arXiv:2012.09841 ,\n2020.\n[47] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and\nH. Xia, â€œEnd-to-end video instance segmentation with transform-\ners,â€ arXiv preprint arXiv:2011.14503, 2020.\n[48] M. Bhat, J. Francis, and J. Oh, â€œTrajformer: Trajectory prediction\nwith local self-attentive contexts for autonomous driving,â€ arXiv\npreprint arXiv:2011.14910, 2020.\n[49] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly\net al. , â€œAn image is worth 16x16 words: Transformers for image\nrecognition at scale,â€ arXiv preprint arXiv:2010.11929, 2020.\n[50] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,\nâ€œImagenet: A large-scale hierarchical image database,â€ in IEEE\nConference on Computer Vision and Pattern Recognition , 2009, pp.\n248â€“255.\n[51] W. Wang, E. Xie, X. Li, D.-P . Fan, K. Song, D. Liang, T. Lu, P . Luo,\nand L. Shao, â€œPyramid vision transformer: A versatile back-\nbone for dense prediction without convolutions,â€ arXiv preprint\narXiv:2102.12122, 2021.\n[52] Z. Chen, L. Xie, J. Niu, X. Liu, L. Wei, and Q. Tian, â€œVisformer: The\nvision-friendly transformer,â€ arXiv preprint arXiv:2104.12533, 2021.\n[53] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\nâ€œCvt: Introducing convolutions to vision transformers,â€ arXiv\npreprint arXiv:2103.15808, 2021.\n[54] Z. Liu, S. Luo, W. Li, J. Lu, Y. Wu, C. Li, and L. Yang, â€œConvtrans-\nformer: A convolutional transformer network for video frame\nsynthesis,â€ arXiv preprint arXiv:2011.10185, 2020.\n[55] S. Zhao, H. Cai, H. Liu, J. Zhang, and S. Chen, â€œFeature selection\nmechanism in cnns for facial expression recognition.â€ inProcedings\nof the British Machine Vision Conference, 2018, p. 317.\n[56] Y. Li, G. Lu, J. Li, Z. Zhang, and D. Zhang, â€œFacial expression\nrecognition in the wild using multi-level features and attention\nmechanisms,â€ IEEE Transactions on Affective Computing, 2020.\n[57] C. Huang, â€œCombining convolutional neural networks for emo-\ntion recognition,â€ in IEEE MIT Undergraduate Research Technology\nConference (URTC), 2017, pp. 1â€“4.\n[58] S. Miao, H. Xu, Z. Han, and Y. Zhu, â€œRecognizing facial expres-\nsions using a shallow convolutional neural network,â€ IEEE Access,\nvol. 7, pp. 78 000â€“78 011, 2019.\n[59] X. Fan, Z. Deng, K. Wang, X. Peng, and Y. Qiao, â€œLearning dis-\ncriminative representation for facial expression recognition from\nuncertainties,â€ in 2020 IEEE International Conference on Image Pro-\ncessing, 2020, pp. 903â€“907.\n[60] J. Zeng, S. Shan, and X. Chen, â€œFacial expression recognition with\ninconsistently annotated datasets,â€ in Proceedings of the European\nConference on Computer Vision, 2018, pp. 222â€“237.\n[61] D. P . Kingma and J. Ba, â€œAdam: A method for stochastic optimiza-\ntion,â€ arXiv preprint arXiv:1412.6980, 2014.\n[62] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , â€œPytorch: An\nimperative style, high-performance deep learning library,â€ arXiv\npreprint arXiv:1912.01703, 2019.\n[63] S. Happy, A. Dantcheva, and F. Bremond, â€œExpression recognition\nwith deep features extracted from holistic and part-based models,â€\nImage and Vision Computing, vol. 105, p. 104038, 2021.\n[64] Y. Li, Y. Lu, B. Chen, Z. Zhang, J. Li, G. Lu, and D. Zhang, â€œLearn-\ning informative and discriminative features for facial expression\nrecognition in the wild,â€ IEEE Transactions on Circuits and Systems\nfor Video Technology, 2021.\n[65] F. Zhang, M. Xu, and C. Xu, â€œWeakly-supervised facial expression\nrecognition in the wild with noisy data,â€ IEEE Transactions on\nMultimedia, 2021.\n[66] W. Xie, H. Wu, Y. Tian, M. Bai, and L. Shen, â€œTriplet loss with\nmultistage outlier suppression and class-pair margins for facial\nexpression recognition,â€ IEEE Transactions on Circuits and Systems\nfor Video Technology, 2021.\n[67] W. Hayale, P . S. Negi, and M. Mahoor, â€œDeep siamese neural\nnetworks for facial expression recognition in the wild,â€ IEEE\nTransactions on Affective Computing, 2021.\n[68] Y. Xia, H. Yu, X. Wang, M. Jian, and F.-Y. Wang, â€œRelation-aware\nfacial expression recognition,â€ IEEE Transactions on Cognitive and\nDevelopmental Systems, 2021.",
  "topic": "Discriminative model",
  "concepts": [
    {
      "name": "Discriminative model",
      "score": 0.7493268251419067
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7062506079673767
    },
    {
      "name": "Computer science",
      "score": 0.6395127177238464
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5784709453582764
    },
    {
      "name": "Facial expression",
      "score": 0.563066840171814
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4878992736339569
    },
    {
      "name": "Fusion",
      "score": 0.44405877590179443
    },
    {
      "name": "Computer vision",
      "score": 0.4374341368675232
    },
    {
      "name": "Facial expression recognition",
      "score": 0.4338092803955078
    },
    {
      "name": "Transformer",
      "score": 0.4209522008895874
    },
    {
      "name": "Speech recognition",
      "score": 0.36835578083992004
    },
    {
      "name": "Facial recognition system",
      "score": 0.29794788360595703
    },
    {
      "name": "Engineering",
      "score": 0.10150814056396484
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16609230",
      "name": "Hunan University",
      "country": "CN"
    }
  ],
  "cited_by": 249
}