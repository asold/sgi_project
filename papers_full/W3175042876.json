{
  "title": "On Orthogonality Constraints for Transformers",
  "url": "https://openalex.org/W3175042876",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2102121030",
      "name": "Aston Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2090173634",
      "name": "Alvin Chan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2738935859",
      "name": "Yi Tay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1985012419",
      "name": "Jie Fu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127805350",
      "name": "Shuohang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107936128",
      "name": "Shuai Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2535364173",
      "name": "Huajie Shao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2230127352",
      "name": "Shuochao Yao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4207905942",
      "name": "Roy Ka-Wei Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963103976",
    "https://openalex.org/W3006966862",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2993325947",
    "https://openalex.org/W2994199588",
    "https://openalex.org/W2175402905",
    "https://openalex.org/W3006710048",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2807880213",
    "https://openalex.org/W2963042606",
    "https://openalex.org/W2553902701",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2587753047",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2964304579",
    "https://openalex.org/W2963544536",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W4287864664",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963543570",
    "https://openalex.org/W4297747548",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2761590056",
    "https://openalex.org/W2759764766",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2729046720",
    "https://openalex.org/W4295126650"
  ],
  "abstract": "Orthogonality constraints encourage matrices to be orthogonal for numerical stability. These plug-and-play constraints, which can be conveniently incorporated into model training, have been studied for popular architectures in natural language processing, such as convolutional neural networks and recurrent neural networks. However, a dedicated study on such constraints for transformers has been absent. To fill this gap, this paper studies orthogonality constraints for transformers, showing the effectiveness with empirical evidence from ten machine translation tasks and two dialogue generation tasks. For example, on the large-scale WMT'16 En -&gt; De benchmark, simply plugging-and-playing orthogonality constraints on the original transformer model (Vaswani et al., 2017) increases the BLEU from 28.4 to 29.6, coming close to the 29.7 BLEU achieved by the very competitive dynamic convolution (Wu et al., 2019).",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 375–382\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n375\nOn Orthogonality Constraints for Transformers\nAston Zhang‡∗, Alvin Chan⋄∗, Yi Tay†, Jie Fu◁, Shuohang Wang◦,\nShuai Zhang•, Huajie Shao⊿, Shuochao Yao⋆, Roy Ka-Wei Lee∧\n‡AWS, ⋄NTU Singapore, †Google, ◁Mila, Universit´e de Montr´eal\n◦SMU, •ETH Z¨urich, ⊿UIUC, ⋆George Mason University, ∧SUTD\naz@astonzhang.com\nAbstract\nOrthogonality constraints encourage matrices\nto be orthogonal for numerical stability. These\nplug-and-play constraints, which can be conve-\nniently incorporated into model training, have\nbeen studied for popular architectures in nat-\nural language processing, such as convolu-\ntional neural networks and recurrent neural\nnetworks. However, a dedicated study on\nsuch constraints for transformers has been\nabsent. To ﬁll this gap, this paper stud-\nies orthogonality constraints for transformers,\nshowing the effectiveness with empirical evi-\ndence from ten machine translation tasks and\ntwo dialogue generation tasks. For example,\non the large-scale WMT’16 En →De bench-\nmark, simply plugging-and-playing orthogo-\nnality constraints on the original transformer\nmodel (Vaswani et al., 2017) increases the\nBLEU from 28.4 to 29.6, coming close to the\n29.7 BLEU achieved by the very competitive\ndynamic convolution (Wu et al., 2019).\n1 Introduction\nTransformers (Vaswani et al., 2017) are a class of\nneural architectures that have made a tremendous\ntransformative impact on modern natural language\nprocessing research and applications. Transform-\ners have not only served as a powerful inductive\nbias for general-purpose sequence transduction (Ott\net al., 2018) but also lived as the core of large pre-\ntrained language models (Devlin et al., 2018; Rad-\nford et al., 2018; Dai et al., 2019). That said, the\nstudy of more effective training for this class of\nmodels is still an open research question, bearing\ngreat potential to impact a myriad of applications\nand domains.\nTo improve numerical stability during training,\nthe trick of enforcing orthogonality constraints has\n∗Equal contribution.\n†Work was done at NTU.\nsurfaced recently. In the analysis of numerical\nstability, enforcing orthogonality constraints can\nupper-bound the Lipschitz constant of linear trans-\nformations. The Lipschitz constant is a measure\nthat approximates the rate of change (variation)\nof representations. Theoretically, controlling the\nLipschitz constant, which may be achieved via or-\nthogonality constraints, yields representations that\nare robust and less sensitive to perturbations.\nIn view of this, orthogonality constraints have\nbeen studied for convolutional neural networks\n(CNNs) (Bansal et al., 2018; Huang et al., 2018)\nand recurrent neural networks (RNNs) (Arjovsky\net al., 2016; V orontsov et al., 2017; Rodr´ıguez et al.,\n2016). Such plug-and-play constraints can be in-\ncorporated into model training without additional\nhassle. For example, CNN-based models incorpo-\nrating orthogonality constraints have demonstrated\nempirical effectiveness for tasks such as person re-\nidentiﬁcation (Han et al., 2019) and keyword spot-\nting (Lee et al., 2019), while RNN-based models\nthat enforce such constraints have shown promis-\ning empirical results for response generation (Tao\net al., 2018) and text classiﬁcation (Wei et al., 2020;\nKrishnan et al., 2020). However, a dedicated study\non orthogonality constraints for transformers has\nbeen absent so far.\nTo ﬁll this research gap, we study orthogonality\nconstraints for transformers, which are imposed\non (i) linear transformations in self-attention and\nposition-wise feed-forward networks and (ii) the\nafﬁnity matrix in self-attention. Mathematically, or-\nthogonality constraints on the weights of these lin-\near transformations can be motivated by bounded\nLipschitz constants. We also formally analyze the\nself-attention mechanism by bounding perturba-\ntions to the afﬁnity matrix in the face of input\nchanges.\nFurthermore, we conduct extensive experiments\non ten neural machine translation (both subword-\n376\nlevel and character-level) tasks and two dialogue\ngeneration tasks. Our experimental results are\npromising, demonstrating that the performance\nof transformers can be consistently boosted with\northogonality constraints. For example, on the\nlarge-scale WMT’16 En→De benchmark, simply\nplugging-and-playing orthogonality constraints on\nthe original transformer model (Vaswani et al.,\n2017) increases the BLEU from 28.4 to 29.6, com-\ning close to the 29.7 BLEU achieved by the very\ncompetitive dynamic convolution (Wu et al., 2019).\nNotation For any vector x and any matrix X,\n∥x∥and ∥X∥denote their L2-norm and spectral\nnorm, respectively.\n2 Orthogonality Constraints for\nTransformers\nRecall that in the transformer architecture, keys,\nqueries, and values all come from the same place in\nthe self-attention module. They are linearly trans-\nformed for computing multiple attention heads,\nwhere all the heads are aggregated by another lin-\near transformation. The position-wise feed-forward\nnetwork is also built on two linear transformations\nwith activations. In the following, we will de-\nscribe orthogonality constraints for (i) linear trans-\nformations in self-attention and position-wise feed-\nforward networks and (ii) the afﬁnity matrix in\nself-attention.\n2.1 For Linear Transformations in\nSelf-Attention and Position-wise\nFeed-Forward Networks\nNote that linear transformations in self-attention\nand position-wise feed-forward networks are in the\nform:\ny = Wx + b,\nwhere y is the output, x is an input, W is a linear\ntransformation weight matrix, and b is an optional\nbias term. This form provides us with convenient\ntools for motivating the application of orthogonality\nconstraints to the weights of such linear transfor-\nmations.\nSpeciﬁcally, as described in Section 1, robust-\nness of linear transformations to small perturba-\ntions can be measured by Lipschitz constants. Thus,\nwe begin with motivating orthogonality constraints\nfrom the perspective of bounding Lipschitz con-\nstants of linear transformations.\nFormally, the linear transformation (layer) of the\naforementioned form y = Wx+b has a Lipschitz\nconstant equal to the largest singular value of the\nweight matrix W. The linear layer is Lipschitz\ncontinuous with the constant Lif for all x and x′,\nit holds that\n∥(Wx + b) −(Wx′+ b)∥≤ L∥x −x′∥,\nwhich can be re-written as\n∥W(x −x′)∥\n∥x −x′∥ ≤L,\nwhere x ̸= x′. Therefore, the smallest Lipschitz\nconstant is\nsup\nx̸=x′\n∥W(x −x′)∥\n∥x −x′∥ .\nFor numerical stability, our goal is to force the\nLipschitz constant to be no greater than one at every\nlinear transformation so that their multiplication\nthroughout compositions of transformations is also\nupper bounded by one. Mathematically, we need\nto constrain the Lipschitz constant (the largest sin-\ngular value) of W to be no greater than one, which\nrequires the following orthogonality constraint:\nW⊤W ≈I.\nBack to the context of multi-head self-attention\nof transformers, denote by P the concatenation\nof the linear transformation weights for the query,\nkey, value, and the multi-head aggregation. To\nimpose the orthogonality constraint for these linear\ntransformations, we add the following loss to the\ntransformer model for every layer:\nLLA = λ∥P⊤P −I∥2\nF .\nLikewise, for position-wise feed-forward net-\nwork with two linear transformation weight matri-\nces M1 and M2, the orthogonality constraint can\nbe imposed with another additional loss:\nLLF = λ\n[\n∥M⊤\n1 M1 −I∥2\nF + ∥M⊤\n2 M2 −I∥2\nF\n]\n.\n2.2 For the Afﬁnity Matrix in Self-Attention\nIn transformers, given the query matrix Q and the\nkey matrix K in the self-attention module, the afﬁn-\nity matrix\nA = softmax(αQK⊤), (1)\n377\nwhere αis typically 1√\nd (dis the dimension of the\nkey and the query). Given the value matrix V,\nthe self-attention computes representations via the\nmatrix multiplication AV.\nWithin the context of sequence transduction,\nwhen an input word token is aligned with another\nsemantically similar token, we would expect a\nsmall change in the behavior of the self-attention\nmechanism, rather than a huge change in the output.\nIn the afﬁnity matrix A as deﬁned in (1), let Ai,∗\nbe the row vector indexed by i. Essentially, each\nAi,∗is a probability distribution over the tokens\nin the sequence that directs the alignment-based\npooling operation. Intuitively, for a robust self-\nattention mechanism, noisy perturbations should\nhave a limited effect on the afﬁnity scores of the\ntokens.\nMore formally, let us analyze the self-attention\nmechanism by bounding perturbations to the afﬁn-\nity matrix in the face of input changes. Mathemat-\nically, changes to the afﬁnity scores are bounded\nsuch that ∥A′\ni,∗−Ai,∗∥≤ 2α∥K∥ϵ, where ϵ =\n∥Q′\ni,∗−Qi,∗∥is the noise from the query matrix.\nWe can see this as the result of the following theo-\nrem.\nTheorem 2.1 (Bounded Perturbations to the Afﬁn-\nity Matrix). Expressing Ai,∗to be theith row of the\nafﬁnity matrix A as deﬁned in (1) and Qi,∗to be\nthe ith row of the query matrix Q, the perturbation\nto the afﬁnity matrix is bounded as such:\n∥A′\ni,∗−Ai,∗∥≤ 2α∥K∥ϵ,\nwhere A′ = softmax(αQ′K⊤) and ϵ = ∥Q′\ni,∗−\nQi,∗∥is the L2 perturbation value in Qi,∗.\nThe detailed proof of Theorem 2.1 is provided\nin the appendix. In standard training, the spec-\ntral norm of the key matrix ∥K∥or the noise ϵ\nfrom the query matrix may be large, and as a result\nthe changes to afﬁnity scores may become “un-\nbounded”. We speculate that this may hurt the\ngeneralization of the self-attention mechanism.\nWe impose orthogonality constraints on the afﬁn-\nity matrix A. More concretely, we obtain an addi-\ntional loss term for every layer of the transformer\nmodel using the Frobenius norm ∥·∥F :\nLAM = λ∥A⊤A −I∥2\nF ,\nwhere I is the identity matrix and λ is a scaling\nfactor to control the ratio to the original task loss.\nWith orthogonally constrained afﬁnity scores,\neach row vector of A is now orthonormal to all\nthe other row vectors. Given that each row vector\nis a probability distribution over the tokens in the\nsequence that directs the alignment-based pooling\noperation, a diverse form of the self-attention mech-\nanism would be more encouraged. This could be\nviewed as an additional quality of orthogonality\nconstrained transformers.\n3 Experiments\nWe evaluate the effectiveness of orthogonality con-\nstrained transformers (OC-transformers for brevity)\non ten neural machine translation tasks and two di-\nalogue generation tasks. Speciﬁcally, we assess\nthree variants, largely pertaining to where orthogo-\nnality constraints are applied, i.e., (i) AM (for the\nafﬁnity matrix in self-attention), (ii) LA (for the\nlinear transformations in self-attention), and (iii)\nLF (for the linear transformations in position-wise\nfeed-forward networks). We evaluate them in an\nincremental fashion with three main model labels:\nV AR-I (AM only), V AR-II (AM + LA), and V AR-\nIII (AM + LA + LF). The scaling factor λis tuned\namongst {10−6,10−8,10−10}.\n3.1 Neural Machine Translation\nFor neural machine translation (NMT), we evaluate\non both the subword-level and character-level tasks.\nExperimental Setup For subword-level NMT,\nwe evaluate our models on seven NMT datasets us-\ning the Tensor2Tensor1 framework (Vaswani et al.,\n2018), namely IWSLT’14 De →En, IWSLT’14\nRo→En, IWSLT’15 En→Vi, IWSLT’17 En→Id,\nWMT’17 En→Et, SETIMES En→Mk, and the\nwell-established large-scale WMT’16 En→De.\nAll the models are trained with the transformer-\nbase setting. Owing to the smaller size, we use the\ntransformer-small setting for IWSLT’14 datasets.\nFor the WMT’16 En→De dataset, we train both\nthe transformer-base and transformer-big settings\non 4×GPUs with gradient accumulation of 2×\nto emulate 8×GPU training. By determining im-\nprovement on approximate BLEU scores on the\nvalidation set, we train models for 2M steps for\nthe transformer-base setting and 800K steps for\nthe transformer-big setting. Note that between the\nstandard transformer and OC-transformer, we main-\ntain all the other hyperparameters to keep the com-\nparisons as fair as possible. For character-level\nNMT, we evaluate on three language pairs, namely\n1https://github.com/tensorflow/\ntensor2tensor\n378\nTable 1: Experimental results on subword-level neural machine translation.\nBLEU\nModel De→En Ro →En En →Vi En →Id En →Et En →Mk\nTransformer 34.68 32.36 28.43 47.40 14.17 13.96\nOC-transformer (V AR-I) 34.87 32.68 30.16 48.09 14.83 14.74\nOC-transformer (V AR-II) 34.92 32.63 30.51 48.05 15.06 14.70\nOC-transformer (V AR-III) 35.20 32.44 30.42 48.33 14.87 14.62\nRelative Gain (%) +1.5% +1.0% +7.3% +2.0% +6.3% +5.3%\nTable 2: Experimental results on neural machine translation with the WMT’16 En→De newstest2014 test set.\nModel BLEU\nMoE (Shazeer et al., 2017) 26.0\nTransformer-base (Vaswani et al., 2017) 27.3\nTransformer-big (Vaswani et al., 2017) 28.4\nTransformer-ott-big (Ott et al., 2018) 29.3\nDynamic convolution (Wu et al., 2019) 29.7\nOC-transformer-base based on (Vaswani et al., 2017) (V AR-III) 28.5\nOC-transformer-big based on (Vaswani et al., 2017) (V AR-III) 29.6\nWMT En→Fr, IWSLT’14 Ro→En, and IWSLT’15\nDe→En. The transformer-small setting is used for\nall the three language pairs and trained for 200K\nsteps.\nExperimental Results Table 1 reports experi-\nmental results on subword-level NMT datasets.\nOverall, we note that performance of transform-\ners is consistently boosted by orthogonality con-\nstraints, ascertaining the effectiveness of adopting\nsuch plug-and-play tricks. More speciﬁcally, they\nare able to achieve +1.0% to +7.3% relative gain\nover the standard transformer. Notably, all the\nvariants (V AR-I, V AR-II, and V AR-III) boost the\nperformance of transformers: it demonstrates that\northogonality constraints are indeed useful. More-\nover, orthogonal constraints on the self-attention\nafﬁnity matrix are beneﬁcial in general even if the\nrest of the model is not fully enforced with orthog-\nonality constraints.\nTable 2 reports the results on the large-scale\nWMT’16 En→De dataset. Orthogonality con-\nstraints boost the performance of the transformer-\nbig setting based on (Vaswani et al., 2017), increas-\ning the BLEU from 28.4 to 29.6. This result outper-\nforms the more advanced transformer-ott-big pro-\nposed in (Ott et al., 2018) and comes close to 29.7\nthat was achieved by the very competitive dynamic\nconvolution model (Wu et al., 2019). Likewise, or-\nthogonality constraints also boost the performance\nof the transformer-base setting with the BLEU in-\ncreased from 27.3 to 28.5.\nTable 3 reports the results on character-level\nNMT. We observe that orthogonality constraints\nconsistently boost the performance of standard\ntransformers on all the three language pairs:\nEn→Fr (+3.5%), Ro→En (+2.6%), and De →En\n(+1.6%).\n3.2 Sequence-to-Sequence Dialogue\nGeneration\nWe conduct experiments on the sequence-to-\nsequence dialog generation task whereby the goal\nis to generate the reply in a two-way conversation.\nExperimental Setup We use two datasets: Per-\nsonaChat (Zhang et al., 2018) and DailyDialog\n(Li et al., 2017). We implement our task in Ten-\nsor2Tensor using the transformer-small setting in\na sequence-to-sequence fashion (Sutskever et al.,\n2014). We train all the models for 20K steps,\nwhich we ﬁnd sufﬁcient for model convergence.\nBeam search of beam size 4 and length penalty 0.6\nis adopted for decoding the output sequence. We\nevaluate all the models with the language genera-\ntion evaluation suite in (Sharma et al., 2017).\nExperimental Results Table 4 reports our re-\nsults on the PersonaChat and DailyDialog datasets.\nThe key observation is that all the variants of enforc-\ning orthogonality constraints boost performance\nof standard transformers. The best results of OC-\ntransformers make a substantial improvement in all\n379\nTable 3: Experimental results on character-level neural machine translation.\nBLEU\nModel En→Fr Ro →En De →En\nTransformer (Vaswani et al., 2017) 18.74 22.04 27.59\nOC-transformer based on (Vaswani et al., 2017) (V AR-III) 19.40 22.61 28.02\nRelative Gain (%) +3.5% +2.6% +1.6%\nTable 4: Experimental results on the PersonaChat dataset (Zhang et al., 2018) and the DailyDialog dataset (Li et al.,\n2017) on nine evaluation measures (Sharma et al., 2017). SkipT stands for SkipThought cosine similarity, EmbA\nstands for embedding average, VecE stands for vector extrema, and GreedyM stands for greedy matching.\nTransformer OC-transformer\n(V AR-I)\nOC-transformer\n(V AR-II)\nOC-transformer\n(V AR-III) Relative Gain\nPersonaChat\nBLEU-1 13.2 15.1 15.4 16.3 +23.5%\nBLEU-4 2.04 2.28 2.38 2.50 +22.5%\nMeteor 6.10 6.55 6.60 6.70 +9.8%\nRouge 14.2 14.7 15.1 15.1 +6.3%\nCIDEr 18.2 18.7 19.3 18.3 +6.0%\nSkipT 41.9 42.8 43.9 43.3 +4.8%\nEmbA 84.3 84.6 84.9 84.6 +0.7%\nVecE 49.0 48.2 49.0 48.6 +0.0%\nGreedyM 65.8 66.2 66.5 66.4 +1.1%\nDailyDialog\nBLEU-1 12.1 13.5 13.3 14.0 +15.7%\nBLEU-4 6.22 6.70 6.52 7.11 +14.3%\nMeteor 8.23 8.43 8.39 8.72 +6.0%\nRouge 21.1 21.4 21.7 21.7 +2.8%\nCIDEr 79.3 79.2 79.6 82.1 +3.5%\nSkipT 66.9 67.1 67.1 67.2 +0.4%\nEmbA 84.9 85.7 85.6 85.5 +0.9%\nVecE 53.1 53.3 53.4 53.2 +0.5%\nGreedyM 72.1 72.3 72.6 72.2 +0.7%\nthe nine evaluation measures. Notably, on both\ndatasets, the best variants are either V AR-II or\nV AR-III. V AR-I performs decently and boosts per-\nformance of standard transformers on both tasks,\nsignifying that the orthogonality constrained afﬁn-\nity matrix in self-attention is sufﬁciently effective.\nThis mirrors the results on neural machine trans-\nlation and is consistent across the ﬁndings. The\nrelative gain of applying orthogonality constraints\nis also promising, peaking at +23.5% on BLEU-1\nscores and +2.8% to +6.3% on Rouge.\n4 Conclusion\nWe studied orthogonality constraints for trans-\nformers, which are imposed on (i) linear transfor-\nmations in self-attention and position-wise feed-\nforward networks and (ii) the afﬁnity matrix in\nself-attention. We showed that such plug-and-play\nconstraints, which can be conveniently incorpo-\nrated, consistently boost performance of transform-\ners on ten different machine translation tasks and\ntwo dialogue generation tasks. For example, on\nthe large-scale WMT’16 En→De benchmark, sim-\nply plugging-and-playing orthogonality constraints\non the original transformer model (Vaswani et al.,\n2017) increases the BLEU from 28.4 to 29.6, com-\ning close to the 29.7 BLEU achieved by the very\ncompetitive dynamic convolution (Wu et al., 2019).\nBroader Impact Given widespread adoptions of\ntransformer models, the proposed plug-and-play\northogonal constraints could also be useful to com-\nputer vision, automatic speech recognition, time\nseries analysis, and biological sequence analysis.\n380\nReferences\nMartin Arjovsky, Amar Shah, and Yoshua Bengio.\n2016. Unitary evolution recurrent neural networks.\nIn International Conference on Machine Learning ,\npages 1120–1128. PMLR.\nNitin Bansal, Xiaohan Chen, and Zhangyang Wang.\n2018. Can we gain more from orthogonality regu-\nlarizations in training deep cnns? arXiv preprint\narXiv:1810.09102.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nChuchu Han, Ruochen Zheng, Changxin Gao, and\nNong Sang. 2019. Complementation-reinforced at-\ntention network for person re-identiﬁcation. IEEE\nTransactions on Circuits and Systems for Video Tech-\nnology, 30(10):3433–3445.\nLei Huang, Xianglong Liu, Bo Lang, Adams Yu,\nYongliang Wang, and Bo Li. 2018. Orthogonal\nweight normalization: Solution to optimization over\nmultiple dependent stiefel manifolds in deep neural\nnetworks. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, volume 32.\nJitin Krishnan, Hemant Purohit, and Huzefa Rangwala.\n2020. Diversity-based generalization for neural un-\nsupervised text classiﬁcation under domain shift.\narXiv preprint arXiv:2002.10937.\nMingu Lee, Jinkyu Lee, Hye Jin Jang, Byeonggeun\nKim, Wonil Chang, and Kyuwoong Hwang. 2019.\nOrthogonality constrained multi-head attention for\nkeyword spotting. In 2019 IEEE Automatic Speech\nRecognition and Understanding Workshop (ASRU) ,\npages 86–92. IEEE.\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\nCao, and Shuzi Niu. 2017. Dailydialog: A manually\nlabelled multi-turn dialogue dataset. arXiv preprint\narXiv:1710.03957.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. arXiv preprint arXiv:1806.00187.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. online.\nPau Rodr ´ıguez, Jordi Gonzalez, Guillem Cucurull,\nJosep M Gonfaus, and Xavier Roca. 2016. Regu-\nlarizing cnns with locally constrained decorrelations.\narXiv preprint arXiv:1611.01967.\nShikhar Sharma, Layla El Asri, Hannes Schulz, and\nJeremie Zumer. 2017. Relevance of unsuper-\nvised metrics in task-oriented dialogue for evalu-\nating natural language generation. arXiv preprint\narXiv:1706.09799.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nChongyang Tao, Shen Gao, Mingyue Shang, Wei Wu,\nDongyan Zhao, and Rui Yan. 2018. Get the point of\nmy utterance! learning towards effective responses\nwith multi-head attention mechanism. In IJCAI,\npages 4418–4424.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan N. Gomez, Stephan Gouws,\nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki\nParmar, Ryan Sepassi, Noam Shazeer, and Jakob\nUszkoreit. 2018. Tensor2tensor for neural machine\ntranslation. CoRR, abs/1803.07416.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nEugene V orontsov, Chiheb Trabelsi, Samuel Kadoury,\nand Chris Pal. 2017. On orthogonality and learn-\ning recurrent networks with long term dependencies.\nIn International Conference on Machine Learning ,\npages 3570–3578. PMLR.\nJiyao Wei, Jian Liao, Zhenfei Yang, Suge Wang, and\nQiang Zhao. 2020. Bilstm with multi-polarity or-\nthogonal attention for implicit sentiment analysis.\nNeurocomputing, 383:165–173.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N\nDauphin, and Michael Auli. 2019. Pay less attention\nwith lightweight and dynamic convolutions. arXiv\npreprint arXiv:1901.10430.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? arXiv preprint arXiv:1801.07243.\n381\nA Proof of Theorem 2.1\nProof. Let x = Qi,∗, g(x) = xK⊤, and f(y) =\nsoftmax(y). Expressing each row in A as Ai,∗=\nsoftmax(αQi,∗K⊤), we have\nAi,∗= f(αg(x)). (2)\nWe ﬁrst consider bounding g(x) with respect to\n∥x′−x∥:\n∥g(x′) −g(x)∥= ∥(x′−x)K⊤∥.\nRecalling the deﬁnition of the spectral norm,\n∥A∥= maxx∈Rl\\{0}\n∥xA∥\n∥x∥ :\n∥g(x′) −g(x)∥≤∥ K∥∥x′−x∥. (3)\nHere, we can observe that the Lipschitz constant\nfor gis ∥K∥.\nNext, we bound f(y) = softmax(y) with re-\nspect to ∥y′−y∥. Since f is a differentiable func-\ntion, it holds that\n∥f(y′) −f(y)∥≤∥ J∥∗∥(y′−y)∥, (4)\nwhere J is the Jacobian matrix off(y) with respect\nto y, i.e., Ji,j = ∂f (y)i\n∂yj\n, and ∥J∥∗ = maxy ∥J∥.\nSince f(y)i = eyi∑eyj , for diagonal entries of J we\nhave\nJi,i = ∂f(y)i\n∂yi\n= eyi\n∑eyj\n− e2yi\n(∑eyj )2\n= fi −fi2,\nwhere fi = f(y)i for brevity. For non-diagonal\nentries of J where i̸= j, we have\nJi,j = ∂f(y)i\n∂yj\n= − eyi eyj\n(∑eyj )2\n= −fifj.\nWith this, we can express the Jacobian J as\nJ =\n\n\nf1 −f12 ··· − f1fn\n... ... ...\n−fnf1 ··· fn −fn2\n\n\n= diag(fi) −f⊤f,\nwhere f = [f1,..,f n] and f⊤f is the outer product\nof f. We can then express the spectral norm of J as\n∥J∥= ∥diag(fi) −f⊤f∥\n≤∥diag(fi)∥+ ∥f⊤f∥.\n(5)\nNote that diag(fi) and f⊤f are both symmetric\nmatrices. The spectral norm of a symmetric matrix\nM is the largest absolute value of its eigenvalues\nλ:\n∥M∥= max\ni\n|λi(M)|. (6)\nFor a diagonal matrix like diag(fi), its eigen-\nvectors are the standard basis vector while its\neigenvalues are the non-zero diagonal entries, i.e.,\nλi(diag(fi)) =fi. Thus, we can get\n∥diag(fi)∥= max\ni\nfi. (7)\nNext, we ﬁnd ∥f⊤f∥through the eigenvalues of\nf⊤f. When we take the product of f⊤f and f⊤,\nf⊤f ·f⊤=\n\n\nf1\n...\nfn\n\n\n[\nf1 ··· fn\n]\n\n\nf1\n...\nfn\n\n\n=\n\n\nf1\n...\nfn\n\n·\n∑\ni\nfi2\n=\n(∑\ni\nfi2\n)\nf⊤.\nFrom this, we know λ1(f⊤f) = ∑\ni fi2, with\nthe corresponding eigenvector v1 = f⊤. Since the\nremaining n−1 eigenvectors are orthogonal to v1,\ni.e., v⊤\n1 vi = fvi = 0,∀i̸= 1, we have\nf⊤f ·vi = f⊤(f ·vi)\n= 0.\nThis implies that ∑\ni fi2 is the only non-zero\neigenvalue of f⊤f. Thus, with (6), this gives\n∥f⊤f∥=\n∑\ni\nfi2.\nCombining this with (5) and (7), we get\n∥J∥≤ max\ni\nfi +\n∑\ni\nfi2. (8)\nRecall that ∥J∥is the largest possible spectral\nnorm of J, i.e., ∥J∥∗= maxy ∥J∥. Moreover, by\n382\ndeﬁnition of probability, it holds that fi ≤1 and\nsum of probabilities ∑fi ≤1. Therefore,\n∥J∥∗≤max\ni,y\nfi + max\ny\n∑\ni\nfi2\n≤1 + 1 = 2.\n(9)\nWith (4) and (9), we get\n∥f(y′) −f(y)∥≤ 2∥(y′−y)∥. (10)\nBounding ∥A′\ni,∗−Ai,∗∥with (2), (10), and (3),\nthis gives\n∥A′\ni,∗−Ai,∗∥= ∥f(αg(x′)) −f(αg(x))∥\n≤∥2αg(x′) −2αg(x)∥\n= 2α∥g(x′) −g(x)∥\n≤2α∥K∥∥x′−x∥\n= 2α∥K∥ϵ.",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.8205204010009766
    },
    {
      "name": "Orthogonality",
      "score": 0.7642005681991577
    },
    {
      "name": "Computer science",
      "score": 0.472788542509079
    },
    {
      "name": "Joint (building)",
      "score": 0.4216132164001465
    },
    {
      "name": "Library science",
      "score": 0.3895154595375061
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3342135548591614
    },
    {
      "name": "Engineering",
      "score": 0.2844679057598114
    },
    {
      "name": "Mathematics",
      "score": 0.2534427046775818
    },
    {
      "name": "History",
      "score": 0.24582049250602722
    },
    {
      "name": "Archaeology",
      "score": 0.10307401418685913
    },
    {
      "name": "China",
      "score": 0.10092401504516602
    },
    {
      "name": "Geometry",
      "score": 0.06786948442459106
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210100430",
      "name": "Google (Switzerland)",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210155582",
      "name": "Centre Universitaire de Mila",
      "country": "DZ"
    },
    {
      "id": "https://openalex.org/I162714631",
      "name": "George Mason University",
      "country": "US"
    }
  ],
  "cited_by": 9
}