{
    "title": "Just Fine-tune Twice: Selective Differential Privacy for Large Language Models",
    "url": "https://openalex.org/W4385573200",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2116760462",
            "name": "Weiyan Shi",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2297767358",
            "name": "Ryan Shea",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2104059051",
            "name": "Si Chen",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2118495952",
            "name": "Zhang Chiyuan",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2169838738",
            "name": "Ruoxi Jia",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2037128165",
            "name": "Zhou Yu",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2473418344",
        "https://openalex.org/W4283172211",
        "https://openalex.org/W4287888099",
        "https://openalex.org/W2810715221",
        "https://openalex.org/W2145938889",
        "https://openalex.org/W3207429447",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4287553002",
        "https://openalex.org/W2946930197",
        "https://openalex.org/W2785361959",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3110164654",
        "https://openalex.org/W4298166788",
        "https://openalex.org/W4231844697",
        "https://openalex.org/W3170764772",
        "https://openalex.org/W4287663285",
        "https://openalex.org/W3206066344",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3041206960",
        "https://openalex.org/W3172407229",
        "https://openalex.org/W2784621220",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2027595342",
        "https://openalex.org/W3188505388",
        "https://openalex.org/W4229053728",
        "https://openalex.org/W3212471751",
        "https://openalex.org/W4297799122"
    ],
    "abstract": "Protecting large language models from privacy leakage is becoming increasingly crucial with their wide adoption in real-world products. Yet applying *differential privacy* (DP), a canonical notion with provable privacy guarantees for machine learning models, to those models remains challenging due to the trade-off between model utility and privacy loss. Utilizing the fact that sensitive information in language data tends to be sparse, Shi et al. (2021) formalized a DP notion extension called *Selective Differential Privacy* (SDP) to protect only the sensitive tokens defined by a policy function. However, their algorithm only works for RNN-based models. In this paper, we develop a novel framework, *Just Fine-tune Twice* (JFT), that achieves SDP for state-of-the-art large transformer-based models. Our method is easy to implement: it first fine-tunes the model with *redacted* in-domain data, and then fine-tunes it again with the *original* in-domain data using a private training mechanism. Furthermore, we study the scenario of imperfect implementation of policy functions that misses sensitive tokens and develop systematic methods to handle it. Experiments show that our method achieves strong utility compared to previous baselines. We also analyze the SDP privacy guarantee empirically with the canary insertion attack.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6327–6340\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nJust Fine-tune Twice: Selective Differential Privacy for Large Language\nModels\nWeiyan Shi†, Ryan Shea†, Si Chen‡, Chiyuan Zhang⋄, Ruoxi Jia‡, Zhou Yu†\nColumbia University†, Virginia Tech‡, Google Research⋄\n{ws2634,rs4235}@columbia.edu, chensi@vt.edu, chiyuan@google.com,\nruoxijia@vt.edu, zy2461@columbia.edu\nAbstract\nProtecting large language models from privacy\nleakage is becoming increasingly crucial with\ntheir wide adoption in real-world products. Yet\napplying differential privacy (DP), a canonical\nnotion with provable privacy guarantees for ma-\nchine learning models, to those models remains\nchallenging due to the trade-off between model\nutility and privacy loss. Utilizing the fact that\nsensitive information in language data tends to\nbe sparse, Shi et al. (2021) formalized a DP no-\ntion extension called Selective Differential Pri-\nvacy (SDP) to protect only the sensitive tokens\ndefined by a policy function. However, their al-\ngorithm only works for RNN-based models. In\nthis paper, we develop a novel framework, Just\nFine-tune Twice (JFT), that achieves SDP for\nstate-of-the-art large transformer-based models.\nOur method is easy to implement: it first fine-\ntunes the model with redacted in-domain data,\nand then fine-tunes it again with the original\nin-domain data using a private training mecha-\nnism. Furthermore, we study the scenario of im-\nperfect implementation of policy functions that\nmisses sensitive tokens and develop systematic\nmethods to handle it. Experiments show that\nour method achieves strong utility compared to\nprevious baselines. We also analyze the SDP\nprivacy guarantee empirically with the canary\ninsertion attack1.\n1 Introduction\nWith the rapid advancement in natural language\nprocessing (NLP), it has become increasingly im-\nportant to protect NLP models from leaking privacy\ninformation. Previous work has attempted to tackle\nthis challenge by applying differential privacy (DP,\nDwork et al., 2014) on these models (McMahan\net al., 2018; Li et al., 2021). However, existing\nDP learning algorithms suffer from limited user\ncontrol and low utility, as they protect the entirety\n1Our code and data are available at https://github.\ncom/wyshi/sdp_transformers\nof each training example (e.g., one complete sen-\ntence) regardless of users’ privacy preference, and\ntherefore tend to be overly pessimistic when only\npartial information in a training example is sensi-\ntive. This problem is particularly pertinent in NLP,\nas NLP training data are often mixed with sparse\ndomain-dependent private information, and not all\ntokens need to be protected. For example, for the\nsentence “My SSN is 123-45-6789”, only the last\nfew tokens of the actual SSN need to be protected.\nIn fact, the definition of DP does not prevent\nus at all from protecting only the sensitive part of\ndata. Specifically, DP ensures that the output of a\ndata analysis algorithm stays roughly the same for\nneighboring datasets, while providing the flexibil-\nity to adjust the definition of neighboring relation\nto specific application contexts. Shi et al. (2021)\nrecently proposed an instantiation of DP, called\nSelective-DP (SDP), which defines neighboring\ndatasets to differ only in the sensitive part of a\ntraining example and as a result, SDP selectively\nhides the difference in the sensitive part only. SDP\nis particularly suitable for NLP and many other\nunstructured, high-dimensional data, wherein sen-\nsitive information only accounts for a small part.\nBut their privacy mechanism to achieve SDP suf-\nfers from three problems: 1) it requires substantial\nknowledge about the model to separate the private\nand public variables, and it is unclear how their al-\ngorithm tailored to recurrent neural networks could\nbe extended to modern Transformer-based NLP\nmodels; 2) it has only been evaluated with explicit\nprivate entities but not with contextual sensitive\ninformation; 3) it doesn’t provide protection for\nundetected sensitive tokens; These constraints limit\nthe applicability of SDP in real-world scenarios.\nLarge language models (LLMs) (Vaswani et al.,\n2017) have achieved tremendous success in NLP.\nThey are pretrained on a massive amount of pub-\nlic textual data, and thus excel at capturing gen-\neral language structures. A common practice in\n6327\nNLP is to fine-tune these LLMs on downstream\ntasks. Such a fine-tuning process also works well\nin the private training context. Previously, Yu et al.\n(2021a) showed that privately fine-tuning an ad-\nditional small set of parameters on top of off-the-\nshelf LLMs with private data achieves comparable\nperformance to non-private baselines. Inspired by\ntheir findings, in this paper, we propose a two-phase\nfine-tuning privacy mechanism,Just fine-tune twice\n(JFT), to achieve SDP for LLMs. Instead of di-\nrectly using off-the-shelf models to fine-tune once,\nwe have two fine-tuning steps: 1) we first redact\nthe in-domain data of the downstream tasks, and\nfine-tune the model with these in-domain redacted\ndata (redacted-fine-tune), and 2) then privately fine-\ntune the model on the original private data (private-\nfine-tune). This additional redacted-fine-tune step\nallows the model to directly learn information from\nthe in-domain data and thus leads to a better model\ninitialization for the second private-fine-tune step.\nMoreover, in the redacted-fine-tune step, we show\nthat even with limited public data (where manual\nscreening is possible), JFT achieves better utility\nthan fine-tune-once baselines. Additionally, we can\napply lightly-noised optimizers and privacy ampli-\nfication to protect undetected sensitive tokens.\nOur contributions are as follows. First, we pro-\npose an effective and generalizable privacy mech-\nanism to achieve SDP for large language models\nfor various NLP tasks. Second, we design secret\ndetectors of different privacy levels (explicit and\ncontextual sensitive data) and study their implica-\ntions on the models. Third, our method can utilize\neven a small amount of public data to achieve bet-\nter utility, and mitigate the missed sensitive token\nproblem with lightly-noised optimizer and privacy\namplification. Finally, we show that, opposite to\nthe common belief that privacy is at odds with util-\nity, private learning doesn’t have to conflict with\nthe utility because private information in the data\ncould be irrelevant to the learning task.\n2 Preliminary\nA differentially private algorithm hides the differ-\nence between two neighboring datasets.\nDefinition 1 (Differential Privacy). Given a do-\nmain D, any two neighboring datasets D,D′⊆D\na randomized algorithm M: D→R is (ϵw,δw)-\ndifferential private if for all neighboring datasets\nDand D′and all T ⊆R,\nPr[M(D) ∈T] ≤eϵw Pr[M(D′) ∈T] + δw.\nThe neighboring relation captures what is pro-\ntected. Traditional DP literature has considered\nneighboring datasets as those differing in one train-\ning example; thus, the corresponding DP protects\neach training example as a whole. We denote byϵw\nand δw the privacy parameters achieved under this\ntraditional neighboring relation definition. Given\nthe sparsity of sensitive information in language\ndata, this instantiation of neighboring relations is\napparently over-pessimistic. Shi et al. (2021) pro-\nposed Selective-DP (SDP), which instantiates the\nneighboring datasets to be those that differ in the\nsensitive attributes of a training sample; as a result,\nSDP selectively hides the difference in the sensitive\npart only. In the context of NLP, a training example\ncould be a sentence or a paragraph depending on\nthe task and the attributes are individual tokens.\nIn this paper, we will focus on designing learning\nalgorithms to achieve SDP. Formally, SDP relies\non a policy function F that specifies the sensitive\ninformation in a training example to be protected\nin an application-dependent fashion.\nDefinition 2 (Policy Function). A policy function\nF : τ →{0,1}|r|decides which attributes of an\nexample r ∈τ are public (F(r)i = 1) or private\n(F(r)i = 0). |r|is the number of attributes in r.\nDetecting private information manually in a\nlarge corpus based on the policy function is of-\nten costly. In that case, one may resort to building\nautomatic secret detectors to identify the sensitive\nattributes. A simple example of a secret detector\nis a regular expression to capture phone numbers.\nHowever, secret detectors could miss some private\nattributes and produce false negatives, which intu-\nitively would weaken the privacy guarantees. Ex-\nisting work (Doudalis et al., 2017; Shi et al., 2021;\nZhao et al., 2022) that selectively protects data ei-\nther assumes a perfect detector or uses an overly\nconservative detector with a low false negative but\nat the cost of a high false positive. In this paper, we\nprovide alternative ways to address this issue with\na better privacy-utility tradeoff (Section 3).\nWith F, SDP defines F-Neighbors.\nDefinition 3. (F-Neighbors). Consider a policy\nfunction F and two datasets D and D′. D′is a\nF-neighbor of D(denoted by D′∈NF(D)) if and\nonly if ∃r ∈Ds.t., F(r) has at least one private\nattribute, ∃r′∈D′s.t., F(r) and F(r′) differ by at\nleast one private attribute, andD′= D\\{r}∪{r′}.\nGiven the definition, the dataset with “My ID\nis 123” and the dataset with “My ID is 456” are\n6328\nFigure 1: The two-phase JFT mechanism. As pre-processing, we apply the secret detector to redact the private\ndata Dand obtain the redacted data D′. Next, depending on the detector’s performance, we use different ways to\nfine-tune the language model on the redacted D′ and obtain a redacted model. Then we fine-tune the model again\non the private data Dwith a private optimizer (e.g., DPSGD) to achieve an SDP-protected model.\nF-Neighbors (because except for the actual ID\nnumber, the other tokens are the same), while\nthe datasets with “Hi there” and the dataset with\n“Hello there” are not F-neighbors because the only\ntoken they differ in is not sensitive. An SDP algo-\nrithm guarantees that F-neighbors cannot be dis-\ntinguished by attackers if they observed the output.\nDefinition 4. (Selective Differential Privacy). Un-\nder a policy function F, a randomized algorithm\nM: D→R satisfies (F,ϵs,δs)-selective differen-\ntial privacy if for ∀D,D′∈NF(D), and ∀T ⊆R,\nPr[M(D) ∈T] ≤eϵs Pr[M(D′) ∈T] + δs.\nIn this paper, we differentiate between SDP and\nDP for two reasons. Firstly, we want to highlight\nthat the privacy parameters associated with SDP\n(ϵs and δs) and DP (ϵw and δw) are incomparable.\nFor instance, one cannot claim which of (1, 0.001)-\nSDP and (2, 0.001)-DP provides stronger privacy\nguarantees because they are under different privacy\nnotions. To meaningfully present the value of these\nprivacy parameters, we need to specify under which\ndefinition these parameters are calculated; to mean-\ningfully compare the value of these parameters, we\nneed to make sure that the parameters are calcu-\nlated under the same privacy definition. Secondly,\nwe would like to remain the same terminology as\nour main reference, Shi et al. (2021), which also\nuses the terms SDP and DP to refer to the privacy\nguarantees under the two different neighboring re-\nlations. In the rest of the paper, we use different\nnotations to distinguish the privacy parameters as-\nsociated with SDP (ϵs and δs) and DP (ϵw and δw).\n3 J FT: Just Fine-tune Twice\nNow we describe JFT, a two-phase privacy mech-\nanism to achieve SDP for large language models\n(Figure 1). In the first redacted-fine-tune phase,\nwe redact the private data D with a secret detec-\ntor to obtain the redacted version D′, and learn a\nredacted model from D′in a privacy-preserving\nway. In the second private-fine-tune phase, we\nfurther fine-tune the redacted model (from phase\none) on the private data Dwith a private optimizer\nto achieve SDP guarantees.\n3.1 Phase 1: Redacted-fine-tune\nJFT is built upon the observation that the public\nportion of the in-domain data does not require pro-\ntection and can be utilized in various ways to help\nthe model learn in-domain information. In this\nphase, we apply the secret detector to redact the\nprivate data Dand obtain redacted in-domain data\nD′. Dependent on the detector performance, we\npropose the three following methods to use D′to\nfine-tune off-the-shelf language models.\nDirect Usage. If the secret detector masks all\nthe sensitive information in D(which is possible\nwhen D is small enough to support thorough in-\nspection or when a detector is very conservative\nand removes most of the essential information, see\nexamples in Table 1), we can use the redactedD′\ndirectly to fine-tune the model with a public and\nunnoised optimizer like SGD.\nSelective Manual Screening. If the secret detec-\ntor is imperfect, we can select an affordable subset\n6329\nfrom D′and then manually sanitize all the missed\nsecrets. Then we fine-tune the model with this\nsmall sanitized subset with a public optimizer. Ex-\nperiments show that even with a small amount of\nsanitized in-domain data, the resulting model still\noutperforms the traditional DP learning algorithms\nthat pessimistically protect every single token.\nLightly-Noised Fine-tuning. When the detector\nis imperfect, besides manually screening out the\nmissed secrets, we could also employ a private opti-\nmizer to train on D′that contains missed sensitive\ntokens. Because missed tokens only account for\na small portion of D′, intuitively, a much smaller\nnoise is needed to ensure the privacy of the missed\ntokens than the noise magnitude required to ensure\nthe privacy of the entireD′. We propose to leverage\nprivacy amplification by subsampling (PAS) (Balle\net al., 2018) to calculate the privacy parameters\nassociated with the private optimizer. The intuition\nof PAS is that if we perform a DP mechanism on\nrandom subsamples of the data, and one data point\nis not included in the subsamples, nothing about\nit could be leaked. In this way, we could amplify\nthe privacy guarantee. In our scenario, we need to\nprotect the missed sensitive tokens. If we know the\nsecret detector’s missing ratem(i.e., m=number of\nmissed sensitive tokens/total tokens, the probability\nof sampling a missed secret), we can calculate the\nprivacy budget ϵs by privacy amplification using\nthe subsampling ratio m.\nNote that the application of PAS requires the\nnumber of missed tokens that appear in any batch\nto be the same, which does not necessarily hold.\nHence, the privacy parameters calculated from pri-\nvacy amplification are an empirical estimate of the\nactual privacy loss. In practice, the secret detector’s\nmissing rate is unknown and we need to estimate\nit (denoted as ˜m). Then we change the original\nsampling rate p0 in moment accounting-based pri-\nvacy parameter calculation (Abadi et al., 2016) to\np = p0 ∗˜mand calculate the noise injected into\neach private optimizer iteration according to a pre-\ndefined privacy budget ϵunder p.\nIn our experiments, we sample 0.01% training\ndata for 10 times, and estimate the 95% confidence\ninterval of the missing rate, [˜mlow, ˜mhigh]. For both\n˜mlow and ˜mhigh, we can calculate an associatedϵlow\nand ϵhigh according to Theorem 9 in Balle et al.\n(2018), and report both ϵ.\n3.2 Phase 2: Private-fine-tune.\nIn the second phase, we initialize the model with\nthe redacted model from phase one, and fine-tune it\nwith the original private Dand a private optimizer\n(e.g., DPSGD (Abadi et al., 2016) or any other\nmore advanced private optimizer that achieves DP).\nUnlike the privacy mechanism in Shi et al.\n(2021), our algorithm does not require knowledge\nabout the models or the tasks, and therefore can be\neasily applied to different models such as GPT2\n(Radford et al., 2019) and Roberta (Liu et al., 2019),\nand different tasks such as language generation and\nnatural language understanding. See Section A.2\nfor more implementation details.\nOne-phase vs two-phase. Compared to conven-\ntional differentially private training, our algorithm\nintroduces an additional stage that involves redac-\ntion and regular unnoised training on redacted data.\nIn fact, the computational cost of the additional\nstage is much lower than the cost originally in-\ncurred by DP learning, because the additional first\nphase does not need costly per-sample gradient\nclipping and noising operations. And redaction is a\ncommon first-step people are already doing and fa-\nmiliar with. Also, there exist abundant off-the-shelf\ntools that allow redaction at scale.\n3.3 Privacy Analysis\nWe provide Theorem 1 for privacy analysis. It\nensures that, if the user has a secret detector with\n100% recall, JFT-trained models achieve (0,0)-SDP\nafter phase one and (ϵ, δ)-SDP after phase two. A\nsecret detector with 100% recall is possible if the\nuser can afford manual inspection or have enough\ndomain knowledge. When a detector with 100%\nrecall is not possible, we use lightly-noised fine-\ntuning to empirically protect the missed secrets as\nmentioned in Section 3.1.\nTheorem 1. Given that 1) in the first phase, the\ndata used for fine-tuning do not contain sensitive\ntokens and a public optimizer is used, and 2) in\nthe second phase, the private optimizer achieves\n(ϵ,δ)-DP ,JFT achieves (ϵ,δ)-SDP .\nThe proofs are deferred to Section A.1. The\ntheorem shows that under direct usage or selective\nscreening of D′, JFT achieves SDP with the same\nprivacy parameter values as the ones pertaining to\nthe private optimizer used in the second phase.\n6330\n4 Secret Detectors of Different Levels\nTypical private information includes personal-\nidentifiable information (PII) such as name and\nbirthday. But as pointed out in Brown et al. (2022),\none key challenge in NLP is that private informa-\ntion is often contextual. For example, they pre-\nsented a dialogue between Alice and Bob about\nAlice’s divorce (Table 1): none of the tokens in\n“What are you going to do about the custody of the\nkids?”, are PII by themselves, but combined to-\ngether, the semantics reveals private information.\nTo build generalizable secret detectors, we uti-\nlize off-the-shelf NER, dependency parser, and\nPOS tagger in spaCy (Honnibal and Montani, 2017)\nto label each token, and redact different sets of\ntokens to achieve the different privacy levels be-\nlow (entity level and contextual level). To qualita-\ntively show their protection levels, we apply them\nto redact two sentences from the divorce dialogue\nin Brown et al. (2022). The results are in Table 1.\nSecret De-\ntector\nWhat are you going to do\nabout the custody of the\nkids?\nDid you hear Alice is\ngetting divorced?\nLow entity What are you going to do about\nthe custody of the kids?\nDid you hear\n<PERSON>is getting\ndivorced??\nHigh en-\ntity\nWhat are you going to do about\nthe custody of the kids?\nDid you hear\n<PERSON>is getting\ndivorced??\nLow con-\ntextual\n<PRON>are<PRON>go-\ning to do about<OBJ>of the\n<OBJ>?\nDid <PRON> hear\n<PROPN> is getting\ndivorced??\nHigh con-\ntextual\n<PRON> are <PRON>\n<VERB>to<VERB>about\n<OBJ>of the<OBJ>?\nDid <PRON>\n<VERB> <PROPN>\nis getting<VERB>??\nTable 1: Results of different secret detectors on the two\nexample sentences from Brown et al. (2022).\nLow entity redacts four types of named entities\n(person, organization, date, and location), which\nare considered PII defined by the US Department of\nLabor2. We use NER in spaCy to detect them. If we\napply this detector, “Did you hear Alice is getting\ndivorced?” becomes “Did you hear <PERSON>\nis getting divorced?” An attacker who attacks a\nmodel trained on the latter sentence can at best\nlearn about the divorce but cannot know who.\nHigh entity redacts all the 18 entities in spaCy in-\ncluding the four above and more, like time entity3.\nThe two secret detectors above rely on named\nentities, so they are more explicit than the two de-\n2https://www.dol.gov/general/ppii\n3For the full list, see https://spacy.io/usage/\nlinguistic-features#named-entities.\ntectors below, which consider the overall sentence\nstructure and thus are more contextual.\nLow contextual protects all the 18 entities plus\nproper nouns, pronouns, and sentence subjects and\nobjects. This detector drastically increases the pri-\nvacy level: we cannot get any useful information\nfrom the left example in Table 1.\nHigh contextual further redacts all the verbs, in ad-\ndition to the tokens redacted by the low contextual\ndetector. It increases the privacy level even further\nand we cannot learn anything from both examples.\nThis is to stress-test JFT and see the model utility\nwhen the majority of the tokens are redacted 4.\nHuman language is diverse, and private infor-\nmation can take various forms. So instead of de-\nsigning sophisticated algorithms, we intentionally\nrely on common NLP tools to build easy-to-use\ndomain-agnostic secret detectors with high recalls\nto protect privacy as much as possible. As shown\nin Table 1, these detectors tend to over-sanitize\nthe sentences. But we will show later, even with\nover-redaction, JFT still achieve good performance.\nPrivate textual information can be treated more\nsophisticatedly, but how to better detect private in-\nformation is not the focus of this paper. Our goal\nis to show that simply redacting tokens achieves\npromising performance and JFT is compatible with\nbetter private information detection algorithms to\nfurther improve the results.\n5 Experiments\nWe conduct experiments on two NLP tasks: 1) nat-\nural language understanding (NLU, on GLUE) and\n2) language generation (on Wikitext-2 and ABCD).\nDatasets. 1) GLUE (Wang et al., 2018) is a widely-\nused multi-task benchmark dataset for NLU. It\ncontains sensitive information such as name and\ndate. 2) Wikitext-2 (Merity et al., 2017) contains\nWikipedia articles with private information such as\nname and date. 3) ABCD (Chen et al., 2021) is a\nhuman-human customer service dialogue dataset\nunder real-world scenarios with user private infor-\nmation such as name and order IDs.\nModels. We use Roberta (Liu et al., 2019) for the\nNLU classification task and GPT2 (Radford et al.,\n2019) for the language generation task. Due to com-\nputational constraints, we use Roberta-base and\nGPT2-small for the experiments. We use an effi-\n4Note that “divorced” is actually an adjective in the ex-\nample. As mentioned earlier, we can address the detector’s\nmistakes with lightly-noised fine-tuning.\n6331\ncient implementation of DPSGD in Li et al. (2021).\nBased on previous studies (Li et al., 2021; Yu et al.,\n2021a), larger DP models usually achieve better\nresults and thus we expect that larger SDP models\nwill achieve even better performances.\nBaselines. 1) No-DP: the model is fine-tuned\nusing regular Adam optimizer (Kingma and Ba,\n2014) without extra noise and hence it does not\nhave any privacy guarantees (i.e., ϵw = ϵs = ∞).\n2) DPSGD5: the model is fine-tuned with tradi-\ntional DPSGD Abadi et al. (2016) where the gra-\ndient is clipped and noised in every gradient de-\nscent iteration (we employ the DP-Adam variant\nwhere the optimizer is Adam but its gradient pri-\nvatization is the same as DPSGD, and we keep the\nterm DPSGD as it is more accessible to the com-\nmunity). While DPSGD was originally proposed\nto achieve the DP guarantees that protect a train-\ning example as a whole, it can also achieve SDP\nguarantees with the same privacy parameters (i.e.,\nϵs = ϵw and δs = δw). 3) CRT: the model is\ntrained with the recently proposed Confidentially\nRedacted Training (Zhao et al., 2022) that achieves\n(ϵc,δc)-Confidentiality. Confidentiality is a new\ndefinition related to SDP but different from SDP,\nit ensures the indistinguishability between a secret\nand a <MASK> token, so its privacy parameters are\nnot directly comparable to SDP. Thus, we add the\nsame amount of noise to CRT and SDP, empirically\ncompare SDP and CRT with the canary insertion\nattack in Figure 2 and 3, and report the utility in\nTable 6 in the Appendix. 4) Redacted: We also\npresent the utility of the redacted models since they\nare also privacy-preserving. Note when the secret\ndetector is perfect, the redacted models have a per-\nfect SDP privacy guarantee (i.e.,ϵs = 0). However,\nit does not allow the model to learn from sensitive\ntokens at all. JFT, by contrast, empowers the model\nto learn from sensitive data with a flexible, tunable\ntradeoff between privacy and utility. Moreover, it\nprovides ways to offer quantifiable privacy in the\npresence of imperfect secret detectors.\nOur models. 1) JFT: this is our JFT model di-\nrectly using the redacted data in phase one. 2) JFT\n+manual screening: this is JFT using a subset of\n5The public baseline from Roberta does not use infilling,\nfor a fair comparison, we chose the DPSGD baseline without\ninfilling from (Li et al., 2021). But compared to DPSGD\nbaselines with infilling from (Li et al., 2021), JFT without\ninfilling with a conservative secret detector is still better or\ncomparable. Previous studies found that infilling improves the\nresults, so it is possible that with infilling, JFT can achieve\neven better results.\nthe redacted data where missed secrets are manu-\nally filtered out in phase one. 3) JFT +light noise:\nthis is JFT where we add light noises according to\nthe estimated missing rate in phase one.\n6 Results\nWe show three major findings: 1) the impacts of\nsecret detectors are task-dependent on the result-\ning JFT models, but even for conservative contex-\ntual detectors (30%+ tokens are redacted), JFT still\nachieves better results than naive DPSGD (Sec-\ntion 6.1); 2) despite the small scale, using the man-\nual screened in-domain data still improves the JFT\nmodel utility (Section 6.2); 3) lightly noised opti-\nmizer with privacy amplification protects missed\nsensitive tokens from attacks (Section 6.3).\nThere is always a privacy-utility trade-off, so\nlarger epsilons lead to better utilities but worse\nprivacy. And when comparing models, we need\nto look at the model utility under a similar privacy\nbudget. An epsilon of 1 to 3 is commonly used in\nvarious privacy literature (Yu et al., 2021a; Li et al.,\n2021; Zhao et al., 2022). In our experiments, we\npre-calculated the privacy parameter so that an ϵof\naround 3 is spent when training ends.\n6.1 Secret Detectors of Different Levels\nTable 2 show the results on GLUE (left) and gen-\neration (right). Pct is the percentage of sensitive\ntokens redacted by the detector. ϵs is the SDP pri-\nvacy budget, the lower the better. We compare\nmodel utility under a similar privacy budget ϵs.\nNatural Language Understanding. Table 2 (left)\nshows that under a similar ϵs, all the JFT models\nachieve better performance than the DPSGD base-\nline, even when over 40% of tokens are redacted.\nBesides, for all the tasks, all the redacted mod-\nels achieve reasonable utility, even when a large\nportion of the tokens are redacted. For exam-\nple, the redacted model (high contextual) is better\nthan DPSGD on MNLI ( 83.23 vs 82.10, 44.27%\nredacted) and SST-2 ( 91.17 vs 86.12, 38.13%\nredacted). This confirms the motivation of SDP\nthat when building private NLP models, we should\nnot naively protect all the tokens regardless of their\nproperties. Instead, we should consider if the sen-\nsitive tokens will impact the task. If not, we can\nsimply redact them to build private models.\nAlso, if JFT can improve the redacted model\ndepends on the task. For SST-2 on sentiment anal-\nysis, the private-fine-tune step does not improve\n6332\nDirect Usage NLU on GLUE,δs=1/2|Dtrain| Language Generation,δs=1e-6\nMNLI QQP QNLI SST-2 WIKITEXT-2 ABCD\nModel DetectorPct Acc↑ ϵs Pct Acc↑ ϵs Pct Acc↑ ϵs Pct Acc↑ ϵs Pct PPL↓ ϵs Pct PPL↓ ϵs\nNo-fine-tune- - 31.82 - - 36.82 - - 50.54 - - 50.92 - - 30.08 - - 13.60 -\nNo-DP - - 87.60 - - 91.90 - - 92.80 - - 94.80 - - 20.48 - - 4.96 -DPSGD - - 82.10 2.75- 85.41 2.75- 84.62 2.57- 86.12 2.41- 27.05 2.58- 8.31 2.65DPSGD (+spe)- - - - - - - - - - - - - - 30.32 2.58- 17.75 2.71\nRedacted low ent6.09%86.67 - 6.05%88.74 - 12.19%89.64 - 1.79%93.58 - 11.3%22.50 - 2.7% 6.98 -JFT low ent6.09%85.740.926.05%88.192.5812.19%89.572.371.79%92.092.0611.3%21.862.582.7% 6.092.71\nRedacted high ent8.63%86.50 - 8.30%88.36 - 17.18%88.96 - 3.01%93.58 - 16.4%24.32 - 3.1% 7.32 -JFT high ent8.63%85.610.998.30%88.052.5817.18%89.352.373.01%92.202.1216.4%22.552.583.1% 6.252.71\nRedacted low ctx31.19%85.14 - 32.61%85.59 - 35.68%85.30 - 22.19%92.55 - 34.8%37.90 - 22.3%28.28 -JFT low ctx31.19%85.021.2332.61%87.002.4135.68%87.992.5222.19%92.432.1734.8%25.622.5822.3%8.80 2.71\nStress-test\nRedacted high ctx44.27%83.23 - 45.93%83.48 - 45.59%82.81 - 38.13%91.86 - 45.0%54.29 - 28.6%65.45 -JFT high ctx44.27%84.111.1845.93%86.422.6745.59%87.062.4138.13%91.172.1745.0%27.19 1.9628.6%12.93 2.71\nTable 2: Model utility and privacy guarantee on GLUE dev sets for NLU (left) and WikiText-2 and ABCD for\ngeneration (right). Detector: the secret detector to realize the policy function. low ent: low entity detector. low ctx:\nlow contextual detector. Pct: the percentage of sensitive tokens. Acc: accuracy. PPL: perplexity. ϵs: SDP privacy\nguarantee. DPSGD(+spe): DPSGD with added special tokens. For QNLI and SST-2, δ≈1e-5; for MNLI and QQP,\nδ≈1e-6 due to the large data size; No-DP and DPSGD results are from Liu et al. (2019) and Li et al. (2021). For\ngeneration tasks, δ=1e-6, we train the baselines and report the results. The models in bold are better than DPSGD.\nthe redacted model. This is because, the redacted\nmodels achieve a high accuracy (even the worst\naccuracy is 91.86, only a 2.94 drop from the SOTA\npublic model with an accuracy of 94.8), and fine-\ntuning them on the private data with noisy gradients\nis not enough to close the small gap. But for tasks\nwith a bigger gap between the redacted and No-DP\nmodels (e.g., MNLI, QQP, and QNLI), JFT can fur-\nther improve the redacted model. Besides, the gap\nbetween the redacted model and the correspond-\ning JFT model becomes bigger as the privacy level\nincreases: for QNLI (low contextual), the gap is\n87.99-85.30=2.69, while for QNLI (high contex-\ntual), the gap is 87.06-82.81=4.25. This shows\nthe model does learn useful information from the\nsensitive tokens during the private-fine-tune step.\nLanguage Generation. Table 2 (right) shows the\nlanguage generation results. We note that language\ngeneration is different from NLU tasks, because\nfor NLU, the models used for initialization without\nany fine-tuning (“No-fine-tune” in Table 2) start\nwith a bad accuracy (≤50%, just random guess),\nand adding special tokens to it would still start with\na random guess, so additional special tokens will\nnot impact the final results greatly. But for the gen-\neration task, the “No-fine-tune” GPT2 is already a\nstrong model for initialization with ppl=30.08 on\nWikitext-2 and 13.60 on ABCD, and we found that\nadding special tokens would disturb this initializa-\ntion and greatly impact the final result. Because\nall the JFT models have added special tokens like\n“<MASK>” and “SYS:”, for a fair comparison, we\nreport two DPSGD baselines, one without special\ntokens (“DPSGD”) and one with special tokens\n(“DPSGD (+spe)”). See Section A.2 for more dis-\ncussions on the impact of special tokens.\nCompared to “DPSGD (+spe)”, all JFT models\nachieve better model utility on both datasets. For\n“DPSGD” without special tokens, fine-tuning on\nthe downstream tasks improves the model from\n30.08 to 27.05 (the improvement ∆=3.03); for JFT\n(low contextual), it is initialized with the redacted\nmodel with ppl=37.90, and privately fine-tuning it\nimproves the perplexity to 25.62 (∆=12.28). This\nshows that although the initialization seems worse\n(30.08 vs 37.90), since the redacted model is fine-\ntuned directly on in-domain redacted data, it does\nlearn useful information from the first redacted-\nfine-tune step, and the secondprivate-fine-tune step\ncan further improve upon the redacted model. For\nJFT (high contextual) for the stress test, although\n45% tokens are masked and the language structure\nis largely impacted, JFT still improves the redacted\nmodel utility from 54.29 to 27.19 (∆=27.10) and\nperforms on par with DPSGD (27.05 vs 27.19).\n6.2 Selective Manual Screening\nAs mentioned earlier, secret detectors can miss\ncertain secrets and we can manually filter out the\nmissed secrets at a small scale and fine-tune with\nthe small manually sanitized set. Denote the orig-\ninal data as D0. We sample 0.1% data from D0,\napply the high entity secret detectors (because it is\nless conservative and could miss secrets), and man-\nually sanitize the missed secrets to get D′. We use\nD′ = 0.1%D0 during redacted-fine-tune to train\n6333\nMNLI QQP QNLI SST-2 WikiText-2 ABCD\nModel Acc↑ 95%-ϵs Acc↑ 95%-ϵs Acc↑ 95%-ϵs Acc↑ 95%-ϵs PPL↓ 95%-ϵs PPL↓ 95%-ϵs\nDPSGD 82.10 2.75 85.41 2.75 84.62 2.57 86.12 2.41 27.05 2.58 8.31 2.65\nMissing ratem(95% CI) (0.3%, 1.2%) (0.3%, 1.2%) (0.1%, 0.6%) (0%, 1.8%) (0.4%, 0.7%) (0.1%, 1.2%)Recall (95% CI) (87.5, 96.7) (85.9, 96.1) (96.4, 99.3) (40.2, 100) (95.6, 97.8) (62.7, 97.4)JFT+light noise 82.76(0.08, 0.43)85.28 (1.40, 1.71)84.88(2.29, 2.68)89.33(0, 0.43)25.21(2.73, 2.92)5.78 (1.08, 1.60)\nConservative Estimation\nMissing ratem 8.6% 8.3% 17.2% 3.0% 16.4% 3.0%Recall 0 0 0 0 0 0JFT+light conservative noise82.00 0.45 84.77 2.91 84.02 2.95 89.22 0.43 26.59 3.03 6.64 1.67\nTable 3: Privacy-amplified JFT performance on all the tasks, with the high entity detector. We report the estimated\n95% confidence interval (CI) of the missing rate m, recall and the corresponding 95% CI of the ϵs.\nManual ScreeningD′(redacted)=0.1%D0,D(private)=100%D0\nTask MNLI QQP QNLI SST-2WikiText-2 ABCDAcc↑ Acc↑Acc↑ Acc↑ PPL↓ PPL↓\nD′size 300 300 100 100 10 10\nDPSGD 82.10 85.41 84.62 86.1227.05 8.31\nRedacted 52.52 75.25 66.48 88.8828.06 9.36JFT+manual screening82.45 86.24 85.00 90.8326.72 7.84\nTable 4: Manual screening results on the high entity\nsecret detector. D0: original data. D′: the inspected\nredacted data. D: the private data. ϵ≈3. D′ size is the\nnumber of records used in the redacted-fine-tune phase.\nthe redacted model, and the entire D0 as Dduring\nprivate-fine-tune to obtain JFT models.\nTable 4 shows the results. D′=0.1%D0 contains\n100∼300 examples for GLUE and 10 articles and\n10 dialogues for Wikitext-2 and ABCD respectively.\nOn all the tasks, JFT achieves better utilities than\nDPSGD. This shows that even fine-tuning with a\nsmall manually-screened in-domain subset can still\nhelp the model learn in-domain information, and\nlead to better utility. We also simulate a completely\nlow-resource setting where we simply have limited\ntraining data (i.e., D′=0.1% D0, D=0.1% D0). See\nSection A.4 for the results.\n6.3 Lightly Noised Optimizer with Privacy\nAmplification\nBesides manually inspecting the missed secrets, we\ncan also use noised optimizers in the first phase to\nprotect the missed secrets from attacks and then\nadopt privacy amplification to estimate the corre-\nsponding privacy parameters. We again perform\nthe experiments on the high entity detector and Ta-\nble 3 shows the results. We convert the missing\nrate m(%) to the recall of the secret detector, i.e.,\nrecall=(1-m/Pct), where Pct is the percentage\nof sensitive tokens among all tokens. “ JFT+light\nnoise” shows the model performance with a noised\noptimizer and privacy amplification employed in\nthe first step. Besides, we add more noise than\nneeded to obtain a conservative model (“JFT+light\nconservative noise”): for instance, MNLI data con-\ntains 8.63% sensitive tokens, although the secret de-\ntector’s missing rate mranges from (0.3%,1.2%)\nwith 95% probability, we assume mto be 8.63%\n(i.e., it miss all sensitive tokens and thus recall=0)\nto calculate and add the noises that are more than\nactually needed.\nThe results show that “JFT+light noise” achieves\nbetter utility than DPSGD, especially on gener-\nation tasks. The perplexity is (25.21 vs 27.05,\n5.78 vs 8.31), and the estimated ϵs ranges from\n(2.73,2.92) and (1.08,1.60) with 95% probability.\nEven for the conservative models, “JFT+light con-\nservative noise” is still better than DPSGD (26.59\nvs 27.05, and 6.64 vs 8.31).\n6.4 Attack Results\nWe perform the canary insertion attack (Carlini\net al., 2019) to empirically show how much the\nmodels memorize the training data unintentionally.\nThe attack is to insert a canary of a certain format\ninto the training data, and calculate its exposure,\nwhich is the rank of the inserted canary amongst all\npossible values of the same format. The lower the\nexposure is, the safer the model is. In our experi-\nments, we insert the canary“My ID is341752” into\nthe training data for 10 times to make the perfor-\nmance difference of different models more salient.\nBy definition, for a six-digit canary, an exposure\nclose to log2(106) ≈19.9 means the canary can be\nextracted by the attackers. The result is in Figure 2.\nEach point on the figure is a model checkpoint.\nThe X-axis shows the perplexity (utility), and the\ny-axis is the exposure (privacy), so Figure 2 shows\ndifferent models’ privacy-utility tradeoffs.\nOne major reason why the model remembers a\ncanary is that it has seen the canary many times.\nFor “No-DP”, initially, its exposure is low because\nit hasn’t seen the canary many times. But because\n6334\nFigure 2: Canary exposure for different models.\nthe model is unprotected, its exposure is unbounded\nand increases dramatically after it accesses the data\nfor more epochs. This suggests that models without\nprotection do memorize the data unintentionally.\nFor protected models (DPSGD, redacted, and\nJFT), if the canary is captured by the detector (“not\nmissed” in the figure), then the exposure does not\nincrease much even if the data are accessed many\ntimes. Under similar exposure, JFT achieves better\nutility than DPSGD and the redacted models.\nBut if the secret detector misses the canary (we\npurposely code it to mark the canary as public), the\nexposure is increased for both “Redacted (missed)”\nand “JFT (missed)”. But if we add light noise in the\nfirst phase (“Redacted+light noise” in red), even if\nthe canary is missed for 10 times, its exposure is\nstill low. If we continue to privately fine-tune in\nthe second phase (“JFT+light noise” in pink), we\ncan further improve the utility but still achieve a\nlow exposure value. Both DPSGD and CRT also\nachieve a low exposure value if the canary is missed\nby the secret detector, but with worse utilities than\n“JFT+light noise”. This shows “ JFT+light noise”\ncan protect missed secrets while achieving better\nutility. We also performed the canary insertion at-\ntack with one canary inserted once, and 10 different\ncanaries inserted once, shown in Section A.5.\nWe also tested the membership inference attack\n(MIA), but it wasn’t successful (inference accuracy\nis around 60% even for public models). Previous\nstudies also observed unsuccessful MIA (Shi et al.,\n2021; Zhao et al., 2022) and our future work in-\ncludes developing better MIA for NLP.\n7 Related Work\nRecent work studied private language models on\nvarious model architectures such as RNNs (McMa-\nhan et al., 2018; Ramaswamy et al., 2020) and large\nlanguage models (Anil et al., 2021; Li et al., 2021;\nYu et al., 2021a). Li et al. (2021) proposed ghost-\nclipping to reduce the computational cost in per-\nsample gradients of DPSGD, and achieved strong\nprivate LLMs. Yu et al. (2021a) added a small set\nof private parameters to off-the-shelf LLMs and\nprivately tune them on private data and obtained\nperformant private models. Most previous works\nachieve canonical DP. Shi et al. (2021) proposed\nSelective-DP for applications with sparse sensitive\ninformation like NLP, and a privacy mechanism\nfor RNN models. Our work proposes an effective\nmechanism for LLMs to achieve SDP and study\nthe impact of secret detectors at different levels.\nOur work is also closely related to utilizing pub-\nlic data for private learning (Papernot et al., 2018;\nTramer and Boneh, 2020; Ghazi et al., 2021; Yu\net al., 2021a). One working direction assumes ac-\ncess to large unlabeled public data to train DP mod-\nels. For example, PATE (Papernot et al., 2016) used\nunlabeled public data and knowledge distillation to\nbuild DP models. Hoory et al. (2021) used public\nmedical data to pre-train domain-specific private\nvocabulary and models. Another direction lever-\naged small public data to guide the private updates\nin lower-dimension subspaces (Zhou et al., 2020;\nYu et al., 2021b). Our work is distinct from previ-\nous studies: instead of querying public data from\noutside, we utilize the public portion of in-domain\ndata and achieve SDP with better model utility.\n8 Conclusions\nIn this paper, we propose JFT, which can achieve\nSelective-DP for large language models. We also\ndesign generalizable secret detectors to provide pro-\ntection at different levels and study their impacts\non the resulting SDP models, and address the prob-\nlem of missed sensitive tokens via selective manual\nscreening and private training with reduced noise,\nwhich is justified by privacy amplification. The\nresults show that the proposed JFT produces SDP\nmodels with strong performance while remaining\nrobust to the canary insertion attack.\nLimitations\nParameter search in DP learning is challenging as\nthe training process takes a long time and is quite\nsensitive to different parameters (Li et al., 2021).\nSo the findings in the paper are based on the param-\neter tuning performed by the authors (Table 5), and\nmore parameter tuning could potentially lead to\n6335\nbetter results than the results reported in the paper.\nIn our experiments, we simply fine-tuned the\nmodels on the in-domain redacted data without ad-\njustment for the redaction. We could potentially\nutilize more sophisticated methods to train better\nredacted models to further improve the JFT utility.\nBesides, for the selective manual screening experi-\nments, we did not adjust theredacted-fine-tune step\nfor the low-resource setting where D′= 0.1%D0.\nFuture work includes how to train a better redacted\nmodel given limited data.\nWhen the gap between the SOTA public model\nand the redacted model is small, the private-fine-\ntuning step cannot further improve the results be-\ncause of the noisy gradients (e.g., in SST-2), we\nplan to develop better algorithms to utilize the\nredacted data and apply denoising methods (Welch\net al., 1995) to close the gap further.\nOne thing to note is that if the secret detector\nmisses a secret and the secret appears multiple\ntimes in the data, then it is likely that the secret\ndetector will miss it multiple times. Therefore,\ndeduplicating the data first is important. We plan\nto study data deduplication in NLP in the future.\nEthical Considerations\nTo prevent real-world harm, all the datasets and\nmodels used in this paper are already public with\neither public or synthesized information. So no real\npersonal information will be leaked.\nThis work tackles the challenge of privacy pro-\ntection and can be utilized in various domains or ap-\nplications to build models that preserve privacy. We\nwill release the code to facilitate privacy-preserving\nmodel building. The canary insertion attack is well-\nknown (Carlini et al., 2019, 2020) and adjusted\nspecifically for our setting, so it cannot be directly\nutilized to attack real-world models successfully.\nAcknowledgements\nWe would like to thank Da Yu, Xuechen Li, and\nZhiliang Tian for the valuable discussions. We also\nthank the anonymous area chair and reviewers for\ntheir insightful suggestions.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC con-\nference on computer and communications security,\npages 308–318.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar,\nand Pasin Manurangsi. 2021. Large-scale differen-\ntially private bert. arXiv preprint arXiv:2108.01624.\nBorja Balle, Gilles Barthe, and Marco Gaboardi. 2018.\nPrivacy amplification by subsampling: Tight analyses\nvia couplings and divergences. Advances in Neural\nInformation Processing Systems, 31.\nHannah Brown, Katherine Lee, Fatemehsadat\nMireshghallah, Reza Shokri, and Florian Tramèr.\n2022. What does it mean for a language model to\npreserve privacy? arXiv preprint arXiv:2202.05520.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neural\nnetworks. In 28th {USENIX}Security Symposium\n({USENIX}Security 19), pages 267–284.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ul-\nfar Erlingsson, et al. 2020. Extracting training\ndata from large language models. arXiv preprint\narXiv:2012.07805.\nDerek Chen, Howard Chen, Yi Yang, Alex Lin, and\nZhou Yu. 2021. Action-based conversations dataset:\nA corpus for building more in-depth task-oriented\ndialogue systems. arXiv preprint arXiv:2104.00783.\nStelios Doudalis, Ios Kotsogiannis, Samuel Haney, Ash-\nwin Machanavajjhala, and Sharad Mehrotra. 2017.\nOne-sided differential privacy. Proceedings of the\nVLDB Endowment.\nCynthia Dwork, Aaron Roth, et al. 2014. The algorith-\nmic foundations of differential privacy. Foundations\nand Trends in Theoretical Computer Science , 9(3-\n4):211–407.\nBadih Ghazi, Noah Golowich, Ravi Kumar, Pasin Ma-\nnurangsi, and Chiyuan Zhang. 2021. Deep learning\nwith label differential privacy. Advances in Neural\nInformation Processing Systems, 34.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremental\nparsing. To appear.\nShlomo Hoory, Amir Feder, Avichai Tendler, Sofia Erell,\nAlon Peled-Cohen, Itay Laish, Hootan Nakhost, Uri\nStemmer, Ayelet Benjamini, Avinatan Hassidim, et al.\n2021. Learning and evaluating a differentially pri-\nvate pre-trained language model. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 1178–1189.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\n6336\nXuechen Li, Florian Tramer, Percy Liang, and Tatsunori\nHashimoto. 2021. Large language models can be\nstrong differentially private learners. arXiv preprint\narXiv:2110.05679.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nH Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2018. Learning differentially private\nrecurrent language models. In International Confer-\nence on Learning Representations.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. ICLR.\nNicolas Papernot, Martín Abadi, Ulfar Erlingsson,\nIan Goodfellow, and Kunal Talwar. 2016. Semi-\nsupervised knowledge transfer for deep learn-\ning from private training data. arXiv preprint\narXiv:1610.05755.\nNicolas Papernot, Shuang Song, Ilya Mironov, Ananth\nRaghunathan, Kunal Talwar, and Úlfar Erlingsson.\n2018. Scalable private learning with pate. ICLR.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nSwaroop Ramaswamy, Om Thakkar, Rajiv Mathews,\nGalen Andrew, H Brendan McMahan, and Françoise\nBeaufays. 2020. Training production language mod-\nels without memorizing user data. arXiv preprint\narXiv:2009.10031.\nWeiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, and Zhou\nYu. 2021. Selective differential privacy for language\nmodeling. arXiv preprint arXiv:2108.12944.\nFlorian Tramer and Dan Boneh. 2020. Differentially\nprivate learning needs better features (or much more\ndata). arXiv preprint arXiv:2011.11660.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nGreg Welch, Gary Bishop, et al. 1995. An introduction\nto the kalman filter.\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi,\nHuseyin A Inan, Gautam Kamath, Janardhan Kulka-\nrni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,\net al. 2021a. Differentially private fine-tuning of\nlanguage models. arXiv preprint arXiv:2110.06500.\nDa Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-\nYan Liu. 2021b. Large scale private learning via low-\nrank reparametrization. In International Conference\non Machine Learning, pages 12208–12218. PMLR.\nXuandong Zhao, Lei Li, and Yu-Xiang Wang. 2022.\nProvably confidential language modelling. arXiv\npreprint arXiv:2205.01863.\nYingxue Zhou, Zhiwei Steven Wu, and Arindam Baner-\njee. 2020. Bypassing the ambient dimension: Pri-\nvate sgd with gradient subspace identification. arXiv\npreprint arXiv:2007.03813.\n6337\nA Appendix\nA.1 Proofs\nTheorem 1 (restated). Given that 1) in the first\nphase, the data used for fine-tuning does not con-\ntain sensitive tokens and a public optimizer is used,\nand 2) in the second phase, the private optimizer\nachieves (ϵ,δ)-DP ,JFT achieves (ϵ,δ)-SDP .\nProof. Since the first phase does not incur any pri-\nvacy loss on the sensitive tokens, the first phase\nachieves (0,0)-SDP.\nDP implies SDP. In other words, if an algorithm\nachieves (ϵ,δ)-DP, then it also satisfies (ϵ,δ)-SDP.\nHence, the second phase achieves (ϵ,δ)-SDP. By\nthe composition of SDP, JFT achieves (ϵ,δ)-SDP.\nNote that the first phase achieves (0,0)-SDP but\ncannot achieve (0,0)-DP. DP aims to protect the\nentire token sequence, whether it is considered\nsensitive or non-sensitive by the policy function.\nBecause the first phase does not noise the non-\nsensitive tokens at all, it cannot ensure DP.\nA.2 Implementation Details\nNotes on special tokens. When fine-tuning LLMs,\nit is a common practice to add new special tokens\nto fit the need of the downstream tasks. For exam-\nple, in dialogue tasks, we often have prompts like\n“SYS:” and “USR:” to indicate the speaker in the\ndata. This step doesn’t affect the public models that\nmuch (20.48 without special tokens vs 20.44 with\nspecial tokens), but as it does change the model\nstructure (additional embeddings) and the model\ninitialization, we notice that in our experiments,\nDPSGD is sensitive to the addition of special to-\nkens (because the model initialization is changed):\nafter reasonable amounts of parameter tuning (see\nTable 5), DPSGD initialized with the original GPT\nachieves 27.05 in PPL, while DPSGD with added\nspecial tokens achieves 30.32 in PPL on Wikitext-2.\nThe gap could potentially be reduced with more pa-\nrameter tuning, but we just want to mention that in\npractice, it may not be easy to find the best param-\neters. In our experiments, for WikiText-2, we add\n<mask>as the special token; for ABCD, as it is a\ndialogue task, we add <mask>, “ACT:”, “SYS:”,\nand “USR:”. Since all the JFT models have added\nspecial tokens, we report two DPSGD results, one\nwithout special tokens and one with special tokens,\nfor a fair comparison in terms of model structure.\nAlso, the secret detectors replace the sensitive\ninformation with artificial special tokens such as\n“<SSN>” and “ <NAME>”. But these tokens\ndon’t appear in the validation or test set and thus in-\nserting them will skew the training data distribution\nand lead to inferior results, especially when the sen-\nsitive token portion is high. In our experiments, we\nmask the detected sensitive information with the\nsame “<mask>” token and ignore this special to-\nken in the loss calculation. In this way, for models\nwith an existing “<mask>” token (like Roberta),\nwe can utilize the existing embedding; for models\nwithout “<mask>”, the model only needs to learn\none additional special embedding. This improves\nthe validation perplexity from 64.82 to 37.90 for\nthe redacted GPT2 model with the low contextual\nsecret detector.\nWe could potentially apply the same secret detec-\ntor on the validation and test set to mitigate special\ntoken issues. However, this causes two concerns:\n1) if the secrete detector redacts 45% of tokens (e.g.\nthe high contextual one redacts all the verbs, etc),\nthen the performance on validation/test is not infor-\nmative at all, and cannot be compared to the public\nbaseline; 2) in the past privacy literature (Papernot\net al., 2018; Ghazi et al., 2021), the conventional\nproblem setup considers validation/test sets as pub-\nlic and focuses only on the training privacy. We\ninherit the same treatment to be comparable with\nprior literature. But in privacy-related NLP prob-\nlems, how to treat the validation/test sets remains\nan open question as they can contain private infor-\nmation.\nOur experiments find that adding many special\ntokens impacts the results. In the future, we plan to\nstudy how to treat special tokens better in privacy-\npreserving LMs.\nParameters Range\nϵ 3\nClipping norm C 0.1\nBatch size {256, 512, 1024}\nLearning rate {5, 10, 50} ·10−5\nEpochs E {10, 100, 200, 600}\nNoise scale σ Pre-calculated so that ϵis\nspent when training ends\nTable 5: Hyper-parameter tuning range.\nHyper-parameter tuning. Hyper-parameter tun-\ning remains a challenging problem in DP learning\nas the training takes a long time, and the model\ncan be sensitive to the hyper-parameters. Guided\n6338\nMNLI QQP QNLI SST-2 WIKITEXT-2 ABCD\nModelDetectorAcc↑ PrivacyAcc↑ PrivacyAcc↑ PrivacyAcc↑ Privacy PPL↓ PrivacyPPL↓ Privacy\nCRT low ent81.45 (2.21,δc)-Conf84.10 (2.47,δc)-Conf83.36 (2.30,δc)-Conf87.39 (2.22,δc)-Conf28.20 (2.19,δc)-Conf9.09 (2.73,δc)-ConfJFT low ent85.74(0.92,δs)-SDP88.19(2.58,δs)-SDP89.57(2.37,δs)-SDP92.09(2.06,δs)-SDP21.86(2.58,δs)-SDP6.09(2.71,δs)-SDP\nCRT high ent81.24 (2.63,δc)-Conf83.64 (2.13,δc)-Conf82.78 (2.56,δc)-Conf87.27 (2.22,δc)-Conf28.47 (1.38,δc)-Conf9.20 (2.71,δc)-ConfJFT high ent85.61(0.99,δs)-SDP88.05(2.58,δs)-SDP89.35(2.37,δs)-SDP92.20(2.12,δs)-SDP22.55(2.58,δs)-SDP6.25(2.71,δs)-SDP\nCRT low ctx78.85 (2.46,δc)-Conf79.17 (2.50,δc)-Conf81.15 (2.24,δc)-Conf85.67 (2.22,δc)-Conf28.87 (0.69,δc)-Conf12.70 (0.34,δc)-ConfJFT low ctx85.02(1.23,δs)-SDP87.00(2.41,δs)-SDP87.99(2.52,δs)-SDP92.43(2.17,δs)-SDP25.62(2.58,δs)-SDP8.80(2.71,δs)-SDP\nStress-test\nCRT high ctx74.61 (2.68,δc)-Conf77.57 (2.64,δc)-Conf79.41 (2.30,δc)-Conf86.01 (2.33,δc)-Conf29.13 (0.47,δc)-Conf13.11 (0.35,δc)-ConfJFT high ctx84.11(1.18,δs)-SDP86.42(2.67,δs)-SDP87.06(2.41,δs)-SDP91.17(2.17,δs)-SDP27.19(1.96,δs)-SDP12.93(2.71,δs)-SDP\nTable 6: Comparison between JFT and CRT. JFT achieves (ϵs,δs)-Selective-DP, while CRT achieves (ϵc,δc)-\nConfidentiality, so the ϵare not directly comparable. For QNLI and SST-2, δs = δc ≈1e-5; for MNLI and QQP,\nδs = δc ≈1e-6. For generation task, δs = δc ≈1e-6. We add the same amount of noise to JFT and CRT (noise\ncalculated based on ϵ=3), and report the best model validation utility. “CRT” results are based on our implementation\nof Zhao et al. (2022).\nBatch size Epoch PPL\n256 200 27.04\n512 200 27.05\n1024 200 27.18\n1024 600 27.01\n1024 10 28.84\nTable 7: Model utility with different hyper-parameters.\nby Li et al. (2021), we tune the learning rate, the\nnumber of training epochs, and the batch size on\nthe validation set and report the best results. Please\nrefer to Table 5 for the parameter range.\nIn our experiments, we tuned the batch size,\nlearning rate, and epoch number for the best models\nin different settings. In practice, we found increas-\ning the epoch number and batch size helps, but\nthese two factors can interact with each other, the\ngain could be small after the batch size and epoch\nnumber are large enough. Table 7 shows the conver-\ngent model utility of DPSGD on WikiText-2 with\ndifferent hyper-parameters. In our experiments, we\npick the best set of parameters that balances the\ncomputational cost and the utility.\nA.3 Comparison with CRT\nTable 6 shows the comparison between JFT and\nCRT on the model utility and privacy guarantee.\nNote that JFT achieves (ϵs,δs)-SDP, and CRT re-\nalizes (ϵc,δc)-Confidentiality, because the underly-\ning privacy notion is different, we cannot directly\ncompare the ϵ. We pre-calculate the noises given\nϵ= 3, add the same amount of noises to both CRT\nand JFT, and report the best valid utility. With the\nsame amount of noise added, JFT achieves better\nmodel utilities than CRT for all the tasks across\ndifferent secret detectors.\nA.4 Low-Resource Results\nPrivacy protection is important in oftentimes low-\nresource domains such as health care. We simulate\nthe low-resource setting where we have both lim-\nited redacted and private data, i.e., (D′=0.1% D0,\nD=0.1% D0) and the results are in Table 8.\nThe redacted model always performs better than\nDPSGD, suggesting that for low-resource settings,\nwe can simply redact the data to train the model\ninstead of employing differential privacy. Also,\nfor the QNLI task, JFT shows promising results.\nWith 0.1% training data (100 records), the redacted\nmodel improve the accuracy from random guess\nto 66.5%. JFT can even further improve the accu-\nracy to 67.49%. But the baseline DPSGD fails to\nimprove the model at all (accuracy=50.54%). We\nplan to study how to better fine-tune the redacted\nmodel privately with limited data.\nManual Screening D′=0.1%D0,D=0.1%D0\nTask MNLI QQP QNLI SST-2 WikiText-2 ABCDAcc↑Acc↑Acc↑Acc↑ PPL↓ PPL↓\nD′ 300 300 100 100 10 10\nDPSGD 32.86 63.18 50.54 56.77 30.08 13.66\nRedacted 52.52 75.2566.4888.88 28.06 9.36JFT+manual screening50.61 75.1867.4988.53 28.15 9.40\nTable 8: Manual screening results when both Dand D′\nare limited in size. D′ is the redacted data, D is the\nprivate data.\nA.5 Canary Insertion Attack\nFigure 3 shows the canary insertion attack result\nwhen the canary is inserted only once. We see\nthat the exposure is low for all the models (<3,\nso not extractable) including the public “No-DP”\nwithout any protections. This agrees with Figure 4\nin Carlini et al. (2019) that when the canary is\ninserted for very limited times, its exposure is low.\n6339\nFigure 3: Exposure for different models when the canary\nis inserted only once. The exposures are all small (<3)\neven for public models.\nFigure 4 shows the canary insertion attack re-\nsults when we insert ten different canaries into the\ntraining data. The exposure is the average expo-\nsure of the ten canaries. In this experiment, we\ntreat the inserted canaries as the only secrets, so\nthe “Redacted” and “JFT” model utilities are close\nto the black “No-DP” model, and we can better\ncompare with the “No-DP” model. We also artifi-\ncially vary the recall of the secret detector to see\nthe effect. “Recall=0.4” means that the detector\ncan only detect four of the ten canaries. Because\neach canary only appears once in the dataset, the\nexposure is low, similar to the ones in Figure 3. But\nif the recall is higher (0.6), the exposure will still be\nlower. “JFT +light noise” achieves low exposure,\nsimilar to the baseline DPSGD that protects all ca-\nnaries, but with much higher utility over DPSGD.\nThese experiments may suggest that for large\nNLP models, if the sensitive tokens only appear for\nvery limited times, they may not be extracted using\nthe canary insertion attack.\nFigure 4: Exposure for different models when we insert\nten different canaries.\n6340"
}