{
  "title": "Vision Transformer With Attentive Pooling for Robust Facial Expression Recognition",
  "url": "https://openalex.org/W4311414773",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222533627",
      "name": "Xue, Fanglei",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A4311414924",
      "name": "Wang, Qiangchang",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A4221991932",
      "name": "Tan, Zichang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2683191266",
      "name": "Ma Zhongsong",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2489541830",
      "name": "Guo, Guodong",
      "affiliations": [
        "Baidu (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6795986427",
    "https://openalex.org/W6769650007",
    "https://openalex.org/W3209798173",
    "https://openalex.org/W2887057293",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W6797824439",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2713788831",
    "https://openalex.org/W6661087397",
    "https://openalex.org/W3209397829",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3195286673",
    "https://openalex.org/W3118405827",
    "https://openalex.org/W2954148997",
    "https://openalex.org/W6754484989",
    "https://openalex.org/W6791705549",
    "https://openalex.org/W6792695861",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W6787972765",
    "https://openalex.org/W2964347177",
    "https://openalex.org/W2083021723",
    "https://openalex.org/W6798391549",
    "https://openalex.org/W6798433237",
    "https://openalex.org/W6797970987",
    "https://openalex.org/W6792155083",
    "https://openalex.org/W2436394355",
    "https://openalex.org/W6730179637",
    "https://openalex.org/W3035086596",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W3208945181",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W3209454485",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W1834627138",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W1976376589",
    "https://openalex.org/W2481681431",
    "https://openalex.org/W3003720578",
    "https://openalex.org/W6755364507",
    "https://openalex.org/W3019410392",
    "https://openalex.org/W2738672149",
    "https://openalex.org/W2745497104",
    "https://openalex.org/W2904483377",
    "https://openalex.org/W2963712289",
    "https://openalex.org/W2955557105",
    "https://openalex.org/W2942109599",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3210643189",
    "https://openalex.org/W3210812086",
    "https://openalex.org/W3163598089",
    "https://openalex.org/W6672481139",
    "https://openalex.org/W6800945776",
    "https://openalex.org/W2965924668",
    "https://openalex.org/W3179103990",
    "https://openalex.org/W2934832377",
    "https://openalex.org/W6762148536",
    "https://openalex.org/W2886300652",
    "https://openalex.org/W1968684116",
    "https://openalex.org/W2900164843",
    "https://openalex.org/W6804700279",
    "https://openalex.org/W3115534470",
    "https://openalex.org/W3177017840",
    "https://openalex.org/W2294427751",
    "https://openalex.org/W2145310492",
    "https://openalex.org/W4388315058",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6753412334",
    "https://openalex.org/W3214877835",
    "https://openalex.org/W3184571633",
    "https://openalex.org/W2724944050",
    "https://openalex.org/W6726946684",
    "https://openalex.org/W3034552680",
    "https://openalex.org/W3175546442",
    "https://openalex.org/W2341528187",
    "https://openalex.org/W3118530108",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W2165731615",
    "https://openalex.org/W4226487943",
    "https://openalex.org/W6796721132",
    "https://openalex.org/W2963363102",
    "https://openalex.org/W3034504038",
    "https://openalex.org/W6737664043",
    "https://openalex.org/W3185372235",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3035336958",
    "https://openalex.org/W2014185685",
    "https://openalex.org/W2143238378",
    "https://openalex.org/W1996958556",
    "https://openalex.org/W2969985801",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W6726453277",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W3182935764",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3122081138",
    "https://openalex.org/W2515770085",
    "https://openalex.org/W2088575594",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W4287082647",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4288102735",
    "https://openalex.org/W3200032182",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W4226407477",
    "https://openalex.org/W2896277673",
    "https://openalex.org/W2520774990",
    "https://openalex.org/W3180874665",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2952634764",
    "https://openalex.org/W2952979850",
    "https://openalex.org/W3172801447",
    "https://openalex.org/W2744909235",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W3167651619",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4287083813",
    "https://openalex.org/W4297775537",
    "https://openalex.org/W2561238782",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3101998545"
  ],
  "abstract": "Facial Expression Recognition (FER) in the wild is an extremely challenging\\ntask. Recently, some Vision Transformers (ViT) have been explored for FER, but\\nmost of them perform inferiorly compared to Convolutional Neural Networks\\n(CNN). This is mainly because the new proposed modules are difficult to\\nconverge well from scratch due to lacking inductive bias and easy to focus on\\nthe occlusion and noisy areas. TransFER, a representative transformer-based\\nmethod for FER, alleviates this with multi-branch attention dropping but brings\\nexcessive computations. On the contrary, we present two attentive pooling (AP)\\nmodules to pool noisy features directly. The AP modules include Attentive Patch\\nPooling (APP) and Attentive Token Pooling (ATP). They aim to guide the model to\\nemphasize the most discriminative features while reducing the impacts of less\\nrelevant features. The proposed APP is employed to select the most informative\\npatches on CNN features, and ATP discards unimportant tokens in ViT. Being\\nsimple to implement and without learnable parameters, the APP and ATP\\nintuitively reduce the computational cost while boosting the performance by\\nONLY pursuing the most discriminative features. Qualitative results demonstrate\\nthe motivations and effectiveness of our attentive poolings. Besides,\\nquantitative results on six in-the-wild datasets outperform other\\nstate-of-the-art methods.\\n",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nVision Transformer with Attentive Pooling for\nRobust Facial Expression Recognition\nFanglei Xue, Qiangchang Wang, Zichang Tan, Zhongsong Ma, and Guodong Guo,Senior Member, IEEE\nAbstract—Facial Expression Recognition (FER) in the wild is an extremely challenging task. Recently, some Vision Transformers (ViT)\nhave been explored for FER, but most of them perform inferiorly compared to Convolutional Neural Networks (CNN). This is mainly\nbecause the new proposed modules are difﬁcult to converge well from scratch due to lacking inductive bias and easy to focus on the\nocclusion and noisy areas. TransFER, a representative transformer-based method for FER, alleviates this with multi-branch attention\ndropping but brings excessive computations. On the contrary, we present two attentive pooling (AP) modules to pool noisy features\ndirectly. The AP modules include Attentive Patch Pooling (APP) and Attentive Token Pooling (ATP). They aim to guide the model to\nemphasize the most discriminative features while reducing the impacts of less relevant features. The proposed APP is employed to\nselect the most informative patches on CNN features, and ATP discards unimportant tokens in ViT. Being simple to implement and\nwithout learnable parameters, the APP and ATP intuitively reduce the computational cost while boosting the performance by ONL Y\npursuing the most discriminative features. Qualitative results demonstrate the motivations and effectiveness of our attentive poolings.\nBesides, quantitative results on six in-the-wild datasets outperform other state-of-the-art methods.\nIndex Terms—Facial expression recognition, attentive pooling, vision transformer, affect, deep learning.\n!\n1 I NTRODUCTION\nF\nACIAL expression is one of the most important ways\nfor people to express their emotions [1]. Facial Expres-\nsion Recognition (FER) requires that the computer program\ncould automatically recognize the expression from an in-\nput face image. The FER task has attracted broad interest\nin the computer vision community [2], [3], [4] due to its\nwide applications in human-computer interactions, medical\nprogress monitoring, driver fatigue monitoring, and many\nother ﬁelds.\nHowever, FER is a very challenging task, especially in\nthe wild. This is mainly because of the signiﬁcant intra-\nclass variances and inter-class similarities among expression\ncategories, which differ from the general image classiﬁcation\ntask. For example, the same people in the same illumination\nand pose may have different expressions, while people with\ndifferent identities, ages, gender, and pose may express the\nsame emotion. In the past few years, with the development\nof the convolutional neural network, many methods [4], [5],\n[6], [7], [8], [9] have been proposed and greatly improve the\nperformance of FER.\n• This work was done when Fanglei Xue and Qiangchang Wang were\ninterns at Institute of Deep Learning, Baidu Research. The ﬁrst two\nauthors contribute equally. Corresponding authors: Zhongsong Ma and\nGuodong Guo.\n• Fanglei Xue and Zhongsong Ma are with University of Chinese Academy\nof Sciences, and also with the Key Laboratory of Space Utilization, Tech-\nnology and Engineering Center for Space Utilization, Chinese Academy\nof Sciences, Beijing, China. (xuefanglei19@mails.ucas.ac.cn; mazhong-\nsong@csu.ac.cn)\n• Qiangchang Wang is with West Virginia University, Morgantown, USA.\n(qw0007@mix.wvu.edu)\n• Zichang Tan and Guodong Guo are with Institute of Deep Learning, Baidu\nResearch, and also with National Engineering Laboratory for Deep Learn-\ning Technology and Application, Beijing, China (tanzichang@baidu.com;\nguodong.guo@mail.wvu.edu)\nManuscript received April 19, 2005; revised August 26, 2015.\nFig. 1. An illustration of attention maps in TransFER [10] (left) and the\nattentive pooling results of APP (right). Our APP directly discards the\nbackground features to reduce the inﬂuence of noisy areas and save\ncomputation time.\nRecently, the Vision Transformer (ViT) was proposed\nfor image classiﬁcation [11] and achieved promising per-\nformance with the non-local self-attention mechanism. It\nshows great potential for solving visual tasks. Some re-\nsearchers adopt the ViT for FER [10], [12], [13]. However, the\nperformance is inferior to the state-of-the-art CNNs except\nTransFER [10]. The main issue is that the ViT needs a large\namount of data to train due to a large number of parameters\nand lacks the inductive bias [11]. Existing FER datasets\nare much smaller compared to general image classiﬁcation\ndatasets (i.e. ImageNet [14]), making it hard for newly pro-\nposed Transformer-based modules to converge well and are\neasy to mistakenly focus on some occlusion or background\nareas. Many regularization [15], [16] and attention [6], [8],\n[10], [13], [17] methods have been proposed to address this\nissue. The TransFER model generates an attention map and\nmultiplies it with the feature maps to reduce the impact\nof noisy features. We investigated the TransFER model and\nﬁnd that the model has learned to distinguish informative\nareas from noisy areas (as illustrated in Fig. 1). However,\narXiv:2212.05463v1  [cs.CV]  11 Dec 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nthe noisy features are still fed into the downstream models\nin TransFER. Hence, we raise a question: Why do we still\ncompute noisy features even though we have already\nknown they are noises?\nBeneﬁting from the ﬂexible design of the Transformer\nmodel, it could adopt any number of tokens as input\nwithout changing the model parameters. Inspired by this,\nwe propose the APP module to discard the noisy features\ndirectly. As illustrated in the right part of Fig. 1, the noisy\nfeatures are now directly pooled (denoted as a small white\ngrid) instead of multiplying with a small value.\nAs for the Transformer block, it is built based on the\nattention mechanism, making it more intuitive to perform\nattentive pooling. Recently, CPVT [18] found that using\nthe global average pooling (GAP) to replace the [class]\ntoken could produce an even better performance. DeepViT\n[19] further investigated this phenomenon, ﬁnding that the\nattention maps become similar after particular layers. To re-\nduce the redundancy in deep blocks, DeepViT [19] proposed\na Re-attention method to increase the diversity of different\nlayers, LV-ViT [20] proposed a token labelling strategy to\ngive a label to every token to supervise. Different from that,\nand inspired by the Max Pooling in CNNs (as illustrated in\nFig. 2 (a)) to reduce the size of feature maps and obtains an\nabstract and robust representation, we directly drop the less\nessential tokens and only remain a small number of tokens\nto embed the information. Speciﬁcally, the ATP module\nutilizes the attention mechanism in the Transformer and\ndirectly discards the less important tokens in deep blocks.\nFor a better understanding, the differences between these\nmethods are illustrated in Fig. 2.\nFig. 2 (b) demonstrates the vanilla ViT model where\nthe patch token number remains constant. To interrupt the\npropagation of noisy features, the ATP gradually decreases\nthe token number after a speciﬁc layer, as illustrated in\nFig. 2 (c). Compared with the Max Pooling in CNN models,\nwhich can only be pooled to a quarter at least, the ATP\ndecreases the feature map size in a more controllable way.\nOur proposed APViT model that combines APP and ATP is\nillustrated in Fig. 2 (d).\nOur contributions include the following:\n1) An Attentive Patch Pooling (APP) module is pro-\nposed to select the most distinguishable local\npatches from the CNN feature maps, which could\nprevent the noisy patches from passing through to\nthe downstream modules to affect the recognition\nperformance.\n2) An Attentive Token Pooling (ATP) module is de-\nveloped to utilize the attention mechanism of the\nTransformer model only to pay attention to the top-\nk relevant tokens to reduce the inﬂuence caused\nby noises and occlusions and save the computation\ntime.\n3) Experimental results on several challenging in-the-\nwild datasets demonstrate the advantages of our\nAPViT method over the state of the arts.\n2 R ELATED WORK\nFacial expression recognition (FER) frameworks mainly con-\nsist of three processes: face detection and alignment, feature\n(a)CNN (b)ViT\n(c)ATP (d)APP&ATP\nout\nout\n:[class]token\noutAPP\nout\nFig. 2. Illustration of the feature map size or token number of different\narchitectures. (a) denotes the Max Pooling in CNN to quarter the feature\nmap size. (b) denotes the vanilla ViT with a ﬁxed token number. (c)\ndenotes our ATP , which gradually reduces the number of tokens. (d)\ndenotes our APViT, which consists of the APP and ATP to pool CNN\nfeatures and ViT tokens simultaneously.\nextraction, and classiﬁcation. To reduce the impact of noisy\nbackground, a face detector (such as MTCNN [23] and\nRetinaFace [24]) ﬁrst detects faces from the image and aligns\nthem to canonical alignments with corresponding detected\nlandmarks. After that, a feature extractor is responsible for\nextracting discriminative features from the facial image.\nThese features are then classiﬁed into expression categories.\nIn the past decades, FER usually relied on well-designed\nhand-crafted features, which could be further grouped into\ntexture-based and geometry-based features. For example,\nLBP [2], Gabor [25], HOG [26], and SIFT [27] are proposed\nto utilize texture-based features. On the other hand, many\ngeometry-based methods [28], [29], [30] take consider of fea-\ntures of some typical areas (such as eyes, noses, and mouths)\nbased on landmarks. Most of these methods perform well\non laboratory-collected databases. However, when facing in-\nthe-wild facial images, such as various poses and occlusions,\ntheir performances are not as good as recently proposed\nlearning-based methods [4], [5], [6], [7], [8], [9]. With the help\nof public large-scale in-the-wild facial expression datasets,\ndeep learning methods have achieved many breakthroughs.\nLi et al . [4] proposed the DLP-CNN method to extract\ndiscriminative features while Cai et al. [5] proposed a island\nloss function to achieve the similar goal. Li et al. [6] utilized\nan attention mechanism to let the CNN network focus on\nthe most discriminative un-occluded regions to overcome\nthe occlusion problem.\nOur proposed attentive pooling modules are mainly\nbased on the patch attention mechanism and could affect\ntokens in the Transformer model. So we review the previous\nworks in these two respects as follows:\n2.1 FER Based on Patch Attention\nFor high-resolution images, ATS [31] samples locations of\ninformative patches from an attention distribution based on\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nSelfAttention\nCNNExtractor LANet\nSelectTop-k\nSelecttopk\n×M\noutputMLPHead\nATPBlock\n…\nCNN P Flatten *Patch tokens\nP:Poolingthefeaturemapbytopkattentionpositions\n+Position embedding\n∗:Learnable [class] token\nAttn. Module\nP\n…Flatten\nATPBlocks\nATPBlock\n…\n………\n…………\n**\nAPP\n/u1D441\" /u1D458(/u1D458</u1D4412) /u1D458⋅r('/2)tokennum:\ncls\n…0.10.00.1\n0.0\n0.10.20.20.2 …ATP\nscores\nFig. 3. The overall architecture of our APViT model. Firstly, feature maps are extracted from the facial images by a CNN extractor (e.g. ResNet [21],\nMobileNet [22]). Secondly, an attention map is calculated by the attention module (attn f), which guides the APP module to select corresponding\ninformative tokens. Thirdly, the selected tokens are ﬂattened and fed into M stacked ATP enhanced Transformer Blocks (denote as ATP Blocks)\nwith an additional learnable [class] token attached. Similar to the APP , the ATP Blocks further select the most discriminative tokens with the\nmulti-head self-attention mechanism. Thus, the patch information would be gathered to the [class] token as the number of kept patch tokens\ndecreases gradually. Finally, an MLP Head is attached to generate the ﬁnal classiﬁcation result from the [class] token. The left bottom blue\ndashed rectangle indicates the pooling process from the view of feature information ﬂow. The APP and ATP select the most distinguished patches\nand tokens, avoiding the noisy and occluded regions passing to the downstream calculation.\na lower resolution image to save computation costs. NTS-\nTop [32] improves this attention sampling with a differ-\nentiable top-K patch selection module. STTS [33] selects\ntokens in both temporal and spatial dimensions to save\ncomputation in video tasks based on a scorer network\nand a differentiable top-K selection. For FER task, the\nfacial expression is invoked by the movements of several\nfacial muscles. Some researchers found that few patch areas\ncontribute mostly to expression recognition [34]. Many FER\nmethods [28], [35], [36] split the face image into patches and\nselect discriminative regions with an attention mechanism.\nZhong et al . [35] proposed a two-stage multi-task sparse\nlearning framework to locate common patches shared by\ndifferent expressions and speciﬁc patches useful for a par-\nticular expression. Happy et al. [28] adopted the one-against-\none classiﬁcation method to determine salient facial patches.\nNineteen patches around the eyes, nose, and mouth are pre-\nselected as candidate patches. A patch is selected if it can\nclassify the two expressions accurately.\nRecent deep learning methods utilize attention mech-\nanisms that learn an attention weight and multiply this\nweight with the features. Li et al . [6] selected 24 patches\nbased on facial landmarks and proposed the Patch-Gated\nUnit to perceive occluded patches. Zhao et al. [37] divided\nextracted feature maps into several local feature maps and\nemployed a Convolution Block Attention Module [38] to\ngenerate attention maps. The generated attention maps are\nmultiplied by the original local feature maps to propa-\ngate gradients. MViT [13] utilizes two transformer mod-\nules where one generates a mask to ﬁlter out complex\nbackgrounds and occlusion patches, and the other is the\nvanilla ViT model to perform the classiﬁcation. However,\nbackground features in these methods still participate in\nthe following calculation even though we know that they\nare less critical and may cause misfocus to be prejudicial\nto recognition. Different from this, our Attentive Pooling\nmodules directly discard these unimportant features, which\ncould prevent the noisy information from passing to the\nfollowing module and guide the model to ONLY focus on\nthe most informative features. Another difference between\nour method and these methods is that we ﬁnd the hand-\ndesigned mask generator performs better than the learning-\nbased one. And beneﬁtting this, our APViT could apply the\norigin top-K operation to replace the fancy differentiable\nones.\n2.2 Vision Transformers on FER\nThe Transformer architecture [39] was ﬁrst proposed for\nmachine translation based on attention mechanisms. And\nit is interesting to explore the Transformer to visual tasks\n[40], [41]. Vision Transformer (ViT) [11] is the ﬁrst work\nto directly apply the vanilla Transformer with few modi-\nﬁcations for image classiﬁcation. By pre-training on large\ndatasets (e.g. ImageNet-1k, ImageNet-21k [14], and JFT\n[42]) and ﬁne-turning on the downstream target dataset,\nViT has achieved excellent results compared to the CNN\nnetworks [43], [44], [45].\nSome researchers also introduced ViTs to FER. Ma et al.\n[12] proposed an attention selective fusion module to lever-\nage feature maps which are generated by two-branch CNNs,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nand applied the vanilla ViT module to explore relationships\nbetween visual tokens. Li et al . [13] proposed the MViT,\nutilizing two transformer modules where one generates\na mask to ﬁlter out complex backgrounds and occlusion\npatches, and the other one is the vanilla ViT model to\nperform the classiﬁcation. A squeeze and excitation module\n[46] was proposed before the MLP head in the ViT module\nto optimize the learning process on small facial expression\ndatasets. TransFER [10] extracts abundant attention infor-\nmation with multi-branch local CNNs and the multi-head\nself-attention in the ViT and forces these attention features\nto be diverse by a multi-attention dropping module.\nUnlike these methods, we proposed two attentive pool-\ning methods without bringing extra training parameters to\nthe ViT model, fully using the pre-trained weights. The\npooling operation also forces the model only to pay atten-\ntion to the most discriminative features, reducing the inﬂu-\nence of occlusions and noises and saving the computation\ntime.\n3 P ROPOSED METHOD\n3.1 Overview\nWe propose two attentive pooling modules to gradually\npool features in the convolutional neural network and vision\ntransformer. As discussed above, our primary motivation is\nto select informative features and drop the features of back-\nground and occlusion with as few additional parameters\nas possible to overcome the over-ﬁtting problem caused by\nlimited data.\nFig. 3 illustrates the framework of our proposed attentive\npooling methods based on the hybrid ViT-small model\n[11]. A facial image is ﬁrst input into a CNN backbone to\nextract feature maps, then ﬂattened and fed into multiple\nTransformer Blocks.\nBesides generating the patch tokens, the feature maps\nextracted from the CNN are encoded to an attention map\nby the attention generating method (denoted as attn f).\nThe proposed APP module takes this attention map as\ninput and selects the top-k patch tokens according to their\ncorresponding attention values.\nThe selected patch tokens and the [class] token are\njointly fed into the Transformer blocks [39] which consist\nof alternating layers of multi-head self-attention (MSA) and\nMLP blocks to explore the relationship between different\npatches in a global scope. The ATP module is proposed\nfor the Transformer Blocks to gradually decrease the token\nnumber, forcing the model to pay attention to discriminative\ntokens and eliminate the noisy tokens about background\nand occlusions. After the Transformer encoder, the[class]\ntoken is used to generate the ﬁnal predictions through a\nsingle fully connected layer.\n3.2 Attentive Patch Pooling\nSince the CNNs slide through the whole face, features from\nuninformative areas may introduce noises to the feature\nmaps, which may be harmful to recognition. Our APP\nmodule is employed to locate informative areas and drop\nthe uninformative ones to boost recognition performance.\nThe shape of the feature maps extracted from the CNN\nbackbone is [H, W, C] where H, W, and C represent the\n1×1Conv1×1Conv\nSum\nAbs(a)\n(b)\nFig. 4. Illustration of the two kinds of pooling criteria. (a) denotes the\nhand-designed generator, which calculates the sum of the absolute\nvalues of the feature maps. (b) denotes the learning-based generator.\nTaking the LANet [47] as an example, two 1 × 1 convolutional layers\ngradually decrease the channel dimension to 1.\nfeature maps’ height, width, and channel number, respec-\ntively. The APP module takes these attention maps as input\nand selects the top-k most important patches from the\ntotal H ×W patches according to the important weight of\neach position. To achieve this, we need to deﬁne a pooling\ncriterion to guide where to pool and a pooling method to\nprocess pooling.\nPooling Criterion. To determine which parts to pool,\nwe need a pooling criterion to denote the importance of\nevery pixel. Each pixel in the feature map denotes a small\npatch of the original facial input because the Max Pooling\nlayers enlarge the reception ﬁeld. So given the feature maps\nin [H, W, C], an attention map with a size [H, W,1] is\nneeded to denote the relevant weight. In other words, the\npooling criterion Fis responsible for reducing the channel\ndimension of the feature maps to one.\nF: RH×W×C − →RH×W×1 (1)\nThere are a few methods that could complete this\ntask. We group them into hand-designed generators and\nlearning-based generators.\n1) Hand-designed generatorsutilize mathematical op-\nerations to reduce the channel dimension. For sim-\nplicity, the sum of values ( FSUM ), the sum of\nabsolute values ( FABS), and the max of values\n(FMAX ) [48] are introduced here. As illustrated in\nFig. 4 (a), the FABS sums the absolute values of\nthe feature maps along the channel dimension. The\ngenerated attention map puts high weight on loca-\ntions where multiple channels respond with high\nactivations. Similarly, the FMAX takes the maximal\nvalue among all neuron activations.\n2) Learning-based generatorsutilize neural networks\nto learn the relationship between the feature maps\nof the attention map. The 1×1 convolutional layer is\nwidely used for this operation to reduce the channel\ndimension without changing the spatial resolution.\nThe LANet [47] illustrated in Fig. 4 (b) is one of\nthe examples of these generators. It combines two\nconvolutional layers to gradually reduce the chan-\nnel dimension. The sigmoid layer at the end of the\noriginal LANet is discarded here because only the\nrelative relationship matters.\nPooling Method. After getting the attention map, we\nneed a pooling method to drop the less relative patches\nand retain the most relative ones. A common method is\nto multiply the feature maps by the attention map, like in\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\n[36], [47]. However, this method fails to discard the noisy\npatches. Although locations with lower attention weights\nhave smaller feature values, they are still computed in the\nincoming modules, which may bring noise and lead to\nmisfocus.\nThe proposed APP module takes the top-k attention\nactivation positions and selects the corresponding features.\nThe detailed process is illustrated in Fig. 3.\n3.3 Attentive Token Pooling\nThe APP module is efﬁcient in pooling the uninformative\nand noise patches by the attention mechanism, but it lacks\nthe global scope because of the small reception ﬁeld of CNN.\nAs illustrated in Fig. 3, the Transformer encoder enhanced\nby ATP (denoted as ATP Block) is utilized to explore the\nrelationship among selected patches in a global scope. We\nstart with a brief description of the Transformer Encoder.\nTransformer Encoder.The Transformer model is ﬁrstly\nproposed to process natural language processing (NLP)\ntasks and then utilized for image recognition [11]. It contains\na stack of encoder blocks, with each block mainly consisting\nof Multi-head Self-Attention (MSA), Multi-Layer Perception\n(MLP), and Layer Normalization (LN). The MSA module\ntakes a series of tokens as input, and linearly transforms\nthem into three parts, denoted as queries ( Q ∈R(n+1)×d),\nkeys (K ∈R(n+1)×d), and values ( V ∈R(n+1)×d), where n\nis the sequence length of the tokens, and d is the embedding\ndimensions. The [class] token is concatenated to the\ninput tokens, so the total length of the input is n + 1. Every\ntoken is updated based on the correlation with each other.\nIt can be considered a weighted sum calculated by the dot\nproduct of Q and K. This process can be formulated as:\nAttention(Q, K, V) =softmax(QKT\n√\nd\n)V (2)\nAfter the Transformer encoder, only the [class] token\nis converted by a single-layer MLP layer to generate the\noutput score.\nAs we can see, the Transformer encoder does not change\nthe length of the input sequence, nor do the MLP or the LN.\nIt is different from the CNN models, where the Max Pooling\nis widely used to reduce the spatial resolution and increase\nthe generalization ability. So, we propose the ATP directly\ndrop the less important ones and only remain a small\nnumber of tokens to embed the information. For a better\nunderstanding, the differences between these methods are\nillustrated in Fig. 2.\nPooling Criterion. Like the APP described above, the\nATP also needs a pooling criterion to guide where to pool.\nHowever, unlike the CNN, the Transformer architecture is\nbuilt based on the attention mechanism. The Transformer\nmodel is much more intuitive to ﬁnd where to pay more at-\ntention without an additional attention-generating module.\nIn the self-attention module, every token’s query will\ncompute a relation with the key of other tokens by a dot\nproduct, as formulated in Eq. (2). The output of a token is\ncalculated by a weighted sum of the values of input tokens.\nMoreover, the weight is determined by the query and key.\nThe higher the QKT value is, the more relevant between\nthese two tokens. As described above, the [class] token\nis utilized to represent the whole image. The more relative\nbetween some patch tokens and the [class] token, the\nmore important these patch tokens are to recognize the\nexpression. So we take the QKT of the [class] token as\nthe pooling criterion to remove the less relative tokens. As\nthere are multiple heads, the sum of all heads is utilized.\nPooling Method.The pooling method in ATP is similar\nto APP . We only consider the top-k tokens and remove the\nrest based on the pooling criterion. It is worth noting that\nthe pooling only acts on patch tokens, and the [class]\ntoken is always reserved. Because there are several stacked\nattention blocks in the Transformer encoder, we propose a\nkeep rate r to adjust the pooling process, which is deﬁned\nas follows:\nkeep num = ⌊r ∗input token number⌋ (3)\nwhere ⌊x⌋denotes the ﬂoor operation. As discovered in\nDeepViT, the attention collapse issue only happens in deep\nblocks, so the ATP is only applied to the second half of\nTransforme blocks. We setr in these blocks to the same value\nfor convenience. Speciﬁcally, denote there areM blocks. The\noutput token number will be about k ·rM/2, ignoring the\ninﬂuence of the [class] token.\n4 E XPERIMENTS\n4.1 Datasets\nWe evaluate our proposed method on six in-the-wild\nFER datasets: RAF-DB [4], FERPlus [49], AffectNet [3],\nSFEW [50], ExpW [51], and Aff-Wild2 [52].\nRAF-DB contains 29,672 real-world facial images col-\nlected by searching on Flickr and was labelled by 40 trained\nhuman workers. In our experiments, the basic annotation\nimages are utilized, resulting in 12,271 images for training\nand 3,068 images for testing, with six basic expressions and\nneutral.\nFERPlus is extended from FER2013 [53], providing more\naccurate annotations relabeled by ten workers to eight ex-\npression categories (six basic expressions, neutral, and con-\ntempt). It contains 28,709 training images, 3,589 validation\nimages, and 3,589 test images.\nAffectNet contains about 1M facial images collected\nby three major search engines, where about 420K images\nwere manually annotated. Following the settings in [54], we\nonly used about 280K training images and 3,500 validation\nimages (500 images per category) with seven emotion cate-\ngories for a fair comparison.\nSFEW 2.0 was created by extracting static frames from\nthe Acted Facial Expressions in the Wild (AFEW) [55],\na temporal facial expressions database. It was used as a\nbenchmark in the third Emotion Recognition in the Wild\nChallenge (EmotiW2015), with 958 facial images for training\nand 436 372 images for validation and testing. Since the\nlabel on the test set is not public, the performance on the\nvalidation set is reported.\nExpW was collected by searching combined emotion-\nrelated keywords (such as ”excited”, ”panic”) and occupa-\ntion nouns (such as ”student”, ”layer”) on the Google image\nsearch. Facial images are ﬁltered by a facial detector and\nlabelled into six basic expressions and neutral. There is no\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nTABLE 1\nEvaluation (%) of APP , ATP ,[class] token (CLT), global average\npooling (GAP) on RAF-DB and AffectNet, and corresponding FLOPs\ncompared with the baseline.\nAPP ATP Head RAF-DB AffectNet FLOPs\nGAP 90.77 65.66 100%\nCLT 91.26 65.91 100%\n✓ CLT 91.52 66.65 94%\n✓ GAP 90.91 66.34 88%\n✓ CLT 91.65 66.51 88%\n✓ ✓ CLT 91.98 66.94 83%\nTABLE 2\nEvaluation (%) of APP and ATP on Aff-Wild2 and corresponding FLOPs\ncompared with the baseline.\nAPP ATP Aff-Wild2 FLOPs\n85.02 100%\n✓ 85.70 81%\n✓ 85.39 61%\n✓ ✓ 86.03 54%\nofﬁcial training set split approach. For a fair comparison, we\nfollow [56] to split the dataset into 75% for training, 10% for\nvalidation, and 15% for testing with random sampling. As a\nresult, there are 68,845, 9,179, and 13,769 images for training,\nvalidation, and testing.\nAff-Wild2 [52], [57] is a recently proposed, large-scale\nfacial expression recognition database. It is an extension of\nthe Aff-Wild database [58] and by far is the largest public\nFER dataset in the wild. It contains 564 videos of around\n2.8M frames and labels them with three representations,\nincluding Categorical Emotions (CE), Action Units (AU),\nand Valence Arousal (VA). Following [59], we adopt the\ntraining set of both CE and AU tasks for training and\nthe validation set of the CE task for testing. For a fair\ncomparison, we take the same head design of [59], but with\na different loss weight. Speciﬁcally, we sum the expression\nclassiﬁcation loss and action unit loss with weights 10 and 1\nto strengthen the classiﬁcation performance.\n4.2 Implementation Details\nAll images were aligned and cropped to 112 ×112 pixels\nby MTCNN [23] before being input into the model. The\nﬁrst three stages of IR50 [60], pre-trained on Ms-Celeb-1M\n1 [61] are used as the feature extractor. The ViT-small [11]\nmodel with 8 Transformer blocks pre-trained on ImageNet 2\nis used as Transformer Encoders. The keep number ( k) of\nthe APP is set to 160 for RAF-DB, AffectNet, and FERPlus;\nand 120 for SFEW. The keep rate ( r) of ATP is set to 0.9 for\nthese four databases. It is worth noting that, ExpW and Aff-\nWild2 achieve the best performance with only k = 80and\nr = 0.6, indicating that our APViT works more efﬁciently\non large-scale datasets. It is worth noting that there are 105\ntokens reserved before the classiﬁcation head when k = 160\nand r = 0.9, and only 10 tokens reserved when k = 80\n1. The pre-trained weight is downloaded from https://github.com/\nZhaoJ9014/face.evoLVe.PyTorch#Model-Zoo.\n2. The pre-trained weight is downloaded from https://github.com/\nrwightman/pytorch-image-models/.\nFig. 5. Evaluation (%) of different keep numbers (k) and keep rates (r) on\nAffectNet. As the keep number and keep rate decade, the performance\nﬁrst increases and then decreases. From the top left to the bottom\nright, our AP modules reduce more than half of the computations,\nand the performance is robust without hard decreases, indicating the\neffectiveness of our AP modules.\nand r = 0.6. Our APViT maintains the performance with\nabout half computation costs. Our model was trained with\nthe SGD optimizer to minimize the cross-entropy loss. The\nmomentum was set to 0.9, and the weight decay was set to\n5e-4. The mini-batch size was set to 128, and the learning\nrate was set to 1e-3 for RAF-DB, FERPlus, SFEW, Aff-Wild2,\nand 5e-4 for AffectNet and ExpW with cosine decay [62]. We\nalso found that gradient clipping of 10 based on the L2-norm\nis helpful, as described in [11]. During the training process,\nwe follow the same augmentation methods as TransFER\n[10]. At the test time, only the resize operation is reserved.\nWe train our model with one NVIDIA V100 GPU, 20 epochs\nfor FERPlus, 40 epochs for RAF-DB and SFEW, 6k iterations\nfor AffectNet, 9k iterations for ExpW, and 150k iterations for\nAff-Wild2. The overall accuracy is reported by default.\n4.3 Ablation Studies\nEffectiveness of the Proposed Modules. To evaluate the\neffect of the proposed modules, we designed the ablation\nstudy on RAF-DB and AffectNet to better understand the\nimpact of the proposed APP and ATP . Our two pooling\nmodules are designed to resolve the over-ﬁtting problem\nwhen transferring the pre-trained Transformer model to\nsmall datasets and restrain the impact of noisy patches.\nAs is shown in Tab. 1, both modules increase the per-\nformance in FER compared with the baseline strategy. The\nAPP and ATP contribute similarly when applied solely.\nThe APP improves the performance by 0.39% and 0.60%\nin RAF-DB and AffectNet, respectively. The ATP brings a\nsimilar performance gain, 0.26%, and 0.74%, respectively.\nHowever, the APP takes effect more early, thus decreasing\nmore FLOPs than ATP . The best improvement is obtained\nwhen combining two pooling methods. Speciﬁcally, the\nbaseline is improved from 91.26% to 91.98% on RAF-DB and\nfrom 65.91% to 66.94% on AffectNet with only 83% FLOPs 3.\nWe also compared the [class] token (CLT) with global\n3. The number of FLOPs is calculated using the fvcore toolkit pro-\nvided by Facebook AI Research\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nTABLE 3\nPerformances with or without pre-trained weights.\nPre-trained CNN Pre-trained ViT RAF-DB AffectNet\n81.55 53.88\n✓ 90.84 65.34\n✓ 78.91 51.56\n✓ ✓ 91.98 66.91\nTABLE 4\nPerformances (%) with gradually decayed and constant k and r.\nDatabase Gradually decay Constant\nRAF-DB 91.46 91.98\nFERPlus 90.19 90.86\nAffectNet 66.08 66.91\naverage pooling (GAP) with different strategies. Although\nGAP outperforms CLT in CVPT [18], it fails to achieve a\nbetter performance here. We suspect different pre-trained\nstrategies cause this. The model should change as little as its\npre-trained for downstream ﬁnetuning. These experiments\nillustrate that the APP and ATP could help the model to\nonly focus on more discriminative patches and reduce the\nover-ﬁtting problem caused by the noisy patches.\nImpact of the Keep Numberk in APP and Keep Rate\nr in ATP.The keep number ( k) and keep rate ( r) are two\nhyper-parameters of our proposed model to control how\nmany patches to keep. To explore the impact of these two\nparameters, we study different values from 196 to 40 for\nk, and 1 to 0.6 for r on AffectNet. When k = 196 and\nr = 1, it represents the baseline model with no pooling\ntaking effect. The results are shown in Fig. 5, the best\nperformance (66.94%) could be obtained when k = 160and\nr = 0.9. The smaller k and r are, the fewer patches are kept\nfor recognition, which forces the model to focus on more\ndiscriminative patches and take less computation. Too less\ninformation, however, may make it harder to recognize, so\nthe performance decreases when k and r are too small. It is\nworth noting that the minimal model ( k = 40and r = 0.6)\nhere only has ﬁve tokens reserved but still outperforms the\nbaseline model. Speciﬁcally, it boosts the performance from\n65.91% to 66.48%, but with only 45.12% FLOPs compared to\nthe baseline model.\nEvaluation of Pre-trained Weights.To evaluate the ef-\nfects of pre-trained weights on CNN and ViT, we conduct\nexperiments on RAF-DB and AffectNet. As illustrated in\nTab. 3, the pre-trained weight is more critical for CNN,\nwhich boosts the performance from 81.85% to 90.84% on\nRAF-DB. This indicates that CNN pre-trained on facial\nrecognition databases can extract helpful features for FER.\nIn addition, pre-training on ViT has a negative inﬂuence\nwhen no CNN pre-trained weight is loaded but could help\nthe model to achieve better performance with pre-trained\nCNN.\nComparison of Gradually Decayk and r. As AP mod-\nules rely on accurate attention maps, however training from\nscratch cannot get accurate attention maps right from the\nstart. Intuitively, not to drop patches in the early stage of\ntraining, gradually increasing the proportion of dropped\nTABLE 5\nEvaluation (%) of different pooling methods on RAF-DB, the method\naction on APP and ATP are denoted as APP method and ATP method,\nrespectively.\nAPP Method ATP Method RAF AffectNet\nLANet\nSUM\n91.23 66.28\nMAX 91.36 66.51\nSUM 91.72 66.66\nABS 91.98 66.94\nABS ABS 91.33 66.34\nMAX 91.39 66.54\nTABLE 6\nPerformances with “soft” and “hard” pooling approaches.\nDatabase Soft Hard (Ours)\nRAF-DB 91.07 91.98\nFERPlus 90.55 90.86\nAffectNet 65.80 66.91\npatches as the network learns is a more appropriate training\nstrategy. To evaluate the impact of this strategy with the\nconstant one, we designed experiments with gradual de-\ncay of the k and r on RAF-DB, FERPlus, and AffectNet.\nSpeciﬁcally, the k and r are initially set to 196 and 1.0\nat the beginning and linearly decayed to the database’s\ncorresponding values. Surprisingly, as we can see from\nTab. 4, the performances with gradually decayedk and r are\nslightly inferior to the constant ones. This may be because\ninaccurate attention maps perform like random drop out at\nthe beginning of the training period, pushing the model to\nﬁnd the informative features and preventing the model from\nover-ﬁtting to noisy backgrounds. However, the gradual\ndecay strategy lacks this beneﬁt.\nComparison of Different Attention Generate Methods.\nAs discussed in the Method section, there are several differ-\nent methods to generate attention maps. We run an ablation\nstudy to specify the different impacts of these methods on\nRAF-DB and AffectNet. As illustrated in Tab. 5, whenFSUM\nis adopted as the method in ATP ,FLANet (adopted by Trans-\nFER [10]) performs the worst (91.23% and 66.28%) and even\nworse than the baseline (91.26% ) on RAF-DB. This may be\nbecause the LANet needs to multiply the attention map with\nthe features to backward the gradient. The multiplication\noperation changes the magnitude of features and makes\nthe downstream network harder to adapt. Hand-designed\nmethods do not have this problem and achieve better per-\nformance. The best performance (91.98% and 66.94%) is\nachieved by FABS, which indicates that the negative values\nin CNN features also contain rich semantic information .\nWe also compare three hand-designed methods in ATP .\nThe FABS here does not achieve comparable performance as\nin APP did and is much lower than FSUM . It is because the\ndot product attention mechanism has a strong mathematical\nmeaning: small values (including negative ones) indicate\na poor relationship. While taking the absolute value as\nattention value breaks this mechanism. The FMAX takes\nthe max value among multiple heads, but certain heads\nmay dominate the result, which reduces the discriminative\nability.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\n(a) Confusion Matrix of APViT on RAF-DB (b) Confusion Matrix of AP ViT on AffectNet\n(e) Confusion Matrix of APViT on ExpW(d) Confusion Matrix of APViT on SFEW\n (f) Confusion Matrix of APViT on Aff-Wild2\n(c) Confusion Matrix of APViT on FERPlus\nFig. 6. Confusion matrices of our APViT model on RAF-DB, AffectNet, FERPlus, ExpW, SFEW, and Aff-Wild2 databases.\nTABLE 7\nPerformance comparison (%) with SOTA methods on RAF-DB and\nAffectNet. “*” indicates Transformer-based models.\nMethod RAF-DB AffectNet\nDLP-CNN [4] 80.89 54.47\ngACNN [6] 85.07 58.78\nIPA2LT [7] 86.77 55.71\nLDL-ALSG [63] 85.53 59.35\nRAN [8] 86.90 59.50\nCovPool [64] 87.00 -\nSCN [65] 87.03 60.23\nDLN [66] 86.40 63.70\nDACL [67] 87.78 65.20\nKTN [54] 88.07 63.97\nEfﬁcientFace [68] 88.36 59.89\nPAT-Res101 + attr [69] 88.43 -\nFDRL [70] 89.47 -\nPT [71] 89.57 58.54\nVTFF* [12] 88.14 61.85\nMViT* [13] 88.62 64.57\nTransFER* [10] 90.91 66.23\nAPViT* (Ours) 91.98 66.91\nComparison of ”Soft” Pooling with Our ”Hard” Pool-\ning. In most deep-learning-based models, like the LANet\nin the TransFER model, multiplying the mask with the\nfeatures (denoted as ”soft” pooling) is the only choice to\npropagate gradients to the mask-generating branch. We\novercome this restriction by utilizing the hand-designed\nattention generator and found that its performance is good\nenough and could reduce the computation. To compare\nthese two approaches, we conduct experiments on three\ndatabases, as illustrated in Tab. 6. As we can see, the ”soft”\npooling approach performs much worse than the ”hard”\none, especially for the noisy database (AffectNet). The con-\nclusion is the same as the LANet, which has been illustrated\nin Tab. 5. This indicates that our proposed ”hard” pooling is\nmore efﬁcient in helping the model to ﬁnd the informative\nfeatures.\n4.4 Comparison with the State-of-the-Art Methods\nIn this section, we compare our proposed method with\nseveral state-of-the-art (SOTA) methods on six major in-the-\nwild datasets in Tab. 7 - 12. We also illustrate the confusion\nmatrices of these databases on Fig. 6.\nResults on RAF-DB. The comparison with other pro-\ngressive methods on RAF-DB is illustrated in Tab. 7.\nTFE [74] adopts joint learning with identity and emotion\ninformation. DLP-CNN [4], DACL [67], and KTN [54] utilize\nnew proposed loss functions to enhance the discriminative\npower of deep features; IPA2LT [7], LDL-ALSG [63], and\nSCN [65] tend to increase the performance by solving the\nuncertain problem in the FER task. VTFF [12], MViT [13] and\nTransFER [10] use Transformer to obtain global-scoop atten-\ntions. Among that, TransFER obtained a big performance\ngain with 90.91%. Based on their architecture, our proposed\nAPViT further improves the recognition performance with\nthe attentive pooling modules by restraining the noisy in-\nﬂuence. We also ﬁnd that realigning the facial images in\nthe ofﬁcial RAF-DB database could further improve the\nperformance. Our model outperforms the state-of-the-art\nmodels with 91.98%.\nThe per-class results and the average accuracies on RAF-\nDB are illustrated in Tab. 8. As we can see, our APViT\noutperforms other methods in most classes. Happiness has\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nTABLE 8\nPer-class performance comparison (%) with the state-of-the-art methods on RAF-DB. “*” indicates Transformer-based models.\nMethod Anger Disgust Fear Sadness Happiness Surprise Neutral Average\nVGG [54] 53.09 33.75 14.87 77.41 92.41 67.48 82.35 60.19\nResNet [54] 62.58 18.13 22.97 59.21 90.04 72.04 82.50 58.35\nCL-CNN [72] 71.61 14.38 9.46 83.26 94.09 86.63 83.68 63.30\nDLP-CNN [4] 71.60 52.15 62.16 80.13 92.83 81.16 80.29 74.20\nIL-CNN [5] 70.99 45.63 32.43 76.78 91.48 78.42 75.74 67.35\nBoosting-POOF [73] 73 57 64 74 89 80 76 73.19\nSTSN [54] 82.72 66.25 59.46 87.87 94.35 86.63 85.00 80.32\nKTN [54] 81.48 65.62 68.92 87.24 94.60 83.28 88.53 81.38\nPT [71] 81 55 54 87 96 87 92 78.86\nPAT-Res101 + attr [69] 77.16 60.00 67.57 86.61 95.53 89.67 88.38 80.70\nMViT* [13] 78.40 63.75 60.81 87.45 95.61 87.54 89.12 80.38\nVTFF* [12] 85.80 68.12 64.86 87.24 94.09 85.41 87.50 81.20\nTransFER* [10] 88.89 79.37 68.92 88.70 95.95 89.06 90.15 85.86\nAPViT* (Ours) 86.42 73.75 72.97 88.70 97.30 93.31 92.06 86.36\nTABLE 9\nPerformance comparison (%) with SOTA methods on FERPlus. “*”\nindicates Transformer-based models.\nMethod FERPlus\nTFE-joint learning [74] 84.29\nPLD [49] 85.10\nRAN [8] 88.55\nSeNet50 [75] 88.80\nRAN-VGG16 [8] 89.16\nSCN [65] 89.35\nPT [71] 86.60\nKTN [54] 90.49\nVTFF* [12] 88.81\nMViT* [13] 89.22\nTransFER* [10] 90.83\nAPViT* (Ours) 90.86\nthe highest accuracy with 97.30%, and fear has the lowest\naccuracy with 72.97% (but still outperforms other methods).\nThe RAF-DB database is collected in the wild and is class-\nimbalanced because negative expressions (such as anger,\ndisgust, fear, and sadness) are rare and challenging to collect\nin daily life. As is illustrated in Fig. 6 (a), thanks to the reli-\nable label annotation, our method performs well in negative\nexpressions (the left top part of the confusion matrix). We\nbelieve this is because the attentive pooling module forces\nthe model to ﬁnd and focus on the most class-relative patch\nfeatures.\nResults on AffectNet.Since the testing set of AffectNet\nis not public. We evaluate and compare the performance of\nthe validation set. As the training set of the AffectNet is\nimbalanced and the validation set is balanced, we employ a\nheavy over-sampling strategy as RAN [8], SCN [65], MA-\nNet [37], and TransFER does. For a fair comparison, the\n7-categories results are illustrated in the third column of\nTab. 7. As we can see, we obtain 66.91% FER accuracy,\nwhich outperforms all other state-of-the-art methods. The\nconfusion matrix, illustrated in Fig. 6 (b), demonstrates that\nour model has a similar recognition capacity (57% to 69%)\nin all seven expression categories, while happiness has the\nhighest accuracy with 88%.\nResults on FERPlus. Different from other databases,\nFERPlus has eight expression categories with an additional\nTABLE 10\nPerformance comparison (%) with SOTA methods on SFEW.\nMethod SFEW 2.0 (Val)\nDLP-CNN [4] 51.05\nKim’s CNN [76] 52.5\nLBAN-IL [77] 55.28\nRAN(VGG16+ResNet18) [8] 56.40\nAPM [78] 57.57\nDAN [79] 57.88\nAHA [80] 58.89\nMA-Net [37] 59.40\nPAT-Res101 [69] 53.90\nPAT-Res101 + attr [69] 57.57\nAPViT (Ours) 61.92\ncontempt. As illustrated in Tab. 9, our method performs\nbest with an accuracy of 90.86%. Compared with TransFER,\nwe maintain the performance with 83% computations. The\nconfusion matrix of FERPlus is illustrated in Fig. 6 (c). As\nwe can see, contempt performs worst and is hard to dis-\ntinguish from neutral. This is because there are only 0.47%\ncontempt facial images versus about 29% neutral images in\nthe training set, making model predictions biased to neutral\nas illustrated in the right bottom of Fig. 6 (c).\nResults on SFEW.We compare our APViT with several\nstate-of-the-art methods on SFEW in Tab. 10. LBAN-IT [77]\nproposed a local binary attention network and an islets loss\nto discover local changes in the face. Since the training\nsamples of SFEW are limited (less than 1,000 images for\ntraining), MA-Net [37] pre-trained the model in FER2013,\nwhile APM [78] pre-trained the model in RAF-DB. With ﬁne-\ntuning, they achieved performance with 59.40% and 57.57%.\nPAT-Res101 [69] extend the training dataset with RAF-DB\nto have 50% RAF-DB samples in every mini-batch. PAT-\nRes101+attr [69] further improved the performance by intro-\nduce attribute-annotated samples to 57.57%. However, we\nonly use the pre-trained weights on the RAF-DB database\nand outperform these methods without additional data with\nan accuracy of 61.92%. As we can see from the confusion\nmatrix, which is illustrated in Fig. 6 (d), the accuracy of\ndisgust and fear are both 9% only. Since the distribution of\nSFEW is extremely imbalanced, there are only 54 disgust\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nTABLE 11\nPerformance comparison (%) with SOTA methods on ExpW.\nMethod ExpW\nHOG + SVM [51] 60.66\nBaseline DCN [51] 65.06\nDCN + AP [51] 70.06\nLian et al. [81] 71.90\nPham et al. [82] 72.84\nPAT-Res101 [69] 69.29\nPAT-Res101 + attr [69] 72.93\nSchiNet [56] 73.10\nAPViT (Ours) 73.48\nTABLE 12\nPerformance comparison (%) with SOTA methods on Aff-Wild2.\nMethod Aff-Wild2 (val) Rank\nZhang et al. [83] 85.6 1\nJin et al. [84] 63.0 2\nThinh et al. [59] 82.6 3\nWang et al. [85] 50.1 4\nDeng, Wu, and Shi [86] 57.79 5\nAPViT (Ours) 86.03 -\nsamples and 81 fear samples in the training set. The poor\nperformance in these two categories is reasonable and ac-\nceptable.\nResults on ExpW. Same as SFEW, due to the severe\nimbalanced problem, the performance in disgust and fear\nare poor, and predictions are biased to neutral as illustrated\nin Tab. 6 (e). The comparison of our APViT with other\nstate-of-the-art methods on ExpW is illustrated in Tab. 11.\nLian et al. [81] divided the face into six areas to explore the\nauthenticity of <emotion, region >pairs. They only chose\nthe facial images with conﬁdent scores greater than 60 in\nthe database and achieved 71.90% accuracy. Pham et al. [82]\nused action unit features to ﬁnd the images with a similar\nemotion for oversampling. SchiNet [56] combined ExpW\nwith other databases (CEW [87], CelebA [88], and Emotion-\nNet [89]) and achieved a promising accuracy, 73.10%. With\nonly training on the target database, our APViT outperforms\nthese methods with 73.48% accuracy. Moreover, beneﬁting\nfrom the large-scale database, our APViT achieves the best\nperformance with only about half of (54.07%) computations.\nResults on Aff-Wild2.We compare our proposed APViT\nwith other state-of-the-art methods in the second ABAW2\nCompetition [52], as illustrated in Tab. 12. As the test set\nis private, we only report the classiﬁcation accuracy on\nthe validation set. The rank column indicates their leader\nboard rank on the test set. [85], [86], [90] utilized all three\nannotations, including CE, AU, and VA, achieving 85.6%,\n50.1%, and 57.79% on the validation set, respectively. [59]\nutilized only CE and AU and achieved a promising perfor-\nmance of 82.6%. [91] took the audio information as another\nmodal and achieved 63.0%. Following [59], we only take the\nCE and AU as input. With the help of our proposed two\nattentive pooling modules to reduce the inﬂuence of the\nnoisy features, our APViT outperforms these state-of-the-\nart methods with an accuracy of 86.03% on the validation\nset. And as illustrated in Fig. 6 (f), the accuracy of disgust\nand fear is higher than that of SFEW and ExpW due to more\ntraining samples in these categories.\nTABLE 13\nComparison of accuracy and computation cost against state-of-the-art\nmethods on RAF-DB. (160, 0.9) indicates k = 160, r = 0.9 in APViT.\nMethod GFLOPs RAF-DB\nTransFER [10] 15.30 90.91\nAPViT (160, 0.9) 12.67 91.98\nPAT-CNN [69] 7.64 88.43\nKTN [54] 7.60 88.07\nVTFF [12] 6.08 88.14\nMViT [13] 5.95 88.62\nAPViT (10, 0.6) 5.89 90.67\nTABLE 14\nPerformance comparison (%) on lighter Transformer models on\nRAF-DB.\nModel Input GFLOPs RAF-DB\nHybrid-DeiT-Tiny 112 2 6.72 90.91\nHybrid-DeiT-Tiny + AP 112 2 6.10 91.26\nDeiT-Tiny 224 2 1.26 85.72\nDeiT-Tiny + AP 224 2 0.96 86.70\n4.5 Further Analysis\nAccuracy and computation comparison with state-of-the-\nart methods. To better demonstrate the effectiveness of\nour proposed attentive pooling modules, we illustrate the\nGFLOPs and accuracy on RAF-DB in Tab. 13. PAT-CNN [69]\nand KTN [54] are CNN-based methods; TransFER [10],\nVTFF [12], and MViT [13] are based on the Transformer.\nOur APViT (160, 0.9) achieves the best performance (91.98%)\nwith 82.81% FlOPs compared with TransFER. And when we\npool more patches and tokens to keep comparable FLOPs\nwith other methods, our APViT (10, 0.6) outperforms them\nwith 2.05% higher accuracy. This indicate that our attentive\npooling module drops background and noises and forces\nthe model to only focus on discriminative information.\nExperiments on Lighter Transformer models.Tab. 14\nshows the results on RAF-DB by exploring our two AP\nmodules to two versions of lighter transformer models. The\nHybrid-DeiT-Tiny model replaces the ViT-Small in APViT\nwith DeiT-Tiny [43]. Our AP reports 0.35% and 0.98% per-\nformance improvements and 9.18%, 23.68% computation\nreduction on Hybrid-DeiT-Tiny and DeiT-Tiny, respectively.\nThe k and r for Hybrid-DeiT are 120 and 0.9, respectively;\nand the r for DeiT-Tiny is 0.8. Without the APP module, the\nATP in DeiT-Tiny needs to pool more noisy features to get\nthe best performance.\nExperiments on CIFAR-10. Most FER methods might\nnot be suitable for general image classiﬁcation tasks. This is\nmainly because face images are cropped and aligned before\ninput into the FER model, making faces’ position ﬁxed\nand uniﬁed. While for general image classiﬁcation tasks,\nforeground objects may occur at any place and scale. To\ninvestigate the generalization ability of our APViT model,\nwe conduct experiments on CIFAR-10 [92]. As shown in\nTab. 15, our APViT maintains the performance with only\n54.07% computation costs and could slightly increase the\nperformance from 96.02% to 96.23%, indicating the good\ngeneralization of the proposed method.\nAttention Visualization. In Fig. 7, we visualize the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nTABLE 15\nPerformance and GFLOPs comparison (%) on CIFAR-10.\nk r CIFAR-10 FLOPs\nbaseline 96.02 100%\n120 0.9 96.23 70.85%\n80 0.6 96.11 54.07%\ninputk=160 w/o APw APk=120k=80\nFig. 7. Visualization of attention maps in complex conditions. The sec-\nond to fourth columns demonstrate attention maps in APP with different\nkeep numbers of 160, 120, and 80, respectively. The last two columns\nare the attention maps in ATP without and with our proposed two AP\nmodules, indicating the focused areas by Transformer.\nattention map generated by our APViT on RAF-DB. The\nﬁrst column is the input images, and the second to fourth\ncolumns are reserved patches with our APP with different\nkeep numbers. The pooled patches are erased to white\nbackground. The last two columns are the attention maps\nextracted by the method [93] to demonstrate the focus area\nof the model without and with the proposed AP modules.\nThe selected images have complex conditions with occlu-\nsion and pose variation, varying with race and age. We can\nﬁnd that these attention maps are robust and accurate, and\nthe pooled areas are noisy for the FER task. Thus 80 patches\nare enough to represent the facial expression. Compared\nwith the baseline (w/o AP), AP modules could guide the\nmodel to pool the occluded and background areas and only\nfocus on the distinguished ones, e.g. the hand in the ﬁrst\nrow and the chin in the second row. These visualizations\nalso indicate that our approach is meaningful theoretically.\n5 C ONCLUSION\nIn this paper, we have proposed two Attentive Pooling (AP)\nmodules with Vision Transformer (APViT) to utilize the\npre-trained Transformer model for the limited size of the\nFacial Expression Recognition dataset. The proposed APViT\ncould focus on the most discriminative features and discard\nthe less relevant ones. This could prevent the model from\nfocusing on occlusion or other noisy areas. Experiments\non six major in-the-wild FER datasets demonstrated that\nour APViT model outperforms the state-of-the-art methods.\nVisualization showed the intuition and robustness of our\nattentive poolings. We believe that decreasing the number of\ntokens is a promising direction to pursue. We hope our work\ncan further inspire researchers to explore this paradigm with\nmore effective methods.\nREFERENCES\n[1] C. Darwin and P . Prodger, The expression of the emotions in man and\nanimals. Oxford University Press, USA, 1998.\n[2] C. Shan, S. Gong, and P . W. McOwan, “Facial expression recog-\nnition based on local binary patterns: A comprehensive study,”\nImage and vision Computing, vol. 27, no. 6, pp. 803–816, 2009.\n[3] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A\ndatabase for facial expression, valence, and arousal computing in\nthe wild,” IEEE T-AC, vol. 10, no. 1, pp. 18–31, 2017.\n[4] S. Li, W. Deng, and J. Du, “Reliable Crowdsourcing and Deep\nLocality-Preserving Learning for Expression Recognition in the\nWild,” in CVPR, vol. 28. IEEE, Jul. 2017, pp. 2584–2593.\n[5] J. Cai, Z. Meng, A. S. Khan, Z. Li, J. O’Reilly, and Y. Tong, “Island\nLoss for Learning Discriminative Features in Facial Expression\nRecognition,” in 2018 13th IEEE International Conference on Auto-\nmatic Face Gesture Recognition (FG 2018) , May 2018, pp. 302–309.\n[6] Y. Li, J. Zeng, S. Shan, and X. Chen, “Occlusion Aware Facial\nExpression Recognition Using CNN With Attention Mechanism,”\nIEEE T-IP, vol. 28, no. 5, pp. 2439–2450, 2019.\n[7] J. Zeng, S. Shan, and X. Chen, “Facial expression recognition with\ninconsistently annotated datasets,” in ECCV, 2018, pp. 222–237.\n[8] K. Wang, X. Peng, J. Yang, D. Meng, and Y. Qiao, “Region attention\nnetworks for pose and occlusion robust facial expression recogni-\ntion,” IEEE T-IP, vol. 29, pp. 4057–4069, 2020.\n[9] Y. Fan, V . Li, and J. C. Lam, “Facial Expression Recognition with\nDeeply-Supervised Attention Network,” IEEE T-AC, pp. 1–1, 2020.\n[10] F. Xue, Q. Wang, and G. Guo, “TransFER: Learning Relation-aware\nFacial Expression Representations with Transformers,” in ICCV,\nMar. 2021.\n[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly\net al. , “An image is worth 16x16 words: Transformers for image\nrecognition at scale,” arXiv preprint arXiv:2010.11929, 2020.\n[12] F. Ma, B. Sun, and S. Li, “Facial expression recognition with visual\ntransformers and attentional selective fusion,” IEEE T-AC, pp. 1–1,\n2021.\n[13] H. Li, M. Sui, F. Zhao, Z. Zha, and F. Wu, “MViT: Mask Vision\nTransformer for Facial Expression Recognition in the wild,” arXiv\npreprint arXiv:2106.04520, Jun. 2021.\n[14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Ima-\ngeNet: A large-scale hierarchical image database,” in CVPR, Jun.\n2009, pp. 248–255.\n[15] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: a simple way to prevent neural\nnetworks from overﬁtting,” The journal of machine learning research,\nvol. 15, no. 1, pp. 1929–1958, 2014.\n[16] G. Ghiasi, T.-Y. Lin, and Q. V . Le, “Dropblock: A regularization\nmethod for convolutional networks,” in NeurIPS, 2018.\n[17] Z. Tan, Y. Yang, J. Wan, H. Hang, G. Guo, and S. Z. Li, “Attention-\nbased pedestrian attribute analysis,” T-IP, vol. 28, no. 12, pp. 6126–\n6140, 2019.\n[18] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, and C. Shen,\n“Conditional Positional Encodings for Vision Transformers,”arXiv\npreprint arXiv:2102.10882, feb 2021.\n[19] D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian, Z. Jiang, Q. Hou, and\nJ. Feng, “DeepViT: Towards Deeper Vision Transformer,” arXiv\npreprint arXiv:2103.11886, mar 2021.\n[20] Z. Jiang, Q. Hou, L. Yuan, D. Zhou, Y. Shi, X. Jin, A. Wang, and\nJ. Feng, “All Tokens Matter: Token Labeling for Training Better\nVision Transformers,” in NeurIPS, 2021.\n[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in CVPR, 2016, pp. 770–778.\n[22] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient\nconvolutional neural networks for mobile vision applications,”\narXiv preprint arXiv:1704.04861, 2017.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\n[23] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection\nand alignment using multitask cascaded convolutional networks,”\nIEEE Signal Processing Letters, vol. 23, no. 10, pp. 1499–1503, 2016.\n[24] J. Deng, J. Guo, E. Ververas, I. Kotsia, and S. Zafeiriou, “Reti-\nnaFace: Single-Shot Multi-Level Face Localisation in the Wild,”\nin CVPR, 2020, pp. 5203–5212.\n[25] C. Liu and H. Wechsler, “Gabor feature based classiﬁcation using\nthe enhanced ﬁsher linear discriminant model for face recogni-\ntion,” IEEE T-IP, vol. 11, no. 4, pp. 467–476, 2002.\n[26] N. Dalal and B. Triggs, “Histograms of oriented gradients for\nhuman detection,” in CVPR, vol. 1. IEEE, 2005, pp. 886–893.\n[27] P . C. Ng and S. Henikoff, “Sift: Predicting amino acid changes that\naffect protein function,” Nucleic acids research , vol. 31, no. 13, pp.\n3812–3814, 2003.\n[28] S. Happy and A. Routray, “Automatic facial expression recogni-\ntion using features of salient facial patches,” IEEE T-AC, vol. 6,\nno. 1, pp. 1–12, 2014.\n[29] D. Ghimire and J. Lee, “Geometric feature-based facial expression\nrecognition in image sequences using multi-class adaboost and\nsupport vector machines,” Sensors, vol. 13, no. 6, pp. 7714–7734,\n2013.\n[30] A. Saeed, A. Al-Hamadi, R. Niese, and M. Elzobi, “Frame-based\nfacial expression recognition using geometrical features,”Advances\nin human-computer interaction, vol. 2014, 2014.\n[31] A. Katharopoulos and F. Fleuret, “Processing megapixel images\nwith deep attention-sampling models,” in International Conference\non Machine Learning. PMLR, 2019, pp. 3282–3291.\n[32] J.-B. Cordonnier, A. Mahendran, A. Dosovitskiy, D. Weissenborn,\nJ. Uszkoreit, and T. Unterthiner, “Differentiable patch selection for\nimage recognition,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2021, pp. 2351–2360.\n[33] J. Wang, X. Yang, H. Li, Z. Wu, and Y.-G. Jiang, “Efﬁcient video\ntransformers with spatial-temporal token selection,” arXiv preprint\narXiv:2111.11591, 2021.\n[34] G. Duchenne de Bologne, “Mechanisme de la physionomie hu-\nmaine,” Jules Renouard Libraire. Paris, 1862.\n[35] L. Zhong, Q. Liu, P . Yang, B. Liu, J. Huang, and D. N. Metaxas,\n“Learning active facial patches for expression analysis,” in CVPR,\n2012, pp. 2562–2569.\n[36] S. Xie, H. Hu, and Y. Wu, “Deep multi-path convolutional neural\nnetwork joint with salient region attention for facial expression\nrecognition,” Pattern Recognition, vol. 92, pp. 177–191, 2019.\n[37] Z. Zhao, Q. Liu, and S. Wang, “Learning Deep Global Multi-scale\nand Local Attention Features for Facial Expression Recognition in\nthe Wild,” IEEE T-IP, pp. 1–1, 2021.\n[38] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional\nblock attention module,” in ECCV, 2018, pp. 3–19.\n[39] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\narXiv preprint arXiv:1706.03762, 2017.\n[40] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-End Object Detection with Transformers,”\nin ECCV. Springer, May 2020.\n[41] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr:\nDeformable transformers for end-to-end object detection,” arXiv\npreprint arXiv:2010.04159, 2020.\n[42] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, “Revisiting Un-\nreasonable Effectiveness of Data in Deep Learning Era,” in ICCV,\n2017, pp. 843–852.\n[43] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ´egou, “Training data-efﬁcient image transformers & distilla-\ntion through attention,” arXiv preprint arXiv:2012.12877, 2020.\n[44] S. D’Ascoli, H. Touvron, M. L. Leavitt, A. S. Morcos, G. Biroli,\nand L. Sagun, “ConViT: Improving Vision Transformers with Soft\nConvolutional Inductive Biases,” inICML. PMLR, 2021, pp. 2286–\n2296.\n[45] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\n“Swin Transformer: Hierarchical Vision Transformer using Shifted\nWindows,” in ICCV.\n[46] M. Aouayeb, W. Hamidouche, C. Soladie, K. Kpalma, and\nR. Seguier, “Learning Vision Transformer with Squeeze and Ex-\ncitation for Facial Expression Recognition,” arXiv:2107.03107 [cs],\nJul. 2021.\n[47] Q. Wang, T. Wu, H. Zheng, and G. Guo, “Hierarchical pyramid\ndiverse attention networks for face recognition,” CVPR, pp. 8323–\n8332, 2020.\n[48] S. Zagoruyko and N. Komodakis, “Paying more attention\nto attention: Improving the performance of convolutional\nneural networks via attention transfer,” in ICLR. International\nConference on Learning Representations, ICLR, dec 2017. [Online].\nAvailable: https://github.com/szagoruyko/attention-transfer.\n[49] E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang, “Training deep\nnetworks for facial expression recognition with crowd-sourced\nlabel distribution,” in Proceedings of the 18th ACM International\nConference on Multimodal Interaction, 2016, pp. 279–283.\n[50] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon, “Static facial expres-\nsion analysis in tough conditions: Data, evaluation protocol and\nbenchmark,” ICCV, pp. 2106–2112.\n[51] Z. Zhang, P . Luo, C. C. Loy, and X. Tang, “From Facial Expression\nRecognition to Interpersonal Relation Prediction,” IJCV, vol. 126,\nno. 5, pp. 550–569.\n[52] D. Kollias, I. Kotsia, E. Hajiyev, and S. Zafeiriou, “Analysing af-\nfective behavior in the second abaw2 competition,” arXiv preprint\narXiv:2106.15318, 2021.\n[53] I. J. Goodfellow, D. Erhan, P . L. Carrier, A. Courville, M. Mirza,\nB. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee et al. ,\n“Challenges in representation learning: A report on three machine\nlearning contests,” in International conference on neural information\nprocessing. Springer, 2013, pp. 117–124.\n[54] H. Li, N. Wang, X. Ding, X. Yang, and X. Gao, “Adaptively learning\nfacial expression representation via cf labels and distillation,”IEEE\nT-IP, vol. 30, pp. 2016–2028, 2021.\n[55] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon, “Acted facial\nexpressions in the wild database,” Australian National University,\nCanberra, Australia, Technical Report TR-CS-11, vol. 2, p. 1, 2011.\n[56] M. Bishay, P . Palasek, S. Priebe, and I. Patras, “SchiNet: Automatic\nEstimation of Symptoms of Schizophrenia from Facial Behaviour\nAnalysis,” IEEE T-AC, vol. 12, no. 4, pp. 949–961.\n[57] D. Kollias and S. Zafeiriou, “Expression, affect, action unit recog-\nnition: Aff-wild2, multi-task learning and arcface,” arXiv preprint\narXiv:1910.04855, 2019.\n[58] S. Zafeiriou, D. Kollias, M. A. Nicolaou, A. Papaioannou,\nG. Zhao, and I. Kotsia, “Aff-wild: Valence and arousal ‘in-the-\nwild’challenge,” inCVPR Workshops. IEEE, 2017, pp. 1980–1987.\n[59] P . T. D. Thinh, H. M. Hung, H.-J. Yang, S.-H. Kim, and G.-S. Lee,\n“Emotion Recognition with Incomplete Labels Using Modiﬁed\nMulti-task Learning Technique.”\n[60] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive\nangular margin loss for deep face recognition,” in CVPR, 2019,\npp. 4690–4699.\n[61] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao, “Ms-celeb-1m: A\ndataset and benchmark for large-scale face recognition,” in ECCV.\nSpringer, 2016, pp. 87–102.\n[62] I. Loshchilov and F. Hutter, “SGDR: Stochastic Gradient Descent\nwith Warm Restarts,” ICLR, aug 2016.\n[63] S. Chen, J. Wang, Y. Chen, Z. Shi, X. Geng, and Y. Rui, “Label\nDistribution Learning on Auxiliary Label Space Graphs for Facial\nExpression Recognition,” in CVPR, 2020, pp. 13 984–13 993.\n[64] D. Acharya, Z. Huang, D. Pani Paudel, and L. Van Gool, “Covari-\nance pooling for facial expression recognition,” in CVPR Work-\nshops, 2018, pp. 367–374.\n[65] K. Wang, X. Peng, J. Yang, S. Lu, and Y. Qiao, “Suppressing un-\ncertainties for large-scale facial expression recognition,” in CVPR,\n2020, pp. 6897–6906.\n[66] W. Zhang, X. Ji, K. Chen, Y. Ding, and C. Fan, “Learning a Facial\nExpression Embedding Disentangled From Identity,” in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2021, pp. 6759–6768.\n[67] A. H. Farzaneh and X. Qi, “Facial expression recognition in the\nwild via deep attentive center loss,” in WACV, 2021, pp. 2402–\n2411.\n[68] Z. Zhao, Q. Liu, and F. Zhou, “Robust lightweight facial expression\nrecognition network with label distribution training,” in Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence , vol. 35, 2021,\npp. 3510–3519.\n[69] J. Cai, Z. Meng, A. S. Khan, Z. Li, J. O’Reilly, and Y. Tong, “Proba-\nbilistic Attribute Tree Structured Convolutional Neural Networks\nfor Facial Expression Recognition in the Wild,” IEEE T-AC , pp.\n1–1.\n[70] D. Ruan, Y. Yan, S. Lai, Z. Chai, C. Shen, and H. Wang, “Feature\ndecomposition and reconstruction learning for effective facial ex-\npression recognition,” in CVPR, 2021, pp. 7660–7669.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\n[71] J. Jiang and W. Deng, “Boosting Facial Expression Recognition by\nA Semi-Supervised Progressive Teacher,” IEEE T-AC, pp. 1–1.\n[72] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, “A discriminative feature\nlearning approach for deep face recognition,” in European confer-\nence on computer vision. Springer, 2016, pp. 499–515.\n[73] Z. Liu, S. Li, and W. Deng, “Boosting-poof: boosting part based\none vs one feature for facial expression recognition in the wild,” in\n2017 12th IEEE International Conference on Automatic Face & Gesture\nRecognition (FG 2017). IEEE, 2017, pp. 967–972.\n[74] M. Li, H. Xu, X. Huang, Z. Song, X. Liu, and X. Li, “Facial\nexpression recognition with identity and emotion joint learning,”\nIEEE T-AC, vol. 12, no. 2, pp. 544–550, 2021.\n[75] S. Albanie, A. Nagrani, A. Vedaldi, and A. Zisserman, “Emotion\nrecognition in speech using cross-modal transfer in the wild,” in\nProceedings of the 26th ACM international conference on Multimedia ,\n2018, pp. 292–301.\n[76] B.-K. Kim, H. Lee, J. Roh, and S.-Y. Lee, “Hierarchical\ncommittee of deep cnns with exponentially-weighted decision\nfusion for static facial expression recognition,” in Proceedings\nof the 2015 ACM on International Conference on Multimodal\nInteraction, ser. ICMI ’15. New York, NY, USA: Association\nfor Computing Machinery, 2015, p. 427–434. [Online]. Available:\nhttps://doi.org/10.1145/2818346.2830590\n[77] H. Li, N. Wang, Y. Yu, X. Yang, and X. Gao, “Lban-il: A novel\nmethod of high discriminative representation for facial expression\nrecognition,” Neurocomputing, vol. 432, pp. 159–169, 2021.\n[78] Z. Li, S. Han, A. S. Khan, J. Cai, Z. Meng, J. O’Reilly, and Y. Tong,\n“Pooling map adaptation in convolutional neural network for\nfacial expression recognition,” in 2019 IEEE international conference\non multimedia and expo (ICME). IEEE, 2019, pp. 1108–1113.\n[79] Z. Wen, W. Lin, T. Wang, and G. Xu, “Distract Your Attention:\nMulti-head Cross Attention Network for Facial Expression Recog-\nnition.”\n[80] J. Weng, Y. Yang, Z. Tan, and Z. Lei, “Attentive hybrid feature\nwith two-step fusion for facial expression recognition,” in 2020\n25th International Conference on Pattern Recognition (ICPR) , 2021,\npp. 6410–6416.\n[81] Z. Lian, Y. Li, J.-H. Tao, J. Huang, and M.-Y. Niu, “Expression\nAnalysis Based on Face Regions in Real-world Conditions,” In-\nternational Journal of Automation and Computing , vol. 17, no. 1, pp.\n96–107.\n[82] T. T. D. Pham and C. S. Won, “Facial Action Units for Training\nConvolutional Neural Networks,” IEEE Access, vol. 7, pp. 77 816–\n77 824.\n[83] W. Zhang, Z. Guo, K. Chen, L. Li, Z. Zhang, Y. Ding, R. Wu,\nT. Lv, and C. Fan, “Prior aided streaming network for multi-task\naffective analysis,” in ICCV Workshops, 2021, pp. 3532–3542.\n[84] Y. Jin, T. Zheng, C. Gao, and G. Xu, “MTMSN: Multi-Task and\nMulti-Modal Sequence Network for Facial Action Unit and Ex-\npression Recognition,” in ICCV Workshops, pp. 3590–3595.\n[85] L. Wang, S. Wang, J. Qi, and K. Suzuki, “A multi-task mean teacher\nfor semi-supervised facial affective behavior analysis,” 2021.\n[86] D. Deng, L. Wu, and B. E. Shi, “Iterative Distillation for Better\nUncertainty Estimates in Multitask Emotion Recognition,” inICCV\nWorkshops, pp. 3557–3566.\n[87] F. Song, X. Tan, X. Liu, and S. Chen, “Eyes closeness detection\nfrom still images with multi-scale histograms of principal oriented\ngradients,” Pattern Recognition, vol. 47, no. 9, pp. 2825–2838, 2014.\n[88] Z. Liu, P . Luo, X. Wang, and X. Tang, “Deep learning face attributes\nin the wild,” in ICCV, 2015, pp. 3730–3738.\n[89] C. Fabian Benitez-Quiroz, R. Srinivasan, and A. M. Martinez,\n“Emotionet: An accurate, real-time algorithm for the automatic\nannotation of a million facial expressions in the wild,” in CVPR,\n2016, pp. 5562–5570.\n[90] W. Zhang, Z. Guo, K. Chen, L. Li, Z. Zhang, and Y. Ding. Prior\nAided Streaming Network for Multi-task Affective Recognitionat\nthe 2nd ABAW2 Competition.\n[91] Y. Jin, T. Zheng, C. Gao, and G. Xu. A Multi-modal and Multi-task\nLearning Method for Action Unit and Expression Recognition.\n[92] A. Krizhevsky, G. Hinton et al. , “Learning multiple layers of\nfeatures from tiny images,” 2009.\n[93] H. Chefer, S. Gur, and L. Wolf, “Transformer interpretability\nbeyond attention visualization,” in CVPR, 2021, pp. 782–791.",
  "topic": "Pooling",
  "concepts": [
    {
      "name": "Pooling",
      "score": 0.8049948215484619
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6813583374023438
    },
    {
      "name": "Discriminative model",
      "score": 0.6621872186660767
    },
    {
      "name": "Computer science",
      "score": 0.6597409248352051
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6302492022514343
    },
    {
      "name": "Transformer",
      "score": 0.605449378490448
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.5239394307136536
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5215842127799988
    },
    {
      "name": "Security token",
      "score": 0.5190377831459045
    },
    {
      "name": "Machine learning",
      "score": 0.4916401505470276
    },
    {
      "name": "Computation",
      "score": 0.43453675508499146
    },
    {
      "name": "Facial expression",
      "score": 0.41705143451690674
    },
    {
      "name": "Speech recognition",
      "score": 0.38321202993392944
    },
    {
      "name": "Engineering",
      "score": 0.11684605479240417
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Algorithm",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}