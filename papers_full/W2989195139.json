{
  "title": "What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning",
  "url": "https://openalex.org/W2989195139",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2273667721",
      "name": "Lee Jae-Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289619967",
      "name": "Tang, Raphael",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2672090663",
      "name": "Lin, Jimmy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2969624041",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2806120502",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2475287302",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2980282514"
  ],
  "abstract": "Pretrained transformer-based language models have achieved state of the art across countless tasks in natural language processing. These models are highly expressive, comprising at least a hundred million parameters and a dozen layers. Recent evidence suggests that only a few of the final layers need to be fine-tuned for high quality on downstream tasks. Naturally, a subsequent research question is, \"how many of the last layers do we need to fine-tune?\" In this paper, we precisely answer this question. We examine two recent pretrained language models, BERT and RoBERTa, across standard tasks in textual entailment, semantic similarity, sentiment analysis, and linguistic acceptability. We vary the number of final layers that are fine-tuned, then study the resulting change in task-specific effectiveness. We show that only a fourth of the final layers need to be fine-tuned to achieve 90% of the original quality. Surprisingly, we also find that fine-tuning all layers does not always help.",
  "full_text": "What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning\nJaejun Lee, Raphael Tang,and Jimmy Lin\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\nAbstract\nPretrained transformer-based language models\nhave achieved state of the art across countless\ntasks in natural language processing. These\nmodels are highly expressive, comprising at\nleast a hundred million parameters and a dozen\nlayers. Recent evidence suggests that only a\nfew of the ﬁnal layers need to be ﬁne-tuned for\nhigh quality on downstream tasks. Naturally,\na subsequent research question is, “how many\nof the last layers do we need to ﬁne-tune?” In\nthis paper, we precisely answer this question.\nWe examine two recent pretrained language\nmodels, BERT and RoBERTa, across standard\ntasks in textual entailment, semantic similarity,\nsentiment analysis, and linguistic acceptabil-\nity. We vary the number of ﬁnal layers that are\nﬁne-tuned, then study the resulting change in\ntask-speciﬁc effectiveness. We show that only\na fourth of the ﬁnal layers need to be ﬁne-tuned\nto achieve 90% of the original quality. Surpris-\ningly, we also ﬁnd that ﬁne-tuning all layers\ndoes not always help.\n1 Introduction\nTransformer-based pretrained language models\nare a battle-tested solution to a plethora of natu-\nral language processing tasks. In this paradigm, a\ntransformer-based language model is ﬁrst trained\non copious amounts of text, then ﬁne-tuned on\ntask-speciﬁc data. BERT (Devlin et al., 2019),\nXLNet (Yang et al., 2019), and RoBERTa (Liu\net al., 2019) are some of the most well-known\nones, representing the current state of the art in\nnatural language inference, question answering,\nand sentiment classiﬁcation, to list a few. These\nmodels are extremely expressive, consisting of at\nleast a hundred million parameters, a hundred at-\ntention heads, and a dozen layers.\nAn emerging line of work questions the need\nfor such a parameter-loaded model, especially on\na single downstream task. Michel et al. (2019), for\nexample, note that only a few attention heads need\nto be retained in each layer for acceptable effec-\ntiveness. Kovaleva et al. (2019) ﬁnd that, on many\ntasks, just the last few layers change the most af-\nter the ﬁne-tuning process. We take these obser-\nvations as evidence that only the last few layers\nnecessarily need to be ﬁne-tuned.\nThe central objective of our paper is, then, to de-\ntermine how many of the last layers actually need\nﬁne-tuning. Why is this an important subject of\nstudy? Pragmatically, a reasonable cutoff point\nsaves computational memory across ﬁne-tuning\nmultiple tasks, which bolsters the effectiveness\nof existing parameter-saving methods (Houlsby\net al., 2019). Pedagogically, understanding the re-\nlationship between the number of ﬁne-tuned layers\nand the resulting model quality may guide future\nworks in modeling.\nOur research contribution is a comprehensive\nevaluation, across multiple pretrained transform-\ners and datasets, of the number of ﬁnal layers\nneeded for ﬁne-tuning. We show that, on most\ntasks, we need to ﬁne-tune only one fourth of the\nﬁnal layers to achieve within 10% parity with the\nfull model. Surprisingly, on SST-2, a sentiment\nclassiﬁcation dataset, we ﬁnd that not ﬁne-tuning\nall of the layers leads to improved quality.\n2 Background and Related Work\n2.1 Pretrained Language Models\nIn the pretrained language modeling paradigm, a\nlanguage model (LM) is trained on vast amounts\nof text, then ﬁne-tuned on a speciﬁc downstream\ntask. Peters et al. (2018) are one of the ﬁrst to suc-\ncessfully apply this idea, outperforming state of\nthe art in question answering, textual entailment,\nand sentiment classiﬁcation. Their model, dubbed\nELMo, comprises a two-layer BiLSTM pretrained\non the Billion Word Corpus (Chelba et al., 2014).\narXiv:1911.03090v1  [cs.CL]  8 Nov 2019\nFurthering this approach with more data and\nimproved modeling, Devlin et al. (2019) pre-\ntrain deep 12- and 24-layer bidirectional trans-\nformers (Vaswani et al., 2017) on the entirety of\nWikipedia and BooksCorpus (Zhu et al., 2015).\nTheir approach, called BERT, achieves state of the\nart across all tasks in the General Language Under-\nstanding Evaluation (GLUE) benchmark (Wang\net al., 2018), as well as the Stanford Question An-\nswering Dataset (Rajpurkar et al., 2016).\nAs a result of this development, a ﬂurry of\nrecent papers has followed this more-data-plus-\nbetter-models principle. Two prominent exam-\nples include XLNet (Yang et al., 2019) and\nRoBERTa (Liu et al., 2019), both of which con-\ntest the present state of the art. XLNet proposes\nto pretrain two-stream attention-augmented trans-\nformers on an autoregressive LM objective, in-\nstead of the original cloze and next sentence pre-\ndiction (NSP) tasks from BERT. RoBERTa pri-\nmarily argues for pretraining longer, using more\ndata, and removing the NSP task for BERT.\n2.2 Layerwise Interpretability\nThe prevailing evidence in the neural network lit-\nerature suggests that earlier layers extract univer-\nsal features, while later ones perform task-speciﬁc\nmodeling. Zeiler and Fergus (2014) visualize the\nper-layer activations in image classiﬁcation net-\nworks, ﬁnding that the ﬁrst few layers function\nas corner and edge detectors, and the ﬁnal layers\nas class-speciﬁc feature extractors. Gatys et al.\n(2016) demonstrate that the low- and high-level\nnotions of content and style are separable in con-\nvolutional neural networks, with lower layers cap-\nturing content and higher layers style.\nPretrained transformers. In the NLP litera-\nture, similar observations have been made for pre-\ntrained language models. Clark et al. (2019) an-\nalyze BERT’s attention and observe that the bot-\ntom layers attend broadly, while the top layers\ncapture linguistic syntax. Kovaleva et al. (2019)\nﬁnd that the last few layers of BERT change the\nmost after task-speciﬁc ﬁne-tuning. Similar to our\nwork, Houlsby et al. (2019) ﬁne-tune the top lay-\ners of BERT, as part of their baseline comparison\nfor their model compression approach. However,\nnone of the studies comprehensively examine the\nnumber of necessary ﬁnal layers across multiple\npretrained transformers and datasets.\nModel Embedding Per-Layer Output Total\nBERTBASE 24M (22%) 7M (7%) 0.6M (0.5%) 110M\nRoBERTaBASE 39M (31%) 7M (6%) 0.6M (0.5%) 125M\nBERTLARGE 32M (10%) 13M (4%) 1M (0.3%) 335M\nRoBERTaLARGE 52M (15%) 13M (4%) 1M (0.3%) 355M\nTable 1: Parameter statistics for the base and large vari-\nants of BERT and RoBERTa. Note that “per-layer” in-\ndicates the number of parameters in one intermediate\nlayer, which is more relevant to our study.\nModel CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE\nMCC Acc. ρ ρ F1 Acc. Acc. Acc.\nBERTBASE 58.8 92.7 90.4 89.5 87.8 84.3 91.3 68.2\nRoBERTaBASE 59.9 94.6 92.8 90.8 88.8 87.4 92.7 78.2\nBERTLARGE 61.8 93.4 90.6 89.7 88.3 86.4 92.2 71.1\nRoBERTaLARGE 66.0 95.5 92.8 91.9 89.1 89.9 94.3 84.5\nTable 2: Reproduced results of BERT and RoBERTa\non the development sets.\n3 Experimental Setup\nWe conduct our experiments on NVIDIA Tesla\nV100 GPUs with CUDA v10.1. We run the mod-\nels from the Transformers library (v2.1.1; Wolf\net al., 2019) using PyTorch v1.2.0.\n3.1 Models and Datasets\nWe choose BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019) as the subjects of\nour study, since they represent state of the art\nand the same architecture. XLNet (Yang et al.,\n2019) is another alternative; however, they use\na slightly different attention structure, and our\npreliminary experiments encountered difﬁculties\nin reproducibility with the Transformers library.\nEach model has base and large variants that con-\ntain 12 and 24 layers, respectively. We denote\nthem by appending the variant name as a subscript\nto the model name.\nWithin each variant, the two models display\nslight variability in parameter count—110 and 125\nmillion in the base variant, and 335 and 355 in\nthe large one. These differences are mostly at-\ntributed to RoBERTa using many more embedding\nparameters—exactly 63% more for both variants.\nFor in-depth, layerwise statistics, see Table 1.\nFor our datasets, we use the GLUE bench-\nmark, which comprises the tasks in natural lan-\nguage inference, sentiment classiﬁcation, linguis-\ntic acceptability, and semantic similarity. Speciﬁ-\ncally, for natural language inference (NLI), it pro-\nvides the Multigenre NLI (MNLI; Williams et al.,\n2018), Question NLI (QNLI; Wang et al., 2018),\nModel Frozen CoLA SST-2 MRPC STS-B QQP MNLI MNLI-mm QNLI RTE\nup to MCC Acc. F 1 ρ F1 Acc. Acc. Acc. Acc.\nBERTBASE\n0th 58.3 92.7 90.3 88.8 87.9 84.2 84.8 91.4 67.6\n9th 47.5 90.8 85.4 88.0 85.3 82.0 82.4 89.5 62.3\n12th 29.4 84.9 81.5 78.1 72.0 56.4 57.1 74.5 57.5\nTable 3: Development set results of BERT, with none, some, and all of the nonoutput layer weights ﬁne-tuned.\nResults are averaged across ﬁve runs.\nModel Frozen CoLA SST-2 MRPC STS-B\nup to MCC Acc. F 1 ρ\nBERTBASE\n0th 58.3 92.7 90.3 88.9\n9th 47.5 90.8 85.4 88.0\n12th 29.4 84.9 81.5 78.1\nRoBERTaBASE\n0th 59.4 94.3 92.3 90.6\n7th 58.6 93.3 89.5 87.7\n12th 0.0 80.2 81.2 20.0\nTable 4: Development set results of all base models,\nwith none, some, and all of the nonoutput layer weights\nﬁne-tuned. Results are averaged across ﬁve runs.\nRecognizing Textual Entailment (RTE; Bentivogli\net al., 2009), and Winograd NLI (Levesque et al.,\n2012) datasets. For semantic textual similarity and\nparaphrasing, it contains the Microsoft Research\nParaphrase Corpus (MRPC; Dolan and Brockett,\n2005), the Semantic Textual Similarity Bench-\nmark (STS-B; Cer et al., 2017), and Quora Ques-\ntion Pairs (QQP; Iyer et al.). Finally, its single-\nsentence tasks consist of the binary-polarity Stan-\nford Sentiment Treebank (SST-2; Socher et al.,\n2013) and the Corpus of Linguistic Acceptabil-\nity (CoLA; Warstadt et al., 2018).\n3.2 Fine-Tuning Procedure\nOur ﬁne-tuning procedure closely resembles those\nof BERT and RoBERTa. We choose the Adam\noptimizer (Kingma and Ba, 2014) with a batch\nsize of 16 and ﬁne-tune BERT for 3 epochs and\nRoBERTa for 10, following the original papers.\nFor hyperparameter tuning, the best learning rate\nis different for each task, and all of the origi-\nnal authors choose one between 1 × 10−5 and\n5 × 10−5; thus, we perform line search over the\ninterval with a step size of 1 × 10−5. We report\nthe best results in Table 2.\nOn each model, we freeze the embeddings and\nthe weights of the ﬁrst N layers, then ﬁne-tune\nthe rest using the best hyperparameters of the full\nmodel. Speciﬁcally, if L is the number of lay-\nModel Frozen CoLA SST-2 MRPC STS-B\nup to MCC Acc. F 1 ρ\nBERTLARGE\n0th 61.9 93.4 90.3 89.8\n18th 51.6 92.7 85.4 88.0\n24th 24.4 87.8 81.3 71.7\nRoBERTaLARGE\n0th 66.1 95.1 92.2 92.0\n17th 60.5 95.1 91.3 89.6\n24th 0.0 79.2 81.2 11.2\nTable 5: Development set results of all large models,\nwith none, some, and all of the nonoutput layer weights\nﬁne-tuned. Results are averaged across ﬁve runs.\ners, we explore N = L\n2 ,L\n2 + 1,...,L . Due to\ncomputational limitations, we set half as the cut-\noff point. Additionally, we restrict our compre-\nhensive all-datasets exploration to the base vari-\nant of BERT, since the large model variants and\nRoBERTa are much more computationally in-\ntensive. On the smaller CoLA, SST-2, MRPC,\nand STS-B datasets, we comprehensively evaluate\nboth models. These choices do not substantially\naffect our analysis.\n4 Analysis\n4.1 Operating Points\nWe report three relevant operating points in Tables\n3–5: two extreme operating points and an interme-\ndiate one. The former is self-explanatory, indicat-\ning ﬁne-tuning all or none of the nonoutput layers.\nThe latter denotes the number of necessary layers\nfor reaching at least 90% of the full model quality,\nexcluding CoLA, which is an outlier.\nFrom the reported results in Tables 3–5, ﬁne-\ntuning the last output layer and task-speciﬁc lay-\ners is insufﬁcient for all tasks—see the rows corre-\nsponding to 0, 12, and 24 frozen layers. However,\nwe ﬁnd that the ﬁrst half of the model is unnec-\nessary; the base models, for example, need ﬁne-\ntuning of only 3–5 layers out of the 12 to reach\n90% of the original quality—see Table 4, middle\nall 11 10 9 8 7 60.6\n0.4\n0.2\n0.0\nMCC\nCoLA-base\nall 11 10 9 8 7 60.20\n0.15\n0.10\n0.05\n0.00\nAcc.\nSST-2-base\nall 11 10 9 8 7 60.20\n0.15\n0.10\n0.05\n0.00\nF1\nMRPC-base\nall 11 10 9 8 7 60.20\n0.15\n0.10\n0.05\n0.00\nSTS-B-base\nall 22 20 18 16 14 120.6\n0.4\n0.2\n0.0\nMCC\nCoLA-large\nall 22 20 18 16 14 120.20\n0.15\n0.10\n0.05\n0.00\nAcc.\nSST-2-large\nall 22 20 18 16 14 120.20\n0.15\n0.10\n0.05\n0.00\nF1\nMRPC-large\nall 22 20 18 16 14 120.20\n0.15\n0.10\n0.05\n0.00\nSTS-B-large\nall2322212019181716151413120.20\n0.15\n0.10\n0.05\n0.00\nBERT RoBERTa\nFigure 1: Relative change in quality compared to the full models, with respect to the number of frozen initial\nlayers, represented by the x-axes.\nsubrow of each row group. Similarly, ﬁne-tuning\nonly a fourth of the layers is sufﬁcient for the large\nmodels (see Table 5); only 6 layers out of 24 for\nBERT and 7 for RoBERTa.\n4.2 Per-Layer Study\nIn Figure 1, we examine how the relative qual-\nity changes with the number of frozen layers. To\ncompute a relative score, we subtract each frozen\nmodel’s results from its corresponding full model.\nThe relative score aligns the two baselines at zero,\nallowing the fair comparison of the transformers.\nThe graphs report the average of ﬁve trials to re-\nduce the effects of outliers.\nWhen every component except the output layer\nand the task-speciﬁc layer is frozen, the ﬁne-tuned\nmodel achieves only 64% of the original quality,\non average. As more layers are ﬁne-tuned, the\nmodel effectiveness often improves drastically—\nsee CoLA and STS-B, the ﬁrst and fourth verti-\ncal pairs of subﬁgures from the left. This demon-\nstrates that gains decompose nonadditively with\nrespect to the number of frozen initial layers. Fine-\ntuning subsequent layers shows diminishing re-\nturns, with every model rapidly approaching the\nbaseline quality at ﬁne-tuning half of the network;\nhence, we believe that half is a reasonable cutoff\npoint for characterizing the models.\nFinally, for the large variants of BERT and\nRoBERTa on SST-2 (second subﬁgure from both\nthe top and the left), we observe a surprisingly\nconsistent increase in quality when freezing 12–16\nlayers. This ﬁnding suggests that these models\nmay be overparameterized for SST-2.\n5 Conclusions and Future Work\nIn this paper, we present a comprehensive evalu-\nation of the number of ﬁnal layers that need to\nbe ﬁne-tuned for pretrained transformer-based lan-\nguage models. We ﬁnd that only a fourth of the\nlayers necessarily need to be ﬁne-tuned to ob-\ntain 90% of the original quality. One line of\nfuture work is to conduct a similar, more ﬁne-\ngrained analysis on the contributions of the atten-\ntion heads.\nAcknowledgments\nThis research was supported by the Natu-\nral Sciences and Engineering Research Council\n(NSERC) of Canada, and enabled by computa-\ntional resources provided by Compute Ontario and\nCompute Canada.\nReferences\nLuisa Bentivogli, Ido Kalman Dagan, Dang Hoa,\nDanilo Giampiccolo, and Bernardo Magnini. 2009.\nThe ﬁfth PASCAL recognizing textual entailment\nchallenge. In TAC 2009 Workshop.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings of\nthe 11th International Workshop on Semantic Eval-\nuation.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2014. One billion word benchmark for mea-\nsuring progress in statistical language modeling. In\nFifteenth Annual Conference of the International\nSpeech Communication Association.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does\nBERT look at? An analysis of BERT’s attention.\narXiv:1906.04341.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing.\nLeon A. Gatys, Alexander S. Ecker, and Matthias\nBethge. 2016. Image style transfer using convolu-\ntional neural networks. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recog-\nnition.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn International Conference on Machine Learning.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai.\nFirst Quora dataset release: Question pairs.\nDiederik P. Kingma and Jimmy Ba. 2014.\nAdam: A method for stochastic optimization.\narXiv:1412.6980.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. arXiv:1908.08593.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The Winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv:1907.11692.\nPaul Michel, Omer Levy, and Graham Neubig.\n2019. Are sixteen heads really better than one?\narXiv:1905.10650.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Process-\ning.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018. Neural network acceptability judg-\nments. arXiv:1805.12471.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus for\nsentence understanding through inference. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace’s Trans-\nformers: State-of-the-art natural language process-\ning. arXiv:1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G.\nCarbonell, Ruslan Salakhutdinov, and Quoc V . Le.\n2019. XLNet: generalized autoregressive pretrain-\ning for language understanding. arXiv:1906.08237.\nMatthew D. Zeiler and Rob Fergus. 2014. Visualizing\nand understanding convolutional networks. In Euro-\npean Conference on Computer Vision.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE In-\nternational Conference on Computer Vision.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8136752843856812
    },
    {
      "name": "Computer science",
      "score": 0.6937092542648315
    },
    {
      "name": "Natural language processing",
      "score": 0.56878662109375
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47593024373054504
    },
    {
      "name": "Language model",
      "score": 0.45044460892677307
    },
    {
      "name": "Voltage",
      "score": 0.09379589557647705
    },
    {
      "name": "Engineering",
      "score": 0.06807497143745422
    },
    {
      "name": "Electrical engineering",
      "score": 0.06092885136604309
    }
  ]
}