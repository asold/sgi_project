{
  "title": "Transformer-based artificial neural networks for the conversion between chemical notations",
  "url": "https://openalex.org/W3186326839",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3182613748",
      "name": "Lev Krasnov",
      "affiliations": [
        "Lomonosov Moscow State University",
        "Skolkovo Institute of Science and Technology",
        "Synergy University"
      ]
    },
    {
      "id": "https://openalex.org/A2694353971",
      "name": "Ivan Khokhlov",
      "affiliations": [
        "Synergy University"
      ]
    },
    {
      "id": "https://openalex.org/A1973288037",
      "name": "Maxim V. Fedorov",
      "affiliations": [
        "Skolkovo Institute of Science and Technology",
        "Synergy University"
      ]
    },
    {
      "id": "https://openalex.org/A1903426752",
      "name": "Sergey Sosnin",
      "affiliations": [
        "Skolkovo Institute of Science and Technology",
        "Synergy University"
      ]
    },
    {
      "id": "https://openalex.org/A3182613748",
      "name": "Lev Krasnov",
      "affiliations": [
        "Lomonosov Moscow State University",
        "Skolkovo Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2694353971",
      "name": "Ivan Khokhlov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1973288037",
      "name": "Maxim V. Fedorov",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1903426752",
      "name": "Sergey Sosnin",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2162012799",
    "https://openalex.org/W1981100279",
    "https://openalex.org/W2048347251",
    "https://openalex.org/W2034354062",
    "https://openalex.org/W3082060554",
    "https://openalex.org/W3200621845",
    "https://openalex.org/W3158543275",
    "https://openalex.org/W2899070097",
    "https://openalex.org/W2947423323",
    "https://openalex.org/W3030978062",
    "https://openalex.org/W3088265803",
    "https://openalex.org/W2981644170",
    "https://openalex.org/W3186326839",
    "https://openalex.org/W3108121102",
    "https://openalex.org/W3094771832",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W3133325765"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2021) 11:14798  | https://doi.org/10.1038/s41598-021-94082-y\nwww.nature.com/scientificreports\nTransformer‑based artificial neural \nnetworks for the conversion \nbetween chemical notations\nLev Krasnov1,2,3, Ivan Khokhlov2, Maxim V. Fedorov1,2 & Sergey Sosnin1,2*\nWe developed a Transformer‑based artificial neural approach to translate between SMILES and IUPAC \nchemical notations: Struct2IUPAC and IUPAC2Struct. The overall performance level of our model is \ncomparable to the rule‑based solutions. We proved that the accuracy and speed of computations as \nwell as the robustness of the model allow to use it in production. Our showcase demonstrates that \na neural‑based solution can facilitate rapid development keeping the required level of accuracy. \nWe believe that our findings will inspire other developers to reduce development costs by replacing \ncomplex rule‑based solutions with neural‑based ones.\nBefore the Information Age, chemical names were a universal language for description of chemical structures. \nAt the infancy stage of organic chemistry, there were no common rules for the naming of chemical compounds. \nHowever, the extensive growth of the explored part of chemical space in the XIX century motivated chemists to \nmake efforts to harmonize chemical naming globally. In 1919 International Union of Pure and Applied Chemistry \n(IUPAC) was founded, and this non-commercial organization still leads the development of chemical nomen -\nclature. IUPAC publishes the Nomenclature of Organic Chemistry, commonly known as the “Blue Book. ”1 The \n“Blue Book” provides guidelines on the unambiguous names for chemical compounds.\nNowadays there are several alternative representations for organic structures. For example, Simplified Molecu-\nlar Input Line Entry System (SMILES) was designed to provide convenience for both human-based and com-\nputer-based processing of chemical information. However, IUPAC nomenclature still plays an important role in \norganic chemistry. The IUPAC notations are obligatory for processing chemicals in many regulated protocols, \nfor example: REACH registration in the EU, patent application submission in many countries, regulatory sub -\nmission to FDA in the U.S. Most chemical journals require IUPAC names for published organic structures too. \nMoreover, chemists quite often just prefer to use them. Overall, it is quite probable that the IUPAC nomenclature \nwill be still in use for a while.\nIn the past, chemists created IUPAC names manually. This process was error-prone because it requires deep \nknowledge of the nomenclature as well as a high level of  attention2. It is hard for humans to perform the naming \nprocess accurately because it involves a complex algorithm. Moreover, chemists are biased towards trivial names \nwhich poses an extra challenge for the proper conversion between different notations. Computers alleviate this \nproblem. Now chemists use software tools for the name generation widely.\nThe history of names generators begins from the pioneering work of  Garfield3. However, the first “everyday” \nsoftware for chemists was created and distributed only at the end of the XX century. Now, there exist several \ncommercial programs for generating IUPAC names: ACD/Labs, ChemDraw, Marvin, IMnova IUPAC Name, etc. \nAlso, there is a framework LexiChem TK that provides an application programming interface (API) for some \nprogramming  languages4. Nevertheless, there is no an open-source tool for the structure-to-name translation. \nLicensing agreements with the existing solutions, like ChemDraw JS and LexiChem TK, require special permis-\nsions for embedding to other platforms.\nWe note that there is an open-source tool for the name-to-structure translation: OPSIN developed by Daniel \n Lowe5. But, as we mentioned above, there is no one for the inverse problem: structure-to-name conversion.\nRecurrent neural networks and Transformer have been successfully used for natural language  translation6,7. \nIt is worth mentioning that a neural model for direct translation from English to Chinese languages was pro-\nposed  recently8. After our  preprint9 several studies about the conversion between structural representations and \nIUPAC names have been published. Rajan et. al. proposed an RNN-based approach for SMILES to IUPAC name \n conversion10. Omote et. al. proposed a multitask Transformer model and byte-pair encoding for the conversion \nOPEN\n1Center for Computational and Data-Intensive Science and Engineering, Skolkovo Institute of Science and \nTechnology, Bolshoy Boulevard 30, bld. 1, Moscow 121205, Russia. 2Syntelly LLC, Bolshoy Boulevard 30, bld. 1, \nMoscow 121205, Russia. 3Department of Chemistry, Lomonosov Moscow State University, GSP -1, 1-3 Leninskiye \nGory, Moscow 119991, Russia. *email: sergey.sosnin@skoltech.ru\n2\nVol:.(1234567890)Scientific Reports |        (2021) 11:14798  | https://doi.org/10.1038/s41598-021-94082-y\nwww.nature.com/scientificreports/\nbetween chemical names and SMILES and InChI  strings11. An interesting feature of this research was the attempt \nto convert non-standard chemical names (denoted in PubChem as Synonyms).\nWe believe that the development costs for a tool for structure-to-name translation “from scratch” are unac-\nceptable in the era of neural networks and artificial intelligence. Instead, we built a Transformer-based neural \nnetwork that can convert molecules from SMILES representations to IUPAC names and the other way around. \nIn this paper, we describe our solution, discuss its advantages and disadvantages, and show that the Transformer \ncan provide something that resembles human chemical intuition.\nSurprisingly, our neural-based solutions achieved a good level of performance that was comparable with the \nrules-based software. We believe that our approach is suitable for solving the problem of conversions between \nother technical notations (or other algorithmic challenges) and hope that our findings highlight a new way to \nresolve issues when the development of a rules-based solution is costly or time-consuming.\nMaterials and methods\nDatabase. Deep learning techniques require large amount of data. PubChem is the largest freely-available \ncollection of chemical compounds with  annotations12. We used chemical structures and their corresponding \nIUPAC names from this database. It had 94,726,085 structures in total. The processing and training on the full \nPubChem database is time-consuming, and about 50M samples seem to be enough for training; so we split the \ndatabase into two parts and used one half for training and the other one for testing. Structures that can not be \nprocessed by RDkit were removed resulting in 47,312,235 structures in the training set and 47,413,850 in the \ntest set.\nIUPAC and SMILES tokenizers. Tokenization—is a process of partition of a sequence into chunks and \ndemarkation such chunks (tokens). It is a common preprocessing stage for language models. We use a character-\nbased SMILES tokenization and implemented a rule-based IUPAC tokenizer (Fig.  1). Our IUPAC tokenizer \nwas manually designed and curated. We collected all suffixes (-one, -al, -ol, etc.), prefixes (-oxy, -hydroxy, -di, \n-tri, -tetra, etc.), trivial names (naphthalene, pyrazole, pyran, adamantane, etc.) and special symbols, numbers, \nstereochemical designations ((, ), [, ], -, N, R(S), E(Z), /afii9838 , etc.). We did not include some tokens in the IUPAC \ntokenizer because they were very rare or represented trivial names. Also, there is a technical mistake with \n”selena” token that makes it impossible to process molecules containing aromatic selenium. We excluded from \nthe training and test sets all molecules that cannot be correctly tokenized. Saying that, we note that our tokenizer \nwas able to correctly process more than 99% of molecules from PubChem.\nTransformer model. Transformer is a modern neural architecture designed by the Google team, mostly \nto boost the quality of machine  translation6. Since its origin, Transformer based networks has notably boosted \nthe performance of NLP applications leading to newsworthy GPT  models13. Transformer has been successfully \napplied to chemical-related problem: prediction of outcomes of organic  reactions14, QSAR  modelling15 and the \ncreation of molecular  representations16. We used the standard Transformer architecture with 6 encoder and \ndecoder layers, and 8 attention heads. The attention dimension was 512, and the dimension of the feed-for -\nward layer was 2048. We trained two models: Struct2IUPAC that converts SMILES strings to IUPAC names and \nIUPAC2Srtuct—that performs reverse conversion. Basically, there is no need for IUPAC2Srtuct model because \nan open-source OPSIN can be successfully used instead. However, studying the reverse conversion performance \nand following the aesthetic symmetry principle, we created these two models. The schema of our Struct2IUPAC \nmodel is given in Fig. 2.\nVerification step. Our scheme involves artificial neural networks and its training on data, therefore, the \ngenerated solution has a statistical nature with some stochastic components in it. But the generation of a chemi-\ncal name is a precise task: a name can be either correct or wrong. We believe that the denial of incorrect transla-\ntion is better than false conversion. Transformer can generate several versions of a sequence using beam search. \nUsing OPSIN we can validate generated chemical names to guarantee that these names correspond to the correct \nstructure. So, we can detect failures of our generator and do not display the wrong name. The flowchart of the \nverification step is given in Fig. 3.\nFigure 1.  Demonstration of SMILES tokenization (top) and IUPAC names tokenization (bottom).\n3\nVol.:(0123456789)Scientific Reports |        (2021) 11:14798  | https://doi.org/10.1038/s41598-021-94082-y\nwww.nature.com/scientificreports/\nResults and discussion\nIn order to validate the quality of our models we sampled randomly 100,000 molecules from the test set and \ncalculated the percentage of correct predictions with different beam size. Our SMILES to IUPAC names con -\nverter, running with verification step, with beam size = 5, achieved 98.9% accuracy (1075 mistakes per 100,000 \nmolecules) on a subset of 100,000 random molecules from the test set. Transformer demonstrates the ability for \nthe precise solution of an algorithmic problem, and this fact raises a new paradigm in software development. \nBefore that there was a consensus opinion that artificial neural networks should not be used if a precise (strict \nalgorithmic) solution is possible. Meanwhile, our approach is built on top of typical neural architecture and \nrequires minimal chemical rules collection (only for tokenizer). The implementation of our system required \nabout one and a half employee months for the whole pipeline. It is hard to estimate the resources required to \ndevelop an algorithmic-based generator with competitive performance. Our preliminary estimation about the \ndevelopment of IUPAC names generator “from scratch, ” even using the source of OPSIN, would take more than \na year by a small team. Anyway, we did not quantify our potential expenses, so we prefer to leave this question \nto the discretion of the reader. Also, we believe, that our approach can be even more helpful for legacy data. \nSometimes there is a lack of documentation for legacy technical notation within the presence of some coincide \ndata. Engineers have to perform ”Rosetta Stone investigations” to make a converter. In our approach, a neural \nnetwork solves this task saving developers time.\nMolecules with extra-large number of tokens (oligomers, peptides etc.) are underrepresented in our dataset \n(see Fig. 5). That can be a possible reason explaining the decline of performance for such molecules. One can also \nsee the apparent decrease of performance for very small molecules. For example, testing the model manually, \nFigure 2.  A scheme of Struct2IUPAC Transformer. Adopted  from6.\n4\nVol:.(1234567890)Scientific Reports |        (2021) 11:14798  | https://doi.org/10.1038/s41598-021-94082-y\nwww.nature.com/scientificreports/\nwe found a problematic molecule: methane. A possible explanation could be that the Transformer uses a self-\nattention mechanism that analyses the correlation between tokens at the input sequence. For an extra-short \nsequence, it is hard to grasp the relations between tokens; for example, for the extreme example of the methane \nmolecule (one token in SMILES notation), it is just impossible. To estimate the applicability domain of our model \nwe took 1000 examples for each length from 3 to 10 with a step of 1, from 10 to 100 with a step of 5 and 100 to \n300 with a step of 10. As a result, we found that our model achieves accuracy close to 100% in the interval from \n10 to 60 SMILES tokens. The result of the experiment is given in Fig. 4.\nWe also showed the distribution of sequence lengths on the test set (Fig.  5). The mean value of the SMILES \nlength is 46,0 tokens and IUPAC length is 40,7 tokens. So the majority of the PubChem molecules is within the \napplicability domain of our model.\nWe compared our IUPAC to SMILES Transformer model (IUPAC2Struct) with the rules-based tool OPSIN on \nthe test set (Table 1). Our converter achieved 99.1% accuracy (916 mistakes per 100,000 molecules) and OPSIN \nperformed 99.4% (645 mistakes per 100,000 molecules).\nThe Transformer architecture requiures high computational costs. The application of Transformer can be \nnotably slower than an algorithmic-based solutions. To understand the practical applicability of the model in \nFigure 3.  A scheme of Verification step.\nFigure 4.  The dependence between model accuracy and the length of SMILES.\n5\nVol.:(0123456789)Scientific Reports |        (2021) 11:14798  | https://doi.org/10.1038/s41598-021-94082-y\nwww.nature.com/scientificreports/\nterms of the execution time, we estimated the speed of name generation both on CPU and GPU. We measured \nthe dependence between the number of output IUPAC tokens and the time required for several Transformer runs \nwithout beam search and validation feedback loop. The result of the experiment is given in Fig.  6. Transformer \nconsists of two parts: encoder and decoder. Encoder runs only once to read SMILES input, whereas decoder \nprocesses each output token. For this reason, only the output sequence length influences the time of execution. \nOne can see that GPU is notably faster than CPU. GPU application requires less than 0.5 seconds even for chemi-\ncal names with maximal length. This time-frame is acceptable for the practical usage.\nOur solution requires signficant computational resources to train the model. The final models has been trained \nfor ten days on a machine with 4 Tesla V100 GPUs and 36 CPUs, with the full load. However, it is still far less \nexpensive than employing human beings for this task.\nThe most intriguing ability of Transformer is that it can operate with the IUPAC nomenclature in a chemically \nreasonable way. One can see that the model can infer some chemical knowledge. For example for a molecule \non Fig.  7 model generates four chemically correct names (OPSIN converts these IUPAC names to the same \nstructures):\nFigure 5.  The distribution of the lengths of SMILES and IUPAC on the test set.\nTable 1.  Accuracy (%) of models on the test set of 100k molecules with different beam size.\nStruct2IUPAC IUPAC2Struct\nOPSINBeam 1 Beam 3 Beam 5 Beam 1 Beam 3 Beam 5\n96.1 98.2 98.9 96.6 98.6 99.1 99.4\nFigure 6.  The correlation of mean time and output sequence length.\n6\nVol:.(1234567890)Scientific Reports |        (2021) 11:14798  | https://doi.org/10.1038/s41598-021-94082-y\nwww.nature.com/scientificreports/\n• 3-[[2-[3-(tert-butylcarbamoyl)anilino]acetyl]amino]-N-(3-methoxypropyl)benzamide\n• N-(3-methoxypropyl)-3-[[2-[3-(tert-butylcarbamoyl)anilino]acetyl]amino]benzamide\n• N-tert-butyl-3-[[2-[3-(3-methoxypropylcarbamoyl)anilino]-2-oxoethyl]amino]benzamide\n• 3-[[2-[3-(3-methoxypropylcarbamoyl)anilino]-2-oxoethyl]amino]-N-tert-butylbenzamide\nThis is due to the fact that the molecule has two parent benzamide groups (in red in Fig. 7 ). Depending on the \nchoice of the parent group, there are two different ways to name the molecule. Also, for each of the two ways, \nthere are two more ways associated with the substituents enumeration order. As a result, we have four correct \nIUPAC names for one molecule, and all of these variants are correct.\nIntrigued by this observation we analyzed the distribution of valid and correct molecules in a batch. We took \n10,000 molecules from our 100,000 test subset and calculated: (a) the number of true (correct) IUPAC names \n(reverse translation by OPSIN leads to the same molecule) (b) the number of chemically valid names (OPSIN can \nprocess a molecule, however there is no guarantee that the molecule is the same). These distributions are given \nin Fig. 8. One can see that Transformer can generate up to two correct names for more than 20% of molecules, \nand about 1% of molecules can have up to 4 correct IUPAC names. It supports our claims that Transformer does \nnot just memorize common patterns but infers the logic behind IUPAC nomenclature.\nAn important question is the robustness of our model for various chemical representations: resonance struc-\ntures, canonical/uncanonical SMILES, etc. The majority of structures are represented as canonical SMILES in \nunkekulized form in PubChem. To explore the ability of our model to struggle with kekulized SMILES strings, \nwe converted structures to kekulized SMILES by RDKit and calculated the performance on a subset of our test set \nthat contains 10000 molecules. The results of the experiment are given in Table 3. One can see that there is a mar-\nginal performance drop; however, the overall quality remains high. The situation is the opposite for augmented \n(non-canonical SMILES), where we have a tremendous performance drop. The most probable explanation is the \nlack of non-canonical SMILES in the training set. That is why the model relies on canonical representation. It is \nworth mentioning that another  publication17 demonstrates the possibility (or even advisability) of augmented \nSMILES for Transformer.\nAlso, we studied the behavior of Struct2IUPAC model on compounds with many stereocenters. We took \nfrom our test set all compounds with length from 10 to 60 tokens, and calculated an index for each compound: \nI = S\nN  where N is the number of tokens in a molecule and S is the number of stereocenters. We sorted this subset \nand took the first 10000 compounds. Our test subset was enriched with compounds that have maximal ”stereo-\ndensity”—the fraction of stereocenters per token. The results of our model on this subset are given in Table  3. \nOne can see that for the stereo-enriched compounds the performance drops. We have inspected the most com-\nmon mistakes and saw that the typical errors are in stereo tokens indeed. It is interesting, that in most cases the \nFigure 7.  An example of a molecule with four correctly generated IUPAC names.\n7\nVol.:(0123456789)Scientific Reports |        (2021) 11:14798  | https://doi.org/10.1038/s41598-021-94082-y\nwww.nature.com/scientificreports/\nmodel tries to vary stereo configurations during the beam search for stereo-enriched compounds. We want to \nstress that this is the most challenging stereo-compounds from the whole 50M test set and we believe that the \ndemonstrated performance is good regarding the complexity of these compounds.\nAnother question is the processing of chemical tautomers. If we consider typical types of tautomerism (e.g., \nketo-enol tautomerism, enamine–imine, etc.), the tautomeric forms are represented by different canonical \nSMILES and have different IUPAC names. We revealed that our model processes tautomers well, probably \nbecause PubChem has a diverse set of tautomeric forms of compounds. The predictions for various tautomers \nof guanine and uracil are given in Table 2.\nTable 2.  The generated IUPAC names for various tautameric forms of Guanine and Uracil.\nMolecule SMILES Image IUP AC names\nGuanine\nN=c1nc(O)c2[nH]cnc2[nH]1\n 2-Imino-3,7-dihydropurin-6-ol \n2-imino-1,7-dihydropurin-6-ol\nNc1nc(=O)c2nc[nH]c2[nH]1\n 2-Amino-3,9-dihydropurin-6-one\nNc1nc(=O)c2[nH]cnc2[nH]1\n2-Amino-3,7-dihydropurin-6-one \n 2-amino-6,7-dihydro-3H-purin-6-one\n2-amino-3,6-dihydropurin-6-one\n2-amino-7H-purin-6-one\nNc1[nH]c(=O)c2[nH]cnc2n1\n 2-Amino-1,7-dihydropurin-6-one\n2-amino-1,6-dihydropurin-6-one\nNc1nc(O)c2[nH]cnc2n1\n 2-Amino-7H-purin-6-ol\n2-aminopurin-6-ol\nUracil\nO=c1cc[nH]c(=O)[nH]1\n 1H-Pyrimidine-2,4-dione\nOc1ccnc(O)n1\n Pyrimidine-2,4-diol\nO=c1ccnc(O)[nH]1\n 2-Hydroxy-1H-pyrimidin-6-one\n2-hydroxypyrimidin-6-one\nOc1cc[nH]c(=O)n1\n 4-Hydroxy-1H-pyrimidin-2-one\n 4-hydroxypyrimidin-2-one\nTable 3.  The performance of Struct2IUPAC model for different validation tasks.\nTask Beam 1 (%) Beam 3 (%) Beam 5 (%)\nKekule representation 95.6 97.4 97.7\nAugmented SMILES 27.49 34.00 37.16\nStereo-enriched 44.11 61.24 66.52\n8\nVol:.(1234567890)Scientific Reports |        (2021) 11:14798  | https://doi.org/10.1038/s41598-021-94082-y\nwww.nature.com/scientificreports/\nIt is interesting to track the behavior of Transformer outside the applicability domain. Our observations \nrevealed that the performance of Transformer drops down with large molecules. In the range from 200 to 300 \ntokens there are two common types of mistakes. The first one is the situation when the model loses the opening \nor closing squared bracket. It fails the whole structure due to a lexical mistake. That means that the model is \nundertrained on such an extra-size molecules. This behavior was expected because there were small amount of \nvery large molecules in the training set. The second typical case is losing a part of a large molecule. In this case, \nTransformer generates a chemically valid molecule, albeit that is shorter than the original. However, Transformer-\nbased models are known for ability to work with thousands-long sequences, and we suppose, that with enough \nlarge samples in a dataset, Transformer can achieve good performance on extra large molecules too.\nDespite the fact that the accuracy of the model does not exceed 50% on very large molecules, we found the \ninteresting examples of complex molecules for which IUPAC names were correctly generated (Fig. 9).\nConclusions\nIn this paper, we propose the Transformer-based solution for generation of IUPAC chemical names using cor-\nresponding SMILES notations. This model achieved 98.9% accuracy on the test set from PubChem. Also, the \nmodel reached close to 100% accuracy within 10 to 60 tokens length range. Our reverse model (IUPAC2Struct) \nreached accuracy 99.1%, which is comparable to open-source OPSIN software. We demonstrated that the com-\nputation time is generally applicable for using this approach in production. We showed that our model operates \nwell within a wide range of molecule size. Our research inspires a new paradigm for software development . We \ndemonstrated that one can replace a complex rule-based solution with modern ”heavy” neural architectures. \nWe believe that neural networks can now solve a wide range of so-called ” exact” problems (problems for which \nan exact algorithm or solution exists or may exist) with comparable performance. We believe that many groups \nof researchers and software developers can use and validate this idea for other algorithmic-based challenges.\nFigure 8.  The distribution of the number of names variations using a Transformer’s beam search.\nFigure 9.  Two examples of challenging molecules for which Transformer generates correct names.\n9\nVol.:(0123456789)Scientific Reports |        (2021) 11:14798  | https://doi.org/10.1038/s41598-021-94082-y\nwww.nature.com/scientificreports/\nData availability\nThe data is located on Zenodo (https:// doi. org/ 10. 5281/ zenodo. 42808 14). It contains a subset of 100,000 chemical \ncompounds that were used for testing Transformer, a subset of compounds on which OPSIN fails and compounds \non which our IUPAC2Smiles model fails. Also, the 100,000 subset is placed on GitHub repository: https:// github. \ncom/ sergsb/ IUPAC 2Stru ct with the IUPAC2Struct Transformer code.\nModel availability\nOur Struct2IUPAC model is available for the community on Syntelly platform: https:// app. synte lly. com/ smile s2iup \nac—text interface only for testing SMILES to IUPAC model, https:// app. synte lly. com/ indiv idual—graphical user \ninterface for the prediction of properties of organic compounds and IUPAC names. Transformer and pretrained \nIUPAC2Struct model are available on GitHub: https:// github. com/ sergsb/ IUPAC 2Stru ct.\nReceived: 10 February 2021; Accepted: 6 July 2021\nReferences\n 1. Nomenclature of Organic Chemistry: IUPAC Recommendations and Preferred Names 2013 (Royal Society of Chemistry, Cambridge, \n2013).\n 2. Eller, G. A. Improving the quality of published chemical names with nomenclature software. Molecules (Basel, Switzerland) 11, \n915–928. https:// doi. org/ 10. 3390/ 11110 915 (2006).\n 3. Garfield, E. Chemico-linguistics: Computer translation of chemical nomenclature. Nature 192, 192. https:// doi. org/ 10. 1038/ 19219 \n2a0 (1961).\n 4. Cannon, E. O. New benchmark for chemical nomenclature software. J. Chem. Inf. Model. 52, 1124–1131. https:// doi. org/ 10. 1021/ \nci300 0419 (2012).\n 5. Lowe, D. M., Corbett, P . T., Murray-Rust, P . & Glen, R. C. Chemical name to structure: OPSIN, an open source solution. J. Chem. \nInf. Model. 51, 739–753. https:// doi. org/ 10. 1021/ ci100 384d (2011).\n 6. Vaswani, A. et al. Attention is all you need. arXiv: 1706. 03762 [cs] (2017).\n 7. Sutskever, I., Vinyals, O. & Le, Q. V . Sequence to sequence learning with neural networks. arXiv: 1409. 3215 [cs] (2014).\n 8. Xu, T. et al. Neural machine translation of chemical nomenclature between English and Chinese. J. Cheminform.  12, 50. https:// \ndoi. org/ 10. 1186/ s13321- 020- 00457-0 (2020).\n 9. Krasnov, L., Khokhlov, I., Fedorov, M. & Sosnin, S. Struct2iupac—transformer-based artificial neural network for the conversion \nbetween chemical notations. https:// doi. org/ 10. 26434/ chemr xiv. 13274 732. v1 (2020).\n 10. Rajan, K., Zielesny, A. & Steinbeck, C. STOUT: SMILES to IUPAC names using neural machine translation. J. Cheminformatics  \n13, 1–14. https:// doi. org/ 10. 1186/ s13321- 021- 00512-4 (2021).\n 11. Omote, Y ., Matsushita, K., Iwakura, T., Tamura, A. & Ninomiya, T. Transformer-based approach for predicting chemical compound \nstructures. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the \n10th International Joint Conference on Natural Language Processing, 154–162 (Association for Computational Linguistics, Suzhou, \nChina, 2020).\n 12. Kim, S. et al. PubChem 2019 update: improved access to chemical data. Nucleic Acids Res. 47, D1102–D1109. https:// doi. org/ 10. \n1093/ nar/ gky10 33 (2019).\n 13. Brown, T. B. et al. Language models are few-shot learners. arXiv: 2005. 14165 [cs] (2020).\n 14. Schwaller, P . et al. Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. ACS Cent. Sci.  5, \n1572–1583. https:// doi. org/ 10. 1021/ acsce ntsci. 9b005 76 (2019).\n 15. Karpov, P ., Godin, G. & Tetko, I. V . Transformer-CNN: Swiss knife for QSAR modeling and interpretation. J. Cheminform. 12, 17. \nhttps:// doi. org/ 10. 1186/ s13321- 020- 00423-w (2020).\n 16. Chithrananda, S., Grand, G. & Ramsundar, B. ChemBERTa: Large-scale self-supervised pretraining for molecular property predic-\ntion. arXiv: 2010. 09885 [physics, q-bio] (2020).\n 17. Tetko, I. V ., Karpov, P ., Deursen, R. V . & Godin, G. State-of-the-art augmented NLP transformer models for direct and single-step \nretrosynthesis. Nat. Commun. 11, 1–11. https:// doi. org/ 10. 1038/ s41467- 020- 19266-y (2020).\n 18. Zacharov, I. et al. “Zhores”—Petaflops supercomputer for data-driven modeling, machine learning and artificial intelligence \ninstalled in Skolkovo Institute of Science and Technology. Open Eng. 9, 512–520. https:// doi. org/ 10. 1515/ eng- 2019- 0059 (2019).\nAcknowledgements\nThe authors acknowledge the use of Skoltech’s Zhores GPU  cluster18 for obtaining the results presented in this \npaper. The authors are thankful to Arkadiy Buianov for the help in creation of the web-demo. The authors \nacknowledge Daniel Lowe for the fruitful discussion and valuable comments and suggestions. We used Figma \n(https:// www. figma. com/) to create all drawings in this paper.\nAuthor contributions\nL.K., I.K., and S.S. performed the experiments described in this paper and conducted the analyses. I.K. imple -\nmented Transformer and trained the models. M.V .F . and S.S. initially designed the study. S.S. and L.K. wrote \nthe draft of the manuscript, I.K. and M.V .F . revised it. S.S. supervised the project. All authors read and approved \nthe final paper.\nCompeting interests \nThe authors declare the following competing interests: Maxim Fedorov and Sergey Sosnin are co-founders of \nSyntelly LLC. Lev Krasnov and Ivan Khokhlov are employees of Syntelly LLC. The authors are going to integrate \nthe functionality described in this paper to Syntelly online platform.\nAdditional information\nCorrespondence and requests for materials should be addressed to S.S.\nReprints and permissions information is available at www.nature.com/reprints.\n10\nVol:.(1234567890)Scientific Reports |        (2021) 11:14798  | https://doi.org/10.1038/s41598-021-94082-y\nwww.nature.com/scientificreports/\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2021",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7837371826171875
    },
    {
      "name": "Artificial neural network",
      "score": 0.7038723826408386
    },
    {
      "name": "Transformer",
      "score": 0.6934918165206909
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.681275486946106
    },
    {
      "name": "Notation",
      "score": 0.679867148399353
    },
    {
      "name": "Computation",
      "score": 0.6321750283241272
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5300129652023315
    },
    {
      "name": "Rule-based system",
      "score": 0.4474138617515564
    },
    {
      "name": "Machine learning",
      "score": 0.3826925754547119
    },
    {
      "name": "Algorithm",
      "score": 0.1278260052204132
    },
    {
      "name": "Engineering",
      "score": 0.07951560616493225
    },
    {
      "name": "Arithmetic",
      "score": 0.07579472661018372
    },
    {
      "name": "Chemistry",
      "score": 0.07409864664077759
    },
    {
      "name": "Electrical engineering",
      "score": 0.06920996308326721
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I125989756",
      "name": "Skolkovo Institute of Science and Technology",
      "country": "RU"
    },
    {
      "id": "https://openalex.org/I4210115303",
      "name": "Moscow University «Synergy»",
      "country": "RU"
    }
  ]
}