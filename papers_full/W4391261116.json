{
  "title": "Multi-task approach based on combined CNN-transformer for efficient segmentation and classification of breast tumors in ultrasound images",
  "url": "https://openalex.org/W4391261116",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5093797222",
      "name": "Jaouad Tagnamas",
      "affiliations": [
        "Sidi Mohamed Ben Abdellah University"
      ]
    },
    {
      "id": "https://openalex.org/A2385891081",
      "name": "Hiba Ramadan",
      "affiliations": [
        "Sidi Mohamed Ben Abdellah University"
      ]
    },
    {
      "id": "https://openalex.org/A215346023",
      "name": "Ali Yahyaouy",
      "affiliations": [
        "Sidi Mohamed Ben Abdellah University"
      ]
    },
    {
      "id": "https://openalex.org/A148402391",
      "name": "Hamid Tairi",
      "affiliations": [
        "Sidi Mohamed Ben Abdellah University"
      ]
    },
    {
      "id": "https://openalex.org/A5093797222",
      "name": "Jaouad Tagnamas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2385891081",
      "name": "Hiba Ramadan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A215346023",
      "name": "Ali Yahyaouy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A148402391",
      "name": "Hamid Tairi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2781525129",
    "https://openalex.org/W2889646458",
    "https://openalex.org/W4387597699",
    "https://openalex.org/W2123727507",
    "https://openalex.org/W3023384820",
    "https://openalex.org/W3087507349",
    "https://openalex.org/W2922512202",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W4383710235",
    "https://openalex.org/W4318566866",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W4387778010",
    "https://openalex.org/W6600213211",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2991372685",
    "https://openalex.org/W4281665821",
    "https://openalex.org/W3009175488",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3049209383",
    "https://openalex.org/W2892923422",
    "https://openalex.org/W3035148740",
    "https://openalex.org/W2804525729",
    "https://openalex.org/W3044261867",
    "https://openalex.org/W3160439140",
    "https://openalex.org/W4306393227",
    "https://openalex.org/W4321794144",
    "https://openalex.org/W4313830783",
    "https://openalex.org/W2125063860",
    "https://openalex.org/W2072965986",
    "https://openalex.org/W2132569563",
    "https://openalex.org/W2982092517",
    "https://openalex.org/W2740028789",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3005546201",
    "https://openalex.org/W2906785117",
    "https://openalex.org/W2809348156",
    "https://openalex.org/W4308119975",
    "https://openalex.org/W4284972641",
    "https://openalex.org/W4317474370",
    "https://openalex.org/W4323074058",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6828894009",
    "https://openalex.org/W2744692634",
    "https://openalex.org/W4313531815",
    "https://openalex.org/W2735039185",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W3204614423",
    "https://openalex.org/W4319764655",
    "https://openalex.org/W4386873870",
    "https://openalex.org/W4226085666",
    "https://openalex.org/W3105636206",
    "https://openalex.org/W4213099919"
  ],
  "abstract": null,
  "full_text": "Tagnamas et al. \nVisual Computing for Industry, Biomedicine, and Art  (2024) 7:2 \nhttps://doi.org/10.1186/s42492-024-00155-w\nORIGINAL ARTICLE Open Access\n© The Author(s) 2024, corrected publication 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 \nInternational License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To \nview a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nVisual Computing for Industry,\nBiomedicine, and Art\nMulti-task approach based on combined \nCNN-transformer for efficient segmentation \nand classification of breast tumors in ultrasound \nimages\nJaouad Tagnamas1*  , Hiba Ramadan1, Ali Yahyaouy1 and Hamid Tairi1 \nAbstract \nNowadays, inspired by the great success of Transformers in Natural Language Processing, many applications of Vision \nTransformers (ViTs) have been investigated in the field of medical image analysis including breast ultrasound (BUS) \nimage segmentation and classification. In this paper, we propose an efficient multi-task framework to segment \nand classify tumors in BUS images using hybrid convolutional neural networks (CNNs)-ViTs architecture and Multi-\nPerceptron (MLP)-Mixer. The proposed method uses a two-encoder architecture with EfficientNetV2 backbone \nand an adapted ViT encoder to extract tumor regions in BUS images. The self-attention (SA) mechanism in the Trans-\nformer encoder allows capturing a wide range of high-level and complex features while the EfficientNetV2 encoder \npreserves local information in image. To fusion the extracted features, a Channel Attention Fusion (CAF) module \nis introduced. The CAF module selectively emphasizes important features from both encoders, improving the integra-\ntion of high-level and local information. The resulting feature maps are reconstructed to obtain the segmentation \nmaps using a decoder. Then, our method classifies the segmented tumor regions into benign and malignant using \na simple and efficient classifier based on MLP-Mixer, that is applied for the first time, to the best of our knowledge, \nfor the task of lesion classification in BUS images. Experimental results illustrate the outperformance of our framework \ncompared to recent works for the task of segmentation by producing 83.42% in terms of Dice coefficient as well \nas for the classification with 86% in terms of accuracy.\nKeywords Breast Ultrasound segmentation and classification, Breast tumors, Convolutional Neural Networks, Self-\nAttention, MLP-Mixer, Channel Attention\nIntroduction\nBreast cancer is considered the most common can -\ncer and the second leading cause of cancer-related \nmortality in women [1 ]. The International Agency for \nResearch on Cancer performed a study [2 ] and found \nthat nearly 2.1 million new breast cancer cases and over \nhalf a million new deaths were reported globally dur -\ning 2018. Breast ultrasound (BUS) imaging is emerg -\ning as a complementary screening method for women \nand can be used as a diagnostic method for breast can -\ncer [3 ]. Early detection and diagnosis of breast tumors \ncan reduce the mortality rate. Therefore, BUS remains \na cheap and safe technique that can be executed using \nportable devices at the patient’s bedside and is acces -\nsible globally [4 ]. However, diagnosis using BUS \nrequires probes that depend heavily on the operator [5 ]. \nMoreover, two or three volumes are acquired for each \nbreast per examination, which results in radiologists \n*Correspondence:\nJaouad Tagnamas\njaouad.tagnamas@usmba.ac.ma\n1 Department of Informatics, Faculty of Sciences Dhar El Mahraz, \nUniversity of Sidi Mohamed Ben Abdellah, 30000 Fez, Morocco\nPage 2 of 15Tagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\nand clinicians spending an inordinate amount of time \nreviewing large volumes of BUS images and making \naccurate disease diagnoses [6 ]. In addition, handheld \nprobes are highly sensitive instruments, which makes \nthem susceptible to capturing noise in addition to the \nultrasonic images; consequently, it is difficult to prop -\nerly perform the diagnosis process [7 ]. Hence, there is \nan urgent need to develop structured and intelligent \nsystems to help medical professionals diagnose breast \ntumors with high accuracy. However, developing such \nsystems is challenging because of the high similarity \nbetween benign and malignant lesions, irregular tumor \nboundaries, and the various sizes of shapes and sized \nfor lesions.\nDeep learning algorithms have recently been applied \nin several research domains including the medical \nimaging field. In recent years, computer vision tasks \nsuch as segmentation, classification, and detection have \nbeen performed using convolutional neural networks \n(CNNs), where they have obtained state of the art \n(SOTA) results and remain the most commonly used \nnetworks in medical imaging analysis applications, par -\nticularly UNet architectures [8 ]. Despite their popular -\nity, the primary limitation of CNNs is that they learn \ninformation from images using localized receptive \nfields, which causes their learning capabilities to fail \nwhen capturing long-range dependencies [9 ]. Owing \nto the great success of transformers [10] in natural lan -\nguage processing, great attention has been paid to self-\nattention (SA) mechanism-based architectures in many \ncomputer vision tasks [11] to improve their nonlocal \nmodeling capability, as they are not subject to the limi -\ntations of CNN architectures [12].\nRecently, vision transformer (ViT), which is a trans -\nformer for vision applications [13], have been inves -\ntigated in medical image analysis and have achieved \nSOTA results for many tasks including organ and \ntumor segmentation as well as disease detection and \nclassification [14]. For medical image segmentation, \ntwo designs that employ transformers have been pro -\nposed in literature: pure transformers-based mod -\nels and hybrid models. The first category is U-shaped \nmodels built upon ViTs or its variants without any \nconvolutional layers. This allows the learning of long-\nrange semantic information, in contrast to CNN-based \narchitectures. An example is Swin-Unet [15], which is \na purely transformer-based method. The second fam -\nily is hybrid models that modify the encoder-decoder \narchitecture by replacing either the encoder or decoder \nmodule with a transformer [16]. An example is Tran -\nsUNet [17], which has demonstrated good performance \nbecause its ability to capture long-range dependency \nowing to the SA mechanism of transformers, as well as \npreserving low-level details owing to the intrinsic local -\nity of the convolution operations.\nIn addition to medical image segmentation, another \nchallenging problem in medical imaging is classifying \ninput images or regions of interest (ROI) in these images \ninto meaningful categories. In addition to CNNs, ViTs \nhave been successfully applied to medical image recog -\nnition and classification [14]; and a competitive alterna -\ntive called multilayer perceptron (MLP)-Mixer has been \nproposed by Tolstikhin et al. [18] to perform image clas -\nsification using exclusive MLPs without convolutions or \nattention blocks. The experiments reported in ref. [18] \ndemonstrated that MLP-Mixer is built upon a simple \narchitecture and produces comparable results to SOTA \nclassifiers, while achieving a good compromise between \nthe accuracy and computational resources required for \ntraining and inference.\nMotivated by the great success of ViTs in medical \nimage analysis and in particular in the task of segmen -\ntation and classification, and inspired by the works in \nTransUNet [17] and MLP-Mixer [18], we propose an \nefficient multi-task framework that performs sequential \nBUS tumor segmentation and tumor type classification. \nOur framework contains two main steps: first, segmen -\ntation of the tumor region, which helps in focusing only \non the features of that part of the image; and second, \nclassification of the extracted lesion region into two \nclasses: malignant and benign. To perform the segmen -\ntation task, which was inspired by TransUNet [17], we \npropose an encoder-decoder-based model using a modi -\nfied U-Net architecture. The encoder is composed of \ntwo encoders, where the images are passed in parallel to \nboth efficientNetV2 [19] and an adapted ViTs encoder to \nextract enriched features and context information at dif -\nferent scales. To combine the feature maps extracted by \nboth the encoders, we design a channel attention fusion \n(CAF) module that incorporates a squeeze-and-excita -\ntion (SE) block [20] for channel attention. The SE block \nselectively emphasizes the informative features from \nboth encoders, facilitating the integration of high-level \nand local information. The attention mechanism within \nthe CAF module enables effective feature combinations. \nThe combined features using the CAF module consti -\ntute the input to the decoder, where the mask image is \nreconstructed using skip connections from the efficient -\nNetV2 encoder. In addition to the power of the ViTs \nencoder, we opted to use efficientNetV2 [19] instead of \na CNN encoder, because efficientNetV2 uses a technique \ncalled compound scaling to increase the depth, width, \nand resolution of the network in a balanced manner. This \nallows efficientNetV2 to capture more context from the \nimage and produce more accurate segmentation, and \nit is designed in a way that uses fewer parameters and \nPage 3 of 15\nTagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\n \nfewer computational resources as compared to other \nnetworks. In the second step, a robust tumor classifier is \nproposed by testing both the ViTs architecture and MLP-\nMixer model. ViTs are exploited to leverage the capabili -\nties of the SA mechanism for accurate detection of BUS \ntumors. In addition, we explore a pre-trained MLP-Mixer \nmodel that depends solely on MLPs to classify seg -\nmented tumor regions. We investigate the performance \nof the latter model in comparison with that of the ViT \nmodel. Specifically, we fine-tune the BUSI dataset [21] \non the pre-trained models of both ViTs and MLP-Mixer \nafter segmenting the lesion regions from the images. \nThe results obtained underscore the great capabilities of \nclassifying the images when using the ViT architecture, \ndepending on the attention mechanism, or the MLP-\nMixer relying exclusively on MLPs. In summary, the pro -\nposed method contributes to literature as follows:\n1. We propose an efficient multi-task framework for \nBUS segmentation and classification. We leverage the \nstrengths of both efficientNetV2 and adapted ViTs \nencoders, extracting enriched features and context \ninformation at different scales.\n2. We design a CAF module based on the SE block \nfor effective feature combination between the dual \nencoders. Specifically, the module selectively empha -\nsizes important features from both encoders, improv-\ning the integration of high-level and local informa -\ntion.\n3. We leverage the MLP-Mixer model that depends \nsolely on MLPs to perform BUS images classifica -\ntion. The latter is being used for the first time for this \ntask, to the best of our knowledge. We demonstrate \nthe capabilities of both ViTs and MLP-Mixer in accu-\nrately classifying BUS images, with ViTs relying on \nattention mechanisms and MLP-Mixer relying exclu-\nsively on MLPs.\nComputer-aided diagnosis (CAD) systems are increas -\ningly utilized to aid healthcare professionals in diagnos -\ning various diseases and cancers, including breast cancer. \nTasks such as the detection, segmentation, and classifi -\ncation of tumor regions in BUS are largely addressed in \nCAD systems.\nBUS segmentation\nPrevious studies have described various CNN-based \nmethods for breast mass segmentation. Vigil et  al. [22] \npresented an architecture based on a deep convolu -\ntional autoencoder to extract latent-space features for \nsegmenting BUS images. Xing et  al. [23] utilized a gen -\nerative adversarial network (GAN) and a CNN based on \nResNet [24] as generators for tumor region segmentation \nto form a semi-pixel-wise cycle model. Singh et  al. [25] \nsegmented BUS tumors using a context-aware network \nbased on atrous convolutions, where GAN was utilized \nto evaluate the performance of the segmentation. Lei \net  al. [26] proposed a network that performs boundary \nregularization to segment BUS images. In addition, Lei \net  al. [27] improved the segmentation of breast struc -\nture results using the self-co-attention technique. Kumar \net  al. [28] introduced a U-shaped architecture called \nmulti-UNet to perform the segmentation of BUS images. \nIn ref. [29], an architecture where attention blocks are \nincorporated into the U-Net architecture was proposed. \nTong et al. [30] introduced a modified U-Net architecture \nbased on a mixed attention loss function to segment BUS \ntumors. Cao et  al. [15] created Swin-UNet by substitut -\ning the convolutional encoding and decoding operations \nof U-Net with a swine transformer module. Chen et  al. \n[17] proposed TransUNet, which implements CNNs to \nextract features and subsequently feeds them directly to \na transformer to capture richer features. Based on the \nTransUNet backbone, Yang HN and Yang DP [31] com -\nbined CNN and swine transformer blocks for feature \nextraction as an encoder in a pyramid-shaped network \nfor BUS segmentation. Recently, Al-Battal et  al. [32] \nproposed a weakly trained U-shaped segmentation net -\nwork with an encoder and a multipath decoder, where \nthe latter provides more loss propagation from feature \nmaps to deeper layers and the encoder, as well as efficient \nupsampling of feature maps that leads to the preserva -\ntion of high-resolution information. Farooq et  al. [33] \nproposed a semi-supervised mean teacher and student \nmodel that utilizes the U-Net model with residual and \nattention blocks as a backbone network for BUS image \nsegmentation.\nBUS classification\nIn literature, methods classify BUS images can rely and \nuse the manual extraction of different type of features like \nshape, texture, lesion borders, margin and orientation. \nIn this context, Moon et  al. [34] relied on a mixture of \nfeatures extracted from ultrasound images, composed of \ntexture, morphological, and descriptor features, to clas -\nsify tumors. Flores et al. [35] relied on the use of distinct \nmorphological and textual spatial information to perform \nclassification tasks. Similarly, Gómez et al. [36] extracted \n22 morphological features after applying the water -\nshed transformation technique to segment BUS images, \nwhere feature selection was performed using the mini -\nmum-redundancy-maximal-relevance criteria. Tanaka \net  al. [37] suggested the implementation of a CAD sys -\ntem based on CNNs to differentiate between benign and \nmalignant breast lesions in ultrasound images. The data -\nset consisted of more than 1000 images, and the reported \nPage 4 of 15Tagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\naccuracy of the system was 89%. Han et  al. [38] trained \nGoogleNet [39] using a dataset that included 7408 ultra -\nsound images, consisting of 4254 benign and 3154 malig -\nnant lesions, with an accuracy of approximately 0.9, a \nsensitivity of 0.86, and a specificity of 0.96. Wang et  al. \n[40] suggested a CNN architecture based on a modified \nInceptionv3 architecture to effectively extract features \nfrom BUS images. Byra et  al. [41] used transfer learn -\ning to retrain pre-trained models, mainly VGG19, on \nan ultrasound image dataset after applying the rescaling \nlayer to the image pixels, which aimed to convert them \nto an RGB representation. Xiao et al. [42] examined the \neffectiveness of transfer learning using the InceptionV3, \nXception, and ResNet50 models on an ultrasound data -\nset. Ayana and Choe [43] investigated the effectiveness \nof ViT for classifying BUS images by introducing a novel \nmethod for transfer learning. In ref. [44], an architecture \n(SAFNet) was proposed that combines ResNet-18 and \na spatial attention mechanism to form a backbone for \nfeature extraction. Zhong et al. [45] developed a feature \nfusion network called MsGoF to classify BUS tumors \nas malignant or benign. Sirjani et al. [46] classified BUS \ntumors using a modified InceptionV3 network in which \nthey adjusted the number of residual modules and other \nhyperparameters.\nThe proposed method bridges a significant gap \nbetween existing research methods and introduces con -\ntributions. While previous methods for both segmenta -\ntion and classification predominantly relied on CNNs \nor transformers for BUS image analysis, the proposed \nmethod combines the strengths of both architectures. \nThis integration allows the capture of fine-grained \nspatial details through CNNs and models the global \ncontext and long-range dependencies through trans -\nformers. Additionally, the proposed method addresses \nthe limitations of TransUNet, which is one of the few \nmethods that incorporates both CNNs and transform -\ners, by introducing an efficientNetV2 encoder and a CAF \nmodule. These enhancements improve the understand -\ning of complex spatial relationships, facilitate effective \nfeature combinations, and enhance segmentation and \nclassification accuracy. Furthermore, the exploration of \nViTs and MLP-Mixer models as alternative classifica -\ntion approaches adds novelty, offering insights into the \neffectiveness of attention mechanisms and MLPs in BUS \nimage classification.\nMethods\nIn this study, we propose an efficient multi-task frame -\nwork for segmenting and classifying tumors from BUS \nimages. The proposed approach performs two tasks dur -\ning two main phases. First, the segmentation architec -\nture was trained on BUS images and their corresponding \nmasks to extract the tumor region from its surroundings \nin the image. Secondly, the proposed approach enhances \nits performance by exploiting the potential of transform -\ners, in which an adapted ViT model was employed for \nthe classification of the segmented tumor region. Addi -\ntionally, we investigated the MLP-Mixer to capitalize \non its ability to classify BUS images by relying solely on \nMLP blocks without the need for high computational \nresources for training and inference. Figure  1 shows an \noverview of the proposed framework.\nSegmentation architecture\nFigure 2 shows the proposed model for the segmentation \ntask, which comprises two parallel encoders, each with \ndistinct characteristics. The first encoder is built on the \nEfficientNetV2-L backbone, which serves as the founda -\ntion for feature extraction from the input image instead \nFig. 1 Flow chart of our contribution. (a) The proposed architecture to perform the segmentation of BUS images; (b) The proposed method \nto classify the segmented BUS images\nPage 5 of 15\nTagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\n \nof a conventional set of convolution layers. To accomplish \nthis, several layers within the blocks of the backbone are \nemployed as feature extractors to capture diverse aspects \nof image information. In addition, using EfficientNetV2-\nL [19] as an encoder allows the network to require fewer \ncomputational resources than other commonly used \nCNN models. Furthermore, EfficientNetV2-L incorpo -\nrates both MBConv and fused-MBConv, enabling it to \ncapture more diverse and informative feature maps from \nthe images. Therefore, using efficientNet as the back -\nbone of the encoder can lead to more accurate segmen -\ntation results, particularly for the studied segmentation \ntasks. We investigated different combinations of blocks \nto construct the encoder from EfficientNetV2-L with \npre-trained ImageNet [47] weights, where blocks that are \ncloser to the input image tend to capture low-level fea -\ntures, including textures, edges, and patterns, whereas \nthe deep blocks in the network contribute to learning \nhigher-level semantic features. For further details on the \nadvantages of EfficientNetV2 compared to previous CNN \narchitectures, please refer to ref. [19]. Table 1 lists the lay-\ners of EfficientNetV2-L, which was used as the backbone \nof the first encoder. Concurrently, the second encoder \noperates based on a transformer architecture and func -\ntions similar to the original ViT [ 13] except for the \ninput image resolution, projection dimension, number \nof multi-head-self-attention (MSA) heads, and number \nof transformer layers, as shown in Table  2. The encoder \nextracts global deep features from the input image by \nleveraging the SA mechanism of the transformer model, \nthereby providing a complementary representation of \nthe image features. The first and second encoders out -\nput feature maps with dimension (8  × 8 × 3840) and \nH\nP × W\nP × d  , respectively, where H, W, and P repre -\nsent the height, width, and patch size of the input image, \nrespectively, and d represents the output feature vector.\nTo enhance the fusion of the feature maps extracted \nby the efficientNetV2 and ViTs encoders, we propose \na CAF module. The CAF module incorporates an SE \nblock [ 20] that enables efficient channel attention. The \nCAF module uses feature maps from both encoders as \nFig. 2 The proposed architecture to perform the segmentation of BUS images\nTable 1 EfficientNetV2-L layers used as components for the CNN \nencoder\nNumber Layer Output shape\n1 input 1 256 × 256 × 3\n2 block1d_project_activation 128 × 128 × 32\n3 block2g_expand_activation 64 × 64 × 256\n4 block4a_expand_activation 32 × 32 × 384\n5 block6a_expand_conv 16 × 16 × 1344\nTable 2 Our adapted Transformer parameters used in the \nencoder\nViT version Image \nresolution\nProjection \ndimension\nNumber of \nMSA heads\nNumber of \ntransformers \nlayers\nViT-base 224 × 224 768 12 12\nOur adapted \ntransformer\n256 × 256 64 8 12\nPage 6 of 15Tagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\ninputs. First, to ensure compatibility, the dimensions of \nthe feature maps from the efficientNetV2 encoder are \nadjusted using a 1 × 1 convolutional layer to match the \nshapes of the feature maps from the ViTs encoder. Next, \nchannel attention is applied to both sets of feature \nmaps. This is achieved by passing the adjusted feature \nmaps from the efficientNetV2 encoder and the feature \nmaps from the ViTs encoder through the SE block. The \nSE block performs global average pooling on the input \nfeature maps to obtain the global channel descriptors. \nThese descriptors are then passed through two dense \nlayers. The first dense layer reduces the dimensional -\nity of the descriptors by a factor determined by the \nspecified ration = 8 . The second dense layer applies the \nsigmoid activation function to generate channel-wise \nattention weights. The obtained attention weights are \nmultiplied element-wise using their respective feature \nmaps. This process selectively emphasizes the informa -\ntive features in the feature maps of each encoder, guided \nby the attention weights. Finally, the fused features are \nobtained by element-wise addition of the adjusted fea -\nture maps from the efficientNetV2 encoder multiplied \nby the attention weights from the ViTs encoder, and \nthe feature maps from the ViTs encoder multiplied by \nthe attention weights from the adjusted feature maps. \nThis fusion process enables the integration of high-level \nand local information from both encoders. The CAF \nmodule improves the feature fusion process, allowing \na more effective combination of complementary infor -\nmation captured by the efficientNetV2 and ViTs encod -\ners. By selectively emphasizing important features and \nintegrating them at the channel level, the CAF module \nenhances the overall representation and discriminative \npower of the fused features. By adaptively recalibrating \nthe importance of different channels, the CAF module \nencourages encoders to focus on the most discrimi -\nnative and relevant information for the segmentation \ntask. This regularization helps prevent the model from \noverfitting noisy or irrelevant features, thereby improv -\ning generalization. Consequently, it leads to a more \naccurate BUS segmentation and improves the perfor -\nmance of the multi-task framework. Figure  3 shows the \nstructure of the proposed CAF module.\nThe combined feature maps using the CAF mod -\nule serve as the input for the subsequent decoder stage, \nwhere the objective is to reconstruct the segmented \nimage. To enhance reconstruction quality, skip connec -\ntions from the first encoder are incorporated into the \ndecoding process. These skip connections transmit high-\nresolution spatial information from the first encoder’s \nearlier layers directly towards the corresponding decoder \nlayers, thereby mitigating the loss of fine-grained details \nduring the upsampling process. The resulting decoder \noutput yields a precise and accurate segmented image, \neffectively leveraging the strengths of both encoder \narchitectures.\nClassification architecture\nFigure 1b shows an overview of the classification of the \nsegmented breast tumor regions extracted from the BUS \nimages. Specifically, we leverage the strengths of the \nMLP-Mixer [18] model, which is a recently proposed \narchitecture for image classification tasks, to achieve an \naccurate classification of BUS tumors. The MLP-Mixer \nmodel combines MLPs and channel-mixing layers to \neffectively capture both local and global features in the \nimages. In addition to the promising quality of image \nclassification achieved by this model, to the best of our \nknowledge, this is the first study that utilizes the MLP-\nMixer model for the classification of BUS images. As \nshown in Fig.  1b, the tumor region is extracted by apply -\ning the segmented mask to the image and then resized to \n(224 × 224 × 3) to fit the input of the pre-trained MLP-\nMixer to obtain the final tumor class.\nThe architecture of the MLP-Mixer is similar to \nthat of the ViT, where the mixer block takes linearly \nFig. 3 Structure of CAF\nPage 7 of 15\nTagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\n \nprojected nonoverlapping patches from the input image. \nLet the input image x  be of size (H × W × C) and (P , P) \nbe the size of each patch; therefore, the image is split into \nS= HW /P2 patches. Furthermore, each patch is linearly \nprojected to form an input vector for the mixer block of \nsize (S × C ) . The mixer is composed of multiple identi -\ncally sized layers, with each layer comprising two MLP \nblocks. The first block (token-mixing MLP block) acts on \nthe transpose of x, which are the columns of the linear \nprojection table constructed from image X patches. Every \nrow contains the same channel information for all patches. \nThis is fed into a block of two fully connected layers. This \nblock identifies features in the image across patches and \naggregates all channels in which the feature occurs. More-\nover, the weights are shared in MLP 1. The second block \nis the channel-mixing MLP block, which operates on the \nrows after another transpose of x. In this phase, computa-\ntions are applied across all the channels of the patch. This \ninvolves searching for features only in that patch and asso-\nciating it with the channel, whereas in the token-mixing \nblock, it searches for features in all channels. The weights \nof the MLP blocks are shared across all rows. All the rows \nof the MLP block share the same weights. Each MLP block \nincludes two fully connected layers, which apply the GELU \nactivation function [48] independently to each row of \nthe input data. Equations  1 and 2 represent the layers of \nMLP-Mixer:\nwhere DS and DC are the tunable hidden widths of the \nchannel-and token-mixing MLPs, respectively. σ denotes \nthe GELU activation function. The computational com -\nplexity of the network is linear in the number of input \npatches S owing to the independent selection of DS from \nthe number of patches, preventing it from growing quad -\nratically, unlike ViTs. Furthermore, the model applies the \nsame MLP to every row and column of image x. This pre-\nvents the model from becoming overly complicated and \ngrowing too rapidly when the hidden dimension C or \nsequence length S is increased. This approach also results \nin significant memory saving. In addition to MLP lay -\ners, the mixer employs other conventional architectural \nelements, including skip connections and layer normali -\nzation [49]. Following the mixer block, a conventional \nclassification head that includes a global average pooling \nlayer is utilized. This is then succeeded by a linear classi -\nfier that produce the final predicted class output.\nIn addition to employing the MLP-Mixer as a classifier \nfor BUS images, we also investigate the performance of ViT \n(1)U ∗,i= X∗,i+ W 2 σ\n(\nW 1 LayerNorm(X )∗,i\n)\n, i= 1 ...C\n(2)Yj,∗= U j,∗+ W 4 σ\n(\nW 3 LayerNorm(U )j,∗\n)\n, j= 1 ... S\nfamily models. Specifically, we leverage these models to \nclassify BUS images. The ViT model captures global con -\ntextual information from the input image by employing the \nSA mechanism. Specifically, it enables the efficient extrac-\ntion of relevant features from segmented breast tumor \nregions.\nThe ViT’s transformer encoder receives a sequence of \nflattened, positionally encoded patches of the masked \nBUS image after resizing it to 224 × 224 × 3. An MSA and \nan MLP layer make up the transformer encoder module. \nThe MSA layer divides inputs into several heads, allowing \neach head to learn varying levels of SA. All head outputs \nare then combined and passed to the MLP to output the \nfinal class.\nResults and Discussion\nDatasets\nThis study involved the assessment of our approach using \ntwo publicly available BUS image datasets. Dataset 1 \n(BUSI) was provided online by Al-Dhabyani et al. [21] and \ncontains 600 BUS images of female patients in a total of 780 \nPNG format images and their corresponding masks with an \naverage image size of 500px × 500px. The 780 images were \nsplit into three classes: normal, benign, and malignant. The \nbenign class had 487 breast tumors, and the malignant \nclass had 210 images. In our work, we only used the malig-\nnant and benign. This is because in the first phase, we per-\nformed tumor region segmentation and the normal class \nimages did not have any tumor region. Dataset 2 (UDIAT) \n[50], which was gathered by the Diagnostic Center of the \nParc Taul ́ ı Corporation, Sabadell (Spain) contains 110 \nbenign and 53 malignant images totaling 163 BUS images \nand their corresponding masks, which were collected using \na Siemens ACUSON Sequoia C512 system 17L5 HD linear \narray transducer. By evaluating these datasets, we provide a \ncomprehensive analysis of the effectiveness and robustness \nof the proposed method. The utilization of publicly acces-\nsible datasets in our study ensures the reproducibility and \ngeneralizability of our findings and allows for comparison \nwith other related methods and techniques.\nEvaluation metrics\nTo assess the effectiveness of the segmentation models, we \nemployed the following widely used metrics: Dice coef -\nficient (DC), Jaccard index intersection over union (IoU), \nprecision, recall, sensitivity, specificity, and F1-score. By \nutilizing these common metrics, we could comprehensively \nevaluate the performance of the segmentation models and \ncompare our results with those of other studies in the field.\n(3)Accuracy= TP + TN\nTP + TN + FP + FN\nPage 8 of 15Tagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\nTo evaluate the segmentation results, the DC, IoU, \naccuracy, recall, and precision were used, whereas the \nmetrics accuracy, precision, recall, specificity, F1-score \nwere used to evaluate the classification results.\nImplementation details\nTo conduct the experiments, we utilized a fivefold cross-\nvalidation method for the same dataset partition. During \nthe training process, 80% of the images were used with \nthe remaining 20% for testing. The validation process was \nperformed using 20% of the training data. For the seg -\nmentation task, all the images were resized to 256 × 256 \npixels. The first encoder, based on the efficientNetV2-\nL backbone, was used with pre-trained ImageNet [47] \nweights, and the second encoder, based on MSA, was \ntrained from scratch. Various combinations of learning \nrates, batch sizes, and epochs were examined. The best \nresults were achieved using the Adam [51] optimizer \n(4)Precision= TP\nTP + FP\n(5)Recall= TP\nTP + FN\n(6)Speciﬁty= TN\nTN + FP\n(7)Sensitivity= TP\nTP + FP\n(8)F 1 − score= 2 × Precision× Recall\nPrecision+ Recall\n(9)DC = 2 × TP\n2 × TP + FP+ FN\n(10)IoU = TP\nTP + FP + FN\nwith an initial learning rate of  1e−4, β1 = 0.9, β2 = 0.999 \nand  1e−7, training for 200 epochs with a batch size of \neight, and early stopping. A range of data augmentation \ntechniques was employed on the training set, including \nrandom rotation and horizontal flips. The selection of \nthe loss function used in our experiments had a notable \neffect on the outcomes of our study. To overcome the \nchallenge of an imbalanced class distribution in the data -\nset, we employed a custom segmentation loss function \nthat combined the binary cross-entropy (BCE) and Dice \nloss. This combined loss function is denoted by LT and is \ndefined as follows:\nUsing this combined loss function, we were able \nto account for the class imbalance in the dataset and \nimprove the accuracy and reliability of our segmentation \nmodel. As for the classification phase we used two archi -\ntectures: MLP-Mixer [18] and ViT [13]. The two models \ntake the segmented part of the images that contains only \nthe tumor region and predicts its corresponding class. \nFigure 4 illustrates an example of the masked images fed \nto the classification models. The ViT model was trained \non masked images resulting from segmentation of the \ntumor region using the proposed segmentation model. \nThree versions of ViT model were trained with B/16, \nB/32, and L/32, using transfer learning. All models were \nfollowed by a flattening layer, batch normalization, a \ndropout of 0.6, a hidden layer of 11 neurons with GELU \nactivation, batch normalization, and an output layer with \nsigmoid activation. Similarly, we fine-tuned the versions \nof MLP-Mixer: Mixer-B/16 and Mixer-L/16, with the \nprovided pre-trained weights, where they were followed \n(11)\nLBCE =− 1\nN\n∑ N\ni=1\n[\nyilog(pi) +\n(\n1 − yi\n)\nlog(1 − pi)\n]\n(12)LDice = 1 − 2∑ N\ni=1yipi+ ǫ\n∑ N\ni=1\n(\nyi + pi\n)\n+ ǫ\n(13)LT = LDice+ LBCE\nFig. 4 The initial image, the segmentation and the resulted masked image\nPage 9 of 15\nTagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\n \nby the same architecture as ViT’s base model. We used \na BCE loss function, and the Adam optimizer with an \ninitial learning rate of  1e−4, β1 = 0.9, β2 = 0.999 and epsi -\nlon equal to  1e−7. The models were trained for 50 epochs \nwith a batch size of eight, and early stopping was enabled.\nBUS segmentation results and discussion\nIn the ablation study, we evaluated the performance \nof our segmentation model with and without the CAF \nmodule to assess the contribution of the CAF module \nin improving the segmentation results. Tables  3 and 4 \ndepict the ablation study results for the base model with -\nout the CAF module and the proposed method using the \nCAF module when trained and tested on the BUSI data -\nset [21], and trained on BUSI dataset [21] and tested on \nthe UDIAT dataset [50] respectively. For the base model \nwithout the CAF module, we observed that it achieved \ncompetitive segmentation performance with 81.94% in \nterms of the DC on BUSI dataset, accurately segmenting \ntumor regions in the BUS images. Similarly, upon inte -\ngrating the CAF module into the model, we observed a \nnotable improvement in the segmentation results. The \nCAF module effectively enhanced the feature fusion pro -\ncess with a 1.48% increase in the DC, allowing for bet -\nter integration of complementary information from the \nefficientNetV2 and ViTs encoders. We noticed a similar \nincrease in performance when training and testing on \nthe BUSI dataset as well as when training on the BUSI \ndataset and testing on the UDIAT dataset. This improved \nfeature fusion led to more precise and detailed segmen -\ntation boundaries, resulting in enhanced segmentation \naccuracy and overall performance. The ablation study \nresults demonstrate the significant benefit of incorpo -\nrating the CAF module, highlighting its effectiveness in \nimproving the segmentation capabilities of our multi-\ntask framework.\nWe compared the segmentation results with exist -\ning methods. We evaluated our segmentation results \nmainly using the results found in ref. [52], where a novel \napproach for the segmentation of BUS images was pro -\nposed. Ma et  al. [52] introduced a U-shaped architec -\nture called ATFE-Net, which they integrated into an \naxial-trans (axial transformer) to extract long-range \ndependencies, and a transformer-based feature enhance -\nment module (trans-FE) was used to capture the reli -\nance between different layers at different depths of the \nnetwork. In their work, they evaluated their proposed \nmethod on the two available BUS datasets: BUSI and \nUDIAT. Furthermore, we conducted the training process \nof our suggested segmentation model in accordance with \ntheir prescribed methodology, ensuring identical training \nconditions and fair evaluation for comparative purposes. \nThey evaluated their findings using the following SOTA \nmethods: TransUNet [17], LinkNet [53], D-LinkNet [54], \nAxial-DeepLab [40], U-Net [8] and UT-Net [55]. Table  5 \ncompares our results with refs. [52] and [56] on the BUSI \ndataset, whereas Table  6 presents a comparison of the \nresults with ref. [52] when trained on the BUSI dataset \nand tested on UDIAT dataset.\nTable 3 Results of performing ablation study of our proposed method trained and tested on the BUSI [21] dataset\nFold Accuracy (%) DC (%) IoU (%) Precision (%) Recall (%)\nBaseline 93.860 ± 0.010 81.940 ± 0.004 69.700 ± 0.010 76.820 ± 0.020 87.900 ± 0.010\nBaseline + CAF 94.040 ± 0.010 83.420 ± 0.007 72.560 ± 0.010 80.100 ± 0.010 88.100 ± 0.008\nTable 4 Results of performing ablation study of our proposed method trained on the BUSI [21] dataset and tested on the UDIAT [50] \ndataset\nFold Accuracy (%) DC (%) IoU (%) Precision (%) Recall (%)\nBaseline 97.760 ± 0.002 81.440 ± 0.040 70.260 ± 0.010 89.460 ± 0.020 76.400 ± 0.030\nBaseline + CAF 97.880 ± 0.000 81.520 ± 0.007 70.320 ± 0.010 90.320 ± 0.020 76.680 ± 0.020\nTable 5 Quantitative comparison of segmentation performance \nwith different methods on the BUSI [21] dataset\nModel Accuracy (%) DC (%) IoU (%) Sensitivity (%)\nU-Net [8] 95.55 77.19 62.71 74.74\nUT-Net [55] 95.58 78.08 64.02 77.93\nLinkNet [54] 96.07 81.22 68.21 81.77\nTransUNet [17] 96.10 81.57 68.69 82.05\nD-LinkNet [54] 96.21 81.72 68.68 82.56\nAxial-DeepLab [40] 96.31 82.01 69.00 80.36\nATFE-Net [52] 96.32 82.46 69.73 82.78\nOurs 94.04 83.42 72.56 80.10\nPage 10 of 15Tagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\nThe quantitative comparison results presented in \nTable  5 and 6 demonstrate the competitive segmenta -\ntion performance of the proposed method compared \nwith SOTA methods on the BUSI and UDIAT data -\nsets. Our model achieved DCs of 83.42% and 81.52% \nwhen tested on the BUSI and UDIAT datasets respec -\ntively, outperforming all other methods in accurately \ndelineating tumor regions. The IoU metric further sup -\nported our model’s performance, with scores of 72.56% \nand 70.32% when tested on BUSI and UDIAT datasets \nrespectively, indicating a substantial overlap between the \npredicted and ground truth segmentation masks. While \nour model exhibited a slightly lower accuracy of 94.04% \nwhen tested on BUSI dataset compared to other meth -\nods, it is important to note that accuracy alone may not \nfully capture the quality of the segmentation results. \nFurthermore, our model achieved a sensitivity of 80.10% \nand 90.32%, effectively capturing the majority of tumor \nregions. Overall, the proposed method demonstrated \ncompetitive performance in segmenting BUSI tumors, \nwith particular emphasis on achieving high-precision \nand accurate tumor boundary delineation. The incorpo -\nration of the CAF module into our model contributed to \nthese improved segmentation results by enhancing the \nfusion of features and capturing fine details in the tumor \nregions.\nTo justify the necessity of using a dual-branch architec-\nture for the proposed method, we conducted a quantita -\ntive comparison of the results provided by a U-Net with \na pre-trained EfficientNetV2 as a backbone and the pro -\nposed method with the dual encoders (EfficientNetV2 \nand adapted ViTs). The DC is higher for the proposed \nmethod (83.42%) than for U-Net with EfficientNetV2-\nL backbone (81.80%). This indicated that the proposed \nmethod accurately captured a larger portion of the tumor \nregion. The IoU coefficient was also higher for the pro -\nposed method (72.56%) than for U-Net with Efficient -\nNetV2-L (70.04%). Moreover, the precision and recall \nvalues for the proposed method (80.10% and 88.10%, \nrespectively) are higher than those for U-Net with Effi -\ncientNetV2-L back-bone (77.62% and 88.04%, respec -\ntively). The proposed method achieved an accuracy of \n94.04%, whereas U-Net with EfficientNetV2-L achieved \na slightly higher accuracy of 96.32%. However, the pro -\nposed method outperformed U-Net in terms of the other \nsegmentation performance metrics. Therefore, the pro -\nposed method with a dual-branch architecture outper -\nformed U-Net with EfficientNetV2-L backbone in terms \nof the DC, IoU, and precision, and achieved competitive \naccuracy. These findings provide additional experimental \nevidence justifying the necessity of using a dual-branch \narchitecture in the segmentation network. The dual-\nbranch architecture likely enables better capture of both \nlocal and global contextual information, leading to more \naccurate and robust tumor segmentation results in BUS \nimages.\nFigure 5 shows a visual representation of the segmen -\ntation examples. For both Dataset 1 and Dataset 2, the \ninput images with red contours represent the segmenta -\ntion results, and the green contour represents the actual \nmask. The model demonstrated an impressive ability to \nsegment both malignant and benign tumors. It is par -\nticularly adept at segmenting benign tumors, although \nit encounters some challenges with malignant tumors. \nSome challenging tumor examples can be observed in the \nsecond row and malignant column of the BUSI dataset, \nwhich features a large malignant tumor in the third row \nwith a hidden lesion, and in the fourth row with an irreg -\nular shape. The model can segment both large and small \ntumor regions owing to the incorporation of Efficient -\nNetV2-L features at various blocks with a wide range of \nscales combined with the global features extracted using \nthe transformer encoder. Figure  5 shows visual examples \nof the segmentation on the UDIAT dataset [50] using the \nmodel trained on the BUSI dataset [21]. These results \nindicate that the model can be effectively generalized \nfor various datasets. Tumors of varying sizes are recog -\nnized and segmented accurately; however, some difficul -\nties arise when handling malignant tumors. Although our \nproposed network exhibited strong segmentation out -\ncomes, it had limitations in terms of accurately segment -\ning specific BUS images. Figure  6 illustrates instances of \nunsuccessful segmentation, emphasizing the difficulties \nencountered when accurately delineating lesion regions \nwhere the boundary is unclear and irregularly shaped, \nand the lesion region is hidden.\nBUS classification results and discussion\nOur study examined the effectiveness of diverse versions \nof MLP-Mixer and ViTs in classifying the segmented \nTable 6 Quantitative comparison of segmentation performance \nwith different methods trained on the BUSI dataset and tested \non UDIAT dataset\nModel Accuracy (%) DC (%) IoU (%) Sensitivity (%)\nU-Net [8] 97.08 70.02 54.45 75.57\nUT-Net [55] 95.79 58.40 41.84 67.60\nLinkNet [54] 96.92 73.55 59.09 89.77\nTransUNet [17] 97.61 76.37 62.51 87.28\nD-LinkNet [54] 97.66 77.96 64.74 88.00\nAxial-DeepLab [40] 97.60 77.16 62.33 84.31\nATFE-Net [52] 97.81 78.44 65.03 85.20\nOurs 97.88 81.52 70.32 90.32\nPage 11 of 15\nTagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\n \nFig. 5 Segmentation examples predicted by our proposed method on BUSI [21] and UDIAT [50] datasets. The red contours represent \nthe segmentation results, while the green contour represents the ground truth\nFig. 6 Visualization of some failed cases for segmenting the tumor region of the BUS images by the proposed method. The red contours represent \nthe segmentation results, while the green contour represents the ground truth\nPage 12 of 15Tagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\nBUS lesions, with the aim of determining their efficacy. \nTable 7 presents the quantitative results of different mod-\nels trained on the segmented tumor regions of the BUSI \n[21] dataset using a fivefold cross validation.\nWe evaluated the classification results found with the \nproposed models: ViT and MLP-Mixer and compared it \nwith the classification results reported in ref. [45]. In their \nresearch, the authors developed a network, referred to as \nMsGoF, to classify BUS tumors as malignant or benign. \nMoreover, they trained their method on three available \ndatasets including the BUSI [21] dataset. A fivefold cross \nvalidation was utilized to fully evaluate the effectiveness \nof the method, with 20% of the training set used as a vali -\ndation set. Our results were also compared with those \nreported in ref. [46]. Their work focused on classify -\ning BUS tumors using a modified InceptionV3 network. \nThey introduced an increased number of residual mod -\nules and adjusted the hyperparameters of their models. \nMoreover, we compared our classification results with \nthose reported in ref. [57]. Their study used a combina -\ntion of supervised and unsupervised learning methods to \nclassify BUS images. To ensure a fair comparison, all the \nresults reported in this section were obtained using the \naforementioned methods from the cited studies on the \nsame dataset. Table  8 presents a quantitative compari -\nson of the obtained results with those of previous studies \nreported in refs. [45] and [46].\nThe classification accuracy results demonstrate \nthat the ViT with Base size and 16 × 16 patches (ViT-\nB/16) achieved the highest accuracy of 86% among the \nconsidered models. Although ViT-B/16 has a smaller \narchitecture compared to ViT-B/32 and ViT-L/32, requir-\ning fewer computational resources, it provided slightly \nbetter performance. This can be attributed to two fac -\ntors. Firstly, the larger ViT-L/32 model is a more com -\nplex network trained on a substantially larger dataset, \nwhich may lead to overfitting, especially for the smaller \nBUS dataset. Secondly, the smaller patch size of 16 × 16 \nin ViT-B/16 enables the extraction of more granular \nfeatures by the transformer encoder compared to the \n32 × 32 patches in ViT-L/32. Despite being a data-inten -\nsive model, the MLP-Mixer also achieved a competitive \naccuracy of 85.46% for MLP-Mixer-L/16. This substanti -\nates the significance and effectiveness of MLPs in these \narchitectures.\nThe results demonstrate that model complexity and \npatch size are crucial considerations for optimizing the \nperformance of ViT models, especially when limited data \nis available. For the given dataset, ViT-B/16 provides the \noptimal balance, yielding the highest accuracy with rea -\nsonable computational requirements. The competitive \nperformance of MLP-Mixer-L/16 also highlights the \nimportance of MLPs in extracting local and global fea -\ntures. Furthermore, in the context of medical imaging, it \nis crucial to lower the false negative rate of a predictive \nmodel and augment the true positives rate. The quanti -\ntative results of our study demonstrate that, despite hav -\ning a lower accuracy of 85.46% compared to ViT’s 86.00%, \nthe MLP-Mixer model exhibits superior sensitivity with \na score of 89.42%. This indicates that the MLP-Mixer \nTable 7 Classification results reported by the proposed models trained on BUSI [21] dataset\nMethod Accuracy (%) Precision (%) Recall (%) F1-score (%) Sensitivity (%) Specificity (%)\nViT-B/16 86.00 86.11 86.02 85.93 86.45 85.26\nViT-B/32 83.61 83.87 83.60 83.56 86.66 78.55\nViT-L/32 85.06 84.40 84.53 84.40 86.87 82.10\nMLP-Mixer-B/16 84.13 84.49 84.13 84.09 89.42 79.64\nMLP-Mixer-L/16 85.46 85.73 85.46 85.47 87.52 82.10\nTable 8 Quantitative comparison with classification models from the literature\nMethod Accuracy (%) Precision (%) Recall (%) F1-score (%) Sensitivity (%) Specificity (%)\n[46] 81.00 83.00 77.00 80.00 - -\n[45] 85.32 - - 78.96 85.24 88.57\nURepNet-v1 + SVM (linear) [57] 77.44 68.67 64.38 64.50 - -\nURepNet-v2 + SVM (linear) [57] 77.59 66.30 66.19 65.67 - -\nURepNet-v3 + SVM (linear) [57] 80.06 75.47 62.05 65.21 - -\nOurs (ViT-B/16) 86.00 86.11 86.02 85.93 86.45 85.26\nOurs (MLP-Mixer-B/16) 84.13 84.49 84.13 84.09 89.42 79.64\nPage 13 of 15\nTagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\n \nmodel is more capable of accurately detecting positive \ncases or true positives and identifying the presence of \ntumors in BUS images. As a result, MLP-Mixer’s higher \nsensitivity score leads to improved performance and \ngreater accuracy in predicting tumors in BUS images.\nTable  8 presents a comprehensive quantitative com -\nparison of the proposed method and other classifica -\ntion models. Our method achieved an accuracy of \n86.00%, outperforming the results reported in ref. [45] \n(85.32%). In terms of recall, our method achieved a score \nof 86.02%, surpassing the results reported in ref. [45] \n(77.00%) and URepNet-v3 + SVM (linear) (62.05%). The \nF1-score for our method was 85.93%, which is higher \nthan the values reported in ref. [44] (78.96%) and URe -\npNet-v3 + SVM (linear) (65.21%). Although the sensitiv -\nity and specificity scores were not reported in previous \nstudies [46, 57], our method exhibited a sensitivity of \n86.45%, indicating its ability to correctly identify posi -\ntive samples, and a specificity of 85.26%, highlighting its \ncapability to correctly classify negative samples. How -\never, the method reported in ref. [45] achieved a better \nspecificity recording 88.57%, which might be explained \nby the fact that the authors may have employed specific \nstrategies or features in their model design that are par -\nticularly effective in distinguishing non-lesion regions, \nor by the choice of evaluation metrics and thresholds \nthat can impact specificity. Our method outperformed \nother models in terms of accuracy, precision, recall, and \nF1-score, suggesting its effectiveness for BUS classifica -\ntion. Notably, our method demonstrated competitive \nresults when using two different backbone architectures, \nViT-B/16 and MLP-Mixer-B/16, with accuracies of \n86.00% and 84.13%, respectively. These findings validate \nthe superiority of our proposed approach in accurately \nclassifying breast lesions and showcase the potential of \nboth ViT and MLP-Mixer architectures for this task. Our \nmethod, which utilizes MLP-Mixer and ViT architectures \nto classify the overlay of the generated segmentation over \nthe original images, achieved superior results owing to \nseveral key factors. First, both MLP-Mixer and ViT are \npowerful neural network architectures that have demon -\nstrated excellent performance in various computer vision \ntasks. These architectures excel at capturing intricate \npatterns and relationships in images, enabling them to \neffectively analyze the overlay of the generated segmen -\ntation and original images. Second, MLP-Mixer and ViT \narchitectures can capture global contextual information \nfrom the entire image. This global perspective helps cap -\nture important features and contexts that contribute to \naccurate classification. Additionally, the classification of \nthe overlay of the generated mask on the original images \nserves as a valuable visual cue for classification, where the \nnetwork, while fine-tuning, focuses solely on the tumor \nregions.\nOne limitation of our current work is the potential \noversight of the tumor’s surrounding environment in \nthe diagnostic process. Although our multi-task frame -\nwork focuses on accurate tumor segmentation and clas -\nsification, it does not explicitly incorporate contextual \ninformation regarding the tumor’s immediate surround -\nings. This omission may hinder further improvements in \nclassification accuracy, as the surrounding environment \ncan provide valuable insights for diagnosis. We acknowl -\nedge the importance of considering this aspect and will \naddress it in future studies. Future research endeavors \nwill explore methods that explicitly model the tumor’s \nsurrounding environment to enhance the classification \naccuracy and provide a more comprehensive understand-\ning of the tumor’s diagnostic characteristics. By incor -\nporating this contextual information, we aim to further \nimprove the accuracy and reliability of the proposed \nmethod.\nConclusions\nIn conclusion, this study explored a novel hybrid \nmethod for the segmentation and classification of \nbreast tumors in BUS images by leveraging the capa -\nbilities of CNNs, attention mechanisms, and MLPs. \nThe framework utilizes a two-encoder architecture \nthat incorporates an EfficientNetV2 backbone and a \ncustomized ViT encoder to effectively extract tumor \nregions from BUS images. The SA mechanism of the \ntransformer encoder enables the capture of a broad \nrange of high-level and complex features, whereas the \nEfficientNetV2 encoder preserves the local information \nwithin the images. To fuse these extracted features, a \nCAF module was introduced, selectively emphasizing \nimportant features from both encoders. The integration \nof high-level and local information results in improved \nfeature integration. The feature maps obtained were \nsubsequently reconstructed using a decoder to gener -\nate segmentation maps that effectively delineated the \ntumor regions. Furthermore, the proposed method \nincorporated a novel approach for lesion classification \nin BUS images, employing an MLP-Mixer-based clas -\nsifier, which, to the best of our knowledge, has been \napplied for the first time in this specific task. The clas -\nsification results demonstrate the effectiveness of the \nproposed approach, achieving an accuracy of 86.00%. \nThe experimental evaluation shows the superior per -\nformance of the proposed framework compared with \nrecent related works. The segmentation results exhib -\nited an impressive DC of 83.42%, indicating highly \naccurate tumor region delineation. In addition, the \nclassification accuracy of 86% further supports the \nPage 14 of 15Tagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\nsuperiority of the proposed method. These findings \nhighlight the potential of ViTs and MLP-Mixer archi -\ntectures in medical image analysis, specifically for \nBUS image segmentation and classification tasks. The \nproposed multi-task framework, which incorporates \nthe hybrid architecture and CAF module, effectively \nintegrates high-level and local information, leading \nto improved segmentation and classification results. \nThis study contributes to advancing the field of medi -\ncal image analysis by introducing a novel and efficient \napproach that outperforms existing methods in terms \nof both segmentation and classification.\nAbbreviations\nCNN  Convolutional neural network\nViT  Vision transformer\nMLP  Multilayer perceptron\nSA  Self-attention\nCAF  Channel attention fusion\nBUS  Breast ultrasound\nSOTA  State of the art\nNLP  Natural language processing\nROI  Regions of interest\nSE  Squeeze-and-excitation\nCAD  Computer-aided diagnosis\nGAN  Generative adversarial network\nSCA  Self-co-attention\nGELU  Gaussian error liinear unit\nMSA  Multi-head-self-attention\nIoU  Intersection over union\nDC  Dice coefficient\nBCE  Binary cross-entropy\nAcknowledgements\nThe authors thank the authors from refs.   [21] and   [50] for providing the \ndatasets used in this study.\nAuthors’ contributions\nJT and HR provided the conceptualization, investigation, methodology, \nimplementation, validation, visualization, and wrote original draft; AY and HT \nprovided the validation and supervision.\nFunding\nNot applicable.\nAvailability of data and materials\nThe public datasets used in this study are publicly available BUSI provided \nonline  [21] as well as UDIAT dataset  [50].\nDeclarations\nCompeting interests\nThe authors declare that they have no known competing financial interests \nor personal relationships that could have appeared to inference the work \nreported in this paper.\nReceived: 16 October 2023   Accepted: 11 January 2024\nPublished: 26 January 2024\nReferences\n 1. Siegel RL, Miller KD, Jemal A (2018) Cancer statistics, 2018. CA Cancer J \nClin 68(1):7-30. https:// doi. org/ 10. 3322/ caac. 21442\n 2. Bray F, Ferlay J, Soerjomataram I, Siegel RL, Torre LA, Jemal A (2018) Global \ncancer statistics 2018: GLOBOCAN estimates of incidence and mortality \nworldwide for 36 cancers in 185 countries. CA Cancer J Clin 68(6):394-\n424. https:// doi. org/ 10. 3322/ caac. 21492\n 3. Zhang HY, Meng ZL, Ru JY, Meng YQ, Wang K (2023) Application and \nprospects of AI-based radiomics in ultrasound diagnosis. Vis Comput Ind \nBiomed Art 6(1):20. https:// doi. org/ 10. 1186/ s42492- 023- 00147-2\n 4. Sippel S, Muruganandan K, Levine A, Shah S (2011) Review article: use of \nultrasound in the developing world. Int J Emerg Med 4:72. https:// doi. \norg/ 10. 1186/ 1865- 1380-4- 72\n 5. Barra S, Carta SM, Corriga A, Podda AS, Recupero DR (2020) Deep learning \nand time series-to-image encoding for financial forecasting. IEEE/CAA J \nAutom Sin 7(3):683-692. https:// doi. org/ 10. 1109/ JAS. 2020. 10031 32\n 6. Piccialli F, Somma VD, Giampaolo F, Cuomo S, Fortino G (2021) A survey \non deep learning in medicine: why, how and when? Inf Fusion 66:111-\n137. https:// doi. org/ 10. 1016/j. inffus. 2020. 09. 006\n 7. Le EPV, Wang Y, Huang Y, Hickman S, Gilbert FJ (2019) Artificial intel-\nligence in breast imaging. Clin Radiol 75(5):357-366. https:// doi. org/ 10. \n1016/j. crad. 2019. 02. 006\n 8. Ronneberger O, Fischer P , Brox T (2015) U-Net: convolutional networks \nfor biomedical image segmentation. In: Navab N, Hornegger J, Wells WM, \nFrangi AF (eds) Medical image computing and computer-assisted inter-\nvention. 18th international conference, Munich, October 2015. Lecture \nnotes in computer science (Image processing, computer vision, pattern \nrecognition, and graphics), vol 9351. Springer, Heidelberg, pp 234-241. \nhttps:// doi. org/ 10. 1007/ 978-3- 319- 24574-4_ 28\n 9. Hu H, Zhang Z, Xie ZD, Lin S (2019) Local relation networks for image \nrecognition. In: Proceedings of 2019 IEEE/CVF international conference \non computer vision, IEEE, Seoul, 27 October-2 November 2019. https:// \ndoi. org/ 10. 1109/ ICCV. 2019. 00356\n 10. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN et al \n(2017) Attention is all you need. In: Proceedings of the 31st international \nconference on neural information processing systems, Curran Associates, \nInc., Long Beach, 4-9 December 2017\n 11. Han K, Wang YH, Chen HT, Chen XH, Guo JY, Liu ZH et al (2023) A survey \non vision transformer. IEEE Trans Pattern Anal Mach Intell 45(1):87-110. \nhttps:// doi. org/ 10. 1109/ TPAMI. 2022. 31522 47\n 12. Al-hammuri K, Gebali F, Kanan A, Chelvan IT (2023) Vision transformer archi-\ntecture and applications in digital health: a tutorial and survey. Vis Comput \nInd Biomed Art 6(1):14. https:// doi. org/ 10. 1186/ s42492- 023- 00140-9\n 13. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai XH, Unterthiner \nT et al (2021) An image is worth 16 × 16 words: transformers for image \nrecognition at scale. In: Proceedings of the 9th international conference \non learning representations, ICLR, Vienna, 3-7 May 2021\n 14. Li J, Chen JY, Tang YC, Wang C, Landman BA, Zhou SK (2023) Transform-\ning medical imaging with transformers? A comparative review of key \nproperties, current progresses, and future perspectives. Med Image Anal \n85:102762. https:// doi. org/ 10. 1016/j. media. 2023. 102762\n 15. Cao H, Wang YY, Chen J, Jiang DS, Zhang XP , Tian Q et al (2023) Swin-Unet: \nUnet-like pure transformer for medical image segmentation. In: Karlinsky L, \nMichaeli T, Nishino K (eds) Computer vision - ECCV 2022 workshops. ECCV \n2022. Lecture notes in computer science, vol 13803. Springer, Cham, pp \n205-218. https:// doi. org/ 10. 1007/ 978-3- 031- 25066-8_9\n 16. Azad R, Kazerouni A, Heidari M, Aghdam EK, Molaei A, Jia YW et al (2024) \nAdvances in medical image analysis with vision transformers: a compre-\nhensive review. Med Image Anal 91:103000. https:// doi. org/ 10. 1016/j. \nmedia. 2023. 103000\n 17. Chen JN, Lu YY, Yu QH, Luo XD, Adeli E, Wang Y et al (2021) TransUNet: \ntransformers make strong encoders for medical image segmentation. \narXiv preprint arXiv: 2102.04306\n 18. Tolstikhin IO, Houlsby N, Kolesnikov A, Beyer L, Zhai XH, Unterthiner T \net al (2021) MLP-mixer: an all-MLP architecture for vision. In: Proceedings \nof the 34th international conference on neural information processing \nsystems, NeurIPS, Online, 6-14 December 2021\n 19. Tan MX, Le Q (2021) EfficientNetV2: smaller models and faster training. In: \nProceedings of the 38th international conference on machine learning, \nICML, Online, 18-24 July 2021\n 20. Hu J, Shen L, Sun G (2018) Squeeze-and-excitation networks. In: Proceed-\nings of 2018 IEEE/CVF conference on computer vision and pattern \nrecognition, IEEE, Salt Lake City, 18-23 June 2018. https:// doi. org/ 10. 1109/ \nCVPR. 2018. 00745\nPage 15 of 15\nTagnamas et al. Visual Computing for Industry, Biomedicine, and Art  (2024) 7:2\n \n 21. Al-Dhabyani W, Gomaa M, Khaled H, Fahmy A (2020) Dataset of breast \nultrasound images. Data Brief 28:104863. https:// doi. org/ 10. 1016/j. dib. \n2019. 104863\n 22. Vigil N, Barry M, Amini A, Akhloufi M, Maldague XPV, Ma L et al (2022) Dual-\nintended deep learning model for breast cancer diagnosis in ultrasound \nimaging. Cancers 14:2663. https:// doi. org/ 10. 3390/ cance rs141 12663\n 23. Xing J, Li ZR, Wang BY, Qi YJ, Yu BB, Zanjani FG et al (2021) Lesion segmen-\ntation in ultrasound using semi-pixel-wise cycle generative adversarial \nnets. IEEE/ACM Trans Comput Biol Bioinform 18(6):2555-2565. https:// doi. \norg/ 10. 1109/ TCBB. 2020. 29784 70\n 24. He KM, Zhang XY, Ren SQ, Sun J (2016) Deep residual learning for image \nrecognition. In: Proceedings of 2016 IEEE conference on computer vision \nand pattern recognition, IEEE, Las Vegas, 27-30 June 2016. https:// doi. org/ \n10. 1109/ CVPR. 2016. 90\n 25. Singh VK, Abdel-Nasser M, Akram F, Rashwan HA, Sarker MMK, Pandey \nN et al (2020) Breast tumor segmentation in ultrasound images using \ncontextual-information-aware deep adversarial learning framework. \nExpert Syst Appl 162:113870. https:// doi. org/ 10. 1016/j. eswa. 2020. 113870\n 26. Lei BY, Huang S, Li R, Bian C, Li H, Chou YH et al (2018) Segmentation of \nbreast anatomy for automated whole breast ultrasound images with \nboundary regularized convolutional encoder-decoder network. Neuro-\ncomputing 321:178-186. https:// doi. org/ 10. 1016/j. neucom. 2018. 09. 043\n 27. Lei BY, Huang S, Li H, Li R, Bian C, Chou YH et al (2020) Self-co-attention \nneural network for anatomy segmentation in whole breast ultrasound. \nMed Image Anal 64:101753. https:// doi. org/ 10. 1016/j. media. 2020. 101753\n 28. Kumar V, Webb JM, Gregory A, Denis M, Meixner DD, Bayat M et al (2018) \nAutomated and real-time segmentation of suspicious breast masses \nusing convolutional neural network. PLoS One 13(5):e0195816. https:// \ndoi. org/ 10. 1371/ journ al. pone. 01958 16\n 29. Vakanski A, Xian M, Freer PE (2020) Attention-enriched deep learning \nmodel for breast tumor segmentation in ultrasound images. Ultrasound \nMed Biol 46(10):2819-2833. https:// doi. org/ 10. 1016/j. ultra smedb io. 2020. \n06. 015\n 30. Tong Y, Liu YY, Zhao MX, Meng L, Zhang JC (2021) Improved U-net MALF \nmodel for lesion segmentation in breast ultrasound images. Biomed \nSignal Process Control 68:102721. https:// doi. org/ 10. 1016/j. bspc. 2021. \n102721\n 31. Yang HN, Yang DP (2023) CSwin-PNet: a CNN-Swin transformer combined \npyramid network for breast lesion segmentation in ultrasound images. \nExpert Syst Appl 213:119024. https:// doi. org/ 10. 1016/j. eswa. 2022. 119024\n 32. Al-Battal AF, Lerman IR, Nguyen TQ (2023) Multi-path decoder U-Net: \na weakly trained real-time segmentation network for object detec-\ntion and localization in ultrasound scans. Comput Med Imaging Graph \n107:102205. https:// doi. org/ 10. 1016/j. compm edimag. 2023. 102205\n 33. Farooq MU, Ullah Z, Gwak J (2023) Residual attention based uncertainty-\nguided mean teacher model for semi-supervised breast masses segmen-\ntation in 2D ultrasonography. Comput Med Imaging Graph 104:102173. \nhttps:// doi. org/ 10. 1016/j. compm edimag. 2022. 102173\n 34. Moon WK, Shen YW, Huang CS, Chiang LR, Chang RF (2011) Computer-\naided diagnosis for the classification of breast masses in automated \nwhole breast ultrasound images. Ultrasound Med Biol 37(4):539-548. \nhttps:// doi. org/ 10. 1016/j. ultra smedb io. 2011. 01. 006\n 35. Flores WG, Pereira WCDA, Infantosi AFC (2015) Improving classification \nperformance of breast lesions on ultrasonography. Pattern Recognit \n48(4):1125-1136. https:// doi. org/ 10. 1016/j. patcog. 2014. 06. 006\n 36. Gómez W, Rodríguez A, Pereira WCA, Infantosi AFC (2013) Feature selec-\ntion and classifier performance in computer-aided diagnosis for breast \nultrasound. In: Proceedings of the 2013 10th international conference \nand expo on emerging technologies for a smarter world, IEEE, Melville, \n21-22 October 2013. https:// doi. org/ 10. 1109/ CEWIT. 2013. 67137 55\n 37. Tanaka H, Chiu SW, Watanabe T, Kaoku S, Yamaguchi T (2019) Computer-\naided diagnosis system for breast ultrasound images using deep learning. \nPhys Med Biol 64(23):235013. https:// doi. org/ 10. 1088/ 1361- 6560/ ab5093\n 38. Han S, Kang HK, Jeong JY, Park MH, Kim W, Bang WC et al (2017) A deep \nlearning framework for supporting the classification of breast lesions in \nultrasound images. Phys Med Biol 62(19):7714-7728. https:// doi. org/ 10. \n1088/ 1361- 6560/ aa82ec\n 39. Szegedy C, Liu W, Jia YQ, Sermanet P , Reed S, Anguelov D et al (2015) \nGoing deeper with convolutions. In: Proceedings of 2015 IEEE conference \non computer vision and pattern recognition, IEEE, Boston, 7-12 June \n2015. https:// doi. org/ 10. 1109/ CVPR. 2015. 72985 94\n 40. Wang Y, Choi EJ, Choi Y, Zhang H, Jin GY, Ko SB (2020) Breast cancer clas-\nsification in automated breast ultrasound using multiview convolutional \nneural network with transfer learning. Ultrasound Med Biol 46(5):1119-\n1132. https:// doi. org/ 10. 1016/j. ultra smedb io. 2020. 01. 001\n 41. Byra M, Galperin M, Ojeda-Fournier H, Olson L, O’Boyle M, Comstock C \net al (2019) Breast mass classification in sonography with transfer learning \nusing a deep convolutional neural network and color conversion. Med \nPhys 46(2):746-755. https:// doi. org/ 10. 1002/ mp. 13361\n 42. Xiao T, Liu L, Li K, Qin WJ, Yu SD, Li ZC (2018) Comparison of transferred \ndeep neural networks in ultrasonic breast masses discrimination. BioMed \nRes Int 2018:4605191. https:// doi. org/ 10. 1155/ 2018/ 46051 91\n 43. Ayana G, Choe SW (2022) Buvitnet: breast ultrasound detection via vision \ntransformers. Diagnostics 12(11):2654. https:// doi. org/ 10. 3390/ diagn ostic \ns1211 2654\n 44. Lu SY, Wang SH, Zhang YD (2022) SAFNet: a deep spatial attention net-\nwork with classifier fusion for breast cancer detection. Comput Biol Med \n148:105812.  https:// doi. org/ 10. 1016/j. compb iomed. 2022. 105812\n 45. Zhong SZ, Tu C, Dong XY, Feng QJ, Chen WF, Zhang Y (2023) MsGoF: \nbreast lesion classification on ultrasound images by multi-scale \ngradational-order fusion framework. Comput Methods Programs Biomed \n230:107346. https:// doi. org/ 10. 1016/j. cmpb. 2023. 107346\n 46. Sirjani N, Oghli MG, Tarzamni MK, Gity M, Shabanzadeh A, Ghaderi P \net al (2023) A novel deep learning model for breast lesion classifica-\ntion using ultrasound images: a multicenter data evaluation. Phys Med \n107:102560. https:// doi. org/ 10. 1016/j. ejmp. 2023. 102560\n 47. Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma SA et al (2015) \nImageNet large scale visual recognition challenge. Int J Comput Vis \n115(3):211-252. https:// doi. org/ 10. 1007/ s11263- 015- 0816-y\n 48. Hendrycks D, Gimpel K (2016) Gaussian error linear units (GELUs). arXiv \npreprint arXiv: 1606.08415\n 49. Ba JL, Kiros JR, Hinton GE (2016) Layer normalization. arXiv preprint arXiv: \n1607.06450\n 50. Yap MH, Pons G, Martí J, Ganau S, Sentís M, Zwiggelaar R et al (2018) \nAutomated breast ultrasound lesions detection using convolutional \nneural networks. IEEE J Biomed Health Inform 22(4):1218-1226. https:// \ndoi. org/ 10. 1109/ JBHI. 2017. 27318 73\n 51. Kingma DP , Ba J (2015) Adam: a method for stochastic optimization. In: \nProceedings of the 3rd International conference on learning representa-\ntions, ICLR, San Diego, 7-9 May 2015\n 52. Ma Z, Qi YL, Xu CB, Zhao W, Lou M, Wang YM et al (2023) ATFE-Net: axial \ntransformer and feature enhancement-based CNN for ultrasound breast \nmass segmentation. Comput Biol Med 153:106533. https:// doi. org/ 10. \n1016/j. compb iomed. 2022. 106533\n 53. Chaurasia A, Culurciello E (2017) LinkNet: Exploiting encoder representa-\ntions for efficient semantic segmentation. In: Proceedings of 2017 IEEE \nvisual communications and image processing, IEEE, St. Petersburg, 10-13 \nDecember 2017. https:// doi. org/ 10. 1109/ VCIP . 2017. 83051 48\n 54. Zhou ZW, Siddiquee MMR, Tajbakhsh N, Liang JM (2018) UNet++: a nested \nU-Net architecture for medical image segmentation. In: Stoyanov D, Taylor Z, \nCarneiro G, Syeda-Mahmood T, Martel A, Maier-Hein L et al (eds) Deep learn-\ning in medical image analysis and multimodal learning for clinical decision \nsupport. DLMIA ML-CDS 2018 2018. Lecture Notes in Computer Science(), vol \n11045. Springer, Cham, pp 3-11. https:// doi. org/ 10. 1007/ 978-3- 030- 00889-5_1\n 55. Gao YH, Zhou M, Metaxas DN (2021) UTNet: a hybrid transformer \narchitecture for medical image segmentation. In: de Bruijne M, Cattin PC, \nCotin S, Padoy N, Speidel S, Zheng YF et al (eds) Medical image comput-\ning and computer assisted intervention–MICCAI 2021. MICCAI 2021. \nLecture Notes in Computer Science(), vol 12903. Springer, Cham, pp \n61-71. https:// doi. org/ 10. 1007/ 978-3- 030- 87199-4_6\n 56. He QQ, Yang QJ, Xie MH (2023) HCTNet: a hybrid CNN-transformer \nnetwork for breast ultrasound image segmentation. Comput Biol Med \n155:106629. https:// doi. org/ 10. 1016/j. compb iomed. 2023. 106629\n 57. Song M, Kim Y (2024) Optimizing proportional balance between super-\nvised and unsupervised features for ultrasound breast lesion classifica-\ntion. Biomed Signal Process Control 87:105443. https:// doi. org/ 10. 1016/j. \nbspc. 2023. 105443\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7944042682647705
    },
    {
      "name": "Segmentation",
      "score": 0.7648525238037109
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6888826489448547
    },
    {
      "name": "Convolutional neural network",
      "score": 0.673486590385437
    },
    {
      "name": "Encoder",
      "score": 0.6049702167510986
    },
    {
      "name": "Breast ultrasound",
      "score": 0.5737413763999939
    },
    {
      "name": "Deep learning",
      "score": 0.5448749661445618
    },
    {
      "name": "Transformer",
      "score": 0.4822150468826294
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.47634032368659973
    },
    {
      "name": "Computer vision",
      "score": 0.43332144618034363
    },
    {
      "name": "Image segmentation",
      "score": 0.4148690104484558
    },
    {
      "name": "Breast cancer",
      "score": 0.18174800276756287
    },
    {
      "name": "Mammography",
      "score": 0.13156312704086304
    },
    {
      "name": "Engineering",
      "score": 0.07044026255607605
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Cancer",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I81605866",
      "name": "Sidi Mohamed Ben Abdellah University",
      "country": "MA"
    }
  ],
  "cited_by": 29
}