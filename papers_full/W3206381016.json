{
    "title": "Invariant Language Modeling",
    "url": "https://openalex.org/W3206381016",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2507747120",
            "name": "Maxime Peyrard",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2529009144",
            "name": "Sarvjeet Ghotra",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2891085859",
            "name": "Martin Josifoski",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A3207655789",
            "name": "Vidhan Agarwal",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2940149276",
            "name": "Barun Patra",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2559984696",
            "name": "Dean Carignan",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A1994052019",
            "name": "Emre Kiciman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2633850636",
            "name": "Saurabh Tiwary",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2067945905",
            "name": "Robert West",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4288019212",
        "https://openalex.org/W2963043696",
        "https://openalex.org/W3095992020",
        "https://openalex.org/W2953494151",
        "https://openalex.org/W3102127365",
        "https://openalex.org/W3129043507",
        "https://openalex.org/W2889624842",
        "https://openalex.org/W2963612262",
        "https://openalex.org/W3103937687",
        "https://openalex.org/W3135588948",
        "https://openalex.org/W3198690080",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3128232076",
        "https://openalex.org/W3174451219",
        "https://openalex.org/W2134067266",
        "https://openalex.org/W2952984539",
        "https://openalex.org/W2976191627",
        "https://openalex.org/W3035723985",
        "https://openalex.org/W2740983644",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2887768933",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2990408532",
        "https://openalex.org/W3168236463",
        "https://openalex.org/W3118781290",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W3132577852",
        "https://openalex.org/W2962787423",
        "https://openalex.org/W3095799614",
        "https://openalex.org/W2963060032",
        "https://openalex.org/W3121432811",
        "https://openalex.org/W3142917731",
        "https://openalex.org/W3121696984",
        "https://openalex.org/W2970789589",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2970119729",
        "https://openalex.org/W2790376986",
        "https://openalex.org/W1484551447",
        "https://openalex.org/W2963524349",
        "https://openalex.org/W4288404646",
        "https://openalex.org/W2126151240",
        "https://openalex.org/W3095807935",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W2889965839",
        "https://openalex.org/W4288287305",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W2964124968",
        "https://openalex.org/W2807912816",
        "https://openalex.org/W3124977280",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3124626396",
        "https://openalex.org/W2801890059",
        "https://openalex.org/W2155858138",
        "https://openalex.org/W2964299886"
    ],
    "abstract": "Maxime Peyrard, Sarvjeet Ghotra, Martin Josifoski, Vidhan Agarwal, Barun Patra, Dean Carignan, Emre Kiciman, Saurabh Tiwary, Robert West. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5728‚Äì5743\nDecember 7-11, 2022 ¬©2022 Association for Computational Linguistics\nInvariant Language Modeling\nMaxime Peyrard,‚ô¢ Sarvjeet Singh Ghotra,‚ô† Martin Josifoski,‚ô¢ Vidhan Agarwal,‚ô†\nBarun Patra,‚ô† Dean Carignan,‚ô† Emre Kƒ±cƒ±man,‚ô† Saurabh Tiwary,‚ô† Robert West‚ô¢\n‚ô¢EPFL ‚ô†Microsoft Corporation\n{maxime.peyrard, martin.josifoski, robert.west}@epfl.ch\n{saghotra, vidhan.agarwal}@microsoft.com\n{barun.patra, dcarig, emrek, satiwary}@microsoft.com\nAbstract\nLarge pretrained language models are critical\ncomponents of modern NLP pipelines. Yet,\nthey suffer from spurious correlations, poor\nout-of-domain generalization, and biases. In-\nspired by recent progress in causal machine\nlearning, in particular the invariant risk mini-\nmization (IRM) paradigm, we propose invari-\nant language modeling, a framework for learn-\ning invariant representations that generalize bet-\nter across multiple environments. In particu-\nlar, we adapt a game-theoretic formulation of\nIRM (IRM-games) to language models, where\nthe invariance emerges from a specific train-\ning schedule in which all the environments\ncompete to optimize their own environment-\nspecific loss by updating subsets of the model\nin a round-robin fashion. We focus on con-\ntrolled experiments to precisely demonstrate\nthe ability of our method to (i) remove struc-\ntured noise, (ii) ignore specific spurious corre-\nlations without affecting global performance,\nand (iii) achieve better out-of-domain general-\nization. These benefits come with a negligible\ncomputational overhead compared to standard\ntraining, do not require changing the local loss,\nand can be applied to any language model. We\nbelieve this framework is promising to help\nmitigate spurious correlations and biases in lan-\nguage models.\n1 Introduction\nWhile modern pretrained transformer models have\nled to dramatic progress on many NLP tasks, im-\nportant limitations remain. In particular, pretrained\nlanguage models suffer from poor generalization,\neven under small perturbations of the input distri-\nbution (Moradi and Samwald, 2021). Indeed, these\nmodels encode (Moradi and Samwald, 2021) and\nexploit (Tu et al., 2020; Niven and Kao, 2019) spu-\nrious correlations, i.e., correlations that do not gen-\neralize across data distributions. Since language\nmodels are trained on large unverified corpora, they\n‚Ä¶\t\tùëã#\n\tùëí%\n\t\tùëã&\n\tùëå\n\t\tùëã#\n\tùëí(\n\t\tùëã&\n\tùëå\n\t\tùëã#\n\tùëí)\n\t\tùëã&\n\tùëå\n\t\tùëã#\n\tùê∏\n\t\tùëã&\n\tùëå Target variable\nSpurious features\nEnvironment index\nStable features\nLatent causal \nmodel\nUnobserved / Unknown\nText. Env. 1\n Text. Env. 2 Text. Env. n\n‚Ä¶\nObservedTraining\neLM ignores environments\niLM finds invariant \nrepresentations\n‚Ñí ‚à™ \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n‚Ñí(\t\t\t\t\t) ‚Ñí(\t\t\t\t\t) ‚Ñí(\t\t\t\t\t)\nFigure 1: High-level overview using a simplified\ncausal structure. The distinction between environ-\nments makes it possible to separate spurious from sta-\nble features. Indeed, the relationship between the tar-\nget variable Y and the stable features XC is invariant\nacross environments: E[Y |XC,E] =E[Y |XC]. However,\nthe correlation between Y and XS is spurious and does\nnot generalize across environments: E[Y |XS,E = e] Ã∏=\nE[Y |XS,E = e‚Ä≤] for e Ã∏= e‚Ä≤. Language models trained\nwith the standard ERM, denoted as eLM in this work,\nexploit all correlations available during training and aim\nto learn E[Y |XC,XS]. Our proposed invariant language\nmodels, denoted as iLM, focus on invariant features and\naim to learn E[Y |XC]. In language modeling, Y could\nrepresent the missing-word prediction task.\nalso suffer from biases (Nadeem et al., 2021; Bor-\ndia and Bowman, 2019). Here the term ‚Äúbiases‚Äù\nrefers to correlations that may or may not be spu-\nrious according to the available textual data distri-\nbutions, but are nevertheless undesired. Existing\ntechniques aiming to remove spuriousness or biases\ninvolve computationally expensive domain align-\nment (Akuzawa et al., 2019; Liu et al., 2020; Zhao\net al., 2020), domain transfer (Balaji et al., 2018),\nor the addition of penalty terms to the loss targeted\nat specific undesired correlations (Qian et al., 2019;\nZhao et al., 2018). Alternatively, data preprocess-\ning (Zhao et al., 2017; Zhou et al., 2021) or manipu-\nlation such as counterfacual data-augmentation (Lu\n5728\net al., 2018) can yield datasets where undesired cor-\nrelations are less present. Pretraining with larger\nand more diverse datasets can also help (Tu et al.,\n2020; Brown et al., 2020).\nHowever, recent works on the theory of causality\n(Pearl, 2018; Sch√∂lkopf, 2019) argue that removal\nof spurious correlations requires altogether differ-\nent learning and training paradigms going beyond\npurely statistical learning. Indeed, generalization,\nspuriousness, and biases are all better understood in\nthe language of causality (Pearl, 2018). Intuitively,\ncausal relationships are the ones expected to be sta-\nble (Sch√∂lkopf et al., 2021; Peters et al., 2017) and\ngeneralizable (Peters et al., 2016). When the causal\ngraph underlying the data generation mechanism is\nknown, there exist causal identification algorithms\nto distinguish desired from undesired correlations\n(Shpitser and Pearl, 2008). However, for complex\ntasks of interest, the underlying causal model is not\nknown. Language modeling is one of these tasks,\nwhere it is unclear what would even be the relevant\nrandom variables constituting the causal model.\nTherefore, causal identification from the causal\ngraph seems out-of-reach for language modeling.\nSimilarly, removing undesired correlations one by\none is impractical due to the sheer amount of pos-\nsible correlations to consider. In this work, we\npropose to benefit from recent progress in causal\nmachine learning to offer a new and more flex-\nible lever for dealing with spuriousness and bi-\nases. We take inspiration from the invariance prin-\nciple, which states that only relationships invari-\nant across training environments should be learned\n(Peters et al., 2016). Under specific assumptions,\nthe invariant representation would then only en-\ncode the causal relationships relevant to the task\nand should thus generalize. Environments corre-\nspond to different views of the learning task, i.e.,\ndifferent data distributions. The invariance princi-\nple is illustrated by Fig. 1 with a simplified causal\nmodel as an example. E represents environment\nindices, Y is the target variable, XC are the causal\nfeatures, such that E[Y |XC] is stable across envi-\nronments (E[Y |XC,E] =E[Y |XC]), and XS are the\nspurious features, not generalizing across environ-\nments (E[Y |XS,E = e] Ã∏= E[Y |XS,E = e‚Ä≤] for e Ã∏= e‚Ä≤).\nLanguage models trained with standard empirical\nrisk minimization (ERM), denoted as eLM in this\nwork, exploit all correlations available during train-\ning and aim to learn E[Y |XC,XS]. Our proposed\ninvariant language models, denoted as iLM, focus\non invariant features and aim to learn E[Y |XC]. In\npractice, since the causal model is unknown, it\nis the choice of environments that defines what\ncorrelations are spurious. Invariant learning with\nappropriate choices of environments is the lever\nwe propose to employ to more flexibly deal with\nspuriousness and biases.\nA practical formulation of the invariance princi-\nple was proposed by Arjovsky et al. (2019). They\nintroduced invariant risk minimization (IRM), an\nalternative to ERM as a training objective enforc-\ning the learning of invariant representations. Ahuja\net al. (2020) later improved the training procedure\nto solve the IRM objective with a method called\nIRM-games. Unlike previous methods for remov-\ning biases and spurious correlations, IRM-games\ndoes not modify the loss with a regularization\nterm and does not compute domain alignment (or\nmatching) statistics. The invariance benefits come\nfrom the specific training schedule where environ-\nments compete to optimize their own environment-\nspecific loss by updating subsets of the model in a\nround-robin fashion.\nWe argue that the IRM paradigm, and IRM-\ngames specifically, is well-suited to improve NLP\nsystems. Textual data naturally comes from dif-\nferent environments, e.g., encyclopedic texts, so-\ncial media posts, news articles, etc. Moreover,\nnot knowing the causal mechanisms behind lan-\nguage generation within these environments is not\na blocker, as the relevant variables can now remain\nlatent. By adapting IRM-games to language mod-\neling, we introduce invariant language modeling\n(iLM), where the training of existing pretrained\nmodels is continued to enforce invariant represen-\ntations, using a simple and efficient modification\nof the training process. We then investigate the\nability of iLM to deal with undesired correlations\nin a series of controlled experiments, answering\nour core research question: Does the invariance\nprinciple give rise to a practical strategy to deal\nwith spurious correlations in language models?\nContributions. (i) We introduce a new training\nparadigm (iLM) for language models based\non the invariance principle (Sec. 3). Thanks\nto the use of the IRM-games training schedule\n(see Sec. 2), our iLM framework results in\nnegligible computational overhead compared to\nstandard ERM training, does not require changing\nthe local loss, and is agnostic to the language\nmodel architecture. (ii) In a series of controlled\n5729\nexperiments (Sec. 4), we demonstrate the ability of\niLM to remove structured noise (Sec. 4.1), ignore\nspecific spurious correlations without affecting\nglobal performance (Sec. 4.2), and achieve better\nout-of-domain generalization (Sec. 4.3). (iii) We\ndiscuss our contributions in relation to previous\nwork (Sec. 5). (iv) Finally, we release Huggingface-\ncompatible code for training iLM using existing\nlanguage model checkpoints (Wolf et al., 2020):\nhttps://github.com/epfl-dlab/\ninvariant-language-models\n2 Background\n2.1 Invariance Across Environments (IaE)\nRecent works on the theory of causality (Pearl,\n2018; Sch√∂lkopf, 2019) have argued that out-of-\ndistribution generalization and removal of spurious\ncorrelations require going beyond purely statisti-\ncal learning. This is motivated by the intuition\nthat causal relationships are the ones that are ex-\npected to be robust and generalizable (Peters et al.,\n2016). In causal machine learning, these ideas crys-\ntallized in the invariance principle which states\nthat only relationships invariant across training en-\nvironments should be learned (Peters et al., 2016;\nMuandet et al., 2013). In this paradigm, different\nenvironments correspond to data collected in differ-\nent setups, i.e., different data distributions (Pearl,\n2018). For NLP, spurious correlations and lack of\nout-of-distribution generalization are particularly\nwell-documented and important problems (Moradi\nand Samwald, 2021; Tu et al., 2020; Niven and\nKao, 2019). Fortunately, separations between en-\nvironments naturally emerge in textual data: ency-\nclopedic, news, social media, movie subtitles, etc.\nThis separation makes invariance-based approaches\nparticularly well-suited for NLP.\n2.2 Invariant Risk Minimization (IRM)\nWhile the invariance principle is a general and\npowerful idea, works based on this principle of-\nten require knowing which random variables are\npart of the causal model (Akuzawa et al., 2019;\nPeters et al., 2016). Arjovsky et al. (2019) intro-\nduced invariant risk minimization (IRM), an al-\nternative to empirical risk minimization (ERM),\nand a practical training objective enforcing invari-\nance in the learned latent representation. IRM also\nbuilds on the idea that the training data comes from\ndifferent environments e ‚ààE. Each environment\ne ‚ààE induces i.i.d. samples De from a distribution\nP(Xe,Y e). The goal, then, is to use these multi-\nple datasets to learn a predictor Y ‚âàf (X), which\nperforms well across the set of all environments\nE‚àó, only part of which were seen during training:\nE ‚äÇE‚àó. This is accomplished by decomposing f\ninto a feature representation œïand a classifier w,\nas f = w ‚ó¶œï, where ‚ó¶denotes function composi-\ntion. The feature representation œïelicits an invari-\nant representation of the data if the same classifier\nw is simultaneously optimal for all environments\ne ‚ààE. Intuitively, œïlearns a representation that is\ninvariant with respect to the environments if its rep-\nresentation is equally useful for all environments.\nFor NLP, we propose to use the main body of a\nlanguage model as the invariant feature learner œï.\nWhen trained on a language modeling task, w will\nbe the language modeling heads. Then, Y is the\nmasked word and X the context.\n2.3 IRM-Games\nIRM is a challenging bi-level optimization origi-\nnally solved (Arjovsky et al., 2019) by setting the\ninvariance criteria as a regularizer. Later, Ahuja\net al. (2020) improved the training procedure by us-\ning a game-theoretic perspective in which each en-\nvironment e is tied to its own classifierwe. A global\nclassifier w is then defined as the ensemble of\nall environment-specific classifiers: w = 1\n|E|\n‚àë\ne‚ààE\nwe\n(where the predictions, not the weights, are aver-\naged). Then, environments take turns making a\nstochastic gradient update to minimize their own\nlocal empirical risk, by updating only the weights\nof their own classifier we, while the shared œï is\nupdated periodically. For more details see the al-\ngorithm called V-IRM in the original paper. Ahuja\net al. (2020) showed that the equilibrium of this\ngame is a solution to the IRM objective, i.e., the\nresulting œïlearns invariant features. For NLP, we\nargue that IRM-games is a particularly meaningful\ncandidate to adapt to language modeling because it\nrequires little structural modifications.\n2.4 Why Invariance Is Needed for NLP\nTextual data is particularly subject to distribution\nshifts and out-of-domain distributions as texts nat-\nurally come from different environments. This cre-\nates a highly non-i.i.d. setting with problems of gen-\neralizability and spurious correlations. The curse\nbecomes a blessing when moving to invariance-\nbased ideas, as having diverse and naturally emerg-\n5730\ning environments is the necessary starting point of\nalgorithms like IRM-games.\nAs a simple example, consider gender bias in\npretrained language models. When the model is\nqueried with q = ‚ÄúMASK is the best doctor‚Äù, it\nfeeds q into its main body œï, from which a lan-\nguage modeling head w outputs softmax scores\nw ‚ó¶œï(q). Despite the context q containing no gen-\nder information, existing models score the pronoun\nhe much higher than she. The problem comes from\nthe presence of spurious correlations, where the\ncontext, here the word doctor, is correlated with\nhe. In an invariance-based approach, the training\ndata comes from different environments. Suppose\nthere is an environment e where the data is not\ngender-biased, i.e., there is no correlation between\nthe latent representation œï(q) and he. It is thus not\nstable across environments (not invariant) and will\nnot be learned. Now, consider the slightly different\nquery q‚Ä≤= ‚ÄúMASK is the best doctor, she is great!‚Äù.\nHere, the context œï(q‚Ä≤) contains gender informa-\ntion. In all environments, the pronoun she should\nbe preferred. This association arises not from a\nspurious correlation in data but from a common-\nsense, almost grammatical, constraint. Therefore,\nthis correlation is invariant and will be learned by\ninvariance-based approaches.\nThis exemplifies the potential benefits of\ninvariance-based approaches and illustrates the im-\nportance of choosing environment splits appropri-\nately. One should not expect any arbitrary split of\nenvironments to magically yield generalization ben-\nefits. However, the choice of environments within\nthe invariance-based learning framework provides\na flexible new lever to inject (i) inductive biases,\n(ii) knowledge about the data generation mecha-\nnism, and (iii) desirable stable properties (like re-\nmoving gender bias).\n3 Model\nWe introduce a way to train language models in-\nspired by the IRM-games setup. This involves dis-\ntinguishing the shared invariant feature extractor\nœïfrom the environment-specific we‚Äôs. With mod-\nern language model architectures, a natural choice\nemerges: œïas the main body of the encoder, and\nwe as the language modeling head that outputs the\nlogits after the last layer.\nFormally, suppose we have n environments con-\nsisting of data {(Xe,Y e)}e=1,...,n. For a batch\n(xi,yi) ‚àºP(Xi,Y i) from environment i, the model\noutput is formed using an ensemble of n language\nmodeling heads {we}e=1,...,n on top of the trans-\nformer encoder: ÀÜy = softmax\n(\n1\nn\nn‚àë\ne=1\nwe ‚ó¶œï(xi)\n)\n.\nThen, a (masked) language modeling loss L is\ncomputed on the model output ÀÜy. Note that it is the\npredictions of the n heads that are averaged not the\nweights or the gradients. No head gets to predict\nalone; the n heads always predict together as an\nensemble. The heads are subject to competitive\ngradient updates in a round-robin fashion as de-\nscribed below, which in turn creates the conditions\nthat enforce the invariance.\nTraining. The training of iLM follows the pseudo-\ncode described in Alg. 1, where environments take\nturns to send a batch of data and update œï and\ntheir associated head. An illustration is provided\nin Appendix A. Each head periodically gets an op-\nportunity to pull the global ensemble classifier w\nand the feature learner œïtowards fitting the distri-\nbution of its associated environment. Intuitively,\nsince each head gets the same amount of updates,\nthe game converges to a global classifier that is\nsimultaneously optimal for each environment, as\ndemonstrated by Ahuja et al. (2020). While the V-\nIRM algorithm of Ahuja et al. (2020) only updates\nœïperiodically, we found it more stable to update it\ntogether with every head update.\nAlgorithm 1 iLM training\n1: Initialize(œï,{we}e‚ààE )\n2: for iteration ‚àà{1,2,..., Nsteps\n|E| }do\n3: for environment e ‚ààE do\n4: (xi,yi) ‚ÜêGetBatchFromEnv(e)\n5: CompetitiveUpdate(xi,yi,œï,{we}e‚ààE )\n6: end for\n7: end for\n8: function COMPETITIVE UPDATE(xi,yi,œï,{we})\n9: L = L\n(\nsoftmax\n(\n1\nn\nn‚àë\ne=1\nwe ‚ó¶œï(xi)\n)\n,yi\n)\n10: GradientUpdate(L,œï,wi)\n11: end function\nAn advantage of this implementation is that in-\nvariance is obtained with few modifications to lan-\nguage models. Such simplicity arises from our\nleveraging of IRM-games, where invariance comes\nfrom the training schedules and ensembling of clas-\nsifiers. Furthermore, we implement two baselines\nthat appear similar but do not enjoy the same the-\noretical properties: mtLM and ensLM. The multi-\n5731\ndistilBERT R oBERT a\neLM 4.71¬± .04 3.93 ¬± .06\nmtLM 4.65¬± .05 3.74 ¬± .05\nensLM 4.66¬± .03 3.79 ¬± .02\niLM 4.43¬± .03 3.66¬± .04\nTable 1: Robustness to noise. Average perplexity over\nhyper-parameters (lower is better). The differences be-\ntween iLM and the others are statistically significant\n(paired t-test, p <10‚àí7).\ntask baseline (Liu et al., 2019a), mtLM, also uses\ndata split into environments with one head per en-\nvironment and each environment being seen as a\ndifferent task. The ensemble baseline (lan et al.,\n2018), ensLM, has a similar architecture as iLM,\nensembling n heads for predictions but always up-\ndating every head with every batch. The ensemble\nbaseline has the same forward pass as iLM but does\nnot perform the competitive gradient update. These\nbaselines serve as ablations of iLM to demonstrate\nthe importance of splitting the data into environ-\nments, ensembling the heads, and using the com-\npetitive gradient update.\n4 Experiments\nInvariance training comes with the promise of ro-\nbustness and generalization (Peters et al., 2016;\nMuandet et al., 2013; Ahuja et al., 2020). In the fol-\nlowing series of experiments, we test whether our\nproposed architecture for language modeling can\nprovide such benefits. Since our approach is agnos-\ntic to the language model, we focus on two small\nLMs used heavily in practice: distil BERT (Sanh\net al., 2019) and RoBERT a (Liu et al., 2019b). In\nthis work, we do not aim to engineer the best possi-\nble LM but rather precisely test iLM in controlled\nsetups by crafting environments whose difference\nis known, from which we know the expected be-\nhavior. We describe three experiments: robustness\nto noise, bias removal, and out-of-domain general-\nization.\nThroughout the experiments, we report esti-\nmated uncertainties with 95% confidence inter-\nvals. We repeat experiments for varying hyper-\nparameters and different random seeds (see Ap-\npendix B).\n4.1 Robustness to Noise\nIn this experiment, we test robustness in a con-\ntrolled setup. We craft two environments: Env-A\nmade of clean Wikipedia articles and Env-B made\nof full HTML pages of Wikipedia articles. We use\n120K articles split equally into the two environ-\nments (see Appendix B.1 for data details). Then,\nwe continue the training with the masked language\nmodeling (MLM) loss from existing checkpoints\nfor each of iLM, eLM, mtLM, and ensLM with\nthese two environments and evaluate the MLM per-\nplexity on a held-out dataset of clean Wikipedia\narticles (25K held-out sentences). Intuitively, eLM\nshould try to fit the HTML part of the training data\nand thus be more surprised by the clean Wikipedia\narticles during the test set. However, iLM should\nlearn to ignore the HTML because it does not gen-\neralize from Env-B to Env-A.\nResults. The results averaged over 16 hyper-\nparameters choices are reported in Table 1. See Ap-\npendix B.1 for hyper-parameters considered. For\nreference, the perplexities on the same test set of\noff-the-shelf pretrained distilBERT and RoBERT a\nare, respectively, 14.43 and 6.71. We observe that\niLM systematically has a significantly better test\nperplexity. Also, ensLM and mtLM perform sig-\nnificantly better than eLM but significantly worse\nthan iLM. This indicates that splitting data in n\nenvironments and ensembling n heads gives some\nrobustness benefits. The full benefit comes when\nfurther combined with the training schedule of iLM.\nWe come back to this discussion in Sec. 4.4.\nTo compare architectures over the test set with\ndifferent hyper-parameters, base transformers, and\nrandom seeds, we also performed paired aggrega-\ntion comparison based on the Bradley‚ÄìTerry model,\nfollowing the recommendations of Peyrard et al.\n(2021). The Pairformance tool1 measures the prob-\nability that iLM beats eLM when hyper-parameters\nare matched. We obtain that iLM significantly beats\neLM with .98 estimated probability. Similarly, iLM\nbeats ensLM with .89 estimated probability and\nmtLM with .92 estimated probability. In these ex-\nperiments, paired comparisons are particularly im-\nportant because varying hyper-parameters result\nin large variations of perplexity, such that blindly\naveraging can amplify the variance and hide the\nstructure of model performance.\n4.2 Bias Removal\nIn this experiment, we test the capacity to remove\none precise and known correlation by crafting two\n1https://github.com/epfl-dlab/\npairformance\n5732\n20 40 60 80 100\nRelative sizes (in %)\n0.20\n0.25\n0.30\n0.35\n0.40Average bias (test set)\niLM\neLM\nDistilBERT\nRoBERTa\nFigure 2: Bias removal. The x-axis represents the rela-\ntive size (x = 1‚àíp\np in percentages) between the modified\nenvironment and the unmodified one and the y-axis\nis the average bias for both iLM and eLM. Note that,\naccording to Pairformance, P(iLM beats eLM) >0.95\nwhen the relative size is <80%, and that eLM and iLM\nbecome indistinguishable for relative sizes >80%. Due\nto space, we report the results obtained by ensLM and\nmtLM in Appendix B.2 which also shows that they per-\nform in between iLM and eLM.\nenvironments differing only in this specific corre-\nlation. We use binary gendered terms and create\ntwo environments where the gendered terms are\nused differently.2 We follow the standard setup\nof Counterfactual Data Augmentation (CDA) (Lu\net al., 2018): we take a textual data source with\nknown gender bias, in this case, Wikitext-2 (Merity\net al., 2016). A fraction p of the data goes into Env-\nA, the rest (1 ‚àíp) goes into Env-B. Env-A remains\nuntouched and preserves all the properties of the\noriginal data source, whereas Env-B is intervened\nupon by inverting all gendered terms based on a\ndictionary provided by previous work (Bordia and\nBowman, 2019). When p = 1 ‚àíp = 0.5 and the\nlanguage model is finetuned with eLM, this setup\nmatches the CDA method (Lu et al., 2018) used\nto mitigate gender bias in NLP. Intuitively, iLM\nshould learn to ignore gender-based correlations no\nmatter what the fraction p. However, eLM is only\nexpected to ignore them when p = 1‚àíp = 0.5, i.e.,\nthe two environments precisely balance each other.\nExperimental setup. To measure whether the cor-\nrelation has been successfully removed, we (i) take\nall gendered terms in the test set, (ii) replace them\nby the MASK token, (iii) use trained models to pre-\n2We recognize the non-binary nature of gender as well\nas the many ethical principles in the design, evaluation, and\nreporting of results in studying gender as a variable in NLP\n(Larson, 2017). Because iLM is not limited to training only\nwith two environments, this architecture can also support more\ngeneral bias removal goals.\ndict the missing term, (iv) look in the softmax for\nthe scores received by the terms of the target gen-\ndered pair. We note sf and sm the score assigned to\nthe female and male terms in the softmax. Finally,\n(v) we compute an entropy-based bias measure:\nBH = H2\n(1\n2\n)\n‚àíH2\n( sf\nsf +sm\n)\n,where H2 is the binary\nentropy (note that H2\n(1\n2\n)\n= 1). BH measures the\nextent to which a softmax has a preference for the\nmale or female term in a gendered pair of terms.\nFor example, in the sentence ‚ÄúMASK is the best doc-\ntor‚Äù we look at the softmax score of the gendered-\npair [he, she]. If a model has learned to ignore\ngender-based correlation, the entropy should be\nhigh (entropy-bias low), not favoring one gendered\nterm over the other. We remove sentences with\nseveral gendered terms from the test set to avoid\npenalizing models for preferring a gender when the\ncontext contains gender information.\nWe ran the experiments for varying values of p\naveraging across different hyper-parameters, and\nreport the results in Fig. 2 for iLM and eLM.\nThe results for ensLM and mtLM are reported\nin Appendix B.2. See Appendix B.2 for hyper-\nparameters considered. For reference, the entropy\nbias of distilBERT and RoBERT a before training are,\nrespectively, 0.39 and 0.46.\nAnalysis. Compared to off-the-shelf models, both\neLM and iLM largely decrease the average entropy\nbias in the balanced setup but only iLM succeeds\nin the unbalanced setup. In the balanced setup (rel-\native sizes close to 100%), eLM and iLM perform\nwithin each other‚Äôs confidence intervals. However,\nin the unbalanced setup, iLM largely outperforms\neLM. We note that, according to Pairformance,\nthe probability that iLM beats eLM for any given\nhyper-parameter configuration is >0.9 for both\ndistilBERT and RoBERT a when the relative sizes\nis below 80%. As desired iLM is not affected by\nthe relative size of the environments. These results\nconfirm the hypothesis, that bias removal needs\na precisely balanced dataset for eLM (Lu et al.,\n2018), while it does not matter for iLM. Further-\nmore, this entropy bias reduction does not happen\nat the cost of worst general perplexities (see Ap-\npendix B.2). These findings are significant for the\nfield of bias removal, as iLM offers a practical and\nefficient way of removing biases. It is now not\nnecessary to carefully counter-balance the bias in\nthe augmented data. In Fig. 2, we see that already\nat 10% of relative size, iLM performs as well as\nexisting approaches (100% relative size + eLM).\n5733\nInD-LM‚Üì OoD-LM‚ÜìGLUE‚Üë\ndistilBERT\neLM 26.02 ¬± 0.35 31.52¬± 0.20 72.12\nensLM 22.31¬± 0.56 32.80¬± 0.23 72.34\nmtLM 22.73 ¬± 0.29 31.16¬± 0.44 72.22\niLM 20.25*¬± 0.52 30.32*¬± 0.43 72.45\nRoBERTa\neLM 14.55 ¬± 0.21 17.72¬± 0.25 76.89\nensLM 12.40¬± 0.34 17.68¬± 0.22 77.49\nmtLM 12.56 ¬± 0.33 17.43¬± 0.23 76.55\niLM 11.88*¬± 0.28 16.97*¬± 0.19 78.54*\nTable 2: ThePile environment experiments. The first\ncolumn is for language modeling evaluation in-domain\n(perplexity, lower is better), the second column is for\nlanguage modeling evaluation out-of-domain (perplex-\nity, lower is better), and the last column is for GLUE\ntasks averaged (higher is better). We mark with * the\ncases where iLM is statistically significantly better than\nother architectures (paired t-test).\n4.3 Out-of-Domain Generalization\nIn this experiment, we venture beyond controlled\nenvironments and test out-of-domain generaliza-\ntion with naturally occurring environments. We use\nthePile dataset (Gao et al., 2020) which contains\n20 very diverse textual domains: OpenSubtitles,\nArXiv papers, News, GitHub comments, etc.\nExperimental setup. We randomly sample 11 do-\nmains from thePile for training, the remaining 9\ndomains are used for testing language models out-\nof-domain. Once the models are trained, using\ndomains as environments, we evaluate their per-\nplexity in-domain (InD) using held-out data from\nthe training environments and OoD using data from\nunseen environments. See Appendix B.3 for details\nregarding training domains and hyper-parameters.\nFurthermore, the trained models are evaluated on\nthe GLUE benchmark. Indeed, models trained with\niLM can be used downstream exactly as if they\nwere trained with eLM. We report aggregated re-\nsults in Table 2. The results also show significant\nimprovement of iLM over other architecture across\nthe board. In particular, iLM is beneficial for both\nin-domain (InD) and out-of-domain (OoD) evalua-\ntion.\n4.4 Ablation\nThe eLM, mtLM, and ensLM architectures serve\nas ablated versions of iLM testing the three main\ncomponents of iLM: splitting the data into envi-\neLM mtLM ensLM iLM\neLM - .92 ¬± .06 .26¬± .09 .28¬± .09\nmtLM .08 ¬± .06 - .04 ¬± .04 .03¬± .04\nensLM .72 ¬± .09 .96¬± .04 - .37¬± .10\niLM .74¬± .09 .97¬± .04 .63¬± .10 -\nTable 3: Paired aggregated results. Estimated prob-\nability that one architecture (row i) is better than any\nother (column j) across all previous experiments, based\non the pairwise Bradley‚ÄìTerry aggregation model.\nronments with one head per environment (mtLM\nover eLM), ensembling the heads during training\n(ensLM over mtLM), using the specific competi-\ntive gradient update schedule (iLM over ensLM).\nThe four variants were run over all experiments pre-\nviously described varying hyper-parameters yield-\ning a total of 1320 experimental results (see Ap-\npendix B for details) per architecture. To get a\nglobal view, we again aggregated these results with\nthe paired aggregation given by the Bradley‚ÄìTerry\nmodel. It estimates a strength for each architecture\nbased on how likely it is to beat other architec-\nture on the same experiments with the same hyper-\nparameters. It provides a scale-independent metric-\nindependent way to aggregate scores (Peyrard et al.,\n2021) across tasks and experiments.\nThe results are reported in Table 3 and confirm\nthe intuition built-up with previous experiments\nthat simply having n environments with n heads\nis not beneficial on its own, as mtLM does not\nprovide benefits over eLM. However, when com-\nbined with head ensembling (ensLM), significant\nimprovements can be observed over both eLM and\nmtLM. Further significant benefits arise from the\ncompetitive gradient update specific to iLM. While\nboth mtLM and ensLM have slightly better capac-\nity to overfit with their n heads, they don‚Äôt bene-\nfit from the invariance regularization provided by\ncompetitive gradient updates. Notice that iLM is\nsignificantly better than any other architecture, as\nshown by the last row of Table 3 (or equivalently,\nthe last column).\n5 Discussion\nIn this section, we discuss our contributions in the\ncontext of previous work.\n5734\n5.1 Related Work\nDomain generalization. The performance of\ndeep learning models substantially degrades on\nout-of-domain (OoD) datasets, even in the face\nof small variations of the data-generating process\n(Hendrycks and Dietterich, 2019). Blanchard et al.\n(2011) have proposed domain generalization (DG)\nas a formalism for studying this problem. In DG,\nthe goal is to learn a model using data from a single\nor multiple related but distinct training domains,\nin such a way that the model generalizes well to\nany OoD testing domain, unknown during training.\nRecently, the problem of DG has attracted a lot of\nattention, and has been approached from different\nfacets. Most of the existing methods fall under\nthe paradigm of domain alignment (Muandet et al.,\n2013; Li et al., 2018b; Akuzawa et al., 2019; Liu\net al., 2020; Zhao et al., 2020). Motivated by the\nidea that features that are stable across the train-\ning domains should also be robust to the unseen\ntesting domains, these methods try to learn domain-\ninvariant representations. A group of other methods\nis based on meta-learning (Dou et al., 2019; Balaji\net al., 2018; Li et al., 2018a). The motivation be-\nhind this approach is that it exposes the model to\ndomain shifts during training, which will allow it\nto generalize better during testing. Regularization\nthrough data augmentation is commonly used in\nthe training of machine learning models to allevi-\nate overfitting and thereby improve generalization\n(Zhou et al., 2021, 2020). Based on this idea, (Zhou\net al., 2021, 2020) apply transformations on the\noriginal data to simulate a domain shift in training.\nDomain generalization applied to language mod-\nels. In NLP, the default pipeline involves pre-\ntraining a task-agnostic language model, which\nis then finetuned on downstream tasks. This pre-\ntraining/finetuning division of learning is already\nknown to improve robustness on downstream tasks\n(Hendrycks and Dietterich, 2019). However, the\nlanguage models themselves suffer from spuri-\nous correlations and poor generalization even with\nsmall perturbations of the inputs (Moradi and\nSamwald, 2021). To alleviate such problems, Oren\net al. (2019) adapted Distribution Robust Optimiza-\ntion (Ben-Tal et al., 2013) to language models. This\nresulted in a new loss minimizing the worst-case\nperformance over subsamples of the training set.\nThey focused on domains with topic shifts. Later,\nVernikos et al. (2020) used domain -adversarial\nregularization to improve testing performance on\nunseen domains.\nCompared to these previous works, iLM enjoys\ntheoretical justification rooted in the causal frame-\nwork of invariance (Peters et al., 2016). Our im-\nplementation is simple, comes at negligible com-\nputational cost and can be applied directly to any\ntransformer LM.\n5.2 Environment Design\nOne question that might arise from the iLM training\nschedule is what happens when environments have\nno lexical overlap. Maybe no correlation would\nremain for iLM to model? We emphasize that\niLM learns a latent representation œï and stable\ncorrelations are those connecting this latent rep-\nresentation to observables, and not surface corre-\nlations between observables. To demonstrate that\niLM operates on latent variables and not just on\nsurface-level correlations, we performed a simple\nexperiment with languages as environments. We\ntrained iLM with a pretrained multilingual model\n(XLM-RoBERT a) using English Wikipedia articles\nand Farsi Wikipedia articles as two environments.\nDespite almost no surface-level overlap, iLM is\nstill able to improve perplexity in each language in-\ndividually and does not destroy previously learned\ncorrelations. This experiment is detailed in Ap-\npendix B.4.\nAlso, if the number of environments grows ar-\nbitrarily large, certainly iLM would not find any\nstable correlations in the data? We emphasize that\nthe choice of environments is not intended to be ar-\nbitrary; simply contriving as many environments as\npossible could not be expected to be useful. Rather,\nthe choice of environments has to reflect assump-\ntions about the underlying data generation mecha-\nnism; iLM then leverages the assumptions encoded\nin the choice of environments.\nIndeed, after this work has shown that iLM can\neffectively remove unstable correlations, the next\nquestion becomes that of environment design:\nhow to choose environment splits to be useful in\npractice? Useful environment splits will likely\nbe different for different tasks and different pur-\nposes. This work already demonstrated that the new\nparadigm of (i) environment design then (ii) iLM\nis practical for language-related problems. Choos-\ning environment splits is a flexible way to inject\npriors and inductive biases, compared to manually\ndeciding which correlations are desired (as in bias\n5735\nremoval) or fully learning the causal graph (as in\ncausal reasoning). Now, iLM provides a compu-\ntationally efficient framework to inject such pri-\nors and move the discussion from model inductive\nbiases to data inductive biases. It already offers\nrobustness to noise, a ready-to-use bias removal\nstrategy for any existing language model needing\nfew data points, and improves OoD generalization.\n6 Limitations\nIn this work, we focus on crafting controlled experi-\nments with easily manageable dataset and language\nmodel sizes to carefully test the invariance benefits\nof iLM. However, it is possible to expect different\nqualitative behavior for large-scale language mod-\nels recently deployed due to emergent properties.\nOur implementation could be applied largely\nto various downstream tasks, other than language\nmodeling measured by perplexity. Here, we fo-\ncus on the language modeling task and perplexity\nmeasure because they allow clear and precise ex-\nperiments measuring the ability of iLM to deal with\nspurious correlations. The strong positive results\nobserved in this work motivate future work to test\niLM in other setups closer to direct practical use-\ncases.\nIt is expected that different choices of environ-\nment splits will be useful for different downstream\ntasks. While this work demonstrates that iLM is\nuseful to remove spurious correlation, it does not\nsay how to choose environments for which tasks.\nFor instance, we observed smaller improvements\nwhen using thePile datasets and evaluating on the\ndownstream GLUE tasks, indicating that thePile\nenvironment splits are not optimal for these down-\nstream tasks. We believe that environment design\nis an important avenue for future research.\nAcknowledgments\nThis project is a collaboration between EPFL\nand Microsoft as part of the Microsoft Tur-\ning Academic Program (MS-TAP). With support\nfrom Swiss National Science Foundation (grant\n200021_185043), European Union (TAILOR, grant\n952215), and gifts from Microsoft, Facebook,\nGoogle.\nReferences\nKartik Ahuja, Karthikeyan Shanmugam, Kush R. Varsh-\nney, and Amit Dhurandhar. 2020. Invariant risk min-\nimization games. In Proceedings of the 37th Inter-\nnational Conference on Machine Learning, ICML\n2020, 13-18 July 2020, Virtual Event, volume 119 of\nProceedings of Machine Learning Research, pages\n145‚Äì155. PMLR.\nKei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo.\n2019. Adversarial Invariant Feature Learning with\nAccuracy Constraint for Domain Generalization. In\nJoint European Conference on Machine Learning and\nKnowledge Discovery in Databases, pages 315‚Äì331.\nSpringer International Publishing.\nMartin Arjovsky, L√©on Bottou, Ishaan Gulrajani, and\nDavid Lopez-Paz. 2019. Invariant risk minimization.\narXiv preprint arXiv:1907.02893.\nYogesh Balaji, Swami Sankaranarayanan, and Rama\nChellappa. 2018. Metareg: Towards domain gen-\neralization using meta-regularization. In Advances\nin Neural Information Processing Systems 31: An-\nnual Conference on Neural Information Processing\nSystems 2018, NeurIPS 2018, December 3-8, 2018,\nMontr√©al, Canada, pages 1006‚Äì1016.\nAharon Ben-Tal, Dick den Hertog, Anja De Waegenaere,\nBertrand Melenberg, and Gijs Rennen. 2013. Robust\nsolutions of optimization problems affected by uncer-\ntain probabilities. Management Science, 59(2):341‚Äì\n357.\nGilles Blanchard, Gyemin Lee, and Clayton Scott. 2011.\nGeneralizing from several related classification tasks\nto a new unlabeled sample. In Advances in Neural In-\nformation Processing Systems 24: 25th Annual Con-\nference on Neural Information Processing Systems\n2011. Proceedings of a meeting held 12-14 December\n2011, Granada, Spain, pages 2178‚Äì2186.\nShikha Bordia and Samuel R. Bowman. 2019. Identify-\ning and reducing gender bias in word-level language\nmodels. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Student Research Workshop,\npages 7‚Äì15, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n5736\n4171‚Äì4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nQi Dou, Daniel Coelho de Castro, Konstantinos Kamnit-\nsas, and Ben Glocker. 2019. Domain generalization\nvia model-agnostic learning of semantic features. In\nAdvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-\n14, 2019, Vancouver, BC, Canada, pages 6447‚Äì6458.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nRuocheng Guo, Pengchuan Zhang, Hao Liu, and Emre\nKiciman. 2021. Out-of-distribution prediction with\ninvariant risk minimization: The limitation and an\neffective fix. CoRR, abs/2101.07732.\nDan Hendrycks and Thomas G. Dietterich. 2019.\nBenchmarking neural network robustness to common\ncorruptions and perturbations. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nxu lan, Xiatian Zhu, and Shaogang Gong. 2018. Knowl-\nedge distillation by on-the-fly native ensemble. In\nAdvances in Neural Information Processing Systems,\nvolume 31. Curran Associates, Inc.\nBrian Larson. 2017. Gender as a variable in natural-\nlanguage processing: Ethical considerations. In Pro-\nceedings of the First ACL Workshop on Ethics in\nNatural Language Processing, pages 1‚Äì11, Valencia,\nSpain. Association for Computational Linguistics.\nDa Li, Yongxin Yang, Yi-Zhe Song, and Timothy M.\nHospedales. 2018a. Learning to generalize: Meta-\nlearning for domain generalization. In Proceedings\nof the Thirty-Second AAAI Conference on Artificial\nIntelligence, (AAAI-18), the 30th innovative Applica-\ntions of Artificial Intelligence (IAAI-18), and the 8th\nAAAI Symposium on Educational Advances in Artifi-\ncial Intelligence (EAAI-18), New Orleans, Louisiana,\nUSA, February 2-7, 2018, pages 3490‚Äì3497. AAAI\nPress.\nYa Li, Mingming Gong, Xinmei Tian, Tongliang Liu,\nand Dacheng Tao. 2018b. Domain generalization via\nconditional invariant representations. In Proceedings\nof the Thirty-Second AAAI Conference on Artificial\nIntelligence, pages 3579‚Äì3587. AAAI Press.\nChang Liu, Xinwei Sun, Jindong Wang, Tao Li, Tao\nQin, Wei Chen, and Tie-Yan Liu. 2020. Learning\ncausal semantic representation for out-of-distribution\nprediction. CoRR, abs/2011.01681.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019a. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4487‚Äì4496, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-\ncharla, and Anupam Datta. 2018. Gender bias\nin neural natural language processing. CoRR,\nabs/1807.11714.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. CoRR, abs/1609.07843.\nMilad Moradi and Matthias Samwald. 2021. Evaluating\nthe robustness of neural language models to input\nperturbations. CoRR, abs/2108.12237.\nKrikamol Muandet, David Balduzzi, and Bernhard\nSch√∂lkopf. 2013. Domain generalization via invari-\nant feature representation. In Proceedings of the\n30th International Conference on Machine Learning,\nvolume 28 of Proceedings of Machine Learning Re-\nsearch, pages 10‚Äì18, Atlanta, Georgia, USA. PMLR.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356‚Äì5371, Online. Association for\nComputational Linguistics.\nTimothy Niven and Hung-Yu Kao. 2019. Probing neu-\nral network comprehension of natural language argu-\nments. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4658‚Äì4664, Florence, Italy. Association for Compu-\ntational Linguistics.\nYonatan Oren, Shiori Sagawa, Tatsunori B. Hashimoto,\nand Percy Liang. 2019. Distributionally robust lan-\nguage modeling. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 4227‚Äì4237, Hong Kong, China. Association\nfor Computational Linguistics.\nJudea Pearl. 2018. Theoretical impediments to machine\nlearning with seven sparks from the causal revolution.\nCoRR, abs/1801.04016.\nJonas Peters, Peter B√ºhlmann, and Nicolai Meinshausen.\n2016. Causal inference by using invariant prediction:\nidentification and confidence intervals. Journal of\nthe Royal Statistical Society: Series B (Statistical\nMethodology), 78(5):947‚Äì1012.\n5737\nJonas Martin Peters, Dominik Janzing, and Bernard\nSch√∂lkopf. 2017. Elements of Causal Inference:\nFoundations and Learning Algorithms. MIT Press,\nCambridge, MA, USA.\nMaxime Peyrard, Wei Zhao, Steffen Eger, and Robert\nWest. 2021. Better than average: Paired evaluation\nof NLP systems. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 2301‚Äì2315, Online. Association for\nComputational Linguistics.\nYusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun.\n2019. Reducing gender bias in word-level language\nmodels with a gender-equalizing loss function. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics: Student Re-\nsearch Workshop, pages 223‚Äì228, Florence, Italy.\nAssociation for Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter. CoRR,\nabs/1910.01108.\nB. Sch√∂lkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalch-\nbrenner, A. Goyal, and Y . Bengio. 2021. Toward\ncausal representation learning. Proceedings of the\nIEEE - Advances in Machine Learning and Deep\nNeural Networks, 109(5):612‚Äì634.\nBernhard Sch√∂lkopf. 2019. Causality for machine learn-\ning. CoRR, abs/1911.10500.\nIlya Shpitser and Judea Pearl. 2008. Complete identifi-\ncation methods for the causal hierarchy. Journal of\nMachine Learning Research, 9(64):1941‚Äì1979.\nLifu Tu, Garima Lalwani, Spandana Gella, and He He.\n2020. An empirical study on robustness to spuri-\nous correlations using pre-trained language models.\nTransactions of the Association for Computational\nLinguistics, 8:621‚Äì633.\nGiorgos Vernikos, Katerina Margatina, Alexandra\nChronopoulou, and Ion Androutsopoulos. 2020. Do-\nmain Adversarial Fine-Tuning as an Effective Regu-\nlarizer. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 3103‚Äì3112,\nOnline. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38‚Äì45, Online. Association\nfor Computational Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2017. Men also like\nshopping: Reducing gender bias amplification using\ncorpus-level constraints. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2979‚Äì2989, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-\nWei Chang. 2018. Learning gender-neutral word em-\nbeddings. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4847‚Äì4853, Brussels, Belgium. Association\nfor Computational Linguistics.\nShanshan Zhao, Mingming Gong, Tongliang Liu, Huan\nFu, and Dacheng Tao. 2020. Domain generalization\nvia entropy regularization. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nKaiyang Zhou, Yongxin Yang, Timothy M. Hospedales,\nand Tao Xiang. 2020. Learning to generate novel\ndomains for domain generalization. In Computer\nVision - ECCV 2020 - 16th European Conference,\nGlasgow, UK, August 23-28, 2020, Proceedings, Part\nXVI, volume 12361 of Lecture Notes in Computer\nScience, pages 561‚Äì578. Springer.\nKaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang.\n2021. Domain generalization with mixstyle. In 9th\nInternational Conference on Learning Representa-\ntions, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021. OpenReview.net.\n5738\nA Illustration of iLM Architecture\nIn the main paper, we described formally the\npseudo-code involved in training iLM models. The\nmodel architecture and the logic of the training\nschedule is illustrated in Fig. 3 for the special-case\nof 2 environments (n = 2).\nA.1 mtLM and ensLM baselines\nWe implemented two similar architectures that do\nnot enjoy the same theoretical justifications.\nIn the mtLM baseline, the data is also split into\nn environments with one head per environment. As\nin iLM, environments take turns to send a batch\nof data and perform a batch update on the body of\nthe transformer œïand the head associated with this\nenvironment. This is like viewing different envi-\nronments as different tasks with uniform weights,\neven though they are all language modeling loss.\nIn the ensLM baseline, the data is split into n en-\nvironments with one head per environment. How-\never, here, the heads are always predicting as an\nensemble like iLM. Here also the environments\ntake turns to send a batch of data. The forward pass\nis exactly the same as the one of iLM. In the back-\nward pass, every head and the transformer body œï\nare always updated for every batch of every envi-\nronment.\nB Details about Experiments\nB.1 Robustness to Noise\nData. The data used for this experiment comes\nfrom an HTML Wikipedia Dump of August 2018.\nThe files were pre-processed to remove the HTML\ncontent resulting in clean text articles. We ran-\ndomly selected 60K articles with HTML (Env-B),\nand 60K different articles without HTML (Env-A).\nThe test set contains 25K sentences coming from\nWikipedia without HTML.\nHyper-parameters. We ran the experiments\nreported in the main paper while varying\nseveral hyper-parameters: base transform-\ners ( œï): [distil BERT , RoBERT a], learning\nrates: [1e‚àí5,5e‚àí5], number of training steps:\n[10,100,200,500,2500,5000], 5 random restarts\nwith different random seeds, 2 ¬∑2 ¬∑6 ¬∑5 = 120, ran\nwith eLM, mtLM, ensLM, and iLM resulting in\n480 experiments.\nNumber of lines vs. number of articles. In the\nmain paper, we report the results of iLM and eLM\n25 50 75 100\ndistilBERT\neLM .372 ¬± .012 .358¬± .033 .326¬± .001 .308¬± .016\nmtLM .363 ¬± .010 .352¬± .037 .308¬± .022 .328¬± .022\nensLM .322 ¬± .003 .350¬± .032 .324¬± .020 .315¬± .015\niLM .309¬± .006 .322¬± .033 .318¬± .012 .309¬± .004\nRoBERT a\neLM .317 ¬± .010 .305¬± .008 .273¬± .045 .259¬± .025\nmtLM .308 ¬± .011 .299¬± .009 .271¬± .29 .260¬± .12\nensLM .291 ¬± .011 .300¬± .011 .270¬± .031 .271¬± .033\niLM .290¬± .013 .291¬± .003 .271¬± .033 .267¬± .025\nTable 4: Complementary gender-bias removal results.\nAverage bias BH as described in Sec. 4.2 across 4 differ-\nent relative sizes of environments (25%, 50%, 75% and\n100%).\nwhen trained with environments having the same\nnumber of articles. However, the HTML articles\nhave more lines and thus more sentences. There-\nfore, we also report in Fig. 4 the same analysis\nrepeated when the number of lines between Env-A\nand Env-B is the same, meaning Env-B contains\nfewer articles. The conclusion remains largely un-\nchanged in this scenario. As seen in Fig. 4 (c), iLM\nhas still a probability of beating eLM for match\nhyper-parameters close to 1, highly significantly\nfar away from 0.5.\nB.2 Bias Removal\nData. The dataset used for this experiment is\nWikitext-2 (Merity et al., 2016) where we follow\nthe existing train/dev/test split. The dictionary of\ngendered terms comes from Bordia and Bowman\n(2019) which was originally constructed to mea-\nsure gender bias in language models.\nThe dictionary contains basic gender-pairs aug-\nmented with their variations in terms of casing,\nplural vs. singular forms and different spellings.\nThe basic gendered pairs are: (actor, actress), (boy,\ngirl), (boyfriend, girlfriend), (father, mother), (gen-\ntleman, lady), (grandson, granddaughter), (he, she),\n(hero, heroine), (him, her), (husband, wife), (king,\nqueen), (male, female), (man, woman), (mr., mrs.),\n(prince, princess), (son, daughter), (spokesman,\nspokeswoman), (stepfather, stepmother), (uncle,\naunt)\nHyper-parameters. We ran the experiments re-\nported in the main paper while varying several\nhyper-parameters: base-model ( œï): [distil BERT ,\n5739\nForward-pass (architecture)\n12#‚Üíùë§!ùë§\"\nùúô‚Ä¶‚Ä¶‚Ä¶ ‚Üê‚Ñíùëúùë†ùë†Backpropagateonly through ùë§!and ùúô\nBatchenvA\n12#‚Üíùë§!ùë§\"\nùúô‚Ä¶‚Ä¶‚Ä¶ ‚Üê‚Ñíùëúùë†ùë†Backpropagateonly through ùë§\"and ùúô\nBatchenvB\n‚Ä¶‚Ä¶‚Ä¶ 12#‚Üíùë§!ùë§\"\nùúô ùë§!+ùë§\"2‚àòùúôWe[MASK]‚Ä¶books\nDataset of two environments: envAand envB\nTraining schedule\nAlternate batch updates between\nenvA\nenvB\nFigure 3: Model description. In the forward pass, input text goes through the main body of language model noted\nœï(e.g., a Transformer (Devlin et al., 2019)), then one head per environment predicts logits over the vocabulary.\nThese predictions are averaged over all heads and go through a softmax. During training, the model receives a batch\nof data from one environment e and performs a gradient update only on the parameters of the main body of the\nlanguage model (œï) and on the parameters of the head tied to this environment we. Then batches are taken from\neach environment in a round-robin fashion.\nDistilBERT RoBERTa\nBase model\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0a) Average perplexity\niLM eLM\n0 1000 2000 3000 4000 5000\nNumber of training steps\n4\n5\n6\n7b) Average perplexity\niLM\neLM\nDistilBERT\nRoBERTa\nDistilBERT RoBERTa\nBase model\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0c) (iLM beats eLM)\nStructured noise removal\nFigure 4: Structured noise removal experiment with environments having the same number of lines: a) average\nperplexity over all hyper-parameters b) average perplexity as a function of the number of training steps (for learning\nrate 10‚àí5), c) Probability that iLM is better than eLM when compared on the same hyper-parameters\nRoBERT a], learning-rates: [1e‚àí5,5e‚àí5], number\nof training steps: [10,50,100,200,1000,2500],\n5 random restarts with different random seeds.\nThis gives 2 ¬∑2 ¬∑6 ¬∑5 = 120 experimental param-\neters, ran for eLM, iLM, mtLM, and ensLM\nwhile varying the relative sizes of environments\nin [10,25,30,50,70,75,90,100] resulting in 3840\nexperiments.\nResults for mtLM and ensLM. In Fig. 4, we re-\nport the average bias as a function of the relative\nsizes of environments for mtLM and ensLM along-\nside those of iLM and eLM. We also observe here\nthat iLM outperform other architectures. Interest-\ningly, ensLM seems to bring benefits in comparison\nto eLM and mtLM.\nDetails about the results. Here, we report comple-\nmentary analysis compared to the results described\nin the paper. We report the performance of eLM\nand iLM as a function of the number of training\nsteps and the probability that iLM is better then\neLM when matched on hyper-parameter configura-\ntion as computed by the Bradley-Terry model. This\nis reported by Fig. 5 for two relative size: 25% (the\nmodified environment has 4 times fewer examples)\nand 100%.\nPerplexities after training. To ensure that the\ngender-based correlations were not removed at the\ncost of a worse perplexity, we report in Table 5\nthe perplexities of iLM models in comparison eLM\nones on the test set of Wikitext-2. For reference,\nbefore our training distilBERT and RoBERT a had,\nthis same test set, perplexities of 14.25 and 6.92,\nrespectively.\nIn Table 5, the 95% confidence intervals all give\nuncertainties ‚âà0.15, meaning that for a fixed base\nmodel (distilBERT or RoBERT a) all perplexities are\nwithin each other‚Äôs error bounds. There is no signif-\nicant perplexity difference between eLM and iLM\nor between the unbalanced and balanced setups.\nB.3 Out-of-domain Generalization\nData. The data used for this experiment comes\nfrom subsamples of thePile (Gao et al., 2020).\nWe randomly selected train and test domains as\nfollow:\n5740\nDistilBERT RoBERTa\n0.2\n0.3\n0.4\n0.5\nUnbalanced setup (25%) \n a) Average bias\niLM eLM\n0 500 1000 1500 2000 2500\n0.2\n0.3\n0.4\n0.5b) Average bias\niLM\neLM\nDistilBERT\nRoBERTa\nDistilBERT RoBERTa\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0c) (iLM beats eLM)\n= 0.5\nDistilBERT RoBERTa\nBase model\n0.2\n0.3\n0.4\n0.5\nBalanced setup \n  d) Average bias\n0 500 1000 1500 2000 2500\nNumber of training steps\n0.2\n0.3\n0.4\n0.5e) Average bias\nDistilBERT RoBERTa\nBase model\n0.0\n0.2\n0.4\n0.6f) (iLM beats eLM)\nControlled correlation removal\nFigure 5: Controlled correlation removal experiment: On the first row, the modified environment is 25% of the\nsize of the unmodified environment. On the second row, both have the same number of samples. On the left-most\ncolumn, average bias over all hyper-parameters. On the center column: average bias as a function of the number of\ntraining steps. On the right-most column: Probability that iLM is less biased than eLM when compared on the same\nhyper-parameters.\nUnbalanced Balanced\niLM RoBERT a 4.16 4.13\niLM distilBERT 5.82 5.81\neLM RoBERT a 4.14 4.14\neLM distilBERT 5.82 5.85\nTable 5: Perplexities of iLM and eLM models after\ntraining.\n‚Ä¢ Train: \"europarl\", \"freelaw\", \"dm mathe-\nmatics\", \"youtubesubtitles\", \"USPTO back-\ngrounds\", \"arxiv\", \"books3\", \"wikipedia(en)\",\n\"stackexchange\", \"hackernews\", \"pile-cc\"\n‚Ä¢ Test: \"github\", \"ubuntu irc\", \"openwebtext2\",\n\"pubmed central\", \"enron emails\", \"pubmed\nabstracts\", \"gutenberg pg-19\"\nHyper-parameters. We ran the experiments re-\nported in the main paper while varying several\nhyper-parameters: base-model ( œï): [distil BERT ,\nRoBERT a], learning-rates: [1e‚àí5,5e‚àí5], number\nof training steps: [2500,5000,25000,50000], 5\nrandom restarts with different random seeds, for\neLM, mtLM, ensLM, and iLM. This results in\n2 ¬∑2 ¬∑4 ¬∑5 ¬∑4 = 320 experimental models, each eval-\nuated in 3 tasks: in-domain language modeling,\nout-of-domain language modeling, GLUE. This is\na total of 960 experimental setups.\nEvaluation. For the in-domain language modeling\nevaluation, we measure perplexity on 10K held-out\nsentences from each of the train domain. Similarly\nfor out-of-domain language modeling evaluation,\nwe measure perplexity on 10K sentences from each\nof the test domain.\nFor GLUE, we used the default scripts from hug-\ngingface to evaluate trained models from check-\npoints.\nB.4 Languages as Environments\nOne question that might arise from iLM training\nschedule is whether it simply focuses on surface-\nlevel lexical correlations in the data. For example,\nif the lexical correlations are different across envi-\nronments, maybe no correlation remain generaliz-\nable and iLM learns an empty set of correlations.\nTo better demonstrate that iLM operate on latent\nvariable and not on surface-level correlations, we\nperform a simple experiment with languages as\nenvironments.\nDescription. We use two languages with no lexical\noverlap: English and Farsi. We put english Wikipe-\ndia articles as one environment and farsi Wikipedia\narticles as the other. In this setup, no surface-level\ncorrelations can generalize across environment as\nthe two environments don‚Äôt even have the same\nvocabulary.\nWe train iLM with a multilingual pre-trained\nRoBERT a: XLM- RoBERT a for 5000 steps with\nthese two environments of equal size (10K arti-\ncles per language). Then, we test whether this\nchoice of environments destructs previously learn\ncorrelations in the language model by comparing\nperplexities on a balanced held-out test set of en-\nglish and farsi documents against the model before\nfinetuning. If the perplexities decrease, we would\n5741\n1000 2000 3000 4000 5000\nNumber of training steps\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nDistance between\nclassifiers weights\n1e 6\nIn-domain\nOut-domain\nFigure 6: Comparing distance between heads weights in-\nand out-domain as functions of the number of training\nstep. (95% confidence interval from random restart with\ndifferent seeds.)\nconclude that iLM destroy surface-level correla-\ntions.\nResults. We found that before finetuning, XLM-\nRoBERT a had a perplexity of 14.56 on the held-out\ntest set, where iLM could improve it perplexity\ndown to 6.44. This indicates that iLM with environ-\nments having no lexical overlap does not destroy\npreviously learned correlations. It can even im-\nprove its perplexities for each language. A possible\nreason why iLM can even improve so dramatically\ncompared to before finetuning might come from\nthe fact that œïlearns to recognize the languages,\nseparate them and treat them separately. Similar\neffects have been observed in previous work (Guo\net al., 2021) when the correlation between the envi-\nronment index and the target variable is very strong\n(which is the case here).\nB.5 Head dynamics\nThe main components of our framework are the\nheads and their training dynamic. Therefore, we in-\nvestigate aspects related to behaviour of the heads.\nDescription. During training, the loss of each head\nis still entangled with the prediction of every other\nhead. So we wonder whether the heads still capture\ninformation related to the environment it is tied to\nduring training. In particular, we ask (i) whether the\nparameters of the heads for different environments\nare drifting apart during training? Indeed, all heads\nare initialized to the same pretrained weights at the\nbeginning of training. (ii) Are the parameters of\nthe heads predicting which environments are more\nsimilar?\nExperimental setup. To answer these two ques-\ntions in one go, we take two environments A and B\nand split each of them into two new environments\nresulting in A1, A2, B1, and B2 such that A1 and\nA2 are very similar B1 and B2 are very similar but\nAi and Bi are different. We then train iLM with\nthe four environments and, thus, with four heads\nwA1 , wA2 , wB1 , and wB2 . We measure whether the\nheads‚Äô weights can predict the similarities between\nA‚Äôs and B‚Äôs environments.\nDin = 1\n2 (d(wA1 ,wA2 )+ d(wB1 ,wB2 )), (1)\nDout = 1\n4\n‚àë\ni,j\nd(wAi ,wBj ), (2)\nwhere d is the L2 distance between the linearized\nweights of two heads. Then, Din is the average\ndistance between heads tied the same domain, and\nDout is the average distance between heads tied\nto different domains. Remember that in this case,\nthere are 2 domains A and B and 4 environments\nAi and Bi.\nIn this experiment, we randomly select the base\nenvironments A and B from the domains of thePile\n(A is the Enron-Email, and B is PubMed abstract).\nWe create Ai and Bi by randomly subsampling 2\nenvironments of the same size from each domain.\nWe train iLM withRoBERT a for 5000 training steps,\ntaking checkpoints of the heads every 500 steps.\nWe perform 10 random restarts with different seeds\nto uncertainty estimates. In Fig. 6, we report Din\nand Dout as functions of the number of training\nsteps.\nMaths\nEnron Emails\nFreeLaw\nGithub\nHackerNews\nStackExchange\nUSPTO\nWikipedia\nYoutube-Subtitles\n2D MDS embedding of heads\n after 5000 steps\nFigure 7: Heads embeddings: 2D projection of the\nheads parameters similarity structure after training iLM\nwith RoBERT a for 5000 steps with 9 domains. Each dot\nrepresent one head of the model after training and the\nlabels indicate to which domain it is tied to.\nAnalysis. We first notice that indeed the heads\nare drifting apart from each other as training ad-\nvances. More interestingly, the distance between\n5742\nheads from the same domain is significantly much\nsmaller than the distance between heads from dif-\nferent domains. We conclude that heads retain\nenvironment-specific information in their parame-\nters and are predictive of environment similarities.\nNow, we visualize the geometry of head similar-\nity by training iLM with RoBERT a for 5000 steps\nwith 9 environments from thePile: . After train-\ning, we take the heads‚Äô parameters and compute\nthe pairwise distance between all 9 heads and em-\nbed them in 2D with Multi-Dimensional Scaling\nto visualize the similarity structure. The result is\ndepicted in Fig. 7.\n5743"
}