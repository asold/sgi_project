{
  "title": "Multi-Modal Attribute Prompting for Vision-Language Models",
  "url": "https://openalex.org/W4392426151",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5114860063",
      "name": "Xin Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5025651569",
      "name": "Jiamin Wu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5027804200",
      "name": "Anne En-Tzu Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5049974020",
      "name": "Xu Zhou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100648981",
      "name": "Tianzhu Zhang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6791353385",
    "https://openalex.org/W6790019176",
    "https://openalex.org/W4207055979",
    "https://openalex.org/W3107094551",
    "https://openalex.org/W4395027807",
    "https://openalex.org/W4323338501",
    "https://openalex.org/W4394951233",
    "https://openalex.org/W4392223601",
    "https://openalex.org/W4386699374",
    "https://openalex.org/W4360892253",
    "https://openalex.org/W6853690301",
    "https://openalex.org/W4386065742",
    "https://openalex.org/W6856935247",
    "https://openalex.org/W6854061876",
    "https://openalex.org/W4386790226",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W4321021726",
    "https://openalex.org/W4312310776",
    "https://openalex.org/W4390874497",
    "https://openalex.org/W4390872773",
    "https://openalex.org/W4229453513",
    "https://openalex.org/W4390872306",
    "https://openalex.org/W4386071547",
    "https://openalex.org/W6854250507",
    "https://openalex.org/W6849976536",
    "https://openalex.org/W4390190253",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4233762729",
    "https://openalex.org/W6682962330",
    "https://openalex.org/W2981165461",
    "https://openalex.org/W3106542916",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W3213454282",
    "https://openalex.org/W6803872405",
    "https://openalex.org/W3027585699",
    "https://openalex.org/W3169422999",
    "https://openalex.org/W4379382677",
    "https://openalex.org/W4289792608",
    "https://openalex.org/W3209086464",
    "https://openalex.org/W2948968650",
    "https://openalex.org/W4318764497",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3198571508",
    "https://openalex.org/W4386187806",
    "https://openalex.org/W4312651322",
    "https://openalex.org/W6678800043",
    "https://openalex.org/W2126448884",
    "https://openalex.org/W4386138397",
    "https://openalex.org/W2032699694",
    "https://openalex.org/W2170881581",
    "https://openalex.org/W2789209438",
    "https://openalex.org/W6776700526",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6800217721",
    "https://openalex.org/W6857256894",
    "https://openalex.org/W2596142952",
    "https://openalex.org/W12634471",
    "https://openalex.org/W2047643928",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2155904486",
    "https://openalex.org/W2964194231",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W6638677478",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W3037492894",
    "https://openalex.org/W3177096435",
    "https://openalex.org/W6764990469",
    "https://openalex.org/W6763468762",
    "https://openalex.org/W4313175608",
    "https://openalex.org/W6856800273"
  ],
  "abstract": "Pre-trained Vision-Language Models (VLMs), like CLIP, exhibit strong\\ngeneralization ability to downstream tasks but struggle in few-shot scenarios.\\nExisting prompting techniques primarily focus on global text and image\\nrepresentations, yet overlooking multi-modal attribute characteristics. This\\nlimitation hinders the model's ability to perceive fine-grained visual details\\nand restricts its generalization ability to a broader range of unseen classes.\\nTo address this issue, we propose a Multi-modal Attribute Prompting method\\n(MAP) by jointly exploring textual attribute prompting, visual attribute\\nprompting, and attribute-level alignment. The proposed MAP enjoys several\\nmerits. First, we introduce learnable visual attribute prompts enhanced by\\ntextual attribute semantics to adaptively capture visual attributes for images\\nfrom unknown categories, boosting fine-grained visual perception capabilities\\nfor CLIP. Second, the proposed attribute-level alignment complements the global\\nalignment to enhance the robustness of cross-modal alignment for\\nopen-vocabulary objects. To our knowledge, this is the first work to establish\\ncross-modal attribute-level alignment for CLIP-based few-shot adaptation.\\nExtensive experimental results on 11 datasets demonstrate that our method\\nperforms favorably against state-of-the-art approaches.\\n",
  "full_text": "IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 1\nMulti-modal Attribute Prompting for\nVision-Language Models\nXin Liu , Jiamin Wu, Wenfei Yang\n†\nXu Zhou, Tianzhu Zhang\n†\nAbstract—Pre-trained Vision-Language Models (VLMs), like\nCLIP, exhibit strong generalization ability to downstream tasks\nbut struggle in few-shot scenarios. Existing prompting techniques\nprimarily focus on global text and image representations, yet\noverlooking multi-modal attribute characteristics. This limitation\nhinders the model’s ability to perceive fine-grained visual details\nand restricts its generalization ability to a broader range of\nunseen classes. To address this issue, we propose a Multi-modal\nAttribute Prompting method (MAP) by jointly exploring textual\nattribute prompting, visual attribute prompting, and attribute-\nlevel alignment. The proposed MAP enjoys several merits. First,\nwe introduce learnable visual attribute prompts enhanced by\ntextual attribute semantics to adaptively capture visual attributes\nfor images from unknown categories, boosting fine-grained visual\nperception capabilities for CLIP. Second, the proposed attribute-\nlevel alignment complements the global alignment to enhance\nthe robustness of cross-modal alignment for open-vocabulary\nobjects. To our knowledge, this is the first work to establish\ncross-modal attribute-level alignment for CLIP-based few-shot\nadaptation. Extensive experimental results on 11 datasets demon-\nstrate that our method performs favorably against state-of-the-art\napproaches.\nIndex Terms—Few-shot classification, Prompt learning, Vision-\nlanguage model, Attribute.\nI. I NTRODUCTION\nP\nRE-TRAINED Vision-Language Models (VLMs), such as\nCLIP [1] and ALIGN [2], have demonstrated promising\ngeneralization power and transferability on a wide range of\ndownstream tasks [3]–[9], including image classification [1],\nobject detection [10], [11] and 3D understanding [12]–[14].\nThrough contrastive training on a large-scale dataset of image-\ntext pairs, CLIP achieves a global alignment between images\nand textual descriptions by learning a joint embedding space.\nThe robust cross-modal alignment empowers the CLIP model\nwith the open-vocabulary visual recognition capability. In\nCLIP, class-specific weights for open vocabulary classification\ncan be constructed by plugging the class name in a predefined\nprompt template like ‘A photo of a [CLASS].’ Despite its\nimpressive generalization capability, it remains challenging to\nadapt CLIP to downstream tasks in few-shot scenarios. Due\n†Corresponding author.\nXin Liu, Jiamin Wu, Wenfei Yang, and Tianzhu Zhang are with the School\nof Information Science and Technology, University of Science and Tech-\nnology of China, Hefei 230027, China (e-mail: xinliu99@mail.ustc.edu.cn;\njiaminwu@mail.ustc.edu.cn; yangwf@ustc.edu.cn; tzzhang@ustc.edu.cn).\nXu Zhou is with the Sangfor Technologies Inc., Shenzhen 518000, China\n(e-mail: zhouxu@sangfor.com.cn).\nCopyright © 2024 IEEE. Personal use of this material is permitted.\nHowever, permission to use this material for any other purposes must be\nobtained from the IEEE by sending an email to pubs-permissions@ieee.org.\nThe definitive version of this paper can be found\nat: 10.1109/TCSVT.2024.3424566\nto the large number of parameters in CLIP and the limited\nnumber of samples in few-shot task settings, naive fine-tuning\nof the entire model would likely lead to overfitting, resulting\nin performance degradation [15], [16].\nTo enhance the few-shot adaptation capability of CLIP,\nprompting techniques [17]–[23], such as CoOp [16] and Co-\nCoOp [18] have been proposed. These techniques replace hard\ntemplate context with learnable context in combination with\nthe class name to construct the text prompt. The classification\nresult can be obtained by calculating the similarity between the\nglobal image feature and the encoded text prompt. However, as\nshown in Figure 1 (a), these prompting methods rely solely on\nclass names and may struggle to fully encapsulate categorical\nsemantics when new unseen classes emerge, causing an issue\nof ‘lexical weak tie’ where the class name has a tenuous\nlink with its literal semantics. Consider ‘Rocky Road’ as\nan example, which textually resembles ‘rock’ and ‘road’ but\nrefers to a dessert in reality. When introduced as a new class,\nthe classification weight generated by the model may diverge\nfrom its true semantics, potentially causing misclassification.\nTo address this issue, recent works [24]–[26], as shown in\nFigure 1 (b), introduce textual attribute descriptions obtained\nfrom Large Language Models [27]–[29]. These textual at-\ntribute descriptions are appended to the class name to construct\ntext attribute prompts enriched with more semantics. The final\nclassification result is determined by matching scores between\nthe global image feature and the outputs of text attribute\nprompts across categories.\nDespite the performance improvements demonstrated by\nprior methods, two crucial aspects have been overlooked.\n(1) Visual Attribute Modeling. Previous methods rely on\na single global image feature for classification (see Figure\n1 (a) and (b)). However, global image features may fall\nshort in capturing fine-grained visual attribute information\ncrucial for distinguishing visually similar classes in few-\nshot scenarios. As shown in Figure 2, the Moon Orchid and\nJapanese Anemone exhibit quite similar overall appearances,\nmaking it challenging to differentiate between them relying\nsolely on global features. However, distinguishing them be-\ncomes much easier by relying on their distinct leaf shapes\nand reproductive structures. (2) Attribute-Level Alignment.\nThe open-vocabulary visual recognition ability of the CLIP\nmodel stems from its global alignment between global image\nfeatures and textual descriptions. However, when adapted to\nunseen tasks, the global alignment may lack robustness against\ndisruptions from complex image backgrounds and irrelevant\nimage details, hampering the image recognition ability. While\nprevious methods have attempted to model class-specific tex-\narXiv:2403.00219v3  [cs.CV]  11 Jul 2024\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 2\nText Encoder\nMatch\nGlobal image feature\nImage Encoder\nContext + [CLASS Name]\nText prompt Context + [CLASS Name], Attribute1\nContext + [CLASS Name], Attribute2\nContext + [CLASS Name], Attribute3\nText Encoder\nMatch\nText attribute prompts\n(a) (b) (c)\nText Encoder\nMatch\nText attribute prompts\nGlobal image feature\nImage Encoder Image Encoder\nContext + [CLASS Name], Attribute1\nContext + [CLASS Name], Attribute2\nContext + [CLASS Name], Attribute3\nVisual attribute features\nFig. 1: (a) Conventional prompting methods use hand-crafted or learnable context in combination with the class name to\nconstruct the text prompt. (b) Recent methods introduce attribute descriptions to create text attribute prompts containing more\nsemantic content. (c) Our method jointly explores multi-modal attributes and attribute-level alignment, enhancing fine-grained\nvisual perception and achieving attribute-level alignment between images and text categories.\nLeaves with serrated edges\nYellow stamens\nThick, oval-shaped leaves \nTube-like structure(a) (b)\nFig. 2: (a) Moon Orchid and (b) Japanese Anemone exhibit\nstrikingly similar overall appearances. Visual attributes play a\ncrucial role in distinguishing between them, such as the central\nyellow stamens of Japanese Anemone.\ntual attributes, as depicted in Figure 1 (b), they still focus\non alignment with the global image features and fall short in\naddressing disruptions present in images. To address this issue,\nin addition to the global alignment, establishing attribute-\nlevel alignment is imperative, i.e., alignment between fine-\ngrained visual and textual attribute features (see Figure 1 (c)).\nThis alignment empowers the model to selectively emphasize\nthe distinctive visual attribute features described in the textual\nattributes, thereby enhancing the ability to handle disruptions\nin images.\nInspired by the above insights, we propose Multi-modal\nAttribute Prompting (MAP) by jointly exploring textual\nattribute prompting, visual attribute prompting, and attribute-\nlevel alignment to enhance the adaptability of CLIP in down-\nstream few-shot tasks. For textual attribute prompting , we\ngenerate class-specific textual descriptions using a pre-trained\nlarge language model. Subsequently, these textual descriptions\nare utilized to create multiple textual attribute prompts, each\nencompassing context words, the class name, and an attribute\ndescription. It’s challenging to directly capture appropriate dis-\ncriminative visual attributes in an unknown test image without\nprior information. Hence, for visual attribute prompting ,\nfirst, we use learnable initial visual attribute prompts to aggre-\ngate regional features by interacting with image tokens. Then,\nwe utilize the specially designed Adaptive Visual Attribute\nEnhancement (A V AE) module, in which the initial visual\nattribute prompts are enhanced by adaptively selected textual\nattribute prompts. Through interaction with both image tokens\nand textual attribute prompts, visual attribute prompts can\nadaptively capture visual attribute features in an unseen image.\nFinally, we reformulate the attribute-level alignment between\nvisual attribute prompts and textual attribute prompts as an\nOptimal Transport problem [30] and use the Sinkhorn algo-\nrithm [31] to solve it. The ultimate classification result is de-\ntermined by both the global matching score and the attribute-\nlevel matching score. This integration of additional attribute\nalignment, alongside global alignment, achieves multi-level\nrobust alignment between images and text categories.\nOur main contributions can be summarized as follows:\n• We propose Multi-modal Attribute Prompting , which\njointly explores textual attribute prompting, visual at-\ntribute prompting, and attribute-level alignment between\nimages and text categories. To our knowledge, this is\nthe first work to model visual attributes and establish\nattribute-level alignment between images and text cate-\ngories for adapting the pre-trained CLIP model to down-\nstream few-shot tasks.\n• Extensive experimental results on 11 benchmark datasets\ndemonstrate that our method performs favorably against\nstate-of-the-art approaches.\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 3\nII. R ELATED WORKS\nIn this section, we introduce several lines of research in\npre-trained vision-language models and prompt learning.\nA. Vision-Language Models.\nIn recent years, pre-trained vision-language models [3],\n[4], [32]–[36] have shown exceptional performance in diverse\ndownstream tasks. Among them, CLIP [1] stands out as a rep-\nresentative approach. By training its vision and text encoders\nto map both modalities closely in a shared embedding space,\nCLIP establishes a comprehensive global alignment between\nimages and their corresponding textual descriptions, enabling\nopen-vocabulary classification tasks. The classification result\ncan be obtained by computing the similarity scores of the\nglobal image feature with class names encoded by the text\nencoder. However, as classification relies solely on the global\nmatching score, the accuracy may be affected by disruptions\nin images, such as complex backgrounds, especially in few-\nshot settings [37]–[43], where only a few training samples are\navailable. To improve the robustness of cross-modal alignment,\nwe achieve multi-level alignment for CLIP by introducing ad-\nditional attribute-level alignment between dynamically learned\ntextual and visual attribute features. In this manner, our method\nenhances the fine-grained perception capability with the pre-\ntrained global knowledge preserved.\nB. Prompt Learning.\nPrompt learning is initially introduced in the field of natural\nlanguage processing (NLP) [44]–[48]. With language mod-\nels frozen, prompt learning methods effectively facilitate the\nadaptation of pre-trained language models to downstream few-\nshot tasks by involving additional hand-crafted or learnable\nprompt tokens. Prompt learning has recently been employed\nto enhance the adaptation of the CLIP model to downstream\nfew-shot tasks, where limited training samples are available.\nCoOp [16] constructs prompts by concatenating learnable\ncontinuous vectors and class name tokens. CoCoOp [18]\nextends CoOp by further learning a lightweight neural net-\nwork to generate an input-conditional vector for each image,\ntackling the poor generalizability to broader unseen classes\nin CoOp [16]. ProDA [21] optimizes a set of prompts by\nlearning the distribution of prompts. Instead of focusing on\ntext-modal prompts, VPT [49] introduces learnable vectors to\nthe Vision Transformer [50] to refine image features within the\nfrozen vision encoder. DAPT [19], RPO [22], and MaPLe [23]\nimprove the generalization ability of VLMs via multimodal\nprompting. PromptSRC [20] introduces regularization loss to\nprompt learning. These methods rely solely on class names for\ntext prompt construction and may struggle to fully encapsulate\ncategorical semantics.\nC. Textual Attribute Prompts.\nTo enrich the semantic description for different classes, re-\ncent works [24]–[26], instead of relying solely on class names,\nhave shifted towards the utilization of attribute descriptions to\nconstruct textual attribute prompts for each class. This shift is\nfacilitated by the development of pre-trained large language\nmodels (LLMs) like the GPT family [27], [28]. Attribute\ndescriptions can be easily obtained by querying the LLM with\nsuitable question templates. However, these methods focus on\nattributes in text space only, neglecting the modeling of visual\nattributes, leading to limited visual perception capabilities of\nthe model and misalignment between global visual and local\ntextual features. In contrast, we jointly model visual and\ntextual attribute features and establish attribute-level alignment\nbetween images and text categories.\nD. Visual Attributes.\nVisual attributes refer to intuitive properties of objects,\nencompassing low-level semantics (e.g., color, texture, and\nshape) and high-level semantics (e.g., head, body, and tail of\nobjects) [51]. Utilizing visual attributes has led to significant\nprogress in various vision tasks, including image search [52],\nimage recognition [53], and scene understanding [54]. Some\nprevious works on learning attributes [52], [55], [56] usually\nrequire extensive manual attribute annotations, which are\nlabor-intensive. Dealing with this issue, a recent work [57]\ndeveloped an encoder-decoder network to unsupervisedly dis-\ntill high-level attribute-specific vectors without requiring at-\ntribute annotations. V APNet [58] achieves semantic details by\nutilizing local image patches to distill visual attributes from\nthese discovered semantics. Different from these methods,\nour approach uniquely leverages visual prompts to model\nvisual attributes. By incorporating visual attribute prompts\nas learnable tokens within Vision Transformers, our method\ncaptures and aggregates relevant image features effectively.\nIII. M ETHODOLOGY\nIn this section, we first provide a concise overview of\nCLIP [1]. Then, we present a comprehensive introduction to\nour proposed multi-modal attribute prompting, as illustrated in\nFigure 3, including textual attribute prompting, visual attribute\nprompting, and attribute-level alignment. The main symbols\nand instructions are shown in Table I.\nA. Review of CLIP\nThe Contrastive Language-Image Pre-training (CLIP)\nmodel [1] is a well-known vision-language model trained on\nlarge-scale image-text pairs. CLIP consists of two primary\ncomponents: an image encoder ϕ(·) for converting input\nimages into visual embeddings and a text encoder θ(·) for\nencoding textual information. During pre-training, CLIP trains\nencoders using a contrastive loss objective [59], with the\npurpose of achieving a global alignment between images and\ntextual descriptions. The CLIP model can be easily applied to\ndownstream tasks.\nGiven a set V of C class names, the text prompts {ti}C\ni=1\nare formulated as manually designed templates, such as ‘A\nphoto of a [CLASS].’ The classification vectors {wi}C\ni=1 are\nderived by passing text prompts {ti}C\ni=1 to the text encoder:\nwi = θ(ti). Given an image x and its label y, the global image\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 4\nTABLE I\nMAIN SYMBOLS AND INSTRUCTIONS\nSymbol Instruction\nϕ(·) the image encoder\nθ(·) the text encoder\nV set of class names\nC the number of class names\nx the input image\ny the ground-truth label\nf the global image feature\npn\nk the n-th textual attribute prompt of k-th class\ngn\nk encoded n-th textual attribute prompt of k-th class\nGk encoded textual attribute prompts of the k-th class\nlj the j-th ViT layer\nEj image tokens output from j-th ViT layer\nsj [CLS] token output from j-th ViT layer\nUj visual attribute prompts output from j-th ViT layer\nF visual attribute prompts output from ViT\nT∗ the optimal transportation plan\nΓ adaptive visual attribute enhancement module\nψ(·, ·) similarity function\nM the number of visual attribute prompts\nN the number of textual attribute prompts\nL the number of transformer layers in ViT\nQ, K, V queries, keys, and values in the attention layer\nWQ,WK, WV linear projections of the attention layer\n1N N-dimensional all-one vector\np,q discrete distributions\nµ,ν discrete probability vectors\nfeature f is extracted by the image encoder: f = ϕ(x). The\nclassification probability is formulated as\nP(y = i|x) = exp (cos (wi, f) /τ)PC\nj=1 exp (cos (wj, f) /τ)\n, (1)\nwhere τ is a temperature parameter and cos(·, ·) denotes the\ncosine similarity.\nB. Textual Attribute Prompting\nTo address the potential ‘lexical weak tie’ issue of relying\nsolely on class names for text prompt construction, we create\nmultiple textual attribute prompts for each class, which helps\nenrich the semantic content in text prompts.\nAttribute Descriptions. Consistent with previous meth-\nods [24]–[26], we obtain category attribute descriptions by\nquerying a Large Language Model (LLM) using a predefined\nquestion template: ‘What are useful visual features for dis-\ntinguishing a [CLASS] in an image?’ In response, the LLM\nprovides discriminative attribute descriptions for the queried\nclass. We select N descriptions for each class from the query\nresults.\nTextual Attribute Prompt Construction. We formulate N\ntextual attribute prompts for each class by combining attribute\ndescription sentences with a standardized prompt template. For\ninstance, for the k-th class, with the template ‘A photo of\na [CLASS]’ we construct a textual attribute prompt: pn\nk={A\nphoto of a class ( k), tn\nk}, where class ( k) denotes the class\nname corresponding to the k-th class, and tn\nk denotes the\nn-th attribute description for the k-th class. To enhance the\nadaptability of textual attribute prompts, we replace the hand-\ncrafted context, i.e., ‘A photo of a’ with several learnable\ncontext vectors. Following CoOp [16], we use four learnable\nclass-agnostic context vectors, concatenated with the class\nname and attribute description to construct the textual attribute\nprompt. These vectors are optimized during training to better\nadapt to downstream tasks, providing a more flexible context.\nBy feeding the textual attribute prompts into the text encoder\nθ, we can obtain encoded textual attribute prompts:\nGk = {gn\nk |N\nn=1}, gn\nk = θ(pn\nk), (2)\nwhere Gk is the textual attribute prompt set for the k-class.\nC. Visual Attribute Prompting\nTo improve fine-grained visual perception, we model vi-\nsual attributes with visual attribute prompts. However, it is\nchallenging to directly learn discriminative visual attributes\nfor an unknown image without prior information. Therefore,\nwe design an adaptive visual attribute enhancement module\nto adaptively establish visual attribute prompts under the\nguidance of textual attribute information.\nLearnable Visual Attribute Prompts. We model visual\nattributes by introducing M visual attribute prompts U =\n{ui}M\ni=1, where each attribute prompt ui is a randomly ini-\ntialized learnable vector with the dimension of dv. {ui}M\ni=1\nare inserted into the first Vision Transformer (ViT) layer and\nare then propagated into deeper layers. For the j-th ViT layer\nlj, visual attribute prompts Uj−1 output from the ( j-1)-th\nViT layer are concatenated with image tokens Ej−1 and the\nlearnable classification token sj−1 ([CLS]), forming the input\nsequence of the current layer. Formally,\n[sj, Uj, Ej] =lj([sj−1, Uj−1, Ej−1]), j= 1, 2, ..., L, (3)\nwhere [·, ·] indicates the concatenation along the sequence\nlength dimension. In early layers of ViT, the visual at-\ntribute prompts progressively aggregate image regional fea-\ntures through interaction with image tokens facilitated by\nthe attention mechanism. Learnable visual attribute prompts\ncompute similarity with image tokens and aggregate infor-\nmation accordingly. Similar to the [CLS] token in models\nlike BERT [60] and ViT [50], visual prompts can read and\naggregate visual information from image tokens [22]. Previous\nresearch [61], [62] indicates that ViTs will attend to local\ninformation in early layers. This property, together with the\nattention mechanism, helps aggregate image regional features.\nAdaptive Visual Attribute Enhancement Module. A V AE,\nrepresented as Γ, is designed to dynamically refine visual\nattribute prompts with textual attribute guidance for arbitrary\nimages from unseen classes. As the category of the test\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 5\nText encoder\nLarge Language Model\nA photo of a CLASS, attribute 1.\nA photo of a CLASS, attribute 2.\n                           ... \nA photo of a CLASS, attribute N.\nVision Encoder Layers\nVision Encoder Layers\nAdaptive Visual Attribute \nEnhancement Module\nImage TokensWhat are useful visual features for \ndistinguishing a CLASS in an image?\n…\n…\n…\n…\n… ……\n…\n…\n…\n…\n.\n+\nK\nQ\n.V\n…\nClass 1 Class \nCandidate Classes\nInitial Visual Attribute Prompts\nTextual Attribute Prompts Visual Attribute Prompts\nInitial Visual Attribute Prompts\nVisual Attribute Prompts\nRose, Sun Flower,…\n \nMulti-modal Attribute Alignment Module\nSimilarity Score\n…\nClass 2\n…\n…\nVisual Attribute Prompts\nTextual Attribute Prompts\n Match\nSimilarity\nScore\nCosine \nSimilarities\nAlignment Matrix\nAttribute-Aware\nCross-Attention\nMulti-modal Attribute Prompting Multi-modal Attribute Alignment Module\nAdaptive Visual Attribute Enhancement Module\n❄ ❄\n❄ ❄\n❄ Frozen\nFig. 3: The architecture of our method: MAP leverages textual attribute descriptions to construct textual attribute prompts and\nincorporates learnable visual attribute prompts for capturing visual attributes. In the Adaptive Visual Attribute Enhancement\nmodule, initial visual attribute prompts are enhanced by textual attribute prompts via the attribute-aware cross-attention layer.\nThe Multi-modal Attribute Alignment module calculates the similarity score between visual attributes and textual attributes\nwith the optimal transport.\nimage is unknown, we select possibly related textual attribute\nprompts from the most similar classes. Specifically, we first\ncompute the similarities between the global image feature, i.e.,\nthe classification token s, and textual category embeddings\nrepresented by the mean of textual attribute prompts. Based\non these similarities, we select the most similar λ categories as\nthe candidate classes and gather their textual attribute prompts\nas G′ = {gj|λN\nj=1}. Subsequently, the textual attribute prompts\nG′ are employed as the semantic guidance to enhance visual\nattribute prompts at the l-th ViT layer:\n{˜u(l)\ni }M\ni=1 = Γ({ui\n(l)}M\ni=1, G′), (4)\nwhere Γ takes the initial visual attribute prompts {ui(l)}M\ni=1\ngenerated from l-th layer as the input, and refine them con-\nditioned on textual attribute prompts G′. Then the enhanced\nvisual attribute prompt ˜u(l)\ni is inserted into the ( l + 1)-th layer\nfor progressive attribute learning.\nTo better inject the semantic clues of selected textual\nprompts into visual attribute prompts, we design an attribute-\naware cross-attention layer in Γ. Here, the visual attribute\nprompt tokens {ui(l)}M\ni=1 function as queries Q. Simultane-\nously, the textual attribute prompt features G′ of candidate\nclasses are utilized as keys K and values V . The enhanced\nvisual attribute prompt ˜u(l)\ni is formulated as\n˜αij = exp(αij)\nPλN\nj′=1 exp(αij′)\n, αij = u(l)\ni WQ · (gjWK)T\n√dK\n, (5)\n˜u(l)\ni = u(l)\ni +\nλNX\nj=1\n˜αij(gjWV ), i= 1, 2, ··· , λN, (6)\nwhere WQ,WK and WV are linear projections of the atten-\ntion layer. Attention scores ˜αij indicate the correspondence\nbetween visual and textual attribute prompts, emphasizing\nrelevant image-specific semantic attribute patterns for en-\nhancing the visual attribute prompts. After the text-guided\nenhancement, the refined visual attribute prompts {˜u(l)\ni }M\ni=1\nare propagated into the remaining vision encoder layers and\ncontinue to capture visual attributes through interaction with\nimage tokens.\nD. Attribute-Level Alignment\nTo achieve precise alignment between visual attribute\nprompts {ui(L)}M\ni=1 and textual attribute prompts Gk =\n{gn\nk |N\nn=1}, we formulate the attribute-level matching task as\nan Optimal Transport (OT) problem [30]. For simplicity, we\nrefer to {ui(L)}M\ni=1 as F = {fm|M\nm=1} hereafter. Optimal\nTransport (OT) [30] is a powerful tool to measure the distance\nbetween two distributions. Given two sets of feature points\nF = {fm|M\nm=1} and Gk = {gn\nk |N\nn=1}, their distributions can\nbe formulated as p = PM\nm=1 µmδfm, q = PN\nn=1 νnδgn\nk , δfm\nis a Dirac delta function centered at a specific point fm in\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 6\nthe embedding space. Here, µ ∈ RM , ν ∈ RN are two\ndiscrete distribution vectors. We define the cost matrix between\nF = {fm|M\nm=1} and Gk = {gn\nk |N\nn=1} as C ∈ RM×N , where\nCm,n = 1 − ⟨fm, gn\nk ⟩ is the transport cost from fm to gn\nk .\nThe transport cost between p and q is ⟨T, C⟩, where T is\nthe transport plan, and Tm,n is the probability or “flow” of\ntransporting from fm to gn\nk . The goal of OT is to transport p\nto q at the smallest cost with the optimal transport plan T∗:\nT∗ = arg min\nT∈Π(p,q)\n⟨T, C⟩,\ns.t. T1N = µ, TT 1M = ν,\n(7)\nwhere Q(p, q) is the joint distribution with marginals µ and ν,\nand ⟨·, ·⟩ denotes the Frobenius inner product. To accelerate\nthe solving process, we use the Sinkhorn algorithm, which\nintroduces the entropic regularization term to the transport\ncost to encourage smoother solutions: min\nT\n⟨T, C⟩ −γh(T),\nγ is a constant hyperparameter controlling the intensity of\nregularization term. Instead of solving the constrained op-\ntimization directly, the Sinkhorn algorithm [31] employs an\niterative procedure:\nT∗ = diag(U(t))Adiag(V (t)),\nA = exp(−C/γ) (8)\nwhere in the t-th iteration, U(t) = µ/(AV (t − 1)), V (t) =\nν/AT U(t)), with the initiation V (0) =1. With Equation (8),\nwe can obtain T∗ to serve as the alignment matrix, and then\ndefine the final similarity score between the visual attribute\nprompts F and textual attribute prompts Gk as:\nψ(F, Gk) =\nMX\nm=1\nNX\nn=1\n⟨fm, gn\nk ⟩T∗\nm,n, (9)\nwhere ψ(·, ·) denotes the similarity function.\nE. Training Objectives\nBased on the attribute-level alignment, we can classify the\nimage x with fine-grained visual attributes:\nPa(y = i|x) = exp(ψ ((F, Gi) /τ))PC\nj=1 exp(ψ(F, Gj/τ))\n. (10)\nFurthermore, relying on the global alignment in CLIP, the\nprediction probability is computed as\nPg(y = i|x) = exp(cos ((f, gi) /τ))PC\nj=1 exp(cos(f, gj/τ))\n, (11)\nwhere f is the global feature of the image x, i.e., the class\ntoken sL, and gi is the textual categorical embedding of the\ni-th class, i.e., the mean value of textual prompts in Gi. The\nfinal prediction probability is\nP(y = i|x) =Pg(y = i|x) +βPa(y = i|x), (12)\nwhich incorporates both global-level prediction scores and\nadditional attribute-level matching scores, achieving multi-\nlevel robust alignment between images and categorical texts.\nNaturally, the classification loss is formulated as:\nLcls = − 1\nB\nBX\ni=1\nlog(P(y = yi|xi)), (13)\nwhere B is the batch size of image-text pairs, and yi denotes\nthe ground-truth label of the input image xi.\nIV. E XPERIMENTS\nIn this section, we begin by introducing the benchmark set-\ntings and implementation details, followed by a comprehensive\npresentation of the experimental results.\nAll the models used are based on the open-source CLIP [1]\nmodel. We evaluate the adaptation and generalization capa-\nbility of MAP in four distinct settings following previous\nmethdos [16], [18].\nBase-to-novel generalization. Datasets are split into base\nand novel classes. The model is trained on the training dataset,\nwhich is constructed by randomly selecting 16 images per\nclass from base classes. Then the model is evaluated on\nboth base and novel classes. The evaluation encompasses 11\nimage recognition datasets, including Food101 (Foo) [64],\nDTD [65], ImageNet (Img) [66], Caltech101 (Cal) [67], Eu-\nroSAT (Eur) [68], StanfordCars (Car) [69], FGVCAircraft\n(FGV) [70], Flowers102 (Flo) [71], OxfordPets (Pet) [72],\nUCF101 (UCF) [72], and SUN397 (SUN) [73].\nFew-shot image classification. To evaluate the learning\ncapacity under extremely limited supervision, we assess the\nmodel’s performance across varying shot scenarios, namely,\n1, 2, 4, 8, and 16 shots. Similar to the base-to-novel general-\nization setting, we employ the same 11 datasets.\nDomain generalization. To assess the robustness under\ndomain shifts, we train the model using the source dataset\nImageNet and subsequently evaluate its performance on out-\nof-distribution target datasets, namely ImageNet-R (-R) [74],\nImageNet-A (-A) [75], ImageNetV2 (V2) [76], and ImageNet-\nSketch (-S) [77].\nCross-dataset evaluation. In the cross-dataset transfer set-\nting, we train the models on the source dataset ImageNet\nand directly evaluate them on target datasets. Specifically,\nthe target datasets include Food101, DTD, Caltech101, Eu-\nroSAT, StanfordCars, FGVCAircraft, Flowers102, OxfordPets,\nUCF101, and SUN397.\nImplementation Details. In all the experiments, we use the\npre-trained CLIP [1] with ViT-B/16 image encoder backbone\nas the base model. We use the GPT-3.5 as the large language\nmodel. For MAP, we set the number of textual attribute\nprompts N to 4, and the number of visual attribute prompts M\nto 4. The A V AE module is inserted into the 7th transformer\nlayer in the Vision Transformer (ViT). The default value of\nλ is set as 10. β is set as 1. We train the model using the\nSGD optimizer with a learning rate of 0.002. For the base-to-\nnovel generalization setting, the model is trained for 20 epochs\nwith a batch size of 16. For few-shot image classification, the\nmaximum epoch is set to 200 for 16/8 shots, 100 for 4/2 shots,\nand 50 for 1 shot (except for ImageNet, where the maximum\nepoch is fixed to 50).\nA. Base-to-Novel Generalization\nTo demonstrate generalization to label-shift, where labels\nare divided into base and novel classes for each dataset, we\ntrain the model on training datasets constructed by randomly\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 7\nTABLE II\nCOMPARISON WITH CLIP, C OOP AND COCOOP IN THE BASE -TO-NOVEL GENERALIZATION SETTING . THE RESULTS\nDEMONSTRATE THE STRONG GENERALIZABILITY TO NOVEL CLASSES OF OUR MAP. HM: H ARMONIC MEAN TO\nHIGHLIGHT THE GENERALIZATION TRADE -OFF [63]. T HE BEST RESULTS IN EACH COLUMN ARE SHOWN IN BOLD FONT .\n(A) AVERAGE RESULTS\nMethod Base Novel HM\nCLIP 69.34 74.22 71.70\nCoOp 82.69 63.22 71.66\nCoCoOp 80.47 71.69 75.83\nOurs 83.66 75.76 79.36\n(D) DTD\nMethod Base Novel HM\nCLIP 53.24 59.90 56.37\nCoOp 79.44 41.18 54.24\nCoCoOp 77.01 56.00 64.85\nOurs 82.63 66.23 73.53\n(G) OXFORD PETS\nMethod Base Novel HM\nCLIP 91.17 97.26 94.12\nCoOp 93.67 95.29 94.47\nCoCoOp 95.20 97.69 96.43\nOurs 95.43 96.90 96.16\n(J) FOOD 101\nMethod Base Novel HM\nCLIP 90.10 91.22 90.66\nCoOp 88.33 82.26 85.19\nCoCoOp 90.70 91.29 90.99\nOurs 90.30 89.30 89.80\n(B) IMAGE NET\nMethod Base Novel HM\nCLIP 72.43 68.14 70.22\nCoOp 76.47 67.88 71.92\nCoCoOp 75.98 70.43 73.10\nOurs 76.60 70.60 73.48\n(E) EURO SAT\nMethod Base Novel HM\nCLIP 56.48 64.05 60.03\nCoOp 92.19 54.74 68.69\nCoCoOp 87.49 60.04 71.21\nOurs 92.13 76.10 83.33\n(H) STANFORD CARS\nMethod Base Novel HM\nCLIP 63.37 74.89 68.65\nCoOp 78.12 60.40 68.13\nCoCoOp 70.49 73.59 72.01\nOurs 76.70 73.73 75.18\n(K) FGVCA IRCRAFT\nMethod Base Novel HM\nCLIP 27.19 36.29 31.09\nCoOp 40.44 22.30 28.75\nCoCoOp 33.41 23.71 27.74\nOurs 41.63 36.43 38.84\n(C) CALTECH 101\nMethod Base Novel HM\nCLIP 96.84 94.00 95.40\nCoOp 98.00 89.81 93.73\nCoCoOp 97.96 93.81 95.84\nOurs 98.30 93.80 96.00\n(F) UCF101\nMethod Base Novel HM\nCLIP 70.53 77.50 73.85\nCoOp 84.69 56.05 67.46\nCoCoOp 82.33 73.45 77.64\nOurs 86.67 78.77 82.52\n(I) FLOWERS 102\nMethod Base Novel HM\nCLIP 72.08 77.80 74.83\nCoOp 97.60 59.67 74.06\nCoCoOp 94.87 71.75 81.71\nOurs 97.57 75.23 84.95\n(L) SUN397\nMethod Base Novel HM\nCLIP 69.36 75.35 72.23\nCoOp 80.60 65.89 72.51\nCoCoOp 79.74 76.86 78.27\nOurs 82.33 76.30 79.20\nTABLE III\nCOMPARING MAP AGAINST MORE METHODS ON THE\nAVERAGE ACCURACY OVER 11 DATASETS .\nMethod Base Novel HM\nCLIP [1] 69.34 74.22 71.70\nCoOp [16] 82.69 63.22 71.66\nCoCoOp [18] 80.47 71.69 75.83\nProDA [21] 81.56 72.30 76.65\nRPO [22] 81.13 75.00 77.78\nVDT-Adapter [26] 82.48 74.51 78.09\nMaPLe [23] 82.28 75.14 78.55\nMAP 83.66 75.76 79.36\nselecting 16 images per class from base classes. The model is\ntrained using this few-shot sampled data for 3 random seeds,\nand the results are averaged. We evaluate accuracy on test data\ncorresponding to both the base and novel classes and use their\nharmonic mean [63] as the final evaluation metric.\nCompared to CoOp, MAP exhibits higher harmonic mean\naccuracy across all datasets. As shown in Table II, MAP, on\naverage, increases novel accuracy by 12.54% and base accu-\nracy by 0.97%. This demonstrates that MAP not only enhances\nthe model’s generalization to novel classes but also achieves\nbetter alignment between visual and textual modalities within\nbase classes.\nCompared to CoCoOp, MAP demonstrates superior general-\nization to novel classes, achieving an impressive average gain\nof up to 4.07%. When considering both base and novel classes,\nMAP outperforms CoCoOp with an absolute average gain of\n3.53%. Among the 11 datasets, MAP exhibits higher accuracy\nthan CoCoOp in 10 base datasets and 7 novel datasets.\nWe present the average accuracy results across 11 datasets\nfor MAP compared with several other methods in Table III.\nMAP outperforms other methods by a significant margin,\ndemonstrating our superior performance over other methods.\nIt’s worth noting that VDT-Adapter [26], which leverages\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 8\n67\n72\n77\n82\n1 2 4 8 16\nOurs\nCoOp+VPT\nCoOp\nAccuracy (%)\nNumber of shots per class\nAverage over 11 datasets \n65\n68\n71\n74\n1 2 4 8 16\nOurs\nCoOp+VPT\nCoOp\nAccuracy (%)\nNumber of shots per class\nImageNet\n90\n92\n94\n96\n1 2 4 8 16\nOurs\nCoOp+VPT\nCoOp\nAccuracy (%)\nNumber of shots per class\nCaltech101\n24\n30\n36\n42\n1 2 4 8 16\nOurs\nCoOp+VPT\nCoOp\nAccuracy (%)\nNumber of shots per class\nFGVCAircraft\n67\n72\n77\n82\n1 2 4 8 16\nOurs\nCoOp+VPT\nCoOp\nAccuracy (%)\nNumber of shots per class\nStanfordCars\n47\n55\n63\n71\n1 2 4 8 16\nOurs\nCoOp+VPT\nCoOp\nAccuracy (%)\nNumber of shots per class\nDTD\n69\n74\n79\n84\n1 2 4 8 16\nOurs\nCoOp+VPT\nCoOp\nAccuracy (%)\nNumber of shots per class\nUCF101\n79\n82\n85\n88\n1 2 4 8 16\nOurs\nCoOp+VPT\nCoOp\nAccuracy (%)\nNumber of shots per class\nFood101\n87\n89\n91\n93\n95\n1 2 4 8 16\nOurs\nCoOp+VPT\nCoOp\nAccuracy (%)\nNumber of shots per class\nOxfordPets\n57\n67\n77\n87\n1 2 4 8 16\nOurs\nCoOp+VPT\nCoOp\nAccuracy (%)\nNumber of shots per class\nEuroSAT\n79\n84\n89\n94\n1 2 4 8 16\nOurs\nCoOp+VPT\nCoOp\nAccuracy (%)\nNumber of shots per class\nFlowers102\n61\n65\n69\n73\n1 2 4 8 16\nOurs\nCoOp+VPT\nCoOp\nAccuracy (%)\nNumber of shots per class\nSUN397\nFig. 4: Main results of few-shot image classification on 11 datasets. MAP consistently outperforms other CLIP adaptation\nmethods across all datasets, demonstrating the strong few-shot adaptability of MAP.\ntextual attributes obtained from GPT-4 to formulate prompts,\nimproves the novel accuracy compared to CoOp. However,\nit neglects modeling visual attributes and fails to leverage the\nrole of attributes fully. MAP outperforms VDT-Adapter 1.18%\nin base classes and 1.25% in novel classes.\nB. Few-Shot Image Classification\nTo evaluate few-shot learning ability, we adopt the few-\nshot evaluation protocol from CLIP [1], utilizing 1, 2, 4, 8,\nand 16 shots per class for training and deploying models\nin full test sets. Figure 4 summarizes the performance of\nMAP in few-shot learning on 11 datasets. Each plot compares\nMAP with CoOp and CoOp+VPT. CoOp+VPT refers to the\ncombination of CoOp and VPT, i.e., the integration of both\nlearnable text prompts and learnable visual prompts [49]\ninto the CLIP model simultaneously. In terms of the overall\nperformance (Figure 4, top-left), compared to CoOp, the\ncombination of CoOp and VPT shows some improvement,\nthough not significant. However, in the 1-shot setting, the\nperformance of the combination is even worse than CoOp\nalone. This suggests that simply introducing more learnable\nparameters in the vision encoder brings limited performance\nimprovement in the extreme few-shot setting. However, MAP\nconsistently delivers significant performance improvements,\neven in scenarios with very few training samples (e.g., 1-shot),\nshowcasing the effectiveness of our visual attribute prompts\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 9\n46\n56\n66\n76\n1 2 4 8 16\nOurs\nMaPLe\nLinear probe CLIP\nTip-adapter\nTip-adapter-F\nAccuracy (%)\nNumber of shots per class\nAverage over 11 datasets \nFig. 5: The average few-shot image classification results of\nmore methods across 11 datasets.\nenhanced by textual guidance. Furthermore, on certain datasets\n(Caltech101, Flowers102, DTD, SUN397, and OxfordPets),\nCoOp+VPT does not outperform CoOp alone, whereas MAP\nconsistently achieves superior performance across all bench-\nmark datasets, demonstrating the generalizability of MAP\nacross diverse datasets.\nIn Figure 5, we present the performance results of additional\nmethods for few-shot image classification. Tip-adapter-F [78],\nthe fine-tuned version of Tip-adapter, requires fine-tuning on\nthe few-shot training data to update the adapter. The results\nshow that Tip-adapter-F consistently achieves better perfor-\nmance than Tip-adapter and Linear probe CLIP. MaPLe [23]\nachieves performance comparable to Tip-adapter-F overall.\nNotably, MAP consistently outperforms both MaPLe [23]\nand Tip-adapter-F [78] in few-shot image classification across\nvarious shot settings, highlighting the effectiveness of our\nproposed approach.\nC. Domain Generalization\nTo evaluate the model’s robustness under domain shifts,\nwe initially train the model using the source dataset, Im-\nageNet [66]. Subsequently, we evaluate its performance on\ntarget out-of-distribution datasets, namely ImageNetV2 [76],\nImageNet-Sketch [77], ImageNet-A [75] and ImageNet-\nR [74]. The overall results are summarized in Table IV. From\nthe experimental results, the fully fine-tuned CLIP model\nshows poorer performance compared to the zero-shot CLIP\non the ImageNet dataset and variants of ImageNet. This\ndemonstrates that naive fine-tuning of the entire CLIP model\nmay cause overfitting on the training set, leading to perfor-\nmance degradation. MAP achieves remarkable performance\non unseen data compared to zero-shot CLIP [1], linear probe\nCLIP, CoOp [16] and CoCoOp [18]. Compared to MaPLe,\nMAP shows slightly lower performance on ImageNet-Sketch\nbut outperforms MaPLe [23] on other target datasets (Ima-\nNumber of shots per class\nAbsolute improvement (%)\n-0.3 0.2 0.7 1.2\n1\n2\n4\n8\n16\nFig. 6: The absolute accuracy improvements provided by using\nA V AEcompared to scenarios without A V AE.\ngeNetV2, ImageNet-A, and ImageNet-R). This underscores\nthe robustness of MAP to domain shifts.\nD. Cross-Dataset Evaluation\nTo demonstrate the model’s capacity for generalization\nbeyond a single dataset, we conduct training on ImageNet [66]\nand subsequently evaluate its performance on the other 10\ndatasets. When transferring to other datasets, textual attribute\nprompts are constructed using class attribute descriptions of\nthe target dataset classes, which are also collected from the\nLLM. The learned parameters can be directly transferred,\nallowing effective inference despite category differences be-\ntween the source and target datasets. Table V presents a\ncomprehensive overview of the performance comparison be-\ntween MAP and previous methodologies on the cross-dataset\nevaluation benchmark. On the source dataset, MAP achieves\nthe highest score, underscoring its effectiveness in the source\ndomain. When compared with CoOp [16], CoCoOp [18], and\nMaPLe [23], MAP demonstrates a superior capacity for gen-\neralization across diverse datasets. Specifically, it outperforms\nthese methodologies in 7 out of 10, 6 out of 10, and 6 out\nof 10 datasets, respectively. This suggests that MAP exhibits\nrobustness to varied data distributions.\nE. Ablation Study\nIn this section, we perform ablation studies to demonstrate\nthe effectiveness of each design of the proposed method.\nEffectiveness of Attribute Prompts. We denote Textual\nAttribute Prompts as TAP and Visual Attribute Prompts as\nV AP. We remove TAP and V AP from MAP as our baseline.\nThe results in Table VI are analyzed as follows: (1) Compared\nto the baseline, utilizing TAP powered by the LLM effectively\nimproves the novel accuracy, achieving an accuracy gain\nof 1.43%, which demonstrates textual attributes enrich the\nsemantics for novel classes. (2) The incorporation of V AP\nshows a distinct performance boost on both base (+1.6%) and\nnovel classes (+2.11%). This proves that V AP contributes to\nenhancing fine-grained visual perception ability by capturing\nvisual attributes.\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 10\nTABLE IV\nDOMAIN GENERALIZATION EVALUATION . M ETHODS ARE TRAINED ON THE SOURCE DATASET IMAGE NET AND\nEVALUATED ON DATASETS WITH DOMAIN SHIFTS , INCLUDING IMAGE NETV2, I MAGE NET-S, I MAGE NET-A, AND\nIMAGE NET-R.\nSource Target\nImageNet ImageNetV2 ImageNet-S ImageNet-A ImageNet-R Avg.\nCLIP [1] 66.73 60.83 46.15 47.77 73.96 57.18\nFully Fine-Tuned CLIP 61.65 52.70 26.10 17.55 50.15 36.63\nLinear probe CLIP [1] 67.42 57.19 35.97 36.19 60.10 47.36\nCoOp [16] 71.51 64.20 47.99 49.71 75.21 59.28\nCoCoOp [18] 71.02 64.07 48.75 50.63 76.18 59.91\nMaPLe [23] 70.72 64.07 49.15 50.90 76.98 60.27\nMAP 71.60 64.47 49.07 51.07 77.37 60.49\nTABLE V\nCROSS -DATASET EVALUATION . MODELS ARE TRAINED ON IMAGE NET AND EVALUATED ON TARGET DATASETS . MAP\nACHIEVES OVERALL FAVORABLE PERFORMANCE .\nSource Target\nImageNet Cal Pet Car Flo Foo Air SUN DTD Eur UCF\nCoOp [16] 71.51 93.70 89.14 64.51 68.71 85.30 18.47 64.15 41.92 46.39 66.55\nCoCoOp [18] 71.02 94.43 90.14 65.32 71.88 86.06 22.94 67.36 45.73 45.37 68.21\nMaPLe [23] 70.72 93.53 90.49 65.57 72.23 86.20 24.74 67.01 46.49 48.06 68.69\nMAP 71.60 93.93 90.80 63.00 68.40 86.07 24.87 68.10 51.87 42.63 68.73\n（a) Moon Orchid\n（b) Japanese Anemone\n(c) Egyptian Mau\n(d) Abyssinian\nFig. 7: The visualization of visual attribute prompts. Guided by textual attribute semantics, visual attribute prompts focus on\ndistinctive visual details, such as the different leaf shapes of the Moon Orchid and Japanese Anemone, the spotted coat of the\nEgyptian Mau, and the large ears of the Abyssinian.\n73\n75\n77\n79\n81\n83\n1 2 3 4 5 6 7 8 9 10 11\n Different ViT Layers\nAccuracy (%)\nFig. 8: The impact of inserting A V AEinto different layers of\nViT with 1 shot per class.\n75\n77\n79\n81\n83\n0 1 2 4 6 8 10\nAccuracy (%)\nThe number of visual attribute prompts\nFig. 9: The impact of the number of visual attribute prompts\nin the base-to-novel generalization setting.\nEffectiveness of Adaptive Visual Attribute Enhance-\nment. To verify the accuracy improvement when using A V AE,\nwe conduct few-shot image classification experiments on\n6 datasets (Flowers102, DTD, UCF101, OxfordPets, Cal-\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 11\nFig. 10: The impact of the number of textual attribute prompts\nper class in the base-to-novel generalization setting.\nTABLE VI\nABLATION RESULTS .\nMethod Base Novel HM\nBaseline 82.20 72.22 76.41\n+TAP(LLM) 82.06 73.65 77.36\n+TAP+V AP (MAP) 83.66 75.76 79.36\nTABLE VII\nCOMPLEXITY RESULTS .\nCoCoOp MaPLe MAP\nparameters 0.04M 3.56M 0.74M\nGFLOPs 83.83 55.23 84.80\ntest time 56.70s 9.58s 9.79s\nTABLE VIII\nTHE IMPACT OF USING DIFFERENT LLM S.\nMethod Base Novel HM\nQwen-1.8B-Chat 97.47 73.23 83.63\nGPT-3.5 97.57 75.23 84.95\nQwen1.5-72B-Chat 97.77 75.30 85.08\ntech101, Food101). As shown in Figure 6, the employment\nof A V AE brings remarkable performance gains. Furthermore,\nwe investigate the impact of placing A V AE into different\nViT layers. As observed from Figure 8, placing A V AE in\nthe middle layers (Layer 6-8) attains superior performance.\nWhen applying A V AE in the shallow or deep layers, the perfor-\nmance deteriorates obviously compared to the middle layers.\nTherefore, the A V AE module should be placed in the middle\nlayers. Initial visual attribute prompts can aggregate visual\nregional features in shallow layers and continue to capture\nvisual attributes in the remaining layers after enhancement by\nA V AE.\nAnalysis of Number of Visual Attribute Prompts. Figure\n9 illustrates the averaged harmonic mean accuracy of using\nvarying numbers of visual prompts over 10 datasets in the\nbase-to-novel generalization setting. When the number is as\nsmall as 1, the performance gain is quite limited. The accu-\nracy increases with more visual attribute prompts, as more\nvisual attribute characteristics can be captured. However, the\naccuracy decreases slightly when the number is beyond 4, as\nan excessive amount of visual attribute prompts may contain\nredundancy and noises.\nAnalysis of Number of Textual Attribute Prompts. Fig-\nure 10 illustrates the averaged harmonic accuracy of using\ndifferent numbers of textual attribute prompts. According to\nthe experimental results, the introduction of textual attribute\nprompts indeed improves the performance, demonstrating\nthe effectiveness of textual attribute prompts. The accuracy\nimproves with the incorporation of more textual attribute\nprompts, as this introduces more descriptive information.\nHowever, when the number of textual attribute prompts ex-\nceeds four, the performance decreases. This may be attributed\nto the fact that additional prompts introduce more redundancy.\nThe initial prompts are usually the most relevant and effective,\nwhile later ones may include less useful or intuitive descrip-\ntions. Increased complexity and less discriminative attributes\nlike size or height can also burden the model, resulting in\nreduced performance. Overall, the accuracy changes relatively\nsmoothly with different prompt numbers.\nImpact of Different LLMs. We conduct experiments using\nother large language models (LLMs), specifically Qwen-1.8B-\nChat and Qwen-1.5-72B-Chat [79], and examine performance\nvariations on the Flowers102 dataset. The results in Table VIII\nshow that Qwen-1.5-72B-Chat achieves performance compara-\nble to GPT-3.5. However, when using Qwen-1.8B-Chat, there\nis a significant performance drop compared to using GPT-3.5\nand Qwen-1.5-72B-Chat. This decline may be attributed to\nthe fact that the outputs from Qwen-1.8B-Chat are sometimes\ninconsistent, noisy, and occasionally lack meaningful infor-\nmation. These findings suggest that selecting a large language\nmodel capable of generating consistent and clear outputs is\ncrucial for maintaining performance.\nAnalysis of Complexity. We compare different prompting\nmethods about the number of parameters, the GFLOPs, and\nthe test time in Table VII. MaPLe [23] and MAP enjoy faster\ninference speeds than CoCoOp [18]. Compared with MaPLe,\nMAP is more parameter-efficient (0.74M vs 3.56M). The\ncomputation cost (GFLOPs) of MAP is higher, but considering\nthe performance improvement, it is acceptable.\nVisualization of Visual Attribute Prompts. We visualize\nvisual attribute prompts output by the Vision Transformer in\nFigure 7. It can be observed that different visual attribute\nprompts focus on various aspects of the image and highlight\ndistinctive visual details. This visualization demonstrates the\ncapacity of visual attribute prompts to augment the model’s\nfine-grained visual perception ability.\nV. L IMITATION AND FUTURE WORK\nWe use text attributes directly from GPT without manual\nfiltering. Text attributes may contain noise that may hinder\naccurate classification, such as attributes with high uncertainty,\nlike colors of toad lilies (white, purple, pink, or yellow). On\nFlowers102 [71], we manually filter improper attributes, result-\ning in an improvement of 0.37% in HM. Filtering improper\nones has the potential to improve results. We’ll design an\nautomatic filter plan in the future.\nVI. C ONCLUSION\nIn this paper, we propose a Multi-modal Attribute Prompt-\ning method to adapt pre-trained Vision-Language models for\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 12\ndownstream few-shot tasks. Our method involves modeling\nvisual attributes to enhance the visual fine-grained perception\nability. We establish attribute-level alignment, complementing\nthe global alignment to achieve multi-level robust alignment\nbetween images and text categories. Extensive experimental\nresults demonstrate the effectiveness.\nACKNOWLEDGMENTS\nThis work was supported by National Defense Basic Sci-\nentific Research Program of China (JCKY2020903B002), Na-\ntional Natural Science Foundation of China (62306294), An-\nhui Provincial Natural Science Foundation (2308085QF222),\nChina Postdoctoral Science Foundation (2023M743385) and\nYouth Innovation Promotion Association CAS.\nREFERENCES\n[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable\nvisual models from natural language supervision,” in International\nConference on Machine Learning . PMLR, 2021, pp. 8748–8763.\n[2] C. Jia, Y . Yang, Y . Xia, Y .-T. Chen, Z. Parekh, H. Pham, Q. Le, Y .-H.\nSung, Z. Li, and T. Duerig, “Scaling up visual and vision-language\nrepresentation learning with noisy text supervision,” in International\nConference on Machine Learning . PMLR, 2021, pp. 4904–4916.\n[3] T. Mei, J. J. Corso, G. Kim, J. Luo, C. Shen, and H. Zhang, “Guest\neditorial introduction to the special section on video and language,”IEEE\nTransactions on Circuits and Systems for Video Technology , vol. 32,\nno. 1, pp. 1–4, 2022.\n[4] W. Zhang, C. Ma, Q. Wu, and X. Yang, “Language-guided navigation\nvia cross-modal grounding and alternate adversarial learning,” IEEE\nTransactions on Circuits and Systems for Video Technology , vol. 31,\nno. 9, pp. 3469–3481, 2020.\n[5] Z. Wei, Z. Zhang, P. Wu, J. Wang, P. Wang, and Y . Zhang, “Fine-\ngranularity alignment for text-based person retrieval via semantics-\ncentric visual division,” IEEE Transactions on Circuits and Systems for\nVideo Technology, 2024.\n[6] H. Zhu, C. Zhang, Y . Wei, S. Huang, and Y . Zhao, “Esa: External space\nattention aggregation for image-text retrieval,” IEEE Transactions on\nCircuits and Systems for Video Technology , 2023.\n[7] W. Zhou and Z. Zhou, “Unsupervised domain adaption harnessing\nvision-language pre-training,” IEEE Transactions on Circuits and Sys-\ntems for Video Technology, 2024.\n[8] X. Lin, M. Zhu, R. Dang, G. Zhou, S. Shu, F. Lin, C. Liu, and Q. Chen,\n“Clipose: Category-level object pose estimation with pre-trained vision-\nlanguage knowledge,” IEEE Transactions on Circuits and Systems for\nVideo Technology, 2024.\n[9] L. Wang, H. Qiu, B. Qiu, F. Meng, Q. Wu, and H. Li, “Tridentcap:\nImage-fact-style trident semantic framework for stylized image caption-\ning,” IEEE Transactions on Circuits and Systems for Video Technology ,\n2023.\n[10] R. Arandjelovi ´c, A. Andonian, A. Mensch, O. J. H ´enaff, J.-B. Alayrac,\nand A. Zisserman, “Three ways to improve feature alignment for open\nvocabulary detection,” arXiv preprint arXiv:2303.13518 , 2023.\n[11] P. Kaul, W. Xie, and A. Zisserman, “Multi-modal classifiers for open-\nvocabulary object detection,” in International Conference on Machine\nLearning. PMLR, 2023, pp. 15 946–15 969.\n[12] S. Peng, K. Genova, C. Jiang, A. Tagliasacchi, M. Pollefeys,\nT. Funkhouser et al. , “Openscene: 3d scene understanding with open\nvocabularies,” inProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2023, pp. 815–824.\n[13] C. Zhu, W. Zhang, T. Wang, X. Liu, and K. Chen, “Object2scene: Putting\nobjects in context for open-vocabulary 3d detection,” arXiv preprint\narXiv:2309.09456, 2023.\n[14] A. Takmaz, E. Fedele, R. W. Sumner, M. Pollefeys, F. Tombari, and\nF. Engelmann, “Openmask3d: Open-vocabulary 3d instance segmenta-\ntion,” in Advances in Neural Information Processing Systems 36: Annual\nConference on Neural Information Processing Systems , 2023.\n[15] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y . Zhang, H. Li, and Y . Qiao,\n“Clip-adapter: Better vision-language models with feature adapters,”\nInternational Journal of Computer Vision , vol. 132, no. 2, pp. 581–595,\n2024.\n[16] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Learning to prompt for vision-\nlanguage models,” International Journal of Computer Vision , vol. 130,\nno. 9, pp. 2337–2348, 2022.\n[17] C. Ma, Y . Liu, J. Deng, L. Xie, W. Dong, and C. Xu, “Understanding\nand mitigating overfitting in prompt tuning for vision-language models,”\nIEEE Transactions on Circuits and Systems for Video Technology, 2023.\n[18] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Conditional prompt learning\nfor vision-language models,” in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , 2022, pp. 16 816–\n16 825.\n[19] E. Cho, J. Kim, and H. J. Kim, “Distribution-aware prompt tuning for\nvision-language models,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2023, pp. 22 004–22 013.\n[20] M. U. Khattak, S. T. Wasim, M. Naseer, S. Khan, M.-H. Yang, and F. S.\nKhan, “Self-regulating prompts: Foundational model adaptation without\nforgetting,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision , 2023, pp. 15 190–15 200.\n[21] Y . Lu, J. Liu, Y . Zhang, Y . Liu, and X. Tian, “Prompt distribution\nlearning,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2022, pp. 5206–5215.\n[22] D. Lee, S. Song, J. Suh, J. Choi, S. Lee, and H. J. Kim, “Read-\nonly prompt optimization for vision-language few-shot learning,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2023, pp. 1401–1411.\n[23] M. U. Khattak, H. Rasheed, M. Maaz, S. Khan, and F. S. Khan,\n“Maple: Multi-modal prompt learning,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2023, pp.\n19 113–19 122.\n[24] Z. Feng, A. Bair, and J. Z. Kolter, “Leveraging multiple descrip-\ntive features for robust few-shot image learning,” arXiv preprint\narXiv:2307.04317, 2023.\n[25] S. Menon and C. V ondrick, “Visual classification via description from\nlarge language models,” in International Conference on Learning Rep-\nresentations,, 2023.\n[26] M. Maniparambil, C. V orster, D. Molloy, N. Murphy, K. McGuinness,\nand N. E. O’Connor, “Enhancing clip with gpt-4: Harnessing visual\ndescriptions as prompts,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2023, pp. 262–271.\n[27] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askellet al., “Language models\nare few-shot learners,” Advances in Neural Information Processing\nSystems, vol. 33, pp. 1877–1901, 2020.\n[28] R. OpenAI, “Gpt-4 technical report. arxiv 2303.08774,” View in Article,\n2023.\n[29] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang,\nJ. Zhang, Z. Dong et al. , “A survey of large language models,” arXiv\npreprint arXiv:2303.18223, 2023.\n[30] C. Villani, Optimal transport: old and new . Springer, 2009, vol. 338.\n[31] M. Cuturi, “Sinkhorn distances: Lightspeed computation of optimal\ntransport,” Advances in neural information processing systems , vol. 26,\n2013.\n[32] J. Yu, J. Li, Z. Yu, and Q. Huang, “Multimodal transformer with multi-\nview visual representation for image captioning,” IEEE transactions on\ncircuits and systems for video technology , vol. 30, no. 12, pp. 4467–\n4480, 2019.\n[33] Z. Yang, T. Kumar, T. Chen, J. Su, and J. Luo, “Grounding-tracking-\nintegration,” IEEE Transactions on Circuits and Systems for Video\nTechnology, vol. 31, no. 9, pp. 3433–3443, 2020.\n[34] A. Singh, R. Hu, V . Goswami, G. Couairon, W. Galuba, M. Rohrbach,\nand D. Kiela, “Flava: A foundational language and vision alignment\nmodel,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2022, pp. 15 638–15 650.\n[35] X. Zhai, X. Wang, B. Mustafa, A. Steiner, D. Keysers, A. Kolesnikov,\nand L. Beyer, “Lit: Zero-shot transfer with locked-image text tuning,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 18 123–18 133.\n[36] L. Yuan, D. Chen, Y .-L. Chen, N. Codella, X. Dai, J. Gao, H. Hu,\nX. Huang, B. Li, C. Li et al. , “Florence: A new foundation model for\ncomputer vision,” arXiv preprint arXiv:2111.11432 , 2021.\n[37] W. Jiang, K. Huang, J. Geng, and X. Deng, “Multi-scale metric learning\nfor few-shot learning,” IEEE Transactions on Circuits and Systems for\nVideo Technology, vol. 31, no. 3, pp. 1091–1102, 2020.\n[38] M. Cheng, H. Wang, and Y . Long, “Meta-learning-based incremental\nfew-shot object detection,” IEEE Transactions on Circuits and Systems\nfor Video Technology, vol. 32, no. 4, pp. 2158–2169, 2021.\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 13\n[39] X. Wang, X. Wang, B. Jiang, and B. Luo, “Few-shot learning meets\ntransformer: Unified query-support transformers for few-shot classifica-\ntion,” IEEE Transactions on Circuits and Systems for Video Technology,\n2023.\n[40] R. Xu, L. Xing, S. Shao, L. Zhao, B. Liu, W. Liu, and Y . Zhou,\n“Gct: Graph co-training for semi-supervised few-shot learning,” IEEE\nTransactions on Circuits and Systems for Video Technology , vol. 32,\nno. 12, pp. 8674–8687, 2022.\n[41] M. Zhang, M. Shi, and L. Li, “Mfnet: Multiclass few-shot segmentation\nnetwork with pixel-wise metric learning,” IEEE Transactions on Circuits\nand Systems for Video Technology, vol. 32, no. 12, pp. 8586–8598, 2022.\n[42] C. Zhang, C. Li, and J. Cheng, “Few-shot visual classification using\nimage pairs with binary transformation,” IEEE Transactions on Circuits\nand Systems for Video Technology, vol. 30, no. 9, pp. 2867–2871, 2019.\n[43] Z. Dang, M. Luo, C. Jia, C. Yan, X. Chang, and Q. Zheng, “Counter-\nfactual generation framework for few-shot learning,” IEEE Transactions\non Circuits and Systems for Video Technology , 2023.\n[44] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How can we know\nwhat language models know?” Transactions of the Association for\nComputational Linguistics, vol. 8, pp. 423–438, 2020.\n[45] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts\nfor generation,” in Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing , 2021, pp. 4582–\n4597.\n[46] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for\nparameter-efficient prompt tuning,” in Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language Processing, , pp.\n3045–3059.\n[47] Y . Gu, X. Han, Z. Liu, and M. Huang, “PPT: pre-trained prompt tuning\nfor few-shot learning,” in Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics , 2022, pp. 8410–8423.\n[48] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang, “Gpt\nunderstands, too,” AI Open, 2023.\n[49] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan,\nand S.-N. Lim, “Visual prompt tuning,” in European Conference on\nComputer Vision. Springer, 2022, pp. 709–727.\n[50] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words: Trans-\nformers for image recognition at scale,” in International Conference on\nLearning Representations, ICLR 2021 .\n[51] V . Ferrari and A. Zisserman, “Learning visual attributes,” Advances in\nneural information processing systems , vol. 20, 2007.\n[52] N. Kumar, A. Berg, P. N. Belhumeur, and S. Nayar, “Describable visual\nattributes for face verification and image search,” IEEE Transactions on\nPattern Analysis and Machine Intelligence , vol. 33, no. 10, pp. 1962–\n1977, 2011.\n[53] S. Wang, Z. Wang, H. Li, J. Chang, W. Ouyang, and Q. Tian, “Accurate\nfine-grained object recognition with structure-driven relation graph net-\nworks,” International Journal of Computer Vision , vol. 132, no. 1, pp.\n137–160, 2024.\n[54] G. Patterson, C. Xu, H. Su, and J. Hays, “The sun attribute database: Be-\nyond categories for deeper scene understanding,” International Journal\nof Computer Vision , vol. 108, pp. 59–81, 2014.\n[55] J. Huang, R. S. Feris, Q. Chen, and S. Yan, “Cross-domain image\nretrieval with a dual attribute-aware ranking network,” in Proceedings of\nthe IEEE international conference on computer vision , 2015, pp. 1062–\n1070.\n[56] H. Zhang, X. Cao, and R. Wang, “Audio visual attribute discovery for\nfine-grained object recognition,” in Proceedings of the AAAI Conference\non Artificial Intelligence , vol. 32, no. 1, 2018.\n[57] X.-S. Wei, Y . Shen, X. Sun, H.-J. Ye, and J. Yang, “Learning attribute-\naware hash codes for large-scale fine-grained image retrieval,” Advances\nin Neural Information Processing Systems , vol. 34, pp. 5720–5730,\n2021.\n[58] S. Wang, J. Chang, H. Li, Z. Wang, W. Ouyang, and Q. Tian, “Learning\nto parameterize visual attributes for open-set fine-grained retrieval,”\nAdvances in Neural Information Processing Systems , vol. 36, 2024.\n[59] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y . Tian, P. Isola,\nA. Maschinot, C. Liu, and D. Krishnan, “Supervised contrastive learn-\ning,” Advances in Neural Information Processing Systems , vol. 33, pp.\n18 661–18 673, 2020.\n[60] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[61] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Dosovitskiy,\n“Do vision transformers see like convolutional neural networks?” Ad-\nvances in neural information processing systems , vol. 34, pp. 12 116–\n12 128, 2021.\n[62] D. Jiang, Y . Liu, S. Liu, X. Zhang, J. Li, H. Xiong, and Q. Tian,\n“From clip to dino: Visual encoders shout in multi-modal large language\nmodels,” 2023.\n[63] Y . Xian, B. Schiele, and Z. Akata, “Zero-shot learning-the good, the\nbad and the ugly,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2017, pp. 4582–4591.\n[64] L. Bossard, M. Guillaumin, and L. Van Gool, “Food-101–mining dis-\ncriminative components with random forests,” in European Conference\non Computer Vision . Springer, 2014, pp. 446–461.\n[65] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi,\n“Describing textures in the wild,” inProceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2014, pp. 3606–3613.\n[66] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition . Ieee, 2009,\npp. 248–255.\n[67] L. Fei-Fei, R. Fergus, and P. Perona, “Learning generative visual models\nfrom few training examples: An incremental bayesian approach tested\non 101 object categories,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition . IEEE, 2004, pp. 178–178.\n[68] P. Helber, B. Bischke, A. Dengel, and D. Borth, “Eurosat: A novel\ndataset and deep learning benchmark for land use and land cover classi-\nfication,” IEEE Journal of Selected Topics in Applied Earth Observations\nand Remote Sensing , vol. 12, no. 7, pp. 2217–2226, 2019.\n[69] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3d object representations\nfor fine-grained categorization,” in Proceedings of the IEEE Interna-\ntional Conference on Computer Vision Workshops , 2013, pp. 554–561.\n[70] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi, “Fine-\ngrained visual classification of aircraft,” arXiv preprint arXiv:1306.5151,\n2013.\n[71] M.-E. Nilsback and A. Zisserman, “Automated flower classification over\na large number of classes,” in Indian Conference on Computer Vision,\nGraphics & Image processing . IEEE, 2008, pp. 722–729.\n[72] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar, “Cats and\ndogs,” in Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. IEEE, 2012, pp. 3498–3505.\n[73] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, “Sun\ndatabase: Large-scale scene recognition from abbey to zoo,” in Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. IEEE, 2010, pp. 3485–3492.\n[74] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo,\nR. Desai, T. Zhu, S. Parajuli, M. Guo et al. , “The many faces of\nrobustness: A critical analysis of out-of-distribution generalization,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 8340–8349.\n[75] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song, “Natural\nadversarial examples,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2021, pp. 15 262–15 271.\n[76] B. Recht, R. Roelofs, L. Schmidt, and V . Shankar, “Do imagenet\nclassifiers generalize to imagenet?” in International Conference on\nMachine Learning. PMLR, 2019, pp. 5389–5400.\n[77] H. Wang, S. Ge, Z. Lipton, and E. P. Xing, “Learning robust global\nrepresentations by penalizing local predictive power,” Advances in\nNeural Information Processing Systems , vol. 32, 2019.\n[78] R. Zhang, W. Zhang, R. Fang, P. Gao, K. Li, J. Dai, Y . Qiao, and H. Li,\n“Tip-adapter: Training-free adaption of clip for few-shot classification,”\nin European conference on computer vision . Springer, 2022, pp. 493–\n510.\n[79] J. B. et al., “Qwen technical report,” arXiv preprint arXiv:2309.16609 ,\n2023.\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. XX, NO. XX, MAY 2024 14\nXin Liu received a bachelor’s degree in Information\nSecurity from the University of Science and Tech-\nnology of China in 2022. She is now pursuing a\nmaster degree in Control Science and Engineering at\nUniversity of Science and Technology of China. Her\nresearch interests include computer vision and deep\nlearning, especially few-shot learning and multi-\nmodal learning.\nJiamin Wu received the bachelor’s degree in the\nSchool of Electronic Engineering, Xidian University,\nXian, Shaanxi, China. She is studying for her doc-\ntorate in the Department of Automation, University\nof Science and Technology of China, Hefei, Anhui,\nChina. Her research interests include pattern recog-\nnition, computer vision and deep learning. She is\ncurrently focusing on zero-shot and few-shot learn-\ning.\nWenfei Yang received the bachelor’s degree in\nElectronic Engineering and Information Science in\n2017, and the Ph.D. degree in pattern recognition\nand intelligent systems from the department of\nAutomation, University of Science and Technology\nof China, Hefei, China, in 2022. Currently, he is\na post-doctor in Control Science and Engineering,\nUniversity of Science and Technology of China. His\ncurrent research interests include computer vision\nand machine learning, especially action detection\nand object detection.\nXu Zhou received the PhD degree in computer\nscience and technology from Huazhong University\nof Science and Technology in 2016. His research\ninterests span the areas of large language model,\nNLP system design and reinforcement learning.\nTianzhu Zhang received the bachelor’s degree in\ncommunications and information technology from\nBeijing Institute of Technology, Beijing, China, in\n2006, and the Ph.D. degree in pattern recognition\nand intelligent systems from the Institute of Au-\ntomation, Chinese Academy of Sciences, Beijing,\nChina, in 2011. Currently, he is a Professor at the\nDepartment of Automation, University of Science\nand Technology of China, Hefei, Anhui, China. His\ncurrent research interests include computer vision\nand multimedia, especially action recognition, object\nclassification, object tracking, and social event analysis.",
  "topic": "Modal",
  "concepts": [
    {
      "name": "Modal",
      "score": 0.687570333480835
    },
    {
      "name": "Computer science",
      "score": 0.5664368867874146
    },
    {
      "name": "Natural language processing",
      "score": 0.42086178064346313
    },
    {
      "name": "Linguistics",
      "score": 0.4101920425891876
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37456345558166504
    },
    {
      "name": "Philosophy",
      "score": 0.11410155892372131
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    }
  ],
  "institutions": []
}