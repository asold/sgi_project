{
  "title": "Fingerprinting Fine-tuned Language Models in the Wild",
  "url": "https://openalex.org/W3173622593",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3020219751",
      "name": "Nirav Diwan",
      "affiliations": [
        "Indraprastha Institute of Information Technology Delhi",
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2128156271",
      "name": "Tanmoy Chakraborty",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2298902016",
      "name": "Zubair Shafiq",
      "affiliations": [
        "University of California, Davis"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3096831026",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2523437799",
    "https://openalex.org/W4288253152",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W3010285678",
    "https://openalex.org/W3034287667",
    "https://openalex.org/W2077414053",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4299574851",
    "https://openalex.org/W3101891351",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W3114326827",
    "https://openalex.org/W2106120499",
    "https://openalex.org/W3155632693",
    "https://openalex.org/W2123407642",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2986729576",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W1507711477",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3085904224",
    "https://openalex.org/W2970295111",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2964191699",
    "https://openalex.org/W2104384523",
    "https://openalex.org/W3010649878",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3046764764",
    "https://openalex.org/W2157331557"
  ],
  "abstract": "There are concerns that the ability of language models (LMs) to generate high quality synthetic text can be misused to launch spam, disinformation, or propaganda.Therefore, the research community is actively working on developing approaches to detect whether a given text is organic or synthetic.While this is a useful first step, it is important to be able to further fingerprint the author LM to attribute its origin.Prior work on fingerprinting LMs is limited to attributing synthetic text generated by a handful (usually < 10) of pre-trained LMs.However, LMs such as GPT2 are commonly fine-tuned in a myriad of ways (e.g., on a domain-specific text corpus) before being used to generate synthetic text.It is challenging to fingerprinting fine-tuned LMs because the universe of fine-tuned LMs is much larger in realistic scenarios.To address this challenge, we study the problem of large-scale fingerprinting of fine-tuned LMs in the wild.Using a real-world dataset of synthetic text generated by 108 different fine-tuned LMs, we conduct comprehensive experiments to demonstrate the limitations of existing fingerprinting approaches.Our results show that fine-tuning itself is the most effective in attributing the synthetic text generated by fine-tuned LMs.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4652–4664\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4652\nFingerprinting Fine-tuned Language Models in the Wild∗\nNirav Diwan1, Tanmoy Chakravorty1, and Zubair Shaﬁq2\n1IIIT-Delhi, India\n2University of California, Davis\n1{nirav17072,tanmoy}@iiitd.ac.in\n2zubair@ucdavis.edu\nAbstract\nThere are concerns that the ability of language\nmodels (LMs) to generate high quality syn-\nthetic text can be misused to launch spam, dis-\ninformation, or propaganda. Therefore, the re-\nsearch community is actively working on de-\nveloping approaches to detect whether a given\ntext is organic or synthetic. While this is a\nuseful ﬁrst step, it is important to be able to\nfurther ﬁngerprint the author LM to attribute\nits origin. Prior work on ﬁngerprinting LMs\nis limited to attributing synthetic text gener-\nated by a handful (usually<10) of pre-trained\nLMs. However, LMs such as GPT2 are com-\nmonly ﬁne-tuned in a myriad of ways (e.g.,\non a domain-speciﬁc text corpus) before being\nused to generate synthetic text. It is challeng-\ning to ﬁngerprinting ﬁne-tuned LMs because\nthe universe of ﬁne-tuned LMs is much larger\nin realistic scenarios. To address this chal-\nlenge, we study the problem of large-scale ﬁn-\ngerprinting of ﬁne-tuned LMs in the wild. Us-\ning a real-world dataset of synthetic text gener-\nated by 108 different ﬁne-tuned LMs, we con-\nduct comprehensive experiments to demon-\nstrate the limitations of existing ﬁngerprinting\napproaches. Our results show that ﬁne-tuning\nitself is the most effective in attributing the\nsynthetic text generated by ﬁne-tuned LMs.\n1 Introduction\nBackground & motivation. State-of-the-art lan-\nguage models (LMs) can now generate long, co-\nherent, and grammatically valid synthetic text\n(Devlin et al., 2019; Radford et al., 2018, 2019;\nBrown et al., 2020). On one hand, the ability\nto generate high quality synthetic text offers a\nfast and inexpensive alternative to otherwise labor-\nintensive useful applications such as summariza-\n∗Supplementary contains dataset, source code, and an\nappendix (including hyper-parameter setting and additional\nresults). The code and dataset are available at https:\n//github.com/LCS2-IIITD/ACL-FFLM .\ntion and chat bots (Yoo and Jeong, 2020; Yu et al.,\n2020; Wang et al., 2019; Liu and Lapata, 2019).\nOn the other hand, such high quality synthetic text\ncan also be misused by bad actors to launch spam,\ndisinformation, or propaganda. For example, LMs\nsuch as Grover (Zellers et al., 2019) are shown to\nbe capable of generating full-blown news articles,\nfrom just brief headlines, which are more believ-\nable than equivalent human written news articles.\nIn fact, prior work has shown that humans cannot\ndistinguish between organic (i.e., human written)\nand synthetic (i.e., generated by LM) text (Ippolito\net al., 2020; Jawahar et al., 2020; Munir et al.,\n2021). Thus, this ability to generate high qual-\nity synthetic text can further be misused for social\nimpersonation and phishing attacks because users\ncan be easily misled about the authorship of the\ntext.\nProblem statement. To mitigate the poten-\ntial misuse of LMs, the research community has\nstarted developing new text attribution techniques.\nHowever, as shown in Figure 1, the attribution of\nsynthetic text is a multistage problem. The ﬁrst\nstep is to distinguish between organic and syn-\nthetic text (P1). Prior work has used the LM’s\noutput word probability distribution to detect syn-\nthetic text (Ippolito et al., 2020; Gehrmann et al.,\n2019; Zellers et al., 2019). However, there are sev-\neral publicly available pre-trained LMs that might\nbe used to generate synthetic text. Thus, the sec-\nond step is to detect the pre-trained LM used to\ngenerate synthetic text (P2). Prior approaches\nshowed promising results by attempting to ﬁnger-\nprint the LM based on its distinct semantic em-\nbeddings (Pan et al., 2020; Uchendu et al., 2020).\nHowever, pre-trained LMs such as GPT2 (Radford\net al., 2019) are commonly ﬁne-tuned before be-\ning used to generate synthetic text. Thus, the third\nstep is to detect the ﬁne-tuned LM used to generate\nsynthetic text (P3). To the best of our knowledge,\n4653\nprior work lacks approaches to effectively ﬁnger-\nprint ﬁne-tuned LMs.\nTechnical challenges. It is particularly challeng-\ning to ﬁngerprint ﬁne-tuned LMs simply because\nof the sheer number of possible ﬁne-tuned vari-\nants. More speciﬁcally, a pre-trained LM can be\nﬁne-tuned in a myriad of ways (e.g., separately\non different domain-speciﬁc text corpora), result-\ning in a large number of classes. Another chal-\nlenge to ﬁngerprint ﬁne-tuned LMs is that we can-\nnot make assumptions about the nature of ﬁne-\ntuning (e.g., parameters, training data) or gener-\nation (e.g., prompts). Prior work on ﬁngerprinting\npre-trained LMs is limited to evaluation on a small\nnumber of classes (<10 classes) and on synthetic\ntext that is artiﬁcially generated using set prompts.\nProposed approach. To ﬁngerprint synthetic\ntext generated by ﬁne-tuned LMs, we utilize the\nRoBERTa model (Liu et al., 2019) and attach a\nCNN-based classiﬁer on top. We ﬁne-tune the\nRoBERTa model for the downstream task of sen-\ntence classiﬁcation using a synthetic text corpus.\nThe ﬁne-tuned model is used to extract embed-\ndings as features that are then fed to the CNN\nclassiﬁer. We show that the ﬁne-tuned RoBERTa\nmodel is able to capture the topic-speciﬁc distin-\nguishing patterns of the synthetic text. Upon visu-\nalizing the generated features, the samples form\nclosely-condensed distinguishable clusters based\non the topic of the organic corpus the LMs have\nbeen ﬁne-tuned upon. Therefore, we conclude that\nﬁne-tuning itself signiﬁcantly helps ﬁngerprinting\na ﬁne-tuned LM. Note that our ﬁngerprinting ap-\nproach does not assume access to the text corpus\nused for LM ﬁne-tuning. We only assume access\nto arbitrary synthetic text generated by ﬁne-tuned\nLMs.\nDataset. We gather a real-world dataset of syn-\nthetic text generated by ﬁne-tuned LMs in the\nwild. More speciﬁcally, we extract synthetic text\nfrom the subreddit r/SubSimulatorGPT2. Each of\nthe 108 users on r/SubSimulatorGPT2 is a GPT2\nLM that is ﬁne-tuned on 500k posts and com-\nments from a particular subreddit (e.g., r/askmen,\nr/askreddit,r/askwomen). It is noteworthy that\nusers on r/SubSimulatorGPT2 organically interact\nwith each other using the synthetic text in the pre-\nceding comment/reply as their prompt.\nEvaluation. We evaluate our models using a suite\nof evaluation metrics. We also adapt conﬁdence-\nbased heuristics, such as the gap statistic. Our best\nFigure 1: Attribution of text formulated as a series of\nthree problems: P1, P2, and P3.\nmodel is accurate for a large number of classes,\nacross a variety of evaluation metrics, showing\nimpressive results for the largest setting of 108\nclasses. While it obtains around 46% precision\nand 43% recall, its top-10 accuracy is about 70%.\nIn other words, the correct class is one of the top-\n10 predictions in about 70% of the cases. If we\ngive the model the option to not make a classiﬁca-\ntion decision, via conﬁdence estimation, it doubles\nthe precision of the top classiﬁcation from 46% to\naround 87% with a decrease of recall from 43% to\n27%.\nWe summarize our key contributions as follows:\n1. Problem formulation. To the best of our\nknowledge, we are the ﬁrst to explore the prob-\nlem of attribution of synthetic text generated by\nﬁne-tuned LMs (P3). We are also the ﬁrst to\ninvestigate synthetic text attribution for a large\nnumber of classes on a real-world dataset. We\nalso show that P3 is a much more challenging\nproblem as compared to P1 and P2.\n2. Comprehensive model. We design and im-\nplement a comprehensive set of different fea-\nture extraction techniques. We use them on a\nvariety of machine learning and deep learning\npipelines to build detection models.\n3. Rigorous evaluation. We conduct rigorous\nevaluation on a real-world dataset of synthetic\ntext generated by ﬁne-tuned LMs. We use sev-\neral evaluation metrics such as top-k accuracy\nand precision-recall tradeoff to compare differ-\nent detection models. We also provide insights\ninto the performance of different feature sets\nand classiﬁcation algorithms.\nPaper Organization: The rest of the paper is or-\nganized as follows. Section 2 contextuailzes our\nwork with respect to prior literature. We analyze\nthe real-world dataset of synthetic text generated\n4654\nby ﬁne-tuned LMs in Section 3. Section 4 de-\nscribes different feature sets and classiﬁcation al-\ngorithms for ﬁngerprinting ﬁne-tuned LMs. Sec-\ntion 5 presents the results of our experimental eval-\nuation before concluding in Section 6.\n2 Related Work\nFigure 1 illustrates three different problem formu-\nlations for attribution of synthetic text generated\nby LMs. The ﬁrst line of research ( P1) aims to\ndistinguish between organic (by humans) and syn-\nthetic (by LMs) text. Given synthetic text, the sec-\nond line of research ( P2) further aims to attribute\nthe synthetic text generated by pre-trained LMs\nsuch as BERT and GPT. Finally, in this paper, we\nfurther aim to (P3) attribute the synthetic text gen-\nerated by ﬁne-tuned LMs. Here, we ﬁrst discuss\nprior work on P1 and P2 and then highlight the\nimportance and unique challenges of P3.\nP1: As the quality of synthetic text generated\nby LMs has improved, the problem of distinguish-\ning between organic and synthetic text has gar-\nnered a lot of attention. Gehrmann et al. (2019)\naimed to distinguish between synthetic text gener-\nated by GPT2 and Heliograf versus organic text\nby books, magazines, and newspapers. They\nshowed that humans had a hard time classifying\nbetween organic and synthetic. Their proposed\nGLTR model, which uses probability and ranks\nof words as predicted by pre-trained LMs as fea-\ntures, was able to achieve 87% AUC. Zellers et al.\n(2019) developed Grover, a LM to generate fake\nnews. They also showed that humans had a hard\ntime distinguishing between organic and synthetic\ntext generated by Grover. A classiﬁer based on\nGrover achieved near perfect accuracy and sig-\nniﬁcantly outperformed other classiﬁers based on\npre-trained LMs. Ippolito et al. (2020) presented\nan interesting trade-off between distinguishability\nof organic and synthetic text. They showed that\nsynthetic text optimized to fool humans is actu-\nally much easily detected by automated classiﬁ-\ncation approaches. They generated synthetic text\nfrom pre-trained GPT2 using different sampling\nstrategies and parameters, and used different clas-\nsiﬁers such as GLTR that use pre-trained LMs\nand a purpose-built ﬁne-tuned BERT based clas-\nsiﬁer. They showed that their ﬁne-tuned BERT\nbased classiﬁer was able to signiﬁcantly outper-\nform other approaches as well as humans.\nP2: Given synthetic text, recent work has fur-\nther attempted to attribute authorship of synthetic\ntext generated by pre-trained LMs. Uchendu et al.\n(2020) ﬁngerprinted pre-trained LMs by attribut-\ning synthetic text to its author LM. They con-\nducted exhaustive experiments using conventional\nauthorship attribution models (Kim, 2014; Zhang\net al., 2015; Cho et al., 2014) for eight pre-trained\nLMs – GPT (Radford et al., 2018), GPT2 (Rad-\nford et al., 2019) , GROVER (Zellers et al., 2019),\nFAIR (Ng et al., 2019), CTRL (Keskar et al.,\n2019), XLM (Conneau and Lample, 2019), XL-\nNET (Yang et al., 2019), and PPLM (Dathathri\net al., 2020). They showed that derived lin-\nguistic features when used with simple classiﬁers\n(Random Forest, SVM) perform the best. Pan\net al. (2020) prepared a corpus of organic text\nand queried each of the ﬁve LMs – BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2019), GPT,\nGPT2 and XLNET, to generate pre-trained embed-\ndings. Then, they trained a multilayer perceptron\nusing the embeddings and obtained perfect accu-\nracy in ﬁngerprinting the pre-trained LM. Munir\net al. (2021) used stylometric features as well as\nstatic and dynamic embeddings using ML classi-\nﬁers to attribute synthetic text generated by four\npre-trained LMs – GPT, GPT2, XLNet, and BART\n(Lewis et al., 2020). They obtained near perfect\naccuracy in ﬁngerprinting pre-trained LMs on a\npurpose-built dataset of synthetic text.\nP3: In this paper, we further attempt to at-\ntribute authorship of synthetic text generated by\nﬁne-tuned LMs. This problem is relevant in the\nreal-world because malicious actors typically ﬁne-\ntune a pre-trained LM for domain adaption (e.g.,\nto generate fake news articles vs. food reviews)\n(Zellers et al., 2019). There are two main novel\naspects of P3 that make it more challenging than\nP1 and P2. First, LMs can be ﬁne-tuned in numer-\nous different ways. More speciﬁcally, an attacker\ncan use various datasets to ﬁne-tune a pre-trained\nLM and further set the ﬁne-tuning parameters and\nepochs in many different ways. Therefore, the\nsize of the universe of ﬁne-tuned LMs is expected\nto be quite large in P3. In P1, the problem is a\nsimple binary classiﬁcation problem (organic vs.\nsynthetic). Similarly, in P2, the problem has still\nlimited number classes because only a handful of\npre-trained LMs are publicly available (e.g., GPT,\nGPT2, etc.). For instance, Munir et al. (2021),\nPan et al. (2020), and Uchendu et al. (2020) re-\nspectively considered a universe of four, ﬁve, and\n4655\neight pre-trained LMs. In contrast, in this paper,\nwe consider 108 ﬁne-tuned LMs. Second, prior\nwork mostly considered attributing authorship of\nsynthetic text generated in a controlled setting. For\nexample, synthetic text is often generated by pro-\nviding a topical prompt (Uchendu et al., 2020).\nAs another example, the attribution classiﬁer of-\nten assumes some information about the training\ndata (Zellers et al., 2019). In contrast, here we\nconsider the problem of attributing synthetic text\nin an uncontrolled setting assuming no control or\nknowledge about training or generation phases of\nﬁne-tuning LM.\n3 Dataset\nPrior work on detection and attribution of syn-\nthetic text has relied on artiﬁcial purpose-built\ndatasets in a controlled environment. We\novercome this issue by gathering a real-world\ndataset of synthetic text by different GPT2 bots\non the R/SUBSIMULATOR GPT2. Note that\nR/SUBSIMULATOR GPT2 is independently de-\nsigned and operated by its moderators. Each\nuser on the r/S UBSIMULATOR GPT2 subreddit\nis a GPT2 small (345 MB) bot that is ﬁne-\ntuned on 500k posts and comments from a\nparticular subreddit (e.g., r/askmen, r/askreddit,\nr/askwomen). The bots generate posts on\nr/SUBSIMULATOR GPT2, starting off with the\nmain post followed by comments (and replies)\nfrom other bots. The bots also interact with each\nother by using the synthetic text in the preceding\ncomment/reply as their prompt. In total, the sub-\nreddit contains 401,214 comments posted between\nJune 2019 and January 2020 by 108 ﬁne-tuned\nGPT2 LMs (or class). The complete details of\nvarious design choices are described here: https:\n//www.reddit.com/r/SubSimulatorGPT2Meta/\nTable 1 lists some representative examples of\nthe synthetic text generated by three different ﬁne-\ntuned GPT2 LMs on our dataset. We note that\nthe synthetic text is fairly coherent and also cap-\ntures the unique vocabulary and style of the sub-\nreddit used for ﬁne-tuning. For example, the ex-\ncerpt from r/conspiracy reads like a conspiracy\ndiscussion, the excerpt from r/confession men-\ntions a suggestive reply to the main post, and the\nexcerpt for r/wallstreetbets uses the specialised ﬁ-\nnance terms like “puts”.\nNext, we quantitatively compare and contrast\nsynthetic and organic texts corresponding to dif-\nSubreddit Synthetic text excerpt\nr/conspiracy\nI’m sure the elite have been working to\ncontrol what they do, but I don’t think\nthey have the resources or manpower\nto control something as massive as the\nglobal economy.\nr/confession\nYou need to tell her how you feel! She\nneeds to know how you feel so that you\ncan work out how to make the right\ndecision. You can tell her how much\nyou hate being a coward and how you’d\nnever be able to live up to your promise.\nr/wallstreetbets\nThere’s a huge amount of volatility\nright now that I don’t know how well\nmy eyes can handle. I’m not going\nto buy any puts for the next week.\nTable 1: Excerpts of synthetic text generated by GPT2\nLMs ﬁne-tuned on different subreddits.\nferent subreddits. To this end, we collect organic\ntext from the corresponding subreddits. Speciﬁ-\ncally, we randomly sample 1,000 comments from\neach subreddit class of synthetic and organic text.\nWe contrast basic lexical characteristics, vocabu-\nlary, and readability of synthetic and organic text.\nLexical. First, we contrast the following ba-\nsic lexical features: average/standard deviation of\nnumber of words/sentences per comment. We also\nmeasure the Pearson correlation coefﬁcient (ρ) be-\ntween pairs of synthetic and organic texts in Fig-\nures 2a to 2d. We note a high correlation ( ρ >\n0.83) across all lexical features. Thus, we con-\nclude that there is a strong dependency between\nthe lexical characteristics of the synthetic and or-\nganic text. This ﬁnding indicates thatsynthetic text\ngenerated by ﬁne-tuned GPT2 models indeed cap-\ntures the lexical characteristics of the correspond-\ning organic text used for ﬁne-tuning.\nVocabulary. Second, we compare the vocabu-\nlaries of synthetic and organic text of each class.\nWe do some basic pre-processing: lower-case, to-\nkenize, and lemmatize all words, remove all punc-\ntuation and emojis, and replace hyperlinks and\nnumbers with standard tags. Figure 2e compares\nthe vocabulary size of synthetic and organic text.\nWhile organic text seems to have a large vocabu-\nlary than synthetic text, we note a strong correla-\ntion ( ρ = 0.76) between their vocabulary sizes.\nWe further compute Jaccard similarity to mea-\nsure pair-wise vocabulary overlap between dif-\nferent synthetic and organic texts. Figure 3 vi-\nsualizes the similarity matrices between all pairs\nclasses – synthetic-synthetic, organic-organic, and\nsynthetic-organic. It is noteworthy that the left\n4656\n(a) Average #Words\n (b) SD Words\n (c) Average #Sentences\n(d) SD Sentences\n (e) V ocabulary Size\n (f) KF Readability\nFigure 2: A class-wise comparison of organic and synthetic text indicating strong correspondence in terms of:\n(a) average number of words; (b) standard deviation of number of words; (c) average number of sentences; (d)\nstandard deviation of number of sentences; (e) vocabulary size; and (f) Kincaid-Flescher readability. ρ is the\nPearson correlation coefﬁcient and mis the slope of the linear ﬁt.\ndiagonal represents much higher overlap between\ncorresponding pairs of synthetic and organic text\nclasses, even across synthetic-organic text classes.\nThis ﬁnding indicates that the ﬁne-tuned GPT2\nmodels indeed pick up the vocabulary from the or-\nganic text in the corresponding subreddit . It is\nalso noteworthy that the average vocabulary over-\nlap among synthetic text classes in Figure 3a is\nmuch higher than that among organic text classes\nas shown in Figure 3b. This indicates that syn-\nthetic text vocabulary combines information from\nboth pre-training and ﬁne-tuning – the text corpus\nused to pre-train the base GPT2 model as well as\nthe text corpus from the particular subreddit used\nto ﬁne-tune it.\nReadability. Finally, we compare the readabil-\nity of the synthetic and organic text. Figure 2f\ncompares the Kincaid-Flescher readability score\n(Kincaid et al., 1975) of synthetic and organic text.\nWe note that the average readability of synthetic\ntext (16.8) is less than that of organic text (11.5).\nThis observation corroborates the ﬁndings in re-\ncent prior work (Uchendu et al., 2020). Similar to\nlexical and vocabulary analysis, we note a strong\ncorrelation (ρ = 0.84) between the readability of\nsynthetic and organic text.\nAdditionally, using the organic text as refer-\nence, we measure the quality of the synthetic text\nusing well-known metrics – METEOR (Baner-\njee and Lavie, 2005) and BLEU (Papineni et al.,\n2002). We observe that the synthetic text achieves\nhigh average scores across all the metrics. We in-\nclude the results in the appendix.\nOverall, we have two key takeaways: (1) syn-\nthetic text is coherent although with lower over-\nall readability than organic text; (2) synthetic text\ncaptures the characteristics of the organic text\nused to ﬁne-tune it.\n4 Methods\nFingerprinting ﬁne-tuned LMs needs to operate\nunder the following realistic assumptions. (i) Fin-\ngerprinting methods cannot assume any knowl-\nedge about the nature of the LM, ﬁne-tuning (e.g.,\nparameters, layers) or generation (e.g., prompt).\n(ii) They also cannot assume any access to the\norganic text used for LM ﬁne-tuning. (iii) Since\na LM can be ﬁne-tuned in a myriad of ways,\nthese methods need to consider a large number of\nclasses. (iv) These methods are assumed to have\naccess to a limited sample of synthetic text gener-\nated by each of the potential ﬁne-tuned LMs.\n4.1 Pre-processing\nWe lower cased the comments and replaced all hy-\nperlinks with a standard tag [ LINK]. Next, we to-\n4657\n(a)\n (b)\n (c)\nFigure 3: Pair-wise vocabulary overlap between classes of (a) synthetic text, (b) organic text, and (c) cross-\ncomparison of organic and synthetic text. The higher intensity in (a) indicates more vocabulary overlap between\nclasses of (a) synthetic text as compared to (b) organic text. The dark diagonal (highlighted in red box) in (c)\nindicates signiﬁcant vocabulary overlap between the synthetic and organic text from the same subreddit.\nkenized the comments. Any comment with 5 or\nfewer tokens was removed. The maximum num-\nber of tokens in a comment is limited to 75. In\ncase a comment is larger, only the ﬁrst 75 tokens\nin a comment are taken into consideration. This\nis consistent for all the models we experimented\nwith. The limit was decided based on the limit of\nthe size of the GPU (11GB) used for ﬁne-tuning\nGPT2 and RoBERTa models.\n4.2 Features\nWe consider several different feature extraction ar-\nchitectures to encode the synthetic text into rep-\nresentation vectors. As we discuss later, these\nrepresentation vectors are then fed to classiﬁers.\nWriteprints: Writeprints feature set has been\nwidely used for authorship attribution (Iqbal et al.,\n2008; Pearl and Steyvers, 2012; Abbasi and Chen,\n2006). We extract a total of 220 features that in-\nclude lexical (e.g., total number of words, total\nnumber of characters, average number of char-\nacters per word, digits percentage, upper case\ncharacters percentage), syntactic (e.g., frequen-\ncies of function words, POS tags unigram, bi-\ngrams, trigrams), content-based (e.g., bag of\nwords, bigrams/trigrams) and idiosyncratic (e.g.,\nmisspellings percentage) features.\nGLTR: Gehrmann et al. (2019) used pre-\ntrained LMs to extract word likelihood features\n– word ranks and probabilities. We follow the\noriginal approach to average the word probabil-\nities of the text based on pre-trained BERT and\nGPT2. We also bin the word ranks into 10 un-\nequal ranges. The bins are: [1], [2-5], [6-10],\n[11-25], [26-50], [51-100], [101-250], [251-500],\n[501-1000], [>1000].\nGlove: Glove embeddings (Pennington et al.,\n2014) have been commonly used in large-scale au-\nthorship attribution (Ruder et al., 2016). We fol-\nlow (Kim, 2014; Zhang et al., 2015) to create a\nrepresentation vector of the size (max # tokens ×\n100, where max # tokens is set to 75) using Glove.\nPre-trained LMs (GPT2 and RoBERTa): We\nalso extract the embeddings for each comment us-\ning the pre-trained GPT2/RoBERTa model. Sim-\nilar to Nils and Gurevych (2019); Feng et al.\n(2020); Zhu and de Melo (2020), we take the\n[CLS] token representation from the last layer to\nextract the embeddings of size 1 ×768. The ﬁnal\nembeddings are then scaled between the values of\n[-3, 3] using min-max normalization.\nFine-tuned (FT) LMs (GPT2 and\nRoBERTa): We add a softmax classiﬁcation\nlayer to the pre-trained GPT2/RoBERTa model.\nThen, we ﬁne-tune the LM for the task of sen-\ntence classiﬁcation using the synthetic text in the\ntraining dataset. We again extract the embeddings\n(size = 1 ×768) by taking the [CLS] token\nrepresentation from the second last layer. The\nﬁnal embeddings are then scaled between the\nvalues of [-3, 3] using min-max normalization.\n4.3 Classiﬁers\nShallow classiﬁers: We use a probabilistic clas-\nsiﬁer – Gaussian Naive Bayes (GNB), an en-\nsemble decision classiﬁer – Random Forest (RF),\nand a feed-forward multilayer perceptron (MLP)\nacross all of the feature representations.\nCNN classiﬁer: We experiment with a stacked\nCNN model where convolution layers with differ-\nent kernel sizes are stacked before the embedding\nlayer (Kim, 2014). In this architecture, we attach\ntwo stacked convolution 1D layer, followed by a\nbatch normalisation layer and a max pooling layer.\n4658\nArchitecture Classiﬁer Macro Top-k\nPrec Recall 5 10\nGLTR\nGNB 5.5 4.4 12.9 20.9\nRF 7.8 6.6 12.6 19.0\nMLP 3.6 6.3 15.6 23.7\nWriteprints\nGNB 8.2 5.8 14.1 21.4\nRF 10.2 8.4 14.9 21.8\nMLP 16.9 14.7 30.8 42.1\nGloVE\nGNB 19.2 9.3 21.9 31.2\nRF 20.5 16.9 27.1 36.2\nMLP 29.7 27.2 44.4 54.1\nCNN 31.1 26.7 44.2 53.5\nGPT2\nGNB 24.8 12.4 27.8 37.7\nRF 10.5 7.8 15.8 27.1\nMLP 44.9 29.0 47.5 56.9\nCNN 30.9 28.7 49.1 59.1\nRoBERTa\nGNB 39.2 15.8 30.8 41.0\nRF 11.1 8.4 16.6 25.8\nMLP 44.0 34.8 54.8 62.5\nCNN 33.5 32.0 53.1 63.0\nFT-GPT2\nGNB 40.1 37.0 56.9 66.0\nRF 27.6 22.8 34.8 45.2\nMLP 40.2 36.4 55.7 64.0\nCNN 44.6 42.1 60.9 68.9\nFT-RoBERTa\nGNB 47.7 41.5 57.9 64.9\nRF 42.0 36.8 46.9 53.2\nMLP 42.8 41.5 58.2 65.3\nCNN 46.0 43.6 62.0 69.7\nTable 2: Performance of multi-class classiﬁers based\non macro Precision (Prec), Recall and top- k accuracy\n(k= 5,10) for the largest setting of 108 classes.\nThe output is then fed to two dense layers, the lat-\nter of which is a softmax layer.\nIn addition, we also experiment with other\nshallow classiﬁers (SVM, Decision Tree) and\ntwo more types of feature generators (ﬁne-tuned\nGLTR, trainable word embeddings). We report\nadditional results, observations and the hyper-\nparameters for all the models in the appendix.\n5 Evaluation\nWe conduct experiments to evaluate these methods\nusing the real-world Reddit dataset as described\nin Section 3. Our training, validation, and test\nsets consist of 800, 100 and 200 synthetic com-\nments respectively from each of the 108 subreddit\nclasses. In total, our training, validation and test\nsets comprise 86k, 11k and 22k comments respec-\ntively. For evaluation, we use macro precision and\nrecall. We also measure top- k accuracy based on\nthe conﬁdence score to assess the accuracy of the\nclassiﬁers in k(k= 5,10) guesses for 108 classes.\n5.1 Results\nTable 2 lists the results of different feature rep-\nresentations and classiﬁers. Overall, classiﬁers\nbased on ﬁne-tuned LM embeddings perform\nthe best, with RoBERTa slightly outperforming\nGPT2. Fine-tuned embeddings are successfully\nable to capture the domain of the organic text\n(a) Fine-tuned RoBERTa\n (b) Pre-trained RoBERTa\n(c) Fine-tuned GPT2\n (d) Pre-trained GPT2\nFigure 4: Visualisation of ﬁne-tuned (a,c) and pre-trained\nembeddings (b,d) of speciﬁc classes. Closely condensed clus-\nters speciﬁc to the domain of the organic text form in the ﬁne-\ntuned embeddings.\nthe LM is ﬁne-tuned on. To provide further in-\nsights into our best performing feature represen-\ntations, we visualize the feature embeddings of\npre-trained and ﬁne-tuned RoBERTa and GPT2.\nFigure 4 plots the 2D projection of synthetic text\n(using t-SNE) generated by different LMs that are\nﬁne-tuned on various subreddits. Fine-tuned em-\nbeddings form more cohesive and separated clus-\nters than pre-trained embeddings. Thus, we con-\nclude that ﬁne-tuning these embeddings is beneﬁ-\ncial in attributing synthetic text generated by dif-\nferent ﬁne-tuned LMs. Note that certain clusters\nare more cohesive and better separated than others.\nFor example, the most distinct cluster is observed\nfor r/wallstreetbets in Figures 4a and 4c for ﬁne-\ntuned embeddings. However, it is not quite dis-\ntinct in Figures 4b and 4d for pre-trained embed-\ndings. We also note that some clusters with high\ntopical similarity (e.g., r/science and r/askscience)\nare closer to each other. On the other hand, some\nclusters with likely lower topical similarity (e.g.,\nr/socialism and r/conservative) are far apart.\nDespite combining word probabilities from\nboth BERT and GPT2, GLTR is ineffective. We\nﬁnd that synthetic texts generated from differ-\nent ﬁne-tuned models have similar word proba-\nbilities because perhaps they are more impacted\nby the pre-training process rather than the subse-\nquent ﬁne-tuning. This shows that the classiﬁer\nthat performs well for distinguishing between or-\nganic and synthetic text (P1) does not work well\n4659\n(a) Micro\n (b) Macro\nFigure 5: (a) Micro and (b) Macro precision-recall trade-off\nby varying the gap statistic threshold. The comparison with\nall baselines is included in the appendix.\nfor distinguishing between synthetic text by dif-\nferent ﬁne-tuned LMs (P3). Writeprints feature set\nprovides some improvement but is still ineffective.\nOur ﬁnding corroborates Manjavacas et al. (2017),\nwho reported that linguistic and stylistic features\nas used in Writeprints are not effective in distin-\nguishing between synthetic text. GloVE again of-\nfers some improvement over Writeprints but its\nperformance remains signiﬁcantly worse than that\nof our best performing method.\nOverall, ﬁne-tuned RoBERTa embeddings with\nCNN performs the best with 46.0% precision and\n43.6% recall. In about 44% of the cases, this\nclassiﬁer can correctly ﬁngerprint ﬁne-tuned LM\namongst 108 classes; in about 70% of cases the\ncorrect prediction is one of the top-10 guesses.\nIt is noteworthy that Random Forest which per-\nformed exceedingly well in prior work on ﬁnger-\nprinting pre-trianed LMs (Uchendu et al., 2020),\ndoes not perform well for ﬁngerprinting ﬁne-tuned\nLMs. Surprisingly, a relatively simple classiﬁer\nlike GNB achieves comparable precision and re-\ncall for our top guess. However, CNN outperforms\nGNB by a small margin, achieving the best top-10\naccuracy of 69.7% for FT-RoBERTa.\n5.2 Discussion\nNext, we analyze the performance of our best per-\nforming RoBERTa feature representation and clas-\nsiﬁer (CNN) under different conditions.1\nPrecision-Recall trade-off. We evaluate the\nprecision-recall trade-off by imposing a threshold\non the conﬁdence of our prediction. To this end,\nwe use the gap statistic, deﬁned as the difference\nbetween the probability of the highest and second\nhighest prediction (Narayanan et al., 2012). If the\ngap statistic is lower than our threshold, the clas-\nsiﬁer chooses to not make a prediction for the test\n1Other baseline results are reported in the appendix.\nsample. This invariably has an impact on precision\nand recall. Typically, the precision of the classiﬁer\nincreases, since it can more accurately predict the\ncorrect class for the samples it has a high conﬁ-\ndence in. Due to certain samples not being pre-\ndicted for, recall is expected to decrease. Note that\nsince the classiﬁer may make different number of\npredictions across classes, micro and macro preci-\nsion/recall could be different.\nFigures 9a and 9b respectively plot the micro\nand macro precision/recall as we vary the gap\nstatistic. Overall, the classiﬁer using FT-RoBERTa\nembeddings achieves a better precision-recall\ntrade-off compared to using standard RoBERTa\nembeddings. As expected, precision improves at\nthe expense of recall for larger values of gap statis-\ntic. Micro precision increases 46% to over 87%\nwith a reduction in the micro recall from 43% to\n27%. Similarly, despite potential class imbalance,\nmacro precision increases 46% to over 81% with\na reduction in the micro recall from 43% to 26%.\nThus, we conclude that the conﬁdence of our best\nperforming ﬁngerprinting method can be tuned to\nachieve very high precision with some compro-\nmise on recall.\nImpact of number of training samples. Next,\nwe evaluate the impact on the performance of our\nbest models by varying training size from 50 to\n800 samples per class. As we vary the training\ndata, we keep the same test set, i.e., 200 samples\nfrom each class. Figures 8a and 8b, respectively\nshow that precision and recall of FT-RoBERTa\nplateau at around 400 samples per class. Despite at\ntwice the training data, using training 800 samples\nper class has similar precision/recall as compared\nto using 400 training samples per class. We con-\nclude that having more training samples of syn-\nthetic text may not always lead to a signiﬁcant im-\nprovement in ﬁngerprinting performance.\nImpact of number of classes. We further vary\nthe number of classes from 10 to 108 and re-\nport the performance of our best models. Fig-\nures 8c and 8d show that for a 10-class prob-\nlem, FT-RoBERTa + CNN achieves 63.0% preci-\nsion and 61.7% recall. As expected, both preci-\nsion and recall decrease as the number of classes\nincreases. For 108 classes, the same classiﬁer\nachieves 46.0% precision and 43.6% recall. This\nindicates that ﬁngerprinting a ﬁne-tuned LM is\nhighly challenging when the universe of potential\nﬁne-tuned LMs is larger.\n4660\n(a)\n (b)\n (c)\n (d)\nFigure 6: Comparison between the performances of pre-trained and ﬁne-tuned RoBERTa by varying different parameters. (a)\nPrecision and (b) Recall with the varying training size. (c) Precision and (c) Recall with the varying number of classes. Overall,\nﬁne-tuned RoBERTa outperforms pre-trained RoBERTa. The comparison withall baselines is included in the appendix.\n6 Conclusion\nIn this paper, we studied the problem of attribu-\ntion of synthetic text generated by ﬁne-tuned LMs.\nWe designed a comprehensive set of feature ex-\ntraction techniques and applied them on a num-\nber of different machine learning and deep learn-\ning pipelines. The results showed that the best\nperforming approach used ﬁne-tuned RoBERTa\nembeddings with CNN classiﬁer. Our ﬁndings\npresent opportunities for future work on ﬁnger-\nprinting LMs in even more challenging open-\nworld scenarios, where the list of potential LMs\nmight be incomplete or synthetic text is not avail-\nable to train attribution classiﬁers.\nEthics and Broader Impact\nAny biases found in the gathered dataset are unin-\ntentional, and we do not intend to do harm anyone.\nWe would also like to acknowledge that the use\nof large-scale transformer models requires non-\ntrivial compute resources (GPUs/TPUs) that con-\ntributes to global warming (Strubell et al., 2019).\nHowever, we did not train the models from scratch\nand simply ﬁne-tuned them for our work.\nAcknowledgement\nThe work was partially supported by the Ramanu-\njan fellowship and the Infosys Centre for AI, IIIT-\nDelhi.\nReferences\nAhmed Abbasi and Hsinchun Chen. 2006. Visualiz-\ning authorship for identiﬁcation. In International\nConference on Intelligence and Security Informat-\nics, pages 60–71. Springer.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings\nof the acl workshop on intrinsic and extrinsic evalu-\nation measures for machine translation and/or sum-\nmarization, pages 65–72.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In 34th Conference on Neural Information\nProcessing Systems (NeurIPS 2020), pages 1–15.\nKyunghyun Cho, Bart van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724–\n1734.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32,\npages 7059–7069.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. In\nICLR, pages 1–25.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\nArivazhagan, and Wei Wang. 2020. Language-\nagnostic BERT sentence embedding. arXiv preprint\narXiv:2007.01852.\nSebastian Gehrmann, Hendrik Strobelt, and Alexan-\nder M Rush. 2019. Gltr: Statistical detection and vi-\nsualization of generated text. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics: System Demonstrations , pages\n111–116.\n4661\nDaphne Ippolito, Daniel Duckworth, Chris Callison-\nBurch, and Douglas Eck. 2020. Automatic detec-\ntion of generated text is easiest when humans are\nfooled. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1808–1822.\nFarkhund Iqbal, Rachid Hadjidj, Benjamin CM Fung,\nand Mourad Debbabi. 2008. A novel approach of\nmining write-prints for authorship attribution in e-\nmail forensics. digital investigation, 5:S42–S51.\nGanesh Jawahar, Muhammad Abdul-Mageed, and\nLaks Lakshmanan, V .S. 2020. Automatic detection\nof machine generated text: A critical survey. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 2296–2309,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nYoon Kim. 2014. Convolutional neural networks\nfor sentence classiﬁcation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746–1751,\nDoha, Qatar. Association for Computational Lin-\nguistics.\nJ Peter Kincaid, Robert P Fishburne Jr, Richard L\nRogers, and Brad S Chissom. 1975. Derivation of\nnew readability formulas (automated readability in-\ndex, fog count and ﬂesch reading ease formula) for\nnavy enlisted personnel. Technical report, Naval\nTechnical Training Command Millington TN Re-\nsearch Branch.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3721–3731.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nEnrique Manjavacas, Jeroen De Gussem, Walter Daele-\nmans, and Mike Kestemont. 2017. Assessing the\nstylistic properties of neurally generated text in au-\nthorship attribution. In Proceedings of the Workshop\non Stylistic Variation, pages 116–125.\nShaoor Munir, Brishna Batool, Zubair Shaﬁq, Padmini\nSrinivasan, and Fareed Zaffar. 2021. Through the\nLooking Glass: Learning to Attribute Synthetic Text\nGenerated by Language Models. In Proceedings of\nthe 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics (EACL).\nArvind Narayanan, Hristo Paskov, Neil Zhenqiang\nGong, John Bethencourt, Emil Stefanov, Eui\nChul Richard Shin, and Dawn Song. 2012. On\nthe feasibility of internet-scale author identiﬁcation.\nIn 2012 IEEE Symposium on Security and Privacy ,\npages 300–314.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\nMichael Auli, and Sergey Edunov. 2019. Facebook\nfair’s wmt19 news translation task submission. In\nProceedings of the Fourth Conference on Machine\nTranslation (Volume 2: Shared Task Papers, Day 1),\npages 314–319.\nR Nils and I Gurevych. 2019. Sentence-BERT: Sen-\ntence Embeddings using Siamese BERT-Networks.\nIn Proceedings of the 2019 Conference on Empir-\nical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, Hong Kong, China, pages 3–\n7.\nXudong Pan, Mi Zhang, Shouling Ji, and Min Yang.\n2020. Privacy risks of general-purpose language\nmodels. In 2020 IEEE Symposium on Security and\nPrivacy (SP), pages 1314–1331.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Compu-\ntational Linguistics, pages 311–318.\nLisa Pearl and Mark Steyvers. 2012. Detecting author-\nship deception: a supervised machine learning ap-\nproach using author writeprints. Literary and lin-\nguistic computing, 27(2):183–196.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532–1543.\nAlec Radford, Karthik Narasimhan, Tim Sali-\nmans, and Ilya Sutskever. 2018. Improv-\ning language understanding by generative pre-\ntraining (2018). URL https://s3-us-west-2. amazon-\naws. com/openai-assets/research-covers/language-\nunsupervised/language understanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\n4662\nSebastian Ruder, Parsa Ghaffari, and John G Breslin.\n2016. Character-level and multi-channel convolu-\ntional neural networks for large-scale authorship at-\ntribution. arXiv preprint arXiv:1609.06686.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nAdaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee.\n2020. Authorship attribution for neural text gener-\nation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 8384–8395.\nQicai Wang, Peiyu Liu, Zhenfang Zhu, Hongxia Yin,\nQiuyue Zhang, and Lindong Zhang. 2019. A text\nabstraction summary model based on bert word em-\nbedding and reinforcement learning. Applied Sci-\nences, 9(21):4701.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems, volume 32, pages\n5753–5763.\nSoYeop Yoo and OkRan Jeong. 2020. An intelligent\nchatbot utilizing bert model and knowledge graph.\nJournal of Society for e-Business Studies, 24(3).\nShi Yu, Yuxin Chen, and Hussain Zaidi. 2020. A ﬁ-\nnancial service chatbot based on deep bidirectional\ntransformers. arXiv preprint arXiv:2003.04987.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Advances in Neural Information Process-\ning Systems, pages 9054–9065.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. Advances in neural information process-\ning systems, 28:649–657.\nXunjie Zhu and Gerard de Melo. 2020. Sentence\nanalogies: Exploring linguistic relationships and\nregularities in sentence embeddings. arXiv preprint\narXiv:2003.04036.\n4663\nFigure 7: Comparison of different metrics on the syn-\nthetic corpus taking the reference of the organic corpus.\nThe classes are sorted by increasing CHRF scores.\nA Appendix\nA.1 Implementation, Infrastructure,\nSoftware\nWe run all experiments on a 24-core machine with\ntwo Intel(R) Xeon(R) Silver 4116 CPU@2.10GHz\nCPU’s and 512 GB RAM. Additionally, the server\nhas a GeForce RTX 2080 Ti (11 GB) GPU card.\nWe use huggingface, pytorch (1.4.0) and Tensor-\nﬂow (v.1.23.0) to implement and evaluate all deep\nlearning models in the paper. For classical ML, we\nutilize scikit-learn (v.0.23.1). All implementations\nare done using Python language (v.3.7).\nA.2 Hyper-parameters\nFine-tuned RoBERTa + CNN:We attach 2 CNN\nconvolutional 1D of kernel sizes 2 and 3 layers\nback to back. The stride for both the layers is 1.\nThe number of output ﬁlters for each of the two\nconvolutional layers is 16. We then attach a batch\nnormalization layer of size 16. This is followed by\na max pooling layer of size 2 with stride 1. We\nﬁne-tuned the model for 15 epochs and monitored\nthe validation loss. If the validation loss did not\nimprove for 5 consecutive epochs, we stopped the\nﬁne-tuning.\nFine-tuned RoBERTa + Dense: We used\nRoBERTaForSequenceClassiﬁcation wrapper\nfrom the huggingface module, which attaches a\nsoftmax layer on top of pre-trained RoBERTa\nmodel. We extracted the embeddings of the\nsecond to last layer of size 1x768. For all of\nour models, we used a AdamW optimizer with a\nlearning rate of 0.00005. We used batch size of 48\nand only picked the ﬁrst 75 tokens of each token.\nWe did not use any padding.\nFor the soft classiﬁers, using the ﬁne-tuned\nRoBERTa embeddings, we obtained the best re-\nsults with the following parameters -\nSVM: C = 0.1, cachesize=200, classweight =\nNone, coef0 = 0.0, degree=3, gamma=‘scale’,\nkernel=‘linear’, tol=0.001\nMLP: activation=‘relu’, alpha=0.01,epsilon=1e-\n08, hiddenlayersizes = 64, learn-\ningrate=‘adaptive’, learningrateinit=0.0001\nRF - criterion=‘entropy’, maxdepth=None,\nmaxfeatures=‘auto’, maxleafnodes=None, min-\nsamplesleaf = 1\nDT - criterion = ‘entropy’, maxdepth = None,\nmaxfeatures = None, maxleafnodes=None, min-\nsamplesleaf = 1\nGNB - Default sklearn parameters performed the\nbest\nA.3 Running Time\nAll ﬁne-tuned models took a maximum of 8 min-\nutes per epoch. For the soft classiﬁers, MLP and\nSVM took the maximum time. Due to the large\nsize of the dataset and the large embedding space,\nSVM took about 6 hours to train. MLP took an\naverage of 3 hours. Rest of the classiﬁers took less\nthan an hour for training.\nA.4 Additional Results\nA.4.1 Additional Dataset Analysis\nBesides the analaysis in the dataset section of the\nmain text, we used 4 metrics - METEOR, BLEU,\nGLEU and CHRF for measuring the coherency of\nthe synthetic text, using the organic text as ref-\nerence. Using 1000 comments from each class\nof synthetic and organic text, we obtained high\nclass-wise average scores on all metrics - ME-\nTEOR (0.31), BLEU (0.58), GLEU (0.63) and\nCHRF (0.51). This provides further evidence that\nsynthetic text is coherent and readable when com-\npared to its organic counterpart. In ﬁgure 7 , we il-\nlustrate the scores for each individual class, sorted\nby increasing CHRF scores.\nA.4.2 Feature extraction\nBesides the feature extraction methods we men-\ntioned in the Methods section of the main text, we\nalso experimented with two more methods:\n1. Fine-tuned GLTR: Using the training set, we\nﬁne-tuned separate BERT and GPT2 models for\neach class for the task of mask completion. All\nmodels were then used for extracting GLTR word\nlikelihood features for the complete training and\ntest sets. Subsequently, the training representa-\ntions were then fed to a sequential neural classi-\n4664\n(a)\n (b)\n (c)\n (d)\nFigure 8: A comparison between the performances of various embeddings by varying problem parameters. (a) Precision and\n(b) Recall with the varying training set size. (a) Precision and (b) Recall with the varying number of classes. Overall, ﬁne-tuned\nRoBERTa outperforms all the baselines.\n(a) Micro\n (b) Macro\nFigure 9: (a) Micro and (b) Macro precision-recall trade-off\nby varying the gap statistic threshold.\nBase ArchitectureClassiﬁer Macro Top - 5Top - 10PrecisionRecall\nWriteprints DT 6.9 6.6 6.6 6.6SVM 19.3 16.3 33.2 44.8\nGLTR DT 5.7 5.5 5.57 5.58SVM 7.3 7.0 10 13.1\nRoBERTa DT 6.3 5.3 6.3 6.3SVM 8.3 6.9 7.8 8.9\nTrainable-WordMLP 20.4 18.9 34.3 44.3CNN 28.6 27.0 44.0 53.1\nFT RoBERTaDense 44.0 42.3 60.8 68.9DT 29.5 28.7 28.7 28.7SVM 42.7 41.1 58.3 65.6\nTable 3: Results of Decision Tree (DT) and SVM with\nthe embeddings. We also include the results of the\ntrainable word embeddings model.\nﬁer like Bi-LSTM. The intuition was that word\nlikelihoods extracted using the class’s ﬁne-tuned\nGLTR model would be high for synthetic text gen-\nerated for the class’s language model. For ex-\nample, a r/wallstreetbets ﬁne-tuned GLTR feature\nextractor would extract higher word likelihoods,\nas compared to other GLTR models, from a syn-\nthetic comment generated by the language model\nﬁne-tuned on r/wallstreetbets. However, for an\nextremely small setting of 5 classes, we only ob-\ntained a precision and recall of around 33% each.\nDue to the (a) expensive cost of ﬁne-tuning 108\nBERT and GPT2 models, (b) expensive cost of ex-\ntracting GLTR features for a dataset of 100k exam-\nples, (c) extremely poor results of the approach on\na small setting, we did not continue with experi-\nmenting the model in a larger setting.\n2. Trainable word embeddings - We tokenized\nand represented each comment using an allocated\nset of integers based on the vocabulary of the train-\ning set. Then, similar to the GloVE feature extrac-\ntion, we passed the representation into a trainable\nword embedding layer, followed by MLP or CNN.\nThe results for these were slightly worse than that\nfor the GloVE features. We report the results in\nTable 3.\nAdditionally, for all the feature representations\nmentioned in the main text, we tested SVM and\ndecision tree. Overall, they were outperformed by\nthe other classiﬁers with one notable exception.\nFor Writeprints features, SVM showed the best re-\nsults of precision and recall of 19.3% and 13.3%\nrespectively. We report the results in Table 3.\nA.5 Other baselines\nPrecision-recall trade-off. For the precision-\nrecall trade-off in the results section of the main\ntext, we presented a comparison with additional\nbaselines in Figure 9. Fine-tuned RoBERTa per-\nforms the best among all methods for both micro\nand macro precision-recall trade-offs. They are\nfollowed in a decreasing order by the pre-trained\nRoBERTa embeddings, the trainable word embed-\ndings, the GloVE word embeddings, Writeprints,\nand GLTR respectively.\nTraining set size and classes. We report the com-\nparison with all other baselines for the varying\ntraining size and the number of classes in Fig-\nure 8. Similar to the precision-recall trade-off,\nthe ﬁne-tuned RoBERTa embeddings perform the\nbest, followed by pre-trained RoBERTa embed-\ndings, GloVE word embeddings, trainable word\nembeddings, Writeprints, and GLTR.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5805111527442932
    }
  ]
}