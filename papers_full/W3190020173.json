{
  "title": "Learning Attributed Graph Representation with Communicative Message Passing Transformer",
  "url": "https://openalex.org/W3190020173",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2099115485",
      "name": "Jianwen Chen",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2910834915",
      "name": "Shuangjia Zheng",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2096887020",
      "name": "Ying Song",
      "affiliations": [
        "Sun Yat-sen University",
        "National Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2980531056",
      "name": "Jiahua Rao",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2130433203",
      "name": "Yuedong Yang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2594183968",
    "https://openalex.org/W1738019091",
    "https://openalex.org/W2964113829",
    "https://openalex.org/W2968734407",
    "https://openalex.org/W3007488165",
    "https://openalex.org/W3095883070",
    "https://openalex.org/W2785720803",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2986232138",
    "https://openalex.org/W2987522751",
    "https://openalex.org/W2972693922",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2964051675",
    "https://openalex.org/W3198974858",
    "https://openalex.org/W2964380716",
    "https://openalex.org/W3014550317",
    "https://openalex.org/W3034516664",
    "https://openalex.org/W3000478925",
    "https://openalex.org/W4286715520",
    "https://openalex.org/W2962876364",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2290847742"
  ],
  "abstract": "Constructing appropriate representations of molecules lies at the core of numerous tasks such as material science, chemistry, and drug designs. Recent researches abstract molecules as attributed graphs and employ graph neural networks (GNN) for molecular representation learning, which have made remarkable achievements in molecular graph modeling. Albeit powerful, current models either are based on local aggregation operations and thus miss higher-order graph properties or focus on only node information without fully using the edge information. For this sake, we propose a Communicative Message Passing Transformer (CoMPT) neural network to improve the molecular graph representation by reinforcing message interactions between nodes and edges based on the Transformer architecture. Unlike the previous transformer-style GNNs that treat molecule as a fully connected graph, we introduce a message diffusion mechanism to leverage the graph connectivity inductive bias and reduce the message enrichment explosion. Extensive experiments demonstrated that the proposed model obtained superior performances (around 4% on average) against state-of-the-art baselines on seven chemical property datasets (graph-level tasks) and two chemical shift datasets (node-level tasks). Further visualization studies also indicated a better representation capacity achieved by our model.",
  "full_text": "Learning Attributed Graph Representations with Communicative Message\nPassing Transformer\nJianwen Chen1y, Shuangjia Zheng1;4y\u0003, Ying Song2 , Jiahua Rao1;4 and Yuedong Yang1;3\u0003\n1School of Computer Science and Engineering, Sun Yat-sen University\n2School of Systems Science and Engineering, Sun Yat-sen University\n3Key Laboratory of Machine Intelligence and Advanced Computing, Sun Yat-sen University\n4Galixir Technologies Ltd, Beijing\n{chenjw48, zhengshj9, songy75, raojh6}@mail2.sysu.edu.cn, yangyd25@mail.sysu.edu.cn\nAbstract\nConstructing appropriate representations of\nmolecules lies at the core of numerous tasks such\nas material science, chemistry and drug designs.\nRecent researches abstract molecules as attributed\ngraphs and employ graph neural networks (GNN)\nfor molecular representation learning, which have\nmade remarkable achievements in molecular graph\nmodeling. Albeit powerful, current models either\nare based on local aggregation operations and thus\nmiss higher-order graph properties or focus on\nonly node information without fully using the edge\ninformation. For this sake, we propose a Commu-\nnicative Message Passing Transformer (CoMPT)\nneural network to improve the molecular graph\nrepresentation by reinforcing message interactions\nbetween nodes and edges based on the Transformer\narchitecture. Unlike the previous transformer-style\nGNNs that treat molecules as fully connected\ngraphs, we introduce a message diffusion mecha-\nnism to leverage the graph connectivity inductive\nbias and reduce the message enrichment explosion.\nExtensive experiments demonstrated that the\nproposed model obtained superior performances\n(around 4% on average) against state-of-the-art\nbaselines on seven chemical property datasets\n(graph-level tasks) and two chemical shift datasets\n(node-level tasks). Further visualization studies\nalso indicated a better representation capacity\nachieved by our model.\n1 Introduction\nAccurate characterization of molecular properties remains\nlargely an open challenge, and its solution may unlock a\nwidespread use of deep learning in the drug discovery indus-\ntry [Wu et al., 2018 ]. Traditionally, this has involved trans-\nlating a molecule m to a dense feature vector with a repre-\nsentation function, h= g(m), and then applying a variety of\n†These two authors contributed equally.\n*Corresponding authors.\n‡https://github.com/jcchan23/CoMPT\ntechniques to predict the targeted property based on the rep-\nresentation by y= f(h).\nEarly predictive modeling methods such as quantitative\nstructure-property relationships (QSPR) have been performed\nbased on ﬁxed representations such as expert-crafted physico-\nchemical descriptors and molecular ﬁngerprints [Rogers and\nHahn, 2010 ]. However, descriptor-based methods presume\nthat all target property-related information is covered by the\nchosen descriptor set, limiting the capability for a model to\nmake problem-speciﬁc decisions.\nMore naturally, a molecular structure can be abstracted as\na topological graph with attributed nodes and edges, where\nnode features correspond to atom properties like atomic iden-\ntity and degree, edge features correspond to bond properties,\nlike bond type and aromaticity. In this sense, graph repre-\nsentation models, especially Graph Neural Networks (GNN),\ncan be intuitively introduced to learn the representations of\nmolecules. Generally, the procedure of GNN framework\ncan be summarized in three main steps: (1) Initialization\nstep, where nodes are initialized with their initial attributes or\nstructural features; (2) Message Passing step, where the fea-\ntures at each node are transmitted from its neighbors across\nthe molecular graph into a message vector; (3) Read-out\nstep, where the node messages are aggregated or pooled into\na ﬁxed-length feature vector. Under the above framework,\nmany GNN architectures have been proposed for effective\ngraph representation learning, that achieve promising results\nin many property prediction tasks [Duvenaud et al., 2015;\nYang et al., 2019; Song et al., 2020].\nDespite the fruitful progress, several issues still impede\nthe performance of the current GNN in the molecular graph.\nFirst, common graph convolutional operations aggregate only\nlocal information and suffer from the suspended animation\nproblem when stacking excessive GNN layers [Zhang and\nMeng, 2019], so these models are naturally difﬁcult to learn\nlong-range dependencies and the global chemical environ-\nment of each atom. Second, main-stream GNN and its vari-\nants mainly focus on obtaining effective nodes embedding\nbut weaken the information carried by edges that is also im-\nportant for informative graph representations [Shang et al.,\n2018]. Meanwhile, the node representations obtained by such\ndeep models tend to be over-smoothed and hard to distinguish\n[Li et al., 2018]. Such issues greatly hinder the applications\nof GNNs for molecular representation learning tasks.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n2242\nTo address the above problems, many efforts have been\nmade from different directions. On the one hand, with\nthe emerging of Transformer [Vaswani et al., 2017 ] in se-\nquence modeling, several Transformer-style GNNs [Chen et\nal., 2019; Maziarka et al., 2020 ] have been introduced to\nlearn the long-range dependencies in graph-structured data.\nThese methods can be viewed as a variant of the Graph At-\ntention Network (GAT) [Veliˇckovi´c et al., 2017 ] on a fully\nconnected graph constructed by all atoms, which ignore the\ngraph connectivity inductive bias. As a result, they perform\npoorly in the tasks where graph topology plays an important\nrule. On the other hand, the directed message passing neu-\nral network [Yang et al., 2019 ] and its variants [Song et al.,\n2020] have been proposed to transmit messages through di-\nrected edges rather than vertices. Such methods make use of\nthe edge information explicitly and avoid unnecessary loops\nin the message passing trajectory, but they still cannot deal\nwith the long-range dependencies.\nBased on these observations, we propose a Communica-\ntive Message Passing Transformer (CoMPT) neural network\nfor molecular representation learning. In contrast to the previ-\nous Transformer-style GNNs that emphasize the node infor-\nmation, CoMPT invokes a communicative message-passing\nparadigm by strengthening the message interactions between\nedges and nodes. In our framework, both the edge and node\nembeddings are updated during the training process. Besides,\nwe reﬁne the message passing process by using the topolog-\nical connection matrix with a diffusion mechanism to reduce\nthe message enrichment explosion. By selectively propagat-\ning information within a molecular graph, CoMPT is able to\nextract more expressive representation for down-stream tasks.\nThe main contributions of this work include:\n• We propose a novel communicative message passing\ntransformer, namely CoMPT, that explicitly captures the\natom and bond information of molecular graphes and in-\ncorporates both local and global structural information.\n• Our model includes an elegant way to fuse topology\nconnection matrix using the message diffusion mech-\nanism inspired by the thermal diffusion phenomenon,\nwhich was further demonstrated to alleviate the over-\nsmoothing problem.\n• Numerical experiments are conducted on both graph-\nlevel and node-level public datasets to demonstrate the\neffectiveness of our method. CoMPT surpasses the state-\nof-the art models on all nine tasks by up-to 4% improve-\nment in the average performance.\n2 Related Work\nMolecular representation learning. One of the most pop-\nular representations of molecules is the ﬁxed representations\nthrough chemical ﬁngerprints, such as Extended Connectiv-\nity Fingerprints (ECFP) [Rogers and Hahn, 2010] and chemi-\ncal descriptors. The heuristics integrated in descriptor gener-\nation algorithms typically embed high-level chemistry prin-\nciples, attempting to maximize the information content of\nthe resulting feature vectors. While these methods can be\nclearly successful, they always feature a trade-off by em-\nphasizing certain molecular features, while neglecting oth-\ners. The selections of features are hard-coded in the algo-\nrithm and not amenable to problem-speciﬁc tuning. Recent\nworks started to explore the molecular graph representation.\nEarly studies learned to only encode the node features [Du-\nvenaud et al., 2015 ] without considering bond information.\nTo gain supplementary information from edges, [Kearnes et\nal., 2016 ] proposed to utilize attributes of both atoms and\nbonds, and [Gilmer et al., 2017] summarized it into a MPNN\nframework. Though a few more studies used the informa-\ntion of the edges through network modules such as the edge\nmemory module [Withnall et al., 2020 ], these models were\nmainly built upon the node-based MPNN and thus still suf-\nfered from the information redundancy during message ag-\ngregations. DMPNN [Yang et al., 2019 ] was introduced as\nan alternative as it abstracted the molecular graph as an edge-\noriented directed graph, avoiding the unnecessary loops in\nmessage passing procedure. CMPNN [Song et al., 2020] fur-\nther extend this work by strengthening the message interac-\ntions between nodes and edges through a communicative ker-\nnel. Our work is closely related to CMPNN, while our model\nbuilt on Transformer is more elegant to capture long-range\ndependencies and structural variety.\nTransformer-style graph neural network. Several at-\ntempts have been made to integrate transformer and graph\nneural network. [Chen et al., 2019 ] introduced the Path-\nAugmented Graph Transformer Networks to explicitly take\naccount for longer-range dependencies in molecular graph.\nOne closely related work is [Maziarka et al., 2020 ], which\nproposed a Molecule Attention Transformer by augmenting\nthe attention mechanism in Transformer using inter-atomic\ndistances and the molecular graph structure. Another work\nworth to note is GROVER [Rong et al., 2020 ], which pro-\nvided a self-supervised pre-trained transformer-style message\npassing neural network for molecular representation learn-\ning. While our model also builds on Transformer [Vaswani\net al., 2017] to encode graphs, we contribute new techniques\nto leverage the graph connectivity inductive bias.\n3 Methods\nIn this section, we ﬁrst brieﬂy review basic concepts of the\nTransformer model [Vaswani et al., 2017 ]. Then, we focus\non our contributions, describing our alternative in the Trans-\nformer encoder framework that uses node-edge message in-\nteraction module instead of the self-attention mechanism to\npass the message and learn expressive representations for at-\ntributed molecular graphs. Finally, we introduce a message\ndiffusion mechanism for leveraging the graph connectivity in-\nductive bias and reducing the message enrichment explosion.\n3.1 Preliminary\nNotation and problem deﬁnition. A molecular structure\ncan be considered as an attributed graph G = (V;E), where\njVj= ndenotes a set of natoms (nodes) and jEj= mde-\nnotes a set of mbonds (edges). Nv is utilized to denote the\nset of node v’s neighbors. For each node and edge, we use\nfnode and fedge represents the feature dimensions, respec-\ntively. Following [Yang et al., 2019 ] that passing message\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n2243\nMessage\nUpdate\nNorm\nMessage\nInteract\nNorm\nNorm\nMultihead\nAttention\nNorm\nNode\nUpdate\nMessage\nAggregate\nNorm\nAttention\nUpdate\nNorm\nFeed\nForward\nMPNN Transformer CoMPT\nNode Attribute Edge Attribute\n⊕ ⊕\n⊕\n⊕\n⊕⊕\n⊕ Feed\nForward\nFeed\nForward\nFigure 1: Comparing message passing procedure among MPNN\n(left), Transformer (middle) and CoMPT (right).\nwith directed edges, we treat molecular structures as directed\ngraphs to avoid messages being passed along any unneces-\nsary loops in the aggregation procedure. As such, we use\nxv 2Rfnode to represent the initial features of node v, and\neuv 2Rfedge are the initial features of the edge (u;v) with\ndirection u !v. X 2Rn\u0002fnode and E 2Rn\u0002n\u0002fedge are\nthe matrices form for all nodes and edges. Besides, there are\ngenerally two categories of supervised tasks in the molecular\ngraph learning problems: i) Graph classiﬁcation/regression,\nwhere a set of molecular graphs fG1;:::;G Ngand their la-\nbels/targets fy1;:::;y Ngare given, and the task is to pre-\ndict the label/target of a new graph. ii) Node classiﬁca-\ntion/regression, where each node v in multiple graphs has a\nlabel/target yv, and the task is to predict the labels/targets of\nnodes in the unseen graphs.\nAttention mechanism. Our CoMPT model are built on the\ntransformer encoder framework, in which the attention mod-\nule is the main building block. The usual implementation of\nthe attention module is the dot product self-attention, which\ntakes inputs with a set of queries, keys, and values (q;k;v )\nthat are projected from hidden node features h(X). Then\nit computes the dot product of the query with all keys and\napplies a softmax function to obtain weights on the values.\nBy stacking the set of (q;k;v ) s into matrices (Q;K;V ),\nit allows highly optimized matrix multiplication operations.\nSpeciﬁcally, the outputs can be formulated as:\n\u001a [Q;K;V ] =h(X)[WQ;WK;WV]\nAttention(Q;K;V ) =softmax(QKT=\np\nd)V (1)\nwhere d is the dimension of q and k. Furthermore, we fo-\ncus on the multi-head attention, where l attention layers are\nstacked together. The output matrix can be extended as,\n\u001a Multihead(Q; K; V) =Cat(head1; : : : ; headl)WO\nheadi = Attention(h(X)WQ\ni ; h(X)WK\ni ; h(X)WV\ni ) (2)\nwhere WQ\ni ;WK\ni ;WV\ni are the projection matrices of head i.\n3.2 The Framework of CoMPT\nEncoding for node position and edge direction. Com-\npared to the Transformer encoder that only takes node at-\ntributes, our CoMPT takes three inputs: node features X,\nedge features E, and the topology connection matrix A 2\nRn\u0002n that is computed by using the length of the shortest\npath between any two nodes. Since the initial node and edge\nfeatures (more details are listed in the Appendix) do not in-\nvolve information related to the node position and the edge\ndirection, we need to add annotations explicitly with these\nfeatures before being fed into the CoMPT model. Speciﬁ-\ncally, for any node vi(i = 1;2;:::;n ) and its correspond-\ning initial feature xi, we train a learnable position embedding\nvector posi according to the atomic index of the node, and\nthen add the initial features to get the hidden features, which\ncan be formulated as:\nh(xi) =node_embedding(xi) +posi (3)\nwhere node_embedding() projects the initial feature to the\ncorresponding dimension. For any directed edge euv, we fol-\nlowed the previous method in [Song et al., 2020 ] by adding\nthe source node feature to the initial feature. It could be for-\nmulated as:\nh(euv) =edge_embedding(euv) +h(xu) (4)\nwhere edge_embedding() also projects the initial feature to\nthe corresponding dimension. For convenience, we set the\nsame dimension f for all hidden features.\nNode-Edge message interaction module. The key idea be-\nhind CoMPT is that we use hidden node features h(X) and\nhidden edge features h(E) to compute a message interaction\nscores M, which replaces the self-attention scores in the orig-\ninal encoder layer. Speciﬁcally, three matrices Q;K;V are\nﬁrstly calculated by the formula:\u001a\n[Q;V ] =h(X)[WQ;WV]\nK = h(E)WK (5)\nwhere WQ;WK;WV are the projection matrices. The mes-\nsage interaction matrix T is generated by the inner product:\nT = matmul(Q;K:tranpose(\u00002;\u00001)) (6)\nor equivalently in each position\nT[i;u;v ] =matmul(qi;kuv) (7)\nwhere qi and kuv denote the hidden vector of node iand di-\nrected edge (u;v), respectively. The intuition behind this ten-\nsor product is straight-forward: we compute the scalar prod-\nuct between each node and edge in order to generate the struc-\ntural message related to the molecule for the ﬁnal prediction.\nSubsequently, a selection step is applied to the message inter-\naction matrix T to preserve the molecular graph connectiv-\nity. Here, we select three types of matrices according to the\nnode’s neighbors: the node interacts with its outgoing edges\n(Mo), incoming edges (M i) and the self-loop edge (M d).\nFor convenience, the generation and the selection step could\nbe merged by einsum operation with the formula below:\n( Mo = einsumnf;nmf!nm(Q;K)\nMi = einsumnf;mnf!nm(Q;K)\nMd = diag(Mo) =diag(Mi)\n(8)\nAfter computing three matrices, we normalize them to sum up\nto 1.0 in each row with the softmax function \u001b, and compute\nthe ﬁnal message by:\nM = \u001b(Mo) +\u001b(Mi) \u0000\u001b(Md) (9)\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n2244\nThis operation eliminates the double-counted self-loop mes-\nsage and avoids the information explosion. This ﬁnal mes-\nsage is further utilized to update the node hidden features\nh(X) and edge hidden features h(E) in each encoder layer.\nFurthermore, in the situation of multi-head attention, similar\nto the original transformer framework, we also stack all lat-\ntention blocks at the end of each layer.\nResidual message update module. In the original trans-\nformer encoder framework, the self-attention scores are uti-\nlized to compute the weighted sum of vectors for each node,\nwhich could be regarded as an updated operation that is ap-\nplied to all nodes. In contrast, we utilize the message inter-\naction scores M to update the node hidden features h(X).\nBesides, inspired by [Song et al., 2020], the edge hidden fea-\ntures are also updated according to the rich information that\ncomes from M. The update operation is concluded as:\n\u001a\nh(X) =matmul(M;V )\nh(E) =M \fK (10)\nwhere \f denotes the element-wise operation. Besides,\nCoMPT model has multiple stacked layers, where each en-\ncoder layer consists of a multi-head message interaction mod-\nule, message update module and a position-wise forward\nmodule. To make the training step more stable, we adopt\nthe post layer norm module [Xiong et al., 2020 ] before get-\nting into each module. Furthermore, residual connections be-\ntween any two encoder layers are added for reducing the van-\nishing of the gradient, which can be formulated as:\n\u001a\nhk+1(X) =hk(X) +Encoder(hk(X);hk(E))\nhk+1(E) =hk(E) +Encoder(hk(X);hk(E)) (11)\nwhere k represents the index of the encoder layer,\nEncoder(\u0001) denotes the whole encoder layer with the vari-\nous modules mentioned above.\n3.3 Message Diffusion and Global Pooling\nThe key to accurately predict the properties on the graph-\nlevel/node-level tasks is how to keep the message inter-\nacting correctly. Previous studies have shown that deep\nmessage aggregation of GNN will lead to an over-smooth\nphenomenon[Li et al., 2018 ], where the features of nodes\nwithin the graph will converge to the same values.\nTo alleviate this issue, we design a simple attenuation\nmechanism for message passing during iteration to delay the\nprocess of aggregating redundant information to nodes. In\nparticular, each hidden vector in message interaction scores\nM could be regarded as the prepared message sent from the\nrow index of the node to the column index of the node, and the\ntopology connection matrix Ashows the distance of shortest\npath between two nodes. We add the attenuation coefﬁcient\nby using the Gaussian kernel function with the formula:\nM(u;v) =M(u;v)e\u0000\u000bA(u;v) (12)\nwhere M(u;v) denotes the message sending from u to v,\nA(u;v) means the shortest path betweenuand v, \u000b2[0;1] is\na trainable coefﬁcient to control the attenuation level. It is ob-\nvious that with the increase of distance, the message will de-\ncay rapidly in the beginning, and then turn smooth for a long\ndistance. After applying this mechanism, we can defer the\nover-smoothness of the aggregating process to a certain de-\ngree. The ablation study in the section 4.3 also shows that the\nattenuation mechanism improves the prediction performance.\nFinally, for the graph-level tasks, a readout opera-\ntor/generation layer is added to obtain a ﬁxed feature vec-\ntor for the molecule. Here, we adopt a Gated Recurrent\nUnit (GRU) for global pooing following[Gilmer et al., 2017;\nSong et al., 2020] as:\nz=\nX\nx2V\nGRU(h(x)) (13)\nwhere h(x) is the set of atom representations in the molec-\nular graph, and GRU is the Gated Recurrent Unit. Finally,\nwe perform downstream property prediction ^y= f(h) where\nf(\u0001) is a fully connected layer.\n4 Experiments\nIn this section, we evaluate the proposed model CoMPT on\nthree kinds of tasks. We aim to answer the following research\nquestions:\n• RQ1: How does CoMPT model perform compared with\nstate-of-the-art molecular property prediction methods?\n• RQ2: How do different components (i.e, node position,\nedge direction, and message diffusion mechanism) af-\nfect CoMPT?\n• RQ3: Can CoMPT model provide better representations\nfor the attributed molecular graphs?\n4.1 Experiment Setups\nBenchmark Datasets\nTo enable head-to-head comparisons of CoMPT to exist-\ning molecular representation methods, we evaluated our pro-\nposed model on nine benchmark datasets across three kinds\nof tasks from [Wu et al., 2018] and [Jonas and Kuhn, 2019],\neach kind of which consists of 2 to 4 public benchmark\ndatasets, including BBBP, Tox21, Sider, and ClinTox for\nGraph Classiﬁcation tasks, ESOL, FreeSolv and Lipophilic-\nity for Graph Regression tasks, chemical shift prediction of\nhydrogen and carbon for Node Regression tasks. The statis-\ntics of datasets are shown in Table S1.\nIn the graph-level task, following the previous works, we\nutilized a 5-fold cross-validation and replicate experiments\non each task ﬁve times. Note that we adopted the scaffold\nsplit method recommended by [Yang et al., 2019] to split the\ndatasets into training, validation, and test, with a 0.8/0.1/0.1\nratio. Scaffold Split is a more challenging and realistic eval-\nuation setting in molecular property prediction tasks by guar-\nanteeing the high molecular scaffold diversity of the training,\nvalidation, and test sets.\nIn the node-level task, we follow the previous study[Jonas\nand Kuhn, 2019] by randomly splitting the dataset into 80%\nas the training set and 20% as the test set, and then use 95%\nof training data to train the model and the remaining 5% to\nvalidate the model for early stopping. All methods report the\nmean and standard deviation of corresponding metrics. To\nimprove model performance, we applied the grid search to\nobtain the best hyper-parameters of the models.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n2245\nTask Graph Classiﬁcation(ROC-AUC) Graph Regression(RMSE)\nDataset BBBP Tox21 Sider ClinTox ESOL FreeSolv Lipophilicity\nTF_Robust 0.860 \u0006 0.087 0.698 \u0006 0.012 0.607 \u0006 0.033 0.765 \u0006 0.085 1.722 \u0006 0.038 4.122 \u0006 0.085 0.909 \u0006 0.060\nGCN 0.877 \u0006 0.036 0.772 \u0006 0.041 0.593 \u0006 0.035 0.845 \u0006 0.051 1.068 \u0006 0.050 2.900 \u0006 0.135 0.712 \u0006 0.049\nWeave 0.837 \u0006 0.065 0.741 \u0006 0.044 0.543 \u0006 0.034 0.823 \u0006 0.023 1.158 \u0006 0.055 2.398 \u0006 0.250 0.813 \u0006 0.042\nSchNet 0.847 \u0006 0.024 0.767 \u0006 0.025 0.545 \u0006 0.038 0.717 \u0006 0.042 1.045 \u0006 0.064 3.215 \u0006 0.755 0.909 \u0006 0.098\nN-Gram 0.912 \u0006 0.013 0.769 \u0006 0.027 0.632 \u0006 0.005 0.855 \u0006 0.037 1.100 \u0006 0.160 2.512 \u0006 0.190 0.876 \u0006 0.033\nAttentiveFP 0.908 \u0006 0.050 0.807 \u0006 0.020 0.605 \u0006 0.060 0.933 \u0006 0.020 0.853 \u0006 0.060 2.030 \u0006 0.420 0.650 \u0006 0.030\nMPNN 0.913 \u0006 0.041 0.808 \u0006 0.024 0.595 \u0006 0.030 0.879 \u0006 0.054 1.167 \u0006 0.430 2.185 \u0006 0.952 0.672 \u0006 0.051\nMGCN 0.850 \u0006 0.064 0.707 \u0006 0.016 0.552 \u0006 0.018 0.634 \u0006 0.042 1.266 \u0006 0.147 3.349 \u0006 0.097 0.650 \u0006 0.030\nDMPNN 0.919 \u0006 0.030 0.826 \u0006 0.023 0.632 \u0006 0.023 0.897 \u0006 0.040 0.980 \u0006 0.258 2.177 \u0006 0.914 0.653 \u0006 0.046\nCMPNN 0.927 \u0006 0.017 0.806 \u0006 0.016 0.616 \u0006 0.003 0.902 \u0006 0.008 0.798 \u0006 0.112 2.007 \u0006 0.442 0.614 \u0006 0.029\nSmiles Transformer 0.900 \u0006 0.053 0.706 \u0006 0.021 0.559 \u0006 0.017 0.905 \u0006 0.064 1.144 \u0006 0.118 2.246 \u0006 0.237 1.169 \u0006 0.031\nGROVER 0.911 \u0006 0.008 0.803 \u0006 0.020 0.624 \u0006 0.006 0.884 \u0006 0.013 0.911 \u0006 0.116 1.987 \u0006 0.072 0.643 \u0006 0.030\nCoMPT 0.938 \u0006 0.021 0.809 \u0006 0.014 0.634 \u0006 0.030 0.934 \u0006 0.019 0.774 \u0006 0.058 1.855 \u0006 0.578 0.592 \u0006 0.048\nTable 1: Prediction results of CoMPT and baselines on seven chemical graph datasets. We used a 5-fold cross validation with scaffold split\nand replicated experiments on each tasks for ﬁve times. Mean and standard deviation of AUC or RMSE values are reported.\nBaselines Comparison\nWe comprehensively compared CoMPT against with 12 base-\nline methods in the graph level task. These models were most\nshown in the MoleculeNet [Wu et al., 2018 ] and GROVER\nas follows: TF_Robust [Ramsundar et al., 2015 ] is a DNN-\nbased multitask framework taking the molecular ﬁngerprints\nas the input. GCN, Weave, and SchNet [Duvenaud et al.,\n2015; Kearnes et al., 2016 ] are three graph convolutional\nmodels. N-Gram [Liu et al., 2019] is a state-of-the-art unsu-\npervised representation method for molecular property pre-\ndiction. AttentiveFP [Xiong et al., 2019 ] is an extension of\nthe graph attention network. MPNN and its variants MGCN\n[Lu et al., 2019], DMPNN and CMPNN are models consid-\nering the edge features during message passing. Speciﬁcally,\nto demonstrate the power of the message-interaction mod-\nule, we also compare CoMPT with two transformer model:\nSmiles Transformer [Honda et al., 2019] and GROVER. For\na fair comparison, we only report the results without the pre-\ntrained strategy.\nIn the node level task, we compared our CoMPT model\nwith the other 3 proposed methods in this benchmark. The\nﬁrst one is HOSE codes, which attempted to summarize the\nneighborhood around each atom in concentric spaces, and\nthen use a nearest-neighbor approach to predict the particu-\nlar shift value. The rest baselines include GCN [Jonas and\nKuhn, 2019 ] and MPNN [Kwon et al., 2020 ], where they\nused different deep graph neural networks to improve the per-\nformance of prediction.\n4.2 Performance Comparison (RQ1)\nPerformance in graph level task. Table 1 displays the\ncomplete results of each model on all datasets, where cells\nin the gray shadow denote the previous best methods, and\ncells with the bold style show the best result achieved by\nCoMPT. Table 1 presents some observations: (1) Both the\nmessage passing neural network and transformer framework\nperform better than graph neural network on most datasets,\nand CoMPT combines the advantages of them to achieve\nthe best performances on 6 out of 7 datasets. Compared\nto the previous best message passing method CMPNN and\ntransformer method GROVER, the general improvements are\nTask Node Regression(MAE)\nDataset 1H-NMR 13C-NMR\nHOSE 0.33 2.85\nGCN 0.28 1.43\nMPNN 0.229 \u00060.002 1.355 \u00060.022\nCoMPT 0.214 \u00060.003 1.321 \u00060.012\nTable 2: Performance on Node-level tasks\n3.4% (2.0% on classiﬁcation tasks and 3.4% on regression\ntasks) and 4.7% (2.7% on classiﬁcation tasks and 4.7% on\nregression tasks), respectively. This notable incresing sug-\ngests the effectiveness of the structural representation learned\nby CoMPT for graph level prediction tasks. (2) The message\npassing neural network performs better than the transformer\nneural network, indicating the importance of edge features\nrelative to only an adjacency matrix or distance matrix. (3) In\nthe situation of small dataset, such as the Freesolv task with\nonly 642 labeled molecules, CoMPT gains a 6.6% relative\nimprovement over previous SOTAs, conﬁrming that CoMPT\nmodel could enhance the performance on the task with few\nlabeled data.\nPerformance in node level task. Table 2 shows the com-\nparison results of the baseline and CoMPT on the predic-\ntion of 1H-NMR and 13C-NMR spectra in terms of MAE.\nWe performed experiments 5 times independently with dif-\nferent random seeds and report the average and standard de-\nviation over the 5 repetitions. The average values over the 5\nrepetitions for 1H-NMR and 13C-NMR are 0.214 and 1.321\nppm per NMR-active atom, respectively. This indicates that\nour CoMPT model could extract the meaningful latent node\nrepresentations and thus enable more accurate predictions of\nNMR spectra for new molecules.\n4.3 Ablation Study (RQ2)\nWe conducted ablation studies on three benchmark datasets\nto investigate factors that inﬂuence the performance of the\nproposed CoMPT framework.\nAs shown in Table 3, CoMPT with the node position, edge\ndirection, and message diffusion shows the best performance\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n2246\nCMPNN CoMPT (without diffusion) CoMPT\nFigure 2: T-SNE visualization of atom embeddings for three similar molecules (in three colors) that have a common scaffold but various\nside-chains in ClinTox dataset. Ideally, the scaffold atom embeddings of these three molecules should be mixed together while the unique\nside-chains’ embeddings should be distinguishable.\nDataset ClinTox Lipophilicity 1H-NMR\nWithout All 0.862 0.653 0.231\nWithout message diffusion 0.868 0.651 0.221\nWithout node position 0.903 0.614 0.217\nWithout edge direction 0.902 0.612 0.218\nCoMPT 0.934 0.592 0.214\nTable 3: Ablation results on three kinds of datasets\namong all architectures. The exclusion of all three modules in\nthe “without All” variant performed the worst. The exclusion\nof the message diffusion mechanism caused larger decreases\nin performances than the ones excluding two other modules,\nshowing the importance of reducing the message enrichment\nexplosion. Additionally, the uses of node position and edge\ndirection are both helpful for the ﬁnal performance.\n4.4 Atomic Representation Visualization (RQ3)\nAs shown in [Li et al., 2018], the node embeddings obtained\nby deep GNNs tend to be over-smoothed and become indis-\ntinguishable, while shallow GNNs cannot capture atom po-\nsitions within the broader context of the molecular graph.\nTo investigate whether the CoMPT alleviated these issues as\nexpected, we used t-distributed stochastic neighbor embed-\nding (t-SNE) to visualize the atom embedding distributions\nof three similar compounds that have a common scaffold (the\nfour-membered ring) but different side-chains. Ideally, the\nscaffold atom embeddings of these three molecules should\nbe mixed together while the unique side-chains’ embeddings\nshould be distinguishable.\nFigure 2 shows the projected atomic embeddings extracted\nfrom different models using the t-SNE with default settings.\nOverall, three methods provide reasonable results. MPNN in-\nherits the over-smoothness issue from the GNN, making the\natom embeddings indistinguishable within the graph . In con-\ntrast, both CoMPT models (with or without diffusion) can\nscatter the atoms well with distinguishable node embeddings.\nRelative to CoMPT without diffusion, CoMPT could exactly\nmix the scaffold atom embeddings and differentiate the side\nchains. More interestingly, CoMPT can distinguish the same\nfunctional groups in different chemical environments (the\nHydroxy Ketones in green and in yellow, respectively). These\nresults suggest that CoMPT can not only alleviate the over-\nsmoothness but also capture better representations within the\nbroader context of the molecular graph as expected.\nIn the node-level tasks, our CoMPT model reaches 0.214\nMAE that many molecules with densely packed 1H-NMR\nspectra can be resolved at these levels of accuracy. As an ex-\nample, Figure S1 depicts the structure of the 3-Formylbenzoic\nacid, which has 6 hydrogen atoms, labeled from 11 to 16 with\npeaks within the 4-14 ppm range. The small difference be-\ntween the ground truth and the prediction further proves that\nour model has a good capability in the node-level tasks.\n5 Conclusions\nIn this paper, we propose a Communicative Message Pass-\ning Transformer (CoMPT) neural network to improve the\nmolecular representation by reinforcing the message inter-\nactions between nodes and edges based on the Transformer\nmodel. Further, we introduce a message diffusion mechanism\nto decay the message enrichment explosion as well as over-\nsmoothness during the message passing process. Extensive\nexperiments demonstrate that our CoMPT model obtains su-\nperior performance against state-of-the-art baselines on both\ngraph-level tasks and node-level tasks.\nAcknowledgments\nThis work has been supported by the National Key R&D\nProgram of China(2020YFB0204803), National Natural Sci-\nence Foundation of China(61772566), Guangdong Key Field\nR&D Plan(2019B020228001, 2018B010109006), Introduc-\ning Innovative and Entrepreneurial Teams(2016ZT06D211),\nGuangzhou S&T Research Plan(202007030010).\nReferences\n[Chen et al., 2019] Benson Chen, Regina Barzilay, and\nTommi Jaakkola. Path-augmented graph transformer net-\nwork. arXiv preprint arXiv:1905.12712, 2019.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n2247\n[Duvenaud et al., 2015] David K Duvenaud, Dougal\nMaclaurin, Jorge Iparraguirre, Rafael Bombarell, Tim-\nothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams.\nConvolutional networks on graphs for learning molecular\nﬁngerprints. In Advances in neural information processing\nsystems, pages 2224–2232, 2015.\n[Gilmer et al., 2017] Justin Gilmer, Samuel S Schoenholz,\nPatrick F Riley, Oriol Vinyals, and George E Dahl. Neural\nmessage passing for quantum chemistry. InProceedings of\nthe 34th International Conference on Machine Learning-\nVolume 70, pages 1263–1272. JMLR. org, 2017.\n[Honda et al., 2019] Shion Honda, Shoi Shi, and Hiroki R\nUeda. Smiles transformer: Pre-trained molecular ﬁn-\ngerprint for low data drug discovery. arXiv preprint\narXiv:1911.04738, 2019.\n[Jonas and Kuhn, 2019] Eric Jonas and Stefan Kuhn. Rapid\nprediction of nmr spectral properties with quantiﬁed un-\ncertainty. Journal of cheminformatics, 11(1):1–7, 2019.\n[Kearnes et al., 2016] Steven Kearnes, Kevin McCloskey,\nMarc Berndl, Vijay Pande, and Patrick Riley. Molecular\ngraph convolutions: moving beyond ﬁngerprints. Jour-\nnal of computer-aided molecular design, 30(8):595–608,\n2016.\n[Kwon et al., 2020] Youngchun Kwon, Dongseon Lee,\nYoun-Suk Choi, Myeonginn Kang, and Seokho Kang.\nNeural message passing for nmr chemical shift predic-\ntion. Journal of Chemical Information and Modeling,\n60(4):2024–2030, 2020.\n[Li et al., 2018] Qimai Li, Zhichao Han, and Xiao-Ming Wu.\nDeeper insights into graph convolutional networks for\nsemi-supervised learning. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 32, 2018.\n[Liu et al., 2019] Shengchao Liu, Mehmet F Demirel, and\nYingyu Liang. N-gram graph: Simple unsupervised rep-\nresentation for graphs, with applications to molecules.\nIn Advances in Neural Information Processing Systems,\npages 8464–8476, 2019.\n[Lu et al., 2019] Chengqiang Lu, Qi Liu, Chao Wang,\nZhenya Huang, Peize Lin, and Lixin He. Molecular prop-\nerty prediction: A multilevel quantum interactions model-\ning perspective. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, volume 33, pages 1052–1060,\n2019.\n[Maziarka et al., 2020] Łukasz Maziarka, Tomasz Danel,\nSławomir Mucha, Krzysztof Rataj, Jacek Tabor, and\nStanisław Jastrz˛ ebski. Molecule attention transformer.\narXiv preprint arXiv:2002.08264, 2020.\n[Ramsundar et al., 2015] Bharath Ramsundar, Steven\nKearnes, Patrick Riley, Dale Webster, David Konerding,\nand Vijay Pande. Massively multitask networks for drug\ndiscovery. arXiv preprint arXiv:1502.02072, 2015.\n[Rogers and Hahn, 2010] David Rogers and Mathew Hahn.\nExtended-connectivity ﬁngerprints. Journal of chemical\ninformation and modeling, 50(5):742–754, 2010.\n[Rong et al., 2020] Yu Rong, Yatao Bian, Tingyang Xu,\nWeiyang Xie, Ying Wei, Wenbing Huang, and Junzhou\nHuang. Self-supervised graph transformer on large-scale\nmolecular data. Advances in Neural Information Process-\ning Systems, 33, 2020.\n[Shang et al., 2018] Chao Shang, Qinqing Liu, Ko-Shin\nChen, Jiangwen Sun, Jin Lu, Jinfeng Yi, and Jinbo Bi.\nEdge attention-based multi-relational graph convolutional\nnetworks. arXiv preprint arXiv:1802.04944, 2018.\n[Song et al., 2020] Ying Song, Shuangjia Zheng, Zhang-\nming Niu, Zhang-Hua Fu, Yutong Lu, and Yuedong\nYang. Communicative representation learning on at-\ntributed molecular graphs. In Proceedings of the Twenty-\nNinth International Joint Conference on Artiﬁcial Intelli-\ngence,(IJCAI 2020), pages 2831–2838, 2020.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing sys-\ntems, pages 5998–6008, 2017.\n[Veliˇckovi´c et al., 2017] Petar Veliˇckovi´c, Guillem Cucurull,\nArantxa Casanova, Adriana Romero, Pietro Lio, and\nYoshua Bengio. Graph attention networks. arXiv preprint\narXiv:1710.10903, 2017.\n[Withnall et al., 2020] M Withnall, E Lindelöf, O Engkvist,\nand H Chen. Building attention and edge message pass-\ning neural networks for bioactivity and physical–chemical\nproperty prediction. Journal of Cheminformatics, 12(1):1,\n2020.\n[Wu et al., 2018] Zhenqin Wu, Bharath Ramsundar, Evan N\nFeinberg, Joseph Gomes, Caleb Geniesse, Aneesh S\nPappu, Karl Leswing, and Vijay Pande. Moleculenet: a\nbenchmark for molecular machine learning. Chemical sci-\nence, 9(2):513–530, 2018.\n[Xiong et al., 2019] Zhaoping Xiong, Dingyan Wang, Xi-\naohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong Li,\nZhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang,\net al. Pushing the boundaries of molecular representation\nfor drug discovery with the graph attention mechanism.\nJournal of Medicinal Chemistry, 2019.\n[Xiong et al., 2020] Ruibin Xiong, Yunchang Yang, Di He,\nKai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,\nYanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer nor-\nmalization in the transformer architecture. arXiv preprint\narXiv:2002.04745, 2020.\n[Yang et al., 2019] Kevin Yang, Kyle Swanson, Wengong\nJin, Connor Coley, Philipp Eiden, Hua Gao, Angel\nGuzman-Perez, Timothy Hopper, Brian Kelley, Miriam\nMathea, et al. Are learned molecular representations ready\nfor prime time? arXiv preprint arXiv:1904.01561, 2019.\n[Zhang and Meng, 2019] Jiawei Zhang and Lin Meng. Gres-\nnet: Graph residual network for reviving deep gnns from\nsuspended animation. arXiv preprint arXiv:1909.05729,\n2019.\nProceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)\n2248",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7411692142486572
    },
    {
      "name": "Message passing",
      "score": 0.6393354535102844
    },
    {
      "name": "Theoretical computer science",
      "score": 0.5896531343460083
    },
    {
      "name": "Inductive bias",
      "score": 0.5016696453094482
    },
    {
      "name": "Feature learning",
      "score": 0.49405935406684875
    },
    {
      "name": "Graph",
      "score": 0.4833900034427643
    },
    {
      "name": "Molecular graph",
      "score": 0.4652123749256134
    },
    {
      "name": "Transformer",
      "score": 0.44895848631858826
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.445330947637558
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3572177588939667
    },
    {
      "name": "Distributed computing",
      "score": 0.2654263377189636
    },
    {
      "name": "Multi-task learning",
      "score": 0.13179093599319458
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    }
  ],
  "cited_by": 36
}