{
    "title": "Reference-Free Summarization Evaluation with Large Language Models",
    "url": "https://openalex.org/W4392669730",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2315112532",
            "name": "Abbas Akkasi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2482369292",
            "name": "Kathleen Fraser",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2306539953",
            "name": "Majid Komeili",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3159259047",
        "https://openalex.org/W4206256378",
        "https://openalex.org/W3113199825",
        "https://openalex.org/W2970785793",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W4297435087",
        "https://openalex.org/W4283462007",
        "https://openalex.org/W2250342921",
        "https://openalex.org/W4384211370",
        "https://openalex.org/W1789328935",
        "https://openalex.org/W2935960257",
        "https://openalex.org/W2970892365",
        "https://openalex.org/W4385572904",
        "https://openalex.org/W2560079626",
        "https://openalex.org/W4379470871",
        "https://openalex.org/W4313448817",
        "https://openalex.org/W3098968529",
        "https://openalex.org/W4389523995",
        "https://openalex.org/W4322760121",
        "https://openalex.org/W3035628162",
        "https://openalex.org/W2123301721",
        "https://openalex.org/W4362598574",
        "https://openalex.org/W4392669714"
    ],
    "abstract": "With the continuous advancement in unsupervised learning methodologies, text generation has become increasingly pervasive. However, the evaluation of the quality of the generated text remains challenging. Human annotations are expensive and often show high levels of disagreement, in particular for certain tasks characterized by inherent subjectivity, such as translation and summarization.Consequently, the demand for automated metrics that can reliably assess the quality of such generative systems and their outputs has grown more pronounced than ever. In 2023, Eval4NLP organized a shared task dedicated to the automatic evaluation of outputs from two specific categories of generative systems: machine translation and summarization. This evaluation was achieved through the utilization of prompts with Large Language Models. Participating in the summarization evaluation track, we propose an approach that involves prompting LLMs to evaluate six different latent dimensions of summarization quality. In contrast to many previous approaches to summarization assessments, which emphasize lexical overlap with reference text, this method surfaces the importance of correct syntax in summarization evaluation. Our method resulted in the second-highest performance in this shared task, demonstrating its effectiveness as a reference-free evaluation.",
    "full_text": "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 193–201\nNovember 1, 2023 ©2023 Association for Computational Linguistics\nReference-Free Summarization Evaluation with Large Language Models\nAbbas Akkasi\nSchool of Computer Science, Carleton University\nabbasakkasi@cunet.carleton.ca\nKathleen C. Fraser\nNational Research Council Canada and\nSchool of Computer Science,\nCarleton University\nkathleen.fraser@nrc-cnrc.gc.ca\nMajid Komeili\nSchool of Computer Science,\nCarleton University\nmajidkomeili@cunet.carleton.ca\nAbstract\nWith the continuous advancement in unsuper-\nvised learning methodologies, text generation\nhas become increasingly pervasive. However,\nthe evaluation of the quality of the generated\ntext remains challenging. Human annotations\nare expensive and often show high levels of dis-\nagreement, in particular for certain tasks charac-\nterized by inherent subjectivity, such as transla-\ntion and summarization. Consequently, the de-\nmand for automated metrics that can reliably as-\nsess the quality of such generative systems and\ntheir outputs has grown more pronounced than\never. In 2023, Eval4NLP organized a shared\ntask dedicated to the automatic evaluation of\noutputs from two specific categories of genera-\ntive systems: machine translation and summa-\nrization. This evaluation was achieved through\nthe utilization of prompts with Large Language\nModels. Participating in the summarization\nevaluation track, we propose an approach that\ninvolves prompting LLMs to evaluate six differ-\nent latent dimensions of summarization qual-\nity. In contrast to many previous approaches\nto summarization assessments, which empha-\nsize lexical overlap with reference text, this\nmethod surfaces the importance of correct syn-\ntax in summarization evaluation. Our method\nresulted in the second-highest performance in\nthis shared task, demonstrating its effectiveness\nas a reference-free evaluation.\n1 Introduction\nText summarization is a natural language process-\ning (NLP) task that aims to condense a given text\ninto a shorter version while retaining its most es-\nsential information. It plays a crucial role in infor-\nmation retrieval, content extraction, and document\nmanagement. Automatic summarization systems,\nwhether extractive (selecting and rearranging ex-\nisting sentences) or abstractive (generating novel\nsentences), offer significant advantages in various\ndomains such as news articles, legal documents,\nacademic papers, and online content. The ability\nto generate concise and coherent summaries en-\nhances information accessibility, facilitates quicker\ndecision-making, and improves user experience\nin an era of information overload (Cajueiro et al.,\n2023).\nA good summary plays a pivotal role in informa-\ntion processing and communication across various\ndomains. It serves as a concise yet comprehensive\nrepresentation of a larger body of text, distilling the\ncore ideas, key information, and essential insights.\nThe importance of a good summary lies in its abil-\nity to save time and effort for readers, enabling\nthem to grasp the main points quickly and make\ninformed decisions without delving into extensive\ndocuments or articles. A well-crafted summary is\nnot merely a condensation of content; it is a bridge\nbetween complex information and its audience, en-\nsuring that knowledge is accessible and actionable.\nEvaluating the output of summarization systems\nis of paramount importance to ensure their effec-\ntiveness and utility. It involves assessing key fac-\ntors like coherence, informativeness, and fluency.\nAdequate evaluation frameworks help researchers\nand practitioners to fine-tune algorithms, identify\nareas for improvement, and compare different sum-\nmarization methods (Indu and Kavitha, 2016). A\ncomprehensive evaluation not only facilitates the\ndevelopment of robust summarization algorithms\nbut also guides their practical applications in real-\nworld scenarios, addressing the increasing need for\nefficient content summarization in the digital age.\nNumerous well-established evaluation metrics,\nas detailed in Section 2, are typically employed\nto assess the quality of generated summaries\ncompared to reference summaries. These met-\nrics include, but are not limited to, ROUGE\n(Recall-Oriented Understudy for Gisting Evalua-\ntion), BLEU (Bilingual Evaluation Understudy),\nMETEOR, BertScore, and MoverScore, among oth-\ners. The majority of the metrics employed for the\nevaluation of generated summaries share a com-\n193\nmon requisite; namely, the availability of reference\nsummaries. Although reference-based evaluation\nmethods can offer valuable insights into the perfor-\nmance of summarization systems, they come with\ninherent limitations. One significant challenge is\nthe subjectivity of reference summaries. Summa-\nrization tasks often involve multiple valid ways to\ncondense and express content, leading to diverse\nreference summaries for the same source text. Con-\nsequently, reliance on a limited set of references\ncan introduce bias and fail to capture the full spec-\ntrum of acceptable summarization outputs (Stein-\nberger and Ježek, 2009).\nAnother problem with reference-based evalua-\ntion is the issue of task-specific references. Cre-\nating reference summaries requires significant hu-\nman effort, making it impractical to amass a large\nand diverse reference set for every possible source\ntext. As a result, reference summaries may not\nadequately cover the variety of linguistic styles,\ndomain-specific terminologies, or nuances in sum-\nmarization needs, leading to biased evaluations that\nfavor systems generating summaries similar to the\navailable references.\nFurthermore, most reference-based metrics pri-\nmarily hinge on the presence or absence of specific\nwords within generated summaries as the core el-\nement of their evaluation criteria. Nevertheless,\nother critical factors, such as coherence, readabil-\nity, fluency, and consistency, among others, have\nbeen recognized as pivotal elements in the usabil-\nity of text summaries (Fabbri et al., 2020). These\nessential aspects of summary assessment can be\nregarded as latent dimensions in the overall quality\nassessment.\nTo overcome the previously discussed challenges\nand in light of the recent advancements in Large\nLanguage Models (LLMs) and their widespread\napplicability, the Eval4NLP workshop organized a\nshared task. This task was specifically designed to\ninvestigate whether LLMs can be used to evaluate\ntext summarizes, solely on the basis of the original\ntext. With this aim in mind, the organizers provided\na list of six LLMs sourced from Hugging Face, as\noutlined in Section 5. These models are diverse in\ntheir parameter counts and training data.\nWe participate in this challenge by designing\ndifferent types of prompts focusing on the latent di-\nmensions of the evaluation process. We conducted\nvarious experiments combining different prompts\nwith the six available LLMs – including both large\nand small models – and evaluated the results on\nthe training and validation sets to develop the final\nmethodology. The final evaluation on the test set\nrevealed that our best proposed prompt, coupled\nwith a smaller LLM, achieved a notable Kendall\nτ correlation value of 0.49. This outcome posi-\ntioned our system as the second-best performer in\nthe competition.\nThe remainder of this paper is structured as\nfollows: we commence with a review of related\nwork in Section 2. Section 3 is dedicated to the\ndataset employed in our experiments, providing an\noverview of its characteristics. Subsequently, in\nSection 4, we delve into the solutions implemented.\nIn Section 5, we elaborate on the experimental\nframework and present the results obtained. Lastly,\nwe conclude the paper in Section 6 with a discus-\nsion of our findings and areas for future work.\n2 Related Work\nThe quality evaluation of textual data generated\nin the era of natural language processing has al-\nways been seen as a difficult task because of the\ninherent complexity and diversity of textual data\n(Chen et al., 2023). The fact that a single idea can\nbe expressed in multiple ways poses a challenge\nfor reference-based methods, as they cannot cover\nall possible scenarios comprehensively, besides the\ncosts of preparing the references for the evaluation.\nOn the other hand, creating dependable reference-\nfree metrics is not a straightforward endeavor and\ncan be problematic as they must be able to correctly\nevaluate the different summaries generated from a\nsame source text.. Traditional metrics of summa-\nrization quality have also failed to take into account\nimportant aspects such as coherence, fluency, and\nconsistency (Zhang et al., 2019; Shen et al., 2022).\nVarious reference-based evaluation metrics are\nfrequently used in text generation tasks. Some of\nthe important ones are as follows: ROUGE stands\nas a widely adopted metric in the assessment of\nsummarization quality. It quantifies the degree of\noverlap in n-grams between the generated summary\nand the reference summary. ROUGE is computed\nfor various word n-gram sizes, such as 1-gram,\n2-gram, and 3-gram, and the resulting scores are\naggregated to produce a comprehensive evaluation\nscore(Lin, 2004).\nBLEU is another reference-based metric used\nto assess the quality of machine-generated text\nsummaries by measuring how closely they match\n194\nhuman-written reference summaries. It quantifies\nthe precision of n-grams in the machine summary\nthat also appear in the reference summary, pro-\nviding a score that indicates the summary’s accu-\nracy and fluency (Papineni et al., 2002). Though\nBLEU and ROUGE both evaluate language qual-\nity, they diverge in their emphasis and methodol-\nogy. BLEU places a primary focus on precision,\nwhereas ROUGE prioritizes recall as its key metric.\nMETEOR (Metric for Evaluation of Transla-\ntion with Explicit ORdering) is a text summariza-\ntion metric that evaluates the quality of machine-\ngenerated summaries by considering a variety of\nlinguistic aspects, including unigram matching,\nstemming, synonyms, and word order. It pro-\nvides a comprehensive measure of overall sum-\nmary quality and can account for different ways\nof expressing the same information, making it a\nrobust evaluation metric for text summarization\n(Banerjee and Lavie, 2005). MoverScore (Zhao\net al., 2019) and CHRF (Popovi ´c, 2015a) assess\nthe quality of generated summaries by comparing\ncharacter n-grams between the generated summary\nand human reference summaries. CHRF accounts\nfor both precision and recall and is particularly use-\nful for languages with complex morphology and\nword forms(Popovi´c, 2015b).\nMoving away from the reference-based ap-\nproach, Scialom et al. (2019) have introduced new\nmetrics that rely on question-answering and demon-\nstrated their positive outcomes when employed as\nrewards in a reinforcement learning setting. Im-\nportantly, these metrics do not depend on human\nreferences and can be computed directly from the\ntext to be summarized. In another study by Chen\net al. (2023), the authors explored the viability of\nLLMs, focusing on ChatGPT and the text-davinci\nseries models, for reference-free text quality assess-\nment. They conducted a comparative analysis of\nvarious techniques for evaluating text quality and\nidentified the utilization of an explicit score gener-\nated by the GPT model as the most efficacious and\nconsistent approach. They also discussed prompt\ndesign as an important factor influencing quality of\nscores generated by GPT model.\nBertScore is another reference-free text summa-\nrization metric that leverages BERT (Bidirectional\nEncoder Representations from Transformers) em-\nbeddings to measure the similarity between the\nmachine-generated summary and human reference\nsummaries. It considers contextual information\nand semantic similarity, providing a more nuanced\nand accurate evaluation of summary quality(Zhang\net al., 2019).\nChen and Eger (2023), introduces a novel ap-\nproach by advocating the direct utilization of pre-\ntrained Natural Language Inference (NLI) models\nas evaluation metrics. Furthermore, they devel-\noped a novel preference-based adversarial test suite\nfor machine translation and summarization met-\nrics. With this approach, there is no need for hu-\nman annotators and it is particularly well-suited\nfor reference-free evaluation. Additionally, their\nresearch findings indicate that NLI metrics exhibit\nstrong performance in the context of summariza-\ntion but yield results below the established standard\nmetrics in the domain of machine translation. In\nthe study conducted by (Kocmi and Federmann,\n2023), GEMBA, an assessment method based on\nGPT technology, was introduced. The researchers\nconducted an evaluation of their metrics by com-\nparing them to the metrics included in the WMT22\nMetrics shared task. Remarkably, their approach\ndemonstrated state-of-the-art performance on the\nMQM 2022 test set across three distinct language\npairs: English to German, English to Russian, and\nChinese to English.\nFernandes et al. (2023), did a comprehensive\nanalysis of the potential of large language mod-\nels in the context of machine translation evalua-\ntion through score prediction. They introduced a\nnovel prompting technique known as AUTOMQM,\nwhich effectively harnesses the Multidimensional\nQuality Metrics (MQM) framework for the pur-\npose of achieving interpretable machine translation\n(MT) evaluation using Large Language Models\n(LLMs).\nA study by Goyal et al. (2022) aimed to assess\nthe alignment of current reference-free evaluation\nmetrics with human preferences when ranking sum-\nmarization systems. They focused on two principal\ncategories of metrics: quality and factuality metrics.\nWithin the quality metrics, they examined SUPERT\n(Gao et al., 2020), which assesses the quality of\ngenerated summaries by contrasting them with au-\ntomatically identified pivotal sentences from the\ninput, along with BLANC (Vasilyev et al., 2020),\nwhich scrutinizes summaries via language under-\nstanding tasks. The second category of metrics is\nspecifically designed to gauge the presence of in-\naccuracies in generated summaries concerning the\nsource article.\n195\nErmakova et al. (2019) provided a comprehen-\nsive overview of existing metrics for summary eval-\nuation. They pointed out various limitations in\nthese existing evaluation frameworks and intro-\nduced an automatic evaluation framework that elim-\ninates the need for human annotations. They cate-\ngorized the evaluation metrics intoinformative met-\nrics like ROUGE and readability metrics including\ncoherence, conciseness, content, grammar, recall,\npithiness etc. Sai et al. (2022) conducted another\nextensive survey of the currently available auto-\nmatic evaluation metrics in the domain of Natural\nLanguage Generation (NLG). They subsequently\nintroduced a systematic taxonomy to categorize\nthese evaluation metrics, with the categorization\nstructured around the methodologies they employ.\nJain et al. (2023), showed that in-context learn-\ning can serve as a viable alternative to fine-tuned\nevaluation metrics for assessing NLG tasks. By\nemploying a limited set of examples, in-context\nlearning evaluators can achieve, and in some cases\nsurpass, the current state-of-the-art performance\nin multi-dimensional evaluation. This approach’s\nrobustness is evident across various in-context ex-\namples. Furthermore, the research reveals a strong\nalignment between in-context learning evaluators\nand human judgments when evaluating summaries\ngenerated by GPT-3.\nThe present study shares similarities with the\npreviously discussed reference-free evaluation met-\nrics in that it operates without the need for refer-\nence summaries. However, unlike other approaches\nthat entail intricate configurations, the model intro-\nduced here solely relies on straightforward prompts\nused with pre-trained LLMs.\n3 Data and Evaluation\nIn the Eval4NLP 2023 shared task, the dataset\nprovided for the summarization track comprises\ntraining and validation subsets, each containing\nsource texts along with their corresponding sum-\nmaries. These summaries have been generated by\na summarization model that was trained on the\nCNN/DailyMail dataset, as documented by (Fabbri\net al., 2020). Notably, the training dataset includes\nassociated scores for each generated summary rela-\ntive to its source text, which are intended for use in\nthe system development process.\nFurthermore, the organizers have also introduced\na test set, which encompasses sentences and para-\ngraphs extracted from English Wikipedia pages\ncreated subsequent to the date of July 15, 2023\n(i.e., beyond the LlAMA2 training cutoff) (Leiter\net al., 2023). For a comprehensive overview of\nthe dataset, including key statistics, please refer to\nTable 1.\nThe validation and test data sets do not include\nexplicit score annotations, necessitating partici-\npants to submit their results on the shared task page\nhosted on CodaBench 1\nThe evaluation process in this study adheres to\nthe metrics established in the WMT22 competi-\ntion, as described by (Freitag et al., 2022), and\nemploys segment-level Kendall correlation as the\nprimary evaluation metric. In the realm of statistics,\nthe Kendall rank correlation coefficient, commonly\nknown as Kendall τ coefficient, is a statistical mea-\nsure employed to assess the ordinal association\nbetween two measured variables. A τ test, which\nis a non-parametric hypothesis test used to deter-\nmine statistical dependence based on the τ coeffi-\ncient, is employed for this purpose. The ranking of\nsystems in the shared task will be determined by\ntheir Kendall correlation scores on the test set, with\nthe highest correlation indicating superior perfor-\nmance.\n4 Solution\nIrrespective of the type of summarization, whether\nit pertains to single or multi-document summariza-\ntion or falls within the categories of abstractive or\nextractive summarization, certain fundamental cri-\nteria must be met by any generated summary. As\nhighlighted by ter Hoeve et al. (2020), five of these\ncriteria include: (1) coherence (does information\nflow logically from one sentence to the next?), (2)\ncompleteness (does the summary capture the most\nimportant information from the text?), (3) concise-\nness (is the summary brief and to the point?), (4)\nconsistency (does the information in the summary\nalign with that in the original text), and (5) read-\nability (is the summary written in a clear and un-\nderstandable manner?). Additionally, adhering to\nthe conventions of correct language syntax stands\nas an imperative prerequisite, representing a sixth\ncriterion complementing the other aforementioned\nfactors for any text generated for various purposes.\nIn our approach to the Eval4NLP shared task,\nwe devised straightforward prompts encompassing\nthe six latent dimensions mentioned above. These\n1https://www.codabench.org/competitions/1359/#/pages-\ntab\n196\nNumber of samples Average length of the source text Average length of the summary\ntrain 320 361.56 62.08\nvalidation 1280 358.77 63.21\ntest 825 199.57 38.55\nTable 1: Statistics of data used for the experiments.\nModel Name\nM1 Guanaco-65B-GPTQ\nM2 Platypus2-70B-Instruct-GPTQ\nM3 Nous-Hermes-13b\nM4 OpenOrca-Platypus2-13B\nM5 WizardLM-13B-V1.1-GPTQ\nM6 orca_mini_v3_7b\nTable 2: List of LLMs provided by task organizers\nprompts were then input into the LLMs provided\nby the organizers, as detailed in Table 2. In Table 3,\nwe present an overview of the prompts tailored to\neach of the evaluation factors. Our aim was to keep\nthe prompts as simple as possible, instructing the\nLLMs to produce a score ranging from 0 to 100 for\neach pair of (source text, generated summary). Fur-\nthermore, we combined the prompt definitions for\nall these factors to create a single comprehensive\nprompt, denoted as \"All.\"\nSubsequently, we proceeded to assess the perfor-\nmance of the six varying-sized LLMs by employing\nall the prompts on both the training and validation\ndatasets (see Section 5 for results). Following this\nevaluation, and guided by the outcomes obtained\nfrom the training and validation data, we selected\nthe most promising prompt for application to the\ntest dataset. Subsequently, we submitted the results\nfor evaluation to CodaBench (Xu et al., 2022) to\nobtain the final scoring.\n5 Experiments\nIn line with the prompt design outlined in Section 4,\nwe leveraged the computational resources offered\nby the Canada Digital Alliance to apply the desig-\nnated models with diverse prompts across both the\ntraining and validation datasets.\nTable 4 presents the performance results in terms\nof Kendall τ on training and validation data. It\nis important to emphasize that the performance\nmetrics for the training data were calculated using\nthe available reference scores. However, for the\nvalidation data (which did not include reference\nscores), the performance metrics were computed\nby submitting the scores through the CodaLab page\nof the SharedTask.2.\nThe organizers categorized models with param-\neters fewer than 25b as \"small\" and the rest as\n\"large\" models. We conducted experiments across\nall these models, and the performance variations,\nas indicated in Table 4, underscore how the model’s\neffectiveness depends on the nature of the prompts\nthey receive. Notably, it becomes evident that, in\ngeneral, models M3 and M4 (both small models)\nconsistently outperform the others across various\nprompt types. It is pertinent to observe that leverag-\ning a prompt in conjunction with a specific model\nmight yields superior results compared to other\nprompt-model combinations.\nWhen evaluated on the training data, the best per-\nformance was achieved by the following prompts\n(in ranked order): P7, P2, P1, P5, P6, P4, and\nP3. In contrast, for the validation data, a slightly\ndifferent order emerged, with P5, P2, P6, P7, P4,\nP1, and P3 being more effective. This variation is\nreasonable given that the source texts and gener-\nated summaries for the two datasets originate from\ndifferent sources.\nSubsequently, we proceeded to apply certain\nmodel-prompt combinations that had demonstrated\npromising results during the training and validation\nphases to the released test data. The performance\nof these selected model-prompt pairs, as evaluated\nby the organizers on the test data, is presented in\nTable 5.\nUpon comparing the similarity between the re-\nsults from the validation and test sets, it becomes\nevident that the test set exhibits greater similar-\nity to the validation data rather than the training\ndata. These results confirm that the utilization of\nlarge-scale language models (i.e. the LLMs with\nan extensive parameter count) without fine-tuning\ndoes not consistently yield high performance in the\ncontext of evaluation score generation tasks. In\naddition, the best results were achieved using the\nprompt for syntax, emphasizing the significance\n2https://codalab.lisn.upsaclay.fr/competitions/15072#participate-\nsubmit_results\n197\nName Prompt Definition\nP1 ALL The summary of a source text should be coherent and easy to understand’,\nwith a clear beginning, middle, and end.\\n Summary completeness is a mea-\nsure of how well a summary captures the most important information from\nthe source text. \\n A summary with high completeness will include all the\nkey points and main ideas from the source text, while a summary with low\ncompleteness may omit or overlook important information.\\nA summary is\nconcise if it is brief and to the point, avoiding unnecessary details and using\nclear language to convey the main idea of the source text.\\n A summary is\nreadable if it is written in a clear and understandable manner. It should use\nsimple language, concise sentences, and organized structure to effectively\nconvey the main points of the source text.\\n A summary is syntactically\ncorrect if it has proper sentence structure and arrangement of words. This\nincludes using correct word order, subject-verb agreement, and appropriate\nuse of phrases and clauses to convey the intended meaning accurately. \\n\nSummary and the source text are consistent if summary accurately reflects\nthe main ideas and key information of the source text without introducing\nnew or conflicting information.\\n The summary should align with the overall\nmessage, tone, and context of the original document to maintain coherence\nand reliability.\\nGive a consistency score between 0 and 100 to the summary\ncreated from the source text.\\n Zero means that ’summary and source text\nare not consistent, summary is not complete, coherent, readable, concise,\nand syntactically correct’ at all and 100 means summary is ’fully consistent,\ncoherent, readable, concise, complete and syntactic.’\nP2 Coherence The summary of a source text should be coherent and easy to understand’,\nwith a clear beginning, middle, and end.\\n Give a coherence score for the\ngiven summary of the source text below on a continuous scale from 0 to 100,\n\\n where a score of zero means ’no coherent’ and score of one hundred means\n’fully coherent’.\nP3 Completeness Summary completeness is a measure of how well a summary captures the\nmost important information from the source text. \\nA summary with high\ncompleteness will include all the key points and main ideas from the source\ntext, while a summary with low completeness may omit or overlook important\ninformation.\\nGive a completeness score between 0 and 100 to the summary\ncreated from the source text. \\nZero means a ’very incomplete’ and 100\nmeans ’a complete summary.’\nP4 Conciseness A summary is concise if it is brief and to the point, avoiding unnecessary de-\ntails and using clear language to convey the main idea of the source text.\\nGive\na conciseness score between 0 and 100 to the summary created from the source\ntext. Zero means a ’inoncise’ and 100 means a ’fully concise summary.’\nP5 Consistency Summary and the source text are consistent if summary accurately reflects\nthe main ideas and key information of the source text without introducing\nnew or conflicting information.\\nThe summary should align with the overall\nmessage, tone, and context of the original document to maintain coherence\nand reliability.\\n Give a consistency score between 0 and 100 to the summary\ncreated from the source text.\\n Zero means that ’summary and source text are\nnot consistent’ at all and 100 means they are ’fully consistent.’\nP6 Readability A summary is readable if it is written in a clear and understandable manner. It\nshould use simple language, concise sentences, and organized structure to\neffectively convey the main points of the source text.\"\\n Give a readability\nscore between 0 and 100 to the summary created from the source text.\\n Zero\nmeans the ’summary is not readable’ and 100 means summary is ’fully\nreadable.’\nP7 Syntax A summary is syntactically correct if it has proper sentence structure and\narrangement of words. This includes using correct word order, subject-verb\nagreement, and appropriate use of phrases and clauses to convey the intended\nmeaning accurately. \\n Give a syntax score between 0 and 100 to the sum-\nmary created from the source text.\\n Zero means a ’the syntax is completely\nunacceptable’ and 100 means the syntax of summary is ’fully correct.’\nTable 3: Prompts’ Definition\n198\nTrain Validation\nP1 P2 P3 P4 P5 P6 P7 P1 P2 P3 P4 P5 P6 P7\nLarge Models M1 0.36 0.44 0.25 0.41 0.45 0.42 0.40 0.35 0.45 0.26 0.42 0.42 0.44 0.42\nM2 0.24 0.25 0.21 0.22 0.24 0.11 0.22 0.27 0.22 0.21 0.24 0.26 0.12 0.22\nSmall Models\nM3 0.45 0.47 0.4 0.42 0.41 0.41 0.49 0.41 0.22 0.41 0.41 0.45 0.4 0.43\nM4 0.45 0.47 0.4 0.42 0.41 0.41 0.49 0.41 0.44 0.41 0.41 0.45 0.4 0.43\nM5 0.17 0.18 0.12 0.27 0.33 0.26 0.26 0.23 0.18 0.17 0.28 0.31 0.28 0.28\nM6 0.36 0.32 0.32 0.35 0.39 0.31 0.38 0.36 0.36 0.35 0.33 0.37 0.33 0.37\nTable 4: Performance of different models with different prompts in terms of Kendall τ . M1:Platypus2-70B-\nInstruct-GPTQ, M2:Guanaco-65B-GPTQ,M3:Nous-Hermes-13b, M4:OpenOrca-Platypus2-13B, M5:WizardLM-\n13B-V1.1-GPTQ, M6:orca_mini_v3_7b and P1:All Explained, P2: Coherence, P3: Completeness, P4:Conciseness,\nP5:Consistency, P6:Readability, P7:Syntax\nP1 P2 P3 P4 P5 P6 P7\nLarge Models M1 - 0.46 - - - 0.41 -\nM2 - - - - - - -\nSmall Models\nM3 - - - - - - -\nM4 0.46 - 0.47 0.45 - - 0.49\nM5 - - - - - - -\nM6 - - - - 0.44 - -\nTable 5: Performance results on test data. M1:Platypus2-70B-Instruct-GPTQ, M2:Guanaco-65B-GPTQ,M3:Nous-\nHermes-13b, M4:OpenOrca-Platypus2-13B, M5:WizardLM-13B-V1.1-GPTQ, M6:orca_mini_v3_7b and P1:All\nExplained, P2: Coherence, P3: Completeness, P4:Conciseness, P5:Consistency, P6:Readability, P7:Syntax\nof this latent dimension in the quality of the gen-\nerated summaries. Syntax is largely overlooked\nby reference-based metrics that focus on lexical\noverlap between the generated summary and a ref-\nerence summary; however, our results suggest that\nit plays an important role in evaluation. The second-\nhighest score was achieved using the prompt for\ncompleteness, consistent with the idea that a sum-\nmary should include the most salient points from\nthe original text.\nIt is worth highlighting that regulatory con-\nstraints imposed on participants prevented us from\nexploring the possibility of combining the scores\nfrom various prompts and models during our exper-\nimental phase. However, by employing a solitary\nmodel, we achieved a notable second-place ranking\nin the competition.\n6 Conclusion\nThe assessment of summarization system outputs\nis vital to ascertain their efficiency and usefulness.\nTraditional approaches to summarization evalua-\ntion involve comparing the generated text with\nhuman-written reference summaries. However, the\nconstraints associated with reference-based met-\nrics encourage the researchers and practitioners to\nseek reference-free metrics for the evaluation and\ncomparison of various summarization methods.\nWith the objective of formulating effective\nprompts for utilization along with LLMs, the\nEval4NLP organized a collaborative initiative. The\nprimary goal of this endeavor was to systematically\nexamine the potential utility of LLMs in the eval-\nuation of text summaries, relying exclusively on\nthe source text. In this study, we actively engaged\nin the development of prompts tailored to each of\nthe six latent dimensions (i.e. completeness, con-\nciseness, readability, coherence, consistency and\nsyntax) found to be relevant to summary evalua-\ntion. One specifically devised prompt, centered on\nthe syntactic assessment of generated summaries,\ngarnered a noteworthy score of 0.49 in terms of\nKendall τ , thereby securing the second-highest po-\nsition among performance evaluation systems.\nOur primary focus in the present work involved\nthe utilization of individual LLMs. Nevertheless,\nwe acknowledge that the collaborative use of vari-\nous models presents a promising avenue for poten-\ntial performance enhancement, which we consider\nas a valuable direction for future investigations.\n199\nAcknowledgments\nThe authors would like to express their sincere ap-\npreciation to the Natural Sciences and Engineering\nResearch Council of Canada (NSERC). We are also\ngrateful for the assistance and resources provided\nby the Canada Digital Alliance, which significantly\ncontributed to the success of this project.\nReferences\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved cor-\nrelation with human judgments. In Proceedings of\nthe acl workshop on intrinsic and extrinsic evaluation\nmeasures for machine translation and/or summariza-\ntion, pages 65–72.\nDaniel O Cajueiro, Arthur G Nery, Igor Tavares,\nMaísa K De Melo, Silvia A dos Reis, Li Weigang,\nand Victor RR Celestino. 2023. A comprehensive\nreview of automatic text summarization techniques:\nmethod, data, evaluation and coding. arXiv preprint\narXiv:2301.03403.\nYanran Chen and Steffen Eger. 2023. Menli: Robust\nevaluation metrics from natural language inference.\nTransactions of the Association for Computational\nLinguistics, 11:804–825.\nYi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and\nRuifeng Xu. 2023. Exploring the use of large lan-\nguage models for reference-free text quality evalua-\ntion: A preliminary empirical study. arXiv preprint\narXiv:2304.00723.\nLiana Ermakova, Jean Valère Cossu, and Josiane Mothe.\n2019. A survey on evaluation of summarization\nmethods. Information processing & management,\n56(5):1794–1814.\nAlexander R Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2020. Summeval: Re-evaluating summariza-\ntion evaluation. arXiv preprint arXiv:2007.12626.\nPatrick Fernandes, Daniel Deutsch, Mara Finkelstein,\nParker Riley, André FT Martins, Graham Neubig,\nAnkush Garg, Jonathan H Clark, Markus Freitag,\nand Orhan Firat. 2023. The devil is in the errors:\nLeveraging large language models for fine-grained\nmachine translation evaluation. arXiv preprint\narXiv:2308.07286.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,\nCraig Stewart, Eleftherios Avramidis, Tom Kocmi,\nGeorge Foster, Alon Lavie, and André FT Martins.\n2022. Results of wmt22 metrics shared task: Stop\nusing bleu–neural metrics are better and more ro-\nbust. In Proceedings of the Seventh Conference on\nMachine Translation (WMT), pages 46–68.\nYang Gao, Wei Zhao, and Steffen Eger. 2020. Supert:\nTowards new frontiers in unsupervised evaluation\nmetrics for multi-document summarization. arXiv\npreprint arXiv:2005.03724.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of\ngpt-3. arXiv preprint arXiv:2209.12356.\nM Indu and KV Kavitha. 2016. Review on text summa-\nrization evaluation methods. In 2016 international\nconference on research advances in integrated navi-\ngation systems (RAINS), pages 1–4. IEEE.\nSameer Jain, Vaishakh Keshava, Swarnashree Mysore\nSathyendra, Patrick Fernandes, Pengfei Liu, Gra-\nham Neubig, and Chunting Zhou. 2023. Multi-\ndimensional evaluation of text summarization with in-\ncontext learning. arXiv preprint arXiv:2306.01200.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint arXiv:2302.14520.\nChristoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao,\nRotem Dror, and Steffen Eger. 2023. The eval4nlp\n2023 shared task on prompting large language models\nas explainable metrics. In Proceedings of the 4th\nWorkshop on Evaluation and Comparison for NLP\nsystems.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nMaja Popovi´c. 2015a. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nMaja Popovi´c. 2015b. chrf: character n-gram f-score\nfor automatic mt evaluation. In Proceedings of the\ntenth workshop on statistical machine translation,\npages 392–395.\nAnanya B Sai, Akash Kumar Mohankumar, and\nMitesh M Khapra. 2022. A survey of evaluation met-\nrics used for nlg systems. ACM Computing Surveys\n(CSUR), 55(2):1–39.\nThomas Scialom, Sylvain Lamprier, Benjamin Pi-\nwowarski, and Jacopo Staiano. 2019. Answers unite!\nunsupervised metrics for reinforced summarization\nmodels. arXiv preprint arXiv:1909.01610.\nLingfeng Shen, Lemao Liu, Haiyun Jiang, and Shuming\nShi. 2022. On the evaluation metrics for paraphrase\ngeneration. arXiv preprint arXiv:2202.08479.\n200\nJosef Steinberger and Karel Ježek. 2009. Evaluation\nmeasures for text summarization. Computing and\nInformatics, 28(2):251–275.\nMaartje ter Hoeve, Julia Kiseleva, and Maarten de Rijke.\n2020. What makes a good summary? reconsidering\nthe focus of automatic summarization. arXiv preprint\narXiv:2012.07619.\nOleg Vasilyev, Vedant Dharnidharka, and John Bohan-\nnon. 2020. Fill in the blanc: Human-free quality\nestimation of document summaries. arXiv preprint\narXiv:2002.09836.\nZhen Xu, Sergio Escalera, Adrien Pavao, Magali\nRichard, Wei-Wei Tu, Quanming Yao, Huan Zhao,\nand Isabelle Guyon. 2022. Codabench: Flexible,\neasy-to-use, and reproducible meta-benchmark plat-\nform. Patterns, 3(7).\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M Meyer, and Steffen Eger. 2019. Moverscore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. arXiv preprint\narXiv:1909.02622.\nAppendix\nIn the course of this research, we utilized the sub-\nsequent modules:\n1. PyTorch: 2.0.1+cu117\n2. guidance: 0.0.64\n3. transformers: 4.34.5\n4. auto_gptq: 0.3.2\n201"
}