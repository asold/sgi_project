{
  "title": "Improving drug-drug interaction prediction via in-context learning and judging with large language models",
  "url": "https://openalex.org/W4410940275",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2081212042",
      "name": "He Qi",
      "affiliations": [
        "Harbin Institute of Technology",
        "National Medical Products Administration"
      ]
    },
    {
      "id": "https://openalex.org/A2098139441",
      "name": "Xiaoqiang Li",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Suzhou Institute of Biomedical Engineering and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109811450",
      "name": "Chengcheng Zhang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2041510793",
      "name": "Tianyi Zhao",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2081212042",
      "name": "He Qi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098139441",
      "name": "Xiaoqiang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109811450",
      "name": "Chengcheng Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2041510793",
      "name": "Tianyi Zhao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3027847423",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3140854437",
    "https://openalex.org/W4400417542",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4399554295",
    "https://openalex.org/W2137052779",
    "https://openalex.org/W4200351086",
    "https://openalex.org/W3024894285",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4401306886",
    "https://openalex.org/W3211951295",
    "https://openalex.org/W3109892317",
    "https://openalex.org/W4380989429",
    "https://openalex.org/W3036412162",
    "https://openalex.org/W4395022846",
    "https://openalex.org/W4378942305",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2024985940",
    "https://openalex.org/W4387561528",
    "https://openalex.org/W4396597709",
    "https://openalex.org/W4386614347",
    "https://openalex.org/W4405903187",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4228996904",
    "https://openalex.org/W4399912177",
    "https://openalex.org/W4385572408",
    "https://openalex.org/W2753953057",
    "https://openalex.org/W4403574812",
    "https://openalex.org/W2007069550",
    "https://openalex.org/W2145578524",
    "https://openalex.org/W1992821438",
    "https://openalex.org/W2044834685",
    "https://openalex.org/W3157889929",
    "https://openalex.org/W2802200505",
    "https://openalex.org/W2901039866",
    "https://openalex.org/W4394840983",
    "https://openalex.org/W2469049024",
    "https://openalex.org/W2156070536",
    "https://openalex.org/W2604319603",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2162251320",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4391156874",
    "https://openalex.org/W4408221130",
    "https://openalex.org/W4405016591",
    "https://openalex.org/W4403218259",
    "https://openalex.org/W2907492528",
    "https://openalex.org/W4382240023",
    "https://openalex.org/W6848511598",
    "https://openalex.org/W2798133167",
    "https://openalex.org/W3213715512",
    "https://openalex.org/W4220967131",
    "https://openalex.org/W4391788770",
    "https://openalex.org/W1018047830",
    "https://openalex.org/W4296777868",
    "https://openalex.org/W4400390698",
    "https://openalex.org/W4401730694"
  ],
  "abstract": "Introduction Large Language Models (LLMs), recognized for their advanced capabilities in natural language processing, have been successfully employed across various domains. However, their effectiveness in addressing challenges related to drug discovery has yet to be fully elucidated. Methods In this paper, we propose a novel LLM based method for drug-drug interaction (DDI) prediction, named DDI-JUDGE, achieved through the integration of judging and ICL prompts. The proposed method outperforms existing LLM approaches, demonstrating the potential of LLMs for predicting DDIs. We introduce a novel in-context learning (ICL) prompt paradigm that selects high-similarity samples as positive and negative prompts, enabling the model to effectively learn and generalize knowledge. Additionally, we present an ICL-based prompt template that structures inputs, prediction tasks, relevant factors, and examples, leveraging the pre-trained knowledge and contextual understanding of LLMs to enhance DDI prediction capabilities. To further refine predictions, we employ GPT-4 as a discriminator to assess the relevance of predictions generated by multiple LLMs. Results DDI-JUDGE achieves the best performance among all models in both zero-shot and few-shot settings, with an AUC of 0.642/0.788 and AUPR of 0.629/0.801, respectively. These results demonstrate its superior predictive capability and robustness across different learning scenarios. Development These findings highlight the potential of LLMs in advancing drug discovery through more effective DDI prediction. The modular prompt structure, combined with ensemble reasoning, offers a scalable framework for knowledge-intensive biomedical applications. The code for DDI-JUDGE is available at https://github.com/zcc1203/ddi-judge .",
  "full_text": "Improving drug-drug interaction\nprediction via in-context learning\nand judging with large language\nmodels\nHe Qi1,2, Xiaoqiang Li3, Chengcheng Zhang4* and Tianyi Zhao1,5*\n1School of Medicine and Health, Harbin Institute of Technology, Harbin, China,2Center for Drug\nEvaluation and Inspection for Heilongjiang Province, Harbin, China,3Suzhou Institute of Biomedical\nEngineering and Technology, Chinese Academy of Sciences, Suzhou, China,4Faculty of Computing,\nHarbin Institute of Technology, Harbin, China,5Harbin Institute of Technology Zhengzhou Research\nInstitute, Zhengzhou, China\nIntroduction: Large Language Models (LLMs), recognized for their advanced\ncapabilities in natural language processing, have been successfully employed\nacross various domains. However, their effectiveness in addressing challenges\nrelated to drug discovery has yet to be fully elucidated.\nMethods: In this paper, we propose a novel LLM based method for drug-drug\ninteraction (DDI) prediction, named DDI-JUDGE, achieved through the integration of\njudging and ICL prompts. The proposed method outperforms existing LLM\napproaches, demonstrating the potential of LLMs for predicting DDIs. We\nintroduce a novel in-context learning (ICL) prompt paradigm that selects high-\nsimilarity samples as positive and negative prompts, enabling the model to effectively\nlearn and generalize knowledge. Additionally, we present an ICL-based prompt\ntemplate that structures inputs, prediction tasks, relevant factors, and examples,\nleveraging the pre-trained knowledge and contextual understanding of LLMs to\nenhance DDI prediction capabilities. To further reﬁne predictions, we employ GPT-4\nas a discriminator to assess the relevance ofpredictions generated by multiple LLMs.\nResults: DDI-JUDGE achieves the best performance among all models in both\nzero-shot and few-shot settings, with an AUC of 0.642/0.788 and AUPR of 0.629/\n0.801, respectively. These results demonstrate its superior predictive capability\nand robustness across different learning scenarios.\nDevelopment: These ﬁndings highlight the potential of LLMs in advancing drug\ndiscovery through more effective DDI prediction. The modular prompt structure,\ncombined with ensemble reasoning, offers a scalable framework for knowledge-\nintensive biomedical applications. The code for DDI-JUDGE is available athttps://\ngithub.com/zcc1203/ddi-judge.\nKEYWORDS\nlarge language models, drug-drug interactions, in-context learning, zero-shot, few-shot\n1 Introduction\nPolypharmacy, or the simultaneous use of multiple drugs, is common in the treatment\nof patients with various diseases (van Roon et al., 2005). However, it can lead to adverse drug\nreactions (DDIs) due to drug–drug interactions. DDIs are responsible for 30% of all\nreported adverse drug reactions, signiﬁcantly impacting patient safety, morbidity, mortality,\nOPEN ACCESS\nEDITED BY\nLihong Peng,\nHunan University of Technology, China\nREVIEWED BY\nBo-Wei Zhao,\nChinese Academy of Sciences (CAS), China\nZhiyuan Chen,\nUniversity of Nottingham Malaysia Campus,\nMalaysia\n*CORRESPONDENCE\nChengcheng Zhang,\nzcc1203_hit@163.com\nTianyi Zhao,\nzty2009@hit.edu.cn\nRECEIVED 08 March 2025\nACCEPTED 20 May 2025\nPUBLISHED 02 June 2025\nCITATION\nQi H, Li X, Zhang C and Zhao T (2025) Improving\ndrug-drug interaction prediction via in-context\nlearning and judging with large\nlanguage models.\nFront. Pharmacol. 16:1589788.\ndoi: 10.3389/fphar.2025.1589788\nCOPYRIGHT\n© 2025 Qi, Li, Zhang and Zhao. This is an open-\naccess article distributed under the terms of the\nCreative Commons Attribution License (CC BY).\nThe use, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in this\njournal is cited, in accordance with accepted\nacademic practice. No use, distribution or\nreproduction is permitted which does not\ncomply with these terms.\nFrontiers inPharmacology frontiersin.org01\nTYPE Original Research\nPUBLISHED 02 June 2025\nDOI 10.3389/fphar.2025.1589788\nand healthcare costs (Ryu et al., 2018). Given the complexity of\ndiseases and the limitations of single-drug therapies, combination\ntherapies have the potential to improve efﬁcacy, but they also\nincrease the risk of unintended interactions (Deng et al., 2020).\nTherefore, accurate DDI prediction is crucial for improving\ntreatment outcomes and minimizing adverse effects. Although\nDDI research has become a major focus, the identi ﬁcation of\nDDIs remains challenging due to limited clinical trial resources\nand the rapid growth of biomedical data.\nCurrent state-of-the-art DDI prediction methods include\ntraditional machine learning and deep learning approaches.\nAmong these, deep learning methods leverage technologies such\nas deep neural networks (DNNs) (Sze et al., 2017), convolutional\nneural networks (CNNs) (Alzubaidi et al., 2021), graph neural\nnetworks (GNNs) (Wu et al., 2020) and transformer (Vaswani,\n2017), achieving remarkable performance. However, these methods\noften perform poorly in zero-shot scenarios and exhibit limited\ncapability in learning from large-scale, multi-source data\nintegration.\nLarge language models (LLMs), exempliﬁed by architectures\nsuch as GPT-4 (Achiam et al., 2023), Claude (Ryu et al., 2018; Bai\net al., 2022), llama (Touvron et al., 2023a), and Mistral (Jiang et al.,\n2023), have demonstrated transformative capabilities in general-\ndomain tasks through their massive parameter spaces, self-\nsupervised pretraining frameworks, and attention-based neural\narchitectures. While LLMs demonstrate exceptional performance\nin general tasks, their capabilities in specialized application domains\nremain signiﬁcantly constrained.\nIn the ﬁeld of drug discovery, LLM have demonstrated\nsigniﬁcant potential in several directions, including the\nintegration of multi-source data (Wan et al., 2024), the design of\ndownstream tasks (Guo et al., 2023), and the optimization of\nprompting strategies for speciﬁc applications (Guo et al., 2024).\nThese advancements have enabled LLMs to perform tasks such as\nmolecular property prediction and molecular translation. However,\ncritical challenges persist in applying LLMs to DDI prediction: 1) the\nscarcity of high-quality, annotated DDI datasets due to expensive\nexperimental validation; 2) poor generalizability under zero-shot\nlearning conditions, particularly for rare interaction types; 3)\nineffective fusion of heterogeneous data modalities spanning\nmolecular structures, pharmacological pathways, and\nclinical context.\nTo overcome these limitations, we introduced the DDI-JUDGE\nmodel, which employs in-Context Learning (ICL) to propose a\nprompt paradigm tailored for DDI tasks and leverages a judge to\nintegrate the predictive capabilities of multiple LLMs.\nThe main contributions of this paper are as follows:\n1) We propose a DDI prediction method based on large language\nmodels enhanced by judging and in-context learning, named\nDDI-JUDGE.\n2) We propose a novel ICL prompt paradigm for DDI prediction,\nemploying cosine similarity-based exemplar retrieval for in-\ncontext learning and coupling it with an ensemble\ndiscriminator module, such as GPT-4, that strategically\naggregates predictions from heterogeneous LLMs through\nconﬁdence-weighted voting, thereby improving robustness\nagainst model bias.\n3) The effectiveness of our method has been demonstrated\nthrough comprehensive experiments in both zero-shot and\nfew-shot scenarios, outperforming other LLM methods.\nThe structure of this paper is as follows: The Related Work\nsection provides a brief review focusing on methods of DDI\nprediction. The Methods section offers a detailed description of\nthe proposed DDI-JUDGE method. The Experiments and Results\nsection presents the experimental setup and analyzes the results.\nFinally, the Conclusion section summarizes the key points and\ndiscusses potential directions for future research.\n2 Related work\n2.1 Methods of drug-drug interactions\nprediction\nDDI prediction has been an essential area of research due to its\ncritical implications for rational drug use, enhancing therapeutic\nefﬁcacy, and minimizing adverse drug reactions. Numerous\ncomputational models, including traditional machine learning and\ndeep learning approaches, have been developed for DDI prediction.\nTraditional machine learning models predict DDIs by leveraging\nfeatures such as drug similarity, protein-protein interaction\nnetworks, and drug phenotypic proﬁles. For instance, Bayesian\nmodels calculate interaction scores based on protein networks\nand drug phenotype similarity ( Huang et al., 2013 ). Label\npropagation-based models ( Zhang et al., 2015 ) integrate drug\nside effects and chemical structure data, while probabilistic\nframeworks, such as the collective soft logic model (Sridhar et al.,\n2016) rely on multi-source similarity features. Additionally,\nmanifold regularization and matrix factorization approaches, like\nDDINMF (Yu et al., 2018) and TMFUF (Shi et al., 2018), enhance\npredictions by incorporating semi-nonnegative matrix\ndecomposition and manifold structures.\nDeep learning methods have signi ﬁcantly enhanced DDI\nprediction by enabling complex feature extraction and multi-\nsource data integration. Models like DDIMDL (Deng et al., 2020)\nand CNN-DDI (Zhang et al., 2022) employ deep neural networks\n(DNNs) and CNNs, respectively, to calculate interaction\nprobabilities using drug similarity matrices. Graph-based\nmethods, such as SSI-DDI ( Nyamabo et al., 2021 ), convert\nSMILES strings into molecular graphs and utilize graph attention\nnetworks (GATs) to extract substructure representations. Tensor-\nbased approaches like STNN-DDI (Yu et al., 2022) employ tensor\nfactorization to predict interaction types. Network-based methods\nhave further reﬁned DDI prediction by incorporating multi-relation\nand heterogeneous data. For instance, META-DDIE (Deng et al.,\n2022) combines frequent substructure mining and neural encoding\nfor DDI type prediction, while DANN-DDI ( Liu et al., 2022 )\nemploys attention mechanisms to generate comprehensive drug\nembeddings from heterogeneous networks. MRCGNN ( Xiong\net al., 2023 ) utilizes multi-relation DDI event graphs with\nrelational graph convolutional networks for feature extraction.\nSimilarly, SubGE-DDI (Shi et al., 2024) integrates substructure\nrepresentations from molecular graphs with attention-based\nmechanisms to improve prediction accuracy. Furthermore, KGE-UNIT\nFrontiers inPharmacology frontiersin.org02\nQi et al. 10.3389/fphar.2025.1589788\n(Zhang et al., 2024) enhances DDI prediction performance by multi-\ntask learning. These network-driven and hybrid approaches offer\nsigniﬁcant improvements by combining molecular, structural, and\ncontextual data in highly integrated frameworks. However, the\ncurrent method has insufﬁcient learning ability for massive multi-\nsource data and cannot adapt well to the zero-shot scenario.\n2.2 Large language models for\ndrug discovery\nLLM have shown signiﬁcant potential in advancing molecular\nscience by bridging textual information and molecular data, which\nhas facilitated applications such as molecule retrieval, reaction\nprediction, and drug discovery. Recent studies, such as Text2Mol\n(Edwards et al., 2021), Molxpt (Liu Z. et al., 2023), and Mol-\nInstructions (Fang et al., 2023 ), have established connections\nbetween molecular structures and textual descriptions, enhancing\ntasks such as molecule editing, annotation, and retrosynthesis. In\ndrug development, Y-Mol (Ma et al., 2024) and DrugReAlign (Wei\net al., 2024) demonstrate the versatility of LLMs in tackling complex\ntasks. Y-Mol offers a biomedical knowledge-guided approach for\nvirtual screening, property prediction, and drug interaction\nprediction, enhancing domain-speci ﬁc reasoning. Meanwhile,\nDrugReAlign focuses on improving drug repurposing through a\nmultisource prompt framework that integrates spatial interaction\ndata and leverages LLMs for reliable drug-target analysis. In\ndomains such as protein analysis and drug design, Protst ( Xu\net al., 2023) and Drugchat (Liang et al., 2023) employ in-context\nlearning and interactive design to align with user-speciﬁc needs.\nHowever, molecular interactions prediction tasks, such as DDIs,\nstill face many challenges. Existing methods typically rely on high-\nquality ﬁne-tuning data and computationally intensiveﬁne-tuning\nalgorithms (Hu et al., 2021), but the high cost of data acquisition and\nmodel training presents signiﬁcant obstacles. Enhancing the ability\nof LLMs to predict DDIs under the constraints of limited data and\ntraining resources remains a key issue to be addressed.\n2.3 Prompt engineering for LLM\nThe framework that combines pre-training and prompts has\nbecome a widely recognized bes t practice in natural language\nprocessing, particularly for a ddressing few-shot and zero-shot\ntasks (L i uP .e ta l . ,2 0 2 3). This approach is founded on the\nprinciple that LLM possess the capability for in-context\nlearning by leveraging input contexts and instructions (Brown,\n2020). Several studies have explored the use of LLM-based\napproaches for drug design b y incorporating various\nprompting strategies. Li et al. (2024) propose a retrieval-based\nprompting approach for molecule-caption translation. Liu Y.\net al. (2024) introduce MolecularGPT, which provides curated\nmolecular instructions for over 1000 property prediction tasks.\nChaves et al. (2024)present TxT-LLM, a method that combines\nfree-text instructions with string representations of molecules\nthroughout different stages of the drug discovery process.\nHowever, these methods may not fully capture the complexity\nof DDIs, which involve various factors including molecular,\npharmacological, and cli nical considerations.\nICL (Dong et al., 2022) can improve the model’s ability to\nunderstand and adapt to different drug combinations by leveraging\ncontextual information from multiple drug-related tasks.\nAdditionally, it allows the model to ﬂexibly adjust its responses\nbased on new data or conditions, such as changes in drug\nformulations or patient-speci ﬁc factors, thereby enhancing its\npredictive capability for DDIs. However, designing more effective\nICL prompting paradigms for DDI prediction is an area that\nrequires further study.\n3 Methods\nIn this section, we will provide a detailed explanation of the\nDDI-JUDGE method. This method aims to explore how existing\nLLMs can be used for DDI prediction, with the overall framework\nillustrated in Figure 1. The method is primarily divided into three\nparts: 1) Selecting ICL samples, 2) Building prompts based on ICL,\nand 3) Generating an LLM-based discriminator to integrate multi-\nmodel results. First, DDI-JUDGE leverages drug similarity to select\noptimal prompt samples, performing positive sample selection and\nhard negative sample mining. Next, based on the selected prompt\nsamples, we construct prompt templates speciﬁcally designed for\nDDI prediction. Finally, we use GPT to generate an LLM-based\ndiscriminator, which scores the predictions of multiple LLMs and\nintegrates the results based on the scores.\n3.1 Selecting better ICL samples of DDIs\nICL is a prompting paradigm applied to LLMs, which enhances the\ncapabilities of LLM by using a small number of demonstration prompts.\nIn DDI prediction, we need to study how toﬁnd more suitable prompt\nexamples. In order to better selectprompt examples, we propose an ICL\npositive and negative samples selection method for DDI based on drug\nsimilarity calculation. Three widely used similarity measures include\nTanimoto similarity (Tanimoto, 1958), Cosine similarity, and Dice\nsimilarity. Tanimoto To more effectively assess the similarity of drug\nfeature vectors, we examine the variations in the outcomes produced by\nthese methods. In drug similarity calculations, Dice similarity emphasizes\nshared structural features, making it suitable for identifying common\nsubstructures. Cosine similarity focuses on the angular relationship of\nfeature vectors, ideal for analyzinghigh-dimensional molecular data.\nTanimoto similarity balances shared and unique molecular features,\nmaking it particularly effective for comparing molecularﬁngerprints\nin chem-informatics. Let x/equals( x\n1,x2, ... ,xn) and y/equals( y1,y2, ... ,yn)\nrepresent the binary molecularﬁngerprints of two drugs got from Rdkit\n(RDKit, 2013), where each element indicates the presence or absence of a\nspeciﬁc substructure. The Tanimoto similarity is deﬁned as shown in\nEquations 1:\nSimT x, y() /equals x · y\nx∥∥ 2 × y\n/vextendouble/vextendouble\n/vextendouble/vextendouble\n/vextendouble/vextendouble\n/vextendouble/vextendouble\n2 /equals ∑xiyi/radicaltpext/radicaltpext/radicaltpext/radicaltpext\n∑x2\ni\n√\n+\n/radicaltpext/radicaltpext/radicaltpext/radicaltpext\n∑y2\ni\n√\n− ∑xiyi\n(1)\nwhere x · y denotes the dot product (inner product) of x and y,\ncalculated as∑ xiyi, i.e., the summation of element-wise products.\nFrontiers inPharmacology frontiersin.org03\nQi et al. 10.3389/fphar.2025.1589788\n∥x∥2 and ∥y∥2 represent the squared norms of vectors x and y,\nrespectively. Here, the numerator represents the number of shared\nsubstructures, while the denominator captures the total number of\nunique substructures across both molecules.\nIn addition to Tanimoto similarity, we also evaluate Cosine\nsimilarity, which captures the angular distance between vectors of\ndrugs. For the molecular ﬁngerprints of two drugs x and y, the\nsimilarity can be calculated as shown inEquations 2:\nSim\nC x, y() /equals x · y\nx∥∥ 2 × y\n/vextendouble/vextendouble/vextendouble/vextendouble\n/vextendouble/vextendouble/vextendouble/vextendouble\n2 /equals ∑xiyi/radicaltpext/radicaltpext/radicaltpext/radicaltpext\n∑x2\ni\n√ /radicaltpext/radicaltpext/radicaltpext/radicaltpext\n∑y2\ni\n√ (2)\nHere, this formulation captures the relative orientation between\nmolecular feature vectors. Further, Dice similarity can be calculated\nas shown inEquations 3:\nSim\nD x, y() /equals 2∑xiyi\n∑x2\ni + ∑y2\ni\n(3)\nwhere the dice similarity ranges from 0 to 1. In addition to traditional\nﬁngerprint-based similarity measures, we further explore two additional\ncategories of similarity metrics to enhance the selection of ICL examples:\ngraph-based similarity and embedding-based similarity. To explore\nstructural similarity at the graph level, we utilize the Weisfeiler-\nLehman graph kernel. LetG\n1 and G2 denote two molecular graphs,\nand let∅(G) be the mapping of a graph to a high-dimensional feature\nspace based on its structural patterns. The similarity between two\nembeddings is computed using their dot product, as deﬁned in\nEquations 4:\nSim\nG G1,G 2() /equals 〈∅ G1() , ∅ G2() 〉 (4)\nwhere 〈*〉 represents the dot product. This approach captures\ntopological information beyond atom-level ﬁngerprints, enabling\ngraph-level matching when selecting prompts based on\nmolecular structure.\nFor embedding-based similarity, we leverage pretrained deep\nlearning models to extract SMILES-based embeddings. Given a pair\nof drugs x and y, their corresponding embedding vectors are denoted\nas e\nx and ey. The cosine similarity between the embedding vectors is\nused to compute their similarity, as shown inEquations 5:\nSimE x, y() /equals ex · ey\nex∥∥ 2 × ey\n/vextendouble/vextendouble\n/vextendouble/vextendouble\n/vextendouble/vextendouble\n/vextendouble/vextendouble\n2 (5)\nThis embedding-based metric captures both structural and\nfunctional properties encoded d uring pretraining, providing a\ncomplementary perspective to symbolic similarity. In our\nimplementation, the embeddings are generated from SMILES\nsequences using the pretrained MolBERT (Fabian et al., 2020)m o d e l .\nFinally, We utilize the Tanimoto similarity based on 2048-bit Morgan\nﬁngerprints (Morgan, 1965) with a radius of two to calculate molecular\nscaffold similarity. The similarity score for each candidate drug pair is\ncalculated as the product of the similarity scores of the two drugs. Among\nthe known positive DDI samples, we identify the top-k most similar\nmolecular SMILES pairs to construct positive sample prompts. Similarly,\nfor negative sample prompts, we select the top-k most similar SMILES\nDDI pairs.\n3.2 Building prompts based on ICL\nIn recent advances in language models, ICL has emerged as a\nmethod to enable models to learn tasks without explicitﬁne-tuning.\nICL achieves this by providing examples within the input, allowing\nthe model to understand the task through context and generate\naccurate outputs. Based on ﬁltered positive and negative sample\nFIGURE 1\nThe workﬂow of DDI-JUDGE.\nFrontiers inPharmacology frontiersin.org04\nQi et al. 10.3389/fphar.2025.1589788\nexamples, we constructed prompts for DDI prediction, which are\ncategorized into zero-shot and few-shot scenarios.\nIn the zero-shot scenario, the model makes predictions based purely\non its pre-trained knowledge, without relying on speciﬁce x a m p l e s .T h i s\napproach is suited for predicting interactions between novel or previously\nunseen drug combinations as shown inFigure 2. In contrast, the few-shot\nscenario provides a small set of examples to help guide the model’s\npredictions, particularly when limited data or related examples are\navailable as shown in Figure 3. The prompt follows a structured\nformat, consisting of several key components: input requirements,\nprediction task, consideration factors, and examples. The input\nrequirements specify the drug n ames and their corresponding\nSMILES structures. The prediction task involves predicting whether\nan interaction exists between the two drugs, with the outcome being\n“yes” or “no.” Consideration factors include an analysis of\npharmacodynamics, metabolic pathways, receptor interactions, and\nrelevant clinical data, including FDA labels and peer-reviewed\nliterature. Finally, the exampl es section provides a practical\ndemonstration of the input format and expected prediction output,\nensuring clarity in applying the model.\n3.3 Predicting DDIs based on judging\nIn this study, we use GPT-4 as a judge to evaluate DDIs prediction\ngenerated by multiple LLMs. The discriminator assesses the quality of\nthe explanations provided for each DDI prediction based on four key\ncriteria: scientiﬁc accuracy, clarity and coherence, evidence support, and\nFIGURE 2\nThe zero-shot prompt of DDIs prediction.\nFrontiers inPharmacology frontiersin.org05\nQi et al. 10.3389/fphar.2025.1589788\nrelevance. A detailed prompt is designed for both zero-shot and few-\nshot scenarios as shown inFigure 4. In the zero-shot scenario, the\nprompt clearly outlines the evaluation criteria and instructions for GPT\nto assess each prediction and explanation. In the few-shot scenario, the\nprompt includes several examples ofhigh-quality evaluations to help\nthe model understand how to assign scores. Each explanation is scored\non a scale from one toﬁve for each criterion, and an overall score is\nassigned based on the evaluation. After scoring the results from all\nmodels, the predictions are combined using a weighted fusion\napproach, where each model ’s score is multiplied by a\npredetermined weight reﬂecting its reliability or performance, and\nthe weighted scores are summed to generate theﬁnal DDI prediction.\nAfter scoring the results from all models, the predictions are\ncombined using a weighted fusion approach, where the weightw\ni\nFIGURE 3\nThe few-shot prompt of DDIs prediction.\nFrontiers inPharmacology frontiersin.org06\nQi et al. 10.3389/fphar.2025.1589788\nfor each model i is determined by the score given by the\ndiscriminator to that model ’s output Smodel is calculated as\nshown in Equations 6:\nSfinal /equals ∑\nN\ni/equals 1\nwiSmodel (6)\nCompared to other ensemble learning techniques such as\nstacking or boosting, we adopt a weighted fusion strategy to\nmaintain a streamlined, inference-oriented work ﬂow. This\napproach eliminates the need to train extra models andﬁts well\nwith LLM workﬂows that rely mainly on inference rather than\nsupervised training.\n4 Results\n4.1 Datasets\nI nt h ep a p e r ,w eu s et h eL u o’s dataset (Luo et al., 2017)c o n t a i n st h e\nfollowing information, as shown inTable 1. It integrates information\nfrom multiple authoritative biomedical sources. Speciﬁcally, drug-related\ninformation was obtained from DrugBank 3.0 (Craig et al., 2010), protein\ndata from Human Protein Reference Database (Suraj et al., 2004), disease\nassociations from Comparative Toxicogenomics Database (Mattingly\net al., 2003), and side-effect information from SIDER (Michael et al.,\n2016). These heterogeneous entities— including drugs, proteins, diseases,\nand side effects— were incorporated into a uniﬁed heterogeneous\nFIGURE 4\nThe prompt of the DDIs prediction judge.\nFrontiers inPharmacology frontiersin.org07\nQi et al. 10.3389/fphar.2025.1589788\nnetwork. The dataset includes 12,015 nodes (708 drugs, 1,512 proteins,\n5,603 diseases, and 4,191 side effects) and over 1.89 million edges, with\n10,036 known drug-drug interactions.This large-scale, multi-relational\nstructure allows for comprehensive modeling of biomedical interactions.\nIn our study, we employ cross-validation to evaluate the\neffectiveness of our proposed method. Speci ﬁcally, we use a\n10-fold cross-validation appr oach. The dataset is randomly\npartitioned into ten subsets, from which nine subsets are used\nfor training and the remaining one for testing. This process is\nrepeated ten times, with each subset serving as the test set once.\nThe ﬁnal performance result is computed as the average of the\noutcomes from all ten iterations. The ﬁnal performance is\nreported as the average of the r esults across all ten folds,\nwhich helps reduce variance due to random partitioning and\nenables reliable comparison of different models. In the zero-shot\nscenario, the model is directly tested using the test set. In the few-\nshot scenario, positive and negative samples are selected from the\ntraining set for use in context learning prompts.\n4.2 Evaluation criteria\nDDI prediction is a classiﬁcation task where the outcomes are\ncategorized into four types: true positive (TP), false positive (FP),\ntrue negative (TN), and false negative (FN). Based on these\nclassiﬁcations, AUPR (Area Under the Precision-Recall Curve)\nand AUC (Area Under the ROC Curve) are widely used\nevaluation metrics. 1) AUC assesses the model’s ability to rank\ntrue DDIs higher than non-DDIs across all possible thresholds. It\nreﬂects the trade-off between the true positive rate (TPR = TP/(TP +\nFN)) and the false positive rate (FPR = FP/(FP + TN)), providing a\ncomprehensive view of classiﬁcation performance. 2) AUPR focuses\non the balance between precision (TP/(TP + FP)) and recall (TP/(TP\n+ FN)), which is particularly informative in imbalanced datasets\nsuch as DDI, where positive examples are much rarer than negatives.\nIn summary, higher AUC and AUPR values indicate that the model\nis better at identifying true interactions while minimizing false positives,\nwhich is critical in real-world pharmaceutical applications where\nmissing or wrongly predicting DDIs can have serious consequences.\n4.3 Comparison models\nGiven our focus on zero-shot and few-shot scenarios, we\nprimarily selected models based on LLMs. These models include\nGPT-4 (Achiam et al., 2023), GPT-3.5 (Brown, 2020), Davinci-003,\nand llama 2 (Touvron et al., 2023b). In addition to these well-\nestablished models, we also included more recent state-of-the-art\nmodels such as llama 3 (Dubey et al., 2024), GPT-4o, DeepSeek V3\n(Liu A. et al., 2024), and Claude 3.5 (Bae et al., 2024). All these\nmodels leverage the Transformer architecture, utilizing self-\nattention mechanisms and large-scale pretraining to achieve\nefﬁcient generation and understanding of natural language\nprocessing tasks through deep learning techniques. Speci ﬁcally,\nGPT-4 and GPT-3.5 are known for their advanced reasoning and\nlanguage understanding capabilities, while Davinci-003 provides a\nrobust foundation for few-shot learning. The inclusion of GPT-4o,\nDeepSeek V3, and Claude 3.5 ensures that our benchmark is up-to-\ndate with the latest advancements in theﬁeld.\n4.4 Comparison experiments\nFirst, to analyze the impact of drug similarity on the selection of\npositive and negative drug pairs as ICL prompts in DDI-JUDGE, we\ndiscuss the effects of Cosine similarity, Dice similarity, and Tanimoto\nsimilarity on theﬁnal results. As shown inTable 2, Tanimoto similarity\nachieves the best performance, although the differences among the three\nare minimal. Tanimoto similarity is particularly suitable for drug\nsimilarity calculation as it effectively balances shared and unique\nfeatures, accurately capturing the chemical relationships between\ndrug molecules. To provide a more comprehensive discussion on\nthe role of similarity metrics in DDI-JUDGE, we further evaluate\ntwo additional approaches: graph-based similarity and embedding-\nbased similarity. Speciﬁcally, we apply the WL graph kernel to\ncompute graph similarity, and use SMILES-based embeddings\ngenerated by MolBERT to measure embedding similarity. According\nto Table 2, the embedding-based method achieves the best overall\nperformance, with an AUC of 0.794 and AUPR of 0.815, surpassing all\nother methods. The graph-based approach also performs competitively,\nwith results close to those of Tanimoto, indicating that incorporating\nmolecular topology can be beneﬁcial. These results suggest that\nembedding-based similarity is particularly effective for capturing\ndeeper structural and semantic information.\nOverall, while Tanimoto similarity remains the most efﬁcient\nand effective choice in our method, graph-based and embedding-\nbased similarities present valuable alternatives that can be further\nexplored or integrated in future improvements.\nWe mainly conducted experiments in two scenarios: zero-shot\nand few-shot. Zero-shot refers to the model predicting DDIs without\nany prior training examples or speciﬁc task prompts, relying solely\non its pre-trained knowledge. Few-shot involves providing the\nTABLE 2 The results of three similarity measure.\nSimilarity AUC AUPR\nCosine 0.763 0.799\nDice 0.779 0.787\nTanimoto 0.788 0.801\nGraph-based 0.785 0.798\nEmbedding-based 0.794 0.815\nTABLE 1 The detail of LUO’s datasets.\nNode types Num Edge types Num\nDrug 708 Drug-drug 10,036\nProtein 1,512 Drug-protein 1,923\nDisease 5,603 Protein-protein 7,363\nSide Effect 4,192 Drug-Disease 199,214\nProtein-Disease 1,596,745\nFrontiers inPharmacology frontiersin.org08\nQi et al. 10.3389/fphar.2025.1589788\nmodel with a small number of examples, such as known interactions\nbetween drugs, to help it understand the task requirements before\nmaking predictions.\nThe experimental results under the zero-shot setting, as shown\nin Table 3, reveal that DDI-JUDGE demonstrates the best\nperformance among all models, achieving the highest AUC\n(0.642) and AUPR (0.629). GPT-4o and DeepSeek V3 also\nperform well with AUC and AUPR values of 0.585/0.557 and\n0.603/0.581, respectively. Llam a two exhibits relatively weak\nperformance, with an AUC of 0.382 and an AUPR of 0.400.\nIn the few-shot setting, as presented inTable 4, DDI-JUDGE\nonce again achieves the highest performance, with an AUC of\n0.788 and an AUPR of 0.801, showcasing its robustness when\nprovided with a few examples. Davinci-003 and llama two show\ncomparatively weaker performance, with AUC/AUPR values of\n0.525/0.553 and 0.417/0.488, respectively. The results\ndemonstrate that DDI-JUDGE effectively leverages few-shot\nexamples to maintain its superior predictive capabilities.\nComparing the two settings, it is evident that all models beneﬁt\nfrom the few-shot scenario, as providing a small number of\nexamples improves their performance. DDI-JUDGE shows\nsigniﬁcant improvement, with its AUC increasing from 0.642\n(zero-shot) to 0.768 (few-shot) and its AUPR rising from\n0.629 to 0.760. Overall, few-shot learning enhances the models’\npredictive performance, with DDI-JUDGE maintaining its leading\nposition across both settings.\n4.5 The impact of the number of ICL\nprompt samples\nNext, we discussed the impact of different numbers of ICL\nprompt samples on the predicti ve performance, as shown in\nTable 5 . As the number of ICL prompt samples increases,\nDDI-JUDGE ’s performance improves signi ﬁcantly. Starting\nfrom zero-shot, the AUC and AUPR steadily rise as more\nprompt samples are provided, with the best performance\nachieved when eight samples are used. These results suggest\nthat increasing the number of I CL prompt samples provides\nmore contextual information, allowing the model to better\nunderstand the task and make more accurate predictions.\n4.6 Case study\nNowadays, an increasing number of studies are exploring\nwhether methods can be directly translated into practical\nimprovements in real-world drug discovery and are conducting\nrelevant experiments (Zhao et al., 2022; Wang et al., 2025a; Wang\net al., 2025b; Zhao et al., 2024; Zhao et al., 2025). To demonstrate the\ncapability of our method in addressing real-world drug discovery\nissues, we conducted experiments and identiﬁed several DDIs that\nare not present in the DrugBank database.\n1) When Rivaroxaban is used concomitantly with\nDihydroxyaluminum Sodium Carbonate, the anticoagulant\neffect of Rivaroxaban may be compromised due to the\npotential for increased gastrointestinal bleeding in patients\nwith gastroduodenal ulcers (Goldhaber, 2020).\n2) When romidepsin is used concomitantly with quinidine, the\nrisk or severity of QT interval prolongation may be increased.\nRomidepsin, a histone deacetylase inhibitor, is employed in the\ntreatment of certain types of lymphoma; quinidine is an\nantiarrhythmic agent (Abu Rmilah et al., 2020).\n3) Simvastatin is a cholestero l-lowering drug that works by\ninhibiting the enzyme HMG-CoA reductase. Fluconazole\nis a triazole antifungal agent. Studies have shown that the\nconcurrent use of these two drugs may increase the risk of\nmyopathy or rhabdomyolysis ( Molden et al., 2008).\nTABLE 3 The experimental results on the zero-shot scenario.\nMethods AUC AUPR\nGPT-4o 0.585 0.603\nGPT-4 0.557 0.581\nGPT-3.5 0.521 0.535\nDavinci-003 0.443 0.416\nllama 2 0.382 0.400\nllama 3 0.573 0.551\nClaude 3.5 0.536 0.577\nDeepSeekV3 0.541 0.589\nDDI-JUDGE 0.642 0.629\nTABLE 4 The experimental results on the few-shot scenario.\nMethods AUC AUPR\nGPT-4o 0.681 0.643\nGPT-4 0.656 0.637\nGPT-3.5 0.632 0.622\nDavinci-003 0.525 0.553\nllama 2 0.417 0.488\nllama 3 0.631 0.658\nClaude 3.5 0.647 0.626\nDeepSeekV3 0.679 0.631\nDDI-JUDGE 0.788 0.801\nTABLE 5 The results on different numbers of ICL prompt samples.\nMethods AUC AUPR\nDDI-JUDGE (zero-shot) 0.642 0.629\nDDI-JUDGE (n = 1) 0.662 0.671\nDDI-JUDGE (n = 2) 0.679 0.694\nDDI-JUDGE (n = 4) 0.731 0.752\nDDI-JUDGE (n = 8) 0.788 0.801\nFrontiers inPharmacology frontiersin.org09\nQi et al. 10.3389/fphar.2025.1589788\nThe case studies demonstrate the capacity of DDI-JUDGE to\nidentify novel DDIs. Consequently, DDI-JUDGE exerts a beneﬁcial\ninﬂuence on the design and development process of new drugs.\n5 Conclusion\nIn this paper, we propose anL L M - b a s e dm e t h o df o rD D I\nprediction, which is achieved through the integration of judging and\nICL prompts. The proposed method outperforms existing LLM\napproaches, demonstrating the potential of LLMs for predicting\ncomplex relationships in drug molecules.\nFirst, we propose a novel ICL prompt paradigm for DDI prediction.\nThis approach selects high-similarity samples as positive and negative\nprompts, enabling the LLM to effectively learn and generalize\nknowledge. Additionally, we introduce an ICL-based prompt\ntemplate that organizes structured prompts, including input\nrequirements, prediction tasks, relevant factors, and examples. By\nleveraging the pre-trained knowledge and contextual understanding\nof LLMs, this template enhances DDI prediction capabilities. Finally, we\nemploy GPT-4 as a discriminator to assess the predictions of multiple\nLLMs based on scientiﬁc accuracy, clarity, evidence support, and\nrelevance. These individual results are then combined through a\nweighted fusion method to improve prediction accuracy.\nIn addition, this study emphasizes zero-shot and few-shot\nprompting scenarios, which re ﬂect the practical challenges of\nreal-world DDI prediction, where labeled data are often scarce.\nAs shown in our analysis, performance improves as the number of\nprompt examples increases, including the one-shot setting. Many-\nshot prompting, although potentially beneﬁcial, was not explored\nfurther due to input length limitations and diminishing marginal\ngains. These ﬁndings highlight that zero-shot and few-shot\nprompting offer an effective and scalable approach to DDI\nprediction in settings with limited labeled data.\nThe method currently has the following limitations: While it\nexplores the potential of applying LLMs to DDI prediction, there is\nstill a lack of domain-speciﬁc drug knowledge. For example, GPT-4,\nas a discriminator, may introduce potential biases due to its inability\nto fully understand domain-speci ﬁc knowledge and scienti ﬁc\ncontext. Future work will incorporate more drug-related data and\nperform ﬁne-tuning to further optimize the performance.\nData availability statement\nThe original contributions presented in the study are included in\nthe article/supplementary material, further inquiries can be directed\nto the corresponding authors.\nAuthor contributions\nHQ: Writing – original draft, Data curation. XL: Writing –\noriginal draft. CZ: Writing – review and editing. TZ: Writing –\nreview and editing, Project administration.\nFunding\nThe author(s) declare thatﬁnancial support was received for the\nresearch and/or publication of this article. This research was funded\nby National Key R&D Program (2022YFC3321103).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Generative AI was used in the\ncreation of this manuscript.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors and\ndo not necessarily represent those of their afﬁliated organizations, or\nthose of the publisher, the editors and the reviewers. Any product that\nmay be evaluated in this article, or claim that may be made by its\nmanufacturer, is not guaranteed or endorsed by the publisher.\nReferences\nAbu Rmilah, A. A., Lin, G., Begna, K. H., Friedman, P. A., and Herrmann, J. (2020).\nRisk of QTc prolongation among cancer patients treated with tyrosine kinase inhibitors.\nInt. J. cancer147 (11), 3160–3167. doi:10.1002/ijc.33119\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., et al. (2023).\nGpt-4 technical report. doi:10.48550/arXiv.2303.08774\nAlzubaidi, L., Zhang, J., Humaidi, A. J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., et al.\n(2021). Review of deep learning: concepts, CNN architectures, challenges, applications,\nfuture directions. J. big Data8, 53–74. doi:10.1186/s40537-021-00444-8\nBae, J., Kwon, S., and Myeong, S. (2024). Enhancing software code vulnerability\ndetection using gpt-4o and claude-3.5 sonnet: a study on prompt engineering\ntechniques. Electronics 13 (13), 2657. doi:10.3390/electronics13132657\nBai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., et al. (2022).\nConstitutional ai: harmlessness from ai feedback. doi:10.48550/arXiv.2212.08073\nBrown, T. B. (2020). Language models are few-shot learners. doi:10.48550/ARXIV.\n2005.14165\nChaves, J. M. Z., Wang, E., Tu, T., Vaishnav, E. D., Lee, B., Mahdavi, S. S., et al. (2024).\nTx-LLM: a large language model for therapeutics. doi:10.48550/arXiv.2406.06316\nCraig, K., Vivian, L., Timothy, J., Philip, L., Son, L., Alex, F., et al. (2010). DrugBank\n3.0: a comprehensive resource for’Omics’ research on drugs. Nucleic Acids Res. 39\n(Database issue), D1035–D1041. doi:10.1093/nar/gkq1126\nDeng, Y., Qiu, Y., Xu, X., Liu, S., Zhang, Z., Zhu, S., et al. (2022). META-DDIE:\npredicting drug–drug interaction events with few-shot learning.Brieﬁngs Bioinforma.\n23 (1), bbab514. doi:10.1093/bib/bbab514\nDeng, Y., Xu, X., Qiu, Y., Xia, J., Zhang, W., and Liu, S. (2020). A multimodal deep\nlearning framework for predicting drug–drug interaction events.Bioinformatics 36 (15),\n4316–4322. doi:10.1093/bioinformatics/btaa501\nDong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., et al. (2022). A survey on in-context\nlearning. doi:10.48550/arXiv.2301.00234\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., et al. (2024).\nThe llama 3 herd of models. doi:10.48550/arXiv.2407.21783\nFrontiers inPharmacology frontiersin.org10\nQi et al. 10.3389/fphar.2025.1589788\nEdwards, C., Zhai, C., and Ji, H. (2021). “Text2mol: cross-modal molecule\nretrieval with natural language queries, ” in Proceedings of the 2021 conference\non empirical methods in natural language processing ,5 9 5–607. doi:10.18653/v1/\n2021.emnlp-main.47\nFabian, B., Edlich, T., Gaspar, H., Segler, M., and Ahmed, M. (2020). Molecular\nrepresentation learning with language models and domain-relevant auxiliary tasks.\ndoi:10.48550/arXiv.2011.13230\nFang, Y., Liang, X., Zhang, N., Liu, K., Huang, R., Chen, Z., et al. (2023). Mol-\ninstructions: a large-scale biomolecular instruction dataset for large language models.\n2023. doi:10.48550/arXiv.2306.08018\nGoldhaber, S. Z. (2020). Thromboembolism prophylaxis for patients discharged from\nthe hospital: easier said than done.Am. Coll. Cardiol. Found. Wash. D.C.75, 3148–3150.\ndoi:10.1016/j.jacc.2020.05.023\nGuo, B., Wang, H., Xiao, W., Chen, H., Lee, Z., Han, S., et al. (2024). Sample design\nengineering: an empirical study of what makes good downstreamﬁne-tuning samples\nfor LLMs. doi:10.48550/arXiv.2404.13033\nGuo, T., Nan, B., Liang, Z., Guo, Z., Chawla, N., Wiest, O., et al. (2023). What can large\nlanguage models do in chemistry? a comprehensive benchmark on eight tasks.Adv.\nNeural Inf. Process. Syst.36, 59662–59688. doi:10.48550/arXiv.2305.18365\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., et al. (2021). Lora: low-\nrank adaptation of large language models.arXiv Prepr.doi:10.48550/arXiv.2106.09685\nHuang, J., Niu, C., Green, C. D., Yang, L., Mei, H., and Han, J.-D. J. (2013). Systematic\nprediction of pharmacodynamic drug-drug interactions through protein-protein-\ninteraction network. PLoS Comput. Biol. 9 (3), e1002998. doi:10.1371/journal.pcbi.\n1002998\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D., et al.\n(2023). Mistral 7B. arXiv preprint arXiv:231006825. doi:10.48550/arXiv.2310.06825\nLi, J., Liu, Y., Fan, W., Wei, X.-Y., Liu, H., Tang, J., et al. (2024). Empowering molecule\ndiscovery for molecule-caption translation with large language models: a chatgpt\nperspective. IEEE Trans. Knowl. Data Eng. 36, 6071–6083. doi:10.1109/tkde.2024.\n3393356\nLiang, Y., Zhang, R., Zhang, L., and Xie, P. (2023). Drugchat: towards enabling\nchatgpt-like capabilities on drug molecule graphs. doi:10.48550/arXiv.2309.03907\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., et al. (2024b). Deepseek-v3\ntechnical report. doi:10.48550/arXiv.2412.19437\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. (2023b). Pre-train,\nprompt, and predict: a systematic survey of prompting methods in natural language\nprocessing. ACM Comput. Surv.55 (9), 1–35. doi:10.1145/3560815\nLiu, S., Zhang, Y., Cui, Y., Qiu, Y., Deng, Y., Zhang, Z., et al. (2022). Enhancing drug-\ndrug interaction prediction using deep attention neural networks.IEEE/ACM Trans.\nComput. Biol. Bioinforma.20 (2), 976–985. doi:10.1109/TCBB.2022.3172421\nLiu, Y., Ding, S., Zhou, S., Fan, W., and Tan, Q. (2024a). MolecularGPT: open large\nlanguage model (LLM) for few-shot molecular property prediction. doi:10.48550/arXiv.\n2406.12950\nLiu, Z., Zhang, W., Xia, Y., Wu, L., Xie, S., Qin, T., et al. (2023a). Molxpt: wrapping\nmolecules with text for generative pre-training. arXiv Prepr. arXiv:230510688 ,\n1606–1616. doi:10.18653/v1/2023.acl-short.138\nLuo, Y., Zhao, X., Zhou, J., Yang, J., Zhang, Y., Kuang, W., et al. (2017). A network\nintegration approach for drug-target interaction prediction and computational drug\nrepositioning from heterogeneous information.Nat. Commun.8 (1), 573. doi:10.1038/\ns41467-017-00680-8\nMa, T., Lin, X., Li, T., Li, C., Chen, L., Zhou, P., et al. (2024). Y-mol: a multiscale\nbiomedical knowledge-guided large language model for drug development. doi:10.\n48550/arXiv.2410.11550\nMattingly, C. J., Colby, G. T., Forrest, J. N., and Boyer, J. L. (2003). The comparative\nToxicogenomics database (CTD).Environ. Health Perspect.111 (6), 793–795. doi:10.\n1289/ehp.6028\nMichael, K., Ivica, L., Juhl, J. L., and Peer, B. (2016). The SIDER database of drugs and\nside effects. Nucleic Acids Res.44 (D1), D1075–D1079. doi:10.1093/nar/gkv1075\nMolden, E., Skovlund, E., and Braathen, P. (2008). Risk management of simvastatin or\natorvastatin interactions with CYP3A4 inhibitors.Drug Saf.31, 587–596. doi:10.2165/\n00002018-200831070-00004\nMorgan, H. L. (1965). The generation of a unique machine description for chemical\nstructures-a technique developed at chemical abstracts service.J. Chem. documentation\n5 (2), 107–113. doi:10.1021/c160017a018\nNyamabo, A. K., Yu, H., and Shi, J.-Y. (2021). SSI–DDI: substructure–substructure\ninteractions for drug–drug interaction prediction.Brieﬁngs Bioinforma.22 (6), bbab133.\ndoi:10.1093/bib/bbab133\nRdkit, L. G. (2013). A software suite for cheminformatics, computational chemistry,\nand predictive modeling.Greg Landrum 8 (31.10), 5281.\nRyu, J. Y., Kim, H. U., and Lee, S. Y. (2018). Deep learning improves prediction of\ndrug–drug and drug –food interactions. Proc. Natl. Acad. Sci. 115 (18), E4304-\nE4311–E4311. doi:10.1073/pnas.1803294115\nShi, J.-Y., Huang, H., Li, J.-X., Lei, P., Zhang, Y.-N., Dong, K., et al. (2018). TMFUF: a\ntriple matrix factorization-based uniﬁed framework for predicting comprehensive drug-\ndrug interactions of new drugs.BMC Bioinforma.19, 411–437. doi:10.1186/s12859-018-\n2379-8\nShi, Y., He, M., Chen, J., Han, F., and Cai, Y. (2024). SubGE-DDI: a new prediction\nmodel for drug-drug interaction established through biomedical texts and drug-pairs\nknowledge subgraph enhancement.PLOS Comput. Biol.20 (4), e1011989. doi:10.1371/\njournal.pcbi.1011989\nSridhar, D., Fakhraei, S., and Getoor, L. (2016). A probabilistic approach for collective\nsimilarity-based drug–drug interaction prediction.Bioinformatics 32 (20), 3175–3182.\ndoi:10.1093/bioinformatics/btw342\nSuraj, P., Daniel, N. J., Kristiansen, T. Z., Ramars, A., Vineeth, S., Babylakshmi, M.,\net al. (2004). Human protein reference database as a discovery resource for proteomics.\nNucleic Acids Res.32 (Suppl. l_1), D497–D501. doi:10.1093/nar/gkh070\nSze, V., Chen, Y.-H., Yang, T.-J., and Emer, J. S. (2017). Efﬁcient processing of deep\nneural networks: a tutorial and survey.Proc. IEEE 105 (12), 2295–2329. doi:10.1109/\njproc.2017.2761740\nTanimoto, T. T. (1958). Elementary mathematical theory of classiﬁcation and prediction.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., et al.\n(2023a). LLaMA: open and efﬁcient foundation language models. doi:10.48550/arXiv.\n2302.13971\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., et al. (2023b).\nLlama 2: open foundation andﬁne-tuned chat models. doi:10.48550/arXiv.2307.09288\nvan Roon, E. N., Flikweert, S., le Comte, M.,Langendijk, P. N., Kwee-Zuiderwijk, W. J.,\nSmits, P., et al. (2005). Clinical relevance of drug-drug interactions: a structured assessment\nprocedure.Drug Saf.28, 1131–1139. doi:10.2165/00002018-200528120-00007\nVaswani, A. (2017). Attention is all you need.Adv. Neural Inf. Process. Syst. doi:10.\n48550/arXiv.1706.03762\nWan, F., Huang, X., Cai, D., Quan, X., Bi, W., and Shi, S. (2024). Knowledge fusion of\nlarge language models. doi:10.48550/arXiv.2401.10491\nWang, J., Feng, J., Kang, Y., Pan, P., Ge, J., Wang, Y., et al. (2025b). Discovery of\nantimicrobial peptides with notable antibacterial potency by an LLM-based foundation\nmodel. Sci. Adv. 11 (10), eads8932. doi:10.1126/sciadv.ads8932\nWang, J., Luo, H., Qin, R., Wang, M., Wan, X., Fang, M., et al. (2025a). 3DSMILES-\nGPT: 3D molecular pocket-based generation with token-only large language model.\nChem. Sci. 16 (2), 637–648. doi:10.1039/d4sc06864e\nWei, J., Zhuo, L., Fu, X., Zeng, X., Wang, L., Zou, Q., et al. (2024). DrugReAlign: a\nmultisource prompt framework for drug repurposing based on large language models.\nBMC Biol. 22 (1), 226. doi:10.1186/s12915-024-02028-3\nWu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Philip, S. Y. (2020). A\ncomprehensive survey on graph neural networks.IEEE Trans. neural Netw. Learn.\nSyst. 32 (1), 4–24. doi:10.1109/TNNLS.2020.2978386\nXiong, Z., Liu, S., Huang, F., Wang, Z., Liu, X., Zhang, Z., et al. (2023). Multi-relational\ncontrastive learning graph neural network for drug-drug interaction event prediction.\nProc. AAAI Conf. Artif. Intell.37 (4), 5339–5347. doi:10.1609/aaai.v37i4.25665\nXu, M., Yuan, X., Miret, S., and Tang, J. (2023).“Protst: multi-modality learning of\nprotein sequences and biomedical texts,” in International conference on machine\nlearning. Honolulu, HI: PMLR, 38749–38767.\nYu, H., Mao, K.-T., Shi, J.-Y., Huang, H., Chen, Z., Dong, K., et al. (2018). Predicting\nand understanding comprehensive drug-drug interactions via semi-nonnegative matrix\nfactorization. BMC Syst. Biol.12, 14–110. doi:10.1186/s12918-018-0532-7\nYu, H., Zhao, S., and Shi, J. (2022). Stnn-ddi: a substructure-aware tensor neural\nnetwork to predict drug–drug interactions. Brieﬁngs Bioinforma. 23 (4), bbac209.\ndoi:10.1093/bib/bbac209\nZhang, C., Lu, Y., and Zang, T. (2022). CNN-DDI: a learning-based method for\npredicting drug –drug interactions using convolution neural networks. BMC\nBioinforma. 23 (Suppl. 1), 88. doi:10.1186/s12859-022-04612-2\nZhang, C., Zang, T., and Zhao, T. (2024). KGE-UNIT: toward the uniﬁcation of\nmolecular interactions prediction based on knowledge graph and multi-task learning on\ndrug discovery. Brieﬁngs Bioinforma. 25 (2), bbae043. doi:10.1093/bib/bbae043\nZhang, P., Wang, F., Hu, J., and Sorrentino, R. (2015). Label propagation prediction of\ndrug-drug interactions based on clinical side effects.Sci. Rep.5 (1), 12339. doi:10.1038/\nsrep12339\nZhao, B.-W., Su, X.-R., Hu, P.-W., Ma, Y.-P., Zhou, X., and Hu, L. (2022). A geometric\ndeep learning framework for drug repositioning over heterogeneous information\nnetworks. Brieﬁngs Bioinforma. 23 (6), bbac384. doi:10.1093/bib/bbac384\nZhao, B.-W., Su, X.-R., Yang, Y., Li, D.-X., Li, G.-D., Hu, P.-W., et al. (2024). A\nheterogeneous information network learning model with neighborhood-level structural\nrepresentation for predicting lncRNA-miRNA interactions.Comput. Struct. Biotechnol.\nJ. 23, 2924–2933. doi:10.1016/j.csbj.2024.06.032\nZhao, B.-W., Su, X.-R., Yang, Y., Li, D.-X., Li, G.-D., Hu, P.-W., et al. (2025).\nRegulation-aware graph learning for drug repositioning over heterogeneous biological\nnetwork. Inf. Sci. 686, 121360. doi:10.1016/j.ins.2024.121360\nFrontiers inPharmacology frontiersin.org11\nQi et al. 10.3389/fphar.2025.1589788",
  "topic": "Drug",
  "concepts": [
    {
      "name": "Drug",
      "score": 0.6561518907546997
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6076383590698242
    },
    {
      "name": "Computer science",
      "score": 0.467792809009552
    },
    {
      "name": "Drug-drug interaction",
      "score": 0.4167885482311249
    },
    {
      "name": "Pharmacology",
      "score": 0.3880528509616852
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32621943950653076
    },
    {
      "name": "Medicine",
      "score": 0.3243068754673004
    },
    {
      "name": "Biology",
      "score": 0.1184762716293335
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}