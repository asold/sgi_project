{
    "title": "Predictive power of word surprisal for reading times is a linear function of language model quality",
    "url": "https://openalex.org/W2795342569",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2922543115",
            "name": "Adam Goodkind",
            "affiliations": [
                "Northwestern University"
            ]
        },
        {
            "id": "https://openalex.org/A2117820577",
            "name": "Klinton Bicknell",
            "affiliations": [
                "Northwestern University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6676968181",
        "https://openalex.org/W2139450036",
        "https://openalex.org/W2151073408",
        "https://openalex.org/W2118276816",
        "https://openalex.org/W2394729563",
        "https://openalex.org/W2054125330",
        "https://openalex.org/W2141440284",
        "https://openalex.org/W6684227215",
        "https://openalex.org/W2100719658",
        "https://openalex.org/W138474712",
        "https://openalex.org/W1995875735",
        "https://openalex.org/W2578785875",
        "https://openalex.org/W2108010971",
        "https://openalex.org/W6605210145",
        "https://openalex.org/W2009499611",
        "https://openalex.org/W126222424",
        "https://openalex.org/W2114093986",
        "https://openalex.org/W2164418233",
        "https://openalex.org/W2041404167",
        "https://openalex.org/W2582743722",
        "https://openalex.org/W2081908062",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W2951714314",
        "https://openalex.org/W2891613568",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W2259472270"
    ],
    "abstract": "Within human sentence processing, it is known that there are large effects of a word's probability in context on how long it takes to read it.This relationship has been quantified using informationtheoretic surprisal, or the amount of new information conveyed by a word.Here, we compare surprisals derived from a collection of language models derived from n-grams, neural networks, and a combination of both.We show that the models' psychological predictive power improves as a tight linear function of language model linguistic quality.We also show that the size of the effect of surprisal is estimated consistently across all types of language models.These findings point toward surprising robustness of surprisal estimates and suggest that surprisal estimated by low-quality language models are not biased.",
    "full_text": "Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018), pages 10–18,\nSalt Lake City, Utah, USA, January 7, 2018.c⃝2018 Association for Computational Linguistics\nPredictive power of word surprisal for reading times is a linear function of\nlanguage model quality\nAdam Goodkind and Klinton Bicknell\nDepartment of Linguistics\nNorthwestern University\nEvanston, IL 60208\na.goodkind@u.northwestern.edu kbicknell@northwestern.edu\nAbstract\nWithin human sentence processing, it is\nknown that there are large effects of\na word’s probability in context on how\nlong it takes to read it. This relationship\nhas been quantiﬁed using information-\ntheoretic surprisal, or the amount of new\ninformation conveyed by a word. Here,\nwe compare surprisals derived from a col-\nlection of language models derived from\nn-grams, neural networks, and a combi-\nnation of both. We show that the mod-\nels’ psychological predictive power im-\nproves as a tight linear function of lan-\nguage model linguistic quality. We also\nshow that the size of the effect of surprisal\nis estimated consistently across all types\nof language models. These ﬁndings point\ntoward surprising robustness of surprisal\nestimates and suggest that surprisal esti-\nmated by low-quality language models are\nnot biased.\n1 Introduction\nDecades of work studying human sentence pro-\ncessing have demonstrated that a word’s proba-\nbility in context is strongly related to the amount\nof time it takes to read it. This relationship has\nbeen quantiﬁed by surprisal theory (Hale, 2001;\nLevy, 2008), which states that processing difﬁ-\nculty of a word w in context c is proportional\nto its information-theoretic surprisal, deﬁned as\n−log p(w|c). As a word is more likely to occur\nin its context, and thus communicates less infor-\nmation (Shannon, 1948), it is read more quickly.\nOne difﬁculty in testing such effects of a word’s\nprobability in context is the need to construct esti-\nmates of a word’s probability in context. One way\nof estimating such probabilities is to give human\nsubjects a context, have them guess the next word,\nand estimate p(w|c) as the proportion of partici-\npants who guess wordw in contextc. This method,\ncalled a Cloze task (Taylor, 1953), may yield reli-\nable estimates for words that have relatively high\nprobabilities in their context, and it has been used\nin a number of studies of the effects of probabili-\nties in context on reading. However, it is an open\nquestion whether these human guess-derived pro-\nportions may be biased from objective probabili-\nties in some way (Smith & Levy, 2011). Problem-\natically for studying surprisal speciﬁcally, how-\never, the Cloze task cannot in principle yield reli-\nable estimates of word probabilities in context that\nare relatively low, say less than 1 in 100, as many\nword probabilities are, without requiring an ex-\ntremely large number of participants (Levy, 2008).\nAdditionally, it is not practical to use the Cloze\ntask to estimate probabilities for large datasets on\nwhich surprisal is often studied, for which there\ncan easily be tens of thousands of contexts that\nwould require estimation.\nThe alternative is to estimate the probabilities\nof words in context using computational language\nmodels, which are trained on large language cor-\npora to estimate the probabilities of words in con-\ntext. Many studies of surprisal have used such lan-\nguage models (e.g. Hale, 2001; Levy, 2008; Dem-\nberg & Keller, 2008; Mitchell et al., 2010; Mon-\nsalve et al., 2012).\nUnfortunately, however, computational lan-\nguage models are still substantially worse than\nhumans at predicting upcoming words, meaning\nthere is some mismatch between the probabilities\np(w|c) being estimated computationally and the\nimplicit probabilities in the brains of readers that\nhumans are using. This situation raises the ques-\ntion of to what extent we can trust results about the\neffects of surprisal as estimated by such language\nmodels. To try to get some information about pos-\nsible biases that might exist in our results based\non language models being worse than humans at\n10\npredicting upcoming words, poor linguistic qual-\nity, we can compare a range of computational lan-\nguage models of varying linguistic quality and see\nhow the estimated effects of surprisal change. If\nthere is a trend in results as the linguistic quality\nof the language models improves, that would pro-\nvide evidence that such a trend may be even more\npresent in language models with human-level lin-\nguistic quality.\nAdditionally, recent years have seen rapid\nprogress in computational language modeling, en-\nabled by recent advances in neural networks. As a\nresult, the linguistic quality of contemporary lan-\nguage models is far beyond what has been used\nin previous work studying surprisal. In this paper,\nwe address both these concerns by analyzing how\nthe predictive power of these surprisal estimates,\ntheir psychological quality, varies as a function of\nlanguage model linguistic quality and type.\nThere has also been substantial interest in the\nshape of the effects of surprisal on reading times,\nbecause of theories that predict it to be linear\n(Levy, 2008; Smith & Levy, 2013; Bicknell &\nLevy, 2010). A secondary goal of this work is\nto investigate whether the shape of this effect de-\npends on language model quality or type.\nIn particular, we compare surprisal estimates\nusing a range of language models of varying\nlinguistic qualities and types, from the n-gram\nmodels that have been used in most previous\nwork on surprisal to state-of-the-art LSTM and\ninterpolated-LSTM models. We assess the predic-\ntive ability and the size and shape of surprisals de-\nrived from each language model using generalized\nadditive mixed-effects models (Wood, 2017) ﬁt to\na corpus of eye movements in reading.\nThe plan for the remainder of this paper is as\nfollows. Section 2 introduces the set of language\nmodels we compare and establishes the linguis-\ntic quality of each. Then, in Section 3 we quan-\ntify the ability of surprisals derived from each lan-\nguage model to predict reading times and see the\nextent to which this changes with language model\ntype and quality, assuming that effects of surprisal\non reading times are linear. In Section 4 we do the\nsame but allow surprisal to have non-linear effects,\nand we additionally use the non-linear models to\nassess whether there is evidence that the shape of\nthe surprisal effect changes with language model\ntype or quality. Finally, Section 5 concludes.\n2 Language Models\n2.1 Corpus\nThe corpus used for language model estimation\nwas the Google One Billion Word Benchmark\n(Chelba et al., 2013), hereafter referred to as the\n“1b corpus”. The text data was obtained from news\nperiodicals (similar to the Dundee corpus used for\neye-tracking data below). The ﬁnal corpus con-\ntained approximately 0.8 billion words with a vo-\ncabulary size of about 800,000.\nAlthough the Dundee Corpus (Kennedy et al.,\n2003) tokenized entire words with punctuation,\nour models were trained using separate punctua-\ntion as well separated possessives (e.g. Bill’s →\n[Bill , ’s]). Contractions were tokenized into their\nconstituent full-form words, although contractions\nwere counted as a single word when utilizing word\ncount in e.g. perplexity calculations. These calcu-\nlations can be seen in Table 1.\n2.2 Model types\nWe compare seven language models of three\ntypes: four n-gram models, one LSTM, and two\ninterpolations.\n2.2.1 n-gram\nThe n-gram, count-based models were calculated\nusing kenlm (Heaﬁeld et al., 2013). kenlm uses\nModiﬁed Kneser-Ney Smoothing, and is similar in\nfunctionality but signiﬁcantly faster than SRILM\n(Stolcke et al., 2011). We calculated 5-grams, 4-\ngrams, trigram, bigrams and unigrams. Unigram\nresults were not included in the study, but rather\nused as a count of word frequency for controlling\nother models.\n2.2.2 LSTM\nNeural network-based language models were gen-\nerated from a Recurrent Neural Network (RNN)\nwith Long-Short Term Memory (LSTM). Each\nword was encoded as a 50-dimensional one-hot\nvector, This vector was then fed into a sequence\nmodel with an LSTM of 50 hidden units. The\nmodel did not evaluate character-level sequences,\nbut rather only word-level sequences. The prob-\nability of the next word in the sequence was\nselected from the output layer of the sequence\nmodel.\n2.2.3 Interpolation\nIn addition to the LSTM and n-gram models, two\ninterpolated models were also built from the two\n11\nmodels with the lowest perplexity on the Dundee\nCorpus used in this study (see Table 1). This\nwas similar to the interpolation method utilized\nin Jozefowicz et al. (2016). Similar to Jozefow-\nicz et al. (2016), the present study also found op-\ntimal weightings for combining an LSTM model\nwith a smoothed n-gram model. Optimal weight-\ning was operationalized as the blend weights that\nresulted in the lowest perplexity. Perplexity of\nthe interpolated LSTM+5=gram model was opti-\nmal (lowest) when an interpolated model weighted\nthe LSTM probabilities by 0.71, with the 5-gram\nmodel weighted by 0.29. In addition to this op-\ntimal model, a balanced interpolated model was\nalso constructed using equal weighting of the\nLSTM and 5-gram probabilities.\n2.3 Dundee corpus surprisals\nThe Dundee Corpus (see Section 3 for corpus de-\ntails) was tokenized at the word (rather than to-\nken) level with leading, trailing and internal punc-\ntuation included, e.g. Bill’s, couldn’tor exist!. Be-\ncause the 1b Corpus was tokenized, we were re-\nquired to break words made up of multiple to-\nkens into their constituent parts. The surprisal (log\nprobability) for each token was matched to the 1b\nCorpus surprisals. In order to realign the tokens\nwith the Dundee Corpus’s words, the log probabil-\nities of each constituent token were added together\nto form a sum total log probability of the word.\nOf the approximately 61,000 tokens in the\nDundee Corpus, 175 were OOV in the 1b Corpus.\nThese OOV words were removed from the ﬁnal\nanalysis. In adition, although the 1b Corpus used\nthe sentence-ﬁnal delimiter </s>, the Dundee\nCorpus did not. Therefore, while sentence-ﬁnal\ndelimiters were used in constructing the probabil-\nities of the respective language models, they were\nalso removed from the ﬁnal analysis.\n2.4 Perplexity\nFor each language model, the words’ surprisals\n(log probabilities) were summed and normalized\nby the word count. The exponent of the inverse\nof this sum was then calculated. A lower per-\nplexity is indicative of a more accurate language\nmodel. For example, a perplexity of 50 means\nthat the model can guess 1 of 50 different op-\ntions for the model with equal probability. There-\nfore a lower perplexity means that there are fewer\nequally likely model options. The perplexity of the\nseven language models is laid out in Table 1. The\nLanguage Model Perplexity\n(All Tokens)\nPerplexity\n(Excluding OOV)\nInterpolated-Optimal 73.39 73.41\nInterpolated-Balanced 76.39 76.36\nLSTM 113.27 113.59\n5-gram 168.98 161.43\n4-gram 172.24 164.56\n3-gram 191.13 182.65\n2-gram 290.88 278.36\nTable 1: Perplexity of language models generated\neither as a LSTM, n-grams, or an interpolation\nof both the LSTM model as well as the 5-gram\nmodel. Perplexities were calculated for the entire\nDundee corpus (60, 916 tokens) as well as for only\nthe tokens in the 1b corpus (60, 741 tokens).\noptimal interpolated model achieved the lowest\nperplexity, while the bigram model had the worst\n(highest) perplexity.\nIt should be noted that the perplexities of both\nthe optimal interpolated model (73) and the LSTM\nmodel (113) are worse than the respective models\nreported in Jozefowicz et al. (2016) and Chelba\net al. (2013). Whereas our best 5-gram model\nachieves a perplexity of 169 on the Dundee cor-\npus, Jozefowicz et al. (2016) achieves a perplex-\nity of 67 on the lm 1b benchmark using a similar\nmodel. However, an important distinction is that\nthe perplexities in Table 1 were calculated after all\nunknown words were excluded. On the other hand,\nChelba et al. (2013) used an <UNK> token for\nwords that were OOV on the test portion of the 1b\nCorpus. This suggests a substantial mismatch be-\ntween the test benchmark corpus and the Dundee\ncorpus, even though both corpora are sourced from\nnews media. Nonetheless, both perplexity ﬁgures\ncould be considered strong, low perplexities.\n3 Linear effects of surprisal\nIn this section we investigate the ability of sur-\nprisals derived from each of these seven language\nmodels described above to predict reading times in\na large corpus of eye movements in reading.\n3.1 Methods\n3.1.1 Eye movement in reading data\nThe eye tracking data for our study came from\nEnglish portion of the Dundee Corpus (Kennedy\net al., 2003), which recorded the eye-movement\ndata from 10 English-speaking participants read-\n12\ning newspaper editorials in The Independent. For\nthis paper speciﬁcally, we predict gaze durations\nfor each word, deﬁned to be the sum of all ﬁxa-\ntions made on a word between the time the word\nis initially ﬁxed and when the eyes ﬁrst move off\nof the word. This measure is only calculated if the\nword is ﬁxated by that reader prior to any ﬁxation\non a later word (i.e., during ‘ﬁrst pass’ reading).\nIf the word was not ﬁxated during ﬁrst pass read-\ning, this is missing data. We used a total of about\n436,000 valid gaze durations in the English por-\ntion of the Dundee corpus. After performing the\nexclusions listed below, we were left with a total\nof 289,726 gaze durations and a vocabulary size of\n37,420 word types.\nIn line with previous studies of gaze durations\nin the Dundee corpus (e.g. Smith & Levy, 2013),\nwe excluded:\n•Words preceding punctuation\n•Words with non-alphabetical characters\n•Words that were presented to participants at\nthe beginning or end of a line of text\n•Words that were outside the vocabulary of the\n1b corpus (and thus the language models)\nBecause our statistical model of the gaze duration\nof each word also included effects of the surprisal\nof the preceding word, we also excluded:\n•Words following punctuation\n•Words that followed words with non-\nalphabetic characters\n•Words that followed words that were outside\nthe vocabulary of the 1b corpus (and thus the\nlanguage models)\n3.1.2 Statistical models\nSimilar to Smith & Levy (2013), we used general-\nized additive mixed-effects models (GAMMs) to\npredict reading times with themgcv (Wood, 2004)\npackage in R (R Core Team, 2013). We estimated\nseven GAMMs, one for each language model.\nEach GAMM modeled gaze duration on a word as\na function of two linear surprisal terms: one for the\nsurprisal of the current word and one for the sur-\nprisal of the previous word. Each GAMM also in-\ncluded random intercepts for each of the 10 read-\ners and a range of linear and non-linear covariates\nnot of direct interest for the present work, identical\nto those included by Smith & Levy (2013). These\ncovariates were:\n•a tensor product interaction between ortho-\ngraphic word length and log-frequency (un-\nigram log probability estimated from the 1b\ncorpus) of the current word\n•a tensor product interaction between ortho-\ngraphic word length and log-frequency of the\nprevious word\n•a spline effect of word number within the text\n•a binary variable of whether or not the previ-\nous word had received a ﬁxation\n3.1.3 Analysis\nWe compare the predictive power of different lan-\nguage models for reading times by comparing the\nlog likelihoods across GAMMs that include sur-\nprisals derived from different language models. 1\nTo enable comparison of log likelihoods across\nmodels, we change two aspects of mgcv’s default\nGAMM ﬁtting procedure: we use maximum like-\nlihood ﬁtting instead of REML and we use splines\nwith ﬁxed degrees of freedom instead of penalized\nsplines. We set the ﬁxed degrees of freedom for\neach covariate to be a bit above the estimated de-\ngrees of freedom from a GAMM estimated in the\ndefault way (which was relatively constant across\nmodels).\nTo measure the added predictive power of the\ntwo linear surprisal terms in each model, we sub-\ntract the models’ log likelihood from a model\nthat only includes the covariates, yielding a mea-\nsure we denote ∆LogLik. (Note that because\nthese models are in a subset relationship -2 times\n∆LogLik is a Chi-square distributed deviance as\nin a likelihood ratio test.)\nTo assess the extent to which this measure of\npredictive power is related to the language model’s\nlinguistic quality, we correlate this ∆LogLik met-\nric with perplexity. Additionally, since these mod-\nels with linear effects of surprisal also estimate the\ncoefﬁcient of surprisal for predicting reading times\n– both for the current word’s surprisal and the prior\nword’s – we also assess the correlation between\nthese coefﬁcients and the model’s perplexity. To\nthe extent to which there are systematic relation-\nships between these coefﬁcients and the language\nmodel’s linguistic quality, it may suggest that poor\n1Technically, these models include log10 probabilities,\nwhich must be multiplied by -1 to get a surprisal, and also\nconverted from bans to bits.\n13\nFigure 1: Improvements in log likelihood for lin-\near models, charted against decreases in perplex-\nity. Distance from the central trend line is indica-\ntive of larger departures in log likelihood as a func-\ntion of perplexity. The blue line represents a linear\nbest ﬁt, with a coefﬁcient of −1.66 and R2 = 0.94\nquality language models cannot be trusted to ac-\ncurately estimate the size of the effect of surprisal\non reading times.\n3.2 Results and discussion\n3.2.1 Log Likelihood\nAs shown in Figure 1 and Table 2, there is a mono-\ntonic effect of language model quality on predic-\ntive power. Better language models (lower per-\nplexity) yield surprisal values that better predict\nreading times, as seen by increased ∆LogLik. In-\ndeed, Figure 1 shows a strikingly strong relation-\nship between a language model’s linguistic qual-\nity (measured by perplexity) and the ability of sur-\nprisal values derived from that model to predict\nreading times (measured by ∆LogLik). These two\nvalues have an R2 of 0.94.\nHowever, there is one relatively clear depar-\nture from this tight linear relationship. Namely,\nthe large decrease in the perplexity going from the\n5-gram model to the LSTM is not reﬂected in a\nlarge jump in∆LogLik. Put another way, although\nthere is a clear systematic relationship between\nlanguage model linguistic quality and ∆LogLik,\nthere is also some evidence for effects of language\nmodel type, such that the LSTM is less useful for\npredicting reading times than would be expected\ngiven its perplexity.\nFigure 2: Changes in the current word’s coefﬁcient\nfor linear models, charted against increases in per-\nplexity. Distances from the central trend line are\nindicative of larger departures of the current word\ncoefﬁcient from the expected trend. Regardless of\nperplexity, the coefﬁcient is stable. The blue line\nrepresents a linear best ﬁt, with a coefﬁcient of\n−2.79 and R2 = 0.007.\n3.2.2 Current Word\nThe effects of two words’ surprisal was incorpo-\nrated into the GAMs: the surprisal of the current\nword and the surprisal of the previous word. De-\nspite the different models’ very different perplex-\nities, the size of the effects of surprisal were es-\ntimated very stably across language models. As\nseen in Figure 2, all models had surprisal coef-\nﬁcients around 3 (although the LSTM model is\nagain somewhat of a low outlier). There is no clear\nrelationship between the coefﬁcients for the sur-\nprisal of the current word and language model\nquality, with both the best model (optimal inter-\npolation) and the worst model (bigrams) having a\nvalue of 3.04.\n3.2.3 Previous Word\nSimilar to the results above for the current word,\nthe previous word’s surprisal also had an inconsis-\ntent effect across models. In other words, the coef-\nﬁcient for the previous word’s surprisal (see Table\n2) bore no clear relationship with relative improve-\nments in language model perplexity.\n4 Non-linear effects of surprisal\nIn addition to the previous set of analyses analyz-\ning the predictive power of linear effects of sur-\nprisal on reading times, we conducted another set\nof analyses allowing for non-linear effects of sur-\n14\nLanguage Model ∆LogLik Current Word\nCoefﬁcient\nPrevious Word\nCoefﬁcient\nInterpolated-Optimal 284 -3.04 -4.57\nInterpolated-Balanced 280 -3.12 -4.68\nLSTM 231 -2.32 -2.56\n5-gram 228 -2.69 -3.82\n4-gram 224 -2.69 -3.81\n3-gram 218 -2.97 -3.92\n2-gram 151 -3.04 -3.98\nTable 2: As the perplexity of a language model increases, its improvement over baseline log likelihood\n(∆LogLik) decreases. The coefﬁcients for both the current and previous words do not bear a consistent\nrelationship with model perplexity.\nFigure 3: Regression plot of coefﬁcients on the\nprevious word. The blue line represents a linear\nbest ﬁt, with a coefﬁcient of 0.001 and R2 = 0.03.\nprisal. These models also let us ask whether the\nshape of the estimated effect of surprisal on read-\ning times varies with language model quality.\n4.1 Methodology\nThe primary methodology was identical to that\nfrom the previous analysis, except that instead of\nincluding linear effects of current and previous\nword surprisal in the GAMMs, we included cubic\nsplines (40 d.f.) of current and previous word sur-\nprisal. For this non-linear model, since there are\nnot coefﬁcients of current and previous word sur-\nprisal, we also investigate the F statistic associated\nwith the strength of each surprisal term predictor.\nAdditionally, to analyze whether the shape of\nthe surprisal effect differs across conditions, we\nﬁt additional GAMMs that had the same struc-\nture but were estimated in mgcv’s usual way (i.e.,\nwith splines penalized and REML). These addi-\nR2 p\nLinear\nLog Likelihood 0.94 0.0003\nCurrent Word Coefﬁcient 0.01 0.86\nPrevious Word Coefﬁcient 0.03 0.73\nNon-Linear\nLog Likelihood 0.98 0.00002\nCurrent Word F 0.25 0.26\nPrevious Word F 0.99 0.000008\nTable 3: Correlation results for metrics of predic-\ntors of linear and non-linear GAMMs\ntional models were only used for visualization.\n4.2 Results and discussion\nWhen allowing for non-linear effects of surprisal,\nthe relationship between linguistic quality and pre-\ndictive power for reading times becomes even\nmore clear. The relationship between ∆LogLik\nand perplexity becomes even stronger (Figure 4),\nwith an R2 of 0.98. Further, as seen in Table 4,\nwhile the F statistic for the current word surprisal\nis inconsistent as model perplexity improves (sim-\nilar to the coefﬁcients of surprisal in the linear\nmodels), the F statistic of the previous word is\ntightly related to perplexity. As perplexity of a\nmodel improves, the F statistic of the previous\nword improves in lockstep. This suggests that at\nleast in the non-linear models, many of the im-\nprovements in predictive ability may come specif-\nically from effects of prior word surprisal.\nAs can be seen in the GAM plots in Figures 5\nand 6, there are no large differences in the shape\n15\nFigure 4: Improvements in log likelihood for non-\nlinear models, charted against decreases in per-\nplexity. The blue line is a linear best ﬁt line with a\ncoefﬁcient of -1.66, R2 = 0.98.\nFigure 5: GAM plots on current word using nor-\nmal estimation\nof surprisal as language model quality improves –\nall look roughly linear. If a trend in shape does ex-\nist, the highest quality models (interpolation) ap-\npear to have the most linear slopes. Additionally,\nthe slope for surprisal of the prior word appears to\nﬂatten out for LSTMs for high surprisals.2\n5 General Discussion\nTaking all of the results together, we have shown\nevidence here for a strong effect of language\nmodel linguistic quality on the predictive power\nof surprisals estimated from that language model\nfor reading times. This effect holds regardless of\nwhether surprisal is modeled as a linear or non-\nlinear effect. Despite this clear relationship with\nlinguistic quality in terms of predictive power, we\nalso saw remarkable consistency. Across language\n2This approach was followed rather than performing a sta-\ntistical model comparison testing for non-linearity because\nour GAMM models lacked by-word random slopes. Because\nthe model lacks these parameters, we would expect the model\nto capture variance across word tokens in the corpus by bend-\ning the curve away from linearity.\nFigure 6: GAM plots on previous word using nor-\nmal estimation\nmodels that varied by more than a factor of 4 in\nperplexity, the size of the effect of surprisal was\nestimated to be the similar and the shape of the ef-\nfect of surprisal was estimated to be roughly linear.\nThese results suggest that we can put a reasonable\namount of trust in results about surprisal estimated\nwith computational language models, despite the\nstate-of-the-art still being far from human quality.\nIn addition, the way that the language models\nwere composed seems to play a role in its ﬁt to\nthe data. The LSTM-based model does seem to be\nsomewhat of a low-performing outlier. However,\nwhen the LSTM model is used with the 5-gram\nmodel in interpolation, these yield superior results.\nTherefore, although a purely LSTM-based model\ndoes not predict reading time as well as other\nmodels, it provides a good ﬁt for the data. When\nused in conjunction with a count-based model, this\ncombination provides more accurate predictions\nof the reading time data.\nA number of studies have used the Dundee eye-\ntracking corpus in conjunction with a probabilistic\nlanguage model. Demberg & Keller (2008), using\nless sophisticated linear models, found that sur-\nprisal is an accurate measure of processing com-\nplexity as measured by eye gaze duration. Ac-\ncording to Demberg & Keller (2008), greater word\nsurprisal invokes higher “integration costs,” which\naccounts for prolonged gaze duration.\nIn a neural network language model, word de-\npendencies can span an arbitrary word distance,\ni.e. not all dependencies are contingent upon adja-\ncent words or even a neighboring word. For ex-\nample, ellipsis can span multiple clause bound-\naries to resolve an anaphoric relationship. For this\n16\nLanguage Model ∆LogLik Current Word\nF Statistic\nPrevious Word\nF Statistic\nInterpolated-Optimal 297 21.13 63.8\nInterpolated-Balanced 294 21.76 63.27\nLSTM 284 17.58 55.16\n5-gram 250 21.31 50.47\n4-gram 246 21.18 50.13\n3-gram 241 22.86 48.12\n2-gram 166 15.6 34.94\nTable 4: Log likelihood and F statistics for GAMMs with nonlinear smoothers on all covariates\nreason, surprisal that accounts for the hierarchical\nstructure of language has also been studied, to see\nif taking hierarchy into account can better predict\neye gaze duration. Frank & Bod (2011) concludes\nthat including hierarchy information does not bet-\nter account for variance compared to a sequence-\nbased model. According to their study, hierarchi-\ncal information does not noticeably affect the gen-\neration of expectations of the following word.\nFossum & Levy (2012), on the other hand, make\nvarious modiﬁcations to the models used in Frank\n& Bod (2011), adding additional lexical informa-\ntion to the unlexicalized hierarchical models. Fos-\nsum & Levy (2012) concludes that hierarchical in-\nformation, when properly lexicalized, can improve\nsequence-only lexical models. Similarly, Mitchell\net al. (2010) created a model that interpolates syn-\ntactic and distributional semantic information, and\nfound that this improved the prediction of eye\ntracking durations.\nAs this bears on the present study, the LSTM\nmodel is able to detect word relationships that\nspan arbitrary distances. While the LSTM model\nis not explicitly representing hierarchical informa-\ntion, the model does capture long distance infor-\nmation. Our results show that the LSTM model\noutperforms the purely n-gram models in terms\nof predictive capabilities. Thus, while we do not\nneed to build hierarchical information explicitly\ninto our model, the long-distance information does\nimprove both linguistic and psychological accu-\nracy. This could point to the conclusion that eye\ngaze duration is also sensitive to, if not hierar-\nchical information, then information provided at\na long distance from the current word.\nIn a similar vein to our results, Monsalve et al.\n(2012) shows that perplexity of a language model\n(linguistic accuracy) bears a strong relationship to\nthe log likelihood of a reading time model (psy-\nchological accuracy). The key differences between\nthis study and ours is that Monsalve et al. (2012)\nanalyzes self-paced reading data rather than eye-\ntracking, and that we use higher-performing state-\nof-the-art language models.\nFinally, the present study can, in many respects,\nbe viewed as a follow-up to Smith & Levy (2013).\n(Smith & Levy, 2013) measured the shape of the\nsurprisal curve, similar to our experiment in Sec-\ntion 4; however, the present study demonstrates\nthat the the effect of surprisal is still linear even\nwith much more (linguistically and psychologi-\ncally) accurate language models.\nAs many studies have noted (Monsalve et al.,\n2012; Frank et al., 2013), a corpus such as the\nDundee corpus, collected from newspapers, of-\nten requires a great deal of global, extra-sentential\ncontext. Therefore, when processing a given sen-\ntence, the reader must also take into account in-\nformation provided many sentences prior, or even\nnot provided in the document at all. This limitation\ncould impact the results reported herein.\nDespite possible limitations, the results above\nprovide consistent evidence that improving the lin-\nguistic accuracy of language models will improve\nthe models’ ability to make psychological predic-\ntions. This underscores the importance of under-\nstanding language structure in order to better un-\nderstand cognitive processes such as eye gaze du-\nration.\nAcknowledgements\nWe wish to thank Tal Linzen for providing code\nfor interfacing with Google’s lm 1b LSTM lan-\nguage model. This research was supported by NSF\nAward 1734217 (Bicknell)\n17\nReferences\nBicknell, K., & Levy, R. (2010). A rational model\nof eye movement control in reading. In J. Ha-\njivc, S. Carberry, S. Clark, & J. Nivre (Eds.), Pro-\nceedings of the 48th annual meeting of the associ-\nation for computational linguistics (acl)(pp. 1168–\n1178). Uppsala, Sweden: Association for Computa-\ntional Linguistics.\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants,\nT., Koehn, P., & Robinson, T. (2013). One\nbillion word benchmark for measuring progress\nin statistical language modeling. arXiv preprint\narXiv:1312.3005.\nDemberg, V ., & Keller, F. (2008). Data from eye-\ntracking corpora as evidence for theories of syntac-\ntic processing complexity. Cognition, 109(2), 193–\n210.\nFossum, V ., & Levy, R. (2012). Sequential vs. hierar-\nchical syntactic models of human incremental sen-\ntence processing. In Proceedings of the 3rd work-\nshop on cognitive modeling and computational lin-\nguistics (pp. 61–69).\nFrank, S. L., & Bod, R. (2011). Insensitivity of the\nhuman sentence-processing system to hierarchical\nstructure. Psychological science, 22(6), 829–834.\nFrank, S. L., Monsalve, I. F., Thompson, R. L., &\nVigliocco, G. (2013). Reading time data for eval-\nuating broad-coverage models of english sentence\nprocessing. Behavior Research Methods, 45(4),\n1182–1190.\nHale, J. (2001). A probabilistic Earley parser as a psy-\ncholinguistic model. In Proceedings of the second\nmeeting of the north american chapter of the as-\nsociation for computational linguistics on language\ntechnologies (pp. 1–8).\nHeaﬁeld, K., Pouzyrevsky, I., Clark, J. H., &\nKoehn, P. (2013, August). Scalable modi-\nﬁed Kneser-Ney language model estimation.\nIn Proceedings of the 51st annual meeting of\nthe association for computational linguistics\n(pp. 690–696). Soﬁa, Bulgaria. Retrieved\nfrom https://kheafield.com/papers/\nedinburgh/estimate paper.pdf\nJozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N.,\n& Wu, Y . (2016). Exploring the limits of language\nmodeling. arXiv preprint arXiv:1602.02410.\nKennedy, A., Hill, R., & Pynte, J. (2003). The dundee\ncorpus. In Proceedings of the 12th european confer-\nence on eye movement.\nLevy, R. (2008). Expectation-based syntactic compre-\nhension. Cognition, 106(3), 1126–1177.\nMitchell, J., Lapata, M., Demberg, V ., & Keller, F.\n(2010). Syntactic and semantic factors in process-\ning difﬁculty: An integrated measure. In Proceed-\nings of the 48th annual meeting of the association\nfor computational linguistics(pp. 196–206).\nMonsalve, I. F., Frank, S. L., & Vigliocco, G. (2012).\nLexical surprisal as a general predictor of reading\ntime. In Proceedings of the 13th conference of the\neuropean chapter of the association for computa-\ntional linguistics(pp. 398–408).\nR Core Team. (2013). R: A language and environment\nfor statistical computing [Computer software man-\nual]. Vienna, Austria. Retrieved from http://\nwww.R-project.org/\nShannon, C. E. (1948). A mathematical the-\nory of communication. Bell System Tech-\nnical Journal , 27(3), 379–423. Retrieved\nfrom http://dx.doi.org/10.1002/\nj.1538-7305.1948.tb01338.x doi:\n10.1002/j.1538-7305.1948.tb01338.x\nSmith, N., & Levy, R. (2011). Cloze but no cigar:\nThe complex relationship between cloze, corpus,\nand subjective probabilities in language process-\ning. In Proceedings of the cognitive science society\n(V ol. 33).\nSmith, N., & Levy, R. (2013). The effect of word pre-\ndictability on reading time is logarithmic. Cogni-\ntion, 128(3), 302–319.\nStolcke, A., Zheng, J., Wang, W., & Abrash, V . (2011).\nSrilm at sixteen: Update and outlook. In Proceed-\nings of ieee automatic speech recognition and un-\nderstanding workshop(V ol. 5).\nTaylor, W. L. (1953). ” Cloze procedure”: a new tool\nfor measuring readability. Journalism quarterly.\nWood, S. N. (2004). Stable and efﬁcient multiple\nsmoothing parameter estimation for generalized ad-\nditive models. Journal of the American Statistical\nAssociation, 99(467), 673-686.\nWood, S. N. (2017). Generalized additive models:\nAn introduction with R (2nd ed.). Chapman and\nHall/CRC.\n18"
}