{
  "title": "Towards Full-line Code Completion with Neural Language Models",
  "url": "https://openalex.org/W3087261808",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5026622910",
      "name": "Wenhan Wang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5045653035",
      "name": "Sijie Shen",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5100447682",
      "name": "Ge Li",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5049100391",
      "name": "Zhi Jin",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2533695286",
    "https://openalex.org/W2963794306",
    "https://openalex.org/W2557805692",
    "https://openalex.org/W2962728167",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2964315653",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2973083087",
    "https://openalex.org/W2890867094",
    "https://openalex.org/W2740130862",
    "https://openalex.org/W18991458",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2142403498",
    "https://openalex.org/W2165747537",
    "https://openalex.org/W2753108589",
    "https://openalex.org/W2963617989",
    "https://openalex.org/W3011564318",
    "https://openalex.org/W3099302725"
  ],
  "abstract": "A code completion system suggests future code elements to developers given a partially-complete code snippet. Code completion is one of the most useful features in Integrated Development Environments (IDEs). Currently, most code completion techniques predict a single token at a time. In this paper, we take a further step and discuss the probability of directly completing a whole line of code instead of a single token. We believe suggesting longer code sequences can further improve the efficiency of developers. Recently neural language models have been adopted as a preferred approach for code completion, and we believe these models can still be applied to full-line code completion with a few improvements. We conduct our experiments on two real-world python corpora and evaluate existing neural models based on source code tokens or syntactical actions. The results show that neural language models can achieve acceptable results on our tasks, with significant room for improvements.",
  "full_text": "Towards Full-line Code Completion with Neural Language Models\nWenhan Wang,1 Sijie Shen, 1 Ge Li, 1 Zhi Jin 1\n1 Key Laboratory of High Conﬁdence Software Technologies (Peking University), MoE; Software Institute, Peking University,\n100871, P. R. China\nwwhjacob@pku.edu.cn, sjshen@pku.edu.cn, lige@pku.edu.cn, zhijin@pku.edu.cn\nAbstract\nA code completion system suggests future code elements\nto developers given a partially-complete code snippet. Code\ncompletion is one of the most useful features in Integrated\nDevelopment Environments (IDEs). Currently, most code\ncompletion techniques predict a single token at a time. In\nthis paper, we take a further step and discuss the probabil-\nity of directly completing a whole line of code instead of a\nsingle token. We believe suggesting longer code sequences\ncan further improve the efﬁciency of developers. Recently\nneural language models have been adopted as a preferred ap-\nproach for code completion, and we believe these models can\nstill be applied to full-line code completion with a few im-\nprovements. We conduct our experiments on two real-world\npython corpora and evaluate existing neural models based on\nsource code tokens or syntactical actions. The results show\nthat neural language models can achieve acceptable results\non our tasks, with signiﬁcant room for improvements.\nIntroduction\nCode completion has become an essential feature of Inte-\ngrated Development Environments (IDEs). It speeds up the\nprocess of software development by suggesting the next\nprobable token based on existing code. The main goal of\nmost existing code completion systems is to suggest accu-\nrate variables, arguments, or APIs to developers. Recently,\nalong with the development of deep learning technolo-\ngies and easy-to-acquire open-source codebases, researchers\nhave started to tackle code completion by learning from\nlarge-scale code corpora.\nIn this paper, we deﬁne a new code completion task:\nfull-line code completion. Given a partially completed code\nsnippet, full-line code completion requires predicting the\nnext line of code, different from traditional code completion\nwhich only predicts the next code element. Figure 1 shows\na motivating example for our task. To complete the last line\nin Figure 1, traditional code completion needs to predict at\nleast six times separately, and each time the developer needs\nto choose the correct token. But if we generate the entire\nline simultaneously, even if the prediction is only partially\ncorrect, the developer can correct the code line with fewer\noperations.\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nCurrently, the most popular technique in the research area\nof code completion is language models, especially neural\nlanguage models. Neural language model is a powerful tool\nfor predicting the next token given a token sequence, which\nnaturally ﬁts the scenario of code completion. Recent re-\nsearches have shown that large-scale neural language mod-\nels like GPT-2 (Radford et al. 2019) are capable of gener-\nating long text, which brings the potential of code sequence\ngeneration.\nOne of the key challenges in full-line code generation\nis to guarantee the syntactical correctness of the generated\ncode. To tackle this challenge, we draw lessons from past\nresearches on semantic parsing. We adopted a widely used\nframework for syntax-based code generation, which con-\nverts the generation of a code snippet into generating its\nabstract syntax tree (AST) with a sequence of construction\nactions.\nWe conduct experiments on two public Python datasets\nthat contain Python ﬁles crawled from Github repositories.\nOne dataset is in Python2, and the other one is in Python3.\nWe evaluate the performance of the state-of-the-art approach\nfor traditional code completion, along with a group of neu-\nral language models. Our results show that on both datasets,\nTransformer language models outperform RNN-based mod-\nels, which is consistent with past researches in language\nmodeling. We also ﬁnd that syntax-based approaches do not\noutperform token-based approaches, indicating that directly\napplying techniques in syntax-based code generation into\nfull-line code completion can be ineffective.\nThe main contributions of this paper are summarized as\nfollows:\n1) We propose a novel code completion task: full-line\ncode completion and build datasets for this task.\n2) We evaluate state-of-the-art models used in traditional\ncode completion and a group of neural language models on\nour datasets.\n3) We analyze the performance of plain token sequence-\nbased language models versus syntax-based language mod-\nels, and discussed the effectiveness of incorporating syntax\ninformation in full-line code completion and some possible\nimprovements in the future.\narXiv:2009.08603v1  [cs.SE]  18 Sep 2020\n \n(a) (b) \nFigure 1: An example showing the difference between traditional code completion (a) and full-line code completion (b)\nRelated Work\nCode Completion with Language Models\nSince (Hindle et al. 2012) have found out that source code\nshares mutual statistical properties with natural language, re-\nsearchers started using statistical language models on source\ncode. Most early researches use the N-gram language mod-\nels to model source code. (Tu, Su, and Devanbu 2014)\npointed out that source code has a unique property of ”local-\nness,” which could not be captured by the traditional N-gram\nmodel. (Hellendoorn and Devanbu 2017) introduced an im-\nproved cache-based N-gram model to address the localness\nand unlimited vocabulary in source code. Their evaluation\nresults on code completion showed that their model out-\nperformed existing statistical and neural language models.\n(Raychev, Bielik, and Vechev 2016) proposed a probabilistic\nlanguage model based on decision trees and domain-speciﬁc\ngrammars. They performed experiments on predicting AST\nnodes rather than directly performing on source code.\nRecently, deep learning-based language models have been\napplied to source code for the code completion task. (Liu\net al. 2016) proposed a code completion model based on\nan LSTM network. (Bhoopchand et al. 2016) proposed an\nRNN model with a pointer mechanism aiming at copying\ntokens from the past. Similarly, (Li et al. 2018) proposed a\npointer mixture network to address the Out-of-V ocabulary\n(OoV) issue. In predicting the type of the next AST node,\ntheir model outperforms (Raychev, Bielik, and Vechev 2016)\non both Python and JavaScript datasets. (Karampatsis et al.\n2020) use a GRU language model for code completion and\nuse byte-pair encoding (BPE) (Sennrich, Haddow, and Birch\n2016) to overcome the open vocabulary problem. In the\nabove works, RNN language models are adopted to model\nthe programs. However, RNNs share a common weakness:\nthey cannot efﬁciently capture the long-term dependencies\nin sequential data (Khandelwal et al. 2018). An efﬁcient\nway of mitigating long-term dependency problems in neu-\nral network language models is to use the Transformer\n(Vaswani et al. 2017) model. (Radford et al. 2018) ﬁrst use\nTransformer to build autoregressive language models and\nachieved improvement over LSTM-based models on vari-\nous natural language classiﬁcation and generation tasks. For\ncode completion, (Liu et al. 2019) adopted Transformer-XL\n(Dai et al. 2019) on the AST node completion task, and\nachieved state-of-the-art results.\nSyntax-based code generation\nThe task of automatically generating program code with\ndeep neural networks has been discussed in the natural lan-\nguage processing community as a part of semantic pars-\ning researches. One of the key challenges of code genera-\ntion is to generate syntactically valid code snippets. (Dong\nand Lapata 2016) proposed SEQ2TREE, which uses a tree-\nstructured LSTM to directly generate ASTs, but this model\ncannot guarantee the validness of generated trees. (Yin and\nNeubig 2017) addresses this issue by converting the gener-\nation of a code snippet into applying a sequence of actions\ndeﬁnes by the grammar of the programming language. An\naction either apply a grammar production rule or emit a ter-\nminal token. (Rabinovich, Stern, and Klein 2017) ﬁrst intro-\nduced abstract syntax description language (ASDL) (Wang\net al. 1997) grammar into code generation by using a tree-\nstructured decoder. (Yin and Neubig 2018) proposed tranX,\nwhich further improved the code generation framework in\n(Yin and Neubig 2017) by replacing the context-free gram-\nmar used to generate action sequences into ASDL grammars\nand expand the framework into a wider range of languages\nincluding regular expressions, Python3 and SQL.\nTask Deﬁnition\nIn this section we will describe the task of full-line code\ncompletion in details and discuss its difference with tradi-\ntional code completion.\nGiven a sequence S = w1, w2, ..., wn, language models\nmeasures the probability of S by:\nP(S) =\nn∏\nt=1\nP(wt |w1w2...wt−1) (1)\nHere P(wt |w1w2...wt−1) can be modeled with a statisti-\ncal language model or neural network language model. This\nprobability naturally ﬁts the task of code completion, which\nis predicting the next token in a code snippet.\nIn full-line code completion, instead of predicting the next\nsingle token, the model predicts a sequence of tokens that\nform a complete statement. Given a partially complete code\ncontext c1c2...ck, we need to generate a statement composed\nof a sequence of tokens s1s2...sn. If we use a language\nmodel to perform full-line code completion, the model need\nto predict\nP(s1s2...sn |c1c2...ck) =\nn∏\nt=1\nP(st |c1c2...ck, s1s2...st−1)\n(2)\nNext, we will specify the granularity of ”full line” in our\ntask. Roughly, we can take a single line of code as a state-\nment. But this brings an issue, which is in Python we can\nuse a line continuation symbol to make a statement to cover\nseveral lines. To solve this issue, we use the Python ofﬁcial\nlibrary tokenize 1 to split programs into lines. Generally,\nthere exist two types of code lines in full-line code comple-\ntion:\n• Simple statements which implements a intact action, e.g.\nassignment, return, assertion...\n• The declaration of a compound statement. e.g. declara-\ntions for functions, loops, If statements...\nOur Approach\nIn this section, we will describe the models for full-line code\ncompletion in detail. First, we introduce a framework for\ngenerating code lines with neural language models. Then we\ndescribe the approach of generating lines of code following\nthe syntax in ASTs.\nNeural Model for Code Completion\nIn this paper we perform neural language models in GRU\nand Transformer in our experiment. Our Transformer lan-\nguage model follows the architecture of GPT and GPT-2\n(Radford et al. 2018, 2019):\nh0 = We ·C + Wp (3)\nhl = transformer block(hl−1), ∀l ∈[1, N] (4)\nP(u) =softmax(hN ·WT\ne ) (5)\nWhere C is the code context, We is the token embedding\nmatrix and Wp is the position embedding matrix. The hyper-\nparameters of our model are listed in Table 1. We train the\nmodel like traditional language models and maximize the\nlog-likelihood of:\nL =\n∑\nt\nlogP(at |a1, a2, ..., at−1) (6)\nWhere at is the token at timestep t in the input program.\nWe do not follow the training procedure in GPT-like mod-\nels (Radford et al. 2018, 2019) which cut input sequences\ninto equal length. Instead, we perform traditional batched\ntraining with padding. We assume that splitting action se-\nquences will destroy the syntactical dependencies within a\nprogram ﬁle, so we keep the input sequences intact in our\n1https://docs.python.org/3/library/tokenize.html\nexperiment. Accordingly, as the length of input sequences\ngreatly varies, instead of applying learned position embed-\ndings, we apply the sinusoidal position embedding in the\noriginal Transformer (Vaswani et al. 2017). During infer-\nence, we apply beam search to keep candidate sequences\nwith the highest probabilities.\nTo prepare the action sequence for our full-line code com-\npletion task, we need to mark the end of each line in order\nto terminate the generation after a full line is completed. To\nachieve this, we manually add an end of line token ’<eol>’\nafter each line in a source code ﬁle.\nTable 1: The hyperparameters of our SG-GPT model.\nHyperparameter Value\nDimension of self-attention layer dmodel 128\nDimension of embeddings dembed 128\nDimension of the feedforward layer dff 512\nNumber of attention heads Nhead 4\nNumber of Transformer layers Nlayers 4\nMaximum length of input 1500\nDropout keeping probability 0.9\nAmong all types of code tokens, we take an additional\nstep to handle tokens with string type. The contents of\nstrings vary drastically among different programs and often\nhave little relevance to its context, so it is nearly impossible\nfor a language model to suggest an accurate string. Besides,\nsuggesting a wrong string to programmers usually have a\nnegative effect on the user experience. So we mask out all\nthe strings in our data during experiments by replacing all\nstrings in our datasets with a uniﬁed token ’<str>’.\nSyntax-based Code Completion\nIn order to make sure the generated code line is syntacti-\ncally correct, a promising approach is converting the gen-\neration procedure of a line into generating its partial AST.\nIn this paper, we adopt the parser of TranX (Yin and Neubig\n2018) which can decompose an AST into a sequence of tree-\nconstructing actions following an abstract syntax description\nlanguage (ASDL) (Wang et al. 1997) grammar.\nAn ASDL grammar can be seen as an extension of tradi-\ntional context-free grammar (CFG). Figure 2 shows a typi-\ncal example of a production rule in the ASDL grammar of\nPython. Similar to CFG, the values on the right side of an\nASDL production are the children of the value on the left.\nAn ASDL grammar has two basic constructs: types and con-\nstructors. For example, in Figure 2 a value of type expr\ncan be constructed by a Call constructor. Some construc-\ntors have a sequence of ﬁelds that describe the type of val-\nues associated with a constructor. In this example, theCall\nconstructor denotes function call expressions, and has ﬁve\nﬁelds: func, args, keywords, starargs and kwargs. Each ﬁeld\nin the constructor also has a type that speciﬁes the type of\nvalue the ﬁeld can hold. Apart from the type information,\neach ﬁeld also has a qualiﬁer (single, optional ? and sequen-\ntial ∗), which speciﬁes the number of children the ﬁeld can\nexpr\t=\tCall(expr\tfunc,\texpr*\targs,\tkeyword*\tkeywords,\texpr?\tstarargs,\texpr?\tkwargs)\nTypeConstructor\nField name Qualiﬁer\nFigure 2: An example of a production rule from the ASDL grammar of Python2.\nhold. For example, in Figure 2 Thefunc ﬁeld can only con-\ntain one value, and ﬁeldargs can contain an arbitrary num-\nber of values.\nAn AST is generated in top-down, left-to-right order. The\naction sequence is composed of three types of actions:\n• APPLYRULE: apply a production rule from the pro-\ngramming language grammar to expand a non-terminal\nAST node.\n• GENTOKEN: generate an exact value for a terminal\nnode, i.e., variable, constant, API call, etc.\n• REDUCE: a REDUCE action marks the completion of\nthe generation of children for a ﬁeld with optional (?) or\nmultiple (∗) qualiﬁer.\nWith these three types of actions, a Python code snip-\npet can be converted into an action sequence and back into\nsource code unambiguously. Figure 3 shows an example of\nconverting a Python statement to an action sequence. We\ncan train and evaluate neural language models on action se-\nquences similar to source code token sequences.\nDuring each timestep of generating the next code line,\ntranX will inspect the already generated actions and select\na set of valid actions at the current timestep. Before apply-\ning the softmax function to predict the next action, we apply\na mask on the hidden state of the language model to set the\npositions of all invalid actions to 0. This ensures that the\ngenerated action sequence can always be converted back to\nsource code.\nA problem of converting AST to action sequence is for\nsome code lines like function declaration or for/while it-\nerators, their syntax is not complete thus cannot be parsed\ninto ASTs. In the inference stage, if the model generates the\naction sequence for these statements, the tranX parser cannot\nconvert them back to source code. When we counter these\nsituations, we manually add the action sequence of a pass\nstatement after the generated sequence. Then we can convert\nthe modiﬁed sequence back to source code and remove the\nadded pass statement to get the ﬁnal output code.\nExperiments\nDataset\nWe evaluate our approach on two public Python datasets\ncrawled from Github repositories. The ﬁrst one, Py150\n(Raychev, Bielik, and Vechev 2016) contains 150,000\nPython2 ﬁles, split in to a training set of 100,000 ﬁles and\ntest set of 50,000 ﬁles. In our experiments, we take 10% of\nthe training ﬁles as the validation set. The second dataset\nPyCodeSuggest (Bhoopchand et al. 2016) consists of 949\nprojects compatible with Python3. We create our experiment\ndata by removing python ﬁles with action sequences longer\nthan the maximum input length 1500 for both datasets. We\nlist the detailed information of tokens and actions of two\ndatasets in Table 2. We can see that program ﬁles in Py150\ncontain more lines, while the code lines in PyCodeSuggest\nare usually longer than those in Py150. For both datasets,\nconverting source code tokens to action sequences increases\nthe average length of code lines.\nTable 2: Details of the dataset.\nPy150 PyCodeSuggest\nAvg. lines of code 28.2 11.4\nAvg. tokens per statement 10.6 23.6\nAvg actions per statement 14.7 35.6\nMetrics and Baselines\nWe use the following metrics to evaluate the performance of\nour approaches:\n• Accuracy: We compare the exact matching accuracy be-\ntween the generated statement and the ground truth. We\nlist the accuracy of top-1 and top-5 candidates in our pa-\nper. We further calculate the accuracy when neglecting\nidentiﬁer names to measure the ability to generate correct\nstatement structures for our models.\n• Mean reciprocal rank (MRR): we calculate MRR from the\ntop-5 candidates of every test sample. We use it to mea-\nsure the ability to give the correct sequence a high rank,\nwhich is similar to the real scenario of code completion,\nwhere developers are given a group of suggestions.\n• BLEU (Papineni et al. 2002): BLEU measures the preci-\nsion of N-grams, so we use it to measure the similarity\nbetween the target statement and the generated statement.\n• Edit similarity: if the suggested code line is not precisely\ncorrect, developers want to make as few edits as possible\nto correct the code line. The character-level edit similarity\nof the predicted output ˆy and the target output y is com-\nputed by:\nsim = 1−lev(ˆy, y)\n|y|+ |ˆy| (7)\nExpr\nCall\nRoot\nAttribute\nkeywordName sort\nmy_list reverse Name\nFalse\nvalue \nfunc \nvalue attr \nid \nargs keywords \nkeywordName sort\nmy_list reverse Name\nFalse\nstmt -> Expr(expr value)\narg value \nid \nexpr -> Call(expr func, expr* args, keyword*\nkeywords, expr? starargs, expr? kwargs)\nexpr -> Attribute(expr value, identiﬁestarargs kwargs \nField\twith\toptional\tor \nsequential\tqualifier\t \nAST\ttraversal\tsequence \nTimestep Action\n1 ApplyRule[stmt ->Expr(expr value)]\n2 ApplyRule[expr ->Call(expr func, expr* args, keyword*\nkeywords, expr? starargs, expr? kwargs)]\n3 ApplyRule[expr ->Attribute(expr value, identiﬁer attr)]\n4 ApplyRule[expr ->Name(identiﬁer id)]\n5 GenToken[my list]\n6 GenToken[sort]\n7 Reduce\n8 ApplyRule[keyword ->keyword(identiﬁer arg,\nexpr value)]\n9 GenToken[reverse]\n10 ApplyRule[expr ->Name(identiﬁer id)]\n11 GenToken[False]\n12 Reduce\n13 Reduce\n14 Reduce\nFigure 3: The AST of Python statement my list.sort(reverse=False) (left) and its corresponding action sequence\n(right).\nHere lev() is the Levenshtein distance (edit distance) and\n|·|is the length (number of characters) of the sequence.\nWe evaluate the following models on our datasets:\n• BPE-NLM (Karampatsis et al. 2020): a GRU language\nmodel for code completion using BPE to split source\ncode tokens. This approach achieved state-of-the-art re-\nsults on code completion tasks in multiple programming\nlanguages.\n• Language models: we use Transformer and GRU lan-\nguage models on source code token sequences or ASDL\naction sequences. In order to analyze the effectiveness\nof byte-pair encoding, we also run Transformer language\nmodels on BPE subtokens.\nWe have also attempted to apply encoder-decoder models\nfor this task, i.e., using an encoder to encode the program\ncontext and a decoder to generate the target statement. How-\never, for encoder-decoder models, we have to create separate\ntraining samples for each statement in the code ﬁles, which\ntremendously increase the size of the training set, making\nthe training time unacceptable.\nThere also exists a group of researches on code comple-\ntion by predicting the next AST node in the ﬂattened se-\nquence of ASTs (Raychev, Bielik, and Vechev 2016; Li et al.\n2018; Liu et al. 2019). Our goal is to recommend a line of\ncode to developers, while a sequence of AST nodes cannot\nalways be transferred into source code. So these approaches\nare not suitable for full-line code completion, and we do not\nevaluate them in our experiments.\nExperimental Setup\nIn each test sample, the input is the ﬁrst k lines of a Python\nﬁle, and the target output is the k + 1th line. So for a Python\nﬁle of N lines, we can createN −1 test samples. We remove\ntest samples whose target is anImport statement or longer\nthan 100 tokens. We exclude Import because they are of-\nten irrelevant to the previous code context. For code state-\nments longer than 100 tokens, we ﬁnd out these statements\nare often the deﬁnition of large lists or dictionaries, which\nare incapable for neural language models to complete.\nFor token-based and syntax-based approaches, we set the\nvocabulary size to 80,000. For BPE-based approaches, we\nset the number of merge operations to 30,000. The hidden\nsize of GRU models is 512. The batch size is 8 for GRU and\n4 for Transformer models, since larger batches cannot be fed\nonce into the memory. We use the Adam optimizer (Kingma\nand Ba 2015) to train all models. We implement our models\nin PyTorch (Paszke et al. 2019) and run our experiments on\na NVIDIA Tesla V100 GPUs with 16GB memory.\nResults and Analysis\nTable 3 and Table 4 show the results of all models on the two\ndatasets. Results on all metrics are reported in percentage\n(%). We can see that in all experiment settings, Transformer\nmodels outperform GRU models on all evaluation metrics.\nA somewhat unexpected ﬁnding is that syntax-based ap-\nproaches do not outperform token sequence-based ones, and\neven perform worse on Py150. We assume this is caused by\nthe differences in sequence length between these two types\nof approaches. From Table 2, we can clearly ﬁgure out that\nthe action sequence for a Python program is often longer\nthan its source code token sequence, which brings an extra\nburden to language models. The main advantage of ASDL\naction sequences over source code tokens is that an action\nsequence can be necessarily converted to a syntactically cor-\nrect code snippet. This is important for NL-based code gen-\neration tasks like Django (Oda et al. 2015) or CoNaLa (Yin\net al. 2018) since in these tasks, the source code corpus\nis small-scaled and each code sample is short. However,\nin large-scale real-world corpora with ﬁle-level code snip-\npets, neural language models are capable of learning pro-\nTable 3: Experiment results for all models on Py150.\nModel acc@1 acc@1 w/o id acc@5 acc@5 w/o id MRR BLEU-4 Edit similarity\nBPE-NLM 6.94 11.06 11.62 20.29 8.73 11.95 48.31\nGRU+Token 8.38 10.84 13.94 20.03 10.43 16.59 51.81\nGRU+Syntax 6.93 8.86 10.47 15.78 8.73 15.14 47.76\nTransformerLM+Token 8.93 12.47 15.98 23.76 11.47 19.58 54.03\nTransformerLM+Syntax 7.73 10.98 12.68 20.25 9.97 19.27 50.90\nTransformerLM+BPE 8.55 14.12 14.72 25.82 10.82 16.11 52.22\nTable 4: Experiment results for all models on PyCodeSuggest.\nModel acc@1 acc@1 w/o id acc@5 acc@5 w/o id MRR BLEU-4 Edit similarity\nBPE-NLM 2.98 5.71 5.10 10.61 3.77 5.77 40.11\nGRU+Token 2.78 4.84 5.10 9.08 3.60 6.22 40.40\nGRU+Syntax 1.41 4.39 2.28 8.43 1.98 6.55 39.17\nTransformerLM+Token 4.32 8.35 8.65 17.05 5.81 12.61 48.59\nTransformerLM+Syntax 3.58 8.72 6.82 17.64 4.87 14.92 47.67\nTransformerLM+BPE 3.90 7.88 7.81 16.55 5.27 11.31 46.68\ngram grammar from data, so the effect of explicit syntax re-\nstrictions becomes undermined. We manually inspected the\nstatements generated by TransformerLM+token, and nearly\nall of them are syntactically correct. Also, we must notice\nthat currently, our syntax guidance is still very coarse for\ncode completion. First, we do not augment our language\nmodels with syntactical dependencies like AST parent-child\nconnections. The neural language models are still purely se-\nquential. Second, the ASDL parser in our experiments only\nguarantee that generated code lines can be parsed into ASTs,\nwhile in the real scenario of code completion, the restriction\nof outputs is much stronger, including restriction on vari-\nable usage, API calls, and arguments, etc. A promising way\nof applying these restrictions is to leverage powerful static\nanalysis tools. Static analysis can be applied to source code\ntokens, which relieves language models from learning on\nlonger ASDL action sequences.\nAnother phenomenon is that language models with BPE\noutperform token-level language models on accuracy with-\nout identiﬁers but achieved similar or lower results than\ntoken-level models on accuracy with identiﬁers. This im-\nplies that BPE can mitigate the out-of-vocabulary problem to\na certain extent, but for some identiﬁers, token-based mod-\nels can generate them with only one step, while in BPE, they\nare separated to subtokens, which increase the difﬁculty of\ngenerating them.\nWe also make an evaluation of the time efﬁciency of our\nmodels. Table 5 compare the average time consuming for in-\nferencing a whole line of code for all models. We report the\ntime cost when the beam size is set to 5. Although all mod-\nels have acceptable time efﬁciency, the inference speed of\nsyntax-based models are much slower than token-based or\nBPE subtoken-based models. This shows that masking out\ninvalid actions with an ASDL parser is much more time con-\nsuming than predicting a code element with neural language\nmodels. From our experiments, we believe that in order to\nkeep a high time efﬁciency for full-line code completion,\nTable 5: The inference time of all models on Py150.\nModel Avg. inference time (s)\nBPE-NLM 0.07\nGRU+Token 0.08\nGRU+Syntax 1.41\nTransformerLM+Token 0.07\nTransformerLM+Syntax 1.34\nTransformerLM+BPE 0.08\nour model should be based on source code tokens. We need\nto explore the possibility of adding grammatical constraints\nto source token sequences.\nConclusion and Future Work\nIn this paper, we deﬁne a new task, full-line code comple-\ntion, and studied the performance of neural language mod-\nels on this task. Apart from token-based and BPE-based ap-\nproaches, which have already been evaluated on token-level\ncode completion tasks, we additional conduct experiments\nwith ASDL syntax-based models. Our experiments show\nthat Transformer language model on token sequences cur-\nrently performs best on our datasets.\nIn the future, we plan to further improve the effectiveness\nof language models on full-line code completion by train-\ning on more data and using models with larger parameter\nsize. Meanwhile, we aim to utilize more powerful software\nanalyzing tools to further narrow down the output space of\nour model, e.g., adding restrictions on variable names and\nAPI usage. Furthermore, we would like to improve our neu-\nral model to incorporate syntax structures like parent-child\nlinks in ASTs and incorporate BPE or copy mechanism to\ntackle the out-of-vocabulary problem.\nReferences\nBhoopchand, A.; Rockt ¨aschel, T.; Barr, E.; and Riedel, S.\n2016. Learning python code suggestion with a sparse pointer\nnetwork. arXiv preprint arXiv:1611.08307 .\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J. G.; Le, Q.; and\nSalakhutdinov, R. 2019. Transformer-XL: Attentive Lan-\nguage Models beyond a Fixed-Length Context. In Proceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 2978–2988.\nDong, L.; and Lapata, M. 2016. Language to Logical Form\nwith Neural Attention. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 33–43.\nHellendoorn, V . J.; and Devanbu, P. 2017. Are deep neural\nnetworks the best choice for modeling source code? In Pro-\nceedings of the 2017 11th Joint Meeting on Foundations of\nSoftware Engineering, 763–773.\nHindle, A.; Barr, E. T.; Su, Z.; Gabel, M.; and Devanbu, P.\n2012. On the naturalness of software. In 2012 34th Inter-\nnational Conference on Software Engineering (ICSE), 837–\n847. IEEE.\nKarampatsis, R.-M.; Babii, H.; Robbes, R.; Sutton, C.;\nand Janes, A. 2020. Big Code!= Big V ocabulary: Open-\nV ocabulary Models for Source Code. arXiv preprint\narXiv:2003.07914 .\nKhandelwal, U.; He, H.; Qi, P.; and Jurafsky, D. 2018. Sharp\nNearby, Fuzzy Far Away: How Neural Language Models\nUse Context. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), 284–294.\nKingma, D. P.; and Ba, J. 2015. Adam: A Method for\nStochastic Optimization. In Bengio, Y .; and LeCun, Y ., eds.,\n3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Confer-\nence Track Proceedings.\nLi, J.; Wang, Y .; Lyu, M. R.; and King, I. 2018. Code com-\npletion with neural attention and pointer networks. In Pro-\nceedings of the 27th International Joint Conference on Arti-\nﬁcial Intelligence, 4159–25.\nLiu, C.; Wang, X.; Shin, R.; Gonzalez, J. E.; and Song, D.\n2016. Neural code completion .\nLiu, F.; Li, G.; Wei, B.; Xia, X.; Li, M.; Fu, Z.; and\nJin, Z. 2019. A Self-Attentional Neural Architecture for\nCode Completion with Multi-Task Learning. arXiv preprint\narXiv:1909.06983 .\nOda, Y .; Fudaba, H.; Neubig, G.; Hata, H.; Sakti, S.; Toda,\nT.; and Nakamura, S. 2015. Learning to generate pseudo-\ncode from source code using statistical machine translation\n(t). In 2015 30th IEEE/ACM International Conference on\nAutomated Software Engineering (ASE), 574–584. IEEE.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBLEU: a method for automatic evaluation of machine trans-\nlation. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 311–318.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. In Advances in neural information\nprocessing systems, 8026–8037.\nRabinovich, M.; Stern, M.; and Klein, D. 2017. Abstract\nSyntax Networks for Code Generation and Semantic Pars-\ning. In Proceedings of the 55th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Pa-\npers), 1139–1149.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners. OpenAI Blog 1(8): 9.\nRaychev, V .; Bielik, P.; and Vechev, M. 2016. Probabilistic\nmodel for code with decision trees. In Proceedings of the\n2016 ACM SIGPLAN International Conference on Object-\nOriented Programming, Systems, Languages, and Applica-\ntions, 731–747.\nSennrich, R.; Haddow, B.; and Birch, A. 2016. Neural Ma-\nchine Translation of Rare Words with Subword Units. In\nProceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n1715–1725.\nTu, Z.; Su, Z.; and Devanbu, P. 2014. On the localness of\nsoftware. In Proceedings of the 22nd ACM SIGSOFT Inter-\nnational Symposium on Foundations of Software Engineer-\ning, 269–280.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWang, D. C.; Appel, A. W.; Korn, J. L.; and Serra, C. S.\n1997. The Zephyr Abstract Syntax Description Language.\nIn DSL, volume 97, 17–17.\nYin, P.; Deng, B.; Chen, E.; Vasilescu, B.; and Neubig, G.\n2018. Learning to mine aligned code and natural language\npairs from stack overﬂow. In 2018 IEEE/ACM 15th Interna-\ntional Conference on Mining Software Repositories (MSR) ,\n476–486. IEEE.\nYin, P.; and Neubig, G. 2017. A Syntactic Neural Model for\nGeneral-Purpose Code Generation. In Proceedings of the\n55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 440–450.\nYin, P.; and Neubig, G. 2018. TRANX: A Transition-based\nNeural Abstract Syntax Parser for Semantic Parsing and\nCode Generation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing: Sys-\ntem Demonstrations, 7–12.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8294402360916138
    },
    {
      "name": "Security token",
      "score": 0.6910943984985352
    },
    {
      "name": "Redundant code",
      "score": 0.6756705641746521
    },
    {
      "name": "Snippet",
      "score": 0.6572825908660889
    },
    {
      "name": "Code (set theory)",
      "score": 0.6477251052856445
    },
    {
      "name": "Python (programming language)",
      "score": 0.6330658197402954
    },
    {
      "name": "Programming language",
      "score": 0.5678731799125671
    },
    {
      "name": "Source code",
      "score": 0.5056686401367188
    },
    {
      "name": "Code generation",
      "score": 0.5026130676269531
    },
    {
      "name": "Line code",
      "score": 0.44875842332839966
    },
    {
      "name": "Language model",
      "score": 0.44275155663490295
    },
    {
      "name": "Dead code",
      "score": 0.44244006276130676
    },
    {
      "name": "Source lines of code",
      "score": 0.43425580859184265
    },
    {
      "name": "Artificial intelligence",
      "score": 0.347595751285553
    },
    {
      "name": "Natural language processing",
      "score": 0.3224751651287079
    },
    {
      "name": "Software",
      "score": 0.19775325059890747
    },
    {
      "name": "Operating system",
      "score": 0.13905417919158936
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.08187440037727356
    },
    {
      "name": "Key (lock)",
      "score": 0.07400766015052795
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Bandwidth (computing)",
      "score": 0.0
    },
    {
      "name": "Baseband",
      "score": 0.0
    }
  ]
}