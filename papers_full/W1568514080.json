{
  "title": "Video (language) modeling: a baseline for generative models of natural videos",
  "url": "https://openalex.org/W1568514080",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4294328612",
      "name": "Ranzato, MarcAurelio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221350752",
      "name": "Szlam, Arthur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3090958724",
      "name": "Bruna, Joan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4293405110",
      "name": "Mathieu, Michaël",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222857666",
      "name": "Collobert, Ronan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2500712225",
      "name": "Chopra, Sumit",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2163202312",
    "https://openalex.org/W2079256176",
    "https://openalex.org/W1489081407",
    "https://openalex.org/W2141967239",
    "https://openalex.org/W179875071",
    "https://openalex.org/W24089286",
    "https://openalex.org/W2105464873",
    "https://openalex.org/W2952186347",
    "https://openalex.org/W2146444479",
    "https://openalex.org/W2074551195",
    "https://openalex.org/W2100495367",
    "https://openalex.org/W2138960858",
    "https://openalex.org/W2135341757",
    "https://openalex.org/W2109034718",
    "https://openalex.org/W97016928",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2152681473",
    "https://openalex.org/W2171490498",
    "https://openalex.org/W2133257461",
    "https://openalex.org/W2091662199",
    "https://openalex.org/W2157655975",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2136163184",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2950612966",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2125772040",
    "https://openalex.org/W2025768430"
  ],
  "abstract": "We propose a strong baseline model for unsupervised feature learning using video data. By learning to predict missing frames or extrapolate future frames from an input video sequence, the model discovers both spatial and temporal correlations which are useful to represent complex deformations and motion patterns. The models we propose are largely borrowed from the language modeling literature, and adapted to the vision domain by quantizing the space of image patches into a large dictionary. We demonstrate the approach on both a filling and a generation task. For the first time, we show that, after training on natural videos, such a model can predict non-trivial motions over short video sequences.",
  "full_text": "arXiv:1412.6604v5  [cs.LG]  4 May 2016\nV IDEO (LANGUAGE ) MODELING : A BASELINE\nFOR G ENERATIVE M ODELS OF N ATURAL V IDEOS\nMarc'Aurelio Ranzato1, Arthur Szlam1, Joan Bruna1,2, Michael Mathieu1,2, Ronan Collobert1, Sumit Chopra1\n1Facebook AI Research 2Courant Institute of Mathematical Sciences\nMenlo Park CA & New York NY USA\n{ranzato,aszlam,locronan,spchopra}@fb.com{bruna, mathieu}@cims.nyu.edu\nA BSTRACT\nWe propose a strong baseline model for unsupervised featurelearning using video\ndata. By learning to predict missing frames or extrapolate future frames from an\ninput video sequence, the model discovers both spatial and temporal correlations\nwhich are useful to represent complex deformations and motion patterns. The\nmodels we propose are largely borrowed from the language modeling literature,\nand adapted to the vision domain by quantizing the space of image patches into a\nlarge dictionary.\nWe demonstrate the approach on both a ﬁlling and a generationtask. For the ﬁrst\ntime, we show that, after training on natural videos, such a model can predict\nnon-trivial motions over short video sequences.\n1 I NTRODUCTION\nThe human visual system has the ability to learn new visual concepts from only a handful of in-\nstances. It can generalize to new views, new lighting conditions, and it is robust to large within-class\nvariations. Arguably, a key property of a good artiﬁcial visual system is to also be able to discover\npatterns on-the-ﬂy and to generalize from very few labeled instances. Therefore, it seems natural\nto investigate computational models that can leverage unlabeled data to discover regularities and\nstructure from the visual world without using any annotation.\nDespite great advances in object recognition, detection, and parsing over the past few\nyears (Krizhevsky et al., 2012; Hariharan et al., 2014; Simonyan & Zisserman, 2014; Girshick et al.,\n2014), none of the widely used methods for these tasks relieson unlabeled data. Instead, they\nrequire very large and carefully annotated datasets. Thereis a vast body of literature on unsu-\npervised learning for vision, for example (Hinton & Salakhutdinov, 2006; Vincent et al., 2008;\nKavukcuoglu et al., 2008), but these methods have not found success in practical applications yet.\nThe biggest hurdle to overcome when learning without supervision is the design of an objective\nfunction that encourages the system to discover meaningfulregularities. One popular objective is\nsquared Euclidean distance between the input and its reconstruction from some extracted features.\nUnfortunately, the squared Euclidean distance in pixel space is not a good metric, since it is not stable\nto small image deformations, and responds to uncertainty with linear blurring. Another popular ob-\njective is log-likelihood, reducing unsupervised learning to a density estimation problem. However,\nestimating densities in very high dimensional spaces can bedifﬁcult, particularly the distribution of\nnatural images which is highly concentrated and multimodal.\nAlthough some recent works have shown some progress on small er resolution im-\nages (Goodfellow et al., 2014; Zoran & Weiss, 2012; Ranzato et al., 2013; Theis et al., 2011) using\ngenerative models, it is unclear how these models can scale to handle full resolution inputs, due\nto the curse of dimensionality. Other models attempt to overcome this problem by using priors\nover the features, such as sparsity (Olshausen & Field, 1997; Kavukcuoglu et al., 2008; Lee et al.,\n2007). Although these constraints make the learning problem better posed, they are still too simplis-\ntic to capture complex interactions between features. While several authors have reported realistic\ngeneration of small image patches, fewer works have operated at the scale of high-resolution im-\nages (Theis et al., 2012; Ranzato et al., 2013) and success has been more limited.\n1\nOn the other hand, many have argued that learning without (human) supervision can become much\neasier once we consider not just a collection of independently drawn natural images, but a dataset\nof natural videos (Ostrovsky et al., 2009). Then, spatial-temporal correlations can provide powerful\ninformation about how objects deform, about occlusion, object boundaries, depth, and so on. By just\nlooking at a patch at the same spatial location across consecutive time steps, the system can infer\nwhat the relevant invariances and local deformations are. Even more so when studying generative\nmodels of natural images, modeling becomes easier when conditioning on the previous frame as\nopposed to unconditional generation, yet this task is non-trivial and useful as the model has to\nunderstand how to propagate motion and cope with occlusion.\nResearch on models that learn unsupervised from videos is still in its infancy. In their seminal work,\nvan Hateren & Ruderman (1998) and Hurri & Hyv¨ arinen (2003) have applied ICA techniques to\nsmall video cubes of patches. Wiskott & Sejnowski (2002) have proposed instead a method based\non slowness of features through time, an idea later extendedin a bilinear model by Gregor & LeCun\n(2010). Bilinear models of transformations between nearbyframes have also been investigated\nby Memisevic & Hinton (2009); Sutskever et al. (2009); Taylor et al. (2011); Michalski et al. (2014)\nas well as Miao & Rao (2007) via Lie group theory. Related workby Cadieu & Olshausen (2009)\nuses a hierarchical model with a predeﬁned decomposition between amplitude (which varies slowly)\nand phase (encoding actual transformations). Perhaps withthe only exception given by the layered\nmodel proposed by Jojic & Frey (2001), all the above mentioned models have been demonstrated on\neither small image patches or small synthetic datasets (Sutskever et al., 2009; Pachitariu & Sahani,\n2012; Michalski et al., 2014). One major drawback of these models is that they are often not easy to\nextend to large frames, they do not scale to datasets larger than a few thousand frames and they do\nnot generalize to a large number of generic transformations.\nIn this work, we propose a stronger baseline for video modeling. Inspired by work in the language\nmodeling community (Bengio et al., 2003; Mikolov et al., 2010), we propose a method that is very\nsimple yet very effective, as it can be applied to full resolution videos at a modest computational\ncost. The only assumption that we make is local spatial and temporal stationarity of the input\n(in other words, we replicate the model and share parametersboth across space and time), but no\nassumption is made on what features to use, nor on how to modelthe transformation between nearby\nframes. Despite our rather simplistic assumptions, we showthat the model is actually able to capture\nnon-trivial deformations. To the best of our knowledge, we demonstrate for the ﬁrst time that a\nparametric model can generate realistic predictions of short video sequences after being trained on\nnatural videos.\n2 M ODEL\nGiven a sequence of consecutive frames from a video, denotedby (X1, X 2, . . . , X t), we might want\nto train a system to predict the next frame in the sequence,Xt+1, where the subscript denotes time.\nMore generally, given some context of frames, we might want to try to predict some frames that\nhave been left out of the context. This is a simple and well deﬁned task which does not require\nlabels, yet accurate predictions can only be produced by models that have learned motion primitives\nand understand the local deformations of objects. At test time, we can validate our models on both\ngeneration and ﬁlling tasks (see sec. 3.3 and 3.4).\nIn order to design a system that tackles these tasks, we draw inspiration from classical methods from\nnatural language processing, namely n-grams, neural net language models (Bengio et al., 2003) and\nrecurrent neural networks (Mikolov et al., 2010). We will ﬁrst review these methods and then explain\nhow they can be extended to model video sequences (as opposedto sequences of words).\n2.1 L ANGUAGE M ODELING\nIn language modeling, we are given a sequence ofdiscretesymbols (e.g., words) from a ﬁnite (but\nvery large) dictionary. Let a symbol in the sequence be denoted byXk (symbol at positionk in the\nsequence); ifV is the size of the dictionary,Xk is an integer in the range[1, V ].\nIn language modeling, we are interested in computing the probability of a sequence of words,\np(X1, X 2, . . . , X N ) = p(XN |XN− 1, . . . , X 1)p(XN− 1|XN− 2, . . . , X 1) . . . p (X2|X1)p(X1).\n2\nTherefore, everything reduces to computing the conditional distribution:p(Xk|Xk− 1, . . . , X 1). In\nthe following, we will brieﬂy review three methods to estimate these quantities.\n2.1.1 N -GRAM\nThe n-gram is a table of normalized frequency counts under the Markovian assumption that\np(Xt|Xt− 1, . . . X 1) = p(Xt|Xt− 1, . . . , X t− n+1). In this work, these conditional probabilities are\ncomputed by the count ratio:\np(Xt|Xt− 1, . . . , X t− n+1) = count(Xt− n+1, . . . , X t) + 1\ncount(Xt− n+1, . . . , X t− 1) +V (1)\nwhere the constants in the numerator and denominator are designed tosmooth the distribution and\nimprove generalization on unfrequent n-grams (Laplace smoothing). In this work, we considered\nbigrams and trigrams (n=2 and n=3, respectively).\n2.1.2 N EURAL N ET L ANGUAGE M ODEL\nThe neural net language model (NN) (Bengio et al., 2003) is a parametric and non-linear extension\nof n-grams. Let1(Xk) be the 1-hot vector representation of the integerXk, that is, a vector with\nall entries set to 0 except theXk-th component which is set to 1. In this model, the words in the\ncontext (those upon which we condition) are ﬁrst transformed into their 1-hot vector representation,\nthey are then linearly embedded using matrixWx, the embeddings are concatenated and ﬁnally fed\nto a standard multilayer neural network. This network is trained using a cross-entropy loss to predict\nthe next word in the sequence (usual multi-class classiﬁcation task withV classes). In this model,\nthe output probability is then given by:\np(Xt|Xt− 1, . . . , X t− n+1) =SM (MLP [Wx1(Xt− 1), . . . , W x1(Xt− n+1)]) (2)\nwhere SM stands for softmax and MLP any multi-layer neural network (in our case, it is a one\nhidden layer neural network with ReLU units). Note that the ﬁrst layer of MLP acts like a look up\ntable (given the input encoding), mapping each discrete symbol into a continuous embedding vector.\n2.1.3 R ECURRENT N EURAL N ETWORK\nThe recurrent neural network (rNN) (Mikolov et al., 2010), works similarly to the model above\nexcept that: 1) it takes only one input at the time, and 2) the hidden layer of the MLP takes as input\nalso a linear transformation of the hidden state at the previous time step. This enables the rNN to\npotentially leverage a variable size context without compromising computational efﬁciency. The\nequations that regulate the rNN are:\nht− 1 = σ(Whht− 2 + Wx1(Xt− 1)), p (Xt|ht− 1) =SM (Woht− 1 + bo) (3)\nTraining the parameters of the model,{Wx, W h, W o, bo}, proceeds by minimization of the standard\ncross-entropy loss on the next symbol using back-propagation through time (Rumelhart et al., 1986)\nand gradient clipping (Mikolov et al., 2010).\n2.2 V IDEO (LANGUAGE ) MODELING\nThe above mentioned methods work on a sequence of discrete input values; however, video frames\nare usually received as continuous vectors (to the extent that 8-bit numbers are continuous). If we\nwant to use these methods to process video sequences, we can follow two main strategies. We can\neither replace the cross-entropy loss with mean squared error (or some other regression loss), or we\ncan discretize the frames.\nThe former approach turns the classiﬁcation problem into regression. As mentioned in sec. 1, this\nis hard because it is very easy for the model to produce relatively low reconstruction errors by\nmerely blurring the last frame. In our experiments, we foundthat this approach was harder to\noptimize and yielded results only marginally better than simply predicting the last frame (relative\nMSE improvement of 20% only).\nThe other option is to discretize the input, for instance by using k-means. Instead of operating in\nthe pixel space where we do not know a good metric for comparing image patches, we operate in\n3\nQuant.\n+\nW\nSM\nx\nh\no\nrNN\nX\nt\nt-1\nh ht\n(i,j)\n(i,j)~\nQuant.\n+\nSM\nh\nrCNN\nX\nt\nt-1\nh ht\n(i,j)\nXt-1\n(i+1,j)\nXt-1\n(i,j+1)\n...\nConvx\nConvx\nConv\nConvo\nConvo\n1\n1\n2\n2\n(i,j)~\nX\nX\nX\nh\nh\nh\nht-3\nt-2\nt-1\nt\nt\nt-1\nt-2\nt-2\nt-1\nt-3\n~\n~\n~\nFigure 1: Left: outline of an rNN building block applied to video patch modeling. Center: outline\nof rCNN building block applied to frame-level video modeling. The rCNN takes as input a patch of\nquantized patches and uses both spatial and temporal context to predict the central patch at the next\ntime step. At test time, it can be unrolled spatially over anyframe size. Right: example of how such\nblocks are replicated over time to model a video sequence (sharing parameters over time).\na very sparse feature space, where each patch is coded by ak-means atom. This sparsity enforces\nstrong constraints on what is a feasible reconstruction, asthek-means atoms “parameterize” the\nspace of outputs. The prediction problem is then simpler because thevideo modeldoes not have to\nparameterize the output space; it only has to decide where inthe output space the next prediction\nshould go. On the other hand, with even a small set of centroids, there are a huge number of possible\ncombinations of centroids that could reasonably occur in animage or video sequence, and so the\nprediction problem is still non-trivial. There is clearly atrade-off between quantization error and\ntemporal prediction error. The larger the quantization error (the fewer the number of centroids), the\neasier it will be to predict the codes for the next frame, and vice versa. In this work, we quantize\nsmall gray-scale 8× 8 patches using 10,000 centroids constructed viak-means, and represent an\nimage as a 2d array indexing the centroids.\nTo summarize, we apply the language modeling methods described above by quantizing video\nframes usingk-means on non-overlapping image patches. When modeling video cubes of patches\n(i.e., patches at the same location but across consecutive time steps, see ﬁg. 1 left), the approach is\nrather straightforward and will be empirically evaluated in sec. 6.1.\n2.2.1 R ECURRENT C ONVOLUTIONAL N EURAL N ETWORK\nThe last model we propose is a simple extension of the rNN to better handle spatial correlations\nbetween nearby image patches. In the rNN, nearby patches aretreated independently while there\nare almost always very strong spatial correlations betweennearby patches. In the recurrent convo-\nlutional neural network (rCNN) we therefore feed the systemwith not only a single patch, but also\nwith the nearby patches. The model will not only leverage temporal dependencies but also spatial\ncorrelations to more accurately predict the central patch at the next time step (see ﬁg. 1 center).\nThe rCNN that we will use in our experiments, takes as input a patch of size 9× 9. Each integer\nin the grid corresponds to a quantized patch of size 8× 8 pixels. In this work, these patches do not\noverlap although everything we describe would apply to overlapping patches as well. This input\npatch of integers is ﬁrst embedded into a continuous featurespace as in a standard rNN (matrix\nWx in eq. 3), and then passed through two convolutional layers.In ﬁg. 1 center, “Conv1\nx” actually\nrepresents: embedding followed by convolution and logistic non-linearity. All convolutional layers\nuse 128 ﬁlters of size 3× 3. Because of border effects, the recurrent code has 128 feature maps\nof size 5× 5. To avoid border effects in the recurrent code (which couldpropagate in time with\ndeleterious effects), the transformation between the recurrent code at one time step and the next\none is performed by using 1× 1 convolutional ﬁlters (effectively, by using a fully connected layer\nwhich is shared across all spatial locations). Finally, therecurrent code isdecoded through other\ntwo convolutional layers with 128 ﬁlters of size 3× 3. These produce a vector of size 128 and spatial\nsize 1× 1 which is fed to a fully connected layer withV outputs (in our case, 10,000).\n4\nTable 1: Entropy in bits/patch and (perplexity) for predicting the next 8× 8 quantized patch (10,000\natoms in the dictionary). Left: van Hateren dataset. Right:UCF 101 dataset.\nModel Training Validation Test\nbi-gram 8.3 (314) 9.9 (884) 9.3 (647)\nNN 5.9 (59) 7.6 (192) 7.2 (146)\nrNN 5.9 (59) 7.7 (211) 7.3 (156)\nModel Training Validation Test\nbi-gram 4.8 (27) 4.8 (27) 4.9 (29)\nNN 4.4 (21) 4.4 (21) 4.5 (22)\nrNN 4.0 (16) 4.3 (20) 4.3 (20)\nrCNN 3.7 (13) 3.8 (14) 3.9 (15)\nAt generation (test) time, we unroll the rCNN on a larger frame (since all layers are convolutional1).\nThe use of several convolutional layers in the decoder is a good guarantee that nearby predictions\nare going to be spatially consistent because most of the computation is shared across them. Even\nthough the recurrent code can ﬂuctuate rapidly in response to a rapidly varying input dynamics, the\nprediction is going to favor spatially coherent regions.\n3 E XPERIMENTS\nIn this section, we empirically validate the language modeling techniques discussed in sec. 2. Quan-\ntitatively, we evaluate models in terms of their ability to predict patches one frame ahead of time.\nWe also show examples of generation and ﬁlling of short videosequences that capture non trivial\ndeformations.\nIn our experiments, we used the following training protocol. First, we do not pre-process the data\nin any way except for gray-scale conversion and division by the standard deviation. Second, we use\n10,000 centroids fork-means quantization. Third, we cross-validate all our hyper-parameters on the\nvalidation set and report accuracy on the test set using the model that gave the best accuracy on the\nvalidation set. For the van Hateren’s dataset, we used 3 videos for validation and 3 videos for testing\n(out of the 56 available). For the UCF 101 dataset, we used thestandard split (Soomro et al., 2012).\nResults on the van Hateren’s dataset are reported in the Supplementary Material for lack of space.\n3.1 UCF-101 D ATASET\nThe UCF-101 dataset (Soomro et al., 2012) is a standard benchmark dataset for human action recog-\nnition. It has 13320 videos of variable length belonging to 101 human action categories, and each\nframe has size 160× 320 pixels. This dataset is interesting also for unsupervised learning because:\na) it is much larger than the van Hateren dataset, and b) it is much more realistic since the motion\nand the spatial scale of objects have not been normalized. This dataset is by no means ideal for\nlearning motion patterns either, since many videos exhibitjpeg artifacts and duplicate frames due to\ncompression, which further complicate learning.\nTab. 1 (right) compares several models. In this case, biggermodels worked generally better. In\nparticular, the rCNN yields the best results, showing that not only the temporal but also the spatial\ncontext are important for predicting a patch at the next timestep. The best rCNN was trained by\nusing: 8 back-propagation through time steps, mini-batch size 128, learning rate set to 0.005 with\nmomentum set to 0.9, and it had 128 feature maps at the output of each convolutional layer.\nIn order to understand how much quantization hurts generation and to make our results comparable\nto methods that directly regress pixel values (for which entropy and perplexity do not apply), we\nalso report the average root mean square error (RMSE) per pixel value (using pixel values scaled\nin the range from 0 to 255). On the test set, rCNN achieves an average RMSE of 15.1 while a\nperfect model of the temporal dynamics (i.e., accounting only for the quantization error) gets an\nRMSE of 8.9. Although RMSE is not a good metric to measure similarity between images, we\ncan conclude that quantization error accounts for about half of the total error and that temporal\ndynamics are captured fairly accurately (in average). In order to compute RMSE, we reconstructed\neach patch using the most likely dictionary element and we averaged predictions for all 8× 8 spatial\ndisplacements, so that each pixel is the average of 64 values. Averaging over spatial displacements\nwas important to remove blocking artifacts due to the non-overlapping nature of the quantization\nused.\n1 Fully connected layers can be interpreted as limit case of convolutional layers with kernels of size 1× 1.\n5\nFigure 2: Left: Example of embeddings learned by rCNN. The ﬁrst column shows thek-means\ncentroid corresponding to a certain atom in the dictionary.The other columns show the centroids\nwhose corresponding embeddings are the nearest neighbors to the one in the ﬁrst column. Each row\ncorresponds to a randomly picked atom. Right: Each 3× 3 grid of patches shows those input patches\n(from the validation set) that make a certain unit (output ofthe ﬁrst convolutional layer) ﬁre the\nmost. Only a random subset of these units and embeddings are shown.\nTable 2: Analysis of the static (left) and dynamic (right) part of rCNN. Perplexities are computed on\na subset of UCF 101 validation set. See main text for details.\nModel Perplexity\n1 frame (copy of previous), natural layout2.1\nstatic video (long squence), natural layout1.3\n1 frame (copy of previous), random layout6.6\nstatic video (long squence), random layout2.0\nModel Perplexity\nnatural 12.2\nreversed 12.3\nrandom 100,000+\nskip 1 frame 30\n3.2 A NALYZING THE M ODEL\nIn this section, we investigate what the rCNN has learned after training on the UCF 101 dataset.\nFirst, we analyze the parameters in the embedding matrix andﬁrst convolutional layer.\nThere is one embedding perk-means centroid and we look at the centroids corresponding to the\nnearest neighbor embeddings. Fig. 2 (left) shows that the rCNN, but similarly the rNN and NN,\nlearns to cluster together similar centroids. This means that, despite the quantization step, the model\nlearns to become robust to small distortions. It does not matter much which particular centroid we\npick, the vector that we output is going to be nearby in space for similar looking input patches.\nWe also visualize (a random subset of) the ﬁrst layer convolutional ﬁlters by looking at the input\nexamples (from the validation set) that make the output ﬁre the most. Fig. 2 (right) shows that these\npatterns exhibit similar structure but at slightly different position, orientation and scale.\nFinally, we try to disentangle the static (only spatial) andthe dynamic (only temporal) part of rCNN.\nIn the left part of tab. 2 we compute the model’s score for static video sequences (initializing on a\ngiven frame) of different length. rCNN assigns high likelihood to non-moving sequences. However,\nif we randomly permute the order of the patches (and maintainsuch order for the whole sequence),\nthe likelihood is lower - demonstrating a preference for natural videos. This experiment show that\nrCNN does take into account the spatial context and that it does not learn a mere identity function.\nThe right part of the table investigates the temporal part ofthe model. rCNN is barely able to\ndistinguish between video sequences that are played in the natural versus reversed order, but it\ndeﬁnitely prefers temporally ordered video sequences.\n3.3 G ENERATION\nAfter training on UCF-101, we used the rCNN to predict futureframes after conditioning upon 12\nreal consecutive frames. Generation proceeds as follows: a) we unroll the rCNN on whatever frame\nsize we use at test time and run it forward on all the frames we condition upon, b) we take the most\nlikely predicted atom as next input, and iterate. In order toalleviate quantization errors, we perform\n6\nFigure 3: Examples of generations after training on the UCF 101 dataset. The ﬁrst two\nframes (columns) are used for initialization of the rCNN, the next two frames are gen-\nerations from rCNN. The last column shows a zoom of the 20× 20 pixel patch marked\nin red (ﬁrst frame on the top). Frames have size 160× 320 pixels. More examples at:\nhttps://research.facebook.com/publications/video-language-modeling-a-baseline-for-generative-models-of-natural-videos/\nthe same procedure on all 64 spatial offsets (combination of8 horizontal and 8 vertical shifts) and\nthen average the predictions for each pixel.\nFig. 3 (right) shows that rCNN is fairly good at completing the motion, even capturing fairly\ncomplex out of plane rotations and deformations. However, the predictions tend to slow\ndown motion quite rapidly, and eventually the model converges to a still image after a cou-\nple of frames in average. Animations, longer generations and comparisons are available at\nhttps://research.facebook.com/publications/video-language-modeling-a-baseline-for-generative-models-of-natural-videos/.\nGenerally speaking, the model is good at predicting motion of fairly fast moving objects of large\nsize, but it has trouble completing videos with small or slowly moving objects.\nIn the url above, we also compare to the generation produced by a baseline method based on optical\nﬂow. This estimates the ﬂow on the ﬁrst two frames, and applies it to the remaining ones. This\nmethod exploits knowledge of low level image structure and local distorions. Although, this baseline\nmay produce good predictions of the next frame, it also degrades the quality of subsequent frames\nby introducing signiﬁcant smearing artifacts.\n3.4 F ILLING\nWe also evaluate a neural network language model on the task of ﬁlling in frames from a video,\ngiven boundary values. For simplicity, the boundary valuesinclude the top/bottom/left/right borders\nof the whole video cube (15 pixels wide), in addition to the frames used for temporal context (both\npast and future). We use a model which takes as input two 3× 3 patches of atoms at the same location\nfrom framesj and j + 2, and it is trained to predict the middle atom in the corresponding 3× 3 patch\nof the (j + 1)-th frame.\nAt test time, we use an iterative procedure to ﬁll in larger gaps. At each iteration and spatio-temporal\nlocationz, we take our current estimate of the spatio-temporal contextC ofz, and use it as input to\nour language model to re-estimate the atom atz. In our experiments, we update each spatio-temporal\nlocation before updating the contexts and the iteration counter. Finally, we reconstruct the quantized\nframes by averaging over the 64 shifts of the model as before.Fig. 4 shows examples of ﬁlling in\n3 consecutive frames from a UCF-101 video in the validation set. The model compares favorably\nagainst both linear interpolation and an optical ﬂow baseline.\nNote that unsurprisingly, the problem of ﬁlling a single frame given its temporal neighbors is easier\nthan extrapolating to the future. For example, the simple model described here achieves a validation\nentropy of2. 8 bits/patch; compare to Table 1.\n7\nFigure 4: Examples of 3 frames ﬁlled in by our algorithm, optical ﬂow and linear interpolation (from\ntop to bottom). Time goes from left to right. On the left, we show a zoom of a patch.\n4 D ISCUSSION\nDespite the promising results reported in the previous section, language modeling based techniques\nhave also several limitations, which may open avenues of future work.\nMulti-Scale Prediction:Multiscale, coarse-to-ﬁne approaches are classic in motion estimation\nmodels (Brox et al., 2004). Similar approaches could be easily adopted for language modeling in the\ncontext of video, and in particular, for rCNN. For instance,one could use a standard multi-resolution\npyramid, whereby a ﬁner resolution level is fed with the residual errors produced by a coarser reso-\nlution level. Moreover, since motion statistics are roughly scale invariant, the same rCNN could be\nused at all resolutions (except for the coarsest one, which operates on the local means rather than\nthe local differences). Such scheme would allow to better model motion, regardless of the speed and\nsize of objects.\nMulti-Step Prediction:One of the limitations of the current model is that it cannot perform predic-\ntions further than a few frames ahead into the future. Without injecting sampling noise in the system,\nthe model converges rapidly to a static image because a) in average (over all the frames in the data)\nthere is little if any motion and b) the peak of the distribution (recall that we propagate the max)\ndoes not capture its variance (i.e., how uncertain the modelis about the position of a certain edge).\nIn particular, the fact that the distribution of natural videos has a strong bias for non-moving inputs\nindicates that this task is intrinsically different from the usual language modeling one. On the other\nhand, injecting sampling noise independently at each spatial location hampers spatial coherence (see\nsupplementary material).\nAlthough we do not have a full understanding of this issue, weconjecture that one way to combat\nthis phenomenon is to predict several steps ahead of time, feeding the system with its own predic-\ntions. This will have several beneﬁts: it will encourage thesystem to produce longer sequences and,\nat the same time, it will make the system robust against its own prediction errors.\nAnother strategy is to modify the inference at generation time. Although running full Viterbi decod-\ning is prohibitive due to the large number of spatio-temporal interaction terms, one could modify the\ngreedy generation algorithm to take into account the joint spatio-temporal co-occurrences of image\npatches, for instance with n-grams over temporal and spatial slices.\nStructured Prediction versus Regression:While we found it difﬁcult to directly regress output\nframes in thel2 metric, quantization also introduces some drawbacks. Besides visual artifacts, it\nmakes the learning task harder than necessary, because all targets are assumed to be equally dissim-\nilar, even though they are not. Moreover, quantization makes it hard to learn end-to-end the whole\nsystem, to back-propagate easily through a multi-step prediction model, and to efﬁciently perform\njoint inference of all patches in a given frame (given the combinatorial nature of the discrete prob-\nlem).\nImplicit VS Explicit Modeling of Transformations:The model we proposed does not have any\nexplicit representation of transformations. It is therefore difﬁcult to generate perpetual motion, to\nextract motion features and to relate objects characterized by similar motion (or vice versa, to tell\nwhether the same object is moving in a different way). The “what” and “where” are entangled. How-\never, it seems straightforward to extend the proposed modelto account for motion speciﬁc features.\nFor instance, part of the learned representation could be tied across frames to encode the “what”,\nwhile the rest could be dedicated to represent transformations, perhaps using bilinear models.\n8\n5 C ONCLUSION\nWe have presented a baseline for unsupervised feature learning inspired by standard language mod-\neling techniques. The method is simple, easy to reproduce and should serve as a stronger baseline\nfor research work on unsupervised learning from videos. It consists of a quantization step, followed\nby a convolutional extension of rNN. We evaluated the performance of this model on a relatively\nlarge video dataset showing that the model is able to generate short sequences exhibiting non-trivial\nmotion.\nThis model shows that it is possible to learn the local spatio-temporal geometry of videos purely from\ndata, without relying on explicit modeling of transformations. The temporal recurrence and spatial\nconvolutions are key to regularize the estimation by indirectly assuming stationarity and locality.\nHowever, much is left to be understood. First, we have shown generation results that are valid only\nfor short temporal intervals, after which long range interactions are lost. Extending the prediction\nto longer spatio-temporal intervals is an open challenge against the curse of dimensionality, which\nimplies moving from pixel-wise predictions to more high-level features. Next, it remains to be seen\nwhether the resulting features are useful to supervised tasks, such as action recognition.\nACKNOWLEDGEMENTS\nThe authors would like to acknowledge Piotr Dollar for providing us the optical ﬂow estimator,\nManohar Paluri for his help with the data, and all the FAIR team for insightful comments.\nR EFERENCES\nBengio, Y ., Ducharme, R., Vincent, P., and Jauvin, C. A neural probabilistic language model.JMLR , 2003.\nBrox, T., Bruhn, A., Papenberg, N., and Weickert, J. High accuracy optical ﬂow estimation based on a theory\nfor warping. InComputer Vision-ECCV 2004, pp. 25–36. Springer, 2004.\nCadieu, C. and Olshausen, B. Learning transformational invariants from natural movies. InNIPS, 2009.\nGirshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik,Jitendra. Rich feature hierarchies for accurate object\ndetection and semantic segmentation. InProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2014.\nGoodfellow, I, Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y .\nGenerative adversarial nets. InNIPS, 2014.\nGregor, K. and LeCun, Y . Emergence of complex-like cells in atemporal product network with local receptive\nﬁelds. arXiv:1006.0448, 2010.\nHariharan, B., Arbel´ aez, P., Girshick, R., and Malik, J. Simultaneous detection and segmentation. InECCV ,\n2014.\nHinton, G.E. and Salakhutdinov, R. R. Reducing the dimensionality of data with neural networks.Science,\n2006.\nHurri, J. and Hyv¨ arinen, A. Simple-cell-like receptive ﬁelds maximize temporal coherence in natural video.\nNeural Computation, 2003.\nJojic, N. and Frey, B.J. Learning ﬂexible sprites in video layers. InCVPR , 2001.\nKavukcuoglu, K., Ranzato, M., and LeCun, Y . Fast inference in sparse coding algorithms with applications to\nobject recognition. ArXiv 1010.3467, 2008.\nKrizhevsky, A., Sutskever, I., and Hinton, G. ImageNet classiﬁcation with deep convolutional neural networks.\nInNIPS, 2012.\nLee, H., Chaitanya, E., and Ng, A. Y . Sparse deep belief net model for visual area v2. InAdvances in Neural\nInformation Processing Systems, 2007.\nMemisevic, R. and Hinton, G.E. Learning to represent spatial transformations with factored higher-order\nboltzmann machines.Neural Computation, 22:1473–1492, 2009.\nMiao, X. and Rao, R. Learning the lie groups of visual invariance.Neural Computation, 2007.\n9\nMichalski, V ., Memisevic, R., and Konda, K. Modeling deep temporal dependencies with recurrent ”grammar\ncells”. InNIPS, 2014.\nMikolov, T., Karaﬁat, M., Burget, L., Cernocky, J., and Khudanpur, S. Recurrent neural network based language\nmodel. InProc.Interspeech, 2010.\nOlshausen, B. A. and Field, D. J. Sparse coding with an overcomplete basis set: a strategy employed by v1?\nVision Research, 37:3311–3325, 1997.\nOstrovsky, Y ., Meyers, E., Ganesh, S., Mathur, U., and Sinha, P. Visual parsing after recovery from blindness.\nPsychological Science, 2009.\nPachitariu, M. and Sahani, M. Learning visual motion in recurrent neural networks. InNIPS, 2012.\nRanzato, M., Mnih, V ., Susskind, J., and Hinton, G. Modelingnatural images using gated mrfs.PAMI , 2013.\nRumelhart, D.E., Hinton, G.E., and Williams, R.J. Learningrepresentations by back-propagating errors.Na-\nture, 323:533–536, 1986.\nSimonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. InNIPS,\n2014.\nSoomro, K., Zamir, A.R., and Shah, M. Ucf101: A dataset of 101human action classes from videos in the\nwild. CRCV-TR-12-01, 2012.\nSutskever, I., Hinton, G.E., and Taylor, G.W. The recurrenttemporal restricted boltzmann machine. InNIPS,\n2009.\nTaylor, G.W., Hinton, G.E., and Roweis, S. T. Two distributed-state models for generating high-dimensional\ntime series.JMLR , 2011.\nTheis, L., Gerwinn, S., Sinz, F., and Bethge, M. In all likelihood, deep belief is not enough.JMLR , 2011.\nTheis, L., Hosseini, R., and Bethge, M. Mixtures of conditional gaussian scale mixtures applied to multiscale\nimage representations.PLoS ONE , 7(7), 2012.\nvan Hateren, J.H. and Ruderman, D.L. Independent componentanalysis of natural image sequences yields\nspatio-temporal ﬁlters similar to simple cells in primary visual cortex.Royal Society, 1998.\nVincent, P., Larochelle, H., Bengio, Y ., and Manzagol, P.A.Extracting and composing robust features with\ndenoising autoencoders. InICML , 2008.\nWiskott, L. and Sejnowski, T. Slow feature analysis: unsupervised learning of invariances.Neural Computa-\ntion, 14(4):715–770, 2002.\nZoran, D. and Weiss, Y . Natural images, gaussian mixtures and dead leaves. InNIPS, 2012.\n10\n6 S UPPLEMENTARY M ATERIAL\n6.1 VAN H ATEREN ’S D ATASET\nThe van Hateren dataset of natural videos has been a standarddataset for investigat-\ning models of temporal sequences (Cadieu & Olshausen, 2009;Olshausen & Field, 1997).\nOur version was downloaded from the material provided by Cadieu & Olshausen (2009)\nathttps://github.com/cadieu/twolayer. It consists of 56 videos, each 64 frames long.\nEach frame has size 128× 128 pixels. This is a very small dataset, where objects are highly textured\nand move at similar speeds. Given the small dataset size, we could only evaluate patch based mod-\nels. Fig. 5 shows examples of video patches extracted from this dataset, along with the effect of\nquantization on the images.\nTab. 1 (left) compares n-grams (tri-grams worked worse thanbi-grams and are therefore omitted),\nneural net and rNN language models. Given the small dataset size, overﬁtting prevented bigger\nmodels to perform better (on the validation/test sets). We found that the neural net and the rNN\nwork similarly and better than the bi-gram.\nFigure 5: van Hateren’s video dataset: example of consecutive patches extracted at random spatial\nlocations (left) and example of a frame and its quantized version.\nFigure 6: Example of generation of 8× 8 pixel image patches after training on the van Hateren’s\nvideo dataset. Each row is an independent sequence. Columnsare consecutive time steps (read from\nleft to right).\n11\n6.2 G ENRATION E XAMPLES\nPlease, refer to\nhttps://research.facebook.com/publications/video-language-modeling-a-baseline-for-generative-models-of-natural-videos/\nfor more examples.\n6.3 F ILLING E XAMPLES\nHere, we provide more examples and details about how we ran ﬁlling experiments. The optical ﬂow\nbaseline of ﬁg. 4, was computed by: a) estimating the ﬂow fromtwo frames in the past, b) estimating\nthe ﬂow from two frames in the future, c) linearly interpolating the the ﬂow for the missing frames\nand d) using the estimated ﬂow to reconstruct the missing frames.\nBelow, you can ﬁnd some examples of ﬁlled in frames; more examples are available at:\nhttps://research.facebook.com/publications/video-language-modeling-a-baseline-for-generative-models-of-natural-videos/.\nt t+4\nground truth\nour model\noptical flow\nlinear interpolation\nFigure 7: Example of ﬁlling in missing frames. Top: the frames used for conditioning. Second row:\nground truth missing frames. Third row: our model. Fourth row: optical ﬂow based algorithm. Fifth\nrow: linear interpolation. Last column: zoom of a patch fromthe missing middle frame. See sec. 3.4\nfor details.\n12\nt t+4\nground truth\nour model\noptical flow\nlinear interpolation\nFigure 8: Example of ﬁlling in missing frames. Top: the frames used for conditioning. Second row:\nground truth missing frames. Third row: our model. Fourth row: optical ﬂow based algorithm. Fifth\nrow: linear interpolation. Last column: zoom of a patch fromthe missing middle frame. See sec. 3.4\nfor details.\n13\nt t+4\nground truth\nour model\noptical flow\nlinear interpolation\nFigure 9: Example of ﬁlling in missing frames. Top: the frames used for conditioning. Second row:\nground truth missing frames. Third row: our model. Fourth row: optical ﬂow based algorithm. Fifth\nrow: linear interpolation. Last column: zoom of a patch fromthe missing middle frame. See sec. 3.4\nfor details.\n14\nt t+4\nground truth\nour model\noptical flow\nlinear interpolation\nFigure 10: Example of ﬁlling in missing frames. Top: the frames used for conditioning. Second\nrow: ground truth missing frames. Third row: our model. Fourth row: optical ﬂow based algorithm.\nFifth row: linear interpolation. Last column: zoom of a patch from the missing middle frame. See\nsec. 3.4 for details.\n15",
  "topic": "Baseline (sea)",
  "concepts": [
    {
      "name": "Baseline (sea)",
      "score": 0.795271098613739
    },
    {
      "name": "Computer science",
      "score": 0.7917715311050415
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7162808179855347
    },
    {
      "name": "Generative model",
      "score": 0.6701619625091553
    },
    {
      "name": "Generative grammar",
      "score": 0.6187066435813904
    },
    {
      "name": "Task (project management)",
      "score": 0.6176914572715759
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.6032689809799194
    },
    {
      "name": "Motion (physics)",
      "score": 0.5497888326644897
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4926450848579407
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.4608027935028076
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4386079013347626
    },
    {
      "name": "Natural language",
      "score": 0.4269050359725952
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4163534939289093
    },
    {
      "name": "Computer vision",
      "score": 0.3700084090232849
    },
    {
      "name": "Machine learning",
      "score": 0.3382037281990051
    },
    {
      "name": "Mathematics",
      "score": 0.095333993434906
    },
    {
      "name": "Geography",
      "score": 0.07290148735046387
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 302
}