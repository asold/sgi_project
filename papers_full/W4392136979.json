{
  "title": "Integrated visual transformer and flash attention for lip-to-speech generation GAN",
  "url": "https://openalex.org/W4392136979",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2129288378",
      "name": "Qiong Yang",
      "affiliations": [
        "Xi'an Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2294234465",
      "name": "Yuxuan Bai",
      "affiliations": [
        "Xi'an Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2006442482",
      "name": "Feng Liu",
      "affiliations": [
        "Xi'an Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A1506313323",
      "name": "Wei Zhang",
      "affiliations": [
        "China Mobile (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2129288378",
      "name": "Qiong Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2294234465",
      "name": "Yuxuan Bai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2006442482",
      "name": "Feng Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1506313323",
      "name": "Wei Zhang",
      "affiliations": [
        "China Mobile (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3088985939",
    "https://openalex.org/W3211862173",
    "https://openalex.org/W4220822444",
    "https://openalex.org/W4308356480",
    "https://openalex.org/W4375869161",
    "https://openalex.org/W2046030755",
    "https://openalex.org/W4387478200",
    "https://openalex.org/W4362579348",
    "https://openalex.org/W4302552117",
    "https://openalex.org/W2964352155",
    "https://openalex.org/W4286482502",
    "https://openalex.org/W3035626590",
    "https://openalex.org/W3160305627",
    "https://openalex.org/W4287817261",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W4225665589",
    "https://openalex.org/W4289361892",
    "https://openalex.org/W4385966019",
    "https://openalex.org/W4384920092",
    "https://openalex.org/W6600310816",
    "https://openalex.org/W4385240187"
  ],
  "abstract": "Abstract Lip-to-Speech (LTS) generation is an emerging technology that is highly visible, widely supported, and rapidly evolving. LTS has a wide range of promising applications, including assisting speech impairment and improving speech interaction in virtual assistants and robots. However, the technique faces the following challenges: (1) Chinese lip-to-speech generation is poorly recognized. (2) The wide range of variation in lip-speaking is poorly aligned with lip movements. Addressing these challenges will contribute to advancing Lip-to-Speech (LTS) technology, enhancing the communication abilities, and improving the quality of life for individuals with disabilities. Currently, lip-to-speech generation techniques usually employ the GAN architecture but suffer from the following problems: The primary issue lies in the insufficient joint modeling of local and global lip movements, resulting in visual ambiguities and inadequate image representations. To solve these problems, we design Flash Attention GAN (FA-GAN) with the following features: (1) Vision and audio are separately coded, and lip motion is jointly modelled to improve speech recognition accuracy. (2) A multilevel Swin-transformer is introduced to improve image representation. (3) A hierarchical iterative generator is introduced to improve speech generation. (4) A flash attention mechanism is introduced to improve computational efficiency. Many experiments have indicated that FA-GAN can recognize Chinese and English datasets better than existing architectures, especially the recognition error rate of Chinese, which is only 43.19%, the lowest among the same type.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:4525  | https://doi.org/10.1038/s41598-024-55248-6\nwww.nature.com/scientificreports\nIntegrated visual transformer \nand flash attention \nfor lip‑to‑speech generation GAN\nQiong Yang 1,2, Yuxuan Bai 1*, Feng Liu 1 & Wei Zhang 3\nLip‑to‑Speech (L TS) generation is an emerging technology that is highly visible, widely supported, \nand rapidly evolving. L TS has a wide range of promising applications, including assisting speech \nimpairment and improving speech interaction in virtual assistants and robots. However, the technique \nfaces the following challenges: (1) Chinese lip‑to‑speech generation is poorly recognized. (2) The wide \nrange of variation in lip‑speaking is poorly aligned with lip movements. Addressing these challenges \nwill contribute to advancing Lip‑to‑Speech (L TS) technology, enhancing the communication abilities, \nand improving the quality of life for individuals with disabilities. Currently, lip‑to‑speech generation \ntechniques usually employ the GAN architecture but suffer from the following problems: The \nprimary issue lies in the insufficient joint modeling of local and global lip movements, resulting in \nvisual ambiguities and inadequate image representations. To solve these problems, we design Flash \nAttention GAN (FA‑GAN) with the following features: (1) Vision and audio are separately coded, \nand lip motion is jointly modelled to improve speech recognition accuracy. (2) A multilevel Swin‑\ntransformer is introduced to improve image representation. (3) A hierarchical iterative generator is \nintroduced to improve speech generation. (4) A flash attention mechanism is introduced to improve \ncomputational efficiency. Many experiments have indicated that FA‑GAN can recognize Chinese and \nEnglish datasets better than existing architectures, especially the recognition error rate of Chinese, \nwhich is only 43.19%, the lowest among the same type.\nLip-to-speech1,4 generation is a highly regarded emerging technology that is experiencing rapid growth and many \nimportant breakthroughs. This technology not only provides a new communication tool for people who are \nspeech-impaired or deaf but also plays an important role in the field of education. Lip-to-speech generation can \nbe utilized to improve learners’ speech articulation and oral expression.\nHowever, the field of lip-to-speech generation still faces many challenges. The challenges arise from several \ncomplexities. For instance, when someone says the sound \"ma\" in a video (a syllable in Mandarin), different \npronunciations might correspond to different meanings, like \"妈\" (m a , referring to \"mother\"). These challenges \nin practical applications could potentially lead to decreased accuracy in lip-reading, especially when sensitive \nto syllabic or lexical meanings. Consequently, lip-to-speech generation in Mandarin is relatively challenging, \nyielding poorer recognition results and being less prevalent in this field. In summary, these challenges are mul-\ntifaceted. Firstly, noise, homophones, missing context, and brief lip movement segments might result in mis-\nmatched synthesized speech with the current context, causing distortion in speech synthesis. Secondly, ensuring \nquality and accuracy in speech synthesis requires more efficient techniques for handling image representation \nquality and processing long video sequences. The broader range of lip variations in Mandarin, coupled with more \nhomophones and synonyms, makes recognition more challenging, leading to decreased accuracy in lip-to-speech \ngeneration. These challenges make it more difficult for lip synthesis systems to match audio and visual informa-\ntion, posing significant obstacles to achieving accurate and natural lip-to-speech synthesis.\nTo solve these problems, \"Lip-to-Speech Synthesis with Visual Context Attentional GAN\" 2 introduces the \nVCA-GAN deep architecture. This framework enables accurate speech generation by encoding images and audio, \njointly modeling local and global visual features. Despite its decent performance, there’s room for improvement \nin image representation, refinement of speech generation, and the substantial computational resources required \nfor processing large-scale videos, leading to longer processing times. We propose the FA-GAN deep architecture, \nincorporating the Swin Transformer to enhance visual feature extraction. Simultaneously, a hierarchical iterative \ngenerator refines speech generation, focusing on various audio stages to boost recognition rates and generate \nOPEN\n1School of Computer Science, Xi’an Polytechnic University, Xi’an 710048, Shaanxi, China. 2Shaanxi Key Laboratory \nof Clothing Intelligence, School of Computer Science, Xi’an Polytechnic University, Xi’an 710048, Shaanxi, \nChina. 3China Mobile System Integration Co, Ltd, Xi’an 710077, China. *email: 18691711979@163.com\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:4525  | https://doi.org/10.1038/s41598-024-55248-6\nwww.nature.com/scientificreports/\nspeech closely resembling real speech. The introduction of the flash attention mechanism enhances computational \nefficiency, streamlining the burden and augmenting model performance. These methods have been extensively \nevaluated on datasets like  GRID2 and CN-CVS6, yielding significant outcomes.\nCollectively, The main contributions of this research include the following: (1) Introduced the FA-GAN archi-\ntecture in the Chinese lip-to-speech generation domain, good recognition rates are obtained with the English \ndatasets. (2) Implemented the Swin Transformer to optimize the existing process, enhancing the quality of image \nrepresentation. (3) Introduced a hierarchical iterative generator that improved the speech generation process, \nenabling the model to focus more on different audio stages’ features and variations, consequently improving \nrecognition rates. 4. Introduced the flash attention mechanism to reduce the computational burden from multiple \ninteractions of the hierarchical iterative generator, thereby enhancing the model’s  performance7–10.\nRelate work\nLip‑reading research\nThe origins of lip-reading research can be traced to the early 1900s, when research focused primarily on the cor-\nrelation between sound production and lip and facial movements. However, as technology continued to advance, \nespecially with the widespread use of video cameras and computers, lip-reading research gradually showed its \npotential. Simultaneously, lip-speaking4 generation techniques began to gradually evolve. In the earliest stage, \nlip-to-speech generation techniques relied mainly on a unimodal approach, i.e., using only a video camera to \ncapture images and videos of the mouth and then recognizing the speech information by analysing the shape, \nmovement, and contour of the lips. However, this approach still suffers from some challenges, such as poor image \ncharacterization and limitations such as insufficiently fine-grained speech generation processes, which limit the \naccuracy of lip-based speech generation.\nWith the rise of multimodal lip-speaking research, this means no longer relying solely on visual information \nand incorporating other sensors, such as microphones and sound processing techniques, to improve the accu -\nracy of lip-speaking recognition. Yusheng Dai et al. 24 proposed an audio-guided, cross-modal fusion encoder \n(CMFE) neural network that utilizes the main training parameters of multiple cross-modal attention layers to \nfully utilize modal complementarity. To improve audio-visual speech recognition (AVSR) within the framework \nof pretraining and fine-tuning training, Minsu Kim et al. 2 proposed a lip-synthesis method based on a visual \ncontextual attention GAN, which further improves the performance of lip recognition by better combining the \nvisual features with the contextual context. Jeong Hun Y eo et al.25 proposed an audio knowledge-enabled visual \nspeech recognition framework (AKVSR) to utilize audio modalities to supplement the shortcomings of visual \nmodal speech information. However, these methods still face challenges in terms of speech generation and image \nrepresentation. Most of these models are primarily applied to multimodal lip-reading in English. Therefore, in \nthe Chinese context, research faces a series of challenges. In the Chinese context, there is a greater diversity in \nlip movements, and Chinese pronunciation and speech have unique aspects that add complexity to the model’s \nprocessing.\nHence, our research explicitly proposes an approach called FA-GAN, which involves separately encoding the \nvisual and audio features of videos to fully leverage multimodal data. This approach is designed for the task of \nlip-to-speech generation in both Chinese and English. It aims to address issues in speech generation and image \nrepresentation. The specificity of Chinese pronunciation and the diversity of lip movements in the Chinese \ncontext give this method potential advantages in the field of Chinese lip-to-speech synthesis. FA-GAN improves \nthe extraction of local and global visual information from videos by introducing the Swin Transformer, which \ncaptures subtle variations in lip movements more accurately. Additionally, the hierarchical iterative generator \nis utilized to optimize the speech generation process, enabling the model to better focus on both the local and \noverall characteristics of speech, thereby enhancing the accuracy of the generated speech. However, this also \nresults in a significant increase in the computational burden of the model. To mitigate these effects, the genera-\ntor needs to better capture the global dependencies of the image at the middle layer through the Flash Attention \nmodule after producing speech from local features. While enhancing the accuracy of synthesized speech in both \nChinese and English, lip-to-speech generation, the specific structure is illustrated in Fig.  1.\nMethods\nProposed method\nSynthesizing speech from a silent video by lip recognition is a complex task that requires precise temporal align-\nment of the speaker’s audio speech with their lip movements and encoding of the visual features of the speaker in \nthe video separately from the audio. Additionally, the generated speech must have the same duration as the input \nsilent video, thus obtaining a more lifelike speech synthesis effect and improving the lip recognition accuracy. \nTherefore, our proposed framework involves preprocessing the input video and extracting visual features. Sub-\nsequently, in the speech generation stage, a hierarchical iterative generator is introduced to optimize and refine \nthe speech generation. On the other hand, Flash Attention is introduced to reduce the computational burden \ncaused by multiple interactions of the hierarchical iterative generator. The specific structure is shown in Fig. 1.\nThe local visual coder Pv encodes the input video V into local visual features V f ={ v1\nf,v2\nf, ...,vn\nf}∈ RT ×D  , \nwhere D is the embedding dimension. The local visual coder Pv is a combination of 2D convolution and 3D \nconvolution, and each local visual feature vn\nf  contains the edited local lip motion embedded by 3D convolution. \nAt this point, we introduce n layers of iterative generators to generate and refine the speech representation of \nthe obtained local visual features V f step by step from low resolution to high resolution. This process results \nin hierarchical audio feature generation, which allows the model to focus on the different representations and \nvariations of the audio phase. The first generator synthesizes a coarse speech representation using the following \nformula: (Eq. (1))\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:4525  | https://doi.org/10.1038/s41598-024-55248-6\nwww.nature.com/scientificreports/\nwhere z is noise from a standard normal distribution; R :RT×D → R\nF\n4×T×D  is a repetition operator that forms \na 3D tensor array of height F , width T, and certain number of channels D by repeating the input features F\n4 times; \nand [;] denotes a join or splice  operation2.\nDetailed description\nFirst, a video V with frame number T, height H, width W , and pass size C is provided as input. Second, after the \nprocess of feature extraction and coding, local and global visual information is captured from the video and \nthen transformed into a feature representation. FA-GAN significantly improves speech recognition accuracy \nby jointly modeling local and global lip movements. This approach allows the model to better comprehend the \nrelationship between lip movements and audio, capturing a richer visual context, thereby enhancing the quality \nof speech synthesis. For instance, suppose a speaker in a video has slight blurriness or local inconsistencies in \nlip movements. Traditional methods might struggle in recognizing specific phonemes or words due to insuf-\nficient information to determine the correct pronunciation. However, through the joint modeling of local and \nglobal lip movements in FA-GAN, the model better understands such blurry or inconsistent movements, lead-\ning to more accurate predictions of the correct pronunciation. For example, a speaker utters a phoneme \"ba\" in \na video. During a certain period of local lip movement, there might be some blurriness or inconsistency due \nto video quality or other reasons, which traditional methods could inaccurately recognize. However, FA-GAN \nintegrates local blurry lip movements with the overall context, comprehending the specific phoneme the speaker \nis pronouncing more comprehensively, hence generating the corresponding speech more accurately. This joint \nmodeling approach enables the model to handle local noise or blurriness better, grasping the speaker’s intention \nholistically, thereby enhancing speech recognition accuracy. Third, an attention mechanism is used to align the \ninformation from audio and video. By learning the weights, the attention mechanism can identify the appropriate \nalignment to ensure that the synthesized speech matches the video. The generated speech is S∈ RM×4T  , where M \ndenotes the dimension of the Meier spectrum and 4T denotes that the frame length of the speech is four times \nthat of the video. This outcome is achieved by applying a short-time Fourier transform and frame shift operation \nto ensure that the speech duration matches the video. To further improve the quality of speech synthesis, we \nintroduce a hierarchical iterative generator that assigns the task of synthesizing speech to n generators, which \nfurther refines the speech synthesis process so that the model can focus on different stages of audio changes and \nimprove the accuracy of speech synthesis.\nTo reduce the computational burden of the hierarchical iterative generator, we introduce the flash attention \nmechanism. This mechanism automatically learns the weights of each modality (audio and image) to facilitate \nbetter information transfer and interaction between them. This approach not only helps model the temporal \nrelationship between audio and images but also improves the performance of the model and reduces the com -\nputational burden. Notably, we treat the Mel spectrogram as an image and use a 2D GAN to train the model. \n(1)V 1\na = A 1([R(V f;z)])\nFigure 1.  A detailed overview of the overall framework of FA-GAN is provided. Through the multimodal \nattention module provided in this article to the hierarchical iteration generator, flash attention is introduced to \nreduce the computational burden brought by the hierarchical iteration generator.\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:4525  | https://doi.org/10.1038/s41598-024-55248-6\nwww.nature.com/scientificreports/\nThe advantage of this approach is that it efficiently handles the alignment and fusion between audio and video \nto generate more realistic speech and improves the performance of the  model2.\nFlash attention GAN\nWhen considering lip-to-speech synthesis from silent video input, the emphasis is on focusing on local lip move-\nments while also considering the overall context of the video. This not only provides additional information but \nalso reduces interference from homophones based on the tone. Therefore, our research explicitly proposes an \napproach called FA-GAN, which involves modeling and training on both local visual features and global visual \ncontext, fully leveraging multimodal data. The introduction of the Swin Transformer enhances the model’s \ncapacity to extract both local and global visual information from videos, enabling a more precise capture of \nsubtle variations and features in lip movements. This enhancement provides the model with superior image \nrepresentation capabilities, enabling a more accurate interpretation and synthesis of lip movements and speech.\nAdditionally, to improve the speech generation process, we employ a hierarchical iterative generator. This \napproach enables the model to better focus on the local and overall characteristics of speech, thereby enhancing \nthe accuracy of the generated speech. This approach allows the model to improve and enhance the synthesis \nprocess at various levels, targeting different stages to enhance speech features and representations, ultimately \nenhancing the quality and authenticity of the generated speech. However, this also results in a significant increase \nin the computational burden of the model. To mitigate these effects, the generator needs to improve its capture \nof the global dependencies of the image at the middle layer through the Flash Attention module after producing \nspeech from local features. By applying the attention mechanism into the intermediate layers of the generator, \nthe model can focus more effectively on local features and global dependencies. As a result, during the speech \ngeneration process, the model can effectively focus on important parts of the image instead of globally processing \nthe entire image. This refined approach significantly reduces the use of computational resources and improves \ncomputational efficiency. The specific structure is shown in Fig. 1.\nSwin Transformer\nThe Swin Transformer is a deep learning model based on the Transformer architecture, designed for computer \nvision tasks such as image classification, object detection, and semantic segmentation. The specific framework \ndiagram is shown in Fig. 2. It divides the input image into non-overlapping image blocks called \"windows.\" These \nwindows compose a \"window partition.\" Within each window, the model performs self-attention operations to \ncapture local features within the image. Through a cascade of window partitions and self-attention operations, \nthe model gradually acquires features from different levels of the image, achieving hierarchical context. To reduce \nthe computational complexity, the Swin Transformer introduces the shifted window strategy, which restricts \nself-attention operations to take place within a specific window rather than calculating globally. This window can \nshift across different resolution levels, thus covering the entire image. Furthermore, to maintain the continuity \nof image features, the Swin Transformer introduces cross-window  connections5.\nThe introduction of the Swin Transformer has had a significant impact on the image representation within \nFA-GAN. By enhancing the extraction of local and global visual information from the video, it has empowered \nthe model with improved image representation capabilities. This enhancement is crucial for accurate lip-to-\nspeech synthesis as it enables more precise capture of subtle variations and features in lip movements, allowing \nthe model to precisely comprehend and synthesize mouth shapes and pronunciations. Lip-to-speech synthesis \nnecessitates the model’s ability to accurately capture details of lip movements and features across different stages \nto generate synthesized speech closer to real human speech. The Swin Transformer, by better capturing both \nlocal and global information within the video, has provided the model with more expressive and accurate image \nrepresentations, significantly contributing to the creation of more natural and lifelike speech synthesis effects.\nFigure 2.  (a) Architectural diagram depicting the principles of the Swin Transformer utilized in the framework. \n(b) Two successive Swin-transformer blocks: window-based multihead self-attention (W-MSA) and sliding \nwindow multi-head self-attention (SW-MSA).\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:4525  | https://doi.org/10.1038/s41598-024-55248-6\nwww.nature.com/scientificreports/\nAttention mechanisms\nHave had a significant impact in various research areas, such as facial recognition, machine translation, lip-\nreading, and speech recognition. Their main advantage lies in their ability to focus on relevant information, \neffectively reducing interference from irrelevant information. Furthermore, attention mechanisms have begun \nto be applied in fields such as generative adversarial networks (GANs). For instance, research teams led by Chan \nand  Jaitly22 introduced attention mechanisms to convert the alignment problem between acoustic features and \ntext sequences into a sequence-to-sequence problem, leading to significant performance improvements in speech \nrecognition tasks. In this study, the hierarchical iterative generator introduced improves the quality of coarse \nspeech by jointly modeling local and global visual features, leading to hierarchical audio feature generation. This \nallows the model to focus on different representations and variations of the audio phase. However, it also results \nin an increase in computational load because of the multiple interactions with the hierarchical iterative genera-\ntor during the speech synthesis process. Therefore, we propose a multimodal attention module that consists of \na global visual encoder and flash attention. This aims to decrease the significant computational load caused by \nmultiple interactions with the hierarchical iterative generator during the speech synthesis process. The basic \nprinciple is as follows: the global visual coder Gv generates the global visual feature Cv ∈ RT×D  by considering \nthe relationships among the whole local visual features V f through the Swin Transformer network. Flash attention \nthen proceeds to look for complementary cues related to speech synthesis, considering the importance of the \nglobal visual feature Cv in relation to the i-th resolution speech representation V i\na ∈ RVi×Ti×D i Simultaneously, \nflash attention simplifies the computational steps for long sequences and reduces the read/write operations to \nHBM/SRAM, which is mainly based on the following formulas:\nFigure 3 is the operational flowchart of Flash attention. Based on the size of the SRAM, the size of the row/\ncolumn modules is calculated. Then, the output matrix O = (0)N×d ∈RN×d is initialized with all zeros, which \nwill serve as an accumulator. The purpose of τ = (0)N ∈ RN is to store the cumulative denominator of the softmax \noperation (i.e., the sum of exp scores). M = (−∞)N ∈RN is used to store the maximum score row-wise. Next, \nper the specifications of Eq. (2), Q, K, and V are partitioned into blocks of the same size. Similarly, O, τ , and M \nare partitioned with the same block size. Next, a cross-column loop is initiated, which means traversing key/value \nvectors. K i and V j blocks are loaded from HBM to SRAM. However, at this point, 50% of SRAM remains unused, \nspecifically reserved for Q and O. Consequently, it enters an inner cross-row loop, which involves traversing \nquery vectors. Q i and O i blocks and τi and M i are loaded into SRAM. The dot product between Q i(Br × d) and \nthe transposed K T\nj is computed according to Eq. (5), resulting in S ij.\nNext, the scores calculated in the previous step are utilized to calculate ˜M ij , ˜τij , and ˜P ij following Eqs. (6 and \n7), and the row-wise sum of the matrix P is obtained per the equation. Then, in accordance with Eq. ( 8), M new\ni  \n(2)Bc =[ M\n4 d],Br = min([M\n4 d],d),Tr =[ N\nBr\n],Tc =[ N\nBc\n]\n(3)Q i = Q\nTr\n, Ki = K\nTc\n, V i = V\nTc\n(4)V i\nc = S(softmax( Q iK T\ni√\nd\n)V i)\n(5)Sij= Q iK T\nj ∈RBr×B c\nFigure 3.  Overview and modelling of the flash attention principle proposed in this article.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:4525  | https://doi.org/10.1038/s41598-024-55248-6\nwww.nature.com/scientificreports/\nand τnew\ni  are computed. Subsequently, according to the definition in Eq. (9), the matrix O i (where the exp term is \nused to modify ˜P ij and O i to eliminate the effect of the previous iteration’s M) is  derived9. As specified in Eq. (10).\nThe obtained global visual joint feature V i\nc is connected to the unrefined speech V i\na , which is subsequently and \niteratively improved in a hierarchical iterative generator using the following formula (Eq. (11)):\nThus, the following generator allows us to obtain cues for synthesizing accurate speech from the global visual \nenvironment in the process of obtaining phoneme mappings using local visual features. Additionally, to generate \nmore fine-grained images (e.g., Mel spectrograms), we use multiple  discriminators2. From the generated multi-\nscale Mel spectrogram ( ˆx1 , ˆx2 , ...,ˆxn ) , each generated speech representation (V 1\na,V 2\na, ...,V n\na ) after 1*1 convolution \nis passed to the multiple discriminator, as shown in Fig. 1.\nSynchronization2\nSince humans are very sensitive to audio-visual synchronization, it is important to synthesize the input lip-\nsynchronized video with synchronized speech. To generate synchronized speech, we use two strategies: 1. Speech \nis synthesized by maintaining the temporal representation of the input video. Our approach ensures that the \ngenerated speech is temporally consistent with the video to ensure audiovisual synchronization. The generator is \ndirected to focus on synchronization. As previously mentioned, each visual representation vn\nf  encoded by Pv con-\ntains lip motion information at the local level. Our goal in designing the generator is to synthesize speech based \non local visual features V f without destroying this information. In this way, the output speech can be naturally \nsynchronized by phonemic-to-phoneme mapping without disturbing its temporal information. Furthermore, \nto ensure synchronization, we employ the modern concept of deep synchronization, which learns in a self-\nsupervised manner not only audio-visual synchronization representations but also discriminative representations. \nTo achieve this goal, a local audio encoder LOC a , which encodes local audio features V a ={ v1\na,v2\na, ..., vn\na }∈ RT∗D  \nfrom the ground truth Mel spectrogram Q, is introduced. Using the encoded local audio features Va and local \nvisual features V f , we perform a comparative learning of the synchronization representation by using the fol-\nlowing InfoNCE  loss21 (Eq. (12)):\nwhere cos is the cosine similarity measure, τ is the temperature parameter used to control the scale or smooth -\nness of the probability distribution in some deep learning models, and Va and V f have the same time horizon. \nThe loss function serves to guide the allocation of high similarity to pairs that are aligned between audio repre-\nsentations and visual representations while allocating low similarity to pairs that are not aligned. We obtain the \nsynchronization loss (Eq. (13)) for both encoders:\nwhere the second term is formed by applying a symmetric approach to the negative audio samples, similar to \nEq. (12). In addition, the audio feature ˆV na encoded from the final generated Mayer spectrogram ˆxn is compared \nwith the visual representation to guide the generator in synthesizing synchronized speech. This comparison is \nperformed by means of the loss function (Eq. (14)):\nwhich maximizes the cosine similarity between the generated audio features and the given visual features, thus \nsynchronizing the generated Mel spectrogram with the input video. The final loss of synchronization is defined as:\n(6)˜M ij= rowmax (Sij) ∈RBr,˜Pij= exp(Sij− ˜M ij) ∈RBr×B c (pointwise)\n(7)˜τij= rowsum (˜Pij) ∈ RBr\n(8)M new\ni = max (M i,˜M ij) ∈RBr,τnew\ni = eM i−M new\ni τi + e˜M ij−M new\ni ˜τij∈RBr\n(9)O i ← diag(τnew\ni )−1(diag(τi)eM i−M new\ni O i+e˜M ij−M new\ni ˜PijV j)\n(10)τi ← τnew\ni ,M i ← M new\ni\n(11)V i+1\na = A i([Vi\na;V i\nc]),fori= 1, 2, ...,n − 1\n(12)E c(V a , V f) =− E[log(\nexp(cos(vj\na , vj\nf)/τ )\n∑\nn exp(cos(vj\na , vn\nf )/τ )\n)]\n(13)Ee_sync = 1\n2 (Ec(V a,V f) + Ec(V f,V a))\n(14)Eg_sync = ||1− cos( ˆV na ,V f)||1\n(15)Esync = Ee_sync + Eg_sync\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:4525  | https://doi.org/10.1038/s41598-024-55248-6\nwww.nature.com/scientificreports/\nWaveform  conversion2\nThe Mel spectrograms generated in this study have a wide range of applications in several speech-related tasks, \nincluding automatic speech recognition and audiovisual speech recognition. However, to improve the accuracy \nof speech recognition and to produce an auditory experience that resembles real speech, we also need to convert \nthe generated Mel spectrograms into sound waveforms. To accomplish this conversion task, existing vocoder \nalgorithms are usually employed. In our study, on the other hand, we chose to use the Griffin-Lim17 algorithm, \nwhich is a classical audio signal processing method that is mainly utilized to reduce linear spectrograms to their \ncorresponding audio waveforms. The main application of this algorithm is to perform audio signal inversion, \ni.e., to reduce the sound signal from spectral information. However, in this study, our model generates Mel \nspectrograms instead of linear spectrograms. Therefore, before using the Griffin-Lim algorithm, we need to \nprocess the model through an additional postprocessing network, PostNet, which is a neural network architec-\nture specifically designed to improve the quality of the generated spectrograms. PostNet further processes the \nspectrograms through operations such as convolution, residual joining, and batch normalization to improve the \nquality of the synthesized speech and the audibility. Therefore, we chose to use PostNet in this session to map \nthe generated Mel spectrograms to linear spectrograms. While training PostNet, we used the L1 reconstruction \nloss function and compared the generated results with the real linear spectrogram to ensure the accuracy of the \nconversion process. These training strategies help generate high-quality sound waveforms that match the Mel \nspectrogram, providing high-quality speech data for subsequent speech synthesis and recognition tasks. This \napproach allows us to establish meaningful mapping relationships that allow the generated sounds to perform \nwell in a variety of speech  applications2.\nLoss  function2\nTo make the final generated Meier spectrograms more realistic, we introduce a loss function while defining the \nobjective function of the FA-GAN generator part as (Eq. (16)):\nwhere /afii9838recon is the weight parameter used for reconstruction loss and /afii9838sync is the weight parameter used for \nsynchronization loss. EGAN denotes the generated loss, Erecon is the reconstruction loss, and Esync is the synchro-\nnization loss, which collectively simulate the conditional and unconditional distributions as follows (Eq. (17)):\nwhere D i denotes the ith defender. The first half denotes the unconditional GAN loss term, which is used to ensure \nthat the generated Mel spectrogram is similar to the real Mel spectrogram to improve the fidelity of the gener -\nated image. The second half represents the conditional GAN loss term, which is used to bootstrap the generated \nMel spectrogram to match the global visual context M(Cv), where M(-) represents the time-averaged pooling \noperation. This process ensures that the generated images are consistent with the global visual information. To \ncomplete the GAN training, the discriminator loss is defined as (Eq. (18)):\nThe reconstruction loss Erecon is defined in terms of the L1 distance between the generated truth spectrogram \nat resolution i and the ground truth  spectrogram2 (Eq. (19)):\nExperiment and results\nExperimental preparation\nCN-CVS6 is currently the largest and most populous, multimodal Chinese dataset that is available, with more \nthan 2500 speakers, more than 200,000 data entries, and a total duration exceeding 300 h. The dataset is primarily \ndivided into two sections—News and Speech—with data sourced from television news programs and speech-\noriented web content.In this paper, only single-speaker data are used for correction, and the model is evaluated \nunder four different environmental conditions.\nThe  GRID2 dataset comprises video and audio recordings from hundreds of speakers, showcasing their \ndiverse mouth shapes and speech patterns used to articulate words and phrases. These meticulously recorded \nand annotated data aim to explore the correlation between speech and mouth movements. Each speaker’s video \ndata includes lip movements, recorded on a word or phrase basis using multiple cameras to capture various \nperspectives of lip movements. The GRID dataset offers comprehensive multimodal annotations, encompassing \nlip positions in the video and speech transcripts in the audio.\nImplementation  details2\nFor the visual encoder part, we adopted the Swin Transformer architecture, which is currently one of the most \npopular architectures in the field of computer vision. When the input image reaches the Swin Transformer archi-\ntecture, it undergoes several steps. Patch Partition Module: the image is divided into small patches and flattened \nin the channel direction. Linear Embedding Layer: a linear transformation is applied to the channel data of each \npixel to alter the feature representation. Swin Transformer Block: feature maps are constructed through four \n(16)E = EGAN + /afii9838reconErecon+ /afii9838syncEsync\n(17)EGAN =− 1\n2Ei[logDi(ˆx) + logDi( ˆxi,M(Cv))]\n(18)Ed =− 1\n2 Ei\n[\nlogDi(xi) + log\n(\n1 − D i\n(\nˆxi\n))\n+ logDi(xi,M (Cv)) + log\n(\n1 − D\n(\nˆxi,M (Cv)\n))]\n(19)E recon = Ei[||xi − ˆxi||1]\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:4525  | https://doi.org/10.1038/s41598-024-55248-6\nwww.nature.com/scientificreports/\ndifferent stages, and Swin Transformer blocks are stacked to extract visual features. Layer normalization is applied \nto normalize the features. A global pooling operation is performed to reduce the dimension of the features. The \noutput is obtained through a fully connected layer. The audio encoder part includes two convolutional layers \nwith a stride of 2 and a residual block to extract audio features.  PostNet2 consists of three 1D residual blocks and \ntwo 1D convolutional layers to improve the quality of the generated spectrogram. The discriminator typically \nconsists of 2, 3, or 4 residual blocks to determine whether the generated spectrogram is real. In terms of data \npreprocessing, the audio is resampled to 16 kHz, is high-pass filtered, and is then converted to a Mel spectrogram \nwith 80 Mel filter banks (F  = 80). For video datasets with different frame rates, different window sizes and hop \nsizes are used to obtain matching Mel spectrograms. During training, the random sampling sizes for consecutive \nsequences are 40 and 50, while during inference, the network can handle video frames of any length to generate \nspeech. For multiscale ground truth Mel spectrograms ( x1 and x2 ), bilinear interpolation is applied to the ground \ntruth Mel spectrogram x3 . The optimizer uses Adam with a learning rate of 0.0001. α, /afii9838recon , and /afii9838sync are set to \n2, 50, and 0.5, respectively, and the temperature parameter τ is set to 1. For GAN loss, nonsaturating adversarial \nloss and R1 regularization are employed. The computations are performed using an NVIDIA 3090.\nHyper‑parameter comparative experiment\nHyper-parameters Considering the selection of hyper-parameters, we’ve introduced Extended Short-Time Objec-\ntive Intelligibility (ESTOI) as a criterion to gauge the suitability of these parameters for the model. The hyper-\nparameters included in this paper are: \" Lr(learning_rate)\", \"α\" adjusts the weights or influence of different parts \nwithin the loss function. \"T(Temperature parameter)\". Along with the loss function’s /afii9838recon and /afii9838sync.\nThis dissertation conducted comparative experiments on different parameters using an 2080ti GPU on the \nGRID dataset. The “ Lr(learning_rate)” was set between 0.0001 and 0.1, “α” ranged from 1 to 3, and the tem-\nperature parameter “T” varied between 0.05 and 1. Various numerical experiments were conducted on “α” , “ λ” , \n“T” , and “ Lr ” . According to the results in Table 1, when Lr = 0.0001, α = 2, /afii9838recon = 50, and /afii9838sync = 0.5, the model \nexhibited better performance, whereas variations in the temperature parameter “T” had minimal impact on the \nresults. Therefore, the choice of these parameters significantly affects the model’s performance.\nComparative experiment\nExperiment description\nFor our comparative experiments, we introduced five evaluation metrics: Short-TimeObjective Intelligibility \n(STOI), Extended Short-Time Objecti-ve Intelligi-\nBility (ESTOI), Perceptual Evaluation of Speech Quality (PESQ), word error rate (WER) and Mean Option \nScores (MOS).\nThese metrics collectively help us assess the performance of the proposed method on different datasets. \nAccording to the results in Table 2, we can see that our designed framework performs exceptionally well on the \nEnglish dataset GRID, especially outperforming existing deep learning architectures in terms of ESTOI and \nachieving excellent recognition rates.However, due to the complexity of the Chinese language, progress in the \nfield of Chinese lip-reading recognition has been relatively slow. Chinese includes numerous characters and \nphonemes, making its pronunciation more intricate. The presence of homophones and tones further increases \nthe difficulty of recognition. Based on the results in Table  3, we observe that our designed framework excels \non the Chinese dataset CN-CVS, with all metrics surpassing existing architectures. A particularly high level of \nrecognition accuracy is achieved in this field.\nTable 1.  The table primarily tests the impact of all hyper-parameters in this paper on the model’s performance. \nBold values represent the optimal values.\nParameters Number of parameters ESTOI\nLr(learning_rate)\n0.01 0.514\n0.001 0.607\n0.0001 0.625\nα\n1 0.624\n2 0.629\n3 0.618\nT\n1 0.629\n0.05 0.629\n/afii9838recon\n1 0.413\n20 0.574\n50 0.629\n/afii9838sync\n1 0.577\n0.5 0.629\n0.2 0.615\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:4525  | https://doi.org/10.1038/s41598-024-55248-6\nwww.nature.com/scientificreports/\nAblation experiment\nTo validate the functionality of each module in FA-GAN, we conducted ablation experiments using the CN-CVS \ndataset. The results are shown in Table 4.We progressively introduced each module, constructing four different \nmodel configurations, as shown in the table above. To measure WER, we utilized a pretrained ASR model on \nthe CN-CVS dataset under the same settings. First, we removed the Swin-Transformer module from the original \ndesign. In this configuration, STOI was 0.528, ESTOI was 0.412, PESQ was 1.623, MOS was 3.4 and WER was \n52.17%. All metrics experienced a decrease. Second, we retained the Swin-Transformer module but removed \nthe Multi Generator module. In this scenario, STOI was 0.439, ESTOI was 0.392, PESQ was 1.608, MOS was \n2.8 and WER was 58.03%. The metrics exhibited a more significant decline, which suggests that the hierarchical \niterative generator plays a critical role in speech generation. The generator not only refines the speech generation \nprocess but also significantly influences the accuracy of the generated speech. Subsequently, we retained the Swin-\nTransformer module and Multi Generator module but replaced flash attention module with cross-attention28. In \nthis configuration, STOI was 0.574, ESTOI was 0.500, PESQ was 1.683, MOS was 3.9 and WER was 48.69%. Last, \nwhen we retained all modules, the STOI was 0.614, the ESTOI was 0.580, the PESQ was 1.772, the MOS was 4.0 \nand the WER was 43.19%. This finding indicates that these three modules play a crucial role in the model, and \nthe resulting WER of 43.19% is the lowest error rate in Chinese lip-reading to speech generation.\nInference time experiment\nInference Time refers to the time needed for a deep learning model to process input data and generate output \nresults. We conducted performance tests on both the original framework and our improved framework using \nthe same CN-CVS dataset and the identical NVIDIA 3090 graphics card. As indicated in Table 5, our improved \nframework outperforms the original framework in terms of \"Inference time,\" \"ESTOI,\" and \"Word Error Rate.\"\nTable 2.  Performance of the FA-GAN framework proposed in this article on GRID. Bold values represent the \noptimal values.\nMethod STOI ESTOI PESQ WER (%) MOS\nVid2Speech11 0.491 0.335 1.734 44.92 2.8\nLip2AudSpec12 0.513 0.352 1.673 32.51 3.4\n1D GAN-based13 0.564 0.361 1.684 26.64 3.5\nLip2Wav14 0.731 0.535 1.772 14.08 4.3\nV AE-based15 0.724 0.540 1.932 – 4.2\nVocoder-based16 0.648 0.455 1.900 23.33 3.9\nVCA-GAN2 0.724 0.609 2.008 12.25 4.6\nFA-GAN 0.724 0.625 1.939 12.67 4.6\nTable 3.  Performance of the FA-GAN framework proposed in this article on CN-CVS. Bold values represent \nthe optimal values.\nMethod STOI ESTOI PESQ WER (%) MOS\nGAN-based13 0.414 0.306 1.596 63.57 2.6\nV AE-based15 0.528 0.412 1.623 52.71 3.2\nVCA-GAN2 0.598 0.471 1.604 49.70 3.4\nFA-GAN 0.614 0.580 1.772 43.19 4.0\nTable 4.  Ablation experiments of different modules in the FA-GAN framework on CN-CVS. Bold values \nrepresent the optimal values.\nModules Efficiency\nBase-line Swin-transformer Flash attention Multi generator STOI ESTOI PESQ WER (%) MOS\n√ × √ √ 0.528 0.412 1.623 52.71 3.4\n√ √ √ × 0.439 0.392 1.608 58.03 2.8\n√ √ × √ 0.574 0.500 1.683 48.69 3.9\n√ √ √ √ 0.614 0.580 1.772 43.19 4.0\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:4525  | https://doi.org/10.1038/s41598-024-55248-6\nwww.nature.com/scientificreports/\nPerformance experiment of hierarchical iterative generator\nLearning visual features in videos may simultaneously have a negative impact not only on model performance \nand but also the recognition rate. By introducing hierarchical iteration, we can gradually guide the model to \nlearn from shallow layers to deep layers, reduce the burden on the model, and improve the recognition rate. \nThe results in Table 6 show that different iteration levels will lead to a continuous increase in ESTOI, but after a \ncertain stage, ESTOI will tend to stabilize.\nResults analysis and comparisons\nBased on all the results above, our study demonstrates that the performance of the proposed FA-GAN framework \non the English dataset GRID indicates a slight lag in the STOI metric compared to the  Lip2Wav1414 model, with \na decrease of 0.007. However, in terms of the ESTOI metric, the FA-GAN model outperforms similar models by \nachieving the highest level. In contrast, in terms of the PESQ and WER metrics, it slightly lags behind the VCA-\nGAN model, with a decrease of 0.069 and 0.42%, respectively. On the Chinese dataset CN-CVS, the FA-GAN \nmodel outperforms the comparable models in all metrics, particularly the WER metric, achieving a significantly \nlower rate of 43.19% compared to other models.\nThe uniqueness of the FA-GAN model lies in its complete utilization of multimodal data. Compared to other \nmodels, it can comprehensively integrate visual and audio information, thereby enhancing the accuracy of lip-\nto-speech synthesis. The introduction of the Swin Transformer significantly enhances the model’s capacity to \nextract both local and global visual information from videos. This advantage allows the model to more accurately \ncapture subtle variations in lip movements, enhancing the quality of image representation. The FA-GAN utilizes \na hierarchical iterative generator to optimize the speech generation process, enabling the model to better focus \non the local and overall characteristics of speech, thereby enhancing the accuracy of the generated speech. In \nthe Chinese domain, the FA-GAN model still encounters several challenges, such as the increased diversity in \nlip movements and the inherent complexity of Chinese pronunciation and speech. In the field of lip-to-speech \nsynthesis, FA-GAN stands out for its unique and important contributions, including the innovative integration \nof multimodal data, the introduction of an advanced Transformer structure, and the optimization of the speech \ngeneration process. Its potential benefits in Chinese lip-to-speech synthesis position it as a significant contribu-\ntion to relevant research.\nDiscussion\nAfter analyzing the model we proposed, it lags slightly behind other existing English sentence-level lip-to-speech \nsynthesis frameworks in metrics such as WER, STOI, PESQ, etc. Therefore, there are still numerous challenges \nahead, and we will continually refine the model in our future work to achieve higher levels of performance. Future \nresearch extensions may include: (a) improving attention accuracy and efficiency for better feature capturing; (b) \nconsidering the incorporation of facial expressions to optimize speech emotions and enhance overall model per-\nformance; (c) further researching methods to enhance the model’s adaptability to various noises and environmen-\ntal changes, as well as handling variations in lip movements and pronunciations, to ensure robust performance in \ndifferent scenarios. Additionally, exploring the model’s application in multilingual environments could expand \nits practical utility. For the advancement of lip-to-speech generation technology, we should also explore the \nintegration of more advanced deep learning and natural language processing techniques to enhance the overall \nperformance and applicability of the model. These research directions will contribute to further advancements \nin the field of lip-to-speech synthesis and offer more comprehensive solutions for practical application scenarios.\nTable 5.  Comparison of performance between the original framework and the improved framework in this \narticle on CN-CVS.\nMethod Inference time (ms) ESTOI WER (%)\nVCA-GAN2 23.67 0.471 49.70\nFA-GAN 18.24 0.580 43.19\nTable 6.  Output performance of hierarchical iteration generators in different layers.\nNumber of layers ESTOI\n1 0.3\n2 0.485\n3 0.580\n4 0.510\n5 0.498\n6 0.500\n… …\n10 0.500\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:4525  | https://doi.org/10.1038/s41598-024-55248-6\nwww.nature.com/scientificreports/\nConclusion\nIn this study, we have introduced a new sentence-level lip-to-speech synthesis architecture, FA-GAN, for the first \ntime in the Chinese lip-to-speech synthesis domain. The aim is to tackle issues related to visual ambiguity, inad-\nequate extraction of image features, and insufficient refinement in the speech generation process. To address the \nissue of inadequate image feature extraction, we utilized the Swin Transformer to improve the extraction of both \nlocal and global visual information from videos. By implementing an attention mechanism, we have minimized \nvisual ambiguity, reduced computational load, and ensured that the synthesized speech aligns with the video. \nAdditionally, we have implemented a hierarchical iterative generator to improve the accuracy and naturalness \nof speech generation. Experimental results on the GRID and CN-CVS datasets demonstrated that the proposed \nFA-GAN architecture outperforms existing Chinese Mandarin sentence-level lip-to-speech synthesis frameworks \nin metrics such as STOI and ESTOI. It also outperforms current English sentence-level lip-to-speech synthesis \nframeworks in metrics such as ESOI and MOS.\nData availability\nBoth CN-CVS and GRID are publicly available and free datasets without certification.\nReceived: 1 November 2023; Accepted: 21 February 2024\nReferences\n 1. Thézé, R. et al. Animated virtual characters to explore audio-visual speech in controlled and naturalistic environments. Sci. Rep.  \n10, 15540. https:// doi. org/ 10. 1038/ s41598- 020- 72375-y (2020).\n 2. Kim, M., Hong, J. & Ro, Y . M. Lip to speech synthesis with visual context attentional GAN. NeurIPS https:// doi. org/ 10. 48550/ \narXiv. 2204. 01726 (2022).\n 3. Akinpelu, S. & Viriri, S. Speech emotion classification using attention based network and regularized feature selection. Sci. Rep.  \n13, 11990. https:// doi. org/ 10. 1038/ s41598- 0- 23- 38868-2 (2023).\n 4. Lu, Y . et al. Decoding lip language using triboelectric sensors with deep learning. Nat. Commun. 13, 1401. https:// doi. org/ 10. 1038/ \ns41467- 022- 29083-0 (2022).\n 5. Zhao, D. Z. et al. A swin transformer-based model for mosquito species identification. Sci. Rep. 12, 18664 (2022).\n 6. Chen, C., Wang, D., & Zheng, T. F . CN-CVS: A mandarin audio-visual dataset for large vocabulary continuous visual to speech \nsynthesis, in ICASSP 2023–2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 5, 10.1109 \n(2023).https:// ieeex plore. ieee. org/ docum ent/ 10095 796\n 7. Song, C., Schwarzkopf, D. & Rees, G. Variability in visual cortex size reflects tradeoff between local orientation sensitivity and \nglobal orientation modulation. Nat. Commun. 4, 2201. https:// doi. org/ 10. 1038/ ncomm s3201 (2013).\n 8. Tan, D. & Liang, X. Multiclass malaria parasite recognition based on transformer models and a generative adversarial network. \nSci. Rep. 13, 17136. https:// doi. org/ 10. 1038/ s41598- 023- 44297-y (2023).\n 9. Dao, T., Fu, D., Ermon, S., Rudra, A. & Ré, C. FlashAttention: fast and memory-efficient exact attention with IO-awareness. Adv. \nNeural Inform. Process. Syst. 35, 16344–16359. https:// doi. org/ 10. 48550/ arXiv 2205. 14135 (2022).\n 10. Ektefaie, Y . et al. Multimodal learning with graphs. Nat. Mach. Intell. 5, 340–350. https://  doi. org/ 10. 1038/ s42256- 023- 00624-6 \n(2023).\n 11. Ephrat, A., & Peleg, S. Vid2speech: Speech reconstruction from silent video, in 2017 IEEEIntemnational Conference on Acoustics, \nSpeech and Signal Processing (ICASSP) 5095–5099 (IEEE, 2017). https:// doi. org/ 10. 48550/ arXiv. 1701. 00495\n 12. Akbari, H., Arora, H., Cao, L., & Mesgarani, N. Lip2audspec: Speech reconstructionfrom silent lip movements video, in 2018 IEEE \nInternational Conference on Acoustics, Speech and SignaProcessing (ICASSP) 2516–2520 (IEEE, 2018)\n 13. Vougioukas, K., Ma, P ., Petridis, S., & Pantic, M. Video-driven speech reconstruction using generative advers arial networks (2019). \nhttps:// doi. org/ 10. 48550/ arXiv. 1906. 06301\n 14. Prajwal, K. R., Mukhopadhyay, R., Namboodiri, V . P ., & Jawahar, C. V . Learning individualspeaking styles for accurate lip to speech \nsynthesis, in Proceedings of the lEEE/CVF Conference or Computer Vision and Pattern Recognition 13796–13805 (2020).\n 15. Y adav, R., Sardana, A., Namboodiri, V . P ., & Hegde, R. M. Speech prediction in silent videos using variational autoencoders, in \nICASSP 2021–2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 7048–7052 (IEEE, 2021). \nPreprint at arXiv: 2011. 07340 v1\n 16. Michelsanti, D., Slizovskaia, O., Haro, G., Gómez, E., Tan, Z. H., & Jensen, J. Vocoder-based speech synthesis from silent videos \n(2020). Preprint at arXiv: 2004. 02541, https:// doi. org/ 10. 48550/ arXiv. 2004. 02541\n 17. He, K., Zhang, X., Ren, S., et al. Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer \nvision and pattern recognition 770–778 (2016).\n 18. Chen, C.F ., Fan, Q., Panda, R. Crossvit: Cross-attention multi-scale vision transformer for image classification, in Proceedings of \nthe IEEE/CVF International Conference on Computer Vision 357–366 (2021). Preprint at arXiv: 2103. 14899 v2\n 19. Vielzeuf, V ., Antipov, G. Are E2E ASR models ready for an industrial usage? (2021). https:// doi. org/ 10. 48550/ arXiv. 2112. 12572\n 20. Prenger, R., Valle, R. Catanzaro B Waveglow: A flow-based generative network for speech synthesis, in ICASSP 2019–2019 IEEE \nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) 3617–3621 (IEEE, 2019). https:// doi. org/ 10. 48550/ \narXiv. 1811. 00002\n 21. Oord, A. V . D., Li, Y ., & Vinyals, O. Representation learning with contrastive predictive coding. Preprint at arXiv: 1807. 03748 , \narXiv: 1807. 03748 v2 (2018)\n 22. Chan, W ., Jaitly, N., Le, Q. V ., & Vinyals, O. Listen, Attend and Spell. Preprint at arXivpreprint.16.1508.01211v2, https:// arxiv. org/ \npdf/ 1508. 01211 (2015)\n 23. Bowden, R., de Campos, T., Wang, M. Lip Reading in Profile, in Proceedings of the IEEE Conference on Computer Vision and Pattern \nRecognition (CVPR), (2013)\n 24. Dai, Y ., Chen, H., Du, J., Ding, X., Ding, N., Jiang, F ., & Lee, C. H (2023) Improving audio-visual speech recognition by lip-subword \ncorrelation based visual pre-training and cross-modal fusion encoder, in 2023 IEEE International Conference on Multimedia and \nExpo (ICME) 2627–2632 (IEEE, 2023). https:// doi. org/ 10. 48550/ arXiv. 2308. 08488.\n 25. Y eo, J. H., Kim, M., Choi, J., Kim, D. H., & Ro, Y . M. AKVSR: Audio knowledge empowered visual speech recognition by compress-\ning audio knowledge of a pretrained model (2023). https:// doi. org/ 10. 48550/ arXiv 2308. 07593.\n 26. Peymanfard, J., Saeedi, V ., Mohammadi, M. R., Zeinali, H., & Mozayani, N. Leveraging Visemes for Better Visual Speech Representa-\ntion and Lip Reading 7 (2023). https:// doi. org/ 10. 48550/ arXiv. 2307. 10157\n 27. Wang, G., Zhang, P ., Xiong, J., Y ang, F ., Huang, W ., & Zha, Y . FTFDNet: learning to detect talking face video manipulation with \ntri-modality interaction. 13. Preprint at arXiv: 2307. 03990 v1 (2023)\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:4525  | https://doi.org/10.1038/s41598-024-55248-6\nwww.nature.com/scientificreports/\n 28. Kharel, A., Paranjape, M., & Bera, A. DF-TransFusion: Multimodal deepfake detection via lip-audio cross-attention and facial \nself-attention. Preprint at https:// arxiv. org/ pdf/ 2309. 06511 (2023).\nAuthor contributions\nThese authors contributed equally to this work.\nFunding\nThis work was supported by the Shaanxi Natural Science Y outh Foundation (Grant No. 2021JQ-693).\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to Y .B.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8099616765975952
    },
    {
      "name": "Transformer",
      "score": 0.6053878664970398
    },
    {
      "name": "Speech recognition",
      "score": 0.5547263026237488
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35107529163360596
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I27599042",
      "name": "Xi'an Polytechnic University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I180662265",
      "name": "China Mobile (China)",
      "country": "CN"
    }
  ]
}