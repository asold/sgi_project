{
  "title": "Transformer- and Generative Adversarial Network–Based Inpatient Traditional Chinese Medicine Prescription Recommendation: Development Study",
  "url": "https://openalex.org/W4226020019",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2027511036",
      "name": "Hong Zhang",
      "affiliations": [
        "Guang’anmen Hospital",
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2120994304",
      "name": "Jiajun Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2608975139",
      "name": "Wandong Ni",
      "affiliations": [
        "State Administration of Traditional Chinese Medicine of the People's Republic of China"
      ]
    },
    {
      "id": "https://openalex.org/A2224523991",
      "name": "Youlin Jiang",
      "affiliations": [
        "Guang’anmen Hospital",
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2225444099",
      "name": "Kun-Jing Liu",
      "affiliations": [
        "Guang’anmen Hospital",
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2171726822",
      "name": "Daying Sun",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1958446717",
      "name": "Jing Li",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College",
        "Guang’anmen Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2027511036",
      "name": "Hong Zhang",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College",
        "Guang’anmen Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2120994304",
      "name": "Jiajun Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2608975139",
      "name": "Wandong Ni",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224523991",
      "name": "Youlin Jiang",
      "affiliations": [
        "Guang’anmen Hospital",
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2225444099",
      "name": "Kun-Jing Liu",
      "affiliations": [
        "Guang’anmen Hospital",
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2171726822",
      "name": "Daying Sun",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1958446717",
      "name": "Jing Li",
      "affiliations": [
        "Guang’anmen Hospital",
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1972978393",
    "https://openalex.org/W2915161943",
    "https://openalex.org/W3207974842",
    "https://openalex.org/W2625625371",
    "https://openalex.org/W2966351171",
    "https://openalex.org/W2610332124",
    "https://openalex.org/W2964006392",
    "https://openalex.org/W2404901863",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3010405236",
    "https://openalex.org/W2395172628",
    "https://openalex.org/W2784499877",
    "https://openalex.org/W2469314752",
    "https://openalex.org/W2518582440",
    "https://openalex.org/W3017092352",
    "https://openalex.org/W2977786340",
    "https://openalex.org/W2163922914",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2734608416",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W2776958168",
    "https://openalex.org/W1987710883",
    "https://openalex.org/W3104523752",
    "https://openalex.org/W1503398984",
    "https://openalex.org/W3098949126",
    "https://openalex.org/W3102251128"
  ],
  "abstract": "Background Traditional Chinese medicine (TCM) practitioners usually follow a 4-step evaluation process during patient diagnosis: observation, auscultation, olfaction, inquiry, pulse feeling, and palpation. The information gathered in this process, along with laboratory test results and other measurements such as vital signs, is recorded in the patient’s electronic health record (EHR). In fact, all the information needed to make a treatment plan is contained in the EHR; however, only a seasoned TCM physician could use this information well to make a good treatment plan as the reasoning process is very complicated, and it takes years of practice for a medical graduate to master the reasoning skill. In this digital medicine era, with a deluge of medical data, ever-increasing computing power, and more advanced artificial neural network models, it is not only desirable but also readily possible for a computerized system to mimic the decision-making process of a TCM physician. Objective This study aims to develop an assistive tool that can predict prescriptions for inpatients in a hospital based on patients’ clinical EHRs. Methods Clinical health records containing medical histories, as well as current symptoms and diagnosis information, were used to train a transformer-based neural network model using the corresponding physician’s prescriptions as the target. This was accomplished by extracting relevant information, such as the patient’s current illness, medicines taken, nursing care given, vital signs, examinations, and laboratory results from the patient’s EHRs. The obtained information was then sorted chronologically to produce a sequence of data for the patient. These time sequence data were then used as input to a modified transformer network, which was chosen as a prescription prediction model. The output of the model was the prescription for the patient. The ultimate goal is for this tool to generate a prescription that matches what an expert TCM physician would prescribe. To alleviate the issue of overfitting, a generative adversarial network was used to augment the training sample data set by generating noise-added samples from the original training samples. Results In total, 21,295 copies of inpatient electronic medical records from Guang’anmen Hospital were used in this study. These records were generated between January 2017 and December 2018, covering 6352 types of medicines. These medicines were sorted into 819 types of first-category medicines based on their class relationships. As shown by the test results, the performance of a fully trained transformer model can have an average precision rate of 80.58% and an average recall rate of 68.49%. Conclusions As shown by the preliminary test results, the transformer-based TCM prescription recommendation model outperformed the existing conventional methods. The extra training samples generated by the generative adversarial network help to overcome the overfitting issue, leading to further improved recall and precision rates.",
  "full_text": "Original Paper\nTransformer- and Generative Adversarial Network–Based Inpatient\nTraditional Chinese Medicine Prescription Recommendation:\nDevelopment Study\nHong Zhang1*, MSc; Jiajun Zhang2*, PhD; Wandong Ni3, PhD; Youlin Jiang1, MSc; Kunjing Liu1, BSc; Daying Sun4,\nPhD; Jing Li1, MSc\n1Guanganmen Hospital, China Academy of Chinese Medical Sciences, Beijing, China\n2School of Electronic Information Engineering, Wuxi University, Wuxi, China\n3Physician Qualification Program, Certification Center of Traditional Chinese Medicine, State Administration of Traditional Chinese Medicine, Beijing,\nChina\n4School of Electronic Engineering and Optoelectronic Technology, Nanjing University of Science and Technology, Nanjing, China\n*these authors contributed equally\nCorresponding Author:\nWandong Ni, PhD\nPhysician Qualification Program\nCertification Center of Traditional Chinese Medicine\nState Administration of Traditional Chinese Medicine\nNo 5 Beixian Ge Road\nXicheng District\nBeijing, 100053\nChina\nPhone: 86 13311127900\nEmail: 2592967878@qq.com\nAbstract\nBackground: Traditional Chinese medicine (TCM) practitioners usually follow a 4-step evaluation process during patient\ndiagnosis: observation, auscultation, olfaction, inquiry, pulse feeling, and palpation. The information gathered in this process,\nalong with laboratory test results and other measurements such as vital signs, is recorded in the patient’s electronic health record\n(EHR). In fact, all the information needed to make a treatment plan is contained in the EHR; however, only a seasoned TCM\nphysician could use this information well to make a good treatment plan as the reasoning process is very complicated, and it takes\nyears of practice for a medical graduate to master the reasoning skill. In this digital medicine era, with a deluge of medical data,\never-increasing computing power, and more advanced artificial neural network models, it is not only desirable but also readily\npossible for a computerized system to mimic the decision-making process of a TCM physician.\nObjective: This study aims to develop an assistive tool that can predict prescriptions for inpatients in a hospital based on patients’\nclinical EHRs.\nMethods: Clinical health records containing medical histories, as well as current symptoms and diagnosis information, were\nused to train a transformer-based neural network model using the corresponding physician’s prescriptions as the target. This was\naccomplished by extracting relevant information, such as the patient’s current illness, medicines taken, nursing care given, vital\nsigns, examinations, and laboratory results from the patient’s EHRs. The obtained information was then sorted chronologically\nto produce a sequence of data for the patient. These time sequence data were then used as input to a modified transformer network,\nwhich was chosen as a prescription prediction model. The output of the model was the prescription for the patient. The ultimate\ngoal is for this tool to generate a prescription that matches what an expert TCM physician would prescribe. To alleviate the issue\nof overfitting, a generative adversarial network was used to augment the training sample data set by generating noise-added\nsamples from the original training samples.\nResults: In total, 21,295 copies of inpatient electronic medical records from Guang’anmen Hospital were used in this study.\nThese records were generated between January 2017 and December 2018, covering 6352 types of medicines. These medicines\nwere sorted into 819 types of first-category medicines based on their class relationships. As shown by the test results, the\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 1https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nperformance of a fully trained transformer model can have an average precision rate of 80.58% and an average recall rate of\n68.49%.\nConclusions: As shown by the preliminary test results, the transformer-based TCM prescription recommendation model\noutperformed the existing conventional methods. The extra training samples generated by the generative adversarial network\nhelp to overcome the overfitting issue, leading to further improved recall and precision rates.\n(JMIR Med Inform 2022;10(5):e35239) doi: 10.2196/35239\nKEYWORDS\ntraditional Chinese medicine; transformer; generative adversary networks; electronic health records; artificial intelligence; natural\nlanguage processing; machine learning; word2Vec\nIntroduction\nThe widespread use of electronic health record (EHR) systems\nhas led to the explosive growth of digitized health care data. As\nthe amount and complexity of data grow, medical analysis and\ndecision-making become increasingly time-consuming and error\nprone. In reality, a human physician cannot fully use all the\navailable information at his or her disposal in a timely fashion.\nTherefore, harnessing the information contained in EHR data,\nmost of which is in textual form, is critical for driving innovation\nresearch, improving health care quality, and reducing costs.\nNatural language processing (NLP) is essential for transforming\nrelevant information sequestered in freestyle texts into structured\ndata for further computerized processing. The development of\na predictive model with EHR data was motivated by the desire\nto offer a medication-oriented decision support tool to clinical\nhealth care providers. To build such a predictive model, we used\nNLP techniques to convert a patient’s EHR data into a\nrepresentation, which then becomes the input to a deep learning\nmodel to predict medical events, such as medication orders.\nBiomedical NLP has experienced great progress in the past 30\nyears [1,2] and has become especially active in recent years [3].\nPreviously, EHR data were analyzed using traditional machine\nlearning and statistical techniques such as logistic regression,\nsupport vector machine, and random forest [4]. However, in\nrecent years, as reviewed in the studies by Shickel et al [5],\nSheikhalishahi et al [6], and Miotto et al [7], many research\nefforts have been devoted to the application of deep learning\ntechniques to EHR data for clinical informatics tasks.\nAutoencoders have been used by researchers [8] to predict a\nspecific set of diagnoses. A long short-term memory (LSTM)\nsequence model [9] was trained to provide patient-specific and\ntime-specific predictions of medication orders for patients who\nare hospitalized [10]. A convolutional neural network (CNN)\nmodel was used to predict discharge medications using the\ninformation available at admission [11]. Numerous articles were\nsurveyed in the study by Goldstein et al [12] regarding the\ndevelopment of a risk prediction model using EHR data. A\ncomprehensive study on applying deep learning techniques to\nEHR data for a variety of prediction problems was reported in\nthe study by Rajkomar et al [13]. Recurrent neural networks\nwere successfully trained using EHR data to detect medical\nevents [14-16].\nThe research on applying artificial intelligence in traditional\nChinese medicine (TCM) has been very active in the past decade\n[17,18]. Data mining techniques have been used for TCM\nsyndrome modeling and prescription recommendation for\ndiabetes [19]. The PageRank algorithm [20] was modified and\napplied to TCM prescription recommendations [21]. In our\nprevious work [17], a CNN was used to predict TCM diseases,\nand XGBoost, along with other neural networks, was used to\npredict TCM syndromes. Following the sequence-to-sequence\nparadigm, researchers from Peking University used bidirectional\ngated recurrent neural networks to generate TCM prescriptions\nfrom symptom descriptions [22]. They proposed a coverage\nmechanism along with a soft loss function as a remedy for the\nrepetition problem they encountered. However, the requirement\nof curated descriptions of symptoms as inputs hinders the\npracticality of this approach. Ideally, the model generates TCM\nprescriptions directly from raw EHR data, similar to how a\nhuman TCM physician conducts deductive reasoning.\nGenerating prescriptions from raw EHR data typically comprises\n2 parts. The first part uses biomedical NLP [3] techniques to\nextract relevant information used by a human physician to form\na feature representation [23]. The second part uses deep learning\ntechniques [7] to map this feature representation into a\nprescription order.\nThe primary task of biomedical NLP is to extract relevant\ninformation from clinical narratives written in free-form text\nand store the gathered information as structured data. Numerous\ndeep learning techniques [24-26], such as bidirectional LSTM\n(BiLSTM), have been used in the biomedical NLP field. Both\nBiLSTM conditional random field (CRF) and transformer CRF\nhave been used for named entity recognition (NER) of EHR\nnotes written in Chinese [27,28]. The recognized entities are\nthen formed into distinct tokens. Then, the feature representation\nof a patient’s EHR data becomes a sequence of tokens. The\ntokens are then converted into real-valued multidimensional\nvectors using word embedding techniques [29].\nThe purpose of this study was to develop an assistive tool that\ncan prescribe TCM prescriptions for inpatients in a hospital\nbased on the patient’s clinical EHRs. The predictive model for\nTCM prescription generation is based on a sequence-transducing\nmodel called the transformer [30]. This model is entirely based\non attention, replacing the recurrent layers most commonly used\nin encoder-decoder architectures with multihead self-attention.\nThe training used in this predictive model was supervised\ntraining with human-authored prescriptions contained in the\nEHR data set as the training targets. Furthermore, a generative\nadversarial network (GAN) [31] model was designed to augment\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 2https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nthe training set to further enhance the overall system\nperformance by reducing the effects of overfitting.\nMethods\nThis section is arranged as follows: the overall system\narchitecture is briefly described; then, each constituent\nsubsystem, which may comprise some functional blocks, is\nintroduced; finally, the training process is described in the\nTraining subsection, where a GAN model was used to generate\nnoise-added samples from the original samples.\nSystem Overview\nHospitals and medical institutes in China are rapidly moving\ntoward standardizing their EHRs to conform to the regulations\nand specifications issued by the Ministry of Health of the\nPeople’s Republic of China [32-34]. A standard EHR document\nfor a patient may contain up to 53 parts, depending on the\npatient’s situation. These may include the following:\n• A first page record containing the patient’s basic personal\ninformation, such as sex, age, occupation, and marital status\n• An admission record containing the description of a\npatient’s illness upon admission to the hospital, including\nchief complaints, medical history, and family medical\nhistory\n• A laboratory tests record containing the list of tests and the\ncorresponding results\n• A nursing record containing nurse notes of the patient’s\ncondition, treatments taken and nursing care taken, body\ntemperatures and vital signs taken, and physician’s orders\n• A treatment procedure record containing the entire\nin-hospital diagnosis and treatment process and any changes\nto the patient’s illness or illnesses\nA high-level block diagram of the proposed system is shown\nin Figure 1. The system comprises 4 subsystems: the NLP\nsubsystem, the feature extraction subsystem, the vectorization\nsubsystem, and the prescription prediction subsystem. The NLP\nsubsystem processes the EHR file and produces structured data,\nwhich in turn are processed by the feature extraction subsystem\nto extract relevant clinical information for prescription\nprediction. The vectorization subsystem maps the sequence of\ntokens written in Chinese characters to digital numbers,\npresented as a vector in a multidimensional space. The\nprescription prediction subsystem, which is a transformer-based\ndeep learning model, automatically generates a prescription\nbased on input vector data. Together, the first 3 subsystems\naccomplish the task of extracting relevant information from an\nEHR file to form input variables for the prediction model.\nSimilar representation learning operations were described in\nour previous paper [17].\nIn short, NLP normalizes the raw EHR data, the feature extractor\nconverts the normalized data into a sequence of tokens, the\nvectorization subsystem maps the tokens into vectors of real\nnumbers, and the predictive model performs the reasoning\nprocess to produce a prescription.\nFigure 1. Block diagram of the prescription generation system. EHR: electronic health record; NLP: natural language processing.\nThe NLP Subsystem\nThis subsystem is responsible for generating structured data\nfrom original EHR documents. The internal block diagram of\nthe subsystem is shown in Figure 2. There are 3 functional\nblocks in this subsystem: the preprocessing block, NER block,\nand British Medical Journal block.\nThe preprocessing block cleans the raw EHR document by\nremoving pictures and unusable components. This ensures the\ncompleteness and accuracy of the electronic medical records.\nElectronic medical records with incomplete or inconsistent\ninformation are discarded.\nAfter the initial cleaning, the content of the EHR file is then\ndivided into distinct sections. For example, the admission record\nis divided into sections of chief complaints, medical history,\nand others. Then, all the resultant sections are sorted, formatted,\nand subsequently fed to the NER block.\nOnly a small part of the EHR document is in a fixed format,\nand the remainder is in unstructured freestyle narratives. For\nfixed-format texts, a script is used to extract named entities to\nform structured data.\nFor freestyle narratives, a functional block called entity\nrecognition is used to extract named entities to form structured\ndata entries. The NER block is implemented using a BiLSTM\nnetwork with CRF (BiLSTM-CRF) [24].\nThen, the extracted named entities such as symptoms, illness,\nmedicine, examinations, and tests are further standardized\naccording to a Chinese version of the British Medical Journal\nBest Practice knowledge base.\nFigure 3 shows an example of the processing result, where the\nadmission record of a raw EHR note is converted into structured\ndata, with the marked words being named entities.\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 3https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 2. Block diagram of the named entity recognition subsystem. BiLSTM: bidirectional long short-term memory; EMR: electronic medical record;\nBiLSTM-CRF: Bidirectional long short term memory – conditional random fields; BMJ: British Medical Journal.\nFigure 3. Example of converting a freestyle narrative into structured data. EHR: electronic health record.\nThe Feature Extraction Subsystem\nTo effectively mimic the reasoning process conducted by a\nhuman physician, accurate and relevant input variables must be\nchosen properly. These variables should represent the complete\nset of factors that a human physician should take into\nconsideration when making treatment decisions. Textbox 1\nsummarizes the predominant factors that TCM experts consider\nwhen making treatment decisions.\nThe feature extraction subsystem extracts the aforementioned\nkey features from the standardized structured data to form a\nsequence of tokens. Figure 4 shows an example of this feature\nextraction, in which a sequence of tokens is generated from\nstructured data.\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 4https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTextbox 1. Text type and the content to extract.\nDemography\n• Sex, age, height, weight, and BMI\nChief complaints\n• Symptoms and signs\nRecent medical history\n• Symptoms, signs, and general information\nPast medical history\n• Past illness and medicines taken\nPresent illness\n• Tongue coating and pulses\nBody check\n• Vital signs\nTreatment process records\n• Current illness situation and treatment plan\nPhysician’s orders\n• Prescriptions\nNursing notes\n• Vital signs and medication records\nExamination reports\n• Examination items and findings\nLaboratory reports\n• Items tested and qualitative and quantitative test results\nFigure 4. Example of converting structured data into a sequence of tokens.\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 5https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nThe Vectorization Subsystem\nOverview\nUntil this point, all medical information needed to make a\ntreatment decision was encapsulated in textual data expressed\nin Chinese characters. To be used by the deep learning\nnetwork—the Transformer—the information must be mapped\ninto a digital variable. In this vectorization process, a Chinese\nword or phrase is represented as a real-valued vector in\nmultidimensional feature space. This section explains how\ntokenized features are further processed through word\nembedding.\nTraining the Word Embedding Model\nThe corpus was a collection of 102,596 electronic medical\nrecords from Guang’anmen Hospital and other hospitals. The\nJieba tokenizer was used to perform tokenization. The\nopen-source modeling tool Gensim was used to train the\nword2vec [29] model with the following major parameters:\nmin_count=2, vector_size=100, window=5, sg=1, hs=1, and\nepochs=50.\nThe Skip-Gram model was used, as indicated by the parameters.\nEach word was represented by a real-valued vector of 100\ndimensions.\nVectorization\nOnce the word embedding model is trained, each token is\nrepresented by a 100-dimension vector. For each word in the\ninput sequence, a unique identifier is assigned using a\nnumerical-type value expressed as a name-value-unit before\nanother unique identifier is assigned. Once all tokens are\nconverted into vectors, the vectors are then concatenated to form\na single vector variable, which then serves as the input to the\ntransformer.\nThe NLP, feature extraction, and vectorization subsystems\ntogether accomplish the task of feature learning by converting\nan EHR document into a multidimensional real-valued vector.\nFigure 5 shows an example of mapping from EHR text to word\nvectors.\nFigure 5. Illustration of converting electronic health record text to word vectors.\nThe Transformer Subsystem\nThe transformer subsystem is responsible for recommending a\nprescription for every given input embedding, as shown in\nFigure 6. The subsystem is described in the following\nparagraphs.\nInput embedding is a vector of max_num_tokens× vector_size\ndimensions. For example, max_num_tokens=759 and\nvector_size=100. Zero padding is used if the number of tokens\nin a sequence is smaller than max_num_tokens. Conversely, if\nthe number of tokens in a sequence is larger than\nmax_num_tokens, the number of tokens is capped at\nmax_num_tokens by dropping off tokens corresponding to the\noldest time stamp with respect to the current prescription\ngeneration time. The input embedding sample is first added to\nthe position vector of the same size, becoming the input to the\nfirst encoder.\nThe main body of the subsystem comprises 2 identical cascaded\ntransformer encoders. Unlike the encoder of the original\ntransformer [30], which comprises 6 identical layers, the encoder\nused in this research had only 1 layer with 4 sublayers. The first\nwas a multihead self-attention layer with Multi_heads=4 and\nhead_dim=8. The second was a residual layer of 100 neurons\nwith normalization. The third was a simple, position-wise, fully\nconnected feedforward network of 2048 neurons. The fourth\nwas a residual layer of 100 neurons with normalization.\nThe second encoder was followed by a linear layer, a\nfeedforward layer of 2048 neurons, a hidden layer, and an output\nlayer, as shown in Figure 6. The output layer comprised 819\nneurons with a sigmoid activation function. Each of the 819\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 6https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nneurons corresponded to an herbal ingredient. The hidden layer\ncomprised 128 neurons with a dropout mechanism and\nnormalization. The dropout rate was set to 0.4740. The purpose\nof this hidden layer was to prevent overfitting.\nThe final result from the output layer was a list of probabilities\nfor the 819 drug ingredients, valued between 0 and 1. The\nrecommended prescription was then obtained by setting a\nthreshold for these probabilities.\nFigure 6. The transformer subsystem.\nTraining\nTraining the Transformer\nTraining of the transformer is a supervised learning process.\nThe input is a real-valued vector representation of a patient’s\nEHR, and the output is the prescription. The learning goal is\nfor a machine-generated prescription to match the medical order\nprescribed by a human physician.\nAugmenting the Training Data\nTo alleviate the overfitting effect of the proposed prediction\nmodel, a GAN [31] network was used to augment the training\ndata set. Following the fundamental idea of the GAN network,\nthe generative model G is trained to represent the distribution\nof the original training data set, and the discriminative model\nD is trained to detect whether the sample originates from the\noriginal sample set or from the output of the generative model.\nDuring the training phase, the entire system looks like that\nshown in Figure 7. For every original training sample, there is\na noise-added sample. The use of a GAN in this system\neffectively doubled the number of training samples.\nThe internal structure of our GAN network was designed as\nshown in Figure 8. Generator G comprises 2 identical LSTM\nlayers, each with a size of 279. Each LSTM layer is followed\nby a normalization layer with a residual connection. The input\nto the discriminator G could be either an original word\nembedding sample or a noise-added sample generated by the\ngenerator G. The discriminator D comprises an LSTM layer\nwith a size of 279, a residual and normalization layer with a\nsize of 100, and a full connection layer with a size of 256.\nFinally, the discriminator D outputs a binary value using a\nsigmoid function.\nWe followed a typical GAN network training procedure [31]\nto train the GAN subsystem, simultaneously training the\ndiscriminator and generator. The discriminator and generator\nalternate in their training until a Nash equilibrium is reached.\nThe generator first produces a batch_size noise-added EHR,\nembedding samples with randomly initialized coefficients of\nthe generator network. These samples are concatenated with\nthe original noise-free EHR embedding samples to form\n(2×batch_size) embedding samples, each with\nmax_num_tokens×vector_size real values. For example, we can\nhave batch_size=500, max_num_tokens=560, vector_size=100.\nThese (2×batch_size) samples were used as inputs to the\ndiscriminator. For every input sample, an output label indicates\nwhether the sample is from the true original embedding or from\nthe generator. The discriminator network was trained using a\nbackpropagation algorithm with the objective of minimizing\nthe prediction error. The training of the discriminator is halted\nwhen the binary cross-entropy loss function stops decreasing.\nThe discriminator training is then temporarily halted to yield\nto the generator training.\nTo train the generator, all network coefficients of the\ndiscriminator must be frozen. The discriminator now works in\ntandem with the generator during generator training. The\ngenerator produces batch_size noise-added embedding samples,\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 7https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nand for every sample, the discriminator outputs a prediction.\nThe generator updates its parameters using a backpropagation\nalgorithm based on the discriminator output. The training of the\ngenerator is halted when the binary cross-entropy loss function\nstops increasing. The generator training is then temporarily\nhalted to yield the discriminator training.\nThe aforementioned discriminator and generator training\nprocesses together form 1 training epoch. The entire GAN\nnetwork training is accomplished through several epochs. The\ntraining stops when a Nash equilibrium is reached.\nThe entire training process is illustrated using the Python\npseudocode included in Multimedia Appendix 1.\nFigure 7. Block diagram of the predictive modeling system during the training phase. EHR: electronic health record; GAN: generative adversarial\nnetwork; NLP: natural language processing.\nFigure 8. The internal structure of the generative adversarial network subsystem. LSTM: long short-term memory; *size of the neural network used\nin that layer.\nEthics Approval\nThis study received institutional review board review through\nGuanganmen Hospital Ethic Committee (SQ2017YFGX\n060073).\nResults\nData Set\nEHRs generated in Guang’anmen Hospital between January 1,\n2017, and December 31, 2018, were used as the data set in this\nstudy. Initially, there were 27,846 copies of EHR notes, out of\nwhich 6551 (23.53%) copies were discarded because of quality\ncontrol. An EHR note should be discarded if it satisfies one of\nthe following conditions:\n• The note is incomplete for missing certain basic pages.\n• The note contains inconsistent information.\n• The note does not use standard descriptions.\n• The note contains special EHR circumstances such as\nchemotherapy, after an operation, and removal of fracture\nsettings.\nEvaluation Metrics\nThe data set contained 6352 drug varieties. A complete TCM\nprescription includes drug ingredients, dosages, and decoction\npreparation instructions. It is still very challenging, if not\nimpossible, for a machine to generate such a complete TCM\nprescription. At our current stage of research, we focus only on\nthe drug ingredients of a prescription.\nJudging whether the 2 TCM prescriptions are the same is often\nnot straightforward, given the distinctive nature of TCM [35].\nOften, 2 different herbs may have the same medical effect. When\na TCM physician prescribes a medication order, he or she often\nhas multiple choices at hand for herbal ingredients. As a result,\nthe 2 TCM physicians may prescribe different herbs for the\nsame patient with the same diagnosed condition. Therefore, it\nis necessary to have a unified method of evaluating\nmachine-generated prescriptions. To this end, we need a higher\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 8https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nlevel of abstraction. Figure 9 shows an example of the\norganization of TCM drugs. In this example, 2 TCM drugs\n(antiphlogistic powder and Jingfang decoction) have different\nherbal ingredients but belong to the same parent drug category\nand have the same medical treatment effect. In our research, we\nconcluded that the recommended drug should be considered a\ncorrect recommendation as long as the recommended drug\nbelongs to the same parent category as that of the\nhuman-authored prescription.\nTo quantitatively evaluate the performance of the\ntransformer-based deep learning model, we compared the\nprescription generated by the machine with that prescribed by\na human physician. Here, we used the metrics of precision rate\nand recall rate, which we based on 3 variables. True positive\n(TP) is defined as the number of drugs that exist in the\nphysician’s prescription and also exist in the machine’s\nprescription. False positive (FP) is the number of drugs that do\nnot exist in the physician’s prescription but exist in the\nmachine’s prescription. False negative (FN) is defined as the\nnumber of drugs that exist in the physician’s prescription but\nnot in the machine’s prescription. With these definitions, we\ndefined the precision and recall rates as follows:\nPrecision rate = TP / (TP + FP) (1)\nRecall rate = TP / (TP + FN) (2)\nFigure 9. Classification of herbal drugs.\nHyperparameter Tuning With GridSearchCV\nThe data set was divided into training and test sets, with the\ntraining set comprising 90% of the data set and the test set\ncomprising the rest. The model was trained using a 10-fold\ncross-validation method; that is, the training set was randomly\nsplit into 10 folds, with the model being trained 10 times. During\neach of the 10 training times, the hyperparameters were tuned\nusing the GridSearchCV method. Each training resulted in a set\nof hyperparameters, with the ultimate hyperparameters being\nthe average of these 10 sets of parameters.\nThe values of the hyperparameters of the transformer network\nmodel have a great influence on the accuracy of the model. The\noptimal values of these parameters were determined through\niterations using the grid search method. The sparse characters\nof each type were embedded into a d-dimensional embedding\nlayer. Then, all vectors were combined using a new method:\nvectors of the same type and time were averaged using the\nweights of self-learning.\nThe model was optimized using a minimal log loss. Many\nregularization methods were used, such as the vector loss rate\nand the embedded layer loss rate. In addition, small-scale L2\nweight punishment was used, which increased the punishment\nfor large weights. The training batch size was chosen as 128,\nplacing sentences with similar sizes into the same batch. Each\nbatch contained approximately 12,000 words. Finally, the\nmultilabel task was processed using an Adam function. For\nmultilabel tasks, the input with the last time stamp was\nmultiplied with the special end of sequence embedding. The\ntraining was executed using the Kears framework on a server\nwith 8 NVIDIA P100 graphics processing unit. The fine-tuned\nhyperparameters along with their respective ranges are shown\nin Table 1.\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 9https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 1. Some hyperparameters of the model.\nParameter rangeValuesHyperparameters\n(0.1, 0.5, 1.0, 1.1)0.1245Gradient\n(4, 8)4Attention heads\n(0.25, 0.35, 0.5)0.4410Vector loss rate\n(0.25, 0.35, 0.5)0.4740Hidden layer loss rate\n(0, 1)0.4375Learning rate\n(0, 0.01)0.000001566L2 punishment rate\nExperimental Results\nTo intuitively explain our experimental results, we start with a\nconcrete example that illustrates how EHR notes lead to\nprescription orders. An example of this is shown in Figure 10.\nThe left side shows a snapshot of the patient’s EHR. On the\nright side is a table showing a side-by-side comparison between\na human-authored order and the prescription generated by our\nmodel. The physician’s order contains 12 ingredients, whereas\nthe model’s order has 11. The first 5 ingredients are identical\non both sides. The sixth ingredient from each side is the same,\nalthough they have different Chinese names. This is because\nthe physician used a nickname for the herb. The remaining\ningredients differ not only in name but also in substance.\nHowever, these 2 orders are still considered equivalent so far\nas the medical treatment effect is concerned. This is because in\nTCM terminology, a diagnosis must conclude with the name\nof the disease (illness) and a list of syndromes [17]. In this\nparticular case, the diagnosed disease is emaciation-thirst, with\nthe primary syndrome being kidney and liver deficiency and the\nsecondary syndrome being dampness and stasis. The first 6\nherbal ingredients target the primary syndrome. The remaining\ningredients in each prescription are for the treatment of the\nsecondary syndrome called dampness and stasis. As these 2\norders are only slightly different in their ingredients for treating\nsecondary syndrome, they are treated as the same prescription\nin our research.\nTo further explain this prescription comparison, we present\nanother picture, as shown in Figure 11. The physician’s order\nis called Qiju Dihuang pill, and the model’s order is called\nLiuwei Dihuang pill. They are category II prescriptions that\nbelong to the same parent category TCM prescription called\nnourishing liver and kidney. They differ only in how to dispel\ndampness and resolve phlegm to address only the secondary\nsyndrome.\nTo evaluate the performance of the transformer-based predictive\nmodel, we first conducted model training using only the original\nsamples, purposefully excluding the noise-added samples. The\nresults are described in the following paragraphs.\nOn the basis of the time sequences, the system produced\nprescription recommendations at admission, 24 hours after\nadmission, 48 hours after admission, 3 days after admission,\nand 1 week after admission. The test results are shown in Table\n2.\nFrom Table 2, we first observe that the precision and recall rates\nobtained from the training data set are higher than their\nrespective counterparts from the test data set. This is\nunderstandable as the model has seen the samples from the\ntraining data set before but not from the test data set. The second\nobservation is that as time progresses, both the precision and\nrecall rates improve. After admission, at each subsequent\nmedication order time, more relevant information is collected,\nand the prediction becomes more accurate. Although the number\nof feature tokens was <260 for 98% of the patients at the time\nof admission, this number increased to 296 in 24 hours, 333 in\n48 hours, 366 in 72 hours, and 759 in 7 days. In our experiment,\nwe set max_num_tokens=759. This means that when the number\nof feature tokens was <759, zero padding was used, and clipping\nwas used when there were >759 feature tokens. Selecting the\nproper value for max_num_tokens is important for balancing\nthe trade-off between overall system performance and\ncomputational efficiency. If the value is too large, training and\ninferencing will consume too much computation horsepower.\nIf the value is too small, then some critical information gathered\nat admission will be lost because of clipping, leading to reduced\nprecision and recall rates for prescription predictions at a time\nthat is far from the admission time (eg, 2 weeks after admission).\nThe second set of experimental results was obtained using more\ntraining samples to train the predictive model. The size of the\ntraining data set was doubled, as for every training sample, a\nnoise-added sample was generated by the GAN network. The\nprecision and recall rates are listed in Table 3.\nAs can be seen in Table 3, both the precision and recall rates\nconsistently improved by a noticeable margin. The results\nconvincingly prove that inserting noise-added training samples\ngenerated by the GAN module can effectively overcome the\noverfitting issue, leading to better prediction performance.\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 10https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 10. Side-by-side comparison of physician’s order versus model’s order.\nFigure 11. Prescription comparison: physician’s order versus model’s order.\nTable 2. The precision rates and recall rates with transformer only.\nTest setTraining setTime\nPrecision rate (%)Precision rate (%)Recall rate (%)Precision rate (%)\n61.2573.8269.4981.58Admission\n62.6974.5671.8883.37In 24 hours\n63.0474.8171.2683.92In 48 hours\n65.3876.2473.8985.16In 3 days\n67.1577.9475.1787.02In 1 week\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 11https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 3. The precision rates and recall rates with transformer+generative adversarial network.\nTest setTraining setTime\nRecall rate (%)Precision rate (%)Recall rate (%)Precision rate (%)\n68.4980.5870.6582.22Admission\n70.882.3772.1884.15In 24 hours\n70.2682.9272.5684.32In 48 hours\n74.3885.0475.1087.04In 3 days\n76.2386.8276.7988.91In 1 week\nComparison Study\nTo compare the performance of our proposed model with that\nof existing prescription generation models, we implemented 3\nother models. The CNN-based model [11] comprises a word\nembedding layer, a convolution layer that contains 3 filters of\ndifferent sizes, a pooling layer, and a full connection layer. The\noutput layer contains 819 neurons, equal to the number of\nprescribed herb varieties. The seq2seq [36] model comprises a\nCNN encoder and an LSTM decoder. The MedAR [37] model\ncomprises a word embedding layer, followed by an attention\nlayer, and finally, a RethinkNet layer to complete the multilabel\nclassification. The learning rate was 0.001, the dropout rate was\n0.8, and the optimization function was Adam. The final output\nlayer used the sigmoid function, where all other layers used the\nnon-linear activation function ReLU, which outputs an input x\nas zero if x is negative, and outputs x itself if x is larger than or\nequal to zero. Table 4 shows the respective precision and recall\nrates at admission for all 4 models in discussion. The results\nsuggest that the proposed model has superior performance in\nterms of precision and recall rates.\nTable 4. Performance comparison for different models.\nRecall rate (%)Precision rate (%)Model\n31.0047.54Convolutional neural network\n48.7464.02Seq2seqa\n53.0871.46MedARb\n68.4980.58Transformer+generative adversarial network\naSeq2seq: sequence to sequence model.\nbMedAR: Medical data attention Rethink Net.\nDiscussion\nPrincipal Findings\nThe following tasks have been finished in this research:\n1. Deep learning NLP techniques were used to convert raw\nChinese EHR texts into feature representations.\n2. The major contribution of this study is the proposal of a\ntransformer-based predictive modeling scheme for\nmedication order generation from a feature representation\nof EHR data.\n3. The secondary contribution of this study is the use of GAN\nto augment the training data set, leading to a noticeable\nperformance improvement of the predictive model. Using\nthe GAN, noise-added samples were generated to double\nthe number of original training samples. This helped\nalleviate the overfitting problem, making the model more\nrobust in terms of generalization.\nLimitations\nDespite the efforts made in many aspects of the diagnosis and\ntreatment scheme recommendations, there is still much room\nfor improvement. The training data set is still relatively small,\nand there may be some frequently used medicines that are not\nincluded in the training data set. The TCM prescription\nknowledge base is still incomplete. Some medicines do not have\nstandard names, and no corresponding parent medicine name\nexists in the database. Therefore, the recommended medicine\nnames are still the original hospital medicine names. For a\nmultilabel prediction task, an increased number of labels will\nincrease the difficulty of the model prediction and lower the\nprediction accuracy. Therefore, as a more complete knowledge\nbase is developed, the label set will be further optimized, leading\nto a greater prediction accuracy of the model.\nFuture Work\nThis paper reports the preliminary research results of automated\nmedication order generation from EHR texts for TCM inpatients\nwho are hospitalized. The recommended medicines include\nWestern and Chinese medicines. For Chinese medicines, only\nthe medicine names are recommended. In the future, the dosage\nof the herbal ingredients, as well as the medicine preparation\ninstructions, will be included in the recommendations.\nImproving the model prediction accuracy to the level of category\nII is also a direction for future work. Future work could expand\nthe training data set to optimize the model.\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 12https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nPython pseudocode for the training of generative adversarial network with Keras Framework.\n[DOCX File , 14 KB-Multimedia Appendix 1]\nReferences\n1. Friedman C, Rindflesch TC, Corn M. Natural language processing: state of the art and prospects for significant progress,\na workshop sponsored by the National Library of Medicine. J Biomed Inform 2013 Oct;46(5):765-773 [FREE Full text]\n[doi: 10.1016/j.jbi.2013.06.004] [Medline: 23810857]\n2. Hasan SA, Farri O. Clinical natural language processing with deep learning. In: Consoli S, Reforgiato Recupero D, Petković\nM, editors. Data Science for Healthcare: Methodologies and Applications. Cham, Switzerland: Springer; 2019:147-171.\n3. Houssein EH, Mohamed RE, Ali AA. Machine learning techniques for biomedical natural language processing: a\ncomprehensive review. IEEE Access 2021 Oct 13;9:140628-140653. [doi: 10.1109/access.2021.3119621]\n4. Murphy KP. Machine Learning: A Probabilistic Perspective. Cambridge, MA, USA: MIT Press; 2012.\n5. Shickel B, Tighe PJ, Bihorac A, Rashidi P. Deep EHR: a survey of recent advances in deep learning techniques for electronic\nhealth record (EHR) analysis. IEEE J Biomed Health Inform 2018 Sep;22(5):1589-1604 [FREE Full text] [doi:\n10.1109/JBHI.2017.2767063] [Medline: 29989977]\n6. Sheikhalishahi S, Miotto R, Dudley JT, Lavelli A, Rinaldi F, Osmani V. Natural language processing of clinical notes on\nchronic diseases: systematic review. JMIR Med Inform 2019 Apr 27;7(2):e12239 [FREE Full text] [doi: 10.2196/12239]\n[Medline: 31066697]\n7. Miotto R, Wang F, Wang S, Jiang X, Dudley JT. Deep learning for healthcare: review, opportunities and challenges. Brief\nBioinform 2018 Nov 27;19(6):1236-1246 [FREE Full text] [doi: 10.1093/bib/bbx044] [Medline: 28481991]\n8. Miotto R, Li L, Kidd BA, Dudley JT. Deep patient: an unsupervised representation to predict the future of patients from\nthe electronic health records. Sci Rep 2016 May 17;6:26094 [FREE Full text] [doi: 10.1038/srep26094] [Medline: 27185194]\n9. Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput 1997 Nov 15;9(8):1735-1780. [doi:\n10.1162/neco.1997.9.8.1735] [Medline: 9377276]\n10. Rough K, Dai AM, Zhang K, Xue Y, Vardoulakis LM, Cui C, et al. Predicting inpatient medication orders from electronic\nhealth record data. Clin Pharmacol Ther 2020 Jul;108(1):145-154 [FREE Full text] [doi: 10.1002/cpt.1826] [Medline:\n32141068]\n11. Yang Y, Xie P, Gao X, Cheng C, Li C, Zhang H, et al. Predicting discharge medications at admission time based on deep\nlearning. arXiv (forthcoming) 2017 [FREE Full text]\n12. Goldstein BA, Navar AM, Pencina MJ, Ioannidis JP. Opportunities and challenges in developing risk prediction models\nwith electronic health records data: a systematic review. J Am Med Inform Assoc 2017 Jan;24(1):198-208 [FREE Full text]\n[doi: 10.1093/jamia/ocw042] [Medline: 27189013]\n13. Rajkomar A, Oren E, Chen K, Dai AM, Hajaj N, Hardt M, et al. Scalable and accurate deep learning with electronic health\nrecords. NPJ Digit Med 2018 May;1:18 [FREE Full text] [doi: 10.1038/s41746-018-0029-1] [Medline: 31304302]\n14. Jagannatha AN, Yu H. Bidirectional RNN for medical event detection in electronic health records. Proc Conf 2016\nJun;2016:473-482 [FREE Full text] [doi: 10.18653/v1/n16-1056] [Medline: 27885364]\n15. Choi E, Bahadori MT, Schuetz A, Stewart W, Sun J. Doctor AI: predicting clinical events via recurrent neural networks.\nJMLR Workshop Conf Proc 2016 Aug;56:301-318 [FREE Full text] [Medline: 28286600]\n16. Choi E, Schuetz A, Stewart WF, Sun J. Using recurrent neural network models for early detection of heart failure onset. J\nAm Med Inform Assoc 2017 Mar 01;24(2):361-370 [FREE Full text] [doi: 10.1093/jamia/ocw112] [Medline: 27521897]\n17. Zhang H, Ni W, Li J, Zhang J. Artificial intelligence-based traditional Chinese medicine assistive diagnostic system:\nvalidation study. JMIR Med Inform 2020 Jun 15;8(6):e17608 [FREE Full text] [doi: 10.2196/17608] [Medline: 32538797]\n18. Zhikui C, Xin S, Jing G, Jianing Z, Peng L. Research progress in data mining-based TCM diagnoses. Chinese J Traditional\nChinese Med 2020;38(12):1-9.\n19. Guo Y, Ma J. Data mining technique in application of syndromes and prescriptions of Traditional Chinese medicine for\ndiabetes (in Chinese). Med & Pharm J Chin PLA 2015;27:34-38.\n20. Page L, Brin S, Motwani R, Winograd T. The pagerank citation ranking: bringing order to the Web. In: Proceedings of the\n7th International Conference on World Wide Web. 1998 Presented at: WWW7 '98; 1998; Brisbane, Australia p. 161-172.\n21. Zhang Y, Hu H, Yang T, Xie J, Shen G. Prescription recommendation algorithm of traditional Chinese medicine treatment\nof lung cancer based on complex network. Lishizhen Med and Materia Medica Res 2019;5:1257-1260 [FREE Full text]\n22. Li W, Yang Z, Sun X. Exploration on generating Traditional Chinese medicine prescription from symptoms with an\nend-to-end method. In: Proceedings of the 8th CCF International Conference on Natural Language Processing and Chinese\nComputing. 2019 Presented at: NLPCC '19; October 9–14, 2019; Dunhuang, China p. 486-498. [doi:\n10.1007/978-3-030-32233-5_38]\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 13https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n23. Bengio Y, Courville A, Vincent P. Representation learning: a review and new perspectives. IEEE Trans Pattern Anal Mach\nIntell 2013 Aug;35(8):1798-1828. [doi: 10.1109/TPAMI.2013.50] [Medline: 23787338]\n24. Huang Z, Xu W, Yu K. Bidirectional LSTM-CRF models for sequence tagging. arXiv 2015 Aug 09 [FREE Full text] [doi:\n10.48550/arXiv.1508.01991]\n25. Lample G, Ballesteros M, Subramanian S, Kawakami K, Dyer C. Neural architectures for named entity recognition. In:\nProceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies. 2016 Presented at: NAACL '16; June 12-17, 2016; San Diego, CA, USA p. 260-270. [doi:\n10.18653/v1/n16-1030]\n26. Habibi M, Weber L, Neves M, Wiegandt DL, Leser U. Deep learning with word embeddings improves biomedical named\nentity recognition. Bioinformatics 2017 Jul 15;33(14):i37-i48 [FREE Full text] [doi: 10.1093/bioinformatics/btx228]\n[Medline: 28881963]\n27. Gang L, Rongqing P, Jin M, Yujie C. Entity recognition of Chinese electronic medical records based on BiLSTM-CRF\nnetwork and dictionary resources. J Modern Inf 2020;40(4):3-12.\n28. Li B, Kang X, Zhang H, Wang Y, Chen Y, Bai F. Named entity recognition in Chinese electronic medical records using\ntransformer-CRF (in Chinese). Computer Engineering and Applications 2020;2020(56):153-159.\n29. Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. In: Proceedings of\nthe 2013 International Conference on Learning Representations. 2013 Presented at: ICLR '13; May 2-4, 2013; Scottsdale,\nAZ, USA.\n30. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. arXiv 2017 Jun 12\n[FREE Full text] [doi: 10.48550/arXiv.1706.03762]\n31. Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, et al. Generative adversarial networks. In:\nProceedings of the 27th Advances in Neural Information Processing Systems. 2014 Presented at: NeurIPS '14; December\n8-13, 2014; Montreal, Canada. [doi: 10.1145/3422622]\n32. Zhang H, Ni W, Li J, Jiang Y, Liu K, Ma Z. On standardization of basic datasets of electronic medical records in traditional\nChinese medicine. Comput Methods Programs Biomed 2019 Jun;174:65-70. [doi: 10.1016/j.cmpb.2017.12.024] [Medline:\n29292098]\n33. Specification for drafting of health information basic dataset (WS 445-2014)S. Ministry of Health of the People’s Republic\nof China. 2014. URL: https://wenku.baidu.com/view/4c17892d760bf78a6529647d27284b73f342365b.html [accessed\n2022-05-04]\n34. Specification for sharing documents of electronic medical record - part 1: medical record summary (WS/t 5001-2016)S.\nMinistry of Health of the People’s Republic of China. 2016. URL: https://ishare.iask.sina.com.cn/f/rIcgxKYJb9.html\n[accessed 2022-04-21]\n35. Cheung F. TCM: made in China. Nature 2011 Dec 21;480(7378):S82-S83. [doi: 10.1038/480S82a] [Medline: 22190085]\n36. Xiong H. Research on the recommendation method of traditional Chinese medicine dynamic diagnosis and treatment plan\nbased on real-world clinical data. Beijing Jiaotong University. 2020. URL: https://www.docin.com/p-2607021734.html\n[accessed 2022-05-04]\n37. Xiaolu G. Application of deep learning in electronic health records data. Xiamen University. 2019. URL: https://kns.cnki.net/\nkcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202002&filename=1019064889.nh&uniplatform=NZKPT&v=\nJdsETSDw9TG7mLWRMKxEtYFZWzSE1ntYAQYDF6L2Z3Tl3ccV-citYLrD1g30mRob [accessed 2022-04-21]\nAbbreviations\nBiLSTM: bidirectional long short-term memory\nCNN: convolutional neural network\nCRF: conditional random field\nEHR: electronic health record\nGAN: generative adversarial network\nLSTM: long short-term memory\nNER: named entity recognition\nNLP: natural language processing\nTCM: traditional Chinese medicine\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 14https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nEdited by C Lovis; submitted 27.11.21; peer-reviewed by SD Boie; comments to author 08.01.22; revised version received 05.03.22;\naccepted 11.04.22; published 31.05.22\nPlease cite as:\nZhang H, Zhang J, Ni W, Jiang Y, Liu K, Sun D, Li J\nTransformer- and Generative Adversarial Network–Based Inpatient Traditional Chinese Medicine Prescription Recommendation:\nDevelopment Study\nJMIR Med Inform 2022;10(5):e35239\nURL: https://medinform.jmir.org/2022/5/e35239/\ndoi: 10.2196/35239\nPMID:\n©Hong Zhang, Jiajun Zhang, Wandong Ni, Youlin Jiang, Kunjing Liu, Daying Sun, Jing Li. Originally published in JMIR Medical\nInformatics (https://medinform.jmir.org), 31.05.2022. This is an open-access article distributed under the terms of the Creative\nCommons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided the original work, first published in JMIR Medical Informatics, is properly cited. The\ncomplete bibliographic information, a link to the original publication on https://medinform.jmir.org/, as well as this copyright\nand license information must be included.\nJMIR Med Inform 2022 | vol. 10 | iss. 5 | e35239 | p. 15https://medinform.jmir.org/2022/5/e35239/\n(page number not for citation purposes)\nZhang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX",
  "topic": "Medical prescription",
  "concepts": [
    {
      "name": "Medical prescription",
      "score": 0.622027575969696
    },
    {
      "name": "Medicine",
      "score": 0.550774872303009
    },
    {
      "name": "Auscultation",
      "score": 0.4909534752368927
    },
    {
      "name": "Vital signs",
      "score": 0.48833176493644714
    },
    {
      "name": "Medical emergency",
      "score": 0.4698459506034851
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34099894762039185
    },
    {
      "name": "Computer science",
      "score": 0.333815336227417
    },
    {
      "name": "Nursing",
      "score": 0.18610098958015442
    },
    {
      "name": "Radiology",
      "score": 0.0
    },
    {
      "name": "Surgery",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200296433",
      "name": "Chinese Academy of Medical Sciences & Peking Union Medical College",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210146387",
      "name": "Guang’anmen Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2799435774",
      "name": "State Administration of Traditional Chinese Medicine of the People's Republic of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    }
  ]
}