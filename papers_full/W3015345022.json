{
  "title": "Downstream Model Design of Pre-trained Language Model for Relation Extraction Task",
  "url": "https://openalex.org/W3015345022",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1944106018",
      "name": "Li Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2024071360",
      "name": "Tian Ye",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2515462165",
    "https://openalex.org/W2949212908",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2107598941",
    "https://openalex.org/W2251622960",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2739874095",
    "https://openalex.org/W2798734500",
    "https://openalex.org/W2892094955",
    "https://openalex.org/W2949602493",
    "https://openalex.org/W2577666659",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2892316911",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W3034617555",
    "https://openalex.org/W2250521169",
    "https://openalex.org/W143521272",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1604644367",
    "https://openalex.org/W2975326399",
    "https://openalex.org/W2946345909",
    "https://openalex.org/W2099779943",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2948131150",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W2962753370"
  ],
  "abstract": "Supervised relation extraction methods based on deep neural network play an important role in the recent information extraction field. However, at present, their performance still fails to reach a good level due to the existence of complicated relations. On the other hand, recently proposed pre-trained language models (PLMs) have achieved great success in multiple tasks of natural language processing through fine-tuning when combined with the model of downstream tasks. However, original standard tasks of PLM do not include the relation extraction task yet. We believe that PLMs can also be used to solve the relation extraction problem, but it is necessary to establish a specially designed downstream task model or even loss function for dealing with complicated relations. In this paper, a new network architecture with a special loss function is designed to serve as a downstream model of PLMs for supervised relation extraction. Experiments have shown that our method significantly exceeded the current optimal baseline models across multiple public datasets of relation extraction.",
  "full_text": "Downstream Model Design of Pre-trained\nLanguage Model for Relation Extraction Task\nCheng Li, Ye Tian\nAI Application Research Center, Huawei Technologies, Shenzhen, China\nlicheng81@huawei.com\nAbstract\nSupervised relation extraction methods based on deep neural network\nplay an important role in the recent information extraction ﬁeld. However, at\npresent, their performance still fails to reach a good level due to the existence\nof complicated relations. On the other hand, recently proposed pre-trained\nlanguage models (PLMs) have achieved great success in multiple tasks of\nnatural language processing through ﬁne-tuning when combined with the\nmodel of downstream tasks. However, original standard tasks of PLM do not\ninclude the relation extraction task yet. We believe that PLMs can also be\nused to solve the relation extraction problem, but it is necessary to establish a\nspecially designed downstream task model or even loss function for dealing\nwith complicated relations. In this paper, a new network architecture with a\nspecial loss function is designed to serve as a downstream model of PLMs\nfor supervised relation extraction. Experiments have shown that our method\nsigniﬁcantly exceeded the current optimal baseline models across multiple\npublic datasets of relation extraction.\n1 Introduction\nAs a subtask of information extraction, the predeﬁned relation extraction task plays\nan important role in building structured knowledge. In recent years, with the rapid\ndevelopment of deep learning, many relation extraction methods based on deep\nneural network structure have been proposed. At present, these popular neural\nnetwork methods [15, 29, 19, 14, 9, 6, 2, 25] can be mainly summarized by the\nfollowing steps:\n1. Obtain embeddings of the target text from an encoder. The embeddings\nare usually dense vectors in a low dimensional space [ 17]. For example, Glove\nvector [20], or vectors from a pre-trained language model like BERT [1].\n2. Process these embeddings and integrate their information according to a\ncertain network structure, such as CNN [ 19], Attention Mechanism [ 14, 6] and\nGCN [2] and so on, to obtain the representation of the target relation.\n3. Perform training on labeled dataset based on a certain classiﬁer using the\nencoded information as input, such as the Softmax classiﬁer.\n1\narXiv:2004.03786v1  [cs.CL]  8 Apr 2020\nHowever, the current methods do not perform very well on public datasets\ngiven the existence of complicated relations, such as long-distance relation, single\nsentence with multiple relations, and overlapped relations on entity-pairs.\nRecently, the emergence of pre-trained language models has provided new\nideas for solving relation extraction problems. Pre-trained language models [ 1,\n21, 28, 22] are a kind of super-large-scale neural network model based on the\ndeep Transformer [ 26] structure. Their initial parameters are learned through\nsuper-large self-supervised training, and then combined with multiple downstream\nmodels to ﬁx special tasks by ﬁne-tuning. Experiments show that the downstream\ntasks’ performances of this kind of models are usually far superior to those of\nconventional models [16]. Unfortunately, the relation extraction task is not in the\noriginal downstream tasks list directly supported by the pre-trained language model.\nOur current work attempts to leverage the power of the PLMs to establish an\neffective downstream model that is competent for the relation extraction tasks. In\nparticular, we implement three important improvements in the main steps, as\ndescribed above:\n1. Use a pre-trained language model (this article uses BERT [ 1]) instead of\nthe traditional encoder, and obtain a variety of token embeddings from the model.\nWe extract the embeddings from two different layers to represent the head and tail\nentities separately, because it may help to learn reversible relations. On the other\nhand, we also add the context information into the entity to deal with long-distance\nrelations.\n2. After then, we calculate a parameterized asymmetric kernel inner prod-\nuct matrix between all the head and tail embeddings of each token in a sequence.\nSince kernels are different between each relation, we believe such a product is\nhelpful for distinguishing multiple relations between same entity pairs. Thus the\nmatrix can be treated as the tendency score to indicate where a certain relation\nexists.\n3. Use the Sigmoid classiﬁer instead of the Softmax classiﬁer , and use the\naverage probability of token pairs corresponding to each entity pair as the ﬁnal\nprobability that the entity pair has a certain relation. Thus for each type of relation,\nand each entity pair in the input text, we can independently calculate the probability\nof whether a relation exists. This mechanism allows the model to predict multiple\nrelations even overlapped on same entities.\nWe also notice that it is very easy to integrate Named Entity Recognition (NER)\ntask into our model to deal with joint extraction task. For instance, adding Bi-LSTM\nand CRF [8] after the BERT encoding layer makes it easy to build an efﬁcient NER\ntask processor. By simply adding the NER loss on original loss, we can build a joint\nextraction model to avoid the pipeline errors. However, in this paper we will focus\non relation extraction and will not pay attention to this method.\nOur experiments mainly verify two conclusions.\n1. Pre-trained language model can perform well in relation extraction task after\nour re-designing. We evaluate the method on 3 public datasets: SemEval 2010\nTask 8 [7], NYT [23], WebNLG [3]. The experimental results show that our model\n2\nachieves a new state-of-the-art.\n2. A test for our performance in different overlapped relation situations and\nmultiple relation situations shows that our model is robust when faced with complex\nrelations.\n2 Related Work\nIn recent years, numerous models have been proposed to process the supervised\nrelation extraction task with deep neural networks. The general paradigm is to\ncreate a deep neural network model by learning from texts labeled with entities\ninformation and the relation-type-label between them. The model then can predict\nthe possible relation-type-label for the speciﬁed entities in a new text. We introduce\nthree kinds of common neural network structures here.\n2.1 CNN-based methods\nCNN based methods apply Convolutional Neural Network [ 13] to capture text\ninformation and reconstruct embeddings [ 15]. Besides simple CNN, a series of\nimproved models have been proposed [ 15, 29, 19, 14, 9]. However, the CNN\napproach limits the model’s ability to handle remote relations. The information that\nis fused through the CNN network is often local, so it is difﬁcult to deal with distant\nrelations. Therefore, these methods are currently limited to a large extent and are\nnot able to achieve a good level of application. Since more efﬁcient methods are\nproposed recently, we will not compare our approach with this early work in this\narticle.\n2.2 GNN-based methods\nGNN based methods use Graph Neural Network [24], mainly Graph Convolutional\nNetwork [12] to deal with entities’ relations. GNN is a neural network that can cap-\nture the topological characteristics of the graph type data, so it is necessary to deﬁne\na prior graph structure. The GNN-based relation extraction methods [31, 4, 2] usu-\nally use the text dependency tree as an input prior graph structure, thereby obtaining\nricher information representation than CNN. However, this kind of methods relies\non the dependency parser thus pipeline errors exist. In addition, grammar-level\ndependency tree is still a shallow representation which fails to efﬁciently express\nrelations between words.\n2.3 Methods with Pre-trained Language Model\nSome recent approaches consider relation extraction as a downstream task for PLMs.\nThese methods [32, 25, 10, 27] have made some success, but we believe that they\nhave not yet fully utilized the language model. The main reason is the lack of a\nvalid representation of the relation - those methods tend to express the relation as\n3\na one-dimensional vector. We believe that since the relation is determined by the\nvectors’ correlations of head and tail entities, it should naturally be represented as a\nmatrix rather than one-dimensional vector. In this way, more information like the\norder of the entities and their positions in the text will be used while predicting their\nrelations.\nFigure 1: Our method’s network architecture. F is an asymmetric kernel inner\nproduct function of embeddings at each position. Thus we get a product matrix of\nl ×l (l = 7in this ﬁgure) for each relation type. We further use Sigmoid activation\nfunction to scale each element and get probability matrix P. For each relation type,\nthe average value of the elements that correspond to the entity-pair can be treated as\nthe ﬁnal predicted score for the possible triplet ⟨relation?head : tail⟩.\n3 Methodology\nWe introduce the method from two aspects: network structure and loss function.\nFigure 1 shows the overall architecture of this method.\nFrom the perspective of the network structure, our model has two major parts.\nThe ﬁrst part is an encoder that utilizes pre-trained language model like BERT.\nWe obtain three embeddings for a given input text from the BERT model: the\nembedding Ew for each token in the text , the embeddings Ep obtained by passing\nEw through a self-attention Transformer, and the embedding Ea of the entire text\n(that is, the CLS embedding provided by BERT). The second part is the relation\ncomputing layer. In this layer, we assume that the Ep represents the tail entity\nencoded with some available predicate information, while Ea combined with Ew\nrepresents the head entity with context information. By performing a correlation\ncalculation F on those embeddings, the tendency scores matrix Si of relation i in\nall entity pairs can be obtained.\n4\nFrom the perspective of loss function, we ﬁrst use the Sigmoid activation\nfunction to compute probabilites Pi of relation i by using Si. We use locations\nof entities to construct a mask matrix M and use it to preserve information in Pi\nwhich represents existing entity-pairs in a sentence (See details in Section 3.3 and\nFigure 1). Based on the labels indicating whether each entity-pair is an instance\nof the i-th relation type or not, we use the average values in each area of Pi to\ncompute a Binary Cross Entropy (BCE) loss of this speciﬁc relation. Eventually, the\nﬁnal loss sums all values from all relations. This formulation allows the model to\npredict multiple relations in a single sentence or even multiple relations for a single\nentity-pair. Details and formulas are described in subsections below.\n3.1 Encoder\nThe current pre-training language models are basically based on the Transformer\nstructure [ 26] . They have a common feature: each layer of the Transformer\ncan output a hidden vector corresponding to the input text T. T is an array of\ntokens with length l. Taking BERT as an example, some simple analyses [ 1] show\nthat the hidden vector of each layer can be used as word embeddings of T, with\nmodest difference in precisions. Generally speaking, the deepest hidden vector\nrepresentation of the Transformer network tends to work best for a downstream ﬁne-\ntuning task thanks to the information integration performed by a deeper network.\nHowever, here we select the penultimate layer output vector as the initial embedding\nEw (Ew ∈ Rl×h, where h is the number of hidden dimensions), for the text\nrepresentation with entity information.\nTo get Ep, we use the last Transformer layer of BERT which is actually a\nmulti-head self-attention with a fully connected FFN [26] to deal with our initial\nembeddings:\nEp = Transformer (Ew), (1)\nwhere Ep is the last output vector of BERT,Ep ∈Rl×h.\nSuch an operation is applied so that the embedding of every token inEp will, in\naddition to Ew, fuse some information from tokens in other positions. In this way,\nAlthough dataset annotations usually do not carry explicit predicate information, the\nTransformer structure of the BERT model allows Ep to selectively blend contextual\ninformation that is helpful for the ﬁnal task. We expect that after well ﬁne-tuning\ntraining, words with higher attention association scores correspond to a predicate of\na certain relation to some extent. Ew and Ep can be respectively used as basic entity\nrepresentations and entity representations that incorporate predicate information. In\norder to better capture the overall context information, the BERT’s CLS embedding\nEa(Ea ∈Rh) is also added to each token’s embedding to improve the basic entity\nrepresentation:\nEb = Ew + Ea. (2)\nNote Ea is actually broadcasting to all tokens.\n5\n3.2 Relation Computing Layer\nWe apply an asymmetric kernel inner product method to calculate the similarity\nbetween Eb and Ep:\nSi = Fi(Eb, Ep), (3)\nwhere\nFi(X, Y ) =XW hi ·(Y Wti)T . (4)\nHere Si ∈Rl×l; Whi, Wti ∈Rh×h.\nActually, Whi and Wti are respectively the transformation matrices of head-\nentity and tail-entity embeddings in i-th relation. They are the parameters learned\nduring the training process for each relation.\nIf there are N tokens in one input text, we ﬁnd thatSi is actually a square matrix\nwith N rows and columns. Thus it can be treated as unnormalized probability scores\nfor i-th relation between all the tokens. That is to say, Si mn, an element of position\n(m, n), represents the existence possibility of i-th relation between tokens at these\ntwo locations. Finally we use Sigmoid functions to normalize Si to range (0, 1):\nPi = 1\n1 +e−Si\n, (5)\nwhere Pi is the normalized probability matrix of i-th relation.\n3.3 Loss Calculation\nA problem of Pi is that it describes relations between tokens, not entities. Therefore,\nwe use entity-mask matrix to ﬁx this problem. For each entity pair, the location\ninformation of the entities is known. Suppose that all entities from input text T\nconstitute a set of entity pairs in the form:\nS = {(x, y)}.\nSuppose (Bx, Ex) is the beginning and end of the position index of an entity x in\nthe token array. Therefore, we construct a mask matrix M(M ∈Rl×l) to satisfy\n∀(x, y) ∈S, Mmn =\n{1, Bx ≤m ≤Ex ∧By ≤n ≤Ey\n0, otherwise (6)\nwhere m, nis the subscript of the matrix element. Similarly, we can construct a\nlabel matrix Yi(Yi ∈Rl×l) for the i-th relation:\n∀(x, y) ∈Yi, Yimn =\n{1, Bx ≤m ≤Ex ∧By ≤n ≤Ey\n0, otherwise (7)\nwhere Yi is the labeled i-th relation set of entity pairs from the input text T. We\nuse this mask matrix to reserve the predicted probabilities of every entity pair from\n6\nHyper-parameters SemEval NYT WebNLG\nBatch size 64 20 20\nLearning Rate 3 ×10−5 5 ×10−5 3 ×10−5\nMaximum Training Epochs 50 10 30\nMaximum Sequence Length 512 100 512\nTable 1: Hyper-parameters used for training on each dataset.\nSituations SemEval NYT WebNLG\nNormal 10695 33566 12391\nEPO 0 30775 121\nSEO 0 13927 19059\nSingle 10673 33287 12237\nDouble 22 24174 9502\nMultiple 0 12249 9772\nAll 10695 69710 31511\nTable 2: Statistics of different types of sample sentences (No repetition) in multiple\ndatasets. Note a sample sentence may belong to both EPO and SEO.\nPi, and then use the average Binary Cross Entropy to calculate the target loss Li of\nrelation i:\nLi = BCEavg(Pi ∗M, Y i) (8)\nwhere ∗is Hadamard product and\nBCEavg(X, Y )\n=\n∑\nmn,∀Y mn=1\nlog(Xmn) + ∑\nmn,∀Y mn=0\nlog(1 −Xmn)\n∑\nmn,∀Mmn=1\nY mn\n(9)\nThus the ﬁnal loss Lr of relation predication is\nLr =\n∑\ni\nLi (10)\nwhere i is the index of each relation. While predicting, we use the average value\nof elements in Pi, whose location accords with a certain entity-pair (x, y), as the\nprobability of the possible triplet ⟨i?x : y⟩consisting of i-th relation and entity-pair\n(x, y).\n4 Experiments\nThis section describes the experimental process and best results while testing our\nmethods on multiple public datasets. We performed overall comparison experiments\n7\nMethods SemEval NYT WebNLG\nC-AGGCN [4] 85.7 – –\nGraphRel2p [2] – 61.9 42.9\nBERTEM-MTB [25] 89.5 – –\nHBT [27] – 87.5 88.8\nours 91.0 89.8 96.3\nTable 3: Micro-F1 scores of our method tested on multiple datasets, compared with\nother four baseline methods. The ﬁrst two methods are GNN-based while the last\ntwo are PLM-based. Their performances come from their original papers, as quoted\nabove.\nSituations SemEval NYT WebNLG\nP R F1 P R F1 P R F1\nNormal 94.2 88.0 91.0 95.1 94.0 94.5 96.2 92.9 94.5\nEPO - - - 96.3 73.2 83.2 100.0 90.3 94.9\nSEO - - - 92.2 78.9 85.0 97.2 96.3 96.8\nSingle 94.2 88.0 91.0 95.1 94.0 94.6 96.1 92.7 94.4\nDouble 83.3 83.3 83.3 89.6 80.2 84.7 96.9 96.5 96.7\nMultiple - - - 95.8 75.3 84.3 97.3 96.3 96.8\nAll 94.2 88.0 91.0 94.2 85.7 89.8 97.0 95.7 96.3\nTable 4: Precision, Recall and Micro-F1 scores of our model tested on different\ntypes of relations in multiple datasets. Note for some relation types the score is not\navailable (denoted as “-”) because there is no such type in the dataset (see Table 2).\n8\nwith the baseline methods and completed more ﬁne-grained analysis and comparison\nin different types of complex relations. Codes and more details can be found in\nSupplementary Materials.\n4.1 Experimental Settings\nWe use Nvidia Tesla V100 32GB for training. The BERT model we use is\n[BERT-Base, Uncased]. Hyper-parameters are shown in Table 1. The optimizer is\nAdam [11]. Based on our problem formulation as described in Section 3, our model\nactually ﬁts a binary classiﬁer to predict whether a triplet exists or not. Therefore, it\nactually gives a probability for each possible triplet. We still need a threshold to\ndivide the positive and negative classes, and we set it as0.5 for balance. More details\nare shown in Supplementary Materials. Our codes are developed on OpenNRE [5].\n4.2 Baseline and Evaluation Metrics\nAs described above, Pre-trained Language Model (PLM) is so powerful that it may\nlead to unfairness in the comparison between our method and some old methods.\nTherefore, we chose some recent work (C-AGGCN [4], GraphRel2p [2], BERTEM-\nMTB [ 25], HBT [ 27]) published after the appearance of PLMs, especially\nBERT, as our baseline. Such a selection is useful for measuring whether we have\nbetter exploited the potential of PLM in relational extraction tasks, rather than only\nrelied on its power. As usual, we used the micro-F1 score as evaluation criteria.\n4.3 Datasets\nWe performed our experiments on three commonly used public datasets (SemEval\n2010 Task 8 [7], NYT [23], WebNLG [3]) and compared the performance of our\nmethod to the baseline methods mentioned above. We followed splits and special\nprocess for the datasets conducted by the previous baseline models. More details\nare in Supplementary Materials.\nWe found that the complexity of the samples in these data sets varied widely.\nSimilar to the approach of Copy re [ 30], we measured the complexity of the\nrelations in the three datasets from two dimensions, i.e., the number of samples\nwith overlapping relations and the number of samples with multiple relations.\nFor overlapping relations, we followed the method proposed in Copy re [30] to\ndivide samples into three types: “Normal” as all relations in the sample is normal;\n“EPO” as there are at least two relations overlapped in the same entity-pair in the\nsample; “SEO” as there are at least two relations sharing a single entity in the\nsample. These three types can reﬂect the complexity of relations. For multiple\nrelations, we also divide samples into three types: “Single” as only one relation\nappears in the sample, while “Double” as two and “Multiple” as no less than three.\nThese three types can reﬂect the complexity of samples.\nTable 2 shows the complexity analysis of each dataset.\n9\n4.4 Results and Analysis\nTable 3 shows the performance of our method on all test data sets and the compar-\nisons with the corresponding baseline methods. Given different test settings, we\nﬁnd that our model generally outperformed the baseline models, with a margin\nover the optimal baseline ranging from 1% to 8%.\nFigure 2: Micro-F1 scores of Normal, Entity Pair Overlapped (EPO) and Single\nEntity Overlapped (SEO). Our methods are tested on NYT and WebNLG, with\ncomparison of GraphRel and HBT.\nTo explain the difference in performance margin, we design a detailed experi-\nment to evaluate how our model performs in each relation type as shown in Table 4.\nOn the other hand, we also conducted the comparison regarding each relation type\nwith 3 baseline methods (GraphRel1p [2], GraphRel2p [2], HBT [27]) in Figure 2\nand Figure 3. We only compared F1 scores on NYT and WebNLG, since nearly no\ncomplex relations exist in SemEval. Detailed analyses on each dataset are listed as\nbelow.\nSemEval. SemEval only has around 20 samples with two relations (“Others”\nclass excluded), with only 6 of them in test dataset. Thus in Double-relation type,\nour model’s performance crashes down about8% because of large variance.\nNYT. On NYT our method has experienced large ﬂuctuations. The reason is\nthat NYT is the only dataset constructed by distant supervision [ 18], so the data\nquality is low. Distant supervision methods, which automatically label data from\nknowledge graphs, bring more errors in complex relations than simple relations.\nTherefore, although it seems that the amount of complex relations is sufﬁciently\nlarge, the performance of our model on complex relations still lags behind that\non simple relations (around 10% lower in F1 score, around 17% lower in Recall).\nNonetheless, comparison results still show that our metric scores while dealing with\nsimple relations are higher than both baselines (around 7% higher than HBT and\n25% higher than GraphRel). Even for complex relations, we are still signiﬁcantly\nbetter (around 30%) than GraphRel, but a little bit lower (around 3%) than HBT.\nWebNLG. It is easy to ﬁnd our performances (Precision, Recall and Micro-F1\nscore) keep stable no matter how complicated the type is, except for EPO. It is\n10\nFigure 3: Micro-F1 scores of x-relations in one sample sentence. (Here x =\n1, 2, 3, 4 or x ≥5.) Our methods are tested on NYT and WebNLG, with comparison\nof GraphRel and HBT.\nbecause there are only around 100 samples in EPO thus the model’s performances\nare of high variance. Interestingly, the Micro-F1 scores of complex relations are\neven higher than simple relations around 2%, which is further discussed below.\nComparison results show our model is far better than baselines (around 8% higher\nthan HBT and 50% higher than GraphRel).\nGiven the results from all the datasets, our method shows consistent high perfor-\nmance on simple relation extraction tasks. Furthermore, it generally demonstrates\nstable performance when faced with more challenging settings including over-\nlapped relation and multiple relation extraction.\nAnother interesting phenomenon is, on WebNLG, our model does better (around\n1.5% higher on F1 score, 3% higher on Recall) while dealing with complicated re-\nlations than simple relations. Our guess is that since our model can predict multiple\nrelations at the same time, it may combine semantic correlations between mul-\ntiple relations to ﬁnd more annotated relations by preventing some semantic\ndrift. On the other hand, on NYT, where semantic correlations between multiple\nrelations generated by distant supervision are very likely to be fake, our model tends\nto neglect those meaningless semantic correlations. Therefore, it ﬁlters out potential\nfalsely labeled relations and generate lower Recall.\nTo support the above reasoning, Table 5 illustrates some real examples from\nWebNLG and NYT dataset. On WebNLG, our examples demonstrate the beneﬁcial\neffects from properly labeled complex relations on our model. In the simple sample,\n11\nNYT WebNLG\nComplex Simple Complex Simple\nText\nErnst\nHaeﬂiger...died\non Saturday in\nDavos,\nSwitzerland,\nwhere he\nmaintained a\nsecond home.\nGeorgia\nPowers...said\nLouisville was\nﬁnally ready to\nwelcome\nMuhammad Ali\nhome.\nThe 1 Decembrie\n1918 University\nis located in Alba\nIulia, Romania.\nThe capital of the\ncountry is\nBucharest...\nThe Germans of\nRomania are one\nof the ethnic\ngroups in\nRomania...the 1\nDecembrie 1918\nUniversity is\nlocated in the city\nof Alba Iulia.\nLabels\n(place of\nbirth:Ernst\nHaeﬂiger, Davos),\n(place of\ndeath:Ernst\nHaeﬂiger, Davos)\n(place of\nbirth:Muhammad\nAli, Louisville)\n(is country\nof:Romania, Alba\nIulia),\n(is capital\nof:Bucharest,\nRomania)\n(is country\nof:Romania, Alba\nIulia)\nPredictions\n(place of\ndeath:Ernst\nHaeﬂiger, Davos)\n(place of\nbirth:Muhammad\nAli, Louisville)\n(is country\nof:Romania, Alba\nIulia),\n(is capital\nof:Bucharest,\nRomania)\n(ethnic groups\nin:Romania, Alba\nIulia)\nTable 5: Examples from NYT and WebNLG to compare inﬂuences of semantic\ncorrelations while predicting simple and complex relations. Red triplets are fake\nrelations.\nour model made a mistake to consider Romania as an ethnic group in Alba Iulia,\nwhile in the complex sample from WebNLG, the model made the correct prediction\nthat Romania is the country of Alba Iulia, by successfully identifying Bucharest as\nthe capital of Romania. In comparison, for the simple sample from NYT, the model\npredicted “place of birth” correctly, while failed to predict it in the complex sample,\nsince this relation is not real and also has no semantic correlations with the correct\nrelation “place of death”.\n5 Conclusion\nThis paper introduces a downstream network architecture of pre-trained language\nmodel to process supervised relation extraction. The network calculates the relation\nscore matrix of all entity pairs on all relation types by extracting the different head\nand tail entities’ embeddings from the pre-trained language model. Experiments\nhave shown that it has achieved signiﬁcant improvements across multiple public\ndatasets when compared to current best practices. Moreover, further experiments\ndemonstrate the ability of this method to deal with complex relations. Also, we\n12\nbelieve this network will not conﬂict with many other methods, thus it can be\ncombined with them (e.g., use other special PLMs like ERNIE [ 32], BERTEM-\nMTB [25]) and performs better.\nIn addition, we believe that the current architecture has the potential to be\nimproved for dealing with many other relation problems, including applications in\nlong-tail relation extraction, open relation extraction, and joint extraction and so on.\nReferences\n[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.\nBert: Pre-training of deep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\n[2] Tsu-Jui Fu, Peng-Hsuan Li, and Wei-Yun Ma. 2019. Graphrel: Modeling text\nas relational graphs for joint entity and relation extraction. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics,\npages 1409–1418.\n[3] Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-\nBeltrachini. 2017. Creating training corpora for nlg micro-planning.\n[4] Zhijiang Guo, Yan Zhang, and Wei Lu. 2019. Attention guided graph convolu-\ntional networks for relation extraction. arXiv preprint arXiv:1906.07510.\n[5] Xu Han, Tianyu Gao, Yuan Yao, Demin Ye, Zhiyuan Liu, and Maosong Sun.\n2019. Opennre: An open and extensible toolkit for neural relation extraction.\narXiv preprint arXiv:1909.13078.\n[6] Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, and Peng Li. 2018. Hierarchi-\ncal relation extraction with coarse-to-ﬁne grained attention. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natural Language Processing,\npages 2236–2245.\n[7] Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid\n´O S´eaghdha, Sebastian Pad ´o, Marco Pennacchiotti, Lorenza Romano, and\nStan Szpakowicz. 2009. Semeval-2010 task 8: Multi-way classiﬁcation of\nsemantic relations between pairs of nominals. In Proceedings of the Workshop\non Semantic Evaluations: Recent Achievements and Future Directions, pages\n94–99. Association for Computational Linguistics.\n[8] Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for\nsequence tagging. arXiv preprint arXiv:1508.01991.\n[9] Xiaotian Jiang, Quan Wang, Peng Li, and Bin Wang. 2016. Relation extraction\nwith multi-instance multi-label convolutional neural networks. In Proceed-\nings of COLING 2016, the 26th International Conference on Computational\nLinguistics: Technical Papers, pages 1471–1480.\n13\n[10] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer,\nand Omer Levy. 2019. Spanbert: Improving pre-training by representing and\npredicting spans. arXiv preprint arXiv:1907.10529.\n[11] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980.\n[12] Thomas N Kipf and Max Welling. 2016. Semi-supervised classiﬁcation with\ngraph convolutional networks. arXiv preprint arXiv:1609.02907.\n[13] Yann LeCun, L ´eon Bottou, Yoshua Bengio, Patrick Haffner, et al. 1998.\nGradient-based learning applied to document recognition. Proceedings of\nthe IEEE, 86(11):2278–2324.\n[14] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2016.\nNeural relation extraction with selective attention over instances. In Pro-\nceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2124–2133.\n[15] ChunYang Liu, WenBo Sun, WenHan Chao, and Wanxiang Che. 2013. Con-\nvolution neural network for relation extraction. In International Conference\non Advanced Data Mining and Applications, pages 231–242. Springer.\n[16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,\nOmer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692.\n[17] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.\n2013. Distributed representations of words and phrases and their compo-\nsitionality. In Advances in neural information processing systems, pages\n3111–3119.\n[18] Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant super-\nvision for relation extraction without labeled data. In Proceedings of the Joint\nConference of the 47th Annual Meeting of the ACL and the 4th International\nJoint Conference on Natural Language Processing of the AFNLP: Volume\n2-Volume 2, pages 1003–1011. Association for Computational Linguistics.\n[19] Thien Huu Nguyen and Ralph Grishman. 2015. Relation extraction: Perspec-\ntive from convolutional neural networks. In Proceedings of the 1st Workshop\non Vector Space Modeling for Natural Language Processing, pages 39–48.\n[20] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of the 2014 conference\non empirical methods in natural language processing (EMNLP), pages 1532–\n1543.\n14\n[21] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and\nIlya Sutskever. 2019. Language models are unsupervised multitask learners.\nOpenAI Blog, 1(8).\n[22] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text transformer.arXiv preprint\narXiv:1910.10683.\n[23] Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling rela-\ntions and their mentions without labeled text. In Joint European Conference\non Machine Learning and Knowledge Discovery in Databases, pages 148–163.\nSpringer.\n[24] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and\nGabriele Monfardini. 2008. The graph neural network model. IEEE Transac-\ntions on Neural Networks, 20(1):61–80.\n[25] Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom\nKwiatkowski. 2019. Matching the blanks: Distributional similarity for relation\nlearning. arXiv preprint arXiv:1906.03158.\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in neural information processing systems, pages\n5998–6008.\n[27] Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and Yi Chang. 2019. A novel\nhierarchical binary tagging framework for joint extraction of entities and\nrelations. arXiv preprint arXiv:1909.03227.\n[28] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdi-\nnov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for\nlanguage understanding. arXiv preprint arXiv:1906.08237.\n[29] Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, Jun Zhao, et al. 2014.\nRelation classiﬁcation via convolutional deep neural network.\n[30] Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu, Jun Zhao, et al. 2018.\nExtracting relational facts by an end-to-end neural model with copy mecha-\nnism.\n[31] Yuhao Zhang, Peng Qi, and Christopher D Manning. 2018. Graph convolution\nover pruned dependency trees improves relation extraction. arXiv preprint\narXiv:1809.10185.\n[32] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun\nLiu. 2019. Ernie: Enhanced language representation with informative entities.\narXiv preprint arXiv:1905.07129.\n15",
  "topic": "Downstream (manufacturing)",
  "concepts": [
    {
      "name": "Downstream (manufacturing)",
      "score": 0.7298427820205688
    },
    {
      "name": "Task (project management)",
      "score": 0.6754565238952637
    },
    {
      "name": "Relation (database)",
      "score": 0.6112276315689087
    },
    {
      "name": "Computer science",
      "score": 0.587963879108429
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.5866259932518005
    },
    {
      "name": "Relationship extraction",
      "score": 0.46995165944099426
    },
    {
      "name": "Natural language processing",
      "score": 0.3885459899902344
    },
    {
      "name": "Engineering",
      "score": 0.18339502811431885
    },
    {
      "name": "Data mining",
      "score": 0.12511873245239258
    },
    {
      "name": "Operations management",
      "score": 0.0895490050315857
    },
    {
      "name": "Systems engineering",
      "score": 0.06394949555397034
    },
    {
      "name": "Chemistry",
      "score": 0.05730995535850525
    },
    {
      "name": "Chromatography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250955327",
      "name": "Huawei Technologies (China)",
      "country": "CN"
    }
  ]
}