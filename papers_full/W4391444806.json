{
  "title": "DLM-DTI: a dual language model for the prediction of drug-target interaction with hint-based learning",
  "url": "https://openalex.org/W4391444806",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2106466016",
      "name": "Jong-Hyun Lee",
      "affiliations": [
        "Hanyang University",
        "Anyang University"
      ]
    },
    {
      "id": "https://openalex.org/A2020686978",
      "name": "Dae Won Jun",
      "affiliations": [
        "Hanyang University",
        "Anyang University"
      ]
    },
    {
      "id": "https://openalex.org/A2432928833",
      "name": "Ildae Song",
      "affiliations": [
        "Kyungsung University"
      ]
    },
    {
      "id": "https://openalex.org/A2097070245",
      "name": "Yun Kim",
      "affiliations": [
        "Catholic University of Daegu"
      ]
    },
    {
      "id": "https://openalex.org/A2106466016",
      "name": "Jong-Hyun Lee",
      "affiliations": [
        "Anyang University"
      ]
    },
    {
      "id": "https://openalex.org/A2020686978",
      "name": "Dae Won Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2432928833",
      "name": "Ildae Song",
      "affiliations": [
        "Kyungsung University"
      ]
    },
    {
      "id": "https://openalex.org/A2097070245",
      "name": "Yun Kim",
      "affiliations": [
        "Catholic University of Daegu"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2558461040",
    "https://openalex.org/W2068325418",
    "https://openalex.org/W4287503054",
    "https://openalex.org/W3123655913",
    "https://openalex.org/W2062941476",
    "https://openalex.org/W1985270532",
    "https://openalex.org/W1527782499",
    "https://openalex.org/W2134967712",
    "https://openalex.org/W2474546929",
    "https://openalex.org/W2109991441",
    "https://openalex.org/W2605952223",
    "https://openalex.org/W2785947426",
    "https://openalex.org/W2899788782",
    "https://openalex.org/W4210842338",
    "https://openalex.org/W3152586663",
    "https://openalex.org/W3096561213",
    "https://openalex.org/W4220872781",
    "https://openalex.org/W4300717171",
    "https://openalex.org/W4205478771",
    "https://openalex.org/W4307432689",
    "https://openalex.org/W3201477029",
    "https://openalex.org/W3000369508",
    "https://openalex.org/W4361218849",
    "https://openalex.org/W3191452947",
    "https://openalex.org/W4308187210",
    "https://openalex.org/W4318391592",
    "https://openalex.org/W4323653718",
    "https://openalex.org/W3018980093",
    "https://openalex.org/W6725181839",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W3040739508",
    "https://openalex.org/W4220853970",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W4292265045",
    "https://openalex.org/W3034368386",
    "https://openalex.org/W4296780589",
    "https://openalex.org/W6608093658",
    "https://openalex.org/W2086286404",
    "https://openalex.org/W2096864392",
    "https://openalex.org/W1966716734",
    "https://openalex.org/W6602634393",
    "https://openalex.org/W4387819650",
    "https://openalex.org/W6826116265",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W3199258042",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W4362720950",
    "https://openalex.org/W3207161423",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W3138154797"
  ],
  "abstract": null,
  "full_text": "Lee et al. Journal of Cheminformatics           (2024) 16:14  \nhttps://doi.org/10.1186/s13321-024-00808-1\nRESEARCH\nDLM-DTI: a dual language model \nfor the prediction of drug-target interaction \nwith hint-based learning\nJonghyun Lee1, Dae Won Jun1,2, Ildae Song3 and Yun Kim4* \nAbstract \nThe drug discovery process is demanding and time-consuming, and machine learning-based research is increasingly \nproposed to enhance efficiency. A significant challenge in this field is predicting whether a drug molecule’s structure \nwill interact with a target protein. A recent study attempted to address this challenge by utilizing an encoder that lev-\nerages prior knowledge of molecular and protein structures, resulting in notable improvements in the prediction per-\nformance of the drug-target interactions task. Nonetheless, the target encoders employed in previous studies exhibit \ncomputational complexity that increases quadratically with the input length, thereby limiting their practical utility. To \novercome this challenge, we adopt a hint-based learning strategy to develop a compact and efficient target encoder. \nWith the adaptation parameter, our model can blend general knowledge and target-oriented knowledge to build \nfeatures of the protein sequences. This approach yielded considerable performance enhancements and improved \nlearning efficiency on three benchmark datasets: BIOSNAP , DAVIS, and Binding DB. Furthermore, our methodology \nboasts the merit of necessitating only a minimal Video RAM (VRAM) allocation, specifically 7.7GB, during the train-\ning phase (16.24% of the previous state-of-the-art model). This ensures the feasibility of training and inference even \nwith constrained computational resources.\nKeywords Drug-target interactions, Pre-trained language model, Knowledge adaptation, Lightweight framework\nIntroduction\nThe process of drug discovery is often compared to find -\ning a needle in a haystack, requiring substantial funds and \nlabor forces. Unfortunately, most newly discovered drugs \nfail to obtain approval for clinical use due to unexpected \nadverse drug reactions, insufficient drug effects, and low \nbinding affinity [1–5]. Artificial intelligence has emerged \nas a promising tool for reducing expenses in various \nfields of drug discovery, including the predictions of \ndrug toxicity, drug-drug interaction, and molecule prop -\nerties, among others. In the first step of drug discovery, \nwhich involves drug repurposing and/or repositioning, it \nis critical to identify candidates of druggable molecules \nthat target a specific protein. In this context, drug-target \ninteraction (DTI) prediction tasks have emerged as a cru-\ncial area of research.\nPrevious studies on DTI prediction can be broadly \ncategorized into three categories: simulation-based \nmolecular docking, structural similarity, and deep neu -\nral network (DNN) approach. Molecular docking simu -\nlation utilized 3D structures of proteins and molecules \nOpen Access\n© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nJournal of Cheminformatics\n*Correspondence:\nYun Kim\nykim71@cu.ac.kr\n1 Department of Medical and Digital Engineering, Hanyang University \nCollege of Engineering, 222, Wangsimni-ro, Seongdong-gu, Seoul 04763, \nKorea\n2 Department of Internal Medicine, Hanyang University College \nof Medicine, 222, Wangsimni-ro, Seongdong-gu, Seoul 04763, Korea\n3 Department of Pharmaceutical Science and Technology, Kyungsung \nUniversity, 309, Suyeong-ro, Nam-gu, Busan 48434, Korea\n4 College of Pharmacy, Deagu Catholic University, 13-13, Hayang-ro, \nHayang-eup, Gyeongsan-si 38430, Gyeongsangbuk-do, Korea\nPage 2 of 12Lee et al. Journal of Cheminformatics           (2024) 16:14 \nand simulated the binding sites [6–8]. While it offers a \nclear visual understanding, obtaining a 3D structure of \na feature is challenging and it was hard to collect large \ndatasets effectively. Conversely, the similarity-based tech-\nnique proposed binding candidates using priorly estab -\nlished drug-target pairs. While this approach showed \nconsiderable predictions for recognized pairs based on \nsimilarity, it confronts difficulties in determining simi -\nlarity for previously unobserved pairs [9, 10]. DNNs \nhave exhibited proficient results in DTI prediction, simi -\nlar to their successful implementations in various other \ndomains. A pioneering study, DeepDTA [11], employed \na drug and target encoder built on Convolutional Neural \nNetworks (CNN) for the prediction of binding affinities. \nInstead of relying on highly complex datasets, the Deep -\nDTA leveraged 1D expressions of the molecular structure \nsystem, Simplified Molecular Input Line Entry System \n(SMILES), and amino acid sequences, for drug and tar -\nget, respectively. With hierarchical CNN layers, simi -\nlar to conventional CNNs used for image recognition, \nDeepDTA can interpret the interactions of a given drug-\ntarget pair. After the DeepDTA, a multitude of research \ninitiatives have been undertaken to either enhance the \nencoder’s capability or predict interactions more effec -\ntively. Such advancements encompass the deployment of \nCNNs [ 12–14], the development of interactions within \ngated cross attentions [15], the adoption of encoders that \nperceive molecular structures in graph format [16–18], \ncomputing similarity using enhanced DNN-based ker -\nnels [19–21], encode sequence using generative models \n[22, 23], and the integration of multi-modal techniques \n[24–27].\nThe Transformer architecture [28], renowned for its \nproficiency in sequence processing, has been exten -\nsively employed as an encoder [29–37]. Nonetheless, it \npossesses a fundamental limitation: the computational \nexpense escalates quadratically with the increase in the \ninput length (see more details in Appendix C). Conse -\nquently, a majority of research initiatives have leaned \ntowards its application as a drug encoder rather than for \nproteins [30–33, 37]. Recent advancements have brought \nforth efficient transformer methodologies, suggesting the \npotential for significantly reducing the computational \ndemands in protein-encoding [38–41]. Concurrently, \nthe ProtTrans project [35], leveraging the established \nBidirectional Encoder Representations from Transform -\ners (BERT) [42] model and its training methodology \nhas undertaken pre-training of a protein encoder using \nan expansive set of amino acids and subsequently made \nit publicly available. As of now, the academic commu -\nnity lacks a publicly accessible, pre-trained model based \non the efficient transformer, thereby preserving the rel -\nevance and utility of ProtTrans. A recent study, that \nutilized both transformer-based encoders for represent -\ning drugs and targets was proposed [43]. The prediction \nperformances were considerably improved, however, due \nto the large size of the protein encoder, they truncated \nthe protein language model into half its size.\nTo reach an efficient computing model, knowledge dis -\ntillation techniques were proposed [44, 45]. The key con-\ncept of knowledge distillation is distilling the knowledge \nfrom the large and complex model to the small and sim -\nple model with minimum loss of knowledge (See more \ndetails on Appendix A). However, DistillProtBERT (260 \nmillion parameters) [46], a model employing knowledge \ndistillation from ProtBERT (420 million parameters) \n[35], is less efficient due to the inherent complexity of the \namino acid sequence.\nTo address this, we proposed a more efficient learning \nmethod than knowledge distillation, namely hint-based \nknowledge adaptation. This method involves using the \nintermediate features of the teacher model as hints, rep -\nresenting an expansion of knowledge distillation inspired \nby FitNet [47]. We term this approach “general knowl -\nedge” as it provides a general understanding of the target \nsequence, though lacking direct knowledge of the DTI \ntask. It is assumed that this general knowledge, serving as \na hint to the sequence, will facilitate successful learning \ndespite the small size and simplicity of the student model. \nConversely, the student model, designed to directly learn \nDTI performance, was structured in a simplified form \ncompared to the original ProtBERT. In essence, knowl -\nedge adaptation presents an efficient means of leveraging \nboth general knowledge of the target sequence and task-\nspecific knowledge related to DTI simultaneously. This \nunderscores the concept of adapting the teacher’s knowl -\nedge to the student’s knowledge, in contrast to knowl -\nedge distillation, which directly conveys task-specific \nknowledge.\nIn this study, we proposed a Dual Language Model-\nbased DTI model named DLM-DTI. The DLM-DTI was \na lightweight and efficient, but accurate DTI prediction \nmodel. With the knowledge adaptation, the rich informa-\ntion from ProtBERT successfully adapted to predict DTI \ntasks. This study has several key contributions: \n1. The hint-based knowledge adaptation technique, \ndespite its compact parameterization, demonstrates \nconsiderably improved performance compared to \nbaseline methods.\n2. By utilizing cached outputs from the teacher net -\nwork, we achieved a notable reduction in computa -\ntional costs.\n3. The knowledge adaptation approach is model-agnos -\ntic, offering flexibility in the selection of pre-trained \nmodels and architectures.\nPage 3 of 12\nLee et al. Journal of Cheminformatics           (2024) 16:14 \n \nMaterials and methods\nProblem definition\nIn binary DTI classification, the goal is to predict the tar -\nget value, Y i , for a given pair of X i , where \nX i ={ xi\ndrug ,xi\ntarget} , and Y i ∈{ 0, 1} for i= 1, ···,N  . The \nprediction of DTI can be viewed as a mapping function \nf(X i) →[ 0, 1] , which maps the drug-target pairs to a \nprobability score of the interaction.\nSequence representation\nSequence representations and embeddings involve con -\nverting a sequence, like a sentence, into a format that a \ncomputer model can understand. The first step is turning \neach part of the sequence into tokens, which are basically \ninteger numbers that the model can work with. In this \nstudy, each part of the sequence is treated as a separate \ntoken. Special tokens, like a class token, are also added \nto grasp the overall meaning of the entire sequence. The \nconcept of tokenization and special tokens is illustrated \nin Fig. 1.\nDataset configurations\nWe employ three datasets, namely DAVIS, Binding DB, \nand BIOSNAP , to train and evaluate the DLM-DTI. \nThe DAVIS dataset consists of 68 drugs and 379 pro -\nteins, with 11,103 interactions measured in dissociation \nconstant ( Kd ) [48]. The interactions are categorized \nas positive or negative, with 1506 and 9597 instances, \nrespectively. Similarly, the Binding DB dataset includes \n10,665 drugs and 1413 proteins, with 32,601 inter -\nactions measured in Kd [ 49]. The interactions are \ncategorized as positive or negative, with 9166 and \n23,435 instances, respectively. In this study, the thresh -\nold value for Kd is set to 30 units, and interactions \nwith Kd values less than 30 units are considered posi -\ntive binding interactions between the given drug and \nprotein pair [29, 43]. The BIOSNAP dataset is initially \ncomposed of positive interactions only; however, nega -\ntive pairs are added in the MolTrans study. The BIOS -\nNAP dataset comprises 4510 drugs and 2181 proteins, \nwith 27,482 interactions, including 13,741 positive and \n13,741 negative instances [29].\nThe integrated data training was first proposed by \nKang et al., and they demonstrated improvements [43]. \nIn this setting, training and validation datasets were \nmerged, and a model was trained using integrated data -\nsets. After the training steps, the trained model with \nintegrated training datasets was evaluated on individ -\nual test datasets. For example, to test the BIOSNAP test \ndataset, the model was first trained using DAVIS, Bind -\ning DB, and BIOSNAP’s training datasets, and then \ntested on BIOSNAP’s test dataset. Generally, the diver -\nsity and quantity of datasets are linked to the improve -\nment of prediction performance. Therefore, we also \nassessed the impact of dataset integrations using DLM-\nDTI. A summary of the dataset description is presented \nin Table 1.\nTo ensure a fair comparison of model performance, \nwe employ the same training, validation, and testing \ndatasets used in previous studies [29, 43]. The datasets \nare split into training, validation, and testing datasets in \nthe ratio of 7:1:2, respectively. The number of interac -\ntions for each data splitting is summarized in Table 2 .\nFig. 1 The concept of sequence representation and pre-training is illustrated. In A, the tokenization of a drug sequence (SMILES string) is depicted. \nIn B, the tokenized elements are converted into integer values according to the predefined dictionary, and the encoder model (in this example, \nChemBERTa) restores masked tokens into the original tokens (tokens colored in gray). After pre-training, the class token (CLS) is used to represent \na given sequence\nPage 4 of 12Lee et al. Journal of Cheminformatics           (2024) 16:14 \nModel configurations\nThe process flow of DLM-DTI is depicted in Fig.  2. \nDLM-DTI was comprised of three primary compo -\nnents: the drug encoder, target encoder, and inter -\naction prediction head. Notably, the target encoder \nencompasses both the teacher and student models of \nlanguage models for protein sequences.\nDrug encoder\nThe drug encoder converts SMILES sequences into \nmeaningful features, serving as a mapping function from \nmolecule sequences to a meaningful chemical space. We \nemployed the ChemBERTa encoder, which was trained \non various canonical SMILES and learned chemical \nspace. Further details are described in Appendix B.\nThe class token of the last hidden layer was extracted \nas input for the interaction prediction head. The encod -\ning process of the drug sequence can be represented as \nfollows:\nwhere LN(·) denotes the layer normalization layer, f (·) \ndenotes the projection function used to align the dimen -\nsions, and the hidden dimensions were set to 512 in this \nstudy. The upper limit of the drug sequence length was \n512 tokens, corresponding to the maximum sequence \nlength of the original ChemBERTa encoder [31].\nTarget encoder\nSimilar to the drug encoder, the target encoder also extracts \nmeaningful features from raw target sequences (amino \nacid sequences). The target encoder in this study was com-\nposed of both a teacher and a student model. The teacher \n(1)zdrug = f(LN (xclass)),\nTable 1 The description of datasets\n1 The unique values after integaration\nDataset Drugs Targets Interactions\nPositive Negative\nDAVIS 68 379 1506 9597\nBinding DB 10,665 1413 9166 23,435\nBIOSNAP 4510 2181 13,741 13,741\nIntegrated1 11,700 3067 24,413 46,773\nTable 2 The number of interactions for each split\n1 Training and validation are conducted using merged datasets; however, testing \nis performed on individual datasets\nSetting Training Validation Testing\nDAVIS 2086 3006 6011\nBinding DB 12,668 6644 13,289\nBIOSNAP 19,238 2748 5496\nIntegrated1 33,992 12,398 (6011/13,289/5496)\nFig. 2 The process flow of DLM-DTI. The drug and target sequences feed into their respective encoders. The encoded sequences are then merged, \nand the probability of bindings is computed using the interaction prediction head. DLM-DTI only utilizes the class token (CLS) of each encoded \nsequence because the class token preserves the abstract meaning of the entire sequence. The features of target sequences are computed using \na teacher-student-based architecture, specifically employing a hint-based learning strategy\nPage 5 of 12\nLee et al. Journal of Cheminformatics           (2024) 16:14 \n \nmodel used for target sequence encoding was the Prot -\nBERT model, pre-trained on UniRef and big fantastic data-\nbase databases [35]. Details of ProtBERT are described in \nAppendix C. The original ProtBERT model was trained on \nsequences up to 40 K characters, with 420 million parame-\nters. The student model was designed to match the original \nteacher model, ProtBERT, however, the number of layers \nwas reduced. Except for the number of layers, the student \nmodel followed the hyperparameters of the teacher model. \nThe number of parameters of the student model was 6.2% \nof the teacher model; teacher model: 420.0 million, student \nmodel: 26 million. The detailed parameters of the target \nencoder are presented in Table 3.\nIn most cases, fully fine-tuning the large model was \nimpractical due to restrictions on datasets and the associ-\nated computational expenses. To address this challenge, \nwe adopted a hint-based training scheme that kind of \nknowledge distillation comprises both a teacher model \nand a student model. The teacher model was prevented \nfrom parameter updates, enabling solely the parameters \nof the student model to be updated. Given that the teacher \nmodel’s output was not subject to training, it retained a \nfixed form, thus enabling us to cache outputs of the teacher \nmodel prior to the training and inference step. This strat -\negy markedly minimizes computational redundancy, \nthereby optimizing computational efficiency. Consider -\ning the teacher model’s output was not trained, it served \nas a form of hint to which the task-specific model (student \nmodel) could refer. The teacher and student models were \ncombined using class token mixing to encode the target \nsequence. The output class token was treated as a “hint” \nthat contained general knowledge of the given protein \nsequence. On the other hand, the output class token of the \nstudent model was considered as task-oriented specific \nknowledge. To mix the general knowledge and task-spe -\ncific knowledge, we added two class tokens with learnable \ngating parameters ( /afii9838 ). The encoding process of the target \nsequence can be represented as follows:\n(2)\nztarget= /afii9838g\n(\nLN (xstudent\nclass )) + (1 − /afii9838)h(LN(xteacher\nclass )\n)\n,\nwhere g(·) and h(·) are the projection functions used to \nalign the dimensions, and the adaptation parameter /afii9838 is \na learnable parameter initialized randomly from a uni -\nform distribution, /afii9838∼ Uniform(0, 1) . The term “adapta -\ntion” was employed to describe the process of adjusting \ngeneral knowledge to suit the specific requirements of \na particular task. An elevated value of the adaptation \nparameter indicated an increased emphasis of the model \non the class token derived from the teacher model. In \ncontrast, a decreased value of the adaptation parameter \nsignified a predominant utilization of task-specific infor -\nmation obtained from the student model. The hidden \ndimensions of the class token mixing were set to 1024 in \nthis study. The maximum length of the target sequence \nwas set to 545 tokens, which covered 95% of proteins in \nthe datasets, and the same max protein sequence lengths \nof previous studies [29, 43].\nInteraction prediction head\nThe class tokens of drug and target sequences have \nabstract meanings for each sequence. The interaction \nprediction head aggregated the features of drug-target \npairs and predicted binding probability. In this step, there \nwere multiple choices for mixing the features; for exam -\nple, cross attention, capsule network, etc. However, we \nsimply employed concatenation that showed stable per -\nformances in the previous work [43].\nThe interaction module consists of three sequential \nblocks. Each block is structured with a Fully Connected \n(FC) layer, followed by an activation function and subse -\nquently a dropout layer. The respective dimensions of the \nFC layers are 2048, 1024, and 512. The chosen activation \nfunction for these blocks is the Gaussian Error Linear \nUnit (GeLU). Additionally, a dropout rate of 0.1 has been \nemployed for regularization. A detailed schematic of \nthis configuration can be found in Fig.  3, and the specific \nparameter values are summarized in Table 4.\nExperimental setup\nEvaluation metrics\nWe used the Area Under the Receiver Operation Char -\nacteristics curve (AUROC) and the Area Under the \nPrecision-Recall Curve (AUPRC) as primary evaluation \nmetrics. AUROC is one of the most favorable metrics \nto measure classification performance, particularly in \nthe medical field; however, it could be easily overesti -\nmated when the data has class imbalance [50]. Therefore, \nAUPRC is a relatively robust metric for measuring clas -\nsification performance in imbalanced settings [50]. Sensi-\ntivity and specificity scores were utilized as sub-metrics, \nand the threshold for these sub-metrics was simply set to \n0.5.\nTable 3 The specific parameters of target encoder\nTeacher Student\nNumber of hidden layers 30 2\nNumber of attention heads 16 16\nHidden dimension 1024 1024\nIntermediate-size 4096 4096\nNumber of parameters 420 M 26 M\nPage 6 of 12Lee et al. Journal of Cheminformatics           (2024) 16:14 \nModel training hyperparameters\nThe DLM-DTI was optimized using the AdamW opti -\nmizer with a learning rate of 0.0001. A cosine annealing \nlearning rate scheduler was employed to adjust the learn-\ning rate. The binary cross-entropy loss was used to calcu -\nlate the difference between predictions and ground truth. \nThe model was trained for 50 epochs, and the best-per -\nforming parameters were selected based on the AUPRC \nscore during validation. Due to severe class imbalance, \nthe model could easily be overfitted to the dominant \nclass. To prevent the selection of an overfitted model, we \nset the selection criteria as AUPRC rather than AUROC \nor the minimum loss coefficient. Automated mixed pre -\ncision was utilized, and the batch size was set to 32. The \nbest combination of hyperparameters was determined \nthrough iterative experiments.\nThe use of a class imbalance sampler did not show any \nbenefit for model training; therefore, we did not apply an \nimbalance sampler. Instead, AUPRC-based optimization \ndemonstrated better performance in predicting binding \nprobability.\nHardware and softward\nWe used a single NVIDIA A100 GPU to train DLM-DTI. \nThe Python (v3.8) and PyTorch deep learning framework \n(v1.13) for trained DLM-DTI.\nResults\nBinding probability prediction\nThe baseline models, namely MolTrans [29] and the \napproach by Kang et  al. [43], along with our proposed \nDLM-DTI, were trained on the same training datasets \nand evaluated using identical test datasets. Table  5 pre-\nsents a summary of the evaluation results obtained from \nthese experiments. MolTrans was exclusively trained on \nindividual datasets and evaluated individually. In con -\ntrast, both Kang et  al. and our DLM-DTI were trained \nusing both individual and combined dataset settings. \nThis approach was claimed in Kang et al., and therefore \nFig. 3 Structure of the interaction prediction head. The interaction prediction head mixes the features of the drug-target pair to predict the binding \nprobability of a given pair. The number under the block indicates the feature dimension\nTable 4 The detailed parameters of interaction prediction head\n1 Feature of drug sequence, zdrug\n2 Feature of target sequence, ztarget\nBlock Layers Input \ndimensions\nOutput \ndimensions\nDropout rate\nInput Concatenation 5121 , 5122 1024\nBlock 1 FC layer 1024 2048\nGeLU & Drop-\nout\n2048 2048 0.1\nBlock 2 FC layer 2048 1024\nGeLU & Drop-\nout\n1024 1024 0.1\nBlock 3 FC layer 1024 512\nGeLU & Drop-\nout\n512 512 0.1\nOutput FC layer 512 1\nPage 7 of 12\nLee et al. Journal of Cheminformatics           (2024) 16:14 \n \nthe previous study, MolTrans, did not experiment with \nan integrated dataset.\nWithin the BIOSNAP dataset, DLM-DTI showed an \nimproved AUPRC score (absolute value; percentage) than \nMolTrans (0.013; 1.44%), and Kang et al. (0.014 ∼ 0.017; \n1.56 ∼ 1.90%). The AUROC score was improved com -\npared to MolTrans (0.019; 2.12%), however, the AUROC \nshowed similarity to Kang et  al. ’s model. Similarly, in \nthe Binding DB, DLM-DTI exhibited a considerably \nimproved AUPRC score than other methods, MolTrans \n(0.021; 3.38%), and Kang et al. ’s model (0.004 ∼ 0.02; 0.63 \n∼ 3.21%), respectively.\nIn the DAVIS dataset, the performance of the DLM-\nDTI was degraded, and its performance was similar to \nthat of MolTrans. The training with an integrated data -\nset showed benefits for the DLM-DTI only in the DAVIS \ndataset.\nAdaptation parameter, /afii9838\nDuring the training, the randomly initialized adaptation \nparameter /afii9838 gradually decreased and converged, as illus -\ntrated in Fig.  4. The adaptation parameter controlled the \nfeature weights from the teacher and student encoder. As \nmentioned earlier, the teacher encoder contained gen -\neral knowledge of the target sequence, and the student \nencoder had narrow but specific task-related knowledge. \nWith the adaptation parameter, the DLM-DTI modu -\nlated the importance of each feature to accurately predict \nbinding probability.\nTo evaluate the effect of teacher-student architecture-\nbased target sequence encoding, two ablation studies \nwere conducted.\n• /afii9838 set to 0: Only the teacher encoder (general knowl -\nedge) was utilized.\n• /afii9838 set to 1: Only the student encoder (task-specific \nknowledge) was utilized.\nThe adaptation setting (which utilized both teacher-stu -\ndent encoders) showed the best performance (AUROC: \n0.912; AUPRC: 0.643) compared to the teacher encoder-\nonly setting (AUROC: 0.911; AUPRC: 0.635) or the \nTable 5 The prediction performance of binding affinity\nS: single dataset, I: integrated dataset\nPerformances of five randomly initialized runs were averaged\nBest performance is highlighted in bold\nDataset Model AUROC AUPRC Sensitivity Specificity\nBIOSNAP MolTrans 0.895 ± 0.002 0.901 ± 0.004 0.775 ± 0.032 0.851 ± 0.014\nKang et al., S 0.914 ± 0.006 0.900 ± 0.007 0.862 ± 0.025 0.847 ± 0.007\nKang et al., I 0.910 ± 0.012 0.897 ± 0.014 0.830 ± 0.029 0.863 ± 0.011\nDLM-DTI, S 0.914 ± 0.003 0.914 ± 0.006 0.848 ± 0.016 0.844 ± 0.024\nDLM-DTI, I 0.910 ± 0.005 0.914 ± 0.004 0.850 ± 0.014 0.821 ± 0.006\nDAVIS MolTrans 0.907 ± 0.002 0.404 ± 0.016 0.800 ± 0.022 0.876 ± 0.013\nKang et al., S 0.920 ± 0.002 0.395 ± 0.007 0.824 ± 0.026 0.889 ± 0.015\nKang et al., I 0.942 ± 0.005 0.517 ± 0.017 0.903 ± 0.017 0.866 ± 0.015\nDLM-DTI, S 0.895 ± 0.003 0.373 ± 0.017 0.833 ± 0.044 0.802 ± 0.070\nDLM-DTI, I 0.898 ± 0.026 0.406 ± 0.026 0.860 ± 0.016 0.786 ± 0.022\nBindingDB MolTrans 0.914 ± 0.001 0.622 ± 0.007 0.797 ± 0.005 0.896 ± 0.007\nKang et al., S 0.922 ± 0.001 0.623 ± 0.010 0.814 ± 0.025 0.916 ± 0.016\nKang et al., I 0.926 ± 0.001 0.639 ± 0.018 0.802 ± 0.022 0.928 ± 0.013\nDLM-DTI, S 0.912 ± 0.004 0.643 ± 0.006 0.888 ± 0.014 0.793 ± 0.015\nDLM-DTI, I 0.912 ± 0.004 0.636 ± 0.007 0.869 ± 0.023 0.811 ± 0.010\nFig. 4 Variation of the adaptation parameter ( /afii9838 ) during model \ntraining process\nPage 8 of 12Lee et al. Journal of Cheminformatics           (2024) 16:14 \nstudent encoder-only setting (AUROC: 0.900; AUPRC: \n0.635). The effect of the /afii9838 parameter is summarized in \nTable 6.\nThe student encoder-only setting exhibited the poor -\nest prediction performance (Rank: 3rd ). This implies that \ntwo layers of simple and shallow networks were not suffi -\ncient to capture the complex patterns and features of tar-\nget sequences to accurately predict DTIs. However, the \nteacher encoder-only setting demonstrated comparable \nperformance (Rank: 2nd ). This suggests that the general \nknowledge of the teacher model has the potential to pre -\ndict binding probability. The teacher encoder-only setting \ncorresponds to linear probing, where the training strat -\negy only updates the prediction head without adjusting \nthe weights of the encoder [51, 52]. The prediction per -\nformance of linear probing is considered as an encoder’s \nexisting knowledge.\nTime and memory analysis\nTypically, a model’s performance exhibits a direct cor -\nrelation with its parameter count, suggesting that larger \nmodels often yield superior outcomes. Nonetheless, \nthis advantage comes with a caveat; substantial models \nnecessitate considerable computational resources dur -\ning both the training and inference stages. In light of \nthis, we embarked on a systematic analysis comparing \ntraining time and parameter counts (Table  7). The met -\nric for training time was derived by computing the mean \nlearning time across three epochs, utilizing the Binding \nDB dataset.\nDLM-DTI showed the best AUPRC score (0.643), only \nwith 24.56% (86.7 million) of parameters compared to the \nKang et  al. (353.0 million) [43]. Additionally, DLM-DTI \nrequired 7.7 GB video random access memory (VRAM), \nand 63.00  s for a single training epoch. It was 16.24% \n(47.4 GB), and 9.98% (631.00  s) of the Kang et  al. [43]. \nThe MolTrans required the smallest VRAM (5.9 GB), \nhowever, the AUPRC score (0.622) was slightly lower \nthan DLM-DTI (0.643). In our experimental setting, \nDLM-DTI required 7.7 GB of VRAM, therefore, it could \nbe trained on conventional graphic processing units \n(GPUs), not for high-performing research machines (See \ndetails on 2.5.2).\nCold drug, target, and bindings\nIn addressing DTI challenges, the cold splitting testing \napproach is widely adopted [36, 53], primarily due to \nthe inherent difficulties in dataset procurement and the \nparamount importance of achieving generalization for \nnovel pairs. The term “cold splitting” pertains to scenar -\nios where previously unseen drug-target interactions are \ninvolved, ones that were excluded from both the train -\ning and validation datasets. To simulate this condition, \nwe conducted experiments where we isolated cold drugs, \ncold targets, and cold binding interactions from the test \nset of models trained to utilize the Binding DB dataset. \nWe identified a total of 2,127 cold drugs and 136 cold tar-\ngets. Specifically, a cold drug configuration encompasses \nall interactions associated with a cold drug, while a cold \ntarget configuration comprises all interactions associated \nwith a cold target. The cold bindings were the interac -\ntions between cold drugs and cold targets, and only 114 \npairs were identified. The performances of cold-splitting \ndatasets are summarized in Table  8. DLM-DTI’s per -\nformance was comparable to the baseline models in the \ncontext of the cold drug, yet exhibited a minor deterio -\nration to the cold target and was found to be most defi -\ncient in addressing cold binding. Conversely, Kang et al. \n[43] manifested commendable prediction capabilities \nacross all testing scenarios. MolTrans [29] exhibited a \nTable 6 The prediction performance of binding affinity\nPerformances of five randomly initialized runs were averaged\nBest performance is highlighted in bold\nBindingDB dataset is utilized\nAUROC AUPRC Sensitivity Specificity\nStudent only 0.900 ± \n0.001\n0.612 ± \n0.003\n0.845 ± \n0.024\n0.805 ± \n0.018\nTeacher only 0.911 ± \n0.002\n0.635 ± \n0.002\n0.880 ± \n0.012\n0.800 ± 0.014\nAdaptation 0.912 ± \n0.004\n0.643 ± \n0.006\n0.888 ± \n0.014\n0.793 ± 0.015\nTable 7 Time and memory analysis of baseline models and DLM-DTI\nM: millions, S: seconds, GB: Giga bytes\n1 Mean ± SD\n2 Batch size is matched to 32\n3 Results of Binding DB with single training setting. Best performance is highlighted in bold\nParameters (M) Training time (S)1 Memory capacity (GB)2 AUPRC3\nMolTrans 62.8 75.33 ± 3.51 5.9 0.622 ± 0.007\nKang et al. 353.0 631.00 ± 17.06 47.4 0.623 ± 0.010\nDLM-DTI 86.7 63.00 ± 1.00 7.7 0.643 ± 0.006\nPage 9 of 12\nLee et al. Journal of Cheminformatics           (2024) 16:14 \n \nperformance metric closely mirroring Kang et al. in terms \nof AUROC, but fell short when evaluated using AUPRC.\nDiscussion\nIn this study, we suggested a lightweight but accurate DTI \nprediction model, namely DLM-DTI. The main hurdle \nfor utilizing protein sequence-based language models, \nsuch as ProtBERT [35], was heavy computing resource \nrequirements. To comprehend the complex and long \nsequence of a protein, it needed heavy and large archi -\ntectures and an intensive pre-training process. The DLM-\nDTI mitigated the computational burden caused by the \nprotein encoder, by using a knowledge adaptation. DLM-\nDTI achieved improved AUPRC performance, especially \nin Binding DB (0.63 ∼ 3.38%), and BIOSNAP (1.44 ∼ \n1.9%) datasets. The most interesting point was that DLM-\nDTI utilized only 25% of parameters (86.7 million) com -\npared to the previous state-of-the-art model, Kang et al. \n(353 million) [43]. Additionally, DLM-DTI required only \n7.7 GB of VRAM, and 63 s for each training epoch, that \nof 16.24%, and 9.98% of Kang et al. [43].\nThe Transformer-based language model has exhib -\nited impressive capabilities across various applications, \nincluding molecular and protein sequences. However, \npre-training has emerged as a key approach to further \noptimize the model’s functional and semantic relation -\nship learning from large sequence datasets [35–37, 42, \n43]. Despite the promising results, the computational \ncost of the language model increases significantly with \nthe input length. To address this challenge, Kang et  al. \nproposed a Kang et  al. approach, which employed only \nhalf of the pre-trained target encoder [43]. The meth -\nodology employed by the ELECTRA-DTA model aligns \nclosely with our approach [36]. In the ELECTRA-DTA \nframework, the features originating from the pre-trained \ndrug encoder and protein encoder are individually aver -\naged. Subsequently, these averaged features are com -\npactly represented as a compressed feature vector. This \nvector is subsequently incorporated into a squeeze-and-\nexcitation network, aiming to enhance the predictive \ncapabilities of the model. Their approach can also be per -\nceived as a tactical maneuver to circumvent the neces -\nsity of fine-tuning the complete encoder. However, it is \nimportant to note that we could not directly compare \nthe prediction performance of our DLM-DTI approach \nto that of ELECTRA-DTA due to differences in the tar -\nget tasks, with DLM-DTI using binary classification and \nELECTRA-DTA using pKd regression.\nIn our study, we introduced an adaptation parameter \nto efficiently generate meaningful protein features. The \nadaptation parameter, denoted as /afii9838 , was randomly ini -\ntialized and tuned. This parameter controlled the weights \nof knowledge from both the teacher model (provid -\ning general knowledge) and the student model (captur -\ning task-specific knowledge). In the ablation studies \n(Table 6), the absence of knowledge adaptation resulted \nin significant degradation of performance for both the \nteacher-only and student-only settings. However, the \nDLM-DTI with knowledge adaptation exhibited weak -\nnesses in generalization performance. Kang et  al. ’s [43] \nwork also demonstrated strong performance under cold-\nsplitting conditions (Table  8). In contrast, our DLM-\nDTI, which either matched or outperformed Kang et al. \non the complete dataset, showed reduced effectiveness \nin cold-splitting evaluations, particularly concerning \ncold-binding interactions. This may be attributed to the \nover-reduction of the student model, limiting gener -\nalization performance. Inspired by recent examples that \nincorporate natural language-based prior knowledge to \nenhance prediction performance, we aim to improve our \napproach by adding natural language information related \nto the function of proteins in future work [54]. Interest -\ningly, integrated dataset training did not prove beneficial \nfor DLM-DTI. In Kang et al. [43], training with integrated \ndatasets demonstrated outstanding performances. Large-\nscale Transformer-based architectures typically require \na substantial amount of data to realize their full poten -\ntial. However, DLM-DTI introduces a small-scale student \nmodel, and it is speculated that the small size was suffi -\ncient for effective learning.\nRecently, foundation models based on large language \nmodels have been widely studied [55, 56]. A shared chal-\nlenge between these models and protein sequence encod-\ners pertains to the intricacies involved in fine-tuning. \nDue to the scarcity of annotated data and the extensive \nparameters within these models, innovative strategies for \nTable 8 The classification performances within the cold splitting settings\nBest performance is highlighted in bold\nMolTrans Kang et al. DLM-DTI\nAUROC AUPRC AUROC AUPRC AUROC AUPRC\nCold Drug 0.853 0.562 0.884 0.617 0.850 0.584\nCold Target 0.841 0.668 0.855 0.716 0.789 0.527\nCold Binding 0.718 0.370 0.744 0.448 0.622 0.261\nPage 10 of 12Lee et al. Journal of Cheminformatics           (2024) 16:14 \neffective fine-tuning have been proposed. For instance, \na method called low-rank adaptation (LoRA) [57], simi -\nlar to our own approach, adopt a technique where only \nthe adaptation layer is adjusted. This is achieved by inte -\ngrating a low-rank adaptation layer, which eliminates the \nneed for comprehensive fine-tuning across all layers. This \napproach proves to be more cost-effective and quicker to \nconverge compared to the resource-intensive process of \ncomplete fine-tuning. Therefore, in our future study, we \nplan to compare the performances of a fine-tuning model \nusing LoRA’s adaptation approaches. Furthermore, there \nis a need for enhancement in the design of the interac -\ntion head. Currently, this component is composed of a \nsequence of straightforward FC layers, which exhibits \nreduced effectiveness in cold bindings. To address this, \npotential strategies include the integration of a squeeze-\nand-excitation network [58], capsule network [59], cross-\nattention [60], and other alternatives.\nConclusion\nIn this study, we employed knowledge adaptation to effi -\nciently and accurately predict binding probability. The \nknowledge adaptation was efficiently tuned with both \ngeneral knowledge and task-specific knowledge through \nthe teacher-student architectures. With only 25% of the \nmodel parameters, DLM-DTI exhibited considerable \nperformance compared to the previous state-of-the-art \nmodel. Notably, DLM-DTI required 7.7 GB of VRAM, \nallowing training on conventional GPUs without the \nneed for high-performing GPUs.\nAppendix A Knowledge Distillation\nRecent high-performing DNN models boast millions or \nbillions of parameters, necessitating extensive and high-\nperformance hardware resources, such as GPU clusters \nand TPU pods. Knowledge distillation was proposed to \ndevelop a lightweight model while retaining robust infor -\nmation processing capabilities [44, 45]. The knowledge \ndistillation process involves two models, specifically the \nteacher model and the student model. Conventionally, \nknowledge distillation begins by training the teacher \nmodel, a complex and high-capacity model, on the tar -\nget task. Subsequently, the acquired knowledge from \nthe teacher model is transferred to the student model, a \nmore lightweight counterpart. This transfer is typically \nachieved by encouraging the student model to mimic \nthe outputs [44] or internal representations [47] of the \nteacher model. The overarching goal is to distill the com -\nprehensive knowledge captured by the teacher model \ninto a more compact and computationally efficient stu -\ndent model.\nFitNet [47] introduces the concept of “hints” to \nenhance the knowledge distillation approach. In addi -\ntion to replicating the output of the current teacher \nmodel, hints guide the student to mimic intermedi -\nate features together. This inclusion of hints enhances \nthe performance of knowledge distillation by enabling \nthe learning of not only the final result but also the \nintermediate features. In this context, a hint can be \ninterpreted as providing information about both the \nintermediate features and the final feature.\nAppendix B Drug Encoder: ChemBERTa\nChemBERTa is a Transformer-based model pre-trained \nusing 10 million SMILES sequences [31]. Based on \nRoBERTa [61], a model known for its outstanding \nperformance in natural language processing, Chem -\nBERTa comprises 12 attention heads and 6 layers. \nDrug sequences, expressed in Canonical SMILES, are \ntokenized using a subword-level tokenizer, while a byte-\npair encoder (BPE) tokenizer is employed to group fre -\nquently occurring elements together into larger chunks \nfor more efficient processing. BPE stands as a blend of \ncharacter and word-level representations, facilitating \nthe management of extensive vocabularies in natural \nlanguage corpora. Guided by the insight that less com -\nmon or unfamiliar words can frequently be broken \ndown into several recognized subwords, BPE identifies \nthe optimal word segmentation through an iterative and \ngreedy merging of frequently occurring character pairs \n[62]. ChemBERTa has a total of 767 tokens, including a \nclass token to encapsulate the abstract meaning of the \nentire sequence, a start of sequence token (SOS), an \nend of sequence token (EOS), and a pad token to mark \nthe start and end of the sequence.\nChemBERTa was trained using masked language \nmodeling (MLM), where the task involves masking \na portion of the entire sequence and then restoring \nthe corresponding tokens; 15% of the total sequence \nwas masked. The maximum processable sequence \nlength is 512 tokens. ChemBERTa, pre-trained using \nMLM tasks, can then be used as an encoder for drug \nsequences because it has been trained on restoration \ntasks and has an understanding of molecule sequences. \nChemBERTa can perform comparably to the commonly \nused extended-connectivity fingerprint (ECFP) [63] in \nmolecule properties prediction tasks using the Chem -\nBERTa encoder, and it was employed in this study due \nto its availability through the HuggingFace API, facili -\ntating easy utilization.\nPage 11 of 12\nLee et al. Journal of Cheminformatics           (2024) 16:14 \n \nAppendix C Target Encoder: ProtBERT\nProtBERT, a component of the ProtTrans project, is a \nBERT model trained on an extensive dataset of amino \nacid sequences [35]. It underwent training using the \nsame MLM approach as ChemBERTa, with 15% mask -\ning (Appendix B). However, owing to the intricacy of \namino acid sequences, ProtBERT consists of 30 layers \nand 16 attention heads, resulting in a total parameter \ncount of 4.2 million. Each element is considered one \ntoken in ProtBERT, and it comprises 30 tokens, includ -\ning special tokens. Notably, it was trained to handle \nsequences of up to 4000 tokens, accommodating the \ntypically extended length of amino acid sequences.\nHowever, ProtBERT uses the Transformer’s core \noperation, self-attention, where the amount of compu -\ntation increases as the square of the length of a given \nsequence. The self-attention operation is as follows:\nwhere the query (Q) is the product of input sequence x \nand learnable parameter WQ , and key (K) is the product \nof input sequence x and learnable parameter WK.\nTherefore, a substantial amount of memory and com -\nputational resources must be allocated to manage long \nsequences of amino acids. This constitutes a significant \nbottleneck in the practical utilization of ProtBERT. \nWhile recent proposals, such as efficient self-atten -\ntion computations using linear transformers [64] and \nNystrom approximation [38], aim to address this chal -\nlenge, pre-training with such approaches remains \nexpensive. As an illustration, ProtBERT underwent \ntraining utilizing 1,024 tensor processing units (TPUs), \na resource allocation typically inaccessible in stand -\nard research environments. Consequently, this study \nemphasizes the efficient utilization of the previously \npublished ProtBERT, prioritizing practical applica -\ntion over creating a new pre-training model that might \nreduce computational requirements.\nAuthor contributions\nConceptualization, JL, DJ, IS, and YK; methodology, JL, and YK; writing—origi-\nnal draft preparation, JL; writing—review and editing, YK; supervision, DJ, and \nYK; formal analysis, JL; resources, YK; All authors have read and agreed to the \npublished version of the manuscript.\nFunding\n(1) This work was supported by research grants from Daegu Catholic \nUniversity in 2022. (2) This work was supported by the National Research \nFoundation of Korea(NRF) grant funded by the Korea government(MSIT) \n(RS-2022-00166945).\nAvailability of data and materials\nThe datasets are available at: https:// github. com/ kexin huang 12345/ MolTr ans/ \ntree/ master/ datas et.\n(C1)Attention(Q,K ) = softmax\n(\nQK T\n√\ndk\n)\n,\nCode availability\nThe source codes are available at: https:// github. com/ jongh yunle e1993/ DLM- \nDTI_ hint- based- learn ing/ tree/ master.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe author(s) declare that they have no conflict of interest.\nReceived: 9 September 2023   Accepted: 22 January 2024\nReferences\n 1. Anusuya S, Kesherwani M, Priya KV, Vimala A, Shanmugam G, Velmurugan \nD, Gromiha MM (2018) Drug-target interactions: prediction methods and \napplications. Curr Protein Pept Sci 19(6):537–561\n 2. Ledford H (2011) 4 ways to fix the clinical trial: clinical trials are crumbling \nunder modern economic and scientific pressures. Nature looks at ways \nthey might be saved. Nature 477(7366):526–529\n 3. Zheng Y, Wu Z (2021) A machine learning-based biological drug-target \ninteraction prediction method for a tripartite heterogeneous network. \nACS Omega 6(4):3037–3045\n 4. Ashburn TT, Thor KB (2004) Drug repositioning: identifying and develop-\ning new uses for existing drugs. Nat Rev Drug Discovery 3(8):673–683\n 5. Strittmatter SM (2014) Overcoming drug development bottlenecks with \nrepurposing: old drugs learn new tricks. Nat Med 20(6):590–591\n 6. Li H, Leung K-S, Wong M-H, Ballester PJ (2015) Low-quality structural and \ninteraction data improves binding affinity prediction via random forest. \nMolecules 20(6):10947–10962\n 7. Trott O, Olson AJ (2010) Autodock vina: improving the speed and accu-\nracy of docking with a new scoring function, efficient optimization, and \nmultithreading. J Comput Chem 31(2):455–461\n 8. Luo H, Mattes W, Mendrick DL, Hong H (2016) Molecular docking for \nidentification of potential targets for drug repurposing. Curr Top Med \nChem 16(30):3636–3645\n 9. Pahikkala T, Airola A, Pietilä S, Shakyawar S, Szwajda A, Tang J, Aittokallio \nT (2015) Toward more realistic drug-target interaction predictions. Brief \nBioinform 16(2):325–337\n 10. He T, Heidemeyer M, Ban F, Cherkasov A, Ester M (2017) Simboost: a \nread-across approach for predicting drug-target binding affinities using \ngradient boosting machines. J Cheminformatics 9(1):1–14\n 11. Öztürk H, Özgür A, Ozkirimli E (2018) Deepdta: deep drug-target binding \naffinity prediction. Bioinformatics 34(17):821–829\n 12. Lee I, Keum J, Nam H (2019) Deepconv-dti: prediction of drug-target \ninteractions via deep learning with convolution on protein sequences. \nPLoS Comput Biol 15(6):1007129\n 13. Lee I, Nam H (2022) Sequence-based prediction of protein binding \nregions and drug-target interactions. J Cheminformatics 14(1):1–15\n 14. Zeng Y, Chen X, Luo Y, Li X, Peng D (2021) Deep drug-target binding affin-\nity prediction with multiple attention blocks. Brief Bioinform 22(5):117\n 15. Kim Y, Shin B (2021) An interpretable framework for drug-target interac-\ntion with gated cross attention. In: Machine Learning for Healthcare \nConference, pp. 337–353. PMLR\n 16. Nguyen T, Le H, Quinn TP , Nguyen T, Le TD, Venkatesh S (2021) Graphdta: \npredicting drug-target binding affinity with graph neural networks. \nBioinformatics 37(8):1140–1147\n 17. Thafar MA, Alshahrani M, Albaradei S, Gojobori T, Essack M, Gao X (2022) \nAffinity2vec: drug-target binding affinity prediction through representa-\ntion learning, graph mining, and machine learning. Sci Rep 12(1):1–18\nPage 12 of 12Lee et al. Journal of Cheminformatics           (2024) 16:14 \n 18. Liao J, Chen H, Wei L, Wei L (2022) Gsaml-dta: an interpretable drug-target \nbinding affinity prediction model based on graph neural networks with \nself-attention mechanism and mutual information. Comput Biol Med \n150:106145\n 19. Su X, Hu L, You Z, Hu P , Wang L, Zhao B (2022) A deep learning method \nfor repurposing antiviral drugs against new viruses via multi-view \nnonnegative matrix factorization and its application to sars-cov-2. Brief \nBioinform 23(1):526\n 20. Li Y-C, You Z-H, Yu C-Q, Wang L, Wong L, Hu L, Hu P-W, Huang Y-A (2022) \nPpaedti: personalized propagation auto-encoder model for predicting \ndrug-target interactions. IEEE J Biomed Health Inform 27(1):573–582\n 21. Thafar MA, Olayan RS, Albaradei S, Bajic VB, Gojobori T, Essack M, Gao X \n(2021) Dti2vec: drug-target interaction prediction using network embed-\nding and ensemble learning. J Cheminformatics 13(1):1–18\n 22. Zhao L, Wang J, Pang L, Liu Y, Zhang J (2020) Gansdta: predicting drug-\ntarget binding affinity using gans. Front Genetics 1243\n 23. Chen Y, Wang Z, Wang L, Wang J, Li P , Cao D, Zeng X, Ye X, Sakurai T (2023) \nDeep generative model for drug design from protein target sequence. J \nCheminformatics 15(1):38\n 24. Liu G, Singha M, Pu L, Neupane P , Feinstein J, Wu H-C, Ramanujam J, \nBrylinski M (2021) Graphdti: a robust deep learning predictor of drug-\ntarget interactions from multiple heterogeneous data. J Cheminformatics \n13(1):1–17\n 25. Yan X, Liu Y (2022) Graph-sequence attention and transformer for predict-\ning drug-target affinity. RSC Adv 12(45):29525–29534\n 26. Hua Y, Song X, Feng Z, Wu X (2023) Mfr-dta: a multi-functional and robust \nmodel for predicting drug-target binding affinity and region. Bioinfor-\nmatics 39(2):056\n 27. Bian J, Zhang X, Zhang X, Xu D, Wang G (2023) Mcanet: shared-weight-\nbased multiheadcrossattention network for drug-target interaction \nprediction. Brief Bioinform 24(2):082\n 28. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, \nPolosukhin I (2017) Attention is all you need. Adv Neural Inf Proc Syst 30\n 29. Huang K, Xiao C, Glass LM, Sun J (2021) Moltrans: molecular interac-\ntion transformer for drug-target interaction prediction. Bioinformatics \n37(6):830–836\n 30. Honda S, Shi S, Ueda HR (2019) Smiles transformer: Pre-trained molecular \nfingerprint for low data drug discovery. arXiv preprint arXiv: 1911. 04738\n 31. Chithrananda S, Grand G, Ramsundar B (2020) Chemberta: Large-scale \nself-supervised pretraining for molecular property prediction. arXiv \npreprint arXiv: 2010. 09885\n 32. Maziarka Ł, Danel T, Mucha S, Rataj K, Tabor J, Jastrzębski S (2020) Mol-\necule attention transformer. arXiv preprint arXiv: 2002. 08264\n 33. Fabian B, Edlich T, Gaspar H, Segler M, Meyers J, Fiscato M, Ahmed M \n(2020) Molecular representation learning with language models and \ndomain-relevant auxiliary tasks. arXiv preprint arXiv: 2011. 13230\n 34. Alley EC, Khimulya G, Biswas S, AlQuraishi M, Church GM (2019) Unified \nrational protein engineering with sequence-based deep representation \nlearning. Nat Methods 16(12):1315–1322\n 35. Elnaggar A, Heinzinger M, Dallago C, Rihawi G, Wang Y, Jones L, Gibbs T, \nFeher T, Angerer C, Steinegger M, et al (2020) Prottrans: towards cracking \nthe language of life’s code through self-supervised deep learning and \nhigh performance computing. arXiv preprint arXiv: 2007. 06225\n 36. Wang J, Wen N, Wang C, Zhao L, Cheng L (2022) Electra-dta: a new \ncompound-protein binding affinity prediction model based on the \ncontextualized sequence encoding. J Cheminformatics 14(1):1–14\n 37. Shin B, Park S, Kang K, Ho JC (2019) Self-attention based molecule repre-\nsentation for predicting drug-target interaction. In: Machine Learning for \nHealthcare Conference, pp. 230–248. PMLR\n 38. Xiong Y, Zeng Z, Chakraborty R, Tan M, Fung G, Li Y, Singh V (2021) \nNyströmformer: A nyström-based algorithm for approximating self-\nattention. Proc AAAI Conf Artif Intell 35:14138–14148\n 39. Child R, Gray S, Radford A, Sutskever I (2019) Generating long sequences \nwith sparse transformers. arXiv preprint arXiv: 1904. 10509\n 40. Press O, Smith NA, Lewis M (2021) Train short, test long: Attention with \nlinear biases enables input length extrapolation. arXiv preprint arXiv: \n2108. 12409\n 41. Dao T, Fu D, Ermon S, Rudra A, Ré C (2022) Flashattention: fast and \nmemory-efficient exact attention with io-awareness. Adv Neural Inf \nProcess Syst 35:16344–16359\n 42. Devlin J, Chang M-W, Lee K, Toutanova K (2018) Bert: Pre-training of deep \nbidirectional transformers for language understanding. arXiv preprint \narXiv: 1810. 04805\n 43. Kang H, Goo S, Lee H, Chae J-W, Yun H-Y, Jung S (2022) Fine-tuning of \nbert model to accurately predict drug-target interactions. Pharmaceutics \n14(8):1710\n 44. Hinton G, Vinyals O, Dean J (2015) Distilling the knowledge in a neural \nnetwork. arXiv preprint arXiv: 1503. 02531\n 45. Gou J, Yu B, Maybank SJ, Tao D (2021) Knowledge distillation: a survey. Int \nJ Comput Vision 129:1789–1819\n 46. Geffen Y, Ofran Y, Unger R (2022) Distilprotbert: a distilled protein \nlanguage model used to distinguish between real proteins and their \nrandomly shuffled counterparts. Bioinformatics 38(Supplement–2):95–98\n 47. Romero A, Ballas N, Kahou SE, Chassang A, Gatta C, Bengio Y (2014) \nFitnets: Hints for thin deep nets. arXiv preprint arXiv: 1412. 6550\n 48. Davis MI, Hunt JP , Herrgard S, Ciceri P , Wodicka LM, Pallares G, Hocker M, \nTreiber DK, Zarrinkar PP (2011) Comprehensive analysis of kinase inhibitor \nselectivity. Nat Biotechnol 29(11):1046–1051\n 49. Liu T, Lin Y, Wen X, Jorissen RN, Gilson MK (2007) Bindingdb: a web-acces-\nsible database of experimentally determined protein-ligand binding \naffinities. Nucleic Acids Res 35(suppl-1):198–201\n 50. Saito T, Rehmsmeier M (2015) The precision-recall plot is more informa-\ntive than the roc plot when evaluating binary classifiers on imbalanced \ndatasets. PLoS ONE 10(3):0118432\n 51. Kumar A, Raghunathan A, Jones RM, Ma T, Liang P (2022) Fine-tuning \ncan distort pretrained features and underperform out-of-distribution. \nIn: International Conference on Learning Representations. https:// openr \neview. net/ forum? id= UYneF zXSJWh\n 52. Alain G, Bengio Y (2016) Understanding intermediate layers using linear \nclassifier probes. arXiv preprint arXiv: 1610. 01644\n 53. Chatterjee A, Walters R, Shafi Z, Ahmed OS, Sebek M, Gysi D, Yu R, Eliassi-\nRad T, Barabási A-L, Menichetti G (2021) Ai-bind: improving binding \npredictions for novel protein targets and ligands. arXiv preprint arXiv: \n2112. 13168\n 54. Chen YT, Zou J (2023) Genept: a simple but hard-to-beat foundation \nmodel for genes and cells built from chatgpt. bioRxiv, 2023–10\n 55. Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lacroix T, Rozière \nB, Goyal N, Hambro E, Azhar F, et al (2023) Llama: Open and efficient \nfoundation language models. arXiv preprint arXiv: 2302. 13971\n 56. Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, Barham \nP , Chung HW, Sutton C, Gehrmann S, et al (2022) Palm: scaling language \nmodeling with pathways. arXiv preprint arXiv: 2204. 02311\n 57. Hu EJ, Shen Y, Wallis P , Allen-Zhu Z, Li Y, Wang S, Wang L, Chen W (2021) \nLora: low-rank adaptation of large language models. arXiv preprint arXiv: \n2106. 09685\n 58. Hu J, Shen L, Sun G (2018) Squeeze-and-excitation networks. In: Proceed-\nings of the IEEE Conference on Computer Vision and Pattern Recognition, \npp. 7132–7141\n 59. Sabour S, Frosst N, Hinton GE (2017) Dynamic routing between capsules. \nAdv Neural Inf Proc Syst 30\n 60. Gheini M, Ren X, May J (2021) Cross-attention is all you need: adapting \npretrained transformers for machine translation. arXiv preprint arXiv: 2104. \n08771\n 61. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer \nL, Stoyanov V (2019) Roberta: a robustly optimized bert pretraining \napproach. arXiv preprint arXiv: 1907. 11692\n 62. Shibata Y, Kida T, Fukamachi S, Takeda M, Shinohara A, Shinohara T, \nArikawa S (1999) Byte pair encoding: a text compression scheme that \naccelerates pattern matching\n 63. Rogers D, Hahn M (2010) Extended-connectivity fingerprints. J Chem Inf \nModel 50(5):742–754\n 64. Katharopoulos A, Vyas A, Pappas N, Fleuret F (2020) Transformers are rnns: \nFast autoregressive transformers with linear attention. In: International \nConference on Machine Learning, pp. 5156–5165. PMLR\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8317979574203491
    },
    {
      "name": "Machine learning",
      "score": 0.6890279650688171
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6864997744560242
    },
    {
      "name": "Encoder",
      "score": 0.6341900825500488
    },
    {
      "name": "Inference",
      "score": 0.6028860807418823
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5974140763282776
    },
    {
      "name": "Task (project management)",
      "score": 0.5956664085388184
    },
    {
      "name": "Drug discovery",
      "score": 0.5178565979003906
    },
    {
      "name": "Process (computing)",
      "score": 0.5106979012489319
    },
    {
      "name": "Field (mathematics)",
      "score": 0.4953271448612213
    },
    {
      "name": "Limiting",
      "score": 0.4550006687641144
    },
    {
      "name": "Dual (grammatical number)",
      "score": 0.4412784278392792
    },
    {
      "name": "Computational complexity theory",
      "score": 0.4118097126483917
    },
    {
      "name": "Algorithm",
      "score": 0.1378057599067688
    },
    {
      "name": "Bioinformatics",
      "score": 0.12133759260177612
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I93906172",
      "name": "Anyang University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I4575257",
      "name": "Hanyang University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I118722661",
      "name": "Kyungsung University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I39705031",
      "name": "Catholic University of Daegu",
      "country": "KR"
    }
  ]
}