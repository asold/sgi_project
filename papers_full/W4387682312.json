{
    "title": "Lightweight Structure-Aware Transformer Network for Remote Sensing Image Change Detection",
    "url": "https://openalex.org/W4387682312",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5036558091",
            "name": "Tao Lei",
            "affiliations": [
                "Shaanxi University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5102781756",
            "name": "Yetong Xu",
            "affiliations": [
                "Shaanxi University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5055644335",
            "name": "Hailong Ning",
            "affiliations": [
                "Xi’an University of Posts and Telecommunications"
            ]
        },
        {
            "id": "https://openalex.org/A5047997045",
            "name": "Zhiyong Lv",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5012555970",
            "name": "Chongdan Min",
            "affiliations": [
                "Shaanxi University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5032314861",
            "name": "Yaochu Jin",
            "affiliations": [
                "Bielefeld University"
            ]
        },
        {
            "id": "https://openalex.org/A5010643037",
            "name": "Asoke K. Nandi",
            "affiliations": [
                "Xi'an Jiaotong University",
                "Brunel University of London"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963385418",
        "https://openalex.org/W3207616990",
        "https://openalex.org/W4313438466",
        "https://openalex.org/W2908624219",
        "https://openalex.org/W3036453075",
        "https://openalex.org/W3130754787",
        "https://openalex.org/W2891248708",
        "https://openalex.org/W3015038817",
        "https://openalex.org/W3176330035",
        "https://openalex.org/W3027225766",
        "https://openalex.org/W4387146089",
        "https://openalex.org/W4226361741",
        "https://openalex.org/W3180045188",
        "https://openalex.org/W4312549298",
        "https://openalex.org/W4226228401",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6796538260",
        "https://openalex.org/W2805152403",
        "https://openalex.org/W4313855806",
        "https://openalex.org/W4287890462"
    ],
    "abstract": "Supplemental Items: The supplementary materials include experiments on the DSIFN-CD dataset and ablation experiments on the CDD dataset to validate our LSAT method. Furthermore, we also analyze the effect of pre-training on the performance. DOI:10.1109/LGRS.2023.3323534/mm1 .",
    "full_text": "1\nLightweight Structure-aware Transformer Network\nfor VHR Remote Sensing Image Change Detection\nTao Lei, Senior Member, IEEE, Yetong Xu, Hailong Ning, Zhiyong Lv, Chongdan Min, Yaochu Jin,\nFellow, IEEE, and Asoke K. Nandi, Life Fellow, IEEE\nAbstract—Popular Transformer networks have been success-\nfully applied to remote sensing (RS) image change detection (CD)\nidentifications and achieve better results than most convolutional\nneural networks (CNNs), but they still suffer from two main\nproblems. First, the computational complexity of the Transformer\ngrows quadratically with the increase of image spatial resolution,\nwhich is unfavorable to very high-resolution (VHR) RS images.\nSecond, these popular Transformer networks tend to ignore\nthe importance of fine-grained features, which results in poor\nedge integrity and internal tightness for largely changed objects\nand leads to the loss of small changed objects. To address the\nabove issues, this Letter proposes a Lightweight Structure-aware\nTransformer (LSAT) network for RS image CD. The proposed\nLSAT has two advantages. First, a Cross-dimension Interactive\nSelf-attention (CISA) module with linear complexity is designed\nto replace the vanilla self-attention in visual Transformer, which\neffectively reduces the computational complexity while improving\nthe feature representation ability of the proposed LSAT. Second,\na Structure-aware Enhancement Module (SAEM) is designed to\nenhance difference features and edge detail information, which\ncan achieve double enhancement by difference refinement and\ndetail aggregation so as to obtain fine-grained features of bi-\ntemporal RS images. Experimental results show that the proposed\nLSAT achieves significant improvement in detection accuracy and\noffers a better tradeoff between accuracy and computational costs\nthan most state-of-the-art CD methods for VHR RS images.\nIndex Terms—Change detection, deep learning, remote sensing\nimage, Transformer.\nI. I NTRODUCTION\nR\nEMOTE sensing (RS) image change detection (CD) aims\nto obtain information on surface changes in different\nperiods but the same geographical area [1]. It has been widely\nThis work was supported in part by National Natural Science Foundation\nof China (Program No. 62271296, 62201452), in part by the Natural Science\nBasic Research Program of Shaanxi (Program No. 2021JC-47, 2022JQ-592),\nin part by Key Research and Development Program of Shaanxi (Program\nNo. 2022GY-436, 2021ZDLGY08-07), in part by Shaanxi Joint Laboratory\nof Artificial Intelligence (Program No. 2020SS-03), and in part by Scientific\nResearch Program Funded by Shaanxi Provincial Education Department\n(Program No. 22JK0568). (Corre-sponding author: Hailong Ning.)\nT. Lei, Y . Xu and C. Min are with the Shaanxi Joint Laboratory\nof Artificial Intelligence, Shaanxi University of Science and Technology,\nXi’an 710021,China. (e-mail: leitao@sust.edu.cn; xuyetong1999@163.com;\nbs221611001@sust.edu.cn).\nH. Ning is with the School of Computer Science and Technology, Xi’an\nUniversity of Posts and Telecommunications, Xi’an 710121, China. (e-mail:\nninghailong93@gmail.com).\nZ. Lv is with the School of Computer Science and Engineering,\nXi’an University of Technology, Xi’an 710048, China. (E-mail: Lvzhiy-ong\nfly@hotmail.com).\nY . Jin is with the Faculty of Technology, Bielefeld University, 33619\nBielefeld, Germany (email: yaochu.jin@uni-bielefeld.de).\nA. K. Nandi is with the Department of Electronic and Electrical Engi-\nneering, Brunel University London, Uxbridge, Middlesex, UB8 3PH, U.K.,\nand visiting professor with Xi’an Jiaotong University, Xi’an 710049, China.\n(e-mail: asoke.nandi@brunel.ac.uk).\napplied in urban sprawl monitoring [2], land cover monitoring\n[3], and disaster assessment [4]. Currently, CD has become a\nsignificant research direction in the field of RS.\nIn recent years, deep learning techniques based on CNNs\nhave shown excellent performance in VHR RS image CD tasks\n[5,6]. For example, Daudt et al. [7] first applied Siamese full\nconvolutional networks (FCNs) to the CD and proposed three\nend-to-end networks, which established the mainstream frame-\nwork for subsequent CD. Due to the complexity of RS image\nscenes and susceptibility to light and environment changes,\nvarious multi-scale feature fusion modules and attention mech-\nanisms have been introduced into the Siamese network [8, 9,\n10], which achieves satisfactory results. However, although\nCNN-based methods are effective in extracting discriminative\nfeatures for CD, they struggle to model long-range contextual\ninformation in bi-temporal RS images.\nRecently, Vision Transformer (ViT) has been successfully\napplied to CD tasks due to its excellent ability of capture long-\nrange dependency relationships. For instance, Chen et al. [11]\nproposed a bi-temporal image transformer (BIT) method to\nmodel the long-range contextual information in bi-temporal\nimages. Different from the original ViT model, BIT uses the\nsemantic token to represent the input image features, and\nthus it has fewer parameters and lower computational costs.\nAlthough BIT achieves good change detection results for VHR\nRS images, it is not a pure Transformer network due to the\nemployment of ConvNets in its encoder. To solve the problem,\nChangeFormer [12] drops the ConvNets encoder in BIT and\nonly uses a Transformer encoder and a lightweight MLP de-\ncoder. Consequently, ChnageFormer provides better CD results\nthan BIT. Unlike the above work, SwinSUNet [13] proposes\na pure Swin transformer [14] network with Siamese U-shaped\nstructure, which also achieves good results for VHR RS image\nCD. The above methods perform well in CD tasks but still\nsuffer from two major challenges. First, the computational\ncomplexity of most Transformer-based CD methods grows\nquadratically with the increase of image spatial resolution,\nmaking it difficult to train a Transformer network for VHR RS\nimage CD. Second, existing Transformer-based CD methods\n(e.g., BIT and ChangeFormer) ignore the importance of fine-\ngrained information, resulting in suboptimal edge integrity and\ninternal tightness for largely changed objects and the missed\ndetection for small changed objects.\nTo tackle the above challenges, this Letter proposes a\nLightweight Structure-aware Transformer (LSAT) method for\nVHR CD. Our LSAT is a U-shaped structure consisting of\na dual-branch weight-sharing encoder and a single-branch\ndecoder. Both the encoder and decoder are composed of Cross-\narXiv:2306.01988v1  [cs.CV]  3 Jun 2023\nThis arXiv preprint has been prepared for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. It has not been certified by \npeer review. Citation information: DOI10.1109/LGRS.2023.3323534, IEEE Geoscience and Remote Sensing Letters\nCopyright © 2023 The Authors. This is a preprint made available under the arXiv.org - Non-exclusive license to distribute, see: https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html.  \nPersonal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or \npromotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. See: https://\njournals.ieeeauthorcenter.ieee.org/become-an-ieee-journal-author/publishing-ethics/guidelinesand-policies/post-publication-policies/\n2\n(a) LSAT\nWeight sharing encoder\nCC\nPre-image\nChange map\nEE E\nPost-image\nDecoderDecoder\nWeight sharing encoder\nC\nPre-image\nChange map\nEE E\nPost-image\nDecoder\nLightweight Transformer block with CISA\nPVTv2PVTv2\nMax pooling 22Max pooling 22\nBilinear InterpolationBilinear Interpolation\nLightweight Transformer block with CISA\nPVTv2\nMax pooling 22\nBilinear Interpolation\nE SAEME SAEM\nAFMAFM\nUpsamplingUpsampling\nLightweight Transformer block with CISA\nPVTv2\nMax pooling 22\nBilinear Interpolation\nE SAEM\nAFM\nUpsampling\nContribution 2Lightweight Transformer block with CISA\nPVTv2\nMax pooling 22\nBilinear Interpolation\nE SAEM\nAFM\nUpsampling\nContribution 2\n  ConcatenateCC   ConcatenateC\n(b) CISA Softmax &Matrix MultiplicationSoftmax &Matrix MultiplicationSigmoid & Multiplication\n•\n•Sigmoid & Multiplication\n•\n(b) CISA\n (H W)  C C C C (H W) C H W \n W C H\nK\nAvgPool\nMaxPool\n 2CH 1CH\n•\n H C W\nAvgPool\nMaxPool\n 2CW  1CW\nC H W \nC H W \nC×(H ×W)\nC (H W)Q\nVV\nChannel-Channel Interaction attention\nChannel-Height interaction attention\nChannel-Width interaction attention\nC  H W C  H W C  H W \n1A\n2A\n3A\nConv\nConv\n1\n2\n3\n•\n•\n(b) CISA\n (H W)  C C C C (H W) C H W \n W C H\nK\nAvgPool\nMaxPool\n 2CH 1CH\n•\n H C W\nAvgPool\nMaxPool\n 2CW  1CW\nC H W \nC H W \nC×(H ×W)\nC (H W)Q\nV\nChannel-Channel Interaction attention\nChannel-Height interaction attention\nChannel-Width interaction attention\nC  H W C  H W \n1A\n2A\n3A\nConv\nConv\n1\n2\n3\n•\n(b) CISA Softmax &Matrix MultiplicationSigmoid & Multiplication\n•\n(b) CISA\n (H W)  C C C C (H W) C H W \n W C H\nK\nAvgPool\nMaxPool\n 2CH 1CH\n•\n H C W\nAvgPool\nMaxPool\n 2CW  1CW\nC H W \nC H W \nC×(H ×W)\nC (H W)Q\nV\nChannel-Channel Interaction attention\nChannel-Height interaction attention\nChannel-Width interaction attention\nC  H W C  H W \n1A\n2A\n3A\nConv\nConv\n1\n2\n3\n•\n(a) LSAT\nWeight sharing encoder\nC\nPre-image\nChange map\nEE E\nPost-image\nDecoder\nLightweight Transformer block with CISA\nPVTv2\nMax pooling 22\nBilinear Interpolation\nE SAEM\nAFM\nUpsampling\nContribution 2\n  ConcatenateC\n(b) CISA Softmax &Matrix MultiplicationSigmoid & Multiplication\n•\n(b) CISA\n (H W)  C C C C (H W) C H W \n W C H\nK\nAvgPool\nMaxPool\n 2CH 1CH\n•\n H C W\nAvgPool\nMaxPool\n 2CW  1CW\nC H W \nC H W \nC×(H ×W)\nC (H W)Q\nV\nChannel-Channel Interaction attention\nChannel-Height interaction attention\nChannel-Width interaction attention\nC  H W C  H W \n1A\n2A\n3A\nConv\nConv\n1\n2\n3\n•\nContribution 1\ninX\noutX\n1pX\n2pX\nFig. 1. Overall network architecture. (a) LSAT architecture. (b) The specific architecture of the proposed CISA. The encoder is composed of\na lightweight Transformer block using CISA. The CISA is composed of three branches of channel interaction attention, the first channel A1\nis the Channel-Channel attention branch, the second channel A2 is the Channel-Height interaction attention branch, and the third channel\nA3 is the Channel-Width interaction attention branch.\ndimension Interactive Self-attention (CISA) module with lin-\near complexity. To integrate detailed change information from\nthe dual-branch at each level into the corresponding decoding\nlayer, we propose a Structure-aware Enhancement Module\n(SAEM) between the encoder and the decoder. In addition,\nan Attention-based Fusion Module (AFM) is added after\nthe encoding layer to fuse efficiently the bi-temporal deep\nsemantic features.\nThe main contributions of this Letter are summarized as\nfollows:\n1) A Cross-dimension Interactive Self-attention (CISA)\nmodule with linear complexity is proposed to achieve\na lightweight Transformer network. Different from the\nvanilla self-attention module, our CISA not only ef-\nfectively reduces the computational complexity of net-\nworks, but also achieves excellent change detection\naccuracy for VHR RS images CD.\n2) A Structure-aware Enhancement Module (SAEM) is\ndesigned to learn fine-grained features and improve the\ninternal tightness of the largely changed objects. Differ-\nent from regular single-branch difference enhancement\nmethods, SAEM achieves a double enhancement by\ncalibrating the difference and co-detail features with a\ndual-branch architecture.\n3) An efficient Lightweight Structure-aware Transformer\n(LSAT) network is proposed based on the employ-\nment of both CISA and SAEM. Extensive experiments\non two typical datasets in CD demonstrate that the\nproposed LSAT consistently outperforms other state-\nof-the-art (SOTA) networks in detection accuracy and\ncomputational costs.\nII. METHODOLOGY\nThe framework of the proposed LSAT includes three main\nmodules as shown in Fig.1(a). First, the encoder with the CISA\nmodule is mainly used to extract hierarchical semantic features\nof bi-temporal RS images, and the SAEM module is used\nfor enhancing the fine-grained difference features. Second,\nthe attention-based fusion module (AFM) is mainly used to\ngenerate fine-grained change maps. Third, the fine-grained\ndifference features and bi-temporal deep semantic features are\nintegrated into the decoder to output changed objects in VHR\nRS images.\nA. Cross-dimension Interactive Self-attention\nIn vanilla self-attention (SA), the computational complexity\nof the key-query dot-product interaction grows quadratically\nwith the increasing spatial resolution of input images, as\nO((WH )2) for images of size W × H. This quadratic com-\nplexity greatly increases the training difficulty of the network\nfor the VHR CD task. To solve the problem, we design a\nCross-channel Interactive Self-attention (CISA) module with\nlinear complexity as an alternative to the vanilla SA in the\nproposed Transformer architecture. The architecture of CISA\nmodule is illustrated in Fig. 1(b).\nFirst, inspired by CvT [15], query (Q), key (K) and value\n(V) are generated by depth-wise separable convolutional pro-\njections but not the linear projection, which not only strength-\nens the connection of local contexts, but also reduces semantic\nambiguity caused by the vanilla self-attention mechanism.\nNext, the inter-channel encoding is performed to generate\na Channel-Channel attention map A1. In addition, to capture\ncross-channel long-dependency relationships, the two-branch\ninteraction attentions of Channel-Height A2 and Channel-\nWidth A3 are conducted on features maps to enhance the\ncross-dimension interactions between channel and spatial di-\nmensions. It can improve the global information extraction\nability of the model. Each of the three types of attention can\nbe expressed as:\nA1(Q, K, V) = Softmax(QKT\na )V, (1)\nA2(C, H) = Sigmoid(C1(P(Xp1)))Xp1, (2)\nA3(C, W) = Sigmoid(C1(P(Xp2)))Xp2, (3)\nwhere Xp1 ∈ W×C×H and Xp ∈ W×C×H are the tensors\nafter performing the dimensional transformation for the input\nThis arXiv preprint has been prepared for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. It has not \nbeen certified by peer review. Citation information: DOI10.1109/LGRS.2023.3323534, IEEE Geoscience and Remote Sensing Letters\nCopyright © 2023 The Authors. This is a preprint made available under the arXiv.org - Non-exclusive license to distribute, see: https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html.  \nPersonal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or \npromotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. See: https://\njournals.ieeeauthorcenter.ieee.org/become-an-ieee-journal-author/publishing-ethics/guidelinesand-policies/post-publication-policies/\n3\n--\n+\ncc\naggrf\ndifff\noutf\n- Sub&Abs- Sub&Abs+ Addition+ Addition+ Addition ConcatenateConcatenatecc Concatenatec - Sub&Abs+ Addition Concatenatec\nDetail aggregation branch\n Difference refinement branch\n1× 1Conv3× 3Conv\n3× 3Conv\n3× 3Conv\n3× 3Conv\n1× 1Conv\nSimAMSimAM\nSimAMSimAM\nSimAMSimAM\nSimAMSimAM\nSimAMSimAM\nSimAMSimAM\n++\n1_ if\n2_ if\n++\n-\n+\nc\naggrf\ndifff\noutf\n- Sub&Abs+ Addition Concatenatec\nDetail aggregation branch\n Difference refinement branch\n1× 1Conv3× 3Conv\n3× 3Conv\n3× 3Conv\n3× 3Conv\n1× 1Conv\nSimAM\nSimAM\nSimAM\nSimAM\nSimAM\nSimAM\n+\n1_ if\n2_ if\n+\nFig. 2. SAEM structure, f1 i, f2 i are the dual-time phase output\nfeatures of each layer of the encoder. fdiff is the enhanced feature\nof different-detail branch and faggr is the enhanced feature of detail\naggregation branch.\nfeature Xin, respectively. C1 is a 1×1 convolution operation,\nP represents the parallel operation of maximum pooling and\naverage pooling. The final attention is computed as follows:\nA = λ1A1 + λ2A2 + λ3A3, (4)\nwhere λ1, λ2 and λ3 are the pre-defined hyperparameters.\nNote that the cross-channel self-attention in the A1 branch\nof the Q, KT dot-product interaction generates a transposed\nattention map of size RC×C instead of the conventional\nattention map of size RHW ×HW . In addition, the other two\nbranches are mainly convolution operations, which have low\ncomputational complexity. Therefore, the computational com-\nplexity of CISA is O(C2 + CH + CW ) much smaller than\nthe conventional quadratic complexity O((WH )2). To verify\nthe effectiveness of the proposed CISA module, we present\nthe experiments in Section III.\nB. Structure-aware Enhancement Module\nIn existing CD networks, the importance of fine-grained\nfeatures is often ignored, resulting in poor edge integrity of\nobjects with large-size changes and undetectable edge integrity\nof objects with small-size changes. To solve this problem, the\ndifference image (DI) is often used to enhance the details of\nchanged objects. However, single-branch difference enhance-\nment methods, such as performing subtraction operation on\nbi-temporal images or employing an attention mechanism to\nimprove feature extraction of bi-temporal images [5, 16]. This\nenhancement method is suboptimal because of the limited\nfeature expression ability of single-branch enhancement and\na high computational cost of the attention. Therefore, we\npropose a lightweight Structure-aware Enhancement Module\n(SAEM) to learn the difference information about CDs com-\npletely. Different from single-branch difference enhancement\nmethods, SAEM performs double enhancement by using dual-\nbranch to learn fine-grained features. Fig. 2. shows the details\nof SAEM.\nOur SAEM consists of two branches: the difference refine-\nment and the detail aggregation. In the difference refinement\nbranch, the bi-temporal features are further enhanced by using\nconvolution operation, and then a lightweight 3D attention\nSimAM [17] is used to generate finer-grained features and\nimprove the contours of detection results. This process can be\nexpressed as follows:\nfdiff = Ma |Ma(C3(f1\ni)) − Ma(C3(f2 i))|, (5)\nwhere f1 i, f2 i are the pre-temporal and post-temporal fea-\ntures at i-th level respectively, C3 is a 3 × 3 convolution\noperation, Ma is a lightweight attention mechanism, and |·|\ndenotes the absolute value operation to ensure the availability\nof obtained difference features.\nThe detail aggregation branch is subdivided into two path-\nways, one for enhancing the detail information by adding up\nthe convolved features. The other pathway is to concatenate\nthe convolution features and then use attention to extract\nricher detailed information. The two branches are computed\nas follows:\nfa1 = C1(C3(f1\ni) ⊕ C3(f2 i)), (6)\nfa2 = Ma[C3(f1 i); C3(f2 i)], (7)\nwhere [;] is the concatenation operation. Finally, after di-\nmensionality reduction, the attention is used to aggregate the\ndetailed information of the two pathways, and it is defined as:\nfaggr = Ma(fa1 ⊕ Ma(fa2)). (8)\nThe finally aggregated features contain richer edge details\nthan that generated by a single-branch module of the bi-\ntemporal RS images. The SAEM can reduce the erroneous\nchanges caused by noise and misalignment, thus obtaining\nmore useful fine-grained change features and increasing the\nrobustness of our proposed network. The final output is\ncalculated as follows:\nfout = fdiff ⊕ faggr. (9)\nTo verify the effectiveness of the proposed SAEM, the\nexperiments in Section III.\nIII. EXPERIMENTS\nA. Datasets and Evaluation Metrics\nIn this Letter, experiments are conducted on two publicly\navailable large CD datasets, LEVIR-CD [10] and CDD [18].\nLEVIR-CD dataset contains 637 VHR Google Earth image\npairs with a resolution of 0.5m, and with a size of 1024×1024.\nTo prevent overfitting, data enhancement operations such as\nrandom rotation and random cropping are performed in this\nLetter, and the images are randomly cropped into patches with\nsize of 256×256. 10,000 pairs are set aside for training, 1,024\npairs for validation, and 2,048 pairs for testing.\nCDD dataset is a RS image with seasonal variation of the\nsame region acquired by Google Earth, and a total of 16,000\npairs of image pairs with a size of 256 × 256 are obtained\nthrough random cropping and data enhancement, of which\n10,000 pairs are used for training, 3,000 pairs for validation,\nand the remaining 3,000 pairs for testing.\nIn this Letter, three main evaluation metrics, including Preci-\nsion (Pre), Recall (Rec), F1-Score (F1) and Distance from the\nideal position (DIP) [19] are used to evaluate comprehensively\nthe network, where F1 and DIP comprehensively consider the\ntwo metrics Pre and Rec in different ways.\nThis arXiv preprint has been prepared for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. It has not been \ncertified by peer review. Citation information: DOI10.1109/LGRS.2023.3323534, IEEE Geoscience and Remote Sensing Letters\nCopyright © 2023 The Authors. This is a preprint made available under the arXiv.org - Non-exclusive license to distribute, see: https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html.  \nPersonal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or \npromotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. See: https://\njournals.ieeeauthorcenter.ieee.org/become-an-ieee-journal-author/publishing-ethics/guidelinesand-policies/post-publication-policies/\n4\nB. Implementation Details\nIn this Letter, the proposed method is implemented by using\nPyTorch and trained for 200 epochs on an NVIDIA GeForce\nRTX 3090 GPU. The training process is performed using\nthe AdamW optimizer and the momentum is set to 0.99.\nThe weight decay is set to 0.0005 and the initial learning\nrate was 0.0001. Experiments show that 2:1:1 is the best\nvalue for λ1 : λ2 : λ3, and the default value for subsequent\nexperiments is 2:1:1. The pretrained PVTv2-B1 [20] is used\nfor preliminary feature extraction. To mitigate the effect of the\ncategory imbalance problem, this Letter combines the binary\ncross-entropy loss with the Dice loss for optimizing the model.\nC. Comparison with State-of-the-art\n1) Comparison methods: To demonstrate the effectiveness\nof our proposed LSAT, some popular SOTA networks are used\nfor comparison, including FCN-PP [4], IFNet [5], STANet\n[10], FDCNN [8], SNUNet [6], DSAMNet [9], BIT [11],\nChangeFormer [12], and SwinSUNet [13].\n2) Comparison on the LEVIR-CD dataset:The quantita-\ntive evaluation results of the LEVIR-CD dataset are shown in\nTable I. The best values are shown in bold for all the following\ntables. It can be seen that the proposed LSAT almost obtains\nthe best results. Compared to ChangeFormer, our network\nachieves 1.12%/1.24% higher in F1 and DIP, respectively. To\nfurther illustrate the superiority of LSAT, a visual analysis\nis conducted as shown in Fig. 3(a). Although most of the\ncomparison methods show missed detection and false multiple\ndetections, the proposed LSAT method has better detection\nresults. It can be observed that our LSAT achieves the best\nperformance in the integrity and accuracy of changed objects.\nTABLE I\nPERFORMANCE COMPARISON ON THE LEVIR-CD TEST SET .\nMethod Type Network Pre(%) Rec(%) F1(%) DIP(%)\nCNN\nFCN-PP [4] 80.31 89.48 84.64 84.21\nSTANet [10] 86.14 89.39 87.73 87.65\nIFNet [5] 89.73 86.06 87.80 87.75\nFDCNN [8] 82.99 88.71 85.76 85.56\nSNUNet [6] 89.06 87.53 88.29 88.27\nDSAMNet [9] 82.75 88.39 85.48 85.29\nTransformer\nBIT [11] 89.24 89.37 89.31 89.30\nChangeFormer [12] 92.05 88.80 90.40 90.28\nSwinSunet [13] 90.51 89.72 90.11 90.10\nLSAT(ours) 91.81 91.24 91.52 91.52\n3) Comparison on the CDD dataset:In Table II, the pro-\nposed LSAT also obtains the best results on the CDD dataset,\ncompared to SwinSUNet, our LSAT achieves 1.12%/2.04%\nhigher in F1 and DIP, respectively. A clear visual analysis is\nshown in Fig. 3(b). For the complex change region detection,\nthe comparative networks provide detection results showing\nleakages and inaccuracy on contours, while our LSAT still\nprovides the best detection contours.\nAll the above experimental results show that our LSAT\ncan effectively improve the performance of VHR CD since\nboth long-range contextual information and fine-grained in-\nformation are effectively captured by the proposed LSAT.\nIn this way, we can enhance the edge integrity and internal\ntightness for changed objects, and reduce the missed detection\nfor small changed objects. Furthermore, we conducted more\nexperiments in our supplement materials.\nTABLE II\nPERFORMANCE COMPARISON ON THE CDD TEST SET .\nMethod Type Network Pre(%) Rec(%) F1(%) DIP(%)\nCNN\nFCN-PP [4] 81.69 90.31 85.78 85.35\nSTANet [10] 88.98 93.11 91.00 90.80\nIFNet [5] 90.72 86.50 88.56 88.41\nFDCNN [8] 83.61 91.70 87.47 87.01\nSNUNet [6] 90.92 94.75 92.79 92.58\nDSAMNet [9] 91.67 94.83 93.22 93.06\nTransformer\nBIT [11] 92.89 94.02 93.45 93.43\nChangeFormer [12] 94.26 93.46 93.84 93.84\nSwinSUNet [13] 95.70 92.30 94.00 93.76\nLSAT(ours) 97.02 94.87 95.93 95.80\nD. Ablation Study\nTo verify the effectiveness of the proposed modules, we\nperformed ablation experiments on the LEVIR-CD for CISA,\nSAEM, and AFM modules, respectively. The specific results\nare shown in Table III. Siamese-unet is considered as the\nbaseline. From the experimental results, we can see that the\nproposed CISA not only improves the F1 score by nearly\n2.79% over the baseline, but also significantly reduces the\ncomputational complexity. When adding SAEM to CISA for\nadequately capturing and fusing the multi-scale detail informa-\ntion of changed objects, it can improve the F1 score by nearly\n3.66% over the baseline. The AFM is added after the last layer\nof feature extraction on the basis of CISA to effectively fuse\nthe deep semantic features, which further improves the F1\nscore by 3.49%. The ablation experiments fully demonstrate\nthe effectiveness of our proposed module of CISA, SAEM and\nAFM.\nWe validated the effectiveness of SAEM by visualizing\nthe features in Fig. 4. From the visualization of the feature\nmap, we can see that SAEM locates the change features more\naccurately and obtains a more refined change feature.\nTABLE III\nABLATION EXPERIMENTS OF OUR PROPOSED MODULE ON THE\nLEVIR-CD DATASET.\nMethods LEVIR CD\nPre (%) Rec(%) F1(%) FLOPs(G) Params(M)\nBase 89.40 85.78 87.55 18.04 7.76\nBase+CISA 90.78 89.90 90.34 7.08 14.94\nBase+CISA+SAEM 91.02 91.40 91.21 7.73 15.95\nBase+CISA+AFM 91.21 90.86 91.04 7.19 16.27\nLSAT 91.81 91.24 91.52 7.79 16.91\nE. Model Efficiency\nThe purpose of this Letter is to reduce the computational\ncomplexity of Transformer-based CD networks and achieve\nhigh detection accuracy. We analyzed and compared the results\nin terms of floating-point operations (FLOPs), number of\nparameters (Params), and F1-score (F1), which were given\nin Table IV . We can see that the FLOPs of the proposed\nLSAT are still smaller than those of most comparison net-\nworks, and the detection accuracy F1 is the best, which can\nThis arXiv preprint has been prepared for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. It has not been \ncertified by peer review. Citation information: DOI10.1109/LGRS.2023.3323534, IEEE Geoscience and Remote Sensing Letters\nCopyright © 2023 The Authors. This is a preprint made available under the arXiv.org - Non-exclusive license to distribute, see: https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html.  \nPersonal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or \npromotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. See: https://\njournals.ieeeauthorcenter.ieee.org/become-an-ieee-journal-author/publishing-ethics/guidelinesand-policies/post-publication-policies/\n5\n(b)\nPre image     Post image     label     LSAT     FCN-PP    STANet     IFN     FDCNN     SNUNet    DSAMNet     BIT     ChangeFormer    SwinSunet\n(a)\n(b)\nPre image     Post image     label     LSAT     FCN-PP    STANet     IFN     FDCNN     SNUNet    DSAMNet     BIT     ChangeFormer    SwinSunet\n(a)\nFig. 3. Visual analysis of the detection results, (a)LEVIR-CD, (b)CDD.\npre image post image  label Sub & abs \ndifff\naggrf\noutf\npre image post image  label Sub & abs \ndifff\naggrf\noutf\nFig. 4. Feature visualization. It is clear that SAEM provides better feature maps than vanilla difference operation.\nverify the effectiveness of the proposed method. To enable\na fair comparison, we replaced the PVT-v2 used in LSAT for\nResNet18 in BIT, and we can see that LSAT+Res has a smaller\nnumber of parameters and computational costs compared to\nthe lightweight BIT, along with better detection accuracy.\nTABLE IV\nCOMPARATIVE RESULTS OF MODEL EFFICIENCY ON THE LEVIR-CD\nDATASET.\nMethod Type Network F1(%) FLOPs(G) Params(M)\nCNN\nFCN-PP 84.64 34.65 28.13\nSTANet 87.73 6.58 16.93\nIFNet 87.80 41.18 50.71\nFDCNN 85.76 32.40 13.71\nSNUNet 88.29 33.04 12.03\nDSAMNet 85.48 75.29 16.95\nTransformer\nBIT 89.31 8.44 6.93\nChangeFormer 90.40 202.83 41.01\nSwinSunet 90.11 11.19 40.95\nLSAT (ours) 91.52 7.79 16.91\nLSAT+Res 90.21 6.64 6.34\nIV. CONCLUSION\nIn this Letter, we have proposed a lightweight structure-\naware network LSAT for the VHR RS image CD. The pro-\nposed LSAT employs a cross-channel interactive self-attention\n(CISA) module with linear complexity, which solves the prob-\nlem of quadratic complexity in self-attention mechanism. In\naddition, fine-grained change information is obtained using an\neffective Structure Awareness Enhancement Module (SAEM)\nand Attention-based fusion module (AFM). Experiments on\nthe publicly available large remote sensing image change\ndetection datasets LEVIR-CD and CDD fully demonstrate the\neffectiveness of the proposed LSAT.\nREFERENCES\n[1] A. Sebastian, T. Tuma, N. Papandreou, et al., “Temporal correlation\ndetection using computational phase-change memory,” Nat. Commun.,\nvol. 8, no. 1, pp. 1-10, 2017.\n[2] S. Hafner, A. Nascetti, H. Azizpour and Y . Ban, “Sentinel-1 and Sentinel-\n2 Data Fusion for Urban Change Detection Using a Dual Stream U-Net,”\nIEEE Geosci. Remote Sens. Lett., vol. 19, pp. 1-5, 2022.\n[3] Q. Zhu, X. Guo, W. Deng, S. Shi, et al. “Land-use/land-cover change\ndetection based on a Siamese global learning framework for high spatial\nresolution remote sensing imagery,” ISPRS J. Photogramm. Remote Sens.,\nvol. 184, pp. 63-78, 2022.\n[4] T. Lei, Y . Zhang, Z. Lv, S. Li, S. Liu and A. K. Nandi, “Landslide\nInventory Mapping From Bitemporal Images Using Deep Convolutional\nNeural Networks,” IEEE Geosci. Remote Sens. Lett., vol. 16, no. 6, pp.\n982-986, 2019.\n[5] C. Zhang, P . Y ue, D. Tapete, et al.“A deeply supervised image fusion\nnetwork for change detection in high resolution bi-temporal remote\nsensing images,” ISPRS J. Photogramm. Remote Sens., vol. 166, pp.183-\n200, 2020.\n[6] Fang, K. Li, J. Shao, and Z. Li, “Snunet-cd: A densely connected siamese\nnetwork for change detection of vhr images,” IEEE Geosci. Remote Sens.\nLett., vol. 19, pp. 1–5, 2022.\n[7] R. Caye Daudt, B. Le Saux and A. Boulch, “Fully Convolutional Siamese\nNetworks for Change Detection,” in Proc. Int. Conf. Image Process.\n(ICIP), 2018, pp. 4063-4067.\n[8] M. Zhang and W. Shi, “A feature difference convolutional neural network-\nbased change detection method,” IEEE Trans. Geosci. Remote Sens., vol.\n58, no. 10, pp. 7232-7246, 2020.\n[9] Q. Shi, M. Liu, S. Li et al. “A deeply supervisedattention metric-based\nnetwork and an open aerial image dataset for remote sensing change\ndetection,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1-16, 2021.\n[10] H. Chen and Z. Shi, “A spatial-temporal attention-based method and\nanew dataset for remote sensing image change detection,” Remote Sens.,\nvol. 12, no. 10, p. 1662, 2020.\n[11] H. Chen, Z. Qi, and Z. Shi, “Remote sensing image change detection\nwith transformers,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1-14,\n2022.\n[12] W. G. C. Bandara and V . M. Patel, “A Transformer-Based Siamese\nNetwork for Change Detection,” in Proc. IEEE Int. Geosci. Remote Sens.\nSymp. (IGARSS), 2022, pp.207-210.\n[13] C. Zhang, L. Wang, S. Cheng and Y . Li, “SwinSUNet: Pure Transformer\nNetwork for Remote Sensing Image Change Detection,” IEEE Trans.\nGeosci. Remote Sens., vol. 60, pp. 1-13, 2022.\n[14] Z. Liu et al. “Swin Transformer: Hierarchical Vision Transformer using\nShifted Windows,” in IEEE Int. Conf. Comput. Vis. (ICCV), 2021, pp.\n9992-10002.\n[15] H. Wu, B.Xiao, N.C. Codella, and L. Zhang, “CvT: Introducing Convo-\nlutions to Vision Transformers,” in IEEE Int. Conf. Comput. Vis. (ICCV),\n2021, PP. 22-31.\n[16] Zhang, L., Hu, X., Zhang, M., Shu, Z., Zhou, H, “Object-level change\ndetection with a dual correlation attention-guided detector,” ISPRS J.\nPhotogramm. Remote Sens., vol. 177, pp. 147–160, 2021.\n[17] L. Yang, R. Zhang, L. Li, and X. Xie, “SimAM: A Simple, Parameter-\nFree Attention Module for Convolutional Neural Networks,” in Proc. Int.\nConf. Mach. Learn. (PMLR), 2021, pp. 11863-11874.\n[18] M. A. Lebedev, Y . V . Vizilter, O. V . Vygolov, V . A. Knyaz, and A.\nY . Rubis, “Change detection in remote sensing images using conditional\nadversarial networks,” ISPRS Int. Arch. Photogramm. Remote Sens. Spat.\nInf. Sci., vol. 42, no. 2, pp. 565-571, 2018.\n[19] A. K. Nandi, From Multiple Independent Metrics to Single Performance\nMeasure Based on Objective Function,” IEEE Access, vol. 11, pp. 3899-\n3913, 2023.\n[20] W. Wang, E. Xie, X. Li, et al, “PVT v2: Improved baselines with\nPyramid Vision Transformer,” Comput Vis Media., vol. 8, no.3, pp.\n415–424, 2022.\nThis arXiv preprint has been prepared for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. It has not been \ncertified by peer review. Citation information: DOI10.1109/LGRS.2023.3323534, IEEE Geoscience and Remote Sensing Letters\nCopyright © 2023 The Authors. This is a preprint made available under the arXiv.org - Non-exclusive license to distribute, see: https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html.  \nPersonal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or \npromotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. See: https://\njournals.ieeeauthorcenter.ieee.org/become-an-ieee-journal-author/publishing-ethics/guidelinesand-policies/post-publication-policies/"
}