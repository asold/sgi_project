{
    "title": "Pragmatic Neural Language Modelling in Machine Translation",
    "url": "https://openalex.org/W1573488949",
    "year": 2015,
    "authors": [
        {
            "id": "https://openalex.org/A164460222",
            "name": "Paul Baltescu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A297118547",
            "name": "Phil Blunsom",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2171928131",
        "https://openalex.org/W2158049734",
        "https://openalex.org/W36903255",
        "https://openalex.org/W2951714314",
        "https://openalex.org/W2124807415",
        "https://openalex.org/W2949679234",
        "https://openalex.org/W2089745520",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W141304052",
        "https://openalex.org/W1970689298",
        "https://openalex.org/W2152808281",
        "https://openalex.org/W2949888546",
        "https://openalex.org/W2950797609",
        "https://openalex.org/W2251071050",
        "https://openalex.org/W1965154800",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2172140247",
        "https://openalex.org/W2134800885",
        "https://openalex.org/W2251682575",
        "https://openalex.org/W2250489405",
        "https://openalex.org/W932413789",
        "https://openalex.org/W2131462252",
        "https://openalex.org/W2083545877",
        "https://openalex.org/W2060108852",
        "https://openalex.org/W2950075229",
        "https://openalex.org/W2121227244"
    ],
    "abstract": "This paper presents an in-depth investigation on integrating neural language models in translation systems. Scaling neural language models is a difficult task, but crucial for real-world applications. This paper evaluates the impact on end-to-end MT quality of both new and existing scaling techniques. We show when explicitly normalising neural models is necessary and what optimisation tricks one should use in such scenarios. We also focus on scalable training algorithms and investigate noise contrastive estimation and diagonal contexts as sources for further speed improvements. We explore the trade-offs between neural models and back-off n-gram models and find that neural models make strong candidates for natural language applications in memory constrained environments, yet still lag behind traditional models in raw translation quality. We conclude with a set of recommendations one should follow to build a scalable neural language model for MT.",
    "full_text": null
}