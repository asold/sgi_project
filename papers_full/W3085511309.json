{
    "title": "UPB at SemEval-2020 Task 6: Pretrained Language Models for Definition Extraction",
    "url": "https://openalex.org/W3085511309",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A4379243444",
            "name": "Andrei-Marius AVRAM",
            "affiliations": [
                "Romanian Academy",
                "Artificial Intelligence Research Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2226325462",
            "name": "Dumitru Clementin Cercel",
            "affiliations": [
                "Artificial Intelligence Research Institute",
                "Romanian Academy"
            ]
        },
        {
            "id": "https://openalex.org/A2100754702",
            "name": "Costin Chiru",
            "affiliations": [
                "Romanian Academy",
                "Artificial Intelligence Research Institute"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2914076857",
        "https://openalex.org/W2787263876",
        "https://openalex.org/W2970352191",
        "https://openalex.org/W2988263320",
        "https://openalex.org/W197865394",
        "https://openalex.org/W2804424683",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2964756555",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2142384583",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2164370343",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2962902328",
        "https://openalex.org/W2806592685",
        "https://openalex.org/W2971698774",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2951520515",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2788031953",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2962676330",
        "https://openalex.org/W1584778245",
        "https://openalex.org/W2885940157",
        "https://openalex.org/W2986701177",
        "https://openalex.org/W2906195582",
        "https://openalex.org/W2953061889",
        "https://openalex.org/W2022230578",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2098229810",
        "https://openalex.org/W2147880316"
    ],
    "abstract": "This work presents our contribution in the context of the 6th task of SemEval-2020: Extracting Definitions from Free Text in Textbooks (DeftEval). This competition consists of three subtasks with different levels of granularity: (1) classification of sentences as definitional or non-definitional,(2) labeling of definitional sentences, and (3) relation classification. We use various pretrained language models (i.e., BERT, XLNet, RoBERTa, SciBERT, and ALBERT) to solve each of the three subtasks of the competition. Specifically, for each language model variant, we experiment by both freezing its weights and fine-tuning them. We also explore a multi-task architecture that was trained to jointly predict the outputs for the second and the third subtasks. Our best performing model evaluated on the DeftEval dataset obtains the 32nd place for the first subtask and the 37th place for the second subtask. The code is available for further research at: https://github.com/avramandrei/DeftEval.",
    "full_text": "Proceedings of the 14th International Workshop on Semantic Evaluation, pages 737–745\nBarcelona, Spain (Online), December 12, 2020.\n737\nUPB at SemEval-2020 Task 6: Pretrained Language Models for Deﬁnition\nExtraction\nAndrei-Marius Avram1,2, Dumitru-Clementin Cercel1, Costin-Gabriel Chiru1\nUniversity Politehnica of Bucharest, Faculty of Automatic Control and Computers1\nResearch Institute for Artiﬁcial Intelligence, Romanian Academy2\navram.andreimarius@gmail.com,\n{clementin.cercel, costin.chiru}@cs.pub.ro\nAbstract\nThis work presents our contribution in the context of the 6th task of SemEval-2020: Extracting\nDeﬁnitions from Free Text in Textbooks (DeftEval). This competition consists of three subtasks\nwith different levels of granularity: (1) classiﬁcation of sentences as deﬁnitional or non-deﬁnitional,\n(2) labeling of deﬁnitional sentences, and (3) relation classiﬁcation. We use various pretrained\nlanguage models (i.e., BERT, XLNet, RoBERTa, SciBERT, and ALBERT) to solve each of the\nthree subtasks of the competition. Speciﬁcally, for each language model variant, we experiment\nby both freezing its weights and ﬁne-tuning them. We also explore a multi-task architecture\nthat was trained to jointly predict the outputs for the second and the third subtasks. Our best\nperforming model evaluated on the DeftEval dataset obtains the 32nd place for the ﬁrst subtask\nand the 37th place for the second subtask. The code is available for further research at: https:\n//github.com/avramandrei/DeftEval .\n1 Introduction\nDeﬁnition extraction from text is a challenging research task, addressed by numerous researchers in the\narea of natural language processing (NLP). Factual question-answering systems are possible applications\nthat can beneﬁt from the results of this task (Zhang and Jiang, 2009). As a response to this challenge,\nSpala et al. (2019) introduced the Deﬁnition Extraction from Texts (DEFT) corpus, a human-annotated\nEnglish dataset that contains multi-domain (e.g., biology, sociology, physics) term-deﬁnition pairs from\ntwo types of documents, free (i.e., Textbook) and semi-structured text (i.e., Contracts), as opposed to\nthe domain-speciﬁc WCL dataset (Navigli and Velardi, 2010). In addition, a shared task was proposed\nby SemEval-2020 which was aimed at evaluating the performance of each participant system on three\nsubtasks deﬁned for the DEFT corpus. The three subtasks are the following:\nSubtask 1: Given a labeled dataset of sentences, this subtask is to build a classiﬁer capable of\ndistinguishing between a sentence containing both a deﬁnition and the deﬁned term or not. We provide\ntwo examples of sentences that contain a deﬁnition, from Biology and Physics domains, respectively:\n•The metabolome is the complete set of metabolites that are related to the genetic makeup of an\norganism.\n•Polarization is the separation of charges in an object that remains neutral.\nSubtask 2: Given a dataset of tokenized sentences, we aim to label each token with one of the following\nclasses: Term, Alias-Term, Referential-Term, Deﬁnition, Referential-Deﬁnition, or Qualiﬁer.The meaning\nof each token can be found in the corpus description paper (Spala et al., 2019).\nSubtask 3: Given a dataset of tokenized sentences and the tag id of each token, our goal is to predict,\nfor each token, the type of relationship it had with another token, and also the tag id of the token it had a\nrelation with. This is a classical relation extraction task (Zeng et al., 2018) and the relations that must be\nextracted are: Direct-deﬁnes, Indirect-deﬁnes, Refers-to, AKA, and Supplements.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details:\nhttp://creativecommons.org/licenses/by/4.0/.\n738\nPrevious approaches (Klavans and Muresan, 2001; Fahmi and Bouma, 2006; Zhang and Jiang, 2009) of\nrecognizing deﬁnition sentences mainly focused on the use of linguistic clues (e.g., \"is\", \"means\", \"are\",\n\"a\", or \"()\"). However, these studies fail to classify sentences containing deﬁnitions, where the linguistic\nclues are non-existent. In recent years, neural network-oriented solutions are another line of research in\norder to capture deﬁnitions from the text. Anke and Schockaert (2018) have proposed an architecture\nthat relies on two models, convolutional neural network (Fukushima and Miyake, 1982) and bidirectional\nlong short-term memory network (Hochreiter and Schmidhuber, 1997). Recently, Veyseh et al. (2019)\ncombines more advanced deep learning techniques by leveraging graph convolutional neural networks\nwith both syntactic and semantic information. However, the existing methods for deﬁnition extraction\ncannot beneﬁt from language pretrained models (Lan et al., 2019). Motivated by their recent performances\nin more NLP tasks, we employ these models for solving the previously mentioned subtasks.\nSubtask 2 is related to many NLP tasks that can be formalized as a sequence labeling problem, such as\npart-of-speech tagging (Liu et al., 2018), named entity recognition (Ma and Hovy, 2016; Dumitrescu and\nAvram, 2019), metaphor detection (Wu et al., 2018), emphasis selection (Shirani et al., 2019), keyphrase\nextraction (Alzaidy et al., 2019), fragment-level propaganda detection (Da San Martino et al., 2019),\ncomplex word identiﬁcation (Gooding and Kochmar, 2019), language identiﬁcation (Mave et al., 2018),\nor cyber attacks detection in server logs (Ghimes et al., 2018).\nFor the third subtask, we did not take into consideration the fact that we were given the set of correct tags\nof the second subtask (which tremendously help in the relation classiﬁcation) and, thus, our approaches\nobtained poor results on both development and test datasets, so this subtask will not be discussed in the\nrest of the paper.\nOur main contributions can be summarized as follows:\n•We explore various pretrained language models and we depict their results for the ﬁrst and the second\nsubtasks, respectively.\n•As has been shown on other corpora (Peters et al., 2019), we ﬁnd that ﬁne-tuning the weights of\nthe pretrained language models on the DEFT corpus gives a boost in performance, as opposed to\nfreezing them.\n•Additionally, we investigate a RoBERTa model (Liu et al., 2019) within a multi-task architecture\nthat jointly learns the outputs of the second and the third subtask. However, we report only its\nperformance for the second subtask for the reason presented above.\n2 Neural Architectures\n2.1 Pretrained Language Models\nAll pretrained language models presented below use the Transformer encoder (Vaswani et al., 2017) to\nproduce contextualized embeddings. The Transformer is a self-attention mechanism that can capture\nlong-distance dependencies between its inputs. For each language model, we use the implementation that\nis publicly available in the HuggingFace repository1.\n2.1.1 BERT\nBidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) was bidirectionally\ntrained using two strategies: (1) Masked Language Modeling (MLM), and (2) Next Sentence Prediction\n(NSP), on both the BooksCorpus (Zhu et al., 2015) with a dimensionality of 800M words and the English\nWikipedia with a dimensionality of 500M words. It has improved the existing results on the General\nLanguage Understanding Evaluation (GLUE) dataset (Wang et al., 2018) by 7%.\n2.1.2 XLNet\nXLNet (Yang et al., 2019) trains with a new objective, called Permutation Language Modeling (PLM),\nthat instead of predicting the tokens in a sequential order in the same way as a traditional autoregressive\n1https://github.com/huggingface/transformers\n739\nsolution, it predicts the tokens in a random order. Moreover, aside from using PLM, XLNet relies on\nTransformer XL (Dai et al., 2019), a variation of the transformer structure that can capture a longer context\nthrough recurrence, as its base architecture. XLNet surpass BERT on a series of NLP tasks, and set the\nnew state-of-the-art result on the GLUE dataset with an average F1-score of 88.4%.\n2.1.3 RoBERTa\nLater on, the Robustly optimized BERT pretraining approach (RoBERTa) (Liu et al., 2019) uses the\nMLM strategy of BERT, but removes the NSP objective. Moreover, the model was trained with a much\nlarger batch size and learning rate, on a much larger dataset and showed that the training procedure can\nsigniﬁcantly improve the performance of BERT on a variety of NLP tasks. In particular, with a F1-score\nof 88.5%, RoBERTa reached the top position on the GLUE leaderboard, outperforming the previous\nleader, XLNet. When RoBERTa was released, it obtained new state-of-the-art results on the GLUE dataset,\nimproving the performance of BERT by 6.4% and over the previous leader by 0.1%.\n2.1.4 SciBERT\nScience BERT (SciBERT) (Beltagy et al., 2019) is a pretrained language model based on BERT. As\nopposed to BERT, the novelty here is that SciBERT was trained on large multi-domain corpora of\nscientiﬁc publications to improve performance on domain-aware NLP tasks. Experimental results show\nthat SciBERT signiﬁcantly surpassed BERT on biomedical, computer science, and other scientiﬁc domains.\n2.1.5 ALBERT\nA Lite BERT (ALBERT) (Lan et al., 2019) is a system that has fewer parameters than the classical BERT,\nbut still maintains a high performance. The contributions of ALBERT consist in two key approaches of\nparameter reduction: factorized embedding parameterization and cross parameter sharing. In addition,\nALBERT uses for training the sentence-order prediction instead of NSP. ALBERT obtained an average\nF1-score of 89.4% on the GLUE dataset, pushing the state-of-the-art by 0.6%.\n2.2 Conditional Random Fields\nThe most common method for treating a sequence labeling task is the Conditional Random Field (CRF)\nmodel (Lafferty et al., 2001). As was mentioned by Alzaidy et al. (2019), for a sequence xof input words\nand another sequence yof output tags, CRF works by constructing a conditional probability distribution\nin the following manner:\np(y|x; W,b) ∝exp\n( n∑\ni=1\nWT\nyi−1,yi xi + byi−1,yi\n)\n(1)\nwhere the parameters Wyi−1,yi and byi−1,yi are called the weight matrix and the bias, respectively.\nTo estimate the parameters W and b, we perform a maximization of the log-likelihood function:\nL(W,b) =\nN∑\nj=1\nlog(y(j)|x(j); W,b) (2)\nOnce the CRF is trained, we use the Viterbi algorithm (Forney, 1973) to ﬁnd the most probable sequence\namong all possible tag sequences.\n2.3 Approaches based on Pretrained Language Models\nFor the ﬁrst subtask, we add a two-layered feed-forward neural network with 512 neurons in each layer on\ntop of the [CLS] contextualized embedding (as proposed in the original paper of BERT for the single\nsentence classiﬁcation tasks (Devlin et al., 2019)) that maps this embedding to a scalar. By applying a\nsigmoid function to this mapping, we obtain a trainable scalar that represents the probability of a sentence\nto contain a deﬁnition.\nFor the second subtask, we also map the contextualized embeddings generated by each pretrained\nlanguage model in a lower-dimensional space using a two-layered feed-forward neural network. Then, we\n740\nFigure 1: Label mismatch for the word \"extrapolate\" - green represents the chosen label. Left: the model\npredicts for two of the three subtokens the label B-TERM, so it will become the label of the word. Right:\nthere is no label majority, so the ﬁrst subtoken is chosen.\nFigure 2: Our multi-task learning framework for the subtasks 2 and 3, respectively.\nuse these mappings to train a CRF model that learns to predict the most probable sequence of labels for a\ngiven input. The main problem that we encountered in this subtask was that we needed to apply a special\ntokenization, namely Byte Pair Encoding (BPE), to train the language models, that was different than the\none used to create the corpus. To mitigate this issue, we reconstructed the sentence from its tokens and\nsplited it again in BPE subtokens. Then, to map the subtokens back to the original tokens, we employed a\ncharacter matching algorithm that is similar to the one used by spacy-transformers2.\nThe BPE tokenization also introduced a problem at inference because the predicted labels for the\nsubtokens of a word might not match, so a label for the whole word could not be inferred directly. To\nsolve this problem, we took the label of the majority or, if the labels were equally distributed, we selected\nthe label of the ﬁrst subtoken. Figure 1 depicts the proposed solution for the problem of label mismatch\nfor the word \"extrapolate\".\n2.4 Multi-task Learning Approach\nIn this work, we also experimented with a language model that jointly learned to predict the tags, the\ntag ids, and the relations. Figure 2 depicts the architecture of the multi-task learning setting. To create\nthe framework, we projected the contextualized embeddings generated by RoBERTa in three vectors,\nrepresenting the outputs for the subtasks 2 and 3, respectively.\nThe approach on tag prediction subtask in the multi-task context is identical to its single subtask\ncounterpart. To predict the tags ids, we consider that the maximum number of possible tags in a paragraph\nis 10 and that the id of the tag is given by its position in this context. Once we identify all tag ids, we\npredict the corresponding relations.\nL(ytag,yid,yrel,ˆytag,ˆyid,ˆyrel) =λ1L1(ytag,ˆytag) +λ2L2(yid,ˆyid) +λ3L3(yrel,ˆyrel) (3)\nThe learning objective of the multi-task method is to predict the outputs for the three subtasks by\nminimizing the following multi-task loss function:\n2More details about the algorithm can be found at the Tokenization Alignment section from this repository: https:\n//github.com/explosion/spacy-transformers.\n741\nTag type No. of initial samples No. of ﬁnal samples Multiplication factor\nDeﬁnition 93,204 115,190 1\nTerm 16,162 16,439 1\nAlias-Term 1,586 6,444 4\nQualiﬁer 1,207 5,136 4\nRef-Deﬁnition 969 7,824 8\nRef-Term 256 4,096 16\nTable 1: Statistics of each tag class before and after applying the oversampling technique.\nwhere ytag,yid,yrel are the true labels for tags, tag ids and relations, respectively, while ˆytag,ˆyid,ˆyrel are\nthe corresponding predictions. Also, λ1,λ2,λ3 and L1,L2,L3 are the weights and the individual loss\nfunctions, respectively.\n3 Performance Evaluation\n3.1 Dataset and Preprocessing\nThe DeftEval dataset contains imbalanced classes for all the subtasks considered. For example, the ﬁrst\nsubtask has 11,090 sentences that do not contain a deﬁnition and 5,569 that contain. In order to handle\nthis issue, we balance the classes by doubling the number of sentences that contain a deﬁnition, obtaining\na new total of 11,138 positive samples. Moreover, the second task has highly imbalanced classes, ranging\nfrom 93,204 for the Deﬁnition tag to 256 for the Referential-Term tag. To balance the classes in this case,\nwe oversampled each sentence that contains an under-represented tag by a factor inversely proportional\nto their number3. The number of the initial and ﬁnal entries for each tag (i.e., before and after applying\nthe oversampling technique), along with the corresponding multiplication factor are depicted in Table\n1. As one can note, the ﬁnal number of tags is not equal to the initial number of tags multiplied by its\nfactor. This is because other tags were affected when we oversampled a sentence for a certain tag, with a\nparticular multiplication factor4.\nThe preprocessing step consists of removing the artifacts that could interfere with the language model\nrepresentation of the sentences. For the ﬁrst task, we replace the URLs and equations with two special\ntokens, <url> and <equation>, respectively, during the ﬁne-tuning process, and remove them while\nfreezing the language model weights. We also eliminate the artifacts that came from the text formatting5\nand the spaces before punctuation. For the second subtask, we discard the characters that were not\nrecognized by the language models and that could break down the tag-subtoken matching process, like\nthe \"Âo\" character or Greek letters. We also replace the accented characters with their corresponding\nunaccented version.\n3.2 Experimental Settings\nFor training purposes, we employ the Adam optimizer (Kingma and Ba, 2015) with a learning rate of\n2e−5 for both the frozen and ﬁne-tuned versions. We train each language model for 100 epochs with\na batch size of 16 and we save only the language model that obtained the highest performance on the\ndevelopment dataset. We decided not to use the early stopping setting because we observed that the\nmodels can signiﬁcantly improve the efﬁciency of our results even after a long period of stagnation.\nThe feed-forward layer, which is placed on top of each language model to project the contextualized\nembeddings in the output space, has a hidden layer of size 512. Thus, a more complex family of functions\ncan be learned by the system. We regularize the hidden layer with a high 80% dropout for the ﬁne-tuned\nversions in order to make their learning slower and more robust, and with a 20% dropout for the frozen\nversions to avoid underﬁtting.\n3We note that by using this method, the number of over-represented tags increases as well, but by a much smaller factor.\n4When we oversampled a sentence, we considered only the tag that had the highest multiplication factor in that sentence.\n5The text formatting artifacts usually came from expressions, like size 12 { }.\n742\nDev Test\nModel Precision Recall F1-score Precision Recall F1-score\nFrozen BERT 76.0 75.1 75.5 - - -\nFrozen RoBERTa 74.1 74.4 74.3 - - -\nFrozen SciBERT 71.7 80.6 75.9 - - -\nFrozen XLNet 66.8 70.8 68.7 - - -\nFrozen ALBERT 77.2 69.3 73.0 - - -\nFine-tuned BERT 78.4 84.1 81.2 - - -\nFine-tuned RoBERTa 78.2 84.5 81.3 75.0 80.6 77.7\nFine-tuned SciBERT 79.4 79.7 79.6 - - -\nFine-tuned XLNet 75.5 85.2 80.1 - - -\nFine-tuned ALBERT 69.5 85.9 76.8 - - -\nThe winning team - - - - - 87.7\nDev Test\nModel Precision Recall F1-score Precision Recall F1-score\nFrozen BERT+CRF 27.1 39.8 26.1 - - -\nFrozen RoBERTa+CRF 32.7 27.7 22.6 - - -\nFrozen SciBERT+CRF 29.0 37.5 26.2 - - -\nFrozen XLNet+CRF 29.6 33.4 26.8 - - -\nFrozen Multi-task Learning 4.0 8.9 8.0 - - -\nFine-tuned BERT+CRF 47.9 51.7 45.6 - - -\nFine-tuned RoBERTa+CRF 41.4 66.4 46.0 39.4 55.6 43.9\nFine-tuned SciBERT+CRF 46.7 46.6 41.7 - - -\nFine-tuned XLNet+CRF 33.0 58.5 39.2 - - -\nFine-tuned Multi-task Learning 25.7 25.2 25.5 - - -\nThe winning team - - - - - 84.7\nTable 2: Comparison of performance on both the development and test datasets for the ﬁrst subtask (up)\nand the second subtask (down), respectively. The frozen versions denote that the weights of the respective\nlanguage model were not updated during the training phase, while the ﬁne-tuned versions denote that their\nweights were updated during the training process.\nDue to the computational constraints, we adopt only the base version of all the language models tested\nin the current work. Furthermore, we use only the cased version of each language model when there is the\ncase. Finally, we select the cross-entropy loss function for the multi-task architecture and we set all the\nweights λ1,λ2,λ3 equal to 0.33.\n3.3 Results and Analysis\nAs mentioned above, we conducted experiments with a total of ﬁve pretrained language models, including\nBERT, XLNet, RoBERTa, SciBERT, and ALBERT. More speciﬁcally, we use each language model by\nfreezing and ﬁne-tuning its weights. We also experiment with a multi-task architecture that is a joint\nlearning technique to predict the outputs for both the second and the third subtask. Table 2 reports the\nevaluation metrics, Macro-Precision, Recall, and F1-scores, respectively, on both development and test\ndatasets.\nIt can be observed from the two tables that ﬁne-tuning the weights of the language models offers a high\nboost in performance. That is, the results show an improvement of up to 11.4% on the development dataset\nfor the ﬁrst subtask (in case of XLNet) and up to 23.4% on the development dataset for the second subtask\n(in case of RoBERTa). Moreover, the results on the development dataset also show that a ﬁne-tuned\nRoBERTa is the best performing language model among all others, for both subtasks. Thereby, this was\nthe only model submitted for evaluation, and it obtained a F1-score of 0.777 for the ﬁrst subtask, and a\n743\n(a) (b)\nFigure 3: Confusion matrix for (a) the ﬁrst subtask using the ﬁne-tunned RoBERTa model and (b) the\nsecond subtask using the ﬁne-tunned RoBERTa+CRF model.\nmacro F1-score of 0.439 for the second subtask, ranking 32nd and 37th on the leaderboard, respectively.\nFigure 3 depicts the detailed confusion matrices of the submitted models for the ﬁrst and the second\nsubtasks on the evaluation dataset. In the case of the ﬁrst subtask, we can observe that the model is slightly\nbiased towards predicting positive labels, resulting in more false positives for the input sentences, which\nis somewhat expected, given the balancing method we have applied. To improve the visualization of the\nresults, the confusion matrix for the second subtask was normalized along the true labels for the test set. It\ncan be seen that the dominant tags, Deﬁnition and Term, and also the Ref-Term, the tag that was highly\noversampled, were the least confused by the model, obtaining an accuracy of 64%, which is almost double\nthan the accuracy obtained by the rest of the tags. Moreover, it should be noted that most of the tags are\nmisclassiﬁed by the model with the O tag, exception making the Ref-Term tag which is misclassiﬁed with\nthe Deﬁnition tag, 36% of the time.\n4 Conclusions and Future Developments\nIn this paper, we have presented our solution for the SemEval-2020 Task 6: Extracting Deﬁnitions from\nFree Text in Textbooks (DeftEval). We evaluated different state-of-the-art language models both by\nfreezing and ﬁne-tuning their weights. We observed that the performance of all the selected models can\nbe signiﬁcantly improved by ﬁne-tuning their weights. Through a series of experiments conducted on the\ndevelopment dataset, we showed that RoBERTa signiﬁcantly outperforms other language models for the\ntwo subtasks. According to the ofﬁcial leaderboard, we obtained the 32nd place out of 56 submissions for\nthe ﬁrst subtask and the 37th place out of 51 submissions for the second subtask. One possible direction\nfor future work is to evaluate the multi-task scenario using other language models. We also consider\nthat a larger annotated dataset together with the large variants of the pretrained language models could\ndrastically improve deﬁnition extraction performance. Moreover, we believe that by using class weights\ninstead of oversampling for the ﬁrst subtask, one can mitigate the problems observed in its confusion\nmatrix.\n744\nReferences\nRabah Alzaidy, Cornelia Caragea, and C Lee Giles. 2019. Bi-lstm-crf sequence labeling for keyphrase extraction\nfrom scholarly documents. In The world wide web conference, pages 2551–2557.\nLuis Espinosa Anke and Steven Schockaert. 2018. Syntactically aware neural architectures for deﬁnition\nextraction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 378–385.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language model for scientiﬁc text. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3606–3611.\nGiovanni Da San Martino, Alberto Barron-Cedeno, and Preslav Nakov. 2019. Findings of the nlp4if-2019 shared\ntask on ﬁne-grained propaganda detection. In Proceedings of the Second Workshop on Natural Language\nProcessing for Internet Freedom: Censorship, Disinformation, and Propaganda, pages 162–170.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pages 2978–2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 4171–4186.\nStefan Daniel Dumitrescu and Andrei-Marius Avram. 2019. Introducing ronec–the romanian named entity corpus.\narXiv preprint arXiv:1909.01247.\nIsmail Fahmi and Gosse Bouma. 2006. Learning to identify deﬁnitions using syntactic features. In Proceedings\nof the Workshop on Learning Structured Information in Natural Language Applications.\nG David Forney. 1973. The viterbi algorithm. Proceedings of the IEEE, 61(3):268–278.\nKunihiko Fukushima and Sei Miyake. 1982. Neocognitron: A self-organizing neural network model for a\nmechanism of visual pattern recognition. In Competition and cooperation in neural nets , pages 267–285.\nSpringer.\nAna-Maria Ghimes, Andrei-Marius Avram, and Valentin-Alexandru Vladuta. 2018. A character prediction\napproach in a security context using a recurrent neural network. In2018 International Symposium on Electronics\nand Telecommunications (ISETC), pages 1–4. IEEE.\nSian Gooding and Ekaterina Kochmar. 2019. Complex word identiﬁcation as a sequence labelling task. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1148–1153.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of ICLR.\nJudith L Klavans and Smaranda Muresan. 2001. Evaluation of the deﬁnder system for fully automatic glossary\nconstruction. In Proceedings of the AMIA Symposium, page 324. American Medical Informatics Association.\nJohn Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random ﬁelds: Probabilistic\nmodels for segmenting and labeling sequence data. In ICML.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019.\nAlbert: A lite bert for self-supervised learning of language representations. In International Conference on\nLearning Representations.\nLiyuan Liu, Jingbo Shang, Xiang Ren, Frank Fangzheng Xu, Huan Gui, Jian Peng, and Jiawei Han. 2018.\nEmpower sequence labeling with task-aware neural language model. In Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\n745\nXuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional lstm-cnns-crf. In\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1064–1074.\nDeepthi Mave, Suraj Maharjan, and Thamar Solorio. 2018. Language identiﬁcation and analysis of code-switched\nsocial media text. In Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-\nSwitching, pages 51–61.\nRoberto Navigli and Paola Velardi. 2010. Learning word-class lattices for deﬁnition and hypernym extraction.\nIn Proceedings of the 48th annual meeting of the association for computational linguistics , pages 1318–1327.\nAssociation for Computational Linguistics.\nMatthew E Peters, Sebastian Ruder, and Noah A Smith. 2019. To tune or not to tune? adapting pretrained\nrepresentations to diverse tasks. In Proceedings of the 4th Workshop on Representation Learning for NLP\n(RepL4NLP-2019), pages 7–14.\nAmirreza Shirani, Franck Dernoncourt, Paul Asente, Nedim Lipka, Seokhwan Kim, Jose Echevarria, and Thamar\nSolorio. 2019. Learning emphasis selection for written text in visual media from crowd-sourced label\ndistributions. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,\npages 1167–1172.\nSasha Spala, Nicholas A Miller, Yiming Yang, Franck Dernoncourt, and Carl Dockhorn. 2019. Deft: A corpus\nfor deﬁnition extraction in free-and semi-structured text. In Proceedings of the 13th Linguistic Annotation\nWorkshop, pages 124–131.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008.\nAmir Pouran Ben Veyseh, Franck Dernoncourt, Dejing Dou, and Thien Huu Nguyen. 2019. A joint model for\ndeﬁnition extraction with syntactic connection and semantic consistency. arXiv preprint arXiv:1911.01678.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. Glue: A\nmulti-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355.\nChuhan Wu, Fangzhao Wu, Yubo Chen, Sixing Wu, Zhigang Yuan, and Yongfeng Huang. 2018. Neural metaphor\ndetecting with cnn-lstm model. In Proceedings of the Workshop on Figurative Language Processing , pages\n110–114.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information\nprocessing systems, pages 5754–5764.\nXiangrong Zeng, Shizhu He, Kang Liu, and Jun Zhao. 2018. Large scaled relation extraction with reinforcement\nlearning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.\nChunxia Zhang and Peng Jiang. 2009. Automatic extraction of deﬁnitions. In 2009 2nd IEEE International\nConference on Computer Science and Information Technology, pages 364–368. IEEE.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.\n2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. In Proceedings of the IEEE international conference on computer vision, pages 19–27."
}