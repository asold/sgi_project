{
  "title": "Yet Another Traffic Classifier: A Masked Autoencoder Based Traffic Transformer with Multi-Level Flow Representation",
  "url": "https://openalex.org/W4382239858",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098262578",
      "name": "Ruijie Zhao",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A4320888283",
      "name": "Mingwei Zhan",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2303362850",
      "name": "Xianwen Deng",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2227377054",
      "name": "Yanhao Wang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2103742464",
      "name": "Yijun WANG",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2135040491",
      "name": "Guan Gui",
      "affiliations": [
        "Nanjing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2117835882",
      "name": "Zhi Xue",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2098262578",
      "name": "Ruijie Zhao",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A4320888283",
      "name": "Mingwei Zhan",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2303362850",
      "name": "Xianwen Deng",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2227377054",
      "name": "Yanhao Wang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2103742464",
      "name": "Yijun WANG",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2135040491",
      "name": "Guan Gui",
      "affiliations": [
        "Nanjing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2117835882",
      "name": "Zhi Xue",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2184652140",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W4293093536",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2343828539",
    "https://openalex.org/W3114936184",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2591712613",
    "https://openalex.org/W3134226201",
    "https://openalex.org/W4224315052",
    "https://openalex.org/W2919493784",
    "https://openalex.org/W2753434589",
    "https://openalex.org/W2096118443",
    "https://openalex.org/W6693823607",
    "https://openalex.org/W3181596493",
    "https://openalex.org/W3045016378",
    "https://openalex.org/W2782688857",
    "https://openalex.org/W2190207511",
    "https://openalex.org/W3007562398",
    "https://openalex.org/W2606697812",
    "https://openalex.org/W2980185226",
    "https://openalex.org/W3005920407",
    "https://openalex.org/W4299319711",
    "https://openalex.org/W2978660033",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W2963516518",
    "https://openalex.org/W4385570274",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2272516773"
  ],
  "abstract": "Traffic classification is a critical task in network security and management. Recent research has demonstrated the effectiveness of the deep learning-based traffic classification method. However, the following limitations remain: (1) the traffic representation is simply generated from raw packet bytes, resulting in the absence of important information; (2) the model structure of directly applying deep learning algorithms does not take traffic characteristics into account; and (3) scenario-specific classifier training usually requires a labor-intensive and time-consuming process to label data. In this paper, we introduce a masked autoencoder (MAE) based traffic transformer with multi-level flow representation to tackle these problems. To model raw traffic data, we design a formatted traffic representation matrix with hierarchical flow information. After that, we develop an efficient Traffic Transformer, in which packet-level and flow-level attention mechanisms implement more efficient feature extraction with lower complexity. At last, we utilize the MAE paradigm to pre-train our classifier with a large amount of unlabeled data, and perform fine-tuning with a few labeled data for a series of traffic classification tasks. Experiment findings reveal that our method outperforms state-of-the-art methods on five real-world traffic datasets by a large margin. The code is available at https://github.com/NSSL-SJTU/YaTC.",
  "full_text": "Yet Another Traffic Classifier: A Masked Autoencoder Based Traffic Transformer\nwith Multi-Level Flow Representation\nRuijie Zhao1*, Mingwei Zhan1*, Xianwen Deng1, Yanhao Wang2,\nYijun Wang1, Guan Gui3†, Zhi Xue1†\n1Shanghai Jiao Tong University, Shanghai, China\n2QI-ANXIN, Beijing, China\n3NJUPT, Nanjing, China\n{ruijiezhao, mw.zhan, 2594306528, ericwyj, zxue}@sjtu.edu.cn; wangyanhao136@gmail.com; guiguan@njupt.edu.cn\nAbstract\nTraffic classification is a critical task in network security and\nmanagement. Recent research has demonstrated the effective-\nness of the deep learning-based traffic classification method.\nHowever, the following limitations remain: (1) the traffic rep-\nresentation is simply generated from raw packet bytes, result-\ning in the absence of important information; (2) the model\nstructure of directly applying deep learning algorithms does\nnot take traffic characteristics into account; and (3) scenario-\nspecific classifier training usually requires a labor-intensive\nand time-consuming process to label data. In this paper, we\nintroduce a masked autoencoder (MAE) based traffic trans-\nformer with multi-level flow representation to tackle these\nproblems. To model raw traffic data, we design a format-\nted traffic representation matrix with hierarchical flow in-\nformation. After that, we develop an efficient Traffic Trans-\nformer, in which packet-level and flow-level attention mecha-\nnisms implement more efficient feature extraction with lower\ncomplexity. At last, we utilize the MAE paradigm to pre-\ntrain our classifier with a large amount of unlabeled data,\nand perform fine-tuning with a few labeled data for a series\nof traffic classification tasks. Experiment findings reveal that\nour method outperforms state-of-the-art methods on five real-\nworld traffic datasets by a large margin. The code is available\nat https://github.com/NSSL-SJTU/YaTC.\n1 Introduction\nTraffic classification is attracting the attention of service\nproviders and equipment vendors as a significant solution\nfor solving network management problems. Understanding\nthe content of traffic allows network operators to respond\nswiftly in support of diverse business goals, enhancing ser-\nvice quality and user experience (Papadogiannaki and Ioan-\nnidis 2021). Meanwhile, traffic classification is a critical\ncomponent of the intrusion detection system, which is uti-\nlized to detect threats and safeguard system security (Yao\net al. 2019). However, the growing usage of encrypted traffic\nand anonymous network technology makes analyzing com-\nplicated traffic difficult. To construct a sophisticated traffic\n*These authors contributed equally.\n†Corresponding authors.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFlows Packets\nPacket 1\nPacket 2\nPacket 3\nPacket 4\nPacket 5\nPacket N\nPacket-Level Matrices\nFlow-Level Stacking \nPacket-Level Matrices \nMFR Matrix\nRaw Traffic\nSplit\nRaw Packet Bytes\nHeader Payload\nPacket-Level Matrix \nHeader Matrix \nPayload Matrix\nRaw Packet Bytes\nHeader Payload\nPacket-Level Matrix \nHeader Matrix \nPayload Matrix\n…\nSession Flow\nFigure 1: The schematic illustration of multi-level flow rep-\nresentation (MFR).\nanalyzer for more accurate traffic classification, it is neces-\nsary to capture implicit and robust patterns in diverse traffic.\nTraditional traffic analysis methods identify different net-\nwork services based on fundamental traffic features (e.g.,\ncommunication protocol, port number), which are no longer\nappropriate owing to the complexity and variability of the\ncurrent traffic (Taylor et al. 2016). To address this issue, sev-\neral works use machine learning (ML) algorithms to clas-\nsify traffic through statistical features. However, these ap-\nproaches rely on expert experience to select specific fea-\ntures, and even insignificant statistical features might impact\nanalysis performance (Shen et al. 2020). Deep learning (DL)\ntechniques, which can automatically extract features from\npacket bytes of raw traffic, have been used by an increas-\ning number of researchers for more effective traffic analysis\nin recent years. Unfortunately, once the length of a certain\npacket is too long, the bytes from that packet can overwhelm\nimportant information from other packets. Thus, the lack\nof representation design in traditional DL-based methods\nbrings instability in performance. Besides, they often need\na large amount of labeled traffic data for training. Labeling\ndata is a time-consuming and labor-intensive operation, re-\nsulting in high deployment and update costs.\nPre-training methods with massive volumes of unlabeled\ndata have recently demonstrated outstanding performance in\ncomputer vision (CV) (He et al. 2022; Caron et al. 2021;\nChen, Xie, and He 2021) and natural language processing\n(NLP) (Devlin et al. 2019; Lan et al. 2020) tasks. When\ncompared to the traditional supervised learning paradigm,\nthe self-supervised learning paradigm pre-training method\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n5420\nhas two major advantages: (1) more effective representa-\ntions can reduce the reliance of downstream tasks on the\namount of labeled data, and (2) better initialization param-\neters can speed up convergence and improve classification\nperformance. Recent studies (He, Yang, and Chen 2020; Lin\net al. 2022) in traffic classification directly utilize the BERT-\nbased paradigm (Devlin et al. 2019), a pre-training method\ncreated for NLP tasks, and obtain better performance. How-\never, unlike typical words, traffic bytes as input lack explicit\nhigh-level semantic units, making it unreasonable to build\nvocabularies to learn semantic relations. Furthermore, nei-\nther method designs a dedicated model structure for traffic\nclassification based on traffic characteristics.\nTo address the challenges mentioned above, we introduce\nYet Another Traffic Classifier (YaTC), a novel traffic clas-\nsifier with self-supervised learning that learns latent repre-\nsentations from large amounts of unlabeled traffic data for\nmore successful classification. Specifically, we first create a\nmulti-level flow representation (MFR) matrix to model the\nraw traffic. It is generated from raw packet bytes and con-\ntains traffic information at different granularities through a\nformatted matrix, as illustrated in Figure 1. Then, a novel\nTraffic Transformer with a packet-level attention module\nand flow-level attention module is constructed to analyze\ntraffic using the MFR matrix. Finally, we train our classifier\nbased on the masked autoencoder (MAE) paradigm in two\nstages: pre-training and fine-tuning. In the pre-training stage,\nwe randomly mask some packet bytes in the MFR matrix as\ninput. Next, an autoencoder-based network, which consists\nof a traffic encoder and a small-scale decoder, reconstructs\nthe original MFR matrix. Our traffic encoder leverages a\nlarge amount of unlabeled traffic data to learn latent repre-\nsentations through the reconstruction task. In the fine-tuning\nstage, we load the parameters of the pre-trained traffic en-\ncoder into Traffic Transformer and fine-tune it with a small\nnumber of labeled data for traffic classification. Our contri-\nbutions can be briefly summarized as follows:\n• We propose an MAE-based Traffic Transformer with\nMFR, called YaTC, for traffic classification. YaTC breaks\nthrough the traditional traffic analysis methods in three\nperspectives: traffic representation, classifier structure,\nand training strategy.\n• We design an MFR matrix that fully considers the flow\nhierarchy to represent the raw traffic. To effectively uti-\nlize the MFR matrix for traffic analysis, we build a novel\nTraffic Transformer with packet-level and flow-level at-\ntention mechanisms. It can perform more efficient feature\nextraction with lower complexity and fewer parameters.\n• We apply the MAE-based self-supervised learning\nparadigm to train our classifier. It first leverages large-\nscale unlabeled traffic data to learn generic latent rep-\nresentations, and then performs fine-tuning with a small\namount of labeled data for a series of traffic classification\ntasks.\n• We evaluate YaTC on five real-world traffic datasets. Ex-\nperimental results show that our method outperforms the\nstate-of-the-art methods by a large margin.\n2 Related Work\n2.1 Traffic Analysis Methods\nRule-Based Methods. In the early stage, researchers\nmainly used rule-based methods to accomplish traffic anal-\nysis. Through rules designed by security experts, basic at-\ntributes such as communication protocol and port number of\ntraffic data are used to discover behaviors that violate secu-\nrity policies (Nguyen and Armitage 2008). However, as net-\nwork services and environments become more complicated,\nthe fundamental features are insufficient to fulfill the current\ntraffic analysis requirement (Taylor et al. 2016).\nML-Based Methods. To analyze complicated traffic ef-\nfectively, ML algorithms are introduced to explore the high-\ndimensional statistical features of traffic. For instance, CU-\nMUL (Panchenko et al. 2016) selects 104 optimal statisti-\ncal features through accuracy evaluation, which are then uti-\nlized as input for a support vector machine (SVM) to identify\nwebsite traffic; AppScanner (Taylor et al. 2016) adopts the\nrandom forest (RF) to analyze the statistical features gener-\nated by the traffic of different applications; isAnon (Cai et al.\n2019) designs a hybrid feature selection algorithm to filter\nout redundant statistical features, and uses the extreme gra-\ndient boosting algorithm to detect anonymity network traf-\nfic. Although ML-based methods combined with statistical\nfeatures can analyze complex traffic, they rely on statistical\nfeatures designed by experts and need to select optimal fea-\ntures for different scenarios (Shen et al. 2020).\nDL-Based Methods. DL-based approaches, which an-\nalyze traffic based on raw packets rather than human-\ndesigned features, have become the primary tool to automat-\nically extract traffic representations and achieve remarkable\nperformance improvement. In (Wang et al. 2017), a convo-\nlutional neural network (CNN) is used to identify traffic im-\nages for encrypted traffic classification. (Zhang et al. 2020)\nadopts the improved CNN classifier that leverages multiple\nchannels to enrich feature information and improve analy-\nsis efficiency. TSCRNN (Lin, Xu, and Gao 2021) combines\nCNN and recurrent neural network (RNN) to learn the tem-\nporal characteristics from time-related packets. However,\nsome existing DL-based methods directly use the packet\nbytes in the flow to represent the raw traffic, resulting in\nthe bytes of a long packet overwhelming important infor-\nmation from other packets. Besides, they rely on sufficient\nlabeled training data, and it is intractable to collect and man-\nually label sufficient real traffic samples. Thus, we design\nan MFR matrix to represent multi-level information of the\nraw traffic through a formatted matrix and develop a novel\nTraffic Transformer to implement more efficient feature ex-\ntraction based on these levels. To reduce the dependence on\nlabeled data, we first leverage large-scale unlabeled data for\npre-training, and then fine-tune with a few labeled data for\ntraffic classification.\n2.2 Pre-training Methods\nPre-training methods significantly reduce the appetite for\nlabeled training data by self-supervised learning. Besides,\nunbiased data representations learned from large amounts\n5421\nTraffic\nEncoder\nTraffic\nEncoder\nPacket 1 Patches\nPacket 2 Patches\nPacket 3 Patches\nPacket 4 Patches\nPacket 5 Patches\nPacket 1 Patches\nPacket 2 Patches\nPacket 3 Patches\nPacket 4 Patches\nPacket 5 Patches\nRP\nRP\nRP\nRP\nRP\nPL Attn\nTraffic\nEncoder\nTraffic\nEncoder\nMFR MatrixMFR Matrix\nFL Attn\nShare\nCP LabelLabelLiner\nOutput\nLiner\nOutput\nCE LossCE Loss\nUnlabeled Data\nLabeled Data\nPre-trainingFine-tuning\nCopy\nRP: Row Pooling\nCP: Column Pooling\nRP: Row Pooling\nCP: Column Pooling\nSize: 4 X 20\nMasked MFR MatrixMasked MFR Matrix\n…\nPacket 1\nPacket 5\nDecoderTraffic\nEncoder\nMFR MatrixMFR Matrix\n……\nTargetTarget\nRandom Mask\nMask Token\nEncoder Token\nVisible \nPatches\nMasked MFR Matrix\n…\nPacket 1\nPacket 5\nDecoderTraffic\nEncoder\nMFR Matrix\n…\nTarget\nRandom Mask\nMask Token\nEncoder Token\nVisible \nPatches\nFigure 2: The schematic illustration of YaTC.\nof unlabeled data further improve performance on down-\nstream tasks. Overall, the pre-training methods first obtain\nthe pre-trained model with unlabeled data, and then load the\nmodel parameters to complete downstream tasks. In NLP\ntasks, BERT (Devlin et al. 2019) is the most widely used\nto implement pre-trained models by cloze task and next\nsentence prediction. Some traffic classification studies (He,\nYang, and Chen 2020; Lin et al. 2022) consider the bytes\nof traffic packets as words and introduce BERT for pre-\ntraining to achieve better classification performance. How-\never, since the input traffic bytes lack explicit high-level se-\nmantic units, latent representation extraction of traffic bytes\nis more suitable as a CV task than an NLP task. In CV tasks,\ntraditional pre-training methods (Chen, Xie, and He 2021;\nCaron et al. 2021) rely on data augmentation (e.g., crop-\nping, enlarging) to form pairs of positive and negative sam-\nples for pre-training via contrastive learning. Obviously, data\naugmentation on the traffic image generated by the packet\nbytes will seriously damage the original information, mak-\ning it difficult to apply the contrastive learning paradigm to\nour task. Benefiting from the introduction of the MAE (He\net al. 2022), we are able to obtain an effective pre-trained\nmodel by masking patches for reconstruction training. Since\nthe masking operation does not change other fixed-position\npacket bytes in the traffic image, it conforms to the design of\nfixed positions of different levels of packet content in multi-\nlevel flow representation.\n3 Methodology\nIn this section, we introduce our traffic representation\nmethod, classifier structure, and training strategy in detail in\nSection 3.1, Section 3.2, and Section 3.3, respectively. The\noverview of our YaTC is shown in Figure 2.\n3.1 Multi-Level Flow Representation\nWe design a novel method to produce multi-level flow repre-\nsentations from raw traffic data as input for traffic classifica-\ntion. Most existing methods directly intercept the preceding\nfixed number of bytes in the flow to form a two-dimensional\nmatrix, which can be handled as an image and classified by\nDL algorithms. However, compared with the header, the size\nof the payload is usually significantly larger and is full of in-\ncomprehensible information generated by encrypt operation,\nresulting in excessive low-level semantic information in the\nmatrix, which affects these models’ effectiveness and effi-\nciency. Moreover, in some flows, the first long packet will\noccupy the entire matrix, so the matrix cannot include the\ndata in other packets of the traffic flow.\nTo address the above issues, we propose an MFR ma-\ntrix with a formatted two-dimensional matrix to represent\nraw traffic flow, as illustrated in Figure 1. First, we split the\nraw traffic into flows according to IP address, port number,\nand protocol type. Then, to avoid introducing biased inter-\nference, we remove the Ethernet headers of the flows, set\nthe port numbers to zero, and replace the IPs with random\naddresses but keep their directions. Finally, the M adjacent\npackets in the flow are captured and formatted into a two-\ndimensional matrix of size H ∗ W as the representation of\nthis flow. We still use the raw bytes as the initial character-\nistics of the traffic, i.e., the value of each point in the traf-\nfic representation matrix, but with the following particular\ndesign for capturing the multi-level information of the raw\ntraffic.\n• Byte-level. Each row of the traffic representation ma-\ntrix includes only one type of traffic bytes, classified into\nheader row and payload row.\n• Packet-level. Each packet is represented by the header\nmatrix and payload matrix, forming a packet-level matrix\nof size H/M ∗ W.\n• Flow-level. Since the flow is composed of ordered pack-\nets, M adjacent packet-level matrices are stacked in the\nsecond dimension to form the final MFR matrix.\nIn this way, the information at each level is fixed, and\nthere will be no overflow at the lower level leading to the\nloss of information at the higher level. We set our MFR ma-\ntrix to include 5 packet-level matrices with a total of 40 rows,\n5422\nand each row includes 40 bytes. Each packet’s header is rep-\nresented by 2 header rows with 80 bytes, with the capability\nto contain the IP layer header (20 bytes), TCP header (20\nbytes) or UDP header (8 bytes), and optional headers. We\nassign 6 payload rows to accommodate the payload of each\npacket and perform interception when it exceeds 240 bytes.\nNote that if the number of valid bytes is insufficient, it will\nbe padded with 0 bytes to form a fixed-size representation\nmatrix.\n3.2 Traffic Transformer\nAccording to the characteristics of traffic, we design our\nTraffic Transformer composed of an embedding module,\na packet-level attention module, and a flow-level attention\nmodule, to effectively conduct traffic classification with the\nMFR matrix.\nEmbedding Module. The MFR matrix x ∈ RH×W is\nfirst split into non-overlapping 2D patches of sizeP ×P with\na total number of N = HW/P 2, denoted as xp ∈ RN×P2\n.\nThe patches are then mapped to D-dimensional vectors by\na linear layer as patch embeddings. To maintain the position\ninformation, which is crucial in MFR, the position embed-\ndings are added to the patch embeddings as the input of the\ntraffic encoder.\nx0 = [x1\npE; x2\npE; ...; xN\np E] +Epos. (1)\nSpecifically, we take D = 192,P = 2and N = 20∗ 20 =\n400, so that all elements of one patch represent the same\ntype of raw bytes, and each packet matrix contains one row\nof header patches and three rows of payload patches.\nPacket-level Attention Module. In the packet-level atten-\ntion phase, we develop a traffic encoder based on Vision\nTransformers (Dosovitskiy et al. 2021), which consists of\nalternating multi-head self-attention layers (MSA) and feed-\nforward layers.\nAiming to preferentially learn the dependencies between\nheader patches or payload patches within the packet, the\ntraffic encoder only performs multi-headed self-attention\namong patches in the same packet rather than all patches\nin the MFR matrix. The MSA layer allows patches of\nthe packet to interact with each other effectively ac-\ncording to the degree of relevance by different heads\nConcat(head1, ··· , headn), and each head is computed by\nthe attention function:\nQ = xlWQ, K= xlWK, V= xlWV , (2)\nAttn(Q, K, V) =softmax( QKT\n√Dk\n)V, (3)\nwhere WQ, WK, WV ∈ RD×Dk are learnable parameters.\nIn this work, we employn = 16parallel attention heads and\nL = 4alternating layers.\nNote that our packet-level attention module focuses on\npromoting packet-level information interaction at this stage,\nwhich is also in line with the traffic characteristics that the\ninformation in the packet has a stronger correlation. In other\nwords, it ignores the patch features of other packets and out-\nputs a valid packet-level representation. Furthermore, our\nmethod also has lower time complexity compared to global\nattention (O(N2/M) vs. O(N2)), which can benefit the ef-\nficiency of traffic analysis.\nFlow-level Attention Module. After packet-level atten-\ntion, significant packet-level features of each patch of the\nMFR matrix x′\np ∈ RN×D have been output, which provides\nthe basis for further use of the transformer to extract long-\ndistance relational dependencies between different packets\non the flow. At the flow level, it is unnecessary to continue\nwith the same fine-grained patch as before, and even simply\nmean-pooling all the packet-level patch features can yield\ngood performance. To learn inter-packet relationships at a\ncoarser granularity, we employ row pooling (RP), which\npartially mean-pooling the output patches’ features of the\npacket-level attention by row to generate row patches:\nxr = Pooling (x′\np), (4)\nwhere xr ∈ R\n√\nN×D are row patches output of RP. Since\neach row of input patch features indicates the same type byte\nof traffic, the outputs of RP also can be divided into header\nor payload with fixed meaning.\nIn particular, each packet contains 1 header row patch\nand 3 payload row patches, representing the partial features\nwithin the packet and a MFR matrix has a total of\n√\nN row\npatches. We input all the row patches within an MFR matrix\ninto a traffic encoder, and the MSA layers capture the flow-\nlevel information between the row patches. The final output\nis a column of row patch features xc ∈ R\n√\nN×D, which are\nfurther performed column pooling (CP) to obtain the final\nrepresentation xMFR ∈ RD of the entire MFR matrix:\nxMFR = Pooling (xc). (5)\nBesides, with the coarser granularity, the model also has a\nlighter weight, the number of row patches is only\n√\nN and\nthe time complexity of this stage is only O(N).\n3.3 Training Strategy of YaTC\nPre-training YaTC. As shown in Figure 2, in the pre-\ntraining stage, YaTC utilizes MAE (He et al. 2022) paradigm\nwith an asymmetric encoder-decoder architecture to recon-\nstruct the raw bytes in the MFR matrix. A high percentage\nof the MFR patches are randomly masked, and only a tiny\nfraction of the patches (i.e. visible unmasked patches) are\ninput to the model. After that, our traffic encoder extracts as\nmany valid features of this fraction of the patches as pos-\nsible and then outputs the encoder tokens. Finally, a small\ndecoder uses encoder tokens and mask tokens to recover the\nmasked region of the MFR matrix. MAE is trained with a\nreconstruction loss, i.e., mean squared error (MSE) between\nthe ground-truth pixels yreal and the predicted pixels yrec:\nLrec = MSE (yrec, yreal). (6)\nAfter pre-training, the traffic encoder can extract high-\nquality features and is preserved for downstream tasks.\nThe pre-training does not require costly manual labeling,\nallowing the use of large amounts of unlabeled data from au-\nthentic scenes. Moreover, we find that a very high mask ratio\n5423\n(90%) achieves better results on downstream tasks. A high\nmask ratio also means that only a few patches are visible, re-\nsulting in lacking raw information to capture dependencies\nwithin and between packets. Thus, we perform global atten-\ntion instead of packet-level attention and flow-level attention\nduring pre-training. The difficult pre-training task and global\nattention would enable the traffic encoder to capture features\non packet-level and flow-level together with minimal infor-\nmation.\nFine-tuning YaTC. On the downstream task, the en-\ncoder parameters from pre-training are loaded into both the\npacket-level attention module and flow-level attention mod-\nule in the traffic transformer and used for feature extraction\non packet-level and flow-level. For classification, the fea-\ntures of each patch are mean-pooled in two stages (RP and\nCP) to be used as the classification features of the MFR Ma-\ntrix, which are flattened and input to an MLP to obtain the\nprediction distribution ˆy ∈ RC, where C is the number of\ntraffic categories. Then classification loss is computed ac-\ncording to cross-entropy loss between prediction distribu-\ntion ˆy and the ground-truth label y:\nLCE = H(ˆy, y). (7)\nTo speed up convergence and reduce the model size, in-\nspired by ALBERT (Lan et al. 2020) and CYCLE (Takase\nand Kiyono 2021), we shared parameters between the two\ntraffic encoders and achieved better performance with half\nthe number of parameters. We argue the reason that both\npacket-level attention and flow-level attention are essentially\nperforming inter-patch dependency capture, and the infor-\nmation interactions during multi-headed self-attention are\nsimilar. Moreover, the data in the same row of patches be-\nlong to the same type, i.e., header or payload, and the row-\npooling’s mean operation does not change the feature space\nin which the patches are located. So except for the differ-\nence in granularity, the two traffic encoders on the feature\nspace are very similar to the stacking of transformer layers,\nwhich provides the conditions for the application of parame-\nter sharing. In the fine-tuning stage, the two traffic encoders\ncause the double depth of Transformer layers compared to\nthe pre-training stage, but the amount of training data is very\nlimited. Thus, parameter sharing also makes the model eas-\nier to train.\n4 Experiments\n4.1 Experiment Settings\nData Preparation. Our experiments are conducted on\nfive public real-world encrypted traffic datasets IS-\nCXVPN2016 dataset (Habibi Lashkari et al. 2016), ISCX-\nTor2016 dataset (Lashkari et al. 2017), USTC-TFC2016\ndataset (Wang et al. 2017), CICIoT2022 dataset (Dadkhah\net al. 2022), and Cross-Platform dataset (Van Ede et al.\n2020), respectively.\nTo verify the generality of the method, we conduct eval-\nuations on the above five datasets. The first four training\ndatasets form a large-scale unlabeled training dataset for\npre-training. In the fine-tuning stage, our classifier uses five\ntraining datasets for supervised learning to complete the cor-\nresponding traffic classification task. Note that a generic pre-\ntrained model is obtained in the pre-training stage, while\nthe classifier training in the fine-tuning stage is performed\nseparately. Furthermore, since the training dataset of Cross-\nPlatform is not part of the unlabeled dataset in the pre-\ntraining stage, the Cross-Platform dataset is used for transfer\nlearning experiments in Section 4.5.\nImplementation Details. In the pre-training phase, the\nbatch size is 512 and the total step is 150,000. We use the\nlinear learning rate scaling rule and the base learning rate\nis set to 1 × 10−3 with the AdamW optimizer. The mask-\ning ratio for randomly masking patches is set to 0.9. Then,\nwe use the AdamW optimizer in fine-tuning for 200 epochs,\nwhere the base learning rate is set to2×10−3 and batch size\nis 64. The impact of different masking ratios is discussed\nin Section 4.5. The proposed method is implemented using\nPyTorch 1.9.0 and trained on a server with four NVIDIA\nGeForce RTX3090 GPUs.\nEvaluation Metrics. To measure the classification perfor-\nmance of our method, we calculate the number of True Posi-\ntive (Tp), True Negative (Tn), False Positive (Fp), and False\nNegative (Fn). Based on the above definition, Recall, Preci-\nsion, and F1 can be obtained:\nRecall = Tp\nTp + Fn\n, Precision = Tp\nTp + Fp\n, (8)\nF1 = 2· Precision · Recall\nPrecision + Recall . (9)\n4.2 Comparison with State-of-the-Art Methods\nTo comprehensively evaluate our method, we compare\nYaTC with a range of baselines and state-of-the-art meth-\nods, as listed below:\n• FlowPrint (Van Ede et al. 2020) andAppScanner (Taylor\net al. 2016) are ML-based methods using statistical fea-\ntures for traffic classification.\n• DF (Sirinam et al. 2018), Deeppacket (Lotfollahi et al.\n2020), 2D-CNN (Wang et al. 2017), 3D-CNN (Zhang\net al. 2020), and FS-Net (Liu et al. 2019) are DL-based\ntraffic analysis methods, which use raw packet informa-\ntion for supervised learning.\n• PERT (He, Yang, and Chen 2020) and ET-BERT (Lin\net al. 2022) treat traffic representation extraction as an\nNLP task for pre-training, and then fine-tune the classifier\nwith limited labeled data.\nAs is shown in Table 1, the results demonstrate the ef-\nfectiveness of DL-based methods and the insufficiency of\nML-based methods with statistical features. Since only the\nvector of packets’ directions in the flow is used as input\nfeatures, the classification performance of DF is signifi-\ncantly lower than other DL-based methods. Furthermore,\nwe observe that methods without pre-training on the IS-\nCXTor2016 dataset perform poorly. The reason is that en-\ncryption and obfuscation techniques for anonymous traf-\nfic make it difficult to analyze the payload directly. How-\never, pre-training-based methods learn latent representations\n5424\nMethod ISCXVPN2016 ISCXTor2016 USTC-TFC2016 CICIoT2022\nAcc. F1 Acc. F1 Acc. F1 Acc. F1\nFlowPrint 30.29% 14.09% 25.27% 10.19% 25.30% 12.47% 50.46% 49.14%\nAppScanner 79.93% 80.85% 50.27% 49.68% 60.41% 58.36% 76.52% 76.81%\nDF 62.87% 25.40% 33.24% 7.00% 58.45% 49.15% 60.13% 46.35%\nDeeppacket 80.21% 80.17% 36.81% 26.81% 88.49% 88.83% 88.28% 88.08%\n2D-CNN 81.26% 80.64% 34.62% 33.66% 92.26% 92.05% 90.07% 90.00%\n3D-CNN 81.09% 80.79% 34.89% 33.96% 91.55% 91.16% 89.39% 89.33%\nFS-Net 87.64% 87.30% 52.03% 51.64% 87.05% 86.02% 85.37% 85.30%\nPERT 88.62% 88.61% 80.22% 79.99% 96.63% 96.64% 90.52% 90.49%\nET-BERT 87.74% 87.47% 65.38% 64.98% 96.95% 96.95% 90.35% 90.31%\nOurs (YaTC) 98.07% 98.04% 99.72% 99.72% 97.86% 97.86% 96.58% 96.58%\nTable 1: Comparison results on ISCXVPN2016, ISCXTor2016, USTC-TFC2016, and CICIoT2022 datasets.\nFigure 3: The performance comparison with other models\nusing different labeled sample sizes.\nfrom a large amount of unlabeled traffic data for more effec-\ntive classification. Additionally, pre-training methods out-\nperform other methods on all datasets except CICIoT2022,\nillustrating that pre-trained classifiers can achieve better per-\nformance on downstream traffic classification tasks. Benefit-\ning from well-designed traffic representation, model struc-\nture, and training paradigm, YaTC consistently outperforms\nall compared methods by a large margin on all datasets.\nIt can be concluded that our method is suitable for traffic\nanalysis in various scenarios as well as has excellent perfor-\nmance.\n4.3 Few-Shot Analysis\nTo validate the robustness of YaTC in few-shot scenarios, we\nset the labeled data size to 10%, 50%, and 100% and com-\npare YaTC with other baselines and state-of-the-art meth-\nods on four datasets. Experimental results are shown in Fig-\nure 3. Experiments show that the three pre-training-based\nmethods, YaTC, ET-BERT, and PERT, generally outperform\nother supervised methods in few-shot scenarios. Some other\nmethods achieve similar performance to pre-training-based\nmethods in few cases, but none of them could maintain it\non all four datasets. The pre-training enables the model to\nmaster the ability to extract high-quality representations us-\ning unlabeled data, thereby greatly reducing the dependence\non labeled data. Besides, YaTC achieves better performance\nthan ET-BERET and PERT at all sizes of labeled data, indi-\ncating excellent robustness.\n4.4 Ablation Study\nTo further examine the level of benefit that each compo-\nnent of YaTC brings to the performance, an ablation study\nis performed on four baseline datasets. The evaluation re-\nsults are reported in Table 2. First, compared with apply-\ning global attention, our method not only reduces the com-\nplexity (discussed in Section 3.2) but also achieves better\nresults, demonstrating the benefits of packet-level and flow-\nlevel attention mechanisms. Furthermore, removing packet-\nlevel attention leads to significant performance degrada-\ntion under all configurations, which illustrates the consid-\nerable contribution of packet-level feature extraction. We\nnote that flow-level attention without pre-training sometimes\nleads to performance loss, which would not occur after pre-\ntraining. This is because further attention to small data with-\nout pre-training is easier to cause over-fitting. The abla-\ntions also show that applying parameter sharing during fine-\ntuning brings both lightweight and performance enhance-\nments. Moreover, directly transforming the raw traffic bytes\nwithout MFR or removing only the flow-level stacking in\nMFR also results in weaker performance, obviously for the\nencrypted traffic tasks.\n4.5 Discussions\nImpact of Masking Ratio. To explore the effect of the\nmasking ratio on the quality of learned representation, we\nconduct ablation experiments on four datasets with a range\nof masking ratios. Experimental results are shown in Fig-\nure 4. Overall, a higher mask ratio will result in better perfor-\nmance. On the other hand, an excessively high masking ratio\nmakes the reconstruction task too difficult, and the F1 score\n5425\nMethod ISCXVPN2016 ISCXTor2016 USTC-TFC2016 CICIoT2022\nAcc. F1 Acc. F1 Acc. F1 Acc. F1\nOurs (YaTC) 98.07% 98.04% 99.72% 99.72% 97.86% 97.86% 96.58% 96.58%\nOurs with GA 95.27% 95.14% 98.63% 98.62% 97.86% 97.86% 95.64% 95.61%\nOurs w/o PA 90.19% 90.03% 78.02% 77.28% 96.03% 96.03% 92.84% 92.78%\nOurs w/o FA 95.62% 95.49% 99.18% 99.18% 97.66% 97.62% 95.81% 95.80%\nOurs w/o FS 92.47% 92.35% 97.80% 97.77% 93.48% 93.48% 94.58% 94.57%\nOurs w/o PS 97.55% 97.53% 99.45% 99.45% 97.45% 97.40% 95.41% 95.39%\nOurs w/o PT 87.74% 87.22% 92.03% 91.90% 95.32% 95.25% 92.70% 92.65%\nOurs w/o PT & PA 78.63% 77.58% 39.84% 38.58% 93.28% 93.22% 90.88% 90.79%\nOurs w/o PT & FA 87.74% 87.40% 85.99% 85.84% 95.52% 95.46% 93.19% 93.17%\nOurs w/o PT & FS 81.96% 81.84% 83.52% 83.15% 91.75% 91.48% 91.59% 91.59%\nOurs w/o PT & MFR 80.91% 80.49% 42.86% 42.11% 93.99% 93.90% 91.36% 91.26%\nTable 2: Ablation study of key components in YaTC on ISCXVPN2016, ISCXTor2016, USTC-TFC2016, and CICIoT2022\ndatasets. The abbreviations are explained as follows, GA: global attention, PA: packet-level attention, FA: flow-level attention,\nFS: flow-level stacking, PS: parameter sharing, MFR: multi-level traffic representation, and PT: pre-training.\n25 50 75 90 95\nMasking Ratio (%)\n92\n94\n96\n98\n100F1 (%)\nISCXVPN2016\nISCXTor2016\nUSTC-TFC2016\nCICIoT2022\nFigure 4: The impact of masking ratio on the quality of\nlearned representation.\ndecreases rapidly. The F1 score reaches the highest when\nthe masking ratio is 90% on most datasets except USTC-\nTFC2016, which also has a high best masking ratio of 75%.\nThis supports our view of the traffic task as being more simi-\nlar to images than to words. In the masking strategy of NLP,\nsuch as BERT, the optimal masking radio maintains a low\nvalue and usually does not exceed 20%, which is the oppo-\nsite of the CV according to the recent study (He et al. 2022).\nA high optimal masking rate implies a large information re-\ndundancy, and traffic classification does not require a thor-\nough understanding of its content, which is also impossible\non the encrypted payload. For classification tasks, words are\nhigh-level and abstract information, but traffic bytes are just\nsparse features without clear semantic units, similar to pix-\nels. Hence, rather than the BERT-based methods treating raw\nbytes as words, our image-like MFR matrix is more rational.\nTransfer Learning Experiments. We evaluate transfer\nlearning on the Cross-Platform dataset. Two other pre-\ntraining traffic classification methods (PERT and ET-BERT)\nare introduced for comparison. The pre-trained models are\nall identical to those used in the previous experiments (i.e.,\npre-trained with the other four datasets). It can be seen from\nFigure 5 that our YaTC significantly improves the F1 score\nfrom 69.93% to 82.35%, which is 12.42% better than with-\nout pre-training. Furthermore, both ET-BERT and PERT\nYaTC ET-BERT PERT60\n65\n70\n75\n80\n85F1 (%)\n82.35\n67.32 68.16\n69.93\n67.05 67.97\nwith pre-training\nw/o  pre-training\nFigure 5: The performance comparison with the other two\npre-training methods on the Cross-Platform dataset.\nhave weak boosts using pre-training, indicating that their\npre-trained models are difficult to transfer to the new down-\nstream traffic classification task.\n5 Conclusion\nIn this work, we proposed YaTC, an MAE-based Traffic\nTransformer with MFR for traffic classification. With the\nMAE-based self-supervised learning paradigm, YaTC first\nlearns generic latent representations from a large amount\nof unlabeled traffic data in the pre-training stage, then per-\nforms supervised learning with a few labeled data for a se-\nries of traffic classification tasks in the fine-tuning stage. Our\nmethod is evaluated on five real-world traffic datasets. Ex-\nperimental results show that our YaTC is ahead of the state-\nof-the-art methods, even in the case of minimal labeled data.\nBesides, our pre-trained model exhibits excellent transfer\nability for the new downstream traffic classification task.\nAcknowledgments\nWe are grateful to anonymous reviewers for their construc-\ntive comments on this work. This work is supported by\nSJTU-QI’ANXIN Joint Lab of Information System Secu-\nrity.\n5426\nReferences\nCai, Z.; Jiang, B.; Lu, Z.; Liu, J.; and Ma, P. 2019. isAnon:\nFlow-Based Anonymity Network Traffic Identification Us-\ning Extreme Gradient Boosting. In International Joint Con-\nference on Neural Networks (IJCNN), 1–8.\nCaron, M.; Touvron, H.; Misra, I.; J ´egou, H.; Mairal, J.;\nBojanowski, P.; and Joulin, A. 2021. Emerging Properties\nin Self-Supervised Vision Transformers. In IEEE/CVF In-\nternational Conference on Computer Visio (ICCV), 9630–\n9640.\nChen, X.; Xie, S.; and He, K. 2021. An Empirical\nStudy of Training Self-Supervised Vision Transformers.\nIn IEEE/CVF International Conference on Computer Visio\n(ICCV), 9620–9629.\nDadkhah, S.; Mahdikhani, H.; Danso, P. K.; Zohourian, A.;\nTruong, K. A.; and Ghorbani, A. A. 2022. Towards the\nDevelopment of a Realistic Multidimensional IoT Profiling\nDataset. In International Conference on Privacy, Security &\nTrust.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In North American Chapter of\nthe Association for Computational Linguistics: Human Lan-\nguage Technologies (NAACL-HLT), 4171–4186.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations (ICLR).\nHabibi Lashkari, A.; Draper Gil, G.; Mamun, M.; and Ghor-\nbani, A. 2016. Characterization of encrypted and VPN traf-\nfic using time-related features. In International Conference\non Information Systems Security and Privacy, 407–414.\nHe, H.; Yang, Z.; and Chen, X. 2020. PERT: Payload Encod-\ning Representation from Transformer for Encrypted Traffic\nClassification. In 2020 ITU Kaleidoscope: Industry-Driven\nDigital Transformation, 1–8.\nHe, K.; Chen, X.; Xie, S.; Li, Y .; Doll ´ar, P.; and Girshick,\nR. B. 2022. Masked Autoencoders Are Scalable Vision\nLearners. In IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 1–10.\nLan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.;\nand Soricut, R. 2020. ALBERT: A Lite BERT for Self-\nsupervised Learning of Language Representations. In Inter-\nnational Conference on Learning Representations (ICLR).\nLashkari, A. H.; Draper-Gil, G.; Mamun, M. S. I.; and Ghor-\nbani, A. A. 2017. Characterization of Tor Traffic Using Time\nBased Features. In International Conference on Information\nSystem Security and Privacy, 253–262.\nLin, K.; Xu, X.; and Gao, H. 2021. TSCRNN: A novel\nclassification scheme of encrypted traffic based on flow spa-\ntiotemporal features for efficient management of IIoT.Com-\nputer Networks, 190: 107974.\nLin, X.; Xiong, G.; Gou, G.; Li, Z.; Shi, J.; and Yu, J. 2022.\nET-BERT: A Contextualized Datagram Representation with\nPre-training Transformers for Encrypted Traffic Classifica-\ntion. In The Web Conference (WWW), 633–642.\nLiu, C.; He, L.; Xiong, G.; Cao, Z.; and Li, Z. 2019. FS-Net:\nA Flow Sequence Network For Encrypted Traffic Classifi-\ncation. In IEEE Conference on Computer Communications\n(INFOCOM), 1171–1179.\nLotfollahi, M.; Siavoshani, M. J.; Zade, R. S. H.; and\nSaberian, M. 2020. Deep packet: a novel approach for en-\ncrypted traffic classification using deep learning. Soft Com-\nput., 24(3): 1999–2012.\nNguyen, T. T.; and Armitage, G. 2008. A Survey of\nTechniques for Internet Traffic Classification Using Ma-\nchine Learning. IEEE Communications Surveys & Tutorials,\n10(4): 56–76.\nPanchenko, A.; Lanze, F.; Pennekamp, J.; Engel, T.; Zinnen,\nA.; Henze, M.; and Wehrle, K. 2016. Website Fingerprint-\ning at Internet Scale. In Network and Distributed System\nSecurity Symposium (NDSS), 1–15.\nPapadogiannaki, E.; and Ioannidis, S. 2021. A Survey\non Encrypted Network Traffic Analysis Applications, Tech-\nniques, and Countermeasures. ACM Computing Surveys,\n54(6).\nShen, M.; Liu, Y .; Zhu, L.; Xu, K.; Du, X.; and Guizani, N.\n2020. Optimizing Feature Selection for Efficient Encrypted\nTraffic Classification: A Systematic Approach. IEEE Net-\nwork, 34(4): 20–27.\nSirinam, P.; Imani, M.; Ju ´arez, M.; and Wright, M. 2018.\nDeep Fingerprinting: Undermining Website Fingerprinting\nDefenses with Deep Learning. In ACM SIGSAC Conference\non Computer and Communications Security (CCS), 1928–\n1943.\nTakase, S.; and Kiyono, S. 2021. Lessons on Parameter\nSharing Across Layers in Transformers. arXiv preprint\narXiv:2104.06022.\nTaylor, V . F.; Spolaor, R.; Conti, M.; and Martinovic, I.\n2016. AppScanner: Automatic Fingerprinting of Smart-\nphone Apps from Encrypted Network Traffic. InIEEE Euro-\npean Symposium on Security and Privacy (EuroS&P), 439–\n454.\nVan Ede, T.; Bortolameotti, R.; Continella, A.; Ren, J.;\nDubois, D. J.; Lindorfer, M.; Choffnes, D.; van Steen, M.;\nand Peter, A. 2020. Flowprint: Semi-supervised mobile-app\nfingerprinting on encrypted network traffic. In Network and\nDistributed System Security Symposium (NDSS), volume 27.\nWang, W.; Zhu, M.; Zeng, X.; Ye, X.; and Sheng, Y . 2017.\nMalware Traffic Classification Using Convolutional Neural\nNetwork for Representation Learning. InInternational Con-\nference on Information Networking, 712–717.\nYao, H.; Gao, P.; Zhang, P.; Wang, J.; Jiang, C.; and Lu, L.\n2019. Hybrid Intrusion Detection System for Edge-Based\nIIoT Relying on Machine-Learning-Aided Detection. IEEE\nNetwork, 33(5): 75–81.\nZhang, J.; Li, F.; Ye, F.; and Wu, H. 2020. Autonomous\nUnknown-Application Filtering and Labeling for DL-based\nTraffic Classifier Update. In IEEE Conference on Computer\nCommunications (INFOCOM), 397–405.\n5427",
  "topic": "Autoencoder",
  "concepts": [
    {
      "name": "Autoencoder",
      "score": 0.8355762958526611
    },
    {
      "name": "Computer science",
      "score": 0.8025184869766235
    },
    {
      "name": "Byte",
      "score": 0.5815776586532593
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5698227286338806
    },
    {
      "name": "Traffic classification",
      "score": 0.524864912033081
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5180554389953613
    },
    {
      "name": "Transformer",
      "score": 0.5142764449119568
    },
    {
      "name": "Machine learning",
      "score": 0.5114544630050659
    },
    {
      "name": "Feature learning",
      "score": 0.4893268942832947
    },
    {
      "name": "Network packet",
      "score": 0.4706617593765259
    },
    {
      "name": "Data mining",
      "score": 0.46176499128341675
    },
    {
      "name": "Feature extraction",
      "score": 0.44834229350090027
    },
    {
      "name": "Deep learning",
      "score": 0.4366907775402069
    },
    {
      "name": "Computer network",
      "score": 0.10064184665679932
    },
    {
      "name": "Engineering",
      "score": 0.09736281633377075
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I41198531",
      "name": "Nanjing University of Posts and Telecommunications",
      "country": "CN"
    }
  ]
}