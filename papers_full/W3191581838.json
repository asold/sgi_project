{
  "title": "StrucTexT: Structured Text Understanding with Multi-Modal Transformers",
  "url": "https://openalex.org/W3191581838",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2059017717",
      "name": "Li Yulin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2371852212",
      "name": "Qian Yu-xi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5063726370",
      "name": "YU Yuchen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231797903",
      "name": "Qin, Xiameng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2359890161",
      "name": "Zhang, Chengquan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100987257",
      "name": "Liu Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2142758258",
      "name": "Yao Kun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1990936404",
      "name": "Han Jun-yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3114969528",
      "name": "Liu, Jingtuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2392517532",
      "name": "Ding, Errui",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2194187530",
    "https://openalex.org/W3014074635",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2599765304",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3158362813",
    "https://openalex.org/W2967155990",
    "https://openalex.org/W2986619406",
    "https://openalex.org/W2962772269",
    "https://openalex.org/W3003335533",
    "https://openalex.org/W3090669478",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3003478049",
    "https://openalex.org/W2153182373",
    "https://openalex.org/W3034864438",
    "https://openalex.org/W2605982830",
    "https://openalex.org/W2963855133",
    "https://openalex.org/W3113753692",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W1791544012",
    "https://openalex.org/W3190448953",
    "https://openalex.org/W2891117443",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2898700358",
    "https://openalex.org/W2948784110",
    "https://openalex.org/W3003484198",
    "https://openalex.org/W3092968218",
    "https://openalex.org/W3173306993",
    "https://openalex.org/W3163650427",
    "https://openalex.org/W3003261556",
    "https://openalex.org/W2951692098",
    "https://openalex.org/W3110398855",
    "https://openalex.org/W3175248253",
    "https://openalex.org/W2972985407",
    "https://openalex.org/W2963353821",
    "https://openalex.org/W3093218477",
    "https://openalex.org/W3113463745",
    "https://openalex.org/W2963687456",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2922714365",
    "https://openalex.org/W3092515419",
    "https://openalex.org/W3174349596",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3035089734",
    "https://openalex.org/W3099103595",
    "https://openalex.org/W3104953317",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2935408319"
  ],
  "abstract": "Structured text understanding on Visually Rich Documents (VRDs) is a crucial part of Document Intelligence. Due to the complexity of content and layout in VRDs, structured text understanding has been a challenging task. Most existing studies decoupled this problem into two sub-tasks: entity labeling and entity linking, which require an entire understanding of the context of documents at both token and segment levels. However, little work has been concerned with the solutions that efficiently extract the structured data from different levels. This paper proposes a unified framework named StrucTexT, which is flexible and effective for handling both sub-tasks. Specifically, based on the transformer, we introduce a segment-token aligned encoder to deal with the entity labeling and entity linking tasks at different levels of granularity. Moreover, we design a novel pre-training strategy with three self-supervised tasks to learn a richer representation. StrucTexT uses the existing Masked Visual Language Modeling task and the new Sentence Length Prediction and Paired Boxes Direction tasks to incorporate the multi-modal information across text, image, and layout. We evaluate our method for structured text understanding at segment-level and token-level and show it outperforms the state-of-the-art counterparts with significantly superior performance on the FUNSD, SROIE, and EPHOIE datasets.",
  "full_text": "StrucTexT: Structured Text Understanding with Multi-Modal\nTransformers\nYulin Liâˆ—\nDepartment of Computer Vision\nTechnology (VIS), Baidu Inc.\nliyulin03@baidu.com\nYuxi Qianâˆ—\nBeijing University of Posts\nand Telecommunications\nqianyuxi@bupt.edu.cn\nYuechen Yuâˆ—\nDepartment of Computer Vision\nTechnology (VIS), Baidu Inc.\nyuyuechen@baidu.com\nXiameng Qin\nDepartment of Computer Vision\nTechnology (VIS), Baidu Inc.\nqinxiameng@baidu.com\nChengquan Zhangâ€ \nDepartment of Computer Vision\nTechnology (VIS), Baidu Inc.\nzhangchengquan@baidu.com\nYan Liu\nTaikang Insurance Group\nliuyan146@taikanglife.com\nKun Yao, Junyu Han\nDepartment of Computer Vision\nTechnology (VIS), Baidu Inc.\n{yaokun01,hanjunyu}@baidu.com\nJingtuo Liu, Errui Ding\nDepartment of Computer Vision\nTechnology (VIS), Baidu Inc.\n{liujingtuo,dingerrui}@baidu.com\nABSTRACT\nStructured text understanding on Visually Rich Documents (VRDs)\nis a crucial part of Document Intelligence. Due to the complexity\nof content and layout in VRDs, structured text understanding has\nbeen a challenging task. Most existing studies decoupled this prob-\nlem into two sub-tasks: entity labeling and entity linking , which\nrequire an entire understanding of the context of documents at\nboth token and segment levels. However, little work has been con-\ncerned with the solutions that efficiently extract the structured\ndata from different levels. This paper proposes a unified framework\nnamed StrucTexT, which is flexible and effective for handling both\nsub-tasks. Specifically, based on the transformer, we introduce a\nsegment-token aligned encoder to deal with the entity labeling\nand entity linking tasks at different levels of granularity. Moreover,\nwe design a novel pre-training strategy with three self-supervised\ntasks to learn a richer representation. StrucTexT uses the exist-\ning Masked Visual Language Modeling task and the new Sentence\nLength Prediction and Paired Boxes Direction tasks to incorporate\nthe multi-modal information across text, image, and layout. We eval-\nuate our method for structured text understanding at segment-level\nand token-level and show it outperforms the state-of-the-art coun-\nterparts with significantly superior performance on the FUNSD,\nSROIE, and EPHOIE datasets.\nâˆ—Equal contribution. This work is done when Yuxi Qian is an intern at Baidu Inc.\nâ€ Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nMM â€™21, October 20â€“24, 2021, Virtual Event, China\nÂ© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8651-7/21/10. . . $15.00\nhttps://doi.org/10.1145/3474085.3475345\nCCS CONCEPTS\nâ€¢ Information systems â†’Document structure; Information\nextraction.\nKEYWORDS\ndocument understanding, document information extraction, pre-\ntraining\nACM Reference Format:\nYulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin, Chengquan Zhang, Yan Liu,\nKun Yao, Junyu Han, and Jingtuo Liu, Errui Ding. 2021. StrucTexT: Struc-\ntured Text Understanding with Multi-Modal Transformers. In Proceedings\nof the 29th ACM International Conference on Multimedia (MM â€™21), Octo-\nber 20â€“24, 2021, Virtual Event, China. ACM, New York, NY, USA, 9 pages.\nhttps://doi.org/10.1145/3474085.3475345\n1 INTRODUCTION\nUnderstanding the structured document is a critical component of\ndocument intelligence that automatically explores the structured\ntext information from the Visually Rich Documents (VRDs) such as\nforms, receipts, invoices, etc. Such task aims to extract the key infor-\nmation of text fields and the links among the semantic entities from\nVRDs, which named entity labeling and entity linking tasks [15]\nrespectively. Structured text understanding has attracted increasing\nattention in both academia and industry. In reality, it plays a crucial\nrole in developing digital transformation processes in office au-\ntomation, accounting systems, and electronically archived. It offers\nbusinesses significant time savings on processing the million of\nforms and invoices every day.\nTypical structure extraction methods rely on preliminary Op-\ntical Character Recognition (OCR) engines [19, 34, 39, 40, 47, 49]\nto understand the semantics of documents. As shown in Figure 1,\nthe contents in a document can be located as several text segments\n(pink dotted boxes) by text detectors. The entity fields are presented\nin three forms: partial characters, an individual segment, and mul-\ntiple segment lines. Traditional methods for entity labeling often\nformulated the task as a sequential labeling problem. In this setup,\narXiv:2108.02923v3  [cs.CV]  8 Nov 2021\nGradeSubject\n(a) Token-based Entity Labeling\nCompanyAddress\nDate\nTotalMoney (b) Segment-based Entity Labeling\n (c) Entity Linking\nFigure 1: Examples of VRDs and their key extraction information. The dotted boxes are the text regions and the solid ones are\nthe semantic entity regions. (a) The entity extraction in token-level characters. (b) The entity extraction in segment-level text\nlines. (c) The relationship extraction with key-value pairs at segment-level.\nthe text segments are serialized as a linear sequence with a pre-\ndefined order. Then a Named Entity Recognition (NER) [ 17, 23]\nmodel is utilized to label each token such as word or character\nwith an IOB (Inside, Outside, Beginning) tag. However, the capa-\nbility is limited as the manner that is performed at token-level. As\nshown examples of Figure 1b and 1c, VRDs are usually organized\nin a number of text segments. The segment-level textual content\npresents richer geometric and semantic information, which is vital\nfor structured text understanding. Several methods [1, 8, 14, 41] fo-\ncus on a segment-level representation. On the contrary, they cannot\ncope with the entity composed of characters as shown in Figure 1a.\nTherefore, a comprehensive technique of structure extraction at\nboth segment-level and token-level is worth considering.\nNowadays, accurate understanding of the structured text from\nVRDs remains a challenge. The key to success is the full use of mul-\ntiple modal features from document images. Early solutions solve\nthe entity tasks by only operating on plain texts, resulting in a se-\nmantic ambiguity. Noticing the rich visual information contained in\nVRDs, several methods [6, 16, 26, 32] exploit 2D layout information\nto provides complementation for textual content. Besides, for fur-\nther improvement, mainstream researches [2, 21, 24, 30, 38, 48, 50]\nusually employ a shallow fusion of text, image, and layout to cap-\nture contextual dependencies. Recently, several pre-training mod-\nels [28, 45, 46] have been proposed for joint learning the deep fusion\nof cross-modality on large-scale data and outperform counterparts\non document understanding. Although these pre-training models\nconsider all modalities of documents, they focus on the contribution\nrelated to the text side with less elaborate visual features.\nTo address the above limitations, in this paper, we propose a\nuniform framework named StrucTexT that incorporates the fea-\ntures from different levels and modalities to effectively improves\nthe understanding of various document structures. Inspired by re-\ncent developments in vision-language transformers [ 22, 35], we\nintroduce a transformer encoder to learn cross-modal knowledge\nfrom both images of segments and tokens of words. In addition,\nwe construct an extra segment ID embedding to associate visual\nand textual features at different granularity. Meanwhile, we attach\na 2D position embedding to involve the layout clue. After that, a\nHadamard product works on the encoded features between dif-\nferent levels and modalities for advanced feature fusion. Hence,\nStrucTexT can support segment-level and token-level tasks of struc-\ntured text understanding in a single framework. Figure 2 shows the\narchitecture of our proposed method.\nTo promote the representation capacity of multi-modality, we\nfurther introduce three self-supervised tasks for pre-training learn-\ning of text, image, and layout. Specifically, following the work of\nLayoutLM [45], the Masked Visual Language Modeling (MVLM)\ntask is utilized to extract contextual information. In addition, we\npresent two tasks named Sentence Length Prediction (SLP) and\nPaired Boxes Direction (PBD). SLP task predicts the segment length\nfor enhancing the internal semantics of an entity candidate. PBD\ntask is training to identify the relative direction within a sampled\nsegment pair, which helps our framework discover the geometric\nstructure topology. The three self-supervised tasks make full use of\nboth textual and visual features of the documents. An unsupervised\npre-training strategy with above all tasks is applied at first to get\nan enhanced feature encoder.\nMajor contributions of this paper are summarized as follows:\n(1) In this paper, we present a novel framework named Struc-\nTexT to tackle the tasks of structured text understanding\nwith a unified solution. It efficiently extracts semantic fea-\ntures from different levels and modalities to handle the entity\nlabeling and entity linking tasks.\n(2) We introduce improved multi-task pre-training strategies\nto extract the multi-modal information from VRDs by self-\nsupervised learning. In addition to the MVLM task that ben-\nefits the textual context, we proposed two new pre-training\ntasks of SLP and PBD to take advantage of image and layout\nfeatures. We adopt the three tasks during the pre-training\nstage for a richer representation.\n(3) Extensive experiments on real-world benchmarks show the\nsuperior performance of StrucTexT compared with the state-\nof-the-art methods. Additional ablation studies demonstrate\nthe effectiveness of our pre-training strategies.\n2 RELATED WORK\nStructured Text Understanding The task of structured text un-\nderstanding is to retrieve structured data from VRDs automati-\ncally. It requires the model to extract the semantic structure of\ntextual content robustly and effectively, assigning the major pur-\npose into two parts [15]: entity labeling and entity linking. Gen-\nerally speaking, the entity labeling task is to find named enti-\nties. The entity linking task is to extract the semantic relation-\nships as key-value pairs between entities. Most existing meth-\nods [6, 7, 13, 38, 45, 46, 48, 50] design a NER framework to perform\nentity labeling as a sequence labeling task at token-level. However,\ntraditional NER models organize text in one dimension depending\non the reading order and are unsuitable for VRDs with complex\nlayouts. Recent studies [29, 38, 42, 45, 46, 48, 50] have realized the\nsignificance of segment-level features and incorporate a segment\nembedding to attach extra higher semantics. Although those meth-\nods, such as PICK [48] and TRIE [50], construct contextual features\ninvolving the segment clues, they revert to token-level labeling with\nNER-based schemes. Several works [1, 2, 14, 41] design their meth-\nods at segment-level to solve the tasks of entity labeling and entity\nlinking. Cheng et al. [2] utilizes an attention-based network to ex-\nplore one-shot learning for the text field labeling. DocStruct [41]\npredicts the key-value relations between the extracted text seg-\nments to establish a hierarchical document structure. With the\ngraph-based paradigm, Carbonell et al. [1] and Hwang et al. [14]\ntackle the entity labeling and entity linking tasks simultaneously.\nHowever, they donâ€™t consider the situation where a text segment\nincludes more than one category, which is difficult to identify the\nentity in token granularity.\nIn summary, the methods mentioned above can only handle\none granularity representation. To this end, we propose a unified\nframework to support both token-level and segment-level struc-\ntured extraction for VRDs. Our model is flexible to any granularity-\nmodeling tasks for structured text understanding.\nMulti-Modal Feature Representations One of the most impor-\ntant modules of structured information extraction is to understand\nmulti-modal semantic features. Previous works [3, 5, 7, 13, 17, 27, 31]\nusually adopt language models to extract entities from the plain\ntext. These NLP-based approaches typically operate on text se-\nquences and do not incorporate visual and layout information.\nLater studies [6, 16, 32] firstly tend to explore layout information\nto aid entity extraction from VRDs. Post-OCR [ 13] reconstructs\nthe text sequences based on their bounding boxes. VS2 [32] lever-\nages the heterogeneous layout to perform the extraction in visual\nlogical blocks. A range of other methods [ 6, 16, 51] represent a\ndocument as a 2D grid with text tokens to obtain the contextual\nembedding. After that, some researchers realize the necessity of\nmulti-modal fusion and develop performance by integrating visual\nand layout information. GraphIE [29], PICK [48] and Liu et al. [21]\ndesign a graph-based decoder to improve the semantics of context\ninformation. Hwang et al. [14] and Wang et al. [41] leverage the rel-\native coordinates and explore the link of each key-value pair. These\nmethods only use simple early fusion strategies, such as addition\nor concatenation, without considering the semantic gap of differ-\nent modalities. Recently, pre-training models [7, 22, 35, 36] show a\nstrong feature representation using large-scale unlabeled training\nsamples. Inspired by this, several works [28, 45, 46] combine pre-\ntraining techniques to improve multi-modal features. Pramanik et al.\n[28] introduces a multi-task learning-based framework to yield a\ngeneric document representation. LayoutLMv2 [46] uses 11 million\nscanned documents to obtain a pre-trained model, which shows the\nstate-of-the-art performance in several downstream tasks of docu-\nment understanding. However, these pre-training strategies mainly\nfocus on the expressiveness of language but underuse the struc-\ntured information from images. Hence, we propose a self-supervised\npre-training strategy to better explore the potentials information\nfrom text, image, and layout. Compared with LayoutLMv2, the new\nstrategy supports more useful features with less training data.\n3 APPROACH\nFigure 2 shows the overall illustration of StrucTexT. Given an input\nimage with preconditioned OCR results, such as bounding boxes\nand content of text segments. We leverage various information\nfrom text, image, and layout aspects by a feature embedding stage.\nAnd then, the multi-modal embeddings are fed into the pre-trained\ntransformer network to obtain rich semantic features. The trans-\nformer network has accomplished the cross-modality fusion by\nestablishing interactions between the different modality inputs. At\nlast, the Structured Text Understanding module receives the en-\ncoded features and carries out entity recognition for entity labeling\nand relation extraction for entity linking.\n3.1 Multi-Modal Feature Embedding\nGiven a document image ğ¼ with ğ‘›text segments, we perform open\nsource OCR algorithms [33, 52] to obtain the ğ‘–-th segment region\nwith the top-left and bottom-right bounding boxğ‘ğ‘– = (ğ‘¥0,ğ‘¦0,ğ‘¥1,ğ‘¦1)\nand its corresponding text sentence ğ‘¡ğ‘– = {ğ‘ğ‘–\n1,ğ‘ğ‘–\n2,Â·Â·Â· ,ğ‘ğ‘–\nğ‘™ğ‘–\n}, where ğ‘\nis a word or character and ğ‘™ğ‘– is the length of ğ‘¡ğ‘– .\nLayout Embedding For every segment or word, we use the en-\ncoded bounding boxes as their layout information\nL = Embğ‘™ (ğ‘¥0,ğ‘¦0,ğ‘¥1,ğ‘¦1,ğ‘¤,â„ ) (1)\nwhere Embğ‘™ is a layout embedding layer and ğ‘¤,â„ is the shape of\nbounding box ğ‘. It is worth mentioning that we estimate the bound-\ning box of a word by its belonging text segment in consideration of\nsome OCR results without word-level information.\nOCREngine\nâ€¦CASENAME:DonaldD.Sellersâ€¦\nVisualSegments\nLanguageSentences\nVisualSegmentEmbedding\nLanguageTokenEmbedding\nTransformer\nMulti-ModalInformationEmbeddingCross-ModalInformationFusion\nHeadQuestionAnswerOther\n0.040.910.030.02\nV0V1V2\nV0V1V2\nStructuredTextUnderstanding\n[CLS]\n[SEP]\nT1\nT3\nT4\nV0\nV1\nV2\n[PAD]\nB(t1)\nB(t2)\nB(t3)\nB(t4)\nB(t5)\nB(0)\nB(v0)\nB(v1)\nB(v2)\nB(0)\nB(0)\nLayoutEmbeddingOtherEmbeddings\nâ€¦\nT2\nT5â€¦\nâ€¦\n1\n1\n2\n2\n2\n0\n0\n1\n2\n-1\nN â€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦ â€¦\nâ€¦\nâ€¦\nâ€¦\nText-imageEmbeddingSegmentIDEmbedding [CLS]\n[SEP]\nT1\nT3\nT4\nV0\nV1\nV2\n[PAD]â€¦\nT2\nT5â€¦\nT1\nT3\nT4\nV1\nV2\nT2\nT5 T1T2[MASK]T4T5 V1V0 [PAD][SEP][CLS]\nT3\nPairedBoxesDirection\nSentenceLengthPrediction\nMaskVisionLanguageModel\nEntityLabelling\nEntityLinking\nLength:3Direction:left\nFusedFeatures\nEncodedFeatures\nV2\nPre-trainingtasks\nDocument\nFigure 2: An overall illustration of the model framework and the inform extraction tasks for StrucTexT.\nLanguage Token Embedding Following the common practice [7],\nwe utilize the WordPiece [43] to tokenize text sentences. After that,\nall of text sentences are gathered as a sequenceS by sorting the text\nsegments from the top-left to bottom-right. Intuitively, a pair of\nspecial tags [CLS] and [SEP] are added at the beginning and end of\nthe sequence, as ğ‘¡0 = {[CLS]},ğ‘¡ğ‘›+1 = {[SEP]}. Thus, we can define\nthe language sequence S as follows\nS = {ğ‘¡0,ğ‘¡1,Â·Â·Â· ,ğ‘¡ğ‘›,ğ‘¡ğ‘›+1}\n=\nn\n[CLS],ğ‘1\n1,Â·Â·Â· ,ğ‘1\nğ‘™1\n,Â·Â·Â· ,ğ‘ğ‘›\n1,Â·Â·Â· ,ğ‘ğ‘›\nğ‘™ğ‘›\n,[SEP]\no (2)\nThen, we sum the embedded feature of ğ‘† and layout embedding\nğ¿to obtain the language embedding T\nT = Embğ‘¡ (S)+L (3)\nwhere Embğ‘¡ is a text embedding layer.\nVisual Segment Embedding In the model architecture, we use\nResNet50 [44] with FPN [20] as the image feature extractor to gener-\nate feature maps of ğ¼. Then, the image feature of each text segment\nis extracted from the CNN maps by RoIAlign [10] according to ğ‘.\nThe visual segment embedding V is computed as\nV = Embğ‘£ (ROIAlign(CNN(ğ¼),ğ‘))+ L (4)\nwhere ğ¸ğ‘šğ‘ğ‘£ is the visual embedding layer. Furthermore, the entire\nfeature maps of image ğ¼ is embedded as V0 to introduce the global\ninformation into image features.\nSegment ID Embedding Compared with the vision-language tasks\nbased on wild pictures, understanding the structured document re-\nquires higher semantics to identify the ambiguous entities. Thus,\nwe propose a segment ID embedding ğ‘†ğ‘–ğ‘‘ to allocate a unique num-\nber to each text segment with its image and text features, which\nmakes an explicit alignment of cross-modality clues.\nOther Embeddings In addition, we add two other embeddings [22,\n35] into the input. The position embeddingğ‘ƒğ‘–ğ‘‘ encodes the indexes\nfrom 1 to maximum sequence length, and the segment embedding\nğ‘€ğ‘–ğ‘‘ denotes the modality for each feature. All above embeddings\nhave the same dimensions. In the end, the input of our model is\nrepresented as the combination of the embeddings.\nInput = Concat(ğ‘‡,ğ‘‰ )+ğ‘†ğ‘–ğ‘‘ +ğ‘ƒğ‘–ğ‘‘ +ğ‘€ğ‘–ğ‘‘ (5)\nMoreover, we append several [PAD] tags to fill the short input\nsequence to a fixed length. An empty bounding box with zeros is\nassigned to the special [CLS], [SEP], and [PAD] tags.\n3.2 Multi-Modal Feature Enhance Module\nStrucText collects multi-modal information from visual segments,\ntext sentences, and position layouts to produce an embedding se-\nquence. We support an image-text alignment between different\ngranularities by leveraging the segment IDs mentioned above. At\nthis stage, we perform a transformer network to encode the em-\nbedding sequence to establish deep fusion between modalities and\ngranularities. Crucially, three self-supervised tasks encode the in-\nput features during the pre-training stage to learn task-agnostic\njoint representations. The details are introduced as follows, where\npatterns of all self-supervised tasks are as shown in Figure 3.\nTask 1: Masked Visual Language Modeling\nThe MVLM task promotes capturing a contextual representation\non the language side. Following the pattern of masked multi-modal\nmodeling in ViLBERT [22], we select 15% tokens from the language\nsequences, mask 80% among them with a [MASK] token, replace\n10% of that with random tokens, and keep 10% tokens unchanged.\nThen, the model is required to reconstruct the corresponding tokens.\nRather than following the image region mask in ViLBERT, we retain\nall other information and encourage the model to hunt for the cross-\nmodality clues at all possible.\nTask 2: Segment Length Prediction\n[CLS][MASK] 3 1998FILED [SEP]\nLanguageSentences ViusalSegments\nCross-ModalFeatureFusionwithTransformer\n01234567V1\nSentenceLength:2 V1 V2\nV2\n[MASK]\n[CLS]T1 T4 T5T2 [SEP]T3\nText-imageEmbeddings\nEncodedFeatures\nSelf-supervisedPre-trainingTasks\nDATEAugust\nMaskedVisual LanguageModelingSentenceLengthPredictionPairedBoxesDirectionT1 T2\nFigure 3: The illustration of cross-modal information fusion. Three self-supervised tasks MVLM, SLP, and PBD introduced in\nSection 3.1 are employed simultaneously on the visual and language embeddings in the pre-training stage.\nBesides the MLVM, we introduce a new self-supervised task called\nSequence Length Prediction (SLP) to excavate fine-grained seman-\ntic information on the image side. The SLP task asks the model\nto recognize the length of the text segment from each visual fea-\nture. In this way, we force the encoder to learn from the image\nfeature, more importantly, the language sequence knowledge via\nthe same segment ID. We argue that this information flow could\naccelerate the deep cross-modal fusion among textual, visual, and\nlayout information.\nMoreover, to avoid the disturbance of sub-words produced by\nWordPiece [43], we only count each first sub-word for keeping\nthe same length between language sequences and image segments.\nTherefore, we build an extra alignment between two granularities,\nwhich is simple but effective.\nTask 3: Paired Box Direction\nFurthermore, our third self-supervised task, Paired Box Direction\n(PBD), is designed to exploit global layout information. The PBD\ntask aims at learning a comprehensive geometric topology for doc-\nument structures by predicting the pairwise spatial relationships\nof text segments. First of all, we divide the field of 360 degrees into\neight identical buckets. Secondly, we compute the angleğœƒğ‘– ğ‘—between\nthe text segment ğ‘–and ğ‘— and label it with one of the buckets. Next,\nwe carry out the subtraction between two visual features on the\nimage side and take the result â–³^ğ‘‰ğ‘– ğ‘—as the input of the PBD\nâ–³^ğ‘‰ğ‘– ğ‘—= ^ğ‘‰ğ‘– âˆ’^ğ‘‰ğ‘— (6)\nwhere we use the^symbol to denote the features after transformer\nencoding. ^ğ‘‰ğ‘– and ^ğ‘‰ğ‘— express the visual features for ğ‘–-th segment\nand ğ‘—-th segment.\nFinally, we define PBD as a classification task to estimate the\nrelative positional direction with â–³^ğ‘‰ğ‘– ğ‘—.\n3.3 Structural Text Understanding\nCross-granularity Labeling Module The cross-granularity label-\ning module supports both token-level entity labeling and segment-\nlevel entity labeling tasks. In this module, tokens with the same\nsegment ID on the language side are aggregated into a segment-\nlevel textual feature through the arithmetic average\n^ğ‘‡ğ‘– = mean(^ğ‘¡ğ‘– )= (^ğ‘1 +^ğ‘2 +Â·Â·Â· ^ğ‘ğ‘™ğ‘– )/ğ‘™ğ‘– (7)\nwhere ^ğ‘¡ğ‘– means the features of ğ‘–-th text sentence, ^ğ‘ is the feature\nof token, ğ‘™ğ‘– is the sentence length. After that, a bilinear pooling\nlayer is utilized to compute a Hadamard product to fuse the textual\nsegment feature ğ‘‡ğ‘– and the visual segment feature ğ‘‰ğ‘– .\nğ‘‹ğ‘– = ğ‘‰ğ‘– âˆ—ğ‘‡ğ‘– (8)\nFinally, we apply a fully connected layer on the cross-modal\nfeatures ğ‘‹ğ‘– to predict an entity label for segment ğ‘–with the Cross-\nEntropy loss.\nSegment Relationship Extraction Module The segment rela-\ntionship extraction module is proposed for entity linking. Doc-\numents usually represent their structure as a set of hierarchical\nrelations, such as key-value pair or table parsing. Inspired by Doc-\nstruct [41], we use an asymmetric parameter matrix ğ‘€ to extract\nthe relationship from segments ğ‘– to ğ‘— in probability form\nğ‘ƒğ‘–â†’ğ‘— = ğœ(ğ‘‹jğ‘€ğ‘‹ğ‘‡\nğ‘– ) (9)\nwhere ğ‘ƒğ‘–â†’ğ‘— is the probability of whether ğ‘–links to ğ‘—. ğ‘€ is a param-\neter matrix and ğœ is the sigmoid function.\nWe notice that most of the segment pairs in a document are not\nrelated. To alleviate the data sparsity and balance the number of\nrelated and unrelated pairs, we learn from the Negative Sampling\nmethod [25] and build a sampling set with non-fixed size. Our\nsampling set consists of the same number of positive and negative\nsamples.\nHowever, we also find the training process is unstable only using\nthe above sampling strategy. To utmost handle the imbalanced\ndistribution of entity linking, we combine the Margin Ranking Loss\nand Binary Cross-Entropy to supervise the training simultaneously.\nThus, the linking loss can be formulated as\nLoss = LossBCE +LossRank (10)\nwhere the LossRank is computed as following\nLossRank (ğ‘ƒğ‘–,ğ‘ƒğ‘—,ğ‘¦)= ğ‘šğ‘ğ‘¥(0,âˆ’ğ‘¦âˆ—(ğ‘ƒğ‘– âˆ’ğ‘ƒğ‘— )+Margin), (11)\nNote that ğ‘¦equals 1 if (ğ‘ƒğ‘–,ğ‘ƒğ‘— )is the positive-negative samples\npair or equals 0 for the negative-positive samples pair.\n4 EXPERIMENTS\n4.1 Datasets\nIn this section, we firstly introduce several datasets that are used\nfor pre-training and evaluating our StrucTexT. The extensive ex-\nperiments are conduct on three benchmark databases: FUNSD [15],\nSROIE [12], EPHOIE [38]. Moreover, we perform ablation studies\nto analyze the effects of each proposed component.\nDOCBANK [18] contains 500K document pages (400K for training,\n50K for validation and 50K for testing) for document layout analysis.\nWe pre-train StrucTexT on the dataset.\nRVL-CDIP [9] consists of 400,000 grayscale images in 16 classes,\nwith 25,000 images per class. There are 320,000 training images,\n40,000 validation images and 40,000 test images. We adopt RVL-\nCDIP for pre-training our model.\nFUNSD [15] consists of 199 real, fully annotated, scanned form\nimages. The dataset is split into 149 training samples and 50 testing\nsamples. Three sub-tasks (word grouping, semantic entity labeling,\nand entity linking) are proposed to identify the semantic entity (i.e.,\nquestions, answers, headers, and other) and entity linking present\nin the form. We use the official OCR annotation and focus on the\nlatter two tasks in this paper.\nSROIE [12] is composed of 626 receipts for training and 347 re-\nceipts for testing. Every receipt contains four predefined values:\ncompany, date, address, and total. The segment-level text bounding\nbox and the corresponding transcript are provided according to the\nannotations. We use the official OCR annotations and evaluate our\nmodel for receipt information extraction.\nEPHOIE [38] is collected from actual Chinese examination papers\nwith the diversity of text types and layout distribution. The 1,494\nsamples are divided into a training set with 1,183 images and a\ntesting set with 311 images, respectively. Every character in the\ndocument is annotated with a label from ten predefined categories.\nThe token-level entity labeling task is evaluated in this dataset.\n4.2 Implementation\nFollowing the typical pre-training and fine-tuning strategies, we\ntrain the model end-to-end. Across all pre-training and downstream\ntasks, we rescale the images and pad them to the size of 512 Ã—512.\nThe input sequence is set as a maximum length of 512.\n4.2.1 Pre-training. We extract both token-level text features and\nsegment-level visual features based on a unified joint model by the\nencoder. Due to time and computational resource restrictions, we\nMethod Prec. Recall F1 Params.\nLayoutLM_BASE [45] 94.38 94.38 94.38 113M\nLayoutLM_LARGE [45] 95.24 95.24 95.24 343M\nPICK [48] 96.79 95.46 96.12 -\nVIES [38] - - 96.12 -\nTRIE [50] - - 96.18 -\nLayoutLMv2_BASE [46] 96.25 96.25 96.25 200M\nMatchVIE [37] - - 96.57 -\nLayoutLMV2_LARGE [46] 96.61 96.61 96.61 426M\nOurs 95.84 98.52 96.88 107M\n(Â±0.15)\nTable 1: Model Performance (entity labeling) comparison on\nthe SROIE dataset.\nMethod Prec. Recall F1 Params.\nCarbonell et al. [1] - - 64.0 -\nSPADE [14] - - 70.5 -\nLayoutLM_BASE [45] 75.97 81.55 78.66 113M\nLayoutLM_LARGE [45] 75.96 82.19 78.95 343M\nMatchVIE [37] - - 81.33 -\nLayoutLMv2_BASE [46] 80.29 85.39 82.76 200M\nLayoutLMv2_LARGE [46] 83.24 85.19 84.20 426M\nOurs 85.68 80.97 83.09 107M\n(Â±0.09)\nTable 2: Model Performance (entity labeling) comparison on\nthe FUNSD dataset, We ignore entities belonging to the other\ncategory and use the mean performance of three classes\n(header, question, and answer) as our final results.\nMethod Reconstruction Detection F1mAP mRank Hit@1 Hit@2 Hit@5\nFUNSD [15] - - - - - 4.0\nCarbonell et al. [1] - - - - - 39.0\nLayoutLMâˆ—[45] 47.61 7.11 32.43 45.56 66.41 -\nDocStruct [41] 71.77 2.89 58.19 76.27 88.94 -\nSPADE [14] - - - - - 41.7\nOurs 78.36 3.38 67.67 84.33 95.33 44.1\nTable 3: Model Performance (entity linking) comparison on\nthe FUNSD dataset. (LayoutLM âˆ—is implemented by [41])\nchoose the 12-layer transformer encoder with 768 hidden size and\n12 attention heads. We initialize the transformer and the text embed-\nding layer from the ERNIEBASE [36]. The weights of the ResNet50\nnetwork is initialized using the ResNet_vd [11] pre-trained on the\nImageNet [4]. The rest of the parameters are randomly initialized.\nTo obtain the pre-training OCR results, we apply the PaddleOCR1\nto extract the text segment in both DOCBANK and RVL-CDIP\ndatasets. All three self-supervised tasks are trained for classifica-\ntion with the Cross-Entropy loss. The Adamax optimizer is used\nwith an initial 5 Ã—10âˆ’5 learning rate for a warm-up. And then, we\n1https://github.com/PaddlePaddle/PaddleOCR\nMethod Entities\nSubject Test\nTime Name School Exam\nNumber\nSeat\nNumber Class Student\nNumber Grade Score Mean\nTRIE [50] 98.79 100 99.46 99.64 88.64 85.92 97.94 84.32 97.02 80.39 93.21\nVIES [38] 99.39 100 99.67 99.28 91.81 88.73 99.29 89.47 98.35 86.27 95.23\nMatchVIE [37] 99.78 100 99.88 98.57 94.21 93.48 99.54 92.44 98.35 92.45 96.87\nOurs 99.25 100 99.47 99.83 97.98 95.43 98.29 97.33 99.25 93.73 97.95\nTable 4: Model Performance (token-level entity labeling) comparison on the EPHOIE dataset.\nkeep 1Ã—10âˆ’4 for 2âˆ¼5 epochs and set a linear decay schedule in the\nrest of epochs. We pre-train our architecture in DOCBANK [ 18]\nand RVL-CDIP [9] dataset for 10 epochs with a batch size of 64 on\n4 NVIDIA Tesla V100 32GB GPUs.\n4.2.2 Fine-tuning. We fine-tune our StrucText on three informa-\ntion extraction tasks: entity labeling and entity linking at segment-\nlevel and entity labeling at token-level. For the segment-based entity\nlabeling task, we aggregate token features of the text sentence via\nthe arithmetic average and get the segment-level features by multi-\nplying visual features and textual features. At last, a softmax layer\nis followed by the features for segment-level category prediction.\nThe entity-level F1-score is used as the evaluation metric.\nThe entity linking task takes two segment features as input\nto obtain a pairwise relationship matrix. Then we pass the non-\ndiagonal elements in the relationship matrix through a sigmoid\nlayer to predict the binary classification of each relationship.\nFor the token-based entity labeling task, the output visual feature\nis expanded as a vector with the same length of its text sentence.\nNext, the extended visual features are element-wise multiplied with\nthe corresponding textual features to obtain token-level features to\npredict the category of each token through a softmax layer.\nWe fine-tune our pre-trained model at all downstream tasks\nfor 50 epochs with a batch size of 4 and a learning rate from 1 Ã—\n10âˆ’4 to 1 Ã—10âˆ’5. We use the precision, recall, and F1-score as\nevaluation metrics for entity labeling. Following DocStruct [ 41]\nand SPADE [14], the performance of entity linking is estimated\nwith Hit@1, Hit@2, Hit@5, mAp, mRank, and F1-score.\n4.3 Comparison with the State-of-the-Arts\nWe evaluate our proposed StrucTexT in three publish benchmarks\nfor both the entity labeling and entity linking tasks.\nSegment-level Entity Labeling The comparison results are shown\nin Table 1. We can observe that StrucText exhibits a superior per-\nformance over baseline methods [37, 38, 45, 46, 48, 50] on SROIE.\nSpecifically, our method obtains a precision of 95.84% and a Recall\nof 98.52% in SROIE, which surpass that of LayoutLMv2_LARGE [46]\nby 0.27% in F1-score.\nAs shown in Table 2, our method achieves competitive F1-score\nof 83.09% in FUNSD. Although LayoutLMv2_LARGE beats our F1-\nscore by Ëœ1%, it is worth noting that LayoutLMv2_LARGE using a\nlarger transformer consisting of 24 layers and 16 heads that contains\n426M parameters. Further, our model using only 90K documents\nfor pre-training compared to LayoutLMv2_LARGE which uses 11M\ndocuments. On the contrary, our model shows a better performance\nthan LayoutLMv2_BASE under the same architecture settings. It\nfully proves the superiority of our proposed framework. Moreover,\nto verify the performance gain is statistically stable and signifi-\ncant, we repeat our experiments five times to eliminate the random\nfluctuations and attach the standard deviation below the F1-score.\nSegment-level Entity Linking As shown in Table 3, we compare\nour method with several state-of-the-arts on FUNSD for entity\nlinking. The baseline method [ 15] gets the remarkably worst re-\nsult with a simple binary classification for pairwise entities. The\nSPADE [14] shows a tremendous gain by leading a Graph into\ntheir model. Compared with the SPADE, our method has a 2.4%\nimprovement and achieves 44.1% F1-score. Besides, we evaluate\nthe performance in the mAp, mRank, and Hit metrics mentioned\nin DocStruct [41]. Our method attains 78.36% mAP, 79.19% Hit@1,\n84.33% Hit@2, and 95.33% Hit@5 which outperforms DocStruct\nand obtains a competitive performance at the 3.38 mRank score.\nToken-level Entity Labeling We further perform StrucText on\nEPHOIE. It is noticed that the entities annotated in this dataset are\ncharacter-based. Therefore, we apply our StrucText to calculate the\nentities with the token-level prediction. Table 4 illustrates the over-\nall performance of the EPHOIE dataset. Our StrucText contributes\nto a top-tier performance with 97.95%.\n4.4 Ablation Study\nWe study the impact of individual components in StrucText and\nconduct ablation studies on the FUNSD and SROIE datasets.\nDataset Pre-training\nTasks Prec. Recall F1\nFUNSD MVLM 76.41 79.36 77.71\nMVLM+PBD 81.22 79.46 80.29\nMVLM+SLP 87.45 78.69 82.12\nMVLM+PBD+SLP 85.68 80.97 83.09\nSROIE MVLM 95.25 97.89 96.54\nMVLM+PBD 95.32 98.25 96.75\nMVLM+SLP 95.30 98.16 96.70\nMVLM+PBD+SLP 95.84 98.52 96.88\nTable 5: Ablation Studies with entity labeling on the FUNSD\nand SROIE datasets.\nSelf-supervised Tasks in Pre-training In this study, we evaluate\nthe impact of different pre-training tasks. As shown in Table 5, we\ncan observe that the PBD and SLP tasks make better use of visual\ninformation. Specifically, compared with the model only trained\nDataset Modality Prec. Recall F1\nFUNSD Visual 76.93 77.51 77.22\nLanguage 81.73 79.38 80.49\nVisual + Language 85.68 80.97 83.09\nSROIE Visual 90.14 92.11 91.11\nLanguage 94.54 97.91 96.18\nViusal + Language 95.84 98.52 96.88\nTable 6: Ablation studies with visual-only and language-\nonly entity labeling on the FUNSD and SROIE datasets.\ntotal(other) other(total)\nscore(seatnumber)class(score)\nclass(seatnumber)seatnumber(class)\ntotal(other) total(other)\nquestion(header)answer(question)\n(a) Cases from SROIE labeling\ntotal(other) other(total)\nscore(seatnumber)class(score)\nclass(seatnumber)seatnumber(class)\ntotal(other) total(other)\nquestion(header)answer(question)\n(b) Cases from EPHOIE labeling\ntotal(other) other(total)\nscore(seatnumber)class(score)\nclass(seatnumber)seatnumber(class)\ntotal(other) total(other)\nquestion(header)answer(question) (c) Cases from FUNSD labeling\ntotal(other) other(total)\nscore(seatnumber)class(score)\nclass(seatnumber)seatnumber(class)\ntotal(other) total(other)\nquestion(header)answer(question)\n(d) Cases from FUNSD linking\nFigure 4: Visualization of badcases of StrucText. For the en-\ntity labeling task in (a), (b), and (c), the wrong cases are\nshown as the red boxes (the correct results are hidden) and\ntheir nearby text represents the predications and ground-\ntruths in red and green color, respectively. For the entity\nlinking task in (d), the green/purple lines indicate the cor-\nrect/error predicted linkings.\nwith the MVLM task, MVLM+PBD gains nearly 3% improvement\nin FUNSD and 0.2% improvement in SROIE. Meanwhile, the results\nturn out that the SLP task also improves the performance dramati-\ncally. Furthermore, the incorporation of the three tasks obtains the\noptimal performance compared with other combinations. It means\nthat both the SLP and PBD tasks contribute to richer semantics and\npotential relationships between cross-modality.\nDataset Granularity Prec. Recall F1\nFUNSD Token 81.20 82.10 81.59\nSegment 85.68 80.97 83.09\nSROIE Token 92.77 98.81 95.62\nSegment 95.84 98.52 96.88\nTable 7: Ablation studies with the comparison of token-level\nand segment-level entity labeling on the FUNSD and SROIE\ndatasets.\nMulti-Modal Features Profits As shown in Table 6, then we per-\nform experiments in verifying the benefits of features in multiple\nmodalities. The textual features perform better than visual ones,\nwhich we attribute more semantics to the textual content of docu-\nments. Moreover, combining visual and textual features can achieve\nhigher performance with a notable gap, indicating complementarity\nbetween language and visual information. The results show that the\nmulti-modal feature fusion in our model can get a richer semantic\nrepresentation.\nGranularity of Feature Fusion We also study the representations\nwith different granularities towards the performance. In detail, we\ncomplete the experiments of entity labeling on SROIE and FUNSD in\ntoken-level supervision. As shown in Table 7, overall, the segment-\nbased results perform better than token-based ones, which proves\nour opinion that the effectiveness of text segment.\n4.5 Error Analysis\nAlthough our work has achieved outstanding performance, we\nalso observe some badcases of the proposed method. This sec-\ntion presents an error analysis on the qualitative results in SROIE,\nFUNSD and EPHOIE, respectively. In Figure 4a, our model makes\nthe mistakes of wrong answers to the total in SROIE. We attribute\nthe errors to the similar semantics of textual contents and the close\ndistance of locations. In addition, as shown in Figure 4b, our model\nis confused by the similar style of digits, which demonstrates the\nrelatively low performance in the numeral entities in EPHOIE, such\nas exam number, seat number, student number, and score in 4. How-\never, these entities can certainly be distinguished by their keywords.\nTo this end, a goal-directed information aggregation of key-value\npair is well worth considering, and we would study it in future\nworks. As shown in Figure 4c, the model is failed in recognizing\nthe header and the question in FUNSD. We analyze the model is\noverfitting the layout position of training data. Then, according\nto Figure 4d, some links are assigned incorrectly. We attribute the\nerrors to ambiguous semantics of relationships.\n5 CONCLUSION\nIn this paper, we further explore improving the understanding of\ndocument text structure by using a unified framework. Our frame-\nwork shows superior performance on three real-world benchmark\ndatasets after applying novel pre-training strategies for the multi-\nmodal and multi-granularity feature fusion. Moreover, we evaluate\nthe influence of different modalities and granularities on the ability\nof entity extraction, thus providing a new perspective to study the\nproblem of structured text understanding.\nREFERENCES\n[1] Manuel Carbonell, Pau Riba, Mauricio Villegas, Alicia FornÃ©s, and Josep LladÃ³s.\n2020. Named Entity Recognition and Relation Extraction with Graph Neural\nNetworks in Semi Structured Documents. In ICPR. IEEE, 9622â€“9627.\n[2] Mengli Cheng, Minghui Qiu, Xing Shi, Jun Huang, and Wei Lin. 2020. One-\nshot Text Field labeling using Attention and Belief Propagation for Structure\nInformation Extraction. In ACM Multimedia. ACM, 340â€“348.\n[3] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan\nSalakhutdinov. 2019. Transformer-XL: Attentive Language Models beyond a\nFixed-Length Context. In ACL. ACL, 2978â€“2988.\n[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. Ima-\ngeNet: A large-scale hierarchical image database. In CVPR. IEEE, 248â€“255.\n[5] Andreas Dengel and Bertin Klein. 2002. smartFIX: A Requirements-Driven System\nfor Document Analysis and Understanding. In DAS. Springer, 433â€“444.\n[6] Timo I Denk and Christian Reisswig. 2019. Bertgrid: Contextualized em-\nbedding for 2d document representation and understanding. arXiv preprint\narXiv:1909.04948 (2019).\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[8] He Guo, Xiameng Qin, Jiaming Liu, Junyu Han, Jingtuo Liu, and Errui Ding. 2019.\nEATEN: Entity-Aware Attention for Single Shot Visual Text Extraction. InICDAR.\nIEEE, 254â€“259.\n[9] Adam W. Harley, Alex Ufkes, and Konstantinos G. Derpanis. 2015. Evaluation\nof deep convolutional nets for document image classification and retrieval. In\nICDAR. IEEE, 991â€“995.\n[10] Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross B. Girshick. 2017. Mask\nR-CNN. In ICCV. 2961â€“2969.\n[11] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li.\n2019. Bag of Tricks for Image Classification with Convolutional Neural Networks.\nIn CVPR. IEEE, 558â€“567.\n[12] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian\nLu, and CV Jawahar. 2019. Icdar2019 competition on scanned receipt ocr and\ninformation extraction. In ICDAR. IEEE, 1516â€“1520.\n[13] Wonseok Hwang, Seonghyeon Kim, Minjoon Seo, Jinyeong Yim, Seunghyun\nPark, Sungrae Park, Junyeop Lee, Bado Lee, and Hwalsuk Lee. 2019. Post-OCR\nparsing: building simple and robust parser via BIO tagging. InNeurIPS Workshop.\n[14] Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang, and Minjoon Seo.\n2021. Spatial Dependency Parsing for Semi-Structured Document Information\nExtraction. In ACL-IJCNLP.\n[15] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. 2019. FUNSD:\nA Dataset for Form Understanding in Noisy Scanned Documents. In ICDAR\nWorkshop. IEEE, 1â€“6.\n[16] Anoop R. Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen\nBickel, Johannes HÃ¶hne, and Jean Baptiste Faddoul. 2018. Chargrid: Towards\nUnderstanding 2D Documents. In EMNLP. ACL, 4459â€“4469.\n[17] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami,\nand Chris Dyer. 2016. Neural Architectures for Named Entity Recognition. In\nACL. ACL, 260â€“270.\n[18] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming\nZhou. 2020. DocBank: A Benchmark Dataset for Document Layout Analysis. In\nCOLING. ICCL, 949â€“960.\n[19] Minghui Liao, Guan Pang, Jing Huang, Tal Hassner, and Xiang Bai. 2020. Mask\ntextspotter v3: Segmentation proposal network for robust scene text spotting. In\nECCV. Springer, 706â€“722.\n[20] Tsung-Yi Lin, Piotr DollÃ¡r, Ross B. Girshick, Kaiming He, Bharath Hariharan,\nand Serge J. Belongie. 2017. Feature Pyramid Networks for Object Detection. In\nCVPR. IEEE, 936â€“944.\n[21] Xiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha Zhao. 2019. Graph Convolu-\ntion for Multimodal Information Extraction from Visually Rich Documents. In\nNAACL-HLT. ACL, 32â€“39.\n[22] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. ViLBERT: Pretraining\nTask-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.\nIn NeurIPS. 13â€“23.\n[23] Xuezhe Ma and Eduard Hovy. 2016. End-to-end Sequence Labeling via Bi-\ndirectional LSTM-CNNs-CRF. In ACL. ACL, 1064â€“1074.\n[24] Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley\nWendt, Qi Zhao, and Marc Najork. 2020. Representation Learning for Information\nExtraction from Form-like Documents. In ACL. ACL, 6495â€“6504.\n[25] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.\nDistributed representations of words and phrases and their compositionality. In\nNIPS. 3111â€“3119.\n[26] Rasmus Berg Palm, Florian Laws, and Ole Winther. 2019. Attend, Copy, Parse\nEnd-to-end Information Extraction from Documents. In ICDAR. IEEE, 329â€“336.\n[27] Rasmus Berg Palm, Ole Winther, and Florian Laws. 2017. CloudScan - A\nConfiguration-Free Invoice Analysis System Using Recurrent Neural Networks.\nIn ICDAR. IEEE, 406â€“413.\n[28] Subhojeet Pramanik, Shashank Mujumdar, and Hima Patel. 2020. Towards a\nMulti-modal, Multi-task Learning based Pre-training Framework for Document\nRepresentation Learning. CoRR abs/2009.14457 (2020). arXiv:2009.14457\n[29] Yujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, and Regina Barzilay. 2019.\nGraphIE: A Graph-Based Framework for Information Extraction. In ACL. ACL,\n751â€“761.\n[30] ClÃ©ment Sage, Alex Aussem, VÃ©ronique Eglin, Haytham Elghazel, and JÃ©rÃ©my\nEspinas. 2020. End-to-End Extraction of Structured Information from Business\nDocuments with Pointer-Generator Networks. In SPNLP. ACL, 43â€“52.\n[31] ClÃ©ment Sage, Alexandre Aussem, Haytham Elghazel, VÃ©ronique Eglin, and\nJÃ©rÃ©my Espinas. 2019. Recurrent Neural Network Approach for Table Field\nExtraction in Business Documents. In ICDAR. IEEE, 1308â€“1313.\n[32] Ritesh Sarkhel and Arnab Nandi. 2019. Visual Segmentation for Information\nExtraction from Heterogeneous Visually Rich Documents. In SIGMOD. ACM,\n247â€“262.\n[33] Baoguang Shi, Xiang Bai, and Cong Yao. 2017. An End-to-End Trainable Neural\nNetwork for Image-Based Sequence Recognition and Its Application to Scene\nText Recognition. TPAMI 39, 11 (2017), 2298â€“2304.\n[34] Bolan Su and Shijian Lu. 2014. Accurate Scene Text Recognition Based on\nRecurrent Neural Network. In ACCV. Springer, 35â€“48.\n[35] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2020.\nVL-BERT: Pre-training of Generic Visual-Linguistic Representations. In ICLR.\n[36] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and\nHaifeng Wang. 2020. ERNIE 2.0: A Continual Pre-Training Framework for Lan-\nguage Understanding. In AAAI. AAAI, 8968â€“8975.\n[37] Guozhi Tang, Lele Xie, Lianwen Jin, Jiapeng Wang, Jingdong Chen, Zhen Xu,\nQianying Wang, Yaqiang Wu, and Hui Li. 2021. MatchVIE: Exploiting Match\nRelevancy between Entities for Visual Information Extraction. InIJCAI. ijcai.org.\n[38] Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Jiaxin Zhang, Shuaitao\nZhang, Qianying Wang, Yaqiang Wu, and Mingxiang Cai. 2021. Towards Robust\nVisual Information Extraction in Real World: New Dataset and Novel Solution.\nIn AAAI. AAAI, 2738â€“2745.\n[39] Pengfei Wang, Chengquan Zhang, Fei Qi, Zuming Huang, Mengyi En, Junyu Han,\nJingtuo Liu, Errui Ding, and Guangming Shi. 2019. A Single-Shot Arbitrarily-\nShaped Text Detector based on Context Attended Multi-Task Learning. InACM\nMultimedia. ACM, 1277â€“1285.\n[40] Pengfei Wang, Chengquan Zhang, Fei Qi, Shanshan Liu, Xiaoqiang Zhang,\nPengyuan Lyu, Junyu Han, Jingtuo Liu, Errui Ding, and Guangming Shi. 2021.\nPGNet: Real-time Arbitrarily-Shaped Text Spotting with Point Gathering Net-\nwork. In AAAI. AAAI, 2782â€“2790.\n[41] Zilong Wang, Mingjie Zhan, Xuebo Liu, and Ding Liang. 2020. DocStruct: A\nMultimodal Method to Extract Hierarchy Structure in Document for General\nForm Understanding. In EMNLP. ACL, 898â€“908.\n[42] Mengxi Wei, Yifan He, and Qiong Zhang. 2020. Robust Layout-aware IE for\nVisually Rich Documents with Pre-trained Language Models. In SIGIR. ACM,\n2367â€“2376.\n[43] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Googleâ€™s neural machine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint arXiv:1609.08144 (2016).\n[44] Saining Xie, Ross B. Girshick, Piotr DollÃ¡r, Zhuowen Tu, and Kaiming He. 2017.\nAggregated Residual Transformations for Deep Neural Networks. In CVPR. IEEE,\n5987â€“5995.\n[45] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. 2020.\nLayoutLM: Pre-training of Text and Layout for Document Image Understanding.\nIn KDD. ACM, 1192â€“1200.\n[46] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu,\nDinei Florencio, Cha Zhang, Wanxiang Che, et al . 2020. LayoutLMv2: Multi-\nmodal pre-training for visually-rich document understanding. arXiv preprint\narXiv:2012.14740 (2020).\n[47] Deli Yu, Xuan Li, Chengquan Zhang, Tao Liu, Junyu Han, Jingtuo Liu, and Errui\nDing. 2020. Towards accurate scene text recognition with semantic reasoning\nnetworks. In CVPR. 12113â€“12122.\n[48] Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, and Rong Xiao. 2020. PICK:\nProcessing Key Information Extraction from Documents using Improved Graph\nLearning-Convolutional Networks. In ICPR. IEEE, 4363â€“4370.\n[49] Chengquan Zhang, Borong Liang, Zuming Huang, Mengyi En, Junyu Han, Errui\nDing, and Xinghao Ding. 2019. Look More Than Once: An Accurate Detector for\nText of Arbitrary Shapes. In CVPR. IEEE, 10552â€“10561.\n[50] Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing Lu, Liang Qiao, Yi Niu,\nand Fei Wu. 2020. TRIE: End-to-End Text Reading and Information Extraction\nfor Document Understanding. In ACM Multimedia. ACM, 1413â€“1422.\n[51] Xiaohui Zhao, Endi Niu, Zhuo Wu, and Xiaoguang Wang. 2019. Cutie: Learning\nto understand documents with convolutional universal text information extractor.\narXiv preprint arXiv:1903.12363 (2019).\n[52] Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, and\nJiajun Liang. 2017. EAST: An Efficient and Accurate Scene Text Detector. In\nCVPR. IEEE, 2642â€“2651.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8388422727584839
    },
    {
      "name": "Security token",
      "score": 0.7954850196838379
    },
    {
      "name": "Encoder",
      "score": 0.638126015663147
    },
    {
      "name": "Transformer",
      "score": 0.6156359314918518
    },
    {
      "name": "Sentence",
      "score": 0.5814810991287231
    },
    {
      "name": "Natural language processing",
      "score": 0.5287646055221558
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5151954293251038
    },
    {
      "name": "Modal",
      "score": 0.5096607208251953
    },
    {
      "name": "Task (project management)",
      "score": 0.4873005151748657
    },
    {
      "name": "Representation (politics)",
      "score": 0.4255872964859009
    },
    {
      "name": "Information retrieval",
      "score": 0.40657365322113037
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}