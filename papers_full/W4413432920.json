{
  "title": "Large language models for depression recognition in spoken language integrating psychological knowledge",
  "url": "https://openalex.org/W4413432920",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2118264050",
      "name": "Yupei Li",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A5113332868",
      "name": "Shuaijie Shao",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2987062809",
      "name": "Manuel Milling",
      "affiliations": [
        "Munich Center for Machine Learning",
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2954012826",
      "name": "Björn W. Schuller",
      "affiliations": [
        null,
        "Imperial College London",
        "Technical University of Munich",
        "Munich Center for Machine Learning"
      ]
    },
    {
      "id": "https://openalex.org/A2118264050",
      "name": "Yupei Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5113332868",
      "name": "Shuaijie Shao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2987062809",
      "name": "Manuel Milling",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2954012826",
      "name": "Björn W. Schuller",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4401746065",
    "https://openalex.org/W4367353919",
    "https://openalex.org/W3037547708",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W4386858145",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4393949820",
    "https://openalex.org/W2946556809",
    "https://openalex.org/W6691669583",
    "https://openalex.org/W6839036921",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W4399282228",
    "https://openalex.org/W4309271221",
    "https://openalex.org/W4392972000",
    "https://openalex.org/W4415062597",
    "https://openalex.org/W4405903187",
    "https://openalex.org/W4401863339",
    "https://openalex.org/W4319762273",
    "https://openalex.org/W4404066756",
    "https://openalex.org/W6858203974",
    "https://openalex.org/W4388423051",
    "https://openalex.org/W4285307867",
    "https://openalex.org/W4250461442",
    "https://openalex.org/W4402273552",
    "https://openalex.org/W4387859613",
    "https://openalex.org/W6847363464",
    "https://openalex.org/W2981677410",
    "https://openalex.org/W2751214333",
    "https://openalex.org/W4377115728",
    "https://openalex.org/W4405707338",
    "https://openalex.org/W3205969292",
    "https://openalex.org/W4403662208",
    "https://openalex.org/W4401490995",
    "https://openalex.org/W4285267995",
    "https://openalex.org/W4366821924",
    "https://openalex.org/W4387891768",
    "https://openalex.org/W4400488557",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2346454595",
    "https://openalex.org/W1976066595",
    "https://openalex.org/W4392947832",
    "https://openalex.org/W4399154371",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4280549687",
    "https://openalex.org/W2252180568",
    "https://openalex.org/W2551706664"
  ],
  "abstract": "Depression is a growing concern gaining attention in both public discourse and AI research. While deep neural networks (DNNs) have been used for its recognition, they still lack real-world effectiveness. Large language models (LLMs) show strong potential but require domain-specific fine-tuning and struggle with non-textual cues. Since depression is often expressed through vocal tone and behavior rather than explicit text, relying on language alone is insufficient. Diagnostic accuracy also suffers without incorporating psychological expertise. To address these limitations, we present, to the best of our knowledge, the first application of LLMs to multimodal depression detection using the DAIC-WOZ dataset. We extract the audio features using the pre-trained model Wav2Vec, and map them to text-based LLMs for further processing. We also propose a novel strategy for incorporating psychological knowledge into LLMs to enhance diagnostic performance, specifically using a question and answer set to grant authorized knowledge to LLMs. Our approach yields a notable improvement in both Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) compared to a base score proposed by the related original paper. The codes are available in Github .",
  "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/two.tnum August /two.tnum/zero.tnum/two.tnum/five.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/two.tnum/nine.tnum/seven.tnum/two.tnum/five.tnum\nOPEN ACCESS\nEDITED BY\nCenk Demiroglu,\nÖzye˘gin University, Türkiye\nREVIEWED BY\nBochao Zou,\nUniversity of Science and Technology Beijing,\nChina\nBiman Najika Liyanage,\nBiman Liyanage, China\n*CORRESPONDENCE\nYupei Li\nyl/seven.tnum/six.tnum/two.tnum/two.tnum@ic.ac.uk\n†These authors have contributed equally to\nthis work\nRECEIVED /one.tnum/six.tnum May /two.tnum/zero.tnum/two.tnum/five.tnum\nACCEPTED /zero.tnum/seven.tnum August /two.tnum/zero.tnum/two.tnum/five.tnum\nPUBLISHED /two.tnum/two.tnum August /two.tnum/zero.tnum/two.tnum/five.tnum\nCITATION\nLi Y, Shao S, Milling M and Schuller BW (/two.tnum/zero.tnum/two.tnum/five.tnum)\nLarge language models for depression\nrecognition in spoken language integrating\npsychological knowledge.\nFront. Comput. Sci./seven.tnum:/one.tnum/six.tnum/two.tnum/nine.tnum/seven.tnum/two.tnum/five.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/two.tnum/nine.tnum/seven.tnum/two.tnum/five.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/five.tnum Li, Shao, Milling and Schuller. This is\nan open-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nLarge language models for\ndepression recognition in spoken\nlanguage integrating\npsychological knowledge\nYupei Li/one.tnum*†, Shuaijie Shao /two.tnum†, Manuel Milling /three.tnum,/four.tnumand\nBjörn W. Schuller /one.tnum,/three.tnum,/four.tnum,/five.tnum\n/one.tnumGroup on Language, Audio, and Music, Imperial College London, London, Uni ted Kingdom,\n/two.tnumUniversity College London, London, United Kingdom, /three.tnumChair of Health Informatics, TUM University\nHospital, Munich, Germany, /four.tnumMunich Center for Machine Learning, Munich, Germany, /five.tnumMunich Data\nScience Institute, Munich, Germany\nDepression is a growing concern gaining attention in both publi c discourse\nand AI research. While deep neural networks (DNNs) have been us ed for its\nrecognition, they still lack real-world eﬀectiveness. Large lan guage models\n(LLMs) show strong potential but require domain-speciﬁc ﬁne- tuning and\nstruggle with non-textual cues. Since depression is often expre ssed through\nvocal tone and behavior rather than explicit text, relying on l anguage\nalone is insuﬃcient. Diagnostic accuracy also suﬀers without inco rporating\npsychological expertise. To address these limitations, we pre sent, to the best of\nour knowledge, the ﬁrst application of LLMs to multimodal depr ession detection\nusing the DAIC-WOZ dataset. We extract the audio features usin g the pre-trained\nmodel Wav/two.tnumVec, and map them to text-based LLMs for further processing.\nWe also propose a novel strategy for incorporating psychologi cal knowledge\ninto LLMs to enhance diagnostic performance, speciﬁcally using a question\nand answer set to grant authorized knowledge to LLMs. Our approach yields a\nnotable improvement in both Mean Absolute Error (MAE) and Roo t Mean Square\nError (RMSE) compared to a base score proposed by the related ori ginal paper.\nThe codes are available in Github.\nKEYWORDS\nlarge language models, depression recognition, psychologica l knowledge, spoken\nlanguage, speech\n/one.tnum Introduction\nAs mental health gains increasing attention from the public, the diagnosis of emotional\ndisorders—particularly depression—has become an essential area in both AI for healthcare\nand medical research. In previous decades, diagnosis has solely depended on the expertise\nof professional psychologists and clinicians. These circumstances introduce potential\nvariability, as diﬀerent clinicians may be inﬂuenced by their own intuition when assessing\nsubjective questions, evident by\nGerber et al. (1989) . To counteract this issue, diagnostic\nprocedures are established with a maximum amount of standardization, which stays\nlimited despite major eﬀorts. One opportunity to increase consistency and reproducibility\nof diagnosis is by leveraging automated assessments as a supporting tool, which is enabled\nby large-scale data and AI techniques (\nZafar et al., 2024 ). A combination of AI, vast\nexperiential data, and the professional knowledge of clinicians may together contribute to\nmore accurate depression recognition, enhancing clinicians’ conﬁdence and convenience,\nFrontiers in Computer Science /zero.tnum/one.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/two.tnum/nine.tnum/seven.tnum/two.tnum/five.tnum\nas well as increasing patient satisfaction, which is shown in the\nwork of\nQassim et al. (2023) . Although the use of AI alone to\nrecognize depression has been proposed as a way to save time and\nreduce costs, patients have expressed skepticism and distrust, which\nRobertson et al. (2023) have revealed. Challenges include not only\nsuboptimal detection rates but also the fact that current AI systems\noften lack integration with professional psychological knowledge,\nrelying instead solely on data-driven experience.\nPrevious literature has made tremendous eﬀorts to recognize\ndepression within the domain of AI in healthcare. Two major\nresearch directions include the utilization and adaptation of various\ndeep neural networks (DNNs), as well as the fusion of multiple data\nmodalities. A wide range of techniques for depression detection has\nbeen surveyed by\nSquires et al. (2023) . Notably, Valstar et al. (2013)\nintroduced the depression detection challenge in 2013, laying the\ngroundwork for subsequent research. Building on this,\nRingeval\net al. (2019) proposed the use of Long Short-Term Memory (LSTM)\nmodels for the task, establishing a baseline for future approaches.\nSince then, various deep neural network (DNN) models have\nbeen developed, achieving promising results. For instance,\nNiu\net al. (2022) introduced Dual Attention and Element Recalibration\nNetworks utilizing acoustic modalities for depression recognition.\nDespite their contributions, these models exhibit performance\nlimitations largely due to the constrained learning capacity of\ntheir architectures. Recently, LLMs have demonstrated exceptional\nlearning capabilities across many diﬀerent ﬁelds (e. g.,\nAmin\net al., 2023 ); however, their potential has not yet been eﬀectively\nexplored in the context of depression detection. Additionally,\nunimodal approaches are often insuﬃcient for accurately assessing\ndepression, as the evaluation process is inherently complex.\nEven clinicians frequently rely on indirect questioning during\ninterviews to avoid triggering sensitive responses. Some prior\nstudies have explored modality-speciﬁc models, such as LLM-\nbased models focused on text (\nFarruque et al., 2024 ), autoencoder-\nbased models centered on audio features ( Sardari et al., 2022 ), and\nmodels targeting image and video modalities ( Ashraf et al., 2020 ).\nThese approaches, however, still lack deﬁnitive and comprehensive\ndiagnostic information due to their unimodal nature. For instance,\nwhile audio may reveal weak pitch variations, the transcribed\ntextual content might appear entirely normal, obscuring underlying\nemotional cues. Consequently, multimodal fusion is essential for\na more holistic understanding of depressive symptoms. However,\nintegrating multiple modalities into LLM-based models remains an\nopen challenge, primarily because LLMs are mostly trained on text-\nbased tokens and are often not designed to natively process or fuse\nnon-textual inputs.\nBeyond technical challenges, it is also crucial for models to\naddress concerns regarding patient trust. Current LLMs are not\nspeciﬁcally trained on psychological or psychiatric knowledge\ncomparable to the clinical experience of trained professionals. As a\nresult, the responses generated by LLMs may lack authenticity and,\nmore critically, may exhibit hallucinations—a particularly serious\nissue in the context of depression recognition. Techniques such as\nknowledge injection, as discussed by\nMartino et al. (2023) , have\nbeen proposed to mitigate this limitation by integrating domain-\nspeciﬁc information into LLMs. However, such methods have not\nyet been widely implemented in LLM-based models for depression\nrecognition, leaving a considerable gap in ensuring both reliability\nand clinical validity.\nTherefore, our paper proposes a novel approach to address the\naforementioned research gaps:\n• To the best of our knowledge, this is the ﬁrst approach\nto directly apply LLMs for spoken language to the ﬁeld of\ndepression recognition.\n• We introduce a pipeline that injects professional psychological\nknowledge into LLMs and demonstrates its eﬀectiveness\nthrough empirical evaluation.\n• Our proposed pipeline considerably outperforms baseline\nmodels on the DAIC-WOZ dataset.\nThe remainder of this paper is organized as follows. Section 2.1\nreviews related work in depression recognition, particularly\nfocusing on DNNs and LLMs in a fusion of multiple streams.\nSection 2.2 describes the dataset used in our study, including\nits composition and its pre-processing. Section 2.3 details our\nproposed multimodal LLM-based pipeline and the methodology\nfor injecting psychological knowledge into the model. Section 3\npresents the experimental results, comparing our approach against\nestablished baselines. Finally, Section 4 concludes the paper and\noutlines potential directions for future research.\n/two.tnum Materials and methods\n/two.tnum./one.tnum Related work\n/two.tnum./one.tnum./one.tnum Text-based depression detection\nIn recent years, Natural Language Processing (NLP) methods\nhave increasingly utilized deep language models to identify\ndepression symptoms in text. This approach is grounded in clinical\npractices, where mental health professionals often assess linguistic\ncues—such as expressions of hopelessness, self-deprecation, or\nwithdrawal—to diagnose depression. In AVEC 2013,\nValstar et al.\n(2013) incorporated the text modality for the ﬁrst time, providing\nASR transcripts alongside audio and video for multimodal\ndepression detection and emotion recognition. Further,\nOgunleye\net al. (2024) applied multiple hybrid models to two social media\ndatasets, each involving binary classiﬁcation tasks distinguishing\nbetween “depressed” and “not depressed” posts. Their combination\nof Sentence-BERT and an ensemble model achieved F1 scores\nof 69% and 76% on the respective datasets, demonstrating that\nincorporating lexicon-based sentiment indicators can enhance\nthe performance of text-based models. This demonstrates that\nincorporating lexicon-based sentiment indicators can enhance the\nperformance of text-based models. Similarly,\nSivamanikandan et al.\n(2022), using a social media dataset from the Language Technology\nfor Equality, Diversity, and Inclusion (LT-EDI) 2022 task published\nby the Association for Computational Linguistics (ACL), trained\nseveral transformer models such as DistilBERT, ALBERT, and\nRoBERTa. Posts in the dataset were categorized as “not depressed, ”\n“moderately depressed, ” or “severely depressed”. Among the tested\nmodels, RoBERTa performed best, achieving an overall F1 score\nof 0.457 in the three-class problem, illustrating the eﬀectiveness\nFrontiers in Computer Science /zero.tnum/two.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/two.tnum/nine.tnum/seven.tnum/two.tnum/five.tnum\nof transformer architectures for text-based depression classiﬁcation\ntasks. These studies are good examples of the current trend in\nresearch, in which researchers are increasingly focused on ﬁne-\ntuning pretrained language models with labeled text. Previous\nstudies also show that combining deep learning approaches with\nlinguistic markers or psycholinguistic lexicons can substantially\nimprove performance (e. g.,\nLyu et al., 2023 ). Kathan et al. (2022b)\nproposed other format of text-based features such as behavioral\nactivation for depression scale-short form (BADSSF), the center for\nepidemiologic studies depression scale (CESD), or the personality\ndynamics diary (PDD). Overall, the trend in text-based depression\ndetection has shifted toward transformer-based models, often\nemphasizing speciﬁc lexical indicators or sentence patterns related\nto emotional states.\n/two.tnum./one.tnum./two.tnum Audio-based depression detection\nDepression symptoms can also manifest in vocal expression,\nprompting researchers to ﬁne-tune pretrained speech models to\ncapture acoustic features. For instance, individuals experiencing\ndepression often exhibit paralinguistic characteristics such as\nreduced pitch variability, slower speech rate, and longer pauses.\nThese features reﬂect the low arousal and negative emotional\nstates often associated with depression. In AVEC 2013\nValstar\net al. (2013) , the ﬁrst audio-based depression detection challenge,\nincorporated the audio modality as a key component for depression\ndetection. Moreover,\nHuang et al. (2024) applied wav2vec 2.0 to\nthe AVEC2017 dataset to extract audio features, achieving 96.5%\naccuracy in binary depression classiﬁcation. This highlights the\ncapability of such models to learn high-quality representations\nwithout requiring complex processing.\nMallol-Ragolta et al. (2024)\napplied multi-triplet loss-based models for categorical depression\nrecognition with four acoustic features. In addition, classical\nacoustic analysis remains eﬀective. For example,\nBerardi et al.\n(2023) extracted voice pathology features from recordings of\npicture descriptions and used them to train SVM classiﬁers.\nA third-degree polynomial SVM achieved over 92% accuracy\nacross all tasks. Their study identiﬁed articulatory precision, pause\nfrequency, and speech variability as the most inﬂuential features.\nThese represent two primary approaches in audio-based depression\ndetection: (1) ﬁne-tuning deep models like wav2vec and (2)\nextracting and classifying key acoustic features using traditional\nmethods like SVM. Both approaches have proven eﬀective for\nthis task.\n/two.tnum./one.tnum./three.tnum Multimodal depression detection\nMultimodal models have been introduced into depression\ndetection to combine textual, audio, and visual information,\nenabling richer feature representation than unimodal approaches.\nA prominent benchmark used in this ﬁeld is the AVEC 2017 “Real-\nlife Depression and Aﬀect Recognition” challenge, which provides\nvideo, audio, and transcript data from interviews.\nRingeval et al.\n(2017) introduced the according dataset with including PHQ-\n8 score regression. The baseline model for depression severity\nestimation contains text, audio, video, and combined audio and\nvideo models, serving as a reference for future work. Several\nstudies have built upon this benchmark to improve it.\nSadeghi\net al. (2024) extracted textual and facial-expression features using\na LLM and a vision model, combining them to predict PHQ-8\nscores. Their multimodal model slightly outperformed the text-\nonly version in terms of mean squared error (MSE). Meanwhile,\nMin et al. (2023) collected annotated YouTube vlogs and conducted\nstatistical analysis to highlight diﬀerences in depressive vs non-\ndepressive videos. Their model learnt from both audio and video\ncues and achieved an F1 score of 77% on the vlog dataset.\nAdditionally,\nHe et al. (2022) proposes a novel multimodal dataset\ncollected from phone sensors, including phone calls, phone usage,\nand user activity. Similarly,\nKathan et al. (2022a) utilizes mobile\nsensors to collect multimodal data. These studies demonstrate\nthat integrating multiple modalities—whether text with facial\nexpressions or audio with video—can enhance model performance\nin depression recognition tasks.\n/two.tnum./one.tnum./four.tnum LLM-based depression recognition\nThe emergence of LLMs has brought major advancements for\nmany application scenarios of AI including depression recognition,\noutperforming conventional non-large DNNs, as were used before\nthe rise of large models.\nSchuller et al. (2024) show that LLMs have\nemotional emergence. Additionally, Shin et al. (2024) demonstrated\nthe eﬀectiveness of LLMs by prompting GPT-3.5 and GPT-4\nwith 428 diaries from 91 users to assess depression risk. With\nsimple prompt engineering and minimal ﬁne-tuning, the model\nachieved an accuracy of 90.2% on binary depression classiﬁcation,\nwhere a PHQ-9 score greater than 10 was considered indicative\nof depression. Notably, the ﬁne-tuned GPT-3.5 outperformed its\nzero-shot untuned counterpart, underscoring the potential of LLMs\nwhen adapted properly. Expanding on this,\nLiu Z. et al. (2024)\nintroduced “EmoLLMs, ” a series of LLMs ﬁne-tuned for aﬀective\ntasks. Trained on a multi-task emotional dataset, EmoLLMs\nsurpassed GPT-4 on standard benchmarks, further showing how\nLLMs can be tailored to emotional understanding. To assess\nhow closely LLMs resemble human performance,\nZhang et al.\n(2024) compared models like ChatGPT, Claude, and Bing Chat\non sentiment intensity tasks. Results showed that GPT-4 achieved\nscores comparable to humans, suggesting that LLMs can replicate\nor even exceed human-level emotional judgement. Together, these\nworks indicate that LLMs represent a major leap forward in the\nﬁeld, with vast potential when appropriately ﬁne-tuned.\n/two.tnum./one.tnum./five.tnum Knowledge-based injection into LLMs\nBuilding on the strong baseline performance of LLMs, recent\nresearch has explored the integration of psychological knowledge to\nfurther improve mental health inference.\nLi et al. (2025) surveyed\nthe eﬀectiveness of continuous learning, where knowledge-based\ninjection served as one potential approach. Speciﬁcally,\nAbbasi\net al. (2024) introduced “PsychoLexLLaMA ”, an LLM designed\nfor psychological assessment. Trained on the PsychoLex Q&A\ndataset, this model learnt specialized psychological knowledge\nand outperformed other LLMs in reasoning tasks related to\npsychology, highlighting the beneﬁts of domain-speciﬁc knowledge\ninjection. In another example,\nLan et al. (2024) proposed DORIS, a\ndepression recognition system that incorporates clinical diagnostic\nknowledge. The model ﬁrst used an LLM to identify social\nFrontiers in Computer Science /zero.tnum/three.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/two.tnum/nine.tnum/seven.tnum/two.tnum/five.tnum\nmedia posts containing DSM-related expressions, then generates\nemotional summaries and estimates emotional intensities. These\noutputs are fed into a conventional classiﬁer, resulting in improved\nperformance for binary depression classiﬁcation (depressed vs\ncontrol) compared to standard models. Similarly,\nTank et al.\n(2024) demonstrated that encoding questionnaire knowledge\ninto prompts enhances LLM eﬀectiveness. Their system for\nPHQ-8 scoring uses structured prompts based on depression\nsymptoms and a two-shot classiﬁcation setup. They found that\nembedding relevant questionnaire knowledge increased prediction\naccuracy. Together, these studies reveal that enriching LLMs with\npsychological or diagnostic knowledge can further elevate their\ncapability in depression recognition tasks.\n/two.tnum./two.tnum Dataset and preprocessing\n/two.tnum./two.tnum./one.tnum Experimental dataset\nThe dataset used in this study is DAIC-WOZ (\nGratch et al.,\n2014). The interviews feature a patient interacting with a virtual\nhuman interviewer named Ellie (as indicated in the transcript ﬁles).\nThe dataset includes audio recordings of complete conversations\nand their corresponding transcripts, which specify the speaker, the\nspoken content, and the start and end times of each utterance.\nAlthough the dataset also contains facial feature data, this was not\nutilized in the current research.\nDAIC-WOZ consists of 189 samples, also referred to as 189\nparticipants, pre-divided into training, validation, and test sets,\ncontaining 107, 35, and 47 samples respectively. Additionally, a\nmetadata table is provided, which includes patient IDs, binary\ndepression labels, PHQ-8 scores, and dimensional depression\nscores (the latter were not used in this study).\n/two.tnum./two.tnum./two.tnum Data preprocessing\nTo enable the LLM to better interpret and extract features from\nthe audio data, the recordings were segmented to accommodate\nthe limited context window of LLMs. Using the transcript ﬁle\ntimestamps and speaker annotations, each sentence spoken by a\nparticipant was isolated. Every ﬁve consecutive utterances were\nthen merged into a single audio ﬁle, ensuring that no audio\nsegments from diﬀerent participants were combined. Given there is\na “speaker” value in the dataset, corresponding transcript data was\nﬁltered to include only the speech of the participant. The sentences\nwere merged in the same manner as the audio: every ﬁve sentences\nwere combined into one element in the new set, subsequently\nmerging the elements into a CSV ﬁle. Some additional metadata\ninformation values were eliminated in this ﬁle, keeping only the\nmerged text content and the associated participant ID. Note that\nthe dataset only provides a single sentence-independent PHQ-8\nscore per participant, which we therefore take as the target for all\nsentences from the same speaker.\nAfter preprocessing, a total of 6,556 audio ﬁles were generated,\neach with a corresponding transcript entry, as both were segmented\nusing the same criteria. Each ﬁle obtains its label in form of the\nparticipant’s PHQ-8 score. During the ﬁnal prediction stage, the\nmodel generates a score for each individual audio segment. These\nsegment-level predictions are then averaged to produce the overall\nPHQ-8 score for the corresponding participant.\n/two.tnum./three.tnum Model pipeline\nWe have organized our model pipeline as illustrated in\nFigure 1.\nWe selected the acoustic information as the primary input. The\nacoustic signal is directly available during clinical interviews,\nwhereas text-based methods require an additional transcription\nstep such as by automatic speech recognition. Nevertheless, the\nspoken text remains valuable for providing clear and explicit\ncontent regarding the subject’s verbal expressions. Therefore, we\nincorporate both types of information to complement each other\nand enhance overall performance.\nWe split our LlaMA ﬁne-tuning with two main phases, namely\nknowledge injection and PHQ-8 score prediction.\n/two.tnum./three.tnum./one.tnum Psychology knowledge injection\nTo address the lack of domain-speciﬁc knowledge in LLMs,\nwe design a learning process that mirrors human cognitive\ndevelopment. Speciﬁcally, we aim for the LLM to read and\ncomprehend psychological knowledge in a manner similar to how\nhumans study and internalize the information. Inspired by the\nwork of\nAbbasi et al. (2024) , we extract question-answer pairs from\npsychological sources and prompt the LLM to generate responses\naccordingly. In contrast to\nAbbasi et al. (2024) , our approach\nintroduces more structured and comprehensive question types to\nfacilitate deeper understanding. Drawing from principles of human\nlearning outlined in\nNovak and Gowin (1984) , eﬀective learning\nrequires understanding what the knowledge is, why it matters, how\nit is applied, and how it connects with prior knowledge to foster\ncritical thinking and practical use. In line with this, we design six\ndistinct types of questions centered on depressive disorders: (1)\nthe deﬁnition of the disorder, (2) the rationale for diagnosing it,\n(3) common symptoms or manifestations, (4) extended or related\nknowledge, and (5) critical thinking questions. This structured\nframework ensures that the LLM not only memorizes facts but also\ndevelops a more nuanced and applicable model of depression.\nWe selected the World Health Organization’s oﬃcial medical\nclassiﬁcation website\n/one.tnumas our primary knowledge source due to its\nauthoritative content, which helps reduce the risk of incorporating\nlow-quality or misleading information that could contribute to\nhallucinations in LLMs. To focus speciﬁcally on depression-related\nknowledge, we extracted a subset of entries by ﬁltering disorder\ntitles using relevant keywords (e. g., anxiety, depress*, mood, stress,\nchronic, isolation), resulting in a total of 123 samples. We then\nemployed the DeepSeek-V3 model from\nLiu A. et al. (2024) to\ngenerate structured question-answer pairs based on the provided\ncontent. Speciﬁcally, we used the following prompt to guide the\ngeneration process:\nConstruct Q&A sets based on one [paragraph] I give you.\n(1) The 10 Q&A sets should be about the key deﬁnitions\nmentioned in the paragraph. The Q&A set should contain no\n/one.tnumhttps://icd.who.int/browse//two.tnum/zero.tnum/two.tnum/four.tnum-/zero.tnum/one.tnum/mms/en\nFrontiers in Computer Science /zero.tnum/four.tnum frontiersin.org\nLi et al. /one.tnum /zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/two.tnum/nine.tnum/seven.tnum/two.tnum/five.tnum\nFIGURE /one.tnum\nTwo-Stage Pipeline of the Large Models considered. Our proposed frame work consists of two key stages. In the ﬁrst stage, we leverage the\nDeepSeek model to extract question-answer pairs from authoritative psychology texts, such as disease deﬁnitions and clinical descriptions from the\nWorld Health Organization (WHO). These extracted pairs, along with transcript data, are used to pretrain the LLM through a process of knowledge\ninjection, enhancing its domain-speciﬁc understanding. In the second stage, we process audio inputs using a feature-extraction DNN, Wav/two.tnumVec to\nobtain their original embeddings. These embeddings are then projected via a feedforward network to align them with the LLM’s hidden space. Once\nthe audio and text modalities are integrated, the LLM, LLaMA is used to predict the depression level.\nextra knowledge. (2) The 10 questions in these sets should\nbe “why” questions. The Q&A set should contain no extra\nknowledge. (3) The 10 Q&A are about the phenomena that\nmay occur on people with such disorder. The Q&A set\nshould contain no extra knowledge. (4) The 5 Q&A sets\nshould be completely based on extended knowledge which\nis not mentioned in the [paragraph], but should also be\nconsidered important about such disorder. (5) The ﬁve Q&A\nsets should show critical thinking. The Q&A set should contain\nno\nextra knowledge.\nThe entire conversation should contain English only.\nT\nhe message you reply must follow the exact format in the\n[example], do not add any extra \" or other marks at the\nbeginning or the end of your question or answer.\n[example]:\nque\nstion: This is the ﬁrst question you construct.\nanswer: This is the ﬁrst answer you construct.\nThis process resulted in a total of 4,920 question-answer\npair\ns. We then provided the questions as prompts to our LLM,\nLLaMA ( Touvron et al., 2023), /two.tnumand employed supervised learning\nto\ntrain the model to generate corresponding answers. Through\nthis process, we aim to eﬀectively inject psychological knowledge\ninto the model, enabling it to better understand and respond to\ndepression-related content.\n/two.tnum./three.tnum./two.tnum Multi-stream training\nTo train the model with multi-stream features, we trained the\nLLM on text features ﬁrst using prompts:\nTranscripts:[Transcript], PHQ Score:\n/two.tnumhttps://huggingface.co/meta-llama/Llama-/two.tnum-/seven.tnumb-hf\nThis enables the LLM to learn text features ﬁrst.\nNe\nxt, to train with audio features and align them with the text-\nbased embedding space of the LLM, we aim to project the audio\nrepresentations into a shared latent space of the LLM. Inspired by\nSALMONN, a speech-based LLM proposed by Tang et al. (2023),\nwe\nadopt a similar pipeline but with a modiﬁcation: instead of\nusing Whisper ( Radford et al., 2023) to extract audio features,\nwhic\nh is primarily designed for speech recognition tasks, we utilize\nWav2Vec 2.0 ( Baevski et al., 2020). We selected Wav2Vec 2.0 due\nto\nits stronger capacity for capturing a broader range of audio\nfeatures, which are more relevant for understanding emotional cues\nand prosodic elements associated with depression. These have been\nformulated below.\nr = Wav2Vec2(rawaudio) (1)\nEmbaudio = feedforward(r) (2)\nPHQ-8 = Linear(LLaMA(Embaudio)−1 ), (3)\nwhere r is in shape of Rs×d a , with s being extracted audio length\nby Wav2Vec2 and da being the hidden dimension, and Embaudio is\nin shape of Rs×d t , with dt being the hidden dimension of LLaMA.\nIn this manner, the audio features are eﬀectively mapped and\nprojected into the embedding space of the LLaMA model, enabling\nseamless integration with its text-based representations. This allows\nLLaMA to learn meaningful representations for the audio modality\nas well. Finally, we utilize the last hidden layer embedding and use\na linear layer to predict the PHQ-8 score.\n/two.tnum./three.tnum./three.tnum Experiments\nTo ensure reproducibility, we ﬁxed the random seed to 42\nusing the random libraries within the respective Numpy and\ntorch libraries in Python. Due to limited GPU resources, we\nemployed Low-Rank Adaptation (LoRA) ( Hu et al., 2022) with\nDistributed\nData Parallel (DDP) for eﬃcient ﬁne-tuning of the\nFrontiers in Computer Science /zero.tnum/five.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/two.tnum/nine.tnum/seven.tnum/two.tnum/five.tnum\nTABLE /one.tnum LLaMA hyper-parameters.\nParameter Value\nr 8\nlora_alpha 16\nlora_dropout 0.1\ntarget_modules “q_proj”, “v_proj”]\nGPU Tesla V100-PCIE-32GB\nTABLE /two.tnum Evaluation on DAIC-WOZ test set for diﬀerent models.\nModel MAE RMSE\nAVEC2016 (audio) (Valstar et al., 2016 ) 5.72 7.78\nLSTM (Afzal Aghaei and Khodaei, 2023 ) 5.7 6.59\nRandom forest ( Afzal Aghaei and\nKhodaei, 2023)\n5.71 6.79\nOurs(audio) 5.373 6.733\nOurs(text) 6.342 8.891\nOurs(audio + text) 5.356 6.713\nOurs(text + knowledge injection) 5.354 6.429\nOurs(audio + knowledge injection) 5.356 6.713\nOurs(audio + text+ knowledge injection) 5.356 6.713\nThe font value is the best performing score.\nLLM. The detailed hyperparameter settings used during training\nare summarized in\nTable 1.\n/three.tnum Results\nWe conducted a series of experiments, and the results are\npresented in Table 2 including ablation studies to measure the\nimpact of certain components of our method. To evaluate model\nperformance, we used Mean Absolute Error (MAE) and Root Mean\nSquare Error (RMSE) as the primary metrics. Additionally, the ﬁnal\nprediction scores are calculated by each participant across multiple\nclips derived from the same individual.\nOur pure audio-based method shows a clear improvement\nover the baseline, which is only available for audio in the AVEC\n2016 challenge (rather than also for text or audio + text) (\nValstar\net al., 2016 ). This demonstrates that the audio features in the\nmodel contribute to a better performance, suggesting that audio\nsignals provide valuable information for the task, outperforming\nthe baseline in both the MAE and RMSE metrics.\nAdditionally, knowledge injection has a noticeable impact\non LLM depression prediction. It lowers both MAE and RMSE\nmarginally compared to the plain audio and text model. The MAE\ndrops from 5.373 to 5.356 for audio, from 6.342 to 5.354 for text,\nand RMSE drops from 6.733 to 6.713 for audio, from 8.891 to 6.429,\nfor text. The performance improves more for text than for audio.\nThis suggests that the text-based knowledge injection has a stronger\neﬀect, likely because the knowledge is provided in a textual form,\nwhich aligns better with the text modality.\nWhen combining both audio and text with knowledge\ninjection, the performance is close to text with knowledge injection,\nand pure text with audio. While this suggests the potential for\ncombining the information streams, it also highlights that the\ncombination may not always result in substantial performance\ngains, possibly due to the fact that the learning ability of LLMs is\nbiased toward audio. Additionally, it may be that the LLM’s learning\nability is limited by small number of trainable parameters from\nLoRA. However, the combination still performs well and indicates\nthat both modalities can be useful when leveraged appropriately.\nTaking another perspective, we investigate whether the\nknowledge injection works as expected. The procedure involves\ninstructing PsycholexLLAMA to generate ﬁfty questions. The\nprompt used is relatively straightforward.\nRegard yourself as a teacher teaching about depression\ndetection, and you are trying to test how well your students\nunderstand this knowledge. Generate num_questions speciﬁc\nquestions about psychological assessment of depression to\nevaluate whether the students know how to assess depression.\nNumber them as 1., 2., 3., etc.\nThe generated questions were then submitted to two diﬀerent\nmodels for response, the one trained with the Question & Answer\npairs and the one without such post-training. Both the questions\nand the corresponding answers were subsequently passed back to\nPsycholexLLAMA, which was tasked with scoring each response on\na scale from 0 to 10. Speciﬁcally, we adopted this prompt:\nRate the following response to a question about depression\non a scale from 0 to 10:\nQUESTION: question\nRESPONSE: response\nEvaluate based on: - Accuracy of information - Sensitivity\nto mental health issues - Helpfulness of the response -\nAppropriate tone\nRate from 0 (completely inappropriate) to 10 (excellent in\nall aspects). Your rating (just the number):\nFor each question, the higher of the two scores was recorded,\nwhich follows the principle of the pass@2 strategy to measure\nperformance of LLMs proposed by\nChen et al. (2021) . Finally, the\naverage of these highest scores was calculated to yield the overall\nperformance score. The original model achieved a score of 7.32,\nwhereas our trained model obtained a score of 8.20, indicating\nthat our proposed strategy is eﬀective and that the model has\nsuccessfully learnt from the psychology knowledge base.\nWhile our strategy demonstrates promising results, it also\nhighlights two key directions for future work. First, although\nLoRA oﬀers a lightweight and eﬃcient ﬁne-tuning method, its\ncapacity to induce substantial changes in model behavior is\nlimited compared to full-scale training of LLMs. However, fully\ntraining an LLM demands signiﬁcant computational resources,\nwhich may not always be feasible. Second, as previously discussed,\nthere is a notable scarcity of psychologically-informed audio data.\nAs a result, the model—despite being equipped with theoretical\nknowledge derived from text—struggles to eﬀectively transfer\nthis understanding to audio-based applications. Generating such\nFrontiers in Computer Science /zero.tnum/six.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/two.tnum/nine.tnum/seven.tnum/two.tnum/five.tnum\naudio content artiﬁcially using AI can often lack authenticity. A\npotential solution would be the development of clinically annotated\naudio datasets that pair spoken examples with corresponding\ntextual explanations—such as diagnostic insights based on criteria\nfrom the WHO. Such resources could serve as a valuable\nbridge between theory and practical application in audio-based\npsychological assessments.\n/four.tnum Conclusion\nIn conclusion, our work presented a novel approach to\nadvancing depression detection by leveraging the capabilities of\nLLMs. By incorporating professional psychological knowledge into\nthe considered LLM through a carefully designed pipeline, we\nenhanced the model’s ability to interpret and evaluate depressive\nsymptoms more eﬀectively. Our empirical results, validated on\nthe DAIC-WOZ dataset, demonstrate that the proposed method\nsubstantially outperforms established baselines (around 0.35 on\nMAE and 1.36 on RMSE), underscoring the potential of LLMs as\na powerful tool in mental health assessment. These ﬁndings pave\nthe way for future research exploring the integration of domain\nexpertise into multimodal AI systems for clinical applications. In\nthe future, the work may continue to fully train LLMs and to obtain\nannotations or suggestions from specialists. Additionally, future\nwork should explore training full-scale LLMs and incorporating\nricher sound descriptions as a form of knowledge injection, in\norder to better bridge the gap between acoustic features and the\ntext-based knowledge learned by LLMs.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nYL: Conceptualization, Software, Writing – original draft,\nProject administration, Methodology, Formal analysis, Writing –\nreview & editing, Visualization, Investigation, Validation, Data\ncuration, Resources. SS: Software, Data curation, Investigation,\nResources, Formal analysis, Writing – original draft. MM: Funding\nacquisition, Resources, Writing – review & editing, Supervision. BS:\nSupervision, Funding acquisition, Writing – review & editing.\nFunding\nThe author(s) declare that ﬁnancial support was received for\nthe research and/or publication of this article. This research was\npartially supported and funded by the Munich Center for Machine\nLearning and the Munich Data Science Institute.\nAcknowledgments\nWe acknowledge Hanqian Li from Shandong University for\nproviding initial draft experiment codes, Adria Mallol Ragolta from\nTechnical University Munich to discuss ideas with us.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Gen AI was used in the creation\nof this manuscript.\nAny alternative text (alt text) provided alongside ﬁgures in\nthis article has been generated by Frontiers with the support of\nartiﬁcial intelligence and reasonable eﬀorts have been made to\nensure accuracy, including review by the authors wherever possible.\nIf you identify any issues, please contact us.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAbbasi, M. A., Mirnezami, F. S., and Naderi, H. (2024). Psych olex: Unveiling\nthe psychological mind of large language models. arXiv [preprint] arXiv:2408.08848.\ndoi: 10.21203/rs.3.rs-4920871/v1\nAfzal Aghaei, A., and Khodaei, N. (2023). Automated depressi on recognition using\nmultimodal machine learning: a study on the daic-woz dataset. Comp. Mathem. Comp.\nModel. Appl. 2, 45–53. doi: 10.48308/CMCMA.2.1.45\nAmin, M. M., Cambria, E., and Schuller, B. W. (2023). Will aﬀecti ve\ncomputing emerge from foundation models and general artiﬁcia l intelligence? A\nﬁrst evaluation of chatGPT. IEEE Intellig. Syst . 38, 15–23. doi: 10.1109/MIS.2023.325\n4179\nAshraf, A., Gunawan, T. S., Riza, B. S., Haryanto, E. V., and Ja nin, Z. (2020). On the\nreview of image and video-based depression detection using m achine learning. Indon\nJ. Elect. Eng. Comp. Sci . 19, 1677–1684. doi: 10.11591/ijeecs.v19.i3.pp1677-1684\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. (2020). wav2v ec 2.0: A framework\nfor self-supervised learning of speech representations. Adv Neural Inf Process Syst . 33,\n12449–12460. doi: 10.48550/arXiv.2006.11477\nBerardi, M., Brosch, K., Pfarr, J.-K., Schneider, K., Sültma nn, A., Thomas-\nOdenthal, F., et al. (2023). Relative importance of speech and voi ce features\nin the classiﬁcation of schizophrenia and depression. Transl Psychiatry . 13:298.\ndoi: 10.1038/s41398-023-02594-0\nFrontiers in Computer Science /zero.tnum/seven.tnum frontiersin.org\nLi et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/six.tnum/two.tnum/nine.tnum/seven.tnum/two.tnum/five.tnum\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kapla n, J., et al. (2021).\nEvaluating large language models trained on code. arXiv [preprint] arXiv:2107.03374.\ndoi: 10.48550/arXiv.2107.03374\nFarruque, N., Goebel, R., Sivapalan, S., and Zaïane, O. R. (2024). Depression\nsymptoms modelling from social media text: an LLM driven semi-s upervised learning\napproach. Lang. Resources Eval . 58, 1013–1041. doi: 10.1007/s10579-024-09720-4\nGerber, P. D., Barrett, J., Barrett, J., Manheimer, E., Whiti ng, R., and Smith, R.\n(1989). Recognition of depression by internists in primary ca re: a comparison of\ninternist and “gold standard” psychiatric assessments. J. Gen. Intern. Med . 4, 7–13.\ndoi: 10.1007/BF02596483\nGratch, J., Artstein, R., Lucas, G., Stratou, G., Scherer, S., N azarian, A., et al.\n(2014). “The distress analysis interview corpus of human and c omputer interviews, ”\nin Proceedings of the Ninth International Conference on Language Resources a nd\nEvaluation (LREC‘14), eds. N. Calzolari, K. Choukri, T. Declerck, H. Loftsson,\nB. Maegaard, J. Mariani, et al. (Reykjavik, Iceland: European L anguage Resources\nAssociation (ELRA)), 3123–3128.\nHe, X., Triantafyllopoulos, A., Kathan, A., Milling, M., Yan, T., R ajamani, S. T., et al.\n(2022). “Depression diagnosis and forecast based on mobile pho ne sensor data, ” in2022\n44th Annual International Conference of the IEEE Engineering in Medicin e & Biology\nSociety (EMBC) (Glasgow: IEEE), 4679–4682.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., et al. (2022). Lora:\nLow-rank adaptation of large language models. ICLR. 1:3.\nHuang, X., Wang, F., Gao, Y., Liao, Y., Zhang, W., Zhang, L., e t al. (2024).\nDepression recognition using voice-based pre-training mode l. Sci Rep . 14:12734.\ndoi: 10.1038/s41598-024-63556-0\nKathan, A., Harrer, M., Küster, L., Triantafyllopoulos, A., He, X., Milling,\nM., et al. (2022a). Personalised depression forecasting using mobile sensor\ndata and ecological momentary assessment. Front. Digital Health 4:964582.\ndoi: 10.3389/fdgth.2022.964582\nKathan, A., Triantafyllopoulos, A., He, X., Milling, M., Yan, T., R ajamani, S. T.,\net al. (2022b). Journaling data for daily PHQ-2 depression predic tion and forecasting, ”\nin 2022 44th Annual International Conference of the IEEE Engineering in Med icine &\nBiology Society (EMBC) (Glasgow: IEEE), 2627–2630.\nLan, X., Cheng, Y., Sheng, L., Gao, C., and Li, Y. (2024). Depre ssion detection\non social media with large language models. arXiv [preprint] arXiv:2403.10750.\ndoi: 10.48550/arXiv.2403.10750\nLi, Y., Milling, M., and Schuller, B. W. (2025). Neuroplasticity in artiﬁcial\nintelligence-an overview and inspirations on drop in & out learn ing. arXiv [preprint]\narXiv:2503.21419. doi: 10.48550/arXiv.2503.21419\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., et al. (2024 ). Deepseek-v3\ntechnical report. arXiv [preprint] arXiv:2412.19437. doi: 10.48550/arXiv.2412.19 437\nLiu, Z., Yang, K., Xie, Q., Zhang, T., and Ananiadou, S. (2024 ). “Emollms: A series\nof emotional large language models and annotation tools for compr ehensive aﬀective\nanalysis, ” inProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining (New York: ACM), 5487–5496. doi: 10.1145/3637528.3671552\nLyu, S., Ren, X., Du, Y., and Zhao, N. (2023). Detecting depres sion of\nchinese microblog users via text analysis: Combining linguist ic inquiry word\ncount (liwc) with culture and suicide related lexicons. Front. Psychiat . 14:1121583.\ndoi: 10.3389/fpsyt.2023.1121583\nMallol-Ragolta, A., Milling, M., and Schuller, B. (2024). “Multi-tri plet loss-based\nmodels for categorical depression recognition from speech, ” in Proc. IberSPEECH 2024,\n31–35. doi: 10.21437/IberSPEECH.2024-7\nMartino, A., Iannelli, M., and Truong, C. (2023). “Knowledge in jection to counter\nlarge language model (LLM) hallucination, ” in European Semantic Web Conference\n(Cham: Springer), 182–185.\nMin, K., Yoon, J., Kang, M., Lee, D., Park, E., and Han, J. (202 3). Detecting\ndepression on video logs using audiovisual features. Humanit. Soc. Sci. Commun . 10,\n1–8. doi: 10.1057/s41599-023-02313-6\nNiu, M., Zhao, Z., Tao, J., Li, Y., and Schuller, B. W. (2022). Du al attention and\nelement recalibration networks for automatic depression level prediction. IEEE Trans.\nAﬀect. Comp. 14, 1954–1965. doi: 10.1109/TAFFC.2022.3177737\nNovak, J. D., and Gowin, D. B. (1984). Learning How to Learn . Cambridge:\nCambridge University Press.\nOgunleye, B., Sharma, H., and Shobayo, O. (2024). Sentiment i nformed sentence\nbert-ensemble algorithm for depression detection. Big Data Cognit. Com . 8:112.\ndoi: 10.3390/bdcc8090112\nQassim, S., Golden, G., Slowey, D., Sarfas, M., Whitmore, K., Per ez, T., et al. (2023).\nA mixed-methods feasibility study of a novel ai-enabled, web- based, clinical decision\nsupport system for the treatment of major depression in adults. J. Aﬀect. Disord. Reports\n14:100677. doi: 10.1016/j.jadr.2023.100677\nRadford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., an d Sutskever, I.\n(2023). “Robust speech recognition via large-scale weak superv ision, ” inInternational\nConference on Machine Learning (New York: PMLR), 28492–28518.\nRingeval, F., Schuller, B., Valstar, M., Cummins, N., Cowie, R., Tavabi, L., et al.\n(2019). “AVEC 2019 workshop and challenge: state-of-mind, de tecting depression with\nAI, and cross-cultural aﬀect recognition, ” in Proceedings of the 9th International on\nAudio/visual Emotion Challenge and Workshop , 3–12.\nRingeval, F., Schuller, B., Valstar, M., Gratch, J., Cowie, R., Sc herer, S., et al. (2017).\n“AVEC 2017: real-life depression, and aﬀect recognition works hop and challenge, ” in\nProceedings of the 7th Annual Workshop on Audio/Visual Emotion Challen ge, A VEC ’17\n(New York, NY: Association for Computing Machinery), 3–9.\nRobertson, C., Woods, A., Bergstrand, K., Findley, J., Balser , C., and Slepian, M. J.\n(2023). Diverse patients’ attitudes towards artiﬁcial inte lligence (ai) in diagnosis. PLOS\nDigital Health 2:e0000237. doi: 10.1371/journal.pdig.0000237\nSadeghi, M., Richer, R., Egger, B., Schindler-Gmelch, L., Rupp, L . H., Rahimi,\nF., et al. (2024). Harnessing multimodal approaches for depressi on detection\nusing large language models and facial expressions. NPJ Mental Health Res . 3:66.\ndoi: 10.1038/s44184-024-00112-8\nSardari, S., Nakisa, B., Rastgoo, M. N., and Eklund, P. (2022) . Audio based\ndepression detection using convolutional autoencoder. Expert Syst Appl . 189:116076.\ndoi: 10.1016/j.eswa.2021.116076\nSchuller, B., Mallol-Ragolta, A., Almansa, A. P., Tsangko, I., Amin , M. M.,\nSemertzidou, A., et al. (2024). Aﬀective computing has change d: the foundation\nmodel disruption. arXiv [preprint] arXiv:2409.08907. doi: 10.48550/arXiv.2409.\n08907\nShin, D., Kim, H., Lee, S., Cho, Y., and Jung, W. (2024). Using large language models\nto detect depression from user-generated diary text data as a novel approach in digital\nmental health screening: Instrument validation study. J. Med. Internet Res . 26:e54617.\ndoi: 10.2196/54617\nSivamanikandan, S., Santhosh, V., Sanjaykumar, N., Jerin M ahibha, C., and\nDurairaj, T. (2022). “scubeMSEC@LT-EDI-ACL2022: Detecti on of depression using\ntransformer models, ” in Proceedings of the Second Workshop on Language Technology\nfor Equality, Diversity and Inclusion , eds. B. R. Chakravarthi, B. Bharathi, J. P.\nMcCrae, M. Zarrouk, K. Bali, and P. Buitelaar (Dublin: Associat ion for Computational\nLinguistics), 212–217.\nSquires, M., Tao, X., Elangovan, S., Gururajan, R., Zhou, X., Ac harya, U. R.,\net al. (2023). Deep learning and machine learning in psychiatry: a survey of current\nprogress in depression detection, diagnosis and treatment. Brain Inform . 10:10.\ndoi: 10.1186/s40708-023-00188-6\nTang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., et al. (2023) . Salmonn: towards\ngeneric hearing abilities for large language models. arXiv [preprint] arXiv:2310.13289.\ndoi: 10.48550/arXiv.2310.13289\nTank, C., Pol, S., Katoch, V., Mehta, S., Anand, A., and Shah, R . R. (2024).\nDepression detection and analysis using large language models on textual and\naudio-visual modalities. arXiv [preprint] arXiv:2407.06125. doi: 10.48550/arXiv.2407.\n06125\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M .-A., Lacroix, T.,\net al. (2023). Llama: Open and eﬃcient foundation language models . arXiv [preprint]\narXiv:2302.13971. doi: 10.48550/arXiv.2302.13971\nValstar, M., Gratch, J., Schuller, B., Ringeval, F., Lalanne, D., T orres Torres, M.,\net al. (2016). “AVEC 2016: Depression, mood, and emotion recog nition workshop and\nchallenge, ” inProceedings of the 6th International Workshop on Audio/Visual Emoti on\nChallenge, 3–10.\nValstar, M., Schuller, B., Smith, K., Eyben, F., Jiang, B., Bilak hia, S., et al.\n(2013). “AVEC 2013: the continuous audio/visual emotion an d depression recognition\nchallenge, ” in Proceedings of the 3rd ACM International Workshop on Audio/Visual\nEmotion Challenge (New York: ACM) 3–10.\nZafar, F., Alam, L. F., Vivas, R. R., Wang, J., Whei, S. J., Mehm ood, S., et al. (2024).\nThe role of artiﬁcial intelligence in identifying depression an d anxiety: a comprehensive\nliterature review. Cureus 16:56472. doi: 10.7759/cureus.56472\nZhang, Z., Peng, L., Pang, T., Han, J., Zhao, H., and Schuller, B . W. (2024).\nRefashioning emotion recognition modelling: the advent of ge neralised large models.\nIEEE Trans. Comp. Soc. Syst . 11, 6690–6704. doi: 10.1109/TCSS.2024.3396345\nFrontiers in Computer Science /zero.tnum/eight.tnum frontiersin.org",
  "topic": "Spoken language",
  "concepts": [
    {
      "name": "Spoken language",
      "score": 0.5766844749450684
    },
    {
      "name": "Depression (economics)",
      "score": 0.546098530292511
    },
    {
      "name": "Computer science",
      "score": 0.5010106563568115
    },
    {
      "name": "Linguistics",
      "score": 0.4669937193393707
    },
    {
      "name": "Natural language processing",
      "score": 0.4542137384414673
    },
    {
      "name": "Psychology",
      "score": 0.44916045665740967
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33748283982276917
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Macroeconomics",
      "score": 0.0
    }
  ]
}