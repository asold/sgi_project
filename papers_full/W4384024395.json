{
  "title": "Evaluating Large Language Models in Extracting Cognitive Exam Dates and Scores",
  "url": "https://openalex.org/W4384024395",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2095945326",
      "name": "Hao Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2995026561",
      "name": "Neil Jethani",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2102875672",
      "name": "Simon Jones",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2070565107",
      "name": "Nicholas Genes",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2096206670",
      "name": "Vincent J. Major",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2889047133",
      "name": "Ian S Jaffe",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3015094690",
      "name": "Anthony B. Cardillo",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3202113543",
      "name": "Noah Heilenbach",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4283182090",
      "name": "Nadia Fazal Ali",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A5019737350",
      "name": "Luke J. Bonanni",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2988780533",
      "name": "Andrew J. Clayburn",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4296685757",
      "name": "Zain Khera",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4384039988",
      "name": "Erica C. Sadler",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2809483708",
      "name": "Jaideep Prasad",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4220215879",
      "name": "Jamie Schlacter",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2100631594",
      "name": "Kevin Liu",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2125265398",
      "name": "Benjamin Silva",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3092633817",
      "name": "Sophie Montgomery",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2115713152",
      "name": "Eric J. Kim",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2538475737",
      "name": "Jacob Lester",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2762440835",
      "name": "Theodore M Hill",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3021378894",
      "name": "Alba Avoricani",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3115361973",
      "name": "Ethan Chervonski",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4384039994",
      "name": "James Davydov",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2025926024",
      "name": "William Small",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2791518936",
      "name": "Eesha Chakravartty",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2139396761",
      "name": "Himanshu Grover",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2182650040",
      "name": "John A. Dodson",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2183521394",
      "name": "Abraham A. Brody",
      "affiliations": [
        "Meyer (China)",
        "New York University",
        "KES College"
      ]
    },
    {
      "id": "https://openalex.org/A1850176264",
      "name": "Yindalon Aphinyanaphongs",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4221759198",
      "name": "Arjun Masurkar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2078741054",
      "name": "Narges Razavian",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2995026561",
      "name": "Neil Jethani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102875672",
      "name": "Simon Jones",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2070565107",
      "name": "Nicholas Genes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096206670",
      "name": "Vincent J. Major",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2889047133",
      "name": "Ian S Jaffe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3015094690",
      "name": "Anthony B. Cardillo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3202113543",
      "name": "Noah Heilenbach",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4283182090",
      "name": "Nadia Fazal Ali",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5019737350",
      "name": "Luke J. Bonanni",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2988780533",
      "name": "Andrew J. Clayburn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296685757",
      "name": "Zain Khera",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4384039988",
      "name": "Erica C. Sadler",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2809483708",
      "name": "Jaideep Prasad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4220215879",
      "name": "Jamie Schlacter",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100631594",
      "name": "Kevin Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125265398",
      "name": "Benjamin Silva",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3092633817",
      "name": "Sophie Montgomery",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115713152",
      "name": "Eric J. Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2538475737",
      "name": "Jacob Lester",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2762440835",
      "name": "Theodore M Hill",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3021378894",
      "name": "Alba Avoricani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3115361973",
      "name": "Ethan Chervonski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4384039994",
      "name": "James Davydov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2025926024",
      "name": "William Small",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2791518936",
      "name": "Eesha Chakravartty",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2139396761",
      "name": "Himanshu Grover",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2182650040",
      "name": "John A. Dodson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2183521394",
      "name": "Abraham A. Brody",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1850176264",
      "name": "Yindalon Aphinyanaphongs",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2078741054",
      "name": "Narges Razavian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3022003261",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4380786006",
    "https://openalex.org/W4319062614",
    "https://openalex.org/W4362521774",
    "https://openalex.org/W4372231834",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W2168845544",
    "https://openalex.org/W4323050332",
    "https://openalex.org/W42510333",
    "https://openalex.org/W4323347604",
    "https://openalex.org/W4317390716",
    "https://openalex.org/W4318263917",
    "https://openalex.org/W4318035272",
    "https://openalex.org/W2122402213",
    "https://openalex.org/W2769851464",
    "https://openalex.org/W2778088240",
    "https://openalex.org/W2146089916",
    "https://openalex.org/W2785420384",
    "https://openalex.org/W745488007",
    "https://openalex.org/W2139865360",
    "https://openalex.org/W2611650229",
    "https://openalex.org/W3103694015",
    "https://openalex.org/W3089862415",
    "https://openalex.org/W2772121968",
    "https://openalex.org/W4379769651",
    "https://openalex.org/W3036909358",
    "https://openalex.org/W1779612606",
    "https://openalex.org/W2897815444",
    "https://openalex.org/W2913961733",
    "https://openalex.org/W3021953657",
    "https://openalex.org/W4375949262",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W3146388672",
    "https://openalex.org/W3112296242",
    "https://openalex.org/W2616393094",
    "https://openalex.org/W2972048113",
    "https://openalex.org/W3046275966",
    "https://openalex.org/W2912533302",
    "https://openalex.org/W3204089331",
    "https://openalex.org/W2949709152",
    "https://openalex.org/W1991952617",
    "https://openalex.org/W2165758561",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W2155243985",
    "https://openalex.org/W2167563976",
    "https://openalex.org/W2133814768",
    "https://openalex.org/W2012639032",
    "https://openalex.org/W2769339277",
    "https://openalex.org/W2239135493",
    "https://openalex.org/W2940299115",
    "https://openalex.org/W2964696298",
    "https://openalex.org/W2908201961",
    "https://openalex.org/W2951441387",
    "https://openalex.org/W2786384427",
    "https://openalex.org/W2795091386",
    "https://openalex.org/W2113105800",
    "https://openalex.org/W2057913811",
    "https://openalex.org/W2053870274",
    "https://openalex.org/W2121920862",
    "https://openalex.org/W2803405387",
    "https://openalex.org/W1441560749",
    "https://openalex.org/W2047761929",
    "https://openalex.org/W2121274101",
    "https://openalex.org/W2914514892",
    "https://openalex.org/W2152726215",
    "https://openalex.org/W1969175160",
    "https://openalex.org/W1795144871",
    "https://openalex.org/W2138721673",
    "https://openalex.org/W2971544035",
    "https://openalex.org/W2948278818",
    "https://openalex.org/W2557074642",
    "https://openalex.org/W1847168837",
    "https://openalex.org/W2109347377",
    "https://openalex.org/W3101156210",
    "https://openalex.org/W3089638105",
    "https://openalex.org/W2008854521",
    "https://openalex.org/W2146417320",
    "https://openalex.org/W2148080316",
    "https://openalex.org/W2058161128",
    "https://openalex.org/W3171757072",
    "https://openalex.org/W2167311298",
    "https://openalex.org/W2807758560"
  ],
  "abstract": "Abstract Importance Large language models (LLMs) are crucial for medical tasks. Ensuring their reliability is vital to avoid false results. Our study assesses two state-of-the-art LLMs (ChatGPT and LlaMA-2) for extracting clinical information, focusing on cognitive tests like MMSE and CDR. Objective Evaluate ChatGPT and LlaMA-2 performance in extracting MMSE and CDR scores, including their associated dates. Methods Our data consisted of 135,307 clinical notes (Jan 12th, 2010 to May 24th, 2023) mentioning MMSE, CDR, or MoCA. After applying inclusion criteria 34,465 notes remained, of which 765 underwent ChatGPT (GPT-4) and LlaMA-2, and 22 experts reviewed the responses. ChatGPT successfully extracted MMSE and CDR instances with dates from 742 notes. We used 20 notes for fine-tuning and training the reviewers. The remaining 722 were assigned to reviewers, with 309 each assigned to two reviewers simultaneously. Inter-rater-agreement (Fleiss’ Kappa), precision, recall, true/false negative rates, and accuracy were calculated. Our study follows TRIPOD reporting guidelines for model validation. Results For MMSE information extraction, ChatGPT (vs. LlaMA-2) achieved accuracy of 83% (vs. 66.4%), sensitivity of 89.7% (vs. 69.9%), true-negative rates of 96% (vs 60.0%), and precision of 82.7% (vs 62.2%). For CDR the results were lower overall, with accuracy of 87.1% (vs. 74.5%), sensitivity of 84.3% (vs. 39.7%), true-negative rates of 99.8% (98.4%), and precision of 48.3% (vs. 16.1%). We qualitatively evaluated the MMSE errors of ChatGPT and LlaMA-2 on double-reviewed notes. LlaMA-2 errors included 27 cases of total hallucination, 19 cases of reporting other scores instead of MMSE, 25 missed scores, and 23 cases of reporting only the wrong date. In comparison, ChatGPT’s errors included only 3 cases of total hallucination, 17 cases of wrong test reported instead of MMSE, and 19 cases of reporting a wrong date. Conclusions In this diagnostic/prognostic study of ChatGPT and LlaMA-2 for extracting cognitive exam dates and scores from clinical notes, ChatGPT exhibited high accuracy, with better performance compared to LlaMA-2. The use of LLMs could benefit dementia research and clinical care, by identifying eligible patients for treatments initialization or clinical trial enrollments. Rigorous evaluation of LLMs is crucial to understanding their capabilities and limitations.",
  "full_text": "Evaluating ChatGPT in Information Extraction: A Case Study of\nExtracting Cognitive Exam Dates and Scores\nAuthors:\nNeilJethani, NYUGrossmanSchool ofMedicine\nSimonJones, NYUGrossmanSchool ofMedicine\nNicholasGenes, NYUGrossmanSchool ofMedicine\nVincent J.Major,NYUGrossmanSchool ofMedicine\nIanS.Jaffe, NYUGrossmanSchool ofMedicine\nAnthonyB.Cardillo, NYUGrossmanSchool ofMedicine\nNoahHeilenbach, NYUGrossmanSchool ofMedicine\nNadiaFazalAli, NYUGrossmanSchool ofMedicine\nLukeJ.Bonanni, NYUGrossmanSchool ofMedicine\nAndrewJ.Clayburn, NYUGrossmanSchool ofMedicine\nZainKhera, NYUGrossmanSchool ofMedicine\nEricaC.Sadler, NYUGrossmanSchool ofMedicine\nJaideepPrasad, NYUGrossmanSchool ofMedicine\nJamieSchlacter, NYUGrossmanSchool ofMedicine\nKevinLiu, NYUGrossmanSchool ofMedicine\nBenjaminSilva,NYUGrossmanSchool ofMedicine\nSophieMontgomery, NYUGrossmanSchool ofMedicine\nEricJ.Kim, NYUGrossmanSchool ofMedicine\nJacobLester, NYUGrossmanSchool ofMedicine\nTheodore M.Hill,NYUGrossmanSchool ofMedicine\nAlbaAvoricani, NYUGrossmanSchool ofMedicine\nEthanChervonski, NYUGrossmanSchool ofMedicine\nJamesDavydov,NYUGrossman School ofMedicine\nWilliamSmall, NYUGrossmanSchool ofMedicine\nEeshaChakravartty, NYUGrossman School ofMedicine\nHimanshuGrover, NYUGrossmanSchool ofMedicine\nJohnA.Dodson, NYUGrossmanSchool ofMedicine\nAbrahamA.Brody, NYURoryMeyersCollegeofNursing,NYUGrossmanSchool ofMedicine\nYindalonAphinyanaphongs, NYUGrossmanSchool ofMedicine\nNargesRazavian*, NYUGrossmanSchool ofMedicine\n* Correspondence should be addressed to:\nNargesRazavian,AssistantProfessor,NYUGrossman School ofMedicine\nSte642,227East30thStreet,NewYork,NY10016\nnarges.razavian@nyulangone.org\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nAbstract\nBackground: Large language models (LLMs) provide powerful natural language processing\n(NLP) capabilities in medical and clinical tasks. Evaluating LLM performance is crucial due to\npotential false results. In this study, we assessed ChatGPT,astate-of-the-artLLM,in extracting\ninformation from clinical notes, focusing on cognitive tests, specifically the Mini Mental State\nExam (MMSE) and the Cognitive Dementia Rating (CDR). We tasked ChatGPT withextracting\nMMSEandCDRscoresandcorrespondingdatesfromclinical notes.\nMethods: Our cohort had 135,307clinical notes(Jan12th,2010 toMay24th,2023) mentioning\nMMSE, CDR, or Montreal Cognitive Assessment (MoCA). After applying inclusion criteria and\nexcluding notes with only MoCA, 34,465 notes remained. Among them, 765 were randomly\nselected and underwent analysis by ChatGPT. 22 medically-trained experts reviewed\nChatGPT's responses and provided ground truth. ChatGPT (GPT-4, version\n\"2023-03-15-preview\") was used on the 765 notes to extract MMSE and CDR instances with\ncorresponding dates. Inference was successful for 742 notes. We used20 notesfor fine-tuning\nand training thereviewers.Theremaining722 wereassignedtoreviewers,with 309assignedto\ntwo reviewers simultaneously. Inter-rater-agreement (Fleiss' Kappa), precision, recall,true/false\nnegativerates,andaccuracywerecalculated.\nResults: For MMSE information extraction, ChatGPT achieved 83% accuracy. It demonstrated\nhigh sensitivity with a Macro-recall of 89.7% and outstanding true-negative rates of 96%. The\nprecision for MMSE was also high at 82.7%. In the case of CDR information extraction,\nChatGPT achieved 89% accuracy. It showed excellent sensitivity with a Macro-recall of 91.3%\nand a perfect true-negative rate of 100%. However, the precision for CDR was lower at 57%.\nAnalyzing the ground truth data, it was found that 89.1% of the notes included an MMSE\ndocumentation, whereas only 14.3% had aCDRdocumentation,whichaffectedtheprecisionof\nCDR extraction. Inter-rater-agreement was substantial, supporting the validity of our findings.\nReviewers considered ChatGPT's responses correct (96% for MMSE, 98% for CDR) and\ncomplete(84%for MMSE,83%for CDR).\nConclusion:ChatGPTdemonstratesoverall accuracyinextractingMMSEandCDRscoresand\ndates, potentially benefiting dementia research and clinical care. Prior probability of the\ninformation appearing in the text impacted ChatGPT’s precision. Rigorous evaluation of LLMs\nfor diverse medical tasksiscrucial tounderstandtheir capabilitiesandlimitations.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nIntroduction\nLarge-scale language models (LLMs) [1–4]haveemergedaspowerful toolsinnatural language\nprocessing (NLP), capable of performing diverse tasks when prompted [5] [6]. These models\nhave demonstrated impressive clinical reasoning abilities [7], successfully passing medical\nlicensing exams [8] [9] [10] and generating medical advice on distinct subjects, including\ncardiovascular disease [11], breast cancer [12], colonoscopy [13], and general health inquiries\n[14], [6], [15] [16]. These models can produce clinical notes [16] and assist in writing research\narticles [16]. Medical journalshavebegundevelopingpoliciesaround useofLLMsinwriting [17]\n[18] [19] [20] [21] [22] and reviewing. Examples of such LLMs include ChatGPT [2] [1],\nMed-PALM-2[3],LLaMA[4],andopen-sourcemodelsactivelyproducedbythecommunity [23].\nIn this study, we focus on evaluatinginformation extraction abilities of ChatGPT (powered by\nGPT-4 [2]) from clinical notes. Information extraction involves the retrieval of specific bits of\ninformation from unstructured clinical notes, a task historically handled by rule-based systems\n[24,25] [26] [27] [28] [29] [30] or language models explicitly trained on datasets annotated by\nhuman experts [31] [32] [33][34][35][36].Rule-basedsystemslackacontextual understanding\nand struggle with complex sentence structures, ambiguous language, and long-distance\ndependencies, often leading to high false positive rates and low sensitivities [37] [38] [39] [40].\nAdditionally, training a new model for this task can be computationally demanding and require\nsubstantial human effort. In contrast, LLMs, such as ChatGPT, operate at “zero-shot” capacity\n[41][42][43],i.e.,onlyrequiringapromptdescribingthe desiredinformationtobeextracted.\nDespite their promise, LLMs also have a potential limitation - the generation of factually\nincorrect yet highly convincing outputs, commonly known as “hallucination.” The massive\narchitecturesandcomplextrainingschemesofLLMshamper “model explanation” andtheability\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nto intrinsically guarantee behavior. This issue has been extensively discussed in the literature,\nemphasizing the need for cautious interpretation and validation of information generated by\nLLMs[44][2][45].\nOneareawhereLLMsmaygreatlybenefithealthcareisinthe identificationofmemoryproblems\nand other symptoms indicative of Alzheimer’s Disease and Alzheimer’s Disease Related\nDementias (AD/ADRD) within clinical notes. AD/ADRD is commonly underdiagnosed or\ndiagnosed later in the disease trajectory, particularly in racial andethnicminoritizedgroups[46]\n[47] [48] [49] [50] [51]. The precise extraction of cognitive test scores holds significant\nimportance in the development and clinical validation of tools that can facilitate early detection\n[52] of AD/ADRD in the clinic. Earlier identification can lead to a host of benefits, including\nassisting with advanced care planning, performing secondary cardiovascular disease\nprevention, which may reduce worsening of cognitive impairment [53] [54], identification for\nserving in research trials [55] [56,57], and with the rapid advancement in biologic therapeutics,\nthe opportunity to receive potentially disease modifying drugs [57] [58]. Accurately extracting\ncognitive exam scores (often buried in clinical notes and not documented in any structured\nfield), enables validation, training and fine-tuning of models at a much larger scale in a clinical\nsetting for a much more racial/ethnically diverse patient population set compared to current\nresearchcohorts.\nThe primary focusofthispaper isthereforeon the validationofastate-of-the-artLLM(ChatGPT\npowered by GPT-4), for information extraction related to cognitive tests, specifically the\nMini-Mental State Examination (MMSE) [59] and Clinical Dementia Rating (CDR) [60], from\nclinical notesofaraciallyandethnicallydiversepatientpopulation.Our objectiveistoaccurately\nextractboththeexamscoreandthedatewhen the examwasadministeredusingChatGPT.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nThis study represents a large-scale formal evaluation of ChatGPT's performance ininformation\nextraction from clinical notes. Going forward, we intend to employ this benchmark dataset to\nvalidate other (open or closed-source) LLMs. Furthermore, we planto adoptasimilar approach\nto validate LLMs for information extraction across various clinical use cases. By prioritizing\nprompt engineering with ChatGPT for extracting clinical information, this research aims to\nenhance our understandingofthepotential ofLLMsinhealthcareandfacilitatethe development\nofreliableandrobustclinical informationextractiontools.\nMethods\nThis study is approved under IRB i20-01095, “Understanding and predicting Alzheimer’s\nDisease.” NYUDataCoreserviceswereutilizedtopreparethedataasdescribedbelow.\nDataset\nAn original cohort of 135,307 clinical notes corresponding to inpatient, outpatient, and\nemergency department visits between January 12th 2010 and May 24th 2023, which included\nany of the following keywords (‘MMSE’, ‘CDR,’ or ‘MoCA’ case-insensitive) wereidentified (see\nFigure 1). MMSE stands for Mini Mental State Exam, CDR stands for Cognitive Dementia\nRating, and MoCa stands for Montreal Cognitive Assessment [61]. These notes belonged to\n52,948 patients. From among these patients, 26,355 had a non-contrast brain Magnetic\nResonance Imaging (MRI) in the system. Limiting the clinical notestothosewhohadanMRIin\nthe system resulted in 77,547 notes. These notes were extracted. At this stage we further\nlimited the notes to those including anymentionsofMMSEand/or CDR(ignoring MoCA),which\nyielded34,465clinical notesfor analysis.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nFigure1: Flowchart of clinicalnotesevaluatedfor inclusioninthefinalsampleof\nGPT-analyzednotes\nThe choice for requiring patients to have a brain MRIaswell asMMSEand/or CDRenablesus\nto have a similar level ofgranularityastheAlzheimer’sDiseaseNeuro-ImagingInitiative (ADNI)\n[62], which also uses MMSE and CDR for definition of mild cognitive impairment anddementia\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nstages. This further enables us to harmonize our clinical dataset with these large research\ncohorts. Similarly, the choice to ignore MoCA was due to the lack of inclusion of MoCA in\nstandard definition for stages of cognitive impairment in ADNI. The mild cognitive impairment\nand (mild, moderate or severe) dementia definition criterias utilized in ADNI are included in\nSupplementary Table S1. Data harmonization is beyond the scope of this paper, although\ninformationextractionplaysasubstantial role inenablingit.\nFrom among 34,465 notes that fit the inclusion criteria, a random selection of 765 notes was\nidentified to undergo information extraction via ChatGPT and manual evaluation. 765 was the\ntotal number of the notesneeded tosatisfytwoconditions:1) Eachreviewer notbeingassigned\nmore than 50 notes to review, and 2) at least around 15 notes per reviewer being\ndouble-reviewed by another random reviewer. From among these 765 notes, ChatGPT\nencountered application programming interface (API) errors in 23 cases (3%). These errors\narose from “Azure content managementviolations'' [63](17cases),APItimeouts(5cases),and\nmaximum length limit errors (1 case). Supplementary Table S2 includes a more detailed\ndescription of these errors. The remaining 742 were considered for assignment to domain\nexpertreviewers.\nGenerative AI\nChatGPT (GPT-4,APIversion“2023-03-15-preview”) wasusedonthese765notestoextractall\ninstances of the cognitivetests—MMSEandCDR—alongwiththe datesatwhich the testswere\nmentioned to have been administered. The inference was successful for 742 notes. The\ncomplete API call, along with the exact prompt, the temperature, and other hyper-parameters\nare included in Supplementary Table S3. The promptincludedarequestto returnthese results\nin a JSON format. ChatGPT’s response(full),aswell asthe JSONformatteddialogueresponse\nwere recorded in one session on June 9th 2023. The notes sent to ChatGPT were text-only,\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nstripped of the rich-text formatting (RTF) nativeto our EHRsystem(EpicSystems,Verona,WI).\nThis reduced token countbyapproximatelyten-fold,enablingnotestofitinto the GPT4-8Kinput\nwindow and removing a substantial source of confusion for the LLM inprompttuning.Thedate\nthat the encounter wasrecordedin Epicwasappended atthebeginningofthenote,proceeding\nwith a column (“:”) then the note text. See Supplementary Table S3 for the API request,\nincludingtheprompt.\nHyper-parameter andPrompt Tuning\nWe assigned 20 notes out of the 742 as our hyper-parameter and prompt tuning set. An\ninteractive cloud-based environment (i.e playground) was utilized initially to fine-tune the\nprompt. After initial exploratory analysis using these 20 notes, they were scored via the API\nusing the best prompt and hyper-parameter found in the interactive mode. All human expert\nreviewers (detailed below) were instructed to first review the results of the 20 cases in a\nRedCap survey. The goal of this step was to train the reviewers, refine the information\npresented in RedCap, improve clarification of the questions, and potentially refine the prompt.\nThese20noteswerethenexcludedfromanyadditional analysis.\nHumanExpert Reviewers\nOur team included 22 medically trained expert reviewers who volunteered and were trained to\nreview a (HTML formatted) note, provide ground truth, and judge the correctness and\ncompleteness of ChatGPT answers for each cognitive test. Fully (HTML) formatted notes were\npulled using an Epic web service, and were fed into the RedCap survey. Redcap survey\nrendered the note’s HTML formatting, to ensure notes could be displayed to users in thesame\nformat as the readers are accustomed to seeing them clinically, rather than the text-only,\ncomputer-friendly format provided to GPT. For 21 of these reviewers, each reviewer was\nassigned approximately 50 clinical notes to evaluate.Fromamongeachreviewer’s50assigned\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nnotes, about 15 notes were assigned to another random reviewer. The assignment algorithm\nrandomly selected a pair of reviewers for each of our 309 double-reviewednotesandassigned\ntheremainingnotestoarandomlyselected reviewer until eachreviewer reached50notesor we\nfully assignedall notes.Thisrandomassignmentwasanecessarystepfor ensuringcorrectness\nof Fleiss’ Kappa [64] metric for inter-rater-agreement. As aresult,therewasaslightvariationin\nthetotal number ofassignednotesfor eachreviewer.\nOverall, 722 notes were assigned to these 21 reviewers, of which 309 were double-reviewed\nand 413 were solo-reviewed. The double-reviewed 309 notes were utilized in reporting\ninter-rater-agreement metrics. After the review, 69 out of 309 notes had at least one\ndisagreement between the two reviewers based on one of the four questions: Whether\nChatGPT's response on MMSE was correct; whether ChatGPT's response on MMSE included\nall instances of MMSE found in the clinical note; whether ChatGPT's response on CDR was\ncorrect; and whether ChatGPT's response on CDR included all instances of CDR found in the\nclinical note. A 22nd reviewer was then tasked toreviewthese69notesagaintoprovideathird\nreview. Majority vote was then employed to identify the final answer and the ground truth\nprovided by the reviewer whose answer was in the majorityvotewasusedtocalculatedetailed\nprecision/recall metrics. When both reviewers fully agreed and their JSON results were both\nvalidfor analysis,werandomlyselectedonetocompute theprecisionandrecall.\nParsingtheJSONresults\nChatGPT was instructed to provide answersinJSONformat.Thehumanexpertreviewerswere\ninstructed to provide ground truth data also in JSON format. To facilitate thereviewprocessfor\nexpert reviewers and reduce errors due to word choices,humanreviewerswererecommended\nto start from ChatGPT’s answer and correct the incorrect parts. Both results were then parsed\nautomatically and turned into a list of (score, date) tuples, (four lists of tuples per note: 1)\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nChatGPT’s MMSE, 2) ChatGPT’s CDR, 3) reviewer’s MMSE, 4) reviewer’s CDR.) MMSE and\nCDR scores were analyzed individually. To normalize the JSON entries, any entry\ncorresponding to the “score” which had obvious wrong test types (i.e “MoCA”, “GDS”) was\nautomaticallyexcluded.\nStatisticalApproach\nFor double-reviewed notes, we reported Fleiss’ Kappa [64] as a measure for\ninter-rater-agreement. We reported this metric for the four questions (Is MMSE\ncomplete/correct, and is CDR complete/correct). Additionally, for double-reviewed notes, we\ncomputed a 2-way Fleiss Kappa for MMSE and CDR lists of (outcome and date) tuples\nextracted from the JSON responses ofexpertreviewers,comparingthemagainsteachother,to\nderive inter-rater-agreement. Fleiss’ Kappa is useful when the assignmentofanotetoreviewer\npairs has been random (uniform), and each note has been reviewed by a subset of reviewers\n[65] [66]. We only considered exact matches (i.e MMSE-27/30 with MMSE-26/30 is justasbad\nas MMSE-5/30). Kappa can be interpreted as follows: 40%–59% would beWeak, 60%–79%\nwould beModerate, 80%–90% would be Strong, and Above90%wouldbeAlmost Perfect [65].\nIn addition to 2-way Kappa, we also report a 3-way Kappa on the entries of MMSE and CDR\nresults extracted from the JSON results, computing the joint agreement between the results of\nChatGPTandthe resultsprovidedbytwohumanreviewers.\nWe also report per test type (MMSE and CDR), Accuracy, True and False Negative Rates,\nMicro- and Macro-Precision and Micro- and Macro-Recall. Accuracy is defined as the\npercentage of correct results (at clinical note level), correct being defined as the list of\n(Value/Date) tuples in the JSON entries for ChatGPT and Ground Truth being fully identical.\nMacro-Precision for MMSE (or CDR) is the average (at the note level) of percentageofcorrect\nMMSE (or CDR) tuples extracted (correct both in date and score values compared to an entry\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nmentioned in the ground truth for MMSE (or CDR)). Macro-Recall for MMSE (or CDR) is the\naverage (at the note level) of the percentage of the MMSE items in the ground truth that are\nextracted by the ChatGPT.Micro-precision is calculated as percentage ofcorrect MMSE (or\nCDR) items extracted by ChatGPT, from among all extracted MMSE (or CDR) items by\nChatGPT and is calculated as one number across all notes combining all notes’ entries.\nMicro-recall is similarly calculated as the percentage of all MMSE (or CDR) itemsmentionedin\nthegroundtruththatwere extractedbyChatGPT.\nResults\nChatGPT analyzed 765 notes for extraction of Mini Mental StatusExam(MMSE) andCognitive\nDementia Rating (CDR) scores and exam dates. Of these, 23encounteredAPIerror (3%),and\n20 were used to fine-tune prompt and hyper-parameters. The remaining 722 notes were\nassigned to human expert reviewers who manually reviewed (and provided ground truth for)\nthese notes. The characteristics of these 722 notes and associated patients are included in\nTable1.\nTable 1: Characteristics of 722 notes which are manually evaluated, and their\ncorrespondingpatients\nFeature\nAllnotes(N=722notes\nfrom458patients)\nDoublereviewed\nnotes(N=309notes\nfrom236patients)\nPatient demographics\nAgeattimeofnote(mean(sd)) 72.64(14.01) 73.68(14.01)\nGender\nFemale (%) 242(52.84%) 124(52.54%)\nMale(%) 216(47.16%) 112(47.46%)\nRace\nAsian 27(5.90%) 10(4.24%)\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nBlack 39(8.52%) 17(7.20%)\nWhite 334(72.93%) 178(75.42%)\nAmericanIndian 1(0.22%) 0(0.00%)\nUnknown 57(12.45%) 31(13.14%)\nNotecharacteristics\nDateranges(mintomax)\n2011/11/21to\n2023/05/10\n2011/11/21to\n2023/05/10\nLength(inwords) (mean(SD)) 8428.2(3822.3) 8306.2(3851.1)\nOpensourceBERT(prompttokens) 2167.3(1049.5) 2123.6(1023.5)\nChatGPT(PromptTokens) 2212.93 (1002.9) 2174.9(992.3)\nChatGPT(CompletionTokens) 64.3(49.6) 64.2(46.5)\nChatGPT(Total Tokens) 2277.3(1017.9) 2239.1(1005.0)\nOf the double-reviewed 309 notes, 69 hadatleastonedisagreementbetweentheresponsesto\nthe four questions (if ChatGPT’s response for MMSE/CDR is correct/complete) and were\nassigned to a new reviewer for a third opinion. Among the responses with disagreement, 9\ndisagreed about correctness of MMSE answers, 40 disagreed about completeness of MMSE\nanswers, 17 disagreed about correctness of CDR answers, and 22 disagreed about\ncompleteness of CDR answers. The average response (at the note level) by the included\nreviews for the four yes/no questions are included in Table 2. Overall reviewers considered\nChatGPT’s response to be 96.5% and 98% correct for MMSE and CDR respectively. The\nassessment for whether ChatGPT’s answers are also complete (i.e. they do notmissanything)\nwasslightlylower averagingabout84%and83%for MMSEandCDRrespectively.\nTable 2: Average response (at the note level) of the responses of reviewers in judging if\nChatGPT’sanswersfor MMSEandCDRarecorrect and/or complete.\nAllnotes(N=722)\nDoublereviewed\nnotes(N=309)\nIsChatGPT’sanswer for MMSEcorrect? 96.5%(sd18.2) 96.4%(sd18.5)\nIsChatGPT’sanswer for MMSEcomplete? 85.0%(sd35.7) 84.7%(sd36.0)\nIsChatGPT’sanswer for CDRcorrect? 98.0%(sd13.7) 99.6%(sd5.6)\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nIsChatGPT’sanswer for CDRcomplete? 80.4%(sd39.6) 83.4%(sd37.1)\nThe inter-rater-agreements between reviewers were calculated basedonFleiss’ Kappaandare\nsummarized in Table 3. In addition to measuring Fleiss’ Kappa between reviewers based on\ndouble-reviewed notes (reported as 2-way Fleiss’ Kappa in Table 3), we also reportagreement\nbetween ChatGPT, and the two human reviewers (reported as 3-way Fleiss’ Kappa inTable 3).\nThe2-wayagreementontheyes/noquestionswashigh (94%agreementbetweenreviewersfor\nMMSE and 89% agreement for CDR). There was some disagreement in judging the\ncompleteness of the answer, leading to a Kappa value of 75% for MMSE (and 85% for CDR).\nMorenotably,when analyzingtheelementsofthe groundtruthJSON,the2-wayagreementwas\nexcellent both for scores (83% for MMSE and 80%for CDR) andfor dates(93%for MMSEand\n79% for CDR). When measuring the 3-way agreement, there was anincreaseinall themetrics\nexceptMMSEdates.\nTable 3: Fleiss’ kappa inter-rater-agreement metric between reviewers (2-way) and\nreviewersandChatGPT(3-way) over thedouble-reviewednotes.\n2-wayFleiss’ kappa(Among\nhuman reviewers)\nOnN=309 double-reviewed\nnotes,n=21reviewers\n3-wayFleiss’ Kappa\n(betweenChatGPTand two\nhuman reviewers)\nOnN=309 double-reviewed\nnotes,n=21reviewers\nBinaryQuestions\nIsMMSElistgenerated by\nChatGPTcorrect?\n94.2% NA\nIsMMSElistgenerated by\nChatGPTcomplete?\n75.2% NA\nIsCDRlistgeneratedby\nChatGPTcorrect?\n89.0% NA\nIsCDRlistgeneratedby\nChatGPTcomplete?\n85.8% NA\nIndividual(value/date) tuplesfromChatGPTandGround-TruthJSONresults.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nMMSEvalues(ofthescores\ninthe note)\n83.6% 93.7%\nMMSEdates(ofthescoresin\nthenote)\n93.3% 87.2%\nCDRvalues(ofthescoresin\nthenote)\n80.5% 87.0%\nCDRdates(ofthescoresin\nthenote)\n79.0% 82.5%\nJSONresponses\nTwo (out of 722) responses from ChatGPT were not in correct JSON format and failed\nautomatic parsing. These two cases were both reporting “noMMSEor CDR.” Another 26ofthe\nremaining 722 ChatGPT JSON results had “MMSE_Scores” and “CDR_Scores” as the JSON\nentry, while the rest had “MMSE” and “CDR” as the entries. The JSON results of 79 noteshad\nadditional entries, primarily including other cognitive tests (47 “MoCA” and 20 “GDS” being the\nmost frequent wrong tests). These additional entries in JSON results of both ChatGPT and\nhuman reviewers were automatically excluded from analysis. Of all 1031 (309×2+413) human\nexpert responses, 131 were not in correct JSON format andfailedautomaticparsing.Most,92,\nof those were missing a single formatting character (e.g. comma, bracket) and were manually\ncorrected. The remaining 39 were long entries not provided in JSON format that would have\nrequired substantial edits and were consequently excluded from analysis. As aresult,12ofthe\nnotes (3 coming from the double-reviewed notes) were excluded from further analysis. In the\nremaining 710 notes (306 of which were double-reviewed), all parsed ground truth and\nChatGPT instances of MMSE and CDR tests were analyzed for computingaccuracy,precision,\nand recall. ChatGPT’s (micro- and macro-) precision and recall for MMSE and CDR items are\nincludedinTable4.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nTable 4: Aggregate Accuracy, True Negative Rate, (Micro- and Macro-) Precision and\nRecallfor MMSEandCDRscoresextractedbyChatGPT.\nAllnoteswith\nparsedJSON\n(N=710)\nDouble-reviewed\nnoteswithparsed\nJSON\n(N=306)\nMMSE\nTotal noteswithoutanyMMSE(ingroundtruth) 115 48\nTotal noteswithoutanyMMSE(inGPTresults) 77 25\nTotal correctlypredicted emptyMMSEs 76 24\nChatGPT’sTrueNegativeRatefor MMSE 98.7% 96%\nChatGPT’s FalseNegative Ratefor MMSE 1.2% 4%\nRemainingnoteswithun-emptyGPTresponse\nundergonePrecision/Recall calculationfor\nMMSE 633 281\nTotal MMSEinstancespredicted 831 366\nMMSEMacroPrecision (mean%(sd%)) 82.9%(sd36.2) 82.7%(sd36.8)\nMMSEMacroRecall (mean%(sd%)) 87.8%(sd30.4) 89.7%(sd28.3)\nMMSEMicroPrecision 83.8% 84.1%\nMMSEMicroRecall 83.7% 87.5%\nTotal noteswithanyerror MMSEresult 121 52\nOverall accuracyofMMSE 82.9% 83.0%\nCDR\nTotal noteswithoutCDR(inground truth) 608 260\nTotal noteswithoutCDR(inGPTresults) 533 233\nTotal correctlypredicted emptyCDR 532 233\nCDRTrueNegativeRate 99.8% 100%\nCDRFalse NegativeRate 0.2% 0%\nRemainingnoteswithun-emptyGPTresponse\nundergonePrecision/Recall calculationfor\nCDR 177 73\nTotal CDRinstancespredicted 256 92\nCDRMacroPrecision (mean%sd%) 48.3%(sd49.9) 57.5%(sd49.4)\nCDRMacroRecall (mean%sd%) 84.3%(sd36.3) 91.3%(sd28.1)\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nCDRMicroPrecision 36.3% 51.0%\nCDRMicroRecall 85.3% 92.1%\nTotal noteswithanyerror CDRresult 91 31\nOverall accuracyofCDR 87.1% 89.8%\nChatGPT had an excellent True Negative Rate—over 96% for MMSE and 100% for CDR in\ndouble-reviewed notes. Both results had high recall (sensitivity), reaching 89.7% for MMSE\n(macro-recall) and 91.3% for CDR (macro-recall). MMSE wasmorefrequentlymentionedinthe\nnotes and ChatGPT’s macro precision (PPV) was 82.7%. CDR, on the other hand, was less\nfrequent, and we observed that ChatGPT hallucinates (factitiously generates) results\noccasionally leading to a macro precision of only 57.5%. We analyzed 31 notes from the\ndouble-reviewed notes where ChatGPT’s predictions for CDR had any incorrect elements in\nthem. In 3 notes, there was no sign of a CDR score and the result washallucinatedwithoutan\nobvious reason. In 4 cases, the date was wrong but the score was correct. The remaining 24\nnotes’ errors were all due to GPT mistakenly reporting results of another test (e.g. GDS, CD4,\nand Cup/Disk Ratio, alsoabbreviatedCDR) insteadofCDR.Wesimilarlyanalyzedthe52 notes\nwhich had any errors for the MMSE predicted list.Thesenotes’ errorsincluded19withanerror\nonly in the reported date while the score was correct; 17 cases ofwrong testscores(i.emostly\nMoCA, only 2 MiniCog and 1 MSK cases) being picked instead of MMSE; 4 cases of\ninconsistency in the note itself; 2 typos (one case where the note said “AOx3” for the score,\nanother where MMSE was writtenas“MMSE”) accordingtothereviewer comments;3casesof\nhallucinations without specific reasons; 4 casesofnon-numericMMSEvalues(i.e.“deferred'' or\n“preserved”); 1 case of reporting partial MMSE scores instead of the total MMSEscore;1case\nwas missed from a table that listed 7 MMSE score/dates that were easy to see in the HTML\nversionofthenote butconfusingintext-onlyversion;andfinally,1casewasactuallycorrectand\nthe error was due to not considering “not found”' equal to an empty list. Taking positive and\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nnegative results into account, overall the MMSE and CDR results were 83%and89%accurate\naccordingtothedouble-reviewednotes.\nDiscussion\nIn this study, our primary objective was to evaluate the performance of ChatGPT, a large-scale\nlanguage model, in extracting information from clinical notes, specifically focusing on cognitive\ntestssuchastheMini-Mental StateExamination(MMSE) and Clinical DementiaRating(CDR).\nTo achieve this, we tasked ChatGPT with extracting MMSE and CDR scores and their\ncorresponding datesfromclinical notes.Weemployed22humanexpertreviewerstoassessthe\naccuracy and completeness of ChatGPT's responses on 722 clinical notes. Through our\nevaluation, we measured accuracy, true negative rate and (micro- and macro-) precision and\nrecall of ChatGPT's extraction results for MMSE and CDR items. We also analyzed the\nfeedback provided by the clinical reviewers while considering inter-rater-agreement using\nstatistical techniques.\nOur results revealed that ChatGPT successfully extracted relevant information for MMSE and\nCDR scores, as well as their associated dates, with high recall, capturing nearly all of the\npertinent details present in the clinical notes. The overall accuracy of theinformationextraction\nresults for MMSEand CDRwere83%and89%respectively.Theextractionwashighlysensitive\n(e.g. Macro-recall of 89.7% for MMSE and 91.3% for CDR) and had outstanding\ntrue-negative-rates (96% for MMSE and 100% for CDR). The precision of the extracted\ninformation was also high for MMSE (82.7%) although in the case of CDR, we observed that\nChatGPT occasionally mistook other tests for CDR and therefore the precision was only 57%.\nBased on the ground-truth provided by our reviewers, 89.1% of the notes included an MMSE\ndocumentation instance, whereas only 14.3% of the notes included a CDR documentation\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \ninstance. This, combined with our analysis of the errors, explain lower precision in the CDR\ncase, and suggest combining ChatGPT with basic NLP preprocessing may improve the LLM\nperformance further. The substantial inter-rater-agreement among our expert reviewers further\nsupported the robustness and validity of our findings, and thereviewersconsideredChatGPT’s\nresponses correct (96% and 98% of reviewers judgingChatGPT’sresponsescorrectfor MMSE\nand CDR respectively) andcomplete(84%and83%ofreviewersjudgingChatGPT’sresponses\ncompletefor MMSEandCDRrespectively.)\nThe findings of our study demonstrate that large language models, such as ChatGPT, offer a\npromising solution for extracting valuable clinical information from unstructured notes. This\napproach provides a more efficient and scalable approach compared to previous methods that\neither rely on rigid rule-based systems that struggle to capture the necessary contextual\ninformation or involve training models on text samples where humans have painstakingly\nextracted the relevant information. This capability is expected to further improve with OpenAI’s\nintroduction of “function calling” [67], which ensures the output of correctJSONfromtheir LLM.\nLLMs can be effortlessly applied to enhance the value of clinical data for research, enable\nharmonization with disease registries and biobanks, improve outreach programs within health\ncenters, andcontributetotheadvancementofprecisionmedicine.Additionally,theavailabilityof\nlarge labeled datasets resulting from this information extraction process can also enable AI\nmodelstobe trainedfor awidevarietyoftasks.\nFurthermore, our findings have implications for future AD/ADRD research. Currently, the\nmajority of research in scalable development and validation of AI tools for early AD/ADRD\ndetection rely on research cohorts. These cohorts are overwhelmingly white (NACC cohort is\n83% white [68]ADNIcohortis92%white [62],anddonotrepresenttrueat-riskpopulationswho\ntend to have higher comorbid disease burden [50]. Due to late detection and diagnosis of\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nAD/ADRD [46] [47] [48] [49], clinical data often lacked the details necessary for accurate case\nidentification (i.e. structured data such as ICD codes would yield low sensitivities). Using LLMs\nto extract data from clinical notes hasthepotential to improvethe qualityofclinical data,paving\nthe way for clinical validation and development of clinically applicable novel AI tools and\nperformingcognitive-healthprecision medicineatscale.\nIn a broader scope, validation and use of LLMs on unstructured Electronic Health Records\n(EHRs) have much larger implications. EHRs provide an untapped resource for data-driven\nknowledge discovery and biomedical research. Active research areas, from causal studies on\nobservational data [69] [70] [71] [72], to disease prognosis [73] [74] [75] [76] [77] [78] [79] [80]\n[81] [82] [83], disease/genomics understanding [84] [85] [86] [87] [88], treatment response\nmonitoring [89] [90] [91] [92], and fairness and bias in medicine [93] [94] [95] [96] have been\nusing and deriving insight from EHR data. Yet, various studies show that the structured EHR\ndata needs to be complemented bytheinformation inunstructuredclinical notes,and structured\nEHR data alone do not contain the complete healthcare data captured in clinics [97] [98] [99]\n[100] [37]. Linguistically motivated (often rule-based) information extraction approaches\nemerged to augment EHRs with information from free text [24,25] [26] [27] [28] [29] [30], but\nthese approaches were not sensitive enough [37] [38] [39] [40]. Standard machine- and\ndeep-learning-based approaches emerged to improve the sensitivity and precision of\ninformation extracted [31] [32] [33] [34] [35], but these approaches were task-specific and\nrequired substantial manual labor to develop and validate tools. With the introduction of LLMs\nwhich enable “zero-shot” learning (i.e. prompt-based inference on a massively trained\ngenerative AI model), the ability to extract diverse information without any task-specific model\npre-training emerged. This creates immense potential to curate high-qualitydatasetsfromEHR\ndata. However, many LLMs have not been developed for healthcaresettings,andareknownto\nhave the ability to hallucinate. Accordingly, we need to rigorously evaluate them in various\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nclinical use cases before adopting them. This study addresses the validation need for two\nspecific cognitive tests (namely MMSE and CDR) and is the beginning of a series of works in\nbenchmarking LLMs for medical information extraction. We also acknowledge that given the\nvariability in LLM sensitivity and specificity, further workisneededto combine theseinformation\nwith other clinical evidence and/or predictive models, toensure andthatLLMsdon’texacerbate\ninequities, as racial and ethnic minoritized individuals are less likely to have the MMSE and\nCDRusedhereperformedonthem.\nWhen considering the implications of this study, it is important to acknowledge its limitations.\nOur focus was on evaluating theinformationextractioncapabilitiesofthecurrentstateoftheart\nof LLMs, specifically ChatGPT powered by GPT-4, rather than comparing it to other LLMs or\nNLP methods. Webelievethatour resultsmaybeenhancedwithbetter promptengineeringand\ncombining LLMs with standard NLP. Additionally, weconductedalarge-scalehumanevaluation\nfor a single dementia use case, prioritizing result reliability over assessing various clinical\nscenarios. It is also importanttonotethatour findingspertain specificallytoinformationretrieval\nfrom clinical notes and do not predict how ChatGPT will perform on medical tasks requiring\ndiagnosis,treatmentrecommendation,or summarization.\nOverall our study findings indicate that large language models such as ChatGPT have the\npotential to enhance the extraction of valuable clinical information from unstructured notes,\nbenefiting research, disease registries, biobanks, and precision medicine. The use of LLMs on\nunstructured electronic health records (EHRs) also holds broader implications for knowledge\ndiscovery, biomedical research, and addressing gaps in structured EHR data. Nevertheless,\nrigorous evaluation and consideration of limitations are crucial, and further work is needed to\ncombine LLM information with other clinical evidence to avoid exacerbating inequities and\nensure equitablehealthcaredelivery.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nReferences\n1. OpenAI.ChatGPT.2023[cited3Jul 2023].Available:http://openai.com/chatgpt(accessed\nJune2023)\n2. OpenAI.GPT-4Technical Report.arXiv[cs.CL].2023.Available:\nhttp://arxiv.org/abs/2303.08774\n3. Singhal K,TuT,GottweisJ,SayresR,WulczynE,Hou L,etal.TowardsExpert-Level\nMedical QuestionAnsweringwithLargeLanguageModels.arXiv[cs.CL].2023.Available:\nhttp://arxiv.org/abs/2305.09617\n4. TouvronH,Lavril T,IzacardG,MartinetX,LachauxM-A,LacroixT,etal.LLaMA:Open and\nEfficientFoundationLanguageModels.arXiv[cs.CL].2023.Available:\nhttp://arxiv.org/abs/2302.13971\n5. BubeckS,ChandrasekaranV,EldanR,GehrkeJ,HorvitzE,Kamar E,etal.Sparksof\nArtificial General Intelligence:EarlyexperimentswithGPT-4.arXiv[cs.CL].2023.Available:\nhttp://arxiv.org/abs/2303.12712\n6. Nori H,KingN,McKinneySM,CarignanD,HorvitzE.CapabilitiesofGPT-4onMedical\nChallengeProblems.arXiv[cs.CL].2023.Available:http://arxiv.org/abs/2303.13375\n7. LeeP,BubeckS,PetroJ.Benefits,Limits,andRisksofGPT-4asanAIChatbotfor\nMedicine.NEngl JMed.2023;388:1233–1239.\n8. KungTH,CheathamM,MedenillaA,SillosC,De LeonL,ElepañoC,etal.Performanceof\nChatGPTonUSMLE:Potential for AI-assisted medical educationusinglargelanguage\nmodels.PLOSDigitHealth.2023;2:e0000198.\n9. GiannosP.EvaluatingthelimitsofAIinmedical specialisation:ChatGPT’sperformance on\ntheUKNeurologySpecialtyCertificateExamination.BMJNeurologyOpen.2023;5.\ndoi:10.1136/bmjno-2023-000451\n10. MatiasY.Our latesthealthAIresearchupdates.In:Google[Internet].14 Mar 2023[cited3\nJul 2023].Available:\nhttps://blog.google/technology/health/ai-llm-medpalm-research-thecheckup/\n11. SarrajuA,Bruemmer D,VanItersonE,ChoL,RodriguezF,LaffinL.Appropriatenessof\nCardiovascular DiseasePreventionRecommendationsObtainedFromaPopular Online\nChat-BasedArtificial IntelligenceModel.JAMA.2023;329:842–844.\n12. Haver HL,Ambinder EB,Bahl M,Oluyemi ET,JeudyJ,Yi PH.AppropriatenessofBreast\nCancer PreventionandScreeningRecommendationsProvidedbyChatGPT.Radiology.\n2023;307:e230424.\n13. LeeT-C,Staller K,BotomanV,Pathipati MP,VarmaS,Kuo B.ChatGPTAnswersCommon\nPatientQuestionsAboutColonoscopy.Gastroenterology.2023.\ndoi:10.1053/j.gastro.2023.04.033\n14. AyersJW,PoliakA,DredzeM,LeasEC,Zhu Z,KelleyJB,etal.Comparing physician and\nartificial intelligencechatbotresponsestopatientquestionspostedtoapublicsocial media\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nforum.JAMAInternMed.2023;183:589–596.\n15. DashD,ThapaR,BandaJM,SwaminathanA,CheathamM,KashyapM,etal.Evaluation\nofGPT-3.5andGPT-4for supportingreal-worldinformationneedsinhealthcaredelivery.\narXiv[cs.AI].2023.Available:http://arxiv.org/abs/2304.13714\n16. CascellaM,Montomoli J,Bellini V,Bignami E.EvaluatingtheFeasibilityofChatGPTin\nHealthcare:AnAnalysisofMultipleClinical andResearchScenarios.JMedSyst.2023;47:\n33.\n17. KooM.TheImportanceofProper UseofChatGPTinMedical Writing.Radiology.2023;307:\ne230312.\n18. Stokel-Walker C.ChatGPTlistedasauthor onresearchpapers:manyscientists\ndisapprove.In:NaturePublishingGroup UK[Internet].18Jan2023[cited 4Jul 2023].\ndoi:10.1038/d41586-023-00107-z\n19. ThorpHH.ChatGPTisfun,butnotanauthor.Science.2023;379:313–313.\n20. Nature.Authorship.In:NatureAuthorship[Internet].Springer Nature;2023[cited4Jul\n2023].Available:https://www.nature.com/nature/editorial-policies/authorship\n21. JAMA.Instructionsfor Authors.In:JAMAAuthorship Guidelines[Internet].4Jul 2023[cited\n4Jul 2023].Available:\nhttps://jamanetwork.com/journals/jama/pages/instructions-for-authors\n22. Hosseini M,RasmussenLM,ResnikDB.UsingAIto write scholarlypublications.Account\nRes.2023;1–9.\n23. ParkD.OpenLLMLeaderboard.In:OpenLLMLeaderboard[Internet].4Jul 2023[cited4\nJul 2023].Available:https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n24. AronsonAR,LangF-M.An overviewofMetaMap:historical perspectiveandrecent\nadvances.JAmMedInformAssoc.2010;17:229–236.\n25. Soysal E,WangJ,JiangM,WuY,PakhomovS,LiuH,etal.CLAMP--atoolkitfor efficiently\nbuildingcustomizedclinical natural languageprocessingpipelines.JAmMedInformAssoc.\n2018;25:331–336.\n26. WuH,Toti G,MorleyKI,IbrahimZM,FolarinA,JacksonR,etal.SemEHR:A\ngeneral-purposesemanticsearchsystemtosurfacesemanticdatafromclinical notesfor\ntailoredcare,trial recruitment,andclinical research.JAmMedInformAssoc.2018;25:\n530–537.\n27. SavovaGK,MasanzJJ,Ogren PV,ZhengJ,SohnS,Kipper-Schuler KC,etal.Mayo\nclinical TextAnalysisandKnowledgeExtractionSystem(cTAKES):architecture,component\nevaluationandapplications.JAmMedInformAssoc.2010;17:507–513.\n28. Chapman WW,Bridewell W,HanburyP,Cooper GF,BuchananBG.ASimpleAlgorithmfor\nIdentifyingNegatedFindingsandDiseasesinDischargeSummaries.JBiomedInform.\n2001;34:301–310.\n29. WangX,Peng Y,LuL,LuZ,Bagheri M,SummersRM.Chestx-ray8:Hospital-scalechest\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nx-raydatabaseandbenchmarksonweakly-supervisedclassification andlocalizationof\ncommonthoraxdiseases.ProceedingsoftheIEEEconferenceoncomputer vision and\npatternrecognition.2017.pp.2097–2106.\n30. IrvinJ,Rajpurkar P,KoM,YuY,Ciurea-IlcusS,ChuteC,etal.Chexpert:Alarge chest\nradiographdatasetwithuncertaintylabelsandexpertcomparison.arXivpreprintarXiv:1901\n07031.2019.Available:https://www.aaai.org/Papers/AAAI/2019/AAAI-IrvinJ.6537.pdf\n31. SmitA,JainS,Rajpurkar P,PareekA,NgAY,LungrenMP.CheXbert:Combining\nAutomaticLabelersandExpertAnnotationsfor AccurateRadiologyReportLabelingUsing\nBERT.arXiv[cs.CL].2020.Available:http://arxiv.org/abs/2004.09167\n32. McDermottMBA,HsuTMH,WengW-H,Ghassemi M,SzolovitsP.CheXpert++:\nApproximatingtheCheXpertlabeler for Speed,Differentiability,and ProbabilisticOutput.\narXiv[cs.LG].2020.Available:http://arxiv.org/abs/2006.15229\n33. LeGlazA,HaralambousY,Kim-Dufor D-H,LencaP,BillotR,RyanTC,etal.Machine\nLearningandNatural LanguageProcessing inMental Health:SystematicReview.JMed\nInternetRes.2021;23:e15708.\n34. WengW-H,Wagholikar KB,McCrayAT,SzolovitsP,ChuehHC.Medical subdomain\nclassificationofclinical notesusingamachinelearning-basednatural language processing\napproach.BMCMedInformDecisMak.2017;17:1–13.\n35. JiangLY,LiuXC,NejatianNP,Nasir-MoinM,Wang D,AbidinA,etal.Healthsystem-scale\nlanguagemodelsareall-purposepredictionengines.Nature.2023;1–6.\n36. Leiter RE,SantusE,JinZ,LeeKC,YusufovM,ChienI,etal.DeepNatural Language\nProcessingtoIdentifySymptomDocumentation inClinical Notesfor PatientsWithHeart\nFailureUndergoingCardiacResynchronization Therapy.JPainSymptomManage.\n2020;60:948–958.e3.\n37. Wei W-Q,TeixeiraPL,MoH,Cronin RM,Warner JL,DennyJC.Combiningbillingcodes,\nclinical notes,andmedicationsfromelectronichealthrecordsprovidessuperior\nphenotyping performance.JAmMed InformAssoc.2015;23:e20–e27.\n38. TaggartM,Chapman WW,SteinbergBA,Ruckel S,Pregenzer-Wenzler A,DuY,etal.\nComparisonof2Natural Language ProcessingMethodsfor Identification ofBleeding\nAmongCriticallyIll Patients.JAMANetwOpen.2018;1:e183451–e183451.\n39. WuY,DennyJC,TrentRosenbloomS,Miller RA,GiuseDA,XuH.Acomparativestudyof\ncurrentclinical natural languageprocessingsystemsonhandlingabbreviationsin discharge\nsummaries.AMIAAnnuSympProc.2012;2012:997.\n40. Fan Y,WenA,ShenF,Sohn S,LiuH,WangL.EvaluatingtheImpactofDictionaryUpdates\nonAutomaticAnnotationsBasedon Clinical NLPSystems.AMIASummitsTransl Sci Proc.\n2019;2019:714.\n41. Larochelle H,ErhanD,Bengio Y.Zero-datalearning ofnewtasks.Proceedingsofthe23rd\nnational conferenceonArtificial intelligence- Volume2.AAAIPress;2008.pp.646–651.\n42. Wei J,BosmaM,ZhaoVY,GuuK,YuAW,Lester B,etal.Finetunedlanguagemodelsare\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nzero-shotlearners.arXiv[cs.CL].2021.Available:https://research.google/pubs/pub51119/\n43. Rezaei M,Shahidi M.Zero-shotlearninganditsapplicationsfromautonomousvehiclesto\nCOVID-19diagnosis:Areview.Intelligence-BasedMedicine.2020;3-4:100005.\n44. Borji A.ACategorical ArchiveofChatGPTFailures.arXiv[cs.CL].2023.Available:\nhttp://arxiv.org/abs/2302.03494\n45. MaynezJ,NarayanS,BohnetB,McDonaldR.OnFaithfulnessandFactualityin\nAbstractive Summarization.Proceedingsofthe58thAnnual MeetingoftheAssociationfor\nComputational Linguistics.Online:Association for Computational Linguistics;2020.pp.\n1906–1919.\n46. TsoyE,Kiekhofer RE,Guterman EL,TeeBL,WindonCC,DorsmanKA,etal.Assessment\nofRacial/EthnicDisparitiesinTimelinessandComprehensivenessofDementiaDiagnosis\ninCalifornia.JAMANeurol.2021;78:657–665.\n47. LinP-J,DalyA,Olchanski N,Cohen JT,NeumannPJ,Faul JD,etal.Dementiadiagnosis\ndisparitiesbyraceandethnicity.AlzheimersDement.2020;16.doi:10.1002/alz.043183\n48. Saadi A,HimmelsteinDU,Woolhandler S,MejiaNI.Racial disparitiesinneurologichealth\ncareaccessandutilizationintheUnitedStates.Neurology.2017;88:2268–2275.\n49. DraboEF,BartholdD,JoyceG,FeridoP,ChangChui H,ZissimopoulosJ.Longitudinal\nanalysisofdementiadiagnosisand specialtycareamong raciallydiverseMedicare\nbeneficiaries.AlzheimersDement.2019;15:1402–1411.\n50. LivingstonG,HuntleyJ,SommerladA,AmesD,BallardC,BanerjeeS,etal.Dementia\nprevention,intervention,andcare:2020reportoftheLancetCommission.Lancet.\n2020;396:413–446.\n51. Harper LC.2022Alzheimer’sAssociationFactsand Figures.https.Available:\nhttps://www.cambridge.org/core/services/aop-cambridge-core/content/view/915A476B938D\n0AF39A218D34852AF645/9781009325189mem_205-207.pdf/resources.pdf\n52. USDeptofHealthand HumanServices.National PlantoAddressAlzheimer’sDisease:\n2020Update.2021[cited1Nov2021].Available:\nhttps://aspe.hhs.gov/reports/national-plan-address-alzheimers-disease-2020-update-0\n53. SPRINTMINDInvestigatorsfor theSPRINTResearchGroup,WilliamsonJD,Pajewski\nNM,AuchusAP,BryanRN,CheluneG,etal.EffectofIntensivevsStandard Blood\nPressureControl onProbableDementia:ARandomizedClinical Trial.JAMA.2019;321:\n553–561.\n54. PragmaticEvaluationofEventsAndBenefitsofLipid-loweringinOlder Adults- Full Text\nView- ClinicalTrials.Gov.[cited27 Oct2021].Available:\nhttps://clinicaltrials.gov/ct2/show/NCT04262206\n55. NIA.NIA-fundedactiveAlzheimer’sand relateddementiasclinical trialsandstudies.In:NIA\n[Internet].2021[cited20Apr 2021].Available:\nhttps://www.nia.nih.gov/research/ongoing-AD-trials\n56. Science.In:AAAS[Internet].[cited10Jul 2023].Available:\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nhttps://www.science.org/content/article/another-alzheimers-drug-flops-pivotal-clinical-trial\n57. DrugApproval Package:Aduhelm(aducanumab-avwa).[cited31Oct2021].Available:\nhttps://www.accessdata.fda.gov/drugsatfda_docs/nda/2021/761178Orig1s000TOC.cfm\n58. ManlyJJ,Glymour MM.WhattheAducanumabApproval RevealsAboutAlzheimer Disease\nResearch.JAMANeurol.2021.doi:10.1001/jamaneurol.2021.3404\n59. FolsteinMF,FolsteinSE,McHugh PR.Mini-Mental StateExamination.JPsychiatr Res.\n1975.doi:10.1037/t07757-000\n60. MorrisJC.TheClinical DementiaRating(CDR):Currentversionand scoring rules.\nNeurology.1993.pp.2412–2412.doi:10.1212/wnl.43.11.2412-a\n61. NasreddineZS,PhillipsNA,BédirianV,CharbonneauS,WhiteheadV,CollinI,etal.The\nMontreal CognitiveAssessment,MoCA:a briefscreening tool for mildcognitiveimpairment.\nJAmGeriatr Soc.2005;53:695–699.\n62. ADNI.2021[cited1Nov2021].Available:\nhttp://adni.loni.usc.edu/data-samples/adni-participant-demographic/\n63. AzureOpenAIServicecontentfiltering- AzureOpenAI.[cited10Jul 2023].Available:\nhttps://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/content-filter\n64. FleissJL.Measuringnominal scale agreementamong manyraters.Psychol Bull.1971;76:\n378–382.\n65. HallgrenKA.ComputingInter-Rater Reliabilityfor Observational Data:AnOverviewand\nTutorial.Tutor QuantMethodsPsychol.2012;8:23.\n66. Maxwell AE.CoefficientsofAgreementBetweenObserversandTheir Interpretation.Br J\nPsychiatry.1977;130:79–83.\n67. Functioncallingandother APIupdates.[cited7Jul 2023].Available:\nhttps://openai.com/blog/function-calling-and-other-api-updates\n68. BeeklyDL,RamosEM,vanBelle G,DeitrichW,ClarkAD,JackaME,etal.TheNational\nAlzheimer’sCoordinatingCenter (NACC) Database:anAlzheimer disease database.\nAlzheimer DisAssocDisord.2004;18:270–277.\n69. Cooper GF,Bahar I,Becich MJ,BenosPV,Berg J,EspinoJU,etal.Thecenter for causal\ndiscoveryofbiomedical knowledgefrombigdata.JAmMedInformAssoc.2015;22:\n1132–1136.\n70. KleinbergS,HripcsakG.Areviewofcausal inferencefor biomedical informatics.JBiomed\nInform.2011;44:1102–1112.\n71. JohnsonKW,GlicksbergBS,HodosRA,Shameer K,DudleyJT.Causal inferenceon\nelectronichealthrecordstoassessbloodpressuretreatmenttargets:anapplicationofthe\nparametricgformula.Biocomputing2018.WORLDSCIENTIFIC;2017.pp.180–191.\n72. SchulamP,SariaS.Reliabledecisionsupportusingcounterfactual models.AdvNeural Inf\nProcessSyst.2017;30.Available:\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \nhttps://proceedings.neurips.cc/paper/2017/hash/299a23a2291e2126b91d54f3601ec162-Ab\nstract.html\n73. RazavianN,Blecker S,SchmidtAM,Smith-McLallenA,NigamS,SontagD.\nPopulation-Level PredictionofType2 DiabetesFromClaimsDataandAnalysisofRisk\nFactors.BigData.2015;3:277–287.\n74. LiuJ,ZhangZ,RazavianN.DeepEHR:ChronicDiseasePredictionUsingMedical Notes.\narXiv[cs.LG].2018.Available:http://arxiv.org/abs/1808.04928\n75. RazavianN,MarcusJ,SontagD.Multi-taskpredictionofdiseaseonsetsfromlongitudinal\nlaboratorytests.MachineLearningfor Healthcare.2016.Available:\nhttp://www.jmlr.org/proceedings/papers/v56/Razavian16.pdf\n76. RazavianN,SontagD.Temporal Convolutional Neural Networksfor DiagnosisfromLab\nTests.arXiv[cs.LG].2015.Available:http://arxiv.org/abs/1511.07938\n77. Hammond R,AthanasiadouR,CuradoS,AphinyanaphongsY,AbramsC,MessitoMJ,et\nal.Predictingchildhoodobesityusingelectronichealthrecordsand publiclyavailabledata.\nPLoSOne.2019;14:e0215571.\n78. TomaševN,GlorotX,Rae JW,Zielinski M,AskhamH,SaraivaA,etal.Aclinically\napplicableapproachtocontinuouspredictionoffutureacutekidneyinjury.Nature.\n2019;572:116–119.\n79. Bahadori MT,LiptonZC.Temporal-ClusteringInvarianceinIrregular Healthcare Time\nSeries.arXiv[cs.LG].2019.Available:http://arxiv.org/abs/1904.12206\n80. Topol EJ.High-performancemedicine:theconvergenceofhuman andartificial intelligence.\nNatMed.2019;25:44–56.\n81. Choi E,Bahadori MT,SongL,StewartWF.GRAM:graph-basedattentionmodel for\nhealthcarerepresentationlearning.Proceedingsofthe23rd.2017.Available:\nhttps://dl.acm.org/doi/abs/10.1145/3097983.3098126?casa_token=INfp-TEjFLEAAAAA:mr_\njWB7QVMoRDuT7fydn63JnSmADd1tA8U2cC5-WO6Fm-Og06vOM7X9NBIgxZxRbTqk81a\n8DG4Qt\n82. AlbersDJ,ElhadadN,ClaassenJ,PerotteR,GoldsteinA,HripcsakG.Estimating\nsummarystatisticsfor electronichealthrecordlaboratorydatafor useinhigh-throughput\nphenotyping algorithms.JBiomedInform.2018;78:87–101.\n83. WangT,QiuRG,YuM.PredictivemodelingoftheprogressionofAlzheimer’sdiseasewith\nrecurrentneural networks.Sci Rep.2018;8:1–12.\n84. DennyJC,RitchieMD,BasfordMA,PulleyJM,Bastarache L,Brown-GentryK,etal.\nPheWAS:demonstratingthefeasibilityofaphenome-widescantodiscover gene–disease\nassociations.Bioinformatics.2010;26:1205–1210.\n85. KohaneIS.Using electronichealth recordsto drivediscoveryindisease genomics.NatRev\nGenet.2011;12:417–428.\n86. KulloIJ,DingK,Jouni H,SmithCY,ChuteCG.AGenome-WideAssociationStudyofRed\nBloodCell TraitsUsingtheElectronicMedical Record.PLoSOne.2010;5:e13011.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint \n87. KulloIJ,FanJ,PathakJ,SavovaGK,Ali Z,ChuteCG.Leveraginginformaticsfor genetic\nstudies:useoftheelectronicmedical recordtoenableagenome-wideassociationstudyof\nperipheral arterial disease.JAmMedInformAssoc.2010;17:568–574.\n88. PolubriaginofFCG,Vanguri R,QuinniesK,BelbinGM,Yahi A,SalmasianH,etal.Disease\nheritabilityinferredfromfamilial relationshipsreportedin medical records.Cell.2018;173:\n1692–1704.e11.\n89. AnanthakrishnanAN,CaganA,Cai T,Gainer VS,ShawSY,SavovaG,etal.Identification\nofNonresponse toTreatmentUsing NarrativeDatainanElectronicHealthRecord\nInflammatoryBowel DiseaseCohort.InflammBowel Dis.2015;22:151–158.\n90. Schmittdiel JA,AdamsSR,Segal J,GriffinMR,RoumieCL,OhnsorgK,etal.Novel use\nandutilityofintegratedelectronichealth recordstoassessratesofprediabetesrecognition\nandtreatment:briefreportfroman integratedelectronichealthrecordspilotstudy.Diabetes\nCare.2014;37:565–568.\n91. PathakJ,Simon G,Li D,BiernackaJM,JenkinsGJ,ChuteCG,etal.Detecting\nAssociationsbetweenMajor DepressiveDisorder TreatmentandEssential Hypertension\nusingElectronicHealthRecords.AMIASummitsTransl Sci Proc.2014;2014:91.\n92. AbernethyAP,EtheredgeLM,GanzPA,WallaceP,GermanRR,Neti C,etal.\nRapid-LearningSystemfor Cancer Care.JClinOncol.2010;28:4268.\n93. ChenIY,SzolovitsP,Ghassemi M.CanAIHelpReduceDisparitiesinGeneral Medical and\nMental HealthCare?AMAJournal ofEthics.2019;21:167–179.\n94. PivovarovR,AlbersDJ,SepulvedaJL,Elhadad N.Identifyingand mitigatingbiasesinEHR\nlaboratorytests.JBiomedInform.2014;51:24–34.\n95. RusanovA,WeiskopfNG,WangS,WengC.Hiddeninplainsight:biastowardssick\npatientswhensamplingpatientswithsufficientelectronichealthrecorddata for research.\nBMCMedInformDecisMak.2014;14:1–9.\n96. GijsbertsCM,GroenewegenKA,Hoefer IE,EijkemansMJC,AsselbergsFW,AndersonTJ,\netal.Race/EthnicDifferencesintheAssociationsoftheFraminghamRiskFactorswith\nCarotidIMTandCardiovascular Events.PLoSOne.2015;10:e0132321.\n97. HorskyJ,Drucker EA,Ramelson HZ.AccuracyandCompletenessofClinical CodingUsing\nICD-10for AmbulatoryVisits.AMIAAnnuSympProc.2017;2017:912.\n98. BurnsEM,RigbyE,MamidannaR,BottleA,AylinP,ZiprinP,etal.Systematicreviewof\ndischargecodingaccuracy.JPublicHealth.2011;34:138–148.\n99. MountcastleSB,JoyceAR,Sasinowski M,Costello N,Doshi S,Zedler BK.Validation ofan\nadministrativeclaimscodingalgorithmfor seriousopioid overdose:Amedical chartreview.\nPharmacoepidemiol DrugSaf.2019;28:1422–1428.\n100.JuniatV,Athwal S,KhandwalaM.Clinical codinganddataqualityin oculoplastic\nprocedures.Eye.2019;33:1733–1740.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 12, 2023. ; https://doi.org/10.1101/2023.07.10.23292373doi: medRxiv preprint ",
  "topic": "Recall",
  "concepts": [
    {
      "name": "Recall",
      "score": 0.5318448543548584
    },
    {
      "name": "Cognitive impairment",
      "score": 0.4984457492828369
    },
    {
      "name": "Kappa",
      "score": 0.4916556477546692
    },
    {
      "name": "Cognition",
      "score": 0.44168710708618164
    },
    {
      "name": "Statistics",
      "score": 0.39455151557922363
    },
    {
      "name": "Psychology",
      "score": 0.3643781542778015
    },
    {
      "name": "Audiology",
      "score": 0.3384120762348175
    },
    {
      "name": "Medicine",
      "score": 0.32161790132522583
    },
    {
      "name": "Mathematics",
      "score": 0.24175360798835754
    },
    {
      "name": "Cognitive psychology",
      "score": 0.17163234949111938
    },
    {
      "name": "Psychiatry",
      "score": 0.13521111011505127
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2802154762",
      "name": "KES College",
      "country": "CY"
    },
    {
      "id": "https://openalex.org/I4210130527",
      "name": "Meyer (China)",
      "country": "HK"
    }
  ]
}