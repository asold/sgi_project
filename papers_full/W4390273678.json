{
  "title": "Turning Large Language Models into AI Assistants for Startups Using Prompt Patterns",
  "url": "https://openalex.org/W4390273678",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Wang, Xiaofeng",
      "affiliations": [
        "Free University of Bozen-Bolzano"
      ]
    },
    {
      "id": null,
      "name": "Attal, MI",
      "affiliations": [
        "Free University of Bozen-Bolzano"
      ]
    },
    {
      "id": null,
      "name": "Rafiq, Usman",
      "affiliations": [
        "Free University of Bozen-Bolzano"
      ]
    },
    {
      "id": null,
      "name": "Hubner-Benz, S",
      "affiliations": [
        "Free University of Bozen-Bolzano"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4361293442",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W6600164255",
    "https://openalex.org/W217287750",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W4360620450"
  ],
  "abstract": "Abstract Most startups operate with limited resources and experience. AI technologies enable them to accomplish many tasks under these constraints. The recent advance of large language models (LLMs) offers new opportunities to support startup endeavors. Given the nascent nature of LLMs, how they could be utilized to support startups is yet to be investigated. Since prompt engineering is believed to be at the core of the effective use of LLMs, we aim to understand how to apply prompt engineering to turn LLMs into AI assistants for startups. As the first step, we investigated the application of a set of prompt patterns to ChatGPT, arguably the most widely known LLM currently. The preliminary results show that some patterns are more suitable for brainstorming which is a typical activity conducted by early-stage startups. Prompt-tuned questions may lead to more specific and more detailed responses, but it is not guaranteed. Meantime, human factors play an important role in the effective application of prompt patterns. Large-size and systematic studies are needed to apply the right patterns to different questions, taking into account the differences among startups in terms of their startup knowledge, domain knowledge, and their attitudes and behaviors towards LLMs.",
  "full_text": "Turning Large Language Models into AI\nAssistants for Startups Using Prompt\nPatterns\nXiaofeng Wang(B) , Mohammad Idris Attal , Usman Raﬁq ,\nand Sylvia Hubner-Benz\nFree University of Bozen-Bolzano, Bolzano 39100, Italy\n{xiaofeng.wang,mohammadidris.attal,urafiq,sylvia.hubner}@unibz.it\nAbstract. Most startups operate with limited resources and experience.\nAI technologies enable them to accomplish many tasks under these con-\nstraints. The recent advance of large language models (LLMs) oﬀers new\nopportunities to support startup endeavors. Given the nascent nature of\nLLMs, how they could be utilized to support startups is yet to be investi-\ngated. Since prompt engineering is believed to be at the core of the eﬀective\nuse of LLMs, we aim to understand how to apply prompt engineering to\nturn LLMs into AI assistants for startups. As the ﬁrst step, we investigated\nthe application of a set of prompt patterns to ChatGPT, arguably the most\nwidely known LLM currently. The preliminary results show that some pat-\nterns are more suitable for brainstorming which is a typical activity con-\nducted by early-stage startups. Prompt-tuned questions may lead to more\nspeciﬁc and more detailed responses, but it is not guaranteed. Meantime,\nhuman factors play an important role in the eﬀective application of prompt\npatterns. Large-size and systematic studies are needed to apply the right\npatterns to diﬀerent questions, taking into account the diﬀerences among\nstartups in terms of their startup knowledge, domain knowledge, and their\nattitudes and behaviors towards LLMs.\nKeywords: Startups\n· AI Assistant · Large Language Models ·\nPrompt Engineering · Prompt Pattern\n1 Introduction\nBuilding a startup is a challenging endeavor, and the failure rate is notoriously\nhigh1. Scarce resources and lack of knowledge and experience in startup processes\nare among the key reasons why startups fail. Support for startups exists in diﬀer-\nent forms, such as mentoring, incubation, and digital tools. With the emergence\nof Generative Artiﬁcial Intelligence (GAI), particularly Large Language Models\n(LLMs), startups now have more opportunities to receive assistance and develop\ntheir innovative ideas. However, given the nascent nature of LLMs, how they\ncould be utilized to support startups is yet to be investigated.\nThe emergent and enduring value of LLMs is fundamentally tied to eﬀective\nprompt engineering [1]. A prompt refers to a set of text instructions crafted to\n1 https://explodingtopics.com/blog/startup-failure-stats.\nc⃝ The Author(s) 2024\nP. Kruchten and P. Gregory (Eds.): XP 2022/2023 Workshops, LNBIP 489, pp. 192–200, 2024.\nhttps://doi.org/10.1007/978-3-031-48550-3\n_19\nAI Assistant for Startups 193\nprogram and customize LLMs for the desired interaction. It provides a scope\nor context for an LLM to act on [ 2]. In turn, prompt engineering is the means\nby which LLMs are programmed via prompts [3]. Prompt engineering skills are\nvital for fully leveraging LLMs, but they do not come naturally and need to be\nlearned. This can pose a challenge to startup teams, who often operate on tight\nresources and have many other critical tasks to attend to [4]. We aim to facilitate\nstartups in utilizing LLMs as AI assistants through prompt engineering. To this\nend, we propose the research question:How to apply prompt engineering to turn\nlarge language models into AI assistants for startups?\nAs the ﬁrst step to answer the research question, we investigated prompt\npatterns which are summaries of eﬀective prompt-tuning techniques. They pro-\nvide a codiﬁed approach to customizing the input and output of LLMs as well\nas interactions with them. Application of prompt patterns may prove beneﬁcial\nfor startups in maximizing the potential of LLMs. We evaluated the prompt\npatterns proposed in [ 3], and identiﬁed a subset of them as more relevant in\nthe startup context (see Sect.2). We ﬁrst tried these patterns using a simulated\nconversation with ChatGPT. Then we applied them in real-life conversations\nbetween a group of students studying entrepreneurship and ChatGPT. The pre-\nliminary results from this initial step helped us achieve a better understanding\nof the requirements for converting LLMs into eﬀective AI assistants.\n2 Background and Related Literature\nThe role of AI in transforming businesses is now well established. However, its\nutility in the context of startups has remained fuzzy [5]. While several startups\nare seen using this technology to support their operations in data analysis, chat-\nbots, and process automation, a vast majority of startups are still unsure how\nto incorporate or leverage this technology in the development of their innova-\ntive ideas. The challenge becomes more signiﬁcant when considering early-phase\nstartup development in which ideation and brainstorming activities are inten-\nsive. The existing literature paid little attention to the role of AI and how to\nleverage its core values for startups [ 5]. The recent upsurge of generative AI,\nparticularly LLMs, calls for more exploration of AI potentials for startups.\nThe development of the ﬁrst GPT (Generative Pre-trained Transformer) model\nin 2018 [6] laid the foundation of LLMs. They gained signiﬁcant attention soon\nafter OpenAI released ChatGPT on November 30, 2022. Dimitrov [7] deﬁnes an\nLLM as a pre-trained model, trained on a very large text dataset. LLMs promise\ngreat potential in generating, summarizing, translating, and performing various\nnatural language tasks [8]. They can oﬀer a series of conversations with a great con-\nversational user experience [9]. However, despite the widespread adoption of LLMs\nin various industries, it is unclear how non-experts can design eﬀective prompts to\ninteract with these models and elicit the desired behaviour [9].\nDwivedi et al. [10] claim that the output of an LLM signiﬁcantly depends on\nhow a prompt is designed and provided to the model. Pengfei et al. [2] conclude\nthe same and propose to experiment with prompts to elicit the desired knowledge\nfrom LLMs. A similar view is presented in another study [7]. The author ascertains\n194 X. Wang et al.\nthat a response from an LLM will be eﬀective if a prompt is good. Therefore, eﬀec-\ntive prompts are essential for a desired answer and future interactions. Similarly,\naccording to [10], there is a need to train users to provide an eﬀective prompt and it\nis going to be an essential future skill. The ﬁeld to design and implement prompts\nfor LLMs is referred to as prompt engineering [3].\nIn this context, White et al. [ 3] propose a catalog of prompt patterns to\nenhance the output of LLMs. The authors propose 15 prompt patterns in their\nstudy, originally inspired by the software design patterns. The presented patterns\nare designed with the intent to oﬀer better yet reusable ways to interact with\nLLMs. Considering all the patterns in the catalog, we selected seven patterns to\nbe further investigated in our study, brieﬂy described below.\n– Persona: In this pattern, a user asks an LLM to play a particular role. In\nrole-playing, the LLM is given a role without providing ﬁne details of that\nrole. The pattern is applicable when users do not know the exact details\nrequired to process the request, however, what they know is the role of a\nperson who is responsible for this kind of job.\n– Context Manager: This pattern helps users to either introduce or remove\na speciﬁc context while having a conversation with LLM. Therefore, users\ndrive LLMs to consider or ignore a few aspects while producing the output.\nOtherwise, LLMs tend to provide broader or generic answers to a particular\nquestion asked by the user.\n– Flipped Interaction: In this pattern, the interaction between the user and\nLLM is ﬂipped. It means that the LLM is supposed to lead the interaction\nand ask questions to accomplish the user’s goals. Communicating a goal to\nthe LLM is a prerequisite in this pattern. As the content of the interaction\nis produced by the LLM according to the speciﬁed goal, therefore, a more\nprecise output is generated by the LLM using the knowledge that the user\ndoes not possess.\n– Cognitive Veriﬁer: This pattern is proposed to restrict LLMs to always\ndecompose the original questions into a series of sub-questions automatically.\nThereafter, by combining answers to sub-questions, an LLM produces the\nanswer to the original question. The original thought for this pattern comes\nfrom a recent study [11]. According to the authors, the LLM can assert more\nreasonably if we divide the key problem into sub-problems and then the LLM\ncan process them in a sequence. This strategy is referred to as least-to-most\nprompting [11]. The primary goal of this pattern is to improve the prompts.\n– Alternative Approaches: It helps to overcome the problem of cognitive\nbiases. Humans naturally have the tendency to exhibit what they see and\nthink. Therefore, the rationale behind this pattern is to overcome cognitive\nerrors so that users may ask for alternative ways of doing a particular task.\nA comparison of alternative practices, in terms of pros and cons, could also\nbe asked by users.\n– Question Reﬁnement: The purpose of this pattern is to prompt an LLM\nto produce a reﬁned version of the user question so that a piece of accurate\ninformation should be produced. Therefore, the LLM needs to have a few\nAI Assistant for Startups 195\ninteractions with a user to produce the reﬁned question. Alongside this, the\nLLM also needs a context to produce a better version of the question. There-\nfore, the goal is to turn the user’s question into a reﬁned version with the\nhelp of a series of interactions.\n– Template: This pattern is recommended when the user needs to restrict\nLLMs to follow a use-case and produce output accordingly. Therefore, LLMs\ndeliver output in a format that the user speciﬁes. In this scenario, LLMs do\nnot have knowledge of the speciﬁed template and the user has to specify it\nwhile asking a question.\n3 Research Process\nWe conducted the study in two steps: 1) Apply the prompt patterns to a simu-\nlated conversation, and 2) Apply the prompt patterns to real-life conversations.\nIn the ﬁrst step, we created a scenario in which a younger entrepreneur is\ninterested in building a startup in the healthcare domain. She did not have a\nclear startup idea to start with and turned to ChatGPT to understand how she\ncould proceed. We designed the conversation following an example provided by\nan entrepreneurship educator in a YouTube video\n2. The educator is an expert\non entrepreneurship education3. The conversation covers various aspects of the\nideation phase, such as problem and solution, customer segment, ﬁrst minimal\nviable product, etc. We created two versions of the conversation, the ﬁrst one\nwith naturally formulated questions, and the second one with prompt-tuned\nquestions using the identiﬁed prompt patterns. We asked an entrepreneurship\neducator to evaluate the two sets of answers and gathered feedback from her. The\nevaluation session was designed as an unstructured interview with open ques-\ntions. The materials evaluated by the entrepreneurship educator are available at\nhttps://ﬁgshare.com/s/0b5588735abbc484098c.\nIn the second step, we invited four students at our university who were work-\ning on various startup ideas as part of an entrepreneurship course. We set the\ncontext of the conversation to problem validation (the ﬁrst phase of startup\ndevelopment based on the Lean Startup methodology [12]) which was the focus\nof their startup projects at the time of the study. Each of them was asked to\ninteract individually with ChatGPT within the deﬁned context. They ﬁrst used\nthe questions they formulated naturally and intuitively. Then we helped them\nrepeat the conversation applying the prompt patterns where applicable to their\nquestions. During these sessions, we observed how they formulated questions,\ninteracted with ChatGPT, and reacted to the answers they received. After these\nsessions, we asked them to evaluate the two sets of answers by commenting\nfreely on them. The documented conversations from these sessions can be found\nat https://ﬁgshare.com/s/0b5588735abbc484098c.\nAll collected ChatGPT conversations, feedback from the entrepreneurship\neducator and students, as well as our observations will be analyzed systematically\n2 https://www.youtube.com/watch?v=bJ8B6hK0pPs.\n3 https://www.teachingentrepreneurship.org/.\n196 X. Wang et al.\nusing appropriate qualitative data analysis techniques. In Sect.4, we report the\npreliminary results.\n4 Preliminary Results\nTable1 lists the prompt patterns selected from [3] (as described in Sect. 2)a n d\ntheir application in the conversations described in Sect. 3. Some patterns are\napplied independently, at the beginning of a conversation, or before a group of\nquestions. Others are integrated as part of the questions.\nTable 1. Prompt patterns used in startup-related conversations with ChatGPT\nPattern name When to use Example of prompt-engineered question\n(Italic text is the original question)\nPersona When the startup team would like\nthe LLM to assume a speciﬁc\nstartup role, e.g., a co-founder, a\nmentor, or an investor, and pro-\nvide a detailed answer accord-\ningly.\n“Please act as a startup mentor, and answer\nmy following questions.” (This prompt was\nput at the beginning of a conversation.)\nContext Manager When the startup team would like\nthe LLM to focus on (or exclude)\na speciﬁc stage or aspect of the\nstartup process.\n“Within the scope of startups, please con-\nsider only the early phases of startup devel-\nopment when answering my question. What\ncould be the approaches to validate the prob-\nlems that our startup intends to solve? ”\nFlipped Interaction When the startup team asks\nabout a topic that they have lim-\nited knowledge on. This pattern\nenables the LLM to guide the\nteam to ask more questions to\nobtain more knowledge on the\ntopic\n“Please ask me questions to answer my fol-\nlowing question. When you have enough\ninformation to answer my question, create an\nanswer to my question with consideration of\nall information provided to you. Which cus-\ntomer segment should our startup serve? ”\nCognitive Veriﬁer When the startup team asks a\nquestion that is too complex and\nneeds to be decomposed into sub-\nquestions.\n“How could we design our landing page to\ntest the problem-solution ﬁt? Please generate\nthree additional questions that would help\nyou give a more accurate answer to this ques-\ntion. When I have answered the three ques-\ntions, combine the answers to produce the\nﬁnal answers to my original question.”\nAlternative Approaches When the startup team looks for\nalternative options and would like\nto compare them using certain cri-\nteria.\n“What is the riskiest assumption for our\nbusiness model? Please generate alternative\nanswers to this question, and then compare\nthe level of risks involved.”\nQuestion Reﬁnement When the startup team is unsure\nwhat is the right question to ask\nand would like to get help to\nrephrase the question.\n“From now on, whenever I ask a question,\nplease suggest a better version of the question\nto use, incorporating information speciﬁc to\nthe question that I am using, and check with\nme if I would like to use the suggested ver-\nsion of the question.” (This prompt was used\nbefore a group of questions that need reﬁne-\nment.)\nTemplate When the startup team would like\nto ask the LLM to produce struc-\ntured output applying a given\ntemplate.\n“Please create a lean canvas based on our\nstartup idea. When doing so, please follow a\ntemplate that I provide to format your out-\nput. The template can be found at https://\ngustdebacker.com/lean-canvas/. Please pre-\nserve the formatting and overall template.”\nAI Assistant for Startups 197\nIt is evident that, based on our initial analysis of the conversations,\nthe prompt-engineered questions tend to elicit more elaborated answers or\nenhance the conversational interactions and experience. The feedback from the\nentrepreneurship educator and the four students was generally positive toward\nthe answers to the prompt-engineered questions. They commented that the\nanswers were more speciﬁc, had more details, and in some cases less assertive\nthan the answers to their original questions. However, in a few cases, both the\neducator and students considered the prompt-engineered questions produced\nworse answers. We need to conduct further analysis to understand whether it is\nbecause the prompt pattern applied was not appropriate.\nOur initial observation of the interactions between the students and Chat-\nGPT indicates that not all of them were equally comfortable or conﬁdent when\nasking questions to ChatGPT. Some of them were struggling to formulate appro-\npriate questions to ask or ask follow-up questions. This may be partially linked\nto how familiar they were with conversing with LLMs, and partially attributed\nto how well they learned problem validation as a startup topic. They also\nrevealed diﬀerent attitudes towards ChatGPT and the answers to some prompt-\nengineered questions. An interesting comment was made by one student when\nhe received the answer to a question prompt-tuned using the “Flipped Inter-\naction” pattern. He commented that he expected ChatGPT to always provide\nclear answers rather than asking further questions. In his opinion, answering one\nquestion with another or more questions was a sign of not being polite.\n5 Discussion\nThe study was conducted using conversations that are meaningful for early-stage\nstartups. Brainstorming is one of the most intensive activities in the initial phase\nof a startup. If a startup team intends to use LLMs to support their brainstorm-\ning activities, they could apply the prompt patterns that can lead to divergent\nideas as well as convergent answers. Several patterns examined in our study\nhave good potential to stimulate divergent thinking. Flipped interaction is\na good pattern for this purpose. Using this pattern to formulate prompts and\ntune questions, rather than being provided with direct answers, the startup team\ncan open their minds and ponder on more questions that are relevant to their\nbusiness idea. This style of interaction is close to the mentoring relationship\nbetween a startup team and their mentor. It facilitates the team to think more\nactively rather than looking for fast and simple answers. Another pattern,Cog-\nnitive Veriﬁer , can produce a similar eﬀect. It diﬀers from Flipped Interaction\nin that it does provide a conclusive answer after decomposing a complex ques-\ntion into several sub-questions. The third pattern, Alternative Approaches ,\ncan also support divergent thinking. Applying this type of prompt to LLMs can\nreturn multiple answers/possibilities to a question to which a single answer is\nnot desirable.\nThe preliminary results indicate that, without applying any prompt engineer-\ning techniques, startups may not obtain the desired beneﬁts from LLMs such as\n198 X. Wang et al.\nmore creative ideas and new insights and understanding of their startup business.\nUsing prompt patterns eﬀectively can help startup teams overcome their lack of\nknowledge regarding startup processes and deﬁcient prompt engineering skills.\nThe results also serve as a reminder that the successful application of prompt\nengineering to LLMs is not solely a technical matter. Socio-cultural and human\nfactors will signiﬁcantly inﬂuence the eﬀectiveness of prompt engineering.\nThere are several limitations in our study. Firstly, we applied the prompt\npatterns to ChatGPT only. Their applicability to other LLMs remains unclear.\nApplying and customizing prompt patterns for other LLMs is an interesting\nfuture study. Another limitation is that we studied a small number of students\nwho develop startup ideas in a university course setting. Further studies are\nneeded to understand what questions real-world startups are asking LLMs and\nwhether the prompt patterns are equally applicable and useful for them. Lastly,\nour study was focused on the initial phase of the startup process. Whether the\nﬁndings are valid for more mature startups is yet to be understood.\n6 Conclusion\nThe recent upsurge of generative AI, including LLMs, opened up numerous excit-\ning opportunities for startups. In this paper, we applied prompt engineering to\nhelp startup teams obtain better results when interacting with LLMs. We inves-\ntigated the application of a set of prompt patterns to ChatGPT. The initial\nresults show that some patterns are more suitable for brainstorming which is a\ntypical activity conducted by early-stage startups. Prompt-tuned questions may\nlead to more speciﬁc and more detailed responses, but it is not guaranteed. In\naddition, human factors can play an important role in the eﬀective application\nof prompt patterns.\nThe study presented in the paper is the ﬁrst step toward building AI assis-\ntants for startups based on LLMs. To reach our goal eventually, we will employ\na design science research approach. Two main artifacts are envisioned: 1) a\n“startup prompt book” in which prompt patterns are a main component. It is a\nknowledge base that contains a set of rules that transform intuitively expressed\nquestions and requests made by startup teams into prompt-engineered questions\nas input to LLMs; and 2) a “prompt engine” that can automate the prompt engi-\nneering process and choreograph conversations with LLMs. These two artifacts\nare at the core of an AI assistant that can become a valuable resource for a\nstartup team, as a co-founder, a team member, or a mentor.\nTo address the limitations mentioned previously, the presented study can be\nextended to include other LLMs, use a more diverse and representative sample\nof startups, and explore the applicability of their ﬁndings to startups in vari-\nous stages of development. We believe that the results we obtain in the startup\ncontext can be generalisable to established companies for endeavours such as\nproduct innovation (e.g., in the ideation phase). However, the generalisability\nneeds to be validated by future research. Last but not least, the potential draw-\nbacks and risks of using LLMs (such as privacy and ethical issues) is an important\nresearch topic that needs to be investigated in future research.\nAI Assistant for Startups 199\nReferences\n1. Short, C.E., Short, J.C.: The artiﬁcially intelligent entrepreneur: ChatGPT,\nprompt engineering, and entrepreneurial rhetoric creation. J. Bus. Ventur. Insights\n19, e00388 (2023). https://doi.org/10.1016/j.jbvi.2023.e00388\n2. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and\npredict: a systematic survey of prompting methods in natural language processing.\nACM Comput. Surv. 55(9), 1–35 (2023)\n3. White, J., et al.: A prompt pattern catalog to enhance prompt engineering with\nChatGPT (2023). https://arxiv.org/abs/2302.11382\n4. Giardino, C., Bajwa, S.S., Wang, X., Abrahamsson, P.: Key challenges in early-\nstage software startups. In: Lassenius, C., Dingsøyr, T., Paasivaara, M. (eds.) XP\n2015. LNBIP, vol. 212, pp. 52–63. Springer, Cham (2015).https://doi.org/10.1007/\n978-3-319-18612-2 5\n5. Nguyen-Duc, A., Hoang, T.N., Bøe, T., Sundbø, I.: Understanding the\nrole of artiﬁcial intelligence in digital startups: a conceptual framework\n(2023). https://www.researchgate.net/proﬁle/Anh-Nguyen-Duc-3/publication/\n369943717\nUnderstanding the Role of Artiﬁcial Intelligence in Digital Startups\nA Conceptual Framework/links/6435aeedad9b6d17dc4ef8b5/\n6. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving lan-\nguage understanding by generative pre-training (2018).https://www.mikecaptain.\ncom/resources/pdf/GPT-1.pdf\n7. Dimitrov, M.: What business leaders should know about using LLMS like\nChatGPT (2023). https://www.forbes.com/sites/forbesbusinesscouncil/2023/02/\n07/what-business-leaders-should-know-about-using-llms-like-chatgpt/\n8. Brown, T., et al.: Language models are few-shot learners. Adv. Neural. Inf. Process.\nSyst. 33, 1877–1901 (2020)\n9. Zamﬁrescu-Pereira, J., Wong, R., Hartmann, B., Yang, Q.: Why johnny can’t\nprompt: how non-AI experts try (and fail) to design LLM prompts. In: Proceedings\nof the 2023 CHI Conference on Human Factors in Computing Systems (CHI 2023)\n(2023)\n10. Dwivedi, Y.K., et al.: So what if ChatGPT wrote it?” multidisciplinary perspectives\non opportunities, challenges and implications of generative conversational AI for\nresearch, practice and policy. Int. J. Inf. Manage. 71, 102642 (2023)\n11. Zhou, D., et al.: Least-to-most prompting enables complex reasoning in large lan-\nguage models, arXiv preprint arXiv:2205.10625 (2022)\n12. Ries, E.: The lean startup: How today’s entrepreneurs use continuous innovation\nto create radically successful businesses. Currency (2011)\n200 X. Wang et al.\nOpen Access This chapter is licensed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium\nor format, as long as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license and indicate if changes were\nmade.\nThe images or other third party material in this chapter are included in the\nchapter’s Creative Commons license, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the chapter’s Creative Commons license and\nyour intended use is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright holder.",
  "topic": "Brainstorming",
  "concepts": [
    {
      "name": "Brainstorming",
      "score": 0.8065166473388672
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6218369603157043
    },
    {
      "name": "Computer science",
      "score": 0.3703247904777527
    },
    {
      "name": "Knowledge management",
      "score": 0.36157387495040894
    },
    {
      "name": "Artificial intelligence",
      "score": 0.1690920889377594
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}