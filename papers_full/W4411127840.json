{
  "title": "Utilizing large language models for semantic enrichment of infrastructure condition data: a comparative study of GPT and Llama models",
  "url": "https://openalex.org/W4411127840",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5012541412",
      "name": "Lea Höltgen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5026502898",
      "name": "Sven Zentgraf",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5091146830",
      "name": "Philipp Hagedorn",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5046951484",
      "name": "Markus König",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4297120072",
    "https://openalex.org/W2490033382",
    "https://openalex.org/W2739575306",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W2185907055",
    "https://openalex.org/W2617512245",
    "https://openalex.org/W6731801563",
    "https://openalex.org/W4285082860",
    "https://openalex.org/W4407416297",
    "https://openalex.org/W4383682582",
    "https://openalex.org/W2137079713",
    "https://openalex.org/W4298012455",
    "https://openalex.org/W3016882567",
    "https://openalex.org/W2486829137",
    "https://openalex.org/W7018758692",
    "https://openalex.org/W4388514671",
    "https://openalex.org/W4285194159",
    "https://openalex.org/W4387559542",
    "https://openalex.org/W4388144216",
    "https://openalex.org/W4404784221",
    "https://openalex.org/W4389520779",
    "https://openalex.org/W4392517661",
    "https://openalex.org/W3033128669",
    "https://openalex.org/W3192290435",
    "https://openalex.org/W4200578334",
    "https://openalex.org/W4390638022",
    "https://openalex.org/W2795345297",
    "https://openalex.org/W2955512557",
    "https://openalex.org/W6678067405",
    "https://openalex.org/W2565768391",
    "https://openalex.org/W2300469216",
    "https://openalex.org/W4200424274",
    "https://openalex.org/W2555020644",
    "https://openalex.org/W4362579589",
    "https://openalex.org/W4283784269",
    "https://openalex.org/W2008896880",
    "https://openalex.org/W3089558719",
    "https://openalex.org/W4389009947",
    "https://openalex.org/W2293402056",
    "https://openalex.org/W2623742745",
    "https://openalex.org/W1493596129",
    "https://openalex.org/W3086055695",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4388551592",
    "https://openalex.org/W4313567538",
    "https://openalex.org/W4380086306",
    "https://openalex.org/W4401754921",
    "https://openalex.org/W335144045"
  ],
  "abstract": "Abstract Relational databases containing construction-related data are widely used in the Architecture, Engineering, and Construction (AEC) industry to manage diverse datasets, including project management and building-specific information. This study explores the use of large language models (LLMs) to convert construction data from relational databases into formal semantic representations, such as the resource description framework (RDF). Transforming this data into RDF-encoded knowledge graphs enhances interoperability and enables advanced querying capabilities. However, existing methods like R2RML and Direct Mapping face significant challenges, including the need for domain expertise and scalability issues. LLMs, with their advanced natural language processing capabilities, offer a promising solution by automating the conversion process, reducing the reliance on expert knowledge, and semantically enriching data through appropriate ontologies. This paper evaluates the potential of four LLMs (two versions of GPT and Llama) to enhance data enrichment workflows in the construction industry and examines the limitations of applying these models to large-scale datasets.",
  "full_text": "Höltgen et al. AI in Civil Engineering            (2025) 4:14  \nhttps://doi.org/10.1007/s43503-025-00055-9\nORIGINAL ARTICLE Open Access\n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nAI in Civil Engineering\nUtilizing large language models for semantic \nenrichment of infrastructure condition data: \na comparative study of GPT and Llama models\nLea Höltgen1, Sven Zentgraf1  , Philipp Hagedorn1*   and Markus König1   \nAbstract \nRelational databases containing construction-related data are widely used in the Architecture, Engineering, and Con-\nstruction (AEC) industry to manage diverse datasets, including project management and building-specific infor-\nmation. This study explores the use of large language models (LLMs) to convert construction data from relational \ndatabases into formal semantic representations, such as the resource description framework (RDF). Transforming \nthis data into RDF-encoded knowledge graphs enhances interoperability and enables advanced querying capabili-\nties. However, existing methods like R2RML and Direct Mapping face significant challenges, including the need \nfor domain expertise and scalability issues. LLMs, with their advanced natural language processing capabilities, offer \na promising solution by automating the conversion process, reducing the reliance on expert knowledge, and seman-\ntically enriching data through appropriate ontologies. This paper evaluates the potential of four LLMs (two versions \nof GPT and Llama) to enhance data enrichment workflows in the construction industry and examines the limitations \nof applying these models to large-scale datasets.\nKeywords Architecture, Engineering, and Construction (AEC), Large language models, Relational databases, \nSemantic enrichment, R2RML, Llama, GPT\n1 Introduction\nRelational databases serve as a cornerstone for structured \ndata storage and management across various industries, \nensuring efficiency, integrity, and consistency through \nwell-defined relationships and constraints  (Bilal et  al., \n2016). Despite their continued relevance, the increasing \ncomplexity and heterogeneity of data in domains such as \nthe Architecture, Engineering, and Construction (AEC) \nindustry pose challenges in terms of data integration, \ninteroperability, and advanced semantic querying.\nThe digital transformation of the construction sec -\ntor has intensified the need for robust data management \nstrategies of distributed and heterogeneous data (Werb -\nrouck et  al., 2023), particularly in the context of digital \nconstruction projects and Building Information Mode -\nling (BIM) (Borrmann et al., 2018). While BIM facilitates \nthe alphanumeric and geometric representation of build -\ning data, lots of relevant data, such as the condition data \nof infrastructure, is stored in traditional relational mod -\nels lacking the expressive capabilities required for com -\nprehensive knowledge integration and reasoning  (Wang \net  al., 2023). Knowledge graphs, structured using the \nResource Description Framework (RDF)  (Klyne et  al., \n2004), address these limitations by embedding explicit \nformal semantics into data, thereby enhancing interoper-\nability, reasoning capabilities, and integration across het -\nerogeneous data sources (Pauwels et al., 2022).\nThe conversion of relational data into RDF has been \nexplored as a way to bridge the gap between structured \nstorage and semantic representation  (De Giacomo \n*Correspondence:\nPhilipp Hagedorn\nphilipp.hagedorn-n6v@rub.de\n1 Computing in Engineering, Faculty of Civil and Environmental \nEngineering, Ruhr University Bochum, Universitätsstraße 150, \nBochum 44801, NRW, Germany\nPage 2 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \net  al., 2018). While linking relational data through \nsemantic technologies without full transformation has \nshown promise  (Sequeda, 2017; Modoni et  al., 2017), \nsignificant challenges persist. Relational databases in \nthe AEC domain maintain complex data and are scat -\ntered across different stakeholders, which makes it \ndifficult to perform information extraction and rea -\nsoning  (Wang et  al., 2023). Moreover, relational mod -\nels inherently lack semantic expressiveness, limiting \nthe ability to infer new knowledge beyond predefined \nschema constraints (Pauwels et al., 2017). By converting \ndata to RDF, advanced semantic querying and reason -\ning become possible through SPARQL, enabling infer -\nence over ontologies and supporting complex queries \nbeyond standard SQL capabilities (Pauwels et al., 2017). \nMoreover, RDF offers a standardized model for inte -\ngrating diverse data sources across multiple stakehold -\ners and platforms (Pauwels et al., 2017).\nIn the AEC domain and particularly in the Operation \nand Maintenance (O&M) of transportation infrastruc -\nture, data is highly distributed and originates from a \nmix of proprietary and open-source applications (Wang \net  al., 2023). As legacy software solutions for manag -\ning long-term building and infrastructure data become \nobsolete and multiple databases and systems need to \nbe integrated, RDF provides a flexible framework that \nensures sustainable data access and interoperability. \nInstead of storing all relational data in a singular static \nRDF knowledge graph, we aim for a solution, in which \nrule-based reasoning can dynamically infer new facts \nbased on multiple legacy data sources and subsets of \ndata as subgraphs using modular ontologies, ensuring \nthat only relevant and meaningful information is pro -\nvided to infrastructure operators for decision-making. \nBased on this foundation, shared terminology in mod -\nular ontologies and automated ontology matching can \nenhance semantic consistency across heterogeneous \ndata sources, improving overall integration efficiency.\nDespite these benefits, transforming subsets of rela -\ntional databases into RDF knowledge graphs pre -\nsents notable challenges. The RDB to RDF Mapping \nLanguage  (R2RML) standard of the World Wide Web \nConsortium (W3C) offers a structured approach for \nmapping relational schemas to RDF, but its implemen -\ntation demands domain expertise and considerable \nmanual effort (Das et al., 2012; Arenas et al., 2012). The \nprocess fundamentally involves making implicit rela -\ntionships explicit, requiring domain experts to carefully \nselect suitable ontologies and correctly map data -\nbase elements to RDF. This process can be resource-\nintensive, especially when scaling up to large datasets, \nhighlighting the need for more automated and scalable \napproaches.\nRecent advancements in AI, particularly with Large \nLanguage Models (LLMs), offer promising solutions \nto these challenges  (Chang et  al., 2024). LLMs have \ndemonstrated remarkable capabilities in natural lan -\nguage processing, reasoning, and automation, enabling \nthem to assist users with complex tasks (Minaee et al., \n2024). Their potential in automating the conversion of \nrelational data to RDF could streamline the process, \nreducing the need for expert intervention and enabling \nfaster, more accessible workflows (Naveed et al., 2024). \nLLMs could adapt to diverse use cases by interpreting \nthe semantics of data and ontologies, effectively making \nRDF conversion more efficient for non-experts while \nalso maintaining or even enhancing the semantic rich -\nness of the data (Freund et al., 2025).\nEfforts to automate the transformation of relational \ndata to RDF have been made (Hazber et al., 2016; Wang \net al., 2024), but none have yet fully explored the capa -\nbilities of LLMs. The integration of LLMs into this \nprocess could represent a significant advancement, \nnot only in improving conversion efficiency but also in \nenhancing the accessibility of semantic technologies \nto a broader range of users. This paper investigates the \npotential of LLMs to transform the landscape of RDF \nconversion, examining their strengths in automating \ncomplex tasks, adapting to various datasets and ontolo -\ngies, and their ability to perform more efficiently than \ntraditional manual methods. However, the limitations \nand challenges of this approach, including the need for \nfurther refinement and validation of LLM-driven pro -\ncesses, are also critically assessed.\nThe aim of this research is to provide a foundation for \na hybrid approach of translating heterogeneous legacy \ndata into RDF knowledge graphs for the integration of \nmultiple data sources and exploiting the cross-domain \nreasoning, querying, and validation possibilities in dis -\ntributed systems, which is particularly important for \nbridge condition assessment and monitoring, where \nstructural condition data, material information, sensor \ndata, traffic data, and road surface data all origin from \nmultiple independent systems. Such a hybrid system \nhas been proposed in previous work by Hagedorn et al. \n(2023) for BIM-enabled infrastructure asset manage -\nment using the Semantic Web. However, their approach \nstill relies on the cumbersome manual generation of \nmapping rules for the integration of heterogeneous \ndata. Therefore, this study seeks to answer the follow -\ning research question (RQ): \nRQ To what extent are LLMs suitable for converting \nbridge condition data from relational databases into \nRDF, and what implications and limitations emerge \nfrom their application in this context?\nPage 3 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nTo address this question, the remainder of this paper is \nstructured as follows: In Sect.  3, background information \non mapping relational databases to knowledge graphs is \ngiven, while subsequently, in Sect.  2, the related work on \nautomating the mapping process into knowledge graphs \nusing LLMs is given. In Sect.  4, the methodological \napproach and the utilized LLMs are reported. The analy -\nsis is carried out in Sect.  5, followed by the results and \nthe discussion of the results in Sects.  6 and 7. The paper \neventually ends with conclusions in Sect. 8.\n2  Related work\n2.1  Legacy relational data accessibility in construction\nThe accessibility of legacy data is a highly relevant issue \nin the construction industry and is receiving increasing \nattention. Legacy data, i.e., data from the long history of \nvarious construction projects, is often stored in different \nformats, systems, or even on paper  (Mayer, 2018). This \ndata contains valuable information that, when provided \nin accessible and usable formats and data sources, could \nbe used to optimize planning, construction and disman -\ntling processes, inform decision-making, and provide \nlong-term documentation. However, the challenge of effi-\nciently accessing and using this legacy data is significant, \nespecially in the complex structure of the construction \nindustry (Liu et al., 2022; Heise et al., 2024; Göbels et al., \n2023).\nA significant challenge associated with utilizing legacy \ndata is the presence of obsolete formats in which it is \nstored. A considerable proportion of this data was cre -\nated using proprietary systems that are no longer in use. \nThe advent of new technologies has also presented a chal-\nlenge, with numerous platforms and software solutions \nemerging over time, resulting in significant incompat -\nibility between systems (Abu Bakar et al., 2022). Further -\nmore, the lack of clear standards for data collection and \nstorage has made consistent integration more challeng -\ning. Another issue is data quality, with legacy data often \nbeing incomplete or inconsistent, which significantly lim-\nits its usability.\nIn the context of the construction industry, it must \nalso be taken into account that the data comes from a \nvariety of heterogeneous sources. Over time, construc -\ntion companies, architects and other project stakehold -\ners have each built up their own data silos. This variety \nof data sources leads to a strong heterogeneity of infor -\nmation, which makes integration more difficult  (Heise \net al., 2024). In addition, much historical project data still \nexists in paper form or in outdated digital formats, which \nmeans that there is a high effort required to transfer this \ninformation into modern systems (Mayer, 2018).\nSeveral research efforts have addressed the above chal -\nlenges, which are presented below. As denoted in Sect.  1, \nthe focus in this paper is on legacy data of transporta -\ntion infrastructure that is available as relational data. \nThe main focus was on work that uses ontologies to help \nstructure and interpret relational data. Other research \nactivities that deal with the digitization and processing of \nplanning or archive data or other legacy data relevant to \nthe construction industry are not covered by this section \nas they are not within the scope of this work. This sec -\ntion does not claim to be complete but provides an over -\nview of current relevant work in this research field, while \na comprehensive review of the literature is still open for \nfuture work.\nIn current research approaches, various methods are \nbeing developed to convert legacy building data into \nmachine-readable formats and to enable efficient use \nof this data through new concepts  (Göbels et  al., 2023; \nHeise et al., 2024; Pregnolato et al., 2022; Stevens et al., \n2020). In their work, Göbels et al. (2023) present meth -\nods for automatically transferring implicit textual loca -\ntion descriptions from bridge maintenance reports into \nthree-dimensional modeling contexts. In existing sys -\ntems, these reports are based on manually recorded \ntextual documentation and are, therefore, inefficient, \nerror-prone, and uninterpretable. They cite the exam -\nple of the German bridge information system SIB-Bau -\nwerke, where damage and locations are described in a \ntext-based, subjective manner, making automation and \naccurate localization difficult. The authors propose a \nrule-based transformation method that integrates these \ntextual descriptions into IFC 3D models, thereby iden -\ntifying and creating explicit model areas and damage \nzones. This approach enables the mapping and geomet -\nric representation of damage in the 3D model and cre -\nates links between BMS data and model information in \na linked data format. They state that, unlike stochastic \nmachine learning approaches, this method is suitable for \nsingle data sets and has the potential for broad applica -\ntion to similar bridge structures, providing the basis \nfor using machine learning to process natural language \ndescriptions in 3D models.\nHeise et al. (2024) present an approach to improve the \naccessibility of legacy data through graph-based meth -\nods for querying and analyzing road and bridge data. The \nauthors show how heterogeneous data systems of differ -\nent infrastructure elements, such as roads and bridges, \ncan be linked to enable comprehensive analyses and \nlocation-based queries. As much of this as-built data is \navailable in isolated systems, it can only be analyzed with \nconsiderable manual effort.\nBy lifting road and bridge data from original formats \ninto semantic-rich RDF, the different data formats and \nsources are made accessible in a common structure. This \nstructure facilitates location-based and comprehensive \nPage 4 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nqueries by linking different spatial reference systems in a \nstandardized model. Such an approach makes it possible \nto link the various infrastructure elements consistently \nand across applications, which supports more efficient \nuse and analysis of inventory data for infrastructure plan-\nning and maintenance.\nTo better use existing legacy data, Pregnolato et  al. \n(2022) examine how the use of Digital Twins (DT) can \nimprove the accessibility of legacy data in the field of \nAEC. The focus is on the development of a workflow for \ncreating a DT for existing infrastructures, which usually \nhave no or only limited digital representations. The aim \nwas to create a digital representation that is accessible \nnot only for collecting and managing historical informa -\ntion but also for maintenance decisions and long-term \nplanning. The proposed workflow provides a systematic \nguide on the transformation of as-built infrastructures \ninto DTs, including existing data sources.\nAnother research paper by Stevens et  al. (2020) pre -\nsents a method that aims to improve the accessibility and \nusability of legacy bridge inspection data by converting \nolder inspection records into the Bridge Condition Index \n(BCI) format. Originally, bridge inspections were based \non visual assessments and numerical condition scores \n(e.g. a scale of 1 to 4), which were often subjective and \ninaccurate. This method was no longer sufficient to effi -\nciently manage the aging and increasingly stressed bridge \nstructures.\nThe introduction of the BCI aims to create a standard -\nized, national rating scale from 0 to 100 that allows for \na more continuous and accurate condition assessment, \nthereby improving the prediction of bridge deteriora -\ntion. The study describes the conversion of approximately \n17 years of inspection data to the BCI system for 6,978 \nbridges in Northern Ireland. They state that transi -\ntion allows for more accurate comparability and more \ninformed condition monitoring over longer periods of \ntime, improving the accessibility and usability of older \ndata for modern bridge management systems and pro -\nviding a basis for more accurate predictions and strategic \ninvestment decisions.\nIn contrast to the research approaches, Khalil et  al. \n(2021) present a more general approach by analyzing the \noverall challenges of digitally recording and managing \ninventory data of historic buildings. They emphasize that \nthe documentation of cultural heritage buildings often \ncontains isolated data types, which limits their accessi -\nbility and usability for future conservation measures. To \nimprove data accessibility, the authors propose a system -\natic categorization of building data into four main areas: \nFirst, geometry recording, which includes a detailed sur -\nvey of the external and internal structures of the build -\ning. Secondly, material and structural surveys, provide \ninformation on the building materials used and the struc-\ntural integrity of the building. Thirdly, the performance \ndata, includes information on the functional perfor -\nmance of the building, such as energy efficiency and air \nconditioning. Fourthly, historical records documenting \nthe building’s history, previous renovations, and cultural \nsignificance. They conclude that integrating this diverse \ndata into a unified digital model, ideally using Heritage \nBuilding Information Modelling (HBIM), improves the \naccessibility and management of as-built data.\n2.2  Knowledge graph creation from databases using large \nlanguage models\nRecent advances in the field of Artificial Intelligence \n(AI), particularly Deep Learning, have led to the rapid \ndevelopment of Large Language Models (LLMs), which \nhave become a cornerstone for a variety of Natural Lan -\nguage Processing (NLP) tasks  (Naveed et  al., 2024). \nLLMs represent a specialized form of neural networks, \ntypically designed to handle and process vast amounts \nof textual data. By leveraging the architecture of deep \nlearning neural networks, LLMs aim to understand and \ngenerate human-like text based on the prediction of word \nsequences. This ability to generate coherent and contex -\ntually relevant text has made LLMs a powerful tool across \ndiverse applications, ranging from machine translation to \nconversational agents and content generation.\nOne of the most significant breakthroughs in the archi-\ntecture of LLMs is the introduction of the Transformer \nmodel, as outlined by Vaswani et  al. (2017). The Trans -\nformer architecture revolutionized NLP by introducing \na mechanism that efficiently handles long-range depend -\nencies within text through its self-attention mechanism, \nwhich allows the model to focus on the most relevant \nparts of the input data. This architecture, with its paral -\nlelization capabilities, has enabled LLMs to be trained \non unprecedented scales, resulting in models like \nGPT (Generative Pre-trained Transformer) and BERT \n(Bidirectional Encoder Representations from Trans -\nformers), which have set new benchmarks in NLP perfor-\nmance (Naveed et al., 2024).\nLLMs are generally pre-trained on large corpora of \ntext in an unsupervised fashion, learning patterns in the \ndata without specific labels  (Naveed et  al., 2024). This \npre-training allows them to build a broad understand -\ning of language. After the initial training, LLMs can be \nfine-tuned to perform more specific tasks, such as text \nclassification, question-answering, or summarization. \nFine-tuning enhances the model’s performance on task-\nspecific data while still benefiting from the vast linguistic \nknowledge acquired during pre-training.\nLLMs’ abilities to convert natural language input into \nSQL queries based on database schema have long been \nPage 5 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nthe subject of various studies (Sequeda et al., 2023; Jiang \net al., 2023; Sun et al., 2024; Zhang et al., 2024; Rajkumar \net  al., 2022). These studies typically investigate LLMs’ \nperformance on subtasks such as schema comprehen -\nsion, SQL generation, and natural language understand -\ning (Hong et  al., 2024). In particular, studies like  (Sun \net al., 2024) and (Jiang et al., 2023) explore how LLMs can \ngenerate SQL queries from structured data and evaluate \ntheir effectiveness in understanding table structures and \nschemas. Lin et al. (2023) proposes a solution to enhance \nChatGPT’s effectiveness in database management by \ndeveloping syntaxes for representing database semantics \nin natural language, which has the potential to streamline \ndatabase operations while addressing privacy concerns.\nSchema matching has been a topic of significant \nresearch (Parciak et  al., 2024; Peeters and Bizer, 2024; \nSheetrit et  al., 2024). The focus of these studies is on \nhow LLMs can match entities between datasets, con -\nsidering semantic similarity and external factors such as \nsynonyms, sub-strings, and prefixes or suffixes (Rahm \nand Bernstein, 2001). For instance, Parciak et  al. (2024) \ninvestigate how much contextual information influences \nLLMs’ performance in schema-matching tasks, showing \nthat detailed prompts lead to more accurate matches. \nSheetrit et  al. (2024) introduced a method called \nReMatch, which combines LLMs with retrieval models to \npre-select matching candidates based on semantic simi -\nlarity, demonstrating superior performance and scalabil -\nity compared to traditional machine learning approaches.\nHertling and Paulheim (2023) address the limitations \nof LLMs when dealing with large ontologies. Consist -\nent with Sheetrit et  al. (2024), they suggest pre-select -\ning smaller subsets of potential matches to improve \naccuracy and efficiency. Other works, like Amini et  al. \n(2024), advocate for using modular ontology structures \nto enhance schema-matching performance, especially for \ncomplex alignments.\nRecent research has focused on the capabilities of \nLLMs in reasoning over graph-structured data and \nknowledge graphs (Guo et al., 2023; Hu et al., 2023; Wang \net  al., 2023). LLMs have been tested on various graph \ntasks, such as node classification, edge prediction, and \ngraph traversal. Studies such as Guo et al. (2023) and Hu \net  al. (2023) emphasize the importance of prompt engi -\nneering techniques, such as chain-of-thought prompting, \nto improve LLMs’ performance in these tasks.\nIn tasks involving knowledge graphs, LLMs have shown \npromising abilities in knowledge extraction, link predic -\ntion, and entity matching, though they still fall short of \nspecialized models in some areas. For instance, Zhu et al. \n(2023) evaluates LLMs’ performance on these tasks and \nfinds that while LLMs can generalize well, they are still \noutperformed by models fine-tuned on specific datasets.\nMoreover, benchmarking studies such as Meyer et  al. \n(2023) and Frey et al. (2023) have evaluated LLMs’ abili -\nties to work with RDF-based knowledge graphs. These \nstudies highlight that while LLMs like GPT-4 show \npromising results, challenges remain in improving the \naccuracy and consistency of their outputs in knowledge \ngraph creation and analysis.\nBesides the opportunities in knowledge graph creation, \nLLMs exhibit significant limitations in logical reasoning \ndue to their statistical nature of processing information \nrather than true deductive or symbolic reasoning  (Jiang \net  al., 2024; Wu and Tsioutsiouliklis, 2024; Kambham -\npati, 2024). While they can handle simple logical struc -\ntures and pattern-based inferences, they often struggle \nwith multi-step reasoning, abstract logic, and formal \nproofs (Jiang et al., 2024). LLMs do not internally repre -\nsent logical axioms or maintain a coherent belief system, \nleading to inconsistencies in responses when faced with \ncomplex logical constructs or paradoxes (Kambhampati, \n2024). Unlike dedicated symbolic logic solvers, LLMs \nlack the ability to systematically verify logical consist -\nency and rely on sub-symbolic models. This challenge \narises because LLMs are primarily trained on linear text \nsequences and may lack the inherent capability to inter -\npret the complex graph structures and relationships \npresent in KGs. Consequently, LLMs might fail to accu -\nrately retrieve or infer information from KGs, leading to \ndecreased performance in tasks that demand such struc -\ntured reasoning (Wu and Tsioutsiouliklis, 2024).\nThe reviewed research demonstrates that LLMs hold \nsignificant promise in tasks related to SQL generation, \nschema matching, and knowledge graph reasoning. How-\never, several challenges remain, particularly in scaling \nLLMs to handle large ontologies and knowledge graphs \nefficiently, such as the symbolic logic reasoning capabili -\nties. Continued exploration of prompt engineering tech -\nniques and modular approaches with formal semantics \nis essential for maximizing LLM performance in these \ncomplex tasks.\n3  Technological foundation\nThis section introduces the core technologies and con -\ncepts necessary for transforming relational database \ndata into Resource Description Framework (RDF) for -\nmat  (Klyne et  al., 2004). It begins with an overview of \nknowledge graphs, which represent real-world entities \nand their relationships through nodes and edges. Defini -\ntions of knowledge graphs vary, but most emphasize the \nneed for schemas, ontologies, and reasoning capabili -\nties. RDF is presented as a format for representing such \nknowledge, with data modeled as triples (subject, predi -\ncate, object). RDF schema (RDFS) enhances RDF by add -\ning semantic meaning to data (Brickley and Guha, 2014), \nPage 6 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nwhile Turtle is introduced as a serialization syntax for \nRDF (Beckett et al., 2014). The section then covers rela -\ntional databases, which store data in tables with rows \nrepresenting tuples and columns representing attrib -\nutes. Structured Query Language  (SQL) is discussed \nas the language for querying and manipulating rela -\ntional databases. Finally, the RDB to RDF Mapping Lan -\nguage (R2RML) is introduced as a tool for transforming \nrelational database data into RDF format. This involves \ndefining mappings between tables and RDF triples using \npredefined concepts like triples maps, logical tables, and \nterm maps. These mappings enable the generation of \nRDF graphs from relational database data.\n3.1  Knowledge graphs\nKnowledge graphs trace their origins to early efforts in \nAI and cognitive science, where semantic networks and \nframe-based systems were developed to represent human \nknowledge and the relationships among concepts, as pro-\nposed by Minsky (1974) and Sowa (1984). These initial \nmodels laid the groundwork for later advancements by \ndemonstrating the utility of graph structures in encoding \ncomplex relational data. The advent of the Semantic Web \nfurther propelled the evolution of knowledge graphs, \nnotably through the introduction of standardized frame -\nworks, such as formal ontology specifications proposed \nby Gruber (1993), which enabled disparate data sources \nto be linked and integrated across the internet. This syn -\nthesis of early representational theories with modern \nweb technologies has culminated in the contemporary \nconception of knowledge graphs, which serve as power -\nful tools for data integration, semantic reasoning, and \nadvanced analytics (Hogan et al., 2022).\nKnowledge graphs are complex structures that repre -\nsent entities and their relationships in the form of nodes \n(entities) and edges (relationships). Although there is no \nsingle standardized definition of knowledge graphs, sev -\neral researchers have proposed varying approaches.\nEhrlinger and Wöß (2016) suggest that a knowledge \ngraph is more than just a graph representing knowledge; \nit requires a knowledge base and a reasoning engine \ncapable of processing information from one or multi -\nple sources. The core of a knowledge graph lies in its \nontology, which defines and organizes the knowledge \nbase. Beyond this, the ability to perform reasoning-log -\nical inference based on the relationships and properties \nwithin the graph-differentiates a knowledge graph from \nsimpler forms of data representation.\nPaulheim (2016) views knowledge graphs as a way to \nrepresent real-world entities across various domains, \nwith relationships unrestricted by pre-set rules. A \nschema is used to define the entity classes and their rela -\ntions. Peng et  al. (2023) align knowledge graphs with \nknowledge bases, where entities (nodes) and their rela -\ntionships (edges) are directional. The foundational struc -\nture is a triple, consisting of a subject, predicate, and \nobject.\nHogan et  al. (2022) provide a more comprehen -\nsive definition by describing knowledge graphs as data \ngraphs enriched with schema, identity, context, ontolo -\ngies, and rules. The schema can be semantic, e.g., RDF, \nRDFS, and Web Ontology Language  (OWL)  (Schneider \net al., 2012), validating, e.g., Shapes Constraint Language \n(SHACL)  (Knublauch and Kontokostas, 2017), or emer -\ngent, which automatically uncovers hidden structures \nin the data. Identity mechanisms, such as International \nResource Identifiers (IRIs), are employed to resolve \nambiguity when different nodes may represent the same \nentity. Contextual data, such as temporal or geographical \nconstraints, can be applied to ensure that certain rela -\ntionships or facts only hold under specific conditions. \nOntologies serve to maintain semantic consistency and \nenable automated reasoning. Knowledge graphs are cat -\negorized into open and enterprise knowledge graphs, \nwhere the former is publicly accessible, while the latter is \nrestricted to organizational use.\nIn summary, a knowledge graph is a directed graph \nwhere nodes represent entities and edges represent their \nrelationships, enriched with schema, ontologies, reason -\ning capabilities, and other data models. These features \nhelp ensure that the graph not only stores data but also \nfacilitates reasoning and interoperability across different \ndomains.\n3.2  Relational databases\nThe emergence of relational databases can be traced back \nto the research of Codd in the early 1970s (Codd, 1970). \nIn his groundbreaking research, Codd proposed a new \ntype of data management model based on set theory and \npredicate logic that replaced the hierarchical and net -\nwork models of the time  (Codd, 1970). This theory laid \nthe foundation for SQL and modern database technology.\nRelational databases are storage systems that adhere \nto the relational model, which stores data in tabular \nform (Meier and Kaufmann, 2019). The primary compo -\nnents of a relational database are tables, each represent -\ning a real-world entity or concept. These tables consist \nof columns that define the attributes or properties of the \nentity, and rows, that contain the actual data in the form \nof tuples. Each row is uniquely identified by a primary \nkey, while foreign keys are used to reference primary keys \nin other tables, establishing relationships between the \ndata across various tables (Meier and Kaufmann, 2019).\nSQL is the language used to interact with relational \ndatabases, allowing users to store, manipulate, and \nquery data  (Meier and Kaufmann, 2019). A typical SQL \nPage 7 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nquery includes several statements that specify the data \nto retrieve or modify. The basic structure of an SQL \nquery follows the pattern: SELECT<attribute(s)> \nFROM<table(s)> WHERE<condition>, with the \nSELECT statement specifying the columns to return \nand the FROM clause designating the tables involved. The \nWHERE clause applies conditions to filter the results, \nusing logical operators such as =, <>, and >. Queries can \nbe further refined with clauses like ORDER BY to sort \nresults, GROUP BY to group data, and LIMIT to restrict \nthe number of rows returned. SQL also supports JOIN \nstatements to combine rows from different tables based \non related columns, with variations such as inner, left, \nright, and outer joins. SQL can also be used to create, \nalter, and delete table data, though these operations were \nbeyond the scope of the provided discussion (Elmasri and \nNavathe, 2016).\n3.3  Mapping languages for RDF from relational databases\nThe RDB to RDF Mapping Language (R2RML) is a W3C \nstandard for transforming relational database data into \nRDF format, facilitating the integration of relational \ndata into the Semantic Web  (Das et  al., 2012). R2RML \ndefines mappings using concepts like Triples Maps, Logi-\ncal Tables, Subject Maps, Predicate-Object Maps, and \nTerm Maps, all represented by specific IRIs. Turtle syntax \nis used to define these mappings in an R2RML mapping \ndocument (Sequeda, 2013).\nAt its core, Triples Maps represent database tables \nand describe how table rows are converted into RDF \ntriples  (Das et  al., 2012). Each Triples Map includes a \nSubject Map and can include multiple Predicate-Object \nMaps. Logical Tables handle data extraction from the \ndatabase, either by directly referencing a table or view or \nby using an R2RML view (derived from an SQL query). \nSubject Maps define how RDF subjects are generated, \noften based on a unique identifier like a primary key. \nPredicate-Object Maps specify the predicates and objects \nfor each RDF triple, defining the relationships and data \nproperties within the RDF graph (Sequeda, 2013).\nTerm Maps, including Subject Maps, Predicate Maps, \nand Object Maps, produce RDF terms (IRIs, blank nodes, \nor literals) based on database rows  (Das et  al., 2012). \nTerm Maps come in three types: constant-valued (fixed \nRDF terms), column-valued (using column data), and \ntemplate-valued (generating IRIs based on templates ref -\nerencing column names). Reference Object Maps enable \ncross-table relationships by linking triples maps via join \nconditions (Sequeda, 2013).\nRDF Mapping Language (RML), an extension of \nR2RML, extends the concept beyond relational databases \nto support a wide variety of data sources, such as CSV, \nXML, JSON, and HTML (Meester et  al., 2022). RML \nretains similar constructs like Triples Maps and Term \nMaps but generalizes them to work with diverse data \nformats  (Iglesias-Molina et  al., 2023). R2RML and its \nextension RML provide robust mechanisms to convert \nrelational and other structured data into RDF, promoting \ndata interoperability in the Semantic Web.\n4  Materials and methods\nBased on the background research and literature review, \na specific experiment procedure (see Sect.  4.1) for \ninstructing the analyzed LLMs to perform tasks related \nto data transformation and generation is implemented \nthat utilizes a specified set of LLMs (see Sect.  4.2) for the \ntransformation process. Moreover, the corresponding \nprompt design (see Sect.  4.3) is specified and evaluation \nmetrics (see Sect.  4.4) are defined to evaluate the perfor -\nmance of the LLMs. For executing the analysis, suitable \ndatasets for the evaluation are selected (see Sect.  4.5). \nThe datasets include the tabular data from a relational \ndatabase as well as ontologies that the target KG should \nadhere to and their vocabulary should be used for map -\nping database entries into semantically rich RDF entities. \nCritical considerations further include the scalability of \nthe approach, the complexity of queries, and the accuracy \nof generated knowledge graphs.\n4.1  Experimental procedure\nThe experimental procedure followed in this paper is \ndescribed in the following. The process encompasses four \nconsecutive steps, which can be summarized as follows: \n1. SQL query generation: The LLM is used to generate \nan SQL query based on natural language input and a \ndatabase schema.\n2. SQL-Ontology mapping generation: The LLM is used \nto generate an SQL-Ontology mapping based on an \nSQL query and an ontology.\n3. R2RML mapping generation: The LLM is used to \ngenerate an R2RML mapping based on the SQL-\nOntology mapping.\n4. RDF conversion: External R2RML processors are \nused to convert database data into RDF using the \ngenerated mapping.\nThe experimental procedure is split into these steps to \nallow for checking and, if necessary, correcting inter -\nmediate results before proceeding to the next step (see \nFig.  1). The first step of the methodology for analyzing \nLLMs in the transformation process from SQL to KGs is \ngenerating an SQL query from a natural language input.\nThis involves converting user requests into SQL que -\nries by analyzing and mapping key components from \nthe input to the database schema (see the prompt in \nPage 8 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nAppendix A Table 5). The process includes handling com-\nplex queries involving multiple conditions and joins. To \nensure accuracy, the model is provided with a dynamic \ndatabase schema and sample data entries. Validation of \ngenerated SQL queries is performed either by executing \nthem against the database or using external frameworks \nto check their correctness.\nFollowing SQL query generation, the next task is map -\nping SQL query elements to RDF ontology elements (see \nthe prompt in Appendix  A Table  6). This step involves \naligning SQL tables and columns with RDF classes and \nproperties. The LLM uses a JSON template to provide \nstructured mappings between SQL elements and ontol -\nogy elements, which can be found in Appendix  B. This \ntemplate helps in generating accurate mappings by pro -\nviding clear instructions and a format for the output. Val-\nidation of the SQL-Ontology mapping is done through \nsyntax checks and external tools, focusing on ensuring \nthat all required information is correctly included.\nThe R2RML mapping generation is crucial for convert -\ning relational data into RDF triples. This task involves \ncreating a blueprint that defines how data from rela -\ntional databases is transformed into RDF. The LLM gen -\nerates R2RML mappings based on the SQL-Ontology \nalignment, with detailed instructions provided in the \nprompt to ensure accuracy (see the prompt in Appen -\ndix A  Table  7). Validation of R2RML mappings is per -\nformed by checking their syntax and converting them to \nRDF triples, while semantic validation requires expert \nknowledge.\nThe final step is converting database data into RDF \nformat based on the R2RML mapping. While LLMs can \ngenerate RDF triples directly, dedicated tools are often \npreferred for this task due to their robustness and pre -\ncision. This step ensures that the generated RDF tri -\nples accurately represent the original database data and \nadhere to the target ontologies.\nAdditionally, a self-correction mechanism is imple -\nmented to address invalid outputs. If errors in the inter -\nmediate results are detected, the LLM is instructed to \ncorrect them based on error messages from validation \ntools (see the prompt in Appendix A Table  8). This cor -\nrection process is performed once to avoid inefficiencies \nand potential loops of repeated errors (see Fig.  1). After \nthe LLM’s correction attempt and a still invalid result, the \nprocess of correction is terminated. The self-correction \nmechanism aims to improve the accuracy of the outputs \nand ensure that the transformation process produces \nvalid results.\nThe methodology includes a continuation pro -\ncess to address the issue of response length limits in \nLLMs. When responses exceed token limits, continu -\nation requests are sent to the model, including previ -\nous prompts and incomplete responses. This approach \nensures that the model can generate coherent and \ncomplete responses by referencing the previous out -\nputs and maintaining context. The continuation pro -\ncess is repeated until the response is fully generated and \nvalidated.\n4.2  Large language models\nA variety of LLMs are available for use and evaluation \nwithin this framework. The choice of models is guided \nby multiple factors, including the specific tasks to be \naddressed, the desired trade-off between accuracy and \ncomputational efficiency, and model accessibility. For \nFig. 1 Experimental procedure flow\nPage 9 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nevaluation, both closed- and open-source models are \nincluded. Among the closed-source options, OpenAI’s \ngpt-4-turbo and gpt-4o, based on GPT-4  (Ope -\nnAI et  al., 2024), are selected. For open-source mod -\nels, Meta’s llama-3-70b-instruct and llama −\n3.1-405b-instruct, built upon and extending Llama \n3  (Dubey et  al., 2024), are chosen, as they reflect the \nstate-of-the-art in natural language processing and gen -\neration tasks (Zhao et al., 2023). The chosen models are \nwell-suited for addressing complex language tasks: gpt-\n4-turbo is optimized for speed and efficiency, while \ngpt-4o offers flexibility for customized solutions (Zhao \net al., 2023). On the open-source side, llama-3-70b-\ninstruct and llama −3.1-405b-instruct stand out as \nrobust and resource-efficient options for instruction-\nfollowing applications  (Zhao et  al., 2023). Further mod -\nels, such as Gemini or Claude, were not evaluated in this \npaper, since they were either not available in Germany at \nthe time of conducting the experiments or access could \nnot be granted for educational purposes.\nFor the evaluation process, the temperature parameter \nis set to zero across all models to ensure that responses \nremain as deterministic as possible. The maximum token \nlimit is configured to match the highest token capac -\nity allowed by each model for generation. The top-k and \ntop-p sampling parameters remain at their default set -\ntings, following OpenAI’s guidance to adjust either the \ntemperature or sampling parameters, but not both simul-\ntaneously  (OpenAI, 2024). Additionally, the presence \npenalty is kept at default values to maintain consistency \nthroughout the evaluations. Table 1 provides an overview \nof the configuration parameters applied to the models \nduring the evaluation.\n4.3  Prompt design\nEffective communication with an LLM is achieved by \nsending requests containing a well-designed prompt, \nwhich is key to guiding the model’s performance  (Rat -\nnayake and Wang, 2024). Crafting the prompt is crucial, \nas it ensures that the model accurately understands the \ntask and produces coherent responses that align with \nthe task’s requirements  (Ratnayake and Wang, 2024). \nA well-structured prompt typically adheres to a clear \npattern: it begins with a brief explanation of the task, \nfollowed by the necessary input data, and ends with \nspecified output restrictions  (Marvin et  al., 2024). This \npattern promotes consistency and adaptability across \nvarious sub-tasks.\nSeveral prompting techniques are utilized to improve \nthe model’s performance  (Ratnayake and Wang, 2024). \nOne-shot and few-shot prompting, which provide the \nmodel with specific examples, help it generate more \naccurate responses even with limited training data. These \ntechniques are particularly useful for complex or spe -\ncialized tasks. In one-shot prompting, a single example \nis provided, while few-shot prompting offers multiple \nexamples, enhancing the model’s understanding (Marvin \net al., 2024).\nDifferent prompt versions can be employed, with vary -\ning levels of task detail. These versions allow for the eval -\nuation of how detailed prompts influence output quality. \nChain-of-thought prompting, which enhances reason -\ning by guiding the model through its thought process, \nand self-consistency, which selects the most common or \nconsistent answer, can also improve accuracy (Ratnayake \nand Wang, 2024). However, due to the specific format -\nting requirements and limitations on prompt size, these \ntechniques will not be used in the framework. Instead, \nprompts will prioritize concise responses suitable for the \nnext step in the transformation process. The prompts \ndeveloped in this research can be found in Appendix A \nTables 5, 6, 7 and 8.\n4.4  Analysis criteria\nEvaluating the transformation framework and its imple -\nmentation presents a complex challenge, as the structure \nof the resulting mappings can vary significantly in both \nsyntax (e.g., differing order of triples) and semantics (e.g., \nuse of joins). It is crucial to recognize that the experi -\nments conducted do not aim for a single “correct” solu -\ntion. While the R2RML mappings in each experiment are \nderived from the same SQL query, the expected output \ndepends heavily on user requirements.\nTable 1 Model configuration parameters\ngpt-4-turbo OpenAI \n(2024)\ngpt-4o OpenAI (2024) llama-3-70b-instruct Replicate \n(2024a)\nllama-3.1-405b-\ninstruct Replicate \n(2024b)\nTemperature 0 0 0 0\nMaximum tokens 4096 4096 4096 4096\nTop-k sampling -- -- 50 (default) 50 (default)\nTop-p sampling 1 (default) 1 (default) 0.95 (default) 0.9 (default)\nPresence penalty 0 (default) 0 (default) 0 (default) 0 (default)\nPage 10 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nIn some cases, it may be beneficial to include explicit \njoins between source tables to emphasize relationships \nbetween entities. In other cases, a simpler mapping that \nonly represents the information specified in the SELECT \nclause may suffice. For example, an SQL query with at \nleast one join condition and two or more tables in the \nSELECT clause could be expressed in two ways. The first \noption involves directly representing the query’s output, \nwhere each table in the query is assigned a triple map, \nthe SQL query serves as the logical source, and each col -\numn in the SELECT clause is mapped to a correspond -\ning predicate-object map. The second option enriches the \nmapping semantically by explicitly including join condi -\ntions to reflect one-to-many or many-to-many relation -\nships between tables.\nWhatever option suits the use case better could, in the -\nory, be chosen by the user and included in the R2RML \nmapping generation prompt before the actual map -\nping generation is performed. However, it is particularly \ninsightful to observe the LLM’s decision-making process \nin different scenarios and how effectively it captures the \nsemantics of SQL queries and the underlying database \nschema when translating them into RDF triples.\nThis inherent variability demands a flexible evaluation \nstrategy. Instead of identifying a singular correct map -\nping, the evaluation should consider multiple potential \nsolutions to analyze the framework’s effectiveness and \nthe LLM’s capabilities. By exploring a range of mappings \ngenerated under various conditions, researchers can bet -\nter understand how the LLM interprets user requests and \ntranslates them into accurate and meaningful R2RML \nmappings. This approach provides deeper insights into \nthe capabilities and limitations of both the transforma -\ntion framework and the LLM’s interpretive abilities.\nThe evaluation follows a semi-automated approach. \nSyntax errors (including errors regarding the mapping \nrules of R2RML) are automatically detected during each \nrun of the transformation process and logged accordingly. \nSemantic errors are evaluated manually by an expert user \n(human-in-the-loop) based on provided input data and \nthe logged output of the overall pipeline. This evaluation \ncan only be done by experts because of the existence of \nmultiple indeterminate solutions and the unavailability \nof external tools for semantic quality assessment. Every \nerror or finding is categorized into its respective cate -\ngory and further classified into sub-categories to ensure \na detailed and systematic evaluation of the framework’s \nperformance (see Table  2). This categorization helps in \nidentifying patterns in the types of errors or issues that \nmay arise. As mentioned before, the two main categories \nare syntax and semantic errors.\nRDF/Turtle errors refer to issues related to the RDF \nor Turtle syntax used to represent the RDF data. These \nerrors can involve syntax mistakes, incorrect use of RDF \nconstructs, or the creation of invalid RDF statements that \ndo not conform to RDF specifications, leading to poten -\ntial problems in data representation and interpretation. \nR2RML mapping rule errors arise from the rules them -\nselves that define the transformation of relational data \ninto RDF triples using R2RML. Issues in this category \ncould include incorrect or incomplete mapping rules, \nimproper application of R2RML constructs, or misalign -\nment between the relational database schema and the \nRDF triples produced. These errors can result in inaccu -\nrate data conversion and flawed RDF outputs. SQL errors \nrelate to problems in the SQL queries used as the logi -\ncal table for a triples map. Such errors can include syntax \nmistakes, missing columns, and logical errors that lead to \nfetching errors. Response format issues relate to the for -\nmat in which the LLM responds. These responses often \ninclude markdowns to highlight code or additional expla-\nnations for the results given. However, including this \ndisturbs the transformation process and leads to a failed \ngeneration attempt. Incorrect join operations involve \npotential problems associated with SQL JOIN opera -\ntions in the queries or incorrect use of join conditions in \nthe triples map of a mapping, which leads to insufficient \njoin representations. Incorrect statements include state -\nments within the R2RML mapping that lead to factu -\nally false or misleading RDF triples. These errors occur \nwhen the mapping generates data that does not accu -\nrately represent the underlying content of the database. \nOverinclusive data refers to the mapped RDF triples \nthat include more data than intended or necessary. This \noccurs when the SQL query or logical source used in the \nR2RML mapping is not sufficiently restrictive, leading to \nthe inclusion of irrelevant or extraneous data in the RDF \noutput. Underinclusive data, on the other hand, refers \nto missing or incomplete RDF triples that result from \nthe mapping process failing to include all relevant data. \nThis occurs when the SQL query or logical source used \nin the R2RML mapping is too restrictive or incorrectly \nformulated, leading to the omission of essential data that \nshould have been included. Additional elements cover \nany elements such as joins, triples maps, or columns (as \nTable 2 Error classification\nSyntax Semantics\nRDF/Turtle errors Incorrect join operations\nR2RML mapping rules Incorrect SQL queries\nInvalid SQL queries Incorrect statements\nResponse format Overinclusive data\nOverinclusive data\nAdditional elements\nPage 11 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \npredicate-object maps) that may factually not be wrong \nbut were not asked of the user in the SQL query that the \nR2RML mapping is based on. It is a matter of interpre -\ntation to what extent this constitutes an actual error, as \nthese additional elements may enrich the RDF graph with \nuseful information. However, the evaluation presented \nhere classified this as an error because these elements \nexceed the user’s original request, potentially complicat -\ning the mapping generation process.\nThe evaluation uses both generation prompt versions \n(brief and detailed) as described in Sect.  4.3 paired with \neither zero- or few-shot prompting examples. For sim -\nplicity, these methods will be referred to as zero-brief, \nzero-detailed, few-brief, and few-detailed, referencing \ntheir respective prompting techniques. Including these \ndifferent prompting methods aims to evaluate the impact \nof instruction detail and example quantity on the quality \nof generated responses. This will help identify optimal \nprompting strategies and provide insights into the inter -\naction between prompt detail and the effectiveness of \nfew-shot learning for future reference.\n4.5  Dataset\nThis paper acknowledges the absence of benchmark data-\nsets that include natural language inputs, correspond -\ning SQL queries, and R2RML mappings. To fill this gap, \nthe existing Bridge Inspection Support System (BISS) \nrelational database  (Embers et  al., 2022) from previous \nresearch is used as the dataset in this experiment, which \nstores information about bridge inspection workflows \n(see Fig. 2). The dataset is expanded by adding data from \nother types of buildings, though these additional entries \nare partly fictional.\nThe presented Entity-Relationship-Diagram in Fig.  2 \nmodels a data management system for bridge damages \nand condition management, integrating entities such \nas Buildings, Components, Construction Plans, Dam-\nages, Inspections, and associated data. Key relationships \ninclude linking buildings to components and inspec -\ntions, damages to construction plans, and components \nto hierarchical sub-components. The model supports \ndetailed documentation of damage characteristics and \ninspection results, enabling robust tracking and analy -\nsis. This schema facilitates efficient data organization \nand retrieval, making it suitable for advanced inspection \nprocess management and bridge degradation monitoring \napplications.\nOntologies suitable for this work include the Build -\ning Topology Ontology  (BOT)  (Rasmussen et  al., 2021) \nand Damage Topology Ontology (DOT) (Hamdan et al., \n2019). The BOT is an ontology designed to represent \nthe core concepts of buildings, such as their spatial, and \nfunctional structure, to facilitate data exchange in the \narchitecture, engineering, and construction (AEC) indus-\ntry (see Fig.  3). The BOT defines zones that are spatial \nareas (e.g., buildings, stories, spaces) and can contain or \nintersect with other zones or elements, share interfaces, \nand are represented by a 3D model with geometry and \nmaterial properties. Moreover, in BOT, an element is \ndefined as a construction entity with a specific technical \nfunction, form, or position, such as a wall, a girder, and \na beam.\nAs a supplement to the BOT, the DOT facilitates \nthe representation and relationships of damages and \naffected construction components, supporting a versatile \napproach applicable to any type of degradation or con -\nstruction, including buildings and bridges (Hamdan et al., \n2019). The ontology provides a flexible damage modeling \napproach (see Fig.  4) applicable to various types of deg -\nradation and construction types, allowing damage to be \nrepresented as areas or individual elements, with dam -\nage patterns grouping adjacent damages. While it does \nnot contain specific taxonomies for damage causation \nor standards, it functions as a core ontology that can be \nextended for specific use cases.\nHowever, this research uses a shortened version of \nthe BOT ontology to avoid exceeding the token limits of \nthe models. Non-English labels are removed as they are \nredundant for the SQL-Ontology mapping. Additionally, \nORDER BY and GROUP BY clauses are excluded since \nR2RML mappings cannot adequately represent these \noperations.\n5  Implementation and experiments\nThis section provides the implementation of the proto -\ntype for the analysis of the LLMs and the specification of \nthe experiments conducted based on the methodology \ndescribed in the preceding Sect. 4.\n5.1  Implementation of the experimental setup\nThe prototype for the experimental setup follows a mod -\nular implementation strategy, enabling straightforward \nmaintenance, testing, and extensibility. The implemen -\ntation is done according to the architecture provided in \nFig. 5.\nPython was selected for the implementation because \nof its popularity in AI development and the abundance \nof libraries and frameworks it offers, which support tasks \nlike data processing, model integration, and RDF conver -\nsion. The primary modules are the database connector, \nprompt generator, large language model (LLM) con -\nnectors (one per model), a retrieval-augmented genera -\ntion (RAG) processor, a validator, and an RDF converter. \nThese modules are coordinated by the main module, \nwhich drives the transformation logic and controls the \nPage 12 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nprocess flow. A configuration file provides input param -\neters for each run of the transformation process. These \nparameters include LLM IDs, settings for few-shot \nprompting, prompt detail versions, the database connec -\ntion string, and paths to relevant ontologies.\nThe LLM connections in this implementation are \nestablished via API, with token-based authentication \nstored in environment variables. For closed-source \nmodels like OpenAI’s GPT, a Python library 1 is used to \ninteract with the API. OpenAI’s API returns responses \nin JSON format, which includes both the model’s output \nand response metadata. This is useful for validating out -\nput completeness. If an incomplete response is detected, \nFig. 2 Bridge inspection support system (BISS) database\n1 https:// pypi. org/ proje ct/ openai/, OpenAI Python API library v1.54.4.\nPage 13 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nthe system sends a continuation prompt to retrieve the \nremaining content. Meta’s LLama models, in contrast, \nare open-source but do not provide self-hosting options. \nInstead, Replicate 2, a model hosting service, is utilized \nfor Meta’s LLama models. Replicate streams model \nresponses without explicit indicators of completion, \nrequiring additional logic to verify if the output reaches \nthe token limit. Each API call, prompt generation, and \nmodel response is saved as a log file for further analysis \nand debugging.\nThe DBConnector is responsible for managing data -\nbase interactions. SQLAlchemy 3 is used as the underly -\ning library, facilitating the connection to and querying of \nthe database. The DBConnector also handles the valida -\ntion of SQL queries generated by the LLM by executing \nthem directly in the database. If the query produces valid \nresults, the query is stored as an SQL file. This approach \nensures that each valid query is available for future refer -\nence or manual execution.\nThe PromptGenerator centralizes creating requests and \nprompts for interaction with the LLMs. The class sup -\nports various levels of detail and prompt shots (zero-shot \nand few-shot) depending on the configuration file set -\ntings. It includes specific methods for generating prompts \ntailored to SQL query generation, ontology mapping, and \nR2RML mapping creation. To handle cases where the \nLLM produces incorrect results, the PromptGenerator \nincludes mechanisms for error handling and response \nFig. 3 Structure of the Building Topology Ontology (BOT) (Rasmussen et al., 2021)\nFig. 4 Structure of the Damage Topology Ontology (DOT) (Hamdan et al., 2019)\n2 https:// github. com/ repli cate Replicate model hosting.\n3 https:// pypi. org/ proje ct/ SQLAl chemy, SQLAlchemy v 2.0.36.\nPage 14 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \ncontinuation. These mechanisms reattempt tasks using \nfeedback from previous failures, improving the accuracy \nof the outputs. Valid outputs are stored as JSON files, \nand prompt logs are saved for auditing and improvement \npurposes.\nThe prototype implements RDF conversion using the \nRDFLib4 library. Additionally, Morph-KGC5 is integrated \nto manage R2RML mappings and convert them into \nRDF. The RDF outputs and R2RML mappings are stored \nin Turtle format for consistency and ease of use within \nRDF-based systems.\nOutput validation is a crucial step in ensuring the reli -\nability of the system. The Validator is responsible for \nperforming checks on the outputs from every step of \nthe transformation pipeline. For SQL queries, the sys -\ntem runs the query in the database using SQLAlchemy, \nmarking the output as valid if data is returned. SQL-\nOntology mappings are validated by checking the JSON \nformat using Python’s native JSON library. When valida -\ntion is successful, the valid output is stored as a JSON file, \nensuring that all results can be traced back to their origin \nfor verification or debugging.\n5.2  Experiments\nThree experiments were conducted to evaluate the qual -\nity and accuracy of SQL queries, SQL-ontology map -\npings, and R2RML mappings. These experiments were \ndesigned with increasing complexity to test various \nSQL query structures, serving as the foundation for the \nmappings, and to assess the effectiveness of R2RML \nmappings in translating relational data into RDF triples \nacross diverse scenarios. Importantly, ordering and sort -\ning clauses were excluded from consideration, as R2RML \ndoes not support their representation. To obtain repre -\nsentative results, 20 transformation runs were conducted \nfor each prompting technique and model, resulting in 20 \nR2RML mappings per configuration. During the RAG \nprocessing phase, top-k sampling for selecting the most \nrelevant triples was set to 40, with a maximum of three \ncompletion attempts allowed per model response.\nThe first experiment was conducted to test the \nbasic understanding of R2RML mappings, their syn -\ntax, and mapping conventions. It focuses on a simple \nSQL structure to establish a baseline for mapping qual -\nity and correctness (see the user input defined for all \nFig. 5 Architecture of the prototype implementation for LLM analysis\n4 https:// github. com/ RDFLib/ pySHA CL/ relea ses/ tag/ v0. 27.0, RdfLib \nv0.27.0.\n5 https:// github. com/ morph- kgc/ morph- kgc/ relea ses/ tag/2. 8.0, Morph-\nKGC v2.8.0.\nPage 15 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nthree experiments in Table  3). This experiment includes \na straightforward user input to be represented by a \nquery that uses a trivial SELECT <attribute(s)> FROM \n<table(s)> WHERE <condition>; structure to filter the \ndata. To accurately fulfill this task, the ID, name, and ori -\nentation of each DamageData entry with a “Crack” as \nDamageType and a “Stability” lower than “3” needed to \nbe selected. The objective was to assess the R2RML map -\npings’ ability to correctly transform straightforward rela -\ntional data into RDF triples without any complex joins, \nfunctions, or subqueries. The mappings were tested for \naccuracy in terms of both the syntax and the semantics of \nthe generated RDF triples.\nThe second experiment tested the models’ ability to \nrepresent SQL aggregate functions in the correspond -\ning R2RML mapping correctly. The task was to select all \ninspections conducted on a specified building and calcu -\nlate the number of damages associated with them, which \ncan be found in the natural language input in Table  3. \nFor this task, it was necessary for the LLM to use a sub -\nquery in the SQL query for calculating the damage num -\nber, meaning that a join had to be performed between \ntwo tables. However, this join nor any other table had \nto be explicitly included in the R2RML, as it can be rep -\nresented merely as a number that does not need to be \nassociated with other subjects from different tables and \ncan be instead treated like an additional column. How -\never, if the model included additional triples maps (e.g., \nto represent the damage explicitly), it was not marked as \nincorrect because it demonstrates an alternative but valid \napproach to representing the data. Even though the task \ncan be achieved without these additional triples maps, \ntheir inclusion shows a deeper understanding of how to \nmodel complex relationships in RDF.\nThe third experiment was the most complex. It focused \non joins, particularly one-to-many and many-to-many \njoins. The difficulty lies in accurately representing these \nrelationships in a graph-based format and explicitly \nadhering to the join rules of R2RML mappings. To ful -\nfill this task as intended, it was necessary to create triples \nmaps for the tables Buildings, Components, and \nDamageData, including predicate-object maps for each \nName column. Additionally, a predicate-object map with \na join condition for every one-to-many join was required \nto represent the relationships between the tables/entries. \nFinally, a triples map for the Components_Dam -\nages_join_table was necessary to represent the \nrelationship between components and damages for the \nmany-to-many join.\n6  Results\nThe results of the three experiments conducted are pre -\nsented in this section.\nEach one includes an assessment of the quality of gen -\nerated R2RML mappings. The SQL-Ontology mappings, \ngenerated SQL queries, and self-correction attempts are \ndiscussed rather broadly, as the focus lies on the in-depth \nevaluation of the R2RML mappings themselves.\n6.1  Experiment 1\nExperiment 1 focused on the generation of a simple \nmapping, including one database table, no joins, and \ntwo conditions. Results regarding syntax correctness \nand compliance with R2RML mapping rules show that \nall evaluated LLMs performed relatively stable for all \nprompting techniques. Figure 6 depicts the overall syntax \ncorrectness of all models across the four prompting tech-\nniques used. This correctness gives information about the \nmodels’ ability to generate valid R2RML mappings with -\nout yet taking the actual content and, thus, semantics \ninto account. An exemplary result for such a generated \nmapping can be found in Appendix C.\nOne notable finding within the zero-brief technique for \nmodel llama-3-70b-instruct is that the LLM was \nnot able to generate a valid R2RML mapping. This was \ndue to the usage of predicate-object maps for columns of \ndatabase tables that were not selected in the SQL query \nused as the logical source, hinting at a lack of under -\nstanding of how an R2RML processor processes these \nmappings when converting data to RDF triples.\nOther occasional mistakes included turtle syntax errors \n(such as a missing dot at the end of a triple) or prefixes \nof namespaces not declared but used in the mapping. \nRegarding the content of the generated mappings for all \nmodels, no errors were made concerning the usage of \ninvalid SQL queries as logical sources of a triples map, \nthe format of the response, incorrectly joining tables, or \nmissing data that should have been included. This may be \nTable 3 User input for the experiments\nExperiment Natural language user input\nExperiment 1 I need all the IDs, names, and orientations of damage data entries where the type of damage is a crack and the stability is lower than 3.\nExperiment 2 List all IDs and dates of inspections conducted on the building with ID 1, and for each inspection, specify the number of damages \ndetected.\nExperiment 3 Please provide me with the buildings and their components that have damage data with widespread impact. Include the ID and name \nfor each building, component and damage data.\nPage 16 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \ndue to the experiment’s task itself, as it was not necessary \nto include joins.\nHowever, several other sources of errors could be \nuncovered during the evaluation. Both GPT models \noccasionally added unnecessary elements to the map -\nping which were factually wrong (e.g., assigning an \nincorrect constant referring to the stability of a damage \nelement). Model gpt-4-turbo added, in most cases, \ncolumns Stability and DamageType as predicate-\nobject maps. Model Llama-3-70b-instruct failed \nto include columns in the SQL query used as the logi -\ncal source, resulting in those columns not being able to \nbe mapped. Most of these errors were made using the \nzero-detailed prompting technique. All models occasion -\nally added data types for the columns DamageDataId \nand Name, which were not classified as errors. OpenAI’s \ngpt-4o and Meta’s Llama-3.1-405b-instruct \nperformed best, achieving 100% correct syntax in all \nprompting scenarios. Figure  7 shows the number of mis -\ntakes made in each prompting setting classified as the \nerror categories defined in Sect. 4.4.\n6.2  Experiment 2\nThe second experiment assessed the models’ capability to \nrepresent SQL aggregate functions in R2RML mappings \naccurately. Figure  8 shows the percentage of valid map -\npings generated by the models.\nAll models performed significantly better in both \nfew-shot scenarios, specifically using the few-detailed \nmethod. Neither gpt-4-turbo nor llama-3-70b-\ninstruct were able to generate any valid mapping in \nthe zero-detailed setting. In contrast, all models pro -\nduced syntactically valid mappings for each run in the \nfew-detailed scenario (see an exemplary result in Appen -\ndix D).\nIn regards to the content of the mappings (see Fig.  9), \nmost notable is the fact that with every prompting \nmethod, all models, in most cases, added additional ele -\nments to the mappings that were not necessary to be \nincluded. This may be due to the inability to interpret the \nuser’s intent correctly, as only columns included in the \nquery’s SELECT clause need to be mapped. Instead of \nincluding the number of damages as a column to the tri -\nples maps for the Buildings entity, the models added \nan additional triples map representing the DamageData \nentities and, even further, occasionally, a join between \nthose two entities. This led to several errors, as either the \njoin or the triples maps were not defined correctly. This \nspecifically happened in the detailed prompting meth -\nods, whereas the GPT models performed better without \nthe detailed instructions. With Llama models, there was \nnot much difference between the prompting techniques. \nBecause of these additional triples maps, the SQL que -\nries chosen as the logical tables were, in most cases, not \nrestrictive enough, meaning data entries of the database \nwere also mapped even though this was not the user’s \nintent.\nFor model gpt-4-turbo another error similar to this \nwas the missing columns in the SELECT clause of the \nSQL query, leading to data that could not be mapped. \nIn all models (except Llama-3.1-405b-instruct), \nthe most prominent mistake was the absence of an SQL \nFig. 6 Experiment 1 — Syntax correctness of R2RML mappings\nPage 17 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nquery as a logical table, which was necessary to calcu -\nlate the number of damages per building. Models gpt-\n4-turbo and llama-3-70b-instruct struggled to \ncorrectly join the tables where, in most cases, the wrong \nID was chosen for the join condition (e.g., rr:parent \n“Inspections.InspectionId” instead of \nrr:parent “InspectionId”). Both models also \nscarcely used SQL as a logical table that did not correctly \ncalculate the number of damages for each (but rather the \nsum of all damages of the selected buildings), leading to \nincorrect data in the RDF triples. Model Llama-3.1-\n405b-instruct used invalid SQL queries as logical \ntables, which included columns of other tables not men -\ntioned in those queries. It also rarely included markdown, \nwhich led to the termination of the entire transformation \nFig. 7 Experiment 1 — Error allocation for each model\nPage 18 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nprocess, even though those generated mappings were \nsyntactically and semantically correct.\nOverall, model GPT-4o performed the best, specifi -\ncally in regards to the semantic quality of the mapping, \nas it produced most mappings that were either exactly \nmatching or similar to the proposed solution in the \nexperiment description.\n6.3  Experiment 3\nExperiment 3 was the most complex of the three, includ -\ning four table joins and one condition. Regarding the \nassessment of the generated mappings (see Fig.  10), only \nmodels gpt-4o and Llama-3.1-405b-instruct \nwere able to produce valid mappings with each prompt -\ning technique. In contrast, model llama-3-70b-\ninstruct struggled the most, only producing mappings \nin one of the prompting scenarios (few-brief), whereas \ngpt-4-turbo generated valid mappings for two of the \nfour prompting techniques. One of these valid mappings \nis displayed in Appendix E.\nExamining the quality of these mappings, it is notice -\nable that in contrast to the previous experiments, the \nsources of error now cluster around the incorrect use of \nR2RML mapping rules in every prompting method (see \nFig.  11). This is specifically the case for gpt-4-turbo \nand llama-3-70b-instruct models, hinting at a \nworse understanding of those rules than their respective \nsuccessor models. Those errors stem from the missing \ninclusion of columns in the logical table’s SQL query of \na triples map, as the R2RML processor is trying to map \ndata that is not included in the source. This is also the \ncase if column names were changed in the base query \nof the entire mapping using SQL’s AS statement. Still, if \nthe respective query is not included as a logical source, \neven though the column name change was adopted in the \nmapping, the conversion to RDF will fail. Model gpt-4-\nturbo also frequently used wrong IDs to join two tables/\ntriples maps, leading to an invalid mapping.\nAnother high error occurrence for this model is the \nover-inclusion of data, which happened whenever \nthe SQL query used for triple maps was not restric -\ntive enough and too many table entries were mapped. \nModel gpt-4o struggled with this problem as well in \nboth zero-shot scenarios. Regarding the Llama models, \nllama-3-70b-instruct specifically had trouble \ngenerating valid SQL queries used for the triples maps. \nThis was due to the selection of columns that belong \nto another table in the database, which in return led to \ninvalid logical tables. Like gpt-4-turbo, the model \nfrequently used wrong IDs for the joins and included \ntoo many data entries in the mappings. The latter \nerror was strongly observable in Llama-3.1-405b-\ninstruct using zero-shot methods. It is also notable \nthat all models occasionally used implicit joins instead \nof explicit joins (by using rr:joinCondition) and \ninstead used the IRI template of the triples map’s sub -\nject to be joined with. This is not inherently wrong, as \nit resulted in correct and valid joins in the RDF triples; \nFig. 8 Experiment 2 — Syntax correctness of R2RML mappings\nPage 19 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nhowever, it would be preferable to use explicit join con -\nditions when defining these mappings. Explicit joins \nprovide more precise and more maintainable mappings \nby explicitly specifying the relationships between dif -\nferent entities, making the mappings easier to under -\nstand. Overall, the best results were achieved with \nmodel gpt-4o in the few-brief prompting setting, \nclosely followed by Llama-3.1-405b-instruct.\n6.4  Additional findings\nResults for the SQL-Ontology mappings generated \nby the LLMs show that the models were able to gener -\nate syntactically valid mappings (meaning correct use of \nthe provided JSON template) in all runs across all three \nexperiments. All mappings included the correct tables \nand their respective columns of the base SQL query, as \nwell as the relationships between the tables. However, \nFig. 9 Experiment 2 — Error allocation for each model\nPage 20 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nthe models significantly struggled with hallucination of \nontology properties and classes. Whenever no suitable \nclass or property was found in the given ontology, the \nmodels opted to make up one based on the name of the \ntable or column. Instead of introducing a new names -\npace for these made-up classes/properties, they wrongly \nassigned them to the existing ontology namespaces. In \nsome cases, the models assigned classes or properties \nthat were not suitable to tables or columns (e.g., assigning \ndot:adjacentDamageElement to the column Ori-\nentation). Overall, the matching of ontology and SQL \nelements was not reliable enough.\nThe quality of R2RML strongly depended on the accu -\nracy of SQL queries. If the query was more complex than \nneeded (which happened the most in Experiment 2), it \nalso added another layer of difficulty to the mapping gen-\neration process for the LLM to solve. In general, all gen -\nerated SQL queries were valid and returned some data \nif run on the database. In very rare cases (with model \nllama-3-70b-instruct in Experiment 2), the SQL \nquery itself was either syntactically wrong or (happened \nas well with model gpt-4-turbo) calculated a wrong \nnumber.\nRegarding the use of self-correction attempts by the \nmodels, it can be noted that their integration into the \ntransformation process can have a significant impact on \nwhether or not a valid mapping is able to be produced \n(see Table  4). Even though most valid mappings were \ngenerated within the first request sent to the model (1st), \nthe attempt to self-correct (2nd) the generated mapping \nstill proved to be beneficial, in one case (for both Llama \nmodels in Experiment 3), even surpassing the number of \nvalid mapping generations of the first try. In most cases, \nthe errors corrected were syntactical ones regarding the \nTurtle syntax. This may be due to the error messages \nbeing more precise for the validation tool used for the \nRDF format rather than the tool used for R2RML map -\nping validation.\nThe execution times of the overall pipeline have been \nrecorded during all three experiments and are docu -\nmented in a boxplot diagram in Fig.  12 based on the 20 \nruns per setup. Therein, it is evident that based on the \ncomplexity of the experiment and, particularly, based \non the correction attempts undertaken as recorded in \nTable 4, also the execution time of the whole pipeline \nvaries. For instance, in llama-3-70b-instruct in \nExperiment 3, none of the first tries provided a solu -\ntion leading to added execution time, as seen in Fig.  12. \nFor comparability of process steps, the mean values of \nthe execution times of a singular process step of the \n20 iterations are calculated and reported in a stacked \nbar diagram in Fig.  13. The process steps add up to the \nentire run time depicted in Fig.  12, which is recorded \nfrom program start to program end, including the \nexecution time of approximately 15 s internal program \noperations that are marked with “program” and do not \nFig. 10 Experiment 3 — Syntax correctness of R2RML mappings\nPage 21 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \ndepend on the use of the LLMs. Moreover, the diagram \nalso incorporates the mean value of the retries (includ -\ning 0 s if a retry did not occur). It can be seen that the \nthree main LLM steps “sql” (Step 1: SQL query gen -\neration), “sqlont” (Step 2: Mapping SQL to Ontology), \nand “r2rml” (Step 3: Generating R2RML mapping) have \ncharacteristic execution times. Additionally, also the \nstep “rdf” (Step 4: Generation of RDF with R2RML pro -\ncessor) has characteristic execution times, which are \nFig. 11 Experiment 3 — Error allocation for each model\nPage 22 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nbarely visible in this figure because they were, in most \ncases, below 1 s.\nSome conclusions about the models’ performance can \nbe drawn from the execution times. However, this must \nbe viewed critically and evaluated in a broader study to \naccount for network or machine-dependent variances in \nthe execution times. Overall, in all three experiments, the \ngpt-4o model performed the fastest in the LLM process \nsteps, whereas the other three models vary from experi -\nment to experiment. With minimum execution times \nof ca. 30 s in the first experiment and maximum execu -\ntion times of more than 160 s in the third experiment, it \nis not yet possible to make a reliable assessment of the \nreal-time capability of the entire pipeline. Eventually, no \ninformation on the usage of tokens or costs and on the \nconsumption of energy or environmental impact is avail -\nable in this research, which would also be important for \nconsideration before implementing such approaches into \nthe industry.\n7  Discussion\nThe results of the experiments show that the evaluated \nLLMs are not yet able to reliably transform relational \ndata into R2RML mappings, which led to either incor -\nrect RDF graphs or invalid mappings. This is due to a lack \nof understanding and reasoning by the models of how \nR2RML exactly operates and processes relational data, \nspecifically regarding logical sources and joins between \ntables. The biggest issue lay in the models’ inability to \nrecognize the connection between SQL queries used as \nthe logical source and the mapping of columns based on \nthese SQL queries. Moreover, the LLMs often struggled \nwith correctly interpreting the structure of relational \ndatabases presented as SQL, especially when dealing \nwith a  complex schema involving multiple tables and \nrelationships. The models were prone to generating map-\npings that either oversimplified the relationships between \nentities or, more often, failed to capture the nuances of \nthe underlying database schema in the mapping, such as \nTable 4 Percentages of self-correction attempts for each model across all prompting techniques\nModels Experiment 1 Experiment 2 Experiment 3\n1st 2nd Fail 1st 2nd ail 1st 2nd Fail\ngpt-4-turbo 88.75 8.75 2.5 58.75 6.25 35 26.25 0 73.75\ngpt-4o 96.25 3.75 0 78.75 0 21.25 66.25 16.25 17.5\nllama-3-70b-instruct 52.5 21.25 30 45 3.75 51.25 0 13.75 86.25\nllama-3.1-405b-instruct 100 0 0 65 21.25 13.75 25 37.5 37.5\nFig. 12 Boxplot of the execution times of the three experiments for each model for the entire pipeline\nPage 23 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nforeign key constraints and many-to-many relationships. \nThis lack of understanding could potentially be improved \nby fine-tuning the models on more examples focused \nexplicitly on complex relational schema and R2RML \nmappings.\nA more resource-efficient approach could include even \nmore examples used in few-shot settings, as the results \nshowed that the models generally performed better in \nthose settings and were more stable, producing less vary -\ning mappings in all of the runs. Regarding the detail level \nof prompts, the models performed better without the \nadditional instructions for each task, specifically within \nthe R2RML mapping generation. One reason may be the \nreduction of distraction, minimizing the risk of misinter -\npretations of the tasks while allowing the LLM to focus \non producing the expected mapping. The more detailed \nprompt, while intended to help guide the model, may \nintroduce complexity that hinders the LLM’s ability to \ngenerate the best result. Nevertheless, especially Ope -\nnAI’s gpt-4o showed promising results even in use \ncases where multiple joins were necessary to generate an \nexpected valid result. It even recognized the underlying \nFig. 13 Stacked bar plot of the execution times (mean value) of the three experiments for each model per each of the execution steps\nPage 24 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nrelationship of tables related through a join table by \nexplicitly representing said relationship in the mapping \nitself. These results were primarily achieved in the few-\nshot setting.\nRegarding the SQL-Ontology mapping, there is still \nroom for improvement as the models frequently hallu -\ncinated ontology classes and properties. These halluci -\nnations occur when the models use ontology elements \nthat do not exist in the provided ontologies. This can lead \nto semantically inaccurate mappings, which may intro -\nduce errors in the data integration process and reduce \nthe overall integrity of the mappings. These hallucina -\ntions of classes and properties were detected in the cur -\nrent implementation only by the expert user; however, in \nfuture work, the hallucinated classes and properties could \nbe automatically identified by checking for definitions \nin the ontologies and their correct usage by also imple -\nmenting a reasoner at the end of the pipeline. Moreover, \nthe authors are aware that the accuracy of mappings is \ndepending on multiple factors in the overall pipeline. For \ncases where mapping is not fully accurate or cannot be \ncompleted, the pipeline could also be connected to ontol-\nogy repositories, for instance, the Linked Open Vocabu -\nlaries database6, to retrieve better fitting ontologies than \nthe ones from the original user input.\nAnother aspect is the generation of SQL that includes \nmore joins than necessary to fulfill the user’s intent. \nLooking at the results, most mistakes occurred when the \nquery was overly complex. However, even if this problem \ndoes not exist, it just goes to show that when the models \nare presented with complex SQL queries, their ability to \nproduce valid mapping diminishes. This underscores the \nproblems regarding the understanding of how mappings \nare being processed, as mentioned at the beginning. Also \nnotable from the evaluation results is the fact that small \nmistakes can lead to mappings not being valid and, there-\nfore, not being able to be used for conversion to RDF \ntriples. This means that the complexity of generating \nR2RML mappings requires exact precision, which LLMs \nare not yet capable of reliably performing.\nRegarding the approach of this research, its use is not \nyet recommendable in real-world construction sce -\nnarios, as the results are too arbitrary and thus not reli -\nable enough. Specifically for non-domain experts, this \ncould lead to the usage of potentially invalid or incor -\nrect R2RML mappings, which in return leads to RDF \ngraphs that may contain errors and false statements if \nthose errors in either the mapping itself or the resulting \nRDF data are not recognized as such. This unpredictabil -\nity could undermine the trustworthiness of the data and \ncompromise the integrity of systems relying on accurate \nRDF data. Moreover, as this presented approach only \nvalidates the syntax and adherence to R2RML mapping \nrules, it is necessary to validate the semantics of each \nmapping manually.\nHowever, the approach certainly has the potential to \nbe of help in the mapping generation process. In said use \ncases, it may be beneficial to incorporate this approach as \nan additional tool rather than a standalone solution. Uti -\nlization of it could minimize the time and effort it takes \nto generate R2RML mappings manually with no start -\ning point while still being able to adjust the mappings \nafter their generation to get a sense of what the map -\nping potentially could look like. This hybrid approach \nwould allow domain experts to leverage the automation \nbenefits without entirely relying on the system for the \nfinal output, thereby lowering the risks of using incor -\nrect or incomplete mappings. In light of these uncov -\nered findings and limitations, it is essential to refine the \napproach before considering its deployment in practical \napplications.\nWith the steady progression of the Semantic Web and \nthe adoption of the RDF standard, legacy databases may \nneed to be integrated into this evolving ecosystem. The \naforementioned hybrid approach aligns well with the cur-\nrent trends in Semantic Web development, specifically \nwithin the construction sector. By combining the power \nof LLMs with expert oversight, it is possible to drive for -\nward the mapping process while still maintaining high \naccuracy. This strategy not only reduces the manual \nworkload but also helps identify potential errors that an \nautomated system might overlook. Furthermore, this \nhybrid model can improve data quality and consistency \nacross projects, which is crucial in an industry where \neven minor mistakes could lead to additional costs or \nproject delays.\n8  Conclusion\nThe aim of this paper was to evaluate the abilities of \nLLMs in generating valid and semantically accurate \nR2RML mappings, based on which the transformation of \ntransportation infrastructure condition data from rela -\ntional databases into a semantically rich dataset in RDF \nas a standard format can be performed. The relevance of \nthis approach is given for the construction domain since \na lot of relevant data on buildings and infrastructure for \nefficient analysis is now unavailable, for instance, for \nuse in subsequent machine learning tasks for fault pre -\ndiction. With this research, the authors aim for a hybrid \n6 Linked Open Vocabularies, https:// lov. linke  ddata. es/ datas et/ lov/ vocabs, \nlast accessed: 13.02.2025.\nPage 25 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \napproach: maintaining the original data while integrating \nparts of the data using modular ontologies with other rel-\nevant information from other systems based on Semantic \nWeb technologies, which can be used for multiple pur -\nposes, such as analyses and semantic reasoning.\nThe research question of to what extent LLMs are \nsuitable for converting these data was answered by con -\nducting background research and a systematic analy -\nsis of the current status of the sub-tasks involved in the \ntransformation process within the experiment in this \npaper. These defined sub-tasks included LLM’s abilities \nto convert natural language input to SQL queries based \non a given database schema, mapping classes and prop -\nerties of ontologies to database tables and columns men -\ntioned in the SQL query, and using both the query and \nthe resulting mapping to generate R2RML mappings. \nR2RML mappings were used as an intermediate medium \nto overcome the black box of LLM data transformation \nand to reuse mappings consistently for multiple conver -\nsions over time instead of directly transforming to RDF \nusing the LLM. Results obtained from the conducted \nexperiments showed that the mapping generation is \nnot reliable enough to use in real-world use cases, as \nthe models often made mistakes regarding the mapping \nrules defined in the R2RML standard rooted in their lack \nof understanding of how these mappings are processed \nduring the conversion to RDF graphs. However, the fact \nthat some models were able to produce mappings that \nexactly matched the expected solution showed that there \nis potential for LLMs to be developed further to gener -\nate more reliable and accurate R2RML mappings. To be \nable to make more far-reaching statements about the \nreliability, the application of the developed pipeline to \nother data sources and ontologies must be examined in \nfurther research to ensure also the scalability and trans -\nferability of the approach.\nEventually, besides the reliability, the LLM query times, \nthe costs, and the environmental impact need to be fur -\nther considered for an industry application. In these \nterms, it needs to be critically questioned whether this \napproach should be chosen just because it is possible \nthrough LLMs, or whether other more resource-efficient \ndeterministic rule-based or AI-based approaches can \nbe used to create the mappings, leaving room for future \nresearch.\nThese findings open up possibilities for future work, \nwhich could include using the proposed methodology \nto evaluate models specifically fine-tuned on R2RML \nmappings, KGs represented in RDF/Turtle format, and \nontology schema matching. These models may perform \nmore stable than the ones evaluated in the experiments \nand produce better-quality mappings. Regarding the \nframework itself, based on the results presented here, the \nprompts used to instruct the LLMs could potentially be \nrefined by including common error sources to help the \nmodel avoid those mistakes. It could also be evaluated \nif using more few-shot examples could further improve \nthe mappings, especially when those examples include \nmultiple use-case scenarios. Even though the research \nquestion of this research was directed toward transform -\ning relational data, the framework can potentially be \nexpanded to RML mappings to allow another data format \nas the source of these mappings.\nPage 26 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nAppendix A LLM Prompts used in this research\nTable 5 Prompts — SQL query generation\nDetail level Prompt\nBrief Your task is to generate an SQL query based \non the user input and a given database schema. \nThis is the database schema: <DB SCHEMA> \nThis is the user input: <NLP INPUT> Do not use \ngrouping or sorting statements, and instead \nopt for subqueries. Do not use the all-selector \n*; instead, state all applicable column names \nexplicitly. Respond only with the SQL query. \nDo not include any code blocks, markdown, \nor explanations in your response.\nDetailed Your task is to generate an SQL query based \non the user input and a given database \nschema. The user input is given in natural \nlanguage and may include various types \nof requests, such as selecting specific columns, \nfiltering by conditions, joining multiple \ntables, and aggregating data. The database \nschema includes tables, columns, data types, \nprimary and foreign keys, and relationships \nbetween tables. Ensure the SQL query \naccurately reflects the user’s intent and adheres \nto the schema structure and constraints. This \nis the database schema: <DB SCHEMA> This \nis the user input: <NLP INPUT> Do not use \ngrouping or sorting statements, and instead \nopt for subqueries. Do not use the all-selector \n*; instead, state all applicable column names \nexplicitly. Respond only with the SQL query. \nDo not include any code blocks, markdown, \nor explanations in your response.\nTable 6 Prompts — SQL-Ontology mapping\nDetail level Prompt\nBrief Your task is to map elements of an SQL query \nto elements of one or more Turtle ontologies. \nUse the attached JSON template to describe \nthe mapping. This is the JSON mapping template: \n<JSON TEMPLATE> This is the SQL query: \n<QUERY> This is ontology 1: <ONTOLOGY \n1> Respond only with the JSON mapping. \nDo not include any code blocks, markdown, \nor explanations in your response.\nDetailed Your task is to map elements of an SQL \nquery to elements of one or more Turtle \nontologies. Use the attached JSON template \nto describe the mapping. This is the JSON \nmapping template: <JSON TEMPLATE> This \nis the SQL query: <QUERY> This is ontology 1: \n<ONTOLOGY 1> First, analyze the given SQL \nquery and identify the main tables, their columns, \nand the relationship between the tables. Then, \nanalyze the given ontologies and identify \nthe classes and properties. Match SQL elements \nto ontology classes and properties based on their \nmeaning and context. If suitable, prioritize classes \nand properties of the given ontologies. Use \nthe following steps as guidance:\n- In ´sqlQuery´, include the SQL query given in the \nuser prompt.\n- In ´sqlTables´, map each table \nto the corresponding ontology class and map \neach column to a corresponding ontology \nproperty.\n- In ´additionalColumnsorSubqueries´, map \nadditional or deviating columns/ subqueries \nto ontology properties.\n- In ´relationships´, map table relationships \nto ontology properties and include corresponding \nontology classes of source and target table \nof the relationship.\n- In ´additionalInformation´, briefly include any \nadditional information or extra details if necessary. \nRespond only with the JSON mapping. Do \nnot include any code blocks, markdown, \nor explanations in your response.\nPage 27 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nAppendix B JSON template used for SQL to ontology mapping\nPage 28 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nAppendix C Exemplary Result of Experiment 1\nPage 29 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nAppendix D Exemplary Result of Experiment 2\nPage 30 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nAppendix E Exemplary Result of Experiment 3\nPage 31 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nPage 32 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nAuthor contributions\nCRediT: Conceptualization: LH, SZ, PH; Data curation: SZ; Formal Analysis: LH; \nFunding acquisition:; Investigation: LH; Methodology: LH, PH; Project admin-\nistration:; Resources: SZ; Software: LH; Supervision: PH, MK; Validation: LH; \nVisualization: LH; Writing - original draft: LH, SZ, PH; Writing - review & editing: \nLH, SZ, PH, MK\nFunding\nNo funding was received.\nData availability\nData and code used in this paper are available upon request to the cor-\nresponding author.\nDeclarations\nCompeting interests\nThe author(s) declare(s) that they have no competing interests.\nReceived: 19 November 2024   Revised: 14 February 2025   Accepted: 25 \nMarch 2025\nReferences\nAbu Bakar, H., Razali, R., & Jambari, D. I. (2022). A qualitative study of legacy sys-\ntems modernisation for citizen-centric digital government. Sustainability. \nhttps:// doi. org/ 10. 3390/ su141 710951\nAmini, R., Norouzi, S.S., Hitzler, P ., & Amini, R. (2024). Towards complex ontology \nalignment using large language models. https:// arxiv. org/ abs/ 2404. 10329. \nAccessed 18 Nov 2024.\nArenas, M., Bertails, A., Prud’hommeaux, E., & Sequeda, J. (2012). A direct map-\nping of relational data to RDF. In: W3C Recommendation. Retrieved July 5, \n2024, from https:// www. w3. org/ TR/ rdb- direct- mappi ng/\nBeckett, D., Berners-Lee, T., Prud’hommeaux, E., & Carothers, G. (2014). RDF 1.1 \nTurtle. In: W3C Recommendation. Retrieved March 28, 2024, from https:// \nwww. w3. org/ TR/ turtle/\nBilal, M., Oyedele, L. O., Qadir, J., Munir, K., Ajayi, S. O., Akinade, O. O., Owolabi, H. \nA., Alaka, H. A., & Pasha, M. (2016). Big data in the construction industry: \nA review of present status, opportunities, and future trends. Advanced \nEngineering Informatics, 30(3), 500–521. https:// doi. org/ 10. 1016/j. aei. 2016. \n07. 001\nBorrmann, A., König, M., Koch, C., & Beetz, J. (Eds.). (2018). Building Information \nModeling: Technology Foundations and Industry Practice (1st ed.). Springer. \nhttps:// doi. org/ 10. 1007/ 978-3- 319- 92862-3\nBrickley, D., & Guha, R.V. (2014). RDF Schema 1.1. In: W3C Recommendation. \nRetrieved March 28, 2024, from https:// www. w3. org/ TR/ rdf11- schema/\nChang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., \nWang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P . S., Yang, Q., & Xie, X. (2024). A \nsurvey on evaluation of large language models. ACM Transactions on Intel-\nligent Systems and Technology. https:// doi. org/ 10. 1145/ 36412 89\nCodd, E. F. (1970). A relational model of data for large shared data banks. Com-\nmunications of the ACM, 13(6), 377–387. https:// doi. org/ 10. 1145/ 362384. \n362685\nDas, S., Sundara, S., & Cyganiak, R.(2012). R2RML: RDB to RDF Mapping Lan-\nguage. In: W3C Recommendation. Retrieved March 30, 2024, from https:// \nwww. w3. org/ TR/ r2rml/\nDe Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., & Rosati, R. (2018). Using \nontologies for semantic data integration. In S. Flesca, S. Greco, E. Masciari, \n& D. Saccà (Eds.), A comprehensive guide through the Italian database \nresearch over the last 25 years: studies in big data (Vol. 1, pp. 187–202). \nSpringer International Publishing and Imprint. https:// doi. org/ 10. 1007/ \n978-3- 319- 61893-7_ 11\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., \nSchelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., \nSravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, \nA., et al. (2024). The Llama 3 Herd of Models . https:// arxiv. org/ abs/ 2407. \n21783. Accessed 18 Nov 2024.\nEhrlinger, L., & Wöß, W. (2016). Towards a definition of knowledge graphs. In: \nInternational Conference on Semantic Systems. https:// ceur- ws. org/ Vol- \n1695/ paper4. pdf\nElmasri, R., & Navathe, S. B. (2016). Fundamentals of database systems—global \nedition (7th ed.). Pearson.\nTable 7 Prompts — R2RML mapping\nDetail level Prompt\nBrief Your task is to generate an R2RML mapping (using Turtle syntax) based on an SQL query and a mapping \nof SQL query elements to ontology classes/properties. This is the SQL-Ontology mapping: <SQL-\nONTOLOGY MAPPING> Respond only with the R2RML mapping. Do not include any code blocks, \nmarkdown, or explanations in your response.\nDetailed Your task is to generate an R2RML mapping (using Turtle syntax) based on an SQL query and a mapping \nof SQL query elements to ontology classes/properties. This is the SQL-Ontology mapping: <SQL-\nONTOLOGY MAPPING> The given mapping states how each table and column of a relational \ndatabase represented in the SQL query corresponds to the classes and properties of an ontology \n(which was given in Turtle format). Use the following steps as guidance:- Define triples maps for each \ntable including a logical table.- For each triples map define subject maps, including a template \nfor the subject’s IRI based on the primary key, and predicate-object maps for the columns of the table.- If \napplicable, define predicate-object maps for the relationships and joins between the tables. Respond \nonly with the R2RML mapping. Do not include any code blocks, markdown, or explanations in your \nresponse.\nTable 8 Prompt — Self-Correction\nPrompt\nYour previous task was to generate an <OUTPUT NAME> based on the user input and a given database schema. But your response resulted \nin an error. Please correct the <OUTPUT NAME> based on the given error message. This is the <OUTPUT NAME> that caused the error: \n<OUTPUT> This is the error message: <ERROR> Respond only with the corrected <OUTPUT NAME>. Do not include any code blocks, markdown, \nor explanations in your response.\nPage 33 of 34\nHöltgen et al. AI in Civil Engineering            (2025) 4:14 \n \nEmbers, S., Zentgraf, S., Herbers, P ., Faltin, B., Celik, F., König, M., Braun, J.-D., \nSteinjan, J., Schammler, D., Nieborowsk, S., & Holst, R.(2022). An artificial \nintelligence and mixed reality approach for optimizing the bridge inspec-\ntion workflow. In: Proceedings of the 2022 European Conference on Comput-\ning in Construction. Computing in Construction, (vol. 3. European Council \non Computing in Construction). https:// doi. org/ 10. 35490/ EC3. 2022. 195\nFreund, M., Dorsch, R., Schmid, S., Wehr, T., & Harth, A. (2025). Enriching RDF \ndata with LLM based named entity recognition and linking on embed-\nded natural language annotations. In S. Tiwari, B. Villazón-Terrazas, F. Ortiz-\nRodríguez, & S. Sahri (Eds.), knowledge graphs and semantic web lecture \nnotes in computer science (Vol. 15459, pp. 109–122). Springer Nature. \nhttps:// doi. org/ 10. 1007/ 978-3- 031- 81221-7_8\nFrey, J., Meyer, L.-P ., Arndt, N., Brei, F., & Bulert, K. (2023). Benchmarking the \nabilities of large language models for RDF knowledge graph creation and \ncomprehension: How well do LLMs speak turtle?. https:// arxiv. org/ abs/ 2309. \n17122. Accessed 18 Nov 2024.\nGöbels, A., Rivadeneyra, F., & Beetz, J. (2023). Transfer of implicit semi-formal \ntextual location descriptions in three-dimensional model contexts. In: \nProceedings of the 2023 European Conference on Computing in Construction \nand the 40th International CIB W78 Conference (vol. 4. European Council on \nComputing in Construction) .https:// doi. org/ 10. 35490/ EC3. 2023. 268\nGruber, T. R. (1993). A translation approach to portable ontology specifications. \nKnowledge Acquisition, 5(2), 199–220. https:// doi. org/ 10. 1006/ knac. 1993. \n1008\nGuo, J., Du, L., Liu, H., Zhou, M., He, X., & Han, S. (2023). GPT4Graph: Can large \nlanguage models understand graph structured data ? An Empirical evalua-\ntion and benchmarking. arXiv. https:// arxiv. org/ abs/ 2305. 15066. Accessed \n18 Nov 2024.\nHagedorn, P ., Liu, L., König, M., Hajdin, R., Blumenfeld, T., Stöckner, M., Billmaier, \nM., Grossauer, K., & Gavin, K. (2023). BIM-enabled infrastructure asset \nmanagement using information containers and semantic web. ASCE \nJournal of Computing in Civil Engineering. https:// doi. org/ 10. 1061/ (ASCE) \nCP . 1943- 5487. 00010 51\nHamdan, A.-H., Bonduel, M., & Scherer, R.J. (2019). An ontological model for the \nrepresentation of damage to constructions. CEUR Workshop Proceedings, \nAachen, Germany. In: Proceedings of the 7th Linked Data in Architecture and \nConstruction Workshop (LDAC). https:// ceur- ws. org/ Vol- 2389/ 05pap er. pdf\nHazber, M.A.G., Li, R., Xu, G., & Alalayah, K.M. (2016). An approach for automati-\ncally Generating R2RML-based direct mapping from relational databases. \nIn: Social computing. Communications in computer and information science \n(vol. 623, Springer. pp. 151–169). https:// doi. org/ 10. 1007/ 978- 981- 10- \n2053-7_ 15\nHeise, I., Göbels, A., Borrmann, A., & Beetz, J. (2024). Enabling comprehensive \nquerying of road and civil structure data using graph-based methods. In: \nProceedings of the 41st International Conference of CIB W78. https:// itc. scix. \nnet/ paper/ w78- 2024- 18\nHertling, S., & Paulheim, H. (2023). OLaLa: Ontology matching with large lan-\nguage models. In: Proceedings of the 12th Knowledge Capture Conference \n2023 (ACM, pp. 131–139). https:// doi. org/ 10. 1145/ 35872 59. 36275 71\nHogan, A., Blomqvist, E., Cochez, M., d’Amato, C., Melo, G.d., Gutierrez, C., Kir-\nrane, S., Gayo, J.E.L., Navigli, R., Neumaier, S., Polleres, A., Rashid, S., Rula, A., \nZimmermann, A., Schmelzeisen, L., Ngomo, A.-C.N., Sequeda, J., & Staab, \nS. (2022). Knowledge graphs. In: Synthesis lectures on data, semantics, and \nknowledge edn (Springer) https:// doi. org/ 10. 1007/ 978-3- 031- 01918-0\nHong, Z., Yuan, Z., Zhang, Q., Chen, H., Dong, J., Huang, F., & Huang, X. (2024). \nNext-generation database interfaces: a survey of LLM-based text-to-SQL. \nhttps:// arxiv. org/ abs/ 2406. 08426. Accessed 18 Nov 2024.\nHu, Y., Zhang, Z., & Zhao, L. (2023). Beyond text: A deep dive into large language \nmodels’ ability on understanding graph data. https:// doi. org/ 10. 48550/ \nARXIV. 2310. 04944. https:// arxiv. org/ abs/ 2310. 04944\nIglesias-Molina, A., Van Assche, D., Arenas-Guerrero, J., De Meester, B., \nDebruyne, C., Jozashoori, S., Maria, P ., Michel, F., Chaves-Fraga, D., & \nDimou, A. (2023). The RML ontology: A community-driven modular \nredesign after a decade of experience in mapping heterogeneous data to \nRDF. In: The Semantic Web – ISWC 2023 (Springer, pp. 152–175). https:// doi. \norg/ 10. 1007/ 978-3- 031- 47243-5_9\nJiang, B., Xie, Y., Hao, Z., Wang, X., Mallick, T., Su, W.J., Taylor, C.J., & Roth, D. \n(2024). A peek into token bias: Large language models are not yet genuine \nreasoners https:// arxiv. org/ abs/ 2406. 11050. Accessed 18 Nov 2024.\nJiang, J., Zhou, K., Dong, Z., Ye, K., Zhao, W.X., & Wen, J.-R. (2023). StructGPT: A \ngeneral framework for large language model to reason over structured data. \nhttps:// arxiv. org/ abs/ 2305. 09645. Accessed 18 Nov 2024.\nKambhampati, S. (2024). Can large language models reason and plan? Annals \nof the New York Academy of Sciences, 1534(1), 15–18. https:// doi. org/ 10. \n1111/ nyas. 15125\nKhalil, A., Stravoravdis, S., & Backes, D. (2021). Categorisation of building data in \nthe digital documentation of heritage buildings. Applied Geomatics, 13(1), \n29–54. https:// doi. org/ 10. 1007/ s12518- 020- 00322-7\nKlyne, G., Carroll, J.J., & McBride, B.(2004). Resource description framework \n(RDF): Concepts and abstract syntax. In: W3C Recommendation. Retrieved \nMarch 27, 2024, from https:// www. w3. org/ TR/ rdf10- conce pts/\nKnublauch, H., & Kontokostas, D. (2017). Shapes constraint language (SHACL): \nW3C recommendation 20 July 2017. Retrieved March 28, 2024, from \nhttps:// www. w3. org/ TR/ shacl/\nLin, W., Babyn, P ., Yan, Y., & Zhang, W. (2023). Context-based ontology model-\nling for database: Enabling ChatGPT for semantic database management. \nhttps:// arxiv. org/ abs/ 2303. 07351. Accessed 18 Nov 2024.\nLiu, L., Hagedorn, P ., & König, M.(2022). BIM-based organization of inspection \ndata using semantic web technology for infrastructure asset manage-\nment. In: Proceedings of the 1st Conference of the European Association on \nQuality Control of Bridges and Structures (Springer, pp. 1117–1126). https:// \ndoi. org/ 10. 1007/ 978-3- 030- 91877-4_ 127\nMarvin, G., Hellen, N., Jjingo, D., Nakatumba-Nabende, J. (2024). Prompt \nengineering in large language models. In: Data Intelligence and Cognitive \nInformatics (Springer, pp. 387–402). https:// doi. org/ 10. 1007/ 978- 981- 99- \n7962-2_ 30\nMayer, H. (2018). Digitalization of legacy building data - preparation of printed \nbuilding plans for the BIM process. In: Proceedings of the 7th International \nConference on Smart Cities and Green ICT Systems - SMARTGREENS (pp. \n304–310) https:// doi. org/ 10. 5220/ 00067 83103 040310\nMeester, B.D., Heyvaert, P ., & Delva, T.(2022). RDF mapping language (RML). \nIn: IDLab. Retrieved from April 4, 2024, from https:// rml. io/ specs/ rml/. \nAccessed 18 Nov 2024.\nMeier, A., & Kaufmann, M. (2019). SQL & NoSQL databases—models, languages, \nconsistency options and architectures for big data management. Springer. \nhttps:// doi. org/ 10. 1007/ 978-3- 658- 24549-8\nMeyer, L.-P ., Frey, J., Junghanns, K., Brei, F., Bulert, K., Gründer-Fahrer, S., & Martin, \nM. (2023). Developing a scalable benchmark for assessing large language \nmodels in knowledge graph engineering. https:// arxiv. org/ abs/ 2308. 16622. \nAccessed 18 Nov 2024.\nMinaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., & \nGao, J. (2024). Large language models: A survey. https:// arxiv. org/ abs/ 2402. \n06196. Accessed 18 Nov 2024.\nMinsky, M. (1974). A framework for representing knowledge. In: Techni-\ncal report, Massachusetts Institute of Technology, Artificial Intelligence \nLaboratory.\nModoni, G. E., Doukas, M., Terkaj, W., Sacco, M., & Mourtzis, D. (2017). Enhancing \nfactory data integration through the development of an ontology: from \nthe reference models reuse to the semantic conversion of the legacy \nmodels. International Journal of Computer Integrated Manufacturing, \n30(10), 1043–1059. https:// doi. org/ 10. 1080/ 09511 92X. 2016. 12687 20\nNaveed, H., Khan, A.U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., \nBarnes, N., & Mian, A. (2024). A comprehensive overview of large language \nmodels. https:// arxiv. org/ abs/ 2307. 06435. Accessed 18 Nov 2024.\nOpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., \nAlmeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, \nI., Balaji, S., Balcom, V., Baltescu, P ., Bao, H., Bavarian, M., Belgum, J., Bello, I., \nBerdine, J., et al. (2024). GPT-4 Technical Report. https:// arxiv. org/ abs/ 2303. \n08774. Accessed 18 Nov 2024.\nOpenAI: Completions. Retrieved August 14, 2024, from https:// platf orm. ope-\nnai. com/ docs/ api- refer ence/ compl etions. Accessed 18 Nov 2024.\nParciak, M., Vandevoort, B., Neven, F., Peeters, L.M., & Vansummeren, S. (2024). \nSchema matching with large language models: An experimental study. \nhttps:// arxiv. org/ abs/ 2407. 11852. Accessed 18 Nov 2024.\nPaulheim, H. (2016). Knowledge graph refinement: A survey of approaches \nand evaluation methods. Semantic Web, 8, 489–508. https:// doi. org/ 10. \n3233/ SW- 160218\nPauwels, P ., Costin, A., & Rasmussen, M.H. (2022). Knowledge graphs and \nlinked data for the built environment. In: Industry 4.0 for the Built \nEnvironment (Springer eBook Collection. Springer International and \nPage 34 of 34Höltgen et al. AI in Civil Engineering            (2025) 4:14 \nSpringer International Publishing, pp. 157–183). https:// doi. org/ 10. 1007/ \n978-3- 030- 82430-3_7\nPauwels, P ., Zhang, S., & Lee, Y.-C. (2017). Semantic web technologies in AEC \nindustry: A literature overview. Automation in Construction, 73, 145–165. \nhttps:// doi. org/ 10. 1016/j. autcon. 2016. 10. 003\nPeeters, R., & Bizer, C. (2024). Entity matching using large language models. \nhttps:// arxiv. org/ abs/ 2310. 11244. Accessed 18 Nov 2024.\nPeng, C., Xia, F., Naseriparsa, M., & Osborne, F. (2023). Knowledge graphs: \nOpportunities and challenges. Artificial Intelligence Review, 56(12), \n13071–13102. https:// doi. org/ 10. 1007/ s10462- 023- 10465-9.\nPregnolato, M., Gunner, S., Voyagaki, E., Risi, R., Carhart, N., Gavriel, G., Tully, \nP ., Tryfonas, T., Macdonald, J., & Taylor, C. (2022). Towards civil engineer-\ning 4.0: Concept, workflow and application of digital twins for existing \ninfrastructure. Automation in Construction, 141, 104421. https:// doi. org/ 10. \n1016/j. autcon. 2022. 104421\nRahm, E., & Bernstein, P . A. (2001). A survey of approaches to automatic \nschema matching. The VLDB Journal, 10(4), 334–350. https:// doi. org/ 10. \n1007/ s0077 80100 057\nRajkumar, N., Li, R., & Bahdanau, D. (2022). Evaluating the text-to-SQL capabilities \nof large language models. https:// arxiv. org/ abs/ 2204. 00498. Accessed 18 \nNov 2024.\nRasmussen, M. H., Lefrançois, M., Schneider, G. F., & Pauwels, P . (2021). BOT: \nThe building topology ontology of the W3C linked building data group. \nSemantic Web, 12, 143–161. https:// doi. org/ 10. 3233/ SW- 200385\nRatnayake, H., & Wang, C. (2024). A prompting framework to enhance \nlanguage model output. In: AI 2023: Advances in Artificial Intelligence \n(Springer, pp. 66–81). https:// doi. org/ 10. 1007/ 978- 981- 99- 8391-9_6\nReplicate: Meta//meta-llama-3.1-405b-instruct. Retrieved August 14, 2024, \nfrom https:// repli cate. com/ meta/ meta- llama-3. 1- 405b- instr uct. Accessed \n14 Aug 2024.\nReplicate: Meta/meta-llama-3-70b-instruct. Retrieved August 14, 2024, from \nhttps:// repli cate. com/ meta/ meta- llama-3- 70b- instr uct\nSchneider, M., Carroll, J., Herman, I., & Patel-Schneider, P . (2012). OWL 2 web \nontology language RDF-based semantics (Second Edition): W3C Recommen-\ndation 11 December 2012. Retrieved March 28, 2024, from https:// www. \nw3. org/ TR/ 2012/ REC- owl2- syntax- 20121 211/. Accessed 28 Mar 2024.\nSequeda, J., Allemang, D., & Jacob, B. (2023). A benchmark to understand the \nrole of knowledge graphs on large language model’s accuracy for question \nanswering on enterprise SQL databases. https:// arxiv. org/ abs/ 2311. 07509. \nAccessed 18 Nov 2024.\nSequeda, J.F. (2013). On the semantics of R2RML and its relationship with the \ndirect mapping. In: Proceedings of the 12th International Semantic Web \nConference (Posters & Demonstrations Track) (vol. 1035, pp. 193–196)\nSequeda, J. F. (2017). Integrating relational databases with the semantic web: A \nreflection. In L. Bertossi, W. Faber, B. Glimm, G. Gottlob, G. Ianni, D. Lembo, \n& S. Staab (Eds.), Reasoning web. Semantic interoperability on the web. Infor-\nmation systems and applications, incl. internet/web, and HCI (pp. 68–160). \nSpringer International Publishing and Imprint. https:// doi. org/ 10. 1007/ \n978-3- 319- 61033-7_4\nSheetrit, E., Brief, M., Mishaeli, M., & Elisha, O. (2024). ReMatch: Retrieval \nenhanced schema matching with LLMs. https:// arxiv. org/ abs/ 2403. 01567. \nAccessed 18 Nov 2024.\nSowa, J. F. (1984). Conceptual structures: Information processing in mind and \nmachine. Addison-Wesley Longman Publishing Co. Inc.\nStevens, N., Lydon, M., Campbell, K., Neeson, T., Marshall, A., & Taylor, S. (2020). \nConversion of legacy inspection data to bridge condition index (BCI) to \nestablish baseline deterioration condition history for predictive mainte-\nnance models. In: Civil Engineering Research in Ireland 2020. https:// sword. \ncit. ie/ ceri/ 2020/3/2\nSun, R., Arik, S., Muzio, A., Miculicich, L., Gundabathula, S., Yin, P ., Dai, H., Nak-\nhost, H., Sinha, R., Wang, Z., & Pfister, T. (2024). SQL-PaLM: Improved large \nlanguage model adaptation for Text-to-SQL (extended). https:// arxiv. org/ \nabs/ 2306. 00739. Accessed 18 Nov 2024.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, \nL., & Polosukhin, I. (2017). Attention is all you need. In: Proceedings 31st \nConference on Neural Information Processing Systems (NIPS 2017), Long \nBeach, CA, USA. arxiv1706.03762\nWang, H., Feng, S., He, T., Tan, Z., Han, X., & Tsvetkov, Y. (2023). Can language \nmodels solve graph problems in natural language? . https:// arxiv. org/ abs/ \n2305. 10037\nWang, S., Yan, J., Liu, Y., Hu, P ., Cai, H., & Jiang, L. (2024). Parallel construction of \nknowledge graphs from relational databases. In: PRICAI 2023: Trends in \nArtificial Intelligence (Springer, pp. 467–479). https:// doi. org/ 10. 1007/ 978- \n981- 99- 7019-3_ 42\nWang, Y., Tang, P ., Liu, K., Cai, J., Ren, R., Lin, J. J., Cai, H., Zhang, J., El-Gohary, N., \nBerges, M., & Golparvar Fard, M. (2023). Characterizing data sharing in civil \ninfrastructure engineering: Current practice, future vision, barriers, and \npromotion strategies. Journal of Computing in Civil Engineering. https:// \ndoi. org/ 10. 1061/ JCCEE5. CPENG- 5077\nWerbrouck, J., Pauwels, P ., Beetz, J., Verborgh, R., & Mannens, E. (2023). ConSolid: \nA federated ecosystem for heterogeneous multi-stakeholder projects. \nSemantic Web. https:// doi. org/ 10. 3233/ SW- 233396\nWu, X., & Tsioutsiouliklis, K. (2024). Thinking with knowledge graphs: Enhancing \nLLM reasoning through structured data. https:// arxiv. org/ abs/ 2412. 10654. \nAccessed 18 Nov 2024.\nZhang, B., Ye, Y., Du, G., Hu, X., Li, Z., Yang, S., Liu, C.H., Zhao, R., Li, Z., & Mao, H. \n(2024). Benchmarking the text-to-SQL capability of large language models: \nA comprehensive evaluation. https:// arxiv. org/ abs/ 2403. 02951. Accessed \n18 Nov 2024.\nZhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, \nJ., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, \nX., Liu, Z., Liu, P ., Nie, J.-Y., & Wen, J.-R. (2023). A survey of large language \nmodels. https:// arxiv. org/ abs/ 2303. 18223. Accessed 18 Nov 2024.\nZhu, Y., Wang, X., Chen, J., Qiao, S., Ou, Y., Yao, Y., Deng, S., Chen, H., & Zhang, \nN. (2023). LLMs for knowledge graph construction and reasoning: Recent \ncapabilities and future opportunities. https:// arxiv. org/ abs/ 2305. 13168. \nAccessed 18 Nov 2024.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5680063366889954
    },
    {
      "name": "Natural language processing",
      "score": 0.34974202513694763
    }
  ]
}