{
  "title": "Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds",
  "url": "https://openalex.org/W4312546175",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2899129408",
      "name": "Chenhang He",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2964895005",
      "name": "Ruihuang Li",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2095659280",
      "name": "Shuai Li",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2082114532",
      "name": "Lei Zhang",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6756795685",
    "https://openalex.org/W3034314779",
    "https://openalex.org/W3205005447",
    "https://openalex.org/W4214520160",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W3035172746",
    "https://openalex.org/W2914821954",
    "https://openalex.org/W6763422710",
    "https://openalex.org/W2964062501",
    "https://openalex.org/W6739778489",
    "https://openalex.org/W3167539120",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W4214526701",
    "https://openalex.org/W4214624153",
    "https://openalex.org/W4214777292",
    "https://openalex.org/W3130463448",
    "https://openalex.org/W3166470370",
    "https://openalex.org/W2988715931",
    "https://openalex.org/W2963727135",
    "https://openalex.org/W3034602892",
    "https://openalex.org/W2963400571",
    "https://openalex.org/W2968296999",
    "https://openalex.org/W6755477022",
    "https://openalex.org/W2558294288",
    "https://openalex.org/W3167732492",
    "https://openalex.org/W2951517617",
    "https://openalex.org/W6754918364",
    "https://openalex.org/W3178887474",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3003618643",
    "https://openalex.org/W2555618208",
    "https://openalex.org/W3118341329",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W3136022415",
    "https://openalex.org/W3113028524",
    "https://openalex.org/W3111535274",
    "https://openalex.org/W6786280728",
    "https://openalex.org/W2981949127",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W4214755140",
    "https://openalex.org/W2897529137",
    "https://openalex.org/W3034428269",
    "https://openalex.org/W3035346742",
    "https://openalex.org/W2798965597",
    "https://openalex.org/W4287725215",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3117804044",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W3031752193",
    "https://openalex.org/W2949708697",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3107819843",
    "https://openalex.org/W2560609797"
  ],
  "abstract": "Transformer has demonstrated promising performance in many 2D vision tasks. However, it is cumbersome to compute the self-attention on large-scale point cloud data because point cloud is a long sequence and unevenly distributed in 3D space. To solve this issue, existing methods usually compute self-attention locally by grouping the points into clusters of the same size, or perform convolutional self-attention on a discretized representation. However, the former results in stochastic point dropout, while the latter typically has narrow attention fields. In this paper, we propose a novel voxel-based architecture, namely Voxel Set Transformer (VoxSeT), to detect 3D objects from point clouds by means of set-to-set translation. VoxSeT is built upon a voxel-based set attention (VSA) module, which reduces the self-attention in each voxel by two cross-attentions and models features in a hidden space induced by a group of latent codes. With the VSA module, VoxSeT can manage voxelized point clusters with arbitrary size in a wide range, and process them in parallel with linear complexity. The proposed VoxSeT integrates the high performance of transformer with the efficiency of voxel-based model, which can be used as a good alternative to the convolutional and point-based backbones. VoxSeT reports competitive results on the KITTI and Waymo detection benchmarks. The source codes can be found at https://github.com/skyhehe123/VoxSeT.",
  "full_text": "Voxel Set Transformer: A Set-to-Set Approach to\n3D Object Detection from Point Clouds\nChenhang He, Ruihuang Li, Shuai Li, Lei Zhang*\nThe Hong Kong Polytechnic University\n{csche, csrhli, csshuaili, cshzeng}@comp.polyu.edu.hk\nAbstract\nTransformer has demonstrated promising performance\nin many 2D vision tasks. However, it is cumbersome to\ncompute the self-attention on large-scale point cloud data\nbecause point cloud is a long sequence and unevenly dis-\ntributed in 3D space. To solve this issue, existing meth-\nods usually compute self-attention locally by grouping the\npoints into clusters of the same size, or perform convolu-\ntional self-attention on a discretized representation. How-\never, the former results in stochastic point dropout, while\nthe latter typically has narrow attention fields. In this pa-\nper, we propose a novel voxel-based architecture, namely\nVoxel Set Transformer (VoxSeT), to detect 3D objects from\npoint clouds by means of set-to-set translation. VoxSeT\nis built upon a voxel-based set attention (VSA) module,\nwhich reduces the self-attention in each voxel by two cross-\nattentions and models features in a hidden space induced\nby a group of latent codes. With the VSA module, VoxSeT\ncan manage voxelized point clusters with arbitrary size\nin a wide range, and process them in parallel with lin-\near complexity. The proposed VoxSeT integrates the high\nperformance of transformer with the efficiency of voxel-\nbased model, which can be used as a good alternative\nto the convolutional and point-based backbones. VoxSeT\nreports competitive results on the KITTI and Waymo de-\ntection benchmarks. The source codes can be found at\nhttps://github.com/skyhehe123/VoxSeT.\n1. Introduction\nObject detection from 3D point cloud has been receiving\nextensive attention as it empowers many applications like\nautonomous driving, robotics and virtual reality. Unlike 2D\nimages, 3D point clouds are naturally sparse and unevenly\ndistributed in continuous space, impeding the CNN layers\nfrom being directly applied. To resolve this issue, some ap-\nproaches [5, 10, 42, 49, 51] first transform the point cloud\n*Corresponding author.\nSelf-attention\n‚ãØ\nCross-attention\nCross-attention\n(a)\nSparse-conv\nDrop Pad\nDummy \npoint\nVoxelization\nSelf-attention\n(b) (c)\nConvFFN\nFigure 1. The illustrations of (a) grouping-based, (b)\nconvolutional-based and (c) our proposed induced set-based atten-\ntion mechanisms.\ninto a discrete representation and then apply CNN models\nto extract high dimensional features. Another class of ap-\nproaches [27, 33, 41, 44, 45] model the point cloud in con-\ntinuous space, where the multi-scale features are extracted\nthrough interleaved grouping and aggregation steps.\nBeyond the above two schemes, transformer-based mod-\nels [9, 21‚Äì23, 31, 47] have recently attracted great interest\nin processing point cloud data as the self-attention used in\ntransformers is invariant to permutation and cardinality of\nthe input components, which makes transformer an appro-\npriate choice for point cloud processing. The main limita-\ntion of transformer models, however, lies in that the self-\nattention computation is quadratic. Each token has to be\nupdated by using all the other tokens from previous layers,\nmaking self-attention intractable for long sequence point\nclouds. Point Transformer [47] builds transformers upon a\nPointNet [28] architecture, which hierarchically groups the\npoint cloud data into different clusters and computes self-\nattention in each cluster. CT3D [31] presents a two-stage\npoint cloud detector, where 3D RoIs are extracted to group\nthe raw points in the first stage and transformers are applied\nto the grouped points in the second stage.\nHowever, since the distribution of point clouds is ex-\ntremely uneven, the number of points in each cluster varies\na lot. To enable the self-attention to run in parallel, cur-\n8417\nThe following publication C. He, R. Li, S. Li and L. Zhang, \"Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds,\" \n2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 2022, pp. 8407-8417 is available at  \nhttps://doi.org/10.1109/CVPR52688.2022.00823.\nThis is the Pre-Published Version.\n¬© 2022 IEEE.  Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, \nincluding reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to  \nservers or lists, or reuse of any copyrighted component of this work in other works.\nrent approaches [23, 31, 47] balance the token number in\neach cluster by stochastically dropping points or padding\ndummy points (see Figure 1(a)). This results in unsta-\nble detection results and redundant computations. Besides,\neach operation of grouping n points to m clusters will cost\nO(nm) complexity, which is relatively intensive. Alterna-\ntively, V oxel Transformer [21] performs self-attention on\na discrete voxel grid, as depicted in Figure 1(b). It com-\nputes self-attention in a convolutional manner and hence\nis as efficient as sparse convolution with O(n) complex-\nity. However, since convolutional attention is a point-wise\noperation, to save the memory, the attention field of the con-\nvolutional kernel is typically small, thus hindering the voxel\ntransformer to model long-range dependencies. It is worth\nmentioning that though Group-free [20] and 3DETR [22]\npresent a promising solution by computing self-attention on\na reduced set of seed points, this solution is only applicable\nto indoor scenes, where the point clouds are relatively dense\nand concentrated. Considering that the point clouds of out-\ndoor scenes are typically sparse, large-scale ( e.g., > 20k),\nand unevenly distributed, the scale and coverage of seed\npoints remain an issue.\nTo address the above issues, we introduce a voxel-based\nset attention (VSA) module. For each VSA, we divide the\nwhole scene into non-overlapping 3D voxels and compute\nthe voxel indices of the input point with instant efficiency.\nWe use these voxels to determine the attentive region which\nis analogous to the window attention in SwinTransformer\n[19]. Unlike image, LiDAR has irregular structures, and\nthe resulting attention groups have different lengths, which\nhinders the parallelization of the model.\nInspired by the induced set transformer [13], we assign a\ngroup of trainable ‚Äúlatent codes‚Äù to each voxel. These latent\ncodes build a fixed-length bottleneck for the point cloud,\nthrough which the information from input points within the\nvoxel can be compressed to a static hidden space. This\nformulation is based on the key observation that the self-\nattention matrix is typically low-rank, and hence we can\ndecompose an intensive full self-attention into two consecu-\ntive cross-attention modules. As shown in Figure 1(c), VSA\nfirst transforms the latent codes, which serve as queries, to\na hidden space by attending to the projected features, i.e.,\nkeys and values, from the input points. The transformed\nhidden features, which encode the context information of\nthe input points in each voxel, are enriched by a convolu-\ntional feed-forward network, in which the features across\nvoxels exchange their information in spatial domain. Af-\nter that, the hidden features are attentively fused with in-\nput, producing output features of the input resolution. By\nleveraging the latent codes, the cross-attention performed\nin all voxels can be vectorized, making VSA a highly par-\nallel module. Given n d-dimensional input features and k\nlatent codes, VSA has a complexity of O(nkd) and it can\nbe implemented with general matrix multiplications.\nWith VSA, we propose a V oxel Set Transformer\n(V oxSeT) to detect 3D objects by learning point cloud fea-\ntures in a set-to-set translation process. V oxSeT is com-\nposed of VSA modules, MLP layers and a shallow CNN for\nBirds-Eye-View (BEV) feature extraction. To verify the ef-\nfectiveness of the proposed model, we conduct experiments\non two 3D detection benchmarks, KITTI and Waymo open\ndataset. V oxSeT achieves competitive performance with\ncurrent state-of-the-arts. In addition, the proposed VSA\nmodule can be seamlessly adopted into point-based detec-\ntors such as PointRCNN [33], and demonstrates advantages\nover the set abstraction module.\nIn summary, in this work we first invent a voxel-based set\nattention module, which can model long-range dependen-\ncies from the token cluster of arbitrary size, bypassing the\nlimitation of current grouped-based and convolution-based\nattention modules. We then present a V oxel Set Transformer\nto learn point cloud features effectively by leveraging the\nsuperiority of transformer on large-scale sequential data.\nOur work provides a novel alternative to the current con-\nvolutional and point-based backbones for 3D point cloud\ndata processing.\n2. Related work\n2.1. 3D object detection from point clouds\nEarly approaches on 3D object detection from point\ncloud can be categorized into two classes. The first class of\nmethods transform the point cloud into more compact rep-\nresentations, e.g., Birds-Eye-View (BEV) images [3,11,34],\nfrontal-view range images [2, 7, 18], and volumetric fea-\ntures [14, 43, 51]. Yan et al. [42] developed a sparse con-\nvolutional backbone to efficiently process the point clouds\nby encoding the point clouds into a 3D sparse tensor. Lang\net al. [12] further accelerated the detection rate by stack-\ning the voxel features as a ‚Äúpillar‚Äù and using 2D CNN to\nprocess. Another class of methods [25, 27, 33, 44, 45] pro-\ncess the point cloud in a continuous space by employing\na PointNet [29] architecture. The point-wise features in\nmulti-scales are extracted in stages with interleaved group-\ning and sampling operations. Shi et al. [33] and Yang et\nal. [45] proposed to generate 3D RoIs from PointNet out-\nputs and apply the RoIs to group point-wise features for\nfurther refinement. Qi et al. [26] proposed a deep voting\nmethod to cluster the points from objects‚Äô surface to de-\ntect the object with insufficient points. Unlike compact rep-\nresentations, point-wise features preserve more details and\nfine-grained structures of original point clouds. Based on\nthis fact, some approaches [10, 32] employ a hybrid repre-\nsentation in both point and voxel spaces to achieve more\nreliable detection outputs.\nOur proposed architecture is largely motivated by voxel-\n8418\n\nbased approaches. We partition the point cloud into voxel\ngrid and execute self-attention locally, endowing our model\nwith inductive bias and computational efficiency.\n2.2. Transformer in point cloud analysis\nRecently, Transformer [38] has demonstrated its great\nsuccess in many computer vision tasks such as image classi-\nfication [6, 37], 2D object detection [1, 19], and other dense\nprediction tasks [30, 48]. For point cloud analysis, Zhao et\nal. [47] proposed a novel subtraction attention based oper-\nator for point cloud classification and segmentation. Guo\net al. [9] investigated a dual attention to process the point\nclouds in feature and edge space. Misraet al. [22] and Liuet\nal. [20] used transformer to process point clouds as sequen-\ntial data, preventing the models from stacking hierarchical\ngrouping and sampling modules. Miao et al. [21] embeded\nself-attention into a sparse convolutional kernel. Sheng et\nal. [31] built the transformer on top of a two-stage detector\nand operated attention on the points grouped by RoIs.\nUnlike the above approaches that perform self-attention\non a fixed-size token cluster, our proposed V oxel Set Trans-\nformer leverages the idea of induced set transformer [13] to\ndecompose self-attention into two cross-attentions, making\nit possible to perform self-attention on the token clusters of\narbitrary size.\n3. Methodology\n3.1. Preliminary\nIt is prohibitive to directly apply self-attention on point\ncloud data due to its quadratic computational complexity.\nTo bypass the issue, an induced set attention block was pro-\nposed in [13], where the full self-attention in a set was ap-\nproximated by two reduced cross-attentions induced by a\ngroup of latent codes. Given an input set X ‚àà Rn√ód of\nsize n with dimension d and k latent codes L ‚àà Rk√ód, the\noutput set O ‚àà Rn√ód from the induced set attention block\ncan be formulated as\nH = CrossAttention(L, X) ‚àà Rk√ód, (1)\nÀÜH = FFN(H) ‚àà Rk√ód, (2)\nO = CrossAttention(X, ÀÜH) ‚àà Rn√ód. (3)\nThe first cross attention transforms the latent features L\ninto hidden features H by attending to the input set. This\nstep costs O(nkd) complexity, which is linear to n as the\nnumber of latent codes k is fixed and usually very small.\nThe transformed hidden features contain information about\nthe input set X and then they are updated by a point-\nwise feed-forward network (FFN). This point-wise opera-\ntion costs O(k) complexity and it learns highly semantic\nfeatures from the input set. The second cross attention at-\ntends the input set to the resulting hidden features, which\ncosts O(nkd) complexity, producing an output set of length\nn. The induced set attention is based on the assumption\nthat the self-attention can be approximated with low-rank\nprojections, thus the self-attention can be regarded as per-\nforming a k-clustering on the inputs where the latent codes\nserve as cluster centers. This is also analogous to the clus-\ntered attention [39] and Linformer [40], where the input set\nis explicitly reduced with linear projection.\n3.2. Voxel-based Set Attention (VSA)\nUnlike images, point clouds are widely distributed and\nhave weak semantic associations in scene level, while they\nhave strong structural details in the local region. Instead\nof compressing all the input points into a hidden space, we\nmodify the above induced set attention to be performed lo-\ncally. Specifically, we partition the scene into a voxel grid\nand assign a set of latent codes to each voxel. We refer to\nthe module as Voxel-based Set Attention (VSA).\nScatter kernel function.As mentioned before, VSA is\na highly parallel module, where the operations across vox-\nels can be vectorized. This vectorization can be achieved\nby the scatter function1, which is a cuda kernel library that\nperforms symmetric reduction, e.g., sum, max and mean,\non different segments of a matrix. In our case, we regard\nthe input set as a single matrix, each row of which corre-\nsponds to a point-wise feature, and its belonging voxel can\nbe indexed by a table of voxel coordinates.\nLet {pi = (xi, yi, zi) : i = 1, ...n} denote the coor-\ndinates of point cloud and [dx, dy, dz] be the voxel size in\nthree dimensions. The voxel coordinates V can be com-\nputed by V = {Vi = (‚åäxi\ndx\n‚åã, ‚åä yi\ndy\n‚åã, ‚åä zi\ndz\n‚åã) : i = 1, ..., n},\nwhere ‚åä¬∑‚åã is the floor function. Hence, given point-wise in-\nput features {Xi : 1 = 1, ...n}, their reduced voxel-wise\nform {Yj : j = 1, ..., m} after a symmetric function F (¬∑)\ncan be represented as:\nY = {F({Xi : Vi = j}) :j = 1, ..., m} (4)\nwhere m is the number of non-empty voxels. With scatter\nfunction F scatter, the above equation can be written in a\nvectorized form, i.e.,\nY = Fscatter(X, V). (5)\nBy deploying VSA, we do not need to stochastically drop\nor pad the points in each voxel and the complexity of the\nmodel is linear.\nIn Figure 2, we illustrate the VSA in a matrix-\nmultiplication form for ease of comprehension. As can be\nseen, the module is analogous to an encoder-decoder archi-\ntecture, where the input set is encoded to a hidden space,\nthen the hidden features are refined through a ConvFFN and\nfinally decoded to produce the output set.\n1https://github.com/rusty1s/pytorch scatter\n8419\n\nn √ó d\nConvFFN\nkey, value = nn.linear(x), nn.linear(x)\nattn = torch.einsum(‚Äònd, kd->nk‚Äô, key, query)\nattn = scatter_softmax(attn, indices)\nh = torch.einsum(‚Äònd, nk-> nkd‚Äô, value, attn)\nkey, value = nn.linear(h), nn.linear(h)\nattn = torch.einsum(‚Äònkd, nd-> nk‚Äô, key, query)\nattn = torch.softmax(attn, dim=1)\nout = torch.einsum(‚Äònk, nkd-> nd, attn, value)\nVQV\nLatent codes (query)\nInput feature\nOutput feature\nhr = scatter_sum(h, indices)\nhr = ConvTensor(hr, voxel_coords)\nhr = nn.Conv(hr)\nh = hr[indices, ‚Ä¶] (broadcast)\nn √ó k n √ó d\nn √ó k √ód\nm √ó k √ód\nn √ó\nd\nn √ókn √ó\nd\n+\nùë£ùëñ ùë£ùëñ\nk √ód\nKK\nEncoder Decoder\nLinear projection\nConv\nm √ó k √ód\nn √ó k √ódn √ó k √ód n √ó k √ód\nFigure 2. The matrix multiplication used in implementing the voxel-based set attention (VSA) module. The pseudo codes in PyTorch-stype\nof each step are presented bellow the diagram.\nEncoder. In the encoder, we first project the input fea-\ntures from the previous module to the key K ‚àà Rn√ód and\nthe value V ‚àà Rn√ód with linear projections, respectively.\nNext, we perform cross attention between the key and the\nlatent codes (query) L ‚àà Rk√ód, producing the attention\nmatrix A ‚àà Rn√ók√ód. The attention matrix A is then nor-\nmalized voxel-wisely to obtain ÀúA, and multiplied with the\nvalue, producing hidden features ÀúH. The calculation of ÀúH\ncan be formulated as:\nÀúA = Softmaxscatter(A, V), A = KLT , (6)\nH = ÀúAT V. (7)\nAfter that, we perform voxel-wise reduction on the hidden\nfeatures based on the voxel indices V:\nHr = Sumscatter(H, V). (8)\nThe overall computations in the encoder include two\nGEMMs and two scatter operations. The overall complex-\nity of the encoder is O(2n(k + 1)d).\nIt is worth mentioning that the cross-attention based en-\ncoding scheme can be viewed as an extension of voxel fea-\nture encoding (VFE) used in [12, 42, 51]. The difference\nis that VFE encodes the points within a voxel into a single\nfeature vector, while our scheme encodes the points based\non a codebook consisting of the latent features. Owing to\nthe high expressive power of VSA, we can use a relatively\nlarge voxel size to capture the features in a wide range.\nConvolutional feed-forward network. The core idea\nof VSA is to encode the region-wise features into a hidden\nspace using latent codes. The hidden features work as a\nbottleneck, through which we apply a ConvFFN to achieve\nmore flexible and complex information update. Unlike con-\nventional FFN that only performs point-wise token update,\nConvFFN enables the information exchange accross voxels,\nwhich is especially important for dense prediction. To adap-\ntively integrate voxel features with global dependencies, we\nscatter the reduced hidden features into a 3D sparse ten-\nsor based on their voxel coordinates Cr, and then conduct\ntwo depth-wise convolutions (DwConv) on them to enforce\nthe feature interaction in spatial domain. Given convolu-\ntional weights W1 and W2, the enriched hidden features\nÀÜHr ‚àà Rm√ók√ód from ConvFFN can be written as:\nÀÜHr = DwConv(œÉ(DwConv(T (Hr, Cr); W1)); W2), (9)\nwhere œÉ denotes the non-linear activation, T denotes the\nformulation of sparse tensor, and the number of groups\nin DwConv equals to that of latent features. This opera-\ntion costs O(HWDkd\ndxdydz\n) complexity, where H, W, Drefer to\nthe point cloud range in three directions, respectively, and\n[dx, dy, dz] specifies the voxel size. The ConvFFN plays\nan important role in VSA as it introduces desirable induc-\ntive bias and global context to the module. More studies on\nConvFNN will be discussed in Sec 4.5.\nDecoder. The decoder reconstructs the output set from\nthe enriched hidden features ÀÜHr. Specifically, we first\nbroadcast the hidden features based on the voxel indices V,\nproducing ÀÜH ‚àà Rn√ók√ód which has the same length as the\ninput set. Then we generate the query, key-value pair from\nthe input set and the hidden features, respectively, with lin-\near projections. Given matrices of query Q ‚àà Rn√ód, key\nK ‚àà Rn√ók√ód and value V ‚àà Rn√ók√ód, the decoder output\nO can be calculated as:\nA = [A1, ...,An] = [K1QT\n1 , ..., KnQT\nn ], (10)\nÀúA = [Softmax(A1), ...,Softmax(An)], (11)\nO = [O1, ..., On] = [ÀúAT\n1 V1, ...,ÀúAT\nn Vn] (12)\nThe overall computational complexity of the decoder is\nO(2nkd). Owing to the flexibility of cross-attention mech-\nanism, VSA formulates the point cloud processing as a set-\nto-set translation problem.\nRelative position embedding. As discussed in [23, 29],\npreserving the local structure of point cloud is crucial to\n8420\n\n(VSA+ MLP) √óùüí\nPE(‚àô) Points-to-BEV \nTransformation\nconv cls\nreg\nMLP\nup\nconv\nVoxel \nIndices\nSoft pool\n2D CNN (shallow)\nMLP\nFigure 3. The overall architecture of the proposed V oxel Set Transformer (V oxSeT).\nimprove performance. Therefore, we introduce a Positional\nEmbedding (PE) module to encode the local coordinates of\nthe point clouds within a voxel to a high dimensional feature\nand inject them into each VSA module. Specifically, the PE\nmodule applies the Fourier parameterization to take values\n[sin(fkœÄx), cos(fkœÄ, x)], given the normalized local coor-\ndinates x ‚àà [0, 1] and the kth frequency fk with bandwidth\nL. The resulted Fourier embedding, which has a dimension\nof 3L, is further mapped to the input dimension of the first\nMLP module through a learnable linear layer.\n3.3. Voxel Set Transformer (VoxSeT)\nThe overall architecture of V oxSeT is illustrated in Fig-\nure 3. Following the traditional transformer paradigm, the\nV oxSeT backbone is composed of inter-connected multi-\nlayer perception (MLP) and VSA modules. We use batch\nnorm as the normalization layer and wrap each VSA mod-\nule into a residual block for optimal gradient flow.\nUnlike grouping-based approaches [25,29] that progres-\nsively downsample and aggregate point-wise features for\ncontext extraction, our backbone extracts point cloud fea-\ntures as a set-to-set translation process. The semantic level\nof the features is controlled by the size of voxels in the VSA\nmodule. We empirically found that applying large voxels\ncan learn richer context information, and demonstrate bet-\nter understanding of the objects with sparse points, espe-\ncially pedestrian and cyclist instances. We will present the\nsettings of VSA modules in Sec. 4.1.\nBirds-eye-view feature encoding. In point cloud de-\ntection, a common phenomenon is that models using dense\nbirds-eye-view (BEV) features [4, 5, 42] generally achieve\nhigher recall than those using sparse point-wise features\n[33, 45]. In this regard, we encode the point-wise features\nfrom the backbone into a BEV representation and apply a\nshallow CNN to increase the feature density. The CNN has\nonly two strides, each involving three convolutions. The\nconvolutional features of two strides are finally concate-\nnated and passed to the detection head for bounding-box\nprediction. To generate BEV features, we aggregate the\npoint-wise features within a pillar of size 0.36m √ó0.36m\nand apply a ‚Äúsoft-pooling‚Äù operation to produce features in\nBEV . Given point-wise output featuresXj ‚àà Rk√ód in the\njth pillar, the pillar-wise features after pooling Fj can be\nformulated as:\nFj =\nkX\nm=1\nXj\nm ‚àó wj\nm, wj\nm = eXj\nm\nPk\nm=1 eXj\nm\n. (13)\nDetection head and training objectives.To enhance\nthe expressiveness of V oxSeT backbone, we follow PointR-\nCNN [33] to apply the foreground segmentation lossLseg to\nthe output features. This forces V oxSeT to capture contex-\ntual information for generating accurate bounding-boxes.\nThe detection head follows the traditional anchor-based de-\nsign [12, 42]. The final loss then becomes:\nL = Lseg + 1\nNp\n(Lcls + Lreg) +Ldir), (14)\nwhere Np is the number of positive samples whose IoU\nwith anchors lies between [œÉ1, œÉ2]. Lcls is the focal loss\nfor bounding-box classification and Lreg is the Smooth- Ll\nloss for bounding-box offsets regression. Ldir is a binary\nentropy loss for bounding-box orientation prediction. The\nreaders are referred to [33] and [42] for details.\nTwo-stage model.It is worth noting that V oxSeT can be\nextended to a two-stage detector, in which we employ the\nefficient RoI head from LiDAR-RCNN [15] as our second-\nstage module. To more clearly illustrate our contribution,\nwe also report the performance of a single-stage detector,\nand our V oxSeT demonstrates superior performance over\nthe current single-stage baselines.\n4. Experiments\nIn this section, we evaluate our proposed V oxSeT on two\npublic detection datasets, KITTI [8] and Waymo [35]. We\nfirst introduce the training details of V oxSeT and the evalu-\nation settings, and then compare our models with state-of-\nthe-art detection models. Finally, we conduct an in-depth\nanalysis of each component of V oxSeT.\n4.1. Implementation details\nModel setup. On the KITTI dataset, we select the Li-\nDAR points that fall into the ranges [0m, 70.4m], [-40m,\n8421\n\nTable 1. Performance comparison with state-of-the-art methods on the Waymo dataset with 202 validation sequences (‚àº 40k samples) for\nvehicle detection.\nMethod Backbone 3D mAP BEV mAP\nOverall 0-30m 30-50m 50m-inf Overall 0-30m 30-50m 50m-inf\nLEVEL1 (IoU=0.7):\nPointPillar [12] (CVPR19) CNN 56.62 81.01 51.75 27.94 75.57 92.10 74.06 55.47\nMVF [50] (CoRL20) CNN 62.93 86.30 60.02 36.02 80.40 93.59 79.21 63.09\nPV-RCNN [32] (CVPR20) SpCNN 70.30 91.92 69.21 42.17 82.96 97.35 82.99 64.97\nV oxel-RCNN [5] (AAAI21) SpCNN 75.59 92.49 74.09 53.15 88.19 97.62 87.34 77.70\nV oTR-TSD [21] (ICCV21)Transformer 74.95 92.28 73.36 51.09 - - - -\nCT3D [31] (ICCV21) SpCNN 76.30 92.51 75.07 55.36 90.50 97.64 88.06 78.89\nVoxSeT (ours) Transformer 76.02 91.13 75.75 54.23 89.12 95.12 87.36 77.78\nVoxSeT + CT3D (RoI head)Transformer 77.82 92.78 77.21 54.41 90.31 96.11 88.12 77.98\nLEVEL2 (IoU=0.7):\nPV-RCNN [32] (CVPR20) SpCNN 65.36 91.58 65.13 36.46 77.45 94.64 80.39 55.39\nV oxel-RCNN [5] (AAAI21) SpCNN 66.59 91.74 67.89 40.80 81.07 96.99 81.37 63.26\nV oTR-TSD [21] (ICCV21)Transformer 65.91 - - - - - - -\nCT3D [31] (ICCV21) SpCNN 69.04 91.76 68.93 42.60 81.74 97.05 82.22 64.34\nVoxSeT (ours) Transformer 68.16 91.03 67.13 42.23 76.13 94.13 81.78 58.13\nVoxSeT + CT3D (RoI head)Transformer 70.21 92.05 70.10 43.20 80.56 96.79 80.44 62.37\nTable 2. Performance comparison with traditional single-stage baseline models on the KITTI validation set. The results are reported by the\nmAP with 11 recall points.\nMethod Vehicle Pedestrian Cyclist\nEasy Moderate Hard Easy Moderate Hard Easy Moderate Hard\nSECOND [42] 88.61 78.62 77.22 56.55 52.98 47.73 80.58 67.15 63.10\nPointPillars [12] 86.46 77.28 74.65 57.75 52.29 47.90 80.04 62.61 59.52\nV oxSeT (single-stage) 88.45 78.48 77.07 60.62 54.74 50.39 84.07 68.11 65.14\nImprovements -0.16 -0.14 -0.15 2.87 1.76 2.49 3.49 0.96 2.04\n40m], [-3m, 1m] along X, Y , Z axes, respectively, and aban-\ndon those points with the frontal view projections out of\nimage. On the Waymo dataset, the points that lie between\n[-75.2m, 75m] in the X and Y axes, and [-2m, 4m] in the\nZ axis are selected. The voxel size of the first VSA layer\nis [0.32m, 0.32m, 4m] on KITTI and [0.32m, 0.32m, 6m]\non Waymo. The voxel size is doubled along the X and Y\naxes in the next VSA block. The feature dimensions of the\nfour VSA blocks are 16, 32, 64 and 128, respectively. The\nnumber of latent codes in each VSA block is 8 and the band-\nwidth L of the Positional Embedding (PE) module is 64.\nTraining and inference. The network is trained end-\nto-end on four RTX Quodra 8000 GPUs for 100 epochs\nwith the Adam optimizer. The batch size, learning rate, and\nweight decay are set to 4, 0.003 and 0.01, respectively. The\nlearning rate is decayed with theonecycle policy, where the\nmomentum has a damping range of [85%, 95%].\nWe apply the anchor settings in SECOND [42] in our\nsingle stage model. For the two-stage model, we sample\n512 RoIs in training and 128 RoIs in inference. In the post-\nprocessing phase, the bounding-boxes are filtered by NMS\nwith an IoU threshold of 0.1, and those having confidence\nover 0.3 are selected as final predictions. Data augmenta-\ntions [4,42,51] are applied to improve the model generaliza-\ntion performance. For other default settings, the readers are\nreferred to the OpenPCDet toolbox [36] used in this work.\n4.2. Dataset and evaluation metrics\nKITTI dataset [8].KITTI contains 7,481 training sam-\nples and 7,518 testing samples. Following the common pro-\ntocol [3], we split the labeled data into a training set with\n3,712 samples and a validation set with 3,769 samples. We\nconduct experiments on the commonly used car category\nwhose detection IoU threshold is 0.7, and report the results\non three difficulty levels (easy, moderate and hard) accord-\ning to the object size, occlusion state and truncation level.\nWaymo open dataset [35]. This dataset consists of\n798 training sequences and 202 validation sequences, where\nthere are 158,361 samples and 40,077 samples, respectively.\nThe evaluation metrics used are 3D mean Average Precision\n(mAP) with IoU threshold of 0.7 on the vehicle category.\nThe measures are reported based on the distances from ob-\njects to sensor, i.e., 0‚àí30m, 30‚àí50m and >50m, respec-\ntively. Two difficulty levels, LEVEL 1 (boxes with more\nthan five LiDAR points) and LEVEL 2 (boxes with at least\none LiDAR point) are considered.\n8422\n\nTable 3. Performance comparison with state-of-the-art methods on\nthe KITTI test set. The results are reported by the mAP with 0.7\nIoU threshold and 40 recall points.\nMethod 3D\nEasy Moderate Hard\nLiDAR + RGB:\nMV3D [3] (CVPR17) 74.97 63.63 54.00\nContFuse [17] (ECCV18) 83.68 68.78 61.67\nA VOD-FPN [11] (IROS18) 83.07 71.76 65.73\nF-PointNet [27] (CVPR18) 82.19 69.79 60.59\nMMF [16] (CVPR19) 88.40 77.43 70.22\n3D-CVF [46] (ECCV20) 89.20 80.05 73.11\nCLOCs [24] (IROS20) 88.94 80.67 77.15\nLiDAR only:\nV oxelNet [51] (CVPR18) 77.47 65.11 57.73\nSECOND [42] (Sensor18) 83.34 72.55 65.82\nPointPillars [12] (CVPR19) 82.58 74.31 68.99\nSTD [45] (ICCV19) 87.95 79.71 75.09\nPointRCNN [33] (CVPR19) 86.96 75.64 70.70\nSA-SSD [10] (CVPR20) 88.75 79.79 74.16\n3DSSD [45] (CVPR20) 88.36 79.57 74.55\nPV-RCNN [45] (CVPR20) 90.25 81.43 76.82\nV oxel-RCNN [45] (AAAI21)87.95 79.71 75.09\nCT3D [31] (ICCV21) 87.83 81.77 77.16\nV oTR-TSD [21] (ICCV21) 89.90 82.09 79.14\nV oxSeT (ours) 88.53 82.06 77.46\n4.3. Results on the Waymo open dataset\nWe first evaluate the performance of V oxSeT on the\nWaymo open dataset. The results are summarized in Ta-\nble 1. V oxSeT outperforms most of CNN-based mod-\nels, leading PV-RCNN [32] by 5% LEVEL 1 mAP and\nV oxelRCNN [5] by 2.4% LEVEL 2 mAP. As one of the\nfew transformer based models, our V oxSeT achieves better\nperformance than its transformer-based competitor V oTR-\nTSD [21], which brings 0.9% and 1.4% improvements\non LEVEL 1 and LEVEL 2 mAP, respectively. V oxSeT\nachieves comparable performance to the state-of-the-art\nmethod CT3D [31]. It should be noted that, however,\nCT3D actually employs a heavy transformer based RoI\nhead, which has three self-attention encoding layers. By\nadopting this RoI head into V oxSeT, our model achieves\nbetter results, outperforming CT3D by 1.5 % LEVEL 1\nmAP and 1.2% LEVEL 2 mAP. This demonstrates that as\na new transformer based backbone network, V oxSeT sur-\npasses Sparse CNN based networks. V oxSeT works es-\npecially well in the range of 30-50m, which indicates that\ntransformer modules are better in capturing the context in-\nformation in long-range areas.\n4.4. Results on the KTTI Dataset\nWe then conduct experiments on the KITTI dataset to\nevaluate the performance of V oxSeT as a single-stage detec-\nTable 4. Performance comparison with state-of-the-art methods\non the KITTI validation set. The results are reported by the mAP\nwith 0.7 IoU threshold and 11 recall points.\nMethod 3D\nEasy Moderate Hard\nLiDAR + RGB:\nMV3D [3] (CVPR17) 71.29 62.68 56.56\nF-PointNet [27] (CVPR18) 83.76 70.92 63.65\n3D-CVF [46] (ECCV20) 89.67 79.88 78.47\nLiDAR only:\nSECOND [42] (Sensor18) 88.61 78.62 77.22\nPointPillars [12] (CVPR19) 86.62 76.06 68.91\nSTD [45] (ICCV19) 89.70 79.80 79.30\nPointRCNN [33] (CVPR19) 88.88 78.63 77.38\nSA-SSD [10] (CVPR20) 90.15 79.91 78.78\n3DSSD [45] (CVPR20) 89.71 79.45 78.67\nPV-RCNN [45] (CVPR20) 89.35 83.69 78.70\nV oxel-RCNN [45] (AAAI21)89.41 84.52 78.93\nCT3D [31] (ICCV21) 89.54 86.06 78.99\nV oTR-TSD [21] (ICCV21) 89.04 84.04 78.68\nV oxSeT (ours) 89.21 86.71 78.56\ntion model. Our competitors are SECOND [42] and Point-\nPillars [12], which represent two widely used baseline fea-\nture extractors. All the three methods use the same detection\nhead and hyper-parameters in training. As shown in Table\n2, V oxSeT achieves comparable performance to SECOND\non vehicle class, but much better performance on Pedes-\ntrian and Cyclist classes. We believe this is because V oxSeT\nhas a wider effective receptive field through the VSA con-\nditioned on the large voxel, which is crucial to detecting the\nobjects with sparse points.\nAs a two-stage detection model, V oxSeT achieves bet-\nter performance than CT3D by 0.7% (Easy), 0.4% (Mod-\nerate) and 0.3% (Hard) mAP, respectively, as shown in Ta-\nble 3. Compared with V oTR-TSD which relies on multi-\nscale backbone features, V oxSeT can still achieve compa-\nrable performance by using only singular point-wise fea-\ntures. We also evaluate V oxSeT on KITTI val. One can\nsee that V oxSeT achieves leading accuracy on ‚ÄúModerate‚Äù\nlevel but slightly lower accuracy on ‚ÄúEasy‚Äù and ‚ÄúHard‚Äù lev-\nels. We believe this is because KITTI has a long-tailed dis-\ntribution, and hence the ‚ÄúModerate‚Äù samples dominate the\nhidden space of the VSA module.\nIt should be pointed out that both CT3D and V oTR em-\nploy convolutional architectures, and their performances on\nKITTI and Waymo datasets are not consistent. Specifically,\nCT3D works better on Waymo but its performance drops\nmuch on KITTI, while V oTR works better on KITTI but\nits performance on Waymo is much worse. In contrast, our\nV oxSeT exhibits consistently superior performance on both\ndatasets, demonstrating its good generalization capacity.\n8423\n\nFigure 4. Visualization of the spatial attention maps induced by different latent codes in the last VSA module.\nTable 5. Effects of enabling token interactions by convolutional\nfeed-forward network. The mAP (11 recall points) of RPN on\nKITTI val are reported.\nSettings Easy Moderate Hard\ndefault 88.31 79.56 77.84\nConvFFN‚ÜíFFN 70.12 69.54 54.23\nTable 6. The mAP (11 recall points) of RPN on KITTI val by\nusing different number of latent codes (LC) in four VSA modules.\nNo. of latent codes 4 8 16\nmAP 76.63 78.25 78.74\nTable 7. Comparison of PointRCNN [33] detectors with V oxSeT\nand PointNet++ backbones. The mAP (11 recall points) on KITTI\nval are reported.\nSettings Easy Moderate Hard\nPointRCNN 88.52 78.95 77.81\nVSA-PointRCNN 89.61 80.14 78.69\nTable 8. The latency and runtime memory on the KITTI dataset,\ntested by NVIDIA 2080Ti GPU.\nModels Latency Memory (runtime)\nSECOND [42] 48 ms 6093MB\nPointPillars [12] 22ms 1508 MB\nV oxSeT (single-stage) 34 ms 2381 MB\n4.5. Ablation study\nWe conduct a series of ablation experiments to compre-\nhend the roles of different components in V oxSeT.\nConvolutional feed-forward network.Table 5 shows\nthat replacing the proposed ConvFFN with the conventional\nFFN significantly degrades the accuracy, indicating that the\nlocal connectivity is crucial to the detection performance.\nEffects of number of latent codes.In Table 6, we in-\nvestigate the number of latent codes used in four VSA mod-\nules. We see that more latent codes can encode more con-\ntext information of point cloud, and enhance the modeling\ncapacity of V oxSeT.\nComparison with PointNet++ backbone. We train a\nPointRCNN [33] variant by replacing its PointNet++ back-\nbone [33] with our V oxSeT backbone. From Table [29],\none can observe obvious performance improvements. We\nbelieve this is because VSA module has better modeling\npower in terms of dynamic learning and large receptive field\nthan the set abstraction (SA) module in PointNet++.\nLatency and runtime memory. Table 8 shows that\nV oxSeT is faster and has less memory consumption com-\npared to the sparse 3D CNN (SECOND). The higher per-\nformance than PointPillars and the acceptable runtime cost\nsuggest that V oxSeT can be a good alternative to PointPil-\nlars in real-time applications.\nVisualization of attention weights.Figure 4 visualizes\nthe spatial attention maps for the latent codes in the last\nVSA module. We show the attention maps for 4 out of a to-\ntal of 8 latent codes. One can observe that the VSA module\nfocuses more on the object region and different latent codes\nencode different contexts of the objects, indicating the high\nexpressiveness of VSA for point cloud data.\n5. Conclusion and discussions\nWe proposed V oxSeT, a novel transformer-based frame-\nwork for 3D object detection from LiDAR point clouds. In\ncontrast to previous 3D LiDAR detectors, which use sparse\nCNN and PointNet backbones to learn point cloud features,\nwe made the first attempt to model point cloud processing\nas set-to-set translation, which preserves the full resolution\nof raw point cloud at every step of feature extraction. We\npresented a voxel-based set attention module that performs\nself-attention on voxel clusters of arbitrary size and encodes\npoint features with more discriminative context information\nfrom a large receptive field. Experimental results on the\nWaymo and KITTI datasets demonstrated that our V oxSeT\ncan achieve competitive performance, making it a good al-\nternative for point cloud modeling.\nIt should be noted that in V oxSeT, we only explored one\npossible formulation of liner attention based on the induced\nlatent codes. This limits the expressive power of V oxSeT to\nrepresent different point cloud structures and their correla-\ntions. By using stronger attention mechanisms, the perfor-\nmance of V oxSeT can be further improved, which will be\nour future research direction.\n8424\n\nReferences\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision , pages 213‚Äì229. Springer, 2020.\n3\n[2] Yuning Chai, Pei Sun, Jiquan Ngiam, Weiyue Wang, Ben-\njamin Caine, Vijay Vasudevan, Xiao Zhang, and Dragomir\nAnguelov. To the point: Efficient 3d object detection in the\nrange image with graph convolution kernels. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR) , pages 16000‚Äì16009, June 2021.\n2\n[3] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.\nMulti-view 3d object detection network for autonomous\ndriving. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 1907‚Äì1915,\n2017. 2, 6, 7\n[4] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast\npoint r-cnn. In Proceedings of the IEEE international con-\nference on computer vision (ICCV), 2019. 5, 6\n[5] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou,\nYanyong Zhang, and Houqiang Li. V oxel r-cnn: Towards\nhigh performance voxel-based 3d object detection. AAAI,\n2021. 1, 5, 6, 7\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ICLR, 2021. 3\n[7] Lue Fan, Xuan Xiong, Feng Wang, Naiyan Wang, and\nZhaoXiang Zhang. Rangedet: In defense of range view\nfor lidar-based 3d object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 2918‚Äì2927, October 2021. 2\n[8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets robotics: The kitti dataset. The Inter-\nnational Journal of Robotics Research , 32(11):1231‚Äì1237,\n2013. 5, 6\n[9] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang\nMu, Ralph R. Martin, and Shi-Min Hu. Pct: Point cloud\ntransformer. Computational Visual Media , 7(2):187‚Äì199,\nApr 2021. 1, 3\n[10] Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua,\nand Lei Zhang. Structure aware single-stage 3d object detec-\ntion from point cloud. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n11873‚Äì11882, 2020. 1, 2, 7\n[11] Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh,\nand Steven L Waslander. Joint 3d proposal generation and\nobject detection from view aggregation. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems\n(IROS), pages 1‚Äì8. IEEE, 2018. 2, 7\n[12] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\nfor object detection from point clouds. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 12697‚Äì12705, 2019. 2, 4, 5, 6, 7, 8\n[13] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-\nungjin Choi, and Yee Whye Teh. Set transformer: A frame-\nwork for attention-based permutation-invariant neural net-\nworks. In Proceedings of the 36th International Conference\non Machine Learning, pages 3744‚Äì3753, 2019. 2, 3\n[14] Bo Li. 3d fully convolutional network for vehicle detection\nin point cloud. In 2017 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), pages 1513‚Äì1518.\nIEEE, 2017. 2\n[15] Zhichao Li, Feng Wang, and Naiyan Wang. Lidar r-cnn: An\nefficient and universal 3d object detector. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 5\n[16] Ming Liang, Bin Yang, Yun Chen, Rui Hu, and Raquel Urta-\nsun. Multi-task multi-sensor fusion for 3d object detection.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 7345‚Äì7353, 2019. 7\n[17] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun.\nDeep continuous fusion for multi-sensor 3d object detection.\nIn Proceedings of the European Conference on Computer Vi-\nsion (ECCV), pages 641‚Äì656, 2018. 7\n[18] Zhidong Liang, Zehan Zhang, Ming Zhang, Xian Zhao, and\nShiliang Pu. Rangeioudet: Range image based real-time 3d\nobject detector optimized by intersection over union. InPro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 7140‚Äì7149, June\n2021. 2\n[19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV) , pages 10012‚Äì10022,\nOctober 2021. 2, 3\n[20] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong.\nGroup-free 3d object detection via transformers. InProceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV) , pages 2949‚Äì2958, October 2021. 2,\n3\n[21] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi\nFeng, Xiaodan Liang, Hang Xu, and Chunjing Xu. V oxel\ntransformer for 3d object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 3164‚Äì3173, October 2021. 1, 2, 3, 6, 7\n[22] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-\nend transformer model for 3d object detection. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV) , pages 2906‚Äì2917, October 2021. 1,\n2, 3\n[23] Xuran Pan, Zhuofan Xia, Shiji Song, Li Erran Li, and Gao\nHuang. 3d object detection with pointformer. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 7463‚Äì7472, June 2021.\n1, 2, 4\n[24] Su Pang, Daniel Morris, and Hayder Radha. Clocs: Camera-\nlidar object candidates fusion for 3d object detection. In2020\n8425\n\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), pages 10386‚Äì10393. IEEE, 2020. 7\n[25] Charles R. Qi, Or Litany, Kaiming He, and Leonidas J.\nGuibas. Deep hough voting for 3d object detection in point\nclouds. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), October 2019. 2, 5\n[26] Charles R. Qi, Or Litany, Kaiming He, and Leonidas J.\nGuibas. Deep hough voting for 3d object detection in point\nclouds. In Proceedings of the IEEE international conference\non computer vision (ICCV), 2019. 2\n[27] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J\nGuibas. Frustum pointnets for 3d object detection from rgb-\nd data. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 918‚Äì927, 2018. 1, 2,\n7\n[28] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classification\nand segmentation. InProceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , pages 652‚Äì660,\n2017. 1\n[29] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. In Advances in neural infor-\nmation processing systems, pages 5099‚Äì5108, 2017. 2, 4, 5,\n8\n[30] Ren ¬¥e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 12179‚Äì12188, October 2021. 3\n[31] Hualian Sheng, Sijia Cai, Yuan Liu, Bing Deng, Jianqiang\nHuang, Xian-Sheng Hua, and Min-Jian Zhao. Improving 3d\nobject detection with channel-wise transformer. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV) , pages 2743‚Äì2752, October 2021. 1,\n2, 3, 6, 7\n[32] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jian-\nping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn:\nPoint-voxel feature set abstraction for 3d object detection.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2020. 2, 6, 7\n[33] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-\ncnn: 3d object proposal generation and detection from point\ncloud. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 770‚Äì779, 2019. 1, 2,\n5, 7, 8\n[34] Martin Simony, Stefan Milzy, Karl Amendey, and Horst-\nMichael Gross. Complex-yolo: an euler-region-proposal for\nreal-time 3d object detection on point clouds. InProceedings\nof the European Conference on Computer Vision (ECCV) ,\npages 0‚Äì0, 2018. 2\n[35] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\nYuning Chai, Benjamin Caine, et al. Scalability in perception\nfor autonomous driving: Waymo open dataset. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2446‚Äì2454, 2020. 5, 6\n[36] OpenPCDet Development Team. Openpcdet: An open-\nsource toolbox for 3d object detection from point clouds.\nhttps://github.com/open-mmlab/OpenPCDet ,\n2020. 6\n[37] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efficient image transformers &amp; distillation through\nattention. In International Conference on Machine Learning,\nvolume 139, pages 10347‚Äì10357, July 2021. 3\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998‚Äì6008, 2017. 3\n[39] Apoorv Vyas, Angelos Katharopoulos, and Franc ¬∏ois Fleuret.\nFast transformers with clustered attention, 2020. 3\n[40] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and\nHao Ma. Linformer: Self-attention with linear complexity,\n2020. 3\n[41] Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming\nZhang, Kai Xu, and Jun Wang. Mlcvnet: Multi-level con-\ntext votenet for 3d object detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2020. 1\n[42] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed-\nded convolutional detection. Sensors, 18(10):3337, 2018. 1,\n2, 4, 5, 6, 7, 8\n[43] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-\ntime 3d object detection from point clouds. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 7652‚Äì7660, 2018. 2\n[44] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd:\nPoint-based 3d single stage object detector. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), June 2020. 1, 2\n[45] Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Ji-\naya Jia. STD: sparse-to-dense 3d object detector for point\ncloud. In Proceedings of the IEEE international conference\non computer vision (ICCV), 2019. 1, 2, 5, 7\n[46] Jin Hyeok Yoo, Yecheol Kim, Jisong Kim, and Jun Won\nChoi. 3d-cvf: Generating joint camera and lidar features us-\ning cross-view spatial feature fusion for 3d object detection.\nIn Computer Vision‚ÄìECCV 2020: 16th European Confer-\nence, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part\nXXVII 16, pages 720‚Äì736. Springer, 2020. 7\n[47] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip H.S. Torr,\nand Vladlen Koltun. Point transformer. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 16259‚Äì16268, October 2021. 1, 2, 3\n[48] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with\ntransformers. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n6881‚Äì6890, June 2021. 3\n[49] Wu Zheng, Weiliang Tang, Sijin Chen, Li Jiang, and Chi-\nWing Fu. Cia-ssd: Confident iou-aware single-stage object\ndetector from point cloud. AAAI, 2021. 1\n8426\n\n[50] Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang\nGao, Tom Ouyang, James Guo, Jiquan Ngiam, and Vijay Va-\nsudevan. End-to-end multi-view fusion for 3d object detec-\ntion in lidar point clouds, 2019. 6\n[51] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning\nfor point cloud based 3d object detection. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 4490‚Äì4499, 2018. 1, 2, 4, 6, 7\n8427\n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7668813467025757
    },
    {
      "name": "Point cloud",
      "score": 0.7505930662155151
    },
    {
      "name": "Voxel",
      "score": 0.7494115233421326
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5535455942153931
    },
    {
      "name": "Transformer",
      "score": 0.5440352559089661
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40226003527641296
    },
    {
      "name": "Computer vision",
      "score": 0.4010081887245178
    },
    {
      "name": "Algorithm",
      "score": 0.35385262966156006
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I14243506",
      "name": "Hong Kong Polytechnic University",
      "country": "HK"
    }
  ],
  "cited_by": 196
}