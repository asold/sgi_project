{
    "title": "‚ÄúJust a pocket knife, not a machete‚Äù: Large language models in TEFL teacher education &amp; digital text sovereignty",
    "url": "https://openalex.org/W4395464943",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5093101002",
            "name": "Jules Buendgens-Kosten",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A5093101002",
            "name": "Jules Buendgens-Kosten",
            "affiliations": [
                "Goethe University Frankfurt"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3133702157",
        "https://openalex.org/W4319746602",
        "https://openalex.org/W4366393300",
        "https://openalex.org/W4233703403",
        "https://openalex.org/W3124662721",
        "https://openalex.org/W4362584541",
        "https://openalex.org/W3182328875",
        "https://openalex.org/W6962879732",
        "https://openalex.org/W2123184471",
        "https://openalex.org/W4385602528",
        "https://openalex.org/W3206366076",
        "https://openalex.org/W4385548759",
        "https://openalex.org/W4224109175",
        "https://openalex.org/W4206806123"
    ],
    "abstract": "This paper approaches AI in TEFL teacher education from a perspective of digital text sovereignty (digitale Textsouver√§nit√§t). Digital sovereignty (digitale Souver√§nit√§t) is a concept that goes beyond media literacy and data literacy as a set of skills, to include personal competences in a more Humboldtian vision of education. Digital text sovereignty focusses on all those aspects of digital sovereignty that apply to the reception and production of texts, from the ability to produce and consume digital texts to the development of a critically-reflexive attitude to these texts (Frederking, 2022; Frederking &amp; Krommer, 2022), making it especially useful in contexts of TEFL teacher education. This paper is based on an empirical study conducted within the EFL teacher education department of a German metropolitan university during the 2023 summer term. It analyses 21 student essays written after an interactive 90-minute training session on ChatGPT conducted in three intact TEFL teacher education seminars. Qualitative content analysis is used to identify domains of digital text sovereignty discussed in these essays, and to identify key themes related to four aspects of digital text sovereignty: Mediality, Source Code, Intentionality, and Veracity. The findings might be of interest for researchers and teachers educators interested in (a) modelling digital text sovereignty as it applies to AI, and (b) developing teacher education material and programs that target digital text sovereignty in an AI context, or that aim to support (future) teachers in developing an AI-informed pedagogy.",
    "full_text": "Technology in Language Teaching & Learning\nISSN 2652-1687 \nhttps://www.castledown.com/journals/tltl/\nTechnology in Language Teaching & Learning, 6(1), 1‚Äì16 (2024) \nhttps://doi.org/10.29140/tltl.v6n1.1192\nCopyright: ¬© 2024 Jules Buendgens-Kosten. This is an open access article distributed under the terms of the  Creative \n Commons Attribution Non-Commercial 4.0 International License, which permits unrestricted use,  distribution, and \nreproduction in any medium, provided the original author and source are credited. Data Availability Statement:  \nAll relevant data are within this paper.\n‚ÄúJust a Pocket Knife, Not a Machete‚Äù:  \nLarge Language Models in TEFL Teacher  \nEducation and Digital Text Sovereignty \nJULES BUENDGENS-KOSTEN  \nGoethe University Frankfurt, Germany  \nbuendgens-kosten@em.uni-frankfurt.de\nThis paper approaches AI in TEFL teacher education from a perspective of digital text sovereignty \n(digitale Textsouver√§nit√§t). Digital sovereignty ( digitale Souver√§nit√§t) is a concept that goes beyond \nmedia literacy and data literacy as a set of skills, to include personal competences in a more Humboldtian \nvision of education. Digital text sovereignty focuses on all those aspects of digital sovereignty that \napply to the reception and production of texts, from the ability to produce and consume digital texts \nto the development of a critically-reflexive attitude to these texts (Frederking, 2022; Frederking & \nKrommer, 2022), making it especially useful in contexts of TEFL teacher education. \nThis paper is based on an empirical study conducted within the EFL teacher education department of \na German metropolitan university during the 2023 summer term. It analyzes 21 student essays written \nafter an interactive 90-minute training session on ChatGPT, conducted in three intact TEFL teacher \neducation seminars. \nQualitative content analysis is used to identify domains of digital text sovereignty discussed in these \nessays, and to identify key themes related to four aspects of digital text sovereignty: Mediality, Source \nCode, Intentionality, and Veracity. The findings might be of interest for researchers and teacher \neducators interested in (a) modelling digital text sovereignty as it applies to AI, and (b) developing \nteacher education material and programs that target digital text sovereignty in an AI context, or that \naim to support (future) teachers in developing an AI-informed pedagogy.\nLarge Language Models (LLMs) in TEFL and TEFL Teacher Education\nFor several years, TEFL and TEFL teacher education have seen recurring debates on the impact of \nAI on these fields. Gr√ºnewald, for example, asked in 2019 if foreign language education was still \nneeded in an age of AI translation tools. In CALL specifically, different AI-based tools such as machine \n‚ÄúJust a Pocket Knife, Not a Machete‚Äù 2\ntranslation tools (Lee, 2020; Lee, 2021) or AI chatbots (Jeon, 2022; Yang et al., 2022) have been of \nongoing interest. With the introduction of ChatGPT, these discussions have developed more urgency \nand have brought an increasing academic interest as well as a growing interest on the part of teachers \nand teacher education students to learn more and to develop their teaching and assessment practices in \nresponse to it. \nEven though a technical description of large language models (LLMs) and their creation process via \nMachine Learning is not the focus of this paper, some features of LLMs in general and ChatGPT \nspecifically will be outlined very briefly. ChatGPT ‚Äì developed by OpenAI (https://openai.com/), \noriginally published in 2022 and currently available in version 4 ‚Äì is a pre-trained large language \nmodel, essentially ‚Äúa system for haphazardly stitching together sequences of linguistic forms it has \nobserved in its vast training data, according to probabilistic information about how they combine, \nbut without any reference to meaning‚Äù (Bender et al., 2021, p. 617). Importantly, it allows access \nthrough a natural language interface. Users can formulate a prompt in ways similar to a teacher \nwriting a prompt for learners, i.e. by specifying the content, text type, length, language, and style of \na text, without being bound to a specific input format. Alternatively, users can also pose questions, \nwhich the LLM will then answer. A text can be the result of multiple ‚Äòturns‚Äô between the user \nand the tool, and a broad range of languages can be used in both prompt and output (including \ngeneration of multilingual texts and translation prompts). The response is generated ‚Äì anew for each \nprompt ‚Äì by drawing on a large language model created through machine learning. ChatGPT has no \nunderstanding of the world, nor an understanding of the rules of language, beyond information on \nwhich words co-occur with which other words. It has been described, consequently, as a ‚Äústochastic \nparrot‚Äù (Bender et al., 2021). \nFrom an educational perspective, easily accessible LLMs like ChatGPT have been both applauded and \nviewed critically. A range of scholarly papers in addition to blogs, websites, and other channels have \npublished (often strongly practice-focused) texts describing the potential of ChatGPT and similar tools \nfor the language classroom (Bonner et al., 2023; Kohnke et al., 2023; Koraishi, 2023; Mahlow, 2023; \nV ogt & Flindt, 2023), and for data analysis in language teaching research (Pack & Maloney, 2023). \nEmpirical research on ChatGPT, on the other hand, is still scarce, though the available literature is \nexpanding nearly by the day.1 \nOne trend, observable in currently available work, might be referred to as a ‚Äúpedagogy of the gaps‚Äù: \nan approach to teaching in an age of digitalization in general, and of easily accessible LLMs and \nother machine-learning and/or AI tools specifically, that focuses on what the machine cannot yet do in \ndetermining the methods of teaching and assessment as well as the goals pursued through said teaching \nand assessment. It is necessarily reactive, changing as the technology changes. This reactiveness is \nmost pronounced on the level of method, where even small changes in the technology might lead to \nthe need to fundamentally revamp how teaching and assessment happen. At the level of educational \ngoals (as discussed below), a certain reactive factor, if balanced out by other factors, might be desirable \nto ensure learners are indeed prepared for life in contemporary society. Therefore, I will only use \nthe term ‚Äúpedagogy of the gaps‚Äù in those cases in which goals are determined nearly entirely by \ntechnological advances, and refrain from using the term when technological advances are seen merely \nas an encouragement to reconceptualize skills and competencies more broadly.\nA look at AI and foreign language education that is not purely reactive is provided by Berthele and \nUdry (2023), which focuses on AI more generally, but the arguments which are made can be applied to \n1 Work on LLMs in (language) education is still in its infancy. Consequently, I have made the choice to also include \npreprints in my literature review. \n3 Technology in Language Teaching & Learning, 6(1)\nLLM contexts and ChatGPT specifically without difficulty. The authors look at educational standards \nfor foreign language teaching in Germany and Switzerland and discuss which of the goals laid down \nin educational policy documents can be reached through tools and without learning. They identify \nreading and writing as areas that can easily be achieved without learning by using e.g., translation \ntools, and they identify spontaneous interactive speech, intercultural competences and metalinguistic \nskills as less ‚Äòreplaceable‚Äô. Instead of falling into a ‚Äúpedagogy of the gaps‚Äù, they suggest an update of \nthe CEFR to include tool support more systematically in the ‚Äúcan do‚Äù statements. \nMahlow (2023), looking at German as a first, second and foreign language, also comes to a balanced \nview on the degree to which AI tools can ‚Äòreplace‚Äô the knowledge and skills of the user: \n‚ÄúBy embracing the opportunities presented by language technology and incorporating it \nthoughtfully into language learning environments, we can enhance writing instruction \nand empower learners to become effective communicators in German and other \nlanguages.‚Äù (Mahlow, 2023, p. 190)\nA theme that runs through a number of these papers is the need for critical skills in dealing with LLMs \nand other AI tools. V ogt & Flindt (2023), for example, focusing mostly on older AI tools but touching \non ChatGPT as well, make the argument that, as students are already using AI tools, the EFL classroom \nhas the potential to integrate reflection on these ‚Äúin a way that is conducive to learning for a variety of \nforeign language competences.‚Äù (V ogt und Flindt, 2023, p. 182). I suggest that this expressed need for \ncritical skills fits well into the ‚Äúdigital text sovereignty‚Äù model discussed below. \nLLMs and Digital Text Sovereignty\nFrederking coined the term digital text sovereignty (digitale Textsouver√§nit√§t)2, drawing on the already \nestablished concept of digital sovereignty (also: data sovereignty) (Friedrichsen & Bisa, 2016). Digital \nsovereignty/Data sovereignty  refer to ‚Äúmeaningful control, ownership, and other claims in data‚Äù \n(Hummel et al., 2021), though beyond this core, the exact interpretation of the term and related terms \ndiffer substantially (e.g., whether it refers to something on an individual or on a group level, and \nwhether it is a right or a skill/competency; Hummel et al., 2021). Digital text sovereignty, focusing \non the individual and skill/competency aspects of the term 3, adds to this the dimension of textuality. \nFrederking argues that the ‚Äúdigital world‚Äù can be understood as a ‚Äúcomplex structured digital textWorld, \na kind of digital meta-text consisting of an infinite number of individual digital texts‚Äù (Frederking, \n2022, p. 3, translation by the author), ‚Äútext‚Äù taken here in its broad sense, encompassing a plurality of \ncodes and modes. \nFrederking argues that digital text sovereignty has four aspects, two on the semiotic level (Mediality \n[Medialit√§t]; Source Code [Quellcode]), and two on the semantic level (Intentionality [Intentionalit√§t ]; \nVeracity [Wahrheitsgehalt]). These four aspects can each be viewed from a functional-technological \nperspective and a personal-reflexive perspective, producing a total of eight dimensions. These eight \ndimensions, in turn, each contain a productive and a receptive element (Table 1). \n2 As I am citing work originally published in German and the translations I have chosen might not be the preferred \ntranslations of the author, I am including the German original when I first discuss a term. In all following discussions, \nthough, I will limit myself to the English term. \n3 Frederking frames digital text sovereignty as a habitus, rather than as a (set of) skill(s) or competence(s), even though \nthis does not appear as a strict delineation within this text, in which he also references skills as components of digital text \nsovereignty. \n‚ÄúJust a Pocket Knife, Not a Machete‚Äù 4\nTable 1 Frederking‚Äô s Eight Dimensions of Digital Text Sovereignty (Frederking, 2022)\nsemiotic Mediality functional-technological receptive\nproductive\npersonal-reflexive receptive\nproductive\nSource Code functional-technological receptive\nproductive\npersonal-reflexive receptive\nproductive\nsemantic Intentionality functional-technological receptive\nproductive\npersonal-reflexive receptive\nproductive\nVeracity functional-technological receptive\nproductive\npersonal-reflexive receptive\nproductive\nI argue that this model has relevance when viewing the topic of LLMs in EFL. Concerning Mediality, \nunderstood here as ‚Äúability to comprehend(ing) reception and production of literal, visual, auditive \nand/or audio-visual text or media elements, their multimodal or symmedial composite structures and \ntheir hypermedial, connective and communicative connections‚Äù (Frederking, 2022, p. 6f, translation \nby the author), the relationship is very clear. ChatGPT as a large language model generates textual \noutput based on textual input. The ability to use it for text generation, and the ability to (critically) \nwork with such generated texts, are important in all languages a person uses, including their foreign \nlanguages ‚Äì maybe even more so in foreign languages, due to the potential of the tool to scaffold lower \ntext production skills and to produce model texts for analysis and reproduction. \nBeyond Mediality, though, the Source Code dimension is equally important. Frederking‚Äôs (2022) \ntext focuses strongly on functional-technological and personal-reflexive skills/habitus related to \nmarkup languages such as HTML, relegating programming languages to non-language subjects \nwhile acknowledging the importance of algorithms (Frederking, 2022). In the case of LLMs, Source \nCode could be interpreted more broadly as an understanding of what this tool is and is not, can do \nand cannot do. Only with a solid foundation in this can semantic skills related to Intentionality and \nVeracity be properly developed. Such a solid understanding of LLMs can, for example, prevent the \nmisconception that LLMs have an understanding of the world, and express this understanding of the \nworld in language, rather than the correct understanding of LLMs as stringing words together in ways \nthat reflect probabilities based on their training data. \nIntentionality comes into play when benign and less benign reasons for utilizing LLMs and LLM-\ngenerated texts are discussed. Frederking understands Intentionality as ‚Äúrecognizing or transparent \nmaking of the explicit and implicit Intentionality of a text (...) in the act of reception and production‚Äù \n(Frederking, 2022, p. 7, translation by the author). Frederking, drawing on Eco (1992), distinguishes \nthree types of Intentionality: intentio operis (text intention), intentio auctoris (author‚Äôs intention), and \nintentio lectoris (reader‚Äôs intention). He argues, again following Eco, that text intention is the one \nthat is best deducible on the basis of textual evidence alone (Frederking, 2022, p. 14). In the case of \nLLM-generated texts, where it is impossible to attribute Intentionality to the LLM itself and it may be \n5 Technology in Language Teaching & Learning, 6(1)\ndifficult to attribute Intentionality to the (machine-generated) text, complex but fascinating questions \nare opened up. Finally, Veracity, understood here as ‚Äúsuccessful assessment of the truth content of \ndigitally consumed or presented text statements‚Äù (Frederking, 2022, p. 7, translation by the author) ties \ninto discourses of the reliability of LLM-generated texts and biases within these texts (cf. e.g., Bender  \net al., 2021). \nFinally, regarding the distinction between functional-technological and personal-reflexive digital text \nsovereignty, ‚Äúfunctional-technological‚Äù refers to a habitus of applied, competent, self-determined \nreception and production of digital texts. Personal-reflexive, on the other hand, is a habitus ‚Äúthat \nunderstands digital texts as phenomena which can be used for creative self-assurance, questions them \nwithin the horizon of one‚Äôs understanding of self and world, and (...) constructively-critically reflects \ntheir consequences‚Äù (Frederking, 2019; 2020, p. 7, translation by the author). \nI argue that digital text sovereignty as a concept is especially suited to the context of TEFL teacher \neducation, as its focus on textuality makes it especially relevant in language learning contexts ‚Äì more \nso than other concepts, such as AI literacy (overviews AI literacy in teacher education & school: Casal-\nOtero et al., 2023; Olari & Romeike, 2021). \nLLMs and Digital Text Sovereignty in Teacher Education\nThe discussion above looked at LLMs such as ChatGPT and digital text sovereignty mostly from the \nperspective of learners. In the remainder of the text, I will focus on TEFL teacher education in the \ncontext of LLMs, using digital text sovereignty as my theoretical lens. \nLLMs deserve to be discussed in the context of (TEFL) teacher education for two reasons. Firstly, \nto respond to the easy availability of LLMs ‚Äì and not only with ‚Äúpedagogy of the gaps‚Äù ‚Äì (future) \nteachers need what might be framed as an ‚ÄúAI-informed pedagogy‚Äù. As part of this, in order to be able \nto foster digital text sovereignty in an age of AI, they also need to possess digital text sovereignty, \nincluding both the functional-technological and the personal-reflexive aspects.4 \nMishra und Koehler‚Äôs TPACK model (Mishra, 2019; Mishra & Koehler, 2006) describes how \nTechnological Knowledge is not enough. Only if Pedagogical Knowledge and Content Knowledge ‚Äì  \nand, importantly, the overlaps between these three main types of knowledge ‚Äì are present can teachers \nsuccessfully teach with media and/or in a context of media usage. Analogous to this model, I do not claim \nthat digital text sovereignty alone leads to a successful AI-informed pedagogy and its implementation, \nbut that without digital text sovereignty (a term that combines aspects of technological aspects and \ncontent knowledge, but goes beyond these with the inclusion of personal-reflexive aspects) such an \nAI-informed pedagogy is not meaningfully possible. \nSecondly, teacher education in itself also uses texts and writing as methods. The writing of term \npapers is not merely an assessment practice, but also a route for skill and knowledge development. \nTeacher education students take notes, summarize research, brainstorm, engage in translation, etc. as \n4 It is worth mentioning that many challenges in education cannot be resolved only through competent actions by individual \nteachers. As V ogt and Flint (2023) stress, institutional support plays an important role in technology integration, and this \nwill most likely impact teachers‚Äô responses to AI as well. Though in this paper I use a purely ‚Äúindividual‚Äù view of digital \ntext sovereignty, the arguments brought forward by V ogt and Flint remind us of the origin of the term in the notion of \ndigital/data sovereignty, and how that term is frequently used to denote features of communities (such as the state), or \nrights, rather than the skills of individuals. Such a broader view of digital text sovereignty, even if not applied in this paper, \nhas some promise. \n‚ÄúJust a Pocket Knife, Not a Machete‚Äù 6\npart of their teacher education. Tools that can support them in that ‚Äì or that can, if used injudiciously, \ntake away from them the opportunity for skill development ‚Äì therefore deserve discussion in teacher \neducation. \nYet, supporting TEFL teacher education students in developing their digital text sovereignty is not \nwithout its challenges. V ogt and Flindt (2023), for example, stress that one key element in preparing \nteachers for using and/or reflecting on AI tools in the EFL classroom is the affective dimension. \nFurthermore, in drawing on experiences with teacher education in the context of digitalization more \ngenerally, they stress the importance of institutional support. \nAnother challenge in preparing teachers for teaching in an age of generative AI and of LLMs is that \nteacher educators, too, require the necessary skills. Moorhouse & Kohnke (pre-publication) looked \nspecifically at English language teacher educators in Hong Kong. The 13 EL teacher educators \ninterviewed (in April and May 2023) indicate a high level of needing to learn more about this topic, \nand a perceived urgency to adapt to a world with generative AI tools.\nResearch Question\nThis paper attempts to contribute to what is at the moment a small number of empirical studies focusing \non TEFL teacher education for an age of AI, including easily accessible LLMs. It attempts to answer \nthe following question: How do TEFL teacher education students address the different dimensions of \ndigital text sovereignty related to AI and LLMs, and to ChatGPT specifically, after participating in a \n90-minute interactive intervention?\nIt is hoped that answering this question may play a role for (a) modelling digital text sovereignty as \nit applies to AI, and (b) developing teacher education material and programs that target digital text \nsovereignty in an AI context, or that aim to support (future) teachers in developing an AI-informed \npedagogy.\nMethod\nParticipants\nStudents from three intact university TEFL teacher education seminars at a German metropolitan \nuniversity participated (Table 2). All lecturers (excepting adjunct-equivalent teachers) within the \ndepartment were invited to participate with their courses. Four lecturers volunteered. One course was \nexcluded, as the specific format of the course and the types of writing in which students engaged in the \ncourse would not have harmonized with the intervention planned. \nThe remaining three courses were all part of a four-year degree program, with two courses targeting \nstudents in the first two years of study (roughly BA-level equivalent), and one course targeting more \nadvanced students (roughly MA-level equivalent). All courses were open to a range of TEFL teacher \neducation specializations, which could include TEFL for primary school, TEFL for different types of \nsecondary school, TEFL for vocational school, or TEFL with a focus on special education. The exact \ncomposition of courses varied. \nBased on the courses students were in, they must have previously completed one or two TEFL didactics \nmodules, and some students would also have already completed their teaching practicum. No demo -\ngraphic data on participants was collected. \n7 Technology in Language Teaching & Learning, 6(1)\nTable 2 Overview Over Sample\nSeminar Seminar G1 Seminar G2 Seminar G3 Total\nAcademic level BA-level \nequivalent\nBA-level equivalent MA-level \nequivalent\nAcademic focus Digital Media in \nTeaching\nTeaching Speaking \nSkills\nCLIL & \nMultilingualism\nParticipants who provided informed \nconsent for use of their essays\n7 9 8 24\nTotal number of essays retained in \nthe study\n7 6 8 21\nMaterials and Procedure\nThe core of this study is a 90-minute intervention that could be integrated flexibly into TEFL teacher \neducation seminars. The intervention was a ‚Äúlearning experience(s) focused on understanding AI‚Äù, \n(Casal-Otero et al., 2023, p. 6) in the form of an interactive, discussion-based format. It consisted of \n(a) an introduction to LLMs, (b) a group discussion of the purpose of writing in TEFL and in TEFL \nteacher education, (c) individualized and flexible work in a ‚Äúwork stations‚Äù-based mode, and (d) an \nend-of-session reflection in which students shared what they had learned. A follow-up assignment was \nset as homework. \nFor the individualized and flexible work in phase (c), students received a worksheet that included three \ntasks. Within the classroom, three stations had been prepared that students could access at their own \npace. Students worked through the three tasks in any order they wished, at the speed they wished, and \nwhile working with any partner(s) they wished. The author and the lecturer of the course were present, \nanswering questions and engaging in discussions with students to encourage them to justify their \nopinions, challenge their assumptions, or consider a question from multiple perspectives. The three \ntasks used can be found in the Appendix. They are also discussed in more detail in Buendgens-Kosten \n(forthcoming), which documents the intervention‚Äôs pilot during the 2022/2023 winter term, with a \nfocus on multiliteracies. \nThe follow-up assignment was identical for the three courses: \nWrite ~500 words on what ChatGPT and similar tools mean ‚Äì or will mean ‚Äì for EFL teaching and \nlearning, including assessment. In your text, draw on the activities done today. Also draw on additional \nresources (formal or informal) that focus either on: \n‚Ä¢ How to use ChatGPT as a tool to support your writing/thinking/planning‚Ä¶\n‚Ä¢ The ethics of tools like ChatGPT (using existing texts to build a language model, \nlow-paid workers, cheating, risks for society, bias in AI, liberating individuals from \nmind-numbing busywork‚Ä¶). \nPlease indicate all sources following AP A 7th edition. \nUpload your reflection to your course LMS area.\nThe intervention took place in a regular seminar session and supported learning goals of that seminar; \nconsequently, participation in classroom activities associated with the intervention was mandatory. \nOnly data from students who provided informed consent was used. Students who did not wish to provide \n‚ÄúJust a Pocket Knife, Not a Machete‚Äù 8\ninformed consent had no negative ramifications, and could enjoy the same educational experience as \nstudents who provided informed consent. \nFor the purposes of this paper, only the 24 essays created as follow-up assignments will be consid -\nered. A detailed discussion of findings related to the worksheets within the pilot study can be found in \nBuendgens-Kosten (forthcoming). \nData Analysis\nThe author and/or the lecturer in the respective class anonymized the essays. As the essays were \nwritten at home, and strong AI usage standards were not yet in place and non-use of AI could \nnot be effectively monitored, the need to establish whether texts were indeed written by students \narises. Consequently, a minimal standard was applied to essays to improve the probability that the \ntexts reflected actual student ideas. All essays had to reference the course activities and reference \nadditional published material, or, in the absence of references to published material, required \nespecially clear references to course materials. This minimal standard was chosen as references to \nthe course material were clear evidence that learners were actively engaged in producing the text, \nif not in writing the whole texts themselves, then in designing prompts that included descriptions \nof course activities or specific ideas they wanted to express. References to published material were \none of the weaknesses of early ChatGPT. At the point in time that data was collected, the presence \nof correct references therefore implied traditional writing or at least editing of a generated text. This \nprocedure led to the exclusion of G2_2, G2_6 and G2_7. Text G2_6 was clearly AI generated: it \nmade no reference to in-class activities and referenced several nonexistent (‚Äòhallucinated‚Äô) texts, a \ntell-tale sign of early ChatGPT. G2_2 and G2_7 are more complex cases that might reflect disregard \nof the task instructions more than actual AI use, especially as their style was unlike the ‚Äòoverly-\npolished‚Äô style frequently found in ChatGPT-generated essays. Still, these texts were excluded out \nof an overabundance of caution.\nThe remaining essays were analyzed using top-down qualitative content analysis. The author identified \nsections in each that reflect ideas or arguments related to digital text sovereignty, based on Frederking‚Äôs \n(2022) eight dimensions, focusing on Mediality, source text, Intentionality and Veracity. For border -\nline cases, it was decided to include them rather than to exclude them, to provide the broadest picture \npossible. When statements could fit into more than one category, they were coded for both categories. \nIn a second step, the author identified key topics within each category. \nResults and Discussion\nMediality\nEvery student (21 out of 21) addressed questions related to Mediality. Primarily, these were descrip -\ntions of what the tool can do, plus discussions of how this is done and what weaknesses might exist. \nDescriptions of use cases were very varied, and in general realistic, encompassing ways that both \nteachers and learners might use the tool, focusing on text generation and input ‚Äì though one student \nmentioned multimodality and one other student mentioned audio input (G1_1, G3_4). Options for \nmultilingual and mediation/translation use were mentioned by several students. Descriptions of ‚Äúhow \nto‚Äù tended not to focus on technological aspects, but rather on integration in e.g. the writing process. \nA recurrent theme was that ChatGPT is good at some tasks and not so good at others, with several \nstudents pointing out that outputs based on assumedly simple questions could be of a surprisingly low \n9 Technology in Language Teaching & Learning, 6(1)\nquality. Similarly, many students commented that reflections and other text types that require drawing \non personal experience and emotions were often of lower quality:\n‚ÄúChat GPT is a suitable tool for simple questions, but it is useless for critical reflection \non personal experiences. (...) During the daily classes, students may use ChatGPT here \nor there, but if it is a game-changer for students also depends on the way of teaching. \nSince we are taught the dominance of TBLL/TBLT (...), ChatGPT is just a pocket knife, \nnot a machete.‚Äù (G1_7)\nThere seems to be a focus on productive use, though the distinction might not be very helpful in this \ncontext, as productive use by the learner might imply receptive use by the teacher, and vice versa. \nThe codes collected for ‚ÄúMediality‚Äù seem to fall primarily under ‚Äúfunctional technological‚Äù, but \nthis may be attributable to the analytical procedure chosen here, as ‚Äúquestion(s)[ing] them within \nthe horizon of one‚Äôs understanding of self and world, and (...) constructively-critically reflect(s)[ing] \ntheir consequences‚Äù (Frederking, 2022, p. 7, translation by the author) touched on Intentionality \n(consequences of academic dishonesty) and Veracity (bias). Some statements clearly touch on the \npersonal-reflexive, though, such as this one: \n‚ÄúAn example could be that students can ask ChatGPT for topics for an essay or a \npresentation. However, it could also lead to the students being less creative (...).‚Äù (G1_1)\nAnother example is this comment, which touches on potential impacts of AI on society more \nbroadly: \n‚ÄúA page that could create paintings or designs in seconds is thus able to do without \nillustrators/cartoonists. Or to people using a writing AI and selling books on renowned \nplatforms such as Amazon (Klotz, 2023).‚Äù (G3_1)\nSource Code\nMost students (15 out of 21) touched upon Source Code, but often in a very superficial way. \nAt the most basic level, a technical-functional understanding of what ChatGPT is involves identifying \nit as an AI tool, more specifically an LLM. Both concepts were introduced in the intervention in a brief, \nnon-technical way. Two thirds of the texts (14 out of 21) explicitly mention AI/Artificial Intelligence \n(or the German equivalent, KI). Usually little additional commentary is provided, making it difficult to \njudge the degree to which this concept is understood. G3_5 is an exception here: \n‚ÄúIn the modern era of technology, artificial intelligence (AI), which refers to the \nreplication of human intelligence processes by machines, particularly computer \nsystems (Burns et al., 2023) has emerged as a pivotal subject in discussions concerning \ntechnological advancement.‚Äù (G3_5)\nContrary to G3_5, G3_1‚Äôs response seems to conflate ChatGPT with AI tools more generally: \n‚ÄúIn the past year, humans have been introduced to artificial intelligence, aka AI.‚Äù\nFour students also explicitly refer to (large) language models (LLMs), though most of them (three) \neither prefer to use the more general term ‚Äúlanguage model‚Äù, or misremembered the term as used \nin class. Again, barely any additional commentary was provided. The fact that so few used the term \n‚ÄúJust a Pocket Knife, Not a Machete‚Äù 10\n‚Äú(large) language model‚Äù, suggests that this was a new concept for students attending the intervention ‚Äì  \nunlike AI, which students were already familiar with, at least at a superficial level. \nInterestingly, two students (G2_9, G3_1) refer to bots, a term not introduced in the intervention, which \nis however not technically incorrect: \n‚ÄúChatGPT is an artificial intelligence-based chat-bot that is available online and can \ngenerate written cohesive responses to the requests and inputs of the user.‚Äù (G2_9)\nG1_3 goes in a similar direction while stressing the natural language processing skills of ChatGPT, \nreferring to it as a ‚Äúnatural language processing tool‚Äù and to ‚Äúnatural language queries‚Äù, technically \ncorrect terminology that was, again, not used/not stressed in the intervention. It shows that in addition \nto the intervention, students drew on prior knowledge, on interactions with fellow students, or on \nreadings. \nI will not reiterate what students wrote about the capabilities and limitations of ChatGPT, as this over-\nlaps with the relevant section on Mediality. Some of the statements of what ChatGPT can or cannot \n(assumedly) do, though, reflect misconceptions of how it works, which I will touch upon now. \nG1_3 demonstrates a minor misconception by writing of watermark technology as if it were already \nwidely implemented. Some students made comments the veracity of which is hard to judge, as it \ndepends on precise ways in which the LLM is trained and user input data is stored and used. This \napplies primarily to G1_4, G1_5, G1_6, G2_3, and G3_2. The example of G2_3 shows, though, how \nin practice their understanding would be sufficient for competent use of the tool: \n‚ÄúTo be more specific, the language model is in some ways limited when it comes down \nto generate answers. The following should be considered, before using ChatGPT: \n1)  The language model is not up to date and can therefore provide altered information, \nstudies etc. that are not entirely correct nowadays. \n2)  The language model has no access to all the sources on the internet and especially \nacademic sources. Therefore, it is difficult to know whether the provided text/answer \nis very reliable, if it will not be checked. \n3)  ChatGPT does not cite (at least not always correctly). As the language model pulls its \ninformation out of other sources, users are likely to plagiarize.‚Äù (G2_3)\nQuestions related to factual correctness of generated text will be taken up again below. \nAs I will also discuss under ‚ÄúVeracity‚Äù below, there is some degree of uncertainty as to whether \nstudents understood that ChatGPT is primarily a stochastic parrot (Bender et al., 2021) as well, i.e. \nthat it generates texts based on probabilities derived from other texts, rather than based on world \nknowledge. \nWhat may be most interesting is which misconceptions were not found in the data. No student attributed \nintelligence, creativity, sentience, etc. to the tool itself. It was clearly labelled as a tool, including by \nmetaphorical references to physical tools. Students did not personify the tool, even in the contexts in \nwhich they referred to it as ‚Äúconversation partner‚Äù (G1_2). Students, in general, seemed to be aware \nthat the tool generated text rather than just copying text ‚Äì though expressions like in G2_3‚Äôs text above \nmight hint at some vagueness in their understanding in that regard. These correct assumptions are \nimportant in many ways, e.g. for understanding how the tool can be used for academic dishonesty, and \n11 Technology in Language Teaching & Learning, 6(1)\nthe options teachers do and do not have in identifying this through technical means ‚Äì a topic that will \nbe discussed in more detail under the header of ‚ÄúIntentionality‚Äù. \nIn general, if ‚Äúproductive use‚Äù is understood here as the ability to develop (large) language models or IT \napplications that build upon them, no evidence for productive skills/habitus can be found in the dataset. \nAt the same time, students seemed to possess enough understanding ‚Äì even if, possibly, not always \ntechnically quite correct ‚Äì to approach generated texts with an understanding of their limitations and \nof the risks involved in the (injudicious) use of those texts. \nIntentionality\nAll but one text touched on the topic of Intentionality. By far the biggest topic in this section were \ncomments related to academic dishonesty, cheating and plagiarism, focusing on learners as perpetrators. \nWith the exception of two students (G2_1 and G1_1), all (i.e. 19 out of 21) student texts addressed \none or more of these. As stated above, Frederking, drawing on Eco (1992), distinguishes three types \nof intention: intentio operis (text intention), intentio auctoris (author‚Äôs intention) and intentio lectoris \n(reader‚Äôs intention), arguing, again following Eco, that text intention is most clearly deducible on \nthe basis of textual evidence alone (Frederking, 2022, p. 14). G3_1, though, commented more on \nthe author‚Äôs intention, framing academic dishonesty as a mismatch between teachers‚Äô and learners‚Äô \nintentions:\n ‚ÄúAt this point, how ethical is the use of artificial intelligence programs? It is indeed a \ntool that, if used correctly, helps to optimize working time. However, we have already \nseen that many students, for example, use these technologies for more free time.  \nA college and a university should be an establishment where students express their \nthoughts and share them with others. What is happening so that there is no desire to \nexpress themselves?‚Äù (G3_1)\nThis, in turn, is framed by several students not just as an issue for assessment, but as an issue for \nlearning: \n‚ÄúUsing Chat GPT to prepare written assignments compromises the success of learning \nthis competence.‚Äù (G2_9)\nThe consensus seems to be that (school and university) students can and do use ChatGPT in ways that \ndo not reflect their writing skills and their own ideas. Many students commented on the difficulty in \nidentifying if ChatGPT or similar tools had been used (receptive, technical-functional Intentionality). \nOn the other hand, G3_3 for example argues that identifying use of ChatGPT should be easy via \ncomparison with a text sample: \n‚ÄúIn the case of assessment, I noticed during the session with Dr. [author‚Äôs name] that \nChatGPT writes too sophisticated and without personal style. Hence, the teacher could \neasily spot whether a text was written by ChatGPT or the students if he/she knew the \nstyle of the students. My idea would be to take a photo of a piece of writing that has \nbeen written in a lesson (perhaps at the beginning of the school year and then later \nin class) to have this as reference point when it comes to assessing a homework that \nshould have been written by the students.‚Äù (G3_3)\nThe kind of analytical skills described by G3_3 perfectly reflects the digital text sovereignty of a \nteacher, who is seen as able to deduce intent of a text (or its author) through detailed textual analysis.\n‚ÄúJust a Pocket Knife, Not a Machete‚Äù 12\nImportantly, none of the students seems to attribute intention to ChatGPT. Instead, intention was \nattributed to the users of ChatGPT, reflecting an understanding of what the tool is and is not (cf. section \non Source Code). \nAnother aspect that could have been addressed here is bias, as bias can be understood as an (in this \ncontext) unintentional content feature of texts. I decided instead to discuss these text segments in the \nfollowing section, which is concerned with Veracity. \nVeracity\nOnly 13 out of 21 essays were coded as touching upon Veracity. \nThere were two main themes in the data. Firstly, many students (7 out of 21) referred to ChatGPT and \nsimilar tools as sources of information. See for example G3_8, referring to a paper they had selected \nthemselves: \n‚ÄúIf students require direct answers, they could use AI as a reliable option (AlAfnan  \net al., 2023, p. 65).‚Äù (G3_8)\nOr, even stronger, G3_2, again drawing on a paper they had selected: \n‚ÄúAdditionally, Biswas argues that ChatGPT could for example help find a solution for \nclimate change. (...) ChatGPT can compress complex topics. This makes it easier to \ngain an overview and develop possible strategies. In order to prepare students for life \nas a responsible member of society, issues like climate change and social problems \nlike poverty are discussed in school. Therefore, it makes sense to explain ChatGPT \nto students, as they will be confronted with the major problems of our society as \nresponsible members of society. ChatGPT has the potential to help find a solution to \nthese big, complex problems.‚Äù (G3_2)\nIt would be wrong to conclude, though, that students viewed this uncritically. They provided many \ncaveats, using expressions such as ‚Äúnot up to date‚Äù (G2_3), ‚Äúnot (...) checked‚Äù (G2_3), ‚Äúoutdated‚Äù \n(G1_5), ‚Äúwrong‚Äù (G1_5), ‚Äúfaulty‚Äù (G1_5), ‚Äúshallow, unsatisfactory‚Äù (G1_4) and ‚Äúcorrect-sounding \nnonsense‚Äù (G1_4). On the other hand, at least one student saw the risks related to Veracity as opportunity \nto develop students‚Äô critical thinking skills (G2_3). \nG3_3 argues that the Veracity of texts generated by ChatGPT reflects patterns of texts written by \nhumans. Drawing on an article they had chosen and read, they argued: \n‚ÄúThe correct usage includes educating students that ChatGPT answers their prompts \nwith the data available to the system (Wiggenbr√∂ker, 2023). This means that the \namount of data determines the answers (Wiggenbr√∂ker, 2023). For instance, if more \narticles on the internet propose that the earth is flat than round, then ChatGPT will give \nthe former as answer as it cannot think for itself (Wiggenbr√∂ker, 2023). Especially \nin the age of misinformation and fake news this is a huge concern. Students need to \nbe aware that information given by ChatGPT must be checked, as it is not always \ncorrect.‚Äù (G3_3)\nWhile there was a widespread awareness of risks related to incorrect information obtained through \nChatGPT, the exact mechanisms of how this occurs (i.e. that ChatGPT is a large language model \n13 Technology in Language Teaching & Learning, 6(1)\nthat uses patterns in language to generate texts, not world knowledge) is not necessarily reflected in \nthese responses. \nFinally, four students address bias, prejudice and stigma. As G3_6 describes it: \n‚ÄúAdditionally, ChatGBT [sic] is fed with information developed from human thinking. \nTherefore, wrong human thinking for example racism is reproduced, which can be \ndangerous.‚Äù (G3_6)\nFor Veracity specifically, drawing the line between functional-technical and personal-reflexive is \ndifficult. The need to fact-check can be seen as a step within a writing or reading process (productive \nperspective), but as a reader (receptive perspective), the knowledge that ChatGPT texts cannot be \ntrusted, even if the prompt author has only the best intentions, will teeter into the personal-reflexive. \nLimitations\nImportantly, as essays were written after an intervention, this study does not document the knowledge, \nskills and habitus of ‚Äòna√Øve‚Äô students. Care needs to be taken before drawing conclusions about teacher \neducation students or in-service teachers in general. \nThis study was conducted in three intact seminars and followed university seminar logic in that \nthe essays analyzed were learning tasks for the students as much as they were sources for research \ndata. The specific design of in-class activities and writing prompts will have impacted the topics \naddressed by students, and other activities and other writing prompts might lead to slightly different \nfindings. \nThe approach chosen was not optimal for detailed diagnostic information on comprehension and skill \nin the area of Source Code. In future studies, pen-and-paper tests might contribute more precise infor-\nmation, albeit at the risk of disregarding the TEFL-specific focus. \nConclusion\nIn this study, I analyzed students‚Äô essays, written after a 90-minute interactive intervention, with a \nfocus on digital text sovereignty. These essays touched on the four aspects of digital text sovereignty,  \nMediality, Source Code, Intentionality and Veracity, though with differing frequency and differing \n intensity. Applying the distinction between productive and receptive use and between technical-  \nfunctional and personal-reflexive was attempted, but not always straightforward. \nSeveral conclusions can be drawn, and several questions arise from the analysis. Firstly, while digital \ntext sovereignty still appears a very valuable theoretical lens for AI in foreign language teaching and \nlearning, in practice, several of the aspects overlapped. For example, Source Text and Veracity are \nintertwined when talking about generative text AI, as without an understanding of how the text is gen-\nerated, judging its veracity is limited to fact-checking.\nSecondly, there is a range of frequently reported misconceptions that students either never held or \nthat were remedied by an interactive 90-minute intervention. More work on students‚Äô knowledge and \nbeliefs can establish which of these two interpretations is more likely. Both interpretations, though, \nare promising regarding supporting students in developing high levels of digital text sovereignty in the \ncontext of AI and LLMs. \n‚ÄúJust a Pocket Knife, Not a Machete‚Äù 14\nSecondly, establishing how well students understood some aspect of Source Code was hard to estab -\nlish through the means chosen here. At the same time, it is unclear if vagueness, for instance about the \nmechanism that causes ChatGPT to ‚Äòhallucinate‚Äô false information, impacts the ability to deal with the \nrisks inherent in that. Or, phrased as a question: How much technical understanding of AI and LLMs \ndo we need to engage competently with AI and LLMs in daily life and in academic settings?\nFinally, it should be acknowledged that, while the teacher education students in this study suggested \nmany ways to deal with perceived possibilities and limitations, including e.g., using ChatGPT to \npractice specific language and transversal skills, as of now, we have very little knowledge on how these \nactions impact school students‚Äô language competence or digital text sovereignty. In other words, which \nimplementations of AI in EFL teaching are indeed ‚Äúconducive to learning for a variety of foreign \nlanguage competences‚Äù (V ogt and Flindt, 2023, p. 182) is, as of yet, unclear. \nReferences\nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of stochastic \nparrots: can language models be too big? ü¶ú. Proceedings of the 2021 ACM Conference on \nFairness, Accountability, and Transparency, 610‚Äì623. https://doi.org/10.1145/3442188.3445922\nBerthele, R., & Udry, I. (2023). Welche fremdsprachlichen Lernziele der Schulen k√∂nnen mit digitalen \nTools aber ohne Lernen erreicht werden? Simply playing the ostrich will not help in the long \nrun. Zeitschrift f√ºr Interkulturellen Fremdsprachenunterricht, 28(April), 443‚Äì461.\nBonner, E., Lege, R., & Frazier, E. (2023). Large language model-based artificial intelligence in the \nlanguage classroom: Practical ideas for teaching. Teaching English with Technology, 2023(1). \nhttps://doi.org/10.56297/BKAM1691/WIEO1749\nBuendgens-Kosten, J. (forthcoming). ‚ÄúLike a shell without actual content‚Äù: Large language models \nas a topic in EFL teacher education. In C. Blume (Ed.), Multiliteracies-aligned Teaching and \nLearning in Digitally-mediated Second Language Teacher Education. Routledge. \nCasal-Otero, L., Catala, A., Fern√°ndez-Morante, C., Taboada, M., Cebreiro, B., & Barro, S. (2023). \nAI literacy in K-12: A systematic literature review. International Journal of STEM Education, \n10(1), 29. https://doi.org/10.1186/s40594-023-00418-7\nFrederking, V . (2022). Digitale Textsouver√§nit√§t. Personale und funktionale textuelle Bildung im \nDeutschunterricht im Zeichen digitaler Transformation: Eine Theorieskizze: Version 3 . \ndeutschdidaktik.phil.fau.de/files/2021/09/digitale-textsouveraenitaet.pdf\nFrederking, V ., & Krommer, A. (2022). Sprachliche, literarische und mediale Bildung in der digitalen \nWelt. In V . Frederking & R. Romeike (Eds.), Fachliche Bildung in der digitalen Welt: \nDigitalisierung, Big Data und KI im Forschungsfokus von 15 Fachdidaktiken. Allgemeine \nFachdidaktik Band 3 (pp. 82‚Äì120). Waxmann.\nFriedrichsen, M., & Bisa, P.-J. (Eds.). (2016). Digitale Souver√§nit√§t: Vertrauen in der \nNetzwerkgesellschaft. Springer VS. https://doi.org/10.1007/978-3-658-07349-7\nGr√ºnewald, A. (2019). Digitaler Wandel: Warum √ºberhaupt noch Fremdsprachen in der Schule lernen? \nIn E. Burwitz-Melzer, C. Riemer, & L. Schmelter (Eds.), Das Lehren und Lernen von Fremd- \nund Zweitsprachen im digitalen Wandel: Arbeitspapiere der 39. Fr√ºhjahrskonferenz zur \nErforschung des Fremdsprachenunterrichts (pp. 80‚Äì89). Narr.\nHummel, P., Braun, M., Tretter, M., & Dabrock, P. (2021). Data sovereignty: A review. Big Data & \nSociety, 8(1), 205395172098201. https://doi.org/10.1177/2053951720982012\nJeon, J. (2022). Exploring AI chatbot affordances in the EFL classroom: Young learners‚Äô experiences \nand perspectives. Computer Assisted Language Learning , 1‚Äì26. https://doi.org/10.1080/0958\n8221.2021.2021241\n15 Technology in Language Teaching & Learning, 6(1)\nKohnke, L., Moorhouse, B. L., & Zou, D. (2023). ChatGPT for Language Teaching and Learning. \nRELC Journal, 54(2), 537‚Äì550. https://doi.org/10.1177/00336882231162868\nKoraishi, O. (2023). Teaching English in the age of AI: Embracing ChatGPT to optimize EFL materials \nand assessment. Language Education & Technology, 3(1), 55‚Äì72.\nLee, S.-M. (2020). The impact of using machine translation on EFL students‚Äô writing. Computer \nAssisted Language Learning , 33(3), 157‚Äì175. https://doi.org/10.1080/09588221.2018.15531\n86\nLee, Y .-J. (2021). Still taboo? Using machine translation for low-level EFL writers. ELT Journal, \n75(4), 432‚Äì441. https://doi.org/10.1093/elt/ccab018\nMahlow, C. (2023). Large language models and artificial intelligence as tools for teaching and learning \nwriting. Osnabr√ºcker Beitr√§ge zur Sprachtheorie , 101, 175‚Äì196. https://doi.org/10.17192/\nOBST.2023.101.8607\nMishra, P. (2019). Considering contextual knowledge: The TPACK diagram gets an upgrade. Journal \nof Digital Learning in Teacher Education, 35(2), 76‚Äì78. https://doi.org/10.1080/21532974.20\n19.1588611\nMishra, P., & Koehler, M. J. (2006). Technological pedagogical content knowledge: A framework for \nteacher knowledge. Teachers College Record: The Voice of Scholarship in Education, 108(6), \n1017‚Äì1054. https://doi.org/10.1111/j.1467-9620.2006.00684.x\nMoorhouse, B., & Kohnke, L. (2023). The Effects of Generative AI on Initial Language Teacher \nEducation: The Perspectives of Teacher Educators [Preprint]. SSRN. https://doi.org/10.2139/\nssrn.4532479\nOlari, V ., & Romeike, R. (2021). Addressing AI and data literacy in teacher education: A review of \nexisting educational frameworks. The 16th Workshop in Primary and Secondary Computing \nEducation, 1‚Äì2. https://doi.org/10.1145/3481312.3481351\nPack, A., & Maloney, J. (2023). Using generative artificial intelligence for language education research: \nInsights from using OpenAI‚Äôs ChatGPT. TESOL Quarterly, tesq.3253. https://doi.org/10.1002/\ntesq.3253\nV ogt, K., & Flindt, N. (2023). Artificial intelligence and the future of language teacher education: A \ncritical review of the use of AI tools in the foreign language classroom. In P. Hohaus & J.-F. \nHeeren (Eds.), The future of Teacher Education: Innovations across Pedagogies, Technologies \nand Societies (pp. 179‚Äì199). Brill.\nYang, H., Kim, H., Lee, J. H., & Shin, D. (2022). Implementation of an AI chatbot as an English \nconversation partner in EFL speaking classes. ReCALL, 34(3), 327‚Äì343. https://doi.org/10.1017/\nS0958344022000039\n‚ÄúJust a Pocket Knife, Not a Machete‚Äù 16\nAppendix\nStation A: Can ChatGPT Do This? \nFind a partner. Together, pick a few different tasks cards. Sort them according to how ‚Äòhard‚Äô you think \nthey‚Äôd be for ChatGPT. Together, discuss how ‚Äì and why ‚Äì you sorted them the way you did. \nOnly after you have committed to one way of sorting them , check the flipsides. Is this what you \nexpected? Take notes. \nStation B: Compare Your Text to the ChatGPT Version\na) Compare the text you have written with one or two of the texts generated by ChatGPT. \nWhich strengths and weaknesses does your text have, which strengths and weaknesses \ndoes the ChatGPT text have? \nb) Exchange your observations (...). Make sure to also discuss the different prompts, and \nhow they relate to the different texts. \nStation C: Can You Create ‚ÄúCheat-Proof‚Äù Tasks?\nLook through the cards. Would these ideas make writing tasks and writing assessments ‚ÄúAI-cheat-\nproof‚Äù? ‚ÄúAI-cheat-proof‚Äù means, in this context, that taking these measures makes it more likely \nthat the work products reflect students‚Äô writing skills and/or content knowledge, and that stu -\ndents are more likely to develop their writing skills and/or content knowledge with these ideas \nin place. \nStep 1: Look at the suggestions provided. Put a green sticker on the ideas you find helpful, and a black \nsticker on the ideas you find not helpful. For this activity, focus on the context of EFL at school. \nStep 2: Find a partner. Together, discuss at least three cards in more detail. What are advantages/disad-\nvantages of this idea? Take notes. \nStep 3: Which other measures could one take? Use the yellow cards for ideas related to TEFL \nteacher education, blue cards for EFL learning at school . Remember: Write in GREEN (on the \ncards) if you are ok with your ideas being used for research."
}