{
    "title": "Florence: A New Foundation Model for Computer Vision",
    "url": "https://openalex.org/W3215626407",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2101029194",
            "name": "Yuan Lu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1943263669",
            "name": "Chen Dongdong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4229208714",
            "name": "Chen, Yi-Ling",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3193800045",
            "name": "Codella, Noel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222155691",
            "name": "Dai, Xiyang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2119363152",
            "name": "Gao, Jianfeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3181947683",
            "name": "Hu Houdong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2274911023",
            "name": "Huang, Xuedong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4284555620",
            "name": "Li Boxin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2225388594",
            "name": "Li, Chunyuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2222005704",
            "name": "Liu, Ce",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2348018239",
            "name": "Liu, Mengchen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2352283912",
            "name": "Liu, Zicheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226099901",
            "name": "Lu, Yumao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099988764",
            "name": "SHI Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1578702133",
            "name": "Wang Li-juan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1901876785",
            "name": "Wang Jian-feng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1585946068",
            "name": "Xiao, Bin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2298792336",
            "name": "Xiao Zhen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1658219663",
            "name": "Yang Jianwei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202139475",
            "name": "Zeng, Michael",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A19601303",
            "name": "Zhou Luowei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2370394005",
            "name": "Zhang, Pengchuan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2425121537",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W3126337491",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W3184735396",
        "https://openalex.org/W2949844032",
        "https://openalex.org/W3102785203",
        "https://openalex.org/W2949474740",
        "https://openalex.org/W3150558292",
        "https://openalex.org/W3038476992",
        "https://openalex.org/W3172942063",
        "https://openalex.org/W3141280416",
        "https://openalex.org/W2796497263",
        "https://openalex.org/W3175343838",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2994814245",
        "https://openalex.org/W2952122856",
        "https://openalex.org/W3106780750",
        "https://openalex.org/W2804935296",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W3036982689",
        "https://openalex.org/W3158986867",
        "https://openalex.org/W3134873017",
        "https://openalex.org/W3170197681",
        "https://openalex.org/W2102605133",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W3210438479",
        "https://openalex.org/W3035688398",
        "https://openalex.org/W3101156210",
        "https://openalex.org/W3130033937",
        "https://openalex.org/W2593116425",
        "https://openalex.org/W3203003533",
        "https://openalex.org/W2170413022",
        "https://openalex.org/W2932083555",
        "https://openalex.org/W3202141913",
        "https://openalex.org/W3172507542",
        "https://openalex.org/W3139732141",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W3037309139",
        "https://openalex.org/W3001555892",
        "https://openalex.org/W3166144525",
        "https://openalex.org/W3193402170",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3110662498",
        "https://openalex.org/W3171087525",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2948672349",
        "https://openalex.org/W3097217077",
        "https://openalex.org/W3035635319",
        "https://openalex.org/W3176153963",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2473156356",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3176258108",
        "https://openalex.org/W2977720775"
    ],
    "abstract": "Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.",
    "full_text": "Florence: A New Foundation Model for Computer Vision\nLu Yuan1 Dongdong Chen * 1 Yi-Ling Chen * 1 Noel Codella * 1 Xiyang Dai * 1 Jianfeng Gao * 2 Houdong Hu * 1\nXuedong Huang * 1 Boxin Li * 1 Chunyuan Li * 2 Ce Liu * 1 Mengchen Liu * 1 Zicheng Liu * 1 Yumao Lu* 1\nYu Shi* 1 Lijuan Wang* 1 Jianfeng Wang* 1 Bin Xiao * 1 Zhen Xiao * 1 Jianwei Yang* 2 Michael Zeng * 1\nLuowei Zhou * 1 Pengchuan Zhang * 2\nAbstract\nAutomated visual understanding of our diverse\nand open world demands computer vision models\nto generalize well with minimal customization for\nspeciÔ¨Åc tasks, similar to human vision. Computer\nvision foundation models, which are trained on\ndiverse, large-scale dataset and can be adapted\nto a wide range of downstream tasks, are criti-\ncal for this mission to solve real-world computer\nvision applications. While existing vision founda-\ntion models such as CLIP (Radford et al., 2021),\nALIGN (Jia et al., 2021), and Wu Dao 2.0 (Wud)\nfocus mainly on mapping images and textual rep-\nresentations to a cross-modal shared representa-\ntion, we introduce a new computer vision foun-\ndation model, Florence, to expand the represen-\ntations from coarse (scene) to Ô¨Åne (object), from\nstatic (images) to dynamic (videos), and from\nRGB to multiple modalities (caption, depth). By\nincorporating universal visual-language represen-\ntations from Web-scale image-text data, our Flo-\nrence model can be easily adapted for various\ncomputer vision tasks, such as classiÔ¨Åcation, re-\ntrieval, object detection, VQA, image caption,\nvideo retrieval and action recognition. Moreover,\nFlorence demonstrates outstanding performance\nin many types of transfer learning: fully sampled\nÔ¨Åne-tuning, linear probing, few-shot transfer and\nzero-shot transfer for novel images and objects.\nAll of these properties are critical for our vision\nfoundation model to serve general purpose vision\ntasks. Florence achieves new state-of-the-art re-\nsults in majority of 44 representative benchmarks,\ne.g. ImageNet-1K zero-shot classiÔ¨Åcation with\ntop-1 accuracy of 83.74 and the top-5 accuracy of\n97.18, 62.4 mAP on COCO Ô¨Åne tuning, 80.36 on\nVQA, and 87.8 on Kinetics-600.\n*Florence Team member in alphabetic order 1Microsoft Cloud\nand AI 2Microsoft Research Redmond. Correspondence to: Lu\nYuan <luyuan@microsoft.com>.\nModality\nSpaceTimeStatic DynamicCoarseFine-grainedMulti-senseVisual (only)\nClassification\nFlowerAction Recognition\nPlaying Soccer\nObject Detection\nEagle\nEagle\nSegmentation\nDepth\nA group of women sitting around a table.Caption,,\nHow many red buttons?Visual Question AnsweringObject Tracking\nVideo Reasoning\nWhat are they talking about?\nFigure 1.Common computer vision tasks are mapped to a Space-\nTime-Modality space. A computer vision foundation model should\nserve as general purpose vision system for all of these tasks.\n1. Introduction\nHuman-like AI is not achieved by designing speciÔ¨Åc models\nto solve speciÔ¨Åc problems, but by holistic, joint models\nthat can simultaneously solve diverse, real-world problems\nwithout too much human involvement. It is thus desirable\nto have new AI architectures that learn joint, fundamental\nrepresentations to support a broad range of downstream AI\ntasks with limited additional domain knowledge, similar\nto what humans would do. One such proposal is XYZ-\ncode (Huang), where monolingual text (X), audio and visual\nsensory signals (Y), and multilingual (Z) are organically\nintegrated to create AI models that can speak, hear, see, and\nunderstand. Another approach is Pathways (Dean), a single\nmodel that can generalize across millions of tasks.\nA concrete step towards this direction is the development\nof foundation models. The term of foundation model was\nÔ¨Årst introduced in (Bommasani et al., 2021) to refer to any\nmodel that is trained from broad data at scale that is capable\nof being adapted (e.g. Ô¨Åne-tuned) to a wide range of down-\nstream tasks. Foundation models become promising due to\ntheir impressive performance and generalization capabilities.\narXiv:2111.11432v1  [cs.CV]  22 Nov 2021\nFlorence: A New Foundation Model for Computer Vision\nTwo kangaroos on a beachRetrieval\nImage-Text Dataset by Data Curationfrom Internet\nImageText\nDog\nRowers carrying boat over heads on a dock\nLanguage Encoder \nUnified Contrastive Learning\nImage Encoder (CoSwin)\nFlorence Pretrained Models\nObject-level Representation (Dynamic Head Adaptor)\nFine-grained V+L Representation (METER Adaptor)\nVideo Representation (Video CoSwin)\nClassification/Retrieval AdaptationFlorence Adaptation ModelsFlorence (Vision Foundation Model)\nClassification\nFlower\nObject Detection\nEagle\nEagle\nHow many red buttons?VQAAction Recognition\nPlaying Soccer\nUnified Vision Stack\nScalable Training Infrastructure\nDeploymentTasks\nFigure 2.Overview of building Florence. Our workÔ¨Çow consists of data curation, uniÔ¨Åed learning, Transformer architectures and adaption.\nIt shows the foundation model can be adapted to various downstream tasks and Ô¨Ånally integrated into modern computer vision system\nto power real-world vision and multimedia applications. Compared with existing image-text pretraining models (Radford et al., 2021;\nJia et al., 2021; Wud), mainly limited on cross-modal shared representation for classiÔ¨Åcation and retrieval (illustrated by light-green\nadaptation module), Florence expands the representation to support object level, multiple modality, and videos respectively.\nThey are quickly integrated and deployed into real-world\nAI systems by many researchers and developers.\nAlthough foundation models have already demonstrated\nhuge impact in NLP, e.g. , BERT (Devlin et al., 2019), GPT-\n3 (Brown et al., 2020), in computer vision it is still standard\npractice to pre-train models on labeled data sets such as\nImageNet (Deng et al., 2009). More recently, large-scale\npre-training methods such as CLIP (Radford et al., 2021),\nALIGN (Jia et al., 2021), and Wu Dao 2.0 (Wud), which\nlearn directly from Web-scale image-text pairs, show very\nencouraging progress for efÔ¨Åcient transfer learning, and\nzero-shot capability. However, such models are restricted\nto image to text mapping only tasks such as classiÔ¨Åcation,\nretrieval, and tagging.\nWe raise the question: ‚ÄúWhat is the foundation model for\ncomputer vision?‚Äù. But Ô¨Årst, in order to better deÔ¨Åne what\n‚Äúfoundation‚Äù means in computer vision, we capture the spec-\ntrum of tasks in a problem space (Figure 1) with three orthog-\nonal axes: 1) Space: from coarse (e.g. scene-level classiÔ¨Å-\ncation) to Ô¨Åne-grained (e.g. object detection), 2) Time: from\nstatic (e.g. images) to dynamic (e.g. videos), and 3) Modal-\nity: from RGB only to multiple senses (e.g. captioning and\ndepth). Due to the diversity nature of visual understanding,\nwe redeÔ¨Åne foundation models for computer visionto be\na pre-trained model and its adapters for solving all vision\ntasks in this Space-Time-Modality space, with transferabil-\nity such as zero-/few-shot learning and fully Ô¨Åne tuning, etc.\nThe adaptation for transferability is restricted to minimum\ncustomization for the pre-trained foundation models, such\nas continuing training, few epochs or few layers for Ô¨Åne\ntuning without signiÔ¨Åcantly increasing or changing model\nparameters.\nIn this paper, we present an emerging paradigm for building\na vision foundation model, called Florence. We use the name\nof Florence as the origin of the trail for exploring vision\nfoundation models, as well as the birthplace of Renaissance.\nFlorence is trained on noisy Web-scale data end-to-end with\na unifying objective, allowing the model to achieve best-in-\nclass performance across a wide range of benchmarks.\nThe ecosystem of constructing Florence consists of data\ncuration, model pretraining, task adaptations and training\ninfrascturue, as shown in Figure 2.\n‚Ä¢ Data curation . Diverse, large-scale data is the\nlifeblood of foundation models. Enabled by large\namounts of publicly available images on the Internet\nwith natural language weak supervision, we curate a\nnew dataset of 900 million image-text pairs for train-\ning. As Web-crawled data is usually noisy free-form\ntexts (e.g. , word, phrase or sentence), to attain more\neffective learning, we considerUniCL, a uniÔ¨Åed image-\ntext contrastive learning objective recently proposed\nin (Yang et al., 2022), which has demonstrated im-\nprovements over contrastive and supervised learning\napproaches.\n‚Ä¢ Model pretraining (representation learning). To learn\na good representation from image-text pairs, we used a\nFlorence: A New Foundation Model for Computer Vision\ntwo-tower architectureincluding an image encoder and\na language encoder, as commonly used in CLIP (Rad-\nford et al., 2021) and ALIGN (Jia et al., 2021). For\nthe image encoder, we chose hierarchical Vision Trans-\nformers (e.g. , Swin (Liu et al., 2021a), CvT (Wu et al.,\n2021), Vision Longformer (Zhang et al., 2021a), Focal\nTransformer (Yang et al., 2021), and CSwin (Dong\net al., 2021)). While inheriting performance beneÔ¨Åts of\nthe transformer self-attention operations (Dosovitskiy\net al., 2021b), these hierarchical architectures model\nthe scale invariance nature of images and have linear\ncomputational complexity with respect to image size, a\nproperty that is essential to dense prediction tasks such\nas object detection and segmentation.\n‚Ä¢ Task adaptations. As we have deÔ¨Åned computer vi-\nsion foundation models to adapt to various downstream\ntasks, it is vital for Florence to be extensible and trans-\nferable for this purpose. We extended the learned fea-\nture representation along space (from scene to objects)\nusing the dynamic head adapter (Dai et al., 2021a),\ntime (from static image to videos) via proposed video\nCoSwin adapter, and modality (from images to lan-\nguage) via METER adapter (Dou et al., 2021). Flo-\nrence is designed to effectively adapted in the open\nworld via few-shot and zero-shot transfer learning, with\nthe ability of efÔ¨Åcient deployment by extra training\nwith few epochs ( e.g. in retrieval). Our model can\nbe customized for various domains that application-\ndevelopers can use.\n‚Ä¢ Training infrastructure. For both energy and cost\nconcerns, it is critical to build foundation models with\nas low cost as possible. We developed scalable training\ninfrastructure to improve training efÔ¨Åciency. It consists\nof several key techniques such as ZeRO (Rajbhandari\net al., 2019), activation checkpointing, mixed-precision\ntraining, gradient cache (Gao et al., 2021) to greatly\nreduce the memory consumption and thus improves\nthe training throughput.\nFlorence signiÔ¨Åcantly outperforms previous large-scale pre-\ntraining methods and achieves new state-of-the-art results\non a wide range of vision and vision-language benchmarks.\nIt showed strength in zero-shot transfer in 12 classiÔ¨Åcation\ndownstream tasks (win 9/12, SOTA in ImageNet-1K zero-\nshot with top-1 accuracy of 83.74 and the top-5 accuracy of\n97.18), linear probe in 11 classiÔ¨Åcation downstream tasks\n(win 9/11), image retrieval zero-shot (90.9/76.7 R@1 on\nFlickr30K image-to-text / text-to-image, 64.7/47.2 R@1\non MSCOCO image-to-text / text-to-image) and Ô¨Åne-tuning\n(97.2/87.9 R@1 on Flickr30K image-to-text / text-to-image,\n81.8/63.2 R@1 on MSCOCO image-to-text/ text-to-image),\nobject detection (62.4 mAP on COCO, 39.3 mAP on Ob-\nject365, 16.2 AP50 on Visual Genome), VQA ( 80.36),\ntext-to-video retrieval zero-shot (37.6 R@1 on MSR-VTT),\nand video action recognition (top-1 accuracy 86.5/87.8 on\nKinetics-400 / Kinetics-600).\n2. Approach\n2.1. Dataset Curation\nWe leverage large quantities of image-text data available\npublicly on the internet. SpeciÔ¨Åcally, we construct a 900\nmillion image-text-pair dataset, called FLD-900M (FLD\nstands for FLorenceDataset), using a programmatic data\ncuration pipeline that processes around 3 billion Internet\nimages and their raw descriptions in parallel. Selection\nand post-Ô¨Åltering is employed to ensure data relevance and\nquality while respecting legal and ethical constraints. To\nimprove data quality, we performed rigorous data Ô¨Åltering,\nsimilar to ALIGN (Jia et al., 2021), including a simple\nhash-based near-duplicate image removal, small-size image\nremoval, image-text relevance, etc. In addition, we follow\nthe sampling strategy introduced in (Radford et al., 2021;\nRamesh et al., 2021) with the goal of achieving improved\nbalance, informativeness, and learnability of the sampled\ndataset. The Ô¨Ånal form of the FLD-900M dataset consists\nof 900M images with 900M free-form texts (ranging from\none word, phase to sentences), 9.7M unique queries, and\n7.5Btokens in total.\n2.2. UniÔ¨Åed Image-Text Contrastive Learning\nCLIP (Radford et al., 2021) implicitly assumes that each\nimage-text pair has its unique caption, which allows other\ncaptions to be considered negative examples. However,\nin web-scale data, multiple images can be associated with\nidentical captions. For example, in FLD-900M, there are\n350M image-text pairs where there are more than one im-\nages corresponding to one identical text, and all images\nassociated with the same text can be treated as positive pairs\nin contrastive learning.\nTo address this issue, we utilize a uniÔ¨Åed image-text con-\ntrastive learning (UniCL) (Yang et al., 2022), where Flo-\nrence is pre-trained in an image-label-description space.\nGiven an image-text pair, we generate a triplet (x,t,y) via\na text hash-table, where x is the image, t is the language\ndescription (i.e. , hash value), and y is the language label\n(i.e. , hash key) indicating the index of unique language\ndescription in the dataset. Note that we only map identical\nlanguage description to the same hash key, i.e. , language\nlabel. Thus, all image-text pairs mapped to the same la-\nbel y are regarded as positive in our universal image-text\ncontrastive learning. Others are still regarded as negative.\nThe uniÔ¨Åed learning objective in the common image-label-\ndescription space uniÔ¨Åes two popular learning paradigms\n‚Äì mapping images to the label for learning discriminative\nFlorence: A New Foundation Model for Computer Vision\nrepresentations (i.e. , supervised learning) and assigning\neach description with a unique label for language-image\npre-training (i.e. , contrastive learning).\nOur empirical experiments indicate that long language de-\nscriptions with rich content would be more beneÔ¨Åcial for\nimage-text representation learning than short descriptions\n(e.g. , one or two words). We have to enrich the short de-\nscription by generating prompt templates such as ‚ÄúA photo\nof the [WORD]‚Äù, ‚ÄúA cropped photo of [WORD]‚Äù, as data\naugmentation. During training, we randomly select one\ntemplate to generate t for each short language description.\nFollowing UniCL (Yang et al., 2022), we denote fŒ∏ and\nfœÜ as the image encoder and text encoder, respectively. u\nand v are the normalized visual feature vector and language\nfeature vector, respectively, where u = fŒ∏(x)\n‚à•fŒ∏(x)‚à•, and v =\nfœÜ(t)\n‚à•fœÜ(t)‚à•. œÑ is a learnable temperature. Given a mini-batch\nB, we use a bi-directional supervised contrastive learning\nobjective between images and language descriptions to train\nthe model as:\nL= Li2t + Lt2i. (1)\nThis objective contains two contrastive terms: the super-\nvised image-to-language contrastive loss\nLi2t = ‚àí\n‚àë\ni‚ààB\n1\n|P(i)|\n‚àë\nk‚ààP(i)\nlog exp(œÑuivk)‚àë\nj‚ààB exp(œÑuivj), (2)\nwhere k‚ààP(i) ={k|k‚ààB,yk = yi}, and the supervised\nlanguage-to-image contrastive loss\nLt2i = ‚àí\n‚àë\nj‚ààB\n1\n|Q(j)|\n‚àë\nk‚ààQ(j)\nlog exp(œÑukvj)‚àë\ni‚ààB exp(œÑuivj), (3)\nwhere k‚ààQ(j) ={k|k‚ààB,yk = yj}.\nThe generated language prompt is not a precise description\nof an image, typically not as informative as the associated\ntext descriptions from the Internet. Although including\ngenerated language prompt might not affect classiÔ¨Åcation\naccuracy, it hurts the performance in retrieval and vision-\nlanguage tasks. To mitigate the negative effect from aug-\nmented prompts, our training is separated into two stages.\nIn the Ô¨Årst stage, we use all data including augmented texts\nfor training; while in the second stage, we exclude all aug-\nmented data for continuing training. We trained 1M it-\nerations in the Ô¨Årst stage, and continuously trained 180K\niterations in the second stage. The Adam optimizer with\ndecoupled weight decay regularization is utilized for model\ntraining. The image size is 224 √ó224 and the maximum\nlanguage description length is truncated at 76. The batch\nsize is 24,576. We further trained 80Kiterations at a higher\nresolution of 384 √ó384 to boost the performance, which\nfollows existing pre-training approaches.\n2.3. Transformer-based Florence Pretrained Models\nOur Florence pretrained model uses a two-tower architec-\nture: a 12-layer transformer (Vaswani et al., 2017) as lan-\nguage encoder, similar to CLIP (Radford et al., 2021), and a\nhierarchical Vision Transformer as the image encoder. The\nhierarchical Vision Transformer is a modiÔ¨Åed Swin Trans-\nformer (Liu et al., 2021a) with convolutional embedding,\ncalled CoSwin Transformer. SpeciÔ¨Åcally, we replace the\npatch embedding and patch merging modules in the Swin\nTransformer (Liu et al., 2021a) with the convolutional em-\nbedding layers as described in CvT (Wu et al., 2021). We\nuse the CoSwin Transformer with global average pooling\nto extract image features. Two linear projection layers are\nadded on top of the image encoder and language encoder to\nmatch the dimensions of image and language features. Our\nFlorence pretrained model has in total 893M parameters,\nincluding the language transformer with 256M parameters\nand the CoSwin-H transformer with 637M parameters. The\nmodel takes 10 days to train on 512 NVIDIA-A100 GPUs\nwith 40GB memory per GPU.\n2.4. Object-level Visual Representation Learning\nWe extend the Florence pretrained model to learn Ô¨Åne-\ngrained ( i.e. , object-level) representation, which is fun-\ndamental to dense prediction tasks such as object detection.\nFor this goal, we add an adaptor Dynamic Head (Dai et al.,\n2021a) (or Dynamic DETR (Dai et al., 2021b)), a uniÔ¨Åed at-\ntention mechanism for the detection head, to the pretrained\nimage encoder ( i.e. , CoSwin). We can continue visual\nrepresentation learning from coarse (scene) to Ô¨Åne (object).\nBased on the hierarchical structure of the image encoder\nCoSwin-H, we can get the output feature pyramids from\nthe different scale levels. The feature pyramid scale lev-\nels can be concatenated and scaled-down or scaled-up into\na 3-dimensional tensor with dimensions level √óspace√ó\nchannel. The key idea of Dynamic Head (Dai et al., 2021a)\nis to deploy three attention mechanisms, each on one of the\northogonal dimensions of the tensor,i.e. , level-wise, spatial-\nwise, and channel-wise. Compared with building a single\nself-attention mechanism over this tensor, Dynamic Head\nmakes the computation more affordable and enables more\nefÔ¨Åcient learning. The above three attention mechanisms\nare applied sequentially, and we can effectively stack multi-\nple blocks consisting of such three attention layers together.\nFigure 3 shows the Dynamic Head building blocks. In this\nwork, Dynamic Head is trained with the one-stage ATSS\nframework and losses.\nWe have constructed a large-scale object detection dataset,\ncalled FLOD-9M (for FLorence Object detection Dataset),\nfor object detection pre-training. We merge several well-\nknown object detection datasets, including COCO (Lin et al.,\n2015), LVIS (Gupta et al., 2019), OpenImages (Krasin\nFlorence: A New Foundation Model for Computer Vision\nDynamic Head\nFeature Pyramid\nSE OFFSET DELTA\n√óùëÅ\nlevel-wise\nattention\nspace-wise\nattention\nchannel-wise\nattention\nFigure 3.Dynamic Head (Dai et al., 2021a) adapter is used for\nobject-level visual representation learning.\net al., 2016), Object365 (Shao et al., 2019). In addition,\nwe generate pseudo bounding boxes on ImageNet-22K\ndataset (Deng et al., 2009) by following (Zoph et al., 2020),\nwhich further enlarges our data. In the end, FLOD-9M\nconsists of 8,967,286 images, 25,190 object categories,\nand 33,408,237 bounding boxes including annotations and\npseudo labels. We then pre-train our Dynamic Head model\nfor 12 epochs with batch size 128, which takes 7 days on\n128 NVIDIA-A100 GPUs.\n2.5. Fine-Grained V+L Representation Learning\nWe use METER (Dou et al., 2021) adapter to expand to\nÔ¨Åne-grained vision-language representation. In the vision-\nlanguage area, e.g. visual question answering (VQA) and\nimage captioning, Ô¨Åne-grained representation (i.e. , object-\nlevel) is indispensable. Thus, the object detector has been\na de facto tool for image feature extraction, followed by a\nfusion network for prediction in many works (Anderson\net al., 2018; Li et al., 2020; Zhang et al., 2021b; Wang et al.,\n2020; Fang et al., 2021; Chen et al., 2020d). Recently, there\nis an increasing trend (Huang et al., 2021; Xue et al., 2021;\nWang et al., 2021; Kim et al., 2021; Dou et al., 2021) of\nend-to-end approaches to reduce dependency on the object\nbounding box, which instead consider grid-based feature\nrepresentations as the Ô¨Åne-grained features for V+L tasks.\nIn the Florence V+L adaptation model, we replace the im-\nage encoder of METER (Dou et al., 2021) with Florence\npretrained model CoSwin, and use a pretrained Roberta (Liu\net al., 2019) as the language encoder, shown in Figure 4.\nThe Florence pretrained language encoder can be used for\nthis adapter as it utilizes BERT-based architecture. Then,\nthe two modalities are fused together to learn the contex-\ntual representation with a transformer network based on co-\nattention. The co-attention model (Figure 4) allows feeding\nthe text and visual features to two Mco-layer transformers\nseparately, and each top transformer encoding layer consists\nof one self-attention block, one cross-attention block, and\none feed-forward network block. We Ô¨Årst train the model\nLanguage \nEncoder \n(RoBERTa)\nVisual Encoder \n(Florence-\nCoSwin)\nSelf-\nattention\nSelf-\nattention\nCross-\nattention\nCross-\nattention\nFeed-\nforward\nFeed-\nforward\nùëÑùëô\nùëâùëô\nùêæùëô\nùëÑùë£\nùêæùë£\nùëâùë£\nùëÑùëô\nùëâùëô\nùêæùëô\nùëÑùë£\nùëâùë£\nùêæùë£\nCoattention \n√óùëÄùëêùëú\nFigure 4.METER (Dou et al., 2021) is used as Florence V+L\nadaptation model, trained with the image-text matching (ITM) loss\nand the masked language modeling (MLM) loss.\nwith the image-text matching loss and the masked-language\nmodeling loss. Then, we Ô¨Åne-tune the model on the down-\nstream task, such as VQA (Goyal et al., 2017) task.\n2.6. Adaption to Video Recognition\nThe self-attention based design in Transformer makes it\npossible to unify the systems of image and video recognition.\nOur Video CoSwin adapter can borrow the image encoder\nfrom CoSwin for the video domain with minimum changes,\nsimilar to prior work (Liu et al., 2021b). First, the image\ntokenization layer is replaced with a video tokenization layer.\nAccordingly, video CoSwin replaces the tokenization layer\nof CoSwin (in Section 2.3) from 2D convolutional layers to\n3D convolutional layers, which converts each 3D tube into\none token. As the initialization to 3D convolutional weights,\nthe pre-trained 2D convolutional weights of CoSwin are\nduplicated along the temporal dimension and divided by\nthe temporal kernel size to keep the mean and variance\nof the output unchanged. Second, video CoSwin uses the\n3D convolution-based patch merging operator instead of\nthe 2D patch merging operator used in (Liu et al., 2021b).\nSuch overlapped token merging can enhance spatial and\ntemporal interactions among tokens. Third, we follow prior\nwork (Liu et al., 2021b) to replace the 2D shifted window\ndesign with 3D shifted local windows in self-attention layers.\nWe duplicate the 2D relative positional embedding matrix\nfrom the pre-trained CoSwin along the temporal dimension\nto initialize the 3D positional embedding matrix. In this\nway, the 2D relative positional embedding is the same for\neach temporal shift. In addition, all other layers and weights\n(including self-attention, FFN) can be inherited directly\nfrom the pre-trained CoSwin. To mitigate memory issues\nin the video training, we adopt the dynamic window size\nstrategy, i.e., a relatively small window size in early stages\nof CoSwin, and large window sizes in its later stages.\nFlorence: A New Foundation Model for Computer Vision\nFood101\nCIFAR10\nCIFAR100\nSUN397\nStanford Cars\nFGVC Aircraft\nVOC2007\nDTD\nOxford Pets\nCaltech101\nFlowers102\nImageNet\nCLIP-ResNet-50x64 91.8 86.8 61.3 48.9 76.0 35.6 83.8 53.4 93.4 90.6 77.3 73.6\nCLIP-ViT-L/14 (@336pix) 93.8 95.7 77.5 68.4 78.8 37.2 84.3 55.7 93.5 92.8 78.3 76.2\nFLIP-ViT-L/14 92.2 95.7 75.3 73.1 70.8 60.2 - 60.7 92.0 93.0 90.1 78.3\nFlorence-CoSwin-H (@384pix) 95.1 94.6 77.6 77.0 93.2 55.5 85.5 66.4 95.9 94.7 86.2 83.7\nTable 1.Zero-shot transfer of image classiÔ¨Åcation comparisons on 12 datasets: CLIP-ResNet-50x64 (Radford et al., 2021), FLIP-ViT-\nL/14 (Yao et al., 2021).\n0\n5\n10\n15\n20\n25\n30\n35\n8 16 32 64\nMemory Per GPU (G)\nBatch Size Per GPU\nTorch-8A100 Torch-32A100 Torch-64A100\nFlorence-8A100 Florence-32A100 Florence-64A100\nFigure 5.GPU memory reduction for various batch sizes. We\ncompared the proÔ¨Åling between Torch (w/o optimization) and\nFlorence (w/ optimization) on various number of GPUs.\n2.7. Scalable Training Infrastructure\nTo train the Florence model on our large-scale dataset, our\nscalable training infrastructure faces two main challenges:\nreducing memory cost on each GPU and increasing the\nthroughput. Reducing the memory cost allows us to feed\nmore data into each GPU and use a larger batch size, which\nhas been proved to be effective for contrastive learning.\nIncreasing the throughput can signiÔ¨Åcantly speed up the\nwhole training process and thus reduce carbon emissions.\nWe have developed several techniques that can be combined\nto achieve the two goals:\nZero Redundancy Optimizer (ZeRO) The ZeRO tech-\nnique (Rajbhandari et al., 2019) partitions the optimizer\nstates, gradients and parameters across the GPUs and\neach partition is only updated locally. Thus, the mem-\nory consumption is largely reduced.\nActivation Checkpointing For a checkpointed model\ncomponent, e.g. , multi-head attention, it reruns a for-\nward pass during backward pass. In this way, the in-\nternal gradients in the component do not need to be\nstored in the forward pass and then reduce the memory\ncost in the training.\nMixed-precision Training In mixed-precision training,\nvarious operations are trained with different numer-\nical precision (i.e. , Ô¨Çoat-32 or Ô¨Çoat-16). Float-32 is\nused for numerically less stable operations, such as\nlayer normalization; while Ô¨Çoat-16 is used for the other\noperations. Such a combination improves the training\nthroughput and maintains the model performance.\nGradient Cache The gradient cache technique (Gao et al.,\n2021) is able to increase the total batch size in a train-\ning step. A large batch size is shown to be beneÔ¨Åcial\nto learn better representations in previous works. How-\never, it is bounded by available GPU memory. To\nresolve this problem, we factor the contrastive loss by\nbreaking the large batch gradient update into several\nsub-updates that can Ô¨Åt into GPU memory. It enables\nus to train big models with a large batch size.\nThanks to these above optimizations, we can achieve con-\nsistent improvement in reducing GPU memory for variable\nbatch sizes on various numbers of NVIDIA-A100s, shown\nin Figure 5.\n3. Experiments\n3.1. Zero-shot Transfer in ClassiÔ¨Åcation\nIn computer vision, zero-shot learning usually refers to the\nstudy of predicting classes that are deÔ¨Åned via descriptive\ntext. As a vision foundation model, Florence can be directly\nused to predict if an image and a text snippet are semanti-\ncally matched together in the task dataset. We follow the\nsame method of CLIP (Radford et al., 2021) to perform\nFlorence: A New Foundation Model for Computer Vision\nFood101\nCIFAR10\nCIFAR100\nSUN397\nStanford Cars\nFGVC Aircraft\nVOC2007\nDTD\nOxford Pets\nCaltech101\nFlowers102\nSimCLRv2-ResNet-152x3 83.6 96.8 84.5 69.1 68.5 63.1 86.7 80.5 92.6 94.9 96.3\nViT-L/16 (@384pix) 87.4 97.9 89.0 74.9 62.5 52.2 86.1 75.0 92.9 94.7 99.3\nEfÔ¨ÅcientNet-L2 (@800pix) 92.0 98.7 89.0 75.7 75.5 68.4 89.4 82.5 95.6 94.7 97.9\nCLIP-ResNet-50x64 94.8 94.1 78.6 81.1 90.5 67.7 88.9 82.0 94.5 95.4 98.9\nCLIP-ViT-L/14 (@336pix) 95.9 97.9 87.4 82.2 91.5 71.6 89.9 83.0 95.1 96.0 99.2\nFlorence-CoSwin-H (@384pix) 96.2 97.6 87.1 84.2 95.7 83.9 90.5 86.0 96.4 96.6 99.7\nTable 2.Comparisons of image classiÔ¨Åcation linear probing on 11 datasets with existing state-of-the-art models, including Sim-\nCLRv2 (Chen et al., 2020c), ViT (Dosovitskiy et al., 2021a), EfÔ¨ÅcientNet (Xie et al., 2020), and CLIP (Radford et al., 2021).\nzero-shot classiÔ¨Åcation. For each dataset, we use the names\nof all the classes in the dataset as the set of potential text\npairings and predict the most probable (image, text) pair\naccording to Florence. We compute the feature embedding\nof the image for CoSwin and the feature embedding of the\nset of possible texts by the language encoder. The cosine\nsimilarities among these embeddings are then calculated,\nand then we rank the similarity scores over all the classes\nto select the Top-1 or Top-5 classes as the predicted classes.\nHere, we do not need to compute the normalized cosine\nsimilarity as done in (Radford et al., 2021), since it won‚Äôt\naffect the ranking order of Ô¨Ånal results.\nWe evaluate our Florence model on the ImageNet-1K\ndataset and 11 downstream datasets from the well-studied\nevaluation suit introduced by (Kornblith et al., 2019). Note\nthat our benchmarks exclude the Birdsnap (Berg et al., 2014)\ndataset from 12 original classiÔ¨Åcation datasets introduced\nin (Kornblith et al., 2019), because 20% of the image URLs\nprovided by the authors are invalid. We follow the same\nprompt templates and engineering, and ensembling as pre-\nviously proposed in (Radford et al., 2021) for evaluating\nzero-shot performance. For all zero-shot tasks in this pa-\nper, we follow the setup in CLIP (Radford et al., 2021) and\nALIGN (Jia et al., 2021) to remove near-duplicate test im-\nages from our training data. Table 1 shows the results over\nthese 12 datasets, in comparison with the best performance\nachieved by both CLIP ResNet and Vision Transformer\nmodels, and the concurrent work FILIP (Yao et al., 2021).\nFlorence outperforms on 9/12 tasks compared with state-\nof-the-art methods. We achieved a remarkable improvement\nin the zero-shot transfer on ImageNet-1K ‚Äì the top-1 accu-\nracy of 83.74% (+5.6% over SOTA result), and the top-5\naccuracy of 97.18%.\n3.2. Linear Probe in ClassiÔ¨Åcation\nLinear probe as another main metric for evaluating represen-\ntation quality has been used in most recent studies, including\nself-supervised learning (Chen et al., 2020b;c), self-training\nwith noisy student (Xie et al., 2020) and contrastive learn-\ning (Radford et al., 2021). We follow the same setting and\nimplementation of CLIP (Radford et al., 2021) for linear\nevaluation, where the image encoder (or vision backbone) is\nfrozen, and only the appended linear layers can be Ô¨Åne-tuned\non the downstream datasets. We use public available models\n(shown in Table 10 (Radford et al., 2021)) to verify the cor-\nrectness of our own implementation. The variance between\nour reproduced results and their reported results is ¬±0.1 for\neach task. Our linear evaluation considers 11 classiÔ¨Åcation\nbenchmarks which are also used for our zero-shot transfer\nof classiÔ¨Åcation. We compared our results with state-of-the-\nart methods with their best performance models, including\nSimCLRv2 (Chen et al., 2020c), ViT (Dosovitskiy et al.,\n2021a), Noisy Student (Xie et al., 2020) and CLIP (Radford\net al., 2021) on Table 2. Our results are consistently better\nthan existing state-of-the-art results, expect for two datasets:\nCIFAR10, CIFAR100. On the two datasets, the input im-\nage resolution is quite low ( i.e. , 32 √ó32). Training with\nhigher resolution deÔ¨Ånitely boosts the performance,such\nas EfÔ¨Åcient-L2 (Xie et al., 2020) which achieves the best\naccuracy compared with all other approaches trained on\nlower-resolution images.\n3.3. ImageNet-1K Fine-tune Evaluation\nFlorence can be easily adapted to support continual Ô¨Åne-\ntuning on target classiÔ¨Åcation tasks. We do not change or\nadd anything into our architecture, but continue the training\non task-speciÔ¨Åc data using the same pre-training loss (shown\nin Equation 1). We feed the class name to the text encoder\nof Florence to get the text feature embedding. We use the\nFlorence: A New Foundation Model for Computer Vision\nModel Params Data Accuracy\nTop-1 Top-5\nBiT-L-ResNet152x4 928M 300M 87.54 98.46\nALIGN-EfÔ¨Åcient-L2 480M 1800M 88.64 98.67\nViT-G/14 1843M 3000M 90.45 -\nCoAtNet-7 2440M 3000M 90.88 -\nFlorence-CoSwin-H 637M 900M 90.05 99.02\nTable 3.ClassiÔ¨Åcation Ô¨Åne tuning on ImageNet-1K. Florence is\ncompared with: BiT-L-ResNet152x4 (Kolesnikov et al., 2020),\nALIGN-EfÔ¨Åcient-L2 (Jia et al., 2021), ViT-G/14 (Zhai et al., 2021),\nCoAtNet-7 (Dai et al., 2021c) in terms of model scale, data scale\nand Top-1/Top-5 accuracy.\nsame prompt templates as in (Radford et al., 2021; Jia et al.,\n2021) to expand the descriptions of ImageNet (Deng et al.,\n2009) class names.\nWe evaluate the performance of continual Ô¨Åne-tuning on\nImageNet ILSVRC-2012 benchmark (Deng et al., 2009).\nOur image encoder CoSwin-H is Ô¨Åne-tuned at the resolution\nof 512 √ó512 with a batch size of 8,192 for 10 epochs. We\nuse a cosine learning rate decay scheduler with 500 warmup\nsteps and a peak learning rate of 0.00002. The comparisons\nwith state-of-the-art results are shown in Table 3. Our model\noutperforms BiT (Kolesnikov et al., 2020) with larger model\nsize and ALIGN (Jia et al., 2021) trained from more data in\nterms of Top-1 and Top-5 accuracy. Our result is slightly\nworse than SOTA (Dai et al., 2021c), but their model and\ndata scale are both 3√ólarger.\n3.4. Few-shot Cross-domain ClassiÔ¨Åcation\nThe Cross-Domain Few-Shot learning benchmark (Guo\net al., 2020) is used to measure an algorithm‚Äôs capability\nto adapt to downstream few-shot target tasks, containing\ndomains with varying levels of dissimilarity to typical con-\nsumer photographs. The datasets in the benchmark include:\nCropDisease (Mohanty et al., 2016) (plant leaf images, 38\ndisease states over 14 plant species), EuroSAT (Helber\net al., 2019) (RGB satellite images, 10 categories), ISIC\n2018 (Codella et al., 2019; Tschandl et al., 2018) (der-\nmoscopic images of skin lesions, 7 disease states), and\nChestX (Wang et al., 2017) (Chest X-rays, 16 conditions).\nExemplar image for each dataset is shown on the top of Ta-\nble 4. The evaluation protocol involves 5-way classiÔ¨Åcation\nacross 5-shot, 20-shot, and 50-shot. The classes and shots\nare randomly sampled for each episode, for 600 episodes\nper way and shot. Average accuracy over all episodes is\nreported.\nTo predict the class, we append a single linear layer as an\nadapter head to our image encoder CoSwin. Training occurs\nISIC EuroSAT CropDisease ChestX\nModel ISIC EuroSAT CropD ChestX mean\n5-shot CW 57.4 88.1 96.6 29.7 68.0\nFlorence 57.1 90.0 97.7 29.3 68.5\n20-shot CW 68.1 94.7 99.2 38.3 75.1\nFlorence 72.9 95.8 99.3 37.5 76.4\n50-shot CW 74.1 96.9 99.7 44.4 78.8\nFlorence 78.3 97.1 99.6 42.8 79.5\nTable 4.Comparison with CW (Liu et al., 2020) (CD-FSL Chal-\nlenge 2020 Winner) on CD-FSL benchmark. The average result\ncomparison is 74.8 (Florence) vs. 73.9 (CW).\nover 100 epochs per episode. We use SGD with momentum,\nwith learning rate and momentum values of 0.9/0.0002,\nrespectively, for CoSwin, and 0.99/0.01, respectively, for\nthe adapter head. Horizontal data Ô¨Çip augmentation is used\nfor training and test, and dropout of 0.5 is used between the\nimage encoder and the classiÔ¨Åer head.\nTable 4 shows the results of adapting our model to the CD-\nFSL benchmark, in comparison to the winner of the chal-\nlenge benchmark (Liu et al., 2020), which employs ensem-\nbes and transductive learning. By comparison, we employ\na single model and no transduction on the test data is per-\nformed, yet we achieve higher results without any ‚Äúbells\nand whistles‚Äù.\n3.5. Image-Text Retrieval\nTable 5 presents the zero-shot transfer and Ô¨Åne-tuning per-\nformance of Florence for both text and image retrieval on\nthe Flickr30k (Plummer et al., 2016) and MSCOCO (Lin\net al., 2015) datasets.\nFor zero-shot retrieval, we feed the input text (or image)\nto the language (or image) encoder of Florence to get the\nfeature embeddings, and also compute the feature embed-\ndings of the set of possible images (or texts) by the image\n(or language) encoder. Then we compute cosine similarity\nof these embeddings and rank the similarity scores over the\ntesting set to select the Top-1 or Top-5 results. Zero-shot\nFlorence matches or outperforms all prior zero-shot results\non these two datasets.\nFor Ô¨Åne-tuning retrieval, we continuously train our language\nand text encoders on the target image-text pair data, as well\nFlorence: A New Foundation Model for Computer Vision\nFlickr30K (1K test set) MSCOCO (5K test set)\nMethod Image ‚ÜíText Text ‚ÜíImage Image ‚ÜíText Text ‚ÜíImage\nR@1 R@5 R@1 R@5 R@1 R@5 R@1 R@5\nZero-shot\nImageBERT (Qi et al., 2020) 70.7 90.2 54.3 79.6 44.0 71.2 32.3 59.0\nUNITER (Chen et al., 2020d) 83.6 95.7 68.7 89.2 - - - -\nCLIP (Radford et al., 2021) 88.0 98.7 68.7 90.6 58.4 81.5 37.8 62.4\nALIGN (Jia et al., 2021) 88.6 98.7 75.7 93.8 58.6 83.0 45.6 69.8\nFLIP (Yao et al., 2021) 89.8 99.2 75.0 93.4 61.3 84.3 45.9 70.6\nFlorence 90.9 99.1 76.7 93.6 64.7 85.9 47.2 71.4\nFine-tuned\nGPO (Chen et al., 2020a) 88.7 98.9 76.1 94.5 68.1 90.2 52.7 80.2\nUNITER (Chen et al., 2020d) 87.3 98.0 75.6 94.1 65.7 88.6 52.9 79.9\nERNIE-ViL (Yu et al., 2020) 88.1 98.0 76.7 93.6 - - - -\nVILLA (Gan et al., 2020) 87.9 97.5 76.3 94.2 - - - -\nOscar (Li et al., 2020) - - - - 73.5 92.2 57.5 82.8\nALIGN (Jia et al., 2021) 95.3 99.8 84.9 97.4 77.0 93.5 59.9 83.3\nFLIP (Yao et al., 2021) 96.6 100.0 87.1 97.7 78.9 94.4 61.2 84.3\nFlorence 97.2 99.9 87.9 98.1 81.8 95.2 63.2 85.7\nTable 5.Image-text retrieval comparisons on Flickr30K and MSCOCO datasets (zero-shot and Ô¨Åne-tuned).\nas classiÔ¨Åcation Ô¨Åne-tuning (shown in Section 3.3). We Ô¨Åne-\ntune our model with a batch size of 3,072 for 12 epochs.\nWe use the cosine learning rate decay scheduler with 200\nwarmup steps and a peak learning rate of 0.00002. Our\nresults are superior to all previous Ô¨Åne-tuning results on the\ntwo datasets. Moreover, our Ô¨Åne tuning on retrieval is more\nefÔ¨Åcient, with only roughly 6% and 8% Ô¨Åne-tuning epochs\nof ALIGN (Jia et al., 2021) on Flickr30k and MSCOCO\nrespectively.\n3.6. Object Detection and Zero-shot Transfer\nObject detection is one of the most prominent applications\nin computer vision. Compared with existing large-scale\npre-trained models ( e.g. , CLIP (Radford et al., 2021),\nALIGN (Jia et al., 2021), Wu Dao 2.0 (Wud)), Florence\nis more desirable for object detection since its adaptation\nhelps learn visual representation at the object level. We eval-\nuate its performance of object-level visual representations\nvia Ô¨Åne-tuned object detection and zero-shot transfer tasks.\nFine-tuning We evaluate Ô¨Åne-tuning on three popular\nobject detection datasets: COCO (Lin et al., 2015), Ob-\nject365 (Shao et al., 2019), and Visual Genome (Krishna\net al., 2016). For COCO, we increase the maximum image\nside to 2,500 and Ô¨Åne-tune with multi-scale training for 12\nepochs. We follow the same multi-scale testing strategy\nwidely used in existing state-of-the-art approaches. For Ob-\nject365, we use the same input resolution of images ( i.e. ,\nthe maximum image side 1,333) as the Multi-dataset Detec-\ntion1 (Zhou et al., 2021) for Ô¨Åne-tuning. For Visual Genome,\nwe increase the maximum side of input resolution to 3,000\nand Ô¨Åne-tune with multi-scale training for 24 epochs. To\nleverage attributes annotations in Visual Genome, we in-\nsert an 1 √ó1 ROI pool on the Ô¨Ånal stage of CoSwin back-\nbone to extract features for attribute learning, which allows\nthe object detection adapter being optimized for multi-task\nlearning.\nWe compare Florence with state-of-the-art results on these\nthree benchmarks in Table 6. In object detection, the stan-\ndard mean average precision (AP) metric is used to report\nresults under different IoU thresholds and object scales for\nall datasets. We follow the metrics used in existing state-\nof-the-art methods. For COCO, Object365 and zero-shot\ntransfer benchmarks, we use mAP,i.e. , average over mul-\ntiple IoUs ( 0.5 : 0.05 : 0.95). For Visual Genome, we\nuse AP50 at IoU threshold 0.5. As we can see, Florence\nestablishes new results in these main benchmarks of object\ndetection.\nZero-shot Transfer Zero-shot object detection is more\nchallenging than zero-shot classiÔ¨Åcation, since neither ob-\nject proposal classiÔ¨Åcation nor location (i.e. , bounding box\nregression) in downstream tasks is seen during training. In\nour zero-shot transfer setting, object proposal and object\nclassiÔ¨Åcation are decoupled into two tasks. Object proposal\ndiscriminates object from background, ignoring semantics\n1This work was ranked 1-st in the object detection track of\nECCV 2020 Robust Vision Challenge.\nFlorence: A New Foundation Model for Computer Vision\nCOCO Object365 Visual Genome Aquarium BCCD Chess Pieces Mask WearingOxford Pets Packages Pistols PKLot Pothole Thermal Wildfire Smoke\nFigure 6.Our Ô¨Åne-tuned detection results on COCO (sparse object boxes), Object365 (dense object boxes), Visual Genome (w/ object\nattributes), and zero-shot transfer results on 11 downstream detection tasks. Boxes with different colors denote different object categories.\nBenchmark Model AP\nCOCO miniVal\nDyHead 60.3\nSoft Teacher 60.7\nFlorence 62.0\nCOCO test-Dev\nDyHead 60.6\nSoft Teacher 61.3\nFlorence 62.4\nObject365 Multi-dataset Detection 33.7\nFlorence 39.3\nVisual Genome VinVL 13.8\nFlorence 16.2\nTable 6.Object detection Ô¨Åne tuning comparisons with state-of-\nthe-art methods, including DyHead (Dai et al., 2021a), Soft\nTeacher (Xu et al., 2021b), Multi-dataset Detection (Zhou et al.,\n2021), VinVL (Zhang et al., 2021b).\nof object categories. ClassiÔ¨Åcation, on the other hand, fo-\ncuses on object semantics for each bounding box proposal.\nIn spirit, this setup is similar to the behavior of R-CNN\nmodel (Girshick et al., 2014) which has been widely used\nfor object detection before. Using this approach, we can fol-\nlow existing work on zero-shot image classiÔ¨Åcation to zero-\nshot transfer in object detection, to evaluate the Florence\nfor novel object recognition. As mentioned in ZSD (Bansal\net al., 2018), it more approaches real world settings.\nFor zero-shot transfer, the training of the detection adapter\ncan be different from Ô¨Åne-tuning. SpeciÔ¨Åcally, we freeze\nthe CoSwin backbones and pre-train the Dynamic Head\non FLOD-9M by neglecting semantics from each object\nbounding box. We treat the object detection pre-training\nas general-purpose object proposal training. Note that the\ndetection pre-training only updates the object adapter, and\ndoes not affect the fused feature representations learned\nfrom large-scale image-text pairs. In inference, we apply\nthe pre-trained CoSwin and Dynamic Head on downstream\ndatasets, and obtain the object proposals for every image.\nFor each object proposal, we apply zero-shot classiÔ¨Åcation,\nas described in Section 3.1.\nTo evaluateFlorence‚Äôs transferability to novel, diverse and\napplication-oriented tasks, following (Li et al., 2021b), we\ncurate an ‚Äúopen-set oject detection benchmark‚Äù which aggre-\ngates 11 public datasets from RoboÔ¨Çow2, spanning scenarios\nincluding Ô¨Åne-grained Ô¨Åshes/chess detection, drone-view\ndetection, and thermal object detection. We use their split\ntest datasets for evaluation. Table 7 shows that our Flo-\nrence model effectively zero-shot transfers to these tasks.\nWe use the results of the baseline approach ZSD (Bansal\net al., 2018), which considers a similar setting, for reference.\nIn our implementation3, we replace their supervised object\ndetector FasterRCNN with the recent SOTA detector (Dai\net al., 2021a) and use pre-trained BERT as the language\nencoder. Both are pre-trained end-to-end on the Objects365\ndataset. Thanks to large-scale image-text pretraining, Flo-\nrence shows remarkable gains on all tasks. Zero-shot in ob-\nject detection still has a long way to be applied to real-world\ntasks. We further compare Florence zero-shot with previ-\nous state-of-the-art detector4 (Dai et al., 2021a) (on COCO)\nÔ¨Åne-tunning on these tasks. We can observe noticeable\nperformance gap between zero-shot and supervised learn-\ning, especially for novel scenarios whose concepts/classes\nmay not be covered by the pre-training dataset, such as\n‚ÄúBCCD‚Äù (blood cells photos), ‚ÄúChess Pieces‚Äù (Chess board\nphotos and various pieces). However, the results are en-\ncouraging when compared with few-shot Ô¨Åne-tuning results.\nFlorence outperforms in 7/11 tasks over 5-shot Ô¨Åne tun-\ning, and outperforms full-set Ô¨Åne-tuning on the ‚ÄúPackages‚Äù\n2https://public.roboÔ¨Çow.com/object-detection\n3We refer to (Li et al., 2021b) for details.\n4It is pre-trained on ImageNet and COCO in supervised way.\nFlorence: A New Foundation Model for Computer Vision\nAquarium\nBCCD\nChess Pieces\nMask Wearing\nOxford Pets\nPackages\nPistols\nPKLot\nPothole\nThermal\nWildÔ¨Åre Smoke\nImages 638 364 292 149 3680 26 2986 12416 665 203 737\nCategories 7 3 12 2 37 1 1 2 1 2 1\nFine-tuned DyHead-Swin-L (full) 53.1 62.6 80.7 52.0 85.9 52.0 74.4 98.0 61.8 75.9 58.7\nDyHead-Swin-L (5-shot) 39.0 40.6 57.3 26.8 47.5 32.8 20.0 22.1 10.8 54.9 14.2\nZero-shot ZSD 16.0 1.2 0.1 0.6 0.3 58.3 31.5 0.2 2.4 37.4 0.002\nFlorence 43.1 15.3 13.4 15.0 68.9 79.6 41.4 31.4 53.3 46.9 48.7\nTable 7.Zero-shot transfer in object detection, in comparison with previous state-of-the-art model DyHead (Dai et al., 2021a) (on COCO)\nÔ¨Åne tuning results on full-set or 5-shot respectively and zero-shot detection baseline model ZSD (Bansal et al., 2018).\ndataset, consisting of only 26 images for training. It demon-\nstrates the foundation models‚Äô great potential of improving\ndata efÔ¨Åciency and reducing deployment cost for new tasks\nor domains.\n3.7. V+L Representation Learning\nThe vision-langauge pretraining (VLP) is performed\non MSCOCO (Lin et al., 2015), Conceptual Captions\n(CC) (Sharma et al., 2018), CC12M (Changpinyo et al.,\n2021), SBU (Ordonez et al., 2011), and Visual Genome\n(VG) (Krishna et al., 2016). These datasets result in 14\nmillion images with 20 million associated captions. Beyond\nreplacing the image encoder withCoSwin-H of our Florence\nmodel on (Dou et al., 2021), we remove the weight decay on\nthe text embedding layer and the modality-speciÔ¨Åc embed-\nding. ITM and MLM are applied for VLP with 43 epochs\nwith the image input size as 384.\nTo evaluate the performance, we Ô¨Åne-tune the pre-trained\nmodel on the challenging VQA (Goyal et al., 2017) task,\nwhich is to answer a question based on the image context.\nThe dataset consists of 82K training images and 41K valida-\ntion images. Only 1K validation images are reserved and the\nrest are merged with the training data for Ô¨Åne-tuning. As a\ncommon practice, the problem is cast as a classiÔ¨Åcation task\nwhere each class corresponds to an answer. The Ô¨Ånal pool-\ning representations are fed into a randomly-initialized multi-\nlayer perceptron (MLP) network to predict the answer over\n3,129 answers. The loss is the binary cross-entropy loss,\nand the inference is to select the answer with the highest\nconÔ¨Ådence. The model is Ô¨Åne-tuned for 10 epochs with the\nlearning rate as 8e‚àí6 and is evaluated on the test-dev\nand test-std. The Ô¨Ånal accuracy is calculated on the\npublic server 5.\n5http://evalai.com\nModel test-dev test-std\nUNITER (Chen et al., 2020d) 73.82 74.02\nVisual Parsing (Xue et al., 2021) 74.00 74.17\nPixelBERT (Huang et al., 2020) 74.45 74.55\nVILLA (Gan et al., 2020) 74.69 74.87\nUNIMO (Li et al., 2021c) 75.06 75.27\nALBEF (Li et al., 2021a) 75.84 76.04\nVinVL (Zhang et al., 2021b) 76.52 76.60\nCLIP-ViL (Shen et al., 2021) 76.48 76.70\nMETER (Dou et al., 2021) 77.68 77.64\nSimVLM (Wang et al., 2021) 80.03 80.34\nFlorence 80.16 80.36\nTable 8.Compare our model with the existing state-of-the-art meth-\nods on VQA.\nFigure 8 shows the comparison results with the existing\nmethods. As we can see, we achieve the new state-of-the-art\nperformance. Compared with SimVLM (Wang et al., 2021),\nwhich uses 1.8B image-text pairs, we only use900M data to\npre-train the image encoder and 20M for VLP, but achieve\nbetter results. This also demonstrates the data efÔ¨Åciency of\nour approach.\n3.8. Zero-Shot Text-to-Video Retrieval\nAlthough Florence is pre-trained on image-text pairs, it can\nbe easily adapted to video tasks (shown in Section 2.6),\nsuch as text-video retrieval. We expand the input 2D patch\nembeddings and positional embeddings to 3D so that the\nencoder can process video inputs, following (Arnab et al.,\n2021). Then, we perform zero-shot text-to-video evaluation\nFlorence: A New Foundation Model for Computer Vision\nMethod Pre-training Type Pre-training Data R@1 R@5 R@10\nMIL-NCE (Miech et al., 2020) Video HowTo100M - - 32.4\nMMV (Alayrac et al., 2020) Video HowTo100M, AudioSet - - 31.1\nVideoCLIP (Xu et al., 2021a) Video‚àó HowTo100M 10.4 22.2 30.0\nV ATT (Akbari et al., 2021) Video HowTo100M, AudioSet - - 29.7\nMCN (Chen et al., 2021) Image and Video HowTo100M - - 33.8\nFrozen-in-Time (Bain et al., 2021) Image and Video ImageNet, CC, WebVid-2M 18.7 39.5 51.6\nCLIP-ViT-B/16 (Radford et al., 2021) Image WIT400M 26.0 49.4 60.7\nFlorence Image FLD-900M 37.6 63.8 72.6\nTable 9.Zero-shot text-to-video retrieval results on MSR-VTT 1K-A test set. (‚àó: Feature extracted from the pre-trained model (Miech\net al., 2020), followed by another stage of video-and-language pre-training) The pretraining data used in these existing methods\ninclude HowTo100M (Miech et al., 2019), AudioSet (Gemmeke et al., 2017), ImageNet (Deng et al., 2009), CC (Sharma et al., 2018),\nWebVid-2M (Bain et al., 2021), WIT400M (Radford et al., 2021)\nMethod Pretraining Data Kinetics-400 Kinetics-600 Views Params\nTop-1 Top-5 Top-1 Top-5\nViViT-H/16x2 JFT-300M 84.8 95.8 85.8 96.5 4 √ó3 648M\nVideoSwin-L ImageNet-22K 84.6 96.5 85.9 97.1 4 √ó3 200M\nVideoSwin-L ImageNet-22K 84.9 96.7 86.1 97.3 10 √ó5 200M\nTokenLearner 16at18+L/10 JFT-300M 85.4 96.3 86.3 97.0 4 √ó3 460M\nFlorence FLD-900M 86.5 97.3 87.8 97.8 4 √ó3 647M\nTable 10.Comparison to state-of-the-art methods, including ViViT (Arnab et al., 2021), VideoSwin (Liu et al., 2021b), TokenLearner (Ryoo\net al., 2021), on Kinetics-400 and Kinetics-600. Views indicate #temporal clip√ó#spatial crop.\non the MSR-VTT (Xu et al., 2016) dataset. We report\nresults on the 1K-A test (Yu et al., 2018), which contains 1K\nvideo and caption pairs. We use the standard recall metrics\nfor evaluation and compare with existing state-of-the-art\nmethods in Table 9. As we can see, these two image-text pre-\ntrained models CLIP6 (Radford et al., 2021) and Florence\noutperform all the state-of-the-art methods by a large margin\nin terms of the R@1 metric. It reveals that the video data\nused for pretraining in these state-of-the-art methods may\nnot be so rich or diverse as image-text data used inFlorence\nor CLIP.\n3.9. Video Action Recognition\nWe evaluateFlorence on Ô¨Åne-tuned video action recognition\ntasks. On the Kinectics-400 and Kinectics-600 datasets,\nwe follow the typical Ô¨Åne-tuning setting (Liu et al., 2021b)\nand Ô¨Åne tune the model (Section 2.6) with 384 √ó384 res-\nolution for 30 epochs. We use the label smoothing, rand\naugmentation, a small learning rate 0.0002 and a relatively\nlarge drop path rate 0.5 to avoid over-Ô¨Åtting the target video\ndatasets. We compare with existing state-of-the-art methods\n6We use a public available CLIP checkpoint for comparison\nin Table 10. Our results are better than the state-of-the-\nart by 1.1% and 1.5% on Kinectics-400 and Kinectics-600,\nrespectively.\n4. Conclusion and Future Work\nIn this paper we investigated a new paradigm of building a\ncomputer vision foundation model, Florence, as a general-\npurpose vision system. Our attempt is a step towards build-\ning XYZ-code (Huang), an integrative AI system that makes\nprogress toward human-like AI. Although the model size\nis still below several other existing billion-scale models,\nFlorence successfully extends to different tasks along space,\ntime, and modality, with great transferbility, and achieves\nnew SOTA results on a wide range of vision benchmarks.\nFor the future work, we plan to include more vision tasks\nand applications, such as depth/Ô¨Çow estimation, tracking,\nand additional vision+language tasks. Florence is designed\nto pave the way for building vision foundation models to\npower millions of real-world vision tasks and applications.\nIn addition, the preliminary progress on zero-shot classiÔ¨Å-\ncation and object detection may motivate more research to\nclose the performance gap to supervised learning.\nFlorence: A New Foundation Model for Computer Vision\nACKNOWLEDGMENT\nWe would like to thank the following people involved in the\ndiscussion for their valuable feedback including Xiaowei\nHu, Yen-Chun Chen, Lin Liang, Yinpeng Chen, Li Dong,\nFuru Wei, Han Hu, Yue Cao, Zheng Zhang, Hao Yang,\nJianmin Bao, Dong Chen, Fang Wen, Jianlong Fu, Houwen\nPeng, Chong Luo, Baining Guo. We would also thank\nQingfen Lin, Cha Zhang for their thoughtful feedback on\nthe broader impacts of the paper. Thanks Mei Gao, Ping Jin\nfor helping run evaluations on benchmark infrastructure. We\nare also grateful to the developers of software toolkits used\nthroughout this project, including Liyang Lu, Robert Gmyr,\nFelipe Cruz Salinas, Canrun Li, Steven Tsai, Min Gao,\nKevin Pan, Shohei Ono, Christina Sun. Additionally, we\nwould like to thank the entire Deepspeed, AI Frameworks,\nand ITP teams for making it possible to train models at this\nscale.\nReferences\nWu dao 2.0. https://gpt3demo.com/apps/wu-\ndao-20.\nAkbari, H., Yuan, L., Qian, R., Chuang, W.-H., Chang, S.-F.,\nCui, Y ., and Gong, B. Vatt: Transformers for multimodal\nself-supervised learning from raw video, audio and text.\nIn NeurIPS, 2021.\nAlayrac, J.-B., Recasens, A., Schneider, R., Arandjelovic,\nR., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S.,\nand Zisserman, A. Self-supervised multimodal versatile\nnetworks. In NeurIPS, volume 2, pp. 7, 2020.\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\nGould, S., and Zhang, L. Bottom-up and top-down atten-\ntion for image captioning and visual question answering.\nIn CVPR, 2018.\nArnab, A., Dehghani, M., Heigold, G., Sun, C., LuÀáci¬¥c, M.,\nand Schmid, C. Vivit: A video vision transformer. In\nICCV, 2021.\nBain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen\nin time: A joint video and image encoder for end-to-end\nretrieval. In ICCV, 2021.\nBansal, A., Sikka, K., Sharma, G., Chellappa, R., and Di-\nvakaran, A. Zero-shot object detection. In Proceedings\nof the European Conference on Computer Vision (ECCV),\npp. 384‚Äì400, 2018.\nBerg, T., Liu, J., Lee, S. W., Alexander, M. L., Jacobs, D. W.,\nand Belhumeur, P. N. Birdsnap: Large-scale Ô¨Åne-grained\nvisual categorization of birds. In 2014 IEEE Conference\non Computer Vision and Pattern Recognition, pp. 2019‚Äì\n2026, 2014.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R.,\nArora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-\nlut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card,\nD., Castellon, R., Chatterji, N., Chen, A., Creel, K.,\nDavis, J. Q., Demszky, D., Donahue, C., Doumbouya,\nM., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh,\nK., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel,\nK., Goodman, N., Grossman, S., Guha, N., Hashimoto,\nT., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu,\nK., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P.,\nKaramcheti, S., Keeling, G., Khani, F., Khattab, O., Koh,\nP. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A.,\nLadhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li,\nX. L., Li, X., Ma, T., Malik, A., Manning, C. D., Mirchan-\ndani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan,\nA., Narayanan, D., Newman, B., Nie, A., Niebles, J. C.,\nNilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadim-\nitriou, I., Park, J. S., Piech, C., Portelance, E., Potts, C.,\nRaghunathan, A., Reich, R., Ren, H., Rong, F., Roohani,\nY ., Ruiz, C., Ryan, J., R¬¥e, C., Sadigh, D., Sagawa, S., San-\nthanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori,\nR., Thomas, A. W., Tram`er, F., Wang, R. E., Wang, W.,\nWu, B., Wu, J., Wu, Y ., Xie, S. M., Yasunaga, M., You, J.,\nZaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y .,\nZheng, L., Zhou, K., and Liang, P. On the opportunities\nand risks of foundation models. In arXiv 2108.07258,\n2021.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. In arXiv 2005.14165, 2020.\nChangpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-\nceptual 12M: Pushing web-scale image-text pre-training\nto recognize long-tail visual concepts. In CVPR, 2021.\nChen, B., Rouditchenko, A., Duarte, K., Kuehne, H.,\nThomas, S., Boggust, A., Panda, R., Kingsbury, B., Feris,\nR., Harwath, D., et al. Multimodal clustering networks\nfor self-supervised learning from unlabeled videos. In\nICCV, 2021.\nChen, J., Hu, H., Wu, H., Jiang, Y ., and Wang, C. Learning\nthe best pooling strategy for visual semantic embedding.\nIn arXiv preprint arXiv:2011.04305, 2020a.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. In Proceedings of the 37th International\nConference on Machine Learning, volume 119, pp. 1597‚Äì\n1607, 13‚Äì18 Jul 2020b.\nFlorence: A New Foundation Model for Computer Vision\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and\nHinton, G. Big self-supervised models are strong semi-\nsupervised learners. arXiv preprint arXiv:2006.10029,\n2020c.\nChen, Y .-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan,\nZ., Cheng, Y ., and Liu, J. Uniter: Universal image-text\nrepresentation learning. In Proceedings of European\nConference on Computer Vision, 2020d.\nCodella, N. C. F., Rotemberg, V ., Tschandl, P., Celebi, M. E.,\nDusza, S. W., Gutman, D. A., Helba, B., Kalloo, A.,\nLiopyris, K., Marchetti, M. A., Kittler, H., and Halpern,\nA. Skin lesion analysis toward melanoma detection 2018:\nA challenge hosted by the international skin imaging\ncollaboration (ISIC). abs/1902.03368, 2019.\nDai, X., Chen, Y ., Xiao, B., Chen, D., Liu, M., Yuan, L.,\nand Zhang, L. Dynamic head: Unifying object detection\nheads with attentions. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pp. 7373‚Äì7382, June 2021a.\nDai, X., Chen, Y ., Yang, J., Zhang, P., Yuan, L., and Zhang,\nL. Dynamic detr: End-to-end object detection with dy-\nnamic attention. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pp.\n2988‚Äì2997, October 2021b.\nDai, Z., Liu, H., Le, Q. V ., and Tan, M. Coatnet: Marrying\nconvolution and attention for all data sizes. In arXiv\n2106.04803, 2021c.\nDean, J. Introducing pathways: A next-generation\nai architecture. https://blog.google/\ntechnology/ai/introducing-pathways-\nnext-generation-ai-architecture/ .\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pp. 248‚Äì255. Ieee, 2009.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In arXiv 1810.04805, 2019.\nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L.,\nChen, D., and Guo, B. Cswin transformer: A general\nvision transformer backbone with cross-shaped windows.\nIn arXiv 2107.00652, 2021.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\nM., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.\nAn image is worth 16x16 words: Transformers for image\nrecognition at scale. ICLR, 2021a.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\nM., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.\nAn image is worth 16x16 words: Transformers for image\nrecognition at scale. In arXiv 2010.11929, 2021b.\nDou, Z.-Y ., Xu, Y ., Gan, Z., Wang, J., Wang, S., Wang, L.,\nZhu, C., Nanyun, Peng, Liu, Z., and Zeng, M. An em-\npirical study of training end-to-end vision-and-language\ntransformers. In arXiv 2111.02387, 2021.\nFang, Z., Wang, J., Hu, X., Wang, L., Yang, Y ., and Liu,\nZ. Compressing visual-linguistic model via knowledge\ndistillation. In ICCV, 2021.\nGan, Z., Chen, Y .-C., Li, L., Zhu, C., Cheng, Y ., and Liu, J.\nLarge-scale adversarial training for vision-and-language\nrepresentation learning. In Proceedings of Neural Infor-\nmation Processing Systems, 2020.\nGao, L., Zhang, Y ., Han, J., and Callan, J. Scaling deep\ncontrastive learning batch size under memory limited\nsetup. In arXiv 2101.06983, 2021.\nGemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A.,\nLawrence, W., Moore, R. C., Plakal, M., and Ritter, M.\nAudio set: An ontology and human-labeled dataset for\naudio events. In ICASSP, pp. 776‚Äì780. IEEE, 2017.\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In 2014 IEEE Conference on Computer\nVision and Pattern Recognition, pp. 580‚Äì587, 2014.\nGoyal, Y ., Khot, T., Summers-Stay, D., Batra, D., and\nParikh, D. Making the V in VQA matter: Elevating the\nrole of image understanding in visual question answering.\nIn CVPR, 2017.\nGuo, Y ., Codella, N. C. F., Karlinsky, L., Smith, J. R., Ros-\ning, T., and Feris, R. S. A new benchmark for evaluation\nof cross-domain few-shot learning. ECCV, 2020.\nGupta, A., Dollar, P., and Girshick, R. Lvis: A dataset for\nlarge vocabulary instance segmentation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), June 2019.\nHelber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:\nA novel dataset and deep learning benchmark for land\nuse and land cover classiÔ¨Åcation. IEEE Journal of Se-\nlected Topics in Applied Earth Observations and Remote\nSensing, 12(7):2217‚Äì2226, 2019.\nHuang, X. A holistic representation toward inte-\ngrative ai. https://www.microsoft.com/\nen-us/research/blog/a-holistic-\nrepresentation-toward-integrative-\nai/.\nFlorence: A New Foundation Model for Computer Vision\nHuang, Z., Zeng, Z., Liu, B., Fu, D., and Fu, J. Pixel-BERT:\nAligning image pixels with text by deep multi-modal\ntransformers. arXiv preprint, 2020.\nHuang, Z., Zeng, Z., Huang, Y ., Liu, B., Fu, D., and Fu,\nJ. Seeing out of the box: End-to-end pre-training for\nvision-language representation learning. In CVPR, 2021.\nJia, C., Yang, Y ., Xia, Y ., Chen, Y .-T., Parekh, Z., Pham,\nH., Le, Q. V ., Sung, Y ., Li, Z., and Duerig, T. Scaling up\nvisual and vision-language representation learning with\nnoisy text supervision. In arXiv 2102.05918, 2021.\nKim, W., Son, B., and Kim, I. Vilt: Vision-and-language\ntransformer without convolution or region supervision.\nIn Meila, M. and Zhang, T. (eds.), ICML, 2021.\nKolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung,\nJ., Gelly, S., and Houlsby, N. Big transfer (bit): Gen-\neral visual representation learning. In arXiv 1912.11370,\n2020.\nKornblith, S., Shlens, J., and Le, Q. V . Do better imagenet\nmodels transfer better? In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npp. 2661‚Äì2671, 2019.\nKrasin, I., Duerig, T., Alldrin, N., Veit, A., Abu-El-Haija,\nS., Belongie, S., Cai, D., Feng, Z., Ferrari, V ., Gomes,\nV ., Gupta, A., Narayanan, D., Sun, C., Chechik, G., and\nMurphy, K. Openimages: A public dataset for large-scale\nmulti-label and multi-class image classiÔ¨Åcation. Dataset\navailable from https://github.com/openimages, 2016.\nKrishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K.,\nKravitz, J., Chen, S., Kalantidis, Y ., Li, L.-J., Shamma,\nD. A., Bernstein, M., and Fei-Fei, L. Visual genome: Con-\nnecting language and vision using crowdsourced dense\nimage annotations. In arXiv 1602.07332, 2016.\nLi, J., Selvaraju, R. R., Gotmare, A. D., Joty, S., Xiong,\nC., and Hoi, S. Align before fuse: Vision and language\nrepresentation learning with momentum distillation. In\nConference on Neural Information Processing Systems\n(NeurIPS), 2021a.\nLi, L. H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y .,\nWang, L., Yuan, L., Zhang, L., Hwang, J.-N., Chang, K.-\nW., and Gao, J. Grounded language-image pre-training.\nIn arXiv In Preparation, 2021b.\nLi, W., Gao, C., Niu, G., Xiao, X., Liu, H., Liu, J., Wu, H.,\nand Wang, H. Unimo: Towards uniÔ¨Åed-modal understand-\ning and generation via cross-modal contrastive learning.\nIn Annual Meeting of the Association for Computational\nLinguistics (ACL), 2021c.\nLi, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang,\nL., Hu, H., Dong, L., Wei, F., Choi, Y ., and Gao, J. Oscar:\nObject-semantics aligned pre-training for vision-language\ntasks. In Proceedings of European Conference on Com-\nputer Vision, 2020.\nLin, T.-Y ., Maire, M., Belongie, S., Bourdev, L., Girshick,\nR., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and\nDoll¬¥ar, P. Microsoft COCO:: Common objects in context,\n2015.\nLiu, B., Zhao, Z., Li, Z., Jiang, J., Guo, Y ., and Ye, J. Fea-\nture transformation ensemble model with batch spectral\nregularization for cross-domain few-shot classiÔ¨Åcation.\n2020.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov,\nV . RoBERTa: A robustly optimized bert pretraining\napproach. arXiv preprint, 2019.\nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S.,\nand Guo, B. Swin transformer: Hierarchical vision trans-\nformer using shifted windows. International Conference\non Computer Vision (ICCV), 2021a.\nLiu, Z., Ning, J., Cao, Y ., Wei, Y ., Zhang, Z., Lin, S.,\nand Hu, H. Video swin transformer. arXiv preprint\narXiv:2106.13230, 2021b.\nMiech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev,\nI., and Sivic, J. Howto100m: Learning a text-video em-\nbedding by watching hundred million narrated video clips.\nIn ICCV, pp. 2630‚Äì2640, 2019.\nMiech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J.,\nand Zisserman, A. End-to-end learning of visual repre-\nsentations from uncurated instructional videos. In CVPR,\npp. 9879‚Äì9889, 2020.\nMohanty, S. P., Hughes, D. P., and Salathe, M. Using deep\nlearning for image-based plant disease detection. Front\nPlant Sci, 7, 2016.\nOrdonez, V ., Kulkarni, G., and Berg, T. L. Im2text: De-\nscribing images using 1 million captioned photographs.\nIn NeurIPS, 2011.\nPlummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C.,\nHockenmaier, J., and Lazebnik, S. Flickr30k entities:\nCollecting region-to-phrase correspondences for richer\nimage-to-sentence models. In arXiv 1505.04870, 2016.\nQi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,\nA. Imagebert: Cross-modal pre-training with large-\nscale weak-supervised image-text data. arXiv preprint-\narXiv:2001.07966, 2020.\nFlorence: A New Foundation Model for Computer Vision\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\nJ., Krueger, G., and Sutskever, I. Learning transferable\nvisual models from natural language supervision. InarXiv\n2103.00020, 2021.\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y . Zero:\nMemory optimization towards training A trillion parame-\nter models. CoRR, 2019.\nRamesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Rad-\nford, A., Chen, M., and Sutskever, I. Zero-shot text-to-\nimage generation. In arXiv 2102.12092, 2021.\nRyoo, M. S., Piergiovanni, A., Arnab, A., Dehghani, M.,\nand Angelova, A. Tokenlearner: What can 8 learned\ntokens do for images and videos? In arXiv 2106.11297,\n2021.\nShao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X.,\nLi, J., and Sun, J. Objects365: A large-scale, high-\nquality dataset for object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), October 2019.\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\nceptual captions: A cleaned, hypernymed, image alt-text\ndataset for automatic image captioning. InACL, pp. 2556‚Äì\n2565, 2018.\nShen, S., Li, L. H., Tan, H., Bansal, M., Rohrbach, A.,\nChang, K.-W., Yao, Z., and Keutzer, K. How much can\nclip beneÔ¨Åt vision-and-language tasks? arXiv preprint,\n2021.\nTschandl, P., Rosendahl, C., and Kittler, H. The ham10000\ndataset, a large collection of multi-source dermatoscopic\nimages of common pigmented skin lesions. Nature Sci-\nentiÔ¨Åc Data, 5, 2018.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten-\ntion is all you need. In Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates, Inc.,\n2017.\nWang, J., Hu, X., Zhang, P., Li, X., Wang, L., Zhang, L.,\nGao, J., and Liu, Z. Minivlm: A smaller and faster vision-\nlanguage model. arXiv preprint arXiv:2012.06946, 2020.\nWang, X., Peng, Y ., Lu, L., Lu, Z., Bagheri, M., and Sum-\nmers, R. M. Chestx-ray8: Hospital-scale chest x-ray\ndatabase and benchmarks on weakly-supervised classi-\nÔ¨Åcation and localization of common thorax diseases. In\narXiv 1705.02315, 2017.\nWang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y ., and Cao,\nY . Simvlm: Simple visual language model pretraining\nwith weak supervision. In arXiv 2108.10904, 2021.\nWu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L.,\nand Zhang, L. Cvt: Introducing convolutions to vision\ntransformers. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pp. 22‚Äì31,\nOctober 2021.\nXie, Q., Luong, M.-T., Hovy, E., and Le, Q. V . Self-training\nwith noisy student improves imagenet classiÔ¨Åcation. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), June 2020.\nXu, H., Ghosh, G., Huang, P.-Y ., Okhonko, D., Aghajanyan,\nA., Metze, F., Zettlemoyer, L., and Feichtenhofer, C.\nVideoclip: Contrastive pre-training for zero-shot video-\ntext understanding. In EMNLP, 2021a.\nXu, J., Mei, T., Yao, T., and Rui, Y . Msr-vtt: A large video\ndescription dataset for bridging video and language. In\nCVPR, pp. 5288‚Äì5296, 2016.\nXu, M., Zhang, Z., Hu, H., Wang, J., Wang, L., Wei, F., Bai,\nX., and Liu, Z. End-to-end semi-supervised object detec-\ntion with soft teacher. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pp.\n3060‚Äì3069, October 2021b.\nXue, H., Huang, Y ., Liu, B., Peng, H., Fu, J., Li, H., and\nLuo, J. Probing inter-modality: Visual parsing with self-\nattention for vision-language pre-training. In NeurIPS,\n2021.\nYang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., and\nGao, J. Focal self-attention for local-global interactions\nin vision transformers. In arXiv 2107.00641, 2021.\nYang, J., Li, C., Zhang, P., Xiao, B., Liu, C., Yuan, L., and\nGao, J. UniÔ¨Åed contrastive learning in image-text-label\nspace. In arXiv In Preparation, 2022.\nYao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H.,\nLiang, X., Li, Z., Jiang, X., and Xu, C. Filip: Fine-\ngrained interactive language-image pre-training. In arXiv\n2111.07783, 2021.\nYu, F., Tang, J., Yin, W., Sun, Y ., Tian, H., Wu, H.,\nand Wang, H. Ernie-vil: Knowledge enhanced vision-\nlanguage representations through scene graph. arXiv\npreprint arXiv:2006.16934, 2020.\nYu, Y ., Kim, J., and Kim, G. A joint sequence fusion model\nfor video question answering and retrieval. In ECCV, pp.\n471‚Äì487, 2018.\nZhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling\nvision transformers. In arXiv 2106.04560, 2021.\nZhang, P., Dai, X., Yang, J., Xiao, B., Yuan, L., Zhang, L.,\nand Gao, J. Multi-scale vision longformer: A new vision\nFlorence: A New Foundation Model for Computer Vision\ntransformer for high-resolution image encoding. ICCV\n2021, 2021a.\nZhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L.,\nChoi, Y ., and Gao, J. Vinvl: Revisiting visual representa-\ntions in vision-language models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 5579‚Äì5588, June 2021b.\nZhou, X., Koltun, V ., and Kr ¬®ahenb¬®uhl, P. Simple multi-\ndataset detection. In arXiv 2102.13086, 2021.\nZoph, B., Ghiasi, G., Lin, T.-Y ., Cui, Y ., Liu, H., Cubuk,\nE. D., and Le, Q. Rethinking pre-training and self-\ntraining. In NeurIPS, 2020."
}