{
  "title": "Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models",
  "url": "https://openalex.org/W3154420890",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2099079019",
      "name": "Ling Liu",
      "affiliations": [
        "University of Colorado System"
      ]
    },
    {
      "id": "https://openalex.org/A2177549942",
      "name": "Mans Hulden",
      "affiliations": [
        "University of Colorado System"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2807188009",
    "https://openalex.org/W2896509746",
    "https://openalex.org/W3037995636",
    "https://openalex.org/W2950825792",
    "https://openalex.org/W2963635689",
    "https://openalex.org/W3026538704",
    "https://openalex.org/W2889368791",
    "https://openalex.org/W2888784389",
    "https://openalex.org/W3037720825",
    "https://openalex.org/W2970963828",
    "https://openalex.org/W2963472176",
    "https://openalex.org/W3117518355",
    "https://openalex.org/W2962680795",
    "https://openalex.org/W3099955547",
    "https://openalex.org/W2625720409",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3153224075",
    "https://openalex.org/W2740149041",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3100160869",
    "https://openalex.org/W2186780112",
    "https://openalex.org/W2845772244",
    "https://openalex.org/W3177044084",
    "https://openalex.org/W4287179473",
    "https://openalex.org/W2515098781",
    "https://openalex.org/W4205498194",
    "https://openalex.org/W2899302455",
    "https://openalex.org/W2508815538"
  ],
  "abstract": "Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has indeed been largely the case in these tasks. Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmata—i.e. under “wug test”-like circumstances. This is true even though the actual number of training examples is very large. While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 739 - 749\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nCan a Transformer Pass the Wug Test? Tuning Copying Bias in Neural\nMorphological Inﬂection Models\nLing Liu and Mans Hulden\nUniversity of Colorado\nfirst.last@colorado.edu\nAbstract\nDeep learning sequence models have been suc-\ncessful with morphological inﬂection genera-\ntion. The SIGMORPHON shared task results\nin the past several years indicate that such mod-\nels can perform well, but only if the training\ndata covers a good amount of different lem-\nmata, or if the lemmata to be inﬂected at test\ntime have also been seen in training, as has in-\ndeed been largely the case in these tasks. Sur-\nprisingly, we ﬁnd that standard models such as\nthe Transformer almost completely fail at gen-\neralizing inﬂection patterns when trained on a\nlimited number of lemmata and asked to inﬂect\npreviously unseen lemmata—i.e. under “wug\ntest”-like circumstances. This is true even\nthough the actual number of training examples\nis very large. While established data augmen-\ntation techniques can be employed to allevi-\nate this shortcoming by introducing a copying\nbias through hallucinating synthetic new word\nforms using the alphabet in the language at\nhand, our experiment results show that, to be\nmore effective, the hallucination process needs\nto pay attention to substrings of syllable-like\nlength rather than individual characters.1\n1 Introduction\nThe Transformer model has delivered convincing\nresults in many different tasks related to word-\nformation and analysis (Vylomova et al., 2020;\nMoeller et al., 2020, 2021; Liu, 2021). Especially\non inﬂection tasks, where an input lemma such\nas dog, and input inﬂectional features such as\n{N,PL}, are expected to produce an output such as\ndogs, the model has shown to be particularly adept\nat generalizing patterns (Vylomova et al., 2020; Liu\nand Hulden, 2020a,b; Wu et al., 2021). However,\nwe have discovered that this is only true if the train-\ning data covers a diversity of lemmata or some\nvariant of the input lemma to be inﬂected has been\n1The code and data are available at https://github.\ncom/LINGuistLIU/transformer-wug-test.\nczech finnish german russian spanish turkish\n79.5 75.6 70.78 77.96\n89.94 88.14\n16.22\n5.72 9.0\n19.0\n32.44 32.23\n54.2\n68.5 62.62 69.26\n91.94\n70.31\nakan ga lingala nyanja southern\nsotho\nswahili\n80.68\n93.61 98.26 98.57 95.15 99.47\n41.18\n49.24\n60.83\n84.99\n6.74\n45.26\n94.79 100.0 93.33 99.57\n88.0\n97.35\nFigure 1: Transformer performance in the common-\npractice setting (left), “wug test”-like setting (middle),\nand ‘‘wug test”-like setting with our best data hallu-\ncination method (right)\nwitnessed during training. In a “wug test” (Berko,\n1958) setting where the witnessed lemmata are usu-\nally limited and a previously unseen lemma—like\nwug—is to be inﬂected in some way, we ﬁnd that\nthe Transformer almost completely fails to general-\nize inﬂection patterns, despite abundant inﬂected\nforms for training. It has been noted earlier that\nneural sequence-to-sequence models are apt to per-\nform poorly for morphological inﬂection if they\nhave been exposed to little training data and data\naugmentation can be leveraged to alleviate the prob-\nlem (Cotterell et al., 2017, 2018; Kann and Schütze,\n2017; Liu and Hulden, 2021). Our starting point\nis our observation that the poor “wug test” perfor-\nmance is maintained even with abundant training\ninﬂected forms.\nIn our study, we show three main results. (1)\nWe demonstrate that, even if trained with relatively\nlarge amounts of inﬂected forms, a Transformer\nmodel of the kind that has been very successful at\nrecent shared tasks largely fails to generalize in-\nﬂection patterns if it has not been exposed during\ntraining to a variety of lemmata or any lemmata in\n739\nthe test set. This is true even for datasets where all\nwords inﬂect in the same way—i.e. there are no in-\nﬂectional classes or allomorphs of morphemes, as\nis found in the low-resource Niger-Congo language\ndatasets used in SIGMORPHON 2020 shared task\n(Vylomova et al., 2020). (2) We show that simply\nexposing the model to uninﬂected lemmata in the\ntest set—without providing a single inﬂected form—\nallows the model to dramatically improve its perfor-\nmance when inﬂecting such lemmata. (3) Further,\nwe investigate several strategies that avoid leverag-\ning test set lemmata. We show that when inducing\na copy bias in the model by hallucinating new lem-\nmata, or by hallucinating new inﬂected forms, the\nmethod of hallucination is much more effective if\nit is sensitive to substrings of syllable-like length\nrather than individual characters or stems. Our\nbest models achieve substantial improvement upon\nearlier state-of-the-art data hallucination methods\n(Silfverberg et al., 2017; Anastasopoulos and Neu-\nbig, 2019).\n2 Data\n2018-languages We use six languages from\nthe CoNLL-SIGMORPHON 2018 shared task 1\nmedium setting, where each language has 1,000\n(LEMMA, TARGET TAGS, TARGET FORM) triples\nfor training (Cotterell et al., 2018). The six\nlanguages, Czech, Finnish, German, Russian,\nSpanish and Turkish, are selected to represent the\ndiversity of language typology and morphological\ninﬂection challenges. Though there are only\n1,000 training triples, they cover a fair number\nof lemmata as each lemma appears only once or\ntwice, an amount very hard to obtain for really\nlow-resource languages. In the original shared\ntask, between 2% and 27% of the lemmata in the\ndev and test sets are also found in the training set.\nTo prepare training data for the “wug test”-like\ncircumstance, we select the UniMorph (Kirov et al.,\n2018) paradigms for the ﬁrst 100 most frequent\nlexemes found in Wikipedia text,2 which are not\nincluded in the 2018 shared task 1 dev and test\nsets. The shared task dev and test sets are used for\nvalidation and evaluation without any change. The\n100 full inﬂection tables give us over 1,000 (for\nCzech, German and Russian) or over 7,000 (for\nFinnish, Spanish and Turkish) training triples.\n2We also experimented with using 100 random UniMorph\nlexemes, and did not ﬁnd substantial difference between using\nrandom ones and the most frequent ones.\nNiger-Congo languages In addition, we use\nsix Niger-Congo languages from SIGMORPHON\n2020 shared task 0 (Vylomova et al., 2020): Akan,\nGa, Lingala, Nyanja, Southern Sotho and Swahili.\nThese languages are low-resource, but the dataset\nonly contains very regular inﬂections. In the orig-\ninal shared task data split, The overlap between\nthe lemmata in the dev and test sets and those in\nthe training set is 100%. The number of paradigms\nwhich we can obtain by combining the training, dev\nand test sets of this dataset is around 100 for Akan,\nGa and Swahili, 227 for Nyanja, 57 for Lingala and\nonly 26 for Southern Sotho.\nFor the “wug test”, we divide the inﬂection tables\nreconstructed from this dataset into a 7:1:2 train-\ndev-test split, i.e. we use the same ratio as the\nshared task, but the division is by inﬂection tables\nrather than lemma-tag-form triples, to ensure that\nthe lemmata used for validation and test are disjoint\nfrom those for training. We provide details on the\ndata statistics in Appendix A for reference.\n3 Experiments\nInﬂection model The Transformer (Vaswani\net al., 2017) is the seq2seq architecture which pro-\nduces the current state-of-the-art result on the mor-\nphological inﬂection task (Vylomova et al., 2020;\nLiu and Hulden, 2020a,b; Wu et al., 2021). It takes\nthe lemma and target tag(s) as input and predicts\nthe target form character by character. Our experi-\nments use the Transformer implemented in fairseq\n(Ott et al., 2019) and adopt the same hyperparame-\nters as Liu and Hulden (2020a). 3\nEvaluation metric The evaluation metric is ac-\ncuracy. For the original shared task data and exper-\niments on 2018 languages, we train ﬁve inﬂection\nmodels each with a different random initialization\nand report the average accuracy with standard de-\nviation. Due to data scarcity, for Niger-Congo lan-\nguages at the “wug test”-like setting, we perform\na 5-fold cross-validation and report the average\naccuracy and the standard deviation.\nCommon-practice test and “wug test” We ﬁrst\ncompare the performance of the Transformer in the\ncommon-practice setting and the “wug test”-like\nsetting. The “common practice” is represented by\n3We also conducted experiments with the encoder-decoder\nwith hard monotonic attention model (Wu and Cotterell, 2019),\nbut found the same conclusion as for the Transformer model.\nExperiments on the hard monotonic model is provided in\nAppendix C for reference.\n740\ng l e i c h e  _ n \ng l e i c h e  s tGenerate dummy-stem and replace stem part\n       Find common substrings as stem\nAlign lemma and  target-form\nOutput\ne i z i e n t s a t i  _ n\ne i z i e n t s a t i  s t\ng l e i c h e _ n \ng l e i c h e s t\ngleichen     V;SBJV;PRS;2;PL       gleichest\n eizientsatin    V;SBJV;PRS;2;PL       eizientsatist \n LEMMA               TARGET-TAG          TARGET-FORM\nLEMMA         TARGET-FORM\nLEMMA         TARGET-FORM\nDUMMY-LEMMA         DUMMY-TARGET-FORM\nDUMMY-LEMMA         TARGET-TAG     DUMMY-TARGET-FORM\n(2) hallucination\nGenerate dummy lemma\nOutput\n(ä, e, k, m, …)     or    (we, mer, nigt, …)ALPHABET           or         N-GRAM SET\nwemerzälknigtäh COPY wemerzälknigtäh   DUMMY-LEMMA      <COPY>     DUMMY-LEMMA\nwemerzälknigtähDUMMY-LEMMA\n(1) copy\n(a) (b)\ng l e i c h e  _ n \ng l e i c h e  s tGenerate dummy-stem and replace stem part\n       Find common substrings as stem\nAlign lemma and  target-form\nOutput\ne i z i e n t s a t i  _ n\ne i z i e n t s a t i  s t\ng l e i c h e _ n \ng l e i c h e s t\ngleichen     V;SBJV;PRS;2;PL       gleichest\n eizientsatin    V;SBJV;PRS;2;PL       eizientsatist \n LEMMA               TARGET-TAG          TARGET-FORM\nLEMMA         TARGET-FORM\nLEMMA         TARGET-FORM\nDUMMY-LEMMA         DUMMY-TARGET-FORM\nDUMMY-LEMMA         TARGET-TAG     DUMMY-TARGET-FORM\n(2) hallucination\nGenerate dummy lemma\nOutput\n(ä, e, k, m, …)     or    (we, mer, nigt, …)ALPHABET           or         N-GRAM SET\nwemerzälknigtäh COPY wemerzälknigtäh   DUMMY-LEMMA      <COPY>     DUMMY-LEMMA\nwemerzälknigtähDUMMY-LEMMA\n(1) copy\n(a) (b)\nFigure 2: (a) Dummy lemma generation with a German example. +copy-2k-char generates random strings by\nuniformly sampling from the alphabet, while+copy-2k-substr samples from the set of 2-, 3- and 4-grams; (b) Data\nhallucination with a German example. +hall-2k-substr is different from +hall-2k-char in how the dummy-stem is\ngenerated.\nprevious years’ shared tasks and related work (Cot-\nterell et al., 2016, 2017, 2018; McCarthy et al.,\n2019; Vylomova et al., 2020); here the training\ndata usually covers a fair number of lemmata and\nthere is overlap between lemmata in the training\nand test sets. We use the shared task data to rep-\nresent the common-practice setting. In the “wug\ntest” setting, we control the number of lemmata\nfor training but not inﬂected forms (as explained\nin Section 2) and the lemmata to be inﬂected are\nalways previously unseen. To our surprise, the per-\nformance of the Transformer at the “wug test”-like\nsetting is very poor despite the large amount of\ntraining triples for 2018-languages or the very reg-\nular and straightforward inﬂection for Niger-Congo\nlanguages. The performance is dramatically infe-\nrior to the common-practice setting, even when the\nnumber of training triples is seven times larger for\nFinnish, Spanish and Turkish (see Figure 1).\nWe hypothesize four reasons for the poor per-\nformance of the model under the “wug test”-like\ncircumstance: (1) missing copy bias regarding the\nentire stem, i.e. the model can’t copy a stem abcde\nif that exact stem has never been seen during train-\ning, (2) missing copy bias on individual letters, i.e.\nthe model can’t copy lettera if the letter is under-\nrepresented in training, (3) missing copy bias on\nsubsequences of letters, i.e. the model can’t copy\nsequence ab if the sequence is underrepresented in\ntraining, (4) some combination of all the factors\nabove. To test these hypotheses, we conduct ﬁve\nexperiments designed to help the model learn to\ncopy with different biases by adding to the training\nset for each language 2,000 4 dummy data points\ngenerated in ﬁve different ways, explained below.\n+copy-dev-test-lemmas In order to test the ﬁrst\nhypothesis that the model does not learn to copy\nparts of a stem it has not seen at the training stage,\nwe augment the training data for each language by\nadding to it the lemmata in its development and\ntest sets with a special tag COPY. In other words,\n2,000 (LEMMA, COPY, LEMMA) triples are added to\nthe initial “wug test” training set for each language.\n+copy-2k-char and +copy-2k-substr Previous\nwork found that adding random strings can help\nseq2seq models learn a copy bias and thus improve\nthe performance when the training data is limited\n(Kann and Schütze, 2017). We adopt a similar\nmethod to augment the training data with dummy\nlemmata generated by the process shown in Figure\n2 (a). The +copy-2k-char method takes as input\nthe alphabet created by collecting characters in the\nlanguage’s training set.\nConsidering that a natural linguistic sub-unit\nof a word is a syllable, we propose to use sub-\n4The choice of 2,000 is in order to match the augmentation\nsize of +copy-dev-test-lemmas method for 2018-languages.\nWe did not try to tune for the best data augmentation size. Ap-\npendix B provides plots of data augmentation size comparison,\nwhere we found no consistent difference in all the languages.\n741\nFigure 3: “Wug test” results. +copy-2k-char adds random strings generated with the alphabet. +copy-2k-substr\nadds random strings generated with the n-gram set. +hall-2k-char adds data hallucinated with the method by\nAnastasopoulos and Neubig (2019). +hall-2k-substr adds data hallucinated with our method.\nstrings of syllable-like length for the +copy-2k-\nsubstr method. The input of this method is the\nset of bigrams, trigrams and four-grams from the\nlanguage’s training data. For both methods, we\ngenerate the dummy lemma by uniformly sampling\nfrom the input and concatenating the sampled items\nto a random length between the minimum and max-\nimum word length we see in the training data. The\noutput of the dummy lemma generation process is\na triple of a dummy lemma, a special symbolCOPY\nand the dummy lemma, which is added to the initial\n“wug test” training set for data augmentation.\n+hall-2k-char and +hall-2k-substr The dummy\nlemma generation methods do not leverage knowl-\nedge about word structure which can be inferred\nfrom the training data. Silfverberg et al. (2017)\nfound that it is very effective to augment training\ndata in low-resource situations with a data halluci-\nnation approach by replacing a hypothesized stem\nof the training triples with a random string. Anas-\ntasopoulos and Neubig (2019) improves this data\nhallucination method by taking into discontinuous\nstems into consideration as well; this is the best\ndata hallucination method so far. We conduct the\n+hall-2k-char experiment by augmenting the initial\n“wug test” training set with dummy data generated\nwith Anastasopoulos and Neubig (2019)’s method.\nThe implementation from SIGMORPHON 2020\nshared task 0 baseline is used.\nIn addition, we propose to generate the dummy\nstem by uniformly sampling from substrings of\nsyllable-like length, i.e. the bigram, trigram and\nfour-gram set. This experiment is referred to as\n+hall-2k-substr. Speciﬁcally, both data hallucina-\ntion methods (illustrated in Figure 2 (b)) take as\ninput a triple from the training set, aligns the lemma\nand the target form with the alignment method from\nSIGMORPHON 2016 shared task baseline (Cot-\nterell et al., 2016), ﬁnds the common substrings\nbetween the lemma and the target form as the stem,\nreplaces the stem with a dummy stem, and out-\nputs a dummy triple which is adopted for data\naugmentation. Our proposed method is different\nfrom Anastasopoulos and Neubig (2019)’s method\nat the dummy stem generation step in two main\naspects: (1) Instead of sampling from the alpha-\nbet, we sample from the set of bigrams, trigrams\nand four-grams. (2) Instead of forcing the dummy\nstem to be of the same length as the stem to be\n742\nreplaced, we only constrain the minimum and max-\nimum length of the stem based on the training data.\nIn addition, for discontinuous stems, we only re-\nplace the ﬁrst part of the stem.5\n4 Results and discussion\n“Wug test” with data augmentation Figure 3\nshows results for the “wug test”-like setting and\nresults after augmenting the initial training set with\ndifferent methods. Every language sees a substan-\ntial improvement with data augmentation, indicat-\ning that the Transformer model in the vanilla “wug\ntest” circumstance will not learn a copy bias well.\nThe substring-based data hallucination we pro-\npose, +hall-2k-substr, achieves accuracies which\nare substantially higher than other methods for\nmost languages. For Turkish and Nyanja, +hall-2k-\nsubstr is lower than the best performance, but the\ndifference is not obvious. For Lingala, +hall-2k-\nsubstr has the same best performance as +hall-2k-\nchar. The consistent advantage of +hall-2k-substr\nimplies that substrings of syllable-like length is\nmore helpful than individual characters for data\nhallucination. It also provides support to the fourth\nhypothesis we made in section 3 that the poor per-\nformance of the Transformer in the vanilla “wug\ntest”-like setting is due to a combination of factors\nincluding missing copying bias for letters, subse-\nquences of letters and even entire stems.\nCommon practice vs “wug test” Figure 1 plots\nthe Transformer accuracies with standard devia-\ntions in the common-practice setting, vanilla “wug\ntest”-like setting, and “wug test”-like setting with\ndata augmentation by the substring-based data hal-\nlucination methods ( +hall-2k-substr). Though\ndata augmentation can improve the model’s per-\nformance for a “wug test”, results are still infe-\nrior to the common practice setting without any\ndata augmentation for most languages, especially\nthe morphologically challenging 2018 CoNLL-\nSIGMORPHON languages.\n5 Conclusion\nIn this work, we examine limiting the number of\ntraining lemmata and keeping training lemmata\ndisjoint from the evaluation sets in morphologi-\ncal inﬂection. By comparing the performance of\n5Using the ﬁrst part only is for implementation simplicity\nin the current work. It should be adjusted for languages with a\nlarge number of discontinuous stems.\nthe Transformer under the “wug test”-like circum-\nstance with the common practice, we ﬁnd that the\ncommon-practice setting where the training data\ncovers a fair amount of lemmata and there is over-\nlap of lemmata in training and evaluation, has ob-\nscured the difﬁculty of the task. We propose to aug-\nment the training data with substring-based data\nhallucination, and achieve substantial improvement\nover previous data hallucination methods.\nConsidering the ﬁndings in this paper, we sug-\ngest that future experiments include evaluations on\nmodel performance using lemmata not found in the\ntraining set and use unique lemma counts rather\nthan triple counts to document data set sizes.\nReferences\nRoee Aharoni and Yoav Goldberg. 2017. Morphologi-\ncal inﬂection generation with hard monotonic atten-\ntion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2004–2015, Vancouver,\nCanada. Association for Computational Linguistics.\nRoee Aharoni, Yoav Goldberg, and Yonatan Belinkov.\n2016. Improving sequence to sequence learning\nfor morphological inﬂection generation: The BIU-\nMIT systems for the SIGMORPHON 2016 shared\ntask for morphological reinﬂection. In Proceed-\nings of the 14th SIGMORPHON Workshop on Com-\nputational Research in Phonetics, Phonology, and\nMorphology, pages 41–48. Association for Compu-\ntational Linguistics.\nAntonios Anastasopoulos and Graham Neubig. 2019.\nPushing the limits of low-resource morphological in-\nﬂection. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n984–996, Hong Kong, China. Association for Com-\nputational Linguistics.\nJean Berko. 1958. The child’s learning of English mor-\nphology. Word, 14(2-3):150–177.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nGéraldine Walther, Ekaterina Vylomova, Arya D.\nMcCarthy, Katharina Kann, Sabrina J. Mielke, Gar-\nrett Nicolai, Miikka Silfverberg, David Yarowsky,\nJason Eisner, and Mans Hulden. 2018. The CoNLL–\nSIGMORPHON 2018 shared task: Universal mor-\nphological reinﬂection. In Proceedings of the\nCoNLL–SIGMORPHON 2018 Shared Task: Univer-\nsal Morphological Reinﬂection , pages 1–27, Brus-\nsels. Association for Computational Linguistics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nGéraldine Walther, Ekaterina Vylomova, Patrick\nXia, Manaal Faruqui, Sandra Kübler, David\nYarowsky, Jason Eisner, and Mans Hulden. 2017.\n743\nCoNLL-SIGMORPHON 2017 shared task: Univer-\nsal morphological reinﬂection in 52 languages. In\nProceedings of the CoNLL SIGMORPHON 2017\nShared Task: Universal Morphological Reinﬂection,\npages 1–30, Vancouver. Association for Computa-\ntional Linguistics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nDavid Yarowsky, Jason Eisner, and Mans Hulden.\n2016. The SIGMORPHON 2016 shared Task—\nMorphological reinﬂection. In Proceedings of the\n14th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphol-\nogy, pages 10–22, Berlin, Germany. Association for\nComputational Linguistics.\nKatharina Kann and Hinrich Schütze. 2017. Unlabeled\ndata for morphological generation with character-\nbased sequence-to-sequence models. In Proceed-\nings of the First Workshop on Subword and Charac-\nter Level Models in NLP, pages 76–81, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nChristo Kirov, Ryan Cotterell, John Sylak-Glassman,\nGéraldine Walther, Ekaterina Vylomova, Patrick\nXia, Manaal Faruqui, Sabrina J. Mielke, Arya Mc-\nCarthy, Sandra Kübler, David Yarowsky, Jason Eis-\nner, and Mans Hulden. 2018. UniMorph 2.0: Uni-\nversal Morphology. In Proceedings of the Eleventh\nInternational Conference on Language Resources\nand Evaluation (LREC 2018), Miyazaki, Japan. Eu-\nropean Language Resources Association (ELRA).\nLing Liu. 2021. Computational morphology with\nneural network approaches. arXiv preprint\narXiv:2105.09404.\nLing Liu and Mans Hulden. 2020a. Analogy mod-\nels for neural word inﬂection. In Proceedings\nof the 28th International Conference on Compu-\ntational Linguistics , pages 2861–2878, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nLing Liu and Mans Hulden. 2020b. Leveraging princi-\npal parts for morphological inﬂection. In Proceed-\nings of the 17th SIGMORPHON Workshop on Com-\nputational Research in Phonetics, Phonology, and\nMorphology, pages 153–161, Online. Association\nfor Computational Linguistics.\nLing Liu and Mans Hulden. 2021. Backtranslation\nin neural morphological inﬂection. In Proceedings\nof the Second Workshop on Insights from Negative\nResults in NLP , pages 81–88, Online and Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nPeter Makarov and Simon Clematide. 2018a. Imita-\ntion learning for neural morphological string trans-\nduction. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2877–2882. Association for Computational\nLinguistics.\nPeter Makarov and Simon Clematide. 2018b. Neu-\nral transition-based string transduction for limited-\nresource setting in morphology. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 83–93. Association for Computa-\ntional Linguistics.\nPeter Makarov and Simon Clematide. 2018c. UZH at\nCoNLL–SIGMORPHON 2018 shared task on uni-\nversal morphological reinﬂection. In Proceedings\nof the CoNLL–SIGMORPHON 2018 Shared Task:\nUniversal Morphological Reinﬂection, pages 69–75,\nBrussels. Association for Computational Linguis-\ntics.\nPeter Makarov, Tatiana Ruzsics, and Simon Clematide.\n2017. Align and copy: UZH at SIGMORPHON\n2017 shared task for morphological reinﬂection. In\nProceedings of the CoNLL SIGMORPHON 2017\nShared Task: Universal Morphological Reinﬂection,\npages 49–57, Vancouver. Association for Computa-\ntional Linguistics.\nArya D. McCarthy, Ekaterina Vylomova, Shijie Wu,\nChaitanya Malaviya, Lawrence Wolf-Sonkin, Gar-\nrett Nicolai, Christo Kirov, Miikka Silfverberg, Sab-\nrina J. Mielke, Jeffrey Heinz, Ryan Cotterell, and\nMans Hulden. 2019. The SIGMORPHON 2019\nshared task: Morphological analysis in context and\ncross-lingual transfer for inﬂection. In Proceedings\nof the 16th Workshop on Computational Research in\nPhonetics, Phonology, and Morphology, pages 229–\n244, Florence, Italy. Association for Computational\nLinguistics.\nSarah Moeller, Ling Liu, and Mans Hulden. 2021. To\nPOS tag or not to POS tag: The impact of POS\ntags on morphological learning in low-resource set-\ntings. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers) ,\npages 966–978, Online. Association for Computa-\ntional Linguistics.\nSarah Moeller, Ling Liu, Changbing Yang, Katharina\nKann, and Mans Hulden. 2020. IGT2P: From inter-\nlinear glossed texts to paradigms. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 5251–\n5262, Online. Association for Computational Lin-\nguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nMiikka Silfverberg, Adam Wiemerslage, Ling Liu, and\nLingshuang Jack Mao. 2017. Data augmentation for\nmorphological reinﬂection. In Proceedings of the\n744\nCoNLL SIGMORPHON 2017 Shared Task: Univer-\nsal Morphological Reinﬂection , pages 90–99, Van-\ncouver. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nEkaterina Vylomova, Jennifer White, Eliza-\nbeth Salesky, Sabrina J. Mielke, Shijie Wu,\nEdoardo Maria Ponti, Rowan Hall Maudslay, Ran\nZmigrod, Josef Valvoda, Svetlana Toldova, Francis\nTyers, Elena Klyachko, Ilya Yegorov, Natalia\nKrizhanovsky, Paula Czarnowska, Irene Nikkarinen,\nAndrew Krizhanovsky, Tiago Pimentel, Lucas\nTorroba Hennigen, Christo Kirov, Garrett Nicolai,\nAdina Williams, Antonios Anastasopoulos, Hilaria\nCruz, Eleanor Chodroff, Ryan Cotterell, Miikka\nSilfverberg, and Mans Hulden. 2020. SIGMOR-\nPHON 2020 shared task 0: Typologically diverse\nmorphological inﬂection. In Proceedings of the\n17th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 1–39, Online. Association for Computational\nLinguistics.\nShijie Wu and Ryan Cotterell. 2019. Exact hard mono-\ntonic attention for character-level transduction. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1530–\n1537, Florence, Italy. Association for Computational\nLinguistics.\nShijie Wu, Ryan Cotterell, and Mans Hulden. 2021.\nApplying the transformer to character-level transduc-\ntion. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pages 1901–1907,\nOnline. Association for Computational Linguistics.\nShijie Wu, Pamela Shapiro, and Ryan Cotterell. 2018.\nHard non-monotonic attention for character-level\ntransduction. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 4425–4438, Brussels, Belgium.\nAssociation for Computational Linguistics.\n745\nA Data information\ntriple-counts lemma-counts lemma-overlap (%)\nLanguage train dev test train dev test dev-train test-train\nczech 1000 1000 1000 848 848 849 24.53 20.38\nﬁnnish 1000 1000 1000 985 983 987 2.34 3.04\ngerman 1000 1000 1000 961 945 962 9.42 9.46\nrussian 1000 1000 1000 973 985 977 3.65 3.79\nspanish 1000 1000 1000 906 902 922 15.74 16.49\nturkish 906 928 912 764 802 779 26.06 26.57\nTable 1: CoNLL-SIGMORPHON 2018 shared task 1 medium-size data information.\ntriple-counts lemma-counts lemma-overlap (%)\nLanguage train train dev-train test-train\nczech 1582 100 0 0\nﬁnnish 7136 100 0 0\ngerman 1290 100 0 0\nrussian 1311 100 0 0\nspanish 7132 100 0 0\nturkish 7632 100 0 0\nTable 2: Data information of the training set we create for 2018-languages. We use the same dev and test sets as\nCoNLL-SIGMORPHON 2018 shared task 1.\ntriple-counts lemma-counts lemma-overlap (%)\nLanguage train dev test train dev test dev-train test-train\nakan 2793 380 763 96 94 95 100.0 100.0\nga 607 79 169 95 59 80 100.0 100.0\nlingala 159 23 46 57 23 34 100.0 100.0\nnyanja 3031 429 853 227 199 226 100.0 100.0\nsouthern sotho 345 50 99 26 24 25 100.0 100.0\nswahili 3374 469 910 97 97 96 100.0 100.0\nTable 3: Data information of Niger-Congo languages from SIGMORPHON 2020 shared task 0.\n746\nB Data augmentation size comparison\n\u0016\u0013\n\u0017\u0013\n\u0018\u0013\n\u0019\u0013\n\u001a\u0013\n\u001b\u0013\n\u001c\u0013\n\u0014\u0013\u0013\nDGG\u0003\n\u0014N\nDGG\u0003\n\u0015N\nDGG\u0003\n\u0016N\nDGG\u0003\n\u0017N\nDGG\u0003\n\u0018N\nDGG\u0003\n\u0019N\nDGG\u0003\n\u001aN\nDGG\u0003\n\u001bN\nDGG\u0003\n\u001cN\nF]HFK\nILQQLVK\nJHUPDQ\nUXVVLDQ\nVSDQLVK\nWXUNLVK\n\u0015\u0013\u0014\u001b\u0010ODQJXDJHV\n\u001a\u0018\n\u001b\u0013\n\u001b\u0018\n\u001c\u0013\n\u001c\u0018\n\u0014\u0013\u0013\n\u0014\u0013\u0018\nDGG\u0003\n\u0014N\nDGG\u0003\n\u0015N\nDGG\u0003\n\u0016N\nDGG\u0003\n\u0017N\nDGG\u0003\n\u0018N\nDGG\u0003\n\u0019N\nDGG\u0003\n\u001aN\nDGG\u0003\n\u001bN\nDGG\u0003\n\u001cN\nDNDQ\nJD\nOLQJDOD\nQ\\DQMD\nVRXWKHUQ\u0003VRWKR\nVZDKLOL\n1LJHU\u0010&RQJR\u0003ODQJXDJHV\nFigure 4: Performance on the dev set in “wug test” after adding different amounts of dummy data generated\nwith our substring-based hallucination method.\n747\nC Performance of the encoder-decoder with hard monotonic attention model\nConsidering that the encoder-decoder with hard monotonic attention model (Aharoni et al., 2016; Aharoni\nand Goldberg, 2017; Makarov et al., 2017; Makarov and Clematide, 2018c,a,b; Wu et al., 2018; Wu and\nCotterell, 2019) is designed for the morphological generation task and bias towards copying symbols in\nthe input by leveraging edit actions, we evaluate the performance of the encoder-decoder with exact hard\nmonotonic attention in the “wug test”-like circumstance as well in order to evaluate whether this deep\nlearning model architecture catered to morphological generation is able to learn the generalization ability.\nWe use the encoder-decoder with exact hard monotonic attention model proposed and implemented by\nWu and Cotterell (2019).6\nThe performance of the encoder-decoder with exact hard monotonic attention model for the original\nshared task setup, the “wug test”-like setup with or without our best data hallucination augmentation\nis presented in Figure 5. Figure 6 provides detailed comparison between different data augmentation\nmethods in the “wug test”-like experimental setup by the encoder-decoder with exact hard monotonic\nattention model. We observe that the encoder-decoder with exact hard monotonic attention model has the\nsame limitation as the Transformer model pointed out in the previous section.\nczech finnish german russian spanish turkish\n66.84\n54.96\n71.06 68.42 76.12 73.0\n17.62 17.76 12.88 17.38\n53.22 47.2848.44\n63.86 62.5 62.68\n90.5\n73.77\nakan ga lingala nyanja southern\nsotho\nswahili\n99.97 95.62 97.83 100.0 91.92 100.0\n60.9\n44.8\n28.75\n89.29\n0.63\n30.31\n95.74 99.06 93.33 100.0\n88.0 98.0\nFigure 5: Performance of the encoder-decoder with exact hard monotonic attention model (Wu and Cotterell, 2019)\nin the common-practice setting (left), “wug test”-like setting (middle), and ‘‘wug test”-like setting with our best\ndata hallucination method (right)\n6https://github.com/shijie-wu/neural-transducer\n748\nFigure 6: “Wug test” results by the encoder-decoder with exact hard monotonic attention model (Wu and Cotterell,\n2019), with or without different data augmentation methods.\n749",
  "topic": "Inflection",
  "concepts": [
    {
      "name": "Inflection",
      "score": 0.8637498021125793
    },
    {
      "name": "Computer science",
      "score": 0.6819049119949341
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6203472018241882
    },
    {
      "name": "Substring",
      "score": 0.6158534288406372
    },
    {
      "name": "Natural language processing",
      "score": 0.595703661441803
    },
    {
      "name": "Transformer",
      "score": 0.5741541385650635
    },
    {
      "name": "Hallucinating",
      "score": 0.5164647102355957
    },
    {
      "name": "Copying",
      "score": 0.47519099712371826
    },
    {
      "name": "Speech recognition",
      "score": 0.3600085973739624
    },
    {
      "name": "Data structure",
      "score": 0.15123260021209717
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}