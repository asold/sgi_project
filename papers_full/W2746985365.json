{
  "title": "A Study on Neural Network Language Modeling",
  "url": "https://openalex.org/W2746985365",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Shi, Dengliang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1558797106",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2121227244"
  ],
  "abstract": "An exhaustive study on neural network language modeling (NNLM) is performed in this paper. Different architectures of basic neural network language models are described and examined. A number of different improvements over basic neural network language models, including importance sampling, word classes, caching and bidirectional recurrent neural network (BiRNN), are studied separately, and the advantages and disadvantages of every technique are evaluated. Then, the limits of neural network language modeling are explored from the aspects of model architecture and knowledge representation. Part of the statistical information from a word sequence will loss when it is processed word by word in a certain order, and the mechanism of training neural network by updating weight matrixes and vectors imposes severe restrictions on any significant enhancement of NNLM. For knowledge representation, the knowledge represented by neural network language models is the approximate probabilistic distribution of word sequences from a certain training data set rather than the knowledge of a language itself or the information conveyed by word sequences in a natural language. Finally, some directions for improving neural network language modeling further is discussed.",
  "full_text": "A Study on Neural Network Language Modeling\nDengliang Shi dengliang.shi@yahoo.com\nShanghai, Shanghai, China\nAbstract\nAn exhaustive study on neural network language modeling (NNLM) is performed in this\npaper. Diﬀerent architectures of basic neural network language models are described and\nexamined. A number of diﬀerent improvements over basic neural network language mod-\nels, including importance sampling, word classes, caching and bidirectional recurrent neural\nnetwork (BiRNN), are studied separately, and the advantages and disadvantages of every\ntechnique are evaluated. Then, the limits of neural network language modeling are ex-\nplored from the aspects of model architecture and knowledge representation. Part of the\nstatistical information from a word sequence will loss when it is processed word by word\nin a certain order, and the mechanism of training neural network by updating weight ma-\ntrixes and vectors imposes severe restrictions on any signiﬁcant enhancement of NNLM. For\nknowledge representation, the knowledge represented by neural network language models is\nthe approximate probabilistic distribution of word sequences from a certain training data\nset rather than the knowledge of a language itself or the information conveyed by word\nsequences in a natural language. Finally, some directions for improving neural network\nlanguage modeling further is discussed.\nKeywords: neural network language modeling, optimization techniques, limits, improve-\nment scheme\n1. Introduction\nGenerally, a well-designed language model makes a critical diﬀerence in various natural\nlanguage processing (NLP) tasks, like speech recognition (Hinton et al., 2012; Graves et al.,\n2013a), machine translation (Cho et al., 2014a; Wu et al., 2016), semantic extraction (Col-\nlobert and Weston, 2007, 2008) and etc. Language modeling (LM), therefore, has been the\nresearch focus in NLP ﬁeld all the time, and a large number of sound research results have\nbeen published in the past decades. N-gram based LM (Goodman, 2001a), a non-parametric\napproach, is used to be state of the art, but now a parametric method - neural network\nlanguage modeling (NNLM) is considered to show better performance and more potential\nover other LM techniques, and becomes the most commonly used LM technique in multiple\nNLP tasks.\nAlthough some previous attempts (Miikkulainen and Dyer, 1991; Schmidhuber, 1996;\nXu and Rudnicky, 2000) had been made to introduce artiﬁcial neural network (ANN) into\nLM, NNLM began to attract researches’ attentions only after Bengio et al. (2003) and did\nnot show prominent advantages over other techniques of LM until recurrent neural net-\nwork (RNN) was investigated for NNLM (Mikolov et al., 2010, 2011). After more than\na decade’s research, numerous improvements, marginal or critical, over basic NNLM have\nbeen proposed. However, the existing experimental results of these techniques are not com-\nc⃝Dengliang Shi.\narXiv:1708.07252v1  [cs.CL]  24 Aug 2017\nD. Shi\nparable because they were obtained under diﬀerent experimental setups and, sometimes,\nthese techniques were evaluated combined with other diﬀerent techniques. Another signiﬁ-\ncant problem is that most researchers focus on achieving a state of the art language model,\nbut the limits of NNLM are rarely studied. In a few works (Jozefowicz et al., 2016) on\nexploring the limits of NNLM, only some practical issues, like computational complexity,\ncorpus, vocabulary size, and etc., were dealt with, and no attention was spared on the\neﬀectiveness of modeling a natural language using NNLM.\nSince this study focuses on NNLM itself and does not aim at raising a state of the art\nlanguage model, the techniques of combining neural network language models with other\nkind of language models, like N-gram based language models, maximum entropy (ME)\nlanguage models and etc., will not be included. The rest of this paper is organized as\nfollows: In next section, the basic neural network language models - feed-forward neural\nnetwork language model (FNNLM), recurrent neural network language model (RNNLM)\nand long-short term memory (LSTM) RNNLM, will be introduced, including the training\nand evaluation of these models. In the third section, the details of some important NNLM\ntechniques, including importance sampling, word classes, caching and bidirectional recurrent\nneural network (BiRNN), will be described, and experiments will be performed on them\nto examine their advantages and disadvantages separately. The limits of NNLM, mainly\nabout the aspects of model architecture and knowledge representation, will be explored\nin the fourth section. A further work section will also be given to represent some further\nresearches on NNLM. In last section, a conclusion about the ﬁndings in this paper will be\nmade.\n2. Basic Neural Network Language Models\nThe goal of statistical language models is to estimate the probability of a word sequence\nw1w2...wT in a natural language, and the probability can be represented by the production\nof the conditional probability of every word given all the previous ones:\nP(wT\n1 ) =\nT∏\nt=1\nP(wt|wt−1\n1 )\nwhere, wj\ni = wiwi+1 ...w j−1wj. This chain rule is established on the assumption that\nwords in a word sequence only statistically depend on their previous context and forms\nthe foundation of all statistical language modeling. NNLM is a kind of statistical language\nmodeling, so it is also termed as neural probabilistic language modeling or neural statistical\nlanguage modeling. According to the architecture of used ANN, neural network language\nmodels can be classiﬁed as: FNNLM, RNNLM and LSTM-RNNLM.\n2.1 Feed-forward Neural Network Language Model, FNNLM\nAs mentioned above, the objective of FNNLM is to evaluate the conditional probability\nP(wt|wt−1\n1 ), but feed-forward neural network (FNN) lacks of an eﬀective way to represent\nhistory context. Hence, the idea of n-gram based LM is adopted in FNNLM that words in\na word sequence more statistically depend on the words closer to them, and only the n−1\n2\nA Study on Neural Network Language Modeling\nFigure 1: Feed-forward neural network language model\ndirect predecessor words are considered when evaluating the conditional probability, this is:\nP(wt|wt−1\n1 ) ≈ P(wt|wt−1\nt−n+1)\nThe architecture of the original FNNLM proposed by Bengio et al. (2003) is showed\nin Figure 1, and w0, wT+1 are the start and end marks of a word sequence respectively.\nIn this model, a vocabulary is pre-built from a training data set, and every word in this\nvocabulary is assigned with a unique index. To evaluate the conditional probability of word\nwt, its n−1 direct previous words wt−n+1,...,w t−1 are projected linearly into feature vectors\nusing a shared matrix C ∈Rk×m according to their index in the vocabulary, where k is\nthe size of the vocabulary and m is the feature vectors’ dimension. In fact, every row of\nprojection matrix C is a feature vector of a word in the vocabulary. The input x ∈Rni\nof FNN is formed by concatenating the feature vectors of words wt−n+1,...,w t−1, where\nni = m×(n−1) is the size of FNN’s input layer. FNN can be generally represented as:\ny = V ·f(U ·x + b) + M ·x + d\nWhere, U ∈Rnh×ni, V ∈Rno×nh are weight matrixes, nh is the size of hidden layer, no = k\nis the size of output layer, weight matrix M ∈Rno×ni is for the direct connections between\ninput layer and output layer,b ∈Rnh and d ∈Rno are vectors for bias terms in hidden layer\nand output layer respectively, y ∈Rno is output vector, and f(·) is activation function.\nThe i-th element of output vector y is the unnormalized conditional probability of the\nword with index iin the vocabulary. In order to guarantee all the conditional probabilities\nof words positive and summing to one, a softmax layer is always adopted following the\noutput layer of FNN:\nP(vi|wt−1\n1 ) ≈ P(vi|wt−1\nt−n+1) = ey(vi,wt−1\nt−n+1)\n∑k\nj=1 ey(vj,wt−1\nt−n+1) ,i = 1,2,...,k\nwhere y(vi,wt−1\nt−n+1) (i= 1,2,...,k ) is the i-th element of output vector y, and vi is the i-th\nword in the vocabulary.\n3\nD. Shi\nTraining of neural network language models is usually achieved by maximizing the pe-\nnalized log-likelihood of the training data:\nL= 1\nT\nT∑\nt=1\nlog(P(wt|wt−1\n1 ; θ)) + R(θ)\nwhere, θ is the set of model’s parameters to be trained, R(θ) is a regularization term.\nThe recommended learning algorithm for neural network language models is stochastic\ngradient descent (SGD) method using backpropagation (BP) algorithm. A common choice\nfor the loss function is the cross entroy loss which equals to negative log-likelihood here.\nThe parameters are usually updated as:\nθ= θ+ α∂L\n∂θ −βθ\nwhere, α is learning rate and β is regularization parameter.\nThe performance of neural network language models is usually measured using perplexity\n(PPL) which can be deﬁned as:\nPPL =\nT\n√\nT∏\ni=1\n1\nP(wi|wi−1\n1 ) = 2 −1\nT\n∑T\ni=1 log2P(wi|wi−1\n1 )\nPerplexity can be deﬁned as the exponential of the average number of bits required to encode\nthe test data using a language model and lower perplexity indicates that the language model\nis closer to the true model which generates the test data.\n2.2 Recurrent Neural Network Language Model, RNNLM\nThe idea of applying RNN in LM was proposed much earlier (Bengio et al., 2003; Castro and\nPrat, 2003), but the ﬁrst serious attempt to build a RNNLM was made by Mikolov et al.\n(2010, 2011). RNNs are fundamentally diﬀerent from feed-forward architectures in the sense\nthat they operate on not only an input space but also an internal state space, and the state\nspace enables the representation of sequentially extended dependencies. Therefore, arbitrary\nlength of word sequence can be dealt with using RNNLM, and all previous context can be\ntaken into account when predicting next word. As showed in Figure 2, the representation\nof words in RNNLM is the same as that of FNNLM, but the input of RNN at every step\nis the feature vector of a direct previous word instead of the concatenation of the n−1\nprevious words’ feature vectors and all other previous words are taken into account by the\ninternal state of previous step. At step t, RNN can be described as:\nst = f(U ·xt + W ·st−1 + b),\nyt = V ·st + M ·xt + d\nwhere, weight matrix W ∈Rnh×nh, and the input layer’s size of RNN ni = m. The outputs\nof RNN are also unnormalized probabilities and should be regularized using a softmax layer.\nBecause of the involvement of previous internal state at every step, back-propagation\nthrough time (BPTT) algorithm (Rumelhart et al., 1986) is preferred for better performance\n4\nA Study on Neural Network Language Modeling\nFigure 2: Recurrent neural network language model\nwhen training RNNLMs. If data set is treated as a single long word sequence, truncated\nBPTT should be used and back-propagating error gradient through 5 steps is enough, at\nleast for small corpus (Mikolov, 2012). In this paper, neural network language models will all\nbe trained on data set sentence by sentence, and the error gradient will be back-propagated\ntrough every whole sentence without any truncation.\n2.3 Long Short Term Memory RNNLM, LSTM-RNNLM\nAlthough RNNLM can take all predecessor words into account when predicting next word in\na word sequence, but it is quite diﬃcult to be trained over long term dependencies because\nof the vanishing or exploring problem (Hochreiter and Schmidhuber, 1997). LSTM-RNN\nwas designed aiming at solving this problem, and better performance can be expected by\nreplacing RNN with LSTM-RNN. LSTM-RNNLM was ﬁrst proposed by Sundermeyer et al.\n(2012), and the whole architecture is almost the same as RNNLM except the part of neural\nnetwork. LSTM-RNN was proposed by Hochreiter and Schmidhuber (1997) and was reﬁned\nand popularized in following works (Gers and Schmidhuber, 2000; Cho et al., 2014b). The\ngeneral architecture of LSTM-RNN is:\nit = σ(U i ·xt + W i ·st−1 + V i ·ct−1 + bi)\nf t = σ(U f ·xt + W f ·st−1 + V f ·ct−1 + bf)\ngt = f(U ·xt + W ·st−1 + V ·ct−1 + b)\nct = f t ∗ct−1 + it ∗gt\not = σ(U o ·xt + W o ·st−1 + V o ·ct + bo)\nst = ot ∗f(ct)\nyt = V ·st + M ·xt + d\nWhere, it, f t, ot ∈Rnh are input gate, forget gate and output gate, respectively. ct ∈Rnh\nis the internal memory of unit. U i, U f, U o, U ∈Rnh×ni, W i, W f, W o, W ∈Rnh×nh,\nV i, V f, V o, V ∈Rnh×nh are all weight matrixes. bi, bf, bo, b ∈Rnh, and d ∈Rno\n5\nD. Shi\nare vectors for bias terms. f(·) is the activation function in hidden layer and σ(·) is the\nactivation function for gates.\n2.4 Comparison of Neural Network Language Models\nComparisons among neural network language models with diﬀerent architectures have al-\nready been made on both small and large corpus (Mikolov, 2012; Sundermeyer et al., 2013).\nThe results show that, generally, RNNLMs outperform FNNLMs and the best performance\nis achieved using LSTM-NNLMs. However, the neural network language models used in\nthese comparisons are optimized using various techniques, and even combined with other\nkind of language models, let alone the diﬀerent experimental setups and implementation\ndetails, which make the comparison results fail to illustrate the fundamental discrepancy in\nthe performance of neural network language models with diﬀerent architecture and cannot\nbe taken as baseline for the studies in this paper.\nComparative experiments on neural network language models with diﬀerent architecture\nwere repeated here. The models in these experiments were all implemented plainly, and\nonly a class-based speed-up technique was used which will be introduced later. Experiments\nwere performed on the Brown Corpus, and the experimental setup for Brown corpus is the\nsame as that in (Bengio et al., 2003), the ﬁrst 800000 words (ca01 ∼cj54) were used for\ntraining, the following 200000 words (cj55 ∼cm06) for validation and the rest (cn01 ∼cr09)\nfor test.\nModels n m nh Direct Bias PPL\nFNNLM 5 100 200 No No 223.85\nRNNLM - 100 200 No No 221.10\nLSTM-RNNLM - 100 200 No No 237.93\nLSTM-RNNLM - 100 200 Yes No 242.54\nLSTM-RNNLM - 100 200 No Yes 237.18\nTable 1: Camparative results of diﬀerent neural network language models\nThe experiment results are showed in Table 1 which suggest that, on a small corpus\nlikes the Brown Corpus, RNNLM and LSTM-RNN did not show a remarkable advantage\nover FNNLM, instead a bit higher perplexity was achieved by LSTM-RNNLM. Maybe\nmore data is needed to train RNNLM and LSTM-RNNLM because longer dependencies are\ntaken into account by RNNLM and LSTM-RNNLM when predicting next word. LSTM-\nRNNLM with bias terms or direct connections was also evaluated here. When the direct\nconnections between input layer and output layer of LSTM-RNN are enabled, a slightly\nhigher perplexity but shorter training time were obtained. An explanation given for this\nphenomenon by Bengio et al. (2003) is that direct connections provide a bit more capacity\nand faster learning of the ”linear” part of mapping from inputs to outputs but impose a\nnegative eﬀect on generalization. For bias terms, no signiﬁcant improvement on performance\nwas gained by adding bias terms which was also observed on RNNLM by Mikolov (2012).\nIn the rest of this paper, all studies will be performed on LSTM-RNNLM with neither\n6\nA Study on Neural Network Language Modeling\ndirect connections nor bias terms, and the result of this model in Table 1 will be used as\nthe baseline for the rest studies.\n3. Optimization Techniques\n3.1 Importance Sampling\nInspired by the contrastive divergence model (Hinton, 2002), Bengio and Senecal (2003b)\nproposed a sampling-based method to speed up the training of neural network language\nmodels. In order to apply this method, the outputs of neural network should be normalized\nin following way instead of using a softmax function:\nP(vi|wt−1\n1 ) = e−y(vi,wt−1\n1 )\n∑k\nj=1 e−y(vj,wt−1\n1 ) , i= 1,2,...,k ; t= 1,2,...,T\nthen, neural network language models can be treated as a special case of energy-based\nprobability models.\nThe main idea of sampling based method is to approximate the average of log-likelihood\ngradient with respect to the parameters θ by samples rather than computing the gradient\nexplicitly. The log-likelihood gradient for the parameters set θcan be generally represented\nas the sum of two parts: positive reinforcement for target word wt and negative reinforce-\nment for all word vi, weighted by P(vi|wt−1\n1 ):\n∂logP(wt|wt−1\n1 )\n∂θ = −∂y(wt,wt−1\n1 )\n∂θ +\nk∑\ni=1\nP(vi|wt−1\n1 )∂y(vi,wt−1\n1 )\n∂θ\nThree sampling approximation algorithms were presented by Bengio and Senecal (2003b):\nMonte-Carlo Algorithm, Independent Metropolis-Hastings Algorithm and Importance Sam-\npling Algorithm. However, only importance sampling worked well with neural network lan-\nguage model. In fact, 19-fold speed-up was achieved during training while no degradation of\nthe perplexities was observed on both training and test data (Bengio and Senecal, 2003b).\nImportance sampling is a Monte-Carlo scheme using an existing proposal distribution,\nand its estimator can be represented as:\nE[\n∑\nx∼P\nP(x)g(x)] = 1\nN\n∑\nx′∈Γ\ng(x\n′\n)P(x\n′\n)\nQ(x\n′\n)\nwhere, Q is an existing proposal distribution, N is the number of samples from Q, Γ is\nthe set of samples from Q. Appling importance sampling to the average log-likelihood\ngradient of negative samples and the denominator of P(vi|wt−1\n1 ), then the overall estimator\nfor example (wt,wt−1\n1 ) using N samples from distribution Q is:\nE[∂logP(wt|wt−1\n1 )\n∂θ ] = −∂y(wt,wt−1\n1 )\n∂θ +\n∑\nw′∈Γ\n∂y(w\n′\n|wt−1\n1 )\n∂θ e−y(w\n′\n,wt−1\n1 )/Q(w\n′\n|wt−1\n1 )\n∑\nw′∈Γ e−y(w′,wt−1\n1 )/Q(w\n′\n|wt−1\n1 )\n7\nD. Shi\nFigure 3: Architecture of class based LSTM-RNNLM\nIn order to avoid divergence, the sample sizeN should be increased as training processes\nwhich is measured by the eﬀective sample size of importance sampling:\nS =\n(∑N\nj=1 rj)2\n∑N\nj=1 r2\nj\n, rj ≈\ne−y(w\n′\nj,wt−1\n1 )/Q(w\n′\nj|wt−1\n1 )\n∑\nw′∈Γ e−y(w′,wt−1\n1 )/Q(w\n′\n|wt−1\n1 )\nAt every iteration, sampling is done block by block with a constant size until the eﬀective\nsample size S becomes greater than a minimum value, and a full back-propagation will be\nperformed when the sampling size N is greater than a certain threshold.\nThe introduction of importance sampling is just posted here for completeness and no\nfurther studies will be performed on it. Because a quick statistical language model which\nis well trained, like n-gram based language model, is needed to implement importance\nsampling. In addition, it cannot be applied into RNNLM or LSTM-RNNLM directly and\nother simpler and more eﬃcient speed-up techniques have been proposed now.\n3.2 Word Classes\nBefore the idea of word classes was introduced to NNLM, it had been used in LM extensively\nfor improving perplexities or increasing speed (Brown et al., 1992; Goodman, 2001b). With\nword classes, every word in vocabulary is assigned to a unique class, and the conditional\nprobability of a word given its history can be decomposed into the probability of the word’s\nclass given its history and the probability of the word given its class and history, this is:\nP(wt|wt−1\n1 ) = P(wt|c(wt),wt−1\n1 )P(c(wt)|wt−1\n1 )\nwhere c(wt) is the class of word wt. The architecture of class based LSTM-RNNLM is\nillustrated in Figure 3, andp, qare the lower and upper index of words in a class respectively.\nMorin and Bengio (2005) extended word classes to a hierarchical binary clustering of\nwords and built a hierarchical neural network language model. In hierarchical neural net-\nwork language model, instead of assigning every word in vocabulary with a unique class,\na hierarchical binary tree of words is built according to the word similarity information\nextracted from WordNet (Fellbaum, 1998), and every word in vocabulary is assigned with a\n8\nA Study on Neural Network Language Modeling\nbit vector b= [b1(vi),b2(vi),...,b l(vi)], i= 1,2,...,k . When b1(vi),b2(vi),...,b j−1(vi) are\ngiven, bj(vi) = 0,j = 1,2,...,l indicates that word vi belongs to the sub-group 0 of current\nnode and bj(vi) = 1 indicates it belongs to the other one. The conditional probability of\nevery word is represented as:\nP(vi|wt−1\n1 ) =\nl∏\nj=1\nP(bj(wt)|b1(wt),...,b j−1(wt),wt−1\n1 )\nTheoretically, an exponential speed-up, on the order of k/log2k, can be achieved with\nthis hierarchical architecture. In Morin and Bengio (2005), impressive speed-up during both\ntraining and test, which were less than the theoretical one, were obtained but an obvious\nincrease in PPL was also observed. One possible explanation for this phenomenon is that\nthe introduction of hierarchical architecture or word classes impose negative inﬂuence on\nthe word classiﬁcation by neural network language models. As is well known, a distribution\nrepresentation for words, which can be used to represent the similarities between words, is\nformed by neural network language models during training. When words are clustered into\nclasses, the similarities between words from diﬀerent classes cannot be recognized directly.\nFor a hierarchical clustering of words, words are clustered more ﬁnely which might lead to\nworse performance, i.e., higher perplexity, and deeper the hierarchical architecture is, worse\nthe performance would be.\nTo explore this point further, hierarchical LSTM-NNLMs with diﬀerent number of hi-\nerarchical layers were built. In these hierarchical LSTM-NNLMs, words were clustered\nrandomly and uniformly instead of according to any word similarity information. The re-\nsults of experiment on these models are showed in Table 2 which strongly support the\nabove hypothesis. When words are clustered into hierarchical word classes, the speed of\nboth training and test increase, but the eﬀect of speed-up decreases and the performance\ndeclines dramatically as the number of hierarchical layers increases. Lower perplexity can be\nexpected if some similarity information of words is used when clustering words into classes.\nHowever, because of the ambiguity of words, the degradation of performance is unavoidable\nby assigning every word with a unique class or path. On the other hand, the similarities\namong words recognized by neural network is hard to deﬁned, but it is sure that they are\nnot conﬁned to linguistical ones.\nModel m nh Method l PPL Words/s\nTrain Test\nLSTM-NNLM 100 200 Uniform 1 227.51 607.09 2798.97\nLSTM-NNLM 100 200 Uniform 3 312.82 683.04 3704.28\nLSTM-NNLM 100 200 Uniform 5 438.58 694.43 3520.27\nLSTM-NNLM 100 200 Freq 1 248.99 600.56 2507.97\nLSTM-NNLM 100 200 Sqrt-Freq 1 237.93 650.16 3057.27\nTable 2: Results for class-based models\nThere is a simpler way to speed up neural network language models using word classes\nwhich was proposed by Mikolov et al. (2011). Words in vocabulary are arranged in descent\n9\nD. Shi\norder according to their frequencies in training data set, and are assigned to classes one by\none using following rule:\ni\nr <\nz∑\nj=1\nfj\nF ≤i+ 1\nr , 0 ≤i≤r−1\nwhere, r is the target number of word classes, fj is the frequency of the j-th word in\nvocabulary, the sum of all words’ frequencies F = ∑k\nj=1 fj. If the above rule is satisﬁed,\nthe z-th word in vocabulary will be assigned to i-th class. In this way, the word classes\nare not uniform, and the ﬁrst classes hold less words with high frequency and the last ones\ncontain more low-frequency words. This strategy was further optimized by (Mikolov, 2012)\nusing following criterion:\ni\nr <\nz∑\nj=1\n√\nfj/F\ndF ≤i+ 1\nr\nwhere, the sum of all words’ sqrt frequencies dF = ∑k\nj=1\n√\nfj/F.\nThe experiment results (Table 2) indicate that higher perplexity and a little more train-\ning time were obtained when the words in vocabulary were classiﬁed according to their\nfrequencies than classiﬁed randomly and uniformly. When words are clustered into word\nclassed using their frequency, words with high frequency, which contribute more to ﬁnal\nperplexity, are clustered into very small word classes, and this leads to higher perplexity.\nOn the other hand, word classes consist of words with low frequency are much bigger which\ncauses more training time. However, as the experiment results show, both perplexity and\ntraining time were improved when words were classiﬁed according to their sqrt frequency,\nbecause word classes were more uniform when built in this way. All other models in this\npaper were speeded up using word classes, and words were clustered according to their sqrt\nfrequencies.\n3.3 Caching\nLike word classes, caching is also a common used optimization technique in LM. The cache\nlanguage models are based on the assumption that the word in recent history are more\nlikely to appear again. In cache language model, the conditional probability of a word\nis calculated by interpolating the output of standard language model and the probability\nevaluated by caching, like:\nP(wt|wt−1\n0 ) = λPo(wt|wt−1\n0 ) + (1 −λ)Pc(wt|wt−1\n0 )\nwhere, Po(wt|wt−1\n0 ) is the output of standard language model, Pc(wt|wt−1\n0 ) is the probability\nevaluated using caching, and λ is a constant, 0 ≤λ≤1.\nSoutner et al. (2012) combined FNNLM with cache model to enhance the performance\nof FNNLM in speech recognition, and the cache model was formed based on the previous\ncontext as following:\nPc(wt|wt−1\nt−N) = 1\nN\nN∑\nj=1\nρδ(wt,wt−j)\n10\nA Study on Neural Network Language Modeling\nwhere, δ(·) means Kronecker delta, N is the cache length, i.e., the number of previous words\ntaken as cache, ρis a coeﬃcient depends on j which is the distance between previous word\nand target word. A cache model with forgetting can be obtained by lowering ρ linearly\nor exponentially respecting to j. A class cache model was also proposed by Soutner et al.\n(2012) for the case in which words are clustered into word classes. In class cache model, the\nprobability of target word given the last recent word classes is determined. However, both\nword based cache model and class one can be deﬁned as a kind of unigram language model\nbuilt from previous context, and this caching technique is an approach to combine neural\nnetwork language model with a unigram model.\nAnother type of caching has been proposed as a speed-up technique for RNNLMs (Bengio\net al., 2001; Kombrink et al., 2011; Si et al., 2013; Huang et al., 2014). The main idea of this\napproach is to store the outputs and states of language models for future prediction given\nthe same contextual history. In Huang et al. (2014), four caches were proposed, and they\nwere all achieved by hash lookup tables to store key and value pairs: probabilityP(wt|wt−1\n0 )\nand word sequence wt\n0; history wt−1\n0 and its corresponding hidden state vector; history wt−1\n0\nand the denominator of the softmax function for classes; history wt−1\n0 , class index c(wt) and\nthe denominator of the softmax function for words. In Huang et al. (2014), around 50-fold\nspeed-up was reported with this caching technique in speech recognition but, unfortunately,\nit only works for prediction and cannot be applied during training.\nInspired by the ﬁrst caching technique, if the previous context can be taken into account\nthrough the internal states of RNN, the perplexity is expected to decrease. In this paper,\nall language models are trained sentence by sentence, and the initial states of RNN are\ninitialized using a constant vector. This caching technique can be implemented by simply\ninitializing the initial states using the last states of direct previous sentence in the same\narticle. However, the experiment result (Table 3) shows this caching technique did not work\nas excepted and the perplexity even increased slightly. Maybe, the Brown Corpus is too\nsmall and more data is needed to evaluated this caching technique, as more context is taken\ninto account with this caching technique.\nModels m nh Caching PPL\nLSTM-NNLM 100 200 No 237.93\nLSTM-NNLM 100 200 Yes 241.45\nTable 3: Results of Cached LSTM-RNNLM\n3.4 Bidirectional Recurrent Neural Network\nIn Sutskever et al. (2014), signiﬁcant improvement on neural machine translation (NMT)\nfor an English to French translation task was achieved by reversing the order of input word\nsequence, and the possible explanation given for this phenomenon was that smaller ”minimal\ntime lag” was obtained in this way. In my opinion, another possible explanation is that a\nword in word sequence may more statistically depend on the following context than previous\none. After all, a number of words are determined by its following words instead of previous\nones in some natural languages. Take the articles in English as examples, indeﬁnite article\n11\nD. Shi\nFigure 4: Encode word sequence using bidirectional recurrent neural network\n”an” is used when the ﬁrst syllable of next word is a vowel while ”a” is preposed before\nwords starting with consonant. What’s more, if a noun is qualiﬁed by an attributive clause,\ndeﬁnite article ”the” should be used before the noun. These examples illustrate that words\nin a word sequence depends on their following words sometimes. To verify this hypothesis\nfurther, an experiment is performed here in which the word order of every input sentence\nis reversed, and the probability of word sequence w1w2...w T is evaluated as following:\nP(wT\n1 ) =\nT∏\nt=1\nP(wt|wT\nt+1)\nHowever, the experiment result (Table 4) shows that almost the same perplexity was\nachieved by reversing the order of words. This indicates that the same amount statistical\ninformation, but not exactly the same statistical information, for a word in a word sequence\ncan be obtained from its following context as from its previous context, at least for English.\nModel m nh Reverse PPL\nLSTM-NNLM 100 200 No 237.93\nLSTM-NNLM 100 200 Yes 240.48\nTable 4: Reverse the order of word sequence\nAs a word in word sequence statistically depends on its both previous and following\ncontext, it is better to predict a word using context from its both side. Bidirectional recur-\nrent neural network (BiRNN) (Schuster and Paliwal, 1997) was designed to process data\nin both directions with two separate hidden layers, so better performance can be expected\nby using BiRNN. BiRNN was introduced to speech recognition by Graves et al. (2013b),\nand then was evaluated in other NLP tasks, like NMT (Bahdanau et al., 2015; Wu et al.,\n2016). In these studies, BiRNN showed more excellent performance than unidirectional\n12\nA Study on Neural Network Language Modeling\nFigure 5: A possible scheme for the architecture of ANN\nRNN. Nevertheless, BiRNN cannot be evaluated in LM directly as unidirectional RNN,\nbecause statistical language modeling is based on the chain rule which assumes that word\nin a word sequence only statistically depends on one side context. BiRNN can be applied\nin NLP tasks, like speech recognition and machine translation, because the input word se-\nquences in these tasks are treated as a whole and usually encoded as a single vector. The\narchitecture for encoding input word sequences using BiRNN is showed in Figure 4. The\nfacts that better performance can be achieved using BiRNN in speech recognition or ma-\nchine translation indicate that a word in a word sequence is statistically determined by the\nwords of its both side, and it is not a suitable way to deal with word sequence in a natural\nlanguage word by word in an order.\n4. Limits of Neural Network Language Modeling\nNNLM is state of the art, and has been introduced as a promising approach to various NLP\ntasks. Numerous researchers from diﬀerent areas of NLP attempt to improve NNLM, ex-\npecting to get better performance in their areas, like lower perplexity on test data, less word\nerror rate (WER) in speech recognition, higher Bilingual Evaluation Understudy (BLEU)\nscore in machine translation and etc. However, few of them spares attention on the limits\nof NNLM. Without a thorough understanding of NNLM’s limits, the applicable scope of\nNNLM and directions for improving NNLM in diﬀerent NLP tasks cannot be deﬁned clearly.\nIn this section, the limits of NNLM will be studied from two aspects: model architecture\nand knowledge representation.\n4.1 Model Architecture\nIn most language models including neural network language models, words are predicated\none by one according to their previous context or following one which is believed to simulate\nthe way human deal with natural languages, and, according to common sense, human\nactually speak or write word by word in a certain order. However, the intrinsic mechanism\nin human mind of processing natural languages cannot like this way. As mentioned above, it\nis not always true that words in a word sequence only depend on their previous or following\ncontext. In fact, before human speaking or writing, they know what they want to express\nand map their ideas into word sequence, and the word sequence is already cached in memory\n13\nD. Shi\nwhen human speaking or writing. In most case, the cached word sequence may be not a\ncomplete sentence but at least most part of it. On the other hand, for reading or listening,\nit is better to know both side context of a word when predicting the meaning of the word\nor deﬁne the grammar properties of the word. Therefore, it is not a good strategy to deal\nwith word sequences in a natural language word by word in a certain order which has also\nbeen questioned by the success application of BiRNN in some NLP tasks.\nAnother limit of NNLM caused by model architecture is original from the monotonous\narchitecture of ANN. In ANN, models are trained by updating weight matrixes and vectors\nwhich distribute among all nodes. Training will become much more diﬃcult or even un-\nfeasible when increasing the size of model or the variety of connections among nodes, but\nit is a much eﬃcient way to enhance the performance of ANN. As is well known, ANN is\ndesigned by imitating biological neural system, but biological neural system does not share\nthe same limit with ANN. In fact, the strong power of biological neural system is original\nfrom the enormous number of neurons and various connections among neurons, including\ngathering, scattering, lateral and recurrent connections (Nicholls et al., 2011). In biological\nneural system, the features of signals are detected by diﬀerent receptors, and encoded by\nlow-level central neural system (CNS) which is changeless. The encoded signals are inte-\ngrated by high-level CNS. Inspired by this, an improvement scheme for the architecture of\nANN is proposed, as illustrated in Figure 5. The features of signal are extracted accord-\ning to the knowledge in certain ﬁeld, and every feature is encoded using changeless neural\nnetwork with careful designed structure. Then, the encoded features are integrated using a\ntrainable neural network which may share the same architecture as existing ones. Because\nthe model for encoding does not need to be trained, the size of this model can be much\nhuge and the structure can be very complexity. If all the parameters of encoding model\nare designed using binary, it is possible to implement this model using hardware and higher\neﬃciency can be expected.\nModel m nh\nTraining Data PPL\nElectronics Books Electronics Books\nLSTM-NNLM 100 200 Yes No 146.62 353.16\nLSTM-NNLM 100 200 No Yes 280.91 246.35\nLSTM-NNLM 100 200 Yes Yes 147.28 302.92\nTable 5: Evaluating NNLM on data sets from diﬀerent ﬁelds\n4.2 Knowledge Representation\nThe word ”learn” appears frequently with NNLM, but what neural network language models\nlearn from training data set is rarely analyzed carefully. The common statement about the\nknowledge learned by neural network language models is the probabilistic distribution of\nword sequences in a natural language. Strictly speaking, it is the probabilistic distribution\nof word sequences from a certain training data set in a natural language, rather than the\ngeneral one. Hence, the neural network language model trained on data set from a certain\nﬁeld will perform well on data set from the same ﬁeld, and neural network language model\n14\nA Study on Neural Network Language Modeling\ntrained on a general data set may show worse performance when tested on data set from a\nspecial ﬁeld. In order to verify this, one million words reviews on electronics and books were\nextracted from Amazon reviews (He and J.Mcauley, 2016; Mcauley et al., 2015) respectively\nas data sets from diﬀerent ﬁelds, and 800000 words for training, 100000 words for validation,\nand the rest for test. In this experiment, two models were trained on training data from\nelectronics reviews and books reviews respectively, and the other one was trained on both.\nThen, all three models were tested on the two test data sets.\nThe lowest perplexity on each test data set was gained by the model trained on corre-\nsponding training data set, instead of the model trained on both training data set (Table\n6). The results show that the knowledge represented by a neural network language model\nis the probabilistic distribution of word sequences from training data set which varies from\nﬁeld to ﬁeld. Except for the probabilistic distribution of word sequences, the feature vec-\ntors of words in vocabulary are also formed by neural network during training. Because\nof the classiﬁcation function of neural network, the similarities between words can be ob-\nserved using these feature vectors. However, the similarities between words are evaluated\nin a multiple dimensional space by feature vectors and it is hard to know which features of\nwords are taken into account when these vectors are formed, which means words cannot be\ngrouped according to any single feature by the feature vectors. In summary, the knowledge\nrepresented by neural network language model is the probabilistic distribution of word se-\nquences from certain training data set and feature vectors for words in vocabulary formed\nin multiple dimensional space. Neither the knowledge of language itself, like grammar, nor\nthe knowledge conveyed by a language can be gained from neural network language mod-\nels. Therefore, NNLM can be a good choice for NLP tasks in some special ﬁelds where\nlanguage understanding is not necessary. Language understanding cannot be achieved just\nwith the probabilistic distribution of word sequences in a natural language, and new kind\nof knowledge representation should be raised for language understanding.\nSince the training of neural network language model is really expensive, it is important\nfor a well-trained neural network language model to keep learning during test or be improved\non other training data set separately. However, the neural network language models built\nso far do not show this capacity. Lower perplexity can be obtained when the parameters\nof a trained neural network language model are tuned dynamically during test, as showed\nin Table 5, but this does not mean neural network language model can learn dynamically\nduring test. ANN is just a numerical approximation method in nature, and it approximate\nthe target function, the probabilistic distribution of word sequences for LM, by tuning\nparameters when trained on data set. The learned knowledge is saved as weight matrixes\nand vectors. When a trained neural network language model is expected to adaptive to new\ndata set, it should be retrained on both previous training data set and new one. This is\nanother limit of NNLM because of knowledge representation, i.e., neural network language\nmodels cannot learn dynamically from new data set.\n5. Future Work\nVarious architectures of neural network language models are described and a number of\nimprovement techniques are evaluated in this paper, but there are still something more\nshould be included, like gate recurrent unit (GRU) RNNLM, dropout strategy for address-\n15\nD. Shi\ning overﬁtting, character level neural network language model and ect. In addition, the\nexperiments in this paper are all performed on Brown Corpus which is a small corpus, and\ndiﬀerent results may be obtained when the size of corpus becomes larger. Therefore, all the\nexperiments in this paper should be repeated on a much larger corpus.\nSeveral limits of NNLM has been explored, and, in order to achieve language under-\nstanding, these limits must be overcome. I have not come up with a complete solution\nyet but some ideas which will be explored further next. First, the architecture showed in\nFigure 5 can be used as a general improvement scheme for ANN, and I will try to ﬁgure\nout the structure of changeless neural network for encoder. What’s more, word sequences\nare commonly taken as signals for LM, and it is easy to take linguistical properties of\nwords or sentences as the features of signals. However, it maybe not a proper way to deal\nwith natural languages. Natural languages are not natural but man-made, and linguistical\nknowledge are also created by human long after natural language appeared. Liguistical\nknowledge only covers the ”right” word sequences in a natural language, but it is common\nto deal with ”wrong” ones in real world. In nature, every natural language is a mechanism\nof linking voices or signs with objects, both concrete and abstract. Therefore, the proper\nway to deal with natural languages is to ﬁnd the relations between special voices or signs\nand objects, and the features of voices or signs can be deﬁned easier than a natural language\nitself. Every voice or sign can be encoded as a unique code, vector or matrix, according to\nits features, and the similarities among voices or signs are indeed can be recognized from\ntheir codes. It is really diﬃcult to model the relation between voices or signs and objects\nat once, and this work should be split into several steps. The ﬁrst step is to covert voice or\nsign into characters, i.e., speech recognition or image recognition, but it is achieved using\nthe architecture described in Figure 5.\n6. Conclusion\nIn this paper, diﬀerent architectures of neural network language models were described,\nand the results of comparative experiment suggest RNNLM and LSTM-RNNLM do not\nshow any advantages over FNNLM on small corpus. The improvements over these models,\nincluding importance sampling, word classes, caching and BiRNN, were also introduced and\nevaluated separately, and some interesting ﬁndings were proposed which can help us have\na better understanding of NNLM.\nAnother signiﬁcant contribution in this paper is the exploration on the limits of NNLM\nfrom the aspects of model architecture and knowledge representation. Although state of\nthe art performance has been achieved using NNLM in various NLP tasks, the power of\nNNLM has been exaggerated all the time. The main idea of NNLM is to approximate the\nModel m nh Dynamic PPL\nLSTM-NNLM 100 200 No 237.93\nLSTM-NNLM 100 200 Yes 174.57\nTable 6: Examine the learning ability of neural network\n16\nA Study on Neural Network Language Modeling\nprobabilistic distribution of word sequences in a natural language using ANN. NNLM can\nbe successfully applied in some NLP tasks where the goal is to map input sequences into\noutput sequences, like speech recognition, machine translation, tagging and ect. However,\nlanguage understanding is another story. For language understanding, word sequences must\nbe linked with any concrete or abstract objects in real world which cannot be achieved just\nwith this probabilistic distribution.\nAll nodes of neural network in a neural network language model have parameters needed\nto be tunning during training, so the training of the model will become very diﬃcult or\neven impossible if the model’s size is too large. However, an eﬃcient way to enhance the\nperformance of a neural network language model is to increase the size of model. One\npossible way to address this problem is to implement special functions, like encoding, using\nchangeless neural network with special struture. Not only the size of the changeless neural\nnetwork can be very large, but also the structure can be very complexity. The performance\nof NNLM, both perplexity and training time, is expected to be improved dramatically in\nthis way.\n17\nD. Shi\nReferences\nD. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to\nalign and translate. In International Conference on Learning Representations, 2015.\nY. Bengio and J. S. Senecal. Quick training of probabilistic neural nets by importance\nsampling. In AISTATS, 2003b.\nY. Bengio, R. Ducharme, and P. Vincent. A neural probabilistic language model. In\nAdvances in Neural Information Processing Systems, volume 13, pages 933–938, 2001.\nY. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model.\nJournal of Machine Learning Research, 3(Feb):1137–1155, 2003.\nP. F. Brown, V. J. D. Pietra, P. V. DeSouza, J. C. Lai, and R. L. Mercer. Class-based\nn-gram models of natural language. Computational Linguistics, 18:467–479, 1992.\nM. J. Castro and F. Prat. New directions in connectionist language modeling. In Lecture\nNotes in Computer Science, pages 1137–1155, 2003.\nK. Cho, B. V. Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and\nY. Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. Computer Science, 15(4):403–434, 2014a.\nK. Cho, B. M. Van, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Ben-\ngio. Learning phrase representations using rnn encoder-decoder for statistical machine\ntranslation. arXiv preprint arXiv:1406.1078, 2014b.\nR. Collobert and J. Weston. Fast semantic extraction using a novel neural network archi-\ntecture. In Proceedings of the Meeting of the Association for Computational Linguistics,\nvolume 45, pages 560–567, 2007.\nR. Collobert and J. Weston. A uniﬁed architecture for natural language processing - deep\nneural networks with multitask learning. In Proceedings of the 25th International Con-\nference on Machine Learning, pages 160–167, 2008.\nC. Fellbaum. WordNet: An Electronic Lexical Database. MIT Press, 1998.\nF. A. Gers and J. Schmidhuber. Recurrent nets that time and count. In Proceedings of the\nIEEE-INNS-ENNS International Joint Conference on, volume 3, pages 189–194, 2000.\nJ. Goodman. Classes for fast maximum entropy training. In International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), volume 1, pages 561–564, 2001b.\nJ. D. Goodman. A bit of progress in language modeling. Computer Speech and Langauge,\n15(4):403–434, 2001a.\nA. Graves, A. r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neu-\nral networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 6645–6649, 2013a.\n18\nA Study on Neural Network Language Modeling\nA. Graves, N. Jaitly, and A. R. Mohamed. Hybrid speech recognition with deep bidirec-\ntional lstm. In Automatic Speech Recognition and Understanding (ASRU), pages 273–278,\n2013b.\nR. He and J.Mcauley. Ups and downs: modeling the visual evolution of fashion trends with\none-class collaborative ﬁltering. In International World Wide Web Conferences Steering\nCommittee, 2016.\nG. Hinton. Training products of experts by minimizing contrastive divergence. Neural\nComputation, 14(8):1771–1800, 2002.\nG. Hinton, L. Deng, D. Yu, G. E. Dahl, A. r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke,\nP. Nguyen, T. N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling\nin speech recognition: The shared views of four research groups. IEEE Signal Processing\nMagazine, 29(6):82–97, 2012.\nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):\n1735–1780, 1997.\nZ. Huang, G. Zweig, and B. Dumoulin. Cache based recurrent neural network language\nmodel inference for ﬁrst pass speech recognition. In Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2014 IEEE International Conference on, pages 6354–6358, 2014.\nR. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu. Exploring the limits of\nlanguage modeling. Neural Computation, 3(2):246–257, 2016.\nS. Kombrink, T. Mikolov, M. Karaﬁat, and L. Burget. Recurrent neural network based\nlanguage modeling in meeting recognization. In INTERSPEECH, 2011.\nJ. Mcauley, C. Targett, Q. Shi, and A. V. D. Hengel. Image-based recommendations on styles\nand substitutes. In International ACM SIGIR Conference on Research and Development\nin Information Retrieval, 2015.\nR. Miikkulainen and M. G. Dyer. Natural language processing with modular pdp neural\nnetworks and distributed lexicon. Cognitive Science, 15(3):343–399, 1991.\nT. Mikolov. Statistical language models based on neural networks. PhD thesis, Brno Uni-\nversity of Technology, 2012.\nT. Mikolov, M. Karaﬁat, L. Burget, J. H. Cernocky, and S. Khudanpur. Recurrent neural\nnetwork based language model. In Interspeech, volume 2, pages 1045–1048, 2010.\nT. Mikolov, S. Kombrink, J. H. Cernocky, and S. Khudanpur. Extensions of recurrent\nneural network based language model. In IEEE International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP), pages 5528–5531, 2011.\nF. Morin and Y. Bengio. Hierarchical probabilistic neural network language model. In\nAistats, volume 5, pages 246–252, 2005.\nJ. G. Nicholls, A. R. Martin, P. A. Brown, M. E. Diamond, and D. A. Weisblat. From\nneuron to brain. Sinauer Associates, Inc, 5th edition, 2011.\n19\nD. Shi\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by\nback-propagating errors. Nature, 323:533–536, 1986.\nJ. Schmidhuber. Sequential neural neural text compression. IEEE Transactions on Neural\nNetwork, 7(1):142–146, 1996.\nM. Schuster and K. K. Paliwal. Bidirectional recurrent neural networks. Signal Processing,\nIEEE Transactions on, 45(11):2673–2681, 1997.\nY. Si, Q. Zhang, T. Li, J. Pan, and Y. Yan. Preﬁx tree based n-best list rescoring for\nrecurrent neural network language model used in speech recognition system. In INTER-\nSPEECH, 2013.\nD. Soutner, Z. Loose, L. Muller, and A. Prazak. Neural network language model with cache.\nIn International Conference on Text, Speech and Dialogue, pages 528–534, 2012.\nM. Sundermeyer, R. Schluter, and H. Ney. Lstm nerual networks for language modeling.\nIn Interspeech, pages 194–197, 2012.\nM. Sundermeyer, I. Oparin, J. L. Gauvain, B. Freiberg, R. Schluter, and H. Ney. Comparison\nof feedforward and recurrent nerual network language models. In Acoustics, Speech and\nSignal Processing (ICASSP), 2013 IEEE International Conference on, pages 8430–8434,\n2013.\nI. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence learning with neural networks. In\nAdvances in Neural Information Processing Systems, volume 4, pages 3104–3112, 2014.\nY. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao,\nQ. Gao, and K. Macherey. Google’s neural machine translation system: bridging the gap\nbetween human and machine translation. Computer Science, 15(4):403–434, 2016.\nW. Xu and A. Rudnicky. Can artiﬁcial neural network learn language models. InProceedings\nof International Conference on Speech and Language Processing, pages 202–205, 2000.\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8044475317001343
    },
    {
      "name": "Artificial neural network",
      "score": 0.7715376615524292
    },
    {
      "name": "Language model",
      "score": 0.7174142003059387
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6443304419517517
    },
    {
      "name": "Word (group theory)",
      "score": 0.6061310172080994
    },
    {
      "name": "Cache language model",
      "score": 0.5999979376792908
    },
    {
      "name": "Natural language processing",
      "score": 0.5643801093101501
    },
    {
      "name": "Probabilistic neural network",
      "score": 0.5306728482246399
    },
    {
      "name": "Natural language",
      "score": 0.5004386901855469
    },
    {
      "name": "Nervous system network models",
      "score": 0.49672919511795044
    },
    {
      "name": "Time delay neural network",
      "score": 0.49581775069236755
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.466572642326355
    },
    {
      "name": "Language identification",
      "score": 0.4621499180793762
    },
    {
      "name": "Representation (politics)",
      "score": 0.453999787569046
    },
    {
      "name": "Universal Networking Language",
      "score": 0.2666533589363098
    },
    {
      "name": "Linguistics",
      "score": 0.09126964211463928
    },
    {
      "name": "Comprehension approach",
      "score": 0.0803515613079071
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}