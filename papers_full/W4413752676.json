{
  "title": "A comprehensive qualitative analysis of patient dialogue summarization using large language models applied to noisy, informal, non-English real-world data",
  "url": "https://openalex.org/W4413752676",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5011076740",
      "name": "Anderson A. Ferreira",
      "affiliations": [
        "Universidade Federal de Minas Gerais",
        "Universidade Federal de Ouro Preto"
      ]
    },
    {
      "id": "https://openalex.org/A5087565388",
      "name": "Leonardo Rocha",
      "affiliations": [
        "Federal University of São João del-Rei"
      ]
    },
    {
      "id": "https://openalex.org/A5018690278",
      "name": "Washington Cunha",
      "affiliations": [
        "Universidade Federal de Minas Gerais"
      ]
    },
    {
      "id": "https://openalex.org/A5103192629",
      "name": "Ana Cláudia Machado",
      "affiliations": [
        "Federal University of São João del-Rei"
      ]
    },
    {
      "id": "https://openalex.org/A5038373610",
      "name": "João L. Campos",
      "affiliations": [
        "Universidade Federal de Minas Gerais"
      ]
    },
    {
      "id": "https://openalex.org/A5093007437",
      "name": "Gabriel Jallais",
      "affiliations": [
        "Universidade Federal de Minas Gerais"
      ]
    },
    {
      "id": "https://openalex.org/A5005439793",
      "name": "Alexandre Pio Viana",
      "affiliations": [
        "Federal University of São João del-Rei"
      ]
    },
    {
      "id": "https://openalex.org/A5111325459",
      "name": "Elisa Tuler",
      "affiliations": [
        "Federal University of São João del-Rei"
      ]
    },
    {
      "id": "https://openalex.org/A5090971199",
      "name": "Iago Breno Araujo",
      "affiliations": [
        "Universidade Federal de Minas Gerais"
      ]
    },
    {
      "id": "https://openalex.org/A5015771998",
      "name": "Víctor Cussiol Macul",
      "affiliations": [
        null,
        "Insper"
      ]
    },
    {
      "id": "https://openalex.org/A5002833442",
      "name": "Osvaldo Coelho Pereira Neto",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5021846291",
      "name": "Antônio Pereira de Souza Júnior",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5103083931",
      "name": "Glauco R. Souza",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5119449806",
      "name": "Joice Marques Pallone",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5069912449",
      "name": "Margarida Soares",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5008530690",
      "name": "Welton Santos",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5112702036",
      "name": "Marcos André Gonçalves",
      "affiliations": [
        "Universidade Federal de Minas Gerais"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4392928544",
    "https://openalex.org/W2617835532",
    "https://openalex.org/W3042185737",
    "https://openalex.org/W4389083881",
    "https://openalex.org/W4409591253",
    "https://openalex.org/W4391766565",
    "https://openalex.org/W4404782882",
    "https://openalex.org/W4385227045",
    "https://openalex.org/W4368367885",
    "https://openalex.org/W4403087472",
    "https://openalex.org/W2038691557",
    "https://openalex.org/W1507711477",
    "https://openalex.org/W4320351386",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2078861931",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4362679631",
    "https://openalex.org/W4385571776",
    "https://openalex.org/W4387428151",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W4391668179",
    "https://openalex.org/W2896389463",
    "https://openalex.org/W2775328204",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4403864624"
  ],
  "abstract": null,
  "full_text": "A comprehensive qualitative \nanalysis of patient dialogue \nsummarization using large \nlanguage models applied to noisy, \ninformal, non-English real-world \ndata\nAnderson A. Ferreira1,3, Leonardo Rocha2, Washington Cunha1, Ana Cláudia Machado2, \nJoão Marcos Campos1, Gabriel Jallais1, Adriana C. F. Viana4, Elisa Tuler2, Iago Araújo1, \nVíctor Macul5,6, Olívio Souza Neto5, Antônio Pereira de Souza Júnior5,  \nGiordano de Pinho Souza5, Joice Marques Pallone5, Mariana Aparecida Dumbá Soares5, \nWelton Augusto Santos5 & Marcos André Gonçalves1\nThis study evaluates the ability of Large Language Models (LLMs) to summarize real-world dialogues \nbetween patients and the healthcare team of an e-health company that provides digital healthcare \nservices, primarily communicating via WhatsApp. The team needs quick access to patient information \nto deliver accurate and personalized responses. Summarizing past messages is the approach examined \nhere, aiming for concise, non-redundant, and truthful summaries that capture the main dialogue \ncharacteristics despite facing (real-world) noisy and informal content in an under-represented \nlanguage - Portuguese. To do so, we collected an anonymized Portuguese dataset of WhatsApp \nmessages exchanged between patients and the healthcare team. Dialogue quality was assessed for \nsize, readability, and correctness before generating summaries with LLaMA3 and Qwen2 using specific \nprompts. Volunteers evaluated these summaries on coverage, relevance, redundancy, and veracity \nusing a 5-point Likert scale. Our qualitative and quantitative experimental results indicate that LLMs \ncan produce effective summaries of dialogues between patients and healthcare teams, even when \nfaced with low-quality data in an underrepresented language. This is a surprising result due to the \nchallenging scenario. Among the tested LLMs, LLaMA3 demonstrated a slight edge over QWen2 in \ncoverage and veracity among the evaluated methods. Our results demonstrate a potential to build \nreal-world practical services to assist healthcare professionals in responding to patient messages \nwith agility, clarity, and cohesion, enhancing both communication efficiency and patient satisfaction. \nUltimately, the advocated approach could significantly improve the landscape of online healthcare \ncommunication, particularly in resource-constrained settings like Brazil, where access to primary care \nis limited.\nThe increasing use of electronic health records as well as of (instant) messaging platforms such as WhatsApp \nand Messenger has expanded the use of e-health portals to exchange messages between patients and healthcare \nteams1. The management of such an increasing volume of messages is a challenge. Indeed, primary care \nphysicians are estimated to spend around 1.5 hours daily analyzing approximately 150 patients’ messages 1. To \nrespond effectively, health professionals must understand the message’s context by reviewing previous ones from \nthe same patient, as well as any other relevant patient information available to them.\n1Computer Science Department, Universidade Federal de Minas Gerais, Belo Horizonte, Minas Gerais, Brazil. \n2Computer Science Department, Universidade Federal de São João del-Rei, São João del-Rei, Minas Gerais, Brazil. \n3Computer Science Department, Universidade Federal de Ouro Preto, Ouro Preto, Minas Gerais, Brazil. 4Psychology \nDepartment, Universidade Federal de São João del-Rei, São João del-Rei, Minas Gerais, Brazil. 5Ana Health, São \nPaulo, São Paulo, Brazil. 6Insper, São Paulo, São Paulo, Brazil. email: anderson.ferreira@ufop.edu.br\nOPEN\nScientific Reports |        (2025) 15:31660 1| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports\n\nIn Brazil, where 34% of the population lacks access to primary care, it is estimated that achieving 100% \ncoverage would require 236,900 healthcare professionals at a cost of BRL 22.9 billion/year 2. An approach to \nscaling such services would be through a Digital Primary Care model, such as that offered by Ana Health \n(https://www.anahealth.com.br/ ), an e-health company that provides digital healthcare services, primarily \ncommunicating via WhatsApp, in order to scale patients. Ana Health’s healthcare professionals primarily \ninteract with the population through a specific messaging system – WhatsApp. In this interaction model, the \nchallenge of processing and comprehending a large volume of messages remains, as it is essential to respond to \neach patient individually.\nIn more detail, at Ana Health, when a healthcare team member receives a new message from a patient, to \nadequately respond, the team member shall comprehend the full context, review previous messages exchanged \nwith the patient, and understand the current message. Only after that can the healthcare team member craft  \na response aimed at maintaining patient engagement. For the patient to receive the response efficiently and \neffectively, it is essential to quickly gather contextual information in order to provide accurate, up-to-date, and \npersonalized responses.\nAn approach to assist the healthcare team in understanding the context of patient messages, which we \nadvocate and analyze in this article, is to apply summarization techniques to prior messages exchanged between \nthe patient and the healthcare team. Such a summarization, if successful, can also support other services the \nhealthcare team offers to the patient, such as psychotherapy, chronic disease monitoring, and drug prescription.\nPrevious work has developed and evaluated summarization techniques 3–5, including the summarization of \nclinical data4, exploiting techniques ranging from rule-based approaches to the use of Large Language Models \n(LLMs)6. However, these techniques are rarely evaluated in real environments using real data 7. Our work tries \nto fulfill this literature gap. That is, our objective is to evaluate the ability of LLMs to generate useful summaries \nfor assisting a healthcare team in understanding a patient context using real and anonymized health data and to \nconduct a thorough evaluation of these summaries with the support of specialists, particularly volunteers from \na healthcare team.\nRecent advances in LLMs based on transformers have helped in several tasks related to natural language \nprocessing, including summarization6. In8, the authors state that LLMs can assist in several healthcare tasks, such \nas pre-consultation, diagnosis, and management, with proper development and supervision. Despite that, some \nconcerns and challenges still remain in the use of LLMs in healthcare, including8–10: (1) data privacy - ensuring \nthat real patient data will not be leaked and used by the owners of the LLMs for other purposes; (2) credibility \nand correctness of information - LLMs can perpetuate incorrect information or produce “hallucination”; and (3) \nbiased data - LLMs are commonly trained with diverse data that may be biased and, consequently, may generate \nbiased responses.\nBack to the task on focus, in this article, we leverage summarization – a process of reducing a long text into a \nshorter version that preserves important information for understanding the original text5,11. We aim to evaluate \nthe LLM’s ability to summarize WhatsApp (full) dialogues between patients and a healthcare team. Ideally, such \nsummaries should capture the main characteristics of the dialogues, be concise, non-redundant, and contain \ntruthful information.\nSpecifically, we propose evaluating open-source and publicly available LLMs in the task of summarizing \npatient messages, aiding healthcare professionals with patient-oriented, concise, quicker-to-read information to \nassist in responding. Moreover, such a summarization should enable healthcare teams to scale personalized care \nmanagement by tailoring approaches to diverse patient needs, serving as a foundational step toward developing \na digital primary care AI agent. To evaluate the feasibility of such an approach, we utilize data from real \nenvironments, including real (anonymized) interactions between patients and healthcare teams, and conduct an \nevaluation of these summaries with the support of volunteers from a real healthcare team.\nA few studies have been conducted on summarizing clinical data. Particularly, in 4, the authors reviewed \npublications on data summarization in the clinical context. The summarized data include mainly patient’s \nelectronic health records. The authors also discuss the importance of such summaries for helping tasks related to \nintensive care units, surgery, diagnostics, hospital care, chronic disease monitoring, oncology, drug prescription, \npsychotherapy, etc. Our work differs from4 in two main aspects: (1) data source – we explore data coming from an \ninstant messaging system, particularly input messages from dialogues between patients and the healthcare team; \nsuch messages are usually noisy as we shall demonstrate in our analyses, potentially impacting summarization \nquality; and (2) application domain – our main purpose is to assist a healthcare team in responding to new \npatient messages, ensuring personalized, humanistic, and contextually-appropriate responses.\nIn sum, the main and novel contribution of this work is the evaluation of the ability of two large language \nmodels (LLMs), LLaMA 3 and Qwen 2, to summarize real-world WhatsApp dialogues between patients and \nhealthcare teams based on real-world, noisy, and informal non-English (Portuguese) data. The study highlights \nthe potential of LLMs to improve digital healthcare communication, particularly in resource-constrained \nsettings like Brazil, where access to primary care is limited.\nMethods\nDataset and preprocessing\nWe conducted this research at the Database Laboratory of Universidade Federal de Minas Gerais. The research \nproject, including data and participation of volunteers in qualitative evaluations, was reviewed and approved by \nthe Ethics Committee of the Federal University of Minas Gerais under registration CAAE 80632524.4.0000.5149, \nin accordance with the Brazilian Resolution CNS 446/12.\nThis study gathered WhatsApp messages from (patient, healthcare team) dialogues from October 27, 2021, \nuntil January 8, 2024, from Ana Health’s cloud-based contact center. This dataset contains 207,040 messages \nwritten in Portuguese, and is available upon request from the authors. We confirm that all experiments were \nScientific Reports |        (2025) 15:31660 2| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\nconducted in accordance with the relevant guidelines and regulations, and approved by the Ethics Committee \nof the Federal University of Minas Gerais. We also inform that consent for the use of the dataset was obtained \nfrom the legal guardian.\nAna Health is a company that provides a digital primary care service to look after people’s physical and mental \nhealth. Through a subscription plan, members (patients) gain unlimited access to Ana Health’s multidisciplinary \nteam, including general practitioners, psychologists, nurses, and gerontologists, who proactively guide them \nthrough a structured healthcare journey, complemented by 24/7 support via text messaging and phone or video \ncalls. The company aims to scale primary care through a WhatsApp-first experience driven by a personalized \ncare management system powered by artificial intelligence. Ana Health provided us with such messages in an \nanonymized way, ensuring that all identifiable information, such as names, emails, phone numbers, and URLs, \nwas removed and replaced with a unique code to safeguard data privacy.\nRegarding preprocessing, we removed special characters (tab, new lines, and duplicated white spaces) and \nfiltered out template messages – standardized Ana Health messages to its patients – resulting in 202,326 messages \nand 1,863 dialogues.\nAnalyzing message quality\nBefore summarizing the dialogues, we analyze the message’s textual quality to understand data limits and the \npotential impact of data quality on the final results. Low-quality texts can lead LLMs to generate incoherent or \ninaccurate summaries or summaries that do not contain the text’s main information. In addition, spelling or \ngrammatical errors may be interpreted by LLMs as unknown or inappropriate words or concepts. We based our \nanalysis on the methodology proposed by Hasan et al12. We analyzed three textual quality dimensions: (1) size, \n(2) readability, and (3) correctness.\nSize\nTotal number of characters, words and sentences. According to Hasan et al. 12, messages of higher quality are \nusually neither too short nor too long. We also define long messages as those exceeding the average length by \nat least ten words, and short messages as those containing five words fewer than the average message length or \nless, similarly to12.\nReadability\nWe used the Flesch-Kincaid grade level13, which generates an overall grade evaluating the complexity of a text, \naccording to Eq. 1.\n \n0.39 × total_words\ntotal_sentences + 11.8× total_syllables\ntotal_words − 15.59 (1)\nAs results, we have a score according to American education levels. The Flesch-Kincaid grade level considers \nsentences with many words or words containing many syllables more difficult to read. The Flesch-Kincaid \nGrade Level is a well-established readability metric that estimates the minimum education level required to \ncomprehend a given text. In our analysis, we did not use the Flesch-Kincaid score to assess whether the texts \nwere well-written per se. Rather, our objective was to perform a comparative analysis of the readability levels of \ntexts authored by patients versus those written by the healthcare team. Although the Flesch-Kincaid grade level \nwas originally developed to indicate the grade level required to understand a document written in English, there \nexist studies14,15 that adapt it to evaluate Portuguese texts. Those works show a high correlation between the \nvalues obtained by the original and adapted formulas. Thus, we may use the original Flesh-Kincaid grade level \nto compare documents written in the same language, as we will do in this work.\nCorrectness\nWe calculate the proportion of words in the messages that are present in a predefined Portuguese dictionary. We \nuse the Portuguese dictionary from the package br.ispell  (  h t    t p s  :  / / g i  t  h u b  . c o m /   f  t i  t n t / b r . i s p e l l - d i c i o n a r i o - p o r t u g u e \ns - b r a s i l e i r o  ) .  \nSummarizing the dialogues\nAs mentioned, our main goal in this work is to evaluate the LLMs capability in summarizing (patient, healthcare \nteam) dialogues. Ideally, such summaries should capture the main dialogue aspects and include no wrongful \ninformation. For the sake of reproducibility and transparency, we focus our study on publicly available open-\nsource LLMs that can be run locally, ensuring data privacy, as we do not send any private patient data to remote \nclosed LLMs such as GPT. In our experiments, we evaluated Qwen 2 (Qwen2-7B-Instruct) and LLaMA 3 (Meta-\nLlama-3-8B-Instruct) as LLMs to summarize dialogues. We selected LLaMA 3 and Qwen 2 based on several key \nconsiderations: \n 1. Open-Source Availability and Data Privacy: Both models are open-source, allowing for local deployment \nwithout reliance on external cloud infrastructure. This aspect is crucial for our application, as it enables the \nsystem to operate without transmitting sensitive user data over the internet, thereby mitigating potential \nrisks related to data privacy and security, especially relevant when considering future production-level adop-\ntion by the partner company.\n 2. Empirical Performance: LLaMA 3 and Qwen 2 have demonstrated state-of-the-art performance across a \nrange of natural language processing tasks, including question answering, classification, code generation, \ntext rewriting, and reasoning. Public benchmarks and evaluations (e.g., Meta AI blog on LLaMA 3 and Hug-\nScientific Reports |        (2025) 15:31660 3| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\nging Face model card for Qwen2-7B) support the superiority of these models compared to other open-source \nalternatives.\n 3. Model Size and Computational Efficiency: We employed the 8-billion-parameter version of LLaMA 3 and \nthe 7-billion-parameter version of Qwen 2. These models strike a balance between performance and compu-\ntational cost, allowing us to run them on standard local hardware without requiring specialized or high-cost \ninfrastructure.\nFor the summarization process, all messages from a (patient and healthcare team) dialogue are grouped together \nto generate a single document (possibly a long one). More formally, for each patient pi belonging to a set of \npatients {p1,p 2, ··· pn}, we produce a document di with all messages related to pi. A summarization method \nreceives a document di for pi as input and must produce a summary si capturing the main aspects and topics \ndiscussed in di.\nFor using LLMs as a summarization method, we developed a prompt, illustrated in Fig.  1. This prompt \ncontains instructions for performing the summarization task. The text to be processed by a LLM must be \ntokenized, i.e., it is broken down into smaller subword units known as tokens, resulting in a sequence of tokens. \nEach subword is identified numerically. The LLM processes the sequence of tokens and generates a different \nsequence to create the respective summaries. Figure 2 depicts the full summarization process.\nLLMs can only receive a limited number of tokens as input. In our work, as we deal with messages in \nchronological order, we use the last 5,000 tokens from a dialogue as input to the LLMs, as the last messages of \nthe dialogue may be more important to respond to a current patient message.\nFig. 2. Summarization process using LLM.\n \nFig. 1. Example of prompt.\n \nScientific Reports |        (2025) 15:31660 4| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\nEvaluating the summaries\nTo evaluate the LLM-generated summaries, we performed an A/B test. We conducted the evaluation of the \ngenerated summaries using volunteers from Ana Health’s healthcare team. It is important to stress that the \nparticipants who evaluated the summaries were not involved in the dialogues with the patients to avoid any bias \nthat may come from previous knowledge of the patient.\nFor evaluation, we did not exploit traditional summarization metrics, such as ROUGE 16, BLEU17, NIST18, \nand METEOR19, which are based on text overlap, or BERTScore20 and BARTScore21, which are similarity-based \nmetrics, as these metrics require ground-truth information, which is not available in our dataset. In other words, \nwe do not know what would be the “perfect summary” . Some recent works also analyze the use of LLMs as \nsummary evaluators22–24. However, our goal is not to analyze the ability of LLMs to evaluate the quality of \nsummaries. Instead, we focus on evaluating their performance in generating useful summaries about patients, \nwhen facing noisy and informal data, with the goal of helping a healthcare professional to appropriately respond \nto the patient’s messages. Thus, we conducted an evaluation by humans, in particular potential real users of the \ngenerated summaries. This constitutes one main contribution of our work.\nEvaluation criteria for the summaries\nWe elaborated a survey in which each participant had to read messages from the original entire dialogue \n(conversation) and, next, rate each summary using a 5-point Likert scale (1 - strongly disagree, 5 - strongly \nagree)22 considering four perspectives:\n• Coverage: The summary covers all important and relevant aspects of the full conversation;\n• Relevance: All aspects of the summary are relevant to the context of the messages, i.e., the summary does not \ncontain irrelevant portions;\n• Redundancy: The summary is non-redundant, i.e., it does not contain repetitive information;\n• Veracity: The summary is truthful (does not contain wrongful information).\nAfter reviewing the literature3,25 and discussing the metrics with the Ana Health healthcare team, we focus our \nassessment on these perspectives. A supplementary file contains an example of our survey.\nParticipants\nAs volunteers/participants to evaluate the summaries, we had five physicians and 19 psychologists. We selected \n24 healthcare professionals who are responsible for welcoming patients, conducting triage based on patient-\nreported information, and maintaining an accurate understanding of each patient’s history and most recent \ninteractions with the company. It is also important to highlight that the literature indicates that usability testing \nwith more than 15 users is typically sufficient to identify the vast majority – if not all – of the major issues 26. \nWe distributed the survey among participants to ensure a balanced representation of professional types. As \nadditional information, the participants are predominantly women, with approximately 22% being men. Each \nsummary was evaluated by two women and one man, or by three women.\nEvaluation protocol\nIn the survey, we applied the same evaluation protocol to all participants (both external and internal) to \nminimize bias in our results. We estimated that, to maintain an evaluation time of around 15 minutes, each \nparticipant should evaluate two summaries to avoid fatigue and ensure effective participation throughout the \nstudy, including the external psychologist. Several studies27,28 in the area of research methodologies indicate that \nsurveys lasting less than 15 minutes tend to have a higher response rate, fewer dropouts midway through the \nquestionnaire and better quality responses. For each summary, participants first read the full dialogue, then read \nthe summary, and finally rate the four perspectives based on the Likert scale.\nBefore randomly selecting the dialogues to be summarized in our experiment, we first filtered dialogues with \na token count between 1,000 and 5,000. This step ensures that the dialogues will be neither truncated by the LLM \nnor too brief to contain insufficient information for generating a summary. Next, we randomly selected eight \nfull dialogues and provided them to both LLMs — LLaMA 3 and Qwen 2. Consequently, we obtained sixteen \nsummaries for evaluation, with each summary assessed by three different participants.\nAnalyzing survey data\nTo analyze the survey data, we first analyze the distributions of responses in each perspective, aiming to obtain \nan initial understanding of the behavior of the participants’ evaluations for each LLM. Then we use the numerical \nscale (1 - strongly disagree – 5 - strongly agree) to analyze the mean values and their standard deviations to \nobtain a quantitative interpretation of the LLMs effectiveness.\nWe also calculated the Kappa-Cohen coefficient (k) to measure the agreement between two participants. It is \nuseful for assessing the consistency of the obtained responses. We applied this metric exclusively to evaluating \nthe same perspective on the same summary by two participants. After calculating the agreement for all possible \npairs of responses, we computed the average to determine an overall agreement value.\n \nk = po − pe\n1 − pe\n (2)\nwhere po represents the observed agreement ratio, reflecting the empirical probability of agreement on the \nanswers provided for any given statement and pe represents the expected agreement, assuming that the evaluators \nrespond randomly. By incorporating pe, we account for the possibility of agreement occurring by chance.\nScientific Reports |        (2025) 15:31660 5| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\nWe conducted a final qualitative analysis of the generated summaries in our case study using the perspective \nof a professional focused on the humanized reception of patients in the context of the psychotherapeutic process.\nData privacy\nThis study was conducted in accordance with the highest ethical standards and received prior approval from an \nindependent institutional ethics committee. Specific attention was given to the issues of data anonymization, \nprivacy preservation, and the ethical implications of processing patient-related communication.\nThe dataset used in our study was anonymized prior to our access, with all identifiable information removed \nby the partnering company in compliance with their internal data governance policies. We did not perform any \ndata collection ourselves. Instead, we received the pre-processed data in anonymized form, with no access to any \npersonal identifiers such as names, contact details, or healthcare identification numbers.\nFurthermore, all data processing, including the summarization task, was conducted locally using on-premise \ninfrastructure. At no stage were the data or derived outputs transmitted to cloud-based services or external \nservers. This approach ensures that the risk of data exposure through insecure communication channels is \neliminated.\nTo strengthen our commitment to confidentiality, all members of the research team signed a legally binding \nNon-Disclosure Agreement (NDA) with the data-providing entity, ensuring that no part of the data or results \ncould be used or disclosed outside the scope of the approved study.\nRegarding the operational deployment of the system, it is important to clarify that only authorized healthcare \nprofessionals who are already granted access to the original patient communications within the company’s clinical \nworkflow will be able to access the summaries generated by the system. These professionals are, by definition, \nbound by professional confidentiality and ethical obligations, and already possess access to more detailed and \npotentially identifiable information than what is present in the summaries. Therefore, the summarization \ncomponent does not introduce any additional privacy risks, nor does it expand the circle of data access beyond \nwhat is already sanctioned by the healthcare provider.\nOur study applied state-of-the-art anonymization techniques, removed direct identifiers, and ensured that \nall work was conducted under strict legal and ethical safeguards. These measures significantly mitigate the risk \nof deanonymization and are consistent with ethical research practices and data protection regulations such as \nnational data protection laws applicable to our context.\nResults\nEvaluating the quality of the messages\nIn this section, we discuss the quality of the messages regarding three dimensions: size, readability, and \ncorrectness.\nSize\nFigure 3 shows the distribution of the size of messages in words, characters, and phrases, considering all messages \nexchanged by patients and the healthcare team, whereas Table 1 shows average, maximum and minimum values \nof characters, words, and phrases - along with their quartile boundaries - considering all messages. Up to 75% of \nthe messages have up to 135 characters, 25 words, and 4 phrases, respectively.\nWe found 47,443 (23.45%) long messages and 112,971 (55.84%) short messages. That is, most of the messages \nare short, while almost a quarter are long.\nReadability\nWe utilized the Flesch-Kincaid grade level for the readability analysis. Before applying the Flesch-Kincaid grade \nlevel, we grouped all messages to and from the same patient, resulting in three types of “documents”: (i) all \nmessages sent by the healthcare team to a patient pi and all the messages sent by pi to the healthcare team, and \n(iii) the union of both, i.e., all messages from or to each patient (all messages per patient). As discussed above, \nwe separated the analysis according to the direction of the message flow. Figure 4 and Table 2 show the results.\nComparing the grade levels of the groups of messages sent by the healthcare team with those of the groups of \nmessages sent by patients, we observed higher average values and quartile limits for those sent by the healthcare \nteam. This indicates that, on average, texts produced by the healthcare team are more complex and less simplistic \nthan those produced by patients (they require a higher educational level to be read). The difference is noteworthy \nand should be considered in the development of future methodologies that rely on this type of data. The \ndistribution of grade levels for the messages sent by patients has a wider range of values, meaning that patient \nmessages have a higher variability in terms of quality.\nCorrectness\n For this analysis, we used a Portuguese dictionary from br.ispell to compare the proportion of correctly spelled \nwords in messages sent by patients with those sent by the healthcare team. The results are shown in Fig.  5 and \nTable 3.\nNotice that healthcare team messages exhibit a higher level of correctness compared with those sent by \npatients. Patients’ messages have very low correctness, indicating poor writing quality, containing a lot of typos, \nmisspellings, abbreviations, unknown words, etc.\nEvaluating the summarization task\nIn this section, we discuss the assessment of the summarization task. Before describing the results, we present \nsome statistics about the lengths (word counts) of our sample of dialogues and their corresponding summaries \ngenerated by LLaMA  3 and Qwen  2. Table  4 presents the word counts of the original dialogues and their \nScientific Reports |        (2025) 15:31660 6| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\ncorresponding summaries generated by LLaMA 3 and Qwen 2. It is worth noting that there is no clear correlation \nbetween the length of the input dialogues and the length of the summaries generated by the language models. \nOn average, the summaries generated by LLaMA  3 and Qwen  2 contain approximately 137 and 147 words, \nrespectively, with standard deviations of 59 and 41 words. This indicates that the models produce summaries \nwith lengths around 150 words, regardless of the length of the original dialogue.\nWe conducted our assessment of the summarization task through a survey with 24 participants, including \nfive physicians and 18 psychologists from Ana Health’s healthcare team, and one external psychologist. To \nanalyze the survey data, we use two approaches. First, we analyze the distributions of the scores the participants \ngave for the evaluated summaries. Second, we employ statistical metrics to obtain a quantitative interpretation \nof the LLMs’ effectiveness.\nAverage (SD) Minimum Maximum Q1 Q2 Q3\nCharacters 91.45 (109.49) 1 2,538 17 48 135\nWords 17.47 (19.95) 1 488 4 10 25\nPhrases 2.71 (2.01) 1 24 1 2 4\nTable 1. Size - average (standard deviation), minimum and maximum values, and quartile boundaries (Q1, Q2 \nan Q3) from 202,326 messages. Up to 75% of the messages have 135 characters, 25 words, and 4 phrases.\n \n(a) Totaln umbero fc haractersi na ll messages\n (b) Totaln umbero fw ordsi na ll messages\n(c) Totaln umbero f phrases in allm essages\nFig. 3. Distribution of the size of a Words, b Characters and c Phrases in all messages (202,326 messages) \nexchanged between patients and the healthcare team. The majority of the messages has few characters, words \nand phrases.\n \nScientific Reports |        (2025) 15:31660 7| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\nDistribution of responses for each perspective\nWe conducted this analysis individually for each evaluated perspective, enabling us to identify response patterns \nand directly compare the LLMs. Figure 6 illustrates the distribution of responses for each perspective when using \nLLaMA 3 in the summarization task.\nFlesch-Kincaid grade level\nGrouping criterion # of groups Average (SD) Minimum Maximum Q1 Q2 Q3\nFrom or to the same patient 1,863 3.72 (2.20) -8.91 15.95 2.36 3.88 4.99\nTo the same patient 1,859 4.18 (2.16) -8.91 18.20 2.82 4.08 5.35\nFrom the same patient 1,224 2.18 (3.85) -11.69 14.95 -0.18 2.32 4.82\nTable 2. Readability - total number of groups of messages, average (standard deviation), minimum and \nmaximum Flesch-Kincaid grade levels, and quartile boundaries (Q1, Q2, and Q3). High average values and \nquartile limits for the groups of messages sent by the healthcare team indicate that the messages sent by the \nhealthcare team require a higher grade level.\n \n(a) Flesch-Kincaidg rade levelo ng roupso f messages from the\nsame patient\n(b) Flesch-Kincaid gradel evel on groupso fm essagest ot he same\npatient\n(c) Flesch-Kincaid grad el evel on groupso fm essagesf roma nd to\nthes ame patient\nFig. 4. Distribution of the Flesch-Kincaid grade level for a Groups of messages sent by each patient to \nthe healthcare team, and b Groups of messages sent by the healthcare team to each patient and c Groups \nof messages from or to the same patient. The messages sent by patients have a wider range of grade levels, \nmeaning that patient messages are spread across a broader grade level interval compared to the healthcare \nteam.\n \nScientific Reports |        (2025) 15:31660 8| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\nNotice that, across all perspectives, positive responses — “strongly agree” and “agree” combined — account \nfor more than 50% of the total responses. More specifically, the percentages are 62.5%, 58.3%, 54.2% and 70.9%, \nrespectively, for coverage, relevance, redundancy, and veracity regarding LLaMA 3 summaries. Furthermore, \n“strongly disagree” responses are a minority across all perspectives and are entirely absent for model’s coverage. \nIn sum, volunteers considered LLaMA 3 summaries predominantly satisfactory in basically all perspectives.\nFor Qwen 2, Fig.  7 also shows a predominance of positive responses, which, as for LLaMA 3, account for \nmore than 50% of the total. That is, the percentages of positive responses are 54.2%, 58.3%, 70.8% and 54.1%, \nfor coverage, relevance, redundancy, and veracity, respectively, also suggesting a good summarization capability \nfor this LLM. However, “strongly disagree” responses for Qwen 2 are significantly more frequent than those for \nLLaMA 3, exceeding 10% across all perspectives.\nAnalyzing statistical metrics\nTable 5 shows the average rates for each perspective organized by LLM. All values are above 3, indicating a \ntendency for positive results and suggesting a good model (average) effectiveness in both cases. For coverage and \nrelevance, LLaMA 3 obtains average values slightly higher than those obtained by Qwen 2. Conversely, Qwen \n# of words\nDialogue Dialogue LLaMA Summary Qwen summary\n1 967 102 195\n2 2069 208 202\n3 1254 85 164\n4 1458 220 167\n5 1422 106 138\n6 2300 67 124\n7 2767 122 107\n8 2659 189 85\nTable 4. Word counts of dialogues and corresponding summaries generated by LLaMA 3 and Qwen 2.\n \nSource Average (SD) Minimum Maximum Q1 Q2 Q3\nAll messages 0.49 (0.24) 0.00 1.00 0.40 0.52 0.67\nHealthcare team’s messages 0.54 (0.18) 0.00 1.00 0.47 0.54 0.66\nPatients’ messages 0.44 (0.29) 0.00 1.00 0.25 0.50 0.67\nTable 3. Correctness - average (standard deviation), minimum and maximum values, and quartile boundaries \n(Q1, Q2, and Q3). 202,326 total messages, 111,011 healthcare team’s messages and 91,315 patients’ messages..\n \n(a) Patients\n (b) Healthcar et eam\nFig. 5. Distribution of correctly spelled words in messages sent by a Patients and b Healthcare team. The \nmessages sent by the healthcare team have a higher level of correctness compared with those sent by patients. \n202,326 total messages, 111,011 healthcare team’s messages and 91,315 patients’ messages..\n \nScientific Reports |        (2025) 15:31660 9| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\n2 stands out in (lack of) redundancy concerning LLaMA 3. The opposite is true for veracity, indicating that, on \naverage, LLaMA 3 is more truthful than Qwen 2.\nWe also note that, for all perspectives, the standard deviations obtained using LLaMA 3 are smaller, indicating \nless data dispersion. This suggests less variation in the participants’ responses, pointing to a higher level of \nconsensus, which is crucial for selecting a model. We elaborate further on that next.\nFig. 7. Distribution of scores for each perspective (24 assessments per perspective) evaluating summaries \ngenerated by Qwen 2. Positive responses – “agree” and “strongly agree” – are more than 50% of the total \nresponses.\n \nFig. 6. Distribution of scores for each perspective (24 assessments per perspective) evaluating summaries \ngenerated by LLaMA 3. Positive responses — “agree” and “strongly agree” — are more than 50% of the total \nresponses.\n \nScientific Reports |        (2025) 15:31660 10| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\nFinally, we did not observe any correlation between the lengths of dialogues and summaries and the qualitative \nassessments conducted in our study. These findings suggest that summary length, within the observed range, \ndoes not significantly influence the evaluated perspectives.\nKappa-Cohen (agreement between participants)\nSince the Kappa-Cohen coefficient is only meaningful when calculated in evaluations carried out on the same \nobject (summary), we selected four summaries generated by LLaMA 3, represented in Table 6 as s1, s2, s3 and \ns4, and four summaries generated by Qwen 2, represented as s5, s6, s7 and s8, totaling eight summaries. Since \nthree participants evaluated each summary, we have three Kappa-Cohen values for each summary and show the \naverage value of these three values in Table 6.\nKappa-Cohen values range from -1 to 1, with values lower than 0 indicating that the agreement is worse than \nwhat would be expected by chance. We observed such a situation in the evaluations of summaries s5 and s7 \ngenerated by Qwen 2. Values between 0.0 and 0.2 indicate weak agreement, which we observe in various results. \nValues between 0.2 and 0.4 indicate reasonable agreement, as occurs in s3 and s8 generated by LLaMA 3 and \nQwen 2, respectively.\nOverall, when analyzing the values obtained by applying the Kappa-Cohen coefficient, we note that the \nagreement values were generally low, with LLaMA 3 results being more consistent than Qwen 2’s. However, it is \nimportant to stress that since this metric considers only total agreement (same response), it is very sensitive to \nthe fact that there are five possible responses. In other words, there is no consideration of the degree of gradation \nof the responses, being the disagreement between “agree” and “strongly agree” considered the same as between \n“strongly agree” and “strongly disagree” , for example.\nExternal Qualitative Analysis\nAiming to have a view of a newly integrated professional into the healthcare team, we asked the external \nparticipant – in this case, a psychologist who was not part of Ana Health’s healthcare team – to assess the \nusefulness of the summaries in helping recently enrolled collaborators understand the context when patients \nsend new messages. In summary, taking two dialogues as input, we used Qwen 2 and LLaMA 3 to generate \ntwo summaries for each dialogue. The participant evaluated the summaries regarding clarity, cohesion, and \nusefulness of the summaries without knowing which LLMs were used.\nWhen evaluating the summaries generated by the first LLM (Qwen 2), she found the first summary satisfactory, \nas it accurately reflected the data discussed in the conversation between the patient and the healthcare team. \nHowever, the participant considered the second summary, also produced by Qwen  2, unsatisfactory since it \nlacked important data necessary for a complete understanding of the case, making it impossible to provide an \nattentive and efficient service.\nFurthermore, according to the participant, the second summary lacked the human aspect and failed to meet \nthe requirements for understanding the case, given that the model failed to provide crucial data, such as the fact \nthat the patient was recovering from COVID, for instance. Furthermore, the summary contained details about \nhow the patient had expressed gratitude for the care, which was considered unimportant to understanding the \npatient’s condition.\nRegarding the summaries generated using the second LLM (LLaMA 3), the participant considered that \nboth summaries maintained a certain standard of quality as well as efficiently addressed the content of the \nconversation. In this sense, the four previously requested observation points were met, as all aspects of the \nconversation were well summarized, had the necessary relevance for understanding the case, were objective and \nwithout irrelevant repetitions, and did not present false or dubious content.\nAccording to the participant, summaries with positive evaluations from the perspectives evaluated can help \na new member understand the context of new messages and thus formulate contextualized and personalized \nresponses to the patient with agility. She also informs that the ability to understand each individual’s context, as \nSummary LLaMA 3 Summary Qwen 2\ns1 0.116 s8 0.313\ns2 0.036 s7 −0.022\ns3 0.283 s6 0.056\ns4 0.087 s5 −0.132\nTable 6. The Kappa-Cohen average values for each LLM per summary.\n \nPerspective LLaMA 3 Qwen 2\nCoverage 3.29 (1.16) 3.21 (1.53)\nRelevance 3.50 (0.98) 3.38 (1.41)\nRedundancy 3.42 (1.50) 3.75 (1.54)\nVeracity 3.83 (1.34) 3.46 (1.44)\nTable 5. Average values for each perspective per LLM. Standard deviation in parentheses.\n \nScientific Reports |        (2025) 15:31660 11| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\nmanaging the care reception process is inherently personal and non-transferable. “Each person is unique and \nrequires a tailored approach to her healthcare. Personalized reception, adapted to each individual’s specific needs \nand characteristics, is essential for creating a truly humanized and effective experience” . Indeed, time is often a \ncritical factor in the integration process. Faster and more accurate access to information improves the efficiency \nof this process29.\nDiscussion\nPrevious work on summarizing clinical data has not delved deeply into the quality of the input data, which, in \nour case, as we shall see, can be very poor and informal. This analysis itself is a contribution of this work, as it \nputs a high burden on the LLM-based summarizers. On top of that, we have the fact that the dialogues are in \nPortuguese. Indeed, while being one of the most spoken languages globally, Portuguese has historically been \nunderrepresented in training many large language models (LLMs) compared to languages like English, Spanish, \nor Chinese. This can be attributed to several factors, including the availability of high-quality training data, the \nfocus of research and development efforts, and the predominant use of English in the tech industry30.\nMessage quality\nRegarding the size dimension, we observed that most messages were considered short (56.12%) and around \n23.58% were considered long. Short texts may lack information or contain incomplete information, while long \ntexts may have redundancies, which may lead LLMs to not focus on the main parts of the text. Both issues may \naffect summary quality.\nThe average Flesch-Kincaid grade levels for the healthcare team and patients suggest that a higher grade \nlevel may be required to understand healthcare team messages compared with those from patients. Regarding \npatients, we observe that their messages have a wider range of values for the Flesch-Kincaid grade level than \nthose from the healthcare team. Although this may indicate a potential diversity of patients in terms of education \nlevel, several other factors may also influence these results, such as the level of detail needed to talk about certain \nsubjects and the use of audio to explain more complex subjects, among others.\nRegarding correctness, most words in the messages sent by the healthcare team are found in a standard \ndictionary, while this trend is not found for patient messages. Indeed, the proportion of “correct” words in \npatient messages is very low, around 44% on average. However, this low correctness level may be taken in context \nand be more carefully analyzed as the language usage of WhatsApp users has its own distinct terms, such as \n“vc” (abbreviation of “você” – you in Portuguese), “tbm” (abbreviation of “também” – also in Portuguese), “pq” \n(abbreviation of “porque” – why or because in Portuguese), in which, from the perspective of the users are \ncommon and seen as correct. In any case, the high occurrence of incorrect or non-standard words may lead \nLLMs to ignore or misinterpret them in the summaries, causing omissions of important parts from the original \ntext or generation of wrong information.\nOverall, considering the dimensions evaluated of size, readability, and correctness, there are clues of \nlow quality in patients’ messages. This (poor) data quality in our dataset may pose significant challenges for \nsummarization tasks using LLMs.\nSummarization quality\nWe evaluated the summaries from four perspectives, coverage, relevance, redundancy and veracity, through \nan online survey. Both LLMs performed well from all four perspectives. More than 50% of the responses are \n“strongly agree” and “agree” in all perspectives and in both LLMs. This shows the ability of the LLMs to generate \nsatisfactory summaries despite the quality of the original text. The few “strongly disagree” responses obtained \nin our analysis (less than 10%) are concentrated on Qwen 2 summaries, being completely absent in LLaMA 3’s \ncoverage.\nComparing the results of LLaMA 3 with those of Qwen 2, considering “strongly agree” and “agree” responses, \nwe notice that, for coverage and veracity, LLaMA  3’s summaries are slightly superior to those produced by \nQwen 2. Both LLMs tie under the relevance perspective and the Qwen 2 summaries are slightly superior regarding \n(lack) of redundancy. Overall, considering all evaluation criteria, LLaMA 3’s summarization performance in our \napplication is slightly superior to Qwen 2’s.\nIn an external qualitative analysis, viewed from the perspective of a newly integrated professional into the \nhealthcare team, the professional observed that the summaries produced by LLaMA  3 are objective, free of \nirrelevant repetitions and do not contain false or dubious content. On the other hand, the summaries produced \nby Qwen 2 do not meet the same standard. While a summary correctly reflects the content of the dialogue, the \nother one lacks important data included in the dialogue.\nPositive evaluations from the analyzed perspectives potentially mean summaries that can effectively help \nteam members quickly understand the context of a new message without needing to review the entire prior \ndialogue with the patient. This also enables team members to respond to the new message in an efficient and \ncontextually appropriate manner. Such agility in providing clear and accurate information may help reduce the \nanxiety of patients awaiting a response, thus making the interaction process between patients and healthcare \nteam members more efficient.\nIn short, despite the messages being written in Portuguese, some clues of limited quality of the messages \nand high variability in the grade level required to understand the messages, the summaries produced by both \nevaluated LLMs are satisfactory for the sake of the target task, with LLaMA 3 showing slight superiority.\nLimitations\nOur work has some limitations. First, we generate summaries using messages exclusively exchanged between \npatients and the Ana Health healthcare team. Our goal in this work is only to evaluate the ability of LLMs to \nScientific Reports |        (2025) 15:31660 12| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\ngenerate summaries. If we aim to create summaries containing complete patient information, such as their health \nhistory, preferences, and demographic details, additional data sources will be required. Second, our evaluation \nof the summaries was carried out with members of the Ana Health healthcare team. Perhaps people from other \nareas or even patients may have different opinions. Third, we evaluated the summaries using only the Likert scale. \nIf we wanted to understand in depth the reason for each value recorded by the participant, we would need other \ntypes of responses, such as comments on the evaluated summaries. Fourth, our evaluation used messages from \nthe dialogues of eight patients as a source. A more comprehensive analysis of LLMs summarization capability \nmay require a larger number of dialogues. Fifth, this work did not evaluate the use of summaries in other tasks, \nsuch as their impact on responding to patient messages.\nFuture directions\nWe are currently exploring several avenues to enhance the performance and applicability of the summarization \nsystem in future work. These efforts include incorporating electronic medical records to enrich the input data, \nintegrating audio processing to capture and utilize verbal communication, and adopting strategies that enable \nthe separate analysis and summarization of distinct segments of the data prior to generating a comprehensive \nfinal summary.\nFurthermore, in our qualitative analysis of LLMs’ ability to summarize dialogues, we deliberately limited the \namount of input data to ensure it remained within the context window of the models. At this stage, we consider \nthis a reasonable constraint, as our primary goal was to assess how effectively the models could capture the \nessence of the most recent interactions between patients and the healthcare team – under the assumption that \nrecent messages are more likely to reflect the patient’s current health status. As part of our future work, we intend \nto investigate methods to address the limitations imposed by the context window size of large language models, \nwhich remains a critical factor influencing summarization quality5,31.\nIn this work, an external psychologist evaluated two summaries according to the same protocol applied to \nall participants. In the future, we also intend to include more external evaluators for improving our evaluation.\nConclusions\nIn this article, we evaluated the ability of two large language models, LLaMA  3 and Qwen  2, to summarize \ndialogues between patients and healthcare teams. These dialogues, written in Portuguese, were conducted \nthrough WhatsApp. Ideally, the generated summaries should highlight important information about each \npatient, enabling the healthcare team to respond to new patient messages quickly and in a personalized manner.\nDifferently from previous work, which applied summarization in high-quality clinical data (e.g., electronic \nhealth records), we start our contributions by first assessing the quality of the dialogues to be summarized \nand found evidence of low data quality, especially due to informality. The primary objective of our study is to \nrigorously evaluate whether state-of-the-art LLMs – specifically open-source models – are capable of producing \nclinically useful summaries from real-world, informal, and noisy patient-provider dialogues in Portuguese, a \nlanguage underrepresented in current medical NLP research.\nOur contribution lies in the application and systematic evaluation of these models in a realistic healthcare \ncommunication scenario, which presents several technical and practical challenges: the dialogues are \nunstructured, colloquial, and often fragmented, reflecting actual communication patterns in digital health \nplatforms; the setting involves asynchronous, chronologically evolving dialogues, requiring the models to \nmaintain contextual coherence over multiple messages; the domain involves sensitive health-related content, \nwhere summarization errors can have non-trivial implications.\nTo address concerns regarding the robustness and safety of the generated summaries, we structured our \nevaluation around four key criteria: coverage, relevance, redundancy, and veracity. These dimensions were chosen \nprecisely to assess potential summarization errors that could impact clinical interpretation. All evaluations were \nconducted manually by human annotators familiar with the healthcare domain and native in Portuguese. Our \nresults suggest that, while both models perform reasonably well, there are notable differences in their ability to \ngenerate true and relevant summaries – issues that are critical in any health-related application.\nTo summarize, the novelty of our work lies in: (1) the application and comparative evaluation of LLMs in \na low-resource, high-stakes domain using real-world Portuguese clinical dialogues; (2) the design of a multi-\nfaceted evaluation framework specifically tailored to identify errors that could compromise clinical decision-\nmaking; and (3) the establishment of a foundation for future clinical validation, which will directly address \nutility and patient safety concerns raised by the reviewer.\nIn conclusion, even when processing low quality Portuguese texts, the evaluated LLMs were able to produce \nsummaries that could significantly improve patient care. These summaries support healthcare team members \nin understanding the context of new messages from patients, enabling them to respond quickly and in a \npersonalized manner, thereby enhancing patient outcomes.\nFrom a broader perspective, our study underscores the crucial role of LLMs in addressing healthcare \ndisparities. In countries like Brazil, where a significant portion of the population lacks access to basic healthcare \nand there is a shortage of health professionals, digital healthcare services are not just essential, but they represent \na critical solution. Strategies, such as those discussed in this article, that can effectively process large volumes of \ndata in underrepresented languages have the potential to make a significant social impact by helping to alleviate \npressing issues.\nData Availability\nThe data generated or analyzed during this study are included in this article and its supplementary information \nfiles. The corresponding author is available to provide additional data regarding this manuscript upon request.\nScientific Reports |        (2025) 15:31660 13| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\nReceived: 7 April 2025; Accepted: 24 July 2025\nReferences\n 1. Liu, S. et al. Leveraging large language models for generating responses to patient messages-a subjective analysis. J. Am. Med. \nInform. Assoc. 31, 1367–1379. https://doi.org/10.1093/jamia/ocae052 (2024).\n 2. Hone, T., Rasella, D., Barreto, M. L., Majeed, A. & Millett, C. Association between expansion of primary healthcare and racial \ninequalities in mortality amenable to primary care in brazil: a national longitudinal analysis. PLoS Med. 14, e1002306.  h t t p s : / / d o i . \no r g / 1 0 . 1 3 7 1 / j o u r n a l . p m e d . 1 0 0 2 3 0 6     (2017).\n 3. El-Kassas, W . S., Salama, C. R., Rafea, A. A. & Mohamed, H. K. Automatic text summarization: A comprehensive survey. Expert \nSyst. Appl. 165, 113679. https://doi.org/10.1016/j.eswa.2020.113679 (2021).\n 4. Keszthelyi, D., Gaudet-Blavignac, C., Bjelogrlic, M. & Lovis, C. Patient information summarization in clinical settings: Scoping \nreview. JMIR Med. Inform. 11, e44639. https://doi.org/10.2196/44639 (2023).\n 5. Zhang, H., Yu, P . S. & Zhang, J. A systematic survey of text summarization: From statistical methods to large language models. \nACM Computing Surveys https://doi.org/10.1145/3731445 (2025). Just Accepted.\n 6. Minaee, S. et al.  Large language models: A survey. arXiv preprint arXiv:2402.06196https://doi.org/10.48550/arXiv.2402.06196 \n(2024).\n 7. Laskar, M. T. R. et al. A systematic survey and critical review on evaluating large language models: Challenges, limitations, and \nrecommendations. In Al-Onaizan, Y ., Bansal, M. & Chen, Y .-N. (eds.) Proceedings of the 2024 Conference on Empirical Methods in \nNatural Language Processing, 13785–13816, https://doi.org/10.18653/v1/2024.emnlp-main.764 (2024).\n 8. Y ang, R. et al. Large language models in health care: Development, applications, and challenges. Health Care Sci.  2, 255–263. \nhttps://doi.org/10.1002/hcs2.61 (2023).\n 9. Dave, T., Athaluri, S. A. & Singh, S. Chatgpt in medicine: An overview of its applications, advantages, limitations, future prospects, \nand ethical considerations. Front. Artif. Intell. 6, 1169595. https://doi.org/10.3389/frai.2023.1169595 (2023).\n 10. Wang, L. et al. Applications and concerns of chatgpt and other conversational large language models in health care: Systematic \nreview. J. Med. Internet Res. 26, e22769. https://doi.org/10.2196/22769 (2024).\n 11. Mani, I. & Maybury, M. T. Advances in Automatic Text Summarization MIT Press (MA, USA, Cambridge, 1999).\n 12. Hasan Dalip, D., André Gonçalves, M., Cristo, M. & Calado, P . Automatic quality assessment of content created collaboratively by \nweb communities: a case study of wikipedia. In Proceedings of the 9th ACM/IEEE-CS Joint Conference on Digital libraries, 295–304, \nhttps://doi.org/10.1145/1555400.1555449 (2009).\n 13. Kincaid, J. P ., Fishburne, R. P . J., Rogers, R. L. & Chissom, B. S. Derivation of new readability formulas (automated readability \nindex, fog count and flesch reading ease formula) for navy enlisted personnel. Tech. Rep. 56, Institute for Simulation and Training \n(1975).\n 14. Martins, T. B. F ., Ghiraldelo, C. M., Nunes, M. d. G. V . & Oliveira Junior, O. N. d. Readability formulas applied to textbooks in \nbrazilian portuguese. Tech. Rep. 28, ICMSC - Universidade de São Paulo (1996).\n 15. Moreno, G. C. d. L., de Souza, M. P ., Hein, N. & Hein, A. K. Alt: A software for readability analysis of portuguese-language texts. \narXiv preprint arXiv:2210.00553https://doi.org/10.48550/arXiv.2210.00553 (2022).\n 16. Lin, C.-Y . ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, 74–81 (2004).\n 17. Papineni, K., Roukos, S., Ward, T. & Zhu, W .-J. Bleu: a method for automatic evaluation of machine translation. In Proceedings \nof the 40th annual meeting of the Association for Computational Linguistics , 311–318, https://doi.org/10.3115/1073083.1073135 \n(2002).\n 18. Doddington, G. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the \nsecond international conference on Human Language Technology Research, 138–145 (2002).\n 19. Banerjee, S. & Lavie, A. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In \nProceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 65–72 \n(2005).\n 20. Zhang, T., Kishore, V ., Wu, F ., Weinberger, K.  Q. & Artzi, Y . Bertscore: Evaluating text generation with bert. arXiv preprint \narXiv:1904.09675https://doi.org/10.48550/arXiv.1904.09675 (2019).\n 21. Yuan, W ., Neubig, G. & Liu, P . BARTSCORE: evaluating generated text as text generation. In Proceedings of the 35th International \nConference on Neural Information Processing Systems, NIPS ’21, https://doi.org/10.5555/3540261.3542349 (2021).\n 22. Gao, M. et al.  Human-like summarization evaluation with chatgpt. arXiv preprint arXiv:2304.02554https://doi.org/10.48550/\narXiv.2304.02554 (2023).\n 23. Jain, S. et al.  Multi-dimensional evaluation of text summarization with in-context learning. In Rogers, A., Boyd-Graber, J. & \nOkazaki, N. (eds.) Findings of the Association for Computational Linguistics: ACL 2023, 8487–8495,  h t t p s : / / d o i . o r g / 1 0 . 1 8 6 5 3 / v 1 / 2 0 \n2 3 . fi   n d i n g s - a c l . 5 3 7     (2023).\n 24. Wu, N., Gong, M., Shou, L., Liang, S. & Jiang, D. Large language models are diverse role-players for summarization evaluation. In \nCCF International Conference on Natural Language Processing and Chinese Computing, 695–707,  h t t p s : / / d o i . o r g / 1 0 . 1 0 0 7 / 9 7 8 - 3 - 0 3 \n1 - 4 4 6 9 3 - 1 _ 5 4     (2023).\n 25. Fabbri, A. R. et al. Summeval: Re-evaluating summarization evaluation. Trans. Assoc. Computat. Linguist. 9, 391–409.  h t t p s : / / d o i . \no r g / 1 0 . 1 1 6 2 / t a c l _ a _ 0 0 3 7 3     (2021).\n 26. Nielsen, J. Why you only need to test with 5 users. in nielsen norman group (2000). Https://www.nngroup.com/articles/why-you-\nonly-need-to-test-with-5-users/.\n 27. Dillman, D. A., Smyth, J. D. & Christian, L. M. Internet, phone, mail, and mixed-mode surveys: The tailored design method. John \nWiley and Sons (2014).\n 28. Revilla, M. & Ochoa, C. Ideal and maximum length for a web survey. Int. J. Mark. Res. 59, 557–565.  h t t p s : / / d o i . o r g / 1 0 . 2 5 0 1 / I J M \nR - 2 0 1 7 - 0 3 9     (2017).\n 29. Alotaibi, Y . K. & Federico, F . The impact of health information technology on patient safety. Saudi medical journal38, 1173,  h t t p s : \n/ / d o i . o r g / 1 0 . 1 5 5 3 7 / s m j . 2 0 1 7 . 1 2 . 2 0 6 3 1     (2017).\n 30. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the dangers of stochastic parrots: Can language models be too \nbig? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, 610–623,  h t t p s : / / d o i . o r g \n/ 1 0 . 1 1 4 5 / 3 4 4 2 1 8 8 . 3 4 4 5 9 2 2     (2021).\n 31. Kotkar, A. D., Mahadik, R. S., More, P . G. & Thorat, S. A. Comparative analysis of transformer-based large language models (llms) \nfor text summarization. In 2024 1st International Conference on Advanced Computing and Emerging Technologies (ACET) , 1–7, \nhttps://doi.org/10.1109/ACET61898.2024.10730348 (2024).\nAcknowledgements\nWe would like to thank Ana Health, which is part of this collaboration, for supporting this project. We also thank \nthe Ana Health team.\nScientific Reports |        (2025) 15:31660 14| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/\nAuthor contributions\nConception or design of the work: AAF , MAG, LR, and WC. Data collection: ACM, APSJ, GJ, GPS, IA, JMC, \nW AS, and WC. Data analysis and interpretation: AAF , ACFV , ACM, ET, GJ, IA, JMC, LR, MAG, and WC. Crit-\nical revision of the article: AAF , ACFV , ACM, APSJ, ET, GJ, GPS, IA, JMC, JMP , LR, MADS, MAG, OSN, VM, \nW AS, and WC. Final approval of the version to be published: AAF , ACFV , ACM, APSJ, ET, GJ, GPS, IA, JMC, \nJMP , LR, MADS, MAG, OSN, VM, W AS, and WC.\nFunding\nThis study was supported by Ana Health, Brazilian Micro and Small Business Support Service (Serviço Brasileiro \nde Apoio às Micro e Pequenas Empresas - SEBRAE), Brazilian Company of Research and Industrial Innovation \n(Empresa Brasileira de Pesquisa e Inovação Industrial -EMBRAPII), and the National Institute of Science and \nTechnology in Artificial Intelligence Responsible for Computational Linguistics, Information Processing and \nDissemination (Instituto Nacional de Ciência e Tecnologia em Inteligência Artificial Responsável para Linguísti-\nca Computacional,Tratamento e Disseminação de Informação - INCT-TILD-IAR) (grant # 408490/2024-1). It \nwas also partially funded by CAPES, CNPq, FAPEMIG, and FINEP .\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nHuman Ethics and consent to participate\nThis work, including data and participation of volunteers in qualitative evaluations, was reviewed and \napproved by the Ethics Committee of the Federal University of Minas Gerais, in accordance with the Brazilian \nResolution CNS 446/12, under registration CAAE 80632524.4.0000.5149. Informed consent was obtained from \nall the participants.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 1 3 5 6 0 - 9     .  \nCorrespondence and requests for materials should be addressed to A.A.F .\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:31660 15| https://doi.org/10.1038/s41598-025-13560-9\nwww.nature.com/scientificreports/",
  "topic": "Health care",
  "concepts": [
    {
      "name": "Health care",
      "score": 0.6364685297012329
    },
    {
      "name": "Readability",
      "score": 0.6239792108535767
    },
    {
      "name": "Computer science",
      "score": 0.6158146262168884
    },
    {
      "name": "CLARITY",
      "score": 0.5084349513053894
    },
    {
      "name": "Automatic summarization",
      "score": 0.48549893498420715
    },
    {
      "name": "Portuguese",
      "score": 0.42660433053970337
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.425495445728302
    },
    {
      "name": "Data science",
      "score": 0.32434070110321045
    },
    {
      "name": "Artificial intelligence",
      "score": 0.22102606296539307
    },
    {
      "name": "Political science",
      "score": 0.11333391070365906
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I110200422",
      "name": "Universidade Federal de Minas Gerais",
      "country": "BR"
    },
    {
      "id": "https://openalex.org/I10824318",
      "name": "Universidade Federal de Ouro Preto",
      "country": "BR"
    },
    {
      "id": "https://openalex.org/I166595947",
      "name": "Federal University of São João del-Rei",
      "country": "BR"
    },
    {
      "id": "https://openalex.org/I52977244",
      "name": "Insper",
      "country": "BR"
    }
  ]
}