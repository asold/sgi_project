{
  "title": "Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages",
  "url": "https://openalex.org/W4389520707",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5011708753",
      "name": "Ethan Wilcox",
      "affiliations": [
        "ETH Zurich",
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A5029033876",
      "name": "Clara Meister",
      "affiliations": [
        "ETH Zurich",
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A5061951606",
      "name": "Ryan Cotterell",
      "affiliations": [
        "ETH Zurich",
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A5102825308",
      "name": "Tiago Pimentel",
      "affiliations": [
        "ETH Zurich",
        "University of Cambridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2164418233",
    "https://openalex.org/W4385573487",
    "https://openalex.org/W2512700785",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2118276816",
    "https://openalex.org/W4385573239",
    "https://openalex.org/W3033254023",
    "https://openalex.org/W4309994492",
    "https://openalex.org/W4306247398",
    "https://openalex.org/W2478708596",
    "https://openalex.org/W2964335542",
    "https://openalex.org/W2108010971",
    "https://openalex.org/W2100750861",
    "https://openalex.org/W2795342569",
    "https://openalex.org/W4361766487",
    "https://openalex.org/W2954933613",
    "https://openalex.org/W4389777835",
    "https://openalex.org/W3160285835",
    "https://openalex.org/W3175306105",
    "https://openalex.org/W2054125330",
    "https://openalex.org/W162788685",
    "https://openalex.org/W2179376720",
    "https://openalex.org/W4389777845",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3174658628",
    "https://openalex.org/W3012990076",
    "https://openalex.org/W4285209661",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2022449868",
    "https://openalex.org/W4404783274",
    "https://openalex.org/W4306964455",
    "https://openalex.org/W4205537173",
    "https://openalex.org/W4297436302",
    "https://openalex.org/W2156545345",
    "https://openalex.org/W4385572280",
    "https://openalex.org/W3176464186",
    "https://openalex.org/W2787322174",
    "https://openalex.org/W4210813972",
    "https://openalex.org/W1995875735"
  ],
  "abstract": "Surprisal theory (Hale, 2001; Levy, 2008) posits that a word’s reading time is proportional to its surprisal (i.e., to its negative log probability given the proceeding context). Since we are unable to access a word’s ground-truth probability, surprisal theory has been empirically tested using surprisal estimates from language models (LMs). Under the premise that surprisal theory holds, we would expect that higher quality language models provide more powerful predictors of human reading behavior—a conjecture we dub the quality–power (QP) hypothesis. Unfortunately, empirical support for the QP hypothesis is mixed. Some studies in English have found correlations between LM quality and predictive power, but other studies using Japanese data, as well as using larger English LMs, find no such correlations. In this work, we conduct a systematic crosslinguistic assessment of the QP hypothesis. We train LMs from scratch on small- and medium-sized datasets from 13 languages (across five language families) and assess their ability to predict eye tracking data. We find correlations between LM quality and power in eleven of these thirteen languages, suggesting that, within the range of model classes and sizes tested, better language models are indeed better predictors of human language processing behaviors.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7503–7511\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLanguage Model Quality Correlates with Psychometric Predictive Power in\nMultiple Languages\nEthan Gotlieb Wilcox\n Clara Meister\n Ryan Cotterell\n Tiago Pimentel\n ,\nETH Zürich\n University of Cambridge\n{ethan.wilcox, clara.meister, ryan.cotterell, tiago.pimentel}@inf.ethz.ch\nAbstract\nSurprisal theory (Hale, 2001; Levy, 2008)\nposits that a word’s reading time is propor-\ntional to its surprisal (i.e., to its negative log\nprobability given the proceeding context). It\nhas been empirically tested using surprisal\nestimates from language models (LMs).\nUnder the premise that surprisal theory\nholds, we would expect that higher quality\nlanguage models, whose predictions are more\naccurate, provide more powerful predictors\nof human reading behavior—a conjecture\nwe dub the quality–power (QP) hypothesis.\nUnfortunately, empirical support for the QP\nhypothesis is mixed. Some studies in English\nhave found correlations between LM quality\nand psychometric predictive power, but other\nstudies using Japanese data, as well as using\nlarger English LMs, find no such correlations.\nIn this work, we conduct a systematic crosslin-\nguistic assessment of the QP hypothesis.\nWe train LMs from scratch on small- and\nmedium-sized datasets from 13 languages\n(across five language families) and assess their\nability to predict eye tracking data. We find\ncorrelations between LM quality and psycho-\nmetric predictive power in eleven of these\nthirteen languages, suggesting that, within the\nrange of model classes and sizes tested, better\nlanguage models provide better predictors of\nhuman language processing behaviors.\nhttps://github.com/rycolab/\nquality-power-hypothesis\n1 Introduction\nThe relationship between a word’s predictability\nand its reading time (RT) is an important object of\nstudy in psycholinguistics because it allows us to\ndraw insights about how humans process sentences\n(Smith and Levy, 2013; Kuperberg and Jaeger,\n2016; Shain et al., 2022). Extensive prior research—\nacross many datasets, populations, reading-time\nmeasurement methodologies and even languages—\nhas found that the more predictable a word is, the\nfaster it will be to process (Kuribayashi et al., 2021;\nMeister et al., 2021; Smith and Levy, 2013; Wilcox\net al., 2020, 2023; Shain et al., 2022; de Varda\nand Marelli, 2022, 2023). Beyond these empirical\nfindings, the relationship between predictability\nand reading time has received formal treatment,\nforming the basis of surprisal theory (Hale,\n2001; Levy, 2008). In surprisal theory, a word’s\npredictability is operationalized in terms of its\nsurprisal, or contextual negative log-likelihood:\ns✓(wt | w<t) = −log p✓(wt | w<t).1 In\npractice, however, we do not know the contextual\nprobability of words according to the data-\ngenerating distribution p✓. Thus, it is common\nto estimate surprisal using the distribution over\nwords from a language model pθ instead, i.e.,\nsθ(wt |w<t) = −log pθ(wt |w<t).\nRecent advances in language modeling have\ndrastically increased the quality of surprisal\nestimates from language models (Hoffmann et al.,\n2022; Chowdhery et al., 2022; OpenAI, 2023).\nThe topic of interest of this work is whether this\nincreased quality also leads to an increase in a\nlanguage model’s psychometric predictive power\n(Frank and Bod, 2011; Goodkind and Bicknell,\n2018), i.e., how well its surprisal estimates can be\nused to predict RTs. To put it simply, we ask: Are\nbetter language models more powerful predictors\nof human behavior? If surprisal theory holds, we\nwould expect this power to correlate positively\nwith language models’ quality. We will refer to\nthis conjecture as the quality–power hypothesis,\nabbreviated as the QP hypothesis.\nEmpirical evidence for the QP hypothesis is\nmixed. When tested on English corpora, some\nstudies have found that models that place higher\nprobability on held-out data do tend to be better\nat predicting the pattern of human reading times\n(Goodkind and Bicknell, 2018; Wilcox et al., 2020).\nMore recent work, however, has discovered two em-\n1Empirical results mostly support the existence of a linear\nrelationship between surprisal and reading times (Smith and\nLevy, 2013; Shain et al., 2022; Wilcox et al., 2023): They\nfind that surprisal is a better psychometric predictor than raw\nprobabilities, and that non-linear functions of surprisal are not\nbetter than linear ones.\n7503\npirical exceptions: The first empirical shortcoming\nwas presented in Kuribayashi et al. (2021)—the\nonly2 non-English study (to the best of our\nknowledge) investigating the quality–power rela-\ntionship.3 Specifically, Kuribayashi et al. find that\nthe QP hypothesis does not hold in Japanese. The\nsecond shortcoming was identified by Shain et al.\n(2022) and Oh and Schuler (2023), who observe\nthat the quality–power relationship does not hold\nfor the best (and most recent) language models,\nsuggesting state-of-the-art language models might\nnot be aligned with human incremental predictions.\nIn this work we present two empirical studies\nthat investigate these shortcomings. First, we\nconduct a systematic crosslinguistic analysis of\nthe QP hypothesis, using small- and medium-sized\nLMs.4 We train 104 language models from scratch\non data from 13 languages, using subsamples of\nthe Wiki40b dataset (Guo et al., 2020). Within\neach language, we operationalize LMs’ qualities\nas their average per-word cross entropy on a test\nset. We then use the surprisals under these models\nto predict human reading times on a multilingual\ncorpus of eye tracking data. We find that, in eleven\nout of thirteen languages, there is a significant\ncorrelation between a language model’s quality\nand its surprisals’ psychometric predictive power.\nTo investigate the second shortcoming, we note\nthat a (potential) issue with the findings presented\nin Oh and Schuler (2023) and Shain et al. (2022)\nis that the training data for these models is not\npublicly available, and might plausibly includes\nthe reading time corpora against which their\npsychometric predictive power was evaluated.\nWe discuss why this might present an issue,\nand perform an investigation of the relationship\nbetween data leakage and model’s predictive\npower in §5, albeit with null results.\n2 Surprisal vs Reading Times\nOriginally proposed by Hale (2001), surprisal the-\nory posits that the amount of effort (and there-\n2Concurrent to this study, de Varda and Marelli (2023)\ninvestigate the QP hypothesis in a crosslinguistic setup. Un-\nlike our work, they rely on pre-trained multilingual models\nwhereas we make use of in-house trained monolingual models.\nThis allows them to investigate larger models than us, but also\nit introduces biases inherent to the use of multilingual models.\n3See Blasi et al. (2022) for a survey on the importance of\ntesting cognitive hypotheses beyond English.\n4Note that our best in-house trained language models are\nstill of far lower quality than the large language models ana-\nlyzed by Shain et al. (2022) and Oh and Schuler (2023).\nfore time) a reader must spend to process a word\nis a monotonically increasing function of its sur-\nprisal, where surprisal is a measure of a word’s\ninformation content (Shannon, 1948; Cover and\nThomas, 2006). Surprisal theory has attracted lots\nof attention over the years, by both the natural\nlanguage processing and cognitive science com-\nmunities (Hale, 2001, 2003, 2016; Keller, 2004;\nvan Schijndel and Schuler, 2016; van Schijndel\nand Linzen, 2018; Shain, 2019, 2021; Shain and\nSchuler, 2021, 2022; Wilcox et al., 2020; Meister\net al., 2021, 2022; Hoover et al., 2022; Oh et al.,\n2021; Oh and Schuler, 2022, 2023; Kuribayashi\net al., 2021, 2022, inter alia).\nOne way surprisal theory has been evaluated\nis by quantifying surprisal’s predictive power. If\na word’s surprisal influences reading times, we\nshould see evidence in naturalistic reading data.\nPsycholinguists thus train regressors pϕto predict\nreading times: r(wt) ∼ pϕ(r | ·), where pϕ’s\ninput is defined as a vector of baseline features xt,\ncomparing the regressors’ performances when sur-\nprisal is included and when it is not. Specifically,\nthe predictive power of surprisal is quantified as\nthe difference in the log-likelihood (llh) of reading\ntime data under the two different regressors:\n∆llh = llh(pϕ(r|xt,s✓)) −llh(pϕ(r|xt)) (1)\nNote that this is an ideal ∆llh, which assumes we\nhave access to surprisals from the data-generating\ndistribution, which we write as s✓ (as a shorthand\nfor s✓(wt |w<t)).\n2.1 Estimated Surprisal\nAs noted in §1, we do not have access to p✓ and\nthus, we cannot compute s✓(wt |w<t) directly. We\nmust therefore estimate it, which we do with the\nuse of a language modelpθ. Formally, the language\nmodels pθthat we discuss here are autoregressive\nprobability distributions over V\ndef\n= V∪{ EOS }, de-\nfined as the conditional distribution pθ(·| w<t).\nA language model’s parameters are typically cho-\nsen with the objective of minimizing its cross en-\ntropy with the true distribution:5\nH✓→θ(W ) =\n∑\nw∈S\np✓(w)\n|w|∑\nt=1\nsθ(wt |w<t) (2)\n5In practice, as we do not have access to p✓ , this is done\nusing a Monte Carlo estimator and a training dataset Dtrn =\n{w(n)}N\nn=1, where w(n) are assumed to be sampled i.i.d.\nfrom p✓ : H✓→θ(W ) ≈ 1\nN\n∑N\nn=1\n∑|w(n)|\nt=1 sθ(w(n)\nt |w(n)\n<t )\n7504\nwhere S\ndef\n= V∗◦{EOS }. A language model’s train-\ning, then, pushes it towards being a better surprisal\nestimator. This can be shown clearly when we\nrewrite the cross entropy as the sum of p✓’s en-\ntropy and the Kullback–Leibler (KL) divergence\nbetween p✓ and pθ:\nH✓→θ(W ) = H✓(W ) + KL(p✓ ||pθ) (3)\nwhere the KL measures a statistical “distance” be-\ntween two distributions, being zero only when\npθ = p✓. Importantly, language models are not\nperfect estimates of p✓, and so they do not achieve\nthis minimum. Intuitively, H✓→θ(W ) should then\ngive us a sense for how similar pθis to p✓, and,\nthus, how similar sθis to s✓. These surprisal esti-\nmates sθare then in turn used to estimate surprisal’s\npredictive power\nˆ∆llh = llh(pϕ(r|xt,sθ)) −llh(pϕ(r|xt)) (4)\nWe can now state the QP hypothesis in terms of\nthese more formal definitions: the QP hypothesis\npredicts that as the cross entropy H✓→θ(W )\ndecreases, a model’s surprisal values should\nbecome better predictors of reading times, leading\nto larger ˆ∆llh.\n3 Experimental Setup\nMeasuring Predictive Power. As discussed\nin §2, and following previous work in this area\n(Goodkind and Bicknell, 2018; Wilcox et al.,\n2020), we quantify a language model’s predictive\npower as its delta log likelihood ( ˆ∆llh). When\npredicting the reading time of word wt in context,\nwe will use features associated with wt and its two\npreceding words wt−1, and wt−2 to account for\nspillover effects.6 We will refer to this combined\nset of three words as our target words. The vector\nof variables in our baseline regressor xt include\nthe log unigram frequency and the length (in\ncharacters) of our target words. The variables\nfor our comparison regressor include xt plus two\nadditional variables: the surprisal and the Rényi\nentropy of our target words. Rényi entropy is a gen-\neralization of Shannon entropy which measures the\nexpected surprisal of a word, given its context, and\nhas been shown previously to impact reading times\nabove and beyond surprisal (Pimentel et al., 2023).\nResults from regressors that use just surprisal, or\n6Spillover effects, common in reading, are when properties\nof a word impact the reading times of subsequent words.\njust Rényi entropy as additional predictors are\npresented in App. A and B. To quantify the power\nof the language model, we report ˆ∆llh (as laid out\nin eq. (4)) measured across ten folds of held-out\ndata. A positive ˆ∆llh means that including surprisal\nand entropy as predictors increases predictive\npower over reading times. A negative value of\nˆ∆llh is also possible and it implies overfitting.\nEye Tracking Data. We measure ˆ∆llh on\nMECO (Siegelman et al., 2022), a multilingual\ncorpus of eye tracking data collected on simple\nWikipedia-style articles from thirteen different\nlanguages. Breaking the languages into their\nrespective families, we have eight Indo-European\nlanguages (Dutch, English, German, Greek, Italian,\nNorwegian, Russian and Spanish), two Uralic\nlanguages (Finnish, Estonian), one Afro-Asiatic\nlanguage (Hebrew), one Koreanic language\n(Korean) and one Turkic language (Turkish).\nArticles in the MECO dataset undergo a multi-step\ntranslation process to ensure that meaning was\npreserved across languages. Following previous\nstudies (Smith and Levy, 2013; Wilcox et al.,\n2020), we use each word’s gaze duration as our\nmeasure of reading time.7 For dependent variables\nin our regressors, we use cross-participant averages,\nand treat skipped words as having a reading time\nof zero (as previously done by Rayner et al., 2011;\nPimentel et al., 2023; Wilcox et al., 2023).\nLanguage Models. We train language models on\ndifferent language sections of the Wiki40B dataset\n(Guo et al., 2020). The data for each language is\npre-split into a training, validation and test set in\nWiki40B. Before training, we fit a UnigramLM\ntokenizer with a vocabulary size of 32k to each lan-\nguage’s entire training set. This tokenizer is then\nshared across all LMs within that language. We\nthen subsample each language’s training set into\nsets with 1M, 3M, 10M, 30M, 100M, 300M and\n1B tokens. We train models independently on each\nof this subsets, as well as on the entire training set\navailable for that language. All models are trained\nusing fairseq (Ott et al., 2019), following their rec-\nommended language modeling hyper-parameters.\nWe use a standard decoder-only transformer with\n6 layers, a context window size of 512 tokens, and\nshared input–output embeddings. We train our\nmodels using Adam (Kingma and Ba, 2015), with\n7Gaze duration is the amount of time between a readers\nfirst fixation on a word and the first time their gaze leaves the\nword.\n7505\nr=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001\nr=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001\nr=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001\nr=−0.92, p<0.01r=−0.92, p<0.01r=−0.92, p<0.01r=−0.92, p<0.01r=−0.92, p<0.01r=−0.92, p<0.01r=−0.92, p<0.01r=−0.92, p<0.01\nr=−0.93, p<0.001r=−0.93, p<0.001r=−0.93, p<0.001r=−0.93, p<0.001r=−0.93, p<0.001r=−0.93, p<0.001r=−0.93, p<0.001r=−0.93, p<0.001\nr=−0.78, p<0.05r=−0.78, p<0.05r=−0.78, p<0.05r=−0.78, p<0.05r=−0.78, p<0.05r=−0.78, p<0.05r=−0.78, p<0.05r=−0.78, p<0.05\nr=−0.96, p<0.001r=−0.96, p<0.001r=−0.96, p<0.001r=−0.96, p<0.001r=−0.96, p<0.001r=−0.96, p<0.001r=−0.96, p<0.001r=−0.96, p<0.001\nr=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001\nr=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001\nr=0.51, p>0.05r=0.51, p>0.05r=0.51, p>0.05r=0.51, p>0.05r=0.51, p>0.05r=0.51, p>0.05r=0.51, p>0.05r=0.51, p>0.05\nr=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001\nr=0.53, p>0.05r=0.53, p>0.05r=0.53, p>0.05r=0.53, p>0.05r=0.53, p>0.05r=0.53, p>0.05r=0.53, p>0.05r=0.53, p>0.05\nr=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001\nRussian Spanish Turkish\nGreek Hebrew Italian Korean Norwegian\nDutch Estonian English Finnish German\n4 6 8 10 12 4 6 8 10 12 4 6 8 10 12\n4 6 8 10 12 4 6 8 10 12\n0.00\n0.01\n0.02\n0.03\n0.04\n−0.005\n0.000\n0.005\n0.010\n−0.010\n−0.005\n0.000\n0.005\n0.00\n0.01\n0.02\n0.03\n0.00\n0.01\n0.02\n0.03\n0.00\n0.02\n0.04\n0.00\n0.01\n0.02\n0.03\n−0.01\n0.00\n0.01\n0.02\n0.03\n0.04\n0.000\n0.005\n0.010\n0.015\n0.000\n0.005\n0.010\n0.015\n0.020\n0.00\n0.01\n0.02\n−0.005\n0.000\n0.005\n0.010\n0.015\n0.020\n0.00\n0.01\n0.02\n0.03\nCross Entropy (on MECO)\nDelta Log Likelihood (average  per word)\n7 8 9\nLog Training \n     Tokens\nFigure 1: Results. Error bars are 95% confidence intervals on heldout data. Log training tokens is given in base 10. Fits are\nlinear lines of best-fit and 95% confidence intervals. Blue labels show the r- and p-value of a Pearson correlation test between\ncross entropy and delta log-likelihood. We find a negative correlation in 11 out of 13 languages tested.\na learning rate of 5e-4, 4000 warm-up updates, and\ndropout of 0.1. We evaluate our models after each\nfull epoch on their respective validation sets, using\nearly stopping with a patience of 3 epochs. As our\nmeasure of language model quality, we report their\ncross entropy on the MECO dataset.\n4 Results of the Crosslinguistic Analysis\nThe results for our crosslinguistic analysis can be\nseen in Fig. 1, with the languages in the different\nfacets, H✓→θ on the x-axis and ˆ∆llh on the y-axis.\nFirst, we note that the majority of language models\nare able to achieve ˆ∆llh above zero, indicating that\nsurprisal and entropy values from the models are\nhelpful in predicting reading times regardless of\ntheir size. This is in line with previous work that\nhas found that surprisal and entropy have psycho-\nmetric predictive power. That being said, the sur-\nprisal and entropy from our worse models (which\nwere trained on only 1 million words) typically do\nnot lead to positive ˆ∆llh, which is not unexpected\nfrom models that are such poor estimates of p✓ (as\nevinced by their large cross entropy).\nTurning to the relationship between cross\nentropy and ˆ∆llh, visually, it is quite clear that\nthe majority of languages exhibit a positive\nrelationship between language model quality and\npsychometric predictive power, i.e., a negative\nrelationship between ˆ∆llh and cross entropy. To\ntest this trend statistically, we fit linear regression\nmodels with ˆ∆llh as the sole dependent variable\nand cross entropy as a predictor. We find a\nsignificant effect of cross entropy on ˆ∆llh in 11\nout of our 13 analyzed languages. 8 We do not\nfind a significant effect of cross entropy in Finnish\nor German. Results are shown in Fig. 1, where\nwe also report the Pearson correlation coefficient\nbetween the cross entropy and mean ˆ∆llh of each\nlanguage model. The high value of rquantitatively\nconfirms the visual trends seen in Fig. 1.\nFinally, we also run a regression model on all our\ndata, pooled across languages. Here, in addition to\nthe fixed effect ofH✓→θ, we include random slopes\nand intercepts for each language. 9 In this regres-\nsion we find a significant effect of H✓→θ on ˆ∆llh\n(β = −0.0026,p< 0.01). Together, these results\nindicate that across a range of languages and model\nsizes, there is a consistent relationship between a\nlanguage model’s quality and its predictive power.\n8Dutch ( β =−0.0047, p < 0.001), English\n(β =−0.0021, p < 0.001), Greek ( β =−0.0028, p <\n0.001), Hebrew ( β =−0.0013, p < 0.001), Italian\n(β =−0.0088, p < 0.001), Norwegian ( β =−0.0020,\np < 0.001), Russian ( β =−0.0037, p <0.001), Turk-\nish ( β =−0.0031, p <0.001), Spanish ( β =−0.0016,\np <0.01), Estonian (β =−0.0008,p < 0.05) and Korean\n(β =−0.0009,p< 0.05).\n9Our lmer call was dll ∼x-ent + (x-ent | lang)\n7506\n5 Analysis of Data Leakage\nAs mentioned in the introduction, prior studies have\nfound that the relationship between LM quality and\npsychometric predictive power does not hold for\nlarge, contemporary LMs (Oh and Schuler, 2023;\nShain et al., 2022). One potential concern for both\nof these studies is that they used models whose\ntraining data is not publicly available, and is so\nlarge in scale, that it quite feasibly suffers from data\nleakage. That is, as human reading time datasets\nwere publicly available, models might have been\ntrained and tested on the same materials.\nMotivation. There are a number of ways in\nwhich a model’s surprisal estimates sθfor leaked\ndata could provide a poorer fit to reading times.\nImportantly, models are likely to underestimate\nsurprisal on training data, and there could feasibly\nbe a difference in the degree to which high surprisal\nvs. low surprisal values are underestimated. Such\na difference could hurt the ability to use surprisal\nto predict RTs when restricted to linear regressors,\nwhich are the main focus of many RT analyses\nsince the relationship between RT and surprisal\nhas been observed to be linear in nature (Smith and\nLevy, 2008, 2013; Shain et al., 2022). Explicitly,\nsince the aforementioned underestimation might\ndisproportionately skew a subset of surprisal\nestimates, linear regressors would be unable to\nmodel this altered relationship.\nExperimental Setup. To assess whether leakage\ncould be a confounding factors in previous\nevidence against the QP hypothesis, we train\nmodels with different amounts of leaked data and\ninvestigate its impact on ˆ∆llh. To do so, we create\nversions of our full (i.e., not subsampled) training\ndatasets in each language that include 50% or\n100% of the MECO materials for that language.\nWe then evaluate these models on MECO as before.\nResults. Results for this experiment can be seen\nin Fig. 2, which shows the average ˆ∆llh across\nlanguages. The average ˆ∆llh for each amount of\nleakage is also printed above the bars. Although\nthe version of MECO with 0% leaked data does\nachieve the highest ˆ∆llh, the confidence intervals\noverlap with each other. To confirm this statisti-\ncally, we first compute delta log likelihood esti-\nmated for each word–RT pair in our corpus:\nˆδllh = log pϕ(r|xt,sθ)) −log pϕ(r|xt) (5)\n0.01758 0.0172 0.01826\n0.000\n0.005\n0.010\n0.015\n0.020\n100% 50% 0%\nPercent MECO Corpus in Training Data\nDelta Log Likelihood\n(average per word)\nFigure 2: Results of the Data Leakage Experiment: Error\nbars are 95% confidence intervals averaged across languages.\nAverage values of ˆ∆llh in each category are printed above the\nbars.\nand then fit a mixed-effects regression model with\nthese ˆδllh as the response variable and a single\ncategorical fixed effect indicating whether the ˆδllh\nwas derived from a data-leaked model or not. We\nrun separate tests comparing between 0% and 50%\nand comparing between 0% and 100% leakage.\nWe included random slopes and intercepts for each\nlanguage. The degree of data leakage was not a\nsignificant predictor of ˆδllh (p> 0.05). Although\nour results can not be taken to show, definitively,\nthat the breakdown in the QP relationship observed\nby Shain et al. (2022) and Oh and Schuler (2023) is\nor is not due to data leakage, they suggest that this\nis likely not the primary cause. We thus encourage\nresearchers to explore other hypotheses about why\nthese larger models appear to be poorer predictors\nof human reading behavior.\n6 Conclusion\nThis paper investigates the QP hypothesis, i.e., the\nrelationship between a language model’s quality\nand its psychometric predictive power. It looks\nspecifically at two settings in which there is some\nevidence against the hypotheses by providing (i) a\ncrosslinguistic assessment of its predictions and (ii)\nan investigation of the role of data leakage in model\npredictive power. Although our results demonstrate\nthat, at small- and medium-sized data scales, the\nQP hypothesis holds in the large majority of lan-\nguages tested, our results do raise some questions.\nPerhaps the most pressing of these has to do with\nour results for Finnish and German, which showed\nnegative (albeit non-significant) correlations be-\ntween quality and power. Further testing in these\ntwo languages, thus, is an important next step for\nfuture research.\n7507\nLimitations\nOne empirical limitation of this study is that we did\nnot assess the QP hypothesis in Japanese (which is\nnot in the MECO dataset). Negative results in that\nlanguage were one of the primary motivations for\nthis work. Assuming that the negative Japanese re-\nsults still hold, what should we make of this? One\npossibility is that the results from Japanese might\nbe due to its writing system, which combines syl-\nlabaries (the kana) and logosyllabic characters (the\nkanji). All of the scripts investigated here either use\nalphabets or, in the case of Korean, blends between\nalphabets and syllabaries. Our results strongly sug-\ngest that the QP hypothesis does hold across lan-\nguages that share a common approach to writing.\nA next logical step would be to test this hypothesis\nacross writing systems in a more systematic way.\nEthics Statement\nThe MECO dataset required human participation.\nWe refer the reader to the associated publication\nfor their ethical guidelines in subject interactions.\nWe do not foresee any ethical issues with our work.\nAcknowledgements\nEGW would like to acknowledge support from an\nETH Postdoctoral Fellowship. CM is funded by a\nGoogle PhD Fellowship. TP is funded by a Face-\nbook PhD Fellowship.\nReferences\nDamián E. Blasi, Joseph Henrich, Evangelia Adamou,\nDavid Kemmerer, and Asifa Majid. 2022. Over-\nreliance on English hinders cognitive science. Trends\nin Cognitive Sciences, 26(12):1153–1170.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. PaLM: Scaling language\nmodeling with pathways. CoRR, abs/2204.02311.\nThomas M. Cover and Joy A. Thomas. 2006. Ele-\nments of Information Theory, second edition. Wiley-\nInterscience.\nAndrea de Varda and Marco Marelli. 2022. The effects\nof surprisal across languages: Results from native\nand non-native reading. In Findings of the Associa-\ntion for Computational Linguistics: AACL-IJCNLP\n2022, pages 138–144, Online only. Association for\nComputational Linguistics.\nAndrea de Varda and Marco Marelli. 2023. Scaling\nin cognitive modelling: a multilingual approach to\nhuman reading times. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 139–\n149, Toronto, Canada. Association for Computational\nLinguistics.\nStefan L. Frank and Rens Bod. 2011. Insensitivity of the\nhuman sentence-processing system to hierarchical\nstructure. Psychological Science, 22(6):829–834.\nAdam Goodkind and Klinton Bicknell. 2018. Predictive\npower of word surprisal for reading times is a linear\nfunction of language model quality. In Proceedings\nof the 8th Workshop on Cognitive Modeling and Com-\nputational Linguistics (CMCL 2018), pages 10–18,\nSalt Lake City, Utah. Association for Computational\nLinguistics.\nMandy Guo, Zihang Dai, Denny Vrandeˇci´c, and Rami\nAl-Rfou. 2020. Wiki-40B: Multilingual language\nmodel dataset. In Proceedings of the 12th Language\nResources and Evaluation Conference, pages 2440–\n2452, Marseille, France. European Language Re-\nsources Association.\nJohn Hale. 2001. A probabilistic Earley parser as a psy-\ncholinguistic model. In Second Meeting of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics, pages 1–8.\nJohn Hale. 2003. The information conveyed by words\nin sentences. Journal of Psycholinguistic Research,\n32(2):101–123.\nJohn Hale. 2016. Information-theoretical complex-\nity metrics. Language and Linguistics Compass,\n10(9):397–412.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and\nLaurent Sifre. 2022. Training compute-optimal large\nlanguage models. arXiv preprint arXiv:2203.15556.\n7508\nJacob Louis Hoover, Morgan Sonderegger, Steven T.\nPiantadosi, and Timothy J. O’Donnell. 2022. The\nplausibility of sampling as an algorithmic theory of\nsentence processing. PsyArXiv preprint.\nFrank Keller. 2004. The entropy rate principle as a\npredictor of processing effort: An evaluation against\neye-tracking data. In Proceedings of the 2004 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 317–324, Barcelona, Spain. Asso-\nciation for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nGina R. Kuperberg and T. Florian Jaeger. 2016. What do\nwe mean by prediction in language comprehension?\nLanguage, Cognition and Neuroscience, 31(1):32–\n59.\nTatsuki Kuribayashi, Yohei Oseki, Ana Brassard, and\nKentaro Inui. 2022. Context limitations make neural\nlanguage models more human-like. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 10421–10436,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nTatsuki Kuribayashi, Yohei Oseki, Takumi Ito, Ryo\nYoshida, Masayuki Asahara, and Kentaro Inui. 2021.\nLower perplexity is not always human-like. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 5203–5217,\nOnline. Association for Computational Linguistics.\nRoger Levy. 2008. Expectation-based syntactic compre-\nhension. Cognition, 106(3):1126–1177.\nClara Meister, Tiago Pimentel, Thomas Clark, Ryan\nCotterell, and Roger Levy. 2022. Analyzing wrap-\nup effects through an information-theoretic lens. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 20–28, Dublin, Ireland. Associ-\nation for Computational Linguistics.\nClara Meister, Tiago Pimentel, Patrick Haller, Lena\nJäger, Ryan Cotterell, and Roger Levy. 2021. Revis-\niting the uniform information density hypothesis. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, Online.\nAssociation for Computational Linguistics.\nByung-Doh Oh, Christian Clark, and William Schuler.\n2021. Surprisal estimators for human reading times\nneed character models. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3746–3757, Online. Association\nfor Computational Linguistics.\nByung-Doh Oh and William Schuler. 2022. Entropy-\nand distance-based predictors from GPT-2 attention\npatterns predict reading times over and above GPT-2\nsurprisal. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9324–9334, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nByung-Doh Oh and William Schuler. 2023. Why does\nsurprisal from larger transformer-based language\nmodels provide a poorer fit to human reading times?\nTransactions of the Association for Computational\nLinguistics, 11:336–350.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 48–53, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nTiago Pimentel, Clara Meister, Ethan G. Wilcox, Roger\nLevy, and Ryan Cotterell. 2023. On the effect of\nanticipation on reading times. Transactions of the\nAssociation for Computational Linguistics.\nKeith Rayner, Timothy J. Slattery, Denis Drieghe, and\nSimon P. Liversedge. 2011. Eye movements and\nword skipping during reading: Effects of word length\nand predictability. Journal of Experimental Psychol-\nogy: Human Perception and Performance, 37(2):514–\n528.\nCory Shain. 2019. A large-scale study of the effects\nof word frequency and predictability in naturalistic\nreading. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4086–4094, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nCory Shain. 2021. CDRNN: Discovering complex dy-\nnamics in human language processing. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3718–3734, Online.\nAssociation for Computational Linguistics.\nCory Shain, Clara Meister, Tiago Pimentel, Ryan Cot-\nterell, and Roger Levy. 2022. Large-scale evidence\nfor logarithmic effects of word predictability on read-\ning time. PsyArXiv preprint.\nCory Shain and William Schuler. 2021. Continuous-\ntime deconvolutional regression for psycholinguistic\nmodeling. Cognition, 215:104735.\nCory Shain and William Schuler. 2022. A deep learn-\ning approach to analyzing continuous-time systems.\narXiv preprint arXiv:2209.12128.\n7509\nClaude E. Shannon. 1948. A mathematical theory of\ncommunication. The Bell System Technical Journal,\n27(3):379–423.\nNoam Siegelman, Sascha Schroeder, Cengiz Acartürk,\nHee-Don Ahn, Svetlana Alexeeva, Simona Amenta,\nRaymond Bertram, Rolando Bonandrini, Marc Brys-\nbaert, Daria Chernova, Sara Maria Da Fonseca, Nico-\nlas Dirix, Wouter Duyck, Argyro Fella, Ram Frost,\nCarolina A. Gattei, Areti Kalaitzi, Nayoung Kwon,\nKaidi Lõo, Marco Marelli, Timothy C. Papadopou-\nlos, Athanassios Protopapas, Satu Savo, Diego E.\nShalom, Natalia Slioussar, Roni Stein, Longjiao Sui,\nAnalí Taboh, Veronica Tønnesen, Kerem Alp Usal,\nand Victor Kuperman. 2022. Expanding horizons\nof cross-linguistic research on reading: The multi-\nlingual eye-movement corpus (MECO). Behavior\nResearch Methods, 54:2843–2863.\nNathaniel J. Smith and Roger Levy. 2008. Optimal\nprocessing times in reading: a formal model and em-\npirical investigation. In Proceedings of the Cognitive\nScience Society, volume 30, pages 595–600.\nNathaniel J. Smith and Roger Levy. 2013. The effect\nof word predictability on reading time is logarithmic.\nCognition, 128(3):302–319.\nMarten van Schijndel and Tal Linzen. 2018. A neural\nmodel of adaptation in reading. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4704–4710, Brussels,\nBelgium. Association for Computational Linguistics.\nMarten van Schijndel and William Schuler. 2016. Ad-\ndressing surprisal deficiencies in reading time models.\nIn Proceedings of the Workshop on Computational\nLinguistics for Linguistic Complexity (CL4LC), pages\n32–37, Osaka, Japan. The COLING 2016 Organizing\nCommittee.\nEthan G. Wilcox, Tiago Pimentel, Clara Meister, Ryan\nCotterell, and Roger Levy. 2023. Testing the predic-\ntions of surprisal theory in 11 languages. Transac-\ntions of the Association for Computational Linguis-\ntics.\nEthan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, Peng\nQian, and Roger Levy. 2020. On the predictive power\nof neural language models for human real-time com-\nprehension behavior. In Proceedings of the Cognitive\nScience Society.\nA Results with Surprisal\nIn order to facilitate a closer comparison with pre-\nvious analyses, which have not looked at the effect\nof entropy (e.g., Goodkind and Bicknell, 2018), we\nconduct a version of our analysis with surprisal as\nthe sole non-baseline predictor. Results are pre-\nsented in Fig. 3. As with the results in the main\nsection, we find a negative relationship between\nˆ∆llh and H✓→θ across languages. To test this trend\nstatistically, we fit linear regression models with\nˆ∆llh as the response variable and H✓→θ as the sole\npredictor variable. We find a significant effect of\nH✓→θ on ˆ∆llh in Dutch, Estonian, Greek, Italian,\nNorwegian, Russian, Turkish (p< 0.001), English,\nHebrew and Spanish (p< 0.05), but not in Finnish\nor Korean. In German, we find a positive effect of\nˆ∆llh on H✓→θ (p< 0.05). As before, we also run\na mixed-effects regression model that tests the ef-\nfect cross-linguistically with random intercepts and\nslopes for language and log-number of training to-\nkens. We find a significant effect of ˆ∆llh on H✓→θ\n(p <0.05). Overall, these results are consistent\nwith the results in the main body of the paper.\nB Results with Entropy\nAdditionally, we conduct a version of our analy-\nsis that includes only (Rényi) entropy as a non-\nbaseline predictor. Results are presented in Fig. 4.\nAgain, we find that the majority of languages show\na negative relationship between H✓→θ and ˆ∆llh.\nConducting the same statistical analysis as dis-\ncussed in the main body of the paper, we find a sig-\nnificant effect of ˆ∆llh on H✓→θ in Dutch, Korean,\nRussian, Turkish (p <0.001), English, German,\nGreek, Italian (p< 0.01), Norwegian, Hebrew and\nSpanish (p <0.05), but no effect in Finnish. We\nfind a positive effect in Estonian (p< 0.001). We\nalso run a mixed-effects regression model that tests\nthe effect cross-linguistically with random inter-\ncepts and slopes for language and log-number of\ntraining tokens. We find a significant effect of ˆ∆llh\non H✓→θ (p< 0.05).\n7510\nr=−0.94, p<0.001r=−0.94, p<0.001r=−0.94, p<0.001r=−0.94, p<0.001r=−0.94, p<0.001r=−0.94, p<0.001r=−0.94, p<0.001r=−0.94, p<0.001\nr=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001\nr=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001\nr=−0.99, p<0.001r=−0.99, p<0.001r=−0.99, p<0.001r=−0.99, p<0.001r=−0.99, p<0.001r=−0.99, p<0.001r=−0.99, p<0.001r=−0.99, p<0.001\nr=−0.73, p<0.05r=−0.73, p<0.05r=−0.73, p<0.05r=−0.73, p<0.05r=−0.73, p<0.05r=−0.73, p<0.05r=−0.73, p<0.05r=−0.73, p<0.05\nr=−0.77, p<0.05r=−0.77, p<0.05r=−0.77, p<0.05r=−0.77, p<0.05r=−0.77, p<0.05r=−0.77, p<0.05r=−0.77, p<0.05r=−0.77, p<0.05\nr=−0.8, p<0.05r=−0.8, p<0.05r=−0.8, p<0.05r=−0.8, p<0.05r=−0.8, p<0.05r=−0.8, p<0.05r=−0.8, p<0.05r=−0.8, p<0.05\nr=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001r=−0.95, p<0.001\nr=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001\nr=0.42, p>0.05r=0.42, p>0.05r=0.42, p>0.05r=0.42, p>0.05r=0.42, p>0.05r=0.42, p>0.05r=0.42, p>0.05r=0.42, p>0.05\nr=0.66, p>0.05r=0.66, p>0.05r=0.66, p>0.05r=0.66, p>0.05r=0.66, p>0.05r=0.66, p>0.05r=0.66, p>0.05r=0.66, p>0.05\nr=0.74, p<0.05r=0.74, p<0.05r=0.74, p<0.05r=0.74, p<0.05r=0.74, p<0.05r=0.74, p<0.05r=0.74, p<0.05r=0.74, p<0.05\nr=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001r=−0.98, p<0.001\nRussian Spanish Turkish\nGreek Hebrew Italian Korean Norwegian\nDutch Estonian English Finnish German\n4 6 8 10 12 4 6 8 10 12 4 6 8 10 12\n4 6 8 10 12 4 6 8 10 12\n0.00\n0.01\n0.02\n0.03\n0.04\n−0.005\n0.000\n0.005\n−0.010\n−0.005\n0.000\n0.005\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.000\n0.005\n0.010\n0.015\n0.020\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.00\n0.01\n0.02\n−0.01\n0.00\n0.01\n0.02\n0.03\n0.04\n0.000\n0.005\n0.010\n0.000\n0.005\n0.010\n0.015\n0.020\n0.00\n0.01\n0.02\n−0.005\n0.000\n0.005\n0.010\n0.015\n0.020\n0.00\n0.01\n0.02\n0.03\nCross Entropy (on MECO)\nDelta Log Likelihood (average  per word)\n7 8 9\nLog Training \n     Tokens\nFigure 3: Results with Surprisal Only: Error bars are 95% confidence intervals on heldout data. Fits are linear lines of best-fit\nand 95% confidence intervals. Blue labels show the r- and p-value of a Pearson correlation test between cross entropy and delta\nlog-likelihood. We find a negative correlation in 10 out of 13 languages tested.\nr=−0.93, p<0.001r=−0.93, p<0.001r=−0.93, p<0.001r=−0.93, p<0.001r=−0.93, p<0.001r=−0.93, p<0.001r=−0.93, p<0.001r=−0.93, p<0.001\nr=−0.9, p<0.01r=−0.9, p<0.01r=−0.9, p<0.01r=−0.9, p<0.01r=−0.9, p<0.01r=−0.9, p<0.01r=−0.9, p<0.01r=−0.9, p<0.01\nr=−0.94, p<0.001r=−0.94, p<0.001r=−0.94, p<0.001r=−0.94, p<0.001r=−0.94, p<0.001r=−0.94, p<0.001r=−0.94, p<0.001r=−0.94, p<0.001\nr=0.96, p<0.001r=0.96, p<0.001r=0.96, p<0.001r=0.96, p<0.001r=0.96, p<0.001r=0.96, p<0.001r=0.96, p<0.001r=0.96, p<0.001\nr=−0.73, p<0.05r=−0.73, p<0.05r=−0.73, p<0.05r=−0.73, p<0.05r=−0.73, p<0.05r=−0.73, p<0.05r=−0.73, p<0.05r=−0.73, p<0.05\nr=−0.78, p<0.05r=−0.78, p<0.05r=−0.78, p<0.05r=−0.78, p<0.05r=−0.78, p<0.05r=−0.78, p<0.05r=−0.78, p<0.05r=−0.78, p<0.05\nr=−0.88, p<0.01r=−0.88, p<0.01r=−0.88, p<0.01r=−0.88, p<0.01r=−0.88, p<0.01r=−0.88, p<0.01r=−0.88, p<0.01r=−0.88, p<0.01\nr=−0.87, p<0.01r=−0.87, p<0.01r=−0.87, p<0.01r=−0.87, p<0.01r=−0.87, p<0.01r=−0.87, p<0.01r=−0.87, p<0.01r=−0.87, p<0.01\nr=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001r=−0.97, p<0.001\nr=0.66, p>0.05r=0.66, p>0.05r=0.66, p>0.05r=0.66, p>0.05r=0.66, p>0.05r=0.66, p>0.05r=0.66, p>0.05r=0.66, p>0.05\nr=−0.99, p<0.001r=−0.99, p<0.001r=−0.99, p<0.001r=−0.99, p<0.001r=−0.99, p<0.001r=−0.99, p<0.001r=−0.99, p<0.001r=−0.99, p<0.001\nr=−0.87, p<0.01r=−0.87, p<0.01r=−0.87, p<0.01r=−0.87, p<0.01r=−0.87, p<0.01r=−0.87, p<0.01r=−0.87, p<0.01r=−0.87, p<0.01\nr=−0.74, p<0.05r=−0.74, p<0.05r=−0.74, p<0.05r=−0.74, p<0.05r=−0.74, p<0.05r=−0.74, p<0.05r=−0.74, p<0.05r=−0.74, p<0.05\nRussian Spanish Turkish\nGreek Hebrew Italian Korean Norwegian\nDutch Estonian English Finnish German\n4 6 8 10 12 4 6 8 10 12 4 6 8 10 12\n4 6 8 10 12 4 6 8 10 12\n0.000\n0.005\n0.010\n−0.002\n0.000\n0.002\n0.004\n−0.008\n−0.004\n0.000\n0.004\n0.00\n0.01\n0.02\n0.000\n0.005\n0.010\n0.015\n−0.005\n0.000\n0.005\n0.010\n−0.005\n0.000\n0.005\n0.010\n0.015\n−0.005\n0.000\n0.005\n−0.0025\n0.0000\n0.0025\n0.0050\n0.0075\n−0.003\n−0.002\n−0.001\n0.000\n0.001\n0.002\n0.000\n0.005\n0.010\n−0.002\n0.000\n0.002\n−0.004\n0.000\n0.004\n0.008\nCross Entropy (on MECO)\nDelta Log Likelihood (average  per word)\n7 8 9\nLog Training \n     Tokens\nFigure 4: Results for Entropy Only: Target models include Rényi entropy with α= 0.5 as an additional predictor. We find a\nnegative correlation in 11 out of 13 languages tested.\n7511",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6596637964248657
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6130546927452087
    },
    {
      "name": "Predictive power",
      "score": 0.5539110898971558
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5456122159957886
    },
    {
      "name": "Language model",
      "score": 0.5422365069389343
    },
    {
      "name": "Word (group theory)",
      "score": 0.5088732242584229
    },
    {
      "name": "Premise",
      "score": 0.4943242073059082
    },
    {
      "name": "Natural language processing",
      "score": 0.4777628481388092
    },
    {
      "name": "Reading (process)",
      "score": 0.45649778842926025
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42097777128219604
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3388981819152832
    },
    {
      "name": "Linguistics",
      "score": 0.29470616579055786
    },
    {
      "name": "Psychology",
      "score": 0.2905462384223938
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}