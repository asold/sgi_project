{
  "title": "JUST-BLUE at SemEval-2021 Task 1: Predicting Lexical Complexity using BERT and RoBERTa Pre-trained Language Models",
  "url": "https://openalex.org/W3184418820",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4229213298",
      "name": "Tuqa Bani Yaseen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3175618064",
      "name": "Qusai Ismail",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3024393918",
      "name": "Sarah Al-Omari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284520692",
      "name": "Eslam Al-Sobh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2621117837",
      "name": "Malak Abdullah",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2807091229",
    "https://openalex.org/W2963510222",
    "https://openalex.org/W3114790042",
    "https://openalex.org/W2251814648",
    "https://openalex.org/W2473989358",
    "https://openalex.org/W3088946171",
    "https://openalex.org/W2951520515",
    "https://openalex.org/W3004442222",
    "https://openalex.org/W3117622096",
    "https://openalex.org/W4302040887",
    "https://openalex.org/W3023401576",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3185421637",
    "https://openalex.org/W2345003182",
    "https://openalex.org/W3115373420",
    "https://openalex.org/W2963733418"
  ],
  "abstract": "Predicting the complexity level of a word or a phrase is considered a challenging task. It is even recognized as a crucial step in numerous NLP applications, such as text rearrangements and text simplification. Early research treated the task as a binary classification task, where the systems anticipated the existence of a word’s complexity (complex versus uncomplicated). Other studies had been designed to assess the level of word complexity using regression models or multi-labeling classification models. Deep learning models show a significant improvement over machine learning models with the rise of transfer learning and pre-trained language models. This paper presents our approach that won the first rank in the SemEval-task1 (sub stask1). We have calculated the degree of word complexity from 0-1 within a text. We have been ranked first place in the competition using the pre-trained language models Bert and RoBERTa, with a Pearson correlation score of 0.788.",
  "full_text": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 661–666\nBangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics\n661\nJUST-BLUE at SemEval-2021 Task 1: Predicting Lexical Complexity\nusing BERT and RoBERTa Pre-trained Language Models\nTuqa Bani Yaseen Qusai Ismail Sarah Al-Omari Eslam Al-Sobh Malak Abdullah\nJordan University of Science and Technology\nIrbid Jordan\ntmbaniyaseen19, qﬁsmail20, ssalomari18, esalsobh19 @ cit.just.edu.jo\nmabdullah@just.edu.jo\nAbstract\nPredicting the complexity level of a word or\na phrase is considered a challenging task. It\nis even recognized as a crucial step in numer-\nous NLP applications, such as text rearrange-\nments and text simpliﬁcation. Early research\ntreated the task as a binary classiﬁcation task,\nwhere the systems anticipated the existence of\na word’s complexity (complex versus uncom-\nplicated). Other studies had been designed\nto assess the level of word complexity using\nregression models or multi-labeling classiﬁca-\ntion models. Deep learning models show a\nsigniﬁcant improvement over machine learn-\ning models with the rise of transfer learning\nand pre-trained language models. This paper\npresents our approach that won the ﬁrst rank\nin the SemEval-task1 (sub stask1). We have\ncalculated the degree of word complexity from\n0-1 within a text. We have been ranked ﬁrst\nplace in the competition using the pre-trained\nlanguage models BERT and RoBERTa, with a\nPearson correlation score of 0.788.\nkeywords: Neuro-linguistic programming\n(NLP), Lexical Complexity Prediction(LCP),\nDeep Learning, RoBERTa, BERT.\n1 Introduction\nLexical complexity plays a signiﬁcant role in the\nreadability level and comprehension. The precise\nanticipation of lexical complexity can help systems\ndirect the user to an acceptable simple text accu-\nrately or modify the text to be more ﬂuid (Broth-\ners and Traxler, 2016). Predicting the complexity\nof words is a subjective and challenging problem,\nwhile it is conjectural, too. Yet, mapping words\ninto their complexity is an essential task to under-\nstand natural language. Numerous components can\ninﬂuence the prediction of lexical complexity. Sev-\neral approaches were proposed to solve or mitigate\nthis type of study using Machine and Deep learn-\ning methods (Sengupta et al., 2020; Gooding and\nKochmar, 2019; Bahja, 2020).\nThis paper describes the JUST-BLUE team’s\nmodel that participated in the SemEval 2021-task1,\nLexical Complexity Prediction (LCP) (Shardlow\net al., 2021). The task provides participants with an\naugmented version of CompLex, a multi-domain\nEnglish dataset with sentences annotated using\na 5-point Likert scale (1-5) (from very easy to\nvery difﬁcult) (Shardlow et al., 2020). The task\nis to predict the complexity value of words in con-\ntext. It is worth mentioning that our model, JUST-\nBLUE, has been ranked ﬁrst in this task. We have\nused the pre-trained language models, BERT and\nRoBERTa Which have proven their effectiveness\nin this area (Liu et al., 2019), along with the ensem-\nbling method (weighted averaging) to achieve the\nhighest Pearson correlation score of 0.788.\nThe rest of this paper is organized as follows:\nSection 2 sheds light on related work. Section 3 de-\nscribes the methodology proposed in this research.\nSection 4 discusses the experimentation setup and\nevaluation results. Whereas Section 5 concludes\nthis research.\n2 Related work\nOne of the most prominent challenges in the current\nera is the prediction of lexical complexity. Predic-\ntion of the word complexity in machine learning\ncan be binary; the word is complex or not com-\nplex. It also can be a non-binary prediction, as\na probabilistic prediction with the measurement\nof complexity within a particular scale (0.6 the\nprobability that the word is complex). SemEval\n2016 introduced the ﬁrst shared task of predict-\ning word complexity with a mission limited to the\nword orders being complex or non-complex (binary\nprediction) (Paetzold and Specia, 2016). Decision\nTree classiﬁers achieved the best results (Zampieri\n662\net al., 2017). It has been noted that word length is\na good indication of word complexity (De Hertog\nand Tack, 2018).\nThe authors in (Shardlow, 2013) discussed the\nimportance of frequency and length of words. They\nused the Keras deep learning library to predict\nwhether an English or Spanish word is complex or\nnot. They used character embedding, word length,\nfrequency count, word embedding, and psycho-\nlogical measures as features to predict complex\nwords and achieved 0.872 as F1-score. The au-\nthors in (Yimam et al., 2018) worked on various\nlanguages, such as English, Spanish, French and\nGerman. They worked on two different methods\nfor predicting complex words. The ﬁrst method\nis to ﬁnd if the word orders are either complex or\nsimple. The second is to ﬁnd the probability that\nthe word is complex. The complex levels depended\non the average of the annotators’ answers. For ex-\nample, if the number of annotators who expected\nthe word to be complex is 6 out of 10, then the\nprobability is 0.6. A claim stated that this anno-\ntating method is considered impractical since the\nprobability of 0.5 cannot be considered complex\nor not complex. So the authors in (Shardlow et al.,\n2020) suggested a Likert scale with 5-point. The\nauthors asserted that this method is more accurate\nscale instead of calling the word complex and non-\ncomplex. We can divide the word into being very\neasy, easy, neutral, difﬁcult, and very difﬁcult. This\nscale is beneﬁcial to our work.\nThe deep learning pre-trained language models,\nBERT and RoBERTa, are considered state-of-the-\nart for NLP. Teams in the previous shared tasks\nof SemEval 2020 had used these models to obtain\nthe best results for different NLP tasks (Al-Khdour\net al., 2020; Shatnawi et al., 2020; Jurkiewicz et al.,\n2020). Our approach experimented with these mod-\nels using different hyperparameters and weighted\naveraging methods that lead to the best result in the\ncompetition for predicting lexical complexity.\n3 Methodology\nThis section describes our approach methodology\nand goes as follows: First, we describe the task’s\ndataset. Then, the preprocessing step. Finally, we\ndescribe the JUST-BLUE approach to predict the\nword’s complexity.\n3.1 Data\nThe SemEval-task 1 competition has provided the\ncontestants with three ﬁles (trial, train, and test\ndata). The ﬁles contain several columns as follows:\n• id: the identiﬁcation number for each entry.\n• corpus: the sources from which the words\nwere being collected. It was extracted from\nthree sources: the bible, biomedical, and The\nEuropean Parliament.\n• sentence: the set of words for which complex-\nity needed to be measured.\n• token: the single word in which complexity\nneeded to be measured.\n• complexity: the degree of complexity of the\nword, ranging from 0 to 1.\n3.2 Pre-Processing Step\nFirst, we cleaned the data and removed all single\nand double quotations manually. This step helped\nto separate some of the merged rows. Next, we\ndeleted any row where columns contain the NaN\nvalue because it will not be effective in the training\nprocess.\n3.3 JUST-BLUE Architecture\nWe have used the pre-trained language models,\nBERT and RoBERTa models. We have imported\nthe BERT model using BERT-sklearn library as it\nincludes SciBERT and BioBERT models for the\nscientiﬁc and biomedical ﬁelds. We also have used\nsimple transformers; classiﬁcation libraries to im-\nport the RoBERTa model. As we mentioned earlier,\nthe goal of the task is to determine the complexity\nof the word. Knowing that the word’s complex-\nity changes slightly based on the complexity of\nthe sentence, we have used both the token (word)\nand the sentence to predict the word’s complexity.\nWe have fed BERT and RoBERTa models with the\n’token,’ and the ’complexity’ label to be trained.\nWe have also inserted ’sentence’ and ’complexity’\ncolumns to both models for training as a second\nstrategy. The results have been combined using\nan ensembling voting method, Weighted Averag-\ning. Our experiments show that the 80:20 ratio for\nweights can achieve the best results. The highest\nvoting rate is for the ”token” model (model 1) since\nwe need to calculate the degree of complexity for a\nsingle word. On the other hand, the complexity of a\n663\nFigure 1: JUST-BLUE workﬂow\nword is affected by the complexity of the sentence\nin which it is included. So, we gave a 20\nThe Simple Averaging method has been used\nas the ensembling technique to merge BERT and\nRoBERTa’s models’ results. Figure 1 illustrates the\nmethodology used.\nFor more clariﬁcation, suppose we have the word\n’sea’ for which we want to calculate the complex-\nity. The ’sea’ word exists in this sentence ”and\nthey entered into the boat, and were going over the\nsea to Capernaum.” First, we feed the word sea to\nmodel1 using RoBERTa. We also feed the sentence\nthat contains the word sea to RoBERTa model2.\nThen, we combine the two results obtained using\nWeighted Averaging. Suppose that the RoBERTa\nmodel1 result is 0.01 (the word sea has a 0.01 com-\nplexity degree) and RoBERTa model2 is 0.13 ( the\nsentence has a 0.13 complexity degree). The re-\nsulted RoBERTa models is 0.01x80% + 0.13x20%,\nwhich is equal to 0.034. We repeat these steps for\nBERT’s models. If the BERT model has a result of\n0.052, then the ﬁnal step is to calculate the average\nof the RoBERTa and BERT model. The complexity\nis (0.034 + 0.052)/2, equal to 0.043, as shown in\nFigure 2.\n4 Results and Discussion\nWe used Python version 3.6 on the Colab environ-\nment to execute our codes. We have experimented\nFigure 2: Example Description\nwith several models to determine which models\nare suitable for this task. We have experimented\nwith BERT and RoBERTa pre-trained models. We\nalso examined SVM and Random Forest machine\nlearning models. Table 1 shows the results we have\nobtained throughout our experiments.\nThe challenging step was to ﬁnd the best weights\nfor the models that used tokens (single words) and\nsentences to get the best result (Table2). As we\nmentioned earlier, some words have a different\ncomplexity degree, depending on their location in\nthe sentences. Therefore, it was necessary to insert\n664\nTable 1: Results of different models\nModels score\nSVM 0.3472\nRandom Forest 0.4503\nBERT 0.8199\nRoBERTa 0.8268\nBERT and RoBERTa 0.8190\nTable 2: Different weights (tokens and sentences)\nmodel score\nToken % Sentence %\n90 10 0.8258\n80 20 0.8268\n70 30 0.8252\neach of the words and sentences for the training\nto verify the best weight. Table 2 shows the best\nweight, which is 80% for words and 20% for sen-\ntences.\nThe next step was to explore BERT and\nRoBERTa’s best hyperparameters, such as learning\nrate, batch size, epochs, and max sequence length.\nTable 3 shows the description of these hyperpa-\nrameters, and Table 4 shows example results of\nﬁne-tuning JUST-BLUE hyperparameters.\nFinally, we thought of determining the effects\nof the base size and large size models of BERT\nand RoBERTa on the accuracy. It is shown by\nour experiments that the large sizes decreased the\naccuracy.\nIn the testing phase, we noticed that the words\n(tokens) in the ﬁle were new. Therefore, we de-\ncided to limit the number of arguments to avoid\noverﬁtting. We just changed ”num-train-epochs\n”=3 in BERT and RoBERTa’s model, but the other\narguments had the default values. We have used\nthree different models. The ﬁrst was the BERT\nmodel, the second was the RoBERTa model, and\nthe third was BERT and RoBERTa together as de-\nscribed in the Methodology Section. Table 5 shows\nthe results we received from the different models\nwe used.\nJUST-BLUE approach achieved the best result\nusing RoBERTa and BERT’s models with a Pear-\nson correlation of 0.788 scores. We have also\nachieved the least Mean Absolute Error(MAE) with\n0.0609. Our model is ranked ﬁrst the LCP-sub\ntask1 of a single word. The Spearman’s Rho (Rho)\nand R-squared (R2) scores are 0.7369 and 0.6172,\nrespectively. The number of teams in the shared\ntask Lexical Complexity Prediction (LCP) was 54\nteams. This shared task is considered a high level\nof CWI 2016 and CWI 2018 with a larger number\nof words from various sources.\n5 Conclusion\nPredicting the complexity of words is one of the\nmost prominent tasks that the NLP research com-\nmunity strives to solve. It is worth noting that in\n2016 and 2018, two tasks were issued to deter-\nmine whether the word was complex or not. Se-\nmEval 2021 introduced task 1, Lexical Complexity\nPrediction (LCP) that aims to predict the word’s\ncomplexity from 0 to 1. This paper described the\ntop-ranked team’s model, JUST-BLUE. The JUST-\nBLUE model obtained the highest Pearson Correla-\ntion score of 0.788 using the pre-trained language\nmodels BERT and RoBERTa. Our strategy depends\non the ensembling methods, Simple and Weighted\nAveraging.\nReferences\nNour Al-Khdour, Mutaz Bni Younes, Malak Abdullah,\nand AL-Smadi Mohammad. 2020. Justmasters at\nSemEval-2020 task 3: Multilingual deep learning\nmodel to predict the effect of context in word sim-\nilarity. In Proceedings of the Fourteenth Workshop\non Semantic Evaluation, pages 292–300.\nMohammed Bahja. 2020. Natural language processing\napplications in business. In E-Business. IntechOpen.\nTrevor Brothers and Matthew J Traxler. 2016. Antic-\nipating syntax during reading: Evidence from the\nboundary change paradigm. Journal of Experimen-\ntal Psychology: Learning, Memory, and Cognition,\n42(12):1894.\nDirk De Hertog and Ana ¨ıs Tack. 2018. Deep learning\narchitecture for complexword identiﬁcation. In Thir-\nteenth Workshop of Innovative Use of NLP for Build-\ning Educational Applications, pages 328–334. Asso-\nciation for Computational Linguistics (ACL); New\nOrleans, Louisiana.\nSian Gooding and Ekaterina Kochmar. 2019. Complex\nword identiﬁcation as a sequence labelling task. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1148–\n1153.\nDawid Jurkiewicz, Łukasz Borchmann, Izabela Kos-\nmala, and Filip Grali ´nski. 2020. Applicaai at\nSemEval-2020 task 11: On roberta-crf, span cls and\nwhether self-training helps them. arXiv preprint\narXiv:2005.07934.\n665\nTable 3: Description of some argument\nArgument Description\nlearning rate The learning rate for training\nnumber of epochs One forward pass and one backward pass of training examples\nmax sequence length Maximum sequence length the model will support\ntrain batch size Determines the number of samples in each mini batch\nearly stopping metric Stop training when early stopping metric doesn’t improve\nearly stopping delta Counts as a better checkpoint\nevaluate during training steps Performs evaluation at every speciﬁed number of steps\nTable 4: Hyperparameter Fine-Tuning for JUST-BLUE model in the training phase\n# learning ratenumber of epochsmax sequence lengthtrain batch sizeearly stopping metricearly stopping deltaevaluate during training stepsscore\n1 4e-5 1 128 8 mcc 0 2000 0.796245\n2 5e-5 1 64 16 eval loss 0.01 1000 0.799285\n3 4e-5 2 128 16 mcc 0.01 2000 0.80589\n4 4e-5 3 128 8 eval loss 0 2000 0.819013\n5 4e-5 3 64 8 mcc 0.01 2000 0.827414\n6 5e-5 3 32 16 eval loss 0.02 1000 0.784574\n7 4e-5 5 128 16 mcc 0 2000 0.815554\n8 3e-5 5 64 8 eval loss 0 500 00.818949\n9 4e-5 4 128 8 mcc 0.01 1000 0.827172\n10 3e-5 4 64 16 eval loss 0.02 2000 0.82385\nTable 5: Results of different models in the testing phase\nModel Score\nRoBERTa 0.7772\nBERT 0.7556\nJUST-BLUE (RoBERTa and BERT) 0.7886\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nGustavo Paetzold and Lucia Specia. 2016. SemEval\n2016 task 11: Complex word identiﬁcation. In Pro-\nceedings of the 10th International Workshop on Se-\nmantic Evaluation (SemEval-2016), pages 560–569.\nSaptarshi Sengupta, Sanchita Basak, Pallabi Saikia,\nSayak Paul, Vasilios Tsalavoutis, Frederick Atiah,\nVadlamani Ravi, and Alan Peters. 2020. A review\nof deep learning with special emphasis on architec-\ntures, applications and recent trends. Knowledge-\nBased Systems, 194:105596.\nMatthew Shardlow. 2013. A comparison of techniques\nto automatically identify complex words. In 51st\nAnnual Meeting of the Association for Computa-\ntional Linguistics Proceedings of the Student Re-\nsearch Workshop, pages 103–109.\nMatthew Shardlow, Michael Cooper, and Marcos\nZampieri. 2020. Complex—a new corpus for lexical\ncomplexity predicition from likert scale data. arXiv\npreprint arXiv:2003.07008.\nMatthew Shardlow, Richard Evans, Gustavo Paetzold,\nand Marcos Zampieri. 2021. SemEval-2021 task 1:\nLexical complexity prediction. In In Proceedings of\nthe 14th International Workshop on Semantic Evalu-\nation (SemEval-2021).\n666\nFara Shatnawi, Malak Abdullah, and Mahmoud Ham-\nmad. 2020. Mlengineer at SemEval-2020 task 7:\nBERT-ﬂair based humor detection model (bfhumor).\nIn Proceedings of the Fourteenth Workshop on Se-\nmantic Evaluation, pages 1041–1048.\nSeid Muhie Yimam, Chris Biemann, Shervin Malmasi,\nGustavo H Paetzold, Lucia Specia, Sanja ˇStajner,\nAna¨ıs Tack, and Marcos Zampieri. 2018. A report\non the complex word identiﬁcation shared task 2018.\narXiv preprint arXiv:1804.09132.\nMarcos Zampieri, Shervin Malmasi, Gustavo Paetzold,\nand Lucia Specia. 2017. Complex word identiﬁca-\ntion: Challenges in data annotation and system per-\nformance. arXiv preprint arXiv:1710.04989.",
  "topic": "SemEval",
  "concepts": [
    {
      "name": "SemEval",
      "score": 0.8366037011146545
    },
    {
      "name": "Computer science",
      "score": 0.818511962890625
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7467382550239563
    },
    {
      "name": "Natural language processing",
      "score": 0.7015677690505981
    },
    {
      "name": "Task (project management)",
      "score": 0.6824985146522522
    },
    {
      "name": "Word (group theory)",
      "score": 0.6227927207946777
    },
    {
      "name": "Binary classification",
      "score": 0.5822405219078064
    },
    {
      "name": "Phrase",
      "score": 0.5776330828666687
    },
    {
      "name": "Language model",
      "score": 0.5689157247543335
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.5364053845405579
    },
    {
      "name": "Support vector machine",
      "score": 0.18732571601867676
    },
    {
      "name": "Linguistics",
      "score": 0.17348146438598633
    },
    {
      "name": "Mathematics",
      "score": 0.08220988512039185
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}