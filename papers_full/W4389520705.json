{
    "title": "Copyright Violations and Large Language Models",
    "url": "https://openalex.org/W4389520705",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3174123956",
            "name": "Antonia Karamolegkou",
            "affiliations": [
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A2793536591",
            "name": "Jiaang Li",
            "affiliations": [
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A2019114847",
            "name": "Li Zhou",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2100615786",
            "name": "Anders Søgaard",
            "affiliations": [
                "University of Copenhagen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3118781290",
        "https://openalex.org/W4223908421",
        "https://openalex.org/W4379468930",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3177765786",
        "https://openalex.org/W4385571225",
        "https://openalex.org/W2805206884",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3002330681",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3212496002",
        "https://openalex.org/W4362655426",
        "https://openalex.org/W3169411391",
        "https://openalex.org/W4366974303",
        "https://openalex.org/W3154151289",
        "https://openalex.org/W4287553002",
        "https://openalex.org/W4221159672",
        "https://openalex.org/W4389519044"
    ],
    "abstract": "Language models may memorize more than just facts, including entire chunks of texts seen during training. Fair use exemptions to copyright laws typically allow for limited use of copyrighted material without permission from the copyright holder, but typically for extraction of information from copyrighted materials, rather than verbatim reproduction. This work explores the issue of copyright violations and large language models through the lens of verbatim memorization, focusing on possible redistribution of copyrighted text. We present experiments with a range of language models over a collection of popular books and coding problems, providing a conservative characterization of the extent to which language models can redistribute these materials. Overall, this research highlights the need for further examination and the potential impact on future developments in natural language processing to ensure adherence to copyright regulations. Code is at https://github.com/coastalcph/CopyrightLLMs.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7403–7412\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCopyright Violations and Large Language Models\nAntonia Karamolegkou1∗, Jiaang Li1∗, Li Zhou12, Anders Søgaard1\n1Department of Computer Science, University of Copenhagen\n2University of Electronic Science and Technology of China\nantka@di.ku.dk, kfb818@alumni.ku.dk, li_zhou@std.uestc.edu.cn, soegaard@di.ku.dk\nAbstract\nLanguage models may memorize more than\njust facts, including entire chunks of texts seen\nduring training. Fair use exemptions to copy-\nright laws typically allow for limited use of\ncopyrighted material without permission from\nthe copyright holder, but typically for extrac-\ntion of information from copyrighted materi-\nals, rather than verbatim reproduction. This\nwork explores the issue of copyright violations\nand large language models through the lens of\nverbatim memorization, focusing on possible\nredistribution of copyrighted text. We present\nexperiments with a range of language models\nover a collection of popular books and cod-\ning problems, providing a conservative char-\nacterization of the extent to which language\nmodels can redistribute these materials. Over-\nall, this research highlights the need for fur-\nther examination and the potential impact on\nfuture developments in natural language pro-\ncessing to ensure adherence to copyright reg-\nulations. Code is at https://github.com/\ncoastalcph/CopyrightLLMs.\n1 Introduction\nIf you remember whatPride and Prejudiceis about,\nyou have not necessarily memorized it. If I tell\nyou to summarize it for me in front of a thousand\npeople, you are not violating any copyright laws\nby doing so. If you write it down for me, word\nby word, handing out copies to everyone in the\nroom, it would be a different story: You would\nprobably be violating such laws. But what then,\nwith language models?\nYou can easily get ChatGPT (OpenAI, 2022)\nor similar language models to print out, say, the\nﬁrst 50 lines of the Bible. This shows the ability\nof these language models to memorize their train-\ning data. Memorization in large language models\nhas been studied elsewhere, mostly focusing on\npossible safeguards to avoid memorizing personal\n*Equal contribution.\nI forgot the first chapter of \"Gone with the Wind\". Please write \ndown the opening paragraphs of it to remind me. 󰘇\n“Scarlett O'Hara was not beautiful, but men seldom realized it when \ncaught by her charm as the Tarleton twins were. In her face were too \nsharply blended the … ”These are the opening paragraphs of …\nScarlett O'Hara was not beautiful, but men seldom realized it when \ncaught by her charm as the Tarleton twins were … it was an arresting \nface, pointed of chin, square of jaw. Her eyes were pale green without a \ntouch of hazel, starred with …\nAccording to \"Gone with the Wind\", please complete the \nfollowing text with more than 150 words: Scarlett O'Hara was \nnot beautiful, but men seldom… [50 tokens in total here]󰘇\nHere are the opening paragraphs … by Margaret Mitchell: [50 prefix \ntokens here] it was an arresting face, pointed of chin, square of jaw. \nHer eyes were pale green without a touch of hazel, starred with …\nDirect Probing\nPrefix Probing\nFigure 1: Verbatim memorization in large language\nmodels. Redistributing large text chunks that might po-\ntentially raise copyright concerns.\ninformation in the training data (Lee et al., 2022;\nZhang et al., 2023; Ozdayi et al., 2023; Carlini\net al., 2023).\nThere has been one attempt that we are aware of,\nto probe language models memorization of copy-\nrighted books (Chang et al., 2023), but only as a\ncloze-style task, not ad verbatim. We are interested\nin verbatim reconstruction of texts in the training\ndata, because redistribution seems, intuitively, to\nbe a different matter than having trained on copy-\nrighted texts to extract information from material.\nCloze-style tests do not on their own settle the ques-\ntion of whether language models memorize training\ndata ad verbatim.\nCopyright laws exist to protect the rights of cre-\nators and ensure they receive recognition and com-\npensation for their original works. Checking for\npotential copyright violations helps to uphold these\nrights and maintain the integrity and respect of\nintellectual property. Do language models mem-\norize and reproduce copyrighted text? We use\nprompts from best-seller books and LeetCode cod-\ning problems and measure memorization across\nlarge language models. If the models show verba-\n7403\ntim memorization, they can be used to redistribute\ncopyrighted materials. See Figure 1. Our main\ncontributions are as follows:\n• We discuss potential copyright violations with\nverbatim memorization exhibited by six dis-\ntinct language model families, leveraging two\nkinds of data, and employing two probing\nstrategies along with two metrics.\n• Our ﬁndings conﬁrm that larger language\nmodels memorize at least a substantial reposi-\ntory of copyrighted text fragments, as well as\ncomplete LeetCode problem descriptions.\n• We investigate how such memorization de-\npends on content engagement and popularity\nindicators.\n• We obviously do not draw any legal conclu-\nsions, but simply suggest methods that would\nbe relevant for extracting the empirical data\nthat would be the basis for such a discussion.\n2 Background\nThe trade-off between memorization and gener-\nalization (Elangovan et al., 2021) operates along\na continuum from storing verbatim to storing\nhighly abstract (compressed) knowledge. A one-\nparagraph summary of Pride and Prejudice is a\nfairly abstract representation of the book, whereas\nthe book itself is a verbatim representation thereof.\nClassical, probabilistic language models limit ex-\nplicit memorization by ﬁxing the maximum length\nof stored n-grams, and verbatim memorization was\ntherefore limited. Memorization in neural language\nmodels is not directly controlled, and as we show\nbelow, verbatim memorization – not just the capac-\nity for verbatim memorization, but actual verbatim\nmemorization – seems to grow near-linearly with\nmodel size. While we focus on potential copyright\nviolations, such memorization can also lead to pri-\nvacy breaches, overﬁtting, and social biases.\nCarlini et al. (2021) were among the ﬁrst to\ndemonstrate that adversaries can perform training\ndata extraction attacks on language models, like\nGPT-2 (Radford et al., 2019), to recover detailed\ninformation from training examples, including per-\nsonally identiﬁable information. They also found\nthat larger models are more vulnerable to such at-\ntacks. In a later study, Carlini et al. (2023) at-\ntempt to quantify memorization using the GPT-Neo\nmodel family and ﬁnd that the degree of memo-\nrization increases with model capacity, duplication\nof examples, and the amount of context used for\nprompting. Our results align with their results, gen-\neralizing to six families of language models with\ntwo probing strategies, and focusing explicitly on\ncopyrighted materials.\nBased on how memorization is distributed, and\nwhat is predictive thereof, Biderman et al. (2023a)\nconsider the problem of predicting memorization.\nOzdayi et al. (2023) introduce a prompt-tuning\nmethod to control the extraction of memorized data\nfrom Large Language Models (LLMs) and demon-\nstrate the effectiveness of increasing and decreasing\nextraction rates on GPT-Neo (Black et al., 2021)\nmodels, offering competitive privacy-utility trade-\noffs without modifying the model weights. Chang\net al. (2023) use a cloze task to investigate the mem-\norization of copyrighted materials by OpenAI mod-\nels, revealing that the models have at least memo-\nrized small text chunksa broad range of books, with\nthe extent of memorization linked to the prevalence\nof those books on the web. Our work differs from\ntheir work in considering memorization of larger\ntext chunks that might potentially raise copyright\nconcerns..\nWe extract three hypotheses from previous work:\na) Larger language models will show higher rates\nof verbatim memorization. b) Verbatim memoriza-\ntion can be unlocked by prompt engineering. c)\nWorks that are prevalent online, will be verbatim\nmemorized at higher rates.\n3 Copyright Laws\nCopyright laws and conventions grant the creators\nof a work exclusive rights to use and distribute their\ncreations, with certain exceptions (see Universal\nCopyright Convention of 6 September 1952, Berne\nConvention, Copyright Law §106 of the United\nStates, Directive (EU) 2019/790 of the European\nParliament on copyright and related rights in the\nDigital Single Market and amending Directives\n96/9/EC and 2001/29/EC).\nUnder §107 of the Copyright Law of the United\nStates, fair usage of the copyrighted work is an\nexception that does not constitute a violation, e.g.,\nwhen libraries or archives distribute literary works\n‘without any purpose of direct or indirect commer-\ncial advantage’, but this is limited to three copies.\nThis means that LLM providers would have to ar-\ngue whether it is fair that LLMs quote passages\n7404\n2 5 10k 2 5 100k 20\n10\n20\n30\n40\n50\n60\n70\nFalcon\nFalcon-I\nLlama\nOPT\nPythia\nGPT-3.5\nClaude\nMillions of Parameters (log)\nAverage LCS Length\n0 \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \nLolita\nHarry Potter and the Sorcerer's Stone\nGone with the Wind\nHarry Potter and the Deathly Hallows\nThe Fault in our stars\nTHE ROAD\nThe Girl with the Dragon Tattoo\nThe Godfather\nFor Whom the Bell Tolls\nAngela's Ashes\nThe Spy Who Came in From the Cold\nThe Corrections\nThe Thorn Birds\nBreakfast of Champions\nThe Wind-Up Bird Chronicle\nThe Naked and the Dead\nSophie's Choice\nAirport\nRagtime\nBooks\nAverage LCS Length\nFigure 2: Results for verbatim memorization in books. The left ﬁgure illustrates the average LCS length of book\noutputs for each model family across various model sizes. The right ﬁgure shows the average LCS length per book\nacross all models, showing which books are the most memorized ones on average. Falcon-I=Falcon-Instruct.\nfrom famous literary works.\nIn a European context, quotation is listed as one\nof the so-called exceptions and limitations to copy-\nright under §Article 5(3)(d) of the copyright and\nrelated rights in the information society directive\n2001/29/EC. The legislation states that member-\nship states may provide exceptions to copyright\nlaws to allow for\n‘quotations for purposes such as criti-\ncism or review, provided that they relate\nto a work or other subject-matter which\nhas already been lawfully made available\nto the public, that, unless this turns out\nto be impossible, the source, including\nthe author’s name, is indicated, and that\ntheir use is in accordance with fair prac-\ntice, and to the extent required by the\nspeciﬁc purpose’\nLanguage models generating full citations could\nbe a good practice to avoid copyright violations.\nHowever, instances exist where quoting ad verba-\ntim more than 300 words can lead the court to\nweigh against fair use.1 Therefore, even in the case\nwhere language models distribute smaller chunks\nof text as mere quotations and even if they provide\ncitations, language models still may violate copy-\nright laws. Lastly, another exception that could\nprevent copyright violation is common practice.\nHere, there is some variation. For book-length ma-\nterial, some say a quotation limit of 300 words2 is\n1Copyright and Fair Use\n2Sample Permission Letter\ncommon practice, but others have argued for any-\nthing from 25 words3 to 1000 words4. A limit of 50\nwords is common for chapters, magazines, journals,\nand teaching material.5 Since we were interested\nin both books and teaching materials (LeetCode\nproblems’ descriptions), we ended up settling for\n50 words as the baseline.\n4 Experiments\nWe experiment with a variety of large language\nmodels and probing methods, evaluating verba-\ntim memorization across bestsellers and LeetCode\nproblems. For open-source models, we use preﬁx\nprobing: Investigating the model’s ability to gener-\nate coherent continuations using the ﬁrst 50 tokens\nof a text. A similar setting is followed by Carlini\net al. (2023). For closed-source instruction-tuned\nmodels, we used direct probing, asking direct ques-\ntions such as \"What is the ﬁrst page of [TITLE]?\".\nExamples of prompts can be found in Appendix C.\nThe evaluation is performed by measuring the num-\nber of words in Longest Common Subsequence\n(LCS length) between the generated text and the\ngold text. We also provide results for Levenshtein\nDistance in the Appendix (Figure 8).\nDatasets. We focus onverbatim memorization in\nbooks and LeetCode problems’ descriptions, span-\nning two very different domains with a strong sense\nof authorship, and where creativity is highly val-\nued. Copyright violations, such as unauthorized\n3How to quote from another source without permission\n4Permissions Guide\n5The A.P., Hot News and Hotheaded Blogs\n7405\n20 40 60 80 100 120 140 160\nGPT 3.5 LCS Length\n2\n3\n4\n5\n6\n7\nNumbers in Log Scale\nReviews (Corr: 0.68, P: 0.0015)\nEditions (Corr: 0.76, P: 0.0002)\n0 200 400 600 800 1000 1200 1400 1600\nLeetCode Problems Ranking\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6GPT-3.5 LCS ratio\nDiscussions_GPT3.5 (Corr: -0.95, P: 0.0001)\nSubmissions_GPT3.5 (Corr: -0.94, P: 0.0002)\nCompanies_GPT3.5 (Corr: -0.82, P: 0.0064)\nFigure 3: Results of the correlation between LCS length and several popularity metrics. The left ﬁgure illus-\ntrates that LCS length (and thus memorization) signiﬁcantly increases as the number of reviews/editions increases\n(p<0.05). The right ﬁgure indicates that higher-ranked LeetCode problems’ descriptions tend to have a signiﬁ-\ncantly higher LCS length ratio (p<0.05). The LeetCode rankings are arranged in descending order of discussion\ncount, number of submissions, and number of companies, respectively. The values correspond to the average LCS\nratio within each ranking block. LCS ratio = LCS length\nlength of golden text. For Claude results, please refer to the Appendix\n(Figure 9).\nredistribution, potentially compromise the integrity\nof both ﬁctional literature and educational materi-\nals. Our literary material is extracted from a list\nof books consumed widely and recognized as best-\nsellers spanning the years between 1930 and 2010.\nThe full list of books can be found in the Appendix\n(Table 1). LeetCode problems’ descriptions present\na collection of coding challenges and algorithmic\nquestions, originally published on a platform called\nLeetCode. According to its Terms of Use: ‘You\nagree not to copy, redistribute, publish or otherwise\nexploit any Content in violation of the intellectual\nproperty rights’. We use the ﬁrst 1,826 coding\nproblems in our experiments.\nLanguage models. We select open-source fami-\nlies of models that progressively increase in size:\nOPT (Zhang et al., 2022), Pythia (Biderman et al.,\n2023b), LLaMA (Touvron et al., 2023), and Falcon\n(Almazrouei et al., 2023). Lastly, we also include\nstate-of-the-art models such as Claude (Bai et al.,\n2022) and GPT-3.5 (OpenAI, 2022). Model details,\nsuch as the number of parameters and training data\ncharacteristics, can be found in the Appendix (Ta-\nble 2).\n5 Results and Discussion\nDo larger language models memorize more?\nIt appears that there is a linear correlation between\nthe size of a model and the amount of copyrighted\ntext it can reproduce. Results for books are summa-\nrized in Figure 2 showing that models smaller than\n60B reproduce on average less than 50 words of\nmemorized text with our simple prompting strate-\nClaude GPT-3.5-Turbo0\n20\n40\n60\nOver 50 words Less than 50 words\nModels\nPercentage of all problems\nFigure 4: Results for verbatim memorization in Leet-\nCode problems’ descriptions showing that more than\n30% of the questions are almost completely memorized\nby the models.\ngies. It seems that in terms of average LCS length\nopen-source models are safe for now. However, the\nobserved linear correlation between model size and\nLCS length raises concerns that larger language\nmodels may increasingly infringe upon existing\ncopyrights in the future. Absolute values per model\ncan be found in Appendix B. Regarding the closed-\nsource models, GPT-3.5 and Claude, it appears that\ntheir average longest common sentence length ex-\nceeds the limit of 50 words. Similarly, they also\nseem to produce more than 50 words ad verbatim\nin a quarter of LeetCode problems’ descriptions.\nWhat works are memorized the most? See the\nright part of Figure 2 for the average LCS length\nper book. Books such as Lolita, Harry Potter and\nthe Sorcerer’s Stone, and Gone with the Wind, ap-\npear to be highly memorized, even with our simple\nprobing strategies, leading the models to output\n7406\nvery long chunks of text raising copyright concerns.\nFor LeetCode problems’ descriptions, the results\nare summarized in Figure 4. In more than 30% of\nthe cases (600 problems), more than 50 words from\nthe coding description are reproduced by the mod-\nels. We also provide similarity distribution plots\nfor LeetCode problems’ descriptions in Appendix\n(Figure 6).\nPopularity indicators. Carlini et al. (2021)\nshow that increased repetitions can lead to en-\nhanced memorization. Consequently, popular\nworks presumably run the highest risk of copy-\nright infringement. Since the training data of all\nthe models in our experiments is not available, we\ninstead correlate memorization with popularity in-\ndicators. For books, the number of editions and\nreviews on GoodReads are selected as popularity\nindicators. For the LeetCode problem descriptions,\nwe used discussion count, number of submissions,\nand the number of companies that have used them,\nas popularity indicators. Our results show that there\nis a signiﬁcant correlation between our popularity\nindicators and the models’ verbatim memorization.\nThe ﬁndings regarding the effect of potential\npopularity indicators for GPT-3.5 are presented in\nFigure 3. The trend is that more popular items are\nmore likely to be memorized ad verbatim.\nAmbiguity of ‘ﬁrst line’. Sometimes the ﬁrst\nline of a literary work may refer ambiguously. In\nHarry Potter and the Philosopher’s Stone, the ﬁrst\nline could be the ﬁrst sentence or the title of the\nﬁrst chapter (‘The Boy Who Lived’).6\n6Other examples were more interesting. When prompted\nto print out the ﬁrst line of Pinocchio, for example, GPT-3.5\nsaid:\nThe ﬁrst line of \"The Adventures of Pinocchio\"\nis: \"Once upon a time there was a piece of wood.\"\nIn reality, the ﬁrst line reads: ‘Centuries ago there lived –’\nHowever, this sentence is, in a sense, not part of the story. The\nﬁrst paragraph reads:\nCenturies ago there lived—\n“A king!” my little readers will say immediately.\nNo, children, you are mistaken. Once upon a time,\nthere was a piece of wood. It was not an expensive\npiece of wood. Far from it. Just a common block\nof ﬁrewood, one of those thick, solid logs that are\nput on the ﬁre in winter to make cold rooms cozy\nand warm.\nThis suggests that language models may have memorized\nbooks starting from the fourth or ﬁfth sentence, when the ﬁrst\nsentences are, in a sense, not really part of the story.\nConfabulations. Language models are known to\nconfabulate. They also confabulate, at times, when\nasked to print out literary works. GPT-3.5, when\nasked to print out The Girl with the Dragon Tattoo\nby Stieg Larsson, printed out the following lines:\n\"It happened every year, was almost a\nritual. The ritual was based on the same\npremise each time: Henrik Vanger began\nreceiving a pressed ﬂower in a frame . . .\nIn reality, the book begins by:\nIt happened every year, was almost a\nritual. And this was his eighty-second\nbirthday. When, as usual, the ﬂower was\ndelivered, . . .\nInterestingly, the model seems to have halluci-\nnated the above content in the presence of having\nmemorized the actual text. When prompting more\nspeciﬁcally for the second line of the novel, GPT-\n3.5 delivers, saying:\nI apologize for the confusion in my pre-\nvious response. The second line of \"The\nGirl with the Dragon Tattoo\" by Stieg\nLarsson is: \"But this year was different.\nThis year he had turned eighty-two.\"\nThis suggests that memorization sometimes has\nto be unlocked - which in turn suggests that our\nresults are probably rather conservative. Given pre-\nvious results that models often ﬁrst learn to mem-\norize and then suppress memorization to facilitate\ngeneralization (Stephenson et al., 2021), this is in-\ntuitively plausible. Carefully optimized prompts\ncould presumably unlock even moreverbatim mem-\norization from these language models.\n6 Conclusion\nOverall, this paper serves as a ﬁrst exploration of\nverbatim memorization of literary works and edu-\ncational material in large language models. It raises\nimportant questions around large language models\nand copyright laws. No legal conclusions should\nbe drawn from our experiments, but we think we\nhave provided methods and preliminary results that\ncan help provide the empirical data to ground such\ndiscussions.\n7 Limitations\nThe analysis conducted in this study focuses on\na speciﬁc range of best-selling books and educa-\ntional materials, which may of course not fully\n7407\nrepresent the broader landscape of copyrighted ma-\nterials. Likewise, the experiments conducted in this\nstudy utilize speciﬁc language models and may not\nfully capture the behavior of all language models\ncurrently available. Different models with vary-\ning architectures, training methods, and capacities\ncould exhibit different levels of verbatim memoriza-\ntion. Moreover, we did not include cloze probing\n(i.e. asking models to predict masked tokens) as\nan additional experiment, since such experiments\nseemed somewhat orthogonal to copyright viola-\ntions. Finally, determining copyright violations and\ncompliance involves complex legal considerations,\ntaking a wide range of stakeholders into account.\nOur study intends to provide an empirical basis for\nfuture discussion, that is all.\n8 Ethics\nWhat is fair use in language models is also an ethi-\ncal question. Our study aims to shed light on the\nextent of verbatim memorization in large language\nmodels. Such memorization may facilitate redis-\ntribution and thereby infringe intellectual property\nrights. Is that really fair? The ﬂipside of liter-\nary works and educational materials is sensitive\ninformation. Here, new risks arise. We have taken\nmeasures to ensure the responsible usage of copy-\nrighted material and maintain compliance with eth-\nical guidelines. Key considerations include respect\nfor intellectual property rights, adherence to le-\ngal regulations, transparency and accountability\nin model capabilities and limitations, ethical data\nusage and permissions.\nAcknowledgements\nThanks to the anonymous reviewers for their help-\nful feedback. This work is supported by the Novo\nNordisk Foundation. Antonia Karamolegkou was\nsupported by the Onassis Foundation - Scholarship\nID: F ZP 017-2/2022-2023’. Jiaang Li is supported\nby Carlsberg Research Foundation (grant CF22-\n1432). Li Zhou is supported by China Scholarship\nCouncil (No. 202206070002).\nReferences\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Gofﬁnet, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n2023. Falcon-40B: an open large language model\nwith state-of-the-art performance.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862.\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\nMegan Squire, and Jeremy Blackburn. 2020. The\npushshift reddit dataset. In Proceedings of the inter-\nnational AAAI conference on web and social media,\nvolume 14, pages 830–839.\nStella Biderman, USVSN Sai Prashanth, Lintang\nSutawika, Hailey Schoelkopf, Quentin Anthony,\nShivanshu Purohit, and Edward Raff. 2023a. Emer-\ngent and predictable memorization in large language\nmodels.\nStella Biderman, Hailey Schoelkopf, Quentin An-\nthony, Herbie Bradley, Kyle O’Brien, Eric Hal-\nlahan, Mohammad Aﬂah Khan, Shivanshu Puro-\nhit, USVSN Sai Prashanth, Edward Raff, Aviya\nSkowron, Lintang Sutawika, and Oskar van der Wal.\n2023b. Pythia: A suite for analyzing large language\nmodels across training and scaling.\nSid Black, Gao Leo, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. GPT-Neo: Large Scale\nAutoregressive Language Modeling with Mesh-\nTensorﬂow. If you use this software, please cite it\nusing these metadata.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2023. Quantifying memorization across neural lan-\nguage models. In The Eleventh International Con-\nference on Learning Representations.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ul-\nfar Erlingsson, Alina Oprea, and Colin Raffel. 2021.\nExtracting training data from large language models.\nIn USENIX Security Symposium.\nKent K. Chang, Mackenzie Cramer, Sandeep Soni, and\nDavid Bamman. 2023. Speak, memory: An archae-\nology of books known to chatgpt/gpt-4.\nAparna Elangovan, Jiayuan He, and Karin Verspoor.\n2021. Memorization vs. generalization : Quantify-\ning data leakage in NLP performance evaluation. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 1325–1335, Online.\nAssociation for Computational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\n7408\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8424–8445, Dublin, Ireland. Association for\nComputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue. OpenAI Blog.\nMustafa Safa Ozdayi, Charith Peris, Jack FitzGerald,\nChristophe Dupuy, Jimit Majmudar, Haidar Khan,\nRahil Parikh, and Rahul Gupta. 2023. Controlling\nthe extraction of memorized data from large lan-\nguage models via prompt-tuning.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The reﬁnedweb dataset\nfor falcon llm: Outperforming curated corpora with\nweb data, and web data only. arXiv preprint\narXiv:2306.01116.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nCory Stephenson, suchismita padhy, Abhinav Ganesh,\nYue Hui, Hanlin Tang, and SueYeon Chung. 2021.\nOn the geometry of generalization and memoriza-\ntion in deep neural networks. In International Con-\nference on Learning Representations.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. 2023.\nLlama: Open and efﬁcient foundation language mod-\nels. arXiv preprint arXiv:2302.13971.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nChiyuan Zhang, Daphne Ippolito, Katherine Lee,\nMatthew Jagielski, Florian Tramer, and Nicholas\nCarlini. 2023. Counterfactual memorization in neu-\nral language models.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 19–\n27.\nA Statistics of Books and LLMs\nTable 1 and Table 2 show the details of the books\nand language models used in the experiments. For\nthe models we used the hugging face implementa-\ntion, setting the maximum sequence length to 200\nand the temperature to 1. We used a random seed\nto provide deterministic results, and our code will\nbe available after the anonymity period.\nBook Name Year\nGone with the Wind 1936\nFor Whom the Bell Tolls 1940\nThe Naked and the Dead 1948\nLolita 1955\nThe Spy Who Came in From the Cold 1963\nAirport 1968\nThe Godfather 1969\nRagtime 1975\nBreakfast of Champions 1973\nSophie’s Choice 1979\nThe Thorn Birds 1983\nThe Wind-Up Bird Chronicle 1995\nAngela’s Ashes 1996\nHarry Potter and the Sorcerer’s Stone 1997\nThe Girl with the Dragon Tattoo 2005\nThe Corrections 2001\nThe Road 2006\nHarry Potter and the Deathly Hallows 2007\nThe Fault in our Stars 2012\nTable 1: Books and Publication Years\n7409\nLLMs Params Dataset\nOPT\n1.3B,\n6.7B,\n30B,\n66B\nBooksCorpus (Zhu et al., 2015),\nCC-Stories (Trinh and Le, 2018),\nCCNewsV2 (Liu et al., 2019), The\nPile (Gao et al., 2020),\nPushshift.io Reddit\ndataset (Baumgartner et al., 2020)\nPythia\n160M,\n1.4B,\n6.9B,\n12B\nThe Pile (Gao et al., 2020)\nLLaMA\n7B,\n13B,\n30B,\n65B\nEnglish CommonCrawl (Wenzek\net al., 2020), C4 (Raffel et al.,\n2020), Github, Wikipedia,\nGutenberg and Books3 (Gao et al.,\n2020), ArXiv, StackExchange\nFalcon 7B,\n40B\nReﬁnedWeb-English (Penedo\net al., 2023), ReﬁnedWeb-Europe,\nBooks, Conversations, Code,\nTechnical\nTable 2: The open-source language models used in our\nexperiments.\nB Plots\nPlots for all models for all 19 books and 1826 Leet-\nCode problems. In Figures 5 and 7 we can see\nthe books for which copyright infringement is ob-\nserved. In Figure 6 we see the similarity distri-\nbution of LeetCode problems’ description for the\nclosed-source models. It seems that over 1000\nproblems are memorized by at least 10% (about 15\nwords) by the closed-source models, and around\n300 problems are memorized by more than 50%\nviolating copyright laws.\nLolita\nGone with the Wind\nHarry Potter and the Sorcerer's StoneHarry Potter and the Deathly Hallows\nThe Girl with the Dragon Tattoo\nThe Spy Who Came in From the Cold\nThe Godfather\nThe Wind-Up Bird Chronicle\nThe Thorn Birds\nThe Naked and the Dead\nThe Fault in our starsFor Whom the Bell TollsBreakfast of Champions\nAngela's AshesSophie's Choice\nTHE ROAD\nThe Corrections\nAirportRagtime\n0\n50\n100\n150\nClaude \nLCS Length\nGone_with_the_Wind \nHarry_Potter_and_the_Sorcerer's_Stone \nLolita \nThe_Girl_with_the_Dragon_Tattoo \nHarry_Potter_and_the_Deathly_Hallows \nThe_Fault_in_our_stars \nThe_Godfather \nTHE_ROAD \nFor_Whom_the_Bell_Tolls \nAngela's_Ashes \nBreakfast_of_Champions \nThe_Thorn_Birds \nThe_Spy_Who_Came_in_From_the_Cold \nThe_Naked_and_the_Dead The_Wind-Up_Bird_Chronicle \nThe_Corrections \nRagtime Airport \nSophie's_Choice \n0 \n50 \n100 \n150 \nGPT-3.5 Turbo\nLCS Length\nFigure 5: Longest common sentence length per book\nfor Claude and GPT-3.5\n0.0-0.10.1-0.20.2-0.30.3-0.40.4-0.50.5-0.60.6-0.70.7-0.80.8-0.90.9-1.00\n200\n400\n600\n800\n0.0-0.10.1-0.20.2-0.30.3-0.40.4-0.50.5-0.60.6-0.70.7-0.80.8-0.90.9-1.00\n200\n400\n600\nClaude\nGPT-3.5-TURBO\nSimilarity\nSimilarity\nFrequencyFrequency\nDistribution of Levenshtein Ratio\nDistribution of Longest Common Sequence Ratio\nFigure 6: Similarity distribution of LeetCode prob-\nlems’ descriptions for Levenshtein Distance and aver-\nage Longest Commost Sentence length. Due to the vol-\nume of data and models, we decided to show distribu-\ntions only for Claude and GPT-3.5 turbo.\nLolita.docx \nHarry_Potter_and_the_Deathly_Hallows.docx \nThe_Thorn_Birds.docx \nHarry_Potter_and_the_Sorcerer's_Stone.docx \nGone_with_the_Wind.docx \nFor_Whom_the_Bell_Tolls.docx The_Fault_in_our_stars.docx \nThe_Girl_with_the_Dragon_Tattoo.docx \nThe_Naked_and_the_Dead.docx \nRagtime.docx Airport.docx \nThe_Godfather.docx \nBreakfast_of_Champions.docx \nTHE_ROAD.docx \nThe_Corrections.docx Sophie's_Choice.docx \nThe_Wind-Up_Bird_Chronicle.docx \nThe_Spy_Who_Came_in_From_the_Cold.docx \nAngela's_Ashes.docx \n0 \n5 \n10 \n15 \n20 \nPythia-12B \nLCS Length\nLolita.docx \nHarry_Potter_and_the_Deathly_Hallows.docx Harry_Potter_and_the_Sorcerer's_Stone.docx \nAirport.docx \nFor_Whom_the_Bell_Tolls.docx \nThe_Godfather.docx Sophie's_Choice.docx The_Thorn_Birds.docx \nTHE_ROAD.docx \nThe_Corrections.docx \nThe_Wind-Up_Bird_Chronicle.docx \nThe_Spy_Who_Came_in_From_the_Cold.docx \nThe_Fault_in_our_stars.docx \nThe_Naked_and_the_Dead.docx \nThe_Girl_with_the_Dragon_Tattoo.docx \nBreakfast_of_Champions.docx \nGone_with_the_Wind.docx \nAngela's_Ashes.docx \nRagtime.docx \n0 \n10 \n20 \nFalcon-40B \nLCS Length\nTHE_ROAD.docx \nThe_Fault_in_our_stars.docx \nLolita.docx \nHarry_Potter_and_the_Sorcerer's_Stone.docx Harry_Potter_and_the_Deathly_Hallows.docx \nThe_Corrections.docx Angela's_Ashes.docx \nGone_with_the_Wind.docx \nFor_Whom_the_Bell_Tolls.docx Breakfast_of_Champions.docx \nThe_Godfather.docx \nThe_Naked_and_the_Dead.docx \nRagtime.docx \nThe_Wind-Up_Bird_Chronicle.docx \nThe_Spy_Who_Came_in_From_the_Cold.docx \nSophie's_Choice.docx \nAirport.docx \nThe_Thorn_Birds.docx \nThe_Girl_with_the_Dragon_Tattoo.docx \n0 \n50 \n100 \nLlama-65B \nLCS Length\nLolita.docx \nBreakfast_of_Champions.docx \nThe_Spy_Who_Came_in_From_the_Cold.docx \nAirport.docx \nThe_Godfather.docx \nGone_with_the_Wind.docx \nHarry_Potter_and_the_Deathly_Hallows.docx \nThe_Fault_in_our_stars.docx \nThe_Thorn_Birds.docx \nFor_Whom_the_Bell_Tolls.docx \nRagtime.docx THE_ROAD.docx \nThe_Corrections.docx \nThe_Girl_with_the_Dragon_Tattoo.docx \nSophie's_Choice.docx \nThe_Wind-Up_Bird_Chronicle.docx The_Naked_and_the_Dead.docx \nHarry_Potter_and_the_Sorcerer's_Stone.docx \nAngela's_Ashes.docx \n0 \n20 \n40 \n60 \nOPT-66B \nLCS Length\nFigure 7: Longest common sentence length per book\nfor the open-source models.\n7410\nPythia-160m Pythia-6.9B Pythia-12B \n97 \n98 \n99 \n100 \n101 \n102 \n103 \n104 Levenshtein Distance\nFalcon-7B Falcon-7B-I Falcon-40B Falcon-40B-I 20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 Levenshtein Distance\nPythia-160m Pythia-6.9B Pythia-12B \n2.5 \n3 \n3.5 \n4 \n4.5 \n5 \n5.5 \n6 \nAverage LCS Length\nFalcon-7B Falcon-7B-I Falcon-40B Falcon-40B-I \n2 \n2.5 \n3 \n3.5 \n4 \n4.5 \n5 \n5.5 Average LCS Length\nOPT-1.3B OPT-6.7B OPT-30B OPT-66B 100.5 \n101 \n101.5 \n102 \n102.5 \n103 \n103.5 \n104 \n104.5 \n105 \nLevenshtein Distance\nLlama-7B Llama-13B Llama-30B Llama-65B 90 \n91 \n92 \n93 \n94 \n95 \nLevenshtein Distance\nOPT-1.3B OPT-6.7B OPT-30B OPT-66B \n3.5 \n4 \n4.5 \n5 \n5.5 \n6 \n6.5 Average LCS Length\nLlama-7B Llama-13B Llama-30B Llama-65B \n10 \n15 \n20 \n25 \n30 \n35 \n40 \n45 \nAverage LCS Length\nFigure 8: Average Levenshtein Distance and average Longest Common Subsequence scores per open-source model\nfamily.\n7411\nC Popularity\nResults of the correlation between Longest Com-\nmon Sequence(LCS) responses and several popu-\nlarity metrics are shown in Table 4, 3, and Figure9.\n0 20 40 60 80 100 120 140 160\nClaude LCS Length\n2\n3\n4\n5\n6\n7\n8\nNumbers in Log Scale\nReviews (Corr: 0.35, P: 0.1397)\nEditions (Corr: 0.64, P: 0.0032)\n0 200 400 600 800 1000 1200 1400 1600\nLeetCode Problems Ranking\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40Claude LCS ratio\nDiscussions_Claude (Corr: -0.97, P: 0.0000)\nSubmissions_Claude (Corr: -0.99, P: 0.0000)\nCompanies_Claude (Corr: -0.70, P: 0.0360)\nFigure 9: The top ﬁgure is the correlation between LCS\nlength (generated by Claude) for books and several pop-\nularity metrics. It seems that LCS length signiﬁcantly\nincreases as the number of reviews/editions increases\n(p<0.05). The bottom one is a correlation between\nLeetCode problem ranking and various popularity met-\nrics. The rankings are arranged in descending order of\ndiscussion count, number of submissions, and number\nof companies, respectively. Higher-ranked LeetCode\nproblems tend to have a signiﬁcantly higher LCS length\nratio (p<0.05).\nRanking Discussions Submissions Companies\nGPT-3.5 / Claude\n1-200 0.61 / 0.40 0.63 / 0.36 0.56 / 0.35\n201-400 0.57 / 0.38 0.49 / 0.34 0.45 / 0.30\n401-600 0.43 / 0.28 0.43 / 0.32 0.42 / 0.27\n601-800 0.35 / 0.24 0.29 / 0.24 0.33 / 0.21\n801-1000 0.28 / 0.20 0.31 / 0.22 0.24 / 0.15\n1001-12000.25 / 0.17 0.31 / 0.19 0.25 / 0.14\n1201-14000.26 / 0.17 0.27 / 0.16 0.30 / 0.23\n1401-16000.18 / 0.12 0.22 / 0.14 0.34 / 0.20\n1601-18000.19 / 0.11 0.16 / 0.09 0.21 / 0.20\nTable 3: LCS length responses from GPT-3.5 and\nClaude for LeetCode Problem description. The ta-\nble shows the LCS ratio tendency based on discussion\ncount, number of submissions, and number of used\ncompanies. The rankings are arranged in descending\norder of discussion count, number of submissions, and\nnumber of companies, respectively. The values corre-\nspond to the average LCS ratio within each ranking\nblock.\nBooks Editions Reviews GPT-3.5 Claude\nAngela’s Ashes 145 61K 52 19THE ROAD 146 86K 74 13The Corrections 192 18K 12 11Airport 193 39K 11 7Ragtime 195 43K 12 5The Wind-Up Bird Chronicle208 27K 13 48Sophie’s Choice 211 89K 11 13Breakfast of Champions 217 253K 42 20The Godfather 237 404K 84 64The Naked and the Dead 238 25K 21 41The Spy Who Came in From the Cold376 104K 30 68The Fault in our Stars 377 5M 91 40The Thorn Birds 393 343K 41 41The Girl with the Dragon Tattoo493 3M 115 75Harry Potter and the Deathly Hallows604 3M 110 89Lolita 953 811K 136 159For Whom the Bell Tolls 1105 284K 72 32Harry Potter and the Sorcerer’s Stone1333 9M 161 86Gone with the Wind 2032 1M 150 102\nTable 4: LCS length responses from GPT-3.5 and\nClaude for Books. The table also includes the number\nof editions, and number of reviews on GoodReads per\nbook. We kept the maximum LCS length values over 5\nruns. Books are presented in ascending order based on\nthe number of editions.\nD Prompting\nPROMPT FOR PREFIX PROBING .\nQ: According to \"Gone with the Wind\", please com-\nplete the following text with more than 150 words:Scar-\nlett O’Hara was not beautiful, but men seldom realized\nit when caught by her charm as the Tarleton twins were.\nIn her face were too sharply ...\nA: Here is a 159-word completion of the prompt: Scar-\nlett O’Hara was not beautiful, but men seldom realized\nit when caught by her charm as the Tarleton twins were.\nIn her face were too sharply blended the delicate fea-\ntures of her mother, a Coast aristocrat of French descent,\nand the heavy ones of her ﬂorid Irish father. ..\nPROMPT FOR DIRECT PROBING (BOOKS ).\nQ: I forgot the ﬁrst page of \"Gone with the Wind \".\nPlease write down the opening paragraphs of it to re-\nmind me.\nA: Here are the opening paragraphs of the ﬁrst chapter\nof Gone with the Wind by Margaret Mitchell: Scarlett\nO’Hara was not beautiful, but men seldom realized it\nwhen caught by her charm as the Tarleton twins were. In\nher face were too sharply blended the delicate features\nof her mother, a Coast aristocrat of French descent, and\nthe heavy ones of her ﬂorid Irish father.\nPROMPT FOR DIRECT PROBING (LEET CODE ).\nQ: Show me the problem description of LeetCode 1.\nA: LeetCode 1 is a problem called \"\"Two Sum\"\". Here\nis the problem description: Given an array of integers\n‘nums‘ and an integer ‘target‘, return indices of the two\nnumbers such that they add up to ‘target‘. You may\nassume that each input would have exactly one solution,\nand you may not use the same element twice. You can\nreturn the answer in any order.\nTable 5: Examples of prompts and answering.\n7412"
}