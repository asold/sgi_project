{
  "title": "LSTM Language Models for LVCSR in First-Pass Decoding and Lattice-Rescoring",
  "url": "https://openalex.org/W2956159074",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222144509",
      "name": "Beck, Eugen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1983461967",
      "name": "Zhou Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3189410112",
      "name": "Schlüter, Ralf",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747500719",
      "name": "Ney, Hermann",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2176797192",
    "https://openalex.org/W1585876329",
    "https://openalex.org/W2026149468",
    "https://openalex.org/W2110415041",
    "https://openalex.org/W2782451907",
    "https://openalex.org/W2748092010",
    "https://openalex.org/W2746475861",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2394835536",
    "https://openalex.org/W2295078202",
    "https://openalex.org/W2963266252",
    "https://openalex.org/W2964191536",
    "https://openalex.org/W2009150118",
    "https://openalex.org/W1965154800",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2963660642",
    "https://openalex.org/W2763120645",
    "https://openalex.org/W2066378046",
    "https://openalex.org/W1773599773",
    "https://openalex.org/W1520465330",
    "https://openalex.org/W38527073",
    "https://openalex.org/W2138204974",
    "https://openalex.org/W2345190899",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2916979304",
    "https://openalex.org/W3103005696",
    "https://openalex.org/W2037942319",
    "https://openalex.org/W2404974730",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2802201485",
    "https://openalex.org/W2091981305"
  ],
  "abstract": "LSTM based language models are an important part of modern LVCSR systems as they significantly improve performance over traditional backoff language models. Incorporating them efficiently into decoding has been notoriously difficult. In this paper we present an approach based on a combination of one-pass decoding and lattice rescoring. We perform decoding with the LSTM-LM in the first pass but recombine hypothesis that share the last two words, afterwards we rescore the resulting lattice. We run our systems on GPGPU equipped machines and are able to produce competitive results on the Hub5'00 and Librispeech evaluation corpora with a runtime better than real-time. In addition we shortly investigate the possibility to carry out the full sum over all state-sequences belonging to a given word-hypothesis during decoding without recombination.",
  "full_text": "arXiv:1907.01030v1  [eess.AS]  1 Jul 2019\nLSTM Language Models for L VCSR in First-Pass Decoding and\nLattice-Rescoring\nEugen Beck 1,2, W ei Zhou 1,2, Ralf Schl ¨uter1, Hermann Ney 1,2\n1Human Language T echnology and Pattern Recognition, Comput er Science Department,\nRWTH Aachen University, 52074 Aachen, Germany\n2AppT ek GmbH, 52062 Aachen, Germany\n{beck, zhou, schlueter, ney}@cs.rwth-aachen.de\nAbstract\nLSTM based language models are an important part of mod-\nern L VCSR systems as they signiﬁcantly improve performance\nover traditional backoff language models. Incorporating t hem\nefﬁciently into decoding has been notoriously difﬁcult. In this\npaper we present an approach based on a combination of one-\npass decoding and lattice rescoring. W e perform decoding wi th\nthe LSTM-LM in the ﬁrst pass but recombine hypothesis that\nshare the last two words, afterwards we rescore the resultin g\nlattice. W e run our systems on GPGPU equipped machines and\nare able to produce competitive results on the Hub5’00 and Li b-\nrispeech evaluation corpora with a runtime better than real -time.\nIn addition we shortly investigate the possibility to carry out the\nfull sum over all state-sequences belonging to a given word-\nhypothesis during decoding without recombination.\nIndex T erms: speech recognition, decoding, LSTM language\nmodels, lattice rescoring\n1. Introduction\nIn recent years, language models (LMs) based on long short-\nterm memory (LSTM) neural networks have become an inte-\ngral part of many state-of-the-art automatic speech recogi tion\nsystems [1, 2, 3]. LSTMs thus supersede traditional backoff -\nmodels which are based on word counts. For count based\nmodels relative frequencies of word n-grams are computed an d\nstored. Due to the sparseness of the training data many n-gra ms\nare not seen and probability mass has to be allocated to them b y\nsmoothing out the probability distribution, e.g. by Kneser -Ney\nsmoothing [4]. During decoding this sparseness can be utili zed\nto recombine word-end hypotheses. LSTM LMs on the other\nhand use an internal state that is updated for each seen word\nto produce word posterior probabilities. This, in theory , g ives\nthem unlimited context that has to be considered which leads to\nlarge increases in required computation time.\nIn this work we show how to deal with this problem from\na decoding point of view . W e have chosen to utilize General\nPurpose Graphics Processing Units (GPGPUs) as an inference\nplatform. This is reasonable, as more and more dedicated co-\nprocessors for machine learning workloads become availabl e.\nOn the server-side, GPGPUs by nVidia/AMD, TPUs by Google\nand (soon) A WS Inferentia-chips provide highly parallel co m-\nputation platforms. Even for low-power devices chips like E dge\nTPU (by Google) or Kirin 970 (by Huawei) provide highly par-\nallel computation platforms. W e show that using an LSTM-LM\nin 1-st pass decoding is better than rescoring of lattices ge ner-\nated with a backoff LM. In addition forcing recombination of\nhistories that share a trigram context during the 1st pass fo l-\nlowed by lattice rescoring yields the same WER at lower RTF .\nThis paper is organized as follows: First we give an\noverview of existing work. Afterwards we present some im-\nplementation details. This is followed by a description of t he\nmodels, corpora and hardware used for our experiments. Then\nwe present a series of experiments on the Switchboard and Lib -\nrispeech corpora.\n2. Related W ork\nUsing Neural Network based Language Models (NN-LMs) in\nDecoding is computationally more expensive than using back -\noff Language Models. In this section we give a short overview\nof how other researchers have dealt with this problem.\nEarly approaches of introducing NN-LMs into decoding in-\nclude some form of conversion to a more traditional backoff\nLM: A very straightforward approach to convert complex mod-\nels is to sample them to create large training corpora on whic h\nback-off LMs can be trained on. This is the approach of [5]. In\n[6] the continues states of an RNN-LM are discretized to cre-\nate a weighted ﬁnite state transducer. The authors of [7] tra ined\nfeed-forward LMs for different orders and extracted the pro ba-\nbilities for the backoff LM directly from the neural network . [8]\ncompares different techniques for conversion and [9] uses t hese\ntechniques to investigate conversion of domain adapted LST M\nLMs.\nAnother option is to reduce the number of operations re-\nquired for NN-LMs. For models with large vocabulary the\nlion’s share of the computations occurs in the ﬁnal layer, wh ere\na large matrix multiplication is required. For models using the\nsoftmax activation function the probability for all words n eeds\nto be computed even if only the probability for one word is re-\nquired. Thus there is a large interest in developing techniq ues\nto avoid this. One popular approach is Noise-Contrastive-\nEstimation [10, 11]. Noise-Contrastive-Estimation is an a dapta-\ntion of the loss function used in training to guide the model i nto\na state where the output before the softmax is already approx -\nimately normalized and thus only a dot product is required to\ncompute the probability of one word. It is used by [12, 13, 14] .\nIn [14] other methods like caching and the choice of activati on\nfunctions within hidden layers are investigated aswell.\nY et another way to employ NN-LMs to improve ASR per-\nformance is to use them in a second pass for lattice rescoring .\nThis is more efﬁcient as only word sequences which are not\npruned away need to be scored by the NN-LM. Examples for\nthis approach are [15, 16, 17, 18, 19, 20] where a variety of\nheuristics is proposed to speed up the rescoring process.\nClosest to the work presented in this paper there are also\npublications where the authors integrated LSTM-LMs into ﬁr st\npass decoding: In [21] a set of caches was introduced to min-\nimize unnecessary computations when evaluating the LSTM-\nLM. In [22], an on-the-ﬂy rescoring approach to integrate\nLSTM-LMs into 1-st pass decoding is presented. The authors\nof [23] use a hybrid CPU/GPGPU architecture for real time de-\ncoding. The HCL transducer is composed with a small n-gram\nmodel and is expanded on the GPU while rescoring with an\nLSTM LM happens on CPU. Caching of previous outputs en-\nables real-time decoding. All three papers use hierarchica l soft-\nmax / word classes to reduce the number of computations in\nthe output layer [24] and with the exception of [21] interpol ate\nthe LSTM-LM with a Max-Entropy LM [25]. The works of\n[23] are extended in [26]. The LSTM Units are replaced with\nGRUs, NCE replaces the hierarchical softmax and GRU states\nare quantized to reduce the number of necessary computation s.\n3. Implementation\nFor this work we extended the decoder of the RWTH ASR\ntoolkit, described extensively in [27]. The decoder uses tr ee-\nconditioned search, which differs from the more common\nHCLG-based decoder in that we do not do static composition\nof the grammar WFST with the rest of the search network. In-\nstead hypotheses from the HCL part of the decoder are grouped\nby their LM-history . Because these histories are opaque ob-\njects to the decoder we do not need to build a static WFST to\nrepresent the LM, which would be infeasible for LSTM-LMs\nanyway . Instead we only have to store the sequence of words\nand the state of the LSTM layers for each history . The languag e\nmodel itself can be any tensorﬂow graph as long as it is compat -\nible with the general idea of a recurrent LM. The LM receives\na state and one word and produces a new state and probabilitie s\nfor the next word. One effect of statically combining Gramma r\nand the HCL automaton is the pushing of the Grammar weights\ntowards the start state of the transducer. In our decoder thi s\nearly LM information is retrieved via Language-Model looka -\nhead which is dynamically computed at runtime. W e found that\nit is not necessary to use the LSTM-LM when computing this\nlookahead information to achieve the best possible WER. Onl y\nfor very small beam-sizes we can observe a difference in WER.\nFor our rescoring experiments we use push-forward rescorin g\nas described in [17].\n4. Experiments\n4.1. Hardware and Measurement Methodology\nEach node used for our experiments has two sockets with Intel\nXeon E5-2620 v4 CPUs with a base-clock speed of 2.1Ghz and\n4 Nvidia Geforce 1080Ti GPUs. Unless stated otherwise, our\ndecoder ran in a single thread. The tensorﬂow runtime spawns\nmore threads as it sees ﬁt. As we are primarily using the GPU\nto do computations we set intra/inter\nop parallelism threads to\n1. T o compute the real time factor (RTF) we measure the total\nwallclock time required by the recognizer/rescorer to proc ess\nall segments within the corpus and divide it by the total dura -\ntion. This includes loading features from disk, forwarding them\nthrough the acoustic model and decoding / rescoring. Startu p\ntime is not included. Features are not extracted on the ﬂy as\nit creates higher load on our ﬁleserver and is not a major part\nduring decoding anyway . In a research context preextractin g\nfeatures for a common task is useful as they are required for\nmany experiments. In a production streaming system, featur e\nextraction can be ofﬂoaded into a separate thread and will on ly\ncontribute to latency , but not (signiﬁcantly) to RTF .\nCorpus Model T opology #out #param\nSwitchboard AM 6x500 9K 47.6M\nLM 2x1024 30K 82.8M\nLibrispeech AM 6x1000 12K 152.5M\nLM 2x2048 200K 486.8M\nT able 1: Sizes of acoustic (AM) and langugae models (LM) used\nin this paper . The format for the topology column is #layers\ntimes #units\nCorpus +LSTM-LM PPL\nHub 5’00 no 79.45\nyes 50.94\nLibrispeech dev no 146.18\nyes 70.59\nLibrispeech test no 151.81\nyes 73.96\nT able 2: P erplexities of Language Models used. Backoff models\noptionally combined with LSTM\n4.2. Corpora and Models\nIn this paper we present results on two tasks. The ﬁrst task is\nthe 300h Switchboard-1 Release 2 which is evaluated on the\nHub500 corpus. The second corpus is Librispeech [28].\nAll our systems use 40 dimensional Gammatone features\n[29]. The acoustic model is a multilayer BLSTM neural net-\nwork trained with the state-level minimum Bayes Risk (sMBR)\ncriterion [30]. The output units of the acoustic models are t ied\ntriphone states obtained using a Classiﬁcation and Regress ion\nTree (CART). When we use an LSTM-LM we interpolate it\nwith the backoff-LM using log-linear combination. The per-\nplexities of the models can be found in T able 2. The sizes of\nall models can be found in T able 1. More information about the\nLibrispeech system can be found in [31].\n4.3. Baseline\nOur baseline uses a 4-gram count model with an sMBR trained\nacoustic model. It saturates at a WER of 13.9% with a RTF of\n0.23. A detailed WER/RTF plot can be found in Figure 1.\n13.8\n14\n14.2\n14.4\n14.6\n14.8\n0 0 .2 0 .4 0 .6 0 .8 1 1 .2\nWER[%]\nRTF\nCount LM\nFigure 1: Backoff-LM baseline\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n0 10 20 30 40 50\nforwarding time [ms]\n#histories per batch\nFigure 2: Time to process one batch for one step with the\nSwitchboard LM divided by the number of histories in the batc h\n4.4. Parallelism\nAs GPGPUs are massively parallel architectures it is import ant\nto provide them with enough opportunities for parallelizat ion\nwhen doing computations. In Figure 2 we show the time it\ntakes to forward one batch for various batch sizes divided by\nthe number of histories for the Switchboard LM, i.e. we divid e\nthe total computation time by the number of histories within\none batch. W e can clearly see that it is much more efﬁcient to\nforward many histories at the same time. Thus, once the LM\nreceives the request to compute a speciﬁc word probability t hat\nis not already computed, we look for other histories that are\nnot forwarded yet. W e prioritize those histories that have a hy-\npothesis close to a word-end state and with a score close to th e\ncurrently best hypothesis.\n4.5. Effect of recombination\nThe biggest difference between traditional backoff models and\nLSTM Language Models from the decoding point of view is\nthe fact that backoff LMs allow for recombination of hypothe -\nses at word-ends. For the LSTM-LM this is not possible, as\neach sequence of words for the history vector forms a unique\nhistory , that in principle can encode all words hithero seen . But\nof course the state of LSTM-Layers is ﬁnite and thus the LSTM\nwill not be able to store an arbitrary amount. Furthermore so me\ninformation about the past context might not be relevant for the\nsearch process anyway . Thus, it is reasonable to assume that\neven for an LSTM LM we can recombine hypotheses if the last\nn words match. T o empirically determine n we conducted ex-\nperiments in which we recombined word-end hypothesis where\nthe last n words matched. W e did not recompute the state of the\nLSTM for this reduced context, but kept the context of the wor d\nend with the lowest score (i.e. highest probability). This f orced\nrecombination changes the lattice structure from a tree bac k to\na directed graph.\nIn Figure 3 we show the results of our experiments. W e\ncan see that for a recombination limit of 5 we already reach th e\nbest WER (11.7%) when rounding to 1 digit, as is usual for this\ntask. Larger beam sizes do not yield signiﬁcant improvement s\nin terms of best achievable WER, but they allow us to reach it\nfaster or get better WER for a ﬁxed RTF value. This is because\nfor n = 5 we need a larger beam to reach the best possible\nWER, while larger n allow for smaller beam sizes. For RTF\n∼ 1 a recombination of n ≥ 9 should be selected.\n11.6\n11.8\n12\n12.2\n12.4\n12.6\n12.8\n13\n0 0 .5 1 1 .5 2 2 .5 3 3 .5 4 4 .5\nWER[%]\nRTF\nRecombine n=2\nRecombine n=3\nRecombine n=5\nRecombine n=9\nRecombine n=13\nRecombine n=inf\nFigure 3: Different recombination limits for the Switchboard\nsystem\n4.6. One-pass vs T wo-pass with Rescoring\nAs a next step, we want to measure the performance of our sys-\ntem when the LSTM-LM is applied during lattice rescoring. As\na baseline, we rescore the lattices generated by decoding wi th\nthe backoff-LM. As can be seen in Figure 4 this is very fast for\nsmall beam sizes. For larger beam sizes it is still faster tha n us-\ning the LSTM-LM in the ﬁrst pass, but it yields slightly worse\nresults (11.8% vs 11.7%). In theory , if the beam-pruning wer e\nbig enough, the lattice would contain the same word sequence s\nthat receive the best scores from the LSTM-LM. However this\ndoes not happen for the usual beam sizes we use during recog-\nnition as Figure 4 shows. As lattice rescoring is relatively fast\n(0.02-0.12 RTF , depending on lattice size) we also use it in c om-\nbination with one-pass decoding with LSTM-LMs. In order to\nkeep the complexity of the ﬁrst pass low we use a small re-\ncombination threshold ( n = 2). This will produce lattices with\nmany recombinations and thus many opportunities for lattic e\nrescoring to ﬁnd new word-sequences. Our experiments show\nthat this indeed is more efﬁcient than performing one-pass d e-\ncoding with a high recombination limit.\n4.7. Full-sum decoding\nThe one pass decoding without recombination makes it pos-\nsible to apply full sum over all state sequences in the HMM\ninstead of Viterbi approximation in the Bayes decision rule .\nW e also explore this effect on the Hub 5’00 data set. W e use\nthe same decoder framework as implementation baseline and\nchanged the auxiliary function of dynamic programming to be\nthe probability summation over all incoming paths instead o f\nmaximization. The pruning threshold of the beam search has t o\nbe set a bit higher to keep most of the contributing paths. Pat hs\nof pronunciation variants are normalized and merged as well .\nThe simple hand-crafted time distortion penalties are norm al-\nized back to probability domain. Grid search is used to ﬁnd th e\noptimal acoustic and language model scaling. T o our surpris e,\nalthough mathematically more accurate, the full-sum decod ing\nleads to the same accuracy w .r.t single best in the lattice, w hile\nthe size of search space is generally larger. However, apply ing\nconfusion network decoding on the lattice additionally fur ther\nimproves the full-sum result from 11.7% to 11.4%, while no\n11.6\n11.8\n12\n12.2\n12.4\n12.6\n12.8\n13\n13.2\n13.4\n0 0 .5 1 1 .5 2 2 .5 3 3 .5\nWER[%]\nRTF\nRecombine n=9\nRescore: backoff-LM lattice\nRescore: LSTM+backoff lattice (n=2)\nFigure 4: Comparison of ﬁrst-pass recognition, lattice rescor-\ning of backoff-models and lattice rescoring of LSTM-LM base d\nlattices\nCorpus 1-st pass LM Recmb. Rescr. LM WER RTF\ndev-clean\nbackoff N/A - 3.72 0.56\nN/A interpolated 2.55 0.97\ninterpolated 10 - 2.39 6.95\n2 interpolated 2.40 4.06\ndev-other backoff N/A\n-\n8.73 2.25\ninterpolated none 5.75 13.38\ntest-clean backoff N/A 4.18 0.56\ninterpolated none 2.78 7.14\ntest-other backoff N/A 9.31 2.59\ninterpolated none 6.23 15.62\nT able 3: Results of various decoding strategies with backoff and\nLSTM-LM for the Librispeech dataset\ndifference is obtained for Viterbi decoding. W e will furthe r in-\nvestigate the effect of full-sum decoding in future work.\n4.8. Librispeech\nW e conducted initial experiments on Librispeech but did not\nyet complete a full analysis. Our results can be found in T a-\nble 3. Due to the signiﬁcantly larger vocabulary compared wi th\nthe Switchboard system (200k vs 30k) our absolute RTF are\nmuch higher when using the LSTM-LM in one-pass mode (if\none wants to get the best possible WER). The best performing\nstrategy on Switchboard of using an LSTM LM in ﬁrst pass with\na short recombination limit and rescoring the resulting lat tice is\nagain the best performing strategy at a RTF of one. However th e\nbest WER is only reached at an RTF far above one. This could\nbe alleviated by training LSTM LMs using Noise Contrastive\nEstimation [11]. This is reserved for future work. Another o b-\nservation of interest here is that decoding on the other condition\ntakes much longer than on the clean condition. This is not too\nsurprising as decoding an utterance where the acoustic mode l\nis less conﬁdent will yield more scores that are close togeth er.\nHere the difference in WER is a factor of around 2, while the\ndifference in RTF is a factor of around 4.\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n3\n3.1\n3.2\n3.3\n3.4\n3.5\n3.6\n3.7\n0 0.5 1 1 .5 2 2 .5 3 3 .5 4 4 .5 5 5 .5 6 6 .5 7\nWER[%]\nRTF\nRecombine n=10\nRescore: backoff-LM lattice\nRescore: LSTM+backoff lattice (n=2)\nFigure 5: Comparison of ﬁrst-pass recognition, lattice rescor-\ning of backoff-models and lattice rescoring of LSTM-LM base d\nlattices for the Librispeech dev-clean corpus\n5. Conclusions\nIn this paper we have shown how to use LSTM-LMs in decod-\ning using a GPGPU. W e have shown that ﬁrst using the LSTM\nLM with a small recombination limit and doing lattice rescor -\ning afterwards yields the most efﬁcient decoding process. T his\napproach yields a WER of 11.7% on the Hub5’00 task at an\nRTF of 1. Further work is required for systems with very large\nvocabulary where the best possible WER is only reached for a\nRTF well above 1. Further improvements to the WER (11.7%\nto 11.4%) were obtained by full-sum decoding with subsequen t\nconfusion-network decoding.\n6. Acknowledgments\nThis project has received funding from the European Researc h\nCouncil (ERC) under the European Unions Horizon 2020 re-\nsearch and innovation program (grant agreement No 694537,\nproject ”SEQCLAS”) and from the European Unions Hori-\nzon 2020 research and innovation program under the Marie\nSkodowska-Curie grant agreement No 644283. The work re-\nﬂects only the authors’ views and the European Research Coun -\ncil Executive Agency (ERCEA) is not responsible for any use\nthat may be made of the information it contains. Eugen Beck\nwas partially funded by the 2016 Google PhD Fellowship for\nNorth America, Europe and the Middle East. W e also want to\nthank our colleagues Kazuki Irie, Christoph L ¨ uscher and Wi l-\nfried Michel for providing us with the acoustic/language mo d-\nels.\n7. References\n[1] W . Xiong, L. Wu, F . Alleva, J. Droppo, X. Huang, and A. Stol cke,\n“The microsoft 2017 conversational speech recognition sys tem, ”\nin 2018 IEEE International Conference on Acoustics, Speech an d\nSignal Processing (ICASSP) , April 2018, pp. 5934–5938.\n[2] G. Saon, G. Kurata, T . Sercu, K. Audhkhasi, S. Thomas, D. D im-\nitriadis, X. Cui, B. Ramabhadran, M. Picheny , L. Lim, B. Room i,\nand P . Hall, “English conversational telephone speech reco gnition\nby humans and machines, ” in Interspeech 2017, 18th Annual Con-\nference of the International Speech Communication Associa tion,\nStockholm, Sweden, August 20-24, 2017 , F . Lacerda, Ed. ISCA,\n2017, pp. 132–136.\n[3] K. J. Han, A. Chandrashekaran, J. Kim, and I. R. Lane, “The CA-\nPIO 2017 conversational speech recognition system, ” CoRR, vol.\nabs/1801.00059, 2018.\n[4] R. Kneser and H. Ney, “Improved backing-off for m-gram la n-\nguage modeling, ” in 1995 International Conference on Acoustics,\nSpeech, and Signal Processing , vol. 1, May 1995, pp. 181–184\nvol.1.\n[5] A. Deoras, T . Mikolov, S. Kombrink, M. Karaﬁt, and S. Khu-\ndanpur, “V ariational approximation of long-span language models\nfor lvcsr, ” in 2011 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , May 2011, pp. 5532–\n5535.\n[6] G. Lecorv´ e and P . Motl´ ıcek, “Conversion of recurrent n eural net-\nwork language models to weighted ﬁnite state transducers fo r au-\ntomatic speech recognition, ” in INTERSPEECH 2012, 13th An-\nnual Conference of the International Speech Communication As-\nsociation, P ortland, Oregon, USA, September 9-13, 2012 . ISCA,\n2012, pp. 1668–1671.\n[7] E. Arsoy , S. F . Chen, B. Ramabhadran, and A. Sethy, “Con-\nverting neural network language models into back-off langu age\nmodels for efﬁcient decoding in automatic speech recogniti on, ”\nIEEE/ACM Transactions on Audio, Speech, and Language Pro-\ncessing, vol. 22, no. 1, pp. 184–192, Jan 2014.\n[8] H. Adel, K. Kirchhoff, N. T . V u, D. T elaar, and T . Schultz,\n“Comparing approaches to convert recurrent neural network s\ninto backoff language models for efﬁcient decoding, ” in INTER-\nSPEECH 2014, 15th Annual Conference of the International\nSpeech Communication Association, Singapore, September 1 4-\n18, 2014 , H. Li, H. M. Meng, B. Ma, E. Chng, and L. Xie, Eds.\nISCA, 2014, pp. 651–655.\n[9] M. Singh, Y . Oualil, and D. Klakow , “ Approximated and dom ain-\nadapted lstm language models for ﬁrst-pass decoding in spee ch\nrecognition, ” in Proc. Interspeech 2017 . ISCA, 2017, pp. 2720–\n2724.\n[10] M. Gutmann and A. Hyv¨ arinen, “Noise-contrastive esti mation: A\nnew estimation principle for unnormalized statistical mod els, ” in\nProceedings of the Thirteenth International Conference on Artiﬁ-\ncial Intelligence and Statistics, AISTATS 2010, Chia Lagun a Re-\nsort, Sardinia, Italy , May 13-15, 2010 , ser . JMLR Proceedings,\nY . W . T eh and D. M. Titterington, Eds., vol. 9. JMLR.org, 2010 ,\npp. 297–304.\n[11] M. Gutmann and A. Hyvarinen, “Noise-contrastive estim ation of\nunnormalized statistical models, with applications to nat ural im-\nage statistics, ” Journal of Machine Learning Research , vol. 13,\npp. 307–361, 2012.\n[12] X. Chen, X. Liu, M. J. F . Gales, and P . C. W oodland, “Recur rent\nneural network language model training with noise contrast ive es-\ntimation for speech recognition, ” in 2015 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP ),\nApril 2015, pp. 5411–5415.\n[13] A. Sethy, S. Chen, E. Arisoy, and B. Ramabhadran, “Unnor mal-\nized exponential and neural network language models, ” in 2015\nIEEE International Conference on Acoustics, Speech and Sig nal\nProcessing (ICASSP) , April 2015, pp. 5416–5420.\n[14] Y . Huang, A. Sethy , and B. Ramabhadran, “Fast neural net work\nlanguage model lookups at n-gram speeds, ” in Proc. Interspeech\n2017, 2017, pp. 274–278.\n[15] A. Deoras, T . Mikolov , and K. Church, “ A fast re-scoring strat-\negy to capture long-distance dependencies, ” in Proceedings of the\nConference on Empirical Methods in Natural Language Proces s-\ning, ser . EMNLP ’11. Stroudsburg, P A, USA: Association for\nComputational Linguistics, 2011, pp. 1116–1127.\n[16] X. Liu, Y . W ang, X. Chen, M. J. F . Gales, and P . C. W ood-\nland, “Efﬁcient lattice rescoring using recurrent neural n etwork\nlanguage models, ” in 2014 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , May 2014,\npp. 4908–4912.\n[17] M. Sundermeyer, Z. T ¨ uske, R. Schl ¨ uter, and H. Ney , “La ttice\ndecoding and rescoring with long-span neural network langu age\nmodels, ” in INTERSPEECH 2014, 15th Annual Conference of\nthe International Speech Communication Association, Sing apore,\nSeptember 14-18, 2014 , H. Li, H. M. Meng, B. Ma, E. Chng, and\nL. Xie, Eds. ISCA, 2014, pp. 661–665.\n[18] X. Liu, X. Chen, Y . W ang, M. J. F . Gales, and P . C. W oodland ,\n“T wo efﬁcient lattice rescoring methods using recurrent ne ural\nnetwork language models, ” IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing , vol. 24, no. 8, pp. 1438–1449,\nAug 2016.\n[19] S. Kumar, M. Nirschl, D. N. Holtmann-Rice, H. Liao, A. T .\nSuresh, and F . X. Y u, “Lattice rescoring strategies for long short\nterm memory language models in speech recognition, ” in 2017\nIEEE Automatic Speech Recognition and Understanding W ork-\nshop, ASRU 2017, Okinawa, Japan, December 16-20, 2017 .\nIEEE, 2017, pp. 165–172.\n[20] H. Xu, T . Chen, D. Gao, Y . W ang, K. Li, N. Goel, Y . Carmiel,\nD. Povey, and S. Khudanpur, “ A pruned rnnlm lattice-rescori ng\nalgorithm for automatic speech recognition, ” in 2018 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Proces sing\n(ICASSP), April 2018, pp. 5929–5933.\n[21] Z. Huang, G. Zweig, and B. Dumoulin, “Cache based recurr ent\nneural network language model inference for ﬁrst pass speec h\nrecognition, ” in 2014 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , May 2014, pp.\n6354–6358.\n[22] T . Hori, Y . Kubo, and A. Nakamura, “Real-time one-pass d e-\ncoding with recurrent neural network language model for spe ech\nrecognition, ” in 2014 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , May 2014, pp.\n6364–6368.\n[23] K. Lee, C. Park, I. Kim, N. Kim, and J. Lee, “ Applying GPGP U\nto recurrent neural network language model based fast netwo rk\nsearch in the real-time L VCSR, ” in Proc. Interspeech 2015 .\nISCA, 2015, pp. 2102–2106.\n[24] F . Morin and Y . Bengio, “Hierarchical probabilistic ne ural net-\nwork language model, ” in Proceedings of the T enth International\nW orkshop on Artiﬁcial Intelligence and Statistics, AISTAT S 2005,\nBridgetown, Barbados, January 6-8, 2005 , R. G. Cowell and\nZ. Ghahramani, Eds. Society for Artiﬁcial Intelligence and\nStatistics, 2005.\n[25] T . Mikolov , A. Deoras, D. Povey , L. Burget, and J. Cernoc k ´ y,\n“Strategies for training large scale neural network langua ge mod-\nels, ” in 2011 IEEE W orkshop on Automatic Speech Recognition &\nUnderstanding, ASRU 2011, W aikoloa, HI, USA, December 11-\n15, 2011 , D. Nahamoo and M. Picheny , Eds. IEEE, 2011, pp.\n196–201.\n[26] K. Lee, C. Park, N. Kim, and J. Lee, “ Accelerating recurr ent\nneural network language model based online speech recognit ion\nsystem, ” in 2018 IEEE International Conference on Acoustics,\nSpeech and Signal Processing, ICASSP 2018 . IEEE, 2018, pp.\n5904–5908.\n[27] D. Nolden, “Progress in decoding for large vocabulary c ontinuous\nspeech recognition, ” Ph.D. dissertation, R WTH Aachen Univ er-\nsity , Computer Science Department, R WTH Aachen University ,\nAachen, Germany , Apr . 2017.\n[28] V . Panayotov , G. Chen, D. Povey , and S. Khudanpur, “Lib-\nrispeech: an asr corpus based on public domain audio books, ” in\nProc. IEEE Int. Conf. on Acoustics, Speech and Signal Proces sing\n(ICASSP). IEEE, 2015, pp. 5206–5210.\n[29] R. Schl ¨ uter, I. Bezrukov , H. W agner, and H. Ney , “Gamma -\ntone features and feature combination for large vocabulary speech\nrecognition, ” in IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , vol. 4, April 2007, pp.\nIV –649–IV –652.\n[30] M. Gibson and T . Hain, “Hypothesis spaces for minimum ba yes\nrisk training in large vocabulary speech recognition, ” in INTER-\nSPEECH 2006 - ICSLP , Ninth International Conference on Spo-\nken Language Processing, Pittsburgh, P A, USA, September 17 -21,\n2006. ISCA, 2006.\n[31] C. L ¨ uscher, E. Beck, K. Irie, M. Kitza, W . Michel, A. Zey er,\nR. Schl ¨ uter, and H. Ney , “Rwth asr systems for librispeech: Hy-\nbrid vs attention, ” in submitted to Interspeech , 2019.",
  "topic": "Decoding methods",
  "concepts": [
    {
      "name": "Decoding methods",
      "score": 0.8963819742202759
    },
    {
      "name": "Computer science",
      "score": 0.8542589545249939
    },
    {
      "name": "Language model",
      "score": 0.6717156171798706
    },
    {
      "name": "Word (group theory)",
      "score": 0.5463736653327942
    },
    {
      "name": "Speech recognition",
      "score": 0.5349406599998474
    },
    {
      "name": "Lattice (music)",
      "score": 0.5299295783042908
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3590346574783325
    },
    {
      "name": "Natural language processing",
      "score": 0.3492797613143921
    },
    {
      "name": "Algorithm",
      "score": 0.3213547468185425
    },
    {
      "name": "Linguistics",
      "score": 0.11232146620750427
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887968799",
      "name": "RWTH Aachen University",
      "country": "DE"
    }
  ]
}