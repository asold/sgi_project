{
    "title": "Industrial Language-Image Dataset (ILID): Adapting Vision Foundation Models for Industrial Settings",
    "url": "https://openalex.org/W4404789553",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Moenck, Keno",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Thieu, Duc Trung",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2747972382",
            "name": "Koch Julian",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Sch√ºppstuhl, Thorsten",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3214845421",
        "https://openalex.org/W4205177130",
        "https://openalex.org/W4304780935",
        "https://openalex.org/W4285137833",
        "https://openalex.org/W2771008497",
        "https://openalex.org/W2948982773",
        "https://openalex.org/W3118600296",
        "https://openalex.org/W4212874935",
        "https://openalex.org/W4380715392",
        "https://openalex.org/W4381337046",
        "https://openalex.org/W4392172801",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4288283129",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4385327621",
        "https://openalex.org/W4306820534",
        "https://openalex.org/W4287326757",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2949650786",
        "https://openalex.org/W2593001809",
        "https://openalex.org/W4365456844",
        "https://openalex.org/W4385261873",
        "https://openalex.org/W4312854774",
        "https://openalex.org/W4389713784",
        "https://openalex.org/W4385373671",
        "https://openalex.org/W4388927959",
        "https://openalex.org/W2157487986",
        "https://openalex.org/W4287758736",
        "https://openalex.org/W4287692599",
        "https://openalex.org/W3091546937",
        "https://openalex.org/W4310430488",
        "https://openalex.org/W4300176521",
        "https://openalex.org/W2592962403",
        "https://openalex.org/W4297448178",
        "https://openalex.org/W4287990345",
        "https://openalex.org/W3100859887",
        "https://openalex.org/W3207750165",
        "https://openalex.org/W4281930370",
        "https://openalex.org/W4385714437",
        "https://openalex.org/W4229042118",
        "https://openalex.org/W3215495159",
        "https://openalex.org/W3212456749",
        "https://openalex.org/W4226381516",
        "https://openalex.org/W4361194507",
        "https://openalex.org/W3205789812",
        "https://openalex.org/W3198377975",
        "https://openalex.org/W4287208373",
        "https://openalex.org/W4226427271",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W6908809",
        "https://openalex.org/W3011304563",
        "https://openalex.org/W4289639938",
        "https://openalex.org/W4362679702",
        "https://openalex.org/W4387947074",
        "https://openalex.org/W3201074629"
    ],
    "abstract": "In recent years, the upstream of Large Language Models (LLM) has also encouraged the computer vision community to work on substantial multimodal datasets and train models on a scale in a self-/semi-supervised manner, resulting in Vision Foundation Models (VFM), as, e.g., Contrastive Language-Image Pre-training (CLIP). The models generalize well and perform outstandingly on everyday objects or scenes, even on downstream tasks, tasks the model has not been trained on, while the application in specialized domains, as in an industrial context, is still an open research question. Here, fine-tuning the models or transfer learning on domain-specific data is unavoidable when objecting to adequate performance. In this work, we, on the one hand, introduce a pipeline to generate the Industrial Language-Image Dataset (ILID) based on web-crawled data; on the other hand, we demonstrate effective self-supervised transfer learning and discussing downstream tasks after training on the cheaply acquired ILID, which does not necessitate human labeling or intervention. With the proposed approach, we contribute by transferring approaches from state-of-the-art research around foundation models, transfer learning strategies, and applications to the industrial domain.",
    "full_text": null
}