{
    "title": "Why Do Masked Neural Language Models Still Need Common Sense Knowledge?",
    "url": "https://openalex.org/W2982906145",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5051373234",
            "name": "Sunjae Kwon",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5057564773",
            "name": "Cheongwoong Kang",
            "affiliations": [
                "Ulsan National Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5006991898",
            "name": "Jiyeon Han",
            "affiliations": [
                "Ulsan National Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5052985764",
            "name": "Jaesik Choi",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963829073",
        "https://openalex.org/W2963371565",
        "https://openalex.org/W2923290299",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2964207259",
        "https://openalex.org/W2908854766",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W1532325895",
        "https://openalex.org/W2107901333",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2084450415",
        "https://openalex.org/W2005814556",
        "https://openalex.org/W2963995027",
        "https://openalex.org/W2970161131",
        "https://openalex.org/W2125436846",
        "https://openalex.org/W2971600926",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2766508367"
    ],
    "abstract": "Currently, contextualized word representations are learned by intricate neural network models, such as masked neural language models (MNLMs). The new representations significantly enhanced the performance in automated question answering by reading paragraphs. However, identifying the detailed knowledge trained in the MNLMs is difficult owing to numerous and intermingled parameters. This paper provides empirical but insightful analyses on the pretrained MNLMs with respect to common sense knowledge. First, we propose a test that measures what types of common sense knowledge do pretrained MNLMs understand. From the test, we observed that MNLMs partially understand various types of common sense knowledge but do not accurately understand the semantic meaning of relations. In addition, based on the difficulty of the question-answering task problems, we observed that pretrained MLM-based models are still vulnerable to problems that require common sense knowledge. We also experimentally demonstrated that we can elevate existing MNLM-based models by combining knowledge from an external common sense repository.",
    "full_text": "Why Do Masked Neural Language Models Still Need Common Sense Knowledge?\nSunjae Kwon,1 Cheongwoong Kang,2 Jiyeon Han,2 Jaesik Choi,1\n1Graduate School of ArtiÔ¨Åcial Intelligence, Korea Advanced Institute of Science and Technology\n2School of Electrical & Computer Engineering, Ulsan National Institute of Science and Technology\nsj91kwon@kaist.ac.kr, cwkang0204@unist.ac.kr, jiyeon@unist.ac.kr, jaesik.choi@kaist.ac.kr\nAbstract\nCurrently, contextualized word representations are\nlearned by intricate neural network models, such as\nmasked neural language models (MNLMs). The new\nrepresentations signiÔ¨Åcantly enhanced the performance\nin automated question answering by reading para-\ngraphs. However, identifying the detailed knowledge\ntrained in the MNLMs is difÔ¨Åcult owing to numerous\nand intermingled parameters. This paper provides em-\npirical but insightful analyses on the pretrained MNLMs\nwith respect to common sense knowledge. First, we pro-\npose a test that measures what types of common sense\nknowledge do pretrained MNLMs understand. From the\ntest, we observed that MNLMs partially understand var-\nious types of common sense knowledge but do not accu-\nrately understand the semantic meaning of relations. In\naddition, based on the difÔ¨Åculty of the question answer-\ning task problems, we observed that pretrained MLM-\nbased models are still vulnerable to problems that re-\nquire common sense knowledge. We also experimen-\ntally demonstrated that we can elevate existing MNLM-\nbased models by combining knowledge from an exter-\nnal common sense repository.\n1 Introduction\nA long-standing problem and a goal of natural language pro-\ncessing (NLP) is to teach machines to effectively understand\nlanguage and infer knowledge (Winograd 1972). In NLP,\nreading comprehension (RC) is a task to predict the correct\nanswer in the associated context for a given question. RC is\nwidely regarded as an evaluation benchmark for a machine‚Äôs\nability for the natural language understanding and reasoning\n(Richardson, Burges, and Renshaw 2013).\nNeural language models (NLMs) that consist of neu-\nral networks to predict a word sequence distribution have\nwidely been utilized in natural language understanding tasks\n(Radford et al. 2018). In particular, masked neural language\nmodels (MNLMs) including BERT (Devlin et al. 2019),\nthat are trained to restore randomly masked sequence of\nwords, have recently led to a breakthrough in various RC\ntasks. However, the ‚Äò black box ‚Äô nature of the neural net-\nworks prohibits analyzing what type of knowledge leads to\nCopyright c‚Éù 2019, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nperformance enhancement and what knowledge remains un-\ntrained.\nRecently, there has been active research work that tries to\nexplain which type of knowledge is trained on the pretrained\nNLMs. One common approach is to probe the existence of\nsimple linguistic features such as parts of speech (S ¬∏ahin\net al. 2019). Previous studies mainly focused on exploring\nwhether the trained model embodies linguistic features for\nsemantic analysis such as tense or named entity recognition\n(NER), and syntactic analysis such as part-of-speech tagging\nor dependency parsing for naturally observed texts. On the\ncontrary, Clark et al. (2019) found that one can well capture\nsyntactic information by observing the self-attention heads‚Äô\nbehavior patterns of the BERT.\nCommon sense knowledge is known to be another essen-\ntial factor for natural language understanding and reasoning\nin the RC task (Mihaylov and Frank 2018). A recent study\nshows how to attain common sense knowledge from pre-\ntrained MNLMs without additional training (Feldman, Davi-\nson, and Rush 2019). However, detailed analysis on which\nknowledge is trained and untrained in the NLMs including\nMNLMs has not been thoroughly examined to the best of\nour knowledge.\nOur main focus in this paper is to verify how much the\nMNLM-based RC models answer or process the compli-\ncated RC tasks by understanding semantic relations among\nthe words. To address this, we raise the following questions\nregarding the common sense understanding of MNLMs:\n1. Do MNLMs understand various types of common sense\nknowledge especially relations of attributes? (Section 3.1)\n2. Do MNLMs understand a relationship between two re-\nlated relations? (Section 3.2)\n3. How do MNLM-based RC models solve problems across\ndifferent levels of difÔ¨Åculty? (Section 4.1)\n4. What are the hardest RC task problems for the MNLM-\nbased RC models? (Section 4.2)\nFor questions 1 and 2, we introduce a new knowledge\nprobing test designed to analyze whether the MNLMs un-\nderstand structured common sense knowledge as seman-\ntic triples in an external repository speciÔ¨Åcally ConceptNet\n(Speer, Chin, and Havasi 2017). Experimental results on the\nknowledge probing test reveal that MNLMs partially under-\nstand various types of common sense knowledge. However\narXiv:1911.03024v1  [cs.CL]  8 Nov 2019\nat the same time, we observe unexpected negative results in\nthat MNLMs have a lot of knowledge still untrained yet, and\ncannot precisely distinguish even the opposite relations.\nFor questions 3 and 4, we Ô¨Årst deÔ¨Åne the difÔ¨Åculty of an\nRC problem with the lexical overlapping between the con-\ntext and the question. Then, we analyze how the MNLMs\nperform on different levels of difÔ¨Åculty and investigate\nwhich type of problems be a critical bottleneck for the cur-\nrent MNLMs. As a result of the analyses, we observe that\nthe lexical variation is a crucial determinant in difÔ¨Åculties\nof the RC task. In addition, we clarify that the problems\nthat require common sense knowledge are challenging for\nthe MNLM-based RC models.\nBased on the above results, we propose a solution that\nwe can ameliorate the limitation of the MNLMs by in-\ntegrating knowledge originated from an external common\nsense repository. To verify our solution, we conducted two\nexperiments. Firstly, we manually changed the question\nto integrate the knowledge which is required to solve the\nproblem. Secondly, we propose a neural network architec-\nture that complements MNLMs with the external common\nsense repository. In both experiments, we observed that\nMNLMs could be complemented by integrating common\nsense knowledge.\nOur main contributions in this paper are as follows:\n‚Ä¢ We proposed a knowledge probing test that measures\ncommon sense knowledge trained in MNLM. We ob-\nserved that MNLMs neither do understand knowledge\ncompletely nor precisely.\n‚Ä¢ By scrutinizing the results of the MNLM based RC mod-\nels, we observed that current MNLMs have a critical bot-\ntleneck with regard to solving problems requiring com-\nmon sense knowledge.\n‚Ä¢ We empirically veriÔ¨Åed that MNLMs can be supple-\nmented by integrating the external common sense reposi-\ntory, manually or automatically.\nThe paper is organized as follows. Section 2 brieÔ¨Çy de-\nscribes the notions required to readily understand our paper.\nSection 3 introduces our knowledge probing test and demon-\nstrates the results of the test. Then, we present the difÔ¨Åculty\nlevels of the RC problems and the limitations of the MNLM\nbased RC model in Section 4. Section 5 discusses what we\nobserved in the previous sections and propose solutions to\nameliorate the limitation. Finally, the conclusion is stated in\nSection 6.\n2 Background\n2.1 Masked Neural Language Models\nIn this paper, we address NLMs that calculate a probability\ndistribution over the sequence of words with neural network.\nEspecially, we mainly discuss the BERT that is reffered as\nthe MNLMs in our paper. Two commonly used models of\nBERT, BERTbase1 and BERTlarge2, are used in our paper.\n1https://storage.googleapis.com/bert models/2018 10 18/\nuncased L-12 H-768 A-12.zip\n2https://storage.googleapis.com/bert models/2018 10 18/\nuncased L-24 H-1024 A-16.zip\nModel Structure The BERT model comprises a trans-\nformer architecture (Vaswani et al. 2017). The model has\nL transformer layers. Each layer comprises S self-attention\nheads and H hidden dimensions.\nTraining Objectives BERT is trained to jointly optimize\ntwo different losses: 1) masked language model (MLM) loss\nand 2) next sentence prediction (NSP) loss.\nDifferent from traditional language models that optimize\nlikelihood of the next word prediction, BERT is optimized\nwith the MLM loss. In the MLM loss, tokens in the text are\nrandomly masked with a special token ‚Äò[MASK]‚Äô at a des-\nignated proportion, and BERT is optimized with the cross-\nentropy loss to predict the correct tokens for the masked in-\nput.\nNSP loss is a binary classiÔ¨Åcation to determine whether\na sentence B naturally follows with A in the data sequence.\nIn a positive example, B is sampled in the original context,\nwhereas in a negative example, B is sampled in a randomly\nselected document.\nPreprocessing For the pretraining, the input of BERT is\na conjunction of two sentences A and B. In the sentences,\neach token is split into a vocabulary, WordPiece (Wu et al.\n2016), with 30,000 tokens. In addition, special delimiter to-\nkens ‚Äò[CLS]‚Äô and ‚Äò[SEP]‚Äô, that indicate ‚ÄòclassiÔ¨Åcation to-\nken‚Äô and ‚Äòsentence separate token‚Äô, respectively, are adopted\nto integrate two sentences into the following input thread:\n[CLS], A1, ..., AN ,[SEP], B1...BM ,[SEP] where {Ai}and\n{Bj}are sequential word tokens in sentences A and B.\nTraining Data BERT is trained on integration of two dif-\nferent corpora: Wikipedia (2,500M words) and Book Corpus\n(800M words) (Zhu et al. 2015). For Wikipedia, only text\npassages are elicited and hyperlinks are disregarded.\n2.2 Common Sense Repositories\nBefore we create common sense queries, determining an\nexternal resource where we can extract common senses is\nnecessary. In our paper, we choose ConceptNet, a seman-\ntic network widely exploited as a common sense repository\nin previous studies (Weissenborn, KoÀácisk`y, and Dyer 2017;\nWang et al. 2018; Talmor et al. 2019).\nConceptNet, part of an open mind common sense\n(OMCS) (Singh et al. 2002) project, is a semantic network\ndesigned to help computers understand the words used by\npeople. ConceptNet includes common sense knowledge that\noriginates from several resources: crowdsourcing, expert-\ncreating, and games with a purpose. In our paper, we use\nConceptNet 5.6.0 version3.\n3 Probing Common Sense Knowledge in\nMNLMs\nThis section investigates which types of common sense\nknowledge are included in the pretrained MNLMs. As the\nknowledge has a structured form and MNLMs have complex\n3https://s3.amazonaws.com/conceptnet/downloads/2018/edges/\nconceptnet-assertions-5.6.0.csv.gz\n(a)\n(b)\n(c)\nFigure 1: Representative probabilistic distributions of the knowledge probing test results. (a), (b) and (c) show results of\n‚Äòantonym of sound‚Äô, ‚Äòantonym of spring‚Äô, and ‚Äòantonym of larboard‚Äô, respectively. The y-axis indicates log10 probability and\nthe x-axis denotes the ranking of the words. Correct answers are marked in red.\nand intermingled attribution, clarifying the trained knowl-\nedge is difÔ¨Åcult. The Cloze test (Chapelle and Abraham\n1990), known to be a reliable assessment for the language\nability of a participant, is a task wherein one Ô¨Ålls in the cor-\nrect answer for the blank in the text. In the following exam-\nple, ‚Äúchildren and are opposite .‚Äù, the answer word would\nbe ‚Äòadults‚Äô rather than ‚Äòkids‚Äô. To infer the correct answer, we\nmust know not only the meaning of each word but also the\nsemantic relation between the words. Inspired by the Cloze\ntest, we introduce a test called the knowledge probing test.\nIn the knowledge probing test, we Ô¨Årst transform a seman-\ntic triple (s, r, o) into a sentence that can be used as an input\nto a designated MNLM. We generate sentences through pre-\ndeÔ¨Åned predicate patterns. For example, a predicate pattern\nof the ‚ÄòAntonym‚Äô relation can be ‚Äús and o are opposite .‚Äù The\npredicate patterns of relations are collected from the OMCS\ndataset 4. All templates used in this paper are provided in\nAppendix A.\nThe object in the generated sentence is masked with a spe-\ncial token ‚Äò[MASK]‚Äô such as ‚Äúchildren and [MASK] are op-\nposite.‚Äù MNLMs then try to predict the object from the given\nsubject and relation. We focus on the objects that comprise\na single WordPiece token as they are frequently observed in\nthe pretraining corpus. As a result, we can obtain a prob-\nabilistic distribution for the masked token and measure the\nunderstanding of the MNLMs on common sense knowledge\nby analyzing the probability of the answer object words.\n3.1 Probing on Various Types of Relations\nWe have conducted the knowledge probing test on 37 rela-\ntions in ConceptNet to verify whether the MNLMs are prop-\n4https://s3.amazonaws.com/conceptnet/downloads/2018/omcs-\nsentences-more.txt\nerly trained on each relation. The list of 37 relations is pro-\nvided in Appendix A.\nWhen we visualize the probability distribution on the pre-\ndiction, we discover that the distributions have roughly three\ntypes of aspects. The Ô¨Årst type of the distribution shows\nan ‚ÄòL‚Äô-shapedgraph, where some words have signiÔ¨Åcantly\nhigh probabilities than others. Fig. 1(a) is one example of\nan ‚ÄòL‚Äô-shaped distribution. It shows the probability distribu-\ntion of the predictions for the ‚Äòspring‚Äôs antonym‚Äô. We can\nsee a drastic jump between the probability of ‚Äòwinter‚Äô and\n‚Äòspring‚Äô, which make the Ô¨Ågure look similar to the charac-\nter ‚ÄòL‚Äô. The second type shows a ‚ÄòU‚Äô-shapedgraph, where\nthe probabilities smoothly decrease. Fig. 1(b) is the distri-\nbution for the ‚Äòsound‚Äôs antonym‚Äô, and this is an example of\nthe ‚ÄòU‚Äô-shaped graph that shows a smooth curve in the dis-\ntribution. The last type shows a ‚Äò‚Äì‚Äô-shapedgraph, where all\ncandidates share similar probabilities. Fig. 1(c) is the distri-\nbution of the ‚Äòlarboard‚Äôs antonym‚Äô, and the graph looks like\na bar. We assume the relations that show ‚ÄòL‚Äô-shaped graphs\nare relatively frequently trained on some words as the model\nis more conÔ¨Ådent on the words than others. If the model is\nproperly trained, those words with high probabilities will be\nthe answers, as shown in the Fig. 1(a). On the contrary, we\nassume the relations are not trained frequently in the train-\ning when the results show ‚Äò-‚Äô-shaped graphs as the model is\nnot as conÔ¨Ådent on any of its predictions.\nTable 1 describes the quantitative results of the knowl-\nedge probing test measured by the hits@K metric (Bordes\net al. 2013). Here, we report micro- and macro-average over\nthe relations. We report individual results on each relation\nin Appendix B. Indeed, large Ô¨Çuctuation can be found in the\nquantitative results for each relation. Some relations (‚ÄòDe-\nÔ¨ÅnedAs‚Äô, ‚ÄòIsA‚Äô, ...) show below 20% in hits@100 while\nsome (‚ÄòNotCapableOf‚Äô, ‚ÄòMadeOf‚Äô, ‚ÄòReceivesAction‚Äô) show\nSamples\nTop 10 ranks for 100 samples in 'MadeOf' relation\nothers\nconcrete\nwater\niron\nrubber\nclay\nsteel\nplastic\nglass\nmetal\nwood\nFigure 2: Color coded results on the top 10 words from the model prediction for 100 samples in the ‚ÄôMadeOf‚Äô relation. Colors\nare labeled with the top 10 most-frequent words. We can notice that top 10 words are redundantly observed in the high rank.\nTable 1: Results of micro and macro average hits@K for the\nConceptNet relations. The macro avg. equally average the\nresults of all relations, while the micro avg. weighted aver-\nage the results of the relations according to the their portion.\nModel Hits@K\n1 5 10 100\nMicro Avg. BERTbase 5.85 10.45 14.32 31.52\nBERTlarge 5.49 10.57 14.13 30.23\nMacro Avg. BERTbase 5.94 13.66 17.82 38.52\nBERTlarge 7.99 15.00 19.45 41.14\nabove 70%. The macro-average displays that BERT large\noutperforms BERTbase while the micro-average shows that\nBERTbase has higher performance than BERT large except\nfor a hits@5. This mainly comes from the inconsistency on\ndata distributions between the training dataset of MNLMs\nand ConceptNet, where some relations occupying a large\nportion of ConceptNet are not trained more elaborately in\nthe larger model whilst most relations are trained better in\nthe larger model generally.\nDespite the high hit ratios, we suspect that the semantic\nrelations in MNLMs are not as accurate as expected. As an\nillustrative example, ‚ÄôMadeOf‚Äô relation shows the highest\nhits@10 with more than 50% of samples predicting the cor-\nrect answer within rank 10. However, when we have a closer\nlook at the predictions, some predictions are repeated across\nthe samples. Figure 2 shows the appearance of the 10 fre-\nquent words, regardless of order, in the top 10 predictions\nfor 100 samples of the ‚ÄòMadeOf‚Äô relation. Regardless of the\nsubject, ‚Äòwood‚Äô appears as a high-rank prediction in most\nsamples. This is followed by ‚Äòmetal‚Äô and ‚Äòglass‚Äô as they ap-\npear in more than 70% of samples as high-rank predictions.\nHerein, we observe that the predictions are biased to the\n‚ÄòMadeOf‚Äô relation and the conditional effect of the subject\nis relatively small, leading the model to output the marginal\nprobability of ‚ÄòMadeOf‚Äô with general materials. This can be\nproblematic when those frequent words are deÔ¨Ånitely not the\nright answer, for example, in cases wherein ‚Äòwood‚Äô is pre-\ndicted as the most probable answer for the question ‚ÄúWhat\nis butter made of?‚Äù.\n3.2 Probing the Relationship Between Two\nRelations\nIn the previous section, we discuss the behavior of MNLMs\nfor each relation but the question ‚ÄúDo MNLMs precisely\nunderstand semantic difference between relations?‚Äù has not\nbeen clariÔ¨Åed yet. To answer the question, we observe re-\nsults from the knowledge probing test of opposite relations\non the same subject. In particular, we focus on ‚ÄòAntonym‚Äô\nand ‚ÄòSynonym‚Äô herein because the two relations are oppo-\nsite and the sets of correct answers for the two relations are\nunable to be compatible. In other words, if the MNLMs pre-\ncisely understand the meaning of relations, the results of the\nopposite relation on the same subject should be completely\ndifferent.\n(a)\n‚àí4 ‚àí2 0\nlogP (word|subj, rel)\nmove\ngo\nbreak\nturn\nrun\nyield\nfunction\nfall\nthrow\nplay\nSynonym of ‚Äômove‚Äô\n‚àí4 ‚àí2 0\nlogP (word|subj, rel)\nmove\ngo\nact\nfall\nturn\nfunction\ndie\nyield\nchange\ntalk\nAntonym of ‚Äômove‚Äô\n(b)\n‚àí4 ‚àí2 0\nlogP (word|subj, rel)\ntrust\nlove\nfriendship\nfear\nhope\nrespect\nfaith\nhonor\nloyalty\ndesire\nSynonym of ‚Äôtrust‚Äô\n‚àí4 ‚àí2 0\nlogP (word|subj, rel)\nlove\ntrust\nfear\ndesire\nhope\nfaith\nfriendship\nlust\nanger\nrespect\nAntonym of ‚Äôtrust‚Äô\nFigure 3: Results on the top 10 words on the opposite rela-\ntions on subject words a) ‚Äòmove‚Äô and b) ‚Äòtrust‚Äô. Words com-\nmonly observed in both results are painted in the same color,\nand the other words are in gray.\nFigure 3 indicates illustrative examples of the opposite\nrelations on the same subject words. Unexpectedly, there\nare words simultaneously predicted in both ‚ÄòSynonym‚Äô and\n‚ÄòAntonym‚Äô. In addition, the quantitative result in Table 2\nmanifests that the proportion of overlapping words in the\nTable 2: Results of top K overlapping ratio of the ‚ÄòAntonym‚Äô\nand ‚ÄòSynonym‚Äô relations.\nModel Overlap@K\n1 5 10 100\nBERTbase 61.19 63.47 64.37 68.71\nBERTlarge 58.29 63.60 64.98 69.74\nTable 3: Experimental results on the incorrect rate between\n‚ÄòSynonym‚Äô and ‚ÄòAntonym‚Äô relations.\nModel Template Answer Hits@K\n10 100\nBERTbase\nSynonym Antonym 31.22 55.16\nAntonym Synonym 26.25 47.18\nBERTlarge\nSynonym Antonym 40.18 61.92\nAntonym Synonym 25.02 48.34\nopposite relations is rather high (> 55%), making it evident\nthat the MNLMs learn the approximate theme of the oppo-\nsite relation rather than accurately understand the meaning\nof the opposite relations. To demonstrate that high overlap-\nping is undesirable, we measure the incorrect rate by grad-\ning the predictions with answers from the opposite relations\nthat are extremely unlikely to be answers. Table 3 lists the\nmeasured results. Hits@K in this case can be interpreted as\nthe incorrect rate. As seen from Table 3, the incorrect rate is\nrather high in all cases, considering that the no-hit is desir-\nable. Thus, we conclude that MNLMs such as BERT with\nthe current training scheme, do not discriminate opposite re-\nlations well.\n4 Analysis on the Reading Comprehension\nover the DifÔ¨Åculties of the Questions\nAs reported in the previous section, MNLMs still have\nincomplete common sense knowledge but MNLM-based\nRC models outperform existing approaches (Radford et al.\n2018; Devlin et al. 2019). In this section, we present re-\nsults on how MNLMs solve RC questions for different dif-\nÔ¨Åculty levels (Section 5.1). Subsequently, we report what\ntypes of questions are still challenging for the MNLM-based\nRC models (Section 5.2).\nWe analyzed BERT base, BERTlarge, and a baseline U-\nNet5 model trained on the SQuAD 2.0 RC task dataset (Ra-\njpurkar, Jia, and Liang 2018). This data comprises two types\nof questions: has answer and no answer. The has answer\nquestion contains a contextual answer, whereas the no an-\nswer question does not have a contextual answer. We train\nthe models with default settings. Finally, as we are unable\nto access the test set of SQuAD, all analyses are conducted\nwith the development set.\n(a)\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nCosine Similarity\n0.4\n0.5\n0.6\n0.7\n0.8Exact Match\nDomain A Domain B Domain C Domain D\nlarge\nbase\nunet\n(b)\n0.0 0.1 0.2 0.3 0.4 0.5\nCosine Similarity\n0.7\n0.8\n0.9\n1.0Accuracy\nlarge\nbase\nunet\nFigure 4: Results on the word overlapping rate and difÔ¨Åculty.\nX-axis indicates cosine similarity of context and question,\nand Y-axis denotes its exact matching score. (a) shows re-\nsults of the has answer questions and we marked the do-\nmains with the arrow. (b) shows results of the no answer\nquestions.\n4.1 Comparative Study with respect to TF-IDF\nSimilarity\nWe look at the difÔ¨Åculty of the RC problem based on a sim-\nple lexical overlapping hypothesis. The hypothesis postu-\nlates that the overlap of words in the context and the ques-\ntion strongly correlates with the difÔ¨Åculty level of the RC\nproblem. More speciÔ¨Åcally, we assume that the has answer\nproblem gets easier when the words in the context and ques-\ntion overlap, whereas the no answer problem gets harder in\nthe similar situation.\nTo verify our assumption, we analyze the relationship be-\ntween the lexical overlap of context and question, and the\nperformance of RC models. In our case, we calculate the\nlexical overlap of the context and the question with cosine\nsimilarity between TF-IDF term-weighted uni-gram bag-of-\nwords vectors (Manning, Raghavan, and Sch ¬®utze 2010). In\naddition, we set the performance index of the RC task as an\nexact matching score and an accuracy value for has answer\nand no answer questions.\nFigure 4 demonstrates the experimental results for our hy-\npothesis. The results demonstrate that the has answer ques-\ntions tend to be more difÔ¨Åcult with less lexical overlap-\nping, whereas the no answer shows the opposite tendency.\nParticularly, there are large performance differences among\nBERTlarge, BERTbase, and U-Net in most intervals for the\nhas answer questions. In other words, the lexical difference\nbetween the question and the context determines the difÔ¨Å-\nculty level of the RC problem.\n5https://github.com/FudanNLP/UNet\nTable 4: Question types and their portion on each domain. In the models, Large and Base indicates BERT large and BERTbase\nrespectively. There are 6 question categories and the categories can be tagged with duplicates except semantic variation and no\nsemantic variation.\nDomain\nModels Question Type\nSampling\nratio\nSemantic Variation Multiple\nSentence\nReasoning\nNo\nSemantic\nVariation\nOthers TypoLarge Base U-Net Synonymy Common Sense\nKnowledge\nA Fail Fail Fail 33.00% 57.00% 17.00% 17.00% 2.00% 25.00% 100 / 281\nB Pass Fail Fail 53.57% 17.86% 16.07% 32.14% 5.36% 10.71% 56 / 56\nC Pass Pass Fail 40.45% 15.73% 16.85% 43.82% 1.12% 6.74% 89 / 89\nD Pass Pass Pass 23.00% 12.00% 11.00% 65.00% 0.00% 3.00% 100 / 531\n4.2 What Types of Questions Are Still Hard for\nMNLMs?\nIn this subsection, we analyze which questions account for\nthe performance differences among the RC models. We be-\ngin with dividing the has answer questions with less lexi-\ncal overlapping ( similarity < 0.2), where relatively dif-\nÔ¨Åcult questions are classiÔ¨Åed into four domains: A) ques-\ntions incorrectly answered by all models, B) questions cor-\nrectly answered only by the BERT large, C) questions cor-\nrectly answered by the BERT base and BERTlarge, and D)\nquestions correctly answered by all models. For each do-\nmain, we sample a maximum of 100 questions. Then, by re-\nferring the question types in (Rajpurkar et al. 2016), we cat-\negorize each question into the six classes listed in Table 4. In\nthis case, synonymy class means there is a synonym relation\nbetween answer sentence and question. The common sense\nknowledge class indicates that common sense is required to\nsolve a question. The no semantic variation category de-\nnotes that the question has neither synonymy nor common\nsense knowledge. Multiple sentence reasoning class indi-\ncates that there are anaphora or clues scattered across mul-\ntiple sentences. Others class indicates that the presented an-\nswers have been incorrectly tagged. Finally, the typo class\ndenotes a typographical error in a question or answer sen-\ntence. Detailed explanations and examples have been pro-\nvided in Appendix C.\nThe experimental results show that the proportion of se-\nmantic variation-type questions increases through domain D\nto A. Especially, the common sense-type questions demon-\nstrate dramatic enhancement in domain A compared to other\ndomains. In addition, the typo-type questions increase sig-\nniÔ¨Åcantly in domain A. However, it also manifests that com-\nmon sense knowledge or typo-type questions are still not\nhandled yet by the MNLM-based RC models.\n5 Discussion and Suggested Solutions\n5.1 What are the Fundamental Limitations of\nCurrent MNLM Learning?\nSection 3 reveals that MNLMs have incomplete informa-\ntion of the common sense knowledge and imprecisely un-\nderstand the semantic relations, whereas MNLMs can proÔ¨Å-\nciently model contextualized word distribution from the text\n(Tenney et al. 2019).\nFrom these Ô¨Åndings, we presume that MNLMs learn only\n1) the observed information in the corpus and 2) the co-\noccurrence of the words instead of Ô¨Åguring out semantic\nrelations among the words while they train contextualized\nword distribution. On the contrary, it is difÔ¨Åcult for MNLMs\nto infer the knowledge not observed in the corpus.\nFirst, MNLMs are incapable of inferring semantic rela-\ntions that can be inferred from what it already knows. For\nexample, even if an MNLM understands ‚Äòcomputer‚Äô is made\nof ‚Äòtransistors‚Äô and ‚Äòtransistors‚Äô are made of ‚Äòsilicon‚Äô, it fails\nto infer ‚Äòcomputer‚Äô is made of ‚Äòsilicon‚Äô if it is hardly found\nin the corpus.\nSecond, MNLMs are still naive in understanding the re-\nlation over relations. As shown in Section 3.2, the perfor-\nmances of synonym and antonym relations remain almost\nunchanged even when we grade them interchangeably, af-\nter which it is expected to be signiÔ¨Åcantly degraded. This\nillustrates that MNLMs cannot characterize synonyms and\nantonyms properly, although they have an obvious relation.\n5.2 Why do MNLMs Perform Well on QA Tasks\nwithout Semantic Relations?\nDespite the limitations, MNLMs such as BERT have shown\nsigniÔ¨Åcant improvements in the RC task. However, in Sec-\ntion 4, we observed that (1) lexical differences affect the dif-\nÔ¨Åculty level of the RC task and (2) there are several RC prob-\nlems which can be easily solved without extra knowledge.\nOn the contrary, particular problems could not be solved by\ncurrent MNLM-based RC models and require knowledge\non semantic variation. In short, it can be inferred that the\nsemantic variation-type of problems related to synonymy\nand common sense knowledge remain weak points for the\nMNLM-based RC models.\n5.3 MNLMs Need a Learned or Learnable\nExternal Common Sense Repository\nAs seen from previous discussions, to implicitly embed\ncommon sense knowledge in current MNLM learning, the\ntraining data should contain all the common sense knowl-\nedge that requires a tremendously large corpus and a large\nmodel accordingly. It is almost impossible to gather all\ncommon sense knowledge and build such a large model.\nHowever, it is obvious that common sense knowledge can\nhelp ameliorate the weakness of the current MNLM-based\nBERT\nConceptNet\n(war, IsA, military action)\n(consider, Synonym, think)\n‚Ä¶\n(can, SimilarTo, may)\nùíîùíÜùíèùíïùíäùíèùíÜùíç ùíÑùë≥ùíÑ\nCommonsense Embeddings\n[CLS] ùííùüè ‚Ä¶ ùííùíè‚àíùüè ùííùíè ùíëùüè ‚Ä¶ ùíëùíé‚àíùüè ùíëùíé[SEP] [SEP]\nùíâùüè ùíâùüê ‚Ä¶ ùíâùíè ùíâùíè+ùüè ùíâùíè+ùüë ‚Ä¶ ùíâùíè+ùíé+ùüè ùíâùíè+ùíé+ùüêùíâùíè+ùüê ùíâùíè+ùíé+ùüë\nùíìùë≥ùíÑ ùíêùë≥ùíÑùíîùë≥ùíÑ\nBi-LSTM + Self-Attention + Softmax\nStart Probability End Probability\nùíÑùüêùíÑùüè\nùíìùüê ùíêùüêùíîùüê ‚Ä¶\nAttention\nùíìùüè ùíêùüèùíîùüè\nCommonsense2Text\nKey (ùë≤)\nQuery (ùë∏) Linear\nLinear\nLinear\nAnswer prediction\nAttention Attention\nCommonsense Encoder\n‚Ä¶\nSoftmax\nText Encoder\nValue (ùëΩ)\nHidden (ùëØ)\nCommon sense \nembeddings (ùë™)\nFigure 5: The architecture of our commonsense knowledge incorporated question answering model.\nTable 5: Empirical analysis on the performances when\nadapting an external common sense repository. In the table,\nC2T is an abbreviation of ‚Äòcommon sense to text‚Äô indicating\nthat we integrate the external common sense repository to\nthe MNLMs.\nModel has answer no answer overall\nf1 exact accuracy f1 exact\nBERTbase 73.39 67.68 80.10 76.75 73.90\n+ C2T 78.30 72.17 77.75 78.02 74.96\nBERTlarge 79.33 73.48 80.89 80.11 77.28\n+ C2T 80.20 74.43 83.53 81.87 78.99\nRC models. We try to verify if the external common sense\nknowledge can be useful for MNLMs in solving RC prob-\nlems on the hardest problems in domain A.\nManually Integrating Common Sense Knowledge First,\nwe manually modify the questions to imply knowledge from\nan external common sense repository and see whether the\nperformance improves. As a result, 56% of the hardest ques-\ntions were correctly answered by the BERTlarge model. This\nshows that we can improve the performance at least for half\nof those questions from the help of an external common\nsense repository.\nAutomatically Integrating Common Sense Knowledge\nSecondly, we design a neural memory network that automat-\nically incorporates the repository to the MNLM. Figure 5\nshows the overall model comprising four parts: (1) text en-\ncoder, (2) common sense encoder, (3) commonsense2text\n(C2T), and (4) answer prediction. In the text encoder, the\nquestion and context are encoded into the set of hidden vec-\ntors H through the BERT. Then, in the common sense en-\ncoder, we extract common sense triples that subject and ob-\nject appear in the text. Elements of each triple are encoded,\nthen pooled into a single vector through an attention mech-\nanism (Bahdanau, Cho, and Bengio 2014). The triple vec-\ntors and a sentinel vector, representing the case where there\nis no relevant knowledge, are gathered to form a common\nsense embedding C. In the commonsense2text, C is selec-\ntively fused into H with the following formula, whereQ is a\nlinear transformation of H, while K and V are linear trans-\nformations of C.\nI = H + Softmax (Q ¬∑K) ¬∑V\nIn the answer prediction, a set of knowledge integrated text\nvectors I is input to the bi-directional long short-term mem-\nory (Bi-LSTM) then through self-attention layer and soft-\nmax function predicting start and end probabilities of the\nanswer position.\nTable 5 lists experimental results of the MNLMs and\nour knowledge integrated RC models on SQuAD. The re-\nsults present that integrating the external common sense\nrepository improves the performance of MNLMs. We ob-\nserved that the knowledge integrated BERT large correctly\nanswered 12 out of 100 questions in Domain A of Fig-\nure 4(a). Especially, all questions except for one question\nare common sense knowledge or synonymy types. It implies\nthat coping with external common sense knowledge can be\na solution for complementing the weakness of MNLMs.\n6 Conclusion\nIn this paper, we have investigated which types of common\nsense knowledge are trained in the pretrained MNLMs by\nproposing a knowledge probing test. We found that MNLMs\npartially understand some common sense knowledge while\nthe trained knowledge is incomplete and not precise to be\ndistinguished from its opposite. We also analyzed how the\nMNLM based RC models perform across different difÔ¨Åculty\nlevels of the RC problems and found that questions requir-\ning common sense knowledge are still challenging to current\nMNLMs. Finally, we empirically veriÔ¨Åed that the limitation\nof the MNLMs can be overcome by integrating common\nsense knowledge into the MNLMs.\nAcknowledgement\nThis work is supported by IITP grant funded by the Ko-\nrea government (MSIT) (2017-0-00255, Autonomous dig-\nital companion framework and application) and IITP grant\nfunded by the Korea government (MSIT) (2017-0-01779,\nXAI).\nReferences\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural ma-\nchine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473.\nBordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and\nYakhnenko, O. 2013. Translating embeddings for modeling\nmulti-relational data. In Proceedings of Advances in Neural\nInformation Processing Systems, 2787‚Äì2795.\nChapelle, C. A., and Abraham, R. G. 1990. Cloze method:\nWhat difference does it make? Language Testing 7(2):121‚Äì\n146.\nClark, K.; Khandelwal, U.; Levy, O.; and Manning, C. D.\n2019. What does bert look at? an analysis of bert‚Äôs attention.\narXiv preprint arXiv:1906.04341.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the Conference of\nthe North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), 4171‚Äì4186.\nFeldman, J.; Davison, J.; and Rush, A. M. 2019. Common-\nsense knowledge mining from pretrained models. In Pro-\nceedings of the Conference on Empirical Methods in Natu-\nral Language Processing.\nManning, C.; Raghavan, P.; and Sch¬®utze, H. 2010. Introduc-\ntion to information retrieval.Natural Language Engineering\n16(1):100‚Äì103.\nMihaylov, T., and Frank, A. 2018. Knowledgeable reader:\nEnhancing cloze-style reading comprehension with external\ncommonsense knowledge. In Proceedings of the Annual\nMeeting of the Association for Computational Linguistics,\nVolume 1 (Long Papers), 821‚Äì832.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training.\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSquad: 100,000+ questions for machine comprehension of\ntext. In Proceedings of the Conference on Empirical Meth-\nods in Natural Language Processing, 2383‚Äì2392.\nRajpurkar, P.; Jia, R.; and Liang, P. 2018. Know what you\ndont know: Unanswerable questions for squad. In Proceed-\nings of the Annual Meeting of the Association for Computa-\ntional Linguistics, Volume 2 (Short Papers), 784‚Äì789.\nRichardson, M.; Burges, C. J.; and Renshaw, E. 2013.\nMctest: A challenge dataset for the open-domain machine\ncomprehension of text. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing , 193‚Äì\n203.\nS ¬∏ahin, G. G.; Vania, C.; Kuznetsov, I.; and Gurevych, I.\n2019. Linspector: Multilingual probing tasks for word rep-\nresentations. arXiv preprint arXiv:1903.09442.\nSingh, P.; Lin, T.; Mueller, E. T.; Lim, G.; Perkins, T.; and\nZhu, W. L. 2002. Open mind common sense: Knowl-\nedge acquisition from the general public. In Proceedings\nof the International Conference on Ontologies, Databases,\nand Applications of Semantics for Large Scale Information\nSystems, 1223‚Äì1237.\nSpeer, R.; Chin, J.; and Havasi, C. 2017. Conceptnet 5.5:\nan open multilingual graph of general knowledge. In Pro-\nceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence ,\n4444‚Äì4451.\nTalmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2019.\nCommonsenseqa: A question answering challenge targeting\ncommonsense knowledge. In Proceedings of the Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), 4149‚Äì4158.\nTenney, I.; Xia, P.; Chen, B.; Wang, A.; Poliak, A.; McCoy,\nR. T.; Kim, N.; Durme, B. V .; Bowman, S.; Das, D.; and\nPavlick, E. 2019. What do you learn from context? prob-\ning for sentence structure in contextualized word representa-\ntions. In Proceedings of International Conference on Learn-\ning Representations.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In Proceedings of Advances in Neu-\nral Information Processing Systems, 5998‚Äì6008.\nWang, L.; Sun, M.; Zhao, W.; Shen, K.; and Liu, J. 2018.\nYuanfudao at semeval-2018 task 11: Three-way attention\nand relational knowledge for commonsense machine com-\nprehension. In Proceedings of the International Workshop\non Semantic Evaluation, 758‚Äì762.\nWeissenborn, D.; KoÀácisk`y, T.; and Dyer, C. 2017. Dynamic\nintegration of background knowledge in neural nlu systems.\narXiv preprint arXiv:1706.02596.\nWinograd, T. 1972. Understanding natural language. Cog-\nnitive psychology 3(1):1‚Äì191.\nWu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .; Norouzi, M.;\nMacherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey, K.;\net al. 2016. Google‚Äôs neural machine translation system:\nBridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144.\nZhu, Y .; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun,\nR.; Torralba, A.; and Fidler, S. 2015. Aligning books and\nmovies: Towards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the IEEE In-\nternational Conference on Computer Vision, 19‚Äì27.\nAppendix A: Details on the Templates\nTable 1: This table presents details of the templates utilized in our paper. Here, we analyze 37 relations in ConceptNet (Speer,\nChin, and Havasi 2017).\nRelation Template # of samples\nRelatedTo [[SUBJ]] is related to [[OBJ]] . 287,459\nHasContext [[SUBJ]] is used in the context of [[OBJ]] . 113,066\nIsA [[SUBJ]] is a [[OBJ]] . 74,316\nDerivedFrom [[OBJ]] is derived from [[SUBJ]] . 69,510\nSynonym [[SUBJ]] and [[OBJ]] are same . 28,379\nFormOf [[OBJ]] is the root word of [[SUBJ]] . 27,208\nEtymologicallyRelatedTo [[SUBJ]] is etymologically related to [[OBJ]] . 10,187\nSimilarTo [[SUBJ]] is similar to [[OBJ]] . 8,384\nAtLocation Something you Ô¨Ånd at [[OBJ]] is [[SUBJ]] . 7,644\nMannerOf [[SUBJ]] is a way to [[OBJ]] . 6,230\nPartOf [[SUBJ]] is part of [[OBJ]] . 5,320\nAntonym [[SUBJ]] and [[OBJ]] are opposite . 3,932\nHasProperty [[SUBJ]] can be [[OBJ]] . 2,886\nUsedFor [[SUBJ]] may be used for [[OBJ]] . 2,145\nDistinctFrom [[SUBJ]] is not [[OBJ]] . 1,256\nHasPrerequisite [[SUBJ]] requires [[OBJ]] . 1,142\nHasSubevent When [[SUBJ]] , [[OBJ]] . 1,119\nCauses [[SUBJ]] causes [[OBJ]] . 999\nHasA [[SUBJ]] contains [[OBJ]] . 943\nInstanceOf [[SUBJ]] is an instance of [[OBJ]] . 902\nCapableOf [[SUBJ]] can [[OBJ]] . 697\nReceivesAction [[SUBJ]] can be [[OBJ]] . 658\nMotivatedByGoal You would [[SUBJ]] because [[OBJ]] . 603\nCausesDesire [[SUBJ]] would make you want to [[OBJ]] . 556\nMadeOf [[SUBJ]] can be made of [[OBJ]] . 316\nHasLastSubevent The last thing you do when you [[SUBJ]] is [[OBJ]] . 302\nEntails [[SUBJ]] entails [[OBJ]] . 298\nHasFirstSubevent The Ô¨Årst thing you do when you [[SUBJ]] is [[OBJ]] . 280\nDesires [[SUBJ]] wants [[OBJ]] . 200\nNotHasProperty [[SUBJ]] is not [[OBJ]] . 161\nCreatedBy [[SUBJ]] is creatd by [[OBJ]] . 118\nDeÔ¨ÅnedAs [[SUBJ]] can be deÔ¨Åned as [[OBJ]] . 80\nNotDesires [[SUBJ]] does not want [[OBJ]] . 71\nNotCapableOf [[SUBJ]] can not [[OBJ]] . 43\nLocatedNear [[SUBJ]] is typically near [[OBJ]] . 36\nEtymologicallyDerivedFrom [[SUBJ]] is etymologically derived from [[OBJ]] . 27\nSymbolOf [[SUBJ]] is an symbol of [[OBJ]] . 4\nAppendix B: Qualitative Analysis for Probabilistic Distributions\nTable 2: Results of the hits@K metric for each relation in ConceptNet.\nRelations\nhits@K\nBERTbase BERTlarge\n1 5 10 100 1 5 10 100\nRelatedTo 7.60 9.30 11.77 25.38 6.51 8.50 10.97 24.14\nHasContext 6.79 16.17 22.38 48.90 6.91 15.84 22.13 47.57\nIsA 0.46 1.56 2.27 15.57 0.41 1.19 1.89 11.67\nDerivedFrom 0.14 5.77 10.70 31.47 0.11 3.41 6.90 23.42\nSynonym 16.16 27.33 33.12 52.70 13.38 26.74 34.69 56.39\nFormOf 0.57 20.10 28.08 42.41 2.84 32.39 38.68 48.76\nEtymologicallyRelatedTo 5.39 8.35 10.71 22.45 3.69 6.59 9.22 21.70\nSimilarTo 1.60 4.39 6.09 14.92 2.84 7.13 10.13 23.61\nAtLocation 2.03 3.72 5.41 23.36 3.04 5.89 8.93 32.28\nMannerOf 2.66 5.05 8.77 35.71 2.17 5.85 9.61 36.25\nPartOf 21.05 34.37 40.91 59.43 24.38 37.18 43.30 58.97\nAntonym 17.14 25.70 32.38 53.69 28.26 34.55 40.65 63.26\nHasProperty 3.22 8.39 12.14 38.04 5.23 12.93 17.75 46.14\nUsedFor 12.87 16.50 21.44 47.16 12.26 14.78 19.25 45.72\nDistinctFrom 1.67 4.36 6.75 23.70 5.10 11.09 15.22 37.81\nHasPrerequisite 11.30 10.56 14.73 37.29 13.75 13.35 17.93 40.54\nHasSubevent 1.79 2.55 4.03 16.20 2.32 3.39 5.11 18.40\nCauses 9.71 12.73 17.05 40.79 10.81 13.90 18.65 45.81\nHasA 4.24 10.55 15.17 40.35 4.67 9.75 14.19 37.22\nInstanceOf 0.00 5.93 10.29 22.43 0.11 4.92 11.12 31.92\nCapableOf 10.04 17.20 24.27 53.13 12.34 22.90 28.19 52.54\nReceivesAction 12.01 28.12 36.51 71.44 14.89 30.52 38.85 72.45\nMotivatedByGoal 0.00 1.07 2.37 17.90 0.00 0.17 0.76 17.74\nCausesDesire 4.32 11.52 17.59 57.25 2.34 7.54 13.95 52.13\nMadeOf 12.34 44.12 51.85 72.94 18.67 42.22 50.63 75.05\nHasLastSubevent 8.61 16.30 22.85 58.73 10.60 18.04 25.09 62.30\nEntails 2.01 4.53 7.38 22.20 2.35 4.53 6.88 24.27\nHasFirstSubevent 12.86 23.96 29.38 63.99 17.50 29.79 37.56 71.55\nDesires 4.00 7.52 7.57 50.90 7.50 9.47 11.12 50.17\nNotHasProperty 4.35 14.29 18.32 42.24 6.83 23.29 27.64 60.87\nCreatedBy 2.54 9.75 15.25 35.88 0.85 5.08 10.17 29.52\nDeÔ¨ÅnedAs 0.00 2.50 3.75 17.92 2.50 4.17 10.42 33.75\nNotDesires 1.41 0.28 2.25 8.74 1.41 1.69 3.66 12.94\nNotCapableOf 16.28 32.56 41.86 73.84 18.60 27.91 40.12 76.74\nLocatedNear 2.78 8.33 13.89 36.11 5.56 8.33 8.33 25.00\nEtymologicallyDerivedFrom 0.00 0.00 0.00 0.00 0.00 0.00 0.00 3.70\nSymbolOf 0.00 50.00 50.00 50.00 25.00 50.00 50.00 50.00\nAppendix C: Details on the Reading Comprehension Question Types\nTable 3: Examples and descriptions for the question type of the has answer questions. The main clues for the categorization of\nthe questions are colored.\nQuestion Types Description Example\nSynonymy There is a clear correspondence be-\ntween question and context.\nQuestion: Which entity is the secondary legisla-\ntive body?\nContext: ... The second main legislative body is\nthe Council, which is composed of different min-\nisters of the member states. ...\nCommon sense\nknowledge\nCommon sense knowledge is required\nto solve the question.\nQuestion: Where is the Asian inÔ¨Çuence strongest\nin Victoria?\nContext: ... Many Chinese miners worked in Vic-\ntoria, and their legacy is particularly strong in\nBendigo and its environs. ...\nNo semantic variation\nThere is no semantic variation such as\nsynonymy or common sense knowl-\nedge.\nQuestion: Who are the un-elected subordinates\nof member state governments?\nContext: ... This means Commissioners are,\nthrough the appointment process, the unelected\nsubordinates of member state governments. ...\nMulti-sentence reasoning Hints for solving questions are shat-\ntered in multiple sentences.\nQuestion: Why didFrance choose to give up con-\ntinental lands?\nContext: ... France chose to cede the former, ...\nThey viewed the economic value of the Caribbean\nislands‚Äô sugar cane ...\nOthers The labeled answer is incorrect.\nQuestion: Who won the battle of Lake George?\nContext: ... The battle ended inconclusively ,\nwith both sides withdrawing from the Ô¨Åeld. ...\nTypo There exist typing errors in the ques-\ntion or context.\nQuestion: What kind of measurements deÔ¨Åne ac-\ncelerlations?\nContext... Accelerations can be deÔ¨Åned through\nkinematic measurements. ..."
}