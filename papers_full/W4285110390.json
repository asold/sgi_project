{
    "title": "Controlling Translation Formality Using Pre-trained Multilingual Language Models",
    "url": "https://openalex.org/W4285110390",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5054828119",
            "name": "Elijah Rippeth",
            "affiliations": [
                "University of Maryland, College Park"
            ]
        },
        {
            "id": "https://openalex.org/A5103140610",
            "name": "Sweta Agrawal",
            "affiliations": [
                "University of Maryland, College Park"
            ]
        },
        {
            "id": "https://openalex.org/A5078390032",
            "name": "Marine Carpuat",
            "affiliations": [
                "University of Maryland, College Park"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3093517588",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3093345276",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W3152788712",
        "https://openalex.org/W3175301726",
        "https://openalex.org/W3006381853",
        "https://openalex.org/W3166567197",
        "https://openalex.org/W2972664115",
        "https://openalex.org/W2963697731",
        "https://openalex.org/W22168010",
        "https://openalex.org/W2998699029",
        "https://openalex.org/W2034742893",
        "https://openalex.org/W4386290290",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W3035016936",
        "https://openalex.org/W2970328625",
        "https://openalex.org/W2572474373",
        "https://openalex.org/W2986670783",
        "https://openalex.org/W2978824654",
        "https://openalex.org/W2467834614",
        "https://openalex.org/W3029164262",
        "https://openalex.org/W3214250531",
        "https://openalex.org/W4285186755",
        "https://openalex.org/W3175315904",
        "https://openalex.org/W2539350388",
        "https://openalex.org/W4287854445",
        "https://openalex.org/W3105214104",
        "https://openalex.org/W3207322569",
        "https://openalex.org/W2758334418"
    ],
    "abstract": "This paper describes the University of Maryland’s submission to the Special Task on Formality Control for Spoken Language Translation at IWSLT, which evaluates translation from English into 6 languages with diverse grammatical formality markers. We investigate to what extent this problem can be addressed with a single multilingual model, simultaneously controlling its output for target language and formality. Results show that this strategy can approach the translation quality and formality control achieved by dedicated translation models. However, the nature of the underlying pre-trained language model and of the finetuning samples greatly impact results.",
    "full_text": "Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), pages 327 - 340\nMay 26-27, 2022c⃝2022 Association for Computational Linguistics\nControlling Translation Formality\nUsing Pre-trained Multilingual Language Models\nElijah Rippeth⇤ and Sweta Agrawal* and Marine Carpuat\nDepartment of Computer Science\nUniversity of Maryland\n{erip,sweagraw,marine}@cs.umd.edu\nAbstract\nThis paper describes the University of Mary-\nland’s submission to the Special Task on For-\nmality Control for Spoken Language Transla-\ntion at IWSLT , which evaluates translation\nfrom English into 6 languages with diverse\ngrammatical formality markers. We investigate\nto what extent this problem can be addressed\nwith a single multilingual model, simultane-\nously controlling its output for target language\nand formality. Results show that this strategy\ncan approach the translation quality and formal-\nity control achieved by dedicated translation\nmodels. However, the nature of the underlying\npre-trained language model and of the ﬁnetun-\ning samples greatly impact results.\n1 Introduction\nWhile machine translation (MT) research has pri-\nmarily focused on preserving meaning across lan-\nguages, linguists and lay-users alike have long\nknown that pragmatic-preserving communication\nis an important aspect of the problem (Hovy, 1987).\nTo address one dimension of this, several works\nhave attempted to control aspects of formality in\nMT (Sennrich et al., 2016; Feely et al., 2019;\nSchioppa et al., 2021). Indeed, this research\narea was formalized as formality-sensitive machine\ntranslation (FSMT ) by Niu et al.(2017), where\nthe translation is not only a function of the source\nsegment but also the desired target formality. The\nlack of gold translation with alternate formality\nfor supervised training and evaluation has lead re-\nsearchers to rely on manual evaluation and syn-\nthetic supervision in past work (Niu and Carpuat,\n2020). Additionally, these works broadly adapt to\nformal and informal registers as opposed to speciﬁ-\ncally controlling grammatical formality.\nThe Special Task on Formality Control on Spo-\nken Language Translation provides a new bench-\nmark by contributing high-quality training datasets\n⇤ equal contribution.\nSource: Do you like1 Legos? did you2 ever\nplay with them as a child or even later?\nGerman Informal: Magst du1 Legos? Hast\ndu2 jemals als Kind mit ihnen gespielt oder\nsogar später?\nGerman Formal:Mögen Sie1 Legos? Haben\nSie2 jemals als Kind mit ihnen gespielt oder\nsogar später?\nTable 1: Contrastive formal and informal translations\ninto German. Grammatical formality markers are\nbolded and aligned via indices.\nfor diverse languages (N˘adejde et al., 2022). In this\ntask, a source segment in English is paired with two\nreferences which are minimally contrastive in gram-\nmatical formality, one for each formality level (for-\nmal and informal; Table1). Training and test sam-\nples are provided in the domains of “telephony data”\nand “topical chat” (Gopalakrishnan et al., 2019) for\nfour language pairs (English-{German (DE), Span-\nish (ES), Hindi (HI), Japanese(JA)}) and a test\ndataset for two additional “zero-shot” (ZS) lan-\nguage pairs (EN-{Russian (RU), Italian (IT)}).\nMarkers of grammatical formality vary across these\nlanguages. Personal pronouns and verb agreement\nmark formality in many Indo-European languages\n(e.g., DE, HI, IT, RU, ES), while inJA, Korean\n(KO) and other languages, distinctions can be more\nextensive (e.g., using morphological markers) to\nexpress polite, respectful, and humble speech.\nIn this work, we investigate how to control gram-\nmatical formality inMT for many languages with\nminimal resources. Speciﬁcally, we ask whether a\nsingle multilingual model can be ﬁnetuned to trans-\nlate in the appropriate formality for any of the task\nlanguages. We introduce additive vector interven-\ntions to encode style on top of mT5-large (Xue\net al., 2021) and mBART-large (Liu et al., 2020),\nand investigate the impact of ﬁnetuning on varying\ntypes of gold and synthetic samples to minimize\nreliance on manual annotation.\n327\n2 Method\nGiven an input sequencex, we design asingle\nmodel that produces an output\ny⇤ = arg maxp(y|x, l, f; ✓LM ,✓ F )\nfor any languagel and formality levelf considered\nin this task. The bulk of its parameters✓LM are\ninitialized with a pre-trained multilingual language\nmodel. A small number of additional parameters\n✓F enable formality control. All parameters are\nﬁnetuned for formality-controlled translation.\n2.1 Multilingual Language Models\nWe experiment with two underlying multilingual\nmodels: 1)mT5-large1 — a multilingual variant of\nT5 that is pre-trained on the Common Crawl-based\ndataset covering101 languages and 2)mBART-\nlarge2 — a Transformer encoder-decoder which\nsupports multilingual machine translation for50\nlanguages. While m BART-large is pre-trained\nwith parallel and monolingual supervision, mT5-\nlarge uses only monolingual dataset during the\npre-training phase. Following standard practice,\nmT5 controls the output language,l, via prompts\n(“Translate to German”), and mBART replaces the\nbeginning of sequence token in the decoder with\ntarget language tags (<2xx>).\n2.2 Additive Formality Control\nWhile large-scale pre-trained language models\nhave shown tremendous success in multiple mono-\nlingual and multilingual controlled generation\n(Zhang et al., 2022) and style transfer tasks, their\napplication to controlled cross-lingual text gener-\nation have been limited. Few-shot style-transfer\napproaches (Garcia et al., 2021; Riley et al., 2021;\nKrishna et al., 2022) hold the promise of minimal\nsupervision but perform poorly on low-resource\nsettings and their outputs lack diversity.\nA popular way of introducing control when\ngenerating text with a particular style attribute\nis tagging, where the desired control tags (e.g.,\n<2formal>) are appended to the source or the tar-\nget sequence. However, as discussed inSchioppa\net al.(2021), this approach has several limitations,\nincluding but not limited to the necessity of includ-\ning the control tokens in the vocabulary at the start\n124 layers with 1024 sized embeddings, 2816 FFN embed-\nding dimension, and 16 heads for both encoder and decoder.\n212 layers with 1024 sized embeddings, 4096 FFN embed-\nding dimension, and 16 heads for both encoder and decoder.\nFigure 1: Controlling the output formality of a multilin-\ngual language model with additive interventions.\nof the training, which restricts the enhancement of\npre-trained models with controllability.\nWe introduce formality control by adapting the\nvector-valued interventions proposed bySchioppa\net al.(2021) for machine translation (MT), as il-\nlustrated in Figure1. Formally, given source text\nx, a formality levelf, an encoderE and decoder\nD, parameterized by✓LM , and a style embedding\nlayer (Emb) parameterized by✓F with the same\noutput dimension asE, we have\nZ = E(x),V = Emb(f)\ny = D(Z + V )\nOur formality levels can take values corresponding\nto formal, informal, and “neutral” translations, the\nlast of which is used to generate “generic” transla-\ntions in which there is no difference in the gram-\nmatical formality of the translation of the source if\ntranslated formally or informally. UnlikeSchioppa\net al.(2021) who use a zero-vector as their neutral\nvector, we learn a separate vector.\n2.3 Finetuning\nFinetuning each multilingual model requires\ntriplets of the form(x, y, f) for each available tar-\nget language,l, wherex, y and f are the source text,\nthe reference translation and the formality label cor-\nresponding to the reference translation respectively.\nThe loss function is then given by:\nL =\nX\n(x,y,l,f )\nlog p(y|x, l, f; ✓LM ,✓ F ) (1)\nGiven paired contrastivetraining samples of the\nform (X, Yf ,Y if ), as provided by the shared task,\nthe loss decomposes into balanced formal and in-\nformal components, but does not explicitly exploit\n328\nLanguage Size Length Style\nTrain Test Source Formal Informal Avg. TER # Phrasal # Neutral\nEN-DE 400 600 22.78 24.68 24.57 0.126 1.89 23\nEN-ES 400 600 22.72 22.64 22.60 0.089 1.56 49\nEN-HI 400 600 22.90 25.92 25.92 0.068 1.57 68\nEN-JA 1000 600 24.61 32.43 30.80 0.165 2.47 20\nTable 2: Shared Task Data Statistics: We use “13a” tokenization for all languages except Japanese for which we use\n“ja-mecab” implemented in the sacrebleu library.\nthe fact thatYi and Yf align to the same input:\nL =\nX\n(x,yf ,l)\nlog p(yf |x, l, f; ✓LM ,✓ F )+\nX\n(x,yif ,l)\nlog p(yif |x, l, if; ✓LM ,✓ F )\n(2)\n2.4 Synthetic Supervision\nSince paired contrastive samples are expensive to\nobtain, we explore the use of synthetic training sam-\nples to replace or complement them. This can be\ndone either by automatically annotating naturally\noccurring bitext for formality, which yields formal\nand informal samples, and additionally by rewrit-\ning the translation to alter its formality to obtain\npaired contrastive samples. The second approach\nwas used byNiu and Carpuat(2020) to control the\nregister ofMT output. However, since this shared\ntask targets grammatical formality and excludes\nother markers of formal vs. informal registers, we\nfocus on the ﬁrst approach, thus prioritizing control\non the nature of the formality markers in the out-\nput over the tighter supervision provided by paired\ncontrastive samples.\nGiven a translation example(x, y), we predict a\nsilver-standard formality label (f) for the targety\nusing two distinct approaches:\n• Rules (ES, DE, IT, RU): We label formality\nusing heuristics based on keyword search, de-\npendency parses, and morphological features.\nWe use spaCy (Honnibal et al., 2020) to (non-\nexhaustively) retrieve documents that imply a\nnecessarily formal, necessarily informal, or am-\nbiguously formal label. In the case of an ambigu-\nously formal label, we treat it as unambiguously\nformal (for examples, seeB). The complete set\nof rules for each of the languages are included\nin the Appendix Table12. While simple to im-\nplement, these heuristics privilege precision over\nrecall, and risk biasing the synthetic data to the\nfew grammatical aspects they encode.\n• Classiﬁers (HI, JA, IT, RU): We train a binary\nformal vs. informal classiﬁer on the shared task\ndata (HI, JA) and on the synthetic data (IT,\nRU). Unlike rules, they can also be transferred\nin a zero-shot fashion to new languages, and\nmight be less biased toward precision when well-\ncalibrated.\n3 Evaluation Settings\nData The shared task provides English source\nsegments paired with two contrastive reference\ntranslations, one for each formality level (informal\nand formal) for four language pairs:EN-{DE, ES,\nJA, HI} in thesupervised setting and two language\npairs: EN-{RU, IT} in thezero-shot setting. The\nsizes and properties of the datasets for the super-\nvised language pairs are listed in Table2. Formal\ntexts tend to be longer and more diverse than infor-\nmal texts forJA compared to other language pairs.\nThe percentage of neutral samples (same formal\nand informal outputs) vary from2% (in JA) to17%\n(in HI). In thezero-shot setting, 600 test samples\nare released for the two language pairs (RU, IT).\nDuring development, the last 50 paired con-\ntrastive examples from each domain are set aside\nas a validation set for each of the supervised lan-\nguages (TASK DEV) and use the remaining samples\nfor training (TASK TRAIN ).\nMetrics We evaluate the translation quality of the\ndetruecased detokenized outputs from each systems\nusing BLEU (Papineni et al., 2002) andCOMET\n(Rei et al., 2020). We use the13A tokenizer to re-\nport SACRE BLEU 3 scores for all languages, except\nJapanese, for which we use theJA-MECAB .W e\nalso report the ofﬁcialformality accuracy(ACC.).\nGiven a set of hypothesesH, sets of corresponding\nphrase-annotated formal referencesF and informal\n3https://pypi.org/project/sacrebleu/2.\n0.0/\n329\nModel Target Language Size Source\nSynthetic Finetuned\nJA 15K JParaCrawl (Morishita et al., 2020)\nHI 13K CCMatrix (Schwenk et al., 2021b)\nIT, RU 15K Paracrawl v8 (Bañón et al., 2020)\nDE 15K CommonCrawl, Europarl v7 (Koehn, 2005)\nES 15K CommonCrawl, Europarl v7, UN (Ziemski et al., 2016)\nBilingual Baselines\nDE,ES,IT,RU 20M Paracrawl v9\nHI 0.7M CCMatrix\nJA 3.2M Wikimatrix (Schwenk et al., 2021a), JESC (Pryzant et al., 2018)\nTable 3: Data sources from which unlabeled formality parallel examples are sampled forEN-X for training the\nSynthetic Finetunedand theBilingual baselines.\nreferences IF , and a function\u0000 yielding phrase-\nlevel contrastive terms from a reference, the task-\nspeciﬁc evaluation metric is deﬁned as follows:\nmatchf =\nX\nj\n[\u0000(Fj) 2 Hj ^ \u0000(IFj) /2 Hj]\nmatchi =\nX\nj\n[\u0000(Fj) /2 Hj ^ \u0000(IFj) 2 Hj]\naccj = matchj\nmatchf + matchi\n,j 2{ f,i }\nWe note that the task accuracy is a func-\ntion of the number ofmatches in the hypothe-\nses, not the number of expected phrases, i.e.\nmatchf + matchif k Hk and discuss the im-\nplications in the Appendix (SectionC).\n4 Experimental Conditions\nWe compare multilingual models, where a single\nmodel is used to generate formal and informal\ntranslations for all languages with bilingual models\ntrained for each language pair, as detailed below.\n4.1 Multilingual Models\nData We consider three ﬁnetuning settings:\n• Gold ﬁnetuned: the model is ﬁnetuned only on\npaired contrastiveshared task data (400 to 1000\nsamples per language pair).\n• Synthetic ﬁnetuned: the model is ﬁnetuned on\nsynthetic silver-labelled triplets(up to 7500 sam-\nples per formality level and language as described\nbelow).\n• Two-pass ﬁnetuned: the Synthetic ﬁnetuned\nmodel is further ﬁnetuned on a mixture of gold\ndata and 1000 examples re-sampled from the syn-\nthetic training set for unseen languages, which\nwe use to avoid catastrophic forgetting from the\nsilver ﬁnetuning stage.\nSynthetic samples are drawn from multiple data\nsources (3), sampling at most7500 examples for\neach language and formality level.4 The formality\nlabels are predicted as described in2.4. Rule-based\npredictors directly give a label. With classiﬁers, we\nassign the formal label ifP(formal|y) \u0000 0.85 and\ninformal ifP(formal|y)  0.15.\nWe additionally compare with the translations\ngenerated from the base mBART-large model with\nno ﬁnetuning, referred to as the “formality agnostic\nmBART-large”.\nTraining settings We ﬁnetune mT5-large and\nmBART-large with a batch size of2 and 8 respec-\ntively for10 and 3 epochs respectively. We mask\nthe formality labels used to generate vector-valued\ninterventions with a probability of0.2. The mT5-\nlarge model — “synthetic ﬁnetuned mT5-large”—\nis trained for an additional5 epochs, with a batch\nsize of 2 on a mixture of task data for seen lan-\nguages and a subset of the sampled synthetic data\nfor unseen languages. Again, we mask the formal-\nity tag with probability0.2 except in the case of un-\nseen languages where the formality tag is masked\nwith probability1.0, resulting in the “two-pass ﬁne-\ntuned mT5-large” model.\nFormality Classiﬁers Following Briakou et al.\n(2021), we ﬁnetuneXLM-R on binary classiﬁca-\ntion between formal and informal classes, using\nthe shared task datasets for each of the supervised\nlanguage pairs (DE, ES, JA, HI) and synthetic\ndatasets for zero-shot language pairs (RU, IT). We\ntreat the “neutral” samples as both “formal” and\n“informal” when training the classiﬁers. We use the\nAdam optimizer, a batch size of32, and a learning\nrate of5⇥10\u00003 to ﬁnetune for3 epochs. We report\n4We do not experiment with varying the sizes of the syn-\nthetic dataset due to the time constraints and leave it to the\nfuture work.\n330\nSAMPLES TO EN-DE EN-HI EN-JA EN-ES\nBLEU ACC.B L E UACC.B L E UACC.B L E UACC.\nPaired Contrastive F 35.0 100 28.7 98.7 33.1 95.3 32.6 100\nUnpaired Triplets F 35.5 100 31.6 100 39.6 100 35.5 100\nPaired Contrastive IF 32.7 98.5 26.4 98.3 32.3 100 33.8 100\nUnpaired Triplets IF 35.9 98.6 30.9 98.4 40.3 100 39.6 97.9\nTable 4: Results on theTASK DEV split when trainingAdditive mT5-large with and without contrastive examples:\nSample diversity from Unpaired triplets improve BLEU and Accuracy over paired contrastive samples.\nDATA EN-DE EN-HI EN-JA EN-ES\nPaired Contrastive 0.397 0.371 0.421 0.505\nUnpaired Triplets 0.459 0.415 0.460 0.580\nTable 5: Results on theTASK DEV split: TER between\ngenerated formal and informal sentences.\nthe accuracy of the learned classiﬁers trained on\nthe TASK TRAIN dataset in Appendix Table14.\n4.2 Bilingual Models\nWe consider two types of bilingual models:\n1. Formality Agnostic: These models were re-\nleased by the shared task organizers. Each\nmodel is bilingual and trained on a sample of20\nmillion lines from the Paracrawl Corpus (V9)\nusing the SockeyeNMT toolkit. Models use\nbig transformers with20 encoder layers,2 de-\ncoder layers, SSRU’s in place of decoder self-\nattention, and large batch training.\n2. Formality Speciﬁc (Gold): We ﬁnetune the\nmodels in [1] to generate a formal model and an\ninformal model for each language pair (except\nthe zero-shot language pairs).\nThe effective capacity of the bilingual, formality\nspeciﬁc models is3.14B parameters.Each model\nhas 314M parameters, resulting in(314 ⇥2 ⇥4) =\n2.5B parameters for the four supervised languages\n(DE, ES, HI, JA) and two pre-trained models\n(314 ⇥ 2) = 628M parameters for the unseen lan-\nguages (RU, IT).This is signiﬁcantly larger than\nthe capacities of our single multilingual models\n(Additive mT5-large: 1.25B, Additive mBART-\nlarge: 610M).\n5 System Development Results\nDuring system development, we explore the im-\npact of different types of training samples and ﬁne-\ntuning strategies on translation quality and formal-\nity accuracy on TASK DEV .\nContrastive Samples We estimate the beneﬁts of\nﬁne-tuning on informal vs. formal translations of\nthe same inputs for this task. We train two variants\nof thegold finetuned mT5-large model\nusing 50% of the paired contrastive samples and\n100% of the unpaired triplets (i.e., selecting one for-\nmality level per unique source sentence) from the\nTASK TRAIN samples (Table4). Results show that\nsample diversity resulting from unpaired triplets\nleads to better translation quality as measured by\nBLEU (Average Gain: Formal +3.2. Informal\n+5.38), without compromising on the formality\naccuracy. Training with paired samples result in\nlower TER between formal and informal output\ncompared to unpaired triplets (Table5), suggesting\nthat the outputs generated by the model trained on\npaired samples are more contrastive. This further\nmotivates our two-pass finetuned model\nwhich uses gold contrastive samples on the ﬁnal\nstage of ﬁnetuning to bias the model towards gen-\nerating contrastive MT outputs.\nWhile TASK DEV is too small to make deﬁnitive\nclaims, we report our system development results\nin Tables6 and 7. We observe that ﬁnetuning on\ngold contrastive examples (gold-finetuned)\nimproves the translation quality and accuracy of the\ntranslation models (formality-agnostic),\nhighlighting the importance of limited but high-\nquality in-domain supervision on the resulting\nmodels. Further, each of themT5-large mod-\nels improves in translation quality with additional\ndata and training. While the results are dramatic\ndue to size of bothTASK TRAIN and TASK DEV ,\nthe trends validate the approach to augment both\nmBART-large and the mT5-large with additive\ninterventions to control formality.\n6 Ofﬁcial Results\nSubmissions We submit ﬁve variants of multi-\nlingual models (numbered[1-5] in Tables8-11),\n331\nMODEL EN-DE EN-ES EN-JA EN-HI\nBLEU COMET ACC.B L E UC O M E TACC.B L E UC O M E TACC.B L E UC O M E TACC.\nBilingual\nFormality Agnostic 33.2 0.432 33.8 41.3 0.675 24.5 13.0 -0.093 25.6 27.8 0.464 96.5\nFormality Speciﬁc (Gold) 49.1 0.539 100.0 56.0 0.790 100.0 26.0 0.242 89.1 37.5 0.694 100.0\nMultilingual Model\nmBART-large\nFormality Agnostic 33.3 0.295 68.9 27.0 0.120 56.5 18.3 -0.016 71.9 20.7 0.340 88.4\nGold Finetuned 42.8 0.462 95.9 41.1 0.548 97.7 24.7 0.326 89.4 29.6 0.678 95.6\nmT5-large\nGold Finetuned 53.3 0.260 100.0 53.5 0.427 100.0 49.8 0.645 98.1 43.5 0.359 100.0\nSynthetic Finetuned 64.5 0.557 100.0 50.7 0.345 100.0 58.5 0.837 97.7 61.2 0.844 100.0\nTwo-pass Finetuned 86.8 0.824 100.0 88.2 1.070 100.0 68.3 0.980 100.0 70.4 0.975 100.0\nTable 6: Results on the TASK DEV split in theformal supervised setting. ACC.: formal accuracy.\nMODEL EN-DE EN-ES EN-JA EN-HI\nBLEU COMET ACC.B L E UC O M E TACC.B L E UC O M E TACC.B L E UC O M E TACC.\nBilingual\nFormality Agnostic 37.2 0.470 66.2 45.8 0.691 75.5 13.5 -0.096 74.4 23.7 0.436 3.5\nFormality Speciﬁc (Gold) 48.4 0.557 98.5 55.1 0.813 95.7 22.6 0.182 97.8 36.3 0.675 91.5\nMultilingual Model\nmBART-large\nFormality Agnostic 29.3 0.262 31.1 26.3 0.101 43.5 16.2 -0.036 28.1 18.7 0.330 11.6\nGold Finetuned 39.6 0.456 76.5 40.4 0.582 95.3 21.6 0.289 72.7 27.7 0.631 82.8\nmT5-large\nGold Finetuned 52.8 0.232 100.0 53.8 0.513 100.0 47.3 0.617 100.0 41.7 0.144 100.0\nSynthetic Finetuned 66.0 0.563 100.0 57.6 0.530 100.0 59.0 0.813 98.5 57.7 0.761 100.0\nTwo-pass Finetuned 86.6 0.843 100.0 87.7 1.081 100.0 69.5 0.976 100.0 70.1 0.957 100.0\nTable 7: Results on the TASK DEV split in theinformal supervised setting. ACC.: informal accuracy.\nand compare them to the bilingual models built on\ntop of the organizers’ baselines. We ﬁrst discuss\nresults on the ofﬁcial test split for thesupervised\nsetting (Tables8, 9). To better understand the de-\ngree of overall control afforded, we also report the\naverage scores of the formal and informal settings\nin Table 10 before turning to thezero-shot setting\nin Table11.\nMultilingual Approach The best multilingual\nmodels ([1] & [4]) consistently outperform\nthe bilingual formality-agnostic\nbaselines, improving both translation quality\n(Worst-case gain in Average BLEU : Formal\n(+1.67), Informal: (+3.7)) and formality accuracy\n(Worst-case gain in Average ACC.: Formal\n(+40.38), Informal: (+31.6)). They approach the\nquality of formal and informal bilingual systems,\nbut the gap in translation quality and formality\naccuracy varies across languages. While forDE\nand ES, there is a large difference in translation\nquality (approx. 10 BLEU points) between the\nmultilingual models and the bilingual baselines,\nthe multilingual models consistently get higher\nformality accuracy across language pairs and style\ndirections and also perform comparably with the\nbilingual models in matching the translation quality\nfor HI and JA. We attribute these differences\nto the amount of training data used across the\nlanguage pairs (HI: 0.7M toDE 20M). This is an\nencouraging result, since the bilingual approach\nuses a much larger language-speciﬁc parameter\nbudget and bitext for training than the all purpose\nmultilingual models, which can beneﬁt from\ntransfer learning across languages.\nmBART vs. m T5 The gold finetuned\nmBART-large model achieves the best overall\ntranslation quality among the multilingual variants\nas expected given that mBART-large is pre-trained\non parallel text. Its translation quality is higher\nthan that of mT5-large models according toBLEU\nand COMET for all languages exceptHI (infor-\nmal), which could be attributed to the nature and\namount of pre-training data used forHI. Its formal-\nity accuracy is in the90’s and within5 percentage\n332\nEN-DE EN-ES EN-JA EN-HI\nBLEU COMET ACC.B L E UC O M E TACC.B L E UC O M E TACC.B L E UC O M E TACC.\nBilingual Models\nFormality Agnostic 33.0 0.472 53.6 37.5 0.646 37.9 14.9 -0.102 23.3 26.5 0.519 98.8\nFormal Gold Finetuned45.9 0.557 100.0 48.6 0.734 98.4 26.0 0.290 87.1 23.0 0.303 98.9\nMultilingual Models\nmBART-large\nFormality Agnostic 35.1 0.344 83.6 26.9 0.210 67.8 18.3 0.051 93.4 20.1 0.383 93.5\n[4]Gold Finetuned 38.6 0.484 93.6 38.3 0.549 96.7 26.1 0.397 78.2 29.7 0.691 98.5\nmT5-large\n[3]Gold Finetuned 7.9 -1.472 100.0 5.2 -1.340 97.0 8.9 -0.792 88.5 3.9 -1.152 99.6\n[2]Synthetic Finetuned 22.1 0.076 92.4 28.1 0.274 86.5 16.3 -0.086 84.5 22.6 0.305 99.5\n[1]Two-pass Finetuned 37.0 0.302 99.4 38.6 0.509 99.5 24.7 0.273 86.3 29.9 0.471 99.4\nTable 8: Results on the ofﬁcial test split in theformal supervised setting. Best scores from multilingual and bilingual\nsystems arebolded. Our ofﬁcial submissions to the shared task are numbered[1-4].\nMODEL EN-DE EN-ES EN-JA EN-HI\nBLEU COMET ACC.B L E UC O M E TACC.B L E UC O M E TACC.B L E UC O M E TACC.\nBilingual Models\nFormality Agnostic 32.3 0.476 46.4 40.4 0.672 62.1 15.5 -0.094 76.7 20.8 0.493 1.2\nFormality Speciﬁc (Gold)43.5 0.559 90.0 48.2 0.762 92.9 23.5 0.272 98.7 31.2 0.724 92.1\nMultilingual Models\nmBART-large\nFormality Agnostic 28.4 0.299 16.4 25.3 0.205 32.2 16.2 0.032 6.6 16.7 0.370 6.5\n[4]Gold Finetuned 36.1 0.472 77.4 38.3 0.549 82.7 22.8 0.346 88.0 27.6 0.670 64.7\nmT5-large\n[3]Gold Finetuned 7.3 -1.424 96.0 5.9 -1.295 96.1 7.2 -0.795 98.9 2.7 -1.205 96.5\n[2]Synthetic Finetuned 21.7 0.046 91.4 28.2 0.337 91.6 13.6 -0.135 83.3 17.8 0.277 8.3\n[1]Two-pass Finetuned 35.9 0.301 96.5 38.0 0.539 93.2 22.3 0.252 97.5 29.2 0.439 98.7\nTable 9: Results on the ofﬁcial test split in theinformal supervised setting. Best scores from multilingual and\nbilingual systems arebolded. Our ofﬁcial submissions to the shared task are numbered[1-4].\npoints to the highest score for all languages except\nJapanese (78.2%) in the formal direction. In the\ninformal direction, the gap between mBART-large\nand the best system on formality accuracy is larger\nacross the board (Average Acc.:+19.3), suggest-\ning that ﬁnetuning on gold data cannot completely\nrecover an informal translation despite generally\nstrong performance in formal translations.\nFinetuning strategies Results show that the com-\nbination of synthetic and gold data is crucial to\nhelp the mT5-large-based model learn to trans-\nlate and mark formality appropriately. Finetun-\ning only on the gold data leads to overﬁtting: the\nmodel achieves high formality accuracy scores, but\npoor translation quality (BLEU < 10). Manual\ninspection of mT5-large-based system outputs sug-\ngests that translations often include tokens in the\nwrong language (Appendix Table13). Finetun-\ning on synthetic data improves translation qual-\nity substantially compared to gold data only (Av-\nerage gain inBLEU : Formal (+15.8), Informal\n(+14.6)). Two-pass ﬁnetuning improves formality\ncontrol (Average gain inACC.: Formal (+5.43), In-\nformal (+27.85)), with additional translation qual-\nity improvement across the board over synthetic-\nﬁnetuned model (Average gain inBLEU : Formal\n(+10.27), Informal (+11.03); COMET : Formal\n(+0.247), Informal (+0.252)). While we primarily\nfocused on the impact of synthetic supervision on\nmT5-large, we believe a similar investigation using\nmBART-large would yield interesting results and\nleave this as future work.\nPerformance across languages While the higher\nresource language pairs (DE, ES) achieve better\ntranslation quality (inBLEU and COMET ) over\nthe relatively lower resource languages (HI, JA),\nthe formality accuracy is more comparable across\nthe language pairs for the multilingual models\n333\nMODEL EN-DE EN-ES EN-JA EN-HI\nBLEU COMET ACC.B L E UC O M E TACC.B L E UC O M E TACC.B L E UC O M E TACC.\nBilingual Models\nFormality Agnostic 32.7 0.474 50.0 39.0 0.659 50.0 15.2 -0.100 50.0 23.7 0.506 50.0\nFormality Speciﬁc (Gold)44.7 0.558 95.0 48.4 0.748 95.7 24.8 0.281 92.9 27.1 0.513 95.5\nMultilingual Models\nmBART-large\nFormality Agnostic 31.8 0.322 50.0 26.1 0.207 50.0 17.3 0.041 50.0 18.4 0.377 50.0\n[4]Gold Finetuned 37.4 0.478 85.5 38.3 0.549 89.7 24.5 0.371 83.1 28.7 0.680 81.6\nmT5-large\n[3]Gold Finetuned 7.6 -1.448 98.0 5.6 -1.317 96.6 8.1 -0.794 93.7 3.3 -1.179 98.1\n[2]Synthetic Finetuned 21.9 0.061 91.9 28.2 0.305 89.1 15.0 -0.111 83.9 20.2 0.291 53.9\n[1]Two-pass Finetuned 36.5 0.301 98.0 38.3 0.524 96.4 23.5 0.263 91.9 29.6 0.455 99.1\nTable 10: Averaged formal and informal results on the ofﬁcial test split in thesupervised setting. Best scores from\nmultilingual and bilingual systems arebolded. Our ofﬁcial submissions to the shared task are numbered[1-4].\nMODEL\nTo Formal To Informal\nEN-IT EN-RU EN-IT EN-RU\nBLEU COMET ACC.B L E UC O M E TACC.B L E U C O M E T ACC.B L E UC O M E TACC.\nBilingual baselines 37.0 0.557 4.5 27.9 0.220 93.3 44.2 0.618 95.5 22.0 0.169 6.7\n[1]mT5-large (ZS) 27.6 0.306 32.8 22.7 0.123 100.0 32.6 0.379 97.9 17.0 0.058 1.1\n[4]mBART-large (ZS)30.2 0.545 38.7 26.2 0.275 100.0 35.0 0.597 95.9 20.8 0.203 13.8\n[5]mT5-large (FS) 27.1 0.302 49.7 20.7 0.007 100.0 31.2 0.346 93.3 15.5 -0.050 1.8\nTable 11: Results on the ofﬁcial test split for thezero-shot setting. Our ofﬁcial submissions to the shared task are\nnumbered [1-5].\n(standard deviation: mT5-large (4), mBART-large\n(10)). We can observe that the task accuracy is low-\nest (< 90%) when translating to formal Japanese.\nBy inspection, we observe three broad classes of er-\nrors: 1) lexical choice, 2) cross-script matching, 3)\nambiguity in politeness levels (Feely et al., 2019).\nLexical choice is invariant in machine translation\nand is occasionally a valid error in the case of mis-\ntranslation, so we focus on the latter two error cases.\nJapanese has three writing systems and false pos-\nitives in formality evaluation can occur when sur-\nface forms do not match as in the case ofs√⌅\nwhich can also be written as⌦B⌫M⌅ (gloss:\n‘interesting’). Finally, there are cases in which the\nsystem and reference formality mismatch but can\nboth be interpreted as formal (e.g.,\"\u0000>⇡ vs.\n\"✏; gloss: ‘work’ (polite) vs. ‘work’ (formal)).\nZero-Shot We observe limited zero-shot trans-\nfer of grammatical formality to unseen lan-\nguages (Table11). For bothmBART-large and\nmT5-large models, theEN-IT performance is\nbiased towards informal translations, whileEN-\nRU is biased in the formal direction. In the case of\nEN-IT, both mBART-large and mT5-large almost\nalways interpret the English second person pronoun\nas second personplural when translating to formal,\nexploiting the ambiguity of English on the source\nside. By contrast, when generating informal transla-\ntions, pronouns are typically preserved as singular.\nIn comparison, with mT5-large-based translations\ninto RU, we see almost unanimous preference to-\nward the formal, likely due to sampling bias when\ncurating the synthetic training set. We also observe\nthat mBART-large prefers to translate in a formal\nmanner irrespective of desired target. In addition,\nwhen mBART-large fails to account for the tar-\nget formality, it often generates paraphrases of the\nformal target. These strong preferences might be\nsymptoms of systematic differences in formality\nacross languages in the training data of these mod-\nels. Finally, the use of silver standard formality\nlabels (“fully supervised” setting (FS)) does not\nimprove over the zero-shot approach, with similar\nobservations of mT5-large-based translations as\noutlined above. We observe that in the case ofEN-\nRU, there is a higher incidence of code-switched\ntranslations. This may indicate noise introduced in\nthe automatic labeling process and requires further\nexamination in future work.\n334\n7 Related Work\nMost MT approaches only indirectly capture the\nstyle properties of the target text. While efforts\nhave been made to generate better outputs in their\npragmatic context via controlling formality (Sen-\nnrich et al., 2016; Feely et al., 2019; Niu and\nCarpuat, 2020; Schioppa et al., 2021), complex-\nity (Marchisio et al., 2019; Agrawal and Carpuat,\n2019), gender (Rabinovich et al., 2017), these stud-\nies only focus a single language pair. Due to the\npaucity of style annotated corpora, zero-shot style\ntransfer within and across languages has received\na lot of attention. However, adapting pre-trained\nlarge-scale language models during inference us-\ning only a few examples (Garcia et al., 2021; Riley\net al., 2021; Krishna et al., 2022) limits their trans-\nfer ability and the diversity of their outputs. While\nprior works use pre-trained language models like\nBERT, GPT to intialize✓LM for improving trans-\nlation quality (Guo et al., 2020; Zhu et al., 2019),\nin this work, we focus on adapting sequence-to-\nsequence multilingual models for controlled gener-\nation of a desired formality and study style transfer\nin multilingual supervised and zero-shot settings.\n8 Conclusion\nWe present the University of Maryland’s submis-\nsion which examines the performance of a single\nmultilingual model allowing control of both tar-\nget language and formality. Results show that\nwhile multilingualFSMT models lag behind large,\nbilingual, formality-speciﬁc models in terms of\nMT quality, they show stronger formality control\nperformance across all the language pairs. Fur-\nthermore, while synthetic unpaired triplets help\nmT5-large with FSMT performance and the\ntwo-stage ﬁnetuning process improvesMT quality\nand contrastive task performance,mBART-large\nstill outperforms this class of models, likely due to\nits large amount of pre-training supervision.\nIn future work, we suggest a deeper investiga-\ntion of potentially confounding roles in the study\nof FSMT , such as the impact of formal register\nas compared to grammatical formality in training\ndata. We also suggest a thorough analysis ofwhat\nis transferred in the zero-shot setting. Finally, we\nrecommend an audit of underlying pre-training and\nﬁnetuning data sources for pre-trained multilingual\nmodels, which we believe hinder zero-shot formal-\nity transfer forEN-IT and EN-RU in which a sin-\ngle formality is strongly preferred.\nReferences\nSweta Agrawal and Marine Carpuat. 2019.Controlling\ntext complexity in neural machine translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1549–\n1564, Hong Kong, China. Association for Computa-\ntional Linguistics.\nMarta Bañón, Pinzhen Chen, Barry Haddow, Kenneth\nHeaﬁeld, Hieu Hoang, Miquel Esplà-Gomis, Mikel L.\nForcada, Amir Kamran, Faheem Kirefu, Philipp\nKoehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere,\nGema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec,\nBrian Thompson, William Waites, Dion Wiggins, and\nJaume Zaragoza. 2020.ParaCrawl: Web-scale acqui-\nsition of parallel corpora. InProceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4555–4567, Online. Association\nfor Computational Linguistics.\nEleftheria Briakou, Sweta Agrawal, Joel Tetreault, and\nMarine Carpuat. 2021. Evaluating the evaluation\nmetrics for style transfer: A case study in multilin-\ngual formality transfer. InProceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1321–1336, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nWeston Feely, Eva Hasler, and Adrià de Gispert.\n2019. Controlling Japanese honoriﬁcs in English-\nto-Japanese neural machine translation. InProceed-\nings of the 6th Workshop on Asian Translation, pages\n45–53, Hong Kong, China. Association for Computa-\ntional Linguistics.\nXavier Garcia, Noah Constant, Ankur Parikh, and Orhan\nFirat. 2021.Towards continual learning for multilin-\ngual machine translation via vocabulary substitution.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1184–1192, Online. Association for Computa-\ntional Linguistics.\nKarthik Gopalakrishnan, Behnam Hedayatnia, Qin-\nlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu\nVenkatesh, Raefer Gabriel, and Dilek Hakkani-Tür.\n2019. Topical-Chat: Towards Knowledge-Grounded\nOpen-Domain Conversations. InProc. Interspeech\n2019, pages 1891–1895.\nJunliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei,\nBoxing Chen, and Enhong Chen. 2020. Incorpo-\nrating bert into parallel sequence decoding with\nadapters. Advances in Neural Information Process-\ning Systems, 33:10843–10854.\nMatthew Honnibal, Ines Montani, Soﬁe Van Lan-\ndeghem, and Adriane Boyd. 2020.spaCy: Industrial-\nstrength Natural Language Processing in Python.\n335\nEduard Hendrik Hovy. 1987.Generating Natural Lan-\nguage under Pragmatic Constraints. Ph.D. thesis,\nUSA. AAI8729079.\nPhilipp Koehn. 2005.Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of\nMachine Translation Summit X: Papers, pages 79–86,\nPhuket, Thailand.\nKalpesh Krishna, Deepak Nathani, Xavier Garcia,\nBidisha Samanta, and Partha Talukdar. 2022.Few-\nshot controllable style transfer for low-resource mul-\ntilingual settings.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020.Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nKelly Marchisio, Jialiang Guo, Cheng-I Lai, and Philipp\nKoehn. 2019. Controlling the reading level of ma-\nchine translation output. InProceedings of Machine\nTranslation Summit XVII: Research Track, pages 193–\n203, Dublin, Ireland. European Association for Ma-\nchine Translation.\nMakoto Morishita, Jun Suzuki, and Masaaki Nagata.\n2020. JParaCrawl: A large scale web-based English-\nJapanese parallel corpus. In Proceedings of the\n12th Language Resources and Evaluation Confer-\nence, pages 3603–3609, Marseille, France. European\nLanguage Resources Association.\nXing Niu and Marine Carpuat. 2020. Controlling neural\nmachine translation formality with synthetic super-\nvision. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 34, pages 8568–8575.\nXing Niu, Marianna Martindale, and Marine Carpuat.\n2017. A study of style in machine translation: Con-\ntrolling the formality of machine translation output.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2814–2819, Copenhagen, Denmark. Association for\nComputational Linguistics.\nMaria N˘adejde, Anna Currey, Benjamin Hsu, Xing\nNiu, Marcello Federico, and Georgiana Dinu. 2022.\nCoCoA-MT: A dataset and benchmark for Con-\ntrastive Controlled MT with application to formality.\nIn Findings of the Association for Computational Lin-\nguistics: NAACL 2022, Seattle, USA. Association for\nComputational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002.Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nReid Pryzant, Youngjoo Chung, Dan Jurafsky, and\nDenny Britz. 2018.JESC: Japanese-English subtitle\ncorpus. InProceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nElla Rabinovich, Raj Nath Patel, Shachar Mirkin, Lucia\nSpecia, and Shuly Wintner. 2017.Personalized ma-\nchine translation: Preserving original author traits. In\nProceedings of the 15th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Volume 1, Long Papers, pages 1074–1084,\nValencia, Spain. Association for Computational Lin-\nguistics.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020.COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685–2702, Online. Association\nfor Computational Linguistics.\nParker Riley, Noah Constant, Mandy Guo, Girish\nKumar, David Uthus, and Zarana Parekh. 2021.\nTextSETTR: Few-shot text style extraction and tun-\nable targeted restyling. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3786–3800, Online. Association\nfor Computational Linguistics.\nAndrea Schioppa, David Vilar, Artem Sokolov, and\nKatja Filippova. 2021.Controlling machine transla-\ntion for multiple attributes with additive interventions.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n6676–6696, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzmán. 2021a.Wiki-\nMatrix: Mining 135M parallel sentences in 1620 lan-\nguage pairs from Wikipedia. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 1351–1361, Online. Association for Computa-\ntional Linguistics.\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov,\nEdouard Grave, Armand Joulin, and Angela Fan.\n2021b. CCMatrix: Mining billions of high-quality\nparallel sentences on the web. InProceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 6490–6500, Online. As-\nsociation for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Controlling politeness in neural machine trans-\nlation via side constraints. InProceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\n336\nLanguage Technologies, pages 35–40, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021.mT5: A massively multilingual\npre-trained text-to-text transformer. InProceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nHanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou,\nand Dawei Song. 2022. A survey of controllable\ntext generation using transformer-based pre-trained\nlanguage models.arXiv preprint arXiv:2201.05337.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tieyan Liu. 2019.\nIncorporating bert into neural machine translation. In\nInternational Conference on Learning Representa-\ntions.\nMichał Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016. The United Nations parallel cor-\npus v1.0. InProceedings of the Tenth International\nConference on Language Resources and Evaluation\n(LREC’16), pages 3530–3534, Portorož, Slovenia.\nEuropean Language Resources Association (ELRA).\n337\nA Rules for Synthetic Data Curation\nLANG Formal Informal\nen-de (P=2 ∈M and Num=Plural∈M) or PP=Sie P=2 ∈M and Num=Plural/∈M\nen-es P=2 ∈M and Form=Polite∈M P=2 ∈M and Num=Singular∈M and Form=Polite/∈M\nen-it PP=voi or PP=lei PP=tu\nen-ru PP= Вы PP=ты\nTable 12: Rules for extracting formal and informal sentences for each language pair from existing bitext. P: Person;\nPP: Personal pronoun; N: Number; x∈M indicates that some token within the sentence has morphological features\nmatching xas produced by spaCy.\nB Glosses\nB.1 Necessarily formal\nAppropriate pronouns with accompanying conjugation imply the sentence is grammatically formal.\n(1) ¿Cuándo\nWhen\nnació\nborn\nusted?\nyou (form.)?\n(Spanish)\n‘When were you (form.) born?’\n(2) Woher\nWhere from\nkommen\ncome\nSie?\nyou (form.)?\n(German)\n‘Where are you (form.) from?’\nB.2 Necessarily informal\nAppropriate pronouns with accompanying conjugation imply the sentence is grammatically informal.\nNote that Spanish is pro-drop, which relaxes the requirement on personal pronouns.\n(3) ¿Cuándo\nWhen\nnaciste\nborn\n(tú)?\nyou (inf.)?\n(Spanish)\n‘When were you (inf.) born?’\n(4) Woher\nWhere from\nkommst\ncome\ndu?\nyou (inf.)?\n(German)\n‘Where are you (inf.) from?’\nB.3 Ambiguously formal\nBecause Spanish is pro-drop, personal pronouns can be omitted depending on context. Since formal\nconjugations are shared with neutral third person subjects, this leaves ambiguity when the pronoun is\ndropped. For sake of gloss, we use ∅to indicate a dropped pronoun.\n(5) ¿Cuándo\nWhen\nnació\nborn\n∅?\n{you (form.), he, she, it}?\n‘When {were you (form.), was {he, she, it}} born?’\nC Official Evaluation\nWe report the number of examples labeled as FORMAL , INFORMAL , NEUTRAL , OTHER by the\nformality scorer for the best multilingual models ( [1, 4]) and the baseline systems for each language\npair and formality direction. As described in 3, the accuracy is computed based on realized matches,\nwhich excludes examples labelled as NEUTRAL and OTHER . Figure 2 shows that the number of these\nexcluded NEUTRAL samples can range from 15% to 43%.\n338\nD Example Outputs\nSource: Wow, that’s awesome! Who is your favorite Baseball team? I like my Az team lol\nGerman Formal Hypothesis: Wow, das ist toll! Wer ist Ihr Lieblings- Baseballteam? Ich mag meine\nAz-Team lol.\nGerman Formal Reference: Wow, das ist fantastisch! Welches ist Ihr Lieblingsbaseballteam? Ich\nstehe auf mein AZ-Team lol.\nGerman Informal Hypothesis: Wow, das ist toll! Wer ist dein Lieblings野球team? Ich mag meine\nAz Team lol.\nGerman Informal Reference: Wow, das ist fantastisch! Welches ist dein Lieblingsbaseballteam? Ich\nstehe auf mein AZ-Team lol.\nTable 13: Contrastive outputs from English-German. Note that there is not only variety in lexical choice between\nreferences and hypotheses, but also between hypotheses of varying formality (i.e., 野球is “baseball” in Japanese)\nE Accuracy of Formality Classifiers\nWe report the accuracy of the learned classifiers on the TASK TRAIN dataset in Table 14.\nLANGUAGE Accuracy\nFormal Informal\nen-de 98% 99%\nen-es 99% 92%\nen-ja 98% 98%\nen-hi 96% 95%\nTable 14: Accuracy of trained formality classifiers on the TASK DEV dataset.\n339\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\n(g)\n (h)\nFigure 2: Class Distribution for the baseline, mBART-large and mT5-large systems for all the supervised language\npairs.\n340"
}