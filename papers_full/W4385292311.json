{
  "title": "CT-Net: Arbitrary-Shaped Text Detection via Contour Transformer",
  "url": "https://openalex.org/W4385292311",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2744950401",
      "name": "Shao, Zhiwen",
      "affiliations": [
        "China University of Mining and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2743938163",
      "name": "Su Yuchen",
      "affiliations": [
        "China University of Mining and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109551865",
      "name": "Zhou Yong",
      "affiliations": [
        "China University of Mining and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A135992405",
      "name": "Meng Fanrong",
      "affiliations": [
        "China University of Mining and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2107031090",
      "name": "Zhu HanCheng",
      "affiliations": [
        "China University of Mining and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1990217128",
      "name": "Liu Bing",
      "affiliations": [
        "China University of Mining and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2105411380",
      "name": "Yao Rui",
      "affiliations": [
        "China University of Mining and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2396529792",
    "https://openalex.org/W2013360608",
    "https://openalex.org/W2969541062",
    "https://openalex.org/W4285216227",
    "https://openalex.org/W4309368547",
    "https://openalex.org/W2980358704",
    "https://openalex.org/W2991626090",
    "https://openalex.org/W2953894958",
    "https://openalex.org/W3035679705",
    "https://openalex.org/W3176754135",
    "https://openalex.org/W3034792612",
    "https://openalex.org/W3181016597",
    "https://openalex.org/W3172799005",
    "https://openalex.org/W3184364189",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3035709993",
    "https://openalex.org/W6726873649",
    "https://openalex.org/W4323897056",
    "https://openalex.org/W6849846199",
    "https://openalex.org/W2914492226",
    "https://openalex.org/W2963299604",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2810028092",
    "https://openalex.org/W2963647456",
    "https://openalex.org/W2902494497",
    "https://openalex.org/W2998621280",
    "https://openalex.org/W2593539516",
    "https://openalex.org/W6620707391",
    "https://openalex.org/W2550687635",
    "https://openalex.org/W2193145675",
    "https://openalex.org/W2784050770",
    "https://openalex.org/W2605982830",
    "https://openalex.org/W3186906052",
    "https://openalex.org/W2963353821",
    "https://openalex.org/W4312593844",
    "https://openalex.org/W2519818067",
    "https://openalex.org/W2967615747",
    "https://openalex.org/W3034514377",
    "https://openalex.org/W3093046205",
    "https://openalex.org/W6796874202",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W6801229998",
    "https://openalex.org/W6779586474",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W2144554289",
    "https://openalex.org/W1988461287",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W2343052201",
    "https://openalex.org/W2785383245",
    "https://openalex.org/W3003921261",
    "https://openalex.org/W6691603626",
    "https://openalex.org/W3003868038",
    "https://openalex.org/W2953606406",
    "https://openalex.org/W2964685115",
    "https://openalex.org/W2968226676",
    "https://openalex.org/W2991609675",
    "https://openalex.org/W3088488048",
    "https://openalex.org/W3012138014",
    "https://openalex.org/W3138649552",
    "https://openalex.org/W4212820886",
    "https://openalex.org/W4283692648",
    "https://openalex.org/W4292264029",
    "https://openalex.org/W4214922754",
    "https://openalex.org/W3027134841",
    "https://openalex.org/W3211070770",
    "https://openalex.org/W4288391612",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W3106228955",
    "https://openalex.org/W3102695566",
    "https://openalex.org/W3200068692",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W4295246343",
    "https://openalex.org/W4319452844",
    "https://openalex.org/W4291675528",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W3179092682",
    "https://openalex.org/W3102710196",
    "https://openalex.org/W639708223",
    "https://openalex.org/W4306987814"
  ],
  "abstract": "Contour based scene text detection methods have rapidly developed recently, but still suffer from inaccurate frontend contour initialization, multi-stage error accumulation, or deficient local information aggregation. To tackle these limitations, we propose a novel arbitrary-shaped scene text detection framework named CT-Net by progressive contour regression with contour transformers. Specifically, we first employ a contour initialization module that generates coarse text contours without any post-processing. Then, we adopt contour refinement modules to adaptively refine text contours in an iterative manner, which are beneficial for context information capturing and progressive global contour deformation. Besides, we propose an adaptive training strategy to enable the contour transformers to learn more potential deformation paths, and introduce a re-score mechanism that can effectively suppress false positives. Extensive experiments are conducted on four challenging datasets, which demonstrate the accuracy and efficiency of our CT-Net over state-of-the-art methods. Particularly, CT-Net achieves F-measure of 86.1 at 11.2 frames per second (FPS) and F-measure of 87.8 at 10.1 FPS for CTW1500 and Total-Text datasets, respectively.",
  "full_text": "IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. X, X 1\nCT-Net: Arbitrary-Shaped Text Detection via\nContour Transformer\nZhiwen Shao, Yuchen Su, Yong Zhou, Fanrong Meng, Hancheng Zhu, Bing Liu, and Rui Yao\nAbstract—Contour based scene text detection methods have\nrapidly developed recently, but still suffer from inaccurate front-\nend contour initialization, multi-stage error accumulation, or\ndeficient local information aggregation. To tackle these limita-\ntions, we propose a novel arbitrary-shaped scene text detection\nframework named CT-Net by progressive contour regression with\ncontour transformers. Specifically, we first employ a contour\ninitialization module that generates coarse text contours without\nany post-processing. Then, we adopt contour refinement modules\nto adaptively refine text contours in an iterative manner, which\nare beneficial for context information capturing and progressive\nglobal contour deformation. Besides, we propose an adaptive\ntraining strategy to enable the contour transformers to learn\nmore potential deformation paths, and introduce a re-score\nmechanism that can effectively suppress false positives. Extensive\nexperiments are conducted on four challenging datasets, which\ndemonstrate the accuracy and efficiency of our CT-Net over state-\nof-the-art methods. Particularly, CT-Net achieves F-measure of\n86.1 at 11.2 frames per second (FPS) and F-measure of 87.8 at\n10.1 FPS for CTW1500 and Total-Text datasets, respectively.\nIndex Terms—Scene text detection, contour transformer, con-\ntour initialization, adaptive training strategy, re-score mecha-\nnism.\nI. I NTRODUCTION\nO\nVER the past few years, scene text detection has at-\ntracted ever-increasing interests in the computer vision\ncommunity, due to its ubiquitous applications in many fields,\nsuch as product search [1], scene understanding [2], and\nautonomous driving [3]. Benefited from the rapid development\nof deep learning [4], [5], scene text detection methods have\nManuscript received November, 2022. This work was supported in\npart by the National Natural Science Foundation of China under Grant\n62106268, in part by the China Postdoctoral Science Foundation under Grant\n2023M732223, in part by the High-Level Talent Program for Innovation\nand Entrepreneurship (ShuangChuang Doctor) of Jiangsu Province under\nGrant JSSCBS20211220, and in part by the Talent Program for Deputy\nGeneral Manager of Science and Technology of Jiangsu Province under\nGrant FZ20220440. It was also supported in part by the National Natural\nScience Foundation of China under Grants 62272461, 62101555, 62276266,\nand 62172417, in part by the Natural Science Foundation of Jiangsu Province\nunder Grants BK20201346 and BK20210488, and in part by the Shanghai\nSailing Program under Grant 23YF1410500. (Zhiwen Shao and Yuchen Su\ncontributed equally to this work. Corresponding author: Yong Zhou.)\nZ. Shao is with the School of Computer Science and Technology, China\nUniversity of Mining and Technology, Xuzhou 221116, China, also with the\nEngineering Research Center of Mine Digitization, Ministry of Education\nof the People’s Republic of China, Xuzhou 221116, China, and also with\nthe Department of Computer Science and Engineering, Shanghai Jiao Tong\nUniversity, Shanghai 200240, China (e-mail: zhiwen shao@cumt.edu.cn).\nY . Su, Y . Zhou, F. Meng, H. Zhu, B. Liu, and R. Yao are with the\nSchool of Computer Science and Technology, China University of Mining\nand Technology, Xuzhou 221116, China, and also with the Engineering\nResearch Center of Mine Digitization, Ministry of Education of the People’s\nRepublic of China, Xuzhou 221116, China (e-mail: {yuchen su; yzhou;\nmengfr; zhuhancheng; liubing; ruiyao }@cumt.edu.cn).\n(a) Initial contour (b) Feature learing on the contour (c) Offsets\nFig. 1. The overall idea of our CT-Net. (a) Generation of initial contour\nwith our contour initialization module; (b) Vertex-wise feature learning, in\nwhich the green and the red nodes denote the input features and the output\nresults, respectively, and the purple cylinder denotes our contour transformer;\n(c) Offsets are regressed at each vertex to deform the contour to the text\nboundary.\nachieved remarkable progress [6]–[10]. However, due to the\ncomplex backgrounds and large variance of scene texts in\ncolor, size, texture, etc., arbitrary-shaped scene text detection\nremains a challenge.\nTo explore an arbitrary-shaped scene text detector, seg-\nmentation based and connected component based approaches\nrepresent text instances with pixel-level classification masks\nor sequential components from a local modeling perspective.\nAlthough the shapes of the scene texts are easy to model,they\nsuffer from the lack of noise resistance and complicated\nheuristic post-processing. More importantly, they commonly\nlack attention to the global geometric distribution that reflects\nthe overall layouts of the text contours.\nTo obtain the global geometric distribution, some single-\nstage contour based methods are proposed to model the text\ncontours directly. For example, ABCNet [11] and FCENet\n[12] represent text contours with Bessel points and Fourier\ncoefficients, respectively. However, these methods regress the\ntext contours based on the limited features at the positive\npoint and perceive the texts with complex geometric layouts\nonly in single stage, resulting in the loss of the predicted\ncontour details. Therefore, some multi-stage contour based\nmethods are proposed to progressively locate the texts, which\nfirst generate the initial contours by a detector and then\nprogressively refine the initial contours by multiple contour\nrefinement modules. Although the existing multi-stage contour\nbased methods have achieved excellent detection performance,\nthey still have three main problems to be resolved.\nFirst, inaccurate front-end contour initialization restricts the\nperformance. For example, PCR [13] regresses directly from\noriented text proposals to arbitrary-shaped contours. This large\ngap between the oriented text proposals and the ground-truth\ntext contours may lead to many unreasonable deformation\narXiv:2307.13310v1  [cs.CV]  25 Jul 2023\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. X, X 2\npaths and huge training difficulty, especially for highly-curved\ntexts. TextBPN [14] uses a segmentation based approach to\ngenerate initial contours. However, this bottom-up approach\ncannot take advantage of the prior knowledge that there are\nno holes inside the texts, maybe resulting in a text instance\nsplit into multiple ones, especially for large character spacing\ntexts.\nSecond, error accumulation exists between different stages,\ne.g., imprecise detection results in the previous stage may\nhinder the estimation in the current stage. Two reasons mainly\ncause this. On the one hand, the contour refinement module\ncannot effectively evaluate the confidence of the contours to\nsolve the false positive problem. Although PCR [13] pro-\nposes a reliable contour localization mechanism (RCLM) to\nevaluate the confidence of the contours, its negative sample\nmining technique does not generate text-like negative samples.\nOn the other hand, the contour refinement module learns\nonly the fixed contour deformation path between the ground-\ntruth initial contour and the ground-truth final contour during\ntraining, which makes it difficult to obtain its refinement\ndirection when the predicted contour deviates from the ground-\ntruth during inference. Besides, the tasks of initial contour\nprediction and progressive contour refinement are independent\nof each other, which may not maximize the final performance\nof text detection.\nThird, local information aggregation mechanism ignores the\naccess of global information. Such local information aggrega-\ntion often uses circular convolution [17] or graph convolution\n[18] to propagate the features of adjacent contour vertexes.\nHowever, when the prediction of individual contour vertex\nhas a large deviation, it is difficult to accurately perceive the\nrefinement direction only through the information of adjacent\ncontour vertexes. Moreover, as the number of contour vertexes\nincreases, the global information becomes more difficult to\naccess, leading to a decrease in detection performance, while\na small number of contour vertexes have limited capability to\ncompletely fit arbitrary-shaped text boundaries.\nIn this paper, we propose a novel contour transformer based\nscene text detection framework called CT-Net, which solves\nthe above problems through three components: 1) a contour\ninitialization module; 2) an adaptive training strategy with a re-\nscore mechanism; and 3) transformer-based global contour re-\nfinement modules. In particular, the contour initialization mod-\nule produces bounding polygons as the initial contours based\non features of positive points, which can roughly represent the\ntext contours, as shown in Fig. 1(a). It introduces negligible\ncomputation overhead compared with typical bounding box\ndetection. Then, the coarse initial contour is deformed by the\ncontour refinement module, as shown in Fig. 1(b) and (c).\nInspired by the vision transformers (ViTs) [19]–[21], the\ncontour refinement module aggregates the global information\nby a multi-layer perceptron (MLP) to achieve the prediction\nof larger offsets, and excavates contextual information by a\nself-attention mechanism to learn the overall layout of the\ncontours. The deformed vertexes are able to enclose closed\ncurves in order, which significantly reduces the difficulty\nof contour deformation. Meanwhile, to allow the contour\nrefinement module to learn more potential deformation paths,\nwe propose an adaptive training strategy to enable the contour\nrefinement module can effectively refine the predicted initial\ncontours during training. Considering errors may be accumu-\nlated by initial contours of false detection, we propose a re-\nscore mechanism to further reduce the error accumulation by\nevaluating the confidence of the predicted contour based on\nthe contour vertex features. The joint optimization of initial\ncontour prediction and progressive contour deformation is\nachieved by the adaptive training strategy and the re-score\nmechanism, in which the process of contour deformation is\nforced to be robust to contour initialization.\nThe main contributions of this paper are summarized as\nfollows:\n• We propose a novel multi-stage contour based framework\nfor arbitrary-shaped scene text detection, which can ac-\ncurately and efficiently localize text contours without any\npost-processing.\n• We propose a novel initial contour representation method\nwith polygons to decrease the training complexity, as well\nas novel contour refinement modules to adaptively refine\ntext contours in an iterative manner.\n• We design an adaptive training strategy with a re-score\nmechanism to jointly optimize initial contour prediction\nand progressive contour refinement, which is beneficial\nfor reducing error accumulation across stages.\n• Extensive experiments show that our CT-Net achieves\nstate-of-the-art performance. Specifically, CT-Net obtains\n86.1 F-measure at 11.2 frames per second (FPS) and 87.8\nF-measure at 10.1 FPS on CTW1500 [22] and Total-\nText [23], respectively.\nII. R ELATED WORK\nWith the booming development of deep learning techniques,\nnumerous inspiring ideas and effective methods for scene text\ndetection tasks have been investigated. In this section, we will\nreview the related methods on this topic.\nA. Segmentation Based Methods\nInspired by semantic segmentation methods [24], [25], some\nworks regard the text detection as a segmentation problem,\nand then separate different text instances through heuristic\npost-processing. TextSnake [26] regarded text instances as a\nseries of overlapping disks, and predicted some geometric\nattributes of the disks to reconstruct the text instances. PSENet\n[27] predicted different scale kernels of text instances, then\nadopted a progressive scale expansion strategy to gradually\nexpand the predefined text kernels. TextField [28] learnt a\ndeep direction field that encodes both text mask and direction\ninformation of each point, then linked neighbor pixels to\ngenerate candidate text instances. SAE [29] treated each text\ninstance as a cluster and adopted a two-step clustering strategy\nto distinguish adjacent text instances from the predicted center\nand full segmentation maps. Moreover, DBNet [30] proposed\na differentiable binarization module, which gives a higher\nthreshold to the text boundaries to distinguish adjacent text\ninstances. However, these segmentation based methods are\nsensitive to the text-like background noises and lack global\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. X, X 3\nContour \nInitialization \nInitial \nContour\nDeformed \nContour\nDeformed \nContour\nU U\nS\nS\nOffsets\nOffsets\nContour\nTransformer \nContour\nTransformer \nS\nU\nVertex Sampling\nVertex Update\nConcatenation\nBackbone+FPN\n. . .\nVertex Features \nFe\n. . .\nFe\nFeature Flow \nVertex Position  Flow \nVertex Offset  Flow \nFig. 2. The architecture of our CN-Net, which is composed of three modules: feature extraction by the ResNet50 [15] and FPN [16], contour initialization\nmodule for initial contour generation, and contour refinement modules for progressive contour deformation. Two stages of contour refinement are illustrated.\nIn each stage, the feature of each vertex token is sampled from Fe according to its spatial position specified by the contour.\nperception of the texts, resulting in many noises and defects\nin the predicted contours.\nB. Regression Based Methods\nRegression based methods rely on the object detection\nframework with box regression, which are easier to train than\nsegmentation based methods. RRPN [31] used a modified\npipeline of Faster R-CNN [32], which adopted rotated bound-\ning boxes to represent multi-oriented texts. TextBoxes [33]\nmodified the shapes of convolutional kernels and increased\nthe scale of the default boxes of SSD [34] to fit the aspect\nratio of texts. TextBoxes++ [35] extended TextBoxes by using\nquadrilateral regression to detect multi-oriented texts. EAST\n[36] and MOST [37] adopted a single-stage anchor-free frame-\nwork to directly regress the offsets between points within text\ninstances and the corresponding boundaries or vertexes. To\neffectively detect curved texts, LOMO [38] introduced a shape\nrepresentation module that uses text regions, center lines and\nboundary offsets to represent text instances, and proposed an\niterative refinement module to detect long texts. Tang et al.\n[39] first obtained representative text region features by feature\nsampling and then used the feature aggregation capability\nof the self-attention mechanism to locate texts accurately.\nHowever, these methods either suffer from error accumulation\nin multi-stage or cannot sufficiently adapt to arbitrary-shaped\ntext detection.\nC. Connected Component Based Methods\nConnected component based methods typically first detect\nindividual text parts, then generate final detection through a\ngroup post-processing procedure. CTPN [40] first extracted a\nseries of text components with a fixed width, then connected\nthem into horizontal text lines. CRAFT [41] adopted two-\ndimensional Gaussian segmentation labels to detect character-\nlevel text regions, then generated final detection based on the\naffinity between characters. DRRG [42] first predicted the\ngeometry attributes of text components via a text proposal\nnetwork, then used a relational reasoning graph network to\ninfer the linkages between the text components. Although\nthese methods allow flexible representation of arbitrary-shaped\ntexts, the complex post-processing of text component cluster-\ning seriously affects the detection efficiency.\nD. Contour Based Methods\nContour based methods can directly obtain the text contours\nwithout complicated post-processing. TextRay [43] formulated\nthe text contours in the polar system, and adopted the Cheby-\nshev polynomials to approximate the text contours. ABCNet\n[11] proposed a single-shot anchor-free framework to predict\na series of Bezier points, then adopted Bernstein polynomial\nto convert these points into Bezier curves that approximate\nthe text contours. FCENet [12] modeled text instances in the\nFourier domain, and adopted discrete Fourier transform to\nfit the text contours. To obtain more accurate text contours,\nTextBPN [14] first obtained boundary proposals based on a\nbottom-up approach, then gradually deformed the boundary\nproposals into text contours through multiple adaptive bound-\nary deformation modules. PCR [13] proposed a progressive\ncontour regression approaches to detect arbitrary-shaped scene\ntexts from a top-down perspective. However, these single-\nstage methods [11], [12], [43] have limited ability to model\nextremely long texts or highly curved texts, while these multi-\nstage methods [13], [14] suffer from the error accumulation.\nIII. A RBITRARY -SHAPED TEXT DETECTION VIA\nCONTOUR TRANSFORMER\nA. Overview\nOur CT-Net is a multi-stage contour based detection frame-\nwork that can fit arbitrary-shaped texts. As illustrated in Fig. 2,\nthis framework mainly contains three parts: feature extraction\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. X, X 4\n(a) Ground-truth (b) Vertex Selection (c) Initial contour\nFig. 3. The generation of initial contours. (a) The ground-truth polygon\nboundary. (b) The sampling on boundary. (c) The ground-truth initial contour\ncomposed of sampling vertexes.\nmodule, contour initialization module, and multiple contour\nrefinement modules. In the feature extraction module, we\nutilize ResNet50 [15] as the backbone and adopt FPN [16]\nto generate multi-scale features, and then fuse these multi-\nscale features by concatenation to obtain a contextual feature\nFe. In the contour initialization module, we adopt a single-shot\nanchor-free framework to predict the initial contour. As shown\nin Fig. 2, we adopt multiple contour refinement modules to\niteratively refine the predicted contours in each contour refine-\nment module, we first utilize multiple transformer encoders to\naggregate the global information of the text contours and then\npredict the offset and confidence of the text contours by an\nMLP, respectively.\nB. Contour Initialization\nTypical single-stage contour based methods like ABCNet\n[11] and FCENet [12] use the Bezier point and Fourier co-\nefficient to represent text contours. However, their parameters\nfluctuate greatly, which makes training difficult. Instead, we\npropose a contour representation method based on bounding\npolygon vertexes, which have a good trade-off between train-\ning complexity and contour representation quality.\nSpecifically, as shown in Fig. 3(a), the ground-truth\npolygon boundary of a scene text contains N points\n{(¯x1, ¯y1), (¯x2, ¯y2), ··· , (¯xN , ¯yN )} with vertexes as well as\npoints in each edge. Firstly, we uniformly sample Na vertexes\n{(ˆx1, ˆy1), (ˆx2, ˆy2), ··· , (ˆxNa , ˆyNa )} from the four edges of\nthe ground-truth bounding box in clockwise order, in which\nthe bounding box can be determined from the ground-truth\npolygon. Then, we sample (xi, yi) by choosing the point in the\nground-truth polygon with the nearest L1 distance to (ˆxi, ˆyi):\n(xi, yi) = arg min\n(¯xj,¯yj)\nNX\nj=1\n(|¯xj − ˆxi| + |¯yj − ˆyi|), (1)\nwhere i = 1, 2, ··· , Na, and Na is usually much smaller than\nN.\nSince (ˆxi, ˆyi) can be directly determined from the bounding\nbox, we propose to regress the position of the bounding box\nas well as the horizontal-vertical offset between each pair of\n(xi, yi) and (ˆxi, ˆyi). In this way, our method is easier to train,\nas the bounding box is less complex than the polygon and the\nfluctuations of the minimum distances between sampled points\nare small.\nIn our contour initialization module, we first use three\nconvolutions to enhance features, then use one convolution\nto predict the probability of each position corresponding to\nVer-Tok\n#1\nVer-Tok\n#2\nVer-Tok\n#Na. . .\nTransformer Encoder\nCls-Tok\n#1\nVer-Tok\n#1\nVer-Tok\n#2\nVer-Tok\n#Na. . . Cls-Tok\n#1\nReg Head\nRe-Score\n Norm\nSelf-Attention\n+\n Norm\n+\nMLP\nL×\nCls \nHead\nOffsets\nFig. 4. The detailed structure of a contour refinement module. “Ver-\nTok” refers to [PATCH] token, which is the feature of the contour vertex\ncorresponding to Fe. “Cls-ToK” refers to [CLS] token, which is a learnable\nembedding for contour confidence.\nthe positive sample, the regression value of each position\ncorresponding to the text bounding box, and the vertex offsets,\nrespectively. To achieve end-to-end prediction without non-\nmaximal suppression (NMS), we select only one positive\nsample for each text instance during training. Inspired by\n[44], our positive sample allocation strategy is as follows: in\neach training iteration, we select only one positive sample that\ncan minimize the training loss for each text instance, and the\ntraining loss is defined as the sum of classification loss and\nregression loss.\nDuring inference, there are three steps for the prediction of\ninitial contour. Firstly, we obtain the text box based on the\nclassification threshold τa and the regression value of the box.\nSecondly, we sample the predicted text box according to the\nsampling strategy during training. Thirdly, the initial contour\nvertexes are obtained based on the position of the sampled\npoints and their corresponding offsets.\nC. Contour Transformer\nIn this module, we first initialize the contour with Na ver-\ntexes, and then employ multiple contour refinement modules\nto progressively refine the initial contours, as shown in Fig. 2.\nFig. 4 elaborates the architecture of a contour refine-\nment module. The main difference between ViT [19] and\nour contour transformer is the prediction head. Specif-\nically, the input of our contour transformer is z0 =\u0002\nx1\nver; x2\nver; ··· ; xNa\nver; xcls\n\u0003\n. Here, xi\nver ∈ R1×C denotes the\nfeature of the i-th contour vertex token, which is sampled from\nFe according to its spatial position, as shown in Fig. 2. C is the\nnumber of input channels. Considering that the contours may\nbe evolved from some false positives, we employ a learnable\nclassification token xcls ∈ R1×C to learn the confidence of\nthe detected contours (see more details in Sec. III-E).\nThe body of our contour transformer is composed of L\ntransformer encoder layers for global information interaction\nbetween contour vertexes. Each transformer encoder layer\nconsists of a multi-head self-attention (MSA) block and an\nMLP block. LayerNorm (LN) [45] is applied before each block\nand residual connection is applied after each block. The MLP\nblock is made of two layers with a non-linearity activation\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. X, X 5\nfunction GELU [46]. The ℓ-th transformer encoder layer can\nbe formulated as\nz′\nℓ = MSA (LN (zℓ−1)) + zℓ−1,\nzℓ = MLP (LN (z′\nℓ)) + z′\nℓ. (2)\nThe decoder of our contour transformer consists of a re-\ngression (Reg) head and a classification (Cls) head, both of\nwhich are implemented by an MLP with separate parameters.\nBoth heads have two hidden layers and the former two fully\nconnected layers are equipped with the ReLU-BN-Dropout\noperation, where the dropout probability is 0.1 and the chan-\nnels of hidden layers are C. The only difference between the\ntwo heads is the channels of the output layer ( Na × 2 for\nthe regression head and 1 for the classification head). For\nregression head, the output is the offset of the contour vertexes\n(a vector of length Na × 2). The offsets and the positions of\ninput features are summed to obtain the deformed contour.\nD. Adaptive Training Strategy\nThe contour refinement module of current multi-stage con-\ntour based approaches [14], [47] learns only the deformation\nfrom the ground-truth initial contour to the ground-truth final\ncontour during training, which may lead to some errors when\nthe predictions of the initial contour are not accurate during\ninference. To alleviate such issues, we propose an adaptive\ntraining strategy for learning more potential deformation paths.\nSpecifically, we make our contour transformer learn not\nonly the offset between the ground-truth initial contour and\nthe ground-truth final contour, but also the offset between the\npredicted initial contour and the ground-truth final contour.\nFormally, the predicted text bounding boxes Bf and the cor-\nresponding predicted contours Cf in the contour initialization\nmodule are obtained in the same way as the inference stage\n(see Sec. III-B). First, the ground-truth index of the i-th\npredicted contour Cf\ni is obtained by:\nindi = arg max\nk∈[1···,Nb]\nIOU\n\u0010\nBf\ni , Bgt\nk\n\u0011\n, (3)\nwhere Bgt denotes totally Nb ground-truth bounding boxes,\nand IOU denotes intersection over union. Then, Cf is used\nfor training contour deformation. Note that there may be\nsome false positives in Cf , as shown in the right side of\nFig. 5. We discard these false positives for the training of\nthe regression head in the contour transformer, since they do\nnot have regression targets. Cf\ni is simply regarded as a false\npositive when:\nioui = max\nk∈[1···,Nb]\nIOU\n\u0010\nBf\ni , Bgt\nk\n\u0011\n< 0.5. (4)\nBesides the contour initialization module, this adaptive train-\ning strategy is also used in each contour refinement module.\nE. Re-Score Mechanism\nTo effectively evaluate the confidences of the detected\ncontours, we exploit a re-score mechanism in parallel with the\ncontour deformation. Specifically, we first add a learnable xcls\ntoken to the input of the contour refinement module to learn\ncls score:    0.72\nre-score:    0.95\ncls score:    0.96\nre-score:    0.15\nFig. 5. The visualization of detection results with the classification score and\nthe re-score.\nthe category information of contour vertexes, then extract the\ncategory information of the xcls token through a classification\nhead to generate contour scores for text/non-text.\nTo learn a robust contour scoring network, it requires\nsamples with different labels to train this network for dis-\ntinguishing the contours of scene texts from those of back-\ngrounds. In particular, the input of the contour refinement\nmodule comes from two parts, 1) the features corresponding\nto the ground-truth contour and 2) the features obtained by\nthe adaptive training strategy. For the classification head of\nthe contour refinement module, we define the classification\nlabels of the former as 1, while the latter has many false\npositives, in which we adopt ioui in Eq. (4) to represent\nthe confidence of these samples as text contours and regard\nthem as classification labels. With the continuous optimization\nof the model in training, false positives will be significantly\nreduced. To improve the model’s ability of distinguishing false\npositives, we adopt quality focal loss (QFL) [48] to increase\nthe loss weights of the false positives. As shown in Fig. 5,\nre-score mechanism can significantly reduce the classification\nscore of the false positives.\nMoreover, since our re-score mechanism can accurately\nevaluate the contour confidence, we propose an early-stop\nmechanism that allows our model to stop itself when it has\nenough confidence score. Specifically, we only send contours\nwith a confidence score below the classification threshold τb to\nthe next contour refinement module for contour deformation.\nF . Full Loss Function\nIn our CT-Net framework, the full loss function is formu-\nlated as\nL = Linit + Ltransform , (5)\nwhere Linit and Ltransform are the losses for the initial con-\ntour module and the contour refinement module, respectively.\nIn Eq. (5), Linit is computed as\nLinit = Lcls + λ1Lbox + λ2Loff 1, (6)\nwhere Lcls, Lbox, and Loff 1 denote classification loss, bound-\ning box regression loss, and offset regression loss, respec-\ntively. We choose cross-entropy loss to optimize Lcls, choose\nsmooth-L1 loss [32] to optimize Loff 1, and use GIoU loss [49]\nfor Lbox following ABCNet [11]. Ltransform is computed as\nLtransform = λ3Loff 2 + λ4Lrescore, (7)\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. X, X 6\nwhere Loff 2 and Lrescore denote contour deformation loss\nand re-score loss, respectively. We choose QFL [48] to opti-\nmize Lrescore, and define the contour deformation loss as\nLoff 2 = min\nu∈[1···,Na]\nNaX\ni=1\nsmooth-L1\n\u0010\npi, p′\n(u+i)%Na\n\u0011\n, (8)\nwhere p and p′ denote the prediction and ground-truth con-\ntour vertex sets, respectively, and “ %” denotes the remainder\noperation.\nIV. E XPERIMENTS\nA. Datasets and Settings\n1) Datasets: We evaluate our CT-Net on four scene\ntext detection benchmarks: CTW1500 [22], Total-Text [23],\nICADAR2015 [50], and MSRA-TD500 [51].\n• CTW1500 is a challenging dataset that contains many\nlong curved texts. It consists of 1, 000 training images\nand 500 test images, in which each text is annotated at\ntext-line level with 14 vertexes.\n• Total-Text includes 1, 255 images for training and 300\nimages for testing. Texts in this dataset have various\nshapes, including horizontal, oriented, and curved shapes.\nAll text instances are annotated with adaptive number of\ncontour vertexes.\n• ICDAR2015 is presented in the ICDAR 2015 Robust\nReading Competition with 1, 000 images for training\nand 500 images for testing. Texts in this dataset are all\nannotated with four vertexes of quadrangle.\n• MSRA-TD500 is a scene text dataset for detecting multi-\norientated long texts. It consists of 300 images for train-\ning and 200 images for testing, including English and\nChinese texts. Besides, the annotations of these datasets\nare all in line-level.\n2) Implementation Details: Our CT-Net is implemented via\nPyTorch [52]. The backbone network ResNet50 [15] is pre-\ntrained on ImageNet [53]. The training of CT-Net can be\ndivided into two steps. First, we pre-train our network on a\nlarge synthetic dataset SynthText [54] with totally 800, 000\nimages by four epochs. Then, we fine-tune our model on\nspecific datasets with a mini-batch size of 4. The model is\ntrained up for 480 epochs with the initial learning rate of\n1×10−4, in which the learning rate is decreased to 1×10−5 at\nthe 360-th epoch. Following other methods [7], [11], [30], the\ntext regions labeled as “DO NOT CARE” are ignored during\ntraining. Moreover, we set Na to 32, L to 4, τa to 0.45, and τb\nto 0.5. In Eq. (6), λ1 = 1, and λ2 = 1. In Eq. (7), λ3 = 0.1,\nand λ4 = 2.\nFor data augmentation, we randomly perform horizontal\nflip, rotation, scale, crop, color jitter, and contrast jitter for\ntraining images. In random scale, the short side of input images\nis uniquely chosen from 640 to 832 with an interval of 32, and\nthe long side is less than 1, 600 for CTW1500, Total-Text,\nand MSRA-TD500. For ICDAR2015, the short side of input\nimages is uniquely chosen from 980 to 1432 with an interval\nof 32, and the long side is less than 2, 800. In random crop,\nwe only crop the non-text regions, in which the crop area is\nTABLE I\nRESULTS FOR DIFFERENT VARIANTS OF OUR CT-NET ON CTW1500 [22]\nWITHOUT PRE -TRAINING , IN WHICH CR AND AT DENOTE THE CONTOUR\nREFINEMENT MODULE AND THE ADAPTIVE TRAINING STRATEGY ,\nRESPECTIVELY .\nMethod Components R P FCR AT re-score\nBNet - - - 81.9 82 .8 82 .3\nBCNet ✓ - - 83.1 84 .9 84 .0\nBCANet ✓ ✓ - 83.7 85.2 84 .5\nBCRNet ✓ - ✓ 81.9 87 .7 84 .7\nCT-Net ✓ ✓ ✓ 82.7 87.9 85 .2\nTABLE II\nCOMPLEXITIES AND RESULTS FOR DIFFERENT VARIANTS OF CONTOUR\nREFINEMENT MODULE ON CTW1500 [22] AND TOTAL-TEXT [23].\nMethod Params\n(×106)\nFLOPs\n(×109)\nFPS R P F\nCTW1500\nCCN [17] 2.85 1 .54 10 .6 82.4 86 .9 84 .6\nGCN [18] 2.71 1 .38 10 .9 82.5 87 .0 84 .7\nCT 2.55 1 .28 11 .3 82.7 87 .9 85 .2\nTotal-Text\nCCN [17] 2.85 1 .54 9 .3 82.6 88 .8 85 .6\nGCN [18] 2.71 1 .38 9 .6 83.1 88 .7 85 .8\nCT 2.55 1 .28 9 .8 83.6 89 .2 86 .3\nlarger than half of the original image area. All experiments\nare carried out on an NVIDIA Tesla V100 GPU. The average\nrunning speeds (FPS) of different methods during inference\nare all computed by feeding a single image into the model at\na time. We employ three popular evaluation metrics recall (R),\nprecision (P), and F-measure (F), in which % in their results\nis omitted for simplicity.\nB. Ablation Study\nIn this section, we conduct ablation study on the CTW1500\ndataset. We introduce a BNet as the baseline whose output\ncomes only from the contour initialization module. BNet is\nnot pre-trained on SynthText [54] for simplicity.\n1) Contour Transformer: As shown in Table I, when BC-\nNet integrates the contour refinement module over BNet, it\nimproves 1.7 in F-measure. The improvement is due to more\naccurate localization by multiple perception on scene texts\nwith complex geometric layouts. To further verify the effec-\ntiveness of our contour transformer, we compare it with the\nprevious deformation methods, circular convolution network\n(CCN) [17] and graph convolution network (GCN) [18]. For\na fair comparison, we only replace the body of our contour\ntransformer (CT) with CCN or GCN in the contour refinement\nmodule.\nTABLE III\nPERFORMANCE OF CT-NET WITH DIFFERENT STAGES OF CONTOUR\nTRANSFORMERS ON CTW1500 [22].\nStage 1 2 3\nF 84.8 85.2 85.2\nFPS 13.3 12.2 11.2\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. X, X 7\nStage 1 Stage 2\nFig. 6. Visual results of different stages. The green contours are detection\nboundaries.\nTABLE IV\nRESULTS OF DIFFERENT VERTEX NUMBERS IN CONTOUR TRANSFORMERS\nON CTW1500 [22].\nNumber R P F\n16 81.9 87.4 84.6\n24 81.6 88.5 84.9\n32 82.7 87.9 85.2\n40 82.6 88.0 85.2\n48 82.1 88.3 85.1\nAs can be seen in Table II, our contour transformer achieves\nthe best performance compared with the other two methods on\nboth CTW1500 and Total-Text. It achieves improvements on\nCTW1500 by 0.6 F-measure over CCN and 0.5 F-measure\nover GCN, respectively. This is because our contour refine-\nment module can obtain the global contour information more\neffectively. Besides, we compare the number of parameters\n(Params), floating point operations (FLOPs), and FPS of these\nthree structures. Compared to CCN and GCN, our CT has less\nparameters and computational complexity, and thus has faster\nrunning speed (0.7 FPS over CCN and 0.4 FPS over GCN on\nCTW1500).\n2) Adaptive Training Strategy: Here we investigate the\neffectiveness of adaptive training strategy. The results on\nCTW1500 dataset are shown in Table I. The F-measure is\nincreased by 0.5 after employing the adaptive training strategy.\nThis gain can be ascribed that the adaptive training strategy\nenables the contour transformer to learn more potential defor-\nmation paths.\n3) Re-score Mechanism: Table I shows the results of us-\ning the re-score mechanism over BCNet and BCANet on\nCTW1500 dataset. We can see that BCRNet and CT-Net\nboth achieve the gain of 0.7 F-measure after using the re-\nscore mechanism. This proves that the re-score mechanism\ncan effectively evaluate the confidence of the contours.\nTABLE V\nRESULTS FOR OUR CT-NET WITH DIFFERENT TRADE -OFF PARAMETERS\nON CTW1500 [22].\nλ1 λ2 λ3 λ4 R P F\n1 3 0.1 2 82.5 87.6 85.0\n3 1 0.1 2 82.3 87.9 85.1\n0.3 0.3 0.1 2 82.5 87.4 84.9\n1 1 0.3 2 81.9 87.3 84.5\n1 1 0.03 2 82.0 86.4 84.2\n1 1 0.1 6 82.2 87.4 84.7\n1 1 0.1 0.5 82.8 86.9 84.8\n1 1 0.1 2 82.7 87.9 85.2\n4) Stage number: To explore the influence of stage number,\nwe compare our CT-Net with different stages. As shown in\nTable III, with the increase of the number of stages, the\ndetection performance is gradually improved to a critical value,\nbut the inference speed is gradually dropped. As the stage\nincreases from 1 to 2, the F-measure boosts from 84.8 to 85.2.\nAfter further increasing to 3, the result is not improved while\nthe inference time is decreased from 12.2 FPS to 11.2 FPS.\nThe example results across stages are shown in Fig. 6, which\nindicate that two contour refinement modules are sufficient to\nrefine an initial contour often with large prediction deviations.\nTherefore, considering the balance of speed and performance,\nwe set the number of stages to 2 in our all experiments.\n5) Contour Vertex Number: In the contour refinement mod-\nule, the number of input vertexes Na for each text affects\nthe fineness of the predicted contour. As shown in Table IV,\nCT-Net improves by 0.6 in F-measure when Na is increased\nfrom 16 to 32. With further increase of Na, the change of\ndetection performance is insignificant. This demonstrates that\nour contour refinement module can effectively capture the\nlong-range context dependencies between contour vertexes.\n6) Trade-Off Parameter: To investigate the influence of\ndifferent trade-off parameter values in the full loss function\non our method, we implement CT-Net with different values\nof λ1, λ2, λ3, and λ4, as presented in Table V. It can be\nobserved that the results are stable for different values, which\ndemonstrates the robustness of our method on the values of\ntrade-off parameters. Note that the impacts of changes to λ3\nand λ4 are slightly greater than those of changes to λ1 and λ2.\nThis is because λ3 and λ4 are related to the joint optimization\nof initial contour prediction and contour refinement, and thus\nbrings greater impacts on the whole model.\nC. Comparison with State-of-the-Art Methods\nWe compare our CT-Net with previous works on four bench-\nmarks, including two benchmarks for multi-oriented texts and\ntwo benchmarks for curved texts. Some qualitative results are\nvisualized in Fig. 7, which demonstrates the effectiveness of\nour CT-Net on scene texts with variety and complexity in size,\nfont, and orientation.\n1) Evaluation on Long Curved Text Benchmark: The com-\nparison results on long curved text dataset CTW1500 are\ngiven in Table VI, from which we can see that CT-Net\nachieves the state-of-the-art results of 83.8, 88.5 and 86.1\nin recall, precision and F-measure, respectively. Compared\nwith the most recent transformer based work [39], CT-Net\nachieves a 0.9 improvement in F-measure with a more concise\nstructure. Besides, our progressive contour regression method\nsoundly outperforms multi-stage contour based methods [13],\n[14]. This can be attributed to two main reasons. First, our\nproposed adaptive training strategy with a re-score mechanism\ncan reduce error accumulation across stages. Second, our\ncontour transformer module is more efficiently to extract\nglobal contour information, which is beneficial for perceiv-\ning the optimized direction of contour points. As shown in\nFig. 6, although there is a large deviation in the prediction\nof the initial contour, our contour refinement module still can\naccurately correct it.\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. X, X 8\n(a) CTW1500\n(b) Total-Text\n(c) ICDAR2015\n(d) MSRA-TD500\nFig. 7. Example results of our CT-Net on scene text datasets CTW1500 [22], Total-Text [23], ICDAR2015 [50], and MSRA-TD500 [51].\n2) Evaluation on Curved Text Benchmark: To test the\nmodel performance for detecting curved texts, we compare\nour CT-Net with other works on Total-Text dataset. The results\non Total-Text are shown in Table VI, where we evaluate CT-\nNet using the protocol in [23]. We can observe that our CT-\nNet achieves competitive results and speed. Specifically, CT-\nNet is superior to previous regression based methods such\nas ATRR [61], ContourNet [9], and TextDCT [67]. Besides,\ncompared to the single-stage contour based approach like Tex-\ntRay [43] and FCENet [12], CT-Net achieves more accurate\nlocalization through multiple perceptions. For example, CT-\nNet significantly boosts the recall of 2.5, precision of 1.5, and\nF-measure of 2.0, compared with the state-of-the-art single-\nstage contour based method FCENet. This is mainly because\nfor some complex texts, such single-stage method is difficult\nto accurately obtain their overall layout with only a single\nperception. In contrast, our CT-Net can iteratively correct the\ndeviations by multiple perceptions on the text contours, as\nshown in Fig. 6.\n3) Evaluation on Oriented Text Benchmark: We evaluate\nour CT-Net on ICDAR2015 dataset to test its effectiveness of\narbitrary-orientated text detection. ICDAR2015 is one of the\nmost popular multi-oriented text datasets with mostly low text\nquality and complex backgrounds. Test results on ICDAR2015\nfollowing the standard evaluation metric are shown in Ta-\nble VII. We can see that our CT-Net achieves 85.6, 88.1, and\n86.8 for recall, precision, and F-measure respectively without\nany extra datasets, and achieves satisfactory results in terms of\nF-measure (88.6 in F-measure) when pre-trained on SynthText.\nCompared with the performance of the recent segmentation\nbased and connected component based methods [42], [69] with\npre-training, our CT-Net can achieve competitive performance\nwithout pre-training, which indicates that our method is more\nrobust to noises by modeling text boundaries from a top-down\nperspective.\n4) Evaluation on Multi-Lingual Benchmark: The compari-\nson results on MSRA-TD500 dataset for evaluating the ability\nof multi-lingual text detection are presented in Table VII.\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. X, X 9\nTABLE VI\nCOMPARISONS WITH RELATED WORKS ON CTW1500 [22] AND TOTAL-TEXT [23] DATASETS . EXT MEANS EXTRA TRAINING DATA . SYN, MLT, AND\nART DENOTE SYNTH TEXT [54], ICDAR2017-MLT [55], AND ICDAR-A RT [56], RESPECTIVELY . MIXT DENOTES A DATASET COMPOSED OF\nCOCO-T EXT [57], S YNTH CURVE [11], AND ICDAR2019-MLT [58].\nMethod Paper CTW1500 Total-Text\nExt Recall Precision F-measure FPS Ext Recall Precision F-measure FPS\nTextSnake [26] ECCV’ 18 Syn 85.3 67.9 75 .6 - Syn 74.5 82 .7 78 .4 -\nSegLink++ [59] PR’ 19 Syn 79.8 82 .8 81 .3 - Syn 80.9 82 .1 81 .5 -\nTextField [28] TIP’ 19 Syn 79.8 83 .0 81 .4 6 .0 Syn 79.9 81 .2 80 .6 6 .0\nMSR [60] IJCAI’ 19 Syn 78.3 85 .0 81 .5 4 .3 Syn 74.8 83 .8 79 .0 4 .3\nATRR [61] CVPR’ 19 - 80.2 80 .1 80 .1 10 .0 - 76.2 80 .9 78 .5 -\nCRAFT [41] CVPR’ 19 Syn 81.1 86 .0 83 .5 - Syn 79.9 87 .6 83 .6 -\nPAN [7] ICCV’ 19 Syn 81.2 86 .4 83 .7 39.8 Syn 81.0 89 .3 85 .0 39.6\nMask-TTD [62] TIP’ 20 - 79.0 79 .7 79 .4 - - 74.5 79 .1 76 .7 -\nTextRay [43] MM’ 20 ArT 80.4 82 .8 81 .6 - ArT 77.9 83 .5 80 .6 -\nABCNet [11] CVPR’ 20 MixT 78.5 84 .4 81 .4 - MixT 81.3 87 .9 84 .5 -\nContourNet [9] CVPR’ 20 - 84.1 83 .7 83 .9 4 .5 - 83.9 86 .9 85 .4 3 .8\nDRRG [42] CVPR’ 20 MLT 83.0 85 .9 84 .5 - MLT 84.9 86.5 85 .7 -\nReLaText [63] PR’ 21 Syn 83.3 86 .2 84 .8 10 .6 Syn 83.1 84 .8 84 .0 3 .2\nOPMP [64] TMM’ 21 - 80.8 85 .1 82 .9 1 .4 Syn 82.7 87 .6 85 .1 1 .4\nNASK [65] TCSVT’ 21 Syn 80.1 83 .4 81 .7 12 .1 Syn 83.2 85 .6 84 .4 8 .4\nPCR [13] CVPR’ 21 MLT 82.3 87 .2 84 .7 - MLT 82.0 88 .5 85 .2 -\nFCENet [12] CVPR’ 21 - 83.4 87 .6 85.5 - - 82.5 89 .3 85 .8 -\nTextBPN [14] ICCV’ 21 Syn 81.4 87 .8 84 .5 12 .1 Syn 84.6 90 .2 87 .3 12.6\nDText [66] PR’ 22 Syn 82.7 86.9 84 .7 − Syn 82.7 90 .5 86 .4 −\nTextDCT [67] TMM’ 22 Syn 85.3 85.0 85 .1 17 .2 Syn 82.7 87 .2 84 .9 15 .1\nZhao et al. [68] TIP’ 22 Syn 82.1 86 .1 84 .1 - Syn 83.3 88 .2 85 .6 -\nDBNet++ [69] TPAMI’ 22 Syn 82.8 87 .9 85 .3 26 .0 Syn 83.2 88 .9 86 .0 28 .0\nTang et al. [39] CVPR’ 22 Syn 82.4 88 .1 85 .2 − Syn 85.7 90 .7 88.1 −\nCT-Net Ours - 82.7 87 .9 85 .2 11 .3 - 83.6 89 .2 86 .3 9 .8\nCT-Net Ours Syn 83.8 88.5 86 .1 11.2 Syn 85.0 90.8 87.8 10 .1\nTABLE VII\nCOMPARISONS WITH STATE -OF-THE -ART WORKS ON ICDAR2015 [50] AND MSRA-TD500 [51]. CO-T DENOTES COCO-T EXT [57].\nMethod Paper ICDAR2015 MSRA-TD500\nExt Recall Precision F-measure FPS Ext Recall Precision F-measure FPS\nRRPN [31] TMM’ 18 - 73.0 82 .0 77 .0 - - 68.0 82 .0 74 .4 4.4\nCheng et al. [6] TCSVT’ 19 CO-T 82.0 90 .0 86 .0 8 .6 - - - - -\nATRR [61] CVPR’ 19 - 86.0 89 .2 87 .6 - - 82.1 85.2 83.6 -\nCRAFT [41] CVPR’ 19 Syn 84.3 89 .8 86 .9 8 .6 Syn 78.2 88 .2 82 .9 8 .6\nDRRG [42] CVPR’ 20 MLT 84.7 88 .5 86 .6 - MLT 82.3 80 .5 85 .1 -\nContourNet [9] CVPR’ 20 - 86.1 87 .6 86 .9 3 .5 - - - - -\nR-Net [70] TMM’ 21 Syn 82.8 88 .7 85 .6 21.4 Syn 79.7 83 .7 81 .7 11 .8\nFCENet [12] CVPR’ 21 - 82.6 90 .1 86 .2 - - - - - -\nMOST [37] CVPR’ 21 Syn 87.3 89.1 88 .2 10 .0 Syn 82.7 90 .4 86 .4 51.8\nPCR [13] CVPR’ 21 - - - - - MLT 83.5 90 .8 87 .0 -\nTextBPN [14] ICCV’ 21 - - - - - Syn 80.7 85 .4 83 .0 12.7\nDText [66] PR’ 22 Syn 85.6 88 .5 87 .0 - Syn 83.1 87 .9 85 .4 -\nTextDCT [67] TMM’ 22 Syn 84.8 88 .9 86 .8 7 .5 - - - - -\nZhao et al. [68] TIP’ 22 Syn 82.4 89 .4 85 .8 - Syn 81.1 88 .7 84 .7 -\nDBNet++ [69] TPAMI’ 22 Syn 83.9 90.9 87.3 10 .0 Syn 83.3 91.5 87.2 29 .0\nRFN [71] TCSVT’ 22 - - - - - - 80.0 88 .4 84 .0 -\nKeserwani et al. [72] TCSVT’ 22 - 82.7 85 .9 84 .3 - - - - - -\nCT-Net Ours - 85.6 88 .1 86 .8 6 .6 - 80.4 89 .8 84 .8 11 .4\nCT-Net Ours Syn 86.4 90.9 88 .6 6.5 Syn 84.4 90.8 87.5 11.6\nCT-Net outperforms at least 0.5 than multi-stage contour\nbased methods [13], [14] in F-measure. Besides, due to the\nsmall number of training images in MSRA-TD500 dataset,\nthe previous methods [42], [61], [69] usually require pre-\ntraining on other datasets to achieve better results. In contrast,\nour method can achieve comparable performance without pre-\ntraining, which demonstrates the good generalization ability of\nour method. The example results on MSRA-TD500 are shown\nin Fig. 7, which shows that our CT-Net can effectively detect\nmulti-lingual and long texts.\n(a)\n (b)\nFig. 8. Failure cases, in which green contours are ground-truths while red\ncontours are predicted results. (a) Unreasonable annotations. (b) Challenging\nilluminations.\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. X, X 10\nD. Limitations\nAccording to the above experimental results, our CT-Net can\nperform well in most challenging scenes. However, there are\na few failure cases as shown in Fig. 8. Although our CT-Net\ncan alleviate false positives through adaptive training strategy\nand re-score mechanism, it cannot solve false positives caused\nby unreasonable annotations. Moreover, CT-Net cannot work\nwell under challenging illuminations like over- and under-\nexposures, owing to the unclear text boundaries. On the other\nhand, it is hard for our method to balance efficiency and\naccuracy, since the multi-stage optimized pipeline in CT-Net\nis complex and requires larger input image sizes to detect\nsmall-sized texts. We will try to improve the robustness on\nchallenging annotations and illuminations, and explore a more\npowerful structure in the future work.\nV. C ONCLUSION\nIn this paper, we have proposed a novel multi-stage contour\nbased framework for arbitrary-shaped scene text detection.\nFirst, we adopt a contour initialization module to generate\ninitial contours. Then, we use multiple contour refinement\nmodules with an adaptive training strategy and a re-score\nmechanism to perform iterative contour refinement, which is\nbeneficial for obtaining more accurate text shapes. Extensive\nexperiments have shown that our method can precisely detect\narbitrary-shaped texts in challenging benchmark datasets. In\nthe future study, we will develop this work and explore an\nend-to-end scene text spotting framework.\nREFERENCES\n[1] B. Xiong and K. Grauman, “Text detection in stores using a repetition\nprior,” in IEEE Winter Conference on Applications of Computer Vision .\nIEEE, 2016, pp. 1–9.\n[2] C. Yi and Y . Tian, “Scene text recognition in mobile applications by\ncharacter descriptor and structure configuration,” IEEE Transactions on\nImage Processing, vol. 23, no. 7, pp. 2972–2982, 2014.\n[3] Z. Hong, Y . Petillot, D. Lane, Y . Miao, and S. Wang, “Textplace: Visual\nplace recognition and topological localization through reading scene\ntexts,” in IEEE International Conference on Computer Vision , 2019,\npp. 2861–2870.\n[4] B. Xu, X. Shu, and Y . Song, “X-invariant contrastive augmentation\nand representation learning for semi-supervised skeleton-based action\nrecognition,” IEEE Transactions on Image Processing, vol. 31, pp. 3852–\n3867, 2022.\n[5] X. Shu, B. Xu, L. Zhang, and J. Tang, “Multi-granularity anchor-\ncontrastive representation learning for semi-supervised skeleton-based\naction recognition,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 45, no. 6, pp. 7559–7576, 2023.\n[6] P. Cheng, Y . Cai, and W. Wang, “A direct regression scene text detector\nwith position-sensitive segmentation,” IEEE Transactions on Circuits\nand Systems for Video Technology, vol. 30, no. 11, pp. 4171–4181, 2019.\n[7] W. Wang, E. Xie, X. Song, Y . Zang, W. Wang, T. Lu, G. Yu, and C. Shen,\n“Efficient and accurate arbitrary-shaped text detection with pixel aggre-\ngation network,” in IEEE International Conference on Computer Vision,\n2019, pp. 8440–8449.\n[8] Z. Tian, M. Shu, P. Lyu, R. Li, C. Zhou, X. Shen, and J. Jia, “Learning\nshape-aware embedding for scene text detection,” in IEEE Conference\non Computer Vision and Pattern Recognition , 2020.\n[9] Y . Wang, H. Xie, Z.-J. Zha, M. Xing, Z. Fu, and Y . Zhang, “Contournet:\nTaking a further step toward accurate arbitrary-shaped scene text detec-\ntion,” in IEEE Conference on Computer Vision and Pattern Recognition,\n2020, pp. 11 753–11 762.\n[10] M. Xing, H. Xie, Q. Tan, S. Fang, Y . Wang, Z.-J. Zha, and Y . Zhang,\n“Boundary-aware arbitrary-shaped scene text detector with learnable\nembedding network,” IEEE Transactions on Multimedia , 2021.\n[11] Y . Liu, H. Chen, C. Shen, T. He, L. Jin, and L. Wang, “Abcnet: Real-\ntime scene text spotting with adaptive bezier-curve network,” in IEEE\nConference on Computer Vision and Pattern Recognition , 2020, pp.\n9809–9818.\n[12] Y . Zhu, J. Chen, L. Liang, Z. Kuang, L. Jin, and W. Zhang, “Fourier con-\ntour embedding for arbitrary-shaped text detection,” in IEEE Conference\non Computer Vision and Pattern Recognition , 2021, pp. 3123–3131.\n[13] P. Dai, S. Zhang, H. Zhang, and X. Cao, “Progressive contour regres-\nsion for arbitrary-shape scene text detection,” in IEEE Conference on\nComputer Vision and Pattern Recognition , 2021, pp. 7393–7402.\n[14] S.-X. Zhang, X. Zhu, C. Yang, H. Wang, and X.-C. Yin, “Adaptive\nboundary proposal network for arbitrary shape text detection,” in IEEE\nInternational Conference on Computer Vision , 2021, pp. 1305–1314.\n[15] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in IEEE Conference on Computer Vision and Pattern\nRecognition, 2016, pp. 770–778.\n[16] T.-Y . Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,\n“Feature pyramid networks for object detection,” in IEEE Conference\non Computer Vision and Pattern Recognition , 2017, pp. 2117–2125.\n[17] S. Peng, W. Jiang, H. Pi, X. Li, H. Bao, and X. Zhou, “Deep snake\nfor real-time instance segmentation,” in IEEE Conference on Computer\nVision and Pattern Recognition , 2020, pp. 8533–8542.\n[18] T. N. Kipf and M. Welling, “Semi-supervised classification with graph\nconvolutional networks,” in International Conference on Learning Rep-\nresentations, 2017.\n[19] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition at\nscale,” in International Conference on Learning Representations , 2021.\n[20] B. Xu, X. Shu, J. Zhang, G. Dai, and Y . Song, “Spatiotemporal\ndecouple-and-squeeze contrastive learning for semisupervised skeleton-\nbased action recognition,” IEEE Transactions on Neural Networks and\nLearning Systems, 2023.\n[21] B. Xu and X. Shu, “Pyramid self-attention polymerization learning\nfor semi-supervised skeleton-based action recognition,” arXiv preprint\narXiv:2302.02327, 2023.\n[22] Y . Liu, L. Jin, S. Zhang, C. Luo, and S. Zhang, “Curved scene text\ndetection via transverse and longitudinal sequence connection,” Pattern\nRecognition, vol. 90, pp. 337–345, 2019.\n[23] C. K. Ch’ng and C. S. Chan, “Total-text: A comprehensive dataset for\nscene text detection and recognition,” in IAPR International Conference\non Document Analysis and Recognition , vol. 1. IEEE, 2017, pp. 935–\n942.\n[24] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for\nsemantic segmentation,” in IEEE Conference on Computer Vision and\nPattern Recognition, 2015, pp. 3431–3440.\n[25] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional net-\nworks for biomedical image segmentation,” in International Conference\non Medical Image Computing and Computer-Assisted Intervention .\nSpringer, 2015, pp. 234–241.\n[26] S. Long, J. Ruan, W. Zhang, X. He, W. Wu, and C. Yao, “Textsnake:\nA flexible representation for detecting text of arbitrary shapes,” in\nEuropean Conference on Computer Vision , 2018, pp. 20–36.\n[27] W. Wang, E. Xie, X. Li, W. Hou, T. Lu, G. Yu, and S. Shao, “Shape\nrobust text detection with progressive scale expansion network,” in IEEE\nConference on Computer Vision and Pattern Recognition , 2019, pp.\n9336–9345.\n[28] Y . Xu, Y . Wang, W. Zhou, Y . Wang, Z. Yang, and X. Bai, “Textfield:\nLearning a deep direction field for irregular scene text detection,” IEEE\nTransactions on Image Processing, vol. 28, no. 11, pp. 5566–5579, 2019.\n[29] Z. Tian, M. Shu, P. Lyu, R. Li, C. Zhou, X. Shen, and J. Jia, “Learning\nshape-aware embedding for scene text detection,” in IEEE Conference\non Computer Vision and Pattern Recognition , 2019, pp. 4234–4243.\n[30] M. Liao, Z. Wan, C. Yao, K. Chen, and X. Bai, “Real-time scene\ntext detection with differentiable binarization,” in AAAI Conference on\nArtificial Intelligence, 2020, pp. 11 474–11 481.\n[31] J. Ma, W. Shao, H. Ye, L. Wang, H. Wang, Y . Zheng, and X. Xue,\n“Arbitrary-oriented scene text detection via rotation proposals,” IEEE\nTransactions on Multimedia , vol. 20, no. 11, pp. 3111–3122, 2018.\n[32] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\nobject detection with region proposal networks,” Advances in Neural\nInformation Processing Systems , vol. 28, pp. 91–99, 2015.\n[33] M. Liao, B. Shi, X. Bai, X. Wang, and W. Liu, “Textboxes: A fast text\ndetector with a single deep neural network,” in AAAI Conference on\nArtificial Intelligence, 2017.\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. X, X 11\n[34] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and A. C.\nBerg, “Ssd: Single shot multibox detector,” in European Conference on\nComputer Vision. Springer, 2016, pp. 21–37.\n[35] M. Liao, B. Shi, and X. Bai, “Textboxes++: A single-shot oriented scene\ntext detector,” IEEE Transactions on Image Processing , vol. 27, no. 8,\npp. 3676–3690, 2018.\n[36] X. Zhou, C. Yao, H. Wen, Y . Wang, S. Zhou, W. He, and J. Liang, “East:\nan efficient and accurate scene text detector,” in IEEE Conference on\nComputer Vision and Pattern Recognition , 2017, pp. 5551–5560.\n[37] M. He, M. Liao, Z. Yang, H. Zhong, J. Tang, W. Cheng, C. Yao,\nY . Wang, and X. Bai, “Most: A multi-oriented scene text detector with\nlocalization refinement,” in IEEE Conference on Computer Vision and\nPattern Recognition, 2021, pp. 8813–8822.\n[38] C. Zhang, B. Liang, Z. Huang, M. En, and X. Ding, “Look more\nthan once: An accurate detector for text of arbitrary shapes,” in IEEE\nConference on Computer Vision and Pattern Recognition , 2019.\n[39] J. Tang, W. Zhang, H. Liu, M. Yang, B. Jiang, G. Hu, and X. Bai,\n“Few could be better than all: Feature sampling and grouping for scene\ntext detection,” in IEEE Conference on Computer Vision and Pattern\nRecognition, 2022, pp. 4563–4572.\n[40] Z. Tian, W. Huang, T. He, P. He, and Y . Qiao, “Detecting text in\nnatural image with connectionist text proposal network,” in European\nConference on Computer Vision . Springer, 2016, pp. 56–72.\n[41] Y . Baek, B. Lee, D. Han, S. Yun, and H. Lee, “Character region\nawareness for text detection,” in IEEE Conference on Computer Vision\nand Pattern Recognition, 2019, pp. 9365–9374.\n[42] S. X. Zhang, X. Zhu, J. B. Hou, C. Liu, and X. C. Yin, “Deep relational\nreasoning graph network for arbitrary shape text detection,” in IEEE\nConference on Computer Vision and Pattern Recognition , 2020.\n[43] F. Wang, Y . Chen, F. Wu, and X. Li, “Textray: Contour-based geometric\nmodeling for arbitrary-shaped scene text detection,” in ACM Interna-\ntional Conference on Multimedia , 2020, pp. 111–119.\n[44] P. Sun, Y . Jiang, E. Xie, W. Shao, Z. Yuan, C. Wang, and P. Luo, “What\nmakes for end-to-end object detection?” in International Conference on\nMachine Learning. PMLR, 2021, pp. 9934–9944.\n[45] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n[46] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” arXiv\npreprint arXiv:1606.08415, 2016.\n[47] B. Wu, Y . Yang, D. Yang, J. Wu, H. Huang, L. Wang, J. Liu, and\nY . Xu, “Progressive hard-case mining across pyramid levels in object\ndetection,” arXiv preprint arXiv:2109.07217 , 2021.\n[48] X. Li, W. Wang, L. Wu, S. Chen, X. Hu, J. Li, J. Tang, and J. Yang,\n“Generalized focal loss: Learning qualified and distributed bounding\nboxes for dense object detection,” Advances in Neural Information\nProcessing Systems, vol. 33, pp. 21 002–21 012, 2020.\n[49] H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese,\n“Generalized intersection over union: A metric and a loss for bounding\nbox regression,” in IEEE Conference on Computer Vision and Pattern\nRecognition, 2019, pp. 658–666.\n[50] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. Ghosh, A. Bagdanov,\nM. Iwamura, J. Matas, L. Neumann, V . R. Chandrasekhar, S. Lu et al.,\n“Icdar 2015 competition on robust reading,” in IAPR International\nConference on Document Analysis and Recognition . IEEE, 2015, pp.\n1156–1160.\n[51] C. Yao, X. Bai, and W. Liu, “A unified framework for multioriented\ntext detection and recognition,” IEEE Transactions on Image Processing,\nvol. 23, no. 11, pp. 4737–4749, 2014.\n[52] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf,\nE. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style, high-\nperformance deep learning library,” in Advances in Neural Information\nProcessing Systems. Curran Associates, Inc., 2019, pp. 8024–8035.\n[53] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification\nwith deep convolutional neural networks,” Advances in Neural Informa-\ntion Processing Systems , vol. 25, pp. 1097–1105, 2012.\n[54] A. Gupta, A. Vedaldi, and A. Zisserman, “Synthetic data for text\nlocalisation in natural images,” in IEEE Conference on Computer Vision\nand Pattern Recognition, 2016, pp. 2315–2324.\n[55] N. Nayef, F. Yin, I. Bizid, H. Choi, Y . Feng, D. Karatzas, Z. Luo, U. Pal,\nC. Rigaud, J. Chazalon et al. , “Icdar2017 robust reading challenge on\nmulti-lingual scene text detection and script identification-rrc-mlt,” in\nIAPR International Conference on Document Analysis and Recognition ,\nvol. 1. IEEE, 2017, pp. 1454–1459.\n[56] C. K. Chng, Y . Liu, Y . Sun, C. C. Ng, C. Luo, Z. Ni, C. Fang, S. Zhang,\nJ. Han, E. Ding et al., “Icdar2019 robust reading challenge on arbitrary-\nshaped text-rrc-art,” in IAPR International Conference on Document\nAnalysis and Recognition . IEEE, 2019, pp. 1571–1576.\n[57] A. Veit, T. Matera, L. Neumann, J. Matas, and S. Belongie, “Coco-text:\nDataset and benchmark for text detection and recognition in natural\nimages,” arXiv preprint arXiv:1601.07140 , 2016.\n[58] N. Nayef, Y . Patel, M. Busta, P. N. Chowdhury, D. Karatzas, W. Khlif,\nJ. Matas, U. Pal, J.-C. Burie, C.-l. Liu et al., “Icdar2019 robust reading\nchallenge on multi-lingual scene text detection and recognition—rrc-\nmlt-2019,” in IAPR International Conference on Document Analysis and\nRecognition. IEEE, 2019, pp. 1582–1587.\n[59] J. Tang, Z. Yang, Y . Wang, Q. Zheng, Y . Xu, and X. Bai, “Seglink++:\nDetecting dense and arbitrary-shaped scene text by instance-aware\ncomponent grouping,” Pattern recognition, vol. 96, p. 106954, 2019.\n[60] C. Xue, S. Lu, and W. Zhang, “Msr: Multi-scale shape regression for\nscene text detection,” in International Joint Conference on Artificial\nIntelligence, 2019.\n[61] X. Wang, Y . Jiang, Z. Luo, C.-L. Liu, H. Choi, and S. Kim, “Arbitrary\nshape scene text detection with adaptive text region representation,” in\nIEEE Conference on Computer Vision and Pattern Recognition , 2019,\npp. 6449–6458.\n[62] Y . Liu, L. Jin, and C. Fang, “Arbitrarily shaped scene text detection with\na mask tightness text detector,” IEEE Transactions on Image Processing,\nvol. 29, pp. 2918–2930, 2019.\n[63] C. Ma, L. Sun, Z. Zhong, and Q. Huo, “Relatext: Exploiting visual\nrelationships for arbitrary-shaped scene text detection with graph con-\nvolutional networks,” Pattern Recognition, vol. 111, p. 107684, 2021.\n[64] S. Zhang, Y . Liu, L. Jin, Z. Wei, and C. Shen, “Opmp: An omnidi-\nrectional pyramid mask proposal network for arbitrary-shape scene text\ndetection,” IEEE Transactions on Multimedia , vol. 23, pp. 454–467,\n2020.\n[65] M. Cao, C. Zhang, D. Yang, and Y . Zou, “All you need is a second look:\nTowards arbitrary-shaped text detection,” IEEE Transactions on Circuits\nand Systems for Video Technology , vol. 32, no. 2, pp. 758–767, 2021.\n[66] Y . Cai, Y . Liu, C. Shen, L. Jin, Y . Li, and D. Ergu, “Arbitrarily shaped\nscene text detection with dynamic convolution,” Pattern Recognition ,\nvol. 127, p. 108608, 2022.\n[67] Y . Su, Z. Shao, Y . Zhou, F. Meng, H. Zhu, B. Liu, and R. Yao, “Textdct:\nArbitrary-shaped text detection via discrete cosine transform mask,”\nIEEE Transactions on Multimedia , 2022.\n[68] M. Zhao, W. Feng, F. Yin, X.-Y . Zhang, and C.-L. Liu, “Mixed-\nsupervised scene text detection with expectation-maximization algo-\nrithm,” IEEE Transactions on Image Processing, vol. 31, pp. 5513–5528,\n2022.\n[69] M. Liao, Z. Zou, Z. Wan, C. Yao, and X. Bai, “Real-time scene text\ndetection with differentiable binarization and adaptive scale fusion,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.\n[70] Y . Wang, H. Xie, Z. Zha, Y . Tian, Z. Fu, and Y . Zhang, “R-net: A\nrelationship network for efficient and accurate scene text detection,”\nIEEE Transactions on Multimedia , vol. 23, pp. 1316–1329, 2020.\n[71] T. Guan, C. Gu, C. Lu, J. Tu, Q. Feng, K. Wu, and X. Guan,\n“Industrial scene text detection with refined feature-attentive network,”\nIEEE Transactions on Circuits and Systems for Video Technology, 2022.\n[72] P. Keserwani, R. Saini, M. Liwicki, and P. P. Roy, “Robust scene text\ndetection for partially annotated training data,” IEEE Transactions on\nCircuits and Systems for Video Technology , 2022.\nZhiwen Shao received his B.Eng. degree in Com-\nputer Science and Technology from the Northwest-\nern Polytechnical University, China in 2015. He\nreceived the Ph.D. degree from the Shanghai Jiao\nTong University, China in 2020. He is now an As-\nsociate Professor at the School of Computer Science\nand Technology, China University of Mining and\nTechnology, China. From 2017 to 2018, he was a\njoint Ph.D. student at the Multimedia and Interactive\nComputing Lab, Nanyang Technological University,\nSingapore. His research interests lie in computer\nvision and deep learning. He has been serving as a PC member in IJCAI\nand AAAI.\nIEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. X, NO. X, X 12\nYuchen Su is currently pursuing the M.S. degree\nat the School of Computer Science and Technology,\nChina University of Mining and Technology, China,\nunder the supervision of Prof. Yong Zhou, Prof.\nFanrong Meng, and Dr. Zhiwen Shao. His current\nresearch interests include scene text detection and\nrecognition.\nYong Zhou received the M.S. and Ph.D. degrees\nin Control Theory and Control Engineering from\nthe China University of Mining and Technology,\nChina in 2003 and 2006, respectively. He is currently\na Professor with the School of Computer Science\nand Technology, China University of Mining and\nTechnology, China. His research interests include\nmachine learning, intelligence optimization, and data\nmining. He has been serving as an Associate Editor\nof ACM TOMM.\nFanrong Meng received the Ph.D. degree from the\nChina University of Mining and Technology, China.\nShe is currently a Professor with the School of\nComputer Science and Technology, China University\nof Mining and Technology, China. Her research\ninterests include intelligent information processing,\ndatabase technology, and data mining.\nHancheng Zhu received the B.S. degree from\nthe Changzhou Institute of Technology, Changzhou,\nChina, in 2012, and the M.S. and Ph.D. degrees from\nthe China University of Mining and Technology,\nXuzhou, China, in 2015 and 2020, respectively. He is\ncurrently a Tenure-Track Associate Professor at the\nSchool of Computer Science and Technology, China\nUniversity of Mining and Technology, China. His re-\nsearch interests include image aesthetics assessment\nand affective computing.\nBing Liu received the B.S., M.S., and Ph.D. de-\ngrees in 2002, 2005, and 2013, respectively, from\nthe China University of Mining and Technology,\nXuzhou, China. He is currently an Associate Pro-\nfessor at the School of Computer Science and Tech-\nnology, China University of Mining and Technology,\nChina. His current research interests include natural\nlanguage processing, image understanding, and deep\nlearning.\nRui Yao received the Ph.D. degree in computer\nscience from the Northwestern Polytechnical Uni-\nversity, Xi’an, China, in 2013. From 2011 to 2012,\nhe was a Visiting Student with the University of\nAdelaide, Adelaide, SA, Australia. He is currently\na Professor with the School of Computer Science\nand Technology, China University of Mining and\nTechnology, Xuzhou, China. His research interests\ninclude computer vision and machine learning.",
  "topic": "Initialization",
  "concepts": [
    {
      "name": "Initialization",
      "score": 0.8318967819213867
    },
    {
      "name": "Computer science",
      "score": 0.7450427412986755
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6767473816871643
    },
    {
      "name": "False positive paradox",
      "score": 0.5496675372123718
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.51441890001297
    },
    {
      "name": "Transformer",
      "score": 0.5060285925865173
    },
    {
      "name": "Computer vision",
      "score": 0.4860982894897461
    },
    {
      "name": "Pixel",
      "score": 0.48157787322998047
    },
    {
      "name": "Contour line",
      "score": 0.4232061207294464
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.41834622621536255
    },
    {
      "name": "Data mining",
      "score": 0.16260167956352234
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I25757504",
      "name": "China University of Mining and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I1327237609",
      "name": "Ministry of Education of the People's Republic of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    }
  ]
}