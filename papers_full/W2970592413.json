{
  "title": "TMU Transformer System Using BERT for Re-ranking at BEA 2019 Grammatical Error Correction on Restricted Track",
  "url": "https://openalex.org/W2970592413",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5005531754",
      "name": "Masahiro Kaneko",
      "affiliations": [
        "Tokyo Metropolitan University"
      ]
    },
    {
      "id": "https://openalex.org/A5008625798",
      "name": "Kengo Hotate",
      "affiliations": [
        "Tokyo Metropolitan University"
      ]
    },
    {
      "id": "https://openalex.org/A5010294982",
      "name": "Satoru Katsumata",
      "affiliations": [
        "Tokyo Metropolitan University"
      ]
    },
    {
      "id": "https://openalex.org/A5061931124",
      "name": "Mamoru Komachi",
      "affiliations": [
        "Tokyo Metropolitan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964258094",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1640336798",
    "https://openalex.org/W2798416860",
    "https://openalex.org/W2964187553",
    "https://openalex.org/W2554764206",
    "https://openalex.org/W2963975242",
    "https://openalex.org/W2977788613",
    "https://openalex.org/W2171043109",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2144950812",
    "https://openalex.org/W2963881719",
    "https://openalex.org/W2890230387",
    "https://openalex.org/W2146574666",
    "https://openalex.org/W2170527467",
    "https://openalex.org/W2758774757",
    "https://openalex.org/W2797885244",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2785047343",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2741494657",
    "https://openalex.org/W2098297786",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2124725212",
    "https://openalex.org/W2964082031",
    "https://openalex.org/W1721115786",
    "https://openalex.org/W2963039614",
    "https://openalex.org/W2153013403"
  ],
  "abstract": "We introduce our system that is submitted to the restricted track of the BEA 2019 shared task on grammatical error correction1 (GEC). It is essential to select an appropriate hypothesis sentence from the candidates list generated by the GEC model. A re-ranker can evaluate the naturalness of a corrected sentence using language models trained on large corpora. On the other hand, these language models and language representations do not explicitly take into account the grammatical errors written by learners. Thus, it is not straightforward to utilize language representations trained from a large corpus, such as Bidirectional Encoder Representations from Transformers (BERT), in a form suitable for the learner’s grammatical errors. Therefore, we propose to fine-tune BERT on learner corpora with grammatical errors for re-ranking. The experimental results of the W&I+LOCNESS development dataset demonstrate that re-ranking using BERT can effectively improve the correction performance.",
  "full_text": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 207–212\nFlorence, Italy, August 2, 2019.c⃝2019 Association for Computational Linguistics\n207\nTMU Transformer System Using BERT for Re-ranking at BEA 2019\nGrammatical Error Correction on Restricted Track\nMasahiro Kaneko Kengo Hotate Satoru Katsumata Mamoru Komachi\nTokyo Metropolitan University, Japan\n{kaneko-masahiro@ed, hotate-kengo@ed,\nkatsumata-satoru@ed, komachi@}.tmu.ac.jp\nAbstract\nWe introduce our system that is submitted to\nthe restricted track of the BEA 2019 shared\ntask on grammatical error correction 1 (GEC).\nIt is essential to select an appropriate hypothe-\nsis sentence from the candidates list generated\nby the GEC model. A re-ranker can evalu-\nate the naturalness of a corrected sentence us-\ning language models trained on large corpora.\nOn the other hand, these language models and\nlanguage representations do not explicitly take\ninto account the grammatical errors written by\nlearners. Thus, it is not straightforward to uti-\nlize language representations trained from a\nlarge corpus, such as Bidirectional Encoder\nRepresentations from Transformers (BERT),\nin a form suitable for the learner’s grammat-\nical errors. Therefore, we propose to ﬁne-\ntune BERT on learner corpora with grammat-\nical errors for re-ranking. The experimen-\ntal results of the W&I+LOCNESS develop-\nment dataset demonstrate that re-ranking using\nBERT can effectively improve the correction\nperformance.\n1 Introduction\nGrammatical error correction (GEC) systems may\nbe used for language learning to detect and cor-\nrect grammatical errors in text written by language\nlearners. GEC has grown in importance over the\npast few years due to the increasing need for peo-\nple to learn new languages. GEC has been ad-\ndressed in the Helping Our Own (HOO) (Dale and\nKilgarriff, 2011; Dale et al., 2012) and Confer-\nence on Natural Language Learning (CoNLL) (Ng\net al., 2013, 2014) shared tasks between 2011 and\n2014.\nRecent research has demonstrated the effective-\nness of the neural machine translation model for\n1https://www.cl.cam.ac.uk/research/nl/\nbea2019st/\nGEC. There are three main types of neural net-\nwork models for GEC, namely, recurrent neural\nnetworks (Ge et al., 2018), a multi-layer convo-\nlutional model based on convolutional neural net-\nworks (Chollampatt and Ng, 2018a) and a trans-\nformer model based on self-attention (Junczys-\nDowmunt et al., 2018). We follow the best prac-\ntices to develop our system based on the trans-\nformer model, which has achieved better perfor-\nmance for GEC (Zhao et al., 2019).\nRe-ranking using a language model trained on\nlarge-scale corpora contributes to the improved\nhypotheses of the GEC model (Chollampatt and\nNg, 2018a). Typically, a language model is trained\nby maximizing the log-likelihood of a sentence.\nHence, such models observe only the positive ex-\namples of a raw corpus. However, these mod-\nels may not be sufﬁcient to take into account the\ngrammatical errors written by language learners.\nTherefore, we ﬁne-tune these models trained from\nlarge-scale raw data on learner corpora to explic-\nitly take into account grammatical errors to re-rank\nthe hypotheses for the GEC tasks.\nBidirectional Encoder Representations from\nTransformer (BERT) (Devlin et al., 2019) can con-\nsider information of large-scale raw corpora and\ntask speciﬁc information by ﬁne-tuning on the\ntarget task corpora. Moreover, BERT is known\nto be effective in the distinction of grammatical\nsentences from ungrammatical sentences (Kaneko\nand Komachi, 2019). They proposed a grammat-\nical error detection (GED) model based on BERT\nthat achieved state-of-the-art results in word-level\nGED tasks. Therefore, we use BERT, pre-trained\nwith large-scale raw corpora, and ﬁne-tune it with\nlearner corpora for re-ranking the hypotheses of\nour GEC model to utilize not only the large-scale\nraw corpora but also a set of information on gram-\nmatical errors.\nThe main contribution of this study is that\n208\nthe experimental results demonstrate that BERT,\nwhich considers both the representations trained\non large-scale and learners corpora, is effective\nfor re-ranking the hypotheses for GEC tasks. Ad-\nditionally, we demonstrated that BERT based on\nself-attention can re-rank sentences corrected from\nthe GEC model by capturing long distance infor-\nmation.\n2 TMU System\nOur system is a GEC model that is combined with\na re-ranker. The GEC model is given a source sen-\ntence as input and generates hypothesis sentences.\nThese hypothesis sentences are given as input to\nthe re-ranker, which selects the ﬁnal corrected sen-\ntence form the hypothesis sentences.\nWe use the transformer (Vaswani et al., 2017)\narchitecture for the GEC model because it is a\nstate-of-the-art model in the GEC task (Zhao et al.,\n2019). The transformer architecture comprises\nmultiple layers of transformer block. The\nlayers of the encoder and decoder have position-\nwise feedforward layers over the tokens of input\nsentences. The decoder has an extra attention layer\nover the encoder’s hidden states. This GEC model\nis optimized by minimizing the label smoothed\ncross-entropy loss.\nThe re-ranker uses ﬁve features. We use BERT\nﬁne-tuned on learner corpora to predict the gram-\nmatical quality as a feature of re-ranking.\n2.1 Architecture and training of BERT for\nre-ranking\nWe used BERT (Devlin et al., 2019) as a fea-\nture for re-ranking the hypotheses of the GEC\nsystem. BERT is designed to learn deep bidi-\nrectional representations by jointly conditioning\nboth the left and right contexts in all layers, based\non transformer block with multi-head self-\nattention and fully connected layers. The param-\neters of BERT were pre-trained using a masked\nlanguage model and the prediction of the next sen-\ntence.\nWe ﬁne-tuned the pre-trained BERT on learner\ncorpora to judge the grammatical quality of the in-\nput sentence, i.e., to distinguish between a sen-\ntence with and without grammatical errors on a\nsentence-level. We annotated sentences from par-\nallel learner corpora having incorrect and correct\nsentences with 0 (incorrect) and 1 (correct) labels.\nHence, using the above, we can take advantage of\nCorpus Train Dev Test\nFCE 28,350 2,191 2,695\nLang-8 1,037,561 - -\nNUCLE 57,151 - -\nW&I+LOCNESS 34,308 4,384 4,477\nTable 1: Number of sentences in corpora on GEC\nshared task for restricted track.\nboth the large-scale raw data and learner corpora\nby using BERT. The model was optimized dur-\ning ﬁne-tuning by minimizing the sentence-level\ncross-entropy loss.\n2.2 Re-ranking\nWe used the following set of features for re-\nranking, which are the same as those in a pre-\nviously reported approach (Chollampatt and Ng,\n2018a), except for BERT:\n•GEC model: The score of the hypothesis\nsentence from the GEC model is computed\nusing the log probabilities of predictions nor-\nmalized by sentence length on a token-level.\n•Language model: A 5-gram language model\nscore is computed by normalizing the log\nprobabilities of the hypothesis sentence by\nsentence length.\n•BERT: The predicted score for the grammat-\nical quality of the hypothesis sentence.\n•Edit operations: Three token level features,\nnamely, denoting the number of substitutions,\ndeletions, and insertions between the source\nsentence and the hypothesis sentence.\n•Hypothesis sentence length: The number of\nwords in the hypothesis sentence to penalize\nshort hypothesis sentences.\nFeature weights are optimized by minimum error\nrate training (MERT) (Och, 2003) on the develop-\nment set.\n3 Experiments\n3.1 Dataset\nIn the restricted track, we only used the corpora\nlisted in Table 1. The First Certiﬁcate in English\n(FCE) corpus (Yannakoudakis et al., 2011), Lang-\n8 learner corpus (Mizumoto et al., 2011), Na-\ntional University of Singapore Corpus of Learner\n209\nParameter Value\nWord embedding size 500\nMulti-head number 10\nLayer size 6\nHidden size 2,048\nOptimizer Adam\nAdam β1 0.9\nAdam β2 0.98\nLearning rate 0.0005\nLearning rate scheduler inverse square root\nWarmup steps 4,000\nMinimum learning rate 1e-09\nDropout 0.3\nWeight decay 0.0001\nLabel smoothing 0.1\nMax token size 4,096\nEnsemble size 3\nTable 2: Hyperparameter values of our transformer\nGEC model.\n# Team Name P R F0.5\n1 UEDIN-MS 72.28 60.12 69.47\n2 Kakao&Brain 75.19 51.91 69.00\n7 ML@IITB 65.70 61.12 64.73\n14 TMU 53.91 51.65 53.45\nTable 3: Results of GEC systems with the highest P, R\nand F0.5 overall vs TMU on restricted track on ofﬁcial\nW&I test data.\nEnglish (NUCLE) (Dahlmeier et al., 2013) and\nWrite & Improve (W&I)+LOCNESS corpus (Yan-\nnakoudakis et al., 2018; Granger, 1998) were used\nfor this shared task. W&I+LOCNESS corpus was\na new corpus released for this shared task and the\nshared task systems were evaluated on a gold test\nset of the overall W&I+LOCNESS dataset.\nWe used FCE (ofﬁcial split of train, dev, and\ntest set), Lang-8, NUCLE, and W&I+LOCNESS\ntraining set as training data and we split the\nW&I+LOCNESS development set into develop-\nment and test data by random selection from each\nCommon European Framework of Reference for\nLanguages (CEFR) levels (beginner, intermediate,\nadvanced, native) for the transformer and BERT.\nThe development and test data sizes were 2,191\nand 2,193, respectively.\nModel P R F0.5\nTMU system 37.79 28.08 35.35\nw/o BERT 38.75 23.76 34.41\nw/o language model 37.85 26.41 34.83\nw/o re-ranking 36.46 22.91 32.60\nTable 4: Effectiveness of re-ranking without different\nfeatures.\n3.2 Setup\nWe implemented the transformer model based on\nthe Fairseq tool2. The hyperparameters used in\nour transformer GEC model are listed in Table\n2. The parameters of the ensemble models were\ninitialized with different values. We initialized\nthe embedding layers of the encoder and decoder\nwith the embeddings pre-trained on the English\nWikipedia using fastText tool3 (Bojanowski et al.,\n2017).\nWe used a publicly available pre-trained BERT\nmodel4, namely the BERTBASE uncased model,\nwhich was pre-trained on large-scale BooksCor-\npus and English Wikipedia corpora. This model\nhad 12 layers, 768 hidden sizes, and 16 heads of\nself-attention. Our model’s hyperparameters for\nre-ranking were similar to the default ones de-\nscribed by Devlin et al. (2019). We used the\nsame learner corpora with incorrect and correct\nsentences used for training our GEC model to ﬁne-\ntune BERT.\nThe 5-gram language model for re-ranking was\ntrained on a subset of the Common Crawl cor-\npus (Chollampatt and Ng, 2018a). 5 We used a\nPython spell checker tool6 on the GEC model hy-\npothesis sentences.\n3.3 Evaluation\nThe systems submitted to the shared task were\nevaluated using the ERRANT 7 scorer (Felice\net al., 2016; Bryant et al., 2017). This metric\nis an improved version of the MaxMatch scorer\n(Dahlmeier and Ng, 2012) originally used in the\n2https://github.com/pytorch/fairseq\n3https://github.com/facebookresearch/\nfastText\n4https://github.com/google-research/\nbert\n5https://github.com/nusnlp/\nmlconvgec2018\n6https://pypi.org/project/\npyspellchecker/\n7https://github.com/chrisjbryant/\nerrant\n210\n(a)\nSource The range of public services will be expanded to remote areas ,it becomemuch more convenient .\nGold The range of public services will be expanded to remote areas , and it will becomemuch more convenient .\nw/o BERT The range of public services will be expanded to remote areas ,has becomemuch more convenient .\nTMU systemThe range of public services will be expanded to remote areas ,and it will becomemuch more convenient .\n(b)\nSource Her sister is 6 years old and you shouldlook after every weekend.\nGold Her sister is 6 years old and you would have to look after her every weekend.\nw/o BERT Her sister is 6 years old and you shouldlook after it every weekend.\nTMU systemHer sister is 6 years old and you shouldlook after it every weekend.\nTable 5: (a) Successful and (b) unsuccessful examples of TMU system for long distance errors. Bold indicates the\nerroneous part of the source sentence; Underline indicates the corrected part of the gold sentence; Italic represents\nthe corrected output of the GEC system.\nCoNLL shared tasks (Ng et al., 2013, 2014). The\nscorer reported the performance in terms of span-\nbased and token-based detection. The system per-\nformance was primarily measured with regard to\nspan-based correction using theF0.5 metric, which\nassigned twice as much weight to the precision. In\nthis study, we report on precision, recall, and F0.5\nbased on the ERRANT score.\n3.4 Results\nTable 3 presents the results of our system (TMU)\nand others on precision (P), recall (R) and F0.5 on\nW&I+LOCNESS test data for the BEA 2019 GEC\nshared task on the restricted track. Our system was\nranked 14 out of 21 teams.\n4 Discussions\nWe investigated whether using BERT as a fea-\nture for re-ranking can improve the corrected re-\nsults. Table 4 presents the experimental results\nof removing the following re-ranking features:\nBERT (w/o BERT); language model (w/o lan-\nguage model); and all features (w/o re-ranking).\nThe recall and F0.5 of the complete model (TMU\nsystem) is higher than those of w/o BERT, indicat-\ning that using BERT for re-ranking can improve\nthe accuracy; especially, the recall is signiﬁcantly\nimproved. We conclude that BERT uses the ad-\nvantage of large-scale raw data to acquire general\nlinguistic expressions and learn grammatical error\ninformation from learner corpora, thus detecting\nand re-ranking errors more effectively.\nAdditionally, we analyzed the type of grammat-\nical errors that were corrected by using BERT for\nre-ranking. Table 5 presents the output examples\nof our system with and without BERT. Example\n(a) demonstrates that our system can correct long\ndistance verb tense errors, matching Gold in this\ncase, where after stating that “... services will be\nexpanded ... ”in the ﬁrst half, our system prop-\nerly corrected “... it become ... ”to “... it will be-\ncome ... ”in the second part of the given sentence.\nOn the other hand, w/o BERT created a sentence\nwith inconsistent verb tense by changing “... it\nbecome ... ”to “... it has become ... ”. Example\n(b) demonstrates that neither of the systems, i.e.,\nwith and without BERT, could properly correct the\ncoreference resolution error. They both failed to\ntrace the reference of “it” to “her sister”. By us-\ning BERT based on self-attention for re-ranking,\nwhich is effective for long distance information,\nour system became better at solving long distance\nerrors; however, there is a room for improvement.\n5 Related Work\nRe-ranking using a language model trained\non large-scale raw data signiﬁcantly improved\nthe results in numerous GEC studies (Junczys-\nDowmunt and Grundkiewicz, 2016; Chollam-\npatt and Ng, 2018a; Grundkiewicz and Junczys-\nDowmunt, 2018; Junczys-Dowmunt et al., 2018;\nZhao et al., 2019). However, their models do not\nexplicitly consider grammatical errors of language\nlearners.\nYannakoudakis et al. (2017) utilized the score\nfrom a GED model as a feature to consider gram-\nmatical errors for re-ranking. Chollampatt and Ng\n(2018b) proposed a neural quality estimator for\nGEC. Their models predict the quality score when\ngiven a source sentence and its corresponding hy-\npothesis. They consider representations of gram-\nmatical errors of learners for re-ranking. However,\ntheir models did not use large-scale raw corpora.\nRei and Søgaard (2018) used a sentence-level\nGED model based on bidirectional long short-term\nmemory (LSTM). The goal of their study was to\npredict the token-level labels on a sentence-level\nusing the attention mechanism for zero-shot se-\nquence labeling.\n211\nKaneko and Komachi (2019) proposed a model\nof applying attention to each layer of BERT for\nGED and achieved state-of-the-art results in word-\nlevel GED tasks. Our BERT model predicts gram-\nmatical quality on a sentence-level for re-ranking.\n6 Conclusion\nIn this paper, we described our TMU system,\nwhich is based on the GEC transformer model\nusing BERT for re-ranking. We evaluated our\nTMU system on the restricted track of the BEA\n2019 GEC shared task. The experimental results\ndemonstrated that using BERT for re-ranking can\nimprove the correction quality.\nIn this work, we only considered the infor-\nmation of the hypothesis sentence. In our fu-\nture work, we will analyze the re-ranker, allowing\nBERT to utilize the information of the source sen-\ntence of the GEC model as well, given both source\nand hypothesis sentences.\n7 Acknowledgments\nWe thank Yangyang Xi of Lang-8, Inc. for kindly\nallowing us to use the Lang-8 learner corpus. This\nwork was partially supported by JSPS Grant-in-\nAid for Scientiﬁc Research (C) Grant Number\nJP19K12099.\nReferences\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching Word Vectors with\nSubword Information. TACL, 5:135–146.\nChristopher Bryant, Mariano Felice, and Ted Briscoe.\n2017. Automatic Annotation and Evaluation of Er-\nror Types for Grammatical Error Correction. In\nACL, pages 793–805, Vancouver, Canada. Associ-\nation for Computational Linguistics.\nShamil Chollampatt and Hwee Tou Ng. 2018a. A Mul-\ntilayer Convolutional Encoder-Decoder Neural Net-\nwork for Grammatical Error Correction. In AAAI,\npages 5755–5762, New Orleans, Louisiana. Associ-\nation for the Advancement of Artiﬁcial Intelligence.\nShamil Chollampatt and Hwee Tou Ng. 2018b. Neu-\nral Quality Estimation of Grammatical Error Correc-\ntion. In EMNLP, pages 2528–2539, Brussels, Bel-\ngium. Association for Computational Linguistics.\nDaniel Dahlmeier and Hwee Tou Ng. 2012. Better\nEvaluation for Grammatical Error Correction. In\nNAACL, pages 568–572, Montreal, Canada. Asso-\nciation for Computational Linguistics.\nDaniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.\n2013. Building a Large Annotated Corpus of\nLearner English: The NUS Corpus of Learner En-\nglish. In BEA, pages 22–31, Atlanta, Georgia. As-\nsociation for Computational Linguistics.\nRobert Dale, Ilya Anisimoff, and George Narroway.\n2012. HOO 2012: A Report on the Preposition\nand Determiner Error Correction Shared Task. In\nBEA, pages 54–62, Montreal, Canada. Association\nfor Computational Linguistics.\nRobert Dale and Adam Kilgarriff. 2011. Helping\nOur Own: The HOO 2011 Pilot Shared Task. In\nENLG, pages 242–249, Nancy, France. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In NAACL, Minneapolis, USA. Asso-\nciation for Computational Linguistics.\nMariano Felice, Christopher Bryant, and Ted Briscoe.\n2016. Automatic Extraction of Learner Errors\nin ESL Sentences Using Linguistically Enhanced\nAlignments. In COLING, pages 825–835, Osaka,\nJapan. The COLING 2016 Organizing Committee.\nTao Ge, Furu Wei, and Ming Zhou. 2018. Fluency\nBoost Learning and Inference for Neural Gram-\nmatical Error Correction. In ACL, pages 1055–\n1065, Melbourne, Australia. Association for Com-\nputational Linguistics.\nSylviane Granger. 1998. The Computer Learner Cor-\npus: A Versatile New Source of Data for SLA Re-\nsearch. pages 3–18. Learner English on Computer.\nRoman Grundkiewicz and Marcin Junczys-Dowmunt.\n2018. Near Human-Level Performance in Gram-\nmatical Error Correction with Hybrid Machine\nTranslation. In NAACL, pages 284–290, New Or-\nleans, Louisiana. Association for Computational\nLinguistics.\nMarcin Junczys-Dowmunt and Roman Grundkiewicz.\n2016. Phrase-based Machine Translation is State-\nof-the-Art for Automatic Grammatical Error Correc-\ntion. In EMNLP, pages 1546–1556, Austin, Texas.\nAssociation for Computational Linguistics.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nShubha Guha, and Kenneth Heaﬁeld. 2018. Ap-\nproaching Neural Grammatical Error Correction as\na Low-Resource Machine Translation Task. In\nNAACL, pages 595–606, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nMasahiro Kaneko and Mamoru Komachi. 2019. Multi-\nHead Multi-Layer Attention to Deep Language Rep-\nresentations for Grammatical Error Detection. In\nCICLing, La Rochelle, France.\n212\nTomoya Mizumoto, Mamoru Komachi, Masaaki Na-\ngata, and Yuji Matsumoto. 2011. Mining Revi-\nsion Log of Language Learning SNS for Auto-\nmated Japanese Error Correction of Second Lan-\nguage Learners. In IJCNLP, pages 147–155, Chi-\nang Mai, Thailand. Asian Federation of Natural Lan-\nguage Processing.\nHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. 2014. The CoNLL-2014 Shared Task\non Grammatical Error Correction. In CoNLL, pages\n1–14, Baltimore, Maryland. Association for Compu-\ntational Linguistics.\nHwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian\nHadiwinoto, and Joel Tetreault. 2013. The CoNLL-\n2013 Shared Task on Grammatical Error Correction.\nIn CoNLL, pages 1–12, Soﬁa, Bulgaria. Association\nfor Computational Linguistics.\nFranz Josef Och. 2003. Minimum Error Rate Training\nin Statistical Machine Translation. In ACL, pages\n160–167, Sapporo, Japan. Association for Compu-\ntational Linguistics.\nMarek Rei and Anders Søgaard. 2018. Zero-Shot\nSequence Labeling: Transferring Knowledge from\nSentences to Tokens. In NAACL, pages 293–302,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NIPS, pages 5998–6008. Curran As-\nsociates, Inc.\nHelen Yannakoudakis, Øistein E. Andersen, Ardeshir\nGeranpayeh, Ted Briscoe, and Diane Nicholls. 2018.\nDeveloping an Automated Writing Placement Sys-\ntem for ESL Learners. pages 251–267. Applied\nMeasurement in Education 31:3.\nHelen Yannakoudakis, Ted Briscoe, and Ben Medlock.\n2011. A New Dataset and Method for Automatically\nGrading ESOL Texts. In NAACL, pages 180–189,\nPortland, Oregon, USA. Association for Computa-\ntional Linguistics.\nHelen Yannakoudakis, Marek Rei, Øistein E. Ander-\nsen, and Zheng Yuan. 2017. Neural Sequence-\nLabelling Models for Grammatical Error Correction.\nIn EMNLP, pages 2795–2806, Copenhagen, Den-\nmark. Association for Computational Linguistics.\nWei Zhao, Liang Wang, Kewei Shen, Ruoyu Jia, and\nJingming Liu. 2019. Improving Grammatical Error\nCorrection via Pre-Training a Copy-Augmented Ar-\nchitecture with Unlabeled Data. In NAACL, Min-\nneapolis, USA. Association for Computational Lin-\nguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8409463167190552
    },
    {
      "name": "Transformer",
      "score": 0.7776361107826233
    },
    {
      "name": "Natural language processing",
      "score": 0.6877356767654419
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6762529611587524
    },
    {
      "name": "Sentence",
      "score": 0.6554635167121887
    },
    {
      "name": "Language model",
      "score": 0.6238136291503906
    },
    {
      "name": "Naturalness",
      "score": 0.6201600432395935
    },
    {
      "name": "Encoder",
      "score": 0.5735867023468018
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.501953125
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}