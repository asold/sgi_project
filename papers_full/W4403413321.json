{
  "title": "Are Large Language Models a Threat to Programming Platforms? An Exploratory Study",
  "url": "https://openalex.org/W4403413321",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Billah, Md Mustakim",
      "affiliations": [
        "University of Saskatchewan"
      ]
    },
    {
      "id": null,
      "name": "Roy, Palash Ranjan",
      "affiliations": [
        "University of Saskatchewan"
      ]
    },
    {
      "id": "https://openalex.org/A4287481122",
      "name": "Codabux, Zadia",
      "affiliations": [
        "University of Saskatchewan"
      ]
    },
    {
      "id": "https://openalex.org/A3041162590",
      "name": "Roy, Banani",
      "affiliations": [
        "University of Saskatchewan"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4389544307",
    "https://openalex.org/W3011584400",
    "https://openalex.org/W4211263275",
    "https://openalex.org/W6903968576",
    "https://openalex.org/W4391386032",
    "https://openalex.org/W3166421076",
    "https://openalex.org/W4312900476",
    "https://openalex.org/W6811216112",
    "https://openalex.org/W4395029328",
    "https://openalex.org/W6851275496",
    "https://openalex.org/W4383196665",
    "https://openalex.org/W4312438588",
    "https://openalex.org/W3166295544",
    "https://openalex.org/W4283321056",
    "https://openalex.org/W4393034946",
    "https://openalex.org/W4281392476",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W2767123881",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4366316542",
    "https://openalex.org/W3122283993"
  ],
  "abstract": "Competitive programming platforms like LeetCode, Codeforces, and HackerRank\\nevaluate programming skills, often used by recruiters for screening. With the\\nrise of advanced Large Language Models (LLMs) such as ChatGPT, Gemini, and Meta\\nAI, their problem-solving ability on these platforms needs assessment. This\\nstudy explores LLMs' ability to tackle diverse programming challenges across\\nplatforms with varying difficulty, offering insights into their real-time and\\noffline performance and comparing them with human programmers.\\n We tested 98 problems from LeetCode, 126 from Codeforces, covering 15\\ncategories. Nine online contests from Codeforces and LeetCode were conducted,\\nalong with two certification tests on HackerRank, to assess real-time\\nperformance. Prompts and feedback mechanisms were used to guide LLMs, and\\ncorrelations were explored across different scenarios.\\n LLMs, like ChatGPT (71.43% success on LeetCode), excelled in LeetCode and\\nHackerRank certifications but struggled in virtual contests, particularly on\\nCodeforces. They performed better than users in LeetCode archives, excelling in\\ntime and memory efficiency but underperforming in harder Codeforces contests.\\nWhile not immediately threatening, LLMs performance on these platforms is\\nconcerning, and future improvements will need addressing.\\n",
  "full_text": "Are Large Language Models a Threat to Programming Platforms?\nAn Exploratory Study\nMd Mustakim Billah\nUniversity of Saskatchewan\nCanada\nmustakim.billah@usask.ca\nPalash Ranjan Roy\nUniversity of Saskatchewan\nCanada\npalash.roy@usask.ca\nZadia Codabux\nUniversity of Saskatchewan\nCanada\nzadiacodabux@ieee.org\nBanani Roy\nUniversity of Saskatchewan\nCanada\nbanani.roy@usask.ca\nAbstract\nBackground:Competitive programming platforms such as Leet-\nCode, Codeforces, and HackerRank provide challenges to evaluate\nprogramming skills. Technical recruiters frequently utilize these\nplatforms as a criterion for screening resumes. With the recent\nadvent of advanced Large Language Models (LLMs) like ChatGPT,\nGemini, and Meta AI, there is a need to assess their problem-solving\nability on the programming platforms.Aims:This study aims to\nassess LLMs’ capability to solve diverse programming challenges\nacross programming platforms with varying difficulty levels, pro-\nviding insights into their performance in real-time and offline sce-\nnarios, comparing them to human programmers, and identifying\npotential threats to established norms in programming platforms.\nMethod:This study utilized 98 problems from LeetCode and 126\nfrom Codeforces, covering 15 categories and varying difficulty lev-\nels. Then, we participated in nine online contests from Codeforces\nand LeetCode. Finally, two certification tests were attempted on\nHackerRank to gain insights into LLMs’ real-time performance.\nPrompts were used to guide LLMs in solving problems, and iter-\native feedback mechanisms were employed. We also tried tofind\nany possible correlation among the LLMs in different scenarios.Re-\nsults:LLMs generally achieved higher success rates on LeetCode\n(e.g., ChatGPT at 71.43%) but faced challenges on Codeforces. While\nexcelling in HackerRank certifications, they struggled in virtual\ncontests, especially on Codeforces. Despite diverse performance\ntrends, ChatGPT consistently performed well across categories, yet\nall LLMs struggled with harder problems and lower acceptance\nrates. In LeetCode archive problems, LLMs generally outperformed\nusers in time efficiency and memory usage but exhibited moderate\nperformance in live contests, particularly in harder Codeforces con-\ntests compared to humans.Conclusions:While not necessarily\na threat, the performance of LLMs on programming platforms is\nindeed a cause for concern. With the prospect of more efficient\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non thefirst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nESEM ’24, October 24–25, 2024, Barcelona, Spain\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-1047-6/24/10\nhttps://doi.org/10.1145/3674805.3686689\nmodels emerging in the future, programming platforms need to\naddress this issue promptly.\nCCS Concepts\n•Software and its engineering → Empirical software valida-\ntion; •Computing methodologies → Natural language genera-\ntion.\nKeywords\nCode Generation, LLMs, ChatGPT, Gemini, Meta AI, Competitive\nProgramming\nACM Reference Format:\nMd Mustakim Billah, Palash Ranjan Roy, Zadia Codabux, and Banani Roy.\n2024. Are Large Language Models a Threat to Programming Platforms?\nAn Exploratory Study. InProceedings of the 18th ACM / IEEE International\nSymposium on Empirical Software Engineering and Measurement (ESEM ’24),\nOctober 24–25, 2024, Barcelona, Spain.ACM, New York, NY, USA, 10 pages.\nhttps://doi.org/10.1145/3674805.3686689\n1 Introduction\nProgramming platforms, also known as online judges [ 49], such\nas Codeforces1, LeetCode 2, and HackerRank 3, are widely used as\ntools for developing and demonstrating programming skills. Suc-\ncess on these platforms often leads to recognition within the pro-\ngramming community, as programmers earn scores based on their\nperformance [3]. Moreover, technical recruiters, including top com-\npanies, frequently use competitive programming as a benchmark\nto evaluate candidates’ suitability for software engineering posi-\ntions [ 15, 28]. Given the time-consuming nature of manual resume\nscreening, recruiters rely on automatic techniques [ 18, 36] that\nprioritize candidates’ online programming activities to expedite the\nhiring process. In academia, high performance and contributions on\nthese platforms can significantly influence author or study rankings\n[38], showcasing expertise and innovation in thefield.\nIn recent years, Large Language Models (LLMs) have demon-\nstrated promising performance in code generation [ 2, 10, 23, 24, 37,\n46]. Additionally, companies such as OpenAI, Google, and Meta\nhave released powerful conversational LLMs, including ChatGPT 4,\n1https://codeforces.com\n2https://leetcode.com\n3https://www.hackerrank.com\n4https://chat.openai.com\nESEM ’24, October 24–25, 2024, Barcelona, Spain Mustakim et al.\nGemini5, and Meta AI 6, which introduce a new potential for dishon-\nesty in programming challenges on online platforms. This could\npotentially mislead recruiters seeking skilled employees for their\ncompanies in resume screening. Misjudgment in recruitment can\nsignificantly hamper the operations and success of a software com-\npany. This can lead to a range of negative consequences, including\ndecreased productivity, increased turnover [ 35], and a decline in\noverall company morale.\nSeveral studies have explored the problem-solving capabilities\nof LLMs on programming platforms [ 1, 7, 29, 40]. However, these\nstudies often concentrate on assessing a single platform, typically\nLeetCode, or a single LLM, frequently ChatGPT, and they tend to\nfocus on specific programming challenges. We contend that these\nfindings may not generalize to other scenarios, as the capabilities\nof the same LLM can vary significantly across different platforms.\nSimilarly, platforms may yield different results when tested with\ndifferent LLMs. Therefore, conducting comprehensive research that\nencompasses a diverse range of programming challenges across\nmultiple online platforms and evaluating the impact of LLMs on\nthese platforms will be beneficial for both platform maintainers\nand technical recruiters.\nIn this study, we investigated three types of programming chal-\nlenges across multiple programming platforms using three conver-\nsational LLMs – ChatGPT, Gemini [ 43], and Meta AI [ 45]. Initially,\nwe compiled a dataset consisting of 224 programming problems\nspanning 15 categories, sourced from online judge archives. This\ndataset comprised 98 problems from LeetCode and 126 from Code-\nforces. Subsequently, we engaged in nine virtual contests, encom-\npassing a total of 49 problems on Codeforces and LeetCode, to\nreplicate real-time programming conditions. Additionally, we com-\npleted two certification tests on HackerRank, each comprising four\nproblems in real-time settings.\nTo the best of our knowledge, this is thefirst attempt to explore\nthe impact of different LLMs on programming platforms by analyz-\ning varied programming challenges across different platforms. Our\nkey contributions include the following:\n(1) Evaluation of LLMs’ problem-solving abilities across a range\nof programming challenges from judge archives and online\ncontests.\n(2) Analysis of variations in LLMs’ performance across differ-\nent problem dimensions, such as category, difficulty level,\nacceptance rate, and programming language.\n(3) Comparison of LLMs’ problem-solving performance with\nthat of human programmers, with implications for online\njudging systems and recruiters.\n(4) Comparative analysis of LLMs’ performance, examining sig-\nnificant differences in problem-solving abilities across vari-\nous LLMs.\n(5) Provision of a comprehensive replication package 7.\nThe rest of the paper is organized as follows. Section 2 provides\nbackground information for our study. Section 3 describes related\nworks. Section 4 outlines the methodology. Sections 5 and 6 present,\nanalyze and discuss ourfindings. Section 7 lists the implications of\n5https://gemini.google.com/app\n6https://www.meta.ai/\n7https://github.com/srlabUsask/LLM_Threat_Programming_Platforms\nour study. Section 8 states the limitations of our study. Finally, Sec-\ntion 9 concludes the study and suggests future research directions.\n2 Background\n2.1 LLMs for Code Generation\nLLMs have seen rapid development, with models like CodeGen [ 32],\nStarCoder [ 21], WizardCoder [ 25], CodeT5 [ 48], CodeT5+ [ 47], and\nIncoder [ 12] specifically designed for code generation. However,\nthese models lack plug-and-play features, requiring a certain level\nof expertise for downloading and utilization. Conversely, some\nLLMs, such as GitHub Copilot 8 and Amazon CodeWhisperer 9, are\nseamlessly integrated into developers’ IDEs as code assistants.\nIn this study, we focus on conversational LLMs only, which are\neasily accessible without much expertise. By conversational LLMs,\nwe refer to online browser versions like ChatGPT, Gemini, and\nMeta AI, which enable users to interact with them through prompts\nand foster conversational interactions.\n2.2 Reasoning With LLMs\nSolving complex problems on programming platforms necessitates\nhuman reasoning and logical analysis. This involves devising logical\npathways while considering various scenarios and learning from er-\nror feedback provided by the platforms. Wei et al. [ 50] demonstrated\nthat LLMs can effectively address challenging reasoning tasks by\nsystematically guiding their logic through chain-of-thought prompt-\ning. This approach has notably enhanced mathematical problem-\nsolving skills at the high school level.\nIn our problem-solving approach (Section 4.4), we adopt a similar\nchain-of-thought prompting strategy with LLMs, providing them\nwith error feedback to facilitate improved reasoning.\n2.3 Online Judge\nProgramming platforms like Codeforces, LeetCode, and Hacker-\nRank, also known as online judges, assess submitted programming\nsolutions. These platforms host various problem types, each cate-\ngorized under a specific problem type and requiring different skill\nsets for effective resolution. For instance, the \"Greedy\" category\ninvolves developing algorithms based on locally optimal choices at\neach stage.\nThe online judge-checking system thoroughly evaluates sub-\nmitted solutions to verify correctness and adherence to problem\nrequirements. The system issues a verdict, indicating acceptance or\nrejection based on specific criteria. Verdicts typically classify errors\ninto four types: Time Limit Exceeded and Memory Limit Exceeded\noccur when solutions exceed time or memory constraints, Com-\npilation Error results from code compilation issues, and Runtime\nError arises during code execution.\n3 Related Work\n3.1 ChatGPT and LeetCode Based Studies\nPrevious studies primarily concentrated on using ChatGPT to solve\nproblems from LeetCode and assessing its performance. Sakib et\nal. [ 40] tried to solve 128 problems of LeetCode. They selected ten\n8https://github.com/features/copilot\n9https://aws.amazon.com/fr/codewhisperer/\nAre Large Language Models a Threat to Programming Platforms? An Exploratory Study ESEM ’24, October 24–25, 2024, Barcelona, Spain\ndifferent types of problems and attempted to solve them twice. They\nreported that ChatGPT solved 71.9% of their problems and could\nimprove solutions for the unsolved problems up to 36.7% in the\nsecond attempt. Nascimento et al. [ 29] tried one LeetCode contest\nand compared their results with 42 participants. They concluded\nthat ChatGPT improves the performance of easy and medium-level\nproblems compared to novice contest programmers. Ekedahl et al.\n[9] also tried 90 LeetCode problems of ten types by GPT-4 version\nand tried for a maximum of three times if a solution failed. They\nshowed that ChatGPT excels in simpler and medium-level pro-\ngramming but struggles with harder challenges, showing declining\naccuracy.\n3.2 Comparative Studies\nSeveral comparative studies [ 1, 4, 41, 42] have examined ChatGPT,\nBard (now known as Gemini), and Llama across various dimen-\nsions. Ahmed et al. [ 1] specifically compared ChatGPT and Bard,\nidentifying a significant shared limitation termed \"Artificial Hal-\nlucinations. \" However, they observed that GPT-4 produces more\naccurate solutions with fewer hallucinations compared to Bard.\nNikolaidis et al. [ 33] focused on 50 LeetCode problems, con-\ncluding that ChatGPT and Copilot excel in providing Java and\nPython solutions but exhibit decreased performance with C lan-\nguage. Similarly, Hans et al. [ 14] discovered, through experiments\nwith 80 medium and 60 easy LeetCode problems, that ChatGPT\nsignificantly outperforms Bard.\nRecently, Coignion et al. [ 7] evaluated 18 LLMs for code gen-\neration efficiency using a LeetCode dataset, revealing comparable\nperformance with human-crafted solutions and indicating poten-\ntial for future optimizations. research conducted by Nascimento\net al. [ 30] and Idrisov et al. [ 17] has compared LLMs with human\nperformance, highlighting certain shortcomings of LLMs.\n3.3 Generative Models\nLi et al. [ 22] introduced AlphaCode, a deep learning-based code\ngeneration system, which achieved an average ranking within the\ntop 54.3% in simulated evaluations during recent Codeforces pro-\ngramming competitions. OpenAI’s Codex [ 11] and Github’s Copilot\n[8] are also capable code generation models. Chen et al. [ 6] eval-\nuated Codex and found that it has a strong performance for easy\ninterview problems. Nguyen et al. [ 31] evaluated the suggested\ncodes of GitHub Copilot with LeetCode and found that it achieved\nat most 57% correctness score.\nIn a case study, Lertbanjongngam et al. [ 20] found that Alpha-\nCode’s performance is comparable to or sometimes worse than\nhumans regarding execution time and memory usage. They also\nnoted that AlphaCode often utilizes too many nested loops and\nunnecessary variable declarations for high-difficulty problems. Fur-\nthermore, they noted that AlphaCode is capable of generating entire\nprograms from lengthy natural language descriptions, distinguish-\ning it from Codex and GitHub Copilot.\nSummary:Unlike previous research, our study seeks to offer\ncomprehensive insights to both programming platform maintain-\ners and technical recruiters. We achieve this by analyzing different\nonline judges across diverse problem categories. Our investiga-\ntion spans various challenges, such as solving archived problems,\nparticipating in real-time virtual contests, and engaging in certifi-\ncation programs. Our sole aim is to evaluate the impact of LLMs\non well-known programming platforms.\n4 Methodology\nThis section outlines the study’s approach, detailing its aims, re-\nsearch inquiries, data collection, and analysis procedures.\n4.1 Goal\nThe goal of the study is described using the Goal-Question-Metric\ntechnique [5] as follows:\nPurpose:To evaluate\nIssue:The impact of LLMs\nObject:On programming platforms\nViewpoint:From the perspective of practitioners.\n4.2 Research Questions\nBased on our goal, we derive the following Research Questions\n(RQs):\nRQ1: How successfully can LLMs solve a diverse set of\nprogramming challenges?\nTo evaluate LLMs’ problem-solving capabilities, we aim to assess\ntheir performance across various types of programming challenges.\nThis research question will provide insights into how LLMs perform\nwhen tackling problems of different complexities in both offline and\nonline environments. A high success rate could indicate potential\nvulnerabilities in online judges, as people might exploit these judges\nusing LLMs. Conversely, a lower success rate would demonstrate\nthat online judges remain robust against LLMs.\nRQ2: How does the performance of LLMs vary across dif-\nferent dimensions?\nWe hypothesize that LLMs will exhibit varied performance based\non problem category, difficulty level, acceptance rate, and program-\nming language. Additionally, LLMs may differ in their performance\nfrom one another.\nRQ3: How does the problem-solving performance of LLMs\ncompare to that of human programmers?\nInvestigating this question will offer insights into how the problem-\nsolving abilities of each LLM stack up against those of human pro-\ngrammers, thereby highlighting potential implications for online\njudging systems.\n4.3 Data Collection\n4.3.1 Problem Category Selection.As of April 26, 2024, LeetCode’s\ncollection comprises 3125 problems spread across 71 distinct cate-\ngories, while the Codeforces archive contains 9460 problems cate-\ngorized into 37 distinct types. For this study, we have selected 15\npopular10 categories that are present on both platforms. These cat-\negories include String (S), Two Pointers (TP), Math (M), Greedy (G),\nBinary Search (BS), Combinatorics (C), Depth-First Search (DFS), Di-\nvide and Conquer (DC), Dynamic Programming (DP), Matrix (MA),\nSorting (SO), Number Theory (N), Shortest Path (SP), Probability\nand Statistics (PS), and Tree (T).\n10https://en.wikipedia.org/wiki/Competitive_programming#Overview\nESEM ’24, October 24–25, 2024, Barcelona, Spain Mustakim et al.\n4.3.2 Problems from Judge Archives.In LeetCode, problems are\ncategorized into three difficulty levels: Easy, Medium, and Hard.\nEach problem also has an associated acceptance rate, indicating\nthe percentage of times the problem was solved compared to the\ntotal submissions by users. Our dataset was structured based on\nthese difficulty levels and acceptance rates, following the approach\noutlined by Sakib et al. [ 40]. They defined three acceptance range\ntiers: High (>70%), Medium (>=30% and <=70%), and Low (<30%).\nThe goal was to include at least one problem from each of the\n15 categories, covering different difficulty levels and acceptance\nranges. By following this methodology, we compiled a set of 98\nproblems, as depicted in TABLE 1.\nCodeforces does not organize problems by difficulty levels or\ndisplay acceptance rates. Instead, it assigns ratings 11 to indicate\ncomplexity and solution frequency. In our study, we classified rat-\nings into Easy (800-1200), Medium (1201-1700), and Hard (exceeding\n1700). To identify problems with High, Medium, and Low accep-\ntance rates, we sorted them based on successful solutions and chose\none problem per rate category. This process resulted in a total of\n126 problems (TABLE 1) selected for our study.\n4.3.3 Online Contests.In addition to the problems obtained from\njudge archives, we actively participated in the latest online contests,\nparticularly on platforms like Codeforces and LeetCode. Codeforces\nhosts various types of contests, ranging from Division 1 (the most\nchallenging) to Division 4 (less challenging), along with unrated Ed-\nucational contests. We selectedfive recent contests from Codeforces,\nrepresenting each contest type, and competed in them virtually.\nVirtual contests provide a real-time competition experience against\nparticipants worldwide within a timed setting.\nMoreover, we engaged in LeetCode’s weekly and biweekly con-\ntests, totaling four recent contests, with two for each contest type,\nthrough virtual participation. This effort resulted in our involve-\nment in a total of nine online contests for our study.\n4.3.4 Certification Tests.We opted for HackerRank for certifica-\ntion tests, considering its well-established reputation as a reliable\nplatform for assessing problem-solving skills through certifications.\nThese tests simulate real-time contests, presenting participants with\na series of problems to solve within specified time constraints. How-\never, unlike our approach with Codeforces and LeetCode, where\nwe selected problems from their archives, we did not directly solve\nany problems from HackerRank’s archive for this study. Instead,\nwe undertook two problem-solving-based certification tests on the\nplatform. Further details about these tests are provided in TABLE 2.\n4.4 Solving Problems\nWe utilized open-source, free conversational language models such\nas ChatGPT-3.5, Gemini 1.0 Pro, and Meta AI for our study. To\nconduct our experiment using three different conversational LLMs,\nwe require a standardized prompt structure. A prompt serves as a\nset of instructions given to an LLM, programming it by customiza-\ntion and/or enhancing its capabilities [ 51]. Following the approach\noutlined in a recent study [ 16], we closely followed the guidelines 12\nprovided by OpenAI, as detailed in their documentation on prompt\n11https://en.wikipedia.org/wiki/Codeforces#Rating_system\n12https://platform.openai.com/docs/guides/prompt-engineering\nengineering to optimize the prompts to get better results from the\nLLMs.\nEach problem on these platforms comes with its own constraints\nregarding memory usage and execution time. These constraints\nestablish the parameters within which a solution must operate. This\nmeans that even if a solution is technically correct, it will not be\naccepted if it exceeds the specified time limit or consumes more\nmemory than allowed. The prompts for solving these problems\nhad to be formatted differently based on the specific requirements\nof each online judge. For instance, on Codeforces, the problem\ndescriptions typically start by outlining the time and memory limit\nconstraints, followed by a detailed problem description. Solutions\non this platform are not required to be written within predefined\nfunctions; instead, a complete solution is expected from start to\nfinish. An example of such a prompt is illustrated in the upper part\nof Figure 1.\nConversely, platforms like LeetCode and HackerRank necessi-\ntate solutions to be written within specific predefined functions\nprovided by the platform. Additionally, these platforms incorporate\ntime and memory limit constraints within the problem description\nitself. This difference in approach results in distinct prompt designs,\nas depicted in the lower part of Figure 1 compared to Codeforces.\nWe chose C++ as the primary language for our study because com-\npetitive programmers commonly use it for problem-solving [16].\nCodeforces\nWrite a C++ solution which will not exceedt\nsecond time limit andmmegabytes memory\nlimit per test, for the following problem:\n“Problem Description”.\nLeetCode & HackerRank\n“Problem Description”, Now complete the\nbelow C++ function for the stated problem\nabove, “Function Body”.\nFigure 1: Prompt used for different platforms\nAfter receiving a solution from an LLM, we submitted it to the rel-\nevant judges to determine whether it would be accepted or rejected.\nRejection could occur for various reasons such as Wrong Answer,\nTime Limit Exceeded, Memory Limit Exceeded, Compilation Error,\nor Runtime Error. If rejected, we provided feedback to the LLMs, as\noutlined in Madaan et al. [ 26], which included specific error details\nto help them address the issues. Following this feedback, the LLMs\nthen generated an alternative solution, incorporating the insights\ngained from our guidance.\nThe workflow of our methodology is illustrated in Figure 2. We\niteratively employed few-shot prompting to tackle a problem. If an\nLLM consistently produced incorrect solutions for k consecutive\nattempts, we stopped the process. In our experiment, we set the\nAre Large Language Models a Threat to Programming Platforms? An Exploratory Study ESEM ’24, October 24–25, 2024, Barcelona, Spain\nTable 1: Cross Platform Dataset Across Problem Domains and Difficulty Levels\nLeetCode | Codeforces\nDifficulty Acceptance S TP M G BS C DFS DC DP MA SO N SP PS T\nHigh 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 0 | 1 0 | 1 1 | 1\nMedium 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 0 | 1 1 | 1 1 | 0 1 | 1 1 | 0 1 | 1 1 | 1 0 | 0 0 | 1 1 | 1Easy\nLow 1 | 1 0 | 1 1 | 1 1 | 1 0 | 1 0 | 1 0 | 1 0 | 0 0 | 1 0 | 0 0 | 1 0 | 1 0 | 1 0 | 1 0 | 0\nHigh 1 | 1 1 | 1 1 | 1 1 | 1 0 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 0 | 0 0 | 0 0 | 0 1 | 1\nMedium 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1Medium\nLow 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 0 | 1 1 | 1 0 | 1 1 | 1 0 | 1 1 | 1 1 | 1 1 | 1 0 | 1 0 | 1\nHigh 1 | 1 0 | 1 0 | 1 1 | 1 1 | 1 0 | 1 1 | 1 0 | 1 0 | 1 1 | 1 1 | 1 0 | 1 1 | 1 0 | 1 0 | 1\nMedium 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1Hard\nLow 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 0 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 1 | 1 0 | 1 1 | 1\nTotal Problems 9 | 9 7 | 9 8 | 9 9 | 9 7 | 9 5 | 9 7 | 9 6 | 7 7 | 9 7 | 7 8 | 9 6 | 8 5 | 7 2 | 8 5 | 8\nTable 2: HackerRank Data\nCertification Name Duration Problems\nProblem Solving (Basic) 150 minutes 2\nProblem Solving (Intermediate) 150 minutes 2\nvalue of k to 5, indicating that we attempted each problemfive\ntimes before discontinuing the process.\nFor both online and offline problem-solving, we directly copied\nthe problem descriptions from the online judges and organized them\naccording to our predefined prompts before inputting them into the\nLLMs. Once a solution, comprising both explanations and code was\ngenerated, we extracted our desired implementation. Furthermore,\nwe post-processed error information to streamline the data before\nre-prompting the LLMs.\nConcerning HackerRank certification tests, we faced limitations\nwhen attempting to copy the problem descriptions directly from\nthe platform. To overcome this obstacle, we employed a third-party\ntool13 for image-to-text conversion. This involved capturing the\nproblem description image, converting it to text using the tool, and\nsubsequently adapting it tofit our prompt structure.\n5 Results\n5.1 RQ 1: Success Rate\nOut of the 98 problems selected from the LeetCode archive, Chat-\nGPT successfully solved 70, accounting for 71.43% of the total prob-\nlems. Meta AI’s success rate was 58.16%, solving 57 problems, while\nGemini solved 67, achieving a success rate of 68.37%.\nFor the 126 problems from Codeforces, ChatGPT, Meta AI, and\nGemini solved 34, 21, and 10 problems, respectively, resulting in\nsuccess rates of 26.98%, 16.67%, and 7.94%. ChatGPT outperformed\nboth Gemini and Meta AI across both platforms, while Gemini and\nMeta AI showed varying performance depending on the platform.\nThe results of our experiment in online contests are shown in\nTABLE 3. We aimed to cover all contest types by participating\nin nine virtual contests on LeetCode and Codeforces. We have\n13https://www.imagetotext.info/\nPrompt to LLMs\nSolution\nSubmit\nVerdict\nAccepted\nError\nRetry With Error Information\nAfter k Tries\nFinish\nGemini ChatGPT Meta AI\nFigure 2: Workflow of solving problems\nhighlighted the best results achieved out of the three LLMs we used.\nAcross 9 online contests, there were 49 problems in total, but the\nESEM ’24, October 24–25, 2024, Barcelona, Spain Mustakim et al.\nTable 3: Codeforces (Cf) and LeetCode (LC) virtual contests’\nresults\nContests Solved Standing\nCf - Educational 0 / 6\nCf - Division 4 1 / 7 22134 / 60343\nCf - Division 3 1 / 8 16324 / 53980\nCf - Division 2 0 / 6\nCf - Division 1 0 / 6\nLC - Weekly 1 1 / 4 10383 / 22688\nLC - Biweekly 1 2 / 4 9999 / 25287\nLC - Weekly 2 1 / 4 15801 / 29736\nLC - Biweekly 2 2 / 4 12834 / 26241\nLLMs were only able to solve 8 of them. To break it down further,\nthey solved only 2 out of 33 Codeforces problems and 6 out of 16\nLeetCode problems.\nWe undertook the two certification tests on HackerRank as de-\ntailed in TABLE 2, utilizing ChatGPT, Gemini, and Meta AI. All\nthree LLMs successfully obtained Problem-Solving certifications at\nthe Basic and Intermediate levels. Additionally, these certificates\nhave been included in the replication package.\nSummary RQ 1:On average, LLMs demonstrated\nhigher success rates on LeetCode, with ChatGPT\nachieving a rate of 71.43%, and a moderate success rate\non Codeforces at 26.98% (ChatGPT). Although LLMs\nexcelled in HackerRank certifications, they encoun-\ntered difficulties during virtual contests, particularly\non Codeforces.\n5.2 RQ 2: Performance Variation\nFigure 3 illustrates the performance of various LLMs across 15\nproblem categories. It is evident that ChatGPT consistently achieved\nhigher success rates in thefirst 11 categories. Conversely, Meta AI\ndemonstrated superior performance in Number Theory (N) and\nProbability and Statistics (PS). On average, Gemini outperformed\nMeta AI. Notably, Meta AI encountered difficulties in six categories,\nwhere it registered the lowest success rates.\nTABLE 4 illustrates how problems solved by difficulty and ac-\nceptance rate were distributed among the three LLMs. ChatGPT,\nGemini, and Meta showed their strongest performance when the\nproblems were easy and had a high acceptance rate. However, their\nperformance noticeably declined as the difficulty of the problems\nincreased to medium and hard, and the acceptance rate decreased\nto medium and low.\nWe conducted an experiment to see if the programming lan-\nguage used affects an LLM’s problem-solving ability. We tackled\n98 LeetCode problems from TABLE 1 using ChatGPT, using both\nC++ and Python separately. We followed identical procedures for\nboth of the programming languages. The results showed that 70\nproblems were solved successfully using C++ and 67 with Python.\nWe then conducted a chi-square [ 27] test of independence, with the\nnull hypothesis being that programming language does not impact\nLLM performance. The resulting p-value was 0.76, which exceeds\nthe significance level of 0.05. This means we failed to reject the\nnull hypothesis, indicating that LLMs demonstrate no significant\ndifference in performance when employing different programming\nlanguages.\nSummary RQ2:LLMs display diverse performance\ntrends. ChatGPT consistently succeeds across cate-\ngories, Meta AI shines in Number Theory and Prob-\nability, and Gemini generally outperforms Meta AI.\nHowever, all LLMs struggle with harder problems\nand lower acceptance rates. Statistical tests confirmed\nthat there is no noticeable difference in performance\nwhen employing different programming languages\nwith LLMs for problem-solving.\n5.3 RQ 3: Human Compatibility\nLeetCode offers valuable insights into participants’ accepted so-\nlutions, providing a comparative analysis of their performance\nregarding time and memory constraints. Figure 4 illustrates the\npercentage by which LLMs’ performance exceeds that of users in\nterms of time efficiency and memory usage across the 98 problems\ndetailed in TABLE 1.\nSpecifically, Gemini exhibited higher time efficiency, outperform-\ning 65.79% of the users. Conversely, Meta AI showcased superior\nmemory usage, surpassing 54.29% of the users, outperforming both\nGemini and LeetCode. On average, LLMs outperformed 63.10% of\nusers in time efficiency and 51.08% of users in memory usage.\nHowever, unlike LeetCode, Codeforces does not offer comparable\ninsights. Instead, it only provides information on the time taken to\nsolve a problem and the memory it consumes.\nWhen we compared the performance of LLMs with that of hu-\nmans in the online contests presented in TABLE 3, which we partic-\nipated in through LLMs, we observed varied results across different\ntypes of contests. We collected the standings of LLMs from the\nvirtual contests they participated in and calculated the percentage\nby which LLMs outperformed other participants in those contests.\nLLMs performed moderately well in the weekly and biweekly con-\ntests hosted by LeetCode, where they outperformed an average of\n53.7% of users. However, their performance was poor in the more\nchallenging Codeforces contests (Division 1 and Division 2). They\nachieved better results in less difficult contests such as Division 4\n(63.32%) and Division 3 (69.76%), but struggled in the Educational\ncontest.\nSummary RQ3:LLMs generally outperformed a sig-\nnificant portion of users, demonstrating strong per-\nformance in both time efficiency and memory us-\nage when tested against problems from the LeetCode\narchive. However, in live contests, LLMs exhibited\nmoderate performance in LeetCode contests but en-\ncountered challenges in harder Codeforces contests\nwhen compared to humans.\nAre Large Language Models a Threat to Programming Platforms? An Exploratory Study ESEM ’24, October 24–25, 2024, Barcelona, Spain\n������\n� ���� �������\n���� �� ����\n����������� �� �������������\n������� ��������� �� ������������������\n��������� � ���������\n������ �������\n������������� ���������� ���\n� � ������������������������\n� � ��\n� � ����������������\n�\n��\n��\n��\n��\n��\n����������� �������\n�����\n�������\n�������\n������\nFigure 3: Success Rates Across Problem Categories\nTable 4: Difficulty and Acceptance Range Wise Solved Problems by ChatGPT (C), Gemini (G), and Meta AI (M)\nDifficulty Acceptance (C) Acceptance (G) Acceptance (M)\nHigh Medium Low High Medium Low High Medium Low\nEasy 23 17 2 18 16 3 22 17 3\nMedium 18 18 5 8 12 7 10 14 2\nHard 8 11 1 3 8 2 4 5 1\n� �� �� �� �� �� ��\n� �� �������\n�������\n������\n�������\n����\n� ���������\n������������\nFigure 4: Comparison with users’ performance with LLMs in\nterms of time and memory usage of LeetCode\n6 Discussion\n6.1 Action is Needed\nGiven the moderate to higher success rates of LLMs in solving\ndiverse programming challenges (RQ 1) and their above-average\nperformance compared to humans (RQ 3), it is imperative for online\nprogramming platforms to strategize how to address this trend. As\nLLMs continue to evolve rapidly with the advent of larger models\nand increased computational capabilities, it is foreseeable that they\nmay emerge as leading performers on platforms such as Codeforces\nand LeetCode in the near future. If programming platforms do not\ntake adequate measures to protect against LLM-related threats, they\nrisk losing trust within the programming community.\nHackerRank’s Certification tests, recognized by the program-\nming community [ 19], prevent users from copying problem descrip-\ntions to deter cheating. However, it is evident from our methodology\n(Subsection 4.4) that obtaining programming certificates listed in\nTABLE 2 can be accomplished quite easily. This exposes a vulner-\nability in the system, indicating insufficient protective measures\nagainst threats related to LLMs across different platforms.\nRecent studies, such as the one by Idialu et al. [ 16], have demon-\nstrated promising outcomes in distinguishing between code au-\nthored by humans and code generated by GPTs. Online program-\nming platforms can adopt similar techniques to determine code\nauthorship. However, their classifier is specifically designed for the\nchallenges posed on CodeChef. Future research endeavors could\nexplore the development of a multi-platform classifier with similar\nESEM ’24, October 24–25, 2024, Barcelona, Spain Mustakim et al.\ncapabilities to counteract the influence of potent LLMs on various\nprogramming platforms.\n6.2 Significance of Feedback\nIn our methodology, we utilized a chain-of-thought prompting\napproach [50] when our initial submission attempts (k=1) were not\naccepted by the online judges. This approach involved an iterative\nprocess where we fed the error details received from the judges\nback into the Language Model Models (LLMs) and made subsequent\nattempts up to a total offive times (k=5).\nFollowing this approach, both ChatGPT and Gemini were able\nto resolve 12.24% of the 98 LeetCode problems, while 4.76% of the\n126 Codeforces problems were resolved by both ChatGPT and Meta\nAI. The process of providing feedback from the online judge to\nthe LLM served as a continuous learning process. It allowed the\nLLM to learn from its mistakes by understanding the specific errors\nencountered, which is consistent with thefindings of Tong et al.\n[44] suggesting that LLMs learn from previous mistakes.\n6.3 Challenges Faced by LLMs\nThe results of RQ 1 and RQ 3 show that all three LLMs encountered\nchallenges in solving problems on Codeforces compared to Leet-\nCode. To delve into the reasons, we analyzed the Division 4 contest\nfrom Codeforces and the Weekly 1 contest from LeetCode (see\nTable 3) to compare the average number of words per problem.\nWe found that the Codeforces contest had an average of 274.14\nwords per problem, while the LeetCode contest had an average of\n164.25 words per problem. This suggests that LLMs might strug-\ngle to understand larger problem descriptions, leading to poorer\nperformance on Codeforces.\n������� ������ �������\n����� ���\n�\n�\n��\n��\n��\n��\n������������� � ������\n�� � ���� ���\n� ���� ������ � ��\n� ������������ ������\n�������������� � ��\n��������������� ������\nFigure 5: Distribution of different errors across LLMs\nTo gain deeper insights into the challenges encountered by LLMs\nduring problem-solving, we provide an analysis of error types across\ndifferent LLMs, as depicted in Figure 5. The distribution illustrates\nthat Gemini and Meta AI encountered the most difficulties in gener-\nating compilable solutions for the problems. Conversely, ChatGPT\nstruggled with producing accurate solutions within the specified\ntime constraints of programming platforms. Despite these chal-\nlenges, all three LLMs demonstrated considerable success in solving\nproblems within the prescribed memory constraints.\nAdditionally, among the 224 problems assessed, 35 included im-\nages in their descriptions to aid programmers in better compre-\nhending the problem. Despite not incorporating these images in\nthe problem descriptions, ChatGPT successfully solved 60% of the\nproblems, while Meta AI managed to solve 42.85% of them. No-\ntably, LLMs demonstrated competence even when confronted with\nproblems featuring images.\n6.4 Number of Attempts\nIn our problem-solving methodology, we attempted each problem\nfive times (k=5). We chose this number based on the belief that it\nrepresents an optimal balance for assessing the impact of provid-\ning feedback to LLMs. Given that our approach involves utilizing\nconversational LLMs, we aimed to avoid relying on paid APIs and\ninstead manually solve problems through copying and pasting. Us-\ning higher values of k would have necessitated significant manual\neffort and time investment. Our decision aligns with previous re-\nsearch [9, 40], which also utilized lower values of k (<=3) in their\nstudies. To verify our assumption, we conducted a pilot study with\na higher k value of 10 during the Division 4 Codeforces contest.\nInterestingly, this adjustment did not alter the results presented in\nTABLE 3.\n7 Implications\n7.1 For Researchers\nOur study’sfindings can be used to investigate LLMs’ shortcom-\nings in complex code generation. Our results reveal that LLMs face\ndifficulties in generating solutions when problems in the hard and\nlow acceptance range are given to them. Researchers associated\nwith developing AI coding assistants like GitHub Copilot can in-\ncorporate ourfindings into their models to make improved coding\nassistant tools that will be able to tackle difficult problem-solving\nscenarios.\n7.2 For Recruiters\nEfforts [ 28] have been made to automatically gather candidates’\nonline programming profile information to facilitate hiring. Our\nstudyfindings indicate that LLMs perform moderately well on\nprogramming platforms and excel in obtaining online certificates\nrelated to programming skills. These insights provide recruiters\nwith a means to verify the credibility of a candidate’s online pro-\ngramming profile. However, while these platforms offer valuable\ninformation, recruiters should supplement their evaluation meth-\nods with interviews, practical assessments, or real-world projects.\nThis holistic approach ensures a thorough understanding of can-\ndidates’ skills, behaviors, and problem-solving abilities, enabling\nrecruiters to make more informed hiring decisions.\n7.3 For Practitioners\nWe conducted a study to assess the effectiveness of various online\nprogramming platforms using multiple LLMs across various pro-\ngramming challenges. This study will help developers associated\nAre Large Language Models a Threat to Programming Platforms? An Exploratory Study ESEM ’24, October 24–25, 2024, Barcelona, Spain\nwith these platforms identify any potential vulnerabilities in their\nsystems and work towards making their platforms more secure\nagainst LLMs. Additionally, ourfindings will enable online judge\nproblem-setters to create programming challenges that are more\nchallenging for LLMs to solve.\n8 Threats to Validity\nAlthough we have diligently strived for accuracy, there is a chance\nthat validity threats could influence the study’s results. Utilizing\nRuneson et al. ’s classification [ 39], we scrutinize multiple factors\nthat might jeopardize the study’s integrity.\n8.1 Construct Validity\nConstruct validity concerns whether our chosen operational mea-\nsures effectively address the main inquiries in our research. In our\ncase, the design of prompts used for solving programming problems\non various online judges might potentially limit LLMs’ ability to\nunderstand programs. This limitation arises from the nature and\nstructure of the prompts, which may not fully represent the com-\nplexity or diversity of real-world problem-solving scenarios. To\naddress this concern, we followed prior research and adhered to\nOpenAI’s guidelines for prompt design (Subsection 4.4).\nTo report the results of our research questions, we used various\nquantitative metrics such as success rate, acceptance rate, time effi-\nciency, and memory efficiency. These metrics are well-established\nand verified by online judges. While not all platforms provide every\nmetric, this did not affect our study’s results, as most quantitative\nmetrics are commonly available across online judges.\n8.2 Internal Validity\nIn this study, we focused exclusively on conversational LLMs. In-\nstead of utilizing the more advanced paid versions of ChatGPT\nand Gemini, such as GPT-4 based ChatGPT Plus 14 and Gemini Ad-\nvanced15, we opted for their free versions. We deliberately chose to\nexclusively use open-source free conversational LLMs, ensuring the\nstudy’s generalizability, given that Meta AI does not offer any paid\nversions. However, this technological decision and assumption may\nnot fully represent the capabilities of LLMs, potentially impacting\nthe study’s generalizability and the accuracy of the conclusions\ndrawn from the experiment. To address this concern, we can refer\nto the work of Hans et al. [ 13], where they compared the coding ca-\npabilities of GPT-3.5 with GPT-4 on LeetCode problems and found\nnegligible differences in their performance.\n8.3 External Validity\nConcerning external validity threats, the concern revolves around\nthe generalizability offindings derived from LLMs’ problem-solving\nbehavior. The non-deterministic nature [ 34] of LLMs, where its so-\nlutions may vary for the same problem, poses a challenge to the\nreliability and generalizability of results. Additionally, selecting\nproblems solely from LeetCode and Codeforces might limit the\nrepresentation of the entire spectrum of programming challenges,\npotentially impacting the broader applicability of the study’sfind-\nings beyond these specific platforms. This limitation could affect\n14https://openai.com/index/chatgpt-plus\n15https://gemini.google.com/advanced\nthe extent to which conclusions drawn from LLMs’ performance\non these platforms can be extrapolated to other problem sets or\nprogramming environments.\n8.4 Conclusion Validity\nRegarding conclusion validity threats, the concerns center around\nthe connection between outcomes and the treatments administered\nin the study. The dataset’s restricted number of problems raises a\npotential concern about the solidity of statistical conclusions. This\nlimitation could impede the generalizability offindings because\na smaller dataset might not sufficiently reflect the wider problem\nlandscape. To address this, we have curated the problems based on\ndiverse problem categories, difficulty levels, and acceptance range\nlevels to compile a more comprehensive dataset.\n9 Conclusion and Future Work\nThis study aimed to evaluate the impact of LLMs on Competitive\nProgramming across diverse platforms through a comprehensive\nanalysis. Our investigation unveiled a nuanced spectrum of results\nacross 15 categories, varying difficulty levels, and acceptance ranges.\nNotably, LLMs demonstrated high accuracy on LeetCode (71.43%)\nyet performed less optimally on Codeforces. While excelling in tests\non platforms like HackerRank, their performance in live Codeforces\ncontests was less favorable. Moreover, We conclude this study by\nhighlighting the need for the programming community to devise\nmeasures to address the potential risks posed by LLMs to the estab-\nlished norms of programming platforms and advise recruiters to\nexercise caution when evaluating candidates.\nFor future exploration, efforts should aim to expand the range of\nplatforms and problem sets analyzed while incorporating a broader\nselection of LLMs.\nAcknowledgments\nThis work was supported in part by the Natural Sciences and Engi-\nneering Research Council of Canada (NSERC) Discovery grants, the\nJohn R. Evans Leaders Fund (JELF) of the Canada Foundation for In-\nnovation (CFI), and the industry-stream NSERC CREATE graduate\nprogram on Software Analytics Research (SOAR) grants.\nReferences\n[1] Imtiaz Ahmed, MashrafiKajol, Uzma Hasan, Partha Protim Datta, Ayon Roy, and\nMd Rokonuzzaman Reza. 2023. Chatgpt vs. bard: A comparative study.UMBC\nStudent Collection(2023).\n[2] Ajmain I. Alam, Palash R. Roy, Farouq Al-Omari, Chanchal K. Roy, Banani Roy,\nand Kevin A. Schneider. 2023. GPTCloneBench: A comprehensive benchmark\nof semantic clones and cross-language clones using GPT-3 model and Semantic-\nCloneBench. In2023 IEEE International Conference on Software Maintenance and\nEvolution (ICSME). 1–13. https://doi.org/10.1109/ICSME58846.2023.00013\n[3] Ian Nery Bandeira, Thiago Veras Machado, Vitor F Dullens, and Edna Dias Canedo.\n2019. Competitive programming: A teaching methodology analysis applied to\nfirst-year programming classes. In2019 IEEE Frontiers in Education Conference\n(FIE). IEEE, 1–8.\n[4] Ali Borji and Mehrdad Mohammadian. 2023. Battle of the Wordsmiths: Comparing\nChatGPT, GPT-4, Claude, and Bard.GPT-4, Claude, and Bard (June 12, 2023)(2023).\n[5] Victor R Basili1 Gianluigi Caldiera and H Dieter Rombach. 1994. The goal\nquestion metric approach.Encyclopedia of software engineering(1994), 528–532.\n[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\net al . 2021. Evaluating large language models trained on code.arXiv preprint\narXiv:2107.03374(2021).\n[7] Tristan Coignion, Clément Quinton, and Romain Rouvoy. 2024. A Performance\nStudy of LLM-Generated Code on C2. InEASE’24 - 28th International Conference\nESEM ’24, October 24–25, 2024, Barcelona, Spain Mustakim et al.\non Evaluation and Assessment in Software Engineering (Proceedings of the 28th\nInternational Conference on Evaluation and Assessment in Software Engineering\n(EASE’24)). Salerno, Italy. https://hal.science/hal-04525620\n[8] Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh,\nMichel C Desmarais, and Zhen Ming Jack Jiang. 2023. Github copilot ai pair\nprogrammer: Asset or liability?Journal of Systems and Software203 (2023),\n111734.\n[9] Hampus Ekedahl and Vilma Helander. 2023. Can artificial intelligence replace\nhumans in programming?\n[10] Yunhe Feng, Sreecharan Vanam, Manasa Cherukupally, Weijian Zheng, Meikang\nQiu, and Haihua Chen. 2023. Investigating Code Generation Performance of Chat-\nGPT with Crowdsourcing Social Data. InProceedings of the 47th IEEE Computer\nSoftware and Applications Conference. 1–10.\n[11] James Finnie-Ansley, Paul Denny, Brett A Becker, Andrew Luxton-Reilly, and\nJames Prather. 2022. The robots are coming: Exploring the implications of openai\ncodex on introductory programming. InProceedings of the 24th Australasian\nComputing Education Conference. 10–19.\n[12] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,\nRuiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2022. Incoder: A\ngenerative model for code infilling and synthesis.arXiv preprint arXiv:2204.05999\n(2022).\n[13] Felix HANS. [n. d.]. GPT-3.5/4-Is the programming performance declining over\ntime?focus7, 8 ([n. d.]), 9. https://doi.org/10.13140/RG.2.2.30950.80966\n[14] Felix Hans. 2023. ChatGPT vs. Bard - Which is better at solving coding problems?\n10.13140/RG.2.2.36200.65284(08 2023).\n[15] Jocelyn Harper. 2022. Interview insight: How to get the job. InA Software\nEngineer’s Guide to Seniority: A Guide to Technical Leadership. Springer, 19–28.\n[16] Oseremen Joy Idialu, Noble Saji Mathews, Rungroj Maipradit, Joanne M Atlee, and\nMei Nagappan. 2024. Whodunit: Classifying Code as Human Authored or GPT-4\nGenerated–A case study on CodeChef problems.arXiv preprint arXiv:2403.04013\n(2024).\n[17] Baskhad Idrisov and Tim Schlippe. 2024. Program Code Generation with Gener-\native AIs.Algorithms17, 2 (2024), 62.\n[18] Sandeep Kaur Kuttal, Xiaofan Chen, Zhendong Wang, Sogol Balali, and Anita\nSarma. 2021. Visual Resume: Exploring developers’ online contributions for\nhiring.Information and Software Technology138 (2021), 106633.\n[19] Paola A Leon Alarcon. 2024.Understanding the Relevance, Efficiency and Effi-\ncacy of Timed Coding Assessments in the Software Engineering Industry. Ph. D.\nDissertation. Massachusetts Institute of Technology.\n[20] Sila Lertbanjongngam, Bodin Chinthanet, Takashi Ishio, Raula Gaikovina Kula,\nPattara Leelaprute, Bundit Manaskasemsak, Arnon Rungsawang, and Kenichi\nMatsumoto. 2022. An empirical evaluation of competitive programming ai: A\ncase study of alphacode. In2022 IEEE 16th International Workshop on Software\nClones (IWSC). IEEE, 10–15.\n[21] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,\nChenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al . 2023.\nStarcoder: may the source be with you!arXiv preprint arXiv:2305.06161(2023).\n[22] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi\nLeblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al . 2022.\nCompetition-level code generation with alphacode.Science378, 6624 (2022),\n1092–1097.\n[23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your\ncode generated by chatgpt really correct? rigorous evaluation of large language\nmodels for code generation.Advances in Neural Information Processing Systems\n36 (2024).\n[24] Zhijie Liu, Yutian Tang, Xiapu Luo, Yuming Zhou, and Liang Feng Zhang. 2024.\nNo need to lift afinger anymore? Assessing the quality of code generation by\nChatGPT.IEEE Transactions on Software Engineering(2024).\n[25] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu,\nChongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder:\nEmpowering code large language models with evol-instruct.arXiv preprint\narXiv:2306.08568(2023).\n[26] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah\nWiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al .\n2024. Self-refine: Iterative refinement with self-feedback.Advances in Neural\nInformation Processing Systems36 (2024).\n[27] Mary L McHugh. 2013. The chi-square test of independence.Biochemia medica\n23, 2 (2013), 143–149.\n[28] Saurav Muke, Samruddhi Ahire, Geetanjali Kale, and Pranali Navghare. 2024.\nICode-An Unified Competitive Coding Profile Platform. In2024 International\nConference on Emerging Smart Computing and Informatics (ESCI). IEEE, 1–5.\n[29] Nathalia Nascimento, Paulo Alencar, and Donald Cowan. 2023. Artificial Intelli-\ngence Versus Software Engineers: An Evidence-based Assessment Focusing on\nNon-functional Requirements. (2023).\n[30] Nathalia Nascimento, Paulo Alencar, and Donald Cowan. 2023. Comparing\nsoftware developers with chatgpt: An empirical investigation.arXiv preprint\narXiv:2305.11837(2023).\n[31] Nhan Nguyen and Sarah Nadi. 2022. An empirical evaluation of GitHub copilot’s\ncode suggestions. InProceedings of the 19th International Conference on Mining\nSoftware Repositories. 1–5.\n[32] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,\nSilvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.arXiv preprint arXiv:2203.13474\n(2022).\n[33] N Nikolaidis, K Flamos, D Feitosa, A Chatzigeorgiou, and A Ampatzoglou. 2023.\nThe End of an Era: Can Ai Subsume Software Developers.Evaluating Chatgpt\nand Copilot Capabilities Using Leetcode Problems(2023).\n[34] Shuyin Ouyang, Jie M Zhang, Mark Harman, and Meng Wang. 2023. LLM is\nLike a Box of Chocolates: the Non-determinism of ChatGPT in Code Generation.\narXiv preprint arXiv:2308.02828(2023).\n[35] Harikumar Pallathadka, V Hari Leela, Sushant Patil, BH Rashmi, Vipin Jain,\nand Samrat Ray. 2022. Attrition in software companies: Reason and measures.\nMaterials Today: Proceedings51 (2022), 528–531.\n[36] Dipendra Pant, Dhiraj Pokhrel, and Prakash Poudyal. 2022. Automatic Software\nEngineering Position Resume Screening using Natural Language Processing,\nWord Matching, Character Positioning, and Regex. In2022 5th International\nConference on Advanced Systems and Emergent Technologies (IC_ASET). IEEE,\n44–48.\n[37] Palash R. Roy, Ajmain I. Alam, Farouq Al-omari, Banani Roy, Chanchal K. Roy, and\nKevin A. Schneider. 2023. Unveiling the Potential of Large Language Models in\nGenerating Semantic and Cross-Language Clones. In2023 IEEE 17th International\nWorkshop on Software Clones (IWSC). 22–28. https://doi.org/10.1109/IWSC60764.\n2023.00011\n[38] Palash Ranjan Roy, Md. Noushin Islam, Labiba Tasfiya Jeba, Md. Adnanul Haq,\nIffat Afsara Prome, Mohammad Kaykobad, and Tanvir Kaykobad. 2022. A Study\non Paper and Author Ranking. In2022 International Conference on Innovations in\nScience, Engineering and Technology (ICISET). 545–549. https://doi.org/10.1109/\nICISET54810.2022.9775821\n[39] Per Runeson and Martin Höst. 2009. Guidelines for conducting and reporting\ncase study research in software engineering.Empirical software engineering14\n(2009), 131–164.\n[40] Fardin Ahsan Sakib, Saadat Hasan Khan, and AHM Karim. 2023. Extend-\ning the frontier of chatgpt: Code generation and debugging.arXiv preprint\narXiv:2307.08260(2023).\n[41] Shashi Kant Singh, Shubham Kumar, and Pawan Singh Mehra. 2023. Chat GPT &\nGoogle Bard AI: A Review. In2023 International Conference on IoT, Communication\nand Automation Technology (ICICAT). IEEE, 1–6.\n[42] Shuo Sun, Yuchen Zhang, Jiahuan Yan, Yuze Gao, Donovan Ong, Bin Chen, and\nJian Su. 2023. Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna\nvs Guanaco vs Bard vs ChatGPT–A Text-to-SQL Parsing Comparison.arXiv\npreprint arXiv:2310.10190(2023).\n[43] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste\nAlayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth,\net al. 2023. Gemini: a family of highly capable multimodal models.arXiv preprint\narXiv:2312.11805(2023).\n[44] Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, and Jingbo Shang.\n2024. Can LLMs Learn from Previous Mistakes? Investigating LLMs’ Errors to\nBoost for Reasoning.arXiv preprint arXiv:2403.20046(2024).\n[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al . 2023. Llama: Open and efficient foundation language models.arXiv\npreprint arXiv:2302.13971(2023).\n[46] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation\nvs. experience: Evaluating the usability of code generation tools powered by\nlarge language models. InChi conference on human factors in computing systems\nextended abstracts. 1–7.\n[47] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and\nSteven CH Hoi. 2023. Codet5+: Open code large language models for code\nunderstanding and generation.arXiv preprint arXiv:2305.07922(2023).\n[48] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-\naware unified pre-trained encoder-decoder models for code understanding and\ngeneration.arXiv preprint arXiv:2109.00859(2021).\n[49] Szymon Wasik, Maciej Antczak, Jan Badura, Artur Laskowski, and Tomasz Sternal.\n2018. A survey on online judge systems and their applications.ACM Computing\nSurveys (CSUR)51, 1 (2018), 1–34.\n[50] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al . 2022. Chain-of-thought prompting elicits reasoning\nin large language models.Advances in neural information processing systems35\n(2022), 24824–24837.\n[51] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert,\nAshraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt\npattern catalog to enhance prompt engineering with chatgpt.arXiv preprint\narXiv:2302.11382(2023).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7577903270721436
    },
    {
      "name": "Programming language",
      "score": 0.4538617432117462
    },
    {
      "name": "Exploratory research",
      "score": 0.42411312460899353
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I32625721",
      "name": "University of Saskatchewan",
      "country": "CA"
    }
  ]
}