{
  "title": "Synthesized Speech Detection Using Convolutional Transformer-Based Spectrogram Analysis",
  "url": "https://openalex.org/W4214896311",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4225031217",
      "name": "Bartusiak, Emily R.",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2751007751",
      "name": "Delp, Edward J.",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2950098109",
    "https://openalex.org/W2940618856",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W2122111042",
    "https://openalex.org/W2314633289",
    "https://openalex.org/W2911424785",
    "https://openalex.org/W3036576316",
    "https://openalex.org/W1976526581",
    "https://openalex.org/W2888728157",
    "https://openalex.org/W2914424550",
    "https://openalex.org/W2184242386",
    "https://openalex.org/W1923358586",
    "https://openalex.org/W2747872086",
    "https://openalex.org/W2801171099",
    "https://openalex.org/W6776384377",
    "https://openalex.org/W2103869314",
    "https://openalex.org/W6834694846",
    "https://openalex.org/W2062826588",
    "https://openalex.org/W3033711348",
    "https://openalex.org/W2982058372",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3180160903",
    "https://openalex.org/W1536990986",
    "https://openalex.org/W3199074910",
    "https://openalex.org/W2090431713",
    "https://openalex.org/W2783089003",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6795140394",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6795062860",
    "https://openalex.org/W3198123200",
    "https://openalex.org/W6765397690",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2955054437",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4232336823",
    "https://openalex.org/W3159778524",
    "https://openalex.org/W3019584399"
  ],
  "abstract": "Synthesized speech is common today due to the prevalence of virtual assistants, easy-to-use tools for generating and modifying speech signals, and remote work practices. Synthesized speech can also be used for nefarious purposes, including creating a purported speech signal and attributing it to someone who did not speak the content of the signal. We need methods to detect if a speech signal is synthesized. In this paper, we analyze speech signals in the form of spectrograms with a Compact Convolutional Transformer (CCT) for synthesized speech detection. A CCT utilizes a convolutional layer that introduces inductive biases and shared weights into a network, allowing a transformer architecture to perform well with fewer data samples used for training. The CCT uses an attention mechanism to incorporate information from all parts of a signal under analysis. Trained on both genuine human voice signals and synthesized human voice signals, we demonstrate that our CCT approach successfully differentiates between genuine and synthesized speech signals.",
  "full_text": "Synthesized Speech Detection Using Convolutional\nTransformer-Based Spectrogram Analysis\nEmily R. Bartusiak, Edward J. Delp\nVideo and Image Processing Lab (VIPER)\nSchool of Electrical and Computer Engineering\nPurdue University\nWest Lafayette, IN, USA\nAbstract—Synthesized speech is common today due to the\nprevalence of virtual assistants, easy-to-use tools for generat-\ning and modifying speech signals, and remote work practices.\nSynthesized speech can also be used for nefarious purposes,\nincluding creating a purported speech signal and attributing it\nto someone who did not speak the content of the signal. We\nneed methods to detect if a speech signal is synthesized. In this\npaper, we analyze speech signals in the form of spectrograms with\na Compact Convolutional Transformer (CCT) for synthesized\nspeech detection. A CCT utilizes a convolutional layer that\nintroduces inductive biases and shared weights into a network,\nallowing a transformer architecture to perform well with fewer\ndata samples used for training. The CCT uses an attention\nmechanism to incorporate information from all parts of a signal\nunder analysis. Trained on both genuine human voice signals\nand synthesized human voice signals, we demonstrate that our\nCCT approach successfully differentiates between genuine and\nsynthesized speech signals.\nIndex Terms—machine learning, deep learning, signal process-\ning, image processing, spectrogram analysis, synthesized audio\ndetection, spoof, convolution, transformer, neural networks\nI. I NTRODUCTION\nWe hear more synthesized voices in our daily lives than ever\nbefore. V oice assistants, such as Apple’s Siri, Amazon’s Alexa,\nMicrosoft’s Cortana, and Google’s Assistant, use synthesized\nhuman speech to communicate with us in our homes [1], [2].\nVirtual assistants answer customer service phone numbers and\ncreate synthetic voices as they assist us. Applications use text-\nto-speak (TTS) methods to generate audio signals to speak text\nmessages for users with poor vision. Social media platforms\noffer tools to generate speech that sounds like a speciﬁc\nperson, such as a friend, an actor, or a politician. Although\nall of these features can be used for innocuous purposes, they\ncan also easily be used to create authentic-sounding speech\nfor more malicious ambitions.\nAttackers may create a purported speech signal and at-\ntribute it to someone who never delivered that message. In\n2021, Goldman Sachs stopped a $40 million investment in a\ncompany when employees realized they were meeting with\nan impersonator using synthesized speech on a conference\ncall [3]. Although standalone synthesized speech replicating\na speciﬁc target’s voice can do a lot of damage on its own,\nits impact can be even greater when the synthesized speech\nis paired with other data modalities. When manipulated video\nFig. 1: Spectrogram of a synthesized speech signal.\naccompanies synthetic speech signals, such as in deepfakes,\nthe potential to inﬂuence public opinion and current events\nis even higher [4], [5]. Because many easy-to-use tools exist\nfor modifying multimedia with high quality, the quantity\nof manipulated media increases exponentially [6]. We need\nmethods to detect if speech signals are synthesized or genuine.\nIn this paper, we present a method to detect synthesized\nspeech from spectrograms. Spectrograms have been used for\na variety of audio tasks to identify events, transfer a signal\ninto a new style, and detect emotion [7]–[12]. They are\neffective visualizations of speech signals because they show\nthe relationship between time, frequency, and intensity of an\naudio signal. Spectrograms are constructed using the Fast\nFourier Transform (FFT), a variant of the Discrete Fourier\nTransform (DFT) [7]. The FFT divides speech signals into\nshorter temporal subsequences (sometimes known as frames).\nThe DFT of each temporal segment is computed to obtain the\nDFT frequency coefﬁcients for each temporal segment. The\nmagnitudes of the DFT coefﬁcients are aligned side-by-side to\ncreate a spectrogram. A spectrogram of a speech signal used\nin our approach is shown in Figure 1. In this paper, we treat\nspectrograms as images and use a convolutional transformer-\nbased approach to determine if the speech signals shown in\nthe spectrograms are synthesized.\nII. R ELATED WORK\nIn order to detect synthetic speech, various machine learning\nmethods convert speech signals into different representations.\nChen et al. use a Multilayer Perceptron Network (MLP),\nResNet-based Convolutional Neural Network (CNN), Long\nShort-Term Memory network (LSTM), Gated Recurrent Unit\narXiv:2205.01800v1  [cs.SD]  3 May 2022\nFig. 2: Block diagram of the proposed approach: the Compact Convolutional Transformer (CCT).\nnetwork (GRU), and Recurrent Neural Network (RNN) to\ndetect spoofed speech [13], [14]. For these methods, the\nspeech waveforms are converted to sequences of coefﬁcients\n– speciﬁcally Constant Q Cepstral Coefﬁcients (CQCCs) [15]\nor Mel Frequency Cepstral Coefﬁcients (MFCCs) [16]–[18]\n– and analyzed by the various neural networks. Chintha\net al. use a CNN-LSTM model to analyze speech signals\ndirectly, rather than converting them to different coefﬁcient\nrepresentations [19]. The authors also explore working with\nlog-melspectrograms, which are spectrograms in which the\nfrequency domain content is mapped to the mel scale [16].\nThe log-melspectrograms are analyzed with a CNN to detect\nsynthesized and genuine audio. For our approach, we utilize\nspectrograms because many methods striving to perform a\nvariety of tasks achieve success with signals represented as\nspectrograms [7]–[12].\nAlthough prior work relies heavily on Convolutional Neu-\nral Networks (CNNs), recent developments in deep learning\nindicate that convolutions may not be necessary to effectively\nanalyze images [20], [21]. These methods succeed in image\nclassiﬁcation tasks without inductive biases provided by con-\nvolutions. Inspired by the success of attention mechanisms\nin Natural Language Processing (NLP) [22], Vision Trans-\nformer (ViT) analyzes patches of an image with an attention\nmechanism for image classiﬁcation tasks [20]. Hassani et al.\nadopt concepts from CNNs and ViT to create a Compact\nConvolutional Transformer (CCT) [23]. CCT leverages the\ninductive biases and efﬁciencies of parameter-sharing that\nconvolutions provide to succeed at machine learning tasks with\nsmaller-sized datasets compared to the datasets used with ViT.\nIt also leverages the attention mechanism of transformers to\ncapture long-range dependencies in images. CCT combines\nthe power of convolutions with the power of transformers. We\nutilize a CCT trained on spectrograms showing genuine and\nsynthesized speech signals to identify synthesized speech.\nIII. P ROPOSED METHOD\nFigure 2 shows an overview of our approach, known as the\nCompact Convolutional Transformer (CCT). The CCT uses a\nstandard transformer encoder, as used in [20], [22]. However,\nCCT introduces two new features – a convolutional image\nencoding block and a sequence pooling layer – that replace\noperations in standard transformer approaches. The CCT ﬁrst\nuses a convolutional block ( i.e., a series of convolutional\nlayers) to embed an input image into a latent space. In our\nexperiments, we utilize two convolutional layers with a kernel\nof size 3x3, a ReLU activation function, and max pooling.\nThe ﬁrst and second convolutional layers produce sets of\n64 and 128 feature maps, respectively. We use this convolu-\ntional block instead of the standard transformer practice of\ndividing input images into non-overlapping patches, which\ncontain only local image information and fail to preserve\ninformation at patch boundaries. The feature maps contain\naggregate information from all regions of an image, so they\nare more salient inputs to the transformer encoder. Because\nthey result from convolution operations, they also introduce\ninductive biases to the network. This enables the transformer\nto train more efﬁciently, which is highly important on smaller-\nsized datasets. Next, we row concatenate each of the 128\n2D feature maps (sized 32x32) into a vector of length 1024,\ncreating the tokens analyzed by the transformer encoder. We\nuse positional embedding (a standard practice in transformers)\nfor each token so that the transformer understands how they\nrelate spatially [20], [24].\nNext, CCT analyzes the tokens with the transformer en-\ncoder, which consists of two transformer encoder layers. Each\ntransformer encoder layer contains the multi-headed attention\nmechanism that captures long-range dependencies between\ndifferent parts of the input. The transformer encoder layers are\nmodeled after typical attention-based layers [20], [22]. Then,\nsequence pooling occurs on the outputs of the transformer\nencoder. The pooling operation smooths the sequence of\noutputs so that the MLP Head can correctly detect whether\nthe speech signal under analysis is synthesized or genuine.\nSequence pooling also eliminates the need for an extra token\n(i.e., a classiﬁcation token) that other transformers use [20],\n[24]. With sequence pooling, the model no longer needs to\ntrack the classiﬁcation token throughout its layers.\nTABLE I: Dataset used in our experiments.\nASVspoof2019 Dataset\nSubset Synthesized\nAudio Tracks\nGenuine\nAudio Tracks\nTotal\nAudio Tracks\nTraining 22,800 2,580 25,380\nValidation 22,296 2,548 24,844\nTesting 63,882 7,355 71,237\nTotal 108,978 12,483 121,461\nIV. E XPERIMENTAL RESULTS\nWe utilize the ASVspoof2019 dataset [25] in our exper-\niments. The dataset – introduced in ASVspoof2019: Au-\ntomatic Speaker Veriﬁcation Spooﬁng and Countermeasures\nChallenge [26] – contains genuine speech signals spoken by\nhumans as well as synthesized speech signals. The synthesized\nspeech signals were generated with neural acoustic models and\ndeep learning methods, including LSTMs [27] and Genera-\ntive Adversarial Networks (GANs) [28]. The ASVspoof2019\ndataset is heavily imbalanced, with signiﬁcantly more synthe-\nsized speech signals than genuine speech signals. We utilize\nthe ofﬁcial dataset split according to the challenge for training,\nvalidating, and testing our approach. Table I summarizes the\ndetails of the dataset.\nWe convert speech waveforms from the dataset into spectro-\ngrams by following a similar procedure as described in [29].\nMore speciﬁcally, we use the Fast Fourier Transform (FFT) to\ncompute Fourier coefﬁcients of signals in our dataset. The FFT\noperates on blocks of the signals consisting of 512 sampled\npoints with 128 points of overlap between consecutive blocks.\nThen, the Fourier coefﬁcients are converted to decibels and\norganized in 2D arrays to construct the spectrograms. We\nrepresent each spectrogram with a matrix of 128x128 values.\nNote that these spectrograms are larger than those used in our\nprevious approach [29]. The larger spectrograms have higher\nresolution, which preserves more details of speech signals for\nthe synthesized speech detector. Next, we perform min-max\nnormalization on the intensity values, mapping the spectro-\ngram intensities to the range of values [0,1]. Normalized values\nenable machine learning models to learn more quickly because\nthey are forced to focus on relative rather than absolute\ndifferences in input values. Figure 1 shows an example of\na grayscale, normalized spectrogram that is analyzed by the\nCCT.\nTo validate our approach, we compare it against sev-\neral other methods. First, we establish three baseline meth-\nods: Baseline-Minority, Baseline-Majority, and Baseline-Prior.\nBaseline-Minority is a classiﬁer that only predicts that speech\nsignals belong to the minority class – in this case, the\ngenuine category. Baseline-Majority is a classiﬁer that does\nthe opposite. It only predicts that speech signals belong to the\nsynthesized class. Considering the signiﬁcant class imbalance\nin the dataset, we expect Baseline-Minority to have the worst\nperformance of all methods. Meanwhile, Baseline-Majority es-\ntablishes a threshold above which a classiﬁer actually performs\nwell. Baseline-Prior is the ﬁnal baseline classiﬁer. It randomly\nassigns a label to a signal under analysis according to the\nknown distribution of genuine vs. synthesized samples in the\ntraining data split. In addition to these baselines, we investigate\nthe effectiveness of a K-Nearest Neighbors classiﬁer [30],\n[31], a Support Vector Machine (SVM) [32], and logistic\nregression [33] on this task. These methods operate on row\nconcatenated versions of the 128x128-sized spectrograms (i.e.,\nvectors of length 16,384). Finally, we compare our results to\nour previous synthesized speech detection method that utilizes\na CNN [29]. We report the performance of the CNN on\nthe smaller-sized spectrograms (dimensions 50x34 pixels) as\nwell as the the new, larger-sized spectrograms (dimensions\n128x128).\nTable II and Figure 3 show the results of all approaches. We\nreport accuracy, weighted precision, weighted recall, weighted\nF-1, Balanced Accuracy, Receiver Operating Characteristic\nArea Under the Curve (ROC AUC), and Precision Recall Area\nUnder the Curve (PR AUC) [34], [35]. Weighted metrics are\ncomputed with a weighted average of each metric obtained\non the two classes, where weights reﬂect the dataset class\nimbalance. Results indicate that the CCT approach outper-\nforms all other methods by a clear margin. It achieves the\nhighest metrics of all methods considered. Although the CCT\nperforms better than our previously proposed CNN, the CNN\nachieves the second highest performance when trained on\nspectrograms of size 128x128 pixels. Overall, the two neural\nnetwork approaches perform the best.\nResults conﬁrm that both larger input images and the\nnew model contribute to better performance. Comparing the\nresults of CNN-50x34 and CNN-128x128, we observe that\nbalanced accuracy, ROC AUC, and PR AUC increase when\nthe CNN is trained and evaluated on larger, higher-resolution\ninputs. In this case, the CNN is presented with more detailed\ninput images that allow it to better discriminate genuine and\nsynthesized speech signals. Comparing the results of CNN-\n128x128 and CCT, we observe that ROC AUC and PR AUC\nincrease even more when an attention mechanism is used.\nThe attention mechanism of the transformer determines the\nmost important part of a spectrogram and focuses on that part\nof the image more so than the less discriminative regions,\nFig. 3: The ROC and PR of all methods examined. The three baseline methods are all represented by the “baselines” method.\nTABLE II: Results of all methods. ROC AUC and PR AUC represent area under the curve of the receiver operating characteristic\nand precision recall curves, respectively.\nMethod Accuracy Precision Recall F-1 Balanced Accuracy ROC AUC PR AUC\nBaseline-Minority 10.32% 1.07% 10.32% 1.93% 50.00% 0.5000 0.1032\nBaseline-Prior 81.59% 81.46% 81.59% 81.52% 49.94% 0.4994 0.1032\nBaseline-Majority 89.68% 80.42% 89.68% 84.79% 50.00% 0.5000 0.1032\nKNN 89.45% 84.99% 89.45% 85.57% 52.08% 0.6751 0.2643\nLogistic Regression 80.60% 91.76% 80.60% 83.98% 84.41% 0.9041 0.4101\nCNN-50x34 85.59% 90.50% 85.59% 87.35% 79.26% 0.9052 0.4649\nSVM 89.93% 90.94% 89.93% 85.25% 50.47% 0.9113 0.5023\nCNN-128x128 85.27% 93.21% 85.27% 87.60% 89.22 % 0.9416 0.6278\nCCT 92.13% 93.79% 92.13% 92.70% 87.78% 0.9646 0.7501\nwhich aids in its detection capabilities. However, transformers\nhave historically required very large-scale datasets in order\nto learn properly, suffering from a lack of inductive biases\nthat CNNs have. Because we utilize convolutional layers, the\nCCT achieves a greater degree of shared weights, learns more\nefﬁciently, and leverages the inductive biases to achieve high\nsuccess, even with fewer data samples from which to learn.\nV. C ONCLUSION\nThis paper improves upon our previous work [29] to\ndemonstrate the beneﬁts of using higher-resolution spectro-\ngrams and an attention mechanism. We demonstrate that a\nneural network that utilizes both convolution and transformer\ncapabilities achieves high success in detecting synthesized\nspeech. Convolution operations convert a spectrogram input\nimage into feature maps that contain salient information for\ndiscriminating synthesized and genuine human speech. It also\nenables a transformer to achieve high success with less data\nthan transformers typically require.\nAlthough this approach is promising, future work should\nconsider more diverse speech features. For example, the\nmethod should be validated on data of different audio formats,\ncompression levels, sampling rates, and durations. Since both\nthe CNN and the CCT perform well, an ensemble of these\ntwo methods could be created and augmented with other\nneural networks. Finally, a speech analysis method such as\nthis could be paired with methods that analyze media’s other\ndata modalities. For example, our synthesized speech detector\ncould analyze speech signals found in videos, while methods\nthat analyze images and videos could analyze the visual con-\ntent [36] [4] [37] [38]. A metadata analysis could strengthen\nthis multi-modal approach even more [39].\nACKNOWLEDGMENT\nThis material is based on research sponsored by DARPA and\nAir Force Research Laboratory (AFRL) under agreement num-\nber FA8750-16-2-0173. The U.S. Government is authorized to\nreproduce and distribute reprints for Governmental purposes\nnotwithstanding any copyright notation thereon. The views\nand conclusions contained herein are those of the authors\nand should not be interpreted as necessarily representing the\nofﬁcial policies or endorsements, either expressed or implied,\nof DARPA and AFRL or the U.S. Government.\nAddress all correspondence to Edward J. Delp,\nace@ecn.purdue.edu.\nREFERENCES\n[1] M. B. Hoy, “Alexa, Siri, Cortana, and More: An Introduction to V oice\nAssistants,” Medical Reference Services Quarterly, vol. 37, no. 1, pp.\n81–88, 2018.\n[2] S. Malodia, N. Islam, P. Kaur, and A. Dhir, “Why Do People Use Ar-\ntiﬁcial Intelligence (AI)-Enabled V oice Assistants?” IEEE Transactions\non Engineering Management, pp. 1–15, December 2021.\n[3] B. Smith, “Goldman Sachs, Ozy Media and a $40 Million Con-\nference Call Gone Wrong,” The New York Times , September\n2021, https://www .nytimes.com/2021/09/26/business/media/ozy-media-\ngoldman-sachs.html.\n[4] A. R ¨ossler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Niess-\nner, “FaceForensics++: Learning to Detect Manipulated Facial Images,”\nProceedings of the IEEE/CVF International Conference on Computer\nVision, August 2019, seoul, Korea.\n[5] R. Toews, “Deepfakes are Going to Wreck Havoc on Society. We are\nNot Prepared.” Forbes, May 2020.\n[6] H. Ajder, G. Patrini, F. Cavalli, and L. Cullen, “The State of Deepfakes:\nLandscape, Threats, and Impact,” Deeptrace Lab, September 2019.\n[7] L. Flanagan, “Speech Analysis, Synthesis and Perception,” Springer-\nVerlag, 1972.\n[8] P. Verma and J. Smith, “Neural Style Transfer for Audio Spectrograms,”\nProceedings of the Conference on Neural Information Processing\nSystems, Workshop for Machine Learning for Creativity and Design,\nDecember 2017, long Beach, CA.\n[9] J. Dennis, H. D. Tran, and H. Li, “Spectrogram Image Feature for\nSound Event Classiﬁcation in Mismatched Conditions,” IEEE Signal\nProcessing Letters, vol. 18, no. 2, pp. 130–133, 2011.\n[10] M. Stolar, M. Lech, R. S. Bolia, and M. Skinner, “Acoustic Charac-\nteristics of Emotional Speech Using Spectrogram Image Classiﬁcation,”\nProceedings the IEEE International Conference on Signal Processing\nand Communication Systems, December 2018, cairns, Australia.\n[11] S. Prasomphan, “Detecting Human Emotion via Speech Recognition\nby Using Speech Spectrogram,” Proceedings of the IEEE International\nConference on Data Science and Advanced Analytics, October 2015,\nparis, France.\n[12] S. Prasomphan, “Improvement of Speech Emotion Recognition with\nNeural Network Classiﬁer by Using Speech Spectrogram,” Proceedings\nof the International Conference on Systems, Signals and Image Process-\ning, September 2015, london, UK.\n[13] Z. Chen, Z. Xie, W. Zhang, and X. Xu, “ResNet and Model Fusion for\nAutomatic Spooﬁng Detection,” Proceedings of the Conference of the\nInternational Speech Communication Association, August 2017.\n[14] Z. Chen, W. Zhang, Z. Xie, X. Xu, and D. Chen, “Recurrent Neural Net-\nworks for Automatic Replay Spooﬁng Attack Detection,” Proceedings\nof the IEEE International Conference on Acoustics, Speech and Signal\nProcessing, April 2018, calgary, Canada.\n[15] B. Bogert, M. Healy, and J. Tukey, “The Quefrency Alanysis of Time\nSeries for Echoes: Cepstrum, Pseudo Autocovariance, Cross-Cepstrum\nand Saphe Cracking,” Proceedings of the Symposium on Time Series\nAnalysis, vol. 15, pp. 209–243, June 1963, new York, NY .\n[16] S. Stevens, J. V olkmann, and E. Newman, “A Scale for the Measurement\nof the Psychological Magnitude Pitch,” Journal of the Acoustical Society\nof America, vol. 8, pp. 185–190, June 1937.\n[17] D. Purves and G. Fitzpatrick, “Neuroscience,” The Audible Spectrum,\nvol. 2nd Edition, 2001, sunderland, MA.\n[18] M. Sahidullah and G. Saha, “Design, Analysis, and Experimental\nEvaluation of Block Based Transformation in MFCC Computation for\nSpeaker Recognition,” Speech Communication, vol. 54, pp. 543–565,\nMay 2012.\n[19] A. Chintha, B. Thai, S. J. Sohrawardi, K. M. Bhatt, A. Hickerson,\nM. Wright, and R. Ptucha, “Recurrent Convolutional Structures for\nAudio Spoof and Video Deepfake Detection,” IEEE Journal of Selected\nTopics in Signal Processing, June 2020.\n[20] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale,” Proceedings of the\nInternational Conference on Learning Representations, May 2021.\n[21] I. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai,\nT. Unterthiner, J. Yung, D. Keysers, J. Uszkoreit, M. Lucic, and\nA. Dosovitskiy, “MLP-Mixer: An all-MLP Architecture for Vision,”\narXiv:2105.01601, May 2021.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is All You Need,” Proceedings\nof the Neural Information Processing Systems, December 2017.\n[23] A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, and H. Shi,\n“Escaping the Big Data Paradigm with Compact Transformers,”\narXiv:2104.05704, 2021.\n[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language understanding,”\nin Proceedings of the Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota:\nAssociation for Computational Linguistics, June 2019, pp. 4171–4186.\n[25] J. Yamagishi, M. Todisco, M. Sahidullah, H. Delgado, X. Wang,\nN. Evans, T. Kinnunen, K. Lee, V . Vestman, and A. Nautsch, “ASVspoof\n2019: The 3rd Automatic Speaker Veriﬁcation Spooﬁng and Counter-\nmeasures Challenge database,” University of Edinburgh. The Centre for\nSpeech Technology Research, 2019.\n[26] M. Todisco, J. Yamagishi, M. Sahidullah, H. Delgado, X. Wang,\nN. Evans, T. Kinnunen, K. Lee, V . Vestman, and A. Nautsch, “ASVspoof\n2019: Automatic Speaker Veriﬁcation Spooﬁng and Countermeasures\nChallenge Evaluation Plan,” ASVspoof Consortium, January 2019.\n[27] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” Neural\nComputation, vol. 9, pp. 1735–1780, November 1997.\n[28] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y . Bengio, “Generative Adversarial Nets,”\nProceedings of the Conference on Neural Information Processing Sys-\ntems, vol. 27, 2014.\n[29] E. R. Bartusiak and E. J. Delp, “Frequency Domain-Based Detection\nof Generated Audio,” Proceedings of the IS&T Media Watermarking,\nSecurity, and Forensics Conference, Electronic Imaging Symposium, pp.\n273(1)–273(7), January 2021.\n[30] B. W. Silverman and M. C. Jones, “E. Fix and J.L. Hodges (1951):\nAn Important Contribution to Nonparametric Discriminant Analysis\nand Density Estimation: Commentary on Fix and Hodges (1951),”\nInternational Statistical Review / Revue Internationale de Statistique,\nvol. 57, no. 3, pp. 233–238, 1989.\n[31] T. Cover and P. Hart, “Nearest Neighbor Pattern Classiﬁcation,” IEEE\nTransactions on Information Theory, vol. 13, no. 1, pp. 21–27, 1967.\n[32] C. Cortes and V . Vapnik, “Support-Vector Networks,”Machine Learning,\nvol. 20, pp. 272–297, 1995.\n[33] C. M. Bishop, “Probabilistic Discriminative Models,” in Pattern Recog-\nnition and Machine Learning. New York: Springer-Verlag, 2005, ch.\n4.3, pp. 205–210.\n[34] A. Tharwat, “Classiﬁcation assessment methods,” in Applied Computing\nand Informatics. Emerald Publishing Limited, December 2021, pp.\n168–192.\n[35] J. Davis and M. Goadrich, “The Relationship between Precision-Recall\nand ROC Curves,” Proceedings of the International Conference on\nMachine Learning, pp. 233–240, 2006, pittsburgh, Pennsylvania, USA.\n[36] D. Montserrat, H. Hao, S. Yarlagadda, S. Baireddy, R. Shao, J. Horv ´ath,\nE. R. Bartusiak, J. Yang, D. G¨uera, F. Zhu, and E. Delp, “Deepfakes De-\ntection with Automatic Face Weighting,” Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops,\nJune 2020, seattle, Washington (Virtual).\n[37] D. G ¨uera and E. Delp, “Deepfake Video Detection Using Recurrent\nNeural Networks,” Proceedings of the IEEE International Conference\non Advanced Video and Signal-based Surveillance, November 2018,\nauckland, New Zealand.\n[38] E. R. Bartusiak, S. K. Yarlagadda, D. G ¨uera, P. Bestagini, S. Tubaro,\nF. M. Zhu, and E. J. Delp, “Splicing Detection and Localization In\nSatellite Imagery Using Conditional GANs,” Proceedings of the IEEE\nConference on Multimedia Information Processing and Retrieval, March\n2019, san Jose, CA.\n[39] D. G ¨uera, S. Baireddy, P. Bestagini, S. Tubaro, and E. Delp, “We Need\nNo Pixels: Video Manipulation Detection Using Stream Descriptors,”\nProceedings of the International Conference on Machine Learning,\nSynthetic-Realities: Deep Learning for Detecting AudioVisual Fakes\nWorkshop, June 2019, long Beach, CA.",
  "topic": "Spectrogram",
  "concepts": [
    {
      "name": "Spectrogram",
      "score": 0.9385969042778015
    },
    {
      "name": "Computer science",
      "score": 0.773451566696167
    },
    {
      "name": "Voice activity detection",
      "score": 0.752399206161499
    },
    {
      "name": "Speech recognition",
      "score": 0.6845435500144958
    },
    {
      "name": "Transformer",
      "score": 0.658186674118042
    },
    {
      "name": "SIGNAL (programming language)",
      "score": 0.4978952407836914
    },
    {
      "name": "Speech processing",
      "score": 0.47944870591163635
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4593970477581024
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3002338111400604
    },
    {
      "name": "Engineering",
      "score": 0.08774477243423462
    },
    {
      "name": "Electrical engineering",
      "score": 0.05856969952583313
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I219193219",
      "name": "Purdue University West Lafayette",
      "country": "US"
    }
  ]
}