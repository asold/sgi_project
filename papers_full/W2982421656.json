{
  "title": "Contextual Text Denoising with Masked Language Models",
  "url": "https://openalex.org/W2982421656",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5075553304",
      "name": "Yifu Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5030361161",
      "name": "Haoming Jiang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2914361693",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W2920117846",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2890230387",
    "https://openalex.org/W2098297786",
    "https://openalex.org/W2767899794",
    "https://openalex.org/W3101767350",
    "https://openalex.org/W1979822206",
    "https://openalex.org/W2952509486",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2525778437"
  ],
  "abstract": "Recently, with the help of deep learning models, significant advances have been made in different Natural Language Processing (NLP) tasks. Unfortunately, state-of-the-art models are vulnerable to noisy texts. We propose a new contextual text denoising algorithm based on the ready-to-use masked language model. The proposed algorithm does not require retraining of the model and can be integrated into any NLP system without additional training on paired cleaning training data. We evaluate our method under synthetic noise and natural noise and show that the proposed algorithm can use context information to correct noise text and improve the performance of noisy inputs in several downstream tasks.",
  "full_text": "arXiv:1910.14080v2  [cs.CL]  5 Mar 2024\nContextual T ext Denoising with Masked Language Models\nYifu Sun ∗\nT encent\nyifusun2016@outlook.com\nHaoming Jiang\nGeorgia T ech\njianghm@gatech.edu\nAbstract\nRecently, with the help of deep learning mod-\nels, signiﬁcant advances have been made in\ndifferent Natural Language Processing (NLP)\ntasks. Unfortunately, state-of-the-art models\nare vulnerable to noisy texts. W e propose a\nnew contextual text denoising algorithm based\non the ready-to-use masked language model.\nThe proposed algorithm does not require re-\ntraining of the model and can be integrated\ninto any NLP system without additional train-\ning on paired cleaning training data. W e evalu-\nate our method under synthetic noise and natu-\nral noise and show that the proposed algorithm\ncan use context information to correct noise\ntext and improve the performance of noisy in-\nputs in several downstream tasks.\n1 Introduction\nBased on our prior knowledge and contextual in-\nformation in sentences, humans can understand\nnoisy texts like misspelled words without dif-\nﬁculty . However, NLP systems break down\nfor noisy text. For example,\nBelinkov and Bisk\n(2017) showed that modern neural machine trans-\nlation (NMT) system could not even translate texts\nwith moderate noise. An illustrative example\nof English-to-Chinese translation using Google\nTranslate\n1 is presented in T able 1.\nT ext correction systems are widely used in\nreal-world scenarios to address noisy text inputs\nproblem. Simple rule-based and frequency-based\nspell-checker are limited to complex language sys-\ntems. More recently , modern neural Grammati-\ncal Error Correction (GEC) systems are developed\nwith the help of deep learning (\nZhao et al. , 2019;\nChollampatt and Ng , 2018). These GEC systems\nheavily rely on annotated GEC corpora, such as\nCoNLL-2014 (\nNg et al. , 2014). The parallel GEC\n∗ W ork done at Georgia T ech.\n1 https://translate.google.com; Access Date:\n08/09/2019\ncorpora, however, are expansive, limited, and even\nunavailable for many languages. Another line\nof researches focuses on training a robust model\nthat inherently deals with noise. For example,\nBelinkov and Bisk (2017) train robust character-\nlevel NMT models using noisy training datasets,\nincluding both synthetic and natural noise. On\nthe other hand,\nMalykh et al. (2018) consider ro-\nbust word vectors. These methods require re-\ntraining the model based on new word vectors or\nnoise data. Retraining is expensive and will af-\nfect the performance of clean text. For example,\nin\nBelinkov and Bisk (2017), the robustness scar-\niﬁes the performance of the clean text by about 7\nBLEU score on the EN-FR translation task.\nIn this paper, we propose a novel text denois-\ning algorithm based on the ready-to-use masked\nlanguage model (MLM,\nDevlin et al. (2018)). No-\ntice that we are using English Bert. For other\nlanguages, W e need to use MLM model pre-\ntrained on that speciﬁc language. The design fol-\nlows the human cognitive process that humans\ncan utilize the context, the spell of the wrong\nword (\nMayall et al. , 1997), and even the location\nof the letters on the keyboard to correct noisy text.\nThe MLM essentially mimics the process that the\nmodel predicts the masked words based on their\ncontext. There are several beneﬁts of the proposed\nmethod:\n• Our method can make accurate corrections\nbased on the context and semantic meaning\nof the whole sentence as T able\n1 shows.\n• The pre-trained masked language model is\nready-to-use ( Devlin et al. , 2018; Liu et al. ,\n2019). No extra training or data is required.\n• Our method makes use of W ord Piece embed-\ndings ( Wu et al. , 2016) to alleviate the out-of-\nvocabulary problem.\nMethod Input T ext Google Translate\nClean Input there is a fat duck swimming in the lake 湖里 有一只胖鸭子在游泳\nNoisy Input there is a fat dack swimming in the leake 在leake 里游泳时有一个胖子\nSpell-Checker there is a fat sack swimming in the leak 在泄露处 有一个肥胖袋在游泳\nGrammaly2 there is a fat dack swimming in the lake 湖里游泳很胖\nOurs there is a fat duck swimming in the lake 湖里 有一只胖鸭子在游泳\nT able 1: Illustrative example of spell-checker and context ual denoising.\n2 Method\nOur denoising algorithm cleans the words in the\nsentence in sequential order. Given a word, the\nalgorithm ﬁrst generates a candidate list using the\nMLM and then further ﬁlter the list to select a can-\ndidate from the list. In this section, we ﬁrst brieﬂy\nintroduce the masked language model, and then\ndescribe the proposed denoising algorithm.\n2.1 Masked Language Model\nMasked language model (MLM) masks some\nwords from a sentence and then predicts the\nmasked words based on the contextual informa-\ntion. Speciﬁcally , given a sentence x = {xi}L\ni=1\nwith L words, a MLM models\np(xj|x1, ..., xj−1,[MASK ], xj+1, ..., xL),\nwhere [MASK ] is a masking token over the j-th\nword. Actually , MLM can recover multiple masks\ntogether, here we only present the case with one\nmask for notation simplicity . In this way , unlike\ntraditional language model that is in left-to-right\norder (i.e., p(xj|x1, ..., xj−1)), MLM is able to\nuse both the left and right context. As a result,\na more accurate prediction can be made by MLM.\nIn the following, we use the pre-trained masked\nlanguage model, BERT (\nDevlin et al. , 2018). So\nno training process is involved in developing our\nalgorithm.\n2.2 Denoising Algorithm\nThe algorithm cleans every word in the sentence\nwith left-to-right order except for the punctuation\nand numbers by masking them in order. For each\nword, MLM ﬁrst provide a candidate list using a\ntransformed sentence. Then the cleaned word is\nselected from the list. The whole process is sum-\nmarized in Algorithm\n1.\nT ext MaskingThe ﬁrst step is to convert the sen-\ntence x into a masked form x′. With the use of\nW ord Piece tokens, each word can be represented\nby several different tokens. Suppose the j-th word\n2 https://app.grammarly.com; Access Date:\n08/09/2019\n(that needs to be cleaned) is represented by the js-\nth token to the je-th token, we need to mask them\nout together. For the same reason, the number of\ntokens of the expected cleaned word is unknown.\nSo we use different number of masks to create the\nmasked sentence {x′\nn}N\nn=1, where x′\nn denotes the\nmasked sentence with n-gram mask. Speciﬁcally ,\ngiven x = x1, ..., xjs , ..., xje , ..., xL, the masked\nform is x′\nn = x1, ...,[MASK ] × n, ..., xL. W e\nmask each word in the noisy sentence by order.\nThe number of masks N can not be too small or\ntoo large. The candidate list will fail to capture the\nright answer with a small N. However, the opti-\nmal answer would ﬁt the noisy text perfectly with\na large enough N. Empirically , we ﬁnd out N = 4\nis sufﬁciently large to obtain decent performance\nwithout too much overﬁtting.\nT ext AugmentationSince the wrong word is also\ninformative, so we augment each masked text x′\nn\nby concatenating the original text x. Speciﬁcally ,\nthe augmented text is ˜xn = x′\nn[SEP ]x, where\n[SEP ] is a separation token.\n3\nCompared with directly leaving the noisy word\nin the original sentence, the masking and augmen-\ntation strategy are more ﬂexible. It is beneﬁted\nfrom that the number of tokens of the expected\nword does not necessarily equal to the noisy word.\nBesides, the model pays less attention to the noisy\nwords, which may induce bias to the prediction of\nthe clean word.\nCandidate Selection The algorithm then con-\nstructs a candidate list using the MLM, which is\nsemantically suitable for the masked position in\nthe sentence. W e ﬁrst construct candidate list V n\nc\nfor each ˜xn, and then combine them to obtained\nthe ﬁnal candidate list Vc = V 1\nc ∪ · · · ∪ V N\nc .\nNote that we need to handle multiple masks when\nn > 1. So we ﬁrst ﬁnd k most possible word\npieces for each mask and then enumerate all pos-\nsible combinations to construct the ﬁnal candidate\n3 In BERT convention, the input also needs to be embraced\nwith a [CLS] and a [SEP ] token.\nlist. Speciﬁcally ,\nV n\nc = T op-k{p([MASK ]1 = w|˜xn)}w∈V\n× · · · × T op-k{p([MASK ]n = w|˜xn)}w∈V ,\nwhere V is the whole vocabulary and × means the\nCartesian product.\nThere may be multiple words that make sense\nfor the replacement. In this case, the spelling\nof the wrong word is useful for ﬁnding the most\nlikely correct word. W e use the edit distance to\nselect the most likely correct word further.\nwc = arg min\nw∈Vc\nE(w, xj ),\nwhere E(w, xj ) represent the edit distance be-\ntween w and the noisy word xj .\nAlgorithm 1: Denoising with MLM\nInput: Noisy sentence x = {xi}L\ni=1\nOutput: Denoised sentence x = {xi}L\ni=1\nfor i = 1,2, ..., Ldo\n{x′\nn}N\nn=1 = Masking(x) ;\n{˜xn}N\nn=1 = {Augment(x′\nn,x)}N\nn=1 ;\nfor n = 1,2, ..., Ndo\nV n\nc = Candidate(˜xn) ;\nend\nVc = V 1\nc ∪ · · · ∪ V N\nc ;\nwc = arg minw∈Vc E(w, xj ) ;\nxi = wc;\nend\n3 Experiment\nW e test the performance of the proposed text de-\nnoising method on three downstream tasks: neural\nmachine translation, natural language inference,\nand paraphrase detection. All experiments are\nconducted with NVIDIA T esla V100 GPUs. W e\nuse the pretrained pytorch Bert-large (with whole\nword masking) as the masked language model\n4.\nFor the denoising algorithm, we use at most N =\n4 masks for each word, and the detailed conﬁgu-\nration of the size of the candidate list is shown in\nT able\n2. W e use a large candidate list for one word\npiece which covers the most cases. For multiple\nmasks, a smaller list would be good enough.\nFor all tasks, we train the task-speciﬁc model\non the original clean training set. Then we com-\npare the model performance on the different test\nsets, including original test data, noise test data,\nand cleaned noise test data. W e use a commercial-\nlevel spell-checker api\n5 as our baseline method.\n4 https://github.com/huggingface/pytorch-pretrained-B ERT\n5 https://rapidapi.com/montanaflynn/api/spellcheck;\nNo. of [MASK ] (n) T op k Size\n1 3000 3000\n2 5 25\n3 3 27\n4 2 16\nT otal: 3068\nT able 2: Size of the candidate list\nIn this section, we ﬁrst introduce how the noise\nis generated, and then present experimental results\nof three NLP tasks.\n3.1 Noise\nT o control the noise level, we randomly pick\nwords from the testing data to be perturbed with\na certain probability . For each selected word, we\nconsider two perturbation setting: artiﬁcial noise\nand natural noise. Under artiﬁcial noise setting,\nwe separately apply four kinds of noise: Swap,\nDelete, Replace, Insert with certain probability .\nSpeciﬁcally ,\n• Swap: W e swap two letters per word.\n• Delete: W e randomly delete a letter in the\nmiddle of the word.\n• Replace: W e randomly replace a letter in a\nword with another.\n• Insert: W e randomly insert a letter in the mid-\ndle of the word.\nFollowing the setting in (\nBelinkov and Bisk ,\n2017), the ﬁrst and the last character remains un-\nchanged.\nFor the artiﬁcial noise , we follow the experi-\nment of Belinkov and Bisk (2017) that harvest nat-\nurally occurring errors (typos, misspellings, etc.)\nfrom the edit histories of available corpora. It gen-\nerates a lookup table of all possible errors for each\nword. W e replace the selected words with the cor-\nresponding noise in the lookup table according to\ntheir settings.\n3.2 Neural Machine T ranslation\nW e conduct the English-to-German translation ex-\nperiments on the TED talks corpus from IWSL T\n2014 dataset\n6 . The data contains about 160,000\nsentence pairs for training, 6,750 pairs for testing.\nW e ﬁrst evaluate the performance using a\n12-layer transformer implemented by fairseq\n(\nOtt et al. , 2019). For all implementation details,\nAccess Date: 08/09/2019\n6 https://wit3.fbk.eu/archive/2014-01/texts/en/de/en-\nwe follow the training recipe given by fairseq\n7. W e also evaluate the performance of Google\nTranslate.\nFor the artiﬁcial noise setting, we perturb 20%\nwords and apply each noise with probability 25%.\nFor that natural noise setting, we also perturb\n20% words. All experiment results is summa-\nrized in T able\n3, where we use BLEU score\n(Papineni et al. , 2002) to evaluate the translation\nresult.\nT ext Source Google Fairseq\nOriginal 31.49 28 .06\nArtiﬁcial Noise 28.11 22 .27\n+ Spell-Checker 26.28 21 .15\n+ Ours 28.96 25 .80\nNatural Noise 25.22 17 .29\n+ Spell-Checker 20.90 15 .04\n+ Ours 25.49 21 .40\nT able 3: BLEU scores of EN-to-DE tranlsation\nAs can be seen, both fairseq model and Google\nTranslate suffer from a signiﬁcant performance\ndrop on the noisy texts with both natural and syn-\nthetic noise. When using the spell-checker, the\nperformance even drops more. Moreover, our pur-\nposed method can alleviate the performance drop.\n3.3 Natural Language Inference\nW e test the algorithm on Natural Language In-\nference (NLI) task, which is one of the most\nchallenge tasks related to the semantics of sen-\ntences. W e establish our experiment based on the\nSNLI (the Stanford Natural Language Inference,\nBowman et al. (2015)) corpus. Here we use accu-\nracy as the evaluation metric for SNLI.\nHere we use state-of-the-art 400 dimensional\nHierarchical BiLSTM with Max Pooling (HBMP)\n(\nT alman et al. , 2019). The implementation follows\nthe publicly released code 8 . W e use the same\nnoise setting as the NMT experiments. All results\nare presented in T able\n4. W e observe performance\nimprovement with our method. T o see if the de-\nnoising algorithm would induce noises to the clean\ntexts, we also apply the algorithm to the original\nsentence and check if performance will degrade.\nIt can be seen that, unlike the traditional robust\nmodel approach, applying a denoising algorithm\non a clean sample has little inﬂuence on perfor-\nmance.\n7 https://github.com/pytorch/fairseq/tree/master/examples/translation\n8 https://github.com/Helsinki-NLP/HBMP\nMethod Original Artiﬁcial Natural\nNoise Noise\nHBMP 84.0 75 .0 74 .0\n+Spell-Checker 84.0∗ 63.0 68 .0\n+Ours 83.0∗ 81.0 77 .0\nT able 4: SNLI classiﬁcation accuracy with artiﬁcial\nnoise and natural noise. ∗ : Applying denoising algo-\nrithm on original texts.\nAs shown in the T able\n4, the accuracy is very\nclose to the original one under the artiﬁcial noise.\nNatural noises contain punctuations and are more\ncomplicated than artiﬁcial ones. As a result, infer-\nence becomes much harder in this way .\n3.4 Paraphrase Detection\nW e conducted Paraphrase detection experiments\non the Microsoft Research Paraphrase Corpus\n(MRPC,\nDolan and Brockett (2005)) consisting of\n5800 sentence pairs extracted from news sources\non the web. It is manually labelled for pres-\nence/absence of semantic equivalence.\nW e evaluate the performance using the state-\nof-the-art model: ﬁne-tuned RoBERT a (\nLiu et al. ,\n2019). For all implemented details follows the\npublicly released code 9 . All experiment results\nis summarized in T able 5. W e increase the size of\nthe candidate list to 10000+25+27+16 = 10068\nbecause there are a lot of proper nouns, which are\nhard to predict.\nMethod Original Artiﬁcial Natural\nNoise Noise\nRoBERT a 84.3 81.9 75.2\n+Spell-Checker 82.6 81.3 75.4\n+Ours 83.6 82.7 76.4\nT able 5: Classiﬁcation F1 score on MRPC\n4 Conclusion and Future W ork\nIn this paper, we present a novel text denois-\ning algorithm using ready-to-use masked language\nmodel. W e show that the proposed method can re-\ncover the noisy text by the contextual information\nwithout any training or data. W e further demon-\nstrate the effectiveness of the proposed method on\nthree downstream tasks, where the performance\ndrop is alleviated by our method. A promising fu-\nture research topic is how to design a better can-\ndidate selection rule rather than merely using the\nedit distance. W e can also try to use GEC corpora,\n9 https://github.com/pytorch/fairseq/tree/master/examp \nsuch as CoNLL-2014, to further ﬁne-tune the de-\nnoising model in a supervised way to improve the\nperformance.\nReferences\nY onatan Belinkov and Y onatan Bisk. 2017. Synthetic\nand natural noise both break neural machine transla-\ntion. arXiv preprint arXiv:1711.02173 .\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\narXiv preprint arXiv:1508.05326 .\nShamil Chollampatt and Hwee T ou Ng. 2018. Neural\nquality estimation of grammatical error correction.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n2528–2539.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805 .\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International W orkshop\non P araphrasing (IWP2005) .\nY inhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and V eselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692 .\nV alentin Malykh, V arvara Logacheva, and T aras\nKhakhulin. 2018. Robust word vectors: Context-\ninformed embeddings for noisy texts. In Proceed-\nings of the 2018 EMNLP W orkshop W-NUT : The 4th\nW orkshop on Noisy User-generated T ext , pages 54–\n63.\nKate Mayall, Glyn W Humphreys, and Andrew Olson.\n1997. Disruption to word or letter processing? the\norigins of case-mixing effects. Journal of Experi-\nmental Psychology: Learning, Memory, and Cogni-\ntion, 23(5):1275.\nHwee T ou Ng, Siew Mei Wu, T ed Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. 2014. The conll-2014 shared task on\ngrammatical error correction. In Proceedings of the\nEighteenth Conference on Computational Natural\nLanguage Learning: Shared T ask , pages 1–14.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations .\nKishore Papineni, Salim Roukos, T odd W ard, and W ei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics , pages 311–318. Association for\nComputational Linguistics.\nAarne T alman, Anssi Yli-Jyr ¨ a, and J ¨ org Tiedemann.\n2019. Sentence embeddings in nli with iterative re-\nﬁnement encoders. Natural Language Engineering ,\n25(4):467–482.\nY onghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, W olfgang Macherey,\nMaxim Krikun, Y uan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144 .\nW ei Zhao, Liang W ang, Kewei Shen, Ruoyu Jia, and\nJingming Liu. 2019. Improving grammatical er-\nror correction via pre-training a copy-augmented\narchitecture with unlabeled data. arXiv preprint\narXiv:1903.00138 .",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7956176400184631
    },
    {
      "name": "Retraining",
      "score": 0.6997004747390747
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6495395302772522
    },
    {
      "name": "Noise (video)",
      "score": 0.6328961253166199
    },
    {
      "name": "Noise reduction",
      "score": 0.6315943002700806
    },
    {
      "name": "Language model",
      "score": 0.6225099563598633
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5784772634506226
    },
    {
      "name": "Natural language processing",
      "score": 0.5263782143592834
    },
    {
      "name": "Natural language",
      "score": 0.4649101197719574
    },
    {
      "name": "Machine learning",
      "score": 0.43167075514793396
    },
    {
      "name": "Speech recognition",
      "score": 0.3546364903450012
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "International trade",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ]
}