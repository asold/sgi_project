{
  "title": "Multi-scale convolutional transformer network for motor imagery brain-computer interface",
  "url": "https://openalex.org/W4409444922",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5100710533",
      "name": "Wei Zhao",
      "affiliations": [
        "Jimei University"
      ]
    },
    {
      "id": "https://openalex.org/A5081767501",
      "name": "Baocan Zhang",
      "affiliations": [
        "Jimei University"
      ]
    },
    {
      "id": "https://openalex.org/A5101602014",
      "name": "Haifeng Zhou",
      "affiliations": [
        "Jimei University"
      ]
    },
    {
      "id": "https://openalex.org/A5090782321",
      "name": "Dezhi Wei",
      "affiliations": [
        "Jimei University"
      ]
    },
    {
      "id": "https://openalex.org/A5038557529",
      "name": "Chenxi Huang",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A5007210627",
      "name": "Quan Lan",
      "affiliations": [
        null,
        "First Affiliated Hospital of Xiamen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2509948470",
    "https://openalex.org/W2540507509",
    "https://openalex.org/W2900802277",
    "https://openalex.org/W2912885887",
    "https://openalex.org/W2144383914",
    "https://openalex.org/W3121810080",
    "https://openalex.org/W4300942166",
    "https://openalex.org/W3209357409",
    "https://openalex.org/W2142280324",
    "https://openalex.org/W2132360759",
    "https://openalex.org/W2791830995",
    "https://openalex.org/W2521878393",
    "https://openalex.org/W2942101380",
    "https://openalex.org/W2967744086",
    "https://openalex.org/W2056873714",
    "https://openalex.org/W2960585436",
    "https://openalex.org/W2096597330",
    "https://openalex.org/W3102856398",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2741907166",
    "https://openalex.org/W2915893085",
    "https://openalex.org/W4221022811",
    "https://openalex.org/W4399740058",
    "https://openalex.org/W2559463885",
    "https://openalex.org/W3082414409",
    "https://openalex.org/W2907080401",
    "https://openalex.org/W2994422921",
    "https://openalex.org/W2896120927",
    "https://openalex.org/W4289538860",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2792724009",
    "https://openalex.org/W4293550306",
    "https://openalex.org/W2954214015",
    "https://openalex.org/W2971518519",
    "https://openalex.org/W3129964406",
    "https://openalex.org/W4206554586",
    "https://openalex.org/W4206927580",
    "https://openalex.org/W4205466227",
    "https://openalex.org/W4312597583",
    "https://openalex.org/W4402037270",
    "https://openalex.org/W4312892255",
    "https://openalex.org/W4389692426",
    "https://openalex.org/W1610928686",
    "https://openalex.org/W3102455230",
    "https://openalex.org/W4207043402"
  ],
  "abstract": "Abstract Brain-computer interface (BCI) systems allow users to communicate with external devices by translating neural signals into real-time commands. Convolutional neural networks (CNNs) have been effectively utilized for decoding motor imagery electroencephalography (MI-EEG) signals in BCIs. However, traditional CNN-based methods face challenges such as individual variability in EEG signals and the limited receptive fields of CNNs. This study presents the Multi-Scale Convolutional Transformer (MSCFormer) model that integrates multiple CNN branches for multi-scale feature extraction and a Transformer module to capture global dependencies, followed by a fully connected layer for classification. The multi-branch multi-scale CNN structure effectively addresses individual variability in EEG signals, enhancing the model’s generalization capabilities, while the Transformer encoder strengthens global feature integration and improves decoding performance. Extensive experiments on the BCI IV-2a and IV-2b datasets show that MSCFormer achieves average accuracies of 82.95% (BCI IV-2a) and 88.00% (BCI IV-2b), with kappa values of 0.7726 and 0.7599 in five-fold cross-validation, surpassing several state-of-the-art methods. These results highlight MSCFormer’s robustness and accuracy, underscoring its potential in EEG-based BCI applications. The code has been released in https://github.com/snailpt/MSCFormer .",
  "full_text": "Multi-scale convolutional \ntransformer network for motor \nimagery brain-computer interface\nWei Zhao1, Baocan Zhang1, Haifeng Zhou2, Dezhi Wei1, Chenxi Huang3 & Quan Lan4,5\nBrain-computer interface (BCI) systems allow users to communicate with external devices by \ntranslating neural signals into real-time commands. Convolutional neural networks (CNNs) have been \neffectively utilized for decoding motor imagery electroencephalography (MI-EEG) signals in BCIs. \nHowever, traditional CNN-based methods face challenges such as individual variability in EEG signals \nand the limited receptive fields of CNNs. This study presents the Multi-Scale Convolutional Transformer \n(MSCFormer) model that integrates multiple CNN branches for multi-scale feature extraction and \na Transformer module to capture global dependencies, followed by a fully connected layer for \nclassification. The multi-branch multi-scale CNN structure effectively addresses individual variability \nin EEG signals, enhancing the model’s generalization capabilities, while the Transformer encoder \nstrengthens global feature integration and improves decoding performance. Extensive experiments \non the BCI IV-2a and IV-2b datasets show that MSCFormer achieves average accuracies of 82.95% (BCI \nIV-2a) and 88.00% (BCI IV-2b), with kappa values of 0.7726 and 0.7599 in five-fold cross-validation, \nsurpassing several state-of-the-art methods. These results highlight MSCFormer’s robustness and \naccuracy, underscoring its potential in EEG-based BCI applications. The code has been released in \nhttps://github.com/snailpt/MSCFormer.\nKeywords Brain-computer interface (BCI), Convolutional neural networks (CNNs), \nElectroencephalography (EEG), Motor imagery (MI), Transformer\nBrain-computer interface (BCI) technology has opened new avenues for direct communication between \nthe brain and external devices 1. BCI systems predominantly rely on various neural signal technologies, \nsuch as functional magnetic resonance imaging, electroencephalography (EEG), electrocorticography, and \nmagnetoencephalography, to monitor and interpret brain activity patterns 2. EEG technology, which records \nelectrical activity in the brain via electrodes placed on the scalp, is particularly valued for its high temporal \nresolution, non-invasiveness, cost-effectiveness, and ease of use 3. The motor imagery (MI) BCI paradigm is \nparticularly noteworthy as it enables control of external devices through the mental simulation of specific \nmovements (e.g., hand or foot movements) without actual execution. The MI-EEG paradigm has become a key \ntechnology in neurorehabilitation4, prosthetic control5, and human-computer interaction6.\nDespite its promising applications, the MI-EEG paradigm faces significant challenges in accurately decoding \nuser intentions7. EEG signals have a very low signal-to-noise ratio and are highly susceptible to interference, \nincluding electromyographic noise, environmental electromagnetic interference, and ocular artifacts, all of \nwhich degrade signal quality and decoding accuracy8. Additionally, when imagining the same motor task, EEG \nsignals exhibit significant variability not only between individuals but also within the same individual at different \ntimes.\nTraditional machine learning techniques have been widely applied to MI-EEG classification, typically \nemploying a two-stage pipeline of feature extraction and classifier training. Among these, common spatial pattern \n(CSP) and its extension, filter bank CSP (FBCSP), are widely used for spatial filtering and frequency-specific \nfeature extraction9,10. Other feature extraction methods, such as power spectral density, wavelet transform, and \nshort-time Fourier transform (STFT), have been explored to characterize EEG signals in different domains11–13, \nwhile non-linear measures like approximate entropy and fractal dimension aim to capture signal complexity14,15. \n1Chengyi College, Jimei University, Xiamen 361021, China. 2School of Marine Engineering, Jimei University, Xiamen \n361021, China. 3School of Informatics, Xiamen University, Xiamen 361005, China. 4Department of Neurology, \nDepartment of Neuroscience, School of Medicine, The First Affiliated Hospital of Xiamen University, Xiamen \nUniversity, Xiamen 361005, China. 5Fujian Key Laboratory of Brain Tumors Diagnosis and Precision Treatment, \nXiamen 361005, China. email: zhfeng216@163.com; xmdylanquan@163.com\nOPEN\nScientific Reports |        (2025) 15:12935 1| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports\n\nClassification is then performed using machine learning algorithms, including support vector machines, linear \ndiscriminant analysis, and k-nearest neighbors16–18.\nDespite their effectiveness, traditional approaches heavily rely on handcrafted features, making them sensitive \nto inter-subject variability and limiting their adaptability. Furthermore, the separation of feature extraction and \nclassification hinders joint optimization, potentially reducing classification performance 19,20. Deep learning \n(DL)-based approaches overcome these limitations by automatically learning task-relevant features from raw \nEEG signals, enabling end-to-end optimization and reducing reliance on manual feature engineering21–23.\nCNNs have become the dominant architecture in DL-based MI-EEG decoding, with recent studies \ninvestigating various convolution techniques, kernel sizes, and network depths. For instance, Schirrmeister et \nal.20 developed end-to-end ConvNets that outperformed FBCSP-based methods by directly learning hierarchical \nfeature representations from raw EEG signals. Lawhern et al.24 introduced EEGNet, a compact CNN architecture \nwith strong generalization across multiple BCI paradigms. To enhance feature extraction, Mane et al.25 proposed \na filter-bank CNN that applies bandpass filtering (BPF) to EEG signals, while Wang et al. 26 and Lee & Choi 27 \nexplored transform-domain CNNs, leveraging short-time Fourier transform (STFT) and continuous wavelet \ntransform for MI-EEG classification.\nRecent advancements show that while single-scale CNNs (SSCNNs) perform well in MI-EEG decoding, their \nlimited ability to capture the complex spatiotemporal characteristics of EEG signals makes them less effective in \nhandling inter-individual variability. Additionally, their restricted receptive field may limit the capture of long-\nrange dependencies, potentially affecting decoding accuracy. LSTM-based models 28 address this limitation by \ncapturing temporal dependencies, but their sequential nature prevents efficient parallelization and can suffer \nfrom vanishing gradients in long EEG sequences. Meanwhile, Transformer-based models29,30 effectively capture \nglobal dependencies, but they may struggle to extract fine-grained local features, which are crucial for MI-EEG \nclassification. Furthermore, training DL models require large amounts of labeled data, yet obtaining high-quality \nMI-EEG data is both time-consuming and resource-intensive due to the lengthy experimental protocols and \nhigh demands on subjects.\nTo tackle these challenges, this study proposes the Multi-Scale Convolutional Transformer (MSCFormer), \nwhich leverages CNNs for local spatial-temporal feature extraction and a Transformer encoder for global \ndependency modeling. The multi-scale CNN module addresses inter-subject variability by extracting features at \ndifferent temporal scales, while the Transformer module mitigates CNN’s receptive field limitations by modeling \nlong-range dependencies. Furthermore, data augmentation techniques are incorporated to enhance model \ngeneralization given the limited availability of EEG training data. Extensive experiments on the BCI IV-2a and \nIV-2b datasets validate the effectiveness of MSCFormer, demonstrating superior classification accuracy and \nrobustness compared to existing methods. The following are the main contributions of this study:\n (1) This work proposes a novel end-to-end hybrid deep learning architecture that improves MI-EEG decoding \nperformance by combining the local feature extraction of multi-scale CNNs with the global dependency \nmodeling of the Transformer’s self-attention mechanism, capturing both detailed local features and broader \ndependencies.\n (2) This work outperforms multiple state-of-the-art (SOTA) methods in decoding performance on the BCI \nCompetition IV-2a and IV-2b datasets, demonstrating its potential as a new benchmark for EEG decoding.\n (3) Comprehensive experiments were conducted to examine the impact of the hyperparameters in the convo-\nlution and Transformer modules, as well as data augmentation.\n (4) To promote reproducibility and support further research, the MSCFormer source code has been made \npublicly available at https://github.com/snailpt/MSCFormer.\nThe rest of this paper is organized as follows: Sect. 2 provides a comprehensive review of related work. Section 3 \nintroduces the datasets, data preprocessing methods, and data augmentation techniques, followed by a detailed \ndescription of the proposed model architecture. Section  4 evaluates the performance of the model through \nextensive experimentation. Section 5 discusses our main findings, and finally, conclusions are drawn.\nRelated work\nIn this section, we describe the key techniques involved in our proposed method, including multi-scale CNN \n(MSCNN)-based approaches and Transformer-based networks. A comparative summary of representative \nworks is provided in Table 1.\nMSCNN-based approaches\nOver the past decade, CNNs have achieved remarkable success in computer vision, largely due to their ability \nto autonomously learn both local and global features through convolution operations. When processing EEG \nsequence signals, convolution operations are also effective in capturing temporal and spatial features, which are \ncritical for decoding brain signals. Consequently, CNNs have been widely applied in BCI 31,32. CNN-based MI-\nEEG decoding methods encompass both SSCNNs and MSCNNs. SSCNNs extract features from EEG signals \nusing a single temporal and spatial convolution scale. However, MI-EEG signals inherently exhibit complex \nspatiotemporal patterns and multi-band frequency characteristics, which cannot be fully captured by a fixed-\nscale temporal convolution. Moreover, due to significant inter-subject variability in EEG signals, the optimal \nconvolution scale often varies across individuals.\nTo address these limitations, MSCNNs perform convolutions across multiple scales, allowing for a more \ncomprehensive capture of the complex characteristics in EEG signals and improving classification performance. \nFor example, Amin et al. 33 proposed MCNN, a multi-layer CNN fusion method that integrates four parallel \nCNN streams with varying depths and kernel sizes. By leveraging transfer learning and feature fusion, MCNN \nScientific Reports |        (2025) 15:12935 2| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\nenhances MI-EEG classification accuracy by capturing diverse spatiotemporal patterns in EEG signals. Dai et \nal.34 proposed HS-CNN, which decomposes raw EEG signals into θ , µ , and β bands to address inter-subject \nvariability, and employs hybrid-scale convolution kernels (1 × 45, 1 × 65, 1 × 85) combined with a time-frequency \ndata augmentation method to achieve SOTA classification accuracy. However, the large parameter count \n(> 420 K per filter band) restricts the system’s applicability. Jia et al.35 developed a multi-branch multi-scale CNN \n(MMCNN) for MI-EEG classification, using five parallel EEG Inception Networks (EINs) with varying kernel \nscales to capture diverse frequency information, and the squeeze-and-excitation (SE) attention mechanism to \nenhance performance by reweighting channel features. Altuwaijri and Muhammad 36 proposed MBEEGNet, \ncomposed of multiple EEGNets with different configurations, and MBShallowConvNet, made up of multiple \ndistinct ShallowConvNets. Their multi-branch structure allows for more comprehensive EEG signal feature \nextraction, overcoming the single-scale limitation of traditional methods and leading to superior classification \nperformance across multiple datasets. Roy et al.37 proposed a multi-scale CNN, which filters EEG signals into δ , \nθ , α , and β  bands, applying multi-scale convolution blocks with varying kernel sizes to each band. The model \nalso incorporates user-specific features such as differential entropy (DE) and neural power spectrum (NPS), \nfurther enhancing performance.\nTransformer-based networks\nWhile MSCNNs capture more information than SSCNNs, their limited receptive field restricts the modeling of \nlong-term dependencies, limiting further improvements in MI-EEG decoding. In contrast, the self-attention \nmechanism in the Transformer architecture, with its global receptive field, effectively captures global dependencies \nand enhances decoding performance. Song et al. 38 proposed the Spatial-Temporal Tiny Transformer (S3T) for \nEEG decoding, addressing the limitations of CNNs in capturing global dependencies. Their model applies spatial \nfiltering before utilizing self-attention along the feature channel and temporal dimensions to enhance relevant \nfeatures. Tao et al. 39 proposed the Gated Transformer, a family of Transformer models incorporating various \ngating mechanisms to enhance EEG classification. By replacing standard residual connections with different \ngating mechanisms, their approach stabilizes training and improves long-term dependency modeling in EEG \nsequences. Xie et al. 29 proposed a Transformer-based deep learning framework for MI-EEG classification, \nincorporating both spatial and temporal dependencies. Their study introduced five Transformer-based models, \nexplored three types of positional embeddings, and achieved SOTA accuracy on the PhysioNet EEG Motor \nImagery Dataset. Song et al.40 proposed EEG Conformer, a Convolutional Transformer model for unified EEG \ndecoding that integrates CNN-based local feature extraction with Transformer-based global feature learning. \nInspired by the Conformer model, Zhao et al.41 proposed CTNet, which integrates a single-scale CNN module \nRelated work Methods Database Accuracy % Comment\nAmin et al. 201933 MCNN BCI IV-2a\nHigh Gamma Dataset\n75.7\n95.4\nMulti-branch CNN with varying depths/kernels, \nusing transfer learning and feature fusion.\nDai et al. 201934 HS-CNN BCI IV-2a\nBCI IV-2b\n91.57 ± 5.41\n87.64 ± 8.00\nHybrid-scale CNN decomposing EEG into θ, µ, and β \nbands with time-frequency data augmentation.\nJia et al. 202035 MMCNN BCI IV-2a\nBCI IV-2b\n81.4 ± 11.7\n84.4 ± 7.5\nMulti-branch CNN with varying kernel sizes and SE \nattention.\nAltuwaijri and \nMuhammad 202236\nMBShallowConvNet / \nMBEEGNet\nBCI IV-2a\nHigh Gamma Dataset\n81.15 ± 9.04\n95.11 ± 4.62 / \n82.01 ± 10.13\n95.30 ± 3.50\nExtensions of ShallowConvNet and EEGNet, \nrespectively, incorporating three CNN branches \nwith different configurations to enhance multi-scale \nfeature extraction.\nRoy et al. 202237 MS-CNN BCI IV-2b 93.74 ± 2.80\nUtilizes four-band decomposition, multi-scale \nconvolution, and user-specific DE and NPS features \nfor feature extraction.\nSong et al. 202138 S3T BCI IV-2a\nBCI IV-2b\n82.59 ± 12.52\n84.26 ± 10.03\nA Transformer-based method with spatial filtering \nand self-attention for spatiotemporal feature learning.\nTao et al. 202139 GRUGate Transformer Brain-Visual Dataset\nPhysioNet\n61.96 ± 10.09\n55.40 ± 2.09\nA Transformer-based method with gating \nmechanisms to enhance stability and long-term \nfeature extraction.\nXie et al. 202229 s-Trans / t-Trans / s-CTrans / \nt-CTrans / f-CTrans PhysioNet\nBest accuracy:\n83.31 (2-class), 74.44 \n(3-class), 64.22 (4-class)\nProposed five Transformer-CNN hybrid models \nintegrating spatiotemporal dependencies with \noptimized positional embeddings.\nSong et al. 202340 Conformer\nBCI IV-2a\nBCI IV-2b\nSEED\n78.66 ± 14.43\n84.63 ± 11.49\n95.30\nA Convolutional Transformer model integrating \nCNN for local feature extraction and Transformer for \nglobal dependency modeling.\nZhao et al. 202441 CTNet BCI IV-2a\nBCI IV-2b\n82.52 ± 9.61\n88.49 ± 9.03\nA hybrid CNN-Transformer model for MI-\nEEG classification, enhancing spatiotemporal \nrepresentation learning.\nAhn et al. 202342 MS-TSformer-DS\nPrivate EEG \nBCI IV-2a\nASU\n62 ± 6\n70 ± 9\n70 ± 7\nA hybrid CNN-Transformer model integrating \nmulti-scale temporal convolution, temporal-spatial \nTransformer, and dual-stream spatial learning.\nTao et al. 202443 ADFCNN\nBCI IV-2a\nBCI IV-2b\nOpenBMI\n79.39 ± 10.23\n87.81 ± 8.40\n65.26 ± 13.50\nA dual-scale CNN integrating self-attention for \nenhanced spectral-spatial fusion.\nTable 1. Summary of related work.\n \nScientific Reports |        (2025) 15:12935 3| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\nsimilar to EEGNet for local feature extraction and incorporates a Transformer for global feature modeling, \nleading to enhanced classification performance.\nAdditionally, some researchers have begun exploring EEG decoding methods that integrate multi-scale CNNs \nwith the self-attention mechanism. Ahn et al.42 proposed MS-TSformer-DS, a hybrid EEG decoding model that \ncombines multi-scale convolutional blocks, temporal-spatial Transformer encoders, and a dual-stream spatial \nlearner to enhance spatial feature representation. Their model demonstrated strong performance across multiple \nEEG datasets, including a private dataset, BCI Competition IV-2a, and the Arizona State University (ASU) \ndataset. Tao et al.43 proposed ADFCNN, an attention-based dual-scale fusion CNN for MI-EEG classification. It \nemploys dual-scale temporal and spatial convolutions to extract spectral-spatial features, while a self-attention \nmechanism enhances feature fusion by capturing cross-scale dependencies.\nBuilding on these previous studies, our research makes further contributions to the field. Inspired by these \nprior studies, we propose MSCFormer as an effective solution for MI-EEG decoding.\nMaterials and methods\nDatasets\nTo evaluate the effectiveness of our proposed model, we used two publicly available benchmark datasets: BCI IV-\n2a and IV-2b. Detailed descriptions of these datasets, preprocessing, and data augmentation are provided below.\n (1) BCI IV-2a dataset (2a): The dataset comprises EEG recordings from nine subjects (A01-A09), each engaged \nin four distinct motor imagery tasks: imagining movements of the left-hand, right-hand, both feet and \ntongue. Each subject participated in two recording sessions conducted on different days, yielding a total \nof 288 trials per session. The EEG recordings were obtained using 22 electrodes, with a sampling rate of \n250 Hz, and each recording lasted for 7 s. In our experiments, we utilized the temporal segment from 2 to \n6 s. Each trial was represented as a matrix of dimensions (22, 1000).\n (2) BCI IV-2b dataset (2b): The dataset comprises EEG recordings from nine right-handed subjects (B01-B09) \nover five sessions, with about 720 trials per subject. The first two sessions lack feedback, whereas the subse-\nquent three sessions provide feedback. Each session includes multiple runs in which subjects imagine left- \nor right-hand movements. The EEG recordings were captured using three bipolar channels at a sampling \nfrequency of 250 Hz. In our experiments, we utilized the temporal segment from 3 to 7 s. Each trial was \nrepresented as a matrix with dimensions (3, 1000).\n (3) Data Preprocessing: The raw EEG recordings are defined as {( Xi,y i )| i = 1,2,...,N }, where Xi ∈ R\nC×T represents the i-th trial consisting of C channels and T sampling time points, yi is the sample label cor-\nresponding to Xi, and N is the total number of trials. In this study, we employed a zero-mean standardiza-\ntion (STD) method for preprocessing the EEG recordings. Notably, we did not apply any band-pass filtering \nor artifact removal techniques. The zero-mean standardization method was used to reduce the influence of \nsignal amplitude variations and enhance the robustness of signal processing and classification algorithms \nby ensuring that the data were on a consistent scale. The calculation method is expressed as follows:\n \n∼\nXi= Xi − µ\nσ  (1)\n where \n∼\nXi is the normalized EEG signal. µ  and σ  denote the mean and standard deviation (S.D.) of the raw \nEEG data, respectively, calculated using the training dataset and then applied directly to the test dataset.\n (4) Data Augmentation: Given the stringent criteria for participant recruitment and the complexities of experi-\nmental setups, it is inherently challenging to acquire substantial, high-quality EEG data. DL models trained \non such small datasets are particularly susceptible to overfitting. Therefore, implementing data augmenta-\ntion techniques to enhance the generalizability and robustness of MI-EEG models is essential. We employ \nthe segmentation and reconstruction (S&R) method44 in the time domain to augment training data, which \ninvolves a systematic approach to artificially increasing the amount and variability of EEG training datasets. \nThe S&R method includes two steps: segmentation and reconstruction. The workflow of the S&R method \nis shown in Fig.  1. In the segmentation phase, each EEG trial is divided into Ns segments based on time \nintervals, ensuring that each segment captures a subset of the entire signal’s timeframe. As shown in Fig. 1, \nNs equals 3, indicating each EEG trial is segmented into three parts, labeled as A, B, and C, respectively. In \nthe reconstruction phase, new artificial EEG trials are generated by randomly recombining these segments \nin a manner that aligns with the natural progression of time within EEG recordings. The number of training \nsamples augmented is denoted as NA. This approach not only increases data diversity by mixing segments \nfrom various trials, introducing new patterns for the model to learn but also maintaining the temporal \nstructure of the EEG signals, which is crucial for retaining the physiological relevance of the EEG data.\nOverview of the proposed framework\nCNNs are effective at capturing local features, while Transformer networks excel at modeling global dependencies. \nIn this paper, we introduce MSCFormer, an end-to-end MI-EEG classification framework that integrates CNNs \nand the Transformer encoder to leverage both architectures’ strengths. The overall framework of MSCFormer is \ndepicted in Fig. 2. MSCFormer first employs a convolution module to extract spatiotemporal features at various \nscales from the EEG data. The Transformer module then applies a multi-head self-attention mechanism (MHA) \nto model global dependencies across these multi-scale features, dynamically emphasizing the most relevant \nScientific Reports |        (2025) 15:12935 4| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\nfeatures for classification. Finally, a fully connected layer classifies the extracted features into distinct categories, \ncompleting the MI-EEG classification process.\nConvolution module\nThe convolution module includes three CNN branches, each similar to the Shallow ConvNet proposed in 20. \nHowever, each branch uses spatial depthwise convolution instead of spatial standard convolution, which has \ndemonstrated better performance. Additionally, each branch employs different hyperparameters compared \nto those in 20. The distinction between the three branches lies in the different kernel sizes used for temporal \nconvolution, enabling the learning of EEG features at various time scales.\nEach branch in the convolution module comprises a temporal convolution layer, a spatial depthwise \nconvolution layer, and an average pooling layer, as illustrated in Fig. 2. The temporal convolution layer utilizes \nF1 temporal filters with a kernel size of (1, Kc). Different values of Kc can capture EEG temporal features at \nvarious scales. Following the setup in reference34, Kc values for the three CNN branches are set to 85, 65, and 45, \nrespectively. The spatial depthwise convolution layer independently applies spatial convolutions to each temporal \nfeature map, effectively learning spatial filters associated with specific frequency bands. Additionally, it reduces \nthe number of training parameters, lowering model complexity and computational resource requirements. The \nspatial convolution kernel size is ( C, 1), producing F1 feature maps that integrate both temporal and spatial \nfeatures with a shape of (F1, T). Following the spatial convolution, a batch normalization (BN) layer is applied, \nstabilizing the data distribution and facilitating smoother gradient flow, thereby improving training efficiency \nand effectiveness. Subsequently, an exponential linear unit (ELU) activation function is applied. An average \npooling layer with a kernel size of (1, P) is then used. This pooling step not only reduces the feature map \nFig. 1. Principle of the S&R data augmentation method in time domain.\n \nScientific Reports |        (2025) 15:12935 5| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\ndimensions but also smooths the spatiotemporal feature maps, reducing local noise and aiding the model in \nlearning global features more effectively. Each branch applies a dropout operation with a probability of 0.5 after \nthe average pooling layer to reduce overfitting. Therefore, the feature dimension output by each CNN branch is \n(F1, Tp), where Tp represents the length of the feature series and is given by T divided by P. Next, the feature maps \nobtained from the three CNN branches are transposed by swapping the convolution feature channel dimension \nwith the time dimension. These feature maps are then concatenated along the feature channel dimension, \nresulting in a fused feature map XF with dimensions (Tp, F2), where F2 is equal to three times F1. In this way, the \nfused feature maps at each temporal point are fed as tokens into the subsequent Transformer module.\nTransformer module\nTo further enhance the features extracted by the CNN module, we employ a Transformer module to model \nthe global dependencies of the multi-scale MI-EEG features. The self-attention mechanism in the Transformer \nmodule provides a global receptive field, enabling the model to capture long-range dependencies. It dynamically \nprioritizes the most relevant features for classification, ensuring the model focuses on the most informative \naspects. To quantify the attention allocated by the model to different feature channels, we introduce an additional \nlearnable vector ( F0\n0 ) as a class token, similar to the class token used in BERT 45, appended to the front of the \nfeature maps. Subsequently, the learnable position embedding Fpos are added to the sequence of features to retain \npositional information. Therefore, the feature embedding F0 serves as the input to the Transformer encoder, \nwhere F0 is calculated as follows.\n F0 =[ F0\n0 ,X F ]+ Fpos (2)\nThe features are then encoded using an L-layer deep Transformer encoder, where the class token ( F0\nL) from \nthe L-th layer represents the output of the Transformer module, serving as the MI-EEG’s feature representation.\nThe Transformer encoder consists of MHA and feed forward (FF) blocks. MHA comprises multiple self-\nattention layers, known as heads, which implement scaled dot-product attention, as illustrated in Fig.  3. Each \nself-attention layer consists of three main components: query Q, keys K, and values V . Q, K, and V  are \ncomputed from the input features F through linear transformations. Specifically, Qi, Ki, and Vi at the i-th head \nof the self-attention layer are calculated using the following formulas :\nFig. 2. The framework of proposed MSCFormer, including a convolution module, a Transformer module, and \na classifier module.\n \nScientific Reports |        (2025) 15:12935 6| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\n Qi = FiWQ\ni  (3)\n Ki = FiWK\ni  (4)\n Vi = FiWV\ni  (5)\nWhere WQ\ni , WK\ni  and WV\ni  are learnable parameters in the linear transformation. The self-attention scores are \ncalculated using the dot product of the Q and K matrices, scaled by the square root of the dimension of the K \n(dk) to prevent the scores from becoming too large:\n \nSA(Qi,K i,V i)= softmax\n(\nQiKT\ni√dk\n)\nVi (6)\nThe softmax function is applied to normalize these scores into probabilities. Finally, these probabilities are used \nto perform a weighted sum of the value vectors. MHA enables the model to focus simultaneously on information \nfrom different representation subspaces at various locations, enhancing its ability to capture complex patterns \nand relationships among EEG features. It executes multiple self-attention operations in parallel and then projects \nits concatenated output.\n MHA(Q, K, V)= Concat (SA( Q1,K 1,V 1), ..., SA(Qh,K h,V h )) WO (7)\nwhere WO is a learnable weight matrix. The output of the MHA block is typically followed by a residual \nconnection and layer normalization (LN):\n OMHA = LN(MHA(Q, K, V)+ F) .\nThe FF block consists of two linear transformations with a Gaussian error linear unit (GELU) activation function \nin between:\n FFN(OMHA )= GELU(OMHA W1 + b1)W2 + b2 (9)\nwhere W1 and W2 are weight matrices, and b1 and b2 are bias terms. Similar to the MHA block, the output of the \nFF block is accompanied by a residual connection and followed by LN:\n O = LN(FFN(OMHA )+ OMHA ) .\nFig. 3. Multi-head self-attention.\n \nScientific Reports |        (2025) 15:12935 7| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\nTherefore, the MI-EEG feature representation F0\nL in FL serves as the output of the Transformer module and also \nacts as the input to the classifier module.\nClassifier module\nThe classifier module consists of a fully connected layer with a softmax function, where the number of neurons \nM is set to match the number of classes in the classification task. To reduce overfitting, dropout is applied to the \ninput features before classification, with the dropout rate set at 0.25. The cross-entropy loss is employed as the \nloss function for model training, which is expressed as:\n Loss(y, ˆy)= − 1\nN\n∑\nN\ni=1\n∑\nM\nc=1ylog (ˆy) (11)\nwhere y represents the actual labels, and ˆy represents the predicted labels.\nPerformance metrics\nWe employ the most commonly used metrics, accuracy and kappa, for the evaluation of the MI-EEG classification \nmethod. Accuracy is defined as follows:\n Accuracy = TP + TN\nTP + TN + FP + FN  (12)\nwhere TP and TN represent true positives and true negatives, respectively, while FP and FN represent false \npositives and false negatives. The kappa coefficient is a normalized measure that takes into account the chance \nlevel and is defined as follows:\n \nkappa = po − pe\n1 − pe\n (13)\nwhere po denotes the observed accuracy (the average accuracy across all the trials) and pe denotes the expected \naccuracy (the accuracy of a random guess). Generally, the higher the accuracy and kappa, the better the model’s \nclassification performance. The Wilcoxon signed-rank test is used to assess statistical significance. A p-value \ngreater than 0.05 indicates the absence of a statistically significant difference. Conversely, a p-value less than 0.05 \n(denoted as ‘*’) signifies a significant difference, while a p-value less than 0.01 (denoted as ‘**’) indicates a highly \nsignificant difference.\nExperiments and results\nExperiment settings\nOur method is implemented in PyTorch and utilizes an Intel Core i9-9820X CPU and an NVIDIA RTX 4090 \nGPU. We classify the BCI IV-2a and IV-2b datasets using only EEG channel data, entirely discarding the three \nEOG channels in our experiments. We conducted subject-specific classification experiments and adhered to the \ndata division scheme outlined in the competition guidelines. For the BCI IV-2a dataset, we used the first session \nas the training set and the second session as the test set. For the BCI IV-2b dataset, we used the first three sessions \nas the training set and the last two sessions as the test set. To evaluate the stability and generalization ability of \nour model, we performed five-fold cross-validation (CV) on the original training set. We divided the original \ntraining set into five approximately equal subsets. Then, we used one subset as the validation set and combined \nthe remaining four subsets with the S&R augmented dataset for training. We selected the model with the lowest \nloss on the validation set as the best model and tested it on the test set. This process was repeated for each of the \nfive original training subsets, and the final performance metric was obtained by averaging the results on the test \nset.\nDuring S&R data augmentation, each EEG trial is segmented into eight segments (Ns = 8). We use the Adam \noptimizer to train the model, with the learning rate, β1, and β2 set to 0.001, 0.5, and 0.999, respectively. The batch \nsize and number of epochs for training are set to 288 and 1000, respectively. These training hyperparameter \nsettings are adopted based on the guidelines in 41. Given the fewer electrode channels in the BCI IV-2b dataset, \nwe applied L2 regularization to reduce overfitting, with the weight decay parameter set to 0.001. The three \nconvolutional kernel sizes in the convolution module were adopted from34, while the remaining hyperparameters \nof the MSCFormer architecture were determined through extensive experiments. Unless specified otherwise, the \nhyperparameters of the MSCFormer architecture are detailed in Table 2.\nAblation study\nTo systematically examine the impact of the Transformer module, data augmentation, and temporal convolution \nkernel sizes within the MSCFormer framework, we conducted a series of rigorous ablation studies on the BCI \nIV-2a and IV-2b datasets. The ablation study was conducted using five distinct experimental configurations: (1) \nthe fully integrated MSCFormer model, (2) the model without the Transformer module (w/o Trans), in which \nmulti-scale CNN features are concatenated, flattened, and fed into the classifier module, (3) the model without \ndata augmentation (w/o Aug), (4) the model lacking both the Transformer and data augmentation (w/o Trans & \nAug), and (5) the fully integrated model with smaller convolutional kernels (w/ Small-K), which utilizes reduced \ntemporal convolution kernel sizes (64, 32, 16), as adopted in22,36.\nFigure 4 illustrates the average decoding accuracies for each subject obtained through a five-fold CV across \nthe specified configurations. MSCFormer consistently achieved the highest average decoding accuracies on \nScientific Reports |        (2025) 15:12935 8| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\nboth datasets, reaching 82.60% on the BCI IV-2a dataset and 88.00% on the BCI IV-2b dataset. Removing the \nTransformer module resulted in a 3.30% decrease in average accuracy on the BCI IV-2a dataset ( p = 0.055), \nwith particularly substantial drops observed in subjects A04, A05, A06, and A07, where accuracies decreased \nby 8.40%, 5.63%, 8.13%, and 5.76%, respectively. On the BCI IV-2b dataset, removing the Transformer led to a \nnotable decrease in average accuracy of 2.38% (p < 0.05). Without data augmentation, removing the Transformer \nmodule reduced the average classification accuracy by 5.69% ( p < 0.01) on the BCI IV-2a dataset and by 1.06% \non the BCI IV-2b dataset. Removing data augmentation alone led to significant decreases in recognition \naccuracy on both the BCI IV-2a and BCI IV-2b datasets, with significant reductions of 9.57% ( p < 0.01) and \n4.12% ( p < 0.01), respectively. Simultaneously removing the Transformer module and data augmentation led \nto significant decreases in average recognition accuracy: 15.26% ( p < 0.01) on the BCI IV-2a dataset and 5.18% \n(p < 0.01) on the BCI IV-2b dataset. These results demonstrate the critical role of the Transformer module and \ndata augmentation in enhancing the decoding accuracy of the MSCFormer model. Furthermore, using smaller \nconvolutional kernels in the convolution module led to a significant decrease in average classification accuracy, \nwith a drop of 2.21% (p < 0.05) on BCI IV-2a and 1.45% (p < 0.01) on BCI IV-2b.\nFigure 5 provides a comparative exposition of kappa coefficients across the various experimental conditions \nfor each subject. The comprehensive MSCFormer setup outperformed all other conditions across both datasets, \nachieving the highest cumulative kappa. This result highlights the essential roles of both the Transformer \nand data augmentation in enhancing the model’s robustness and consistency. Additionally, the use of larger \nconvolutional kernels yielded superior performance.\nImpact of the depth of transformer\nTypically, within the Transformer encoder module, the depth L of the Transformer significantly influences model \nperformance. Figure 6 depicts the evolution of recognition accuracy with increasing depths. On the BCI IV-2a \ndataset, the model achieves the highest average accuracy at a depth of 5, which is 2.94% higher than at depth \n1 (p < 0.05). Beyond this, further increasing the Transformer depth results in a decline in accuracy. Similarly, \non the BCI IV-2b dataset, the average accuracy at depth 5 is notably improved by 0.68% compared to depth 3 \nFig. 4. Radar chart visualization of ablation effects on average accuracy.\n \nTable 2. The hyperparameters of MSCFormer architecture.\n \nScientific Reports |        (2025) 15:12935 9| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\nFig. 6. The impact of Transformer encoder depth on accuracy.\n \nFig. 5. Stacked bar chart of ablation study on average kappa coefficients.\n \nScientific Reports |        (2025) 15:12935 10| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\n(p < 0.05). These findings suggest that while increasing depth improves the model’s ability to capture complexity, \nit also raises the risk of overfitting if the training dataset does not scale accordingly.\nImpact of the number of heads in MHA\nIn Transformer models, the number of heads in the MHA mechanism is a key parameter that helps in \nlearning different aspects of features. We assessed the impact of varying the number of heads on MSCFormer’s \nperformance, with the results shown in Fig. 7. The MSCFormer model with a single head had the lowest average \nclassification accuracy on both datasets. For the BCI IV-2a dataset, the eight-head model achieved 2.58% higher \naccuracy than the single-head model (p < 0.05). On the BCI IV-2b dataset, the eight-head model outperformed \nthe single-head model by 0.96% (p < 0.05) and the 24-head model by 0.75% (p < 0.01). These results suggest that \nappropriately increasing the number of heads can significantly improve accuracy. Additionally, performance on \nthe BCI IV-2a dataset showed greater fluctuation compared to BCI IV-2b, possibly indicating that the model’s \nsensitivity to the number of heads increases with task complexity.\nImpact of the pooling size\nIn our study, we explored the impact of varying pooling sizes on the performance of MSCFormer by adjusting \nthe pooling size from 12 to 72, with increments of 4, resulting in corresponding token lengths ranging from 84 \nto 14. As depicted in Fig. 8, MSCFormer exhibited an increasing trend in accuracy followed by a decline across \nthe BCI IV-2a and IV-2b datasets. Notably, on the BCI IV-2a dataset, two distinct peaks in classification accuracy \nFig. 8. The impact of the pooling size on the accuracy for different datasets.\n \nFig. 7. The impact of the number of heads in MHA on model accuracy across datasets. The orange line within \neach box represents the median, while the green triangle indicates the mean.\n \nScientific Reports |        (2025) 15:12935 11| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\nwere observed at pooling sizes of 28 and 44, both reaching an optimum of 82.95%. In contrast, on the BCI IV-2b \ndataset, the highest average classification accuracy of 88.00% was achieved at a pooling size of 52. Compared to \nthe lowest performance observed at a pooling size of 12, these peaks represent improvements of 3.87% (p < 0.01) \nand 2.64% (p < 0.01) on the BCI IV-2a and IV-2b datasets, respectively. These findings suggest that pooling size \nshould be carefully selected based on the specific characteristics of the dataset.\nComparison of MSCFormer with SOTA methods\nTo comprehensively assess our model’s performance, we selected several SOTA methods, including three \nSSCNN-based models (Shallow ConvNet 20, Deep ConvNet 20, EEGNet 24), two MSCNN-based models \n(MMCNN35, MBEEGNet36), and two hybrid CNN-Transformer models (Conformer40, ADFCNN43). Below is a \nbrief introduction to these methods:\n• Shallow ConvNet: This model uses a large-kernel temporal convolution layer followed by a spatial convolution \nlayer, applying nonlinear activation and pooling, and concludes with a fully connected layer for classification.\n• Deep ConvNet: A more complex architecture for EEG signal decoding, it begins with a spatiotemporal con-\nvolution layer, followed by three convolutional blocks, each paired with max-pooling layers, and ends with a \nfully connected layer for classification.\n• EEGNet: A compact architecture designed for EEG signal decoding, utilizing depthwise and separable convo-\nlutions to capture spatial and temporal features, effectively reducing the number of model parameters.\n• MMCNN: This model consists of five parallel EINs, each comprising an EEG Inception block, a residual \nblock, and a SE block.\n• MBEEGNet: An extension of EEGNet, featuring multiple parallel EEGNet branches, each with different filter \nkernel sizes.\n• Conformer: This model integrates Shallow ConvNet with Transformer architecture to capture both local spa-\ntiotemporal features and global dependencies in EEG features.\n• ADFCNN: It combines large and small convolutional kernels to capture dual-scale features in EEG signals, \nwith a self-attention mechanism that dynamically adjusts feature weights for enhanced performance.\nThe optimal results for MSCFormer on the BCI IV-2a and IV-2b datasets were compared to the SOTA methods. \nTo further validate the effectiveness of our multi-scale convolution approach, we included MSNet, an ablation \nmodel without the Transformer module (w/o Trans), in the comparison.\nTo ensure a relatively fair comparison, we re-evaluated four representative models, including Shallow \nConvNet, Deep ConvNet, EEGNet, and MBEEGNet, whose original experimental conditions in the literature \ndiffered significantly from those in our study. In these reimplemented experiments, we applied identical \nexperimental conditions, including the same data preprocessing, data augmentation strategies, CV methods, and \ntraining hyperparameters (batch size, learning rate, and epochs). Table 3 presents comparisons of accuracy and \nkappa between MSNet, MSCFormer, and SOTA methods on the BCI IV-2a. Table 4 presents comparisons for the \nBCI IV-2b. The data for MMCNN, Conformer, and ADFCNN were obtained from their respective references.\nA comparison of these CNN models reveals that our proposed MSCNN-based model (MSNet) achieved \nthe second-highest average classification accuracy on the BCI IV-2a dataset, while also exhibiting the smallest \naccuracy standard deviation and the highest kappa value. Specifically, its average accuracy increased by 4.08% \n(p < 0.01), 1.84%, 1.47%, and 0.54% compared to Shallow ConvNet, EEGNet, Deep ConvNet, and MMCNN, \nrespectively, but was 2.2% lower than that of MBEEGNet. On the BCI IV-2b dataset, MSNet ranked second in both \naverage accuracy and kappa value, with a relatively small standard deviation. Specifically, its average accuracy \nwas 2.36% ( p < 0.01), 1.24%, 1.16%, and 0.34% higher than those of Deep ConvNet, MMCNN, MBEEGNet, \nand Shallow ConvNet, respectively, but 1.77% lower than that of EEGNet. These comparative results strongly \ndemonstrate the effectiveness of MSNet’s multi-scale design in addressing the challenge of individual variability \nin EEG signals.\nTables 3 and 4 show that MSCFormer achieved the best average classification accuracy and kappa values \nacross both datasets, with relatively small standard deviations compared to the other models. Specifically, on the \nBCI IV-2a dataset, MSCFormer’s average accuracy was 7.80% ( p < 0.01), 5.56% (p < 0.05), and 5.20% ( p < 0.05) \nMethod\\Subject A01 A02 A03 A04 A05 A06 A07 A08 A09 Accuracy S.D. Kappa p-value\nSSCNN\nShallow ConvNet 201720+ 80.97 56.46 91.04 72.29 73.54 61.18 77.71 83.89 79.31 75.15 10.85 0.6687 0.004**\nDeep ConvNet 201720+ 81.94 52.85 88.61 76.67 74.44 67.29 91.04 82.71 84.24 77.75 11.83 0.7034 0.012*\nEEGNet 201824+ 85.56 65.63 92.71 67.64 74.10 58.47 85.21 81.60 85.63 77.39 11.46 0.6986 0.020*\nMSCNN\nMMCNN 202135 82.10 59.80 92.80 69.00 87.30 68.50 89.20 91.60 92.60 81.43 11.75 0.6260 0.910\nMBEEGNet 202236+ 82.85 68.33 92.01 76.39 72.78 65.63 85.97 81.88 82.36 78.69 8.59 0.7158 0.020*\nMSNet (proposed) 86.25 70.69 91.67 77.22 75.63 66.94 79.65 83.40 81.60 79.23 7.65 0.7230 0.074\nHybrid\nConformer 202340 88.19 61.46 93.40 78.13 52.08 65.28 92.36 88.19 88.89 78.66 14.43 0.7155 0.359\nADFCNN 202443 87.15 61.45 93.75 75.69 75.34 65.27 88.54 82.29 85.06 79.39 10.23 – 0.020*\nMSCFormer (proposed) 86.11 65.42 94.10 85.97 80.42 74.58 89.93 84.79 85.21 82.95 8.06 0.7726 –\nTable 3. Comparison of the classification accuracy (%) and kappa on the BCI IV-2a dataset. The bold values \nindicate the best results. The method marked with plus sign (+) are reimplemented.\n \nScientific Reports |        (2025) 15:12935 12| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\nhigher than the SSCNN-based models Shallow ConvNet, EEGNet, and Deep ConvNet, respectively. It also \noutperformed the MSCNN-based models MBEEGNet, MSNet, and MMCNN by 4.26% ( p < 0.05), 3.72%, and \n1.52%, and exceeded the CNN-Transformer hybrid models Conformer and ADFCNN by 4.29% and 3.56% \n(p < 0.05). On the BCI IV-2b dataset, MSCFormer’s average accuracy was 4.72% ( p < 0.01), 3.60%, 3.52% \n(p < 0.01), 3.37%, 2.70% ( p < 0.01), 2.36% (p < 0.05), 0.59%, and 0.19% higher than Deep ConvNet, MMCNN, \nMBEEGNet, Conformer, Shallow ConvNet, MSNet, EEGNet, and ADFCNN, respectively. These comparative \nresults highlight MSCFormer’s superior classification accuracy, consistency, and robustness.\nFigure 9 presents the average confusion matrices from five-fold CV across nine subjects on the BCI IV-\n2a dataset. MSCFormer excelled in decoding the imagined left-hand, right-hand, and feet tasks, achieving \naccuracies of 83.80%, 85.96%, and 83.95%, respectively. In contrast, the results demonstrate that MSNet achieved \nthe highest accuracy in decoding the imagined tongue task, reaching 82.96%, which is at least 4.8% higher than \nFig. 9. Average confusion matrices of the proposed MSCFormer, MSNet and the reimplemented Shallow \nConvNet, Deep ConvNet, EEGNet, and MBEEGNet models. The labels L, R, F , and T in the figure represent \nthe left hand, right hand, feet, and tongue, respectively.\n \nMethod \\ Subject B01 B02 B03 B04 B05 B06 B07 B08 B09 Accuracy S.D. Kappa p-value\nSSCNN\nShallow ConvNet 201720+ 75.94 63.86 83.56 96.44 93.13 85.13 91.19 92.00 86.44 85.30 10.10 0.7059 0.008**\nDeep ConvNet 201720+ 74.00 61.71 80.13 94.38 88.63 82.19 90.00 91.88 86.63 83.28 10.27 0.6656 0.004**\nEEGNet 201824+ 77.56 68.14 86.94 97.44 93.63 87.63 93.38 93.44 88.56 87.41 9.23 0.7482 0.129\nMSCNN\nMMCNN 202135 84.90 70.40 75.50 96.30 92.40 86.30 87.60 84.20 81.80 84.40 7.47 0.6870 0.055\nMBEEGNet 202236+ 77.06 59.50 82.81 94.94 94.19 82.69 91.19 92.75 85.19 84.48 11.19 0.6896 0.008**\nMSNet (proposed) 75.69 64.93 85.63 97.50 90.94 84.75 91.75 93.63 85.94 85.64 9.99 0.7128 0.039*\nHybrid\nConformer 202340 82.50 65.71 63.75 98.44 86.56 90.31 87.81 94.38 92.19 84.63 11.49 0.6926 0.359\nADFCNN 202443 79.37 72.50 82.81 96.25 99.37 84.68 93.43 95.31 86.56 87.81 8.40 – 0.82\nMSCFormer (proposed) 78.06 71.21 82.75 97.69 96.81 87.81 94.00 94.75 88.88 88.00 9.10 0.7599 –\nTable 4. Comparison of the classification accuracy (%) and kappa on the BCI IV-2b dataset. The bold values \nindicate the best results. The method marked with plus sign (+) are reimplemented.\n \nScientific Reports |        (2025) 15:12935 13| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\nany other model. These results suggest that while MSCFormer demonstrates overall superiority in most tasks, \nMSNet may be more effective for certain specific tasks. Figure 10 illustrates the receiver operating characteristic \n(ROC) curves for these comparison models on the BCI IV-2b dataset, plotted based on their true positive rate \n(TPR) and false positive rate (FPR) data. Notably, MSCFormer achieves the highest area under the curve (AUC) \nvalue of 0.955, surpassing other models.\nVisualization of feature distribution\nTo elucidate the discriminatory capacity of the features extracted and enhanced by our MSCFormer model, we \nemployed t-distributed stochastic neighbor embedding (t-SNE) for visualization. This method transforms the \nhigh-dimensional features from EEG sequences into a two-dimensional embedding. We visualized the raw EEG \ndata and the transformation at three critical stages of the MSCFormer model: features learned by the CNN module, \nfeatures enhanced by the first layer of the Transformer module, and features fully enhanced by the complete \nTransformer module, as illustrated in Fig.  11. The visualization data were derived from five-fold CV models \nfor subject A03. Figure 11(a) presents the raw EEG signal features, where the four class labels are intermingled, \nmaking distinctions challenging. Figure  11(b) illustrates that after processing through the MSCFormer’s CNN \nmodule, the four categories become discernible, although inter-class boundaries remain blurred, and intra-class \ndistances are still substantial. Figure 11(c) depicts the features enhanced after the first layer of the Transformer \nmodule, where inter-class boundaries are more defined, and intra-class distances are notably reduced, \nunderscoring the efficacy of the MHA mechanism in global dependency modeling. Figure  11(d) illustrates the \nfeatures after full enhancement by the complete Transformer module, where class labels are distinctly segregated, \ninter-class distances are further enlarged, and intra-class distances are significantly decreased. This demonstrates \nthat increasing Transformer depth significantly enhances MSCFormer’s expressive capability. However, some \nmisclassifications persist, indicating the need for further model optimization to improve accuracy and achieve \nclearer label separation.\nDiscussion\nIn this section, we will conduct a more in-depth discussion of the ablation study, the impact of hyperparameters \non model performance, and comparisons with SOTA methods. Finally, we will discuss the model’s limitations \nand propose potential areas for future enhancement.\nDiscussion on ablation study\nThis study systematically investigates the impact of the Transformer module, data augmentation, and temporal \nconvolutional kernel sizes in MSCFormer through a series of ablation experiments, where each component \nis either removed or modified to assess its contribution to model performance. Removing the Transformer \nmodule led to varying degrees of performance degradation. Without data augmentation, accuracy on the BCI \nIV-2a dataset dropped significantly by 5.69% ( p < 0.01). With augmentation, the BCI IV-2b dataset showed a \nnotable accuracy decrease of 2.36% ( p < 0.05). This demonstrates the crucial role of the Transformer’s global \nreceptive field in capturing global dependencies and complex patterns in MI-EEG signals, enhancing decoding \nperformance regardless of data augmentation.\nRemoving data augmentation alone resulted in a 9.57% ( p < 0.01) and 4.12% (p < 0.01) drop in accuracy on \nthe BCI IV-2a and IV-2b datasets, respectively, indicating the importance of data augmentation in increasing \nthe model’s adaptability and coverage across feature space. This effect is especially pronounced for the BCI \nFig. 10. ROC curves and corresponding AUC values for the reimplemented Shallow ConvNet, Deep ConvNet, \nEEGNet, MBEEGNet, as well as our MSNet and MSCFormer models.\n \nScientific Reports |        (2025) 15:12935 14| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\nIV-2a dataset, suggesting that data augmentation significantly enhances model performance in more complex \nclassification tasks.\nRemoving both the Transformer and data augmentation had the most substantial negative impact, reducing \naverage accuracy by 15.26% ( p < 0.01) and 5.18% ( p < 0.01) on the BCI IV-2a and IV-2b datasets, respectively. \nThis substantial drop underscores the combined importance of the Transformer’s global dependency modeling \nand the diversity introduced by data augmentation.\nThe selection of kernel sizes (85, 65, 45) in our multi-branch CNN module was inspired by 34, aiming to \ncapture diverse temporal features in MI-EEG signals. While previous studies, such as22] and [36, have used smaller \nkernels (64, 32, 16), we conducted an ablation study to assess their impact. The results show that replacing our \noriginal kernel sizes with smaller ones led to a significant drop in classification accuracy, with decreases of 2.21% \n(p < 0.05) on BCI IV-2a and 1.45% (p < 0.01) on BCI IV-2b. This suggests that larger temporal kernels contribute \nto more effective feature extraction, enhancing MI-EEG decoding performance.\nThe impact of removing components on the kappa coefficient was similar to that on accuracy. These \nexperimental results reveal that the Transformer module performs global modeling to the multi-scale features \nextracted from different temporal scales, enhancing the model’s representational capacity and improving \ndecoding performance, while data augmentation ensures robust training in scenarios with limited data.\nDiscussion on hyperparameter impact analysis\nTo investigate the impact of hyperparameters on the performance of the MSCFormer model, we analyzed three \nkey hyperparameters under data augmentation conditions: the depth of the Transformer module, the number of \nheads in the MHA mechanism, and the pooling size of the CNN module.\nThe depth of the Transformer encoder plays a crucial role in the model’s ability to capture complex temporal \ndependencies. Increasing the depth up to a certain point (depth 5) led to an improvement in accuracy, particularly \nfor the BCI IV-2a dataset, where accuracy increased by 2.94% (p < 0.05) compared to depth 1. However, further \nincreases in depth introduce the risk of overfitting.\nThe number of heads in the MHA mechanism also proved to be a critical factor. The eight-head configuration \nyielded the best performance on both datasets, with notable improvements of 2.58% (p < 0.05) on BCI IV-2a and \n0.96% (p < 0.05) on BCI IV-2b, compared to the single-head configuration. However, adding too many heads \n(e.g., 24 heads) resulted in diminishing returns, suggesting that excessive splitting of attention may fragment the \nfeature space.\nFig. 11. Visualization using t-SNE. (a) Raw EEG data distribution. (b) Feature distribution after the CNN \nmodule. (c) Feature distribution after the first layer of the Transformer module. (d) Feature distribution \nfollowing full Transformer integration. Blue dots indicate the left hand, orange dots represent the right hand, \ngreen dots represent the feet, and red dots signify the tongue.\n \nScientific Reports |        (2025) 15:12935 15| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\nThe impact of pooling size showed that an intermediate value optimally balances the retention of temporal \nfeatures and noise reduction. Pooling sizes of 28 and 44 achieved peak performance on the BCI IV-2a dataset, \nwhile a pooling size of 52 was optimal for BCI IV-2b. These results indicate that pooling size should be carefully \ntuned based on the characteristics of the dataset to avoid information loss or over-smoothing.\nDiscussion on comparative with SOTA methods\nTo comprehensively compare the performance of our proposed MSCFormer with SOTA methods, we analyzed \nkey aspects such as data preprocessing, data augmentation techniques, model architecture, and parameter count, \nas summarized in Table 5.\nAcross these models, standardization was the most common preprocessing method. The Conformer model \napplied band-pass filtering before standardization, while ADFCNN incorporated both BPF and electrode-\nwise exponential moving standardization (EEMS). Most models, except ADFCNN, employed the S&R data \naugmentation technique, while MMCNN used a combination of sliding window (SW) and Gaussian noise (GN). \nA notable difference was in the selection of electrode channels: on the BCI IV-2a dataset, MMCNN utilized \ndata from only three channels (C3, Cz, and C4), while other methods utilized data from all 22 channels. Roy et \nal.37 previously investigated the impact of various data augmentation techniques on the BCI IV-2b dataset and \ndemonstrated that the S&R method significantly outperformed GN, SW , and window warping. Additionally, \ncombining these augmentation techniques further improved recognition accuracy. If all models had adopted \nthe hybrid augmentation techniques proposed by Roy et al., their overall performance could have been further \nenhanced.\nThe results in Table  5 show that MSCFormer achieved the highest average accuracy across both datasets, \ndemonstrating strong performance in MI-EEG decoding. However, the margin of superiority may have been \nnarrower, or perhaps not the highest, if all models had used the same data preprocessing and augmentation \ntechniques as MSCFormer.\nAdditionally, although MSCFormer outperforms SOTA models, its high performance comes at the cost of a \nsignificantly larger parameter count. This increased complexity enables MSCFormer to capture intricate patterns \nand long-range dependencies in the data, likely contributing to its superior accuracy. However, the larger \nparameter count also leads to longer training times and higher computational demands, which may present \nchallenges in resource-limited environments. In practical applications, it is essential to balance MSCFormer’s \nparameter count with the available computational resources and the potential risk of overfitting. While its \ncomplexity improves accuracy, this may not always be practical in scenarios requiring real-time processing \nor environments with limited computational capacity. Future research could focus on optimizing the model \narchitecture to maintain high accuracy while reducing computational demands, making MSCFormer more \nadaptable to real-world BCI applications.\nLimitations and future work\nWhile the MSCFormer model proposed in our study outperforms several SOTA methods in subject-specific \nclassification tasks in terms of average recognition accuracy and kappa scores, there are still areas for \nimprovement. First, the MSCFormer model contains numerous hyperparameters, and optimizing them is \ntime-consuming. Key hyperparameters, such as the number of branches in the CNN module and the size and \nnumber of temporal convolution kernels within these branches, have not been fully optimized. As a result, \nthe current experimental results may not yet reflect the model’s optimal performance. Future work will focus \non automating hyperparameter optimization, integrating it with neural architecture search (NAS) to enable \nthe model to autonomously identify the most effective parameter settings during training, thereby further \nimproving performance. Second, the large number of parameters in the MSCFormer model may limit its \ndeployment on devices with constrained hardware resources. Future research will explore strategies such as \nmodel pruning, quantization, and knowledge distillation to reduce the model’s size and computational demands \nwhile preserving its high performance. Third, this study primarily addresses subject-specific classification of MI-\nEEG signals, relying solely on EEG data from individual subjects throughout the training, validation, and testing \nphases, and excluding cross-subject scenarios. This approach limits the assessment of the model’s generalization \nMethods Preprocessing Augmentation Architecture\nParameters Accuracy %\n2a 2b 2a 2b\nShallow ConvNet 201720+ STD S & R SSCNN 46.1 k 10.8 k 75.15 85.30\nDeep ConvNet 201720+ STD S & R SSCNN 283.3 k 268.6 k 77.75 83.28\nEEGNet 201824+ STD S & R SSCNN 2.9 k 2.1 k 77.39 87.41\nMMCNN 202135 STD SW & GN MSCNN + SE 90.3 k 90.3 k 81.43 84.40\nMBEEGNet 202236+ STD S & R MSCNN 7.1 k 4.7 k 78.69 84.48\nMSNet (proposed) STD S & R MSCNN 8.6 k 5.3 k 79.23 86.18\nConformer 202340 BPF & STD S & R SSCNN + Transformer 789.6 k 759.2 k 78.66 84.63\nADFCNN 202443 BPF & EEMS - MSCNN + Transformer 5.4 k 3.0 k 79.39 87.81\nMSCFormer (proposed) STD S & R MSCNN + Transformer 145.9 k 144.9 k 82.95 88.00\nTable 5. Comparative analysis of our proposed methods and SOTA approaches. The bold values indicate the \nbest results. The method marked with plus sign (+) are reimplemented.\n \nScientific Reports |        (2025) 15:12935 16| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\ncapabilities. To address this, future work will focus on applying MSCFormer to cross-subject classification \ntasks, which will help to evaluate its effectiveness in more diverse application contexts. These efforts could lay a \nstronger foundation for advancing MI-EEG decoding technology and its practical applications.\nConclusions\nIn this study, we introduced MSCFormer, a novel model that integrates a multi-scale convolution module with \na Transformer encoder for MI-EEG decoding. The experimental results demonstrated that the multi-branch \nCNN architecture effectively addresses individual variability in EEG signals by capturing features at different \nscales. Additionally, the Transformer encoder models global dependencies across these multi-scale features, \nsignificantly improving feature representation and classification performance. MSCFormer achieved an average \naccuracy of 82.95% with a kappa of 0.7726 on the BCI IV-2a dataset, and an average accuracy of 88.00% with \na kappa of 0.7599 on the BCI IV-2b dataset, outperforming several SOTA methods. These findings highlight \nMSCFormer’s ability to enhance MI-EEG decoding performance, establishing a strong foundation for further \nresearch into multi-scale feature extraction and global dependency modeling in EEG-based BCI systems.\nData availability\nThe BCI IV-2a and IV-2b datasets analyzed during the current study are available in the BCI Competition IV \nrepository [https://www.bbci.de/competition/iv/#datasets].\nReceived: 16 November 2024; Accepted: 31 March 2025\nReferences\n 1. Chaudhary, U., Birbaumer, N. & Ramos-Murguialday, A. Brain–computer interfaces for communication and rehabilitation. Nat. \nRev. Neurol. 12, 513–525 (2016).\n 2. Ramadan, R. A. & Vasilakos, A. V . Brain computer interface: control signals review. Neurocomputing 223, 26–44 (2017).\n 3. Abiri, R., Borhani, S., Sellers, E. W ., Jiang, Y . & Zhao, X. A comprehensive review of EEG-based brain-computer interface \nparadigms. J. Neural Eng. 16 (1), 011001 (2019).\n 4. Lee, M. H. et al. EEG dataset and OpenBMI toolbox for three BCI paradigms: an investigation into BCI illiteracy. GigaScience 8 \n(5), giz002 (2019).\n 5. Pichiorri, F . et al. Brain-computer interface boosts motor imagery practice during stroke recovery. Ann. Neurol. 77 (5), 851–865 \n(2015).\n 6. Gu, X. et al. EEG-based brain-computer interfaces (BCIs): A survey of recent studies on signal sensing technologies and \ncomputational intelligence approaches and their applications. IEEE-ACM Trans. Comput. Biol. Bioinform . 18 (5), 1645–1666 \n(2021).\n 7. Altaheri, H., Muhammad, G. & Alsulaiman, M. Physics-informed attention Temporal convolutional network for EEG-based \nmotor imagery classification. IEEE Trans. Ind. Inf. 19 (2), 2249–2258 (2023).\n 8. Tan, J. et al. Suppressing of power line artifact from electroencephalogram measurements using sparsity in frequency domain. \nFront. Neurosci. 15, 780373 (2021).\n 9. Ramoser, H., Muller-Gerking, J. & Pfurtscheller, G. Optimal Spatial filtering of single trial EEG during imagined hand movement. \nIEEE Trans. Rehabil Eng. 8 (4), 441–446 (2000).\n 10. Ang, K. K., Chin, Z. Y ., Zhang, H. & Guan, C. Filter bank common spatial pattern (FBCSP) in brain-computer interface. Proc. IEEE \nInt. Jt. Conf. Neural Netw. 2390–2397 (2008).\n 11. Kim, C., Sun, J., Liu, D., Wang, Q. & Paek, S. An effective feature extraction method by power spectral density of EEG signal for \n2-class motor imagery-based BCI. Med. Biol. Eng. Comput. 56, 1645–1658 (2018).\n 12. Kevric, J. & Subasi, A. Comparison of signal decomposition methods in classification of EEG signals for motor-imagery BCI \nsystem. Biomed. Signal. Process. Control. 31, 398–406 (2017).\n 13. Gaur, P ., Pachori, R. B., Wang, H. & Prasad, G. An automatic subject specific intrinsic mode function selection for enhancing Two-\nClass EEG-Based motor Imagery-Brain computer interface. IEEE Sens. J. 19 (16), 6938–6947 (2019).\n 14. Ji, N., Ma, L., Dong, H. & Zhang, X. EEG signals feature extraction based on DWT and EMD combined with approximate entropy. \nBrain Sci. 9 (8), 201 (2019).\n 15. Hsu, W . Continuous EEG signal analysis for asynchronous BCI application. Int. J. Neur Syst. 21 (04), 335–350 (2011).\n 16. Jin, J. et al. Correlation-based channel selection and regularized feature optimization for MI-based BCI. Neural Netw. 118, 262–270 \n(2019).\n 17. Barachant, A., Bonnet, S., Congedo, M. & Jutten, C. Multiclass brain-computer interface classification by riemannian geometry. \nIEEE Trans. Biomed. Eng. 59 (4), 920–928 (2012).\n 18. Varsehi, H., Mohammad, S. & Firoozabadi, P . An EEG channel selection method for motor imagery based brain–computer \ninterface and neurofeedback using Granger causality. Neural Netw. 133, 193–206 (2021).\n 19. LeCun, Y ., Bengio, Y . & Hinton, G. Deep learning. Nature 521 (7553), 436–444 (2015).\n 20. Schirrmeister, R., Gemein, L., Eggensperger, K., Hutter, F . & Ball, T. Deep learning with convolutional neural networks for EEG \ndecoding and visualization. Hum. Brain Mapp. 38 (11), 5391–5420 (2017).\n 21. Craik, A., He, Y . & Contreras-Vidal, J. L. Deep learning for electroencephalogram (EEG) classification tasks: a review. J. Neural Eng. \n16, 031001 (2019).\n 22. Salami, A., Andreu-Perez, J. & Gillmeister, H. EEG-ITNet: an explainable inception Temporal convolutional network for motor \nimagery classification. IEEE Access. 10, 36672–36685 (2022).\n 23. Zhao, W . et al. Residual and bidirectional LSTM for epileptic seizure detection. Front. Comput. Neurosci. 18, 1415967 (2024).\n 24. Lawhern, V . J. et al. EEGNet: A compact convolutional neural network for EEG-based brain–computer interfaces. J. Neural Eng. 15 \n(5), 056013 (2018).\n 25. Mane, R., Robinson, N., Vinod, A. P ., Lee, S. W . & Guan, C. A multi-view CNN with novel variance layer for motor imagery brain \ncomputer interface. Proc. 42nd Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. 2950–2953 (2020).\n 26. Wang, Z. et al. Short time fourier transformation and deep neural networks for motor imagery brain computer interface \nrecognition. Concurr Comput. -Pract Exp. 30 (23), e4413 (2018).\n 27. Lee, H. K. & Choi, Y . Application of continuous wavelet transform and convolutional neural network in decoding motor imagery \nBrain-Computer interface. Entropy 21 (12), 1199 (2019).\n 28. Wang, P ., Jiang, A., Liu, X., Shang, J. & Zhang, L. LSTM-based EEG classifcation in motor imagery tasks. IEEE Trans. Neural Syst. \nRehabil Eng. 26 (11), 2086–2095 (2018).\nScientific Reports |        (2025) 15:12935 17| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/\n 29. Xie, J. et al. A transformer-based approach combining deep learning network and spatial-temporal information for Raw EEG \nclassification. IEEE Trans. Neural Syst. Rehabil Eng. 30, 2126–2136 (2022).\n 30. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 6000–6010 (2017).\n 31. Sakhavi, S., Guan, C. & Y an, S. Learning Temporal information for brain-computer interface using convolutional neural networks. \nIEEE Trans. Neural Netw. Learn. Syst. 29 (11), 5619–5629 (2018).\n 32. Roy, A. M. Adaptive transfer learning-based multiscale feature fused deep convolutional neural network for EEG MI \nmulticlassification in brain-computer interface. Eng. Appl. Artif. Intell. 116, 105347 (2022).\n 33. Amin, S. U., Alsulaiman, M., Muhammad, G., Mekhtiche, M. A. & Hossain, S. Deep learning for EEG motor imagery classification \nbased on multi-layer CNNs feature fusion. Futur Gener Comp. Syst. 101, 542–554 (2019).\n 34. Dai, G., Zhou, J., Huang, J. & Wang, N. HS-CNN: A CNN with hybrid Convolution scale for EEG motor imagery classification. J. \nNeural Eng. 17 (1), 016025 (2020).\n 35. Jia, Z. et al. MMCNN: A multi-branch multi-scale convolutional neural network for motor imagery classification. Machine \nLearning and Knowledge Discovery in Databases, F . Hutter, K. Kersting, J. Lijffijt, and I. Valera, Eds. Cham, Switzerland: Springer, \n12459, Lecture Notes in Computer Science 736–751 (2020).\n 36. Altuwaijri, G. A. & Muhammad, G. A multibranch of convolutional neural network models for electroencephalogram-based \nmotor imagery classification. Biosensors-Basel 12 (1), 22 (2022).\n 37. Roy, A. M. An efficient multi-scale CNN model with intrinsic feature integration for motor imagery EEG subject classification in \nbrain-machine interfaces. Biomed. Signal. Process. Control. 74, 103496 (2022).\n 38. Song, Y ., Jia, X., Y ang, L. & Xie, L. Transformer-based spatial-temporal feature learning for EEG decoding. Jun. [Online].  h t t p s : / / a \nr x i v . o r g / a b s / 2 1 0 6 . 1 1 1 7 0     (2021).\n 39. Tao, Y . et al. Gated transformer for decoding human brain EEG signals. Proc. 43rd Annu. Int. Conf. IEEE Eng. Med. Biol. Soc.  \n125–130 (2021).\n 40. Song, Y ., Zheng, Q., Liu, B., & Gao, X.  EEG Conformer: Convolutional transformer for EEG decoding and visualization. IEEE \nTrans. Neural Syst. Rehabil Eng. 31, 710–719 (2023).\n 41. Zhao, W ., Jiang, X., Zhang, B., Xiao, S. & Weng, S. CTNet: A convolutional transformer network for EEG-based motor imagery \nclassification. Sci. Rep. 14 (1), 20237 (2024).\n 42. Ahn, H. J., Lee, D. H., Jeong, J. H. & Lee, S. W . Multiscale convolutional transformer for EEG classification of mental imagery in \ndifferent modalities. IEEE Trans. Neural Syst. Rehabil Eng. 31, 646–656 (2023).\n 43. Tao, W . et al. Attention-based dual-scale fusion convolutional neural network for motor imagery brain–computer interface. IEEE \nTrans. Neural Syst. Rehabil Eng. 32, 154–165 (2024).\n 44. Lotte, F . Signal processing approaches to minimize or suppress calibration time in oscillatory activity-based brain-computer \ninterfaces. Proc. IEEE. 103 (6), 871–890 (2015).\n 45. Devlin, J., Chang, M., Lee, K. & Toutanova, K. BERT: Pre-training of deep bidirectional Transformers for Language Understanding. \nNAACL-HLT 4171–4186 (2019).\nAcknowledgements\nThis work was partially supported by the Natural Science Foundation of Xiamen, China (3502Z202374054), \nNatural Science Foundation of Fujian Province, China (2023J01785), the Education and Scientific Research \nProject for Y oung and Middle-aged Teachers of Fujian Province, China (JAT191153 and JAT201045), and Jimei \nUniversity Chengyi College Provincial and Ministerial-Level Scientific Research Cultivation Project, China \n(CKZ24016).\nAuthor contributions\nW .Z.: Conceptualization, Methodology, Validation, Writing - Original Draft, Software, Funding acquisition, \nWriting - Review & Editing. B.Z.: Software, Visualization, Formal analysis, Investigation, Data Curation. H.Z.: \nMethodology, Writing - Review & Editing, Project administration. D.W .: Methodology, Formal analysis. C.H.: \nFormal analysis, Writing - Review & Editing. Q.L.: Conceptualization, Methodology.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to H.Z. or Q.L.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give \nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and \nindicate if changes were made. The images or other third party material in this article are included in the article’s \nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy \nof this licence, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2025 \nScientific Reports |        (2025) 15:12935 18| https://doi.org/10.1038/s41598-025-96611-5\nwww.nature.com/scientificreports/",
  "topic": "Brain–computer interface",
  "concepts": [
    {
      "name": "Brain–computer interface",
      "score": 0.8894749879837036
    },
    {
      "name": "Computer science",
      "score": 0.8304274082183838
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7737991213798523
    },
    {
      "name": "Electroencephalography",
      "score": 0.5660372972488403
    },
    {
      "name": "Feature extraction",
      "score": 0.5426775217056274
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.49579891562461853
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.48084983229637146
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48038917779922485
    },
    {
      "name": "Encoder",
      "score": 0.46348196268081665
    },
    {
      "name": "Decoding methods",
      "score": 0.4587431848049164
    },
    {
      "name": "Transformer",
      "score": 0.41534367203712463
    },
    {
      "name": "Motor imagery",
      "score": 0.41007715463638306
    },
    {
      "name": "Speech recognition",
      "score": 0.3437207043170929
    },
    {
      "name": "Algorithm",
      "score": 0.06844016909599304
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "cited_by": 9
}