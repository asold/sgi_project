{
  "title": "ViTGAN: Training GANs with Vision Transformers",
  "url": "https://openalex.org/W3180897515",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2747179994",
      "name": "Lee Kwonjoon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129700191",
      "name": "Chang, Huiwen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097595792",
      "name": "Jiang, Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106826012",
      "name": "Zhang, Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2572082798",
      "name": "Tu, Zhuowen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2222005704",
      "name": "Liu Ce",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2775288145",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2963981733",
    "https://openalex.org/W3035687950",
    "https://openalex.org/W2962760235",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W3180059462",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W1834627138",
    "https://openalex.org/W3161838454",
    "https://openalex.org/W3171615848",
    "https://openalex.org/W2792263949",
    "https://openalex.org/W2911910629",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3031246127",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W2963373786",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W3103242287",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W3038287124",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W3103313582",
    "https://openalex.org/W3135593154",
    "https://openalex.org/W2328111639",
    "https://openalex.org/W2962767869",
    "https://openalex.org/W2962879692",
    "https://openalex.org/W3169537799",
    "https://openalex.org/W2963836885",
    "https://openalex.org/W3114896399",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3034456812",
    "https://openalex.org/W2963092440",
    "https://openalex.org/W2739748921",
    "https://openalex.org/W2996690341",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W3104876213",
    "https://openalex.org/W2964316369",
    "https://openalex.org/W2963926543",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W967544008",
    "https://openalex.org/W2909750748",
    "https://openalex.org/W2963800509",
    "https://openalex.org/W3120254195",
    "https://openalex.org/W3136635488",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3033085318",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2893749619",
    "https://openalex.org/W3175491752",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W3176823897",
    "https://openalex.org/W3147387781"
  ],
  "abstract": "Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such performance can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). For ViT discriminators, we observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce several novel regularization techniques for training GANs with ViTs. For ViT generators, we examine architectural choices for latent and pixel mapping layers to facilitate convergence. Empirically, our approach, named ViTGAN, achieves comparable performance to the leading CNN-based GAN models on three datasets: CIFAR-10, CelebA, and LSUN bedroom.",
  "full_text": "Published as a conference paper at ICLR 2022\nVITGAN: T RAINING GAN S WITH VISION TRANS -\nFORMERS\nKwonjoon Lee1,3 Huiwen Chang2 Lu Jiang2 Han Zhang2 Zhuowen Tu1 Ce Liu4\n1UC San Diego 2Google Research 3Honda Research Institute 4Microsoft Azure AI\nkwl042@eng.ucsd.edu {huiwenchang,lujiang,zhanghan}@google.com\nztu@ucsd.edu ce.liu@microsoft.com\nABSTRACT\nRecently, Vision Transformers (ViTs) have shown competitive performance on\nimage recognition while requiring less vision-specific inductive biases. In this\npaper, we investigate if such performance can be extended to image generation.\nTo this end, we integrate the ViT architecture into generative adversarial networks\n(GANs). For ViT discriminators, we observe that existing regularization methods\nfor GANs interact poorly with self-attention, causing serious instability during\ntraining. To resolve this issue, we introduce several novel regularization techniques\nfor training GANs with ViTs. For ViT generators, we examine architectural choices\nfor latent and pixel mapping layers to facilitate convergence. Empirically, our\napproach, named ViTGAN, achieves comparable performance to the leading CNN-\nbased GAN models on three datasets: CIFAR-10, CelebA, and LSUN bedroom.\nOur code is available online1.\n1 I NTRODUCTION\nConvolutional neural networks (CNNs) (LeCun et al., 1989) are dominating computer vision today,\nthanks to their powerful capability of convolution (weight-sharing and local-connectivity) and pooling\n(translation equivariance). Recently, however, Transformer architectures (Vaswani et al., 2017) have\nstarted to rival CNNs in many vision tasks.\nIn particular, Vision Transformers (ViTs) (Dosovitskiy et al., 2021), which interpret an image as a\nsequence of tokens (analogous to words in natural language), have been shown to achieve comparable\nclassification accuracy with smaller computational budgets ( i.e., fewer FLOPs) on the ImageNet\nbenchmark. Unlike CNNs, ViTs capture a different inductive bias through self-attention where each\npatch is attended to all patches of the same image. ViTs, along with their variants (Touvron et al.,\n2020; Tolstikhin et al., 2021), though still in their infancy, have demonstrated advantages in modeling\nnon-local contextual dependencies (Ranftl et al., 2021; Strudel et al., 2021) as well as promising\nefficiency and scalability. Since their recent inception, ViTs have been used in various tasks such\nas object detection (Beal et al., 2020), video recognition (Bertasius et al., 2021; Arnab et al., 2021),\nmultitask pre-training (Chen et al., 2020a), etc.\nIn this paper, we examine whether Vision Transformers can perform the task of image generation\nwithout using convolution or pooling, and more specifically, whether ViTs can be used to train\ngenerative adversarial networks (GANs) with comparable quality to CNN-based GANs. While\nwe can naively train GANs following the design of the standard ViT (Dosovitskiy et al., 2021),\nwe find that GAN training becomes highly unstable when coupled with ViTs, and that adversarial\ntraining is frequently hindered by high-variance gradients in the later stage of discriminator training.\nFurthermore, conventional regularization methods such as gradient penalty (Gulrajani et al., 2017;\nMescheder et al., 2018), spectral normalization (Miyato et al., 2018) cannot resolve the instability\nissue, even though they are proved to be effective for CNN-based GAN models (shown in Fig. 4). As\nunstable training is uncommon in the CNN-based GANs training with appropriate regularization, this\npresents a unique challenge to the design of ViT-based GANs.\n1https://github.com/mlpc-ucsd/ViTGAN\n1\narXiv:2107.04589v2  [cs.CV]  29 May 2024\nPublished as a conference paper at ICLR 2022\nWe propose several necessary modifications to stabilize the training dynamics and facilitate the\nconvergence of ViT-based GANs. In thediscriminator, we design an improved spectral normalization\nthat enforces Lipschitz continuity for stabilizing the training dynamics. In the generator, we propose\ntwo key modifications to the layer normalization and output mapping layers after studying several\narchitecture designs. Our ablation experiments validate the necessity of the proposed techniques and\ntheir central role in achieving stable and superior image generation.\nThe experiments are conducted on three public image synthesis benchmarks: CIFAR-10, CelebA, and\nLSUN bedroom. The results show that our model, named ViTGAN, yields comparable performance\nto the leading CNN-based StyleGAN2 (Karras et al., 2020b; Zhao et al., 2020a) when trained under\nthe same setting. Moreover, we are able to outperform StyleGAN2 by combining the StyleGAN2\ndiscriminator with our ViTGAN generator.\nNote that it is not our intention to claim ViTGAN is superior to the best-performing GAN models\nsuch as StyleGAN2 + ADA (Karras et al., 2020a) which are equipped with highly-optimized hyper-\nparameters, architecture configurations, and sophisticated data augmentation methods. Instead, our\nwork aims to close the performance gap between the conventional CNN-based GAN architectures\nand the novel GAN architecture composed of vanilla ViT layers. Furthermore, the ablation in Table\n4 shows the advantages of ViT’s intrinsic capability ( i.e., adaptive connection weight and global\ncontext) for image generation.\n2 R ELATED WORK\nGenerative Adversarial Networks Generative adversarial networks (GANs) (Goodfellow et al.,\n2014) model the target distribution using adversarial learning. It is typically formulated as a min-\nmax optimization problem minimizing some distance between the real and generated data distri-\nbutions, e.g., through various f-divergences (Nowozin et al., 2016) or integral probability metrics\n(IPMs) (Müller, 1997; Song & Ermon, 2020) such as the Wasserstein distance (Arjovsky et al., 2017).\nGAN models are notorious for unstable training dynamics. As a result, numerous efforts have been\nproposed to stabilize training, thereby ensuring convergence. Common approaches include spectral\nnormalization (Miyato et al., 2018), gradient penalty (Gulrajani et al., 2017; Mescheder et al., 2018;\nKodali et al., 2017), consistency regularization (Zhang et al., 2020; Zhao et al., 2021), and data\naugmentation (Zhao et al., 2020a; Karras et al., 2020a; Zhao et al., 2020b; Tran et al., 2021). These\ntechniques are all designed inside convolutional neural networks (CNN) and have been only verified\nin convolutional GAN models. However, we find that these methods are insufficient for stabilizing\nthe training of Transformer-based GANs. A similar finding was reported in (Chen et al., 2021) on a\ndifferent task of pretraining. This paper introduces several novel techniques to overcome the unstable\nadversarial training of Vision Transformers.\nVision Transformers Vision Transformer (ViT) (Dosovitskiy et al., 2021) is a convolution-free\nTransformer that performs image classification over a sequence of image patches. ViT demonstrates\nthe superiority of the Transformer architecture over the classical CNNs by taking advantage of\npretraining on large-scale datasets. Afterward, DeiT (Touvron et al., 2020) improves ViTs’ sample\nefficiency using knowledge distillation as well as regularization tricks. MLP-Mixer (Tolstikhin et al.,\n2021) further drops self-attention and replaces it with an MLP to mix the per-location feature. In\nparallel, ViT has been extended to various computer vision tasks such as object detection (Beal\net al., 2020), action recognition in video (Bertasius et al., 2021; Arnab et al., 2021), and multitask\npretraining (Chen et al., 2020a). Our work is among the first to exploit Vision Transformers in the\nGAN model for image generation.\nGenerative Transformers in VisionMotivated by the success of GPT-3 (Brown et al., 2020), a\nfew pilot works study image generation using Transformer by autoregressive learning (Chen et al.,\n2020b; Esser et al., 2021) or cross-modal learning between image and text (Ramesh et al., 2021).\nThese methods are different from ours as they model image generation as a autoregressive sequence\nlearning problem. On the contrary, our work trains Vision Transformers in the generative adversarial\ntraining paradigm. Recent work of (Hudson & Zitnick, 2021), embeds (cross-)attention module\nwithin the CNN backbone (Karras et al., 2020b) in a similar spirit to (Zhang et al., 2019). The closest\nwork to ours is TransGAN (Jiang et al., 2021), presenting a GAN model based on Swin Transformer\n2\nPublished as a conference paper at ICLR 2022\n1 2 3 N...\nTransformer Encoder \nLatent  z \nMapping  Network \nw\nA\n...\nMLP \n...\nMLP \nMLP \nMLP \nReal / Fake \nTransformer Encoder \nProjection of \n Flattened Patches \n0 *\n...\nGenerator  Discriminator \nGenerated Patches \nOverlapping Patches \nFourier \nEmbedding \nEfou (x, y) \nMLP \n1 2 3 N...\nFigure 1: Overview of the proposed ViTGAN framework.Both the generator and the discriminator\nare designed based on the Vision Transformer (ViT). Discriminator score is derived from the clas-\nsification embedding (denoted as [*] in the Figure). The generator generates pixels patch-by-patch\nbased on patch embeddings.\nbackbone (Liu et al., 2021b). Our approach is complementary to theirs as we propose key techniques\nfor training stability within the original ViT backbone (Dosovitskiy et al., 2021).\n3 P RELIMINARIES : V ISION TRANSFORMERS (VITS)\nVision Transformer (Dosovitskiy et al., 2021) is a pure transformer architecture for image classifi-\ncation that operates upon a sequence of image patches. The 2D image x ∈ RH×W×C is flattened\ninto a sequence of image patches, following the raster scan, denoted by xp ∈ RN×(P2·C), where\nN = H×W\nP2 is the effective sequence length and P × P × C is the dimension of each image patch.\nFollowing BERT (Devlin et al., 2019), a learnable classification embedding xclass is prepended to\nthe image sequence along with the added 1D positional embeddings Epos to formulate the patch\nembedding h0. The architecture of ViT follows the Transformer architecture (Vaswani et al., 2017).\nh0 = [xclass; x1\npE; x2\npE; ··· ; xN\np E] + Epos, E ∈ R(P2·C)×D, Epos ∈ R(N+1)×D (1)\nh′\nℓ = MSA(LN(hℓ−1)) + hℓ−1, ℓ = 1, . . . , L (2)\nhℓ = MLP(LN(h′\nℓ)) + h′\nℓ, ℓ = 1, . . . , L (3)\ny = LN(h0\nL) (4)\nEquation 2 applies multi-headed self-attention (MSA). Given learnable matrices Wq, Wk, Wv\ncorresponding to query, key, and value representations, a single self-attention head is computed by:\nAttentionh(X) = softmax\n\u0010QK⊤\n√dh\n\u0011\nV, (5)\nwhere Q = XWq, K = XWk, and V = XWv. Multi-headed self-attention aggregates infor-\nmation from H self-attention heads by means of concatenation and linear projection: MSA(X) =\nconcatH\nh=1[Attentionh(X)]W + b.\n4 M ETHOD\nFig. 1 illustrates the architecture of the proposed ViTGAN with a ViT discriminator and a ViT-based\ngenerator. We find that directly using ViT as the discriminator makes the training volatile. We\nintroduce techniques to both generator and discriminator to stabilize the training dynamics and\nfacilitate the convergence: (1) regularization on ViT discriminator and (2) new architecture for\ngenerator.\n3\nPublished as a conference paper at ICLR 2022\n4.1 R EGULARIZING VIT-BASED DISCRIMINATOR\nEnforcing Lipschitzness of Transformer DiscriminatorLipschitz continuity plays a critical role\nin GAN discriminators. It was first brought to attention as a condition to approximate the Wasserstein\ndistance in WGAN (Arjovsky et al., 2017), and later was confirmed in other GAN settings (Fedus\net al., 2018; Miyato et al., 2018; Zhang et al., 2019) beyond the Wasserstein loss. In particular, (Zhou\net al., 2019) proves that Lipschitz discriminator guarantees the existence of the optimal discriminative\nfunction as well as the existence of a unique Nash equilibrium. A very recent work (Kim et al., 2021),\nhowever, shows that Lipschitz constant of standard dot product self-attention (i.e., Equation 5) layer\ncan be unbounded, rendering Lipschitz continuity violated in ViTs. To enforce Lipschitzness of our\nViT discriminator, we adopt L2 attentionproposed in (Kim et al., 2021). As shown in Equation 6, we\nreplace the dot product similarity with Euclidean distance and also tie the weights for the projection\nmatrices for query and key in self-attention:\nAttentionh(X) = softmax\n\u0010\n− d(XWq, XWk)√dh\n\u0011\nXWv, where Wq = Wk, (6)\nWq, Wk, and Wv are the projection matrices for query, key, and value, respectively.d(·, ·) computes\nvectorized L2 distances between two sets of points.\n√dh is the feature dimension for each head. This modification improves the stability of Transformers\nwhen used for GAN discriminators.\nImproved Spectral Normalization. To further strengthen the Lipschitz continuity, we also apply\nspectral normalization (SN) (Miyato et al., 2018) in the discriminator training. The standard SN uses\npower iterations to estimate spectral norm of the projection matrix for each layer in the neural network.\nThen it divides the weight matrix with the estimated spectral norm, so Lipschitz constant of the\nresulting projection matrix equals 1. We observe that making the spectral norm equal to 1 stabilized\nthe training but GANs seemed to be underfitting in such settings (c.f .Table 3b). Similarly, we find\nR1 gradient penalty cripples GAN training when ViT-based discriminators are used (c.f .Figure 4).\n(Dong et al., 2021) suggests that the small Lipschitz constant of MLP block may cause the output\nof Transformer collapse to a rank-1 matrix. Without the spectral normalization, our GAN model\ninitially learns in a healthy manner but the training becomes unstable (later on) since we cannot\nguarantee Lipschitzness of Transformer discriminator.\nWe find that multiplying the normalized weight matrix of each layer with the spectral norm at\ninitialization is sufficient to solve this problem. Concretely, we use the following update rule for our\nspectral normalization, where σ computes the standard spectral norm of weight matrices:\n¯WISN(W) := σ(Winit) · W/σ(W). (7)\nOverlapping Image Patches. ViT discriminators are prone to overfitting due to their exceeding\nlearning capacity. Our discriminator and generator use the same image representation that partitions\nan image as a sequence of non-overlapping patches according to a predefined grid P × P. These\narbitrary grid partitions, if not carefully tuned, may encourage the discriminator to memorize local\ncues and stop providing meaningful loss for the generator. We use a simple trick to mitigate this issue\nby allowing some overlap between image patches (Liu et al., 2021b;a). For each border edge of the\npatch, we extend it by o pixels, making the effective patch size (P + 2o) × (P + 2o).\nThis results in a sequence with the same length but less sensitivity to the predefined grids. It may also\ngive the Transformer a better sense of which ones are neighboring patches to the current patch, hence\ngiving a better sense of locality.\nConvolutional Projection. To allow Transformers to leverage local context as well as global\ncontext, we apply convolutions when computing Q, K, V in Equation 5. While variants of this idea\nwere proposed in (Wu et al., 2021; Guo et al., 2021), we find the following simple option works\nwell: we apply 3 ×3 convolution after reshaping image token embeddings into a feature map of\nsize H\nP × W\nP × D. We note that this formulation does not harm the expressiveness of the original\nTransformer, as the original Q, K, V projection can be recovered by using the identity convolution\nkernel.\n4\nPublished as a conference paper at ICLR 2022\nEmbedding \nw\n(B)\n1 2 3 N...\n(A)  \n1 2 3 N...\nTransformer Encoder \n0 w1 w 2 w 3 w ... N w\nLatent  z \nMapping  Network \nNorm\nMulti-Head Attention \nNorm\nMLP \n+\n+\nw\nA\n(C) \nSelf-Modulated LayerNorm \nin Transformer Encoder \nL x \n...\nMapping  Network Mapping  Network \nTransformer Encoder Transformer Encoder \nLatent  z Latent  z \nA\nA\n...\nMLP \n...\nMLP \nMLP \nMLP \nFourier \nEmbedding \nEfou (x, y) \nMLP \n...\nMLP \nMLP \nMLP \n...\nMLP \n...\nMLP \nMLP \nMLP \nFigure 2: Generator Architecture Variants. The diagram on the left shows three generator\narchitectures we consider: (A) adding intermediate latent embeddingw to every positional embedding,\n(B) prepending w to the sequence, and (C) replacing normalization with self-modulated layernorm\n(SLN) computed by learned affine transform (denoted as A in the figure) from w. On the right, we\nshow the details of the self-modulation operation applied in the Transformer block.\n4.2 G ENERATOR DESIGN\nDesigning a generator based on the ViT architecture is a nontrivial task. A challenge is converting\nViT from predicting a set of class labels to generating pixels over a spatial region. Before introducing\nour model, we discuss two plausible baseline models, as shown in Fig. 2 (A) and 2 (B). Both models\nswap ViT’s input and output to generate pixels from embeddings, specifically from the latent vector\nw derived from a Gaussian noise vector z by an MLP, i.e., w = MLP(z) (called mapping network\n(Karras et al., 2019) in Fig. 2). The two baseline generators differ in their input sequences. Fig. 2 (A)\ntakes as input a sequence of positional embeddings and adds the intermediate latent vector w to every\npositional embedding. Alternatively, Fig. 2 (B) prepends the sequence with the latent vector. This\ndesign is inspired by inverting ViT where w is used to replace the classification embedding h0\nL in\nEquation 4.\nTo generate pixel values, a linear projectionE ∈ RD×(P2·C) is learned in both models to map a D-\ndimensional output embedding to an image patch of shape P × P × C. The sequence of N = H×W\nP2\nimage patches [xi\np]N\ni=1 are finally reshaped to form an whole image x.\nThese baseline transformers perform poorly compared to the CNN-based generator. We propose a\nnovel generator following the design principle of ViT. Our ViTGAN Generator, shown in Fig. 2 (c),\nconsists of two components (1) a Transformer block and (2) an output mapping layer.\nh0 = Epos, Epos ∈ RN×D, (8)\nh′\nℓ = MSA(SLN(hℓ−1, w)) + hℓ−1, ℓ = 1, . . . , L,w ∈ RD (9)\nhℓ = MLP(SLN(h′\nℓ, w)) + h′\nℓ, ℓ = 1, . . . , L (10)\ny = SLN(hL, w) = [y1, ··· , yN ] y1, . . . ,yN ∈ RD (11)\nx = [x1\np, ··· , xN\np ] = [fθ(Efou , y1), . . . , fθ(Efou , yN )] xi\np ∈ RP2×C, x ∈ RH×W×C (12)\nThe proposed generator incorporates two modifications to facilitate the training.\nSelf-Modulated LayerNorm. Instead of sending the noise vector z as the input to ViT, we use z to\nmodulate the layernorm operation in Equation 9. This is known as self-modulation (Chen et al., 2019)\nsince the modulation depends on no external information. The self-modulated layernorm (SLN) in\nEquation 9 is computed by:\nSLN(hℓ, w) = SLN(hℓ, MLP(z)) = γℓ(w) ⊙ hℓ − µ\nσ + βℓ(w), (13)\nwhere µ and σ track the mean and the variance of the summed inputs within the layer, and γl and βl\ncompute adaptive normalization parameters controlled by the latent vector derived from z. ⊙ is the\nelement-wise dot product.\n5\nPublished as a conference paper at ICLR 2022\nTable 1: Comparison to representative GAN architectures on unconditional image generation\nbenchmarks. ∗Results from the original papers. All other results are our replications and trained\nwith DiffAug (Zhao et al., 2020a) + bCR for fair comparison. ↓ means lower is better.\nArchitecture CIFAR 10 CelebA 64x64 LSUN 64x64 LSUN 128x128\nFID ↓ IS ↑ FID ↓ IS ↑ FID ↓ IS ↑ FID ↓ IS ↑\nBigGAN+ DiffAug (Zhao et al., 2020a) 8.59 ∗ 9.25∗ - - - - - -\nStyleGAN2 (Karras et al., 2020b) 5.60 9.41 3.39 3.43 2.33 2.44 3.26 2.26\nTransGAN (Jiang et al., 2021) 9.02 ∗ 9.26∗ - - - - - -\nVanilla-ViT 12.7 8.40 20.2 2.57 218.1 2.20 - -\nViTGAN (Ours) 4.92 9.69 3.74 3.21 2.40 2.28 2.48 2.26\nStyleGAN2-D+ViTGAN-G (Ours) 4.57 9.89 - - 1.49 2.46 1.87 2.32\nImplicit Neural Representation for Patch Generation.We use an implicit neural representa-\ntion (Park et al., 2019; Mescheder et al., 2019; Tancik et al., 2020; Sitzmann et al., 2020) to learn a\ncontinuous mapping from a patch embedding yi ∈ RD to patch pixel values xi\np ∈ RP2×C. When\ncoupled with Fourier features (Tancik et al., 2020) or sinusoidal activation functions (Sitzmann\net al., 2020), implicit representations can constrain the space of generated samples to the space of\nsmooth-varying natural signals. Concretely, similarly to (Anokhin et al., 2021), xi\np = fθ(Efou , yi)\nwhere Efou ∈ RP2·D is a Fourier encoding of P × P spatial locations and fθ(·, ·) is a 2-layer MLP.\nFor details, please refer to Appendix D. We find implicit representation to be particularly helpful for\ntraining GANs with ViT-based generators,c.f .Table 3a.\n5 E XPERIMENTS\nWe train and evaluate our model on various standard benchmarks for image generation, including\nCIFAR-10 (Krizhevsky et al., 2009), LSUN bedroom(Yu et al., 2015) and CelebA (Liu et al., 2015).\nWe consider three resolution settings: 32×32, 64×64, 128×128, and 256×256. We strive to use a\nconsistent setup across different resolutions and datasets, and as such, keep all key hyper-parameters,\nexcept for the number of Transformer blocks and patch sizes, the same. For more details about the\ndatasets and implementation, please refer to Appendix C and D.\n5.1 M AIN RESULTS\nTable 1 shows the main results on three standard benchmarks for image synthesis. Our method\nis compared with the following baseline architectures. TransGAN (Jiang et al., 2021) is the only\nexisting convolution-free GAN that is entirely built on the Transformer architecture. Vanilla-ViT is a\nViT-based GAN that employs the generator illustrated in Fig. 2 (A) and a vanilla ViT discriminator\nwithout any techniques discussed in Section 4.1. For a fair comparison, R1 penalty and bCR (Zhao\net al., 2021) + DiffAug (Zhao et al., 2020a) were used for this baseline. The architecture with the\ngenerator illustrated in Fig. 2 (B) is separately compared in Table 3a. In addition, BigGAN (Brock\net al., 2019) and StyleGAN2 (Karras et al., 2020b) are also included as state-of-the-art CNN-based\nGAN models.\nOur ViTGAN model outperforms other Transformer-based GAN models by a large margin. This\nstems from the improved stable GAN training on the Transformer architecture. As shown in Fig. 4,\nour method overcomes the spikes in gradient magnitude and stabilizes the training dynamics. This\nenables ViTGAN to converge to either a lower or a comparable FID on different datasets.\nViTGAN achieves comparable performance to the leading CNN-based models, i.e. BigGAN and\nStyleGAN2. Note that in Table 1, to focus on comparing the architectures, we use a generic version of\nStyleGAN2. More comprehensive comparisons with StyleGAN2 are included in Appendix A.1. As\nshown in Fig. 3, the image fidelity of the best Transformer baseline (Middle Row) has been notably\nimproved by the proposed ViTGAN model (Last Row). Even compared with StyleGAN2, ViTGAN\ngenerates images with comparable quality and diversity. Notice there appears to be a perceivable\ndifference between the images generated by Transformers and CNNs, e.g.in the background of the\nCelebA images. Both the quantitative results and qualitative comparison substantiate the efficacy of\nthe proposed ViTGAN as a competitive Transformer-based GAN model.\n6\nPublished as a conference paper at ICLR 2022\nCIFAR-10 32 × 32\n(a) StyleGAN2 (FID = 5.60)\n (b) Vanilla-ViT (FID =12.7)\n (c) ViTGAN (FID = 4.92)\nCelebA 64 × 64\n(d) StyleGAN2 (FID = 3.39)\n (e) Vanilla-ViT (FID =23.61)\n (f) ViTGAN (FID = 3.74)\nLSUN bedroom 64 × 64\n(g) StyleGAN2 (FID = 2.33)\n (h) Vanilla-ViT (FID =218.1)\n (i) ViTGAN (FID = 2.40)\nFigure 3: Qualitative Comparison. We compare our ViTGAN with StyleGAN2, and our best\nTransformer baseline, i.e., a vanilla pair of ViT generator and discriminator described in Section 5.1,\non the CIFAR-10 32 × 32, CelebA 64 × 64 and LSUN Bedroom 64 × 64 datasets. Results on LSUN\nBedroom 128 × 128 and 256 × 256 can be found in the Appendix.\n7\nPublished as a conference paper at ICLR 2022\n0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300\nDiscriminator training steps (× 1000)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nGradient Norm\nR1\nSN\nOurs\n(a) Gradient L2 Norm (CIFAR)\n0 50 100 150 200 250 300 350 400 450 500 550\nDiscriminator training steps (× 1000)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nGradient Norm\nR1\nSN\nOurs (b) Gradient L2 Norm (CelebA)\n0 50 100 150 200 250 300 350 400 450\nDiscriminator training steps (× 1000)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nGradient Norm\nR1\nSN\nOurs (c) Gradient L2 Norm (LSUN)\n0 50 100 150 200 250 300 350 400 450 500 550\nDiscriminator training steps (× 1000)\n8\n10\n12\n14\n16\n18\nFID ( ↓)\nR1\nSN\nOurs\n(d) FID (CIFAR)\n200 250 300 350 400 450 500 550\nDiscriminator training steps (× 1000)\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nFID ( ↓)\nR1\nSN\nOurs (e) FID (CelebA)\n100 150 200 250 300 350 400 450\nDiscriminator training steps (× 1000)\n0\n5\n10\n15\n20\nFID ( ↓)\nR1\nSN\nOurs (f) FID (LSUN)\nFigure 4: (a-c) Gradient magnitude (L2 norm over all parameters) of ViT discriminator and (d-f)\nFID score (lower the better)as a function of training iteration. Our ViTGAN are compared with\ntwo baselines of Vanilla ViT discriminators with R1 penalty and spectral norm (SN). The remaining\narchitectures are the same for all methods. Our method overcomes the spikes of gradient magnitude,\nand achieve lower FIDs (on CIFAR and CelebA) or a comparable FID (on LSUN).\n5.2 A BLATION STUDIES\nWe conduct ablation experiments on the CIFAR dataset to study the contributions of the key techniques\nand verify the design choices in our model.\nTable 2: ViTGAN on CIFAR-10 when paired\nwith CNN-based generators or discriminators.\nGenerator Discriminator FID ↓ IS ↑\nViTGAN ViTGAN 4.92 9.69\nStyleGAN2 StyleGAN2 5.60 9.41\nStyleGAN2 Vanilla-ViT 17.3 8.57\nStyleGAN2 ViTGAN 8.02 9.15\nViTGAN StyleGAN2 4.57 9.89\nCompatibility with CNN-based GANsIn Ta-\nble 2, we mix and match the generator and\ndiscriminator of our ViTGAN and the leading\nCNN-based GAN: StyleGAN2. With the Style-\nGAN2 generator, our ViTGAN discriminator\noutperforms the vanilla ViT discriminator. Be-\nsides, our ViTGAN generator still works to-\ngether with the StyleGAN2 discriminator. The\nresults show the proposed techniques are com-\npatible with both Transformer-based and CNN-\nbased generators and discriminators.\nGenerator architecture Table 3a shows GAN\nperformances under three generator architectures, as shown in Fig. 2. Fig. 2 (B) underperforms other\narchitectures. We find that Fig. 2 (A) works well but lags behind Fig. 2 (C) due to its instability.\nRegarding the mapping between patch embedding and pixels, implicit neural representation (denoted\nas NeurRep in Table 3a) offers consistently better performance than linear mapping, suggesting the\nimportance of implicit neural representation in the ViTGAN generators.\nDiscriminator regularization Table 3b validates the necessity of the techniques discussed in\nSection 4.1. First, we compare GAN performances under different regularization methods. Training\nGANs with ViT discriminator under R1 penalty (Mescheder et al., 2018) is highly unstable, as\nshown in Fig. 4, sometimes resulting in complete training failure (indicated as IS=NaN in Row 1 of\nTable 3b). Spectral normalization (SN) is better than R1 penalty. But SN still exhibits high-variance\ngradients and therefore suffers from low quality scores. Our L2+ISN regularization improves the\nstability significantly (c.f .Fig. 4) and achieves the best IS and FID scores as a consequence. On the\nother hand, the overlapping patch is a simple trick that yields further improvement over the L2+ISN\nmethod. However, the overlapping patch by itself does not work well (see a comparison between\n8\nPublished as a conference paper at ICLR 2022\nRow 3 and 9). The above results validate the essential role of these techniques in achieving the final\nperformance of the ViTGAN model.\nTable 3: Ablation studies of ViTGAN on CIFAR-10.\nEmbedding Output Mapping FID ↓ IS ↑\nFig 2 (A) Linear 14.3 8.60\nFig 2 (A) NeurRep 11.3 9.05\nFig 2 (B) Linear 328 1.01\nFig 2 (B) NeurRep 285 2.46\nFig 2 (C) Linear 15.1 8.58\nFig 2 (C) NeurRep 6.66 9.30\n(a) Ablation studies of generator architectures.\nNeurRep denotes implicit neural representation.\nViT discriminator without convolutional projection\nis used for this ablation.\nAug. Reg. Overlap Proj. FID ↓ IS ↑\n✗ R1 ✗ ✗ 2e4 NaN\n✗ R1 ✓ ✗ 129 4.99\n✓ R1 ✓ ✗ 13.1 8.71\n✗ SN ✗ ✗ 121 4.28\n✓ SN ✗ ✗ 10.2 8.78\n✓ L2+SN ✗ ✗ 168 2.36\n✓ ISN ✗ ✗ 8.51 9.12\n✓ L2+ISN ✗ ✗ 8.36 9.02\n✓ L2+ISN ✓ ✗ 6.66 9.30\n✓ L2+ISN ✓ ✓ 4.92 9.69\n(b) Ablation studies of discrminator regularization.\n‘Aug.’, ‘Reg.’, ‘Overlap’ and ‘Proj.’ stand for Dif-\nfAug Zhao et al. (2020a) + bCR Zhao et al. (2021),\nand regularization method, overlapping image patches,\nand convolutional projection, respectively.\nTable 4: Ablations studies of self-attention\non LSUN Bedroom 32x32.\nMethod attention local global FID ↓\nStyleGAN2 ✗ ✓ ✗ 2.59\nViT ✓ ✗ ✓ 1.94\nMLP-Mixer ✗ ✗ ✓ 3.23\nViT-local ✓ ✓ ✗ 1.79\nViTGAN ✓ ✓ ✓ 1.57\nNecessity of self-attention Two intrinsic advan-\ntages of ViTs over CNNs are (a) adaptive connection\nweights based on both content and position, and (b)\nglobally contextualized representation. We show the\nimportance of each of them by ablation analysis in\nTable 4. We conduct this ablation study on the LSUN\nBedroom dataset considering that its large-scale (∼3\nmillion images) helps compare sufficiently trained\nvision transformers.\nFirst, to see the importance of adapting connection\nweights, we maintain the network topology (i.e., each\ntoken is connected to all other tokens) and use fixed weights between tokens as in CNNs. We adopt the\nrecently proposed MLP-Mixer (Tolstikhin et al., 2021) as an instantiation of this idea. By comparing\nthe 2nd and 3rd rows of the table, we see that it is beneficial to adapt connection weight between\ntokens. The result of MLP-Mixer on the LSUN Bedroom dataset suggests that the deficiency of\nweight adaptation cannot be overcome by merely training on a large-scale dataset.\nSecond, to see the effect of self-attention scope (global v.s. local), we adopt a local self-attention of\nwindow size=3×3 (Ramachandran et al., 2019). The ViT based on local attention (4th row in Table 4)\noutperforms the ones with global attention. This shows the importance of considering local context\n(2nd row in Table 4). The ViT with convolutional projection (last row in Table 4) — which considers\nboth local and global contexts— outperforms other methods.\nFor results on applying these techniques into either one of discriminator or generator, please refer to\nTable 6 in the Appendix.\n6 C ONCLUSION AND LIMITATIONS\nWe have introduced ViTGAN, leveraging Vision Transformers (ViTs) in GANs, and proposed\nessential techniques to ensuring its training stability and improving its convergence. Our experiments\non standard benchmarks demonstrate that the presented model achieves comparable performance\nto leading CNN-based GANs. Regarding the limitations, ViTGAN is a novel GAN architecture\nconsisting solely of Transformer layers. This could be improved by incorporating advanced training\n(e.g., (Jeong & Shin, 2021; Schonfeld et al., 2020)) or coarse-to-fine architectures (e.g., (Liu et al.,\n2021b)) into the ViTGAN framework. Besides, we have not verified ViTGAN on high-resolution\nimages. Our paper establishes a concrete baseline of ViT on resolutions up to 256×256 and we hope\nthat it can facilitate future research to use vision transformers for high-resolution image synthesis.\n9\nPublished as a conference paper at ICLR 2022\nETHICS STATEMENT\nWe acknowledge that the image generation techniques have the risks to be misused to produce\nmisleading information. Researchers should explore the techniques responsibly. Future research\nmay have ethical and fairness implication especially when generating images of identifiable faces.\nIt is unclear whether the proposed new architecture would differentially impact groups of people\nof certain appearance (e.g., skin color), age (young or aged), or with a physical disability. These\nconcerns ought to be considered carefully. We did not undertake any subject studies or conduct any\ndata-collection for this project. We are committed in our work to abide by the eight General Ethical\nPrinciples listed at ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics)\nREPRODUCIBILITY STATEMENT\nAll of our experiments are conducted on public benchmarks. We describe the implementation details\nin the experiment section and in the Appendix C and Appendix D. We will release our code and\nmodels to ensure reproducibility.\nACKNOWLEDGMENTS\nThis work was supported in part by Google Cloud Platform (GCP) Credit Award. We would also\nlike to acknowledge Cloud TPU support from Google’s TensorFlow Research Cloud (TRC) program.\nAlso, we thank the anonymous reviewers for their helpful and constructive comments and suggestions.\nREFERENCES\nIvan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor Lempitsky, and Denis\nKorzhenkov. Image generators with conditionally-independent pixel synthesis. In CVPR, 2021.\nMartin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks.\nIn ICML, 2017.\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid.\nVivit: A video vision transformer. arXiv preprint arXiv:2103.15691, 2021.\nJosh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew Zhai, and Dmitry Kislyuk. Toward\ntransformer-based object detection. arXiv preprint arXiv:2012.09958, 2020.\nGedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video\nunderstanding? In ICML, 2021.\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\nimage synthesis. In ICLR, 2019.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.\nHanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In ICML, 2020a.\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, and\nIlya Sutskever. Generative pretraining from pixels. In ICML, 2020b.\nTing Chen, Mario Lucic, Neil Houlsby, and Sylvain Gelly. On self modulation for generative\nadversarial networks. In ICLR, 2019.\n10\nPublished as a conference paper at ICLR 2022\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\ntransformers. CoRR, abs/2104.02057, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In ACL, 2019.\nYihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure\nattention loses rank doubly exponentially with depth. arXiv preprint arXiv:2103.03404, 2021.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.\nIn ICLR, 2021.\nPatrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image\nsynthesis. In CVPR, 2021.\nWilliam Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, and\nIan J. Goodfellow. Many paths to equilibrium: Gans do not need to decrease a divergence at every\nstep. In ICLR, 2018.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved\ntraining of wasserstein gans. In NeurIPS, 2017.\nJianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang. Cmt:\nConvolutional neural networks meet vision transformers, 2021.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017.\nDrew A Hudson and C. Lawrence Zitnick. Generative adversarial transformers. arXiv\npreprint:2103.01209, 2021.\nJongheon Jeong and Jinwoo Shin. Training {gan}s with stronger augmentations via contrastive dis-\ncriminator. In ICLR, 2021. URL https://openreview.net/forum?id=eo6U4CAwVmg.\nYifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two transformers can make one strong\ngan. arXiv preprint arXiv:2102.07074, 2021.\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for\nimproved quality, stability, and variation. In ICLR, 2018.\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In CVPR, 2019.\nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training\ngenerative adversarial networks with limited data. In NeurIPS, 2020a.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing\nand improving the image quality of StyleGAN. In CVPR, 2020b.\nHyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention. In\nICML, 2021.\nNaveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans.\narXiv preprint arXiv:1705.07215, 2017.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\nTechnical report, Citeseer, 2009.\n11\nPublished as a conference paper at ICLR 2022\nY . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel.\nBackpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551,\n1989. doi: 10.1162/neco.1989.1.4.541.\nRui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei Lu, Wenxiu Sun, Xiaogang Wang, Jifeng\nDai, and Hongsheng Li. Fuseformer: Fusing fine-grained information in transformers for video\ninpainting. In ICCV, 2021a.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021b.\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\nICCV, 2015.\nLars Mescheder, Sebastian Nowozin, and Andreas Geiger. Which training methods for gans do\nactually converge? In ICML, 2018.\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.\nOccupancy networks: Learning 3d reconstruction in function space. In CVPR, 2019.\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for\ngenerative adversarial networks. In ICLR, 2018.\nAlfred Müller. Integral probability metrics and their generating classes of functions. Advances in\nApplied Probability, pp. 429–443, 1997.\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers\nusing variational divergence minimization. In NeurIPS, 2016.\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf:\nLearning continuous signed distance functions for shape representation. In CVPR, 2019.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. In ICLR, 2016.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\nStand-alone self-attention in vision models. In NeurIPS, 2019.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\nRené Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.\nArXiv preprint, 2021.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. In NeurIPS, 2016.\nEdgar Schonfeld, Bernt Schiele, and Anna Khoreva. A u-net based discriminator for generative\nadversarial networks. In CVPR, 2020.\nVincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon\nWetzstein. Implicit neural representations with periodic activation functions. In NeurIPS, 2020.\nJiaming Song and Stefano Ermon. Bridging the gap between f-gans and wasserstein gans. In ICML,\n2020.\nRobin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for\nsemantic segmentation. arXiv preprint arXiv:2105.05633, 2021.\nMatthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh\nSinghal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn\nhigh frequency functions in low dimensional domains. In NeurIPS, 2020.\n12\nPublished as a conference paper at ICLR 2022\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-\nterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey\nDosovitskiy. Mlp-mixer: An all-mlp architecture for vision. arXiv preprint arXiv:2105.01601,\n2021.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efficient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\nNgoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, and Ngai-Man Cheung.\nOn data augmentation for gan training. IEEE Transactions on Image Processing, 30:1882–1897,\n2021.\nHung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang. Regularizing generative\nadversarial networks under limited data. In CVPR, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\nTete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, and Ross Girshick. Early\nconvolutions help transformers see better, 2021.\nFisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-\nscale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,\n2015.\nHan Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and Augustus Odena. Self-attention generative\nadversarial networks. In ICML, 2019.\nHan Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for\ngenerative adversarial networks. In ICLR, 2020.\nShengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for\ndata-efficient gan training. In NeurIPS, 2020a.\nZhengli Zhao, Zizhao Zhang, Ting Chen, Sameer Singh, and Han Zhang. Image augmentations for\nGAN training. arXiv preprint:2006.02595, 2020b.\nZhengli Zhao, Sameer Singh, Honglak Lee, Zizhao Zhang, Augustus Odena, and Han Zhang.\nImproved consistency regularization for gans. In AAAI, 2021.\nZhiming Zhou, Jiadong Liang, Yuxuan Song, Lantao Yu, Hongwei Wang, Weinan Zhang, Yong Yu,\nand Zhihua Zhang. Lipschitz generative adversarial nets. In ICML, 2019.\n13\nPublished as a conference paper at ICLR 2022\nA M ORE RESULTS\nA.1 E FFECTS OF DATA AUGMENTATION\nTable 5 presents the comparison of the Convolution-based GAN architectures (BigGAN and Style-\nGAN2) and our Transformer-based architecture (ViTGAN). This table complements the results in\nTable 1 of the main paper by a closer examination of the network architecture performance with and\nwithout using data augmentation. The differentiable data augmentation (DiffAug) (Zhao et al., 2020a)\nis used in this study.\nAs shown, data augmentation plays more critical role in ViTGAN. This is not unexpected because\ndiscriminators built on Transformer architectures are more capable of over-fitting or memorizing\nthe data. DiffAug increases the diversity of the training data, thereby mitigating the overfitting\nissue in adversarial training. Nevertheless, with DiffAug, ViTGAN performs comparably to the\nleading-performing CNN-based GAN models: BigGAN and StyleGAN2.\nIn addition, Table 5 includes the model performance without using the balanced consistency regular-\nization (bCR) (Zhao et al., 2021).\nTable 5: Effectiveness of data augmentation and regularization on the CIFAR-10 dataset. ViTGAN\nresults here are based on the ViT backbone without convolutional projection.\nMethod Data Augmentation Conv FID ↓ IS ↑\nStyleGAN2 None Y 8.32 9.21\nStyleGAN2 DiffAug Y 5.60 9.41\nViTGAN None N 30.72 7.75\nViTGAN DiffAug N 6.66 9.30\nViTGAN w/o. bCR DiffAug N 8.84 9.02\nA.2 FID OF STYLE GAN2\nWe implemented StyleGAN2, Vanilla-ViT, and ViTGAN on the same codebase to allow for a fair\ncomparison between the methods. Our implementation follows StyleGAN2 + DiffAug (Zhao et al.,\n2020a) and reproduces the StyleGAN2 FID reported in (Zhao et al., 2020a) which was published in\nNeurIPS 2020: the FID of our StyleGAN2 (+ DiffAug) re-implementation is 5.60 versus the 5.79\nFID reported in Zhao et al. (2020a).\nThere is another contemporary work called StyleGAN2 (+ ADA) (Karras et al., 2020a). Both\nStyleGAN2 (+ DiffAug) (Zhao et al., 2020a) and StyleGAN2 (+ ADA) (Karras et al., 2020a) use the\nsame StyleGAN2 architecture which is considered as the state-of-the-art CNN-based GAN. Due to\ntheir differences in data augmentation methods and hyperparameter settings, they report different\nFIDs on the CIFAR-10 dataset.\nNote our goal is to compare the architecture difference under the same training setting, we only\ncompare StyleGAN2 (+ DiffAug) (Zhao et al., 2020a) and use the same data augmentation method\nDiffAug in both StyleGAN2 and ViTGAN.\nA.3 N ECESSITY OF SELF -ATTENTION\nIn Table 6, we extend the result from Table 4 by applying self-attention or local attention to either one\nof discriminator (D) or generator (G). The comparison between ViT vs. MLP shows that it is more\nimportant to use adaptive weights on the generator than the discriminator. The comparison between\nViT and ViT-local shows that it is beneficial to have locality on both generator and discriminator.\nA.4 H IGH -RESOLUTION SAMPLES\nFigure 5 and Figure 6 show uncurated samples from our ViTGAN model trained on LSUN Bedroom\n128 × 128 and LSUN Bedroom 256 × 256, respectively. For 256 × 256 resolution, we use the\n14\nPublished as a conference paper at ICLR 2022\nTable 6: Detailed ablations studies of self-attention on LSUN Bedroom 32x32.\nMethod FID ↓\nViT-D + ViT-G 1.94\nMLP-D + ViT-G 2.33\nViT-D + MLP-G 2.86\nMLP-D + MLP-G 3.23\nViT-local-D + ViT-local-G 1.79\nViT-local-D + ViT-G 2.03\nViT-D + ViT-local-G 2.04\nsame architecture as 128 × 128 resolution except for increasing the sequence length to 1024 for the\ngenerator. The FID of ViTGAN on LSUN Bedroom 256 × 256 is 4.67. Our method outperforms\nGANformer (Hudson & Zitnick, 2021), which achieved an FID of 6.51 on LSUN Bedroom256×256.\nFigure 5: Samples from ViTGAN on128 × 128 resolution. Our ViTGAN was trained on LSUN\nBedroom 128 × 128 dataset. We achieved the FID of 2.48.\n15\nPublished as a conference paper at ICLR 2022\nFigure 6: Samples from ViTGAN on256 × 256 resolution. Our ViTGAN was trained on LSUN\nBedroom 256 × 256 dataset. We achieved the FID of 4.67.\n16\nPublished as a conference paper at ICLR 2022\nB C OMPUTATION COST ANALYSIS\nWe would like to note that computational complexity is not the main challenge of using Transformers\nfor high-resolution datasets. Due to various smart tokenization strategies such as non-overlapping\npatches (Dosovitskiy et al., 2021), stack of strided convolutions (Xiao et al., 2021), etc, we are\nable to suppress the explosion of effective sequence length — it is typically just a few hundred for\nstandard image resolutions. As a result, the sequence length stays within a region where a quadratic\ntime complexity is not a big issue. Actually, as shown in Table 7, ViTs are much more compute\nefficient than CNN (StyleGAN2). For our ViT-based discriminator, owing to the approach from (Xiao\net al., 2021), we were able to scale up from 32 ×32 to 128×128 without the increase in sequence\nlength (=64). For our ViT-based generator, since we did not employ any convolution stack, we\nhad to increase the sequence length as we increased the resolution. This issue, in principle, can\nbe addressed by adding up-convolutional (deconvolutional) blocks or Swin Transformer (Liu et al.,\n2021b) blocks after the initial low-resolution Transformer stage. Table 7 reveals that we are able to\nsuppress the increase in computation cost at higher-resolutions by using Swin Transformer blocks.\nFor this hypothetical ViTGAN-Swin variant, we use Swin Transformer blocks starting from 1282\nresolution stage, and the number of channels is halved for every upsampling stage as done in Swin\nTransformer.\nPlease note that the main goal of this paper is not to design a new Transformer block for general\ncomputer vision applications; we instead focus on addressing challenges unique to the combination\nof Transformers and GANs. Techniques presented in this paper are still valid for numerous variants\nof ViTs such as Swin (Liu et al., 2021b), CvT (Wu et al., 2021), CMT (Guo et al., 2021), etc.\nThe experiments in the current paper is not based on Swin Transformers since using Swin Trans-\nformers resulted in inferior FID according to our experiments. However, we believe that with careful\ntuning of architectural configurations such as numbers of heads, channels, layers,etc., ViTGAN-Swin\nmay be able to achieve a better FID score. We were unable to perform this due to our limited\ncomputational resources. We leave leave it to future work.\nTable 7: Generator computation cost comparison among methods.\nMethod #Params@642 FLOPs@642 FLOPs@1282 FLOPs@2562\nStyleGAN2 (Karras et al., 2020b) 24M 7.8B 11.5B 15.2B\nViTGAN 38M 2.6B 11.8B 52.1B\nViTGAN-Swin N/A N/A 3.1B 3.5B\nC E XPERIMENT DETAILS\nDatasets The CIFAR-10 dataset (Krizhevsky et al., 2009) is a standard benchmark for image\ngeneration, containing 50K training images and 10K test images. Inception score (IS) (Salimans\net al., 2016) and Fréchet Inception Distance (FID) (Heusel et al., 2017) are computed over the 50K\nimages. The LSUN bedroomdataset (Yu et al., 2015) is a large-scale image generation benchmark,\nconsisting of ∼3 million training images and 300 images for validation. On this dataset, FID is\ncomputed against the training set due to the small validation set. The CelebA dataset (Liu et al., 2015)\ncomprises 162,770 unlabeled face images and 19,962 test images. We use the aligned version of\nCelebA, which is different from cropped version used in prior literature (Radford et al., 2016; Jiang\net al., 2021).\nImplementation Details. We consider three resolution settings: 32 ×32 on the CIFAR dataset,\n64×64 on CelebA and LSUN bedroom, and 128×128 on LSUN bedroom. For 32×32 resolution, we\nuse a 4-block ViT-based discriminator and a 4-block ViT-based generator. For 64×64 resolution, we\nincrease the number of blocks to 6. Following ViT-Small (Dosovitskiy et al., 2021), the input/output\nfeature dimension is 384 for all Transformer blocks, and the MLP hidden dimension is 1,536. Unlike\n(Dosovitskiy et al., 2021), we choose the number of attention heads to be 6. We find increasing\nthe number of heads does not improve GAN training. For 32 ×32 resolution, we use patch size\n4×4, yielding a sequence length of 64 patches. For 64×64 resolution, we simply increase the patch\nsize to 8×8, keeping the same sequence length as in 32 ×32 resolution. For 128 ×128 resolution\n17\nPublished as a conference paper at ICLR 2022\ngenerator, we use patch size 8×8 and 8 Transformer blocks. For 128×128 resolution discriminator,\nwe maintain the sequence length of 64 and 4 Transformer blocks. Similarly to (Xiao et al., 2021),\n3 × 3 convolutions with stride 2 were used until desired sequence length is reached.\nTranslation, Color, Cutout, and Scaling data augmentations (Zhao et al., 2020a; Karras et al., 2020a)\nare applied with probability 0.8. All baseline transformer-based GAN models, including ours, use\nbalanced consistency regularization (bCR) with λreal = λfake = 10.0. Other than bCR, we do\nnot employ regularization methods typically used for training ViTs (Touvron et al., 2020) such as\nDropout, weight decay, or Stochastic Depth. We found that LeCam regularization (Tseng et al., 2021),\nsimilar to bCR, improves the performance. But for clearer ablation, we do not include the LeCam\nregularization. We train our models with Adam with β1 = 0.0, β2 = 0.99, and a learning rate of\n0.002 following the practice of (Karras et al., 2020b). In addition, we employ non-saturating logistic\nloss (Goodfellow et al., 2014), exponential moving average of generator weights (Karras et al., 2018),\nand equalized learning rate (Karras et al., 2018). We use a mini-batch size of 128.\nBoth ViTGAN and StyleGAN2 are based on Tensorflow 2 implementation2 and trained on Google\nCloud TPU v2-32 and v3-8.\nD I MPLEMENTATION NOTES\nPatch Extraction We use a simple trick to mitigate the over-fitting of the ViT-based discriminator\nby allowing some overlap between image patches. For each border edge of the patch, we extend it by\no pixels, making the effective patch size(P + 2o) ×(P + 2o), where o = P\n2 . Although this operation\nhas a connection to a convolution operation with kernel (P + 2o) × (P + 2o) and stride P × P, we\ndo not regard it as a convolution operator in our model because we do not use convolution in our\nimplementation. Note that the extraction of (non-overlapping) patches in the Vanilla ViT (Dosovitskiy\net al., 2021) also has a connection to a convolution operation with kernel P × P and stride P × P.\nPositional Embedding Each positional embedding of ViT networks is a linear projection of patch\nposition followed by a sine activation function. The patch positions are normalized to lie between\n−1.0 and 1.0.\nImplicit Neural Representation for Patch GenerationEach positional embedding is a linear\nprojection of pixel coordinate followed by a sine activation function (hence the name Fourier\nencoding). The pixel coordinates for P2 pixels are normalized to lie between −1.0 and 1.0. The\n2-layer MLP takes positional embedding Efou as its input, and it is conditioned on patch embedding\nyi via weight modulation as in (Karras et al., 2020b; Anokhin et al., 2021).\n2https://github.com/moono/stylegan2-tf-2.x\n18",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.47180691361427307
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4467323124408722
    },
    {
      "name": "Computer science",
      "score": 0.4455045759677887
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41827335953712463
    },
    {
      "name": "Computer vision",
      "score": 0.3446577191352844
    },
    {
      "name": "Engineering",
      "score": 0.1385825276374817
    },
    {
      "name": "Geography",
      "score": 0.13041478395462036
    },
    {
      "name": "Electrical engineering",
      "score": 0.10382121801376343
    },
    {
      "name": "Voltage",
      "score": 0.03982245922088623
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1283473643",
      "name": "Honda (Japan)",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I2800935791",
      "name": "UC San Diego Health System",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    }
  ]
}