{
  "title": "TTS-GAN: A Transformer-Based Time-Series Generative Adversarial Network",
  "url": "https://openalex.org/W4211265874",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5042058386",
      "name": "Xiaomin Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5031305480",
      "name": "Vangelis Metsis",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5042058204",
      "name": "Huangyingrui Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5110645326",
      "name": "Anne H. H. Ngu",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963470893",
    "https://openalex.org/W2584009249",
    "https://openalex.org/W2964024144",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W4283794074",
    "https://openalex.org/W3176640400",
    "https://openalex.org/W6601854141",
    "https://openalex.org/W3167297489",
    "https://openalex.org/W2089468765",
    "https://openalex.org/W2085963752",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W2964337551",
    "https://openalex.org/W2555209581",
    "https://openalex.org/W1984193729",
    "https://openalex.org/W2162800060",
    "https://openalex.org/W2593414223"
  ],
  "abstract": null,
  "full_text": "TTS-GAN: A Transformer-based Time-Series\nGenerative Adversarial Network\nXiaomin Li, Vangelis Metsis, Huangyingrui Wang, Anne Hee Hiong Ngu\nx_l30, vmetsis, h_w91, angu @txstate.edu\nTexas State University, San Marcos TX 78666, USA\nAbstract. Signal measurements appearing in the form of time series are\none of the most common types of data used in medical machine learning\napplications. However, such datasets are often small, making the train-\ning of deep neural network architectures ineﬀective. For time-series, the\nsuite of data augmentation tricks we can use to expand the size of the\ndataset is limited by the need to maintain the basic properties of the\nsignal. Data generated by a Generative Adversarial Network (GAN) can\nbe utilized as another data augmentation tool. RNN-based GANs suf-\nfer from the fact that they cannot eﬀectively model long sequences of\ndata points with irregular temporal relations. To tackle these problems,\nwe introduce TTS-GAN, a transformer-based GAN which can success-\nfully generate realistic synthetic time-series data sequences of arbitrary\nlength, similar to the real ones. Both the generator and discriminator\nnetworks of the GAN model are built using a pure transformer encoder\narchitecture. We use visualizations and dimensionality reduction tech-\nniques to demonstrate the similarity of real and generated time-series\ndata. We also compare the quality of our generated data with the best\nexisting alternative, which is an RNN-based time-series GAN.\nTTS-GAN source code: github.com/imics-lab/tts-gan\nKeywords: GenerativeAdversarialNetwork · Transformer· Time-Series\nAnalysis · Medical Signal\n1 Introduction\nData shortage is often an issue when analyzing physiology based time-series sig-\nnals with deep learning models. Unlike images and text data used in computer\nvision (CV) and natural language processing (NLP) tasks, which are abundant\non the web, such signals are collected as sensor measurements resulting from\nphysical or biological process. Especially when such processes involve human\nsubjects, data collection, annotation, and interpretation is a costly endeavour.\nFurthermore, diﬀerences in the various collection conﬁgurations make it harder\nfor data collected in diﬀerent settings to be merged together to form larger\ndatasets. Deep learning models require large amounts of data to train success-\nfully. Training deep learning models with a high number of trainable parameters\non small datasets results in over-ﬁtting and low generalization capabilities. As a\narXiv:2202.02691v2  [cs.LG]  26 Jun 2022\n2 X. L. et al.\ncompromise researchers are forced to train shallower deep learning models that\nare not capable of capturing the full complexity of the problem at hand. This is\na common situation encountered in medical and health-related machine learning\nresearch.\nGenerative Adversarial Networks (GANs), ﬁrst introduced in 2014 [1], have\nbeen gaining traction in the deep learning research ﬁeld. They have successfully\ngenerated and manipulated data in CV and NLP domains, such as high-quality\nimage generation [2], style transfer [3], text-to-image synthesis [4], etc. There has\nalso been a movement towards using GANs for time series and sequential data\ngeneration, and forecasting. The review paper [5] gives a thorough summary of\nGAN implementations on time series data.\nA GAN is a generative model consisting of a generator and discriminator,\ntypically two neural network (NN) models. The generator takes as input ran-\ndom vectors of speciﬁed dimensions and generates output vectors of the same\ndimension that are similar to the real training data. The discriminator is a binary\nclassiﬁer used to distinguish the real data and generated data. The generator and\ndiscriminator are updated by back-propagation alternately, playing a zero-sum\ngame against each other and until they reach an equilibrium.\nThe transformer architecture, which relies on multiple self-attention lay-\ners [6], has recently become a prevalent deep learning model architecture. It\nhas been shown to surpass many other popular neural network architectures,\nsuch as CNN over images and RNN over sequential data [7,8], and it has even\ndisplayed properties of a universal computation engine [9]. Some works have al-\nready tried to utilize the transformer model in GAN model architecture design\nwith the goal to either improve the quality of synthetic data or to create a more\neﬃcient training process [10,11] for image and text generation tasks. In work [10],\nthe author, for the ﬁrst time, built a pure transformer-based GAN model and\nveriﬁed its performance on multiple image synthesis tasks.\nPrevious eﬀorts for creating a time-series GAN have mainly relied on Recur-\nrentNeuralNetwork(RNN)-basedarchitectures[12,13,14].Sincethetransformer\nwas ﬁrst invented to handle very long sequential data and does not suﬀer from a\nvanishing gradient problem, theoretically, a transformer GAN model should per-\nform better than other RNN-based models on time-series data. In this work, we\nfollow a process similar to the one Jiang et.al. [10] followed for image generation,\nadapted for time-series data.\nSincetime-seriesdataarenoteasilyinterpretablebyhumans,weusePCA[15]\nand t-SNE [16] to map the multi-dimensional output sequence vectors into two\ndimensions to visually observe the similarity in the distribution of the syn-\nthetic data and real data instances. For a more quantitative comparison, we also\nmeasure several well-known signal properties and compare the similarity of the\ntransformer-generated as well as RNN-generated sequences with real sequences\nof the same class.\nOur contributions can be summarized as follows:\n– We create a pure transformer-based GAN model to generate synthetic time-\nseries data.\nTTS-GAN 3\n– We propose several heuristics to more eﬀectively train a transformer-based\nGAN model on time-series data.\n– We qualitatively and quantitatively compare the quality of the generated\nsequences against real ones and against sequences generated by other state-\nof-the-art time-series GAN algorithms.\nThe rest of the paper is organized as follows. Section 2 discusses the back-\nground and most popular applications of GANs and transformer models. In\nsection 3, we provide the details of our TTS-GAN model architecture and how\nwe process time-series data to feed this model. In section 4, we visually and\nquantitatively verify the ﬁdelity of the synthetic data. Section 5 summarizes our\nwork and concludes this paper.\n2 Background\n2.1 Generative Adversarial Networks (GANs)\nGANs consist of two models, a generator and a discriminator. These two models\nare typically implemented by neural networks, but they can be implemented with\nany form of diﬀerentiable system that maps data from one space to the other.\nThe generator tries to capture the distribution of true examples for new data\nexample generation. The discriminator is usually a binary classiﬁer, discriminat-\ning generated examples from the true examples as accurately as possible. The\noptimization of GANs is a minimax optimization problem, in which the goal\nis to reach Nash equilibrium [17] of the generator and discriminator. Then, the\ngenerator can be thought to have captured the real distribution of true examples.\nGANs have had many applications in diﬀerent areas, but mostly in CV and\nNLP. For example, it can generate examples for image datasets [18], front view\nfaces [19], text-to-image translation [4], etc. While these successes have drawn\nmuch attention, GAN applications have diversiﬁed across disciplines such as\ntime-series data generation. The work [5] gives a thorough summary of the GAN\nimplementations in this ﬁeld. The applicability of GANs to this type of data\ncan solve many issues that current dataset holders face. For example, GANs\ncan augment smaller datasets by generating new, previously unseen data. GANs\ncan replace the artifacts with information representative of clean data. And it\ncan also be used to denoise signals. GANs can also ensure an extra layer of\ndata protection by generating deferentially private datasets containing no risk\nof linkage from source to generated datasets.\n2.2 Transformer\nThe transformer is the state-of-the-art neural network architecture. Unlike recur-\nrent neural networks, which consume a sequence token by token, in a transformer\nnetwork, the entire sequence is fed into layers of transformer modules. The rep-\nresentation of a token at a layer is then computed by attending to the latent\n4 X. L. et al.\nrepresentations of all the other tokens in the preceding layer. Many works in the\nNLP ﬁeld have proved its performance [6,8].\nGiven its strong representation capabilities, researchers have also applied\ntransformers to computer vision tasks. In a variety of visual benchmarks, trans-\nformer models perform similar to or better than other types of networks, such\nas convolutional and recurrent networks. The work in [7] builds a model named\nViT, which applies a pure transformer directly to sequences of image patches.\nThe work in [10] builds a pure transformer GAN model to generate synthetic\nimages,wherethediscriminatordesigningideaisfromtheViTmodel.Themulti-\ndimension time-series data we are dealing with has similarities from both texts\nand images, meaning a sequence contains both temporal and spatial information.\nEach timestep in a sequence is like a pixel on one image. The whole sequence\ncontains an event or multiple events happening, which is similar to a sentence\nin NLP tasks.\nIn this work, we adapt the ideas used in [7] and [10] for images, and view a\ntime-series sequence as aC×H×W tuple, whereC is the number of channels\nof the time-series data,H corresponds to the height of the image, but for time-\nseries that value is set to 1, andW corresponds to the width of the image, which\nfor times-series is the number of timesteps in the sequence. We divide the tuple\ninto multiple patches on theW axis and provide positional encoding to each\npatch. To our best knowledge, it is the ﬁrst work to implement such an idea to\nprocess time-series data and apply it to a transformer GAN model.\n3 Methodology\n3.1 Transformer Time-Series GAN Model Architecture\nThe TTS-GAN model architecture is shown in Fig. 1. It contains two main\ncomponents, a generator, and a discriminator. Both of them are built based on\nthe transformer encoder architecture [6]. An encoder is a composition of two\ncompound blocks. A multi-head self-attention module constructs the ﬁrst block\nand the second block is a feed-forward MLP with GELU activation function. The\nnormalizationlayerisappliedbefore both ofthetwoblocksandthe dropout layer\nis added after each block. Both blocks employ residual connections.\nThe generator ﬁrst takes in a 1D vector with N uniformly distributed ran-\ndom numbers values within the range (0,1), i.e.Ni ∼U(0,1) . N represents\nthe latent dimension of the synthetic signals, which is a hyperparameter that\ncan be tuned. The vector is then mapped to a sequence with the same length\nof the real signals and M embedding dimensions. M is also a hyperparameter\nthat can be changed and not necessarily equal to real signal dimensions. Next,\nthe sequence is divided into multiple patches, and a positional encoding value\nis added to each patch. Those patches are then input to the transformer en-\ncoder blocks. Then the encoder blocks outputs are passed through a Conv2D\nlayer to reduce the synthetic data dimensions. The Conv2D layer is set to have\na kernel size(1,1), which won’t change the width and height of the synthetic\nTTS-GAN 5\nNoise Input \n1D vector\nLinear\nTransform\nPositional Embedding\nLayer Norm\nMulti-head\nAttention\nDropout\nLayer Norm\nFeed Forward\nLayer\nDropout\nRepeat \nN x\nConv2D channel reduction\nSynthetic Signals\nGenerator\nSynthetic Signals  \n& Real Signals\nPositional Embedding\nLayer Norm\nMulti-head\nAttention\nDropout\nLayer Norm\nFeed Forward\nLayer\nDropout\nClassification Head\nReal / Synthetic\nSignals ? \nRepeat \nM x\nDiscriminator\nFig.1: TTS-GAN model architecture\ndata. The ﬁlter size is set to the same dimension size as the real data sequences.\nTherefore, a synthetic data sequence after the generator transformer encoder\nlayers with a data shape(hiddendimensions,1,timesteps) will be mapped to\n(realdatadimensions,1,timesteps). In this way, a random noise vector is trans-\nformed into a sequence with the same shape as the real signals.\nThe discriminator architecture is similar to the ViT model [7], which is a\nbinary classiﬁer to distinguish whether the input sequence is a real signal or\nsynthetic one. In the ViT model, an image is divided evenly into multiple patches\nwith the same width and height. However, in TTS-GAN, we view any input\nsequences like an image with a height of 1. The timesteps of the inputs are\nimage widths. Therefore, to add positional encoding on time series inputs, we\nonly need to divide the width evenly into multiple pieces and keep the height of\neach piece unchanged. This process is explained in detail in section 3.2.\n3.2 Processing Time-Series Data like an image\nWe view a time-series data sequence like an image with a height equal to 1. The\nnumberoftimestepsisthewidthofanimage, W.Atime-seriessequencecanhave\na single channel or multiple channels, and those can be viewed as the number\n6 X. L. et al.\nC dim\nW timesteps\nPositional\nEncoding\nP1 P2 PN...\nInput size \n(Batch Size, C, 1, W)\nFig.2: Processing time-series data\nof channels (RGB) of an image,C. So the input sequences can be represented\nwith the matrix of size(BatchSize,C, 1,W). Then we choose a patch sizeN\nto divide a sequence intoW/N patches. We then add a soft positional encoding\nvalue by the end of each patch, the positional value is learned during model\ntraining. Therefore the inputs to the discriminator encoder blocks will have the\ndata shape(BatchSize,C, 1,(W/N) + 1). This process is shown in Fig. 2.\n3.3 Updating Generator and Discriminator Parameters\nThe transformer blocks in the generator and discriminator both use the Mean\nSquared Error loss to update the parameters. We can usez to denote input\nvectors to the generator. UseG(z) to represent the synthetic data generated by\nthe generator. We use the preﬁxreal to represent the real input signals.D(x)\nis the classiﬁcation output of the discriminator.x can be the real signals or\nsynthetic signals.real_label is set to 1 andfake_label is set to 0. To stabilize\nthe GAN model training, some heuristics can be used when setting label values.\nFor example, we can use soft labels thatreal_label is a ﬂoat number close to\n1 andfake_label is a ﬂoat number close to 0. Sometimes, we can also ﬂip the\nvalues of thereal_label and thefake_label. The usefulness of these strategies\nhas been so fat been tested only on a case-by-case basics. The discriminator loss\ncan be represented as:\nd_real_loss= MSELoss(D(real),real_label)\nd_fake_loss= MSELoss(D(G(z)),fake_label)\nd_loss= d_real_loss+ d_fake_loss\nThe discriminator loss is the sum of real data loss and fake data (synthetic data)\nlosses. The generator loss can be represented as:\ng_loss= MSELoss(D(G(z)),real_label)\n4 Experiments\n4.1 Datasets\nWe evaluate the TTS-GAN model on three datasets. Simulated sinusoidal waves,\nUniMiB human activity recognition (HAR) dataset [20] and the PTB Diagnostic\nTTS-GAN 7\nECG Database [21,22]. A few raw data samples for each dataset are shown in\nFig. 3a.\nThe sinusoidal wavesare simulated with random frequenciesAand phases\nB values between [0, 0.1]. The sequence length is 24 and the number of dimen-\nsions is 5. For each dimensioni∈{1,..., 5}, the sequence can be represented with\nthe formula xi(t) = sin(At+ B), where A ∈(0,0.1) and B ∈(0,0.1). A total\nnumber of 10000 simulated sinusoidal waves are used to train the GAN model.\nFor theUniMiB datase[20], we select 2 categories (Jumping and Running)\nsamples from 24 subjects’ recordings to train GAN models. The two classes have\n600 and 1572 samples respectively. Every sample has 150 timesteps and three\naccelerator values at each timestep. All of the recordings are channel-wisely\nnormalized to a mean of 0 and a variance of 1.\nThe PTB Diagnostic ECG dataset [21,22] contains human heartbeat\nsignals in two categories, normal and abnormal with 4046 and 10506 samples re-\nspectively. Each sequence represents a heart beat sampled at 125Hz. The original\nlengthofeachsequenceis188,paddedwithzerosattheendtocreateﬁxed-length\nsequences. We only use the timesteps 5 to 55 of each sample, which is the part\nof the sequence containing the most useful information of the heatbeat.\n4.2 Evaluation\nWe evaluate TTS-GAN using qualitative visualizations and quantitative metrics,\nand compare it with Time-GAN [13], which is the best current alternative.\nRaw data visualization:Fig. 3b shows samples of synthetic data generated\nby TTS-GAN. Comparing them to the real data in Fig. 3a, we can observe that\nthe synthetic data present visually similar signal patterns to the real data.\nVisualizations with PCA and t-SNE:To further illustrate the similarity\nbetween the real data and synthetic data, we plot visualization example graphs\nof data point distributions mapped to two dimensions using PCA and t-SNE\nin Figure 4. In these plots, red dots denote original data, and blue dots denote\nsynthetic data generated by TTS-GAN. Again, we notice a similar distribution\npattern between real and synthetic data.\nSimilarity scores: To quantitatively compare the similarity of the real and\ngenerated sequences, we deﬁned two similarity scores, average cosine similarity\n(avg_cos_sim) and average Jensen-Shannon distance (avg_jen_dis). The de-\ntailed deﬁnition of these similarity metrics is given in Appendix B. We ﬁrst\nextract 7 well-known signal features from each signal channel C, to form a\n7 ×C dimensional feature vector for each sequence. Theavg_cos_sim mea-\nsures the average cosine similarity among all real signals and synthetic signals\nof the same class. Values closer to 1 indicate high similarity between two feature\nvectors. The Jensen–Shannon distance is a method of measuring the similarity\nbetween two probability-like distributions. We consider each extracted feature\nto be a normally distributed array of values and compute the Jensen-Shannon\ndistance for corresponding features between real and synthetic feature vectors.\nThe avg_jen_dis is the average of all feature vector distances. A value closer\nto zero means a pair of signals have a small distance from each other and thus\n8 X. L. et al.\n0 5 10 15 20\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0 5 10 15 20\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0 5 10 15 20\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0 5 10 15 20\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSimulated Sinusoidal Waves\n0 10 20 30 40 50\n0.1\n0.2\n0.3\n0.4\n0.5\n0 10 20 30 40 50\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0 10 20 30 40 50\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0 10 20 30 40 50\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nptbdb normal\n0 10 20 30 40 50\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0 10 20 30 40 50\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 10 20 30 40 50\n0.18\n0.20\n0.22\n0.24\n0.26\n0.28\n0.30\n0 10 20 30 40 50\n0.25\n0.30\n0.35\n0.40\n0.45\nptbdb abnormal\n(a) Real signals\n0 5 10 15 20\n0.5\n0.6\n0.7\n0.8\n0.9\n0 5 10 15 20\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n0 5 10 15 20\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n0 5 10 15 20\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSynthetic Sinusoidal Waves\n0 10 20 30 40 50\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0 10 20 30 40 50\n0.05\n0.10\n0.15\n0.20\n0.25\n0 10 20 30 40 50\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n0.225\n0.250\n0 10 20 30 40 50\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nSynthetic ptbdb normal\n0 10 20 30 40 50\n0.175\n0.200\n0.225\n0.250\n0.275\n0.300\n0.325\n0.350\n0 10 20 30 40 50\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\n0.26\n0.28\n0 10 20 30 40 50\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0 10 20 30 40 50\n0.00\n0.05\n0.10\n0.15\n0.20\nSynthetic ptbdb abnormal (b) TTS-GAN Synthetic signals\nFig.3: A visual comparison of real data and their corresponding synthetic data\ngenerated by TTS-GAN.\nshare similar distributions. As it can be observed from the experimental results\nshown in Table 1, synthetic samples show a high average cosine similarity and\nlow Jensen–Shannon distance for diﬀerent signal classes. In addition, TTS-GAN\nwins against Time-GAN in 7 out of 10 cases.\n5 Conclusions\nIn this work, we build a transformer-based GAN model (TTS-GAN) that is\nable to generate multi-dimensional time-series data of various lengths. A visual\ncomparison of the raw signal patterns as well as data point distributions mapped\nin two dimensions show the similarity of the original data and the synthetic data.\nTwo similarity scores are also used to quantitatively further verify the ﬁdelity of\nthe synthetic data. Overall, the experimental results demonstrate the viability of\nTTS-GAN as a generator of realistic time-series, when trained on real samples.\nTTS-GAN 9\n7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\nx-pca\n6\n4\n2\n0\n2\n4\n6\ny_pca\nPCA plot\nOriginal\nSynthetic\n8\n 6\n 4\n 2\n 0 2 4 6 8\nx-pca\n6\n4\n2\n0\n2\n4\n6\n8\ny_pca\nPCA plot\nOriginal\nSynthetic\n1\n 0 1 2\nx-pca\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ny_pca\nPCA plot\nOriginal\nSynthetic\n2\n 1\n 0 1 2 3 4\nx-pca\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\ny_pca\nPCA plot\nOriginal\nSynthetic\n15\n 10\n 5\n 0 5 10 15\nx-tsne\n15\n10\n5\n0\n5\n10\n15\ny_tsne\nt-SNE plot\nOriginal\nSynthetic\n(a) Jumping\n15\n 10\n 5\n 0 5 10 15\nx-tsne\n15\n10\n5\n0\n5\n10\n15\ny_tsne\nt-SNE plot\nOriginal\nSynthetic (b) Running\n15\n 10\n 5\n 0 5 10 15\nx-tsne\n20\n15\n10\n5\n0\n5\n10\n15\n20\ny_tsne\nt-SNE plot\nOriginal\nSynthetic (c) Normal ECG\n10\n 5\n 0 5 10\nx-tsne\n20\n10\n0\n10\n20\ny_tsne\nt-SNE plot\nOriginal\nSynthetic (d) Abnormal ECG\nFig.4: The PCA and t-SNE test for real and synthetic data generated by TTS-\nGAN.\nModel NameSimilarity ScoreSinusoidal Jumping Running Normal Abnormal\nTTS-GAN avg_cos_sim 0.9936 0.9982 0.9988 0.9855 0.9768\navg_jen_dis 0.0980 0.0870 0.0497 0.1861 0.2911\nTime-GAN avg_cos_sim 0.9935 0.9980 0.9989 0.9878 0.9719\navg_jen_dis 0.1226 0.0924 0.0470 0.1883 0.3354\nTable 1: The similarity scores between real data and synthetic data of 5 diﬀerent\ndatasets. avg_cos_sim, the bigger the better.avg_jen_dis, the smaller the\nbetter. Bold texts identify better results.\nReferences\n1. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,\nA. Courville, and Y. Bengio, “Generative adversarial nets,”Advances in neural\ninformation processing systems, vol. 27, 2014. 2\n2. C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham, A. Acosta, A. Aitken,\nA. Tejani, J. Totz, Z. Wang,et al., “Photo-realistic single image super-resolution\nusing a generative adversarial network,” inProceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 4681–4690, 2017. 2\n3. K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, “Unsupervised\npixel-level domain adaptation with generative adversarial networks,” inProceedings\nof the IEEE conference on computer vision and pattern recognition, pp. 3722–3731,\n2017. 2\n4. H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. N. Metaxas,\n“Stackgan: Text to photo-realistic image synthesis with stacked generative adver-\nsarial networks,” inProceedings of the IEEE international conference on computer\nvision, pp. 5907–5915, 2017. 2, 3\n5. E. Brophy, Z. Wang, Q. She, and T. Ward, “Generative adversarial networks in\ntime series: A survey and taxonomy,”arXiv preprint arXiv:2107.11098, 2021. 2, 3\n6. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser,\nand I. Polosukhin, “Attention is all you need,” inAdvances in neural information\nprocessing systems, pp. 5998–6008, 2017. 2, 4\n7. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,et al., “An image is\n10 X. L. et al.\nworth 16x16 words: Transformers for image recognition at scale,”arXiv preprint\narXiv:2010.11929, 2020. 2, 4, 5\n8. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of\ndeep bidirectional transformers for language understanding,” arXiv preprint\narXiv:1810.04805, 2018. 2, 4\n9. K.Lu,A.Grover,P.Abbeel,andI.Mordatch,“Pretrainedtransformersasuniversal\ncomputation engines,”arXiv preprint arXiv:2103.05247, 2021. 2\n10. Y. Jiang, S. Chang, and Z. Wang, “Transgan: Two pure transformers can make\none strong gan, and that can scale up,” in Thirty-Fifth Conference on Neural\nInformation Processing Systems, 2021. 2, 4\n11. S. Diao, X. Shen, K. Shum, Y. Song, and T. Zhang, “Tilgan: Transformer-based\nimplicit latent gan for diverse and coherent text generation,” inFindings of the\nAssociation for Computational Linguistics: ACL-IJCNLP 2021, pp. 4844–4858,\n2021. 2\n12. C. Esteban, S. L. Hyland, and G. Rätsch, “Real-valued (medical) time series gen-\neration with recurrent conditional gans,”arXiv preprint arXiv:1706.02633, 2017.\n2\n13. J. Yoon, D. Jarrett, and M. Van der Schaar, “Time-series generative adversarial\nnetworks,” 2019. 2, 7\n14. H. Ni, L. Szpruch, M. Wiese, S. Liao, and B. Xiao, “Conditional sig-wasserstein\ngans for time series generation,”arXiv preprint arXiv:2006.05421, 2020. 2\n15. S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”Chemomet-\nrics and intelligent laboratory systems, vol. 2, no. 1-3, pp. 37–52, 1987. 2\n16. L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.,”Journal of\nmachine learning research, vol. 9, no. 11, 2008. 2\n17. L. J. Ratliﬀ, S. A. Burden, and S. S. Sastry, “Characterization and computation of\nlocal nash equilibria in continuous games,” in2013 51st Annual Allerton Confer-\nence on Communication, Control, and Computing (Allerton), pp. 917–924, IEEE,\n2013. 3\n18. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,\nA. Courville, and Y. Bengio, “Generative adversarial networks,”Communications\nof the ACM, vol. 63, no. 11, pp. 139–144, 2020. 3\n19. R. Huang, S. Zhang, T. Li, and R. He, “Beyond face rotation: Global and local\nperception gan for photorealistic and identity preserving frontal view synthesis,”\nin Proceedings of the IEEE international conference on computer vision, pp. 2439–\n2448, 2017. 3\n20. D. Micucci, M. Mobilio, and P. Napoletano, “Unimib shar: A dataset for human\nactivity recognition using acceleration data from smartphones,”Applied Sciences,\nvol. 7, no. 10, 2017. 6, 7\n21. R. Bousseljot, D. Kreiseler, and A. Schnabel, “Nutzung der ekg-signaldatenbank\ncardiodat der ptb über das internet,” 1995. 7\n22. A. L. Goldberger, L. A. Amaral, L. Glass, J. M. Hausdorﬀ, P. C. Ivanov, R. G.\nMark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley, “Physiobank,\nphysiotoolkit, and physionet: components of a new research resource for complex\nphysiologic signals,”circulation, vol. 101, no. 23, pp. e215–e220, 2000. 7\n23. X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. Paul Smolley, “Least squares\ngenerative adversarial networks,” inProceedings of the IEEE international confer-\nence on computer vision, pp. 2794–2802, 2017. 11\nTTS-GAN 11\nA Appendix 1: Training Details\nWe conduct all experiments on an Intel server with a 3.40GHz CPU, 377GB\nRAM memory and 2 Nvidia 1080 GPUs. For all datasets, the synthetic data are\ngenerated by a generator that takes random vectors of size(100,1) as inputs.\nThe transformer blocks in the generator and discriminator are both repeated\nthree times. We adopt a learning rate of1e−4 for the generator and3e−4\nfor the discriminator. We follow the setting of LSGAN [23] and use loss func-\ntion described in section 3.3 to update model parameters. An Adam optimizer\nwith β1 = 0.9 and β2 = 0.999, and a batch size of 32 for both generator and\ndiscriminator, are used for all experiments.\nB Appendix 2: Similarity Scores\nFeature extractionWe extract several meaningful features from each input\ndata sequence. They are the median, mean, standard deviation, variance, root\nmean square, maximum, and minimum values of each input sequence. Suppose\nwe compute m features from all channels of each sequence and get a feature\nvector with the formatf =<feature 1,feature2,...,feature m >.\nAverage Cosine SimilarityFor each pair of real signal feature vectorfa and\nsynthetic signal feature vectorfb, the vector has the sizem, we can compute its\ncosine similarity as:\ncos_simab = fa ·fb\n∥fa∥∥fb∥=\n∑m\ni=1 faifbi√∑m\ni=1 f2\nai\n√∑m\ni=1 f2\nbi\nThe average cosine similarity score is the average of each cosine similarity\nbetween pairs of feature vectors corresponding to real and synthetic signals of\nthe same class. The average cosine similarity is computed as follows, wheren\nthe total number of signals:\navg_cos_sim= 1\nn\nn∑\ni=1\ncos_simi\nAverage Jensen-Shannon distanceThe average jensen-shannon distance is\nthe average of jensen-shannon distance between each feature from real signals\nand synthetic signals. For each pair of real signal featurefi_real and synthetic\nsignal featurefi_syn, we can compute its jensen-shannon distance as:\njen_simi =\n√\nD(fi_real||m) +D(fi_syn||m)\n2\nWhere mis the pointwise mean offi_real and fi_syn and D is the Kullback-\nLeibler divergence. The average jensens-shannon distance is computed as:\navg_jen_dis=\nm∑\ni=1\njen_simi",
  "topic": "Discriminator",
  "concepts": [
    {
      "name": "Discriminator",
      "score": 0.8221462965011597
    },
    {
      "name": "Computer science",
      "score": 0.7758945226669312
    },
    {
      "name": "Transformer",
      "score": 0.6062751412391663
    },
    {
      "name": "Suite",
      "score": 0.5090193748474121
    },
    {
      "name": "Artificial intelligence",
      "score": 0.496368944644928
    },
    {
      "name": "Recurrent neural network",
      "score": 0.486873060464859
    },
    {
      "name": "Encoder",
      "score": 0.48022136092185974
    },
    {
      "name": "Deep learning",
      "score": 0.4723321497440338
    },
    {
      "name": "Time series",
      "score": 0.46406492590904236
    },
    {
      "name": "Generative adversarial network",
      "score": 0.44551563262939453
    },
    {
      "name": "Artificial neural network",
      "score": 0.44459328055381775
    },
    {
      "name": "Generative grammar",
      "score": 0.42714723944664
    },
    {
      "name": "Synthetic data",
      "score": 0.42244112491607666
    },
    {
      "name": "Network architecture",
      "score": 0.41079413890838623
    },
    {
      "name": "Data mining",
      "score": 0.4004720449447632
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38611066341400146
    },
    {
      "name": "Machine learning",
      "score": 0.35911697149276733
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Detector",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": []
}