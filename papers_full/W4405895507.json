{
    "title": "Beware the Intention Economy: Collection and Commodification of Intent via Large Language Models",
    "url": "https://openalex.org/W4405895507",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5115702491",
            "name": "Yaqub Chaudhary",
            "affiliations": [
                "Leverhulme Trust",
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A5020034322",
            "name": "Jonnie Penn",
            "affiliations": [
                "Leverhulme Trust",
                "University of Cambridge"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1507573241",
        "https://openalex.org/W1528949297",
        "https://openalex.org/W2076270454",
        "https://openalex.org/W2896736038",
        "https://openalex.org/W1967656652",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W7074571454",
        "https://openalex.org/W4390306503",
        "https://openalex.org/W6655065987",
        "https://openalex.org/W4390307196",
        "https://openalex.org/W4388963559",
        "https://openalex.org/W2957654274",
        "https://openalex.org/W4390528774",
        "https://openalex.org/W4319167005",
        "https://openalex.org/W3176023283",
        "https://openalex.org/W4313564591",
        "https://openalex.org/W2153803020",
        "https://openalex.org/W4288366030",
        "https://openalex.org/W4387800384",
        "https://openalex.org/W4388890599",
        "https://openalex.org/W3202773593",
        "https://openalex.org/W2768262152",
        "https://openalex.org/W3213151880",
        "https://openalex.org/W6998381722",
        "https://openalex.org/W4393118975",
        "https://openalex.org/W6682470663",
        "https://openalex.org/W4387074688",
        "https://openalex.org/W4387596201",
        "https://openalex.org/W3133752603",
        "https://openalex.org/W4390041215",
        "https://openalex.org/W4309663019",
        "https://openalex.org/W4381612673",
        "https://openalex.org/W4392153984",
        "https://openalex.org/W4385835571",
        "https://openalex.org/W4387306661",
        "https://openalex.org/W2809286498"
    ],
    "abstract": null,
    "full_text": "Harvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention\nEconomy: Collection and\nCommodi\u0000cation of Intent\nvia Large Language Models\nYaqub Chaudhary1Jonnie Penn1\n1Leverhulme Centre for the Future of Intelligence, University of Cambridge, Cambridge, England,\nUnited Kingdom\nThe MIT Press\nDOI: https://doi.org/10.1162/99608f92.21e6bbaa\nLicense: Creative Commons Attribution 4.0 International License (CC-BY 4.0)\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n2\nA B S T R A C T \nThe rapid proliferation of large language models (LLMs) invites the possibility of a new marketplace for \nbehavioral and psychological data that signals intent. This brief article introduces some initial features of that \nemerging marketplace. We survey recent efforts by tech executives to position the capture, manipulation, and \ncommodification of human intentionality as a lucrative parallel to—and viable extension of—the now-\ndominant attention economy, which has bent consumer, civic, and media norms around users’ finite attention \nspans since the 1990s. We call this follow-on the intention economy. We characterize it in two ways. First, as a \ncompetition, initially, between established tech players armed with the infrastructural and data capacities \nneeded to vie for first-mover advantage on a new frontier of persuasive technologies. Second, as a \ncommodification of hitherto unreachable levels of explicit and implicit data that signal intent, namely those \nsignals borne of combining (a) hyper-personalized manipulation via LLM-based sycophancy, ingratiation, and \nemotional infiltration and (b) increasingly detailed categorization of online activity elicited through natural \nlanguage.\nThis new dimension of automated persuasion draws on the unique capabilities of LLMs and generative AI \nmore broadly, which intervene not only on what users want, but also, to cite Williams, “what they want to \nwant” (Williams, 2018, p. 122). We demonstrate through a close reading of recent technical and critical \nliterature (including unpublished papers from ArXiv) that such tools are already being explored to elicit, infer, \ncollect, record, understand, forecast, and ultimately manipulate, modulate, and commodify human plans and \npurposes, both mundane (e.g., selecting a hotel) and profound (e.g., selecting a political candidate).\nKeywords: intention economy, attention economy, large language models, generative AI, persuasive \ntechnology\nMedia Summary\nThe rapid adoption of AI tools in the 2020s opens new possibilities for online behavior, not all of them good. \nTo show what lies ahead, this article introduces the concept of the intention economy. The intention economy, \nas it appears in the emerging scholarship and corporate announcements we draw together here, builds on the \nattention economy, which for decades has treated your attention as the currency of the internet. In the past, you \nshared your attention with a platform to access products like Instagram and Facebook. In the future, we argue, \nthe intention economy will treat your motivations as currency. This is an unsettling prospect, if left \nunmonitored. Already today, AI agents find subtle ways to manipulate and influence your motivations, \nincluding by writing how you write (to seem familiar), or anticipating what you are likely to say (given what \nothers like you would say). Whichever organization manages this pipeline best stands to make a fortune from \nselling your motivations—be it for a hotel, a car rental, or a political candidate—to the highest bidder. While \nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n3\nprior accounts of an intention economy have positioned this prospect as liberatory for consumers, we argue that \nits arrival will test democratic norms by subjecting users to clandestine modes of subverting, redirecting, and \nintervening on commodified signals of intent.\n1. Introduction\nOn November 20, 2023, amid sudden actions of the board of OpenAI to dismiss its CEO, Sam Altman, the \ntroubled executive posted a short message online stating, confidently, “the mission continues” (Altman, 2023). \nIn what follows, we aim to clarify what appears to us to be a central aspect of this mission, namely: to \nsignificantly expand the depth and scope of the capture of human-generated data online and dominate what we \nrefer to as the intention economy, which we define here as a digital marketplace for commodified signals of \n‘intent.’\nThe structure of this article is as follows. First, we briefly consider how philosophers have defined ‘intention’ \nrelative to that term’s more colloquial use(s) in the tech industry (Section 2). Having worked to introduce one \nroute by which the culture around large language models (LLMs) could naturalize pseudoscientific claims \nabout human behavior, we turn our attention to a series of investments and claims made by a pool of tech \ncompanies vying to establish LLMs at the core of their proprietary infrastructure (Section 3). We conclude by \nquestioning the dubious ways in which LLM developers project false intentionality onto their users (Section 4). \nWe caution that the social implications of an intention economy merit sustained critique given the risks of \npersonalized persuasion-at-scale (Section 5). While ‘intention’ is not the only facet of human psychology \nimpacted by a transition to natural language interfaces, its relation to persuasive technologies gives it \npreference as the main label for this emerging form of digital marketplace.\n2. ‘Intention’ in LLM Development\nThe word ‘intention’ and its cognates have a wide range of conceptualizations. Western analytic philosophers \nrelate intent to purposeful action and individual reasoning (Anscombe, 2000), consciousness (Dennett, \n1989/1987), and mental representations of the future (Searle, 1983). They place less emphasis on the role of \nspontaneity, incapacity, or irrationality in intentional action. Herein, we sidestep debate over any true nature of \nintention to focus instead on the assumptions undergirding its recurring presence in corporate strategies and \nresearch on the capabilities and applications of LLM-based natural language interfaces. In other words, the \naspirations for ‘intention’ that we profile herein appear to be closer in character to the practicalities of computer \nscience than they are to the aims and empirics of the biological or medical sciences.\nSince the set of actors we consider here have tended not to make the scientific rationale for their postulations \nabout human behavior explicit, we focus instead on what we take to be a core assumption of their disparate \ninterventions: that intention, whatever it is at its core, is amenable to computation and can be operationalized as \nsuch. Our approach builds on research by Stark and Hoey (2021), who argue, in connection with the use of \nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n4\nemotion as an evaluative signal in computation design, that scholars need not accept the validity of technical \nand scientific conceptualizations of psychological phenomena such as emotion (or in our case, intention) when \nevaluating the ethics of an AI system (p. 786). In her 2014 paper, “A Database of Intention?” Kylie Jarrett \nargues for a similar distance to be set between scientific and colloquial references to intentionality in the design \nof digital services.\nThe intentions that constitute Google’s database must ( ... ) be understood only as ascriptions of a limited \nconceptualization of intention and not as meaningful manifestations of the embodied affective logics of \nthose intentions. The keywords and clickstream data it collects are reductive products of the full richness \nof our motivating energies. (Jarrett, 2014, p. 22)\nIt is this reductive quality that has, in diverse ways, prompted interest in intention and digitalism in the past. As \ndigital surveillance infrastructure became increasingly global at the turn of the twenty-first century, the scale of \nits real and imagined impact on human intent sparked both affirmation (e.g., given the prospect of highly \nefficient personalized marketplaces) (Searls, 2012) and concern (e.g., given the risk of monopoly powers over \nsuch markets, as epitomized by Google Search) (Battelle, 2003, 2011; Batty, 2013). Scholarship from the \n2000s and 2010s identifies Google as presiding over a “database of intentions” (Battelle, 2003, 2011; Batty, \n2013; Jarrett, 2014) based on its privileged access to user behaviors, which provides closer access to the \n‘bottom of the funnel’ as described in the digital ad literature, or the “zero moment of truth” as coined by Jim \nLecinski (2014), formerly a Google VP, to refer to the priority of search for commercial intent. While we \ncannot do justice to these historical entanglements here (see Ali et al., 2023; Penn, 2023), we can begin to \nprofile how mass LLM infrastructure could bear, in complex ways, on human intentionality.\nWith these reservations in mind, we argue that two assumptions about ‘intention’ underlie facets of recent LLM \nresearch and development in ways that merit scrutiny as that subfield and industry matures. The first \nassumption is that an enclosure of some type will act upon an individuals’ choices. The cognitive scientist \nMargaret Boden (1973) argues that intentionality is, at root, a “highly structured phenomenon arising within a \nhighly structured system” (emphasis added; p. 23). To give an example, the choice architecture of a user’s \ndigital environment shapes their sense of agency, possibility, and with it, their intent (Joler, 2020). One cannot \nlike or swipe on that which cannot be liked or swiped upon, to give a trivial example. LLM research and \ndevelopment takes as a given that this highly structured system would be digital. A second and related \nassumption about intentionality that we identify in LLM research and development is temporal in nature. In a \nsense, the intention economy is the attention economy plotted in time; it seeks to profile the arc of users’ \nattention—how it changes, calcifies, and connects to archetypal patterns of behavior—across various time \nscales. While some intentions are fleeting, others persist, making their discretization lucrative to advertisers. \nThis temporal characteristic, and the enclosures that shape it, are our primary focus.\nIt is beyond the scope of this article to trace the intellectual history that informs these assumptions. In 1999, \nphilosopher of action Michael Bratman (1999) proposed a “planning theory of intention” wherein human intent \nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n5\nhas “elements of stable, partial plans of action concerning present and future conduct” (p. 1). In a 2024 preprint \nentitled “Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies,” a team of \nMicrosoft researchers define ‘intent’ using terms that an LLM can operationalize, as “a user’s purpose for \nconversing with the AI agent” (Shah et al., 2024). They go on to populate this formulation with a list of \ncategories meant to structure user “intents” such as “information retrieval,” “problem solving,” “learning,” \n“content creation,” and “leisure.” Sadly, as is a norm in this emerging body of literature, the provenance of this \ntheory of intention remains oblique. Their claims draw, in part, from information retrieval literature, including \nwork from the early 2000s on taxonomizing web queries. For our present purposes, this understanding of \nintention, with its relation to actions, seems to align closely with Bratman’s planning theory of intention, but \nfurther study would be needed to bear out this genealogy.\nAt time of print, the intention economy is more aspiration than reality. As we will now survey, however, \ninvestments and rhetoric from leading tech firms have positioned generative AI as a technological milestone \nupon which new (or old) parties might supplant market leaders like Google. Drawing on findings from the \nrapidly developing LLM research literature, and statements made by key figures at Microsoft, OpenAI, Apple, \nand NVIDIA, we highlight a convergence of industrial ambitions across leading technology companies. Our \nfocus is on the proposed use of LLMs to anticipate and steer users based on intentional, behavioral, and \npsychological data, from voluntary engagements with such systems, to the purposeful integration of LLMs as \nthe first points of contact between humans and digital information systems.\nTo conclude this overview, a concrete example helps to illustrate how the intention economy, as a digital \nmarketplace for commodified signals of ‘intent,’ would differ from our present-day attention economy. Today, \nadvertisers can purchase access to users’ attention in the present (e.g., via real-time-bidding [RTB] networks \nlike Google AdSense) or in the future (e.g., buying next month’s ad space on, say, a billboard or subway line). \nLLMs diversify these market forms by allowing advertisers to bid for access both in real time (e.g., ‘Have you \nthought about seeing Spiderman tonight?’) and against possible futures (e.g., ‘You mentioned feeling \noverworked, shall I book you that movie ticket we’d talked about?’). If you are reading these examples online, \nimagine that each was dynamically generated to match your personal behavioral traces, psychological profile, \nand contextual indicators. In an intention economy, an LLM could, at low cost, leverage a user’s cadence, \npolitics, vocabulary, age, gender, preferences for sycophancy, and so on, in concert with brokered bids, to \nmaximize the likelihood of achieving a given aim (e.g., to sell a film ticket). Zuboff (2019) identifies this type \nof personal AI ‘assistant’ as the equivalent of a “market avatar” that steers conversation in the service of \nplatforms, advertisers, businesses, and other third parties. While LLMs may not represent the final word in the \nrealization of this vision, their associated infrastructural costs and conspicuous re-designation as ‘foundation’ \nmodels, reminiscent of the lowest load-bearing part of a building (Bommasani et al., 2022; Meredith, 2021; \nNarayanan & Kapoor, 2024) merit skepticism. So, too, does the concomitant shift from direct information \nretrieval to mediated–generative information retrieval, as we will now explore.\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n6\n3. The Weight of Things to Come: Eliciting, Inferring, and \nUnderstanding Signals of Intent\nOur point of departure is the first OpenAI developer conference on November 6, 2023, a high-profile spectacle \nthat prefigured the sequence of events that began with Altman’s firing, rehiring, and eventual reorganization of \nthe company’s board. The updates to OpenAI’s services and platform announced at the conference were \npredominantly for the benefit of developers, and included a wider context window, function calling, JSON \nmode, reproducible outputs, multimodal capabilities, and text-to-speech functionality. These updates were \nmade to allow developers to release customized versions of the ChatGPT platform, each iteration of which they \ncalled a GPT. OpenAI announced revenue sharing for the most popular custom GPTs, presumably to populate \ntheir platform approach to LLMs by creating competition among developers, as well as anyone with basic \ncomputer literacy, to create popular customizations.\nAt the conference, Microsoft CEO Satya Nadella emphasized Microsoft’s attention to building computing \ninfrastructure, and how the rise of LLMs had prompted them to rethink “The system, all the way from thinking \nfrom power to the DC to the rack, to the accelerators, to the network” (OpenAI, 2023a). Nadella expressed \nenthusiasm about his company’s partnership with OpenAI, knowing that its ChatGPT and GPT services \nmaximized workloads, and thus revenues, from Microsoft’s servers. In 2022, Microsoft’s Azure cloud platform \ncomprised over one-third of the company’s total revenue (Franek, 2022; Microsoft, 2022). Nadella emphasized \nthat the computational workloads OpenAI creates were unprecedented in his three decades at Microsoft, to the \nextent that “The shape of Azure is drastically changed and is changing rapidly in support of these models” \n(OpenAI, 2023a). Research analysts Dylan Patel and Myron Xie (2023) highlighted that, “Microsoft is \ncurrently conducting the largest infrastructure buildout that humanity has ever seen,” based on projected annual \nexpenditure on infrastructure of more than $50 billion from 2024 onward. In other words, with LLMs \nMicrosoft is not just positioning itself to be another cloud provider or platform, but to be the cloud platform, \nakin to a utility, as the name Azure, for ‘bright blue in color like a cloudless sky,’ suggests; a homage to the \nvery backdrop of all clouds.\nThis is an audacious goal. In December 2021, two years earlier, employees at Microsoft wrote publicly, in \ncautious terms, about the company’s unprecedented level of commitment to OpenAI. About Microsoft’s initial \none-billion-dollar outlay to the start-up, much of it afforded as access-to-compute, they wrote, “We infer that \nthis must be one of the largest capital investments ever exclusively directed by such a small group. (The ratio \nof Soviet investment in 1975 to number of employees of the state planning agency, Gosplan, for example, was \nroughly the same as that associated with the OpenAI investment.)” (Siddarth et al., 2021, p. 3). Given the scale \nof this initial commitment, and its expansion in the years since, we believe it is pertinent to peer deeper into the \nunderlying strategy for the role of LLMs at Microsoft and in the future of computing platforms and services \nmore generally.\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n7\nAlthough Microsoft, NVIDIA, and OpenAI’s exact aims are difficult to discern empirically, many of their likely\n objectives—or intentions—can be inferred through disparate corporate announcements and emerging \ndirections for LLM research.1 Here we discuss what we believe to be a central ambition behind their sizeable \ninvestments, an aim that otherwise remains understated and cloaked by other sensational discourses on the \nfuture of LLMs. To begin with the research first, there exists an emerging body of literature on the use of \nLLMs for inferring human preferences, intentions, motivations, and other psychological and cognitive \nattributes, much of which we will survey in a moment (Derner et al., 2023; Goodson & Lu, 2023; Huang et al., \n2023; Li et al., 2023; Tan & Jiang, 2023; Wu et al., 2023). Another connected ambition, which follows from \nthe acquisition of large stores of intentional data, is to monopolize and capitalize on this new source of data for \ntraining the next generation of increasingly agentic AI systems. It is, however, beyond the scope of this article \nto explore that strategic aim.\nPrior to reviewing the research, let us consider a series of recent remarks from relevant corporate executives \nand media about their organizations’ aims for LLMs. On the basis of their unique ability to bridge algorithmic \ntechniques to economic outcomes, transformer models have been described as “engines of profit” (Luitse & \nDenkena, 2021, p. 10). To understand the idiosyncratic design of these engines in an increasingly competitive \nspace, we speculate that OpenAI chose to launch and market custom GPTs as, in effect, a dragnet for \nbehavioral and intentional data across numerous domains and application contexts. Consider the following \nfrom an OpenAI blog post on November 9, 2023, after the developer conference:\nWe’re interested in large-scale datasets that reflect human society and that are not already easily \naccessible online to the public today. We can work with any modality, including text, images, audio, or \nvideo. We’re particularly looking for data that expresses human intention (e.g. long-form writing or \nconversations rather than disconnected snippets), across any language, topic, and format. (OpenAI, \n2023b)\nThe key here, which is suggestive of the forthcoming stage of Altman’s mission, is that they are “looking for \ndata that expresses human intention.” This interest in data on human intentions is further elucidated in \nstatements from OpenAI’s partners in the developer conference held the following week. Miqdad Jaffer, as \nDirector of Product at Shopify at the time, for instance, describes his view of a continuum from understanding \nuser intent, to predicting user intent, and finally predicting user action. He states:\nI think we’re on a continuum right now. We’re starting in the understand [sic] the user’s intent, then it’s \npredict the user’s intent, then it’s predict the user’s action [sic]. I think that’s the continuum we’re on \nright now. Chatbots come in to explicitly get the user’s intent. (OpenAI, 2023c)\nThe purpose of this massive expansion of domain and context specific GPTs is thus to create endless channels \nto, per Jaffer, “explicitly get the user’s intent.” Interestingly, as of March 2024, Jaffer joined OpenAI as a \nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n8\n“Product Leader” (Stone, 2024). Elsewhere, this way of using LLMs has been made explicit by Jensen Huang, \nCEO of NVIDIA. Huang states:\nThe canonical use case of the future is a large language model on the front end of just about everything. \nEvery single application, every single database, whenever you interact with a computer, you will likely \nbe first engaging a large language model. That large language model will figure out what is your \nintention, what is your desire, what are you trying to do, given the context, and present the information to \nyou in the best possible way. It will do the smart query, maybe a smart search, augment that query in \nsearch with your question, with your prompt, and generate whatever information necessary. (NVIDIA, \n2023)\nNVIDIA and Microsoft are not alone in seeking to re-architect modern computing infrastructure to position \nLLMs and transformer-based technologies as the first point of contact between humans and information \nsystems. Meta, owner of Facebook, has released research on how to extract behavioral data that signals intent \nfrom visual images. One paper introduces “Intentonomy,” a data set for human intent understanding (Jia et al., \n2021). In this work, the authors seek to create a data set on human intent by manual annotation of visual scenes \nwith 28 intent categories, such as “security and belonging,” “power,” “health,” “family,” “ambition and \nability,” and “financial and occupational success” (Jia et al., 2021, p. 12982). This analysis builds on analogous \nresearch on intent drawn from visual images on Instagram (Kruk et al., 2019).\nThe advent of LLMs allows for the automation of such extractions, enabling the inference and relatively low-\ncost categorization of human intent and motivation at scale. Applied research on LLMs is rapidly emerging to \nshow expanded capabilities for eliciting human preferences via LLMs. In one example, Li et al. address the \nchallenges of writing prompts to guide LLMs to perform highly specific tasks by showing that LLMs \nthemselves can be used to guide the process of their own alignment to “fuzzy” human preferences by eliciting \nhuman preferences via free-form questions (Li et al., 2023). Another example, via Microsoft, explores the use \nof LLMs to generate intent taxonomies that capture “a user’s purpose for conversing with the AI agent” (Shah \net al., 2024). Methods for intent extraction using LLMs have already been incorporated in libraries of key \nMicrosoft products, such as the Teams API library, which includes a “planning engine” that maps user intent to \nprespecified actions, as well as a “predictive engine” for learning these mappings and acting according to an \nestimated confidence level between user intents and actions (Maillot, 2024).\nNeedless to say, inferring such psychological attributes rests on unestablished scientific grounds, and often on \nunrecognized human labor (Gray & Suri, 2019). Much of this research has yet to pass peer review. \nNevertheless, early efforts have yielded incremental progress in the development of systems for capturing \nintent, with the notable example of Meta’s claim to have achieved human-level play in the game Diplomacy \nusing their AI agent, CICERO (Bakhtin et al., 2022). Success in this game is dependent on inferring and \npredicting the intent of opponents, as well as strategic play, and persuasive dialogue to advance one’s position \nin the game.\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n9\nIt remains to be seen how exactly these abilities will be commodified or abused. In the context of digital ad \nmarkets, LLMs and generative AI add the possibility of incorporating automated content generation into \nexisting RTB networks and programmatic approaches to target content. That is, advertisers can attempt to more \nprecisely tailor content to user profiles using generative AI and are no longer constrained by a human-curated \ninventory of advertising content. CICERO, while a proof-of-concept, is significant because it demonstrates the \npossibility of the system itself optimizing its strategy to achieve a prespecified goal or a particular outcome \naround the user’s intent in a variety of scenarios.\nGiven Meta’s existing advertising infrastructure, it is reasonable to expect that they would leverage RTB \nnetworks to auction off user’s intent to book a restaurant, flight, or hotel, and so on. While RTB, political \npolling, market research, and social network analysis have long allowed for interested parties to forecast and \nbid on citizen, consumer, and user behaviors, LLMs distill these practices into a highly quantified, dynamic, \nand personalized format that is simultaneously intimate (e.g., an AI assistant), low cost (e.g., compared to \nhuman interlocutors), and ubiquitous (e.g., widespread conversational brand agents). In recent research, Johnny \nRyan and Wolfie Christl have found that that RTB networks profile upwards of five billion people, including \nchildren. Troublingly, for democratic norms, RTB is also widely used for espionage and crime, such that \n“foreign states and non-state actors can use RTB to spy on target individuals’ financial problems, mental state, \nand compromising intimate secrets” (Ryan & Christl, 2023, p. 4).\nNew appointments to the OpenAI board show evidence of a similar commitment to Meta’s, vis-à-vis the mass \ncollection of human intention data. Bret Taylor, former co-CEO of Salesforce Inc., was also CEO of \nFriendFeed, which was acquired by Facebook in 2009, whereafter he became the platform’s CTO. Crucially, \nthis acquisition led to Facebook embedding the “Like” button in their platform, which portended the conditions \nof possibility for the psychographic targeting methods that were brought to public attention in the political \ncontext by the Cambridge Analytica scandal (Benkler et al., 2018, p. 275; Kosinski et al., 2013). According to \nMatz et al. (2017, p. 4), “The effectiveness of large-scale psychological persuasion in the digital environment \nheavily depends on the accuracy of predicting psychological profiles,” hence an actor wishing to engage in \ndigital mass persuasion would be enabled by having the capacity to “continuously calibrate and update” their \nalgorithms over time.\nLLMs, as conversational assistants in frequent dialogue with users, are thus well-positioned to fulfill the \npurpose of continuous calibration to streams of incoming user-generated data, from dialogue history to action \nsequences. LLMs add to the fidelity, depth, and variety of data that may be collected based on semantically \nrich interactions with humans. Additionally, their generative capabilities provide control over the \npersonalization of content; veiled, as it often is, by LLM’s anthropomorphic qualities. The potential for LLMs \nto be used for manipulating individuals and groups thus far surpasses the simple methods based on Facebook \nLikes that caused concern during the Cambridge Analytica scandal.\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n10\nEfforts are already underway to realize such purposes or draw attention to new ways of siphoning private data. \nFor example, Staab et al. (2023) argue that significant detail about user intent and preferences may be elicited \nvia text using LLMs. They show, for instance, that LLMs can infer personal information through seemingly \nbenign conversational exchanges and can even ‘steer conversations’ in such a way as to provoke responses \nfrom which to infer private information (Staab et al., 2023). Another work, by Zhang et al. (2023), casts LLMs \nas fulfilling the promise of revolutionizing research in recommender systems with the potential to be used as “a \nconfigurable simulation platform for recommender systems” that “faithfully captures user intent and encodes \nhuman cognitive mechanisms.” Their method involves simulating one thousand LLM agents, which includes a \nmemory module to align the agents to the “past viewing behaviours, system interactions, and emotional \nmemories” of real humans represented in the MovieLens-1M data set. Other work by Qi Liu et al. has \nattempted to extract user interests to train a transformer architecture to predict click-through rates from \nnonlinguistic data in the form of “lifelong behavior” sequences (Liu et al., 2023).\nA central danger of these forecasting techniques is, of course, that they would enable unprecedented modes of \nhyper-personalized manipulation, should they meet the researcher’s claims in real-world scenarios. The \nresearch team behind CICERO, for instance, cautions against, “The potential danger for conversational AI \nagents to manipulate: an agent may learn to nudge its conversational partner to achieve a particular objective” \n(Bakhtin et al., 2022, p. A.3, 3). In an unusual case from October 2023, a 21-year-old student in the United \nKingdom was sentenced to nine years in prison for plotting—in conversation with a sympathetic chatbot—to \nkill Queen Elizabeth (Singleton et al., 2023). Our aim is not to sensationalize the degree to which an LLM can \ninfluence its users’ intent (not all users are would be-assassins) but rather to point out that while improved \ncapabilities for intention prediction are necessary to increase the utility of LLMs for users, those same \ncapabilities provide the basis for third parties to intervene upon a users’ intent, including to do harm.\nRegrettably, increased privacy protections may not alleviate such harms. Generative AI creates a workaround \nto the need for third-party cookies by treating the content itself as a proxy through which to infer private \nattributes. In 2024, OpenAI formalized a large number of data partnerships in service, we suspect, of this \nlucrative rearrangement. Central to a “strategic partnership” with Dotdash Meredith (DDM), an American print \nand digital media publisher, is the combination of OpenAI’s models with DDM’s intent targeting tool for \nadvertising, known as D/Cipher. This tool purports to make “ad targeting more granular, more nuanced, and \nmore effective in engaging customers” (Dotdash Meredith, 2024). D/Cipher, launched in May 2023, was \noriginally developed to anticipate a future in which third-party browser cookies are deprecated and advertisers \nare no longer able to track behavior across websites (Dotdash Meredith, 2023).\nAnother noteworthy example of how intention is being used to financialize digital activities is Apple’s new \n“App Intents” developer framework. This framework includes protocols to map intent discovery, relevancy, \nand prediction across apps to “predict actions someone might take in the future” and to procure the relevant \ninterfaces needed “to suggest the app intent to someone in the future using predictions you [the developer] \nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n11\nprovide” (Apple, n.d.). Apple also announced plans to integrate ChatGPT into the system level of its devices as \na third-party service that will complement its own “Apple Intelligence” AI services. Both Apple Intelligence \nand the provisions for third-party AI integrations exemplify the way LLMs and conversational interfaces \npromote a shift toward modes of interaction that require users to declare their intentions through natural \nlanguage. Apple’s move conscripts billions of existing iPhone users into a paradigm for information retrieval \npremised on the traffic of, in their words, intent. In principle, the integration of tools like D/Cipher into the \nmechanics of generative interfaces would advance this goal, too.\n4. Persuasion and Machinic Projection of Intent\nAs we have discussed, LLMs may be used to generate content that is aligned to behavioral and psychological \nprofiles to influence user attitudes, preferences, and behaviors. Meta’s AI agent, CICERO, designed to play the \ngame Diplomacy, provides a proof-of-concept for the capability of inferring human intentions and the \npossibility of “building agents that use language to communicate intentionally with humans in interactive \nenvironments” (Bakhtin et al., 2022, p. 1). For the researchers behind CICERO, a “major long-term goal for \nthe field of artificial intelligence (AI) is to build agents that can plan, coordinate, and negotiate with humans in \nnatural language,” including persuasive communication of the agent’s proposals in such negotiations (Bakhtin \net al., 2022, p. 1). Each of these potentialities is insinuated in the research article describing the implementation \nof CICERO. Crucially, for our present purposes, the authors highlight that the agent “models how the other \nplayers are likely to act on the basis of the game state and their conversations,” and they argue that it \n“successfully changed the other player’s mind by proposing mutually beneficial moves” (Bakhtin et al., 2022, \np. 7). What the CICERO example indicates is corporations’ exploration of the power of so-called strategic \nreasoning using generative AI and LLMs to heighten the fidelity with which they can dynamically calibrate \nand ascribe ‘intents’ to users. Incidentally, Noam Brown, a key figure behind CICERO, was recently recruited \nfrom Meta to OpenAI “to help integrate more planning into its popular language-model-based tools” (Newport, \n2024).\nAs further evidence of the possibility of using LLMs to project intentions upon a user, language models have \nrecently been shown to transmit false information and biases to humans (Kidd & Birhane, 2023). In the case of \ntext, various authors have begun to highlight the persuasive capabilities of LLMs (Breum et al., 2023; Matz et \nal., 2024; Salvi et al., 2024). The persuasive characteristics of LLM-generated text have been shown to be \npresent even without being configured for persuasive messaging. For example, Jakesch et al. (2023) identify \nthe phenomenon of latent persuasion in LLM-powered predictive completion. In this case, the authors suggest \nthat LLM-based predictive suggestions may interrupt individual thought processes of users, who may \nsubsequently change their views during text composition. While this does not necessitate the distortion of a \nuser’s intent (e.g., the autocomplete might be right!), the redirection of attention through dialogue provides one \nsimple example of how intention could be maliciously altered. Other examples recently identified include the \nimage-based projection of bias (Vicente & Matute, 2023) and the ability to influence human perception via \nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n12\nadversarial image manipulation (Veerabadran et al., 2023). In an anonymous exchange on Reddit in 2023, users \nconfigured generative models to create unique QR codes stylized against diverse backgrounds using Stable \nDiffusion and ControlNet (nhciao, 2023). In principle, this method could be generalized to craft synthetic \nimages that conform to the outline of any underlying template, paving the way for the scaled production of \nimages containing subliminal or suggestive messages.\nThe diverse persuasive capabilities of LLMs outlined above can be deployed in various ways, a handful of \nwhich we have surveyed here to convey the possibilities of intervening on—and commodifying—a higher \norder of user intentionality than that seen in the attention economy. The use of LLMs to match content to users’ \npsychological profiles, which we consider as an early example of automated ingratiation, has been shown to be \neffective in altering attitudes, intentions, and behaviors via appeal to their motivational states (Joyal-Desmarais \net al., 2022). In more recent work, Matz et al. (2024) explore the importance of matching effects between the \ncontent of a message and the psychological profile of the recipient, based on the capabilities of LLMs, which \nthey argue “close the loop” in automated personalized persuasion. They provide evidence for the viability of \nusing LLMs to generate “personalized messages that influence people’s attitudes and intentions,” which we \ntake as a form of sycophancy. Matz et al. (2024) argue that the potential for personalized persuasion with \nLLMs is “unprecedented” given the broad number of advertisements people see and the generative capabilities \nof LLMs for personalized content. They suggest, further, that such techniques could include personalized \ntextual content, as well as personalized visual and audio stimuli, each deployed in real time and dynamically \nadjusted as recipients interact with new content. In this connection, we note that NVIDIA has already \nannounced a partnership with WPP, the world’s largest advertising company, via a video demonstration that \ncombines NVIDIA’s generative AI and Omniverse technologies to craft exactly this type of real-time, \ndynamically generated video advertising (NVIDIA, 2023).\nCompetition in this area will be fierce and well-funded. An industry known for its hyperbolic rhetoric will have \nreason to test the public’s appetite for the sort of tooling we have outlined herein, even if they do not admit it \nopenly. At OpenAI’s November developer conference, Sam Altman publicly promised not to collect data from \nGPT interactions (OpenAI, 2023a). We speculate, however, that the aforementioned GPT data partnerships that \nOpenAI seeks to broker in pursuit of “data that expresses human intention” (OpenAI, 2023b) are, as third-party \nsources, suitable for exactly this type of use. Use of third-party data would allow OpenAI reputational and \nliability protection from claims that it drew from its own users’ data. Via data partnerships, it is no longer \n‘your’ data, but a third party’s that the company has repatriated and thus made available for its own purposes. \nWhatever the case, in charting its path through the early productization of LLMs, OpenAI and Microsoft will \nbe up against stiff competition from others to command what appear to be the beginnings of a lucrative yet \ntroubling new marketplace for digital signals of intent.\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n13\n5. Conclusion\nThe possibility for harm made feasible by a large-scale, multiparty intention economy merits sustained \nscholarly, civic, and regulatory scrutiny. In whatever way these data partnerships turn out in practice, the \nambition of making conversational interfaces and generative AI systems unavoidable mediators of human–\ncomputer interaction signals a turn from the attention economy, whereby access to the limited resource of \nhuman attention is traded through advertising exchanges, to the intention economy, whereby commercial and \npolitical actors bid on signals that forecast human intent. This transition would empower diverse actors to \nintervene in new ways on shaping human actions. This ambition must be considered in light of the likely \nimpact such a marketplace would have on other human aspirations, including free and fair elections, a free \npress, fair market competition, and other aspects of democratic life.\nDisclosure Statement\nYaqub Chaudhary and Jonnie Penn have no financial or non-financial disclosures to share for this article.\nReferences\nAli, S. M., Dick, S., Dillon, S., Jones, M. L., Penn, J., & Staley, R. (2023). Histories of artificial intelligence: A \ngenealogy of power. BJHS Themes, 8, 1–18. https://doi.org/10.1017/bjt.2023.15\nAltman, S. [@sama]. (2023, November 20). The mission continues [Post]. X. \nhttps://x.com/sama/status/1726510261509779876 \nAnscombe, G. E. M. (2000). Intention (2nd ed.). Harvard University Press.\nApple. (n.d.). App Intents. Apple Developer Documentation. Retrieved August 11, 2024, from \nhttps://developer.apple.com/documentation/appintents\nBakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., Jacob, A. P., \nKomeili, M., Konath, K., Kwon, M., Lerer, A., Lewis, M., Miller, A. H., Mitts, S., Renduchintala, A., Roller, \nS., … Zijlstra, M. (2022). Human-level play in the game of diplomacy by combining language models with \nstrategic reasoning. Science, 378(6624), 1067–1074. https://doi.org/10.1126/science.ade9097\nBattelle, J. (2003, November 13). The Database of Intentions. John Battelle’s Search Blog. \nhttps://battellemedia.com/archives/2003/11/the_database_of_intentions\nBattelle, J. (2011). The search: How Google and its rivals rewrote the rules of business and transformed our \nculture. Hachette UK.\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n14\nBatty, M. (2013). The Database of Intentions. Environment and Planning B: Planning and Design, 40(3), 381–\n383. https://doi.org/10.1068/b4003ed\nBenkler, Y., Faris, R., & Roberts, H. (2018). Mammon’s algorithm: Marketing, manipulation, and clickbait on \nFacebook. In Y. Benkler, R. Faris, & H. Roberts (Eds.), Network propaganda: Manipulation, disinformation, \nand radicalization in American politics (pp. 269–288). Oxford University Press. \nhttps://doi.org/10.1093/oso/9780190923624.003.0009\nBoden, M. A. (1973). The structure of intentions. Journal for the Theory of Social Behaviour, 3(1), 23–46. \nhttps://doi.org/10.1111/j.1468-5914.1973.tb00314.x\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., \nBosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, \nK., Davis, J. Q., Demszky, D., … Liang, P. (2022). On the opportunities and risks of foundation models. ArXiv. \nhttps://doi.org/10.48550/arXiv.2108.07258 \nBratman, M. E. (1999). Faces of intention: Selected essays on intention and agency. Cambridge University \nPress.\nBreum, S. M., Egdal, D. V., Mortensen, V. G., Møller, A. G., & Aiello, L. M. (2023). The persuasive power of \nlarge language models. ArXiv. https://doi.org/10.48550/arXiv.2312.15523 \nDennett, D. C. (1989). The intentional stance. MIT Press. (Original work published 1987)\nDerner, E., Kučera, D., Oliver, N., & Zahálka, J. (2023). Can ChatGPT read who you are? ArXiv. \nhttps://doi.org/10.48550/arXiv.2312.16070 \nDotdash Meredith. (2023, May 16). Dotdash Meredith launches D/Cipher, a transformative intent-targeting \ntool for advertising. PR Newswire. https://www.prnewswire.com/news-releases/dotdash-meredith-launches-\ndcipher-a-transformative-intent-targeting-tool-for-advertising-301826071.html \nDotdash Meredith. (2024, May 7). Dotdash Meredith announces strategic partnership with OpenAI. Meredith \nCorporation MediaRoom. https://dotdashmeredith.mediaroom.com/2024-05-07-Dotdash-Meredith-Announces-\nStrategic-Partnership-with-OpenAI,-Bringing-Iconic-Brands-and-Trusted-Content-to-ChatGPT\nFranek, K. (2022, September 30). Microsoft revenue breakdown by product, segment and country. \nhttps://www.kamilfranek.com/microsoft-revenue-breakdown/\nGoodson, N., & Lu, R. (2023). Intention and context elicitation with large language models in the Legal Aid \nintake process. ArXiv. https://doi.org/10.48550/arXiv.2311.13281\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n15\nGray, M. L., & Suri, S. (2019). Ghost work: How to stop Silicon Valley from building a new global underclass. \nHoughton Mifflin Harcourt.\nHuang, S., Qin, L., Wang, B., Tu, G., & Xu, R. (2023). SDIF-DA: A shallow-to-deep interaction framework \nwith data augmentation for multi-modal intent detection. ArXiv. https://doi.org/10.48550/arXiv.2401.00424\nJakesch, M., Bhat, A., Buschek, D., Zalmanson, L., & Naaman, M. (2023). Co-writing with opinionated \nlanguage models affects users’ views. In A. Schmidt, K. Väänänen, T. Goyal, P. O. Kristensson, A. Peters, S. \nMueller, J. R. Williamson, & M. L. Wilson (Eds.), CHI ’23: Proceedings of the 2023 CHI Conference on \nHuman Factors in Computing Systems (Article 111). ACM. https://doi.org/10.1145/3544548.3581196\nJarrett, K. (2014). A Database of Intentions? In R. König & M. Rasch (Eds.), Society of the query reader: \nReflections on web search (pp. 17–29). Institute of Network Cultures.\nJia, M., Wu, Z., Reiter, A., Cardie, C., Belongie, S., & Lim, S.-N. (2021). Intentonomy: A dataset and study \ntowards human intent understanding. In 2021 IEEE/CVF Conference on Computer Vision and Pattern \nRecognition (CVPR) (pp. 12981–12991). IEEE. https://doi.org/10.1109/CVPR46437.2021.01279\nJoler, V. (2020). New extractivism. New Extractivism. https://extractivism.online/\nJoyal-Desmarais, K., Scharmer, A. K., Madzelan, M. K., See, J. V., Rothman, A. J., & Snyder, M. (2022). \nAppealing to motivation to change attitudes, intentions, and behavior: A systematic review and meta-analysis \nof 702 experimental tests of the effects of motivational message matching on persuasion. Psychological \nBulletin, 148(7–8), 465–517. https://doi.org/10.1037/bul0000377\nKidd, C., & Birhane, A. (2023). How AI can distort human beliefs. Science, 380(6651), 1222–1223. \nhttps://doi.org/10.1126/science.adi0248\nKosinski, M., Stillwell, D., & Graepel, T. (2013). Private traits and attributes are predictable from digital \nrecords of human behavior. Proceedings of the National Academy of Sciences, 110(15), 5802–5805. \nhttps://doi.org/10.1073/pnas.1218772110\nKruk, J., Lubin, J., Sikka, K., Lin, X., Jurafsky, D., & Divakaran, A. (2019). Integrating text and image: \nDetermining multimodal document intent in Instagram posts. ArXiv. https://doi.org/10.48550/arXiv.1904.09073\n \nLecinski, J. (2014, August 13). ZMOT: Why it matters now more than ever. Think with Google. \nhttps://www.thinkwithgoogle.com/marketing-strategies/search/zmot-why-it-matters-now-more-than-ever/\nLi, B. Z., Tamkin, A., Goodman, N., & Andreas, J. (2023). Eliciting human preferences with language models. \nArXiv. https://doi.org/10.48550/arXiv.2310.11589\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n16\nLiu, Q., Hou, X., Jin, H., Chen, jin, Wang, Z., Lian, D., Qu, T., Cheng, J., & Lei, J. (2023). Deep group interest \nmodeling of full lifelong user behaviors for CTR prediction. ArXiv. https://doi.org/10.48550/arXiv.2311.10764\nLuitse, D., & Denkena, W. (2021). The great transformer: Examining the role of large language models in the \npolitical economy of AI. Big Data & Society, 8(2). https://doi.org/10.1177/20539517211047734\nMaillot, M. (2024, August 21). Introduction to Teams AI Library—Teams. Microsoft. \nhttps://learn.microsoft.com/en-us/microsoftteams/platform/bots/how-to/teams-conversational-ai/teams-\nconversation-ai-overview\nMatz, S. C., Kosinski, M., Nave, G., & Stillwell, D. J. (2017). Psychological targeting as an effective approach \nto digital mass persuasion. Proceedings of the National Academy of Sciences, 114(48), 12714–12719. \nhttps://doi.org/10.1073/pnas.1710966114\nMatz, S. C., Teeny, J. D., Vaid, S. S., Peters, H., Harari, G. M., & Cerf, M. (2024). The potential of generative \nAI for personalized persuasion at scale. Scientific Reports, 14, Article 4692. https://doi.org/10.1038/s41598-\n024-53755-0\nMeredith, W. (2021). The steep cost of capture. Interactions, 28(6), 50–55. https://doi.org/10.1145/3488666\nMicrosoft. (2022). Msft-10k_20220630.htm. \nhttps://www.sec.gov/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\nNarayanan, A., & Kapoor, S. (2024). AI snake oil: What artificial intelligence can do, what it can’t, and how to \ntell the difference. Princeton University Press.\nNewport, C. (2024, March 15). Can an A.I. make plans? The New Yorker. \nhttps://www.newyorker.com/science/annals-of-artificial-intelligence/can-an-ai-make-plans\nnhciao. (2023, June 5). ControlNet for QR Code [Online forum post]. R/StableDiffusion. Reddit. \nwww.reddit.com/r/StableDiffusion/comments/141hg9x/controlnet_for_qr_code/\nNVIDIA. (2023, August 9). NVIDIA keynote at SIGGRAPH 2023 [Video]. YouTube. \nhttps://www.youtube.com/watch?v=Z2VBKerS63A\nOpenAI. (2023a, November 6). OpenAI DevDay, opening keynote [Video]. YouTube. \nhttps://www.youtube.com/watch?v=U9mJuUkhUzk\nOpenAI. (2023b, November 9). OpenAI data partnerships. https://openai.com/blog/data-partnerships \nOpenAI. (2023c, November 13). The business of AI [Video]. YouTube. https://www.youtube.com/watch?\nv=knHW-p31R0c\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n17\nPatel, D., & Xie, M. (2023, November 15). Microsoft infrastructure—AI & CPU custom silicon Maia 100, \nAthena, Cobalt 100. SemiAnalysis. https://www.semianalysis.com/p/microsoft-infrastructure-ai-and-cpu\nPenn, J. (2023). Animo nullius: On AI’s origin story and a data colonial doctrine of discovery. BJHS Themes, 8, \n19–34. https://doi.org/10.1017/bjt.2023.14\nRyan, J., & Christl, W. (2023). Europe’s hidden security crisis. Irish Council for Civil Liberties. \nhttps://www.iccl.ie/digital-data/europes-hidden-security-crisis/\nSalvi, F., Ribeiro, M. H., Gallotti, R., & West, R. (2024). On the conversational persuasiveness of large \nlanguage models: A randomized controlled trial. ArXiv. https://doi.org/10.48550/arXiv.2403.14380\nSearls, D. (2012). The Intention Economy: When Customers Take Charge (1st ed.). Harvard Business Review \nPress.\nSearle, J. R. (1983). Intentionality: An essay in the philosophy of mind. Cambridge University Press.\nShah, C., White, R. W., Andersen, R., Buscher, G., Counts, S., Das, S. S. S., Montazer, A., Manivannan, S., \nNeville, J., Ni, X., Rangan, N., Safavi, T., Suri, S., Wan, M., Wang, L., & Yang, L. (2024). Using large \nlanguage models to generate, validate, and apply user intent taxonomies. ArXiv. \nhttps://doi.org/10.48550/arXiv.2309.13063 \nSiddarth, D., Acemoglu, D., Allen, D., Crawford, K., Evans, J., Jordan, M., & Weyl, E. G. (2021). How AI fails \nus. Harvard University. https://ethics.harvard.edu/files/center-for-ethics/files/aifailsus.jhdcarr_final_2.pdf\nSingleton, T., Gerken, T., & McMahon, L. (2023, October 6). How a chatbot encouraged a man who wanted to \nkill the Queen. BBC News. https://www.bbc.com/news/technology-67012224\nStaab, R., Vero, M., Balunović, M., & Vechev, M. (2023). Beyond memorization: Violating privacy via \ninference with large language models. ArXiv. https://doi.org/10.48550/arXiv.2310.07298\nStark, L., & Hoey, J. (2021). The ethics of emotion in artificial intelligence systems. In FAccT ’21: Proceedings \nof the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 782–793). ACM. \nhttps://doi.org/10.1145/3442188.3445939\nStone, M. (2024, April 9). ChatGPT maker OpenAI poaches Shopify’s former head of support as the AI race \nheats up. Business Insider. https://www.businessinsider.com/openai-hires-former-shopify-executive-glen-\nworthington-2024-4\nTan, Z., & Jiang, M. (2023). User modeling in the era of large language models: Current research and future \ndirections. ArXiv. https://doi.org/10.48550/arXiv.2312.11518\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n18\nVeerabadran, V., Goldman, J., Shankar, S., Cheung, B., Papernot, N., Kurakin, A., Goodfellow, I., Shlens, J., \nSohl-Dickstein, J., Mozer, M. C., & Elsayed, G. F. (2023). Subtle adversarial image manipulations influence \nboth human and machine perception. Nature Communications, 14(1), Article 1. https://doi.org/10.1038/s41467-\n023-40499-0\nVicente, L., & Matute, H. (2023). Humans inherit artificial intelligence biases. Scientific Reports, 13(1), \nArticle 1. https://doi.org/10.1038/s41598-023-42384-8\nWilliams, J. (2018). Stand out of our light: Freedom and resistance in the attention economy. Cambridge \nUniversity Press. https://doi.org/10.1017/9781108453004\nWu, X., Laufer, E., Li, H., Khomh, F., Srinivasan, S., & Luo, J. (2023). Characterizing and classifying \ndeveloper forum posts with their intentions. ArXiv. https://doi.org/10.48550/arXiv.2312.14279\nZhang, A., Sheng, L., Chen, Y., Li, H., Deng, Y., Wang, X., & Chua, T.-S. (2023). On generative agents in \nrecommendation. ArXiv. https://doi.org/10.48550/arXiv.2310.10108\nZuboff, S. (2019). The age of surveillance capitalism: The fight for a human future at the new frontier of \npower. PublicAffairs.\n©2024 Yaqub Chaudhary and Jonnie Penn. This article is licensed under a Creative Commons Attribution (CC \nBY 4.0) International license, except where otherwise indicated with respect to particular material included in \nthe article.\nFootnotes\nReferences\n1.  To avoid confusion, we will no longer alternate between intention and ‘intention’ after this point. It is for \nthe reader to adjudicate the broader phenomenon we question here. ↩ \nAli, S. M., Dick, S., Dillon, S., Jones, M. L., Penn, J., & Staley, R. (2023). Histories of artificial intelligence: \nA genealogy of power. BJHS Themes, 8, 1–18. https://doi.org/10.1017/bjt.2023.15\n ↩ \nAltman, S. [@sama]. (2023, November 20). The mission continues [Post]. X. \nhttps://x.com/sama/status/1726510261509779876 \n ↩ \nAnscombe, G. E. M. (2000). Intention (2nd ed.). Harvard University Press.\n ↩ \nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n19\nApple. (n.d.). App Intents. Apple Developer Documentation. Retrieved August 11, 2024, from \nhttps://developer.apple.com/documentation/appintents\n ↩ \nBakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., Jacob, A. P., \nKomeili, M., Konath, K., Kwon, M., Lerer, A., Lewis, M., Miller, A. H., Mitts, S., Renduchintala, A., Roller, \nS., … Zijlstra, M. (2022). Human-level play in the game of diplomacy by combining language models with \nstrategic reasoning. Science, 378(6624), 1067–1074. https://doi.org/10.1126/science.ade9097\n ↩ \nBattelle, J. (2003, November 13). The Database of Intentions. John Battelle’s Search Blog. \nhttps://battellemedia.com/archives/2003/11/the_database_of_intentions\n ↩ \nBattelle, J. (2011). The search: How Google and its rivals rewrote the rules of business and transformed our \nculture. Hachette UK.\n ↩ \nBatty, M. (2013). The Database of Intentions. Environment and Planning B: Planning and Design, 40(3), \n381–383. https://doi.org/10.1068/b4003ed\n ↩ \nBenkler, Y., Faris, R., & Roberts, H. (2018). Mammon’s algorithm: Marketing, manipulation, and clickbait \non Facebook. In Y. Benkler, R. Faris, & H. Roberts (Eds.), Network propaganda: Manipulation, \ndisinformation, and radicalization in American politics (pp. 269–288). Oxford University Press. \nhttps://doi.org/10.1093/oso/9780190923624.003.0009\n ↩ \nBoden, M. A. (1973). The structure of intentions. Journal for the Theory of Social Behaviour, 3(1), 23–46. \nhttps://doi.org/10.1111/j.1468-5914.1973.tb00314.x\n ↩ \nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., \nBosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, \nK., Davis, J. Q., Demszky, D., … Liang, P. (2022). On the opportunities and risks of foundation models. \nArXiv. https://doi.org/10.48550/arXiv.2108.07258 \n ↩ \nBratman, M. E. (1999). Faces of intention: Selected essays on intention and agency. Cambridge University \nPress.\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n20\n↩ \nBreum, S. M., Egdal, D. V., Mortensen, V. G., Møller, A. G., & Aiello, L. M. (2023). The persuasive power \nof large language models. ArXiv. https://doi.org/10.48550/arXiv.2312.15523 \n ↩ \nDennett, D. C. (1989). The intentional stance. MIT Press. (Original work published 1987)\n ↩ \nDerner, E., Kučera, D., Oliver, N., & Zahálka, J. (2023). Can ChatGPT read who you are? ArXiv. \nhttps://doi.org/10.48550/arXiv.2312.16070 \n ↩ \nDotdash Meredith. (2023, May 16). Dotdash Meredith launches D/Cipher, a transformative intent-targeting \ntool for advertising. PR Newswire. https://www.prnewswire.com/news-releases/dotdash-meredith-launches-\ndcipher-a-transformative-intent-targeting-tool-for-advertising-301826071.html \n ↩ \nDotdash Meredith. (2024, May 7). Dotdash Meredith announces strategic partnership with OpenAI. \nMeredith Corporation MediaRoom. https://dotdashmeredith.mediaroom.com/2024-05-07-Dotdash-Meredith-\nAnnounces-Strategic-Partnership-with-OpenAI,-Bringing-Iconic-Brands-and-Trusted-Content-to-ChatGPT\n ↩ \nFranek, K. (2022, September 30). Microsoft revenue breakdown by product, segment and country. \nhttps://www.kamilfranek.com/microsoft-revenue-breakdown/\n ↩ \nGoodson, N., & Lu, R. (2023). Intention and context elicitation with large language models in the Legal Aid \nintake process. ArXiv. https://doi.org/10.48550/arXiv.2311.13281\n ↩ \nGray, M. L., & Suri, S. (2019). Ghost work: How to stop Silicon Valley from building a new global \nunderclass. Houghton Mifflin Harcourt.\n ↩ \nHuang, S., Qin, L., Wang, B., Tu, G., & Xu, R. (2023). SDIF-DA: A shallow-to-deep interaction framework \nwith data augmentation for multi-modal intent detection. ArXiv. https://doi.org/10.48550/arXiv.2401.00424\n ↩ \nJakesch, M., Bhat, A., Buschek, D., Zalmanson, L., & Naaman, M. (2023). Co-writing with opinionated \nlanguage models affects users’ views. In A. Schmidt, K. Väänänen, T. Goyal, P. O. Kristensson, A. Peters, S. \nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n21\n↩ \nJarrett, K. (2014). A Database of Intentions? In R. König & M. Rasch (Eds.), Society of the query reader: \nReflections on web search (pp. 17–29). Institute of Network Cultures.\n ↩ \nJia, M., Wu, Z., Reiter, A., Cardie, C., Belongie, S., & Lim, S.-N. (2021). Intentonomy: A dataset and study \ntowards human intent understanding. In 2021 IEEE/CVF Conference on Computer Vision and Pattern \nRecognition (CVPR) (pp. 12981–12991). IEEE. https://doi.org/10.1109/CVPR46437.2021.01279\n ↩ \nJoler, V. (2020). New extractivism. New Extractivism. https://extractivism.online/\n ↩ \nJoyal-Desmarais, K., Scharmer, A. K., Madzelan, M. K., See, J. V., Rothman, A. J., & Snyder, M. (2022). \nAppealing to motivation to change attitudes, intentions, and behavior: A systematic review and meta-\nanalysis of 702 experimental tests of the effects of motivational message matching on persuasion. \nPsychological Bulletin, 148(7–8), 465–517. https://doi.org/10.1037/bul0000377\n ↩ \nKidd, C., & Birhane, A. (2023). How AI can distort human beliefs. Science, 380(6651), 1222–1223. \nhttps://doi.org/10.1126/science.adi0248\n ↩ \nKosinski, M., Stillwell, D., & Graepel, T. (2013). Private traits and attributes are predictable from digital \nrecords of human behavior. Proceedings of the National Academy of Sciences, 110(15), 5802–5805. \nhttps://doi.org/10.1073/pnas.1218772110\n ↩ \nKruk, J., Lubin, J., Sikka, K., Lin, X., Jurafsky, D., & Divakaran, A. (2019). Integrating text and image: \nDetermining multimodal document intent in Instagram posts. ArXiv. \nhttps://doi.org/10.48550/arXiv.1904.09073 \n ↩ \nLecinski, J. (2014, August 13). ZMOT: Why it matters now more than ever. Think with Google. \nhttps://www.thinkwithgoogle.com/marketing-strategies/search/zmot-why-it-matters-now-more-than-ever/\n ↩ \nLi, B. Z., Tamkin, A., Goodman, N., & Andreas, J. (2023). Eliciting human preferences with language \nmodels. ArXiv. https://doi.org/10.48550/arXiv.2310.11589\n ↩ \nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n22\nLiu, Q., Hou, X., Jin, H., Chen, jin, Wang, Z., Lian, D., Qu, T., Cheng, J., & Lei, J. (2023). Deep group \ninterest modeling of full lifelong user behaviors for CTR prediction. ArXiv. \nhttps://doi.org/10.48550/arXiv.2311.10764\n ↩ \nLuitse, D., & Denkena, W. (2021). The great transformer: Examining the role of large language models in \nthe political economy of AI. Big Data & Society, 8(2). https://doi.org/10.1177/20539517211047734\n ↩ \nMaillot, M. (2024, August 21). Introduction to Teams AI Library—Teams. Microsoft. \nhttps://learn.microsoft.com/en-us/microsoftteams/platform/bots/how-to/teams-conversational-ai/teams-\nconversation-ai-overview\n ↩ \nMatz, S. C., Kosinski, M., Nave, G., & Stillwell, D. J. (2017). Psychological targeting as an effective \napproach to digital mass persuasion. Proceedings of the National Academy of Sciences, 114(48), 12714–\n12719. https://doi.org/10.1073/pnas.1710966114\n ↩ \nMatz, S. C., Teeny, J. D., Vaid, S. S., Peters, H., Harari, G. M., & Cerf, M. (2024). The potential of \ngenerative AI for personalized persuasion at scale. Scientific Reports, 14, Article 4692. \nhttps://doi.org/10.1038/s41598-024-53755-0\n ↩ \nMeredith, W. (2021). The steep cost of capture. Interactions, 28(6), 50–55. https://doi.org/10.1145/3488666\n ↩ \nMicrosoft. (2022). Msft-10k_20220630.htm. \nhttps://www.sec.gov/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\n ↩ \nNarayanan, A., & Kapoor, S. (2024). AI snake oil: What artificial intelligence can do, what it can’t, and how \nto tell the difference. Princeton University Press.\n ↩ \nNewport, C. (2024, March 15). Can an A.I. make plans? The New Yorker. \nhttps://www.newyorker.com/science/annals-of-artificial-intelligence/can-an-ai-make-plans\n ↩ \nnhciao. (2023, June 5). ControlNet for QR Code [Online forum post]. R/StableDiffusion. Reddit. \nwww.reddit.com/r/StableDiffusion/comments/141hg9x/controlnet_for_qr_code/\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n23\n↩ \nNVIDIA. (2023, August 9). NVIDIA keynote at SIGGRAPH 2023 [Video]. YouTube. \nhttps://www.youtube.com/watch?v=Z2VBKerS63A\n ↩ \nOpenAI. (2023a, November 6). OpenAI DevDay, opening keynote [Video]. YouTube. \nhttps://www.youtube.com/watch?v=U9mJuUkhUzk\n ↩ \nOpenAI. (2023b, November 9). OpenAI data partnerships. https://openai.com/blog/data-partnerships \n ↩ \nOpenAI. (2023c, November 13). The business of AI [Video]. YouTube. https://www.youtube.com/watch?\nv=knHW-p31R0c\n ↩ \nPatel, D., & Xie, M. (2023, November 15). Microsoft infrastructure—AI & CPU custom silicon Maia 100, \nAthena, Cobalt 100. SemiAnalysis. https://www.semianalysis.com/p/microsoft-infrastructure-ai-and-cpu\n ↩ \nPenn, J. (2023). Animo nullius: On AI’s origin story and a data colonial doctrine of discovery. BJHS Themes, \n8, 19–34. https://doi.org/10.1017/bjt.2023.14\n ↩ \nRyan, J., & Christl, W. (2023). Europe’s hidden security crisis. Irish Council for Civil Liberties. \nhttps://www.iccl.ie/digital-data/europes-hidden-security-crisis/\n ↩ \nSalvi, F., Ribeiro, M. H., Gallotti, R., & West, R. (2024). On the conversational persuasiveness of large \nlanguage models: A randomized controlled trial. ArXiv. https://doi.org/10.48550/arXiv.2403.14380\n ↩ \nSearle, J. R. (1983). Intentionality: An essay in the philosophy of mind. Cambridge University Press.\n ↩ \nSearls, D. (2012). The Intention Economy: When Customers Take Charge (1st ed.). Harvard Business \nReview Press.\n ↩ \nShah, C., White, R. W., Andersen, R., Buscher, G., Counts, S., Das, S. S. S., Montazer, A., Manivannan, S., \nNeville, J., Ni, X., Rangan, N., Safavi, T., Suri, S., Wan, M., Wang, L., & Yang, L. (2024). Using large \nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n24\n↩ \nSiddarth, D., Acemoglu, D., Allen, D., Crawford, K., Evans, J., Jordan, M., & Weyl, E. G. (2021). How AI \nfails us. Harvard University. https://ethics.harvard.edu/files/center-for-\nethics/files/aifailsus.jhdcarr_final_2.pdf\n ↩ \nSingleton, T., Gerken, T., & McMahon, L. (2023, October 6). How a chatbot encouraged a man who wanted \nto kill the Queen. BBC News. https://www.bbc.com/news/technology-67012224\n ↩ \nStaab, R., Vero, M., Balunović, M., & Vechev, M. (2023). Beyond memorization: Violating privacy via \ninference with large language models. ArXiv. https\n ↩ \nStark, L., & Hoey, J. (2021). The ethics of emotion in artificial intelligence systems. In FAccT ’21: \nProceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 782–793). \nACM. https://doi.org/10.1145/3442188.3445939\n ↩ \nStone, M. (2024, April 9). ChatGPT maker OpenAI poaches Shopify’s former head of support as the AI race \nheats up. Business Insider. https://www.businessinsider.com/openai-hires-former-shopify-executive-glen-\nworthington-2024-4\n ↩ \nTan, Z., & Jiang, M. (2023). User modeling in the era of large language models: Current research and \nfuture directions. ArXiv. https://doi.org/10.48550/arXiv.2312.11518\n ↩ \nVeerabadran, V., Goldman, J., Shankar, S., Cheung, B., Papernot, N., Kurakin, A., Goodfellow, I., Shlens, J., \nSohl-Dickstein, J., Mozer, M. C., & Elsayed, G. F. (2023). Subtle adversarial image manipulations influence \nboth human and machine perception. Nature Communications, 14(1), Article 1. \nhttps://doi.org/10.1038/s41467-023-40499-0\n ↩ \nVicente, L., & Matute, H. (2023). Humans inherit artificial intelligence biases. Scientific Reports, 13(1), \nArticle 1. https://doi.org/10.1038/s41598-023-42384-8\n ↩ \nWilliams, J. (2018). Stand out of our light: Freedom and resistance in the attention economy. Cambridge \nUniversity Press. https://doi.org/10.1017/9781108453004\nHarvard Data Science Review • Special Issue 5: Grappling With the\nGenerative AI Revolution\nBeware the Intention Economy: Collection and Commodi\u0000cation of\nIntent via Large Language Models\n25\n↩ \nWu, X., Laufer, E., Li, H., Khomh, F., Srinivasan, S., & Luo, J. (2023). Characterizing and classifying \ndeveloper forum posts with their intentions. ArXiv. https://doi.org/10.48550/arXiv.2312.14279\n ↩ \nZhang, A., Sheng, L., Chen, Y., Li, H., Deng, Y., Wang, X., & Chua, T.-S. (2023). On generative agents in \nrecommendation. ArXiv. https://doi.org/10.48550/arXiv.2310.10108\n ↩ \nZuboff, S. (2019). The age of surveillance capitalism: The fight for a human future at the new frontier of \npower. PublicAffairs.\n ↩ "
}