{
    "title": "YOLF-ShipPnet: Improved RetinaNet with Pyramid Vision Transformer",
    "url": "https://openalex.org/W4366392445",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5077065762",
            "name": "Zhiruo Qiu",
            "affiliations": [
                "Capital University of Economics and Business"
            ]
        },
        {
            "id": "https://openalex.org/A5071190449",
            "name": "Shiyang Rong",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A5061462116",
            "name": "Likun Ye",
            "affiliations": [
                "South China University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4294672822",
        "https://openalex.org/W3191257570",
        "https://openalex.org/W4291819654",
        "https://openalex.org/W4285490787",
        "https://openalex.org/W2919011445",
        "https://openalex.org/W3131542065",
        "https://openalex.org/W3146188393",
        "https://openalex.org/W3121020412",
        "https://openalex.org/W3120903390",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W6790690058",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W2400138547",
        "https://openalex.org/W2625829240",
        "https://openalex.org/W4321021814",
        "https://openalex.org/W4280490223",
        "https://openalex.org/W2952034878",
        "https://openalex.org/W4312513332",
        "https://openalex.org/W4214572870",
        "https://openalex.org/W6798838024",
        "https://openalex.org/W4206487945",
        "https://openalex.org/W4220941228",
        "https://openalex.org/W2969278648",
        "https://openalex.org/W2594177559",
        "https://openalex.org/W3200733355",
        "https://openalex.org/W3171395710",
        "https://openalex.org/W2962749812",
        "https://openalex.org/W3035396860"
    ],
    "abstract": "Abstract In the field of ship detection, the intricate nature of ship images arises from a multitude of factors, including variations in ship orientation, color contrasts, and diverse shapes. These factors collectively contribute to the challenge of achieving high detection precision. Thus, it is necessary to investigate the application of advanced networks for ship image detection. In this paper, we have put forward an improved network called YOLF-ShipPnet, which utilizes a popular pyramid vision transformer with increased depth as the backbone for the RetinaNet network. To increase the model’s generalization ability, You Only Look Once eXtreme’s (YOLOX’s) hue, saturation, and value (HSV) random augmentation technique is employed to simulate light and color effects on ship images during the construction of the network. Ablation experiments were conducted on the model with two popular datasets: High-Resolution Ship Collections 2016 (HRSC2016) and SAR Ship Detection Dataset (SSDD). The YOLF-ShipPnet network has been verified to improve detection precision and generalization ability in ship detection by $$5.22\\%$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mn>5.22</mml:mn> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> and $$5.46\\%$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mn>5.46</mml:mn> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> , respectively, compared to RetinaNet baseline, exhibiting strong robustness and high effectiveness. The proposed network is applicable to the field of fine-grained ship detection and achieves an accuracy improvement of $$10.03\\%$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mn>10.03</mml:mn> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> compared to the baseline network.",
    "full_text": "Vol.:(0123456789)1 3\nInternational Journal of Computational Intelligence Systems           (2023) 16:58  \nhttps://doi.org/10.1007/s44196-023-00235-4\nRESEARCH ARTICLE\nYOLF‑ShipPnet: Improved RetinaNet with Pyramid Vision Transformer\nZhiruo Qiu1 · Shiyang Rong2 · Likun Ye3 \nReceived: 11 January 2023 / Accepted: 1 April 2023 \n© The Author(s) 2023\nAbstract\nIn the field of ship detection, the intricate nature of ship images arises from a multitude of factors, including variations in \nship orientation, color contrasts, and diverse shapes. These factors collectively contribute to the challenge of achieving high \ndetection precision. Thus, it is necessary to investigate the application of advanced networks for ship image detection. In this \npaper, we have put forward an improved network called YOLF-ShipPnet, which utilizes a popular pyramid vision transformer \nwith increased depth as the backbone for the RetinaNet network. To increase the model’s generalization ability, You Only \nLook Once eXtreme’s (YOLOX’s) hue, saturation, and value (HSV) random augmentation technique is employed to simulate \nlight and color effects on ship images during the construction of the network. Ablation experiments were conducted on the \nmodel with two popular datasets: High-Resolution Ship Collections 2016 (HRSC2016) and SAR Ship Detection Dataset \n(SSDD). The YOLF-ShipPnet network has been verified to improve detection precision and generalization ability in ship \ndetection by 5.22% and 5.46% , respectively, compared to RetinaNet baseline, exhibiting strong robustness and high effective-\nness. The proposed network is applicable to the field of fine-grained ship detection and achieves an accuracy improvement \nof 10.03% compared to the baseline network.\nKeywords YOLF-ShipPnet · Ship detection · RetinaNet · PVTv2 · HSV · Depth effectiveness · Fine-grained classification\nAbbreviation\nYOLOX  You Only Look Once eXtreme\nHSV  Hue, saturation, and value\nHRSC2016  High-Resolution Ship Collections 2016\nSSDD  SAR Ship Detection Dataset\nYOLO  You Only Look Once\nSSD  Single-stage detector\nFaster RCNN  Fast region-based convolutional neural \nnetwork\nMask RCNN  Mask region-based convolutional neural \nnetwork\nFPN  Feature pyramid network\nR-CNN  Region-based convolutional neural \nnetwork\nNMS  Non-maximum suppression processing\nSkew-NMS  Skew non-maximum suppression\nPVT  Pyramid vision transformer\nPVTv1  Pyramid Vision Transformer Version 1\nPVTv2  Pyramid Vision Transformer Version 2\nResNet  Residual network\nCNN  Convolutional neural network\nRNN  Recurrent neural network\nViT  Vision transformer\nTNT  Transformer iN transformer\nRR-CNN  Rotated region-based convolutional \nneural network\nRRoI  Rotated region of interest\nFCN  Fully convolutional network\nRPN  Region proposal network\nR2PN  Rotated region proposal network\nMSCAF-Net  Multi-scale convolutional attention \nfusion network\nLikun Ye, Zhiruo Qiu and Shiyang Rong have contributed equally to \nthis work and should be considered co-first authors.\n * Likun Ye \n 202030020427@mail.scut.edu.cn\n1 School of Management Engineering, Capital University \nof Economics and Business, Beijing 100070, \nPeople’s Republic of China\n2 College of Information Science and Engineering, \nNortheastern University, Nanhu Campus, Shenyang 110006, \nPeople’s Republic of China\n3 Shien-Ming Wu School of Intelligent Engineering, South \nChina University of Technology, Guangzhou International \nCampus, Guangzhou 511442, People’s Republic of China\n International Journal of Computational Intelligence Systems           (2023) 16:58 \n1 3   58  Page 2 of 15\nGHFormer-Net  Gradient harmonized transformer \nnetwork\nGHM-C  Gradient harmonized single-stage detec-\ntor with context aggregation\nGHM-R  Gradient harmonized regression\nUP-ViTs  UP-vision transformer\nIR-Net  Improved RetinaNet\nSRA  Spatial reduction attention\nFCN  Fully convolutional networks\nSAR  Synthetic aperture radar\nBIGSARDATA   SAR in big data era\nAP  Average precision\nmAP  Mean average precision\n1 Introduction\nShip detection exhibits excellent application prospects in \nmaritime trade, ship traffic control, port transportation, \nand national defense security. The conducted research on \nadvanced networks is of much importance [1 ]. Although \nresearchers have paid much effort into investigating ship \ndetection, it remains challenging due to the various orienta-\ntions of ships, color contrast within specific ship types, and \nthe higher resolution requirements of the ship images. Many \nresearchers have proposed networks for ship detection; how-\never, they often do not incorporate a fine-grained study of \nspecific ships. This is due to the fact that changes in external \nlight intensity or the use of different colors on similar ships \ncan greatly affect detection accuracy.\nWith the advancement in object detection algorithms for \ndeep convolutional neural networks, two detection modes \nwith different stage numbers have emerged and are dis-\ntinguished by whether the region proposal is utilized [2 ]. \nOne-stage algorithm for detection is characterized by direct \naccess to positional coordinates and corresponding regres-\nsion for target categories, which helps to reduce time com-\nplexity. Typical one-stage detectors are You Only Look Once \n(YOLO) [3], single-stage detector (SSD) [4], and RetinaNet \n[5]. Two-stage detection introduces the application of region \nproposal as front refinement, and regions of interest are clas-\nsified and located in the latter stage [6 ]. Typical two-stage \ndetectors are faster region-based convolutional neural net-\nwork (faster RCNN) [ 7], mask region-based convolutional \nneural network (mask RCNN) [8], and feature pyramid net-\nwork (FPN) [9 ]. In this paper, the base classifier is a one-\nstage RetinaNet, which operates through the use of a robust \nfocal loss method. As a result, it is able to combine the ben-\nefits of a one-stage detector, such as fast speed, with those \nof a two-stage detector, such as high detection precision. \nWith respect to computer vision, the horizontal frame target \ndetection algorithm based on a region-based convolutional \nneural network (R-CNN) shows rich application scenarios \nin the classification and detection of remote sensing images. \nHowever, it may generate background noise when detecting \ntargets with large aspect ratios. The omission of detection \ntargets is likely to occur when performing non-maximum \nsuppression (NMS) processing. Therefore, in recent years, \na significant number of scholars have devoted themselves to \nthe study of rotating region proposal algorithms designed by \nintroducing anchor frame rotation angle parameters, which \neffectively retain the target orientation feature information \nas opposed to background noise [10] and improve tilt target \ndetection accuracy with the help of skew non-maximum sup-\npression (skew-NMS).\nIn this paper, we introduce YOLF-ShipPnet, a novel net-\nwork architecture that utilizes the state-of-the-art Pyramid \nVision Transformer Version 2 (PVTv2) as the backbone net-\nwork for the RetinaNet base classifier. The rotating frame is \nused for global and fine-grained ship image detection. Dis-\ntinguished from foregoing works, the depth of the network is \nincreased, and we perform random data augmentation using \nYOLOX’s HSV to improve its fine-grained classification \ncapability for the ship dataset.\nOur main contributions are:\n(1) We propose the YOLF-ShipPnet network, which will \nbe used in ship detection for commercial and military \npurposes. In the YOLF-ShipPnet network, we intro-\nduce the application of popular PVTv2 architecture to \nthe construction of the backbone of the RetinaNet base \nclassifier, fully exploring its depth effectiveness. Also, \nYOLOX’s HSV is led into the field to manipulate ran-\ndom data augmentation on the ship datasets.\n(2) We demonstrate that the detection precision of YOLF-\nShipPnet outperforms the conventional scheme and, as \nthe depth of the network's depth increases, it gradu-\nally shows better performance. After random data aug-\nmentation, it is shown that the model can have better \ngeneralization abilities and is effective in enhancing \nthe designed network. With the deepened network \nand effective data augmentation, the proposed YOLF-\nShipPnet network is applicable for fine-grained ship \ndetection and transcends the baseline by a large margin.\nThe remaining parts of this paper are developed as fol-\nlows. In Sect. 2, we summarize the related work concerning \nthe development of transformer architecture and the evolu-\ntion of rotated frame detection. Also, existing methods with \ndifferent functionalities for ship detection are briefly summa-\nrized. In Sect. 3, we demonstrate the detailed construction of \nthe PVTv2 backbone and the mechanism of YOLOX’s HSV \nfor data augmentation. In Sect.  4, we provide the results of \nthe validation and ablation experiments on two datasets to \nprove the depth effectiveness of the PVTv2 network and the \nvalidity of the YOLOX’s HSV data augmentation strategy. \nInternational Journal of Computational Intelligence Systems           (2023) 16:58  \n1 3 Page 3 of 15    58 \nIt is proved that our proposed network can be applied to \nfine-grained ship detection. We also compare our network \nwith other advanced networks to demonstrate its superiority. \nThe conclusion of our research is given in Sect.  5. Reflec-\ntion on our current work and future research prospect are \ndemonstrated in Sect. 6.\n2  Related Work\nTransformer architecture is used in place of residual net-\nwork (ResNet) [11] to form the backbone of the RetinaNet \n[5] network. In 2017, the Google team first proposed the \ntransformer model, which abandoned the traditional convo-\nlutional neural network (CNN) and recurrent neural network \n(RNN) architecture, making the entire network structure \ncomposed completely of the attention mechanism. Trans-\nformer is the pioneer in transduction mode design with the \nfunctionality of calculating primary substitution of input and \noutput based on the principle of self-attention [12], which \nis widely used in the computer vision field to manipulate \nimage detection tasks.\nThe development of transformer structure can be divided \ninto three stages, with its enhancement in function and \nboost in efficiency. In the first stage, the emergence of the \nattention mechanism enhanced the traditional CNN net-\nwork with an optimized fusion of functionality. Bello et al. \n(2019) introduced a self-attention transformer model with \ntwo-dimensional architecture that combines convolutional \nfeature maps and feature mapping generated by self-atten-\ntion[13]. By leveraging a global perspective to analyze the \nentire image, the model outperforms a CNN that is limited to \nprocessing only local information, resulting in a significant \nimprovement in accuracy for image detection tasks. Later, \nthe transformer architecture reached the level of complete \nreplacement of CNN with its excellent testing performance, \ndue to the attention mechanism being used in image detec-\ntion. For example, Dosovitskiy et al. (2020) introduced a \nvision transformer (ViT), which is directly applied to a series \nof image patches without any reliance on CNNs, demand-\ning less computational power and achieving better detec-\ntion performances as compared to first-class prototypes of \nconvolutional neural networks [14]. Since then, based on \nViT, a series of methods have emerged to improve and opti-\nmize the transformer structure for enhanced efficiency and \neffectiveness. Han et al. (2021) issued a brand-new vision \narchitecture of a transformer called Transformer iN Trans-\nformer (TNT), which divides the local patch into sub-patch. \nIt integrates both information and generates representation at \npatch granularity with the help of the outer transformer [15]. \nWang et al. (2022) proposed the pyramid vision transformer \n(PVT), which can obtain higher output resolution when \ntrained on denser regions of an image and reduce the cost \nof large feature maps’ computations by employing a pro-\ngressively contracting pyramid [16]. To conclude, the trans-\nformer architecture integrates the attention mechanism into \nthe construction of the forward feedback network, which has \nbetter parallelism and global optimization capabilities. It sig-\nnificantly improves the execution of dense image detection \nin terms of efficiency and accuracy, exhibiting broad appli-\ncation prospects in multimodality and object identification.\nRotated frame detection is widely used when conducting \nship detection. For example, Liu et al. (2016) proposed a \nnovel ship rotation bounding box that accurately captures \nthe true shape of ships embedded in complex backgrounds. \nThe method involves generating representative candidate \nregions using a closed-form region approach, which outper-\nforms traditional horizontal frame target detection schemes \n[17]. Hu et al. (2017) introduced the rotated region-based \nconvolutional neural network (RR-CNN), which integrates \na rotated region of interest (RRoI) pooling layer and a \nregression model equipped with a rotating bounding box \nto accomplish ship detection. It excels in the extraction of \nkey features within rotated regions and thus can capture the \ninclined detection targets more precisely [ 18]. Liao et al. \n(2022) proposed a novel rotated region proposal network \n (R2PN) to form multi-directional proposals featured by \nthe angle information of the orientation of ships, which \nadopts a pooling layer activated by rotated region of inter -\nest to manipulate key feature extraction and uses bounding \nboxes regression to increase the accuracy of the inclined \nship region proposals. The proposed network model achieves \nsuperior performance in ship detection, particularly for ships \nwith multiple orientations [10].\nExisting methods for ship detection hardly investigate \ninto the depth effectiveness of the feature extraction net-\nworks and have limited generalization or fine-grained \ndetection ability. Liu et al. (2023) proposed multi-scale con-\nvolutional attention fusion network (MSCAF-Net), a frame-\nwork with PVTv2-B2 backbone for detecting camouflaged \nobjects that focuses on learning features that are sensitive \nto context at different scales. While the efficacy of the net-\nwork is evident for the reference datasets, its potential for \nprofound exploration is constrained due to the utilization \nof a solitary layer of the PVTv2 network [19]. Sun et al. \n(2022) proposed gradient harmonized transformer network \n(GHFormer-Net), which utilizes PVTv2-B1 as the backbone \nnetwork and incorporates gradient harmonized single-stage \ndetector with context aggregation (GHM-C) and gradient \nharmonized regression (GHM-R) loss functions to improve \nfruit detection in low-light conditions. The experimental \nresults demonstrate the effectiveness of the model, but the \nstudy only investigates the first layer of the network and does \nnot explore the potential benefits of using deeper layers of \nPVTv2 [20]. Hao et al. (2021) proposed a unified network \ncalled UP-Vision Transformer (UP-ViTs) for systematic \n International Journal of Computational Intelligence Systems           (2023) 16:58 \n1 3   58  Page 4 of 15\npruning of vision transformers and their extensions. How -\never, their study revealed that when using UP-ViTs to prune \nPVTv2-B2 into UP-PVTv2-B1 on ImageNet–1 k valida-\ntion, it increased the accuracy of PVTv2-B1, but was less \neffective than the deepened PVTv2-B2. This suggests that \nthe lack of depth effectiveness in the network's design may \nhave contributed to the suboptimal results [21]. Liu et al. \n(2017) proposed RR-CNN, which features an intensive task \napproach for non-maximum suppression among different \nclasses, overcoming challenges in detecting strip-like rotated \nassembled objects. The network outperforms baseline mod-\nels by a significant margin, but its compatibility with other \nrotation-based frameworks is limited [18]. Yan et al. (2019) \nproposed an innovative data augmentation method that \nutilizes simulated remote sensing ship images to augment \npositive training samples, thereby improving the quality of \nthe training set. Experimental results on the ship detection \ndataset using Faster R-CNN demonstrate the effectiveness \nof the approach. However, the method is only applicable \nto a limited number of ship models and does not possess \nthe ability of fine-grained classification [22]. Zhao et al. \nexplores low-resolution fine-grained object classification and \nproposes a new model, which combines feature equilibrium \nprinciple and progressive interaction theory. It improves the \naccuracy of network when applied to low-resolution image \ndetection, but when it comes to fine-grained classification, \nit only increases the baseline model by 3.4%, which is not \nsatisfactory enough [23].\n3  Model and Network\nWe propose a brand-new network called YOLF-ShipPnet, \nwhich incorporates deepened PVTv2 into the construction \nof the backbone of the RetinaNet network. The network \nstructure of YOLOF-ShipPnet is demonstrated in Fig.  1. \nThe RetinaNet network is a comprehensive baseline net-\nwork consisting of a backbone, a neck and a head consist-\ning of two subnets. The backbone network, namely, PVTv2, \nimplements the convolutional feature mapping over the tar-\nget image and is treated as a non-self-convolutional network. \nThe neck part of the network is the feature pyramid network \n(FPN), which is utilized for multi-scale feature integration. \nThe output of the FPN is then fed into the head part of the \nnetwork, which comprises two subnets, namely the class \nsubnet and the box subnet. Two subnets are distinguished \nby branch functions, one for classification and the other for \nregression. Specifically, the first subnet carries out object \nclassification with a convolutional technique targeted at the \nbackbone output, and the second subnet implements con-\nvolutional regression with the help of a bounding boxes. In \nYOLF-ShipPnet, considering the need for higher precision \naccompanied by the deepened network, we choose to use \nPVTv2 for its deepened network architecture.\n3.1  Backbone Network: PVTv2 with Transformer \nArchitecture\nSince the introduction of ViT, there has been a large num-\nber of researches on vision transformers, roughly along two \nmain directions: one is to improve the effectiveness of ViT in \nimage classification; the other is to apply ViT to other image \ntasks, such as image segmentation and target detection. The \nPVT [24] introduced in this paper belongs to the latter. PVT \nis a simple, non-convolutional backbone that can be applied \nfor many prediction tasks containing dense images. Unlike \nViT, which employs a pure transformer architecture, PVTv2 \nincorporates a hybrid architecture that combines both trans-\nformer and convolutional neural network (CNN) structure. \nPVT overcomes the difficulty of applying a transformer to \nvarious task-oriented predictions with complex partitions, \nexhibiting better feature extraction performance.\nPVT was originally proposed by Wang Wenhai and Xie \nEnze at Nanjing University and has undergone two genera-\ntions of evolution, Pyramid Vision Transformer Version \n1 (PVTv1) and PVTv2 [25]. Generally, PVTv1 has three \nmain limitations. Firstly, PVTv1 treats the images as a \nseries of non-overlapping facets, which somewhat loses the \nFig. 1  The architecture of YOLF-ShipPnet network diagram of the model\nInternational Journal of Computational Intelligence Systems           (2023) 16:58  \n1 3 Page 5 of 15    58 \ncharacteristic of partial continuity of the images, limiting its \napplication for fined-grained ship classification. Secondly, \nthe size of the encoding of position in PVTv1 architecture \nis pre-determined and invariant for processing images of \ndiscretional size. However, the most significant drawback \nof PVTv1 is that its network architecture has limited depth, \nwhich harms the precision of image classification. Taking \ninto account the fact that the detection precision of our base-\nline network maintains at a low level, we choose to use deep-\nened PVTv2 with depth-wise convolution as the backbone \nto trade off a lightweight network for higher precision, as \nshown in Fig. 2. It can detect dense ship images and perform \nfeature extraction of local features more smoothly for fine-\ngrained classification, which is satisfactory for application \non ship image detection (Table 1). \nDifferent layers of the PVTv2 network (B0–B5) are con-\nstructed by changing the following hyperparameters:\nS i ∶ The stride in stage i for overlapping patch embedding.\nC i ∶ The number of channels in the output of the ith stage.\nL i ∶ The number of encoded overlapping in the ith stage.\nR i Deceleration ratio of the i th stage Spatial Reduction \nAttention (SRA).\nP i ∶ Mean pool size of linear SRA in the ith stage.\nN i ∶ Head number of the self-attention network in the ith \nstage.\nE i ∶ The expansion ratio of the ith stage feedforward layer.\nThe design of the PVTv2 network adheres to the princi-\nple that is used to construct ResNet, where the number of \nchannel dimensions increases as the layers deepen, leading \nto a theoretical improvement in the detection precision of \nPVTv2 with increased depth. So, we suppose that the effect \nFig. 2  Comparison of the depth \nof two versions of PVT\nTable 1  Overall network architecture of PVTv2\nOutput size Layer label Pyramid Vision Transformer \nv2\nB0 B3 B5\nStage 1 H\n4 × W\n4\nOverlapping\npatch embed-\nding\nS1 = 4\nC1 = 32 C1 = 64\nTransformer\nencoder\nR1 = 8\nN1 = 1\nE1 = 8\nL1 = 2\nR1 = 8\nN1 = 1\nE1 = 8\nL1 = 3\nR1 = 8\nN1 = 1\nE1 = 4\nL1 = 3\nStage 2 H\n8 × W\n8\nOverlapping\npatch embed-\nding\nS1 = 2\nC2 = 64 C2 = 128\nTransformer\nencoder\nR2 = 4\nN2 = 2\nE2 = 8\nL2 = 2\nR2 = 4\nN2 = 2\nE2 = 8\nL2 = 3\nR2 = 4\nN2 = 2\nE2 = 4\nL2 = 6\nStage 3 H\n16 × W\n16\nOverlapping\npatch embed-\nding\nS1 = 2\nC3 = 160 C3 = 320\nTransformer\nencoder\nR3 = 2\nN3 = 5\nE3 = 4\nL3 = 2\nR3 = 2\nN3 = 5\nE3 = 4\nL3 = 18\nR3 = 2\nN3 = 5\nE3 = 4\nL3 = 40\nStage 4 H\n32 × W\n32\nOverlapping\npatch embed-\nding\nS1 = 2\nC4 = 256 C4 = 512\nTransformer\nencoder\nR4 = 1\nN4 = 8\nE4 = 4\nL4 = 2\nR4 = 1\nN4 = 8\nE4 = 4\nL4 = 3\nR4 = 1\nN4 = 8\nE4 = 4\nL4 = 3\n International Journal of Computational Intelligence Systems           (2023) 16:58 \n1 3   58  Page 6 of 15\nof depth-wise convolution of PVTv2 is still manifested in \nthe HRSC2016 dataset. Taking into account the matching \nof the dataset, network complexity, and computational cost, \nwe choose to deepen the layer of our network from B1 to B5.\n3.2  Neck: Feature Pyramid Net\nWe apply FPN to the neck part of the network. The origin \nof the idea of FPN is the image pyramid in traditional image \nprocessing [26]. It aims to enhance the robustness of the \nmodel when the input images are of different sizes or vari-\nous objects exist in the scenarios of target detection. FPN \nadopts the multi-scale feature fusion method, which consid-\ners global and local features during target detection. FPN \nenhances the conventional convolutional network with novel \ntransverse connections and top-down pathways, thereby con-\nstructing a comprehensive, multi-dimensional feature pyra-\nmid from singular input images. Each layer of the pyramid \ncan be used to detect objects with various dimensions. FPN \nis a powerful technique for improving multi-dimensional \npredictions from fully convolutional networks (FCN). It has \nbeen used to generate a range of subsequent networks such \nas region proposal network (RPN), deep mask object pro-\nposal, and two-stage detectors like faster R-CNN and mask \nR-CNN.\n3.3  Head: Classification and Regression of Rotating \nFrame Networks\nThe objective of focal loss [27] is to address the issues of \nimbalanced class distribution and the resulting challenges in \nclassification, particularly when the dataset contains a large \nnumber of easy background samples and a few foreground \nsamples that are challenging to classify. Focal loss mitigates \nthese problems and enhances the accuracy of detection by \nmodifying the cross-entropy function, increasing the cat-\negory weights /u1D6FC and the sample difficulty weight modulating \nfactor (1 − p t) . The focal loss function takes the following \nform:\nIn formula 1, −log(p t) stands for the initial cross-entropy \nloss function, /u1D6FC is the weight parameter between categories, \n(1 − pt)/u1D6FE is the modulating factor between simple and com-\nplex samples, and /u1D6FE is the focusing parameter.\nOne common loss function used for bounding box regres-\nsion in the head part of object detection models is the L1 \nloss. In ship detection, L1 loss is particularly useful for accu-\nrately predicting the coordinates of the bounding box around \na ship. By minimizing the mean absolute difference between \nthe predicted and actual bounding box coordinates, L1 loss \nhelps to improve the accuracy of the ship detection model. \nThe formula for L1 loss is as follows:\nwhere yi denotes the true label and f /parenleft.s1x i\n/parenright.s1 indicates the pre-\ndicted label.\n3.4  Data Augmentation Strategy: HSV [28]\nHSV is a color space put forward by a.r. Smith, in 1978 \ninspired by the intuitive properties of the color [29], also \nknown as the hexagonal model. In the field of data aug-\nmentation in ship detection, it is used to enhance the color \ncontrast of the image by adjusting the intensity ratio of hue, \nsaturation, and value channel [30]. It extracts and manifests \nthe feature color space of the ship image corresponding to \nthe change of light state and external color of ships. The \ncolor space of the HSV model can be visualized using a \ncone, as shown in Fig. 3, accompanied by target images with \ncontrast brightness and colors. H (hue) in the cone represents \nthe phase angle of the color, with a range of 0◦ to 360◦ . S \n(saturation) stands for a ratio value, which is correlated with \nthe purity of a specific color. Following the direction of the S \narrow, the purity of color witnessed a significant increase. V \n(value) represents the brightness of the color, ranging from 0 \nto 1. V value of the cone ranges from 0 at the black bottom \n(1)FL /parenleft.s1pt\n/parenright.s1=− /u1D6FCt\n/parenleft.s11 − pt\n/parenright.s1/u1D6FE\nlog/parenleft.s1pt\n/parenright.s1.\n(2)L 1 =\nn/uni2211.s1\ni=1\n/uni007C.x/uni007C.x/uni007C.xyi − f/parenleft.s1xi\n/parenright.s1/uni007C.x/uni007C.x/uni007C.x,\nFig. 3  HSV augmentation for color-oriented data augmentation\nInternational Journal of Computational Intelligence Systems           (2023) 16:58  \n1 3 Page 7 of 15    58 \npoint to 1 at the top white point, with higher values indicat-\ning greater brightness.\nIn the YOLF-ShipPnet network, HSV is used to manipu-\nlate data augmentation on the ship dataset by adequately \nadjusting the three-channel values of the color space, aim-\ning to simulate the background state of ship images under \nvarious lighting conditions and also to adjust the bright-\nness, colors, and other factors of the image to reduce the \nsensitivity of our proposed model to ship colors. The data \naugmentation strategy eliminates disturbance factors such as \npotential changes in light intensity and color differences of a \nspecific ship, which significantly improves the local feature \nextraction ability and the robustness of the network. The \nefficiency of training and the performance of our network is \nalso further enhanced with the help of the YOLOX’s HSV \nrandom data augmentation technique.\n3.5  YOLOF‑ShipPnet\nThe model of YOLOF-ShipPnet is shown in Fig.  4. HSV \ncolor space is employed as a data augmentation technique \nto augment the light effects on ships and the external colors \nof certain ships. This approach produces a set of synthe-\nsized images by leveraging the HRSC2016 dataset, which \nhelps in improving the model's training. We select PVTv2 \nas the backbone network, which is an enhanced transformer \nnetwork with depth inheritance. After testing and refine-\nment, our proposed network is expected to carry out global \nship image and fine-grained classification.\n4  Experiment and Analysis\n4.1  Dataset\nAblation experiments are performed on the famous remote \nsensing dataset HRSC2016 [31] and the synthetic aperture \nradar (SAR) dataset SSDD [32] to validate the effectiveness \nof our proposed YOLF-ShipPnet network.\nNorthwestern Polytechnical University published the \nHRSC2016 [31] dataset in 2016. The set issued by Google \nEarth contains 1,061 images with 4 classes and 19 sub-\nclasses, covering 2976 instances of ships. The training, vali-\ndation, and test sets incorporate 436, 181, and 444 images, \nrespectively. The image sizes of HRSC2016 range from \n300 × 300 to1500 × 900 , with the majority of the images \nhaving sizes greater than 1000 × 600 . The dataset covers 27 \ntypes of remote sensing ground objects. For a fair compari-\nson with other networks, only ship objects are selected for \nour experiments.\nThe SSDD dataset [32] was first unveiled at the SAR \nIn Big Data Era (BIGSARDATA) conference in Beijing in \nReal ship images\nSimulation imagesH RSC2016\nTesting samples\nHSV Data Augmentation Strategy1\n YOLOF-ShipPnet2\nNetwork inheritance\nB0\nB1 B2 B3 B4 B5\nGlobal detection\nFine-grained detection\nPVTv2\nBox subnetsDifferent light intensity\nHSV\nTest & Classification3\nFig. 4  Design flowchart of YOLOF-ShipPnet\n International Journal of Computational Intelligence Systems           (2023) 16:58 \n1 3   58  Page 8 of 15\n2017. The set contains 1160 images and 2456 ships, with an \naverage number of ships of 2.12 per image. The image sizes \nare around500 × 500 . The set is partitioned into the training, \nvalidation, and test sets, with a random ratio of 7 ∶ 1 ∶ 2 . \nThis dataset contains SAR images specially used for ship \ndetection with a single ship type.\nOur ablation experiments use average precision ( AP ) and \nmean average precision ( mAP ) for evaluation of the per -\nformance of YOLF-ShipPnet. In MMROTATE, the general \ndefinition of AP is the gross area below the precision–recall \ncurve. Precision measures the accuracy of prediction, while \nrecall reflects the proportion of positive samples that are \nsuccessfully retrieved. So to calculate them, the quantities \nthat shall be known in advance aretp , the number of correctly \ndetermined positive samples, and fn andfp , which are incor-\nrectly determined negative and positive samples. Formulas \n3 and 4 illustrate the calculation processes of precision and \nrecall:\nAfter plotting corresponding data points of precision and \nrecall into a curve, the value of AP can be calculated by \nintegrating the area beneath the curve. Then, mAP is derived \nby averaging over the AP of each epoch.\n4.2  Configuration of Ablation Experiment \nand Model Training\nAll the experiments are conducted on a deep-learning server. \nThe detailed configuration is shown in Table 2.\n(3)Precision = tp\ntp + fp.\n(4)Recall = tp\ntp + fn.\nOur experiments are trained on the HRSC2016 dataset. \nThe optimizer of YOLF-ShipPnet is AdamW. The momen-\ntum coefficient is 0.9 and the weight decay coefficient is equal \nto 0.05. The original learning rate of the model is 0.0001  . The \nsignificance of weight decay is that the learning rate gradu-\nally reduces during training and converges quickly. Also, a \nthreshold of 72 epochs is set to ensure the convergence of the \nnetwork.\n4.3  Ablation Experiments\nThe YOLF-ShipPnet we propose employs PVTv2 as the back-\nbone, and YOLOX’s HSV is used for random data augmenta-\ntion. To analyze the extent to which the proposed network \nelevates the performance of the model, we design a set of abla-\ntion experiments.\nRetinaNet serves as the baseline object detection framework \nin our experimental setup, acting as a standard of comparison. \nIt is composed of a backbone network and an FPN. The back-\nbone network extracts image features, while the FPN produces \nfeature maps of varying resolutions for further regression and \nclassification. To demonstrate the effectiveness of our back-\nbone network, we compare the performance of PVTv2 with the \nbaseline. For depth effectiveness experiments, we explore the \nefficacy of PVTv2 layer by layer, comparing their mAPs and \ninvestigating the general trend of mAPs with increased depth. \nRandom data augmentation based on HSV is also performed \nand compared with the baseline and the PVTv2 layer with \nthe best performance. Additionally, we assess the fine-grained \nclassification capability of our network and use the baseline \nfor comparison. Finally, the generalization ability of YOLF-\nShipPnet over different datasets is evaluated by replacing the \noriginal dataset with SSDD.\n4.3.1  Effectiveness of PVTv2\nIn this section, we use only PVTv2 as the backbone for the \nbaseline, referred to as PVTv2-B0, to evaluate the effective-\nness of PVTv2. Table 3 below presents the results of the exper-\niment after 72 epochs.\nAccording to the results from Table 3, it can be seen that \nthe feature extraction accuracy reaches52.50% , indicating that \nour baseline is reliable. Compared with the values of AP under \ndifferent categories, the PVTv2 group generally has higher \nAP values than the RetinaNet group, and the mAP is finally \nimproved by 0.41% . Therefore, it can be concluded that PVTv2 \nTable 2  Configuration of parameters\nParameter Configuration\nCentral processing unit (CPU) 12 core Intel(R) \nXeon (R) Platinum \n8255C\nGraphic processing unit (GPU) RTX 2080 Ti\nOperating system Ubuntu 18.04\nProgramming language Python 3.8\nGPU accelerator CUDA 10.2\nTable 3  Ablation experiments \nof PVTv2 on HRSC2016 Method mAP AP50 AP60 AP70 AP80 AP85 AP90 AP95\nRetinaNet 0.5250 0.8380 0.8050 0.7080 0.3990 0.2140 0.1080 0.0200\nPVTv2-B0 0.5291 0.8500 0.8350 0.7240 0.3860 0.2010 0.1030 0.0110\nInternational Journal of Computational Intelligence Systems           (2023) 16:58  \n1 3 Page 9 of 15    58 \neffectively enhances the ability of the feature extraction of the \nYOLF-ShipPnet.\n4.3.2  Depth Effectiveness of PVTv2\nIn this section, PVTv2-B0 is used as the control group. \nWe inherit PVTv2-B0 and modify the weights to obtain \nthe B-series networks based on PVTv2 to prove the depth \neffectiveness of PVTv2. To observe the changes in indica-\ntors, Table 4 lists the effect of B0, B3, and B5: the control \ngroup, the group with moderate effect, and the group with \nthe best effect.\nAccording to Table  4, it can be seen that there is an \nupward trend in the mAP from B0 to B5. The overall detec-\ntion accuracy of the model was improved by 2.27% and \n2.78% in each step, with a total increase of 5.05% . Compar-\ning the average accuracy of each method in Table  4, the \nmean precision level shows an overall upward trend from \nPVTv2-B0 to PVTv2-B5. PVTv2-B5 has the highest mean \naverage precision, which is expected and demonstrates the \ndepth effectiveness of PVTv2.\n4.3.3  Effectiveness of HSV Data Augmentation on Ship \nDataset\nIn this section, we aim to verify the contribution of data \naugmentation to the detection performance of our model. \nBased on the network involved in the above experiments, \nwe only add YOLOXHSVRandom to randomly adjust the hue, \nsaturation, and value of ship images.\nConsidering the inheritance relationship among the \nnetworks, our experiment adds augmentation to the base-\nline only to verify that it can improve the model detection \naccuracy without adding the PVTv2. Then, we add HSV \nstrategy to the PVTv2 B5 to verify that the combination of \ndeepened PVTv2 and data augmentation jointly contribute \nto the model performance.\nTable 5 presents the results of two groups of experiments \nbased on the baseline, with or without data augmentation. \nThe mAP value increases from 52.50 to 53.84% before and \nafter the augmentation, showing a leap of 1.34% in average \nprecision. It indicates that adding data augmentation alone \ncan improve the model effect.\nTable  6 shows the results of the three experimental \ngroups: PVTv2-B0, PVTv2-B5, and PVTv2-B5_Aug. By \ncomparing the results of PVTv2-B5 with and without data \naugmentation, the mAP increases by 0.17% . We can also \nfind that mAPs of PVTv2-B5 and PVTv2-B5_Aug are 5 .05% \nand 5.22% higher than PVTv2-B0. The results demonstrate \nour model’s robustness and show that the augmentation can \nfurther improve the detection performance of the model on \nthe PVTv2, verifying the effectiveness of the data augmenta-\ntion strategy.\n4.3.4  Effectiveness of Fine‑Grained Classification \nExperiment for Ship Dataset\nThe above ablation experiments validate the effectiveness \nof PVTv2-B5_Aug, which is 5.63% more accurate than the \nbaseline.\nIn this section, the baseline and PVTv2-B5_Aug are used \nto detect 31 subclasses of ships to examine the ability of the \nYOLOF-ShipPnet network for fine-grained ship detection. \nTable 7 shows the results of the fine-grained experiments of \nbaseline and PVTv2-B5_Aug on HRSC2016.\nTable 4  Ablation experiments \nof the depth effect of PVTv2 on \nHRSC2016\nMethod mAP AP50 AP60 AP70 AP80 AP85 AP90 AP95\nPVTv2-B0 0.5291 0.8500 0.8350 0.7240 0.3860 0.2010 0.1030 0.0110\nPVTv2-B3 0.5518 0.8600 0.8440 0.7270 0.4940 0.2880 0.0780 0.0100\nPVTv2-B5 0.5796 0.8570 0.8400 0.7370 0.4970 0.2910 0.1120 0.0910\nTable 5  Ablation experiments \nof HSV data augmentation on \nHRSC2016\nMethod Data Aug mAP AP50 AP60 AP70 AP80 AP85 AP90\nRetinaNet 0.5250 0.8380 0.8050 0.7080 0.3990 0.2140 0.1080\nRetinaNet √ 0.5384 0.8440 0.8210 0.7120 0.4400 0.2120 0.1070\nTable 6  Ablation experiments \nof HSV data augmentation with \nincreased depth on HRSC2016\nMethod Data Aug mAP AP50 AP60 AP70 AP80 AP85 AP90\nPVTv2-B0 0.5291 0.8500 0.8350 0.7240 0.3860 0.2010 0.1030\nPVTv2-B5 0.5796 0.8570 0.8400 0.7370 0.4970 0.2910 0.1120\nPVTv2-B5 √ 0.5813 0.8590 0.8470 0.7450 0.5100 0.2890 0.1370\n International Journal of Computational Intelligence Systems           (2023) 16:58 \n1 3   58  Page 10 of 15\nFrom Table  7, it can be known that both the baseline \nand PVTv2-B5_Aug can be used for fine-grained detection. \nPVTv2-B5_Aug performs better on fine-grained detection, \nshowing an enormous leap of 10.03%.\n4.3.5  Performance of YOLF‑ShipPnet on the SSDD Dataset\nIn this section, the dataset is replaced with SSDD to verify \nthe generalization ability of PVTv2-B5_Aug(YOLF-ShipP-\nnet). Table 8 shows the mAP of baseline and PVTv2-B5_Aug \non the SSDD dataset.\nIn comparison to the detection accuracy between the two \ngroups, PVTv2-B5_Aug showed an improvement in per -\nformance of 5.46%. This reflects the strong generalization \nability of our proposed network and indicates its potential \napplication in other datasets.\n4.3.6  Loss Curve for Training\nThe following plots are the training loss of the above abla-\ntion experiments. In these plots, the networks reach conver-\ngence after 72 epochs (Figs. 5, 6, 7).  \n4.4  Visualization of the Result\nWe visualize the results of baseline and PVTv2-B5_Aug on \nHRSC2016 to intuitively compare the detection effect before \nand after the model improvement.\nAs shown in Fig.  8, part (i) shows the visualization \nresults of the baseline and part (ii) demonstrates the results \nof PVTv2-B5_Aug.\nIn Fig. 8, some ships that are not identified with the base-\nline detector are identified by PVTv2-B5_Aug, indicating \nthat PVTv2-B5_Aug shows a better detection performance \nthan baseline.\nIn Fig. 9, the detection precision of PVTv2-B5 is higher \nthan the baseline for the same ship, indicating that PVTv2-\nB5_Aug can identify ships more accurately. From Fig.  10, \nwe discover that the baseline and PVTv2-B5_Aug can detect \nmultiple classes of ships, and PVTv2-B5 performs better in \nterms of identifiability and accuracy.\nAt the same time, PVTv2-B5_Aug on the SSDD dataset \nalso achieves a better detection effect, which verifies the \ngeneralization ability of the model. Figure 11 demonstrates \nthe visualization results, which show the effectiveness of \nPVTv2-B5_Aug on the SSDD dataset.\n4.5  Comparisons Among the Advanced Networks\nTable 9 shows the performance of YOLF-ShipPnet and other \nnetworks on the HRSC2016 dataset. It is observed that the \nmAP of our proposed network shows a significant leap com-\npared to other ship detection models, further verifying the \ndepth effectiveness of the PVTv2 backbone and the excellent \nperformance of the YOLOX’s HSV random data augmenta-\ntion strategy. Then, it is observed that the networks listed can \nonly perform global ship detection. However, our network \nextends the function of fine-grained classification for more \nspecific purpose ship classification.\n5  Conclusion\nThis paper proposes a rotation ship detection network \nYOLF-ShipPnet based on RetinaNet, which innovatively \nintroduces the application of deepened PVTv2 network \nand HSV strategy for data augmentation. Generally, the \nbackbone network utilizes the popular transformer struc-\nture along with the deepened PVTv2 network, which \nfocuses on exploring the depth effectiveness in the context \nof ship image detection. The neck part employs the FPN \nmodel for multi-scale fusion of features. The head part \ntakes in the combined characteristics and performs clas-\nsification and regression of the rotating frame. To further \nimprove the generalization and fine-grained classification \nabilities of our proposed network, we applied the random \ndata augmentation strategy HSV on the ship datasets to \ncomplement the PVTv2 network and achieve a more cohe -\nsive and effective performance. Through a series of valida -\ntion and ablation experiments, it has been confirmed that \nthe YOLF-ShipPnet exhibits promising depth effective-\nness for the detection of ships. Furthermore, the efficacy \nof the HSV data augmentation strategy has been demon-\nstrated, resulting in significantly improved accuracy com-\npared to the baseline model. The use of this strategy also \nmakes the models less sensitive to such external factors \nas color or light changes. In addition, the YOLF-ShipPnet \nhas demonstrated exceptional generalization abilities, \nTable 7  Fine-\ngrained classification \nexperiments on HRSC2016\nMethod Data Aug mAP AP50 AP60 AP70 AP80 AP85 AP90\nRetinaNet 0.1958 0.3140 0.2920 0.2540 0.1690 0.0960 0.0230\nPVTv2-B5 √ 0.2961 0.4330 0.4130 0.3800 0.2800 0.1950 0.0880\nTable 8  Ablation experiments \non the SSDD dataset Method Data Aug mAP\nRetinaNet 0.7151\nPVTv2-B5 √ 0.7697\nInternational Journal of Computational Intelligence Systems           (2023) 16:58  \n1 3 Page 11 of 15    58 \nFig. 5  Loss curve for ablation experiments on HRSC2016\n International Journal of Computational Intelligence Systems           (2023) 16:58 \n1 3   58  Page 12 of 15\nFig. 6  Loss curve for ablation experiments on SSDD\nFig. 7  Loss curve for fine-grained classification experiments on HRSC2016\nFig. 8  Effectiveness of PVTv2-\nB5_Aug on HRSC2016\n\nInternational Journal of Computational Intelligence Systems           (2023) 16:58  \n1 3 Page 13 of 15    58 \nparticularly for fine-grained classification, as verified \nusing the HRSC2016 and SSDD datasets. These results \nsuggest that the proposed network has great potential for \napplications in industrial ship management. Overall, our \nwork highlights the significant strengths of the PVTv2 net-\nwork for enhancement of accuracy in the depth dimension \nand the importance of the HSV data augmentation strategy \nfor improving the generalization capability. The use of this \nnetwork in real-world scenarios may lead to significant \nimprovements in the efficiency and effectiveness of ship \nmanagement systems.\nFig. 9  Higher detection accu-\nracy of PVTv2-B5_Aug on \nHRSC2016\nFig. 10  Effectiveness of \nPVTv2-B5_Aug with fine-\ngrained experiment on \nHRSC2016\nFig. 11  Effectiveness of \nPVTv2-B5_Aug on SSDD\nTable 9  Performance of YOLF-ShipPnet and other networks on \nHRSC2016\nModel Backbone Input_size mAP\nYOLF-ShipPnet PVTv2 800 × 800 0.5813\nIR-Net [19] ResNet 800 × 800 0.5580\nCP [20] Fast R-CNN 800 × 800 0.5570\nKld [33] ResNet50 800 × 512 0.5415\nFR-O [34] VGG-16 1024 × 1024 0.5413\nRetinaNet [35] ResNet50 800 × 512 0.5206\n International Journal of Computational Intelligence Systems           (2023) 16:58 \n1 3   58  Page 14 of 15\n6  Reflection and Future Work\nThe YOLOF-ShipPnet network has shown promising results \nin terms of its depth effectiveness. However, there is still \nroom for improvement in the model's performance by further \ntuning the parameters associated with the number of layers, \nwhich will be investigated in future studies. While FPN has \nbeen used as the neck part of the network, other networks \nsuch as faster R-CNN may offer promising performance in \nship detection tasks due to their robustness and flexibility. \nTherefore, it may be worthwhile to retrain the network using \nfaster R-CNN and compare the results with the previous \nones. Currently, the HSV technique is utilized as a means of \nrandom data augmentation to enhance the network’s ability \nto generalize when presented with ship images that vary in \ncolor or lighting conditions. However, this method is lim-\nited in some circumstances, and other data augmentation \nstrategies should be explored in the future to accommodate \ndifferent application scenarios.\nAcknowledgements All authors acknowledge Northwestern Polytech-\nnical University and Naval Aeronautical and Astronautical University, \nDepartment of Electronic and Information Engineering for providing \nthe experimental data used in this study.\nAuthor Contributions This study was done by the authors' own efforts, \nfrom the formulation of the research idea, to the design of the research \nmethod, the design and execution of the program, the management of \nthe data, the statistical analysis, and the final interpretation. LY con-\nceived the innovative points of the research and was responsible for the \ndesign of ablation and fine-grained classification experiments. ZQ car-\nried out the statistical analysis and played a key role in implementing \nthe experiments. SR performed the modeling and construction of the \nnetwork. All authors participated in drafting the original manuscript. \nLY and ZQ checked and revised the manuscript. These authors con-\ntributed equally to this work and should be considered co-first authors. \nAll authors read and approved the final manuscript.\nFunding Fully self-funded. The author(s) received no financial support \nfor the research, authorship, and/or publication of this article.\nAvailability of Data and Materials Data and material used in the study \nwere obtained from the public database High-Resolution Ship Collec-\ntions 2016 (HRSC2016): ( https:// www. kaggle. com/ datas ets/ guofe ng/ \nhrsc2 016) and SAR Ship Detection Dataset (SSDD) (https:// github.  \ncom/ Tianw enZha ng0825/ Offic ial- SSDD/ blob/ main/ README. md).\nDeclarations \nConflict of Interest It is considered that there is no conflict of interest \nor competing interests and therefore not applicable.\nEthics Approval and Consent to Participate These data have eliminated \npersonal privacy. Therefore, this study is not relevant to personal pri-\nvacy approval.\nConsent for Publication All authors have read and approved the final \nmanuscript (Once the review is completed by both parties, I hereby \nconsent for International Journal of Computational Intelligence Sys-\ntems to public this research).\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Chen, W., Yao, B., Li, Y., Liu, L., Liang, J.: A real-time ship \ndetection system for large-scale optical remote sensing image \non micro-nano satellite. In: 2022 IEEE International Conference \non Real-time Computing and Robotics (RCAR), pp. 450–455 \n(2022)\n 2. Zhang, A., Liao, Y., Liu, S., et al.: Mining the benefits of two-\nstage and one-stage hoi detection. Adv. Neural. Inf. Process. \nSyst. 34, 17209–17220 (2021)\n 3. Diwan, T., Anirudh, G., Tembhurne, J.V.: Object detection using \nYOLO: challenges, architectural successors, datasets and appli-\ncations. Multimedia Tools Appl. 1–33 (2022)\n 4. Cheng, L., Ji, Y., Li, C., et al.: Improved SSD network for fast \nconcealed object detection and recognition in passive terahertz \nsecurity images. Sci. Rep. 12(1), 1–16 (2022)\n 5. Wang, Y., Wang, C., Zhang, H., et al.: Automatic ship detection \nbased on RetinaNet using multi-resolution Gaofen-3 imagery. \nRemote Sens. 11(5), 531 (2019)\n 6. Du, L., Zhang, R., Wang, X.: Overview of two-stage object detec-\ntion algorithms. J. Phys: Conf. Ser. 1544(1), 12033–12039 (2020)\n 7. Li, Z., Li, Y., Yang, Y., et al.: A high-precision detection method \nof hydroponic lettuce seedlings status based on improved Faster \nRCNN. Comput. Electron. Agric. 182, 106054 (2021)\n 8. Xu, Y., Li, D., Xie, Q., et al.: Automatic defect detection and \nsegmentation of tunnel surface using modified Mask R-CNN. \nMeasurement 178, 109316 (2021)\n 9. Gong, Y., Yu, X., Ding, Y., et al.: Effective fusion factor in \nFPN for tiny object detection. In: Proceedings of the IEEE/\nCVF Winter Conference on Applications of Computer Vision, \npp. 1160–1168 (2021)\n 10. Yurong, L., Haining, W., Cunbao, L., et al.: Research progress \nof optical remote sensing image target detection based on deep \nlearning. J. Commun. 43(5), 190–203 (2022)\n 11. Zhang, C., Benz, P., Argaw, D.M., et al.: Resnet or densenet? \nIntroducing dense shortcuts to resnet. In: Proceedings of the \nIEEE/CVF Winter Conference on Applications of Computer \nVision, pp. 3550–3559 (2021)\n 12. Vaswani, A., Shazeer, N., Parmar, N., et al.: Attention is all you \nneed. Adv. Neural Inf. Process. Syst. 30 (2017)\n 13. Bello, I., Zoph, B., Vaswani, A., et al.: Attention augmented \nconvolutional networks. In: Proceedings of the IEEE/CVF Inter -\nnational Conference on Computer Vision, pp. 3286–3295 (2019)\n 14. Dosovitskiy, A., Beyer, L., Kolesnikov, A., et al.: An image \nis worth 16x16 words: Transformers for image recognition at \nscale. Preprint at arXiv: 2010. 11929 (2020)\n 15. Han, K., Xiao, A., Wu, E., et al.: Transformer in transformer. \nAdv. Neural. Inf. Process. Syst. 34, 15908–15919 (2021)\n 16. Wang, W., Xie, E., Li, X., et al.: Pyramid vision transformer: A \nversatile backbone for dense prediction without convolutions. \nInternational Journal of Computational Intelligence Systems           (2023) 16:58  \n1 3 Page 15 of 15    58 \nIn: Proceedings of the IEEE/CVF International Conference on \nComputer Vision, pp. 568–578 (2021)\n 17. Liu, Z., Wang, H., Weng, L., et al.: Ship rotated bounding box \nspace for ship extraction from high-resolution optical satellite \nimages with complex backgrounds. IEEE Geosci. Remote Sens. \nLett. 13(8), 1074–1078 (2016)\n 18. Liu, Z., Hu, J., Weng, L., et al.: Rotated region based CNN \nfor ship detection. In: 2017 IEEE International Conference on \nImage Processing (ICIP). IEEE, pp. 900–904 (2017)\n 19. Liu, Y., Li, H., Cheng, J., Chen, X.: MSCAF-Net: a general \nframework for camouflaged object detection via learning multi-\nscale context-aware features. In: IEEE Transactions on Circuits \nand Systems for Video Technology (2023)\n 20. Sun, M., Xu, L., Luo, R., et al.: GHFormer-Net: Towards more \naccurate small green apple/begonia fruit detection in the night-\ntime. J. King Saud Univ. Comput. Inf. Sci. 34(7), 4421–4432 \n(2022)\n 21. Yu, H., Wu, J.: A Unified Pruning Framework for Vision Trans-\nformers. Preprint at arXiv: 2111. 15127 (2021)\n 22. Yan, Y., Tan, Z., Su, N.: A data augmentation strategy based \non simulated samples for ship detection in RGB remote sensing \nimages. ISPRS Int. J. Geo Inf. 8(6), 276 (2019)\n 23. Zhao, W., et al.: Feature balance for fine-grained object classifica-\ntion in aerial images. IEEE Trans. Geosci. Remote Sens. 60, 1–13 \n(2022)\n 24. Menon, G.S., Murali, S., Elias, J., Aniesrani Delfiya, D.S., et al.: \nExperimental investigations on unglazed photovoltaic-thermal \n(PVT) system using water and nanofluid cooling medium. Renew. \nEnergy 188, 986–996 (2022)\n 25. Ge, Z., Liu, S., Wang, F., et al.: Yolox: Exceeding yolo series in \n2021. Preprint at arXiv: 2107. 08430 (2021)\n 26. Zhou, H., Li, Y., Chen, P., Shen, Y., Zhu, Y.: Improved FPN-based \nship target detection for SAR images in complex scenes. J. Dalian \nMaritime Univ. 1–8 (2022)\n 27. Wang, Z., Xie, X., Yang, J., et al.: Soft focal loss: Evaluating \nsample quality for dense object detection. Neurocomputing 480, \n271–280 (2022)\n 28. Li, Y., Zhou, S., Chen, H.: Attention-based fusion factor in FPN \nfor object detection. Appl. Intell. (2022). https:// doi. org/ 10. 1007/ \ns10489- 022- 03220-0\n 29. Ge, Y., Jialong, Z., Ying, W.: Human body detection and tracking \nalgorithm based on HSV and RGB color space. Autom. Technol. \nAppl. 41(9), 17–2028 (2022). https://  doi. org/ 10. 20033/j. 1003- \n7241. (2022) 09- 0017- 05\n 30. Tellez, D., Litjens, G., Bándi, P., et al.: Quantifying the effects of \ndata augmentation and stain color normalization in convolutional \nneural networks for computational pathology. Med. Image Anal. \n58, 101544–101553 (2019)\n 31. Liu, Z., Yuan, L., Weng, L., Yang, Y.: A high resolution optical \nsatellite image dataset for ship recognition and some new base-\nlines. In: Proceedings of the International Conference on Pat-\ntern Recognition Applications and Methods, vol. 2, pp. 324–331 \n(2017)\n 32. Zhang, T., et al.: Sar ship detection dataset (ssdd): Official release \nand comprehensive data analysis. Remote Sens. 13 (18), 3690 \n(2021)\n 33. Yang, X., et al.: Learning high-precision bounding box for rotated \nobject detection via kullback-leibler divergence. Adv. Neural. Inf. \nProcess. Syst. 34, 18381–18394 (2021)\n 34. Xia, G.-S., et al.: DOTA: A large-scale dataset for object detec-\ntion in aerial images. In: Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition (2018)\n 35. Zhang, S., et al.: Bridging the gap between anchor-based and \nanchor-free detection via adaptive training sample selection. In: \nProceedings of the IEEE/CVF Conference on Computer Vision \nand Pattern Recognition (2020)\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}