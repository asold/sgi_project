{
  "title": "APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models",
  "url": "https://openalex.org/W4389520746",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2127284166",
      "name": "Qifan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2796257948",
      "name": "Yuning Mao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124326937",
      "name": "Jingang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116549248",
      "name": "Hanchao Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2561852407",
      "name": "Shaoliang Nie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166097142",
      "name": "Sinong Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2323056039",
      "name": "Fuli Feng",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2166497444",
      "name": "Lifu Huang",
      "affiliations": [
        "Virginia Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2099425862",
      "name": "Xiaojun Quan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2607121186",
      "name": "Zenglin Xu",
      "affiliations": [
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2109573734",
      "name": "Dong-fang Liu",
      "affiliations": [
        "Rochester Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W3174702398",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3115894062",
    "https://openalex.org/W4299518610",
    "https://openalex.org/W4318751794",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W4221151920",
    "https://openalex.org/W4312651322",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W3206816211",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4226408727",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4288026527",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W3210129272",
    "https://openalex.org/W4385570529",
    "https://openalex.org/W4385570772",
    "https://openalex.org/W3113151582",
    "https://openalex.org/W4221151149",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2970438301",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385567113",
    "https://openalex.org/W4385768107",
    "https://openalex.org/W4385327627",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2804897457"
  ],
  "abstract": "Qifan Wang, Yuning Mao, Jingang Wang, Hanchao Yu, Shaoliang Nie, Sinong Wang, Fuli Feng, Lifu Huang, Xiaojun Quan, Zenglin Xu, Dongfang Liu. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9147–9160\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAPrompt: Attention Prompt Tuning for Efficient Adaptation of\nPre-trained Language Models\nQifan Wang1, Yuning Mao1, Jingang Wang2∗, Hanchao Yu1, Shaoliang Nie1,\nSinong Wang1, Fuli Feng3, Lifu Huang4, Xiaojun Quan5, Zenglin Xu6 and Dongfang Liu7*\n1Meta AI 2Meituan Lab 3University of Science and Technology of China\n4Virginia Tech 5Sun Yat-sen University 6Peng Chen Lab 7Rochester Institute of Technology\nwqfcr@fb.com, yuningm@fb.com\nAbstract\nWith the continuous growth of large language\nmodels, the process of fine-tuning these mod-\nels for new tasks has become increasingly\nparameter-intensive. Prompt tuning, a method\nthat involves tuning a small set of soft prompts,\nhas emerged as an effective and efficient ap-\nproach for adapting large pre-trained language\nmodels. However, most existing prompt tuning\napproaches only introduce prompts at the input\nlayer, limiting their performance and leaving\nlarge rooms for improvement. In this work,\nwe propose a novel Attention Prompt tuning\nmethod, namely APROMPT , for efficient adap-\ntation of pre-trained language models. We first\ndemonstrate that existing prompt tuning can\nbe considered as a special case of attention\nprompt tuning. We then formally introduce\nAPROMPT , which incorporates query, key, and\nvalue prompts into the attention layer to guide\nthe attention computation during fine-tuning.\nExperimental results on the SuperGLUE bench-\nmark consistently demonstrate that our pro-\nposed approach outperforms state-of-the-art\nbaselines and full fine-tuning method with pre-\ntrained models at different scales. In addition,\na comprehensive set of ablation studies validate\nthe effectiveness of the prompt design, as well\nas the efficiency of our approach.\n1 Introduction\nPre-trained Language Models (PLMs) have gained\nsignificant popularity in various natural language\nunderstanding tasks (Devlin et al., 2019; Lewis\net al., 2020; Raffel et al., 2020), exhibiting re-\nmarkable success under the pretrain-then-finetune\nparadigm. It has been consistently demonstrated in\nrecent studies (Aribandi et al., 2022; Zhang et al.,\n2022) that scaling up the size of these models leads\nto improved performance. Consequently, large\nlanguage models such as LLaMA 65B (Touvron\net al., 2023), GPT-3 175B (Brown et al., 2020),\n∗Corresponding authors.\nTunedFrozen\n...\n...\n...\n...\n...\n...\nLN\nL2\nL1\nLN\nL2\nL1\n...\n...\nLN\nL2\nL1\n...\nLN\nL2\nL1\n Fine-Tuning  Prompt Tuning P-Tuning v2 Ours\nAccuracy (%)\n72\n77\n82\n87\n92\n67\nT5-Base (220M) T5-Large (770M) T5-XL (3B)\n Fine-Tuning\n  Prompt Tuning\nP-Tuning v2\nOurs\nFigure 1: Illustration of APROMPT (ours) and previous\nworks, including Fine-Tuning (Aribandi et al., 2022),\nPrompt Tuning (Lester et al., 2021), and P-Tuning V2\n(Liu et al., 2022) methods. Our method consistently\nimproves over prompt tuning methods and also outper-\nforms fine-tuning method across tasks and model scales.\nand PaLM 540B (Chowdhery et al., 2022) are be-\ncoming increasingly prevalent. Despite their com-\npelling performance, fine-tuning large-scale PLMs\nis highly parameter-inefficient due to storing gradi-\nents and updating for all model parameters. This\ninefficiency also arises from the requirement to\nstore and deploy a complete copy of the fine-tuned\nmodel for each individual task, resulting in compu-\ntational expenses that hinder fast model adaptation.\nTo tackle the challenges associated with full\nfine-tuning, researchers have proposed parameter-\nefficient tuning approaches (Guo et al., 2021; He\net al., 2022a; Hu et al., 2022) involving techniques\nsuch as partial tuning and extra module. Partial\ntuning methods (Yosinski et al., 2014) focus on\nfine-tuning only a portion of the backbone, such\nas the classifier head or the last few layers, while\nkeeping the remaining layers frozen. On the other\nhand, extra module methods introduce learnable\nbias terms (Cai et al., 2020) or additional adapters\n(Houlsby et al., 2019) to the network for adapta-\ntion. These strategies operate within the pretrain-\n9147\nthen-finetune paradigm and effectively reduce the\nnumber of learnable parameters. However, in gen-\neral, these approaches tend to underperform the full\nfine-tuning models with large performance gaps.\nRecently, prompt tuning approaches (Lester\net al., 2021; Li and Liang, 2021; He et al., 2022b;\nYang et al., 2023) have been proposed, which uti-\nlize a set of learnable soft prompts prepended to\nthe input. These soft prompts consist of continuous\nembeddings that are updated during the tuning pro-\ncess while keeping the backbone frozen. Prompt\ntuning offers a conceptually simpler and more flex-\nible method compared to other parameter-efficient\ntuning approaches. It has been demonstrated to per-\nform closer to full model tuning, especially with\nlarge-scale PLMs (Ma et al., 2022; Razdaibiedina\net al., 2023a). Prompt tuning provides a promising\nparameter-efficient alternative to fine-tuning, as the\nsoft prompts used in this approach are typically\norders of magnitude smaller, constituting less than\n0.5% of the total model parameters. However, most\nexisting prompt tuning approaches have mainly fo-\ncused on modifying the input layers and have not\nthoroughly explored the core architecture of the\nTransformer’s self-attention mechanism. There-\nfore, they often underperform to full fine-tuning\nand leave substantial room for improvement.\nIn this paper, we present a novel Attention\nPrompt tuning approach, APROMPT , for efficient\nand effective large language model adaptation. We\nbegin by reexamining the prompt tuning approach\nand establish, both theoretically and empirically,\nthat its input prompts can be considered as special-\nized key-value prompts. We then formally intro-\nduce our APROMPT . Unlike previous prompt tun-\ning methods, APROMPT incorporates three sets of\nlearnable prompts: query, key, and value prompts.\nThese prompts are prepended to the respective ma-\ntrices in the self-attention block within the Trans-\nformer layer. During model tuning, these attention\nprompts are learned alongside the original input\nprompts, resulting in more effective guidance of\nattention computation for new tasks. Evaluation on\nthe SuperGLUE benchmark showcases the superior\nperformance of APROMPT compared to state-of-\nthe-art methods. The ablation study results provide\nstrong evidence for the effectiveness and efficiency\nof the proposed attention prompts. We summarize\nthe main contributions as follows:\n• We establish a connection between existing\nprompt tuning methods and our approach,\ndemonstrating that input prompts can be\nviewed as a specialized form of attention\nprompts. This insight serves as valuable\nknowledge, enhancing our understanding of\nboth the existing prompt tuning techniques\nand the novelty of our proposed approach.\n• We design novel attention prompt tuning by\nincorporating query, key, and value prompts\ninto the self-attention computation along with\nthe input prompts. By doing so, these atten-\ntion prompts play a crucial role in effectively\nguiding the model’s fine-tuning process, en-\nabling faster and more accurate adjustments\nduring the adaptation process.\n• We conduct comprehensive experiments on\nvarious tasks in the SuperGLUE benchmark,\ndemonstrating the effectiveness of the pro-\nposed approach over several state-of-the-art\nprompt tuning and full fine-tuning methods.\n2 Related Work\nPre-trained Language Models Pre-trained Lan-\nguage Models (PLMs) (Yang et al., 2019; Ainslie\net al., 2020; Zaheer et al., 2020; Zhao et al., 2023)\nhave demonstrated huge success across various nat-\nural language processing tasks. Pioneering works\nsuch as BERT (Devlin et al., 2019) and RoBERTa\n(Liu et al., 2019) learn contextual representations\nwith masked language model (MLM) and next\nsentence prediction tasks. Recently, a range of\nlarge-scale PLMs, including GPT-3 (Brown et al.,\n2020), T5 (Raffel et al., 2020), and PaLM (Chowd-\nhery et al., 2022), have emerged with diverse pre-\ntraining designs. However, the exponential increase\nin the number of parameters poses challenges in\nfine-tuning these models. It becomes computation-\nally expensive to store and maintain all fine-tuned\nparameters for each tasks.\nParameter-Efficient Tuning As the size of\nPLMs becomes larger, it is increasingly unafford-\nable to update and save full model copies for each\ndownstream application. Parameter-efficient tuning\nmethods (Pfeiffer et al., 2020, 2021) arise in the era\nof LLM. Depending on whether new parameters are\nintroduced, we divide parameter-efficient tuning\nmethods into categories of partial tuning and extra\nmodule. Partial tuning methods simply update parts\nof the model such as the bias term (Zaken et al.,\n2022) or the last layers (Lee et al., 2019). Extra\n9148\nFigure 2: Input prompts P are specialized key and value\nprompts with Pk = HkP and Pv = HvP.\nmodule methods introduce task-specific parameters\nto various locations of the model including Side-\nTuning (Zhang et al., 2020) and Adaptors (Hu et al.,\n2022; Houlsby et al., 2019). There has also been\neffort unifying different parameter-efficient tuning\nmethods for increased robustness and performance\n(Mao et al., 2022; He et al., 2022a).\nPrompt Tuning Prompt tuning (Han et al., 2023;\nYan et al., 2023) inserts learnable parameters to\nthe model as virtual tokens where the insertion can\nhappen at the model input (Lester et al., 2021) or\neach layer (Li and Liang, 2021). Later variants im-\nprove prompt tuning methods for NLU (Liu et al.,\n2022) and NLG (An et al., 2022) tasks respectively\nthrough a series of optimization and adaptations.\nMore recent studies add residual connections to\nimprove the performance and stability of prompt\ntuning (Razdaibiedina et al., 2023b) and extend\nprompt tuning to the continual learning setting\n(Razdaibiedina et al., 2023a). However, most of\nthese methods only simply add prompts to input\nlayers, which greatly limited their performances.\nMost recently, mixture prompt tuning has been\nproposed in MixPrompt (Yang et al., 2023) and\nE2VPT (Han et al., 2023), which combines the in-\nput prompts with key-value prompts. These meth-\nods can be treated as special cases of our attention\nprompt tuning approach.\n3 Prompt Tuning Revisit\n3.1 Preliminary\nPrompt tuning methods (Lester et al., 2021; Liu\net al., 2022) are proposed as a group of parame-\nter efficient models for fast adaptation of large-\nscale PLMs to downstream tasks. They intro-\nduce a set of task-specific prompts or prompt to-\nkens P ∈ Rd×m , and prepend them to the in-\nput sequence X ∈ Rd×n to form a new input\nXnew = [P, X] ∈Rd×(m+n), as shown in left Fig-\nure 2. Here m is the length of prompt tokens, n is\nthe input sequence length, andd is the dimension of\nthe embedding vector. These prompts are learned\non the downstream task during fine-tuning with the\nbackbone frozen. Prompt tuning achieves promis-\nT5-Large WSC CB Boolq\nPrompt Tuning 78.31 88.41 83.65\nFixed Key-value Prompts 78.31 88.41 83.65\nKey-value Prompts 78.88 88.93 84.06\nPrompt Tuning + Key-value Prompts79.12 89.17 84.25\nTable 1: Performance (Accuracy) of Prompt Tuning and\ndifferent key-value prompts variants on WSC, CB and\nBoolq tasks from SuperGLUE with T5-Large model.\ning results compared to other parameter-efficient\ntuning methods.\n3.2 Connection with Key-Value Prompts\nIn this section, we investigate deeper on how the\nprompt tuning works and show that traditional\ninput prompts are equavalent to constrained key-\nvalue prompts. Recall that in prompt tuning, the\nprompt tokens are first prepended to the input to-\nkens. The new sequence Xnew = [P, X] is then\nfed into the Transformer encoder layer to compute\nthe contextual embeddings of the text tokens for\nthe next layer. The self-attention is defined as:\nAttn([P, X]) =softmax(QTKnew√d )Vnew\nQ = HqX, Knew = HkXnew, Vnew = HvXnew\nwhere Q, Knew and Vnew are the new query, key\nand value embedding matrices, with Hq, Hk and\nHv as the pre-trained model parameters that are\nfrozen. It is worth noting that for the query Q,\nthere is no need to compute a new one since only\nthe original text tokens X are updated and used in\nthe next layer. Then we have:\nKnew = HkXnew = [HkP, HkX] = [Pk, K]\nSimilarly, we have Vnew = [Pv, V]. Therefore, we\ncan conclude that adding the prompt tokens P dur-\ning prompt tuning is equivalent of prepending key\nprompts Pk and value prompts Pv to the original\nkey and value matrices respectively, as shown in\nFigure 2. Note that these key-value prompts are\nconstrained or coupled by the input prompts P.\n3.3 Empirical Study\nTo further validate the findings, we conduct an ex-\nperiment by comparing four methods, Prompt Tun-\ning (Lester et al., 2021), Fixed Key-value Prompt,\nKey-value Prompt, and Prompt Tuning + Key-value\nPrompt, on three tasks from SuperGLUE with T5-\nLarge backone. Fixed Key-value Prompt directly\nadds fixed key and value prompts computed from\n9149\nTransformer Encoder Layer\nTransformer Encoder Layer\nTransformer Encoder Layer\nHead\nCLS\n(c) Self-attention Layer(a) Attention Prompt Tuning\nTuned Frozen\n...\n...\n...\nLN\nL2\nL1\nMatMul\nSoftmax\nScale\nMatmul\n...\n...\nQ K V\n...\n...\n...\n ...\nMLP\nLayerNorm\nMSA\nLayerNorm\n(b) Transformer Encoder \nLayer\nFigure 3: Overview of our APROMPT model. We introduce three sets of trainable attention prompts, namely query,\nkey, and value prompts, during the fine-tuning of the model, in addition to the input prompts. These prompts are\nincorporated into the query, key, and value matrices within the multi-head attention computation.\nthe optimal prompts P∗ learned in Prompt Tun-\ning, i.e., Pk = HkP∗and Pv = HvP∗, without\nany tuning. Key-value Prompt learns the optimal\nkey and value prompts during fine-tuning. Prompt\nTuning + Key-value Prompt learns both the input\nprompts and key-value prompts during fine-tuning.\nThe comparative results of these methods are\npresented in Table 1. Firstly, it is clear that the\nFixed Key-value Prompts method achieves identi-\ncal results to Prompt Tuning, which aligns with our\nexpectations and validates the equivalence between\ninput prompts and constrained key-value prompts.\nSecondly, when allowing the key-value prompts to\nbe learned during fine-tuning, we observe improved\nperformance compared to fixed key-value prompts.\nThe reason is that the fixed key-value prompts can\nbe seen as a special case within the search space\nof unconstrained key-value prompts. Lastly, com-\nbining both input prompts and key-value prompts\nduring fine-tuning leads to the highest performance.\nOur hypothesis is that while key-value prompts\ntheoretically have the potential to encompass the\ninformation contained in input prompts, they exist\nin different embedding spaces. In practice, input\nprompts can provide additional value during the\nfine-tuning process. Further analysis and discus-\nsion are provided in the experimental section.\n4 Attention Prompt Tuning\nDrawing inspiration from the insights presented\nin Section 3, we propose an attention prompt tun-\ning approach by introducing the attention prompts\nin the Transformer layers to facilitate attention\ncomputation. The overall model architecture of\nAPROMPT is depicted in Figure 3. Fundamen-\ntally, our model enables the training of only three\ncomponents while keeping all other parameters\nfrozen. These components are as follows: (1) Input\nprompts, denoted asPi, which are inserted at the be-\nginning of the input sequence for each Transformer\nencoder layer. (2) Attention prompts, represented\nby Pq, Pk, and Pv, are incorporated into the query,\nkey, and value matrices within the self-attention\nmodule, respectively. These prompts allow the\nmodel to learn new attention patterns from the fine-\ntuning data. (3) A task-specific head, which is a\nlightweight module dedicated to the specific task\nand can be trained efficiently.\n4.1 Input Prompts\nIn a similar vein to traditional prompt tuning meth-\nods (Lester et al., 2021; Li and Liang, 2021), input\nprompts consist of a set of d-dimensional embed-\nding vectors, where the dimensionality matches\nthat of the text tokens. These prompts are in-\nserted at the beginning of the input sequence in\neach Transformer encoder layer and interact with\nall the text tokens. They facilitate the learning of\ntask-specific embeddings, effectively guiding the\nmodel’s performance on new tasks.\nFormally, these input prompts are defined as Pi\n= {P1\ni , P2\ni , . . . , PN\ni }, where Pj\ni denotes the learn-\nable input prompts in the jth Transformer encoder\nlayer, and N is the total number of layers. Then\nthe encoder layers are represented as:\nZ1 = L1(P1i , E)\n9150\nZj = Lj(Pj\ni , Zj−1) j = 2,3, . . . , N\nwhere Zj represents the contextual embeddings\nof the text tokens computed by the jth encoder\nlayer. The different colors indicate trainable and\nfrozen parameters, respectively. E is the text token\nembeddings initialized from the backbone.\n4.2 Attention Prompts\nWhile input prompts are effective in acquiring\nknowledge about the new task, they do not possess\nthe capability to guide the interaction of informa-\ntion within each encoder layer. During fine-tuning\non a new task with new data, the word distribu-\ntion may differ significantly from the examples\nseen during pre-training. Consequently, it becomes\nimperative to enhance the model’s capacity for cap-\nturing new information from the fine-tuning data.\nThis entails enabling better attention among the\ninput tokens to effectively learn new patterns that\nemerge in the task-specific context.\nFigure 4: The new query, key and value matricies.\nIn order to address this, we introduce a novel\nset of attention prompts that are integrated into the\nattention block within each encoder layer. These at-\ntention prompts can be categorized into two groups:\nquery-key prompts and key-value prompts. The\nquery-key prompts, denoted as PQK\nq and PQK\nk ,\nconsist of small matrices (comprising a few rows)\nthat are appended to the original query and key\nmatrices within the attention module. By incor-\nporating these query-key prompts, we enhance\nthe computation of attention maps among the to-\nkens, thereby improving the attention mechanism.\nThe key-value prompts, represented by PKV\nk and\nPKV\nv , which are two supplementary matrices (a\nfew columns) inserted to the key and value matri-\nces, respectively. These key-value prompts provide\nadditional information for the input tokens to at-\ntend to, thereby enhancing the representation of\nthe learned embeddings. By incorporating both\nquery-key prompts and key-value prompts, we aim\nto enable more effective information interaction\nand capture new patterns during the fine-tuning\nprocess. The new query, key and value matrices\nare augmented with these new attention prompts as\nshown in Figure 4. It is worth noting that the new\nkey matrix is appended by both the key prompts\nfrom query-key and key-value prompts. Then the\nnew attention computations are:\nL(·) =MLP( LN(MSA(·) ) )\nMSA(·) =softmax(QTnewKnew√d )Vnew\nwhere MLP and LN are the frozen multi-layer per-\nceptron and layer norm, andMSA is the multi-head\nself-attention inside the Transformer encoder layer.\nIn this way, the attention prompts can effectively\nguide the model adaptation to the new task.\n4.3 Task-specific Head\nFor each downstream task, we also fine-tune a task-\nspecific head, which is a very small module dedi-\ncated to the specific task to generate the predictions.\ny = Head(ZN)\nwhere ZN is the output contextual embedding from\nthe top layer of the encoder.\n5 Experiments\n5.1 Datasets\nFollowing previous works on prompt tuning (Lester\net al., 2021; Liu et al., 2022), we use NLU tasks\nfrom the SuperGLUE benchmark to evaluate the\nperformance of the language model (Raffel et al.,\n2020; Aribandi et al., 2022). Specifically, we\nuse the following 8 datasets: BoolQ (Clark et al.,\n2019), CB (Jiang and de Marneffe, 2019), COPA\n(Roemmele et al., 2011), MRC (Khashabi et al.,\n2018), ReC (Zhang et al., 2018), RTE (Giampic-\ncolo et al., 2007), WiC (Pilehvar and Camacho-\nCollados, 2019) and WSC (Levesque et al., 2012).\nMore details are provided in the Appendix.\n5.2 Baselines\nOur model is compared with five state-of-the-art\nprompt tuning and fine-tuning methods.\nFine-Tuning (Aribandi et al., 2022) is the stan-\ndard full fine-tuning approach of T5, where all the\npre-trained parameters are fine-tuned.\nPrompt-Tuning (Lester et al., 2021) is the\nvanilla prompt tuning approach which adds the\ninput prompts in the first input layer.\nP-Tuning v2 (Liu et al., 2022) builds on top\nof Prompt-Tuning by inserting a set of individual\nprompts to each Transformer layers.\nXPrompt (Ma et al., 2022) designs more efficient\nprompts by pruning the least important token-level\nand piece-level prompts.\n9151\nMethod Para Boolq CB COPA MRC ReC RTE WiC WSC Average\nAcc F1/Acc Acc F1/EM F1/EM Acc Acc Acc Score\nT5-Base(220M)\nFine-Tuning∗(Aribandi et al., 2022)100% 82.30 91.30 60.00 58.25 80.55 84.50 69.30 81.70 76.10\nPrompt-Tuning (Lester et al., 2021)0.06% 78.12 84.42 54.37 51.14 71.35 75.27 62.29 67.36 68.04\nP-Tuning v2 (Liu et al., 2022)0.53% 80.81 90.23 61.28 55.64 78.13 81.98 67.56 78.32 74.35\nXPrompt (Ma et al., 2022)0.04% 79.67 86.72 56.95 53.08 74.36 78.29 64.31 73.68 70.88\nResPrompt (Razdaibiedina et al., 2023b)0.21% 79.25 85.33 58.64 52.91 73.19 77.14 62.36 70.82 69.95\nAPROMPT(Ours) 0.45% 81.83 91.86 61.54 59.07 81.18 85.76 69.50 81.49 76.84\nT5-Large(770M)\nFine-Tuning∗(Aribandi et al., 2022)100% 88.30 95.35 87.00 67.25 87.85 90.60 73.50 88.50 84.47\nPrompt-Tuning (Lester et al., 2021)0.03% 83.65 88.41 82.67 63.28 82.46 85.19 71.05 78.31 79.25\nP-Tuning v2 (Liu et al., 2022)0.48% 87.92 95.56 86.20 70.47 89.03 89.14 71.81 86.59 84.59\nXPrompt (Ma et al., 2022)0.02% 85.54 91.39 85.05 67.32 85.47 87.30 73.22 80.28 81.95\nResPrompt (Razdaibiedina et al., 2023b)0.15% 83.51 90.64 82.79 65.16 84.72 86.97 71.13 80.36 80.66\nAPROMPT(Ours) 0.37% 90.35 95.83 88.32 71.98 90.64 90.47 74.67 90.13 86.55\nT5-XL(3B)\nFine-Tuning∗(Aribandi et al., 2022)100% 89.60 94.20 96.00 76.15 92.05 91.70 74.30 95.20 88.65\nPrompt-Tuning (Lester et al., 2021)0.01% 87.58 91.25 91.56 73.49 90.14 89.35 74.21 87.16 85.59\nP-Tuning v2 (Liu et al., 2022)0.45% 90.11 94.08 95.33 75.21 92.39 92.13 75.4694.25 88.62\nXPrompt (Ma et al., 2022)0.01% 89.14 92.73 95.18 75.01 91.18 92.16 74.85 89.43 87.46\nResPrompt (Razdaibiedina et al., 2023b)0.04% 88.46 92.54 93.12 75.17 91.20 91.64 75.32 89.15 87.08\nAPROMPT(Ours) 0.32% 90.72 95.48 95.83 78.68 93.75 93.36 76.43 96.17 90.05\nTable 2: Performance comparison result (%) on SuperGLUE development set. ‘∗’ indicates the results reported in\n(Aribandi et al., 2022). ‘Para’ is the number of trainable parameters. The best results are in bold with underline\nrepresenting the second best ones. For tasks with two metrics, the average score is reported. All scores are averaged\nover 5 runs. Results are statistically significant with respect to all baselines on each PLM (all p-value < 0.005).\nResPrompt (Razdaibiedina et al., 2023b) adds\nresidual connections to improve the performance\nand stability of prompt tuning.\n5.3 Implementation Details\nAPROMPT is implemented with the OpenPrompt\nlibrary (Ding et al., 2022), which is a unified and\nextensible toolkit for prompt tuning research. Our\nmodel is trained on 16 NVIDIA Tesla V100 GPUs.\nWe translate each SuperGLUE dataset into a text-\nto-text format following (Raffel et al., 2020). Three\nscales pre-trained models are used: T5-Base, T5-\nLarge and T5-XL with 200M, 770M and 3B pa-\nrameters, respectively. Following previous studies\n(Lester et al., 2021; Ma et al., 2022), we train our\nprompts for 100 epochs with a constant learning\nrate of 0.3 and a batch size of 16. There are three\nhyperparameters in our model: the lengthes of in-\nput, query-key and key-value prompts. For our\nmethod, we set the number of input prompts to\n10 (as we found our model is less sensitive to it),\nand linearly search the best prompt length for both\nquery-key and key-value prompts from {1, 5, 10,\n20, 50}. For all prompt tuning baselines, we search\nthe best input prompt length from {5, 10, 20, 50,\n100}. The best checkpoints are selected via early\nstopping on the development set. The models are\ntrained using the Adafactor (Shazeer and Stern,\n2018) optimizer with weight decay 1e−5.\n5.4 Main Results\nThe main performance comparison results are pre-\nsented in Table 2. There are several key observa-\ntions from these results. First, APROMPT con-\nsistently outperforms all prompt tuning baselines\nacross different backbone models, showcasing the\neffective design of its attention prompts. For in-\nstance, when evaluating on Boolq with T5-Large,\nthe Acc score of APROMPT demonstrates a signifi-\ncant improvement of 5.62% and 2.76% compared\nto two strong methods, XPrompt and P-Tuning\nv2, respectively. This highlights the limitation of\nexisting prompt tuning approaches that primarily\nfocus on designing input prompt tokens, failing\nto capture the intricate token interactions within\nnew data. In contrast, the attention prompts em-\nployed by our approach successfully bridge this\ngap, resulting in enhanced performance. Second,\nAPROMPT outperforms the full fine-tuning method\nin most cases, while other prompt tuning baselines\nstill exhibit certain gaps, particularly when using\nsmaller backbone models like T5-Base. This obser-\nvation highlights the effectiveness of our approach\nacross a range of natural language understanding\ntasks. Moreover, our model achieves these results\nwhile training only around 0.4% of the parame-\n9152\nModel Boolq CB RTE WSC\nFine-Tuning 71.31 75.72 70.35 67.52\nPrompt-Tuning 69.48 75.16 71.73 67.47\nP-Tuning v2 73.36 77.94 73.48 70.50\nXPrompt 71.53 76.57 72.68 69.75\nResPrompt 70.38 76.20 70.85 68.35\nAPROMPT(Ours) 75.66 78.51 75.83 72.30\nTable 3: Performance comparision results with few-\nshot (32 samples) setting on Boolq, CB, RTE and WSC\ntasks for the T5-XL model. APROMPT consistently\noutperforms all baselines in low resource scenarios.\nters in the backbone, making it significantly more\nparameter-efficient than the full fine-tuned model.\nIt is worth noting that although APROMPT intro-\nduces additional attention prompts, the length of\nthe input prompts are largely reduced (fixed to 10)\nand thus resulting in even less total trainable pa-\nrameters compared with P-Tuning v2. Third, it\nis worth noting that the gap between fine-tuning\nand other prompt tuning methods diminishes as\nthe size of the backbone models increases. This\nfinding aligns with previous studies (Lester et al.,\n2021; Ma et al., 2022) and underscores the trend\nof convergence between fine-tuning and alternative\nprompt tuning approaches.\n6 Analysis and Discussion\nResults on Low-resource Scenario We con-\nducted further evaluations to assess the perfor-\nmance of APROMPT and other baseline models\nin a low-resource setting. Following (Schick and\nSchütze, 2021), we randomly selected 32 exam-\nples for each task as the new training set, using\na fixed random seed. We fine-tuned the prompt\nmodel on this limited training set and reported the\nresults on the full dev set using the best checkpoint\nin Table 3. It is evident that all methods experi-\nence a significant drop in performance due to the\nlimited data available for training. Nevertheless,\nAPROMPT consistently outperforms the baseline\nmodels on tasks such as Boolq, CB, RTE, and WSC.\nAdditionally, we observe that most prompt tuning\napproaches achieve better results compared to fine-\ntuning, indicating that despite the challenges of\noverfitting when training with limited data, prompt\ntuning methods exhibit superior generalization ca-\npabilities compared to full fine-tuning.\nImpact of Different Prompts To investigate the\nimpact of different prompts in our model, we\nconducted an ablation study by exploring vari-\nous prompt combinations in APROMPT . Specif-\nT5-Base\n77\n82\n87\n92\nBoolq CB RTE WSC\nw/o input prompt w/o attn prompt w/o KV prompt w/o QK Prompt APrompt\nT5-Large\n86\n88\n90\n92\n94\n96\nBoolq CB RTE WSC\nw/o input prompt w/o attn prompt w/o KV prompt w/o QK Prompt APrompt\nT5-XL\n89\n91\n93\n95\n97\nBoolq CB RTE WSC\nw/o input prompt w/o attn prompt w/o KV prompt w/o QK Prompt APrompt\nFigure 5: Ablation study on the impact of different\nprompt combinations on four tasks of SuperGLUE.\nically, we experimented with four additional mod-\nels: one without input prompts, one without query-\nkey prompts, one without key-value prompts, and\none without both query-key and key-value prompts.\nThe Acc scores obtained on four SuperGLUE tasks\nwith different backbone models are presented in\nFigure 5. The results reveal that the model’s perfor-\nmance drops when any of the trainable prompts is\nremoved, which aligns with our expectations. Fur-\nthermore, we observed that the performance drop\nof APROMPT without input prompts is relatively\nsmall compared to the models without attention\nprompts. This suggests the significance of both\nquery-key and key-value prompts in comparison to\ninput prompts, thereby validating the analysis pre-\nsented in section 3. Once again, it is worth noting\nthat combining all prompts in APROMPT leads to\nthe best performance.\nFigure 6: Ablation study of query-key and key-value\nprompt lengths. We vary the number of prompts for\ndifferent combinations, and evaluate (Acc) on RTE and\nWSC tasks with T5-XL as the backbone model.\nImpact of Prompt Length In APROMPT , the\nlengths of query-key prompts and key-value\n9153\nPosition Boolq CB RTE WSC\nFirst-layer 78.56 85.48 90.28 90.88\nFirst 12-layers 88.35 88.64 92.42 94.32\nLast-layer 75.82 84.37 90.75 91.15\nLast 12-layers 82.55 88.49 92.67 95.36\nAlternative 12-layers90.31 90.14 92.74 95.21\nAPROMPT(All) 90.72 95.48 93.36 96.17\nTable 4: Performance comparison with different prompt\npositions on Boolq, CB, RTE and WSC for T5-XL.\nprompts are the only hyperparameters that require\ntuning. To further analyze the impact of differ-\nent prompt lengths on model performance, we\nconducted an ablation study by modifying both\nprompt lengths across {1, 5, 10, 20, 50}. We exper-\niment over all possible length combinations, and\na detailed discussion on how to balance these two\nprompts will be provided in later experiments. The\nmodel performance results of all prompt length\ncombinations on RTE and WSC are shown in Fig-\nure 6. It can be seen that there is no universal op-\ntimal prompt length that consistently achieves the\nbest performance across both tasks. For instance,\non RTE, the highest score is obtained with 10 key-\nvalue prompts and 5 query-key prompts, while on\nWSC, the best performance is achieved with 20\nkey-value prompts and 10 query-key prompts. We\nhypothesize that different tasks and datasets exhibit\ndistinct data distributions, with ‘hard’ tasks poten-\ntially requiring longer prompts to effectively cap-\nture the underlying patterns and knowledge within\nthe data. However, this comes with the trade-off\nof an increased number of trainable parameters.\nNonetheless, we observed that our model’s perfor-\nmance remains relatively stable within a certain\nrange of prompt lengths.\nImpact of Prompt Positions This study evalu-\nates the impact of prompt positions to the model\nperformance. Concretely, we train five additional\nmodels with different prompt locations (applied\nto both encoder and decoder), including only first\nlayer, last layer, first 12 layers, last 12 layers and\nalternative 12 layers. The performance compari-\nson results on MA VE are reported in Table 4. It\nis not surprising to see that inserting prompts to\nall encoder and decoder layers achieves the best\nperformance. We can also observe that only putting\nthe prompts to the first (input) or last (output) layer\nresults in large performance drops, which is consis-\ntent with the observations in other prompt tuning\nworks (Liu et al., 2022; Jia et al., 2022).\nkey-value prompts ratio\n88\n90\n92\n94\n96\n98\n0.00 0.25 0.50 0.75 1.00\nBoolq CB RTE WSC\nFigure 7: Performance comparison of different ratios of\nthe key-value prompts when fixing the total number of\ntrainable parameters using T5-XL.\nEffect of Attention Prompts Balancing The\nquery-key and key-value prompts in APROMPT\ncontribute differently to the model performance.\nTo further investigate their correlation and ef-\nfectiveness, we conduct an experiment by fix-\ning the total number of trainable parameters, and\nadjusting the ratio of key-value prompts from\n{0, 0.25, 0.5, 0.75, 1}. The model performances\nat different ratios on four SuperGLUE tasks are il-\nlustrated in Figure 7. We observe slightly different\npatterns on different tasks. For example, on WSC,\nkey-value prompts with 0.75 ratio achieves the best\nscore, while key-value prompts with 0.5 gives the\nbest performance on CB. Nevertheless, APROMPT\nwith ratio 0 (no key-value prompts) or 1 (no query-\nkey prompts) underperforms other prompt combi-\nnations in most cases, indicating the effectiveness\nof both attention prompts.\nVariants of APROMPT We compare APROMPT\nwith its two variants to analyze the performance-\nscale trade-off. Firstly, we remove the input\nprompts, retaining only the attention prompts, re-\nsulting in a variant named APROMPT -. Addition-\nally, we apply the pruning technique in XPrompt\n(Ma et al., 2022) to eliminate the least important\nprompts, creating a variant called APROMPT +.\nFrom the results in Table 5, we observe that the\ncontribution of input prompts is not particularly\nsignificant, aligning with the findings in section\n3. When applying prompt trimming, we note that\nwhile the number of trainable parameters decreases,\nthe performance behavior varies across different\ntasks, leaving a room for further exploration in\nterms of finding the optimal balance between the\nnumber of prompts and model performance.\n7 Conclusions\nThis work first connects existing prompt tuning\nwith attention prompt tuning and show that input\n9154\nModel Para Boolq CB RTE WSC\nAPROMPT- 0.25% 90.41 95.28 93.12 95.82\nAPROMPT 0.32% 90.72 95.48 93.36 96.17\nAPROMPT+ 0.21% 91.05 95.34 93.51 96.02\nTable 5: Different variants of our model with T5-XL.\nprompts can be considered as a special case of key-\nvalue prompts in the attention layer. Inspired by\nthe observation, we introduce three sets of new\nprompts, namely query, key, and value prompts,\nand incorporate them into the attention layer to\nguide the attention computation during fine-tuning.\nExperimental results on SuperGLUE demonstrate\nthe superior performance of our model over several\nstate-of-the-art baselines.\nLimitations\nThere are two limitations of our APROMPT model.\nFirst, while APROMPT outperforms other prompt\ntuning and fine-tuning approaches, identifying the\noptimal combination of attention prompts automat-\nically poses a challenge that remains unanswered.\nIn our experiments, we conduct grid search to\nempirically determine the optimal prompt length.\nHowever, in the future, we intend to explore a sys-\ntematic solution that can identify either the optimal\ncombination or a suboptimal one. Second, our cur-\nrent model learns task-specific prompts for each\nindividual task. To address this, we plan to in-\nvestigate a parametric network that can guide the\nlearning of task-agnostic prompts, thereby enhanc-\ning the model’s flexibility and adaptability across\nmultiple tasks.\nReferences\nJoshua Ainslie, Santiago Ontañón, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: encoding long and structured inputs in\ntransformers. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020,\npages 268–284. Association for Computational Lin-\nguistics.\nShengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen,\nQiang Fu, Weizhu Chen, Nanning Zheng, and Jian-\nGuang Lou. 2022. Input-tuning: Adapting unfa-\nmiliar inputs to frozen pretrained models. CoRR,\nabs/2203.03131.\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,\nHuaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-\nglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,\nJai Prakash Gupta, Kai Hui, Sebastian Ruder, and\nDonald Metzler. 2022. Ext5: Towards extreme multi-\ntask scaling for transfer learning. In The Tenth In-\nternational Conference on Learning Representations,\nICLR 2022, Virtual Event, April 25-29, 2022. Open-\nReview.net.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nHan Cai, Chuang Gan, Ligeng Zhu, and Song Han.\n2020. Tinytl: Reduce memory, not parameters for\nefficient on-device learning. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers), pages 2924–2936. Associa-\ntion for Computational Linguistics.\n9155\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen,\nZhiyuan Liu, Haitao Zheng, and Maosong Sun. 2022.\nOpenprompt: An open-source framework for prompt-\nlearning. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2022 - System Demonstrations, Dublin, Ireland,\nMay 22-27, 2022 , pages 105–113. Association for\nComputational Linguistics.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recogniz-\ning textual entailment challenge. In Proceedings of\nthe ACL-PASCAL@ACL 2007 Workshop on Textual\nEntailment and Paraphrasing, Prague, Czech Repub-\nlic, June 28-29, 2007 , pages 1–9. Association for\nComputational Linguistics.\nDemi Guo, Alexander M. Rush, and Yoon Kim. 2021.\nParameter-efficient transfer learning with diff prun-\ning. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n4884–4896. Association for Computational Linguis-\ntics.\nCheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao,\nWenguan Wang, Siyuan Qi, and Dongfang Liu. 2023.\nEˆ2vpt: An effective and efficient approach for visual\nprompt tuning. CoRR, abs/2307.13770.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022a. Towards a\nunified view of parameter-efficient transfer learning.\nIn The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-\n29, 2022. OpenReview.net.\nYun He, Huaixiu Steven Zheng, Yi Tay, Jai Prakash\nGupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuang\nLi, Zhao Chen, Donald Metzler, Heng-Tze Cheng,\nand Ed H. Chi. 2022b. Hyperprompt: Prompt-based\ntask-conditioning of transformers. In International\nConference on Machine Learning, ICML 2022, 17-23\nJuly 2022, Baltimore, Maryland, USA, volume 162 of\nProceedings of Machine Learning Research, pages\n8678–8690. PMLR.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research , pages 2790–2799.\nPMLR.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire\nCardie, Serge J. Belongie, Bharath Hariharan, and\nSer-Nam Lim. 2022. Visual prompt tuning. In Com-\nputer Vision - ECCV 2022 - 17th European Confer-\nence, Tel Aviv, Israel, October 23-27, 2022, Proceed-\nings, Part XXXIII, volume 13693 of Lecture Notes in\nComputer Science, pages 709–727. Springer.\nNanjiang Jiang and Marie-Catherine de Marneffe. 2019.\nEvaluating BERT for natural language inference: A\ncase study on the commitmentbank. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 6085–6090. Association for\nComputational Linguistics.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2018,\nNew Orleans, Louisiana, USA, June 1-6, 2018, Vol-\nume 1 (Long Papers), pages 252–262. Association\nfor Computational Linguistics.\nJaejun Lee, Raphael Tang, and Jimmy Lin. 2019. What\nwould elsa do? freezing layers during transformer\nfine-tuning. CoRR, abs/1911.03090.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021 , pages 3045–\n3059. Association for Computational Linguistics.\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nPrinciples of Knowledge Representation and Rea-\nsoning: Proceedings of the Thirteenth International\nConference, KR 2012, Rome, Italy, June 10-14, 2012.\nAAAI Press.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\n9156\nACL 2020, Online, July 5-10, 2020, pages 7871–7880.\nAssociation for Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pages 4582–\n4597. Association for Computational Linguistics.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022, pages 61–68. As-\nsociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nFang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qifan\nWang, Wei Wu, Xiaojun Quan, and Dawei Song.\n2022. Xprompt: Exploring the extreme of prompt\ntuning. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2022, Abu Dhabi, United Arab Emirates ,\npage 11033–11047. Association for Computational\nLinguistics.\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-\nhairi, Hao Ma, Jiawei Han, Scott Yih, and Madian\nKhabsa. 2022. UniPELT: A unified framework for\nparameter-efficient language model tuning. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 6253–6264, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2021.\nAdapterfusion: Non-destructive task composition for\ntransfer learning. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021 , pages 487–503.\nAssociation for Computational Linguistics.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya\nKamath, Ivan Vulic, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020. Adapterhub: A\nframework for adapting transformers. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demon-\nstrations, EMNLP 2020 - Demos, Online, November\n16-20, 2020, pages 46–54. Association for Computa-\ntional Linguistics.\nMohammad Taher Pilehvar and José Camacho-Collados.\n2019. Wic: the word-in-context dataset for evaluat-\ning context-sensitive meaning representations. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\n2019, Volume 1 (Long and Short Papers), pages 1267–\n1273. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nAnastasia Razdaibiedina, Yuning Mao, Rui Hou, Ma-\ndian Khabsa, Mike Lewis, and Amjad Almahairi.\n2023a. Progressive prompts: Continual learning for\nlanguage models. In The Tenth International Confer-\nence on Learning Representations, ICLR 2023, May\n1-5, 2023. OpenReview.net.\nAnastasia Razdaibiedina, Yuning Mao, Rui Hou, Ma-\ndian Khabsa, Mike Lewis, Jimmy Ba, and Amjad\nAlmahairi. 2023b. Residual prompt tuning: Improv-\ning prompt tuning with residual reparameterization.\nIn Proceedings of the 61th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2023.\nAssociation for Computational Linguistics.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S. Gordon. 2011. Choice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In Logical Formalizations of Commonsense\nReasoning, Papers from the 2011 AAAI Spring Sym-\nposium, Technical Report SS-11-06, Stanford, Cali-\nfornia, USA, March 21-23, 2011. AAAI.\nTimo Schick and Hinrich Schütze. 2021. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11,\n2021, pages 2339–2352. Association for Computa-\ntional Linguistics.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsmäs-\nsan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages\n4603–4611. PMLR.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\n9157\nand Samuel R. Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neu-\nral Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada,\npages 3261–3275.\nLiqi Yan, Cheng Han, Zenglin Xu, Dongfang Liu, and\nQifan Wang. 2023. Prompt learns prompt: Exploring\nknowledge-aware generative prompt collaboration\nfor video captioning. In Proceedings of the Thirty-\nSecond International Joint Conference on Artificial\nIntelligence, IJCAI 2023, 19th-25th August 2023,\nMacao, SAR, China, pages 1622–1630. ijcai.org.\nLi Yang, Qifan Wang, Jingang Wang, Xiaojun Quan,\nFuli Feng, Yu Chen, Madian Khabsa, Sinong Wang,\nZenglin Xu, and Dongfang Liu. 2023. Mixpave: Mix-\nprompt tuning for few-shot product attribute value\nextraction. In Proceedings of the 61th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2023. Association for Computational Linguis-\ntics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 5754–5764.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in Neural Information\nProcessing Systems 27: Annual Conference on Neu-\nral Information Processing Systems 2014, December\n8-13 2014, Montreal, Quebec, Canada, pages 3320–\n3328.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntañón, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020. Big bird: Trans-\nformers for longer sequences. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.\n2022. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), ACL 2022, Dublin, Ireland, May 22-\n27, 2022, pages 1–9. Association for Computational\nLinguistics.\nJeffrey O. Zhang, Alexander Sax, Amir Zamir,\nLeonidas J. Guibas, and Jitendra Malik. 2020. Side-\ntuning: A baseline for network adaptation via addi-\ntive side networks. In Computer Vision - ECCV 2020\n- 16th European Conference, Glasgow, UK, August\n23-28, 2020, Proceedings, Part III, volume 12348 of\nLecture Notes in Computer Science, pages 698–714.\nSpringer.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension. CoRR,\nabs/1810.12885.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\nter, Daniel Simig, Punit Singh Koura, Anjali Srid-\nhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOPT: open pre-trained transformer language mod-\nels. CoRR, abs/2205.01068.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models. CoRR,\nabs/2303.18223.\n9158\nA Dataset Statistics\nTable 6 shows details of the eight datasets from\nSuperGLUE benchmark (Wang et al., 2019) that we\nused for our experiments, along with their training\nsizes and evaluation metrics. Following (Raffel\net al., 2020) and (Lester et al., 2021), for tasks that\nhave two evaluation metrics we use the average of\nboth scores as the final performance metric.\nDataset Examples Task Domain Metric\nBoolq 9,427 QA Wikipedia Acc\nCB 250 NLI various F1/Acc\nCOPA 400 QA blogs, encyclop Acc\nMRC 5,100 QA various F1/EM\nReC 101,000 QA various F1/EM\nRTE 2,500 NLI news, Wiki Acc\nWiC 6,000 WSD lexical databases Acc\nWSC 259 coref. fiction books Acc\nTable 6: The details of 8 SuperGLUE tasks used in our\nexperiments. NLI denotes natural language inference,\nQA denotes questions and answers task, WSD denotes\nword sense disambiguation, EM denotes exact match\nscoring, Acc denotes accuracy.\nB Training Details\nB.1 Settings\nOur model is implemented with the OpenPrompt\nlibrary (Ding et al., 2022), which is a unified and\nextensible toolkit for prompt tuning research. Our\nmodel is trained on 16 NVIDIA Tesla V100 GPUs.\nWe translate each SuperGLUE dataset into a text-\nto-text format following (Raffel et al., 2020). We\ntrain our prompts for 100 epochs with a constant\nlearning rate of 0.3 and a batch size of 16. There are\nthree hyperparameters in our model: the lengthes\nof input, query-key and key-value prompts. For our\nmethod, we set the number of input prompts to 10\nacross all tasks, and linearly search the best prompt\nlength for both query-key and key-value prompts\nfrom {1, 5, 10, 20, 50}. For all prompt tuning\nbaselines, we search the best input prompt length\nfrom {5, 10, 20, 50, 100}. The best checkpoints\nare selected via early stopping on the development\nset. Adafactor (Shazeer and Stern, 2018) optimizer\nis used in model training with weight decay 1e−5.\nB.2 Tokenization and Preprocessing\nFollowing common practice (Lester et al., 2021),\nfor all our experiments, we set the maximum input\nlength (including the input prompt) to 512 tokens.\nWe use padding to maximum length and mask out\nthe padded tokens. In case of input exceeding\n512 tokens, we truncate the input. We do not per-\nform any specific text preprocessing (e.g. remov-\ning punctuation) but instead directly tokenize the\nraw text from SuperGLUE datasets using the cor-\nresponding model tokenizer. For all experiments,\nwe follow T5 (Raffel et al., 2020) formatting. We\nfeed input examples along with their descriptors\n(e.g. ‘sentence1’ and ‘sentence2’), and cast all clas-\nsification tasks into text-to-text format (e.g. 0 and\n1 classes in Boolq task are cast into ‘True’ and\n‘False’) replicating guidelines from T5.\nB.3 Prompt initialization\nIn our experiments, we initialize input prompts us-\ning randomly sampled vocabulary embeddings sim-\nilar to (Lester et al., 2021). We sample uniformly\nacross the whole vocabulary, without limiting to\ntop-k most common tokens. The attention prompts\nare randomly initialized.\nMethods Boolq CB RTE WSC\nFine-Tuning (Aribandi et al., 2022)89.60 94.20 91.70 95.20\nPartial-1 (Yosinski et al., 2014)83.28 84.66 83.92 87.51\nAdapter (Pfeiffer et al., 2020)85.37 86.54 85.75 89.17\nBitFit (Zaken et al., 2022)85.91 90.53 86.34 90.48\nLoRA (Hu et al., 2022)89.48 94.66 91.91 95.82\nAPROMPT(Ours) 90.72 95.48 93.36 96.17\nTable 7: Performance comparison with other non-\nprompt tuning based parameter efficient methods on\nBoolq, CB, RTE and WSC for T5-XL.\nC Comparison with Other Parameter\nEfficient Methods\nTo conduct a full performance evaluation, we fur-\nther conduct comparision of our approach with\nother non-prompt tuning based parameter efficient\nmethods, including Parial tuning (Yosinski et al.,\n2014) (Partial-1 means only fine-tuning the first\nlayer), Adapter (Pfeiffer et al., 2020), BitFit (Za-\nken et al., 2022) and LoRA (Hu et al., 2022). The\nperformance comparision results are reported in\nTable 7. It can be seen that APROMPT outperforms\nall the parameter efficient methods with large mar-\ngins. In fact, existing prompt tuning approaches\nalso achieve better performances compared to these\nbaselines.\nMethodsBoolq CB COPA MRC ReC RTE WiC WSC\nPrompt-Tuning 2h38m 8m 11m 46m 18h25m 57m 51m 8mP-Tuning v2 3h36m 10m 14m 1h15m 20h12m 1h28m 1h14m 12mXPrompt 4h47m 14m 29m 1h53m 28h41m 2h26m 2h19m 19mResPrompt 3h47m 10m 23m 1h21m 22h32m 1h51m 1h34m 12mAPROMPT3h21m 9m 15m 1h7m 21h 1h24m 1h16m 10m\nTable 8: Training time of APROMPT on SuperGLUE.\n9159\nD Training Time\nWe further discuss and report the training time of\ndifferent methods on all the tasks in SuperGLUE in\nTable 8. For Prompt-Tuning, the trainable parame-\nters consist of the prompts designed for the input\nlayer. In the case of P-Tuning V2, the trainable pa-\nrameters encompass the prompts associated with all\nlayers. XPrompt focuses on trainable parameters\nrelated to the pruned prompts, specifically for the\ninput layer, following the pruning process. As for\nResPrompt, the trainable parameters include both\nthe input prompts and the residual network compo-\nnents. The total count of trainable parameters for\neach approach is detailed in Table 2.\n9160",
  "topic": "Adaptation (eye)",
  "concepts": [
    {
      "name": "Adaptation (eye)",
      "score": 0.6267461776733398
    },
    {
      "name": "Computer science",
      "score": 0.6031093597412109
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47304052114486694
    },
    {
      "name": "Natural language processing",
      "score": 0.38968944549560547
    },
    {
      "name": "Psychology",
      "score": 0.2413039207458496
    },
    {
      "name": "Neuroscience",
      "score": 0.1960967779159546
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I859038795",
      "name": "Virginia Tech",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I155173764",
      "name": "Rochester Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 22
}