{
  "title": "Arch-Eval benchmark for assessing chinese architectural domain knowledge in large language models",
  "url": "https://openalex.org/W4409581847",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2102634574",
      "name": "Jie Wu",
      "affiliations": [
        "Guangxi University"
      ]
    },
    {
      "id": null,
      "name": "Mincheng Jiang",
      "affiliations": [
        "Guangxi University"
      ]
    },
    {
      "id": "https://openalex.org/A2791636352",
      "name": "Juntian Fan",
      "affiliations": [
        "Guangxi University"
      ]
    },
    {
      "id": "https://openalex.org/A2099787611",
      "name": "Shimin Li",
      "affiliations": [
        "Guangxi University"
      ]
    },
    {
      "id": "https://openalex.org/A2079380031",
      "name": "Hongtao Xu",
      "affiliations": [
        "Hua Yuan Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2111340394",
      "name": "Ye Zhao",
      "affiliations": [
        "Guangxi University"
      ]
    },
    {
      "id": "https://openalex.org/A2102634574",
      "name": "Jie Wu",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Mincheng Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2791636352",
      "name": "Juntian Fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099787611",
      "name": "Shimin Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2079380031",
      "name": "Hongtao Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111340394",
      "name": "Ye Zhao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3192478068",
    "https://openalex.org/W3170269856",
    "https://openalex.org/W4213425661",
    "https://openalex.org/W4283219089",
    "https://openalex.org/W4388889456",
    "https://openalex.org/W4400699283",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W4376653782",
    "https://openalex.org/W4401042689",
    "https://openalex.org/W4404792954"
  ],
  "abstract": "The burgeoning application of Large Language Models (LLMs) in Natural Language Processing (NLP) has prompted scrutiny of their domain-specific knowledge processing, especially in the construction industry. Despite high demand, there is a scarcity of evaluative studies for LLMs in this area. This paper introduces the \"Arch-Eval\" framework, a comprehensive tool for assessing LLMs across the architectural domain, encompassing design, engineering, and planning knowledge. It employs a standardized dataset with a minimum of 875 questions tested over seven iterations to ensure reliable assessment outcomes. Through experiments using the \"Arch-Eval\" framework on 14 different LLMs, we evaluated two key metrics: Stability (to quantify the consistency of LLM responses under random variations) and Accuracy (the correctness of the information provided by LLMs). The results reveal significant differences in the performance of these models in the domain of architectural knowledge question-answering. Our findings show that the average accuracy difference between Chain-of-Thought (COT) evaluation and Answer-Only (AO) evaluation is less than 3%, but the response time for COT is significantly longer, extending to 26 times that of AO (62.23 seconds per question vs. 2.38 seconds per question). Advancing LLM utility in construction necessitates future research focusing on domain customization, reasoning enhancement, and multimodal interaction improvements.",
  "full_text": "Arch-Eval benchmark for assessing \nchinese architectural domain \nknowledge in large language \nmodels\nJie Wu1, Mincheng Jiang1, Juntian Fan1, Shimin Li1, Hongtao Xu2 & Ye Zhao1\nThe burgeoning application of Large Language Models (LLMs) in Natural Language Processing (NLP) \nhas prompted scrutiny of their domain-specific knowledge processing, especially in the construction \nindustry. Despite high demand, there is a scarcity of evaluative studies for LLMs in this area. This \npaper introduces the “Arch-Eval” framework, a comprehensive tool for assessing LLMs across the \narchitectural domain, encompassing design, engineering, and planning knowledge. It employs a \nstandardized dataset with a minimum of 875 questions tested over seven iterations to ensure reliable \nassessment outcomes. Through experiments using the “Arch-Eval” framework on 14 different LLMs, \nwe evaluated two key metrics: Stability (to quantify the consistency of LLM responses under random \nvariations) and Accuracy (the correctness of the information provided by LLMs). The results reveal \nsignificant differences in the performance of these models in the domain of architectural knowledge \nquestion-answering. Our findings show that the average accuracy difference between Chain-of-\nThought (COT) evaluation and Answer-Only (AO) evaluation is less than 3%, but the response time \nfor COT is significantly longer, extending to 26 times that of AO (62.23 seconds per question vs. 2.38 \nseconds per question). Advancing LLM utility in construction necessitates future research focusing on \ndomain customization, reasoning enhancement, and multimodal interaction improvements.\nKeywords LLMs’ assessment, Construction knowledge, Domain specialization, Answer-only (AO) \nevaluation, Chain-of-thought (COT)\nUse of artificial intelligence has led to a significant transformation in NLP (Natural Language Processing) 1, \nlargely due to the emergence of Large Language Models (LLMs). These models have created new horizons for \nintelligent support in professional fields of research on architecture. Originating with BERT2, LLMs have made \nsignificant strides in diverse language tasks, from text classification to machine translation, by pre-training on \nvast textual datasets to capture nuanced linguistic patterns. The progression has seen models like XLNet 3 and \nRoBERTa4 enhancing comprehension capabilities, culminating in 2023 with GPT-45. This model is notable for its \nmultimodal capabilities, and it can be used to process both images and text, setting a new standard by achieving \nnear-human performance on specialized benchmarks. The developments underscore LLMs’ potential in natural \nlanguage understanding and generation, as well as their expanding role in handling complex, multimodal \nchallenges, signaling a paradigm shift in AI-assisted professional services6.\nDue to the advances in LLM technology, its application in the architectural field has been drawing the \nattention of related professionals, particularly for automated document generation and regulatory text parsing.7,8. \nThe review has summarized the integration of NLP in construction, highlighting its utility in documentation, \nsafety management, Building Information Modeling (BIM) automation, and risk assessment9. The performance \nof NLP tasks was enhanced by the creation of a specialized corpus and the fine-tuning of models 10, providing \ninnovative approaches for document automation and regulatory interpretation. An extensive analysis has been \nconducted on the application of TM/NLP in construction management, highlighting its crucial role in the \nadvancement of construction automation11. Beyond textual analysis, the application of LLMs has extended to \nimage processing and architectural style analysis. The integration of architects’ personal styles and a diverse \nrange of architectural styles into architectural visualization has been successfully achieved through the use of \n1Key Laboratory of Disaster Prevention and Structural Safety of Ministry of Education, Guangxi Key Laboratory \nof Disaster Prevention and Engineering Safety, College of Civil Engineering and Architecture, Guangxi University, \n530004 Nanning, China. 2Hualan Design & Consulting Group, No.39, Huadong Road, 530011 Nanning, China. \nemail: 2310301004@st.gxu.edu.cn; xht@gxhl.com\nOPEN\nScientific Reports |        (2025) 15:13485 1| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports\n\ntext-to-image technology, which has significantly enhanced the efficiency and personalization of the design \nprocess12. Despite these contributions, the application of LLMs in construction remains in its infancy, with \nexisting research not fully addressing the practical deployment challenges. This presents a clear avenue for future \nresearch to enhance the precision and dependability of LLMs in construction and to overcome technical hurdles \nrelated to industry-specific demands.\nDespite the new opportunities for intelligent assistance that LLMs bring to various professional fields, the \ncomplexity and specialized nature of architecture present unique challenges for their application. Traditionally, \narchitectural design and construction processes rely on architects’ profound expertise and practical experience, \nwhich are guided by extensive theoretical knowledge and accumulated practice. The vast and intricate \nknowledge system of architecture, encompassing numerous regulations, standards, and design principles, \nincreases the difficulty for LLMs to understand and apply this knowledge effectively. Therefore, establishing \na framework is crucial for ensuring the effective application of LLMs in architectural research. Utilizing such \na framework facilitates a comprehensive assessment of LLMs’ capabilities in knowledge comprehension and \ninferential reasoning specific to the architectural domain. It enables the identification of the models’ applicative \npotential, the precise identification of their shortcomings, and offers a foundation for informing the trajectory of \nsubsequent model development and educational paradigms.\nRelated work\nExamination of academic databases including Google Scholar, Scopus, and CNKI, indicates a rapid progression in \nthe deployment of LLMs within the discipline of NLP . However, the application of LLMs within the architectural \nsector has yet to be thoroughly investigated. In the NLP domain, a series of established standardized benchmarks \nhas been proposed to serve as critical instruments for the holistic assessment of language models’ competencies \nacross diverse tasks.\nThe GLUE13 benchmark exemplifies this, offering a multi-task assessment platform that evaluates models on a \nrange of Natural Language Understanding (NLU) tasks such as sentiment analysis and natural language inference, \nthereby assessing their linguistic competence. The subsequent iteration, Superglue 14, elevated the benchmark’s \nrigor, fostering model advancement in sophisticated reasoning and interpretative capabilities through the \nintroduction of more arduous tasks. Furthermore, the XNLI 15 benchmark contributed a cohesive evaluative \nframework for cross-linguistic model performance, integrating tasks in 86 languages, which is instrumental for \nthe development of globally adaptive NLP systems. These benchmarks collectively drive the evolution of NLP \nmodels in multi-task learning, intricate reasoning, and multilingual comprehension, underscoring the scientific \nsignificance of the benchmarks and utility in the field.\nIn the domain of Chinese NLP , esteemed evaluation benchmarks such as the CLUE benchmark 16 and the \nC-Eval benchmark17 have been established. The CLUE benchmark encompassed a spectrum of tasks including \ntext categorization and named entity recognition, which served to rigorously evaluate the models’ linguistic \ncomprehension capabilities. The C-Eval expanded upon this by constructing a multi-tiered, interdisciplinary \nframework that assesses a variety of linguistic competencies ranging from foundational to sophisticated tasks, \nsuch as idiomatic expression understanding and comprehensive reading assessment. Hence, it offered a nuanced \nlens through which to scrutinize the performance of LLMs. Despite the comprehensive comparative analysis of \nevaluations provided regarding the models’ processing capabilities within the Chinese linguistic sphere, there \nwas still a discernible gap in the in-depth interrogation of domain-specific knowledge and the assessment of \nmodels’ generalization capabilities in real-world deployment contexts.\nBenchmarks for domain-specific LLM assessment have been introduced sequentially, each tailored to \nevaluate expertise within particular fields. For instance, FinEval18, which targets financial knowledge assessment, \nemploys a multidisciplinary approach through multiple-choice questions that span finance, economics, and \naccounting to effectively gauge LLMs’ domain-specific proficiency. The CBLUE benchmarks 19 concentrated \non language understanding tasks within the biomedical domain, encompassing named entity recognition and \ninformation extraction. Meanwhile, the AGIEval benchmark 20 evaluated model performance on standardized \nhuman-centric tests, such as college entrance exams and professional licensing exams, which are indicative of \ngeneral cognitive abilities.\nThe collective development of these benchmarks has significantly advanced the research and practical \napplication of LLM technology. The methodologies, though insightful, may not be directly transferable to \narchitecture, due to the unique specializations and complex factors involved in the field. Architectural assessment \nnecessitates a multifaceted evaluation that captures LLMs’ capabilities in mathematical computation, design \nconcept interpretation, and historical analysis, among other skills. This calls for bespoke assessment frameworks \nand metrics that are specifically calibrated to the architectural domain’s specific requirements. In conclusion, \ndespite the notable progress of LLMs within the domain of NLP and their incipient impact in select professional \nspheres, their deployment and assessment within the architectural discipline remain in the early phases. \nTherefore, the establishment of an evaluative benchmark specifically for LLMs in architecture is imperative for \nadvancing both the scholarly inquiry and practical integration of LLMs in the field of architectural studies.\nApproach\nOverview\nExisting benchmarks for knowledge assessment of LLMs are notable for their diversity, complexity, and \nstandardization, but they have limitations such as domain-specificity, dataset bias, and selection of assessment \nmetrics. The unique assessment needs of LLMs in the construction domain require us to focus not only on the \ngeneric performance of the models, but also to gain a deeper understanding of their application potential and \nScientific Reports |        (2025) 15:13485 2| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/\npractical effectiveness in specific domains. Therefore, the principles for constructing an LLM assessment system \noriented towards the architectural profession are given as follows: \n 1. Domain specialization and comprehensiveness in architectural assessment : The evaluation framework \nmust be meticulously crafted to address the unique demands and expertise inherent to the architectural do-\nmain, ensuring that the metrics and methodologies employed are intrinsically aligned with the professional \nstandards of architecture. Concurrently, the system should aspire to encompass a holistic view of the critical \nknowledge domains within architectural studies.\n 2. Data set authority: The assessment system must consider the practical use of building models in real con -\nstruction scenarios to ensure that the indicators are meaningful and applicable in actual practice.\n 3. Architectural practice applicability: The assessment system must be closely aligned with the actual appli -\ncation needs of LLM in architectural research, ensuring that the assessment indicators are significant and \neffectively reflect the models’ effectiveness and potential for practical application.\nBased on the requirements of the above three principles, the research methodology can be divided into three \nparts as follows: Knowledge System Construction, Preliminary Experiment, and Formal Experiment (Figure \n1). The construction of the architectural knowledge system provides a basis for the classification of data set \ncollection and organization. The design of these topics often simulates real-world architectural and planning \nproblems, closely related to actual work needs; the Preliminary Experiment is used to confirm the size of the test \nset for a single test and the number of tests for a single LLM, ensuring the coexistence of test accuracy and cost-\neffectiveness; the Formal Experiment phase includes two assessment objectives of ‘stability’ and ‘accuracy’ , using \na variety of assessment methods such as Answer-only (AO) Evaluation and Chain-of-thought (COT) Evaluation \nto assess the comprehensive capabilities of LLM.\nArchitectural knowledge framework for evaluation\nIn this section, a comprehensive architectural knowledge system is proposed with four core domains: \nArchitectural Design and Theory, Structural Engineering and Building Physics, Architectural Regulations, \nEconomic and Business Management, and Urban Planning and Site Design. The system encompasses a broad \nspectrum of architectural disciplines including architectural design, urban and rural planning, structural \nengineering, and environmental science, which can further extend to involve a diverse array of architectural \ntechnologies spanning from antiquity to the modern era. \n (1) Architectural design and theory.  The Architectural Design and Theory area includes the core courses \nin architecture such as architectural style, planning principles, environmental adaptability, and sustaina -\nble design. These courses are essential to the understanding and practice of architectural design, covering \neverything from the aesthetic principles of architectural color and interior design, to the safety require -\nments for building fire prevention and various design codes, to the practical considerations of residential \nand public building design. Such a broad and in-depth knowledge system requires LLMs to possess an \nFig. 1. Technology roadmap.\n \nScientific Reports |        (2025) 15:13485 3| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/\nunderstanding of the deep theoretical foundations of the architectural discipline as well as an accurate grasp \nof the details of practical applications.\n (2) Structural engineering and building physics.  The domain, centered on the structural integrity and en -\nvironmental interaction of buildings, evaluates LLMs’ grasp of structural engineering, materials science, \narchitectural physics, and environmental engineering principles. It aims to assess the LLMs’ acceptability \nto address structural engineering and architectural physics challenges, thereby offering informed decision \nsupport to architectural professionals during the design, construction, and maintenance phases. The mod-\nel’s capabilities are scrutinized in explaining foundational design principles, analyzing seismic resilience \nstrategies, selecting suitable structural systems for diverse building requirements, optimizing daylighting \nand artificial lighting, managing building water and fire safety systems, devising energy-efficient electrical \nsystems, and ensuring the performance of HV AC systems both energy efficiency and occupant comfort. \nAdditionally, it evaluates thermal and acoustic performance control within the built environment.\n (3) Building codes, economics, and management.  The category encompasses legal regulations, professional \nethics, project management, and practical operations within the construction industry. It evaluates com -\npliance, project management proficiency, and hands-on skills in architectural practice. Through the assess-\nment of these representative subjects, a comprehensive understanding of LLMs’ applicability in the realms \nof architectural regulations and economic and business management can be achieved. LLMs are expected \nto accurately interpret the standard processes of construction work execution and acceptance, grasp the \ncomposition and control methods of basic construction costs, understand the key steps in real estate devel-\nopment, adhere to the regulations of construction engineering supervision,have a thorough understanding \nof the relevant laws and regulations governing engineering survey and design, and understand the regu -\nlatory requirements of urban and rural planning. It provides robust support to architectural professionals \nin adhering to legal regulations, conducting economic benefit analyses, and making business management \ndecisions.\n (4) Urban planning and site design.  Urban Planning and Site Design is a pivotal subfield within architec -\nture and urban planning, dedicated to the creation of functional, sustainable, and aesthetic urban spaces. \nThis category encompasses a range of disciplines related to urban development, ecological conservation, \ninfrastructure construction, and site planning, aimed at evaluating LLMs’ comprehensive understanding \nand application of urban planning principles, environmental integration, and site design skills. The key \ndisciplines include urban sociology, urban ecology, and municipal public utilities, which explore the social \nfunctions and ecological impacts of urban spaces, as well as how to improve the quality of life for urban \nresidents through efficient municipal facilities. The assessment provides a thorough understanding of the \nLLMs’ capabilities in urban planning and site design, offering scientific and rational decision support for \nurban planning and site design professionals.\nA comprehensive assessment of LLMs’ competencies in the field of architecture research is facilitated by the \nclassifications mentioned above. Subjects within each category are strategically chosen for their representativeness, \nensuring a wide-ranging and in-depth evaluation (Figure 2). The taxonomy elucidates the model’s strengths \nand weaknesses in different architectural fields and guides the trajectory of future research and development \nendeavors.\nPreliminary experiment\nThe purpose of the preliminary experimental phase is to explore the fundamental performance of LLMs in the \nfield of research on architecture and to determine the appropriate size of the test set. The phase comprises two \nsub-studies:\n• Preliminary experiment ① It aims to assess the performance of the LLM on identical questions. Two involved \ndistinct deployment modes are the locally deployed model of LLM-LLM1 and an online model of LLM2. By \nrandomly selecting a test set (Dataset 1) of 500 questions from the total pool, each models must be queried \nrepeatedly seven times to compare the stability of the responses from these models (i.e., whether the answers \ngiven on multiple occasions are consistent).\n• Preliminary experiment ② The objective is to ascertain the optimal size of the test set to economically eval -\nuate the performance of LLMs. Different test sets with varying sizes are established with increments of 100, \n200, 500, 1000, and 1500 questions, labeled as Dataset 2, Dataset 3, and so on through Dataset 6. To reduce \nrandomness in responses, each set size is replicated seven times (for example, there are seven test sets of 100 \nquestions each, named Dataset 2-1, Dataset 2-2, ..., Dataset 2-7). Subsequently, the sample variance of the \naccuracy rates across these datasets is assessed to determine the efficient and stable test set size.\nMain experiment\nIn the formal experiment, two objectives for evaluating LLMs were defined: stability and accuracy.\n• Stability: It is imperative that the LLM delivers uniform responses to the same query multiple times, signify-\ning a consistent and predictable comprehension of specific inputs by the model. This attribute is essential for \nmitigating discrepancies arising from stochastic elements or model variability, especially within the domain \nof architecture, which demands accurate information. The methodology for examining the criteria involves \nextracting a novel test set, adhering to the optimal size standards established in Preliminary Experiment 2 \n(referred to as Dataset 8-1 through Dataset 8-7). Utilizing the test set, the consistency of each LLM’s respons-\nes is gauged through the sample variance of accuracy rates, thus ascertaining the steadiness of the model’s \nanswer reliability.\nScientific Reports |        (2025) 15:13485 4| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/\n• Accuracy: The precision of information is paramount for reducing the risks stemming from misinformation. \nIn this paper, two distinct questioning methodologies of AO and COT are proposed to assess the LLMs. The \naverage correctness of responses garnered from test sets with diverse content is used as a standard to provide \nan equitable metric for gauging the proficiency of the model’s replies.\nAnswer-only (AO) evaluation\nThe AO Evaluation is designed to evaluate the proficiency of the LLM in directly responding to questions \n(Figure 3), with a specific focus on the accuracy and efficiency of the model in dispensing specialized knowledge \npertaining to architectural design, history, structural engineering, and environmental adaptability. The approach \nexamines the model’s grasp of technical jargon and concepts, as well as its proficiency in providing clear and \nswift answers in practice. The AO Evaluation is valuable in gauging and enhancing the applicability of the model \nwithin the realms of architectural practice and theoretical inquiry. It serves to validate the model’s capability to \ndeliver responses that align with professional benchmarks and to ensure a precise understanding and application \nof architectural concepts and terminology.\nFig. 3. Schematic diagram of AO evaluation.\n \nFig. 2. Arch-Eval framwork.\n \nScientific Reports |        (2025) 15:13485 5| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/\nChain-of-thought (COT) evaluation\nThe COT Evaluation requires that the model not only articulates the answer but also presents the inferential \ntrajectory from question to response (Figure 4), including a comprehensive dissection of architectural design \nprinciples, structural engineering challenges, and environmental adaptability tactics. The evaluation paradigm \ndemonstrates the cognitive processes and reasoning sequences of the model when confronted with intricate \narchitectural inquiries, which can facilitate an assessment of its intellectual capacity and the efficacy of its \nstrategic approaches within the field of architecture.\nBeyond standard experimental protocols, several identified and scrutinized models have been selected to \ntest their superior performance within various research collectives, utilizing the Chain-of-Thought (COT) \nmethodology. A comparative analysis is conducted between the accuracy rates of the COT and Answer-only \n(AO) approaches, with the mean variance in correctness rates between these two graphically represented \nparadigms. To quantitatively evaluate the influence of AO and COT on the LLM’s response efficacy, a paired \nsample t-test is applied to statistically validate the observed discrepancies in mean accuracy.\nResults\nData collection and organization\nThe core of dataset construction is the authority and practicality of its content. Hence, the official examination \nquestions released through public channels for the registered architects and registered planners’ examinations are \nprimarily collected. The question bank includes a collection of actual examination questions from previous years \nand a selection of high-quality mock questions to cover the core knowledge points of professional qualification \ncertification. The entire test set comprises 10,440 questions, all of which are single-choice questions with only \none correct answer among four options. Additionally, paper-based examination papers and practice questions \nfrom universities are also obtained, which have been meticulously compiled by experienced teachers, ensuring \nFig. 4. Schematic diagram of COT evaluation.\n \nScientific Reports |        (2025) 15:13485 6| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/\nthe precision and widespread recognition of the questions and answers. Since these resources are typically not \npublicly available online, they are not easily accessible to web crawlers and provide our dataset with unique value \nand depth.\nTo ensure the scientific rigor and comprehensiveness of the test set, the sampling strategy strictly follows the \ndistribution ratio of questions across various subjects in the entire question bank (Figure 5). This approach is \ndesigned to ensure that the test set fully covers and represents the entire knowledge system, thereby providing \nan accurate and balanced data foundation for assessing the performance of LLMs.\nModels\nTo investigate the latest applications and advancements of LLMs in the Chinese architectural knowledge domain, \na comprehensive evaluation of 14 high-performance LLMs that support Chinese input was conducted (Table 1). \nThe selection of these models was based on their widespread use and high frequency in the Chinese domain, \nas well as the support from professional development and operation teams behind them. This not only ensures \nthe authority and professionalism of the models but also guarantees their potential for future continuous \ndevelopment and optimization.\nPreliminary experimental results\nAll experiments in this paper were conducted in the following configured environment, with the relevant \nconfigurations and their parameters shown in Table 2.\nCreator LLM Parameter Deployment Methods\nOpenAI GPT-3.5-turbo 175B Online\nOpenAI GPT-4-turbo undisclosed Online\nBaichuan Baichuan-13B-Chat 13B Local\nBaichuan Baichuan2-13B-Chat 13B Local\nBaichuan Baichuan2-7B-Chat 7B Local\nTsinghua & Zhipu AI Chatglm2-6B 6B Local\nTsinghua & Zhipu AI Chatglm3-6B 6B Local\nTsinghua & Zhipu AI Chatglm3-6B-32k 6B Local\nShanghai AI Laboratory Internlm-Chat-7B 7B Local\nMeta Llama2-Chinese-13B 13B Local\nAlibaba Cloud Qwen1.5-14B-Chat 14B Local\nAlibaba Cloud Qwen1.5-7B-Chat 7B Local\nAlibaba Cloud Qwen-14B-Chat 14B Local\nAlibaba Cloud Qwen-7B-Chat 7B Local\nTable 1. Large language models participating in the test.\n \nFig. 5. Percentage of questions by subject in the question bank.\n \nScientific Reports |        (2025) 15:13485 7| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/\nPreliminary experiment ①\nPreliminary Experiment ① selected two LLMs, Qwen-14B-Chat and GPT-3.5-turbo, for testing. Qwen-14B is a \nlocally deployed LLM, while GPT-3.5-turbo is an online LLM, representing two different deployment methods \nof LLMs. A test set (Dataset 1) with a random sample of 500 questions was drawn from the total pool. Both \nmodels were asked the same questions for 7 times, using the AO method to streamline the questioning process \nand reduce the time for question and answer. The experimental results (Figure 6) indicate that Qwen-14B-Chat’s \nanswers were completely consistent for the 7 attempts, whereas GPT-3.5-turbo gave different responses. GPT-\n3.5-turbo’s overall output was essentially stable across the 7 responses, with accuracy differences ≤ 0.8%. There \nwas no clear linear relationship between the rounds and accuracy, meaning that the accuracy of GPT-3.5-turbo \ndid not significantly improve during the testing period. The reason might be that the online model GPT-3.5-\nturbo employs unsupervised learning techniques to adjust its parameters, leading to different outputs at different \ntimes. As a locally deployed model, Qwen-14B-Chat likely uses the same environment and parameter settings \neach time it runs, resulting in relatively stable output.\nPreliminary experiment ②\nThe Preliminary Experiment ② aimed to determine an optimal test set size capable of ensuring stable efficiency \nwithout compromising the accuracy and representativeness of the test results. This can streamline the quantity of \nquestions in the test set to save time for subsequent experiments. In this section, we implemented a methodically \nincreased of the test set size, starting with 100 questions and incrementally increasing the number to 200, 500, \n1000, and ultimately reaching 1500 questions, to monitor the variance in the sample distribution.\nThe test results were shown in Figure 7. It was indicated that the sample variance of the model’s accuracy \nstabilizes and remains at a low level when the test set size is approximately 875 questions. Such finding suggested \nthat the influence of test set size on the model’s evaluation outcomes is minimized at the size of 875 questions, \nthus ensuring the stability and reliability of the assessment. Hence, the test size is set to be 875 questions for all \nsubsequent experiments.\nFig. 6. Preliminary Experiment ① Results–Output Stability Test Results for Qwen-14B-Chat and GPT-3.5-\nturbo.\n \nConfiguration Specific Parameters\nCPU 13th Gen Intel(R) Core(TM) i5-13600KF\nGPU NVIDIA GeForce RTX3090 24G\nOperating System Windows-10-10.0.19045-SP0.\nPython Version 3.11.7\nTable 2. Experimental environment configuration.\n \nScientific Reports |        (2025) 15:13485 8| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/\nMain experiment results\nStability assessment of LLMs\nTo assess the stability for LLMs, we utilized an appropriately scaled test set, as determined in the preceding \npreliminary experiments, to evaluate each model. It was achieved by calculating the sample variance of the test \nresults, thereby quantifying the models’ stability. The magnitude of the sample variance was directly correlated \nwith the fluctuation in the LLM’s performance across different test sets: a larger variance indicated greater \nfluctuation in the LLM’s test results, reflecting lower stability. whereas a smaller variance implied more consistent \ntest outcomes, indicating higher stability of LLM.\nAccording to the experimental data in Figure 8, the online mode of GPT-4-turbo exhibited the highest \noverall stability, with a sample variance of 0.000060. Chatglm2-6B and Qwen1.5-7B-Chat were next in line, \nalso demonstrating commendable stability, with sample variances of 0.000068 and 0.000081, respectively. Other \nmodels, including Qwen-7B-Chat and Baichuan2-7B-Chat, also displayed good stability in certain specialized \ndomains.\nThe charts further delineated model stability across different subjects: in the subjects of ‘Design’ and ‘Urban \nPlanning’ , the GPT-3.5-turbo, Qwen-14B-Chat, and Baichuan2-7B-Chat models had relatively low sample \nvariances, suggesting more stable performance. In the subjects of ‘Management’ and ‘Structure’ , the Qwen-7B-\nChat, Baichuan2-7B-Chat, and Qwen1.5-14B-Chat models exhibited lower sample variances, signifying superior \nstability within these disciplines.\nAccuracy assessment of LLMs\nDuring the process of accuracy validation for LLMs, the mean accuracy rates derived from the stability assessment \nwere utilized as the benchmark for evaluation. The latest assessment outcomes indicated that the Qwen1.5-\n14B-Chat model had particularly excelled in terms of accuracy, surpassing the previously leading GPT-4-turbo. \nMoreover, other models in the ‘Qwen’ series had demonstrated remarkable performance (Figure 9).\nIt was particularly noteworthy that in the domain of Building Codes, Economics, and Management, all \nmodels assessed have generally exhibited a high level of accuracy, suggesting that LLMs had already possessed \na relatively mature application potential in the areas. However, the top-ranking models had not achieved the \naverage level of performance in the subject of Design as they have in other areas. Such phenomenon was \nparticularly prominent for the top models. This not only reveals the potential limitations of LLMs in this field \nbut also provides an important reference for future model optimization and fine-tuning training.\nComparison of AO and COT\nIn the study of the predictive guidance methods of LLMs, researchers had paid special attention to the impact \nof prompt variations on obtaining the desired answers. To gain an in-depth understanding of the phenomenon, \nwe selected several models that had shown outstanding performance across different teams to test them using \nthe Chain of Thought (COT) answering approach. Subsequently, we compared the accuracy rates of COT with \nthose of Answer Only (AO). By calculating the difference in the mean accuracy rates between the two answering \nmethods, we visually demonstrated the results’ discrepancy and employed hypothesis testing for an objective \nstatistical validation.\nThe difference in output accuracy rates between COT (Chain of Thought) and AO (Answer Only) was \ndepicted in Figure 10. Among the 5 tested LLMs, AO outperformed COT in 4 of them. For examining the results \nacross different domains, COT only outperforms AO in 5 out of a total of 20 sub-tests. It was indicated that the \nuse of the COT answering method is not necessarily superior to AO, which is in contrast to the widespread belief \nthat COT might be more accurate than AO.\nFig. 7. Preliminary Experiment ② Results–Sample Variance of Accuracy for Qwen-14B-Chat and GPT-3.5-\nturbo Outputs at Different Test Set Sizes.\n \nScientific Reports |        (2025) 15:13485 9| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/\nOtherwise, it was also found that the different answering methods indeed had impacted the LLMs outputs. \nThe difference in the mean accuracy rates between the two answering methods was noted to be smaller than \n3%. However, whether this impact is positive or negative was unclear. To further verify this phenomenon, we \nassumed that there was no significant difference in the accuracy rates of LLMs when answering in AO and \nCOT modes during the evaluation (Figure 11). Paired sample t-tests were performed on the sample data, with \na p-value threshold of 0.05. The test results indicated that, in most cases, there was no statistically significant \ndifference between the results obtained by COT and AO answering methods.\nDuring the experimental procedure, an intriguing observation was noticed that the models required \nsignificantly more time to respond using the COT approach compared to the AO method. Specifically, in the \ncase of the Qwen1.5-14B-Chat model, the mean response time for AO was recorded at 2.38 seconds per question, \nwhereas the COT method extended to 62.23 seconds per question. The revelation underscored the importance \nof incorporating response time as a critical metric alongside accuracy when evaluating the efficiency of LLMs.\nThis study acknowledges the existing challenges in balancing reasoning path length and computational \nefficiency within current COT methodologies. Recent studies have demonstrated the feasibility of streamlining \nCOT processes through approaches such as dynamic pruning of redundant reasoning steps (COT-Influx)21 and \nthe implementation of iterative optimization preference mechanisms 22. Building upon these advancements, \nour future work will focus on developing lightweight reasoning path generation strategies for COT, aiming to \nenhance comprehensive performance in practical applications while maintaining computational efficiency.\nFig. 8. Preliminary Experiment ② Results–Sample Variance of Accuracy for Qwen-14B-Chat and GPT-3.5-\nturbo Outputs at Different Test Set Sizes.\n \nScientific Reports |        (2025) 15:13485 10| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/\nConclusion\nThis research, meticulously crafted through a systematic approach encompassing the synthesis of architectural \nknowledge, preliminary and formal experimentation, has successfully formulated an evaluative framework for \ngauging the efficacy of LLMs within the architectural domain. The conclusions drawn from this study are as \nfollows: \n (1) Initially, we encapsulated the components of the architectural knowledge system, thereby establishing a \nsolid foundation for constructing the test set. Subsequently, based on the results of the preliminary exper -\niments ensuring the stability and reliability of the evaluation, we determined the minimum test set size for \na single evaluation to be 875 questions through seven iterations. In the formal experiments, we established \ntwo key objectives for assessing LLMs: stability and accuracy, which were measured by the sample variance \nand mean of the accuracy rates, respectively.\n (2) Through this process, we conducted an in-depth evaluation of 14 large language models extensively uti -\nlized in the Chinese architectural domain. The study revealed significant differences in the generalization \ncapabilities of language models across various subfields of architecture. Particularly in the areas of Building \nCodes, Economics, and Management, all models demonstrated higher accuracy rates, which may suggest \nthat these models have undergone more in-depth training or optimization in these domains. Conversely, in \nthe field of architectural design and theory, even the higher-performing models exhibited accuracy rates be-\nlow average, implying that even advanced language models require domain-specific fine-tuning to enhance \nprecision.\n (3) Additionally, the experiments also explored the impact of different evaluation methods, including An -\nswer-only (AO) Evaluation and Chain-of-thought (COT) Evaluation, on the model’s output. The results \nindicated that although the COT method can demonstrate the model’s reasoning process, in terms of the \naccuracy of knowledge responses in the architectural domain, there was no significant difference between \nFig. 9. Accuracy test results of outputs from different LLMs.\n \nScientific Reports |        (2025) 15:13485 11| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/\nthe AO and COT methods among the 14 LLMs tested, and the COT method took a considerably longer \ntime.\nThrough in-depth research and exploration, we anticipate more profound applications of LLMs in the architecture \nfield. However, this study has limitations regarding dataset annotation refinement, particularly the incomplete \nclassification labeling of quantitative versus qualitative problems, which may impact the comparative evaluation \nof AO and COT methodologies. Given the hybrid descriptive-numerical nature inherent to architectural \nproblems-where boundaries between conceptual analysis (e.g., design philosophy evaluation) and computational \ntasks (e.g., structural simulation) often blur-future investigations will necessitate collaborative annotation \nFig. 11. T-Test results for AO and COT.\n \nFig. 10. Difference in output accuracy rates between AO and COT for certain LLMs.\n \nScientific Reports |        (2025) 15:13485 12| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/\nframework development with architectural engineers to establish scientifically rigorous classification protocols. \nIn this process, future research should focus on advancing model fine-tuning using domain-specific datasets \nfrom the architecture sector.These efforts will continuously enhance the model’s task comprehension and output \nstability in specialized scenarios such as architectural plan generation, engineering parameter calculation, and \nconstruction progress scheduling.\nIn addition, future work will explore strategies to optimize the COT method to reduce response time while \nimproving accuracy.A hybrid evaluation approach that combines quantitative and qualitative evaluations will \nprovide a more comprehensive perspective on the performance of LLMs. The integration of multimodal data is \nalso noteworthy, as the fusion of visual, spatial, and textual information within LLMs will significantly enhance \nthe model’s ability to handle complex tasks. For example, in design visualization, the combination of images \nwith text or the integration of spatial layouts with engineering parameters in project planning can provide more \nefficient and accurate intelligent support for the architecture field. This increased domain adaptability will enable \nlarge language models to become truly trusted intelligent collaborators for architecture professionals, injecting \ninnovative momentum into the construction industry and ultimately creating greater social value.\nData availability\nThe data that support the findings of this study are available from the corresponding author, [Shimin Li], upon \nreasonable request.\nReceived: 2 January 2025; Accepted: 10 April 2025\nReferences\n 1. Bubeck, S. et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023).\n 2. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. \narXiv preprint arXiv:1810.04805 (2018).\n 3. Y ang, Z. et al. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information \nprocessing systems. 32 (2019).\n 4. Liu, Z., Lin, W ., Shi, Y . & Zhao, J. A robustly optimized bert pre-training approach with post-training. In China National Conference \non Chinese Computational Linguistics. 471–484 (Springer).\n 5. Achiam, J. et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).\n 6. Naveed, H. et al. A comprehensive overview of large language models. arXiv preprint arXiv:2307.06435 (2023).\n 7. Olender, M. L., de la Torre Hernández, J. M., Athanasiou, L. S., Nezami, F . R. & Edelman, E. R. Artificial intelligence to generate \nmedical images: augmenting the cardiologist’s visual clinical workflow. Eur. Heart J. Digit Health. 2, 539–544  h t t p s : / / d o i . o r g / 1 0 . 1 0 \n9 3 / e h j d h / z t a b 0 5 2     (2021).\n 8. Jo, H., Lee, J.-K., Lee, Y .-C. & Choo, S. Generative artificial intelligence and building design: early photorealistic render visualization \nof façades using local identity-trained models. J. Comput. Des. Eng. 11, 85–105 (2024).\n 9. Ding, Y ., Ma, J. & Luo, X. Applications of natural language processing in construction. Autom. Constr. 136, 104169 (2022).\n 10. Zheng, Z., Lu, X.-Z., Chen, K.-Y ., Zhou, Y .-C. & Lin, J.-R. Pretrained domain-specific language model for natural language \nprocessing tasks in the aec domain. Computers in Industry. 142, 103733 (2022).\n 11. Shamshiri, A., Ryu, K. R. & Park, J. Y . Text mining and natural language processing in construction. Autom. Constr. 158, 105200 \n(2024).\n 12. Lee, J.-K., Y oo, Y . & Cha, S.  H. Generative early architectural visualizations: Incorporating architect’s style-trained models. J. \nComput. Des. Eng. https://doi.org/10.1093/jcde/qwae065 (2024).\n 13. Wang, A. et al. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint \narXiv:1804.07461 (2018).\n 14. Sarlin, P .-E., DeTone, D., Malisiewicz, T. & Rabinovich, A. Superglue: Learning feature matching with graph neural networks. In \nProceedings of the IEEE/CVF conference on computer vision and pattern recognition. 4938–4947.\n 15. Conneau, A. et al. Xnli: Evaluating cross-lingual sentence representations. arXiv preprint arXiv:1809.05053 (2018).\n 16. Xu, L. et al. Clue: A chinese language understanding evaluation benchmark. arXiv preprint arXiv:2004.05986 (2020).\n 17. Huang, Y . et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural \nInformation Processing Systems 36 (2024).\n 18. Zhang, L. et al. Fineval: A chinese financial domain knowledge evaluation benchmark for large language models. arXiv preprint \narXiv:2308.09975 (2023).\n 19. Zhang, N. et al. Cblue: A chinese biomedical language understanding evaluation benchmark. arXiv preprint arXiv:2106.08087 \n(2021).\n 20. Zhong, W . et al. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364 (2023).\n 21. Huang, X., Zhang, L. L., Cheng, K.-T., Y ang, F . & Y ang, M. Fewer is more: Boosting llm reasoning with reinforced context pruning. \narXiv preprint arXiv:2312.08901 (2023).\n 22. Pang, R. Y . et al. Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733 (2024).\nAcknowledgements\nThis work was supported by the projects of National Natural Science Foundation of China (NO.51968002) and \nthe Interdisciplinary Scientific Research Foundation of GuangXi University (2022JCC027).\nAuthor contributions\nJ.W . conceived the experiments and administered the project. M.J., H.X., and Y .Z. developed the methodology. \nJ.F ., S.L., and Y .Z. conducted the investigation. M.J. and J.F . developed software. J.W ., M.J., J.F ., and Y .Z. curated \nthe data. M.J. and S.L. visualized data. J.W ., M.J., and S.L. wrote the original draft. J.W ., S.L., and H.X. reviewed \nand edited the manuscript.\nScientific Reports |        (2025) 15:13485 13| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to S.L. or H.X.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:13485 14| https://doi.org/10.1038/s41598-025-98236-0\nwww.nature.com/scientificreports/",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.7968060970306396
    },
    {
      "name": "Computer science",
      "score": 0.6909765005111694
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5260094404220581
    },
    {
      "name": "Natural language processing",
      "score": 0.453371524810791
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40751463174819946
    },
    {
      "name": "Geography",
      "score": 0.17079508304595947
    },
    {
      "name": "Cartography",
      "score": 0.1442316770553589
    },
    {
      "name": "Mathematics",
      "score": 0.07988840341567993
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I150807315",
      "name": "Guangxi University",
      "country": "CN"
    }
  ],
  "cited_by": 1
}