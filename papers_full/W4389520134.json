{
    "title": "Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment",
    "url": "https://openalex.org/W4389520134",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5100340569",
            "name": "Qian Li",
            "affiliations": [
                "Beihang University",
                "Beijing Advanced Sciences and Innovation Center"
            ]
        },
        {
            "id": "https://openalex.org/A5101219893",
            "name": "Cheng Ji",
            "affiliations": [
                "Beihang University",
                "Beijing Advanced Sciences and Innovation Center"
            ]
        },
        {
            "id": "https://openalex.org/A5101293374",
            "name": "Shu Guo",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5104181717",
            "name": "Zhaoji Liang",
            "affiliations": [
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5100443578",
            "name": "Lihong Wang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5072078775",
            "name": "Jianchun Li",
            "affiliations": [
                "Beihang University",
                "Beijing Advanced Sciences and Innovation Center"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3089874281",
        "https://openalex.org/W2890187992",
        "https://openalex.org/W4293518037",
        "https://openalex.org/W4312941859",
        "https://openalex.org/W4294753094",
        "https://openalex.org/W3171266048",
        "https://openalex.org/W4321392589",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4285355278",
        "https://openalex.org/W4312044727",
        "https://openalex.org/W4304099316",
        "https://openalex.org/W2963191264",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W4385572559",
        "https://openalex.org/W2945623882",
        "https://openalex.org/W4313447032",
        "https://openalex.org/W4290944058",
        "https://openalex.org/W4312450628",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2963870853",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W3199346755",
        "https://openalex.org/W4306317476",
        "https://openalex.org/W4386231783",
        "https://openalex.org/W4385573235",
        "https://openalex.org/W3078516413",
        "https://openalex.org/W3098038527",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4386075849",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W4285355576",
        "https://openalex.org/W4367046733",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W4296878971",
        "https://openalex.org/W3093922720"
    ],
    "abstract": "Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify equivalent entity pairs across multi-modal knowledge graphs (MMKGs). However, this task faces challenges due to the presence of different types of information, including neighboring entities, multi-modal attributes, and entity types. Directly incorporating the above information (e.g., concatenation or attention) can lead to an unaligned information space. To address these challenges, we propose a novel MMEA transformer, called Meaformer, that hierarchically introduces neighbor features, multi-modal attributes, and entity types to enhance the alignment task. Taking advantage of the transformer’s ability to better integrate multiple information, we design a hierarchical modifiable self-attention block in a transformer encoder to preserve the unique semantics of different information. Furthermore, we design two entity-type prefix injection methods to redintegrate entity-type information using type prefixes, which help to restrict the global information of entities not present in the MMKGs.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 987–999\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nMulti-Modal Knowledge Graph Transformer Framework for\nMulti-Modal Entity Alignment\nQian Li1,2, Cheng Ji1,2, Shu Guo3, Zhaoji Liang4, Lihong Wang3, Jianxin Li1,2∗\n1School of Computer Science and Engineering, Beihang University, Beijing, China\n2Beijing Advanced Innovation Center for Big Data and Brain Computing, Beijing, China\n3NationalComputerNetworkEmergencyResponseTechnicalTeam/CoordinationCenterofChina\n4School of Computer Science and Technology, University of Chinese Academy of Sciences\n{liqian, jicheng, lijx}@act.buaa.edu.cn, guoshu@cert.org.cn,\nliangzhaoji23@mails.ucas.ac.cn, wlh@isc.org.cn\nAbstract\nMulti-Modal Entity Alignment (MMEA) is a\ncritical task that aims to identify equivalent en-\ntity pairs across multi-modal knowledge graphs\n(MMKGs). However, this task faces challenges\ndue to the presence of different types of infor-\nmation, including neighboring entities, multi-\nmodal attributes, and entity types. Directly in-\ncorporating the above information ( e.g., con-\ncatenation or attention) can lead to an unaligned\ninformation space. To address these challenges,\nwe propose a novel MMEA transformer, called\nMoAlign, that hierarchically introduces neigh-\nbor features, multi-modal attributes, and en-\ntity types to enhance the alignment task. Tak-\ning advantage of the transformer’s ability to\nbetter integrate multiple information, we de-\nsign a hierarchical modifiable self-attention\nblock in a transformer encoder to preserve\nthe unique semantics of different information.\nFurthermore, we design two entity-type prefix\ninjection methods to integrate entity-type in-\nformation using type prefixes, which help to\nrestrict the global information of entities not\npresent in the MMKGs. Our extensive experi-\nments on benchmark datasets demonstrate that\nour approach outperforms strong competitors\nand achieves excellent entity alignment perfor-\nmance.\n1 Introduction\nMulti-modal entity alignment (MMEA) is a chal-\nlenging task that aims to identify equivalent entity\npairs across multiple knowledge graphs that fea-\nture different modalities of attributes, such as text\nand images. To accomplish this task, sophisticated\nmodels are required to effectively leverage informa-\ntion from different modalities and accurately align\nentities. This task is essential for various applica-\ntions, such as cross-lingual information retrieval,\nquestion answering (Antol et al., 2015; Shih et al.,\n2016), and recommendation systems (Sun et al.,\n2020; Xu et al., 2021).\n∗Corresponding author.\nFigure 1: Two examples of the MMEA task, where the\nentity pair in MMKG 1 and MMKG 2 is entity seed and\nthe pair in MMKG 1 and MMKG 3 is not. The blue,\ngreen, and orange circles are entities, textual attributes\nand visual attributes.\nMMEA (Liu et al., 2019; Li et al., 2023b; Liu\net al., 2021; Lin et al., 2022) is challenging due to\nthe heterogeneity of MMKGs (e.g., different neigh-\nbors, multi-modal attributes, distinct types), which\nmakes it difficult to learn rich knowledge repre-\nsentations. Previous approaches such as PoE (Liu\net al., 2019) concatenated all modality features to\ncreate composite entity representations but failed to\ncapture interactions among heterogeneous modal-\nities. More recent works (Chen et al., 2020; Guo\net al., 2021) designed multi-modal fusion mod-\nules to better integrate attributes and entities, but\nstill did not fully exploit the potential interactions\namong modalities. These methods also ignored\ninter-modality dependencies between entity pairs,\nwhich could lead to incorrect alignment. Gen-\nerally speaking, although MMKGs offer rich at-\ntributes and neighboring entities that could be use-\nful for multi-mdoal entity alignment, current meth-\nods have limitations in (i) ignoring the differen-\ntiation and personalization of the aggregation of\nheterogeneous neighbors and modalities leading to\nthe misalignment of cross-modal semantics, and (ii)\nlacking the use of entity heterogeneity resulting in\nthe non-discriminative representations of different\n987\nmeaning/types of entities.\nTherefore, the major challenge of MMEA task\nis how to perform differentiated and personalized\naggregation of heterogeneous information of the\nneighbors, modalities, and types. Although such in-\nformation is beneficial to entity alignment, directly\nfusing will lead to misalignment of the information\nspace, as illustrated in Figure 1. Firstly, notable\ndisparities between different modalities make di-\nrect alignment a challenging task. For example,\nboth the visual attribute of entity Ruby in MMKG1\nand the neighbor information of the entity Ruby\nin MMKG2 contain similar semantics of program-\nming, but data heterogeneity may impede effective\nutilization of this information. Secondly, complex\nrelationships between entities require a thorough\nunderstanding and modeling of contextual informa-\ntion and semantic associations. Entities such as the\nRuby, the Perl, and the entity Larry Wall possess\nunique attributes, and their inter-relationships are\nnon-trivial, necessitating accurate modeling based\non contextual information and semantic associa-\ntions. Furthermore, the existence of multiple mean-\nings for entities further exacerbates the challenge\nof distinguishing between two entities, such as in\nthe case of the Ruby, which has different mean-\nings in the MMKG1 and MMKG3 where it may be\ncategorized as a jewelry entity or a programming\nlanguage entity, respectively.\nTo overcome the aforementioned challenges, we\npropose a novel Multi-Modal Entity Alignment\nTransformer named MoAlign 1. Our framework\nhierarchically introduces neighbor, multimodal at-\ntribute, and entity types to enhance the alignment\ntask. We leverage the transformer architecture,\nwhich is known for its ability to process heteroge-\nneous data, to handle this complex task. Moreover,\nto enable targeted learning on different modalities,\nwe design a hierarchical modifiable self-attention\nblock in the Transformer encoder, which builds\nassociations of task-related intra-modal features\nthrough the layered introduction. Additionally, we\nintroduce positional encoding to model entity rep-\nresentation from both structure and semantics si-\nmultaneously. Furthermore, we integrate entity-\ntype information using an entity-type prefix, which\nhelps to restrict the global information of entities\nthat are not present in the multi-modal knowledge\ngraphs. This prefix enables better filtering out of\n1The source code is available at https://github.com/\nxiaoqian19940510/MoAlign.\nunsuitable candidates and further enriches entity\nrepresentations. To comprehensively evaluate the\neffectiveness of our proposed approach, we design\ntraining objectives for both entity and context eval-\nuation. Our extensive experiments on benchmark\ndatasets demonstrate that our approach outperforms\nstrong competitors and achieves excellent entity\nalignment performance. Our contributions can be\nsummarized as follows.\n• We propose a novel MMEA framework named\nMoAlign, which effectively integrates hetero-\ngeneous information through the multi-modal\nKG Transformer.\n• We design a hierarchical modifiable self-\nattention block to build associations of task-\nrelated intra-modal features through the lay-\nered introduction and design an entity-type\nprefix to further enrich entity representations.\n• Experimental results indicate that the frame-\nwork achieves state-of-the-art performance\non the public multi-modal entity alignment\ndatasets.\n2 Preliminaries\nMulti-Modal Knowledge Graph. A multi-\nmodal knowledge graph (MKG) is represented by\nfour sets: entities (E), relations (R), multi-modal\nattributes (A), and triplets (T). The size of each set\nis denoted by NE, NR, and NA. The multi-modal\nattributes are divided into text ( AT) and image\n(AI) attributes. The relation set Rincludes entity\nrelations (RE), text attribute relations (RT), and\nimage attribute relations (RI). The set of triplets\nTincludes entity triplets, text attribute triplets, and\nimage attribute triplets.\nMulti-Modal Entity Alignment Task. Multi-\nmodal entity alignment (Chen et al., 2020; Guo\net al., 2021; Liu et al., 2021; Chen et al., 2022) aims\nto determine if two entities from different multi-\nmodal knowledge graphs refer to the same real-\nworld entity. This involves calculating the similar-\nity between pairs of entities, known as alignment\nseeds. The goal is to learn entity representations\nfrom two multi-modal knowledge graphs (MKG1\nand MKG2) and calculate the similarity between\na pair of entity alignment seeds (e,e′) taken from\nthese KGs. The set of entity alignment seeds is\ndenoted as S= {(e,e′) |e∈E,e′∈E′,e ≡e′}.\n988\nFigure 2: The framework of the Multi-Modal KG Transformer, MoAlign. The hierarchical modifiable self-attention\nblock learns the entity by hierarchically integrating multi-modal attributes and neighbors. The prefix-injected\nself-attention mechanism introduces entity type information for alignment.\n3 Framework\nThis section introduces our proposed framework\nMoAlign. As shown in Figure 2, we introduce posi-\ntional encoding to simultaneously model entity rep-\nresentation from both modality and structure. To\nhierarchically introduce neighbor and multi-modal\nattributes, we design a hierarchical modifiable self-\nattention block. This block builds associations\nof task-related intra-modal features through the\nlayered introduction. Furthermore, for integrating\nentity-type information, we design a prefix-injected\nself-attention mechanism, which helps to restrict\nthe global information of entities not present in the\nMMKGs. Additionally, MoAlign also design train-\ning objectives for both entity and context evaluation\nto comprehensively assess the effectiveness.\n3.1 Multi-Modal Input Embedding\n3.1.1 Multi-Modality Initialization\nThe textual attribute is initialized by BERT (Devlin\net al., 2019). For the visual attributes, to enable di-\nrect processing of images by a standard transformer,\nthe image is split into a sequence of patches (Doso-\nvitskiy et al., 2021). We then perform a flatten oper-\nation to convert the matrix into a one-dimensional\nvector similar to word embeddings, which is sim-\nilar to embeddings in BERT (Devlin et al., 2019)\nand concatenate them to form the image embedding\nvector, denoted as vv. However, the initialization\nof word embeddings follows a specific distribu-\ntion, whereas the distribution of image embeddings\ngenerated using this method is not consistent with\nthe pretraining model’s initial distribution. There-\nfore, to standardize the distribution of image em-\nbeddings to be consistent with the distribution of\nthe vocabulary used by the pretraining model, we\nperform the following operation:\nvv ←(vv −mean(vv))/std(vv) ·λ, (1)\nwhere mean(vv) and std(vv) are the mean and\nstandard deviation of vv, respectively, and λis the\nstandard deviation of truncated normal function.\n3.1.2 Positional Encoding\nFor the transformer input, we additionally input\ntwo types of positional encoding to maintain struc-\nture information of multi-modal KG as follows.\nModality Positional Encoding. To enable the\nmodel to effectively distinguish between entities,\ntextual attributes, image attributes, and introduced\nentity types, we incorporate a unique position code\nfor each modality. These position codes are then\npassed through the encoding layers of the model to\nenable it to differentiate between different modali-\nties more accurately and learn their respective fea-\ntures more effectively. Specifically, we assign po-\nsition codes of 1, 2, 3, and 4 to entities, textual\nattributes, image attributes, and introduced entity\n989\ntypes, respectively. By incorporating this modality\npositional encoding information into the encoding\nprocess, our proposed model is able to incorporate\nmodality-specific features into the alignment task.\nStructure Positional Encoding. To capture the\npositional information of neighbor nodes, we intro-\nduce a structure positional encoding that assigns a\nunique position code to each neighbor. This allows\nthe model to distinguish between them and effec-\ntively incorporate the structure information of the\nknowledge graph into alignment process. Specifi-\ncally, we assign a position code of 1 to the current\nentity and its attributes. For first-order neighbors,\nwe randomly initialize a reference order and use\nit to assign position codes to each neighbor and\nits corresponding relation as 2n and 2n+ 1, re-\nspectively. Additionally, we assign the same struc-\nture positional encoding of attributes to their cor-\nresponding entities. By doing so, the model can\ndifferentiate between different neighbor nodes and\neffectively capture their positional information.\nTo fully utilize the available information,\nwe extract relation triplets and multi-modal\nattribute triplets for each entity. The entity is\nformed by combining multi-modal sequences as\n{e,(e1,r1),..., (en,rn),(a1,v1),..., (am,vm),eT },\nwhere (ei,ri) represents the i-th neighbor of\nentity e and its relation. (aj,vj) represents the\nj-th attribute of entity e and its value vj, and it\ncontains textual and visual attributes. nand mare\nthe numbers of neighbors and attributes. eT is the\ntype embeddding.\n3.2 Hierarchical Modifiable Self-Attention\nTo better capture the dependencies and relation-\nships of entities and multi-modal attributes, we\nincorporate cross-modal alignment knowledge into\nthe transformer. Specifically, a hierarchical mod-\nifiable self-attention block is proposed to better\ncapture complex interactions between modalities\nand contextual information. The block focuses on\nassociations of inter-modal (between modalities,\nsuch as textual and visual features) by cross-modal\nattention. By doing so, the model can capture more\ninformative and discriminative features.\n3.2.1 Hierarchical Multi-Head Attention\nWe propose a novel hierarchical block that incor-\nporates distinct attention mechanisms for different\ninputs, which facilitates selective attention toward\nvarious modalities based on their significance in the\nalignment task. The transformer encoder comprises\nthree separate attention mechanisms, namely neigh-\nbor attention, textual attention, and visual attention.\nWe utilize different sets of queries, keys, and values\nto learn diverse types of correlations between the\ninput entity, its attributes, and neighbors.\nNeighbor Multi-Head Attention. More specif-\nically, in the first layer of the hierarchical block\nof the l-th transformer layer, we employ the input\nentity as queries and its neighbors as keys and val-\nues to learn relations between the entity and its\nneighbors:\ne(l.1) = MH-Attn(e,ei,ei), (2)\nwhere ei is the neighbor of entity eand e(l.1) is the\nrepresentation of entity ein the first layer.\nTextual Multi-Head Attention. In the second\nlayer of the hierarchical block, the input entity\nand its textual attributes are used as queries and\nkeys/values to learn the correlations between the\nentity and its textual attributes.\ne(l.2) = MH-Attn(e(l.1),[at; vt],[at; vt]), (3)\nwhere (at,vt) represents the textual attribute of\nentity eand its value, ande(l.2) is the representation\nof entity ein the second layer.\nVisual Multi-Head Attention. In the third layer\nof the hierarchical block, the input entity and its\nvisual attributes are used similarly to the second\nlayer, to learn the correlations between the entity\nand its visual attributes.\ne(l.3) = MH-Attn(e(l.2),[av; vv],[av; vv]), (4)\nwhere (av,vv) represents the visual attribute of\nentity eand its value, and e(l.3) is the representa-\ntion of entity ein the third layer. By incorporating\nneighbor attention, textual attribute attention, and\nvisual attribute attention, our model can capture\nvarious correlations between the input entity, its at-\ntributes, and neighbors in a more effective manner.\n3.2.2 Modifiable Self-Attention\nTo learn a specific attention matrix for building\ncorrelations among entities and attributes, we de-\nsign a modifiable self-attention mechanism. We\nmanage to automatically and adaptively generate\nan information fusion mask matrix based on se-\nquence features. The length of the sequence and\nthe types of information it contains may vary de-\npending on the entity, as some entities may lack\ncertain attribute information and images.\n990\nModifiable Self-Attention Mechanism. To\nadapt to the characteristics of different sequences,\nit is necessary to assign \"labels\" to various\ninformation when generating the sequence, such as\nusing [E] to represent entities and [R] to represent\nrelations. This way, the positions of various\ninformation in the sequence can be generated, and\nthe mask matrix can be generated accordingly.\nThese labels need to be processed by the model’s\ntokenizer after inputting the model and generating\nthe mask matrix to avoid affecting the subsequent\ngeneration of embeddings. We can still modify\nthe vocabulary to allow the tokenizer to recognize\nthese words and remove them. The mask matrix\ncan be generated based on the positions of various\ninformation, represented by labels, as follows:\nMij =\n{\n1, if(i,j, ·)∈T or T(i) =T(j)\n0, otherwise , (5)\nwhere T(·) is the type mapping function of entities\nand attributes.\nSubsequently, residual connections and layer\nnormalization are utilized to merge the output of\nhierarchical modifiable self-attention el and apply\na position-wise feed-forward network to each ele-\nment in the output sequence of the self-attention.\nThe output sequence, residual connections, and\nlayer normalization are employed as:\ne(l) = LayerNorm\n(\ne(l−1) + FFN(e(l))\n)\n, (6)\nwhere el is the output of the hierarchical modifiable\nself-attention in the l-th layer of the transformer.\nThe use of residual connections and layer normal-\nization helps stabilize training and improve perfor-\nmance. It enables our model to better understand\nand attend to different modalities , leading to more\naccurate alignment results.\n3.3 Entity-Type Prefix Injection\nPrefix methods can help improve alignment accu-\nracy by providing targeted prompts to the model to\nimprove generalization, including entity type infor-\nmation (Liu et al., 2023). The entity-type injection\nintroduces a type of information that takes as in-\nput a pair of aligned seed entities from different\nMMKGs and uses entities as prompts. The goal is\nto inject entity-type information into the multi-head\nattention and feed-forward neural networks.\nPrefix-Injected Self-Attention Mechanism.\nSpecifically, we create two sets of prefix vectors\nfor entities, pk,pv ∈Rnt×d, for keys and values\nseparately, where nt is the number of entity types.\nThese prefix vectors are then concatenated with\nthe original key K and value V. The multi-head\nattention in Eq.(2) is performed on the newly\nformed prefixed keys and values as follows:\nMH-Attn(Q,K,V)=Concat(h1,···,hN)Wo,\nhi=Pi-Attn\n(\nQ Wq\ni ,[K Wk\ni ; pk],[VWv\ni ; pv]\n)\n,\n(7)\nwhere dN is typically set to d/N, and N is number\nof head. hi denotes i-th output of self-attention in\nl-th layer. Wo ∈Rd×d,Wq\ni ,Wk\ni ,Wv\ni ∈Rd×dN\nare learnable parameters. In this paper, we use the\ntype embedding eT as the prefix.\nIn order to effectively incorporate alignment in-\nformation, we utilize the mask token to influence at-\ntention weights. The mask token serves as a means\nto capture effective neighbors and reduce computa-\ntional complexity. To achieve this, we set attention\nweights mto a large negative value based on the\nmask matrix Mij in the modifiable self-attention\nmechanism. Consequently, attention weights for\nthe mask token are modified as follows:\nPi-Attn(Q,Kt,Vt)=Softmax\n(QKT\nt+m√dk\n)\nVt, (8)\nwhere Q,Kt, and Vt represent the query of the\nentity, the key containing entity type, and the value\ncontaining entity type matrices, respectively. dk\nrefers to the dimensionality of key vectors, while\nmrepresents the attention weights.\nPrefix-Injected Feed Forward Network. Re-\ncent research suggests that the feed-forward layers\nwithin the transformer architecture store factual\nknowledge and can be regarded as unnormalized\nkey-value memories (Yao et al., 2022). Inspired\nby this, we attempt to inject entity type into each\nfeed-forward layer to enhance the model’s ability\nto capture the specific information related to the en-\ntity type. Similar to prefix injection in attention, we\nfirst repeat the entity type eT to create two sets of\nvectors, Φk,Φv ∈Rnt×d. These vectors are then\nconcatenated with the original parameter matrices\nof the first and second linear layers.\nFFN(E) =f\n(\nE ·[Wk\nf ; Φk]\n)\n·[Wv\nf ; Φv], (9)\nwhere f denotes the non-linearity function, E rep-\nresents the output of the hierarchical modifiable\nself-attention. Wk\nf ,Wv\nf are the parameters.\n991\n3.4 Training Objective\nIn order to effectively evaluate the MMEA from\nmultiple perspectives, we propose a training objec-\ntive function that incorporates both aligned entity\nsimilarity and context similarity. This objective\nfunction serves to assess the quality of entity align-\nment and takes into consideration both the entities\nthemselves and their surrounding context.\nAligned Entity Similarity. The aligned entity\nsimilarity constraint loss is designed to measure\nsimilarity between aligned entity pairs and ensure\nthat embeddings of these entities are close to each\nother in the embedding space.\nLEA =sim(e,e′)−sim(e,e′)−sim(e,e′), (10)\nwhere (e,e′) represent the final embeddings of\nthe aligned seed entities (e,e′) from knowledge\ngraphs KG1 and KG2. e and e′denote the nega-\ntive samples of the seed entities. sim(·,·) refers to\nthe cosine distance between the embeddings.\nContext Similarity. The context similarity loss is\nemployed to ensure that the context representations\nof aligned entities are close to each other in the\nembedding space.\nLCon =sim(o,o′)−sim(o,o′)−sim (o,o′), (11)\nwhere (o,o′) denote the final context representa-\ntions ([cls] embedding) of the aligned seed entities\n(e,e′). o and o′represent the negative samples of\nthe seed entities’ context.\nThe total alignment loss Lis computed as:\nL= αLEA + βLCon, (12)\nwhere α,β are learnable hyper-parameters.\n4 Experiment\n4.1 Dataset\nWe have conducted experiments on two of the\nmost popular datasets, namely FB15K-DB15K and\nFB15K-YAGO15K, as described in (Liu et al.,\n2019). The FB15K-DB15K dataset 2 is an en-\ntity alignment dataset of FB15K and DB15K\nMMKGs, while the latter is a dataset of FB15K\nand YAGO15K MMKGs. Consistent with prior\nworks (Chen et al., 2020; Guo et al., 2021), we\nsplit each dataset into training and testing sets in\nproportions of 2:8, 5:5, and 8:2, respectively. The\nMRR, Hits@1, and Hits@10 are reported for eval-\nuation on different proportions of alignment seeds.\n2https://github.com/mniepert/mmkb\n4.2 Comparision Methods\nWe compare our method with three EA baselines\n(TransE (Bordes et al., 2013), GCN-align (Wang\net al., 2018a), and AttrGNN (Liu et al., 2020b)),\nwhich aggregate text attributes and relation infor-\nmation, and introduced image attributes initialized\nby VGG16 for entity representation using the same\naggregation method as for text attributes. We fur-\nther use three transformer models ( BERT (De-\nvlin et al., 2019), ViT (Dosovitskiy et al., 2021),\nand CLIP (Radford et al., 2021)) to incorporate\nmulti-modal information, and three MMEA meth-\nods (PoE (Liu et al., 2019), Chen et al. (Chen\net al., 2020), HEA (Guo et al., 2021), EV A(Liu\net al., 2021), and ACK-MMEA (Li et al., 2023a))\nto focusing on utilizing multi-modal attributes. The\ndetail is given in Appendix B.\n4.3 Implementation Details\nWe utilized the optimal hyperparameters reported\nin the literature for all baseline models. Our\nmodel was implemented using PyTorch, an open-\nsource deep learning framework. We initialized\ntext and image attributes using bert-base-uncased3\nand VGG164, respectively. To ensure fairness, all\nbaselines were trained on the same data set parti-\ntion. The best random dropping rate is 0.35, and\ncoefficients α,β were set to 5 and 2, respectively.\nAll hyperparameters were tuned on validation data\nusing 5 trials. All experiments were performed on\na server with one GPU (Tesla V100).\n4.4 Main Results\nTo verify the effectiveness of MoAlign, we report\noverall average results in Table 1. It shows per-\nformance comparisons on both two datasets with\ndifferent splits on training/testing data of alignment\nseeds, i.e., 2:8, 5:5, and 8:2. From the table, we can\nobserve that: 1) Our model outperforms all base-\nlines of both EA, multi-modal Transformer-based,\nand MMEA methods, in terms of three metrics on\nboth datasets. It demonstrates that our model is\nrobust to different proportions of training resources\nand learns a good performance on few-shot data.\n2) Compared to EA baselines (1-3), especially for\nMRR and Hits@1, our model improves 5% and\n9% up on average on FB15K-DB15K and FB15K-\nYAGO15K, tending to achieve more significant\nimprovements. It demonstrates that the effective-\n3https://github.com/huggingface/transformers\n4https://github.com/machrisaa/tensorflow-vgg\n992\nFB15K-DB15K (20%) FB15K-DB15K (50%) FB15K-DB15K (80%)Methods MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10\nTransE(Bordes et al., 2013)13.4 7.8 24.0 30.6 23.0 44.6 50.7 42.6 65.9GCN-align(Wang et al., 2018a)8.7 5.3 17.4 29.3 22.6 43.5 47.2 41.4 63.5AttrGNN(Liu et al., 2020b)34.3 25.2 53.5 54.7 47.3 72.1 70.3 67.1 83.9\nBERT(Devlin et al., 2019)32.6 24.3 48.0 49.6 45.2 67.9 65.3 64.5 80.1ViT(Dosovitskiy et al., 2021)33.5 25.1 53.9 50.5 45.5 69.0 71.5 66.8 85.7CLIP(Radford et al., 2021)35.4 27.0 55.3 54.1 48.7 71.4 73.9 68.3 86.0\nPoE(Liu et al., 2019) 17.0 12.6 25.1 53.3 46.4 65.8 72.1 66.6 82.0Chen et al.(Chen et al., 2020)35.7 26.5 54.1 51.2 41.7 70.3 68.5 59.0 86.9HEA(Guo et al., 2021) - 12.7 36.9 - 26.2 58.1 - 41.7 78.6EV A(Liu et al., 2021) 35.2 28.9 54.5 53.8 45.3 72.9 71.6 63.5 85.1ACK-MMEA(Li et al., 2023a)38.7 30.4 54.9 62.4 56.0 73.6 75.2 68.2 87.4\nMoAlign (ours) 40.9(↑2.2) 31.8(↑1.4) 56.4(↑1.1) 63.4(↑1.0) 57.6(↑1.6) 74.9(↑1.3) 77.3(↑2.1) 69.9(↑1.6) 88.2(↑0.8)\nFB15K-YAGO15K (20%) FB15K-YAGO15K (50%) FB15K-YAGO15K (80%)Methods MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10\nTransE(Bordes et al., 2013)11.2 6.4 20.3 26.2 19.7 38.2 46.3 39.2 59.5GCN-align(Wang et al., 2018a)15.3 8.1 23.5 29.4 23.5 42.4 47.7 40.6 64.3AttrGNN(Liu et al., 2020b)31.8 22.4 39.5 46.2 38.0 63.9 67.1 59.9 78.7\nBERT(Devlin et al., 2019)30.5 23.6 39.0 48.7 43.1 62.4 67.3 60.8 81.2ViT(Dosovitskiy et al., 2021)32.4 26.8 44.9 52.0 45.7 67.5 71.3 63.1 82.0CLIP(Radford et al., 2021)34.8 29.3 47.1 56.8 49.0 70.2 72.1 65.2 85.2\nPoE(Liu et al., 2019) 15.4 11.3 22.9 41.4 34.7 53.6 63.5 57.3 74.6Chen et al.(Chen et al., 2020)31.7 23.4 48.0 48.6 40.3 64.5 68.2 59.8 83.9HEA(Guo et al., 2021) - 10.5 31.3 - 26.5 58.1 - 43.3 80.1EV A(Liu et al., 2021) 33.5 25.0 46.2 56.1 47.8 68.3 72.5 64.0 84.5ACK-MMEA(Li et al., 2023a)36.0 28.9 49.6 59.3 53.5 69.9 74.4 67.6 86.4\nMoAlign (ours) 37.8(↑1.8) 29.6(↑0.3) 52.5(↑2.9) 61.7(↑2.4) 55.0(↑1.5) 71.3(↑1.1) 76.9(↑2.5) 68.9(↑1.3) 88.4(↑2.0)\nTable 1: Main experiments on FB15K-DB15K (top) (%) and FB15K-YAGO15K (bottom) (%) with different\nproportions of MMEA seeds. The best results are highlighted in bold, and underlined values are the second best\nresult. \"↑\" means the improvement compared to the second best result, and “-\" means that results are not available.\nFB15K-DB15K (80%) FB15K-DB15K (80%)\nVariants MRR Hits@1 Hits@10 △Avg MRR Hits@1 Hits@10 △Avg\nMoAlign (ours) 77.3 69.9 88.2 - 76.9 68.9 88.4 -\nw/o modifiable layer 75.8 66.4 85.9 ↓2.4 74.2 65.3 85.7 ↓3.0\nw/o modifiable self-attention76.5 67.1 86.4 ↓1.8 74.6 66.7 86.2 ↓2.2\nrepl. multi-head self-attention76.0 66.9 87.1 ↓1.8 74.5 65.9 86.0 ↓2.6\nw/o type information 76.2 67.8 87.3 ↓1.4 74.1 66.5 86.7 ↓2.3\nw/o context loss 76.9 68.2 87.1 ↓1.1 75.4 67.2 86.3 ↓1.8\nw/o text attribute 75.1 65.3 85.7 ↓3.1 73.4 65.0 86.4 ↓3.1\nw/o image attribute 75.8 65.4 86.0 ↓2.7 74.0 66.8 85.4 ↓2.7\nTable 2: Variant experiments on FB15K-DB15K (80%) and FB15K-YOGA15K (80%). “w/o” means removing\ncorresponding module from complete model. “repl.” means replacing corresponding module with other module.\nness of multi-modal context information benefits\nincorporating alignment knowledge. 3) Compared\nto multi-modal transformer-based baselines, our\nmodel achieves better results and the transformer-\nbased baselines perform better than EA baselines.\nIt demonstrates that transformer-based structures\ncan learn better MMEA knowledge. 4) Compared\nto MMEA baselines, our model designs a Hierarchi-\ncal Block and modifiable self-attention mechanism,\nthe average gains of our model regarding MRR,\nHits@1, and Hits@10 are 2%, 1.4%, and 1.7%,\nrespectively. The reason is that our method incor-\nporates multi-modal attributes and robust context\nentity information. All the observations demon-\nstrate the effectiveness of MoAlign.\n4.5 Discussions for Model Variants\nTo investigate the effectiveness of each module in\nMoAlign, we conduct variant experiments, show-\ncasing the results in Table 2. The \" ↓\" means the\nvalue of performance degradation compared to the\nMoAlign. We can observe that: 1) The impact of\nthe Hierarchical Block tends to be more significant.\nWe believe the reason is that the adaptive intro-\nduction of multi-modal attributes and neighbors\ncaptures more clues. 2) By replacing the modifi-\nable self-attention to the multi-head self-attention,\nthe performance decreased significantly. It demon-\nstrates that the modifiable self-attention captures\nmore effective multi-modal attribute and relation\ninformation. 3) When we remove all image at-\n993\nAttrGNN CLIP PoE Chen et al. ACK Ours\nModels\n60\n65\n70\n75\n80MRR (%)\nAll\nDel. image attributes\nDel. text attributes\nDel. all attributes\nFigure 3: Results of deleting attributes on FB15K-\nDB15K (80%). “Del.” means deleting the correspond-\ning attribute.\nFB15K-DB15K (80%)\nVariants MRR Hits@1 Hits@10△Avg\nMoAlign (N→V→T) 77.3 69.9 88.2 -\nMoAlign (N→T→V) 76.3 68.0 87.5 ↓1.2\nMoAlign (V→N→T) 76.0 67.4 86.4 ↓1.9\nMoAlign (V→T→N) 75.6 67.3 86.1 ↓2.1\nMoAlign (T→N→V) 75.8 67.7 85.9 ↓2.0\nMoAlign (T→V→N) 76.0 66.9 86.1 ↓2.1\nTable 3: Order Impact on FB15K-DB15K (80%).\ntributes as “w/o image attribute\", our method drops\n2.7% and 2.7% on average on FB15K-DB15K and\nFB15K-Y AGO15K. It demonstrates that image at-\ntributes can improve model performance and our\nmethod utilizes image attributes effectively by cap-\nturing more alignment knowledge.\n4.6 Impact of Multi-modal Attributes\nTo further investigate the impact of multi-modal\nattributes on all compared methods, we report the\nresults by deleting different modalities of attributes,\nas shown in Figure 3. From the figure, we can ob-\nserve that: 1) The variants without the text or im-\nage attributes significantly decline on all evaluation\nmetrics, which demonstrates that the multi-modal\nattributes are necessary and effective for MMEA. 2)\nCompared to other baselines, our model derives bet-\nter results both in the case of using all multi-modal\nattributes or abandoning some of them. It demon-\nstrates our model makes full use of existing multi-\nmodal attributes, and multi-modal attributes are\neffective for MMEA. All the observations demon-\nstrate that the effectiveness of the MMKG trans-\nformer encoder and the type-prompt encoder.\nIn addition, to investigate the impact of the order\nin which multi-modal attributes are introduced to\nthe model, we conduct experiments with different\norders of introducing neighbor information, tex-\n5% 10% 15% 20% 25% 30%\nInterference Proportion of TST-MMEA\n74.0\n74.5\n75.0\n75.5\n76.0\n76.5\n77.0\n77.5\n78.0MRR (%)\nInterf. Textual Attributes\nInterf. Visual Attributes\nInterf. Neighbors\nInterf. Random\nFigure 4: Results of interference attributes or neighbors\non FB15K-DB15K (80%). “Interf.” means interference\nsome proportion of attributes or neighbors.\ntual attributes, and visual attributes. As shown in\nthe Table 3, the introduction order has a signifi-\ncant impact on our model. Specifically, the best\nperformance is achieved when textual attributes,\nvisual attributes, and neighbor information are in-\ntroduced in that order. This suggests that aggre-\ngating attribute information to learn a good entity\nrepresentation first, and then incorporating neigh-\nbor information, can effectively utilize both the\nattributes and neighborhood information of nodes.\n4.7 Impact of Interference Data\nWe investigate the impact of interference data on\nour proposed method for MMEA. Specifically, we\nrandomly replace 5%, 10%, 15%, 20%, 25% and\n30% of the neighbor entities and attribute informa-\ntion to test the robustness of our method, as shown\nin Figure 4. The experimental results demonstrate\nthat our method exhibits better tolerance to inter-\nference compared to the baseline methods. This\nsuggests that our approach, which incorporates hi-\nerarchical information from different modalities\nand introduces type prefix to entity representations,\nis capable of handling interference data and im-\nproving the robustness of the model.\n5 Conclusion\nThis paper proposes a novel MMEA framework.\nIt incorporates cross-modal alignment knowledge\nusing a two-stage transformer encoder to better\ncapture complex inter-modality dependencies and\nsemantic relationships. It includes a MMKG trans-\nformer encoder that uses self-attention mechanisms\nto establish associations between intra-modal fea-\ntures relevant to the task. Our experiments show\nthat our approach outperforms competitors.\n994\nLimitations\nOur work hierarchically introduces neighbor, mul-\ntimodal attribute, and entity types to enhance the\nalignment task. Empirical experiments demon-\nstrate that our method effectively integrates het-\nerogeneous information through the multi-modal\nKG Transformer. However, there are still some\nlimitations of our approach can be summarized as\nfollows:\n• Due to the limitation of the existing MMEA\ndatasets, we only experiment on entity, text,\nand image modalities to explore the influence\nof multi-modal features. We will study more\nmodalities in future work.\n• Our approach employs the transformer encoder\narchitecture, which entails a substantial time over-\nhead. In forthcoming investigations, we intend\nto explore the feasibility of leveraging prompt-\nbased techniques to mitigate the computational\nburden and expedite model training.\nEthics Statement\nIn this work, we propose a new MMEA frame-\nwork that hierarchically introduces neighbor, multi-\nmodal attribute, and entity types to benchmark our\narchitecture with baseline architectures on the two\nMNEA datasets.\nData Bias. Our framework is tailored for multi-\nmodal entity alignment in the general domain.\nNonetheless, its efficacy may be compromised\nwhen confronted with datasets exhibiting dissimilar\ndistributions or in novel domains, potentially lead-\ning to biased outcomes. The experimental results\npresented in the section are predicated on particular\nbenchmark datasets, which are susceptible to such\nbiases. As a result, it is imperative to exercise cau-\ntion when assessing the model’s generalizability\nand fairness.\nComputing Cost/Emission. Our study, which in-\nvolves the utilization of large-scale language mod-\nels, entails a substantial computational overhead.\nWe acknowledge that this computational burden\nhas a detrimental environmental impact in terms of\ncarbon emissions. Specifically, our research neces-\nsitated a cumulative 588 GPU hours of computation\nutilizing Tesla V100 GPUs. The total carbon foot-\nprint resulting from this computational process is\nestimated to be 65.27 kg of CO2 per run, with a\ntotal of two runs being conducted.\nAcknowledgment\nWe thank the anonymous reviewers for their in-\nsightful comments and suggestions. Jianxin Li is\nthe corresponding author. The authors of this pa-\nper were supported by the NSFC through grant\nNo.62225202, 62106059.\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\nDevi Parikh. 2015. Vqa: Visual question answering.\nIn ICCV, pages 2425–2433.\nAntoine Bordes, Nicolas Usunier, Alberto García-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013, Lake\nTahoe, Nevada, United States, pages 2787–2795.\nXianshuai Cao, Yuliang Shi, Jihu Wang, Han Yu, Xin-\njun Wang, and Zhongmin Yan. 2022. Cross-modal\nknowledge graph contrastive learning for machine\nlearning method recommendation. In MM ’22: The\n30th ACM International Conference on Multimedia,\nLisboa, Portugal, October 10 - 14, 2022, pages 3694–\n3702. ACM.\nLiyi Chen, Zhi Li, Yijun Wang, Tong Xu, Zhefeng\nWang, and Enhong Chen. 2020. MMEA: entity align-\nment for multi-modal knowledge graph. In Knowl-\nedge Science, Engineering and Management - 13th\nInternational Conference, KSEM 2020, Hangzhou,\nChina, August 28-30, 2020, Proceedings, Part I, vol-\nume 12274 of Lecture Notes in Computer Science ,\npages 134–147. Springer.\nZhuo Chen, Jiaoyan Chen, Wen Zhang, Lingbing Guo,\nYin Fang, Yufeng Huang, Yuxia Geng, Jeff Z. Pan,\nWenting Song, and Huajun Chen. 2022. Meaformer:\nMulti-modal entity alignment transformer for meta\nmodality hybrid. CoRR, abs/2212.14454.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\n995\nrecognition at scale. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net.\nXiang Fang, Daizong Liu, Pan Zhou, and Yuchong Hu.\n2022. Multi-modal cross-domain alignment network\nfor video moment retrieval. IEEE Transactions on\nMultimedia.\nXiang Fang, Daizong Liu, Pan Zhou, and Guoshun Nan.\n2023a. You can ground earlier than see: An effective\nand efficient pipeline for temporal sentence ground-\ning in compressed videos. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 2448–2460.\nXiang Fang, Daizong Liu, Pan Zhou, Zichuan Xu, and\nRuixuan Li. 2023b. Hierarchical local-global trans-\nformer for temporal sentence grounding. IEEE Trans-\nactions on Multimedia.\nHao Guo, Jiuyang Tang, Weixin Zeng, Xiang Zhao,\nand Li Liu. 2021. Multi-modal entity alignment in\nhyperbolic space. Neurocomputing, 461:598–607.\nPhillip Howard, Arden Ma, Vasudev Lal, Ana Paula\nSimões, Daniel Korat, Oren Pereg, Moshe\nWasserblat, and Gadi Singer. 2022. Cross-domain\naspect extraction using transformers augmented with\nknowledge graphs. In Proceedings of the 31st ACM\nInternational Conference on Information & Knowl-\nedge Management, Atlanta, GA, USA, October 17-21,\n2022, pages 780–790. ACM.\nZhiwei Hu, Víctor Gutiérrez-Basulto, Zhiliang Xiang,\nRu Li, and Jeff Z. Pan. 2022. Transformer-based\nentity typing in knowledge graphs. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npages 5988–6001. Association for Computational\nLinguistics.\nJin Jiang, Mohan Li, and Zhaoquan Gu. 2021. A sur-\nvey on translating embedding based entity alignment\nin knowledge graphs. In Sixth IEEE International\nConference on Data Science in Cyberspace, DSC\n2021, Shenzhen, China, October 9-11, 2021, pages\n187–194. IEEE.\nQian Li, Shu Guo, Yangyifei Luo, Cheng Ji, Li-\nhong Wang, Jiawei Sheng, and Jianxin Li. 2023a.\nAttribute-consistent knowledge graph representation\nlearning for multi-modal entity alignment. In Pro-\nceedings of the ACM Web Conference 2023, WWW\n2023, Austin, TX, USA, 30 April 2023 - 4 May 2023,\npages 2499–2508. ACM.\nYangning Li, Jiaoyan Chen, Yinghui Li, Yuejia Xi-\nang, Xi Chen, and Haitao Zheng. 2023b. Vision,\ndeduction and alignment: An empirical study on\nmulti-modal knowledge graph alignment. CoRR,\nabs/2302.08774.\nZhenxi Lin, Ziheng Zhang, Meng Wang, Yinghui Shi,\nXian Wu, and Yefeng Zheng. 2022. Multi-modal con-\ntrastive representation learning for entity alignment.\nIn Proceedings of the 29th International Confer-\nence on Computational Linguistics, COLING 2022,\nGyeongju, Republic of Korea, October 12-17, 2022,\npages 2572–2584. International Committee on Com-\nputational Linguistics.\nFangyu Liu, Muhao Chen, Dan Roth, and Nigel Collier.\n2021. Visual pivoting for (unsupervised) entity align-\nment. In Thirty-Fifth AAAI Conference on Artificial\nIntelligence, AAAI 2021, Thirty-Third Conference\non Innovative Applications of Artificial Intelligence,\nIAAI 2021, The Eleventh Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2021, Vir-\ntual Event, February 2-9, 2021, pages 4257–4266.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9):195:1–195:35.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020a. K-BERT: en-\nabling language representation with knowledge graph.\nIn The Thirty-Fourth AAAI Conference on Artificial\nIntelligence, AAAI 2020, The Thirty-Second Innova-\ntive Applications of Artificial Intelligence Conference,\nIAAI 2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI 2020,\nNew York, NY, USA, February 7-12, 2020 , pages\n2901–2908. AAAI Press.\nXiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong\nQiu, Mengdi Zhang, Wei Wu, Yuxiao Dong, and Jie\nTang. 2022. Mask and reason: Pre-training knowl-\nedge graph transformers for complex logical queries.\nIn KDD ’22: The 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, Washington,\nDC, USA, August 14 - 18, 2022 , pages 1120–1130.\nACM.\nYe Liu, Hui Li, Alberto García-Durán, Mathias Niepert,\nDaniel Oñoro-Rubio, and David S. Rosenblum. 2019.\nMMKG: multi-modal knowledge graphs. In The Se-\nmantic Web - 16th International Conference, ESWC\n2019, Portorož, Slovenia, June 2-6, 2019, Proceed-\nings, volume 11503 of Lecture Notes in Computer\nScience, pages 459–474. Springer.\nZhiyuan Liu, Yixin Cao, Liangming Pan, Juanzi Li, and\nTat-Seng Chua. 2020b. Exploring and evaluating\nattributes, values, and structures for entity alignment.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 6355–\n6364. Association for Computational Linguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\n996\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research , pages 8748–8763.\nPMLR.\nKevin J Shih, Saurabh Singh, and Derek Hoiem. 2016.\nWhere to look: Focus regions for visual question\nanswering. In CVPR, pages 4613–4621.\nRui Sun, Xuezhi Cao, Yan Zhao, Junchen Wan, Kun\nZhou, Fuzheng Zhang, Zhongyuan Wang, and Kai\nZheng. 2020. Multi-modal knowledge graphs for\nrecommender systems. In CIKM ’20: The 29th ACM\nInternational Conference on Information and Knowl-\nedge Management, Virtual Event, Ireland, October\n19-23, 2020, pages 1405–1414. ACM.\nJiaan Wang, Fandong Meng, Duo Zheng, Yunlong\nLiang, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2022. A\nsurvey on cross-lingual summarization. Trans. Assoc.\nComput. Linguistics, 10:1304–1323.\nJiaan Wang, Fandong Meng, Duo Zheng, Yunlong\nLiang, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2023.\nTowards unifying multi-lingual and cross-lingual\nsummarization. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023, pages 15127–15143. Asso-\nciation for Computational Linguistics.\nXiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and\nTat-Seng Chua. 2019. KGAT: knowledge graph at-\ntention network for recommendation. In Proceedings\nof the 25th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, KDD 2019,\nAnchorage, AK, USA, August 4-8, 2019, pages 950–\n958. ACM.\nZeshi Wang, Mohan Li, and Zhaoquan Gu. 2021. A\nreview of entity alignment based on graph convolu-\ntional neural network. In Sixth IEEE International\nConference on Data Science in Cyberspace, DSC\n2021, Shenzhen, China, October 9-11, 2021, pages\n144–151. IEEE.\nZhichun Wang, Qingsong Lv, Xiaohan Lan, and\nYu Zhang. 2018a. Cross-lingual knowledge graph\nalignment via graph convolutional networks. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, Brussels, Bel-\ngium, October 31 - November 4, 2018 , pages 349–\n357.\nZhichun Wang, Qingsong Lv, Xiaohan Lan, and\nYu Zhang. 2018b. Cross-lingual knowledge graph\nalignment via graph convolutional networks. In Pro-\nceedings of the 2018 conference on empirical meth-\nods in natural language processing, pages 349–357.\nGuohai Xu, Hehong Chen, Feng-Lin Li, Fu Sun, Yun-\nzhou Shi, Zhixiong Zeng, Wei Zhou, Zhongzhou\nZhao, and Ji Zhang. 2021. Alime MKG: A\nmulti-modal knowledge graph for live-streaming e-\ncommerce. In CIKM ’21: The 30th ACM Interna-\ntional Conference on Information and Knowledge\nManagement, Virtual Event, Queensland, Australia,\nNovember 1 - 5, 2021, pages 4808–4812. ACM.\nSen Yang, Xuanhan Wang, Lianli Gao, and Jingkuan\nSong. 2022. MKE-GCN: multi-modal knowledge\nembedded graph convolutional network for skeleton-\nbased action recognition in the wild. In IEEE Inter-\nnational Conference on Multimedia and Expo, ICME\n2022, Taipei, Taiwan, July 18-22, 2022, pages 1–6.\nIEEE.\nYunzhi Yao, Shaohan Huang, Li Dong, Furu Wei,\nHuajun Chen, and Ningyu Zhang. 2022. Kformer:\nKnowledge injection in transformer feed-forward\nlayers. In Natural Language Processing and Chi-\nnese Computing: 11th CCF International Confer-\nence, NLPCC 2022, Guilin, China, September 24–25,\n2022, Proceedings, Part I, pages 131–143. Springer.\nXiangru Zhu, Zhixu Li, Xiaodan Wang, Xueyao Jiang,\nPenglei Sun, Xuwu Wang, Yanghua Xiao, and\nNicholas Jing Yuan. 2022. Multi-modal knowledge\ngraph construction and application: A survey. CoRR,\nabs/2202.05786.\n997\nA Related Work\nA.1 Multi-Modal Entity Alignment\nIn the real-world, due to the multi-modal nature\nof KGs, there have been several works (Zhu et al.,\n2022; Wang et al., 2021; Jiang et al., 2021; Fang\net al., 2022) that have started to focus on MMEA\ntechnology. One popular approach is to use em-\nbeddings to represent entities and their associated\nmodalities (Cao et al., 2022; Yang et al., 2022).\nThese embeddings can then be used to measure the\nsimilarity between entities and align them across\ndifferent KGs. Wang et al. (Wang et al., 2018b)\nproposed a framework that uses cross-modal em-\nbeddings to align entities across different modal-\nities, such as text and images. The model maps\nentities from different modalities into a shared em-\nbedding space, where entities that correspond to\nthe same real-world object are close to each other.\nHowever, this approach cannot capture the poten-\ntial interactions among heterogeneous modalities,\nlimiting its capacity for performing accurate entity\nalignments. To address this limitation, some re-\nsearchers have proposed multi-modal knowledge\nembedding methods that can discriminatively gen-\nerate knowledge representations of different types\nof knowledge and then integrate them. Chen et al.\n(2020) proposed a method that uses a multi-modal\nfusion module to integrate knowledge representa-\ntions of different types. Similarly, Guo et al. (2021)\nproposed a GNN-based model that learns to ag-\ngregate information from different modalities and\npropagates it across the knowledge graph to align\nentities. It developed a hyperbolic multi-modal en-\ntity alignment (HEA) approach that combines both\nattribute and entity representations in the hyper-\nbolic space and uses aggregated embeddings to pre-\ndict alignments. EV A (Liu et al., 2021) combines\nimages, structures, relations, and attributes infor-\nmation for the MMEA with a learnable weighted\nattention to learn the importance of each modal at-\ntributes. Despite these advances, existing methods\noften ignore contextual gaps between entity pairs,\nwhich may limit the effectiveness of alignment.\nA.2 Knowledge Graph Transformer\nThe Transformer architecture, originally proposed\nfor natural language processing tasks, has been ap-\nplied to various knowledge graph tasks as well (Liu\net al., 2022; Hu et al., 2022; Howard et al., 2022;\nWang et al., 2023, 2022; Fang et al., 2023b,a). For\nexample, the KGAT model (Wang et al., 2019)\nuses a graph attention mechanism to capture the\ncomplex relations between entities in a knowledge\ngraph, and a Transformer to learn representations\nof the entities for downstream tasks such as link pre-\ndiction and entity recommendation. The K-BERT\nmodel (Liu et al., 2020a) extends this approach by\npre-training a Transformer on a large corpus of tex-\ntual data, and then fine-tuning it on a knowledge\ngraph to improve entity and relation extraction. The\ntransformer model has the ability to model long-\nrange dependencies, where entities and their asso-\nciated modalities can be distant from each other in\nthe knowledge graph. Furthermore, Transformers\nutilize attention mechanisms to weigh the impor-\ntance of different inputs and focus on relevant in-\nformation, which is particularly useful for aligning\nentities across different modalities.\nB Comparision Methods\nWe compared our method with three EA baselines\nthat aggregate text attributes and relation informa-\ntion, and introduced image attributes initialized by\nVGG16 for entity representation using the same\naggregation method as for text attributes. The\nthree EA methods compared are as follows: (1)\nTransE(Bordes et al., 2013) assumes that the entity\nembedding vshould be close to the attribute embed-\nding aplus their relation r. (2) GCN-align (Wang\net al., 2018a) transfers entities and attributes from\neach language to a common representation space\nthrough GCN. (3) AttrGNN (Liu et al., 2020b) di-\nvides the KG into multiple subgraphs, effectively\nmodeling various types of attributes.\nWe also compared our method with three trans-\nformer models that incorporate multi-modal in-\nformation: (4) BERT (Devlin et al., 2019) is a\npre-trained model to generate representations. (5)\nViT (Dosovitskiy et al., 2021) is a visual trans-\nformer model that partitions an image into patches\nand feeds them as input sequences. (6) CLIP (Rad-\nford et al., 2021) is a joint language and vision\nmodel architecture that employs contrastive learn-\ning.\nIn addition, we compared our method with four\nMMEA methods focusing on utilizing multi-modal\nattributes: (7) PoE (Liu et al., 2019) utilizes im-\nage features and measures credibility by matching\nthe semantics of entities. (8) Chen et al. (Chen\net al., 2020) designs a fusion module to integrate\nmulti-modal attributes. (9) HEA (Guo et al., 2021)\ncharacterizes MMKG in hyperbolic space. (10)\n998\nEV A(Liu et al., 2021) combines multi-modal at-\ntributes and relations with an attention mechanism\nto learn the importance of modality. (11) ACK-\nMMEA (Liu et al., 2021) designs an attribute-\nconsistent KG representation framework to com-\npensate contextual gaps.\n999"
}