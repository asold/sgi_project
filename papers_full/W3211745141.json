{
    "title": "Developing a Clinical Language Model for Swedish: Continued Pretraining of Generic BERT with In-Domain Data",
    "url": "https://openalex.org/W3211745141",
    "year": 2021,
    "authors": [
        {
            "id": null,
            "name": "Department of Computer and Systems Sciences (DSV), Stockholm University, Kista, Sweden",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3213227553",
            "name": "Anastasios Lamproudis",
            "affiliations": [
                "Kista Photonics Research Center",
                "Stockholm University"
            ]
        },
        {
            "id": "https://openalex.org/A2128181539",
            "name": "Aron Henriksson",
            "affiliations": [
                "Stockholm University",
                "Kista Photonics Research Center"
            ]
        },
        {
            "id": null,
            "name": "Department of Computer and Systems Sciences (DSV), Stockholm University, Kista, Sweden",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A64684969",
            "name": "Hercules Dalianis",
            "affiliations": [
                "Stockholm University",
                "Kista Photonics Research Center"
            ]
        },
        {
            "id": null,
            "name": "Department of Computer and Systems Sciences (DSV), Stockholm University, Kista, Sweden",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3099008231",
        "https://openalex.org/W3104578551",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2124216778",
        "https://openalex.org/W1490996061",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W3213934720",
        "https://openalex.org/W2211265567",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W4287727940",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W3100452049",
        "https://openalex.org/W3034238904"
    ],
    "abstract": "The use of pretrained language models, finetuned to perform a specific downstream task, has become widespread in NLP. Using a generic language model in specialized domains may, however, be sub-optimal due to differences in language use and vocabulary. In this paper, it is investigated whether an existing, generic language model for Swedish can be improved for the clinical domain through continued pretraining with clinical text. The generic and domain-specific language models are fine-tuned and evaluated on three representative clinical NLP tasks: (i) identifying protected health information, (ii) assigning ICD-10 diagnosis codes to discharge summaries, and (iii) sentence-level uncertainty prediction. The results show that continued pretraining on in-domain data leads to improved performance on all three downstream tasks, indicating that there is a potential added value of domain-specific language models for clinical NLP.",
    "full_text": null
}