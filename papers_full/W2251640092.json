{
  "title": "The Fixed-Size Ordinally-Forgetting Encoding Method for Neural Network Language Models",
  "url": "https://openalex.org/W2251640092",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A2099219781",
      "name": "Shiliang Zhang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2110828282",
      "name": "Hui Jiang",
      "affiliations": [
        "York University"
      ]
    },
    {
      "id": "https://openalex.org/A2581399812",
      "name": "Mingbin Xu",
      "affiliations": [
        "York University"
      ]
    },
    {
      "id": "https://openalex.org/A2123290482",
      "name": "Junfeng Hou",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2098654552",
      "name": "Lirong Dai",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1533861849",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2170942820",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W4253112633",
    "https://openalex.org/W2072350286",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W1943583106",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2032676284",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2169585179",
    "https://openalex.org/W2271177914",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2150355110"
  ],
  "abstract": "ShiLiang Zhang, Hui Jiang, MingBin Xu, JunFeng Hou, LiRong Dai. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2015.",
  "full_text": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 495–500,\nBeijing, China, July 26-31, 2015.c⃝2015 Association for Computational Linguistics\nThe Fixed-Size Ordinally-Forgetting Encoding Method\nfor Neural Network Language Models\nShiliang Zhang1, Hui Jiang2, Mingbin Xu2, Junfeng Hou1, Lirong Dai1\n1National Engineering Laboratory for Speech and Language Information Processing\nUniversity of Science and Technology of China, Hefei, Anhui, China\n2Department of Electrical Engineering and Computer Science\nYork University, 4700 Keele Street, Toronto, Ontario, M3J 1P3, Canada\n{zsl2008,hjf176}@mail.ustc.edu.cn, {hj,xmb}@cse.yorku.ca, lrdai@ustc.edu.cn\nAbstract\nIn this paper, we propose the new ﬁxed-\nsize ordinally-forgetting encoding (FOFE)\nmethod, which can almost uniquely en-\ncode any variable-length sequence of\nwords into a ﬁxed-size representation.\nFOFE can model the word order in a se-\nquence using a simple ordinally-forgetting\nmechanism according to the positions of\nwords. In this work, we have applied\nFOFE to feedforward neural network lan-\nguage models (FNN-LMs). Experimental\nresults have shown that without using any\nrecurrent feedbacks, FOFE based FNN-\nLMs can signiﬁcantly outperform not only\nthe standard ﬁxed-input FNN-LMs but\nalso the popular recurrent neural network\n(RNN) LMs.\n1 Introduction\nLanguage models play an important role in many\napplications like speech recognition, machine\ntranslation, information retrieval and nature lan-\nguage understanding. Traditionally, the back-off\nn-gram models (Katz, 1987; Kneser, 1995) are\nthe standard approach to language modeling. Re-\ncently, neural networks have been successfully ap-\nplied to language modeling, yielding the state-\nof-the-art performance in many tasks. In neural\nnetwork language models (NNLM), the feedfor-\nward neural networks (FNN) and recurrent neu-\nral networks (RNN) (Elman, 1990) are two pop-\nular architectures. The basic idea of NNLMs is\nto use a projection layer to project discrete words\ninto a continuous space and estimate word con-\nditional probabilities in this space, which may be\nsmoother to better generalize to unseen contexts.\nFNN language models (FNN-LM) (Bengio and\nDucharme, 2001; Bengio, 2003) usually use a lim-\nited history within a ﬁxed-size context window\nto predict the next word. RNN language mod-\nels (RNN-LM) (Mikolov, 2010; Mikolov, 2012)\nadopt a time-delayed recursive architecture for the\nhidden layers to memorize the long-term depen-\ndency in language. Therefore, it is widely re-\nported that RNN-LMs usually outperform FNN-\nLMs in language modeling. While RNNs are the-\noretically powerful, the learning of RNNs needs to\nuse the so-called back-propagation through time\n(BPTT) (Werbos, 1990) due to the internal recur-\nrent feedback cycles. The BPTT signiﬁcantly in-\ncreases the computational complexity of the learn-\ning algorithms and it may cause many problems\nin learning, such as gradient vanishing and ex-\nploding (Bengio, 1994). More recently, some\nnew architectures have been proposed to solve\nthese problems. For example, the long short\nterm memory (LSTM) RNN (Hochreiter, 1997) is\nan enhanced architecture to implement the recur-\nrent feedbacks using various learnable gates, and\nit has obtained promising results on handwriting\nrecognition (Graves, 2009) and sequence model-\ning (Graves, 2013).\nComparing with RNN-LMs, FNN-LMs can be\nlearned in a simpler and more efﬁcient way. How-\never, FNN-LMs can not model the long-term de-\npendency in language due to the ﬁxed-size input\nwindow. In this paper, we propose a novel encod-\ning method for discrete sequences, named ﬁxed-\nsize ordinally-forgetting encoding (FOFE), which\ncan almost uniquely encode any variable-length\nword sequence into a ﬁxed-size code. Relying\non a constant forgetting factor, FOFE can model\nthe word order in a sequence based on a sim-\nple ordinally-forgetting mechanism, which uses\nthe position of each word in the sequence. Both\nthe theoretical analysis and the experimental sim-\nulation have shown that FOFE can provide al-\nmost unique codes for variable-length word se-\nquences as long as the forgetting factor is prop-\nerly selected. In this work, we apply FOFE to\n495\nneural network language models, where the ﬁxed-\nsize FOFE codes are fed to FNNs as input to\npredict next word, enabling FNN-LMs to model\nlong-term dependency in language. Experiments\non two benchmark tasks, Penn Treebank Corpus\n(PTB) and Large Text Compression Benchmark\n(LTCB), have shown that FOFE-based FNN-LMs\ncan not only signiﬁcantly outperform the stan-\ndard ﬁxed-input FNN-LMs but also achieve better\nperformance than the popular RNN-LMs with or\nwithout using LSTM. Moreover, our implementa-\ntion also shows that FOFE based FNN-LMs can\nbe learned very efﬁciently on GPUs without the\ncomplex BPTT procedure.\n2 Our Approach: FOFE\nAssume vocabulary size is K, NNLMs adopt the\n1-of-K encoding vectors as input. In this case,\neach word in vocabulary is represented as a one-\nhot vector e ∈RK. The 1-of-K representation is a\ncontext independent encoding method. When the\n1-of-K representation is used to model a word in a\nsequence, it can not model its history or context.\n2.1 Fixed-size Ordinally Forgetting Encoding\nWe propose a simple context-dependent encoding\nmethod for any sequence consisting of discrete\nsymbols, namely ﬁxed-size ordinally-forgetting\nencoding (FOFE). Given a sequence of words (or\nany discrete symbols), S = {w1,w2,··· ,wT },\neach word wt is ﬁrst represented by a 1-of-K rep-\nresentation et, from the ﬁrst word t= 1to the end\nof the sequence t = T, FOFE encodes each par-\ntial sequence (history) based on a simple recursive\nformula (with z0 = 0) as:\nzt = α·zt−1 + et (1 ≤t≤T) (1)\nwhere zt denotes the FOFE code for the partial\nsequence up to wt, and α(0 < α <1) is a con-\nstant forgetting factor to control the inﬂuence of\nthe history on the current position. Let’s take a\nsimple example here, assume we have three sym-\nbols in vocabulary, e.g., A, B, C, whose 1-of-\nK codes are [1,0,0], [0,1,0] and [0,0,1] respec-\ntively. In this case, the FOFE code for the se-\nquence {ABC}is [α2,α, 1], and that of {ABCBC}\nis [α4,α + α3,1 +α2].\nObviously, FOFE can encode any variable-\nlength discrete sequence into a ﬁxed-size code.\nMoreover, it is a recursive context dependent en-\ncoding method that smartly models the order in-\nFigure 1: The FOFE-based FNN language model.\nformation by various powers of the forgetting fac-\ntor. Furthermore, FOFE has an appealing property\nin modeling natural languages that the far-away\ncontext will be gradually forgotten due to α <1\nand the nearby contexts play much larger role in\nthe resultant FOFE codes.\n2.2 Uniqueness of FOFE codes\nGiven the vocabulary (of K symbols), for any se-\nquence S with a length of T, based on the FOFE\ncode zT computed as above, if we can always de-\ncode the original sequence Sunambiguously (per-\nfectly recovering S from zT ), we say FOFE is\nunique.\nTheorem 1 If the forgetting factor αsatisﬁes 0 <\nα≤0.5, FOFE is unique for any Kand T.\nThe proof is simple because if the FOFE code\nhas a value αt in its i-th element, we may de-\ntermine the word wi occurs in the position t of\nS without ambiguity since no matter how many\ntimes wi occurs in the far-away contexts ( < t),\nthey do not sum to αt (due to α ≤0.5). If wi ap-\npears in any closer context (> t), the i-th element\nmust be larger than αt.\nTheorem 2 For 0.5 < α <1, given any ﬁnite\nvalues of K and T, FOFE is almost unique every-\nwhere for α∈(0.5,1.0), except only a ﬁnite set of\ncountable choices of α.\nRefer to (Zhang et. al., 2015a) for the complete\nproof. Based on Theorem 2, FOFE is unique al-\nmost everywhere between (0.5,1.0) only except a\ncountable set of isolated choices of α. In practice,\nthe chance to exactly choose these isolated values\nbetween (0.5,1.0) is extremely slim, realistically\n496\nFigure 2: Numbers of collisions in simulation.\nalmost impossible due to quantization errors in the\nsystem. To verify this, we have run simulation ex-\nperiments for all possible sequences up to T = 20\nsymbols to count the number of collisions. Each\ncollision is deﬁned as the maximum element-wise\ndifference between two FOFE codes (generated\nfrom two different sequences) is less than a small\nthreshold ϵ. In Figure 2, we have shown the num-\nber of collisions (out of the total 220 tested cases)\nfor various α values when ϵ = 0.01, 0.001 and\n0.0001.1 The simulation experiments have shown\nthat the chance of collision is extremely small even\nwhen we allow a word to appear any times in the\ncontext. Obviously, in a natural language, a word\nnormally does not appear repeatedly within a near\ncontext. Moreover, we have run the simulation to\nexamine whether collisions actually occur in two\nreal text corpora, namely PTB (1M words) and\nLTCB (160M words), using ϵ = 0.01, we have\nnot observed a single collision for nine differentα\nvalues between [0.55,1.0] (incremental 0.05).\n2.3 Implement FOFE for FNN-LMs\nThe architecture of a FOFE based neural network\nlanguage model (FOFE-FNNLM) is shown in Fig-\nure 1. It is similar to regular bigram FNN-LMs ex-\ncept that it uses a FOFE code to feed into neural\nnetwork LM at each time. Moreover, the FOFE\ncan be easily scaled to higher orders like n-gram\nNNLMs. For example, Figure 3 is an illustration\nof a second order FOFE-based neural network lan-\nguage model.\nFOFE is a simple recursive encoding method\nbut a direct sequential implementation may not be\n1When we use a bigger value forα, the magnitudes of the\nresultant FOFE codes become much larger. As a result, the\nnumber of collisions (as measured by a ﬁxed absolute thresh-\nold ϵ) becomes smaller.\nFigure 3: Diagram of 2nd-order FOFE FNN-LM.\nefﬁcient for the parallel computation platform like\nGPUs. Here, we will show that the FOFE compu-\ntation can be efﬁciently implemented as sentence-\nby-sentence matrix multiplications, which are\nsuitable for the mini-batch based stochastic gra-\ndient descent (SGD) method running on GPUs.\nGiven a sentence, S = {w1,w2,··· ,wT },\nwhere each word is represented by a 1-of-K code\nas et (1 ≤t ≤T). The FOFE codes for all par-\ntial sequences in S can be computed based on the\nfollowing matrix multiplication:\nS =\n\n\n1\nα 1\nα2 α 1\n... ... 1\nαT−1 ··· α 1\n\n\n\n\ne1\ne2\ne3\n...\neT\n\n\n= MV\nwhere V is a matrix arranging all 1-of-K codes\nof the words in the sentence row by row, and M\nis a T-th order lower triangular matrix. Each row\nvector of S represents a FOFE code of the partial\nsequence up to each position in the sentence.\nThis matrix formulation can be easily extended\nto a mini-batch consisting of several sentences.\nAssume that a mini-batch is composed of N se-\nquences, L= {S1 S2 ···SN }, we can compute\nthe FOFE codes for all sentences in the mini-batch\nas follows:\n¯S =\n\n\nM1\nM2\n...\nMN\n\n\n\n\nV1\nV2\n...\nVN\n\n\n= ¯M¯V.\n497\nWhen feeding the FOFE codes to FNN as\nshown in Figure 1, we can compute the activation\nsignals (assume f is the activation function) in the\nﬁrst hidden layer for all histories in Sas follows:\nH = f\n(\n( ¯M¯V)UW+b\n)\n= f\n(\n¯M( ¯VU)W+b\n)\nwhere U denotes the word embedding matrix that\nprojects the word indices onto a continuous low-\ndimensional continuous space. As above, ¯VU\ncan be done efﬁciently by looking up the embed-\nding matrix. Therefore, for the computational ef-\nﬁciency purpose, we may apply FOFE to the word\nembedding vectors instead of the original high-\ndimensional one-hot vectors. In the backward\npass, we can calculate the gradients with the stan-\ndard back-propagation (BP) algorithm rather than\nBPTT. As a result, FOFE based FNN-LMs are the\nsame as the standard FNN-LMs in terms of com-\nputational complexity in training, which is much\nmore efﬁcient than RNN-LMs.\n3 Experiments\nWe have evaluated the FOFE method for NNLMs\non two benchmark tasks: i) the Penn Treebank\n(PTB) corpus of about 1M words, following the\nsame setup as (Mikolov, 2011). The vocabu-\nlary size is limited to 10k. The preprocess-\ning method and the way to split data into train-\ning/validation/test sets are the same as (Mikolov,\n2011). ii) The Large Text Compression Bench-\nmark (LTCB) (Mahoney, 2011). In LTCB, we use\nthe enwik9 dataset, which is composed of the ﬁrst\n109 bytes of enwiki-20060303-pages-articles.xml.\nWe split it into three parts: training (153M), val-\nidation (8.9M) and test (8.9M) sets. We limit the\nvocabulary size to 80k for LTCB and replace all\nout-of-vocabulary words by <UNK>. 2\n3.1 Experimental results on PTB\nWe have ﬁrst evaluated the performance of the\ntraditional FNN-LMs, taking the previous several\nwords as input, denoted as n-gram FNN-LMs here.\nWe have trained neural networks with a linear pro-\njection layer (of 200 hidden nodes) and two hid-\nden layers (of 400 nodes per layer). All hidden\nunits in networks use the rectiﬁed linear activation\nfunction, i.e., f(x) = max(0,x). The nets are\ninitialized based on the normalized initialization\n2Matlab codes are available athttps://wiki.eecs.\nyorku.ca/lab/MLL/projects:fofe:start for\nreaders to reproduce all results reported in this paper.\nFigure 4: Perplexities of FOFE FNNLMs as a\nfunction of the forgetting factor.\nin (Glorot, 2010), without using any pre-training.\nWe use SGD with a mini-batch size of 200 and an\ninitial learning rate of 0.4. The learning rate is kept\nﬁxed as long as the perplexity on the validation set\ndecreases by at least 1. After that, we continue six\nmore epochs of training, where the learning rate\nis halved after each epoch. The performance (in\nperplexity) of several n-gram FNN-LMs (from bi-\ngram to 6-gram) is shown in Table 1.\nFor the FOFE-FNNLMs, the net architecture\nand the parameter setting are the same as above.\nThe mini-batch size is also 200 and each mini-\nbatch is composed of several sentences up to 200\nwords (the last sentence may be truncated). All\nsentences in the corpus are randomly shufﬂed at\nthe beginning of each epoch. In this experiment,\nwe ﬁrst investigate how the forgetting factor α\nmay affect the performance of LMs. We have\ntrained two FOFE-FNNLMs: i) 1st-order (using\nzt as input to FNN for each time t; ii) 2nd-order\n(using both zt and zt−1 as input for each time t,\nwith a forgetting factor varying between[0.0,1.0].\nExperimental results in Figure 4 have shown that\na good choice of α lies between [0.5,0.8]. Us-\ning a too large or too small forgetting factor will\nhurt the performance. A too small forgetting fac-\ntor may limit the memory of the encoding while a\ntoo large αmay confuse LM with a far-away his-\ntory. In the following experiments, we setα= 0.7\nfor the rest experiments in this paper.\nIn Table 1, we have summarized the perplexi-\nties on the PTB test set for various models. The\nproposed FOFE-FNNLMs can signiﬁcantly out-\nperform the baseline FNN-LMs using the same\narchitecture. For example, the perplexity of the\nbaseline bigram FNNLM is 176, while the FOFE-\n498\nTable 1: Perplexities on PTB for various LMs.\nModel Test PPL\nKN 5-gram (Mikolov, 2011) 141\nFNNLM (Mikolov, 2012) 140\nRNNLM (Mikolov, 2011) 123\nLSTM (Graves, 2013) 117\nbigram FNNLM 176\ntrigram FNNLM 131\n4-gram FNNLM 118\n5-gram FNNLM 114\n6-gram FNNLM 113\n1st-order FOFE-FNNLM 116\n2nd-order FOFE-FNNLM 108\nTable 2: Perplexities on LTCB for various lan-\nguage models. [M*N] denotes the sizes of the in-\nput context window and projection layer.\nModel Architecture Test PPL\nKN 3-gram - 156\nKN 5-gram - 132\n[1*200]-400-400-80k 241\n[2*200]-400-400-80k 155\nFNN-LM [2*200]-600-600-80k 150\n[3*200]-400-400-80k 131\n[4*200]-400-400-80k 125\nRNN-LM [1*600]-80k 112\n[1*200]-400-400-80k 120\nFOFE [1*200]-600-600-80k 115\nFNN-LM [2*200]-400-400-80k 112\n[2*200]-600-600-80k 107\nFNNLM can improve to 116. Moreover, the\nFOFE-FNNLMs can even overtake a well-trained\nRNNLM (400 hidden units) in (Mikolov, 2011)\nand an LSTM in (Graves, 2013). It indicates\nFOFE-FNNLMs can effectively model the long-\nterm dependency in language without using any\nrecurrent feedback. At last, the 2nd-order FOFE-\nFNNLM can provide further improvement, yield-\ning the perplexity of 108 on PTB. It also outper-\nforms all higher-order FNN-LMs (4-gram, 5-gram\nand 6-gram), which are bigger in model size. To\nour knowledge, this is one of the best reported re-\nsults on PTB without model combination.\n3.2 Experimental results on LTCB\nWe have further examined the FOFE based FNN-\nLMs on a much larger text corpus, i.e. LTCB,\nwhich contains articles from Wikipedia. We have\ntrained several baseline systems: i) two n-gram\nLMs (3-gram and 5-gram) using the modiﬁed\nKneser-Ney smoothing without count cutoffs; ii)\nseveral traditional FNN-LMs with different model\nsizes and input context windows (bigram, trigram,\n4-gram and 5-gram); iii) an RNN-LM with one\nhidden layer of 600 nodes using the toolkit in\n(Mikolov, 2010), in which we have further used\na spliced sentence bunch in (Chen et al. 2014)\nto speed up the training on GPUs. Moreover, we\nhave examined four FOFE based FNN-LMs with\nvarious model sizes and input window sizes (two\n1st-order FOFE models and two 2nd-order ones).\nFor all NNLMs, we have used an output layer of\nthe full vocabulary (80k words). In these exper-\niments, we have used an initial learning rate of\n0.01, and a bigger mini-batch of 500 for FNN-\nLMMs and of 256 sentences for the RNN and\nFOFE models. Experimental results in Table 2\nhave shown that the FOFE-based FNN-LMs can\nsigniﬁcantly outperform the baseline FNN-LMs\n(including some larger higher-order models) and\nalso slightly overtake the popular RNN-based LM,\nyielding the best result (perplexity of 107) on the\ntest set.\n4 Conclusions\nIn this paper, we propose the ﬁxed-size ordinally-\nforgetting encoding (FOFE) method to almost\nuniquely encode any variable-length sequence into\na ﬁxed-size code. In this work, FOFE has been\nsuccessfully applied to neural network language\nmodeling. Next, FOFE may be combined with\nneural networks (Zhang and Jiang, 2015; Zhang\net. al., 2015b) for other NLP tasks, such as sen-\ntence modeling/matching, paraphrase detection,\nmachine translation, question and answer and etc.\nAcknowledgments\nThis work was supported in part by the Science\nand Technology Development of Anhui Province,\nChina (Grants No. 2014z02006) and the Funda-\nmental Research Funds for the Central Universi-\nties from China, as well as an NSERC Discov-\nery grant from Canadian federal government. We\nappreciate Dr. Barlas Oguz at Microsoft for his\ninsightful comments and constructive suggestions\non Theorem 2.\n499\nReferences\nSlava Katz. 1987. Estimation of probabilities from\nsparse data for the language model component of a\nspeech recognizer. IEEE Transactions on Acoustics,\nSpeech and Signal Processing (ASSP) , V olume 35,\nno 3, pages 400-401.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. InProc.\nof International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 181-184.\nPaul Werbos. 1990. Back-propagation through time:\nwhat it does and how to do it. Proceedings of the\nIEEE, volume 78, no 10, pages 1550-1560.\nYoshua Bengio, Patrice Simard and Paolo Frasconi.\n1994. Learning long-term dependencies with gradi-\nent descent is difﬁcult.IEEE Transactions on Neural\nNetworks volume 5, no 2, pages 157-166.\nYoshua Bengio and Rejean Ducharme. 2001. A neural\nprobabilistic language model. In Proc. of NIPS, vol-\nume 13.\nYoshua Bengio, Rejean Ducharme, Pascal Vincent,\nand Christian Jauvin. 2003. A neural probabilistic\nlanguage model. Journal of Machine Learning Re-\nsearch, volume 3, no 2, pages 1137-1155.\nJeffery Elman. 1990. Finding structure in time. Cogni-\ntive science, volume 14, no 2, pages 179-211.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Proc. of\nInterspeech, pages 1045-1048.\nTomas Mikolov, Stefan Kombrink, Lukas Burget, Jan\nCernocky and Sanjeev Khudanpur. 2011. Exten-\nsions of recurrent neural network language model.\nIn Proc. of International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n5528-5531.\nTomas Mikolov and Geoffrey Zweig. 2012. Context de-\npendent recurrent neural network language model.\nIn Proc. of SLT, pages 234-239.\nX. Chen, Y . Wang, X. Liu, et al. 2014. Efﬁcient GPU-\nbased training of recurrent neural network language\nmodels using spliced sentence bunch. In Proc. of In-\nterspeech.\nIlya Sutskever and Geoffrey Hinton. 2010. Temporal-\nkernel recurrent neural networks. Neural Networks.\npages 239-243.\nYong-Zhe Shi, Wei-Qiang Zhang, Meng Cai and Jia\nLiu. 2013. Temporal kernel neural network language\nmodel. In Proc. of International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\npages 8247-8251.\nSepp Hochreiter and Jurgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, volume 9,\nno 8, pages 1735-1780.\nAlex Graves and Jurgen Schmidhuber. 2009. Ofﬂine\nhandwriting recognition with multidimensional re-\ncurrent neural networks. In Proc. of NIPS . pages\n545-552.\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850.\nGlorot Xavier and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In Proc. of AISTATS.\nMatt Mahoney. 2011. Large Text Compression Bench-\nmark. In http://mattmahoney.net/dc/textdata.html.\nBarlas Oguz. 2015. Personal Communications.\nShiling Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou\nand LiRong Dai. 2015a. A Fixed-Size Encoding\nMethod for Variable-Length Sequences with its\nApplication to Neural Network Language Models.\narXiv:1505.01504.\nShiliang Zhang and Hui Jiang. 2015. Hybrid Orthog-\nonal Projection and Estimation (HOPE): A New\nFramework to Probe and Learn Neural Networks.\narXiv:1502.00702.\nShiliang Zhang, Hui Jiang and Lirong Dai. 2015b. The\nNew HOPE Way to Learn Neural Networks. Proc.\nof Deep Learning Workshop at ICML 2015.\n500",
  "topic": "Forgetting",
  "concepts": [
    {
      "name": "Forgetting",
      "score": 0.8478318452835083
    },
    {
      "name": "Encoding (memory)",
      "score": 0.6375898122787476
    },
    {
      "name": "Zhàng",
      "score": 0.6369447708129883
    },
    {
      "name": "Computer science",
      "score": 0.6327804327011108
    },
    {
      "name": "Artificial neural network",
      "score": 0.5839716196060181
    },
    {
      "name": "Computational linguistics",
      "score": 0.518193781375885
    },
    {
      "name": "Natural language processing",
      "score": 0.46845293045043945
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46216297149658203
    },
    {
      "name": "Natural language",
      "score": 0.41456156969070435
    },
    {
      "name": "Cognitive science",
      "score": 0.3723922073841095
    },
    {
      "name": "Linguistics",
      "score": 0.3641888499259949
    },
    {
      "name": "Philosophy",
      "score": 0.1819283664226532
    },
    {
      "name": "China",
      "score": 0.1181466281414032
    },
    {
      "name": "Psychology",
      "score": 0.11195772886276245
    },
    {
      "name": "Political science",
      "score": 0.10660076141357422
    },
    {
      "name": "Law",
      "score": 0.08013403415679932
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I192455969",
      "name": "York University",
      "country": "CA"
    }
  ],
  "cited_by": 55
}