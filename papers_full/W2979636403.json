{
    "title": "Transformers without Tears: Improving the Normalization of Self-Attention",
    "url": "https://openalex.org/W2979636403",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287379581",
            "name": "Nguyen, Toan Q.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3085962355",
            "name": "Salazar Julián",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1815076433",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W2962801832",
        "https://openalex.org/W2796108585",
        "https://openalex.org/W2963410064",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W2970903692",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2885724687",
        "https://openalex.org/W222053410",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2964085268",
        "https://openalex.org/W2994689640",
        "https://openalex.org/W2919290281",
        "https://openalex.org/W2963807318",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2955227499",
        "https://openalex.org/W2911291251",
        "https://openalex.org/W2963418779",
        "https://openalex.org/W2594833348",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2888519496",
        "https://openalex.org/W2963399222",
        "https://openalex.org/W2302255633",
        "https://openalex.org/W2962911098",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2912555327",
        "https://openalex.org/W2905927205",
        "https://openalex.org/W2581377246",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W2963086938",
        "https://openalex.org/W2962761235",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W2887920589"
    ],
    "abstract": "We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PreNorm) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose $\\ell_2$ normalization with a single scale parameter (ScaleNorm) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FixNorm). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT'15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT'14 English-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades performance.",
    "full_text": "Transformers without Tears:\nImproving the Normalization of Self-Attention\nToan Q. Nguyen∗\nUniversity of Notre Dame†\ntnguye28@nd.edu\nJulian Salazar∗\nAmazon AWS AI\njulsal@amazon.com\nAbstract\nWe evaluate three simple, normalization-\ncentric changes to improve Transformer\ntraining. First, we show that pre-norm residual\nconnections (P RENORM ) and smaller initial-\nizations enable warmup-free, validation-based\ntraining with large learning rates. Second,\nwe propose ℓ2 normalization with a single\nscale parameter (S CALE NORM ) for faster\ntraining and better performance. Finally, we\nreafﬁrm the effectiveness of normalizing word\nembeddings to a ﬁxed length (FIXNORM ). On\nﬁve low-resource translation pairs from TED\nTalks-based corpora, these changes always\nconverge, giving an average +1.1 BLEU\nover state-of-the-art bilingual baselines and\na new 32.8 BLEU on IWSLT '15 English-\nVietnamese. We observe sharper performance\ncurves, more consistent gradient norms, and a\nlinear relationship between activation scaling\nand decoder depth. Surprisingly, in the high-\nresource setting (WMT '14 English-German),\nSCALE NORM and FIXNORM remain compet-\nitive but P RENORM degrades performance.\nPreprocessing scripts and code are released at\nhttps://github.com/tnq177/\ntransformers_without_tears.\n1 Introduction\nThe Transformer (Vaswani et al., 2017) has be-\ncome the dominant architecture for neural ma-\nchine translation (NMT) due to its train-time\nparallelism and strong downstream performance.\nVarious modiﬁcations have been proposed to im-\nprove the efﬁciency of its multi-head attention\nand feedforward sublayers (Guo et al., 2019;\nSukhbaatar et al., 2019). Our work focuses on\nlayer normalization (LAYER NORM ) (Ba et al.,\n2015), which we show has an outsized role in the\n∗Equal contribution.\n†Work done during an internship at Amazon AWS AI.\nconvergence and performance of the Transformer\nin two ways:\nPlacement of normalization. The origi-\nnal Transformer uses post-norm residual units\n(POST NORM ), where layer normalization occurs\nafter the sublayer and residual addition. However,\nChen et al. (2018) found that pre-norm residual\nunits (PRENORM ), where layer normalization oc-\ncurs immediately before the sublayer, were instru-\nmental to their model’s performance. Wang et al.\n(2019) compare the two, showing that P RENORM\nmakes backpropagation more efﬁcient over depth\nand training Transformers with deep, 30-layer en-\ncoders.\nOur work demonstrates additional conse-\nquences in the base ( ≤6-layer encoder) Trans-\nformer regime. We show that P RENORM enables\nwarmup-free, validation-based training with large\nlearning rates even for small batches, in contrast\nto past work on scaling NMT (Ott et al., 2018).\nWe also partly reclaim P OST NORM ’s stability\nvia smaller initializations, although P RENORM is\nless sensitive to this magnitude and can improve\nperformance. However, despite P RENORM ’s re-\ncent adoption in many NMT frameworks, we\nﬁnd it degrades base Transformer performance on\nWMT '14 English-German.\nChoice of normalization. Santurkar et al. (2018)\nshow that batch normalization’s effectiveness is\nnot from reducing internal covariate shift, but from\nsmoothing the loss landscape. They achieve simi-\nlar or better performance with non-variance-based\nnormalizations in image classiﬁcation. Hence, we\npropose replacing L AYER NORM with the simpler\nscaled ℓ2 normalization (SCALE NORM ), which\nnormalizes activation vectors to a single learned\nlength g. This is both inspired by and synergis-\ntic with jointly ﬁxing the word embedding lengths\n(FIXNORM ) (Nguyen and Chiang, 2018). These\narXiv:1910.05895v2  [cs.CL]  30 Dec 2019\nchanges improve the training speed and low-\nresource performance of the Transformer without\naffecting high-resource performance.\nOn ﬁve low-resource pairs from the TED\nTalks (Qi et al., 2018) and IWSLT '15 (Cettolo\net al., 2015) corpora, we ﬁrst train state-of-the-\nart Transformer models (+4.0 BLEU on average\nover the best published NMT bitext-only num-\nbers). We then apply P RENORM , FIXNORM , and\nSCALE NORM for an average total improvement\nof +1.1 BLEU, where each addition contributes at\nleast +0.3 BLEU (Section 3), and attain a new 32.8\nBLEU on IWSLT '15 English-Vietnamese. We\nvalidate our intuitions in Section 4 by showing\nsharper performance curves (i.e., improvements\noccur at earlier epochs) and more consistent gra-\ndient norms. We also examine the per-sublayer\ng’s learned by S CALE NORM , which suggest fu-\nture study.\n2 Background\n2.1 Identity mappings for transformers\nResidual connections(He et al., 2016a) were ﬁrst\nintroduced to facilitate the training of deep con-\nvolutional networks, where the output of the ℓ-th\nlayer Fℓ is summed with its input:\nxℓ+1 = xℓ + Fℓ(xℓ). (1)\nThe identity term xℓ is crucial to greatly extending\nthe depth of such networks (He et al., 2016b). If\none were to scalexℓ by a scalarλℓ, then the contri-\nbution of xℓ to the ﬁnal layer FL is (∏L−1\ni=ℓ λi)xℓ.\nFor deep networks with dozens or even hundreds\nof layers L, the term ∏L−1\ni=ℓ λi becomes very large\nif λi > 1 or very small if λi < 1, for enough i.\nWhen backpropagating from the last layer Lback\nto ℓ, these multiplicative terms can cause explod-\ning or vanishing gradients, respectively. Therefore\nthey ﬁx λi = 1, keeping the total residual path an\nidentity map.\nThe original Transformer applies L AYER -\nNORM after the sublayer and residual addition\n(POST NORM ):\nxℓ+1 = LAYER NORM (xℓ + Fℓ(xℓ)). (2)\nWe conjecture this has caused past convergence\nfailures (Popel and Bojar, 2018; Shazeer and\nStern, 2018), with L AYER NORM s in the resid-\nual path acting similarly to λi ̸= 1; furthermore,\nwarmup was needed to let L AYER NORM safely\nadjust scale during early parts of training. Inspired\nby He et al. (2016b), we apply L AYER NORM im-\nmediately before each sublayer (PRENORM ):\nxℓ+1 = xℓ + Fℓ(LAYER NORM (xℓ)). (3)\nThis is cited as a stabilizer for Transformer train-\ning (Chen et al., 2018; Wang et al., 2019) and is\nalready implemented in popular toolkits (Vaswani\net al., 2018; Ott et al., 2019; Hieber et al.,\n2018), though not necessarily used by their de-\nfault recipes. Wang et al. (2019) make a similar\nargument to motivate the success of P RENORM\nin training very deep Transformers. Note that\none must append an additional normalization after\nboth encoder and decoder so their outputs are ap-\npropriately scaled. We compare P OST NORM and\nPRENORM throughout Section 3.\n2.2 Weight initialization\nXavier normal initialization (Glorot and Bengio,\n2010) initializes a layer’s weightsWℓ ∈Rdℓ+1×dℓ\n(dℓ is the hidden dimension) with samples from a\ncentered normal distribution with layer-dependent\nvariance:\n(Wℓ)i,j ∼N\n(\n0,\n√\n2\ndℓ + dℓ+1\n)\n. (4)\nOur experiments with this default initializer ﬁnd\nthat POST NORM sometimes fails to converge, es-\npecially in our low-resource setting, even with a\nlarge number of warmup steps. One explanation is\nthat Xavier normal yields initial weights that are\ntoo large. In implementations of the Transformer,\none scales the word embeddings by a large value\n(e.g.,\n√\nd≈22.6 for d= 512), giving vectors with\nan expected square norm of d. L AYER NORM ’s\nunit scale at initialization preserves this same ef-\nfect. Since feedforward layers already have their\nweights initialized to a smaller standard deviation,\ni.e.,\n√\n2\nd+4d, we propose reducing the attention\nlayers’ initializations from\n√\n2\nd+d to\n√\n2\nd+4d as\nwell (SMALL INIT ), as a corresponding mitigation.\nWe evaluate the effect of this on P OST NORM vs.\nPRENORM in Section 3.1.\n2.3 Scaled ℓ2 normalization and FIXNORM\nLAYER NORM is inspired by batch normalization\n(Ioffe and Szegedy, 2015), both of which aim to\nreduce internal covariate shift by ﬁxing the mean\nand variance of activation distributions. Both\nhave been applied to self-attention (Vaswani et al.,\n2017; Kool et al., 2019). However, Santurkar et al.\n(2018) show that batch normalization’s success\nhas little to do with covariate shift, but comes in-\nstead from smoothing the loss landscape. For ex-\nample, they divide by the pre-centeredℓp norm in-\nstead of the variance and achieve similar or better\nresults in image classiﬁcation.\nHence, we propose replacing L AYER NORM\nwith scaled ℓ2 normalization:\nSCALE NORM (x; g) =g x\n||x||. (5)\nThis can be viewed as projecting d-dimensional\nvectors onto a (d −1)-dimensional hypersphere\nwith learned radius g. This expresses the inductive\nbias that each sublayer’s activations has an ideal\n“global scale,” a notion we empirically validate in\nSection 4.2. S CALE NORM replaces the 2d scale\nand shift parameters of L AYER NORM with a sin-\ngle learned scalar, improving computational and\nparameter efﬁciency while potentially regularizing\nthe loss landscape.\nThis bias has an explicit interpretation at the ﬁ-\nnal layer: large inner products sharpen the output\ndistribution, causing frequent words to dispropor-\ntionately dominate rare words. This led Nguyen\nand Chiang (2018) to introduce F IXNORM (w) =\ng w\n||w|| with ﬁxed gat the last linear layer, to max-\nimize the angular difference of output represen-\ntations and aid rare word translation. By mak-\ning g learnable, we can apply S CALE NORM and\nFIXNORM jointly, which means applying the fol-\nlowing at the ﬁnal linear layer:\n(SCALE NORM +FIXNORM )(x,w; g)\n= g w ·x\n||w||||x||. (6)\nNote that this combination at the last layer is\nequivalent to cosine normalization (Luo et al.,\n2018) with a learned scale.\n2.4 Learning rates\nDespite using an adaptive optimizer, Adam\n(Kingma and Ba, 2015), Transformer training\nuses a learning rate (LR) schedule with a lin-\near warmup and an inverse square root decay\n(INVSQRTDECAY):\nLR(n) = λ√\nd\nmin\n(\n1√n, n\nn1.5warmup\n)\n, (7)\nwhere d is the hidden dimension of the self-\nattention layers, and λ, nwarmup are hyperpa-\nrameters that determine the highest learning rate\nachieved and the number of steps to reach it, re-\nspectively. These two hyperparameters have been\nthe subject of much empirical study (Popel and\nBojar, 2018; Ott et al., 2018). In light of our mod-\niﬁcations however, we revisit various aspects of\nthis schedule:\nWarmup-free training. We conjectured that\nwarmup is primarily needed when using P OST-\nNORM to gradually learn L AYER NORM param-\neters without gradient explosion/vanishing (Sec-\ntion 2.1). Hence, we evaluate both PRENORM and\nPOST NORM without warmup in Section 3.3.\nLarge learning rates. To speed up training, one\noften explores using larger learning rates. In the\ncontext of Transformer, Ott et al. (2018) and Aha-\nroni et al. (2019) take λ ∈{2,3}instead of the\nconventional λ = 1. Ott et al. (2018) showed\nthat one can scale up Adam’s learning rate to\n10−3 with an extremely large batch (400k tokens).\nHowever, the improved convergence provided by\nour modiﬁcations could enable higher learning\nrates with much small batch sizes (4k tokens), as\nexamined in Section 3.3.\nValidation-based decay. For similar reasons, one\nmight wish to adopt a classic validation-based de-\ncay, i.e., training at a high learning rate for as\nlong as tenable, decaying rapidly when develop-\nment scores ﬂatline. This has inspired usage of\nﬁxed decay schemes upon convergence with I N-\nVSQRTDECAY (Dong et al., 2018; Salazar et al.,\n2019). We revisit V ALDECAY under our modiﬁ-\ncations, where we still perform a linear warmup\nbut then multiply by a scale αdecay <1 when per-\nformance on a development set does not improve\nover patienceevaluations.\n3 Experiments and results\nWe train Transformer models for a diverse set of\nﬁve low-resource translation pairs from the TED\nTalks (Qi et al., 2018) and the IWSLT '15 (Cet-\ntolo et al., 2015) corpora. Details are summarized\nin Table 1. For more information motivating our\nchoice of pairs and for exact training details, refer\nto Appendix A.\n# egs. # src. + tgt. toks. # iters./epoch max. epoch # enc./dec. layers # heads/layer dropout # BPE\ngl→en 10k 0.37M 100 1000 4 4 0.4 3k\nsk→en 61k 2.32M 600 200 6 8 0.3 8k\nen→vi 133k 5.99M 1500 200 6 8 0.3 8k\nen→he 212k 7.88M 2000 200 6 8 0.3 8k\nar→en 214k 8.09M 2000 200 6 8 0.3 8k\nTable 1: Data and model properties for low-resource NMT.en→vi is from IWSLT 2015; the rest are from the TED\nTalks corpus.\n3.1 Large vs. small initialization\nTo see the impact of weight initialization, we run\ntraining on the en→vi dataset using warmup steps\nof 4k, 8k, 16k (Table 2). With default initializa-\ntion, POST NORM fails to converge on this dataset\neven with a long warmup of 16k steps, only reach-\ning 5.76 BLEU.\nXavier normal # warmup steps\n4k 8k 16k\nBaseline POST NORM fail fail 5.76\nPRENORM 28.52 28.73 28.32\nSMALL INIT POST NORM 28.17 28.20 28.62\nPRENORM 28.26 28.44 28.33\nTable 2: Development BLEU on en→vi using Xavier\nnormal initialization (baseline versus SMALL INIT ).\nThe second row shows that taking a smaller\nstandard deviation on the attention weights\n(SMALL INIT ) restores convergence to P OST-\nNORM . Though the\n√\n2/5 ≈ 0.63 adjustment\nused here seems marginal, operations like residual\nconnections and the products between queries and\nkeys can compound differences in scale. Though\nboth models now achieve similar performance, we\nnote that P RENORM works in all setups, sug-\ngesting greater stability during training. For all\nremaining experiments, we use P OST NORM and\nPRENORM with SMALL INIT . We ﬁnd this choice\ndoes not affect the performance of PRENORM .\n3.2 Scaled ℓ2 normalization and FIXNORM\nTo compare SCALE NORM and LAYER NORM , we\ntake 8k warmup steps for all further experiments.\nSince we tie the target input word embedding\nand the last linear layer’s weight (Appendix A),\nFIXNORM is implemented by applying ℓ2 nor-\nmalization to the word embedding, with each\ncomponent initialized uniformly in [−0.01,0.01].\nFor non-FIXNORM models, word embeddings are\ninitialized with mean 0 and standard deviation√\n1/d so they sum to unit variance. All g’s in\nSCALE NORM are initialized to\n√\nd.\nTable 3 shows our results along with some pub-\nlished baselines. First, note that our Transformer\nbaselines with P OST NORM + L AYER NORM (1)\nare very strong non-multilingual NMT models on\nthese pairs. They outperform the best published\nnumbers, which are all Transformer models in the\npast year, by an average margin of +4.0 BLEU.\nThen, we see that PRENORM (2) achieves compa-\nrable or slightly better results than POST NORM on\nall tasks. F IXNORM (3) gives an additional gain,\nespecially on ar→en (p< 0.01).\nFinally, we replace L AYER NORM with\nSCALE NORM (4). S CALE NORM signiﬁcantly\nimproves on L AYER NORM for two very low-\nresource pairs, gl →en and sk →en. On the other\ntasks, it performs comparably to L AYER NORM .\nUpon aggregating all changes, our ﬁnal model\nwith SCALE NORM and FIXNORM improves over\nour strong baseline with P OST NORM on all tasks\nby an average of +1.1 BLEU ( p < 0.01), with\neach change contributing an average of at least\n+0.3 BLEU. In Section 4.2 and Appendix B, we\nfurther examine where the performance gains of\nSCALE NORM come from.\nMoreover, S CALE NORM is also faster than\nLAYER NORM . Recall that for each vector of size\nd, L AYER NORM needs to compute mean, stan-\ndard deviation, scaling, and shifting, which costs\nO(7d) operations. For S CALE NORM , we only\nneed O(3d) operations to perform normalization\nand global scaling. This does not account for\nfurther gains due to reduction in parameters. In\nour implementation, training with S CALE NORM\nis around 5% faster than with LAYER NORM , sim-\nilar to the speedups on NMT observed by Zhang\nand Sennrich (2019)’s RMSNORM (which can be\nviewed as S CALE NORM with per-unit scales; see\nSection 4.2).\ngl→en sk→en en→vi en→he ar→en average ∆\nPOST NORM + LAYER NORM (published) 16.2 24.0 29.09 23.66 27.84 -4.05\nPOST NORM + LAYER NORM (1) 18.47 29.37 31.94 27.85 33.39 +0.00\nPRENORM + LAYER NORM (2) 19.09 29.45 31.92 28.13 33.79 +0.27\nPRENORM + FIXNORM + LAYER NORM (3) 19.38 29.50 32.45 28.39 34.35† +0.61\nPRENORM + FIXNORM + SCALE NORM (4) 20.91‡∗ 30.25‡∗ 32.79∗ 28.44∗ 34.15∗ +1.10\nTable 3: Test BLEU using POST NORM or PRENORM and different normalization techniques. Published values are\nfrom Wang et al. (2018); Neubig and Hu (2018); Aharoni et al. (2019).†, ‡and ∗indicate signiﬁcant improvement\nof (3) over (2), (4) over (3), and (4) over (1), respectively;p< 0.01 via bootstrap resampling (Koehn, 2004).\ngl→en sk→en en→vi en→he ar→en\nNOWARMUP 18.00 28.92 28.91 30.33 35.40\nINVSQRT DECAY 22.18 29.08 28.84 30.30 35.33\nVALDECAY 21.45 29.46 28.67 30.69 35.46\nINVSQRT DECAY + 2×LR 21.92 29.03 28.76 30.50 35.33\nVALDECAY + 2×LR 21.63 29.49 28.46 30.13 34.95\nTable 4: Development BLEU for P RENORM + F IXNORM + S CALE NORM , trained with different learning rate\nschedulers.\n3.3 Learning rates\nWe compare the original learning rate schedule\nin equation 7 (I NVSQRT DECAY) with validation-\nbased decay (V ALDECAY), possibly with no\nwarmup (N OWARMUP ). We use λ = 1 ,\nnwarmup = 8k for INVSQRTDECAY and VALDE-\nCAY. For N OWARMUP , we instead use a learning\nrate of 3 ·10−4 for all datasets. For both VALDE-\nCAY and NOWARMUP , we take αdecay = 0.8 and\npatience = 3. For experiments with high learn-\ning rate, we use either V ALDECAY or I NVSQRT-\nDECAY with λ = 2 (giving a peak learning rate\nof ≈ 10−3). All experiments use P RENORM +\nFIXNORM + SCALE NORM .\nIn Table 4, we see that N OWARMUP performs\ncomparably to I NVSQRTDECAY and VALDECAY\nexcept on gl→en. We believe that in general, one\ncan do without warmup, though it remains useful\nin the lowest resource settings. In our 2 ×LR ex-\nperiments, we can still attain a maximum learning\nrate of 10−3 without disproportionately overﬁtting\nto small datasets like gl→en.\nOne might hypothesize that V ALDECAY con-\nverges more quickly to better minima than I N-\nVSQRT DECAY by staying at high learning rates\nfor longer. However, both schedulers achieve sim-\nilar results with or without doubling the learning\nrate. This may be due to the tail-end behavior\nof VALDECAY methods, which can involve mul-\ntiplicative decays in rapid succession. Finally, our\n2×LR experiments, while not yielding better per-\nformance, show that PRENORM allows us to train\nthe Transformer with a very high learning rate de-\nspite small batches (4k tokens).\nSince PRENORM can train without warmup, we\nwonder if P OST NORM can do the same. We run\nexperiments on en→vi with N OWARMUP , vary-\ning the number of encoder/decoder layers. As\nseen in Table 5, P OST NORM often fails with-\nout warmup even with 5 or 6 layers. Even at 4\nlayers, one achieves a subpar result compared to\nPRENORM . This reafﬁrms Section 3.1 in showing\nthat P RENORM is more stable than P OST NORM\nunder different settings.\n4 layers 5 layers 6 layers\nPOST NORM 18.31 fails fails\nPRENORM 28.33 28.13 28.32\nTable 5: Development BLEU on en→vi using\nNOWARMUP , as number of encoder/decoder layers in-\ncreases.\n3.4 High-resource setting\nSince all preceding experiments were in low-\nresource settings, we examine if our claims hold\nin a high-resource setting. We train the Trans-\nformer base model on WMT '14 English-German\nusing FAIRSEQ and report tokenized BLEU scores\non newstest2014. Implementation of our methods\nin FAIRSEQ can be found in Appendix C.\nIn Table 6, S CALE NORM and F IXNORM\nachieve equal or better results than LAYER NORM .\nSince S CALE NORM is also faster, we recom-\nmend using both as drop-in replacements for\nLAYER NORM in all settings. Surprisingly, in\nthis task P OST NORM works notably better than\nPRENORM ; one observes similar behavior in\nWang et al. (2019). We speculate this is related\nto identity residual networks acting like shallow\nensembles (Veit et al., 2016) and thus undermin-\ning the learning of the longest path; further study\nis required.\nnewstest2014\nPOST NORM + LAYER NORM (published) 27.3\nPRENORM + LAYER NORM 26.83\nPRENORM + FIXNORM + SCALE NORM 27.07\nPOST NORM + LAYER NORM 27.58\nPOST NORM + FIXNORM + SCALE NORM 27.57\nTable 6: BLEU scores from WMT '14 English-to-\nGerman. Published value is from Vaswani et al. (2017).\n4 Analysis\n4.1 Performance curves\nFigure 1 shows that P RENORM not only learns\nfaster than P OST NORM , but also outperforms\nit throughout training. Adding F IXNORM also\ngives faster learning at ﬁrst, but only achieves\nclose performance to that with P RENORM and\nno F IXNORM . However, once paired with\nSCALE NORM , we attain a better BLEU score at\nthe end. Because of the slow warmup period,\nSCALE NORM with warmup learns slower than\nSCALE NORM without warmup initially; however,\nthey all converge at about the same rate.\n20 40 60 80 100\nepochs\n18\n20\n22\n24\n26\n28\n30Dev BLEU\nEnglish-Vietnamese development BLEU\nPreNorm+ScaleNorm+FixNorm+NoWarmup\nPreNorm+ScaleNorm+FixNorm\nPreNorm+LayerNorm+FixNorm\nPreNorm+LayerNorm\nPostNorm+LayerNorm\nFigure 1: Development BLEU on en→vi with P OST-\nNORM or P RENORM , and with L AYER NORM or\nSCALE NORM .\nTo visualize how P RENORM helps backpropa-\ngation, we plot the global gradient norms from our\nruns in Figure 2. POST NORM produces noisy gra-\ndients with many sharp spikes, even towards the\nend of training. On the other hand, P RENORM\nhas fewer noisy gradients with smaller sizes, even\nwithout warmup. L AYER NORM has lower global\nnorms than S CALE NORM + FIXNORM but it has\nmore gradient components corresponding to nor-\nmalization.\n0 200 400 600 800 1000 1200\niteration (x100)\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nGlobal norm (log scale)\nGradient global norm\nPostNorm+LayerNorm\nPreNorm+ScaleNorm+FixNorm+NoWarmup\nPreNorm+ScaleNorm+FixNorm\nPreNorm+LayerNorm\nFigure 2: The global norm of gradients when using\nPOST NORM or P RENORM , and with L AYER NORM ,\nSCALE NORM and FIXNORM . Best viewed in color.\n4.2 Activation scaling and the role of g\nOne motivation for S CALE NORM was that it ex-\npressed a good inductive bias for the global scaling\nof activations, independent of distributional stabil-\nity (Section 2.3). In contrast, a contemporaneous\nwork (Zhang and Sennrich, 2019) proposes root\nmean square layer normalization(RMSN ORM ),\nwhich still follows layer normalization’s motiva-\ntion but reduces overhead by forgoing additive\nadjustments, using only a scaling gi per activa-\ntion ai. Despite their differing motives, tying the\ngi of RMSN ORM and dividing by\n√\nd retrieves\nSCALE NORM .\nHence we can frame our comparisons in terms\nof number of learnable parameters. We rerun our\nPRENORM experiments with RMSN ORM . We\nalso consider ﬁxing g =\n√\nd for S CALE NORM ,\nwhere only F IXNORM has learnable g. Table 7\nshows that S CALE NORM always performs com-\nparably or better than RMSN ORM . Surprisingly,\nthe ﬁxed-gmodel performs comparably to the one\nwith learnable g. However, at higher learning rates\n(VALDECAY with and without 2 ×LR), ﬁxed- g\nmodels perform much worse on ar →en, en →he\nand en→vi. We conjecture that learning g is re-\ngl→en sk→en en→vi en→he ar→en\nRMSN ORM + FIXNORM 20.92 30.36 32.54 28.29 33.67\nSCALE NORM + FIXNORM 20.91 30.25 32.79 28.44 34.15\nSCALE NORM (g=\n√\nd) + FIXNORM (learned) 21.18 30.36 32.66 28.19 34.11\nSCALE NORM (g=\n√\nd) + FIXNORM (learned) + VALDECAY 20.36 30.45 32.83 27.97 33.98\nSCALE NORM (g=\n√\nd) + FIXNORM (learned) + VALDECAY + 2×LR 21.15 30.57 31.81 25.00 28.92\nTable 7: Test BLEU of ℓ2-based normalization techniques with different numbers of learned g: O(Ld) vs. O(L)\nvs. O(1).\n11\n12\n13\n14\n15\n16\n17\n18g\nencoder decoder decoder-encoder\nValue of g for attention layers in encoder/decoder\nar en\nen he\nen vi\n10\n15\n20\n25\n30\n35g\nencoder decoder\nValue of g for non-attention layers in encoder/decoder\nar en\nen he\nen vi\nFigure 3: Learned g values for P RENORM + S CALE NORM + F IXNORM models, versus depth. Left: Attention\nsublayers (decoder-encoder denotes decoder sublayers attending on the encoder). Right: Feedforward sublayers\nand the ﬁnal linear layer.\n13\n14\n15\n16\n17\n18g\nencoder decoder decoder-encoder\nValue of g for attention layers in encoder/decoder\nLabel smoothing\nNo label smoothing\n15\n20\n25\n30\n35\n40g\nencoder decoder\nValue of g for non-attention layers in encoder/decoder\nLabel smoothing\nNo label smoothing\nFigure 4: Learned gvalues for our PRENORM + SCALE NORM + FIXNORM en→vi model (with and without label\nsmoothing), versus depth. Left and Right are the same as in Figure 3.\nquired to accommodate layer gradients.\nIn Figure 3, we plot the learned g values for\npairs with 100k+ examples. For all but the\ndecoder-encoder sublayers, we observe a positive\ncorrelation between depth and g, giving credence\nto S CALE NORM ’s inductive bias of global scal-\ning. This trend is clearest in the decoder, where\nglinearly scales up to the output layer, perhaps in\ntandem with the discriminativeness of the hidden\nrepresentations (Liang et al., 2018). We also note a\nnegative correlation between the number of train-\ning examples and the magnitude of gfor attention\nsublayers, which may reﬂect overﬁtting.\nFinally, to afﬁrm our intuition for interpretingg,\nwe plot gvalues with and without label smoothing\n(Figure 4). We see a difference in later layers of\nthe decoder; there, removing label smoothing re-\nsults in lower g values except at the output layer,\nwhere gincreases sharply. This corresponds to the\nknown overconﬁdence of translation models’ log-\nits, on which label smoothing has a downscaling\neffect (M¨uller et al., 2019).\n5 Conclusion\nIn this work, we presented three simple,\nnormalization-centric changes to the Transformer\nmodel, with a focus on NMT. First, we show\nthat while P OST NORM performs better for high-\nresource NMT in the original base Transformer\nregime, P RENORM is both more stable and\nmore competent in low-resource settings. Sec-\nond, we propose replacing L AYER NORM with\nSCALE NORM , a fast and effective scaled ℓ2 nor-\nmalization technique which requires only a sin-\ngle learned parameter. Finally, we reafﬁrm the\neffectiveness of ﬁxing the word embedding norm\n(FIXNORM ). Altogether, PRENORM + FIXNORM\n+ S CALE NORM signiﬁcantly improves NMT on\nlow-resource pairs, with the latter two performing\ncomparably in the high-resource setting, but faster.\nIn the future, we would like to investigate the\nrelationship between POST NORM and PRENORM\nwhen using other optimizers such as RA DAM\n(Liu et al., 2019), which has been shown to\nimprove Transformer training without warmup.\nWe are also interested in seeing if F IXNORM or\nSCALE NORM at the ﬁnal linear layer remains ef-\nfective when paired with an initialization method\nsuch as FIXUP (Zhang et al., 2019), which enables\nthe training of deep neural networks without nor-\nmalization. One could also explore using other ℓp\nnorms (Santurkar et al., 2018).\nAcknowledgements\nThe authors would like to thank David Chiang and\nKatrin Kirchhoff for their support of this research.\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively Multilingual Neural Machine Transla-\ntion. In NAACL-HLT, pages 3874–3884.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. 2015. Layer Normalization. CoRR,\nabs/1607.06450.\nMauro Cettolo, Jan Niehues, Luisa Bentivogli,\nRoldano Cattoni, and Marcello Federico. 2015. The\nIWSLT 2015 Evaluation Campaign. In IWSLT,\npages 3–4.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Niki Parmar, Noam Shazeer, Ashish Vaswani,\nJakob Uszkoreit, Lukasz Kaiser, Mike Schuster,\nZhifeng Chen, Yonghui Wu, and Macduff Hughes.\n2018. The best of both worlds: Combining recent\nadvances in neural machine translation. In ACL,\npages 76–86.\nLinhao Dong, Shuang Xu, and Bo Xu. 2018.\nSpeech-Transformer: A No-Recurrence Sequence-\nto-Sequence Model for Speech Recognition. In\nICASSP, pages 5884–5888.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In AISTATS, pages 249–256.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,\nXiangyang Xue, and Zheng Zhang. 2019. Star-\nTransformer. In NAACL-HLT, pages 1315–1325.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016a. Deep residual learning for image recog-\nnition. In CVPR, pages 770–778.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016b. Identity Mappings in Deep Residual\nNetworks. In ECCV, pages 630–645.\nFelix Hieber, Tobias Domhan, Michael Denkowski,\nDavid Vilar, Artem Sokolov, Ann Clifton, and Matt\nPost. 2018. The Sockeye neural machine translation\ntoolkit. In AMTA, pages 200–207.\nSergey Ioffe and Christian Szegedy. 2015. Batch Nor-\nmalization: Accelerating Deep Network Training by\nReducing Internal Covariate Shift. In ICML, pages\n448–456.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nMethod for Stochastic Optimization. In ICLR.\nPhilipp Koehn. 2004. Statistical signiﬁcance tests for\nmachine translation evaluation. In EMNLP, pages\n388–395.\nWouter Kool, Herke Van Hoof, and Max Welling. 2019.\nAttention, Learn to Solve Routing Problems! In\nICLR.\nDavis Liang, Zhiheng Huang, and Zachary C. Lipton.\n2018. Learning noise-invariant representations for\nrobust speech recognition. In SLT, pages 56–63.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2019. On the variance of the adaptive learning rate\nand beyond. CoRR, abs/1908.03265.\nChunjie Luo, Jianfeng Zhan, Lei Wang, and Qiang\nYang. 2018. Cosine Normalization: Using Cosine\nSimilarity Instead of Dot Product in Neural Net-\nworks. In ICANN, pages 382–391.\nRafael M¨uller, Simon Kornblith, and Geoffrey Hinton.\n2019. When Does Label Smoothing Help? In\nNeurIPS.\nGraham Neubig and Junjie Hu. 2018. Rapid Adap-\ntation of Neural Machine Translation to New Lan-\nguages. In EMNLP, pages 875–880.\nToan Nguyen and David Chiang. 2018. Improving\nLexical Choice in Neural Machine Translation. In\nNAACL-HLT, pages 334–343.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A Fast, Extensible\nToolkit for Sequence Modeling. In NAACL-HLT\n(Demonstrations), pages 48–53.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling Neural Machine Trans-\nlation. In WMT, pages 1–9.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In ACL, pages 311–\n318.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2013. On the difﬁculty of training recurrent neural\nnetworks. In ICML, pages 1310–1318.\nGabriel Pereyra, George Tucker, Jan Chorowski,\nLukasz Kaiser, and Geoffrey E. Hinton. 2017. Reg-\nularizing neural networks by penalizing conﬁdent\noutput distributions. In ICLR (Workshop).\nMartin Popel and Ondej Bojar. 2018. Training Tips\nfor the Transformer Model. Prague Bull. Math. Lin-\nguistics, 110(1):43–70.\nOﬁr Press and Lior Wolf. 2017. Using the Output Em-\nbedding to Improve Language Models. In EACL,\npages 157–163.\nYe Qi, Devendra Sachan, Matthieu Felix, Sarguna Pad-\nmanabhan, and Graham Neubig. 2018. When and\nWhy Are Pre-Trained Word Embeddings Useful for\nNeural Machine Translation? In NAACL-HLT,\npages 529–535.\nJulian Salazar, Katrin Kirchhoff, and Zhiheng Huang.\n2019. Self-attention Networks for Connectionist\nTemporal Classiﬁcation in Speech Recognition. In\nICASSP, pages 7115–7119.\nShibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and\nAleksander Madry. 2018. How does batch normal-\nization help optimization? In NeurIPS, pages 2483–\n2493.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Edinburgh Neural Machine Translation Sys-\ntems for WMT 16. In WMT, pages 371–376.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural machine translation of rare words\nwith subword units. In ACL.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive Learning Rates with Sublinear Memory\nCost. In ICML, pages 4603–4611.\nSainbayar Sukhbaatar, Edouard Grave, Guillaume\nLample, Herve Jegou, and Armand Joulin. 2019.\nAugmenting Self-attention with Persistent Memory.\nCoRR, abs/1907.01470.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe Inception Architecture for Computer Vision. In\nCVPR, pages 2818–2826.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan N Gomez, Stephan Gouws,\nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki\nParmar, Ryan Sepassi, Noam Shazeer, and Jakob\nUszkoreit. 2018. Tensor2Tensor for Neural Machine\nTranslation. In AMTA, pages 193–199.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NeurIPS, pages 5998–6008.\nAndreas Veit, Michael Wilber, and Serge Belongie.\n2016. Residual networks behave like ensembles of\nrelatively shallow networks. NeurIPS, pages 550–\n558.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F Wong, and Lidia S Chao.\n2019. Learning Deep Transformer Models for Ma-\nchine Translation. In ACL, pages 1810–1822.\nXinyi Wang, Hieu Pham, Zihang Dai, and Graham\nNeubig. 2018. Switchout: an efﬁcient data augmen-\ntation algorithm for neural machine translation. In\nEMNLP, pages 856–861.\nBiao Zhang and Rico Sennrich. 2019. Root Mean\nSquare Layer Normalization. NeurIPS.\nHongyi Zhang, Yann N. Dauphin, and Tengyu Ma.\n2019. Fixup initialization: Residual learning with-\nout normalization. In ICLR.\nA Training details\nData and preprocessing. The pairs are English\n(en) to Hebrew (he), Vietnamese (vi), and Galician\n(gl), Slovak (sk), Arabic (ar) to English (en). Be-\ncause the data is already preprocessed, we only ap-\nply BPE (Sennrich et al., 2016b) withfastBPE1.\nDepending on the data size, we use different num-\nbers of BPE operations.\nWe wanted to compare with the latest low-\nresource works of (Neubig and Hu, 2018; Aharoni\net al., 2019) on the TED Talks corpus (Qi et al.,\n2018). In particular, Aharoni et al. (2019) identi-\nﬁed 4 very low-resource pairs (<70k); we took the\ntwo (gl→en, sk→en) that were not extremely low\n(≤6k). They then identiﬁed 4 low-resource pairs\nwith 100k-300k examples; we took the top two\n(ar→en, en→he). To introduce a second English-\nsource pair and to showcase on a well-understood\ntask, we used theen→vi pair from IWSLT '15 with\nan in-between number of examples (133k). In\nthis way, we have examples of different resource\nlevels, language families, writing directions, and\nEnglish-source versus -target.\nModel conﬁguration. We set the hidden dimen-\nsion of the feedforward sublayer to 2048 and the\nrest to 512, matching Vaswani et al. (2017). We\nuse the same dropout rate for output of sublay-\ners, ReLU, and attention weights. Additionally,\n1https://github.com/glample/fastBPE\nwe also do word dropout (Sennrich et al., 2016a)\nwith probability 0.1. However, instead of zero-\ning the word embeddings, we randomly replace to-\nkens with UNK. For all experiments, we use label\nsmoothing of 0.1 (Szegedy et al., 2016; Pereyra\net al., 2017). The source and target’s input and\noutput embeddings are shared (Press and Wolf,\n2017), but we mask out words that are not in the\ntarget’s vocabulary at the ﬁnal output layer before\nsoftmax, by setting their logits to −∞.\nTraining. We use a batch size of 4096 and opti-\nmize using Adam (Kingma and Ba, 2015) with the\ndefault parameters β1 = 0.9, β2 = 0.999, ϵ =\n10−8. Gradients are clipped when global norm ex-\nceeds 1.0 (Pascanu et al., 2013). An epoch is a pre-\ndeﬁned number of iterations for each pair. We stop\ntraining when a maximum number of epochs has\nbeen met or the learning rate becomes too small\n(10−6). We also do early stopping when the de-\nvelopment BLEU has not improved for 20 evalu-\nations. For gl →en, this number is 50. When do-\ning validation-based decay, we use αdecay = 0.8\nand patience = 3. For complete data and model\nstatistics, please refer to Table 1. The best check-\npoint is selected based on the development BLEU\nscore during training.\nEvaluation. We report tokenized BLEU (Pap-\nineni et al., 2002) with multi-bleu.perl\nto be comparable with previous works. We also\nmeasure statistical signiﬁcance using bootstrap\nresampling (Koehn, 2004). For WMT '14 English-\nGerman, note that one needs to put compounds in\nATAT format2 before calculating BLEU score to\nbe comparable with previous works.\nB Further analysis\nLAYER NORM SCALE NORM\ntrain test train test\ngl→en 11.792 54.300 10.151 45.770\nsk→en 14.078 20.460 14.004 19.080\nen→vi 15.961 17.950 16.719 17.100\nen→he 15.562 14.950 15.906 15.080\nar→en 14.372 13.450 14.165 13.290\nTable 8: Label-smoothed train/test perplexities when\nusing LAYER NORM and SCALE NORM .\n2https://github.com/tensorflow/\ntensor2tensor/blob/master/tensor2tensor/\nutils/get_ende_bleu.sh\nWe ask if improvements from SCALE NORM on\nour low-resource tasks are due to improved reg-\nularization (a smaller generalization gap) or im-\nproved overall performance. We record smoothed\ntrain and test perplexities of our P RENORM mod-\nels in Table 8. We see suggestive results but\nno conclusive trends. For ar →en, gl →en, and\nsk→en, train and test drop slightly, with test more\nso than train. For en→vi, train perplexity in-\ncreases and test perplexity decreases an equivalent\namount. For en→he, our smallest change between\nSCALE NORM and LAYER NORM , train perplexity\nnegligibly increased and test perplexity remains\nthe same.\nC Listings\nSee the following page.\nSCALE NORM .\nclass ScaleNorm(nn.Module):\n\"\"\"ScaleNorm\"\"\"\ndef __init__(self, scale, eps=1e-5):\nsuper(ScaleNorm, self).__init__()\nself.scale = Parameter(torch.tensor(scale))\nself.eps = eps\ndef forward(self, x):\nnorm = self.scale / torch.norm(x, dim=-1, keepdim=True).clamp(min=self.eps)\nreturn x * norm\nFAIRSEQ . We follow FAIRSEQ ’s tutorial3 and train a P OST NORM Transformer base model using the\nfollowing conﬁguration:\nfairseq-train \\\ndata-bin/wmt16_en_de_bpe32k/ \\\n--arch transformer_wmt_en_de \\\n--share-all-embeddings \\\n--optimizer adam \\\n--adam-betas ’(0.9, 0.98)’ \\\n--clip-norm 1.0 \\\n--lr 0.001 \\\n--lr-scheduler inverse_sqrt \\\n--warmup-updates 4000 \\\n--warmup-init-lr 1e-07 \\\n--dropout 0.1 \\\n--weight-decay 0.0 \\\n--criterion label_smoothed_cross_entropy \\\n--label-smoothing 0.1 \\\n--max-tokens 8192 \\\n--update-freq 10 \\\n--attention-dropout 0.1 \\\n--activation-dropout 0.1 \\\n--max-epoch 40\nFor PRENORM , simply include the ﬂags:\n--encoder-normalize-before --decoder-normalize-before\nFor S CALE NORM , we replace all L AYER NORM s in fairseq/models/transformer.py and\nfairseq/modules/transformer layer.py with S CALE NORM (implemented above). For\nFIXNORM , we change the word embedding initialization to uniform with range [−0.01,0.01] and nor-\nmalize with torch.nn.functional.normalize.\nWe note that FAIRSEQ uses Xavier uniform initialization, which is big compared to our S MALL INIT\n(Section 3.1). We conjecture that FAIRSEQ training remains stable thanks to its large batch size, which\ngives more stable gradients.\n3https://github.com/pytorch/fairseq/blob/master/examples/scaling_nmt/README.md"
}