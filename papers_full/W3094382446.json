{
  "title": "ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding",
  "url": "https://openalex.org/W3094382446",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5108692923",
      "name": "Dongling Xiao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100416172",
      "name": "Yukun Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100399325",
      "name": "Han Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101870256",
      "name": "Yu Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5071362658",
      "name": "Hao Tian",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100677198",
      "name": "Hua Wu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100386394",
      "name": "Haifeng Wang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963323070",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2769934148",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2140016149",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2252066972",
    "https://openalex.org/W3037983807",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W3113747735",
    "https://openalex.org/W3098065087",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2876111955",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3102892879",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2944852028",
    "https://openalex.org/W3081031588",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2953109491",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3105163367",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2806081754",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W2971871542",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3106031450"
  ],
  "abstract": "Coarse-grained linguistic information, such as named entities or phrases, facilitates adequately representation learning in pre-training. Previous works mainly focus on extending the objective of BERT's Masked Language Modeling (MLM) from masking individual tokens to contiguous sequences of n tokens. We argue that such contiguously masking method neglects to model the intra-dependencies and inter-relation of coarse-grained linguistic information. As an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method to enhance the integration of coarse-grained information into pre-training. In ERNIE-Gram, n-grams are masked and predicted directly using explicit n-gram identities rather than contiguous sequences of n tokens. Furthermore, ERNIE-Gram employs a generator model to sample plausible n-gram identities as optional n-gram masks and predict them in both coarse-grained and fine-grained manners to enable comprehensive n-gram prediction and relation modeling. We pre-train ERNIE-Gram on English and Chinese text corpora and fine-tune on 19 downstream tasks. Experimental results show that ERNIE-Gram outperforms previous pre-training models like XLNet and RoBERTa by a large margin, and achieves comparable results with state-of-the-art methods. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.",
  "full_text": "ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language\nModeling for Natural Language Understanding\nDongling Xiao, Yukun Li, Han Zhang, Yu Sun, Hao Tian,\nHua Wu and Haifeng Wang\nBaidu Inc., China\n{xiaodongling,liyukun01,zhanghan17,sunyu02,\ntianhao,wu hua,wanghaifeng}@baidu.com\nAbstract\nCoarse-grained linguistic information, such as\nnamed entities or phrases, facilitates adequate-\nly representation learning in pre-training. Pre-\nvious works mainly focus on extending the ob-\njective of BERT’s Masked Language Model-\ning (MLM) from masking individual tokens\nto contiguous sequences of n tokens. We ar-\ngue that such contiguously masking method\nneglects to model the intra-dependencies and\ninter-relation of coarse-grained linguistic infor-\nmation. As an alternative, we propose ERNIE-\nGram, an explicitly n-gram masking method\nto enhance the integration of coarse-grained in-\nformation into pre-training. In ERNIE-Gram,\nn-grams are masked and predicted directly us-\ning explicit n-gram identities rather than con-\ntiguous sequences of n tokens. Furthermore,\nERNIE-Gram employs a generator model to\nsample plausible n-gram identities as optional\nn-gram masks and predict them in both coarse-\ngrained and ﬁne-grained manners to enable\ncomprehensive n-gram prediction and rela-\ntion modeling. We pre-train ERNIE-Gram\non English and Chinese text corpora and ﬁne-\ntune on 19 downstream tasks. Experimental\nresults show that ERNIE-Gram outperforms\nprevious pre-training models like XLNet and\nRoBERTa by a large margin, and achieves\ncomparable results with state-of-the-art meth-\nods. The source codes and pre-trained mod-\nels have been released at https://github.\ncom/PaddlePaddle/ERNIE.\n1 Introduction\nPre-trained on large-scaled text corpora and ﬁne-\ntuned on downstream tasks, self-supervised rep-\nresentation models (Radford et al., 2018; Devlin\net al., 2019; Liu et al., 2019; Yang et al., 2019;\nLan et al., 2020; Clark et al., 2020) have achieved\nremarkable improvements in natural language un-\nderstanding (NLU). As one of the most prominent\npre-trained models, BERT (Devlin et al., 2019) em-\nploys masked language modeling (MLM) to learn\nrepresentations by masking individual tokens and\npredicting them based on their bidirectional context.\nHowever, BERT’s MLM focuses on the represen-\ntations of ﬁne-grained text units (e.g. words or\nsubwords in English and characters in Chinese),\nrarely considering the coarse-grained linguistic in-\nformation (e.g. named entities or phrases in English\nand words in Chinese) thus incurring inadequate\nrepresentation learning.\nMany efforts have been devoted to integrate\ncoarse-grained semantic information by indepen-\ndently masking and predicting contiguous se-\nquences of n tokens, namely n-grams, such as\nnamed entities, phrases (Sun et al., 2019b), whole\nwords (Cui et al., 2019) and text spans (Joshi et al.,\n2020). We argue that such contiguously masking\nstrategies are less effective and reliable since the\nprediction of tokens in masked n-grams are inde-\npendent of each other, which neglects the intra-\ndependencies of n-grams. Speciﬁcally, given a\nmasked n-gram w={x1,...,x n},x∈VF, we max-\nimize p(w) = ∏n\ni=1 p(xi|c) for n-gram learning,\nwhere models learn to recover w in a huge and\nsparse prediction space F∈R|VF |n\n. Note that VF\nis the ﬁne-grained vocabulary1 and c is the context.\nWe propose ERNIE-Gram, an explicitly n-\ngram masked language modeling method in\nwhich n-grams are masked with single [MASK]\nsymbols, and predicted directly using explicit n-\ngram identities rather than sequences of tokens,\nas depicted in Figure 1(b). The models learn to\npredict n-gram w in a small and dense prediction\nspace N∈ R|VN|, where VN indicates a prior n-\ngram lexicon2 and normally |VN|≪|V F|n. To\n1VF contains 30K BPE codes in BERT (Devlin et al.,\n2019) and 50K subword units in RoBERTa (Liu et al., 2019).\n2VN contains 300K n-grams, where n∈[2,4) in this pa-\nper, n-grams are extracted in word-level before tokenization.\narXiv:2010.12148v2  [cs.CL]  13 Apr 2021\nTransformer Encoder\n1x\n2x 3x 5x\n4x 6x\nTransformer Encoder\n1x\n2y 4y\n4x 6x\n1 2 3 4 5 1 2 3 4 5\nTransformer Encoder\n1x\n3x 5x2x 2y\n4x 6x\n1 2 3 4 56\n+ + + + + + + + + + + + + + + +\n×L ×L ×L\n4y2z 4z\nEmbedding\nLayer\nFine-grained\nClassiﬁer\nN-gram\nClassiﬁer\n(a) Contiguously MLM (b) Explicitly N-gram MLM (c) Comprehensive N-gram MLM\nPositional Id\ni\nFigure 1: Illustrations of different MLM objectives, where xi and yi represent the identities of ﬁne-grained tokens\nand explicit n-grams respectively. Note that the weights of ﬁne-grained classiﬁer ( WF ∈Rh×|VF |) and N-gram\nclassiﬁer (WN∈Rh×|⟨VF ,VN⟩|) are not used in ﬁne-tuning stage, where his the hidden size and Lis the layers.\nlearn the semantic of n-grams more adequately,\nwe adopt a comprehensive n-gram prediction\nmechanism, simultaneously predicting masked n-\ngrams in coarse-grained (explicitn-gram identities)\nand ﬁne-grained (contained token identities) man-\nners with well-designed attention mask metrics, as\nshown in Figure 1(c).\nIn addition, to model the semantic relationships\nbetween n-grams directly, we introduce an en-\nhanced n-gram relation modeling mechanism,\nmasking n-grams with plausible n-grams identities\nsampled from a generator model, and then recov-\nering them to the original n-grams with the pair\nrelation between plausible and original n-grams.\nInspired by ELECTRA (Clark et al., 2020), we in-\ncorporate the replaced token detection objective to\ndistinguish original n-grams from plausible ones,\nwhich enhances the interactions between explicit\nn-grams and ﬁne-grained contextual tokens.\nIn this paper, we pre-train ERNIE-Gram on both\nbase-scale and large-scale text corpora (16GB and\n160GB respectively) under comparable pre-training\nsetting. Then we ﬁne-tune ERNIE-Gram on 13 En-\nglish NLU tasks and 6 Chinese NLU tasks. Experi-\nmental results show that ERNIE-Gram consistently\noutperforms previous well-performed pre-training\nmodels on various benchmarks by a large margin.\n2 Related Work\n2.1 Self-Supervised Pre-Training for NLU\nSelf-supervised pre-training has been used to learn\ncontextualized sentence representations though var-\nious training objectives. GPT (Radford et al.,\n2018) employs unidirectional language modeling\n(LM) to exploit large-scale corpora. BERT (Devlin\net al., 2019) proposes masked language modeling\n(MLM) to learn bidirectional representations ef-\nﬁciently, which is a representative objective for\npre-training and has numerous extensions such\nas RoBERTa (Liu et al., 2019), UNILM (Dong\net al., 2019) and ALBERT (Lan et al., 2020). XL-\nNet (Yang et al., 2019) adopts permutation lan-\nguage modeling (PLM) to model the dependencies\namong predicted tokens. ELECTRA introduces\nreplaced token detection (RTD) objective to learn\nall tokens for more compute-efﬁcient pre-training.\n2.2 Coarse-grained Linguistic Information\nIncorporating for Pre-Training\nCoarse-grained linguistic information is indispens-\nable for adequate representation learning. There\nare lots of studies that implicitly integrate coarse-\ngrained information by extending BERT’s MLM to\ncontiguously masking and predicting contiguous se-\nquences of tokens. For example, ERNIE (Sun et al.,\n2019b) masks named entities and phrases to en-\nhance contextual representations, BERT-wwm (Cui\net al., 2019) masks whole Chinese words to achieve\nbetter Chinese representations, SpanBERT (Joshi\net al., 2020) masks contiguous spans to improve\nthe performance on span selection tasks.\nA few studies attempt to inject the coarse-\ngrained n-gram representations into ﬁne-grained\ncontextualized representations explicitly, such as\nZEN (Diao et al., 2020) and AMBERT (Zhang and\nLi, 2020), in which additional transformer encoders\nand computations for explicit n-gram representa-\ntions are incorporated into both pre-training and\nﬁne-tuning. Li et al., 2019 demonstrate that explicit\nn-gram representations are not sufﬁciently reliable\nfor NLP tasks because of n-gram data sparsity and\nthe ubiquity of out-of-vocabulary n-grams. Differ-\nently, we only incorporate n-gram information by\nleveraging auxiliary n-gram classiﬁer and embed-\nding weights in pre-training, which will be com-\npletely removed during ﬁne-tuning, so our method\nmaintains the same parameters and computations\nas BERT.\n3 Proposed Method\nIn this section, we present the detailed implemen-\ntation of ERNIE-Gram, including n-gram lexicon\nVN extraction in Section 3.5, explicitly n-gram\nMLM pre-training objective in Section 3.2, compre-\nhensive n-gram prediction and relation modeling\nmechanisms in Section 3.3 and 3.4.\n3.1 Background\nTo inject n-gram information into pre-training,\nmany works (Sun et al., 2019b; Cui et al., 2019;\nJoshi et al., 2020) extend BERT’s masked language\nmodeling (MLM) from masking individual tokens\nto contiguous sequences of ntokens.\nContiguously MLM. Given input sequence x=\n{x1,...,x |x|},x ∈VF and n-gram starting bound-\naries b = {b1,...,b |b|}, let z = {z1,...,z |b|−1}to\nbe the sequence of n-grams, where zi=x[bi:bi+1),\nMLM samples 15% of starting boundaries from\nb to mask n-grams, donating Mas the indexes\nof sampled starting boundaries, zM as the con-\ntiguously masked tokens, z\\M as the sequence\nafter masking. As shown in Figure 1(a), b =\n{1,2,4,5,6,7},z = {x1,x[2:4),x4,x5,x6},M=\n{2,4},zM= {x[2:4),x5}, and z\\M= {x1,[M],\n[M],x4,[M],x6}. Contiguously MLM is per-\nformed by minimizing the negative likelihood:\n−logpθ(zM|z\\M) =−\n∑\nz∈zM\n∑\nx∈z\nlogpθ(x|z\\M). (1)\n3.2 Explicitly N-gram Masked Language\nModeling\nDifferent from contiguously MLM, we employ ex-\nplicit n-gram identities as pre-training targets to\nreduce the prediction space for n-grams. To be\nspeciﬁc, let y = {y1,...,y |b|−1},y ∈⟨VF,VN⟩\nto be the sequence of explicit n-gram identities,\nyMto be the target n-gram identities, and ¯z\\Mto\nbe the sequence after explicitly masking n-grams.\nAs shown in Figure 1(b), yM = {y2,y4}, and\n¯z\\M= {x1,[M], x4,[M],x6}. For masked n-\ngram x[2:4), the prediction space is signiﬁcantly\nreduced from R|VF |2\nto R|⟨VF ,VN⟩|. Explicitly n-\ngram MLM is performed by minimizing the nega-\ntive likelihood:\n−log pθ(yM|¯z\\M) =−\n∑\ny∈yM\nlog pθ(y|¯z\\M). (2)\n3.3 Comprehensive N-gram Prediction\nWe propose to simultaneously predict n-grams\nin ﬁne-grained and coarse-grained manners cor-\nresponding to single mask symbol [M], which\nhelps to extract comprehensive n-gram semantics,\ny ,y {      }\n1x\n2x 3x 5x\n4x 6x\n1 2 3 4 5\n+ +\n2\n+\n2\n+\n4\n++ + +\n×L\nK,V Q K,V Q K,V Q K,V Q K,V Q\n2y 4y\nMulti-Head\nSelf-Attention\n1x\n1x\n4x\n6x\n4x 6x\n1\n1\n2\n3\n4\n5\n2\n2\n4\n2 2 23 4 45\n+ + + + + + + +\n+\n+\n+\n+\n+\n+\n+\n+\nK,V\nQ\n(a)\nAllow to attend\nPrevent from \nattending\nPredict\n(b)\n2 4\n{          }Predict 2 3 5x ,x ,x\nTokens\nFine-grained\nIdentities\nN-gram\nIdentities\nPositions\nFigure 2: (a) Detailed structure of Comprehensive N-\ngram MLM. (b) Self-attention maskMwithout leaking\nlength information of masked n-grams.\nas shown in Figure 1(c). Comprehensive n-gram\nMLM is performed by minimizing the joint nega-\ntive likelihood:\n−log pθ(yM,zM|¯z\\M) =\n−\n∑\ny∈yM\nlog pθ(y|¯z\\M) −\n∑\nz∈zM\n∑\nx∈z\nlog pθ(x|¯z\\M).\n(3)\nwhere the predictions of explicit n-gram yMand\nﬁne-grained tokens xM are conditioned on the\nsame context sequence ¯z\\M.\nIn detail, to predict all tokens contained in a n-\ngram from single [M] other than a consecutive\nsequence of [M], we adopt distinctive mask sym-\nbols [Mi],i=1,...,n to aggregate contextualized\nrepresentations for predicting the i-th token in n-\ngram. As shown in Figure 2(a), along with the same\nposition as y2, symbols [M1] and [M2] are used\nas queries (Q) to aggregate representations from\n¯z\\M(K) for the predictions of x2 and x3, where\nQand Kdonate the query and key in self-attention\noperation (Vaswani et al., 2017). As shown in Fig-\nure 2(b), the self-attention mask metric M controls\nwhat context a token can attend to by modifying\nthe attention weight WA=softmax(QKT\n√dk\n+ M),\nM is assigned as:\nMij =\n{\n0, allow to attend\n−∞, prevent from attending (4)\nWe argue that the length information ofn-grams\nis detrimental to the representations learning, be-\ncause it will arbitrarily prune a number of semanti-\n×L'\nPrediction\nDistribution\nSampler Sampler\n the       proposed       a overhaul of the \nprime minister\npublic official\npresident\n0.05\n0.01\n0.16\naccountant\n...\nnot second to\nnothing short of\nnothing less than\n0.09\n0.08\n0.26\n...\n the public official proposed completely\na overhaul of the tax system .\ncompletely\ntax system .\nAn example of N-gram sampling\n1x 4x 6x\n1 2 3 4 5\n+ + + + +\nTransformer Encoder\n3x 5x2x 2y\n1x 4x 6x\n1 2 3 4 5\n+ + + + +\n×L\n4y\n2y' 4y' 1x 4x 6x\n1 2 3 4 5\n+ + + + +\n2y' 4y'\noriginal original original originalreplaced\nshare\n(b)\n(a)\nѳ Transformer Encoder ѳ\nTransformer Encoder ѳ'\nE\nE'\nFigure 3: (a) Detailed architecture of n-gram relation\nmodeling, where L′ donates the layers of the genera-\ntor model. (b) An example of plausible n-gram sam-\npling, where dotted boxes represent the sampling mod-\nule, texts in green are the original n-grams, and the\nitalic texts in blue donate the sampled n-grams.\ncally related n-grams with different lengths during\npredicting. From this viewpoint, for the predic-\ntions of n-gram {x2,x3}, 1) we prevent context\n¯z\\Mfrom attending to {[M1 ],[M2 ]}and 2) pre-\nvent {[M1 ],[M2 ]}from attending to each other, so\nthat the length information of n-grams will not be\nleaked in pre-training, as displayed in Figure 2(b).\n3.4 Enhanced N-gram Relation Modeling\nTo explicitly learn the semantic relationships be-\ntween n-grams, we jointly pre-train a small genera-\ntor model θ′with explicitly n-gram MLM objective\nto sample plausible n-gram identities. Then we\nemploy the generated identities to preform mask-\ning and train the standard model θ to predict the\noriginal n-grams from fake ones in coarse-grained\nand ﬁne-grained manners, as shown in Figure 3(a),\nwhich is efﬁcient to model the pair relationships\nbetween similar n-grams. The generator model\nθ′will not be used during ﬁne-tuning, where the\nhidden size Hθ′ of θ′has Hθ′ = Hθ/3 empirically.\nAs shown in Figure 3(b), n-grams of different\nlength can be sampled to mask originaln-grams ac-\ncording to the prediction distributions of θ′, which\nis more ﬂexible and sufﬁcient for constructing n-\ngram pairs than previous synonym masking meth-\nods (Cui et al., 2020) that require synonyms and\noriginal words to be of the same length. Note\nthat our method needs a large embedding layer\nE ∈R|⟨VF ,VN⟩|×h to obtain n-gram vectors in pre-\ntraining. To keep the number of parameters consis-\ntent with that of vanilla BERT, we remove the aux-\niliary embedding weights of n-grams during ﬁne-\ntuning (E →E′∈R|VF |×h). Speciﬁcally, let y′\nM\nto be the generated n-gram identities, ¯z′\nMto be the\nsequence masked by y′\nM, where y′\nM= {y′\n2,y′\n4},\nand ¯z′\n\\M={x1,y′\n2,x4,y′\n4,x6}in Figure 3(a). The\npre-training objective is to jointly minimize the neg-\native likelihood of θ′and θ:\n−log pθ′(yM|¯z\\M) −log pθ(yM,zM|¯z′\n\\M). (5)\nMoreover, we incorporate the replaced token\ndetection objective (RTD) to further distinguish\nfake n-grams from the mix-grained context ¯z′\n\\M\nfor interactions among explicit n-grams and ﬁne-\ngrained contextual tokens, as shown in the right\npart of Figure 3(a). Formally, we donate ˆz\\Mto be\nthe sequence after replacing masked n-grams with\ntarget n-gram identities yM, the RTD objective is\nperformed by minimizing the negative likelihood:\n−log pθ\n(1 (¯z′\n\\M= ˆz\\M)|¯z′\n\\M\n)\n= −\n|ˆz\\M|∑\nt=1\nlog pθ\n(1 (¯z′\n\\M,t = ˆz\\M,t)|¯z′\n\\M,t). (6)\nAs the example depicted in Figure 3(a), the target\ncontext sequence ˆz\\M= {x1,y2,x4,y4,x6}.\n3.5 N-gram Extraction\nN-gram Lexicon Extraction. We employ T-test\nto extract semantically-complete n-grams statisti-\ncally from unlabeled text corpora X(Xiao et al.,\n2020), as described in Algorithm 1. We ﬁrst calcu-\nAlgorithm 1 N-gram Extraction with T-test\nInput: Large-scale text corpora Xfor pre-training\nOutput: Semantic n-gram lexicon VN\n⊿given initial hypothesis H0: a randomly constructed\nn-gram w = {x1,...,x n}with probability p′(w) =∏n\ni=1 p(xi) cannot be a statistically semantic n-gram\nfor l in range(2, n) do\nVNl ←⟨⟩ ⊿initialize the lexicon for l-grams\nfor l-gram w in Xdo\ns←(p(w)−p′(w))√\nσ2/Nl\n: t-statistic score ⊿where\nstatistical probability p(w) = Count(w)\nNl\n, deviation\nσ2 = p(w)(1 −p(w)), Nl donates the count of l-\ngrams in X\nVNl.append({w,s})\nVNl ←topk(VNl,kl) ⊿kl is the number of l-gram\nVN ←⟨VN2 ,..., VNn⟩ ⊿merge all lexicons\nreturn VN\nlate the t-statistic scores of all n-grams appearing\nin Xsince the higher the t-statistic score, the more\nlikely it is a semantically-complete n-gram. Then,\nwe select the l-grams with the top kl t-statistic\nscores to construct the ﬁnal n-gram lexicon VN.\nN-gram Boundary Extraction. To incorporate\nn-gram information into MLM objective, n-gram\nboundaries are referred to mask whole n-grams\nfor pre-training. Given an input sequence x =\n{x1,...,x |x|}, we employ maximum matching al-\ngorithm to traverse valid n-gram paths B =\n{b1,..., b|B|}according to VN, then select the short-\nest paths as the ﬁnal n-gram boundaries b, where\n|b|≤| bi|,∀i= 1,..., |B|.\n4 Experiments\nIn this section, we ﬁrst present the pre-training con-\nﬁguration of ERNIE-Gram on Chinese and English\ntext corpora. Then we compare ERNIE-Gram with\nprevious works on various downstream tasks. We\nalso conduct several ablation experiments to access\nthe major components of ERNIE-Gram.\n4.1 Pre-training Text Corpora\nEnglish Pre-training Data. We use two com-\nmon text corpora for English pre-training:\n• Base-scale corpora: 16GB uncompressed text\nfrom WIKIPEDIA and BOOKS CORPUS (Zhu\net al., 2015), which is the original data for BERT.\n• Large-scale corpora: 160GB uncompressed\ntext from WIKIPEDIA , BOOKS CORPUS , OPEN -\nWEBTEXT 3, CC-N EWS (Liu et al., 2019) and\nSTORIES (Trinh and Le, 2018), which is the orig-\ninal data used in RoBERTa.\nChinese Pre-training Data. We adopt the same\nChinese text corpora used in ERNIE2.0 (Sun et al.,\n2020) to pre-train ERNIE-Gram.\n4.2 Pre-training Setup\nBefore pre-training, we ﬁrst extract 200K bi-grams\nand 100K tri-grams with Algorithm 1 to construct\nthe semantic n-gram lexicon VN for English and\nChinese corpora. and we adopt the sub-word dic-\ntionary (30K BPE codes) used in BERT and the\ncharacter dictionary used in ERNIE2.0 as our ﬁne-\ngrained vocabulary VF in English and Chinese.\nFollowing the previous practice, we pre-train\nERNIE-Gram in base size ( L = 12 ,H = 768 ,\nA = 12 , Total Parameters= 110M)4, and set the\n3http://web.archive.org/save/http:\n//Skylion007.github.io/OpenWebTextCorpus\n4We donate the number of layers as L, the hidden size as\nHand the number of self-attention heads as A.\nlength of the sequence in each batch up to 512 to-\nkens. We add the relative position bias (Raffel et al.,\n2020) to attention weights and use Adam (Kingma\nand Ba, 2015) for optimizing. For pre-training on\nbase-scale English corpora, the batch size is set to\n256 sequences, the peak learning rate is 1e-4 for\n1M training steps, which are the same settings as\nBERTBASE. As for large-scale English corpora, the\nbatch size is 5112 sequences, the peak learning rate\nis 4e-4 for 500K training steps. For pre-training on\nChinese corpora, the batch size is 256 sequences,\nthe peak learning rate is 1e-4 for 3M training steps.\nAll the pre-training hyper-parameters are supple-\nmented in the Appendix A.\nIn ﬁne-tuning, we remove the auxiliary embed-\nding weights of explicit n-grams identities for fair\ncomparison with previous pre-trained models.\n4.3 Results on GLUE Benchmark\nThe General Language Understanding Evaluation\n(GLUE; Wang et al., 2018) is a multi-task bench-\nmark consisting of various NLU tasks, which con-\ntains 1) pairwise classiﬁcation tasks like language\ninference (MNLI; Williams et al., 2018, RTE; Da-\ngan et al., 2006), question answering (QNLI; Ra-\njpurkar et al., 2016) and paraphrase detection (QQP,\nMRPC; Dolan and Brockett, 2005), 2) single-\nsentence classiﬁcation tasks like linguistic accept-\nability (CoLA; Warstadt et al., 2019), sentiment\nanalysis (SST-2; Socher et al., 2013) and 3) text\nsimilarity task (STS-B; Cer et al., 2017).\nThe ﬁne-tuning results on GLUE of ERNIE-\nGram and various strong baselines are presented\nin Table 1. For fair comparison, the listed mod-\nels are all in base size and ﬁne-tuned without any\ndata augmentation. Pre-trained with base-scale text\ncorpora, ERNIE-Gram outperforms recent models\nsuch as TUPE and F-TFM by 1.7 and 1.3 points on\naverage. As for large-scale text corpora, ERNIE-\nGram achieves average score increase of 1.7 and\n0.6 over RoBERTa and ELECTRA, demonstrating\nthe effectiveness of ERNIE-Gram.\n4.4 Results on Question Answering (SQuAD)\nThe Stanford Question Answering (SQuAD) tasks\nare designed to extract the answer span within the\ngiven passage conditioned on the question. We con-\nduct experiments on SQuAD1.1 (Rajpurkar et al.,\n2016) and SQuAD2.0 (Rajpurkar et al., 2018) by\nadding a classiﬁcation layer on the sequence out-\nputs of ERNIE-Gram and predicting whether each\ntoken is the start or end position of the answer span.\nModels #ParamMNLI QNLI QQP SST-2 CoLA MRPC RTE STS-BGLUE\nAcc Acc Acc Acc MCC Acc Acc PCC Avg\nResults of single models pre-trained onbase-scaletext corpora (16GB)\nBERT (Devlin et al., 2019) 110M 84.5 91.7 91.3 93.2 58.9 87.3 68.6 89.5 83.1\nTUPE (Ke et al., 2020) 110M 86.2 92.1 91.3 93.3 63.6 89.9 73.6 89.2 85.0\nF-TFMELECTRA(Dai et al., 2020)110M 86.4 92.1 91.7 93.1 64.3 89.2 75.4 90.8 85.4\nERNIE-Gram 110M 87.1 92.8 91.8 93.2 68.5 90.3 79.4 90.4 86.7\nResults of single models pre-trained onlarge-scaletext corpora (160GB or more)\nXLNet (Yang et al., 2019) 110M 86.8 91.7 91.4 94.7 60.2 88.2 74.0 89.5 84.5\nRoBERTa (Liu et al., 2019) 135M 87.6 92.8 91.9 94.8 63.6 90.2 78.7 91.2 86.4\nELECTRA (Clark et al., 2020)110M 88.8 93.2 91.5 95.2 67.7 89.5 82.7 91.2 87.5\nUNILMv2 (Bao et al., 2020) 110M 88.5 93.5 91.7 95.1 65.2 91.8 81.3 91.0 87.3\nMPNet (Song et al., 2020) 110M 88.5 93.3 91.9 95.4 65.0 91.5 85.2 90.9 87.7\nERNIE-Gram 110M 89.1 93.2 92.2 95.6 68.6 90.7 83.8 91.3 88.1\nTable 1: Results on the development set of the GLUE benchmark for base-size pre-trained models. Models using\n16GB corpora are all pre-trained with a batch size of 256 sequences for 1M steps. STS-B and CoLA are reported\nby Pearson correlation coefﬁcient (PCC) and Matthews correlation coefﬁcient (MCC), other tasks are reported by\naccuracy (Acc). Note that results of ERNIE-Gram are the median of over ten runs with different random seeds.\nModels SQuAD1.1 SQuAD2.0\nEM F1 EM F1\nModels pre-trained onbase-scaletext corpora (16GB)\nBERT (Devlin et al., 2019)80.8 88.5 73.7 76.3\nRoBERTa (Liu et al., 2019) - 90.6 - 79.7\nXLNet (Yang et al., 2019) - - 78.2 81.0\nMPNet (Song et al., 2020) 85.0 91.4 80.5 83.3\nUNILMv2 (Bao et al., 2020)85.6 92.0 80.9 83.6\nERNIE-Gram 86.2 92.3 82.1 84.8\nModels pre-trained onlarge-scaletext corpora (160GB)\nRoBERTa (Liu et al., 2019)84.6 91.5 80.5 83.7\nXLNet (Yang et al., 2019) - - 80.2 -\nELECTRA (Clark et al., 2020)86.8 - 80.5 -\nMPNet (Song et al., 2020) 86.8 92.5 82.8 85.6\nUNILMv2 (Bao et al., 2020)87.1 93.1 83.3 86.1\nERNIE-Gram 87.2 93.2 84.1 87.1\nTable 2: Performance comparison between base-size\npre-trained models on the SQuAD development sets.\nExact-Match (EM) and F1 score are adopted for evalu-\nations. Results of ERNIE-Gram are the median of over\nten runs with different random seeds.\nTable 2 presents the results on SQuAD for base-size\npre-trained models, ERNIE-Gram achieves better\nperformance than current strong baselines on both\nbase-scale and large-scale pre-training text corpora.\n4.5 Results on RACE and Text Classiﬁcation\nTasks\nThe ReAding Comprehension from Examinations\n(RACE; Lai et al., 2017) dataset collects 88K long\npassages from English exams at middle and high\nschools, the task is to select the correct choice from\nfour given options according to the questions and\nModels RACE IMDb AG\nTotal High Middle Err. Err.\nPre-trained onbase-scaletext corpora (16GB)\nBERTa 65.0 62.3 71.7 5.4 5.9\nXLNetb 66.8 - - 4.9 -\nMPNetc 70.4 67.7 76.8 4.8 -\nF-TFMdELECTRA - - - 5.2 5.4\nERNIE-Gram 72.7 68.1 75.1 4.6 5.0\nPre-trained onlarge-scaletext corpora (160GB)\nMPNetc 72.0 70.3 76.3 4.4 -\nERNIE-Gram 77.7 75.6 78.8 3.9 4.9\nTable 3: Comparison on the test sets of RACE, IMDb\nand AG. The listed models are all in base-size. In the\nresults of RACE, “High” and “Middle” represent the\ntraining and evaluation sets for high schools and mid-\ndle schools respectively, “Total” is the full training and\nevaluation set. a (Devlin et al., 2019); b (Yang et al.,\n2019); c (Song et al., 2020); d (Dai et al., 2020).\npassages. We also evaluate ERNIE-Gram on two\nlarge scaled text classiﬁcation tasks that involve\nlong text and reasoning, including sentiment anal-\nysis datasets IMDb (Maas et al., 2011) and topic\nclassiﬁcation dataset AG’s News (Zhang et al.,\n2015). The results are reported in Table 3. It can be\nseen that ERNIE-Gram consistently outperforms\nprevious models, showing the advantage of ERNIE-\nGram on tasks involving long text and reasoning.\n4.6 Results on Chinese NLU Tasks\nWe execute extensive experiments on six Chinese\nlanguage understanding tasks, including natural\nlanguage inference (XNLI; Conneau et al., 2018),\nModels\nXNLI LCQMC DRCD CMRC2018 DuReader M-NER\nAcc Acc EM / F1 EM / F1 EM / F1 F1\nDev Test Dev Test Dev Test Dev Dev Dev Test\nRoBERTa-wwn-ext∗LARGE 82.1 81.2 90.4 87.0 89.6 / 94.8 89.6 / 94.5 68.5 / 88.4 - / - - -\nNEZHALARGE(Wei et al., 2019)82.2 81.2 90.9 87.9 - / - - / - - / - - / - - -\nMacBERTLARGE(Cui et al., 2020)82.4 81.3 90.6 87.6 91.2 / 95.6 91.7 / 95.6 70.7 / 88.9 - / - - -\nBERT-wwn-ext∗BASE 79.4 78.7 89.6 87.1 85.0 / 91.2 83.6 / 90.4 67.1 / 85.7 - / - - -\nRoBERTa-wwn-ext∗BASE 80.0 78.8 89.0 86.4 85.6 / 92.0 67.4 / 87.2 67.4 / 87.2 - / - - -\nZENBASE(Diao et al., 2020) 80.5 79.2 90.2 88.0 - / - - / - - / - - / - - -\nNEZHABASE(Wei et al., 2019) 81.4 79.3 90.0 87.4 - / - - / - - / - - / - - -\nMacBERTBASE(Cui et al., 2020)79.0 78.2 89.4 87.0 88.3 / 93.5 87.9 / 93.2 69.5 / 87.7 - / - - -\nERNIE1.0BASE(Sun et al., 2019b)79.9 78.4 89.7 87.4 84.6 / 90.9 84.0 / 90.5 65.1 / 85.1 57.9 / 72.1 95.0 93.8\nERNIE2.0BASE(Sun et al., 2020)81.2 79.790.9 87.9 88.5 / 93.8 88.0 / 93.4 69.1 / 88.6 61.3 / 74.9 95.2 93.8\nERNIE-GramBASE 81.8 81.590.6 88.5 90.2/ 95.0 89.9/ 94.6 74.3 / 90.5 64.2 / 76.8 96.5 95.3\nTable 4: Results on six Chinese NLU tasks for base-size pre-trained models. Results of models with asterisks “ ∗”\nare from Cui et al., 2019. M-NER is in short for MSRA-NER dataset. “ BASE” and “LARGE” donate different\nsizes of pre-training models. Large size models have L= 24,H = 1024,A = 16 and total Parameters=340M.\nmachine reading comprehension (CMRC2018; Cui\net al., 2018, DRCD; Shao et al., 2018 and DuR-\neader; He et al., 2018), named entity recognition\n(MSRA-NER; Gao et al., 2005) and semantic simi-\nlarity (LCQMC; Liu et al., 2018).\nResults on six Chinese tasks are presented in\nTable 4. It is observed that ERNIE-Gram signiﬁ-\ncantly outperforms previous models across tasks\nby a large margin and achieves new state-of-the-\nart results on these Chinese NLU tasks in base-\nsize model group. Besides, ERNIE-GramBASE are\nalso better than various large-size models on XNLI,\nLCQMC and CMRC2018 datasets.\n4.7 Ablation Studies\nWe further conduct ablation experiments to analyze\nthe major components of ERNIE-Gram.\nEffect of Explicitly N-gram MLM. We com-\npare two models pre-trained with contiguously\nMLM and explicitly n-gram MLM objectives in\nthe same settings (the size of n-gram lexicon is\n300K). The evaluation results for pre-training and\nﬁne-tuning are shown in Figure 4. Compared with\ncontiguously MLM, explicitly n-gram MLM ob-\njective facilitates the learning of n-gram semantic\ninformation with lower n-gram level perplexity in\npre-training and better performance on downstream\ntasks. This veriﬁes the effectiveness of explicitly\nn-gram MLM objective for injecting n-gram se-\nmantic information into pre-training.\nSize of N-gram Lexicon. To study the impact\nof n-gram lexicon size on model performance, we\nextract n-gram lexicons with size from 100K to\n400K for pre-training, as shown in Figure 5. As the\nFigure 4: (a) N-gram level perplexity which is cal-\nculated by (∏k\ni=1 PPL(wi))\n1\nk for contiguously MLM,\nwhere wi is the i-th masked n-gram. (b) Perfor-\nmance distribution box plot on MNLI, QNLI, SST-2\nand SQuAD1.1.\nlexicon size enlarges, performance of contiguously\nMLM becomes worse, presumably because more\nn-grams are matched and connected as longer con-\nsecutive spans for prediction, which is more difﬁ-\ncult for representation learning. Explicitly n-gram\nMLM with lexicon size being 300K achieves the\nbest results, while the performance signiﬁcantly de-\nclines when the size of lexicon increasing to 400K\nbecause more low-frequent n-grams are learning\nunnecessarily. See Appendix C for detailed results\nof different lexicon choices on GLUE and SQuAD.\nEffect of Comprehensive N-gram Prediction\nand Enhanced N-gram Relation Modeling. As\nshown in Table 5, we compare several ERNIE-\nGram variants with previous strong baselines un-\nder the BERTBASE setting. After removing com-\nprehensive n-gram prediction (#2), ERNIE-Gram\ndegenerates to a variant with explicitly n-gram\nMLM and n-gram relation modeling and its perfor-\nmance drops slightly by 0.3-0.6. When removing\nenhanced n-gram relation modeling (#3), ERNIE-\nGram degenerates to a variant with comprehen-\n# Models MNLI SST-2 SQuAD1.1 SQuAD2.0\nm mm Acc EM F1 EM F1\nXLNet a 85.6 85.1 93.4 - - 78.2 81.0\nRoBERTab 84.7 - 92.7 - 90.6 - 79.7\nMPNet c 85.6 - 93.6 84.0 90.3 79.5 82.2\nUNILMv2 d 85.6 85.5 93.0 85.0 91.5 78.9 81.8\n#1 ERNIE-Gram 86.5 86.4 93.2 85.2 91.7 80.8 84.0\n#2 −CNP 86.2 86.2 92.7 85.0 91.5 80.4 83.4\n#3 −ENRM 85.7 85.8 93.5 84.7 91.3 79.7 82.7\n#4 −CNP −ENRM 85.6 85.7 92.9 84.5 91.2 79.5 82.4\nTable 5: Comparisons between comprehensive n-gram prediction (CNP)\nand enhanced n-gram relation modeling (ENRM) methods. All the listed\nmodels are pre-trained following the same settings of BERT BASE (De-\nvlin et al., 2019) and without relative position bias. Results of ERNIE-\nGram variants are the median of over ten runs with different random\nseeds. Results in the upper block are from a (Yang et al., 2019), b (Liu\net al., 2019), c (Song et al., 2020) and d (Bao et al., 2020).\nFigure 5: Quantitative study on the size\nof extracted n-gram lexicon. (a) Com-\nparisons on GLUE and SQuAD. Note\nthat SQuAD is presented by the average\nscores of SQuAD1.1 and SQuAD2.0.\n(b) Performance distribution box plot on\nMNLI and SQuAD1.1 datasets.\nFigure 6: (a) Recall rate of whole named entities on dif-\nferent evaluation subsets, which have incremental av-\nerage length of named entities. (b-d) Mean attention\nscores of 12 attention heads in the last self-attention\nlayer. Texts in green and orange boxes are named\nentities standing for organizations and locations.\nsive n-gram MLM and the performance drops by\n0.4-1.3. If removing both comprehensive n-gram\nprediction and relation modeling ( #4), ERNIE-\nGram degenerates to a variant with explicitly n-\ngram MLM and the performance drops by 0.7-1.6.\nThese results demonstrate the advantage of com-\nprehensive n-gram prediction and n-gram relation\nmodeling methods for efﬁciently n-gram semantic\ninjecting into pre-training. The detailed results of\nablation study are supplemented in Appendix C.\n4.8 Case Studies\nTo further understand the effectiveness of our ap-\nproach for learning n-grams information, we ﬁne-\ntune ERNIE-Gram, contiguously MLM and lower-\ncased BERT on CoNLL-2003 named entity recog-\nnition task (Sang and De Meulder, 2003) for com-\nparison. We divide the evaluation set into ﬁve sub-\nsets based on the average length of the named enti-\nties in each sentence. As shown in Figure 6(a), it is\nmore difﬁcult to recognize whole named entities as\nthe length of them increases, while the performance\nof ERNIE-Gram declines slower than contiguously\nMLM and BERT, which implies that ERNIE-Gram\nmodels tighter intra-dependencies of n-grams.\nAs shown in Figure 6(b-d), we visualize the at-\ntention patterns in the last self-attention layer of\nﬁne-tuned models. For contiguously MLM, there\nare clear diagonal lines in named entities that to-\nkens prefer to attend to themself in named entities.\nWhile for ERNIE-Gram, there are bright blocks\nover named entities that tokens attend to most of\ntokens in the same entity adequately to construct\ntight representation, verifying the effectiveness of\nERNIE-Gram for n-gram semantic modeling.\n5 Conclusion\nIn this paper, we present ERNIE-Gram, an ex-\nplicitly n-gram masking and predicting method to\neliminate the limitations of previous contiguously\nmasking strategies and incorporate coarse-grained\nlinguistic information into pre-training sufﬁciently.\nERNIE-Gram conducts comprehensiven-gram pre-\ndiction and relation modeling to further enhance\nthe learning of semantic n-grams for pre-training.\nExperimental results on various NLU tasks demon-\nstrate that ERNIE-Gram outperforms XLNet and\nRoBERTa by a large margin, and achieves state-of-\nthe-art results on various benchmarks. Future work\nincludes constructing more comprehensive n-gram\nlexicon (n>3) and pre-training ERNIE-Gram with\nlarge-size model for more downstream tasks.\nAcknowledgments\nWe would like to thank Zhen Li for his constructive\nsuggestions, and hope everything goes well with his\nwork. We are also indebted to the NAACL-HLT re-\nviewers for their detailed and insightful comments\non our work.\nReferences\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Songhao Piao, Jian-\nfeng Gao, Ming Zhou, et al. 2020. Unilmv2:\nPseudo-masked language models for uniﬁed lan-\nguage model pre-training. In Proceedings of the In-\nternational Conference on Machine Learning, pages\n7006–7016.\nDaniel Cer, Mona Diab, Eneko Agirre, I ˜nigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020. Revisiting pre-\ntrained models for Chinese natural language process-\ning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020 , pages 657–668,\nOnline. Association for Computational Linguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019.\nPre-training with whole word masking for chinese\nbert. arXiv preprint arXiv:1906.08101.\nYiming Cui, Ting Liu, Li Xiao, Zhipeng Chen, Wentao\nMa, Wanxiang Che, Shijin Wang, and Guoping Hu.\n2018. A span-extraction dataset for chinese machine\nreading comprehension. CoRR, abs/1810.07366.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The PASCAL recognising textual entailment\nchallenge. In Proceedings of the First International\nConference on Machine Learning Challenges, pages\n177–190.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc V Le.\n2020. Funnel-transformer: Filtering out sequential\nredundancy for efﬁcient language processing. In Ad-\nvances in Neural Information Processing Systems.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and\nYonggang Wang. 2020. ZEN: Pre-training Chinese\ntext encoder enhanced by n-gram representations. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020 , pages 4729–4740, Online.\nAssociation for Computational Linguistics.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems , volume 32, pages 13063–\n13075. Curran Associates, Inc.\nJianfeng Gao, Mu Li, Andi Wu, and Chang-Ning\nHuang. 2005. Chinese word segmentation and\nnamed entity recognition: A pragmatic approach.\nComputational Linguistics, 31(4):531–574.\nWei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao,\nXinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu,\nQiaoqiao She, Xuan Liu, Tian Wu, and Haifeng\nWang. 2018. DuReader: a Chinese machine read-\ning comprehension dataset from real-world appli-\ncations. In Proceedings of the Workshop on Ma-\nchine Reading for Question Answering , pages 37–\n46, Melbourne, Australia. Association for Computa-\ntional Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nGuolin Ke, Di He, and Tie-Yan Liu. 2020. Rethink-\ning the positional encoding in language pre-training.\narXiv preprint arXiv:2006.15595.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Interna-\ntional Conference on Learning Representations, San\nDiego, CA.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n785–794, Copenhagen, Denmark. Association for\nComputational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nXiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han,\nArianna Yuan, and Jiwei Li. 2019. Is word segmen-\ntation necessary for deep learning of Chinese rep-\nresentations? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3242–3252, Florence, Italy. Associa-\ntion for Computational Linguistics.\nXin Liu, Qingcai Chen, Chong Deng, Huajun Zeng,\nJing Chen, Dongfang Li, and Buzhou Tang. 2018.\nLCQMC:a large-scale Chinese question matching\ncorpus. In Proceedings of the 27th International\nConference on Computational Linguistics , pages\n1952–1962, Santa Fe, New Mexico, USA. Associ-\nation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) , pages 784–\n789, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nErik F Sang and Fien De Meulder. 2003. Introduc-\ntion to the CoNLL-2003 shared task: Language-\nindependent named entity recognition. In Proceed-\nings of the Seventh Conference on Natural Language\nLearning at HLT-NAACL 2003, pages 142–147.\nChih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng,\nand Sam Tsai. 2018. DRCD: a chinese machine\nreading comprehension dataset. arXiv preprint\narXiv:1806.00920.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2020. Mpnet: Masked and permuted pre-\ntraining for language understanding. In Advances in\nNeural Information Processing Systems.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019a. How to ﬁne-tune bert for text classiﬁcation?\nIn China National Conference on Chinese Computa-\ntional Linguistics, pages 194–206. Springer.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019b. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0:\nA continual pre-training framework for language un-\nderstanding. Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, 34(05):8968–8975.\nTrieu H. Trinh and Quoc V . Le. 2018. A sim-\nple method for commonsense reasoning. ArXiv,\nabs/1806.02847.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information\nProcessing Systems, pages 5998–6008. Curran As-\nsociates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nJunqiu Wei, Xiaozhe Ren, Xiaoguang Li, Weny-\nong Huang, Yi Liao, Yasheng Wang, Jiashu\nLin, Xin Jiang, Xiao Chen, and Qun Liu. 2019.\nNEZHA: Neural contextualized representation for\nchinese language understanding. arXiv preprint\narXiv:1909.00204.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nDongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie-\ngen: An enhanced multi-ﬂow pre-training and ﬁne-\ntuning framework for natural language generation.\nIn Proceedings of the Twenty-Ninth International\nJoint Conference on Artiﬁcial Intelligence, IJCAI-\n20, pages 3997–4003. International Joint Confer-\nences on Artiﬁcial Intelligence Organization. Main\ntrack.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems , volume 32, pages\n5753–5763. Curran Associates, Inc.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in Neural Information Pro-\ncessing Systems, volume 28, pages 649–657. Curran\nAssociates, Inc.\nXinsong Zhang and Hang Li. 2020. Ambert: A pre-\ntrained language model with multi-grained tokeniza-\ntion. arXiv preprint arXiv:2008.11869.\nY . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Ur-\ntasun, A. Torralba, and S. Fidler. 2015. Aligning\nbooks and movies: Towards story-like visual expla-\nnations by watching movies and reading books. In\n2015 IEEE International Conference on Computer\nVision (ICCV), pages 19–27.\nA Hyperparameters for Pre-Training\nAs shown in Table 6, we list the detailed hyper-\nparameters used for pre-training ERNIE-Gram on\nbase and large scaled English text corpora and Chi-\nnese text corpora. We follow the same hyperpa-\nrameters of BERT BASE (Devlin et al., 2019) to\npre-train ERNIE-Gram on the base-scale English\ntext corpora (16GB). We pre-train ERNIE-Gram\non the large-scale text corpora (160GB) with the\nsettings in RoBERTa (Liu et al., 2019) except the\nbatch size being 5112 sequences.\nHyperparameters Base-scale Large-scale Chinese\nLayers 12\nHidden size 768\nAttention heads 12\nTraining steps 1M 500K 3M\nBatch size 256 5112 256\nLearning rate 1e-4 4e-4 1e-4\nWarmup steps 10,000 24,000 4,000\nAdam β (0.9, 0.99) (0.9, 0.98) (0.9, 0.99)\nAdam ϵ 1e-6\nLearning rate schedule Linear\nWeight decay 0.01\nDropout 0.1\nGPUs (Nvidia V100) 16 64 32\nTable 6: Hyperparameters used for pre-training on dif-\nferent text corpora.\nB Hyperparameters for Fine-Tuning\nThe hyperparameters for each tasks are searched\non the development sets according to the average\nscore of ten runs with different random seeds.\nB.1 GLUE benchmark\nThe ﬁne-tuning hyper-parameters for GLUE bench-\nmark (Wang et al., 2018) are presented in Table 7.\nHyperparameters GLUE\nBatch size {16, 32}\nLearning rate {5e-5, 1e-4, 1.5e-4}\nEpochs 3 for MNLI and {10, 15}for others\nLR schedule Linear\nLayerwise LR decay 0.8\nWarmup proportion 0.1\nWeight decay 0.01\nTable 7: Hyperparameters used for ﬁne-tuning on the\nGLUE benchmark.\nB.2 SQuAD benchmark and RACE dataset\nThe ﬁne-tuning hyper-parameters for SQuAD (Ra-\njpurkar et al., 2016;Rajpurkar et al., 2018) and\nRACE (Lai et al., 2017) are presented in Table 8.\nHyperparameters SQuAD RACE\nBatch size 48 32\nLearning rate {1e-4, 1.5e-4, 2e-4} {8e-5, 1e-4}\nEpochs {2, 4} { 4, 5}\nLR schedule Linear Linear\nLayerwise LR decay 0.8 0.8\nWarmup proportion 0.1 0.1\nWeight decay 0.0 0.01\nTable 8: Hyperparameters used for ﬁne-tuning on the\nSQuAD benchmark and RACE dataset.\nB.3 Text Classiﬁcation tasks\nTable 9 lists the ﬁne-tuning hyper-parameters for\nIMDb (Maas et al., 2011) and AG’news (Zhang\net al., 2015) datasets. To process texts with a length\nlarger than 512, we follow Sun et al., 2019a to\nselect the ﬁrst 512 tokens to perform ﬁne-tuning.\nHyperparameters IMDb AG’news\nBatch size 32\nLearning rate {5e-5, 1e-4, 1.5e-4}\nEpochs 3\nLR schedule Linear\nLayerwise LR decay 0.8\nWarmup proportion 0.1\nWeight decay 0.01\nTable 9: Hyperparameters used for ﬁne-tuning on\nIMDb and AG’news.\nB.4 Chinese NLU tasks\nThe ﬁne-tuning hyperparameters for Chinese NLU\ntasks including XNLI (Conneau et al., 2018),\nLCQMC (Liu et al., 2018), DRCD (Shao et al.,\n2018), DuReader (He et al., 2018), CMRC2018\nand MSRA-NER (Gao et al., 2005) are presented\nin Table 10.\nTasks Batch Learning Epoch Droputsize rate\nXNLI 256 1.5e-4 3 0.1\nLCQMC 32 4e-5 2 0.1\nCMRC2018 64 1.5e-4 5 0.2\nDuReader 64 1.5e-4 5 0.1\nDRCD 64 1.5e-4 3 0.1\nMSRA-NER 16 1.5e-4 10 0.1\nTable 10: Hyperparameters used for ﬁne-tuning on Chi-\nnese NLU tasks. Note that all tasks use the layerwise lr\ndecay with decay rate 0.8.\nC Detailed Results for Ablation Studies\nWe present the detailed results on GLUE bench-\nmark for ablation studies in this section. The results\non different MLM objectives and sizes of n-gram\nlexicon are presented in Table 11. The detailed\nModels Size of MNLI QNLI QQP SST-2 CoLA MRPC RTE STS-BGLUESQuAD1.1 SQuAD2.0\nLexicon Acc Acc Acc Acc MCC Acc Acc PCC Avg EM F1 EM F1\nBERTReimplement 0K 84.9 91.8 91.3 92.9 58.8 88.1 69.7 88.6 83.4 83.4 90.2 76.4 79.2\n100K 85.4 92.3 91.3 92.9 60.4 88.7 72.6 89.6 84.1 84.2 90.8 78.4 81.5\nContiguously 200K 85.3 92.0 91.5 92.7 59.3 89.0 71.5 89.5 83.9 84.2 90.9 78.3 81.3\nMLM 300K 85.1 92.1 91.3 92.8 59.3 88.6 73.3 89.5 84.0 83.9 90.7 78.5 81.4\n400K 85.0 92.0 91.3 93.1 58.3 89.2 71.8 89.1 83.7 83.9 90.7 78.0 81.1\n100K 85.3 92.2 91.4 92.9 62.3 88.6 72.5 88.0 84.2 84.2 90.9 78.6 81.4\nExplicitly 200K 85.4 92.3 91.3 92.8 62.1 88.4 74.5 88.6 84.4 84.5 91.3 78.9 81.9\nN-gram MLM 300K 85.7 92.3 91.3 92.9 62.6 88.7 75.8 89.4 84.8 84.7 91.2 79.5 82.4\n400K 85.3 92.2 91.4 92.9 61.3 88.5 73.2 89.3 84.3 84.6 91.3 79.0 81.7\nTable 11: Results on the development set of the GLUE and SQuAD benchmarks with different MLM objectives\nand diverse sizes of n-gram lexicon.\n# Models MNLI QNLI QQP SST-2 CoLA MRPC RTE STS-BGLUE\nm mm Acc Acc Acc MCC Acc Acc PCC Avg\n#1 ERNIE-GramBASE 87.1 87.1 92.8 91.8 93.2 68.5 90.3 79.4 90.4 86.7\n#2 #1−relative position bias 86.5 86.4 92.5 91.6 93.2 68.1 90.3 79.4 90.6 86.5\n#3 #2−comprehensiven-gram prediction (CNP) 86.2 86.2 92.4 91.7 92.7 65.5 90.0 78.7 90.5 86.0\n#4 #2−enhancedn-gram relation modeling (ENRM) 85.7 85.8 92.6 91.2 93.5 64.8 88.9 76.9 90.0 85.5\n#5 #4−comprehensiven-gram prediction (CNP) 85.6 85.7 92.3 91.3 92.9 62.6 88.7 75.8 89.4 84.8\nTable 12: Comparisons between several ERNIE-Gram variants on GLUE benchmark. All the listed models are\npre-trained following the same settings of BERTBASE (Devlin et al., 2019).\nFigure 7: (a-c) Mean attention scores in the last self-attention layer. Texts in green, orange , red and blue boxes\nare named entities standing for organizations, locations, person and miscellaneous respectively.\nresults on ERNIE-Gram variants to verify the effec-\ntiveness of comprehensive n-gram prediction and\nenhanced n-gram relation modeling mechanisms\nare presented in Table 12. Results of ablation study\non relative position bias (Raffel et al., 2020) are\npresented in Table 13.\nD More cases on CoNLL2003 Dataset\nWe visualize the attention patterns of three sup-\nplementary cases from CoNLL2003 named entity\nrecognition dataset (Sang and De Meulder, 2003)\nto compare the performance of ERNIE-Gram, con-\ntiguously MLM and BERT (lowercased), as shown\nModels MNLI SST-2 SQuAD1.1 SQuAD2.0\nm mm Acc EM F1 EM F1\nMPNet (Song et al., 2020) 86.2 - 94.0 85.0 91.4 80.5 83.3\n−relative position bias 85.6 - 93.6 84.0 90.3 79.5 82.2\nUNILMv2 (Bao et al., 2020) 86.1 86.1 93.2 85.6 92.0 80.9 83.6\n−relative position bias 85.6 85.5 93.0 85.0 91.5 78.9 81.8\nERNIE-Gram 87.1 87.1 93.2 86.2 92.3 82.1 84.8\n−relative position bias 86.5 86.4 93.2 85.2 91.7 80.8 84.0\nTable 13: Ablation study on relative position bias (Raffel et al., 2020) for ERNIE-Gram and previous strong pre-\ntrained models like MPNet and UNILMv2.\nin Figure 7. For contiguously MLM, there are clear\ndiagonal lines in named entities that tokens prefer\nto attend to themselves. While for ERNIE-Gram,\nthere are bright blocks over named entities that to-\nkens attend to most of tokens in the same entity\nadequately to construct tight representation.",
  "topic": "n-gram",
  "concepts": [
    {
      "name": "n-gram",
      "score": 0.8343982100486755
    },
    {
      "name": "Computer science",
      "score": 0.7628722190856934
    },
    {
      "name": "Language model",
      "score": 0.7330867648124695
    },
    {
      "name": "Gram",
      "score": 0.6883156299591064
    },
    {
      "name": "Masking (illustration)",
      "score": 0.6085492968559265
    },
    {
      "name": "Natural language processing",
      "score": 0.5984267592430115
    },
    {
      "name": "Representation (politics)",
      "score": 0.5539090037345886
    },
    {
      "name": "Relation (database)",
      "score": 0.4761807322502136
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46373453736305237
    },
    {
      "name": "Natural language",
      "score": 0.4354356527328491
    },
    {
      "name": "Data mining",
      "score": 0.10594034194946289
    },
    {
      "name": "Bacteria",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 8
}