{
    "title": "Reasoning and Generalization in RL: A Tool Use Perspective",
    "url": "https://openalex.org/W2954378135",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5023117896",
            "name": "Sam Wenke",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5112042319",
            "name": "Dan Saunders",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5060195071",
            "name": "Mike Qiu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5066827188",
            "name": "Jim Fleming",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2739731449",
        "https://openalex.org/W2153574873",
        "https://openalex.org/W2054949188",
        "https://openalex.org/W2121863487",
        "https://openalex.org/W2142905600",
        "https://openalex.org/W2118450042",
        "https://openalex.org/W2937206389",
        "https://openalex.org/W756131464",
        "https://openalex.org/W2053212631",
        "https://openalex.org/W2481680724",
        "https://openalex.org/W2102959966",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W2902125520",
        "https://openalex.org/W2116817751",
        "https://openalex.org/W2122840034",
        "https://openalex.org/W2736601468",
        "https://openalex.org/W2086807633",
        "https://openalex.org/W1978580730",
        "https://openalex.org/W2898436992",
        "https://openalex.org/W2614839826",
        "https://openalex.org/W2893662673",
        "https://openalex.org/W2806859579",
        "https://openalex.org/W1520036242",
        "https://openalex.org/W2324959332",
        "https://openalex.org/W2042570062",
        "https://openalex.org/W2903181768",
        "https://openalex.org/W2008925121",
        "https://openalex.org/W2281096776",
        "https://openalex.org/W2053404075",
        "https://openalex.org/W2129553141",
        "https://openalex.org/W1632124792",
        "https://openalex.org/W1989678252",
        "https://openalex.org/W2101179296",
        "https://openalex.org/W2134831918",
        "https://openalex.org/W1496829078",
        "https://openalex.org/W2757631751",
        "https://openalex.org/W2765349170"
    ],
    "abstract": "Learning to use tools to solve a variety of tasks is an innate ability of humans and has been observed of animals in the wild. However, the underlying mechanisms that are required to learn to use tools are abstract and widely contested in the literature. In this paper, we study tool use in the context of reinforcement learning and propose a framework for analyzing generalization inspired by a classic study of tool using behavior, the trap-tube task. Recently, it has become common in reinforcement learning to measure generalization performance on a single test set of environments. We instead propose transfers that produce multiple test sets that are used to measure specified types of generalization, inspired by abilities demonstrated by animal and human tool users. The source code to reproduce our experiments is publicly available at https://github.com/fomorians/gym_tool_use.",
    "full_text": "Reasoning and Generalization in RL:\nA Tool Use Perspective\nSam Wenke\nFomoro AI\nsam@fomoro.com\nDan Saunders\nFomoro AI\ndan@fomoro.com\nMike Qiu\nFomoro AI\nmike@fomoro.com\nJim Fleming\nFomoro AI\njim@fomoro.com\nJuly 4, 2019\nAbstract\nLearning to use tools to solve a variety of tasks is an innate ability of humans and has been\nobserved of animals in the wild. However, the underlying mechanisms that are required to learn\nto use tools are abstract and widely contested in the literature. In this paper, we study tool use\nin the context of reinforcement learning and propose a framework for analyzing generalization\ninspired by a classic study of tool using behavior, the trap-tube task. Recently, it has become\ncommon in reinforcement learning to measure generalization performance on a single test set\nof environments. We instead propose transfers that produce multiple test sets that are used\nto measure speciﬁed types of generalization, inspired by abilities demonstrated by animal and\nhuman tool users. The source code to reproduce our experiments is publicly available at https:\n//github.com/fomorians/gym_tool_use.\n1 Introduction\nTool use is considered a hallmark of animal and human intelligence. Analysis of tool-using behaviors\nhave unveiled useful information about the cognitive mechanisms of various species. For example,\nwoodpecker ﬁnches on the Galapogos islands are able to use tools through learning by trial and error\n[1], while wild bottlenose dolphins appear to use marine sponges as foraging tools [2]. Furthermore,\nwild gorillas have been observed using branches to test the depth of water, as an aid in walking\nin deep water, and a bridge to cross a patch of swamp [3]. The sophisticated use of tools demand\npotentially complex cognitive reasoning abilities [4].\nCausal, relational (transitive), analogical, and symbolic reasoning have all been implicated in\nthe cognitive processes of tool users. Causal reasoning has been linked to the cognitive abilities of\ntool using animals and humans [5, 6], though some argue that only humans rely on “higher-order”\ncausal relations to make sense of the world [7, 8]. They suggest that humans possess a uniquely\nsystematic, domain-general relational reasoning ability, whereas other animal tool users rely on\ndomain-speciﬁc expectations for problem solving [9]. Symbolic reasoning requires the subject to\nrepresent a symbol (such as a drawing) as an actual object and as something else that the symbol\nstands for (dual representation [10]). This quality is naturally acquired by children at a young age,\nand may contribute to the generalization of a solution to a wide range of new contexts[11]. To be\nAll authors helped to write and edit the paper. Sam conceived the role of tool use experiments in reinforcement\nlearning, and with Mike’s help, conducted a thorough literature review, which was used to write large portions of the\npaper. Sam devised the experimental setup, and Dan ran the experiments and collated the results. Jim wrote the\nmodel and algorithm code, and with Sam’s help, set the overall direction of the research.\n1\narXiv:1907.02050v1  [cs.NE]  3 Jul 2019\nconsidered a tool user, an animal (or agent) must demonstrate their ability to systematically solve\na variety of reasoning-speciﬁc and domain-general tasks.\nLearning to use tools to solve a variety of tasks is an innate ability of humans and has been\nspeculatively observed of animals in the wild. The underlying mechanisms that are required to learn\nto use tools are abstract and widely contested in literature. Models of the requirements of tool\nusage inspired by animals and humans have been proposed, such as grasping or manipulation in\nrobots [12, 13, 14, 15, 16], combining hard-coded exploratory behaviors to perform more dynamic\nskills [17], and Inductive Logic Programming [18, 19, 20]. More complex models learn to plan future\ntrajectories to perform saw and cut behaviors [21], grasp and manipulate tools for new tasks [22],\nand utilize previously unseen objects from demonstrations and self-supervision through model-free\n[23] and model-based control [24, 25, 26]. Tool use generalization is still broadly unsolved and\nprior works lack the framework to properly measure it. In this paper, we propose a benchmark\nfor analyzing tool use and assessing generalization in a discrete environment inspired by a classic\nexperiment of animal tool using behavior.\nWe introduce a framework for evaluating the generalization of methods that learn to use tools\nin terms of the reinforcement learning (RL) framework [27]. Our contributions are inspired by prior\nwork studying the generalization of deep RL methods due to their notable ability to learn complex,\nhighly successful control algorithms. The generalization capabilities of RL algorithms is interesting\ndue to the lack of supervision given to agents in their training environments and the expectation\nthat the agent generalizes to possibly unseen environments and tasks. Learning policies that are\nsensitive to changes in the environment and can adapt to these variations is an important approach\nto improving the generalization ability of RL algorithms. For example, the introduction of diﬀerent\nmodes of emulated Atari 2600 games allowed for agents to be trained in one environment while\nbeing evaluated in slightly ones [28]. Systematically sampling the parameters of the dynamics of\nan environment measures the ability of RL algorithms to generalize to test environments that are\nsimilar to or diﬀerent from the training environments [29]. Most similar to our work, attempts at\nquantifying generalization have been made by carefully designing a set of procedurally generated\nenvironments that are split into training and testing sets [30].\nOur generalization study, inspired by tool use experiments and the complex reasoning abilities\nof animals, categorizes generalization into multiple test environments that can be used to evaluate\na particular set of reasoning abilities of learned agents. To introduce our framework, we deﬁne a\nset of simply represented discrete environments that test the use of a tool to solve domain-general\nvariations of an interpretation of a classic animal tool use study.\n1.1 Tool Use\nFor the purpose of our study, we present our working deﬁnition of tool use and how it relates to our\nstudy of RL. The deﬁnition of tool use has evolved over time to account for new observations of animal\ntool use as well as to better approximate our intuitive understanding on what it means to use tools.\nWe use the deﬁnition from [31] to frame our study, which can be found in Appendix: Deﬁnitions.\nThe manipulation of an object by the agent and the completion of a task in an environment is an\nimportant objective of RL and is the basis of our study.\n2 Methods\nWe adapt a classic experiment from the study of tool use behavior, called the trap-tube task [5].\nSubjects of the trap-tube task are presented with a transparent tube. Within the tube there is a\ntrap in the center and an object (representing a reward) placed next to the trap and out-of-reach\nof the subject. A stick of a ﬁxed length and a diameter smaller than the tube is placed next to the\nsubject. The stick represents a tool that can be used to push the object away from the trap and\nwithin reach of the subject from outside of the tube.\n2\nVariants of the trap-tube experiment have helped tease apart the cognitive mechanisms involved\nin complex tool use. Studies have shown that most chimpanzees, capuchin monkeys and New\nCaledonian crows rely on perceptual knowledge to solve the trap-tube task [5, 32]. That is, subjects\nthat learned to solve the base task continued to avoid the non-functional trap when the tube was\ninverted, suggesting that their solutions were informed by the perceptual features of the trap rather\nthan the understanding of its physical properties. However, a small subset of chimpanzees and crows\ndemonstrated physical reasoning abilities by solving a modiﬁed trap task where the sides of the tube\nare blocked and the food can only be pushed down a hole to an exit beneath the tube [33]. In this\nsetup, subjects require structural knowledge to solve the task because they need to understand the\nfunctional signiﬁcance of traps and barriers. Finally, a covered version of the trap task, in which\nstickers were placed in the same location as the traps and barriers, reveals the subjects’ ability to\nuse symbolic knowledge. Here, subjects have to reason symbolically by interpreting the stickers as\ntheir respective structures [34].\nAccording to Seed et al., [34] the use of perceptual, structural, and symbolic knowledge requires\nprogressively deeper levels of abstraction. We stick to this proposition and individually redeﬁne these\nrequirements as categories of generalization across a variety of reasoning-speciﬁc and domain-general\ntasks. We use the deﬁnitions for the requirements of generalization Seed et al. [34]:\n• Perceptual: Generalization across stimuli that share perceptual features.\n• Structural: Generalization across stimuli that share abstract, structural features.\n• Symbolic: Generalization across stimuli that share abstract, conceptual features.\nAn agent generalizes to a set of alterations to a ground truth task if it is able to succeed in\nboth the ground truth and altered tasks. In order to test this interpretation, we introduce a set of\nsimulated environments that are used to evaluate generalization to alterations of the classic trap-tube\ntask.\n3 Environments\nWe evaluate the generalization of the behavior of agents in environments given as RL problems\nknown as Markov decision processes (MDP, [27]). An MDP consists of a set of states S, a set of\nactions A, a transition kernel T : S×A→△S , a reward function R: S×A→ R, and an initial\nstate s0 ∈S drawn from a distribution P∈△S . An agent interacts with the MDP sequentially: at\neach timestep it observes the current state s∈S, takes an action a∈A, transitions to the next state\ns′ drawn from the distribution T(s, a), and receives a reward R(s, a). Additionally, we propose a\nstate transfer kernel F: S→S ′ to map a set of states of the MDP to another set of states.\nOur focus is on clarifying environments represented in a simple form, as gridworlds: a grid of\ncells, each occupied by an object with properties. Object properties are represented by a k-tuple:\n(o0, o1, ..., ok) ∈R. For simplicity, we choose k = 3 and can present the grid as an RGB image:\n(o0 = R, o1 = G, o2 = B). All environments use a grid of size 12 ×12. At each time-step, the agent\nobserves the entire grid as the current state sof size 12 ×12 ×3. An agent occupies one of the cells\nat any given time and can move and interact with adjacent objects as deﬁned by the set of grasp\nactions AG = {←g, →g, ↑g, ↓g}and move actions AM = {←m, →m, ↑m, ↓m}that make up\nthe action set of the environment A= AG ×AM . Each move action changes the agent’s position to\nthe adjacent cell in the corresponding direction, and grasp actions allow the agent to move with the\ntool object given that the agent is adjacent to the tool and the grasp action is in the direction of\nthe tool.\n3\nGround\n Agent\n Food\n Tool\n Tube (Top)\nTube (Bottom)\nTrap\n Exit\n(1) Initial state of the percep-\ntual trap-tube environment.\nObject positions and rota-\ntions are randomized and re-\ngenerated each episode.\n(2) The agent locates the tool\nin the top right of the grid,\nand moves it to the left of the\nfood and exit.\n(3) The agent uses the tool\nto move the food through the\nexit, out of the tube and\nwithin reach of the agent.\nFigure 1: States of a perceptual trap-tube environment.\n3.1 Trap-Tube Environments\nThe trap-tube environments test an agent’s ability to control a tool object with the goal of clearing\nthe agent’s path to a reward object. The reward object is surrounded by two tube barrier objects\npositioned opposite of one another, a trap barrier object and an exit object lie directly opposite\nof one another and adjacent to the tube barrier objects. The tool is initially located in a random\nposition within the environment, always in reach of the agent. An agent will be able to move with\nthe tool in a given direction, if it is adjacent to the tool and if a grasp action is made in the direction\nof the tool. The agent is not able to move or pass through a tube, exit, tool, or trap object, while\nthe reward object is able to pass through the exit object. If the agent reaches the reward object, the\nagent is rewarded ( r = 1) and the task is successfully completed. The agent is allowed to observe\n50 states in sequence of the environment task, ending on failure to complete the task if the reward\nobject is not reached ( r= 0).\nTo solve a task that requires the use of a tool is, in general, diﬃcult. The trap-tube environments\nare customized to test the generalization capability of agents. Alterations are made to the environ-\nment using the previously deﬁned categories of generalization: perceptual ( P), structural (St), and\nsymbolic (Sy). For the experiments outlined in this paper, we choose a base set of states Sbase such\nthat P(Sbase ∼△F(S)) > 0. This means that Sbase has a non-zero probability of being sampled\nfrom any given transfer, and is used as the set of states for the base MDP.\n3.1.1 Perceptual\nThe ability to perceive an object as a tool, regardless of the shape, position and rotation, along with\nsurrounding objects, is an innate ability of humans and observed of wild animals. The perceptual\ntransfer FP samples the position and orientation of the tool, the width, height, and orientation of\nthe tube, the position of the trap and exit of the tube, the position of the agent, and the position of\nthe food in the tube. An episode of a perceptual trap-tube environment is shown in Figure 1.\n4\nGround\n Agent\n Food\n Tool\n Tube (Top)\nTube (Bottom)\nTrap\n Exit\n(1) Initial state of the struc-\ntural trap-tube environment.\nObject properties are ran-\ndomized and regenerated each\nepisode.\n(2) The agent locates, suc-\ncessfully grasps, and moves\nthe tool towards the food and\nthe exit.\n(3) The agent uses the tool\nto move the food through the\nexit, out of the tube and\nwithin reach of the agent.\nFigure 2: States of a structural trap-tube environment.\n3.1.2 Structural\nSensory signals interpreted by humans and animals are represented by multiple modalities, such\nas visual and auditory. The generalization of sensory feature representations is important to solve\ntool use tasks; for instance, an agent placed in a dark, color-less room is expected to solve a task\nthat was originally demonstrated in a well-lit, colorful room. The structural transfer FSt samples\nproperties of the objects by category, excluding the agent and food (tool, trap, tube, exit, and\nground). We represent the features of the environment as points on the surface of an N-dimensional\nspherical manifold. This was chosen to have an internal consistency in the representations to enable\ninterpolation and to better represent how the real world is structured and represented consistently.\nN is chosen to be 3 to form RGB colors for the purposes of rendering, thus the manifold is a sphere,\nhowever other values of N should also work. An episode of a structural trap-tube environment is\nshown in Figure 2.\n3.1.3 Symbolic\nThe interpretation that an object can arbitrarily be represented as a tool is an important innovation\nand intuition of humans. The generalization of the relationship between the agent, tool, obstacle,\nreward objects is important to understanding symbolic reasoning. In the trap-tube task, thesymbolic\ntransfer FSy swaps the structural and perceptual properties of the base tool object with another\nobject sampled from the set of possible objects: {tool, trap, tube, exit}. A tool will behave the\nsame, regardless of how it is represented structurally and perceptually, acting as a ”symbol” of the\nenvironment to the agent. An episode of a symbolic trap-tube environment is shown in Figure 3.\n4 Experimental Setup\nIn order to measure the generalization of reinforcement learning algorithms for tool use, agents\nmust be evaluated on all possible transfer sets F: ( {FP }, {FSt}, {FSy}, {FP ,FSt}, {FP ,FSy},\n{FSt, FSy},{FP , FSt, FSy}). We outline the following experiments: (1) train agents on the base\n5\nGround\n Agent\n Food\n Tube (Bottom)\n Tube (Top)\nTool\nTrap\n Exit\n(1) Initial state of the sym-\nbolic trap-tube environment.\nThe base tool object is ran-\ndomly swapped with another\nobject, taking on the respec-\ntive perceptual and structural\nproperties of the objects.\n(2) The agent locates the tool\nrepresented perceptually and\nstructurally as the bottom of\nthe tube. The agent proceeds\nto move the tool away from\nthe direct path to the food.\n(3) The agent moves towards\nthe food. Although the tool is\nrepresented perceptually and\nstructurally as the bottom of\nthe tube, it still maintains the\ninteractions expected of the\ntool, such as passing over the\nexit object.\nFigure 3: States of a symbolic trap-tube environment.\nenvironment and (2) train agents on the transfer set {FP , FSt, FSy}. The goal of each experiment\nis to quantify the generalization of learned agents by measuring their success rate on all possible\ntransfer sets. Experiment (1) challenges learned agents conditioned on a single task sampled from\ntransfer set {FP , FSt, FSy}. Experiment (2) challenges learned agents to extrapolate reasoning\nabilities. Each experiment stresses the priors of learned agents. Successful agents must possess\npriors that enable learning to eﬀectively reason perceptually, structurally, and symbolically.\n5 Results\nWe present baseline results with a proximal policy optimization [35] (PPO) agent to use as a point\nof comparison for future work. Due to the sparsity of the task, we also include an intrinsic curiosity\nmodule [36] (ICM) to improve exploration. The agent observes the entire environment state and\nprocesses it into an embedding using two 2 ×2 convolutions, followed by two residual blocks and\nglobal spatial pooling to form an observation embedding. The agent’s move and grasp actions from\nthe previous time step are embedded into 8-dimensional vectors, while the reward from the previous\ntime step is fed through an 8-unit dense layer to form a reward embedding. These embeddings are\nconcatenated and fed to a 64-unit dense layer before ﬁnally feeding into a GRU [37]. The GRU\noutput is then used to compute action logits for move and grasp actions, a value prediction, and\nforward and inverse model predictions. The value prediction is used as the returns baseline while\nthe forward and inverse model predictions are used for the ICM auxiliary losses and rewards. The\nagent is capable of discovering sparse rewards and learning to use the tool in the base tool use task,\nhowever, it fails to solve the generalization tasks. On both tasks, we train agents on 5 separate trials\nwith a diﬀerent random seed on each.\n6\nFigure 4: Example training (left) and evaluation (right) reward curves averaged over the batch\ndimension for an agent trained with PPO + ICM.\nAlgorithm # trials task is solved µ±σ episodes to solve\nPPO 0 / 5 -\nPPO + ICM 5 / 5 102K ±60K\nTable 1: Number of trials in which the base task is solved, and mean (µ) plus or minus one standard\ndeviation (σ) episodes required to solve the base task, for PPO with and without ICM.\n5.1 Experiment 1\nAs described in Experimental Setup, Experiment 1 consists of training an agent on the base en-\nvironment, and quantifying its performance on all possible transfer sets. We train agents for 250\niterations, with a training batch size of 1024 episodes, or at most 12.8M transitions since episodes\nterminate early on successful completion of the task. We consider the task solved if the agent is able\nto solve it at least once during the evaluation phase, since its actions are deterministic and the task\nremains the same. Results are gathered in Table 1. The agent trained with PPO and ICM is able\nto solve the base task on all 5 trials, whereas, without ICM, the agent fails to solve it on any trial in\nthe allocated time. This is likely due to the sparse nature of the reward in this environment, where\nintrinsic curiosity is needed to encourage exploration. Both agents fail to solve any of the transfer\ntasks except for when it is sampled such that it is identical to the base task (data not shown).\nFigure 4 depicts average training and evaluation rewards from an example training run for an agent\ntraining with PPO and ICM. Since evaluation is deterministic, the policy either completely succeeds\nor completely fails to solve the task.\n5.2 Experiment 2\nFor Experiment 2, we train an agent on the transfer set {FP , FSt, FSy}. We use 500 training\niterations, which amounts to at most 25.6M training transitions. Figure 5 plots the percentage of\ntraining and evaluation tasks solved over the course of training for agents trained with PPO + ICM.\nTable 2 reports the average percentage of solved tasks over all transfer sets for both PPO and PPO\n+ ICM agents at the point in training where the highest overall evaluation is obtained. The PPO\n+ ICM agent outperforms the plain PPO agent, again possibly due to sparse rewards in all transfer\nenvironments.\nThe failure of either agent to completely solve any single transfer set (exhibiting a particular\nkind of reasoning) demonstrates the diﬃculty of the environments for existing RL methods. We\nexpect that, given enough compute, these environments may be conquered by the agents described\nabove without modiﬁcation. However, we hope that this work stimulates research into priors that\n7\nFigure 5: Mean ±1 standard deviation and maximum training and evaluation performance curves\nfrom training 5 PPO + ICM agents to solve tasks from {FP , FSt, FSy}.\nAlgorithm {FP} {FSt} {F Sy} {F P,FSt} {FP,FSy} {F St,FSy} {F P,FSt,FSy}\nPPO 0% 0% 35.5% ±10.9% 0% 7% ±3.6% 19.4% ±3.2% 4.4% ±3.2%\nPPO + ICM2%±3% 0% 40.2%±16.9% 0% 29.7%±8.1% 33.1%±19.7% 24.4%±9%\nTable 2: Percentage of tasks solved (mean ±1 standard deviation over 5 trials) during evaluation\non each of the transfer sets from training on transfer set {FP , FSt, FSy}.\nencourage diﬀerent modes of reasoning that result in improved sample eﬃciency and structured\nexploration.\n6 Discussion\nIn this section, we discuss the gap of research and ideas that exist between the ﬁelds of RL and\nanimal tool use. Although it is not yet evident that there exist methods that provide the prior\nreasoning capabilities needed to learn tool using behaviors in general, we provide some insight into\nwhy this may be and the mechanisms that might be required.\n6.1 Bridging the gap between RL and tool use\nAnimals and humans alike use tools to solve otherwise unsolvable problems, or to arrive at more\neﬃcient or safe solutions that are out of reach without certain tools [11]. In the same way, we\nconjecture that RL agents could beneﬁt by leveraging tools, for example by extending their reach\nwith a stick object as in the simulated trap-tube task. Tools eﬀectively modify the eﬀects of an\nagent’s actions, and accordingly, when equipped, it aﬀects the degree to which it can control its\nenvironment. There is evidence in humans and monkeys that using tools as an extension of the\nbody is followed by changes in neural networks that represent body schema [38], likely due to the\nchange in mechanical and sensory capabilities they aﬀord. Tools that enable better control over the\nenvironment should allow RL agents to discover solutions to problems that take fewer steps, or to\nsolve problems which, without them, cannot be solved at all.\nHow RL agents make use of objects in their environment can be characterized in the language\nof tool use: What objects can be considered tools? Does the agent employ relational, causal, or\nother modes of reasoning to use tools to make tasks easier or solve-able? An agent’s behavior can\n8\nbe studied from the perspective of tool use researchers, and can be updated to better match the\nlearning or exploration behavior exempliﬁed by model organisms. In this way, tool use researchers\nmay build simulation models of the real-world phenomena that they wish to study, enabling rapid\niteration and intuition building on the mechanisms behind tool use.\nWe claim that many of the tasks studied in the tool use literature may be cast in the RL\nframework, and indeed should be in order to test hypotheses concerning the cognitive processes\nunderlying tool use. This work takes a step towards realizing this goal by implementing a simulation\nof the well-studied trap-tube task. As mentioned above, the variations on the trap-tube task are\nwell-suited to studying diﬀerent forms of generalization, all which have real-world counterparts in\nhuman and non-human animal tool use experiments.\n6.2 Potential prior reasoning capabilities\nEach type of generalization introduced in this paper requires speciﬁc characteristics of agent behav-\nior. At present, to develop these characteristics, agents are only sparsely and extrinsically incen-\ntivized to learn to solve the tasks presented. We ask the question: What priors are required to solve\ntool use in general? The RL framework depends heavily on the reward signal provided to the agent\nover time. In the case studied in this paper, the reward signal is sparse and is only seen by the agent\nupon successfully completing the task. Without strong behavioral and reasoning priors, agents will\narrive at tool use seemingly at random and are unreasonably expected to generalize from limited\nexploratory experience.\nFor perceptual reasoning abilities, using a convolutional neural network (CNN) may be a strong\nprior. A CNN ﬁts the modality presented to the agent (i.e., observations of a grid world) and the\nphysically local nature of objects, but alone may not enable learning of tool using behaviors due to\nthe non-local nature of object-object interactions.\nA structural reasoning prior may not be as easily understood due to the nature of the feature\nspace provided. In the structural task, agents must remember the representation of objects in order\nto reason about a solution. Take for example the structural representations of thetrap and the exit:\nsince are always parallel to each other and adjacent to the tube, there may be no way to determine,\nwithout interaction, which object the food will pass through. Thus, the agent must remember\nwhich object-object interactions it has explored during each new episode, possibly developing a\ncausal understanding of the environment.\nSymbolic reasoning with tools is much less understood than perceptual and structural reasoning,\nbut may require similar priors. Agents must learn to discover the identity and proper utilization of\neach object and solve the task through changing object-object interactions on each new episode.\n7 Conclusion\nIn this paper, we reviewed aspects of tool use from the perspective of several ﬁelds of study and\nultimately chose to frame it as a RL problem. With the support of a classic trap-tube experiment\nas an example, we presented a framework for analyzing the generalization of RL agents, taking a\nstep towards understanding the priors required to learn to use tools. Furthermore, we deﬁned two\nexperimental setups, allowing researchers to deﬁne their own hypotheses for understanding tool use\nin the form of a RL generalization problem. Finally, we provided baselines using the well-established\nproximal policy optimization algorithm to compare against in future work.\nWe chose to focus on the deﬁnition of tool use, omitting many topics related to tool use such\nas proto-tool use, meta-tool use and tool manufacture. We brieﬂy outline these for the reader (see\nAppendix: Deﬁnitions) and leave exploration of the diﬀerences between them as they relate to RL\nfor future work.\n9\nAcknowledgements\nWe gratefully thank Danielle Swank for her dedicated mediation in the discussion of the research\nprocess behind this paper. We also thank Ed Costantini and Andrew Wang for their careful review\nof and helpful comments on the paper.\nReferences\n[1] Sabine Tebbich, Kim Sterelny, and Irmgard Teschke. The tale of the ﬁnch: adaptive radia-\ntion and behavioural ﬂexibility. Philosophical Transactions of the Royal Society B: Biological\nSciences, 365(1543):1099–1109, April 2010.\n[2] M. Krutzen, J. Mann, M. R. Heithaus, R. C. Connor, L. Bejder, and W. B. Sherwin. Cultural\ntransmission of tool use in bottlenose dolphins.Proceedings of the National Academy of Sciences,\n102(25):8939–8943, June 2005.\n[3] Thomas Breuer, Mireille Ndoundou-Hockemba, and Vicki Fishlock. First observation of tool\nuse in wild gorillas. PLoS Biology, 3(11):e380, October 2005.\n[4] I. Teschke, C. A. F. Wascher, M. F. Scriba, A. M. P. von Bayern, V. Huml, B. Siemers, and\nS. Tebbich. Did tool-use evolve with enhanced physical cognitive abilities? Philosophical\nTransactions of the Royal Society B: Biological Sciences , 368(1630):20120418–20120418, Octo-\nber 2013.\n[5] Elisabetta Visalberghi and Luca Limongelli. Lack of comprehension of cause-eﬀect relations in\ntool-using capuchin monkeys (cebus apella). Journal of Comparative Psychology, 108(1):15–22,\n1994.\n[6] Teresa McCormack, Christoph Hoerl, and Stephen Butterﬁll, editors. Tool Use and Causal\nCognition. Oxford University Press, August 2011.\n[7] James E. Reaux and Daniel J. Povinelli. The trap-tube problem. In Folk Physics for Apes ,\npages 108–131. Oxford University Press, May 2003.\n[8] Daniel J. Povinelli and Derek C. Penn. Through a ﬂoppy tool darkly. In Tool Use and Causal\nCognition, pages 69–88. Oxford University Press, August 2011.\n[9] Derek C. Penn, Keith J. Holyoak, and Daniel J. Povinelli. Darwin’s mistake: Explaining the\ndiscontinuity between human and nonhuman minds. Behavioral and Brain Sciences, 31(2):109–\n130, April 2008.\n[10] Judy S DeLoache, Kevin F. Miller, and Karl S. Rosengren. The credible shrinking room: Very\nyoung children’s performance with symbolic and nonsymbolic relations. Psychological Science,\n8(4):308–313, July 1997.\n[11] Amanda Seed and Richard Byrne. Animal tool-use. Current Biology, 20(23):R1032–R1039, dec\n2010.\n[12] Z. Li and S.S. Sastry. Task-oriented optimal grasping by multiﬁngered robot hands. IEEE\nJournal on Robotics and Automation , 4(1):32–44, 1988.\n[13] K.B. Shimoga. Robot grasp synthesis algorithms: A survey. The International Journal of\nRobotics Research, 15(3):230–266, June 1996.\n[14] Atsushi Yamashita, Jun Sasaki, Jun Ota, and Tamio Arai. Cooperative manipulation of objects\nby multiple mobile robots with tools *. 1998.\n10\n[15] S.K. Gupta, C.J.J. Paredis, and P.F. Brown. Micro planning for mechanical assembly opera-\ntions. In Proceedings. 1998 IEEE ICRA (Cat. No.98CH36146) . IEEE.\n[16] D. Halperin, J.-C. Latombe, and R. H. Wilson. A general framework for assembly planning:\nThe motion space approach. Algorithmica, 26(3-4):577–601, March 2000.\n[17] A. Stoytchev. Behavior-grounded representation of tool aﬀordances. In Proceedings of the 2005\nIEEE ICRA. IEEE.\n[18] Solly Brown and Claude Sammut. Tool use and learning in robots. In Encyclopedia of the\nSciences of Learning, pages 3327–3330. Springer US, 2012.\n[19] Handy Wicaksono and Claude Sammut. Relational tool use learning by a robot in a real and\nsimulated world. 2016.\n[20] Handy Wicaksono. Towards a relational approach for tool creation by robots. In Proceedings of\nthe Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence . International Joint\nConferences on Artiﬁcial Intelligence Organization, August 2017.\n[21] Ian Lenz, Ross A. Knepper, and Ashutosh Saxena. Deepmpc: Learning deep latent features for\nmodel predictive control. In Robotics: Science and Systems , 2015.\n[22] Kuan Fang, Yuke Zhu, Animesh Garg, Andrey Kurenkov, Viraj Mehta, Li Fei-Fei, and Silvio\nSavarese. Learning task-oriented grasping for tool manipulation from simulated self-supervision.\nCoRR, abs/1806.09266, 2018.\n[23] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, John Schulman, Emanuel Todorov, and\nSergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and\ndemonstrations. CoRR, abs/1709.10087, 2017.\n[24] Frederik Ebert, Chelsea Finn, Alex X. Lee, and Sergey Levine. Self-supervised visual planning\nwith temporal skip connections. CoRR, abs/1710.05268, 2017.\n[25] Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex X. Lee, and Sergey Levine.\nVisual foresight: Model-based deep reinforcement learning for vision-based robotic control.\nCoRR, abs/1812.00568, 2018.\n[26] Annie Xie, Frederik Ebert, Sergey Levine, and Chelsea Finn. Improvisation through physical\nunderstanding: Using novel objects as tools with visual foresight. CoRR, abs/1904.05538, 2019.\n[27] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT\nPress, second edition, 2018.\n[28] Jesse Farebrother, Marlos C. Machado, and Michael Bowling. Generalization and regularization\nin DQN. CoRR, abs/1810.00123, 2018.\n[29] Charles Packer, Katelyn Gao, Jernej Kos, Philipp Kr¨ ahenb¨ uhl, Vladlen Koltun, and Dawn\nSong. Assessing generalization in deep reinforcement learning. CoRR, abs/1810.12282, 2018.\n[30] Karl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim, and John Schulman. Quantifying\ngeneralization in reinforcement learning. CoRR, abs/1812.02341, 2018.\n[31] Robert St Amant and Thomas E. Horton. Revisiting the deﬁnition of animal tool use. Animal\nBehaviour, 75(4):1199–1208, apr 2008.\n[32] Alex Kacelnik, Jackie Chappell, Ben Kenward, and Alex A. S. Weir. Cognitive adaptations for\ntool-related behavior in new caledonian crows. In Comparative CognitionExperimental Explo-\nrations of Animal Intelligence , pages 515–528. Oxford University Press, April 2009.\n11\n[33] Amanda M. Seed, Josep Call, Nathan J. Emery, and Nicola S. Clayton. Chimpanzees solve the\ntrap problem when the confound of tool-use is removed. Journal of Experimental Psychology:\nAnimal Behavior Processes, 35(1):23–34, 2009.\n[34] Amanda Seed, Daniel Hanus, and Josep Call. Causal knowledge in corvids, primates, and\nchildren. In Tool Use and Causal Cognition , pages 89–110. Oxford University Press, August\n2011.\n[35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. CoRR, abs/1707.06347, 2017.\n[36] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven explo-\nration by self-supervised prediction. CoRR, abs/1705.05363, 2017.\n[37] KyungHyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the\nproperties of neural machine translation: Encoder-decoder approaches. CoRR, abs/1409.1259,\n2014.\n[38] Angelo Maravita and Atsushi Iriki. Tools for the body (schema). Trends in Cognitive Sciences,\n8(2):79 – 86, 2004.\n[39] Hugo Van Lawick and Jane Goodall. Innocent Killers. Houghton Miﬄin, 1971.\n[40] John Alcock. The evolution of the use of tools by feeding animals. Evolution, 26(3):464–473,\n1972.\n[41] Benjamin B. Beck. Animal Tool Behavior: The Use and Manufacture of Tools by Animals .\nGarland STPM Press, 1980.\n[42] Alex H. Taylor, Gavin R. Hunt, Jennifer C. Holzhaider, and Russell D. Gray. Spontaneous\nmetatool use by new caledonian crows. Current Biology, 17(17):1504–1507, September 2007.\n12\nA Appendix: Deﬁnitions\nA.1 Tool Use\nAlthough there are many proposed tool use deﬁnitions, in this paper we have decided that the Amant\nand Horton [31] deﬁnition is most representative of our study. There are other deﬁnitions that have\ninspired this work; we use the ﬁrst as the working deﬁnition for this paper:\nThe exertion of control over a freely manipulable external object (the tool) with the goal\nof (1) altering the physical properties of another object, substance, surface or medium\n(the target, which may be the tool user or another organism) via a dynamic mechanical\ninteraction, or (2) mediating the ﬂow of information between the tool user and the\nenvironment or other organisms in the environment. - Amant and Horton [31]\nThe use of an external object as a functional extension of mouth or beak, hand or claw,\nin the attainment of an immediate goal. — Lawick and Goodall [39], page 195\nTool-using involves the manipulation of an inanimate object, not internally manufac-\ntured, with the eﬀect of improving the animal’s eﬃciency in altering the form or position\nof some separate object. — Alcock [40], page 464\nThus tool use is the external employment of an unattached environmental object to alter\nmore eﬃciently the form, position, or condition of another object, another organism, or\nthe user itself when the user holds or carries the tool during or just prior to use and is\nresponsible for the proper and eﬀective orientation of the tool. — Beck [41], page 10\nA.2 Proto-Tool Use\nProto-tool use is distinguished from ”true” tool use in that the outcome is achieved via\na secondary object or substance, albeit not something deﬁned as a tool. — Amant and\nHorton [31]\nThe distinction between proto-tool use and ”true” tool use is made in behavioral neurophysiology\nbecause animals exhibiting the proto-tool use tend to have smaller brains than ”true” tool users.\nA.3 Meta-Tool Use\nThe ability to use one tool on another. – Taylor et al. [42]\nFor our purposes, we deﬁne meta-tool use as the utilization of a static object in a preparatory\nbehavior for tool use.\nA.4 Tool Manufacture\nTool manufacture involves the fashioning or modiﬁcation of objects in the environment\nto improve their suitability as tools. — Amant and Horton [31]\nTool manufacture is essentially multi-step tool use involving a preparation phase where the tool is\nmanufactured and an application phase where the tool is applied.\n13"
}