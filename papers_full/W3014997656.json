{
  "title": "NEWLSTM: An Optimized Long Short-Term Memory Language Model for Sequence Prediction",
  "url": "https://openalex.org/W3014997656",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2042208499",
      "name": "Qing Wang",
      "affiliations": [
        "Shandong University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5051804746",
      "name": "Rong-Qun Peng",
      "affiliations": [
        "Shandong University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2150948738",
      "name": "Jia Qiang Wang",
      "affiliations": [
        "Beijing Academy of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099417000",
      "name": "Zhi Li",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A4316014046",
      "name": "Han‐Bing Qu",
      "affiliations": [
        "Beijing Academy of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2042208499",
      "name": "Qing Wang",
      "affiliations": [
        "Shandong University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5051804746",
      "name": "Rong-Qun Peng",
      "affiliations": [
        "Shandong University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2150948738",
      "name": "Jia Qiang Wang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2099417000",
      "name": "Zhi Li",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A4316014046",
      "name": "Han‐Bing Qu",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6864424756",
    "https://openalex.org/W6729216784",
    "https://openalex.org/W6758055512",
    "https://openalex.org/W6737889394",
    "https://openalex.org/W6750886758",
    "https://openalex.org/W2614801503",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6743485176",
    "https://openalex.org/W2944851425",
    "https://openalex.org/W2977251608",
    "https://openalex.org/W6762929618",
    "https://openalex.org/W6629930100",
    "https://openalex.org/W6743406322",
    "https://openalex.org/W6639024717",
    "https://openalex.org/W2604319603",
    "https://openalex.org/W2964473287",
    "https://openalex.org/W1947758080",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2905559537",
    "https://openalex.org/W2955468701",
    "https://openalex.org/W6762286919",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2116261113",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W6701650085",
    "https://openalex.org/W2910965811",
    "https://openalex.org/W6760353705",
    "https://openalex.org/W2970509139",
    "https://openalex.org/W2982408358",
    "https://openalex.org/W2121029939",
    "https://openalex.org/W2916350893",
    "https://openalex.org/W2945755245",
    "https://openalex.org/W2612810742",
    "https://openalex.org/W2963304263",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963241221",
    "https://openalex.org/W2750814024",
    "https://openalex.org/W2951639640",
    "https://openalex.org/W4302375066",
    "https://openalex.org/W2952160759",
    "https://openalex.org/W2946135363",
    "https://openalex.org/W4288629177",
    "https://openalex.org/W1499864241",
    "https://openalex.org/W2951305674",
    "https://openalex.org/W2964309400",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W1844261860",
    "https://openalex.org/W2799217725",
    "https://openalex.org/W2547418827",
    "https://openalex.org/W2907774327",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2951176429",
    "https://openalex.org/W4294590131",
    "https://openalex.org/W2748780488",
    "https://openalex.org/W2138660131"
  ],
  "abstract": "The long short-term memory (LSTM) model trained on the universal language modeling task overcomes the bottleneck of vanishing gradients in the traditional recurrent neural network (RNN) and shows excellent performance in processing multiple tasks generated by natural language processing. Although LSTM effectively alleviates the vanishing gradient problem in the RNN, the information will be greatly lost in the long distance transmission, and there are still some limitations in its practical use. In this paper, we propose a new model called NEWLSTM, which improves the LSTM model, and alleviates the defects of too many parameters in LSTM and the vanishing gradient. The NEWLSTM model directly correlates the cell state information with current information. The traditional LSTM's input gate and forget gate are integrated, some components are deleted, the problems of too many LSTM parameters and complicated calculations are solved, and the iteration time is effectively reduced. In this paper, a neural network model is used to identify the relationship between input information sequences to predict the language sequence. The experimental results show that the improved new model is simpler than traditional LSTM models and LSTM variants on multiple test sets. NEWLSTM has better overall stability and can better solve the sparse words problem.",
  "full_text": "Received March 20, 2020, accepted March 29, 2020, date of publication April 3, 2020, date of current version April 17, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.2985418\nNEWLSTM: An Optimized Long Short-Term\nMemory Language Model for Sequence Prediction\nQING WANG\n 1, RONG-QUN PENG1, JIA-QIANG WANG\n 2, ZHI LI3, AND HAN-BING QU\n2\n1School of Computer Science and Technology, Shandong University of Technology, Zibo 255049, China\n2Key Laboratory of Artiﬁcial Intelligence and Data Analysis, Beijing Academy of Science and Technology, Beijing 100094, China\n3School of Economics and Management, University of Chinese Academy of Sciences, Beijing 100049, China\nCorresponding author: Rong-Qun Peng (pengrq2006@126.com)\nThis work was supported in part by the National Key Research and Development Program of China under Grant 2018YFF0301000,\nGrant 2018YFC0809700, and Grant 2018YFC0704800, in part by the National Natural Science Foundation of China under\nGrant NSF91746207, in part by the Beijing Postdoctoral Research Foundation under Grant ZZ-2019-76, and in part by the China\nPostdoctoral Science Foundation under Grant 2019M660540.\nABSTRACT The long short-term memory (LSTM) model trained on the universal language modeling task\novercomes the bottleneck of vanishing gradients in the traditional recurrent neural network (RNN) and shows\nexcellent performance in processing multiple tasks generated by natural language processing. Although\nLSTM effectively alleviates the vanishing gradient problem in the RNN, the information will be greatly\nlost in the long distance transmission, and there are still some limitations in its practical use. In this paper,\nwe propose a new model called NEWLSTM, which improves the LSTM model, and alleviates the defects of\ntoo many parameters in LSTM and the vanishing gradient. The NEWLSTM model directly correlates the cell\nstate information with current information. The traditional LSTM’s input gate and forget gate are integrated,\nsome components are deleted, the problems of too many LSTM parameters and complicated calculations are\nsolved, and the iteration time is effectively reduced. In this paper, a neural network model is used to identify\nthe relationship between input information sequences to predict the language sequence. The experimental\nresults show that the improved new model is simpler than traditional LSTM models and LSTM variants on\nmultiple test sets. NEWLSTM has better overall stability and can better solve the sparse words problem.\nINDEX TERMS Gate fusion, exploding gradient, long short-term memory, recurrent neural network.\nI. INTRODUCTION\nAlong with the transition from the traditional n-gram lan-\nguage model to the neural language model, research has\nrevealed the potential of the statistical language model on\nthe basic task of natural language processing (NLP) [1].\nA language model based on a neural network performs the\nimplicit clustering of words in a low-dimensional space,\nwhich can be used to predict many types of signals including\nlanguage [2] and has attracted widespread research attention.\nSequence prediction and classiﬁcation in natural language\nprocessing is a ubiquitous and challenging problem that\nusually requires identifying complex dependencies between\nlong-term inputs. The recurrent neural network (RNN), which\ncan model sequence data for sequence recognition and pre-\ndiction, shows strong performance in various NLP tasks [3].\nThe RNN performs the implicit clustering of words in a low-\ndimensional space, accepts input vectors at each time step,\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Fuhui Zhou\n .\nand updates the hidden state using a non-linear activation\nfunction. The RNN responds to time dependence through\nshort-term memory implemented using feedback [4], which\ncan effectively predict sequences at the next time step, and\nits hidden state can store information as a high-dimensional\ndistributed representation, forming a model with rich infor-\nmation. In addition, the RNN can realize effective and pow-\nerful calculations by using nonlinear dynamics, which can\nbe applied to sequences with highly complex structures to\nperform modeling and prediction tasks.\nThere are still many problems with using the RNN. For\nexample, forward and back propagation in the RNN is per-\nformed sequentially, which is time-consuming during train-\ning, and vanishing and exploding gradients may occur. When\nfaced with long-term memory, it is difﬁcult for the RNN to\ntrain them effectively. Therefore, there are still many chal-\nlenges in using the RNN for long sequence learning.\nIn recent research, many attempts have been made to\novercome these difﬁculties. In research in the word embed-\nding ﬁeld, Takase et al. [5] constructed word embeddings\nVOLUME 8, 2020 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 65395\nQ. Wanget al.: NEWLSTM: Optimized LSTM Language Model for Sequence Prediction\nfrom character n-gram embeddings and proposed an RNN\nlanguage model that uses character information combined\nwith ordinary embeddings. The method achieved the best\nconfusion on each dataset. Wen et al. [6] proposed a statistical\nlanguage generator based on joint recursive and convolutional\nneural network structures, which can be trained on dialog\nactions without any semantic alignment or predeﬁned syntax\ntrees. Pappas and Henderson [7] studied the usability of\npowerful shared mappings for output labels and proposed a\ndeep residual output mapping. The method performed well\nat capturing the structure of the output space and avoiding\noverﬁtting and solved the shortcoming of the lack of shared\nparameters across output labels in neural language models.\nThe long short-term memory (LSTM) proposed by\nSmagulova and James [8] is a variant of the RNN. It over-\ncomes the vanishing gradients in the traditional RNN by\nloading information at each time step for transmission.\nIt improves the structure of the traditional RNN hidden layer\nand solves the problem that the long-term dependence of the\nencoding in the input sequence cannot be achieved due to the\nvanishing gradient in the RNN [9]. In previous studies, for\ngeneral sequence modeling, LSTM has proven to be stable\nand powerful for modeling remote dependencies [10]–[13].\nHowever, although the text generated by LSTM shows long-\nterm correlation characteristics on reproducible scales, it does\nnot solve the common problem of constructing simple recur-\nsive networks. Although the vanishing gradient problem of\nthe RNN is partially avoided, the information loss in the\nlong distance transmission is very serious [14]. LSTM cannot\nexplore the information of prediction sequences that have\nsigniﬁcant changes over a short period of time. To improve\nthe network convergence speed, people add gate structures\nto the standard LSTM structure to enhance the long-term\ndependent learning ability of LSTM variants [15], [16].\nBy using a periodic function to parameterize each hidden\nunit to affect the gradient information ﬂow, the initialization\nparameters need to be ﬁne-grain adjusted. In addition, when\ndealing with complex long-term and short-term dependence\nproblems when capturing multi-dimensional time series data\nincluding future time steps, increasingly more attention-\nbased multi-time series models have been applied [17].\nAccording to the problems existing in LSTM, this article\nproposes that the NEWLSTM model is improved in the fol-\nlowing three aspects. (1) The forget gate and input gate of the\nLSTM model are integrated and processed together to reduce\nthe number of parameters and simplify the calculation of the\nmodel, which reduces the complexity of the model and effec-\ntively reduces the iteration time. (2) The vanishing gradient\nproblem in some regions is solved. (3) It pays attention to the\nhidden information of the current information, emphasizes\nthe subject of the input information, and makes the model\npay more attention to the context.\nII. RELATED WORKS\nLSTM alleviates the vanishing gradient problem caused by\nhindering the back propagation in the RNN. It can solve\nmany tasks that cannot be solved by recurrent neural network\nlearning algorithms. It has been widely used in speech recog-\nnition, picture description, natural language processing and\nother ﬁelds [18]. Yao et al. [19] proposed a convolutional\nLSTM (ConvLSTM) and used it to build a trainable model of\nend-to-end precipitation nowcasting. Aiming at the internal\ncovariate shift between time steps, Cooijmans et al. [20]\nproposed the re-parameterization of LSTM, which proved the\neffectiveness of the batch standardized conversion of hidden\nlayer to hidden layer and used batch normalization to perform\nrecursive network optimization. Zhang et al. [21] developed\na deep adaptive long short-term memory (DA-LSTM) archi-\ntecture, which can dynamically adjust the structure based\non the information distribution without prior knowledge.\nWhen faced with very little information, shallow structures\ncan be used to achieve faster convergence and consume less\ncomputing resources. Ali et al. [22] proposed a new type of\nsemantic knowledge based on the Word2vec model, which\nused the bidirectional long short-term memory (Bi-LSTM)\nmethod to improve the trafﬁc feature extraction and text\nclassiﬁcation tasks.\nAiming at the problems of LSTM, in recent decades,\npeople have done much work to optimize LSTM models\n[9], [14], [23], [24], [25]. Kent and Salem [23] proposed\na slim model to simplify the LSTM model by removing\ncertain components, thereby effectively speeding up the train-\ning and running time in the case of performance changes.\nJie and Lu [24] proposed an LSTM-CRF model guided by\ndependencies. By encoding the complete dependency tree\nand capturing the syntactic relationships for named entity\nrecognition, the study showed that there is a strong relevance.\nde Lhoneux et al. [25] used the features extracted by BiLSTM\nto recursively combine subtrees in a transition-based parser to\nstudy the effect of adding tree layers to the sequential model.\nThe results indicate that Bi-LSTM can effectively capture\nrelated subtrees’ information. By associating forward and\nbackward LSTM with functions and results with language\nattributes, it is proved that the improved backward LSTM is\nparticularly important for the formation of the ﬁnal language.\nIII. MODELS\nA. LONG SHORT-TERM MEMORY\nThe RNN supports variable-length inputs. By mapping the\ntarget vector from all previous records of previous inputs,\nit overcomes the bottleneck that traditional neural network\nstructures can only map from the current input to the target\nvector. It can capture the content of time data and make\ninferences; it is also very effective at the dynamic modeling of\ncontinuous sequence data [18]. However, when the gradient\nis calculated by RNN backpropagation, it is easy for the\ngradient to diverge and disappear due to too many layers, it is\nimpossible to learn long-term information, and the method is\nvery sensitive to the length of the article.\nLSTM uses multiple gates in a similar structure to the\nRNN to adjust the amount of information of each node state,\n65396 VOLUME 8, 2020\nQ. Wanget al.: NEWLSTM: Optimized LSTM Language Model for Sequence Prediction\nwhich effectively solves the problem that the previous infor-\nmation disappears in the RNN because the hidden layer\ncontinuously superimposes the input sequence in the new\ntime state. The disadvantage of forward propagation can\neffectively overcome the vanishing gradient problem [26].\nIts hidden layer update is replaced by a dedicated memory\nunit Ct , which performs better in the face of longer sequences\nand is suitable for context-sensitive language learning [27].\nCt is essentially an accumulator of state information, which\nis capable of remembering information that needs to be stored\nfor a long time.\nIn a two-layer feedforward neural network, connections\nexist between adjacent layers and hidden layers, and the\nnodes in the hidden layers are unconnected. The simple recur-\nrent network adds a feedback connection from the hidden\nlayer to the hidden layer, allowing information about the state\nof the hidden layer to be propagated over time. If the state of\neach moment is regarded as a layer of a feedforward neural\nnetwork, the recurrent neural network can be considered as a\nneural network with weights shared in the time dimension.\nThe LSTM model controls the transmission state through\nthe gated state, which can be accessed through several\nself-parameterized control gates, namely, the forget gate ft ,\nthe input gate it , and the output gate ot . However, due to\nthe introduction of much content, the number of parameters\nis increased, which also makes the training more difﬁcult.\nThe basic unit in the hidden layer of the LSTM network is a\nmemory block, which contains one or more memory units and\na pair of multiply gated units to gate all the units contained in\nthe hidden layer. The speciﬁc implementation of the LSTM\ninternal structure is as follows :\nit =σ(Wxixt +Whiht−1 +Wcict−1 +bi) (1)\nft =σ(Wxf xt +Whf ht−1 +Wcf ct−1 +bf ) (2)\nct =ft ct−1 +it tanh(Wxcxt +Whcht−1 +Whcht−1 +bc) (3)\not =σ(Wxoxt +Whoht−1 +Wcoct +bo) (4)\nht =ot tanh(ct ) (5)\nEach time a new message xt is input, the forget gate ft\nis open, and the past cell state may be discarded during\nthis process. Otherwise, the input gate it is activated, the\ninformation is accumulated in the unit, and the output gate ot\njudges whether it will be propagated to the ﬁnal state ht . At a\ncertain time point in the time series, the output information\nis uploaded to the memory unit to prevent the problem of\ninformation disappearing after too long.\nIn recurrent neural networks, gradients can explode or\nvanish exponentially over time. Therefore, the gradient either\ndominates the next weight adaptation step or effectively dis-\nappears.\nThe LSTM propagates backwards from the top, not from\nthe last error signal, and the error signal decreases or explodes\nwith the multiple layers of non-linear transformation.\nHowever, traditional LSTM performs best on data sequences\nwhere information is evenly distributed between steps. For a\nhigh information ﬂow, the transfer function from one hidden\nstate to the next hidden state in the LSTM is too shallow and\nthere is a lack of depth between the design steps, which can\nbe considered a single linear transformation with activation;\nand the underlying structure in the sequence cannot be cap-\ntured for sequences with uneven information ﬂows between\nsteps [28].\nB. NEWLSTM\nAiming at the problems in LSTM, this paper proposes a new\nvariant called NEWLSTM. NEWLSTM loads the informa-\ntion at each time step for transmission. The forget gate and\ninput gate are fused to combine the current input information\nwith the cell state; the information is accumulated in the\ncell; and, ﬁnally, the output gate ot determines whether the\ninformation is propagated to the ﬁnal state ht .\nNEWLSTM modiﬁed the network architecture and used\nthe tanh activation function to compress new information to\nimprove the vanishing gradient problem.\ntanh x =ex −e−x\nex +e−x (6)\nThe tanh function compresses the input value to the range\nof −1∼1, which solves the non-zero-centered problem of the\nsigmoid function. NEWLSTM saves the selective discarding\nof some input information in the forget gate, which is beneﬁ-\ncial to long sequence memory information.\nNEWLSTM retains the use of the tanh activation function\nin the input gate, focuses on the hidden information in the\nsentence, and makes the model focus on the topic of the\nsentence. The speciﬁc implementation of the internal cell\nstructure of the NEWLSTM is as follows :\nft =σ(Wxf xt +Whf ht−1 +Wcf ct−1 +bf ) (7)\nct =ft (ct−1 +tanh (Wxcxt +Whcht−1 +Whcht−1 +bc) (8)\nxt is an input vector at time t, which is used to store all\nuseful information at time t. W is the weight matrix of the\nhidden state ht and is the weight of each gate. bf ,bc and bo\nrepresent the deviation vectors. ft and ot are the gated scalars\nof the network, ht is the output cell state, and ct is the storage\nunit state. NEWLSTM loads the information at each time step\nfor transmission. The input at time t depends on the output\nat t −1. At time t, when new information xt is input, if the\ngate ft forgets to open, the past cell state may be discarded\nduring this process. Otherwise, the input gate it is activated,\nthe information is accumulated in the unit, and the output gate\not judges whether it will be propagated to the ﬁnal state ht .\nThe internal structure of NEWLSTM is as follows.\nHere, x is the input element and t is the time step. x repre-\nsents the elements of time t, which can represent a set of codes\nfor word features, dense vector features, or sparse features.\nThe output layer represents the probability distribution of the\nlabel at time t, and its size is the same as the label size.\nThe hidden layer at time t combines the output of the hidden\nlayer at time t −1 to realize the transfer of information. After\nthe memory unit is updated, the hidden layer calculates ht\naccording to the result obtained by the current output gate.\nVOLUME 8, 2020 65397\nQ. Wanget al.: NEWLSTM: Optimized LSTM Language Model for Sequence Prediction\nFIGURE 1. New model block.\nNEWLSTM controls the transmission through the gated\nstate, and combines the input gate and the forget gate to\nsimplify the processing of the input layer. When new infor-\nmation is input each time, it is ﬁrst combined with the\nprevious cell state to simplify the calculation of the input\ngate. It forgets what it has learned and new information, and\nselectively deletes or adds information to the next unit by\nadjusting the information ﬂow of the forget gate and output\ngate. The current information is combined with the past cell\nstate, the amount of retained information is determined by\nthe sigmoid function, and the information to be retained in\nthe ﬁnal state is determined according to the output gate ot .\nNEWLSTM retains the dedicated memory storage unit of\nLSTM and retains the dependency relationship, which facili-\ntates the discovery and establishment of long-term dependen-\ncies between input values.\nIV. EXPERIMENTS\nA. TRAINING PROCEDURE\nIn many applications, neural network models have shown\nexcellent performance and the potential to surpass other com-\nputing technologies [18]. The neural network model receives\nthe input sequence and attempts to accurately predict the\noutput sequence based on the input sequence. A given neu-\nral network model can be trained using a large number of\ntraining examples and iteratively repeated until the neural\nnetwork model can consistently draw similar inferences from\nthe training examples that humans may make. It is difﬁcult to\ntrain a recurrent neural network via back-propagation using\ntime [29], which is mainly because the gradient propagated\nback through the network will vanish or grow exponentially.\nThis experiment builds a model on the basis of a neural\nnetwork commonly used in sequence tasks to predict the\nprobability of the next word appearing. The validity of our\nmodel is veriﬁed by comparing the two-layer original LSTM\nwith two-layer LSTM variants.\nBy creating a dictionary using the original text, the arti-\ncles, questions, and answers are mapped to a vocabulary and\nfurther mapped in the form of a vector. The results are\nimported into the two-layer LSTM through the input layer,\nand then the internal features of the sentence are learned\nfrom the information output by the LSTM using the attention\nmechanism. Attention overcomes the limitations in the codec\narchitecture by making the network aware of the input atten-\ntion position of each item in the output sequence. The ﬁnal\nsequence label is predicted by preserving the intermediate\noutputs from each step of the input sequence of the encoder\nLSTM and training the model to learn to selectively focus\non these inputs and associate them with items in the output\nsequence.\n1) DATA\nThis experiment uses 10034964 pieces of data from the Penn\nTreebank Corpus. Adjacent sentences can be derived from\nparagraphs or adjacent paragraphs. Due to the particularity\nof the data, the output and unit state of the last hidden layer\nof each batch of data are used as the input of the next hidden\nlayer and unit state.\nIn the experiment, we used perplexity (ppl) to measure the\nlanguage model, implemented a two-layer LSTM network,\nand then, the LSTM results were used to predict the prob-\nability of the next word appearing. We calculate the cross\nentropy of each probability and the actual next word, then\nwe sum them up and calculate the power of e to obtain the\nconfusion ppl. By calculating the ppl, the probability of a\nsentence appearing is calculated based on each word, and the\nsentence length is used for normalization. It is expressed as\nfollows:\nPP (S)=P(w1,w2 ... wn)−1\nN (9)\nHere, S represents the sentence, N is the sentence length,\nand P( wi) represents the probability of the i-th word. If the\nppl decreases, P (wi)becomes larger and the model becomes\nmore accurate.\nThe variation curves of the training ppl of NEWLSTM are\nshown in Fig. 2, respectively. It can be seen that the initial pre-\ndiction accuracy rate is very low. As the number of iterations\nincreases, the validation ppl and training ppl ﬁrst decrease\nsigniﬁcantly, the prediction accuracy rate greatly increases,\nand it ﬁnally stabilizes after the number of iterations is greater\nthan 5.\n2) RESULTS\nThe evaluation index of the experiment is the perplexity,\nwhich is calculated simultaneously for the training set and the\ntest set. Some of the hyperparameters used in this paper are\ndivided into small, medium, and large speciﬁcations, which\nare mainly derived from the experience of previous paper\nstudies, such as the learning rate, the maximum gradient value\nused to control the exploding gradient, and the dropout ratio.\nIn addition, some parameters are conﬁgured according to the\ndepth of the LSTM and hardware conditions, such as the\nbatch size and the number of GPUs. To verify the validity of\nour model, we use the three conﬁgurations of Small, Medium,\n65398 VOLUME 8, 2020\nQ. Wanget al.: NEWLSTM: Optimized LSTM Language Model for Sequence Prediction\nFIGURE 2. Training ppl of NEWLSTM.\nand Large for comparison. The experimental conﬁgurations\nare as follows.\nIn our experiment, we chose the traditional LSTM and\nthe classic LSTM variant for the experimental comparison.\nSalem and Fathi [30] proved that by simplifying the LSTM\nlayer, three different LSTM variants, SLIM1, SLIM2, and\nSLIM3, can be obtained. It is possible to speed up the train-\ning and running time by changing the conﬁguration of the\nparameters with limited performance changes. PhasedLSTM\nconverges faster than traditional LSTM, improves the perfor-\nmance of LSTM, and reduces the amount of calculations by\norders of magnitude at runtime [31]. HyperLSTM generates\nnon-shared weights for LSTM, requires fewer parameters to\nlearn, and obtains nearly new results on various sequence\nmodeling tasks [32]. DA-LSTM is beneﬁcial for processing\nsequential data with a non-uniform information distribution,\nlearning the potential structure of the sequential input, and\ndynamically adjusting the number of operations performed,\nand it can greatly reduce the amount of calculations [21].\nIn this experiment, the CPU of the machine operates at\n2.8 GHz, and its L3 cache is 6 MB. The size of the\nmodel and the number of training rounds are sequentially\nincreased according to the three different speciﬁcations of\nSmall, Medium, and Large. Through the prediction analysis\nof the language sequence, different experimental results are\nobtained.\nAs seen from the comparison of the results of the three dif-\nferent conﬁgurations, compared to LSTM, SLIM1, SLIM2,\nand SLIM3, the NEWLSTM model can better solve the prob-\nlem of sparse words, the prediction of language sequences\nis good, and the overall performance exceeds that of the\ntraditional LSTM. Experiments have shown that although\nthe standard LSTM imposes a certain structure, the actual\nperformance is not as good as that of NEWLSTM. Selecting\nthe best hyperparameters in SLIM1, SLIM2, and SLIM3 may\nachieve powerful performance in each variable. However,\nit can be seen that the performance of NEWLSTM in the three\nspeciﬁcations is relatively stable.\nFor PhasedLSTM, due to the incompatibility between\nthe underlying structure involved in the sequence and the\nTABLE 1. Three configurations of the experiment.\nTABLE 2. ppl in Small configuration.\ntime assumptions of the phased LSTM, the effect cannot\nexceed that of NEWLSTM. In all experimental comparisons,\nthe performance of DA-LSTM is not very good. HyperLSTM\nshows good performance under the small experimental con-\nﬁguration, but as the experimental speciﬁcations increase,\nVOLUME 8, 2020 65399\nQ. Wanget al.: NEWLSTM: Optimized LSTM Language Model for Sequence Prediction\nTABLE 3. ppl in Medium configuration.\nTABLE 4. ppl in Large configuration.\nFIGURE 3. Average time taken for each training iteration.\nits perplexity also increases, and the performance of the\nstructure decreases.\nUnder the three different conﬁguration speciﬁcations of\nSmall, Medium, and Large, the average time taken by each\nLSTM iteration and variant during training is shown in Fig. 3.\nThe NEWLSTM simpliﬁes the processing of the input gate\nin the LSTM and combines it with the forget gate to simplify\nthe calculation complexity. As shown in Fig. 3, for the three\ndifferent speciﬁcations of the NEWLSTM conﬁguration,\nthe time consumed during the iteration has been effectively\nreduced In addition, the performance is more stable than those\nof SLIM1, SLIM2, and SLIM3, and the time consumed does\nnot ﬂuctuate greatly. The time consumed by the traditional\nLSTM has not changed signiﬁcantly with the change of the\nconﬁguration speciﬁcations.\nFrom the experiment, we can see that the convergence\ntime of the single-cell structure is signiﬁcantly shorter than\nthat of the layered structure. Compared with the traditional\nLSTM and multiple LSTM variants, NEWLSTM maintains\na simple structure, reduces the amount of calculations, and\ngreatly reduces the convergence time.\nV. CONCLUSION\nAiming at the shortcomings of the complex parameters in\nLSTM, the large amount of calculations, and the large loss\nof information transmitted too far away, this paper proposes\nan optimization model called NEWLSTM. It combines the\ncurrent input information with the cell state after processing,\nand the information is loaded at each time. The steps are\ntransmitted, then the information is accumulated in the cell\nunit, and ﬁnally the output gate judges whether the informa-\ntion is propagated to the ﬁnal state. NEWLSTM improves\nthe vanishing gradient problem of LSTM. By receiving the\ninput information and predicting the language sequence,\nexperiments prove that compared with the traditional LSTM\nmodel, the language sequence prediction accuracy of the\nNEWLSTM model is higher, which can effectively reduce the\niteration time. Compared with SLIM1, SLIM2, and SLIM3,\nNEWLSTM does not experience large ﬂuctuations in the\ntraining time due to conﬁguration parameter changes, and\nits performance is relatively stable. The predictions of the\nmodel in natural language processing show a more optimized\neffect than those of LSTM and can better solve the problem\nof sparse words.\nREFERENCES\n[1] Y . Bengio and D. R. V . Pascal, ‘‘A neural probabilistic language model,’’\nJ. Mach. Learn. Res. , vol. 3, pp. 932–938, Jan. 2000.\n[2] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khudanpur,\n‘‘Extensions of recurrent neural network language model,’’ in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP) , Prague,\nCzech Republic, May 2011, pp. 5528–5531, doi: 10.1109/ICASSP.\n2011.5947611.\n[3] Y . Zhao, Y . Shen, and J. Yao, ‘‘Recurrent neural network for text classiﬁ-\ncation with hierarchical multiscale dense connections,’’ in Proc. 28th Int.\nJoint Conf. Artif. Intell. , Aug. 2019, pp. 5450–5456.\n[4] J. Koutník, K. Greff, F. Gomez, and J. Schmidhuber, ‘‘A clockwork RNN,’’\nin Proc. 31st Int. Conf. Mach. Learn. , 2014, pp. 1–4. [Online]. Available:\nhttp://home.process.com/Intranets/wp2.htp\n[5] S. Takase, J. Suzuki, and M. Nagata, ‘‘Character n-gram embeddings to\nimprove RNN language models,’’ in Proc. AAAI Conf. Artif. Intell. , 2019,\npp. 5074–5082. [Online]. Available: https://arxiv.org/abs/1906.05506\n[6] T.-H. Wen, M. Gasic, D. Kim, N. Mrksic, P.-H. Su, D. Vandyke, and\nS. Young, ‘‘Stochastic language generation in dialogue using recur-\nrent neural networks with convolutional sentence reranking,’’ 2015,\narXiv:1508.01755. [Online]. Available: http://arxiv.org/abs/1508.01755\n[7] N. Pappas and J. Henderson, ‘‘Deep residual output layers for neural\nlanguage generation,’’ May 2019, arXiv:1905.05513. [Online]. Available:\nhttp://arxiv.org/abs/1905.05513\n[8] K. Smagulova and A. James, ‘‘Overview of long short-term memory\nneural networks,’’ inDeep Learning Classiﬁers With Memristive Networks .\nCham, Switzerland: Springer, 2020, pp. 139–153.\n[9] F. A. Gers, ‘‘Learning to forget: Continual prediction with LSTM,’’ in Proc.\n9th Int. Conf. Artif. Neural Netw. (ICANN) , Edinburgh, U.K., vol. 2, 1999,\npp. 850–855, doi: 10.1049/cp:19991218.\n65400 VOLUME 8, 2020\nQ. Wanget al.: NEWLSTM: Optimized LSTM Language Model for Sequence Prediction\n[10] F. M. Bianchi, E. Maiorino, M. C. Kampffmeyer, A. Rizzi, and R. Jenssen,\n‘‘An overview and comparative analysis of recurrent neural networks\nfor short term load forecasting,’’ May 2017, arXiv:1705.04378. [Online].\nAvailable: http://arxiv.org/abs/1705.04378\n[11] A. T. Mohan and D. V . Gaitonde, ‘‘A deep learning based approach\nto reduced order modeling for turbulent ﬂow control using LSTM\nneural networks,’’ Apr. 2018, arXiv:1804.09269. [Online]. Available:\nhttp://arxiv.org/abs/1804.09269\n[12] R. G. Hefron, B. J. Borghetti, J. C. Christensen, and C. M. S. Kabban,\n‘‘Deep long short-term memory structures model temporal dependencies\nimproving cognitive workload estimation,’’Pattern Recognit. Lett., vol. 94,\npp. 96–104, Jul. 2017.\n[13] I. Sutskever, O. Vinyals, and Q. V . Le, ‘‘Sequence to sequence learn-\ning with neural networks,’’ 2014, arXiv:1409.3215. [Online]. Available:\nhttp://arxiv.org/abs/1409.3215\n[14] S. Chang, Y . Zhang, W. Han, M. Yu, X. Guo, W. Tan, X. Cui,\nM. Witbrock, M. Hasegawa-Johnson, and T. S. Huang, ‘‘Dilated recur-\nrent neural networks,’’ 2017, arXiv:1710.02224. [Online]. Available:\nhttp://arxiv.org/abs/1710.02224\n[15] Y . Yu, X. Si, C. Hu, and J. Zhang, ‘‘A review of recurrent neural networks:\nLSTM cells and network architectures,’’ Neural Comput. , vol. 31, no. 7,\npp. 1235–1270, Jul. 2019.\n[16] J. Hu and W. Zheng, ‘‘Transformation-gated LSTM: Efﬁcient capture of\nshort-term mutation dependencies for multivariate time series prediction\ntasks,’’ inProc. Int. Joint Conf. Neural Netw. (IJCNN) , Budapest, Hungary,\nJul. 2019, pp. 1–8.\n[17] T. Guo, T. Lin, and N. Antulov-Fantulin, ‘‘Exploring interpretable\nLSTM neural networks over multi-variable data,’’ in Proc. ICML , 2019,\npp. 2494–2504.\n[18] H. Sak, A. Senior, and F. Beaufays, ‘‘Long short-term memory based recur-\nrent neural network architectures for large vocabulary speech recognition,’’\nComput. Sci., pp. 338–342, Sep. 2014.\n[19] K. Yao, T. Cohn, K. Vylomova, K. Duh, and C. Dyer, ‘‘Depth-gated recur-\nrent neural networks,’’ 2015, arXiv:1508.03790v1. [Online]. Available:\nhttps://arxiv.org/abs/1508.03790v1\n[20] T. Cooijmans, N. Ballas, C. Laurent, Ç. Gülşehre, and A. Courville,\n‘‘Recurrent batch normalization,’’ Mar. 2016, arXiv:1603.09025. [Online].\nAvailable: http://arxiv.org/abs/1603.09025\n[21] Y . Zhang, K.-H. Chow, and S.-H. G. Chan, ‘‘DA-LSTM: A long short-\nterm memory with depth adaptive to non-uniform information ﬂow\nin sequential data,’’ Jan. 2019, arXiv:1903.02082. [Online]. Available:\nhttp://arxiv.org/abs/1903.02082\n[22] F. Ali, S. El-Sappagh, and D. Kwak, ‘‘Fuzzy ontology and LSTM-based\ntext mining: A transportation network monitoring system for assisting\ntravel,’’Sensors, vol. 19, no. 2, p. 234, 2019, doi: 10.3390/s19020234.\n[23] D. Kent and F. Salem, ‘‘Performance of three slim variants of the\nlong short-term memory (LSTM) layer,’’ in Proc. IEEE 62nd Int. Mid-\nwest Symp. Circuits Syst. (MWSCAS) , Dallas, TX, USA, Aug. 2019,\npp. 307–310.\n[24] Z. Jie and W. Lu, ‘‘Dependency-guided LSTM-CRF for named entity\nrecognition,’’ in Proc. Conf. Empirical Methods Natural Lang. Process.,\n9th Int. Joint Conf. Natural Lang. Process. (EMNLP-IJCNLP) , 2019,\npp. 1–3, doi: 10.18653/v1/D19-1399.\n[25] M. de Lhoneux, M. Ballesteros, and J. Nivre, ‘‘Recursive subtree com-\nposition in LSTM-based dependency parsing,’’ 2019, arXiv:1902.09781.\n[Online]. Available: http://arxiv.org/abs/1902.09781\n[26] F. A. Gers and E. Schmidhuber, ‘‘LSTM recurrent networks learn simple\ncontext-free and context-sensitive languages,’’ IEEE Trans. Neural Netw. ,\nvol. 12, no. 6, pp. 1333–1340, Nov. 2001.\n[27] V . Sze, Y .-H. Chen, T.-J. Yang, and J. S. Emer, ‘‘Efﬁcient processing of\ndeep neural networks: A tutorial and survey,’’ Proc. IEEE, vol. 105, no. 12,\npp. 2295–2329, Dec. 2017.\n[28] R. Pascanu and Y . Bengio, ‘‘Revisiting natural gradient for deep networks,’’\nComput. Sci., vol. 37, nos. 10–11, pp. 1655–1658, 2013.\n[29] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ‘‘Learning repre-\nsentations by back-propagating errors,’’ Nature, vol. 323, pp. 533–536,\nOct. 1986.\n[30] F. M. Salem, ‘‘SLIM LSTMs,’’ 2018, arXiv:1812.11391. [Online]. Avail-\nable: http://arxiv.org/abs/1812.11391\n[31] D. Neil, M. Pfeiffer, and S.-C. Liu, ‘‘Phased LSTM: Accelerating recurrent\nnetwork training for long or event-based sequences,’’ in Proc. NIPS, 2016,\npp. 3882–3890. [Online]. Available: https://arxiv.org/abs/1610.09513\n[32] D. Ha, A. Dai, and Q. V . Le, ‘‘HyperNetworks,’’ 2016, arXiv:1609.09106.\n[Online]. Available: http://arxiv.org/abs/1609.09106\nQING WANG was born in Linyi, Shandong,\nChina, in 1998. She is currently pursuing the\ndegree with the School of Computer Science and\nTechnology, Shandong University of Technology.\nHer research interests include machine learning,\ndata mining, and natural language processing.\nRONG-QUN PENGwas born in Zibo, Shandong,\nChina, in 1971. He received the B.S. degree in\nradio techniques from Shandong University, Jinan,\nin 1993, the M.S. degree in electrical engineering\nand automation from China Agricultural Univer-\nsity, Beijing, in 2002, and the Ph.D. degree in\ninformation and communication engineering from\nthe Nanjing University of Posts and Telecommu-\nnications, Nanjing, in 2012. Since 1993, he has\nbeen with the School of Computer Science and\nTechnology, Shandong University of Technology, Zibo, as a Teacher and a\nResearch Staff. His research interests include future network architectures\nand key technologies, 5G, and the IoT.\nJIA-QIANG WANG received the M.S. degree\nfrom the Shandong University of Technology\n(SDUT), in 2013, and the Ph.D. degree from\nthe Hebei University of Technology (Hebut),\nin 2018. He is currently a Postdoctoral Scholar\nwith the Beijing Academy of Science and Tech-\nnology (BJAST) and the Director of the Research\nCenter for Big Data. His research interests include\nbiometrics, machine learning, and data mining.\nZHI LI received the B.S. degree from the\nSouthwest University of Political Science and Law\n(SWUPL), in 2001, and the M.S. degree from\nXinjiang University (XJU), in 2010. He is cur-\nrently pursuing the Ph.D. degree with the Univer-\nsity of Chinese Academy of Sciences (UCAS). His\nresearch interests include machine learning and\ndata mining.\nHAN-BING QUreceived the M.S. degree from the\nHarbin Institute of Technology (HIT), in 2003, and\nthe Ph.D. degree with the Institute of Automation,\nChinese Academy of Sciences (CASIA), in 2007.\nHe is currently a Professor with the Beijing\nAcademy of Science and Technology (BJAST).\nHe is also a member of the Intelligent Automation\nCommittee of the Chinese Association of Automa-\ntion (IACAA). His research interests include bio-\nmetrics, machine learning, pattern recognition,\nand computer vision.\nVOLUME 8, 2020 65401",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.740540087223053
    },
    {
      "name": "Term (time)",
      "score": 0.6779074668884277
    },
    {
      "name": "Sequence (biology)",
      "score": 0.6343953609466553
    },
    {
      "name": "Natural language processing",
      "score": 0.43436360359191895
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4170910716056824
    },
    {
      "name": "Speech recognition",
      "score": 0.3663107752799988
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I119203015",
      "name": "Shandong University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210162962",
      "name": "Beijing Academy of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ]
}