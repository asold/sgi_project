{
    "title": "Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model",
    "url": "https://openalex.org/W2353655624",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2753220617",
            "name": "Cho, Kyunghyun",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2176263492",
        "https://openalex.org/W3099884890",
        "https://openalex.org/W196214544",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W2504871398",
        "https://openalex.org/W2180816288",
        "https://openalex.org/W1574901103",
        "https://openalex.org/W2162980292",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W592244745",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2107878631",
        "https://openalex.org/W1846689784",
        "https://openalex.org/W1816313093",
        "https://openalex.org/W2222235228",
        "https://openalex.org/W2463033603",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W6908809",
        "https://openalex.org/W1498436455",
        "https://openalex.org/W2541989759",
        "https://openalex.org/W1985258458",
        "https://openalex.org/W1496559305",
        "https://openalex.org/W183625566",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W648786980",
        "https://openalex.org/W2963247703"
    ],
    "abstract": "Recent advances in conditional recurrent language modelling have mainly focused on network architectures (e.g., attention mechanism), learning algorithms (e.g., scheduled sampling and sequence-level training) and novel applications (e.g., image/video description generation, speech recognition, etc.) On the other hand, we notice that decoding algorithms/strategies have not been investigated as much, and it has become standard to use greedy or beam search. In this paper, we propose a novel decoding strategy motivated by an earlier observation that nonlinear hidden layers of a deep neural network stretch the data manifold. The proposed strategy is embarrassingly parallelizable without any communication overhead, while improving an existing decoding algorithm. We extensively evaluate it with attention-based neural machine translation on the task of En-&gt;Cz translation.",
    "full_text": "arXiv:1605.03835v1  [cs.CL]  12 May 2016\nNoisy Parallel Approximate Decoding\nfor Conditional Recurrent Language Model\nKyunghyun Cho\nNew York University\nkyunghyun.cho@nyu.edu\nAbstract\nRecent advances in conditional recurrent language modelling have mainly focused\non network architectures (e.g., attention mechanism), learning algorithms (e.g.,\nscheduled sampling and sequence-level training) and novelapplications (e.g., im-\nage/video description generation, speech recognition, etc.) On the other hand,\nwe notice that decoding algorithms/strategies have not been investigated as much,\nand it has become standard to use greedy or beam search. In this paper, we pro-\npose a novel decoding strategy motivated by an earlier observation that nonlinear\nhidden layers of a deep neural network stretch the data manifold. The proposed\nstrategy is embarrassingly parallelizable without any communication overhead,\nwhile improving an existing decoding algorithm. We extensively evaluate it with\nattention-based neural machine translation on the task of En→ Cz translation.\n1 Introduction\nSince its ﬁrst use as a language model in 2010 [19], a recurrent neural network has become ade\nfactochoice for implementing a language model [28, 25]. One of theappealing properties of this\napproach to language modelling, to which we refer asrecurrent language modelling, is that a re-\ncurrent language model can generate a long, coherent sentence [26]. This is due to the ability of a\nrecurrent neural network to capture long-term dependencies.\nThis property has come under spotlight in recent years as theconditional version of a recurrent lan-\nguage model began to be used in many different problems that require generating a natural language\ndescription of a high-dimensional, complex input. These tasks include machine translation, speech\nrecognition, image/video description generation and manymore [9] and references therein.\nMuch of the recent advances in conditional recurrent language model have focused either on network\narchitectures (e.g., [1]), learning algorithms (e.g., [4,22, 2]) or novel applications (see [9] and\nreferences therein). On the other hand, we notice that therehas not been much research on decoding\nalgorithms for conditional recurrent language models. In the most of work using recurrent language\nmodels, it is a common practice to use either greedy or beam search to ﬁnd the most likely natural\nlanguage description given an input.\nIn this paper, we investigate whether it is possible to decode better from a conditional recurrent lan-\nguage model. More speciﬁcally, we propose a decoding strategy motivated by earlier observations\nthat nonlinear hidden layers of a deep neural network stretch the data manifold such that a neigh-\nbourhood in the hidden state space corresponds to a set of semantically similar conﬁgurations in\nthe input space [6]. This observation is exploited in the proposed strategy by injecting noise in the\nhidden transition function of a recurrent language model.\nThe proposed strategy, called noisy parallel approximate decoding (NPAD), is a meta-algorithm that\nruns in parallel many chains of the noisy version of an inner decoding algorithm, such as greedy or\nbeam search. Once those parallel chains generate the candidates, the NPAD selects the one with the\n1\nhighest score. As there is effectively no communication overhead during decoding, the wall-clock\nperformance of the proposed NPAD is comparable to a single run of an inner decoding algorithm\nin a distributed setting, while it improves the performanceof the inner decoding algorithm. We\nempirically evaluate the proposed NPAD against the greedy search, beam search as well as stochastic\nsampling and diverse decoding [16] in attention-based neural machine translation.\n2 Conditional Recurrent Language Model\nA language model aims at modelling a probabilistic distribution over natural language text. A recur-\nrent language model is a language model implemented as a recurrent neural network [18].\nLet us deﬁne a probability of a given natural language sentence,1 which we represent as a sequence\nof linguistic symbolsX = (x1, x2, . . . , x T ), as\np(X) =p(x1, x2, . . . , x T ) =p(x1)p(x2|x1)p(x3|x1, x2) · · ·p(xT |x<T ) =\nT∏\nt=1\np(xt|x<t), (1)\nwhere x<t is all the symbols preceding thet-th symbol in the sentenceX. Note that this condi-\ntional dependency structure is not necessary but is preferred over other possible structures due to its\nnaturalness as well as the fact that the length of a given sentenceT is often unknown in advance.\nIn a neural language model [5], a neural network is used to compute each of the conditional prob-\nability terms in Eq. (1). A difﬁculty in doing so is that the input(x1, x2, . . . , x t− 1) to the neural\nnetwork is of variable size. A recurrent neural network cleverly addresses this difﬁculty by reading\none symbol at a time while maintaining an internal memory state:\nht = φ (ht− 1, E [xt]) , (2)\nwhere ht is the internal memory state at timet.E [xt] is a vector representation of thet-th symbol\nin the input sentence. The internal memory stateht effectively summarizes all the symbols read up\nto thet-th time step.\nThe recurrent activation functionφ in Eq. (2) can be as simple as an afﬁne transformation followed\nby a point-wise nonlinearity (e.g.,tanh) to as complicated a function as long short-term memory\n(LSTM, [13]) or gated recurrent units (GRU, [10]). The latter two are often preferred, as they\neffectively avoid the issue of vanishing gradient [7].\nGiven the internal hidden state, the recurrent neural network computes the conditional distribution\nover the next symbolxt+1. Assuming a ﬁxed vocabularyV of linguistic symbols, it is straightfor-\nward to make a parametric function that returns a probability of each symbol in the vocabulary:\np(xt+1 = j|x≤ t) = exp(gj (ht))\n∑ |V |\nj′ =1 exp(gj′ (ht))\n, (3)\nwhere gj(ht) is thej-th component of the output of the functiong : Rdim(ht) → R|V |. The\nformulation on the right-hand side of Eq. (3) is called a softmax function [8].\nGiven Eqs. (2)–(3), the recurrent neural network reads one symbol of a given sentenceX at a\ntime from left to right and computes the conditional probability of each symbol until the end of\nthe sequence is reached. The probability of the sentence is then given by a product of all those\nconditional probabilities. We call this recurrent neural network arecurrent language model.\nConditional Recurrent Language ModelA recurrent language model is turned into aconditional\nrecurrent language model, when the distribution over sentences is conditioned on another modality\nincluding another language. In other words, a conditional recurrent language model estimates\np(X|Y ) =\nT∏\nt=1\np(xt|x<t, Y ). (4)\n1 Although I use a “sentence” here, this is absolutely not necessary, and any level of text, such as a phrase,\nparagraph, chapter and document, can be used as a unit of language modelling. Furthermore, it does not have\nto be a natural language text but any sequence such as speech,video or actions.\n2\nY in Eq. (4) can be anything from a sentence in another language(machine translation), an image\n(image caption generation), a video clip (video description generation) to speech (speech recogni-\ntion). In any of those cases, a previously described recurrent language model requires only a slightest\ntweak in order to take into accountY .\nThe tweak is to compute the internal hidden state of the recurrent language model based not only on\nht− 1 and E [xt] (see Eq. (2)) but also onY such that\nht = φ (ht− 1, E [xt] , f (Y, t)) , (5)\nwhere f is a time-dependent function that maps fromY to a vector. Furthermore, we can makegj\nin Eq. (3) to be conditioned onY as well\np(xt+1 = j|x≤ t) = exp(gj (ht, f (Y, t)))\n∑ |V |\nj′=1 exp(gj′ (ht, f (Y, t)))\n. (6)\nLearning Given a data setD of pairs(X, Y ), the conditional recurrent language model is trained\nto maximize the log-likelihood function which is deﬁned as\nL(θ) = 1\n|D|\nN∑\nn=1\nT n\n∑\nt=1\nlog p(xn\nt |xn\n<t, Y n).\nThis maximization is often done by stochastic gradient descent with the gradient computed by\nbackpropagation [23]. Instead of a scalar learning rate, adaptive learning rate methods, such as\nAdadelta [27] and Adam [14], are often used.\n3 Decoding\nDecoding in a conditional recurrent language model corresponds to ﬁnding a target sequence˜X that\nmaximizes the conditional probabilityp(X|Y ) from Eq. (4):\n˜X = arg max\nX\nlog p(X|Y ).\nAs is clear from the formulation in Eqs. (5)–(6), exact decoding is intractable, as the state space of\nX grows exponentially with respect to the length of the sequence, i.e.,|X |= O(|V ||X|), without\nany trivial structure that can be exploited. Thus, we must resort to approximate decoding.\n3.1 Greedy Decoding\nGreedy decoding is perhaps the most naive way to approximately decode from the conditional re-\ncurrent language model. At each time step, it greedily selects the most likely symbol under the\nconditional probability:\n˜xt = arg max\nj\nlog p(xt = j|˜x<t). (7)\nThis continues until a special marker indicating the end of the sequence is selected.\nThis greedy approach is computationally efﬁcient, but is likely too crude. Any early choice based\non a high conditional probability can easily turn out to be unlikely one due to low conditional\nprobabilities later on. This issue is closely related to thegarden path sentence problem (see Sec. 3.2.4\nof [17].)\n3.2 Beam Search\nBeam search improves upon the greedy decoding strategy by maintainingK hypotheses at each time\nstep, instead of a single one. Let\nHt− 1 =\n{\n(˜x1\n1, ˜x1\n2, . . . , ˜x1\nt− 1), (˜x2\n1, ˜x2\n2, . . . , ˜x2\nt− 1), . . . , (˜xK\n1 , ˜xK\n2 , . . . , ˜xK\nt− 1)\n}\n3\nbe a set of current hypotheses at timet. Then, from each current hypothesis the following|V |\ncandidate hypotheses are generated:\nHk\nt =\n{\n(˜xk\n1 , ˜xk\n2 , . . . , ˜xk\nt− 1, v1), (˜xk\n1 , ˜xk\n2 , . . . , ˜xk\nt− 1, v2), . . . , (˜xk\n1 , ˜xk\n2 , . . . , ˜xk\nt− 1, v|V |)\n}\n,\nwhere vj denotes thej-th symbols in the vocabularyV .\nThe top-K hypotheses from the union of all such hypotheses setsHk\nt , k = 1, . . . , K are selected\nbased on their scores. In other words,\nHt = ∪ K\nk=1Bk,\nwhere\nBk = arg max\n˜X∈A k\nlog p( ˜X|Y ), Ak = Ak− 1 − B k− 1, and A1 = ∪ K\nk′=1Hk′\nt .\nAmong the top-K hypotheses, we consider the ones whose last symbols are the special marker for\nthe end of sequence to be complete and stop expanding such hypotheses. All the other hypotheses\ncontinue to be expanded, however, withK reduced by the number of complete hypotheses. When\nK reaches0, the beam search ends, and the best one among all the completehypotheses is returned.\n4 NPAD: Noisy Parallel Approximate Decoding\nIn this section, we introduce a strategy that can be used in conjunction with the two decoding strate-\ngies discussed earlier. This new strategy is motivated by the fact that a deep neural network, in-\ncluding a recurrent neural network, learns to stretch the input manifold (on which only likely input\nexamples lie) and ﬁll the hidden state space with it. This implies that a neighbourhood in the hidden\nstate space corresponds to a set of semantically similar conﬁgurations in the input space, regardless\nof whether those conﬁgurations are close to each other in theinput space [6]. In other words, small\nperturbation in the hidden space corresponds to jumping from one plausible conﬁguration to another.\nIn the case of conditional recurrent language model, we can achieve this behaviour of efﬁciently\nexploration across multiple modes by injecting noise to thetransition function of the recurrent neural\nnetwork. In other words, we replace Eq. (5) with\nht = φ (ht− 1 + ǫt, E [xt] , f (Y, t)) , (8)\nwhere\nǫt ∼ N (0, σ2\nt I).\nThe time-dependent standard deviationσt should be selected to reﬂect the uncertainty dynamics in\nthe conditional recurrent language model. As the recurrentnetwork models a target sequence in one\ndirection, uncertainty is often greatest when predicting earlier symbols and gradually decreases as\nmore and more context becomes available for the conditionaldistributionp(yt|y<t). This naturally\nsuggests a strategy where we start with a high level of noise (highσt) and anneal it (σt → 0) as the\ndecoding progresses. One such scheduling scheme is\nσt = σ0\nt ,\nwhere σ0 is an initial noise level. Although there are many alternatives, we ﬁnd this simple formu-\nlation to be effective in experiments later.\nWe run M such noisy decoding processes in parallel. This can be done easily and efﬁciently, as\nthere is no communication between these parallel processesexcept at the end of the decoding pro-\ncessing. Let us denote by˜Ym a sequence decoded from them-th decoding process. Among these\nM hypotheses, we select the one with the highest probability assigned by thenon-noisymodel:\n˜Y = arg max\n˜Ym:m=1,...,M\nlog p( ˜Ym|X).\nWe call this decoding strategy, based on running multiple parallel approximate decoding processes\nwith noise injected,noisy parallel approximate decoding(NPAD).\n4\nComputational Complexity Clearly, the proposed decoding strategy isM times more expensive,\ni.e.,O(MD ), whereD is the computational complexity of either greedy or beam search (see Sec. 3.)\nIt is however important to note that the proposed NPAD is embarrassingly parallelizable, which is\nwell suited for distributed and parallel environments of modern computing. By utilizing multi-\ncore machines, the practical cost of computation reduces tosimply running the greedy or beam\nsearch once (with a constant multiplicative factor of2 ± ǫ due to computing the non-noisy score and\ngenerating pseudo random numbers.) This is contrary to, forinstance, when comparing the beam\nsearch to the greedy search, in which case the beneﬁt from parallelization is limited due to the heavy\ncommunication cost at each step.\nQuality Guarantee A major issue with the proposed strategy is that the resulting sequence may be\nworse than running a single inner-decoder, due to the stochasticity. This is however easily avoided\nby settingσ0 to0 for one of theM decoding processes. By doing so, even if all the other noisy\ndecoding processes resulted in sequences whose probabilities are worse than the non-noisy process,\nthe proposed strategy nevertheless returns a sequence thatis as good as a single run of the inner\ndecoding algorithm.\n4.1 Why not Sampling?\nThe formulation of the conditional recurrent language model in Eq. (4) implies that we can generate\nexact samples from the model, as this is a directed acyclic graphical model. At each time stept, a\nsample from the categorical distribution given all the samples of the previous time steps (Eq. (6)) is\ngenerated. This procedure is done iteratively either up toT time steps or another type of stopping\ncriterion is met (e.g., the end-of-sequence symbol is sampled.) Similarly to the proposed NPAD, we\ncan run a set of this sampling procedures in parallel.\nA major difference between this sampling-at-the-output and the proposed NPAD is that the NPAD\nexploits the hidden state space of a neural network in which the data manifold is highlylinearized.\nIn other words, training a neural network tends toﬁll upthe hidden state space as much as possible\nwith valid data points,2 and consequently any point in the neighbourhood of a valid hidden state (ht\nEq. (5)) should map to a plausible point in the output space. This is contrary to the actual output\nspace, where only a fraction of the output space is plausible.\nLater, we show empirically that it is indeed more efﬁcient tosample in the hidden state space than\nin the output state space.\n4.2 Related Work\nPerturb-and-MAP Perturb-and-MAP [21] is an algorithm that reduces probabilistic inference,\nsuch as sampling, to energy minimization in a Markov random ﬁeld (MRF) [20]. For instance,\ninstead of Gibbs sampling, one can use the perturb-and-MAP algorithm to ﬁnd multiple instances of\nconﬁgurations that minimize theperturbedenergy function. Each instance of the perturb-and-MAP\nworks by ﬁrst injecting noise to the energy function of the MRF, i.e.,˜E(x) =E(x)+ ǫ(x), followed\nby maximum-a-posterior (MAP) step, i.e.,arg minx ˜E(x).\nA connection between this perturb-and-MAP and the proposedNPAD is clear. Let us deﬁne the\nenergy function of the conditional recurrent language model as its log-probability, i.e.,E(X|Y ) =\nlog p(X|Y ) (see Eq. (4).) Then, the noise injection to the hidden state in Eq. (8) is a process similar\nto injecting noise to the energy function. This connection arises from the fact that the NPAD and\nperturb-and-MAP share the same goal of “[giving] other low energy states the chance” [20].\nDiverse Decoding One can view the proposed NPAD as a way to generate a diverse set of likely\nsolutions from a conditional recurrent language model. In [16], a variant of beam search was pro-\nposed, which modiﬁes the scoring function at each time step of beam search to promote diverse\ndecoding. This is done by penalizing low ranked hypotheses that share a previous hypothesis. This\napproach is however only applicable to beam search and is notas parallelizable as the proposed\nNPAD. It should be noted that the NPAD and the diverse decoding can be used together.\n2 This behaviour can be further encouraged by regularizing the (approximate) posterior over the hidden\nstate, for instance, as in variational autoencoders (see, e.g., [15, 11].)\n5\nEarlier, Batra et al. [3] proposed another approach that enables decoding multiple, diverse solutions\nfrom an MRF. This method decodes one solution at a time, whileregularizing the energy function\nof an MRF with the diversity measure between the solution currently being decoded and all the\nprevious solutions. Unlike the perturb-and-MAP or the NPAD, this is a deterministic algorithm.\nA major downside to this approach is that it is inherently sequential. This makes it impractical\nespecially for neural machine translation, as already the major issue behind its deployment is the\ncomputational bottleneck in decoding.\n5 Experiments: Attention-based Neural Machine Translation\n5.1 Settings\nIn this paper, we evaluate the proposed noisy parallel approximate decoding (NPAD) strategy in\nattention-based neural machine translation. More speciﬁcally, we train an attention-based encoder-\ndecoder network on the task of English-to-Czech translation and evaluate different decoding strate-\ngies.\nThe encoder is a single-layer bidirectional recurrent neural network with 1028 gated recurrent units\n(GRU, [10]).3 The decoder consists of an attention mechanism [1] and a recurrent neural network\nagain with 1028 GRU’s. Both source and target words were projected to a 512-dimensional contin-\nuous space. We used the code from dl4mt-tutorial available online4 for training. Both source and\ntarget sentences were represented as sequences of BPE subword symbols [24].\nWe trained this model on a large parallel corpus of approximately 12m sentence pairs, available\nfrom WMT’15, 5 for 2.5 weeks. During training, ADADELTA [27] was used to adaptively adjust\nthe learning rate of each parameter, and the norm of the gradient was renormalized to1, if it exceed\n1. The training run was early-stopped based on the validationperplexity using newstest-2013 from\nWMT’15. The model is tested with two held-out sets, newstest-2014 and newstest-2015.6\nWe closely followed the training and test strategies from [12], and more details can be found in it.\nEvaluation Metric The main evaluation metric is the negative conditional log-probability of a\ndecoded sentence, where lower is better. Additionally, we use BLEU as a secondary evaluation\nmetric. BLEU is a de-facto standard metric for automatically measuring the translation quality of\nmachine translation systems, in which case higher is better.\n5.2 Decoding Strategies\nWe evaluate four decoding strategies. We choose the strategies that have comparable computational\ncomplexity per core/machine, assuming multiple cores/machines are available. This selection left\nus with greedy search, beam search, stochastic sampling, diverse decoding and the proposed NPAD.\nGreedy and Beam Search Both greedy and beam search are the most widely used decoding\nstrategies in neural machine translation, as well as other conditional recurrent language models for\nother tasks. In the case of beam search, we test with two beamwidths, 5 and 10. We use the script\nmade available at dl4mt-tutorial.\nStochastic Sampling A naive baseline for injecting noise during decoding is to simply sample\nfrom the output distribution at each time step, instead of taking the top-K entries. We test three\nconﬁgurations, where 5, 10 or 50 such samplers are run in parallel.\nNoisy Parallel Approximate Decoding (NPAD)We extensively evaluate the NPAD by varying\nthe number of parallel decoding (5, 10 or 50), the beamwidth (1, 5 or 10) and the initial noise level\nσ0 (0.1,0.2,0.3 or0.5).\n3 The number 1028 resulted from a typo, when originally we intended to use 1024.\n4 https://github.com/nyu-dl/dl4mt-tutorial/tree/master/session2\n5 http://www.statmt.org/wmt15/translation-task.html\n6 Due to the space constraint, we only report the result on newstest-2014. We however observed the same\ntrend from newstest-2014 on newstest-2015.\n6\nDiverse Decoding We try the diverse decoding strategy from [16]. There is one hyperparameterη,\nand we search over{0.001, 0.01, 0.1, 1}, as suggested by the authors of [16] based on the validation\nset performance.7 Also, we vary the beam width (5 or 10). This is included as a deterministic\ncounter-part to the NPAD.\nValid Test-1\nStrategy σ0 NLL ↓ BLEU ↑ NLL ↓ BLEU ↑\nGreedy - 27.879 15.5 26.4928 16.66\nSto. Sampling - 22.9818 15.64 26.2536 16.76\nNPAD 0.1 21.125 16.06 23.8542 17.48\nNPAD 0.2 20.6353 16.37 23.2631 17.86\nNPAD 0.3 20.4463 16.71 23.0111 18.03\nNPAD 0.5 20.7648 16.48 23.3056 18.13\nTable 1: Effect of noise injec-\ntion. For both stochastic sampling\nand NPAD, we used 50 parallel\nsamplers. For NPAD, we used\nthe greedy decoding as an inner-\ndecoding strategy.\n5.3 Results and Analysis\nEffect of Noise InjectionFirst, we analyze the effect of noise injection by comparingthe stochas-\ntic sampling and the proposed NPAD against the deterministic greedy decoding. In doing so, we\nused 50 parallel decoding processes for both stochastic sampling and NPAD. We varied the amount\nof initial noiseσ0 as well.\nIn Table 1, we present both the average negative log-probability and BLEU for all the cases. As ex-\npected, the proposed NPAD improves upon the deterministic greedy decoding as well as the stochas-\ntic sampling strategy. It is important to notice that the improvement by the NPAD is signiﬁcantly\nlarger than that by the stochastic sampling, which conﬁrms that it is more efﬁcient and effective to\ninject noise in the hidden state of the neural network.\nValid Test-1\nStrategy # Parallels NLL ↓ BLEU ↑ NLL ↓ BLEU ↑\nGreedy 1 27.879 15.5 26.4928 16.66\nNPAD 5 21.5984 16.09 24.3863 17.51\nNPAD 10 21.054 16.33 23.6942 17.81\nNPAD 50 20.4463 16.71 23.0111 18.03\nTable 2: Effect of the number of\nparallel decoding processes. For\nNPAD, σ0 = 0.3.\nEffect of the Number of Parallel ChainsNext, we see the effect of having more parallel decoding\nprocesses of the proposed NPAD. As we show in Table 2, the translation quality, in both the average\nnegative log-probability and BLEU, improves as more parallel decoding processes are used, while it\ndoes signiﬁcantly better than greedy strategy even with ﬁvechains. We observed this trend for all the\nother noise levels. This is an important observation, as it implies that the performance of decoding\ncan easily be improved without sacriﬁcing the delay betweenreceiving the input and returning the\nresult by simply adding in more cores/machines.\nBeam #\nValid Test-1\nStrategy Width σ0 Chains NLL ↓ BLEU ↑ NLL ↓ BLEU ↑\nGreedy 1 - 1 27.879 15.5 26.4928 16.66\nNPAD+G 1 0.3 50 20.4463 16.71 23.0111 18.03\nBeam 5 - 1 20.1842 17.03 22.8106 18.56\nNPAD+B 5 0.3 5 19.8106 17.19 22.1374 18.64\nNPAD+B 5 0.1 10 19.7771 17.16 22.1594 18.61\nBeam 10 - 1 19.9173 17.13 22.4392 18.59\nNPAD+B 10 0.2 5 19.7888 17.16 22.1178 18.68\nNPAD+B 10 0.1 10 19.6674 17.14 21.9786 18.78\nTable 3: NPAD\nwith beam search\n(NPAD+B).\nWe report the\nNPAD+B’s with\nthe best average\nlog-probability on\nthe validation set.\nNPAD with Beam Search As described earlier, NPAD can be used with any other deterministic\ndecoding strategy. Hence, we test it together with the beam search strategy. As in Table 3, we\nobserve again that the proposed NPAD improves the deterministic strategy. However, as the beam\n7 Personal communication.\n7\nsearch is already able to ﬁnd a good solution, the improvement is much smaller than that against the\ngreedy strategy.\nIn Table 3, we observe that difference between the greedy andbeam search strategies is much smaller\nwhen the NPAD is used as an outer loop. For instance, comparing the greedy decoding and beam\nsearch with with 10, the differences without and with NPAD are 7.9617 vs. 0.7789 (NLL) and 1.66\nvs. 0.43 (BLEU). This again conﬁrms that the proposed NPAD has a potential to make the neural\nmachine translation more suitable for deploying in the realworld.\nBeam #\nValid Test-1\nStrategy Width ⋆ Chains NLL ↓ BLEU ↑ NLL ↓ BLEU ↑\nBeam 5 - 1 20.1842 17.03 22.8106 18.56\nNPAD+B 5 0.3 5 19.8106 17.19 22.1374 18.64\nDiverse 5 0.001 1 20.1859 17.03 22.8156 18.56\nBeam 10 - 1 19.9173 17.13 22.4392 18.59\nNPAD+B 10 0.2 5 19.7888 17.16 22.1178 18.68\nDiverse 10 0.1 1 19.8908 17.2 22.4451 18.62\nTable 4: NPAD\nvs. diverse decod-\ning. The hyperpa-\nrameterη0 was se-\nlected based on the\nBLEU on the vali-\ndation set. (⋆) σ0\nif NPAD, andη if\nDiverse.\nNPAD vs Diverse Decoding In Table 4, we present the result using the diverse decoding.The\ndiverse decoding was proposed in [16] as a way to improve the translation quality, and accordingly,\nwe present the best approaches based on the validation BLEU.Unlike what was reported in [16], we\nwere not able to see any substantial improvement by the diverse decoding. This may be due to the\nfact that Li & Jurafsky [16] used additional translation/language models to re-rank the hypotheses\ncollected by diverse decoding. As those additional models are trained and selected for a speciﬁc\napplication of machine translation, we ﬁnd the proposed NPAD to be more generally applicable\nthan the diverse decoding is. It is however worthwhile to note that the diverse decoding may also\nbeneﬁt from having the NPAD as an outer loop.\n6 Conclusion and Future Work\nIn this paper, we have proposed a novel decoding strategy forconditional recurrent language mod-\nels. The proposed strategy, called noisy, parallel approximate decoding (NPAD), exploits the hidden\nstate space of a recurrent language model by injecting unstructured Gaussian noise at each transi-\ntion. Multiple chains of this noisy decoding process are runin parallel without any communication\noverhead, which makes the NPAD appealing in practice.\nWe empirically evaluated the proposed NPAD against the widely used greedy and beam search as\nwell as stochastic sampling and diverse decoding strategies. The empirical evaluation has conﬁrmed\nthat the NPAD indeed improves decoding, and this improvement is especially apparent when the\ninner decoding strategy, which can be any of the existing strategies, is more approximate. Using\nNPAD as an outer loop signiﬁcantly closed the gap between fast, but more approximate greedy\nsearch and slow, but more accurate beam search, increasing the potential for deploying conditional\nrecurrent language models, such as neural machine translation, in practice.\nFuture Work We consider this work as a ﬁrst step toward developing a better decoding strategy for\nrecurrent language models. The success of this simple NPAD suggests a number of future research\ndirections. First, thorough investigation into injectingnoise during training should be done, not\nonly in terms of learning and optimization (see, e.g., [4]),but also in the context of its inﬂuence\non decoding. It is conceivable that there exists a noise injection mechanism during training that\nmay ﬁt better with the noise injection process during decoding (as in the NPAD.) Second, we must\nstudy the relationship between different types and scheduling of noise in the NPAD in addition to\nwhite Gaussian noise with annealed variance investigated in this paper. Lastly, the NPAD should be\nvalidated on the tasks other than neural machine translation, such as image/video caption generation\nand speech recognition (see, e.g., [9] and references therein.)\nAcknowledgments\nKC thanks the support by Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center\nof Excellence 2015-2016).\n8\nReferences\n[1] D. Bahdanau, K. Cho, and Y . Bengio. Neural machine translation by jointly learning to align and translate.\nInICLR 2015, 2015.\n[2] D. Bahdanau, D. Serdyuk, P. Brakel, N. R. Ke, J. Chorowski, A. Courville, and Y . Bengio. Task loss\nestimation for sequence prediction.arXiv preprint arXiv:1511.06456, 2015.\n[3] D. Batra, P. Yadollahpour, A. Guzman-Rivera, and G. Shakhnarovich. Diverse m-best solutions in Markov\nrandom ﬁelds. InComputer Vision–ECCV 2012, pages 1–16. Springer, 2012.\n[4] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction with\nrecurrent neural networks. InNIPS, pages 1171–1179, 2015.\n[5] Y . Bengio, R. Ducharme, and P. Vincent. A neural probabilistic language model.Journal of Machine\nLearning Research, 3:1137–1155, 2003.\n[6] Y . Bengio, G. Mesnil, Y . Dauphin, and S. Rifai. Better mixing via deep representations. InProceedings\nof The 30th International Conference on Machine Learning, pages 552–560, 2013.\n[7] Y . Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difﬁcult.\nNeural Networks, IEEE Transactions on, 5(2):157–166, 1994.\n[8] J. S. Bridle. Probabilistic interpretation of feedforward classiﬁcation network outputs, with relationships\nto statistical pattern recognition. InNeurocomputing, pages 227–236. Springer, 1990.\n[9] K. Cho, A. Courville, and Y . Bengio. Describing multimedia content using attention-based encoder-\ndecoder networks.Multimedia, IEEE Transactions on, 17(11):1875–1886, 2015.\n[10] K. Cho, B. Van Merri¨ enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y . Ben-\ngio. Learning phrase representations using RNN encoder-decoder for statistical machine translation.\narXiv:1406.1078, 2014.\n[11] J. Chung, K. Kastner, L. Dinh, K. Goel, A. Courville, andY . Bengio. A recurrent latent variable model\nfor sequential data. InAdvances in Neural Information Processing Systems (NIPS), 2015.\n[12] O. Firat, K. Cho, and Y . Bengio. Multi-way, multilingual neural machine translation with a shared atten-\ntion mechanism. InNAACL , 2016.\n[13] S. Hochreiter and J. Schmidhuber. Long short-term memory.Neural computation, 9(8):1735–1780, 1997.\n[14] D. Kingma and J. Ba. Adam: A method for stochastic optimization.The International Conference on\nLearning Representations (ICLR), 2015.\n[15] D. P. Kingma and M. Welling. Auto-encoding variationalbayes. InProceedings of the 2nd International\nConference on Learning Representations (ICLR), number 2014, 2013.\n[16] J. Li and D. Jurafsky. Mutual information and diverse decoding improve neural machine translation.arXiv\npreprint arXiv:1601.00372, 2016.\n[17] C. D. Manning and H. Sch¨ utze.Foundations of statistical natural language processing, volume 999. MIT\nPress, 1999.\n[18] T. Mikolov.Statistical Language Models based on Neural Networks. PhD thesis, Brno University of\nTechnology, 2012.\n[19] T. Mikolov, M. Karaﬁ´ at, L. Burget, J. Cernock` y, and S.Khudanpur. Recurrent neural network based\nlanguage model.INTERSPEECH , 2:3, 2010.\n[20] G. Papandreou and A. Yuille. Perturb-and-MAP random ﬁelds: Reducing random sampling to optimiza-\ntion, with applications in computer vision.Advanced Structured Prediction, page 159, 2014.\n[21] G. Papandreou and A. L. Yuille. Perturb-and-MAP randomﬁelds: Using discrete optimization to learn\nand sample from energy models. InComputer Vision (ICCV), 2011 IEEE International Conference on,\npages 193–200. IEEE, 2011.\n[22] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequencelevel training with recurrent neural networks.\narXiv preprint arXiv:1511.06732, 2015.\n[23] D. Rumelhart, G. Hinton, and R. Williams. Learning representations by back-propagating errors.Nature,\npages 323–533, 1986.\n[24] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units.\narXiv preprint arXiv:1508.07909, 2015.\n[25] M. Sundermeyer, H. Ney, and R. Schl¨ uter. From feedforward to recurrent LSTM neural networks for\nlanguage modeling.TASLP , 23(3):517–529, 2015.\n[26] I. Sutskever, J. Martens, and G. E. Hinton. Generating text with recurrent neural networks. InProceedings\nof the 28th International Conference on Machine Learning (ICML-11) , pages 1017–1024, 2011.\n9\n[27] M. D. Zeiler. Adadelta: an adaptive learning rate method.arXiv preprint arXiv:1212.5701, 2012.\n[28] B. Zoph, A. Vaswani, J. May, and K. Knight. Simple, fast noise-contrastive estimation for large RNN\nvocabularies. InNAACL , 2016.\n10"
}