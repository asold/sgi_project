{
  "title": "Stakeholder-centric participation in large language models enhanced health systems",
  "url": "https://openalex.org/W4411400542",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2098504636",
      "name": "Zhiyuan Wang",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2996274084",
      "name": "Runze Yan",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2951115667",
      "name": "Sherilyn Francis",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2132268632",
      "name": "Carmen Díaz",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A3165598327",
      "name": "Tabor Flickinger",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2100602091",
      "name": "Yu-Fen Lin",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2096945033",
      "name": "Xiao Hu",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2157104944",
      "name": "Laura E Barnes",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2281154112",
      "name": "Virginia LeBaron",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2098504636",
      "name": "Zhiyuan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2996274084",
      "name": "Runze Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2951115667",
      "name": "Sherilyn Francis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132268632",
      "name": "Carmen Díaz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3165598327",
      "name": "Tabor Flickinger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100602091",
      "name": "Yu-Fen Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096945033",
      "name": "Xiao Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2157104944",
      "name": "Laura E Barnes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2281154112",
      "name": "Virginia LeBaron",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W4395453679",
    "https://openalex.org/W4387500346",
    "https://openalex.org/W4386537453",
    "https://openalex.org/W4404782931",
    "https://openalex.org/W4313060745",
    "https://openalex.org/W4403420208",
    "https://openalex.org/W2134823012",
    "https://openalex.org/W2049504028",
    "https://openalex.org/W2105982574",
    "https://openalex.org/W4401888862",
    "https://openalex.org/W2140960626",
    "https://openalex.org/W2000049702",
    "https://openalex.org/W2339955350",
    "https://openalex.org/W4367182096",
    "https://openalex.org/W4377197101",
    "https://openalex.org/W4399528455",
    "https://openalex.org/W4392691776",
    "https://openalex.org/W4402492681",
    "https://openalex.org/W4402369854",
    "https://openalex.org/W4383301640",
    "https://openalex.org/W4406307474",
    "https://openalex.org/W4292202284",
    "https://openalex.org/W2257529644",
    "https://openalex.org/W4399254290",
    "https://openalex.org/W2904237019",
    "https://openalex.org/W4386302153",
    "https://openalex.org/W2985011762",
    "https://openalex.org/W4399913244",
    "https://openalex.org/W4396912832",
    "https://openalex.org/W4393904462",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W4394890393",
    "https://openalex.org/W4402909345",
    "https://openalex.org/W6628082049",
    "https://openalex.org/W4406739193",
    "https://openalex.org/W4394063739",
    "https://openalex.org/W1989529511",
    "https://openalex.org/W4404782573",
    "https://openalex.org/W4401164434",
    "https://openalex.org/W4392366638",
    "https://openalex.org/W4398230086",
    "https://openalex.org/W3105800762",
    "https://openalex.org/W4404172855",
    "https://openalex.org/W2158711339",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W1998264665",
    "https://openalex.org/W4400611458",
    "https://openalex.org/W4220671609",
    "https://openalex.org/W4383959059",
    "https://openalex.org/W3003214531",
    "https://openalex.org/W4378471472",
    "https://openalex.org/W4400954145"
  ],
  "abstract": "Abstract Large language models (LLMs) are transforming healthcare by advancing clinical decision support, patient care, and administrative efficiency. However, effectively and sustainably integrating LLMs into healthcare systems requires addressing participatory gaps that may hinder alignment with stakeholders’ practical and ethical needs. This paper explores how participatory methods can be applied throughout the development lifecycle of LLM-enhanced health systems (LLMHS), arguing that: (1) participatory approaches are critical for engaging stakeholders in LLMHS development, and (2) LLM techniques can create novel participatory opportunities that reinforce stakeholder engagement while driving technical innovation in LLMHS. This dual perspective highlights the potential of LLMHS to align technical sophistication with real-world healthcare demands, paving the way for next-generation health systems.",
  "full_text": "npj |health systems Perspective\nhttps://doi.org/10.1038/s44401-025-00024-5\nStakeholder-centric participation in large\nlanguage models enhanced health\nsystems\nCheck for updates\nZhiyuan Wang1 ,R u n z eY a n2,S h e r i l y nF r a n c i s3, Carmen Diaz4, Tabor Flickinger5, Yufen Lin2,X i a oH u2,\nLaura E. Barnes1 &V i r g i n i aL e B a r o n4\nLarge language models (LLMs) are transforming healthcare by advancing clinical decision support,\npatient care, and administrative efﬁciency. However, effectively and sustainably integrating LLMs into\nhealthcare systems requires addressing participatory gaps that may hinder alignment with\nstakeholders’ practical and ethical needs. This paper explores how participatory methods can be\napplied throughout the development lifecycle of LLM-enhanced health systems (LLMHS), arguing\nthat: (1) participatory approaches are critical for engaging stakeholders in LLMHS development, and\n(2) LLM techniques can create novel participatory opportunities that reinforce stakeholder\nengagement while driving technical innovation in LLMHS. This dual perspective highlights the\npotential of LLMHS to align technical sophistication with real-world healthcare demands, paving the\nway for next-generation health systems.\nThe emergence of Large Language Models (LLMs) has opened transfor-\nmative possibilities in health systems,showcasing capabilities such as gen-\nerating human-like text, reasoning, and encoding complex medical domain\nknowledge\n1. Numerous LLMs, both open-source and proprietary, have been\ndeveloped globally, with several tailored speciﬁcally for healthcare appli-\ncations, including Med-PaLM, GatorTron, and ClinicalBERT-which sup-\nport tasks from medical documentation processing2 to clinical decision-\nmaking3 and patient education4. In 2023, only 6% of U.S. healthcare systems\nreported deploying generative AI; however, thisﬁgure is projected to grow\nto 15% by 20245.\nDespite these promising capabilities, signiﬁcant barriers remain in\nintegrating LLMs into healthcare. Many challenges stem from inadequate\nstakeholder engagement, leading to systems that are not fully aligned with\nreal-world healthcare needs. This misalignment not only undermines trust\nin system safety and reliability but also results in outputs that are poorly\ncontextualized and ethically or socially problematic\n6,7.l F o re x a m p l e ,s t a -\nkeholder engagement remains unbalanced: while physicians account for\n70% of perspectives, essential insights from patients, caregivers, developers,\nand administrators/policymakers contributed only 11.4%, 7.7%, 7.5%, and\n3.4% respectively, across 1721 excerpts from surveyed literature that illus-\ntrate a stakeholder’s perspective on clinical AI\n8. Other research indicates that\nonly 5% of healthcare LLM studies (Jan 2022– Feb 2024) used real patient\ncare data9, and evaluations have focused predominantly on accuracy rather\nthan on fairness, bias, and toxicity, even though these additional metrics are\nalso important to stakeholders.\nHealthcare represents a highly intricate ecosystem, encompassing a\ndiverse array of stakeholders, including patients, clinicians, caregivers,\nadministrators, researchers, and policymakers, each with distinct roles and\nneeds. lTo harness the full potentialof LLMs in a healthcare context, it is\nessential to develop solutions that address the speciﬁc requirements and\npriorities of these stakeholders. Inadequate stakeholder engagement, par-\nticularly for health and social care, can misalign research priorities, erode\ncommunity trust, and undermine research relevance, while limited parti-\ncipation from individuals and co mmunities may exacerbate health\ndisparities\n10– 12. Despite some efforts to involve stakeholders, many current\ndevelopment and deployment practices still fall short of the robust stake-\nholder engagement needed to tackle the ethical, practical, and cultural\nchallenges in healthcare\n13.\nIn this paper, we deﬁne LLMHS as the use of LLMs to enhance\nhealthcare processes through AI-driven functionalities. We consider par-\nticipatory approaches14 as methodologies that actively engage stakeholders\nin the design, implementation, andevaluation of LLMHS. We argue that\nembedding such approaches throughout the LLMHS lifecycle ensures that\nsystems are not only technologically advanced but also ethically sound,\npractically relevant, and culturallysensitive. More importantly, LLMs can\nenable new participatory opportunities to enhance stakeholder engagement\n1School of Engineering and Applied Science, University of Virginia, Charlottesville, VA, USA.2Nell Hodgson Woodruff School of Nursing, Emory University, Atlanta,\nGA, USA.3School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, USA.4School of Nursing, University of Virginia, Charlottesville, VA, USA.\n5School of Medicine, University of Virginia, Charlottesville, VA, USA.e-mail: vmf9pr@virginia.edu\nnpj Health Systems|            (2025) 2:22 1\n1234567890():,;\n1234567890():,;\nand healthcare outcomes. We propose a participatory framework (Fig.1)f o r\nco-designing, validating, integrating, and reﬁning LLMHS to ensure they are\nequitable, practical, and impactful for all stakeholders.\nPreliminaries\nParticipatory approaches\nParticipatory approaches refer to methods that actively involve stakeholders\n(such as patients, clinicians, and researchers in this context) in the co-design,\nimplementation, and evaluation of systems. Emerging from the social\nsciences-such as action research\n15 and participatory research14-and later\nadopted by the human-computer in teraction community through\napproaches like participatory design16, these methods aim to ensure diverse\nperspectives foster system relevance and health equity12,17.T h eU . S .F o o d\nand Drug Administration’s Collaborative Communities Toolkit18 released\nin 2019, also emphasizes these approaches, advocating for collaborative\nefforts in the medical devices sectorto address healthcare challenges.\nIn the context of LLMHS, we suggest that participatory approaches are\nindispensable for fostering stakeholder engagement throughout the entire\nlifecycle, from initial design to ongoing reﬁnement. Our proposed partici-\npatory framework aims to not only highlight how LLMHS can beneﬁtf r o m\nstakeholder input but also illustrates how the unique capabilities of LLMs\ncan actively enhance participation.\nParticipatory gaps of LLMHS\nImplementing LLMHS holds great potential, but bridging computational\nadvancements with real-world stakeholder participation remains a key\nchallenge. Here, we outline the key gaps limiting stakeholder engagement:\nTechnological limitations: The growing adoption of LLMs in\nhealthcare faces technological barriers, including high computational\ndemands, energy consumption, and the technical literacy required for\ndeployment\n19. These challenges make LLMs inaccessible to resource-limited\nhealthcare organizations. Additionally, LLMs face issues such as model\nbiases, limited output explainability, and high adaptation costs\n20.I n\nhealthcare settings, these limitations are critical; for instance, an LLMHS\nmay generate plausible but incorrectmedication recommendations due to\nLLM hallucinations, eroding clinician trust21 and endangering patient well-\nbeing. Furthermore, inconsistent outputs from identical clinical prompts\nand underdeveloped standardization mechanisms raise concerns about\nreliability and safety\n22.\nLiability concerns:Liability concerns are a major barrier to adopting\nLLMHS, highlighting the need for clearer guidelines and collaborative\naccountability frameworks. Patients worry about who (or what) is\naccountable for misdiagnoses, treatment errors, data privacy breaches, and\ntechnical failures\n23. Healthcare providers face legal uncertainties, particu-\nlarly when LLMHS recommendations conﬂict with clinical judgment,\ncreating ambiguity about their liability when overriding AI suggestions24.\nMoreover, emerging evidence indicates that despite the promise of reducing\nclinician workload, the necessity for meticulous review of LLM-generated\ncontent may impose a substantial cognitive and administrative burden on\nhealthcare providers, potentially increasing liability risks\n25.\nContextualization gaps: Current LLM systems’ integration into\nhealthcare workﬂows and critical scenarios remains understudied26.C l i n -\nicians report frustration with note-taking systems that overlook workﬂow\nneeds, which reﬂects gaps in stakeholder input and clinical validation27.\nThese gaps are magniﬁed in underserved settings, including historically\nmarginalized, rural, and Indigenous communities. For example, Black and\nHispanic stakeholders may face additional challenges such as mistrust in\nhealthcare technologies due to historical inequities, linguistic barriers, and\nlimited access to the digital infrastructure required to support LLMHS\n28.\nRural clinics may face signiﬁcant barriers with cloud-dependent LLMHS,\nwhich are often incompatible with limited internet infrastructure29,w h i l e\nIndigenous communities canﬁnd these systems poorly aligned with tradi-\ntional healing practices30. Engaging diverse stakeholders during design and\ntesting phases is critical for tailoring LLMHS to varied healthcare contexts,\nimproving usability and acceptance\n31.\nEthical challenges:Integrating LLMs into healthcare raises funda-\nmental ethical challenges that extend beyond technical and legal barriers.\nThe potential for systematic biases in LLM recommendations that\nnegatively impact marginalized populations, such as racial and gender\nminorities, underscores the critical need for equitable model develop-\nment and deployment\n32. Additionally, LLMs may unintentionally erode\nclinical autonomy by subtly in ﬂuencing decision-making, especially\nwhen their recommendations appear authoritative but lack\ntransparency22. This could disempower both clinicians and patients,\ncomplicating shared decision-making. Furthermore, the absence of\nempathetic, human-centered interaction in AI-driven care highlights the\nimportance of preserving compassion in healthcare delivery\n23. To address\nthese ethical concerns, LLM systems must be rigorously validated,\ndesigned with fairness and inclusivity at their core, and integrated in\nways that complement-rather than replace-human judgment and care.\nParticipatory framework in LLMHS lifecycle\nAs illustrated in Table1, we identify participatory gaps and suggest solutions\nthroughout each phase of the LLMHS lifecycle.\nContextualization and development\nThe design and development of an LLMHS must prioritize embedding\nstakeholders’ needs to ensure efﬁciency and effectiveness, with joint efforts\nfrom both technical and healthcare sides. Each stakeholder group, from\nhealthcare professionals to underrepresented community members, brings\nunique objectives that should guide the system’s contextualization and\ndevelopment.\nStakeholder involvement through co-design and iterative feedback\nsessions is essential to guarantee both the usability and efﬁcacy of LLMHS.\nSpeciﬁcally, these systems should leverage interactive features while pro-\nmoting health equity across diverse healthcare settings, particularly among\nindividuals of minoritized groups and within medically underserved areas\n33.\nIncorporating stakeholder-centered methods, such as pre-design surveys\nand interviews, to identify and deﬁne speciﬁc healthcare problems early in\nthe process ensures that solutions are not only innovative but also practically\nStakeholders\nLLM-Enhanced\nHealth Systems\nParticipatory\nApproaches\nInform & \nCo-Design\nServe &\nSupport\nFacilitate\nEngage &\nEmpower\nBenchmark, Risk \nMitigation, & Evaluation\nLLMHS Lifecycle\nFig. 1 | Overview of the triadic relationship among stakeholders, participatory\napproaches, and LLMHS across the system lifecycle.Participatory approaches\nengage and empower stakeholders to inform and co-design LLMHS, which in turn\nbetter serve and support stakeholders. LLMHS enable and facilitate new\nparticipatory opportunities, fostering iterative collaboration across four lifecycle\nphases: Contextualization & Development, Benchmark, Risk Mitigation, &\nEvaluation, Clinical Integration, andLongitudinal Adaptation & Reﬁnement.\nhttps://doi.org/10.1038/s44401-025-00024-5 Perspective\nnpj Health Systems|            (2025) 2:22 2\ngrounded. Deﬁning clear“metrics for success”-such as stakeholder satis-\nfaction, reduction in model bias, andimproved patient outcomes-can fur-\nther ensure participatory solutions deliver measurable value. Digital divide\nissues should be carefully considered ,w i t hs o l u t i o n ss u c ha sa d d i n go fﬂine\nfunctionality, supporting low-bandwidth connections, and partnering with\nlocal communities to enhance technology access34.\nLLM-moderated participatory co-design . LLM-Moderated Co-\nDesign35 leverages LLMs as moderators or active participants to guide\nstakeholder collaboration during design sessions. Figure2 illustrates the\ndesign notes moderated by LLMs for a health system. For example, in co-\ndesign workshops, LLMs could simulate application scenarios, generate\nprototypes from stakeholder input, and pose follow-up questions that\ndeepen discussions. By integrating diverse stakeholder perspectives on\nsystem functions, workﬂows, and target outcomes, the LLM distills these\ninsights into actionable design ideas, drafts mockups, and highlights\npotential gaps. Recent work\n36 shows that structuring interactions between\nhuman stakeholders and LLMs into three phases-Initiation, Discussion,\nand Convergence-accelerates cross-culture idea generation, reduces\nfacilitator cognitive load, and fosters creativity. Compared to conven-\ntional human-driven co-design, this “Human-LLM collaboration ”\napproach can offer more efﬁcient and inclusive participation while pre-\nserving human creativity\n37.\nImportantly, this adaptive co-desi g np r o c e s sn o to n l yf a c i l i t a t e s\ndynamic stakeholder input but also mitigates practical challenges-such as\ninitial resistance and limited engagement from underserved groups-by\nleveraging in-context learning and role-play simulations38. By simulating\ndiverse stakeholder perspectives, LLMs hold the potential to help narrow\nparticipation gaps when certain groups are underrepresented or difﬁcult to\ninvolve, making the design processmore inclusive and context-aware.\nAlignment techniques for enhancing relevance and context in co-\ndesign. Selectively leveraging advanced LLM techniques improves co-\ndesign relevance by dynamically integrating real-time inputs and medical\nknowledge through a pragmatic, phased approach. LLMs’ In-Context\nLearning\n1 can address workﬂow challenges without retraining, yielding\ntailored outputs from existing clinical documentation and standardized\nworkﬂows. For example, when a clinician identiﬁes medication recon-\nciliation as an issue, the LLM can simulate relevant scenarios and suggest\noptimized strategies based on available clinical data. lChain-of-Thought\n(CoT) prompting\n39 breaks down complex design challenges into\nsequential steps to enhance decision transparency. For instance, in a\npatient intake system co-design session, the LLM could list key objectives\n(e.g., reducing wait times), break them into tasks (e.g., streamlining\nregistration, triage, and resource allocation), and propose tailored solu-\ntions. This step-by-step process-shown to improve clinical reasoning in\ndifferential diagnosis, medical reasoning, and management decision-\nmaking\n40-offers stakeholders an actionable roadmap that supports\niterative reﬁnement and improved outcomes. Techniques such as\nRetrieval-Augmented Generation (RAG) 41 and Model Fine-Tuning42\nenhance factuality and relevance by integrating external healthcare and\nsystem design literature. For example, in co-design, a hospital might\nTable 1 | Addressing gaps and participatory solutions throughout LLMHS lifecycle\nPhase of LLMHS Lifecycle Key Stakeholders Participatory Gaps Participatory Solutions\nContextualization and\nDevelopment\nPatients, clinicians, caregivers, and\nadministrators, particularly from\nunderserved groups\nLimited stakeholder engagement; systems fail to\naddress diverse needs in workﬂows and\ncontexts; mistrust in underserved populations\n– LLM-Moderated Co-Design35,36:\nAccelerates idea generation and iteration\nthrough LLM role-play.\n– Multi-Agent Simulations38: Simulates\ndiverse scenarios to reveal context-speciﬁc\nchallenges.\n– Fine-Tuning and RAG41,42: Enhances\nfactuality and contextual relevance through\ndomain-speciﬁc adaptation.\nBenchmarking, Risk\nMitigation, and Evaluation\nClinicians, administrators, and\npolicymakers\nLack of transparency and standardization;\ninsufﬁcient real-world clinical data; bias and\nfairness concerns\n– Stakeholder-Co-Designed\nBenchmarks43: Establishes evaluation\nmetrics aligned with clinical standards.\n– Synthetic Data Generation47: Supplies\nrealistic datasets when real data are scarce.\n– Red-Teaming49,50: Uncovers\nvulnerabilities and mitigates biases via\nadversarial testing.\n– Clinical Trials\n13: Validates efﬁcacy and\nsafety through controlled, real-world\nstudies.\nClinical Integration Clinicians, patients, caregivers, and\nadministrators\nWorkﬂow disruption; trust and liability concerns;\nlack of transparency and interpretability\n– LLM Agent Workﬂows\n55: Streamlines\noperations with automation while\npreserving clinician oversight.\n– Explainable AI (XAI)59: Enhances\ntransparency and bolsters trust via\ninterpretable decision-making.\n– Training Programs\n63: Boosts role-\nspeciﬁc competency and facilitates\neffective adoption.\nLongitudinal Adaptation and\nReﬁnement\nAll stakeholders Limited continuous feedback; systems fail to\nadapt to evolving clinical practices and\nstakeholder needs\n– RLHF and Fine-Tuning64,66: Continuously\nadapts system performance based on\nongoing stakeholder feedback.\n– Real-Time Monitoring: Enables\nproactive adjustments by tracking\nperformance metrics as they occur.\n– Agile Development Cycles\n67: Facilitates\niterative and responsive updates aligned\nwith evolving needs.\n– Pilot Testing: Validates system\nperformance through iterative, real-world\ntrials.\nRAG Retrieval-Augmented Generation,LLM Agent, LLM-Empowered Autonomous Agent,RLHF Reinforcement Learning with Human Feedback.\nhttps://doi.org/10.1038/s44401-025-00024-5 Perspective\nnpj Health Systems|            (2025) 2:22 3\nbegin byﬁne-tuning models on their clinical documentation practices,\nthen integrate RAG with standard clinical guidelines, and gradually\nexpand to more complex workﬂows. This measured expansion helps\nmanage computational requirements while maintaining system relia-\nbility and clinical safety. See the Figure for a demonstration of the LLM-\nmoderated co-design process. Notably, organizations can selectively\nincorporate these modules based on their speciﬁc needs and available\nresources.\nBenchmark, risk mitigation, and evaluation\nRigorous benchmarking, mitigation, and real-world evaluation are critical\nto ensure ethical deployment and stakeholder trust in LLMHS. Although\nmedical AI holds great promise, research indicates that 43.4% of AI health\ntools that received regulatory FDA authorization between 2016 and 2022\nlacked reported clinical validation data\n13, highlighting the critical need for\nrigorous evaluation in LLM systems in healthcare .\nStakeholder-driven benchmarking and clinical evaluation. Bench-\nmarking involves systematically evaluating LLMHS performance against\nclinical standards or expert judgments to assess system effectiveness and\nreliability, using metrics like accuracy, robustness, fairness, and com-\nputing cost. Speciﬁcally, recent study shows that among 519 studies of\nhealthcare LLMs, 95.4% focused on accuracy (e.g., alignment with clin-\nicians’ judgments), 47.0% on comprehensiveness, 18.3% on factuality,\n14.8% on robustness, 15.8% on fairness, bias, and toxicity, 4.6% on\ndeployment metrics, and only 1.2% on calibration and uncertainty\n43. For\ninstance, LLM-generated outputs should be compared with those\nprovided by clinicians to evaluate alignment with professional judgment\nand ensure equitable outcomes. Stakeholder participation should play an\nactive role in co-designing these evaluation metrics, ensuring they not\nonly align with key priorities and patient outcomes but also reﬂect real-\nworld challenges in clinical workﬂow\n44. Moreover, it is critical to address\nthe current gap in reporting additional metrics-speciﬁcally bias, toxicity,\ncalibration, and deployment considerations these dimensions are\nessential for stakeholders to guarantee patient safety and equitable\ncare\n45,46. By proactively participating, stakeholders could help shape a\nbenchmark framework assessing both the technical and ethical dimen-\nsions of LLM performance in health settings. Particularly, when real-\nworld data are scarce or data collection is prolonged, LLM-enabled\nsynthetic data generation\n47 can produce preliminary datasets that facil-\nitate initial analysis. Acknowledging that synthetic data may oversimplify\nclinical complexities and introduce biases-and that its use is not feasible\nin all scenarios-stakeholders’ veriﬁcation and contextualization of these\noutputs is crucial. This alternative benchmark method supports pre-\nliminary proof-of-concept studies (e.g.,\n47,48) before comprehensive\nlongitudinal evaluations.\nCollaborative stakeholder engagement for risk and bias mitigation.\nModel Mitigation focuses on managing risks and biases inherent to\nLLMHS to enhance safety, equity, and inclusion. Red Teaming49 is a\nproactive participatory approach that involves stakeholders in‘adver-\nsarial’testing scenarios (e.g., presenting ambiguous or conﬂicting patient\ndata to assess whether the system generates unsafe, biased, or inequitable\nclinical recommendations) to identify vulnerabilities and failure cases.\nStakeholder Needs:\nProviders: Seek suggestions to improve communication quality.\nPatients: Prioritize understanding and culturally sensitive communication.\nAdmins: Care about cost-effectiveness and long-term system benefits.\nDesign Ideas:\nEmbed LLMHS into off-the-shelf devices to analyze clinical communication,\nprovide actionable feedback, and integrate with EHR systems.\nDesign Considerations:\nMetrics to assess: Empathy, clarity, and emotion support.\nEvidence to capture: Pauses, medical jargon, and empathetic expressions.\nFeedback format: Text summaries and multimedia cues to providers.\n...\nKnowledge-Informed Design Decisions:\nClinical guidelines: Use validated strategies (e.g., motivational interviewing) for\nclinical alignment.\nTechnical constraints: Surface best practices for EHR integration, including\nHIPAA-compliant secure storage and processing of sensitive audio data.\n...\nSystem Prototype:\nA smartwatch interface for LLMHS: with patient consent, clinicians tap \"Record\" to\ncapture audio, processed via a HIPAA-compliant pipeline. Post-meeting, the\nsystem delivers actionable feedback on communication quality. Pilot-test the\nsystem with real-world stakeholders to iteratively fine-tune performance and\nensure contextual relevance.\nLLM Preparation\nTechnique: Supervised Fine-Tuning\nContent: Fine-tunes general-purpose\nLLMs with domain-relevant data, laying the\nfoundation for stakeholder-aligned outputs.\nStakeholder Demand Collection\nTechnique: Prompt Engineering\nContent: Tailors prompts to uncover\nstakeholder needs, feedback gaps, initial\ndesign ideas, and priorities dynamically.\nCo-Design Session\nTechnique: LLM-Empowered Agents\nContent: Simulates scenarios to explore\nstakeholder perspectives, identify design\ngaps, and generate real-time insights.\nKnowledge Enhancement\nTechnique: Retrieval-Augmented Generation\nContent: Integrates external knowledge\nfor evidence-based, constraint-aligned,\nand context-aware insights.\nValidation and Adjustment\nTechnique: Reinforcement Learning with\nHuman Feedback\nContent: Refines LLM outputs using\nstakeholder feedback, ensuring relevance\nand continuous improvement.\nDesign Notes: LLM-Moderated Co-Design for Compassionate Patient-Provider\nCommunication\nFig. 2 | Illustration of an LLM-moderated co-design process for developing the\nCompassionate Patient-Provider Communication Assessment and\nFeedback LLMHS.The process integrates stakeholder needs, iterative discussions,\nand domain knowledge to reﬁne actionable design principles, culminating in a\nprototype leveraging LLM capabilities and techniques throughout the co-design.\nhttps://doi.org/10.1038/s44401-025-00024-5 Perspective\nnpj Health Systems|            (2025) 2:22 4\nFor example, red-teaming simulations revealed that LLMs were more\nlikely to include pain management for White patients but sometimes\nomitted it for Black patients, even when clinical cases were identical.\nCounseling recommendations also reﬂected bias: the model emphasized\nempathy for White patients but focused on medicolegal liability for Black\npatients\n50. As emphasized in interactive machine learning research51,\nintegrating human insights is critical to reﬁning system performance.\nDiverse stakeholder participation fosters participatory auditing, unco-\nvering biases and guiding corrective actions to improve system reliability.\nHowever, stakeholders may also hold implicit biases or lack sufﬁcient\nfamiliarity with LLMs, which can limit their ability to pinpoint system-\ngenerated issues. Addressing these gaps requires targeted stakeholder\neducation and support to clarify LLM-related risks. Bias Mitigation\nmethods\n52, such as safety constraints and guardrails (applying rule-based\nmeasures to detect and block inappropriate outputs), are essential to\nensure equitable outcomes, particularly for marginalized patient popu-\nlations. In these processes, engaging stakeholders directly informs and\nenhances the mitigation methods aligned with issues identiﬁed, from\nprompt engineering to model adjustments, ultimately improving system\nreliability and care quality.\nClinical integration\nEffective clinical integration of LLM-enhanced systems requires embedding\nthese technologies seamlessly into healthcare operations, optimizing\nengagement among stakeholders and minimizing workﬂow disruption.\nSuccessful implementation depends on three key factors: strategic workﬂow\nintegration, robust decision support capabilities, and comprehensive sta-\nkeholder training\n53. These elements not only ensure efﬁcient adoption but\nalso empower stakeholders to leverage the technology’sf u l lp o t e n t i a li n\nclinical settings.\nStakeholder-friendly workﬂow integration. Current LLM integration\ninto clinical workﬂows faces signiﬁcant challenges. For example, LLM-\nbased documentation assistants may inadvertently increase physician\ncognitive load by requiring extensive veriﬁcation, particularly in complex\ncases with multiple comorbidities\n54. When integrated with Electronic\nHealth Records (EHR) systems, LLMs often fail to account for the full\nclinical context. This can result in incomplete or imprecise interpreta-\ntions of patient data, which in turn may compromise the quality of\nclinical decision-making. To address these challenges, implementing a\nstructured feedback mechanism can facilitate continuous system\nreﬁnement. For example, in medication administration workﬂows, a\nstaged veriﬁcation process enables LLMs to identify potential drug\ninteractions and present them in order of severity and urgency. This\nmethod allows physicians to quickly review and verify critical issues while\nintegrating with existing clinical protocols and maintaining essential\nhuman oversight. Additionally, Agentic Workﬂows (or LLM Autono-\nmous Agents)\n55 show promise in automating multi-step healthcare tasks-\nsuch as appointment scheduling56, record keeping57, and initial diagnostic\nassessments58-to reduce administrative burdens on healthcare providers.\nHowever, successful automation depends on establishing clear handoff\nprotocols between AI and clinicians, especially in high-stakes clinical\nsettings. Workﬂow designs must also include explicit veriﬁcation points\nwhere clinicians can review and adjust AI-generated suggestions, thereby\nensuring both efﬁciency and safety.\nTransparency and interpretability in supporting stakeholders .\nChallenges related to transparency extend beyond technical explanations\nto practical clinical utility. While Explainable AI (XAI)\n59 features provide\nfoundational decision rationales and/or reasoning steps, successful\nimplementation demands stakeholder-speciﬁc approaches to explana-\ntion delivery that explicitly address current integration failures60. For\ninstance, when XAI outputs oversimplify or omit critical uncertainties,\nthey may lack sufﬁcient causability-the ability to convey the underlying\nrationale-leading clinicians to misinterpret recommendations and\nultimately eroding trust in the system\n61. To address these issues, parti-\ncipatory design and iterative feedback are essential. We propose inte-\ngrating regular feedback sessions into existing clinical infrastructures\nsuch as quality improvement meetings-where clinicians and AI ethical\nexperts reﬁne and customize XAI outputs and visualizations for better\nintegration, and patient feedback mechanisms help assess explanation\nclarity using visualization tools\n62.\nTraining stakeholders for effective clinical adoption. Effective train-\ning is crucial for healthcare professionals to fully harness the beneﬁts of\nLLM systems in clinical settings. While clinicians and administrative staff\nneed training to interpret and apply LLM-generated insights effectively,\nLLMs themselves can play an active role in this training process. By\noffering tailored learning modules speciﬁc to different roles and simu-\nlating healthcare scenarios, LLMs can facilitate hands-on practice that\ndeepens understanding and builds conﬁdence\n63. These training programs\nfoster AI competency across roles, enabling healthcare professionals to\nleverage LLM capabilities more effectively, thereby improving efﬁciency\nand enhancing patient care outcomes.\nLongitudinal adaptation and reﬁnement\nLongitudinal adaptation and iteration are pivotal for ensuring sustained\nefﬁcacy and acceptance among stakeholders. Continuous monitoring and\nfeedback mechanisms should be established to collect usage data and per-\nformance metrics, including system accuracy, user behaviors, workﬂow\nimpact, and patient outcomes. Regular feedback collected through surveys,\ninterviews, focus groups, and automated system interactions will identify\nareas for reﬁnement. These analytics provide insights into system dynamics,\nenabling ongoing adaptation to optimize performance as the system evolves.\nStakeholder-in-the-loop iterative adaptation. While Reinforcement\nLearning with Human Feedback (RLHF)64 has transformed general LLM\ndevelopment, its adoption in healthcare remains limited due to unique\nclinical challenges. The inherent complexity of medical decision-making,\nstrict regulatory requirements, and the need for coordinated expert\nfeedback have hindered RLHF’s broader implementation. Recent work\nshows that an iterative online RLHF approach-where models are con-\ntinuously updated with fresh human feedback to ensure they are both\nhelpful and harmless-can improve performance without compromising\nspecialized skills\n65. In a robust healthcare RLHF framework, stakeholder-\nin-the-loop iterative adaptation is essential. Clinical experts can assess\nclinical validity and evaluate workﬂow integration, patients and care-\ngivers offer insights on communication clarity and accessibility, and\nhealthcare administrators ensure operational efﬁciency and regulatory\ncompliance. This multi-stakeholder collaboration can unfold over three\nphases: initial training-where baseline metrics and feedback protocols are\nestablished; reﬁnement-where weighted stakeholder input is iteratively\nintegrated through regular validation cycles; and deployment-where\ncontinuous monitoring and systematic updates occur. Additionally,\nModel Fine-tuning\n66 can complement RLHF by enabling targeted\nadjustments based on accumulated feedback, ensuring that the system\nevolves ef ﬁciently. Together, these techniques support continuous\nimprovement, maintain clinical safety, and adapt to evolving healthcare\nneeds. Further research into healthcare-speci ﬁc RLHF metrics and\nstrategies for balancing diverse stakeholder inputs is needed to fully\nrealize the potential of these approaches.\nCustomization and scalability for diverse clinical needs. At the sys-\ntem level, implementing agile development methodologies67 can also\nensure that the system evolves rapidly in response to stakeholder feed-\nback, while pilot testing and validation studies of new features can\nidentify potential issues before full-scale deployment. This iterative\nprocess, combined with stakeholder-driven ﬁnetuning and RLHF,\nensures that LLMHS remain adaptable, effective, and aligned with clinical\nneeds. System customization and personalization are also important.\nhttps://doi.org/10.1038/s44401-025-00024-5 Perspective\nnpj Health Systems|            (2025) 2:22 5\nThese adjustments could enable the system to meet the speciﬁc needs of\ndifferent clinical environments, including underrepresented and min-\nority groups. Developing scalable solutions that can adapt to varying\nresource levels and technological infrastructures is vital for ensuring the\nsystem’s broad applicability in diverse healthcare settings.\nParticipatory methods in action: two scenarios\nTo demonstrate how participatory approaches enhance LLMHS develop-\nment and deployment, we examine two healthcare scenarios where stake-\nholder engagement is crucial for system success. lThese case studies serve as\npreliminary empirical illustrations of our proposed framework in action.\nEvaluating and improving healthcare providers’ clinical\ncommunication skills\nEffective communication between clinicians and patients is critical for trust,\nsatisfaction, and quality health outcomes. Traditional communication\nevaluation methods that rely on human raters and surveys are resource-\nintensive and often overlook cultural nuances\n68. In contrast, LLMHS inte-\ngrated with digital tools offer a scalable solution48, enabling real-time ana-\nlysis of conversations and feedback on key metrics such as empathy and\nclarity\n69,70 that can help improve the quality of communication. Speciﬁcally,\nin68, clinicians and engineers co-designed a smartwatch-based system,\nCommSense, that can audio-record patient-clinician conversations (with\ninformed consent) and provide actionable feedback to clinicians about their\ncommunication. The smartwatch’s embedded microphone records inter-\nactions and uses audio models alongside LLMs to extract pre-determined\ncommunication metrics (e.g., understanding, empathy, emotion, clarity)\nand provide feedback to clinicians. For example, CommSense could provide\nreal-time haptic feedback to alert the clinician that excessive medical jargon\nis being used or that frequent interruptions are occurring, as well as generate\npost-visit summaries and longitudinal tracking related to communication\nperformance. In our prior work, we quantitatively benchmarked Comm-\nSense’s performance using stakeholder-drafted scripts with human\nannotations\n48. The next phase involves real-world clinical validation and\nstakeholder interviews to assess its effectiveness in practice.\nFurthermore, future work in participatory co-design will aim to more\ndeeply engage healthcare providers in customizing preferred communica-\ntion metrics across diverse clinical scenarios and aligning system func-\ntionality with speciﬁcc l i n i c a lw o r kﬂows. Incorporating structured feedback\nloops and RLHF into the iterations of language models and ensuring early\nstakeholder involvement can help mitigate cultural biases and drive con-\ntinuous system adaptation. This integration of technical reﬁnement with\nstakeholder insights demonstrates thepotential of the LLMHS framework to\nimprove compassionate healthcare and patients’ quality of life.\nCreating accessible patient educational materials for dis-\nadvantaged patients and caregivers\nAccessibility of healthcare information presents a signiﬁcant challenge,\nparticularly for underserved communities facing language barriers and\nvarying health literacy levels71. To address this issue, researchers can conduct\nsemi-structured interviews combined with LLMHS-moderated co-design\nsessions to identify the speciﬁc challenges faced by patients and their carers\nin managing cancer symptoms. These challenges may include difﬁculties\nwith medical terminology, the need for practical self-care guidance, and a\npreference for multimedia educational formats. Subsequently, LLMHS can\nbe employed to generate personalized multimedia educational materials-\ntransforming clinical guidelines into accessible visual guides, simpliﬁed\ntexts, and interactive elements that reﬂect the patient’s individual experience\nand needs\n72,73. Stakeholder-engaged outputvalidation is crucial in this\ncontext to ensure content accuracy, cultural alignment, and optimize\nreadability.\nConclusion and recommendations\nTo achieve the sustainable and equitable integration of LLMHS, it is essential\nto embed stakeholder-centric participatory approaches throughout the\nsystem lifecycle. Engaging diverse stakeholders through iterative co-design\nensures that these systems align with the practical, ethical, and cultural needs\nof healthcare communities. By addressing critical gaps in transparency,\ncontextualization, and trust, participatory approaches can advance health-\ncare while fostering broader acceptance among clinicians, patients, and\nother stakeholders.\nThe unique capabilities of LLMs themselves offer transformative\nopportunities for enhancing participatory engagement. Tools such as LLM-\nmoderated co-design and real-time in-context feedback mechanisms enable\ndynamic collaboration during co-design, while technical foundations like\nRAG and RLHF ensure systems remain responsive to evolving clinical\npractices and stakeholder needs. Additionally, integrating XAI components\nand LLM agentic workﬂows fosters transparency, mitigates bias, and safe-\nguards clinical autonomy, enhancing trust and usability. These approaches\nare particularly critical for addressing barriers in underserved and mar-\nginalized communities, where culturally sensitive and adaptable solutions\nare vital.\nBy operationalizing these strategies, digital health practitioners\ncan design LLMHS that are technically robust, ethically sound, and\nsocially responsible. Moving forward,prioritizing stakeholder-in-the-\nloop and agile development methodologies with responsible clinical\nvalidation will be essential to reﬁne these systems and ensure their\nintegration into diverse healthca re settings advances health equity\nand fosters trust among stakeholders. Moreover, future work must\ninvolve continued validation through well-designed empirical studies\nand detailed case analyses to further assess and re ﬁne the frame-\nwork’s effectiveness in clinical contexts.\nData availability\nNo datasets were generated or analysed during the current study.\nReceived: 28 December 2024; Accepted: 7 May 2025;\nReferences\n1. Singhal, K. et al. Large language models encode clinical knowledge.\nNature 620, 172– 180 (2023).\n2. Jeblick, K. et al. ChatGPT makes medicine easy to swallow: an\nexploratory case study on simpliﬁed radiology reports.Eur. Radiol.34,\n2817– 2825 (2024).\n3. Pais, C. et al. Large language models for preventing medication\ndirection errors in online pharmacies.Nat. Med.1– 9 (2024).\n4. Clusmann, J. et al. The future landscape of large language models in\nmedicine. Commun. Med.3, 141 (2023).\n5. Company, B. & Research, K. Healthcare it spending: Innovation,\nintegration, and aihttps://www.bain.com/insights/healthcare-it-\nspending-innovation-integration-ai/ (2024).\n6. Webster, P. Medical AI chatbots: Are they safe to talk to patients?Nat.\nMed. 29, 2677– 2679 (2023).\n7. Liu, F. et al. Large language models are poor clinical decision-makers: A\ncomprehensive benchmark. InProceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,1 3 6 9 6– 13710\n(2024).\n8. Hogg, H. D. J. et al. Stakeholder perspectives of clinical artiﬁcial\nintelligence implementation: systematic review of qualitative\nevidence. J. Med. Internet Res.25, e39742 (2023).\n9. Bedi, S. et al. Testing and Evaluation of Health Care Applications of\nLarge Language Models: A Systematic Review.JAMA 333, 319– 328\n(2025).\n10. Cargo, M. & Mercer, S. L. The value and challenges of participatory\nresearch: strengthening its practice.Annu. Rev. Public Health29,\n325– 350 (2008).\n11. Brett, J. et al. Mapping the impact of patient and public involvement on\nhealth and social care research: a systematic review.Health Expect.\n17, 637– 650 (2014).\nhttps://doi.org/10.1038/s44401-025-00024-5 Perspective\nnpj Health Systems|            (2025) 2:22 6\n12. Wallerstein, N. B. & Duran, B. Using community-based participatory\nresearch to address health disparities.Health Promot. Pract.7,\n312– 323 (2006).\n13. Chouffani El Fassi, S. et al. Not all AI health tools with regulatory\nauthorization are clinically validated.Nature Medicine30, 2718– 2720\n(2024).\n14. Cornwall, A. & Jewkes, R. What is participatory research?Soc. Sci.\nMed. 41, 1667– 1676 (1995).\n15. Lewin, K. Action research and minority problems.J. Soc. issues2,\n34– 46 (1946).\n16. Muller, M. J. & Kuhn, S. Participatory design.Commun. ACM36,\n24– 28 (1993).\n17. Cornish, F. et al. Participatory action research.Nat. Rev. Methods\nPrim. 3, 34 (2023).\n18. U.S. Food and Drug Administration. Collaborative communities toolkit\nhttps://www.fda.gov/media/116467/download. Accessed: 2024-11-\n03. (2019).\n19. Karabacak, M. & Margetis, K. Embracing large language models for\nmedical applications: opportunities and challenges.Cureus 15,\ne39305 https://doi.org/10.7759/cureus.39305.\n20. Gallegos, I. O. et al. Bias and fairness in large language models: A\nsurvey.Comput. Linguist.50, 1097– 1179 (2024).\n21. Park, Y.-J. et al. Assessing the research landscape and clinical utility\nof large language models: A scoping review.BMC Med. Inform. Decis.\nMak. 24, 72 (2024).\n22. Abgrall, G., Holder, A. L., Chelly Dagdia, Z., Zeitouni, K. & Monnet, X.\nShould AI models be explainable to clinicians?Crit. Care28, 301\n(2024).\n23. Mirzaei, T., Amini, L. & Esmaeilzadeh, P. Clinician voices on ethics of\nLLM integration in healthcare: a thematic analysis of ethical concerns\nand implications.BMC Med. Inform. Decis. Mak.24, 250 (2024).\n24. Duffourc, M. & Gerke, S. Generative AI in Health Care and Liability\nRisks for Physicians and Safety Concerns for Patients.JAMA 330,\n313– 314 (2023).\n25. Ohde, J. W., Rost, L. M. & Overgaard, J. D. The burden of reviewing\nLLM-generated content.NEJM AI.2, AIp2400979 (2025).\n26. Zhou, H. et al. A survey of large language models in medicine:\nProgress, application, and challenge.arXiv preprint arXiv:2311.05112\n(2023).\n27. Mannhardt, N. et al. Impact of large language model assistance on\npatients reading clinical notes: A mixed-methods study.arXiv preprint\narXiv:2401.09637 (2024).\n28. Richardson, S., Lawrence, K., Schoenthaler, A. M. & Mann, D. A\nframework for digital health equity.NPJ Digit. Med.5, 119 (2022).\n29. Marcin, J. P., Shaikh, U. & Steinhorn, R. H. Addressing health\ndisparities in rural communities using telehealth.Pediatr. Res.79,\n169– 176 (2016).\n30. Iloanusi, N.-J. & Chun, S. A. AI impact on health equity for\nmarginalized, racial, and ethnic minorities. InProceedings of the 25th\nAnnual International Conference on Digital Government Research,\ndg.o ’24, 841-848https://doi.org/10.1145/3657054.3657152.\n(Association for Computing Machinery, New York, NY, USA, 2024).\n31. Concannon, T. W. et al. Practical guidance for involving stakeholders\nin health research.J. Gen. Intern. Med.34, 458– 463 (2019).\n32. Kotek, H., Dockum, R. & Sun, D. Gender bias and stereotypes in large\nlanguage models. InProceedings of the ACM Collective Intelligence\nConference,1 2– 24 (2023).\n33. Harrington, C., Erete, S. & Piper, A. M. Deconstructing community-\nbased collaborative design: Towards more equitable participatory\ndesign engagements.Proc. ACM Hum.-Comput. Interact.3,1 – 25\n(2019).\n34. Prattipati, S. M. Bridging the digital divide: Overcoming barriers to\ninternet access.Adv. J. Manag. Soc. Sci.7,8 6– 104 (2023).\n35. He, J. et al. AI and the future of collaborative work: Group ideation with\nan LLM in a virtual canvas. InProceedings of the 3rd Annual Meeting of\nthe Symposium on Human-Computer Interaction for Work, CHIWORK\n’24 https://doi.org/10.1145/3663384.3663398 (Association for\nComputing Machinery, New York, NY, USA, 2024).\n36. Lu, L.-C. et al. LLM discussion: Enhancing the creativity of large\nlanguage models via discussion framework and role-play. In\nConference on Language Modeling(2024).\n37. Gao, J. et al. A taxonomy for human-llm interaction modes: An initial\nexploration. InExtended Abstracts of the CHI Conference on Human\nFactors in Computing Systems,1 – 11 (2024).\n38. Park, J. S. et al. Generative agent simulations of 1000 people.arXiv\npreprint arXiv:2411.10109(2024).\n39. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large\nlanguage models.Adv. Neural Inf. Process. Syst.35, 24824– 24837\n(2022).\n40. Brodeur, P. G. et al. Superhuman performance of a large language\nmodel on the reasoning tasks of a physician.arXiv preprint\narXiv:2412.10849 (2024).\n41. Lewis, P. et al. Retrieval-augmented generation for knowledge-\nintensive NLP tasks.Adv. Neural Inf. Process. Syst.33, 9459– 9474\n(2020).\n42. Ding, N. et al. Parameter-efﬁcient\nﬁne-tuning of large-scale pre-\ntrained language models.Nat. Mach. Intell.5, 220– 235 (2023).\n43. Bedi, S. et al. A Systematic Review of Testing and Evaluation of\nHealthcare Applications of Large Language Models (LLMs). medRxiv\n2024.04.15.24305869. https://doi.org/10.1101/2024.04.15.\n24305869 (2024).\n44. Tam, T. Y. C. et al. A framework for human evaluation of large language\nmodels in healthcare derived from literature review.NPJ Digit. Med.7,\n258 (2024).\n45. Wang, Y., Song, Y., Wang, Y., Lian, Y. & Wang, J. Ethics and\ngovernance of artiﬁcial intelligence for health: guidance on large multi-\nmodal models.Chin. Med. Ethics37, 1001– 1022 (2024).\n46. Coalition for Health AI. Blueprint for trustworthy AI implementation\nguidance and assurance for healthcare.https://chai.org/wp-\ncontent/uploads/2024/05/blueprint- for-trustworthy-ai_V1.0-2.pdf\n(2023).\n47. Tang, R., Han, X., Jiang, X. & Hu, X. Does synthetic data generation of\nLLMs help clinical text mining?arXiv preprint arXiv:2303.04360(2023).\n48. Wang, Z., Yuan, F., LeBaron, V., Flickinger, T. & Barnes, L. E. Pallm:\nEvaluating and enhancing palliative care conversations with large\nlanguage models.ACM Trans. Comput. Healthcarehttps://doi.org/\n10.1145/3712300 (2025).\n49. Ganguli, D. et al. Red teaming language models to reduce harms:\nMethods, scaling behaviors, and lessons learned.arXiv preprint\narXiv:2209.07858 (2022).\n50. Chang, C. T. et al. Red Teaming Large Language Models in Medicine:\nReal-World Insights on Model Behavior. medRxiv\n2024.04.05.24305411. https://doi.org/10.1101/2024.04.05.\n24305411 (2024).\n51. Amershi, S., Cakmak, M., Knox, W. B. & Kulesza, T. Power to the\npeople: The role of humans in interactive machine learning.AI Mag.\n35, 105– 120 (2014).\n52. Liang, P. P., Wu, C., Morency, L.-P. & Salakhutdinov, R. Towards\nunderstanding and mitigating social biases in language models. In\nInternational Conference on Machine Learning, 6565– 6576 (PMLR,\n2021).\n53. Kitson, A. L. et al. Evaluating the successful implementation of\nevidence into practice using the PARIHS framework: theoretical and\npractical challenges.Implement. Sci.3,1 – 12 (2008).\n54. Hengle, A. et al. Still not quite there! Evaluating large language models\nfor comorbid mental health diagnosis. InProceedings of the 2024\nConference on Empirical Methods in Natural Language Processing,\n16698– 16721 (2024).\n55. Li, J. et al. Agent hospital: A simulacrum of the hospital with evolvable\nmedical agents.arXiv preprint arXiv:2405.02957(2024).\nhttps://doi.org/10.1038/s44401-025-00024-5 Perspective\nnpj Health Systems|            (2025) 2:22 7\n56. Kwan, H. Y. User-focused telehealth powered by LLMs: Bridging the\ngap between technology and human-centric care delivery. In2024 4th\nInternational Conference on Computer Communication and Artiﬁcial\nIntelligence (CCAI), 187– 191 (IEEE, 2024).\n57. Goyal, S. et al. Healai: A healthcare LLM for effective medical\ndocumentation. InProceedings of the 17th ACM International\nConference on Web Search and Data Mining, 1167– 1168 (2024).\n58. Yang, B. et al. DrHouse: An LLM-empowered diagnostic reasoning\nsystem through harnessing outcomes from sensor data and expert\nknowledge. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.\n8,1 – 29 (2024).\n59. Feng, J., Shaib, C. & Rudzicz, F. Explainable clinical decision support\nfrom text. InProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 1478– 1489\n(2020).\n60. Pareek, S., van Berkel, N., Velloso, E. & Goncalves, J. Effect of\nexplanation conceptualisations on trust in AI-assisted credibility\nassessment. Proc. ACM Hum.-Comput. Interact.8, https://doi.org/\n10.1145/3686922 (2024).\n61. Holzinger, A., Langs, G., Denk, H., Zatloukal, K. & Müller, H.\nCausability and explainability of artiﬁcial intelligence in medicine.\nWiley Interdiscip. Rev.: Data Min. Knowl. Discov.9, e1312 (2019).\n62. Heer, J. & Bostock, M. Declarative language design for interactive\nvisualization. IEEE Trans. Vis. Computer Graph.16, 1149– 1156 (2010).\n63. Kung, T. H. et al. Performance of chatgpt on usmle: potential for ai-\nassisted medical education using large language models.PLoS Digit.\nHealth 2, e0000198 (2023).\n64. Ouyang, L. et al. Training language models to follow instructions with\nhuman feedback.Adv. Neural Inf. Process. Syst.35, 27730– 27744\n(2022).\n65. Bai, Y. et al. Training a helpful and harmless assistant with\nreinforcement learning from human feedback.arXiv preprint\narXiv:2204.05862 (2022).\n66. Liu, H. et al. Few-shot parameter-efﬁcient ﬁne-tuning is better and\ncheaper than in-context learning.Adv. Neural Inf. Process. Syst.35,\n1950– 1965 (2022).\n67. Nerur, S. & Balijepally, V. Theoretical reﬂections on agile development\nmethodologies. Commun. ACM50,7 9– 83 (2007).\n68. Wang, Z. et al. Commsense: A wearable sensing computational\nframework for evaluating patient-clinician interactions.Proc. ACM\nHum.-Comput. Interact.8,1 –\n31 (2024).\n69. LeBaron, V. et al. Exploring the use of wearable sensors and natural\nlanguage processing technology to improve patient-clinician\ncommunication: protocol for a feasibility study.JMIR Res. Protoc.11,\ne37975 (2022).\n70. LeBaron, V. et al. Feasibility and acceptability testing of commsense: A\nnovel communication technology to enhance health equity in\nclinician–patient interactions.Digit. Health9, 20552076231184991 (2023).\n71. Allen-Meares, P., Lowry, B., Estrella, M. L. & Mansuri, S. Health\nliteracy barriers in the health care system: barriers and opportunities\nfor the profession.Health Soc. Work45,6 2– 64 (2020).\n72. Lin, Y. et al. A web-based dyadic intervention to manage\npsychoneurological symptoms for patients with colorectal cancer and\ntheir caregivers: Protocol for a mixed methods study.JMIR Res.\nProtoc. 12, e48499 (2023).\n73. Lin, Y. et al. Development of a technology-based dyadic intervention\nfor underserved colorectal cancer patients and caregivers.Stud.\nHealth Technol. Inform.315, 721– 722 (2024).\nAcknowledgements\nThis work was supported in part by the Gordon and Betty Moore Foundation\nand the National Institute of Mental Health of the National Institutes of Health\nunder award number R01MH132138.\nAuthor contributions\nZ.W. conceived the study idea, developed the conceptual framework, wrote\nthe main manuscript, and prepared theﬁgures. R.Y. contributed to the study\nidea and initial drafting and revised the draft, including reﬁning theﬁgures.\nS.F., C.D., T.F., and Y.L. provided original input and editorial revisions and\nreviewed the manuscript. X.H., L.B., and V.L. supervised the research and\nmanuscript development, ensuring alignment with the study’so b j e c t i v e s\nand overseeing itsﬁnalization. All authors reviewed and approved theﬁnal\nversion of the manuscript for submission.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondenceand requests for materials should be addressed to\nZhiyuan Wang.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s44401-025-00024-5 Perspective\nnpj Health Systems|            (2025) 2:22 8",
  "topic": "Stakeholder",
  "concepts": [
    {
      "name": "Stakeholder",
      "score": 0.6063982844352722
    },
    {
      "name": "Computer science",
      "score": 0.45751234889030457
    },
    {
      "name": "Business",
      "score": 0.4125460982322693
    },
    {
      "name": "Political science",
      "score": 0.24510586261749268
    },
    {
      "name": "Public relations",
      "score": 0.2002096176147461
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I51556381",
      "name": "University of Virginia",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I150468666",
      "name": "Emory University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 2
}