{
  "title": "A survey on large language model based autonomous agents",
  "url": "https://openalex.org/W4393065402",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101394605",
      "name": "Lei Wang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5100410908",
      "name": "Chen Ma",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5101929118",
      "name": "Xueyang Feng",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5100358735",
      "name": "Zeyu Zhang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5101551028",
      "name": "Hao Yang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5101658564",
      "name": "Jingsen Zhang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5070590692",
      "name": "Zhiyuan Chen",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5043446092",
      "name": "Jiakai Tang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5100385692",
      "name": "Xu Chen",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5043098453",
      "name": "Yankai Lin",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5037145565",
      "name": "Wayne Xin Zhao",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5074858555",
      "name": "Zhewei Wei",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5025631695",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2145339207",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4385571775",
    "https://openalex.org/W6600075759",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4386200967",
    "https://openalex.org/W2046759284",
    "https://openalex.org/W4238095402",
    "https://openalex.org/W4389523771",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4387394580",
    "https://openalex.org/W4387993371",
    "https://openalex.org/W2602856279",
    "https://openalex.org/W4401042703",
    "https://openalex.org/W4393160302",
    "https://openalex.org/W4389667254",
    "https://openalex.org/W4389520747",
    "https://openalex.org/W4390874280",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4389518771",
    "https://openalex.org/W4389518608",
    "https://openalex.org/W4388626886",
    "https://openalex.org/W4390872747",
    "https://openalex.org/W4388744821",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W6600669965",
    "https://openalex.org/W2084335597",
    "https://openalex.org/W6602838585",
    "https://openalex.org/W4385567201",
    "https://openalex.org/W4389544300",
    "https://openalex.org/W4307475457",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W4363624465",
    "https://openalex.org/W4390772308",
    "https://openalex.org/W4403275638",
    "https://openalex.org/W4376504550",
    "https://openalex.org/W4289523162",
    "https://openalex.org/W6798182279",
    "https://openalex.org/W4391584331",
    "https://openalex.org/W4388040387",
    "https://openalex.org/W4387567460",
    "https://openalex.org/W4390481193",
    "https://openalex.org/W4389667233",
    "https://openalex.org/W4380763235",
    "https://openalex.org/W4393147219",
    "https://openalex.org/W4296343374",
    "https://openalex.org/W4389519042",
    "https://openalex.org/W4384345748",
    "https://openalex.org/W4378676756",
    "https://openalex.org/W4388483618",
    "https://openalex.org/W4389519250",
    "https://openalex.org/W4389524373",
    "https://openalex.org/W4392240262",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W4388691863",
    "https://openalex.org/W4386566629",
    "https://openalex.org/W4367165010",
    "https://openalex.org/W4309674289"
  ],
  "abstract": "Abstract Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.",
  "full_text": "A survey on large language model based autonomous agents\nLei WANG, Chen MA*, Xueyang FENG*, Zeyu ZHANG, Hao YANG, Jingsen ZHANG,\nZhiyuan CHEN, Jiakai TANG, Xu CHEN (✉), Yankai LIN (✉),\nWayne Xin ZHAO, Zhewei WEI, Jirong WEN\nGaoling School of Artificial Intelligence, Renmin University of China, Beijing 100872, China\n The Author(s) 2024. This article is published with open access at link.springer.com and journal.hep.com.cn\n \nAbstract    Autonomous  agents  have  long  been  a  research\nfocus in academic and industry communities. Previous research\noften focuses on training agents with limited knowledge within\nisolated  environments,  which  diverges  significantly  from\nhuman  learning  processes,  and  makes  the  agents  hard  to\nachieve human-like decisions. Recently, through the acquisition\nof  vast  amounts  of  Web  knowledge,  large  language  models\n(LLMs)  have  shown  potential  in  human-level  intelligence,\nleading  to  a  surge  in  research  on  LLM-based  autonomous\nagents.  In  this  paper,  we  present  a  comprehensive  survey  of\nthese  studies,  delivering  a  systematic  review  of  LLM-based\nautonomous agents from a holistic perspective. We first discuss\nthe construction of LLM-based autonomous agents, proposing a\nunified  framework  that  encompasses  much  of  previous  work.\nThen,  we  present  a  overview  of  the  diverse  applications  of\nLLM-based  autonomous  agents  in  social  science,  natural\nscience, and engineering. Finally, we delve into the evaluation\nstrategies commonly used for LLM-based autonomous agents.\nBased  on  the  previous  studies,  we  also  present  several\nchallenges and future directions in this field.\nKeywords    autonomous  agent, large  language  model,\nhuman-level intelligence\n 1    Introduction\n“An autonomous agent is a system situated within and a part\nof an environment that senses that environment and acts on it,\nover time, in pursuit of its own agenda and so as to effect what\nit senses in the future.”\nFranklin and Graesser (1997)\nAutonomous  agents  have  long  been  recognized  as  a\npromising approach to achieving artificial general intelligence\n(AGI),  which  is  expected  to  accomplish  tasks  through  self-\ndirected planning and actions. In previous studies, the agents\nare  assumed  to  act  based  on  simple  and  heuristic  policy\nfunctions, and learned in isolated and restricted environments\n[1–6]. Such assumptions significantly differs from the human\nlearning  process,  since  the  human  mind  is  highly  complex,\nand  individuals  can  learn  from  a  much  wider  variety  of\nenvironments. Because of these gaps, the agents obtained from\nthe  previous  studies  are  usually  far  from  replicating  human-\nlevel  decision  processes,  especially  in  unconstrained,  open-\ndomain settings.\nIn  recent  years,  large  language  models  (LLMs)  have\nachieved  notable  successes,  demonstrating  significant\npotential  in  attaining  human-like  intelligence  [5–10].  This\ncapability  arises  from  leveraging  comprehensive  training\ndatasets alongside a substantial number of model parameters.\nBuilding  upon  this  capability,  there  has  been  a  growing\nresearch  area  that  employs  LLMs  as  central  controllers  to\nconstruct  autonomous  agents  to  obtain  human-like  decision-\nmaking capabilities [11–17].\nComparing with reinforcement learning, LLM-based agents\nhave  more  comprehensive  internal  world  knowledge,  which\nfacilitates more informed agent actions even without training\non specific domain data. Additionally, LLM-based agents can\nprovide  natural  language  interfaces  to  interact  with  humans,\nwhich is more flexible and explainable.\nAlong this direction, researchers have developed numerous\npromising  models  (see Fig. 1 for  an  overview  of  this  field),\nwhere  the  key  idea  is  to  equip  LLMs  with  crucial  human\ncapabilities  like  memory  and  planning  to  make  them  behave\nlike  humans  and  complete  various  tasks  effectively.\nPreviously,  these  models  were  proposed  independently,  with\nlimited  efforts  made  to  summarize  and  compare  them\nholistically.  However,  we  believe  a  systematic  summary  on\nthis  rapidly  developing  field  is  of  great  significance  to\ncomprehensively  understand  it  and  benefit  to  inspire  future\nresearch.\nIn  this  paper,  we  conduct  a  comprehensive  survey  of  the\nfield  of  LLM-based  autonomous  agents.  Specifically,  we\norganize  our  survey  based  on  three  aspects  including  the\nconstruction,  application,  and  evaluation  of  LLM-based\nautonomous  agents.  For  the  agent  construction,  we  focus  on\ntwo problems, that is, (1) how to design the agent architecture\nto better leverage LLMs, and (2) how to inspire and enhance\nthe agent capability to complete different tasks. Intuitively, the\nfirst problem aims to build the hardware fundamentals for the\nagent, while the second problem focus on providing the agent\n \nReceived March 5, 2024; accepted March 10, 2024\nE-mail: xu.chen@ruc.edu.cn; yankailin@ruc.edu.cn\n* These authors contributed equally to this work.\nFront. Comput. Sci., 2024, 18(6): 186345\nhttps://doi.org/10.1007/s11704-024-40231-1\nREVIEW ARTICLE\nwith  software  resources.  For  the  first  problem,  we  present  a\nunified  agent  framework,  which  can  encompass  most  of  the\nprevious  studies.  For  the  second  problem,  we  provide  a\nsummary  on  the  commonly-used  strategies  for  agents’\ncapability  acquisition.  In  addition  to  discussing  agent\nconstruction,  we  also  provide  an  systematic  overview  of  the\napplications  of  LLM-based  autonomous  agents  in  social\nscience,  natural  science,  and  engineering.  Finally,  we  delve\ninto  the  strategies  for  evaluating  LLM-based  autonomous\nagents, focusing on both subjective and objective strategies.\nIn  summary,  this  survey  conducts  a  systematic  review  and\nestablishes  comprehensive  taxonomies  for  existing  studies  in\nthe  burgeoning  field  of  LLM-based  autonomous  agents.  Our\nfocus  encompasses  three  primary  areas:  construction  of\nagents, their applications, and methods of evaluation. Drawing\nfrom  a  wealth  of  previous  studies,  we  identify  various\nchallenges in this field and discuss potential future directions.\nWe  expect  that  our  survey  can  provide  newcomers  of  LLM-\nbased  autonomous  agents  with  a  comprehensive  background\nknowledge,  and  also  encourage  further  groundbreaking\nstudies.\n 2    LLM-based autonomous agent\nconstruction\nLLM-based  autonomous  agents  are  expected  to  effectively\nperform  diverse  tasks  by  leveraging  the  human-like\ncapabilities of LLMs. In order to achieve this goal, there are\ntwo significant aspects, that is, (1) which architecture should\nbe  designed  to  better  use  LLMs  and  (2)  give  the  designed\narchitecture, how to enable the agent to acquire capabilities for\naccomplishing  specific  tasks.  Within  the  context  of\narchitecture  design,  we  contribute  a  systematic  synthesis  of\nexisting  research,  culminating  in  a  comprehensive  unified\nframework.  As  for  the  second  aspect,  we  summarize  the\nstrategies  for  agent  capability  acquisition  based  on  whether\nthey  fine-tune  the  LLMs.  When  comparing  LLM-based\nautonomous agents to traditional machine learning, designing\nthe agent architecture is analogous to determining the network\nstructure,  while  the  agent  capability  acquisition  is  similar  to\nlearning  the  network  parameters.  In  the  following,  we\nintroduce these two aspects more in detail.\n 2.1    Agent architecture design\nRecent advancements in LLMs have demonstrated their great\npotential  to  accomplish  a  wide  range  of  tasks  in  the  form  of\nquestion-answering  (QA).  However,  building  autonomous\nagents is far from QA, since they need to fulfill specific roles\nand autonomously perceive and learn from the environment to\nevolve  themselves  like  humans.  To  bridge  the  gap  between\ntraditional LLMs and autonomous agents, a crucial aspect is to\ndesign  rational  agent  architectures  to  assist  LLMs  in\nmaximizing  their  capabilities.  Along  this  direction,  previous\nwork has developed a number of modules to enhance LLMs.\nIn this section, we propose a unified framework to summarize\nthese  modules.  In  specific,  the  overall  structure  of  our\nframework  is  illustrated Fig. 2,  which  is  composed  of  a\nprofiling module, a memory module, a planning module, and\nan  action  module.  The  purpose  of  the  profiling  module  is  to\nidentify  the  role  of  the  agent.  The  memory  and  planning\nmodules place the agent into a dynamic environment, enabling\nit to recall past behaviors and plan future actions. The action\nmodule is responsible for translating the agent’s decisions into\nspecific  outputs.  Within  these  modules,  the  profiling  module\nimpacts  the  memory  and  planning  modules,  and  collectively,\nthese  three  modules  influence  the  action  module.  In  the\nfollowing, we detail these modules.\n 2.1.1    Profiling module\nAutonomous  agents  typically  perform  tasks  by  assuming\nspecific  roles,  such  as  coders,  teachers  and  domain  experts\n[18,19]. The profiling module aims to indicate the profiles of\nthe  agent  roles,  which  are  usually  written  into  the  prompt  to\n \n \nFig. 1    Illustration of the growth trend in the field of LLM-based autonomous agents. We present the cumulative number of papers published\nfrom January 2021 to August 2023. We assign different colors to represent various agent categories. For example, a game agent aims to simulate\na game-player, while a tool agent mainly focuses on tool using. For each time period, we provide a curated list of studies with diverse agent\ncategories\n \n2 Front. Comput. Sci., 2024, 18(6): 186345\ninfluence  the  LLM  behaviors.  Agent  profiles  typically\nencompass basic information such as age, gender, and career\n[20],  as  well  as  psychology  information,  reflecting  the\npersonalities of the agent, and social information, detailing the\nrelationships between agents [21]. The choice of information\nto  profile  the  agent  is  largely  determined  by  the  specific\napplication scenarios. For instance, if the application aims to\nstudy  human  cognitive  process,  then  the  psychology\ninformation  becomes  pivotal.  After  identifying  the  types  of\nprofile  information,  the  next  important  problem  is  to  create\nspecific  profiles  for  the  agents.  Existing  literature  commonly\nemploys the following three strategies.\nHandcrafting  method:  in  this  method,  agent  profiles  are\nmanually specified. For instance, if one would like to design\nagents  with  different  personalities,  he  can  use “you  are  an\noutgoing person” or “you are an introverted person” to profile\nthe agent. The handcrafting method has been leveraged in a lot\nof previous work to indicate the agent profiles. For example,\nGenerative Agent [22] describes the agent by the information\nlike  name,  objectives,  and  relationships  with  other  agents.\nMetaGPT  [23],  ChatDev  [18],  and  Self-collaboration  [24]\npredefine  various  roles  and  their  corresponding\nresponsibilities  in  software  development,  manually  assigning\ndistinct  profiles  to  each  agent  to  facilitate  collaboration.\nPTLLM  [25]  aims  to  explore  and  quantify  personality  traits\ndisplayed  in  texts  generated  by  LLMs.  This  method  guides\nLLMs  in  generating  diverse  responses  by  manfully  defining\nvarious  agent  characters  through  the  use  of  personality\nassessment  tools  such  as  IPIP-NEO  [26]  and  BFI  [27].  [28]\nstudies the toxicity of the LLM output by manually prompting\nLLMs with different roles, such as politicians, journalists and\nbusinesspersons.  In  general,  the  handcrafting  method  is  very\nflexible,  since  one  can  assign  any  profile  information  to  the\nagents.  However,  it  can  be  also  labor-intensive,  particularly\nwhen dealing with a large number of agents.\nLLM-generation method: in this method, agent profiles are\nautomatically  generated  based  on  LLMs.  Typically,  it  begins\nby  indicating  the  profile  generation  rules,  elucidating  the\ncomposition  and  attributes  of  the  agent  profiles  within  the\ntarget  population.  Then,  one  can  optionally  specify  several\nseed  agent  profiles  to  serve  as  few-shot  examples.  At  last,\nLLMs  are  leveraged  to  generate  all  the  agent  profiles.  For\nexample,  RecAgent  [21]  first  creates  seed  profiles  for  a  few\nnumber of agents by manually crafting their backgrounds like\nage,  gender,  personal  traits,  and  movie  preferences.  Then,  it\nleverages ChatGPT to generate more agent profiles based on\nthe  seed  information.  The  LLM-generation  method  can  save\nsignificant time when the number of agents is large, but it may\nlack precise control over the generated profiles.\nDataset  alignment  method:  in  this  method,  the  agent\nprofiles are obtained from real-world datasets. Typically, one\ncan  first  organize  the  information  about  real  humans  in  the\ndatasets into natural language prompts, and then leverage it to\nprofile  the  agents.  For  instance,  in  [29],  the  authors  assign\nroles to GPT-3 based on the demographic backgrounds (such\nas  race/ethnicity,  gender,  age,  and  state  of  residence)  of\nparticipants  in  the  American  National  Election  Studies\n(ANES).  They  subsequently  investigate  whether  GPT-3  can\nproduce  similar  results  to  those  of  real  humans.  The  dataset\nalignment method accurately captures the attributes of the real\npopulation,  thereby  making  the  agent  behaviors  more\nmeaningful and reflective of real-world scenarios.\nRemark. While most of the previous work leverage the above\nprofile  generation  strategies  independently,  we  argue  that\ncombining  them  may  yield  additional  benefits.  For  example,\nin  order  to  predict  social  developments  via  agent  simulation,\none can leverage real-world datasets to profile a subset of the\nagents, thereby accurately reflecting the current social status.\nSubsequently, roles that do not exist in the real world but may\nemerge  in  the  future  can  be  manually  assigned  to  the  other\nagents,  enabling  the  prediction  of  future  social  development.\nBeyond this example, one can also flexibly combine the other\nstrategies.  The  profile  module  serves  as  the  foundation  for\nagent  design,  exerting  significant  influence  on  the  agent\nmemorization, planning, and action procedures.\n \n \nFig. 2    A unified framework for the architecture design of LLM-based autonomous agent\n \nLei WANG et al.    A survey on large language model based autonomous agents 3\n 2.1.2    Memory module\nThe memory module plays a very important role in the agent\narchitecture  design.  It  stores  information  perceived  from  the\nenvironment and leverages the recorded memories to facilitate\nfuture  actions.  The  memory  module  can  help  the  agent  to\naccumulate  experiences,  self-evolve,  and  behave  in  a  more\nconsistent,  reasonable,  and  effective  manner.  This  section\nprovides  a  comprehensive  overview  of  the  memory  module,\nfocusing on its structures, formats, and operations.\nMemory  structures:  LLM-based  autonomous  agents\nusually  incorporate  principles  and  mechanisms  derived  from\ncognitive  science  research  on  human  memory  processes.\nHuman  memory  follows  a  general  progression  from  sensory\nmemory that registers perceptual inputs, to short-term memory\nthat  maintains  information  transiently,  to  long-term  memory\nthat  consolidates  information  over  extended  periods.  When\ndesigning  the  agent  memory  structures,  researchers  take\ninspiration from these aspects of human memory. In specific,\nshort-term  memory  is  analogous  to  the  input  information\nwithin  the  context  window  constrained  by  the  transformer\narchitecture. Long-term memory resembles the external vector\nstorage  that  agents  can  rapidly  query  and  retrieve  from  as\nneeded.  In  the  following,  we  introduce  two  commonly  used\nmemory  structures  based  on  the  short-  and  long-term\nmemories.\n● Unified memory. This structure only simulates the human\nshot-term  memory,  which  is  usually  realized  by  in-context\nlearning, and the memory information is directly written into\nthe prompts. For example, RLP [30] is a conversation agent,\nwhich  maintains  internal  states  for  the  speaker  and  listener.\nDuring each round of conversation, these states serve as LLM\nprompts,  functioning  as  the  agent’s  short-term  memory.\nSayPlan  [31]  is  an  embodied  agent  specifically  designed  for\ntask planning. In this agent, the scene graphs and environment\nfeedback serve as the agent’s short-term memory, guiding its\nactions.  CALYPSO  [32]  is  an  agent  designed  for  the  game\nDungeons  &  Dragons,  which  can  assist  Dungeon  Masters  in\nthe creation and narration of stories. Its short-term memory is\nbuilt  upon  scene  descriptions,  monster  information,  and\nprevious summaries. DEPS [33] is also a game agent, but it is\ndeveloped  for  Minecraft.  The  agent  initially  generates  task\nplans  and  then  utilizes  them  to  prompt  LLMs,  which  in  turn\nproduce  actions  to  complete  the  task.  These  plans  can  be\ndeemed  as  the  agent’s  short-term  memory.  In  practice,\nimplementing  short-term  memory  is  straightforward  and  can\nenhance  an  agent’s  ability  to  perceive  recent  or  contextually\nsensitive  behaviors  and  observations.  However,  due  to  the\nlimitation  of  context  window  of  LLMs,  it’s  hard  to  put  all\nmemories into prompt, which may degrade the performance of\nagents.  This  method  has  high  requirements  on  the  window\nlength  of  LLMs  and  the  ability  to  handle  long  contexts.\nTherefore,  many  researchers  resort  to  hybrid  memory  to\nalleviate  this  question.  However,  the  limited  context  window\nof LLMs restricts incorporating comprehensive memories into\nprompts, which can impair agent performance. This challenge\nnecessitates LLMs with larger context windows and the ability\nto  handle  extended  contexts.  Consequently,  numerous\nresearchers  turn  to  hybrid  memory  systems  to  mitigate  this\nissue.\n●  Hybrid  memory.  This  structure  explicitly  models  the\nhuman  short-term  and  long-term  memories.  The  short-term\nmemory  temporarily  buffers  recent  perceptions,  while  long-\nterm  memory  consolidates  important  information  over  time.\nFor  instance,  Generative  Agent  [20]  employs  a  hybrid\nmemory structure to facilitate agent behaviors. The short-term\nmemory  contains  the  context  information  about  the  agent\ncurrent  situations,  while  the  long-term  memory  stores  the\nagent  past  behaviors  and  thoughts,  which  can  be  retrieved\naccording  to  the  current  events.  AgentSims  [34]  also\nimplements  a  hybrid  memory  architecture.  The  information\nprovided  in  the  prompt  can  be  considered  as  short-term\nmemory. In order to enhance the storage capacity of memory,\nthe authors propose a long-term memory system that utilizes a\nvector  database,  facilitating  efficient  storage  and  retrieval.\nSpecifically,  the  agent’s  daily  memories  are  encoded  as\nembeddings  and  stored  in  the  vector  database.  If  the  agent\nneeds to recall its previous memories, the long-term memory\nsystem  retrieves  relevant  information  using  embedding\nsimilarities.  This  process  can  improve  the  consistency  of  the\nagent’s behavior. In GITM [16], the short-term memory stores\nthe  current  trajectory,  and  the  long-term  memory  saves\nreference plans summarized from successful prior trajectories.\nLong-term  memory  provides  stable  knowledge,  while  short-\nterm memory allows flexible planning. Reflexion [12] utilizes\na  short-term  sliding  window  to  capture  recent  feedback  and\nincorporates  persistent  long-term  storage  to  retain  condensed\ninsights.  This  combination  allows  for  the  utilization  of  both\ndetailed  immediate  experiences  and  high-level  abstractions.\nSCM  [35]  selectively  activates  the  most  relevant  long-term\nknowledge  to  combine  with  short-term  memory,  enabling\nreasoning over complex contextual dialogues. SimplyRetrieve\n[36]  utilizes  user  queries  as  short-term  memory  and  stores\nlong-term  memory  using  external  knowledge  bases.  This\ndesign  enhances  the  model  accuracy  while  guaranteeing  user\nprivacy.  MemorySandbox  [37]  implements  long-term  and\nshort-term memory by utilizing a 2D canvas to store memory\nobjects,  which  can  then  be  accessed  throughout  various\nconversations.  Users  can  create  multiple  conversations  with\ndifferent agents on the same canvas, facilitating the sharing of\nmemory objects through a simple drag-and-drop interface. In\npractice, integrating both short-term and long-term memories\ncan  enhance  an  agent’s  ability  for  long-range  reasoning  and\naccumulation  of  valuable  experiences,  which  are  crucial  for\naccomplishing tasks in complex environments.\nRemark.  Careful  readers  may  find  that  there  may  also  exist\nanother  type  of  memory  structure,  that  is,  only  based  on  the\nlong-term  memory.  However,  we  find  that  such  type  of\nmemory is rarely documented in the literature. Our speculation\nis  that  the  agents  are  always  situated  in  continuous  and\ndynamic environments, with consecutive actions displaying a\nhigh correlation. Therefore, the capture of short-term memory\nis very important and usually cannot be disregarded.\nMemory  formats:  In  addition  to  the  memory  structure,\nanother perspective to analyze the memory module is based on\nthe  formats  of  the  memory  storage  medium,  for  example,\n4 Front. Comput. Sci., 2024, 18(6): 186345\nnatural  language  memory  or  embedding  memory.  Different\nmemory formats possess distinct strengths and are suitable for\nvarious  applications.  In  the  following,  we  introduce  several\nrepresentative memory formats.\n●  Natural  languages.  In  this  format,  memory  information\nsuch  as  the  agent  behaviors  and  observations  are  directly\ndescribed  using  raw  natural  language.  This  format  possesses\nseveral  strengths.  Firstly,  the  memory  information  can  be\nexpressed in a flexible and understandable manner. Moreover,\nit  retains  rich  semantic  information  that  can  provide\ncomprehensive  signals  to  guide  agent  behaviors.  In  the\nprevious work, Reflexion [12] stores experiential feedback in\nnatural  language  within  a  sliding  window.  Voyager  [38]\nemploys  natural  language  descriptions  to  represent  skills\nwithin  the  Minecraft  game,  which  are  directly  stored  in\nmemory.\n●  Embeddings.  In  this  format,  memory  information  is\nencoded  into  embedding  vectors,  which  can  enhance  the\nmemory  retrieval  and  reading  efficiency.  For  instance,\nMemoryBank  [39]  encodes  each  memory  segment  into  an\nembedding  vector,  which  creates  an  indexed  corpus  for\nretrieval.  [16]  represents  reference  plans  as  embeddings  to\nfacilitate  matching  and  reuse.  Furthermore,  ChatDev  [18]\nencodes dialogue history into vectors for retrieval.\n● Databases. In this format, memory information is stored in\ndatabases,  allowing  the  agent  to  manipulate  memories\nefficiently  and  comprehensively.  For  example,  ChatDB  [40]\nuses a database as a symbolic memory module. The agent can\nutilize SQL statements to precisely add, delete, and revise the\nmemory information. In DB-GPT [41], the memory module is\nconstructed based on a database. To more intuitively operate\nthe  memory  information,  the  agents  are  fine-tuned  to\nunderstand and execute SQL queries, enabling them to interact\nwith databases using natural language directly.\n●  Structured  lists.  In  this  format,  memory  information  is\norganized  into  lists,  and  the  semantic  of  memory  can  be\nconveyed  in  an  efficient  and  concise  manner.  For  instance,\nGITM  [16]  stores  action  lists  for  sub-goals  in  a  hierarchical\ntree structure. The hierarchical structure explicitly captures the\nrelationships  between  goals  and  corresponding  plans.  RET-\nLLM  [42]  initially  converts  natural  language  sentences  into\ntriplet phrases, and subsequently stores them in memory.\nRemark.  Here  we  only  show  several  representative  memory\nformats,  but  it  is  important  to  note  that  there  are  many\nuncovered ones, such as the programming code used by [38].\nMoreover, it should be emphasized that these formats are not\nmutually exclusive; many models incorporate multiple formats\nto  concurrently  harness  their  respective  benefits.  A  notable\nexample is the memory module of GITM [16], which utilizes\na  key-value  list  structure.  In  this  structure,  the  keys  are\nrepresented by embedding vectors, while the values consist of\nraw  natural  languages.  The  use  of  embedding  vectors  allows\nfor efficient retrieval of memory records. By utilizing natural\nlanguages,  the  memory  contents  become  highly\ncomprehensive, enabling more informed agent actions.\nAbove,  we  mainly  discuss  the  internal  designs  of  the\nmemory  module.  In  the  following,  we  turn  our  focus  to\nmemory  operations,  which  are  used  to  interact  with  external\nenvironments.\nMemory  operations:  The  memory  module  plays  a  critical\nrole in allowing the agent to acquire, accumulate, and utilize\nsignificant  knowledge  by  interacting  with  the  environment.\nThe  interaction  between  the  agent  and  the  environment  is\naccomplished  through  three  crucial  memory  operations:\nmemory reading, memory writing, and memory reflection. In\nthe following, we introduce these operations more in detail.\n● Memory reading. The objective of memory reading is to\nextract  meaningful  information  from  memory  to  enhance  the\nagent’s actions. For example, using the previously successful\nactions  to  achieve  similar  goals  [16].  The  key  of  memory\nreading  lies  in  how  to  extract  valuable  information  from\nhistory  actions.  Usually,  there  three  commonly  used  criteria\nfor information extraction, that is, the recency, relevance, and\nimportance [20]. Memories that are more recent, relevant, and\nimportant  are  more  likely  to  be  extracted.  Formally,  we\nconclude  the  following  equation  from  existing  literature  for\nmemory information extraction:\n \nm\u0003 = ar\ng min\nm2M\n\u000bsrec(q; m) +\fsrel(q; m) +\rsimp (m); (1)\nq\nM\nsr\nec(\u0001)\nsr\nel(\u0001)\nsim\np(\u0001)\nm\nsr\nel(q; m)\nsim\np\nq\n\u000b\n\f\n\r\n\u000b = \r = 0\nsr\nel\n\u000b = \f = \r = 1:0\nwhere  is  the  query,  for  example,  the  task  that  the  agent\nshould address or the context in which the agent is situated. \nis  the  set  of  all  memories. ,  ,  and  are  the\nscoring  functions  for  measuring  the  recency,  relevance,  and\nimportance of the memory .  These scoring functions can be\nimplemented  using  various  methods,  for  example, \ncan be realized based on LSH, ANNOY, HNSW, FAISS, and\nso on. It should be noted that  only reflects the characters\nof the memory itself, thus it is unrelated to the query .  , ,\nand  are  balancing  parameters.  By  assigning  them  with\ndifferent  values,  one  can  obtain  various  memory  reading\nstrategies.  For  example,  by  setting ,\n  many  studies\n[16,30,38,42]  only  consider  the  relevance  score  for\nmemory  reading.  By  assigning ,  [20]  equally\nweights all the above three metrics to extract information from\nmemory.\n●  Memory  writing.  The  purpose  of  memory  writing  is  to\nstore information about the perceived environment in memory.\nStoring valuable information in memory provides a foundation\nfor retrieving informative memories in the future, enabling the\nagent  to  act  more  efficiently  and  rationally.  During  the\nmemory writing process, there are two potential problems that\nshould  be  carefully  addressed.  On  one  hand,  it  is  crucial  to\naddress  how  to  store  information  that  is  similar  to  existing\nmemories  (i.e.,  memory  duplicated).  On  the  other  hand,  it  is\nimportant  to  consider  how  to  remove  information  when  the\nmemory  reaches  its  storage  limit  (i.e.,  memory  overflow).  In\nthe  following,  we  discuss  these  problems  more  in  detail.\n(1)  Memory  duplicated.  To  incorporate  similar  information,\npeople  have  developed  various  methods  for  integrating  new\nand  previous  records.  For  instance,  in  [7],  the  successful\naction sequences related to the same sub-goal are stored in a\nlist. Once the size of the list reaches N(=5), all the sequences\nin  it  are  condensed  into  a  unified  plan  solution  using  LLMs.\nThe  original  sequences  in  the  memory  are  replaced  with  the\nnewly  generated  one.  Augmented  LLM  [43]  aggregates\nduplicate  information  via  count  accumulation,  avoiding\nLei WANG et al.    A survey on large language model based autonomous agents 5\nredundant  storage.  (2)  Memory  overflow.  In  order  to  write\ninformation  into  the  memory  when  it  is  full,  people  design\ndifferent  methods  to  delete  existing  information  to  continue\nthe  memorizing  process.  For  example,  in  ChatDB  [40],\nmemories can be explicitly deleted based on user commands.\nRET-LLM  [42]  uses  a  fixed-size  buffer  for  memory,\noverwriting  the  oldest  entries  in  a  first-in-first-out  (FIFO)\nmanner.\n● Memory reflection. Memory reflection emulates humans’\nability to witness and evaluate their own cognitive, emotional,\nand  behavioral  processes.  When  adapted  to  agents,  the\nobjective  is  to  provide  agents  with  the  capability  to\nindependently  summarize  and  infer  more  abstract,  complex\nand  high-level  information.  More  specifically,  in  Generative\nAgent [20], the agent has the capability to summarize its past\nexperiences stored in memory into broader and more abstract\ninsights.  To  begin  with,  the  agent  generates  three  key\nquestions based on its recent memories. Then, these questions\nare used to query the memory to obtain relevant information.\nBuilding  upon  the  acquired  information,  the  agent  generates\nfive  insights,  which  reflect  the  agent  high-level  ideas.  For\nexample, the low-level memories “Klaus Mueller is writing a\nresearch paper”, “Klaus Mueller is engaging with a librarian to\nfurther  his  research”,  and “Klaus  Mueller  is  conversing  with\nAyesha  Khan  about  his  research” can  induce  the  high-level\ninsight “Klaus  Mueller  is  dedicated  to  his  research”.  In\naddition,  the  reflection  process  can  occur  hierarchically,\nmeaning that the insights can be generated based on existing\ninsights.  In  GITM  [16],  the  actions  that  successfully\naccomplish  the  sub-goals  are  stored  in  a  list.  When  the  list\ncontains more than five elements, the agent summarizes them\ninto  a  common  and  abstract  pattern  and  replaces  all  the\nelements.  In  ExpeL  [44],  two  approaches  are  introduced  for\nthe  agent  to  acquire  reflection.  Firstly,  the  agent  compares\nsuccessful  or  failed  trajectories  within  the  same  task.\nSecondly,  the  agent  learns  from  a  collection  of  successful\ntrajectories to gain experiences.\nA significant distinction between traditional LLMs and the\nagents is that the latter must possess the capability to learn and\ncomplete  tasks  in  dynamic  environments.  If  we  consider  the\nmemory module as responsible for managing the agents’ past\nbehaviors,  it  becomes  essential  to  have  another  significant\nmodule  that  can  assist  the  agents  in  planning  their  future\nactions.  In  the  following,  we  present  an  overview  of  how\nresearchers design the planning module.\n 2.1.3    Planning module\nWhen faced with a complex task, humans tend to deconstruct\nit  into  simpler  subtasks  and  solve  them  individually.  The\nplanning  module  aims  to  empower  the  agents  with  such\nhuman capability, which is expected to make the agent behave\nmore  reasonably,  powerfully,  and  reliably.  In  specific,  we\nsummarize  existing  studies  based  on  whether  the  agent  can\nreceive feedback in the planing process, which are detailed as\nfollows:\nPlanning without feedback: In this method, the agents do\nnot  receive  feedback  that  can  influence  its  future  behaviors\nafter  taking  actions.  In  the  following,  we  present  several\nrepresentative strategies.\n●  Single-path  reasoning.  In  this  strategy,  the  final  task  is\ndecomposed  into  several  intermediate  steps.  These  steps  are\nconnected  in  a  cascading  manner,  with  each  step  leading  to\nonly one subsequent step. LLMs follow these steps to achieve\nthe  final  goal.  Specifically,  Chain  of  Thought  (CoT)  [45]\nproposes  inputting  reasoning  steps  for  solving  complex\nproblems  into  the  prompt.  These  steps  serve  as  examples  to\ninspire LLMs to plan and act in a step-by-step manner. In this\nmethod, the plans are created based on the inspiration from the\nexamples  in  the  prompts.  Zero-shot-CoT  [46]  enables  LLMs\nto generate task reasoning processes by prompting them with\ntrigger  sentences  like “think  step  by  step”.  Unlike  CoT,  this\nmethod  does  not  incorporate  reasoning  steps  as  examples  in\nthe  prompts.  Re-Prompting  [47]  involves  checking  whether\neach step meets the necessary prerequisites before generating\na plan. If a step fails to meet the prerequisites, it introduces a\nprerequisite error message and prompts the LLM to regenerate\nthe  plan.  ReWOO  [48]  introduces  a  paradigm  of  separating\nplans  from  external  observations,  where  the  agents  first\ngenerate  plans  and  obtain  observations  independently,  and\nthen  combine  them  together  to  derive  the  final  results.\nHuggingGPT  [13]  first  decomposes  the  task  into  many  sub-\ngoals,  and  then  solves  each  of  them  based  on  Huggingface.\nDifferent  from  CoT  and  Zero-shot-CoT,  which  outcome  all\nthe  reasoning  steps  in  a  one-shot  manner,  ReWOO  and\nHuggingGPT produce the results by accessing LLMs multiply\ntimes.\n● Multi-path reasoning. In this strategy, the reasoning steps\nfor  generating  the  final  plans  are  organized  into  a  tree-like\nstructure.  Each  intermediate  step  may  have  multiple\nsubsequent  steps.  This  approach  is  analogous  to  human\nthinking,  as  individuals  may  have  multiple  choices  at  each\nreasoning step. In specific, Self-consistent CoT (CoT-SC) [49]\nbelieves  that  each  complex  problem  has  multiple  ways  of\nthinking  to  deduce  the  final  answer.  Thus,  it  starts  by\nemploying  CoT  to  generate  various  reasoning  paths  and\ncorresponding  answers.  Subsequently,  the  answer  with  the\nhighest  frequency  is  chosen  as  the  final  output.  Tree  of\nThoughts  (ToT)  [50]  is  designed  to  generate  plans  using  a\ntree-like reasoning structure. In this approach, each node in the\ntree  represents  a “thought,” which  corresponds  to  an\nintermediate  reasoning  step.  The  selection  of  these\nintermediate  steps  is  based  on  the  evaluation  of  LLMs.  The\nfinal  plan  is  generated  using  either  the  breadth-first  search\n(BFS)  or  depth-first  search  (DFS)  strategy.  Comparing  with\nCoT-SC,  which  generates  all  the  planed  steps  together,  ToT\nneeds  to  query  LLMs  for  each  reasoning  step.  In  RecMind\n[51], the authors designed a self-inspiring mechanism, where\nthe discarded historical information in the planning process is\nalso leveraged to derive new reasoning steps. In GoT [52], the\nauthors  expand  the  tree-like  reasoning  structure  in  ToT  to\ngraph  structures,  resulting  in  more  powerful  prompting\nstrategies. In AoT [53], the authors design a novel method to\nenhance  the  reasoning  processes  of  LLMs  by  incorporating\nalgorithmic  examples  into  the  prompts.  Remarkably,  this\nmethod  only  needs  to  query  LLMs  for  only  one  or  a  few\ntimes. In [54], the LLMs are leveraged as zero-shot planners.\n6 Front. Comput. Sci., 2024, 18(6): 186345\nAt  each  planning  step,  they  first  generate  multiple  possible\nnext  steps,  and  then  determine  the  final  one  based  on  their\ndistances to admissible actions. [55] further improves [54] by\nincorporating  examples  that  are  similar  to  the  queries  in  the\nprompts.  RAP  [56]  builds  a  world  model  to  simulate  the\npotential  benefits  of  different  plans  based  on  Monte  Carlo\nTree Search (MCTS), and then, the final plan is generated by\naggregating  multiple  MCTS  iterations.  To  enhance\ncomprehension,  we  provide  an  illustration  comparing  the\nstrategies of single-path and multi-path reasoning in Fig. 3.\n●  External  planner.  Despite  the  demonstrated  power  of\nLLMs  in  zero-shot  planning,  effectively  generating  plans  for\ndomain-specific  problems  remains  highly  challenging.  To\naddress  this  challenge,  researchers  turn  to  external  planners.\nThese  tools  are  well-developed  and  employ  efficient  search\nalgorithms to rapidly identify correct, or even optimal, plans.\nIn specific, LLM+P [57] first transforms the task descriptions\ninto  formal  Planning  Domain  Definition  Languages  (PDDL),\nand  then  it  uses  an  external  planner  to  deal  with  the  PDDL.\nFinally, the generated results are transformed back into natural\nlanguage by LLMs. Similarly, LLM-DP [58] utilizes LLMs to\nconvert the observations, the current world state, and the target\nobjectives into PDDL. Subsequently, this transformed data is\npassed to an external planner, which efficiently determines the\nfinal action sequence. CO-LLM [22] demonstrates that LLMs\nis good at generating high-level plans, but struggle with low-\nlevel  control.  To  address  this  limitation,  a  heuristically\ndesigned external low-level planner is employed to effectively\nexecute actions based on high-level plans.\nPlanning with feedback: In many real-world scenarios, the\nagents need to make long-horizon planning to solve complex\ntasks.  When  facing  these  tasks,  the  above  planning  modules\nwithout  feedback  can  be  less  effective  due  to  the  following\nreasons:  firstly,  generating  a  flawless  plan  directly  from  the\nbeginning is extremely difficult as it needs to consider various\ncomplex  preconditions.  As  a  result,  simply  following  the\ninitial plan often leads to failure. Moreover, the execution of\nthe  plan  may  be  hindered  by  unpredictable  transition\ndynamics,  rendering  the  initial  plan  non-executable.\nSimultaneously, when examining how humans tackle complex\ntasks, we find that individuals may iteratively make and revise\ntheir  plans  based  on  external  feedback.  To  simulate  such\nhuman  capability,  researchers  have  designed  many  planning\nmodules,  where  the  agent  can  receive  feedback  after  taking\nactions.  The  feedback  can  be  obtained  from  environments,\nhumans, and models, which are detailed in the following.\n● Environmental feedback. This feedback is obtained from\nthe  objective  world  or  virtual  environment.  For  instance,  it\ncould  be  the  game’s  task  completion  signals  or  the\nobservations made after the agent takes an action. In specific,\nReAct [59] proposes constructing prompts using thought-act-\nobservation triplets. The thought component aims to facilitate\nhigh-level reasoning and planning for guiding agent behaviors.\nThe  act  represents  a  specific  action  taken  by  the  agent.  The\nobservation corresponds to the outcome of the action, acquired\nthrough external feedback, such as search engine results. The\nnext thought is influenced by the previous observations, which\nmakes the generated plans more adaptive to the environment.\nVoyager  [38]  makes  plans  by  incorporating  three  types  of\nenvironment  feedback  including  the  intermediate  progress  of\nprogram  execution,  the  execution  error  and  self-verification\nresults. These signals can help the agent to make better plans\nfor  the  next  action.  Similar  to  Voyager,  Ghost  [16]  also\nincorporates  feedback  into  the  reasoning  and  action  taking\nprocesses. This feedback encompasses the environment states\nas  well  as  the  success  and  failure  information  for  each\nexecuted  action.  SayPlan  [31]  leverages  environmental\nfeedback derived from a scene graph simulator to validate and\nrefine  its  strategic  formulations.  This  simulator  is  adept  at\ndiscerning  the  outcomes  and  state  transitions  subsequent  to\nagent  actions,  facilitating  SayPlan’s  iterative  recalibration  of\nits strategies until a viable plan is ascertained. In DEPS [33],\nthe authors argue that solely providing information about the\ncompletion  of  a  task  is  often  inadequate  for  correcting\nplanning  errors.  Therefore,  they  propose  informing  the  agent\nabout the detail reasons for task failure, allowing them to more\neffectively revise their plans. LLM-Planner [60] introduces a\ngrounded  re-planning  algorithm  that  dynamically  updates\nplans  generated  by  LLMs  when  encountering  object\nmismatches  and  unattainable  plans  during  task  completion.\nInner Monologue [61] provides three types of feedback to the\nagent after it takes actions: (1) whether the task is successfully\n \n \nFig. 3    Comparison between the strategies of single-path and multi-path reasoning. LMZSP is the model proposed in [54]\n \nLei WANG et al.    A survey on large language model based autonomous agents 7\ncompleted, (2) passive scene descriptions, and (3) active scene\ndescriptions.  The  former  two  are  generated  from  the\nenvironments,  which  makes  the  agent  actions  more\nreasonable.\n● Human feedback. In addition to obtaining feedback from\nthe  environment,  directly  interacting  with  humans  is  also  a\nvery  intuitive  strategy  to  enhance  the  agent  planning\ncapability. The human feedback is a subjective signal. It can\neffectively  make  the  agent  align  with  the  human  values  and\npreferences,  and  also  help  to  alleviate  the  hallucination\nproblem. In Inner Monologue [61], the agent aims to perform\nhigh-level  natural  language  instructions  in  a  3D  visual\nenvironment.  It  is  given  the  capability  to  actively  solicit\nfeedback from humans regarding scene descriptions. Then, the\nagent  incorporates  the  human  feedback  into  its  prompts,\nenabling more informed planning and reasoning. In the above\ncases,  we  can  see,  different  types  of  feedback  can  be\ncombined  to  enhance  the  agent  planning  capability.  For\nexample, Inner Monologue [61] collects both environment and\nhuman feedback to facilitate the agent plans.\n● Model  feedback.  Apart  from  the  aforementioned\nenvironmental  and  human  feedback,  which  are  external\nsignals,  researchers  have  also  investigated  the  utilization  of\ninternal  feedback  from  the  agents  themselves.  This  type  of\nfeedback is usually generated based on pre-trained models. In\nspecific,  [62]  proposes  a  self-refine  mechanism.  This\nmechanism  consists  of  three  crucial  components:  output,\nfeedback,  and  refinement.  Firstly,  the  agent  generates  an\noutput.  Then,  it  utilizes  LLMs  to  provide  feedback  on  the\noutput  and  offer  guidance  on  how  to  refine  it.  At  last,  the\noutput  is  improved  by  the  feedback  and  refinement.  This\noutput-feedback-refinement  process  iterates  until  reaching\nsome  desired  conditions.  SelfCheck  [63]  allows  agents  to\nexamine  and  evaluate  their  reasoning  steps  generated  at\nvarious stages. They can then correct any errors by comparing\nthe  outcomes.  InterAct  [64]  uses  different  language  models\n(such as ChatGPT and InstructGPT) as auxiliary roles, such as\ncheckers and sorters, to help the main language model avoid\nerroneous and inefficient actions. ChatCoT [65] utilizes model\nfeedback to improve the quality of its reasoning process. The\nmodel  feedback  is  generated  by  an  evaluation  module  that\nmonitors  the  agent  reasoning  steps.  Reflexion  [12]  is\ndeveloped to enhance the agent’s planning capability through\ndetailed  verbal  feedback.  In  this  model,  the  agent  first\nproduces  an  action  based  on  its  memory,  and  then,  the\nevaluator generates feedback by taking the agent trajectory as\ninput.  In  contrast  to  previous  studies,  where  the  feedback  is\ngiven as a scalar value, this model leverages LLMs to provide\nmore  detailed  verbal  feedback,  which  can  provide  more\ncomprehensive supports for the agent plans.\nRemark.  In  conclusion,  the  implementation  of  planning\nmodule  without  feedback  is  relatively  straightforward.\nHowever,  it  is  primarily  suitable  for  simple  tasks  that  only\nrequire  a  small  number  of  reasoning  steps.  Conversely,  the\nstrategy of planning with feedback needs more careful designs\nto handle the feedback. Nevertheless, it is considerably more\npowerful and capable of effectively addressing complex tasks\nthat involve long-range reasoning.\n 2.1.4    Action module\nThe  action  module  is  responsible  for  translating  the  agent’s\ndecisions into specific outcomes. This module is located at the\nmost  downstream  position  and  directly  interacts  with  the\nenvironment.  It  is  influenced  by  the  profile,  memory,  and\nplanning  modules.  This  section  introduces  the  action  module\nfrom four perspectives: (1) Action goal: what are the intended\noutcomes  of  the  actions?  (2)  Action  production:  how  are  the\nactions  generated?  (3)  Action  space:  what  are  the  available\nactions? (4) Action impact: what are the consequences of the\nactions? Among these perspectives, the first two focus on the\naspects  preceding  the  action  (“before-action” aspects),  the\nthird focuses on the action itself (“in-action” aspect), and the\nfourth  emphasizes  the  impact  of  the  actions  (“after-action”\naspect).\nAction  goal:  The  agent  can  perform  actions  with  various\nobjectives. Here, we present several representative examples:\n(1) Task Completion. In this scenario, the agent’s actions are\naimed at accomplishing specific tasks, such as crafting an iron\npickaxe in Minecraft [38] or completing a function in software\ndevelopment  [18].  These  actions  usually  have  well-defined\nobjectives,  and  each  action  contributes  to  the  completion  of\nthe  final  task.  Actions  aimed  at  this  type  of  goal  are  very\ncommon  in  existing  literature.  (2) Communication.  In  this\ncase,  the  actions  are  taken  to  communicate  with  the  other\nagents  or  real  humans  for  sharing  information  or\ncollaboration.  For  example,  the  agents  in  ChatDev  [18]  may\ncommunicate  with  each  other  to  collectively  accomplish\nsoftware  development  tasks.  In  Inner  Monologue  [61],  the\nagent  actively  engages  in  communication  with  humans  and\nadjusts  its  action  strategies  based  on  human  feedback.\n(3) Environment Exploration. In this example, the agent aims\nto  explore  unfamiliar  environments  to  expand  its  perception\nand  strike  a  balance  between  exploring  and  exploiting.  For\ninstance,  the  agent  in  Voyager  [38]  may  explore  unknown\nskills in their task completion process, and continually refine\nthe  skill  execution  code  based  on  environment  feedback\nthrough trial and error.\nAction  production:  Different  from  ordinary  LLMs,  where\nthe  model  input  and  output  are  directly  associated,  the  agent\nmay  take  actions  via  different  strategies  and  sources.  In  the\nfollowing,  we  introduce  two  types  of  commonly  used  action\nproduction  strategies.  (1)  Action  via  memory  recollection.  In\nthis strategy, the action is generated by extracting information\nfrom the agent memory according to the current task. The task\nand the extracted memories are used as prompts to trigger the\nagent  actions.  For  example,  in  Generative  Agents  [20],  the\nagent  maintains  a  memory  stream,  and  before  taking  each\naction, it retrieves recent, relevant and important information\nfrom the memory steam to guide the agent actions. In GITM\n[16],  in  order  to  achieve  a  low-level  sub-goal,  the  agent\nqueries  its  memory  to  determine  if  there  are  any  successful\nexperiences  related  to  the  task.  If  similar  tasks  have  been\ncompleted  previously,  the  agent  invokes  the  previously\nsuccessful  actions  to  handle  the  current  task  directly.  In\ncollaborative agents such as ChatDev [18] and MetaGPT [23],\ndifferent  agents  may  communicate  with  each  other.  In  this\nprocess, the conversation history in a dialog is remembered in\n8 Front. Comput. Sci., 2024, 18(6): 186345\nthe agent memories. Each utterance generated by the agent is\ninfluenced  by  its  memory.  (2)  Action  via  plan  following.  In\nthis  strategy,  the  agent  takes  actions  following  its  pre-\ngenerated plans. For instance, in DEPS [33], for a given task,\nthe  agent  first  makes  action  plans.  If  there  are  no  signals\nindicating  plan  failure,  the  agent  will  strictly  adhere  to  these\nplans.  In  GITM  [16],  the  agent  makes  high-level  plans  by\ndecomposing  the  task  into  many  sub-goals.  Based  on  these\nplans,  the  agent  takes  actions  to  solve  each  sub-goal\nsequentially to complete the final task.\nAction  space:  Action  space  refers  to  the  set  of  possible\nactions that can be performed by the agent. In general, we can\nroughly  divide  these  actions  into  two  classes:  (1)  external\ntools  and  (2)  internal  knowledge  of  the  LLMs.  In  the\nfollowing, we introduce these actions more in detail.\n● External tools. While LLMs have been demonstrated to be\neffective in accomplishing a large amount of tasks, they may\nnot  work  well  for  the  domains  which  need  comprehensive\nexpert  knowledge.  In  addition,  LLMs  may  also  encounter\nhallucination  problems,  which  are  hard  to  be  resolved  by\nthemselves.  To  alleviate  the  above  problems,  the  agents  are\nempowered  with  the  capability  to  call  external  tools  for\nexecuting  action.  In  the  following,  we  present  several\nrepresentative  tools  which  have  been  exploited  in  the\nliterature.\n(1)  APIs.  Leveraging  external  APIs  to  complement  and\nexpand action space is a popular paradigm in recent years. For\nexample,  HuggingGPT  [13]  leverages  the  models  on\nHuggingFace  to  accomplish  complex  user  tasks.  [66,67]\npropose  to  automatically  generate  queries  to  extract  relevant\ncontent  from  external  Web  pages  when  responding  to  user\nrequest.  TPTU  [67]  interfaces  with  both  Python  interpreters\nand  LaTeX  compilers  to  execute  sophisticated  computations\nsuch as square roots, factorials and matrix operations. Another\ntype of APIs is the ones that can be directly invoked by LLMs\nbased on natural language or code inputs. For instance, Gorilla\n[68] is a fine-tuned LLM designed to generate accurate input\narguments for API calls and mitigate the issue of hallucination\nduring external API invocations. ToolFormer [15] is an LLM-\nbased  tool  transformation  system  that  can  automatically\nconvert  a  given  tool  into  another  one  with  different\nfunctionalities  or  formats  based  on  natural  language\ninstructions.  API-Bank  [69]  is  an  LLM-based  API\nrecommendation  agent  that  can  automatically  search  and\ngenerate  appropriate  API  calls  for  various  programming\nlanguages and domains. API-Bank also provides an interactive\ninterface for users to easily modify and execute the generated\nAPI  calls.  ToolBench  [14]  is  an  LLM-based  tool  generation\nsystem  that  can  automatically  design  and  implement  various\npractical  tools  based  on  natural  language  requirements.  The\ntools  generated  by  ToolBench  include  calculators,  unit\nconverters,  calendars,  maps,  charts,  etc.  RestGPT  [70]\nconnects  LLMs  with  RESTful  APIs,  which  follow  widely\naccepted standards for Web services development, making the\nresulting  program  more  compatible  with  real-world\napplications.  TaskMatrix.AI  [71]  connects  LLMs  with\nmillions of APIs to support task execution. At its core lies a\nmultimodal  conversational  foundational  model  that  interacts\nwith  users,  understands  their  goals  and  context,  and  then\nproduces executable code for particular tasks. All these agents\nutilize  external  APIs  as  their  tools,  and  provide  interactive\ninterfaces for users to easily modify and execute the generated\nor transformed tools.\n(2)  Databases  &  Knowledge  Bases.  Integrating  external\ndatabase or knowledge base enables agents to obtain specific\ndomain information for generating more realistic actions. For\nexample,  ChatDB  [40]  employs  SQL  statements  to  query\ndatabases,  facilitating  actions  by  the  agents  in  a  logical\nmanner.  MRKL  [72]  and  OpenAGI  [73]  incorporate  various\nexpert  systems  such  as  knowledge  bases  and  planners  to\naccess domain-specific information.\n(3)  External  models.  Previous  studies  often  utilize  external\nmodels to expand the range of possible actions. In comparison\nto APIs, external models typically handle more complex tasks.\nEach  external  model  may  correspond  to  multiple  APIs.  For\nexample,  to  enhance  the  text  retrieval  capability,\nMemoryBank  [39]  incorporates  two  language  models:  one  is\ndesigned  to  encode  the  input  text,  while  the  other  is\nresponsible for matching the query statements. ViperGPT [74]\nfirstly  uses  Codex,  which  is  implemented  based  on  language\nmodel,  to  generate  Python  code  from  text  descriptions,  and\nthen executes the code to complete the given tasks. TPTU [67]\nincorporates  various  LLMs  to  accomplish  a  wide  range  of\nlanguage generation tasks such as generating code, producing\nlyrics, and more. ChemCrow [75] is an LLM-based chemical\nagent  designed  to  perform  tasks  in  organic  synthesis,  drug\ndiscovery,  and  material  design.  It  utilizes  seventeen  expert-\ndesigned  models  to  assist  its  operations.  MM-REACT  [76]\nintegrates  various  external  models,  such  as  VideoBERT  for\nvideo  summarization,  X-decoder  for  image  generation,  and\nSpeechBERT for audio processing, enhancing its capability in\ndiverse multimodal scenarios.\n● Internal knowledge. In addition to utilizing external tools,\nmany agents rely solely on the internal knowledge of LLMs to\nguide  their  actions.  We  now  present  several  crucial\ncapabilities  of  LLMs  that  can  support  the  agent  to  behave\nreasonably  and  effectively.  (1)  Planning  capability.  Previous\nwork  has  demonstrated  that  LLMs  can  be  used  as  decent\nplaners  to  decompose  complex  task  into  simpler  ones  [45].\nSuch  capability  of  LLMs  can  be  even  triggered  without\nincorporating  examples  in  the  prompts  [46].  Based  on  the\nplanning capability of LLMs, DEPS [33] develops a Minecraft\nagent,  which  can  solve  complex  task  via  sub-goal\ndecomposition.  Similar  agents  like  GITM  [16]  and  Voyager\n[38] also heavily rely on the planning capability of LLMs to\nsuccessfully  complete  different  tasks.  (2)  Conversation\ncapability.  LLMs  can  usually  generate  high-quality\nconversations.  This  capability  enables  the  agent  to  behave\nmore  like  humans.  In  the  previous  work,  many  agents  take\nactions based on the strong conversation capability of LLMs.\nFor example, in ChatDev [18], different agents can discuss the\nsoftware  developing  process,  and  even  can  make  reflections\non  their  own  behaviors.  In  RLP  [30],  the  agent  can\ncommunicate  with  the  listeners  based  on  their  potential\nfeedback  on  the  agent’s  utterance.  (3)  Common  sense\nunderstanding  capability.  Another  important  capability  of\nLei WANG et al.    A survey on large language model based autonomous agents 9\nLLMs  is  that  they  can  well  comprehend  human  common\nsense.  Based  on  this  capability,  many  agents  can  simulate\nhuman  daily  life  and  make  human-like  decisions.  For\nexample,  in  Generative  Agent,  the  agent  can  accurately\nunderstand its current state, the surrounding environment, and\nsummarize  high-level  ideas  based  on  basic  observations.\nWithout the common sense understanding capability of LLMs,\nthese  behaviors  cannot  be  reliably  simulated.  Similar\nconclusions  may  also  apply  to  RecAgent  [21]  and  S3 [77],\nwhere  the  agents  aim  to  simulate  user  recommendation  and\nsocial behaviors.\nAction impact: Action impact refers to the consequences of\nthe action. In fact, the action impact can encompass numerous\ninstances,  but  for  brevity,  we  only  provide  a  few  examples.\n(1)  Changing  environments.  Agents  can  directly  alter\nenvironment states by actions, such as moving their positions,\ncollecting  items,  and  constructing  buildings.  For  instance,  in\nGITM [16] and Voyager [38], the environments are changed\nby the actions of the agents in their task completion process.\nFor  example,  if  the  agent  mines  three  woods,  then  they  may\ndisappear  in  the  environments.  (2)  Altering  internal  states.\nActions  taken  by  the  agent  can  also  change  the  agent  itself,\nincluding  updating  memories,  forming  new  plans,  acquiring\nnovel  knowledge,  and  more.  For  example,  in  Generative\nAgents  [20],  memory  streams  are  updated  after  performing\nactions within the system. SayCan [78] enables agents to take\nactions  to  update  understandings  of  the  environment.\n(3)  Triggering  new  actions.  In  the  task  completion  process,\none  agent  action  can  be  triggered  by  another  one.  For\nexample,  Voyager  [38]  constructs  buildings  once  it  has\ngathered all the necessary resources.\n 2.2    Agent capability acquisition\nIn the above sections, we mainly focus on how to design the\nagent architecture to better inspire the capability of LLMs to\nmake  it  qualified  for  accomplishing  tasks  like  humans.  The\narchitecture  functions  as  the “hardware” of  the  agent.\nHowever,  relying  solely  on  the  hardware  is  insufficient  for\nachieving  effective  task  performance.  This  is  because  the\nagent may lack the necessary task-specific capabilities, skills\nand  experiences,  which  can  be  regarded  as “software”\nresources.  In  order  to  equip  the  agent  with  these  resources,\nvarious strategies have been devised. Generally, we categorize\nthese strategies into two classes based on whether they require\nfine-tuning of the LLMs. In the following, we introduce each\nof them more in detail.\nCapability acquisition with fine-tuning: A straightforward\nmethod to enhance the agent capability for task completion is\nfine-tuning  the  agent  based  on  task-dependent  datasets.\nGenerally,  the  datasets  can  be  constructed  based  on  human\nannotation,  LLM  generation  or  collected  from  real-world\napplications.  In  the  following,  we  introduce  these  methods\nmore in detail.\n● Fine-tuning with human annotated datasets. To fine-tune\nthe  agent,  utilizing  human  annotated  datasets  is  a  versatile\napproach  that  can  be  employed  in  various  application\nscenarios. In this approach, researchers first design annotation\ntasks and then recruit workers to complete them. For example,\nin  CoH  [79],  the  authors  aim  to  align  LLMs  with  human\nvalues  and  preferences.  Different  from  the  other  models,\nwhere  the  human  feedback  is  leveraged  in  a  simple  and\nsymbolic  manner,  this  method  converts  the  human  feedback\ninto  detailed  comparison  information  in  the  form  of  natural\nlanguages.  The  LLMs  are  directly  fine-tuned  based  on  these\nnatural language datasets. In RET-LLM [42], in order to better\nconvert natural languages into structured memory information,\nthe  authors  fine-tune  LLMs  based  on  a  human  constructed\ndataset, where each sample is a “triplet-natural language” pair.\nIn WebShop [80], the authors collect 1.18 million real-world\nproducts form amazon.com, and put them onto a simulated e-\ncommerce website, which contains several carefully designed\nhuman shopping scenarios. Based on this website, the authors\nrecruit 13 workers to collect a real-human behavior dataset. At\nlast, three methods based on heuristic rules, imitation learning\nand  reinforcement  learning  are  trained  based  on  this  dataset.\nAlthough the authors do not fine-tune LLM-based agents, we\nbelieve that the dataset proposed in this paper holds immense\npotential  to  enhance  the  capabilities  of  agents  in  the  field  of\nWeb  shopping.  In  EduChat  [81],  the  authors  aim  to  enhance\nthe  educational  functions  of  LLMs,  such  as  open-domain\nquestion  answering,  essay  assessment,  Socratic  teaching,  and\nemotional  support.  They  fine-tune  LLMs  based  on  human\nannotated  datasets  that  cover  various  educational  scenarios\nand tasks. These datasets are manually evaluated and curated\nby  psychology  experts  and  frontline  teachers.  SWIFTSAGE\n[82]  is  an  agent  influenced  by  the  dual-process  theory  of\nhuman cognition [83], which is effective for solving complex\ninteractive reasoning tasks. In this agent, the SWIFT module\nconstitutes a compact encoder-decoder language model, which\nis fine-tuned using human-annotated datasets.\n●  Fine-tuning  with  LLM  generated  datasets.  Building\nhuman annotated dataset needs to recruit people, which can be\ncostly, especially when one needs to annotate a large amount\nof  samples.  Considering  that  LLMs  can  achieve  human-like\ncapabilities  in  a  wide  range  of  tasks,  a  natural  idea  is  using\nLLMs  to  accomplish  the  annotation  task.  While  the  datasets\nproduced from this method can be not as perfect as the human\nannotated  ones,  it  is  much  cheaper,  and  can  be  leveraged  to\ngenerate  more  samples.  For  example,  in  ToolBench  [14],  to\nenhance  the  tool-using  capability  of  open-source  LLMs,  the\nauthors collect 16,464 real-world APIs spanning 49 categories\nfrom  the  RapidAPI  Hub.  They  used  these  APIs  to  prompt\nChatGPT  to  generate  diverse  instructions,  covering  both\nsingle-tool  and  multi-tool  scenarios.  Based  on  the  obtained\ndataset,  the  authors  fine-tune  LLaMA  [9],  and  obtain\nsignificant  performance  improvement  in  terms  of  tool  using.\nIn  [84],  to  empower  the  agent  with  social  capability,  the\nauthors  design  a  sandbox,  and  deploy  multiple  agents  to\ninteract  with  each  other.  Given  a  social  question,  the  central\nagent  first  generates  initial  responses.  Then,  it  shares  the\nresponses  to  its  nearby  agents  for  collecting  their  feedback.\nBased on the feedback as well as its detailed explanations, the\ncentral  agent  revise  its  initial  responses  to  make  them  more\nconsistent  with  social  norms.  In  this  process,  the  authors\ncollect a large amount of agent social interaction data, which\nis then leveraged to fine-tune the LLMs.\n10 Front. Comput. Sci., 2024, 18(6): 186345\n●  Fine-tuning  with  real-world  datasets.  In  addition  to\nbuilding datasets based on human or LLM annotation, directly\nusing  real-world  datasets  to  fine-tune  the  agent  is  also  a\ncommon  strategy.  For  example,  in  MIND2WEB  [85],  the\nauthors  collect  a  large  amount  of  real-world  datasets  to\nenhance the agent capability in the Web domain. In contrast to\nprior studies, the dataset presented in this paper encompasses\ndiverse  tasks,  real-world  scenarios,  and  comprehensive  user\ninteraction  patterns.  Specifically,  the  authors  collect  over\n2,000  open-ended  tasks  from  137  real-world  websites\nspanning 31 domains. Using this dataset, the authors fine-tune\nLLMs  to  enhance  their  performance  on  Web-related  tasks,\nincluding movie discovery and ticket booking, among others.\nIn SQL-PALM [86], researchers fine-tune PaLM-2 based on a\ncross-domain  large-scale  text-to-SQL  dataset  called  Spider.\nThe  obtained  model  can  achieve  significant  performance\nimprovement on text-to-SQL tasks.\nCapability  acquisition  without  fine-tuning:  In  the  era  of\ntradition  machine  learning,  the  model  capability  is  mainly\nacquired  by  learning  from  datasets,  where  the  knowledge  is\nencoded  into  the  model  parameters.  In  the  era  of  LLMs,  the\nmodel capability can be acquired either by training/fine-tuning\nthe  model  parameters  or  designing  delicate  prompts  (i.e.,\nprompt  engineer).  In  prompt  engineer,  one  needs  to  write\nvaluable  information  into  the  prompts  to  enhance  the  model\ncapability or unleash existing LLM capabilities. In the era of\nagents,  the  model  capability  can  be  acquired  based  on  three\nstrategies:  (1)  model  fine-tuning,  (2)  prompt  engineer,  and\n(3) designing proper agent evolution mechanisms (we called it\nas mechanism  engineering).  Mechanism  engineering  is  a\nbroad  concept  that  involves  developing  specialized  modules,\nintroducing  novel  working  rules,  and  other  strategies  to\nenhance  agent  capabilities.  For  clearly  understanding  such\ntransitions on the strategy of model capability acquisition, we\nillustrate  them  in Fig. 4.  In  the  following,  we  introduce\nprompting  engineering  and  mechanism  engineering  for  agent\ncapability acquisition.\n●  Prompting  engineering.  Due  to  the  strong  language\ncomprehension  capabilities,  people  can  directly  interact  with\nLLMs  using  natural  languages.  This  introduces  a  novel\nstrategy  for  enhancing  agent  capabilities,  that  is,  one  can\ndescribe the desired capability using natural language and then\nuse  it  as  prompts  to  influence  LLM  actions.  For  example,  in\nCoT  [45],  in  order  to  empower  the  agent  with  the  capability\nfor  complex  task  reasoning,  the  authors  present  the\nintermediate  reasoning  steps  as  few-shot  examples  in  the\nprompt. Similar techniques are also used in CoT-SC [49] and\nToT  [50].  In  SocialAGI  [30],  in  order  to  enhance  the  agent\nself-awareness capability in conversation, the authors prompt\nLLMs  with  the  agent  beliefs  about  the  mental  states  of  the\nlisteners and itself, which makes the generated utterance more\nengaging  and  adaptive.  In  addition,  the  authors  also\nincorporate  the  target  mental  states  of  the  listeners,  which\nenables the agents to make more strategic plans. Retroformer\n[87]  presents  a  retrospective  model  that  enables  the  agent  to\ngenerate  reflections  on  its  past  failures.  The  reflections  are\nintegrated into the prompt of LLMs to guide the agent’s future\nactions.  Additionally,  this  model  utilizes  reinforcement\nlearning  to  iteratively  improve  the  retrospective  model,\nthereby refining the LLM prompt.\n●  Mechanism  engineering.  Unlike  model  fine-tuning  and\nprompt  engineering,  mechanism  engineering  is  a  unique\nstrategy  to  enhance  agent  capability.  In  the  following,  we\npresent  several  representative  methods  of  mechanism\nengineering.\n(1) Trial-and-error. In this method, the agent first performs\nan action, and subsequently, a pre-defined critic is invoked to\njudge  the  action.  If  the  action  is  deemed  unsatisfactory,  then\nthe agent reacts by incorporating the critic’s feedback. In RAH\n[88],  the  agent  serves  as  a  user  assistant  in  recommender\nsystems. One of the agent’s crucial roles is to simulate human\nbehavior  and  generate  responses  on  behalf  of  the  user.  To\nfulfill  this  objective,  the  agent  first  generates  a  predicted\nresponse and then compares it with the real human feedback.\nIf the predicted response and the real human feedback differ,\nthe critic generates failure information, which is subsequently\nincorporated  into  the  agent’s  next  action.  In  DEPS  [33],  the\nagent  first  designs  a  plan  to  accomplish  a  given  task.  In  the\nplan  execution  process,  if  an  action  fails,  the  explainer\ngenerates  specific  details  explaining  the  cause  of  the  failure.\nThis information is then incorporated by the agent to redesign\nthe plan. In RoCo [89], the agent first proposes a sub-task plan\nand  a  path  of  3D  waypoints  for  each  robot  in  a  multi-robot\ncollaboration task. The plan and waypoints are then validated\n \n \nFig. 4    Illustration of transitions in strategies for acquiring model capabilities\n \nLei WANG et al.    A survey on large language model based autonomous agents 11\nby a set of environment checks, such as collision detection and\ninverse  kinematics.  If  any  of  the  checks  fail,  the  feedback  is\nappended to each agent’s prompt and another round of dialog\nbegins.  The  agents  use  LLMs  to  discuss  and  improve  their\nplan and waypoints until they pass all validations. In PREFER\n[90],  the  agent  first  evaluates  its  performance  on  a  subset  of\ndata. If it fails to solve certain examples, LLMs are leveraged\nto generate feedback information reflecting on the reasons of\nthe failure. Based on this feedback, the agent improves itself\nby iteratively refining its actions.\n(2)  Crowd-sourcing.  In  [91],  the  authors  design  a  debating\nmechanism  that  leverages  the  wisdom  of  crowds  to  enhance\nagent  capabilities.  To  begin  with,  different  agents  provide\nseparate  responses  to  a  given  question.  If  their  responses  are\nnot  consistent,  they  will  be  prompted  to  incorporate  the\nsolutions from other agents and provide an updated response.\nThis  iterative  process  continues  until  reaching  a  final\nconsensus answer. In this method, the capability of each agent\nis  enhanced  by  understanding  and  incorporating  the  other\nagents’ opinions.\n(3) Experience accumulation. In GITM [16], the agent does\nnot know how to solve a task in the beginning. Then, it makes\nexplorations, and once it has successfully accomplished a task,\nthe actions used in this task are stored into the agent memory.\nIn  the  future,  if  the  agent  encounters  a  similar  task,  then  the\nrelevant memories are extracted to complete the current task.\nIn this process, the improved agent capability comes from the\nspecially  designed  memory  accumulation  and  utilization\nmechanisms.  In  Voyager  [38],  the  authors  equip  the  agent\nwith a skill library, and each skill in the library is represented\nby  executable  codes.  In  the  agent-environment  interaction\nprocess,  the  codes  for  each  skill  will  be  iteratively  refined\naccording  to  the  environment  feedback  and  the  agent  self-\nverification results. After a period of execution, the agent can\nsuccessfully  complete  different  tasks  efficiently  by  accessing\nthe  skill  library.  In  AppAgent  [92],  the  agent  is  designed  to\ninteract with apps in a manner akin to human users, learning\nthrough  both  autonomous  exploration  and  observation  of\nhuman demonstrations. Throughout this process, it constructs\na knowledge base, which serves as a reference for performing\nintricate tasks across various applications on a mobile phone.\nIn  MemPrompt  [93],  the  users  are  requested  to  provide\nfeedback  in  natural  language  regarding  the  problem-solving\nintentions of the agent, and this feedback is stored in memory.\nWhen the agent encounters similar tasks, it attempts to retrieve\nrelated memories to generate more suitable responses.\n(4)  Self-driven  evolution.  In  LMA3  [94],  the  agent  can\nautonomously  set  goals  for  itself,  and  gradually  improve  its\ncapability  by  exploring  the  environment  and  receiving\nfeedback from a reward function. Following this mechanism,\nthe  agent  can  acquire  knowledge  and  develop  capabilities\naccording  to  its  own  preferences.  In  SALLM-MS  [95],  by\nintegrating advanced large language models like GPT-4 into a\nmulti-agent  system,  agents  can  adapt  and  perform  complex\ntasks,  showcasing  advanced  communication  capabilities,\nthereby  realizing  self-driven  evolution  in  their  interactions\nwith  the  environment.  In  CLMTWA  [96],  by  using  a  large\nlanguage model as a teacher and a weaker language model as a\nstudent,  the  teacher  can  generate  and  communicate  natural\nlanguage  explanations  to  improve  the  student’s  reasoning\nskills via theory of mind. The teacher can also personalize its\nexplanations  for  the  student  and  intervene  only  when\nnecessary,  based  on  the  expected  utility  of  intervention.  In\nNLSOM  [97],  different  agents  communicate  and  collaborate\nthrough  natural  language  to  solve  tasks  that  a  single  agent\ncannot  solve.  This  can  be  seen  as  a  form  of  self-driven\nlearning, utilizing the exchange of information and knowledge\nbetween multiple agents. However, unlike other models such\nas LMA3, SALLM-MS, and CLMTWA, NLSOM allows for\ndynamic  adjustment  of  agent  roles,  tasks,  and  relationships\nbased on the task requirements and feedback from other agents\nor the environment.\nRemark.  Upon  comparing  the  aforementioned  strategies  for\nagent  capability  acquisition,  we  can  find  that  the  fine-tuning\nmethod  improves  the  agent  capability  by  adjusting  model\nparameters,  which  can  incorporate  a  large  amount  of  task-\nspecific  knowledge,  but  is  only  suitable  for  open-source\nLLMs.  The  method  without  fine-tuning  usually  enhances  the\nagent  capability  based  on  delicate  prompting  strategies  or\nmechanism engineering. They can be used for both open- and\nclosed-source  LLMs.  However,  due  to  the  limitation  of  the\ninput  context  window  of  LLMs,  they  cannot  incorporate  too\nmuch  task  information.  In  addition,  the  designing  spaces  of\nthe  prompts  and  mechanisms  are  extremely  large,  which\nmakes it not easy to find optimal solutions.\nIn the above sections, we have detailed the construction of\nLLM-based agents, where we focus on two aspects including\nthe architecture design and capability acquisition. We present\nthe  correspondence  between  existing  work  and  the  above\ntaxonomy in Table 1. It should be noted that, for the sake of\nintegrity, we have also incorporated several studies, which do\nnot  explicitly  mention  LLM-based  agents  but  are  highly\nrelated to this area.\n 3    LLM-based autonomous agent\napplication\nOwing  to  the  strong  language  comprehension,  complex  task\nreasoning,  and  common  sense  understanding  capabilities,\nLLM-based  autonomous  agents  have  shown  significant\npotential to influence multiple domains. This section provides\na  succinct  summary  of  previous  studies,  categorizing  them\naccording  to  their  applications  in  three  distinct  areas:  social\nscience, natural science, and engineering (see the left part of\nFig. 5 for a global overview).\n 3.1    Social science\nSocial science is one of the branches of science, devoted to the\nstudy  of  societies  and  the  relationships  among  individuals\nwithin  those  societies.  LLM-based  autonomous  agents  can\npromote  this  domain  by  leveraging  their  impressive  human-\nlike  understanding,  thinking  and  task  solving  capabilities.  In\nthe  following,  we  discuss  several  key  areas  that  can  be\naffected by LLM-based autonomous agents.\nPsychology:  For  the  domain  of  psychology,  LLM-based\nagents  can  be  leveraged  for  conducting  simulation\nexperiments,  providing  mental  health  support  and  so  on\n12 Front. Comput. Sci., 2024, 18(6): 186345\n[102–105].  For  example,  in  [102],  the  authors  assign  LLMs\nwith  different  profiles,  and  let  them  complete  psychology\nexperiments. From the results, the authors find that LLMs are\ncapable of generating results that align with those from studies\ninvolving  human  participants.  Additionally,  it  was  observed\nthat  larger  models  tend  to  deliver  more  accurate  simulation\nresults compared to their smaller counterparts. An interesting\ndiscovery is that, in many experiments, models like ChatGPT\n   \nTable  1    For  the  profile  module,  we  use ①, ②,  and ③ to  represent  the  handcrafting  method,  LLM-generation  method,  and  dataset  alignment  method,\nrespectively. For the memory module, we focus on the implementation strategies for memory operation and memory structure. For memory operation, we use\n① and ② to indicate that the model only has read/write operations and has read/write/reflection operations, respectively. For memory structure, we use ① and\n② to represent unified and hybrid memories, respectively. For the planning module, we use ① and ② to represent planning w/o feedback and w/ feedback,\nrespectively.  For  the  action  module,  we  use ① and ② to  represent  that  the  model  does  not  use  tools  and  use  tools,  respectively.  For  the  agent  capability\nacquisition (CA) strategy, we use ① and ② to represent the methods with and without fine-tuning, respectively. “−” indicates that the corresponding content is\nnot explicitly discussed in the paper\nModel Profile Memory Planning Action CA TimeOperation Structure\nWebGPT [66] − − − − ② ① 12/2021\nSayCan [78] − − − ① ① ② 04/2022\nMRKL [72] − − − ① ② − 05/2022\nInner Monologue [61] − − − ② ① ② 07/2022\nSocial Simulacra [98] ② − − − ① − 08/2022\nReAct [59] − − − ② ② ① 10/2022\nMALLM [43] − ① ② − ① − 01/2023\nDEPS [33] − − − ② ① ② 02/2023\nToolformer [15] − − − ① ② ① 02/2023\nReflexion [12] − ② ② ② ① ② 03/2023\nCAMEL [99] ① ② − − ② ① − 03/2023\nAPI-Bank [69] − − − ② ② ② 04/2023\nViperGPT [74] − − − − ② − 03/2023\nHuggingGPT [13] − − ① ① ② − 03/2023\nGenerative Agents [20] ① ② ② ② ① − 04/2023\nLLM+P [57] − − − ① ① − 04/2023\nChemCrow [75] − − − ② ② − 04/2023\nOpenAGI [73] − − − ② ② ① 04/2023\nAutoGPT [100] − ① ② ② ② ② 04/2023\nSCM [35] − ② ② − ① − 04/2023\nSocially Alignment [84] − ① ② − ① ① 05/2023\nGITM [16] − ② ② ② ① ② 05/2023\nVoyager [38] − ② ② ② ① ② 05/2023\nIntrospective Tips [101] − − − ② ① ② 05/2023\nRET-LLM [42] − ① ② − ① ① 05/2023\nChatDB [40] − ① ② ② ② − 06/2023\nS 3\n [77] ③ ② ② − ① − 07/2023\nChatDev [18] ① ② ② ② ① ② 07/2023\nToolLLM [14] − − − ② ② ① 07/2023\nMemoryBank [39] − ② ② − ① − 07/2023\nMetaGPT [23] ① ② ② ② ② − 08/2023\n \n \n \nFig. 5    The applications (left) and evaluation strategies (right) of LLM-based agents\n \nLei WANG et al.    A survey on large language model based autonomous agents 13\nand  GPT-4  can  provide  too  perfect  estimates  (called “hyper-\naccuracy  distortion”),  which  may  influence  the  downstream\napplications.  In  [104],  the  authors  systematically  analyze  the\neffectiveness  of  LLM-based  conversation  agents  for  mental\nwell-being  support.  They  collect  120  posts  from  Reddit,  and\nfind that such agents can help users cope with anxieties, social\nisolation  and  depression  on  demand.  At  the  same  time,  they\nalso  find  that  the  agents  may  produce  harmful  contents\nsometimes.\nPolitical science and economy: LLM-based agents can also\nbe  utilized  to  study  political  science  and  economy\n[29,105,106].  In  [29],  LLM-based  agents  are  utilized  for\nideology detection and predicting voting patterns. In [105], the\nauthors  focuses  on  understanding  the  discourse  structure  and\npersuasive elements of political speech through the assistance\nof  LLM-based  agents.  In  [106],  LLM-based  agents  are\nprovided with specific traits such as talents, preferences, and\npersonalities  to  explore  human  economic  behaviors  in\nsimulated scenarios.\nSocial simulation: Previously, conducting experiments with\nhuman  societies  is  often  expensive,  unethical,  or  even\ninfeasible.  With  the  ever  prospering  of  LLMs,  many  people\nexplore  to  build  virtual  environment  with  LLM-based  agents\nto  simulate  social  phenomena,  such  as  the  propagation  of\nharmful  information,  and  so  on  [20,34,77,79,107–110].  For\nexample,  Social  Simulacra  [79]  simulates  an  online  social\ncommunity and explores the potential of utilizing agent-based\nsimulations  to  aid  decision-makers  to  improve  community\nregulations.  [107,108]  investigates  the  potential  impacts  of\ndifferent  behavioral  characteristics  of  LLM-based  agents  in\nsocial networks. Generative Agents [20] and AgentSims [34]\nconstruct  multiple  agents  in  a  virtual  town  to  simulate  the\nhuman daily life. SocialAI School [109] employs LLM-based\nagents  to  simulate  and  investigate  the  fundamental  social\ncognitive  skills  during  the  course  of  child  development.  S3\n[77]  builds  a  social  network  simulator,  focusing  on  the\npropagation of information, emotion and attitude. CGMI [111]\nis  a  framework  for  multi-agent  simulation.  CGMI  maintains\nthe  personality  of  the  agents  through  a  tree  structure  and\nconstructs  a  cognitive  model.  The  authors  simulated  a\nclassroom scenario using CGMI.\nJurisprudence:  LLM-based  agents  can  serve  as  aids  in\nlegal  decision-making  processes,  facilitating  more  informed\njudgements  [112,113].  Blind  Judgement  [113]  employs\nseveral  language  models  to  simulate  the  decision-making\nprocesses  of  multiple  judges.  It  gathers  diverse  opinions  and\nconsolidates  the  outcomes  through  a  voting  mechanism.\nChatLaw [112] is a prominent Chinese legal model based on\nLLM.  It  adeptly  supports  both  database  and  keyword  search\nstrategies,  specifically  designed  to  mitigate  the  hallucination\nissue  prevalent  in  such  models.  In  addition,  this  model  also\nemploys  self-attention  mechanism  to  enhance  the  LLM’s\ncapability via mitigating the impact of reference inaccuracies.\nResearch assistant: Beyond their application in specialized\ndomains,  LLM-based  agents  are  increasingly  adopted  as\nversatile assistants in the broad field of social science research\n[105,114].  In  [105],  LLM-based  agents  offer  multifaceted\nassistance,  ranging  from  generating  concise  article  abstracts\nand extracting pivotal keywords to crafting detailed scripts for\nstudies,  showcasing  their  ability  to  enrich  and  streamline  the\nresearch  process.  Meanwhile,  in  [114],  LLM-based  agents\nserve  as  a  writing  assistant,  demonstrating  their  capability  to\nidentify  novel  research  inquiries  for  social  scientists,  thereby\nopening  new  avenues  for  exploration  and  innovation  in  the\nfield.  These  examples  highlight  the  potential  of  LLM-based\nagents  in  enhancing  the  efficiency,  creativity,  and  breadth  of\nsocial science research.\n 3.2    Natural science\nNatural  science  is  one  of  the  branches  of  science  concerned\nwith  the  description,  understanding  and  prediction  of  natural\nphenomena,  based  on  empirical  evidence  from  observation\nand  experimentation.  With  the  ever  prospering  of  LLMs,  the\napplication of LLM-based agents in natural sciences becomes\nmore  and  more  popular.  In  the  following,  we  present  many\nrepresentative  areas,  where  LLM-based  agents  can  play\nimportant roles.\nDocumentation and data management: Natural scientific\nresearch  often  involves  the  collection,  organization,  and\nsynthesis of substantial amounts of literature, which requires a\nsignificant  dedication  of  time  and  human  resources.  LLM-\nbased  agents  have  shown  strong  capabilities  on  language\nunderstanding  and  employing  tools  such  as  the  internet  and\ndatabases for text processing. These capabilities empower the\nagent  to  excel  in  tasks  related  to  documentation  and  data\nmanagement [75,115,116]. In [115], the agent can efficiently\nquery and utilize internet information to complete tasks such\nas  question  answering  and  experiment  planning.  ChatMOF\n[116] utilizes LLMs to extract important information from text\ndescriptions  written  by  humans.  It  then  formulates  a  plan  to\napply relevant tools for predicting the properties and structures\nof  metal-organic  frameworks.  ChemCrow  [75]  utilizes\nchemistry-related  databases  to  both  validate  the  precision  of\ncompound  representations  and  identify  potentially  dangerous\nsubstances.  This  functionality  enhances  the  reliability  and\ncomprehensiveness  of  scientific  inquiries  by  ensuring  the\naccuracy of the data involved.\nExperiment  assistant:  LLM-based  agents  have  the  ability\nto independently conduct experiments, making them valuable\ntools  for  supporting  scientists  in  their  research  projects\n[75,115].  For  instance,  [115]  introduces  an  innovative  agent\nsystem that utilizes LLMs for automating the design, planning,\nand  execution  of  scientific  experiments.  This  system,  when\nprovided  with  the  experimental  objectives  as  input,  accesses\nthe  Internet  and  retrieves  relevant  documents  to  gather  the\nnecessary information. It subsequently utilizes Python code to\nconduct  essential  calculations  and  carry  out  the  following\nexperiments.  ChemCrow  [75]  incorporates  17  carefully\ndeveloped  tools  that  are  specifically  designed  to  assist\nresearchers  in  their  chemical  research.  Once  the  input\nobjectives  are  received,  ChemCrow  provides  valuable\nrecommendations  for  experimental  procedures,  while  also\nemphasizing  any  potential  safety  risks  associated  with  the\nproposed experiments.\nNatural  science  education:  LLM-based  agents  can\ncommunicate  with  humans  fluently,  often  being  utilized  to\n14 Front. Comput. Sci., 2024, 18(6): 186345\ndevelop  agent-based  educational  tools  [115,117–119].  For\nexample,  [115]  develops  agent-based  education  systems  to\nfacilitate  students  learning  of  experimental  design,\nmethodologies, and analysis. The objective of these systems is\nto  enhance  students’ critical  thinking  and  problem-solving\nskills,  while  also  fostering  a  deeper  comprehension  of\nscientific principles. Math Agents [117] can assist researchers\nin  exploring,  discovering,  solving  and  proving  mathematical\nproblems. Additionally, it can communicate with humans and\naids  them  in  understanding  and  using  mathematics.  [118]\nutilize the capabilities of CodeX [119] to automatically solve\nand  explain  university-level  mathematical  problems,  which\ncan  be  used  as  education  tools  to  teach  students  and\nresearchers.  CodeHelp  [120]  is  an  education  agent  for\nprogramming.  It  offers  many  useful  features,  such  as  setting\ncourse-specific  keywords,  monitoring  student  queries,  and\nproviding  feedback  to  the  system.  EduChat  [81]  is  an  LLM-\nbased agent designed specifically for the education domain. It\nprovides  personalized,  equitable,  and  empathetic  educational\nsupport  to  teachers,  students,  and  parents  through  dialogue.\nFreeText [121] is an agent that utilizes LLMs to automatically\nassess  students’ responses  to  open-ended  questions  and  offer\nfeedback.\n 3.3    Engineering\nLLM-based autonomous agents have shown great potential in\nassisting and enhancing engineering research and applications.\nIn this section, we review and summarize the applications of\nLLM-based agents in several major engineering domains.\nCivil engineering: In civil engineering, LLM-based agents\ncan be used to design and optimize complex structures such as\nbuildings, bridges, dams, roads. [122] proposes an interactive\nframework  where  human  architects  and  agents  collaborate  to\nconstruct  structures  in  a  3D  simulation  environment.  The\ninteractive agent can understand natural language instructions,\nplace  blocks,  detect  confusion,  seek  clarification,  and\nincorporate  human  feedback,  showing  the  potential  for\nhuman-AI collaboration in engineering design.\nComputer  science  &  software  engineering:  In  the  field\nof  computer  science  and  software  engineering,  LLM-\nbased  agents  offer  potential  for  automating  coding,\ntesting,  debugging,  and  documentation  generation\n[14,18,23,24,123–125]. ChatDev [18] proposes an end-to-end\nframework,  where  multiple  agent  roles  communicate  and\ncollaborate  through  natural  language  conversations  to\ncomplete the software development life cycle. This framework\ndemonstrates  efficient  and  cost-effective  generation  of\nexecutable software systems. ToolBench [14] can be used for\ntasks such as code auto-completion and code recommendation.\nMetaGPT  [23]  abstracts  multiple  roles,  such  as  product\nmanagers,  architects,  project  managers,  and  engineers,  to\nsupervise code generation process and enhance the quality of\nthe  final  output  code.  This  enables  low-cost  software\ndevelopment. [24] presents a self-collaboration framework for\ncode  generation  using  LLMs.  In  this  framework,  multiple\nLLMs  are  assumed  to  be  distinct “experts” for  specific  sub-\ntasks.  They  collaborate  and  interact  according  to  specified\ninstructions, forming a virtual team that facilitates each other’s\nwork.  Ultimately,  the  virtual  team  collaboratively  addresses\ncode  generation  tasks  without  requiring  human  intervention.\nLLIFT  [126]  employs  LLMs  to  assist  in  conducting  static\nanalysis,  specifically  for  identifying  potential  code\nvulnerabilities.  This  approach  effectively  manages  the  trade-\noff  between  accuracy  and  scalability.  ChatEDA  [127]  is  an\nagent  developed  for  electronic  design  automation  (EDA)  to\nstreamline  the  design  process  by  integrating  task  planning,\nscript generation, and execution. CodeHelp [120] is an agent\ndesigned  to  assist  students  and  developers  in  debugging  and\ntesting  their  code.  Its  features  include  providing  detailed\nexplanations of error messages, suggesting potential fixes, and\nensuring  the  accuracy  of  the  code.  PENTESTGPT  [128]  is  a\npenetration testing tool based on LLMs, which can effectively\nidentify common vulnerabilities, and interpret source code to\ndevelop  exploits.  DB-GPT  [41]  utilizes  the  capabilities  of\nLLMs  to  systematically  assess  potential  root  causes  of\nanomalies in databases. Through the implementation of a tree\nof thought approach, DB-GPT enables LLMs to backtrack to\nprevious  steps  in  case  the  current  step  proves  unsuccessful,\nthus enhancing the accuracy of the diagnosis process.\nIndustrial  automation:  In  the  field  of  industrial\nautomation,  LLM-based  agents  can  be  used  to  achieve\nintelligent planning and control of production processes. [129]\nproposes  a  novel  framework  that  integrates  large  language\nmodels  (LLMs)  with  digital  twin  systems  to  accommodate\nflexible  production  needs.  The  framework  leverages  prompt\nengineering techniques to create LLM agents that can adapt to\nspecific  tasks  based  on  the  information  provided  by  digital\ntwins.  These  agents  can  coordinate  a  series  of  atomic\nfunctionalities  and  skills  to  complete  production  tasks  at\ndifferent levels within the automation pyramid. This research\ndemonstrates the potential of integrating LLMs into industrial\nautomation  systems,  providing  innovative  solutions  for  more\nagile,  flexible  and  adaptive  production  processes.  IELLM\n[130] showcases a case study on LLMs’ role in the oil and gas\nindustry,  covering  applications  like  rock  physics,  acoustic\nreflectometry, and coiled tubing control.\nRobotics & embodied artificial intelligence: Recent works\nhave  developed  more  efficient  reinforcement  learning  agents\nfor  robotics  and  embodied  artificial  intelligence\n[16,38,78,131–138].  The  focus  is  on  enhancing  autonomous\nagents’ abilities  for  planning,  reasoning,  and  collaboration  in\nembodied environments. In specific, [135] proposes a unified\nagent  system  for  embodied  reasoning  and  task  planning.  In\nthis system, the authors design high-level commands to enable\nimproved  planning  while  propose  low-level  controllers  to\ntranslate  commands  into  actions.  Additionally,  one  can\nleverage  dialogues  to  gather  information  [136]  to  accelerate\nthe  optimization  process.  [137,138]  employ  autonomous\nagents  for  embodied  decision-making  and  exploration.  To\novercome  the  physical  constraints,  the  agents  can  generate\nexecutable  plans  and  accomplish  long-term  tasks  by\nleveraging multiple skills. In terms of control policies, SayCan\n[78]  focuses  on  investigating  a  wide  range  of  manipulation\nand  navigation  skills  utilizing  a  mobile  manipulator  robot.\nTaking inspiration from typical tasks encountered in a kitchen\nenvironment, it presents a comprehensive set of 551 skills that\nLei WANG et al.    A survey on large language model based autonomous agents 15\ncover  seven  skill  families  and  17  objects.  These  skills\nencompass various actions such as picking, placing, pouring,\ngrasping,  and  manipulating  objects,  among  others.  TidyBot\n[139] is an embodied agent designed to personalize household\ncleanup  tasks.  It  can  learn  users’ preferences  on  object\nplacement  and  manipulation  methods  through  textual\nexamples.\nTo  promote  the  application  of  LLM-based  autonomous\nagents,  researchers  have  also  introduced  many  open-source\nlibraries,  based  on  which  the  developers  can  quickly\nimplement and evaluate agents according to their customized\nrequirements [19,108,124,140–153]. For example, LangChain\n[145]  is  an  open-source  framework  that  automates  coding,\ntesting,  debugging,  and  documentation  generation  tasks.  By\nintegrating language models with data sources and facilitating\ninteraction with the environment, LangChain enables efficient\nand  cost-effective  software  development  through  natural\nlanguage  communication  and  collaboration  among  multiple\nagent roles. Based on LangChain, XLang [143] comes with a\ncomprehensive  set  of  tools,  a  complete  user  interface,  and\nsupport  three  different  agent  scenarios,  namely  data\nprocessing,  plugin  usage,  and  Web  agent.  AutoGPT  [100]  is\nan  agent  that  is  fully  automated.  It  sets  one  or  more  goals,\nbreaks  them  down  into  corresponding  tasks,  and  cycles\nthrough the tasks until the goal is achieved. WorkGPT [146] is\nan agent framework similar to AutoGPT and LangChain. By\nproviding it with an instruction and a set of APIs, it engages in\nback-and-forth  conversations  with  AI  until  the  instruction  is\ncompleted.  GPT-Engineer  [125],  SmolModels  [123]  and\nDemoGPT  [124]  are  open-source  projects  that  focus  on\nautomating  code  generation  through  prompts  to  complete\ndevelopment tasks. AGiXT [142] is a dynamic AI automation\nplatform  designed  to  orchestrate  efficient  AI  command\nmanagement  and  task  execution  across  many  providers.\nAgentVerse  [19]  is  a  versatile  framework  that  facilitates\nresearchers  in  creating  customized  LLM-based  agent\nsimulations  efficiently.  GPT  Researcher  [148]  is  an\nexperimental application that leverages large language models\nto  efficiently  develop  research  questions,  trigger  Web  crawls\nto  gather  information,  summarize  sources,  and  aggregate\nsummaries. BMTools [149] is an open-source repository that\nextends  LLMs  with  tools  and  provides  a  platform  for\ncommunity-driven  tool  building  and  sharing.  It  supports\nvarious  types  of  tools,  enables  simultaneous  task  execution\nusing multiple tools, and offers a simple interface for loading\nplugins  via  URLs,  fostering  easy  development  and\ncontribution to the BMTools ecosystem.\nRemark. Utilization of LLM-based agents in supporting above\napplications  may  also  entail  risks  and  challenges.  On  one\nhand,  LLMs  themselves  may  be  susceptible  to  illusions  and\nother  issues,  occasionally  providing  erroneous  answers,\nleading to incorrect conclusions, experimental failures, or even\nposing  risks  to  human  safety  in  hazardous  experiments.\nTherefore,  during  experimentation,  users  must  possess  the\nnecessary  expertise  and  knowledge  to  exercise  appropriate\ncaution.  On  the  other  hand,  LLM-based  agents  could\npotentially  be  exploited  for  malicious  purposes,  such  as  the\ndevelopment  of  chemical  weapons,  necessitating  the\nimplementation  of  security  measures,  such  as  human\nalignment, to ensure responsible and ethical use.\nIn summary, in the above sections, we introduce the typical\napplications  of  LLM-based  autonomous  agents  in  three\nimportant  domains.  To  facilitate  a  clearer  understanding,  we\nhave  summarized  the  relationship  between  previous  studies\nand their respective applications in Table 2.\n 4    LLM-based autonomous agent\nevaluation\nSimilar  to  LLMs  themselves,  evaluating  the  effectiveness  of\nLLM-based  autonomous  agents  is  a  challenging  task.  This\nsection  outlines  two  prevalent  approaches  to  evaluation:\nsubjective  and  objective  methods.  For  a  comprehensive\noverview, please refer to the right portion of Fig. 5.\n 4.1    Subjective evaluation\nSubjective evaluation measures the agent capabilities based on\nhuman  judgements  [20,22,29,79,158].  It  is  suitable  for  the\nscenarios where there are no evaluation datasets or it is very\n   \nTable 2    Representative applications of LLM-based autonomous agents\nDomain Work\nSocial\nScience\nPsychology TE [102], Akata et al. [103], Ziems et al. [105], Ma et al. [104]\nPolitical Science and\nEconomy Out of One [29], Horton [106], Ziems et al. [105]\nSocial Simulation Social Simulacra [79], Generative Agents [20], SocialAI School [109], AgentSims [34],\nS3 [77], Williams et al. [110], Li et al. [107], Chao et al. [108]\nJurisprudence ChatLaw [112], Blind Judgement [113]\nResearch Assistant Ziems et al. [105], Bail et al. [114]\nNatural\nScience\nDocumentation and\nData Management ChemCrow [\n75], Boiko et al. [115]\nExperiment Assistant ChemCrow [75], Boiko et al. [115], Grossmann et al. [154]\nNatural Science\nEducation ChemCrow [75], CodeHelp [120], Boiko et al. [115], MathAgent [117], Drori et al. [118]\nEngineering\nCS & SE RestGPT [70], Self-collaboration [24], SQL-PALM [86], RAH [88], DB-GPT [41], RecMind [51], ChatEDA [127],\nInteRecAgent [155], PentestGPT [128], CodeHelp [120], SmolModels [123], DemoGPT [124], GPTEngineer [125]\nIndustrial\nAutomation GPT4IA [129], IELLM [130], TaskMatrix.AI [71]\nRobotics &\nEmbodied AI\nProAgent [156], LLM4RL [131], PET [132], REMEMBERER [133], DEPS [33], Unified Agent [134], SayCan [78],\nLMMWM [157], TidyBot [139], RoCo [89], SayPlan [31]\n \n16 Front. Comput. Sci., 2024, 18(6): 186345\nhard  to  design  quantitative  metrics,  for  example,  evaluating\nthe agent’s intelligence or user-friendliness. In the following,\nwe  present  two  commonly  used  strategies  for  subjective\nevaluation.\nHuman  annotation:  This  evaluation  method  involves\nhuman  evaluators  directly  scoring  or  ranking  the  outputs\ngenerated by various agents [22,29,102]. For example, in [20],\nthe authors employ many annotators, and ask them to provide\nfeedback  on  five  key  questions  that  directly  associated  with\nthe  agent  capability.  Similarly,  [159]  assess  model\neffectiveness by having human participants rate the models on\nharmlessness,  honesty,  helpfulness,  engagement,  and\nunbiasedness,  subsequently  comparing  these  scores  across\ndifferent  models.  In  [79],  annotators  are  asked  to  determine\nwhether  the  specifically  designed  models  can  significantly\nenhance the development of rules within online communities.\nTuring  test:  This  evaluation  strategy  necessitates  that\nhuman  evaluators  differentiate  between  outputs  produced  by\nagents  and  those  created  by  humans.  If,  in  a  given  task,  the\nevaluators  cannot  separate  the  agent  and  human  results,  it\ndemonstrates  that  the  agent  can  achieve  human-like\nperformance  on  this  task.  For  instance,  researchers  in  [29]\nconduct  experiments  on  free-form  Partisan  text,  and  the\nhuman evaluators are asked to guess whether the responses are\nfrom  human  or  LLM-based  agent.  In  [20],  the  human\nevaluators  are  required  to  identify  whether  the  behaviors  are\ngenerated  from  the  agents  or  real-humans.  In  EmotionBench\n[160],  human  annotations  are  collected  to  compare  the\nemotional  states  expressed  by  LLM  software  and  human\nparticipants across various scenarios. This comparison serves\nas  a  benchmark  for  evaluating  the  emotional  intelligence  of\nthe  LLM  software,  illustrating  a  nuanced  approach  to\nunderstanding  agent  capabilities  in  mimicking  human-like\nperformance and emotional expression.\nRemark.  LLM-based  agents  are  usually  designed  to  serve\nhumans. Thus, subjective agent evaluation plays a critical role,\nsince  it  reflects  human  criterion.  However,  this  strategy  also\nfaces  issues  such  as  high  costs,  inefficiency,  and  population\nbias. To address these issues, a growing number of researchers\nare  investigating  the  use  of  LLMs  themselves  as\nintermediaries  for  carrying  out  these  subjective  assessments.\nFor  example,  in  ChemCrow  [75],  researchers  assess  the\nexperimental  results  using  GPT.  They  consider  both  the\ncompletion  of  tasks  and  the  accuracy  of  the  underlying\nprocesses.  Similarly,  ChatEval  [161]  introduces  a  novel\napproach by employing multiple agents to critique and assess\nthe  results  generated  by  various  candidate  models  in  a\nstructured  debate  format.  This  innovative  use  of  LLMs  for\nevaluation  purposes  holds  promise  for  enhancing  both  the\ncredibility  and  applicability  of  subjective  assessments  in  the\nfuture.  As  LLM  technology  continues  to  evolve,  it  is\nanticipated  that  these  methods  will  become  increasingly\nreliable and find broader applications, thereby overcoming the\ncurrent limitations of direct human evaluation.\n 4.2    Objective evaluation\nObjective  evaluation  refers  to  assessing  the  capabilities  of\nLLM-based autonomous agents using quantitative metrics that\ncan be computed, compared and tracked over time. In contrast\nto  subjective  evaluation,  objective  metrics  aim  to  provide\nconcrete, measurable insights into the agent performance. For\nconducting  objective  evaluation,  there  are  three  important\naspects,  that  is,  the  evaluation  metrics,  protocols  and\nbenchmarks.  In  the  following,  we  introduce  these  aspects\nmore in detail.\nMetrics: In order to objectively evaluate the effectiveness of\nthe agents, designing proper metrics is significant, which may\ninfluence  the  evaluation  accuracy  and  comprehensiveness.\nIdeal evaluation metrics should precisely reflect the quality of\nthe  agents,  and  align  with  the  human  feelings  when  using\nthem  in  real-world  scenarios.  In  existing  work,  we  can\nconclude  the  following  representative  evaluation  metrics.\n(1) Task success metrics: These metrics measure how well an\nagent can complete tasks and achieve goals. Common metrics\ninclude  success  rate  [12,22,57,59],  reward/score  [22,59,122],\ncoverage  [16],  and  accuracy  [18,40,102].  Higher  values\nindicate greater task completion ability. (2) Human similarity\nmetrics: These metrics quantify the degree to which the agent\nbehaviors closely resembles that of humans. Typical examples\ninclude  trajectory/location  accuracy  [38,162],  dialogue\nsimilarities  [79,102],  and  mimicry  of  human  responses\n[29,102].  Higher  similarity  suggests  better  human  simulation\nperformance.  (3) Efficiency  metrics:  In  contrast  to  the\naforementioned  metrics  used  to  evaluate  the  agent\neffectiveness,  these  metrics  aim  to  assess  the  efficiency  of\nagent. Commonly considered metrics encompass the length of\nplanning [57], the cost associated with development [18], the\nspeed  of  inference  [16,38],  and  number  of  clarification\ndialogues [122].\nProtocols:  In  addition  to  the  evaluation  metrics,  another\nimportant  aspect  for  objective  evaluation  is  how  to  leverage\nthese  metrics.  In  the  previous  work,  we  can  identify  the\nfollowing  commonly  used  evaluation  protocols:  (1) Real-\nworld  simulation:  In  this  method,  the  agents  are  evaluated\nwithin  immersive  environments  like  games  and  interactive\nsimulators.  The  agents  are  required  to  perform  tasks\nautonomously,  and  then  metrics  like  task  success  rate  and\nhuman  similarity  are  leveraged  to  evaluate  the  capability  of\nthe agents based on their trajectories and completed objectives\n[16,22,33,38,59,80,122,162,163,164]. This method is expected\nto  evaluate  the  agents’ practical  capabilities  in  real-world\nscenarios. (2) Social evaluation: This method utilizes metrics\nto assess social intelligence based on the agent interactions in\nsimulated  societies.  Various  approaches  have  been  adopted,\nsuch  as  collaborative  tasks  to  evaluate  teamwork  skills,\ndebates  to  analyze  argumentative  reasoning,  and  human\nstudies to measure social aptitude [34,98,102,165,166]. These\napproaches  analyze  qualities  such  as  coherence,  theory  of\nmind,  and  social  IQ  to  assess  agents’ capabilities  in  areas\nincluding  cooperation,  communication,  empathy,  and\nmimicking  human  social  behavior.  By  subjecting  agents  to\ncomplex  interactive  settings,  social  evaluation  provides\nvaluable  insights  into  agents’ higher-level  social  cognition.\n(3) Multi-task  evaluation:  In  this  method,  people  use  a  set\nof  diverse  tasks  from  different  domains  to  evaluate  the\nagent,  which  can  effectively  measure  the  agent\nLei WANG et al.    A survey on large language model based autonomous agents 17\ngeneralization  capability  in  open-domain  environments\n[29,80,153,163,165,166,167].  (4) Software  testing:  In  this\nmethod,  researchers  evaluate  the  agents  by  letting  them\nconduct  tasks  such  as  software  testing  tasks,  such  as\ngenerating test cases, reproducing bugs, debugging code, and\ninteracting  with  developers  and  external  tools\n[166,168,169,170].  Then,  one  can  use  metrics  like  test\ncoverage and bug detection rate to measure the effectiveness\nof LLM-based agents.\nBenchmarks:  Given  the  metrics  and  protocols,  a  crucial\nremaining aspect is the selection of an appropriate benchmark\nfor  conducting  the  evaluation.  In  the  past,  people  have  used\nvarious benchmarks in their experiments. For example, many\nresearchers use simulation environments like ALFWorld [59],\nIGLU  [122],  and  Minecraft  [16,33,38]  as  benchmarks  to\nevaluate  the  agent  capabilities.  Tachikuma  [164]  is  a\nbenchmark that leverages TRPG game logs to evaluate LLMs’\nability  to  understand  and  infer  complex  interactions  with\nmultiple  characters  and  novel  objects.  AgentBench  [167]\nprovides a comprehensive framework for evaluating LLMs as\nautonomous agents across diverse environments. It represents\nthe  first  systematic  assessment  of  LLMs  as  agents  on  real-\nworld challenges across diverse domains. SocKET [165] is a\ncomprehensive  benchmark  for  evaluating  the  social\ncapabilities of LLMs across 58 tasks covering five categories\nof  social  information  such  as  humor  and  sarcasm,  emotions\nand  feelings,  and  credibility.  AgentSims  [34]  is  a  versatile\nframework  for  evaluating  LLM-based  agents,  where  one  can\nflexibly  design  the  agent  planning,  memory  and  action\nstrategies,  and  measure  the  effectiveness  of  different  agent\nmodules  in  interactive  environments.  ToolBench  [149]  is  an\nopen-source  project  that  aims  to  support  the  development  of\npowerful  LLMs  with  general  tool-use  capability.  It  provides\nan  open  platform  for  training,  serving,  and  evaluating  LLMs\nbased on tool learning. WebShop [80] develops a benchmark\nfor evaluating LLM-based agents in terms of their capabilities\nfor product search and retrieval. The benchmark is constructed\nusing  a  collection  of  1.18  million  real-world  items.  Mobile-\nEnv [163] is an extendable interactive platform which can be\nused  to  evaluate  the  multi-step  interaction  capabilities  of\nLLM-based  agents.  WebArena  [171]  offers  a  comprehensive\nwebsite environment that spans multiple domains. Its purpose\nis  to  evaluate  agents  in  an  end-to-end  fashion  and  determine\nthe  accuracy  of  their  completed  tasks.  GentBench  [172]  is  a\nbenchmark  designed  to  evaluate  the  agent  capabilities,\nincluding  their  reasoning,  safety,  and  efficiency,  when\nutilizing tools to complete complex tasks. RocoBench [89] is a\nbenchmark with six tasks evaluating multi-agent collaboration\nacross  diverse  scenarios,  emphasizing  communication  and\ncoordination  strategies  to  assess  adaptability  and\ngeneralization  in  cooperative  robotics.  EmotionBench  [160]\nevaluates  the  emotion  appraisal  ability  of  LLMs,  i.e.,  how\ntheir feelings change when presented with specific situations.\nIt  collects  over  400  situations  that  elicit  eight  negative\nemotions  and  measures  the  emotional  states  of  LLMs  and\nhuman  subjects  using  self-report  scales.  PEB  [128]  is  a\nbenchmark  tailored  for  assessing  LLM-based  agents  in\npenetration  testing  scenarios,  comprising  13  diverse  targets\nfrom leading platforms. It offers a structured evaluation across\nvarying  difficulty  levels,  reflecting  real-world  challenges  for\nagents.  ClemBench  [173]  contains  five  Dialogue  Games  to\nassess LLMs’ ability as a player. E2E [174] is an end-to-end\nbenchmark for testing the accuracy and usefulness of chatbots.\nRemark.  Objective  evaluation  facilitates  the  quantitative\nanalysis of capabilities in LLM-based agents through a variety\nof metrics. While current techniques can not perfectly measure\nall  types  of  agent  capabilities,  objective  evaluation  provides\nessential  insights  that  complement  subjective  assessment.\nContinued  advancements  in  benchmarks  and  methodologies\nfor  objective  evaluation  will  enhance  the  development  and\nunderstanding of LLM-based autonomous agents further.\nIn  the  above  sections,  we  introduce  both  subjective  and\nobjective  strategies  for  LLM-based  autonomous  agents\nevaluation. The evaluation of the agents play significant roles\nin  this  domain.  However,  both  subjective  and  objective\nevaluation have their own strengths and weakness. Maybe, in\npractice,  they  should  be  combined  to  comprehensively\nevaluate  the  agents.  We  summarize  the  correspondence\nbetween the previous work and these evaluation strategies in\nTable 3.\n 5    Related surveys\nWith  the  vigorous  development  of  large  language  models,  a\nvariety  of  comprehensive  surveys  have  emerged,  providing\ndetailed  insights  into  various  aspects.  [176]  extensively\nintroduces  the  background,  main  findings,  and  mainstream\ntechnologies of LLMs, encompassing a vast array of existing\nworks.  On  the  other  hand,  [177]  primarily  focus  on  the\napplications  of  LLMs  in  various  downstream  tasks  and  the\nchallenges  associated  with  their  deployment.  Aligning  LLMs\nwith  human  intelligence  is  an  active  area  of  research  to\naddress  concerns  such  as  biases  and  illusions.  [178]  have\ncompiled existing techniques for human alignment, including\ndata collection and model training methodologies. Reasoning\nis  a  crucial  aspect  of  intelligence,  influencing  decision-\nmaking,  problem-solving,  and  other  cognitive  abilities.  [179]\npresents  the  current  state  of  research  on  LLMs’ reasoning\nabilities,  exploring  approaches  to  improve  and  evaluate  their\nreasoning  skills.  [180]  propose  that  language  models  can  be\nenhanced with reasoning capabilities and the ability to utilize\ntools,  termed  Augmented  Language  Models  (ALMs).  They\nconduct a comprehensive review of the latest advancements in\nALMs. As the utilization of large-scale models becomes more\nprevalent, evaluating their performance is increasingly critical.\n[181]  shed  light  on  evaluating  LLMs,  addressing  what  to\nevaluate,  where  to  evaluate,  and  how  to  assess  their\nperformance  in  downstream  tasks  and  societal  impact.  [182]\nalso  discusses  the  capabilities  and  limitations  of  LLMs  in\nvarious  downstream  tasks.  The  aforementioned  research\nencompasses  various  aspects  of  large  models,  including\ntraining,  application,  and  evaluation.  However,  prior  to  this\npaper,  no  work  has  specifically  focused  on  the  rapidly\nemerging and highly promising field of LLM-based Agents. In\nthis  study,  we  have  compiled  100  relevant  works  on  LLM-\nbased  Agents,  covering  their  construction,  applications,  and\nevaluation processes.\n18 Front. Comput. Sci., 2024, 18(6): 186345\n 6    Challenges\nWhile  previous  work  on  LLM-based  autonomous  agent  has\nobtained  many  remarkable  successes,  this  field  is  still  at  its\ninitial  stage,  and  there  are  several  significant  challenges  that\nneed to be addressed in its development. In the following, we\npresent many representative challenges.\n 6.1    Role-playing capability\nDifferent  from  traditional  LLMs,  autonomous  agent  usually\nhas to play as specific roles (e.g., program coder, researcher,\nand  chemist)  for  accomplishing  different  tasks.  Thus,  the\ncapability  of  the  agent  for  role-playing  is  very  important.\nAlthough LLMs can effectively simulate many common roles\nsuch  as  movie  reviewers,  there  are  still  various  roles  and\naspects that they struggle to capture accurately. To begin with,\nLLMs  are  usually  trained  based  on  web-corpus,  thus  for  the\nroles  which  are  seldom  discussed  on  the  Web  or  the  newly\nemerging  roles,  LLMs  may  not  simulate  them  well.  In\naddition, previous research [30] has shown that existing LLMs\nmay  not  well  model  the  human  cognitive  psychology\ncharacters,  leading  to  the  lack  of  self-awareness  in\nconversation  scenarios.  Potential  solution  to  these  problems\nmay  include  fine-tuning  LLMs  or  carefully  designing  the\nagent  prompts/architectures  [183].  For  example,  one  can\nfirstly  collect  real-human  data  for  uncommon  roles  or\npsychology characters, and then leverage it to fine-tune LLMs.\nHowever,  how  to  ensure  that  fine-tuned  model  still  perform\nwell  for  the  common  roles  may  pose  further  challenges.\nBeyond  fine-tuning,  one  can  also  design  tailored  agent\nprompts/architectures  to  enhance  the  capability  of  LLM  on\nrole-playing.  However,  finding  the  optimal  prompts/\narchitectures is not easy, since their designing spaces are too\nlarge.\n 6.2    Generalized human alignment\nHuman  alignment  has  been  discussed  a  lot  for  traditional\nLLMs.  In  the  field  of  LLM-based  autonomous  agent,\nespecially  when  the  agents  are  leveraged  for  simulation,  we\nbelieve  this  concept  should  be  discussed  more  in  depth.  In\norder  to  better  serve  human-beings,  traditional  LLMs  are\nusually fine-tuned to be aligned with correct human values, for\nexample,  the  agent  should  not  plan  to  make  a  bomb  for\navenging society. However, when the agents are leveraged for\nreal-world  simulation,  an  ideal  simulator  should  be  able  to\nhonestly depict diverse human traits, including the ones with\nincorrect  values.  Actually,  simulating  the  human  negative\naspects can be even more important, since an important goal\nof simulation is to discover and solve problems, and without\n   \n✓\nTable 3    For subjective evaluation, we use ① and ② to represent human annotation and the Turing test, respectively. For objective evaluation, we use ①, ②,\n③, and ④ to represent environment simulation, social evaluation, multi-task evaluation, and software testing, respectively. “ ”  indicates that the evaluations\nare based on benchmarks\nModel Subjective Objective Benchmark Time\nWebShop [80] − ① ③\n✓ 07/2022\nSocial Simulacra [98] ① ② − 08/2022\nTE [102] − ② − 08/2022\nLIBRO [168] − ④ − 09/2022\nReAct [59] − ①\n✓ 10/2022\nOut of One, Many [29] ② ② ③ − 02/2023\nDEPS [33] − ①\n✓ 02/2023\nJalil et al. [169] − ④ − 02/2023\nReflexion [12] − ① ③ − 03/2023\nIGLU [122] − ①\n✓ 04/2023\nGenerative Agents [20] ① ② − − 04/2023\nToolBench [149] − ③\n✓ 04/2023\nGITM [16] − ①\n✓ 05/2023\nTwo-Failures [162] − ③ − 05/2023\nVoyager [38] − ①\n✓ 05/2023\nSocKET [165] − ② ③\n✓ 05/2023\nMobileEnv [163] − ① ③\n✓ 05/2023\nClembench [173] − ① ③\n✓ 05/2023\nDialop [175] − ②\n✓ 06/2023\nFeldt et al. [170] − ④ − 06/2023\nCO-LLM [22] ① ① − 07/2023\nTachikuma [164] ① ①\n✓ 07/2023\nWebArena [171] − ①\n✓ 07/2023\nRocoBench [89] − ① ② ③ − 07/2023\nAgentSims [34] − ② − 08/2023\nAgentBench [167] − ③\n✓ 08/2023\nBOLAA [166] − ① ③ ④\n✓ 08/2023\nGentopia [172] − ③\n✓ 08/2023\nEmotionBench [160] ① −\n✓ 08/2023\nPTB [128] − ④ − 08/2023\n \nLei WANG et al.    A survey on large language model based autonomous agents 19\nnegative aspects means no problem to be solved. For example,\nto simulate the real-world society, we may have to allow the\nagent to plan for making a bomb, and observe how it will act\nto implement the plan as well as the influence of its behaviors.\nBased on these observations, people can make better actions to\nstop  similar  behaviors  in  real-world  society.  Inspired  by  the\nabove  case,  maybe  an  important  problem  for  agent-based\nsimulation  is  how  to  conduct  generalized  human  alignment,\nthat  is,  for  different  purposes  and  applications,  the  agent\nshould be able to align with diverse human values. However,\nexisting  powerful  LLMs  including  ChatGPT  and  GPT-4  are\nmostly  aligned  with  unified  human  values.  Thus,  an\ninteresting  direction  is  how  to “realign” these  models  by\ndesigning proper prompting strategies.\n 6.3    Prompt robustness\nTo ensure rational behavior in agents, it’s a common practice\nfor  designers  to  embed  supplementary  modules,  such  as\nmemory  and  planning  modules,  into  LLMs.  However,  the\ninclusion  of  these  modules  necessitates  the  development  of\nmore  complex  prompts  in  order  to  facilitate  consistent\noperation  and  effective  communication.  Previous  research\n[184,185]  has  highlighted  the  lack  of  robustness  in  prompts\nfor  LLMs,  as  even  minor  alterations  can  yield  substantially\ndifferent  outcomes.  This  issue  becomes  more  pronounced\nwhen constructing autonomous agents, as they encompass not\na  single  prompt  but  a  prompt  framework  that  considers  all\nmodules, wherein the prompt for one module has the potential\nto  influence  others.  Moreover,  the  prompt  frameworks  can\nvary significantly across different LLMs. The development of\na  unified  and  resilient  prompt  framework  applicable  across\ndiverse  LLMs  remains  a  critical  and  unresolved  challenge.\nThere  are  two  potential  solutions  to  the  aforementioned\nproblems: (1) manually crafting the essential prompt elements\nthrough  trial  and  error,  or  (2)  automatically  generating\nprompts using GPT.\n 6.4    Hallucination\nHallucination  poses  a  fundamental  challenge  for  LLMs,\ncharacterized  by  the  models’ tendency  to  produce  false\ninformation with a high level of confidence. This challenge is\nnot limited to LLMs alone but is also a significant concern in\nthe  domain  of  autonomous  agents.  For  instance,  in  [186],  it\nwas  observed  that  when  confronted  with  simplistic\ninstructions  during  code  generation  tasks,  the  agent  may\nexhibit  hallucinatory  behavior.  Hallucination  can  lead  to\nserious  consequences  such  as  incorrect  or  misleading  code,\nsecurity risks, and ethical issues [186]. To mitigate this issue,\nincorporating  human  correction  feedback  directly  into  the\niterative process of human-agent interaction presents a viable\napproach [23]. More discussions on the hallucination problem\ncan be seen in [176].\n 6.5    Knowledge boundary\nA pivotal application of LLM-based autonomous agents lies in\nsimulating  diverse  real-world  human  behaviors  [20].  The\nstudy of human simulation has a long history, and the recent\nsurge  in  interest  can  be  attributed  to  the  remarkable\nadvancements  made  by  LLMs,  which  have  demonstrated\nsignificant  capabilities  in  simulating  human  behavior.\nHowever, it is important to recognize that the power of LLMs\nmay  not  always  be  advantageous.  Specifically,  an  ideal\nsimulation  should  accurately  replicate  human  knowledge.  In\nthis  context,  LLMs  may  display  overwhelming  capabilities,\nbeing  trained  on  a  vast  corpus  of  Web  knowledge  that  far\nexceeds what an average individual might know. The immense\ncapabilities of LLMs can significantly impact the effectiveness\nof simulations. For instance, when attempting to simulate user\nselection behaviors for various movies, it is crucial to ensure\nthat LLMs assume a position of having no prior knowledge of\nthese movies. However, there is a possibility that LLMs have\nalready  acquired  information  about  these  movies.  Without\nimplementing  appropriate  strategies,  LLMs  may  make\ndecisions  based  on  their  extensive  knowledge,  even  though\nreal-world users would not have access to the contents of these\nmovies  beforehand.  Based  on  the  above  example,  we  may\nconclude  that  for  building  believable  agent  simulation\nenvironment,  an  important  problem  is  how  to  constrain  the\nutilization of user-unknown knowledge of LLM.\n 6.6    Efficiency\nDue to their autoregressive architecture, LLMs typically have\nslow inference speeds. However, the agent may need to query\nLLMs  for  each  action  multiple  times,  such  as  extracting\ninformation  from  memory,  make  plans  before  taking  actions\nand  so  on.  Consequently,  the  efficiency  of  agent  actions  is\ngreatly affected by the speed of LLM inference.\n 7    Conclusion\nIn this survey, we systematically summarize existing research\nin the field of LLM-based autonomous agents. We present and\nreview  these  studies  from  three  aspects  including  the\nconstruction,  application,  and  evaluation  of  the  agents.  For\neach of these aspects, we provide a detailed taxonomy to draw\nconnections  among  the  existing  research,  summarizing  the\nmajor techniques and their development histories. In addition\nto  reviewing  the  previous  work,  we  also  propose  several\nchallenges in this field, which are expected to guide potential\nfuture directions.\n Acknowledgements    This  work  was  supported  in  part  by  the  National\nNatural  Science  Foundation  of  China  (Grant  No.  62102420),  the  Beijing\nOutstanding  Young  Scientist  Program  (No.  BJJWZYJH012019100020098),\nIntelligent  Social  Governance  Platform,  Major  Innovation  &  Planning\nInterdisciplinary  Platform  for  the “Double-First  Class” Initiative,  Renmin\nUniversity of China, Public Computing Cloud, Renmin University of China,\nfund for building world-class universities (disciplines) of Renmin University\nof China, Intelligent Social Governance Platform.\n Competing  interests    The  authors  declare  that  they  have  no  competing\ninterests or financial conflicts to disclose.\n Open Access    This article is licensed under a Creative Commons Attribution\n4.0 International License, which permits use, sharing, adaptation, distribution\nand  reproduction  in  any  medium  or  format,  as  long  as  you  give  appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if changes were made.\nThe images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line\nto the material. If material is not included in the article’s Creative Commons\nlicence  and  your  intended  use  is  not  permitted  by  statutory  regulation  or\n20 Front. Comput. Sci., 2024, 18(6): 186345\nexceeds  the  permitted  use,  you  will  need  to  obtain  permission  directly  from\nthe copyright holder.\nTo view a copy of this licence, visit creativecommons.org/licenses/by/4.0/.\nReferences\n Mnih V, Kavukcuoglu K, Silver D, Rusu A A, Veness J, Bellemare M\nG, Graves A, Riedmiller M, Fidjeland A K, Ostrovski G, Petersen S,\nBeattie  C,  Sadik  A,  Antonoglou  I,  King  H,  Kumaran  D,  Wierstra  D,\nLegg S, Hassabis D. Human-level control through deep reinforcement\nlearning. Nature, 2015, 518(7540): 529–533\n1.\n Lillicrap T P, Hunt J J, Pritzel A, Heess N, Erez T, Tassa Y, Silver D,\nWierstra  D.  Continuous  control  with  deep  reinforcement  learning.\n2019, arXiv preprint arXiv: 1509.02971\n2.\n Schulman  J,  Wolski  F,  Dhariwal  P,  Radford  A,  Klimov  O.  Proximal\npolicy  optimization  algorithms.  2017,  arXiv  preprint  arXiv:\n1707.06347\n3.\n Haarnoja T, Zhou A, Abbeel P, Levine S. Soft actor-critic: off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor.\nIn:  Proceedings  of  the  35th  International  Conference  on  Machine\nLearning. 2018, 1861−1870\n4.\n Brown T B, Mann B, Ryder N, Subbiah M, Kaplan J D, Dhariwal P,\nNeelakantan  A,  Shyam  P,  Sastry  G,  Askell  A,  Agarwal  S,  Herbert-\nVoss  A,  Krueger  G,  Henighan  T,  Child  R,  Ramesh  A,  Ziegler  D  M,\nWu J, Winter C, Hesse C, Chen M, Sigler E, Litwin M, Gray S, Chess\nB, Clark J, Berner C, McCandlish S, Radford A, Sutskever I, Amodei\nD. Language models are few-shot learners. In: Proceedings of the 34th\nConference  on  Neural  Information  Processing  Systems.  2020,\n1877−1901\n5.\n Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 2019, 1(8):\n9\n6.\n OpenAI.  GPT-4  technical  report.  2024,  arXiv  preprint  arXiv:\n2303.08774\n7.\n Anthropic.  Model  card  and  evaluations  for  Claude  models.  See\nFiles.anthropic.com/production/images/Model-Card-Claude-2, 2023\n8.\n Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M A, Lacroix T,\nRozière  B,  Goyal  N,  Hambro  E,  Azhar  F,  Rodriguez  A,  Joulin  A,\nGrave E, Lample G. LLaMA: open and efficient foundation language\nmodels. 2023, arXiv preprint arXiv: 2302.13971\n9.\n Touvron H, Martin L, Stone K, Albert P, Almahairi A, et al. Llama 2:\nopen  foundation  and  fine-tuned  chat  models.  2023,  arXiv  preprint\narXiv: 2307.09288\n10.\n Chen X, Li S, Li H, Jiang S, Qi Y, Song L. Generative adversarial user\nmodel  for  reinforcement  learning  based  recommendation  system.  In:\nProceedings  of  the  36th  International  Conference  on  Machine\nLearning. 2019, 1052−1061\n11.\n Shinn  N,  Cassano  F,  Gopinath  A,  rasimhan  K,  Yao  S.  Reflexion:\nlanguage agents with verbal reinforcement learning. NaIn: Proceedings\nof  the  37th  Conference  on  Neural  Information  Processing  Systems.\n2023, 36\n12.\n Shen  Y,  Song  K,  Tan  X,  Li  D,  Lu  W,  Zhuang  Y.  HuggingGPT:\nsolving  AI  tasks  with  chatGPT  and  its  friends  in  hugging  face.  In:\nProceedings of the 37th Conference on Neural Information Processing\nSystems. 2023, 36\n13.\n Qin Y, Liang S, Ye Y, Zhu K, Yan L, Lu Y, Lin Y, Cong X, Tang X,\nQian B, Zhao S, Hong L, Tian R, Xie R, Zhou J, Gerstein M, Li D, Liu\nZ,  Sun  M.  ToolLLM:  facilitating  large  language  models  to  master\n16000+ real-world APIs. 2023, arXiv preprint arXiv: 2307.16789\n14.\n Schick T, Dwivedi-Yu J, Dessì R, Raileanu R, Lomeli M, Hambro E,\nZettlemoyer L, Cancedda N, Scialom T. Toolformer: language models\ncan  teach  themselves  to  use  tools.  In:  Proceedings  of  the  37th\nConference on Neural Information Processing Systems. 2023, 36\n15.\n Zhu X, Chen Y, Tian H, Tao C, Su W, Yang C, Huang G, Li B, Lu L,16.\nWang  X,  Qiao  Y,  Zhang  Z,  Dai  J.  Ghost  in  the  minecraft:  generally\ncapable agents for open-world environments via large language models\nwith  text-based  knowledge  and  memory.  2023,  arXiv  preprint  arXiv:\n2305.17144\n Sclar  M,  Kumar  S,  West  P,  Suhr  A,  Choi  Y,  Tsvetkov  Y.  Minding\nlanguage  models’ (lack  of)  theory  of  mind:  a  plug-and-play  multi-\ncharacter belief tracker. In: Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics. 2023, 13960–13980\n17.\n Qian C, Cong X, Liu W, Yang C, Chen W, Su Y, Dang Y, Li J, Xu J,\nLi S, Liu Z, Sun M. Communicative agents for software development.\n2023, arXiv preprint arXiv: 2307.07924\n18.\n Chen W, Su Y, Zuo J, Yang C, Yuan C, Chan C, Yu H, Lu Y, Hung Y,\nQian C, Qin Y, Cong X, Xie R, Liu Z, Sun M, Zhou, J. Agentverse:\nFacilitating  multi-agent  collaboration  and  exploring  emergent\nbehaviors in agents. arXiv preprint arXiv:2308.10848 .\n19.\n Park  J  S,  O’Brien  J,  Cai  C  J,  Morris  M  R,  Liang  P,  Bernstein  M  S.\nGenerative  agents:  interactive  simulacra  of  human  behavior.  In:\nProceedings  of  the  36th  Annual  ACM  Symposium  on  User  Interface\nSoftware and Technology. 2023, 2\n20.\n Zhang H, Du W, Shan J, Zhou Q, Du Y, Tenenbaum J B, Shu T, Gan\nC.  Building  cooperative  embodied  agents  modularly  with  large\nlanguage models. 2024, arXiv preprint arXiv: 2307.02485\n21.\n Hong  S,  Zhuge  M,  Chen  J,  Zheng  X,  Cheng  Y,  Zhang  C,  Wang  J,\nWang  Z,  Yau  S  K  S,  Lin  Z,  Zhou  L,  Ran  C,  Xiao  L,  Wu  C,\nSchmidhuber  J.  MetaGPT:  meta  programming  for  a  multi-agent\ncollaborative framework. 2023, arXiv preprint arXiv: 2308.00352\n22.\n Dong Y, Jiang X, Jin Z, Li G. Self-collaboration code generation via\nchatGPT. 2023, arXiv preprint arXiv: 2304.07590\n23.\n Serapio-García  G,  Safdari  M,  Crepy  C,  Sun  L,  Fitz  S,  Romero  P,\nAbdulhai M, Faust A, Matarić M. Personality traits in large language\nmodels. 2023, arXiv preprint arXiv: 2307.00184\n24.\n Johnson  J  A.  Measuring  thirty  facets  of  the  five  factor  model  with  a\n120-item public domain inventory: development of the IPIP-NEO-120.\nJournal of Research in Personality, 2014, 51: 78–89\n25.\n John O P, Donahue E M, Kentle R L. Big five inventory. Journal of\npersonality and social psychology, 1991.\n26.\n Deshpande  A,  Murahari  V,  Rajpurohit  T,  Kalyan  A,  Narasimhan  K.\nToxicity in chatGPT: analyzing persona-assigned language models. In:\nProceedings  of  Findings  of  the  Association  for  Computational\nLinguistics. 2023, 1236–1270\n27.\n Wang L, Zhang J, Yang H, Chen Z, Tang J, Zhang Z, Chen X, Lin Y,\nSong  R,  Zhao  W  X,  Xu  J,  Dou  Z,  Wang  J,  Wen  J  R.  User  behavior\nsimulation  with  large  language  model  based  agents.  2024,  arXiv\npreprint arXiv: 2306.02552\n28.\n Argyle L P, Busby E C, Fulda N, Gubler J R, Rytting C, Wingate D.\nOut of one, many: using language models to simulate human samples.\nPolitical Analysis, 2023, 31(3): 337–351\n29.\n Fischer  K  A.  Reflective  linguistic  programming  (RLP):  a  stepping\nstone in socially-aware AGI (socialAGI). 2023, arXiv preprint arXiv:\n2305.12647\n30.\n Rana K, Haviland J, Garg S, Abou-Chakra J, Reid I, Suenderhauf N.\nSayPlan: grounding large language models using 3D scene graphs for\nscalable robot task planning. In: Proceedings of the 7th Conference on\nRobot Learning. 2023, 23−72\n31.\n Zhu  A,  Martin  L,  Head  A,  Callison-Burch  C.  CALYPSO:  LLMs  as\ndungeon  master’s  assistants.  In:  Proceedings  of  the  19th  AAAI\nConference  on  Artificial  Intelligence  and  Interactive  Digital\nEntertainment. 2023, 380−390\n32.\n Wang  Z,  Cai  S,  Chen  G,  Liu  A,  Ma  X,  Liang  Y.  Describe,  explain,\nplan  and  select:  interactive  planning  with  large  language  models\nenables  open-world  multi-task  agents.  2023,  arXiv  preprint  arXiv:\n2302.01560\n33.\n Lin J, Zhao H, Zhang A, Wu Y, Ping H, Chen Q. AgentSims: an open-\nsource  sandbox  for  large  language  model  evaluation.  2023,  arXiv\n34.\nLei WANG et al.    A survey on large language model based autonomous agents 21\npreprint arXiv: 2308.04026\n Wang B, Liang X, Yang J, Huang H, Wu S, Wu P, Lu L, Ma Z, Li Z.Enhancing  large  language  model  with  self-controlled  memoryframework. 2024, arXiv preprint arXiv: 2304.13343\n35.\n Ng Y, Miyashita D, Hoshi Y, Morioka Y, Torii O, Kodama T, DeguchiJ.  SimplyRetrieve:  a  private  and  lightweight  retrieval-centricgenerative AI tool. 2023, arXiv preprint arXiv: 2308.03983\n36.\n Huang  Z,  Gutierrez  S,  Kamana  H,  Macneil  S.  Memory  sandbox:\ntransparent  and  interactive  memory  management  for  conversational\nagents. In: Proceedings of the 36th Annual ACM Symposium on User\nInterface Software and Technology. 2023, 97\n37.\n Wang  G,  Xie  Y,  Jiang  Y,  Mandlekar  A,  Xiao  C,  Zhu  Y,  Fan  L,\nAnandkumar  A.  Voyager:  an  open-ended  embodied  agent  with  large\nlanguage models. 2023, arXiv preprint arXiv: 2305.16291\n38.\n Zhong  W,  Guo  L,  Gao  Q,  Ye  H,  Wang  Y.  MemoryBank:  enhancing\nlarge  language  models  with  long-term  memory.  2023,  arXiv  preprint\narXiv: 2305.10250\n39.\n Hu C, Fu J, Du C, Luo S, Zhao J, Zhao H. ChatDB: augmenting LLMs\nwith databases as their symbolic memory. 2023, arXiv preprint arXiv:\n2306.03901\n40.\n Zhou  X,  Li  G,  Liu  Z.  LLM  as  DBA.  2023,  arXiv  preprint  arXiv:\n2308.05481\n41.\n Modarressi A, Imani A, Fayyaz M, Schütze H. RET-LLM: towards a\ngeneral  read-write  memory  for  large  language  models.  2023,  arXiv\npreprint arXiv: 2305.14322\n42.\n Schuurmans  D.  Memory  augmented  large  language  models  are\ncomputationally universal. 2023, arXiv preprint arXiv: 2301.04589\n43.\n Zhao  A,  Huang  D,  Xu  Q,  Lin  M,  Liu  Y  J,  Huang  G.  Expel:  LLM\nagents  are  experiential  learners.  2023,  arXiv  preprint  arXiv:\n2308.10144\n44.\n Wei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F, Chi E H,\nLe Q V, Zhou D. Chain-of-thought prompting elicits reasoning in large\nlanguage  models.  In:  Proceedings  of  the  36th  Conference  on  Neural\nInformation Processing Systems. 2022, 24824−24837\n45.\n Kojima  T,  Gu  S  S,  Reid  M,  Matsuo  Y,  Iwasawa  Y.  Large  language\nmodels are zero-shot reasoners. In: Proceedings of the 36th Conference\non Neural Information Processing Systems. 2022, 22199−22213\n46.\n Raman S S, Cohen V, Rosen E, Idrees I, Paulius D, Tellex S. Planning\nwith  large  language  models  via  corrective  re-prompting.  In:\nProceedings of Foundation Models for Decision Making Workshop at\nNeural Information Processing Systems. 2022\n47.\n Xu B, Peng Z, Lei B, Mukherjee S, Liu Y, Xu D. ReWOO: decoupling\nreasoning from observations for efficient augmented language models.\n2023, arXiv preprint arXiv: 2305.18323\n48.\n Wang  X,  Wei  J,  Schuurmans  D,  Le  Q  V,  Chi  E  H,  Narang  S,\nChowdhery  A,  Zhou  D.  Self-consistency  improves  chain  of  thought\nreasoning  in  language  models.  In:  Proceedings  of  the  11th\nInternational Conference on Learning Representations. 2023\n49.\n Yao S, Yu D, Zhao J, Shafran I, Griffiths T L, Cao Y, Narasimhan K.\nTree  of  thoughts:  deliberate  problem  solving  with  large  language\nmodels. In: Proceedings of the 37th Conference on Neural Information\nProcessing Systems. 2023, 36\n50.\n Wang Y, Jiang Z, Chen Z, Yang F, Zhou Y, Cho E, Fan X, Huang X,\nLu  Y,  Yang  Y.  RecMind:  Large  language  model  powered  agent  for\nrecommendation. 2023, arXiv preprint arXiv: 2308.14296\n51.\n Besta  M,  Blach  N,  Kubicek  A,  Gerstenberger  R,  Podstawski  M,\nGianinazzi  L,  Gajda  J,  Lehmann  T,  Niewiadomski  H,  Nyczyk  P,\nHoefler  T.  Graph  of  thoughts:  solving  elaborate  problems  with  large\nlanguage models. 2024, arXiv preprint arXiv: 2308.09687\n52.\n Sel B, Al-Tawaha A, Khattar V, Jia R, Jin M. Algorithm of thoughts:\nenhancing exploration of ideas in large language models. 2023, arXiv\npreprint arXiv: 2308.10379\n53.\n Huang W, Abbeel P, Pathak D, Mordatch I. Language models as zero-\nshot  planners:  extracting  actionable  knowledge  for  embodied  agents.\n54.\nIn:  Proceedings  of  the  39th  International  Conference  on  Machine\nLearning. 2022, 9118−9147\n Gramopadhye  M,  Szafir  D.  Generating  executable  action  plans  with\nenvironmentally-aware  language  models.  In:  Proceedings  of  2023\nIEEE/RSJ International Conference on Intelligent Robots and Systems.\n2023, 3568−3575\n55.\n Hao S, Gu Y, Ma H, Hong J, Wang Z, Wang D, Hu Z. Reasoning with\nlanguage model is planning with world model. In: Proceedings of 2023\nConference  on  Empirical  Methods  in  Natural  Language  Processing.\n2023, 8154–8173\n56.\n Liu B, Jiang Y, Zhang X, Liu Q, Zhang S, Biswas J, Stone P. LLM+P:\nempowering large language models with optimal planning proficiency.\n2023, arXiv preprint arXiv: 2304.11477\n57.\n Dagan  G,  Keller  F,  Lascarides  A.  Dynamic  planning  with  a  LLM.\n2023, arXiv preprint arXiv: 2308.06391\n58.\n Yao  S,  Zhao  J,  Yu  D,  Du  N,  Shafran  I,  Narasimhan  K  R,  Cao  Y.\nReAct:  synergizing  reasoning  and  acting  in  language  models.  In:\nProceedings  of  the  11th  International  Conference  on  Learning\nRepresentations. 2023\n59.\n Song C H, Sadler B M, Wu J, Chao W L, Washington C, Su Y. LLM-\nplanner:  few-shot  grounded  planning  for  embodied  agents  with  large\nlanguage  models.  In:  Proceedings  of  2023  IEEE/CVF  International\nConference on Computer Vision. 2023, 2986−2997\n60.\n Huang  W,  Xia  F,  Xiao  T,  Chan  H,  Liang  J,  Florence  P,  Zeng  A,\nTompson  J,  Mordatch  I,  Chebotar  Y,  Sermanet  P,  Jackson  T,  Brown\nN,  Luu  L,  Levine  S,  Hausman  K,  Ichter  B.  Inner  monologue:\nembodied  reasoning  through  planning  with  language  models.  In:\nProceedings  of  the  6th  Conference  on  Robot  Learning,  2023,\n1769−1782\n61.\n Madaan A, Tandon N, Gupta P, Hallinan S, Gao L, Wiegreffe S, Alon\nU,  Dziri  N,  Prabhumoye  S,  Yang  Y,  Gupta  S,  Majumder  B  P,\nHermann  K,  Welleck  S,  Yazdanbakhsh  A,  Clark  P.  Self-refine:\niterative  refinement  with  self-feedback.  Advances  in  Neural\nInformation Processing Systems, 2024, 36.\n62.\n Miao N, Teh Y W, Rainforth T. SelfCheck: using LLMs to zero-shot\ncheck  their  own  step-by-step  reasoning.  2023,  arXiv  preprint  arXiv:\n2308.00436\n63.\n Chen P L, Chang C S. InterAct: exploring the potentials of chatGPT as\na cooperative agent. 2023, arXiv preprint arXiv: 2308.01552\n64.\n Chen Z, Zhou K, Zhang B, Gong Z, Zhao X, Wen J R. ChatCoT: tool-\naugmented  chain-of-thought  reasoning  on  chat-based  large  language\nmodels.  In:  Proceedings  of  Findings  of  the  Association  for\nComputational Linguistics. 2023, 14777–14790\n65.\n Nakano R, Hilton J, Balaji S, Wu J, Ouyang L, Kim C, Hesse C, Jain\nS, Kosaraju V, Saunders W, Jiang X, Cobbe K, Eloundou T, Krueger\nG,  Button  K,  Knight  M,  Chess  B,  Schulman  J.  WebGPT:  browser-\nassisted  question-answering  with  human  feedback.  2022,  arXiv\npreprint arXiv: 2112.09332\n66.\n Ruan J, Chen Y, Zhang B, Xu Z, Bao T, Du G, Shi S, Mao H, Li Z,\nZeng  X,  Zhao  R.  TPTU:  large  language  model-based  AI  agents  for\ntask planning and tool usage. 2023, arXiv preprint arXiv: 2308.03427\n67.\n Patil  S  G,  Zhang  T,  Wang  X,  Gonzalez  J  E.  Gorilla:  large  language\nmodel  connected  with  massive  APIs.  2023,  arXiv  preprint  arXiv:\n2305.15334\n68.\n Li M, Zhao Y, Yu B, Song F, Li H, Yu H, Li Z, Huang F, Li Y. API-\nbank:  a  comprehensive  benchmark  for  tool-augmented  LLMs.  In:\nProceedings  of  2023  Conference  on  Empirical  Methods  in  Natural\nLanguage Processing. 2023, 3102–3116\n69.\n Song Y, Xiong W, Zhu D, Wu W, Qian H, Song M, Huang H, Li C,\nWang  K,  Yao  R,  Tian  Y,  Li  S.  RestGPT:  connecting  large  language\nmodels  with  real-world  RESTful  APIs.  2023,  arXiv  preprint  arXiv:\n2306.06624\n70.\n Liang Y, Wu C, Song T, Wu W, Xia Y, Liu Y, Ou Y, Lu S, Ji L, Mao\nS,  Wang  Y,  Shou  L,  Gong  M,  Duan  N.  TaskMatrix.AI:  Completing\n71.\n22 Front. Comput. Sci., 2024, 18(6): 186345\ntasks  by  connecting  foundation  models  with  millions  of  APIs.  2023,\narXiv preprint arXiv: 2303.16434\n Karpas E, Abend O, Belinkov Y, Lenz B, Lieber O, Ratner N, Shoham\nY,  Bata  H,  Levine  Y,  Leyton-Brown  K,  Muhlgay  D,  Rozen  N,\nSchwartz E, Shachaf G, Shalev-Shwartz S, Shashua A, Tenenholtz M.\nMRKL systems: a modular, neuro-symbolic architecture that combines\nlarge  language  models,  external  knowledge  sources  and  discrete\nreasoning. 2022, arXiv preprint arXiv: 2205.00445\n72.\n Ge Y, Hua W, Mei K, Tan J, Xu S, Li Z, Zhang Y. OpenAGI: When\nLLM meets domain experts. In: Proceedings of the 37th Conference on\nNeural Information Processing Systems, 2023, 36\n73.\n Surís D, Menon S, Vondrick C. ViperGPT: visual inference via python\nexecution for reasoning. 2023, arXiv preprint arXiv: 2303.08128\n74.\n Bran A M, Cox S, Schilter O, Baldassari C, White A D, Schwaller P.\nChemCrow:  augmenting  large-language  models  with  chemistry  tools.\n2023, arXiv preprint arXiv: 2304.05376\n75.\n Yang Z, Li L, Wang J, Lin K, Azarnasab E, Ahmed F, Liu Z, Liu C,\nZeng M, Wang L. MM-REACT: Prompting chatGPT for multimodal\nreasoning and action. 2023, arXiv preprint arXiv: 2303.11381\n76.\n Gao C, Lan X, Lu Z, Mao J, Piao J, Wang H, Jin D, Li Y. S3: social-\nnetwork  simulation  system  with  large  language  model-empowered\nagents. 2023, arXiv preprint arXiv: 2307.14984\n77.\n Ichter  B,  Brohan  A,  Chebotar  Y,  Finn  C,  Hausman  K,  et  al.  Do  as  I\ncan,  not  as  I  say:  grounding  language  in  robotic  affordances.  In:\nProceedings of the 6th Conference on Robot Learning. 2023, 287−318\n78.\n Liu  H,  Sferrazza  C,  Abbeel  P.  Chain  of  hindsight  aligns  language\nmodels with feedback. arXiv preprint arXiv: 2302.02676\n79.\n Yao  S,  Chen  H,  Yang  J,  Narasimhan  K.  WebShop:  towards  scalable\nreal-world  Web  interaction  with  grounded  language  agents.  In:\nProceedings of the 36th Conference on Neural Information Processing\nSystems. 2022, 20744−20757\n80.\n Dan Y, Lei Z, Gu Y, Li Y, Yin J, Lin J, Ye L, Tie Z, Zhou Y, Wang Y,\nZhou A, Zhou Z, Chen Q, Zhou J, He L, Qiu X. EduChat: a large-scale\nlanguage model-based chatbot system for intelligent education. 2023,\narXiv preprint arXiv: 2308.02773\n81.\n Lin  B  Y,  Fu  Y,  Yang  K,  Brahman  F,  Huang  S,  Bhagavatula  C,\nAmmanabrolu P, Choi Y, Ren X. SwiftSage: a generative agent with\nfast and slow thinking for complex interactive tasks. In: Proceedings of\nthe 37th Conference on Neural Information Processing Systems. 2023,\n36\n82.\n Evans  J  S  B  T,  Stanovich  K  E.  Dual-process  theories  of  higher\ncognition:  advancing  the  debate.  Perspectives  on  Psychological\nScience, 2013, 8(3): 223–241\n83.\n Liu R, Yang R, Jia C, Zhang G, Zhou D, Dai A M, Yang D, Vosoughi\nS.  Training  socially  aligned  language  models  on  simulated  social\ninteractions. 2023, arXiv preprint arXiv: 2305.16960\n84.\n Weng X, Gu Y, Zheng B, Chen S, Stevens S, Wang B, Sun H, Su Y.\nMind2Web: towards a generalist agent for the Web. In: Proceedings of\nthe 37th Conference on Neural Information Processing Systems. 2023,\n36\n85.\n Sun R, Arik S O, Nakhost H, Dai H, Sinha R, Yin P, Pfister T. SQL-\nPaLm:  improved  large  language  model  adaptation  for  text-to-SQL.\n2023, arXiv preprint arXiv: 2306.00739\n86.\n Yao W, Heinecke S, Niebles J C, Liu Z, Feng Y, Xue L, Murthy R,\nChen Z, Zhang J, Arpit D, Xu R, Mui P, Wang H, Xiong C, Savarese\nS.  Retroformer:  retrospective  large  language  agents  with  policy\ngradient optimization, 2023, arXiv preprint arXiv: 2308.02151\n87.\n Shu Y, Zhang H, Gu H, Zhang P, Lu T, Li D, Gu N. RAH! RecSys-\nassistant-human:  a  human-centered  recommendation  framework  with\nLLM agents. 2023, arXiv preprint arXiv: 2308.09904\n88.\n Mandi  Z,  Jain  S,  Song  S.  RoCo:  dialectic  multi-robot  collaboration\nwith large language models. 2023, arXiv preprint arXiv: 2307.04738\n89.\n Zhang C, Liu L, Wang J, Wang C, Sun X, Wang H, Cai M. PREFER:\nprompt  ensemble  learning  via  feedback-reflect-refine.  2023,  arXiv\n90.\npreprint arXiv: 2308.12033\n Du  Y,  Li  S,  Torralba  A,  Tenenbaum  J  B,  Mordatch  I.  Improving\nfactuality  and  reasoning  in  language  models  through  multiagent\ndebate. 2023, arXiv preprint arXiv: 2305.14325\n91.\n Zhang  C,  Yang  Z,  Liu  J,  Han  Y,  Chen  X,  Huang  Z,  Fu  B,  Yu  G.\nAppAgent:  multimodal  agents  as  smartphone  users.  2023,  arXiv\npreprint arXiv: 2312.13771\n92.\n Madaan  A,  Tandon  N,  Clark  P,  Yang  Y.  Memory-assisted  prompt\nediting  to  improve  GPT-3  after  deployment.  In:  Proceedings  of  2022\nConference  on  Empirical  Methods  in  Natural  Language  Processing.\n2022, 2833–2861\n93.\n Colas  C,  Teodorescu  L,  Oudeyer  P  Y,  Yuan  X,  Côté  M  A.\nAugmenting  autotelic  agents  with  large  language  models.  In:\nProceedings  of  the  2nd  Conference  on  Lifelong  Learning  Agents.\n2023, 205–226\n94.\n Nascimento  N,  Alencar  P,  Cowan  D.  Self-adaptive  large  language\nmodel (LLM)-based multiagent systems. In: Proceedings of 2023 IEEE\nInternational  Conference  on  Autonomic  Computing  and  Self-\nOrganizing Systems Companion. 2023, 104−109\n95.\n Saha S, Hase P, Bansal M. Can language models teach weaker agents?\nTeacher  explanations  improve  students  via  personalization.  2023,\narXiv preprint arXiv: 2306.09299\n96.\n Zhuge M, Liu H, Faccio F, Ashley D R, Csordás R, Gopalakrishnan A,\nHamdi A, Hammoud H A A K, Herrmann V, Irie K, Kirsch L, Li B, Li\nG, Liu S, Mai J, Piękos P, Ramesh A, Schlag I, Shi W, Stanić A, Wang\nW, Wang Y, Xu M, Fan D P, Ghanem B, Schmidhuber J. Mindstorms\nin  natural  language-based  societies  of  mind.  2023,  arXiv  preprint\narXiv: 2305.17066\n97.\n Park  J  S,  Popowski  L,  Cai  C,  Morris  M  R,  Liang  P,  Bernstein  M  S.\nSocial  simulacra:  creating  populated  prototypes  for  social  computing\nsystems.  In:  Proceedings  of  the  35th  Annual  ACM  Symposium  on\nUser Interface Software and Technology. 2022, 74\n98.\n Li  G,  Hammoud  H  A  A  K,  Itani  H,  Khizbullin  D,  Ghanem  B.\nCAMEL:  communicative  agents  for  \"mind\"  exploration  of  large\nlanguage model society. 2023, arXiv preprint arXiv: 2303.17760\n99.\n AutoGPT. See Github.com/Significant-Gravitas/Auto, 2023100.\n Chen L, Wang L, Dong H, Du Y, Yan J, Yang F, Li S, Zhao P, Qin S,\nRajmohan S, Lin Q, Zhang D. Introspective tips: large language model\nfor  in-context  decision  making.  2023,  arXiv  preprint  arXiv:\n2305.11598\n101.\n Aher  G  V,  Arriaga  R  I,  Kalai  A  T.  Using  large  language  models  to\nsimulate  multiple  humans  and  replicate  human  subject  studies.  In:\nProceedings  of  the  40th  International  Conference  on  Machine\nLearning. 2023, 337−371\n102.\n Akata  E,  Schulz  L,  Coda-Forno  J,  Oh  S  J,  Bethge  M,  Schulz  E.\nPlaying  repeated  games  with  large  language  models.  2023,  arXiv\npreprint arXiv: 2305.16867\n103.\n Ma  Z,  Mei  Y,  Su  Z.  Understanding  the  benefits  and  challenges  of\nusing  large  language  model-based  conversational  agents  for  mental\nwell-being  support.  In:  Proceedings  of  AMIA  Symposium.  2023,\n1105−1114\n104.\n Ziems  C,  Held  W,  Shaikh  O,  Chen  J,  Zhang  Z,  Yang  D.  Can  large\nlanguage models transform computational social science? 2024, arXiv\npreprint arXiv: 2305.03514\n105.\n Horton J J. Large language models as simulated economic agents: what\ncan  we  learn  from  homo  silicus?  2023,  arXiv  preprint  arXiv:\n2301.07543\n106.\n Li  S,  Yang  J,  Zhao  K.  Are  you  in  a  masquerade?  Exploring  the\nbehavior  and  impact  of  large  language  model  driven  social  bots  in\nonline social networks. 2023, arXiv preprint arXiv: 2307.10337\n107.\n Li C, Su X, Han H, Xue C, Zheng C, Fan C. Quantifying the impact of\nlarge  language  models  on  collective  opinion  dynamics.  2023,  arXiv\npreprint arXiv: 2308.03313\n108.\n Kovač  G,  Portelas  R,  Dominey  P  F,  Oudeyer  P  Y.  The  SocialAI109.\nLei WANG et al.    A survey on large language model based autonomous agents 23\nschool:  insights  from  developmental  psychology  towards  artificial\nsocio-cultural agents. 2023, arXiv preprint arXiv: 2307.07871\n Williams  R,  Hosseinichimeh  N,  Majumdar  A,  Ghaffarzadegan  N.\nEpidemic  modeling  with  generative  agents.  2023,  arXiv  preprint\narXiv: 2307.04986\n110.\n Shi J, Zhao J, Wang Y, Wu X, Li J, He L. CGMI: configurable general\nmulti-agent  interaction  framework.  2023,  arXiv  preprint  arXiv:\n2308.12503\n111.\n Cui J, Li Z, Yan Y, Chen B, Yuan L. ChatLaw: open-source legal large\nlanguage model with integrated external knowledge bases. 2023, arXiv\npreprint arXiv: 2306.16092\n112.\n Hamilton  S.  Blind  judgement:  agent-based  supreme  court  modelling\nwith GPT. 2023, arXiv preprint arXiv: 2301.05327\n113.\n Bail C A. Can generative AI improve social science? 2023114.\n Boiko D A, MacKnight R, Gomes G. Emergent autonomous scientific\nresearch  capabilities  of  large  language  models.  2023,  arXiv  preprint\narXiv: 2304.05332\n115.\n Kang  Y,  Kim  J.  ChatMOF:  an  autonomous  AI  system  for  predicting\nand generating metal-organic frameworks. 2023, arXiv preprint arXiv:\n2308.01423\n116.\n Swan M, Kido T, Roland E, Santos R P D. Math agents: computational\ninfrastructure,  mathematical  embedding,  and  genomics.  2023,  arXiv\npreprint arXiv: 2307.02502\n117.\n Drori I, Zhang S, Shuttleworth R, Tang L, Lu A, Ke E, Liu K, Chen L,\nTran S, Cheng N, Wang R, Singh N, Patti T L, Lynch J, Shporer A,\nVerma  N,  Wu  E,  Strang  G.  A  neural  network  solves,  explains,  and\ngenerates university math problems by program synthesis and few-shot\nlearning  at  human  level.  Proceedings  of  the  National  Academy  of\nSciences  of  the  United  States  of  America,  2022,  119(32):\ne2123433119\n118.\n Chen  M,  Tworek  J,  Jun  H,  Yuan  Q,  de  Oliveira  Pinto  H  P,  et  al.\nEvaluating  large  language  models  trained  on  code.  2021,  arXiv\npreprint arXiv: 2107.03374\n119.\n Liffiton  M,  Sheese  B  E,  Savelka  J,  Denny  P.  CodeHelp:  using  large\nlanguage models with guardrails for scalable support in programming\nclasses.  In:  Proceedings  of  the  23rd  Koli  Calling  International\nConference on Computing Education Research. 2023, 8\n120.\n Matelsky  J  K,  Parodi  F,  Liu  T,  Lange  R  D,  Kording  K  P.  A  large\nlanguage model-assisted education tool to provide feedback on open-\nended responses. 2023, arXiv preprint arXiv: 2308.02439\n121.\n Mehta N, Teruel M, Sanz P F, Deng X, Awadallah A H, Kiseleva J.\nImproving  grounded  language  understanding  in  a  collaborative\nenvironment  by  interacting  with  agents  through  help  feedback.  2024,\narXiv preprint arXiv: 2304.10750\n122.\n SmolModels. See Githubcom/smol-ai/developer website, 2023123.\n DemoGPT. See Github.com/melih-unsal/Demo website, 2023124.\n GPT-engineer. See Github.com/AntonOsika/gpt website, 2023125.\n Li  H,  Hao  Y,  Zhai  Y,  Qian  Z.  The  hitchhiker’s  guide  to  program\nanalysis:  a  journey  with  large  language  models.  2023,  arXiv  preprint\narXiv: 2308.00245\n126.\n He Z, Wu H, Zhang X, Yao X, Zheng S, Zheng H, Yu B. ChatEDA: a\nlarge  language  model  powered  autonomous  agent  for  EDA.  In:\nProceedings  of  the  5th  ACM/IEEE  Workshop  on  Machine  Learning\nfor CAD. 2023, 1−6\n127.\n Deng G, Liu Y, Mayoral-Vilches V, Liu P, Li Y, Xu Y, Zhang T, Liu\nY,  Pinzger  M,  Rass  S.  PentestGPT:  an  LLM-empowered  automatic\npenetration testing tool. 2023, arXiv preprint arXiv: 2308.06782\n128.\n Xia Y, Shenoy M, Jazdi N, Weyrich M. Towards autonomous system:\nflexible  modular  production  system  enhanced  with  large  language\nmodel  agents.  In:  Proceedings  of  the  2023  IEEE  28th  International\nConference on Emerging Technologies and Factory Automation. 2023,\n1−8\n129.\n Ogundare O, Madasu S, Wiggins N. Industrial engineering with large\nlanguage models: a case study of chatGPT’s performance on oil & gas\n130.\nproblems.  In:  Proceedings  of  the  2023  11th  International  Conference\non Control, Mechatronics and Automation. 2023, 458−461\n Hu  B,  Zhao  C,  Zhang  P,  Zhou  Z,  Yang  Y,  Xu  Z,  Liu  B.  Enabling\nintelligent interactions between an agent and an LLM: a reinforcement\nlearning approach. 2024, arXiv preprint arXiv: 2306.03604\n131.\n Wu Y, Min S Y, Bisk Y, Salakhutdinov R, Azaria A, Li Y, Mitchell T,\nPrabhumoye  S.  Plan,  eliminate,  and  track−language  models  are  good\nteachers for embodied agents. 2023, arXiv preprint arXiv: 2305.02412\n132.\n Zhang  D,  Chen  L,  Zhang  S,  Xu  H,  Zhao  Z,  Yu  K.  Large  language\nmodels  are  semi-parametric  reinforcement  learning  agents.  In:\nProceedings of the 37th Conference on Neural Information Processing\nSystems. 2023, 36\n133.\n Di P N, Byravan A, Hasenclever L, Wulfmeier M, Heess N, Riedmiller\nM.  Towards  a  unified  agent  with  foundation  models.  2023,  arXiv\npreprint arXiv: 2307.09668\n134.\n Dasgupta  I,  Kaeser-Chen  C,  Marino  K,  Ahuja  A,  Babayan  S,  Hill  F,\nFergus  R.  Collaborating  with  language  models  for  embodied\nreasoning. 2023, arXiv preprint arXiv: 2302.00763\n135.\n Zhou W, Peng X, Riedl M O. Dialogue shaping: empowering agents\nthrough NPC interaction. 2023, arXiv preprint arXiv: 2307.15833\n136.\n Nottingham K, Ammanabrolu P, Suhr A, Choi Y, Hajishirzi H, Singh\nS,  Fox  R.  Do  embodied  agents  dream  of  pixelated  sheep:  embodied\ndecision  making  using  language  guided  world  modelling.  In:\nProceedings  of  the  40th  International  Conference  on  Machine\nLearning. 2023, 26311–26325\n137.\n Wu  Z,  Wang  Z,  Xu  X,  Lu  J,  Yan  H.  Embodied  task  planning  with\nlarge language models. 2023, arXiv preprint arXiv: 2307.01848\n138.\n Wu  J,  Antonova  R,  Kan  A,  Lepert  M,  Zeng  A,  Song  S,  Bohg  J,\nRusinkiewicz S, Funkhouser T. TidyBot: personalized robot assistance\nwith  large  language  models.  In:  Proceedings  of  2023  IEEE/RSJ\nInternational  Conference  on  Intelligent  Robots  and  Systems.  2023,\n3546−3553\n139.\n AgentGPT. See Github.com/reworkd/Agent website, 2023140.\n Ai-legion. See Github.com/eumemic/ai website, 2023141.\n AGiXT. See Githubcom/Josh-XT/AGiXT website, 2023142.\n Xlang. See Githubcom/xlang-ai/xlang website, 2023143.\n Babyagi. See Githubcom/yoheinakajima website, 2023144.\n LangChain. See Docs.langchaincom/docs/ website, 2023145.\n WorkGPT. See Githubcom/team-openpm/workgpt website, 2023146.\n LoopGPT. See Githubcom/farizrahman4u/loopgpt website, 2023147.\n GPT-researcher. See Github.com/assafelovic/gpt website, 2023148.\n Qin Y, Hu S, Lin Y, Chen W, Ding N, Cui G, Zeng Z, Huang Y, Xiao\nC, Han C, Fung Y R, Su Y, Wang H, Qian C, Tian R, Zhu K, Liang S,\nShen X, Xu B, Zhang Z, Ye Y, Li B, Tang Z, Yi J, Zhu Y, Dai Z, Yan\nL,  Cong  X,  Lu  Y,  Zhao  W,  Huang  Y,  Yan  J,  Han  X,  Sun  X,  Li  D,\nPhang  J,  Yang  X,  Wu  T,  Ji  H,  Liu  Z,  Sun  M.  Tool  learning  with\nfoundation models. 2023, arXiv preprint arXiv: 2304.08354\n149.\n Transformers  agent.  See  Huggingface.co/docs/transformers/\ntransformers website, 2023\n150.\n Mini-agi. See Github.com/muellerberndt/mini website, 2023151.\n SuperAGI. See Github.com/TransformerOptimus/Super website, 2023152.\n Wu  Q,  Bansal  G,  Zhang  J,  Wu  Y,  Li  B,  Zhu  E,  Jiang  L,  Zhang  X,\nZhang  S,  Liu  J,  Awadallah  A  H,  White  R  W,  Burger  D,  Wang  C.\nAutoGen:  enabling  next-gen  LLM  applications  via  multi-agent\nconversation. 2023, arXiv preprint arXiv: 2308.08155\n153.\n Grossmann I, Feinberg M, Parker D C, Christakis N A, Tetlock P E,\nCunningham  W  A.  AI  and  the  transformation  of  social  science\nresearch: careful bias management and data fidelity are key. Science,\n2023, 380(6650): 1108–1109\n154.\n Huang  X,  Lian  J,  Lei  Y,  Yao  J,  Lian  D,  Xie  X.  Recommender  AI\nagent:  integrating  large  language  models  for  interactive\nrecommendations. 2023, arXiv preprint arXiv: 2308.16505\n155.\n24 Front. Comput. Sci., 2024, 18(6): 186345\n Zhang C, Yang K, Hu S, Wang Z, Li G, Sun Y, Zhang C, Zhang Z, Liu\nA, Zhu S C, Chang X, Zhang J, Yin F, Liang Y, Yang Y. ProAgent:\nbuilding  proactive  cooperative  agents  with  large  language  models.\n2024, arXiv preprint arXiv: 2308.11339\n156.\n Xiang  J,  Tao  T,  Gu  Y,  Shu  T,  Wang  Z,  Yang  Z,  Hu  Z.  Language\nmodels  meet  world  models:  embodied  experiences  enhance  language\nmodels. In: Proceedings of the 37th Conference on Neural Information\nProcessing Systems. 2023, 36\n157.\n Lee M, Srivastava M, Hardy A, Thickstun J, Durmus E, Paranjape A,\nGerard-Ursin I, Li X L, Ladhak F, Rong F, Wang R E, Kwon M, Park\nJ S, Cao H, Lee T, Bommasani R, Bernstein M, Liang P. Evaluating\nhuman-language  model  interaction.  2024,  arXiv  preprint  arXiv:\n2212.09746\n158.\n Krishna R, Lee D, Fei-Fei L, Bernstein M S. Socially situated artificial\nintelligence  enables  learning  from  human  interaction.  Proceedings  of\nthe  National  Academy  of  Sciences  of  the  United  States  of  America,\n2022, 119(39): e2115730119\n159.\n Huang J T, Lam M H, Li E J, Ren S, Wang W, Jiao W, Tu Z, Lyu M\nR. Emotionally numb or empathetic? Evaluating how LLMs feel using\nemotionbench. 2024, arXiv preprint arXiv: 2308.03656\n160.\n Chan  C  M,  Chen  W,  Su  Y,  Yu  J,  Xue  W,  Zhang  S,  Fu  J,  Liu  Z.\nChatEval:  towards  better  LLM-based  evaluators  through  multi-agent\ndebate. 2023, arXiv preprint arXiv: 2308.07201\n161.\n Chen A, Phang J, Parrish A, Padmakumar V, Zhao C, Bowman S R,\nCho K. Two failures of self-consistency in the multi-step reasoning of\nLLMs. 2024, arXiv preprint arXiv: 2305.14279\n162.\n Zhang  D,  Xu  H,  Zhao  Z,  Chen  L,  Cao  R,  Yu  K.  Mobile-env:  an\nevaluation  platform  and  benchmark  for  LLM-GUI  interaction.  2024,\narXiv preprint arXiv: 2305.08144\n163.\n Liang  Y,  Zhu  L,  Yang  Y.  Tachikuma:  understading  complex\ninteractions with multi-character and novel objects by large language\nmodels. 2023, arXiv preprint arXiv: 2307.12573\n164.\n Choi  M,  Pei  J,  Kumar  S,  Shu  C,  Jurgens  D.  Do  LLMs  understand\nsocial knowledge? Evaluating the sociability of large language models\nwith  socKET  benchmark.  In:  Proceedings  of  2023  Conference  on\nEmpirical  Methods  in  Natural  Language  Processing.  2023,\n11370–11403\n165.\n Liu Z, Yao W, Zhang J, Xue L, Heinecke S, Murthy R, Feng Y, Chen\nZ, Niebles J C, Arpit D, Xu R, Mui P, Wang H, Xiong C, Savarese S.\nBOLAA:  benchmarking  and  orchestrating  LLM-augmented\nautonomous agents. 2023, arXiv preprint arXiv: 2308.05960\n166.\n Liu X, Yu H, Zhang H, Xu Y, Lei X, Lai H, Gu Y, Ding H, Men K,\nYang K, Zhang S, Deng X, Zeng A, Du Z, Zhang C, Shen S, Zhang T,\nSu  Y,  Sun  H,  Huang  M,  Dong  Y,  Tang  J.  AgentBench:  evaluating\nLLMs as agents. 2023, arXiv preprint arXiv: 2308.03688\n167.\n Kang S, Yoon J, Yoo S. Large language models are few-shot testers:\nexploring LLM-based general bug reproduction. In: Proceedings of the\n45th  IEEE/ACM  International  Conference  on  Software  Engineering.\n2023, 2312−2323\n168.\n Jalil S, Rafi S, LaToza T D, Moran K, Lam W. ChatGPT and software\ntesting  education:  Promises  &  perils.  In:  Proceedings  of  2023  IEEE\nInternational  Conference  on  Software  Testing,  Verification  and\nValidation Workshops. 2023, 4130−4137\n169.\n Feldt R, Kang S, Yoon J, Yoo S. Towards autonomous testing agents\nvia conversational large language models. In: Proceedings of the 38th\nIEEE/ACM  International  Conference  on  Automated  Software\nEngineering. 2023, 1688−1693\n170.\n Zhou S, Xu F F, Zhu H, Zhou X, Lo R, Sridhar A, Cheng X, Ou T,\nBisk  Y,  Fried  D,  Alon  U,  Neubig  G.  WebArena:  a  realistic  Web\nenvironment  for  building  autonomous  agents.  2023,  arXiv  preprint\narXiv: 2307.13854\n171.\n Xu B, Liu X, Shen H, Han Z, Li Y, Yue M, Peng Z, Liu Y, Yao Z, Xu\nD.  Gentopia.AI:  a  collaborative  platform  for  tool-augmented  LLMs.\n172.\nIn: Proceedings of 2023 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations. 2023, 237−245\n Chalamalasetti  K,  Götze  J,  Hakimov  S,  Madureira  B,  Sadler  P,\nSchlangen D. clembench: Using game play to evaluate chat-optimized\nlanguage  models  as  conversational  agents.  In:  Proceedings  of  2023\nConference  on  Empirical  Methods  in  Natural  Language  Processing.\n2023, 11174–11219\n173.\n Banerjee  D,  Singh  P,  Avadhanam  A,  Srivastava  S.  Benchmarking\nLLM  powered  chatbots:  methods  and  metrics.  2023,  arXiv  preprint\narXiv: 2308.04624\n174.\n Lin  J,  Tomlin  N,  Andreas  J,  Eisner  J.  Decision-oriented  dialogue  for\nhuman-AI collaboration. 2023, arXiv preprint arXiv: 2305.20076\n175.\n Zhao W X, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B,\nZhang J, Dong Z, Du Y, Yang C, Chen Y, Chen Z, Jiang J, Ren R, Li\nY, Tang X, Liu Z, Liu P, Nie J Y, Wen J R. A survey of large language\nmodels. 2023, arXiv preprint arXiv: 2303.18223\n176.\n Yang J, Jin H, Tang R, Han X, Feng Q, Jiang H, Zhong S, Yin B, Hu\nX.  Harnessing  the  power  of  LLMs  in  practice:  a  survey  on  chatGPT\nand beyond. ACM Transactions on Knowledge Discovery from Data,\n2024, doi: 10.1145/3649506\n177.\n Wang Y, Zhong W, Li L, Mi F, Zeng X, Huang W, Shang L, Jiang X,\nLiu  Q.  Aligning  large  language  models  with  human:  a  survey.  2023,\narXiv preprint arXiv: 2307.12966\n178.\n Huang J, Chang K C C. Towards reasoning in large language models:\na  survey.  In:  Proceedings  of  Findings  of  the  Association  for\nComputational Linguistics: ACL 2023. 2023, 1049–1065\n179.\n Mialon G, Dessì R, Lomeli M, Nalmpantis C, Pasunuru R, Raileanu R,\nRozière B, Schick T, Dwivedi-Yu J, Celikyilmaz A, Grave E, LeCun\nY,  Scialom  T.  Augmented  language  models:  a  survey.  2023,  arXiv\npreprint arXiv: 2302.07842\n180.\n Chang Y, Wang X, Wang J, Wu Y, Yang L, Zhu K, Chen H, Yi X,\nWang C, Wang Y, Ye W, Zhang Y, Chang Y, Yu P S. A survey on\nevaluation of large language models. ACM Transactions on Intelligent\nSystems and Technology, 2023, doi: 10.1145/3641289\n181.\n Chang T A, Bergen B K. Language model behavior: a comprehensive\nsurvey. Computational Linguistics, 2024, doi: 10.1162/coli_a_00492\n182.\n Li  C,  Wang  J,  Zhu  K,  Zhang  Y,  Hou  W,  Lian  J,  Xie  X.\nEmotionprompt:  Leveraging  psychology  for  large  language  models\nenhancement  via  emotional  stimulus.  2023,  arXiv  preprint  arXiv:\n2307.11760\n183.\n Zhuo T Y, Li Z, Huang Y, Shiri F, Wang W, Haffari G, Li Y F. On\nrobustness  of  prompt-based  semantic  parsing  with  large  pre-trained\nlanguage model: an empirical study on codex. In: Proceedings of the\n17th  Conference  of  the  European  Chapter  of  the  Association  for\nComputational Linguistics. 2023, 1090–1102\n184.\n Gekhman  Z,  Oved  N,  Keller  O,  Szpektor  I,  Reichart  R.  On  the\nrobustness  of  dialogue  history  representation  in  conversational\nquestion  answering:  a  comprehensive  study  and  a  new  prompt-based\nmethod.  Transactions  of  the  Association  for  Computational\nLinguistics, 2023, 11(11): 351–366\n185.\n Ji Z, Lee N, Frieske R, Yu T, Su D, Xu Y, Ishii E, Bang Y J, Madotto\nA,  Fung  P.  Survey  of  hallucination  in  natural  language  generation.\nACM Computing Surveys, 2023, 55(12): 248\n186.\nLei  Wang  is  a  PhD  candidate  at  Renmin\nUniversity  of  China,  China.  His  research  focuses\non  recommender  systems  and  agent-based  large\nlanguage models.\nLei WANG et al.    A survey on large language model based autonomous agents 25\nChen Ma is currently pursuing a Master’s degree\nat  Renmin  University  of  China,  China.  His\nresearch  interests  include  recommender  system,\nagent based on large language model.\nXueyang  Feng  is  currently  studying  for  a  PhD\ndegree at Renmin University of China, China. His\nresearch  interests  include  recommender  system,\nagent based on large language model.\nZeyu  Zhang  is  currently  pursuing  a  Master’s\ndegree at Renmin University of China, China. His\nresearch  interests  include  recommender  system,\ncausal  inference,  agent  based  on  large  language\nmodel.\nHao Yang is currently studying for a PhD degree\nat  Renmin  University  of  China,  China.  His\nresearch  interests  include  recommender  system,\ncausal inference.\nJingsen  Zhang  is  currently  studying  for  a  PhD\ndegree at Renmin University of China, China. His\nresearch interests include recommender system.\nZhiyuan  Chen  is  pursuing  his  PhD  in  Gaoling\nschool  of  Artificial  Intelligence,  Renmin\nUniversity  of  China,  China.  His  research  mainly\nfocuses  on  language  model  reasoning  and  agent\nbased on large language model.\nJiakai  Tang  is  currently  pursuing  a  Master’s\ndegree at Renmin University of China, China. His\nresearch interests include recommender system.\nXu Chen obtained his PhD degree from Tsinghua\nUniversity,  China.  Before  joining  Renmin\nUniversity of China, he was a postdoc researcher\nat  University  College  London,  UK.  In  the  period\nfrom  March  to  September  of  2017,  he  was\nstudying at Georgia Institute of Technology, USA\nas a visiting scholar. His research mainly focuses\non  the  recommender  system,  reinforcement  learning,  and  causal\ninference.\nYankai Lin received his BE and PhD degrees from\nTsinghua  University,  China  in  2014  and  2019,\nrespectively.  After  that,  he  worked  as  a  senior\nresearcher in Tencent WeChat, and joined Renmin\nUniversity  of  China,  China  in  2022  as  a  tenure-\ntrack  assistant  professor.  His  main  research\ninterests  are  pretrained  models  and  natural\nlanguage processing.\nWayne  Xin  Zhao  received  his  PhD  degree  in\nComputer Science from Peking University, China\nin  2014.  His  research  interests  include  data\nmining,  natural  language  processing  and\ninformation retrieval in general. The main goal is\nto  study  how  to  organize,  analyze  and  mine  user\ngenerated  data  for  improving  the  service  of  real-\nworld applications.\nZhewei Wei received his PhD degree in Computer\nScience  and  Engineering  from  The  Hong  Kong\nUniversity of Science and Technology, China. He\ndid  postdoctoral  research  in  Aarhus  University,\nDenmark from 2012 to 2014, and joined Renmin\nUniversity of China, China in 2014.\nJirong Wen is a full professor, the executive dean\nof  Gaoling  School  of  Artificial  Intelligence,  and\nthe  dean  of  School  of  Information  at  Renmin\nUniversity of China, China. He has been working\nin the big data and AI areas for many years, and\npublishing extensively on prestigious international\nconferences and journals.\n26 Front. Comput. Sci., 2024, 18(6): 186345",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9063363075256348
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33282819390296936
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    }
  ],
  "cited_by": 643
}