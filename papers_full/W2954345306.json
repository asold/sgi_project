{
  "title": "Do Transformer Attention Heads Provide Transparency in Abstractive Summarization?",
  "url": "https://openalex.org/W2954345306",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4297766605",
      "name": "Baan, Joris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3210568736",
      "name": "ter Hoeve, Maartje",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4297766607",
      "name": "van der Wees, Marlies",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4297766608",
      "name": "Schuth, Anne",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742303006",
      "name": "de Rijke, Maarten",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963271116",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W2799051177",
    "https://openalex.org/W2143995218",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2963847595",
    "https://openalex.org/W2951528897",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2341401723",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2594475271",
    "https://openalex.org/W2898694742",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2807015674",
    "https://openalex.org/W2889518897"
  ],
  "abstract": "Learning algorithms become more powerful, often at the cost of increased complexity. In response, the demand for algorithms to be transparent is growing. In NLP tasks, attention distributions learned by attention-based deep learning models are used to gain insights in the models' behavior. To which extent is this perspective valid for all NLP tasks? We investigate whether distributions calculated by different attention heads in a transformer architecture can be used to improve transparency in the task of abstractive summarization. To this end, we present both a qualitative and quantitative analysis to investigate the behavior of the attention heads. We show that some attention heads indeed specialize towards syntactically and semantically distinct input. We propose an approach to evaluate to which extent the Transformer model relies on specifically learned attention distributions. We also discuss what this implies for using attention distributions as a means of transparency.",
  "full_text": "Do Transformer Attention Heads Provide Transparency\nin Abstractive Summarization?\nJoris Baan1, 2 Maartje ter Hoeve1 Marlies van der Wees2 Anne Schuth2 Maarten de Rijke1\n1University of Amsterdam, Amsterdam 2De Persgroep, Amsterdam\njoris.baan@student.uva.nl, m.a.terhoeve@uva.nl, marlies.van.der.wees@persgroep.nl,\nanne.schuth@persgroep.nl, derijke@uva.nl\nABSTRACT\nLearning algorithms become more powerful, often at the cost of\nincreased complexity. In response, the demand for algorithms to be\ntransparent is growing. In NLP tasks, attention distributions learned\nby attention-based deep learning models are used to gain insights\nin the models’ behavior. To which extent is this perspective valid for\nall NLP tasks? We investigate whether distributions calculated by\ndifferent attention heads in a transformer architecture can be used\nto improve transparency in the task of abstractive summarization.\nTo this end, we present both a qualitative and quantitative analysis\nto investigate the behavior of the attention heads. We show that\nsome attention heads indeed specialize towards syntactically and\nsemantically distinct input. We propose an approach to evaluate to\nwhich extent the Transformer model relies on specifically learned\nattention distributions. We also discuss what this implies for using\nattention distributions as a means of transparency.\nCCS CONCEPTS\n• Information systems → Summarization.\nKEYWORDS\nTransformer, Abstractive summarization, Attention, Transparency\n1 INTRODUCTION\nWhen trusting a machine-generated summary it may be crucial to\nhave an understanding of how this summary came to be. Attention\nmechanisms [2, 12] have gained popularity in the context of deep\nlearning-based approaches to summarization [15, 19, 20]. Briefly,\nclassic attention mechanisms learn a function that assigns a score\nto each encoder’s hidden state based on its relevancy to the word\nbeing decoded. Through a weighted average with these softmaxed\nscores, hidden states with high scores are amplified. Because they\nprovide an interpretable heatmap over an input sequence, attention\nmechanisms have been used to gain insights in the behavior of\na given model [3, 9, 11]. However, this may be misleading. First,\nin commonly used architectures such as Bidirectional Recurrent\nNeural Networks (Bi-RNNs) [ 18] and Transformers [ 20] a lot of\ncomputation takes place between an input token and the hidden\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nParis ’19, June 21–25, 2019, Paris, France\n© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\nstate. As a result, it is unclear whether the hidden state that an\nattention weight operates on corresponds to its input token. Second,\nshown heatmaps are usually cherry picked and do not necessarily\ngeneralize over all examples [ 3, 9, 11]. Third, different attention\ndistributions can lead to the same model output, which implies that\n“attention is not explanation” [9].\nCan attention distributions in a Transformer model [20] trained\nfor abstractive summarization be used to address model trans-\nparency for the summarization task? Transformer models consist of\na modular, multi-headed structure. Because of this modularity, we\nmay be able to find distinct interpretable patterns that generalize\nover a large number of examples.\nWe adopt a qualitative and a quantitative approach to investi-\ngate the behavior of attention heads. For the qualitative approach\nwe visually inspect the encoder self-attention and decoder cross-\nattention and find that some heads attend to locations, persons,\norganization nouns, or punctuation. We then introduce several\nmetrics to quantitatively evaluate whether the previous findings\ngeneralize over 1K news articles as well as different initializations\nof the model. Importantly, by doing so we move away from cherry\npicked attention heatmaps. We then discuss a method to investigate\nto what extent the Transformer model relies on certain attention\ndistributions, inspired by recent work on adversarial attention [9].\nThis raises the question whether adversarial methods invalidate the\nuse of learned attention distribution as a means for transparency.\nWith this work we contribute: (1) quantitative metrics that mea-\nsure the degree to which attention heads specialize towards at-\ntending Part-of-Speech (POS), Named Entity (NE) tags and relative\nposition; and (2) a method for adversarial attacks on seq2seq Trans-\nformers to assess the effect of individual attention heads on model\noutput.\n2 RELATED WORK\nTransparency in machine learning has become important as models\nbecome more complex and more frequently play a role in decision\nmaking [7, 14]. Terms such as explainability and transparency are\nhard to define and open for multiple interpretations. Gilpin et al.\n[7] describe an explanation to be an answer to “why questions” and\nconsider it a trade-off between interpretability and completeness.\nInterpretability means being understandable to humans, whereas\ncompleteness covers how well the explanation is faithful to the actual\nmodel mechanics. Doshi-Velez and Kim [5] note that interpretabil-\nity can be used to evaluate desiderata besides performance such\nas causality or trust. Mittelstadt et al. [14] argue that transparency\naddresses how a model functions internally. Such a model or its com-\nponents can be called transparent when they can be comprehended\nentirely. Following [5, 14], we maintain that a fully transparent\narXiv:1907.00570v2  [cs.CL]  8 Jul 2019\nParis ’19, June 21–25, 2019, Paris, France J. Baan et al.\nmodel should be understandable for a user. However, since fully\ntransparent models are not always capable of competitive perfor-\nmance, we argue that a step in the direction of a more interpretable\nmodel already provides transparency. We want to understand the\nattention mechanism and its role in the Transformer to assist in\nthe discussion on whether attention provides transparency.\n2.1 Attention for transparency in NLP\nThe following recent work is aimed towards a better understand-\ning of attention distributions and whether it can be used to ex-\nplain a model. Raganato et al . [17] study the self-attention of a\nTransformer encoder for NMT and observe that some heads mark\nsyntactic dependency relations. Vig [21] visualizes BERT’s [4] self-\nattention and finds patterns such as attention to the surrounding\nwords, identical/related words, predictive words and delimiter to-\nkens. Concurrent to our work, Voita et al. [22] perform a similar\nanalysis of multi-headed attention in NMT and Michel et al. [13]\nfor BERT [4]. They find that heads specialize towards linguistically\ninterpretable roles, but that a majority can be pruned after training\nwithout affecting performance. Jain and Wallace [9] observe that\nattention is commonly (implicitly or explicitly) claimed to provide\ninsight into model dynamics. They argue that if attention is used as\nexplanation, it should exhibit two properties: (1) attention should\ncorrelate with feature importance measures; and (2) adversarially\ncrafted attention distributions should lead to different predictions,\nor be considered equally plausible explanations. Such an adversarial\nattention distribution should maximally differ from the learned at-\ntention, while the corresponding output distribution is constrained\nto be the same within a small range ϵ. With a Bi-RNN or CNN\nencoder it is possible to construct such adversarial distributions\nfor NLP classification tasks such as binary text classification. They\nargue that attention heatmaps should thus not be so easily assumed\nto provide transparency for model predictions [9].\n2.2 The Transformer\nThe Transformer is a seq2seq model that relies solely on (self-)at-\ntention and stacks several encoders and decoders. Self-attention\ncomputes scores between each of the input tokens, as opposed\nto computing scores between encoder and decoder hidden states,\nreferred to ascross-attention. Multi-headed attention refers to having\nmultiple “representation subspaces” or heads governed by separate\nsets of WQ , WK , WV weight matrices. These matrices project each\ninput into a query, key and value vector from which scores and\ncontext vectors are computed. The attention function itself isscaled\ndot product attention and identical to Luong et al. [12]’s dot-product\nattention apart from the scaling factor (Eq. (2)). H represents an\nembedding for the bottom encoder/decoder and a hidden state for\nthe remainder (Eq. (1)).\nheadi = Attention(HW i\nQ , HW i\nK , HW i\nV ) (1)\nAttention(Q, K, V )= softmax\n \nQKT\np\ndkV\n!\n. (2)\nOur work extends and differs from the related work just discussed\nin three important ways: (1) we introduce three metrics relevant to\nsummarization to quantify patterns in attention; (2) we analyze the\ndecoder cross-attention in addition to the encoder self-attention;\nFigure 1: Attention head focusing on locations.\nand (3) our input sequences (news articles) are significantly longer\nthan the short sentences used in previous work.\n3 EXPERIMENTAL SETUP\nWe adopt OpenNMT’s implementation [10] of the CopyGenerator\nTransformer [6]. Both encoder and decoder have four layers with\neight heads. We use scaled dot attention, Gehrmann et al. [6]’s new\nsummary specific coverage function, Wu et al. [23]’s length penalty\nduring beam-search decoding at inference time, and See et al. [19]’s\npointer generator architecture.\nWe use the CNN/Daily Mail [8, 15] dataset containing roughly\n300.000 news articles and use the script provided by Nallapati et al.\n[15] to split this into a train, test and validation set. Articles consist\non average of 781 tokens and summaries of 56 tokens. Following\nSee et al. [19] we truncate articles to 400 words. We train two iden-\ntical models with different parameter initializations to investigate\nwhether stochasticity affects the way attention heads specialize.\nBoth models have similar ROUGE scores (ROUGE-1: 38.76/38.81,\nROUGE-2: 17.13/16.77, ROUGE-L: 36.00/36.28).\n4 QUALITATIVE APPROACH\nWe extend a tool originally created to visualize a copy-generator\nmodel by See et al. [19]. It highlights words in an input article based\non the magnitude of their corresponding attention weights and\ngives control over which attention type, layer or head to visualize.1\nWe compute an overall attention distribution by summing and\nnormalizing attention weights over all time steps.\nFor the encoder, the vast majority of the attention heads seem to\nfocus on preceding, succeeding or surrounding words. Similarly for\nthe decoder, several heads find an occurrence of the currently or\npreviously decoded word. Some heads seem to focus on punctuation\nand delimiters overall, confirming observations from Vig [21].\nStrikingly, when inspecting the overall decoder attention, there\nare heads that seem to focus on key words, locations (Figure 1), or-\nganizations, people or days of the week. These heads appear to have\nlearned to detect such entities without explicit supervisory signals.\nHowever, there are plenty of articles for which these patterns are\nless obvious (Figure 2). Such “counter examples” might indicate\nthat these patterns do not generalize and are based on our bias for\ninterpretability, or the model might sometimes fail to predict the\nspecialized attention, similar to how the ROUGE score is lower for\nsome documents than others.\n5 QUANTITATIVE APPROACH\nTo support our findings from the qualitative visualizations and\nexamine to what extent the observation generalize, we introduce\nthree quantitative metrics.\n1Our version is publicly available at https://aijoris.github.io/attnvis/.\nDo Attention Heads Provide Transparency? Paris ’19, June 21–25, 2019, Paris, France\nFigure 2: Attention head that seemed to focus on named en-\ntities fails to do so in the above example.\n5.1 Relative position\nWe record how often the maximum attention weight is on preceding\nor successive tokens relative to the token currently being encoded\nor decoded. Figure 3 shows that at least nine encoder heads and\nfour decoder heads focus on relative positions. This behavior brings\nFigure 3: Ratio of the max attention weight being assigned\nto neighboring tokens. (Left): Encoder. (Right): Decoder.\nto mind the inductive bias in an RNN where tokens are explicitly\nprocessed sequentially, or a CNN using convolutions to construct\nhidden states. The Transformer does not have such inductive bias\nand solely uses attention. Interestingly, some heads appear to have\nlearned a similar way of processing nonetheless. Model 2 is not\nshown, but has a similar number of relative position heads.\n5.2 POS-KL\nWe tag each article in the test set (see Section 3) with 12 universal\npart-of-speech tags [16] using a POS tagger by FLAIR [1]. For every\narticle we compute a histogram of POS tag counts to serve as\nthe baseline. Then, for each head these tag counts are multiplied\nby the accumulated attention weights of all tokens labeled with\nthat tag and normalized. The degree to which an attention head is\nspecialized can be measured by the difference between its attention-\nweighted POS tag distribution and the baseline POS tag distribution.\nWe use the KL-divergence to quantify this difference and average\nthese over all articles.\nFigure 4a shows the decoder cross-attention weighted POS tag\ndistributions for three heads with the highest KL-divergence. The\npeaks at the punctuation, noun and verb tag confirm that some\nheads consistently focus on specific word categories. For the two\ntrained models, different specializations emerge. Model 2 has two\nheads with a large peak for verbs, and all three heads have a rel-\natively high peak at punctuation as well. Model 1 has no such\npeaks for verbs and only one of the top three heads that focuses on\npunctuation.\n5.3 NEP\nEach article is tagged with four named entities: persons, locations,\norganizations, and miscellaneous, using a NE tagger by FLAIR [1].\nUnlike POS tags, however, not each token is a named entity. This can\ncause a high KL-divergence between the attention weighted named\nentity distribution and baseline (NE-KL), even if a head barely\nattends to named entities. We found computing the proportion of\nattention mass over all named entities (NEP) to be a better method\nfor detecting specialized heads.\nThe baseline ratio of named entities over articles is 0.1. Figure 4b\nshows the top three cross-attention weighted distributions over\nnamed entities based on NEP. Heads shown have a NEP of at least\ndouble the baseline ratio. Large peaks at persons and organizations\ncan be observed for both models. Model 1’s most specialized head\ncorresponds to the ‘location head’ found in our qualitative analysis.\nThis indicates the ability to detect specialized heads using NEP.\nIt simultaneously provides more insight into what such a head\nactually attends and how well our qualitative findings generalize.\nWe refer the reader to the appendix for a complete overview of the\nmetrics for all attention heads, including standard deviations.\n5.4 Analysis\nWe did not detect any POS or NE specialization for the encoder’s\noverall self-attention. This is in line with the earlier observation that\nmost encoder heads attend relative positions. It is important to note\nthat we have not evaluated the models on per-document ROUGE\nscores. This could explain the observed difference in specialization\nbetween models. Perhaps model 2 performs better on articles for\nwhich verbs are important in the summary, resulting in a head that\nmore explicitly attends verbs. Another note is that not every article\ncontains named entities, causing a decrease in NEP. One interesting\nexample can be found in Appendix A, where a NE-specialized head\nhighlights lions in one article. Lions are not named entities but\ndo fulfil a similar role, indicating that NEP might not always fully\ncapture a specialization.\nThe main takeaway is that we show that some attention heads\nspecialize towards attending relative locations, nouns, verbs, punc-\ntuation, persons, locations or named entities. The top 3 specialized\nheads that were found using our quantitative approach line up with\nfindings from visualizations. However, an analysis of POS-KL and\nNEP distributions over articles also indicate that heads only special-\nize to some extend and sometimes take into account a considerable\namount of non-related tokens. This supports claims by Jain and\nWallace [9], urging the research community to be careful in using\nattention as explanation.\n6 ADVERSARIAL ATTENTION\nGiven that some attention heads are found to focus on interpretable\ninput, we want to understand to what extent the model actually\nrelies on these specific attention distributions. For future work, we\npropose to adapt the adversarial attention method by [9] to make it\ncompatible with a seq2seq Transformer model using beam search.\nInstead of requiring the output distributions to be within a small ϵ,\nit is sufficient to constrain the top K output probabilities of each\ndecoding to be within a small ϵ, whereby K = beamsize . This will\nresult in identical output sequences, since the beam search path with\nthe highest probability remains the same. As a consequence, we can\ncraft one adversarial attention distribution for each decoding step\nand aggregate them to evaluate the overall success on a summary.\nParis ’19, June 21–25, 2019, Paris, France J. Baan et al.\n(a) Based on POS-KL. (Left): Model 1; (Right): Model 2.\n (b) Based on NEP. (Left): Model 1; (Right): Model 2.\nFigure 4: A comparison of the top 3 specialized heads.\nAdditionally, we propose to modify the attention distribution of\na specialized head to attend another specific phenomenon. For ex-\nample, we could construct a distribution that solely attends persons\nfor a head that specializes towards locations and observe whether\nlocations change into persons. The Transformer model is large, in\nour case containing 32 heads for both the encoder and decoder. It\nis unclear to what degree modifying the attention distribution of\none head can be expected to affect the output summary.\nHowever, if such an adversarial distribution can be constructed,\nit raises the question to what extend it invalidates the learned at-\ntention distribution as means for transparency. Should an attention\ndistribution have a causal relationship with the model output in\norder to use it for transparency, or does the fact that the model has\nlearned this distribution justify using it as such? Similarly, does the\nuse of attention heads to address transparency become invalidated\nif different specializations form for architecturally identical model\non the same data set? Or does this add to its value because it shows\ndifferences between models that otherwise remain undetected?\n7 CONCLUSION\nWe have presented a qualitative and quantitative approach to better\nunderstand what Transformer attention heads attend to in abstrac-\ntive summarization. Some attention heads do specialize towards\ninterpretable parts of a document, but this does not apply to all\ndocuments. We confirm this with three proposed metrics that quan-\ntify what heads focus on in terms of POS tags, named entities and\nrelative position. We also find that these specializations are not con-\nsistent over differently initialized models. Finally, we discuss the\nuse of adversarial attention to examine the effect of attention distri-\nbutions on model output, and ask what such adversarial methods\nimply for transparency.\nOne limitation of this work is that there is no proof that the\nindex of a hidden state corresponds to a (contextual) representation\nof the corresponding input token. A natural question is why spe-\ncialized heads perform poorly on some articles. Future work could\ncompare per-document ROUGE with POS-KL and NEP to examine\ncorrelations between summarization and head specialization.\nACKNOWLEDGMENTS\nWe thank Mostafa Dehghani for valuable discussions and feedback.\nThis research was partially supported by Ahold Delhaize, the Asso-\nciation of Universities in the Netherlands (VSNU), the Innovation\nCenter for Artificial Intelligence (ICAI), and the Police AI lab. All\ncontent represents the opinion of the authors, which is not nec-\nessarily shared or endorsed by their respective employers and/or\nsponsors.\nREFERENCES\n[1] Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual String Em-\nbeddings for Sequence Labeling. In COLING 2018, 27th International Conference\non Computational Linguistics . 1638–1649.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Ma-\nchine Translation by Jointly Learning to Align and Translate. arXiv preprint\narXiv:1409.0473 (2014).\n[3] Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy\nSchuetz, and Walter Stewart. 2016. Retain: An Interpretable Predictive Model\nfor Healthcare using Reverse Time Attention Mechanism. In Advances in Neural\nInformation Processing Systems . 3504–3512.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv preprint arXiv:1810.04805 (2018).\n[5] Finale Doshi-Velez and Been Kim. 2017. Towards a Rigorous Science of Inter-\npretable Machine Learning. arXiv preprint arXiv:1702.08608 (2017).\n[6] Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. 2018. Bottom-up\nAbstractive Summarization. arXiv preprint arXiv:1808.10792 (2018).\n[7] Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and\nLalana Kagal. 2018. Explaining Explanations: An Approach to Evaluating Inter-\npretability of Machine Learning. arXiv preprint arXiv:1806.00069 (2018).\n[8] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will\nKay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching Machines to Read and\nComprehend. In Advances in neural information processing systems . 1693–1701.\n[9] Sarthak Jain and Byron C Wallace. 2019. Attention is not Explanation. arXiv\npreprint arXiv:1902.10186 (2019).\n[10] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M.\nRush. 2017. OpenNMT: Open-Source Toolkit for Neural Machine Translation.\narXiv preprint arXiv:1701.02810 (2017).\n[11] Tao Lei. 2017. Interpretable Neural Models for Natural Language Processing . Ph.D.\nDissertation. Massachusetts Institute of Technology.\n[12] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective\nApproaches to Attention-based Neural Machine Translation. arXiv preprint\narXiv:1508.04025 (2015).\n[13] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really\nBetter than One? arXiv preprint arXiv:1905.10650 (2019).\n[14] Brent Mittelstadt, Chris Russell, and Sandra Wachter. 2018. Explaining Explana-\ntions in AI. arXiv preprint arXiv:1811.01439 (2018).\n[15] Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. 2016. Ab-\nstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.\narXiv preprint arXiv:1602.06023 (2016).\n[16] Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A Universal Part-of-Speech\nTagset. arXiv preprint arXiv:1104.2086 (2011).\n[17] Alessandro Raganato, Jörg Tiedemann, et al . 2018. An Analysis of Encoder\nRepresentations in Transformer-Based Machine Translation. In 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP .\nACL.\n[18] Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional Recurrent Neural Net-\nworks. IEEE Transactions on Signal Processing 45, 11 (1997), 2673–2681.\n[19] Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the Point:\nSummarization with Pointer-Generator Networks.arXiv preprint arXiv:1704.04368\n(2017).\n[20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nYou Need. In Advances in Neural Information Processing Systems . 5998–6008.\n[21] Jesse Vig. 2018. Deconstructing BERT: Distilling 6 Patterns from 100 Million Pa-\nrameters. towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-\n100-million-parameters-b49113672f77. Accessed: 2019-04-29.\n[22] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019.\nAnalyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting,\nthe Rest Can Be Pruned. arXiv preprint arXiv:1905.09418 (2019).\n[23] Yonghui Wu et al . 2016. Google’s Neural Machine Translation System:\nBridging the Gap between Human and Machine Translation. arXiv preprint\narXiv:1609.08144 (2016).\nDo Attention Heads Provide Transparency? Paris ’19, June 21–25, 2019, Paris, France\nA APPENDIX\nFigure 5: Specialized named entity head focusing on football\nteams.\nFigure 6: Specialized head focusing on the location Antarc-\ntica.\nFigure 7: Specialized NE head with a low NEP. This is in-\nteresting because this head attends animals in this article,\nwhich are not named entities. However, intuitively this ex-\nample still shows a form of specialization, but this is not\nreflected by the NEP metric.\nFigure 8: Specialized NE head showing a non interpretable\npattern.\nParis ’19, June 21–25, 2019, Paris, France J. Baan et al.\nTable 1: Metric scores for the decoder cross attention of model 1. #1 POS and #1 NE show the most attended POS tag or named\nentity for that attention head along with its ratio compared to the other tags. For each column, the three heads with the highest\nscores are boldfaced.\nPOS-KL NEP NER-KL #1 POS #1 NE\nLayer 0\nHead 0 0.03 ±0.02 0.15 ±0.08 0.04 ±0.05 NOUN: 0.340 PER: 0.610\nHead 1 0.05 ±0.03 0.13 ±0.07 0.1 ±0.09 NOUN: 0.360 PER: 0.560\nHead 2 0.03 ±0.02 0.13 ±0.08 0.06 ±0.09 NOUN: 0.330 PER: 0.490\nHead 3 0.1 ±0.04 0.1 ±0.06 0.21 ±0.19 NOUN: 0.240 PER: 0.760\nHead 4 0.04 ±0.03 0.16 ±0.09 0.06 ±0.05 NOUN: 0.350 PER: 0.570\nHead 5 0.07 ±0.03 0.15 ±0.08 0.15 ±0.13 NOUN: 0.390 PER: 0.630\nHead 6 0.12 ±0.05 0.09 ±0.05 0.08 ±0.08 ADP: 0.240 PER: 0.430\nHead 7 0.09 ±0.03 0.16 ±0.07 0.1 ±0.1 NOUN: 0.350 PER: 0.520\nLayer 1\nHead 0 0.08 ±0.04 0.09 ±0.06 0.12 ±0.11 NOUN: 0.370 PER: 0.520\nHead 1 0.15 ±0.06 0.17 ±0.09 0.13 ±0.11 NOUN: 0.300 PER: 0.670\nHead 2 0.15 ±0.05 0.13 ±0.08 0.19 ±0.15 NOUN: 0.390 ORG: 0.420\nHead 3 0.07 ±0.04 0.15 ±0.08 0.2 ±0.17 NOUN: 0.350 PER: 0.720\nHead 4 0.42 ±0.14 0.09 ±0.05 0.07 ±0.07 PUNC: 0.430 PER: 0.660\nHead 5 0.14 ±0.06 0.2 ±0.09 0.11 ±0.12 NOUN: 0.320 PER: 0.640\nHead 6 0.09 ±0.06 0.15 ±0.07 0.11 ±0.1 NOUN: 0.350 PER: 0.540\nHead 7 0.13 ±0.04 0.27 ±0.09 0.15 ±0.15 NOUN: 0.380 ORG: 0.470\nLayer 2\nHead 0 0.15 ±0.05 0.23 ±0.09 0.08 ±0.1 NOUN: 0.440 PER: 0.640\nHead 1 0.11 ±0.06 0.15 ±0.08 0.21 ±0.16 NOUN: 0.230 PER: 0.780\nHead 2 0.25 ±0.09 0.26 ±0.13 0.1 ±0.1 NOUN: 0.560 PER: 0.610\nHead 3 0.09 ±0.07 0.12 ±0.13 0.13 ±0.13 NOUN: 0.290 PER: 0.680\nHead 4 0.18 ±0.06 0.18 ±0.09 0.11 ±0.11 NOUN: 0.480 PER: 0.830\nHead 5 0.14 ±0.08 0.16 ±0.09 0.1 ±0.09 NOUN: 0.390 PER: 0.590\nHead 6 0.06 ±0.03 0.12 ±0.07 0.22 ±0.19 NOUN: 0.330 PER: 0.720\nHead 7 0.12 ±0.07 0.15 ±0.11 0.09 ±0.1 NOUN: 0.300 PER: 0.460\nLayer 3\nHead 0 0.07 ±0.04 0.15 ±0.08 0.2 ±0.17 NOUN: 0.350 PER: 0.690\nHead 1 0.17 ±0.12 0.09 ±0.05 0.11 ±0.11 PUNC: 0.230 PER: 0.760\nHead 2 0.11 ±0.06 0.12 ±0.09 0.2 ±0.18 NOUN: 0.420 PER: 0.620\nHead 3 0.19 ±0.18 0.2 ±0.25 0.16 ±0.16 NOUN: 0.350 ORG: 0.540\nHead 4 0.1 ±0.06 0.14 ±0.09 0.09 ±0.08 NOUN: 0.270 PER: 0.670\nHead 5 0.11 ±0.06 0.14 ±0.06 0.16 ±0.14 NOUN: 0.300 PER: 0.420\nHead 6 0.16 ±0.07 0.18 ±0.1 0.1 ±0.1 NOUN: 0.490 PER: 0.680\nHead 7 0.07 ±0.04 0.13 ±0.08 0.19 ±0.17 NOUN: 0.360 PER: 0.750\nDo Attention Heads Provide Transparency? Paris ’19, June 21–25, 2019, Paris, France\nTable 2: Metric scores for the decoder cross attention of\nmodel 2. #1 POS and #1 NE show the most attended POS tag\nor named entity for that attention head along with its ratio\ncompared to the other tags. For each column, the three heads\nwith highest scores are boldfaced.\nPOS-KL NEP NER-KL #1 POS #1 NE\nLayer 0\nHead 0 0.04 ±0.03 0.14 ±0.07 0.04 ±0.05 NOUN: 0.320 PER: 0.480\nHead 1 0.05 ±0.03 0.18 ±0.09 0.06 ±0.06 NOUN: 0.370 PER: 0.580\nHead 2 0.06 ±0.03 0.13 ±0.08 0.24 ±0.2 NOUN: 0.310 PER: 0.560\nHead 3 0.04 ±0.02 0.14 ±0.06 0.04 ±0.04 NOUN: 0.350 PER: 0.490\nHead 4 0.06 ±0.03 0.11 ±0.07 0.2 ±0.17 NOUN: 0.280 ORG: 0.490\nHead 5 0.05 ±0.03 0.19 ±0.09 0.08 ±0.07 NOUN: 0.380 PER: 0.580\nHead 6 0.17 ±0.05 0.14 ±0.08 0.06 ±0.05 NOUN: 0.290 PER: 0.690\nHead 7 0.1 ±0.04 0.17 ±0.07 0.06 ±0.06 NOUN: 0.410 PER: 0.520\nLayer 1\nHead 0 0.18 ±0.07 0.16 ±0.07 0.09 ±0.08 PUNC: 0.290 PER: 0.560\nHead 1 0.14 ±0.07 0.16 ±0.08 0.08 ±0.07 NOUN: 0.370 PER: 0.620\nHead 2 0.09 ±0.04 0.17 ±0.09 0.13 ±0.12 NOUN: 0.400 PER: 0.600\nHead 3 0.1 ±0.05 0.19 ±0.07 0.11 ±0.1 NOUN: 0.310 PER: 0.440\nHead 4 0.06 ±0.03 0.11 ±0.06 0.06 ±0.06 NOUN: 0.360 PER: 0.620\nHead 5 0.17 ±0.07 0.17 ±0.1 0.1 ±0.1 NOUN: 0.370 PER: 0.710\nHead 6 0.19 ±0.08 0.13 ±0.06 0.1 ±0.1 VERB: 0.340 PER: 0.670\nHead 7 0.22 ±0.1 0.17 ±0.08 0.1 ±0.1 VERB: 0.320 PER: 0.740\nLayer 2\nHead 0 0.08 ±0.03 0.18 ±0.09 0.1 ±0.09 NOUN: 0.400 ORG: 0.500\nHead 1 0.05 ±0.03 0.1 ±0.05 0.07 ±0.07 NOUN: 0.300 PER: 0.670\nHead 2 0.13 ±0.07 0.13 ±0.08 0.08 ±0.08 NOUN: 0.310 PER: 0.830\nHead 3 0.05 ±0.02 0.16 ±0.08 0.08 ±0.08 NOUN: 0.290 PER: 0.550\nHead 4 0.06 ±0.03 0.12 ±0.07 0.21 ±0.18 NOUN: 0.300 PER: 0.520\nHead 5 0.14 ±0.05 0.16 ±0.08 0.08 ±0.08 NOUN: 0.390 PER: 0.610\nHead 6 0.24 ±0.09 0.23 ±0.12 0.07 ±0.08 NOUN: 0.430 PER: 0.690\nHead 7 0.09 ±0.04 0.1 ±0.06 0.21 ±0.19 NOUN: 0.340 PER: 0.760\nLayer 3\nHead 0 0.08 ±0.05 0.16 ±0.09 0.2 ±0.17 NOUN: 0.360 PER: 0.470\nHead 1 0.11 ±0.05 0.21 ±0.09 0.12 ±0.12 NOUN: 0.410 PER: 0.680\nHead 2 0.12 ±0.12 0.24 ±0.18 0.11 ±0.12 NOUN: 0.420 PER: 0.490\nHead 3 0.15 ±0.07 0.17 ±0.09 0.16 ±0.15 NOUN: 0.400 PER: 0.490\nHead 4 0.07 ±0.03 0.11 ±0.06 0.21 ±0.18 NOUN: 0.370 PER: 0.570\nHead 5 0.08 ±0.09 0.12 ±0.08 0.1 ±0.11 NOUN: 0.290 PER: 0.510\nHead 6 0.1 ±0.05 0.13 ±0.08 0.11 ±0.11 NOUN: 0.360 PER: 0.700\nHead 7 0.07 ±0.03 0.12 ±0.07 0.13 ±0.12 NOUN: 0.330 PER: 0.730",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9021766185760498
    },
    {
      "name": "Transformer",
      "score": 0.7809329628944397
    },
    {
      "name": "Computer science",
      "score": 0.7730345726013184
    },
    {
      "name": "Transparency (behavior)",
      "score": 0.6472601294517517
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5892545580863953
    },
    {
      "name": "Natural language processing",
      "score": 0.5128767490386963
    },
    {
      "name": "Machine learning",
      "score": 0.46465247869491577
    },
    {
      "name": "Architecture",
      "score": 0.4627537727355957
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}