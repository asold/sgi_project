{
  "title": "MEEP: Is this Engaging? Prompting Large Language Models for Dialogue Evaluation in Multilingual Settings",
  "url": "https://openalex.org/W4389520226",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2136770259",
      "name": "Amila Ferron",
      "affiliations": [
        "Portland State University"
      ]
    },
    {
      "id": "https://openalex.org/A5104211353",
      "name": "Amber Shore",
      "affiliations": [
        "Portland State University"
      ]
    },
    {
      "id": "https://openalex.org/A2766159295",
      "name": "Ekata Mitra",
      "affiliations": [
        "Portland State University"
      ]
    },
    {
      "id": "https://openalex.org/A2155196028",
      "name": "Ameeta Agrawal",
      "affiliations": [
        "Portland State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3097023354",
    "https://openalex.org/W4386566514",
    "https://openalex.org/W2916772188",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W2963672599",
    "https://openalex.org/W4378908626",
    "https://openalex.org/W4366459745",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4385573114",
    "https://openalex.org/W2151814822",
    "https://openalex.org/W4392669868",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3176264844",
    "https://openalex.org/W2998494704",
    "https://openalex.org/W4385572754",
    "https://openalex.org/W3036394672",
    "https://openalex.org/W4306802385",
    "https://openalex.org/W2078684842",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4290062243",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4385574031",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4322760121",
    "https://openalex.org/W4362679631",
    "https://openalex.org/W2962717182",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W3093956460",
    "https://openalex.org/W3034808773",
    "https://openalex.org/W4221160645",
    "https://openalex.org/W4390035079",
    "https://openalex.org/W4379539933",
    "https://openalex.org/W3201085013",
    "https://openalex.org/W2565288664",
    "https://openalex.org/W4381827283",
    "https://openalex.org/W4361230777",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W4206147258",
    "https://openalex.org/W4362706556",
    "https://openalex.org/W2894060751",
    "https://openalex.org/W4311000453",
    "https://openalex.org/W2963903950",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W3035148359",
    "https://openalex.org/W4308169881",
    "https://openalex.org/W4385574004",
    "https://openalex.org/W4389519239",
    "https://openalex.org/W4293790366"
  ],
  "abstract": "As dialogue systems become more popular, evaluation of their response quality gains importance. Engagingness highly correlates with overall quality and creates a sense of connection that gives human participants a more fulfilling experience. Although qualities like coherence and fluency are readily measured with well-worn automatic metrics, evaluating engagingness often relies on human assessment, which is a costly and time-consuming process. Existing automatic engagingness metrics evaluate the response without the conversation history, are designed for one dataset, or have limited correlation with human annotations. Furthermore, they have been tested exclusively on English conversations. Given that dialogue systems are increasingly available in languages beyond English, multilingual evaluation capabilities are essential. We propose that large language models (LLMs) may be used for evaluation of engagingness in dialogue through prompting, and ask how prompt constructs and translated prompts compare in a multilingual setting. We provide a prompt-design taxonomy for engagingness and find that using selected prompt elements with LLMs, including our comprehensive definition of engagingness, outperforms state-of-the-art methods on evaluation of engagingness in dialogue across multiple languages.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2078–2100\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nMEEP: Is this Engaging? Prompting Large Language Models for Dialogue\nEvaluation in Multilingual Settings\nAmila Ferron and Amber Shore and Ekata Mitra and Ameeta Agrawal\nDepartment of Computer Science, Portland State University\n{aferron,ashore,ekata,ameeta}@pdx.edu\nAbstract\nAs dialogue systems become more popular,\nevaluation of their response quality gains im-\nportance. Engagingness highly correlates with\noverall quality and creates a sense of connec-\ntion that gives human participants a more ful-\nfilling experience. Although qualities like co-\nherence and fluency are readily measured with\nwell-worn automatic metrics, evaluating en-\ngagingness often relies on human assessment,\nwhich is a costly and time-consuming process.\nExisting automatic engagingness metrics evalu-\nate the response without the conversation his-\ntory, are designed for one dataset, or have lim-\nited correlation with human annotations. Fur-\nthermore, they have been tested exclusively on\nEnglish conversations. Given that dialogue sys-\ntems are increasingly available in languages\nbeyond English, multilingual evaluation capa-\nbilities are essential. We propose that large\nlanguage models (LLMs) may be used for eval-\nuation of engagingness in dialogue through\nprompting, and ask how prompt constructs and\ntranslated prompts compare in a multilingual\nsetting. We provide a prompt-design taxon-\nomy for engagingness and find that using se-\nlected prompt elements with LLMs, including\nour comprehensive definition of engagingness,\noutperforms state-of-the-art methods on evalu-\nation of engagingness in dialogue across multi-\nple languages.1\n1 Introduction\nDialogue systems are becoming more popular, but\ntheir quality is usually evaluated in terms of metrics\nsuch as fluency, coherence, or sensibility. Recent\nadvancements in large language model-based di-\nalogue systems enable high levels of proficiency,\nthus shifting the emphasis from fluency evalua-\ntion to more nuanced aspects such as engagingness,\nwhich has emerged as an important quality of dia-\nlogue systems (Yu et al., 2016; Zhou et al., 2022;\n1Code available at https://github.com/PortNLP/MEEP\nPrompt \nEngagingness \nDefinition \nLLM \nPrompt Design\nElements\nDialogue\nResponse\nDialogue: \n\" 嗨！ \", \" 哇，你好。 ...\", ...\n \nResponse: \n\" 这是一次有趣的旅行。 ...\"\nDialogue: \n\"¡Hola!\", \"Guau, hola...\", ...\n \nResponse: \n\"Fue un viaje interesante...\"\nDialogue: \n\"Hi!\",\"Wow, hello...\", ...\n \nResponse: \n\"It was an interesting trip...\"\nEN: 90\nES: 85\nZH: 90\nFigure 1: Overview of proposed methodology, MEEP,\non datasets in English, Spanish, and simplified Chi-\nnese. Our prompt is distinguished by its definition of\nengagingness, drawn from previous research, linguistic\nanalysis, and the study of inclusivity. Extensive exper-\niments informed the use of additional prompt design\nelements. Scores shown here are in a range from 0 (not\nengaging) to 100 (very engaging).\nCohen et al., 2022). Development of models that\nproduce more engaging responses can be supported\nby automatic metrics to complement human anno-\ntation.\nDespite the ground-breaking work of existing\nmetrics for engagingness, they evaluate the re-\nsponse without the conversation history (Xu et al.,\n2022), or are designed for a specific dataset (Liu\net al., 2023), and are not highly correlated with\nhuman annotations (Ghazarian et al., 2020). Al-\nthough multi-dimensional automatic metrics are\ndesirable, they have limited success with complex\nqualities like engagement (Mehri and Eskenazi,\n2020a; Deng et al., 2021; Zhang et al., 2022a). En-\nthusiasm for multi-dimensional evaluation is bal-\nanced by calls to develop metrics that measure spe-\ncific dialogue qualities in order to complement ex-\nisting metrics (Gehrmann et al., 2022; Mehri et al.,\n2022; Colombo et al., 2022).\n2078\nFurthermore, dialogue systems are increasingly\navailable in languages beyond English so it is im-\nportant to be able to test systems in these languages\n(Rodríguez-Cantelar et al., 2023). With the rise of\nvirtual assistants, the importance of engaging dia-\nlogue (Richardson et al., 2023) and multilingualism\nhas increased significantly, as they need to actively\nassist users, facilitate knowledge exploration, and\nfoster effective communication, making engaging-\nness a vital parameter in evaluating dialogues. Our\nresearch therefore develops an automatic metric for\nengagingness that can evaluate dialogue responses\nin multiple languages.\nWe establish a comprehensive definition of en-\ngagingness in conversation and construct an ex-\ntensive range of prompts designed with our dimen-\nsions of engagingness and prompt engineering tech-\nniques (see Figure 1). These prompts are employed\nacross several LLMs, which show impressive ca-\npabilities in many tasks and have had limited ex-\nploration into their use for automatic evaluation of\ndialogue.\nTo our knowledge, this is the first work to ex-\ntensively test prompt design for the dedicated eval-\nuation of engagingness in dialogue in a multilin-\ngual setting. We test the prompts first on English-\nonly datasets, and select the best-performing set\nof prompting methods to test against four strong\nbaseline metrics on Spanish-language and sim-\nplified Chinese-language datasets. We also test\ntranslated versions of these prompts. We find\nthat our proposed framework – MEEP: Metric for\nEngagingness Evaluation using Prompting – can be\neffective for evaluation of enganginess in dialogue\nin multiple languages with the broader implication\nthat LLMs can be used for evaluation of complex\nqualities of dialogue in multiple languages through\nprompting alone.\nOur contributions are as follows:\n• A thorough definition of engagingness us-\ning five dialogue engagement dimensions in-\nformed by previous engagingness research\nand a linguistics-based approach. It frames\nthe goals of engagingness in conversation in\nterms of human-aligned dimensions indepen-\ndent of existing datasets and provides a more\nnuanced target for the development of engag-\ning dialogue systems.\n• A novel method for measuring engagingness\nin dialogue using a set of prompt engineering\napproaches that improve LLM performance.\n• An extensive evaluation across ten datasets\nand three languages (English, Spanish, and\nChinese).\n2 Related Work\nAutomatic Dialogue Metrics From the develop-\nment of PARADISE, the earliest automatic met-\nric to evaluate dialogue (Walker et al., 1997),\nreference-based automatic metrics such as BLEU\n(Papineni et al., 2002), ROUGE (Lin, 2004), and\nBERTS CORE (Zhang et al., 2020) have been used\nextensively for evaluation of dialogue but have\nweak correlation with human judgments (Liu et al.,\n2016; Novikova et al., 2017), and do not suit the\nmultiplicity of valid responses possible in any con-\nversation (Zhao et al., 2017; Mehri et al., 2022).\nAutomatic Metrics for Engagingness in Dialogue\nGhazarian et al. (2020) use engagingness as a mea-\nsure of dialogue system performance in the first\nattempt to learn engagingness scores at the utter-\nance level from annotated data. This BERT-based\nmetric achieves modest correlations with human an-\nnotated scores, and leaves room for future improve-\nment. A recent effort at creating a more robust\nautomatic metric for engagingness is ENDEX (Xu\net al., 2022). This method uses several features of a\nReddit-based dataset to learn engagingness scores\nand evaluate the responses in a turn-level dataset\nwithout considering the dialogue context. Some\nmulti-dimensional dialogue metrics include engag-\ningness as one of the dimensions measured (Mehri\nand Eskenazi, 2020a; Zhang et al., 2021), however,\nthe engagingness scores for these metrics are not\nhighly correlated with human evaluation. Other\nmultidimensional dialogue metrics use results from\nhuman-directed questions about interestingness to\nevaluate engagingness for an imprecise approxi-\nmation (Deng et al., 2021; Zhong et al., 2022; Liu\net al., 2023).\nMultilingual Automatic Dialogue Metrics\nMultilingual dialogue models are commonly evalu-\nated using similarity measures such as SacreBLEU,\nBERTS CORE , BLEU , and perplexity (Agarwal\net al., 2021; Zhang et al., 2022b). Development\nof non-similarity-based automatic metrics for a\nmultilingual context remains an open question for\nresearch. The recent DSTC11 shared task proposal\nfor Task 4 acknowledges this lack as motivation\nfor the task (Rodríguez-Cantelar et al., 2023).\nLarge Language Models for NLG Evaluation\n2079\nGao et al. (2023) find that for text summarization\nevaluation, ChatGPT provides scores in the style\nof human evaluation, and that adding dimension\ndefinitions leads to slight improvements in corre-\nlation with human annotations. GEMBA (Kocmi\nand Federmann, 2023) achieves state-of-the-art per-\nformance for machine translation evaluation using\nGPT-3.5 and larger. Most attempts to study the\neffectiveness of prompting in the context of auto-\nmatic evaluation use modified versions of annota-\ntor instructions as prompts (Kocmi and Federmann,\n2023; Wang et al., 2023; Luo et al., 2023; Fu et al.,\n2023; Gao et al., 2023). Prompt techniques im-\nprove through the use of clarifying examples (Yuan\net al., 2021) or assigning LLMs roles (Shen et al.,\n2023). Svikhnushina and Pu (2023) use new di-\nalogues generated by an LLM interacting with a\nchatbot that are then scored with the same LLM.\nLarge Language Models for Evaluation of Dia-\nlogue G-E VAL (Liu et al., 2023), evaluates sum-\nmarization and dialogue response generation, us-\ning prompts that include definitions of the task and\nquality to be evaluated, Chain of Thought reason-\ning, and a scoring function. GPTS CORE (Fu et al.,\n2023) provides an adaptable metric using zero-shot\ninstruction with a prompt template that includes\nthe task specification and a definition of the aspect\nto be evaluated. Each has only been tested on one\n(English) dataset.\n3 Engagingness in Dialogue\nEngagingness is central to our conception of con-\nversation, as demonstrated by the prevalence of\nthe word “conversation” as a collocate2 of the verb\n“engage” (COCA). What then is engagingness? In\nprior research, several metrics conflate engaging-\nness with interestingness (Deng et al., 2021; Zhong\net al., 2022; Liu et al., 2023). Others provide no\ndefinition and the training of these metrics rely in-\nstead on the implicit judgements from the training\nset annotators (Logacheva et al., 2018). We posit\nthat there is no standard definition of engagingness\nin NLP because of the subjectivity of human judg-\nment. Humans, like models, cannot be relied upon\nfor self reporting (Gao et al., 2021; Rosenman et al.,\n2011). We put forward a definition of engagingness\nthat is useful for our goal: to create an evaluation\ntool that will improve dialogue model performance.\n2A collocate is a word that appears in conjunction (or in\nthe same context) as the other word with greater than random\nfrequency.\n0 0.2 0.4 0.6 0.8 1\nInteresting\nSpecific\nRelevant\nCorrect\nSemantically appropriate\nUnderstandable\nFluent\nOverall\nSpearman Pearson\nFigure 2: Correlation of several dialogue dimensions\nwith engagingness as annotated in the FED dataset.\nTo ground our definition, we start with the dictio-\nnary. Dictionary definitions identify three consis-\ntent aspects of the quality of being engaging: atten-\ntion, interest, and participation (Merriam-Webster,\n2023; Wiktionary, 2023; Dictionary, 2023). A re-\nply is engaging when it gets the user’s attention,\ninterests the user, and incites participation from the\nuser. These three aspects form the core of what we\nexpect an engagingness metric to measure.\nWe propose the following five subdimensions of\nengagingness:\n• Response Diversity: Engaging responses are\ndiverse, not repetitive and generic. When\na model produces a standard or generic re-\nsponse, it can constrain the responses the user\ncan reasonably produce (Stasaski and Hearst,\n2023), creating predictable dialogue which\ncan limit engagingness.\n• Interactional Quality: Engaging responses\nencourage a response from the user. A defini-\ntionally necessary component of engagement\nis participation, so signs of elicited participa-\ntion in the user, such as 1) the presence of any\nresponse, and 2) the presence of a high-quality\nresponse, demonstrate this aspect.\n• Interestingness: We pose interestingness as\na necessary component of engagingness. Re-\nsearch has commonly used interestingness as\na proxy for engagingness (Deng et al., 2021;\nZhong et al., 2022; DSTC11 2023; Liu et al.,\n2023), and we see high correlation between\nscores for engagingness and interestingness in\nthe FED dataset (Mehri and Eskenazi, 2020a)\n(see Figure 4). However, we do not believe\nthat there is a one-to-one correspondence be-\ntween interestingness and engagingness. Inter-\nestingness captures factual appeal (Mehri and\nEskenazi, 2020b), while definitions of engag-\ningness emphasize active participation. Both\n2080\nPrompt Code Description\nNaive simple baseline prompt; directly asking for a score\nNaive+R Naive prompt with role assignment\nHD prompt derived from instructions given to human annotators of engagingness\nHD+R the human-directed prompt with role assignment\nMEEP an intro like the HD prompt, and short phrases for each of our six subdimensions of engagingness\nMEEP+SA the MEEP prompt with the \"such as\" phrase\nMEEP+R the MEEP prompt with role assignment\nMEEP+SA+R the MEEP prompt with the \"such as\" phrase and role assignment\nMEEP+SA-DIAL the MEEP+SA prompt edited to be used at the dialogue-level\nMEEP+SA+R-DIAL the MEEP+SA+R prompt edited to be used at the dialogue-level\nTable 1: Overview of prompt styles. Some prompts are also translated into other languages.\ncontribute to meaningful conversations from\ndifferent perspectives.\n• Contextual Specificity: Engaging responses\nare specific to the conversational context. This\nis another aspect of avoiding generic user re-\nsponses. See et al. (2019) show that control-\nling for specificity improves the engagingness\nof their dialogue model.\n• Othering: Engaging responses create a sense\nof belonging, not one of othering (Powell and\nMenendian, 2022; Alexander and Andersson,\n2022). Even an interesting conversation can\nquickly turn dull when we feel misunderstood,\nunheard, and looked down on. Conversations\nwhere rapport is established, on the other hand,\nare often enjoyable even in the absence of con-\ntent. This aspect of engagingness is particu-\nlarly underexplored in the research.\n4 MEEP: Metric for Engagingness\nEvaluation using Prompting\nWe design several categories of prompts. A straight-\nforward naive prompt acts as the basis against\nwhich we compare our more developed prompt\nsets. We adapt dialogue annotator directions into\nprompts for the human-directed prompt set and\nour own proposed sub-dimensions of engagingness\nform the third prompt set. For each prompt set, we\ncreate a version casting the LLM in a role. Table 1\nprovides an overview of the prompt types, and the\nfull text of each prompt is available in Appendix A.\nWe experiment with prompts in English as well as\nSpanish and Chinese.\nNaive Prompt (Naive) We use theNaive prompt\nas a baseline comparator from which to develop\nmore performant prompts. Kocmi and Federmann\n(2023) report best performance from the least con-\nstrained prompting method, implying that this\nnaive prompt approach is valid as its own path\nof inquiry, and a similar prompt is used in Wang\net al. (2023).\nHuman-directed Prompts (HD) Since LLMs\nare developed to approximate natural language and\nnatural linguistic interaction, we theorize that an\nevaluation prompt styled after the same instructions\nthat human annotators were given should lead to\nincreased performance over the Naive prompt.\nPrompts With Our Proposed Dimensions of En-\ngagingness (MEEP) We incorporate each of our\nfive subdimensions of engagingness into a word or\nphrase in order to form the definition of engaging-\nness within the prompt. This is demonstrated in\nFigure 3.\nFigure 3: The MEEP prompt annotated to show the five\nengagingness subdimensions.\n“Such As” and Role Assignment (SA, R) Yuan\net al. (2021) find that even when controlling for the\npresence of example responses, the phrase “such\nas” increases performance in the domain of ma-\nchine translation evaluation. When added to our\nprompts, it is accompanied by an example response.\nRole assignment is theorized to increase the qual-\nity of the responses generated by dialogue models\n(Svikhnushina and Pu, 2023), and we explore its\nutility in dialogue response evaluation. We incor-\nporate role assignment using methods specific to\nthe model used. When possible, we make the as-\nsignment directly in the API call, using the “sys-\ntem” option. If this method is not supported by\n2081\nthe model, the role assignment is prefixed to the\nprompt text.\nTranslation (†) We translate a selection of our\nprompts from English into Spanish and Chinese,\nto match the language of the dialogues evaluated.\nSpanish text was manually translated, and Chinese\ntranslations are with GPT-4 and checked by a hu-\nman translator for quality.\n5 Experiments\nWe first test our prompt combinations using two\nLLMs on the English-language FED dataset. Five\nexamples from these tests are randomly selected,\non which we perform a qualitative analysis. We\nthen select the best performing prompts to test over\nsix multilingual turn-level datasets, two in each\nof English, Spanish, and Chinese. For these tests,\nwe evaluate the prompts in English as well as the\nversions translated into the language of the dataset.\nThe scores reported by each LLM/prompt style pair\nover six datasets are correlated with the human-\nannotated scores to evaluate the proposed methods\nand four strong baseline methods. A final set of ex-\nperiments evaluates the performance of the highest-\nperforming prompts on dialogue-level datasets in\nEnglish and Chinese and correlates with human-\nannotated scores. These are compared with the\nstrongest baseline method from turn-level testing.\n5.1 Large Language Models\nFor our base model, we select text-davinci-003\n(GPT-3.5) (Ouyang et al., 2022),\ngpt-3.5-turbo-0301 (ChatGPT), and\ngpt-3.5-turbo-0613 (ChatGPT0613) from\nthe GPT-series of LLMs 3 (Radford et al., 2022).\nWe set the temperature to 0, top_p to 1, and\nn to 1. Presence penalty and frequency\npenalty are set to 0, logit bias to null, and\nwe request the best of 1. We also use LLaMA2\n(Touvron et al., 2023) in the 7B size with the\nchat_completion function with max_seq_len set\nto 1024, max_gen_len set to None, temperature\nset to 0, max_batch_size set to 1, and top_p set\nto 1. We do not set a max length for generation\nbecause the responses are long and the location of\nscores within the returned text is unpredictable.\n3We note that ChatGPT is available at the time of writing\nas gpt-3.5-turbo in the free version, and gpt-4 in the paid\nversion. We take the liberty of using ChatGPT as the short-\nhand for gpt-3.5-turbo-0301, the free version at the time\nof testing.\n5.2 Benchmark Datasets\nWe use four existing datasets, all from the 11th\nDialogue System Technology Challenge, Track\n44 (DSTC11 2023). Challenge organizers ma-\nchine translated two English turn-level datasets into\nSpanish and Chinese, and two Chinese dialogue-\nlevel datasets into English to produce a total of\nten datasets spanning three languages. Annota-\ntions from the original datasets are used to test the\ndatasets in the additional languages.\nFED The Fine-grained Evaluation of Dialogue\n(FED) dataset (Mehri and Eskenazi, 2020a) pro-\nvides annotations of English-language dialogues\nfirst collected by Adiwardana et al. (2020) by way\nof multi-dimensional evaluations per dialogue turn,\nincluding engagingness as one of the evaluated di-\nmensions.\nSEE The Persona-See (SEE) dataset (See et al.,\n2019) expands the PersonaChat (Zhang et al., 2018)\ntask to provide human annotations at the turn-level,\nevaluating multiple dimensions of conversation per\ndialogue. They consider enjoyment to be an equiv-\nalent measure of engagement, relying on users’\nactive involvement and positive perception. We\nregard it as a reliable proxy for engagingness. Orig-\ninally containing over 3,000 annotated dialogues,\nthis is much larger than the other datasets, seen in\nTable 2. For efficiency and consistency of scale,\nwe randomly select a subset of 300 dialogues for\ncomparative analysis. These same 300 dialogues\nare used in each of English, Spanish, and Chinese.\nKDCONV The Knowledge-driven Conversation\n(KDCONV) dataset consists of Chinese-language\ndialogues between two humans who are given\nknowledge graphs from which to draw their re-\nsponses (Zhou et al., 2020).\nLCCC The large-scale cleaned Chinese conver-\nsation dataset (LCCC) contains conversations from\nChinese social media posts of users heuristically\nidentified to be human (Wang et al., 2020). These\ndialogues are combined with those from publicly\navailable databases and cleaned.\nDialogues for KDCONV and LCCC were manu-\nally annotated at the dialogue-level for engaging-\nness and machine translated into English for the\nDSTC11 Track 4 Challenge.\nTranslation Quality Analysis We briefly look at\ndataset translation quality to assess the validity of\n4https://chateval.org/dstc11\n2082\nDataset # samples Language Annotation level COMET-20 COMET-21 CosSim1 CosSim2\nFED-EN 375 English turn – – – –\nFED-ES 375 Spanish turn 0.458 0.117 0.846 0.914\nFED-ZH 375 Chinese turn 0.323 0.084 0.789 0.883\nSEE-EN 300 English turn – – – –\nSEE-ES 300 Spanish turn 0.510 0.128 0.873 0.921\nSEE-ZH 300 Chinese turn 0.376 0.102 0.800 0.875\nKDCONV-EN 392 English dialogue 0.242 0.053 0.788 0.857\nKDCONV-ZH 392 Chinese dialogue – – – –\nLCCC-EN 22 English dialogue -0.132 -0.40 0.690 0.776\nLCCC-ZH 22 Chinese dialogue – – – –\nTable 2: Dataset specifications, with average machine translation quality scores of datasets that were translated. The\nhighest scores for each measure are in bold. Second highest scores are underlined.\nthe annotations as an accurate measure of engaging-\nness for translated datasets. This informs analysis\nof our test results in Section 6.2. Translations into\nSpanish and Chinese were obtained by the DSTC11\norganizers with the MS Azure service5 and Tencent\nMachine Translation service6, respectively.\nThe datasets include four measures of translation\nquality for every utterance: two quality estimator\nmetrics from COMET versions from 2020 (Rei\net al., 2020) and 2021 (Rei et al., 2021), and two\ncosine similarity measures from embeddings gen-\nerated from the original utterance and translation.\nTwo methods of embedding generation were used.\nDetails are available in Appendix B. We average\nthe provided translation quality scores across all ut-\nterances for each translated dataset and list them in\nTable 2. The Spanish-language datasets score con-\nsistently higher than the Chinese-language datasets.\n5.3 Baselines\nThe results of our proposed method are compared\nagainst four recent baselines.\nENDEX evaluates the engagingness of dialogues\nbased on human reactions (Xu et al., 2022). This\napproach uses fine-tuned RoBERTa-large models\non datasets evaluated at the turn level, measuring\ndimensions of engagement including attentional,\nbehavioral, emotional, and task-based aspects of\nreplies. Each response is given a binary score for\nengagingness. Due to the inherent constraints of\nENDEX, we encountered difficulties in adapting it\nto languages other than English.\nUNIEVAL employs T5-Large as the underlying\nmodel to assess any dialogue dimension, includ-\ning those that are not seen during training (Zhong\n5https://azure.microsoft.com/en-us/products/\ncognitive-services/translator/\n6https://www.tencentcloud.com/products/tmt\net al., 2022). It accomplishes this by formulating\nsingle evaluation dimensions as boolean question-\nanswering (QA) tasks. In evaluating the engaging-\nness of a response, UNIEVAL takes three parame-\nters: the conversational context, an additional con-\ntext (such as an intriguing fact), and the response\nitself. To facilitate testing with the FED dataset\nwhich does not include an additional fact, an empty\nstring is used as the additional context. We use\nUNIEVAL in a multilingual context where it had\nnot been previously evaluated.\nGPTS CORE evaluates dialogue response gener-\nation at the turn-level across eight dimensions,\nincluding engagingness (Fu et al., 2023). Their\nprompt for evaluation of engagingness is in-\ncluded in Appendix A. Our approximation of\ntheir methodology is adapted from their code for\ntext summarization evaluation. A prompt that in-\ncludes task definition, identification of the qual-\nity to be evaluated (engagingness), and the dia-\nlogue, followed by “Answer: Yes.”, is passed to\nthe LLM with temperature=0, max_tokens=0,\nlogprobs=0, echo=True, and n=0. The log prob-\nabilities returned for the token ’Yes’ become the\nscore for engagingness.\nG-E VAL is an LLM-based evaluator comprising a\nprompt, Chain-of-Thought reasoning, and a scor-\ning function (Liu et al., 2023). It achieves strong\ncorrelations with human evaluations using GPT-3.5\nand GPT-4 models in NLG tasks, including the\nassessment of engagingness in dialogues. We use\ntheir GPT-3.5 model, which shows higher correla-\ntions, and modify the prompt for testing datasets\nwithout additional context, as seen in Appendix A.\n2083\nPrompt GPT3.5 ChatGPT ChatGPT0613 LLaMA2-7B Average\nS P S P S P S P S P\nNaive 0.445 0.521 0.434 0.463 0.416 0.435 0.356 0.272 0.413 0.423\nNaive+R 0.416 0.466 0.464 0.469 0.471 0.480 0.356 0.322 0.427 0.434\nHD 0.509 0.511 0.462 0.472 0.511 0.535 -0.007* 0.046* 0.369 0.391\nHD+R 0.492 0.500 0.497 0.508 0.518 0.526 0.112 0.029* 0.405 0.391\nMEEP 0.475 0.508 0.548 0.553 0.511 0.504 0.333 0.111 0.467 0.419\nMEEP+R 0.494 0.507 0.540 0.543 0.514 0.516 0.402 -0.057* 0.487 0.377\nMEEP+SA 0.532 0.520 0.558 0.573 0.537 0.549 0.410 0.091* 0.509 0.433\nMEEP+SA+R 0.526 0.517 0.568 0.566 0.541 0.548 0.376 0.113 0.503 0.436\nTable 3: Results for prompts: Naive, Human-Directed (HD), and MEEP, with ablation results for prompt elements\nsuch as (SA) and system role (R). Spearman (S) and Pearson (P) results are listed for each experiment. Results\nmarked with * are not statistically significant. The highest result for each column is in bold, with the second highest\nresult underlined. The FED-EN dataset was used for testing.\n6 Results and Analysis\n6.1 Effect of Prompt Styles\nEffect of MEEP Average correlations of system\noutput and human annotations for each prompt are\nshown in Table 3. Prompts including MEEP gen-\nerally give higher correlation than experiments not\nincluding MEEP in the prompts. The method with\nlowest correlation is the human-directed prompt,\nwith average correlations of 0.420 (S) and 0.429\n(P). The MEEP-based prompts have average corre-\nlations of 0.492 (S) and 0.481 (P). This indicates\nthat including our fine-grained subdimensions of\nengagingness in prompting for evaluation of engag-\ningness is an important component of an effective\nevaluation method.\nEffect of ‘such as’ As seen in Table 3, prompts\nwith our engagingness dimensions (MEEP) and the\nuse of ‘such as’ have the highest and second highest\ncorrelations, in 5 of 8 measures. Prompts including\n‘such as’ have average correlations of 0.506 (S) and\n0.492 (P). Their performance dominates both with\nand without defining the system role.\nEffect of system role In Table 3, we see that the\nuse of system role has mixed impact. Performance\nfor ChatGPT and ChatGPT0613 improves with sys-\ntem role in most cases, while performance varies\nfor LLaMA2-7B and generally falls for GPT-3.5.\nDefining the system role gives the most consistent\ngains in performance when used with ChatGPT on\nprompts that do not include MEEP. The inconsis-\ntent improvements with the use of system role may\nbe because it offers a contextual shift that is not as\nclosely aligned as other methods, like MEEP. This\nmore approximate alignment at times moves the\nmodel to a less-aligned position than it would be\nif using a better aligned method alone. GPT-3.5\nalmost entirely performs worse with the prepended\nsystem role, indicating that the system role prompt\nis not beneficial when used with GPT-3.5 in this set-\nting. This combined with mixed performance when\nused on LLaMA2-7B suggests that it may add a\nlevel of complexity to which the simpler models\ncannot rise.\nComparing Models Considering that ChatGPT\nis an interactive LLM specifically trained on con-\nversational data, we are curious to see whether it\nbrings additional gains to our task. In Table 3,\nwe see that ChatGPT has an average of 0.023 (S)\nand 0.012 (P) higher correlations than GPT-3.5\nacross all English-language turn-level experiments.\nChatGPT0613 has similar though slightly lower ag-\ngregate performance to that of ChatGPT. LLaMA\nhas 0.151 (S) and 0.302 (P) lower average perfor-\nmance than GPT3.5, considering statistically signif-\nicant results only. GPT-3.5 has better performance\nthan ChatGPT and ChatGPT0613 for the simplest\nprompts, when used without system role assign-\nment, otherwise ChatGPT is the model that is most\naligned with human annotations.\n6.2 Performance in a Multilingual Setting\n6.2.1 Turn-level Evaluation\nTable 4 reports Spearman coefficients from ex-\nperiments on multilingual turn-level datasets with\nprompts in English, Spanish, and Chinese (Pear-\nson coefficients are available in Appendix C).\nOur highest-performing English prompts have\nan average of 0.064 higher correlation than the\nhighest-performing baseline metric ( G-E VAL or\nGPTS CORE ) for that dataset. Average increases\nof the highest-correlated English prompt over\nthe highest-correlated baseline are 0.060, 0.082,\nand 0.050 for the English, Spanish, and Chinese-\nlanguage datasets, respectively.\n2084\nModel Prompt FED SEE\nEN ES ZH EN ES ZH\nENDEX – 0.290 – – 0.164 – –\nUNIEVAL – 0.190 0.258 0.076* 0.015* 0.073* 0.038*\nGPTS CORE GPTS CORE 0.176 0.146 0.230 0.087* 0.153 0.140\nG-E VAL G-E VAL 0.488 0.448 0.402 0.194 0.131 0.062*\nGPT-3.5 MEEP+SA 0.532 0.481 0.451 0.236 0.223 0.189\nChatGPT MEEP+SA 0.558 0.516 0.431 0.169 0.138 0.133\nChatGPT MEEP+SA+R 0.568 0.542 0.435 0.160 0.150 0.140\nChatGPT-0613 MEEP+SA+R 0.550 0.471 0.400 0.214 0.200 0.175\nGPT-3.5 MEEP+SA† – 0.438 0.520 – 0.128 0.085*\nChatGPT MEEP+SA† – 0.500 0.408 – 0.161 0.149\nChatGPT MEEP+SA+R† – 0.525 0.444 – 0.123 0.168\nChatGPT-0613 MEEP+SA+R† – 0.542 0.374 – 0.273 0.227\nTable 4: Correlation results using Spearman coefficient for multilingual turn-level experiments. All results are\nstatistically significant except those labeled with *. ‘†’ denotes a translated version of the prompt into the language\nof the dataset. Highest results are in bold.\nModel Prompt KDCONV LCCC\nEN ZH EN ZH\nG-E VAL G-E VAL-DIAL 0.327* 0.189 0.149 0.248\nGPT-3.5 MEEP+SA-DIAL 0.286* 0.223* 0.282 0.287\nChatGPT MEEP+SA-DIAL 0.178* 0.392* 0.073* 0.191\nChatGPT MEEP+SA+R-DIAL 0.185* 0.428 -0.004* 0.193\nChatGPT-0613 MEEP+SA+R-DIAL 0.186* 0.362* 0.032* 0.117\nTable 5: Correlation results using Spearman coefficient for multilingual dialogue-level experiments. Results that are\nnot statistically significant are marked with *. Highest scores for each dataset are in bold.\nResults indicate that correlation using our\nmethod is highest for English-language dialogues,\nfollowed by Spanish-language dialogues, with\nChinese-language dialogues having the lowest cor-\nrelations. We would expect to see comparable per-\nformance because they are all high-resource lan-\nguages. The unexpected results may be related to\ntranslation quality of the datasets, which is lower\nfor the Chinese datasets as seen in Table 2. A\ncomparison of results for the Spanish and Chinese\ndatasets in Figure 4 shows that translating a prompt\ninto the language of the dataset does not consis-\ntently improve correlation, but our best scores for\nthe Spanish and Chinese datasets are nevertheless\nseen with translated prompts. The ChatGPT0613\nmodel provides several of these highest correla-\ntions, indicating that improved system role capabil-\nities with this model may have included multilin-\ngual system role training.\n6.2.2 Dialogue-level Evaluation\nResults for dialogue-level evaluation are in Table\n5. Achieving statistically significant results is less\nconsistent on the dialogue-level than on the turn-\nlevel datasets, especially for the translated English-\nlanguage versions. Despite this, we can see better\nperformance with our prompts than with the base-\nline.\nFigure 4: Correlation of each prompt on the Spanish\nand Chinese datasets.\nQualitative Analysis An example from qualita-\ntive analysis is provided in Table 6, with more ex-\namples in Appendix D. The randomly-selected ex-\namples show mixed consistency with general trends\nobserved with testing. We note that MEEP prompts\nimprove performance over aggregate results from\nHD and Naive prompts, although exceptions can be\nfound. The effect of the system role is difficult to\ndiscern at this scale. ChatGPT performs better or\nsimilarly to GPT-3.5 in four out of five examples,\nholding with the pattern exhibited in the data.\nWe see a pattern of GPT-3.5 more often giving\nscores that are higher than the ground truth anno-\ntations. When we average the scores for the top-\nperforming models, GPT-3.5 and ChatGPT, across\nall the model/prompt combinations used in Table 3,\nwe find that GPT-3.5 produces scores appreciably\nhigher than ChatGPT, with averages of 0.8138 and\n2085\nDialogue: \"Hi!\"\n\"Hiii! How are you?\"\n\"I am good\"\n...\n\"You’re welcome! How long until you’re done with train-\ning models?\"\n\"We keep on trying to improve them so I guess it’ll be a\nwhile\"\nResponse: \"That’s understandable. Good luck, I hope it\ngoes smoothly!\"\nNormalized average of human annotations: 0.6\nPrompt GPT-3.5 δ ChatGPT δ\nNaive 0.9 0.3 0.85 0.25\nNaive + R 1.0 0.4 0.85 0.25\nHD 1.0 0.4 0.5 0.1\nHD+R 1.0 0.4 0.5 0.1\nMEEP 0.9 0.3 0.8 0.2\nMEEP+R 0.9 0.3 0.8 0.2\nMEEP+SA 0.8 0.2 0.8 0.2\nMEEP+SA+R 0.9 0.3 0.8 0.2\nTable 6: Comparison of results for one example dia-\nlogue from the FED dataset for qualitative analysis. The\ndistance from the ground truth (δ) for each score is listed\nto the right of the score.\n0.7307, respectively. This indicates that GPT-3.5\nhas a positive bias in evaluation when compared to\nChatGPT, which may reveal underlying limitations\nin the model’s capability.\n7 Discussion\nInsights from our experiments showcase the power\nof our proposed LLM-based evaluator as follows:\n• Our MEEP dimensions of engagingness im-\nprove alignment with human annotations and\nare an effective component of an LLM-based\nmetric, improving over state-of-the-art met-\nrics in evaluation of engagingness.\n• Clear improvement is seen with the use of\nsuch as with clarifying examples in this con-\ntext and we conclude that examples written\nin this form will improve dialogue evaluation\nperformance with these LLMs.\n• Defining the system role offers improvement\nwhen used with ChatGPT or ChatGPT0613\nand prompts that are naive. It does not im-\nprove performance when used with LLaMA2-\n7B, or GPT-3.5.\n• ChatGPT performs better than GPT-3.5 with\nmore complex prompts. When prompts are\nin their simplest form, GPT-3.5 has higher\ncorrelation with human annotations. LLaMA\ngives highest correlations when used with the\nnaive prompt. We infer that to see an increase\nwith the more powerful model, the context\nmust be appropriately set, as with MEEP+SA,\nand that simpler prompts are more appropriate\nfor smaller LLMs.\n• The results for our MEEP prompts used on\nmultilingual datasets show improvement over\nstate-of-the-art baselines in the evaluation\nof engagingness in dialogue across Chinese,\nSpanish, and English. This is also consistent\nacross turn-level and dialogue-level datasets.\n• In the multilingual setting, for turn-level dia-\nlogues, automatic evaluation is most strongly\ncorrelated with human evaluation when the\nprompt is in the language of the dialogues.\n8 Conclusion and Future Work\nWe propose a novel method for estimating engag-\ningness in dialogue using LLMs as automatic eval-\nuators. Our method – MEEP – outperforms state-\nof-the-art baselines, and when used in prompts con-\ntaining a ‘such as’ phrase with examples, leads\nto better correlation with human annotated scores.\nThis performance is demonstrated in a multilingual\ncontext, using prompts and dialogues in English,\nSpanish, and Chinese and for both turn-level and\ndialogue-level evaluation. Our findings indicate\nthat there is promise in the evaluation of other com-\nplex dialogue qualities – such as a model’s ability\nto provide emotional support – through a similar\nuse of prompting with LLMs. We see opportuni-\nties for future work in the development and use\nof non-translated datasets in languages other than\nEnglish, with annotations for a well-defined mea-\nsure of engagingness. In the future, we would like\nto continue to explore lighter models like LLaMA\n(Touvron et al., 2023) or ORCA (Mukherjee et al.,\n2023) with our method for a more energy-efficient\napproach (Appendix E).\nEthical Considerations\nThe use of LLM-based metrics in evaluating lan-\nguage models raises concerns regarding potential\nbias (Gao and Wan, 2022) and self-reinforcement\n(Liu et al., 2023). Language models like GPTs\nare trained on large datasets, which may contain\nbiases and inaccuracies that can impact evaluation\ntasks. This is particularly important in the context\nof self-AI feedback (Fernandes et al., 2023), where\nLLM-based metrics may prefer LLM-generated\n2086\ntexts, potentially reinforcing biases. More specifi-\ncally, the dialogues and annotations are produced\nby crowd workers with unknown backgrounds. Ide-\nally, they would represent a wide range of ethnic,\nracial, and economic backgrounds, with varied di-\nalects. We are not aware of the composition of the\nworkers selected. Since their annotations become\nour ground-truth, there is a possibility that we base\nour evaluation of engagingness on that of a pop-\nulation that does not fully represent world-wide\ndiversity. Differences in dialect, tone, or wordiness\ncan be interpreted uniquely depending on cultural\nor individual preferences. By potentially limiting\nour definition of engagingness to what is seen in\nthese datasets, we admit the possibility of training\ndialogue systems that are engaging to a limited pop-\nulation, limiting the accessibility of the tools that\nuse these systems.\nWhile more engaging dialogue systems have\npromising applications in domains like virtual as-\nsistants and medical support, the use of evaluation\nmetrics beyond evaluation can lead to unintended\nconsequences. Additionally, ethical issues arise\ndue to the lack of transparency of AI models, as\nwell as privacy and data security concerns when\nhandling sensitive information. It is crucial to be\naware of these considerations and prioritize the eth-\nical and responsible use of LLMs and evaluation\nmetrics.\nLimitations\nOur research on engagingness in conversations is\nlimited to existing datasets available for this spe-\ncific quality. We see promise in the creation of\na dataset with intentionally diverse perspectives\non engagingness for the development of evalua-\ntion metrics that can represent the plurality of user\nbackgrounds.\nOur evaluation of prompt styles is not exhaustive,\nas the possibilities are vast. We limit our prompt\nstyles to those found to be useful in existing re-\nsearch, or with a strong theoretical basis to support\nthem. We leave for further exploration the evalua-\ntion of a wider range of prompt styles.\nIn Spanish- and Chinese-language dialogue eval-\nuation, our findings are limited to the evaluation\nof translations from English. For a more robust\nmultilingual evaluation, we would use datasets cre-\nated in each language and evaluated by speakers of\nthose languages.\nOur experiments with LLaMA use only the\nsmallest version due to limited resources.\nThe lack of transparency in AI models presents\na challenge as it hampers a comprehensive under-\nstanding of the assessment process. These limi-\ntations highlight the importance of further explo-\nration, diverse datasets, and increased transparency\nto strengthen the validity and applicability of our\nresearch findings on engagingness in conversations.\nAcknowledgements\nWe thank the anonymous reviewers and the\nPortNLP group for their insightful comments and\nsuggestions. We express gratitude to Yufei Tao and\nRussell Scheinberg for assistance with translation,\nand to Aekta Shah, PhD for suggesting othering\nand belonging as a possible impact on engaging-\nness. This research was supported by the National\nScience Foundation under Grant No. CRII:RI-\n2246174.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R. So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\nand Quoc V . Le. 2020. Towards a human-like open-\ndomain chatbot. CoRR, abs/2001.09977.\nVibhav Agarwal, Pooja Rao, and Dinesh Babu Jayagopi.\n2021. Towards code-mixed Hinglish dialogue gener-\nation. In Proceedings of the 3rd Workshop on Natural\nLanguage Processing for Conversational AI, pages\n271–280, Online. Association for Computational Lin-\nguistics.\nJessica Joelle Alexander and Camilla Semlov Ander-\nsson. 2022. Fællesskab and belonging. In Míriam\nJuan-Torres González, Yvette Tetteh, and EJ Top-\npin, editors, Paper Series: On Belonging in Europe.\nOthering Belonging Institute.\nCOCA. Corpus of Contemporary American English.\nhttps://www.english-corpora.org/coca/. Ac-\ncessed: June 5, 2023.\nDeborah Cohen, Moonkyung Ryu, Yinlam Chow, Orgad\nKeller, Ido Greenberg, Avinatan Hassidim, Michael\nFink, Yossi Matias, Idan Szpektor, Craig Boutilier,\nand Gal Elidan. 2022. Dynamic planning in open-\nended dialogue using reinforcement learning.\nPierre Colombo, Maxime Peyrard, Nathan Noiry,\nRobert West, and Pablo Piantanida. 2022. The glass\nceiling of automatic evaluation in natural language\ngeneration.\nMingkai Deng, Bowen Tan, Zhengzhong Liu, Eric Xing,\nand Zhiting Hu. 2021. Compression, transduction,\nand creation: A unified framework for evaluating\n2087\nnatural language generation. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7580–7605, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nOxford English Dictionary. 2023. Engaging.\nDSTC11 2023. Dstc11: Dialogue system technol-\nogy challenge 11. https://chateval.org/dstc11.\n[Accessed 06-Jun-2023].\nPatrick Fernandes, Aman Madaan, Emmy Liu, António\nFarinhas, Pedro Henrique Martins, Amanda Bertsch,\nJosé G. C. de Souza, Shuyan Zhou, Tongshuang\nWu, Graham Neubig, and André F. T. Martins. 2023.\nBridging the gap: A survey on integrating (human)\nfeedback for natural language generation.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire.\nMingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Ship-\ning Yang, and Xiaojun Wan. 2023. Human-like sum-\nmarization evaluation with chatgpt.\nMingqi Gao and Xiaojun Wan. 2022. Social biases in\nautomatic evaluation metrics for nlg.\nNan Gao, Mohammad Saiedur Rahaman, Wei Shao, and\nFlora D. Salim. 2021. Investigating the reliability of\nself-report data in the wild: The quest for ground\ntruth.\nSebastian Gehrmann, Elizabeth Clark, and Thibault Sel-\nlam. 2022. Repairing the cracked foundation: A sur-\nvey of obstacles in evaluation practices for generated\ntext.\nSarik Ghazarian, Ralph Weischedel, Aram Galstyan,\nand Nanyun Peng. 2020. Predictive engagement:\nAn efficient metric for automatic evaluation of open-\ndomain dialogue systems. Proceedings of the AAAI\nConference on Artificial Intelligence, 34(05):7789–\n7796.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow NOT to evaluate your dialogue system: An\nempirical study of unsupervised evaluation metrics\nfor dialogue response generation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2122–2132, Austin,\nTexas. Association for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nVarvara Logacheva, Mikhail Burtsev, Valentin Malykh,\nVadim Polulyakh, and Aleksandr Seliverstov. 2018.\nConvai dataset of topic-oriented human-to-chatbot\ndialogues. In The NIPS ’17 Competition: Building\nIntelligent Systems, pages 47–57, Cham. Springer\nInternational Publishing.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency evaluator\nfor text summarization.\nShikib Mehri, Jinho Choi, Luis Fernando D’Haro, Jan\nDeriu, Maxine Eskenazi, Milica Gasic, Kallirroi\nGeorgila, Dilek Hakkani-Tur, Zekang Li, Verena\nRieser, Samira Shaikh, David Traum, Yi-Ting Yeh,\nZhou Yu, Yizhe Zhang, and Chen Zhang. 2022. Re-\nport from the nsf future directions workshop on auto-\nmatic evaluation of dialog: Research directions and\nchallenges.\nShikib Mehri and Maxine Eskenazi. 2020a. Unsuper-\nvised evaluation of interactive dialog with DialoGPT.\nIn Proceedings of the 21th Annual Meeting of the\nSpecial Interest Group on Discourse and Dialogue,\npages 225–235, 1st virtual meeting. Association for\nComputational Linguistics.\nShikib Mehri and Maxine Eskenazi. 2020b. USR: An\nunsupervised and reference free evaluation metric\nfor dialog generation. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 681–707, Online. Association for\nComputational Linguistics.\nMerriam-Webster. 2023. Engaging.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawa-\nhar, Sahaj Agarwal, Hamid Palangi, and Ahmed\nAwadallah. 2023. Orca: Progressive learning from\ncomplex explanation traces of gpt-4.\nJekaterina Novikova, Ond ˇrej Dušek, Amanda Cer-\ncas Curry, and Verena Rieser. 2017. Why we need\nnew evaluation metrics for NLG. In Proceedings of\nthe 2017 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2241–2252, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic evalu-\nation of machine translation. In Proceedings of the\n2088\n40th Annual Meeting on Association for Computa-\ntional Linguistics, ACL ’02, page 311–318, USA.\nAssociation for Computational Linguistics.\nJohn A. Powell and Stephen Menendian. 2022. On\nbelonging: An introduction to othering belonging\nin europe. In Míriam Juan-Torres González, Yvette\nTetteh, and EJ Toppin, editors, Paper Series: On\nBelonging in Europe. Othering Belonging Institute.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2022.\nRobust speech recognition via large-scale weak su-\npervision.\nRicardo Rei, Ana C Farinha, Chrysoula Zerva, Daan\nvan Stigt, Craig Stewart, Pedro Ramos, Taisiya\nGlushkova, André F. T. Martins, and Alon Lavie.\n2021. Are references really needed? unbabel-IST\n2021 submission for the metrics shared task. In Pro-\nceedings of the Sixth Conference on Machine Trans-\nlation, pages 1030–1040, Online. Association for\nComputational Linguistics.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. Unbabel’s participation in the WMT20\nmetrics shared task. In Proceedings of the Fifth Con-\nference on Machine Translation, pages 911–920, On-\nline. Association for Computational Linguistics.\nChristopher Richardson, Sudipta Kar, Anjishnu Ku-\nmar, Anand Ramachandran, Omar Zia Khan, Zeynab\nRaeesy, and Abhinav Sethy. 2023. Learning to re-\ntrieve engaging follow-up queries. In EACL 2023.\nMario Rodríguez-Cantelar, Chen Zhang, Chengguang\nTang, Ke Shi, Sarik Ghazarian, João Sedoc, Luis Fer-\nnando D’Haro, and Alexander Rudnicky. 2023.\nOverview of robust and multilingual automatic eval-\nuation metrics for open-domain dialogue systems at\ndstc 11 track 4.\nRobert Rosenman, Vidhura Tennekoon, and Laura G.\nHill. 2011. Measuring bias in self-reported data. In-\nternational Journal of Behavioural and Healthcare\nResearch, 2(4):320–332.\nAbigail See, Stephen Roller, Douwe Kiela, and Jason\nWeston. 2019. What makes a good conversation?\nhow controllable attributes affect human judgments.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1702–1723,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nXinyue Shen, Zeyuan Chen, Michael Backes, and Yang\nZhang. 2023. In chatgpt we trust? measuring and\ncharacterizing the reliability of chatgpt.\nKatherine Stasaski and Marti A. Hearst. 2023. Pragmat-\nically appropriate diversity for dialogue evaluation.\nEkaterina Svikhnushina and Pearl Pu. 2023. Approx-\nimating human evaluation of social chatbots with\nprompting.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nMarilyn A. Walker, Diane J. Litman, Candace A. Kamm,\nand Alicia Abella. 1997. PARADISE: A frame-\nwork for evaluating spoken dialogue agents. In\n35th Annual Meeting of the Association for Com-\nputational Linguistics and 8th Conference of the Eu-\nropean Chapter of the Association for Computational\nLinguistics, pages 271–280, Madrid, Spain. Associa-\ntion for Computational Linguistics.\nJiaan Wang, Yunlong Liang, Fandong Meng, Zengkui\nSun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu,\nand Jie Zhou. 2023. Is chatgpt a good nlg evaluator?\na preliminary study.\nYida Wang, Pei Ke, Yinhe Zheng, Kaili Huang, Yong\nJiang, Xiaoyan Zhu, and Minlie Huang. 2020. A\nlarge-scale chinese short-text conversation dataset.\nIn Natural Language Processing and Chinese Com-\nputing: 9th CCF International Conference, NLPCC\n2020, Zhengzhou, China, October 14–18, 2020, Pro-\nceedings, Part I, page 91–103, Berlin, Heidelberg.\nSpringer-Verlag.\nWiktionary. 2023. Engaging.\nGuangxuan Xu, Ruibo Liu, Fabrice Harel-Canada, Nis-\nchal Reddy Chandra, and Nanyun Peng. 2022. En-\nDex: Evaluation of dialogue engagingness at scale.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 4884–4893, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nZhou Yu, Leah Nicolich-Henkin, Alan W Black, and\nAlexander Rudnicky. 2016. A Wizard-of-Oz study\non a non-task-oriented dialog systems that reacts to\nuser engagement. In Proceedings of the 17th Annual\nMeeting of the Special Interest Group on Discourse\nand Dialogue, pages 55–63, Los Angeles. Associa-\ntion for Computational Linguistics.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. In Advances in Neural Information Processing\nSystems, volume 34, pages 27263–27277. Curran As-\nsociates, Inc.\nChen Zhang, Yiming Chen, Luis Fernando D’Haro,\nYan Zhang, Thomas Friedrichs, Grandee Lee, and\nHaizhou Li. 2021. DynaEval: Unifying turn and di-\nalogue level evaluation. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5676–5689, Online. Association\nfor Computational Linguistics.\n2089\nChen Zhang, Luis Fernando D’Haro, Qiquan Zhang,\nThomas Friedrichs, and Haizhou Li. 2022a. FineD-\neval: Fine-grained automatic dialogue-level evalu-\nation. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3336–3355, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nQingyu Zhang, Xiaoyu Shen, Ernie Chang, Jidong Ge,\nand Pengke Chen. 2022b. Mdia: A benchmark for\nmultilingual dialogue generation in 46 languages.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2204–2213,\nMelbourne, Australia. Association for Computational\nLinguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi. 2017.\nLearning discourse-level diversity for neural dialog\nmodels using conditional variational autoencoders.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 654–664, Vancouver, Canada.\nAssociation for Computational Linguistics.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. 2022. Towards a unified multi-\ndimensional evaluator for text generation. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 2023–\n2038, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nHao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang,\nand Xiaoyan Zhu. 2020. KdConv: A Chinese\nmulti-domain dialogue dataset towards multi-turn\nknowledge-driven conversation. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 7098–7108, Online. As-\nsociation for Computational Linguistics.\nPei Zhou, Hyundong Cho, Pegah Jandaghi, Dong-Ho\nLee, Bill Yuchen Lin, Jay Pujara, and Xiang Ren.\n2022. Reflect, not reflex: Inference-based common\nground improves dialogue response quality. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 10450–\n10468, Abu Dhabi, United Arab Emirates. Associa-\ntion for Computational Linguistics.\n2090\nA Full Text of Prompts\nPrompt\nCode\nDescription\nNaive Score the following response given the corresponding dialogue context on a continuous\nscale from 0 to 100, where a score of zero means ‘disengaging’ and a score of 100 means\n‘very engaging’. Assume the response immediately follows the dialogue context.\nDialogue context: <dialogue>\nResponse: <dialogue>\nScore:\nHD You will be given a conversation between two individuals. You will then be given several\npotential responses for the next turn in the conversation. Is the response engaging?\nA response is considered engaging if it can engage the user. This might be an inquisitive\nquestion or an interesting response that can be followed-up on.\nA score of 1 means No: the response is boring and does little to engage the user.\n· Hi there.\n· Oh wow! That’s cool!\nA score of 2 means Somewhat: the response is not particularly engaging but still leaves\nroom for follow-up.\n· My favourite colour is blue.\n· Nope. I’m not very good with cooking.\nA score of 3 means Yes: the response is actively engaging the user and trying to move\nforward the conversation.\n· I have a feeling that if you can dodge a wrench you can dodge a ball.\n· What kind of shows do you like?\nDialogue context: <dialogue>\nResponse: <dialogue>\nScore:\nTable 7: Full text for English-language prompts Naive and HD used in our experiments.\n2091\nMEEP Score the following response given the corresponding dialogue context on a continuous\nscale from 0 to 100, where a score of zero means ‘disengaging’ and a score of 100 means\n‘very engaging’. Assume the response immediately follows the dialogue context. Consider\nthat engagingness of a response is defined by the following qualities: variety of response ac-\ncording to the context, likelihood of encouraging the other participant to respond, likelihood\nof encouraging a quality response from the other participant, interestingness, specificity, and\nlikelihood of creating a sense of belonging for the other participant.\nDialogue context: <dialogue>\nResponse: <response>\nScore:\nMEEP\n+ SA\nScore the following response given the corresponding dialogue context on a continuous\nscale from 0 to 100, where a score of zero means ‘disengaging’ and a score of 100 means\n‘very engaging’. Assume the response immediately follows the dialogue context. Consider\nthat engagingness of a response is defined by the following qualities: variety of response\naccording to the context (such as responding to ‘Hi how are you?’ with ‘I feel magnificent,\nbecause I just successfully defended my PhD! How are you?’ instead of ‘Good, how are\nyou?’), likelihood of encouraging the other participant to respond (such as ‘I love legos!\nI like using them to make funny things. Do you like legos?’ instead of ‘I like legos.’),\nlikelihood of encouraging a quality response from the other participant, interestingness,\nspecificity, and likelihood of creating a sense of belonging for the other participant.\nDialogue context: <dialogue>\nResponse: <response>\nScore:\nTable 8: Full text for English-language prompts MEEP and MEEP+SA used in our experiments.\n2092\nPrompt\nCode\nDescription\nES(MEEP\n+SA)\nEvalúa la siguiente respuesta dada el contexto de diálogo correspondiente en una escala\ncontinua del 0 al 100, donde una puntuación de cero significa “desinteresante” y una\npuntuación de 100 significa “muy interesante”. Supongamos que la respuesta sigue\ninmediatamente después del contexto de diálogo. Considera que la cualidad de una\nrespuesta interesante se define por las siguientes características: variedad de respuesta\nde acuerdo al contexto (por ejemplo, responder a “Hola, ¿cómo estás?” con “Me siento\nmagnífico porque acabo de defender exitosamente mi tesis doctoral. ¿Y tú?” en lugar de\n“Bien, ¿y tú?”), probabilidad de incentivar al otro participante a responder (por ejemplo,\n“¡Me encantan los legos! Me gusta usarlos para hacer cosas divertidas. ¿Te gustan los\nlegos?” en lugar de “Me gustan los legos.”), probabilidad de incentivar una respuesta de\ncalidad del otro participante, interés, especificidad y probabilidad de crear un sentido de\npertenencia para el otro participante.\nContexto de diálogo: <dialogue>\nRespuesta: <response>\nPuntuación:\nZH(MEEP\n+SA)\n根据相应的对话背景，对以下回应在0到100的连续刻度上进行评分，其中0分表\n示“不吸引人”，100分表示“非常吸引人”。假设回应紧随对话背景之后。请考虑\n回应的吸引力是由以下特质定义的：根据背景的回应多样性（例如对“你好，你\n怎么样？”的回应是“我感觉很棒，因为我刚成功地为我的博士学位进行了答辩！\n你怎么样？”而不是“好的，你怎么样？”）、鼓励其他参与者回应的可能性（例\n如：“我喜欢乐高！我喜欢用它们制作有趣的东西。你喜欢乐高吗？”而不是“我\n喜欢乐高。”）、鼓励其他参与者提供高质量回应的可能性、有趣性、具体性和\n为其他参与者创造归属感的可能性.\n对话背景：<dialogue>\n回应：<response>\n评分：\n+R You are an expert evaluator of dialogue.\nES(+R) Eres experto en evaluación de diálogos.\nZH(+R) 你是一名对话评估的专家。\nTable 9: Full text for MEEP+SA prompt translated into Spanish and Chinese; system role prompt in English,\nSpanish, and Chinese\n2093\nPrompt\nCode\nDescription\nMEEP\n+SA\n-DIAL\nIn this task, you will be shown part of a dialogue. Score the dialogue on a continuous\nscale from 0 to 100, where a score of zero means ‘disengaging’ and a score of 100 means\n‘very engaging’. Consider that engagingness of each dialogue is defined by the following\nqualities: variety of responses according to the context (such as responding to ‘Hi how\nare you?’ with ‘I feel magnificent, because I just successfully defended my PhD! How are\nyou? instead of ‘Good, how are you?’), likelihood of encouraging the other participant to\nrespond (such as ‘I love legos! I like using them to make funny things. Do you like legos?’\ninstead of ‘I like legos.’), likelihood of encouraging quality responses from the other\nparticipant, interestingness, specificity, and likelihood of creating a sense of belonging\nfor the other participant. Consider the overall engagingness of the conversation.\nDialogue: <dialogue>\nScore: <score>\nG-EV AL\n-DIAL\nYou will be given a conversation between two individuals.\nYour task is to rate the conversation on one metric.\nPlease make sure you read and understand these instructions carefully. Please keep this\ndocument open while reviewing, and refer to it as needed.\nEvaluation Criteria:\nEngagingness (1-3) Is the conversation dull/interesting?\n- A score of 1 (dull) means that the conversation is generic and dull.\n- A score of 2 (somewhat interesting) means the conversation is somewhat interesting and\ncould engage you in the conversation (e.g., an opinion, thought)\n- A score of 3 (interesting) means the conversation is very interesting or presents an\ninteresting fact\nEvaluation Steps:\n1. Read the conversation carefully.\n2. Rate the conversation on a scale of 1-3 for engagingness, according to the criteria\nabove.\n3. Provide a brief explanation for your rating, referring to specific aspects of the conver-\nsation.\nExample:\nConversation: <dialogue>\nEvaluation Form (scores ONLY):\n- Engagingness:\nTable 10: Full text for dialogue-level MEEP+SA-DIAL and G-EV AL-DIALprompts. G-Eval prompts are edited\nslightly for applicability to dialogue-level evaluation.\n2094\nPrompt\nCode\nDescription\nGPTS CORE Answer the question based on the conversation between a human and AI.\nQuestion: Are the responses of AI engaging? (a) Yes. (b) No.\nDialogue context: <conversation>\nResponse: <response>\nAnswer: Yes.\nG-E VAL You will be given a conversation between two individuals. You will then be given one\npotential response for the next turn in the conversation.\nYour task is to rate the responses on one metric.\nPlease make sure you read and understand these instructions carefully. Please keep this\ndocument open while reviewing, and refer to it as needed.\nEvaluation Criteria:\nEngagingness (1-3) Is the response dull/interesting?\n- A score of 1 (dull) means that the response is generic and dull.\n- A score of 2 (somewhat interesting) means the response is somewhat interesting and\ncould engage you in the conversation (e.g., an opinion, thought)\n- A score of 3 (interesting) means the response is very interesting or presents an interest-\ning fact\nEvaluation Steps:\n1. Read the conversation, the corresponding fact and the response carefully.\n2. Rate the response on a scale of 1-3 for engagingness, according to the criteria above.\n3. Provide a brief explanation for your rating, referring to specific aspects of the response\nand the conversation.\nExample:\nConversation History: <dialogue>\nResponse: <response>\nEvaluation Form (scores ONLY):\n- Engagingness:\nTable 11: Full text for GPTS CORE and G-E VAL prompts.\n2095\nB Translation Quality Metrics\nCOMET-20 and COMET-21 are quality es-\ntimator metrics from the COMET models\nwmt20-comet-qe-da-v2 (Rei et al., 2020) and\nwmt21-comet-qe-mqm (Rei et al., 2021). CosSim1\nand CosSim2 measure cosine similarity of the orig-\ninal utterance and the translation after generat-\ning embeddings for both with the SentenceTrans-\nformer library 7. They use the multilingual mod-\nels distiluse-base-multilingual-cased-v1,\nand paraphrase-xlm-r-multilingual-v1 re-\nspectively.\n7https://www.sbert.net/\n2096\nC Results for Multilingual Datasets - Pearson Coefficients\nModel Prompt FED-EN FED-ES FEZ-ZH SEE-EN SEE-ES SEE-ZH\nENDEX – 0.260 – – 0.159 – –\nUNIEVAL – 0.155 0.161 0.079* 0.008 * 0.063* 0.024*\nGPTS CORE GPTS CORE 0.174 0.102 0.222 0.076* 0.158 0.115\nG-E VAL G-E VAL 0.452 0.427 0.371 0.254 0.219 0.171\nGPT-3.5 MEEP+SA 0.520 0.495 0.467 0.265 0.254 0.202\nChatGPT MEEP+SA 0.573 0.562 0.449 0.205 0.172 0.168\nChatGPT MEEP+SA+R 0.566 0.573 0.439 0.213 0.197 0.179\nChatGPT-0613 MEEP+SA+R 0.556 0.490 0.406 0.277 0.250 0.231\nGPT-3.5 MEEP+SA† - 0.020* 0.493 - -0.092* 0.092*\nChatGPT MEEP+SA† - 0.498 0.414 - 0.185 0.174\nChatGPT MEEP+SA+R† - 0.512 0.440 - 0.166 0.168\nChatGPT-0613 MEEP+SA+R† - 0.519 0.362 - 0.283 0.263\nTable 12: Pearson correlation results on multilingual datasets. ‘†’ denotes a human-translated version of the prompt\ninto the language of the dataset. All results are statistically significant except those labeled with *. Best results are\nhighlighted in bold. Second best results are underlined.\n2097\nD Select Examples from English-language FED dataset\nDialogue: \"Hi!\"\n\"Hey there! How’s it going?\"\n\"Hi, how are you?\"\n\"I’m doing well, thank you. How are you?\"\n\"I’m fine. What’s up?\"\n\"Not much. How about you?\"\n\"nothing new. Anything exciting happening recently?\"\n\"Not really. You?\"\n\"I watched a movie recently which was pretty good\"\n\"That’s good. What movie?\"\n\"the irishman, have you seen it?\"\n\"I have not. Should I?\"\n\"I think so\"\n\"Alright, I’ll put it on my watch list.\"\n\"How about you? Have you watched anything interesting recently?\"\n\"I’ve been waiting to see the new Star Trek movie.\"\n\"what is it about?\"\n\"The adventures of Kirk, Spock, and McCoy.\"\n\"when was it released?\"\n\"It just came out today.\"\n\"are you sure, I think it’s been out for a while\"\n\"I’m positive. I know because I just watched it.\"\n\"well, the fact you just watched it doesn’t mean it was released today\"\n\"That’s what I said, I did watch it today.\"\n\"ok, I’ll put it on my watchlist\"\n\"Let me know what you think of it when you watch it.\"\n\"sure, I need to go, nice talking to you\"\nResponse: \"Good talking to you too.\"\nNormalized average of human annotations: 0.4\nPrompt GPT-3.5 δ ChatGPT δ\nNaive 0.8 0.4 0.8 0.4\nNaive + R 0.8 0.4 0.8 0.4\nHD 0.0 0.4 0.5 0.1\nHD+R 0.5 0.1 0.5 0.1\nMEEP 0.7 0.2 0.5 0.1\nMEEP+R 0.7 0.2 0.5 0.1\nMEEP+SA 0.7 0.2 0.5 0.1\nMEEP+SA+R 0.7 0.2 0.5 0.1\nTable 13: Comparison of results for one example dialogue from the FED dataset for qualitative analysis. The\ndistance from the ground truth (δ) for each score is listed to the right of the score.\nDialogue: \"Hi!\"\n\"Have you seen any good movies lately?\"\n\"Last weekend I saw ‘The Parasite’. Ever heard of it?\"\nResponse: \"No. Why did you pick that movie?\"\nNormalized average of human annotations: 0.9\nPrompt GPT-3.5 δ ChatGPT δ\nNaive 0.8 0.1 0.6 0.3\nNaive + R 0.8 0.1 0.6 0.3\nHD 1.0 0.1 0.5 0.4\nHD+R 1.0 0.1 0.5 0.4\nMEEP 0.8 0.1 0.6 0.3\nMEEP+R 0.8 0.1 0.6 0.3\nMEEP+SA 0.8 0.1 0.6 0.3\nMEEP+SA+R 0.8 0.1 0.6 0.3\nTable 14: Comparison of results for one example dialogue from the FED dataset for qualitative analysis. The\ndistance from the ground truth (δ) for each score is listed to the right of the score.\n2098\nDialogue: \"Hi!\"\n\"Hey!\"\n\"How’s your day going?\"\n\"Having fun, but have a bit of a headache How about you?\"\n\"I’m good! Just getting ready to watch a basketball game. Hope your headache goes away!\"\n\"Haha, thanks! Never get them, so it’s super weird Who’s playing?\"\n\"Hmm, try taking some REDACTED_TERM! I’m going to see the Warriors vs. the Grizzlies. Ever see them play?\"\n\"I haven’t Honestly I probably wouldn’t be able to identify which sport they played\"\n\"Haha, all the animal names confuse me as well! Who came up with these naming conventions? I’d name my team\nsomething more original: The New York Chairs!, sounds better to me :) I’m actually at the REDACTED_TERM\nopening remarks\"\n\"Oh cool! What is ‘NeurIPS’?\"\n\"Neural Information Processing Systems\"\n\"Oh wow! Sounds fancy! What is that about?\"\n\"It’s the biggest ML conference in the world\"\n\"Oh, ML?\"\nResponse: \"Yes, machine learning\"\nNormalized average of human annotations: 0.8\nPrompt GPT-3.5 δ ChatGPT δ\nNaive 0.8 0.0 0.8 0.0\nNaive + R 0.8 0.0 0.9 0.1\nHD 0.0 0.8 1.0 0.2\nHD+R 0.0 0.8 1.0 0.2\nMEEP 0.4 0.4 0.8 0.0\nMEEP+R 0.7 0.1 0.7 0.1\nMEEP+SA 0.4 0.4 0.7 0.1\nMEEP+SA+R 0.4 0.4 0.7 0.1\nTable 15: Comparison of results for one example dialogue from the FED dataset for qualitative analysis. The\ndistance from the ground truth (δ) for each score is listed to the right of the score.\nDialogue: \"Hi!\"\n\"Hi, how’s it going?\"\n\"All good! It’s cold\"\nResponse: \"It’s pretty cold here too.\"\nNormalized average of human annotations: 0.7\nPrompt GPT-3.5 δ ChatGPT δ\nNaive 0.8 0.1 0.7 0.0\nNaive + R 0.8 0.1 0.8 0.1\nHD 0.5 0.2 0.5 0.2\nHD+R 0.5 0.2 0.5 0.2\nMEEP 0.8 0.1 0.7 0.0\nMEEP+R 0.8 0.1 0.7 0.0\nMEEP+SA 0.7 0.0 0.5 0.2\nMEEP+SA+R 0.7 0.0 0.5 0.2\nTable 16: Comparison of results for one example dialogue from the FED dataset for qualitative analysis. The\ndistance from the ground truth (δ) for each score is listed to the right of the score.\n2099\nE Carbon Emissions\nResearchers are actively considering environmental\nimplications and making efforts to address and re-\nduce the effects associated with the deployment of\nlarge-scale NLP models. CodeCarbon.io is a dedi-\ncated emission tracker library designed to quantify\ncarbon emissions accurately. NLP techniques al-\nways vary in accuracy and generalizability depend-\ning upon hardware variations. We addressed this\nby accounting for our hardware specification and\nrecorded reliable emissions estimations and pro-\ngrammatic energy usage readings from CodeCar-\nbon. The total energy consumed (E) is determined\nusing the following formula:\nE(kWh) = 1.103 ∗ codecarbonkWh\nWith our CO2 emission results, we converted\nour emissions at the time of submission to human-\nunderstandable emission parameters like “miles\ndriven by an average gasoline-powered passen-\nger vehicle” using EPA Greenhouse Gas Equiva-\nlencies Calculator and found that our total emission\nfor our core research phase was about 0.16 miles\n“driven by an average gasoline-powered passen-\nger vehicle”. This does not include energy usage\nby OpenAI to service our API calls. It is also note-\nworthy that our GPU energy consumption is 0.854\nkWh which is comparable to 0.0001 “barrels of\noil consumed”, whereas a full masked language\nmodel training cost 1200 times higher. We also\nanalyze that energy usage and efficiency are es-\nsentially a function of running time, assuming the\nsame hardware.\nCarbon Emission\nParameter Data Recorded\nDuration 18 Hr\nEmissions 1.44E-01\nRmissions Rate 2.8E-03\nGPU Power 83E+02\nGPU Energy 8.54E-01\nEnergy Consumed 1.04E+00\nTable 17: Carbon Emission data for this project. The\nrecorded data is inclusive of failed and successful test\ncases during this project’s core phase.\n2100",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8028334379196167
    },
    {
      "name": "Conversation",
      "score": 0.7846702337265015
    },
    {
      "name": "Fluency",
      "score": 0.7769194841384888
    },
    {
      "name": "Natural language processing",
      "score": 0.5038556456565857
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5021986961364746
    },
    {
      "name": "Process (computing)",
      "score": 0.4789586365222931
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4639427661895752
    },
    {
      "name": "Economic shortage",
      "score": 0.45818084478378296
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.4305974245071411
    },
    {
      "name": "Human–computer interaction",
      "score": 0.32326096296310425
    },
    {
      "name": "Linguistics",
      "score": 0.2956714332103729
    },
    {
      "name": "Programming language",
      "score": 0.0934630036354065
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Government (linguistics)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}