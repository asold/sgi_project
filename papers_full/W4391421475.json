{
    "title": "Learning Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT &amp; NetLogo Chat",
    "url": "https://openalex.org/W4391421475",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2267814427",
            "name": "Chen John",
            "affiliations": [
                "Northwestern University"
            ]
        },
        {
            "id": "https://openalex.org/A2110779040",
            "name": "Lu Xi",
            "affiliations": [
                "University of California, Irvine"
            ]
        },
        {
            "id": null,
            "name": "Rejtig, Michael",
            "affiliations": [
                "Northwestern University"
            ]
        },
        {
            "id": "https://openalex.org/A3008438646",
            "name": "Du David",
            "affiliations": [
                "University of Massachusetts Boston"
            ]
        },
        {
            "id": null,
            "name": "Bagley, Ruth",
            "affiliations": [
                "Northwestern University"
            ]
        },
        {
            "id": "https://openalex.org/A4276904163",
            "name": "Horn, Michael S.",
            "affiliations": [
                "Northwestern University"
            ]
        },
        {
            "id": null,
            "name": "Wilensky, Uri J.",
            "affiliations": [
                "Northwestern University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4223908421",
        "https://openalex.org/W4382654255",
        "https://openalex.org/W3000240292",
        "https://openalex.org/W2074197417",
        "https://openalex.org/W2123536472",
        "https://openalex.org/W4385656600",
        "https://openalex.org/W2888000346",
        "https://openalex.org/W4312282218",
        "https://openalex.org/W4380442321",
        "https://openalex.org/W4379540196",
        "https://openalex.org/W4288359812",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W4360980141",
        "https://openalex.org/W2105849154",
        "https://openalex.org/W4366596891",
        "https://openalex.org/W4211263275",
        "https://openalex.org/W1969211477",
        "https://openalex.org/W4380479489",
        "https://openalex.org/W3030109276",
        "https://openalex.org/W4312056062",
        "https://openalex.org/W4402665833",
        "https://openalex.org/W3085840990",
        "https://openalex.org/W2943696148",
        "https://openalex.org/W4225120919",
        "https://openalex.org/W4382239980",
        "https://openalex.org/W2255937970",
        "https://openalex.org/W3159506381",
        "https://openalex.org/W2079528076",
        "https://openalex.org/W4313563813",
        "https://openalex.org/W4382492037",
        "https://openalex.org/W4386584937",
        "https://openalex.org/W2079372196",
        "https://openalex.org/W3037097818",
        "https://openalex.org/W4367860052",
        "https://openalex.org/W2165365113",
        "https://openalex.org/W4291476001",
        "https://openalex.org/W3032979151",
        "https://openalex.org/W4366587430",
        "https://openalex.org/W4384644593",
        "https://openalex.org/W4320854981",
        "https://openalex.org/W4308616811",
        "https://openalex.org/W2111209739",
        "https://openalex.org/W2065207408",
        "https://openalex.org/W2165344899",
        "https://openalex.org/W4229050671",
        "https://openalex.org/W4321013654",
        "https://openalex.org/W2997124840",
        "https://openalex.org/W4381587445",
        "https://openalex.org/W2050638549",
        "https://openalex.org/W2060486579",
        "https://openalex.org/W2534380090",
        "https://openalex.org/W4384520759",
        "https://openalex.org/W2167759247",
        "https://openalex.org/W1981807900",
        "https://openalex.org/W4225108562",
        "https://openalex.org/W3128904553",
        "https://openalex.org/W3160674997",
        "https://openalex.org/W1687473687",
        "https://openalex.org/W4324138978",
        "https://openalex.org/W3133959936",
        "https://openalex.org/W3029044752",
        "https://openalex.org/W3162051685",
        "https://openalex.org/W4384461076",
        "https://openalex.org/W4366548330",
        "https://openalex.org/W3165682343",
        "https://openalex.org/W1605597939",
        "https://openalex.org/W4287758476",
        "https://openalex.org/W761179351",
        "https://openalex.org/W2795728764"
    ],
    "abstract": "Large Language Models (LLMs) have the potential to fundamentally change the\\nway people engage in computer programming. Agent-based modeling (ABM) has\\nbecome ubiquitous in natural and social sciences and education, yet no prior\\nstudies have explored the potential of LLMs to assist it. We designed NetLogo\\nChat to support the learning and practice of NetLogo, a programming language\\nfor ABM. To understand how users perceive, use, and need LLM-based interfaces,\\nwe interviewed 30 participants from global academia, industry, and graduate\\nschools. Experts reported more perceived benefits than novices and were more\\ninclined to adopt LLMs in their workflow. We found significant differences\\nbetween experts and novices in their perceptions, behaviors, and needs for\\nhuman-AI collaboration. We surfaced a knowledge gap between experts and novices\\nas a possible reason for the benefit gap. We identified guidance,\\npersonalization, and integration as major needs for LLM-based interfaces to\\nsupport the programming of ABM.\\n",
    "full_text": "Learning Programming of Agent-based Modeling with LLM\nCompanions: Experiences of Novices and Experts Using ChatGPT\n& NetLogo Chat\nJohn Chen\nNorthwestern University\nEvanston, IL, United States of\nAmerica\ncivitas@u.northwestern.edu\nXi Lu\nUniversity of California, Irvine\nIrvine, CA, United States of America\nxlu30@uci.edu\nDavid Du\nNorthwestern University\nEvanston, IL, United States of\nAmerica\nduyuzhou2013@gmail.com\nMichael Rejtig\nUniversity of Massachusetts Boston\nBoston, MA, United States of America\nmichael.rejtig001@umb.edu\nRuth Bagley\nNorthwestern University\nEvanston, IL, United States of\nAmerica\nruth.bagley@northwestern.edu\nMichael S. Horn\nNorthwestern University\nEvanston, IL, United States of\nAmerica\nmichael-horn@northwestern.edu\nUri J. Wilensky\nNorthwestern University\nEvanston, IL, United States of\nAmerica\nuri@northwestern.edu\nABSTRACT\nLarge Language Models (LLMs) have the potential to fundamentally\nchange the way people engage in computer programming. Agent-\nbased modeling (ABM) has become ubiquitous in natural and social\nsciences and education, yet no prior studies have explored the\npotential of LLMs to assist it. We designed NetLogo Chat to support\nthe learning and practice of NetLogo, a programming language for\nABM. To understand how users perceive, use, and need LLM-based\ninterfaces, we interviewed 30 participants from global academia,\nindustry, and graduate schools. Experts reported more perceived\nbenefits than novices and were more inclined to adopt LLMs in\ntheir workflow. We found significant differences between experts\nand novices in their perceptions, behaviors, and needs for human-\nAI collaboration. We surfaced a knowledge gap between experts\nand novices as a possible reason for the benefit gap. We identified\nguidance, personalization, and integration as major needs for LLM-\nbased interfaces to support the programming of ABM.\nCCS CONCEPTS\n• Human-centered computing →Empirical studies in HCI ;\nNatural language interfaces ; • Computing methodologies →\nSimulation support systems .\nACM Reference Format:\nJohn Chen, Xi Lu, David Du, Michael Rejtig, Ruth Bagley, Michael S. Horn,\nand Uri J. Wilensky. 2024. Learning Programming of Agent-based Modeling\nwith LLM Companions: Experiences of Novices and Experts Using ChatGPT\n& NetLogo Chat. In Proceedings of Proceedings of the CHI Conference on\nHuman Factors in Computing Systems (CHI ’24) (CHI ’24). ACM, New York,\nNY, USA, 18 pages. https://doi.org/10.1145/3613904.3642377\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\n2024. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00\nhttps://doi.org/10.1145/3613904.3642377\n1 INTRODUCTION\nThe advent of coding-capable Large Language Models (LLMs) has\nthe potential to fundamentally change the way people engage in\ncomputer programming[25]. As LLM-based programming inter-\nfaces (e.g. GitHub Copilot; ChatGPT) become increasingly popular[46],\nsome studies started to study their user perceptions[84]. However,\nthe research on their potential learning impacts is still limited.\nMany prior studies only focus on impressions of educators[ 46]\nor students[100], with little empirical data on the actual learning\nusage of these tools. On the other hand, a few studies started to\nexplore how LLM-based interfaces can be designed to facilitate\nprogramming education, indicating potential advantages for learn-\ners. Notably, these studies suggest that learners with more prior\nprogramming experience tend to benefit more[ 42, 57]. While a\nrecent study identifies some challenges for novice learners with\nLLM-based interfaces[101], there is a gap in understanding why\nexperienced programmers seem to gain more learning benefits from\nthese tools.\nIn this paper, we present the design of a novel LLM-based in-\nterface, NetLogo Chat, for the learning and practice of NetLogo.\nNetLogo is a widely used programming language for agent-based\nmodeling (ABM), which applies simple rules on multiple individual\nagents to simulate complex systems[94]. It is particularly powerful\nin capturing emergent phenomena, e.g., the spread of viruses or\npredator-prey systems[93]. It is an important methodology in com-\nputational modeling across scientific disciplines and education from\nK-12 to postgraduate levels[88], where scientists and educators are\nhighly in need of LLM-based interfaces[21, 59]. As an important\npart of computational modeling, the priorities of ABM differ from\ngeneral programming[65]. A modeler needs to verify that their\narXiv:2401.17163v2  [cs.HC]  31 Jan 2024\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nChen, et al.\nconceptual design of individual rules matches the real-world pat-\nterns (e.g. a predator needs food to survive), the code matches the\ndesign (i.e. there are no unexpected or implicit assumptions), and\nthe aggregated outcome matches real-world phenomena (e.g. if all\nprey die out, predators die too)[28]. As most LLM-related studies on\ncomputer programming work on general-purpose languages that\nLLMs perform best (e.g. Python or Javascript), no LLM-related stud-\nies have explored ABM or other forms of computational modeling\nat this point.\nNetLogo Chat was designed with constructionist learning princi-\nples and incorporated known best practices for ABM and computer\nprogramming. Constructionism advocates for the design of learn-\ning experiences where learners construct their understanding of\nthe world (e.g. knowledge of ABM) through building personally\nmeaningful artifacts (e.g. an agent-based model around learners’\ninterests)[61]. Similar to GitHub Copilot Chat[ 1], NetLogo Chat\nwas integrated into an integrated development environment (IDE).\nDifferent from previous designs, it aims to give users more control\nover the human-AI collaboration processes, strives to incorporate\nauthoritative sources, and tries to provide more support for trou-\nbleshooting.\nUsing both ChatGPT and NetLogo Chat as a probe[101], we con-\nducted a qualitative study to highlight the different perceptions,\nbehaviors, and needs of experts and novices during open-ended\nmodeling sessions. We interviewed 30 expert and novice partici-\npants from academia, industry, and graduate schools around the\nworld. Participants proposed diverse NetLogo tasks from their disci-\nplines and worked toward their modeling goals. We asked interview\nquestions before, during, and after their interaction with each de-\nsign. We answered the research questions:\n(1) What perceptions - strengths, weaknesses, and adoption\nplans - do expert and novice users perceive LLM-driven\ninterfaces to support their NetLogo learning and practice?\n(2) How do expert and novice users use LLM-driven interfaces\nto support their NetLogo learning and practice?\n(3) What are expert and novice users’ needs for LLM-based\ninterfaces to support their NetLogo learning and practice?\nLearners generally agreed with our design principles and sug-\ngested additional features for future designs. As in other studies,\nexperts reported more perceived benefits than novices. Compar-\ning the different interaction patterns between experts and novices,\nour study reveals a behavioral gap that might explain the gap in\nbenefits. We found that experts collaborated with LLM-based in-\nterfaces with more human judgment in all activities than novices,\nhelping them overcome AI hallucinations, while novices struggled\nwith evaluating and debugging AI responses. From there, we identi-\nfied components of a knowledge gap between novices and experts.\nWe reported experts’ and novices’ needs in LLM-based interfaces\nin three key themes: guidance (from LLMs); personalization (of\nLLMs); and integration (into modeling environments), many of\nwhich confirm and develop the design decisions of NetLogo Chat.\nThe contributions of this paper include:\n(1) The design and implementation of NetLogo Chat, an LLM-\nbased system that supports learning and practice of NetLogo,\na widely-used programming language for ABM;\n(2) An empirical study that contributes to the understanding of\nhow novices and experts perceive, use, and express needs\nfor LLM-based programming interfaces in different ways;\n(3) A theorization of the knowledge gap between experts and\nnovices that might lead to the behavioral gap, and sugges-\ntions of potential design interventions;\n(4) The design discussion and suggestions for building LLM-\nbased programming interfaces that benefit both experts and\nnovices in agent-based modeling more equitably.\n2 RELATED WORK\n2.1 LLMs for Computational Programming and\nModeling\nResearchers have been exploring natural-language-based interfaces\nfor programming for decades, yet early attempts were mostly ex-\nploratory, being limited in capabilities. NaturalJava[64] required\nusers to follow a strict pattern when prompting, while later systems\n(e.g. NaLIX[47] or Eviza[73]) asked for a specific set of English ex-\npressions. This created difficulties for users and system designers, as\nthey felt “a main challenge of NLP interfaces is in communicating to\nthe user what inputs are supported. ”[73] Without the capability to\ngenerate natural languages, those interfaces were also constrained\nto one-off interactions.\nRecently, a new generation of LLMs demonstrated the capability\nto understand and generate both natural languages and computer\nlanguages. GPT-3 was examined in writing code explanations[52],\ndocumentation[44], and providing feedback for assignments[ 3].\nSoon, educators started to believe that Codex could be used to solve\nsimple programming problems[27, 90]. Embedded in ChatGPT, GPT-\n3.5-turbo and GPT-4 demonstrated even stronger capabilities in\nprogramming. More and more LLMs have started to gain the capa-\nbility of coding (e.g. PALM 2; Claude 2; CodeLLaMA 2), ushering\nin a new era of natural language interfaces for programming.\nEven the most powerful LLMs suffer from hallucinations and\nmay misunderstand human intentions. Early users of ChatGPT\ncomplained about incorrect responses and struggled to prompt\nChatGPT for a desired output[74]. While LLMs might outperform\naverage humans in specific, structured tasks[ 58], the evaluation\ncriteria might have been flawed[50], as LLMs struggled to combine\nexisting solutions for a novel challenge[23]. A study suggested that\ndevelopers should not rely on ChatGPT when dealing with new\nproblems [81].\nLLMs are naturally less prepared in low-resource programming\nlanguages (LRPL). Here, our working definition for LRPL is similar\nto that of natural languages: with relatively scarce online resources\nand have been less studied by the AI field[53]. LRPLs are not less im-\nportant: NetLogo, the most widely used programming language for\nagent-based modeling (ABM)[80], is used by hundreds of thousands\nof scientists, educators, and students for computational modeling.\nUsing simple computational rules for individual agents, ABM could\nsimulate complicated emergent phenomena. It has been frequently\nused in different scientific disciplines[93] and science education[35]\nfor recent decades. With considerably fewer online resources to\ntrain on, LLMs are much more prone to errors and/or hallucinations\nwith LRPLs[79].\nLearning Programming of ABM with LLM Companions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nA few studies attempted to improve LLMs’ performance with\nLRPLs in two directions. First, some studies fine-tuned foundational\nLLMs with LRPL datasets[12]. While this approach demands con-\nsiderable datasets and computational power, it has not been applied\nto generative tasks yet[31]. Second, some studies used prompt en-\ngineering techniques. For example, aiming at simple tasks, a study\ncreates sets of grammar rules for LLMs to fill in[86]. Another study\nleveraged compiler outputs, allowing LLMs to iteratively improve\ntheir Rust code, but was only tested in a smaller number of fixed\ntasks[97]. The potential of LLMs in scientific disciplines, including\nin computational modeling, is rarely explored. At this point, the\nonly study targeted at STEM helps with a very specific engineering\ntask [45].\n2.2 User Perception and Behaviors with\nLLM-based Programming Interfaces\nTwo strands of user perception and behaviors studies informed our\ndesign and study: studies of conversational agents (CAs); and of\nLLM-based programming interfaces. For education, CAs were used\nto develop learners’ writing[85], self-talk[29], and programming\nskills[95]. Many of them are pedagogical conversational agents\n(PCA) with the aim to adaptively mimic the behaviors of human\ntutors[96]. PCAs could serve in multiple roles, such as tutors[85],\nmotivators[11], peer players[30], or learning companions[29].\nPrior research of CAs underscored the importance of understand-\ning user perception and behaviors[30], yet the technical boundaries\nof the pre-LLM era limited the freedom of designers. Previous stud-\nies have explored aspects such as trust, mutual understanding,\nperceived roles[18], privacy[69], human-likeness[36], utilitarian\nbenefits, and user-related factors[49] to understand users’ accep-\ntance and willingness to use CAs. However, many CAs before LLMs\nhad to use pre-programmed responses[87], and simply emulating\nfunctional rules from human speech failed to deliver people’s high\nexpectations of CAs[19]. Without the capability to read or write\ncode, pre-LLM CAs for computing education were largely limited\nto providing relevant knowledge[95] or supporting conceptual un-\nderstanding of programming[48].\nRecent studies have started to understand user perception and\nbehaviors with LLM-based programming interfaces. In education,\nearly studies focused on instructors’ and students’ perceptions of\nLLM-based interfaces for programming. Computer science students\nself-reported many potential benefits of using ChatGPT and were\nless inclined to report potential drawbacks[100]. On the other hand,\ncomputer science instructors were significantly concerned over\nstudents’ widespread usage of ChatGPT[46]. While some instruc-\ntors went as far as banning ChatGPT altogether, others suggested\nexposing students to the capabilities and limitations of AI tools,\nleveraging mistakes in generated code for learning opportunities.\nBoth instructors and students expressed the need to adapt to a new,\nLLM-era way of teaching and learning[102].\nFor professionals, challenges and opportunities co-exist with\nLLM-based programming interfaces. Recent studies found program-\nmers preferred to use Copilot[ 84] and finished tasks faster with\nCopilot[62]. Yet, Copilot struggled with more complicated prob-\nlems, providing buggy or non-reproducible solutions[23]. Profes-\nsional programmers faced difficulties in understanding and de-\nbugging Copilot-generated code, which hinders their task-solving\neffectiveness[84]. Programmers who trusted AI were prone to write\ninsecure code with AI[63]. For conversational interfaces, despite\ninputs being in natural languages, users felt that they needed to\nlearn LLM’s “syntax”[26, 37].\nOur understanding of user perception and behaviors with LLM-\nbased interfaces during (the learning of) computer programming is\nstill very limited. As the field just started exploring this direction,\nprevious studies mostly focused on general user impressions[102],\nor conducted behavioral tasks on pre-scripted, close-ended tasks[62].\nWhile close-ended settings made it easier to assess objective metrics[7],\nopen-ended contexts open a wider window to understanding users’\nlearning patterns, behaviors, perceptions, and preferences[8]. For\nexample, a recent study observed two modes that professional pro-\ngrammers interact in open-ended tasks with Copilot: acceleration,\nwhere the programmer already knows what they want to do next;\nand exploration, where the programmer uses AI to explore their\noptions[4]. Another study on professionals’ prompt engineering\nshed light on their struggles, challenges, and potential sources of\nbehaviors[101].\nStill, we noticed two gaps in previous studies. First, a majority\nof studies chose professional programmers or computer science\ninstructors/students as participants, while LLM-based interfaces\nare also used by millions of people without a CS background for pro-\ngramming tasks. Second, as HCI studies mostly focus on languages\nthat LLMs are known to perform best, e.g. Python or HTML, little is\nknown about user perceptions and behaviors when computational\nmodeling or LRPLs are involved.\n2.3 LLM-based Interfaces for Learning\nProgramming and Modeling\nWhile LLMs have shown promising potential in supporting human-\nAI collaboration in programming, most design studies were pre-\nliminary, and LLM-based interfaces for computational modeling\nremained understudied. For example, the Programmer’s Assistant\nintegrated a chat window into an IDE[68]. Going beyond simple\nintegrations, GitHub Copilot Chat[1] provided in-context support\nwithin code editors, yet its user studies were still preliminary[10].\nA similar design was done on XCode without a user study[ 78].\nAnother study explored the integration between computational\nnotebooks with LLMs and emphasized the role of the domain (in\nthis case, data science) on LLM-based interface design[55].\nLLMs have gained much attention among programming educa-\ntors, but the design study is insufficient. Recent studies tested LLMs\non introductory programming tasks and achieved unsurprisingly\nhigh scores[16, 70]. This prospect leads to great concerns among\ncomputer science instructors as they observed the widespread us-\nage of ChatGPT among students[46]. Yet, only a few LLM-based\ndesign studies targeted programming learning. Using a Wizard of\nOz prototype, a study underscored the importance of supporting\nstudents’ varied degrees of prior expertise[67]. A design study re-\nported positive short-term performance gains when young, novice\nprogramming learners engaged with Codex[42]. Another study also\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nChen, et al.\nfound LLMs’ benefits for novice programmers[ 57]. Both studies\nfound that more experienced programmers tended to benefit more,\nyet the reason was still unclear.\nIn this study, we invoke the learning theory of Constructionism[61]\nto inform our LLM-based system and empirical study design. While\nthere is no rigid definition for Constructionism, it argues that learn-\ning happens most felicitously when learners \"consciously engage\nin constructing a public entity\"[ 61]. In the context of computer\nprogramming, it means learning happens naturally through pro-\ngramming computers, as it iteratively externalizes learners’ internal\nunderstanding of the world in code, and then allows learners to im-\nprove their understanding through watching how the code runs[60].\nMoreover, it argues that computer programming is not as abstract\nor formal as it appears; individual programmers’ approaches are\noften concrete and personal, in pluralistic ways[83]. However, the\npluralism in thoughts is more difficult to capture by close-ended\ntasks (such as a problem set) and objective metrics (such as comple-\ntion rate/time)[8]. As such, constructionist learning studies often\nprefer open-ended tasks (e.g. making games[40], designing instruc-\ntional software[32], creating agent-based models in NetLogo[ 8])\nand qualitative studies, as they open windows into the nuances of\nlearners’ perceptions and behaviors in more natural and realistic\nsettings.\nThe Logo programming language and its descendants (e.g. Scratch;\nAlice; NetLogo) succeeded in supporting multiple ways of knowing\nand thinking in computing education and in scientific research[75],\nyet to our knowledge, no published studies have explored their\nsynergy with LLMs. Many prominent constructionist design princi-\nples could be applied to AI-based interfaces[41] and inspired the\ndesign of NetLogo Chat. For instance, “low floor, high ceiling, wide\nwalls” asks learning environments to provide 1) an easy entrance\nfor novices (low floor); 2) the possibility for experts to work on\nsophisticated projects (high ceiling); 3) the support of a wide range\nof different explorations (wide walls); 4) the support of many learn-\ning paths and styles[ 66]. We also learned from previous design\nstudies that stress the importance of adaptive scaffolding[14, 72]\nand support debugging[9] for novices to learn NetLogo. Hence, we\ncontributed to the field one of the first design studies of LLM-based\ninterfaces for learning programming that follow the constructionist\ntradition.\n3 NETLOGO CHAT SYSTEM\nNetLogo Chat is an LLM-based system for learning and program-\nming with NetLogo. It comprises two main parts: a web-based in-\nterface integrated with Turtle Universe (a version of NetLogo)[13]\n(See 3.1); and an LLM-based workflow that improves the quality\nof AI responses and powers the interface (See 3.2). We iteratively\ndesigned the system by:\n(1) Based on authors’ experiences in teaching NetLogo, we cre-\nated a design prototype based on the constructionist learning\ntheory (see 2.3), with a focus on supporting users iteratively\nbuild up their prompts and smaller code snippets before\nworking on entire models. We developed a proof-of-concept\nsystem, using prompt engineering techniques to interact\nwith GPT-3.5-turbo-0314.\n(2) We internally evaluated the proof-of-concept with a group\nof NetLogo experts. During this process, we encountered\nfrequent hallucinations with NetLogo (grammatical or con-\nceptual mistakes; inventing keywords that do not exist; etc).\nFor the system to provide guidance, we realized that author-\nitative sources are necessary for LLMs’ performance;\n(3) We incorporated the official NetLogo documentation and\ncode examples into the system using prompt engineering\ntechniques (see 3.2), evaluated other LLMs’ potential, and\nthen conducted pilot interviews to evaluate the system with\nthree external NetLogo experts invited from NetLogo’s mail-\ning lists. The interviews used a protocol similar to the one\nwe formally used (see 4.2), with more flexibility and open-\nendedness;\n(4) Based on the external feedback, we identified the need for\nsupporting troubleshooting, leading to the design decision\n3.1.3. We upgraded the underlying LLM to GPT-3.5-turbo-\n0613, fixed many minor usability issues, and finalized the\nprototype that we used in the empirical study.\n3.1 Design Overview\nFigure 1: NetLogo Chat asking for details about human’s\nneeds.\n3.1.1 Enable users to program the computer, rather than being pro-\ngrammed by the computer. Over-reliance on LLM-based interfaces\nhas become a major concern among both educators and some learn-\ners, where students blindly follow the instructions given by LLMs\nwithout attempting to construct their representations of knowledge.\nSuch a scenario is antithetical to the constructionist learning tradi-\ntion, where Seymour Papert’s fear of \"computers program children\"\ncomes back to life again[60].\nInspired by the Logo language, the design of NetLogo Chat aims\nto give control back to learners: to suppress LLMs’ tendency to give\na quick response that often assumes too much about the learner’s\ninclination, we force it to ask clarification questions more often. Fig\n1 and Fig 2 provide an exemplary comparison between NetLogo\nChat and ChatGPT’s reaction to a simple modeling request. Here,\nChatGPT immediately assumes details of the user’s needs and gen-\nerates an entire model for the user to copy and paste. Whereas,\nLearning Programming of ABM with LLM Companions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nFigure 2: ChatGPT assuming details of human’s needs.\nNetLogo Chat attempts to first clarify the user’s needs by asking\nfollow-up questions and suggesting exemplar answers. The sug-\ngestions in Fig 1 serve as both an inspiration, in case learners get\nconfused about what to write; and a shortcut, in case learners find\nany suggestions immediately usable.\nFor this feature to work effectively, it is essential to ask questions\nwith quality. To achieve this, we used a few-shot approach and\ncrafted templates for LLMs to follow. We conducted an informal\nevaluation of LLM’s generated questions during our development\nprocess and empirical study. Across the board, the LLM we used\nwas able to generate questions with acceptable quality, similar to\nthe one demonstrated in Fig 1. A future design could embed a larger\nset of templates and retrieve a few relevant templates when needed.\n3.1.2 Invoke Authoritative Sources Whenever Possible. Hallucina-\ntion is another major concern for LLMs, particularly in an LRPL\nlike NetLogo. For example, the code generated by ChatGPT in Fig\n2 contains multiple syntax issues and requires human experts to\naddress them. More powerful LLMs suffer from the same symptoms.\nWe submitted similar sample requests to GPT-4, PaLM2, Anthropic\nClaude 2, and Falcon-180B: none was able to produce syntactically\ncorrect code for a classical NetLogo model.\nFollowing previous examples in related tasks[38], we integrated\nNetLogo’s official documentation and model examples to help im-\nprove LLMs’ and human performance. Different from previous\nstudies, we not only provided related examples to LLMs, but also\nrevealed them to users. By doing so, we seek to improve the trans-\nparency of LLM’s mechanism, foster trust in the LLM-driven system,\nand provide authoritative guides and examples for users even when\nLLMs might fail to provide precise support.\n3.1.3 Integrate with the IDE and Enhance Troubleshooting. We seek\nto integrate NetLogo Chat into NetLogo’s IDE beyond integrating\na conversational assistant parallel to the code editor. To facilitate\na constructionist learning experience, the code editor needs to be\nFigure 3: NetLogo Chat’s embedded editor for generated code.\nintegrated into the conversational interface, where learners can\nwork with smaller snippets of code with more ease. Thus, the design\nmight lower the threshold for learners to tinker with the code, a\nkey learning process advocated by the constructionist literature\n[61, 83].\nFig 3 provides a concrete example, where the embedded editor\ndisplays a piece of generated code. Instead of having to copy and\npaste the piece back into the main editor, the user could first see\nif any syntax issues exist in the code; run the code within a con-\nversation; and ask follow-up questions or raise additional requests,\nbefore putting back a working code snippet into their projects.\nTo further support the user’s troubleshooting, in addition to\nerror messages, NetLogo Chat will display extra debugging options\nfor users. Users could choose to look for an explanation, or ask the\nLLM to attempt fixing the issue on its own, or with the user’s ideas.\nDuring the process, the system will attempt to find documentation\nand related code examples to reduce hallucinations. Building on\nthe literature on error messages’ impact on learning[ 5], we also\nclarified many messages to provide a better context for humans\nand both LLM-based systems used in the study.\n3.2 Technical Implementation\nFigure 4: A brief outline for NetLogo Chat’s LLM workflow.\nSince OpenAI started to provide fine-tuning on GPT-3.5-turbo\n(the version also used in ChatGPT Free) only after we concluded the\nmain study in July, NetLogo Chat was implemented with prompt en-\ngineering techniques. We built our project on ReAct[99], a prompt-\nbased framework that could reduce hallucination, improve human\ninterpretability, and increase the trustworthiness of LLMs. By re-\nquiring LLMs to generate an action plan and delegate the action to\na third-party conventional agent (e.g. search for documentation, ask\nclarification questions, conduct a static syntax check, etc.) before\ncomposing the final response, the framework provides a promising\npathway to integrate external inputs (e.g. human input, official\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nChen, et al.\ndocumentation) into LLM workflows. Fig 4 depicts a rough outline\nof NetLogo Chat’s workflow. Imagine a user requests to \"create a\npredation model\":\n(1) The LLM is instructed, in the prompt, to first elaborate on\nthe request (planning): \"The user intends to create an agent-\nbased biology model related to predation. However, it is\nunclear what exactly the user wants. We need to ask follow-\nup questions. \"\n(2) Next, the LLM is instructed to choose an action from the\nlist: Ask clarification question(s); Search for documentation;\nWrite a response; Say sorry. Here, imagine the LLM chooses\n\"Ask clarification question(s)\" based on the planning.\n(3) Then, the LLM needs to generate some questions based on\nthe request. Because LLMs are trained on real-world data,\nit is not difficult for them to come up with some ideas. For\nexample, \"What species do you want to put in the model?\"\nThe LLM is also instructed to provide some examples, e.g.\n\"Wolf\", \"Sheep\".\n(4) When the user replies to the questions, the loop restarts\nfrom step (1). Since there is sufficient information about\nthe request, the LLM decides to search for information, and\nalso generates keywords for the search, e.g. \"Wolf-sheep\npredation model in NetLogo\".\n(5) The system conducts a semantic search on a pre-assembled\ndatabase of NetLogo’s official documentation and code ex-\namples. The system returns the search result, use it as a new\nround of input, and restarts from step (1).\n(6) With inputs from both the user, who clarified the request;\nand the database, which supplies the example; the LLM plans\nagain, chooses to write a response, and generates its final\nresponse.\nIn the example, we initiated three requests with the LLM, each\nwith a prompt template that results in a structured response[99] (e.g.\nany response needs to have a Plan, an Action, and a Parameter).\nEach request could use a different LLM that works best for the\nspecific request. Using this approach, the system has the potential\nto balance cost, performance, speed, and privacy. For example, a\nfuture iteration of NetLogo Chat could leverage a fine-tuned local\nLLM to probe the user’s intentions and search for documentation.\nThen, with any personal or sensitive information stripped away,\nthe system could forward the compiled request to a powerful online\nLLM (e.g. GPT-4).\nFor the empirical study, we chose GPT-3.5-turbo-0613 as Net-\nLogo Chat’s LLM backend. First, we expect most participants to\nbe using the free version of ChatGPT, driven by the same LLM.\nIn this way, we would have a fair playing field for the empirical\nstudy, where both systems will be used. Second, at the time of our\nstudy, the response time for GPT-4 was too long to sustain a real-\ntime experience, while we had no access to other NetLogo-capable\nLLMs’ APIs. Although we did observe some remarkable improve-\nment when internally evaluating the system (e.g. ChatGPT has\ntrouble answering questions for lesser-known NetLogo keywords,\nwhile NetLogo Chat does not), a more systematic evaluation rubric\nis needed for future research.\nTable 1: Overview of Participant Demographics (n=30)\nGender Females: 10 (33%); Male: 19 (63%); Non-binary:\n1 (3%)\nGeography Africa: 1 (3%); Asia and Oceania: 5 (17%); Eu-\nrope: 8 (27%); Latin America: 2 (7%); North\nAmerica: 14 (47%).\nOccupation Academics: 14 (47%); Professionals: 12 (40%);\nStudents: 4 (13%)\n4 EMPIRICAL STUDY\n4.1 Participants\nFor the empirical study, we recruited 30 adult participants through\nNetLogo’s official Twitter and mailing lists; and through the Com-\nplexity Explorer, a website run by Santa Fe Institute (SFI) to distrib-\nute learning resources of agent-based modeling (ABM). The exact\nbreakdown of participants’ demographic data can be seen in Table\n1. The participant pool largely represented the scientific modeling\ncommunity in NetLogo’s main audience, with a majority of partici-\npants coming from STEM disciplines. Many participants were also\nrelated to the educator sector. 6 participants (20%) were instructors\nwho teach or are interested in teaching NetLogo in classrooms; 4\n(13%) were graduate-level students interested in learning NetLogo,\nmaking up a third of the population. Participation in the study\nwas voluntary. All participants signed an online consent form on\nQualtrics.\nBuilding on the tradition of understanding the difference be-\ntween experts and novices[17], we separated the participants into\nexperts and novices using self-reported survey data. To mitigate the\neffect of inaccurate responses, NetLogo experts in the team, who\nhave been core developers and instructors of NetLogo, watched\nevery video and decided if a participant greatly overestimated or\nunderestimated their capabilities. We considered the participant’s\ndiscussions with the interviewer, the think-aloud process, and the\ncoding behaviors. A vast majority of users’ reports correspond\nwith the experts’ judgment. Then, to simplify the analysis, we sepa-\nrated participants (Table 2) by their levels into two main categories:\nexperts, who are either experts in NetLogo or programming in\ngeneral; and novices. In the study, we denote experts by the prefix\nE (E01-E17) and novices by N (N01-N13). 13 experts had previ-\nous experience with ChatGPT (76%), including programming (65%,\nn=11). 11 novices (85%) also used ChatGPT before, but much less\nfor programming (38%, n=5).\n4.2 Interviews\nOur study was conducted in 3 phases:\n(1) We pilot interviewed 3 experts invited from NetLogo’s online\ncommunity. Each was asked to comment on LLMs for NetL-\nogo learning, as well as on ChatGPT and an early prototype\nof NetLogo Chat.\n(2) We improved the design of NetLogo Chat based on what we\nlearned from the pilot interviews and revised the interview\nprotocol accordingly.\n(3) We conducted formal interviews with 27 online participants\n(30 in total).\nLearning Programming of ABM with LLM Companions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nTable 2: Participant Information\nID Region Level (NetLogo) Level (Programming) Occupation\nE01 North America Expert Expert Professional\nE02 Asia and Oceania Expert Intermediate Academic\nE03 Latin America Intermediate Expert Academic\nE04 North America Expert Expert Academic\nE05 Europe Intermediate Expert Academic\nE06 North America Intermediate Intermediate Academic\nE07 Latin America Intermediate Intermediate Professional\nE08 Asia and Oceania Intermediate Intermediate Professional\nE09 Asia and Oceania Intermediate Expert Professional\nE10 North America Intermediate Intermediate Academic\nE11 Africa Intermediate Expert Academic\nE12 North America Intermediate Intermediate Academic\nE13 Europe Expert Novice Academic\nE14 Europe Intermediate Intermediate Academic\nE15 Asia and Oceania Expert Expert Student\nE16 Asia and Oceania Novice Expert Professional\nE17 Europe Intermediate Expert Academic\nN01 North America Novice Novice Professional\nN02 North America Novice Novice Academic\nN03 North America Novice Novice Professional\nN04 North America Novice Intermediate Student\nN05 Europe Novice Intermediate Student\nN06 Europe Intermediate Novice Student\nN07 North America Novice Intermediate Professional\nN08 North America Novice Intermediate Professional\nN09 North America Novice Novice Professional\nN10 North America Novice Intermediate Professional\nN11 Europe Novice Intermediate Academic\nN12 Europe Novice Novice Academic\nN13 North America Intermediate Novice Professional\nEach semi-structured interview lasted between 60-90 minutes\nand was video recorded. Prior to each formal interview, participants\nwere asked to come up with a short NetLogo task that they were\ninterested in working on. Almost every participant brought forward\na modeling task from their career domain or personal interest, e.g.\nto model \"how honeybees decide to regulate the temperature of\nthe hive\", or \"the spread of conflicting ideas\". Only once, when\nthe task scope was too complicated for the session, did we ask\nthe participant to bring another. During any part of the interview\nprocess, interviewers generally followed the protocol, asking follow-\nup questions when needed. Specifically:\n(1) We asked baseline questions, e.g., “What do you think are\nthe potential advantages / disadvantages of using LLMs in\nsupporting your learning and programming of NetLogo?”\n(in 2 separate questions)\n(2) We asked the participant to work on their task with the help\nof ChatGPT. Then, we asked the same baseline questions\nagain, then asked “What do you like or dislike about the\ninterface”. Repeat the procedure with NetLogo Chat;\n(3) If time permitted, we further asked about their preferences\nfor learning and/or programming with NetLogo and asked\nwhich feature they wanted to add/remove from either system.\nHere, the objective was not to strictly compare between the\ntwo systems, but to elicit more in-depth discussions over\nLLM-based interfaces.\nSince almost all users have already engaged with ChatGPT, we\ndid not randomize the order of ChatGPT/NetLogo Chat. Also, 3\nparticipants used the paid version (GPT-4) during the task with\nChatGPT. While much of the generated data comes from the in-\nevitable comparison between the two systems, we chose not to\ninterpret them as objective comparisons. Instead, the different de-\nsign principles underpinning the systems presented two objects to\nthink with[60], that our participants drew on during their reflec-\ntions and discussions of LLM-based programming interfaces.\n4.3 Data Analysis\nOur interviews resulted in around 40 hours of video data. Around\nhalf of our data is behavioral in nature, where participants worked\non their tasks and were encouraged to think aloud; the other half is\nmore verbal, where participants answered questions. As such, each\ninterview was not only transcribed verbatim, but also watched by\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nChen, et al.\na researcher to create observational notes. The two streams were\nthen combined into a single archive for analysis.\nBased on our research questions, we iteratively applied the\ngrounded theory approach[22] to analyze our data. During each\nstep, the research team fully discussed the discrepancies between\neach researcher and iteratively refined the codebook to improve\nconsistency. The analysis reached theoretical saturation at around\n50% of interviews, when additional interviews no longer revealed\nunexpected major insights for our research questions. Then, we\nfinished the rest of qualitative coding with the finalized codebook\n(Table 3).\n(1) Four researchers open-coded 2 interviews, one from a novice\nand one from an expert, to summarize the topics mentioned\nby participants. During this process, researchers coded in\ndifferent tabs to avoid interference. Three broad themes\nemerged from this phase: participants’ approaches to pro-\ngramming; participants’ interactions with AI systems; and\ntheir comments on AI systems.\n(2) Taking notes of the emerging themes, the first author created\na preliminary codebook that categorizes dozens of codes\ninto themes. Each researcher coded another 2 interviews\nin different tabs. In this phase, we refined the themes into\napproaches to programming (which also helps to separate\nexperts and novices); perceptions and observed behaviors\nrelated to AI systems; and comments on AI systems’ abilities.\n(3) Based on the coding results, the first author created a formal\ncodebook, with definitions clarified based on the discrepan-\ncies between researchers (Table 3). To reduce the unbalanced\ninfluence of subjective interpretation, researchers only coded\nexplicit behaviors; or direct comments. To avoid missing in-\nsights, researchers were instructed to highlight places where\nexisting codes are insufficient to cover the topics. During\nthe first two weeks, a few codes were created or merged as a\nresult of discussions. We retrospectively revised our coding.\nBased on the codebook, the first author iteratively incorporates\nthemes into an outline. To further mitigate individual differences,\nresearchers were asked to include as many codes as possible for\neach quote or observation.\n5 FINDINGS\n5.1 Perception: Before and After Interaction\n5.1.1 Before Interaction: Positive Expectations. Prior to the tasks,\nboth novices and experts had positive expectations of LLM-based\ninterfaces for NetLogo, with novices holding higher expectations\nthan experts.\nBoth novices and experts expected LLM-based interfaces to save\nhuman time and support human effort, especially compared to other\nhelp-seeking activities. With LLMs, human time and energy could\nbe liberated for more high-level tasks ( E12 , N03 ). Educators\nfelt that LLMs could facilitate more efficient teaching, allowing\nstudents to “more complicated things with relative ease”, spiking\n“their imagination.” ( E02 ) LLMs can also bring emotional benefits\nby reducing the fear of “bothering the teachers or the experts”\n( E14 ) or asking “stupid questions” ( N06 ).\nMost participants highlighted AI’s potential to help them with\nNetLogo’s syntax. For most participants, NetLogo is not the main\nprogramming language they used. Before the advent of ChatGPT,\nN06 felt that she needed to “recite the words (syntax of NetLogo)”.\nYet, the need was eliminated when “AI can teach you very quickly”.\nMany experts also needed support, as NetLogo “has very strict\nsyntax rules” ( E07 ) which makes writing more difficult.\nNovices, in particular, expected that AI could be helpful for trou-\nbleshooting. N08 , for instance, felt that LLMs could help him\nthrough the troubleshooting process by describing “what I’m try-\ning to do and get a snippet of code that helps get me past that\nblock”. For novices without a background in programming, this\nfuture looks promising. N12 is interested in the potential to “make\nprogramming more approachable to students”.\n5.1.2 Before Interaction: Negative Expectations. Almost every par-\nticipant expressed concerns or reservations about LLM-based inter-\nfaces. Yet, the concerns of novices and experts were conspicuously\ndifferent.\nExperts focused on preserving human judgment. E01 believed\nthat AI should not “replace human judgment and ability”. Similarly,\nE06 insisted that “(human) has to do the main thinking and ideas\nand all of that.” E17 felt that humans cannot let AI “take over\nthe main reasoning and emotions, the emotions intervening in the\ndecisions.” Many educators were also “concerned about learning”\n( E13 ), fearing the tendency to “default to the AI system to come up\nwith the answers instead of working through it ourselves” ( E12 ).\nMany experts explicitly explained their rationales. For example,\nE08 was concerned that “if a model points me to a suboptimal di-\nrection, I will have no idea, because I haven’t considered alternative\nstructure”. E15 feared that relying on AI responses might “make\nyour horizon narrow” because she would miss learning opportuni-\nties when browsing through the models library. For computational\nmodeling, AI also might lack “in-depth knowledge in a specific field”\nto create an entire model ( E05 ). As such, E05 would only trust\nAI to “finish a specific task”.\nNovices were more optimistic and more concerned with their ca-\npabilities of understanding AI’s responses or making AI understand\nthem. For example, while N04 thought “one of the hypothetical\ndrawbacks” to LLMs being “confidently incorrect”, they added that\n“people are like this too”. On the other hand, N03 feared that she\nwould waste more time with AI if “it didn’t understand me, or if\nI had difficulty expressing”. N02 acknowledged that “there is a\nlimitation to not knowing how to code (on how much AI could\nhelp).” Without knowledge of NetLogo, N11 felt difficult to spot\nLLM-generated mistakes.\n5.1.3 After Interactions: Different Impacts of Hallucination. All par-\nticipants encountered AI hallucinations throughout the sessions.\nWhile some participants rated NetLogo Chat higher than ChatGPT’s\nfree version, most participants had similar changes in perceptions:\nexperts, in general, reported more benefits from LLMs than novices.\nSome participants reported more positively about NetLogo Chat’s\ncapabilities. Several experts questioned ChatGPT’s training in Net-\nLogo, yet they trusted more in NetLogo Chat, for it incorporates\nauthoritative sources (see 3.1.2). E16 believed that NetLogo Chat\nLearning Programming of ABM with LLM Companions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nTable 3: An Overview of the Codebook\nCode Definition\nApproaches User’s perceptions about their approach to programming tasks, e.g. planning, separating into\nsmaller pieces, or working on it as a whole.\nLearning How users learn NetLogo or programming in general, or think that people should learn.\nCoding How users organize or write their code, or think that people should organize or write.\nHelp-seeking How users seek help in general, or think that people should seek help.\nHuman-AI User’s perception and behaviors related to Human-AI relationship.\nPrior Users’ prior experiences with ChatGPT or other AI-based interfaces.\nAttitude Users’ attitudes toward AI in general, or specific AI-based systems.\nEffort AI’s influence on how much, and what kind of, efforts that humans made or need to make.\nAbilities User’s perception related to AI’s abilities.\nResponse AI’s ability to provide desirable responses for humans.\nSupport AI’s ability to support learning/coding of NetLogo.\nInteractivity AI’s ability to facilitate helpful interactions with humans.\nTable 4: Novices and Experts’ Perceptions on LLM-based Interfaces for NetLogo\nExperts Novices\nLLMs could save human time and effort,\nespecially in syntax.\nLLMs could save human time and effort,\nespecially for syntax, and provide emo-\ntional benefits.Before, Positive LLMs could help troubleshooting.\nLLMs could mislead humans to subopti-\nmal directions.\nWhile LLMs may make mistakes, it is no\nworse than humans.\nLLMs could hinder learning processes. LLMs may not understand human inten-\ntions.Before, Negative\nLLMs could only work on smaller tasks. LLMs’ responses are difficult to under-\nstand.\nLLMs supported learning or practicing by\nsaving time.\nLLMs supported learning or practicing by\nsaving time.After Interaction Will continue to use LLMs for learning or\npracticing NetLogo.\nWill seek alternative learning resources\nbefore continuing to use LLMs.\n“understands your NetLogo syntax” and “the basic aspects of Net-\nLogo”. N02 thought NetLogo Chat still had bugs but was “much\nmore informative and precise than ChatGPT.” As NetLogo Chat\nis designed to support troubleshooting (see 3.1.3), E04 thought\nNetLogo Chat “was able to kind of do some better troubleshooting\nto a certain extent, for it clarifies error codes”.\nIn both cases, experts understood hallucinations as an inevitable\npart of human-AI collaboration and reacted with more leniency.\nWhen E03 first encountered an incorrect response, he exclaimed:\n“Very interesting! You’re mistaken.” E05 felt that LLMs helped him\n“finish most of the code”, though he still needed to “debug and see if\nthe code makes sense logically.” As experts did not rely on LLMs to\nresolve issues but mostly leveraged them as a shortcut, E06 stated\nthat hallucinations were instances “where the programmer needs\nto use own experience and discretion”, as risks would escalate if\none extrapolates “what ChatGPT provides you in a wrong manner”.\nNovices, on the other hand, reported more obstacles and frustra-\ntion, as they relied more on LLMs for their tasks. N07 emotionally\nresponded to a hallucination that ChatGPT “apparently made that\nshit up”. N01 had difficulties to “fix the bugs that were in it (the\ngenerated code).” N08 ’s session ended up “hitting a dead end”,\nwith the frustration leading him to “go consult other resources”.\nMost novices and experts still thought that LLM-based interfaces\nsupported their learning or practicing by saving time. Even though\nN03 had “low trust” in ChatGPT, she still felt more confident after\ncollaboration, for it “narrowed down the stuff I have to figure out\nmyself and has made me much faster already.” As an educator,N12\nfelt that LLMs facilitated a constructionist learning experience in\nwhich “you’re being thrown into the culture and have to learn it\non the fly.” E13 thought he learned a syntax from ChatGPT that\nwould “save me time in the future” and the learning process was “a\nlot faster than if I were doing it by hand”.\nAs experts reported more perceived benefits, they predominantly\nintended to continue using LLM-based interfaces for NetLogo. After\nthe task, E11 felt confident that “I can write anything I want to\nwrite”. Yet, many novices, driven by their frustration with LLMs,\nsought alternative learning resources before considering a return.\nN04 , for instance, had a 180-degree turn: expressing great hope\nbefore the tasks, they now inclined to “build more by myself with\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nChen, et al.\nmy own code, without AI.” N13 thought that she would prefer\nto work with “someone who is familiar with the programming\nlanguage” together with LLMs.\n5.2 The Behavioral Gap Between Novices and\nExperts\n5.2.1 Behavioral Gap in Planning and Prompting. While experts’\nand novices’ tasks were similar in terms of complexity, we observed\ndifferences between how novices and experts plan out their tasks.\nSince most participants gradually adapted their prompting styles,\nwe focused on participants’ first-round prompts.\nTwo initial prompting patterns, one emphasizing modeling the\nentire system and another focusing on smaller, initial aspects of the\ntask, emerged from our interviews. Most novices adopted the first\npattern (11/13, 85%), while many experts adopted the second pattern\n(9/17, 53%). Below, we introduce one vignette for each pattern:\n(1) N05 started by asking: “I need to make a model of the bunch\nof agents who are trying to promote political views to other\npeople (...)”. Although he used GPT-4, the returned code still\ncame with several syntax errors. N05 then spent the next\n20 minutes trying to ask GPT-4 to fix issues without success.\nHe expected to “put the idea into it and we’ll run the code”,\nbut in the end “it didn’t happen.”\n(2) E07 started by asking ChatGPT to “write code for drawing\na rectangle”. When GPT-3.5 failed to further divide the rec-\ntangle, E07 instantly pivoted to another strategy: “I have\nthe following code that draws a rectangle. I want you to\nmodify it so the rectangle is divided by two”. GPT-3.5 still\nfailed, yet it produced working code and did “something\nclose to it”.\nThe second prompting pattern involved remarkable mental ef-\nforts to decompose and plan out the task. For example, E07 de-\nscribed his approach as “separate into small, general tasks you want\nto do.” E04 explained that he “just likes to iteratively build (the\ncode)”. On the other hand, in the first pattern, many participants\nattempted to shortcut the efforts by delegating the tasks to AI, as\nN05 said: “I just want to ask it (ChatGPT) to just directly make a\ncode for this task and that’s it.”\nBy the end of the task, most participants had realized the impor-\ntance of breaking tasks into smaller pieces for coding with AI. Nat-\nurally, when an LLM-based interface generated code with mistakes,\na participant would be (implicitly) guided to ask smaller follow-up\nquestions. Soon, many of them realized the benefits. N01 thought\nit would be better if one “works through real small problems first,\nbefore getting to more complicated problems.” N10 would “start\nwith something really basic.” Experts using the first pattern had\nsimilar ideas. For example, E12 decided to restart “with something\nsimple and just work with it.”\n5.2.2 Behavioral Gap in Coding and Debugging. As most partici-\npants engaged with an agent-based modeling task that they never\nworked on, both experts and novices learned some aspects of Net-\nLogo with the help of AI - although, in different ways. Experts\nusually took a much more measured, prudent, and critical approach\nduring coding and debugging, while novices mostly followed AI’s\ninstructions.\nMost novices focused on reading AI’s explanations and followed\nAI’s instructions during their coding processes. ChatGPT often\ngives instructions like “You can copy and paste this code into NetL-\nogo and run it”. Even without this hint, almost all novices would\ncopy and paste the generated code without much reading. The ten-\ndency worried some novices, but they had no choice: “I feel like\nI’m waiting for someone to tell me the answer, rather than learning\nhow to solve it.” ( N11 )\nExperts put more emphasis on the code, often ignoring the ex-\nplanations provided by AI. During their reading, experts evaluated\nand often criticized the responses, planning their next steps along\nthe way. Only a few experts tried copying and pasting the code to\nsee if they worked out of the box. Other experts selectively copied\nand pasted parts of the code into their programs, or wrote their\nprograms with generated code on the side. Even when they copied\nand pasted the code, experts were more cautious. For example,\nwhile E04 decided to “just take this and see what this does”, he\nalso realized that AI-generated code would override his ideas and\nmanually edited the code.\nAll participants inevitably had to debug parts of the generated\ncode. Yet, novices sought support from AI more frequently and\noften struggled with AI responses. For example, N12 would regu-\nlarly “copy the code that doesn’t make sense and go back to AI to\nsee if it can help me.” N09 complained that while ChatGPT gave\nsuggestions, “it obviously requires fiddling around with it.” As she\nhad little idea about NetLogo, it became a purely trial-and-error ex-\nperience. Even when AI did solve some errors, it was challenging for\nnovices to learn from the process. For example, N04 commented\nthat while NetLogo Chat provided an automated process, it was still\ndifficult for him to get the lesson, “since I didn’t write it myself.”\n5.2.3 Behind the Behavioral Gap: The Knowledge Gap. We identi-\nfied a knowledge gap that may lead to the behavioral gap. When\nnovices realized that they needed to spend more effort decomposing\nthe task or vetting AI responses, they found themselves lacking\nthe necessary knowledge. We summarized, in participants’ own\nwords, the four components of a knowledge gap that novices need\nto overcome when working with AI.\nNovices reported the need for conceptual knowledge of modeling.\nFor example, N07 described his experience as “like being adrift on\nan ocean. Without a compass, and without a map.” With only a basic\nunderstanding of agent-based modeling, N11 felt compelled to\naccept ChatGPT’s response as “I don’t really know how to interpret\nsome of the output from it.” Such feelings correspond with novices’\ntendency to skim through AI responses. Whereas, some novices\nasked for help from LLMs with different degrees of success. N04\nfirst asked: “(...) Can you tell me what I will need to do before we\nbegin?” With AI’s suggestions, N04 had some more success asking\nfollow-up questions.\nThe unfamiliarity with the basic concepts of NetLogo and/or\ncoding in general further adds to the difficulty in prompting and\nunderstanding. After reading a guide suggested by NetLogo Chat,\nN07 realized that he “probably wouldn’t have chosen NetLogo\nto ever begin with” for his database-related task. Other novices\nLearning Programming of ABM with LLM Companions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nTable 5: Novices and Experts’ Behaviors During Human-AI Collaboration\nExperts Novices\nMany start by asking LLMs for a\nsmaller aspect of the task.\nMost start by asking LLMs to work on the\nentire task.Planning & Prompting \"NetLogo, I would like to spawn 50 tur-\ntles\"\n\"I want to use netlogo to help me model how\nhoneybees regulate the temperature in their\nhive. What should I do?\"\nFocus more on the generated code. Focus more on the generated instructions.Evaluating \"Talks too much. I want the code, not the\nexplanation yet. \"\n\"I am reading the text a little bit and it spits\nout a bunch of code. So it did give me steps,\nwhich is nice. \"\nMost selectively copy and paste code,\nor write code on their own.\nMost start by copying and pasting LLM-\ngenerated code.Coding \"It’d be that I just take this and see what\nthis does. \"\n“This time it gives me.. two boxes to copy. ”\nDebug themselves, or with help from\nAI.\nDebug with (more) help from AI.\nDebugging \"Oh, I didn’t ask him to move. That is\nmy problem. \"\n\"I’m going to ask it the same question, but I’m\nconfused why it said something about patches. \"\nwere often confused by NetLogo’s terms, even when they were\nmostly in plain English. N03 was confused about “why (ChatGPT)\nsaid something about patches” (note: patches are static agents that\nform NetLogo’s modeling world), and that deepened her reliance\non ChatGPT. N10 realized that she “only understand 20% of what\nI am reading, so I can’t vet it myself.” When the interviewer asked\nabout adding comments into code, N03 replied that while it might\nbe helpful, she was still missing “the high-level understanding of\nhow it comes together.”\nMany novices also lack the experience for debugging, leading to\nmore unsuccessful attempts and more frustrations. Participants, in\nparticular novices, were often confused by error messages from Net-\nLogo. N01 acknowledged that “without background knowledge,\nit is hard to figure out what the bugs are, if (LLM) gives you infor-\nmation that is inaccurate.” Without experience in debugging, many\nnovices felt frustrated and helpless as previously reported. On the\nother hand, E12 noted that his students “might not be comfortable\nwith the idea that debugging is a normal part of the process.” E01\nbelieved that “the user needs a little practice in debugging their\nown code” before working with LLM-based interfaces.\nMost novices felt a need to learn to interact with LLMs. After\nrepeated failures, N01 felt that he did not “even know what ques-\ntions to ask to get it to, because it is not doing the right thing.”\nN06 thought AI would help a lot if she could “learn more about\nhow to use AI.” N05 realized that he needed to use the correct\nkeywords, for otherwise it “will never generate a good model.” This\nknowledge is relatively easier to acquire though: while N09 felt\nthat “how to ask questions is very important”, she believed that\n“you learn by actually doing it.”\nTable 6: Users’ Needs for LLMs: Guidance, Personalization,\nand Integration\nGuidance Personalization Integration\nShould provide\nclear, less technical\nresponses, stay\non topic, and give\nsmaller pieces of\ninformation at a\ntime.\nShould provide\nresponses based\non users’ preferred\nstyles.\nShould provide bet-\nter support for cod-\ning chunks and iter-\native modeling.\nShould provide\nresponses based\non authoritative\nsources and in Net-\nLogo’s language.\nShould provide re-\nsponses based on\nthe knowledge lev-\nels and interests of\nusers.\nShould support\nworking on exist-\ning modeling code.\nShould assume less,\nclarify more, and\nstick to user inten-\ntions for modeling.\nShould support hu-\nman help-seeking\npreferences in dif-\nferent ways.\nShould support\ninput and output\nof computational\nmodeling.\n5.3 Needs for Guidance, Personalization, and\nIntegration\n5.3.1 \"Good\" Responses, \"Bad\" Responses. Participants generally\nappreciate and expect less technical, clear instructions. Many of\nthem appreciate NetLogo Chat’s design decisions that include au-\nthoritative sources in responses (see 3.1.2) and ask back clarification\nquestions (see 3.1.1). However, participants’ preferences are also\nhighly personal and situational.\nFor both designs, some participants explicitly went against exces-\nsive or unnecessary explanations, particularly when the goal is pri-\nmarily to accomplish a task at hand. For instance, E09 complained\nthat GPT-4 “talks too much. I want the code, not the explanation\nyet.” E14 complained that while related code samples provided\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nChen, et al.\nby NetLogo Chat could “contain a lot of good suggestions”, she\nwanted to move them to “another box or an expandable line”.\nSome participants appreciated and hoped that LLMs could stay\non topic and give smaller pieces of information at a time. E01\nthought NetLogo Chat would be more helpful if it only attempted\nto solve a bug “one at a time”, for users “always overfill their buffer”.\nNovices, in particular, prefer concrete, step-by-step responses, given\nthe focus they put on AI-generated instructions. N04 wanted to\n“test one by one if (LLM) gave me multiple suggestions.” Going\nbeyond text responses, N03 hoped that there could be “a visual\nto help me better understand, or internalize what different ele-\nments of the code are”, so her learning could move to a higher-level\nunderstanding of the code’s intention.\nFor NetLogo Chat, most participants reacted positively to the\nreference to authoritative sources (see 3.1.2), the usage of NetLogo’s\nlanguage, and the provision of links to sources. E03 believed that\n“the possibility to go directly from this AI to the documentation”\nwould be helpful for his students. N10 “automatically like (Net-\nLogo Chat’s response) better” because it used “NetLogo’s kind of\nturtle and patch language.” E12 felt “a little bit more confident\nin the information I was getting because it seemed to be coming\nfrom inside of the application.” However, sticking too much to au-\nthoritative explanations might have a downside. E10 complained\nthat NetLogo Chat gave him “dictionary reference”, and “dictionary\ndefinitions are not especially helpful.”\nMany participants, in particular experts, reacted positively when\nNetLogo Chat assumed less about and stuck more to their inten-\ntions (e.g. asking questions back, see 3.1.1). For example, E09\ncommented that ChatGPT (GPT-4) “assumed what I wanted it to do,\nwhereas this one makes you specify your assumptions.” He prefers\nNetLogo Chat’s approach, because “it makes you think about the\ncode more.” E12 felt that NetLogo Chat’s clarification of intention\nwas akin to “progressively guiding me towards a better prompt.”\nAs transparency is a key factor in computational modeling, N11\nfeared that if “anyone can produce an agent-based model, but with-\nout actually understanding all the parameters”, hidden assumptions\nintroduced by ChatGPT could be detrimental.\n5.3.2 Need for Personalization. In this section, we break down\nthe strong needs of experts and novices for more personalization,\nbesides response styles, into two themes: knowledge levels and\nhelp-seeking needs.\nNovices, in particular, felt a strong need for LLM-based inter-\nfaces to acknowledge their knowledge levels and produce responses\naccordingly. N07 gave a stringent critique of both systems, feeling\nboth systems were “not useful at all”, for both “presumes you know\nsomething about NetLogo”. N08 felt that “ChatGPT has no idea\nof how much or how little I know about how to code in NetLogo,\nor how to code in general.” Solving this issue would require more\npersonalized approaches. Coming from an educational background,\nboth N02 and E03 suggested that LLMs should first probe the\nknowledge level of users before providing answers.\nParticipants gave a variety of suggestions that were at times\nconflicting:\n(1) Some participants prefer a guided walkthrough. N08\nhoped that LLMs could walk him through the process and\nprovide starting points. Both E14 and N03 hoped that\nLLMs could be used alongside video tutorials, where they\ncould first see a successful example of human-AI collabora-\ntion and then ask follow-up questions.\n(2) Some participants prefer contextual recommendations.\nN11 hoped that LLMs could show related code examples\nand provide “two or three other ways that you might look\nwith”. E10 suggested that LLMs provide in-context expla-\nnations if “you don’t remember the definition or explanation\nof a particular command”.\n(3) Some participants hope that LLMs could support help-\nseeking from humans. E01 hoped that LLMs could help\nnovices “explain my situation so that I can paste it to the\nuser group”, so human experts could intervene more easily\nwhen AI fails to unstuck novices. Similarly, E17 suggested\nthat AI could be combined with “peer to peer answers and\ncollaboration”.\n(4) Some participants believed that incorrect responses\ncould become a learning opportunity. E02 was con-\ncerned that students might be “exposed to fewer options”\nwith AI, compared with “coding from scratch”. E03 feared\nthat a system capable of directly producing solutions might\ndeprive students of the debugging process, where they would\nhave learned. ” Novices also had similar feelings. After many\nhallucinated responses, N08 thought that ChatGPT “forces\nme to learn as opposed to just getting code that’s ready to\ngo.” To fully transform the moment of mistake into learning\nopportunities, educators suggest the design not to frame mis-\ntakes as failures, but rather “as a learning moment” ( E12 ).\n5.3.3 Need for Integration. Compared with ChatGPT in a separate\nbrowser window, most participants appreciated the NetLogo Chat\ninterface being an integrated part of the modeling environment.\nThey are particularly in favor of the deep integration in NetLogo\nChat’s design that goes beyond placing a CA and an IDE side-by-\nside. We further identified many participants’ need for a deeper\nintegration.\nMany participants appreciated the integration of a sandbox-like\ncode editor in NetLogo Chat, where they can tinker with smaller,\nAI-generated code chunks and execute them on the fly (see 3.1.3).\nN12 “definitely liked this feature of being able to go easily be-\ntween the code and see what was changed and what was added.”\nN04 appreciated that one can “see the code run” in the NetLogo\nIDE, which ChatGPT could not do. N13 thought while some code\ngenerated by ChatGPT was “so comprehensive”, NetLogo Chat was\nable to break it down and make them “more conducive”. Partic-\nipants also expressed further needs for iterative modeling. E13\nhoped that NetLogo Chat could help him “modularize all of my\ncommands” by splitting the code into many smaller, more manage-\nable chunks. E12 asked for a comparison feature between versions\nof code chunks that could help him “iterative changes quickly”.\nIn addition, participants also hoped that LLMs could help them\nreflect on longer pieces of (existing) code. N02 and E02 wanted\nAI to support the combination of multiple, smaller code chunks into\na single, coherent code. As such, LLM-based interfaces should be\nable to work with longer pieces of code. BothN06 and E08 hoped\nLearning Programming of ABM with LLM Companions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nthat NetLogo Chat could “look at my code and make suggestions\nbased on my code”.\nMany participants needed adaptive support for modeling more\nthan just coding. Many requested AI support in building model\ninterfaces that could be used to take in inputs or send out outputs.\nFor example, N06 needed NetLogo for her academic paper, hence\nplotting became “very important”. For educators like E13 , while\nthe canvas output was “good for the three-quarters of a project”, it\nhid “the real power of agent-based modeling - tracking the emer-\ngent properties of the model, rather than simply making bits run\naround the screen.” During the modeling processes, many interface\nparts could become necessary or unnecessary depending on situa-\ntional needs. Integrated LLM-based interfaces need to go beyond a\n“side chat window” and support various spatial configurations for\nadvanced users to decide on.\n6 DISCUSSIONS\nOur study first reported, in detail, how novices and experts per-\nceive and use LLM-based interfaces (ChatGPT & NetLogo Chat)\ndifferently to support their learning and practice of computational\nmodeling in an open-ended setting. Most participants appreciated\nthe design direction NetLogo Chat is heading toward. However,\nthey also expressed their needs for improved guidance, personaliza-\ntion, and integration which opens up huge design spaces for future\nimprovement.\n6.1 Guidance: Bridging the Novice-Expert Gap\nFigure 5: A preliminary theorization of the novice-expert\nknowledge gap.\nFor most participants, guidance is what they need most from\nLLMs in programming. While hallucinations from LLMs con-\nstantly present a challenge to everyone, with a higher frequency\nto evaluate and debug AI responses, experts suffered less negative\nimpact than novices. As a result, experts reported higher levels of\nperceived gains and more optimistic adoption plans than novices.\nWhile novices in our study also attempt to evaluate and debug\nAI responses, they are ill-equipped for these tasks. Without un-\nderstanding the knowledge gap between experts and novices, it\nbecomes impossible to design effective guidance.\nBased on our empirical findings, we theorize the two types of\nknowledge novices might need when collaborating with AI in com-\nputational modeling (Fig 5). First, the knowledge to effectively\ndecompose and plan modeling tasks. Second, the knowledge to\nevaluate AI responses and identify potential issues. We further\nidentified four components of knowledge that both novices and\nexperts reported to be essential: conceptual knowledge of modeling;\nbasic concepts of NetLogo and coding; experiences of debugging;\nand how to interact with LLMs. To mitigate the impact of currently\ninevitable hallucinations of LLMs, it is essential to help novices get\nover the knowledge gap.\nWe propose three learning moments where design intervention\nmight work best. The first moment is when users plan their next\nsteps. While most novices started by delegating the planning pro-\ncess to AI, most of them eventually planned on their own. Here,\nwe follow the constructionist learning theory for a broader under-\nstanding of planning that includes both rigid, formal plans and\n\"softer\", ad-hoc exploration of problem spaces[83]. Both planning\nstyles should be recognized as legitimate in learning and supported\nby the design [83]. With our current design, most novices reported\npositive feelings when NetLogo Chat attempted to clarify their\nintentions and produce a plan for their task. Since this phase does\nnot involve any generated code, more support could be provided, as\nnovices may have fewer problems reading and evaluating natural\nlanguage responses. They may also feel more comfortable asking\nquestions about modeling or programming ideas, relating them\nto the generated code later, without fearing that they cannot (yet)\nread or write code. Moreover, LLMs could expand learners’ visions\nby suggesting new ideas, proposing new plans, or taking notes of\nhuman ideas. When novices are confused about basic concepts,\nLLMs could suggest video or textual tutorials and provide Q&A\nalong the way.\nThe second moment is when users read and evaluate LLM-\ngenerated code. Reading and understanding code is one of the most\nimportant aspects of computing education[51]. However, novices\nin our study were neither confident nor equipped for reading code.\nAs a result, they intended to skip the code section. As predicted by\nthe interest development framework[56], the lack of skills (knowl-\nedge) and confidence (identity) may mutually enhance each other.\nBreaking the feedback loop requires designers to scaffold their read-\ning experiences in both directions. By making explanations within\ncode (as comments or tooltips) or visualizing the code structures\n(e.g. [76]), we might be able to help build novices’ connections be-\ntween code syntax and real-world meanings. To build up learners’\nconfidence, LLMs should deliver code pieces and explanations in\nadaptive sizes that work for learners. For learners who still could not\nsucceed, the interface should further provide ad-hoc support that\nhelps novices ask follow-up questions, or lead them to appropriate\nlearning resources.\nThe third moment is when users need to debug their code.\nDebugging is considered a rich learning opportunity in construc-\ntionist learning[39]. However, it is often associated with negative\nfeelings that both manifested in prior literature[91], as well as our\nfindings. Unfortunately, cognitive science has found that negative\nmoods may further impede debugging performance[43], enlarging\nthe gap between novices and experts. Following the suggestions\nof educators in our study, we suggest that LLM-based interfaces\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nChen, et al.\ncould frame bugs in a more positive light, while providing a link\nto a successful human-AI collaborative debugging process for first-\ntime learners. Both novices’ and LLMs’ debugging processes are\noften stuck in loops[92, 97]. While such situations are inevitable,\nsome expert participants suggest that LLM-based interfaces could\nencourage learners to seek help from another human. Help-seeking\nis recognized as an important part of programming education, yet\nnovices often struggle with it[54]. In such cases, LLM-based inter-\nfaces should further help them frame questions for human experts.\n6.2 Personalization: Beyond “Correctness” of\nLLMs\nPersonalization has been identified as an essential factor for per-\nceived autonomy when users interact with conversational agents[98],\nfor emotional and relational connections[89], and for various edu-\ncational benefits[6]. Adding to previous literature, we found per-\nsonalization to be a crucial factor for LLMs to facilitate effective\nguidance for learning, as participants expect LLMs to recognize\ntheir knowledge levels and react accordingly.\nWhile LLMs might have the potential to further the personal-\nization of learning, recent research in LLMs focused on the “ob-\njective” capabilities, ignoring the personalized aspect of its evalu-\nation. For example, technical reports of LLMs all reported bench-\nmarks in whether they could produce functionally correct programs\n(HumanEval)[15]; if they could correctly answer multi-choice ques-\ntions (MMLU)[33]; or if they could produce the correct answer of\ngrade school mathematical problems (GSM-8K)[20]. While working\ntoward such “correctness” benchmarks is certainly crucial for LLMs\nto reduce hallucination and produce better responses, it becomes\nproblematic when the definition of “helpfulness” or “harmfulness” is\nmeasured with a ubiquitous scale without individual differences [2],\nand such a definition has since been adopted by all major players\nin LLMs.\nAt least in learning and practice programming, we argue that\nhelpfulness cannot be a singular metric, but instead varies based on\nmany factors. Corroborating with constructionist design principles[66],\nwe identified some potentially important factors such as knowledge\nlevels and help-seeking preferences, while other factors such as\nculture, ethnicity, and gender could be as important. To support\nhuman learning, the full potential of LLMs could only be achieved\nthrough the recognition of epistemological pluralism[83]: humans\nhave different approaches toward learning, and technology needs\nto be tailored to human needs.\nMost participants in our study expected or asked for personaliza-\ntion, in the sense that LLMs recognize their knowledge levels and\nhelp-seeking needs, yet today’s designs are still far from that. While\nit is virtually impossible to fine-tune thousands of LLM variants,\nLLMs’ role-play capabilities and novel prompt-based workflows\n(e.g. the one used by NetLogo Chat, or the concept of GPTs very\nrecently released by OpenAI) have shown promising potential. As\npersonalization requires the inevitable and sometimes controver-\nsial collection of user data, we suggest a more upfront approach:\nonly collecting data that directly contributes to a more helpful AI\n(e.g. the knowledge level), only using data for this purpose, and ex-\nplaining the benefits, risks, and privacy processes at the beginning.\nAlternatively, designers could also consider flowing the pathway\nof cognitive modeling, which deduces learners’ knowledge levels\nfrom known interactions with the system[77]. On the other hand,\nour understanding of users’ perceptions, behaviors, and needs for\nLLM-based programming interfaces has just begun, and we call on\nmore studies to pursue this direction.\n6.3 Integration: LLMs for Computational\nModeling\nFor most participants, integration between LLM-based interfaces\nand modeling environments goes beyond stitching a chat win-\ndow into the IDE. While most of them appreciated NetLogo Chat’s\ndesign directions, they put forward many needs that are worth\nconsidering in future design. Here, we briefly discuss the two major\nthemes: support for troubleshooting; and support for modeling. For\ntroubleshooting:\n(1) The capability to work on smaller snippets of code,\nwith the capability to execute, explain, and debug code\nin context. For both humans and LLMs[34], debugging com-\nplicated code is known to be difficult. NetLogo Chat has made\nthe first step in reducing the scope to smaller code chunks. As\nsuch, it becomes easier for both humans to debug and LLMs\nto support their debugging processes. Whereas, more work\nis needed to bring together the code chunks into coherent\nfull programs.\n(2) The capability to leverage authoritative NetLogo doc-\numentation in generated responses, as well as for the\nuser’s own reference. In debugging contexts, LLMs’ ten-\ndency to hallucinate becomes more frustrating. By providing\nusers and LLMs with authoritative explanations within the\ndebugging context, NetLogo Chat may reduce the effort for\nusers to seek related information, which is also known to be\ndifficult for novices[24]. More work is needed to explain in a\nmore personalized way: for example, pure novices may need\nexplanations for every basic term.\n(3) The capability to automatically send in contextual in-\nformation (i.e. code and error messages) for LLM to\ntroubleshoot. Users generally appreciated NetLogo Chat’s\ndesign decision to support troubleshooting. However, the\nconvenience came with a potential price: when using NetL-\nogo Chat, users were more likely to ask LLMs for help, which\nmight lead to fewer human attempts and learning opportu-\nnities. Further studies are needed to understand this design\nbalance better.\nMany participants also asked for features that specifically sup-\nport their computational modeling tasks, which are known to have\ndifferent priorities from programming in general[ 65]. Here, two\nmore capabilities are warranted:\n(1) The capability to assume less, actively probe, and stick\nto user intentions. In addition to the potential learning\nopportunities (see Discussion 1), for participants, hidden\nassumptions in scientific modeling are particularly harmful.\nWhile users appreciate NetLogo Chat’s direction in having\nLLMs ask questions back, future interfaces should be able\nto facilitate the conversational build-up of plans and steps,\nLearning Programming of ABM with LLM Companions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nfurther supporting users to program computers piece-by-\npiece rather than falling to hidden assumptions made by\nLLMs.\n(2) The capability to support modeling practices beyond\ncoding. Building the program is only one step; computa-\ntional modeling also involves design, data visualization, and\nvalidation[88]. For LLM-based interfaces to support model-\ning practices, future interfaces should go beyond coding to\nsupport users’ efforts throughout the modeling process.\n7 LIMITATIONS AND FUTURE WORK\nThere are limitations to our study that warrant future work. As\na widely used agent-based modeling language, a deeper under-\nstanding of user perceptions, behaviors, and needs for LLM-based\ninterfaces around NetLogo may inform us of design choices for\nother modeling environments. Future work should consider com-\nputational modeling or programming environments that might\nhave different priorities. Since the NetLogo language was designed\nfor an audience without a computer science background[ 82], it\nbecomes more important and meaningful to understand how to\ndesign for bridging the novice-expert gap in LLM-based interfaces.\nHowever, it is unclear whether our findings and suggestions would\nsufficiently support novices’ and experts’ learning and practice of\nNetLogo. Using a more rigid rubric to distinguish between experts\nand novices might improve the rigor of our study. A quantitative,\ncontrolled study in the future might further (in)validate our find-\nings and suggestions. As such, we plan to work on a new iteration\nof NetLogo Chat design and empirical study to fully understand\nthe design implications.\nAlthough we aimed to recruit participants representative of Net-\nLogo’s global audience, our participant pool was not as represen-\ntative as we hoped in two key dimensions. First, our participants\nwere mostly professionals, academics, and graduate students. While\nK-12 teachers and learners are another major audience for NetLogo\nand agent-based modeling and may have different priorities and\npreferences[71], only one K-12 teacher was present in the study.\nMore studies are warranted to further the empirical understanding\nof LLM-based interfaces in education contexts. Second, the demo-\ngraphics of our participants skewed towards North American and\nEuropean, highly educated, and male. Such a group of participants,\nrecruited voluntarily, might manifest higher than average accept-\nability toward novel technology, e.g. most of our participants have\nalready engaged with ChatGPT. For future work, researchers need\nto recruit a more balanced and diverse group of participants, if the\ngoal is for LLM-based programming interfaces to equitably support\nnovices and experts throughout the world.\n8 CONCLUSION\nAs Large language models (LLMs) have the potential to fundamen-\ntally change how people learn and practice computational modeling\nand programming in general, it is crucial that we gain a deeper\nunderstanding of users’ perceptions, behaviors, and needs in a more\nnaturalistic setting. For this purpose, we designed and developed\nNetLogo Chat, a novel LLM-based system that supports and inte-\ngrates with a version of NetLogo IDE. We conducted an interview\nstudy with 30 adult participants to understand how they perceived,\ncollaborated with, and asked for LLM-based interfaces for learning\nand practice of NetLogo. Consistent with previous studies, experts\nreported more perceived benefits than novices. We found remark-\nable differences between novices and experts in their perceptions,\nbehaviors, and needs. We identified a knowledge gap that might\nhave contributed to the differences. We proposed design recommen-\ndations around participants’ main needs: guidance, personalization,\nand integration. Our findings inform future design of LLM-based\nprogramming interfaces, especially for computational modeling.\nACKNOWLEDGMENTS\nWe would like to express our gratitude to the NetLogo Online com-\nmunity and Complexity Explorer for their help and support. We\nare especially thankful to the hundreds of NetLogo users who vol-\nunteered for the study. We would also like to thank current and\nformer members of our lab and anonymous youth users of Turtle\nUniverse, who provided valuable feedback and ideas during our\ndesign process. Specifically, we want to acknowledge the intellec-\ntual contributions of Umit Aslan; Aaron Brandes; Jeremy Baker;\nJason Bertsche; Matthew Berland; Sharona Levy; Jacob Kelter; Leif\nRasmussen; David Weintrop; and Lexie Zhao. Finally, we appreciate\nthe valuable and actionable feedback from our anonymous CHI\nreviewers, which significantly strengthened the paper.\nREFERENCES\n[1] [n. d.]. Using GitHub Copilot Chat. https://ghdocs-prod.azurewebsites.net/en/\ncopilot/github-copilot-chat/using-github-copilot-chat\n[2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova\nDasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas\nJoseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nel-\nson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott John-\nston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei,\nTom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared\nKaplan. 2022. Training a Helpful and Harmless Assistant with Reinforcement\nLearning from Human Feedback. https://doi.org/10.48550/arXiv.2204.05862\narXiv:2204.05862 [cs].\n[3] Rishabh Balse, Bharath Valaboju, Shreya Singhal, Jayakrishnan Madathil War-\nriem, and Prajish Prasad. 2023. Investigating the Potential of GPT-3 in Providing\nFeedback for Programming Assessments. In Proceedings of the 2023 Confer-\nence on Innovation and Technology in Computer Science Education V. 1 (ITiCSE\n2023). Association for Computing Machinery, New York, NY, USA, 292–298.\nhttps://doi.org/10.1145/3587102.3588852\n[4] Shraddha Barke, Michael B. James, and Nadia Polikarpova. 2023. Grounded\ncopilot: How programmers interact with code-generating models. Proceedings\nof the ACM on Programming Languages 7, OOPSLA1 (2023), 85–111. Publisher:\nACM New York, NY, USA.\n[5] Brett A Becker, Paul Denny, Raymond Pettit, Durell Bouchard, Dennis J Bouvier,\nBrian Harrington, Amir Kamil, Amey Karkare, Chris McDonald, Peter-Michael\nOsera, et al. 2019. Compiler error messages considered unhelpful: The landscape\nof text-based programming error message research. Proceedings of the working\ngroup reports on innovation and technology in computer science education (2019),\n177–210.\n[6] Matthew L. Bernacki, Meghan J. Greene, and Nikki G. Lobczowski. 2021. A\nsystematic review of research on personalized learning: Personalized by whom,\nto what, how, and for what purpose (s)? Educational Psychology Review 33, 4\n(2021), 1675–1715. Publisher: Springer.\n[7] Paulo Blikstein. 2011. Using learning analytics to assess students’ behavior in\nopen-ended programming tasks. InProceedings of the 1st international conference\non learning analytics and knowledge . 110–116.\n[8] Paulo Blikstein, Marcelo Worsley, Chris Piech, Mehran Sahami, Steven Cooper,\nand Daphne Koller. 2014. Programming pluralism: Using learning analytics\nto detect patterns in the learning of computer programming. Journal of the\nLearning Sciences 23, 4 (2014), 561–599. Publisher: Taylor & Francis.\n[9] Corey Brady, Melissa Gresalfi, Selena Steinberg, and Madison Knowe. 2020.\nDebugging for Art’s Sake: Beginning Programmers’ Debugging Activity in an\nExpressive Coding Context. (June 2020). https://repository.isls.org//handle/1/\n6319 Publisher: International Society of the Learning Sciences (ISLS).\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nChen, et al.\n[10] Christopher Bull and Ahmed Kharrufa. 2023. Generative AI Assistants in\nSoftware Development Education: A vision for integrating Generative AI into\neducational practice, not instinctively defending against it. IEEE Software (2023),\n1–9. https://doi.org/10.1109/MS.2023.3300574 Conference Name: IEEE Software.\n[11] Santi Caballé and Jordi Conesa. 2019. Conversational agents in support for\ncollaborative learning in MOOCs: An analytical review. InAdvances in Intelligent\nNetworking and Collaborative Systems: The 10th International Conference on\nIntelligent Networking and Collaborative Systems (INCoS-2018) . Springer, 384–\n394.\n[12] Fuxiang Chen, Fatemeh H. Fard, David Lo, and Timofey Bryksin. 2022. On the\ntransferability of pre-trained language models for low-resource programming\nlanguages. In Proceedings of the 30th IEEE/ACM International Conference on\nProgram Comprehension. 401–412.\n[13] John Chen and Uri J. Wilensky. 2021. Turtle Universe. https://turtlesim.com/\nproducts/turtle-universe/\n[14] John Chen, Lexie Zhao, Horn Michael, and Wilensky Uri. 2023. The Pocket-\nworld Playground: Engaging Online, Out-of-School Learners with Agent-based\nProgramming. In Proceedings of the ACM Interaction Design and Children (IDC) .\n[15] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\nde Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nand Greg Brockman. 2021. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374 (2021).\n[16] Zhutian Chen, Chenyang Zhang, Qianwen Wang, Jakob Troidl, Simon Warchol,\nJohanna Beyer, Nils Gehlenborg, and Hanspeter Pfister. 2023. Beyond Generating\nCode: Evaluating GPT on a Data Visualization Course. https://doi.org/10.48550/\narXiv.2306.02914 arXiv:2306.02914 [cs].\n[17] Michelene TH Chi, Paul J Feltovich, and Robert Glaser. 1981. Categorization and\nrepresentation of physics problems by experts and novices. Cognitive science 5,\n2 (1981), 121–152.\n[18] Douglas Clark, Brian Nelson, Pratim Sengupta, and Cynthia D’Angelo. 2009.\nRethinking science learning through digital games and simulations: Genres,\nexamples, and evidence. In Learning science: Computer games, simulations, and\neducation workshop sponsored by the National Academy of Sciences, Washington,\nDC.\n[19] Leigh Clark, Nadia Pantidi, Orla Cooney, Philip Doyle, Diego Garaialde, Justin\nEdwards, Brendan Spillane, Emer Gilmartin, Christine Murad, and Cosmin\nMunteanu. 2019. What makes a good conversation? Challenges in designing\ntruly conversational agents. In Proceedings of the 2019 CHI conference on human\nfactors in computing systems . 1–12.\n[20] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,\nLukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,\nChristopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math\nWord Problems. https://doi.org/10.48550/arXiv.2110.14168 arXiv:2110.14168\n[cs].\n[21] Grant Cooper. 2023. Examining Science Education in ChatGPT: An Exploratory\nStudy of Generative Artificial Intelligence. Journal of Science Education and\nTechnology 32, 3 (June 2023), 444–452. https://doi.org/10.1007/s10956-023-\n10039-y\n[22] Juliet M. Corbin and Anselm Strauss. 1990. Grounded theory research: Proce-\ndures, canons, and evaluative criteria. Qualitative sociology 13, 1 (1990), 3–21.\nPublisher: Springer.\n[23] Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh,\nMichel C. Desmarais, and Zhen Ming Jack Jiang. 2023. Github copilot ai pair\nprogrammer: Asset or liability? Journal of Systems and Software 203 (2023),\n111734. Publisher: Elsevier.\n[24] Brian Dorn, Adam Stankiewicz, and Chris Roggi. 2013. Lost while searching:\nDifficulties in information seeking among end-user programmers. Proceedings\nof the American Society for Information Science and Technology 50, 1 (2013), 1–10.\nPublisher: Wiley Online Library.\n[25] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. Gpts\nare gpts: An early look at the labor market impact potential of large language\nmodels. arXiv preprint arXiv:2303.10130 (2023).\n[26] Alexander J. Fiannaca, Chinmay Kulkarni, Carrie J. Cai, and Michael Terry. 2023.\nProgramming without a Programming Language: Challenges and Opportunities\nfor Designing Developer Tools for Prompt Programming. In Extended Abstracts\nof the 2023 CHI Conference on Human Factors in Computing Systems . 1–7.\n[27] James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and\nJames Prather. 2022. The robots are coming: Exploring the implications of openai\ncodex on introductory programming. In Proceedings of the 24th Australasian\nComputing Education Conference . 10–19.\n[28] Kenneth R. Fleischmann and William A. Wallace. 2009. Ensuring transparency\nin computational modeling. Commun. ACM 52, 3 (March 2009), 131–134. https:\n//doi.org/10.1145/1467247.1467278\n[29] Yue Fu, Mingrui Zhang, Lynn K. Nguyen, Yifan Lin, Rebecca Michelson,\nTala June Tayebi, and Alexis Hiniker. 2023. Self-Talk with Superhero Zip:\nSupporting Children’s Socioemotional Learning with Conversational Agents. In\nProceedings of the 22nd Annual ACM Interaction Design and Children Conference .\n173–186.\n[30] Katy Ilonka Gero, Zahra Ashktorab, Casey Dugan, Qian Pan, James Johnson,\nWerner Geyer, Maria Ruiz, Sarah Miller, David R. Millen, Murray Campbell,\nSadhana Kumaravel, and Wei Zhang. 2020. Mental Models of AI Agents in a\nCooperative Game Setting. In Proceedings of the 2020 CHI Conference on Human\nFactors in Computing Systems (CHI ’20) . Association for Computing Machinery,\nNew York, NY, USA, 1–12. https://doi.org/10.1145/3313831.3376316\n[31] Zi Gong, Yinpeng Guo, Pingyi Zhou, Cuiyun Gao, Yasheng Wang, and Zenglin\nXu. 2022. MultiCoder: Multi-Programming-Lingual Pre-Training for Low-\nResource Code Completion. https://doi.org/10.48550/arXiv.2212.09666\narXiv:2212.09666 [cs].\n[32] Idit Harel and Seymour Papert. 1990. Software design as a learning environment.\nInteractive learning environments 1, 1 (1990), 1–32. Publisher: Taylor & Francis.\n[33] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn\nSong, and Jacob Steinhardt. 2020. Measuring Massive Multitask Language\nUnderstanding. https://openreview.net/forum?id=d7KBjmI3GmQ\n[34] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo,\nDavid Lo, John Grundy, and Haoyu Wang. 2023. Large Language Models for\nSoftware Engineering: A Systematic Literature Review. http://arxiv.org/abs/\n2308.10620 arXiv:2308.10620 [cs].\n[35] Nicole M. Hutchins, Gautam Biswas, Ningyu Zhang, Caitlin Snyder, Ákos\nLédeczi, and Miklós Maróti. 2020. Domain-specific modeling languages in\ncomputer-based learning environments: A systematic approach to support sci-\nence learning through computational modeling.International Journal of Artificial\nIntelligence in Education 30 (2020), 537–580. Publisher: Springer.\n[36] Yuin Jeong, Juho Lee, and Younah Kang. 2019. Exploring Effects of Conver-\nsational Fillers on User Perception of Conversational Agents. In Extended Ab-\nstracts of the 2019 CHI Conference on Human Factors in Computing Systems\n(CHI EA ’19) . Association for Computing Machinery, New York, NY, USA, 1–6.\nhttps://doi.org/10.1145/3290607.3312913\n[37] Ellen Jiang, Edwin Toh, Alejandra Molina, Kristen Olson, Claire Kayacik, Aaron\nDonsbach, Carrie J. Cai, and Michael Terry. 2022. Discovering the syntax and\nstrategies of natural language programming with generative language models. In\nProceedings of the 2022 CHI Conference on Human Factors in Computing Systems .\n1–19.\n[38] Harshit Joshi, José Cambronero Sanchez, Sumit Gulwani, Vu Le, Gust Ver-\nbruggen, and Ivan Radiček. 2023. Repair Is Nearly Generation: Multilingual\nProgram Repair with LLMs. Proceedings of the AAAI Conference on Artificial In-\ntelligence 37, 4 (June 2023), 5131–5140. https://doi.org/10.1609/aaai.v37i4.25642\nNumber: 4.\n[39] Yasmin Kafai, Gautam Biswas, Nicole Hutchins, Caitlin Snyder, Karen Brennan,\nPaulina Haduong, Kayla DesPortes, Morgan Fong, Virginia J. Flood, and Oia\nWalker-van Aalst. 2020. Turning bugs into learning opportunities: understand-\ning debugging processes, perspectives, and pedagogies. (2020). Publisher:\nInternational Society of the Learning Sciences (ISLS).\n[40] Yasmin B. Kafai and Quinn Burke. 2015. Constructionist Gaming: Understanding\nthe Benefits of Making Games for Learning. Educational Psychologist 50, 4\n(Oct. 2015), 313–334. https://doi.org/10.1080/00461520.2015.1124022 Publisher:\nRoutledge _eprint: https://doi.org/10.1080/00461520.2015.1124022.\n[41] Ken Kahn and Niall Winters. 2021. Constructionism and AI: A history and\npossible futures. British Journal of Educational Technology 52, 3 (2021), 1130–\n1142. Publisher: Wiley Online Library.\n[42] Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara J. Ericson, David\nWeintrop, and Tovi Grossman. 2023. Studying the effect of AI Code Generators\non Supporting Novice Learners in Introductory Programming. arXiv preprint\narXiv:2302.07427 (2023).\n[43] Iftikhar Ahmed Khan, Willem-Paul Brinkman, and Robert M Hierons. 2011. Do\nmoods affect programmers’ debug performance? Cognition, Technology & Work\n13 (2011), 245–258.\n[44] Junaed Younus Khan and Gias Uddin. 2022. Automatic code documentation gen-\neration using gpt-3. InProceedings of the 37th IEEE/ACM International Conference\non Automated Software Engineering . 1–6.\n[45] Varun Kumar, Leonard Gleyzer, Adar Kahana, Khemraj Shukla, and George Em\nKarniadakis. 2023. MyCrunchGPT: A chatGPT assisted framework for scientific\nmachine learning. https://doi.org/10.48550/arXiv.2306.15551 arXiv:2306.15551\n[physics].\n[46] Sam Lau and Philip J. Guo. 2023. From\" Ban It Till We Understand It\" to\"\nResistance is Futile\": How University Programming Instructors Plan to Adapt as\nMore Students Use AI Code Generation and Explanation Tools such as ChatGPT\nand GitHub Copilot. (2023).\n[47] Yunyao Li, Huahai Yang, and Hosagrahar V. Jagadish. 2005. Nalix: an interactive\nnatural language interface for querying xml. In Proceedings of the 2005 ACM\nSIGMOD international conference on Management of data . 900–902.\n[48] Phoebe Lin, Jessica Van Brummelen, Galit Lukin, Randi Williams, and Cynthia\nBreazeal. 2020. Zhorai: Designing a Conversational Agent for Children to\nExplore Machine Learning Concepts. Proceedings of the AAAI Conference on\nArtificial Intelligence 34, 09 (April 2020), 13381–13388. https://doi.org/10.1609/\naaai.v34i09.7061 Number: 09.\nLearning Programming of ABM with LLM Companions\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\n[49] Erin Chao Ling, Iis Tussyadiah, Aarni Tuomi, Jason Stienmetz, and Athina\nIoannou. 2021. Factors influencing users’ adoption and use of conversational\nagents: A systematic review. Psychology & marketing 38, 7 (2021), 1031–1051.\nPublisher: Wiley Online Library.\n[50] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is\nYour Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large\nLanguage Models for Code Generation. https://doi.org/10.48550/arXiv.2305.\n01210 arXiv:2305.01210 [cs].\n[51] Mike Lopez, Jacqueline Whalley, Phil Robbins, and Raymond Lister. 2008. Rela-\ntionships between reading, tracing and writing skills in introductory program-\nming. In Proceedings of the fourth international workshop on computing education\nresearch. 101–112.\n[52] Stephen MacNeil, Andrew Tran, Dan Mogil, Seth Bernstein, Erin Ross, and\nZiheng Huang. 2022. Generating diverse code explanations using the gpt-3\nlarge language model. InProceedings of the 2022 ACM Conference on International\nComputing Education Research-Volume 2 . 37–39.\n[53] Alexandre Magueresse, Vincent Carles, and Evan Heetderks. 2020. Low-resource\nLanguages: A Review of Past Work and Future Challenges. http://arxiv.org/\nabs/2006.07264 arXiv:2006.07264 [cs].\n[54] Samiha Marwan, Anay Dombe, and Thomas W Price. 2020. Unproductive help-\nseeking in programming: What it is and how to address it. In Proceedings of\nthe 2020 ACM Conference on Innovation and Technology in Computer Science\nEducation. 54–60.\n[55] Andrew M. McNutt, Chenglong Wang, Robert A. Deline, and Steven M. Drucker.\n2023. On the design of ai-powered code assistants for notebooks. In Proceedings\nof the 2023 CHI Conference on Human Factors in Computing Systems . 1–16.\n[56] Joseph E. Michaelis and David Weintrop. 2022. Interest Development Theory in\nComputing Education: A Framework and Toolkit for Researchers and Designers.\nACM Transactions on Computing Education (TOCE) (2022). Publisher: ACM New\nYork, NY.\n[57] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and\nBrad Myers. 2023. In-IDE Generation-based Information Support with a Large\nLanguage Model. https://doi.org/10.48550/arXiv.2307.08177 arXiv:2307.08177\n[cs].\n[58] OpenAI. 2023. GPT-4 Technical Report. https://doi.org/10.48550/arXiv.2303.\n08774 arXiv:2303.08774 [cs].\n[59] Soumen Pal, Manojit Bhattacharya, Sang-Soo Lee, and Chiranjib Chakraborty.\n2023. A Domain-Specific Next-Generation Large Language Model (LLM) or\nChatGPT is Required for Biomedical Engineering and Research. Annals of\nBiomedical Engineering (2023), 1–4. Publisher: Springer.\n[60] Seymour Papert. 1980. Mindstorms: Children, computers, and powerful ideas.\n(1980). Publisher: Basic Books.\n[61] Seymour Papert and Idit Harel. 1991. Situating constructionism. constructionism\n36, 2 (1991), 1–11.\n[62] Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. 2023. The\nImpact of AI on Developer Productivity: Evidence from GitHub Copilot. https:\n//doi.org/10.48550/arXiv.2302.06590 arXiv:2302.06590 [cs].\n[63] Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh. 2022. Do Users\nWrite More Insecure Code with AI Assistants? https://doi.org/10.48550/arXiv.\n2211.03622 arXiv:2211.03622 [cs].\n[64] David Price, Ellen Rilofff, Joseph Zachary, and Brandon Harvey. 2000. Natural-\nJava: A natural language interface for programming in Java. In Proceedings of\nthe 5th international conference on Intelligent user interfaces . 207–211.\n[65] Zenon W. Pylyshyn. 1978. Computational models and empirical constraints.\nBehavioral and Brain Sciences 1, 1 (March 1978), 91–99. https://doi.org/10.1017/\nS0140525X00059793 Publisher: Cambridge University Press.\n[66] Mitchel Resnick and Brian Silverman. 2005. Some reflections on designing\nconstruction kits for kids. In Proceedings of the 2005 conference on Interaction\ndesign and children . 117–122.\n[67] Peter Robe and Sandeep Kaur Kuttal. 2022. Designing PairBuddy—A Conver-\nsational Agent for Pair Programming. ACM Transactions on Computer-Human\nInteraction 29, 4 (May 2022), 34:1–34:44. https://doi.org/10.1145/3498326\n[68] Steven I. Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and\nJustin D. Weisz. 2023. The programmer’s assistant: Conversational interac-\ntion with a large language model for software development. In Proceedings of\nthe 28th International Conference on Intelligent User Interfaces . 491–514.\n[69] Shruti Sannon, Brett Stoll, Dominic DiFranzo, Malte F. Jung, and Natalya N.\nBazarova. 2020. “I just shared your responses”: Extending Communication\nPrivacy Management Theory to Interactions with Conversational Agents. Pro-\nceedings of the ACM on Human-Computer Interaction 4, GROUP (Jan. 2020),\n08:1–08:18. https://doi.org/10.1145/3375188\n[70] Jaromir Savelka, Arav Agarwal, Marshall An, Chris Bogart, and Majd Sakr. 2023.\nThrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle\nto Pass Assessments in Higher Education Programming Courses. In Proceedings\nof the 2023 ACM Conference on International Computing Education Research V.1 .\n78–92. https://doi.org/10.1145/3568813.3600142 arXiv:2306.10073 [cs].\n[71] Pratim Sengupta, Amanda Dickes, Amy Voss Farris, Ashlyn Karan, David Martin,\nand Mason Wright. 2015. Programming in K-12 science classrooms. Commun.\nACM 58, 11 (Oct. 2015), 33–35. https://doi.org/10.1145/2822517\n[72] Pratim Sengupta, John S. Kinnebrew, Satabdi Basu, Gautam Biswas, and Douglas\nClark. 2013. Integrating computational thinking with K-12 science education\nusing agent-based computation: A theoretical framework. Education and Infor-\nmation Technologies 18, 2 (June 2013), 351–380. https://doi.org/10.1007/s10639-\n012-9240-x\n[73] Vidya Setlur, Sarah E. Battersby, Melanie Tory, Rich Gossweiler, and Angel X.\nChang. 2016. Eviza: A natural language interface for visual analysis. In Pro-\nceedings of the 29th annual symposium on user interface software and technology .\n365–377.\n[74] Marita Skjuve, Asbjørn Følstad, and Petter Bae Brandtzaeg. 2023. The User\nExperience of ChatGPT: Findings from a Questionnaire Study of Early Users. In\nProceedings of the 5th International Conference on Conversational User Interfaces .\n1–10.\n[75] Cynthia Solomon, Brian Harvey, Ken Kahn, Henry Lieberman, Mark L. Miller,\nMargaret Minsky, Artemis Papert, and Brian Silverman. 2020. History of logo.\nProceedings of the ACM on Programming Languages 4, HOPL (2020), 1–66. Pub-\nlisher: ACM New York, NY, USA.\n[76] Juha Sorva, Ville Karavirta, and Lauri Malmi. 2013. A review of generic pro-\ngram visualization systems for introductory programming education. ACM\nTransactions on Computing Education (TOCE) 13, 4 (2013), 1–64.\n[77] Ron Sun. 2008. Introduction to computational cognitive modeling. Cambridge\nhandbook of computational psychology (2008), 3–19.\n[78] Chee Wei Tan, Shangxin Guo, Man Fai Wong, and Ching Nam Hang. 2023.\nCopilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-\nbased Large Language Models. http://arxiv.org/abs/2307.14349 arXiv:2307.14349\n[cs].\n[79] Artur Tarassow. 2023. The potential of LLMs for coding with low-resource\nand domain-specific programming languages. http://arxiv.org/abs/2307.13018\narXiv:2307.13018 [cs].\n[80] J. C. Thiele, W. Kurth, and V. Grimm. 2011. Agent-and individual-based modeling\nwith NetLogo: Introduction and new NetLogo extensions. Deutscher Verband\nForstlicher Forschungsanstalten, Sektion Forstliche Biometrie und Informatik-22.\nTagung (2011).\n[81] Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques\nKlein, and Tegawendé F. Bissyandé. 2023. Is ChatGPT the Ultimate Programming\nAssistant–How far is it? arXiv preprint arXiv:2304.11938 (2023).\n[82] Seth Tisue and Uri Wilensky. 2004. Netlogo: A simple environment for modeling\ncomplexity. In International conference on complex systems , Vol. 21. Citeseer,\n16–21.\n[83] Sherry Turkle and Seymour Papert. 1990. Epistemological pluralism: Styles\nand voices within the computer culture. Signs: Journal of women in culture and\nsociety 16, 1 (1990), 128–157. Publisher: University of Chicago Press.\n[84] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation\nvs. experience: Evaluating the usability of code generation tools powered by\nlarge language models. In Chi conference on human factors in computing systems\nextended abstracts . 1–7.\n[85] Thiemo Wambsganss, Tobias Kueng, Matthias Soellner, and Jan Marco Leimeis-\nter. 2021. ArgueTutor: An adaptive dialog-based learning system for argu-\nmentation skills. In Proceedings of the 2021 CHI conference on human factors in\ncomputing systems . 1–13.\n[86] Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A. Saurous, and Yoon Kim.\n2023. Grammar Prompting for Domain-Specific Language Generation with\nLarge Language Models. http://arxiv.org/abs/2305.19234 arXiv:2305.19234 [cs].\n[87] Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, and Ashok Goel. 2021.\nTowards mutual theory of mind in human-ai interaction: How language reflects\nwhat students perceive about a virtual teaching assistant. In Proceedings of the\n2021 CHI conference on human factors in computing systems . 1–14.\n[88] David Weintrop, Elham Beheshti, Michael Horn, Kai Orton, Kemi Jona, Laura\nTrouille, and Uri Wilensky. 2016. Defining Computational Thinking for Mathe-\nmatics and Science Classrooms. Journal of Science Education and Technology 25,\n1 (Feb. 2016), 127–147. https://doi.org/10.1007/s10956-015-9581-5\n[89] Galit Wellner and Ilya Levin. 2023. Ihde meets Papert: combining postphe-\nnomenology and constructionism for a future agenda of philosophy of education\nin the era of digital technologies. Learning, Media and Technology (2023), 1–14.\nPublisher: Taylor & Francis.\n[90] Michel Wermelinger. 2023. Using GitHub Copilot to solve simple programming\nproblems. In Proceedings of the 54th ACM Technical Symposium on Computer\nScience Education V. 1 . 172–178.\n[91] Jacqueline Whalley, Amber Settle, and Andrew Luxton-Reilly. 2021. Novice\nreflections on debugging. In Proceedings of the 52nd ACM technical symposium\non computer science education . 73–79.\n[92] Jacqueline Whalley, Amber Settle, and Andrew Luxton-Reilly. 2021. Novice Re-\nflections on Debugging. In Proceedings of the 52nd ACM Technical Symposium on\nComputer Science Education (SIGCSE ’21) . Association for Computing Machinery,\nNew York, NY, USA, 73–79. https://doi.org/10.1145/3408877.3432374\n[93] Uri Wilensky and William Rand. 2015. An introduction to agent-based modeling:\nmodeling natural, social, and engineered complex systems with NetLogo . Mit\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA,\nChen, et al.\nPress.\n[94] Uri J. Wilensky. 1997. NetLogo Wolf Sheep Predation model. http://ccl.\nnorthwestern.edu/netlogo/models/WolfSheepPredation\n[95] Rainer Winkler, Sebastian Hobert, Antti Salovaara, Matthias Söllner, and\nJan Marco Leimeister. 2020. Sara, the lecturer: Improving learning in online\neducation with a scaffolding-based conversational agent. In Proceedings of the\n2020 CHI conference on human factors in computing systems . 1–14.\n[96] Rainer Winkler and Matthias Söllner. 2018. Unleashing the potential of chatbots\nin education: A state-of-the-art analysis. InAcademy of Management Proceedings ,\nVol. 2018. Academy of Management Briarcliff Manor, NY 10510, 15903. Issue: 1.\n[97] Xingbo Wu, Nathanaël Cheriere, Cheng Zhang, and Dushyanth Narayanan.\n2023. RustGen: An Augmentation Approach for Generating Compilable Rust\nCode with Large Language Models. (June 2023). https://openreview.net/forum?\nid=y9A0vJ5vuM\n[98] Xi Yang and Marco Aurisicchio. 2021. Designing conversational agents: A self-\ndetermination theory approach. In Proceedings of the 2021 CHI Conference on\nHuman Factors in Computing Systems . 1–16.\n[99] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R.\nNarasimhan, and Yuan Cao. 2022. ReAct: Synergizing Reasoning and Act-\ning in Language Models. In The Eleventh International Conference on Learning\nRepresentations.\n[100] Ramazan Yilmaz and Fatma Gizem Karaoglan Yilmaz. 2023. Augmented in-\ntelligence in programming learning: Examining student views on the use of\nChatGPT for programming learning. Computers in Human Behavior: Artificial\nHumans 1, 2 (2023), 100005. Publisher: Elsevier.\n[101] J. D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang.\n2023. Why Johnny can’t prompt: how non-AI experts try (and fail) to design\nLLM prompts. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems . 1–21.\n[102] Cynthia Zastudil, Magdalena Rogalska, Christine Kapp, Jennifer Vaughn, and\nStephen MacNeil. 2023. Generative AI in Computing Education: Perspectives of\nStudents and Instructors. arXiv preprint arXiv:2308.04309 (2023).\nReceived 14 September 2023; revised 12 December 2023; accepted 19 January\n2024"
}