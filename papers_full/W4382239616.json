{
  "title": "PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction",
  "url": "https://openalex.org/W4382239616",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2159437367",
      "name": "Jiang Jiawei",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A4382258504",
      "name": "Chengkai Han",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2307999729",
      "name": "Wayne Xin Zhao",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2104795587",
      "name": "Jingyuan Wang",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2159437367",
      "name": "Jiang Jiawei",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A4382258504",
      "name": "Chengkai Han",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2307999729",
      "name": "Wayne Xin Zhao",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2104795587",
      "name": "Jingyuan Wang",
      "affiliations": [
        "Peng Cheng Laboratory",
        "Beihang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3038981236",
    "https://openalex.org/W2097308346",
    "https://openalex.org/W116902681",
    "https://openalex.org/W2990436499",
    "https://openalex.org/W4200634602",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6787995345",
    "https://openalex.org/W6774097426",
    "https://openalex.org/W3170140111",
    "https://openalex.org/W2903871660",
    "https://openalex.org/W3126367810",
    "https://openalex.org/W3171958173",
    "https://openalex.org/W4283819096",
    "https://openalex.org/W3127180809",
    "https://openalex.org/W4309395882",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W3113297449",
    "https://openalex.org/W2768008502",
    "https://openalex.org/W3101687079",
    "https://openalex.org/W2996936831",
    "https://openalex.org/W4290927668",
    "https://openalex.org/W2999016964",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6746052068",
    "https://openalex.org/W3045642713",
    "https://openalex.org/W2950817888",
    "https://openalex.org/W4291126441",
    "https://openalex.org/W2996847713",
    "https://openalex.org/W3034944009",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2583466634",
    "https://openalex.org/W3012562343",
    "https://openalex.org/W3028192203",
    "https://openalex.org/W6762978078",
    "https://openalex.org/W3000386982",
    "https://openalex.org/W3153890903",
    "https://openalex.org/W2788134583",
    "https://openalex.org/W3111993719",
    "https://openalex.org/W4200547309",
    "https://openalex.org/W6691900184",
    "https://openalex.org/W3123191313",
    "https://openalex.org/W3103720336",
    "https://openalex.org/W2528639018",
    "https://openalex.org/W3176398102",
    "https://openalex.org/W2991165231",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965341826",
    "https://openalex.org/W3207461654",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W3193812480",
    "https://openalex.org/W3174022889",
    "https://openalex.org/W4287829537",
    "https://openalex.org/W3040607188",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4283817628",
    "https://openalex.org/W3080253043",
    "https://openalex.org/W2998559444",
    "https://openalex.org/W2253276617",
    "https://openalex.org/W2997848713",
    "https://openalex.org/W3176075655",
    "https://openalex.org/W4321649216",
    "https://openalex.org/W2963358464",
    "https://openalex.org/W4385270681"
  ],
  "abstract": "As a core technology of Intelligent Transportation System, traffic flow prediction has a wide range of applications. The fundamental challenge in traffic flow prediction is to effectively model the complex spatial-temporal dependencies in traffic data. Spatial-temporal Graph Neural Network (GNN) models have emerged as one of the most promising methods to solve this problem. However, GNN-based models have three major limitations for traffic prediction: i) Most methods model spatial dependencies in a static manner, which limits the ability to learn dynamic urban traffic patterns; ii) Most methods only consider short-range spatial information and are unable to capture long-range spatial dependencies; iii) These methods ignore the fact that the propagation of traffic conditions between locations has a time delay in traffic systems. To this end, we propose a novel Propagation Delay-aware dynamic long-range transFormer, namely PDFormer, for accurate traffic flow prediction. Specifically, we design a spatial self-attention module to capture the dynamic spatial dependencies. Then, two graph masking matrices are introduced to highlight spatial dependencies from short- and long-range views. Moreover, a traffic delay-aware feature transformation module is proposed to empower PDFormer with the capability of explicitly modeling the time delay of spatial information propagation. Extensive experimental results on six real-world public traffic datasets show that our method can not only achieve state-of-the-art performance but also exhibit competitive computational efficiency. Moreover, we visualize the learned spatial-temporal attention map to make our model highly interpretable.",
  "full_text": "PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for\nTraffic Flow Prediction\nJiawei Jiang1*, Chengkai Han1*, Wayne Xin Zhao4, Jingyuan Wang1,2,3†\n1School of Computer Science and Engineering, Beihang University, Beijing, China\n2Pengcheng Laboratory, Shenzhen, China\n3School of Economics and Management, Beihang University, Beijing, China\n4Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\nAbstract\nAs a core technology of Intelligent Transportation System,\ntraffic flow prediction has a wide range of applications. The\nfundamental challenge in traffic flow prediction is to effec-\ntively model the complex spatial-temporal dependencies in\ntraffic data. Spatial-temporal Graph Neural Network (GNN)\nmodels have emerged as one of the most promising meth-\nods to solve this problem. However, GNN-based models have\nthree major limitations for traffic prediction: i) Most methods\nmodel spatial dependencies in a static manner, which limits\nthe ability to learn dynamic urban traffic patterns; ii) Most\nmethods only consider short-range spatial information and\nare unable to capture long-range spatial dependencies; iii)\nThese methods ignore the fact that the propagation of traf-\nfic conditions between locations has a time delay in traffic\nsystems. To this end, we propose a novel P\nropagation Delay-\naware dynamic long-range transFormer , namely PDFormer,\nfor accurate traffic flow prediction. Specifically, we design a\nspatial self-attention module to capture the dynamic spatial\ndependencies. Then, two graph masking matrices are intro-\nduced to highlight spatial dependencies from short- and long-\nrange views. Moreover, a traffic delay-aware feature transfor-\nmation module is proposed to empower PDFormer with the\ncapability of explicitly modeling the time delay of spatial in-\nformation propagation. Extensive experimental results on six\nreal-world public traffic datasets show that our method can\nnot only achieve state-of-the-art performance but also exhibit\ncompetitive computational efficiency. Moreover, we visualize\nthe learned spatial-temporal attention map to make our model\nhighly interpretable.\nIntroduction\nIn recent years, rapid urbanization has posed great chal-\nlenges to modern urban traffic management. As an indis-\npensable part of modern smart cities, intelligent transporta-\ntion systems (ITS) (Yin et al. 2015) have been developed to\nanalyze, manage, and improve traffic conditions (e.g.,reduc-\ning traffic congestion). As a core technology of ITS, traffic\nflow prediction (Tedjopurnomo et al. 2022) has been widely\nstudied, aiming to predict the future flow of traffic systems\n*These authors contributed equally.\n†Corresponding author: jywang@buaa.edu.cn\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(a) Node Location\n0 24100\n300\n500\nStrong Weak\n(b) Dynamic\nNode A\nNode B\n0 24100\n300\n500\n(c) Long-Range\nNode A\nNode C\n0 24100\n300\n500\nSudden Drop\n(d) Propagation Delay\nNode D\nNode E\nMon Wed Fri Sun\n100\n200\n300\n400\n(e) Periodicity\nFigure 1: The Findings about Traffic Prediction.\nbased on historical observations. It has been shown that ac-\ncurate traffic flow prediction can be useful for various traffic-\nrelated applications (Wang et al. 2021), including route plan-\nning, vehicle dispatching, and congestion relief.\nFor traffic flow prediction, the fundamental challenge is\nto effectively capture and model the complex and dynamic\nspatial-temporal dependencies of traffic data (Yin et al.\n2022). Many attempts have been made in the literature to de-\nvelop various deep learning models for this task. As early so-\nlutions, convolutional neural networks (CNNs) were applied\nto grid-based traffic data to capture spatial dependencies,\nand recurrent neural networks (RNNs) were used to learn\ntemporal dynamics (Zhang, Zheng, and Qi 2017; Yao et al.\n2018). Later, graph neural networks (GNNs) were shown to\nbe more suited to model the underlying graph structure of\ntraffic data (Li et al. 2018; Yu, Yin, and Zhu 2018), and thus\nGNN-based methods have been widely explored in traffic\nprediction (Wu et al. 2019; Song et al. 2020; Wu et al. 2020;\nBai et al. 2020; Chen et al. 2020; Ye et al. 2021; Li and Zhu\n2021; Fang et al. 2021; Choi et al. 2022).\nDespite the effectiveness, GNN-based models still have\nthree major limitations for traffic prediction. Firstly, the spa-\ntial dependencies between locations in a traffic system are\nhighly dynamic instead of being static, which are time-\nvarying as they are affected by travel patterns and unex-\npected events. For example, as shown in Figure 1(b), the\ncorrelation between nodesA and B becomes stronger during\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n4365\nthe morning peak and weaker during other periods. While,\nexisting methods model spatial dependencies mainly in a\nstatic manner (either predefined or self-learned), which lim-\nits the ability to learn dynamic urban traffic patterns. Sec-\nondly, due to the functional division of the city, two distant\nlocations, such as nodes A and C in Figure 1(c), may reflect\nsimilar traffic patterns, implying that the spatial dependen-\ncies between locations are long-range. Existing methods are\noften designed locally and unable to capture long-range de-\npendencies. For example, GNN-based models suffer from\nover-smoothing, making it difficult to capture long-range\nspatial dependencies. Thirdly, the effect of time delay might\noccur in the spatial information propagation between loca-\ntions in a traffic system. For example, when a traffic accident\noccurs in one location, it will take several minutes (a delay)\nto affect the traffic condition in neighboring locations, such\nas nodes D and E in Figure 1(d). However, such a feature\nhas been ignored in the immediate message passing mecha-\nnism of typical GNN-based models.\nTo address the above issues, in this paper, we propose a\nP\nropagation Delay-aware dynamic long-range transFormer\nmodel, namely PDFormer, for traffic flow prediction. As the\ncore technical contribution, we design a novel spatial self-\nattention module to capture the dynamic spatial dependen-\ncies. This module incorporates local geographic neighbor-\nhood and global semantic neighborhood information into the\nself-attention interaction via different graph masking meth-\nods, which can simultaneously capture the short- and long-\nrange spatial dependencies in traffic data. Based on this\nmodule, we further design a delay-aware feature transforma-\ntion module to integrate historical traffic patterns into spa-\ntial self-attention and explicitly model the time delay of spa-\ntial information propagation. Finally, we adopt the temporal\nself-attention module to identify the dynamic temporal pat-\nterns in traffic data. In summary, the main contributions of\nthis paper are summarized as follows:\n• We propose the PDFormer model based on the spatial-\ntemporal self-attention mechanism for accurate traffic\nflow prediction. Our approach fully addresses the issues\ncaused by the complex characteristics from traffic data,\nnamely dynamic, long-range, and time-delay.\n• We design a spatial self-attention module that models\nboth local geographic neighborhood and global semantic\nneighborhood via different graph masking methods and\nfurther design a traffic delay-aware feature transforma-\ntion module that can explicitly model the time delay in\nspatial information propagation.\n• We conduct both multi-step and single-step traffic flow\nprediction experiments on six real-world public datasets.\nThe results show that our model significantly outper-\nforms the state-of-the-art models and exhibits compet-\nitive computational efficiency. Moreover, the visualiza-\ntion experiments show that our approach is highly inter-\npretable via the learned spatial-temporal attention.\nPreliminaries\nIn this section, we introduce some notations and formalize\nthe traffic flow prediction problem.\nFigure 2: The Overall Framework of PDFormer.\nNotations and Definitions\nDefinition 1 (Road Network). We represent the Road Net-\nwork as a graph G = (V, E, A), where V = {v1, ··· , vN }\nis a set of N nodes (|V | = N), E ⊆ V × Vis a set of\nedges, and A is the adjacency matrix of networkG. Here, N\ndenotes the number of nodes in the graph.\nDefinition 2 (Traffic Flow Tensor). We use Xt ∈ RN×C\nto denote the traffic flow at time t of N nodes in the road\nnetwork, where C is the dimension of the traffic flow. For\nexample, C = 2when the data includes inflow and outflow.\nWe use X = (X1, X2, ··· , XT ) ∈ RT×N×C to denote the\ntraffic flow tensor of all nodes at total T time slices.\nProblem Formalization\nTraffic flow prediction aims to predict the traffic flow of a\ntraffic system in the future time given the historical observa-\ntions. Formally, given the traffic flow tensor X observed on\na traffic system, our goal is to learn a mapping function f\nfrom the previous T steps’ flow observation value to predict\nfuture T′ steps’ traffic flow,\n[X(t−T+1), ··· , Xt; G]\nf\n−→[X(t+1), ··· , X(t+T′)]. (1)\nMethods\nFigure 2 shows the framework of our proposed PDFormer,\nwhich consists of a data embedding layer, stackedL spatial-\ntemporal encoder layers, and an output layer. We describe\neach module below in detail.\nData Embedding Layer\nThe data embedding layer converts the input into a high-\ndimensional representation. First, the raw input X is trans-\nformed into Xdata ∈ RT×N×d through a fully connected\nlayer, d is the embedding dimension. Then, we further de-\nsign a spatial-temporal embedding mechanism to incorpo-\nrate the necessary knowledge into the model, including\n4366\nthe spatial graph Laplacian embedding to encode the road\nnetwork structure and the temporal periodic embedding to\nmodel the periodicity of traffic flow.\nTo represent the structure of the road network, we use\nthe graph Laplacian eigenvectors (Belkin and Niyogi 2003),\nwhich better describe the distance between nodes on the\ngraph. First, we obtain the normalized Laplacian matrix by\n∆ = I−D−1/2AD−1/2, where A is the adjacency matrix,\nD is the degree matrix, and I is the identity matrix. Then,\nwe perform the eigenvalue decomposition ∆ = U⊤ΛU\nto obtain the eigenvalue matrix Λ and the eigenvector ma-\ntrix U. We use a linear projection on the k smallest non-\ntrivial eigenvectors to generate the spatial graph Laplacian\nembedding Xspe ∈ RN×d. Laplacian eigenvectors embed\nthe graph in Euclidean space and preserve the global graph\nstructure information (Dwivedi et al. 2020).\nIn addition, urban traffic flow, influenced by people’s\ntravel patterns and lifestyle, has an obvious periodicity, such\nas morning and evening peak hours. Therefore, we introduce\ntwo embeddings to cover the weekly and daily periodicity,\nrespectively, denoted as tw(t), td(t) ∈ Rd. Here w(t) and\nd(t) are functions that transform time t into the week index\n(1 to 7) and minute index (1 to 1440), respectively. The tem-\nporal periodic embeddings Xw, Xd ∈ RT×d are obtained\nby concatenating the embeddings of all T time slices.\nFollowing the original Transformer (Vaswani et al. 2017),\nwe also employ a temporal position encodingXtpe ∈ RT×d\nto introduce position information of the input sequence.\nFinally, we get the output of the data embedding layer by\nsimply summing the above embedding vectors as:\nXemb = Xdata + Xspe + Xw + Xd + Xtpe. (2)\nXemb will be fed into the following spatial-temporal en-\ncoders, and we use X to replace Xemb for convenience.\nSpatial-Temporal Encoder Layer\nWe design a spatial-temporal encoder layer based on the\nself-attention mechanism to model the complex and dy-\nnamic spatial-temporal dependencies. The core of the en-\ncoder layer includes three components. The first is a spa-\ntial self-attention module consisting of a geographic spatial\nself-attention module and a semantic spatial self-attention\nmodule to capture the short-range and long-range dynamic\nspatial dependencies simultaneously. The second is a delay-\naware feature transformation module that extends the geo-\ngraphic spatial self-attention module to explicitly model the\ntime delay in spatial information propagation. Moreover, the\nthird is a temporal self-attention module that captures the\ndynamic and long-range temporal patterns.\nTo formulate self-attention operations, we use the follow-\ning slice notation. For a tensor X ∈ RT×N×D, the t slice is\nthe matrix Xt:: ∈ RN×D and the n slice is X:n: ∈ RT×D.\nSpatial Self-Attention (SSA ). We design a Spatial Self-\nAttention module to capture dynamic spatial dependencies\nin traffic data. Formally, at time t, we first obtain the query,\nkey, and value matrices of self-attention operations as:\nQ(S)\nt = Xt::WS\nQ, K(S)\nt = Xt::WS\nK, V (S)\nt = Xt::WS\nV , (3)\nwhere WS\nQ, WS\nK, WS\nV ∈ Rd×d′\nare learnable parameters\nand d′ is the dimension of the query, key, and value matrix\nin this work. Then, we apply self-attention operations in the\nspatial dimension to model the interactions between nodes\nand obtain the spatial dependencies (attention scores) among\nall nodes at time t as:\nA(S)\nt = (Q(S)\nt )(K(S)\nt )⊤\n√\nd′\n. (4)\nIt can be seen that the spatial dependencies A(S)\nt ∈ RN×N\nbetween nodes are different in different time slices, i.e., dy-\nnamic. Thus, the SSA module can be adapted to capture the\ndynamic spatial dependencies. Finally, we can obtain the\noutput of the spatial self-attention module by multiplying\nthe attention scores with the value matrix as:\nSSA(Q(S)\nt , K(S)\nt , V (S)\nt ) = softmax(A(S)\nt )V (S)\nt . (5)\nFor the simple spatial self-attention in Eq. (5), each node\ninteracts with all nodes, equivalent to treating the spatial\ngraph as a fully connected graph. However, only the interac-\ntion between a few node pairs is essential, including nearby\nnode pairs and node pairs that are far away but have similar\nfunctions. Therefore, we introduce two graph masking ma-\ntrices Mgeo and Msem to simultaneously capture the short-\nrange and long-range spatial dependencies in traffic data.\nFrom the short-range view, we define the binary geo-\ngraphic masking matrix Mgeo, and only if the distance (i.e.,\nhops in the graph) between two nodes is less than a threshold\nλ, the weight is 1, and 0 otherwise. In this way, we can mask\nthe attention of node pairs far away from each other. From\nthe long-range view, we compute the similarity of the his-\ntorical traffic flow between nodes using the Dynamic Time\nWarping (DTW) (Berndt and Clifford 1994) algorithm. We\nselect the K nodes with the highest similarity for each node\nas its semantic neighbors. Then, we construct the binary se-\nmantic masking matrixMsem by setting the weight between\nthe current node and its semantic neighbors to 1 and 0 other-\nwise. In this way, we can find distant node pairs that exhibit\nsimilar traffic patterns due to similar urban functions.\nBased on the two graph masking matrices, we further de-\nsign two spatial self-attention modules, namely,Geographic\nSpatial Self-Attention (GeoSSA) and Semantic Spatial Self-\nAttention (SemSSA), which can be defined as:\nGeoSSA(Q(S)\nt , K(S)\nt , V (S)\nt ) = softmax(A(S)\nt ⊙ Mgeo)V (S)\nt ,\nSemSSA(Q(S)\nt , K(S)\nt , V (S)\nt ) = softmax(A(S)\nt ⊙ Msem)V (S)\nt ,\n(6)\nwhere ⊙ indicates the Hadamard product. In this way, the\nspatial self-attention module simultaneously incorporates\nshort-range geographic neighborhood and long-range se-\nmantic neighborhood information.\nDelay-aware Feature Transformation (DFT ). There ex-\nists a propagation delay in real-world traffic conditions. For\nexample, when a traffic accident occurs in one region, it may\ntake several minutes to affect traffic conditions in neighbor-\ning regions. Therefore, we propose a traffic delay-aware fea-\nture transformation module that captures the propagation de-\nlay from the short-term historical traffic flow of each node.\n4367\nFigure 3: Delay-aware Feature Transformation.\nThen, this module incorporates delay information into the\nkey matrix of the geographic spatial self-attention module to\nexplicitly model the time delay in spatial information propa-\ngation. Since traffic data can have multiple dimensions, such\nas inflow and outflow, here we only present the calculation\nprocess of this module using one dimension as an example.\nFirst, we identify a group of representative short-term\ntraffic patterns from historical traffic data. Specifically, we\nslice the historical traffic data with a sliding window of size\nS and obtain a set of traffic flow series. Then, we perform k-\nShape clustering algorithm (Paparrizos and Gravano 2016)\non these traffic flow series. The k-Shape algorithm is a time\nseries clustering method that preserves the shape of the time\nseries and is invariant to scaling and shifting. We use the\ncentroid pi of each cluster to represent that cluster, where\npi is also a time series of length S. Then, we use the set\nP = {pi|i ∈ [1, ··· , Np]} to represent the clustering re-\nsults, where Np is the total number of clusters. We can re-\ngard P as a set of short-term traffic patterns.\nSimilar traffic patterns may have similar effects on neigh-\nborhood traffic conditions, especially abnormal traffic pat-\nterns such as congestion. Therefore, we compare the histor-\nical traffic flow series for each node with the extracted traf-\nfic pattern set P to fuse the information of similar patterns\ninto the historical flow series representation of each node as\nshown in Figure 3. Specifically, given the S-step historical\ntraffic flow series of node n from time slice (t − S + 1)to t,\ndenoted as x(t−S+1:t),n, we first use the embedding matrix\nWu to obtain a high-dimensional representation ut,n as:\nut,n = x(t−S+1:t),nWu. (7)\nThen, we use another embedding matrix Wm to convert\neach traffic flow series in the traffic pattern setP into a mem-\nory vector as:\nmi = piWm. (8)\nWe compare the historical traffic flow representationut,n of\nnode n with the traffic pattern memory vectormi and obtain\nthe similarity vector as:\nwi = softmax(u⊤\nt,nmi). (9)\nThen, we perform a weighted sum of the traffic pattern setP\naccording to the similarity vector w to obtain the integrated\nhistorical series representation rt,n as:\nrt,n =\nNpX\ni=1\nwi(piWc), (10)\nwhere Wc is a learnable parameter matrix. The integrated\nhistorical series representation rt,n contains the historical\ntraffic flow information from time slice (t − S + 1)to t of\nnode n. Finally, we use the integrated representation of N\nnodes, denoted as Rt, to update K(S)\nt in Eq. (4) as:\n˜K(S)\nt = K(S)\nt + Rt, (11)\nwhere Rt ∈ RN×d′\nis obtained by concatenating all the in-\ntegrated representation rt,n of N nodes and d′ is the dimen-\nsion of the key matrix in this work.\nIn this way, the new key matrix ˜K(S)\nt at time slice t in-\ntegrates the historical traffic flow information of all nodes\nfrom time slice (t − S + 1)to t. When computing the prod-\nuct of the query matrix and the new key matrix to obtain the\nspatial dependencies A(S)\nt at time slicet in Eq. (4), the query\nmatrix can take into account the historical traffic conditions\nof other nodes. This process explicitly models thetime delay\nin spatial information propagation. We do not add this mod-\nule to the semantic spatial self-attention module because the\nshort-term traffic flow of a distant node has little impact on\nthe current node.\nTemporal Self-Attention (TSA). There are dependencies\n(e.g., periodic, trending) between traffic conditions in differ-\nent time slices, and the dependencies vary in different situa-\ntions. Thus, we employ a Temporal Self-Attentionmodule to\ndiscover the dynamic temporal patterns. Formally, for node\nn, we first obtain the query, key, and value matrices as:\nQ(T)\nn = X:n:WT\nQ , K(T)\nn = X:n:WT\nK , V (T)\nn = X:n:WT\nV ,\n(12)\nwhere WT\nQ , WT\nK, WT\nV ∈ Rd×d′\nare learnable parameters.\nThen, we apply self-attention operations in the temporal di-\nmension and obtain the temporal dependencies between all\ntime slices for node n as:\nA(T)\nn = (Q(T)\nn )(K(T)\nn ))⊤\n√\nd′\n. (13)\nIt can be seen that the temporal self-attention can discover\nthe dynamic temporal patterns in traffic data that are differ-\nent for different nodes. Moreover, the temporal self-attention\nhas a global receptive to model the long-range temporal de-\npendencies among all time slices. Finally, we can obtain the\noutput of the temporal self-attention module as:\nTSA(Q(T)\nn , K(T)\nn , V (T)\nn ) = softmax(A(T)\nn )V (T)\nn . (14)\nHeterogeneous Attention Fusion. After defining the three\ntypes of attention mechanisms, we fuse heterogeneous atten-\ntion into a multi-head self-attention block to reduce the com-\nputational complexity of the model. Specifically, the atten-\ntion heads include three types, i.e., geographic (GeoSAH),\nsemantic (SemSAH ), and temporal (TAH) heads, corre-\nsponding to the three types of attention mechanisms, respec-\ntively. The results of these heads are concatenated and pro-\njected to obtain the outputs, allowing the model to integrate\nspatial and temporal information simultaneously. Formally,\nthe spatial-temporal self-attention block is defined as:\nSTAttn =⊕(Zgeo\n1···hgeo , Zsem\n1···hsem , Zt\n1···ht )WO, (15)\n4368\nDatasets #Nodes #Interval Time range\nPeMS04 307 5min 01/01/2018-02/28/2018\nPeMS07 883 5min 05/01/2017-08/31/2017\nPeMS08 170 5min 07/01/2016-08/31-2016\nNYTaxi 75(15x5) 30min 01/01/2014-12/31/2014\nCHBike 270(15x18) 30min 07/01/2020-09/30/2020\nTDrive 1024(32x32) 60min 02/01/2015-06/30/2015\nTable 1: Data Description.\nwhere ⊕ represents concatenation, Zgeo, Zsem, Zt are out-\nput concatenations and hgeo, hsem, ht are the numbers of at-\ntention heads of GeoSSA, SemSSA and TSA, respectively,\nand WO ∈ Rd×d is a learnable projection matrix. In this\nwork, we set the dimension d′ = d/(hgeo + hsem + ht).\nIn addition, we employ a position-wise fully connected\nfeed-forward network on the output of the multi-head self-\nattention block to get the outputs Xo ∈ RT×N×d. We also\nuse layer normalization and residual connection here follow-\ning the original Transformer (Vaswani et al. 2017).\nOutput Layer\nWe use a skip connection, consisting of 1 × 1 convolutions,\nafter each spatial-temporal encoder layer to convert the out-\nputs Xo into a skip dimension Xsk ∈ RT×N×dsk . Here dsk\nis the skip dimension. Then, we obtain the final hidden state\nXhid ∈ RT×N×dsk by summing the outputs of each skip\nconnection layer. To make a multi-step prediction, we di-\nrectly use the output layer to transform the final hidden state\nXhid to the desired dimension as:\nˆX = Conv2(Conv1(Xhid)), (16)\nwhere ˆX ∈ RT′×N×C is T′ steps’ prediction results, Conv1\nand Conv2 are 1 × 1 convolutions. Here we choose the di-\nrect way instead of the recursive manner for multi-step pre-\ndiction considering cumulative errors and model efficiency.\nExperiments\nDatasets\nWe verify the performance of PDFormer on six real-world\npublic traffic datasets, including three graph-based highway\ntraffic datasets, i.e., PeMS04, PeMS07, PeMS08 (Song et al.\n2020), and three grid-based citywide traffic datasets, i.e.,\nNYTaxi (Liu et al. 2021a), CHBike (Wang et al. 2021),\nTDrive (Pan et al. 2019). The graph-based datasets contain\nonly the traffic flow data, and the grid-based datasets contain\ninflow and outflow data. Details are given in Table 1.\nBaselines\nWe compare PDFormer with the following 9 baselines be-\nlonging to two classes. (1) Graph Neural Network-based\nModels: We choose DCRNN (Li et al. 2018), STGCN (Yu,\nYin, and Zhu 2018), GWNET (Wu et al. 2019), MT-\nGNN (Wu et al. 2020), STFGNN (Li and Zhu 2021) and\nSTGNCDE (Choi et al. 2022). (2)Self-attention-based Mod-\nels: We choose STTN (Xu et al. 2020), GMAN (Zheng et al.\n2020) and ASTGNN (Guo et al. 2021).\nExperimental Settings\nDataset Processing. To be consistent with most modern\nmethods, we split the three graph-based datasets into train-\ning, validation, and test sets in a 6:2:2 ratio. In addition, we\nuse the past hour (12 steps) data to predict the traffic flow for\nthe next hour (12 steps), i.e., a multi-step prediction. For the\ngrid-based datasets, the split ratio is 7:1:2, and we use the\ntraffic inflow and outflow of the past six steps to predict the\nnext single-step traffic inflow and outflow. Before training,\nwe use Z-score normalization on all datasets to standardize\nthe inputs.\nModel Settings. All experiments are conducted on a ma-\nchine with the NVIDIA GeForce 3090 GPU and 128GB\nmemory. We implement PDFormer1 with Ubuntu 18.04, Py-\nTorch 1.10.1, and Python 3.9.7. The hidden dimension d is\nsearched over {16, 32, 64, 128} and the depth of encoder\nlayers L is searched over {2, 4, 6, 8}. The optimal model is\ndetermined based on the performance in the validation set.\nWe train our model using AdamW optimizer (Loshchilov\nand Hutter 2017) with a learning rate of 0.001. The batch\nsize is 16, and the training epoch is 200.\nEvaluation Metrics. We use three metrics in the experi-\nments: (1) Mean Absolute Error (MAE), (2) Mean Abso-\nlute Percentage Error (MAPE), and (3) Root Mean Squared\nError (RMSE). Missing values are excluded when calculat-\ning these metrics. When we test the models on grid-based\ndatasets, we filter the samples with flow values below 10,\nconsistent with (Yao et al. 2018). Since the flow of CHBike\nis lower than others, the filter threshold is 5. Besides, we\ntake the average value of inflow and outflow evaluation met-\nrics as the final result for grid-based datasets. We repeated\nall experiments ten times and reported the average results.\nPerformance Comparison\nThe comparison results with baselines on graph-based and\ngrid-based datasets are shown in Table 2. The bold results\nare the best, and the underlined results are the second best.\nBased on this table, we can make the following observa-\ntions. (1) Our PDFormer significantly outperforms all base-\nlines in terms of all metrics over all datasets according to\nStudent’s t-test at level 0.01. Compared to the second best\nmethod, PDFormer achieves an average improvement of\n4.58%, 5.00%, 4.79% for MAE/MAPE/RMSE. (2) Among\nthe GNN-based models, MTGNN and STGNCDE lead to\ncompetitive performance. Compared to these GNN-based\nmodels, whose message passing is immediate, PDFormer\nachieves better performance because it considers the time\ndelay in spatial information propagation. (3) As for the\nself-attention-based models, ASTGNN is the best baseline,\nwhich combines GCN and the self-attention module to ag-\ngregate neighbor information. Compared with ASTGNN,\nPDFormer simultaneously captures short- and long-range\nspatial dependencies via two masking matrices and achieves\ngood performance.\n1https://github.com/BUAABIGSCity/PDFormer\n4369\nModels DCRNN STGCN GWNET MTGNN STFGNN STGNCDE STTN GMAN ASTGNN PDFormer\nPeMS04\nMAE 22.737 21.758 19.358 19.076 19.830 19.211 19.478 19.139 18.601 18.321\nMAPE 14.751 13.874 13.301 12.961 13.021 12.772 13.631 13.192 12.630 12.103\nRMSE 36.575 34.769 31.719 31.564 31.870 31.088 31.910 31.601 31.028 29.965\nPeMS07\nMAE 23.634 22.898 21.221 20.824 22.072 20.620 21.344 20.967 20.616 19.832\nMAPE 12.281 11.983 9.075 9.032 9.212 8.864 9.932 9.052 8.861 8.529\nRMSE 36.514 35.440 34.117 34.087 35.805 34.036 34.588 34.097 34.017 32.870\nPeMS08\nMAE 18.185 17.838 15.063 15.396 16.636 15.455 15.482 15.307 14.974 13.583\nMAPE 11.235 11.211 9.514 10.170 10.547 9.921 10.341 10.134 9.489 9.046\nRMSE 28.176 27.122 24.855 24.934 26.206 24.813 24.965 24.915 24.710 23.505\nNYTaxi\nMAE 13.625 13.462 13.296 13.233 14.257 13.279 13.366 13.270 12.978 12.364\nMAPE 14.349 14.156 13.941 13.818 14.727 13.926 13.984 13.893 13.647 12.781\nRMSE 21.971 21.911 21.708 21.613 23.869 21.675 21.834 21.661 21.189 20.176\nTDriv\ne MAE 21.938 21.143 19.553 18.955 22.510 19.289 20.513 19.104 18.794 17.788\nMAPE 17.566 17.261 16.560 16.409 18.540 16.504 16.659 16.449 15.843 14.680\nRMSE 38.411 37.836 36.179 35.689 40.554 36.118 37.143 36.053 33.934 31.553\nCHBike\nMAE 4.224 4.180 4.126 4.099 4.249 4.109 4.139 4.102 4.024 3.893\nMAPE 31.043 31.003 30.922 30.855 32.272 30.873 30.956 30.906 30.874 30.064\nRMSE 5.908 5.866 5.806 5.738 5.904 5.796 5.827 5.792 5.713 5.480\nTable 2: Performance on Graph-based and Grid-based Datasets. (MAPE is in %.)\nAblation Study\nTo further investigate the effectiveness of different parts in\nPDFormer, we compare PDFormer with the following vari-\nants. (1) w/ GCN: this variant replaces spatial self-attention\n(SSA) with Graph Convolutional Network (GCN) (Kipf\nand Welling 2016), which cannot capture the dynamic and\nlong-range spatial dependencies. (2) w/o Mask : this vari-\nant removes two masking matrices Mgeo and Msem, which\nmeans each node attends to all nodes. (3) w/o GeoSAH: this\nvariant removes GeoSAH. (4) w/o SemSAH: this variant re-\nmoves SemSAH. (5) w/o Delay: this variant removes the\ndelay-aware feature transformation module, which accounts\nfor the spatial information propagation delay.\nFigure 4 shows the comparison of these variants on the\nPeMS04 and NYTaxi datasets. For the NYTaxi dataset, only\nthe results for inflow are reported since the results for out-\nflow are similar. Based on the results, we can conclude the\nfollowing: (1) The results show the superiority of SSA over\nGCN in capturing dynamic and long-range spatial depen-\ndencies. (2) PDFormer leads to a large performance im-\nprovement over w/o Mask, highlighting the value of using\nthe mask matrices to identify the significant node pairs. In\naddition, w/o SemSAH and w/o GeoSAH perform worse than\nPDFormer, indicating that both local and global spatial de-\npendencies are significant for traffic prediction. (3) w/o De-\nlay performs worse than PDFormer because this variant ig-\nnores the spatial propagation delay between nodes but con-\nsiders the spatial message passing as immediate.\nCase Study\nIn this section, we analyze the dynamic spatial-temporal at-\ntention weight map learned by the spatial-temporal encoder\nof PDFormer to improve its interpretability and demonstrate\nthe effectiveness of focusing on short- and long-range spatial\ndependencies simultaneously.\n17.5\n18.5\n19.5\n20.5\n21.5\n(a) MAE on PeMS04 11.5\n12.5\n13.5\n14.5\n(b) MAPE on PeMS04\nw/ GCN\nw/o Mask\nw/o GeoSAH\nw/o SemSAH\nw/o Delay\nPDFormer\n29\n30\n31\n32\n33\n34\n(c) RMSE on PeMS04\n13.0\n13.4\n13.8\n14.2\n(d) MAE on NYTaxi\n12.5\n13.5\n14.5\n(e) MAPE on NYTaxi 21.5\n22.5\n23.5\n(f) RMSE on NYTaxi\nFigure 4: Ablation Study on PeMS04 and NYTaxi inflow.\nWe compare and visualize the attention map in two cases,\ni.e., with or without the two spatial mask matricesMgeo and\nMsem. Here, for simplicity, we merge the attention map of\nGeoSAH and SemSAH. As shown in Figure 5(a),(d), with-\nout the mask matrices, the model focuses on the major ur-\nban ring roads (or highways) with high traffic volume, or\nthe attention distribution is diffuse, and almost the entire city\nshares the model’s attention. However, low-traffic locations\nshould focus on locations with similar patterns rather than\nhot locations. Moreover, locations that are too far away have\nlittle impact on the current location. The model performance\nwill weaken if it focuses on all locations diffusely. Instead,\nwhen Mgeo and Msem are introduced, attention focuses on\nsurrounding locations and distant similar-pattern locations\nas shown in Figure 5(b),(e).\nLet us take Region 592 in Figure 5(b) as an example.\nHighway S12 passes through this region, so the traffic vol-\nume is always high. In addition to the regions located up-\nstream and downstream of the highway, region 592 also fo-\n4370\n(a) Rg592 Before Mask\n (b) Rg592 After Mask\n(d) Rg252 Before Mask\n (e) Rg252 After Mask 0 24\n500\n1000\n1500\n(c) 24h Average Flow\n648\n753\n592\n0 24\n100\n200\n300\n(f) 24h Average Flow\n370\n403\n842\n252\n(g) Region 592\n (h) Region 648\n (i) Region 753\nFigure 5: Case Study of Attention Map.\ncuses on regions 648 and 753. From Figure 5(c), we can see\nthat these two regions have similar historical traffic volumes\nas 592. Besides, from Figure 5(h)(i), these two regions are\nlocated near the Guomao Interchange and Beijing Second\nRing Road, respectively, which are similar to 592 in their\ncities function as major traffic hubs. In another case, region\n252 has low traffic volume, but we can observe similar pat-\nterns from regions 370, 403 and 842 that region 252 focused\non, i.e., similar functional and historical traffic variations.\nThis case study shows that after introducing the spatial\nmask matrices, PDFormer not only considers the short-range\nspatial dependencies but also identifies the global functional\narea to capture the long-range spatial dependencies. The\nabove ablation experiments also quantitatively show that the\nmodel performance drops sharply after removing the mask\nmatrices, which supports the idea presented here.\nModel Efficiency Study\nDue to the better performance of the attention-based mod-\nels, we compare the computational cost of PDFormer with\nother self-attention-based baselines on the PeMS04 and\nthe NYTaxi datasets. Table 3 reports the average train-\ning and inference time per epoch. We find that PDFormer\nachieves competitive computational efficiency in both short-\nand long-term traffic prediction. Compared to the best per-\nforming baseline ASTGNN on PeMS04, PDFormer reduces\nthe training and inference time of over 35% and 80%, re-\nspectively. GMAN and ASTGNN retain a time-consuming\nencoder-decoder structure, which is replaced by a forward\nprocedure in STTN and PDFormer.\nRelated Work\nDeep Learning for Traffic Prediction\nIn recent years, more and more researchers have employed\ndeep learning models to solve traffic prediction problems.\nEarly on, convolutional neural networks (CNNs) were ap-\nplied to grid-based traffic data to capture spatial dependen-\ncies in the data (Wang et al. 2016; Zhang, Zheng, and Qi\n2017; Yao et al. 2018; Lin et al. 2020a). Later, thanks to\nDataset PeMS04 NYTaxi\nModel Train Infer Train Infer\nGMAN 501.58 38.84 130.67 4.26\nASTGNN 208.72 62.02 119.09 4.60\nPDFormer 133.87 8.12 85.31 2.73\nSTTN 100.40 12.60 68.04 2.65\nTable 3: Training and inference time per epoch comparison\nbetween self-attention-based models. (Unit: seconds)\nthe powerful ability to model graph data, graph neural net-\nworks (GNNs) were widely used for traffic prediction (Li\net al. 2018; Yu, Yin, and Zhu 2018; Wu et al. 2019; Ji et al.\n2020; Chen et al. 2020; Ye et al. 2021; Wu et al. 2020; Zhang\net al. 2021; Song et al. 2020; Li and Zhu 2021; Oreshkin\net al. 2021; Han et al. 2021; Ji et al. 2022; Fang et al. 2021;\nChoi et al. 2022; Liu et al. 2022). Recently, the attention\nmechanism has become increasingly popular due to its ef-\nfectiveness in modeling the dynamic dependencies in traffic\ndata (Guo et al. 2019; Wang et al. 2020; Lin et al. 2020b; Yan\nand Ma 2021; Ye et al. 2022). Unlike these work, our pro-\nposed PDFormer not only considers the dynamic and long-\nrange spatial dependencies through a self-attention mecha-\nnism but also incorporates the time delay in spatial propaga-\ntion through a delay-aware feature transformation layer.\nTransformer\nTransformer (Vaswani et al. 2017) is a network architecture\nbased entirely on self-attention mechanisms. Transformer\nhas been proven effective in multiple natural language pro-\ncessing (NLP) tasks. In addition, large-scale Transformer-\nbased pre-trained models such as BERT (Devlin et al. 2018)\nhave achieved great success in the NLP community. Re-\ncently, Vision Transformers have attracted the attention of\nresearchers, and many variants have shown promising re-\nsults on computer vision tasks (Liu et al. 2021b). In addi-\ntion, the Transformer architecture performs well in repre-\nsentation learning, which has been demonstrated in recent\nstudies (Dwivedi and Bresson 2020; Ren et al. 2021; Ren,\nWang, and Zhao 2022; Jiang et al. 2023).\nConclusion\nIn this work, we proposed a novel PDFormer model with\nspatial-temporal self-attention for traffic flow prediction.\nSpecifically, we developed a spatial self-attention module\nthat captures the dynamic and long-range spatial dependen-\ncies and a temporal self-attention module that discovers the\ndynamic temporal patterns in the traffic data. We further de-\nsigned a delay-aware feature transformation module to ex-\nplicitly model the time delay in spatial information prop-\nagation. We conducted extensive experiments on six real-\nworld datasets to demonstrate the superiority of our pro-\nposed model and visualized the learned attention map to\nmake the model interpretable. As future work, we will apply\nPDFormer to other spatial-temporal prediction tasks, such as\nwind power forecasting (Jiang, Han, and Wang 2023). In ad-\ndition, we will explore the pre-training techniques in traffic\nprediction to solve the problem of insufficient data.\n4371\nAcknowledgments\nThis work was supported by the National Key R&D Pro-\ngram of China (Grant No. 2019YFB2102100). Prof. Wang’s\nwork was supported by the National Natural Science Foun-\ndation of China (No.72222022, 82161148011, 72171013),\nthe Fundamental Research Funds for the Central Universi-\nties (YWF-22-L-838) and the DiDi Gaia Collaborative Re-\nsearch Funds. Prof. Zhao’s work was supported by the Na-\ntional Natural Science Foundation of China (No. 62222215).\nReferences\nBai, L.; Yao, L.; Li, C.; Wang, X.; and Wang, C. 2020.\nAdaptive Graph Convolutional Recurrent Network for Traf-\nfic Forecasting. In NeurIPS.\nBelkin, M.; and Niyogi, P. 2003. Laplacian Eigenmaps for\nDimensionality Reduction and Data Representation. Neural\nComput., 15(6): 1373–1396.\nBerndt, D. J.; and Clifford, J. 1994. Using Dynamic Time\nWarping to Find Patterns in Time Series. InKDD Workshop,\n359–370. AAAI Press.\nChen, W.; Chen, L.; Xie, Y .; Cao, W.; Gao, Y .; and Feng, X.\n2020. Multi-Range Attentive Bicomponent Graph Convolu-\ntional Network for Traffic Forecasting. InAAAI, 3529–3536.\nAAAI Press.\nChoi, J.; Choi, H.; Hwang, J.; and Park, N. 2022. Graph\nNeural Controlled Differential Equations for Traffic Fore-\ncasting. In AAAI, 6367–6374. AAAI Press.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2018.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. CoRR, abs/1810.04805.\nDwivedi, V . P.; and Bresson, X. 2020. A Generalization of\nTransformer Networks to Graphs. CoRR, abs/2012.09699.\nDwivedi, V . P.; Joshi, C. K.; Laurent, T.; Bengio, Y .; and\nBresson, X. 2020. Benchmarking Graph Neural Networks.\nCoRR, abs/2003.00982.\nFang, Z.; Long, Q.; Song, G.; and Xie, K. 2021. Spatial-\nTemporal Graph ODE Networks for Traffic Flow Forecast-\ning. In KDD, 364–373. ACM.\nGuo, S.; Lin, Y .; Feng, N.; Song, C.; and Wan, H. 2019. At-\ntention Based Spatial-Temporal Graph Convolutional Net-\nworks for Traffic Flow Forecasting. In AAAI, 922–929.\nAAAI Press.\nGuo, S.; Lin, Y .; Wan, H.; Li, X.; and Cong, G. 2021. Learn-\ning Dynamics and Heterogeneity of Spatial-Temporal Graph\nData for Traffic Forecasting.IEEE Trans. Knowl. Data Eng.,\n1–1.\nHan, L.; Du, B.; Sun, L.; Fu, Y .; Lv, Y .; and Xiong, H. 2021.\nDynamic and Multi-faceted Spatio-temporal Deep Learning\nfor Traffic Speed Forecasting. In KDD, 547–555. ACM.\nJi, J.; Wang, J.; Jiang, Z.; Jiang, J.; and Zhang, H. 2022.\nSTDEN: Towards Physics-Guided Neural Networks for\nTraffic Flow Prediction. In AAAI, 4048–4056. AAAI Press.\nJi, J.; Wang, J.; Jiang, Z.; Ma, J.; and Zhang, H. 2020. In-\nterpretable Spatiotemporal Deep Learning Model for Traffic\nFlow Prediction based on Potential Energy Fields. InICDM,\n1076–1081. IEEE.\nJiang, J.; Han, C.; and Wang, J. 2023. BUAA\nBIGSCity:\nSpatial-Temporal Graph Neural Network for Wind Power\nForecasting in Baidu KDD CUP 2022. arXiv preprint\narXiv:2302.11159.\nJiang, J.; Pan, D.; Ren, H.; Jiang, X.; Li, C.; and Wang,\nJ. 2023. Self-supervised Trajectory Representation Learn-\ning with Temporal Regularities and Travel Semantics. In\n2023 IEEE 39th international conference on data engineer-\ning (ICDE). IEEE.\nKipf, T. N.; and Welling, M. 2016. Semi-Supervised Clas-\nsification with Graph Convolutional Networks. CoRR,\nabs/1609.02907.\nLi, M.; and Zhu, Z. 2021. Spatial-Temporal Fusion Graph\nNeural Networks for Traffic Flow Forecasting. In AAAI,\n4189–4196. AAAI Press.\nLi, Y .; Yu, R.; Shahabi, C.; and Liu, Y . 2018. Diffusion Con-\nvolutional Recurrent Neural Network: Data-Driven Traffic\nForecasting. In ICLR (Poster). OpenReview.net.\nLin, H.; Bai, R.; Jia, W.; Yang, X.; and You, Y . 2020a.\nPreserving Dynamic Attention for Long-Term Spatial-\nTemporal Prediction. In KDD, 36–46. ACM.\nLin, Z.; Li, M.; Zheng, Z.; Cheng, Y .; and Yuan, C. 2020b.\nSelf-Attention ConvLSTM for Spatiotemporal Prediction.\nIn AAAI, 11531–11538. AAAI Press.\nLiu, D.; Wang, J.; Shang, S.; and Han, P. 2022. MSDR:\nMulti-Step Dependency Relation Networks for Spatial Tem-\nporal Forecasting. In KDD, 1042–1050. ACM.\nLiu, L.; Zhen, J.; Li, G.; Zhan, G.; He, Z.; Du, B.; and Lin,\nL. 2021a. Dynamic Spatial-Temporal Representation Learn-\ning for Traffic Flow Prediction. IEEE Trans. Intell. Transp.\nSyst., 22(11): 7169–7183.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.;\nLin, S.; and Guo, B. 2021b. Swin Transformer: Hierar-\nchical Vision Transformer using Shifted Windows. CoRR,\nabs/2103.14030.\nLoshchilov, I.; and Hutter, F. 2017. Fixing Weight Decay\nRegularization in Adam. CoRR, abs/1711.05101.\nOreshkin, B. N.; Amini, A.; Coyle, L.; and Coates, M. 2021.\nFC-GAGA: Fully Connected Gated Graph Architecture for\nSpatio-Temporal Traffic Forecasting. In AAAI, 9233–9241.\nAAAI Press.\nPan, Z.; Liang, Y .; Wang, W.; Yu, Y .; Zheng, Y .; and Zhang,\nJ. 2019. Urban Traffic Prediction from Spatio-Temporal\nData Using Deep Meta Learning. In KDD, 1720–1730.\nACM.\nPaparrizos, J.; and Gravano, L. 2016. k-Shape: Efficient and\nAccurate Clustering of Time Series. SIGMOD Rec., 45(1):\n69–76.\nRen, H.; Wang, J.; and Zhao, W. X. 2022. Generative Ad-\nversarial Networks Enhanced Pre-training for Insufficient\nElectronic Health Records Modeling. In KDD, 3810–3818.\nACM.\nRen, H.; Wang, J.; Zhao, W. X.; and Wu, N. 2021. RAPT:\nPre-training of Time-Aware Transformer for Learning Ro-\nbust Healthcare Representation. InKDD, 3503–3511. ACM.\n4372\nSong, C.; Lin, Y .; Guo, S.; and Wan, H. 2020. Spatial-\nTemporal Synchronous Graph Convolutional Networks: A\nNew Framework for Spatial-Temporal Network Data Fore-\ncasting. In AAAI, 914–921. AAAI Press.\nTedjopurnomo, D. A.; Bao, Z.; Zheng, B.; Choudhury, F. M.;\nand Qin, A. K. 2022. A Survey on Modern Deep Neural\nNetwork for Traffic Prediction: Trends, Methods and Chal-\nlenges. IEEE Trans. Knowl. Data Eng., 34(4): 1544–1561.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In NIPS, 5998–6008.\nWang, J.; Gu, Q.; Wu, J.; Liu, G.; and Xiong, Z. 2016. Traf-\nfic Speed Prediction and Congestion Source Exploration: A\nDeep Learning Method. In ICDM, 499–508. IEEE Com-\nputer Society.\nWang, J.; Jiang, J.; Jiang, W.; Li, C.; and Zhao, W. X. 2021.\nLibCity: An Open Library for Traffic Prediction. InSIGSPA-\nTIAL/GIS, 145–148. ACM.\nWang, X.; Ma, Y .; Wang, Y .; Jin, W.; Wang, X.; Tang, J.;\nJia, C.; and Yu, J. 2020. Traffic Flow Prediction via Spatial\nTemporal Graph Neural Network. In WWW, 1082–1092.\nACM / IW3C2.\nWu, Z.; Pan, S.; Long, G.; Jiang, J.; Chang, X.; and Zhang,\nC. 2020. Connecting the Dots: Multivariate Time Series\nForecasting with Graph Neural Networks. In KDD, 753–\n763. ACM.\nWu, Z.; Pan, S.; Long, G.; Jiang, J.; and Zhang, C. 2019.\nGraph WaveNet for Deep Spatial-Temporal Graph Model-\ning. In IJCAI, 1907–1913. ijcai.org.\nXu, M.; Dai, W.; Liu, C.; Gao, X.; Lin, W.; Qi, G.; and\nXiong, H. 2020. Spatial-Temporal Transformer Networks\nfor Traffic Flow Forecasting. CoRR, abs/2001.02908.\nYan, H.; and Ma, X. 2021. Learning dynamic and hierarchi-\ncal traffic spatiotemporal features with Transformer. CoRR,\nabs/2104.05163.\nYao, H.; Wu, F.; Ke, J.; Tang, X.; Jia, Y .; Lu, S.; Gong, P.;\nYe, J.; and Li, Z. 2018. Deep Multi-View Spatial-Temporal\nNetwork for Taxi Demand Prediction. In AAAI, 2588–2595.\nAAAI Press.\nYe, J.; Sun, L.; Du, B.; Fu, Y .; and Xiong, H. 2021. Coupled\nLayer-wise Graph Convolution for Transportation Demand\nPrediction. In AAAI, 4617–4625. AAAI Press.\nYe, X.; Fang, S.; Sun, F.; Zhang, C.; and Xiang, S. 2022.\nMeta Graph Transformer: A Novel Framework for Spatial-\nTemporal Traffic Prediction. Neurocomputing, 491: 544–\n563.\nYin, C.; Xiong, Z.; Chen, H.; Wang, J.; Cooper, D.; and\nDavid, B. 2015. A literature survey on smart cities. Sci.\nChina Inf. Sci., 58(10): 1–18.\nYin, X.; Wu, G.; Wei, J.; Shen, Y .; Qi, H.; and Yin, B. 2022.\nDeep Learning on Traffic Prediction: Methods, Analysis,\nand Future Directions. IEEE Trans. Intell. Transp. Syst.,\n23(6): 4927–4943.\nYu, B.; Yin, H.; and Zhu, Z. 2018. Spatio-Temporal Graph\nConvolutional Networks: A Deep Learning Framework for\nTraffic Forecasting. In IJCAI, 3634–3640. ijcai.org.\nZhang, J.; Zheng, Y .; and Qi, D. 2017. Deep Spatio-\nTemporal Residual Networks for Citywide Crowd Flows\nPrediction. In AAAI, 1655–1661. AAAI Press.\nZhang, X.; Huang, C.; Xu, Y .; Xia, L.; Dai, P.; Bo, L.;\nZhang, J.; and Zheng, Y . 2021. Traffic Flow Forecasting\nwith Spatial-Temporal Graph Diffusion Network. In AAAI,\n15008–15015. AAAI Press.\nZheng, C.; Fan, X.; Wang, C.; and Qi, J. 2020. GMAN: A\nGraph Multi-Attention Network for Traffic Prediction. In\nAAAI, 1234–1241. AAAI Press.\n4373",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7617195844650269
    },
    {
      "name": "Data mining",
      "score": 0.5207452774047852
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.48034942150115967
    },
    {
      "name": "Traffic flow (computer networking)",
      "score": 0.4330317974090576
    },
    {
      "name": "Spatial analysis",
      "score": 0.42264193296432495
    },
    {
      "name": "Engineering",
      "score": 0.1252884566783905
    },
    {
      "name": "Computer network",
      "score": 0.0966205894947052
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    },
    {
      "name": "Remote sensing",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ]
}