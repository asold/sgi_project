{
  "title": "OAG-BERT: Towards A Unified Backbone Language Model For Academic Knowledge Services",
  "url": "https://openalex.org/W3134634493",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5075936732",
      "name": "Xiao Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5110195053",
      "name": "Da Yin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100635711",
      "name": "Xingjian Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5011672302",
      "name": "Kai Su",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101675207",
      "name": "Kan Wu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5082599714",
      "name": "Hongxia Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5044791875",
      "name": "Jie Tang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2982925423",
    "https://openalex.org/W3010021337",
    "https://openalex.org/W2801930304",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2933655602",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W3133587129",
    "https://openalex.org/W3101016419",
    "https://openalex.org/W2966694634",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2911926823",
    "https://openalex.org/W2163107094",
    "https://openalex.org/W1932742904",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2147994374",
    "https://openalex.org/W2743104969",
    "https://openalex.org/W2889399843",
    "https://openalex.org/W3141023492",
    "https://openalex.org/W3091617571",
    "https://openalex.org/W2807953821",
    "https://openalex.org/W2914796493",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2164948578",
    "https://openalex.org/W2766801974",
    "https://openalex.org/W3106224367",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W2970217403",
    "https://openalex.org/W2964179635",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3034238904"
  ],
  "abstract": "Academic knowledge services have substantially facilitated the development of the science enterprise by providing a plenitude of efficient research tools. However, many applications highly depend on ad-hoc models and expensive human labeling to understand scientific contents, hindering deployments into real products. To build a unified backbone language model for different knowledge-intensive academic applications, we pre-train an academic language model OAG-BERT that integrates both the heterogeneous entity knowledge and scientific corpora in the Open Academic Graph (OAG) -- the largest public academic graph to date. In OAG-BERT, we develop strategies for pre-training text and entity data along with zero-shot inference techniques. In OAG-BERT, we develop strategies for pre-training text and entity data along with zero-shot inference techniques. Its zero-shot capability furthers the path to mitigate the need of expensive annotations. OAG-BERT has been deployed for real-world applications, such as the reviewer recommendation function for National Nature Science Foundation of China (NSFC) -- one of the largest funding agencies in China -- and paper tagging in AMiner. All codes and pre-trained models are available via the CogDL toolkit.",
  "full_text": "OAG-BERT: Towards A Unified Backbone Language Model For\nAcademic Knowledge Services\nXiao Liu‚àó\nTsinghua University\nliuxiao21@mails.tsinghua.edu.cn\nDa Yin‚àó\nTsinghua University\nyd18@mails.tsinghua.edu.cn\nJingnan Zheng\nNational University of Singapore\ne0718957@u.nus.edu\nXingjian Zhang\nTsinghua University\nzhangxj18@mails.tsinghua.edu.cn\nPeng Zhang\nZhipu AI\nzpjumper@gmail.com\nHongxia Yang\nDAMO Academy, Alibaba Group\nyang.yhx@alibaba-inc.com\nYuxiao Dong\nTsinghua University\nyuxiaod@tsinghua.edu.cn\nJie Tang‚Ä†\nTsinghua University\njietang@tsinghua.edu.cn\nABSTRACT\nAcademic knowledge services have substantially facilitated the de-\nvelopment of the science enterprise by providing a plenitude of\nefficient research tools. However, many applications highly depend\non ad-hoc models and expensive human labeling to understand\nscientific contents, hindering deployments into real products. To\nbuild a unified backbone language model for different knowledge-\nintensive academic applications, we pre-train an academic language\nmodel OAG-BERT that integrates both the heterogeneous entity\nknowledge and scientific corpora in the Open Academic Graph\n(OAG)‚Äîthe largest public academic graph to date. In OAG-BERT,\nwe develop strategies for pre-training text and entity data along\nwith zero-shot inference techniques. OAG-BERT achieves outper-\nformance over baselines on nine academic tasks including two demo\napplications, demonstrating its potential to serve as one foundation\nmodel for academic knowledge services. Its zero-shot capability\nfurthers the path to mitigate the need of expensive annotations.\nOAG-BERT has been deployed for real-world applications, such as\nthe reviewer recommendation function for National Nature Science\nFoundation of China (NSFC)‚Äîone of the largest funding agencies in\nChina‚Äîand paper tagging in AMiner (https://www.aminer.cn). All\ncodes and pre-trained models are available via the CogDL toolkit1.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíLanguage models ; Data mining ;\n‚Ä¢ Computing methodologies ‚ÜíKnowledge representation\nand reasoning; Supervised learning by classification .\n‚àóThe authors contributed equally to this research.\n‚Ä†Jie Tang is the corresponding author.\n1https://github.com/thudm/oag-bert.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA\n¬© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9385-0/22/08. . . $15.00\nhttps://doi.org/10.1145/3534678.3539210\nKEYWORDS\nPre-Training; Language Model; Heterogeneous Knowledge Graph\nACM Reference Format:\nXiao Liu, Da Yin, Jingnan Zheng, Xingjian Zhang, Peng Zhang, Hongxia\nYang, Yuxiao Dong, and Jie Tang. 2022. OAG-BERT: Towards A Unified\nBackbone Language Model For Academic Knowledge Services . In Proceed-\nings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data\nMining (KDD ‚Äô22), August 14‚Äì18, 2022, Washington, DC, USA. ACM, New\nYork, NY, USA, 11 pages. https://doi.org/10.1145/3534678.3539210\n1 INTRODUCTION\nAcademic knowledge services, such as AMiner [40], Google Scholar,\nMicrosoft Academic Service [43], and Semantic Scholar, have been\nof great assistance to advance the science enterprise. Beyond collect-\ning statistics, e.g., citation count, an increasing attention of these\nplatforms has been focused on providing AI-powered academic\nknowledge applications, including paper recommendation [8, 14],\nexpert matching [30], taxonomy construction [34], and knowledge\nevolution [50].\nHowever, most of these applications are built with specified mod-\nels to understand scientific contents. For example, OAG Zhang et al.\nemploys the doc2vec [19] embeddings trained on a small corpus\nfor academic entity alignment [51]. Zhang et al. [55] leverage an\nattention strategy to model the text and metadata embeddings for\npaper tagging. In other words, the success of such academic systems\nheavily rely on different language understanding components. In\naddition, task-specific annotated datasets required by these compo-\nnents demand arduously expensive labeling cost.\nThe newly emerging pre-trained models, such as BERT [ 10]\nand GPT [ 31], have substantially promoted the development of\nnatural language processing (NLP). Specifically, pre-trained lan-\nguage models for academic data have also been developed, such as\nBioBERT [20] for the biomedical field and SciBERT [3] for scientific\nliterature. However, these models mainly focus on scientific texts\nand ignore the connected entity knowledge that can be crucial for\nmany knowledge-intensive applications. For example, in author\nname disambiguation [7, 56], the affiliations of a paper‚Äôs authors\noffer important signals about their identities.\nIn light of these issues, we propose to pre-train a unified entity-\naugmented academic language model, OAG-BERT, as the backbone\narXiv:2103.02410v3  [cs.CL]  3 Oct 2022\nKDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Xiao Liu et al.\nmodel for diverse academic mining tasks and knowledge applica-\ntions. OAG-BERT is pre-trained from the Open Academic Graph\n(OAG) [51], which is to date the largest publicly available hetero-\ngeneous academic entity graph. It contains more than 700 million\nentities (papers, authors, fields of study, venues, and affiliations), 2\nbillion relationships, and 5 million papers with full contents and\n110 million abstracts as corpora.\nTo handle the heterogeneous knowledge, we design the entity\ntype embedding for each type of entities, respectively. To implement\nthe masked language pre-training over entity names with various\nlengths, we leverage a span-aware entity masking strategy that\ncan select to mask a continuous span of tokens according to the\nentity length. To better ‚Äúnotify\" the OAG-BERT model with the\nentity span and sequence order, we propose the entity-aware 2D\npositional encoding to take both the inter-entity sequence order\nand intra-entity token order into consideration.\nWe apply OAG-BERT to nine academic knowledge applications,\nincluding name disambiguation [7, 56], literature retrieval, entity\ngraph completion [11, 16], paper recommendation [8], user activ-\nity prediction [8], fields-of-study tagging [27], venue prediction,\naffiliation prediction, and automatic title generation. Moreover, we\npresent a number of prompt-based zero-shot usages of OAG-BERT,\nincluding the predictions of a paper‚Äôs venue, affiliations, and fields\nof study, in which the annotation cost is significantly mitigated.\nTo sum up, we make the following contributions in this paper:\n‚Ä¢A Unified Backbone Model OAG-BERT: We identify the\nchallenge in existing academic knowledge applications, which\nheavily depend on ad-hoc models, corpora, and task-specific\nannotations. To address the problem, we present OAG-BERT as\na unified backbone model with 110M parameters to support it.\n‚Ä¢Entity-Augmented Language Model Pre-Training: In OAG-\nBERT, we enrich the language model with the massive hetero-\ngeneous entity knowledge from OAG. We design pre-training\nstrategies to incorporate entity knowledge into the model.\n‚Ä¢Prompt-based Zero-Shot Inference: We design a decoding\nstrategy to allow OAG-BERT to perform well on prompt-based\nzero-shot inference, which offers the potential to significantly\nreduce the annotation cost in many downstream applications.\n‚Ä¢System Deployment and Open OAG-BERT Model: We\ndemonstrate the effectiveness of OAG-BERT on nine academic\nknowledge applications. In addition, OAG-BERT has been de-\nployed as the infrastructure of AMiner2 and also used for NSFC‚Äôs\ngrant reviewer recommendation. The pre-trained model is open\nto public access through the CogDL [6] package for free.\n2 RELATED WORKS\nThe proposed OAG-BERT model is based on BERT [ 10], a self-\nsupervised [23] bidirectional language model. It employs multi-\nlayer transformers as its encoder and uses masked token prediction\nas its objective, allowing massive unlabeled text data as a training\ncorpus. BERT has many variants. SpanBERT [ 17] develops span-\nlevel masking which benefits span selection tasks. ERNIE [57] in-\ntroduces explicit knowledge graph inputs to the BERT encoder and\nachieves significant improvements over knowledge-driven tasks.\n2https://www.aminer.cn/\nOAG-LM\nArnet Miner [M] ‚Ä¶ Data min-\ning\nKnow\nledge [M] [M] [M] [M] Tsing\nhua\nDisco\nvery and Data Minin\ng\nExtra\nction\nPaper/Text Author FOS Venue AÔ¨Éliat\nion\nWord embedding\nHeterogeneous\nentity type embedding\n+\nMask Prediction\n2D-positional embedding\n+ (0, 0) (0, 1) (0, 2) (1, 0) (2, 0) (2, 1) (3, 0) (3, 1) (3, 2) (3, 3) (3, 4) (4, 0)(‚Ä¶)\n‚Ä¶\nOAG-BERT\nFigure 1: Heterogeneous entity augmentation in OAG-BERT.\n1) Heterogeneous entity type embedding allows OAG-BERT be aware\nof different types of entities, 2) Span-aware entity masking selects\na continuous span within long entities (such as the ‚ÄúKnowledge\nDiscovery and Data Mining‚Äù), and 3) Entity-aware 2D-positional\nembedding jointly models inter and intra-entity token orders.\nAs for the academic domain, previous works such as\nBioBERT [20] or SciBERT [3] leverage the pre-training process on\nscientific domain corpus and achieve state-of-the-art performance\non several academic NLP tasks. The S2ORC-BERT [ 26], applies\nthe same method with SciBERT on a larger scientific corpus and\nslightly improves the performance on downstream tasks. Later\nworks [15] further show that continuous training on specific do-\nmain corpus also benefits the downstream tasks. These academic\npre-training models rely on large scientific corpora. SciBERT uses\nthe semantic scholar corpus [2]. Other large academic corpora in-\ncluding AMiner [40], OAG [40, 51], and Microsoft Academic Graph\n(MAG) [18] also integrate massive publications with rich graph\ninformation as well, such as authors and research fields.\nOn academic graphs, some tasks involve not only text informa-\ntion from papers but also structural knowledge lying behind graph\nlinks. For example, to disambiguate authors or concepts with the\nsame names [7, 22, 56], the model needs to learn node representa-\ntions in the heterogeneous graph. To better recommend papers for\nonline academic search [13, 14], graph information including re-\nlated academic concepts and published venues could provide great\nbenefits. To infer experts‚Äô trajectory across the world [46], associ-\nating authors with their affiliation on semantic level would help.\nCapturing features from paper titles or abstracts is far from enough\nfor these types of challenges.\n3 OAG-BERT: A LANGUAGE MODEL WITH\nACADEMIC KNOWLEDGE\nWe present the OAG-BERT model with the goal of pre-training on\nboth the academic corpus and heterogeneous entity graph. Over-\nall, OAG-BERT is a bidirectional Transformer-based [10, 42] pre-\ntraining model with 12 Transformer encoder layers. We introduce\nimprovements to the model architecture and the pre-training pro-\ncess to handle not only text data but also the entity graph. OAG-\nBERT is trained on the Open Academic Graph (OAG) [ 51]‚Äîthe\nlargest public heterogeneous entity graph to date.\n3.1 The Model Architecture\nThe key challenge of building OAG-BERT is how to integrate the\nheterogeneous entity knowledge into language models. Most previ-\nous approaches [21, 57] focus on injecting homogeneous entities\nOAG-BERT: Towards A Unified Backbone Language Model For Academic Knowledge Services KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA\nand relations from knowledge graphs like Wikidata, while hetero-\ngeneous entities for language models have been largely unexplored.\nTo augment the language model with the heterogeneous entity\nknowledge, we place the text of each paper in OAG (its title and\nabstract) and associated entities (its authors, their affiliations, fields\nof study, and venue) into a single sequence as one training instance.\nTo further help OAG-BERT model different types of entities, we\nleverage the following three techniques: heterogeneous entity type\nembedding, entity-aware 2D-positional encoding , and span-aware\nentity masking .\nHeterogeneous Entity Type Embedding. In order to distin-\nguish different types of entities, we propose to leverage the entity\ntype embedding in the pre-training process to indicate the entity\ntype, which is similar to the token type embedding used in BERT.\nFigure 1 illustrates one paper example as the input of OAG-BERT.\nThe entities from the ‚ÄúArnetMiner‚Äù paper are concatenated as an\ninput sequence, whose length is limited to at most 512 tokens. For\neach type of entities, including the paper title and abstract as the\ntext entity, we label the text string with the original entity type\nindex, e.g., 0 for text, 1 for authors, and so on. Additionally, different\nentities should be order-invariant in the input sequence. We thus\nshuffle their order in the sequence to let the model avoid learning\nany positional biases of these entities.\nEntity-Aware 2D-Positional Encoding. It is known that the\nTransformer model [ 42] is permutation-invariant (i.e., unaware\nof the sequence order) and the critical technique to indicate the\nsequence order in natural language is to add positional embeddings .\nHowever, the existing positional embeddings for texts [42] are\ngenerally not applicable to capture the entity knowledge, as they\ncannot distinguish words from entities adjacent to each other and of\nthe same type. For instance, if two affiliations ‚ÄúTsinghua University‚Äù\nand ‚ÄúUniversity of California‚Äù are placed next to each other in a\nsequence, the vanilla Transformer model would assume that there is\nan affiliation named ‚ÄúTsinghua University University of California‚Äù.\nEssentially, the positional embedding should be able to 1) imply\nthe inter-entity sequence order for distinguishing different entities,\nand 2) indicate the intra-entity token sequence order that is used\nas the traditional positional embedding.\nWe design the entity-aware 2D-positional embedding that solves\nboth the inter-entity and intra-entity issues. Its first dimension is\nused for the inter-entity order, indicating which entity the token is\nin, and its second dimension is for the intra-entity order, indicating\nthe sequence of tokens. For a given position, the final positional\nembedding is calculated by adding the two positional embeddings\ntogether.\nSpan-Aware Entity Masking. When performing masking, we\nadopt the same random masking strategy as in BERT for text con-\ntents such as paper titles and abstracts. But for the heterogeneous\nacademic entities, we expect OAG-BERT to memorize them well and\nthus develop a span-aware entity masking strategy that combines\nthe advantages of both ERNIE [57] and SpanBERT [17].\nThe intuition is that some of the entities are too long for OAG-\nBERT to learn when using random masking at the single-token\ngranularity. The span-aware entity masking strategy not only alle-\nviates the problem, but also preserves the sequential relationship\nof an entity‚Äôs tokens. Specifically, for an entity that has fewer than\n4 tokens, we mask the whole entity; and for others, the masking\nlength is decided by sampling from a geometric distributionGeo(ùëù)\nthat satisfies:\nùëù = 0.2 , and 4‚â§Geo(p)‚â§ 10 (1)\nIf the sampled length is less than the entity length, we only mask\nout the entity. For both text contents and entity contents, we mask\n15% of the tokens.\nPre-LN BERT. In addition, we also adopt the Pre-LN BERT as used\nin DeepSpeed [32], where the layer normalization is placed inside\nthe residual connection instead of after the add-operation in Trans-\nformer blocks. Previous work [54] demonstrates that training with\nPre-LN BERT avoids vanishing gradients when using aggressive\nlearning rates, making it more stable than the traditional Post-LN\nversion for optimization.\n3.2 The Pre-Training Details\nThe pre-training of OAG-BERT is separated into two stages. In the\nfirst stage, we only use scientific texts (paper title, abstract, and\nbody) as the model inputs, without using the entity augmented\ninputs introduced above. This process is similar to the pre-training\nof the original BERT model. We name the intermediate pre-trained\nmodel as the vanilla version of OAG-BERT. In the second stage,\nbased on the vanilla OAG-BERT, we continue to train the model\non the heterogeneous entities, including titles, abstracts, venues,\nauthors, affiliations, and FOS.\nFirst Stage: Pre-train the vanilla OAG-BERT. In the first stage\nof pre-training, we construct the training corpus from two sources:\none comes from the PDF storage of AMiner; and the other comes\nfrom the PubMed XML dump. We clean up and sentencize the cor-\npus with SciSpacy [28]. The corpus adds up to around 5 million\nunique paper full-text from multiple disciplines. In terms of vocab-\nulary, we construct our OAG-BERT vocabulary using WordPiece,\nwhich is also used in the original BERT implementation. This ends\nup with 44,000 unique tokens in our vocabulary. The original BERT\nemploys a sentence-level loss, namely Next Sentence Prediction\n(NSP), to learn the entailment between sequences, which has been\nfound not that useful [17, 25] and thus we do not adopt it.\nTo better handle the entity knowledge of authors in the OAG,\nwe transform the author name list into a sentence for each paper\nand place it between the title and abstract in the data prepossess-\ning. Therefore, compared to previous models like SciBERT, our\nvocabulary contains more tokens from author names. Following\nthe training procedures of BERT, the vanilla OAG-BERT is first\npre-trained on samples with a maximum of 128 tokens and then\nshift to pre-training it over samples with 512 tokens.\nSecond Stage: Enrich OAG-BERT with entity knowledge. In\nthe second stage of pre-training, we use papers and related entities\nfrom the OAG corpus. Compared to the corpus used in the first\nstage, we do not have full texts for all papers in OAG. Thus, we only\nuse paper title and abstract as the paper text information. From this\ncorpus, we select all authors with at least 3 papers published. Then\nwe filter out all papers not linked to these selected authors. Finally,\nwe got 120 million papers, 10 million authors, 670 thousand FOS, 53\nthousand venues, and 26 thousand affiliations. Each paper and its\nconnected entities are concatenated into a single training instance,\nKDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Xiao Liu et al.\nfollowing the input construction method described above. In this\nstage, we integrate the three strategies mentioned in Section 3.1 to\nendow OAG-BERT the ability to ‚Äúnotice‚Äù the entities, rather than\nregarding them as pure texts. For tasks that require document-level\nrepresentations, we present a version of OAG-BERT with additional\ntask-agnostic triplet contrast pre-training, which uses papers from\nthe same authors in OAG as the positive pair and papers from\nauthors with similar names as the negative pair.\nOur pre-training is conducted with 32 Nvidia Tesla V100 GPUs\nand an accumulated batch size of 32768. We use the default BERT\npre-training configurations in deepspeed. We run 16K steps for the\nfirst stage pre-training and another 4K steps for the second stage.\n4 APPLICATIONS\nWe choose 9 fundamental academic mining tasks that are either\ndirectly deployed in the AMiner system or serve as prerequisites to\nother academic knowledge services, in which the entity knowledge\nmay play an indispensable role. These applications feature 5 typical\ndownstream applications:\n‚Ä¢Author Name Disambiguation [7, 39, 56]\n‚Ä¢Scientific Literature Retrieval [8, 44]\n‚Ä¢Paper Recommendation [13, 14, 37]\n‚Ä¢User Activity Prediction [8, 49, 52]\n‚Ä¢Entity Graph Completion [4, 12, 16]\nand 4 prompt-based zero-shot applications without need for any\nannotations:\n‚Ä¢Fields-of-study Tagging [27, 35]\n‚Ä¢Venue Prediction [1, 48]\n‚Ä¢Affiliation Prediction [45, 46]\n‚Ä¢Automatic Title Generation [29, 53]\nWe take SciBERT [3] as our major compared method to demon-\nstrate the importance of our entity-augmented pre-training. Other\nbaselines compared are introduced individually in each section.\n4.1 Downstream Applications\nCompared to language models for common NLP tasks, backbone\nlanguage models for academic mining are usually combined with\nother downstream supervised learning algorithms, such as cluster-\ning for name disambiguation [ 7, 56] and graph neural networks\nfor relation completion [16]. This requires our language model to\nprovide more informative representations on different entities.\nConventionally, these applications rely on individually trained\nrepresentation upon their own small dataset or corpus. For example,\nin [16], to acquire embeddings for heterogeneous entities such as\nvenues, fields-of-study and affiliations, the authors leverage metap-\nath2vec [11] embeddings which contains no semantic information;\nin [56] authors use word2vec embeddings trained on a small portion\nof paper abstracts from AMiner systems. As an effort to unifying\ninfrastructure for these applications, in the following evaluation\nOAG-BERT reports to achieve better performance on all of them.\nTable 1: The Macro Pairewise F1 scores for the author name\ndisambiguation competition whoiswho-v1.\nInputs SciBERT OAG-BERT\nUnsupervised\ntitle 0.3690 0.4120\n+fos 0.4101 0.4643\n+venue 0.3603 0.4247\n+fos+venue 0.3903 0.4823\nSupervised Leader Board Top1 0.4900\nAuthor Name Disambiguation. Name disambiguation, or\nnamely ‚Äúdisambiguating who is who‚Äù, is a fundamental challenge\nfor curating academic publication and author information, as du-\nplicated names widely exist in our lives. For example, Microsoft\nAcademic reports more than 10,000 authors named ‚ÄúJames Smith‚Äù in\nUnited States [36]. Without effective author name disambiguation\nalgorithms, it is difficult to identify the belonging-ship of certain\npapers for supporting applications such as expert matching, citation\ncounting and h-index computing.\nGiven a set of papers with authors of the same name, the prob-\nlem is usually formulated as designing algorithm to separate these\npapers into clusters, where papers in the same cluster belong to\nthe same author and different clusters represent different authors.\nWe use the public dataset whoiswho-v1 [7, 56]3 and apply the em-\nbeddings generated by pre-trained models to solve name disam-\nbiguation from scratch. Following dataset setting, for each paper,\nwe use the paper title and other attributes such as FOS or venue as\ninput. We average over all the output token embeddings for title\nas the paper embedding. Then, we build a graph with all papers\nas the graph nodes and set a threshold to select edges. The edges\nare between papers where the pairwise cosine similarity of their\nembeddings is larger than the threshold. Finally, for each connected\ncomponent in the graph, we treat it as a cluster. We searched the\nthresholds from 0.65 to 0.95 on the validation set and calculated the\nmacro pairwise f1 score on test.\nThe results in Table 1 indicate that the embedding of OAG-BERT\nis significantly better than the SciBERT embedding while directly\nused in the author name disambiguation. We also observe that for\nSciBERT the best threshold is always 0.8 while this value for OAG-\nBERT is 0.9, which reflects that the paper embeddings produced by\nOAG-BERT are generally closer than the ones produced by SciBERT.\nIn Table 1 we list a range of experimental results given title, field-\nof-study, and venue as inputs respectively. Though we attempted\nto use the abstract, author, and affiliation information, there is no\nperformance improvement as expected. We speculate it is because\nthese types of information are more complex to use, which might\nrequire additional classifier head or fine-tuning, as the supervised\nclassification task mentioned above. In addition, we also report the\ntop 1 score in the name disambiguation challenge leaderboard4 and\nfind that our proposed OAG-BERT reaches close performance as\ncompared with the top-1 ad-hoc model for the contest.\n3https://www.aminer.cn/whoiswho\n4https://www.biendata.xyz/competition/aminer2019/leaderboard/\nOAG-BERT: Towards A Unified Backbone Language Model For Academic Knowledge Services KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA\nTable 3: Paper recommendation and User Activity Predic-\ntion (Co-View and Co-Read) on Scidocs [8].\nModels Paper Rec. Co-View Co-Read\nnDCG P@1 MAP nDCG MAP nDCG\nRandom 51.3 16.8 25.2 51.6 25.6 51.9\ndoc2vec 51.7 16.9 67.8 82.9 64.9 81.6\nSent-BERT 51.6 17.1 68.2 83.3 64.8 81.3\nSciBERT 52.1 17.9 50.7 73.1 47.7 71.1\nOAG-BERT 52.6 18.6 74.7 86.3 71.4 84.7\nTable 2: Scientific Literature Retrieval evaluation on OAG-\nQA (Top-100) between SciBERT and OAG-BERT.\nSciBERT OAG-BERT\nGeometry 0.097 0.147\nMath. & Stats. 0.099 0.166\nAlgebra 0.071 0.069\nCalculus 0.091 0.160\nNumber theory 0.067 0.085\nLinear algebra 0.111 0.160\nAstrophysics 0.041 0.072\nQuantum mechanics 0.047 0.080\nClassical mechanics 0.085 0.197\nChemistry 0.181 0.216\nBiochemistry 0.146 0.319\nHealth care 0.041 0.262\nNatural science 0.101 0.277\nAlgorithm 0.084 0.209\nNeuroscience 0.054 0.120\nComputer vision 0.035 0.205\nData mining 0.082 0.161\nDeep learning 0.044 0.138\nMachine learning 0.085 0.177\nNLP 0.05 0.160\nEconomics 0.055 0.151\nAverage 0.079 0.168\nScientific Literature Retrieval. Scientific literature retrieval,\nwhich assists researchers finding relevant scientific literature given\ntheir natural language queries, is closely related to a wide-range\nof top-level applications including publication search, citation pre-\ndiction and scientific question and answering. For example, for a\nprofessional question like ‚ÄúDoes sleeping fewer hours than needed\ncause common cold?‚Äù , we may retrieve a related paper ‚ÄúSick and\ntired: does sleep have a vital role in the immune system?‚Äù .\nWe evaluate OAG-BERT with triplet contrastive training over a\nfine-grained topic-specific literature retrieval dataset OAG-QA [38],\nwhich is constructed by collecting high-quality pairs of questions\nand cited papers in answers from Online Question-and-Answers\n(Q&A) forums (Stack Exchange and Zhihu). It consists of 22,659\nunique query-paper pairs from 21 scientific disciplines and 87 fine-\ngrained topics. Given each topic is accompanied by 10,000 candidate\npapers including the groundtruth, and their titles and abstracts are\ntaken as the corpus. We compute the cosine similarity between\noutput embeddings of the query and paper for ranking. Results in\nTable 2 suggest that OAG-BERT has a consistently better perfor-\nmance than SciBERT across 20 scientific disciplines .\nPaper Recommendation & User Activity Prediction. As the\nnumber of scientific publications keeps soaring up, paper recom-\nmendation is playing an increasingly crucial role in many online\nTable 4: Results on Entity Graph Completion using HGT.\nOAG-BERT yields better initialization for heterogeneous entities.\nModels Paper-Field Paper-Venue\nNDCG MRR NDCG MRR\nXLNet 0.3939 0.4473 0.4385 0.2584\nSciBERT 0.4740 0.5743 0.4570 0.2834\nOAG-BERT 0.4892 0.6099 0.4844 0.3131\nacademic systems, and therefore it is important to evaluate a back-\nbone model‚Äôs ability in boosting a production recommendation\nsystem. We consider the situation when users are browsing cer-\ntain papers in our systems, and we want to 1) recommend them\nrelated papers of the ones they are reading, 2) predict papers they\nsimultaneously viewed (Co-View) or pdf-accessed (i.e., Co-Read) in\na user‚Äôs browser session. In practice, for paper recommendation, it\nis often conducted in an ensemble manner: together with cosine\nsimilarities of textual embeddings encoded by language models, we\njointly take other features such as citation overlaps, clicking counts\nand author similarities into consideration, and train a classifier to\nmake the final decision; for user activity prediction, we mainly\nmeasure co-viewed or co-read papers‚Äô textual similarities.\nWe adopt Scidocs [8] paper recommendation and user activity\nprediction dataset for offline evaluation, which is constructed from\nreal user clickthroughs and loggings in a publication search engine.\nThe recommendation dataset consists of 22k samples, in which 20k\nclickings are used for training the recommender, 1k for validation,\nand 1k for testing. For Co-View and Co-Read dataset, each of them\ncontains 30k papers. Besides SciBERT, we compare with common\npassage representation methods, including doc2vec [19] and Sent-\nBERT [33]. Results in Table 3 show that OAG-BERT with triplet\ncontrastive training can bring a consistent gain over compared\nmethods in both paper recommendation and user activity prediction\nsetting.\nEntity Graph Completion. Academic entity graph, which con-\nsists of heterogeneous entities including papers, authors, fields-\nof-study, venues, affiliations and other potential entities with at-\ntributes, is a powerful organization form of academic knowledge\nand finds wide adoptions in many academic systems such as Mi-\ncrosoft Academic Graph [36] and AMiner [40]. However, such entity\ngraphs have been suffered from the long-standing challenge of in-\ncomplete and missing relations, and therefore the task of entity\ngraph completion becomes vital to their maintenance.\nIn this section, we apply the heterogeneous entity embeddings\nof OAG-BERT as pre-trained initialization for entity embeddings\non the academic graph and show that OAG-BERT can also work\ntogether with other types of models. Specifically, we take the het-\nerogeneous graph transformer (HGT) model from [16], a state-of-\nthe-art graph neural network, to conduct entity graph completion\npre-trained embeddings from OAG-BERT.\nTo make predictions for the links in the heterogeneous graph,\nthe authors of HGT first extract node features and then apply HGT\nlayers to encode graph features. For paper nodes, the authors use\nXLNet [47], a well-known general-domain pre-trained language\nmodel, to encode titles as input features. For other types of nodes,\nHGT use metapath2vec [ 11] to initialize the features. However,\nKDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Xiao Liu et al.\nOAG-BERT\nthe review of manage\n-ment\naccount-\ning ‚Ä¶... [MASK] [MASK]\nPaper Paper Paper Paper Paper ‚Ä¶... FOS FOS\n0 0 0 0 0 ‚Ä¶... 1 1\n0 1 2 3 4 ‚Ä¶... 0 1\n(knowledge, .192) (public, .191) (computer, .151) relations(.048), ...\n(science, .625) (accounting, .199) (management, .090) management(.150), ...\nOAG-BERT\nthe review of manage\n-ment\naccount-\ning ‚Ä¶... [MASK] science\nPaper Paper Paper Paper Paper ‚Ä¶... FOS FOS\n0 0 0 0 0 ‚Ä¶... 1 1\n0 1 2 3 4 ‚Ä¶... 0 1\n(political, .526) (computer, .274) (management, .117) library(.016) ...\nRound 2Round 1\nDecoded Tokens\nEntity Type\nInter-entity Pos\nIntra-entity Pos\nFigure 2: The decoding process of OAG-BERT. The left figure indicates that OAG-BERT decodes the masked token ‚Äúscience‚Äù at the\nsecond position with the highest probability (0.625) for the first round. Then it decodes ‚Äúpolitical‚Äù at the first position with the highest\nprobability (0.526) for the second round as shown in the right figure.\nTable 5: The results for zero-shot inference tasks.\nMethod Paper Tagging Venue Affiliation\nHit@1 MRR Hit@1 MRR Hit@1 MRR\nSciBERT 19.93% 0.37 9.87% 0.22 6.93% 0.19\n+prompt 29.59% 0.47 10.03% 0.21 8.00% 0.20\n+abstract 25.66% 0.43 18.00% 0.32 10.33% 0.22\n+both 35.33% 0.52 9.83% 0.22 12.40% 0.25\nOAG-BERT 34.36% 0.51 21.00% 0.37 11.03% 0.24\n+prompt 37.33% 0.55 22.67% 0.39 11.77% 0.25\n+abstract 49.59% 0.67 39.00% 0.57 21.67% 0.38\n+both 49.51% 0.67 38.47% 0.57 21.53% 0.38\nXLNet was pre-trained on universal language corpus, lacking aca-\ndemic domain knowledge, and can only encode paper nodes by\nusing their titles and is unable to generate informative embeddings\nfor other types of nodes.\nTo this end, we replace the original XLNet encoder with our OAG-\nBERT model, which can tackle the two challenges mentioned above.\nWe use the OAG-BERT model to encode all types of nodes and use\nthe generated embeddings as their node features. To demonstrate\nthe effectiveness of OAG-BERT on encoding heterogeneous nodes,\nwe also compare the performance of SciBERT with OAG-BERT. We\nexperimented on the CS dataset released by HGT5. The details of\nthe dataset are delivered in the appendix. The NDCG and MRR\nscores for the Paper-Field and Paper-Venue link prediction are\nreported in Table 4. It shows that SciBERT surpasses the original\nXLNet performance significantly, due to the pre-training on the\nlarge scientific corpus. Our proposed OAG-BERT made further\nimprovements on top of that, as it can better understand the entity\nknowledge on the heterogeneous graph.\n4.2 Prompt-based Zero-shot Applications\nDespite OAG-BERT‚Äôs qualification in providing unified support\nto various downstream applications to get rid of ad-hoc models\nand corpus, a more challenging topic is to reduce task-specific\nannotations, which can be expensive in business deployment.\n5https://github.com/acbull/pyHGT\nTake affiliation prediction as an example, a common approach\nis to train a ùëò-class classifier for ùëò given candidate institutions.\nHowever, as the progress of science, new universities, laborato-\nries and companies emerge and to incorporate them into the pool\nmay require re-annotating and re-training of the classifier with\nconsiderably high cost.\nIn light of the recent prompt-based [24] zero-shot and few-shot\nadvances of large-scale pre-trained language models such as GPT-\n3 [5], in this section we also explore the potential of applying OAG-\nBERT to zero-shot applications in academic mining. We discover\nthat OAG-BERT works surprisingly well on some fundamental\napplications, such as paper tagging, venue/affiliation prediction,\nand generation tasks such as title generation. We will first introduce\nhow we implement the zero-shot inference on OAG-BERT, and then\nthe details of our applications.\nOAG-BERT‚Äôs zero-shot inference strategies. Although not us-\ning a unidirectional decoder structure like GPT-3, we find that the\nbidirectional encoder-based OAG-BERT is also capable of decod-\ning entities based on the knowledge it learned during pre-training.\nA running-example is provided in Figure 2. In MLM, the token\nprediction can be seen as maximizing the probability of masked\ninput tokens, treating predictions on each token independently by\nmaximizing √ç\nùë§‚ààmasked log ùëÉ (ùë§ |C), where masked is the collection\nof masked tokens and Cdenotes contexts. But in entity decoding,\nwe cannot ignore the dependencies between tokens in each entity,\nand thus need to jointly consider the probability of all tokens in\none entity as following log ùëÉ (ùë§1, ùë§2, ..., ùë§ùëô |C), where ùëô is the entity\nlength and ùë§ùëñ is the ùëñ-th token in the entity. As MLM is not unidi-\nrectional model, the decoding order for the tokens in one entity can\nbe arbitrary. Suppose the decoding order is ùë§ùëñ1 , ùë§ùëñ2 , ..., ùë§ùëñùëô , where\nùëñ1, ùëñ2, ..., ùëñùëô is a permutation of 1, 2, ..., ùëô. Then the prediction target\ncan be reformed as maximizing\n‚àëÔ∏Å\n1‚â§ùëò ‚â§ùëô\nlog ùëÉ (ùë§ùëñùëò |C, ùë§ùëñ1 , ùë§ùëñ2 , ..., ùë§ùëñùëò‚àí1 ) (2)\nAs the solution space is getting larger as ùëô increases, we adopt the\nfollowing two strategies to determine the decoding order:\nOAG-BERT: Towards A Unified Backbone Language Model For Academic Knowledge Services KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA\nFigure 3: Deployed zero-shot paper tagging service in\nAMiner. OAG-BERT yields fine-grained tags at various lengths.\n‚Ä¢Greedy: we use greedy selection to decide the decoding order,\nby choosing the token with maximal probability to decode. An\nexample is depicted in Figure 2.\n‚Ä¢Beam search: we can also use beam search [41] to search the\ntoken combinations with the highest probability.\nAnother challenge lies in choosing the appropriate entity length.\nInstead of using a fixed length, we traverse all entity lengths in\na pre-defined range depending on the entity type and choose top\ncandidates according to the calculated probability in Equation 2.\nFields-of-study Tagging. Fields-of-study tagging, referred to as\nfields-of-study (FOS) linking, is a fundamental mission to associate\nunstructured scientific contents with structured disciplinary tax-\nonomy (a case is presented in Figure 3). Its results also serve as\nindispensable features for various downstream supervised applica-\ntions.\nHowever, it is a notoriously arduous undertaking to discover\nnew FOS from enormous corpora; in addition, how to continuously\ndrive algorithms to link a paper with newly discovered FOS also\nremains largely unexplored. However, thanks to the massive entity\nknowledge OAG-BERT has grasped in pre-training, it can be solved\nnow using OAG-BERT‚Äôs zero-shot inference without a lift of fingers.\nWe present a case study, where OAG-BERT is applied to tag the\npaper of GPT-3 [5] given its title and abstract. Using beam search\nwith a width of 16 to decode FOS entities, we search from single-\ntoken entities to quadruple-token entities. The top 16 generated\nones are listed in Table 6. The groud truth (or namely gold) FOS\nlater annotated in MAG are all included in the top 16. Surprisingly,\nsome fine-grained correct entities, though not in existing FOS, are\nalso generated, such as Autoregressive language model or Few shot\nlearning. Despite some ill-formed or inappropriate entities such\nas Architecture or Artificial language processing , OAG-BERT‚Äôs zero-\nshot tagging capability is still quite amazing.\nTo quantitatively evaluate the performance paper tagging, we\nadapt FOS prediction task from MAG. First, we choose 19 top-level\nfield-of-studies (FOS) such as ‚Äúbiology‚Äù and ‚Äúcomputer science‚Äù.\nThen, from the paper data which were not used in the pre-training\nprocess, we randomly select 1,000 papers for each FOS. The task\nis to rank all FOS for each paper by estimating the probabilities of\nEquation 2 given paper title and optional abstract.\nTable 6: OAG-BERT‚Äôs zero-shot paper tagging on the paper\nof GPT-3 given its title and abstract. The groundtruth FOS are\nbolded. Newly created FOS by OAG-BERT are underlined.\nTitle Language Models are Few-Shot Learners\nAbstract Recent work has demonstrated substantial gains on many NLP tasks\nand benchmarks by pre-training on a large corpus of text followed by\nfine-tuning on a specific task. While typically task-agnostic in archi-\ntecture, this method still requires task-specific fine-tuning datasets\nof thousands or tens of thousands of examples. By contrast, humans\ncan generally...\nGenerated\nFOS\nNatural language processing, Autoregressive language model, Com-\nputer science, Sentence, Artificial intelligence, Domain adaptation,\nLanguage model, Few shot learning, Large corpus, Arithmetic, Ma-\nchine learning, Architecture, Theoretical computer science, Data\nmining, Linguistics, Artificial language processing\nGold FOS Language model, Computer science, Linguistics\nWe also apply two techniques to improve the model decoding\nperformance. The first technique is to add extraprompt word to the\nend of the paper title (before masked tokens). We select ‚ÄúField of\nstudy:‚Äù as the prompt words in the FOS inference task. The second\ntechnique is to concatenate the paper abstract to the end of the\npaper title. We report the Hit@1 and MRR scores in Table 5.\nVenue and Affiliation Prediction. Analogously to paper tag-\nging, venue and affiliation prediction of certain papers can also\nbe conducted in zero-shot learning setting. From non-pretrained\npapers, we choose the 30 most frequent arXiv categories and 30\naffiliations as inference candidates, with 100 papers randomly se-\nlected for each candidate. Full lists of the candidates including FOS\ncandidates are enclosed in the appendix.\nThe experiment settings completely follow the FOS inference\ntask, except that we use ‚ÄúJournal or Venue:‚Äù and ‚ÄúAffiliations:‚Äù as\nprompt words respectively. The entity type embeddings for masked\nentities in OAG-BERT are also replaced by venue and affiliation\nentity type embeddings accordingly.\nIn Table 5, we can see that the proposed augmented OAG-BERT\noutperforms SciBERT by a large margin. Although SciBERT was not\npre-trained with entity knowledge, it still performs much greater\nthan a random guess, which means the inference tasks are not\nindependent of the paper content information. We speculate that\nthe pre-training process on paper content (as used in SciBERT) also\nhelps the model learn some generalized knowledge on other types\nof information, such as field-of-studies or venue names.\nWe also observe that the proposed use of abstract can always\nhelp improve the performance. On the other hand, the prompt\nwords works well with SciBERT but only provide limited help for\nOAG-BERT. Besides, the affiliation inference task appears to be\nharder than the other two tasks. Further analysis are provided in\nthe A.1. Two extended experiments are enclosed as well, which\nreveal two findings:\n(1) Using the summation of token log probabilities as the entity\nlog probability is better than using the average.\n(2) The out-of-order decoding is more suitable for encoder-based\nmodels like SciBERT and OAG-BERT, compared with the left-\nto-right decoding.\nKDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Xiao Liu et al.\nFigure 4: Deployed zero-shot Title Generation application\nfor Turing Test in Wudao. 7\nTable 7: Upper: case study in OAG-BERT generated titles and\noriginal title. Lower: Online testing result from 660 random human\nviews on 50 pairs of OAG-BERT generated and original titles.\nOAG-BERT Generated v.s. Original\nOAG-BERT OAG-LM: A Unified Backbone for Academic Knowledge Services\nOAG-LM: A Unified Backbone Language Model for Academic\nKnowledge Services\nAMiner ArnetMiner: A System for Extracting and Mining Academic Social\nNetworks\nArnetMiner: Extraction and Mining of Academic Social Networks\nResNet Deep Residual Networks for Visual Recognition : A Comparison of\nDeep and VGG Networks\nDeep Residual Networks for Image Recognition\nSciBERT SciBERT: A Pretrained Language Model for Scientific NLP\nSciBERT: A Pretrained Language Model for Scientific Text\nMethod Total Select Selection Rate\nOAG-BERT Generated 330 157 47.6%\nOriginal 330 163 52.4%\nAutomatic Title Generation. How to summarize the contribu-\ntions of a research paper into one sentence? Given the abstract,\neven senior experts may not figure out in a few seconds, but OAG-\nBERT can generate titles comparable to original human-written\nones in a zero-shot manner. During the pre-training,the span mask-\ning strategy will also be applied to titles, allowing OAG-BERT to\nlearn to summarize. Some case studies are presented in Table 7, in\nwhich we observe that OAG-BERT can generate quite the same title\nas origin given the paper abstract, even for our paper itself.\nWe also provide an interactive testing demo application online (as\nshown in Figure 4)6 to test if graduate-level students can distinguish\nbetween OAG-BERT generated and original titles. Results suggest\nthat there is probably only a small gap in performance between\nOAG-BERT‚Äôs generation and human assignment‚Äôs.\n6Try demo at: https://wudao.aminer.cn/turing-test/v1/game/pubtitle\n5 DEPLOYED APPLICATIONS\nIn this section, we will introduce several real-world applications\nwhere our OAG-BERT is employed.\nFirst, the results on the name disambiguation tasks indicate that\nthe OAG-BERT is relatively strong at encoding paper information\nwith multi-type entities, which further help produce representative\nembeddings for the paper authors. Thus, we apply the OAG-BERT\nto the NSFC reviewer recommendation problem [9]. The National\nNatural Science Foundation of China is one of the largest science\nfoundations, where an enormous number of applications are re-\nviewed every year. Finding appropriate reviewers for applications\nis time-consuming and laborious. To tackle this problem, we col-\nlaborate with Alibaba and develop a practical algorithm on top of\nthe OAG-BERT which can automatically assign proper reviewers\nto applications and greatly benefits the reviewing process.\nIn addition to that, we also integrate the OAG-BERT as a fun-\ndamental component for the AMiner [40] system. In AMiner, we\nutilize OAG-BERT to handle rich information on the academic het-\nerogeneous graph. For example, with the ability of decoding FOS\nentities, we use the OAG-BERT to automatically generate FOS can-\ndidates for unlabeled papers. Besides, we similarly amalgamate the\nOAG-BERT into the name disambiguation framework. Finally, we\nemploy OAG-BERT to recommend related papers for users, lever-\naging its capability in encoding paper embeddings. The OAG-BERT\nmodel is also released in CogDL package.\n6 CONCLUSION\nIn conclusion, we propose OAG-BERT, a heterogeneous entity-\naugmented language model to serve as the backbone for academic\nknowledge services. It incorporates entity knowledge during pre-\ntraining, which benefits many downstream tasks involving strong\nentity knowledge. OAG-BERT is applied to nine typical academic\napplications and also deployed in AMiner and for NSFC grant re-\nviewer recommendation. We release the pre-trained OAG-BERT\nmodel in CogDL for free use to everyone.\nACKNOWLEDGEMENT\nWe thank the reviewers for their valuable feedback to improve this\nwork. This work is supported by Technology and Innovation Major\nProject of the Ministry of Science and Technology of China un-\nder Grant 2020AAA0108400 and 2020AAA0108402, Natural Science\nFoundation of China (Key Program, No. 61836013), and National Sci-\nence Foundation for Distinguished Young Scholars (No. 61825602).\nREFERENCES\n[1] Hamed Alhoori and Richard Furuta. 2017. Recommendation of scholarly venues\nbased on dynamic user interests. Journal of Informetrics 11, 2 (2017), 553‚Äì563.\n[2] Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Craw-\nford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu\nHa, et al. 2018. Construction of the Literature Graph in Semantic Scholar. In\nNAACL. 84‚Äì91.\n[3] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language\nModel for Scientific Text. In EMNLP.\n[4] Nesserine Benchettara, Rushed Kanawati, and Celine Rouveirol. 2010. Super-\nvised machine learning applied to link prediction in bipartite social networks. In\nASONAM. IEEE, 326‚Äì330.\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. NIPS (2020).\nOAG-BERT: Towards A Unified Backbone Language Model For Academic Knowledge Services KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA\n[6] Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Xingcheng Yao,\nAohan Zeng, Shiguang Guo, Peng Zhang, Guohao Dai, et al. 2021. CogDL: an\nextensive toolkit for deep learning on graphs. arXiv preprint arXiv:2103.00959\n(2021).\n[7] Bo Chen, Jing Zhang, Jie Tang, Lingfan Cai, Zhaoyu Wang, Shu Zhao, Hong\nChen, and Cuiping Li. 2020. CONNA: Addressing Name Disambiguation on The\nFly. TKDE (2020).\n[8] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld.\n2020. SPECTER: Document-level Representation Learning using Citation-\ninformed Transformers. In ACL. 2270‚Äì2282.\n[9] David Cyranoski. 2019. Artificial intelligence is selecting grant reviewers in\nChina. Nature 569, 7756 (2019).\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL.\n[11] Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017. metapath2vec:\nScalable representation learning for heterogeneous networks. In SIGKDD.\n[12] Yuxiao Dong, Jie Tang, Sen Wu, Jilei Tian, Nitesh V Chawla, Jinghai Rao, and\nHuanhuan Cao. 2012. Link prediction and recommendation across heterogeneous\nsocial networks. In ICDM. IEEE, 181‚Äì190.\n[13] Zhengxiao Du, Jie Tang, and Yuhui Ding. 2018. Polar: Attention-based cnn for\none-shot personalized article recommendation. In ECML-PKDD. Springer.\n[14] Zhengxiao Du, Jie Tang, and Yuhui Ding. 2019. POLAR++: Active One-shot\nPersonalized Article Recommendation. TKDE (2019).\n[15] Suchin Gururangan, Ana Marasoviƒá, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,\nDoug Downey, and Noah A Smith. 2020. Don‚Äôt Stop Pretraining: Adapt Language\nModels to Domains and Tasks. arXiv preprint arXiv:2004.10964 (2020).\n[16] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020. Heterogeneous\ngraph transformer. In WWW.\n[17] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and\nOmer Levy. 2020. Spanbert: Improving pre-training by representing and predict-\ning spans. TACL 8 (2020).\n[18] Anshul Kanakia, Zhihong Shen, Darrin Eide, and Kuansan Wang. 2019. A scalable\nhybrid research paper recommender system for microsoft academic. In WWW.\n[19] Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and\ndocuments. In ICML. PMLR, 1188‚Äì1196.\n[20] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,\nChan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinformatics 36, 4 (2020).\n[21] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping\nWang. 2020. K-bert: Enabling language representation with knowledge graph. In\nAAAI, Vol. 34.\n[22] Xiao Liu, Li Mian, Yuxiao Dong, Fanjin Zhang, Jing Zhang, Jie Tang, Peng Zhang,\nJibing Gong, and Kuansan Wang. 2021. OAG_know: Self-supervised Learning for\nLinking Knowledge Graphs.IEEE Transactions on Knowledge and Data Engineering\n(2021).\n[23] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie\nTang. 2021. Self-supervised learning: Generative or contrastive.IEEE Transactions\non Knowledge and Data Engineering (2021).\n[24] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and\nJie Tang. 2021. GPT understands, too. arXiv preprint arXiv:2103.10385 (2021).\n[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[26] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Dan S Weld. 2019.\nS2orc: The semantic scholar open research corpus.arXiv preprint arXiv:1911.02782\n(2019).\n[27] Cameron Marlow, Mor Naaman, Danah Boyd, and Marc Davis. 2006. HT06,\ntagging paper, taxonomy, Flickr, academic article, to read. In HT Conference .\n31‚Äì40.\n[28] Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. Scispacy: Fast\nand robust models for biomedical natural language processing. arXiv preprint\narXiv:1902.07669 (2019).\n[29] Vahed Qazvinian and Dragomir Radev. 2008. Scientific Paper Summarization\nUsing Citation Summary Networks. In COLING. 689‚Äì696.\n[30] Yujie Qian, Jie Tang, and Kan Wu. 2018. Weakly learning to match experts in\nonline community. In Proceedings of the 27th International Joint Conference on\nArtificial Intelligence. 3841‚Äì3847.\n[31] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language models are unsupervised multitask learners. OpenAI\nblog 1, 8 (2019).\n[32] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deep-\nspeed: System optimizations enable training deep learning models with over 100\nbillion parameters. In SIGKDD.\n[33] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In EMNLP. 3982‚Äì3992.\n[34] Jiaming Shen, Zhihong Shen, Chenyan Xiong, Chi Wang, Kuansan Wang, and\nJiawei Han. 2020. TaxoExpan: Self-supervised taxonomy expansion with position-\nenhanced graph neural network. In WWW. 486‚Äì497.\n[35] Zhihong Shen, Hao Ma, and Kuansan Wang. 2018. A Web-scale system for\nscientific knowledge exploration. ACL (2018), 87.\n[36] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and\nKuansan Wang. 2015. An overview of microsoft academic service (mas) and\napplications. In WWW.\n[37] Kazunari Sugiyama and Min-Yen Kan. 2010. Scholarly paper recommendation\nvia user‚Äôs recent research interests. In JCDL. 29‚Äì38.\n[38] Weng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong,\nJiahua Liu, Maodi Hu, and Jie Tang. 2022. Parameter-Efficient Prompt Tun-\ning Makes Generalized and Calibrated Neural Text Retrievers. arXiv preprint\narXiv:2207.07087 (2022).\n[39] Jie Tang, Alvis CM Fong, Bo Wang, and Jing Zhang. 2011. A unified probabilistic\nframework for name disambiguation in digital library. TKDE (2011).\n[40] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnet-\nminer: extraction and mining of academic social networks. In SIGKDD.\n[41] C. Tillmann and H. Ney. 2003. Word Reordering and a Dynamic Programming\nBeam Search Algorithm for Statistical Machine Translation. Computational\nLinguistics 29 (2003), 97‚Äì133.\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762 (2017).\n[43] Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong,\nand Anshul Kanakia. 2020. Microsoft academic graph: When experts are not\nenough. Quantitative Science Studies 1, 1 (2020), 396‚Äì413.\n[44] Howard D White, H Cooper, LV Hedges, et al. 2009. Scientific communication\nand literature retrieval. The handbook of research synthesis and meta-analysis 2\n(2009), 51‚Äì71.\n[45] Kan Wu, Jie Tang, Zhou Shao, Xinyi Xu, Bo Gao, and Shu Zhao. 2018. CareerMap:\nvisualizing career trajectory. Science China Information Sciences (2018).\n[46] Kan Wu, Jie Tang, and Chenhui Zhang. 2018. Where Have You Been? Inferring\nCareer Trajectory from Academic Social Network.. In IJCAI.\n[47] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\nand Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language\nunderstanding. arXiv preprint arXiv:1906.08237 (2019).\n[48] Zaihan Yang and Brian D Davison. 2012. Venue recommendation: Submitting\nyour paper with style. In ICMLA, Vol. 1. IEEE, 681‚Äì686.\n[49] Jihang Ye, Zhe Zhu, and Hong Cheng. 2013. What‚Äôs your next move: User activity\nprediction in location-based social networks. In SDM. SIAM, 171‚Äì179.\n[50] Da Yin, Weng Lam Tam, Ming Ding, and Jie Tang. 2021. MRT: Tracing the\nEvolution of Scientific Publications. TKDE (2021).\n[51] Fanjin Zhang, Xiao Liu, Jie Tang, Yuxiao Dong, Peiran Yao, Jie Zhang, Xiaotao\nGu, Yan Wang, Bin Shao, Rui Li, et al . 2019. Oag: Toward linking large-scale\nheterogeneous entity graphs. In SIGKDD.\n[52] Fanjin Zhang, Jie Tang, Xueyi Liu, Zhenyu Hou, Yuxiao Dong, Jing Zhang, Xiao\nLiu, Ruobing Xie, Kai Zhuang, Xu Zhang, et al . 2021. Understanding WeChat\nUser Preferences and ‚ÄúWow‚Äù Diffusion. TKDE (2021).\n[53] Jian-Guo Zhang, Pengcheng Zou, Zhao Li, Yao Wan, Xiuming Pan, Yu Gong, and\nPhilip S Yu. 2019. Multi-Modal Generative Adversarial Network for Short Product\nTitle Generation in Mobile E-Commerce. NAACL HLT 2019 (2019), 64‚Äì72.\n[54] Minjia Zhang and Yuxiong He. 2020. Accelerating Training of Transformer-\nBased Language Models with Progressive Layer Dropping. arXiv preprint\narXiv:2010.13369 (2020).\n[55] Yu Zhang, Zhihong Shen, Yuxiao Dong, Kuansan Wang, and Jiawei Han. 2021.\nMATCH: Metadata-Aware Text Classification in A Large Hierarchy. InWWW\n(WWW ‚Äô21) . ACM, New York, NY, USA, 3246‚Äì3257.\n[56] Yutao Zhang, Fanjin Zhang, Peiran Yao, and Jie Tang. 2018. Name Disambiguation\nin AMiner: Clustering, Maintenance, and Human in the Loop.. In SIGKDD.\n[57] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu.\n2019. ERNIE: Enhanced language representation with informative entities. arXiv\npreprint arXiv:1905.07129 (2019).\nKDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Xiao Liu et al.\nA EXPERIMENT SUPPLEMENTARY\nA.1 Zero-Shot Inference\nUse of Prompt Word As shown in Table 5, the use of proposed\nprompt words in the FOS inference task, turns out to be fairly useful\nfor SciBERT to decode paper fields (FOS). We conjecture it is because\nthe extra appended prompt words can help alter the focus of the\npre-training model while making predictions on masked tokens.\nHowever, the improvement for SciBERT is marginal on affiliation\ninference. When decoding venue, it even hurts the performance.\nThis is probably due to the improper choice of prompt words.\nFor OAG-BERT, this technique has limited help as our expecta-\ntion. Instead of using continuous positions as SciBERT, OAG-BERT\nencodes inter-entity positions to distinguish different entities and\npaper texts. Thus the additional appended prompt word is treated\nas part of the paper title and is not adjacent to the masked entities\nfor OAG-BERT.\nUse of Abstract The use of abstracts can greatly improve the\nmodel inference performance in both SciBERT and OAG-BERT.\nBoth models frequently accept long text inputs in the pre-training\nprocess, which makes them naturally favor abstracts. Besides, ab-\nstracts contain rich text information which can help the pre-training\nmodel capture the main idea of the whole paper.\nTask Comparisons The affiliation generation task appears to be\nmuch harder than the other two tasks. This is probably due to\nthe weak semantic information contained in affiliation names. The\nwords in field-of-studies can be seen as sharing the same language\nwith paper contents and most venue names also contain informative\nconcept words such as ‚ÄúMachine Learning‚Äù or ‚ÄúHigh Energy‚Äù. This\nis not always true for affiliation names. For universities like ‚ÄúHar-\nvard University‚Äù or ‚ÄúUniversity of Oxford‚Äù, their researchers could\nfocus on multiple unrelated domains which are hard for language\nmodels to capture. For companies and research institutes, some\nmay focus on a single domain but it is not necessary to have such\ndescriptions in their names, which also confuses the pre-training\nlanguage model.\nDiscussion for Entity Probability In Equation 2, we use the sum\nof log probabilities of all tokens to calculate the entity log proba-\nbility. This method seems unfair for entities with longer lengths\nas the log probability for each token is always negative. However,\nfor MLM-based models, the encoding process not only encodes\n‚Äú[MASK]‚Äù tokens but also captures the length of the masked entity\nand each token‚Äôs position. Therefore, if the pre-training corpus\nhas fewer long entities than short entities, in the decoding process,\nthe decoded tokens in a long entity will generally receive higher\nprobability, compared to the ones in a short entity.\nEven so, the sum of log probabilities is still not necessary to\nbe the best choice depending on the entity distribution in the pre-\ntraining corpus. We conduct a simple experiment to test different\naverage methods. We reform the calculation of entity log probability\nin Equation 2 as 1\nùêøùõº\n√ç\n1‚â§ùëò ‚â§ùëô log ùëÉ (ùë§ùëñùëò |C, ùë§ùëñ1 , ùë§ùëñ2 , ..., ùë§ùëñùëò‚àí1 ), where\nùêø denotes the length of the target entity. Whenùõº = 0, this equation\ndegrades to the summation version used in previous tasks. When\nùõº = 1, this equation degrades to the average version.\nWe compare different averaging methods by using various ùõº\nand test their performance on the zero-shot inference tasks. We\nselect the input features with the best performance according to\nTable 5. For SciBERT, we use both abstract and prompt word for\nFOS and affiliation inference. We do not use the prompt word for\nvenue inference. For OAG-BERT, we only use abstract as the prompt\nword does not work well. The results in Table 8 show that for the\nmost time, using the summation strategy outperforms the average\nstrategy significantly. The simple average (ùõº = 1) appears to be the\nworst choice. However, for some situations, a moderate average\n(ùõº = 0.5) might be beneficial.\nTable 8: The results for using different average methods\nwhile calculating entity log probabilities. Hit@1 and MRR\nare reported.\nMethod ùõº = 0 ùõº = 0.5 ùõº = 1\nSciBERT\nFOS 35.33%, 0.52 32.07%, 0.51 14.85%, 0.36\nVenue 18.00%, 0.32 19.30%, 0.33 7.07%, 0.23\nAffiliation 12.40%, 0.25 10.83%, 0.23 9.23%, 0.21\nOAGBERT\nFOS 49.59%, 0.67 48.08%, 0.66 45.36%, 0.63\nVenue 39.00%, 0.57 38.20%, 0.57 36.13%, 0.55\nAffiliation 21.67%, 0.38 19.90%, 0.36 16.47%, 0.31\nDiscussion for Decoding Order In our designed decoding pro-\ncess, we do not strictly follow the left-to-right order as used in\nclassical decoder models. The main reason is that for encoder-based\nBERT model, the decoding for each masked token relies on all\nbidirectional context information, rather than only prior words.\nWe compare the performance of using left-to-right decoding and\nout-of-order decoding in Table 9.\nThe results show that for FOS, there is no significant difference\nbetween two decoding orders, since the candidate FOS only has\none or two tokens inside. As for venue and affiliation, it turns out\nthat the out-of-order decoding generally performs much better\nthan left-to-right decoding, except when OAG-BERT uses abstract\nwhere differences are relatively small as well. We also present the\nresults for models using left-to-right decoding and prompt words\nin Table 9, which indicates that the left-to-right decoding will some-\ntimes undermine the effectiveness of prompt words significantly,\nespecially for OAG-BERT.\nOAG-BERT: Towards A Unified Backbone Language Model For Academic Knowledge Services KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA\nTable 9: The results for using left-to-right decoding and out-\nof-order decoding order. Hit@1 and MRR are reported. Re-\nsults with difference larger than 1% Hit@1 were bolded.\nMethod FOS Venue Affiliation\nHit@1 MRR Hit@1 MRR Hit@1 MRR\nSciBERT\nLeft-to-Right 20.05% 0.37 8.40% 0.20 6.90% 0.18\nOut-of-Order 19.93% 0.37 9.87% 0.22 6.93% 0.19\nSciBERT +prompt\nLeft-to-Right 29.65% 0.47 9.57% 0.21 8.03% 0.20\nOut-of-Order 29.59% 0.47 10.03% 0.21 8.00% 0.20\nSciBERT +abstract\nLeft-to-Right 25.67% 0.43 11.43% 0.24 7.63% 0.19\nOut-of-Order 25.66% 0.43 18.00% 0.32 10.33% 0.22\nSciBERT +both\nLeft-to-Right 35.21% 0.52 11.17% 0.24 11.47% 0.23\nOut-of-Order 35.33% 0.52 9.83% 0.22 12.40% 0.25\nOAG-BERT\nLeft-to-Right 34.94% 0.53 11.33% 0.24 5.47% 0.17\nOut-of-Order 34.36% 0.51 21.00% 0.37 11.03% 0.24\nOAG-BERT +prompt\nLeft-to-Right 37.84% 0.56 12.53% 0.26 5.50% 0.17\nOut-of-Order 37.33% 0.55 22.67% 0.39 , 11.77% 0.25\nOAG-BERT +abstract\nLeft-to-Right 49.75% 0.67 40.50% 0.59 21.93% 0.38\nOut-of-Order 49.59% 0.67 39.00% 0.57 21.67% 0.38\nOAG-BERT +both\nLeft-to-Right 49.83% 0.67 22.17% 0.38 6.80% 0.19\nOut-of-Order 49.51% 0.67 38.47% 0.57 21.53% 0.38\nTable 10: A full list of used candidates in zero-shot inference\ntasks and supervised classification tasks.\nFOS: Art, Biology, Business, Chemistry, Computer science, Economics,\nEngineering, Environmental science, Geography, Geology, History, Ma-\nterials science, Mathematics, Medicine, Philosophy, Physics, Political\nscience, Psychology, Sociology\nVenue: Arxiv: algebraic geometry, Arxiv: analysis of pdes, Arxiv: astro-\nphysics, Arxiv: classical analysis and odes, Arxiv: combinatorics, Arxiv:\ncomputer vision and pattern recognition, Arxiv: differential geometry,\nArxiv: dynamical systems, Arxiv: functional analysis, Arxiv: general\nphysics, Arxiv: general relativity and quantum cosmology, Arxiv: geo-\nmetric topology, Arxiv: group theory, Arxiv: high energy physics -\nexperiment, Arxiv: high energy physics - phenomenology, Arxiv: high\nenergy physics - theory, Arxiv: learning, Arxiv: materials science, Arxiv:\nmathematical physics, Arxiv: mesoscale and nanoscale physics, Arxiv:\nnuclear theory, Arxiv: number theory, Arxiv: numerical analysis, Arxiv:\noptimization and control, Arxiv: probability, Arxiv: quantum physics,\nArxiv: representation theory, Arxiv: rings and algebras, Arxiv: statistical\nmechanics, Arxiv: strongly correlated electrons\nAffiliation: Al azhar university, Bell labs, Carnegie mellon university,\nCenters for disease control and prevention, Chinese academy of sciences,\nElectric power research institute, Fudan university, Gunadarma univer-\nsity, Harvard university, Ibm, Intel, Islamic azad university, Katholieke\nuniversiteit leuven, Ludwig maximilian university of munich, Max\nplanck society, Mayo clinic, Moscow state university, National scientific\nand technical research council, Peking university, Renmin university of\nchina, Russian academy of sciences, Siemens, Stanford university, Sun\nyat sen university, Tohoku university, Tsinghua university, University\nof california berkeley, University of cambridge, University of oxford,\nUniversity of paris\nTable 11: The sizes for datasets used in supervised classifica-\ntion tasks.\nTask Categories Train Validation Test\nFOS 19 152000 19000 19000\nVenue 30 24000 3000 3000\nAffiliation 30 24000 3000 3000\nTable 12: Details for the CS heterogeneous graph used in the\nlink prediction.\nPapers Authors FOS\nNodes 544244 510189 45717\n1116163 Venues Affiliations\n6934 9079\n#Paper-Author #Paper-FOS #Paper-Venue\n#Edges 1862305 2406363 551960\n6389083 #Author-Affiliation #Paper-Paper #FOS-FOS\n519268 992763 56424",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6904376149177551
    },
    {
      "name": "Inference",
      "score": 0.6522954702377319
    },
    {
      "name": "Language model",
      "score": 0.5850423574447632
    },
    {
      "name": "Knowledge graph",
      "score": 0.5392917394638062
    },
    {
      "name": "Graph",
      "score": 0.4619036316871643
    },
    {
      "name": "Data science",
      "score": 0.4305225908756256
    },
    {
      "name": "Function (biology)",
      "score": 0.4110589325428009
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3818971812725067
    },
    {
      "name": "Theoretical computer science",
      "score": 0.1554095447063446
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [],
  "cited_by": 6
}