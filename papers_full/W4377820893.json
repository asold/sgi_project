{
  "title": "Improving Pre-trained Language Models",
  "url": "https://openalex.org/W4377820893",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5025445408",
      "name": "Gerhard Paaß",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A5080184077",
      "name": "Sven Giesselbach",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3171434230",
    "https://openalex.org/W3047763836",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W3215698779",
    "https://openalex.org/W3099403624",
    "https://openalex.org/W3092683409",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W4280494215",
    "https://openalex.org/W6776048684",
    "https://openalex.org/W2163922914",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W3034782826",
    "https://openalex.org/W2153225416",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2257408573",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W2066636486",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3201763589",
    "https://openalex.org/W3044749682",
    "https://openalex.org/W3106255016",
    "https://openalex.org/W3156665996",
    "https://openalex.org/W2998653236",
    "https://openalex.org/W6761628794",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2626804490",
    "https://openalex.org/W3094152627",
    "https://openalex.org/W4226153346",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3021393704",
    "https://openalex.org/W2906625520",
    "https://openalex.org/W3134171575",
    "https://openalex.org/W4205922070",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W3007684729",
    "https://openalex.org/W3137214022",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W6779941907",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W3145602566",
    "https://openalex.org/W3007007518",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3126960149",
    "https://openalex.org/W3116342879",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W6779590286",
    "https://openalex.org/W3185293939",
    "https://openalex.org/W2962756421",
    "https://openalex.org/W3209374680",
    "https://openalex.org/W4221145950",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W4226275767",
    "https://openalex.org/W6793032698",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W6632455782",
    "https://openalex.org/W6790209031",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2913946806",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W3080997787",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W2612228435",
    "https://openalex.org/W3039017601",
    "https://openalex.org/W3134144764",
    "https://openalex.org/W3190965961",
    "https://openalex.org/W3003265726",
    "https://openalex.org/W2985884876",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2105934661",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W6999611261",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W6766222867",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W6788781968",
    "https://openalex.org/W2341748398",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W6779791889",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3179111421",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W3103801878",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6792108999",
    "https://openalex.org/W2903538854",
    "https://openalex.org/W6676984168",
    "https://openalex.org/W3109919947",
    "https://openalex.org/W6757053730",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W6727099177",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W3193158708",
    "https://openalex.org/W3133264589",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W3174601928",
    "https://openalex.org/W205829674",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2133081131",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W1549465597",
    "https://openalex.org/W2963910749",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2963899988",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W2983915252",
    "https://openalex.org/W4205712089",
    "https://openalex.org/W2155069789",
    "https://openalex.org/W3034255912",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2986922898",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3106504817",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3181186005",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W6796854725",
    "https://openalex.org/W2997517014",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W6741002519",
    "https://openalex.org/W2898856000",
    "https://openalex.org/W4221144042",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3105082862",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2798762751",
    "https://openalex.org/W3200543719",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2911300548",
    "https://openalex.org/W3176214425",
    "https://openalex.org/W2022166150",
    "https://openalex.org/W3114916066",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W3147602080",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W6783267081",
    "https://openalex.org/W3123161422",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W3035990676",
    "https://openalex.org/W3169726359",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W3104058276",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2966892770",
    "https://openalex.org/W2983102021",
    "https://openalex.org/W6801486040",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3198002980",
    "https://openalex.org/W3046415819",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3039883906",
    "https://openalex.org/W4205523161",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W4221141430",
    "https://openalex.org/W2994811754",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W3202105401",
    "https://openalex.org/W2767321762",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W3043372854",
    "https://openalex.org/W4205902686",
    "https://openalex.org/W6784441095",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W3035475042",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2970903692",
    "https://openalex.org/W3000577518",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2935033494",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3166574921",
    "https://openalex.org/W3181262653",
    "https://openalex.org/W3118999024",
    "https://openalex.org/W3041133507",
    "https://openalex.org/W4289828103",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W3035204084"
  ],
  "abstract": "Abstract This chapter describes a number of different approaches to improve the performance of Pre-trained Language Models (PLMs), i.e. variants of BERT, autoregressive language models similar to GPT, and sequence-to-sequence models like Transformers. First we may modify the pre-training tasks to learn as much as possible about the syntax and semantics of language. Then we can extend the length of the input sequence to be able to process longer inputs. Multilingual models are simultaneously trained with text in different languages. Most important is the inclusion of further knowledge into the PLM to produce better predictions. It turns out that by increasing the number of parameters, the size of the training data and the computing effort the performance of the models can always be increased. There are a number of different fine-tuning strategies which allow the model to be adapted to special tasks. In addition, models may be instructed by few-shot prompts to solve specific tasks. This is especially rewarding for larger PLMs, which therefore are called Foundation Models.",
  "full_text": "Chapter 3 \nImproving Pre-trained Language Models \nAbstract This chapter describes a number of different approaches to improve \nthe performance of Pre-trained Language Models (PLMs), i.e. variants of BERT, \nautoregressive language models similar to GPT, and sequence-to-sequence models \nlike Transformers. First we may modify the pre-training tasks to learn as much \nas possible about the syntax and semantics of language. Then we can extend the \nlength of the input sequence to be able to process longer inputs. Multilingual models \nare simultaneously trained with text in different languages. Most important is the \ninclusion of further knowledge into the PLM to produce better predictions. It turns \nout that by increasing the number of parameters, the size of the training data and the \ncomputing effort the performance of the models can always be increased. There are \na number of different ﬁne-tuning strategies which allow the model to be adapted to \nspecial tasks. In addition, models may be instructed by few-shot prompts to solve \nspeciﬁc tasks. This is especially rewarding for larger PLMs, which therefore are \ncalled Foundation Models. \nKeywords Pre-training objective · Input size · Multilingual model · Long \ndependencies · Additional knowledge · Fine-tuning \nThis chapter describes a number of different approaches to improve the performance \nof Pre-trained Language Models (PLMs), i.e. variants of BERT, autoregressive lan-\nguage models similar to GPT, and sequence-to-sequence models like Transformers. \nWhen these models have a large number of parameters, they can be instructed by \ninput prompts to solve new tasks and are called Foundation Models . \n• Modiﬁcation of the pre-training tasks . During pre-training with a large corpus \nthe PLM should learn as much as possible about the syntax and semantics of \nlanguage. By adapting and enhancing the pre-training objectives the performance \nof PLMs can be improved markedly, as shown in Sect. 3.1. \n• Increase of the input size . The length of the input sequence restricts the context, \nwhich can be taken into account by a PLM. This is especially important for \napplications like story generation. Simply increasing input length does not work, \n© The Author(s) 2023 \nG. Paaß, S. Giesselbach, Foundation Models for Natural Language Processing , \nArtiﬁcial Intelligence: Foundations, Theory, and Algorithms, \nhttps://doi.org/10.1007/978-3-031-23190-2_3\n79\n80 3 Improving Pre-trained Language Models\nas then the number of parameters grows quadratically. In Sect. 3.2, alternatives \nfor establishing sparse attention patterns for remote tokens are explored. \n• Multilingual training simultaneously trains the same model in different lan-\nguages. By appropriate pre-training targets the models can generate a joint \nmeaning representation in all languages. Especially for languages with little \ntraining data better results can be achieved Sect. 3.3. \n• Adding extra knowledge. PLMs can be enhanced by including additional \ninformation not covered by the training data. This is important as due to the \nrestricted number of parameters PLMs cannot memorize all details included in \nthe training data. Moreover, strict rules are usually represented only as weak \nassociations and need to be reinforced. By incorporating facts and rules from an \noutside knowledge base (KB) or an additional text collection PLMs can obtain \nnecessary information and keep the content up-to-date, as shown in Sect.\n3.4. \n• Changing the model size. Theoretical results show that model performance \nimproves when the PLMs become larger (Foundation Models). Hence, there is a \ngeneral trend to increase model size, e.g. by forming mixture-of-experts. On the \nother hand, it may be necessary to reduce the computation effort and the memory \nfootprint of a PLM. There are a number of techniques to achieve this without \nsacriﬁcing much performance, as described in Sect.\n3.5. \n• Fine-tuning for speciﬁc applications. This can be performed according to \ndifferent strategies, e.g. with several ﬁne-tuning steps or multiple ﬁne-tuning \ntasks. Larger PLMs usually can be instructed by prompts to perform speciﬁc \ntasks and are called Foundation Models. In addition, few-shot prompts may \nbe optimized to achieve a more adequate model reaction. This is described in \nSect.\n3.6. \nNote that nearly all proposals may be combined for most model types, resulting in \nthe vast number of model variants that is currently discussed. \n3.1 Modifying Pre-training Objectives \nThe basic BERT model [ 49] has two pre-training tasks: the prediction of masked \ntokens with the masked language model (MLM) and next sentence prediction (NSP) \n(Sect. 2.1). These tasks were chosen heuristically and there are many plausible \nloss functions and architectures. Researchers have investigated many alternative \ntraining objectives, model structures, and attention mechanisms. In this section, the \nmost promising of these variations of the BERT and Transformer architecture are \ndiscussed and their relative merits are compared. \nAn important question is the level of aggregation of the input sequence. Here \nsubword tokens are standard. One option is to use raw letters as input. However, \nthis may lead to a high computational burden, as the computational cost of self-\n3.1 Modifying Pre-training Objectives 81\nattention grows quadratically with the size of the input. Another option is the use of \ndomain-adapted knowledge to model the input sequence by learned tokenizations or \npatch embeddings (e.g. for image representation, Sect. 7.2). These methods reduce \nthe input complexity, but may potentially ignore useful information in the input [ 19]. \n3.1.1 Autoencoders Similar to BERT \nTo improve BERT’s performance a number of alternatives to capture knowledge \nfrom the unlabeled data were proposed: \n• RoBERTa dynamically changes masks during training. \n• ALBERT replaces the matrices for self-attention by a matrix product and shares \nparameters across all layers. \n• Predicting single masked tokens can be generalized. SpanBERT masks spans \nof tokens and predicts them. ELECTRA detects randomly replaced tokens at \narbitrary positions. XLNet permutes the order of tokens in a sentence and predicts \ntokens left to right similar to a language model. \n• DeBERTa disentangles the embeddings for content and position. \nThe details are given in the following paragraphs. Popular loss functions are deﬁned \nin Table \n3.1. A list of prominent autoencoders is provided in Table 3.2.T h e y \ncan be compared by their performance on natural language understanding tasks \n(Sect.\n2.1.5) like GLUE [ 218]. \nRoBERTa [ 127] is an enhanced BERT model boosted by tweaking parts of \nthe pre-training process. The authors improved the BERT .BASE architecture by the \nfollowing changes: (1) Instead of using the same mask for all epochs, they replicate \ntraining sequences with different masks. (2) They remove the Next-Sentence-\nPrediction objective and found that performance is best, when all sentences in \na batch are from the same document. (3) Larger batches with larger step sizes \nincrease perplexity for both the masked language model task and downstream task \nperformance. (4) A 10-fold increase of training data to 160 GB, which is used in \nlarge batches. The resulting model achieves an impressive S\nOTA result of 88.5 on \nGLUE (language understanding [ 217]), and the reading comprehension tasks RACE \nand SQuAD [ 173]. \nSpanBERT [ 98] introduces a span-level pre-training approach. Rather than \nmasking single tokens during pre-training, spans of one or more complete words are \nmasked covering about 15% of the tokens. A new span-boundary objective (SBO) \nis introduced, where tokens inside of the masked span are predicted, using only \nrepresentations of the tokens just outside the boundaries of the span combined with \npositional information. The details are shown in Fig.\n3.1. SBO is used together with \nthe usual MLM objective. Finally, the authors omit the next sentence prediction task \nas in [\n127] and only use single text fragments/sentences for training. The authors \nﬁnd that masking random spans is more effective than masking linguistic units. \nSpanBERT has the same conﬁguration as BERT\n.LARGE and is pre-trained on the\n82 3 Improving Pre-trained Language Models\nTable 3.1 Loss functions for PLMs. A sequence is denoted by .x=(x 1,...,x T ) and . z=\n(z1,...,z R) is a related sequence, e.g. a translation \nName Loss function Description \nMC multivariate \nclassiﬁcation \n.LMC =−logp(y|x) For each training instance \n.(x,y) , e.g. logistic classiﬁer, \nSect. 1.3 \nNM neighborhood model .LNM =\n− ∑ T\nt=1\n∑\ni∈N(t) logp(x i|xt )\nFor neighborhood . N(t)=\n.{t−k,...,t−1,t+1,...,t+k} , \ne.g. word2vec, Sect. 1.5 \nLM language model .LLM =− ∑ T\nt=1 logp(x t |x<t ) e.g. RNN Sect. 1.6,G P T \nSect. 2.2.2 \nS2S \nsequence-to-sequence \nmodel \n.LS2S =\n− ∑ nz\nt=1 logp(z t |z<t ,x)\nFor input sequence \n.x=(x 1,...,x T ) and \ntranslation . z=(z 1,...,z R)\nSects. 1.6 and 2.3 \nMLM masked language \nmodel \n.LMLM =\n− ∑\nt∈m(x) logp(x t |˜x)\n.m(x) contains the indices of \nmasked tokens in . x.I n . ˜x the \nmasked tokens are replaced by \nMASK, e.g. BERT, Sect. 2.1 \nTLM translation masked \nlanguage model \n.LTL M =− ∑\nt∈m(x) logp(x t |˜x) .m(x) contains the indices of \nmasked tokens. . ˜x contains a \nsentence and its translation. \nMasked tokens are replaced by \nMASK,e . g .m B E R T ,S e c t .3.3 \nSBO span boundary \nobjective \n.LSMLM =\n− ∑\n(i:j)∈m(x) logp(x i:j |˜x)\n.m(x) contains the spans . (i:j)\nof masked tokens in . x.I n . ˜x the \nmasked tokens are replaced by \nother tokens, e.g. SpanBERT, \nSect.\n3.1.1 \nPLM permutation \nlanguage model \n.LPLM =− ∑ T\nt=1 logp(z t |z<t ) .z=perm(x) is a permutation \nof . x, e.g. XLNet, Sect. 3.1.1 \nNSP next sentence \nprediction \n.LNSP =−logp(ξ|x,z) .ξ=1 if text . z after x (else . z is \nrandomly selected), e.g. BERT, \nSect.\n2.1 \nSOP sentence order \nprediction \n.LSOP =−logp(ξ|x,z) .ξ=1 if text . z after . x (else . x after \n. z), e.g. ALBERT, Sect. 3.1.1 \nRTD replaced token \ndetection \n.LRTD =\n−log ∑ T\nt=1 p(xt =˜xt |˜x)\nIn . ˜x randomly selected elements \nof . x were replaced, e.g. \nELECTRA, Sect. 3.1.1 \nBooksCorpus and the English Wikipedia. SpanBERT achieves a new S OTA of 79.6% \nF1 on the OntoNotes coreference task [ 164], which requires identifying pronouns \nand the corresponding nouns or two phrases referring to the same thing (Sect. 5.4.1).\n3.1 Modifying Pre-training Objectives 83\nTable 3.2 Autoencoders similar to BERT. The pre-training and ﬁne-tuning loss functions are \ndeﬁned in Table 3.1. The benchmark ﬁgures are only a hint, as they depend on the number of \nparameters and the computing effort \nModel Section Pre-training Fine-tuning Extra Benchmark \nELMo [ 156] 1.6 BiLM MC Use bidirectional \nLSTM \nGLUE 71.0 \nBERT [49] 2.1 MLM + NSP MC Predict masked tokens GLUE 80.5 \nRoBERTa [127] 3.1.1 MLM MC Train longer, new \nmask in new epoch \nGLUE 88.5 \nSpanBERT [98] 3.1.1 PLM, SBO MC Predict spans of \ntokens \nGLUE 82.8 \nELECTRA [ 223] 3.1.1 RTD MC Replaced token \ndetection \nGLUE 89.4 \nStructBERT [39] 3.1.1 RTD MC Reorder shufﬂed \ntokens \nGLUE 89.0 \nALBERT [113] 3.1.1 MLM + SOP MC Factorized \nembeddings, \nparameter sharing \nGLUE 89.4 \nXLNET [ 240] 3.1.1 PLM MC Predict permuted \ntokens \nGLUE 90.5 \nDeBERTa [76] 3.1.1 MLM MC, S2S Disentangled attention GLUE 90.0 \nProd. Key [ 112] 3.1.1 MLM MC Nearest neighbor – \nUniLM [ 8] 3.1.3 MLM, LM MC, LM Uni- and bidirectional GLUE 87.3 \nBigBird [ 247] 3.2.1 MLM MC, S2S Sparse attention \nmechanism \nTriviaQA 84.5 \nFig. 3.1 SpanBERT [ 98] concatenates the embeddings outside the border of a span with a position \nembedding. With this input a 2-layer model predicts the probabilities of masked tokens \nStructBERT [ 223] enhances the original BERT MLM objective by the task to \npredict the order of shufﬂed token triples. In addition, the order of three sentences \nhas to be detected. Using models with the same number of parameters, StructBERT \ncan increase the S\nOTA on GLUE in comparison to BERT and RoBERTa to 83.9 and \n89.0, respectively.\n84 3 Improving Pre-trained Language Models\nElectra [ 39] proposes a new pre-training task called replaced token detection \n(RTD). In the paper a generator network, trained with a masked language model \nloss, is combined with a discriminator network. Some tokens in the input sequence \nare replaced with plausible alternatives which are generated by a small language \nmodel (about \n.1/4 of the size of the discriminator). The discriminator network has \nto predict for every token, whether it is a replacement or not. This corruption \nprocedure solves a mismatch in BERT, where \nMASK tokens appear in pre-training \nbut not in ﬁne-tuning. The model learns from all input tokens instead of just the \nsmall masked subset, making it more computationally efﬁcient than e.g. BERT \nand RoBERTa, while performing better on several tasks, e.g. 89.4% on the GLUE \nlanguage understanding task. \nALBERT (a lite BERT) [\n113] uses two parameter-reduction techniques to tackle \nthe huge memory consumption of BERT and its slow training speed. The ﬁrst tweak \nis untying the dimensionality of the WordPiece embeddings from the hidden layer \nsize of BERT. Instead of using a single embedding matrix M, the authors factorize \n.M=A∗B , such that the joint number of parameters in A and B is much lower \nthan the number of parameters in M. The second tweak is sharing all parameters \nacross all layers of BERT, which is shown to stabilize training and keep the number \nof parameters ﬁxed even if more layers are added. In addition to the two tweaks, a \nnew sentence order prediction (SOP) is introduced. Speciﬁcally, the model has to \npredict if the order of two sentences is correct or reversed. The authors report that \nthis task improves accuracy compared to BERT’s NSP task, which could be solved \nby comparing the topics of the two sentences. It is still unclear, however, if this is \nthe best way to incorporate text structure in training. ALBERT achieved new S\nOTA \nresults on GLUE and SQuAD. \nXLNet solves an autoregressive pre-training task instead of predicting masked \nwords [ 240]. This addresses the problem that BERT’s [MASK] token only appears \nduring pre-training and not in ﬁne-tuning. The words in a sequence, e.g. “The. 1\nmouse. 2 likes. 3 cheese. 4”, are reordered together with their position information \n(indices) by a random permutation, e.g. “cheese. 4 The. 1 likes. 3 mouse. 2”. The task \nis to successively predict the tokens in the permuted sequence similarly to a GPT \nlanguage model. The model has to predict, e.g. p( mouse|2, cheese. 4, The. 1, likes. 3). \nNote that the model must additionally know the position, here 2, of the word \nto be predicted. The transformer, however, mixes the position information with \nthe content information by forming a sum. Hence, the position information is \ninseparable from the token embedding. \nTherefore, the authors decided to compute an additional self-attention embedding \ncalled query stream, which as query only receives the target position and then can \ncompute the attention with the key and value vectors (Sect.\n2.1.1). The resulting \nembedding encodes the position of the token to be predicted and correlations to other \ntokens, but has no information on the content of that token. This information can be \nadded as input to the model. The normal self-attention and the query stream have \nthe same parameter matrices Q (query),K (key), V (value). To save training effort, \nXLNet only predicts a few tokens at the end of the permuted sequence. In addition, \nXLNet integrates the segment recurrence mechanism and relative encoding scheme\n3.1 Modifying Pre-training Objectives 85\nof Transformer-XL (Sect. 3.2.2) into pre-training, which empirically improves the \nperformance especially for tasks involving a longer text sequence. \nWhen a token is predicted information about tokens before and after it may \nbe used. Therefore, the model is a bidirectional encoder. With BERT, if the two \ntokens “New” and “Y ork” are masked, both words are predicted independently, \nignoring valuable information. In contrast, XLNet properly handles the dependence \nof masked tokens. XLNet was able to outperform BERT and RoBERTa on many \ntasks, e.g. the GLUE language understanding tasks, reading comprehension tasks \nlike SQuAD (Sect. 2.1.5), text classiﬁcation tasks such as IMDB (movie review \nclassiﬁcation) [ 130]. \nProduct Keys [ 112] replace the dot-product attention by a nearest neighbor \nsearch. A query . qr is split into two sub-queries .q[1]\nr and . q[2]\nr\n. For each sub-query the \nk closest sub-keys .k[1]\ni and .k[2]\nj\nare selected. From the . k2 combinations of sub-keys \nthe highest dot products can be efﬁciently computed and the k highest combinations \nare selected. The results are normalized with the softmax function and used for \nthe computation of a weighted sum of value vectors. During optimization only \nthe k optimal keys are affected reducing the training effort. The approach allows \nvery large transformers to be deﬁned with only a minimal computational overhead. \nWith 12 layers the authors achieve the same performance as a 24 layer BERT \nmodel using only half of the computation time. In a comprehensive comparison \nof transformer architectures [\n142] the approach yields an increase for SuperGLUE \nNLU task (Sect. 4.1.2) from 71.7% for the standard T5 model to 75.2%. \nDeBERTa [76]u s e sa disentangled attention mechanism, where each word is \nrepresented by two different types of vectors encoding content and position. The \nattention weights between tokens are computed using different matrices for content \nand relative position. In addition, DeBERTa includes absolute word positions in \nthe last layer to capture different syntactic roles in the sentence. During ﬁne-\ntuning the model employs an “adversarial” training approach, where embeddings \nare normalized to probability vectors. Then the model is trained to be robust against \nsmall perturbations of embeddings. According to the authors, this improves the \nperformance of ﬁne-tuned models. The large version of the model with 1.5B param-\neters has superior performance in several application areas, e.g. in natural language \nunderstanding (Sect.\n4.1.2), where DeBERTa surpasses the human performance on \nthe SuperGLUE benchmark [ 219] for the ﬁrst time, increasing the macro-average \nscore to 89.9%. \nBengio et al. [ 12] argue that representations, e.g. embeddings, should be disen-\ntangled and should represent different content aspects, e.g. syntax, style, semantics, \nin different parts of the embedding vector. Locatello et al. [\n129]h a v ep r o v e nt h a t \nthis is not possible in an unsupervised way. Hence, some explicit supervision or \nprior information has to be used to generate interpretable subvectors of embeddings. \nDeBERTaV3[75] substitutes the MLM loss of DeBERTa with the replaced token \ndetection (RTD) of Electra (Sect. 3.1.1). In addition, a new gradient-disentangled \nembedding sharing method is employed that improves both training efﬁciency and \nthe quality of the pre-trained model. Its largest version has a 128k-token vocabulary,\n86 3 Improving Pre-trained Language Models\n24 layers, and 304M parameters. For the GLUE benchmark with ﬁne-tuning, the \nmodel increases the score by 1.4% to a new S OTA of 91.4%. The multi-language \nversion of the model mDeBERTa .BASE outperforms XLM-R.BASE by 3.6% in terms \nof the cross lingual transfer accuracy on the XNLI task (Sect. 3.3.1). \n3.1.2 Autoregressive Language Models Similar to GPT \nBy increasing the number of parameters and the training set size the capabilities of \nGPT models can be markedly improved. An overview is given in Table \n3.3. \nGPT-3 [25] is a language model with extreme dimensions. Its largest version has \n96 layers, 96 attention heads, 175 billion parameters and covers sequences of length \n2048. It was trained on a text collection of books, Wikipedia and web pages of \nabout 500 billion tokens. The details of the architecture are not known yet. GPT-3 is \nstructurally similar to GPT-2, and therefore its higher level of accuracy is attributed \nto its increased capacity and higher number of parameters. The model achieved an \nunprecedented performance in language modeling, question answering, etc. Some \nresults are compiled in Table \n3.4 and many more in the paper [ 25]. \nTable 3.3 Autoregressive language models (LM) similar to GPT. ‘Details’ provides the number \nof parameters and speciﬁc features. The ‘benchmark’ ﬁgures are only a hint, as they depend on the \nselected number of parameters and the computing effort. Best benchmark value printed in bold \nModel Section Details Benchmark \nGPT-2 [167] 2.2 1.6B LM to generate text Lambada 0-shot 63.2% \nRetro [ 21] 6.2.3 7B LM with retrieval to generate text Lambada 73.0% \nMegatron-LM [193] 3.1.2 8.3B LM to generate text Lambada 66.5% \nTuring-NLG [179] 3.1.2 17B LM to generate text Lambada 68.0% \nChinchilla [ 83] 3.1.2 70B LM to generate text Lambada 0-shot 77.4% \nGPT-3 [25] 3.1.2 175B long sequence LM to generate \ntext \nLambada 0-shot 76.2% \nWebGPT [25] 6.2.3 175B GPT-3 + Bing search engine Same as GPT-3 \nInstructGPT [ 151] 3.6.5 175B GPT-3 ﬁne-tuned for \ninstructions \nSame as GPT-3 \nOPT [ 151] 3.1.2 free 175B LM similar to GPT-3 Lambada 0-shot 74.7% \nBLOOM [ 151] 3.1.2 176B LM for European languages Lambada 0-shot 67.2% \nPanGu-. α [248] 3.1.2 200B long sequence LM to generate \ntext \nChinese benchmarks \nGopher [ 168] 3.1.2 280B LM to generate text Lambada 0-shot 74.5% \nMT-NLG [4] 3.1.2 530B Megatron variant Lambada 76.6% \nPaLM [ 35] 3.1.2 540B shared key-value projections Lambada 0-shot 77.9% \nGLaM [ 51] 3.5.2 1200B mixture-of-experts LM Lambada 0-shot 73.7% \nWuDao-2.0 [178] 3.5.2 1750B mixture-of-experts LM Lambada: better than \nTuring-NLG\n3.1 Modifying Pre-training Objectives 87\nTable 3.4 Comparing different versions of PaLM, GPT-3, Chinchilla, Gopher, OPT, GLaM, and \nBLOOM on a number of popular benchmarks covering text completion, pronoun coreference, \ncommon sense reasoning and question answering (QA) [\n22, 25, 35, 51]. FLOPS measures the \ncomputational effort in ﬂoating point operations per second. Best benchmark values printed in \nbold \nPaLM PaLM PaLM GPT-3 Chinchilla Gopher OPT GLaM BLOOM \nModel size (billion \nparameters) \n8 62 540 175 70 280 175 1200 176 \nNum. training \nTokens (billion) \n780 795 780 400 1400 300 180 1600 350 \nTraining effort \n(\n.1021 FLOPS) \n37.4 295.7 2527 314.0 588.0 504.0 .≈50 . ≈105\nLambada 0-shot \n(text compl.) \n69.5 75.4 77.9 76.2 77.4 74.5 73.7 67.2 \nHellaSW AG 0-shot \n(text compl.) \n68.7 79.7 83.4 78.9 80.8 79.2 79.0 77.1 73.0 \nPIQA 0-shot \n(common sense) \n77.1 80.5 82.3 80.5 81.8 81.8 78.5 80.4 \nWinogrande 0-shot \n(coreference) \n66.3 77.0 81.1 70.2 74.9 70.1 74.0 73.4 70.1 \nBoolQ 0-shot (QA) 68.3 84.8 88.0 60.5 83.7 79.3 64.0 83.0 \nNatural questions \n0-shot (QA) \n8.4 18.1 21.2 14.6 16.6 10.1 21.5 \nNatural questions \nfew-shot (QA) \n14.6 27.6 36.0 29.9 31.5 24.5 \nTrivia QA 0-shot \n(QA) \n39.5 67.3 76.9 64.3 67.0 52.8 68.0 \nTrivia QA few-shot \n(QA) \n48.5 72.7 81.4 71.2 73.2 63.6 \nAverage task metric 51.2 64.8 69.8 60.7 65.2 59.5 \nGPT-3 is able to generate ﬂuent texts and covers a huge amount of world \nknowledge, as the example in Fig. 3.2 shows. Examples of generated texts can be \nfound in many locations [ 23, 149]. The amount and quality of knowledge captured \nby PLMs is discussed in Chap. 4. In contrast to other language models, GPT-3 \ncan be instructed by a few sentences to perform quite arbitrary tasks (few-shot \nlearning). This is a very simple way to use GPT-3 to solve quite speciﬁc tasks such \nas translating into another language, summarizing a document, correcting grammar, \nwriting an essay on a given topic, etc. Details are discussed in Sect. 3.6.3. \nAt the end of 2021 OpenAI provided an API to ﬁne-tune GPT-3 with user-speciﬁc \ndata [ 123]. In this way, the model can be adapted to a speciﬁc domain language \nand, in addition, be prepared to perform speciﬁc classiﬁcation tasks. In general, this \nyields higher quality results than prompt design. In addition, no few-shot examples \nare necessary anymore. Details of ﬁne-tuning GPT-3 are discussed in Sect.\n3.6.2. \nTable 3.4 compares GPT-3 with other more recent language models on a number of \npopular benchmarks. There is a clear advantage of the new PaLM model.\n88 3 Improving Pre-trained Language Models\nFig. 3.2 Text generated by GPT-3 in response to an input. Quoted with kind permission of the \nauthors [ 25, p. 28] \nGPT-J-6B is an open-source GPT model with 28 layers, 16 heads, a context size \nof 2048, and 6B parameters [ 221]. It has a similar performance as the GPT-3 version \nwith 6.7B parameters. There is an interactive web demo where users can enter \ntheir prompts and a continuation text is generated [ 220]. GPT-Neo [ 16] is another \nfree version of GPT with 2.7B parameters. It was trained on the Pile, a 825 GB \ndata set containing data from 22 diverse sources, including academic sources (e.g. \nArXiv), Internet webpages (e.g. StackExchange), dialogs from subtitles, GitHub, \netc. It outperforms the GPT-3 version with the same parameter size on some natural \nlanguage understanding tasks [\n89]. Recently, GPT-NeoX-20B [ 215] was released. \nIt has 44 layers, an internal vector dimension of 6144, 64 heads and uses batches of \nsize 3.1M for training. In the LAMBADA benchmark (Sect.\n4.1.3) with the task of \npredicting the missing last word of the last sentence of each passage, it achieves an \naccuracy of 72.0%. This value is close to GPT-3 with 75.2%. \nMegatron-LM [193] scale language models such as GPT-2 and BERT efﬁciently \nby introducing intra-layer model parallelism. The authors place self-attention heads \nas well as feed-forward layers on different GPUs, reducing the memory burden \nof a single GPU. They present a GPT-variant with 8.3B parameters and a 3.9B\n3.1 Modifying Pre-training Objectives 89\nparameter model similar to BERT. Highlights of the approach include 76% scaling \nefﬁciency when using 512 GPUs. Their GPT model reduces the WikiText-103 [ 134] \nSOTA perplexity from 15.8 to 10.8 and their BERT model increases RACE (reading \ncomprehension) [ 110] accuracy to 90.9%. \nJurassic-1 [ 122] is an autoregressive language model similar to GPT-3 with \n178B parameters. The authors chose a token vocabulary of 256k instead of 50k for \nGPT-3, which also included frequent multi-word expressions such as named entities \nand common phrases. The training text could be represented with 28% fewer tokens \nthan GPT-3. Hence, the model can process queries up to 1.4 . × faster when using the \nsame architecture. The model used a maximal sequence length of 2048 tokens. In \nspite of the larger vocabulary only 2% of all parameters were required for the input \nembeddings. The model was trained on 300B tokens drawn from public text corpora \nusing a ﬁnal batch size of 3.2M tokens. \nPanGu-. α [248] is a model of Huawei similar to GPT-3 with up to 200B \nparameters. It was trained on 1.1TB Chinese text, and was applied to a large number \nof tasks in zero-shot, one-shot, and few-shot settings without any ﬁne-tuning. The \nmodel has a performance comparable to GPT-3. \nOPT-175B(Open Pre-trained Transformer) [ 253] is a suite of 8 GPT models with \n125M to 175B parameters developed by Meta. It was trained on publicly available \ndatasets with 180B tokens. The largest models has 96 layers, each with 96 heads. \nAlthough OPT-175B has the same parameter count as GPT-3, its training required \nonly 1/7th of computing effort of GPT-3. The model was evaluated on 16 NLP tasks \nand showed approximately the same performance as GPT-3 (Table \n3.4). All trained \nmodels up to 30B parameters are freely available. The large 175B parameter model \nis only available to academic researchers upon request to discourage the production \nof fake news. The model can be trained and deployed on only 16 NVIDIA V100 \nGPUs. Some benchmark results are provided in Table \n3.4. \nBLOOM [139] is an autoregressive large language model with 176B parameters. \nIt has 70 layers with 112 attention-heads per layer and 2048 token sequence length. \nIt was developed by the BigScience initiative of over 1000 AI researchers to provide \na free large language model for everyone who wants to try. Its training data covers \n46 natural languages (English 30%, Chinese 16%, French 12%, Spanish 11%, ...) \nand 11% code (java, php, ...) w i t h 350B tokens. The 176B BLOOM model has \nbeen trained using the Megatron-DeepSpeed library [\n26] offering different types of \nparallelism. The model can be evaluated on 8 large GPUs. Hence, BLOOM is one of \nthe largest trained model available for research purposes. Some benchmark results \nare provided in Table \n3.4. \nGopher [ 168] employed the GPT-2 architecture with two modiﬁcations. For \nregularization the authors used RMSNorm (Sect. 2.4.2) instead of LayerNorm and \nthey employed the relative positional encoding scheme [ 44] instead of absolute \npositional encoding. Gopher has 80 layers with 128 attention heads and 280B \nparameters. All models were trained on 300B tokens with a context window of \n2048 tokens and a batch size of up to 6M tokens. For the large models a 16 bit \nﬂoat numbers was used to reduce memory and increase training throughput.\n90 3 Improving Pre-trained Language Models\nSix model versions with different numbers of parameters were trained to assess \nthe effect of model size. The authors present a comprehensive evaluation on 152 \ntasks described in Table 4.3. Gopher shows an improvement on 100 of 124 tasks. \nOne of these is the LAMBADA benchmark [ 154] where Gopher generates a zero-shot \nscore of 74.5, which is only slightly below the value 76.6 of MT-NLG model with \n530B parameters [\n106]. For instance Gopher achieves S OTA for all 12 benchmarks \non humanities covering areas like econometrics and psychology surpassing the best \nsupervised results for 11 benchmarks. Some results are provided in Table 3.4 while \nSect. 4.1.4 describes more details. \nChinchilla [ 83] is a mid-size encoder model with 70B parameters, which has \nthe same compute budget as the larger Gopher model, but four times as much \ndata. Chinchilla consistently has a better performance than Gopher (Table 3.4) and \nsigniﬁcantly outperforms GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing \nNLG (530B) on a large set of downstream evaluation tasks. For every doubling of \nmodel size the number of training tokens should also be doubled. This is a much \nlarger scaling rate than that predicted by Kaplan et al. [\n102] in Sect. 3.5.1. \nTuring-NLG [179] introduces an autoregressive language model with 78 trans-\nformer layers, a hidden vector-size of 4256, 28 attention heads and 17B parameters. \nAs a model with more than 1.3B parameters cannot ﬁt into a single GPU with \n32 GB memory it must be parallelized, or broken into pieces, across multiple GPUs. \nTuring-NLG leverages a S\nOTA Deep Learning hardware with high communication \nbandwidth, the Megatron-LM framework, and the DeepSpeed library, which further \noptimizes the training speed and reduces the resources needed. The model achieved \nSOTA performance on language modeling tasks and also proved to be effective for \nzero-shot question answering and abstractive summarization. \nIts successor MT-NLG [ 4] is a 105-layer encoder model with 530B parameters \nand was trained across 280 GPUs with a huge batch size of 1920. Similar to GPT-\n3 it improves performance on zero-, one- and few-shot tasks. For the LAMBADA \nbenchmark [\n154], for example, the model has to predict the last word of paragraph \n(Sect. 4.1.3). On this benchmark MT-NLG improves the few-shot accuracy of GPT-\n3 (86.4%) to the S OTA 87.2%. \nPaLM [35] is an autoregressive language model developed by Google with 540B \nparameters. It has 118 layers, 48 heads and an input sequence length of 2048. \nThere are also smaller versions with 8B and 62B parameters. It uses a standard \nautoregressive decoder with SwiGLU activation function and shared query-value \nprojections for the heads of a layer, which improves autoregressive decoding speed. \nThe model is trained on a high-quality dataset with 780B tokens, where sloppy \nand toxic language have been ﬁltered. Each training example is used only once. \nThe training set contains social media conversation (50%), multilingual web pages \n(27%), books (13%), source code ﬁles (5%), multilingual Wikipedia articles (4%), \nand news articles (1%). Training required 3072 TPU chips for 1368 h, resulting in a \ntotal emission that is 50% higher than the emissions for a direct round-trip ﬂight in \nan aircraft between San Francisco and New Y ork [\n35, p. 18]. \nPaLM was evaluated on hundreds of natural language inference, mathematical, \nreasoning and knowledge intensive tasks and achieved S OTA accuracy in the large\n3.1 Modifying Pre-training Objectives 91\nFig. 3.3 Evaluation of PaLM, GPT-3, Gopher, and Chinchilla (left). Previous models were only \nevaluated on a subset of tasks, so this graph shows the aggregated results on the 58 tasks where all \nthree models have been evaluated [\n35]. The medium accuracy of PaLM is better than the average \nperformance of humans. The right side shows the results for four speciﬁc BIG-tasks. A detailed \ncomparison between the performance of three PaLM models of different size as well as human \nlevels is presented in [\n35,p .1 5 f ] \nmajority of benchmarks, e.g. in 28 of 29 most widely evaluated English language \nunderstanding benchmarks (cf. Table 3.4). This demonstrates that the scaling effects \ncontinue to hold for large Foundation Models. Figure 3.3 shows the results on BIG-\nbench data compared to prior models. PaLM 540B 5-shot outperforms the prior \nS\nOTA on 44 out of the 58 common tasks, and on average is signiﬁcantly better \nthan the other models (Gopher, Chinchilla, GPT-3). Moreover, PaLM 540B 5-shot \nachieves a higher score than the average score of the humans asked to solve the same \ntasks. When ﬁne-tuned on SuperGLUE, the model outperforms the best decoder-\nonly model and is competitive with encoder-decoder models, which in general \nperform better for ﬁne-tuning. A signiﬁcant number of tasks showed discontinuous \nimprovements from model scale, meaning that the performance improvement from \nthe smaller version to the largest model was higher than expected. \nPaLM has been ﬁne-tuned on program code documents. The resulting model is \ncalled PaLM-Coder [ 35, p.23]. The quality of the code is measured by the pass@k \nmetric, in which for each problem in the test set, k samples of source code are \ngenerated by PaLM-Coder, and a problem is counted as solved if any sample solves \nthe problem. PaLM-Coder is able to solve a number of benchmark tasks with about \na pass@1-value of about 50. There is an elaborate evaluation of the properties of the \nPaLM-Coder model.\n92 3 Improving Pre-trained Language Models\nFig. 3.4 Few-shot example of a chain-of-thought prompt for a common sense question-answering \ntask [ 35, p. 38]. The same two example chains of thought were combined with different prompts \nrequiring an answer \nFor about a quarter of tasks the authors observe a discontinuous jump in accuracy, \nif the model is increased from 58B to 540B parameters, far exceeding the ‘power \nlaw’ postulated by Kaplan et al. [ 102] (Sect. 3.5.1). Examples are ‘english proverbs’ \nand ‘logical sequence’ shown in Fig. 3.3. This suggests that new abilities of PLMs \ncan evolve when the model reaches a sufﬁcient size, and that these abilities also \ndevelop beyond the model sizes studied so far. \nThe training data contains 22% multilingual documents. For translation between \ndifferent languages, the few-shot PaLM model comes close to or even exceeds the \nﬁne-tuned S\nOTA. For English-French translation, Palm 540B few-shot achieves 44.0 \nBLEU compared to a S OTA of 45.6. For German-English, PaLM 540B few-shot \nreaches 47.5 B LEU vs. a 45.6 B LEU SOTA. For other tasks like summarization and \nquestion answering, Palm 540B few-shot comes close to the ﬁne-tuned models, and \ncan outperform them in a few cases. \nReasoning with a number of intermediate steps was always difﬁcult for language \nmodels. Recently chain-of-thought prompting (Sect. 3.6.4) was proposed which \nadds intermediate reasoning steps [ 226] into the few-shot prompts (Fig. 3.4). \nFollowing this recipe, the PaLM model similarly produces its own intermediate \nsteps for a multistep problem before giving the ﬁnal answer. This leads to a boost in \nperformance for a number of benchmark tasks. Using this technique PaLM is even \nable to explain jokes, as Fig.\n3.5 demonstrates.\n3.1 Modifying Pre-training Objectives 93\nFig. 3.5 By using thought-chain-prompts PaLM can explain jokes [ 35] \n3.1.3 Transformer Encoder-Decoders \nThe Transformer encoder-decoder [ 212] was pre-trained with a translation task \n(Sect. 2.3). To improve performance a number of alternatives were proposed: \n• Different targets to restore corrupted pre-training data are proposed by MASS, \nBART and PEGASUS. Examples are predicting masked spans, ordering per-\nmuted sentences, or inserting omitted tokens. \n• T5 formulates many language understanding and language generation tasks as \ntext translations and handles them with the same model. \n• Longformer, Reformer and Transformerl-XL extend the size of the input text \nwithout increasing the number of parameters. They are discussed in Sect. 3.2. \nThe details are given in the following paragraphs. A representative list of trans-\nformer encoder-decoders is provided in Table \n3.5. \nMASS [ 196] is based on the transformer architecture. In contrast to the original \ntransformer, a sequence of consecutive tokens in the encoder is masked and the \ndecoder’s task is to predict the masked tokens recursively (Fig.\n3.6). Therefore, \nMASS can jointly train the encoder and decoder to develop the capability of \nextracting embeddings and language modeling. MASS is ﬁne-tuned on language \ngeneration tasks such as neural machine translation, summarization and con-\nversational response generation. It shows signiﬁcant performance improvements \ncompared to prior transformer architectures. \nBART [\n119] uses a standard Transformer-based encoder-decoder architec-\nture. The pre-training task is to recover text corrupted by a number of different \napproaches (Fig.\n3.6): predict masked tokens as with BERT; predict deleted tokens \nand their positions, predict the missing tokens replaced by a single mask, reconstruct \na permuted sentence as with XLNet, and ﬁnd the beginning of a rotated document. \nBART was ﬁne-tuned on a number of tasks like GLUE, SQuAD, summarization, \nand machine translation. BART achieved the best performance with the prediction \nof missing tokens replaced by a single mask. A large version of BART was trained\n94 3 Improving Pre-trained Language Models\nTable 3.5 Transformer encoder-decoders. The pre-training and ﬁne-tuning loss functions are \ndeﬁned in Table 3.1. Benchmarks: En-De WMT2014 English-to-German BLEU, GLUE Sect. 4.1.1 \naccuracy, SuperGLUE Sect. 4.1.2 accuracy, TriviaQA [ 99] Sect. 6.2.1 accuracy, Penn Treebank \n[136] perplexity. The benchmark ﬁgures are only a hint, as they depend on the number of \nparameters and the computing effort \nModel Section Pre-training Fine-tuning Extra Benchmark \nTransformer [212] 2.3 S2S S2S Predict translated \ntokens \nEn-De 26.4 \nUniLM [ 8] 3.1.3 MLM, LM MC, LM Uni- and \nbidirectional \nGLUE 87.3 \nMASS [ 196] 3.1.3 S2S S2S Predict masked \ntokens \nEn-De 28.3 \nBART [119] 3.1.3 DAE MC, LM, S2S Restore corrupted \ntext \nGLUE 88.4 \nT5 [ 170] 3.1.3 S2S MC, LM, S2S Solve many NLP \nt a s k sa sS 2 S \nproblems \nGLUE 89.7 \nGLM [ 54] 3.1.3 LM LM Solve all task by \nautoregressive \nprediction \nSuperGLUE \n82.9 \nLongformer [ 10] 3.2.1 MLM, S2S LM, MC, S2S Sparse attention \nmechanism \nTriviaQA \n77.3 \nReformer [ 108] 3.2.2 LM, S2S LM, MC, S2S Locality-sensitive \nhashing, reversible \nresidual layers \nEn-De 29.1 \nTransformer-XL [44] 3.2.2 MLM, S2S MC, S2S Sparse attention \nmechanism \nPenn-Tree \nBank 54.5 \nFig. 3.6 Different pre-training tasks to restore corrupted text by the transformer. Span masking is \nthe task for MASS [\n196]. BART uses all tasks from token masking to document rotation [ 119] \nwith a hidden size of 1024 and 12 encoder and decoder layers with a similar dataset \nas used by RoBERTa. The resulting performance was similar to that of RoBERTa. \nFor abstractive summarization, e.g. on the CNN/Daily Mail benchmark [ 78], BART \nachieves S OTA.\n3.1 Modifying Pre-training Objectives 95\nFig. 3.7 Every task in T5 is expressed as a translation task, where the type of the task is a preﬁx \nto the input text (on the left) and the model produces the corresponding output (right) . Adapted \nfrom [\n170, p.3] with kind permission of the authors \nPEGASUS [ 251] proposed pre-training large Transformer-based Seq2seq mod-\nels on massive text corpora with a new objective: gap-sentences generation, where \nsentences instead of tokens are masked or removed. The model has to generate these \nmodiﬁed parts as a one sentence output. On 12 document summarization tasks the \nmodel achieves S\nOTA performance. \nT5 [ 170] is based on the standard transformer architecture. Pre-training is \nperformed on a huge training set by restoring corrupted texts, which is formulated as \na sequence-to-sequence tasks. The comparison of different pre-training tasks listed \nin Fig. 3.6 found that, similar to BART, text inﬁlling achieves the best results. If \nthe original text is “Thank you for inviting me to your party last week .” the model \nreceives the input “Thank you [X] me to your party [Y] week .” with masked phrases \nand has to generate the output “[X] for inviting [Y] last [Z]” to reconstruct the \nmasked phrases. \nSalient span masking [ 72] was especially effective. To focus on relevant phrases \na BERT-tagger was trained to recognize named entities (person names, locations, \netc. Sect.\n2.1.3), and dates were identiﬁed by regular expressions. If the model \nhad to recreate these spans the model performance was signiﬁcantly increased. By \npredicting the omitted tokens, the model is able to collect an enormous amount of \ninformation on syntactic and semantic knowledge. Extensive comparisons show that \nthe sequence-to-sequence architecture yields better results than other architectures, \ne.g. autoregressive language models. \nT5 is pre-trained on a multitask mixture of unsupervised and supervised tasks \nusing a training dataset of 750 GB of cleaned English web text. Its largest version \nhas 24 layers, 128 attention heads, and 11B parameters. For each task the data is \nconverted into a text-to-text format (Fig.\n3.7). The model achieves S OTA results on \nmany benchmarks, for example summarization, question answering, text classiﬁca-\ntion, and more. The results for GLUE is 90.3% [\n11].\n96 3 Improving Pre-trained Language Models\nPrimer [ 195] proposes two modiﬁcations of the original self-attention architec-\nture. First the ReLU activation function is squared. In addition, a convolution layer \nis added after each of the multi-head projections for query Q,k e y K, and value V. \nFor the original T5 architecture this reduces the training cost by a factor 4. \nUniLM2 [ 8] simultaneously pre-trains a bidirectional language models and a \nsequence-to-sequence model for language generation. The model parameters are \nshared between the two tasks, and the encoding results of the context tokens are \nreused. The model uses two mask types, one for bidirectional masking similar to \nBERT and pseudo masks for language modeling. With special self-attention masks \nand position embeddings, the model can perform both language modeling tasks \nin one forward pass without redundant computation of context. The model beats \nBART\n.BASE for reading comprehension on SQuAD 1.1 and T5 .BASE for abstractive \nsummarization on CNN/Daily Mail. \nGLM (General Language Model) [ 54, 55] is a successor of UniLM2 aiming to \ncombine the different learning paradigms of BERT, GPT and the transformer. For \npre-training GLM has the task to generate multiple text spans in an autoregressive \nway basically using the GPT architecture. From the input text \n. x=(x 1,...,x T )\na number m spans .xi1 ,...,x i1+li are sampled. Each span is replaced with a single \n[MASK] token yielding the corrupted input .xcorrupt. The model then successively \ngenerates the tokens of the spans having access to the corrupted input and the \nalready generated tokens of the spans (Fig.\n3.8). Within the input text all tokens \nare connected by self attention while in the output section a masked self-attention \nis used. Each span is ﬁnished by an [END] token. To identify the positions of \ngenerated tokens two positions are encoded by embeddings: the input position and \nthe position within a span. Note that the mask prediction can be done in arbitrary \nsequence and the model has to predict the length of the spans during reconstruction. \nFor ﬁne-tuning, text classiﬁcation tasks are converted to word predictions. To \nassess the sentence “The waiters were friendly.” in a sentiment classiﬁcation task \nFig. 3.8 During pre-training GLM has the task to reconstruct masked single words or multi-word \nphrases. The position of generated words in the text and in the masks are indicated by position \nembeddings, which are added to the token embeddings. The generated answers are terminated by \nan \n[END] token [ 54]\n3.1 Modifying Pre-training Objectives 97\nthe input is extended to “The waiters were friendly. It’s really [MASK].” where \n[MASK] has to be replaced by “good” or “bad”. For a text generation task \na [MASK] token is appended to the input text. Then the model generates the \ncontinuation as the output text in an autoregressive way. In contrast to BERT the \nmodel observes the dependency between masked tokens yielding more consistent \npredictions. In comparison to XLNet no additional attention for position encoding \nis needed reducing the computational requirements. Compared to T5, GLM predicts \nthe spans in arbitrary order and requires fewer extra tokens. \nTo evaluate the model performance, Du et al. [\n54]t r a i n G L M .BASE and \nGLM.LARGE with the same training data and parameter counts (110M and 340M) \nas BERT.BASE and BERT.LARGE. For both model conﬁgurations, GLM outperforms \nBERT on SuperGLUE (Sect. 4.1.2), e.g. GLM .LARGE has an average score of 77.0 \ncompared to 72.0 for BERT .LARGE. On a larger pre-training dataset for a model \nwith the same size as RoBERTa they yield an average SuperGLUE score of 82.9 \ncompared to 81.5 for RoBERTa. They show that by multitask learning, a single \nmodel with the same parameters can simultaneously achieve higher accuracy in \nNLU, generating text given an input, and solve other tasks such as summarization \n[\n53]. \nLarger models like GLaM [ 51] and WuDao-2.0 [ 257] have a mixture-of-experts \narchitecture and are described in Sect. 3.5.2. \n3.1.4 Systematic Comparison of Transformer V ariants \nAs an example of a fair comparison of architectural features, we report the following \nexperimental analysis of PLMs, where Narang et al. [\n142] evaluated the effect of \na number of transformer modiﬁcations. The following transformer features were \ninvestigated: \n• Activation functions: In addition to the ReLU-activation in the feedforward layers \n11 different activations functions were assessed. \n• Normalization: Together with the original layer normalization, ﬁve different \nregularization techniques were explored. \n• Number of layers: The number . dL of layers was varied between 6 and 24. To keep \nthe comparison fair, the number of parameters was held constant by varying the \nnumber\n. dH of heads and the widths . dff of internal embeddings. \n• Token embeddings: The original transformer embeddings were compared to ﬁve \nvariants of factored embeddings. In addition, the sharing of transformer blocks \nwas investigated. \n• Softmax: The standard softmax to compute token probabilities was contrasted to \nthree softmax variants. \n• Architecture: The authors compared the base transformer with 17 other architec-\ntures. In most cases, the number of parameters was kept about the same.\n98 3 Improving Pre-trained Language Models\nThe authors evaluated the variants in two settings: Transfer learning based on the \nT5 transformer (Sect. 3.1.3) and supervised machine translation on the WMT2014 \nEn-De [ 17]. With some caution, the results can also be applied to other types of \nPLMs like BERT and GPT. \nEach architecture variant of T5 was pre-trained on the C4 dataset [ 171]o f \n806 GB using the “span corruption” masked language modeling objective. Subse-\nquently, T5 was ﬁne-tuned on three tasks: the SuperGLUE language understanding \ntask [219], theXSumabstractive summarization dataset [ 143], and the WebQuestions \nbenchmark [ 13], where no additional knowledge was provided as background \ninformation. The computing effort and the number of parameters for each model \nwas ﬁxed to the same level. An exception was an architecture with signiﬁcantly \nfewer parameters, which was trained for longer. \nSeveral activation functions achieve a better performance compared to the \nReLU activation, especially SwiGLU and GEGLU, which are gated linear units \n(GLU) forming a product with another activation [ 189]. The improvement can be \nobserved for pre-training, ﬁne-tuning, and supervised training without affecting the \ncomputation time. For SuperGLUE, for instance, an increase from 71.7% to about \n76.0% can be observed. Replacing layer normalization with RMS normalization \n[249] causes performance gains for all tasks. The SuperGLUE score, for example, \nwas improved from 71.7% to 75.5%. In addition, the training speed was higher. \nAs expected, increasing the depth of a models usually led to a better performance \neven if the number of parameters is kept constant. On SuperGLUE the model with \n18 layers achieved a score of 76.5% compared to 71.7% for the base model. Similar \nimprovements can be observed for WebQuestions and translation, while there were \nno improvements for the summarization task. This is in line with theoretical results \n(Sect.\n3.5.1). A drawback is that deeper models require more computation time. \nArchitectures, which share parameters in different layers, usually lead to a \ndecreased performance. The effect of using the same embeddings for encoders \nand decoders is mixed. Factorization of embeddings into a matrix product usually \ncause inferior results. If a Mixture of Softmaxes [ 239] is used to predict the output \nprobabilities, the performance usually is better, e.g. an increase to 76.8% for \nSuperGLUE. However, this approach requires up to 40% more computation effort. \nOf the architectural variants evaluated, two combinations of the Synthesizers with \ndot-product attention (Sect. 3.2.2) perform better than the standard Transformer. \nThe Synthesizers do not compute a “correlation” of embeddings but determine \nthe attention weights from a single embedding or randomly. Switch Transformer, \nMixture-of-experts, and Product key memories all have signiﬁcantly more parame-\nters than the baseline transformer but are able to improve performance. The Switch \ntransformer ([\n56] Sect. 3.5.2) has many more parameters than the base T5 model. \nTo reach the same performance as Switch, T5 needs seven times more training \nFLOPS (ﬂoating point operations per second). The Mixture-of-experts model [ 116] \ndistributes computations to 2 expert models in both the encoder and the decoder. \nProduct key memory ([ 112] Sect. 3.1.1) replaces the dot-product attention by a \nnearest neighbor search.\n3.1 Modifying Pre-training Objectives 99\nFor all other 12 architectures, there were no improvements over the standard \ntransformer [ 142]. This is different to the ﬁndings of the papers proposing the mod-\nels. A reason seems to be that changes of the transformer architecture are difﬁcult to \ntransfer to other code bases and applications. Therefore, the authors propose to try \nout new modiﬁcations on different low-level implementations. In addition, a new \napproach should be evaluated on a variety of downstream applications including \ntransfer learning, supervised learning, and language modeling. Hyperparameter \noptimization should be kept ﬁxed to assure the robustness of the approach. Finally, \nthe mean and standard deviation of results should be reported to avoid the selection \nof a single best result. \n3.1.5 Summary \nThe modiﬁcation of pre-training tasks has a profound inﬂuence on the performance \nof PLMs. Many different types of pre-training losses have been evaluated, such as \nmasked phrase prediction, replaced token detection, or sentence order recognition. \nAccording to the benchmarks, the prediction of permuted tokens by XLNET is \nespecially rewarding because XLNET takes into account the dependency between \nmasked tokens. In addition, DeBERTa’s disentangled token and position embed-\ndings are able to boost the performance in downstream classiﬁers. With respect \nto applications, autoencoders like BERT are particular important for information \nextraction in Chap.\n5. \nFor autoregressive PLMs like GPT, a number of variants with larger model \nsize and larger training data have been presented. However, in most cases, the \npre-training tasks were not changed. The training of the larger models required \nimprovements in the parallel computing infrastructure and resulted in an unprece-\ndented performance in text generation. By creating custom start texts (prompting), \nthe models can solve a large number of speciﬁc tasks with very high accuracy \nwithout further ﬁne-tuning (Sect.\n3.6.3). The amount and quality of knowledge \ncaptured by PLMs is surprisingly high and is discussed in Chap. 4. In terms of \napplications, autoregressive PLMs are used in particular for text (Chap. 6) and image \ngeneration (Sect. 7.2). Because of their versatility and the tremendous increase in \nperformance, recent large-scale PLMs are called Foundation Models. \nEncoder-decoder transformers were introduced for translating a text from one \nlanguage to another. A number of new pre-training tasks were evaluated for these \nmodels. Some of them are similar to the tasks for autoencoders, such as predicting \nmasked spans or inserting omitted tokens. Others were adapted to the input-\noutput architecture, e.g. the reconstruction of sentence permutations and document \nrotations. Here BART and T5 achieved the best performances in the GLUE and \nSuperGLUE natural language understanding tasks. By creating additional synthetic \ntraining examples, the performance of T5 and other models can be increased \n(Sect.\n3.6.6).\n100 3 Improving Pre-trained Language Models\nA systematic comparison of transformer architectures demonstrated that several \narchitectural changes increased performance. The SwiGLU and GEGLU activation \nfunction instead of ReLU increased accuracy for SuperGLUE by more than 4%. \nSimilar gains were observed when using RMS normalization instead of layer \nnormalization. Increasing the model depth resulted in better performance even when \nthe number of parameters was held constant. Synthesizers, mixtures-of-experts, and \nProduct keys replacing scalar products by k-means clustering also performed better \nthan the standard transformer. \nT5 and GLM demonstrate that transformers, controlled by instructive prompts, \ncan be used to solve arbitrary problems of text classiﬁcation, text generation, and \ntext translation. They thus combine the capabilities of BERT, GPT, and translation \nmodels. Transformers are used extensively in complex text generation tasks, e.g. \nmachine translation (Sect.\n6.3), dialog (Sect. 6.6), and image generation (Sect. 7.2). \n3.2 Capturing Longer Dependencies \nA well-known concern with self-attention is the quadratic time and memory com-\nplexity, which can hinder the scalability of the model in many settings (Sect. 2.1.6). \nIf the sequence length T is increased to 2T then four times as many associations \n(attentions) between tokens have to be computed. This limits the direct applicability \nof models when a task requires larger contexts, such as answering questions or \nsummarizing a document. Moreover, a larger memory is required to store the \nattentions for training. Therefore, a number of concepts have been proposed to cover \nlong sequences without excessive computational and memory demands. \n• Sparse attention matrices are employed by BigBird, the Sparse Transformer, \nLongformer, and GPT-3 to reduce the number of parameters. \n• Clustering tokens by locality-sensitive hashing reduces the number of attentions \ncomputed by the Reformer. \n• Low-rank-approximation of attention matrices or by a kernel-based formulation \nof self-attention decreases the number of parameters of the Performer and the \nLinear Transformer. \n• Transformer-XL and the Linear Transformer reuse computations from previous \ntext segments in an autoregressive manner to lower computational overhead. \nSurveys of techniques for enlarging the input sequence are provided by Tay et al. \n[\n207] and Fournier et al. [ 59]. \n3.2.1 Sparse Attention Matrices \nBigBird [ 247] reduces the number of attention computations by omitting entries \naccording to some pre-determined pattern from the matrix of attention relations.\n3.2 Capturing Longer Dependencies 101\nFig. 3.9 Attention mechanism used in BigBird [ 247] to compute the association between input \ntokens. Matrix indicating attention between pairs of tokens: attentions between sequence neighbors \n(left), global attentions to a few tokens (second left), random attentions (third from left), the \ncombined BigBird attentions (right). White blocks indicate omitted attention pairs \nBigBird extends transformer-based models, e.g. BERT, and uses a set of g global \ntokens attending on all tokens of the sequence. In addition, each token . vt attends to \na set of . nl local neighboring tokens and to a set of . nr random tokens. The resulting \nassociation matrices are shown in Fig. 3.9. If the numbers g, . nl, and . nr do not \nincrease with sequence length T the number of attentions grows linearly with T. \nThe model is constructed in such a way that the length of the path between \narbitrary token pairs along intermediate tokens is kept small, as in a small-world \ngraph. The authors prove that their model allows to express all continuous sequence-\nto-sequence functions with only \n.O(T) inner products (Table 3.6). In addition, \nthey show that under standard assumptions BigBird is Turing complete, i.e. can \nperform arbitrary computations (see also [\n246]). The BigBird attention module can \nbe used in BERT, autoregressive language models, and Transformer architectures. \nIn a number of applications BigBird using a sequence length of 4096 is able to \nimprove the S OTA, e.g. for question answering requiring multi-hop reasoning from \nthe given evidences. Note that BigBird without random attention performed better \nthan BigBird with random attention in a set of experiments. \nPrior models using these concepts were the Sparse Transformer [ 33] and the \nLongformer [ 10], which similarly to WaveNet [ 148] employ strided or “dilated” \nneighborhoods. Here not all adjacent neighbors are attended by a token, but only \nevery d-th neighbor with .d>1 .I f k layers are used, this construction covers . dk\nneighbors and thus allows associations over large distances. The Extended Trans-\nformer Construction (ETC) model [ 3] generalizes the idea of global tokens, which \ncan communicate associations between far-away tokens of the whole sequence. \nGPT-3 [25] (Sect. 3.1.2) is a recent language model with 96 layers, 96 attention \nheads, 175 billion parameters covering sequences of length 2048. To cope with the \nexcessive sequence length the authors used “alternating dense and locally banded \nsparse attention patterns in the layers of the transformer, similar to the Sparse \nTransformer” [\n33]. The details of the architecture are not yet known. The model \nachieved an unprecedented performance in language modeling, question answering, \netc., which is discussed in Sect.\n3.6.3.\n102 3 Improving Pre-trained Language Models\nTable 3.6 Important models with sparse self-attention for long dependencies. T is the sequence \nlength, g number of global tokens, k is window size. (cf. [ 207]) \nComplexity Low Sparse/random Learnable \nModel O(·) rank/Kernels Recurrence Memory patterns patterns \nTransformer-XL \n[44] \nT 2 – X – – – \nReformer [ 108] T log T – – – – X \nRouting \ntransformer \n[\n180] \nT log T – – X – X \nCompressive \ntransformer \n[\n169] \nT 2 – X X – – \nETC [ 3] g2 + Tg – – X X – \nGPT–3 [25] T\n√\nT – – – X – \nPerformer [ 34] T X – – – – \nLinear \ntransformer \n[\n105] \nT X – – – – \nBigBird [ 247] T – – X X – \nS4 [ 68] T X – – – – \n3.2.2 Hashing and Low-Rank Approximations \nThe Reformer [ 108] introduces locality-sensitive hashing to cluster tokens with \nsimilar key/query vectors. This approach hashes similar input items into the same \n“buckets” with high probability. For each cluster the same query/key parameters are \nused. In this way, tokens are aggregated in a data-driven fashion. In a similar way, \nthe Routing Transformer [ 180] clusters tokens by k-means clustering. \nTransformer-XL [ 44] reuses computation results from prior segments of a \nsequence. With this recurrence mechanism applied to every two consecutive \nsegments of a corpus, it essentially creates a segment-level recurrence in the hidden \nstates. With multiple layers, the effective context being utilized can go way beyond \njust two segments. A similar approach is used by the Compressive Transformer \n[\n169]. Segatron is a variant that encodes a paragraph index in a document, a sentence \nindex in a paragraph, and token index in a sentence as embeddings to be added to \nthe token embedding. This modiﬁcation leads to a better perplexity in language \nmodeling. \nThe P erformer [ 34] reduces the computational load by employing low rank \napproximations of the self-attention matrix. It uses a random kernel with positive \northogonal random features to compute the self-attention. By orthogonality, the \nauthors avoid computing the full square matrix of products, since the dot product \nof orthogonal features is 0. Hence, computation requirements grow linearly with \nsequence length. The authors are able to prove that their model allows nearly-\n3.2 Capturing Longer Dependencies 103\nunbiased estimation of the full attention matrix as well as uniform convergence and \nlower variance of the approximation. \nThe Linear T ransformer [ 105] also uses a kernel-based formulation of self-\nattention reducing complexity to linear. For predicting the future elements from past \ninputs, the authors are able to construct an iterative algorithm similar to RNNs that \nis dramatically faster than standard transformers. The model has been shown to \nimprove inference speeds up to three orders of magnitude without much loss in \npredictive performance. \nThe T ransformer-LS (Long-Short Transformer) [ 258] has a local sliding win-\ndow attention between neighboring tokens and a long-range attention with dynamic \nprojections to represent relationships between distant tokens. The dynamic low-rank \nprojections depends on the content of the input sequence. The authors claim that the \napproach is more robust against insertion, deletion, paraphrasing, etc. The scheme \nachieves S\nOTA perplexities in language modeling for different benchmarks, e.g. 0.99 \nfor enwik8 and S OTA results as vision transformer on ImageNet. \nThe Combiner [ 174] represents groups of embeddings by key vectors. The \nprobability that a given token . vt attends to a token . vs is described by a product, \nwhere . vt ﬁrst attends to the key vector that represents a group of locations containing \n. vs multiplied by the probability of choosing . vs within that group. In this way, \nthe Combiner can be applied to sequences of length up to 12,000. The approach \nis able to achieve S\nOTA perplexity on large benchmarks. In addition, it improves \nthe average performance on the Long Range Arena benchmark [ 209] speciﬁcally \nfocused on evaluating model quality for long documents. \nThe Synthesizer [ 206] replaces the pairwise dot products of attention with \n“synthesizing functions” that learn attention matrices, which may or may not depend \non the input tokens (cf. Sect. 3.1.4). In the Dense Synthesizer, each token embedding \n. xi, .i=1,...,T , in a layer is projected to a vector of the length T using a \ntwo-layered nonlinear feed-forward network with a ReLU activation. The values \nof this vector are used as weights to determine the mixture of values to form the \noutput embedding. Hence, no “correlations” between embeddings are computed to \ndetermine their similarity, as it is done for the standard self-attention. There is an \nextreme variant, where the mixing proportions are set randomly. Nevertheless, on \nmultiple tasks such as machine translation, language modeling, dialogue generation, \nmasked language modeling and document classiﬁcation, this “synthetic” attention \ndemonstrates competitive performance compared to vanilla self-attention. The \ncombination of Random Synthesizers with normal dot-product attention is able to \nbeat T5 on several benchmarks. \nThe P erceiver [ 93] deﬁnes an asymmetric attention mechanism iteratively \nconverting the long input sequence .x1,...,x T (e.g. the 50k pixels of an image) into \na shorter sequence of latent units .u1,...,u n (e.g. .n=512 ) that form a bottleneck \nthrough which the inputs must pass (Fig. 3.10). With cross-attention (Sect. 2.3.1) \nthe Q-transformed latent sequence embeddings .Qui and the K-transformed long \ninput sequence embeddings .Kxj form a scalar product .(Qui)⊺(Kxj ).I ti su s e d \nas a weight for the V-transformed long sequence embedding .Vx j to generate the \nnew short embeddings. The Perceiver is basically a BERT model with a sequence\n104 3 Improving Pre-trained Language Models\nFig. 3.10If the input sequence is too long, a short latent sequence is deﬁned by the Perceiver. By\ncross-attention between the long sequence and the latent sequence the information is compressed.\nA standard transformer block computes the self-attentions between the latent sequence elements,\nwhich in the end generates a classiﬁcation [\n93] \nlength of n instead of T, which avoids that the computing effort scales quadratically \nwith the input length. The iterative approach enables the model to devote its limited \ncapacity to the most relevant inputs. In experiments the Perceiver was able to beat \nthe leading ResNet-50 CNN with respect to image classiﬁcation [ 93]. Perceiver IO \n[92] projects the resulting n output embeddings of a Perceiver to a larger sequence \nof output embeddings by another cross-attention operation, which, for instance, gets \nthe position embeddings of output elements as query vectors. The Perceiver AR \n[\n73] extends the Perceiver to generate an output sequentially similar to the encoder-\ndecoder transformer. \nS4 [ 68] is a Structured State Space Sequence model based on the Kalman ﬁlter \nfor the observation of a state model with errors [ 101]. A continuous state space \nmodel is deﬁned by \n.x′(t)=Ax(t)+Bu(t)y(t)=Cx t +Du(t), (3.1) \nwhich maps an input signal .u(t) to output .y(t) through a latent state .x(t).T h e \nauthors reparametrize the matrices . A and decompose them as the sum of a low-rank \nand skew-symmetric term. Moreover, they compute its generating function of the \nassociated inﬁnite sequence truncated to some length L in frequency space. The\n3.2 Capturing Longer Dependencies 105\nlow-rank term can be corrected by the Woodbury identity for matrix inversion. The \nskew-symmetric term can be diagonalized and can be reduced to a Cauchy kernel \n[153]. \nThe . A matrix is initialized with an special upper-triangular “HIPPO” matrix that \nallows the state .x(t) to memorize the history of the input .u(t). The authors prove \nthat in complex space . Cthe corresponding state-space model can be expressed by \nmatrices .(/Lambda1−PQ∗,B,C) for some diagonal matrix . /Lambda1and vectors . P,Q,B,C∈\nC.T h e s ea r et h e5N trainable parameters of S4, where N is the state dimension. \nOverall, S4 deﬁnes a sequence-to-sequence map of shape (batch size, sequence \nlength, hidden dimension), in the same way as related sequence models such as \nTransformers, RNNs, and CNNs. For sequence length L this requires a computing \neffort of \n.∼O(N+L) and .O(N+L) memory space, which is close to the \nlowest value for sequence models. Gu et al. [ 69] provide a detailed exposition and \nimplementation of the S4 model. \nIn empirical evaluations it turned out that S4 for an input length of 1024 is 1.6 \ntimes faster than the standard transformer and requires only 43% of its memory. For \nan input length of 4096, S4 is 5 times faster and requires just 9% of the memory of \nthe standard transformer. For the benchmarks of the Long Range Arena benchmark \nS4 increased S\nOTA average accuracy from 59.4% to 80.5% (Table 3.7). Moreover, \nS4 was able to solve the extremely challenging Path-X task that involves reasoning \nover sequences of length 16k where all previous models have failed. Finally, S4 \nwas able to perform raw speech signal classiﬁcation on sequences of length 16k and \nachieves a new S\nOTA of 98.3% accuracy. S4 involves a genuine breakthrough in \nlong range sequence processing. In addition, S4 is better in long-range time-series \nforecasting, e.g. reducing Mean Square Error by 37% when forecasting 30 days of \nweather data. DSS [\n70] is a variant of S4 that is simpler to formulate and achieves a \nslightly lower performance. \n3.2.3 Comparisons of Transformers with Long Input \nSequences \nThe Long Range Arena [ 209] aims to evaluate the performance on tasks with long \ninput sequences from 1k to 16k tokens. It contains six different benchmark datasets \ncovering text, images, mathematical expressions, and visual spatial reasoning. The \ntasks include ListOps (computations in a list-notation), text classiﬁcation (classify \nIMDB reviews using character sequences), document retrieval (based on document \nembeddings), image classiﬁcation (based on a sequence of pixels), and pathﬁnder \n(detection of circles) in two versions. The authors evaluate nine transformer \narchitectures with the ability to process long inputs. \nThe results are shown in Table 3.7. For the hierarchically structured data of \nListOps, it turns out that kernel-based approaches, for instance the Performer and \nthe Linear Transformer, are not appropriate. For text classiﬁcation, kernel-based\n106 3 Improving Pre-trained Language Models\nTable 3.7 Accurac y results for the Long-Range Arena Benchmark. The best score is printed in \nbold, results improving the standard transformer are underlined (cf. [ 209]) \nModel ListOps Text classif. Retrieval Image classif. Pathﬁnder Path-X Average \nTransformer 36.3 64.3 57.5 42.4 71.4 .× 54.4 \nReformer 37.3 56.1 53.4 38.1 68.5 .× 50.7 \nSynthesizer 37.0 61.9 54.7 41.6 69.5 .× 52.9 \nBigBird 36.0 64.0 59.3 40.8 74.9 .× 55.0 \nLinear transf. 16.1 65.9 53.1 42.3 75.3 .× 50.6 \nPerformer 18.0 65.4 53.8 42.8 77.0 .× 51.4 \nS4 58.4 76.0 87.1 87.3 86.1 88.1 80.5 \nmethods perform particularly well. For image classiﬁcation most models do well, \nexcept for the Reformer. The pathﬁnder task is solved by all models with an \nacceptable performance, with the Performer doing best. However, all models except \nS4 fail on the extended Pathﬁnder task and are not able to ﬁnd a solution. In terms \nof all benchmarks, S4 is the best model by a wide margin. \nWith respect to speed, the Performer was best, being 5.7 times faster than the \nstandard transformer on sequences of length 4k. Memory consumption ranged from \n9.5 GB for the standard transformer to about 1.1 GB for the Linear Transformer. All \nother models except the Synthesizer require less than 3 GB with S4 doing well in \nboth aspects. \n3.2.4 Summary \nThere are a variety of proposals for PLMs to efﬁciently process long input \nsequences. Often a sparse attention matrix is employed, where only a part of the \npossible attentions is used to establish the connection between far-away positions. \nUsually, full attention is computed for near positions. Some tokens have a global \nattention to communicate information between positions not connected directly. A \nprominent example is BigBird, which adds random attentions. Its computational \neffort only grows linearly with input size and it still can perform arbitrary sequence \ncomputations. There are other architectures like the Performer and the Linear \nTransformer, which also exhibit linear growth. \nSome architectures either approximate the attention matrices by low-rank factor-\nizations or aggregate tokens, which express similar content (Reformer, Combiner). \nAnother approach is to use a recurrence mechanism such that computations are \nreduced for far-away tokens (Transformer-XL, Linear Transformer, Transformer-\nLS, Perceiver). An alternative is the factorization of the self-attention matrix \n(Performer) or its replacement with simpler computations (Synthesizer). Recently, \nthe S4 model has been proposed that applies a state-space model to long-range \nprediction. It uses an architecture based on complex number computations, which\n3.3 Multilingual Pre-trained Language Models 107\nis completely different from the usual transformer setup. It outperforms all prior \nmodels by a large margin and is efﬁcient in terms of computation time and memory. \nThe performance of these approaches was evaluated with six different bench-\nmarks of the Long Range Arena. It turned out that S4 beats the other models \nwith respect to all benchmarks. All approaches were able to reduce memory \nconsumption compared to the standard transformer. The larger input length allow \nnew applications, e.g. in raw speech processing, image processing or genomics \n[\n247]. \n3.3 Multilingual Pre-trained Language Models \nThere are more than 7100 languages in the world [ 9], and each language can \nexpress almost all facts and concepts. Therefore, PLMs should also be able to \ngenerate consistent representations for concepts in different languages. Languages \ndiffer to some extent in the basic word order of verbs, subjects, and objects in \nsimple declarative sentences. English, German, French, and Mandarin, for example, \nare SVO languages (subject-verb-object) [\n100]. Here, the verb is usually placed \nbetween the subject and the object. Hindi and Japanese, on the other hand, are SOV \nlanguages, meaning that the verb is placed at the end of the main clause. Irish and \nArabic, on the other hand, are VSO languages. Two languages that have the same \nbasic word order often have other similarities. For example, VO languages generally \nhave prepositions, while OV languages generally have postpositions. Also, there \nmay be a lexical gap in one language, where no word or phrase can express the exact \nmeaning of a word in the other language. An example is the word \n“Schadenfreude” \nin German, which roughly translates to “have joy because some other person has \nbad luck”. More such differences are discussed by Jurafsky and Martin [ 100]. \nTo gain cross-lingual language understanding, a PLM has to be trained with more \nthan one language and has to capture their structural differences. During training, \nPLMs can establish an alignment between concepts in different languages. \n• Training large PLMs models, e.g. T5 or BERT, on multilingual data with a joint \ntoken vocabulary leads to models that transfer information between languages by \nexploiting their common structure. \n• BERT-like models can be trained to associate the words of a sentence in one \nlanguage with the words of its translation to another language by masked \nlanguage modeling. However, it has been shown that multilingual processing is \npossible, even when little or no parallel training data is available. \n• Transformer encoder-decoder models are explicitly trained to translate a text \nfrom one language to another language. \nTraining a language model with several languages in parallel can improve the \nperformance—especially for languages with little training data. This could already \nbe demonstrated for static word embeddings [\n194].\n108 3 Improving Pre-trained Language Models\n3.3.1 Autoencoder Models \nmBERT (multilingual BERT) [ 48] is a standard BERT model. It has been pre-\ntrained with the MLM loss on non-parallel Wikipedia texts from 104 languages \nand has a shared token vocabulary of 110k WordPiece tokens for all languages. \nThis implies that Chinese is effectively character-tokenized. Each training sample is \na document in one language, and there are no cross-lingual dictionaries or training \ncriteria. To demonstrate its properties the model was ﬁne-tuned to a multilingual \nversion XNLI [\n40] of the Natural Language Inference (NLI) benchmark, i.e. the \ntask to predict, whether the ﬁrst sentence entails the second. It turns out that mBERT \nmay be ﬁne-tuned with a single language on NLI and still yields good test results \non related languages [\n40, 232]. \nThe results for 6 languages [ 111] are shown in Table 3.8. Compared to ﬁne-\ntuning XNLI with all languages, there is only a small drop in accuracy for related \nlanguages, e.g. Spanish and German, if the ﬁne-tuning is done with XNLI in English \nand the evaluation in the other language. For the other languages the reduction \nof performance is larger, but the results are still good. There is even a transfer of \ninformation between languages with different scripts, e.g. for Arabic and Urdu. The \nauthors also consider the embeddings of a word and its translation. It turns out that \nthe cosine similarity between a word and its translation is 0.55, although there is no \nalignment between languages. \nKarthikeyan et al. [ 104] investigate the factors for the success of mBERT. \nThey ﬁnd that mBERT has cross-lingual capabilities even if there is absolutely no \noverlap in the token vocabulary. Moreover, a higher number of identical tokens in \nboth vocabularies contributes little to the performance improvements. Comparing \ndifferent language pairs the authors show that a large network depth and a high \ntotal number of parameters of a bilingual BERT are crucial for both monolingual \nand cross-lingual performance, whereas the number of attention heads is not a \nsigniﬁcant factor. On the other hand, the structural similarity of the source and \ntarget language, i.e. word order and frequency of words, has a large inﬂuence on \ncross-lingual performance. \nXLM [ 111] improves the transfer of knowledge between different languages \nby using translated sentences from different language pairs during pre-training. \nThe authors concatenate a sentence with its translations to another language for \nTable 3.8 Cross-lingual natural language inference (XNLI) [ 40] test accuracy for 6 languages. \nFine-tuning with XNLI for all languages is compared to ﬁne-tuning with XNLI only for English. \nResults for mBERT [ 48]a n dX L M[ 111] \nFine-tune with . . . Model English Chinese Spanish German Arabic Urdu \nAll languages mBERT 81.9 76.6 77.8 75.9 70.7 61.6 \nEnglish only mBERT 81.4 63.8 74.3 70.5 62.1 58.3 \nAll languages XLM 85.0 78.6 80.8 80.3 76.5 63.2 \nEnglish only XLM 85.0 76.5 78.9 77.8 73.1 57.3\n3.3 Multilingual Pre-trained Language Models 109\nFig. 3.11 The translation language modeling (TLM) task is applied to pairs of translated \nsentences. To predict a masked English word, the model can attend to both the English sentence \nand its French translation, and is thus encouraged to align English and French representations [\n111] \ntraining and introduce a new translation language modeling (TLM) objective for \nimproving cross-lingual pre-training. To predict masked words in the input sentence, \nthe algorithm can attend to the words in the translated sentence. In this way, the \nmodel learns to correlate words from different languages. An example is shown in \nFig.\n3.11. As shown in Table 3.8, XLM has a much higher cross-lingual accuracy \nfor XNLI compared to mBERT. The transfer from a model ﬁne-tuned in English to \nother languages incurs only a small loss. The experiments show that TLM is able \nto increase the XNLI accuracy for 3.6% on average. The model was also evaluated \nfor unsupervised machine translation from German and other languages to English, \nyielding a very good performance (cf. Sect.\n6.3). \nUnicoder [ 88] is an improved XLM model with three additional training \ntasks. Cross-lingual word alignment learns to associate the corresponding words in \ntranslated sentences. Cross-lingual paraphrase detection takes two sentences from \ndifferent languages as input and classiﬁes whether they have the same meaning. \nThe document-level cross-lingual masked language model applies the MLM task to \ndocuments where part of the sentences are replaced by their translations. On XNLI \nthe authors report an average accuracy improvement of 1.8%. \nXLM-R is an optimized version of XLM [ 41]. It is based on RoBERTa and \ntrained on a huge multilingual CommonCrawl dataset of 2.5TB covering 100 \nlanguages with a common vocabulary of 250k tokens. It increased the S OTA on \nthe XNLI-score to 79.2%. For cross-lingual question answering, models are ﬁne-\ntuned on the English SQuAD dataset and evaluated on 7 other languages. XLM-R \nimproves the F1 score on this SQuAD version by 9.1%–70.7%. It outperforms \nmBERT on cross-lingual classiﬁcation by up to 23% accuracy on low-resource \nlanguages. The performance of XLM-R is nearly as good as that of strong \nmonolingual models. \nThese results support the observation that the performance of PLMs can be \nimproved by training on large volumes of text [ 102]. More languages lead to \nbetter cross-lingual performance on low-resource languages under the condition that\n110 3 Improving Pre-trained Language Models\nthe model capacity is large enough. Combined with the approach of Aghajanyan \net al. [ 2], which avoids too large changes in representation during ﬁne-tuning \n(Sect. 3.6), the XLM-R .LARGE model increases the S OTA in XNLI to 81.4%. If \nan additional criterion of separating semantically-equivalent sentences in different \nlanguages from other sentences is added to XLM-R, the accuracy on semantic tasks \nis increased [\n228]. Even larger models like XLM-R XXL [66] with 10.7B parameters \nwere pre-trained on CC-100, which consists of 167B tokens of non-parallel text also \ncovering low-resource languages, and increased the XNLI performance by 2.4%. \nRemBERT [ 37] redistributes the parameters of multilingual models. First the \nauthors showed that using different input and output embeddings in state-of-the-art \npre-trained language models improved model performance. Then they demonstrated \nthat assigning more parameters to the output embeddings increased model accuracy, \nwhich was maintained during ﬁne-tuning. As a consequence Transformer represen-\ntations were more general and more transferable to other tasks and languages. The \nXtreme collection [\n86] is a multitask benchmark for evaluating the cross-lingual \ngeneralization capabilities of multilingual representations across 40 languages and \n9 tasks. RemBERT outperformed XLM-R on Xtreme, despite being trained only on \na smaller subset of training data and ten additional languages. \nPLMs like BERT generate contextual token embeddings. However, the user \noften needs contextual embeddings for passage or sentences to compare their \ncontent. LaBSE [ 57] is a language-agnostic generator of passage embeddings, \nwhere source and target sentences are encoded separately using a shared BERT-\nbased encoder. The representations of [CLS] in the ﬁnal layer were taken as the \nsentence embeddings for each input. LaBSE combined a masked language model \n(MLM) and a translation language model (TLM) loss with a margin criterion. This \ncriterion computes the cosine distance\n.cos(x, y) between the passage embeddings . x\nand the embedding . y of its correct translation. Then it is required that . cos(x,y)−m\nis larger than .cos(x,y i), where m is a positive margin and the . yi are embeddings \nof arbitrary other passages. LaBSE was trained using 17B monolingual sentences \nand 6B bilingual translated sentences. The resulting sentence embeddings markedly \nimprove the retrieval accuracy S\nOTA of sentences in cross-lingual information \nretrieval (cf. Sect. 6.1). The code and pre-trained models are available. \n3.3.2 Seq2seq Transformer Models \nmT5 is a multilingual version of the T5 Seq2seq transformer (Sect. 3.1.3) with up \nto 13B parameters [ 236]. It was pre-trained using a training dataset of web pages \ncovering 101 languages with about 48B tokens and a common vocabulary of 250k \ntokens. For pre-training, the model had to predict masked phrases in monolingual \ndocuments in the same way as T5. Similar to T5 the model may be instructed to \nperform different tasks by a preﬁx, e.g. “summarize”. These tasks were trained by \nﬁne-tuning on the corresponding datasets.\n3.3 Multilingual Pre-trained Language Models 111\nFor the XNLI benchmark [ 40] the model has to decide, if the ﬁrst sentence entails \nthe second sentence. When the model is ﬁne-tuned on XNLI with English data and \nperformance is measured for 15 languages, accuracy is 84.8% compared to 65.4% \nfor mBERT, 69.1% for XLM, and 79.2% for XLM-R. Although the texts in the \ndifferent languages are not parallel, the model is able to exploit structural similarities \nbetween languages to solve the task. The code of this model is available at [ 235]. \nSimilar models are used for multilingual translation (Sect. 6.3). mT6 [ 31] enhances \nthe training of mT5 with pairs of translated sentences and deﬁnes new training \ntasks. Experimental results show that mT6 has improved cross-lingual capabilities \ncompared to mT5. A further improvement is Switch [\n56] with a mixtur e-of-experts \n(MoE) architecture of mT5 requiring only one ﬁfth of the training time of mT5 while \nyielding a performance gain across all 101 languages (Sect.\n3.5.2). \nmBART [ 126] is a multilingual encoder-decoder based on the BART model \n(Sect. 3.1.3). The input texts are corrupted by masking phrases and permuting \nsentences, and a single Transformer model is pre-trained to recover the corrupted \ntext. This is performed for the training documents covering 25 languages. Sub-\nsequently, the pre-trained model is ﬁne-tuned with a translation task between a \nsingle language pair. In addition, back-translation may be used, where another \nmodel is trained to translate the target sentence back to the source language and \nan additional loss encourages to reconstruct the source sentence. mBART adds \na language symbol both to the end of the encoder input and the beginning of \nthe decoder input. This enables models to know the languages to be encoded \nand generated. It turns out that pre-training improves translation, especially for \nlanguages with little parallel training data. In addition, back-translation markedly \nameliorates the translation results. Many experiments are performed to analyze \nthe effect of different algorithmic features. Pre-training is especially important if \ncomplete documents are translated instead of single sentences. \nmBART may also be used for unsupervised machine translation, where no \nparallel text of any kind is used. Here the authors initialize the model with pre-\ntrained weights and then learn to predict the monolingual sentences from the source \nsentences generated by back-translation. The results for languages with similar \nstructure are very good, e.g. for En-De mBART achieves a B\nLEU -value of 29.8, \nwhich is close to the supervised value of 30.9. Note that mBART has a similar \nperformance as MASS (Sect.\n3.1.3). For dissimilar pairs of languages, e.g. English-\nNepali, mBART has reasonable results where other approaches fail. \nMARGE [ 118] is a multilingual Seq2seq model that is trained to reconstruct a \ndocument x in one language by retrieving documents .z1,...,z k in other languages. \nIt was trained with texts in 26 languages from Wikipedia and CC-News. A document \nwas encoded by the output embedding of the ﬁrst token of a Transformer [\n212]. \nA retrieval model scores the relevance .f( x,zj ) of the target document x to each \nevidence document . zj by embedding each document and computing their cosine \nsimilarities. A transformer receives the embedded texts of .z1,...,z k and auxiliary \nrelevance scores .f( x,zj ) from retrieval as input and is trained to generate the target \ndocument x as output. The similarity score is used to weight the cross-attention \nfrom the decoder to the encoder, so that the decoder will pay more attention to\n112 3 Improving Pre-trained Language Models\nmore relevant evidence documents. The models jointly learn to do retrieval and \nreconstruction, given only a random initialization. In a zero-shot setting the model \ncan do document translation with B LEU scores of up to 35.8 in the WMT2019 \nDe-En benchmark, as well as abstractive summarization, question answering and \nparaphrasing. Fine-tuning gives additional strong performance on a range of tasks \nin many languages, showing that MARGE is a generally applicable pre-training \nmethod. \nXLNG [ 32] pre-trains the same Seq2seq model simultaneously using an MLM \nand a translation TLM loss (Table 3.1). The pre-training objective generates \nembeddings for different languages in a common space, enabling zero-shot cross-\nlingual transfer. In the ﬁne-tuning stage monolingual data is used to train the \npre-trained model on natural language generation tasks. In this way, the model \ntrained in a single language can directly solve the corresponding task in other \nlanguages. The model outperforms methods based on machine translation for zero-\nshot cross-lingual question generation and abstractive summarization. In addition, \nthis approach improves performance for languages with little training data by \nleveraging data from resource-rich languages. \n3.3.3 Autoregressive Language Models \nGenerative models like GPT-3 are trained on huge collections of documents which \nusually contain texts from different languages. By this training data, the model \nalso acquires the knowledge about these languages and generates joint contextual \nrepresentations of meanings. As described in Sect.\n3.6.3, it is able to translate \nbetween languages if given an appropriate prompt and some examples (few-shot \nlearning). On WMT2016 En . →De, for instance, GPT-3 achieves a few-shot B LEU \nof 29.7 compared to a supervised S OTA of 41.2, whereas in the De . →En direction \nGPT-3 outperforms the current S OTA of 40.2 B LEU with 40.6 B LEU [25]. \nWinata et al. [ 231] evaluate in detail the multilingual capabilities of GPT-2, \nGPTNEO and T5 with 1.6B, 6B, and 3B parameters respectively. The models are \nable to use the context from English to predict the answer in non-English languages. \nThe authors ﬁnd that the largest model GPT\nNEO always performs best on a set \nof multilingual benchmarks. The performance depends on the language pair. The \nmodels, for instance, achieve higher performance for En\n. →Es than for the other two \ntarget languages (De and Fr). For the MultiNLU benchmark [ 187] the error 12.1% \nof the S OTA model fully trained on the target language is not much lower than the \nerror of 17.3% for few-shot prompts of GPT NEO.\n3.4 Additional Knowledge for Pre-trained Language Models 113\n3.3.4 Summary \nMachine translation is one of the most widely used applications of NLP . Languages \nhave both structural and lexical differences that make translation difﬁcult. The joint \nprocessing of multiple languages must take these differences into account. \nWhen BERT is trained with documents from multiple languages, it is able to \ntransfer knowledge between languages, e.g. solve language inference tasks, even if \nit has no access to parallel texts. Knowledge transfer is improved in XLM by using \nthe translation language modeling loss, such that translated sentences are employed \nto reconstruct masked tokens. There are a number of improved versions of XLM \nthat are able to increase the accuracy of cross-language inference. \nEncoder-decoder models such as T5 can be generalized to multiple languages and \ninduce powerful multilingual embeddings. mT5 can be controlled by a preﬁx and \nsolves various task like translation, summarization, and language inference. mT6 \nand Switch are more effective variants of mT5. mBART is pre-trained by recovering \ncorrupted text in different languages. It can even be used for unsupervised machine \ntranslation. XNLG generates joint embeddings in a multilingual space and MARGE \nleverages retrieval of background documents to reconstruct a target document. \nBoth models are able to perform multiple tasks such as abstractive summarization, \nquestion answering, and paraphrasing. Note, however that specialized models are \nused for translating single language pairs (Sect.\n6.3.1). \nAutoregressive language models such as GPT-3 are trained on huge corpora, \nwhich also contain multilingual documents. Therefore, these models can also be \ninstructed by few-shot learning to perform multilingual tasks such as translations or \nquestion answering. However, performance is usually not as good as for dedicated, \nﬁne-tuned models. \n3.4 Additional Knowledge for Pre-trained Language Models \nDuring unsupervised pre-training, PLMs like BERT and GPT2 are forced to predict \nmissing words from the context. They are optimized to predict either the next word \nin a sequence or some masked words (e.g. \n“Einstein was [MASK] in the city of \nUlm.”). Trained on this task, they obviously gather knowledge about real-world \nfacts and relations from the training data. PLMs do surprisingly well in reproducing \nfacts and relations based on unsupervised training. In Sect.\n4.2 we discuss, what \nknowledge is covered by standard PLMs. It turns out, however that due to the \nstill limited number of parameters only a fraction of knowledge contained in the \ntraining data can be remembered by a PLM. In addition, events that occurred after \nthe training are missed.\n114 3 Improving Pre-trained Language Models\nFig. 3.12 A PLM gets an input text and collects additional knowledge from different sources. This \nknowledge may be added beforehand or can be retrieved on demand. Subsequently, an output is \ngenerated using the additional knowledge \nThis section presents methods for extending factual knowledge in PLMs, either \nduring training or on the ﬂy during actual model usage Fig. 3.12.A Knowledg e \nBase (KB) describes knowledge about the world, e.g. by entities and their relations. \nWe outline a number of different approaches with which information in KBs or \nother knowledge sources such as text collections can be incorporated into PLMs \n(Table \n3.9): \nKnowledge Base Embeddings: There are techniques to represent the entities and \nrelations in a KB by embeddings. A number of approaches try to combine these \nembeddings with the token embeddings created by a PLM. In this way, the \ninformation in the KB can be injected into the PLM and used for downstream \ntasks. \nTextual Encoding of Tables: Often additional knowledge is available in tables. \nThe entries in these tables can be encoded in a special text format. A PLM can \nbe trained with this text to acquire the knowledge in the rows and columns, in a \nsimilar way as the relation between the words of two languages can be learned. \nTextual Encoding of KB Relations: An alternative way to use KB information \nstarts with identifying entities or concepts in a text. The relations available for \nthese entities and concepts can be extracted from the KB and can be included in \nthe training process either as text or in another appropriate form. \nAdding Retrieved Facts: When a PLM needs to answer a question or create a text, \nit can formulate a query on the topic and retrieve corresponding text content from \na KB or the Internet. This textual information may be picked up by a transformer \nand enhance the output. In this way, the model can use comprehensive and up-\nto-date information on the ﬂy.\n3.4 Additional Knowledge for Pre-trained Language Models 115 \nTable 3.9 Models integrating additional knowledge (cf. [ 166, p. 10]). Benchmarks: GLUE nat-\nural language understanding Sect. 4.1.1, TACRED relation extraction Sect. 5.4.2 [199], TriviaQA \nquestion answering Sect. 6.2.1 [99], English all word WSD [ 14], Nat. Quest question answering \n[109] Sect. 6.1.2 \nModel Train task Fine-tuning Extra Benchmark \nUsing knowledge base embeddings in pre-trained language models \nERNIE(THU) [ 255] MLM+NSP + \nmasked NEs \nGLUE, etc. KB NE embeddings \ncombined with \ntoken embeddings \nGLUE 79.6 \nKnowBERT [157] MLM+NSP +EL GLUE, etc Translate token \nembeddings\n. ↔ KB \nNE embeddings \nKEPLER [ 224] MLM+KE GLUE, etc Combine token \nembeddings with \nNE embeddings; use \nTransE loss \nTACRED \n71.5 F1 \nUsing textual information from knowledge bases \nK-Adapter [ 222] MLM + rel. extr. – Add parallel adapter \nnetwork to \nRoBERTa \nTACRED \n72.0 F1 \nWKLM [ 234] MLM+ERD – Detect replaced NEs \nin text \nTriviaQA \n63.1 F1 \nCoLAKE [ 202] MLM – Create graph from \ntextual relation \ntriples and tokens \nGLUE 86.3 \nLUKE [ 234] MLM+ERD – Masked language \nmodeling for text \nand contained \nentities \nTACRED \n72.7% F1 \nEWISER [ 14] MLM Word sense \nclassiﬁcation \nInclude wordnet \nsupersense graph \nEnglish all \nword WSD \n80.1% F1 \nUsing text passages retrieved from text collections \nFiD [ 91] MLM, S2S QA Encode query and \nKB by BERT; \ncombine query and \nretrieved docs with \nSeq2seq \nNat. Quest. \n51.4% acc. \nRetro [ 21] LM Language \ngeneration with \nperiodical retrieval \nNat. Quest. \n45.5% acc. \nEnhancing Logical Consistency: PLMs sometimes do not generate logically con-\nsistent content. By additional ﬁne-tuning tasks a model can be trained to respect \nlogical consistency. \nSurveys of methods to incorporate domain knowledge into Deep Neural Networks \nare given by Dash et al. [\n45] and Y u et al. [ 243]. \n116 3 Improving Pre-trained Language Models\n3.4.1 Exploiting Knowledge Base Embeddings \nTypically, Knowledge Bases are graph structures where the nodes correspond to \nentities and the edges represent relations connecting the entities. Many large-scale \nKBs, such as WordNet [ 137], YAGO [200], Freebase [18], DBpedia [ 15], and DiffBot \n[77] have been released in recent years with millions of entities. Figure 3.13 shows \na small subset of the WordNet hierarchy. In most cases a KB can be described by \ntriples .( h ,r ,t), where h and t are entities in a set E, and r is a relation holding \nbetween these entities. To assess the semantic contents of a KB, it was proposed to \nencode its entities as well as its relations as embeddings in a low-dimensional space, \nallowing to determine the similarity of entities and relations [\n43]. Subsequently, \nthese embeddings can be used to disambiguate entities (entity linking, Sect. 5.3.3), \nor predict new relations (Sect. 5.4). \nFor the embeddings .emb(word) of words generated by Word2V ec [ 135]i t \nturned out that relations between entities often are represented in the space of \nword embeddings as vector differences between entity embeddings (Sect. 1.5). An \nexample is the relation between a country and its capital, for which we have \napproximately .emb(Germany)−emb( Berlin)≈emb( France)−emb( Paris) . \nThe TransE model [ 20] is built on this pattern. TransE adapts the embeddings in \nsuch a way that whenever .( h ,r ,t)holds and .emb(h) and .emb(t) are the embeddings \nof h and t, then equation .emb(h)+emb(r)≈emb(t) should be approximately \nvalid for some vector .emb(r), which is considered as the embedding of the relation \nr. Consequently, for all triples .( h ,r ,t)in the set S of correct triples the TransE-loss \nFig. 3.13 Small part of the WordNet knowledge base describing the relations between English \nwords. It contains synsets of word with approximately the same meaning, which are related by the \nhypernym (is-a) meronym (has-part) and member-of relations [\n137]\n3.4 Additional Knowledge for Pre-trained Language Models 117\nFig. 3.14 KEPLER [ 224] trains a conventional BERT-like model by the MLM-loss. For a \nknowledge base with text entries it generates entity embeddings using the special .<S> token \nand encodes relations by the TransE-loss. Both loss functions are added during training \n.fr(h, t)= ∥emb(h)+emb(r)−emb(t) ∥2\n2 should become 0. The TransE-model \nuses the hinge loss to approximate this goal, which modiﬁes the embeddings in \nsuch a way that .fr(h, t) for correct relation triples gets lower than .fr(˜h, ˜t) for \nrandomly selected incorrect triples .(˜h, r,˜t). The models and embeddings are trained \nwith relations from WordNet and Freebase. \nThere are a number of more elaborate models to encode relations from KBs, as \ndescribed in the surveys [ 43, 94]. T ransH overcomes TransE’s inability to model \ncomplex relations, and TransD aims to reduce the parameters by proposing two \ndifferent mapping matrices for head and tail. But these alternatives are rarely \nused for contextual embeddings. Another method for KB representation is tensor \nfactorization [\n144, 145]. This approach, however, is not based on word embeddings \nand therefore mainly used for KB completion and not to enhance PLMs. \nIn the rest of the section we describe approaches, which merge KB-embeddings \nusually computed by TransE and token embeddings generated by language models. \nA difﬁculty is to establish a relation between the token embeddings and the entities, \nwhich usually contain several tokens. \nKEPLER [ 224] consists of a BERT-like language model generating token \nembeddings by the MLM objective. In addition, it computes embeddings for entities \nfrom descriptive text in the KB using a special token “ .<S>” at the beginning of \nthe input text. This token is trained to produce an embedding of the named entity \nargument of the relation, e.g. for the input “ .<S> Johannes Kepler” in Fig. 3.14.I n \nthis way, the arguments h and t of the relation are embedded. The embedding of the \nrelation r is either a parameter to be trained, or it may be determined by the text \nverbalizing the relation. These embeddings are fed into the TransE loss and used as \nan extra training criterion in addition to MLM (Fig. 3.14). In a number of language \nunderstanding tasks the approach is able to achieve good results. On the relation \nextraction benchmark TACRED [ 254] the approach reaches 71.5% F1-value. \nKnowBERT [ 157] explicitly models entity spans in the input text and uses \nan entity linker to retrieve precomputed entity embeddings from a KB to form \nknowledge enhanced entity-span representations. The KB-embeddings are precom-\n118 3 Improving Pre-trained Language Models\nputed with a loss function similar to TransE. Projection mappings are used to \ntransform LM-embeddings to KB-embeddings and vice versa. Information from \nthe best matching KB-embeddings is averaged and retransformed to enhance the \nLM-embeddings. These computations form an additional layer of BERT. Wikipedia \nand WordNet were used as KBs. To test KnowBERT’s ability to retrieve facts \nfrom the KB, a relation was formulated and one argument of the relation was \nmasked. KnowBERT reaches a mean reciprocal rank (MRR) of 0.31, indicating \nthat on average the correct entity appeared on rank 3, whereas for BERT it shows \nup on rank 9. Hence, the model generates better answers than BERT, but is only \napproximately able to reproduce the relations of the KB. However, it often leads to \nimprovements in downstream tasks. \nERNIE-THU [ 255] relates named entities in a KB to the named entities in a \ndocument in a similar way, and transforms embeddings between these two spaces. \nE-BERT [ 162] is similar in spirit to KnowBert, but it requires no expensive further \npre-training of the BERT encoder. Facts as Experts [ 213] also links factual informa-\ntion and entities using embeddings, and in this way can inject new information into \nthe model. \nIn summary the methods presented in this section directly infuse domain-speciﬁc \nknowledge expressed by relation embeddings into token embeddings of PLMs. \nThere are, however, a number of disadvantages. The KB entity embeddings are \nseparately pre-trained with some knowledge embedding models (e.g., TransE [\n20]) \nand ﬁxed during training of the PLMs. Thus KB-embedding and token embeddings \nare not learned simultaneously. Moreover, the KB entity embeddings often cannot \nfully capture the rich contextual and relational information of an entity in the KB. \nFurthermore, they are static and do not depend on the context. In addition, they rely \nto a great extent on the performance of the linking algorithm and on the reliability \nof graph embeddings. This means that in general other approaches perform better, \ne.g. for relation extraction (Sect.\n5.4). \n3.4.2 Pre-trained Language Models for Graph Learning \nRelations between objects and concepts can be joined in a graph and provide a \nuniform representation for the relatedness of many items. Using the structure of \na graph many properties of nodes can be predicted. In recent years there was \na great effort to design models which can capture the composition of a graph \nand predict its parts, e.g. node2vec [\n67]o r gr aph convolutional networks [ 107]. \nHowever, the node representations obtained by such deep models tend to be over-\nsmoothed and also become very vague. PLMs potentially are able to improve the \nrepresentation by self-attention over long distances. Xia et al. [ 233] provide a survey \non PLMs for graphs. Nodes and edges are characterized by different feature and \nposition embeddings, and are processed with different types of PLMs. Prominent \napplications are recommender systems exploiting user-product graphs and drug \ndiscovery evaluating molecule structures.\n3.4 Additional Knowledge for Pre-trained Language Models 119\nGraph-BERT [250] is trained on sample nodes taken from a large graph together \nwith their context. These samples are drawn using the closeness according to the \nPageRank algorithm [ 24] and contain no direct link information. Nodes are char-\nacterized by feature embeddings, embeddings based on the PageRank information, \nand hop-based distance embeddings. These embeddings are summarized and form \nthe input of a BERT model. The model is pre-trained to reconstruct the information \nof masked nodes and to predict the relation between two nodes by evaluating \ntheir cosine similarity. The model is ﬁne-tuned for node classiﬁcation and graph \nclustering. Graph-BERT achieves the second-best accuracies for node classiﬁcation \non three graph benchmarks [\n128,p .1 6 ] . \nGPT-GNN [87] proposes an autoregressive PLM to perform an iterative recon-\nstruction on given graphs. The method assumes a random order on the edges and \nnodes. Given the edges and nodes up to a speciﬁc position, it predicts the properties \nof the next nodes/edges. GPT-GNN generates one masked node and its edges at \na time and optimizes the parameterized models via maximizing the likelihood of \nthe node and edges generated in the current iteration. Then, it iteratively generates \nnodes and edges until all masked nodes are generated. The model is trained on a \ngraph of 178M scientiﬁc papers with their features, the venue and the authors, and \non a graph with 83M Amazon reviews, users and products. On both benchmarks the \nmodel has the best accuracies. \nMPG [ 120] consists of a BERT model encoding node and edge features. As a \npre-training task, the model has to learn whether two graphs divided into two halves \nactually belong together or whether the halves are a random pair. The model is \napplied to the modeling of molecules and achieves S\nOTA results on a range of 14 \nbenchmarks, especially drug discovery. \nGraphFormers [ 238] jointly models a graph structure together with sequences \nof words. Each node of the graph contains a text. A center node and its neighbors \nare tokenized into sequences of tokens. The model has special transformer layers for \ncomputing the embeddings of text tokens and for the derivation of node embeddings \nby aggregating the corresponding text embeddings. The model is pre-trained with \nthe task to predict, if two nodes are linked or not. GraphFormers is tested on three \nbenchmark tasks, e.g. a graph with scientiﬁc papers characterized by their titles and \ntheir citation graph. The model consistently outperforms all prior approaches in the \nprediction of links. \n3.4.3 T extual Encoding of T ables \nTabular data probably makes up the majority of all business and administrative \ndata today. Examples are retail transactions, ofﬁcial statistics, processing data from \nindustrial applications, etc. A survey on the interpretation of tables on the web is \nprovided by de Alwis et al. [\n46]. Previous work often relies on manually selected \nfeatures, cannot handle the ﬂexible schemas in web tables, and does not generalize \nwell across tasks.\n120 3 Improving Pre-trained Language Models\nFig. 3.15 Learning table relations with TURL [ 47]. On the left side the table caption and the \ncolumn headers are trained. On the right side the row markers together with input entities (cells in \na speciﬁc row) are processed \nFig. 3.16 TaBERT [ 241] encodes the rows of a table as text in a special format. The “context” \ncontains corresponding text. Each table cell is represented as (column header, column value type, \nvalue). Here the ﬁrst table row is encoded by the line starting with [CLS] \nTURL [ 47] characterizes a relational table by the table caption C (a short \ntext, may be enhanced by section title), column headers . hi (a sequence of tokens) \ndescribing the table scheme .H={h 1,...,h m} and cell values, where each cell \nmay represent an entity, e.g. a person. Cells in the same row share some relation, \nand cells in the same column share another relation. This requires a structure-\naware attention mechanism implemented by a visibility matrix, which restricts the \nattention to speciﬁc columns and rows. \nTURL is pre-trained according to the masked language model loss on a large \nunstructured dataset consisting of the table captions and headers. Subsequently, the \nrelation between entities in the same row or column can be learned. Entities in a \ntable are masked, and the model has the task to predict them based on the table \ncontext and the visibility matrix. By this target TURL can learn factual relations \nfrom the table and encode them into entity embeddings (Fig.\n3.15). \nThe model is trained on 570k tables extracted from Wikipedia. All columns \ncontaining at least one linked cell are marked as entity columns. After ﬁne-tuning, \nthe model is able to predict the masked contents of table cells in the test set with \nprecision of 54.8%, beating competing approaches. An ablation study shows that \nthe visibility attention matrix is essential for achieving a high performance. \nTaBERT [241] aims to include both, natural language text and structured table \ndata. TaBERT is trained on 26.6M tables and surrounding text from English \nWikipedia and the WDC WebTable Corpus [ 115]. Each table cell is described \nas (column header, column value type, value). Subsequently, the table rows are \nencoded as text, as shown in Fig. 3.16. For pre-training 20% of the columns of\n3.4 Additional Knowledge for Pre-trained Language Models 121\na table are randomly selected and the model has to predict the masked column \nnames and types. In addition, the cell values are reconstructed according to a special \nscheme. The model is ﬁne-tuned on the WikiTableQuestions benchmark [ 155], \nwhich contains questions requiring compositional, multi-hop reasoning over a series \nof entries in the given table. To reduce effort only table rows containing query tokens \nare encoded. TaBERT is able to increase the S\nOTA accuracy on this benchmark \nto 51.8%. The authors show that their table cell encoding is more effective than \nalternatives. RPT [ 205] proposes a similar scheme for table encoding. BRIDGE \n[124] is a system for semantic parsing, which converts information from text and \ntables to an SQL query extracting information from a database. \nTapas [ 81] is a variant of BERT optimized for table processing. The table is \nﬂattened ro w-by-row, tokenized and enhanced with position embeddings. Following \nembeddings are added: a row id embedding, a column id embedding, and a rank \nembedding indicating the rank in the sorted sequence, e.g. for numbers. The model \nis pre-trained on 6.2M table-text pairs from the English Wikipedia with the task to \nrestore words in both table and text that have been replaced with a mask. The model \ncan do this with relatively high accuracy (71.4% accuracy on a test set). \nDuring ﬁne-tuning the model learns to answer questions from a table, e.g. \n“Which wrestler had the most number of reigns?” for a table with wrestling \nresults. [CLS] and a query are prepended to the ﬂattened table and both parts are \ndistinguished by an additional segment embedding. The model has two output types: \n(1) a score for each table cell with the probability that this cell will be part of \nthe answer and (2) a probability of the result type (none, count, sum, average) for \n[CLS] to produce the ﬁnal answer. Together the result indicates which operation \nshould be performed over which table cells to generate the ﬁnal answer. On several \nbenchmarks Tapas reaches S\nOTA results, e.g. improving from 55.1% to 67.2% for \nSQA benchmark [ 90]. The source code and pre-trained models are available at \nHugging Face . \nThe results show that the models described above are able to extract information \nfrom tables and answer question about the table content. This makes it possible to \nuse a large source of information, since tables are ubiquitous in text documents and \nweb pages. In principle, the approach can also be used by large Foundation Models \nto include table information in the text they generate. \nTableGPT [63] generate a text from a table using the GPT-2 language model. It \nenhances GPT-2 for table-to-text generation with two auxiliary tasks, table structure \nreconstruction and content matching, for improving text ﬁdelity. \n3.4.4 T extual Encoding of Knowledge Base Relations \nA number of proposals try to verbalize KB-relations as text. In this way, KB-\nrelations may be directly incorporated in the training text of the language models. \nWKLM [234] randomly replaces a fraction of the entity mentions in the original \ndocument with names of other entities of the same type. The model is trained to\n122 3 Improving Pre-trained Language Models\nFig. 3.17 CoLAKE [ 202] identiﬁes entities and encodes them with speciﬁc embeddings. Type \nembeddings distinguish words, entities and relations. The input embeddings are the sum of \ntoken/entity, position, and type embeddings. For all entities in the input text relations are extracted \nfrom the Knowledge Base and appended after “[SEP]”,e . g .m o t h e r ( H a r r y P o t t e r ,L i l yP o t t e r ) .A \nmasking mechanism ensures that relation elements can attend only to their corresponding elements \nin the input text. During pre-training the model has to predict masked tokens and entities \ndistinguish the correct entity mention from the randomly chosen ones. In addition, \nthe model has to predict masked token. The types of entities are obtained from \nWikidata [\n214]. In this way, the model can better capture entity information from \nnatural language and yields better results for entity-related NLP tasks. WKLM is \nable to predict relation arguments much better than BERT. In question answering \n(SQuAD and open domain, Sect.\n6.2) the model is also able to reach S OTA \nresults. Similar approaches [ 191, 203, 234] propose entity and phrase masking and \nreplacement schemes. \nCoLAKE [ 202] extracts the knowledge context of an entity from large-scale \nknowledge bases. The model links entity mentions to the underlying entities in a \nKB by an entity linker. The mention nodes are then replaced by their linked entities. \nThe CoLAKE model is initialized with the RoBERTa\n.BASE model. It is trained on \nWikipedia with 3 million entity embeddings and 822 relation embeddings aligned \nto the Wikidata5M KB [\n224] on 26M training samples. The example input “[CLS] \nHarry Potter points his wand at Lord V oldemort [SEP]” is shown in Fig. 3.17.T h e \ntype of inputs (word, entity, relation) is encoded as type embeddings and added \nto the token and position embeddings. To introduce a relation from the KB, e.g. \n“(Harry Potter, mother, Lily Potter)” , the relation node “mother” and the entity \nnode “Lily Potter” are introduced with the position embeddings 2 and 3, as the ﬁrst \nrelation argument “Harry Potter” is located at position 1. Self attention is computed \nbetween text inputs. There is a masking mechanism restricting the self-attention for \nrelation elements, e.g. to the pairs “(Harry Potter, mother)” as well as “(mother, Lily \nPotter)” in our example. \nDuring pre-training about 15% of the input elements (words, entities, relations) \nare masked and have to be predicted by the model. As entity nodes simultaneously \nappear in the input text and the knowledge base this helps to align the representations\n3.4 Additional Knowledge for Pre-trained Language Models 123\nof language and relations. Masking relation nodes helps CoLAKE to learn contex-\ntualized representation for relations. On the language understanding tasks of GLUE \nthe CoLAKE model achieves a similar average of 86.3 as RoBERTa. An alternative \ntask consist of the completion of relation triplets\n.( h ,r ,t)using a sentence describing \nthe relation. It turns out that CoLAKE is much better than its competitors, e.g. the \ncorrect relation is inferred from two entities in 72.1% of the cases. \nLUKE [ 237] treats words and entities in a given text as independent tokens, \nand outputs contextualized representations of both. The model is based on BERT \nand trained to predict randomly masked words and entities in a large entity-\nannotated corpus derived from Wikipedia. It contains an entity-aware self-attention \nmechanism that is an extension of BERT’s self-attention. It takes into account \nembeddings indicating if a token represents text or an entity. LUKE yields S\nOTA \nresults in relation classiﬁcation, entity typing and NER. K-adapter [ 222]i sar e l a t e d \napproach using RoBERTa (Sect. 3.1.1) as ﬁxed background model and building \nseveral independent “Adapters” to include knowledge from different KBs. \nEWISER [14] similarly targets word sense disambiguation (WSD). Starting with \nBERT embeddings, it computes scores for WordNet synsets (sets of words with \nsimilar meaning). Exploiting the interdependence of the synset graph the approach \ncomputes ﬁnal scores that a word belongs to a synset. It achieves a new S\nOTA on a \nnumber of WSD benchmarks (Sect. 5.2). \nPET (P attern-Exploiting Training) [ 184] as an alternative constructs an addi-\ntional training set using only a few labeled examples. Consider a 5-star scale rating \nfor a restaurant in the Yelp dataset [ 185]. The authors add text to the reviews to \nexpress the ratings, e.g. “All in all it was great” . Using this approach the authors \nconvert the Yelp dataset to a task for predicting masked words, e.g. “All in all it was \n[MASK]”. However, they provide the verbalized labels only for a small number of \nexamples. Subsequently, they predict the best class for the non-labeled examples \nand train the model with the predicted classes as well as the language modeling loss \nto avoid catastrophic forgetting. This can be done in several iterations. Although \nonly a few labels have been used, the model performs better on Yelp than standard \nsupervised approaches. The SuperGLUE benchmark data covers eight challenging \nNLP tasks. With just 32 labeled examples the PET approach trained according to the \nabove schema yields a better average (75.4%) than GPT-3 (71.8%) with the same \nnumber of few-shot examples. This shows that good results can be achieved with \na small model (223M) and only few labeled examples. Note that the ﬁne-trained \nS\nOTA for SuperGLUE is 90.4% using T5 and Meena. \nTeKGen [ 1] is a data-to-text sequence-to-sequence model to verbalize a com-\nplete KB. It is applied to the English Wikidata knowledge base [ 214] with . ≈ 6M \nentities and about 1500 relations. The model starts with a large training corpus \nof heuristically aligned Wikipedia text and Wikidata triples. Relations sharing a \ncommon entity subject are converted to the input subject relation . 1 object. 1, ..., \nrelation. n object. n for the T5 transformer (Sect. 3.1.3). As an example “To kill a \nMockingbird, author: Harper Lee, publication date: 11 July 1960” is translated to \n“To Kill a Mockingbird is a novel by Harper Lee published in 1960.” The T5 model \nis ﬁne-tuned and subjected to an addition check to generate good verbalizations.\n124 3 Improving Pre-trained Language Models\nThe resulting dataset of verbalized triples was used in a question answering task. \nIt was able to increase the accuracy in the Natural QuestionsNatural Questions \n(NQ) benchmark [ 109] (Sect. 6.1.2) from 38.8% to 41.5%. KGPT [ 30] in a similar \nway converts structural knowledge into the serialized text and lets model learn \nknowledge-text alignments. \nIn summary these methods transform KB relations into text, e.g. as complete \nsentences expressing relations or as concatenated triples (e.g., [head text, relation \ntext, tail text]) into LMs for training or ﬁne-tuning. This text is transformed into \ncontextual embeddings and the model is trained to detect the underlying relation. \nThe drawback is that focusing on knowledge base completion tends to over-adapt \nthe models to this speciﬁc task, which comes at the cost of generalization. \n3.4.5 Enhancing Pre-trained Language Models by Retrieved \nT exts \nAn open domain question answering system has the task of answering questions \nnot restricted to a speciﬁc domain [ 27]. Consider the following example from the \nTriviaQA benchmark [ 99]. “Question: The Dodecanese Campaign of WWII that \nwas an attempt by the Allied forces to capture islands in the Aegean Sea was the \ninspiration for which acclaimed 1961 commando ﬁlm?” “Answer: The Guns of \nNavarone”. It is not plausible that the model can reproduce such a speciﬁc response \nfrom the knowledge stored in its parameters, even if it was present in the data \nbefore training. Therefore, it would be desirable for the system to be able to gather \nadditional evidence by a retriever collecting relevant documents from a large text \nrepository. Subsequently, it has to align the retrieved information with the question \nand generate an answer by another PLM, a reader. New web search techniques \ncan be used for this approach. They are based on comparing embeddings for \nwords or passages consisting of several sentences. There are numerous applications \nsuch as question answering, summarization, and dialog systems. In Sect.\n6.1 this is \ndiscussed in more detail. Recent surveys are provided by Zhu et al. [ 259] and Y u et \nal. [244]. \nDPR (Dense Passage Retriever) [ 103] employs a PLM to encode KB-passages \n. di, e.g. from Wikipedia, as embeddings .emb(di). This can be achieved by ﬁne-\ntuning a BERT model to encode passages by the embedding of the token [CLS]. \nThese embeddings can be stored in an index for fast access. Then the DPR retriever \nprocesses the query sequence x by another BERT model and generates the query \nembedding .emb(x). A number of .k=100 passages . dj with maximal inner product \n.emb(x)⊺emb(dj ) is retrieved by a nearest-neighbor search. Both BERT encoders \ncan be trained together to generate appropriate embeddings using weak supervision \nin the form of question-answer pairs (cf. Sect.\n6.1.5). If, for instance, the query is \n“Who is the bad guy in lord of the rings” , the algorithm can retrieve “Sala Baker \nis best known for portraying the villain Sauron in the Lord of the Rings trilogy” ,\n3.4 Additional Knowledge for Pre-trained Language Models 125\nbecause “bad guy” and “villain” have similar embeddings. Therefore, DPR can \nﬁnd passages with similar meaning, expressed with different words. Karpukhin et \nal. [ 103], for instance, show that already with 1000 training examples the dense \nretriever is better than the classical keyword search. For 40k training examples the \ntop-20 retrieved passages contain the correct answer in about 79% of the time, while \nthis value is only 59% for the classical retrieval. An in-depth discussion is given in \nSect.\n6.1.5. \nThe DPR r eader is another BERT model. Similar to BERT’s text pair classi-\nﬁcation, it is ﬁne-tuned to predict a probability for each retrieved passage that \nthis passage contains the correct answer. In addition, it selects a span of tokens \nby span prediction, which probably provides the answer. In the example it selects \n“Sala Baker” as the answer. Together both components form a retriever-reader \narchitecture, which recently became popular. The approach can be easily applied to \nKBs with billions of passages [ 103, 201]. On the Natur al Questions [ 109] it yields \na test set accuracy of 41.5%. \nDensePhrases is a different system creating embeddings for phrases of up \nto 20 words in the KB, which are computed without knowing the query [ 114]. \nThe processing of the retrieved phrases directly yields the answer without much \ncomputational effort. Using careful workﬂow optimization the authors achieve near-\nS\nOTA results with a much lower processing time than dense passage retrieval \nsystems, e.g. a test set accuracy of 40.9% on Natural Questions. \nFiD (Fusion in Decoder) [ 91] employs DPR as retriever. In the reader step it \nuses the special tokens “question:”, “title:”, and “context:”. These tokens mark \nthe question, the retrieved passage title and the passage text and are concatenated \nforming the input. Subsequently, these k retrieved triples are fed one-by-one into \na transformer encoder like T5 [ 170] (770M parameters), which independently \nprocesses each triples by the encoder. Only in the decoder the passages are handled \njointly and the text of the answer is generated. This approach drastically reduces the \ncomputational effort. The transformer is ﬁne-tuned on a QA-task. The architecture \nof the model is shown in Fig. 3.18.R a f f e le ta l . [ 170] provided evidence that \n\"Ques/g415on: The \nDodecanese \nCampaign of … \nBERT \nencoder \ninner \nproduct text 3 \ntext 2 \ntext 1 \nBERT \nencoded \nKB \nQues/g415on: \nThe … \nT5 \nencoder \nencoded \nques/g415on \n+ text \nT5 \ndecoder \nconcat Answer: \nThe Guns of \nNavarone \nretriever reader \nFig. 3.18 A retrieval enhanced language model [ 91] encodes the query and the KB passages as \nembeddings and uses a pre-trained retriever to ﬁnd passages corresponding to the query. The reader \nis a Seq2seq model (T5) combining the query and the passages to generate the answer. This model \nsetup is ﬁne-tuned with different benchmark datasets\n126 3 Improving Pre-trained Language Models\ngenerative models like T5 are even competitive for QA-tasks such as SQuAD [ 173], \nwhere answers are spans in a given document. \nThe system achieves a test set exact match accuracy of 51.4% on the Natural \nQuestions benchmark compared to 41.5% for DPR. The TriviaQA benchmark [ 99] \ncontains a set of trivia questions with answers that were originally scraped from the \nWeb. On this benchmark the model yields S OTA results with 80.1% exact match \naccuracy [ 211]. This is better than the accuracy of other much larger models, \nlike GPT3 with 175B parameters (71.2% EM), or T5 without retrieval and 11B \nparameters (60.5% EM). It turns out that increasing the number of retrieved passages \nstrongly enhances the answer quality. \nThere are a number of new approaches to augment PLMs with text from an \nexternal KB. In Sect. 6.1 we describe different PLMs for retrieval that can be used by \nweb search engines. In Sect. 6.2 we investigate systems for question answering that \noften employ a PLM-based retrieval mechanism and an additional PLM to generate \nthe answer text. It combines the query, the knowledge acquired during training, as \nwell as the information in the retrieved documents. \nIn summary, combining language models with retrieval is currently the most \nefﬁcient way to incorporate additional information into PLMs. The new information \nis focused on the current query and thus very informative. The retrieval model \ncan access semantically related passages within fractions of a second using new \napproximate open-source nearest neighbor index structures. By relying on embed-\ndings, synonyms and paraphrases can be found and the meaning of words can be \ndisambiguated. In addition, the underlying knowledge bases can be updated on the \nﬂy to keep the information current. \n3.4.6 Summary \nThe knowledge covered by the textual training data can be leveraged in various \nways to improve the performance of PLMs. Entities and relations from a knowledge \nbase can be represented by embeddings, e.g. by TransE. However, the utilization \nof these embeddings for PLMs is not very efﬁcient and error-prone. A more \npromising alternative is the direct use of table content or knowledge base relations \nby specialized PLMs, which capture relationships between entities and table cells \nby speciﬁc self-attention patterns. Similar to Graph-CNNs PLMs have been directly \nused to acquire the relationship between the nodes of a graph by encoding the \nfeatures of links by embeddings in a BERT-like model. Along this line a promising \nway to transfer relational knowledge from a graph to a language model is proposed \nby GraphFormers. \nA very simple and efﬁcient approach of incorporating tables and knowledge \nbases in PLMs is the creation of text that expresses the information content. This can \nbe used by the PLM either as conditioning text or during training. However, the most \npromising way to include knowledge is retrieval, since most information is stored \nin the form of unstructured text on the Web or databases. Here, the retriever-reader\n3.5 Changing Model Size 127\narchitecture emerged as an effective way to collect relevant passages. Subsequently, \nthe PLM generates new text by combining the internal knowledge, the start text, and \nthe retrieved passages. \nMuch effort was devoted to the extension of the length of input sequences \n(Sect. 3.2). This was mainly achieved by sparse attention patterns reducing the \nincrease in computational effort from quadratic to linear with S4 as a leading \napproach. Nevertheless, larger input sequences still have limited range of context \nboth within the same sample and outside of it. \nIn contrast, retrieval can cover an indeﬁnite context within the same sample by \ngathering appropriate passages, even if there is no simultaneous attention over the \nwhole context. In addition, retrieval can access relevant information in huge docu-\nment collections. Either the highly developed traditional keyword search engines \nmay be used. Alternatively dense retrieval may be employed which compares \nembeddings of the query and passages using approximate nearest neighbor search \nover an index. It turns out that relatively small retrieval-based models outperform \nlarge Foundation Models like GPT-3. FiD, for example, achieves an exact match \naccuracy of 51.4% on the Natural Questions benchmark compared to 29.9% for \nGPT-3. Retrieval is extensively used by recent models such as WebGPT and Retro. \n3.5 Changing Model Size \nThe size of a model, especially its number of parameters, has a marked inﬂuence \non the performance of the model, its memory requirements and the computational \nresources required for training. In the ﬁrst section we discuss that models with \nmore parameters potentially have a better performance. This, however, requires a \nlarger computational effort during training and model utilization. An alternative \nare mixture-of-experts models, which deﬁne a number of parallel model structures \nwhich selectively compute a solution. This is described in the second section. \nAs initial versions of successful models often are extremely large, a variety of \nmodel compression and acceleration techniques have been developed. They reduce \nmemory requirements and training time without noticeable degradation of accuracy, \nand allow the models to be deployed on low resource computing devices, such as cell \nphones. There are three main techniques for model size reduction [\n65]—parameter \ncompression and reduction, low-rank factorization, and knowledge distillation— \nwhich are outlined in the subsequent sections. \n3.5.1 Larger Models Usually Have a better Performance \nAs a rule for machine learning, the number of parameters of a model should be \nlimited to avoid overﬁtting, i.e. adapting to random ﬂuctuations in the data. It turned \nout that this does not hold for PLMs if the amount of training data and the number of\n128 3 Improving Pre-trained Language Models\nmodel parameters are increased simultaneously. Larger PLMs have been shown to \nhave better performance on NLP tasks, which is underscored by theoretical work on \nPLMs [ 19, p. 117]. The beneﬁts of increasing the number of parameters come from \ntwo factors: additional computations at training and inference time, and increased \nmemorization of the training data. Kaplan et al. [ 102] empirically investigated \nin detail the dependency between the number of model parameters R (excluding \nembeddings), the size N of the training data, and the amount of computing effort C \nused for training. They evaluated a large number of models and draw the following \nconclusions: \n• The performance of the models depends largely on the size quantities .R,N,C . \nOther architectural features such as width or depth have only a weak inﬂuence. \n• The performance follows a smooth power-law dependency with each of .R,N,C , \nif the other quantities are not too small. As an example the loss is approximately \n.L≈(N/(5.4∗10 13))−0.095. \n• If R and N are increased at the same rate, the model accuracy grows reliably. If \none of these factors is held constant the improvement gets lower. To get the best \nperformance, the model size R should grow with the factor 8, if the data N is \nincreased 5 times. \n• Training loss has a predictable dependency on computing effort and can be \nextrapolated. \n• The performance of ﬁne-tuning of a pre-trained model on a different training task \ndepends strongly on the loss for the pre-training validation set. Therefore, transfer \nto a different distribution induces a constant penalty, but roughly improves with \nthe performance on the pre-training set. \n• Large models are better able to extract information from data than small models. \nThey reach the same level of accuracy with fewer optimization steps and using \nfewer data points. If there is only a ﬁxed amount of computation time, but no \nrestrictions on size or data, one should use very large models and stop before \nconvergence (Fig.\n3.19). The optimal batch size depends on the gr adient noise, \nwhich is easy to measure during training [ 132] and is larger than assumed before. \nThese ﬁndings show that the success of larger PLMs is a systematic feature. A \nlarger number of model parameters is much more sample efﬁcient than thought \nbefore, when overﬁtting was a major problem for smaller training tasks. This also \nexplains the success of large models like T5, BigBird, or GPT-3. Hernandez et \nal. [\n80] investigate empirical scaling laws for the transfer from pre-training to ﬁne-\ntuning. Figure 3.20 plots the training efforts of some Deep Learning models during \nthe last two decades.\n3.5 Changing Model Size 129\nFig. 3.19 A series of language model training runs with varying model sizes [ 102]. The left \ngraph shows that larger models require fewer samples to reach a ﬁxed test loss. The right graph \ndemonstrates that the model size should grow with compute budget. Image reprinted with kind \npermission of the authors [ 102,p .4 ] \nFig. 3.20 Number of parameters for Deep Learning Models since 2017 [ 188]. Note that the \nparameter scale is logarithmic. The number of parameters roughly increased from 100M up to \n1000B \n3.5.2 Mixture-of-Experts Models \nAs discussed above a model with more parameters usually can achieve a better \nperformance. A simple way to increase the number of parameters without a higher \ntraining effort is a mixture-of-experts architecture. It was already proposed in the \nnineties by Nowlan et al. [\n147] and has a strong resemblance to decision tree models\n130 3 Improving Pre-trained Language Models\n[152]. It consists of a single gating module and a number of expert modules with \nidentical architecture but different parameters. Each expert specializes in only a \nsubset of the data, and the gating module assigns each input to the appropriate \nexperts. Speciﬁcally, the gating network computes a probability distribution over \nthe experts indicating how well each expert is able to process the incoming input. A \nreduction in computational effort can be achieved, if only a few expert modules \nare actually used. The model is trained by stochastic gradient descent, which \ncan compute the parameter gradient despite the discontinuities if some expert is \nexchanged. Increasing the number of experts keeps the computational cost constant \nbecause the model always selects the same small number of experts for each input, \nregardless of the total number of experts. The architecture enables massive models \nand is particularly efﬁcient for distributed systems where the experts are spread \nacross different computational devices. \nClark et al. [ 38] analyze the theoretical properties of such r outing networks, \nwhere each input is processed only by subnetworks with a fraction of the network’s \nparameters.The authors analyze three different architectures and get the following \nresults. \n• Routing improves the performance of PLMs in all investigated sizes and variants. \n• Improvement follows a power-law in the number of experts E that diminishes \nwith model size N, and can be further generalized across routing architectures. \nThe analysis is based on the evaluation of several magnitudes of size, including \nmodels with hundreds of experts and hundreds of billions of parameters. \nGLaM [ 51] is an autoregressive mixtur e-of-experts (MoE) model with up to \n1200B parameters. It replaces the fully connected layer of every second encoder \nblock (Sect.\n2.1.1) with 64 copies having different parameters. For each embedding, \na gating module selects two of these 64 fully connected layer for processing. The \narchitecture is shown in Fig. 3.21. The model was trained on a huge collection of \n1.6T tokens documents and quality-checked web pages. It has approximately 7 times \nmore parameters than GPT-3 but requires only 1/3 of its training effort. In this way, \nthe model has many more parameters increasing its representational capacity. As \nfor a given input token, only two expert models are used, the computational effort \nfor training and application is lower. The zero-shot and one-shot performance is \nbetter than for GPT-3 on 29 NLP tasks. Some results are compared to those of other \nmodels in Tables \n3.3 and 3.4. GLaM is remarkable as it requires only 1/3 of the \ntraining effort of GPT-3 but it achieves a similar or better performance than GPT-3 \non NLP tasks. \nWuDao-2.0 [175, 178, 257] is a recent giant autoregressive language model with \n1750B parameters, ten times larger than GPT-3. It has mixture-of-experts layers, \nwhere a gating network selects a submodule for processing based on the input. \nWuDao-2.0 uses the FastMoE library [ 74] and employs the GLM 2.0 architecture \n(Sect. 3.1.3) combining the different learning paradigms of BERT, GPT and the \nencoder-decoder transformer [ 175]. \nThe training data consist of 1.2TB Chinese text, 2.5TB Chinese graphic data and \n1.2TB English text data from the Pile corpus [ 61]. The Co gview model is used for\n3.5 Changing Model Size 131\nFig. 3.21 Architecture of GLaM [ 51]. For each input token, e.g., “likes”, the gating module \ndynamically selects two most relevant experts out of 64 available experts. This is indicated by \nthe blue grid. The weighted average of the outputs from these two experts’ feedforward models \nis then passed to the next encoder block. For the other inputs different experts are selected. A \nmixture-of-experts layer is used in every second encoder block \nthe joint processing of images Sect. 7.2. In addition, WuDao-2.0 can learn on the ﬂy, \ndraw pictures and compose poetry. These capabilities are a signiﬁcant difference to \nGPT-3. \nThe published performance claims are impressive. On the LAMA benchmark for \nmeasuring world knowledge [ 158] it scores higher than AutoPrompt [ 192]. For the \nSuperGLUE fe w-shot natural language understanding task [ 219] it achieves S OTA \nand surpasses GPT-3. For the Lambada benchmark (Sect. 4.1.3), where the last word \nof a paragraph has to be predicted, it yields better results than Microsoft Turing \nNLG. In addition, it increases S OTA for a number of text-graphics tasks (Sect. 7.2.8). \nSwitch [ 56] is a variant of the transformer encoder-decoder T5 (Sect. 3.1.3). It \nhas a mixture-of-experts architecture, which replaces the fully connected layer of \neach encoder block with .k=128 copies having different parameters. There is a \nsimple linear gating network, which selects one of the 128 single fully connected \nlayers (the experts) per token. Hence, the number of parameters is drastically \nincreased with approximately constant computational effort. For this architecture \na gradient can be computed and the model may be optimized using a number \nof speciﬁc strategies and a special TensorFlow version. It turns out that Switch \nachieves the same loss level compared to the standard T5 version with 1/7 of the \ncomputing time. On a number of ﬁne-tuning tasks the large Switch model with \n1600B parameters and 2048 experts yields better results than T5-large (Sect.\n3.1.3) \nwith 13B parameters requiring a quarter of the computational training effort. \nAs an alternative to the gating network in the mixtures-of-experts architecture, \nit is possible to use hash values to activate different parts of the network. Token \nSwitch [\n177] computes a hash value for each input token and routes the generated \nembeddings of each token to different feedforward networks based on the hash\n132 3 Improving Pre-trained Language Models\nvalues. The authors show that their approach compares favorable to Switch and \nworks well on comprehensive language modeling tasks. \nST-MoE-32B [261] is a mixture-of-experts model with 269B parameters and a \ncomparable training cost of a 32B dense model. The authors modify the routing \nalgorithm which dispatches token embeddings to one or two experts, and resolve \ninstability issues. The model is similar to a T5-Large encoder-decoder [ 170]. The \nST-MoE-32B has 32 experts with an expert layer frequency of 1/4, such that every \nfourth feedforward layer of T5 is replaced by an MoE layer. The authors use the \nGEGLU activation function, which contains multiplicative elements [\n142] \n.FFN GEGLU (x,W,V,b,c)=GELU(xW+b)⊙(xV+c). (3.2) \nThe authors compare a large number of variants and hyperparameters to improve \ntraining. \nThe model achieves S OTA in many transfer learning benchmarks, e.g. for \nSuperGLUE with an average accuracy of 93.2% beating the PaLM LM with 540B \nparameters. Other S\nOTA results were reached for summarization (XSum [ 143] with \n27.1 R OUGE -2, CNN/Daily Mail [ 78] with 21.7 R OUGE -2), closed book question \nanswering (WebQA [ 13] 47.4% exact match, Natural Questions [ 109] 41.9% \nexact match), and adversarially constructed tasks for common sense reasoning \n(Winogrande [182] 96.6%, ANLI R3 [ 146] 74.4%). \n3.5.3 Parameter Compression and Reduction \nModel quantization is a parameter reduction technique, where parameters are stored \nin low precision and therefore the computations in PLMs are also less precise. \nConventional models normally use parameters of 32 bits or 16 bits, while parameters \nafter quantization can have 8 bits or even 1 or 2 bits. Q-BERT [ 190], for example, \nquantizes Transformer models to ultra-low precision. This reduces the model size \n13-fold while only loosing 2.3% performance. The authors avoid the naive approach \nof simply reducing weight precision, but use additional training steps to adjust the \nquantized weights and allow higher precision for more “sensitive” parameters. Other \nauthors propose to delete parameters with small values [\n64]. ALBERT [ 113]u s e s \nthe same weights across all layers and achieves a signiﬁcant parameter reduction. \nNevertheless, ALBERT has the same or better performance compared to BERT. \nAnother approach aims to reduce the number of parameters, e.g. by removing \nattention heads. It was shown that most attention heads focus only on nearly \nidentical positional relations and can be replaced with ﬁxed attention patterns [ 172]. \nIt turned out that high performance is possible with only 1–2 attention heads per \nencoder unit instead of the 16 attention heads of the original model. A detailed \noverview on parameter compression techniques is provided by Ganesh et al. [\n60]. \nAnother method to reduce model parameters is model pruning, which cuts off \nirrelevant parts in PLMs to achieve a smaller memory footprint and faster execution\n3.5 Changing Model Size 133\nwithout compromising performance. It could be shown, for example that some \nattention heads of the transformer may be removed with little impact on the accuracy \n[256]. Other researchers prune the weights of attention layers and linear layers to \nreduce the number of parameters without reducing the accuracy [ 29, 64]. Note that \nmodel pruning does not always lead to speedups, as sparse computations may be \nhard to parallelize on GPUs. \n3.5.4 Low-Rank F actorization \nThis technique employs matrix and tensor decomposition to reduce the number \nof parameters of full rank parameter matrices and already has been discussed \nin Sect.\n3.2.2 for the extension of the input sequence length. Examples are the \nPerformer [34] and the Linear Transformer [ 105] (Sect. 3.2.2). As an alternative, \nALBERT (Sect. 3.1.1) approximates the embedding matrix as a product of two \nsmaller matrices. \n3.5.5 Knowledge Distillation \nIn machine learning the knowledge distillation approach [ 82] transfers knowledge \nfrom a large teacher model to a smaller student model. The large model can \noften be trained successfully to approximate a functional relation without using \nits full representational capacity. To reduce the high computational and memory \nrequirements during application, a smaller model is trained to imitate the large \nmodel without sacriﬁcing accuracy. \nThe advantage of this approach is that the student model may be trained to \napproximate internal activations of the teacher model. Often the target probabilities \ngenerated by the teacher model are used to train the student network . Typically the \noutputs of the teacher model for an input . x is .z(x), which can be translated to a \nprobability by a scaled softmax \n.y(x|τ)= [exp(z1(x) / τ) ,...,exp(zk(x))/τ]\nexp(z1(x)/τ)+···+exp(z k(x)/τ) , (3.3) \nwhere .y(x|τ) is a probability vector and . τ is a parameter called temperature, which \nfor a standard softmax is normally set to 1.0. The student model is trained to imitate \nthe probabilities\n.ˆy(x|τ) generated by the teacher model by minimizing cr oss entropy \n.E(y|τ)=−\nk∑\nj=1\nˆyj (x|τ)logy j (x|τ), (3.4)\n134 3 Improving Pre-trained Language Models\nwhere .y(x|τ) is the output probability vector of the student model. If observed \nvalues are available the probabilities of the teacher model .yj (x|τ) may be replaced \nby 1.0 for the observed class and 0.0 otherwise. During training the temperature \nmay be varied. A high temperature avoids extreme probability values and reduces \nthe gradients. This may lead to a faster convergence in the beginning of the \noptimization. \nDistilBERT [ 183] uses MLM cross-entropy loss to predict token probabilities \nand in addition the cosine similarity between the embedding matrices of the teacher \nand student networks to train a smaller BERT model. It utilizes knowledge distilla-\ntion during pre-training to reduce the size of BERT by 40% while retaining 99% of \nits original capabilities and making the inference 60% faster. MobileBERT [\n204]i s \nbased on a speciﬁc large BERT model and transfers information about multi-head-\nattention as well as the resulting embeddings. Experiments show that MobileBERT \nis 4.3. × smaller and 5.5 . × faster than BERT while achieving competitive results on \nwell-known benchmarks. \nTinyBERT [ 97] proposes distillation of a BERT model during pre-training \nand ﬁne-tuning. The model is adapted to: (1) the output of the embedding of \nselected layers; (2) the hidden states and attention matrices derived from selected \nTransformer layers; (3) the logit outputs of the prediction layer. As distillation \nis also performed during ﬁne-tuning the model can be better adapted to the ﬁne-\ntuned BERT. On a number of benchmarks TinyBERT is on par with BERT .BASE and \noutperforms DistilBERT. \nNote that the knowledge distillation methods discussed above require the data \nused for pre-training the teacher model, which is often not released because of data \ncopyright. It has not yet been evaluated whether distillation is also feasible with new \ndata. The training time for knowledge distillation is high, because the teacher model \nneeds to perform a forward prediction over the entire pre-training data to generate \nactivation values or intermediate representations. \nRogers et al. [ 176] list a large number of size reduction studies for BERT \nand report parameter size and computing time reduction as well as the resulting \nperformance. For a number of approaches there is a marked reduction in memory \nand computing effort with nearly identical performance. \n3.5.6 Summary \nThe number of model parameters, the size of the training data and the amount of \ncomputation effort for training are the determining factors for the performance of a \nmodel. Kaplan et al. [\n102] show by experiments that increasing parameter count and \ntraining set size reliably lead to a better performance and provide a detailed formula \nfor the dependency. If a ﬁxed compute budget is available, one should use a very \nlarge model and much data. \nMixtures-of-experts follow this approach by increasing the number of parameters \nwithout requiring more computational effort. By routing inputs to speciﬁc subnet-\n3.6 Fine-Tuning for Speciﬁc Applications 135\nworks they are able to increase performance compared to monolithic networks. \nExamples are GLaM, WuDao-2.0, and Switch. However, these networks have \nhundreds of billions of parameters and require a speciﬁc parallel computational \ninfrastructure. \nOften the trained networks are too large and have to be reduced to ﬁt to smaller \ncomputing devices. A viable approach is low-precision computation, which reduces \nmemory requirements for parameter storing. Low-Rank factorization of matrices \nalso has a lower memory footprint as a side effect. Finally, knowledge distillation \nmay be employed to create a student model which imitates the inner working of \na large trained teacher network. DistilBERT, for example, was able to reduce the \nmemory size by 40%, kept 99% of the original performance and was 60% faster. \nThere are a number of other size reduction approaches with similar results. \n3.6 Fine-Tuning for Speciﬁc Applications \nSelf-supervised pre-training of language models on large text collections and subse-\nquent ﬁne-tuning them to solve speciﬁc tasks has become the standard paradigm in \nnatural language processing and understanding. It has been shown that pre-trained \nlanguage models such as BERT are excellent for generalization and can easily be \nﬁne-tuned to multiple tasks. However, sometimes simple ﬁne-tuning to a domain-\nspeciﬁc task is not sufﬁcient, and other transfer learning approaches have to be used \nto better adapt models to domain-shift in the data [\n166]. There are a number of \nsurveys covering transfer learning in depth [ 230, 252, 260] \nFine-tuning updates all the model layers, including the embedding layer, but there \nare larger changes in the higher layers [ 133]. First, we discuss whether ﬁne-tuning \ncan destroy the knowledge gained during pre-training. Standard ﬁne-tuning adapts \na large pre-trained PLM with many parameters to a relatively small ﬁne-tuning \ntraining data set with little computational effort. We investigate whether overﬁtting \noccurs during this phase. Subsequent sections introduce different approaches for \nﬁne-tuning: \n• Intermediate Fine-Tuning performs an in-between ﬁne-tuning step with a larger \ntraining set before a ﬁnal target ﬁne-tuning takes place. \n• Multitask ﬁne-tuning enhances the model capabilities by simultaneously ﬁne-\ntuning on a number of tasks. \n• Fine-tuning a frozen model adapts a small additional layer to the ﬁne-tuning task \ninstead of changing all weights of the large pre-trained model. \n• Creating Prompts for Few-Shot Instructions aims to generate inputs for a large \nautoregressive PLM like GPT-3 to solve a task in a zero or few-shot approach.\n136 3 Improving Pre-trained Language Models\n3.6.1 Properties of Fine-Tuning \nFine-tuning of PLMs is commonly employed to adapt a pre-trained model to a \nspeciﬁc task by supervised training. This adaption of the model from a source task to \na related target task is also called transfer learning. Transfer learning is especially \nrewarding if we have abundant training data for self-supervised learning—as it is \ntypical for non-annotated text—and only little annotated data for the target task. A \nsurvey of transfer learning is provided by Zhuang et al. [\n260]. Fine-tuning has a \nnumber of advantages: \n• The model acquires detailed knowledge about the language, its syntax and \nsemantics by exploiting the content provided in the pre-training data. \n• Pre-trained models can easily be adapted to new tasks, e.g. by an additional layer \nwith a simple classiﬁer. The language representations of the pre-trained model \nsupport ﬁne-tuning and are only slightly changed during this process. \n• Fine-tuning even with a small data set yields a much better performance than \ndirect training of a classiﬁer on the limited data. \nAutoencoder models like BERT are typically ﬁne-tuned for classiﬁcation tasks, \nwhere the logistic classiﬁers for masked language modeling and next sentence \nprediction have to be removed. Using the \n[CLS] token or other tokens as input, \nnew logistic classiﬁer models as well as all model parameters are trained end-to-end \nwith the new task for a few epochs (Sect.\n2.1.3). Compared to pre-training, ﬁne-\ntuning is relatively inexpensive. Usually, only a small fraction of the pre-training \neffort is required to achieve good results. \nTripuraneni et al. [ 210] have theoretically proven that transfer learning requires \nfar less data than learn tasks in isolation. They prove that transfer learning improves \nif the task diversity is enhanced. Bansal et al. [ 7] investigate the theoretical \nproperties of ﬁne-tuning a classiﬁer using pre-trained embeddings. The authors \nprove that these classiﬁers have a smaller generalization gap between their train \nand test accuracy, than standard classiﬁers. \nCatastrophic Forgetting \nThe question is whether ﬁne-tuning can destroy the original capabilities of the \nmodel. This means, after ﬁne-tuning a pre-trained model for a few epochs, it could \nlose predictive performance available after pre-training. A possible reason can be \ncatastrophic forgetting, where all parameters are adapted to a new learning task \nwhile forgetting learned content. \nMerchant et al. [ 133] ﬁne-tune BERT .BASE with three different tasks: (1) MNLI \nsentence pair classiﬁcation task [ 229] measuring if the ﬁrst sentence entails the \nsecond; (2) SQuAD question answering [ 173], where the answer to a question has to \nbe marked in a text; (3) Dependency Parsing [ 50] to capture the syntactic structure of \nsentences. Then they investigate the performance of a number of probing classiﬁers\n3.6 Fine-Tuning for Speciﬁc Applications 137\nbefore and after ﬁne-tuning. The results demonstrate that the ﬁne-tuned models only \nshow a small decrease in the accuracy to detect linguistic concepts. The reduction \ncause by the MNLI task in most cases is less than 1%, while higher differences (less \nthan 3%) are observed for SQuAD and dependency parsing. Therefore, catastrophic \nforgetting cannot be observed. The authors state that ﬁne-tuning primarily changes \nthe top layers of BERT, with dependency parsing also affecting deeper layers. More \ndetailed results are provided by Wallat et al. [\n216]. \nFine-tuning only beneﬁts from the pre-training, if there are similarities between \nthe two tasks. Hence, pre-training should have a loss function which enforces the \nlearning of semantics at word, phrase and document level. In addition, its training \ndocuments should originate from a domain close to the ﬁne-tuning task. Otherwise \nthe vocabulary may not include many domain-speciﬁc words. As a result, domain-\nspeciﬁc words are split into a number of tokens which hinders model learning and \ndegrades its performance in downstream tasks. In the next sections we will discuss \nalternative training regimes which improve BERT’s capabilities. \nFine-Tuning and Overﬁtting \nDuring pre-training BERT’s parameters are adapted to the pre-training data, acquir-\ning universal language representations. As pre-training provides a good initializa-\ntion, it avoids overﬁtting on the small ﬁne-tuning datasets, if the ﬁne-tuning error is \nnot minimized too much. \nSince PLMs have a very large number of parameters, there is the risk of \noverﬁtting on the ﬁne-tuning data. As a result, generalization from unseen data \ncan be poor and counterstrategies may be required. D’Amour [ 42] present a \ncomprehensive discussion of this underspeciﬁcation phenomenon. Jiang et al. [ 95] \nintroduces a form of regularization, which makes the model invariant to small \nperturbations of the input, inducing smoothness in the local neighborhood. They \ndevelop a class of Bregman proximal point optimization methods, which penalize \nlarge updates of the model at each iteration. Aghajanyan et al. [\n2] introduce the \nnotion of representational collapse, stating that ﬁne-tuned models lose their ability \nto generalize. They propose ﬁne-tuning optimization based on trust-region theory, \nwhich alleviates representational collapse at a fraction of the cost of other recently \nproposed ﬁne-tuning methods and, for instance, improves the best known results on \nﬁne-tuning RoBERTa on GLUE. \nFine-tuning the same model with multiple random seeds can lead to large \nvariance in task performance. Most papers argue that this effect is caused by \ncatastrophic forgetting and the small size of the ﬁne-tuning datasets. However, \nMosbach et al. [\n140] show that often ﬁne-tuning has an optimization problem due to \nvanishing gradients. In addition, it can often occur that a model does not generalize \nwell, although it has the same ﬁne-tuning loss as a successful model. This is an \nindication for the underspeciﬁcation mention above. The authors recommend to \nuse small learning rates with bias correction to avoid vanishing gradients early \nin training. In addition, they propose to use more iterations for ﬁne-tuning. More \nrecipes to improve ﬁne-tuning are provided by Rogers et al. [\n176].\n138 3 Improving Pre-trained Language Models\n3.6.2 Fine-Tuning V ariants \nFine-Tuning in Two Stages \nThe intermediate training set should be closer to the ﬁnal task. Although this \napproach can increase performance in some cases, an experimental evaluation \ndemonstrates a decrease in performance in 44% of the cases [ 163]. An intermediate \ntraining with a task requiring high-level inference and reasoning abilities tend to \nwork best, as was shown in a large experiment [ 165]. However, the authors also \nobserve catastrophic forgetting of the pre-trained abilities. Gururangan et al. [ 71] \nhave shown that a second phase of pre-training, using domain-speciﬁc data, leads to \nsigniﬁcant performance gains, both in high- and low-resource settings. In addition, \npre-training on tasks-speciﬁc unlabeled data improves performance on various tasks \nand domains. \nFine-Tuning for Multiple Tasks \nFor each task, a task-speciﬁc layer is added to the underlying pre-trained model. \nThen the model is simultaneously trained with all tasks. However, it sometimes \nhappens that performance does not increase compared to standard ﬁne-tuning [\n141], \nperhaps because of contradicting requirements of tasks. As an alternative, a subset \nof ﬁne-tuning tasks from the available datasets may be selected based on similarity \nmeasures [\n131]. \nHyperGrid [ 208] is a multitask learning approach evaluated on the T5 model. \nIt learns grid-wise projections that help to specialize regions in weight matrices \nfor different tasks. As an example, a single model is simultaneously adapted to all \nGLUE and SuperGLUE tasks at once. In spite of the multitude of tasks, the model \nhas a slightly better performance on SuperGLUE than the single models. \nMeta-Learning to Accelerate Fine-Tuning \nDuring ﬁne-tuning a pre-trained PLM is adapted to a new NLP task. It is usually \ntrained for two or three epochs on a labeled ﬁne-tuning dataset. Although this is \nmuch faster than pre-training the model on a large training corpus it still requires a \nlot of effort. To reduce this effort researchers tried to prepare the pre-trained model \nto ﬁne-tuning by meta-learning. A survey of meta-learning is provided by Yin [\n242]. \nUsually, there is a set . Tof related ﬁne-tuning tasks . Ti. During meta-training \nat a s k . Ti is sampled from a distribution .p(T). Then the model is trained with K \ntraining samples from .T train\ni and then tested on the validation set of .T val\ni .T h e\n3.6 Fine-Tuning for Speciﬁc Applications 139\nvalidation error of . Ti is utilized as the training error of the meta-learning framework \nfor the current iteration. The MAML algorithm [ 58] follows this pattern: \n• Copy .w[i] of the initial model parameters . w. \n• Train the model on the training set .T train\ni with a K gradient updates: . ˆw[i] ←\nw[i] −γ∂L i(w[i],T train\ni )/∂w\n• Apply the model with the updated parameters . ˆw[i] on the validation set .T val\ni . \n• Update the initial model parameters . w using the loss on the validation set . w←\nw−β∂L i( ˆw[i],T val\ni )/∂w\nThis scheme was applied to BERT [ 6]. The authors generate a large, rich, meta-\nlearning task distribution from unlabeled text by gathering tokens-to-be masked \nfrom a few vocabulary terms. On 17 NLP tasks, they show that this type of meta-\ntraining leads to better few-shot generalization than language-model pre-training \nfollowed by ﬁne-tuning. Chen et al. [ 28] provide data-dependent generalization \nbounds for these approaches. \nFine-Tuning a Frozen Model by Adapters \nA downside of ﬁne-tuning for task-adoption is that new model parameters are \nneeded for every task. Task adapters [\n84] aim to mitigate this problem. The authors \nintroduce adapter layers, which are inserted in a encoder block after the multi-head \nattention and the feedforward layer ( 2.7). Now, to ﬁne-tune transformer models to \nnew tasks, instead of relearning all parameters, all weights of the network are frozen \nexcept for the adapter layers and the normalization layers. On tasks like GLUE this \nyields a signiﬁcant reduction of parameters that need to be trained while preserving \nmodel quality. \nRather than having multiple adapters for different tasks, Stickland et al. [ 197] \npropose training a multitasking version of BERT that can be used for several tasks \nsimultaneously. They add low-dimensional projected attention layers as bypass \nto BERT encoder blocks, which connect the input to layer-norm layers and the \nsubsequent layer-norm layers. They sample data from the different tasks during \ntraining proportionally to the sizes of the respective training sets and use an \nannealing mechanism to converge towards equally distributed training samples by \nthe end of the training. Their results surpass the results of a BERT\n.BASE model. \nMAD-X [ 160] is a framework to adapt multilingual models to arbitrary lan-\nguages and tasks. The authors introduce language- and task-speciﬁc adapters, which \nconsist of a linear down-projection to a small vector, a ReLU activation and a linear \nup-projection. The language speciﬁc adapters are trained with an MLM objective, \nwhile the rest of the model is frozen. The task-speciﬁc adapters are trained with \nthe task-speciﬁc data, ﬁxing the rest of the parameters. Finally, invertible adapters \nare added after the input embedding layer and before the output embedding layer \nto mitigate differences between the multilingual vocabulary and the target language\n140 3 Improving Pre-trained Language Models\nvocabulary. MAD-X achieves S OTA for NER and common sense reasoning for a set \nof different languages. \nLoRA [ 85] freezes the weights of the pre-trained model and adds trainable \nbypasses to the model, which consist of trainable matrix transformations to a \nshort vector and to the full rank. This drastically reduces the number of trainable \nparameters (1/30 for GPT-3 and 1/100 for GPT-2) while achieving better results than \nwith traditional ﬁne-tuning on many NLP tasks. AdapterHub [\n161] is a repository \nfor adapters that as of writing contains around 380 adapters. AdapterHub is built \non the Hugging Face transformer library for compatibility with existing transformer \nmodels. \nFine-Tuning GPT-3 \nGPT-3 is an extremely powerful Foundation Model, but it is not publicly available \n(Sect.\n3.1.2). By using the API for ﬁne-tuning GPT-3 with user-speciﬁc data [ 123], \nthe model can be adapted to speciﬁc domain languages and particular tasks. \nThis typically yields a higher quality than few-shot examples and prompt design \ndescribed below. To ﬁne-tune the 175B parameter model on a 1M token ﬁle for four \nepochs OpenAI charges about $120. The ﬁne-tuning can be used in a number of \nways [\n123]: \n• Completion: Generate a completion for a prompt. \n• Search: Given a search query and a set of documents or labels, the model ranks \neach document with a score based on its semantic similarity to the query. \n• Classiﬁcation: Input is a query and a set of labeled examples, e.g., [“I am feeling \nawesome”, “Positive”]. Then GPT-3 will predict the most probable label for the \nquery. This can be used similar to BERT for any type of classiﬁcation task. \n• Answer: Input is a question, a set of documents with background information, and \nsome examples. Based on the information in the documents and the examples, an \nanswer is generated. This is similar to the reading comprehension task of question \nanswering (Sect.\n6.2). \n• Fine-tune: Adapts GPT-3 to a speciﬁc domain text. \n• Embeddings: Get a vector of contextual embeddings for an input text for further \nprocessing or exploration. \nIt can be assumed that GPT-3 and other Foundation Models like PaLM ﬁne-tuned in \nthis way will increase S\nOTA in many areas due to their comprehensive knowledge \nabout language. \n3.6.3 Creating F ew-Shot Prompts \nFor zero-shot learning the model just gets a task description or prompt,e . g . \n“Translate English to French: cheese = . >”, and directly generates the answer\n3.6 Fine-Tuning for Speciﬁc Applications 141\nFig. 3.22 The accuracy of few-shot learning of GPT-3 is increased by extending the model size \nas well as the number of presented examples [ 25]. The task is to remove random symbols from a \nword. A natural language description of the task can support the model especially in the one-shot \nregime. Image reprinted with kind permission of the authors [ 25,p .4 ] \n“fromage”.F o r one-shot or few-shot learning the model receives a task description \nas well as one or more examples, e.g. “Translate English to French: sea otter = . >\nloutre de mer; cheese = . >”, which helps the model to ﬁnd the answer “fromage”. \nThis happens without training, the parameters of the model are not changed, and the \nmodel creates the answer based on the knowledge acquired during pre-training. \nIn this way, GPT-3 can be instructed by natural language prompts to generate \nshort stories, songs, answers to questions, press releases, technical manuals, and \nmore [181]. It can adapt its output texts to speciﬁc styles, personalities or ideologies. \nHere are some of the recommended prompts used for few-shot learning [ 150]: \n• Summarization: the model receives a long story and the prompt “tl;dr:”. \n• Grammar correction “Original: She no went to the market. Standard American \nEnglish:” \n• Translation: “English: I do not speak French. French: Je ne parle pas français. \nEnglish: Where is the restroom?” French: \n• Generate an outline for an essay: “Create an outline for an essay about Walt \nDisney and his contributions to animation: \nI: Introduction” \nFigure 3.22 shows the accuracy of “few-shot learning” for different GPT-3 model \nsizes and different numbers of given examples. \nIn a comprehensive survey Liu et al. [ 125] compile approaches to prompt design \nto create prompts for language models that reliably generate the desired response. \nFor example, when we want to recognize the sentiment of the text “I missed the\n142 3 Improving Pre-trained Language Models\nbus today.”, we may insert the prompt “I felt so ”, and use the language model to \nreplace the blank. There are two types of prompts: cloze prompts [ 159], which ﬁll in \nthe blanks of a textual string by an autoencoder model similar to BERT, and preﬁx \nprompts [117], which continue a text by an autoregressive language model. \nFor prompt mining [ 96], for instance, a large number of sentences with phrases x \nand y are collected. Subsequently, prompts are generated using the words between \nx and y, or on the dependency path generated by parser. Another approach is \nbased on paraphrasing existing prompts, for instance by translation to another \nlanguage and back-translation. The probability of desired answers may be increased \nby gradient-based search [\n192] as demonstrated with the A utoPrompt model. \nAlternative approaches are described in [ 62, 245]. It should be noted, however, that \nthe output of a model instructed with few-shot prompts can be easily altered if an \nadversary adds some new prompts [ 79]. \nInstead of improving prompt tokens, which generate a desired output by the \nlanguage model, one can optimize the input embeddings of some “virtual” tokens, \nsuch that the desired answer is created. The embeddings of this “continuous” prompt \ncan be optimized by gradient descent while keeping the parameters of the language \nmodel ﬁxed [\n121]. Lester et al. [ 117] apply this approach with a continuous prompt \nsequence of 100 tokens to the T5 transformer. On the SuperGLUE benchmark they \nachieve the same performance of 90.5% as for ﬁne-tuning T5. This demonstrates \nthat prompt tuning becomes competitive with ﬁne-tuning and is much better than \nfew-shot instructions. Note that the effort for prompt tuning is much lower than for \nﬁne-tuning, as the number of parameters is much smaller. It would be interesting to \nsee this technique applied to recent autoregressive models like GPT-3 or PaLM. \n3.6.4 Thought Chains for F ew-Shot Learning of Reasoning \nTo improve the reasoning capabilities of language models, prompts can contain a \nchain of thought, a sequence of short sentences that imitate the reasoning process \na person might have when answering a question [\n226]. Two examples are shown \nin Fig. 2.21. The idea is that a chain of thought allows language models to split a \nmultistep problem into intermediate steps that are solved one at a time, rather than \nsolving an entire multistep problem in a single pass. \nThe approach has a number of advantages. First, the chain-of-thought approach \nenables a model to decompose complex reasoning tasks into simpler intermediate \nsteps, which can be solved by the model. To solve an entire class of problems, only \na few chains of thought need to be provided. Second, when a model performs the \nintermediate steps, it is easier to check where the model has introduced an error. This \nmay give a clue how to improve the chain of thought. Chain of thought reasoning \ncan be applied to symbolic manipulation, common sense reasoning and math tasks, \nand is potentially applicable to any task that humans can solve via language. \nPrompts also do not need to be restricted to input-output pairs or explanations \nand can cover many arguments, including things to avoid, rules of thumb, reasoning\n3.6 Fine-Tuning for Speciﬁc Applications 143\nchains, positive or negative examples. Mishra et al. [ 138] consider instructions \nfor crowdworkers, which contain very detailed prescriptions how to solve a task. \nThey compile a dataset of tasks, instructions and generated input-output pairs. \nSubsequently, they investigate how well models are able to generalize to similar \ntasks. The results show that PLMs beneﬁt from instructions when evaluated in terms \nof generalization to unseen tasks (19% improvement). However, there is much room \nfor improvement. \nDu et al. [ 52] investigate few-shot learning theoretically. They investigate the \ncase that a model is pre-trained on a number of tasks with a large training set and \nsubsequently ﬁne-tuned on a related task. They theoretically derive bounds on the \nrequired sample size for the ﬁne-tuning task, which can be reduced when there is a \ngood common representation. \n3.6.5 Fine-Tuning Models to Execute Instructions \nInstead of querying autoregressive PLMs by few-shot instructions it is possible to \nﬁne-tune these models to execute instructions without additional examples. \nInstructGPT [ 151] is a new version of GPT-3. It is optimized to follow \ninstructions instead of predicting the probable next words. Instead of needing a \nseries of examples, GPT-3 now directly executes an instruction, e.g. “Write a short \nstory about the moon and the stars:” , and the model generates a plausible story. In \na ﬁrst trial a dataset of 13k pairs of instructions and completions was collected \nto adapt GPT-3. GPT-3 was ﬁne-tuned using this data. However, the model did \nnot adequately match the intended human preferences. Therefore, the model was \nmodiﬁed using a different training approach. \nTo adjust GPT-3 a r einforcement learning approach with human feedback was \nused. The proximal policy optimization (PPO) [ 186] follows the policy gradient \npattern. It approximates the conditional distribution .π(at|st;w) of actions . at ∈A\nat step t conditional to the current observation .st ∈S about the state of the \nenvironment and a vector . w of parameters. In usual reinforcement learning, the \nenvironment generates a reward and the algorithm tries to maximize the weighted \nsum of rewards. The gradient for this optimization (policy gradient) can be easily \ncomputed from the model. PPO computes an update at each step that minimizes \nthe cost function while ensuring the deviation from the previous policy is relatively \nsmall [\n186]. \nThe algorithm needs a numeric score to measure the quality of each generated \nsequence. To reduce the data necessary for optimization, a human can express \npreferences [ 198] between trajectories .τ=(y,x) for pairs of instructions . x and \ngenerated text . y. Informally, the goal is to produce trajectories which are preferred \nby the human, while querying the human as little as possible. To achieve this \ngoal, a reward function .r(y,x)∈R is postulated [ 36] with the property that \n.(y[1],x [1]) is preferred to .(y[2],x [2]) if .r(y[1],x [1])>r (y [2],x [2]). The original \npolicy .π(at|st;w) induces a conditional distribution .π(y|x;w) . To construct this,\n144 3 Improving Pre-trained Language Models\nFig. 3.23 InstructGPT is trained in three steps [ 151, p. 3]. First GPT-3 is ﬁne-tuned on instructions \nand the corresponding completions. Then a reward model is generated by optimizing the selection \nof a completion for an instruction. Finally, a policy is trained to generate token by token of the \nanswer with maximal reward. Credits for image parts in Table A.1 \nthe reward function .r(y,x) is approximated by a deep neural network . ˆr(y,x;u)\nwith parameter . u. The network is trained by three alternating steps (Fig. 3.23): \n1. The policy .π(y|x;w) is used to generate set of trajectories .{τ1,...,τ i}.T h e \nparameter . w is updated by reinforcement learning in order to maximize the \nreward .ˆr(y,x;u). \n2. Pairs of trajectories .(σ[1],σ [2]) from the .{τ1,...,τ i} are selected and submitted \nto a human for comparison. \n3. The parameters . u of the reward function .ˆr(y,x;u) are optimized to correspond \nto the comparisons collected from the human up to now. \nFor a set of 33k instructions, a reward model .ˆr(y,x;u) was built with 6B \nparameters, where . x is the instruction and . y a completion [ 198]. It selects the best \ncompletion from a small set of proposed completions. Proximal policy optimization \n(PPO) was used as reinforcement model [ 151, p. 41]. To avoid catastrophic \nforgetting (Sect. 3.6.1), pre-training samples were mixed into ﬁne-tuning. \nThe reward model was then applied to create a ﬁnal model by another reinforce-\nment learning step. During this process, InstructGPT generates a completion for \nan instruction. The reward model calculates a reward and the policy is updated to \napproximate the preferences encoded in the reward model. By mimicking human \nutterances, the model implicitly learns human intentions and preferences. This \nprocess is called alignment to human preferences and is extensively discussed by \nAskell et al. [ 5].\n3.6 Fine-Tuning for Speciﬁc Applications 145\nInstructGPT Results \nThe GPT-3 model with 175B parameters ﬁned-tuned in a supervised way to the 13k \ninstruction-completion examples was taken as the base model called SFT. The ﬁnal \ncompletions were again scored by human raters [ 151]. The InstructGPT completions \nwere preferred to the standard GPT-3 output in 85% of cases and to few-shot-GPT-3 \nin 71% of cases. \nSpeciﬁcally, raters found that InstructGPT attempts to follow the correct instruc-\ntion in 92% of cases, compared to 85% for SFT and 75% for few-shot GPT-3 \n[151, p. 53]. In addition, InstructGPT follows explicit constraints in 50% of the \ncases, compared to 43% for SFT and 34% for SFT and 28% for few-shot GPT-\n3. Hallucinations were observed for 20% of the cases for InstructGPT compared \nto 16% for SFT and 50% for few-shot GPT-3. Finally, the raters found that the \nlanguage use is appropriate for a customer assistant in 92% of the cases for \nInstructGPT, about 90% for SFT and about 85% for GPT-3 few-shot. InstructGPT \nwas also evaluated on a few natural language benchmarks where it achieved very \nsimilar results to GPT-3 [\n151,p .5 6 ] . \nIt turned out that InstructGPT is able to generalize to unseen labeler preferences. \nThus, InstructGPT does not simply adapt to the preferences of a few training label-\ners. In addition, InstructGPT produces slightly less toxic language than standard \nGPT-3. However, InstructGPT still makes simple mistakes, e.g., given an instruction \nwith a false premise, the model sometimes incorrectly assumes the premise is true. \nNote that the results depend on the subjective preferences of the labelers. \nComparisons between alternatives are not necessarily the most effective \napproach to generate an improvement signal. For example, one could ask labelers to \nedit model responses to make them better, or generate critiques of model responses \nin natural language. There is also a vast space of options for designing interfaces \nfor labelers to provide feedback to language models; this is an interesting human-\ncomputer interaction problem. The authors note that the cost of aligning GPT-3 to \nhuman preferences described above is just 1.6% of the cost spent to train GPT-3. \nTherefore, it seems to make sense to put more effort into alignment than into the \nmere enlargement of the models. \nThe results show that the InstructGPT techniques potentially make language \nmodels more helpful, truthful, and harmless. In a way InstructGPT works like an \nintelligent assistant for speech generation and information provision. However, the \nmodel is currently not ﬁt for use in safety-critical applications, because failures \ncannot be ruled out. What is still missing is a comprehensive evaluation similar to \nGopher or PaLM (Sect.\n3.1.2) that shows the real utility of this approach. It can be \nexpected that the combination of this approach with retrieval techniques as used \nfor WebGPT (Sect. 6.2.3) and Retro (Sect. 6.2.3) will increase the performance, \nreliability, and correctness of InstructGPT.\n146 3 Improving Pre-trained Language Models\nFig. 3.24 FLAN instruction tuning ﬁne-tunes a pre-trained language models on a set of tasks with \ninstructions of ten different templates (left). The trained model can be applied to unseen tasks by \nformulating prompts according to these templates (right). Image adapted from [\n227, p. 1] with kind \npermission of the authors \nInstruction Tuning with FLAN \nFLAN [ 227] uses instruction tuning to improve the ability of the language model \nto respond to natural language prompts. The language model has to learn through \nsupervision to perform tasks described by prompts, and to follow instructions, \neven for unfamiliar tasks (Fig. 3.24). The authors group 62 publicly available NLP \ndatasets into twelve task clusters, e.g. “sentiment” “natural language inference”, \n“summarization”, etc. For each of the datasets they compose ten templates describ-\ning the task in natural language. Then an existing language model is ﬁne-tuned to \nprovide better answers to the prompts. \nThe approach was applied to a LaMDA-PT language model with 137B param-\neters using retrieval and ﬁlters (Sect. 6.6.3). For 18 NLI tasks the FLAN model \nwas compared to LaMDA-PT 137B, GPT-3 175B, and GLaM 64B. In 14 of 18 \ncases FLAN substantially improved the performance of its unmodiﬁed counterpart \nand achieved better results than the competitors, while in 4 cases it was surpassed \nby GLaM [\n227]. FLAN even outperforms few-shot GPT-3 by a large margin on a \nnumber of tasks. \n3.6.6 Generating Labeled Data by Foundation Models \nThe performance of GPT-3 and other Foundation Models in few-shot learning \nenables the generation of new high-quality training data for other models. By \nUnsupervised Data Generation (UDG) the creation of ﬁne-tuning data for models \nof downstream tasks is possible that would otherwise be produced by manual human \nannotation. This approach is similar to Sect.\n4.2.3.\n3.6 Fine-Tuning for Speciﬁc Applications 147\nFig. 3.25 New data can be generated by GPT-3 and other Foundation Models using the few-shot \nUDG strategy. Here the prompts for two examples, Amazon reviews and Copa common sense \nreasoning, and the generated answers are shown [\n225] \nThe idea for data generation is to utilize the language model to learn the input-\nlabel relation based on the task description and a few sample input-label pairs [ 225]. \nInstead of generating and predicting a label for a classiﬁcation task the language \nmodel has to create the input text using the output class and a task description as \ninput. For a classiﬁcation task like product reviews on Amazon, the approach is able \nto produce 10k new examples for each class, covering a much larger spectrum as \nthe currently available labeled data. It turns out that up to 32 few-shot examples still \nincrease the quality of the generated training data. Examples are shown in Fig.\n3.25. \nThe authors use an additional module to ﬁlter out noisy examples. In this approach, \na given training example is removed if the trained classiﬁer does not match its label \nwith high probability. \nThe T5-XXL encoder-decoder model ﬁne-tuned on SuperGLUE data enhanced \nwith UDG data is able to improve the overall accuracy on the SuperGLUE task for \nnatural language understanding to 90.4% and is even able to beat DeBERTa with \n90.3%. Moreover, the approach achieves very high performance scores on a list of \ntext classiﬁcation and sentiment analysis tasks [\n225]. \n3.6.7 Summary \nWhen pre-training Foundation Models on a big text collection and subsequent \nsupervised ﬁne-tuning on a small labeled dataset, PLMs achieved unprecedented \nperformance on many NLP tasks. Fine-tuning has been shown to change model \nparameters only slightly and, in general, no catastrophic forgetting occurs. Usually, \nno overﬁtting is observed if ﬁne-tuning is stopped after a few epochs. If necessary, \nthere are some approaches to avoid overﬁtting. \nFine-tuning can be performed in different ways. It has been suggested to use an \nintermediate ﬁne-tuning with a more related dataset before the ﬁnal ﬁne-tuning on\n148 3 Improving Pre-trained Language Models\nthe small dataset takes place. The results of such approaches have been mixed. Also, \nsimultaneous ﬁne-tuning to several tasks is possible. In some cases, it could improve \nperformance. As an alternative, there are strategies to accelerate ﬁne-tuning by \nmeta-learning. To avoid that the full model is changed adapter layers can be deﬁned, \nand only their parameters are adapted. This can drastically reduce the number of \ntrainable parameters and nevertheless lead to good performance on the ﬁne-tuning \ntasks. Finally, ﬁne-tuning APIs have been recently provided for proprietary models \nlike GPT-3. \nFoundation Models like GPT-3 and PaLM can be instructed by prompts to \nsolve speciﬁc tasks without training. A large number of different prompts has been \ncollected to order the model to complete a task. InstructGPT is a new version of \nGPT-3 that directly takes instructions and provides the answers for a large spectrum \nof tasks. The model was customized to carry out the instructions by adapting to user \njudgments through reinforcement learning. Instruction tuning is a variant, where a \nFoundation Model is ﬁne-tuned to provide improved answers to instructions for a \nnumber of tasks. It turns out that afterwards the model generates better answers even \nfor unseen tasks. \nFinally, big language models may be employed to generate high-quality training \ndata for ﬁne-tuning. Again, the few-shot learning technique is used to generate input \ntexts for speciﬁc learning tasks. In this way, the scarce training data can be expanded \nand better ﬁne-tuning results can be achieved. \nReferences \n1. O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou. “Knowledge Graph Based Synthetic Corpus \nGeneration for Knowledge-Enhanced Language Model Pre-training”. Mar. 13, 2021. arXiv: \n2010.12688. \n2. A. Aghajanyan, A. Shrivastava, A. Gupta, N. Goyal, L. Zettlemoyer, and S. Gupta. “Better \nFine-Tuning by Reducing Representational Collapse”. Aug. 6, 2020. arXiv: 2008.03156. \n3. J. Ainslie, S. Ontanon, C. Alberti, P . Pham, A. Ravula, and S. Sanghai. “ETC: Encoding Long \nand Structured Data in Transformers”. 2020. arXiv: 2004.08483. \n4. A. Alvi. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World \nfs Largest and Most Powerful Generative Language Model. Microsoft Research. Oct. 11, \n2021. \nURL : https://www.microsoft.com/en-us/research/blog/using-deepspeed-andmegatron-\nto-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-\nlanguage-model/ (visited on 11/12/2021). \n5. A. Askell et al. “A General Language Assistant as a Laboratory for Alignment”. Dec. 9, 2021. \narXiv: 2112.00861 [cs]. \n6. T. Bansal, R. Jha, T. Munkhdalai, and A. McCallum. “Self-Supervised Meta-Learning for \nFew-Shot Natural Language Classiﬁcation Tasks”. 2020. arXiv: 2009.08445. \n7. Y . Bansal, G. Kaplun, and B. Barak. “For Self-Supervised Learning, Rationality Implies \nGeneralization, Provably”. 2020. arXiv: 2010.08508. \n8. H. Bao et al. “Unilmv2: Pseudo-masked Language Models for Uniﬁed Language Model Pre-\nTraining”. In: Int. Conf. Mach. Learn. PMLR, 2020, pp. 642–652. \n9. A. Bapna et al. Building Machine Translation Systems for the Next Thousand Languages. \nMay 16, 2022. arXiv: 2205.03983 [cs]. \n10. I. Beltagy, M. E. Peters, and A. Cohan. “Longformer: The Long-Document Transformer”. \n2020. arXiv: 2004.05150.\nReferences 149 \n11. benchmark. GLUE Benchmark. Aug. 5, 2021. URL : https://gluebenchmark.com/ (visited on \n08/05/2021). \n12. Y . Bengio, A. Courville, and P . Vincent. “Representation Learning: A Review and New \nPerspectives”. In: IEEE Trans. Pattern Anal. Mach. Intell. 35.8 (2013), pp. 1798–1828. \n13. J. Berant, A. Chou, R. Frostig, and P . Liang. “Semantic Parsing on Freebase from Question-\nAnswer Pairs”. In: Proc. 2013 Conf. Empir . Methods Nat. Lang. Process. EMNLP 2013. \nSeattle, Washington, USA: Association for Computational Linguistics, Oct. 2013, pp. 1533– \n1544. \nURL : https://aclanthology.org/D13-1160 (visited on 12/14/2021). \n14. M. Bevilacqua and R. Navigli. “Breaking through the 80% Glass Ceiling: Raising the State \nof the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information”. \nIn: Proc Assoc. Comput. Linguist. 2020, pp. 2854–2864. \n15. C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, and S. Hellmann. \n“DBpedia-A Crystallization Point for the Web of Data”. In: J. Web Semant. 7.3 (2009), \npp. 154–165. \n16. S. Black, G. Leo, P . Wang, C. Leahy, and S. Biderman. GPT-Neo: Large Scale Autoregressive \nLanguage Modeling with Mesh-Tensorﬂow. Zenodo, Mar. 21, 2021. \nhttps://doi.org/10.5281/ \nzenodo.5297715. \n17. O. Bojar et al. “Findings of the 2014 Workshop on Statistical Machine Translation”. In: Proc. \nNinth Workshop Stat. Mach. Transl. 2014, pp. 12–58. \n18. K. Bollacker, C. Evans, P . Paritosh, T. Sturge, and J. Taylor. “Freebase: A Collaboratively \nCreated Graph Database for Structuring Human Knowledge”. In: Proc. 2008 ACM SIGMOD \nInt. Conf. Manag. Data. 2008, pp. 1247–1250. \n19. R. Bommasani et al. “On the Opportunities and Risks of Foundation Models”. 2021. arXiv: \n2108.07258. \n20. A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. “Translating \nEmbeddings for Modeling Multi-Relational Data”. In: Adv. Neural Inf. Process. Syst.2 6 \n(2013), pp. 2787–2795. \n21. S. Borgeaud et al. “Improving Language Models by Retrieving from Trillions of Tokens”. \nDec. 8, 2021. arXiv: 2112.04426 [cs]. \n22. A. Borzunov et al. Petals: Collaborative Inference and Fine-tuning of Large Models.S e p t . 2 , \n2022. https://doi.org/10.48550/2209.01188.a r X i v : 2209.01188 [cs]. \n23. G. Branwen. “GPT-3 Creative Fiction”. In: (June 19, 2020). URL : https://www.gwern.net/ \nGPT-3 (visited on 11/14/2021). \n24. S. Brin and L. Page. “The Anatomy of a Large-Scale Hypertextual Web Search Engine”. In: \nComput. Netw. ISDN Syst. 30.1-7 (1998), pp. 107–117. \n25. T. B. Brown et al. “Language Models Are Few-Shot Learners”. 2020. arXiv: 2005.14165. \n26. J. Casper. What Is This Fork of Megatron-LM and Megatron-DeepSpeed. BigScience Work-\nshop, Oct. 25, 2022. \nURL : https://github.com/bigscience-workshop/Megatron-DeepSpeed \n(visited on 10/25/2022). \n27. D. Chen. Openqa-Tutorial Danqi/Acl2020. July 5, 2020. URL : https://github.com/danqi/ \nacl2020-openqa-tutorial (visited on 02/24/2021). \n28. Q. Chen, C. Shui, and M. Marchand. “Generalization Bounds For Meta-Learning: An \nInformation-Theoretic Analysis”. In: Adv. Neural Inf. Process. Syst. 34 (2021). \n29. T. Chen, J. Frankle, S. Chang, S. Liu, Y . Zhang, Z. Wang, and M. Carbin. “The Lottery Ticket \nHypothesis for Pre-Trained Bert Networks”. 2020. arXiv: 2007.12223. \n30. W. Chen, Y . Su, X. Yan, and W. Y . Wang. “KGPT: Knowledge-Grounded Pre-Training for \nData-to-Text Generation”. 2020. arXiv: 2010.02307. \n31. Z. Chi, L. Dong, S. Ma, S. H. X.-L. Mao, H. Huang, and F. Wei. “mT6: Multilingual \nPretrained Text-to-Text Transformer with Translation Pairs”. 2021. arXiv: 2104.08692. \n32. Z. Chi, L. Dong, F. Wei, W. Wang, X.-L. Mao, and H. Huang. “Cross-Lingual Natural \nLanguage Generation via Pre-Training.” In: AAAI. 2020, pp. 7570–7577. \n33. R. Child, S. Gray, A. Radford, and I. Sutskever. “Generating Long Sequences with Sparse \nTransformers”. 2019. arXiv: 1904.10509. \n34. K. Choromanski et al. “Rethinking Attention with Performers”. 2020. arXiv: 2009.14794. \n150 3 Improving Pre-trained Language Models \n35. A. Chowdhery et al. “PaLM: Scaling Language Modeling with Pathways”. Apr. 5, 2022. \narXiv: 2204.02311 [cs]. \n36. P . F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. “Deep Reinforcement \nLearning from Human Preferences”. In: Adv. Neural Inf. Process. Syst. 30 (2017). \n37. H. W. Chung, T. Févry, H. Tsai, M. Johnson, and S. Ruder. “Rethinking Embedding Coupling \nin Pre-Trained Language Models”. 2020. arXiv: 2010.12821. \n38. A. Clark et al. “Uniﬁed Scaling Laws for Routed Language Models”. Feb. 9, 2022. arXiv: \n2202.01169 [cs]. \n39. K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning. “Electra: Pre-training Text Encoders as \nDiscriminators Rather than Generators”. 2020. arXiv: 2003.10555. \n40. A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman, H. Schwenk, and V . \nStoyanov. “XNLI: Evaluating Cross-lingual Sentence Representations”. Sept. 13, 2018. \narXiv: 1809.05053. \n41. A. Conneau et al. “Unsupervised Cross-Lingual Representation Learning at Scale”. Apr. 8, \n2020. arXiv: 1911.02116. \n42. A. D’Amour. How Underspeciﬁcation Presents Challenges for Machine Learning. Google AI \nBlog. Oct. 18, 2021. \nURL : http://ai.googleblog.com/2021/10/how-underspeciﬁcationpresents. \nhtml (visited on 10/25/2021). \n43. Y . Dai, S. Wang, N. N. Xiong, and W. Guo. “A Survey on Knowledge Graph Embedding: \nApproaches, Applications and Benchmarks”. In: Electronics 9.5 (2020), p. 750. \n44. Z. Dai, Z. Yang, Y . Yang, W. W. Cohen, J. Carbonell, Q. V . Le, and R. Salakhutdinov. \n“Transformer-XL: Language Modeling with Longer-Term Dependency, 2019”. In: URL \nHttpsopenreview Netforum. 2019. \n45. T. Dash, S. Chitlangia, A. Ahuja, and A. Srinivasan. “Incorporating Domain Knowledge into \nDeep Neural Networks”. 2021. arXiv: 2103.00180. \n46. L. de Alwis, A. Dissanayake, M. Pallewatte, K. Silva, and U. Thayasivam. “Survey on \nSemantic Table Interpretation”. In: (July 13, 2018). \nURL : http://semantic-web-journal.org/ \nsystem/ﬁles/swj1946.pdf. \n47. X. Deng, H. Sun, A. Lees, Y . Wu, and C. Y u. “Turl: Table Understanding through Represen-\ntation Learning”. Dec. 3, 2020. arXiv: 2006.14806. \n48. J. Devlin. mBERT - Multilingual BERT. GitHub. 2019. URL : https://github.com/ \ngoogleresearch/bert/blob/master/multilingual.md (visited on 02/21/2021). \n49. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. “Bert: Pre-training of Deep Bidirectional \nTransformers for Language Understanding”. 2018. arXiv: 1810.04805. \n50. T. Dozat and C. D. Manning. “Deep Biafﬁne Attention for Neural Dependency Parsing”. \n2016. arXiv: 1611.01734. \n51. N. Du et al. “GLaM: Efﬁcient Scaling of Language Models with Mixture-of-Experts”. Dec. \n13, 2021. arXiv: 2112.06905 [cs]. \n52. S. S. Du, W. Hu, S. M. Kakade, J. D. Lee, and Q. Lei. “Few-Shot Learning via Learning the \nRepresentation, Provably”. 2020. arXiv: 2002.09434. \n53. Z. Du. GLM. THUDM, Dec. 14, 2021. URL : https://github.com/THUDM/GLM (visited on \n12/17/2021). \n54. Z. Du, Y . Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. “All NLP Tasks Are Generation \nTasks: A General Pretraining Framework”. Mar. 18, 2021. arXiv: 2103.10360 [cs]. \n55. Z. Du, Y . Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. GLM: General Language Model \nPretraining with Autoregressive Blank Inﬁlling. Nov. 1, 2021. \nURL : https://aclanthology.org/ \n2022.acl-long.26/ (visited on 12/17/2021). \n56. W. Fedus, B. Zoph, and N. Shazeer. “Switch Transformers: Scaling to Trillion Parameter \nModels with Simple and Efﬁcient Sparsity”. 2021. arXiv: 2101.03961. \n57. F. Feng, Y . Yang, D. Cer, N. Arivazhagan, and W. Wang. “Language-Agnostic BERT Sentence \nEmbedding”. July 3, 2020. arXiv: 2007.01852 [cs]. \n58. C. Finn, P . Abbeel, and S. Levine. “Model-Agnostic Meta-Learning for Fast Adaptation of \nDeep Networks”. In: Int. Conf. Mach. Learn. PMLR, 2017, pp. 1126–1135. \nReferences 151 \n59. Q. Fournier, G. M. Caron, and D. Aloise. “A Practical Survey on Faster and Lighter \nTransformers”. Mar. 26, 2021. arXiv: 2103.14636 [cs]. \n60. P . Ganesh et al. “Compressing Large-Scale Transformer-Based Models: A Case Study on \nBert”. 2020. arXiv: 2002.11985. \n61. L. Gao et al. “The Pile: An 800GB Dataset of Diverse Text for Language Modeling”. 2020. \narXiv: 2101.00027. \n62. T. Gao, A. Fisch, and D. Chen. “Making Pre-Trained Language Models Better Few-Shot \nLearners”. 2020. arXiv: 2012.15723. \n63. H. Gong, Y . Sun, X. Feng, B. Qin, W. Bi, X. Liu, and T. Liu. “Tablegpt: Few-shot Tableto-\nText Generation with Table Structure Reconstruction and Content Matching”. In: Proc. 28th \nInt. Conf. Comput. Linguist. 2020, pp. 1978–1988. \n64. M. A. Gordon, K. Duh, and N. Andrews. “Compressing BERT: Studying the Effects of Weight \nPruning on Transfer Learning”. 2020. arXiv: 2002.08307. \n65. J. Gou, B. Y u, S. Maybank, and D. Tao. “Knowledge Distillation: A Survey”. Jan. 26, 2021. \narXiv: 2006.05525. \n66. N. Goyal, J. Du, M. Ott, G. Anantharaman, and A. Conneau. “Larger-Scale Transformers for \nMultilingual Masked Language Modeling”. 2021. arXiv: 2105.00572. \n67. A. Grover and J. Leskovec. “Node2vec: Scalable Feature Learning for Networks”. In: Proc. \n22nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Min. 2016, pp. 855–864. \n68. A. Gu, K. Goel, and C. Ré. “Efﬁciently Modeling Long Sequences with Structured State \nSpaces”. 2021. arXiv: 2111.00396. \n69. A. Gu, K. Goel, and C. Ré. The Annotated S4. 2021. URL : https://srush.github.io/annotateds4/ \n(visited on 04/05/2022). \n70. A. Gupta. “Diagonal State Spaces Are as Effective as Structured State Spaces”. 2022. arXiv: \n2203.14343. \n71. S. Gururangan, A. Marasovi ´c, S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, and N. A. \nSmith. “Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks”. 2020. \narXiv: 2004.10964. \n72. K. Guu, K. Lee, Z. Tung, P . Pasupat, and M.-W. Chang. “Realm: Retrieval-augmented \nLanguage Model Pre-Training”. 2020. arXiv: 2002.08909. \n73. C. Hawthorne et al. “General-Purpose, Long-Context Autoregressive Modeling with Per-\nceiver AR”. 2022. arXiv: 2202.07765. \n74. J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang. “FastMoE: A Fast Mixture-of-Expert \nTraining System”. Mar. 24, 2021. arXiv: 2103.13262 [cs]. \n75. P . He, J. Gao, and W. Chen. “Debertav3: Improving Deberta Using Electra-Style Pre-Training \nwith Gradient-Disentangled Embedding Sharing”. 2021. arXiv: 2111.09543. \n76. P . He, X. Liu, J. Gao, and W. Chen. “DeBERTa: Decoding-enhanced BERT with Disentangled \nAttention”. Jan. 11, 2021. arXiv: 2006.03654. \n77. W. D. Heaven. This Know-It-All AI Learns by Reading the Entire Web Nonstop. MIT Technol-\nogy Review. Sept. 4, 2020. URL : https://www.technologyreview.com/2020/09/04/1008156/ \nknowledge-graph-ai-reads-web-machine-learning-natural-language-processing/ (visited on \n12/01/2021). \n78. K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P . \nBlunsom. “Teaching Machines to Read and Comprehend”. 2015. arXiv: 1506.03340. \n79. A. Hern. “TechScape: AI’s Dark Arts Come into Their Own”. In: The Guardian. Technology \n(Sept. 21, 2022). \nISSN : 0261-3077. URL : https://www.theguardian.com/technology/2022/sep/ \n21/ais-dark-arts-come-into-their-own (visited on 10/01/2022). \n80. D. Hernandez, J. Kaplan, T. Henighan, and S. McCandlish. “Scaling Laws for Transfer”. Feb. \n1, 2021. arXiv: 2102.01293 [cs]. \n81. J. Herzig, P . K. Nowak, T. Müller, F. Piccinno, and J. M. Eisenschlos. “Tapas: Weakly \nSupervised Table Parsing via Pre-Training”. 2020. arXiv: 2004.02349. \n82. G. Hinton, O. Vinyals, and J. Dean. “Distilling the Knowledge in a Neural Network”. 2015. \narXiv: 1503.02531. \n152 3 Improving Pre-trained Language Models \n83. J. Hoffmann et al. “Training Compute-Optimal Large Language Models”. 2022. arXiv: \n2203.15556. \n84. N. Houlsby et al. “Parameter-Efﬁcient Transfer Learning for NLP”. In: Int. Conf. Mach. \nLearn. PMLR, 2019, pp. 2790–2799. \n85. E. J. Hu, Y . Shen, P . Wallis, Z. Allen-Zhu, Y . Li, S. Wang, and W. Chen. “LoRA: Low- Rank \nAdaptation of Large Language Models”. 2021. arXiv: 2106.09685. \n86. J. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Firat, and M. Johnson. “Xtreme: A Massively \nMultilingual Multi-Task Benchmark for Evaluating Cross-Lingual Generalisation”. In: Int. \nConf. Mach. Learn. PMLR, 2020, pp. 4411–4421. \n87. Z. Hu, Y . Dong, K. Wang, K.-W. Chang, and Y . Sun. “Gpt-Gnn: Generative Pre-Training of \nGraph Neural Networks”. In: Proc. 26th ACM SIGKDD Int. Conf. Knowl. Discov. Data Min. \n2020, pp. 1857–1867. \n88. H. Huang, Y . Liang, N. Duan, M. Gong, L. Shou, D. Jiang, and M. Zhou. “Unicoder: A \nUniversal Language Encoder by Pre-Training with Multiple Cross-Lingual Tasks”. 2019. \narXiv: 1909.00964. \n89. A. Iyer. GPT-3’s Free Alternative GPT-Neo Is Something to Be Excited About. V enture- Beat. \nMay 15, 2021. URL : https://venturebeat.com/2021/05/15/gpt-3s-free-alternative-gptneo-is-\nsomething-to-be-excited-about/ (visited on 01/03/2022). \n90. M. Iyyer, W.-t. Yih, and M.-W. Chang. “Search-Based Neural Structured Learning for \nSequential Question Answering”. In: Proc. 55th Annu. Meet. Assoc. Comput. Linguist. V ol. 1 \nLong Pap. 2017, pp. 1821–1831. \n91. G. Izacard and E. Grave. “Leveraging Passage Retrieval with Generative Models for \nOpen Domain Question Answering”. In: Proc. 16th Conf. Eur . Chapter Assoc. Comput. \nLinguist. Main V ol. EACL 2021. Online: Association for Computational Linguistics, Apr. \n1, 2021, pp. 874–880. \nURL : https://www.aclweb.org/anthology/2021.eacl-main.74 (visited on \n06/16/2021). \n92. A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira. “Perceiver: General \nPerception with Iterative Attention”. June 22, 2021. arXiv: 2103.03206 [cs, eess]. \n93. A. Jaegle et al. “Perceiver IO: A General Architecture for Structured Inputs & Outputs”. Aug. \n2, 2021. arXiv: 2107.14795. \n94. S. Ji, S. Pan, E. Cambria, P . Marttinen, and S. Y . Philip. “A Survey on Knowledge Graphs: \nRepresentation, Acquisition, and Applications”. In: IEEE Trans. Neural Netw. Learn. Syst. \n(2021). \n95. H. Jiang, P . He, W. Chen, X. Liu, J. Gao, and T. Zhao. “SMART: Robust and Efﬁcient \nFine-Tuning for Pre-trained Natural Language Models through Principled Regularized \nOptimization”. In: Proc. 58th Annu. Meet. Assoc. Comput. Linguist. ACL 2020. Online: \nAssociation for Computational Linguistics, July 2020, pp. 2177–2190. \nhttps://doi.org/10. \n18653/v1/2020.acl-main.197. \n96. Z. Jiang, F. F. Xu, J. Araki, and G. Neubig. “How Can We Know What Language Models \nKnow?” In: Trans. Assoc. Comput. Linguist. 8 (2020), pp. 423–438. \n97. X. Jiao et al. “Tinybert: Distilling Bert for Natural Language Understanding”. 2019. arXiv: \n1909.10351. \n98. M. Joshi, D. Chen, Y . Liu, D. S. Weld, L. Zettlemoyer, and O. Levy. “Spanbert: Improving \nPre-Training by Representing and Predicting Spans”. In: Trans. Assoc. Comput. Linguist.8 \n(2020), pp. 64–77. \n99. M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. “Triviaqa: A Large Scale Distantly \nSupervised Challenge Dataset for Reading Comprehension”. 2017. arXiv: 1705.03551. \n100. D. Jurafsky and J. H. Martin. Speech and Language ProcessingAn Introduction to Natural \nLanguage Processing,Computational Linguistics, and Speech Recognition. 3rd Draft. Jan. 12, \n2022. \n101. R. E. Kalman. “A New Approach to Linear Filtering and Prediction Problems”. In: (1960). \n102. J. Kaplan et al. “Scaling Laws for Neural Language Models”. 2020. arXiv: 2001.08361. \n103. V . Karpukhin, B. O ˘guz, S. Min, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. “Dense Passage \nRetrieval for Open-Domain Question Answering”. 2020. arXiv: 2004.04906. \nReferences 153 \n104. K. Karthikeyan, Z. Wang, S. Mayhew, and D. Roth. “Cross-Lingual Ability of Multilingual \nBERT: An Empirical Study”. Feb. 15, 2020. arXiv: 1912.07840. \n105. A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. “Transformers Are Rnns: Fast \nAutoregressive Transformers with Linear Attention”. In: Int. Conf. Mach. Learn.P M L R , \n2020, pp. 5156–5165. \n106. P . Kharya and A. Alvi. Using DeepSpeed and Megatron to Train Megatron-Turing NLG \n530B, the World’s Largest and Most Powerful Generative Language Model. NVIDIA \nDeveloper Blog. Oct. 11, 2021. \nURL : https://developer.nvidia.com/blog/using-deepspeed-\nandmegatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-\ngenerativelanguage-model/ (visited on 01/08/2022). \n107. T. N. Kipf and M. Welling. “Semi-Supervised Classiﬁcation with Graph Convolutional \nNetworks”. 2016. arXiv: 1609.02907. \n108. N. Kitaev, L. Kaiser, and A. Levskaya. “Reformer: The Efﬁcient Transformer”. 2020. arXiv: \n2001.04451. \n109. T. Kwiatkowski et al. “Natural Questions: A Benchmark for Question Answering Research”. \nIn: Trans. Assoc. Comput. Linguist. 7 (2019), pp. 453–466. \n110. G. Lai, Q. Xie, H. Liu, Y . Yang, and E. Hovy. “Race: Large-scale Reading Comprehension \nDataset from Examinations”. 2017. arXiv: 1704.04683. \n111. G. Lample and A. Conneau. “Cross-Lingual Language Model Pretraining”. 2019. arXiv: \n1901.07291. \n112. G. Lample, A. Sablayrolles, M. Ranzato, L. Denoyer, and H. Jégou. “Large Memory Layers \nwith Product Keys”. 2019. arXiv: 1907.05242. \n113. Z. Lan, M. Chen, S. Goodman, K. Gimpel, P . Sharma, and R. Soricut. “Albert: A Lite BERT \nfor Self-Supervised Learning of Language Representations”. 2020. arXiv: 1909.11942. \n114. J. Lee, M. Sung, J. Kang, and D. Chen. “Learning Dense Representations of Phrases at Scale”. \nJan. 2, 2021. arXiv: 2012.12624. \n115. O. Lehmberg, D. Ritze, R. Meusel, and C. Bizer. “A Large Public Corpus of Web Tables \nContaining Time and Context Metadata”. In: Proc. 25th Int. Conf. Companion World Wide \nWeb. 2016, pp. 75–76. \n116. D. Lepikhin et al. “Gshard: Scaling Giant Models with Conditional Computation and \nAutomatic Sharding”. 2020. arXiv: 2006.16668. \n117. B. Lester, R. Al-Rfou, and N. Constant. “The Power of Scale for Parameter-Efﬁcient Prompt \nTuning”. 2021. arXiv: 2104.08691. \n118. M. Lewis, M. Ghazvininejad, G. Ghosh, A. Aghajanyan, S. Wang, and L. Zettlemoyer. “Pre-\nTraining via Paraphrasing”. 2020. arXiv: 2006.15020. \n119. M. Lewis et al. “Bart: Denoising Sequence-to-Sequence Pre-Training for Natural Language \nGeneration, Translation, and Comprehension”. 2020. arXiv: 1910.13461. \n120. P . Li et al. “An Effective Self-Supervised Framework for Learning Expressive Molecular \nGlobal Representations to Drug Discovery”. In: Brief Bioinform 22.6 (Nov. 5, 2021), \nbbab109. \nISSN : 1477-4054. https://doi.org/10.1093/bib/bbab109.p m i d : 33940598. \n121. X. L. Li and P . Liang. “Preﬁx-Tuning: Optimizing Continuous Prompts for Generation”. 2021. \narXiv: 2101.00190. \n122. O. Lieber, O. Sharir, B. Lentz, and Y . Shoham. “Jurassic-1: Technical Details and Evalua-\ntion”. In: (2021), p. 9. \nURL : https://uploads-ssl.webﬂow.com/60fd4503684b466578c0d307/ \n61138924626a6981ee09caf6_jurassic_tech_paper.pdf. \n123. R. Lim, M. Wu, and L. Miller. Customizing GPT-3 for Your Application. OpenAI. Dec. 14, \n2021. URL : https://openai.com/blog/customized-gpt-3/ (visited on 02/16/2022). \n124. X. V . Lin, R. Socher, and C. Xiong. “Bridging Textual and Tabular Data for Cross-Domain \nText-to-Sql Semantic Parsing”. 2020. arXiv: 2012.12627. \n125. P . Liu, W. Y uan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. “Pre-Train, Prompt, and Predict: \nA Systematic Survey of Prompting Methods in Natural Language Processing”. 2021. arXiv: \n2107.13586. \n126. Y . Liu et al. “Multilingual Denoising Pre-Training for Neural Machine Translation”. 2020. \narXiv: 2001.08210. \n154 3 Improving Pre-trained Language Models \n127. Y . Liu et al. “Roberta: A Robustly Optimized Bert Pretraining Approach”. 2019. arXiv: \n1907.11692. \n128. Y . Liu, S. Pan, M. Jin, C. Zhou, F. Xia, and P . S. Y u. “Graph Self-Supervised Learning: A \nSurvey”. 2021. arXiv: 2103.00111. \n129. F. Locatello, S. Bauer, M. Lucic, G. Raetsch, S. Gelly, B. Schölkopf, and O. Bachem. \n“Challenging Common Assumptions in the Unsupervised Learning of Disentangled Repre-\nsentations”. In: Int. Conf. Mach. Learn. PMLR, 2019, pp. 4114–4124. \n130. A. Maas, R. E. Daly, P . T. Pham, D. Huang, A. Y . Ng, and C. Potts. “Learning Word V ectors \nfor Sentiment Analysis”. In: Proc. 49th Annu. Meet. Assoc. Comput. Linguist. Hum. Lang. \nTechnol. 2011, pp. 142–150. \n131. D. Mahajan et al. “Identiﬁcation of Semantically Similar Sentences in Clinical Notes: Iterative \nIntermediate Training Using Multi-Task Learning”. In: JMIR Med. Inform. 8.11 (2020), \ne22508. \n132. S. McCandlish, J. Kaplan, D. Amodei, and O. D. Team. “An Empirical Model of Large-Batch \nTraining”. 2018. arXiv: 1812.06162. \n133. A. Merchant, E. Rahimtoroghi, E. Pavlick, and I. Tenney. “What Happens To BERT \nEmbeddings During Fine-tuning?” Apr. 29, 2020. arXiv: 2004.14448. \n134. S. Merity, C. Xiong, J. Bradbury, and R. Socher. “Pointer Sentinel Mixture Models”. 2016. \narXiv: 1609.07843. \n135. T. Mikolov, K. Chen, G. Corrado, and J. Dean. “Efﬁcient Estimation of Word Representations \nin V ector Space”. 2013. arXiv: 1301.3781. \n136. T. Mikolov and G. Zweig. “Context Dependent Recurrent Neural Network Language Model”. \nIn: 2012 IEEE Spok. Lang. Technol. Workshop SLT. IEEE, 2012, pp. 234–239. \n137. G. A. Miller. “WordNet: A Lexical Database for English”. In: Commun. ACM 38.11 (1995), \npp. 39–41. \n138. S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi. “Cross-Task Generalization via Natural \nLanguage Crowdsourcing Instructions”. Mar. 14, 2022. arXiv: 2104.08773 [cs]. \n139. M. Mitchell. BigScience Large Open-science Open-access Multilingual Language Model. \nJuly 6, 2022. URL : https://huggingface.co/bigscience/bloom (visited on 10/25/2022). \n140. M. Mosbach, M. Andriushchenko, and D. Klakow. “On the Stability of Fine-Tuning Bert: \nMisconceptions, Explanations, and Strong Baselines”. Mar. 25, 2021. arXiv: 2006.04884. \n141. A. Mulyar, O. Uzuner, and B. McInnes. “MT-clinical BERT: Scaling Clinical Information \nExtraction with Multitask Learning”. In: J. Am. Med. Inform. Assoc. 28.10 (2021), pp. 2108– \n2115. \n142. S. Narang et al. “Do Transformer Modiﬁcations Transfer Across Implementations and \nApplications?” Sept. 10, 2021. arXiv: 2102.11972 [cs]. \n143. S. Narayan, S. B. Cohen, and M. Lapata. “Don’t Give Me the Details, Just the Summary! \nTopic-Aware Convolutional Neural Networks for Extreme Summarization”. In: Proc. 2018 \nConf. Empir . Methods Nat. Lang. Process. EMNLP 2018. Brussels, Belgium: Association \nfor Computational Linguistics, Oct. 2018, pp. 1797–1807. \nhttps://doi.org/10.18653/v1/D18-\n1206. \n144. M. Nayyeri, S. V ahdati, C. Aykul, and J. Lehmann. “5* Knowledge Graph Embeddings with \nProjective Transformations”. 2020. arXiv: 2006.04986. \n145. M. Nickel, V . Tresp, and H.-P . Kriegel. “A Three-Way Model for Collective Learning on \nMulti-Relational Data”. In: Icml. 2011. \n146. Y . Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, and D. Kiela. “Adversarial Nli: A New \nBenchmark for Natural Language Understanding”. 2019. arXiv: 1910.14599. \n147. S. J. Nowlan and G. E. Hinton. “Evaluation of Adaptive Mixtures of Competing Experts.” In: \nNIPS. V ol. 3. 1990, pp. 774–780. \n148. A. van den Oord et al. “Wavenet: A Generative Model for Raw Audio”. 2016. arXiv: \n1609.03499. \n149. OpenAi. OpenAI API. 2021. URL : https://beta.openai.com (visited on 11/14/2021). \n150. OpenAi. Prompt Examples for GPT-3. Sept. 3, 2021. URL : https://beta.openai.com/examples \n(visited on 09/03/2021). \nReferences 155 \n151. L. Ouyang et al. “Training Language Models to Follow Instructions with Human Feedback”. \nJan. 31, 2022. arXiv: 2203.02155. \n152. G. Paass and J. Kindermann. “Bayesian Classiﬁcation Trees with Overlapping Leaves \nApplied to Credit-Scoring”. In: Res. Dev. Knowl. Discov. Data Min.E d . b y X . W u ,R .K o \ntagiri, and K. B. Korb. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer, \n1998, pp. 234–245. \nISBN : 978-3-540-69768-8. https://doi.org/10.1007/3-540-64383-4_20. \n153. V . Pan. “Fast Approximate Computations with Cauchy Matrices and Polynomials”. In: Math. \nComput. 86.308 (2017), pp. 2799–2826. \n154. D. Paperno et al. “The LAMBADA Dataset: Word Prediction Requiring a Broad Discourse \nContext”. June 20, 2016. arXiv: 1606.06031 [cs]. \n155. P . Pasupat and P . Liang. “Compositional Semantic Parsing on Semi-Structured Tables”. 2015. \narXiv: 1508.00305. \n156. M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. \n“Deep Contextualized Word Representations”. In: Proc. NAACL-HLT. 2018, pp. 2227–2237. \n157. M. E. Peters, M. Neumann, R. L. Logan IV , R. Schwartz, V . Joshi, S. Singh, and N. A. Smith. \n“Knowledge Enhanced Contextual Word Representations”. 2019. arXiv: 1909.04164. \n158. F. Petroni. LAMA: LAnguage Model Analysis. Meta Research, 2020. URL : https://github.com/ \nfacebookresearch/LAMA (visited on 03/08/2022). \n159. F . P e t r o n i ,T .R o c k t ä s c h e l ,P .L e w i s ,A .B a k h t i n ,Y .W u , A . H . M i l l e r ,a n d S . R i e d e l . \n“Language Models as Knowledge Bases?” 2019. arXiv: 1909.01066. \n160. J. Pfeiffer, I. Vuli `c, I. Gurevych, and S. Ruder. “Mad-x: An Adapter-Based Framework for \nMulti-Task Cross-Lingual Transfer”. 2020. arXiv: 2005.00052. \n161. J. Pfeiffer et al. “Adapterhub: A Framework for Adapting Transformers”. 2020. arXiv: \n2007.07779. \n162. N. Poerner, U. Waltinger, and H. Schütze. “Bert Is Not a Knowledge Base (yet): Factual \nKnowledge vs. Name-Based Reasoning in Unsupervised Qa”. 2019. arXiv: 1911.03681. \n163. C. Poth, J. Pfeiffer, A. Rücklé, and I. Gurevych. “What to Pre-Train on? Efﬁcient Intermediate \nTask Selection”. 2021. arXiv: 2104.08247. \n164. S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and Y . Zhang. “CoNLL-2012 Shared \nTask: Modeling Multilingual Unrestricted Coreference in OntoNotes”. In: J t .C o n f .E M N L P \nCoNLL-Shar . Task. 2012, pp. 1–40. \n165. Y . Pruksachatkun et al. “Intermediate-Task Transfer Learning with Pretrained Models for Nat-\nural Language Understanding: When and Why Does It Work?” 2020. arXiv: 2005.00628. \n166. X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang. “Pre-Trained Models for Natural \nLanguage Processing: A Survey”. In: Sci. China Technol. Sci. 63.10 (June 23, 2021), \npp. 1872–1897. \nISSN : 1674–7321, 1869–1900. https://doi.org/10.1007/s11431-020-1647-3. \narXiv: 2003.08271. \n167. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. “Language Models Are \nUnsupervised Multitask Learners”. In: OpenAI blog 1.8 (2019), p. 9. \n168. J. W. Rae et al. “Scaling Language Models: Methods, Analysis & Insights from Training \nGopher”. In: ArXiv Prepr . ArXiv211211446 (Dec. 8, 2021), p. 118. \n169. J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P . Lillicrap. “Compressive Transformers \nfor Long-Range Sequence Modelling”. 2019. arXiv: 1911.05507. \n170. C. Raffel et al. “Exploring the Limits of Transfer Learning with a Uniﬁed Text-to-Text \nTransformer”. In: J. Mach. Learn. Res. 21.140 (2020), pp. 1–67. \n171. c. raffel. C4 | TensorFlow Datasets. TensorFlow. 2019. URL : https://www.tensorﬂow.org/ \ndatasets/catalog/c4 (visited on 12/14/2021). \n172. A. Raganato, Y . Scherrer, and J. Tiedemann. “Fixed Encoder Self-Attention Patterns in \nTransformer-Based Machine Translation”. 2020. arXiv: 2002.10260. \n173. P . Rajpurkar, J. Zhang, K. Lopyrev, and P . Liang. “Squad: 100,000+ Questions for Machine \nComprehension of Text”. 2016. arXiv: 1606.05250. \n174. H. Ren, H. Dai, Z. Dai, M. Yang, J. Leskovec, D. Schuurmans, and B. Dai. “Combiner: Full \nAttention Transformer with Sparse Computation Cost”. In: Adv. Neural Inf. Process. Syst.3 4 \n(2021). \n156 3 Improving Pre-trained Language Models \n175. J. Rodriguez. Five Key Facts Wu Dao 2.0: The Largest Transformer Model Ever Built. \nDataSeries. Sept. 21, 2021. URL : https://medium.com/dataseries/ﬁve-key-facts-wu-dao-2-0-\nthe-largest-transformer-model-ever-built-19316159796b (visited on 12/12/2021). \n176. A. Rogers, O. Kovaleva, and A. Rumshisky. “A Primer in {Bertology}: What We Know about \nHow {BERT} Works”. In: Trans. Assoc. Comput. Linguist. 8 (2021), pp. 842–866. \n177. S. Roller, S. Sukhbaatar, A. Szlam, and J. Weston. “Hash Layers For Large Sparse Models”. \n2021. arXiv: 2106.04426. \n178. A. Romero. GPT-3 Scared You? Meet Wu Dao 2.0: A Monster of 1.75 Trillion Parameters. \nMedium. June 8, 2021. URL : https://towardsdatascience.com/gpt-3-scared-you-meet-wu-\ndao-2-0-a-monster-of-1-75-trillion-parameters-832cd83db484 (visited on 07/29/2021). \n179. C. Rosset. “Turing-Nlg: A 17-Billion-Parameter Language Model by Microsoft”. In: \nMicrosoft Blog — 13.02 2020 (2019). \n180. A. Roy, M. Saffar, A. V aswani, and D. Grangier. “Efﬁcient Content-Based Sparse Attention \nwith Routing Transformers”. 2020. arXiv: 2003.05997. \n181. A. Sabeti. GPT-3: An AI That’s Eerily Good at Writing Almost Anything. Arram \nSabeti. July 9, 2020. URL : https://arr.am/2020/07/09/gpt-3-an-ai-thats-eerily-good-at-\nwriting-almostanything/ (visited on 09/04/2021). \n182. K. Sakaguchi, R. Le Bras, C. Bhagavatula, and Y . Choi. “Winogrande: An Adversarial \nWinograd Schema Challenge at Scale”. In: Proc. AAAI Conf. Artif. Intell. V ol. 34. 05. 2020, \npp. 8732–8740. \n183. V . Sanh, L. Debut, J. Chaumond, and T. Wolf. “DistilBERT, a Distilled V ersion of BERT: \nSmaller, Faster, Cheaper and Lighter”. 2019. arXiv: 1910.01108. \n184. T. Schick and H. Schütze. “Exploiting Cloze Questions for Few-Shot Text Classiﬁcation and \nNatural Language Inference”. Jan. 25, 2021. arXiv: 2001.07676. \n185. T. Schick and H. Schütze. “It’s Not Just Size That Matters: Small Language Models Are Also \nFew-Shot Learners”. Apr. 12, 2021. arXiv: 2009.07118. \n186. J. Schulman, F. Wolski, P . Dhariwal, A. Radford, and O. Klimov. “Proximal Policy Optimiza-\ntion Algorithms”. 2017. arXiv: 1707.06347. \n187. S. Schuster, S. Gupta, R. Shah, and M. Lewis. “Cross-Lingual Transfer Learning for \nMultilingual Task Oriented Dialog”. 2018. arXiv: 1810.13327. \n188. J. Sevilla, L. Heim, A. Ho, T. Besiroglu, M. Hobbhahn, and P . Villalobos. Compute Trends \nAcross Three Eras of Machine Learning. Mar. 9, 2022. \nhttps://doi.org/10.48550/arXiv.2202. \n05924.a r X i v : 2202.05924 [cs]. \n189. N. Shazeer. “GLU V ariants Improve Transformer”. Feb. 12, 2020. arXiv: 2002.05202 \n[cs, stat]. \n190. S. Shen et al. “Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT.” In: \nAAAI. 2020, pp. 8815–8821. \n191. T. Shen, Y . Mao, P . He, G. Long, A. Trischler, and W. Chen. “Exploiting Structured Knowl-\nedge in Text via Graph-Guided Representation Learning”. 2020. arXiv: 2004.14224. \n192. T. Shin, Y . Razeghi, R. L. Logan IV , E. Wallace, and S. Singh. “Autoprompt: Eliciting \nKnowledge from Language Models with Automatically Generated Prompts”. 2020. arXiv: \n2010.15980. \n193. M. Shoeybi, M. Patwary, R. Puri, P . LeGresley, J. Casper, and B. Catanzaro. “Megatron-\nLm: Training Multi-Billion Parameter Language Models Using Model Parallelism”. In: arXiv \n(2019), arXiv—1909. \n194. K. Singla, D. Can, and S. Narayanan. “A Multi-Task Approach to Learning Multilingual \nRepresentations”. In: Proc. 56th Annu. Meet. Assoc. Comput. Linguist. V ol. 2 Short Pap. 2018, \npp. 214–220. \n195. D. R. So, W. Ma ´nke, H. Liu, Z. Dai, N. Shazeer, and Q. V . Le. “Primer: Searching for Efﬁcient \nTransformers for Language Modeling”. Jan. 24, 2022. arXiv: 2109.08668 [cs]. \n196. K. Song, X. Tan, T. Qin, J. Lu, and T.-Y . Liu. “Mass: Masked Sequence to Sequence Pre-\nTraining for Language Generation”. 2019. arXiv: 1905.02450. \n197. A. C. Stickland and I. Murray. “Bert and Pals: Projected Attention Layers for Efﬁcient \nAdaptation in Multi-Task Learning”. In: Int. Conf. Mach. Learn. PMLR, 2019, pp. 5986– \n5995. \nReferences 157 \n198. N. Stiennon et al. “Learning to Summarize with Human Feedback”. In: Adv. Neural Inf. \nProcess. Syst. 33 (2020), pp. 3008–3021. \n199. G. Stoica, E. A. Platanios, and B. Póczos. “Re-Tacred: Addressing Shortcomings of the Tacred \nDataset”. In: Proc. AAAI Conf. Artif. Intell. V ol. 35. 15. 2021, pp. 13843–13850. \n200. F. M. Suchanek, G. Kasneci, and G. Weikum. “Yago: A Core of Semantic Knowledge”. In: \nProc. 16th Int. Conf. World Wide Web. 2007, pp. 697–706. \n201. P . Sun. Announcing ScaNN: Efﬁcient V ector Similarity Search. Google AI Blog. July 28, 2020. \nURL : http://ai.googleblog.com/2020/07/announcing-scann-efﬁcient- vector.html (visited on \n02/18/2021). \n202. T. Sun, Y . Shao, X. Qiu, Q. Guo, Y . Hu, X. Huang, and Z. Zhang. “CoLAKE: Contextualized \nLanguage and Knowledge Embedding”. 2020. arXiv: 2010.00309. \n203. Y . Sun et al. “Ernie: Enhanced Representation through Knowledge Integration”. 2019. arXiv: \n1904.09223. \n204. Z. Sun, H. Y u, X. Song, R. Liu, Y . Yang, and D. Zhou. “MobileBERT: A Compact Task-\nAgnostic BERT for Resource-Limited Devices”. Apr. 14, 2020. arXiv: 2004.02984. \n205. N. Tang et al. “RPT: Relational Pre-trained Transformer Is Almost All Y ou Need towards \nDemocratizing Data Preparation”. 2020. arXiv: 2012.02469. \n206. Y . Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng. “Synthesizer: Rethinking \nSelf-Attention in Transformer Models”. May 24, 2021. arXiv: 2005.00743 [cs]. \n207. Y . Tay, M. Dehghani, D. Bahri, and D. Metzler. “Efﬁcient Transformers: A Survey”. 2020. \narXiv: 2009.06732. \n208. Y . Tay, Z. Zhao, D. Bahri, D. Metzler, and D.-C. Juan. “HyperGrid Transformers: Towards A \nSingle Model for Multiple Tasks”. In: Int. Conf. Learn. Represent. 2021. \n209. Y . Tay et al. “Long Range Arena: A Benchmark for Efﬁcient Transformers”. 2020. arXiv: \n2011.04006. \n210. N. Tripuraneni, M. Jordan, and C. Jin. “On the Theory of Transfer Learning: The Importance \nof Task Diversity”. In: Adv. Neural Inf. Process. Syst. 33 (2020), pp. 7852–7862. \n211. L. TriviaQA. CodaLab - Competition. Feb. 28, 2021. URL : https://competitions.codalab.org/ \ncompetitions/17208#results (visited on 02/28/2021). \n212. A. V aswani et al. “Attention Is All Y ou Need”. In: Adv. Neural Inf. Process. Syst. 2017, \npp. 5998–6008. \n213. P . V erga, H. Sun, L. B. Soares, and W. W. Cohen. “Facts as Experts: Adaptable and \nInterpretable Neural Memory over Symbolic Knowledge”. 2020. arXiv: 2007.00849. \n214. D. Vrande ˇci´c and M. Krötzsch. “Wikidata: A Free Collaborative Knowledgebase”. In: \nCommun. ACM 57.10 (2014), pp. 78–85. \n215. K. Wali. EleutherAI Launches GPT-NeoX-20B, the Biggest Public-Access Language \nModel. Analytics India Magazine. Feb. 14, 2022. \nURL : https://analyticsindiamag. \ncom/eleutherailaunches-gpt-neox-20b-the-biggest-public-access-language-model/ (visited \non 02/23/2022). \n216. J. Wallat, J. Singh, and A. Anand. “BERTnesia: Investigating the Capture and Forgetting of \nKnowledge in BERT”. 2020. arXiv: 2010.09313. \n217. A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. “GLUE: A Multi-\nTask Benchmark and Analysis Platform for Natural Language Understanding”. 2018. arXiv: \n1804.07461. \n218. A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. “Glue: A Multi-Task \nBenchmark and Analysis Platform for Natural Language Understanding”. Feb. 22, 2019. \narXiv: 1804.07461. \n219. A. Wang et al. “Superglue: A Stickier Benchmark for General-Purpose Language Understand-\ning Systems”. In: Adv. Neural Inf. Process. Syst. 2019, pp. 3266–3280. \n220. B. Wang. EleutherAI - Text Generation Testing UI. 2021. URL : https://6b.eleuther.ai/ (visited \non 11/14/2021). \n221. B. Wang. Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language \nModel with JAX. May 1, 2021. \nURL : https://github.com/kingoﬂolz/mesh-transformerjax \n(visited on 11/14/2021). \n158 3 Improving Pre-trained Language Models \n222. R. Wang et al. “K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters”. \nDec. 28, 2020. arXiv: 2002.01808. \n223. W. Wang et al. “Structbert: Incorporating Language Structures into Pre-Training for Deep \nLanguage Understanding”. 2019. arXiv: 1908.04577. \n224. X. Wang, T. Gao, Z. Zhu, Z. Liu, J. Li, and J. Tang. “KEPLER: A Uniﬁed Model for \nKnowledge Embedding and Pre-Trained Language Representation”. Nov. 23, 2020. arXiv: \n1911.06136. \n225. Z. Wang, A. W. Y u, O. Firat, and Y . Cao. “Towards Zero-Label Language Learning”. Sept. \n19, 2021. arXiv: 2109.09193 [cs]. \n226. J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. “Chain of Thought \nPrompting Elicits Reasoning in Large Language Models”. 2022. arXiv: 2201.11903. \n227. J. Wei et al. “Finetuned Language Models Are Zero-shot Learners”. In: ICLR 2022 (2022), \np. 46. \n228. X. Wei, Y . Hu, R. Weng, L. Xing, H. Y u, and W. Luo. “On Learning Universal Representations \nacross Languages”. 2020. arXiv: 2007.15960. \n229. A. Williams, N. Nangia, and S. R. Bowman. “A Broad-Coverage Challenge Corpus for \nSentence Understanding through Inference”. 2017. arXiv: 1704.05426. \n230. G. Wilson and D. J. Cook. “A Survey of Unsupervised Deep Domain Adaptation”. In: ACM \nTrans. Intell. Syst. Technol. TIST 11.5 (2020), pp. 1–46. \n231. G. I. Winata, A. Madotto, Z. Lin, R. Liu, J. Y osinski, and P . Fung. “Language Models Are \nFew-shot Multilingual Learners”. Sept. 15, 2021. arXiv: 2109.07684. \n232. S. Wu and M. Dredze. “Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of \nBERT”. In: Proc. 2019 Conf. Empir . Methods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. \nLang. Process. EMNLP-IJCNLP. EMNLP-IJCNLP 2019. Hong Kong, China: Association for \nComputational Linguistics, Nov. 2019, pp. 833–844. \nhttps://doi.org/10.18653/v1/D19-1077. \n233. J. Xia, Y . Zhu, Y . Du, and S. Z. Li. “A Survey of Pretraining on Graphs: Taxonomy, Methods, \nand Applications”. 2022. arXiv: 2202.07893. \n234. W. Xiong, J. Du, W. Y . Wang, and V . Stoyanov. “Pretrained Encyclopedia: Weakly Supervised \nKnowledge-Pretrained Language Model”. 2019. arXiv: 1912.09637. \n235. L. Xue. mT5-code: Multilingual T5. Google Research, Feb. 25, 2021. URL : https://github. \ncom/google-research/multilingual-t5 (visited on 02/26/2021). \n236. L. Xue et al. “mT5: A Massively Multilingual Pre-Trained Text-to-Text Transformer”. 2020. \narXiv: 2010.11934. \n237. I. Yamada, A. Asai, H. Shindo, H. Takeda, and Y . Matsumoto. “LUKE: Deep Contextualized \nEntity Representations with Entity-Aware Self-Attention”. 2020. arXiv: 2010.01057. \n238. J. Yang et al. “GraphFormers: GNN-nested Transformers for Representation Learning on \nTextual Graph”. In: Adv. Neural Inf. Process. Syst. 34 (2021). \n239. Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen. “Breaking the Softmax Bottleneck: A \nHigh-Rank RNN Language Model”. 2017. arXiv: 1711.03953. \n240. Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V . Le. “Xlnet: Generalized \nAutoregressive Pretraining for Language Understanding”. In: Adv. Neural Inf. Process. Syst. \n2019, pp. 5753–5763. \n241. P . Yin, G. Neubig, W.-t. Yih, and S. Riedel. “TaBERT: Pretraining for Joint Understanding of \nTextual and Tabular Data”. 2020. arXiv: 2005.08314. \n242. W. Yin. “Meta-Learning for Few-Shot Natural Language Processing: A Survey”. 2020. arXiv: \n2007.09604. \n243. W. Y u, M. Jiang, Z. Hu, Q. Wang, H. Ji, and N. Rajani. “Knowledge-Enriched Natural \nLanguage Generation”. In: (Nov. 10, 2021), p. 6. \n244. W. Y u, C. Zhu, Z. Li, Z. Hu, Q. Wang, H. Ji, and M. Jiang. “A Survey of Knowledge-Enhanced \nText Generation”. July 5, 2021. arXiv: 2010.04389. \n245. W. Y uan, G. Neubig, and P . Liu. “Bartscore: Evaluating Generated Text as Text Generation”. \nIn: Adv. Neural Inf. Process. Syst. 34 (2021). \nReferences 159 \n246. C. Y un, Y .-W. Chang, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. “O( n) \nConnections Are Expressive Enough: Universal Approximability of Sparse Transformers”. \n2020. arXiv: 2006.04862. \n247. M. Zaheer et al. “Big Bird: Transformers for Longer Sequences”. In: Adv . Neural Inf. Process. \nSyst. 33 (Jan. 8, 2021). \n248. W. Zeng et al. “PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models \nwith Auto-parallel Computation”. 2021. arXiv: 2104.12369. \n249. B. Zhang and R. Sennrich. “Root Mean Square Layer Normalization”. 2019. arXiv: \n1910.07467. \n250. J. Zhang, H. Zhang, C. Xia, and L. Sun. “Graph-Bert: Only Attention Is Needed for Learning \nGraph Representations”. Jan. 22, 2020. arXiv: 2001.05140 [cs, stat]. \n251. J. Zhang, Y . Zhao, M. Saleh, and P . Liu. “Pegasus: Pre-training with Extracted Gap-Sentences \nfor Abstractive Summarization”. In: Int. Conf. Mach. Learn. PMLR, 2020, pp. 11328–11339. \n252. L. Zhang. “Transfer Adaptation Learning: A Decade Survey”. 2019. arXiv: 1903.04687. \n253. S. Zhang et al. OPT : Open Pre-trained Transformer Language Models. May 5, 2022. arXiv: \n2205.01068 [cs]. \n254. Y . Zhang, V . Zhong, D. Chen, G. Angeli, and C. D. Manning. “Position-Aware Attention \nand Supervised Data Improve Slot Filling”. In: Proc. 2017 Conf. Empir . Methods Nat. Lang. \nProcess. 2017, pp. 35–45. \n255. Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu. “ERNIE: Enhanced Language \nRepresentation with Informative Entities”. June 4, 2019. arXiv: 1905.07129. \n256. Z. Zhang, F. Qi, Z. Liu, Q. Liu, and M. Sun. “Know What Y ou Don’t Need: Single-Shot \nMeta-Pruning for Attention Heads”. In: AI Open 2 (2021), pp. 36–42. \n257. A. Zhavoronkov. Wu Dao 2.0 - Bigger , Stronger , Faster AI From China. Forbes. July 19, 2021. \nURL : https://www.forbes.com/sites/alexzhavoronkov/2021/07/19/wu-dao-20biggerstronger-\nfaster-ai-from-china/ (visited on 07/29/2021). \n258. C. Zhu, W. Ping, C. Xiao, M. Shoeybi, T. Goldstein, A. Anandkumar, and B. Catanzaro. \n“Long-Short Transformer: Efﬁcient Transformers for Language and Vision”. In: Adv. Neural \nInf. Process. Syst. 34 (2021). \n259. F. Zhu, W. Lei, C. Wang, J. Zheng, S. Poria, and T.-S. Chua. “Retrieving and Reading: A \nComprehensive Survey on Open-Domain Question Answering”. 2021. arXiv: 2101.00774. \n260. F. Zhuang et al. “A Comprehensive Survey on Transfer Learning”. In: Pr oc. IEEE 109.1 \n(2020), pp. 43–76. \n261. B. Zoph et al. “Designing Effective Sparse Expert Models”. 2022. arXiv: 2202.08906. \nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 \nInternational License ( http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made. \nThe images or other third party material in this chapter are included in the chapter’s Creative \nCommons license, unless indicated otherwise in a credit line to the material. If material is not \nincluded in the chapter’s Creative Commons license and your intended use is not permitted by \nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7849156856536865
    },
    {
      "name": "Language model",
      "score": 0.6502623558044434
    },
    {
      "name": "Transformer",
      "score": 0.5926668047904968
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5760743618011475
    },
    {
      "name": "Artificial intelligence",
      "score": 0.55305415391922
    },
    {
      "name": "Syntax",
      "score": 0.5401986241340637
    },
    {
      "name": "Autoregressive model",
      "score": 0.4752761423587799
    },
    {
      "name": "Natural language processing",
      "score": 0.46965184807777405
    },
    {
      "name": "Process (computing)",
      "score": 0.46846315264701843
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4148513972759247
    },
    {
      "name": "Machine learning",
      "score": 0.35380837321281433
    },
    {
      "name": "Programming language",
      "score": 0.19657018780708313
    },
    {
      "name": "Engineering",
      "score": 0.07076412439346313
    },
    {
      "name": "Mathematics",
      "score": 0.06942299008369446
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    }
  ]
}