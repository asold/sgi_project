{
    "title": "On Pre-trained Language Models for Antibody",
    "url": "https://openalex.org/W4318679072",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2108591233",
            "name": "Danqing Wang",
            "affiliations": [
                "University of California, Santa Barbara"
            ]
        },
        {
            "id": "https://openalex.org/A1996700895",
            "name": "Fei Ye",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2106821041",
            "name": "Zhou Hao",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2108591233",
            "name": "Danqing Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1996700895",
            "name": "Fei Ye",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106821041",
            "name": "Zhou Hao",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3024602798",
        "https://openalex.org/W3040739508",
        "https://openalex.org/W3111287171",
        "https://openalex.org/W3205490018",
        "https://openalex.org/W3095930322",
        "https://openalex.org/W2594817103",
        "https://openalex.org/W2118602822",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2950118240",
        "https://openalex.org/W3213832129",
        "https://openalex.org/W4280625391",
        "https://openalex.org/W4249802798",
        "https://openalex.org/W2801965704",
        "https://openalex.org/W2766842037",
        "https://openalex.org/W3010387158",
        "https://openalex.org/W3156522942",
        "https://openalex.org/W1599013943",
        "https://openalex.org/W3179485843",
        "https://openalex.org/W2924247059",
        "https://openalex.org/W3160906242",
        "https://openalex.org/W2027231041",
        "https://openalex.org/W3206187363",
        "https://openalex.org/W4207069608",
        "https://openalex.org/W3208873406",
        "https://openalex.org/W2951433247",
        "https://openalex.org/W3049166744",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W3002599793",
        "https://openalex.org/W4214902164",
        "https://openalex.org/W2787473680",
        "https://openalex.org/W3217583433",
        "https://openalex.org/W1758612015",
        "https://openalex.org/W3044470192",
        "https://openalex.org/W2029525307",
        "https://openalex.org/W2889633239",
        "https://openalex.org/W4225086377",
        "https://openalex.org/W4200262916",
        "https://openalex.org/W3211728297"
    ],
    "abstract": "A bstract Antibodies are vital proteins offering robust protection for the human body from pathogens. The development of general protein and antibody-specific pre-trained language models both facilitate antibody prediction tasks. However, few studies comprehensively explore the representation capability of distinct pre-trained language models on different antibody problems. Here, to investigate the problem, we aim to answer the following key questions: (1) How do pre-trained language models perform in antibody tasks with different specificity? (2) How many benefits will the model gain if we introduce the specific biological mechanism to the pre-training process? (3) Do the learned antibody pre-trained representations make sense in real-world antibody problems, like drug discovery and immune process understanding? Previously, no benchmark available largely hindered the study to answer these questions. To facilitate the investigation, we provide an A n T ibody U nderstanding E valuation ( ATUE ) benchmark. We comprehensively evaluate the performance of protein pre-trained language models by empirical study along with conclusions and new insights. Our ATUE and code is released at https://github.com/dqwang122/EATLM .",
    "full_text": null
}