{
  "title": "On Pre-trained Language Models for Antibody",
  "url": "https://openalex.org/W4318679072",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2108591233",
      "name": "Danqing Wang",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A1996700895",
      "name": "Fei Ye",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2106821041",
      "name": "Zhou Hao",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2108591233",
      "name": "Danqing Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1996700895",
      "name": "Fei Ye",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106821041",
      "name": "Zhou Hao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3024602798",
    "https://openalex.org/W3040739508",
    "https://openalex.org/W3111287171",
    "https://openalex.org/W3205490018",
    "https://openalex.org/W3095930322",
    "https://openalex.org/W2594817103",
    "https://openalex.org/W2118602822",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2950118240",
    "https://openalex.org/W3213832129",
    "https://openalex.org/W4280625391",
    "https://openalex.org/W4249802798",
    "https://openalex.org/W2801965704",
    "https://openalex.org/W2766842037",
    "https://openalex.org/W3010387158",
    "https://openalex.org/W3156522942",
    "https://openalex.org/W1599013943",
    "https://openalex.org/W3179485843",
    "https://openalex.org/W2924247059",
    "https://openalex.org/W3160906242",
    "https://openalex.org/W2027231041",
    "https://openalex.org/W3206187363",
    "https://openalex.org/W4207069608",
    "https://openalex.org/W3208873406",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W3049166744",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W3002599793",
    "https://openalex.org/W4214902164",
    "https://openalex.org/W2787473680",
    "https://openalex.org/W3217583433",
    "https://openalex.org/W1758612015",
    "https://openalex.org/W3044470192",
    "https://openalex.org/W2029525307",
    "https://openalex.org/W2889633239",
    "https://openalex.org/W4225086377",
    "https://openalex.org/W4200262916",
    "https://openalex.org/W3211728297"
  ],
  "abstract": "A bstract Antibodies are vital proteins offering robust protection for the human body from pathogens. The development of general protein and antibody-specific pre-trained language models both facilitate antibody prediction tasks. However, few studies comprehensively explore the representation capability of distinct pre-trained language models on different antibody problems. Here, to investigate the problem, we aim to answer the following key questions: (1) How do pre-trained language models perform in antibody tasks with different specificity? (2) How many benefits will the model gain if we introduce the specific biological mechanism to the pre-training process? (3) Do the learned antibody pre-trained representations make sense in real-world antibody problems, like drug discovery and immune process understanding? Previously, no benchmark available largely hindered the study to answer these questions. To facilitate the investigation, we provide an A n T ibody U nderstanding E valuation ( ATUE ) benchmark. We comprehensively evaluate the performance of protein pre-trained language models by empirical study along with conclusions and new insights. Our ATUE and code is released at https://github.com/dqwang122/EATLM .",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7388172149658203
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.680772066116333
    },
    {
      "name": "Language model",
      "score": 0.5119131803512573
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5070538520812988
    },
    {
      "name": "Process (computing)",
      "score": 0.4764428734779358
    },
    {
      "name": "Representation (politics)",
      "score": 0.4652335047721863
    },
    {
      "name": "Machine learning",
      "score": 0.43476852774620056
    },
    {
      "name": "Mechanism (biology)",
      "score": 0.4295095205307007
    },
    {
      "name": "Valuation (finance)",
      "score": 0.4166123569011688
    },
    {
      "name": "Natural language processing",
      "score": 0.3648560047149658
    },
    {
      "name": "Programming language",
      "score": 0.1418590545654297
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}