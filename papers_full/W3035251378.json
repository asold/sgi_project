{
  "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing",
  "url": "https://openalex.org/W3035251378",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2229152611",
      "name": "Hanrui Wang",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2981547106",
      "name": "Zhanghao Wu",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2102403465",
      "name": "Zhijian Liu",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2133106922",
      "name": "Han Cai",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2773936351",
      "name": "Ligeng Zhu",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2112762928",
      "name": "Chuang Gan",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2101446197",
      "name": "Song Han",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W2519314406",
    "https://openalex.org/W3016832937",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2967733054",
    "https://openalex.org/W2805493160",
    "https://openalex.org/W4297775537",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W4287812455",
    "https://openalex.org/W2970295111",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W3023198000",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W4289143671",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2964213727",
    "https://openalex.org/W2983981554",
    "https://openalex.org/W2964259004",
    "https://openalex.org/W3092618035",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2904529841",
    "https://openalex.org/W2885820039",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W3096533519",
    "https://openalex.org/W3006683367",
    "https://openalex.org/W2810075754",
    "https://openalex.org/W2970529093",
    "https://openalex.org/W2963034893",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3017024317",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W2954711779",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W3117975394",
    "https://openalex.org/W2994749257",
    "https://openalex.org/W2962915948",
    "https://openalex.org/W2891911973",
    "https://openalex.org/W2886851211",
    "https://openalex.org/W3035332806",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963374479",
    "https://openalex.org/W2964240726",
    "https://openalex.org/W2963918968",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W2932077855",
    "https://openalex.org/W3106104873",
    "https://openalex.org/W2949941638",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2948197522",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W2946794439"
  ],
  "abstract": "Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT’14 translation task on Raspberry Pi-4, HAT can achieve 3× speedup, 3.7× smaller size over baseline Transformer; 2.7× speedup, 3.6× smaller size over Evolved Transformer with 12,041× less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7675–7688\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n7675\nHAT: Hardware-Aware Transformers for\nEfﬁcient Natural Language Processing\nHanrui Wang1, Zhanghao Wu1, Zhijian Liu1, Han Cai1, Ligeng Zhu1,\nChuang Gan2, Song Han1\n1Massachusetts Institute of Technology, 2MIT-IBM Watson AI Lab\n{hanrui,zhwu,zhijian,hancai,ligeng,chuangg,songhan}@mit.edu\nAbstract\nTransformers are ubiquitous in Natural Lan-\nguage Processing (NLP) tasks, but they are dif-\nﬁcult to be deployed on hardware due to the in-\ntensive computation. To enable low-latency in-\nference on resource-constrained hardware plat-\nforms, we propose to design Hardware-Aware\nTransformers (HAT) with neural architecture\nsearch. We ﬁrst construct a large design space\nwith arbitrary encoder-decoder attention and\nheterogeneous layers. Then we train a Super-\nTransformer that covers all candidates in the\ndesign space, and efﬁciently produces many\nSubTransformers with weight sharing. Fi-\nnally, we perform an evolutionary search with\na hardware latency constraint to ﬁnd a special-\nized SubTransformer dedicated to run fast on\nthe target hardware. Extensive experiments\non four machine translation tasks demonstrate\nthat HAT can discover efﬁcient models for\ndifferent hardware (CPU, GPU, IoT device).\nWhen running WMT’14 translation task on\nRaspberry Pi-4, HAT can achieve3× speedup,\n3.7× smaller size over baseline Transformer;\n2.7× speedup, 3.6× smaller size over Evolved\nTransformer with12,041×less search cost and\nno performance loss. HAT is open-sourced.\n1 Introduction\nTransformer (Vaswani et al., 2017) has been widely\nused in natural language processing tasks. By stack-\ning multiple identical encoder/decoder layers with\nattention modules, it provides a signiﬁcant perfor-\nmance improvement over previous convolutional\nor recurrent neural network models (Kim, 2014).\nNevertheless, it is challenging to deploy Trans-\nformers on mobile devices due to the high com-\nputation cost. For instance, in order to translate a\nsentence with only 30 words, a Transformer-Big\nmodel needs to execute 13G FLOPs and takes 20\nseconds on a Raspberry Pi. Such long latency will\nhurt the user experience on edge devices. Thus we\nSuperTransformer\nEvolutionary Search with \nHardware Constraints\nSpecialized Deployment is Efficient\nHardware \nLatency \nFeedback\nSubTransformers\n12/3/2019QRXQ_VeQVRU_1895988.VYJ\nÀOe:///UVeUV/KaQUXLZaQJ/DRZQORadV/QRXQ_VeQVRU_1895988.VYJ1/1\nCYea[ed b` CaYVlina Cani fYVm [he NV\\n PYVjec[ IoT\n12/3/2019 QRXQ_gSX_1132940.VYg\nÀOe:///UVeUV/KaQUXLZaQg/DRZQORadV/QRXQ_gSX_1132940.VYg 1/1\nCYea[ed b` MiZha Pe[YiZhche]fYVm [he NV\\n PYVjec[\nGPU\n12/3/2019 Electronic DeYice\nÀle:///Users/hanruiZang/DoZnloads/noun_CPU_2880768.sYg1/1\nCYea[ed b` M\\haTTad KhViY\\S ATfYVT [he NV\\U PYVjec[CPU\nDifferent Hardware\n12/6/2019QRXQ_GaVKbRaUG_50324.VYJ\nÀOH:///UVHUV/KaQUXLZaQJ/DRZQORaGV/QRXQ_GaVKbRaUG_50324.VYJ1/1\nCYea[ed b` Da^id SVbVSe^ZkifYVT [he NV\\U PYVjec[\nTinyML Deeper Wider\n（Weight-Sharing）\nFigure 1: Framework for searching Hardware-Aware\nTransformers. We ﬁrst train a SuperTransformer that\ncontains numerous sub-networks, then conduct an evo-\nlutionary search with hardware latency feedback to ﬁnd\none specialized SubTransformer for each hardware.\nneed hardware-efﬁcient Transformers (Figure 1).\nThere are two common pitfalls when evaluat-\ning the efﬁciency of a Transformer. (1) FLOPs\ndoes not reﬂect the measured latency . Although\nFLOPs is used as an metric for efﬁciency in prior\narts (Howard et al., 2017; Wu et al., 2020), it is not\na good latency proxy. As in Figure 2 (Right), mod-\nels with the same FLOPs can result in verydifferent\nmeasured latencies; (2) different hardware prefers\ndifferent Transformer architecture.As in Table 1,\nthe Transformer model optimized on one hardware\nis sub-optimal for another because latency is in-\nﬂuenced by different factors on different hardware\nplatforms. For example, the embedding size has\nsigniﬁcant impact on the Raspberry Pi latency but\nhardly inﬂuences the GPU latency (Figure 2).\nInspired by the success of Neural Architecture\nSearch (NAS) (Bender et al., 2018; Guo et al., 2019;\nPham et al., 2018; Cai et al., 2019a), we propose to\nsearch for Hardware-Aware Transformers (HAT)\n7676\nOn Titan xp MACs vs latency\nChange #Layers Change #Layers Change Hidden \nDim\nChange Hidden \nDim\nChange Embed \nDim\nChange Embed \nDim\n159.171850 5.24E+01 159.171850 5.24E+01 159.171850 5.24E+01\n192.514580 9.62E+01 194.561290 5.09E+01 194.564170 5.14E+01\n225.857310 1.37E+02 226.018570 5.13E+01 229.956490 5.42E+01\n259.200040 1.74E+02 257.475850 5.17E+01 265.348810 5.15E+01\n292.542770 2.16E+02 288.933130 5.04E+01 300.741130 5.17E+01\n325.885500 2.61E+02 320.390410 5.25E+01 336.133450 5.15E+01\nTable 2\nFrom Scratch #5 Acc Loss Prune+Distill #5 Acc Loss GAN Compression Untitled 1\nUntitled 1 10.6 33.55 10.603462656 34.61 7.944142848 35.75\nUntitled 2 7.5 34 7.51501312 32.89 6.9599232 34.84\nUntitled 3 6.17 33.09 6.168969216 29.76 5.664538624 34.34\nUntitled 4 4.955045888 31.62 4.955045888 30.27 4.67861504 33.14\nUntitled 5\nUntitled 6\nUntitled 7\nUntitled 8\nUntitled 9\nUntitled 10\nUntitled 11\nUntitled 12\nUntitled 13\nUntitled 14\nUntitled 15\nUntitled 16\nUntitled 17\nmAP (↑)\n28\n30\n32\n34\n36\n4 5 6 7 8 9 10 11\nFrom Scratch\nPrune+Distill\nGAN Compression\n0\n75\n150\n225\n300\n120 180 240 300 360\nOn raspberry pi MACs vs latency\nChange #Layers Change #Layers Change Hidden \nDim\nChange Hidden \nDim\nChange Embed \nDim\nChange Embed \nDim\n159.171850 1012.378561 159.171850 1012.378561 159.171850 1012.378561\n192.514580 1321.4497 194.561290 1147.812423 194.564170 1114.87281\n225.857310 1630.328098 226.018570 1281.604218 229.956490 1270.467544\n259.200040 1925.809944 257.475850 1451.090652 265.348810 1403.119045\n292.542770 2266.552657 288.933130 1585.657468 300.741130 1635.061976\n325.885500 2580.540898 320.390410 1730.037206 336.133450 1831.49392\n600\n1150\n1700\n2250\n2800\n120 180 240 300 360\nHidden Dim Scaling\n Embed Dim Scaling\n Layer Number Scaling\nOn 40-core Xeon CPU FLOPs vs latency\nChange #Layers Change #Layers Change Hidden \nDim\nChange Hidden \nDim\nChange Embed \nDim\nChange Embed \nDim\n159.171850 57.81710545 159.171850 57.81710545 159.171850 57.81710545\n192.514580 100.9847422 194.561290 73.918281 194.564170 62.03208764\n225.857310 150.8454541 226.018570 80.10061979 229.956490 63.28665018\n259.200040 205.2778184 257.475850 83.99975101 265.348810 69.9605157\n292.542770 268.6143021 288.933130 89.46600159 300.741130 70.42247256\n325.885500 295.1284428 320.390410 96.02826834 336.133450 77.27889021\n10\n90\n170\n250\n330\n120 180 240 300 360\nJatson Nano GPU FLOPs vs latency\n#Layers latency_layers FFN Hidden Dim latency_hidden Embedding Dim latency_embed\n159.171850 276.686898 159.171850 276.686898 159.171850 276.686898\n192.514580 502.7163233 194.561290 276.6293568 194.564170 279.5712204\n225.857310 729.463118 226.018570 278.9684391 229.956490 283.4879486\n259.200040 998.0243301 257.475850 287.5616707 265.348810 285.4414597\n292.542770 1279.55703 288.933130 299.8618034 300.741130 294.368388\n325.885500 1450.864827 320.390410 309.1898384 336.133450 297.9405331\nLatency on Nvidia GPU (ms)\n200\n550\n900\n1250\n1600\n120 180 240 300 360\nLayer Num\nFFN Hidden Dim\nEmbedding Dim\n#Mult-Add (M)\nFLOPs (M)\nNvidia GPURaspberry Pi ARM CPU  Intel CPU\nHidden & Embed \ndim have no \nimpact on Nvidia \nGPU latency\nHidden & Embed dim \nhave large impact on \nRaspberry Pi ARM CPU \nlatency\nHidden & Embed \ndim have small \nimpact on Intel \nCPU latency\nLatency (ms)\nFLOPs (M) FLOPs (M)\nSimilar FLOPs, \nDiﬀerent Latency\nFigure 2: Latency of different Transformer models on different hardware. We ﬁnd (1) FLOPs does not reﬂect\nthe real measured latency; (2) Latency inﬂuencing factors of different hardware are contrasting. Thus we need to\nconsider hardware latency feedback to design specialized models for different hardware.\nMeasured On → GPU ARM CPU\nSpecialized For ↓ BLEU Latency Latency\nHAT (GPU) 28.10 147 ms 6491 ms\nHAT (ARM CPU) 28.15 184 ms 6042 ms\nTable 1: BLEU score and measured inference latency\nof HAT on WMT’14 En-De task. The efﬁcient model\nfor GPU is not efﬁcient for ARM CPU and vice versa.\nby directly involving the latency feedback into the\ndesign loop. In this way, we do not need FLOPs\nas the latency proxy and can search specialized\nmodels for various hardware.\nWe ﬁrst construct a large search space with arbi-\ntrary encoder-decoder attention and heterogeneous\nTransformer layers. Traditional Transformer has an\ninformation bottleneck between the encoder and de-\ncoder. Arbitrary encoder-decoder attention breaks\nthe bottleneck, allowing all decoder layers to attend\nto multiple and different encoder layers instead of\nonly the last one. Thus low-level information from\nthe encoder can also be used by the decoder. Mo-\ntivated by Figure 2, we introduce heterogeneous\nTransformer layers to allow different layers to have\ndifferent architecture adapting various hardware.\nTo perform a low-cost search in such a large\ndesign space, we ﬁrst train a Transformer super-\nnet – SuperTransformer, which contains many Sub-\nTransformers sharing the weights. We train all\nSubTransformers simultaneously by optimizing the\nuniformly sampled SubTransformers from the Su-\nperTransformer. The performance of a SubTrans-\nformer with inherited weights from the SuperTrans-\nformer can provide a good relative performance\napproximation for different architectures trained\nfrom-scratch. Unlike conventional NAS, we only\nneed to pay the SuperTransformer training cost for\nonce and can evaluate all the models in the design\nspace with it. Finally, we conduct an evolutionary\nsearch to ﬁnd the best SubTransformer under the\nhardware latency constraint. Experiments show\nthat HAT can be naturally incorporated with model\ncompression techniques such as quantization and\nknowledge distillation.\nWe evaluate HAT with WMT’14 En-De,\nWMT’14 En-Fr, WMT’19 En-De, and IWSLT’14\nDe-En tasks on Raspberry Pi ARM CPU, Intel\nXeon CPU, and Nvidia TITAN Xp GPU. Com-\npared with previous work (Vaswani et al., 2017; So\net al., 2019; Gu et al., 2019; Wu et al., 2020), HAT\nachieves up to 3× speedup, 3.7× smaller size over\nTransformer-Big without loss of accuracy. With\n12,041× less search cost, HAT outperforms the\nEvolved Transformer with 2.7× speedup and 3.6×\nsmaller size. It also achieves up to 1.9× speedup\nover Levenshtein and Lite Transformers with no\nBLEU score loss. With 4-bit quantization, HAT\ncan further reach 25× model size reduction.\nHAT has three contributions: (1) Hardware-\nAware and Specialization. To our best knowl-\nedge, we are the ﬁrst to directly involve the hard-\nware feedback in the model design, to reduce NLP\nmodel latency for target hardware, instead of rely-\ning on proxy signals (FLOPs). For different hard-\nware platforms, specialized models for low-latency\ninference are explored. (2) Low-cost Neural Ar-\nchitecture Search with a Large Design Space.\nWe propose arbitrary encoder-decoder attention\nto break the information bottleneck; and heteroge-\nneous layer to let different layers alter its capac-\nity. A weight-shared SuperTransformer is trained\nto search for efﬁcient models at a low cost. (3)\nDesign Insights. Based on the search results, we\nreveal some design insights: Attending to multiple\nencoder layers is beneﬁcial for the decoder; GPU\nprefers shallow and wide models while ARM CPU\nprefers deep and thin ones.\n7677\nElastic Layer \nNum in Encoder\nElastic Head Num \n(Self Attention)\nElastic Hidden \nDim in FFN\nEncoder Layer 2\nEncoder Layer m\nElastic Embedding Dim\nElastic Head Num \n(Self Attention)\nElastic Hidden \nDim in FFN\nElastic Embedding Dim\nElastic Head Num \n(En-Decoder Attention)\nDecoder Layer n\nElastic Layer \nNum in Decoder\nArbitrary \nEncoder- \nDecoder \nAttention\nconcat\n❶ Train a SuperTransformer by uniformly sampling SubTransformers with weight sharing\n12/3/2019QRXQ_VeQVRU_1895988.VYJ\nÀOe:///UVeUV/KaQUXLZaQJ/DRZQORadV/QRXQ_VeQVRU_1895988.VYJ1/1\nCYea[ed b` CaYVlina CanifYVm [he NV\\n PYVjec[IoT\n12/3/2019 QRXQ_gSX_1132940.VYg\nÀOe:///UVeUV/KaQUXLZaQg/DRZQORadV/QRXQ_gSX_1132940.VYg 1/1\nCYea[ed b` MiZha Pe[YiZhche]fYVm [he NV\\n PYVjec[\nGPU\n12/3/2019Electronic DeYice\nÀle:///Users/hanruiZang/DoZnloads/noun_CPU_2880768.sYg1/1\nCYea[ed b` M\\haTTad KhViY\\S ATfYVT [he NV\\U PYVjec[CPU\nLatency \nPredictor\n❷ Collect Hardware Latency Datasets\n12/3/2019noun_database_2997685.svg\nﬁle:///Users/hanruiwang/Downloads/noun_database_2997685.svg1/1\nLayer Num\nEmbed Dim\nHeads Num\nLatency\n❸ Train a Latency Predictor        \n    for each Hardware\nEvolutionary Search \nEngine\n12/3/2019QRXQ_LQcUHaVH_2500412.VYJ\nÀOH:///UVHUV/KaQUXLZaQJ/DRZQORadV/QRXQ_LQcUHaVH_2500412.VYJ1/1\nCYea[ed b` ASice DeZigUfYVT [he NV\\U PYVjec[\n❹ Evolutionary Search\nLatencySubTransformer \nArchitecture \nVal Loss\nEncoder Layer 1\nDecoder Layer 1\ne.g.  \n4 heads\ne.g.  \n2 heads\nSubTransformer \nArchitecture \nSuperTransformer\nFigure 3: HAT Overview. A large design space is constructed with Arbitrary Encoder-Decoder Attention and\nHeterogeneous Layers. (1) Train a weight-shared SuperTransformer by iteratively optimizing randomly sampled\nSubTransformers. It can provide a performance proxy for SubTransformers. (2) Collect(SubTransformer architec-\nture, latency) data pairs on the target hardware. (3) Train a latency predictor for each hardware to provide fast and\naccurate latency feedback. (4) Perform an evolutionary search with hardware latency constraint to ﬁnd the model\nwith the lowest validation loss. (5) Finally, the searched model is trainedfrom scratch to get the ﬁnal performance.\nFirst \nMatrix A\n= =\nSecond \nMatrix B\na1 aN-1\nb1\nbN-1\nCN-1\nPartial \nMatrices Ci\nResultant \nMatrix C\nC1\nC0\nMultiply Stage Merge Stage Final Results\n{\nFirst \nMatrix A\n= =\nSecond \nMatrix B\na1 aN-1\nb1\nbN-1\nCN-1\nPartial \nMatrices Ci\nResultant \nMatrix C\nC1\nC0\nMultiply Stage Merge Stage Final Results\nSelf Attention with \nElastic #Heads\nEncoder Layer\nElastic \nHidden \nDim\nElastic \nEmbedding \nDim\nEncoder Layer\nSelf Attention with \nElastic #Heads\nDecoder Layer\nElastic \nHidden \nDim\nElastic \nEmbedding \nDim\nEncoder Attention with \nElastic #Heads\nElastic Encoder #Layers\nEncoder Layer Decoder Layer\nEncoder Layer Decoder Layer\nConcat\nConcat\nConcat\nElastic Encoder \nAttention\nOne decoder layer \ncan attend to \narbitrary multiple \nencoder layers\nConcat in the \nsequence length \ndimension\nElastic #Layers  \nin Encoder\nElastic #Heads \n(Self Attention)\nElastic Hidden \nDim in FFN\nEncoder Layer 2\nEncoder Layer m\nElastic Embedding Dim\nElastic #Heads \n(Self Attention)\nElastic Hidden \nDim in FFN\nElastic Embedding Dim\nElastic #Heads \n(En-Decoder Attention)\nDecoder Layer n\nElastic #Layers \nin Decoder\nArbitrary \nEncoder-\nDecoder \nAttention\nconcat\n❶ Training SuperTransformer by uniformly sampling and training all SubTransformers\n12/3/2019QRXQ_VeQVRU_1895988.VYJ\nÀOe:///UVeUV/KaQUXLZaQJ/DRZQORadV/QRXQ_VeQVRU_1895988.VYJ1/1\nCYea[ed b` CaYVlina CanifYVm [he NV\\n PYVjec[IoT\n12/3/2019 QRXQ_gSX_1132940.VYg\nÀOe:///UVeUV/KaQUXLZaQg/DRZQORadV/QRXQ_gSX_1132940.VYg 1/1\nCYea[ed b` MiZha Pe[YiZhche]fYVm [he NV\\n PYVjec[\nGPU\n12/3/2019Electronic DeYice\nÀle:///Users/hanruiZang/DoZnloads/noun_CPU_2880768.sYg1/1\nCYea[ed b` M\\haTTad KhViY\\S ATfYVT [he NV\\U PYVjec[CPU\nHardware \nLatency \nPredictor\n❷ Collecting Hardware Latency Dataset\n12/3/2019noun_database_2997685.svg\nﬁle:///Users/hanruiwang/Downloads/noun_database_2997685.svg1/1\n#Layers\nEmbed Dim\n#Heads\nLatency\n❸ Training Hardware         \n   Latency Predictor\nEvolutionary Search \nEngine\n12/3/2019QRXQ_LQcUHaVH_2500412.VYJ\nÀOH:///UVHUV/KaQUXLZaQJ/DRZQORadV/QRXQ_LQcUHaVH_2500412.VYJ1/1\nCYea[ed b` ASice DeZigUfYVT [he NV\\U PYVjec[\n❹ Evolutionary Search\nLatencySubTransformer \nArchitecture \nVal Loss\nEncoder Layer 1\nDecoder Layer 1\ne.g. \n4 heads\ne.g. \n2 heads\nSubTransformer \nArchitecture \nSuperTransformer\nEncoder Layer j\nSelf Attention\nDecoder Layer m\nEncoder Layer i\nKj Vj\nKi Vi\nconcat\nEn-Decoder Attention\nQVK\nFigure 4: Arbitrary Encoder-Decoder Attention. Each\nencoder-decoder attention in one decoder layer can at-\ntend to the outputs from multiple encoder layers, fully\nleveraging the features extracted by the encoder.\n2 Proposed Approaches\nAn overview of the HAT framework is shown\nin Figure 3. We ﬁrstly train a SuperTransformer\nwith a large design space. Then, for a given hard-\nware platform, we collect a dataset of (SubTrans-\nformer architecture, measured latency) pairs for\ndifferent models, and train a latency predictor. Fi-\nnally, we conduct an evolutionary search with a\nlatency constraint to ﬁnd an efﬁcient model special-\nized for the target hardware.\n2.1 Design Space\nWe construct a large design space by breaking two\nconventions in the Transformer design: (1) All\ndecoder layers only attend to the last encoder layer;\n(2) All the layers are identical.\nArbitrary Encoder-Decoder Attention. Differ-\nent encoder layers extract features on different ab-\nstraction levels. Conventionally, all the decoder lay-\ners only attend to the last encoder layer. It forms an\ninformation bottleneck that forces all the decoder\nlayers to learn solely from the high abstraction level\nand ignore the low-level information. To break the\nbottleneck, we propose Arbitrary Encoder-Decoder\nAttention to learn the most suitable connections\nbetween the encoder and the decoder. Each de-\ncoder layer can choose multiple encoder layers to\nattend. The key and value vectors from encoder\nlayers are concatenated in the sentence length di-\nmension (Figure 4) and fed to the encoder-decoder\ncross attention module. The mechanism is efﬁcient\nbecause it introduces no additional parameters. The\nlatency overhead is also negligible. For example,\nwith each decoder layer attending to two encoder\nlayers, the latency of Transformer-Base on Nvidia\nTITAN Xp GPU barely increases by 0.4%. It im-\nproves the model capacity by allowing attention to\ndifferent abstraction levels.\nHeterogeneous Transformer Layers. Previous\nTransformers repeat one architecture for all layers.\nIn HAT, instead, different layers areheterogeneous,\nwith different numbers of heads, hidden dim, and\nembedding dim. In attention layers, different heads\nare used to capture various dependencies. However,\nV oita et al. (2019) shows that many heads are re-\ndundant. We thereby make attention head number\nelastic so that each attention module can decide its\nnecessary number of heads.\nIn the FFN layer, the input features are cast to\na higher dimension (hidden dim), followed by an\n7678\nEncoder Layer j\nSelf Attention\nDecoder Layer m\nEncoder Layer i\nKj Vj\nKi Vi\nconcat\nEn-Decoder Attention\nQVK\nSelf Attention\nFC2\nFC1\nFixed Dimension for Q \nvectors(e.g. 512)Max Embedding \nDim (e.g. 640)\nSampled Embedding \nDim (e.g. 512)\nWeight for \nQ Vector\nFFN\nMax Hidden \nDim (e.g. 1024)\nMax Embedding \nDim (e.g. 640)\nWeight for \nFC1 in FFN\nSampled Hidden \nDim (e.g. 768)\nSampled Embedding \nDim (e.g. 512)\nEn-Decoder Attention\nSampled \nEmbedding Dim\nSampled \nHidden Dim Input\nInput\nOutput Output\nFigure 5: Weight Sharing of the SuperTransformer. All\nSubTransformers share the front portion of word em-\nbeddings, and weights in the fully-connected layers.\nactivation layer. Traditionally, the hidden dim is\nset as 2 × or 4× of the embedding dim, but this\nis sub-optimal since different layers need differ-\nent capacities depending on the feature extraction\ndifﬁculty. We hence make the hidden dim elastic.\nMoreover, we also support elastic embedding\ndim of encoder and decoder, but it is consistent\ninside encoder/decoder. The number of encoder &\ndecoder layers are also elastic to learn the proper\nlevel of feature encoding and decoding. Other de-\nsign choices such as the length of Q, K, Vvectors\nin attention modules can be naturally incorporated\nin our framework, which we leave for future work.\n2.2 SuperTransformer\nIt is critical to have a large design space in or-\nder to ﬁnd high-performance models. However,\ntraining all the models and comparing their BLEU\nscores is infeasible. We thus propose SuperTrans-\nformer, a supernet for performance approxima-\ntion, which can judge the performance of a model\nwithout fully training it. The SuperTransformer is\nthe largest model in the search space with weight\nsharing (Pham et al., 2018; Liu et al., 2019; Cai\net al., 2019a). Every model in the search space\n(a SubTransformer) is a part of the SuperTrans-\nformer. All SubTransformers share the weights of\ntheir common parts. For elastic embedding dim,\nall SubTransformers share the front portion of the\nlongest word embedding and corresponding FC\nlayer weights. As in Figure 5, for elastic FFN\nhidden dim, the front part of the FC weights is\nshared. For elastic head number in attention mod-\nules, the whole Q, K, Vvectors (the lengths are\nﬁxed in our design space) are shared by dividing\ninto head number parts. Elastic layer numbers let\nall SubTransformers share the ﬁrst several layers.\nTable 1\n1.5178 1.50593357682228\n1.1434 1.14718132615089\n1.0149 1.00300032496452\n1.2197 1.20205979943275\n1.6494 1.64196475148201\n0.9041 0.898979330062866\n0.9911 0.991105431318283\n0.9521 0.95180076956749\n1.6207 1.60490390658379\n1.4752 1.47023892402649\n1.0844 1.09002708792686\n1.6500 1.63886147737503\n0.8097 0.792078709602356\n0.8498 0.847105264663696\n1.5309 1.52864699363709\n1.4066 1.37659643292427\n1.4817 1.4772674202919\n1.4555 1.45519551038742\n1.3673 1.36139337420464\n1.3707 1.3588189125061\n0.9996 0.99359764456749\n1.2229 1.20859974622726\n0.9742 0.970739436149597\n1.0622 1.05947124361992\n1.8017 1.79538866281509\n1.1291 1.1233959197998\n0.7868 0.766475886106491\n1.0599 1.05622818470001\n1.5618 1.55641422271729\n1.7710 1.75518612861633\n0.8626 0.870744729042053\n0.9984 1.00627853274345\n1.5722 1.56294302940369\n1.5985 1.59023655653\n0.8117 0.793386089801788\n1.4222 1.40984793305397\n1.7724 1.76731238365173\n1.3663 1.33999783992767\n0.7344 0.723225969076157\n1.1668 1.1424438893795\n1.2460 1.240658390522\n1.7029 1.701433634758\n1.0270 1.01597212553024\n1.8053 1.80152567028999\n1.4714 1.46477878689766\n0.8561 0.868675410747528\n1.8001 1.78646185994148\n1.2830 1.28321353793144\n0.9980 0.985619872808456\n1.2962 1.29664304852486\n1.7727 1.76657628417015\n1.4766 1.4801920235157\n1.0826 1.06322033405304\n0.6731 0.695399868488312\n0.7604 0.766498827934265\n0.9908 0.973572784662247\n0.9896 0.978963994979858\n1.3425 1.34111167192459\n0.6709 0.665217226743698\n1.4095 1.39349360466003\n1.5280 1.49141528606415\n0.9428 0.936236673593521\n1.7842 1.77054509520531\n1.0744 1.06516550779343\n1.1835 1.18174540400505\n1.1948 1.18113697767258\n1.4038 1.39932782649994\n1.8561 1.84637687802315\n1.5419 1.53355121016502\n1.6674 1.68622553944588\n1.5090 1.50206628441811\n1.2355 1.2271223127842\n1.1829 1.17586224675179\n1.3721 1.36318392753601\n1.4547 1.45504038929939\n1.2156 1.20998092293739\n1.8282 1.82363837361336\n0.9673 0.970936596393585\n1.5072 1.49910877943039\n1.6487 1.64696587324142\n0.8580 0.863358724117279\n1.5656 1.5531984269619\n0.7029 0.706401991844177\n1.1816 1.16616471409798\n0.9026 0.906013929843903\n0.9180 0.912524074316025\n0.6798 0.677733498811722\n1.7238 1.70794804692268\n1.3737 1.37029504776001\n1.0194 1.00554203987122\n0.8551 0.832619142532349\n0.6936 0.709289699792862\n1.4564 1.46749358177185\n1.5534 1.5471689581871\n0.9967 0.981475734710693\n1.3975 1.39085614085197\n0.9863 0.990083646774292\n1.5168 1.50389415025711\n1.7451 1.73703204393387\n0.8441 0.838332480192184\n0.9966 0.99056761264801\n1.5203 1.52719410061836\n1.4752 1.47681596279144\n1.0136 1.00116587281227\n1.6784 1.68521544337273\n1.3733 1.37443827390671\n1.6469 1.64791175127029\n1.0025 0.999753212928772\n0.7686 0.754834520816803\n1.3132 1.30511344075203\n0.8506 0.848865991830826\n1.4571 1.45009902715683\n0.9782 0.976251971721649\n1.4543 1.44249878525734\n1.0830 1.07190092802048\n0.9868 0.984612947702408\n0.7015 0.71085416674614\n1.8315 1.81005554199219\n1.7707 1.76479898691177\n1.0146 1.00876839756966\n1.1453 1.14576952457428\nTable 1-1\nPredicted Latency\n7589 7529.6678841114 59.3321158886\n5717 5735.90663075445 -18.90663075445\n5074.5 5015.0016248226 59.4983751774\n6098.5 6010.29899716375 88.20100283625\n8247 8209.82375741005 37.17624258995\n4520.5 4494.89665031433 25.60334968567\n4955.5 4955.52715659142 -0.02715659142\n4760.5 4759.00384783745 1.49615216255\n8103.5 8024.51953291895 78.98046708105\n7376 7351.19462013245 24.80537986755\n5422 5450.1354396343 -28.1354396343\n8250.0000 8194.30738687515 55.69261312485\n4048.5 3960.39354801178 88.10645198822\n4249 4235.52632331848 13.47367668152\n7654.5 7643.23496818545 11.26503181455\n7033 6882.98216462135 150.01783537865\n7408.5 7386.3371014595 22.1628985405\n7277.5 7275.9775519371 1.5224480629\n6836.5 6806.9668710232 29.5331289768\n6853.5 6794.0945625305 59.4054374695\n4998 4967.98822283745 30.01177716255\n6114.5 6042.9987311363 71.5012688637\n4871 4853.69718074799 17.30281925201\n5311 5297.3562180996 13.6437819004\n9008.5 8976.94331407545 31.55668592455\n5645.5 5616.979598999 28.520401001\n3934 3832.37943053246 101.62056946754\n5299.5 5281.14092350005 18.35907649995\n7809 7782.07111358645 26.92888641355\n8855.0000 8775.93064308165 79.06935691835\n4313 4353.72364521027 -40.72364521027\n4992 5031.39266371725 -39.39266371725\n7861 7814.71514701845 46.28485298155\n7992.5 7951.18278265 41.31721735\n4058.5 3966.93044900894 91.56955099106\n7111 7049.23966526985 61.76033473015\n8862 8836.56191825865 25.43808174135\n6831.5 6699.98919963835 131.51080036165\n3672 3616.12984538079 55.87015461921\n5834 5712.2194468975 121.7805531025\n6230.0000 6203.29195261 26.70804739\n8514.5 8507.16817379 7.33182621\n5135.0000 5079.8606276512 55.1393723488\n9026.5 9007.62835144995 18.87164855005\n7357 7323.8939344883 33.1060655117\n4280.5 4343.37705373764 -62.87705373764\n9000.5 8932.3092997074 68.1907002926\n6415.0000 6416.0676896572 -1.0676896572\n4990.0000 4928.09936404228 61.90063595772\n6481 6483.2152426243 -2.2152426243\n8863.5 8832.88142085075 30.61857914925\n7383 7400.9601175785 -17.9601175785\n5413 5316.1016702652 96.8983297348\n3365.5 3476.99934244156 -111.49934244156\n3802 3832.49413967133 -30.49413967133\n4954 4867.86392331124 86.13607668876\n4948 4894.81997489929 53.18002510071\n6712.5 6705.55835962295 6.94164037705\n3354.5 3326.08613371849 28.41386628151\n7047.5 6967.46802330015 80.03197669985\n7640.0000 7457.07643032075 182.92356967925\n4714 4681.18336796761 32.81663203239\n8921 8852.72547602655 68.27452397345\n5372 5325.82753896715 46.17246103285\n5917.5 5908.72702002525 8.77297997475\n5974 5905.6848883629 68.3151116371\n7019 6996.6391324997 22.3608675003\n9280.5 9231.88439011575 48.61560988425\n7709.5 7667.7560508251 41.7439491749\n8337 8431.1276972294 -94.1276972294\n7545.0000 7510.33142209055 34.66857790945\n6177.5 6135.611563921 41.888436079\n5914.5 5879.31123375895 35.18876624105\n6860.5 6815.91963768005 44.58036231995\n7273.5 7275.20194649695 -1.70194649695\n6078 6049.90461468695 28.09538531305\n9141 9118.1918680668 22.8081319332\n4836.5 4854.68298196793 -18.18298196793\n7536 7495.54389715195 40.45610284805\n8243.5 8234.8293662071 8.6706337929\n4290.0000 4316.7936205864 -26.7936205864\n7828 7765.9921348095 62.0078651905\n3514.5 3532.00995922089 -17.50995922089\n5908 5830.8235704899 77.1764295101\n4513 4530.06964921952 -17.06964921952\n4590.0000 4562.62037158013 27.37962841987\n3399 3388.66749405861 10.33250594139\n8619 8539.7402346134 79.2597653866\n6868.5 6851.47523880005 17.02476119995\n5097 5027.7101993561 69.2898006439\n4275.5 4163.09571266175 112.40428733825\n3468 3546.44849896431 -78.44849896431\n7282 7337.46790885925 -55.46790885925\n7767 7735.8447909355 31.1552090645\n4983.5 4907.37867355347 76.12132644653\n6987.5 6954.28070425985 33.21929574015\n4931.5 4950.41823387146 -18.91823387146\n7584 7519.47075128555 64.52924871445\n8725.5 8685.16021966935 40.33978033065\n4220.5 4191.66240096092\n4983 4952.83806324005\n7601.5 7635.9705030918\n7376 7384.0798139572\n5068 5005.82936406135\n8392 8426.07721686365\n6866.5 6872.19136953355\n8234.5 8239.55875635145\n5012.5 4998.76606464386\n3843 3774.17260408402\n6566 6525.56720376015\n4253 4244.32995915413\n7285.5 7250.49513578415\n4891 4881.25985860825\n7271.5 7212.4939262867\n5415.0000 5359.5046401024\n4934 4923.06473851204\n3507.5 3554.2708337307\n9157.5 9050.27770996095\n8853.5 8823.99493455885\n5073 5043.8419878483\n5726.5 5728.8476228714\n2500\n4375\n6250\n8125\n10000\n2500 4375 6250 8125 10000\nReal Latency on  \nRaspberry Pi ARM CPU (s)\nPredicted Latency (s)\nTable 1-1-1\nPredicted Latency\n7589 7529.6678841114 59.3321158886 3520.29997581826\n5717 5735.90663075445 -18.90663075445 357.460686485115\n5074.5 5015.0016248226 59.4983751774 3540.05664875065\n6098.5 6010.29899716375 88.20100283625 7779.41690132018\n8247 8209.82375741005 37.17624258995 1382.07301310681\n4520.5 4494.89665031433 25.60334968567 655.531515126698\n4955.5 4955.52715659142 -0.02715659142 0.000737480457552818\n4760.5 4759.00384783745 1.49615216255 2.23847129350304\n8103.5 8024.51953291895 78.98046708105 6237.91418034082\n7376 7351.19462013245 24.80537986755 615.306870373455\n5422 5450.1354396343 -28.1354396343 791.602963415339\n8250.0000 8194.30738687515 55.69261312485 3101.66715667421\n4048.5 3960.39354801178 88.10645198822 7762.74688195252\n4249 4235.52632331848 13.47367668152 181.539963318136\n7654.5 7643.23496818545 11.26503181455 126.900941782824\n7033 6882.98216462135 150.01783537865 22505.3509316957\n7408.5 7386.3371014595 22.1628985405 491.194071716497\n7277.5 7275.9775519371 1.5224480629 2.31784810422796\n6836.5 6806.9668710232 29.5331289768 872.205707160304\n6853.5 6794.0945625305 59.4054374695 3529.00600094267\n4998 4967.98822283745 30.01177716255 900.706768454558\n6114.5 6042.9987311363 71.5012688637 5112.43144911912\n4871 4853.69718074799 17.30281925201 299.387554067728\n5311 5297.3562180996 13.6437819004 186.152784545683\n9008.5 8976.94331407545 31.55668592455 995.824426540692\n5645.5 5616.979598999 28.520401001 813.413273257842\n3934 3832.37943053246 101.62056946754 10326.7401389071\n5299.5 5281.14092350005 18.35907649995 337.055689931016\n7809 7782.07111358645 26.92888641355 725.164923473878\n8855.0000 8775.93064308165 79.06935691835 6251.96320348142\n4313 4353.72364521027 -40.72364521027 1658.41527921195\n4992 5031.39266371725 -39.39266371725 1551.78195474034\n7861 7814.71514701845 46.28485298155 2142.2876155237\n7992.5 7951.18278265 41.31721735 1707.11244954714\n4058.5 3966.93044900894 91.56955099106 8384.98266870434\n7111 7049.23966526985 61.76033473015 3814.33894598017\n8862 8836.56191825865 25.43808174135 647.096002679604\n6831.5 6699.98919963835 131.51080036165 17295.0906117618\n3672 3616.12984538079 55.87015461921 3121.47417717443\n5834 5712.2194468975 121.7805531025 14830.5031139508\n6230.0000 6203.29195261 26.70804739 713.319795386486\n8514.5 8507.16817379 7.33182621 53.755675573643\n5135.0000 5079.8606276512 55.1393723488 3040.35038301961\n9026.5 9007.62835144995 18.87164855005 356.139118996604\n7357 7323.8939344883 33.1060655117 1096.01157366497\n4280.5 4343.37705373764 -62.87705373764 3953.52388672607\n9000.5 8932.3092997074 68.1907002926 4649.9716063952\n6415.0000 6416.0676896572 -1.0676896572 1.13996120409185\n4990.0000 4928.09936404228 61.90063595772 3831.68873197018\n6481 6483.2152426243 -2.2152426243 4.90729988451555\n8863.5 8832.88142085075 30.61857914925 937.497389118887\n7383 7400.9601175785 -17.9601175785 322.565823433545\n5413 5316.1016702652 96.8983297348 9389.28630539403\n3365.5 3476.99934244156 -111.49934244156 12432.1033649003\n3802 3832.49413967133 -30.49413967133 929.892554294582\n4954 4867.86392331124 86.13607668876 7419.42370733194\n4948 4894.81997489929 53.18002510071 2828.11506971215\n6712.5 6705.55835962295 6.94164037705 48.1863711242909\n3354.5 3326.08613371849 28.41386628151 807.347797063531\n7047.5 6967.46802330015 80.03197669985 6405.11729448533\n7640.0000 7457.07643032075 182.92356967925 33461.0323441994\n4714 4681.18336796761 32.81663203239 1076.93133794929\n8921 8852.72547602655 68.27452397345 4661.4106238012\n5372 5325.82753896715 46.17246103285 2131.89615783005\n5917.5 5908.72702002525 8.77297997475 76.9651776373645\n5974 5905.6848883629 68.3151116371 4666.95447798944\n7019 6996.6391324997 22.3608675003 500.008395365973\n9280.5 9231.88439011575 48.61560988425 2363.47752441759\n7709.5 7667.7560508251 41.7439491749 1742.55729271663\n8337 8431.1276972294 -94.1276972294 8860.0233857096\n7545.0000 7510.33142209055 34.66857790945 1201.9102942636\n6177.5 6135.611563921 41.888436079 1754.64107714447\n5914.5 5879.31123375895 35.18876624105 1238.24926956726\n6860.5 6815.91963768005 44.58036231995 1987.40870457802\n7273.5 7275.20194649695 -1.70194649695 2.89662187848038\n6078 6049.90461468695 28.09538531305 789.350675888746\n9141 9118.1918680668 22.8081319332 520.210882282258\n4836.5 4854.68298196793 -18.18298196793 330.620833246068\n7536 7495.54389715195 40.45610284805 1636.696257652\n8243.5 8234.8293662071 8.6706337929 75.1798903705794\n4290.0000 4316.7936205864 -26.7936205864 717.898104127958\n7828 7765.9921348095 62.0078651905 3844.97534548322\n3514.5 3532.00995922089 -17.50995922089 306.598671917231\n5908 5830.8235704899 77.1764295101 5956.20127192743\n4513 4530.06964921952 -17.06964921952 291.37292447746\n4590.0000 4562.62037158013 27.37962841987 749.644052410153\n3399 3388.66749405861 10.33250594139 106.76067902886\n8619 8539.7402346134 79.2597653866 6282.11040913888\n6868.5 6851.47523880005 17.02476119995 289.842493915323\n5097 5027.7101993561 69.2898006439 4801.0764732714\n4275.5 4163.09571266175 112.40428733825 12634.7238120199\n3468 3546.44849896431 -78.44849896431 6154.16698975335\n7282 7337.46790885925 -55.46790885925 3076.68891321806\n7767 7735.8447909355 31.1552090645 970.647051852703\n4983.5 4907.37867355347 76.12132644653 5794.45633997919\n6987.5 6954.28070425985 33.21929574015 1103.52160947155\n4931.5 4950.41823387146 -18.91823387146 357.899572815256\n7584 7519.47075128555 64.52924871445 4164.02393965135\n8725.5 8685.16021966935 40.33978033065 1627.2978771251\n3354.39824205721\n57.9171670755503\nTable 1-2\n1.10686560471853 1.1073\n1.12427930037181 1.1762\n0.56603428390291 0.6595\n1.16446213589774 1.1680\n0.735942622025808 0.7626\n0.702480143970913 0.7382\n1.08080430163278 1.1356\n1.66595975557963 1.5293\n0.827956828806135 0.8847\n1.40140480465359 1.3022\n0.579667621188694 0.4994\n0.967412292957306 0.9582\n0.578798254330953 0.5268\n1.39106131262249 1.3250\n0.563722915119595 0.4970\n1.57160838445028 1.5957\n0.914486481083764 0.9549\n1.13511294126511 1.1786\n1.07352372672823 0.9386\n0.516253213087718 0.5848\n1.42109930515289 1.3234\n1.01445747746362 0.9683\n1.230595211188 1.2517\n0.87366908788681 0.9339\n1.09563897053401 1.1264\n1.25254955556658 1.2800\n1.33334855238597 1.3223\n0.552535918023851 0.4981\n1.53952754206128 1.3916\n1.00563446680705 0.9534\n1.05830252170563 1.1121\n1.52117126517826 1.5642\n1.31693896320131 1.3465\n0.500659777058495 0.4996\n0.550811953014798 0.6854\n1.22056394815445 1.1722\n1.24317036734687 1.3197\n1.54166804419623 1.5927\n1.51578810479906 1.4975\n1.50076703892814 1.3200\n0.927355686823527 0.9791\n0.898585187064277 0.9793\n1.21480349037382 1.1494\n1.11047390434477 1.1618\n0.484960277875264 0.4391\n0.589951376120249 0.5943\n1.18380337953568 1.1112\n1.23281872934765 1.2754\n0.606961581442091 0.5472\n0.605644583702087 0.5329\n0.817568765746223 0.8046\n1.21156553427378 1.1972\n0.528497927718692 0.5245\n1.22941212521659 1.1234\n1.45581271913317 1.4809\n1.36199576987161 1.3909\n0.562038885222541 0.5525\n0.553324222564697 0.5981\n1.1288234922621 1.0856\n1.28251595629586 1.3577\n1.33740875456068 1.3477\n0.731115837891897 0.7840\n1.5019749601682 1.6102\n1.23858428663678 1.2989\n1.41174303160773 1.3403\n0.518149071269565 0.4870\n0.868234203921424 0.9217\n0.70454176929262 0.7827\n0.752536389562819 0.8228\n0.528047548400031 0.6011\n0.755933158927494 0.8705\n0.868509676721361 0.8628\n0.597036116653019 0.5628\n1.06230600012673 1.1047\n1.37483388185501 1.4214\n1.17454287078646 1.1190\n0.946183635128869 0.9038\n0.915215445889367 0.9388\n1.01070993476444 0.9593\n1.19738876819611 1.1387\n0.913374159071181 0.9924\n1.07625570562151 0.9371\n1.99019349283642 1.6467\n1.20291454924477 1.1427\n1.32695668273502 1.3277\n0.799555712276035 0.8378\n0.837250583701664 0.7787\n1.29070891274346 1.2582\n0.726786388291253 0.7940\n1.31501422988044 1.1338\n0.5515875087844 0.5769\n0.630333854092492 0.7405\n1.15295324060652 1.1532\n1.15896142191357 1.0983\n1.31093002027935 1.3168\n1.49981399377187 1.5399\n1.06453044546975 1.1086\n0.538767079512278 0.5424\n1.54827154344983 1.5219\n0.847577916251289 0.8829\n1.27162744601568 1.2872\n1.84584408998489 1.5946\n1.41902102364434 1.3073\nTable 1-1-2\nPredicted Latency\n332.059681415559 332.19 -0.130318584441\n337.283790111543 352.86 -15.576209888457\n169.810285170873 197.85 -28.039714829127\n349.338640769322 350.4000 -1.061359230678\n220.782786607742 228.78 -7.997213392258\n210.744043191274 221.46 -10.715956808726\n324.241290489834 340.68 -16.438709510166\n248.387048641841 265.41 -17.022951358159\n420.421441396077 390.66 29.761441396077\n173.900286356608 149.82 24.080286356608\n290.223687887192 287.46 2.763687887192\n173.639476299286 158.04 15.599476299286\n417.318393786747 397.5000 19.818393786747\n169.116874535879 149.1000 20.016874535879\n471.482515335084 478.71 -7.227484664916\n274.345944325129 286.47 -12.124055674871\n340.533882379533 353.58 -13.046117620467\n322.057118018469 281.58 40.477118018469\n154.875963926315 175.44 -20.564036073685\n426.329791545867 397.02 29.309791545867\n304.337243239086 290.49 13.847243239086\n369.1785633564 375.51 -6.3314366436\n262.100726366043 280.17 -18.069273633957\n328.691691160203 337.92 -9.228308839797\n375.764866669974 384.0000 -8.235133330026\n400.004565715791 396.69 3.314565715791\n165.760775407155 149.43 16.330775407155\n461.858262618384 417.48 44.378262618384\n301.690340042115 286.02 15.670340042115\n317.490756511689 333.63 -16.139243488311\n456.351379553478 469.26 -12.908620446522\n395.081688960393 403.95 -8.868311039607\n150.197933117549 149.88 0.317933117549\n165.243585904439 205.62 -40.376414095561\n366.169184446335 351.66 14.509184446335\n372.951110204061 395.91 -22.958889795939\n462.500413258869 477.81 -15.309586741131\n454.736431439718 449.25 5.486431439718\n278.206706047058 293.73 -15.523293952942\n269.575556119283 293.79 -24.214443880717\n364.441047112146 344.82 19.621047112146\n333.142171303431 348.54 -15.397828696569\n145.488083362579 131.73 13.758083362579\n176.985412836075 178.29 -1.304587163925\n355.141013860704 333.36 21.781013860704\n369.845618804295 382.62 -12.774381195705\n182.088474432627 164.16 17.928474432627\n181.693375110626 159.87 21.823375110626\n245.270629723867 241.38 3.890629723867\n363.469660282134 359.16 4.309660282134\n158.549378315608 157.35 1.199378315608\n368.823637564977 337.02 31.803637564977\n436.743815739951 444.27 -7.526184260049\n408.598730961483 417.27 -8.671269038517\n168.611665566762 165.75 2.861665566762\n165.997266769409 179.43 -13.432733230591\n338.64704767863 325.68 12.96704767863\n384.754786888758 407.31 -22.555213111242\n401.222626368204 404.31 -3.087373631796\n219.334751367569 235.2000 -15.865248632431\n450.59248805046 483.06 -32.46751194954\n371.575285991034 389.67 -18.094714008966\n423.522909482319 402.09 21.432909482319\n155.44472138087 146.1000 9.34472138087\n260.470261176427 276.51 -16.039738823573\n211.362530787786 234.81 -23.447469212214\n225.760916868846 246.84 -21.079083131154\n158.414264520009 180.33 -21.915735479991\n226.779947678248 261.15 -34.370052321752\n260.552903016408 258.84 1.712903016408\n179.110834995906 168.84 10.270834995906\n318.691800038019 331.41 -12.718199961981\n412.450164556503 426.42 -13.969835443497\n352.362861235938 335.7000 16.662861235938\n283.855090538661 271.14 12.715090538661\n274.56463376681 281.64 -7.07536623319\n303.212980429332 287.79 15.422980429332\n359.216630458833 341.61 17.606630458833\n274.012247721354 297.72 -23.707752278646\n322.876711686453 281.13 41.746711686453\n360.874364773431 342.81 18.064364773431\n398.087004820506 398.31 -0.222995179494\n239.866713682811 251.34 -11.473286317189\n251.175175110499 233.61 17.565175110499\n387.212673823038 377.46 9.752673823038\n218.035916487376 238.2000 -20.164083512624\n165.47625263532 173.07 -7.59374736468\n189.100156227748 222.15 -33.049843772252\n345.885972181956 345.96 -0.074027818044\n347.688426574071 329.49 18.198426574071\n393.279006083805 395.04 -1.760993916195\n449.944198131561 461.97 -12.025801868439\n319.359133640925 332.58 -13.220866359075\n161.630123853683 162.72 -1.089876146317\n464.481463034949 456.57 7.911463034949\n254.273374875387 264.87\n381.488233804704 386.16\n425.706307093302 392.19\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n2500\n2502.5\n2505\n2507.5\n2510\n200 400 600 800 1000\nReal Latency (ms)\nPredicted Latency (ms)\nTable 1-1-1-1\nPredicted Latency\n332.059681415559 332.19 -0.130318584441 0.016982933450706\n337.283790111543 352.86 -15.576209888457 242.618314489266\n169.810285170873 197.85 -28.039714829127 786.225607698765\n349.338640769322 350.4000 -1.061359230678 1.1264834165454\n220.782786607742 228.78 -7.997213392258 63.9554220413107\n210.744043191274 221.46 -10.715956808726 114.831730326481\n324.241290489834 340.68 -16.438709510166 270.231170359622\n499.787926673889 458.79 40.997926673889 1680.82999155758\n248.387048641841 265.41 -17.022951358159 289.780872942247\n420.421441396077 390.66 29.761441396077 885.743393972126\n173.900286356608 149.82 24.080286356608 579.860191016241\n290.223687887192 287.46 2.763687887192 7.63797073781178\n173.639476299286 158.04 15.599476299286 243.343660811986\n417.318393786747 397.5000 19.818393786747 392.768732286572\n169.116874535879 149.1000 20.016874535879 400.675266185121\n471.482515335084 478.71 -7.227484664916 52.2365345815959\n274.345944325129 286.47 -12.124055674871 146.992726007372\n340.533882379533 353.58 -13.046117620467 170.20118496706\n322.057118018469 281.58 40.477118018469 1638.39708308107\n154.875963926315 175.44 -20.564036073685 422.879579639818\n426.329791545867 397.02 29.309791545867 859.063880462177\n304.337243239086 290.49 13.847243239086 191.746145322413\n369.1785633564 375.51 -6.3314366436 40.0870899719208\n262.100726366043 280.17 -18.069273633957 326.498649658814\n328.691691160203 337.92 -9.228308839797 85.1616840426755\n375.764866669974 384.0000 -8.235133330026 67.8174209633051\n400.004565715791 396.69 3.314565715791 10.9863458842971\n165.760775407155 149.43 16.330775407155 266.694225398939\n461.858262618384 417.48 44.378262618384 1969.43019302626\n301.690340042115 286.02 15.670340042115 245.559557035513\n317.490756511689 333.63 -16.139243488311 260.475180374989\n456.351379553478 469.26 -12.908620446522 166.632481832366\n395.081688960393 403.95 -8.868311039607 78.6469406952154\n150.197933117549 149.88 0.317933117549 0.101081467234426\n165.243585904439 205.62 -40.376414095561 1630.25481521622\n366.169184446335 351.66 14.509184446335 210.516433297769\n372.951110204061 395.91 -22.958889795939 527.110620662072\n462.500413258869 477.81 -15.309586741131 234.383446184214\n454.736431439718 449.25 5.486431439718 30.1009299427261\n450.230111678442 396.0000 54.230111678442 2940.90501265629\n278.206706047058 293.73 -15.523293952942 240.972655149446\n269.575556119283 293.79 -24.214443880717 586.339292452393\n364.441047112146 344.82 19.621047112146 384.985489777053\n333.142171303431 348.54 -15.397828696569 237.093128568884\n145.488083362579 131.73 13.758083362579 189.284857811673\n176.985412836075 178.29 -1.304587163925 1.70194766827787\n355.141013860704 333.36 21.781013860704 474.41256480018\n369.845618804295 382.62 -12.774381195705 163.184814933182\n182.088474432627 164.16 17.928474432627 321.43019548136\n181.693375110626 159.87 21.823375110626 476.25970121909\n245.270629723867 241.38 3.890629723867 15.1369996482374\n363.469660282134 359.16 4.309660282134 18.5731717474033\n158.549378315608 157.35 1.199378315608 1.43850834395068\n368.823637564977 337.02 31.803637564977 1011.47136236442\n436.743815739951 444.27 -7.526184260049 56.6434495162093\n408.598730961483 417.27 -8.671269038517 75.1909067383435\n168.611665566762 165.75 2.861665566762 8.18912981599128\n165.997266769409 179.43 -13.432733230591 180.438322044224\n338.64704767863 325.68 12.96704767863 168.144325499864\n384.754786888758 407.31 -22.555213111242 508.737638493543\n401.222626368204 404.31 -3.087373631796 9.53187594230922\n219.334751367569 235.2000 -15.865248632431 251.706114168854\n450.59248805046 483.06 -32.46751194954 1054.13933219352\n371.575285991034 389.67 -18.094714008966 327.41867506627\n423.522909482319 402.09 21.432909482319 459.36960887728\n155.44472138087 146.1000 9.34472138087 87.3238176860889\n260.470261176427 276.51 -16.039738823573 257.273221528435\n211.362530787786 234.81 -23.447469212214 549.783812457723\n225.760916868846 246.84 -21.079083131154 444.327745650101\n158.414264520009 180.33 -21.915735479991 480.299461628936\n226.779947678248 261.15 -34.370052321752 1181.30049659997\n260.552903016408 258.84 1.712903016408 2.93403674361963\n179.110834995906 168.84 10.270834995906 105.490051513127\n318.691800038019 331.41 -12.718199961981 161.752610272934\n412.450164556503 426.42 -13.969835443497 195.156302318385\n352.362861235938 335.7000 16.662861235938 277.650944568125\n283.855090538661 271.14 12.715090538661 161.673527406346\n274.56463376681 281.64 -7.07536623319 50.0608073337652\n303.212980429332 287.79 15.422980429332 237.868325323558\n359.216630458833 341.61 17.606630458833 309.993436113906\n274.012247721354 297.72 -23.707752278646 562.057518105645\n322.876711686453 281.13 41.746711686453 1742.78793663183\n597.058047850926 494.01 103.048047850926 10618.9001658867\n360.874364773431 342.81 18.064364773431 326.321274667575\n398.087004820506 398.31 -0.222995179494 0.0497268500775613\n239.866713682811 251.34 -11.473286317189 131.636298916196\n251.175175110499 233.61 17.565175110499 308.535376662494\n387.212673823038 377.46 9.752673823038 95.1146466985706\n218.035916487376 238.2000 -20.164083512624 406.590263904075\n394.504268964132 340.14 54.364268964132 2955.47374000449\n165.47625263532 173.07 -7.59374736468 57.6649990385844\n189.100156227748 222.15 -33.049843772252 1092.29217337026\n345.885972181956 345.96 -0.074027818044 0.00548011784435557\n347.688426574071 329.49 18.198426574071 331.182729771854\n393.279006083805 395.04 -1.760993916195 3.1010995728758\n449.944198131561 461.97 -12.025801868439 144.619910578951\n319.359133640925 332.58 -13.220866359075 174.791307284521\n161.630123853683 162.72 -1.089876146317 1.18783001431079\n464.481463034949 456.57 7.911463034949 62.5912473533644\n497.69809519237\n22.3091482399569\n50\n175\n300\n425\n550\n50 175 300 425 550\nPredicted Latency (ms)\nReal Latency (ms)\nPredict latency is very \nclose to real latency.\nTable 1-1-3\nPredicted Latency\n7.589 7.5296678841114 0.0593321158886\n5.717 5.73590663075445 -0.01890663075445\n5.0745 5.0150016248226 0.0594983751774\n6.0985 6.01029899716375 0.08820100283625\n8.247 8.20982375741005 0.03717624258995\n4.5205 4.49489665031433 0.02560334968567\n4.9555 4.95552715659142 -0.00002715659142\n4.7605 4.75900384783745 0.00149615216255\n8.1035 8.02451953291895 0.07898046708105\n7.376 7.35119462013245 0.02480537986755\n5.422 5.4501354396343 -0.0281354396343\n8.2500 8.19430738687515 0.05569261312485\n4.0485 3.96039354801178 0.08810645198822\n4.249 4.23552632331848 0.01347367668152\n7.6545 7.64323496818545 0.01126503181455\n7.033 6.88298216462135 0.15001783537865\n7.4085 7.3863371014595 0.0221628985405\n7.2775 7.2759775519371 0.0015224480629\n6.8365 6.8069668710232 0.0295331289768\n6.8535 6.7940945625305 0.0594054374695\n4.998 4.96798822283745 0.03001177716255\n6.1145 6.0429987311363 0.0715012688637\n4.871 4.85369718074799 0.01730281925201\n5.311 5.2973562180996 0.0136437819004\n9.0085 8.97694331407545 0.03155668592455\n5.6455 5.616979598999 0.028520401001\n3.934 3.83237943053246 0.10162056946754\n5.2995 5.28114092350005 0.01835907649995\n7.809 7.78207111358645 0.02692888641355\n8.8550 8.77593064308165 0.07906935691835\n4.313 4.35372364521027 -0.04072364521027\n4.992 5.03139266371725 -0.03939266371725\n7.861 7.81471514701845 0.04628485298155\n7.9925 7.95118278265 0.04131721735\n4.0585 3.96693044900894 0.09156955099106\n7.111 7.04923966526985 0.06176033473015\n8.862 8.83656191825865 0.02543808174135\n6.8315 6.69998919963835 0.13151080036165\n3.672 3.61612984538079 0.05587015461921\n5.834 5.7122194468975 0.1217805531025\n6.2300 6.20329195261 0.02670804739\n8.5145 8.50716817379 0.00733182621\n5.1350 5.0798606276512 0.0551393723488\n9.0265 9.00762835144995 0.01887164855005\n7.357 7.3238939344883 0.0331060655117\n4.2805 4.34337705373764 -0.06287705373764\n9.0005 8.9323092997074 0.0681907002926\n6.4150 6.4160676896572 -0.0010676896572\n4.9900 4.92809936404228 0.06190063595772\n6.481 6.4832152426243 -0.0022152426243\n8.8635 8.83288142085075 0.03061857914925\n7.383 7.4009601175785 -0.0179601175785\n5.413 5.3161016702652 0.0968983297348\n3.3655 3.47699934244156 -0.11149934244156\n3.802 3.83249413967133 -0.03049413967133\n4.954 4.86786392331124 0.08613607668876\n4.948 4.89481997489929 0.05318002510071\n6.7125 6.70555835962295 0.00694164037705\n3.3545 3.32608613371849 0.02841386628151\n7.0475 6.96746802330015 0.08003197669985\n7.6400 7.45707643032075 0.18292356967925\n4.714 4.68118336796761 0.03281663203239\n8.921 8.85272547602655 0.06827452397345\n5.372 5.32582753896715 0.04617246103285\n5.9175 5.90872702002525 0.00877297997475\n5.974 5.9056848883629 0.0683151116371\n7.019 6.9966391324997 0.0223608675003\n9.2805 9.23188439011575 0.04861560988425\n7.7095 7.6677560508251 0.0417439491749\n8.337 8.4311276972294 -0.0941276972294\n7.5450 7.51033142209055 0.03466857790945\n6.1775 6.135611563921 0.041888436079\n5.9145 5.87931123375895 0.03518876624105\n6.8605 6.81591963768005 0.04458036231995\n7.2735 7.27520194649695 -0.00170194649695\n6.078 6.04990461468695 0.02809538531305\n9.141 9.1181918680668 0.0228081319332\n4.8365 4.85468298196793 -0.01818298196793\n7.536 7.49554389715195 0.04045610284805\n8.2435 8.2348293662071 0.0086706337929\n4.2900 4.3167936205864 -0.0267936205864\n7.828 7.7659921348095 0.0620078651905\n3.5145 3.53200995922089 -0.01750995922089\n5.908 5.8308235704899 0.0771764295101\n4.513 4.53006964921952 -0.01706964921952\n4.5900 4.56262037158013 0.02737962841987\n3.399 3.38866749405861 0.01033250594139\n8.619 8.5397402346134 0.0792597653866\n6.8685 6.85147523880005 0.01702476119995\n5.097 5.0277101993561 0.0692898006439\n4.2755 4.16309571266175 0.11240428733825\n3.468 3.54644849896431 -0.07844849896431\n7.282 7.33746790885925 -0.05546790885925\n7.767 7.7358447909355 0.0311552090645\n4.9835 4.90737867355347 0.07612132644653\n6.9875 6.95428070425985 0.03321929574015\n4.9315 4.95041823387146 -0.01891823387146\n7.584 7.51947075128555 0.06452924871445\n8.7255 8.68516021966935 0.04033978033065\n4.2205 4.19166240096092\n4.983 4.95283806324005\n7.6015 7.6359705030918\n7.376 7.3840798139572\n5.068 5.00582936406135\n8.392 8.42607721686365\n6.8665 6.87219136953355\n8.2345 8.23955875635145\n5.0125 4.99876606464386\n3.843 3.77417260408402\n6.566 6.52556720376015\n4.253 4.24432995915413\n7.2855 7.25049513578415\n4.891 4.88125985860825\n7.2715 7.2124939262867\n5.4150 5.3595046401024\n4.934 4.92306473851204\n3.5075 3.5542708337307\n9.1575 9.05027770996095\n8.8535 8.82399493455885\n5.073 5.0438419878483\n5.7265 5.7288476228714\n2\n4\n6\n8\n10\n2 4 6 8 10\nReal Latency on  \nRaspberry Pi ARM CPU (s)\nPredicted Latency (s)\nPredicted latency is very \nclose to real latency.\ny = x\nPredicted latency  \n= f (SubTransformer Architecture)\nFigure 6: The latency predictor is very accurate, with\nan average prediction error (RMSE) of 0.1s.\nIn the SuperTransformer training, all possible\nSubTransformers are uniformly sampled, and the\ncorresponding weights are updated. In practice, the\nSuperTransformer only needs to be trained for the\nsame steps as a baseline Transformer model, which\nis fast and low-cost. After training, we can get\nthe performance proxy of sampled models in the\ndesign space by evaluating the corresponding Sub-\nTransformers on the validation set without training.\n2.3 Evolutionary Search for SubTransformer\nGiven a latency requirement, we perform an evo-\nlutionary search to ﬁnd a satisfactory SubTrans-\nformer. There are two ways to evaluate the hard-\nware latency of a SubTransformer: (1) Online mea-\nsurement in which we measure the models during\nthe search process. (2) Ofﬂine, where we train a la-\ntency predictor to provide the latency. We apply the\nofﬂine method here because it is fast and accurate.\nFor the online method, a single sampled SubTrans-\nformer requires hundreds of inferences to get an\naccurate latency, which lasts for minutes and slows\ndown the searching. For the ofﬂine method, we\nencode the architecture of a SubTransformer into\na feature vector, and predict its latency instantly\nwith a multi-layer perceptron (MLP). Trained with\nthousands of real latency data points, the predic-\ntor yields high accuracy (Figure 6). Note that the\npredicted latency is only used in the search pro-\ncess, and we report real measured latency in the\nexperiment section. Compared with deducing a\nclosed-form latency model for each hardware, the\nlatency predictor method is more general and faster.\nWe use an evolutionary algorithm to conduct the\nsearch process. As in Figure 3, the search engine\nqueries the latency predictor for SubTransformer\nlatency, and validates the loss on the validation\nset. The engine only adds SubTransformers with\n7679\nlatency smaller than the hardware constraint to the\npopulation. We then train the searched modelsfrom\nscratch to obtain the ﬁnal performance.\n3 Experiments\n3.1 Datasets\nWe conduct experiments on four machine trans-\nlation tasks: WMT’14 En-De, WMT’14 En-Fr,\nWMT’19 En-De, and IWSLT’14 De-En, consisting\nof 4.5M, 36.3M, 43.0M, and 160K pairs of train-\ning sentences, respectively. For WMT’14 En-De,\nwe apply 32K source-target BPE vocabulary, train\non WMT’16, validate on newstest2013 and test on\nnewstest2014, replicating Wu et al. (2019b); For\nWMT’14 En-Fr, we use 40K source-target BPE vo-\ncabulary, validate on newstest2012&2013, and test\non newstest2014, replicating Gehring et al. (2017).\nWMT’19 En-De adopts 49.6K source-target BPE\nvocabulary, validates on newstest2017, and tests\non newstest2018, the same as Junczys-Dowmunt\n(2019). We use 10K joint BPE vocabulary in lower\ncase for IWSLT’14 De-En (Grave et al., 2017).\n3.2 Experiment Setups\nBaselines. Our baseline models are Trans-\nformer (Vaswani et al., 2017), Levenshtein Trans-\nformer (Gu et al., 2019), both with the Ott et al.\n(2019) implementation, Evolved Transformer (So\net al., 2019) and Lite Transformer (Wu et al., 2020).\nEvaluation Metrics. For evaluation, we use\nbeam four and length penalty 0.6 for WMT, and\nbeam ﬁve for IWSLT (Vaswani et al., 2017). All\nBLEUs are calculated with case-sensitive tokeniza-\ntion1, but we also apply the compound splitting\nBLEU2 for WMT, the same as Vaswani et al.\n(2017). We test the model with the lowest valida-\ntion set loss for WMT and the last ten checkpoints\naveraged for IWSLT.\nWe test the latency of the models by measur-\ning translation from a source sentence to a target\nsentence with the same length. The length is the\naverage output length on the test set – 30 for WMT\nand 23 for IWSLT. For each model, we measure\nthe latency for 300 times, remove the fastest and\nslowest 10% and then take the average of the rest\n80%. We conduct experiments on three represen-\ntative hardware platforms: Raspberry Pi-4 with an\nARM Cortex-A72 CPU, Intel Xeon E5-2640 CPU,\nand Nvidia TITAN Xp GPU.\n1https://github.com/moses-smt/mosesdecoder\n2https://github.com/tensorﬂow/tensor2tensor\n3.3 Implementation Details\nSuperTransformer Setups. The SuperTrans-\nformer for WMT has the following design space:\n[512, 640] for embedding dim, [1024, 2048, 3072]\nfor hidden dim, [4, 8] for the head number in all\nattention modules, [1, 2, 3, 4, 5, 6] for decoder\nlayer number. Due to decoder auto-regression, en-\ncoder only accounts for less than 5% of the mea-\nsured latency; thereby, we set the encoder layer\nnumber ﬁxed as 6. For arbitrary encoder-decoder\nattention, each decoder can choose to attend to the\nlast one, two, or three encoder layers. The Super-\nTransformer design space for IWSLT is the same as\nWMT except for [2048, 1024, 512] for hidden dim\nand [4, 2] for head number. We set the Q, K, V\nvector dim ﬁxed as 512. The design space contains\naround 1015 possible SubTransformers and covers\na wide range of model size and latency (largest =\n6×smallest). We train the SuperTransformers of\nWMT for 40K steps and 50K steps for IWSLT.\nHardware-Aware Evolutionary Search Setups.\nThe input of the latency predictor is a feature vec-\ntor of SubTransformer architecture with ten ele-\nments: layer number, embed dim, average hidden\ndim, average self-attention heads, of both encoder\nand decoder; plus average encoder-decoder atten-\ntion heads, and the average number of encoder\nlayers each decoder layer attends. A dataset of\n2000 (SubTransformer architecture, measured la-\ntency) samples for each hardware is collected, and\nsplit into train:valid:test=8:1:1. We normalize the\nfeatures and latency, and train a three-layer MLP\nwith 400 hidden dim and ReLU activation. We\nchoose three-layer because it is more accurate than\nthe one-layer model, and over three layers do not\nimprove accuracy anymore. With the predictor, we\nconduct an evolutionary search for 30 iterations in\nthe SuperTransformer, with population 125, par-\nents population 25, mutation population 50 with\n0.3 probability and crossover population 50.\nTraining Settings. Our training settings are in\nline with Wu et al. (2019b) and Wu et al. (2020).\nFor WMT, we train for 40K steps with Adam\noptimizer and a cosine learning rate (LR) sched-\nuler (Kingma and Ba, 2015; Loshchilov and Hut-\nter, 2017), where the LR is linearly warmed up\nfrom 10−7 to 10−3, and then cosine annealed. For\nIWSLT, we train for 50K steps with inverse square\nroot LR scheduler. The baseline Transformers are\ntrained with the same settings as the searched Sub-\nTransformers for fair comparisons.\n7680\nWMT14 En-De latency on Intel CPU\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n125.6744315 24.37 318.8635727 24.58 137.8659119 25.83 25.63 1 0 1 0\n181.4779053 25.85 415.5805737 27.63 204.1926424 27.62 27.13 2 1 1 0\n267.8353677 26.54 630.4540406 28.4 278.7041912 27.9 27.28 3 1 2 0\n303.4228822 27.04 340.1943684 28.1 27.49 4 2 2 0\n357.4422042 27.53 369.6464062 28.2 27.59 5 2 3 0\n415.5805737 27.63 2.03523774811584 450.9188682 28.53 27.93 6 3 3 0\n21 9 12 0\nWMT14 En-De latency on Arm CPU-1\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n3.320969731 24.37 3.119880478 24.58 3.510976136 25.82 25.60 1 1 0 0\n4.10328116 25.85 7.347423464 27.63 3.99612087 26.91 26.64 2 2 0 0\n4.906528513 26.54 20.54689761 28.4 4.507411629 27.62 27.13 2 1 1 0\n5.675740421 27.04 5.006854216 27.83 27.23 3 2 1 0\n6.494072417 27.53 6.0 28.15 27.61 4 3 1 0\n7.347423464 27.63 2.96925593028383 6.919880971 28.44 27.81 5 2 3 0\n17 11 6 0\nWMT14 En-De latency on titanxp-1\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n56.4535487 24.37 233.4403444 24.58 57.11806165 25.83 25.63 1 0 1 0\n93.52474536 25.85 245.1151872 27.63 91.17974952 27.62 27.13 2 1 1 0\n131.0418878 26.54 254.7298993 28.4 126.0203572 27.9 27.28 3 1 2 0\n170.571674 27.04 1.47E+02 28.1 27.49 3 1 2 0\n210.3917578 27.53 208.1187446 28.5 27.84 5 2 3 0\n245.1151872 27.63 2.68826344106412\n135.37 14 5 9 0\n24\n25\n26\n27\n28\n29\n100 234 368 501 635\nBLEU Score\n24\n25\n26\n27\n28\n29\n3 7 12 16 21\n     \n                                                                          \n                                                    \n24\n25\n26\n27\n28\n29\n50 101 153 204 255\nWMT14 En-Fr latency on intel CPU-1\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n144.0132509 37.23 326.9822071 37.13 154.6823412 39.09 36.31 1 1 0 0\n201.8358638 38.89 431.1191042 40.6 208.7727835 40 37.15 2 1 0 1\n259.3422612 39.94 737.3172412 41.2 329.3679456 41.1 38.24 4 2 1 1\n329.1240702 40.21 394.5148965 41.41 38.53 5 2 1 2\n374.3495574 40.46 441.962711 41.66 38.81 6 3 2 1\n431.1191042 40.6\n2.23858226354374 18 9 4 5\n37\n38\n39\n40\n41\n42\n120 275 430 585 740\nWMT14 En-Fr latency on Arm CPU\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n4.079504708 37.23 3.829777499 37.13 4.269129376 38.75 35.95 1 1 0 0\n4.854586174 38.89 8.044359436 40.6 5.33629785 40.14 37.29 2 1 0 1\n5.650409361 39.94 23.15697475 41.2 5.818866263 40.64 37.76 3 2 1 0\n6.465141773 40.21 6.859018068 41.11 38.26 4 3 1 0\n7.269941688 40.46 7.827191518 41.4 38.54 5 3 2 0\n8.044359436 40.6 2.95852921149897 9.065828112 41.8 38.86 6 3 3 0\n21 13 7 1\nBLEU Score\n37\n38\n39\n40\n41\n42\n3 8 13 18 23\nIntel CPU latency (ms)\nHAT (Ours)\n3.0× Faster \n3.7× Smaller\nTransformer-Base\nRaspberry Pi ARM CPU latency (s) Nvidia GPU latency (ms)\n1.6× Faster\nLayer Number Scaling of Transformer (Vaswani et al.) Dimension Scaling of Transformer (Vaswani et al.)\nTransformer-Big\n1.5\n1.5\n1.5 Dimension scaling \ncan hardly reduce latency \non Nvidia GPU\nTransformer-Base\n2.7× Faster\nWMT ’14 En-Fr\n3.0× Faster \n3.6× Smaller\nTransformer-Big\nRaspberry Pi ARM CPU latency (s) Intel CPU latency (ms)\nWMT ’14 En-Fr\n1.6\n2.2× Faster\n1.9\nWMT ’14 En-De WMT ’14 En-De WMT ’14 En-De\n2.0× Faster\nTransformer-Base\nTransformer-Big Transformer-Big\nTransformer-Base\nTransformer-Big\nTransformer-Base\nWMT14 En-De latency on titanxp-1-1\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n57.03669935 37.23 233.1844045 37.13 69.31571099 39.1 36.31 1 1 0 0\n90.18124064 38.89 234.7934725 40.6 94.92767955 40 37.15 2 1 0 1\n128.0396621 39.94 238.828734 41.2 132.8543258 40.7 37.81 3 2 1 0\n166.6058949 40.21 168.3487089 41.1 38.25 4 2 1 1\n202.3486488 40.46 208.3378961 41.7 38.78 5 2 2 1\n234.7934725 40.6 1.76730016946125\n188.3 15 8 4 3\n37\n38\n39\n40\n41\n42\n50 98 145 193 240\nNvidia GPU latency (ms)\nWMT ’14 En-Fr\n1.8× Faster\n1.9\nTransformer-Big\nTransformer-Base\nDimension scaling \ncan hardly reduce latency \non Nvidia GPU\nFigure 7: Inference latency and BLEU trade-offs of WMT’14 En-De and En-Fr on three hardware platforms. HAT\nconsistently outperforms the baseline Transformers and achieves up to 3× faster inference speed and 3.7× smaller\nsize over Transformer-Big. Speciﬁc latency, BLEU and SacreBLEU (Post, 2018) are in Appendix Table 8.\nIwslt 14 de-en\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n4.32E+01 33.2 45.60 33.44 1 1 0 0\n7.60E+01 34.01 1.96E+02 33.48 74.53 34.2 33.25 2 2 0 0\n1.03E+02 34.11 1.97E+02 34.5 109.04 34.54 33.59 3 2 0 1\n1.33E+02 34.22 2.03E+02 34.7 137.78 34.7 33.75 4 3 0 1\n1.66E+02 34.4 168.77 34.8 33.89 5 4 0 1\n1.97E+02 34.5 1.80667644900954 201.66\n15 12 0 3\n40 81 122 162 203\nIWSLT ’14 De-En\n1.8× Faster\nNvidia GPU latency (ms)\nWMT19 En-de\nChange #Layer Change #Layer Change \nDimension\nChange \nDimension\nOur HAT Our HAT sacrebleu Total To last one To last two To last three\n51.94430365 40.37 55.68625701 42.39 41.85 1 0 1 0\n87.99107674 42.64 237.9384397 40.89 93.20854973 44.42 43.9 2 1 0 1\n125.6872312 43.82 245.3763484 45.2 134.5091038 45.38 44.66 3 1 2 0\n245.3763484 45.2 176.1489895 46.23 45.56 4 2 2 0\n204.467315 46.49 45.72 5 3 2 0\n1.82423599197306 237.7964813 46.74 46.04 6 3 3 0\n267.73 21 10 10 1\n40\n41\n42\n43\n44\n45\n46\n47\n40 93 145 198 250\n                                        \n                                                                          \n                                                     \nWMT ’19 En-De\n1.8× Faster\n2.0\nHAT (Ours)\nLayer Number Scaling of Transformer (Vaswani et al.)\nDimension Scaling of Transformer (Vaswani et al.)\nTransformer-Base\nBLEU Score\n33\n34\n35\nTransformer-Base\nFigure 8: Inference latency and BLEU trade-offs of\nWMT’19 and IWSLT’14 tasks on Nvidia GPU.\n4 Results\n4.1 HAT Performance Comparisons\nIn Figure 7, 8 and Appendix Table 8, we com-\npare HAT with Transformer baselines on four tasks.\nThe embedding dims are 512 and 1024 for the\nTransformer-Base and Big, respectively. The hid-\nden dims are 4× and 2× of the embedding dim for\nWMT and IWSLT. The IWSLT models are smaller\nto prevent overﬁtting (Wu et al., 2019b). We ob-\ntain a series of baseline models with layer number\nscaling (yellow) and dimension scaling (blue). We\nset different latency constraints on three hardware\nto get a series of HAT models. HAT consistently\noutperforms baselines with a large gap under dif-\nferent latency constraints. On ARM CPU, HAT is\n3× faster and 3.7× smaller than Transformer-Big\nwith the same BLEU. On Intel CPU, HAT achieves\nover 2× speedup. On Nvidia GPU, the blue dash\nline is nearly vertical, indicating that dimension\nscaling can hardly reduce the latency. In this case,\nHAT can still ﬁnd models with low latency and\nhigh performance.\nWe further compare various aspects of HAT with\nTransformer (Vaswani et al., 2017) and Evolved\nTransformer (So et al., 2019) in Table 2. HAT\nachieves up to 1.6×, 3×, and 3.4× speedup with\nup to 1.4×, 3.7×, and 4× smaller size than base-\nlines. We report FLOPs for translating a 23-token\nsentence for IWSLT and 30 for WMT. We show\nthe overall GPU hours for training the SuperTrans-\nformer and the searched SubTransformer. We also\ncalculate the cloud computing costs with differ-\nent modes: “preemptable” is cheaper ($0.74/h)\nthan “on-demand” ($2.48/h) (Strubell et al., 2019).\nHAT is highly affordable since the total GPU-hour\nis over 12000 × smaller than the Evolved Trans-\nformer, and is even smaller than Transformer-Big\nby virtue of the compact model size.\nIn Table 3, we compare HAT with other latest\nmodels. We scale down all models to have similar\nBLEU scores with Levenshtein for fair compar-\nisons. We adopt the average iteration time of 2.88\nfor decoding (Gu et al., 2019), without limiting\nthe length of the output sentence (12 tokens after\n7681\nHardware-\nAware\nHetero.\nLayers Latency #Params FLOPs\n(G) BLEU GPU\nHours\nCO2e\n(lbs)\nCloud\nComp. Cost\nIWSLT’14\nDe-En\nTransformer \u0017 \u0017 3.3s 32M 1.5 34.5 2 5 $12 - $40\nHAT (Ours) \u0013 \u0013 2.1s 23M 1.1 34.5 4 9 $24 - $80\nWMT’14\nEn-Fr\nTransformer \u0017 \u0017 23.2s 176M 10.6 41.2 240 68 $178 - $595\nEvolved Trans. \u0017 \u0017 20.9s 175M 10.8 41.3 2,192,000 626,000 $1.6M - $5.5M\nHAT (Ours) \u0013 \u0013 7.8s 48M 3.4 41.4 216 61 $159 - $534\nHAT (Ours) \u0013 \u0013 9.1s 57M 3.9 41.8 224 64 $166 - $555\nWMT’14\nEn-De\nTransformer \u0017 \u0017 20.5s 176M 10.6 28.4 184 52 $136 - $456\nEvolved Trans. \u0017 \u0017 7.6s 47M 2.9 28.2 2,192,000 626,000 $1.6M - $5.5M\nHAT (Ours) \u0013 \u0013 6.0s 44M 2.7 28.2 184 52 $136 - $456\nHAT (Ours) \u0013 \u0013 6.9s 48M 3.0 28.4 200 57 $147 - $495\nTable 2: Comparisons of latency, model size, FLOPs, BLEU and training cost in terms of CO2 emissions (lbs) and\ncloud computing cost (USD) for Transformer, the Evolved Transformer and HAT. The training cost estimation is\nadapted from Strubell et al. (2019). The training time is for one Nvidia V100 GPU, and the latency is measured on\nthe Raspberry Pi ARM CPU. The cloud computing cost is based on AWS.\nLatency BLEU\nTransformer (Vaswani et al., 2017) 4.3s 25.85\nLevenshtein (Gu et al., 2019) 6.5s 25.20\nEvolved Transformer (So et al., 2019) 3.7s 25.40\nLite Transformer (Wu et al., 2020) 3.4s 25.79\nHAT (Ours) 3.4s 25.92\nTable 3: Raspberry Pi ARM CPU latency and BLEU\ncomparisons with different models on WMT’14 En-De.\nHAT has the lowest latency with the highest BLEU.\nevo_test0\n0 4.208625277404600\n0 4.209958957798910\n0 4.210334553856780\n0 4.210905868528300\n0 4.2151032243625200\n0 4.215591798935550\n0 4.216164532344550\n0 4.217707900460050\n0 4.218880138602530\n0 4.21961313897787\n0 4.221601260307420\n0 4.22189414145149\n0 4.224524245690080\n0 4.225675404247820\n0 4.227898246879480\n0 4.228011938960830\n0 4.228210856029990\n0 4.228249590081780\n0 4.228641656926300\n0 4.2288321035450300\n0 4.229742294997760\n0 4.230000866200430\n0 4.230590800752070\n0 4.231728838086790\n0 4.232213769274810\n1 4.20081189551904\n1 4.201222128919280\n1 4.203368074300320\n1 4.204007219734390\n1 4.205180540818510\n1 4.2062271325472300\n1 4.207978652117870\n1 4.208002199802150\n1 4.2085310446930100\n1 4.208625277404600\n1 4.208868922481550\n1 4.209232732104940\n1 4.209958957798910\n1 4.210082480303900\n1 4.210334553856780\n1 4.210412525654140\n1 4.210905868528300\n1 4.210978475986890\n1 4.213427418362110\n1 4.213768234364380\n1 4.213998791797900\n1 4.214823472836350\n1 4.214984696820870\n1 4.2150500091145200\n1 4.2151032243625200\n2 4.196382748201530\n2 4.197111366440970\n2 4.1997303810214800\n2 4.199785577465020\n2 4.200558025458330\n2 4.200768569458950\n2 4.20081189551904\n2 4.201222128919280\n2 4.201476796495140\n2 4.20182421088594\n2 4.201997523521210\n2 4.202339733076280\n2 4.203073329489250\n2 4.203249588733150\n2 4.203368074300320\n2 4.204007219734390\n2 4.204488683830210\n2 4.205144954852860\n2 4.205180540818510\n2 4.20525582624903\n2 4.205896533133830\n2 4.2062271325472300\n2 4.20664772524957\n2 4.207517209849870\n2 4.207683260899790\n3 4.193733310516630\n3 4.195953013457060\n3 4.196059082972520\n3 4.196304079627780\n3 4.196382748201530\n3 4.196392830472060\n3 4.196585266681230\n3 4.196619291196160\n3 4.196766697181280\n3 4.197111366440970\n3 4.197223169670640\n3 4.197314875518440\n3 4.1975371471892600\n3 4.197729281182160\n3 4.197981892008420\n3 4.198334284572760\n3 4.198400612648870\n3 4.198406413522250\n3 4.199059334981250\n3 4.199091957548490\n3 4.199102744990300\n3 4.199168241971680\n3 4.1997303810214800\n3 4.199785577465020\n3 4.199830733612520\n4 4.191208830864000\n4 4.1915494537836500\n4 4.1922442825652500\n4 4.192382420584790\n4 4.192629348066210\n4 4.193042175489420\n4 4.193733310516630\n4 4.193879902196810\n4 4.194098505297920\n4 4.194228609401650\n4 4.194821767593480\n4 4.194872212525690\n4 4.195013045306960\n4 4.195094694068920\n4 4.195219056063540\n4 4.1953027699700000\n4 4.19533376392735\n4 4.195356883471910\n4 4.195431161514870\n4 4.195596809609770\n4 4.195782899277300\n4 4.195953013457060\n4 4.196059082972520\n4 4.196304079627780\n4 4.1963053976265100\n5 4.189203281729610\n5 4.189290941237360\n5 4.189348421092720\n5 4.189410450981890\n5 4.189621868051720\n5 4.189632831786370\n5 4.1899565138051700\n5 4.190138968482570\n5 4.191109628373750\n5 4.191208830864000\n5 4.1912914198543500\n5 4.1915494537836500\n5 4.191578525309730\n5 4.191772993083820\n5 4.1922442825652500\n5 4.192382420584790\n5 4.192382420584790\n5 4.192427173777270\n5 4.192495038119410\n5 4.192629348066210\n5 4.193042175489420\n5 4.19308106904424\n5 4.193234099609920\n5 4.193322002569660\n5 4.193360216137880\n6 4.185076837584920\n6 4.187136760463000\n6 4.187218283301510\n6 4.187247925680550\n6 4.1872833941176500\n6 4.187285383708090\n6 4.187976300467990\n6 4.188454717216430\n6 4.188696036907090\n6 4.188815999976020\n6 4.188831958673990\n6 4.189117553048020\n6 4.189203281729610\n6 4.189203281729610\n6 4.189290941237360\n6 4.18932709805597\n6 4.189348421092720\n6 4.189410450981890\n6 4.189621868051720\n6 4.189632831786370\n6 4.189643677992460\n6 4.189895096743430\n6 4.1899565138051700\n6 4.190138968482570\n6 4.19021712496765\n7 4.185076837584920\n7 4.185736239903900\n7 4.1862475310664700\n7 4.186659569369420\n7 4.186811684891400\n7 4.187118157372670\n7 4.187136760463000\n7 4.187136760463000\n7 4.187215193979650\n7 4.187218283301510\n7 4.187240042872870\n7 4.187247925680550\n7 4.187247925680550\n7 4.1872833941176500\n7 4.1872833941176500\n7 4.187285383708090\n7 4.187540487818560\n7 4.187574336040660\n7 4.1877433504890000\n7 4.1877433504890000\n7 4.187751804149640\n7 4.187799394817080\n7 4.187845155397120\n7 4.187871078837060\n7 4.187976300467990\n8 4.184518090073240\n8 4.185076837584920\n8 4.185298832224160\n8 4.185565378578360\n8 4.185736239903900\n8 4.18582189303142\n8 4.1859154457564100\n8 4.186113649259380\n8 4.186180019309970\n8 4.18623564389323\n8 4.1862475310664700\n8 4.1862475310664700\n8 4.1862475310664700\n8 4.1862475310664700\n8 4.1862475310664700\n8 4.1863359629046900\n8 4.186569626449860\n8 4.186602341360960\n8 4.1866213809859000\n8 4.1866213809859000\n8 4.186639253720240\n8 4.186659569369420\n8 4.186811684891400\n8 4.186899982411270\n8 4.187075838700140\n9 4.184518090073240\n9 4.184518090073240\n9 4.184518090073240\n9 4.1846040286272400\n9 4.185076443024790\n9 4.185076837584920\n9 4.185084073985580\n9 4.185298832224160\n9 4.185298832224160\n9 4.185331278498580\n9 4.185372850025330\n9 4.185411063593540\n9 4.185431614299820\n9 4.185523756682230\n9 4.185565378578360\n9 4.185565378578360\n9 4.185565378578360\n9 4.185565378578360\n9 4.185565378578360\n9 4.185652290940340\n9 4.185736239903900\n9 4.185736239903900\n9 4.18582189303142\n9 4.1859154457564100\n9 4.186113649259380\n10 4.18280808326508\n10 4.18280808326508\n10 4.183126904638890\n10 4.1836508469102700\n10 4.183993283127540\n10 4.1841563791739400\n10 4.1842142200098400\n10 4.184354481938160\n10 4.184518090073240\n10 4.184518090073240\n10 4.184518090073240\n10 4.184518090073240\n10 4.184518090073240\n10 4.184518090073240\n10 4.184518090073240\n10 4.184518090073240\n10 4.184518090073240\n10 4.184518090073240\n10 4.18457845777294\n10 4.1846040286272400\n10 4.1846040286272400\n10 4.184714908418310\n10 4.184890135089960\n10 4.185041730128360\n10 4.185076443024790\n11 4.18280808326508\n11 4.18280808326508\n11 4.18280808326508\n11 4.183093106786160\n11 4.18309977233387\n11 4.18309977233387\n11 4.183126904638890\n11 4.183282025533760\n11 4.183511928165370\n11 4.183562893581150\n11 4.1836445003686300\n11 4.1836445003686300\n11 4.1836508469102700\n11 4.1836508469102700\n11 4.183993283127540\n11 4.1841563791739400\n11 4.1842142200098400\n11 4.184354481938160\n11 4.184354481938160\n11 4.184354481938160\n11 4.184452962467310\n11 4.184518090073240\n11 4.184518090073240\n11 4.184518090073240\n11 4.184518090073240\n12 4.1822010315197700\n12 4.182796405964250\n12 4.18280808326508\n12 4.18280808326508\n12 4.18280808326508\n12 4.18280808326508\n12 4.18280808326508\n12 4.1829533569465200\n12 4.183013137003480\n12 4.183088229351380\n12 4.183088229351380\n12 4.183093106786160\n12 4.183093106786160\n12 4.18309977233387\n12 4.18309977233387\n12 4.18309977233387\n12 4.183126904638890\n12 4.183126904638890\n12 4.183282025533760\n12 4.183282025533760\n12 4.183282025533760\n12 4.183282025533760\n12 4.183503373765990\n12 4.183511928165370\n12 4.183511928165370\n13 4.1822010315197700\n13 4.1822010315197700\n13 4.1822010315197700\n13 4.1822010315197700\n13 4.1822010315197700\n13 4.1822010315197700\n13 4.1822010315197700\n13 4.182796405964250\n13 4.182796405964250\n13 4.182796405964250\n13 4.182796405964250\n13 4.182796405964250\n13 4.182796405964250\n13 4.182796405964250\n13 4.182796405964250\n13 4.18280808326508\n13 4.18280808326508\n13 4.18280808326508\n13 4.18280808326508\n13 4.18280808326508\n13 4.18280808326508\n13 4.18280808326508\n13 4.18280808326508\n13 4.18280808326508\n13 4.18280808326508\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.1822010315197700\n14 4.182796405964250\n14 4.182796405964250\n14 4.182796405964250\n14 4.182796405964250\n14 4.182796405964250\n14 4.182796405964250\n14 4.182796405964250\n14 4.182796405964250\n14 4.182796405964250\n14 4.182796405964250\n14 4.182796405964250\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n15 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n16 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n17 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n18 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n19 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n20 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n21 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n22 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n23 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n24 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n25 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n26 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n27 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n28 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\n29 4.1822010315197700\nevo_test_random\n0 4.19909210865662\n0 4.20501325571882\n0 4.2127913454601000\n0 4.216637928944970\n0 4.219772658798430\n0 4.220363013094880\n0 4.220871324069250\n0 4.220917403655350\n0 4.221113432880160\n0 4.223513265111700\n0 4.225638794104810\n0 4.226889398604780\n0 4.227363450007110\n0 4.22822945072542\n0 4.231154459278110\n0 4.231739608738820\n0 4.2326655406222100\n0 4.233461880490560\n0 4.234011402011140\n0 4.234721736166310\n0 4.236636788317970\n0 4.236899330306620\n0 4.237078368261210\n0 4.239803192115380\n0 4.240372256954680\n1 4.19909210865662\n1 4.203244963145260\n1 4.203418049118330\n1 4.20501325571882\n1 4.207005012038490\n1 4.208679583989130\n1 4.2127913454601000\n1 4.213033219213920\n1 4.213606296813670\n1 4.216637928944970\n1 4.219772658798430\n1 4.220168562110590\n1 4.220363013094880\n1 4.220871324069250\n1 4.220917403655350\n1 4.221113432880160\n1 4.222327402052430\n1 4.223513265111700\n1 4.223954097909160\n1 4.225638794104810\n1 4.226889398604780\n1 4.227363450007110\n1 4.227810520212560\n1 4.2278898267984300\n1 4.22813651082786\n2 4.19909210865662\n2 4.203244963145260\n2 4.203418049118330\n2 4.20501325571882\n2 4.207005012038490\n2 4.208679583989130\n2 4.208930876816660\n2 4.209490346289420\n2 4.212546306830360\n2 4.212615052636620\n2 4.2127913454601000\n2 4.213033219213920\n2 4.213606296813670\n2 4.214246348896550\n2 4.216637928944970\n2 4.218409990827000\n2 4.219619989213300\n2 4.219772658798430\n2 4.220085964725340\n2 4.220168562110590\n2 4.2202687468037000\n2 4.220363013094880\n2 4.220655130303390\n2 4.220871324069250\n2 4.220917403655350\n3 4.19909210865662\n3 4.201270248465190\n3 4.2021127266839100\n3 4.203244963145260\n3 4.203418049118330\n3 4.20501325571882\n3 4.207005012038490\n3 4.2077612075124600\n3 4.208679583989130\n3 4.208930876816660\n3 4.209490346289420\n3 4.211152586137320\n3 4.212546306830360\n3 4.212615052636620\n3 4.2127913454601000\n3 4.213033219213920\n3 4.213606296813670\n3 4.214246348896550\n3 4.21507272570406\n3 4.216637928944970\n3 4.217308840666860\n3 4.218409990827000\n3 4.219619989213300\n3 4.219772658798430\n3 4.220085964725340\n4 4.19909210865662\n4 4.201270248465190\n4 4.2021127266839100\n4 4.203244963145260\n4 4.203418049118330\n4 4.20437635172207\n4 4.20501325571882\n4 4.205092419591460\n4 4.2066463400916700\n4 4.207005012038490\n4 4.2077612075124600\n4 4.208679583989130\n4 4.208930876816660\n4 4.209490346289420\n4 4.211152586137320\n4 4.211182849738680\n4 4.212546306830360\n4 4.212615052636620\n4 4.2127913454601000\n4 4.213033219213920\n4 4.213606296813670\n4 4.214246348896550\n4 4.214295098059690\n4 4.21507272570406\n4 4.216637928944970\n5 4.19909210865662\n5 4.201270248465190\n5 4.2021127266839100\n5 4.203244963145260\n5 4.203418049118330\n5 4.20437635172207\n5 4.20501325571882\n5 4.205092419591460\n5 4.2066463400916700\n5 4.206805977440780\n5 4.207005012038490\n5 4.2077612075124600\n5 4.208679583989130\n5 4.208930876816660\n5 4.209490346289420\n5 4.211152586137320\n5 4.211182849738680\n5 4.212546306830360\n5 4.212615052636620\n5 4.2127913454601000\n5 4.213033219213920\n5 4.213606296813670\n5 4.214246348896550\n5 4.214295098059690\n5 4.214733664235130\n6 4.19909210865662\n6 4.201270248465190\n6 4.2021127266839100\n6 4.203244963145260\n6 4.203418049118330\n6 4.20437635172207\n6 4.20501325571882\n6 4.205092419591460\n6 4.206323782988980\n6 4.2066463400916700\n6 4.206805977440780\n6 4.207005012038490\n6 4.2077612075124600\n6 4.208679583989130\n6 4.208731304945580\n6 4.208930876816660\n6 4.209227510479400\n6 4.209490346289420\n6 4.210308017589400\n6 4.211152586137320\n6 4.211182849738680\n6 4.212546306830360\n6 4.212615052636620\n6 4.2127913454601000\n6 4.213033219213920\n7 4.19909210865662\n7 4.201270248465190\n7 4.2021127266839100\n7 4.203244963145260\n7 4.203418049118330\n7 4.20437635172207\n7 4.20501325571882\n7 4.205092419591460\n7 4.205289397439560\n7 4.206323782988980\n7 4.2066463400916700\n7 4.206805977440780\n7 4.207005012038490\n7 4.2077612075124600\n7 4.208229432856720\n7 4.208679583989130\n7 4.208731304945580\n7 4.208930876816660\n7 4.209070853318500\n7 4.209227510479400\n7 4.209490346289420\n7 4.210308017589400\n7 4.210391194222500\n7 4.210447280525050\n7 4.210781044814380\n8 4.19909210865662\n8 4.201270248465190\n8 4.2021127266839100\n8 4.203244963145260\n8 4.203418049118330\n8 4.20437635172207\n8 4.204571474298080\n8 4.20501325571882\n8 4.205092419591460\n8 4.205289397439560\n8 4.206323782988980\n8 4.2066463400916700\n8 4.206805977440780\n8 4.207005012038490\n8 4.207186904257840\n8 4.2077612075124600\n8 4.208229432856720\n8 4.208679583989130\n8 4.208731304945580\n8 4.208930876816660\n8 4.209070853318500\n8 4.209227510479400\n8 4.209490346289420\n8 4.210308017589400\n8 4.210391194222500\n9 4.19909210865662\n9 4.200154961299580\n9 4.201270248465190\n9 4.2021127266839100\n9 4.203244963145260\n9 4.203418049118330\n9 4.20437635172207\n9 4.2045218856453\n9 4.204571474298080\n9 4.20501325571882\n9 4.205092419591460\n9 4.205289397439560\n9 4.206323782988980\n9 4.2066463400916700\n9 4.206805977440780\n9 4.207005012038490\n9 4.207186904257840\n9 4.2077612075124600\n9 4.208229432856720\n9 4.208679583989130\n9 4.208731304945580\n9 4.208930876816660\n9 4.209070853318500\n9 4.209227510479400\n9 4.209490346289420\n10 4.19909210865662\n10 4.200154961299580\n10 4.201270248465190\n10 4.2021127266839100\n10 4.203244963145260\n10 4.203418049118330\n10 4.20437635172207\n10 4.2045218856453\n10 4.204571474298080\n10 4.20501325571882\n10 4.205092419591460\n10 4.205289397439560\n10 4.206323782988980\n10 4.2066463400916700\n10 4.206805977440780\n10 4.207005012038490\n10 4.207186904257840\n10 4.2077612075124600\n10 4.208229432856720\n10 4.208679583989130\n10 4.208731304945580\n10 4.208930876816660\n10 4.209070853318500\n10 4.209227510479400\n10 4.209490346289420\n11 4.19909210865662\n11 4.200154961299580\n11 4.201270248465190\n11 4.2021127266839100\n11 4.203244963145260\n11 4.203418049118330\n11 4.20437635172207\n11 4.2045218856453\n11 4.204571474298080\n11 4.20501325571882\n11 4.205092419591460\n11 4.205289397439560\n11 4.206323782988980\n11 4.2066463400916700\n11 4.206805977440780\n11 4.207005012038490\n11 4.207186904257840\n11 4.2077612075124600\n11 4.208229432856720\n11 4.208679583989130\n11 4.208731304945580\n11 4.208930876816660\n11 4.209070853318500\n11 4.209227510479400\n11 4.209490346289420\n12 4.19909210865662\n12 4.200154961299580\n12 4.201270248465190\n12 4.2021127266839100\n12 4.203244963145260\n12 4.203418049118330\n12 4.20437635172207\n12 4.2045218856453\n12 4.204571474298080\n12 4.20501325571882\n12 4.205092419591460\n12 4.205289397439560\n12 4.206323782988980\n12 4.2066463400916700\n12 4.206805977440780\n12 4.207005012038490\n12 4.207186904257840\n12 4.2077612075124600\n12 4.208229432856720\n12 4.208679583989130\n12 4.208731304945580\n12 4.208930876816660\n12 4.209070853318500\n12 4.209227510479400\n12 4.209490346289420\n13 4.19909210865662\n13 4.200154961299580\n13 4.201270248465190\n13 4.2021127266839100\n13 4.203244963145260\n13 4.203418049118330\n13 4.20437635172207\n13 4.2045218856453\n13 4.204571474298080\n13 4.20501325571882\n13 4.205092419591460\n13 4.205289397439560\n13 4.206183882041200\n13 4.206323782988980\n13 4.2066463400916700\n13 4.206805977440780\n13 4.207005012038490\n13 4.207099387463320\n13 4.207186904257840\n13 4.2077612075124600\n13 4.208229432856720\n13 4.208679583989130\n13 4.208731304945580\n13 4.208930876816660\n13 4.209070853318500\n14 4.19909210865662\n14 4.200154961299580\n14 4.201270248465190\n14 4.2021127266839100\n14 4.203244963145260\n14 4.203418049118330\n14 4.20437635172207\n14 4.2045218856453\n14 4.204571474298080\n14 4.20501325571882\n14 4.205092419591460\n14 4.205289397439560\n14 4.206183882041200\n14 4.206323782988980\n14 4.2066463400916700\n14 4.206805977440780\n14 4.207005012038490\n14 4.207099387463320\n14 4.207186904257840\n14 4.2077612075124600\n14 4.208229432856720\n14 4.208679583989130\n14 4.208731304945580\n14 4.208930876816660\n14 4.209070853318500\n15 4.19909210865662\n15 4.200154961299580\n15 4.201270248465190\n15 4.2021127266839100\n15 4.203244963145260\n15 4.203418049118330\n15 4.20437635172207\n15 4.2045218856453\n15 4.204571474298080\n15 4.20501325571882\n15 4.205092419591460\n15 4.205289397439560\n15 4.206183882041200\n15 4.206323782988980\n15 4.2066463400916700\n15 4.206805977440780\n15 4.207005012038490\n15 4.207099387463320\n15 4.207186904257840\n15 4.207756136995060\n15 4.2077612075124600\n15 4.208229432856720\n15 4.208679583989130\n15 4.208731304945580\n15 4.208930876816660\n16 4.19909210865662\n16 4.200154961299580\n16 4.201270248465190\n16 4.2021127266839100\n16 4.203244963145260\n16 4.203418049118330\n16 4.20437635172207\n16 4.2045218856453\n16 4.204571474298080\n16 4.20501325571882\n16 4.205092419591460\n16 4.205289397439560\n16 4.206183882041200\n16 4.206323782988980\n16 4.2066463400916700\n16 4.206805977440780\n16 4.207005012038490\n16 4.207099387463320\n16 4.207126998277440\n16 4.207186904257840\n16 4.207756136995060\n16 4.2077612075124600\n16 4.208229432856720\n16 4.208679583989130\n16 4.208731304945580\n17 4.19909210865662\n17 4.200154961299580\n17 4.201270248465190\n17 4.2021127266839100\n17 4.203232975233260\n17 4.203244963145260\n17 4.203418049118330\n17 4.20437635172207\n17 4.2045218856453\n17 4.204571474298080\n17 4.20501325571882\n17 4.205092419591460\n17 4.205289397439560\n17 4.206183882041200\n17 4.206323782988980\n17 4.2066463400916700\n17 4.206805977440780\n17 4.207005012038490\n17 4.207099387463320\n17 4.207126998277440\n17 4.207186904257840\n17 4.207756136995060\n17 4.2077612075124600\n17 4.208229432856720\n17 4.208538205539600\n18 4.19909210865662\n18 4.200154961299580\n18 4.201270248465190\n18 4.2021127266839100\n18 4.203232975233260\n18 4.203244963145260\n18 4.203418049118330\n18 4.20437635172207\n18 4.2045218856453\n18 4.204571474298080\n18 4.20501325571882\n18 4.205092419591460\n18 4.205289397439560\n18 4.206183882041200\n18 4.206323782988980\n18 4.2066463400916700\n18 4.206805977440780\n18 4.207005012038490\n18 4.207099387463320\n18 4.207126998277440\n18 4.207186904257840\n18 4.207756136995060\n18 4.2077612075124600\n18 4.208229432856720\n18 4.208538205539600\n19 4.19909210865662\n19 4.200154961299580\n19 4.200223916978250\n19 4.201270248465190\n19 4.2021127266839100\n19 4.203232975233260\n19 4.203244963145260\n19 4.203418049118330\n19 4.20437635172207\n19 4.2045218856453\n19 4.204571474298080\n19 4.20501325571882\n19 4.205092419591460\n19 4.205289397439560\n19 4.206183882041200\n19 4.206323782988980\n19 4.2066463400916700\n19 4.206805977440780\n19 4.207005012038490\n19 4.207099387463320\n19 4.207126998277440\n19 4.207186904257840\n19 4.207756136995060\n19 4.2077612075124600\n19 4.208229432856720\n20 4.19909210865662\n20 4.200154961299580\n20 4.200223916978250\n20 4.201270248465190\n20 4.2021127266839100\n20 4.203232975233260\n20 4.203244963145260\n20 4.203418049118330\n20 4.20437635172207\n20 4.2045218856453\n20 4.204571474298080\n20 4.204903526028550\n20 4.20501325571882\n20 4.205092419591460\n20 4.205289397439560\n20 4.206183882041200\n20 4.206323782988980\n20 4.2066463400916700\n20 4.206805977440780\n20 4.207005012038490\n20 4.207099387463320\n20 4.207126998277440\n20 4.207186904257840\n20 4.207756136995060\n20 4.2077612075124600\n21 4.19909210865662\n21 4.200154961299580\n21 4.200223916978250\n21 4.201270248465190\n21 4.2021127266839100\n21 4.203232975233260\n21 4.203244963145260\n21 4.203418049118330\n21 4.203805490374960\n21 4.20437635172207\n21 4.2045218856453\n21 4.204571474298080\n21 4.204903526028550\n21 4.20501325571882\n21 4.205092419591460\n21 4.205289397439560\n21 4.206183882041200\n21 4.206323782988980\n21 4.2066463400916700\n21 4.206805977440780\n21 4.207005012038490\n21 4.207099387463320\n21 4.207126998277440\n21 4.207186904257840\n21 4.207756136995060\n22 4.19909210865662\n22 4.200154961299580\n22 4.200223916978250\n22 4.201270248465190\n22 4.2020682169434300\n22 4.2021127266839100\n22 4.203232975233260\n22 4.203244963145260\n22 4.203418049118330\n22 4.203805490374960\n22 4.20437635172207\n22 4.2045218856453\n22 4.204571474298080\n22 4.204903526028550\n22 4.20501325571882\n22 4.205092419591460\n22 4.205289397439560\n22 4.2056506969889300\n22 4.206183882041200\n22 4.206323782988980\n22 4.206474194346990\n22 4.2066463400916700\n22 4.206805977440780\n22 4.207005012038490\n22 4.207099387463320\n23 4.19909210865662\n23 4.200154961299580\n23 4.200223916978250\n23 4.201270248465190\n23 4.2020682169434300\n23 4.2021127266839100\n23 4.203232975233260\n23 4.203244963145260\n23 4.203418049118330\n23 4.203805490374960\n23 4.20437635172207\n23 4.2045218856453\n23 4.204571474298080\n23 4.204615245287680\n23 4.204903526028550\n23 4.20501325571882\n23 4.205092419591460\n23 4.205289397439560\n23 4.2056506969889300\n23 4.206183882041200\n23 4.206323782988980\n23 4.206474194346990\n23 4.2066463400916700\n23 4.206805977440780\n23 4.207005012038490\n24 4.19909210865662\n24 4.200154961299580\n24 4.200223916978250\n24 4.201270248465190\n24 4.2020682169434300\n24 4.2021127266839100\n24 4.203232975233260\n24 4.203244963145260\n24 4.203418049118330\n24 4.203805490374960\n24 4.20437635172207\n24 4.2045218856453\n24 4.204571474298080\n24 4.204615245287680\n24 4.204903526028550\n24 4.20501325571882\n24 4.205092419591460\n24 4.205289397439560\n24 4.2056506969889300\n24 4.205757622783820\n24 4.206026066384600\n24 4.206183882041200\n24 4.206323782988980\n24 4.206474194346990\n24 4.2066463400916700\n25 4.19909210865662\n25 4.200154961299580\n25 4.200223916978250\n25 4.201270248465190\n25 4.2020682169434300\n25 4.2021127266839100\n25 4.202276032602720\n25 4.203232975233260\n25 4.203244963145260\n25 4.203418049118330\n25 4.203805490374960\n25 4.20437635172207\n25 4.2045218856453\n25 4.204571474298080\n25 4.204615245287680\n25 4.204903526028550\n25 4.20501325571882\n25 4.205092419591460\n25 4.205289397439560\n25 4.2056506969889300\n25 4.205757622783820\n25 4.206026066384600\n25 4.206183882041200\n25 4.206323782988980\n25 4.206474194346990\n26 4.19909210865662\n26 4.200154961299580\n26 4.200223916978250\n26 4.201270248465190\n26 4.2020682169434300\n26 4.2021127266839100\n26 4.202276032602720\n26 4.203232975233260\n26 4.203244963145260\n26 4.203418049118330\n26 4.203805490374960\n26 4.20437635172207\n26 4.2045218856453\n26 4.204571474298080\n26 4.204615245287680\n26 4.204903526028550\n26 4.20501325571882\n26 4.205092419591460\n26 4.205289397439560\n26 4.2056506969889300\n26 4.205757622783820\n26 4.206026066384600\n26 4.206183882041200\n26 4.206323782988980\n26 4.206474194346990\n27 4.19909210865662\n27 4.200154961299580\n27 4.200223916978250\n27 4.201270248465190\n27 4.2020682169434300\n27 4.2021127266839100\n27 4.202276032602720\n27 4.203232975233260\n27 4.203244963145260\n27 4.203418049118330\n27 4.203805490374960\n27 4.20437635172207\n27 4.2045218856453\n27 4.204571474298080\n27 4.204615245287680\n27 4.204903526028550\n27 4.20501325571882\n27 4.205092419591460\n27 4.205289397439560\n27 4.205648531105670\n27 4.2056506969889300\n27 4.205757622783820\n27 4.206026066384600\n27 4.206183882041200\n27 4.206323782988980\n28 4.195620995306170\n28 4.19909210865662\n28 4.200154961299580\n28 4.200223916978250\n28 4.201270248465190\n28 4.2020682169434300\n28 4.2021127266839100\n28 4.202276032602720\n28 4.203232975233260\n28 4.203244963145260\n28 4.203418049118330\n28 4.203805490374960\n28 4.20437635172207\n28 4.2045218856453\n28 4.204571474298080\n28 4.204615245287680\n28 4.204903526028550\n28 4.20501325571882\n28 4.205092419591460\n28 4.205289397439560\n28 4.205648531105670\n28 4.2056506969889300\n28 4.205757622783820\n28 4.206026066384600\n28 4.206183882041200\n29 4.195620995306170\n29 4.19909210865662\n29 4.200154961299580\n29 4.200223916978250\n29 4.201270248465190\n29 4.2020682169434300\n29 4.2021127266839100\n29 4.202276032602720\n29 4.203232975233260\n29 4.203244963145260\n29 4.203418049118330\n29 4.203805490374960\n29 4.20437635172207\n29 4.2045218856453\n29 4.204571474298080\n29 4.204615245287680\n29 4.204903526028550\n29 4.204984880969140\n29 4.20501325571882\n29 4.205092419591460\n29 4.205289397439560\n29 4.205648531105670\n29 4.2056506969889300\n29 4.205757622783820\n29 4.206026066384600\nOn raspberry pi MACs vs latency\nChange #Layers Change #Layers Change Hidden \nDim\nChange Hidden \nDim\n1 4.208625277404600\n1 4.209958957798910 1 4.20501325571882\n1 4.210334553856780 1 4.2127913454601000\n1 4.210905868528300 1 4.216637928944970\n1 4.2151032243625200 1 4.219772658798430\n1 4.215591798935550 1 4.220363013094880\n1 4.216164532344550 1 4.220871324069250\n1 4.217707900460050 1 4.220917403655350\n1 4.218880138602530 1 4.221113432880160\n1 4.21961313897787 1 4.223513265111700\n1 4.221601260307420 1 4.225638794104810\n1 4.22189414145149 1 4.226889398604780\n1 4.224524245690080 1 4.227363450007110\n1 4.225675404247820 1 4.22822945072542\n1 4.227898246879480 1 4.231154459278110\n1 4.228011938960830 1 4.231739608738820\n1 4.228210856029990 1 4.2326655406222100\n1 4.228249590081780 1 4.233461880490560\n1 4.228641656926300 1 4.234011402011140\n1 4.2288321035450300 1 4.234721736166310\n1 4.229742294997760 1 4.236636788317970\n1 4.230000866200430 1 4.236899330306620\n1 4.230590800752070 1 4.237078368261210\n1 4.231728838086790 1 4.239803192115380\n1 4.232213769274810 1 4.240372256954680\n2 4.20081189551904 2 4.19909210865662\n2 4.201222128919280 2 4.203244963145260\n2 4.203368074300320 2 4.203418049118330\n2 4.204007219734390 2 4.20501325571882\n2 4.205180540818510 2 4.207005012038490\n2 4.2062271325472300 2 4.208679583989130\n2 4.207978652117870 2 4.2127913454601000\n2 4.208002199802150 2 4.213033219213920\n2 4.2085310446930100 2 4.213606296813670\n2 4.208625277404600 2 4.216637928944970\n2 4.208868922481550 2 4.219772658798430\n2 4.209232732104940 2 4.220168562110590\n2 4.209958957798910 2 4.220363013094880\n2 4.210082480303900 2 4.220871324069250\n2 4.210334553856780 2 4.220917403655350\n2 4.210412525654140 2 4.221113432880160\n2 4.210905868528300 2 4.222327402052430\n2 4.210978475986890 2 4.223513265111700\n2 4.213427418362110 2 4.223954097909160\n2 4.213768234364380 2 4.225638794104810\n2 4.213998791797900 2 4.226889398604780\n2 4.214823472836350 2 4.227363450007110\n2 4.214984696820870 2 4.227810520212560\n2 4.2150500091145200 2 4.2278898267984300\n2 4.2151032243625200 2 4.22813651082786\n3 4.196382748201530 3 4.19909210865662\n3 4.197111366440970 3 4.203244963145260\n3 4.1997303810214800 3 4.203418049118330\n3 4.199785577465020 3 4.20501325571882\n3 4.200558025458330 3 4.207005012038490\n3 4.200768569458950 3 4.208679583989130\n3 4.20081189551904 3 4.208930876816660\n3 4.201222128919280 3 4.209490346289420\n3 4.201476796495140 3 4.212546306830360\n3 4.20182421088594 3 4.212615052636620\n3 4.201997523521210 3 4.2127913454601000\n3 4.202339733076280 3 4.213033219213920\n3 4.203073329489250 3 4.213606296813670\n3 4.203249588733150 3 4.214246348896550\n3 4.203368074300320 3 4.216637928944970\n3 4.204007219734390 3 4.218409990827000\n3 4.204488683830210 3 4.219619989213300\n3 4.205144954852860 3 4.219772658798430\n3 4.205180540818510 3 4.220085964725340\n3 4.20525582624903 3 4.220168562110590\n3 4.205896533133830 3 4.2202687468037000\n3 4.2062271325472300 3 4.220363013094880\n3 4.20664772524957 3 4.220655130303390\n3 4.207517209849870 3 4.220871324069250\n3 4.207683260899790 3 4.220917403655350\n4 4.193733310516630 4 4.19909210865662\n4 4.195953013457060 4 4.201270248465190\n4 4.196059082972520 4 4.2021127266839100\n4 4.196304079627780 4 4.203244963145260\n4 4.196382748201530 4 4.203418049118330\n4 4.196392830472060 4 4.20501325571882\n4 4.196585266681230 4 4.207005012038490\n4 4.196619291196160 4 4.2077612075124600\n4 4.196766697181280 4 4.208679583989130\n4 4.197111366440970 4 4.208930876816660\n4 4.197223169670640 4 4.209490346289420\n4 4.197314875518440 4 4.211152586137320\n4 4.1975371471892600 4 4.212546306830360\n4 4.197729281182160 4 4.212615052636620\n4 4.197981892008420 4 4.2127913454601000\n4 4.198334284572760 4 4.213033219213920\n4 4.198400612648870 4 4.213606296813670\n4 4.198406413522250 4 4.214246348896550\n4 4.199059334981250 4 4.21507272570406\n4 4.199091957548490 4 4.216637928944970\n4 4.199102744990300 4 4.217308840666860\n4 4.199168241971680 4 4.218409990827000\n4 4.1997303810214800 4 4.219619989213300\n4 4.199785577465020 4 4.219772658798430\n4 4.199830733612520 4 4.220085964725340\n5 4.191208830864000 5 4.19909210865662\n5 4.1915494537836500 5 4.201270248465190\n5 4.1922442825652500 5 4.2021127266839100\n5 4.192382420584790 5 4.203244963145260\n5 4.192629348066210 5 4.203418049118330\n5 4.193042175489420 5 4.20437635172207\n5 4.193733310516630 5 4.20501325571882\n5 4.193879902196810 5 4.205092419591460\n5 4.194098505297920 5 4.2066463400916700\n5 4.194228609401650 5 4.207005012038490\n5 4.194821767593480 5 4.2077612075124600\n5 4.194872212525690 5 4.208679583989130\n5 4.195013045306960 5 4.208930876816660\n5 4.195094694068920 5 4.209490346289420\n5 4.195219056063540 5 4.211152586137320\n5 4.1953027699700000 5 4.211182849738680\n5 4.19533376392735 5 4.212546306830360\n5 4.195356883471910 5 4.212615052636620\n5 4.195431161514870 5 4.2127913454601000\n5 4.195596809609770 5 4.213033219213920\n5 4.195782899277300 5 4.213606296813670\n5 4.195953013457060 5 4.214246348896550\n5 4.196059082972520 5 4.214295098059690\n5 4.196304079627780 5 4.21507272570406\n5 4.1963053976265100 5 4.216637928944970\n6 4.189203281729610 6 4.19909210865662\n6 4.189290941237360 6 4.201270248465190\n6 4.189348421092720 6 4.2021127266839100\n6 4.189410450981890 6 4.203244963145260\n6 4.189621868051720 6 4.203418049118330\n6 4.189632831786370 6 4.20437635172207\n6 4.1899565138051700 6 4.20501325571882\n6 4.190138968482570 6 4.205092419591460\n6 4.191109628373750 6 4.2066463400916700\n6 4.191208830864000 6 4.206805977440780\n6 4.1912914198543500 6 4.207005012038490\n6 4.1915494537836500 6 4.2077612075124600\n6 4.191578525309730 6 4.208679583989130\n6 4.191772993083820 6 4.208930876816660\n6 4.1922442825652500 6 4.209490346289420\n6 4.192382420584790 6 4.211152586137320\n6 4.192382420584790 6 4.211182849738680\n6 4.192427173777270 6 4.212546306830360\n6 4.192495038119410 6 4.212615052636620\n6 4.192629348066210 6 4.2127913454601000\n6 4.193042175489420 6 4.213033219213920\n6 4.19308106904424 6 4.213606296813670\n6 4.193234099609920 6 4.214246348896550\n6 4.193322002569660 6 4.214295098059690\n6 4.193360216137880 6 4.214733664235130\n7 4.185076837584920 7 4.19909210865662\n7 4.187136760463000 7 4.201270248465190\n7 4.187218283301510 7 4.2021127266839100\n7 4.187247925680550 7 4.203244963145260\n7 4.1872833941176500 7 4.203418049118330\n7 4.187285383708090 7 4.20437635172207\n7 4.187976300467990 7 4.20501325571882\n7 4.188454717216430 7 4.205092419591460\n7 4.188696036907090 7 4.206323782988980\n7 4.188815999976020 7 4.2066463400916700\n7 4.188831958673990 7 4.206805977440780\n7 4.189117553048020 7 4.207005012038490\n7 4.189203281729610 7 4.2077612075124600\n7 4.189203281729610 7 4.208679583989130\n7 4.189290941237360 7 4.208731304945580\n7 4.18932709805597 7 4.208930876816660\n7 4.189348421092720 7 4.209227510479400\n7 4.189410450981890 7 4.209490346289420\n7 4.189621868051720 7 4.210308017589400\n7 4.189632831786370 7 4.211152586137320\n7 4.189643677992460 7 4.211182849738680\n7 4.189895096743430 7 4.212546306830360\n7 4.1899565138051700 7 4.212615052636620\n7 4.190138968482570 7 4.2127913454601000\n7 4.19021712496765 7 4.213033219213920\n8 4.185076837584920 8 4.19909210865662\n8 4.185736239903900 8 4.201270248465190\n8 4.1862475310664700 8 4.2021127266839100\n8 4.186659569369420 8 4.203244963145260\n8 4.186811684891400 8 4.203418049118330\n8 4.187118157372670 8 4.20437635172207\n8 4.187136760463000 8 4.20501325571882\n8 4.187136760463000 8 4.205092419591460\n8 4.187215193979650 8 4.205289397439560\n8 4.187218283301510 8 4.206323782988980\n8 4.187240042872870 8 4.2066463400916700\n8 4.187247925680550 8 4.206805977440780\n8 4.187247925680550 8 4.207005012038490\n8 4.1872833941176500 8 4.2077612075124600\n8 4.1872833941176500 8 4.208229432856720\n8 4.187285383708090 8 4.208679583989130\n8 4.187540487818560 8 4.208731304945580\n8 4.187574336040660 8 4.208930876816660\n8 4.1877433504890000 8 4.209070853318500\n8 4.1877433504890000 8 4.209227510479400\n8 4.187751804149640 8 4.209490346289420\n8 4.187799394817080 8 4.210308017589400\n8 4.187845155397120 8 4.210391194222500\n8 4.187871078837060 8 4.210447280525050\n8 4.187976300467990 8 4.210781044814380\n9 4.184518090073240 9 4.19909210865662\n9 4.185076837584920 9 4.201270248465190\n9 4.185298832224160 9 4.2021127266839100\n9 4.185565378578360 9 4.203244963145260\n9 4.185736239903900 9 4.203418049118330\n9 4.18582189303142 9 4.20437635172207\n9 4.1859154457564100 9 4.204571474298080\n9 4.186113649259380 9 4.20501325571882\n9 4.186180019309970 9 4.205092419591460\n9 4.18623564389323 9 4.205289397439560\n9 4.1862475310664700 9 4.206323782988980\n9 4.1862475310664700 9 4.2066463400916700\n9 4.1862475310664700 9 4.206805977440780\n9 4.1862475310664700 9 4.207005012038490\n9 4.1862475310664700 9 4.207186904257840\n9 4.1863359629046900 9 4.2077612075124600\n9 4.186569626449860 9 4.208229432856720\n9 4.186602341360960 9 4.208679583989130\n9 4.1866213809859000 9 4.208731304945580\n9 4.1866213809859000 9 4.208930876816660\n9 4.186639253720240 9 4.209070853318500\n9 4.186659569369420 9 4.209227510479400\n9 4.186811684891400 9 4.209490346289420\n9 4.186899982411270 9 4.210308017589400\n9 4.187075838700140 9 4.210391194222500\n10 4.184518090073240 10 4.19909210865662\n10 4.184518090073240 10 4.200154961299580\n10 4.184518090073240 10 4.201270248465190\n10 4.1846040286272400 10 4.2021127266839100\n10 4.185076443024790 10 4.203244963145260\n10 4.185076837584920 10 4.203418049118330\n10 4.185084073985580 10 4.20437635172207\n10 4.185298832224160 10 4.2045218856453\n10 4.185298832224160 10 4.204571474298080\n10 4.185331278498580 10 4.20501325571882\n10 4.185372850025330 10 4.205092419591460\n10 4.185411063593540 10 4.205289397439560\n10 4.185431614299820 10 4.206323782988980\n10 4.185523756682230 10 4.2066463400916700\n10 4.185565378578360 10 4.206805977440780\n10 4.185565378578360 10 4.207005012038490\n10 4.185565378578360 10 4.207186904257840\n10 4.185565378578360 10 4.2077612075124600\n10 4.185565378578360 10 4.208229432856720\n10 4.185652290940340 10 4.208679583989130\n10 4.185736239903900 10 4.208731304945580\n10 4.185736239903900 10 4.208930876816660\n10 4.18582189303142 10 4.209070853318500\n10 4.1859154457564100 10 4.209227510479400\n10 4.186113649259380 10 4.209490346289420\n11 4.18280808326508 11 4.19909210865662\n11 4.18280808326508 11 4.200154961299580\n11 4.183126904638890 11 4.201270248465190\n11 4.1836508469102700 11 4.2021127266839100\n11 4.183993283127540 11 4.203244963145260\n11 4.1841563791739400 11 4.203418049118330\n11 4.1842142200098400 11 4.20437635172207\n11 4.184354481938160 11 4.2045218856453\n11 4.184518090073240 11 4.204571474298080\n11 4.184518090073240 11 4.20501325571882\n11 4.184518090073240 11 4.205092419591460\n11 4.184518090073240 11 4.205289397439560\n11 4.184518090073240 11 4.206323782988980\n11 4.184518090073240 11 4.2066463400916700\n11 4.184518090073240 11 4.206805977440780\n11 4.184518090073240 11 4.207005012038490\n11 4.184518090073240 11 4.207186904257840\n11 4.184518090073240 11 4.2077612075124600\n11 4.18457845777294 11 4.208229432856720\n11 4.1846040286272400 11 4.208679583989130\n11 4.1846040286272400 11 4.208731304945580\n11 4.184714908418310 11 4.208930876816660\n11 4.184890135089960 11 4.209070853318500\n11 4.185041730128360 11 4.209227510479400\n11 4.185076443024790 11 4.209490346289420\n12 4.18280808326508 12 4.19909210865662\n12 4.18280808326508 12 4.200154961299580\n12 4.18280808326508 12 4.201270248465190\n12 4.183093106786160 12 4.2021127266839100\n12 4.18309977233387 12 4.203244963145260\n12 4.18309977233387 12 4.203418049118330\n12 4.183126904638890 12 4.20437635172207\n12 4.183282025533760 12 4.2045218856453\n12 4.183511928165370 12 4.204571474298080\n12 4.183562893581150 12 4.20501325571882\n12 4.1836445003686300 12 4.205092419591460\n12 4.1836445003686300 12 4.205289397439560\n12 4.1836508469102700 12 4.206323782988980\n12 4.1836508469102700 12 4.2066463400916700\n12 4.183993283127540 12 4.206805977440780\n12 4.1841563791739400 12 4.207005012038490\n12 4.1842142200098400 12 4.207186904257840\n12 4.184354481938160 12 4.2077612075124600\n12 4.184354481938160 12 4.208229432856720\n12 4.184354481938160 12 4.208679583989130\n12 4.184452962467310 12 4.208731304945580\n12 4.184518090073240 12 4.208930876816660\n12 4.184518090073240 12 4.209070853318500\n12 4.184518090073240 12 4.209227510479400\n12 4.184518090073240 12 4.209490346289420\n13 4.1822010315197700 13 4.19909210865662\n13 4.182796405964250 13 4.200154961299580\n13 4.18280808326508 13 4.201270248465190\n13 4.18280808326508 13 4.2021127266839100\n13 4.18280808326508 13 4.203244963145260\n13 4.18280808326508 13 4.203418049118330\n13 4.18280808326508 13 4.20437635172207\n13 4.1829533569465200 13 4.2045218856453\n13 4.183013137003480 13 4.204571474298080\n13 4.183088229351380 13 4.20501325571882\n13 4.183088229351380 13 4.205092419591460\n13 4.183093106786160 13 4.205289397439560\n13 4.183093106786160 13 4.206323782988980\n13 4.18309977233387 13 4.2066463400916700\n13 4.18309977233387 13 4.206805977440780\n13 4.18309977233387 13 4.207005012038490\n13 4.183126904638890 13 4.207186904257840\n13 4.183126904638890 13 4.2077612075124600\n13 4.183282025533760 13 4.208229432856720\n13 4.183282025533760 13 4.208679583989130\n13 4.183282025533760 13 4.208731304945580\n13 4.183282025533760 13 4.208930876816660\n13 4.183503373765990 13 4.209070853318500\n13 4.183511928165370 13 4.209227510479400\n13 4.183511928165370 13 4.209490346289420\n14 4.1822010315197700 14 4.19909210865662\n14 4.1822010315197700 14 4.200154961299580\n14 4.1822010315197700 14 4.201270248465190\n14 4.1822010315197700 14 4.2021127266839100\n14 4.1822010315197700 14 4.203244963145260\n14 4.1822010315197700 14 4.203418049118330\n14 4.1822010315197700 14 4.20437635172207\n14 4.182796405964250 14 4.2045218856453\n14 4.182796405964250 14 4.204571474298080\n14 4.182796405964250 14 4.20501325571882\n14 4.182796405964250 14 4.205092419591460\n14 4.182796405964250 14 4.205289397439560\n14 4.182796405964250 14 4.206183882041200\n14 4.182796405964250 14 4.206323782988980\n14 4.182796405964250 14 4.2066463400916700\n14 4.18280808326508 14 4.206805977440780\n14 4.18280808326508 14 4.207005012038490\n14 4.18280808326508 14 4.207099387463320\n14 4.18280808326508 14 4.207186904257840\n14 4.18280808326508 14 4.2077612075124600\n14 4.18280808326508 14 4.208229432856720\n14 4.18280808326508 14 4.208679583989130\n14 4.18280808326508 14 4.208731304945580\n14 4.18280808326508 14 4.208930876816660\n14 4.18280808326508 14 4.209070853318500\n15 4.1822010315197700 15 4.19909210865662\n15 4.1822010315197700 15 4.200154961299580\n15 4.1822010315197700 15 4.201270248465190\n15 4.1822010315197700 15 4.2021127266839100\n15 4.1822010315197700 15 4.203244963145260\n15 4.1822010315197700 15 4.203418049118330\n15 4.1822010315197700 15 4.20437635172207\n15 4.1822010315197700 15 4.2045218856453\n15 4.1822010315197700 15 4.204571474298080\n15 4.1822010315197700 15 4.20501325571882\n15 4.1822010315197700 15 4.205092419591460\n15 4.1822010315197700 15 4.205289397439560\n15 4.1822010315197700 15 4.206183882041200\n15 4.1822010315197700 15 4.206323782988980\n15 4.182796405964250 15 4.2066463400916700\n15 4.182796405964250 15 4.206805977440780\n15 4.182796405964250 15 4.207005012038490\n15 4.182796405964250 15 4.207099387463320\n15 4.182796405964250 15 4.207186904257840\n15 4.182796405964250 15 4.2077612075124600\n15 4.182796405964250 15 4.208229432856720\n15 4.182796405964250 15 4.208679583989130\n15 4.182796405964250 15 4.208731304945580\n15 4.182796405964250 15 4.208930876816660\n15 4.182796405964250 15 4.209070853318500\n16 4.1822010315197700 16 4.19909210865662\n16 4.1822010315197700 16 4.200154961299580\n16 4.1822010315197700 16 4.201270248465190\n16 4.1822010315197700 16 4.2021127266839100\n16 4.1822010315197700 16 4.203244963145260\n16 4.1822010315197700 16 4.203418049118330\n16 4.1822010315197700 16 4.20437635172207\n16 4.1822010315197700 16 4.2045218856453\n16 4.1822010315197700 16 4.204571474298080\n16 4.1822010315197700 16 4.20501325571882\n16 4.1822010315197700 16 4.205092419591460\n16 4.1822010315197700 16 4.205289397439560\n16 4.1822010315197700 16 4.206183882041200\n16 4.1822010315197700 16 4.206323782988980\n16 4.1822010315197700 16 4.2066463400916700\n16 4.1822010315197700 16 4.206805977440780\n16 4.1822010315197700 16 4.207005012038490\n16 4.1822010315197700 16 4.207099387463320\n16 4.1822010315197700 16 4.207186904257840\n16 4.1822010315197700 16 4.207756136995060\n16 4.1822010315197700 16 4.2077612075124600\n16 4.1822010315197700 16 4.208229432856720\n16 4.1822010315197700 16 4.208679583989130\n16 4.1822010315197700 16 4.208731304945580\n16 4.1822010315197700 16 4.208930876816660\n17 4.1822010315197700 17 4.19909210865662\n17 4.1822010315197700 17 4.200154961299580\n17 4.1822010315197700 17 4.201270248465190\n17 4.1822010315197700 17 4.2021127266839100\n17 4.1822010315197700 17 4.203244963145260\n17 4.1822010315197700 17 4.203418049118330\n17 4.1822010315197700 17 4.20437635172207\n17 4.1822010315197700 17 4.2045218856453\n17 4.1822010315197700 17 4.204571474298080\n17 4.1822010315197700 17 4.20501325571882\n17 4.1822010315197700 17 4.205092419591460\n17 4.1822010315197700 17 4.205289397439560\n17 4.1822010315197700 17 4.206183882041200\n17 4.1822010315197700 17 4.206323782988980\n17 4.1822010315197700 17 4.2066463400916700\n17 4.1822010315197700 17 4.206805977440780\n17 4.1822010315197700 17 4.207005012038490\n17 4.1822010315197700 17 4.207099387463320\n17 4.1822010315197700 17 4.207126998277440\n17 4.1822010315197700 17 4.207186904257840\n17 4.1822010315197700 17 4.207756136995060\n17 4.1822010315197700 17 4.2077612075124600\n17 4.1822010315197700 17 4.208229432856720\n17 4.1822010315197700 17 4.208679583989130\n17 4.1822010315197700 17 4.208731304945580\n18 4.1822010315197700 18 4.19909210865662\n18 4.1822010315197700 18 4.200154961299580\n18 4.1822010315197700 18 4.201270248465190\n18 4.1822010315197700 18 4.2021127266839100\n18 4.1822010315197700 18 4.203232975233260\n18 4.1822010315197700 18 4.203244963145260\n18 4.1822010315197700 18 4.203418049118330\n18 4.1822010315197700 18 4.20437635172207\n18 4.1822010315197700 18 4.2045218856453\n18 4.1822010315197700 18 4.204571474298080\n18 4.1822010315197700 18 4.20501325571882\n18 4.1822010315197700 18 4.205092419591460\n18 4.1822010315197700 18 4.205289397439560\n18 4.1822010315197700 18 4.206183882041200\n18 4.1822010315197700 18 4.206323782988980\n18 4.1822010315197700 18 4.2066463400916700\n18 4.1822010315197700 18 4.206805977440780\n18 4.1822010315197700 18 4.207005012038490\n18 4.1822010315197700 18 4.207099387463320\n18 4.1822010315197700 18 4.207126998277440\n18 4.1822010315197700 18 4.207186904257840\n18 4.1822010315197700 18 4.207756136995060\n18 4.1822010315197700 18 4.2077612075124600\n18 4.1822010315197700 18 4.208229432856720\n18 4.1822010315197700 18 4.208538205539600\n19 4.1822010315197700 19 4.19909210865662\n19 4.1822010315197700 19 4.200154961299580\n19 4.1822010315197700 19 4.201270248465190\n19 4.1822010315197700 19 4.2021127266839100\n19 4.1822010315197700 19 4.203232975233260\n19 4.1822010315197700 19 4.203244963145260\n19 4.1822010315197700 19 4.203418049118330\n19 4.1822010315197700 19 4.20437635172207\n19 4.1822010315197700 19 4.2045218856453\n19 4.1822010315197700 19 4.204571474298080\n19 4.1822010315197700 19 4.20501325571882\n19 4.1822010315197700 19 4.205092419591460\n19 4.1822010315197700 19 4.205289397439560\n19 4.1822010315197700 19 4.206183882041200\n19 4.1822010315197700 19 4.206323782988980\n19 4.1822010315197700 19 4.2066463400916700\n19 4.1822010315197700 19 4.206805977440780\n19 4.1822010315197700 19 4.207005012038490\n19 4.1822010315197700 19 4.207099387463320\n19 4.1822010315197700 19 4.207126998277440\n19 4.1822010315197700 19 4.207186904257840\n19 4.1822010315197700 19 4.207756136995060\n19 4.1822010315197700 19 4.2077612075124600\n19 4.1822010315197700 19 4.208229432856720\n19 4.1822010315197700 19 4.208538205539600\n20 4.1822010315197700 20 4.19909210865662\n20 4.1822010315197700 20 4.200154961299580\n20 4.1822010315197700 20 4.200223916978250\n20 4.1822010315197700 20 4.201270248465190\n20 4.1822010315197700 20 4.2021127266839100\n20 4.1822010315197700 20 4.203232975233260\n20 4.1822010315197700 20 4.203244963145260\n20 4.1822010315197700 20 4.203418049118330\n20 4.1822010315197700 20 4.20437635172207\n20 4.1822010315197700 20 4.2045218856453\n20 4.1822010315197700 20 4.204571474298080\n20 4.1822010315197700 20 4.20501325571882\n20 4.1822010315197700 20 4.205092419591460\n20 4.1822010315197700 20 4.205289397439560\n20 4.1822010315197700 20 4.206183882041200\n20 4.1822010315197700 20 4.206323782988980\n20 4.1822010315197700 20 4.2066463400916700\n20 4.1822010315197700 20 4.206805977440780\n20 4.1822010315197700 20 4.207005012038490\n20 4.1822010315197700 20 4.207099387463320\n20 4.1822010315197700 20 4.207126998277440\n20 4.1822010315197700 20 4.207186904257840\n20 4.1822010315197700 20 4.207756136995060\n20 4.1822010315197700 20 4.2077612075124600\n20 4.1822010315197700 20 4.208229432856720\n21 4.1822010315197700 21 4.19909210865662\n21 4.1822010315197700 21 4.200154961299580\n21 4.1822010315197700 21 4.200223916978250\n21 4.1822010315197700 21 4.201270248465190\n21 4.1822010315197700 21 4.2021127266839100\n21 4.1822010315197700 21 4.203232975233260\n21 4.1822010315197700 21 4.203244963145260\n21 4.1822010315197700 21 4.203418049118330\n21 4.1822010315197700 21 4.20437635172207\n21 4.1822010315197700 21 4.2045218856453\n21 4.1822010315197700 21 4.204571474298080\n21 4.1822010315197700 21 4.204903526028550\n21 4.1822010315197700 21 4.20501325571882\n21 4.1822010315197700 21 4.205092419591460\n21 4.1822010315197700 21 4.205289397439560\n21 4.1822010315197700 21 4.206183882041200\n21 4.1822010315197700 21 4.206323782988980\n21 4.1822010315197700 21 4.2066463400916700\n21 4.1822010315197700 21 4.206805977440780\n21 4.1822010315197700 21 4.207005012038490\n21 4.1822010315197700 21 4.207099387463320\n21 4.1822010315197700 21 4.207126998277440\n21 4.1822010315197700 21 4.207186904257840\n21 4.1822010315197700 21 4.207756136995060\n21 4.1822010315197700 21 4.2077612075124600\n22 4.1822010315197700 22 4.19909210865662\n22 4.1822010315197700 22 4.200154961299580\n22 4.1822010315197700 22 4.200223916978250\n22 4.1822010315197700 22 4.201270248465190\n22 4.1822010315197700 22 4.2021127266839100\n22 4.1822010315197700 22 4.203232975233260\n22 4.1822010315197700 22 4.203244963145260\n22 4.1822010315197700 22 4.203418049118330\n22 4.1822010315197700 22 4.203805490374960\n22 4.1822010315197700 22 4.20437635172207\n22 4.1822010315197700 22 4.2045218856453\n22 4.1822010315197700 22 4.204571474298080\n22 4.1822010315197700 22 4.204903526028550\n22 4.1822010315197700 22 4.20501325571882\n22 4.1822010315197700 22 4.205092419591460\n22 4.1822010315197700 22 4.205289397439560\n22 4.1822010315197700 22 4.206183882041200\n22 4.1822010315197700 22 4.206323782988980\n22 4.1822010315197700 22 4.2066463400916700\n22 4.1822010315197700 22 4.206805977440780\n22 4.1822010315197700 22 4.207005012038490\n22 4.1822010315197700 22 4.207099387463320\n22 4.1822010315197700 22 4.207126998277440\n22 4.1822010315197700 22 4.207186904257840\n22 4.1822010315197700 22 4.207756136995060\n23 4.1822010315197700 23 4.19909210865662\n23 4.1822010315197700 23 4.200154961299580\n23 4.1822010315197700 23 4.200223916978250\n23 4.1822010315197700 23 4.201270248465190\n23 4.1822010315197700 23 4.2020682169434300\n23 4.1822010315197700 23 4.2021127266839100\n23 4.1822010315197700 23 4.203232975233260\n23 4.1822010315197700 23 4.203244963145260\n23 4.1822010315197700 23 4.203418049118330\n23 4.1822010315197700 23 4.203805490374960\n23 4.1822010315197700 23 4.20437635172207\n23 4.1822010315197700 23 4.2045218856453\n23 4.1822010315197700 23 4.204571474298080\n23 4.1822010315197700 23 4.204903526028550\n23 4.1822010315197700 23 4.20501325571882\n23 4.1822010315197700 23 4.205092419591460\n23 4.1822010315197700 23 4.205289397439560\n23 4.1822010315197700 23 4.2056506969889300\n23 4.1822010315197700 23 4.206183882041200\n23 4.1822010315197700 23 4.206323782988980\n23 4.1822010315197700 23 4.206474194346990\n23 4.1822010315197700 23 4.2066463400916700\n23 4.1822010315197700 23 4.206805977440780\n23 4.1822010315197700 23 4.207005012038490\n23 4.1822010315197700 23 4.207099387463320\n24 4.1822010315197700 24 4.19909210865662\n24 4.1822010315197700 24 4.200154961299580\n24 4.1822010315197700 24 4.200223916978250\n24 4.1822010315197700 24 4.201270248465190\n24 4.1822010315197700 24 4.2020682169434300\n24 4.1822010315197700 24 4.2021127266839100\n24 4.1822010315197700 24 4.203232975233260\n24 4.1822010315197700 24 4.203244963145260\n24 4.1822010315197700 24 4.203418049118330\n24 4.1822010315197700 24 4.203805490374960\n24 4.1822010315197700 24 4.20437635172207\n24 4.1822010315197700 24 4.2045218856453\n24 4.1822010315197700 24 4.204571474298080\n24 4.1822010315197700 24 4.204615245287680\n24 4.1822010315197700 24 4.204903526028550\n24 4.1822010315197700 24 4.20501325571882\n24 4.1822010315197700 24 4.205092419591460\n24 4.1822010315197700 24 4.205289397439560\n24 4.1822010315197700 24 4.2056506969889300\n24 4.1822010315197700 24 4.206183882041200\n24 4.1822010315197700 24 4.206323782988980\n24 4.1822010315197700 24 4.206474194346990\n24 4.1822010315197700 24 4.2066463400916700\n24 4.1822010315197700 24 4.206805977440780\n24 4.1822010315197700 24 4.207005012038490\n25 4.1822010315197700 25 4.19909210865662\n25 4.1822010315197700 25 4.200154961299580\n25 4.1822010315197700 25 4.200223916978250\n25 4.1822010315197700 25 4.201270248465190\n25 4.1822010315197700 25 4.2020682169434300\n25 4.1822010315197700 25 4.2021127266839100\n25 4.1822010315197700 25 4.203232975233260\n25 4.1822010315197700 25 4.203244963145260\n25 4.1822010315197700 25 4.203418049118330\n25 4.1822010315197700 25 4.203805490374960\n25 4.1822010315197700 25 4.20437635172207\n25 4.1822010315197700 25 4.2045218856453\n25 4.1822010315197700 25 4.204571474298080\n25 4.1822010315197700 25 4.204615245287680\n25 4.1822010315197700 25 4.204903526028550\n25 4.1822010315197700 25 4.20501325571882\n25 4.1822010315197700 25 4.205092419591460\n25 4.1822010315197700 25 4.205289397439560\n25 4.1822010315197700 25 4.2056506969889300\n25 4.1822010315197700 25 4.205757622783820\n25 4.1822010315197700 25 4.206026066384600\n25 4.1822010315197700 25 4.206183882041200\n25 4.1822010315197700 25 4.206323782988980\n25 4.1822010315197700 25 4.206474194346990\n25 4.1822010315197700 25 4.2066463400916700\n26 4.1822010315197700 26 4.19909210865662\n26 4.1822010315197700 26 4.200154961299580\n26 4.1822010315197700 26 4.200223916978250\n26 4.1822010315197700 26 4.201270248465190\n26 4.1822010315197700 26 4.2020682169434300\n26 4.1822010315197700 26 4.2021127266839100\n26 4.1822010315197700 26 4.202276032602720\n26 4.1822010315197700 26 4.203232975233260\n26 4.1822010315197700 26 4.203244963145260\n26 4.1822010315197700 26 4.203418049118330\n26 4.1822010315197700 26 4.203805490374960\n26 4.1822010315197700 26 4.20437635172207\n26 4.1822010315197700 26 4.2045218856453\n26 4.1822010315197700 26 4.204571474298080\n26 4.1822010315197700 26 4.204615245287680\n26 4.1822010315197700 26 4.204903526028550\n26 4.1822010315197700 26 4.20501325571882\n26 4.1822010315197700 26 4.205092419591460\n26 4.1822010315197700 26 4.205289397439560\n26 4.1822010315197700 26 4.2056506969889300\n26 4.1822010315197700 26 4.205757622783820\n26 4.1822010315197700 26 4.206026066384600\n26 4.1822010315197700 26 4.206183882041200\n26 4.1822010315197700 26 4.206323782988980\n26 4.1822010315197700 26 4.206474194346990\n27 4.1822010315197700 27 4.19909210865662\n27 4.1822010315197700 27 4.200154961299580\n27 4.1822010315197700 27 4.200223916978250\n27 4.1822010315197700 27 4.201270248465190\n27 4.1822010315197700 27 4.2020682169434300\n27 4.1822010315197700 27 4.2021127266839100\n27 4.1822010315197700 27 4.202276032602720\n27 4.1822010315197700 27 4.203232975233260\n27 4.1822010315197700 27 4.203244963145260\n27 4.1822010315197700 27 4.203418049118330\n27 4.1822010315197700 27 4.203805490374960\n27 4.1822010315197700 27 4.20437635172207\n27 4.1822010315197700 27 4.2045218856453\n27 4.1822010315197700 27 4.204571474298080\n27 4.1822010315197700 27 4.204615245287680\n27 4.1822010315197700 27 4.204903526028550\n27 4.1822010315197700 27 4.20501325571882\n27 4.1822010315197700 27 4.205092419591460\n27 4.1822010315197700 27 4.205289397439560\n27 4.1822010315197700 27 4.2056506969889300\n27 4.1822010315197700 27 4.205757622783820\n27 4.1822010315197700 27 4.206026066384600\n27 4.1822010315197700 27 4.206183882041200\n27 4.1822010315197700 27 4.206323782988980\n27 4.1822010315197700 27 4.206474194346990\n28 4.1822010315197700 28 4.19909210865662\n28 4.1822010315197700 28 4.200154961299580\n28 4.1822010315197700 28 4.200223916978250\n28 4.1822010315197700 28 4.201270248465190\n28 4.1822010315197700 28 4.2020682169434300\n28 4.1822010315197700 28 4.2021127266839100\n28 4.1822010315197700 28 4.202276032602720\n28 4.1822010315197700 28 4.203232975233260\n28 4.1822010315197700 28 4.203244963145260\n28 4.1822010315197700 28 4.203418049118330\n28 4.1822010315197700 28 4.203805490374960\n28 4.1822010315197700 28 4.20437635172207\n28 4.1822010315197700 28 4.2045218856453\n28 4.1822010315197700 28 4.204571474298080\n28 4.1822010315197700 28 4.204615245287680\n28 4.1822010315197700 28 4.204903526028550\n28 4.1822010315197700 28 4.20501325571882\n28 4.1822010315197700 28 4.205092419591460\n28 4.1822010315197700 28 4.205289397439560\n28 4.1822010315197700 28 4.205648531105670\n28 4.1822010315197700 28 4.2056506969889300\n28 4.1822010315197700 28 4.205757622783820\n28 4.1822010315197700 28 4.206026066384600\n28 4.1822010315197700 28 4.206183882041200\n28 4.1822010315197700 28 4.206323782988980\n29 4.1822010315197700 29 4.195620995306170\n29 4.1822010315197700 29 4.19909210865662\n29 4.1822010315197700 29 4.200154961299580\n29 4.1822010315197700 29 4.200223916978250\n29 4.1822010315197700 29 4.201270248465190\n29 4.1822010315197700 29 4.2020682169434300\n29 4.1822010315197700 29 4.2021127266839100\n29 4.1822010315197700 29 4.202276032602720\n29 4.1822010315197700 29 4.203232975233260\n29 4.1822010315197700 29 4.203244963145260\n29 4.1822010315197700 29 4.203418049118330\n29 4.1822010315197700 29 4.203805490374960\n29 4.1822010315197700 29 4.20437635172207\n29 4.1822010315197700 29 4.2045218856453\n29 4.1822010315197700 29 4.204571474298080\n29 4.1822010315197700 29 4.204615245287680\n29 4.1822010315197700 29 4.204903526028550\n29 4.1822010315197700 29 4.20501325571882\n29 4.1822010315197700 29 4.205092419591460\n29 4.1822010315197700 29 4.205289397439560\n29 4.1822010315197700 29 4.205648531105670\n29 4.1822010315197700 29 4.2056506969889300\n29 4.1822010315197700 29 4.205757622783820\n29 4.1822010315197700 29 4.206026066384600\n29 4.1822010315197700 29 4.206183882041200\n30 4.1822010315197700 30 4.195620995306170\n30 4.1822010315197700 30 4.19909210865662\n30 4.1822010315197700 30 4.200154961299580\n30 4.1822010315197700 30 4.200223916978250\n30 4.1822010315197700 30 4.201270248465190\n30 4.1822010315197700 30 4.2020682169434300\n30 4.1822010315197700 30 4.2021127266839100\n30 4.1822010315197700 30 4.202276032602720\n30 4.1822010315197700 30 4.203232975233260\n30 4.1822010315197700 30 4.203244963145260\n30 4.1822010315197700 30 4.203418049118330\n30 4.1822010315197700 30 4.203805490374960\n30 4.1822010315197700 30 4.20437635172207\n30 4.1822010315197700 30 4.2045218856453\n30 4.1822010315197700 30 4.204571474298080\n30 4.1822010315197700 30 4.204615245287680\n30 4.1822010315197700 30 4.204903526028550\n30 4.1822010315197700 30 4.204984880969140\n30 4.1822010315197700 30 4.20501325571882\n30 4.1822010315197700 30 4.205092419591460\n30 4.1822010315197700 30 4.205289397439560\n30 4.1822010315197700 30 4.205648531105670\n30 4.1822010315197700 30 4.2056506969889300\n30 4.1822010315197700 30 4.205757622783820\n30 4.1822010315197700 30 4.206026066384600\n4.18\n4.20\n4.22\n4.24\n1 6 11 15 20\nEvolutionary Search\nRandom Search\nSearch Iterations\nValidation Loss\nFigure 9: Evolutionary search can ﬁnd better SubTrans-\nformers in the SuperTransformer than random search.\ndecoding). HAT runs 1.3× faster than Transformer\nwith higher BLEU; 1.9× faster than Levenshtein\nwith 0.7 higher BLEU. Under similar latency, HAT\nalso outperforms Lite Transformer. These results\ndemonstrate HAT’s effectiveness in lower latency\nscenarios. Our framework can also be adopted to\nspeedup those models.\n4.2 Analysis\nDesign Insights. For all HAT WMT models\nin Figure 7, 10% of all decoder layers attend to\nSubTransformer Latency #Params BLEU\nWMT’14\nEn-De\nLargest 10.1s 71M 28.1\nSearched HAT 6.9s 48M 28.4\nWMT’14\nEn-Fr\nLargest 10.1s 71M 41.4\nSearched HAT 9.1s 57M 41.8\nTable 4: The searched HAT compared with the largest\nSubTransformer in the design space. Larger models do\nnot necessarily have better performance. HAT models\nhave lower latency, smaller size, and higher BLEU.\nthree encoder layers, 40% attend to two encoder\nlayers. That demonstrates the necessity of arbitrary\nencoder-decoder attentions.\nIn Appendix Figure 12, we visualize the mod-\nels specialized for different hardware mentioned in\nTable 1. We ﬁnd that the GPU model is wide but\nshallow; the Raspberry Pi model is deep but thin.\nThe phenomenon echos with our latency proﬁling\n(Figure 2) as GPU latency is insensitive to embed-\nding and hidden dim, but Raspberry Pi is highly\nsensitive. It guides manual designs: on GPU, we\ncan reduce the layer number and increase dimen-\nsion to reduce latency and keep high performance.\nAblation Study. HAT achieves higher BLEU\nwith 1.5× lower latency and 1.5× smaller size com-\npared with the largest SubTransformer (Table 4).\nThis suggests that larger models do not always\nprovide better performance, and demonstrates the\neffectiveness of HAT. We also compare the evo-\nlutionary search with random search (Figure 9).\nEvolutionary search can ﬁnd models with lower\nlosses than random search.\n7682\nWMT’14 En-De WMT’14 En-Fr\nInherited\nVal Loss\nInherited\nBLEU\nFrom-\nScratch\nBLEU\nInherited\nVal Loss\nInherited\nBLEU\nFrom-\nScratch\nBLEU\n4.71 24.9 25.8 3.92 37.4 38.8\n4.40 25.8 27.6 3.71 38.0 40.0\n4.07 26.3 28.1 3.48 39.5 41.1\n4.02 26.7 28.2 3.46 39.6 41.4\n4.01 26.9 28.4 3.45 39.7 41.7\nTable 5: The performance of SubTransformers with in-\nherited weights are close to those trained from-scratch,\nand have the same relative performance order.\nTable 1\nHuman Life 11023 5000\nAmerican Life 36156\nUS car including \nfuel\n126000\nEvolved \nTransformer\n626155\nHAT (Ours) 6000\n626,155\n126,000\n36,156\n11,023\nHuman Life \n(Avg. 1 year)\nAmerican Life \n(Avg. 1 year)\nUS Car w/ Fuel \n(Avg. 1 lifetime)\nEvolved \nTransformer\nHAT (Ours) 52 12041×\n0 175K 350K 525K 700K\nCO2 Emission (lbs)\nFigure 10: The search cost measured in pounds of CO2\nemission. Our framework for searching HAT reduces\nthe cost by four orders of magnitude than the Evolved\nTransformer (So et al., 2019).\nSubTransformer Performance Proxy. All Sub-\nTransformers inside the SuperTransformer are uni-\nformly sampled and thus equally trained, so the\nperformance order is well-preserved during train-\ning. We conduct experiments to show the effective-\nness of the SubTransformer performance proxy as\nin Table 5 and Appendix Figure 11. The BLEUs\nof SubTransformers with inherited weights and\nweights trained from-scratch are very close. More\nimportantly, they also have the same relative per-\nformance order. Therefore, we can rely on the\nproxy to search high-performance model architec-\nture, signiﬁcantly reducing the search cost.\nLow Search Cost. As shown in Table 2 and Fig-\nure 10, the search cost of HAT is 12,041× lower\nthan the Evolved Transformer. Although both are\nusing Evolutionary Search, the key difference is\nthat Evolved Transformer needs to train all individ-\nual models and sort their ﬁnal performance to pick\ntop ones; on the contrary, HAT trains all models\ntogether inside SuperTransformer and sorts their\nperformance proxy to pick top ones. The superior\nperformance of HAT proves that the performance\nproxy is accurate enough to ﬁnd good models.\nFinetuning Inherited SubTransformers In sec-\ntion 4.1, we trained each searched SubTransformer\nTask From-Scratch 40K Inherit-Finetune 10K\nWMT’14\nEn-Fr\n41.5 41.7\n40.0 40.2\nWMT’14\nEn-De\n28.0 28.0\n27.5 27.4\nTable 6: The SubTransformer inherited from the Super-\nTransformer can achieve similar or better performance\nthan the same SubTransformer trained from-scratch.\nTraining steps are saved by 4×.\nfrom-scratch in order to conduct fair comparisons\nwith baselines. In practice, we can also directly\nﬁnetune the SubTransformers with the inherited\nweights from the SuperTransformer to further re-\nduce the training cost. With 10K ﬁnetuning steps\n(1/4 of from-scratch training), the inherited Sub-\nTransformers can achieve similar or better perfor-\nmance than trained from-scratch ones (Table 6).\nIn this way, the training cost for a model under a\nnew hardware constraint can be further reduced\nby 4×, since the SuperTransformer training cost is\namortizable among all searched models.\nQuantization Friendly. HAT is orthogonal to\nother model compression techniques such as quan-\ntization. We apply K-means quantization to HAT\nand further reduce the model size. We initialize\ncentroids uniformly in the range of [min, max] of\neach weight matrix and run at most 300 iterations\nfor each of them. Even without any ﬁnetuning,\n4-bit quantization can reduce the model size by\n25× with negligible BLEU loss compared to the\nTransformer-Big baseline (Table 7). Interestingly,\nthe 8-bit model even has 0.1 higher BLEU than the\nfull precision model, indicating the robustness of\nsearched HAT. Compared with the Transformer-\nBase 4-bit quantization baseline, which has 24MB\nmodel size and 38.9 BLEU score, HAT has 2.2\nhigher BLEU with similar model size.\nKnowledge Distillation Friendly. HAT is also\northogonal to knowledge distillation (KD) because\nHAT focuses on searching for an efﬁcient architec-\nture while KD focuses on better training a given\narchitecture. We combine KD with HAT by dis-\ntilling token-level knowledge (top-5 soft labels)\nfrom a high-performance SubTransformer to a low-\nperformance SubTransformer on WMT’14 En-De\ntask. The teacher model has a BLEU of 28.5 and\n49M parameters; the student model has 30M pa-\nrameters. KD can improve the BLEU of the student\nmodel from 25.8 to 26.1.\n7683\nBLEU Model Size Reduction\nTransformer Float32 41.2 705MB –\nHAT Float32 41.8 227MB 3 ×\nHAT 8 bits 41.9 57MB 12 ×\nHAT 4 bits 41.1 28MB 25 ×\nTable 7: K-means quantization of HAT models on\nWMT’14 En-Fr. 4-bit quantization reduces the model\nsize by 25 × with only 0.1 BLEU loss compared with\nthe transformer baseline. 8-bit quantization even has\n0.1 higher BLEU than its full precision version.\n5 Related Work\nTransformer. Transformer (Vaswani et al., 2017)\nhas prevailed in sequence modeling (Ng et al.,\n2019; Junczys-Dowmunt, 2018). By stacking iden-\ntical blocks, the model obtains a large capacity\nbut incurs high latency. Recently, a research trend\nis to modify the Transformer to improve the per-\nformance (Chen et al., 2018; Wu et al., 2019b;\nSukhbaatar et al., 2019; Wang et al., 2019). Among\nthem, Wu et al. (2019b) introduced a convolution-\nbased module to replace the attention; Wang et al.\n(2019) proposed to train deep Transformers by\npropagating multiple layers together in the encoder.\nZhang et al. (2018) and Kim et al. (2019) also\nproposed AAN and SSRU to replace the attention\nmechanism. HAT is orthogonal to them and can\nbe combined to search for efﬁcient architecture\nwith those new modules. Another trend is to ap-\nply non- or partially-autoregressive models to cut\ndown the iteration number for decoding (Gu et al.,\n2019; Akoury et al., 2019; Wei et al., 2019; Gu\net al., 2018). Although reducing latency, they some-\ntimes suffer from low performance. Bapna et al.\n(2018) explored using learned linear combinations\nof encoder outputs as decoder inputs, while HAT\nconcatenates the outputs without linear combina-\ntions, thus better preserving the low-level informa-\ntion. Wu et al. (2020) investigated mobile settings\nfor NLP tasks and proposed a multi-branch Lite\nTransformer. However, it relied on FLOPs for efﬁ-\ncient model design, which is an inaccurate proxy\nfor hardware latency (Figure 2). There are also\nworks (Kim and Rush, 2016; Junczys-Dowmunt\net al., 2018; Kim et al., 2019; Yan et al., 2020) us-\ning Knowledge Distillation (KD) to obtain small\nstudent models. Our method is orthogonal to KD\nand can be combined with it to improve the efﬁ-\nciency further. There are also hardware acceler-\nators (Ham et al., 2020; Zhang et al., 2020) for\nattention and fully-connected layers in the Trans-\nformer to achieve efﬁcient processing.\nNeural Architecture Search. In the computer\nvision community, there has been an increasing\ninterest in automating efﬁcient model design with\nNeural Architecture Search (NAS) (Zoph and Le,\n2017; Zoph et al., 2018; Pham et al., 2018; He\net al., 2018). Some applied black-box optimization\nsuch as evolutionary search (Wang et al., 2020b)\nand reinforcement learning (Cai et al., 2019b; He\net al., 2018; Wang et al., 2018, 2020a; Mao et al.,\n2019); Some leveraged backpropagation with dif-\nferentiable architecture search (Liu et al., 2019).\nSome also involved hardware constraints into op-\ntimizations such as MNasNet (Tan et al., 2019),\nProxylessNAS (Cai et al., 2019b), FBNet (Wu et al.,\n2019a) and APQ (Wang et al., 2020b). To reduce\nthe NAS cost, supernet based methods (Pham et al.,\n2018; Bender et al., 2018; Guo et al., 2019) apply\na proxy for sub-network performance and adopt\nsearch algorithms to ﬁnd good sub-networks. For\nNLP tasks, the beneﬁts of the architecture search\nhave not been fully investigated. Recently, So et al.\n(2019) proposed the Evolved Transformer to search\nfor architectures under model size constraints and\nsurpassed the original Transformer baselines. How-\never, it suffered from very high search costs (250\nGPU years), making it unaffordable to search spe-\ncialized models for various hardware and tasks. In\naddition, hardware latency feedback was not taken\ninto account for better case-by-case specializations.\nSince different hardware has distinct architecture\nand features (Cong et al., 2018), feedback from\nhardware is critical for efﬁcient NLP.\n6 Conclusion\nWe propose Hardware-Aware Transformers (HAT)\nframework to solve the challenge of efﬁcient de-\nployments of Transformer models on various hard-\nware platforms. We conduct hardware-aware neu-\nral architecture search in an ample design space\nwith an efﬁcient weight-shared SuperTransformer,\nconsuming four orders of magnitude less cost than\nthe prior Evolved Transformer, and discover high-\nperformance low-latency models. We hope HAT\ncan open up an avenue towards efﬁcient Trans-\nformer deployments for real-world applications.\nAcknowledgment\nWe thank NSF Career Award #1943349, MIT-IBM\nWatson AI Lab, Semi-conductor Research Corpo-\nration (SRC), Intel, and Facebook for supporting\nthis research.\n7684\nReferences\nNader Akoury, Kalpesh Krishna, and Mohit Iyyer.\n2019. Syntactically supervised transformers for\nfaster neural machine translation. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1269–1281, Florence,\nItaly. Association for Computational Linguistics.\nAnkur Bapna, Mia Chen, Orhan Firat, Yuan Cao,\nand Yonghui Wu. 2018. Training deeper neural\nmachine translation models with transparent atten-\ntion. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3028–3033, Brussels, Belgium. Association\nfor Computational Linguistics.\nGabriel Bender, Pieter-Jan Kindermans, Barret Zoph,\nVijay Vasudevan, and Quoc Le. 2018. Understand-\ning and simplifying one-shot architecture search. In\nProceedings of the 35th International Conference\non Machine Learning, volume 80 of Proceedings of\nMachine Learning Research, pages 550–559, Stock-\nholmsmssan, Stockholm Sweden. PMLR.\nHan Cai, Chuang Gan, and Song Han. 2019a. Once for\nall: Train one network and specialize it for efﬁcient\ndeployment. arXiv preprint arXiv:1908.09791.\nHan Cai, Ligeng Zhu, and Song Han. 2019b. Proxy-\nlessNAS: Direct neural architecture search on target\ntask and hardware. In International Conference on\nLearning Representations.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Mike Schuster, Noam Shazeer, Niki Parmar,\nAshish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,\nZhifeng Chen, Yonghui Wu, and Macduff Hughes.\n2018. The best of both worlds: Combining recent\nadvances in neural machine translation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 76–86, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nJason Cong, Zhenman Fang, Michael Lo, Hanrui Wang,\nJingxian Xu, and Shaochong Zhang. 2018. Un-\nderstanding performance differences of fpgas and\ngpus. In 2018 IEEE 26th Annual International Sym-\nposium on Field-Programmable Custom Computing\nMachines (FCCM), pages 93–96. IEEE.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine\nLearning Research, pages 1243–1252, International\nConvention Centre, Sydney, Australia. PMLR.\n´Edouard Grave, Armand Joulin, Moustapha Ciss ´e,\nDavid Grangier, and Herv ´e J ´egou. 2017. Efﬁcient\nsoftmax approximation for GPUs. In Proceedings\nof the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine\nLearning Research, pages 1302–1310, International\nConvention Centre, Sydney, Australia. PMLR.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O.K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In Inter-\nnational Conference on Learning Representations.\nJiatao Gu, Changhan Wang, and Jake Zhao. 2019. Lev-\nenshtein Transformer. arXiv.\nZichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng,\nZechun Liu, Yichen Wei, and Jian Sun. 2019. Sin-\ngle path one-shot neural architecture search with uni-\nform sampling.\nTae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H\nOh, Yeonhong Park, Yoonho Song, Jung-Hun Park,\nSanghee Lee, Kyoung Park, Jae W Lee, et al. 2020.\nAˆ 3: Accelerating attention mechanisms in neural\nnetworks with approximation. In 2020 IEEE In-\nternational Symposium on High Performance Com-\nputer Architecture (HPCA), pages 328–341. IEEE.\nYihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li,\nand Song Han. 2018. Amc: Automl for model com-\npression and acceleration on mobile devices. In Pro-\nceedings of the European Conference on Computer\nVision (ECCV), pages 784–800.\nAndrew G. Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand,\nMarco Andreetto, and Hartwig Adam. 2017. Mo-\nbilenets: Efﬁcient convolutional neural networks for\nmobile vision applications.\nMarcin Junczys-Dowmunt. 2018. Microsoft’s submis-\nsion to the wmt2018 news translation task: How i\nlearned to stop worrying and love the data. arXiv\npreprint arXiv:1809.00196.\nMarcin Junczys-Dowmunt. 2019. Microsoft transla-\ntor at wmt 2019: Towards large-scale document-\nlevel neural machine translation. arXiv preprint\narXiv:1907.06170.\nMarcin Junczys-Dowmunt, Kenneth Heaﬁeld, Hieu\nHoang, Roman Grundkiewicz, and Anthony Aue.\n2018. Marian: Cost-effective high-quality neural\nmachine translation in C++. In Proceedings of the\n2nd Workshop on Neural Machine Translation and\nGeneration, pages 129–135, Melbourne, Australia.\nAssociation for Computational Linguistics.\nYoon Kim. 2014. Convolutional neural net-\nworks for sentence classiﬁcation. arXiv preprint\narXiv:1408.5882.\nYoon Kim and Alexander M Rush. 2016. Sequence-\nlevel knowledge distillation. arXiv preprint\narXiv:1606.07947.\nYoung Jin Kim, Marcin Junczys-Dowmunt, Hany Has-\nsan, Alham Fikri Aji, Kenneth Heaﬁeld, Roman\nGrundkiewicz, and Nikolay Bogoychev. 2019. From\nresearch to production and back: Ludicrously fast\n7685\nneural machine translation. In Proceedings of the\n3rd Workshop on Neural Generation and Transla-\ntion, pages 280–288, Hong Kong. Association for\nComputational Linguistics.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nMethod for Stochastic Optimization. In Interna-\ntional Conference on Learning Representations.\nHanxiao Liu, Karen Simonyan, and Yiming Yang.\n2019. DARTS: Differentiable architecture search.\nIn International Conference on Learning Represen-\ntations.\nIlya Loshchilov and Frank Hutter. 2017. SGDR:\nStochastic Gradient Descent with Warm Restarts. In\nInternational Conference on Learning Representa-\ntions.\nHongzi Mao, Parimarjan Negi, Akshay Narayan, Han-\nrui Wang, Jiacheng Yang, Haonan Wang, Ryan Mar-\ncus, Mehrdad Khani Shirkoohi, Songtao He, Vikram\nNathan, et al. 2019. Park: An open platform for\nlearning-augmented computer systems. In Advances\nin Neural Information Processing Systems , pages\n2490–2502.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\nMichael Auli, and Sergey Edunov. 2019. Facebook\nFAIR’s WMT19 news translation task submission.\nIn Proceedings of the Fourth Conference on Ma-\nchine Translation (Volume 2: Shared Task Papers,\nDay 1), pages 314–319, Florence, Italy. Association\nfor Computational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nHieu Pham, Melody Guan, Barret Zoph, Quoc Le, and\nJeff Dean. 2018. Efﬁcient neural architecture search\nvia parameters sharing. In Proceedings of the 35th\nInternational Conference on Machine Learning, vol-\nume 80 of Proceedings of Machine Learning Re-\nsearch, pages 4095–4104, Stockholmsmssan, Stock-\nholm Sweden. PMLR.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. arXiv preprint arXiv:1804.08771.\nDavid So, Quoc Le, and Chen Liang. 2019. The\nevolved transformer. In Proceedings of the 36th In-\nternational Conference on Machine Learning , vol-\nume 97 of Proceedings of Machine Learning Re-\nsearch, pages 5877–5886, Long Beach, California,\nUSA. PMLR.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive at-\ntention span in transformers. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 331–335, Florence, Italy.\nAssociation for Computational Linguistics.\nMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasude-\nvan, Mark Sandler, Andrew Howard, and Quoc V .\nLe. 2019. Mnasnet: Platform-aware neural architec-\nture search for mobile. In The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Conference on Neural Information Pro-\ncessing Systems.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nHanrui Wang, Kuan Wang, Jiacheng Yang, Linxiao\nShen, Nan Sun, Hae-Seung Lee, and Song Han.\n2020a. Gcn-rl circuit designer: Transferable transis-\ntor sizing with graph neural networks and reinforce-\nment learning. In ACM/IEEE 57th Design Automa-\ntion Conference (DAC).\nHanrui Wang, Jiacheng Yang, Hae-Seung Lee, and\nSong Han. 2018. Learning to design circuits. In\nNeurIPS 2018 Machine Learning for Systems Work-\nshop.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1810–1822, Florence, Italy. Associa-\ntion for Computational Linguistics.\nTianzhe Wang, Kuan Wang, Han Cai, Ji Lin, Zhijian\nLiu, Hanrui Wang, Yujun Lin, and Song Han. 2020b.\nApq: Joint search for network architecture, pruning\nand quantization policy. In Conference on Computer\nVision and Pattern Recognition.\nBingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang\nLin, and Xu Sun. 2019. Imitation learning for non-\nautoregressive neural machine translation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1304–\n1312, Florence, Italy. Association for Computational\nLinguistics.\n7686\nBichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan\nWang, Fei Sun, Yiming Wu, Yuandong Tian, Peter\nVajda, Yangqing Jia, and Kurt Keutzer. 2019a. Fb-\nnet: Hardware-aware efﬁcient convnet design via\ndifferentiable neural architecture search. In The\nIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR).\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2019b. Pay less attention with\nlightweight and dynamic convolutions. In Interna-\ntional Conference on Learning Representations.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. 2020. Lite transformer with long-short range\nattention. In International Conference on Learning\nRepresentations.\nZhongxia Yan, Hanrui Wang, Demi Guo, and Song\nHan. 2020. Micronet for efﬁcient language model-\ning. Journal of Machine Learning Research.\nBiao Zhang, Deyi Xiong, and Jinsong Su. 2018. Accel-\nerating neural transformer via an average attention\nnetwork. arXiv preprint arXiv:1805.00631.\nZhekai Zhang, Hanrui Wang, Song Han, and William J\nDally. 2020. Sparch: Efﬁcient architecture for\nsparse matrix multiplication. In 2020 IEEE Interna-\ntional Symposium on High Performance Computer\nArchitecture (HPCA), pages 261–274. IEEE.\nBarret Zoph and Quoc V Le. 2017. Neural Architec-\nture Search with Reinforcement Learning. In Inter-\nnational Conference on Learning Representations.\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and\nQuoc V Le. 2018. Learning Transferable Architec-\ntures for Scalable Image Recognition. In The IEEE\nConference on Computer Vision and Pattern Recog-\nnition (CVPR).\nTable 1\nValidation Loss From-Scratch \nBLEU\n4.011039 28.4\n4.011109 28.3\n4.011112 28.3\n4.028058 28.2\n4.028403 28.2\n4.045922 28.1\n4.072190 28.1\n4.072542 28.2\n4.075409 28.1\n4.097665 27.9\n4.165567 27.9\n4.165906 28.0\n4.172398 27.8\n4.182698 27.8\n4.405200 27.4\n4.449993 27.4\n4.468512 26.9\n4.704716 25.8\n4.705631 25.8\n4.717223 25.8\n4.40 27.6\n25\n26\n27\n28\n29\n3.90 4.13 4.35 4.58 4.80\nTable 1-1\nValidation Loss From-Scratch \nBLEU\n3.448210 41.8\n3.454411 41.5\n3.454629 41.7\n3.455616 41.4\n3.459664 41.4\n3.477138 41.1\n3.478086 41.1\n3.483934 41.1\n3.487714 41.0\n3.533458 40.7\n3.533737 40.7\n3.547844 40.6\n3.549524 40.6\n3.707655 40.0\n3.707659 40.1\n3.708909 40.1\n3.749576 39.7\n3.916193 39.1\n3.924879 38.8\n3.926471 38.7\n38\n39\n40\n41\n42\n3.30 3.48 3.65 3.83 4.00\nWMT ’14 En-Fr\nWMT ’14 En-De\nInherited SubTransformer \nValidation Loss (Proxy)\nSubTransformer trained \nfrom-scratch BLEU (Target)\nInherited SubTransformer \nValidation Loss (Proxy)\nThe larger the inherited val \nloss, the lower the trained \nfrom-scratch BLEU.\nThe larger the inherited val \nloss, the lower the trained \nfrom-scratch BLEU.\nSubTransformer trained \nfrom-scratch BLEU (Target)\nFigure 11: The validation loss of SubTransformers is\na good performance proxy for BLEU of from-scratch\ntrained SubTransformers. The larger the validation\nloss, the lower the BLEU score.\nA Appendix for “HAT: H ardware-Aware\nTransformers for Efﬁcient Natural\nLanguage Processing”\nA.1 SubTransformer Performance Proxy\nIn Figure 11, we show the relationship between\nthe validation loss of SubTransformers directly in-\nherited from the SuperTransformer, and the BLEU\nscore of the SubTransformers trained from-scratch.\nWe can observe that the larger the validation loss,\nthe lower the BLEU score. Therefore the validation\nloss can be a good performance proxy.\nA.2 Visualizations of Searched Models on\nWMT’14 En-De Task\nWe show the HAT models searched for Raspberry\nPi ARM Cortex-A72 CPU and Nvidia TITAN Xp\nGPU in Figure 12. The searched model for Rasp-\nberry Pi is deep and thin, while that for GPU is\nshallow and wide. The BLEU scores of the two\nmodels are similar: 28.10 for Raspberry Pi CPU,\nand 28.15 for Nvidia GPU.\n7687\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\nDecoder Layer 1\nEncoder Layer 1\n512 Embed Dim\nEncoder Layer 2 Encoder Layer 3 Encoder Layer 4 Encoder Layer 5 Encoder Layer 6\nDecoder Layer 2 Decoder Layer 3 Decoder Layer 4\n2048 Hidden \nDim FFN\n2048 Hidden \nDim FFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n512 Embed Dim\nConcat\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\nDecoder Layer 1\nEncoder Layer 1\n640 Embed Dim\nEncoder Layer 2 Encoder Layer 3 Encoder Layer 4 Encoder Layer 5 Encoder Layer 6\nDecoder Layer 2 Decoder Layer 3\n2048 Hidden \nDim FFN\n3072 Hidden Dim \nFFN\n3072 Hidden Dim \nFFN\n640 Embed Dim\nConcat\nInput Input\nOutput\nOutput\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n4 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nSelf Attention\n8 Heads \nEn-Decoder \nAttention\n8 Heads \nEn-Decoder \nAttention\n8 Heads \nEn-Decoder \nAttention\n4 Heads \nEn-Decoder \nAttention\n8 Heads \nEn-Decoder \nAttention\n8 Heads \nEn-Decoder \nAttention\n4 Heads \nEn-Decoder \nAttention\nSubTransformer Optimized for Nvidia TITAN Xp GPUSubTransformer Optimized for Raspberry Pi \nARM Cortex-A72 CPU\nFigure 12: SubTransformers optimized for Raspberry Pi ARM CPU and Nvidia GPU on WMT’14 En-De task are\ndifferent. The CPU model has BLEU 28.10, and GPU model has BLEU 28.15.\nA.3 Latency, BLEU and SacreBLEU of\nsearched HAT models.\nIn Table 8, we show the speciﬁc latency numbers,\nBLEU and SacreBLEU (Post, 2018) scores for\nsearched HAT models in Figure 7 and Figure 8.\n7688\nTask Hardware Latency BLEU SacreBLEU\nWMT’14\nEn-De\nRaspberry Pi\nARM Cortex-A72\nCPU\n3.5s 25.8 25.6\n4.0s 26.9 26.6\n4.5s 27.6 27.1\n5.0s 27.8 27.2\n6.0s 28.2 27.6\n6.9s 28.4 27.8\nIntel\nXeon E5-2640\nCPU\n137.9ms 25.8 25.6\n204.2ms 27.6 27.1\n278.7ms 27.9 27.3\n340.2ms 28.1 27.5\n369.6ms 28.2 27.6\n450.9ms 28.5 27.9\nNvidia\nTITAN Xp\nGPU\n57.1ms 25.8 25.6\n91.2ms 27.6 27.1\n126.0ms 27.9 27.3\n146.7ms 28.1 27.5\n208.1ms 28.5 27.8\nWMT’14\nEn-Fr\nRaspberry Pi\nARM Cortex-A72\nCPU\n4.3s 38.8 36.0\n5.3s 40.1 37.3\n5.8s 40.6 37.8\n6.9s 41.1 38.3\n7.8s 41.4 38.5\n9.1s 41.8 38.9\nIntel\nXeon E5-2640\nCPU\n154.7ms 39.1 36.3\n208.8ms 40.0 37.2\n329.4ms 41.1 38.2\n394.5ms 41.4 38.5\n442.0ms 41.7 38.8\nNvidia\nTITAN Xp\nGPU\n69.3ms 39.1 36.3\n94.9ms 40.0 37.2\n132.9ms 40.7 37.8\n168.3ms 41.1 38.3\n208.3ms 41.7 38.8\nIWSLT’14\nDe-En\nNvidia\nTITAN Xp\nGPU\n45.6ms 33.4 32.5\n74.5ms 34.2 33.3\n109.0ms 34.5 33.6\n137.8ms 34.7 33.8\n168.8ms 34.8 33.9\nWMT’19\nEn-De\nNvidia\nTITAN Xp\nGPU\n55.7ms 42.4 41.9\n93.2ms 44.4 43.9\n134.5ms 45.4 44.7\n176.1ms 46.2 45.6\n204.5ms 46.5 45.7\n237.8ms 46.7 46.0\nTable 8: Speciﬁc latency numbers, BLEU and Sacre-\nBLEU scores for searched HAT models in Figure 7 and\nFigure 8.",
  "topic": "Speedup",
  "concepts": [
    {
      "name": "Speedup",
      "score": 0.8250565528869629
    },
    {
      "name": "Computer science",
      "score": 0.8101552724838257
    },
    {
      "name": "Machine translation",
      "score": 0.5211820006370544
    },
    {
      "name": "Encoder",
      "score": 0.4933587610721588
    },
    {
      "name": "Computer hardware",
      "score": 0.49108409881591797
    },
    {
      "name": "Hardware acceleration",
      "score": 0.4496420919895172
    },
    {
      "name": "Transformer",
      "score": 0.4266892373561859
    },
    {
      "name": "Hardware architecture",
      "score": 0.41669464111328125
    },
    {
      "name": "Computation",
      "score": 0.4145805537700653
    },
    {
      "name": "Embedded system",
      "score": 0.38492026925086975
    },
    {
      "name": "Computer architecture",
      "score": 0.38257861137390137
    },
    {
      "name": "Field-programmable gate array",
      "score": 0.36420896649360657
    },
    {
      "name": "Parallel computing",
      "score": 0.29150649905204773
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2083471715450287
    },
    {
      "name": "Operating system",
      "score": 0.17819654941558838
    },
    {
      "name": "Programming language",
      "score": 0.15428775548934937
    },
    {
      "name": "Software",
      "score": 0.12323448061943054
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1341412227",
      "name": "IBM (United States)",
      "country": "US"
    }
  ],
  "cited_by": 206
}