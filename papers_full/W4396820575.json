{
  "title": "How secure is AI-generated code: a large-scale comparison of large language models",
  "url": "https://openalex.org/W4396820575",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4378495741",
      "name": "Tihanyi, Norbert",
      "affiliations": [
        "Eötvös Loránd University",
        "Technology Innovation Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4381031308",
      "name": "Bisztray, Tamas",
      "affiliations": [
        "University of Oslo"
      ]
    },
    {
      "id": "https://openalex.org/A2956101626",
      "name": "Ferrag Mohamed Amine",
      "affiliations": [
        "University of Guelma"
      ]
    },
    {
      "id": "https://openalex.org/A4378495742",
      "name": "Jain, Ridhi",
      "affiliations": [
        "Technology Innovation Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4226180864",
      "name": "Cordeiro, Lucas C.",
      "affiliations": [
        "Universidade Federal do Amazonas",
        "University of Manchester"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3169915924",
    "https://openalex.org/W4366449580",
    "https://openalex.org/W2802300457",
    "https://openalex.org/W6603385254",
    "https://openalex.org/W3166095789",
    "https://openalex.org/W6614030999",
    "https://openalex.org/W4378473878",
    "https://openalex.org/W4324370640",
    "https://openalex.org/W4387298393",
    "https://openalex.org/W6600050674",
    "https://openalex.org/W2913717796",
    "https://openalex.org/W2949558051",
    "https://openalex.org/W2966672852",
    "https://openalex.org/W2162436321",
    "https://openalex.org/W2031525781",
    "https://openalex.org/W4384345745",
    "https://openalex.org/W3091588759",
    "https://openalex.org/W2888321432",
    "https://openalex.org/W2929483929",
    "https://openalex.org/W2963762601",
    "https://openalex.org/W2179309410",
    "https://openalex.org/W4394769152",
    "https://openalex.org/W6601102799",
    "https://openalex.org/W4376122476",
    "https://openalex.org/W6601399257",
    "https://openalex.org/W6604738668",
    "https://openalex.org/W4402665833",
    "https://openalex.org/W6603612406",
    "https://openalex.org/W6601851198",
    "https://openalex.org/W4385572162",
    "https://openalex.org/W6845729170",
    "https://openalex.org/W4321354332",
    "https://openalex.org/W4284676027",
    "https://openalex.org/W4389158474",
    "https://openalex.org/W2025411198",
    "https://openalex.org/W4391307510",
    "https://openalex.org/W4392564384",
    "https://openalex.org/W327452528",
    "https://openalex.org/W4312669928",
    "https://openalex.org/W6603984088",
    "https://openalex.org/W6600961993",
    "https://openalex.org/W6602680078",
    "https://openalex.org/W4377866432",
    "https://openalex.org/W1964962870",
    "https://openalex.org/W4393942893",
    "https://openalex.org/W4285490434",
    "https://openalex.org/W2098035443",
    "https://openalex.org/W6607896898",
    "https://openalex.org/W4288057765",
    "https://openalex.org/W4385187279",
    "https://openalex.org/W4391558545",
    "https://openalex.org/W4388858772",
    "https://openalex.org/W4321013654",
    "https://openalex.org/W2962960733",
    "https://openalex.org/W2067705817",
    "https://openalex.org/W4408047557",
    "https://openalex.org/W4391558363",
    "https://openalex.org/W4386362878",
    "https://openalex.org/W6600547436",
    "https://openalex.org/W4318562265",
    "https://openalex.org/W7035904466",
    "https://openalex.org/W4311165836",
    "https://openalex.org/W3122473522",
    "https://openalex.org/W4221164274",
    "https://openalex.org/W4406458640",
    "https://openalex.org/W6907179428",
    "https://openalex.org/W4389215044",
    "https://openalex.org/W4402340189",
    "https://openalex.org/W2006849174",
    "https://openalex.org/W4391974543",
    "https://openalex.org/W4402670831",
    "https://openalex.org/W6600109629",
    "https://openalex.org/W4389162178",
    "https://openalex.org/W4386436496",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W2511803001",
    "https://openalex.org/W4385500670",
    "https://openalex.org/W4378942602",
    "https://openalex.org/W6601211009",
    "https://openalex.org/W6630224890",
    "https://openalex.org/W4402457546",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4391579639",
    "https://openalex.org/W4390956105",
    "https://openalex.org/W4388482966",
    "https://openalex.org/W6600195515",
    "https://openalex.org/W4398239389",
    "https://openalex.org/W2899171197"
  ],
  "abstract": null,
  "full_text": "How secure is AI-generated Code: A Large-Scale\nComparison of Large Language Models\nNorbert Tihanyi1,2, Tamas Bisztray3, Mohamed Amine Ferrag4,\nRidhi Jain2, Lucas C. Cordeiro5,6\n1 Eötvös Loránd University (ELTE), Budapest, Hungary.\n2 Technology Innovation Institute (TII), Abu Dhabi, UAE.\n3 University of Oslo, Oslo, Norway.\n4Guelma University, Guelma, Algeria.\n5The University of Manchester, Manchester, UK.\n6Federal University of Amazonas, Manaus, Brazil.\nAbstract\nThis study compares state-of-the-art Large Language Models (LLMs) on their\ntendency to generate vulnerabilities when writing C programs using a neutral\nzero-shot prompt. Tihanyi et al. introduced the FormAI dataset at PROMISE\n’23, featuring 112,000C programs generated by GPT-3.5-turbo, with over 51.24%\nidentified as vulnerable. We extended that research with a large-scale study\ninvolving 9 state-of-the-art models such as OpenAI’s GPT-4o-mini, Google’s\nGemini Pro 1.0, TII’s 180 billion-parameter Falcon, Meta’s 13 billion-parameter\nCode Llama, and several other compact models. Additionally, we introduce the\nFormAI-v2 dataset, which comprises 331000 compilable C programs generated\nby these LLMs. Each program in the dataset is labeled based on the vulnerabil-\nities detected in its source code through formal verification, using the Efficient\nSMT-based Context-Bounded Model Checker (ESBMC). This technique mini-\nmizes false positives by providing a counterexample for the specific vulnerability\nand reduces false negatives by thoroughly completing the verification process.\nOur study reveals that at least 62.07% of the generated programs are vulnerable.\nThe differences between the models are minor, as they all show similar cod-\ning errors with slight variations. Our research highlights that while LLMs offer\npromising capabilities for code generation, deploying their output in a produc-\ntion environment requires proper risk assessment and validation.Please cite\nthis once published: https://doi.org/10.1007/s10664-024-10590-1.\nKeywords: Large Language Models, Vulnerability Classification, Formal Verification,\nSoftware Security, Artificial Intelligence, Dataset.\n1\narXiv:2404.18353v2  [cs.CR]  11 Dec 2024\n1 Introduction\nLarge Language Models (LLMs) are transforming software development and program-\nming [1–3]. Every day, developers and computer scientists utilize various code creation\nand completion models to tackle different tasks [4, 5]. Research related to program\nsynthesis using Generative Pre-trained Transformers (GPT) [6] is gaining significant\ntraction, where initial studies indicate that GPT models can generate syntactically\ncorrect yet vulnerable code [7].\nA study conducted at Stanford University suggests that software engineers assisted\nby OpenAI’s codex-davinci-002 modelwere at a higher risk of introducing security\nflaws into their code [8]. As the usage of AI-based tools in coding continues to expand,\nunderstanding their potential to introduce software vulnerabilities becomes increas-\ningly important. Given that LLMs are trained on data freely available on the internet,\nincluding potentially vulnerable code, there is a high risk that AI tools could replicate\nthe same patterns. This raises a critical question:Is it safe to employ these models in\nreal projects?\nAs a first step towards answering this question, Tihanyi et al. published the For-\nmAI dataset [9] at the 19th International Conference on Predictive Models and Data\nAnalytics in Software Engineering (PROMISE’23). This dataset is the first and largest\ncollection of AI-generated compilable C programs with vulnerability classification, fea-\nturing 112 000 samples. To guarantee the diversity of generated C codes, the authors\ndeveloped a framework designed to produce a variety of programs that cover multiple\ncoding scenarios, efficiently facilitating real-world bugs. The study employed Bounded\nModel Checking (BMC), a technique within Formal Verification (FV), to evaluate the\nsecurity properties of the dataset. This initial study revealed that at least51.24% of\nthe C programs generated by GPT-3.5-turbo were vulnerable.\nContinuing the original research presented in [9], we aim to expand the scope of\nthe study by addressing the key limitations highlighted by the research community.\nWe identified four main limitations that we intend to address from the original paper:\n1. The first paper exclusively focuses on OpenAI’s GPT-3.5-turbo, without evaluating\nother models. To bridge this gap, this paper compares nine state-of-the-art LLMs\nin secure coding, such as Google’sGemini Pro 1.0 [10], OpenAI’s GPT-4o-mini,\nTII’s Falcon-180B [11], and Meta’s Code LLama 13B [12]. In addition, we have\nexpanded the original dataset from112 000to331 000samples, where incorporating\nC code generated by different LLMs also enhances diversity.\n2. The initial findings on the percentage of vulnerable samples in the dataset (51.24%)\nmay have been under-reported due to the limitations of bounded model check-\ning, indicating that the actual percentage of vulnerabilities could be higher. To\naddress this issue, we transitioned our verification approach from bounded to\nunbounded verification, thereby enhancing the depth and accuracy of our security\nevaluation [13–16].\n3. We have incorporated new labels into the dataset to enhance its usability for a\nbroader research community. While all necessary features can be extracted and\nreproduced directly from the provided source codes, we have enhanced the dataset’s\ncomprehensiveness by calculating the cyclomatic complexity (CC) [17] for each\n2\nprogram, adding source lines of code (SLOC), including the exact stack trace for\ncounterexamples, and providing a code snippet entry that captures only the 5\nlines before and after the vulnerability. These additional features are valuable for\nmachine learning tasks to help models generalize the problem and identify vulnera-\nbilities more effectively and conduct more detailed comparisons in various research\ncontexts.\n4. To enhance the dataset, we have removed all Type 1, Type 2, Type 3-1, and Type\n3-2 (with 10% deviation threshold) clones using the NiCad (Automated Detection\nof Near-Miss Intentional Clones) [18] tool. We note, that removing Type 3-2 clones\nwith a larger threshold is not our goal. Even minor changes can be significant\nand determine whether a vulnerability is present or absent, potentially introducing\ndifferent security risks. Moreover, different representations of a vulnerability can\nhelp models better generalize during machine learning training.\nThis study answers the following research questions:\n• RQ1: How does the security of LLM-generated code differ across various\nmodels?\n• RQ2: What are the most typical vulnerabilities introduced during C code\ngeneration by different LLMs using neutral zero-shot prompts?\n1.1 Main contribution\nTo summarize, this paper holds the following original contributions:\n• We present theFormAI-v2 dataset, consisting of 331 000 compilable C programs\n(310 531 with the exclusion of any Type 1, Type 2, Type 3-1 and Type 3-2 clones)\ngenerated by nine different LLMs. Each C sample has been systematically labeled\nbased on vulnerabilities identified through formal verification methods, particularly\nusing the Efficient SMT-based Bounded Model Checker (ESBMC) [14–16] tool with\nan unbounded setting;\n• A detailed study to determine which models produce code with the highest and the\nlowest number of vulnerabilities;\n• We provide a comprehensive analysis of the generated programs, detailing the\ndistribution of vulnerabilities and highlighting the most frequently encountered\ntypes;\n• We made theFormAI-v2 dataset available to the research community, including all\ngenerated C samples and classification results. The dataset can be accessed on our\nproject website at https://github.com/FormAI-Dataset.\nThe remaining sections are organized as follows: Section 2 provides an in-depth\ndiscussion on the motivation. Section 3 presents a comprehensive overview of the lit-\nerature related, highlighting significant previous studies and their findings. Section 4\nintroduces the concepts of formal verification, focusing on the ESBMC module.\nSection 5 details the methodology we adopted to develop and label our dataset.\nSection 6 presents our findings and discusses their implications. Section 7 explores the\n3\nlimitations and threats to the validity, and proposes potential future research direc-\ntions. Finally, Section 8 concludes the paper by summarising our contributions and\naddressing the research questions posed in this study.\n2 Motivation\nIn program synthesis, LLMs are generally used for simple tasks like writing a prime\nnumber generator or a basic program to sort an array, rather than handling large-\nscale projects involving thousands of lines of code [8]. The latest generation of LLMs\ncan easily solve these simple tasks without facing any challenges. So far, the main\narea of interest in LLM-based code generation has been correctness. Datasets such as\nHumanEval [19] provide programming challenges to assess the performance of models\nin correctly solving various problems. For example, GPT-4 achieves a67% success\nrate in solving tasks compared to48.1% for GPT-3.5-turbo [20]. On the contrary,\neven for basic programming tasks, state-of-the-art LLMs may adopt insecure coding\npractices. To illustrate the issue, imagine a situation where a programmer asks GPT-\n4 the following:“Create a C program that prompts the user to input two numbers and\nthen calculate their sum”. The code generated by GPT-4 is presented on the left in\nFigure 1, while the output from the formal verification tool ESBMC 7.6.1 is shown\non the right.\nFig. 1: Motivation example: GPT-4 produced code with security vulnerabilities,\ndemonstrated through formal verification results.\nThis simple code contains three potential security vulnerabilities. It exhibits an integer\noverflow during the addition of the variablesnumber1 and number2, as well as two\nbuffer overflows through thescanf() functions that retrieve input from the user. In\n32-bit computing architectures, integers are commonly stored as4 bytes (32 bits),\nwhich results in a maximum integer value of2 147 483 647, equivalent to231 −1. If\none attempts to add2 147 483 647 + 1using this small program, the result will be\n4\nincorrect due to integer overflow. The incorrect result will be−2 147 483 648instead\nof the expected2 147 483 648. The addition exceeds the maximum representable value\nfor a signed 32-bit integer231 −1, causing the integer to wrap around and become\nnegative due to the two’s complement representation.\nWhen GPT-4 is requested to write a secure version of this code using the following\nprompt: “Create a C program that prompts the user to input two numbers and then\ncalculates their sum. Be careful and avoid security vulnerabilities.”, it only attempts\nto fix entering non-integer inputs by adding the following code snippet (Figure 2):\nFig. 2: GPT-4 generated code snippet response after requesting a secure version of\nthe code in Figure 1.\nEven after requesting a “secure” zero-shot prompt, all three original issues remain\nunresolved. Despite the significant advancements from GPT-3.5-turbo—which exhib-\nited the same issue [9]—to GPT-4, our motivational example indicates that GPT-4\ncontinues to produce code with vulnerabilities. Even if specifically requested in the\nprompt to avoidinteger overflow in the program, the issue persists (Figure 3).\nFig. 3: Zero-shot prompt requesting a fix for integer overflow (failing to do so).\nWe want to emphasize that simply requesting a secure version is not an effective\napproach towards achieving a secure code for the following reason: Code completion\ntools such as GitHub Copilot1 or Amazon Code Whisperer2 suggest code snippets\nbased on contextual analysis and training data, which has also been shown to produce\nvulnerable code [21]. In such scenarios, the ability to prompt is limited (it can be\n1https://github.com/features/copilot/\n2https://aws.amazon.com/codewhisperer/\n5\nattempted through comments in the code). In addition, GitHub Copilot is powered by\navariantoftheGPT(GenerativePre-trainedTransformer)modelcalledCodex,which\nOpenAI developed. The underlying issue will remain if these models are not trained to\nproduce secure code. Based on this observation, we aim to conduct a comprehensive\nstudy involving various state-of-the-art models to address our research questions.\n3 Related Work\nThis section overviews automated vulnerability detection and notable existing\ndatasets containing vulnerable code samples for various training and benchmarking\npurposes.\n3.1 LLMs in Software Engineering\nIn software engineering (SE), it is essential to ensure three main aspects of the code:\ncorrectness, safety, and security of the programs created. Functionally correct code\nshould yield the expected outcomes for each input it processes. Code safety means\nconstructing fail-safe systems, protecting against accidental or unexpected inputs that\nmight produce logically correct but undesirable results. Software security involves\nfortifying the software against external threats and deliberate attacks [22]. In a com-\nprehensive study, Anwar et al. [23] highlight important safety issues related to LLMs\nbeyond SE, from the disruptive socioeconomic impacts and cybersecurity risks to\nethical issues. Vassilka et al. [24] discuss the need for SE education to adapt to AI\nadvancements and prepare future software engineers to effectively and ethically utilize\nthese technologies in their careers.\nTo assess correctness, datasets such as HumanEval [19] serve as a benchmark\nto measure the problem-solving abilities of AI models for problems related to lan-\nguage comprehension, reasoning, algorithms, simple mathematics, coding, and logical\nthinking. There are several other similar datasets, such as MBPP [25] to assess code\nsynthesis capabilities on elementary Python challenges, or CodeXGLUE [26], to test\ncode completion, translation, and understanding of different LLMs.\nFrameworks and techniques for turning prompts into executable code for SE are\nrapidly emerging, but the main focus mostly often functional correctness, omitting\nimportant security aspects [27–29], or reliability [30–34]. There has been an arms\nrace between researchers to excel in correctness benchmarks using zero or few-shot\nframeworks [35, 36], multi-agent frameworks [37], fine-tuned models [38], and various\nother methods. As AI models evolve, their problem-solving capabilities improve sig-\nnificantly. However, whether these advancements also enhance the safety and security\nproperties of the code they generate remains largely unclear and under-researched.\nIn [39], Lin et al. assessed different software process models to evaluate how\nthese models affect code correctness (Pass@13). They also assessed the code quality\nof the AI-generated code by running static code checkers to uncover code smells4.\n3This metric highlights the model’s ability to produce correct and functional code on its first try without\nany revisions or corrections.\n4Code smells are patterns in code that hint at potential problems, making maintenance harder but not\nnecessarily causing immediate errors. They suggest areas where the code may need to be refactored for\nbetter quality and reliability.\n6\nThis work had an interesting finding: the proposed software process models improved\nthe quality of the generated code by significantly reducing code smells compared to\nwhat GPT-3.5-turbo outputs by itself. Code smells or bad coding practices will not\noutright introduce vulnerabilities. However, several small-scale studies point to the\nfact that LLMs negatively impact software development from a security perspective.\nIn [40], the authors generated21 small programs in five different languages: C, C++,\nPython, HTML, and Java. Combining manual verification with GPT-based vulner-\nability detection, the study found that only5 of the 21 generated programs were\ninitially secure.\nIn [41], Pearce et al. conclude that the control group utilized GitHub’s Copilot to\nsolve arithmetic operations accurately. This work highlights an important lesson: to\naccurately measure the role of AI tools in code generation or completion, it is essential\nto choose coding scenarios mirroring a diverse set of relevant real-world settings,\nthereby facilitating the occurrence of various vulnerabilities. This necessitates the\ncreation of code bases replicating a wide range of scenarios, which is one of the primary\ngoals the FormAI dataset strives to achieve. These studies indicate that AI tools, and\nin particular ChatGPT, can produce code containing vulnerabilities as of today.\nMa et al. [42] assessed the capabilities and limitations of ChatGPT for SE and\nprovided initial insights into why the programs generated by language models are\nsyntactically correct but potentially vulnerable. A study by Microsoft [43] found that\nGPTmodelsencounterdifficultieswhenaccuratelysolvingarithmeticoperations.This\naligns with our findings in Section 2.\nIn a comprehensive literature review, Hou et al. [44] examined LLMs’ application,\neffects, and possible limitations on SE. This study reveals that LLMs are extensively\nemployed across software development, appearing in229 papers for 24 distinct SE\ntasks, predominantly in code generation and program repair. It also identifies over70\nLLMs, classifying them into three architectural types: decoder-only, encoder-decoder,\nand encoder-only. Each architecture serves specific functions—encoder-only for in-\ndepth understanding, encoder-decoder for combined understanding and generation,\nand decoder-only primarily for generation tasks. This work highlights an interesting\ngap: there are dozens of research papers aiming to perform vulnerability detection\nin source code using machine learning (ML) and LLMs [45–57], however, assessing\nsoftware safety and security properties of LLM-generated code on a large-scale has\nnot yet been performed apart from our original work [9] for C, and recently by [58]\nfor PHP code. Both studies evaluated a single model in a zero-shot code generation\nscenario, while our current work also conducts a comparison of the performance of\ndifferent models.\nIn [59] Shumailov et al. highlighted a phenomenon known as“model collapse”.\nTheir research demonstrated that integrating content generated by LLMs can lead to\npersistent flaws in subsequent models when using the generated data for training. This\nhints that training ML algorithms only on purely AI-generated content is insufficient\nif one aims to prepare these models for detecting vulnerabilities in human-generated\ncode. This is essentially due to using a dataset during the training phase, which is\nnot diverse enough and misrepresents edge cases. This raises the question of whether\nthe FromAI dataset is suitable for fine-tuning and ML purposes. It is important to\n7\nnote that the AI-generated code is just one part of the dataset. Most importantly, the\nvulnerability labeling was not done by AI but by the ESBMC formal verification tool.\nThis way, models trained on this dataset can essentially learn the skills of a formal\nverification tool (or at least try to achieve the best optimal outcomes).\nThe programs are generated through a dynamic zero-shot prompting method, and\nthe generated programs are not modified by any AI system afterward. While the\nprimary goal of our paper is to investigate and compare the secure coding abilities\nof different LLMs, these conditions make the FormAI-v2 dataset suitable for ML\npurposes. On the other hand, AI models were trained on human-generated content;\nthus, the vulnerabilities produced have roots in incorrect code created by humans.\nYet, as discussed in the next section, existing datasets notoriously include synthetic\ndata(differentfromAI-generated),whichcanbeusefulforbenchmarkingvulnerability\nscanners but has questionable value for training purposes [60].\n3.2 Existing Databases for Vulnerable C Code\nWe show how theFormAI-v2 dataset compares to seven widely studied datasets con-\ntaining vulnerable code and the previous version of the dataset published in [9].\nThe examined datasets are: Big-Vul [61], Draper [62, 63], SARD [64], Juliet [65],\nDevign [66], REVEAL [67], DiverseVul [60], and FormAI-v1 [9]. Table 1 presents a\ncomprehensive comparison of the datasets across various metrics. Some of this data\nis derived from review papers that evaluate these datasets [60, 68].\nTable 1: Comparison of Various C Code Datasets\nSpecs\nDataset Big-Vul Draper SARD Juliet Devign REVEAL Diverse\nVul FormAI FormAI-v2\nLanguage C/C++ C/C++ Multi Multi C C/C++ C/C++ C C\nSource RW Syn +\nRW\nSyn +\nRW Syn RW RW RW AI AI\nDataset\nsize 189k 1274k 101k 106k 28k 23k 379k 112k 331k\nVul.\nSnippets 100% 5.62% 100% 100% 46.05% 9.85% 7.02% 51.24% 62.07%\nMulti.\nVulns. x ✓ x x x x x ✓ ✓\nCompilable x x x ✓ x x x ✓ ✓\nGranularity Func Func Prog Prog Func Func Func Prog Prog\nClass.\nType\nCVE\nCWE CWE CWE CWE CVE CVE CWE CWE CWE\nAvg. LOC. 30 29 114 125 112 32 44 79 86\nLabelling\nMethod P S B/S/M B M P P FV FV\nLegend:\nMulti: Multi-Language Dataset,RW: Real World,Syn: Synthetic,AI: AI-generated,\nFunc: Function level granularity,Prog: Program level granularity,\nCVE: Common Vulnerabilities and Exposures,CWE: Common Weakness Enumeration,\nP: GitHub Commits Patching a Vulnerability,S: Static Analyzer,\nB: By Design Vulnerable,FV: Formal Verification with ESBMC,M: Manual Labeling\n8\nBig-Vul,Draper,Devign,REVEAL,andDiverseVulcomprisevulnerablereal-world\nfunctions from open-source applications. These five datasets do not include all the\nsamples’ dependencies; therefore, they are non-compilable. SARD and Juliet contain\nsynthetic, compilable programs. In their general composition, the programs contain a\nvulnerable function, its equivalent patched function, and a main function calling these\nfunctions. All datasets indicate whether a code is vulnerable, using various vulnerabil-\nity labeling methodologies such asP, where functions are considered vulnerable before\nreceiving GitHub commits that address detected vulnerabilities; M, which involves\nmanual labeling; S, which uses static analyzers; andB, designated as by design vul-\nnerable without the use of a vulnerability verification tool. It’s important to note that\nthe size of these datasets can be misleading, as many include samples from languages\nother than the one primarily studied. For example, SARD includes not only C and\nC++ but also Java, PHP, and C#. Moreover, newly released sets often incorporate\nprevious datasets or scrape the same GitHub repositories, making them redundant.\nFor example, Draper contains C and C++ code from the SATE IV Juliet Test\nSuite, Debian Linux distribution, and public Git repositories. Since the open-source\nfunctions from Debian and GitHub were not labeled, the authors used a suite of\nstatic analysis tools: CPPcheck [69] and Flawfinder [62]. However, the paper does\nnot mention if vulnerabilities were manually verified or if any confirmation has been\nperformed to root out false positives. In [60], on top of creating DiverseVul, Chen et al.\nmerged all datasets that were based on GitHub commits and removed duplicates, thus\nmaking the most comprehensive collection of GitHub commits containing vulnerable\nC and C++ code.\n3.3 Vulnerability Scanning and Repair\nSoftware verification is crucial for ensuring software’s safety and security properties.\nIt employs a variety of techniques, each with its strengths and limitations. These\ntechniques include manual verification, static analysis, dynamic analysis, formal veri-\nfication, and increasingly, machine learning-based approaches such as those involving\nLLMs [42, 70–73].\nManual verification involves human-driven processes such as code reviews and\nmanual testing. While these methods effectively catch complex errors that automated\ntools might miss, they are labor-intensive and not scalable to large codebases or\nfrequent updates. Static analysis evaluates source code without executing it, using\nstatic symbolic execution, data flow analysis, and control flow analysis. Style checking\nenforces coding standards for better readability and maintainability.\nThese methods collectively enhance software integrity. The drawbacks are that this\nmethod can miss vulnerabilities that manifest only during runtime interactions and\noften introduce false positive results. Dynamic analysis tests the software’s behavior\nduring execution [74]. It includes fuzzing, automated testing, run-time verification,\nand profiling. This technique requires executable code and often significant setup to\nsimulate different environments and may not cover all execution paths.\nFormalVerification(FV)usesmathematicalproofstoverifythecorrectnessofalgo-\nrithms against their specifications. It is the most rigorous form of software verification\n9\nand is used in applications where reliability is critical, such as aerospace and medi-\ncal devices. However, FV can be time-consuming and requires specialized knowledge,\nlimiting its widespread adoption [22]. Recent advancements include machine learning\ntechniques, particularly LLMs, in various aspects of software verification [75]. LLMs\ncan assist in automated code review by suggesting improvements, detecting vulnera-\nbilities, generating test cases, fixing bugs, and creating documentation. Despite their\npotential [76–82], LLMs, on their own face limitations such as a lack of understanding\nof code semantics and difficulty in handling highly domain-specific knowledge [83],\nand they depend heavily on the quality and variety of the training data. Using LLMs\nas part of a framework to complement other techniques is, however, a promising area\nof research [7, 9, 84, 85]. An earlier work from2022 examined the ability of various\nLLMs to fix vulnerabilities, where the models showed promising results, especially\nwhen combined. Still, the authors noted that such tools are not ready to be used\nin a program repair framework, where further research is necessary to incorporate\nbug localization. They further highlighted challenges in the tool’s ability to generate\nfunctionally correct code [86].\nWhile LLMs struggle with detection by themselves, in [7], the authors demon-\nstrated that GPT-3.5-turbo could efficiently fix errors if the output of the ESBMC\nverifier is provided. Program repair is another emerging area where the application of\nLLMs is showing real promise, where in addition to fine-tuning strategies, the com-\nbination of LLMs with other tools appears to be an effective method [47, 87–103].\nIn [104], the authors call for innovations to enhance automated vulnerability repair,\nparticularly for developing more extensive training datasets to optimize LLMs.\n4 Formal Verification (FV) and Bounded Model\nChecking (BMC)\nBefore presenting the methodology used to construct the dataset and examining the\nperformance of different LLMs, this section will introduce key Formal Verification\n(FV) concepts to clarify the approach adopted in developing the dataset. Since man-\nually labeling the entire dataset is not feasible for such a large volume of data, we\nuse an FV technique known as Bounded Model Checking (BMC) to detect vulnera-\nbilities in the generated C samples precisely. In contrast to traditional static analysis\ntools, which frequently produce a high number of false positives due to their reliance\non pattern recognition without a solid mathematical foundation [105], BMC provides\nrigorous validation that can help minimize both false positives and false negatives in\nthe findings.\n4.1 Preliminaries for the Data Labeling Method\nTo enhance understanding and ensure the reproducibility of our methodology, we\nintroduce some key definitions, including State Transition Systems (STS), the BMC\nproblem, and the specific tools chosen for our labeling method, considering the many\nFV tools available in the market.\n10\n4.1.1 State Transition System\nA state transition systemM = (S,T,S 0) represents an abstract machine consisting\nof a collection of statesS, whereS0 ⊆S indicates the initial states, andT ⊆S×S\nspecifies the transition relation, illustrating the potential state transitions within the\nsystem. Every states ∈S is characterized by the value of the program counter (pc)\nand the values of all program variables. The initial states0 sets the program’s starting\nlocation. Transitions between states denoted asT = (si,si+1) ∈T, between any two\nstates si and si+1, are associated with a logical formulaT(si,si+1) that describes the\nconstraints on the program counter and program variables relevant to that transition.\n4.1.2 Bounded Model Checking\nBMC is employed in FV to ascertain the correctness of a system up to a finite number\nof steps. This approach models the system as a finite state transition system and\nmethodically examines its state space to a predefined depth. Recent BMC modules\nare capable of processing a variety of programming languages such as C, C++, JAVA,\nor Kotlin [105–111]. The process begins with the program code, from which a control-\nflow graph (CFG) [112] is derived. In this CFG, nodes represent deterministic or\nnondeterministic assignments or conditional statements, while edges indicate potential\nchanges in the program’s control flow.\nEssentially, each node is a block that encapsulates a set of instructions with a\nunique entry and exit point, and edges indicate potential transitions to other blocks.\nThe CFG is then converted into Static Single Assignment (SSA) form and further into\na State Transition System (STS), which a Satisfiability Modulo Theories (SMT) solver\ncan interpret. The SMT solver checks if a given formula, representing the program’s\ncorrectness within a bound k, is satisfiable, indicating the existence of a potential\ncounterexample to the properties being verified. If no errors are found and the formula\nis unsatisfiable within the boundk, it suggests the program has no vulnerabilities\nwithin that limit. Thus, if the solver concludes satisfiability within a bound≤k, it\nconfirms the presence of a vulnerability through a counterexample.\nConsider a programPunder verification modeled as a finite STS, denoted by the\ntriplet ST = (S,R,I ), whereS represents the set of states,R⊆S×S represents the\nset of transitions, andI ⊆S, including elements such assn,...,s m, marks the initial\nstate set. In a state transition system, a state denoted ass∈Sconsists of the program\ncounter value, referred to aspc, and the values of all program variables. The initial\nstate, s0, specifies the initial program location on the CFG. Every transitionT =\n(si,si+1) ∈R, connecting two statessi and si+1, correlates with a logical expression\nT(si,si+1) that constrains the program counter (pc) and variable values pertinent to\nthe transition.\nIn the context of BMC, the properties under examination are defined as follows:\nϕ(s) represents a logical formula reflecting states that fulfill a safety or security cri-\nterion, whereas ψ(s) encodes a logical statement representing states that meet the\ncompleteness threshold, synonymous with program termination. Notably,ψ(s) incor-\nporates loop unwinding to avoid surpassing the program’s maximum loop iterations.\n11\nTermination and error conditions are mutually exclusive, renderingϕ(s) ∧ψ(s) inher-\nently unsatisfiable. IfT(si,si+1)∨ϕ(s) is unsatisfiable, statesis considered a deadlock\nstate.\n4.1.3 The Bounded Model Checking Problem\nBased on this information, we can define the bounded model checking problem as\nBMCΦ, which involves creating a logical statement. The truth of this statement\ndetermines if the programPhas a counterexample with a maximum length ofk. The\nformula can only be satisfied if a counterexample fitting within the predetermined\nlength restriction is present, i.e.:\nBMCΦ(k) = I(s0) ∧\nk−1⋀\ni=1\nT(si,si+1) ∧\nk⋁\ni=1\n¬ϕ(si). (1)\nHerein,Idenotestheinitialstatesetof ST,and T(si,si+1) embodiesthetransition\nrelation within ST between consecutive time steps i and i+ 1. Thus, the logical\nexpression I(s0) ∧⋀k−1\ni=1 T(si,si+1) depicts the execution pathways ofST spanning a\nlength k, andBMCΦ(k) can be satisfied if and only if for somei ≤k there exists a\nreachable state at time stepiin whichϕis violated. IfBMCΦ(k) is satisfied, it implies\na violation ofϕ, permitting an SMT solver to deduce a satisfying assignment from\nwhich the program variables’ values can be derived to assemble a counterexample. By\ndefinition, a counterexample, or trace, for a violated propertyϕ, is defined as a finite\nsequence of statess0,...,s k, wheres0,...,s k ∈S and T(si,si+1) holds for0 ≤i<k .\nThese counterexamples hold significant importance for us, as we explicitly seek out\nthese violations to compare and determine which code generated by LLMs is “more\nsecure”. Fewer violated properties indicate that the LLM can produce more secure\ncode.\nIn this context, it’s important to note that fewer errors in a C program generated\nbyan LLMdonotnecessarilyindicatesuperiority;the modelmaysimplybeproducing\nsimpler, shorter programs. Therefore, evaluating both property violations and code\ncomplexity metrics, such as Source Lines of Code (SLOC) orCyclomatic Complexity\n(CC) [17], can be a good starting point to determine the complexity of the generated\nprograms. For example, a basic “print hello world” program will not contain any\nvulnerabilities, but that doesn’t mean it’s a good program. This is why metrics like\nSLOC (Source Lines of Code) and CC (Cyclomatic Complexity) are crucial, as they\nhelp identify overly simplistic or short code that may have fewer vulnerabilities simply\ndue to its simplicity, not because it’s well-written.\nIf the Equation (1) is unsatisfiable, it implicates no error state as reachable within\nk steps or fewer. Hence, no software vulnerability exists within the bound k. By\nsearching for counterexamples within this bound, we can establish, based on mathe-\nmatical proofs, whether a counterexample exists and whether our programPcontains\na security vulnerability. This method detects security issues such as buffer overflows,\ndivision by zero, and null dereference failures. Notably, if a program is identified as\nvulnerable, this determination is based on counterexamples, effectively reducing the\nlikelihood of false positives. Conversely, in cases where no counterexample is found,\nwe can confidently state that the program is free from vulnerabilities up to the bound\n12\nk, thereby minimizing false negatives. By adopting this strategy, we aim to classify\neach program by detecting violated properties in the generated code.\n4.2 Efficient SMT-based Context-Bounded Model Checker\nNumerousBMCtoolscouldmeetourneeds.However,weaimedtoselectatooloffering\nhigh performance and detection rates. Annually, the International Competition on\nSoftwareVerification,knownasSV-COMP,challengesvariousprogramstodetectbugs\nand ensure software safety. In this competition, the Efficient SMT-based Bounded\nModel Checker (ESBMC) [14] stands out by solving the highest number of verification\ntasks within a strict 10-30 second time limit per program, as demonstrated in SV-\nCOMP 2023 [113].\nGiven its performance, ESBMC was selected as our primary BMC tool. As\na robust, open-source model checker for C/C++, Kotlin, and Solidity programs,\nESBMC addresses a wide range of safety properties and program assertions, including\nout-of-bounds array access, illegal pointer dereference, integer overflows, and mem-\nory leaks. It employs advanced verification techniques such as incremental BMC\nand k-induction, supported by state-of-the-art SMT and Constraint Programming\n(CP) solvers. ESBMC’s effectiveness in bug-finding is highlighted by its numerous\nachievements in SV-COMP, earning 6 gold, 4 silver, and 10 bronze medals.\n4.2.1 Identifiable Bugs Using ESBMC\nAlthough using ESBMC to identify bugs provides greater precision than traditional\nstaticanalysistools,itisalsomoretime-consuming,requiressubstantialresources,and\nis limited to detecting a specific set of vulnerabilities. This raises a natural question:\nwhat types of vulnerabilities can BMC detect, and which are the ones it cannot?\nBMCs primarily address low-level, code-centric issues such as buffer overflows,\nmemory leaks, and assertion failures. For instance, ESBMC identifies software errors\nby simulating a limited portion of a program’s execution with all possible inputs.\nHowever, vulnerabilities such as SQL injection, code injection, and XSS generally fall\noutsidethescopeofBMCsbecausecreatingageneralmathematicalmodeltorepresent\nhow a web browser or database interprets code is highly challenging. Additionally,\nSQL queries and HTML scripts can be written in various ways, making it impossible\nto create an exact abstract formula. This is particularly problematic because the\nprimary goal of formal verification is to model all possible inputs to verify the system\nand identify property violations effectively.\nWhen using ESBMC, the verification result of each C program falls into one of\nfour major categories: Verification Success (VS), Verification Failed (VF), Verification\nUnknown (VU), and Parsing Errors (ER), as illustrated in Table 2. These categories\nare mutually exclusive, meaning a single sample cannot belong to more than one\ncategory.\nThe most relevant category for our analysis is Verification Failed (VF), which\ncan be further subdivided into five main types: dereference failures (DF), arithmetic\noverflows (AO), buffer overflows (BO), array bounds violations (ABV), and other\n13\nTable 2: The Four Main Categories for Vulnerability Classification With ESBMC\nCategory Description\nVS: Verification Success The set of samples for which the verification process was com-\npleted successfully with no vulnerabilities detected.\nVU : Verification Unknown\n(Timeout)\nThe set of samples for which the verification process did not\ncomplete within the allotted time frame. Although no counterex-\nample was found within the time limit, this does not guarantee\nthe absence of vulnerabilities in the program with a longer time\nframe; therefore, the verification status remains unknown.\nVF : Verification Failed The set of samples for which the verification status failed, vulner-\nabilities detected by ESBMC based on counterexamples.\nER: Error The set of samples for which the verification status resulted in an\nerror. This typically occurs due to a parsing error in ESBMC, an\nissue in the GOTO converter, or other problems with the SMT\nsolver.\nTable 3: CWEs From 2023’s MITRE Top 25.\nRank CWE Description\n1 CWE-787: Out-of-bounds Write\n4 CWE-416: Use After Free\n6 CWE-20: Improper Input Validation\n7 CWE-125: Out-of-bounds Read\n12 CWE-476: NULL Pointer Dereference\n14 CWE-190: Integer Overflow or Wraparound\nmiscellaneous vulnerabilities (MV). These five categories encompass 33 subcategories\nthat ESBMC can identify, as illustrated in Table 4.\nIt is important to note that when we refer to “Verification Successful” (VS), it\nindicates that the specific bugs listed in Table 4 were not detected in the programs.\nHowever, this does not rule out the presence of other types of vulnerabilities in these\nprograms, such as command injection, cryptographic weaknesses, SQL injection, and\nsimilar issues. Table 4 also displays each vulnerability’s corresponding Common Weak-\nness Enumeration (CWE) number. It is important to note that ESBMC does not\nprovide an exact mapping of vulnerabilities to CWE numbers; the mapping presented\nhere was performed manually.\nThe multifaceted nature of software flaws often results in a single vulnerability\nassociated with multiple CWE identifiers. Table 4 categorizes the most common vul-\nnerabilities and the corresponding CWEs identified within these categories. In total,\n42 unique CWE were identified in the dataset. From MITRE’s Top 25 Most Dangerous\nSoftware Weaknesses for 2023 list, six is present in our list as shown in Table 3.\nThe remaining CWEs in the top 25 list are related to web vulnerabilities like SQL\ninjection, XSS, and authentication, which are irrelevant to our C language samples. It\nis vital to emphasize that, in our situation, classifying the C programs based on CWE\nidentifiers is not practical, contrary to what is done for other databases like Juliet.\nAs shown in Table 1, most datasets contain only one vulnerability per sample. In the\n14\ndatasets ReVeal, BigVul, and Diversevul, a function is vulnerable if the vulnerability-\nfixing commit changes it, while in Juliet, a single vulnerability is introduced for each\nprogram.\nIn FormAI, a single file often contains multiple vulnerabilities. As noted, a single\nvulnerability can be associated with multiple CWEs. Additionally, multiple CWEs can\nbe required for a vulnerability to be exploitable. As an example,“CWE-120: Buffer\nCopy without Checking Size of Input (Classic Buffer Overflow)”, can happen as a\nresult of“CWE-676: Use of Potentially Dangerous Function”, which can be thescanf\nfunction. If this is combined with“CWE-20: Improper Input Validation”, it can result\nin “CWE-787: Out-of-bounds Write”. Labeling the vulnerable function name, line\nnumber, and vulnerability type identified by the ESBMC module provides granular\ninformation that can benefit machine learning training. This level of detail can allow\nmodels to discern patterns and correlations with higher precision, thereby improving\nvulnerability prediction and detection capabilities.\nSince our programs exhibit numerous vulnerabilities, including multiple occur-\nrences of the same type, categorizing each solely into one CWE group, as seen with\nJuliet, would be sub-optimal for training purposes. This method fails to communicate\ncrucial details about the vulnerabilities. For instance, both “Arithmetic overflow on\nadd” and “Arithmetic overflow on div” are assigned the same primary CWE, man-\nifesting differently in the source code. Therefore, merely labeling them with CWEs\ndoes not offer the same level of granularity and makes the dataset less suitable for ML.\nWhile other datasets focus more on CWEs related to vulnerabilities that could be\nexploited, ESBMC also detects issues related to software safety. For this reason, in\nthe FormAI dataset, we did not assign a single CWE to each vulnerability. However,\nbased on our mapping in Table 4, one can easily associate an ESBMC vulnerability\nwith the closest CWE number if needed.\n5 Methodology and Dataset Creation\nFigure 4 provides an overview of the generation and vulnerability labeling mechanism\nfor the FormAI-v2 dataset. This process is divided into two main components: the C\nprogram generation (consisting of 1. C program generation using different LLMs and\n2. dataset preprocessing) and the classification (including 3. ESBMC classification\nand 4. dataset creation).\n5.1 Code Generation\nDuring the creation process, special attention was given to ensure the diversity of the\nFormAI-v2 dataset, which contains331 000 compilable C samples. Using a prompt\nlike “generate a C program”repeatedly, would yields similar outputs, such as adding\ntwo numbers or simple string manipulations, which does not satisfy our objectives.\nInstead, our goal is to generate a diverse and comprehensive set of small programs.\nTo meet this, we have developed systematic prompting method consisting a dynamic\nand a static part. The static component remains unchanged for all prompts, while\nthe dynamic portion undergoes continuous variation. An example of how our prompt\ntemplate looks like is shown under Figure 5.\n15\nTable 4: Detailed Categorization of Vulnerabilities Detected by ESBMC\nDescription CWE Associated CWEs\nDF: Dereference failures:\n1. NULL pointer CWE-476 CWE-690, CWE-391\n2. Invalid pointer CWE-822 CWE-119, CWE-787, CWE-822\n3. Forgotten memory CWE-825 CWE-401, CWE-404, CWE-459\n4. Array bounds violated CWE-125 CWE-119, CWE-787\n5. Invalidated dynamic object CWE-824 CWE-416, CWE-415\n6. Access to object out of bounds CWE-125 CWE-119, CWE-787\n7. Accessed expired variable pointer CWE-416 CWE-825\n8. Write access to string constant CWE-843 CWE-758\n9. Of non-dynamic memory CWE-590 CWE-415, CWE-415, CWE-762\n10. IBTA CWE-843 CWE-119\n11. Oversized field offset CWE-787 CWE-119, CWE-125, CWE-823\n12. Data object accessed with code type CWE-843 CWE-686, CWE-704\nAO: Arithmetic overflows:\n13. On sub CWE-191 CWE-20, CWE-190, CWE-192\n14. On add CWE-190 CWE-20, CWE-191, CWE-192\n15. On mul CWE-190 CWE-20, CWE-191, CWE-192\n16. Floating-point ieee_mul CWE-190 CWE-681\n17. Floating-point ieee_div CWE-682 CWE-369, CWE-681\n18. Floating-point ieee_add CWE-190 CWE-681\n19. Floating-point ieee_sub CWE-190 CWE-681\n20. On div CWE-190 CWE-20, CWE-369\n21. On shl CWE-190 CWE-192\n22. On modulus CWE-190 CWE-20, CWE-191\n23. On neg CWE-191 CWE-190, CWE-192\nBO: Buffer overflow:\n24. On scanf CWE-120 {CWE-20, CWE-121, CWE-122\n25. On fscanf CWE-120 CWE-129, CWE-131, CWE-628\n26. On sscanf CWE-120 CWE-676, CWE-680, CWE-787}\nABV: Array bounds violations:\n27. lower bound CWE-129 {CWE-119, CWE-125, CWE-129\n28. upper bound CWE-788 CWE-131, CWE-193, CWE-787}\n29. VLA array size in bytes overflows CWE-190 CWE-131, CWE-680\nMV: Miscellaneous Vulnerabilities:\n30. Division by zero CWE-369 CWE-691\n31. The pointer to a file must be valid CWE-476 CWE-690, CWE-459\n32. Same object violation CWE-628 CWE-843, CWE-668\n33. ZOFO CWE-761 CWE-415, CWE-590\nLegend:\nZOFO: Operand of free must have zero pointer offset, IBTA: Object accessed with incompatible base type\nThe dynamic part of the prompt, highlighted as[Type] and [Style], represent\ndistinct categories within the prompt, each encompassing different elements. Every\nAPI call randomly selects a Type category from a set of200 elements. This cate-\ngory contains topics such as Wi-Fi Signal Strength Analyzer, QR code reader, Image\nSteganography, Pixel Art Generator, Scientific Calculator Implementation, etc. Sim-\nilarly, a coding Style is chosen from a set of100 elements during each query. This\nhelps minimize repetition, as coding styles such as “excited”, “relaxed”, or “mathe-\nmatical” are randomly combined with a Type category. Our primary objective was to\n16\nFinal FormAI-v2 dataset with vulnerability classification\nCodeLlama 13BFalcon180B\nC program generation  using different LLMs\nDynamic prompt for LLMs\nto generate C programs\n1\nCoding\nTask\nCompilable, unique C programs with 50+ lines of code,\nfree of Type 1 and Type 2 clones.\nGPT-3.5 Gemini\nPro 1.0\nMistral\n7B\nCoding\nStyle\n72k78k\nGemma\n7B\nGPT-4omini Llama2\n13B\n10k 47k40k\nFalcon211B\n12k\nFormAI-v2JSON\nDataset Preprocessing Phase\nClone\nDetection\n(Nicad 7.0)\nCyclomaticComplexity\nAnalyzer(Lizard 1.17.10)\nCompiler(gcc 13.2)\nSymbolicExecutionSMTsolver GOTOconverter\nclangcompiler\nASTconverter\nEfficient Bouned Model Checker (ESBMC) module\nUnknown\nVerification\nSuccessful\nVerification Failed \nProperty violation\nEach C program is validated using the ESBMC formal\nverification tool.\n12k 40k 20k\n2 3\n4\nFormAI-v2 dataset\nFig. 4: FormAI-v2 dataset generation Framework using different LLMs.\nidentify and capture as many vulnerabilities as possible. This method can generate\n200×100 = 20 000 distinct combinations. As demonstrated by insights from [86, 114],\nthere’s a need for a code base that supports diverse settings while ensuring tasks\nremain concise to fit within the token constraints of large language models (LLMs).\nThis raises a key question: If we generate a dataset of over 300,000 instances\nbut only 20,000 distinct combinations, will it lead to redundancy? Will the same or\ndifferent models produce identical outputs for these repeated prompts? To address\nthis, we will conduct clone code detection in the next section to ensure the generated\ncode is unique. Selecting prompts that LLMs can efficiently process is important,\n17\nFig. 5: Dynamic Code Generation Prompt.\ntherefore we designed tasks in the Type category accordingly. For instance, complex\nprompts like “Create a CRUD application using React for the front-end, Node.js with\nExpress for the back-end, and MongoDB for the database” must be broken down\ninto smaller, manageable tasks. Furthermore, tasks with different styles, such as ’File\nhandling’ with a ’romantic’ versus a ’happy’ style, lead to distinct outputs, which are\nreflected in different representations in the vector space upon tokenization. Despite\npotential compatibility issues between certain Type-Style combinations, encouraging\nLLMs to code in varied styles has generally enhanced the diversity of responses to\nidentical Types.\nDecreasing the number of unsuccessful queries by refining the prompt is important\nfrom an efficiency perspective. We have established five instructions in each prompt\nto minimize the error within the generated code. These instructions, along with their\ncorresponding explanations, are the following:\n1. Minimum 50 lines: This encourages the LLM to avoid the generation of overly\nsimplistic code with only a few lines (which occasionally still happens);\n2. Be creative!: The purpose of this instruction is to generate a more diverse\ndataset;\n3. Do not say I am sorry: This instruction aims to circumvent objections and\nresponsessuchas“AsanAImodel,Icannotgeneratecode”,andsimilarstatements.\n4. Make sure the program compiles: This instruction encourages the model to\ninclude header files and create a complete and compilable program.\n5. Generate a code snippet that starts with “‘c: Enable easy extraction of\nthe C code from the response.\nOnce a C code is generated, the GNU C compiler5 is employed to verify whether\nthe corresponding code is compilable. During the code generation process, we ensure\nthat the FormAI-v2 dataset exclusively consists of compilable code while excluding\nany other code that does not meet this criterion. Different models can generate vary-\ning percentages of compilable code depending on their parameter size. Models like\nGPT-4o-mini, Gemini Pro, or Falcon-180B can achieve compilation rates higher than\n90%, whereas smaller models with 7B parameters typically produce C code with a\ncompilability rate between 55-70%.\nThe primary reason for having non-compilable code was due to the absence of\nnecessary headers, such asmath.h, ctype.h, orstdlib.h. As the cost of generation\nassociated with different models can significantly vary, we did not generate the same\n5https://gcc.gnu.org\n18\nTable 5: Content of the FormAI-v2 Dataset.\nLLM Model Company Size License Sample Size\nGPT-4o-mini OpenAI N/A Proprietary 40 000\nLlama2 13B Meta 13B Open 20 000\nMistral-7B Mistral AI 7B Apache 2.0 10 000\nCode Llama 13B Meta 13B Proprietary 12 000\nGemini Pro 1.0 Google N/A Proprietary 40 000\nGemma-7B Google 7B Gemma-TOU 47 000\nFalcon-180B TII 180B Apache 2.0 72 000\nFalcon2-11B TII 11B Apache 2.0 12 000\nGPT-3.5-turbo OpenAI 175B Proprietary 78 000\nnumber of samples from each model. While some tested models are open source, their\noperational costs and GPU usage remain significant. For instance, running Falcon-\n180BonAWScancostaround40USDperhour.Table5presentsthesamplesobtained\nfrom each LLM.\n5.2 Clone Code Detection\nFor our purposes, we do not require entirely different programs, even if they are\nintended to accomplish the same task. Our goal is to observe how frequently models\nintroduce vulnerabilities into the programs. Therefore, even small changes can be\ninteresting, or if a model repeatedly makes the same coding errors which lead to\ndifferent vulnerabilities. Additionally, minor variations help to generalize the dataset\nduring machine learning training, so completely removing similar code is not our\nobjective.\nTo measure how diverse the dataset is, we used a code clone detection mecha-\nnisms to flag highly similar files. Using the state-of-the-art tool NiCad 7 (Automated\nDetection of Near-Miss Intentional Clones)[18], we performed clone detection within\nindividual tasks and across datasets generated by different models. NiCad can detect\nfour distinct types of clones, each with varying thresholds.Type 1 clones are exact\nduplicates of code fragments, with no changes except for whitespace and com-\nments. Type 2 clones permit minor modifications, such as renaming variables or\naltering formatting, while maintaining the original logic and structure.\nType 3-1 clones introduce greater flexibility, allowing small additions, deletions,\nor modifications while preserving overall functionality.Type 3-2 clones allow for even\nmore substantial changes, but the core behavior of the code remains consistent. In our\nclone detection process forType 3-1andType 3-2clones, we applied a 10% threshold\nto eliminate similar codes. A larger threshold could eliminate valuable code fragments\nfor machine learning, where small deviations may still result in the same vulnerability.\nEven slight variations can improve the training process by offering diverse represen-\ntations of the same vulnerability, enabling the model to recognize it more effectively\nacross various scenarios. Table 6 shows each dataset’s number of clones identified.\n19\nTable 6: Different Types of Clones Removed From the Dataset\nLLM Model Sample size Type1 Type2 Type 3-1 Type 3-2 ∆(%)\nFalcon2-11B 12 000 0 0 1 36 0 .30\nMistral-7B 10 000 1 1 11 59 0 .59\nCodeLlama-13B 12 000 3 5 12 128 1 .10\nGPT-3.5-turbo 78 000 118 301 502 1 756 2 .25\nGPT-4o-mini 40 000 0 24 31 1 075 2 .69\nGemini Pro 1.0 40 000 12 150 187 1 255 3 .14\nFalcon-180B 72 000 42 363 541 3 464 4 .81\nLlama2-13B 20 000 165 607 1 001 2 214 11 .07\nGemma-7B 47 000 657 3 229 2 997 10 199 21 .70\nThe last column, ∆(%), shows the percentage of Type 3-2 clones that were\ndetected and removed from the dataset. A higher percentage indicates that the LLM\ngenerated more similar, redundant code samples.\nIn terms of clone categories,Type 1 and Type 2 are hierarchical: Type 1 clones\nare a subset ofType 2, meaning that allType 1 clones are also consideredType 2.\nSimilarly, Type 3-1 and Type 3-2 are inclusive, whereType 3-1 clones fall within\nthe broader Type 3-2 category. In other words,Type 1 ⊆Type 2 and Type 3-1 ⊆\nType 3-2.\nHowever, Type 2 is not a subset ofType 3-1 because the threshold forType 2\nclones is exactly zero—meaning only variable changes are allowed across the entire\ncode with no additional modifications. In contrast,Type 3-1 allows for up to a10%\nmodification threshold, which can include variable changes, deletions, additions, or\nstructural modifications, as long as they remain within the10% limit.\nThe most flexible clone category isType 3-2, where a10% threshold applies to the\nentire code without restrictions. This means that any kind of modification, including\nvariable changes throughout the entire program, is allowed. To ensure the dataset’s\nquality, we removed all clones up to and includingType 3-2.\nAfter filtering out these clones from each LLM-generated subset, we appliedType\n3-2detection to the entire dataset to identify any similar code across different models.\nThis process revealed an additional283 Type 3-2 clones, which were subsequently\nremoved.Intotal, 20 469programswereexcluded,resultinginafinaldatasetof 310 531\nunique files. This demonstrates that the dataset is diverse, with only6.18% of the\noriginal programs being similar.\n5.3 Vulnerability Classification\nAfter code generation and elimination, the next step was classifying our dataset\nusing ESBMC. Compared to the classification in [9], a significant change has been\nmade. The original work used bounded model checking (BMC) with a bound set to\nk = 1 . For example, if a property violation occurs at levelk = 2 , then BMCΦ(1)\nwill not detect the vulnerability and it simply returns Verification Success, giving a\n20\nfalse impression. As a result, in theFormAI-v1 dataset, numerous samples were previ-\nously classified as“NON-VULNERABLE up to boundk”. We have transitioned from\nbounded tounbounded model checking to capture more vulnerabilities or prove their\nabsence for each sample. This approach incrementally unwinds the program until a\nbug is found or the completeness threshold is reached, meaning all possible terminat-\ning states have been explored. Incremental BMC ensures that smaller problems are\nsolved sequentially, avoiding the need to guess an upper bound for verification.\nApplying these settings, we have successfully identified more vulnerabilities in the\nprograms. Consequently, if the verification process is completed successfully, we can\nconclude that the program has no violated properties (that can be detected by the\ncurrently used ESBMC version). While this approach requires significantly more com-\nputational power, it has proven effective in revealing more vulnerabilities or proving\ntheir absence, as we will demonstrate in Section 6.\n5.3.1 ESBMC Parameter Selection\nModel-checking tools like ESBMC provide various parameters, and the identified vul-\nnerabilities may differ depending on the parameters chosen. Default parameters, such\nas those used in competitions like SV-COMP, may not be suitable for all software\ntypes, potentially leading to fewer detected vulnerabilities. This naturally leads to the\nquestion: which options should we use? Should we use thek-induction switch with\na large time limit, such as 100 seconds, or should we opt for thebmc switch? These\nquestions are not straightforward. To address them, we conducted a detailed analysis\nto understand how different settings impact verification outcomes. We have randomly\nselected 1 000 samples from the dataset, serving as the basis for selecting the ESBMC\nparameters for the entire dataset. By experimenting with different switches and time\nframes, we were able to select the options that best met our needs.\nFor these samples, we tested multiple parameter configurations of ESBMC to\ndetermine which settings yielded the best results regarding runtime efficiency and\nvulnerability detection. We focus on two objectives. Firstly, to minimize verification\nunknown outcomes (VU) through thet (time) parameter and preferably completing\nthe verification process; and secondly, to identify as many vulnerabilities as possi-\nble. Table 7 illustrates the verification outcomes of the 1,000 samples, demonstrating\nhow various combinations of unwind (u) and time (t), alongside the utilization of\nk-induction, incremental BMC, or falsification techniques, impact the results.\nIn this context, “unwind” refers to the number of iterations for which we should\nunroll the loops. For example, u = 1 means that we unroll a loop for only one iteration,\nwhile u =∞indicates no limit on the number of iterations for loop unrolling. Our\nanalysis revealed that merely increasing the unwind parameteruwhile keeping a short\ntimeout (e.g., 1 second) often leads to timeouts. For example, setting the unwind to\n10 with a 1-second timeout resulted in most samples (684) falling intoVU. A larger\nunwind parameter enhances the detection of vulnerabilities in loops, provided there is\nsufficient processing time. We can also observe that thek-induction switch increases\nthe number of detected vulnerabilities,|ϕ|, as the allotted time increases. Therefore,\nthe best approach for us is to set the timeout as high as possible withk-induction\nenabled. In our architecture, we set the timeout to 500 seconds and allowed unlimited\n21\nTable 7: Classification Results for the 1000-Sample Dataset With Varying Param-\neters.\nESBMC Parameters RESULTS\nu time k-ind bmc fls Runtime |ϕ| VS VF VU ER\nx 300 ✓ x x 1698:53 1 678 471 491 25 13\n2 1000 x x x 1418:03 1 638 505 407 70 18\n3 1000 x x x 2100:36 1 620 495 390 94 21\nx 100 ✓ x x 653:05 1 583 486 468 33 13\n2 100 x x x 224:25 1 580 496 393 96 15\n1 1000 x x x 419:45 1 529 538 428 21 13\nx 30 ✓ x x 216:28 1 513 494 448 45 13\nx 30 x ✓ x 216:20 1 511 494 448 45 13\nx 30 x x ✓ 232:36 1 511 494 448 45 13\n2 30 x x x 99:05 1 500 486 371 129 14\n1 100 x x x 79:09 1 465 536 421 30 13\nx 10 ✓ x x 84:11 1 432 500 430 57 13\n3 100 x x x 344:01 1 408 478 350 158 14\n1 10 x x x 21:47 1 351 527 399 61 13\n2 10 x x x 47:48 1 272 469 330 187 14\n3 10 x x x 62:14 951 433 272 281 14\nx 1 ✓ x x 13:23 941 474 336 177 13\nx 1 x x ✓ 13:39 938 475 335 177 13\nx 1 x ✓ x 13:34 936 476 334 177 13\n1 1 x x x 7:41 911 487 323 177 13\n2 1 x x x 10:30 559 404 205 377 14\n10 1 x x x 14:14 158 224 79 684 13\nx 10 x x x 152:41 69 75 25 887 13\nLegend:\n✓: Enabled; x: Not set;|ϕ|: Number of Vulnerabilities detected;k-ind: k-induction;\nbmc: incremental-bmc; fls: falsification technique;u: unwind; (Runtime in (m:s))\nk-steps, transitioning from bounded tounbounded modelchecking. This adjustment\nensures that if the verification is completed within this time frame, we either identify\na counterexample or confirm the absence of the examined vulnerabilities. For our\ndataset classification, we used a machine with 192 CPUs and 1.5 TB of memory,\nwhich allowed us to set a time frame of 500. A larger time frame is not feasible in our\ntest environment, as concurrently verifying 192 C programs would exceed the 1.5 TB\nmemory capacity. Based on our experiments, we used the following ESBMC switches\nduring our experiments, as depicted in Figure 6.\n22\nFig. 6: ESBMC Command Employed to Verify Each Sample in the Dataset.\nNote, that the–overflow, –memory-leak-check, and–multi-property switches\nare used to identify the maximum number of potential vulnerabilities. These switches\ndo not affect the running time. Using these parameters on our1000 sample set, 416\nfiles were deemed non-vulnerable, while 519 files were vulnerable. Among these 519\nfiles, a total of 2116 unique vulnerabilities were detected. Considering the classification\nof 331 000 programs, the worst-case scenario is that every program from FormAI-v2\nwould utilize its allocated time, resulting in 500 seconds dedicated to verifying each\nsample. Using 192 CPU threads, the entire verification process on our experimental\nsetup would take approximately9,97 days in this worst-case scenario, calculated as\n331 000×500/60/60/24/192.\n6 Verification Results\nIn this section, we summarize our key results, beginning with an analysis of statis-\ntics for the entire dataset, focusing on overall verification outcomes and vulnerability\ntypes. It is important to note that the analysis is based on310,531 programs, as all\nclones up toType 3-2 have been excluded from the initial331,000. We then evaluate\neachLLM,comparingthecomplexityofthecodetheygenerateandtheirsecurecoding\ncapabilities. This is followed by evaluating each LLM and comparing the complexity\nof the code they generate and their secure coding capabilities.\nIn the original FormAI dataset, only112,000 compilable C samples were created\nusing GPT-3.5-turbo. Furthermore, the complexity of each program was not mea-\nsured. This research closes this gap by comparing nine state-of-the-art LLMs and\nproviding a vulnerability-labelled dataset to the research community. We have exam-\nined 26 633 156 lines of C code, with an average of85.77 lines per sample. In total,\nwe performed the verification process on310 531 C program files, and our results for\nthe entire dataset are shown in Table 8. The TOP 10 violations throughout the entire\ndataset are presented in Table 9. Table 10 provides a breakdown of the distribution\nfor each of the top five main categories of vulnerabilities.\nDuring the500-second verification time-frame, ESBMC identified192 757 unique\nprograms with vulnerabilities. In contrast, only25 674 programs, representing8.27%,\nwere verified as secure. Expanding computational resources may increase the number\nofprogramsuncoveredfrom VU,therebypotentiallyextendingthe VFcategory.These\nresults provide an even better lower bound compared to [9], on what percentage of\nLLM-generated code is vulnerable. The situation is more concerning than merely\n23\nTable 8: Overview of Statistics and Verification Results for Each LLM.\nModel\nName\nSamples\nw.o clones\nMax\nSLOC\nAvg\nSLOC\nAvg\nCC\nVS\n(%)\nVU\n(%)\nVF\n(%)\nER\n(%)\nGemma-7B 36 787 351 67 .28 5 .25 11 .62 16 .30 67 .01 5 .07\nGPT-3.5-turbo 76 168 616 96 .79 6 .07 7 .29 26 .09 65 .07 1 .55\nGemini Pro 1.0 38 695 332 98 .87 4 .56 9 .49 24 .13 63 .91 2 .47\nFalcon2-11B 11 946 338 77 .75 6 .34 10 .28 24 .56 63 .16 2 .00\nMistral-7B 9 934 161 75 .06 3 .84 8 .36 25 .88 62 .08 3 .68\nFalcon-180B 68 463 181 71 .93 4 .38 6 .48 28 .67 62 .07 2 .78\nGPT-4o-mini 38 921 347 103 .58 3 .40 4 .23 36 .77 57 .14 1 .86\nCodeLlama-13B 11 838 258 83 .36 4 .54 15 .48 29 .52 52 .71 2 .39\nLlama2-13B 17 779 207 75 .51 4 .12 12 .36 31 .78 51 .30 4 .56\nFormAI-v2 310 531 616 85.77 4.85 8.27 26.99 62.07 2.67\nLegend:\nMax SLOC: Maximum Source Lines of Code in a sample.Avg SLOC: Average Source\nLines of Code per sample.Avg CC: Average Cyclomatic Complexity, which measures the\ncomplexity of the code based on the number of linearly independent paths.VS: Verifications\nSuccess. VU: Verification Unkown.VF: Verification Failed (vulnerable).ER: Error.\nTable 9: Top 10 Violations Across All Categories in FormAI-v2 dataset.\nRank Category Violation Type Count Percentage\n1 DF Dereference failure: NULL pointer 289 548 37.83%\n2 BO Buffer overflow onscanf 214 255 27.99%\n3 DF Dereference failure: invalid pointer 73 838 9.65%\n4 DF Dereference failure: array bounds violated 23 586 3.08%\n5 ABV Array bounds violated: upper bound 23 380 3.05%\n6 DF Dereference failure: forgotten memory 21 108 2.76%\n7 ABV Array bounds violated: lower bound 19 918 2.60%\n8 AO Arithmetic overflow on sub 18 345 2.40%\n9 AO Arithmetic overflow on add 15 966 2.09%\n10 AO Arithmetic overflow on mul 12 462 1.63%\nstating that 62.07% of the generated files are vulnerable, as a single file can contain\nmultiple vulnerabilities. On average, each file contains3.97 vulnerabilities. The total\nnumber of property violations detected by ESBMC for the overall dataset is765 366.\nThe most common type of vulnerability is related to “Dereference failures”\naccounting for54.54% of the cases, predominantly due to NULL pointer issues. This\ncategory includes a variety of pointer-related issues, such as invalid pointers, a forgot-\nten memory, and array-bounds violations, among others. “Buffer overflows”, mainly\ntriggered by the scanf function, comprise a significant 27.99% of the vulnerabili-\nties. This highlights common issues in handling buffer sizes and input functions.\n24\nTable 10: Detailed Categorisation of Vulnerabilities in the Entire Dataset\nDescription Count Percentage\nDereference failures:\n- NULL pointer 289 548 37.83%\n- Invalid pointer 73 838 9.65%\n- Forgotten memory 21 108 2.76%\n- Array bounds violated 23 586 3.08%\n- Invalidated dynamic object 3 145 0.41%\n- Access to object out of bounds 3 221 0.42%\n- Accessed expired variable pointer 1 227 0.16%\n- Write access to string constant 913 0.12%\n- Non-dynamic memory 342 0.04%\n- Object accessed with incompatible base type 379 0.05%\n- Oversized field offset 170 0.02%\n- Data object accessed with code type 14 0.00%\nArithmetic overflows:\n- On sub 18 345 2.40%\n- On add 15 966 2.09%\n- On mul 12 462 1.63%\n- IEEE mul 9 673 1.26%\n- IEEE div 3 522 0.46%\n- IEEE add 2 375 0.31%\n- IEEE sub 1 632 0.21%\n- On div 813 0.11%\n- On shl 972 0.13%\n- On modulus 348 0.05%\n- On neg 155 0.02%\nBuffer overflows:\n- Onscanf 214 255 27.99%\n- Onfscanf 8 252 1.08%\n- Onsscanf 4 184 0.55%\nArray bounds violations:\n- Upper bound 23 380 3.05%\n- Lower bound 19 918 2.60%\n- VLA array size in bytes overflows address space size 4 222 0.55%\nMiscellaneous Vulnerabilities:\n- Division by zero 4 311 0.56%\n- The pointer to a file object must be a valid argument1 225 0.16%\n- Invalid Function argument issues 443 0.06%\n- Same object violation 123 0.02%\n- Operand of free must have zero pointer offset 134 0.02%\n“Arithmetic overflows” are also notable, covering various operations like subtraction,\naddition, multiplication, and division, indicating frequent issues in handling numeric\ncalculations without adequate checks. The table further lists “Array bounds viola-\ntions” and “Division by zero” as common issues, illustrating challenges in correctly\nmanaging arrays and arithmetic operations. A smaller portion of the table covers\n“Miscellaneous Vulnerabilities” which includes a variety of less frequent but notable\nissues such as invalid file object pointers and operand violations in memory dealloca-\ntion. Overall, the data emphasizes the need for robust handling of pointers, buffers,\nand numeric operations within the source code to mitigate the risk of vulnerabilities.\n25\n6.1 General observation about code complexity\nNIST defines Cyclomatic Complexity (CC) as “the amount of decision logic in a\nsource code function” and recommends a maximum value of 10 [115]. According to\nNIST, “higher numbers are bad and lower numbers are good.” As Figure 7 shows,\nmany individual programs generated by Gemma-7B exceed the threshold of 10. While\nSLOC and CC cannot be used to determine whether code is vulnerable directly, we\nobserved that higher cyclomatic complexity can lead to an increased likelihood of\nvulnerabilities. Models such as GPT-3.5-turbo, Gemma-7B, and Falcon2-11B, which\nhave high CC, also display the highest rates of verification failures.\nAs earlier shown in Table 8, the Avg. CC (Average cyclomatic complexity per sam-\nple) and Avg. SLOC (Average Source Lines of Code per sample) provide insight into\nthe complexity of the code generated by a certain model. As previously mentioned, if a\nmodel produces only non-vulnerable code, it doesn’t necessarily indicate high quality;\nit could suggest that the generated code is very simple (e.g., generating only “print\n’hello world”’ examples). While observing SLOC and CC cannot precisely determine\na model’s code quality, it is interesting to observe that GPT-4o-mini, CodeLlama-\n13B, and Llama2-13B had the least lowest verification failed results and the lowest\nCC scores.\nThe analysis of Table 8 shows that GPT-4o-mini does not necessarily generate\nshorter or simpler code. It produces the longest C programs, with an average SLOC\nof 103.48, and has the highest verification unknown score (36.77%), indicating that\nthe ESBMC verification process takes longer for GPT-4o-mini samples. In contrast,\nGemma-7B generates the shortest average SLOC and also has the lowest verification\nunknown result (16.30%). Additionally, GPT-4o-mini produces code with a lower CC,\nwhich implies better maintainability and quality, while Gemma-7B has a much higher\naverage CC.\n6.2 Keyword Frequency\nWhenassessingvulnerabilitiesinLLM-generatedcode,akeyquestionarises:Howdoes\nLLM-generated code compare to human-written code? If the generated code differs\nsignificantly, the dataset may not support meaningful comparisons with real-world\n0 5000 10000 15000 20000 25000 30000 35000 40000\n0\n10\n20\n30\n40\n50\nAvg. CC = 3.40\n(a) GPT4o-mini.\n0 5000 10000 15000 20000 25000 30000 35000\n0\n10\n20\n30\n40\n50\nAvg. CC = 5.25 (b) Gemma-7B.\nFig. 7: Comparison of Cyclomatic Complexity between GPT4o-mini and Gemma-7B.\n26\nBigVul GPT-4o-mini Gemma-7B Falcon-180B Gemini-Pro\nif\nreturn\nstruct\nint\nconst\ncase\nelse\nvoid\nchar\ngoto\nunsigned\nfor\nlong\nbool\nwhile\nswitch\ndouble\nstatic\nsizeof\nregister\nfloat\nshort\ndo\nenum\nauto\nunion\nvolatile\ntypedef\nbreak\nsigned\nextern\ndefault\ncontinue\n96012 45687 32027 41093 46165\n44190 28197 25819 30799 41172\n27847 15001 23654 21025 22625\n27303 97717 91534 106378 92644\n19971 15632 68 3446 3989\n17261 13219 19869 7412 7092\n16199 12050 11528 10342 10188\n12604 40261 17174 27736 21408\n11541 39740 24815 38441 34986\n10144 9 4 6 88\n9653 3924 597 1321 3735\n7313 25320 29811 27322 27974\n4258 3223 196 480 1144\n4006 1599 1 457 772\n2993 10656 9139 12055 7691\n2377 2590 3936 1356 1522\n1709 8492 6350 6652 9748\n927 144 5 153 675\n858 25 0 4 18\n808 0 0 0 1\n615 5234 1732 2141 2812\n558 346 122 54 146\n477 1179 26 709 189\n451 329 56 190 817\n316 0 0 0 0\n182 24 47 3 16\n134 52 0 1 13\n64 7188 8909 8308 7638\n53 0 0 0 2\n46 0 0 0 0\n28 0 0 0 1\n18 0 0 0 1\n7 0 0 0 1\nNormalized Average Keyword Frequency Heatmap (Per Million Lines of Code)\n0\n20000\n40000\n60000\n80000\n100000\nFig. 8: 32 C keyword distribution\ncode. A practical starting point for this analysis is comparing keyword frequencies.\nIn real-world C/C++ projects, such as those from GitHub and datasets like BigVul,\ncommon keywords include ‘if’, ‘return’, ‘struct’, ‘int’, and ‘const’, while less frequent\nkeywords include ‘continue’, ‘default’, and ‘extern’. Significant differences in keyword\nfrequencies between LLM-generated and real-world code would question the dataset’s\nvalidity.\nTo investigate, we used a token-based keyword-counting method to analyze the\nfrequency of 32 C keywords in each LLM-generated subset. Ideally, LLM-generated\ncode should exhibit a similar keyword distribution to real-world code. Figure 8 shows\nthe normalized keyword frequency (occurrences per million lines of code) for various\nLLM-generated codes, with BigVul as a real-world benchmark. The heatmap reveals\nthat LLM-generated and real-world code have closely matching keyword distributions,\nlikely due to the LLMs being trained on human-written GitHub projects.\nWhile there are slight variations in the distribution between LLMs and BigVul\nmainly for the less frequent words, LLMs show great similarity on how they handle\nstatements, expressions, and variables in distinct ways. Note, that while all LLM\ngenerated codes are fully compilable on our dataset, this is not the case with BigVul\nsamples and other human written code datasets.\n27\n6.3 Vulnerability Ranking\nTable 11 (Parts I, II, and III) provides an overview of the top 10 vulnerabilities gen-\nerated by each model. Note that raw vulnerability counts are not directly comparable\ndue to the differing number of samples produced by each model. To enable a fair com-\nparison across LLMs, the table also includes the percentage representation of each\nvulnerability. This analysis does not offer a comprehensive review of all identified\nCWEs but focuses on vulnerabilities explicitly verified by ESBMC.\nBuffer overflow vulnerabilities related toscanf are consistently ranked among the\ntop three across all LLM models. The functionsscanf, fscanf, and sscanf do not\nrestrict the input size of their respective buffers, creating a risk of buffer overflow.\nThis vulnerability can allow attackers to execute arbitrary code or trigger crashes.\nAs previously mentioned, these issues relate to several CWEs, including CWE-676,\nCWE-20, and CWE-787. Although buffer overflow is a type of out-of-bounds write,\nCWE-787 covers a broader range of vulnerabilities. CWE-120 specifically addresses\nclassic buffer overflow scenarios caused by unchecked input sizes during buffer copy\noperations. While more complex issues like arithmetic overflows and array bounds vio-\nlationsrequiredeeperprogrammingcontext,simplerissuessuchas scanferrorsshould\nbe easier to avoid. However, all tested models consistently exhibit buffer overflow\nerrors withscanf.\nDereference failures, particularly NULL pointer dereferences, are among the most\nprevalent vulnerabilities across all LLMs. This is due in part to the varied and often\nunsafe examples of pointer usage in training datasets, combined with the inherent\ncomplexity of dynamic memory management in C. LLMs rely on pattern recognition\nrather than deep understanding, which leads them to mishandle pointers and fail\nto replicate the nuanced behavior of real-world applications. This results in frequent\ndereference issues and flawed pointer handling, highlighting significant risks when\ndeploying LLM-generated code in critical systems where security and reliability are\nparamount.\nThe severity and frequency of these vulnerabilities vary significantly among mod-\nels. For instance, Gemma-7B exhibits a notably high rate of NULL pointer dereference\nfailures at 60.50%, indicating substantial weaknesses in memory management. Arith-\nmetic overflows also consistently appear across all models in the top 10 list, and differ\nbased on specific operations (addition, subtraction, multiplication), underscoring var-\nied arithmetic handling. Notably, Llama2-13B stands out with less than 10% ofscanf\nviolations, with Gemini Pro 1.0 close behind at approximately 11%; however, both\nmodels, like Gemma-7B, show high rates of NULL pointer dereference failures.\nThe consistent occurrence of certain errors across different models underscores the\nneed for comprehensive testing and validation frameworks to address these recurring\nissues before deployment. While all models share similar vulnerabilities, significant\ndifferences in the frequency and types of other vulnerabilities—such as arithmetic\noverflows—suggest that model-specific optimizations and enhancements are neces-\nsary. To mitigate these risks, developing enhanced training methodologies focused on\nrobust memory handling is crucial. Implementing advanced code analysis tools and\nframeworks is also essential to detect and rectify vulnerabilities before deployment for\nreal-world applications.\n28\nTable 11: Top 10 Vulnerabilities in LLM Generated Code - Part I\nRank Category Violation Type Count Percentage\nGPT-3.5-turbo\n1 BO Buffer overflow onscanf 84 213 38.23%\n2 DF Dereference failure: NULL pointer 56 690 25.74%\n3 DF Dereference failure: invalid pointer 20 617 9.36%\n4 DF Dereference failure: forgotten memory 4 631 2.10%\n5 DF Array bounds violated: lower bound 8 102 3.68%\n6 DF Array bounds violated: upper bound 8 101 3.68%\n7 AO Arithmetic overflow on sub 6 627 3.01%\n8 DF Dereference failure: array bounds violated 6 537 2.97%\n9 AO Arithmetic overflow on add 5 228 2.37%\n10 AO Arithmetic overflow on mul 4 285 1.95%\nFalcon-180B\n1 BO Buffer overflow onscanf 49 175 34.37%\n2 DF Dereference failure: NULL pointer 42 177 29.48%\n3 DF Dereference failure: invalid pointer 15 732 11.00%\n4 DF Dereference failure: forgotten memory 5 442 3.80%\n5 DF Dereference failure: array bounds violated 4 545 3.18%\n6 DF Array bounds violated: upper bound 4 310 3.01%\n7 AO Arithmetic overflow on sub 3 315 2.32%\n8 DF Array bounds violated: lower bound 3 611 2.52%\n9 AO Arithmetic overflow on add 2 858 2.00%\n10 BO Buffer overflow onfscanf 2 532 1.77%\nLlama2-13B\n1 DF Dereference failure: NULL pointer 17 630 54.45%\n2 DF Dereference failure: invalid pointer 3 089 9.54%\n3 BO Buffer overflow onscanf 2 775 8.57%\n4 DF Dereference failure: array bounds violated 1 611 4.98%\n5 DF Dereference failure: forgotten memory 1 254 3.87%\n6 AO Arithmetic overflow on add 883 2.73%\n7 DF Array bounds violated: upper bound 818 2.53%\n8 AO Arithmetic overflow on mul 599 1.85%\n9 AO Arithmetic overflow on sub 571 1.76%\n10 BO Division by zero 462 1.43%\nGemma-7B\n1 DF Dereference failure: NULL pointer 59 433 60.50%\n2 BO Buffer overflow onscanf 14 950 15.22%\n3 DF Dereference failure: invalid pointer 3 617 3.68%\n4 DF Dereference failure: forgotten memory 3 191 3.25%\n5 DF Array bounds violated: upper bound 3 379 3.44%\n6 DF Array bounds violated: lower bound 2 784 2.83%\n7 AO Arithmetic overflow on sub 2 040 2.08%\n8 DF Dereference failure: array bounds violated 1 786 1.82%\n9 AO Arithmetic overflow on floating-point ieee_mul 1 152 1.17%\n10 AO Arithmetic overflow on add 1 302 1.33%\n29\nTable 11 (Cont.): Top 10 Vulnerabilities in LLM Generated Code - Part II\nRank Category Violation Type Count Percentage\nCodeLlama-13B\n1 DF Dereference failure: NULL pointer 11 546 44.75%\n2 BO Buffer overflow onscanf 5 169 20.03%\n3 DF Dereference failure: invalid pointer 3 481 13.49%\n4 DF Dereference failure: array bounds violated 897 3.48%\n5 DF Dereference failure: forgotten memory 695 2.69%\n6 DF Array bounds violated: upper bound 683 2.65%\n7 AO Arithmetic overflow on add 524 2.03%\n8 DF Array bounds violated: lower bound 465 1.80%\n9 AO Arithmetic overflow on mul 456 1.77%\n10 AO Arithmetic overflow on sub 380 1.47%\nGemini Pro 1.0\n1 DF Dereference failure: NULL pointer 65 376 55.95%\n2 DF Dereference failure: invalid pointer 13 272 11.36%\n3 BO Buffer overflow onscanf 12 948 11.08%\n4 DF Dereference failure: array bounds violated 4 250 3.64%\n5 DF Dereference failure: forgotten memory 3 340 2.86%\n6 AO Arithmetic overflow on mul 2 466 2.11%\n7 DF Array bounds violated: upper bound 2 285 1.96%\n8 DF Array bounds violated: lower bound 1 952 1.67%\n9 AO Arithmetic overflow on sub 1 899 1.63%\n10 AO Arithmetic overflow on add 1 895 1.62%\nMistral-7B\n1 DF Dereference failure: NULL pointer 6 294 33.17%\n2 BO Buffer overflow onscanf 5 125 27.01%\n3 DF Dereference failure: invalid pointer 2 460 12.97%\n4 DF Dereference failure: array bounds violated 738 3.89%\n5 DF Array bounds violated: lower bound 622 3.28%\n6 AO Arithmetic overflow on sub 473 2.49%\n7 DF Array bounds violated: upper bound 453 2.39%\n8 DF Dereference failure: forgotten memory 414 2.18%\n9 AO Arithmetic overflow on add 400 2.11%\n10 BO Buffer overflow onsscanf 388 2.04%\nGPT-4o-mini\n1 BO Buffer overflow onscanf 33 307 42.60%\n2 DF Dereference failure: NULL pointer 17 539 22.43%\n3 DF Dereference failure: invalid pointer 7 055 9.02%\n4 AO Arithmetic overflow on sub 2 479 3.17%\n5 DF Array bounds violated: upper bound 2 277 2.91%\n6 AO Arithmetic overflow on add 2 114 2.70%\n7 DF Dereference failure: array bounds violated 1 956 2.50%\n8 AO Arithmetic overflow on floating-point ieee_mul 1 857 2.38%\n9 AO Arithmetic overflow on mul 1 536 1.96%\n10 DF Array bounds violated: lower bound 1 398 1.79%\n30\nTable 11 (Cont.): Top 10 Vulnerabilities in LLM Generated Code - Part III\nRank Category Violation Type Count Percentage\nFalcon2-11B\n1 DF Dereference failure: NULL pointer 12 863 40.71%\n2 BO Buffer overflow onscanf 6 593 20.87%\n3 DF Dereference failure: invalid pointer 4 515 14.29%\n4 DF Dereference failure: array bounds violated 1 266 4.01%\n5 DF Dereference failure: forgotten memory 1 106 3.50%\n6 DF Array bounds violated: upper bound 1 074 3.40%\n7 AO Arithmetic overflow on add 762 2.41%\n8 DF Array bounds violated: lower bound 613 1.94%\n9 AO Arithmetic overflow on sub 561 1.78%\n10 AO Arithmetic overflow on mul 459 1.45%\n6.4 LLM Ranking: Which model is the most secure coder\nTo compare which model is performing the “worst” or the “best” when it comes to\nsecure coding—and to do this as fairly as possible—we will investigate several metrics,\nsuch as the ratio of verification results, average property violation per file, and average\nproperty violation per line of code.\nThe results indicate that there is no clear winner.Mistral-7B, despite hav-\ning the fewest property violations per file, writes shorter code, reducing its likelihood\nof coding errors. However, this model also performs poorly in theVS metric, with\nonly 8.36% of its samples categorized as being free of vulnerabilities. CodeLlama-13B\nachieved the highestVS rate, followed by Llama2-13B, and theirVF ratio ranking\nis third and second respectively, which is a good result for the Llama family. Still, it\nis best to remember that nearly half of their samples had vulnerabilities. Moreover,\ntheir VU is fairly high at30% and 32%, which means that with further verification,\nthere is still a chance that other models will take the lead.\nTable 12: Verification Results Summary, Sorted by Average Property Violation\nper Line.\nCategory\nAvg Prop.\nViol.\nper Line\nRank VS Rank VF VU\n(Timeout)\nAvg Prop.\nViol.\nper File\nGPT-4o-mini 0.0165 3 4.23% 2 57.14% 36.77% 3.40\nLlama2-13B 0.0234 2 12.36% 1 51.30% 31.78% 3.62\nMistral-7B 0.0254 7 8.36% 4 62.08% 25.88% 3.07\nCodeLlama-13B 0.0260 1 15.48% 3 52.71% 29.52% 4.13\nFalcon-180B 0.0291 8 6.48% 5 62.07% 28.67% 3.38\nGPT-3.5-turbo 0.0295 6 7.29% 7 65.07% 26.09% 4.42\nGemini Pro 1.0 0.0305 5 9.49% 6 63.91% 24.13% 4.70\nGemma-7B 0.0437 4 11.62% 8 67.01% 16.30% 4.20\nLegend:\nVS: Verification Success;VF: Verification Failed;VU: Verification Unknown (Timeout).\nBest performance in a category is highlighted with bold and/or Rank.\n31\nGPT-4o-mini outperforms GPT-3.5-turbo while showing the highestVU percent-\nage under current ESBMC settings, indicating its ability to produce more complex and\nlonger outputs. It is important to note that this complexity is not reflected by the CC\nnumber as discussed earlier, which confirms the criticism towards Cyclomatic Com-\nplexity by practitioners. While GPT-4o-mini ranks third inVSand second inVF, it\nfinishes first with an average property violation per line. This might be the fairest way\nto compare models, as the more lines, the more chances to have vulnerabilities, while\nthis metric doesn’t punish models producing shorter codes. While there is no defini-\ntive winner in this analysis, Gemma-7B, Gemini-Pro, and GPT-3.5-turbo—with the\ncurrent verification settings— have the highestVF ratios and highest average prop-\nerty violation both per line and file which indicates that these models are performing\nworse in our test.\nIt is important to underline that it might be tempting to speculate on a winner,\nhaving such a high verification failed ratio is unacceptable from an SE perspective for\nany model. All models surpassed theVFthreshold of 50%, indicating that nearly half\nor more of the generated programs are vulnerable. The conclusions of this analysis\nmust be clear:Using code generated by the state-of-the-art Large Language\nModels, without any additional framework for validation and vulnerability\nanalysis, carries severe risks.While LLMs can be useful for automating simple\ntasksandscripting,directlyincluding such codes in production software without\noversight from experienced software engineers is irresponsible and should\nbe avoided.\n7 Limitations and Future Research\n7.1 Future Research Directions\nThe dataset, consisting of331 000 C program files and their corresponding vulnera-\nbility classifications, is available on GitHub6. The dataset is well-suited for machine\nlearning applications and fine-tuning LLMs due to its large size. Moreover, the diverse\nstructure of the C programs generated by various LLMs in the FormAI-v2 dataset\nmakes it ideal for an unexpected use case: fuzzing different applications. We dis-\ncovered and reported seventeen bugs in the ESBMC application, including issues\nrelated to unsigned overflow checks, SMT solver problems, conversion errors in the\nGOTO converter, and flaws in implementing thek-induction proof rule. Furthermore,\nwe identified bugs in the CBMC [116] tool while using the FormAI-v2 dataset and\npromptly communicated these findings to the respective developers. After validating\nthe reported issues, the ESBMC developers have already resolved thirteen.\nOur results give rise to several interesting research directions:\n• It would be important to investigate why programs under “Verification Successful”\nare void of vulnerabilities. Is it because of better coding practices or simply because,\nfor example, they don’t take user input, thereby avoiding buffer overflows?\n6https://github.com/FormAI-Dataset\n32\n• What is the right path towards LLMs producing secure code: Re-training models\non better data, fine-tuning, or using current models in various few-shot frameworks\nwith better prompting?\n• Since several codes contain multiple vulnerabilities, this dataset is ideal for bench-\nmarking and testing various vulnerability detection tools.\n• As our motivation section showcased, GPT-4o-mini did not excel at avoiding\nand fixing the vulnerability in the example. How do different LLMs compare in\nunderstanding, correctly fixing, and detecting coding errors?\n• We aim further to grow the FormAI dataset, including more state-of-the-art models,\nand increase the number of samples for each LLM to have an overall larger dataset.\n• How do different programming Tasks or Styles impact vulnerable coding patterns?\nAre there tasks that LLMs consistently mess up?\nWhile we can partially address the last question, noting the use of insecure func-\ntions and poor input sanitation in handling user inputs, exploring this issue across\nvarious domains, such as networking or cryptography, would be beneficial.\n7.2 Limitations and Threats to Validity\nESBMC might find slightly more vulnerabilities in a given program with a larger\ntimeout setting. Whether the verifier can finish the process under a given timeout is\nup to the available computational capacity. The same parameter setting can yield a\nhigherorlowerdetectionrateondifferentarchitectures.Tofindallerrorsdetectableby\nESBMC, unwind must be set to infinite, and ESMBC must complete the verification\nprocess. As we provided the original C programs and the instructions on how to run\nESBMC, researchers who invest additional computational resources have the potential\nto enhance our findings. As the “Verification Unknown” category still contains samples\nfor every model, the current results are strongly bound to the percentage of vulnerable\nfiles LLMs produce.\nWhile ESBMC is a robust tool for detecting many types of errors in C, it is not\ncurrently suited to detect design flaws, semantic errors, or performance issues. As\nsuch, more vulnerabilities might be present besides the ones detected in the code.\nThus, we recommend that the training and fine-tuning applications be restricted to\nthe vulnerabilities detectable by ESBMC on this dataset.\nAll programs shorter than 50 lines were removed from the dataset. However,\nresearchers interested in smaller programs can still find all programs under 50 lines,\ngenerated by GPT-3.5-turbo, in the original FormAI-v1 dataset.\n8 Conclusions\nThis research analyzed nine state-of-the-art Large Language Models to assess their\nlikelihood of introducing vulnerabilities during neutral prompt-based code genera-\ntion, and to compare their performance. The models included in our analysis were\nMistral-7B, Falcon-180B, Falcon2-11B GPT-4o-mini, Llama2-13B, CodeLlama-13B,\nGemma-7B, GPT-3.5-turbo, and Gemini-Pro. We employed a zero-shot prompt-\ning method to encompass numerous programming scenarios for C code generation.\n33\nThese programs constitute the FormAI-v2 dataset, containing331 000 independent\ncompilable C programs.\nWe used the Efficient SMT-based Bounded Model Checker (ESBMC), a state-of-\nthe-art formal verification tool, to identify vulnerabilities. Each program was given\na verification period of 500 seconds with the unwinding parameter set to infinite,\nuncovering a total of765 366 vulnerabilities. Overall 62.07% of the codes were vul-\nnerable. Detailed labeling of each sample—including filename, type of vulnerability,\nfunction name, error type, and source code—is documented in a .json file, as detailed\nin Appendix Fig. 1, to facilitate the dataset’s use in machine learning applications.\nAdditionally, the FormAI-v2 dataset proved instrumental for fuzzing various appli-\ncations and identifying multiple bugs in ESBMC and CBMC. These findings provide\nclear answers to our research questions:\n• RQ1: How does the security of LLM-generated code differ across various\nmodels?\n• Answer: CodeLlama-13B, Llama-13B, and GPT-4o-mini perform slightly\nbetter,butallexaminedmodelsnotoriouslyintroducevulnerabilitiesintothe\nC code they generate at unacceptable rates. Our research revealed that all\nexamined models introduced vulnerabilities in at least 50% of the generated\ncode.\n• RQ2: What are the most typical vulnerabilities introduced by different\nLLMs during code generation (focusing on C)?\n• Answer: Dereference failures and buffer overflow issues are the most preva-\nlentvulnerabilitiesacrossallmodels,rankingarithmeticoverflowasthethird\nmost common type. No model is completely free from any of the examined\nvulnerabilities; the variations lie in the frequency of occurrence.\nWhile the literature reveals significant variations in these models’ ability to solve\ntasks, this is not mirrored in their susceptibility to produce vulnerabilities in source\ncode. Our findings conclusively show that despite differences among the examined\nmodels in terms of generating code, they all consistently introduce severe vulnera-\nbilities when prompted with simple coding tasks. Our study indicates that despite\nthe impressive capabilities of Large Language Models in code generation, employing\ntheir output in production requires detailed risk assessment. Relying on these models\nwithout expert oversight in a production context is inadvisable.\nAcknowledgement\nWe extend our sincere thanks to the anonymous reviewers for their valuable feedback,\nwhich has significantly improved the quality of this paper. This research is supported\nby the Technology Innovation Institute (TII), Abu Dhabi. Additionally, partial sup-\nport is provided by the EPSRC grant EP/T026995/1, titled “EnnCore: End-to-End\n34\nConceptual Guarding of Neural Architectures” under the Security for All in an AI-\nenabled Society initiative. This work is also partially supported by the TKP2021-NVA\nFunding Scheme under Project TKP2021-NVA-29.\nData Availability Statements\nThis study generated and examined a total of331 000 C samples. The findings and\nall the generated C samples are available for access and download from the project’s\nwebsite at https://github.com/FormAI-Dataset.\nConflicts of interest\nThe authors have no competing interests to declare that are relevant to the content\nof this article.\nReferences\n[1] Wang, J., Huang, Y., Chen, C., Liu, Z., Wang, S., Wang, Q.: Software testing\nwith large language models: Survey, landscape, and vision. IEEE Transactions\non Software Engineering (2024)\n[2] Xu, F.F., Alon, U., Neubig, G., Hellendoorn, V.J.: A systematic evaluation\nof large language models of code. In: Proceedings of the 6th ACM SIGPLAN\nInternational Symposium on Machine Programming, pp. 1–10 (2022)\n[3] Jain, N., Vaidyanath, S., Iyer, A., Natarajan, N., Parthasarathy, S., Rajamani,\nS., Sharma, R.: Jigsaw: Large language models meet program synthesis. In:\nProceedings of the 44th International Conference on Software Engineering, pp.\n1219–1231 (2022)\n[4] Bui, N.D.Q., Le, H., Wang, Y., Li, J., Gotmare, A.D., Hoi, S.C.H.: CodeTF:\nOne-stop Transformer Library for State-of-the-art Code LLM. arXiv (2023).\nhttp://arxiv.org/abs/2306.00029 Accessed 2023-06-22\n[5] Ross, S.I., Martinez, F., Houde, S., Muller, M., Weisz, J.D.: The Programmer’s\nAssistant: Conversational Interaction with a Large Language Model for Software\nDevelopment.In:Proceedingsofthe28thInternationalConferenceonIntelligent\nUser Interfaces. IUI ’23, pp. 491–514. Association for Computing Machinery,\nNew York, NY, USA (2023).https://dl.acm.org/doi/10.1145/3581641.3584037\nAccessed 2023-06-22\n[6] Chavez, M.R., Butler, T.S., Rekawek, P., Heo, H., Kinzler, W.L.: Chat Genera-\ntive Pre-trained Transformer: why we should embrace this technology. American\nJournal of Obstetrics and Gynecology228(6), 706–711 (2023) https://doi.org/\n10.1016/j.ajog.2023.03.010 . Accessed 2023-06-22\n35\n[7] Charalambous, Y., Tihanyi, N., Jain, R., Sun, Y., Ferrag, M.A., Cordeiro, L.C.:\nA New Era in Software Security: Towards Self-Healing Software via Large Lan-\nguageModelsand Formal Verification.arXiv (2023).http://arxiv.org/abs/2305.\n14752 Accessed 2023-05-31\n[8] Perry, N., Srivastava, M., Kumar, D., Boneh, D.: Do users write more inse-\ncure code with ai assistants? In: Proceedings of the 2023 ACM SIGSAC\nConference on Computer and Communications Security. CCS ’23, pp. 2785–\n2799. Association for Computing Machinery, New York, NY, USA (2023).\nhttps://doi.org/10.1145/3576915.3623157\n[9] Tihanyi, N., Bisztray, T., Jain, R., Ferrag, M.A., Cordeiro, L.C., Mavroeidis,\nV.: The formai dataset: Generative ai in software security through the lens\nof formal verification. In: Proceedings of the 19th International Conference on\nPredictiveModelsandDataAnalyticsinSoftwareEngineering.PROMISE2023,\npp. 33–43. Association for Computing Machinery, New York, NY, USA (2023).\nhttps://doi.org/10.1145/3617555.3617874\n[10] Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R.,\nSchalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable\nmultimodal models. arXiv preprint arXiv:2312.11805 (2023)\n[11] Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah,\nM., Goffinet, É., Hesslow, D., Launay, J., Malartic, Q., et al.: The falcon series\nof open language models. arXiv preprint arXiv:2311.16867 (2023)\n[12] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.E., Adi, Y.,\nLiu, J., Remez, T., Rapin, J., et al.: Code llama: Open foundation models for\ncode. arXiv preprint arXiv:2308.12950 (2023)\n[13] Gadelha, M.Y.R., Ismail, H.I., Cordeiro, L.C.: Handling loops in bounded model\nchecking of C programs via k-induction. Int. J. Softw. Tools Technol. Transf.\n19(1), 97–114 (2017) https://doi.org/10.1007/s10009-015-0407-9\n[14] Gadelha, M.R., Monteiro, F.R., Morse, J., Cordeiro, L.C., Fischer, B., Nicole,\nD.A.: Esbmc 5.0: an industrial-strength c model checker. In: Proceedings of the\n33rdACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering,\npp. 888–891. ACM, Montpellier, France (2018)\n[15] Gadelha, M.Y.R., Monteiro, F.R., Cordeiro, L.C., Nicole, D.A.: ESBMC v6.0:\nVerifying C programs using k-induction and invariant inference - (competition\ncontribution). In: Beyer, D., Huisman, M., Kordon, F., Steffen, B. (eds.) Tools\nand Algorithms for the Construction and Analysis of Systems (TACAS). LNCS,\nvol. 11429, pp. 209–213 (2019). Springer\n[16] Menezes, R.S., Aldughaim, M., Farias, B., Li, X., Manino, E., Shmarov, F.,\nSong, K., Brauße, F., Gadelha, M.R., Tihanyi, N., Korovin, K., Cordeiro, L.C.:\n36\nESBMC v7.4: Harnessing the power of intervals - (competition contribution). In:\nTools and Algorithms for the Construction and Analysis of Systems (TACAS).\nLNCS, vol. 14572, pp. 376–380 (2024). Springer\n[17] McCabe, T.J.: A complexity measure. IEEE Transactions on Software Engineer-\ning SE-2(4), 308–320 (1976) https://doi.org/10.1109/TSE.1976.233837\n[18] Cordy, J.R., Roy, C.K.: The nicad clone detector. 2011 IEEE 19th International\nConference on Program Comprehension, 219–220 (2011)\n[19] Chen, M., Tworek, J., Jun, H., Yuan, Q., Oliveira Pinto, H.P., Kaplan, J.,\nEdwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger,\nG., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder,\nN., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such,\nF.P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A.,\nGuss, W.H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji,\nS., Jain, S., Saunders, W., Hesse, C., Carr, A.N., Leike, J., Achiam, J., Misra, V.,\nMorikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K.,\nWelinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., Zaremba,\nW.: Evaluating large language models trained on code (2021) arXiv:2107.03374\n[cs.LG]\n[20] OpenAI: GPT-4 Technical Report. arXiv (2023). http://arxiv.org/abs/2303.\n08774 Accessed 2023-05-29\n[21] Nehorai, N.: Analyzing Common Vulnerabilities Introduced by\nCode-Generative AI | HackerNoon (2024). https://hackernoon.com/\nanalyzing-common-vulnerabilities-introduced-by-code-generative-ai Accessed\n2024-02-28\n[22] Cordeiro, L.C., Lima Filho, E.B., Bessa, I.V.: Survey on automated symbolic\nverification and its application for synthesising cyber-physical systems. IET\nCyper-Phys. Syst.: Theory & Appl.5(1), 1–24 (2020) https://doi.org/10.1049/\nIET-CPS.2018.5006\n[23] Anwar, U., Saparov, A., Rando, J., Paleka, D., Turpin, M., Hase, P., Lubana,\nE.S., Jenner, E., Casper, S., Sourbut, O., et al.: Foundational challenges\nin assuring alignment and safety of large language models. arXiv preprint\narXiv:2404.09932 (2024)\n[24] Kirova, V.D., Ku, C.S., Laracy, J.R., Marlowe, T.J.: Software engineering edu-\ncation must adapt and evolve for an llm environment. In: Proceedings of the\n55th ACM Technical Symposium on Computer Science Education V. 1. SIGCSE\n2024, pp. 666–672. Association for Computing Machinery, New York, NY, USA\n(2024). https://doi.org/10.1145/3626252.3630927\n[25] Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang,\n37\nE., Cai, C., Terry, M., Le, Q., et al.: Program synthesis with large language\nmodels (2021)\n[26] Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C.,\nDrain, D., Jiang, D., Tang, D., et al.: Codexglue: A machine learning benchmark\ndataset for code understanding and generation. arXiv preprint arXiv:2102.04664\n(2021)\n[27] White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A.,\nSpencer-Smith, J., Schmidt, D.C.: A prompt pattern catalog to enhance prompt\nengineering with chatgpt. arXiv preprint arXiv:2302.11382 (2023)\n[28] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., Narasimhan, K.:\nTree of thoughts: Deliberate problem solving with large language models. In:\nAdvances in Neural Information Processing Systems, vol. 36 (2024)\n[29] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,\nD.: Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems35, 24824–24837 (2022)\n[30] Tihanyi, N., Bisztray, T., Dubniczky, R.A., Toth, R., Borsos, B., Cherif, B.,\nFerrag, M.A., Muzsai, L., Jain, R., Marinelli, R., et al.: Dynamic intelligence\nassessment: Benchmarking llms on the road to agi with a focus on model\nconfidence. arXiv preprint arXiv:2410.15490 (2024)\n[31] Mirzadeh, I., Alizadeh, K., Shahrokhi, H., Tuzel, O., Bengio, S., Farajtabar,\nM.: Gsm-symbolic: Understanding the limitations of mathematical reasoning in\nlarge language models. arXiv preprint arXiv:2410.05229 (2024)\n[32] Honarvar, S., Wilk, M., Donaldson, A.: Turbulence: Systematically and auto-\nmatically testing instruction-tuned large language models for code. arXiv\npreprint arXiv:2312.14856 (2023)\n[33] Wang, S., Long, Z., Fan, Z., Wei, Z., Huang, X.: Benchmark self-\nevolving: A multi-agent framework for dynamic llm evaluation. arXiv preprint\narXiv:2402.11443 (2024)\n[34] Liang, X., Song, S., Zheng, Z., Wang, H., Yu, Q., Li, X., Li, R.-H., Xiong, F., Li,\nZ.: Internal consistency and self-feedback in large language models: A survey.\narXiv preprint arXiv:2407.14507 (2024)\n[35] Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X.,\nWu, Y., Li, Y., et al.: Deepseek-coder: When the large language model meets\nprogramming–the rise of code intelligence. arXiv preprint arXiv:2401.14196\n(2024)\n[36] Wang, H., Liu, Z., Wang, S., Cui, G., Ding, N., Liu, Z., Yu, G.: Intervenor:\n38\nPrompt the coding ability of large language models with the interactive chain\nof repairing. arXiv preprint arXiv:2311.09868 (2023)\n[37] Huang, D., Bu, Q., Zhang, J.M., Luck, M., Cui, H.: Agentcoder: Multi-agent-\nbased code generation with iterative testing and optimisation. arXiv preprint\narXiv:2312.13010 (2023)\n[38] Muennighoff, N., Liu, Q., Zebaze, A., Zheng, Q., Hui, B., Zhuo, T.Y., Singh, S.,\nTang, X., Von Werra, L., Longpre, S.: Octopack: Instruction tuning code large\nlanguage models. arXiv preprint arXiv:2308.07124 (2023)\n[39] Lin, F., Kim, D.J., et al.: When llm-based code generation meets the software\ndevelopment process. arXiv preprint arXiv:2403.15852 (2024)\n[40] Khoury, R., Avila, A.R., Brunelle, J., Camara, B.M.: How secure is code gener-\nated by chatgpt? In: 2023 IEEE International Conference on Systems, Man, and\nCybernetics (SMC), pp. 2445–2451 (2023). https://doi.org/10.1109/SMC53992.\n2023.10394237\n[41] Pearce, H., Ahmad, B., Tan, B., Dolan-Gavitt, B., Karri, R.: Asleep at the\nkeyboard? assessing the security of github copilot’s code contributions. In: 2022\nIEEE Symposium on Security and Privacy (SP), pp. 754–768. IEEE, ??? (2022)\n[42] Ma, W., Liu, S., Wang, W., Hu, Q., Liu, Y., Zhang, C., Nie, L., Liu, Y.: The\nScope of ChatGPT in Software Engineering: A Thorough Investigation. arXiv\n(2023). http://arxiv.org/abs/2305.12138 Accessed 2023-06-10\n[43] Imani,S.,Du,L.,Shrivastava,H.:Mathprompter:Mathematicalreasoningusing\nlarge language models (2023). https://doi.org/10.48550/arXiv.2303.05398\n[44] Hou, X., Zhao, Y., Liu, Y., Yang, Z., Wang, K., Li, L., Luo, X., Lo, D., Grundy,\nJ., Wang, H.: Large language models for software engineering: A systematic\nliterature review. ACM Transactions on Software Engineering and Methodology\n(2023)\n[45] Chan, A., Kharkar, A., Moghaddam, R.Z., Mohylevskyy, Y., Helyar, A., Kamal,\nE., Elkamhawy, M., Sundaresan, N.: Transformer-based vulnerability detec-\ntion in code at edittime: Zero-shot, few-shot, or fine-tuning? arXiv preprint\narXiv:2306.01754 (2023)\n[46] Nguyen, V., Yuan, X., Wu, T., Nepal, S., Grobler, M., Rudolph, C.: Deep\nlearning-based out-of-distribution source code data identification: How far we\nhave gone? arXiv preprint arXiv:2404.05964 (2024)\n[47] Gao, Z., Wang, H., Zhou, Y., Zhu, W., Zhang, C.: How far have we\ngone in vulnerability detection using large language models. arXiv preprint\narXiv:2311.12420 (2023)\n39\n[48] Gao, S., Mao, W., Gao, C., Li, L., Hu, X., Xia, X., Lyu, M.R.: Learning in the\nwild: Towards leveraging unlabeled data for effectively tuning pre-trained code\nmodels. In: Proceedings of the IEEE/ACM 46th International Conference on\nSoftware Engineering, pp. 1–13 (2024)\n[49] Grishina,A.,Hort,M.,Moonen,L.:Theearlybirdcatchesthebug:Onexploiting\nearly layers of encoder models for more efficient code classification. In: Proceed-\nings of the 31st ACM Joint European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering, pp. 895–907 (2023)\n[50] Khare,A.,Dutta,S.,Li,Z.,Solko-Breslin,A.,Alur,R.,Naik,M.:Understanding\nthe effectiveness of large language models in detecting security vulnerabilities.\narXiv preprint arXiv:2311.16169 (2023)\n[51] Noever, D.: Can large language models find and fix vulnerable software? arXiv\npreprint arXiv:2308.10345 (2023)\n[52] Shestov, A., Cheshkov, A., Levichev, R., Mussabayev, R., Zadorozhny, P.,\nMaslov, E., Vadim, C., Bulychev, E.: Finetuning large language models for\nvulnerability detection. arXiv preprint arXiv:2401.17010 (2024)\n[53] Steenhoek, B., Gao, H., Le, W.: Dataflow analysis-inspired deep learn-\ning for efficient vulnerability detection. In: Proceedings of the IEEE/ACM\n46th International Conference on Software Engineering. ICSE ’24.\nAssociation for Computing Machinery, New York, NY, USA (2024).\nhttps://doi.org/10.1145/3597503.3623345\n[54] Sun,Y.,Wu,D.,Xue,Y.,Liu,H.,Ma,W.,Zhang,L.,Shi,M.,Liu,Y.:Llm4vuln:\nA unified evaluation framework for decoupling and enhancing llms’ vulnerability\nreasoning. arXiv preprint arXiv:2401.16185 (2024)\n[55] Tang, W., Tang, M., Ban, M., Zhao, Z., Feng, M.: Csgvd: A deep learning\napproach combining sequence and graph embedding for source code vulnerabil-\nity detection. J. Syst. Softw.199(C) (2023) https://doi.org/10.1016/j.jss.2023.\n111623\n[56] Thapa, C., Jang, S.I., Ahmed, M.E., Camtepe, S., Pieprzyk, J., Nepal, S.:\nTransformer-based language models for software vulnerability detection. In:\nProceedings of the 38th Annual Computer Security Applications Conference.\nACSAC ’22, pp. 481–496. Association for Computing Machinery, New York,\nNY, USA (2022).https://doi.org/10.1145/3564625.3567985\n[57] Zhang, C., Liu, H., Zeng, J., Yang, K., Li, Y., Li, H.: Prompt-enhanced software\nvulnerability detection using chatgpt. arXiv preprint arXiv:2308.12697 (2023)\n[58] Tóth, R., Bisztray, T., Erdődi, L.: Llms in web development: Evaluating llm-\ngenerated php code unveiling vulnerabilities and limitations. In: Computer\n40\nSafety, Reliability, and Security. SAFECOMP 2024 Workshops, pp. 425–437.\nSpringer, Cham (2024)\n[59] Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., Anderson, R.:\nThe Curse of Recursion: Training on Generated Data Makes Models Forget.\narXiv (2023). http://arxiv.org/abs/2305.17493 Accessed 2023-06-27\n[60] Chen, Y., Ding, Z., Alowain, L., Chen, X., Wagner, D.: DiverseVul: A\nNew Vulnerable Source Code Dataset for Deep Learning Based Vulner-\nability Detection. In: Proceedings of the 26th International Symposium\non Research in Attacks, Intrusions and Defenses. RAID ’23, pp. 654–\n668. Association for Computing Machinery, New York, NY, USA (2023).\nhttps://doi.org/10.1145/3607199.3607242\n[61] Fan, J., Li, Y., Wang, S., Nguyen, T.N.: A C/C++ Code Vulnerability\nDataset with Code Changes and CVE Summaries. In: Proceedings of the\n17th International Conference on Mining Software Repositories. MSR ’20, pp.\n508–512. Association for Computing Machinery, New York, NY, USA (2020).\nhttps://doi.org/10.1145/3379597.3387501 Accessed 2023-06-27\n[62] Russell, R.L., Kim, L.Y., Hamilton, L.H., Lazovich, T., Harer, J.A., Ozdemir,\nO., Ellingwood, P.M., McConley, M.W.: Automated Vulnerability Detection\nin Source Code Using Deep Representation Learning. In: 2018 17th IEEE\nInternational Conference on Machine Learning and Applications (ICMLA), pp.\n757–762. IEEE, Orlando, FL, USA (2018). https://doi.org/10.1109/ICMLA.\n2018.00120\n[63] Kim, L., Russell, R.: Draper VDISC Dataset - Vulnerability Detection in Source\nCode. Publisher: OSF (2018). https://osf.io/d45bw/ Accessed 2023-06-27\n[64] Black, P.E.: A Software Assurance Reference Dataset: Thousands of Programs\nWith Known Bugs. Journal of Research of the National Institute of Standards\nandTechnology 123,1–3 (2018)https://doi.org/10.6028/jres.123.005. Accessed\n2023-06-27\n[65] Jr, F.E.B., Black, P.E.: The Juliet 1.1 C/C++ and Java Test Suite. NIST\n45(10), 88–90 (2012). Last Modified: 2021-10-12T11:10-04:00 Publisher: Fred-\nerick E. Boland Jr., Paul E. Black. Accessed 2023-05-28\n[66] Zhou, Y., Liu, S., Siow, J., Du, X., Liu, Y.: Devign: Effective Vulnerability\nIdentification by Learning Comprehensive Program Semantics via Graph Neural\nNetworks,pp.10197–10207.CurranAssociatesInc.,RedHook,NY,USA(2019)\n[67] Chakraborty, S., Krishna, R., Ding, Y., Ray, B.: Deep Learning Based Vulnera-\nbilityDetection:AreWeThereYet?IEEETransactionsonSoftwareEngineering\n48(9), 3280–3296 (2022) https://doi.org/10.1109/TSE.2021.3087402\n41\n[68] Jain, R., Gervasoni, N., Ndhlovu, M., Rawat, S.: A code centric evaluation of\nc/c++ vulnerability datasets for deep learning based vulnerability detection\ntechniques. In: Proceedings of the 16th Innovations in Software Engineering\nConference, pp. 1–10. ACM, Prayagraj, India (2023)\n[69] Daniel Marjamäki: Cppcheck: A Tool for Static Analysis of C/C++ Code.\nhttps://cppcheck.sourceforge.io/. [Online], Available at: https://cppcheck.\nsourceforge.io/ (Accessed: 12 September 2024) (2024)\n[70] Cordeiro, L., Fischer, B., Marques-Silva, J.: SMT-Based Bounded Model\nChecking for Embedded ANSI-C Software. IEEE Transactions on Software\nEngineering 38(4), 957–974 (2012) https://doi.org/10.1109/TSE.2011.59\n[71] D’Silva, V., Kroening, D., Weissenbacher, G.: A Survey of Automated Tech-\nniques for Formal Software Verification. IEEE Transactions on Computer-Aided\nDesign of Integrated Circuits and Systems 27(7), 1165–1178 (2008) https:\n//doi.org/10.1109/TCAD.2008.923410\n[72] Morse, J., Cordeiro, L.C., Nicole, D.A., Fischer, B.: Context-bounded model\nchecking of LTL properties for ANSI-C software. In: Barthe, G., Pardo, A.,\nSchneider, G. (eds.) Software Engineering and Formal Methods - 9th Interna-\ntional Conference, SEFM 2011, Montevideo, Uruguay, November 14-18, 2011.\nProceedings. Lecture Notes in Computer Science, vol. 7041, pp. 302–317 (2011).\nSpringer\n[73] Wallace, D.R., Fujii, R.U.: Software verification and validation: an overview.\nIEEE Software6(3), 10–17 (1989) https://doi.org/10.1109/52.28119 . Accessed\n2023-06-22\n[74] Alshmrany, K.M., Aldughaim, M., Bhayat, A., Cordeiro, L.C.: Fusebmc: An\nenergy-efficient test generator for finding security vulnerabilities in C programs.\nIn: Loulergue, F., Wotawa, F. (eds.) Tests and Proofs - 15th International Con-\nference, TAP 2021, Held as Part of STAF 2021, Virtual Event, June 21-22, 2021,\nProceedings. Lecture Notes in Computer Science, vol. 12740, pp. 85–105 (2021).\nSpringer\n[75] Braberman, V.A., Bonomo-Braberman, F., Charalambous, Y., Colonna, J.G.,\nCordeiro, L.C., Freitas, R.: Tasks People Prompt: A Taxonomy of LLM\nDownstream Tasks in Software Verification and Falsification Approaches (2024)\n[76] Hao, Y., Chen, W., Zhou, Z., Cui, W.: E&v: Prompting large language models to\nperformstaticanalysisbypseudo-codeexecutionandverification.arXivpreprint\narXiv:2312.08477 (2023)\n[77] Yang, A.Z., Le Goues, C., Martins, R., Hellendoorn, V.: Large language mod-\nels for test-free fault localization. In: Proceedings of the 46th IEEE/ACM\nInternational Conference on Software Engineering, pp. 1–12 (2024)\n42\n[78] Quan, V.L.A., Phat, C.T., Van Nguyen, K., Duy, P.T., Pham, V.-H.: Xgv-bert:\nLeveraging contextualized language model and graph neural network for efficient\nsoftware vulnerability detection. arXiv preprint arXiv:2309.14677 (2023)\n[79] Sun, T., Allix, K., Kim, K., Zhou, X., Kim, D., Lo, D., Bissyandé, T.F., Klein,\nJ.: Dexbert: Effective, task-agnostic and fine-grained representation learning of\nandroid bytecode. IEEE Transactions on Software Engineering49(10), 4691–\n4706 (2023) https://doi.org/10.1109/TSE.2023.3310874\n[80] Tian, H., Liu, K., Li, Y., Kaboré, A.K., Koyuncu, A., Habib, A., Li, L., Wen, J.,\nKlein, J., Bissyandé, T.F.: The best of both worlds: Combining learned embed-\ndings with engineered features for accurate prediction of correct patches. ACM\nTrans. Softw. Eng. Methodol.32(4) (2023) https://doi.org/10.1145/3576039\n[81] Wang, W., Wang, Y., Joty, S., Hoi, S.C.H.: Rap-gen: Retrieval-augmented\npatch generation with codet5 for automatic program repair. In: Proceedings\nof the 31st ACM Joint European Software Engineering Conference and Sym-\nposium on the Foundations of Software Engineering. ESEC/FSE 2023, pp.\n146–158. Association for Computing Machinery, New York, NY, USA (2023).\nhttps://doi.org/10.1145/3611643.3616256\n[82] Zhang, Y., Jin, Z., Xing, Y., Li, G.: Steam: simulating the interactive behavior of\nprogrammers for automatic bug fixing. arXiv preprint arXiv:2308.14460 (2023)\n[83] Wu,Y.,Li,Z.,Zhang,J.M.,Papadakis,M.,Harman,M.,Liu,Y.:Largelanguage\nmodels in fault localisation. arXiv preprint arXiv:2308.15276 (2023)\n[84] Mohajer, M.M., Aleithan, R., Harzevili, N.S., Wei, M., Belle, A.B., Pham,\nH.V., Wang, S.: Skipanalyzer: An embodied agent for code analysis with large\nlanguage models. arXiv preprint arXiv:2310.18532 (2023)\n[85] Li, T.-O., Zong, W., Wang, Y., Tian, H., Wang, Y., Cheung, S.-C.: Finding\nFailure-Inducing Test Cases with ChatGPT (2023)\n[86] Pearce, H., Tan, B., Ahmad, B., Karri, R., Dolan-Gavitt, B.: Examining zero-\nshot vulnerability repair with large language models. In: 2023 IEEE Symposium\non Security and Privacy (SP), pp. 2339–2356. IEEE, ??? (2023)\n[87] Cao, J., Li, M., Wen, M., Cheung, S.-c.: A study on prompt design, advantages\nand limitations of chatgpt for deep learning program repair. arXiv preprint\narXiv:2304.08191 (2023)\n[88] Deligiannis,P.,Lal,A.,Mehrotra,N.,Rastogi,A.:Fixingrustcompilationerrors\nusing llms. arXiv preprint arXiv:2308.05177 (2023)\n[89] Fan, Z., Gao, X., Mirchev, M., Roychoudhury, A., Tan, S.H.: Automated repair\nofprogramsfromlargelanguagemodels.In:2023IEEE/ACM45thInternational\n43\nConference on Software Engineering (ICSE), pp. 1469–1481 (2023). IEEE\n[90] Huang, Q., Zhu, J., Xing, Z., Jin, H., Wang, C., Xu, X.: A chain of ai-based solu-\ntions for resolving fqns and fixing syntax errors in partial code. arXiv preprint\narXiv:2306.11981 (2023)\n[91] Islam,N.T.,Najafirad,P.:Codesecurityvulnerabilityrepairusingreinforcement\nlearning with large language models. arXiv preprint arXiv:2401.07031 (2024)\n[92] Jin, M., Shahriar, S., Tufano, M., Shi, X., Lu, S., Sundaresan, N., Svyatkovskiy,\nA.: Inferfix: End-to-end program repair with llms. In: Proceedings of the 31st\nACM Joint European Software Engineering Conference and Symposium on the\nFoundations of Software Engineering, pp. 1646–1656 (2023)\n[93] Lajkó, M., Csuvik, V., Vidács, L.: Towards javascript program repair with gener-\native pre-trained transformer (gpt-2). In: Proceedings of the Third International\nWorkshop on Automated Program Repair, pp. 61–68. IEEE, ??? (2022)\n[94] Paul, R., Mohib Hossain, M., Hasan, M., Iqbal, A.: Automated program repair\nbased on code review: How do pre-trained transformer models perform? arXiv\ne-prints, 2304 (2023)\n[95] Peng, Y., Gao, S., Gao, C., Huo, Y., Lyu, M.: Domain knowledge matters:\nImproving prompts with fix templates for repairing python type errors. In:\nProceedings of the IEEE/ACM 46th International Conference on Software Engi-\nneering. ICSE ’24. Association for Computing Machinery, New York, NY, USA\n(2024). https://doi.org/10.1145/3597503.3608132\n[96] Tian, H., Liu, K., Kaboré, A.K., Koyuncu, A., Li, L., Klein, J., Bissyandé,\nT.F.: Evaluating representation learning of code changes for predicting patch\ncorrectness in program repair. In: Proceedings of the 35th IEEE/ACM Inter-\nnational Conference on Automated Software Engineering. ASE ’20, pp. 981–\n992. Association for Computing Machinery, New York, NY, USA (2021).\nhttps://doi.org/10.1145/3324884.3416532\n[97] Wei, Y., Xia, C.S., Zhang, L.: Copiloting the copilots: Fusing large language\nmodels with completion engines for automated program repair. In: Proceed-\nings of the 31st ACM Joint European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering. ESEC/FSE 2023, pp.\n172–184. Association for Computing Machinery, New York, NY, USA (2023).\nhttps://doi.org/10.1145/3611643.3616271\n[98] Widjojo, P., Treude, C.: Addressing compiler errors: Stack overflow or large\nlanguage models? arXiv preprint arXiv:2307.10793 (2023)\n[99] Xia, C.S., Wei, Y., Zhang, L.: Practical program repair in the era of large pre-\ntrained language models. arXiv preprint arXiv:2210.14179 (2022)\n44\n[100] Xia, C.S., Zhang, L.: Keep the conversation going: Fixing 162 out of 337 bugs\nfor $0.42 each using chatgpt. arXiv preprint arXiv:2304.00385 (2023)\n[101] Zhang, Q., Fang, C., Sun, W., Liu, Y., He, T., Hao, X., Chen, Z.: Appt:\nBoosting automated patch correctness prediction via fine-tuning pre-trained\nmodels. IEEE Transactions on Software Engineering 50(3), 474–494 (2024)\nhttps://doi.org/10.1109/TSE.2024.3354969\n[102] Zhang, Q., Fang, C., Zhang, T., Yu, B., Sun, W., Chen, Z.: Gamma: Revis-\niting template-based automated program repair via mask prediction. In: 2023\n38th IEEE/ACM International Conference on Automated Software Engineering\n(ASE), pp. 535–547 (2023). IEEE\n[103] Zhang, Y., Li, G., Jin, Z., Xing, Y.: Neural program repair with program depen-\ndence analysis and effective filter mechanism. arXiv preprint arXiv:2305.09315\n(2023)\n[104] Wu, Y., Jiang, N., Pham, H.V., Lutellier, T., Davis, J., Tan, L., Babkin, P.,\nShah, S.: How effective are neural networks for fixing security vulnerabilities. In:\nProceedings of the 32nd ACM SIGSOFT International Symposium on Software\nTesting and Analysis, pp. 1282–1294 (2023)\n[105] Gadelha, M.Y.R., Steffinlongo, E., Cordeiro, L.C., Fischer, B., Nicole, D.A.:\nSmt-based refutation of spurious bug reports in the clang static analyzer. In:\nAtlee, J.M., Bultan, T., Whittle, J. (eds.) Proceedings of the 41st International\nConference on Software Engineering, pp. 11–14. IEEE / ACM, Montreal, QC,\nCanada (2019). https://doi.org/10.1109/ICSE-Companion.2019.00026\n[106] Sadowski, C., Yi, J.: How developers use data race detection tools. In: Pro-\nceedings of the 5th Workshop on Evaluation and Usability of Programming\nLanguages and Tools, pp. 43–51. ACM, Portland, USA (2014)\n[107] White, M., Tufano, M., Vendome, C., Poshyvanyk, D.: Deep learning code\nfragmentsforcodeclonedetection.In:Proceedingsofthe31stIEEE/ACMInter-\nnationalConferenceonAutomatedSoftwareEngineering,pp.87–98.Association\nfor Computing Machinery, New York, USA (2016)\n[108] Zhao, G., Huang, J.: Deepsim: deep learning code functional similarity. In: Pro-\nceedingsofthe201826thACMJointMeetingonEuropeanSoftwareEngineering\nConference and Symposium on the Foundations of Software Engineering, pp.\n141–151. ACM, Lake Buena Vista, USA (2018)\n[109] Cordeiro, L.C., Kroening, D., Schrammel, P.: JBMC: bounded model checking\nfor java bytecode - (competition contribution). In: Tools and Algorithms for the\nConstruction and Analysis of Systems (TACAS). LNCS, vol. 11429, pp. 219–223\n(2019). Springer\n45\n[110] Menezes, R., Moura, D., Cavalcante, H., Freitas, R., Cordeiro, L.C.: Esbmc-\njimple: verifying kotlin programs via jimple intermediate representation. In:\nRyu, S., Smaragdakis, Y. (eds.) ISSTA ’22: 31st ACM SIGSOFT International\nSymposium on Software Testing and Analysis, Virtual Event, South Korea, July\n18 - 22, 2022, pp. 777–780 (2022). ACM\n[111] Gadelha, M.R., Monteiro, F.R., Morse, J., Cordeiro, L.C., Fischer, B., Nicole,\nD.A.: Esbmc 5.0: an industrial-strength c model checker. In: Proceedings of the\n33rdACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering.\nASE ’18, pp. 888–891. Association for Computing Machinery, New York, NY,\nUSA (2018).https://doi.org/10.1145/3238147.3240481\n[112] Aho, A.V., Lam, M.S., Sethi, R., Ullman, J.D.: Compilers: Principles, Tech-\nniques, And Tools, 2nd edn. Addison-Wesley Longman Publishing Co., Inc.,\nBoston, MA (2006)\n[113] Beyer, D.: Competition on software verification and witness validation: Sv-comp\n2023. In: Sankaranarayanan, S., Sharygina, N. (eds.) Tools and Algorithms for\nthe Construction and Analysis of Systems, pp. 495–522. Springer, Cham (2023)\n[114] Sandoval, G., Pearce, H., Nys, T., Karri, R., Garg, S., Dolan-Gavitt, B.: Lost\nat c: A user study on the security implications of large language model code\nassistants. In: 32nd USENIX Security Symposium (USENIX Security 23), pp.\n2205–2222 (2023). USENIX Association\n[115] Mikejo5000: Code metrics - Cyclomatic complexity - Visual Studio (Win-\ndows) (2024). https://learn.microsoft.com/en-us/visualstudio/code-quality/\ncode-metrics-cyclomatic-complexity?view=vs-2022 Accessed 2024-04-18\n[116] Kroening, D., Tautschnig, M.: Cbmc–c bounded model checker: (competition\ncontribution). In: Tools and Algorithms for the Construction and Analysis of\nSystems: TACAS 2014, pp. 389–391. Springer, Grenoble, France (2014)\nAppendix\n46\nFig. 1: Example JSON Labels for a GPT-3.5-turbo Generated Sample: FormAI-v2\ndataset\n47",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6792435050010681
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6040246486663818
    },
    {
      "name": "Counterexample",
      "score": 0.5230589509010315
    },
    {
      "name": "Code (set theory)",
      "score": 0.49245962500572205
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4818093180656433
    },
    {
      "name": "False positive paradox",
      "score": 0.4695095717906952
    },
    {
      "name": "Vulnerability (computing)",
      "score": 0.45497724413871765
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.44548267126083374
    },
    {
      "name": "Programming language",
      "score": 0.3913952708244324
    },
    {
      "name": "Computer security",
      "score": 0.31528741121292114
    },
    {
      "name": "Artificial intelligence",
      "score": 0.29911118745803833
    },
    {
      "name": "Geography",
      "score": 0.13348644971847534
    },
    {
      "name": "Statistics",
      "score": 0.11830726265907288
    },
    {
      "name": "Mathematics",
      "score": 0.10681861639022827
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.10204005241394043
    },
    {
      "name": "Cartography",
      "score": 0.0763692855834961
    },
    {
      "name": "Discrete mathematics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": []
}