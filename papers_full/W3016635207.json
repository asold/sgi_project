{
  "title": "Understanding the Difficulty of Training Transformers",
  "url": "https://openalex.org/W3016635207",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2112714395",
      "name": "Liu Li-yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1971393576",
      "name": "Liu Xiaodong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119363152",
      "name": "Gao, Jianfeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2049887450",
      "name": "Chen, Weizhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125747881",
      "name": "Han, Jiawei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2753358588",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3041866211",
    "https://openalex.org/W2964093309",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2896060389",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2796108585",
    "https://openalex.org/W3010768098",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W2952626150",
    "https://openalex.org/W2962933129",
    "https://openalex.org/W2992505801",
    "https://openalex.org/W2962761235",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2963037478",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2964050767",
    "https://openalex.org/W2257408573",
    "https://openalex.org/W2989571009",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W3034465644",
    "https://openalex.org/W2994689640",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2125930537",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W3066373881",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding designing cutting-edge optimizers and learning rate schedulers carefully (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand $\\textit{what complicates Transformer training}$ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially -- for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin ($\\textbf{Ad}$aptive $\\textbf{m}$odel $\\textbf{in}$itialization) to stabilize stabilize the early stage's training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance. Implementations are released at: https://github.com/LiyuanLucasLiu/Transforemr-Clinic.",
  "full_text": "Understanding the Difficulty of Training Transformers\nLiyuan Liu†‡ Xiaodong Liu‡ Jianfeng Gao‡ Weizhu Chen§ Jiawei Han†\n{ll2, hanj}@illinois.edu , {xiaodl,jfgao,wzchen}@microsoft.com\n†University of Illinois at Urbana-Champaign\n‡Microsoft Research\n§ Microsoft Dynamics 365 AI\nAbstract\nTransformers have proved effective in many\nNLP tasks. However, their training requires\nnon-trivial efforts regarding designing cutting-\nedge optimizers and learning rate schedulers\ncarefully (e.g., conventional SGD fails to train\nTransformers effectively). Our objective here\nis to understand what complicates Transformer\ntraining from both empirical and theoretical\nperspectives. Our analysis reveals that unbal-\nanced gradients are not the root cause of the\ninstability of training. Instead, we identify\nan amplification effect that influences training\nsubstantially–for each layer in a multi-layer\nTransformer model, heavy dependency on its\nresidual branch makes training unstable, since\nit amplifies small parameter perturbations (e.g.,\nparameter updates) and results in significant\ndisturbances in the model output. Yet we ob-\nserve that a light dependency limits the model\npotential and leads to inferior trained models.\nInspired by our analysis, we propose Admin\n(Adaptive model initialization) to stabilize the\nearly stage’s training and unleash its full poten-\ntial in the late stage. Extensive experiments\nshow that Admin is more stable, converges\nfaster, and leads to better performance1.\n1 Introduction\nTransformers (Vaswani et al., 2017) have led to\na series of breakthroughs in various deep learn-\ning tasks (Devlin et al., 2019; Velickovic et al.,\n2018). They do not contain recurrent connections\nand can parallelize all computations in the same\nlayer, thus improving effectiveness, efficiency, and\nscalability. Training Transformers, however, re-\nquires extra efforts. For example, although stochas-\ntic gradient descent (SGD) is the standard algo-\nrithm for conventional RNNs and CNNs, it con-\nverges to bad/suspicious local optima for Trans-\n1Implementations are released at: https://github.\ncom/LiyuanLucasLiu/Transforemr-Clinic\n0 50 100\n4.4\n4.6\n4.8\n5.0\nDev PPL on WMT’14 En-De\n18-Layer Transformer\n50 100\n4.6\n4.7\n4.8\n4.9 6-layer Post-LN\nconverges, but\n18-layer Post-LN\ndoes not.\n6-Layer Transformer\nPre-LN\nPost-LN\nAdmin\n(Post-LN)\nEpoch # (iterations over the training set)\nFigure 1: Lacking enough robustness and stability, the\n18-Layer Post-LN Transformer training (i.e.the original\narchitecture) diverges and is omitted in the left graph.\nAdmin not only stabilizes model training but unleashes\nthe model potential for better performance.\nformers (Zhang et al., 2019b). Moreover, com-\nparing to other neural architectures, removing the\nwarmup stage in Transformer training results in\nmore severe consequences such as model diver-\ngence (Popel and Bojar, 2018; Liu et al., 2020a).\nHere, we conduct comprehensive analyses in empir-\nical and theoretical manners to answer the question:\nwhat complicates Transformer training.\nOur analysis starts from the observation: the\noriginal Transformer (referred to as Post-LN) is\nless robust than its Pre-LN variant2 (Baevski and\nAuli, 2019; Xiong et al., 2019; Nguyen and Salazar,\n2019). We recognize that gradient vanishing issue\nis not the direct reason causing such difference,\nsince fixing this issue alone cannot stabilize Post-\nLN training. It implies that, besides unbalanced gra-\ndients, there exist other factors influencing model\ntraining greatly.\nWith further analysis, we recognize that for each\nTransformer residual block, the dependency on its\n2As in Figure 2, Post-LN places layer norm outside of\nresidual blocks, and Pre-LN moves them to the inside.\narXiv:2004.08249v3  [cs.LG]  1 Oct 2023\nAttention\nAdd\nLayer Norm\nFFN\nAdd\nLayer Norm\nAttention\nAdd\nLayer Norm\nFFN\nAdd\nLayer Norm\nAttention\nAdd\nLayer Norm\nx\n( od )\n0x\n( oe )\n0\nLayer Norm\nAttention\nAdd\nLayer Norm\nFFN\nAdd\n⇥ N\nLayer Norm\nAttention\nAdd\nLayer Norm\nAttention\nAdd\nLayer Norm\nFFN\nAdd\n⇥ N\nx\n( pd )\n0 x\n( pe )\n0\nPre-LN Post-LN\nEncoder Encoder DecoderDecoder\n⇤ ( pd )\n⇤ ( pe )\n⇤ ( oe )\n⇤ ( od )\n: Pre-LN decoder\n: Pre-LN encoder\n: Post-LN encoder\n: Post-LN decoder\n: sub-layers outputs (i.e., FFN, Self-Attention and Encoder-Attention) \nNotation Table\n: intermediate output\n: residual output N : layer #\nD : hidden #\nH : head # ⇥ N\n⇥ N\nLayer Norm\nx\n( pe )\nLayer Norm\nx\n( pd )\nx\n( od )\nx\n( oe )\nx\n( od )\n3 i \u0000 3\nx\n( od )\n3 i \u0000 2\nx\n( od )\n3 i \u0000 1\nx\n( od )\n3 i\na\n( od )\n3 i \u0000 2\nb\n( od )\n3 i \u0000 2\na\n( od )\n3 i \u0000 1\nb\n( od )\n3 i \u0000 1\na\n( od )\n3 i\nb\n( od )\n3 i\nx\n( oe )\n2 i \u0000 2\nx\n( oe )\n2 i \u0000 1\nx\n( oe )\n2 i\na\n( oe )\n2 i \u0000 1\nb\n( oe )\n2 i \u0000 1\na\n( oe )\n2 i\nb\n( oe )\n2 i\nx\n( pe )\n2 i \u0000 1\nx\n( pe )\n2 i \u0000 2\nx\n( pe )\n2 i\nx\n( pd )\n3 i \u0000 3\nx\n( pd )\n3 i \u0000 2\nx\n( pd )\n3 i \u0000 1\nx\n( pd )\n3 i\nx\na\nb\nˆ⇤ : normalized outputs, i.e., Var[ ˆ⇤ ]=1\nVar[ · ] : dimension-wise variance\nFigure 2: The Architecture and notations of Pre-LN Transformers (Left) and Post-LN Transformers (Right).\nresidual branch3 plays an essential role in training\nstability. First, we find that a Post-LN layer has a\nheavier dependency on its residual branch than a\nPre-LN layer. As in Figure 7, at initialization, a\nPre-LN layer has roughly the same dependency on\nits residual branch and any previous layer, whereas\na Post-LN layer has a stronger dependency on its\nresidual branch (more discussions are elaborated in\nSection 4.1). We find that strong dependencies of\nPost-LN amplify fluctuations brought by parameter\nchanges and destabilize the training (as in Theo-\nrem 2 and Figure 4). Besides, the loose reliance\non residual branches in Pre-LN generally limits the\nalgorithm’s potential and often produces inferior\nmodels.\nIn light of our analysis, we propose Admin, an\nadaptive initialization method which retains the\nmerits of Pre-LN stability without hurting the per-\nformance. It restricts the layer dependency on its\nresidual branches in the early stage and unleashes\nthe model potential in the late stage. We conduct\nexperiments on IWSLT’14 De-En, WMT’14 En-\nDe, and WMT’14 En-Fr; Admin is more stable,\nconverges faster, and achieves better performance.\nFor example, without introducing any additional\nhyper-parameters, Admin successfully stabilizes\n72-layer Transformer training on WMT’14 En-Fr\nand achieves a 43.80 BLEU score.\n3For a residual block x + f(x), its shortcut output refers\nto x, its residual branch output refers to f(x), and the depen-\ndency on its residual branch refers to Var[f(x)]\nVar[x+f(x)] .\n2 Preliminaries\nTransformer Architectures and Notations. The\nTransformer architecture contains two types of sub-\nlayers, i.e., Attention sub-layers and Feedforward\n(FFN) sub-layers. They are composed of mainly\nthree basic modules (Vaswani et al., 2017), i.e.,\nLayer Norm ( fLN), Multi-head Attention ( fATT),\nand Feedforward Network (fFFN).\nAs illustrated in Figure 2, the Pre-LN Trans-\nformer and the Post-LN Transformer organize\nthese modules differently. For example, a Pre-\nLN encoder organizes the Self-Attention sub-\nlayer as x(pe)\n2i−1 = x(pe)\n2i−2 + fS-ATT(fLN(x(pe)\n2i−2))\nand a Post-LN encoder as x(oe)\n2i−1 = fLN(x(oe)\n2i−2 +\nfS-ATT(x(oe)\n2i−2)), where x(·)\n2i−2 is the input of the i-\nth Transformer layer and x(·)\n2i−1 is the output of\nthe i-th Self-Attention sub-layer. Here, we refer\nfS-ATT(fLN(x(pe)\n2i−2)) and fS-ATT(x(oe)\n2i−2) as the resid-\nual branches and their outputs as the residual out-\nputs, in contrast to layer/sub-layer outputs, which\nintegrates residual outputs and shortcut outputs.\nNotation elaborations are shown in Figure 2. In\nparticular, we use superscripts to indicate network\narchitectures (i.e., the Pre-LN Encoder), use sub-\nscripts to indicate layer indexes (top layers have\nlarger indexes), all inputs and outputs are formu-\nlated as Sequence-Len × Hidden-Dim.\nLayer Norm. Layer norm (Ba et al., 2016) plays a\nvital role in Transformer architecture. It is defined\n10°1\n100\nPre-LN Encoder Post-LN Encoder Pre-LN Deocder Post-LN Deocder\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\n10°1\n100\nGradient vanishing only happens in backpropagations for Encoder-Attention sub-layers\ni.e., from Encoder-Attention outputs to Self-Attention outputs.\nSelf Attention (PostLN Decoder) Encoder Attention (PostLN Decoder) Feedforward (PostLN Decoder)\nFigure 3: Relative gradient norm histogram (on a log scale) of 18-layer Transformers on the WMT’14 En-De dataset,\ni.e., the gradient norm of sub-layer outputs, scaled by the largest gradient norm in the same network.\nas fLN(x) = γ x−µ\nσ + ν, where µ and σ are the\nmean and standard deviation of x.\nFeedforward Network. Transformers use two-\nlayer perceptrons as feedforward networks, i.e.,\nfFFN(x) = ϕ(xW(1))W(2), where ϕ(·) is the non-\nlinear function4, and W(·) are parameters.\nMulti-head Attention. Multi-head Attentions\nallows the network to have multiple focuses\nin a single layer and plays a crucial role in\nmany tasks (Chen et al., 2018). It is de-\nfined as (with H heads): fATT(q, k, v) =PH\nh=1 fs(qW(Q)\nh W(K)\nh kT )vW(V1)\nh W(V2)\nh , where\nfs is the row-wise softmax function and W(·)\nh are\nparameters. W(Q)\nh and W(V1)\nh are D × D\nH matrices,\nW(K)\nh and W(V2)\nh are D\nH × D matrices, where D\nis the hidden state dimension. Parameters with-\nout subscript refer the concatenation of all H-\nhead parameters, e.g., W(Q) = [W(Q)\n1 , ··· , W(Q)\nH ].\nIn Transformer, this module is used in two dif-\nferent settings: Encoder-Attention ( fE-ATT(x) =\nfATT(x, x(·e), x(·e)) and x(·e) is the encoder output),\nand Self-Attention (fS-ATT(x) = fATT(x, x, x)).\n3 Unbalanced Gradients\nIn this study, we strive to answer the question:\nwhat complicates Transformer training. Our analy-\nsis starts from the observation: Pre-LN training is\nmore robust than Post-LN, while Post-LN is more\nlikely to reach a better performance than Pre-LN.\nIn a parameter grid search (as in Figure 10), Pre-LN\n4Our analysis uses ReLU as the activation function, while\nAdmin can be applied to other non-linear functions.\nconverges in all 15 settings, and Post-LN diverges\nin 7 out of 15 settings; when Post-LN converges,\nit outperforms Pre-LN in 7 out of 8 settings. We\nseek to reveal the underlying factor that destabilizes\nPost-LN training and restricts the performance of\nPre-LN.\nIn this section, we focus on the unbalanced gra-\ndients (e.g., gradient vanishing). We find that, al-\nthough Post-LN suffers from gradient vanishing\nand Pre-LN does not, gradient vanishing is not the\ndirect reason causing the instability of Post-LN.\nSpecifically, we first theoretically and empirically\nestablish that only Post-LN decoders suffer from\ngradient vanishing and Post-LN encoders do not.\nWe then observe that fixing the gradient vanishing\nissue alone cannot stabilize training.\n3.1 Gradients at Initialization\nAs gradient vanishing can hamper convergence\nfrom the beginning, it has been regarded as the\nmajor issue causing unstable training. Also, re-\ncent studies show that this issue exists in the Post-\nLN Transformer, even after using residual connec-\ntions (Xiong et al., 2019). Below, we establish that\nonly Post-LN decoders suffer from the gradient\nvanishing, and neither Post-LN encoders, Pre-LN\nencoders, nor Pre-LN decoders.\nWe use ∆x to denote gradients, i.e., ∆x = ∂L\n∂x\nwhere L is the training objective. Following previ-\nous studies (Glorot and Bengio, 2010), we analyze\nthe gradient distribution at the very beginning of\ntraining and find only Encoder-Attention sub-layers\nin Post-LN suffers from gradient vanishing.\nFirst, we conduct analysis from a theoretical\n0 100 200\n0\n500\n1000\n1500\n2000\n100 101 1020\n50\n100\n150\n0 100 200100\n101\n102\n103\n104\n105\nPre-LN\nPost-LN\nAdmin (Post-LN)\nNum of Sub-Layers (FFN or Self-Attention) in the Encoder\nRandom Perturbations, i.e.,  Gradient Updates, i.e., \n|F ( x 0 ,W ) \u0000 F ( x 0 ,W\n⇤\n) |\n2\n2\nW\n⇤\n= W + \u0000\nPost-LN is less \nstable than Pre-LN \nPost-LN:|F \u0000 F ⇤\n| 2\n2 = O ( N )\nPre-LN |F \u0000 F\n⇤\n|\n2\n2 = O (log N )\nAdmin :\nR\n2\n=0 . 99R\n2\n=0 . 99\nW\n⇤\n= W + Adam( r W L ( F ))\nFigure 4: Encoder output changes for parameter changes, i.e., |F(x0, W) −\nF(x0, W∗)|2\n2 where W∗ − W is random perturbations (left) or gradient updates\n(right). Intuitively, very large |F − F∗| indicates the training to be ill-conditioned.\n0 50 100\n10°2\n10°1\n100\nRelative\nGradient Norm\n10°1\n100\nRelative Parameter\nUpdate Norm\nEpoch # (iterations over the training set)\nThe update magnitude is \nconsistent,  even with \nunbalanced gradients.\n18-Layer Pre-LN  Encoder Self-Attention\nLight color indicates higher layers\nFigure 5: Histogram of rel-\native norm of gradient and\n|Wi+1 − Wi| where Wi is\nthe checkpoint saved after\ntraining for i epochs.\nEncoder Decoder Gradient Training\nPost-LN Post-LN Vanishing Diverged\nPost-LN Pre-LN Vanishing Diverged\nPre-LN Pre-LN Vanishing Converged\nTable 1: Changing decoders from Post-LN to Pre-LN\nfixes gradient vanishing, but does not stabilize model\ntraining successfully. Encoder/Decoder have 18 layers.\nperspective. Similar to Xiong et al. (2019), we\nestablish that Pre-LN networks do not suffer from\ngradient vanishing (as elaborated in Appendix A.1).\nUnlike Xiong et al. (2019), we recognize that not\nall Post-LN networks suffer from gradient vanish-\ning. As in Theorem 1, we establish that Post-LN\nEncoder networks do not suffer from gradient van-\nishing. Detailed derivations are elaborated in Ap-\npendix A.2.\nTHEOREM 1. — For Post-LN Encoders, if γ and\nν in the Layer Norm are initialized as 1 and 0 re-\nspectively; all other parameters are initialized by\nsymmetric distributions with zero mean; x(oe)\ni and\n∆x(oe)\ni are subject to symmetric distributions with\nzero mean; the variance of x(oe)\ni is 1 (i.e., normal-\nized by Layer Norm); ∆x(oe)\ni and the derivatives\nof modules in i-th sub-layer are independent, we\nhave Var[∆xi−1] ≥ Var[∆xi].\nTo make sure that the assumptions of Theo-\nrem 2 match the real-world situation, we further\nconduct empirical verification. At initialization,\nwe calculate ||∆x(·)\ni ||2 for 18-layer Transformers5\n5Note if E[∆x(p·)\ni−1] = 0, Var[∆x(p·)\ni−1] ≈ |∆x(p·)\ni−1|2\n2.\nand visualize ||∆x(·)\ni ||2\nmaxj ||∆x(·)\nj ||2\nin Figure 3. It verifies\nthat only Post-LN decoders suffer from the gradi-\nent vanishing. Besides, we can observe that the\ndropping of gradient norms mostly happens in the\nbackpropagation from encoder-attention outputs\n(encoder-attention bars) to its inputs (self-attention\nbars, since the output of self-attention is the in-\nput of encoder-attention). This pattern is further\nexplained in Appendix A.3.\n3.2 Impact of the Gradient Vanishing\nNow, we explore whether gradient vanishing is the\ndirect cause of training instability.\nFirst, we design a controlled experiment to show\nthe relationship between gradient vanishing and\ntraining stability. We construct a hybrid Trans-\nformer by combining a Post-LN encoder and a\nPre-LN decoder. As in Section 3.1, only Post-LN\ndecoders suffer from gradient vanishing, but not\nPost-LN encoders. Therefore, this hybrid Trans-\nformer does not suffer from gradient vanishing.\nAs shown in Table 1, fixing gradient vanishing\nalone (i.e., changing Post-LN decoders to Pre-LN\ndecoders) fails to stabilize model training. This\nobservation provides evidence supporting that the\ngradient vanishing issue is not the direct cause of\nunstable Post-LN training.\nMoreover, we observe that gradients of all at-\ntention modules are unbalanced, while adaptive\noptimizers mostly address this issue. As in Fig-\nure 5, adaptive optimizers successfully assign dif-\nferent learning rates to different parameters and\nlead to consistent update magnitudes even with un-\nbalanced gradients. It explains why the standard\nSGD fails in training Transformers (i.e., lacking the\nAttentionFFN\nPre-LN\nAttention\na\npe\n1a\npe\n2\nLayer Norm\nAttentionFFN\nPost-LN\nAttention\nLayer Norm a\noe\n2 a\noe\n1\nLayer Norm\nFigure 6: The major difference between Pre-LN and\nPost-LN is the position of layer norms.\nability to handle unbalanced gradients) and necessi-\ntates using adaptive optimizers. More discussions\nare included in Appendix A.4.\n4 Instability from Amplification Effect\nWe find that unbalanced gradients are not the root\ncause of the instability of Post-LN, which implies\nthe existence of other factors influencing model\ntraining. Now, we go beyond gradient vanishing\nand introduce the amplification effect. Specifically,\nwe first examine the difference between Pre-LN\nand Post-LN, including their early-stage and late-\nstage training. Then, we show that Post-LN’s train-\ning instability is attributed to layer dependency’s\namplification effect, which intensifies gradient up-\ndates and destabilizes training.\n4.1 Impact of Layer Norms Positions\nAs described in Section 2, both Pre-LN and Post-\nLN employ layer norm to regularize inputs and out-\nputs. Different residual outputs are aggregated and\nnormalized in residual networks before serving as\ninputs of other layers (i.e., residual outputs will be\nscaled to ensure the integrated input to have a con-\nsistent variance). To some extend, layer norm treats\nthe variance of residual outputs as weights to aver-\nage them. For example, for Post-LN Self-Attention,\nwe have x(o·)\n2i−1 =\nx(o·)\n2i−2+a(o·)\n2i−1q\nVar[x(o·)\n2i−2]+Var[a(o·)\n2i−1]\nat initial-\nization. Larger Var[a(o·)\n2i−2] not only increases the\nproportion of a(o·)\n2i−2 in x(o·)\n2i−2 but decreases the pro-\nportion of other residual outputs. Intuitively, this is\nsimilar to the weight mechanism of the weighted\naverage.\nThe position of layer norms is the major differ-\nence between Pre-LN and Post-LN and makes them\naggregate residual outputs differently ( i.e., using\ndifferent weights). As in Figure 6, all residual out-\nputs in Pre-LN are only normalized once before\nfeeding into other layers (thus only treating resid-\nual output variances as weights); in Post-LN, most\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\nx10\nx11\nx12\nx1\na1\nx2\na2\nx3\na3\nx4\na4\nx5\na5\nx6\na6\nx7\na7\nx8\na8\nx9\na9\nx10\na10\nx11\na11\nx12\na12\n a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n6-Layer Post-LN 6-Layer Pre-LN\nAt InitializationAfter 100 Epochs\nComparing ﬁnal models, Post-LN layer has a larger dependency on its residual branch.\nbranch outputs. branch outputs. \nPost-LN layer outputs always \ndepend more on its residual  \nPre-LN layer outputs learn to \n depend more on its residual\nFigure 7: βi,j in 6-Layer Post-LN and Pre-LN on the\nWMT-14 En-De dataset (contains 12 sub-layers).\nresidual outputs are normalized more than once,\nand different residual outputs are normalized for\ndifferent times. For example, if all layers are initial-\nized in the same way, output variances of different\nPre-LN residual branches would be similar, and the\naggregation would be similar to the simple average.\nSimilarly, for Post-LN, nearby residual outputs are\nnormalized by fewer times than others, thus having\nrelatively larger weights. We proceed to calculate\nand analyze these weights to understand the impact\nof layer norm positions.\nFirst, we use bai to refer ai√Varai\n(i.e., normal-\nized outputs of i-th residual branch) and bxi to re-\nfer xi√Varxi\n(i.e., normalized outputs of i-th layer\nor normalized inputs of (i+1)-th residual branch).\nThen, we describe their relationships as bxi =P\nj≤i βi,jbaj, where βi,j integrates scaling opera-\ntions of all layer norms (including\np\nVar[ai]). For\nexample, Pre-LN sets βi,j =\n√\nVar[aj]√\nVar[P\nk≤i ak]. Intu-\nitively,βi,j describes the proportion ofj-th residual\nbranch outputs in i-th layer outputs, thus reflects\nthe dependency among layers.\nWe visualize βi,j in Figure 7. For a Post-LN\nlayer, its outputs rely more on its residual branch\nfrom the initialization to the end. At initialization,\nPre-LN layer outputs have roughly the same re-\nliance on all previous residual branches. As the\ntraining advances, each layer starts to rely more on\nits own residual outputs. However, comparing to\nPost-LN, Pre-LN layer outputs in the final model\nstill has less reliance on their residual branches.\nIntuitively, it is harder for Pre-LN layers to de-\npend too much on their own residual branches. In\nPre-LN, layer outputs (i.e., x(p·)\ni ) are not normal-\nized, and their variances are likely to be larger for\nhigher layers6. Since βi,i =\n√\nVar[ai]q\nVar[x(p·)\ni−1+ai]\n, βi,i\nis likely to be smaller for higher layers, which re-\nstricts i-th layer outputs from depending too much\non its residual branch and inhibits the network from\nreaching its full potential. In other words, Pre-LN\nrestricts the network from being too deep ( i.e., if\nit is hard to distinguish x(p·)\ni and x(p·)\ni+1, appending\none layer would be similar to doubling the width\nof the last layer), while Post-LN gives the network\nthe choice of being wider or deeper.\n4.2 Amplification Effect at Initialization\nAlthough depending more on residual branches al-\nlows the model to have a larger potential, it ampli-\nfies the fluctuation brought by parameter changes.\nFor a network bx = F(x0, W) where x0 is the\nmodel input and W is the parameter, the out-\nput change caused by parameter perturbations is\nVar[F(x0, W)−F(x0, W∗)], where W∗ = W +δ.\nIts relationship with N is described in Theorem 2,\nand the derivation is elaborated in Appendix B.\nTHEOREM 2. — Consider a N-layer Transformer\nbx = F(bx0, W) at initialization, where bx0 is the\ninput and W is the parameter. If the layer depen-\ndency stays the same after a parameter change (i.e.,\nβi,j has the same value after changing W to W∗,\nwhere W is randomly initialized and δ = W∗ −W\nis independent to W), the output change ( i.e.,\nVar[F(x0, W) − F(x0, W∗)]) can be estimated\nas PN\ni=1 β2\ni,iC where C is a constant.\nIf Var[ai] is the same for all layers, Pre-LN sets\nβ2\ni,i as 1/i, and Post-LN sets β2\ni,i as a constant.\nThus, we have Corollary 1 and 2 as below.\nCOROLLARY 1. — For a N-layer Pre-LN F, we\nhave Var[F(x0, W) − F(x0, W∗)] = O(log N).\nCOROLLARY 2. — For a N-layer Post-LN F, we\nhave Var[F(x0, W) − F(x0, W∗)] = O(N).\nThey show that, since Post-LN relies more on\nresidual branches than Pre-LN ( i.e., has a larger\nβ2\ni,i), the perturbation is amplified to a larger mag-\nnitude. To empirically verify these relationships,\nwe calculate |F(x0, W) − F(x0, W∗)|2\n2 for Pre-\nLN and Post-LN and visualize the results in Fig-\n6If a0 and a1 are independent, Var[a0 + a1] = Var[a0] +\nVar[a1]; also, in our experiments Var[x(p·)\ni ] increases as i\nbecomes larger\nure 4. In Corollary 2, N is linearly associated with\n|F − F∗|2\n2 for Post-LN; and in Corollary 1, log N\nis linearly associated with |F − F∗|2\n2 for Pre-LN.\nThese relationships match the observation in our\nexperiments (as in Figure 4). For further verifica-\ntion, we measure their correlation magnitudes by\nR2 and find R2 = 0.99 in both cases.\nMoreover, we replace the random noise δ with\noptimization updates ( i.e., setting W∗ = W +\nAdam(∆W), where opt(·) is update calculated by\nthe Adam optimizer) and visualize output shifts.\nThis replacement makes the correlation between\n|F − F∗|2\n2 and N (for Post-LN) or log N (for Pre-\nLN) to be weaker ( i.e., R2 = 0 .75). Still, as in\nFigure 4, the output shift |F − F∗|2\n2 for Post-LN is\nlarger than Pre-LN by multiple magnitudes.\nIntuitively, large output shifts would destabilize\nthe training (Li et al., 2018). Also, as elaborated\nin Appendix B, the constant C in Theorem 2 is\nrelated to network derivatives and would be smaller\nas training advances, which explains why warmup\nis also helpful for the standard SGD. Therefore, we\nconjecture it is the large output shift of Post-LN\nresults in unstable training. We proceed to stabilize\nPost-LN by controlling the dependency on residual\nbranches in the early stage of training.\n4.3 Admin – Adaptive Model Initialization\nIn light of our analysis, we add additional param-\neters (i.e., ω) to control residual dependencies of\nPost-LN and stabilize training by adaptively initial-\nizing ω to ensure an O(log N) output change.\nDue to different training configurations and\nmodel specificities (e.g., different models may use\ndifferent activation functions and dropout ratios),\nit is hard to derive a universal initialization method.\nInstead, we decompose model initialization into\ntwo phrases: Profiling and Initialization. Specif-\nically, Admin adds new parameters ω and con-\nstructs its i-th sub-layer as xi = fLN(bi), where\nbi = xi−1 · ωi + fi(xi−1), ωi is a D-dimension\nvector and · is element-wise product. Then the\nProfiling phrase and Initialization phrase are:\nProfiling. After initializing the network with a\nstandard method (initializing ωi as 1), conduct for-\nward propagation without parameter updating and\nrecord the output variance of residual branches (i.e.,\ncalculate Var[fi(xi−1)]). Since all elements in the\nsame parameter/output matrix are independent to\neach other and are subject to the same distribution,\nit is sufficient to use a small number of instances in\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n18-Layer Admin (Post-LN) 18-Layer Pre-LN\nAt InitializationAfter 100 Epochs\nNormalized Layer Outputs\nx 1\nx 1\na 1 a 1\nNormalized Outputs of Residual Branches\nAdmin stabilizes \nmodel training by \navoid over-large\nWith a Post-LN structure, \nAdmin allows layer outputs\ndependencies.\nof the ﬁnal model to \ndepend more on \ntheir residual \nbranches.\nFigure 8: βi,j of 18-Layer Admin (Post-LN) and Pre-\nLN on the WMT-14 En-De dataset.\nthis phrase. In our experiments, the first batch (no\nmore than 8192 tokens) is used.\nInitialization. Set ωi =\nqP\nj<i Var[fj(xj−1)]\nand initialize all other parameters with the same\nmethod used in the Profiling phrase.\nIn the early stage, Admin sets β2\ni,i to approxi-\nmately 1\ni and ensures an O(log N) output change,\nthus stabilizing training. Model training would be-\ncome more stable in the late stage (the constant\nC in Theorem 2 is related to parameter gradients),\nand each layer has the flexibility to adjust ω and\ndepends more on its residual branch to calculate the\nlayer outputs. After training finishes, Admin can\nbe reparameterized as the conventional Post-LN\nstructure (i.e., removing ω). More implementation\ndetails are elaborated in Appendix C.\nTo verify our intuition, we calculate the layer\ndependency of 18-Layer models and visualize the\nresult in Figure 8. Figures 7 and 8 show thatAdmin\navoids over-large dependencies at initialization and\nunleashes the potential to make the layer outputs\ndepend more on their residual outputs in the final\nmodel. Moreover, we visualize the output change\nof Admin in Figure 4. Benefiting from the adap-\ntive initialization, the output change of Admin gets\nroughly the same increase speed as Pre-LN, even\nconstructed in the Post-LN manner. Also, although\nAdmin is formulated in a Post-LN manner and\nsuffers from gradient vanishing, 18-layer Admin\nsuccessfully converges and outperforms 18-layer\nPre-LN (as in Table 2). This evidence supports\nour intuition that the large dependency on resid-\nual branches amplifies the output fluctuation and\ndestabilizes training.\n0 50\n4.4\n4.6\n4.8\n5.0\nDev PPL on WMT’14 En-De\n12-Layer Transformer\n50 75\n4.8\n4.9\n5.0\n5.1\n5.2\nDev PPL on IWSLT’14 De-En\nTransformer Small\nPre-LN\nPost-LN\nAdmin\n(Post-LN)\nFigure 9: Development PPL on the WMT’14 En-De\ndataset and the IWLST’14 De-En dataset.\n5 Experiments\nWe conduct experiments on IWSLT’14 De-En,\nWMT’14 En-De, and WMT’14 En-Fr. More details\nare elaborated in Appendix D.\n5.1 Performance Comparison\nWe use BLEU as the evaluation matric and sum-\nmarize the model performance in Table 2. On the\nWMT’14 dataset, we use Transformer-base models\nwith 6, 12, or 18 layers. Admin achieves a bet-\nter performance than Post-LN and Pre-LN in all\nthree settings. Specifically, 12-Layer and 18-Layer\nPost-LN diverges without the adaptive initializa-\ntion. Pre-LN converges in all settings, but it results\nin sub-optimal performance. Admin not only sta-\nbilizes the training of deeper models but benefits\nmore from the increased model capacity then Pre-\nLN, which verifies our intuition that the Pre-LN\nstructure limits the model potential. As in Figure 1\nand Figure 9, although the 6-layer Pre-LN con-\nverges faster than Post-LN, its final performance is\nworse than Post-LN. In contrast, Admin not only\nachieves the same convergence speed with Pre-LN\nin the early stage but reaches a good performance\nin the late stage.\nWe use 6-layer Transformer-small (its hidden\ndimension is smaller than the base model) on the\nIWSLT’14 dataset, and all methods perform sim-\nilarly. Still, as in Figure 10, Admin outperforms\nthe other two by a small margin. Together with\nWMT’14 results, it implies the training stability is\nrelated to layer number. For shallow networks, the\nstability difference between Post-LN and Pre-LN\nis not significant (as in Figure 4), and all methods\nreach reasonable performance. It is worth mention-\ning that attention and activation dropouts have an\nenormous impact on IWSLT’14, which is smaller\nthan WMT’14 datasets.\nTable 2: BLEU on IWSLT’14 De-En and WMT’14 En-Fr/De (AL-BL refers A-layer encoder & B-layer decoder).\nDataset IWSLT’14 De-En WMT’14 En-Fr WMT’14 En-De\nEnc #–Dec # 6L–6L (small) 6L–6L 60L–12L 6L–6L 12L–12L 18L–18L\nPost-LN 35.64±0.23 41.29 failed 27.80 failed failed\nPre-LN 35.50±0.04 40.74 43.10 27.27 28.26 28.38\nAdmin 35.67±0.15 41.47 43.80 27.90 28.58 29.03\nTo further explore the potential of Admin, we\ntrain Transformers with a larger size. Specifi-\ncally, we expand the Transformer-base configu-\nration to have a 60-layer encoder and a 12-layer\ndecoder. As in Table 2, our method achieves a\nBLEU score of 43.8 on the WMT’14 En-Fr dataset,\nthe new state-of-the-art without using additional\nannotations (e.g., back-translation). More discus-\nsions are conducted in Appendix F to compare\nthis model with the current state of the art. Fur-\nthermore, in-depth analyses are summarized in\nLiu et al. (2020b), including systematic evalua-\ntions on the model performance (with TER, ME-\nTEOR, and BLEU), comprehensive discussions on\nmodel dimensions (i.e., depth, head number, and\nhidden dimension), and fine-grained error analysis.\nIt is worth mentioning that the 60L-12L Admin\nmodel achieves a 30.1 BLEU score on WMT’14\nEn-De (Liu et al., 2020b).\n5.2 Connection to Warmup\nOur previous work (Liu et al., 2020a) establishes\nthat the need for warmup comes from the unstable\nadaptive learning rates in the early stage. Still, re-\nmoving the warmup phrase results in more severe\nconsequences for Transformers than other architec-\ntures. Also, warmup has been found to be useful\nfor the vanilla SGD (Xiong et al., 2019).\nTheorem 1 establishes that Var[F(x0, W) −\nF(x0, W∗)] ≈ PN\ni=1 β2\ni,iC where C =\nVar[Gi(bx∗\ni−1, Wi) − Gi(bx∗\ni−1, W∗\ni )]. In the early\nstage of training, the network has larger parame-\nter gradients and thus larger C. Therefore, using\na small learning rate at initialization helps to al-\nleviate the massive output shift of Post-LN. We\nfurther conduct experiments to explore whether\nmore prolonged warmups can make up the stabil-\nity difference between Post-LN and Pre-LN. We\nobserve that 18-layer Post-LN training still fails af-\nter extending the warmup phrase from 8 thousand\nupdates to 16, 24, and 32 thousand. It shows that\nlearning rate warmup alone cannot neutralize the\n0.999 0.995 0.99\n1 × 10−4\n2 × 10−4\n3 × 10−4\n4 × 10−4\n5 × 10−4\n34.64 34.65 34.41\n35.65 35.58 35.51\n35.87 0.00 0.00\n33.56 0.00 0.00\n0.00 0.00 0.00\nPost-LN\n0.999 0.995 0.99\n33.98 33.81 33.76\n34.74 34.91 34.87\n35.09 35.15 35.19\n35.06 35.28 35.31\n35.51 35.45 35.55\nPre-LN\n0.999 0.995 0.99\n34.58 34.53 34.60\n35.26 35.03 35.25\n35.38 35.62 35.57\n35.74 35.89 35.69\n35.61 35.83 35.84\nAdmin (Post-LN)\n33.0\n33.5\n34.0\n34.5\n35.0\n35.5\n36.0\nFigure 10: BLEU score of Post-LN, Pre-LN and Ad-\nmin on the IWSLT’14 De-En dataset (x-axis is the β2\nfor adaptive optimizers and y-axis is the learning rate).\nPre-LN converges in all settings while Post-LN diverges\nin 7 out of 15 settings. When Post-LN converges, it\noutperforms Pre-LN in 7 out of 8 settings. Admin stabi-\nlizes Post-LN training and outperforms Pre-LN (its best\nperformance is comparable with Post-LN).\ninstability of Post-LN. Intuitively, massive output\nshifts not only require a small learning rate but also\nunsmoothes the loss surface (Li et al., 2018) and\nmake the training ill-conditioned.\nAdmin regularizes the model behavior at ini-\ntialization and stabilizes the training. To explore\nwhether Admin is able to stabilize the training\nalone, we remove the warmup phase and conduct\na grid search on optimizer hyper-parameters. The\nresults are visualized in Figure 10. It shows that as\nPost-LN is more sensitive to the choice of hyper-\nparameters, Admin successfully stabilizes the train-\ning without hurting its potential.\n5.3 Comparing to Other Initializations\nWe compare our methods with three initialization\nmethods, i.e., ReZero (Bachlechner et al., 2020),\nFixUp (Zhang et al., 2019a), and LookLinear (Bal-\nduzzi et al., 2017a). Specifically, we first conduct\nexperiments with 18-layer Transformers on the\nWMT’14 De-En dataset. In our experiments, we\nobserve that all of ReZero (which does not con-\ntain layer normalization), FixUp (which also does\nnot contain layer normalization), and LookLinear\n(which is incorporated with Post-LN) leads to di-\nvergent training. With further analysis, we find that\nthe half-precision training and dropout could desta-\nbilize FixUp and ReZero, due to the lack of layer\nnormalization. Simultaneously, we find that even\nfor shadow networks, having an over small reliance\non residual branches hurts the model performance,\nwhich also supports our intuition. For example,\nas elaborated in Appendix E, applying ReZero to\nTransformer-small leads to a 1-2 BLEU score drop\non the IWSLT’14 De-En dataset.\n6 Related Work\nTransformer.Transformer (Vaswani et al., 2017)\nhas led to a series of breakthroughs in various do-\nmains (Devlin et al., 2019; Velickovic et al., 2018;\nHuang et al., 2019; Parmar et al., 2018; Ramachan-\ndran et al., 2019). Liu et al. (2020a) show that com-\npared to other architectures, removing the warmup\nphase is more damaging for Transformers, espe-\ncially Post-LN. Similarly, it has been found that\nthe original Transformer (referred to as Post-LN)\nis less robust than its Pre-LN variant (Baevski and\nAuli, 2019; Nguyen and Salazar, 2019; Wang et al.,\n2019). Our studies go beyond the existing litera-\nture on gradient vanishing (Xiong et al., 2019) and\nidentify an essential factor influencing Transformer\ntraining greatly.\nDeep Network Initialization. It has been observed\nthat deeper networks can lead to better performance.\nFor example, Dong et al. (2020) find that the net-\nwork depth players a similar role with the sample\nnumber in numerical ODE solvers, which hinders\nthe system from getting more precise results. Many\nattempts have been made to clear obstacles for train-\ning deep networks, including various initialization\nmethods. Based on the independence among initial-\nized parameters, one method is derived and found\nto be useful to handle the gradient vanishing (Glo-\nrot and Bengio, 2010). Similar methods are further\ndeveloped for ReLU networks (He et al., 2015). He\net al. (2016) find that deep network training is still\nhard even after addressing the gradient vanishing\nissue and propose residual networks. Balduzzi et al.\n(2017b) identifies the shattered gradient issue and\nproposes LookLinear initialization.\nOn the other hand, although it is observed that\nscaling residual outputs to smaller values helps\nto stabilize training (Hanin and Rolnick, 2018;\nMishkin and Matas, 2015; Zhang et al., 2019a;\nBachlechner et al., 2020; Goyal et al., 2017), there\nis no systematic analysis on what complicates\nTransformer training or its underlying connection\nto the dependency on residual branches. Here, we\nidentify that unbalanced gradients are not the di-\nrect cause of the Post-LN instability, recognize the\namplification effect, and propose a novel adaptive\ninitialization method.\n7 Conclusion\nIn this paper, we study the difficulties of training\nTransformers in theoretical and empirical manners.\nOur study in Section 3 suggests that the gradient\nvanishing problem is not the root cause of unsta-\nble Transformer training. Also, the unbalanced\ngradient distribution issue is mostly addressed by\nadaptive optimizers. In Section 4, we reveal the\nroot cause of the instability to be the strong depen-\ndency on residual branches, which amplifies the\nfluctuation caused by parameter changes and desta-\nbilizes model training. In light of our analysis, we\npropose Admin, an adaptive initialization method\nto stabilize Transformers training. It controls the\ndependency at the beginning of training and main-\ntains the flexibility to capture those dependencies\nonce training stabilizes. Extensive experiments ver-\nify our intuitions and show that, without introduc-\ning additional hyper-parameters, Admin achieves\nmore stable training, faster convergence, and better\nperformance.\nOur work opens up new possibilities to not only\nfurther push the state-of-the-art but understand\ndeep network training better. It leads to many inter-\nesting future works, including generalizing Theo-\nrem 2 to other models, designing new algorithms\nto automatically adapt deep networks to different\ntraining configurations, upgrading the Transformer\narchitecture, and applying our proposed Admin to\nconduct training in a larger scale.\nAcknowledge\nWe thank all reviewers for their constructive com-\nments; Chengyu Dong, Haoming Jiang, Jingbo\nShang, Xiaotao Gu, and Zihan Wang for valuable\ndiscussions and comments; Jingbo Shang for shar-\ning GPU machines; and Microsoft for setting up\nGPU machines. The research was sponsored in\npart by DARPA No. W911NF-17-C-0099 and No.\nFA8750-19-2-1004, National Science Foundation\nIIS-19-56151, IIS-17-41317, IIS 17-04532, and IIS\n16-18481, and DTRA HDTRA11810026.\nReferences\nJimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\n2016. Layer normalization. ArXiv, abs/1607.06450.\nThomas C. Bachlechner, Bodhisattwa Prasad Majumder,\nHuanru Henry Mao, Garrison W. Cottrell, and Ju-\nlian J. McAuley. 2020. Rezero is all you need: Fast\nconvergence at large depth. ArXiv, abs/2003.04887.\nAlexei Baevski and Michael Auli. 2019. Adaptive input\nrepresentations for neural language modeling. In\nICLR.\nDavid Balduzzi, Marcus Frean, Lennox Leary, J. P.\nLewis, Kurt Wan-Duo Ma, and Brian McWilliams.\n2017a. The shattered gradients problem: If resnets\nare the answer, then what is the question? In ICML.\nDavid Balduzzi, Marcus Frean, Lennox Leary, J P\nLewis, Kurt Wan-Duo Ma, and Brian McWilliams.\n2017b. The shattered gradients problem: If resnets\nare the answer, then what is the question? In ICML.\nYoshua Bengio, Patrice Y . Simard, and Paolo Frasconi.\n1994. Learning long-term dependencies with gradi-\nent descent is difficult. IEEE transactions on neural\nnetworks.\nOndˇrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve Saint-\nAmand, et al. 2014. Findings of the 2014 workshop\non statistical machine translation. In Workshop on\nStatistical Machine Translation.\nMauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa\nBentivogli, and Marcello Federico. 2014. Report on\nthe 11th iwslt evaluation campaign, iwslt 2014. In\nInternational Workshop on Spoken Language Trans-\nlation, Hanoi, Vietnam.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Niki Parmar, Michael Schuster, Zhi-Feng\nChen, Yonghui Wu, and Macduff Hughes. 2018. The\nbest of both worlds: Combining recent advances in\nneural machine translation. In ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT.\nChengyu Dong, Liyuan Liu, Zichao Li, and Jingbo\nShang. 2020. Towards adaptive residual network\ntraining: A neural-ode perspective. In ICML.\nXavier Glorot and Yoshua Bengio. 2010. Understanding\nthe difficulty of training deep feedforward neural\nnetworks. In AISTATS.\nPriya Goyal, Piotr Dollár, Ross B. Girshick, Pieter No-\nordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew\nTulloch, Yangqing Jia, and Kaiming He. 2017. Ac-\ncurate, large minibatch sgd: Training imagenet in 1\nhour. ArXiv, abs/1706.02677.\nBoris Hanin and David Rolnick. 2018. How to start\ntraining: The effect of initialization and architecture.\nIn NeurIPS.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Delving deep into rectifiers: Surpassing\nhuman-level performance on imagenet classification.\nIn ICCV.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In CVPR.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszko-\nreit, Ian Simon, Curtis Hawthorne, Noam Shazeer,\nAndrew M. Dai, Matthew D. Hoffman, Monica Din-\nculescu, and Douglas Eck. 2019. Music transformer:\nGenerating music with long-term structure. In ICLR.\nHao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein.\n2018. Visualizing the loss landscape of neural nets.\nIn NeurIPS.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2020a. On the variance of the adaptive learning rate\nand beyond. In ICLR.\nXiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng\nGao. 2020b. Very deep transformers for neural ma-\nchine translation. ArXiv, abs/2008.07772.\nYiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong,\nTao Qin, Liwei Wang, and Tie-Yan Liu. 2020. Un-\nderstanding and improving transformer from a multi-\nparticle dynamic system point of view. InICLR Work-\nshop DeepDiffEq.\nDmytro Mishkin and Juan E. Sala Matas. 2015. All you\nneed is a good init. In ICLR.\nToan Q. Nguyen and Julian Salazar. 2019. Transformers\nwithout tears: Improving the normalization of self-\nattention. In IWSLT.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for se-\nquence modeling. In NAACL-HLT Demonstrations.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. 2018. Image transformer. In ICML.\nMartin Popel and Ondrej Bojar. 2018. Training tips\nfor the transformer model. The Prague Bulletin of\nMathematical Linguistics, 110:43 – 70.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. ArXiv, abs/1910.10683.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani,\nIrwan Bello, Anselm Levskaya, and Jonathon Shlens.\n2019. Stand-alone self-attention in vision models. In\nNeurIPS.\nAndrew M Saxe, James L McClelland, and Surya Gan-\nguli. 2013. Exact solutions to the nonlinear dynamics\nof learning in deep linear neural networks. ArXiv,\nabs/1312.6120.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nCVPR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Liò, and Yoshua Bengio.\n2018. Graph attention networks. In ICLR.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for machine\ntranslation. In ACL.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2019a. Pay less attention with\nlightweight and dynamic convolutions. In ICLR.\nLijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei Gao,\nTao Qin, Jianhuang Lai, and Tie-Yan Liu. 2019b.\nDepth growing for neural machine translation. In\nACL.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shu\nxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan,\nLi-Wei Wang, and Tie-Yan Liu. 2019. On layer nor-\nmalization in the transformer architecture. ArXiv,\nabs/2002.04745.\nHongyi Zhang, Yann N. Dauphin, and Tengyu Ma.\n2019a. Fixup initialization: Residual learning with-\nout normalization. In ICLR.\nJingzhao Zhang, Sai Praneeth Karimireddy, Andreas\nVeit, Seungyeon Kim, Sashank J. Reddi, Surinder\nKumar, and Suvrit Sra. 2019b. Why adam beats sgd\nfor attention models. ArXiv, abs/1912.03194.\nGuangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang,\nand Liangchen Luo. 2019. Muse: Parallel multi-scale\nattention for sequence to sequence learning. ArXiv,\nabs/1911.09483.\nAppendices\nA Gradients at Initialization\nHere, we first reveal that Pre-LN does not suffer from the gradient vanishing. Then we establish that only\nthe Post-LN decoder suffers from the gradient vanishing, but not the Post-LN encoder. For simplicity,\nwe use ∆x to denote gradients, i.e., ∆x = ∂L\n∂x where L is the training objective. Following the previous\nstudy (Bengio et al., 1994; Glorot and Bengio, 2010; He et al., 2015; Saxe et al., 2013), we analyze the\ngradient distribution at the very beginning of training, assume that the randomly initialized parameters\nand the partial derivative with regard to module inputs are independent.\nA.1 Pre-LN Analysis\nFor Pre-LN encoders, we have x(pe)\n2i = x(pe)\n2i−1 + fFFN(fLN(x(pe)\n2i−1)) and ∆x(pe)\n2i−1 = ∆ x(pe)\n2i (1 +\n∂fFFN(fLN(x(pe)\n2i−1))\n∂x(pe)\n2i\n). At initialization, the two terms on the right part are approximately independent\nand E[\n∂fFFN(fLN(z(pe)\n2i−1))\n∂x(pe)\n2i\n] = 0 . Therefore we have Var[∆x(pe)\n2i−1] ≥ Var[∆x(pe)\n2i ]. Similarly, we can get\nVar[∆x(pe)\n2i−2] ≥ Var[∆x(pe)\n2i−1] thus ∀i ≤ j, Var[∆x(pe)\ni ] ≥ Var[∆x(pe)\nj ]. Applying the same analysis\nto Pre-LN decoders, we can get ∀i ≤ j, Var[∆x(pd)\ni ] ≥ Var[∆x(pd)\nj ]. Thus, lower layers have larger\ngradients than higher layers, and gradients do not vanish in the backpropagation.\nREMARK 1. — For Pre-LN, if ∀i, ∆x(p·)\ni and the derivatives of modules in the i-th sub-layer are\nindependent, then ∀i ≤ j, Var[∆x(p·)\ni ] ≥ Var[∆x(p·)\nj ].\nA.2 Post-LN Encoder Analysis\nDifferent from Pre-LN, x(oe)\ni and x(oe)\ni−1 are associated with not only the residual connection but the\nlayer normalization, which makes it harder to establish the connection on their gradients. After making\nassumptions on the model initialization, we find that lower layers in Post-LN encoder also have larger\ngradients than higher layers, and gradients do not vanish in the backpropagation through the encoder.\nTHEOREM 1. — For Post-LN Encoders, if γ and ν in the Layer Norm are initialized as 1 and 0\nrespectively; all other parameters are initialized by symmetric distributions with zero mean; x(oe)\ni and\n∆x(oe)\ni are subject to symmetric distributions with zero mean; the variance of x(oe)\ni is 1 (i.e., normalized\nby Layer Norm); ∆x(oe)\ni and the derivatives of modules in i-th sub-layer are independent, we have\nVar[∆xi−1] ≥ Var[∆xi].\nProof. We first prove Var[∆x(oe)\n2i−1] ≥ Var[∆x(oe)\n2i ], i.e., the backpropagation through FFN sublayers does\nnot suffer from gradient vanishing. In Post-LN encoders, the output of FFN sublayers is calculated as\nx(oe)\n2i = fLN(b(oe)\n2i ) where b(oe)\n2i = x(oe)\n2i−1 + max(0, x(oe)\n2i−1W(1))W(2). Since at initialization, W(1) and\nW(2) are independently randomized by symmetric distributions, we have E[b(oe)\n2i ] = 0 and\nx(oe)\n2i = x(oe)\n2i−1 + max(x(oe)\n2i−1W(1), 0)W(2)\nσb,2i\nwhere σ2\nb,2i = Var[b(oe)\n2i ]. Referring to the dimension of W(1) as D ×Df , He et al. (2015) establishes that\nVar[max(x(oe)\n2i−1W(1), 0)W(2)] = 1\n2DDf Var[w(1)] Var[w(2)] Var[x(oe)\n2i−1].\nSince in Post-LN, x(oe)\n2i−1 is the output of layer norm, we have Var[x(oe)\n2i−1] = 1. Thus,\nσ2\nb,2i = Var[b(oe)\n2i ] = Var[x(oe)\n2i−1] + Var[max(x(oe)\n2i−1W(1), 0)W(2)]\n= 1 + 1\n2DDf Var[w(1)] Var[w(2)]. (1)\nAssuming different terms are also independent in the backpropagation, we have\nVar[∆x(oe)\n2i−1] ≥ Var[ 1\nσb,2i\n(∆x(oe)\n2i + ∆x(oe)\n2i\n∂ max(x(oe)\n2i−1W(1), 0)W(2)\n∂x(oe)\n2i−1\n)].\nAt initialization, He et al. (2015) establishes that\nVar[∆x(oe)\n2i\n∂ max(x(oe)\n2i−1W(1), 0)W(2)\n∂x(oe)\n2i−1\n] = 1\n2DDf Var[w(1)] Var[w(2)] Var[∆x(oe)\n2i ].\nTherefore, we have\nVar[∆x(oe)\n2i−1] ≥ 1\nσ2\nb,2i\n(1 + 1\n2DDf Var[w(1)] Var[w(2)]) Var[∆x(oe)\n2i ]. (2)\nCombining Equation 1 with Equation 2, we have\nVar[∆x(oe)\n2i−1] ≥ Var[∆x(oe)\n2i ] (3)\nwhich shows the backpropagation through FFN sublayers does not suffer from gradient vanishing.\nNow we proceed to prove that, Var[∆x(oe)\n2i−2] ≥ Var[∆x(oe)\n2i−1], i.e., the backpropagation through\nSelf-Attention sublayers do not suffer from gradient vanishing. In Post-LN encoders, the output of Self-\nAttention sublayers are calculated as x(oe)\n2i−1 = fLN(b(oe)\n2i−1) where b(oe)\n2i−1 = x(oe)\n2i−2 + a(oe)\n2i−1 and a(od)\n2i−1 =\nP\nh fs(x(oe)\n2i−2W(Q)\nh W(K)\nh xT (oe)\n2i−2)x(oe)\n2i−2W(V1)\nh W(V2)\nh . At initialization, since W(Q), W(K), W(V1), and\nW(V2) are independently randomized by symmetric distributions, we have E[b(od)\n2i−1] = 0, thus x(oe)\n2i−1 =\nb(oe)\n2i−1\nσb,2i−1\n, where σ2\nb,2i−1 = Var[b(oe)\n2i−1] = Var[x(oe)\n2i−2] + Var[a(oe)\n2i−1].\nReferring E[fs2(x(oe)\n2i−2W(Q)\nh W(K)\nh xT (oe)\n2i−2)] as Ph, we have\nVar[a(od)\n2i−1] = Var[x(oe)\n2i−2W(V1)\nh W(V2)\nh ]HPh.\nSimilar to He et al. (2015), we have\nVar[x(oe)\n2i−2W(V1)\nh W(V2)\nh ] = D2\nH Var[x(oe)\n2i−2] Var[w(V1)] Var[w(V2)].\nSince x(oe)\n2i−2 is the output of layer norm, we have Var[x(oe)\n2i−2] = 1. Thus,\nσ2\nb,2i−1 = 1 + D2Ph Var[x(oe)\n2i−2] Var[w(V1)] Var[w(V2)]. (4)\nIn the backpropagation, we have\nVar[∆x(oe)\n2i−2] ≥ Var[ 1\nσb,2i−1\n(∆x(oe)\n2i−1 + ∆x(oe)\n2i−1\nX\nh\n∂fs(x(oe)\n2i−2W(Q)\nh W(K)\nh xT (oe)\n2i−2)x(oe)\n2i−2W(V1)\nh W(V2)\nh\n∂x(oe)\n2i−2\n)]\n≥ 1\nσ2\nb,2i−1\n(Var[∆x(oe)\n2i−1] + Var[∆x(oe)\n2i−1\nX\nh\nfs(x(oe)\n2i−2W(Q)\nh W(K)\nh xT (oe)\n2i−2)∂x(oe)\n2i−2W(V1)\nh W(V2)\nh\n∂x(oe)\n2i−2\n])\nAt initialization, we assume ∆x(oe)\n2i−1 and model parameters are independent (He et al., 2015), thus\nVar[∆x(oe)\n2i−1\nX\nh\nfs(x(oe)\n2i−2W(Q)\nh W(K)\nh xT (oe)\n2i−2)∂x(oe)\n2i−2W(V1)\nh W(V2)\nh\n∂x(oe)\n2i−2\n]\n=D2Ph Var[∆x(oe)\n2i−1] Var[w(V1)] Var[w(V2)]\n0 50 100\n10−2\n10−1\n100\nRelative Gradient\nNorm\n0 50 100\n 0 50 100\n 0 50 100\nW(Q)\n W(V1)\n W(V2)\n10−1\n100\nRelative Parameter\nUpdate Norm\nW(K)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\nAlthough the gradient distribution is unbalanced ( e.g., W(V 1) and W(V 2) have larger gradients than W(K) and W(Q)),\nadaptive optimizers lead to consistent update magnitudes for diﬀerent parameters.\nEpoch # (iterations over the training set)\nFigure 11: Relative Norm of Gradient (∆Wi, where Wi is the checkpoint of i-th epoch) and Update (|Wi+1 − Wi|)\nof Self-Attention Parameters in 12-Layer Pre-LN.\nTherefore, we have\nVar[∆x(oe)\n2i−2] ≥ 1\nσ2\nb,2i−1\n(1 + D2Ph Var[w(V1)] Var[w(V2)]) Var[∆x(oe)\n2i−1]. (5)\nIntegrating Equation 4 with Equation 5, we have\nVar[∆x(oe)\n2i−2] ≥ Var[∆x(oe)\n2i−1]. (6)\nCombining Equation 3 and Equation 6, we have Var[∆xi−1] ≥ Var[∆xi].\nA.3 Post-LN Decoder Analysis\nIn Post-LN, the Encoder-Attention sub-layer suffers from gradient vanishing. The Encoder-Attention\nsub-layer calculates outputs as x(od)\n3i−1 = fLN(b(od)\n3i−1) where b(od)\n3i−1 = x(od)\n3i−2 + a(od)\n3i−1 and a(od)\n3i−1 =\nP\nh fs(x(od)\n3i−2W(Q)\nh W(K)\nh xT (oe))x(oe)W(V1)\nh W(V2)\nh . Here x(oe) is encoder outputs and fs is the row-wise\nsoftmax function. In the backpropagation, ∆x(od)\n3i−2 ≈\n∆x(od)\n3i−1\nσb,3i−1\n(1 +\n∂a(od)\n3i−1\n∂x(od)\n3i−2\n). All of the backpropagations\nfrom a(od)\n3i−1 to x(od)\n3i−2 went through the softmax function, we have Var[\n∂a(od)\n3i−1\n∂x(od)\n3i−2\n] + 1 ≤ σ2\nb,3i−1. Thus, those\nbackpropagations suffer from gradient vanishing. This observation is further verified in Figure 3, as the\nencoder attention bars (gradients of encoder-attention outputs) are always shorter than self-attention bars\n(gradients of encoder-attention inputs), while adjacent self-attention bars and fully connected bars usually\nhave the same length.\nA.4 Distributes of Unbalanced Gradients\nAs in Figure 5 and Figure 11, the gradient distribution of Attention modules is unbalanced even for\nPre-LN. Specifically, parameters within the softmax function (i.e., W(K) and W(V1)) suffer from gradient\nvanishing (i.e., ∂fs(x0,···,xi,···)\n∂xi\n≤ 1) and have smaller gradients than other parameters.\nWith further analysis, we find it is hard to neutralize the gradient vanishing of softmax. Unlike\nconventional non-linear functions like ReLU or sigmoid, softmax has a dynamic input length (i.e., for\nthe sentences with different lengths, inputs of softmax have different dimensions). Although this setting\nallows Attention modules to handle sequential inputs, it restricts them from having stable and consistent\nbackpropagation. Specifically, let us consider the comparison between softmax and sigmoid. For the\nsigmoid function, although its derivation is smaller than 1, this damping effect is consistent for all inputs.\nThus, sigmoid can be neutralized by a larger initialization (Glorot and Bengio, 2010). For softmax, its\ndamping effect is different for different inputs and cannot be neutralized by a static initialization.\nAlso, we observe that adaptive optimizers largely address this issue. Specifically, we calculate the\nnorm of parameter change in consequent epochs (e.g., |W(K)\nt+1 − W(K)\nt | where W(K)\nt is the checkpoint\nsaved after t epochs) and visualize the relative norm (scaled by the largest value in the same network) in\nFigure 11. Comparing the relative norm of parameter gradients and parameter updates, we notice that:\nalthough the gradient distribution is unbalanced, adaptive optimizers successfully assign different learning\nrates to different parameters and lead to consistent update magnitudes. This result explains why the vanilla\nSGD fails for training Transformer (i.e., lacking the ability to handle unbalanced gradient distributions).\nBesides, it implies that the unbalanced gradient distribution (e.g., gradient vanishing) has been mostly\naddressed by adaptive optimizers and may not significantly impact the training instability.\nB Proof of Theorem 2\nHere, we elaborate the derivation for Theorem 2, which establishes the relationship between layer number\nand output fluctuation brought by parameter change.\nTHEOREM 2. — Consider a N-layer Transformer bx = F(bx0, W), where bx0 is the input and W is the\nparameter. If the layer dependency stays the same after a parameter change (i.e., βi,j has the same value\nafter changing W to W∗, where W is randomly initialized and δ = W∗ − W is independent to W), the\noutput change (i.e., Var[F(x0, W) − F(x0, W∗)]) can be estimated as PN\ni=1 β2\ni,iC where C is a constant.\nProof. We refer the module in i sub-layer as ai = Gi(bxi−1, Wi), where bxi = P\nj≤i βi,jbaj is the normal-\nized residual output and bai = ai√Varai\nis the normalized module output. The final output is marked as\nbx = F(x0, W) = P\nj≤N βN,j baj. To simplify the notation, we use the superscript ∗ to indicate variables\nrelated to W∗, e.g., bx∗ = F(x0, W∗) and a∗\ni = Gi(bx∗\ni−1, W∗\ni ).\nAt initialization, all parameters are initialized independently. Thus ∀i ̸= j, bai and baj are independent\nand 1 = Var[ P\nj≤i βi,jbaj] = P\nj≤i β2\ni,j. Also, since k-layer and (k + 1)-layer share the residual\nconnection to previous layers, ∀i, j≤ k we have βi,k\nβj,k\n= βi,k+1\nβj,k+1\n. Thus ∀i ≤ k, β2\ni,k+1 = (1 −β2\nk,k)β2\ni,k and\nVar[bxi − bx∗\ni ] = Var[\nX\nj≤i\nβi,j(baj − ba∗\nj )] =\nX\nj≤i\nβ2\ni,j Var[baj − ba∗\nj ]\n= β2\ni,i Var[bai − ba∗\ni ] + (1− β2\ni,i) Var[bxi − bx∗\ni ]. (7)\nNow, we proceed to analyze Var[bai − ba∗\ni ]. Specifically, we have\nVar[bai − ba∗\ni ] = Var[Gi(bxi−1, Wi) − Gi(bx∗\ni−1, W∗\ni )]\n= Var[Gi(bxi−1, Wi) − Gi(bx∗\ni−1, Wi) + Gi(bx∗\ni−1, W∗\ni ) − Gi(bx∗\ni−1, W∗\ni )]\n= Var[Gi(bxi−1, Wi) − Gi(bx∗\ni−1, Wi)] + Var[Gi(bx∗\ni−1, Wi) − Gi(bx∗\ni−1, W∗\ni )]. (8)\nSince W is randomly initialized, Var[Gi(bx∗\ni−1, Wi) − Gi(bx∗\ni−1, W∗\ni )] should have the same value for\nall layers, thus we use a constant C to refer its value ( C = Var[Gi(bx∗\ni−1, Wi) − Gi(bx∗\ni−1, W∗\ni )] and\nC ≈ |δ|·|∇G i(bx∗\ni−1, Wi)|). As to Var[Gi(bxi−1, Wi)−Gi(bx∗\ni−1, Wi)], since the sub-layer of Transformers\nare mostly using linear weights with ReLU nonlinearity and 1 = Var[Gi(bxi−1, Wi)] = Var[bxi−1], we\nhave Var[Gi(bxi−1, Wi) − Gi(bx∗\ni−1, Wi)] ≈ Var[bxi−1 − bx∗\ni−1]. Thus, we can rewrite Equation 8 and get\nVar[bai − ba∗\ni ] ≈ Var[bxi−1 − bx∗\ni−1] + C\nWith Equation 7, we have\nVar[bxi − bx∗\ni ] = β2\ni,i Var[bai − ba∗\ni ] + (1− β2\ni,i) Var[bxi − bx∗\ni ]\n≈ β2\ni,i(Var[bxi−1 − bx∗\ni−1] + C) + (1− β2\ni,i) Var[bxi − bx∗\ni ]\n= Var[bxi − bx∗\ni ] + β2\ni,iC\nTherefore, we have Var[F(x0, W) − F(x0, W∗)] ≈ PN\ni=1 β2\ni,iC.\nC Admin Implementation Details\nAs introduced in Section 4.3, we introduce a new set of parameters to rescale the module outputs.\nSpecifically, we refer these new parameters as ω and construct the Post-LN sub-layer as:\nxi = fLN(bi), where bi = xi−1 · ωi + fi(xi−1)\nwhere · is the element-wise product.\nAfter training, Admin can be reparameterized as the conventional Post-LN structure (i.e., removing ωi).\nSpecifically, we consider xi = bi\nσb\nγ + ν. Then, for feedforward sub-layers, we have\nbi = xi−1 · ω + max(0, xi−1W(1))W(2), where xi = bi−1\nσb\nγ + ν.\nIt can be reparameterized by changing γ, ν, W(1) to γωi, νωi, 1\nωi\nW(1) respectively, i.e.,\nb′\ni = x′\ni−1 + max(0, x′\ni−1\n1\nωi\nW(1))W(2), where x′\ni−1 = b′\ni−1\nσb\nγωi + νωi.\nFor Self-Attention sub-layers, we have\nbi = xi−1 +\nX\nh\nfs(xi−1W(Q)\nh W(K)\nh xi−1)xi−1W(V1)\nh W(V2)\nh , where xi = bi−1\nσb\nγ + ν.\nIt can be reparameterized by changing γ, ν, W(Q)\nh , W(K)\nh , W(V1)\nh to γωi, νωi, 1\nωi\nW(Q)\nh , 1\nωi\nW(K)\nh\n1\nωi\nW(V1)\nh respectively, i.e.,\nb′\ni = x′\ni−1 +\nX\nh\nfs(x′\ni−1\n1\nωi\nW(Q)\nh W(K)\nh\n1\nωi\nx′\ni−1)x′\ni−1\n1\nωi\nW(V1)\nh W(V2)\nh , where x′\ni−1 = b′\ni−1\nσb\nγωi + νωi.\nFor Encoder-Attention sub-layers, we have\nbi = xi−1 +\nX\nh\nfs(xi−1W(Q)\nh W(K)\nh x·e)x·eW(V1)\nh W(V2)\nh , where xi = bi−1\nσb\nγ + ν.\nIt can be reparameterized by changing γ, ν, W(Q)\nh to γωi, νωi, 1\nωi\nW(Q)\nh respectively, i.e.,\nb′\ni = x′\ni−1 +\nX\nh\nfs(x′\ni−1\n1\nωi\nW(Q)\nh W(K)\nh x·e)x·e 1\nωi\nW(V1)\nh W(V2)\nh , where x′\ni−1 = b′\ni−1\nσb\nγωi + νωi.\nIt is easy to find b′\ni = bi in all three situations.\nFrom the previous analysis, it is easy to find that introducing the additional parameter ωi is equivalent\nto rescale some model parameters. In our experiments on IWSLT14 De-En, we find that directly rescaling\ninitialization parameters can get roughly the same performance with introducing ωi. However, it is not\nvery stable when conducting training in a half-precision manner. Accordingly, we choose to add new\nparameters ωi instead of rescaling parameters.\nD Experimental Setup\nOur experiments are based on the implementation from the fairseq package (Ott et al., 2019). As to\npre-processing, we follow the public released script from previous work (Ott et al., 2019; Lu et al., 2020).\nFor WMT’14 datasets, evaluations are conducted on the provided ‘newstest14‘ file, and more details about\nthem can be found in Bojar et al. (2014). For the IWSLT’14 De-En dataset, more analysis and details can\nbe found in Cettolo et al. (2014).\nTable 3: ReZero Performance on IWSLT’14 De-En. Models are Transformer-small w. 6-layer encoder & decoder.\nModels Admin Post-LN Pre-LN ReZero ReZero+Post-LN\nBLEU 35.67±0.15 35.64 ±0.23 35.50 ±0.04 33.67 ±0.14 34.67 ±0.08\nTable 4: Performance and model size on WMT’14 En-Fr (AL-BL refers A-layer encoder & B-layer decoder).\nMethods Param. # dim( W(1)) in FFN Enc#-Dec# BLEU\nT5-Base (Raffel et al., 2019) 220 M 512 × 2048 6L-6L 41.2\nT5-Large (Raffel et al., 2019) 770 M 1024 × 4096 12L-12L 41.5\nT5-3B (Raffel et al., 2019) 3 B 1024 × 16384 24L-24L 42.6\nT5-11B (Raffel et al., 2019) 11 B 1024 × 65536 24L-24L 43.4\nTrans.Big-RNMT+ (Chen et al., 2018) 377 M 1024 × 8192 6L-6L 41.12\nDynamicConv (Wu et al., 2019a) 213 M 1024 × 4096 7L-7L 43.2\nDG-Transformer (Wu et al., 2019b) 264 M 1024 × 4096 8L-8L 43.27\nPrime (Zhao et al., 2019) 252 M 1024 × 4096 6L-6L 43.48\nPre-LN (60L–12L) 262 M 512 × 2048 60L-12L 43.10\nAdmin (60L–12L) 262 M 512 × 2048 60L-12L 43.80\nAs to model specifics, we directly adopt Transformer-small configurations on the IWSLT’14 De-En\ndataset and stacks more layers over the Transformer-base model on the WMT’14 En-De and WMT’14 En-\nFr datasets. Specifically, on the IWSLT’14 De-En dataset, we use word embedding with 512 dimensions\nand 6-layer encoder/decoder with 4 heads and 1024 feedforward dimensions; on the WMT’14 En-De\nand WMT’14 En-Fr datasets, we use word embedding with 512 dimension and 8-head encoder/decoder\nwith 2048 hidden dimensions. Label smoothed cross entropy is used as the objective function with an\nuncertainty = 0.1 (Szegedy et al., 2016).\nFor Model training, we use RAdam as the optimizer (Liu et al., 2020a) and adopt almost all hyper-\nparameter settings from Lu et al. (2020). Specifically, for the WMT’14 En-De and WMT’14 En-Fr dataset,\nall dropout ratios (including (activation dropout and attention dropout) are set to 0.1. For the IWSLT’14\nDe-En dataset, after-layer dropout is set to 0.3, and a weight decay of 0.0001 is used. As to optimizer, we\nset (β1, β2) = (0.9, 0.98), use inverse sqrt learning rate scheduler with a warmup phrase (8000 steps on\nthe WMT’14 En-De/Fr dataset, and 6000 steps on the IWSLT’14 De-En dataset). The maximum learning\nrate is set to 1e−3 on the WMT’14 En-De dataset and 7e−4 on the IWSLT’14 De-En and WMT’14 En-Fr\ndatasets. We conduct training for 100 epochs on the WMT’14 En-De dataset, 90 epochs on the IWSLT’14\nDe-En dataset and 50 epochs on the WMT’14 En-Fr dataset, while the last 10 checkpoints are averaged\nbefore inference.\nOn the IWSLT’14 De-En dataset, we conduct training on one NVIDIA GeForce GTX 1080 Ti GPU\nand set the maximum batch size to be 4096. On the WMT’14 En-De dataset, we conduct training on four\nNVIDIA Quadro R8000 GPUs and set maximum batch size (per GPU) as 8196. On the WMT’14 En-Fr\ndataset, we conduct training with the Nvidia DGX-2 server (6L-6L uses 4 NVIDIA TESLA V100 GPUs\nand 60L-16L uses 16 NVIDIA TESLA V100 GPUs) and set the maximum batch size (per GPU) as 8000\nfor 6L-6L and 5000 for 60L-16L. On the IWSLT’14 De-En dataset, Transformer-small models (w. 37\nM Param.) take a few hours to train. On the WMT’14 En-De dataset, 6L-6L models (w. 63 M Param.)\ntake ∼ 1 day to train, 12L-12L (w. 107M Param.) models take ∼ 2 days to train, and 18L-18L (w. 151M\nParam.) models take ∼ 3 days to train. On the WMT’14 En-Fr dataset, 6L-6L models (w. 67 M Param.)\ntakes ∼ 2 days to train, and 60L-12L models (w. 262M Param.) takes ∼ 2.5 days to train. All training\nis conducted in half-precision with dynamic scaling (with a 256-update scaling window and a 0.03125\nminimal scale). All our implementations and pre-trained models would be released publicly.\nE Comparison to ReZero\nHere, we first conduct comparisons with ReZero (Bachlechner et al., 2020) under two configurations–\nthe first employs the original ReZero model, and the second adds layer normalizations in a Post-LN\nmanner. As summarized in Table 3, the ReZero initialization leads to a performance drop, no matter\nlayer normalization is used or not. It verifies our intuition that over small dependency restricts the model\npotential. At the same time, we find that adding layer normalization to ReZero helps to improve the\nperformance. Intuitively, as dropout plays a vital role in regularizing Transformers, layer normalization\nhelps to not only stabilize training but alleviate the impact of turning off dropouts during the inference.\nF Performance on the WMT’14 En-Fr\nTo explore the potential of Admin, we conduct experiments with 72-layer Transformers on the WMT’14\nEn-Fr dataset (with a 60-layer encoder and 12-layer decoder, we add less layers to decoder to encourage\nthe model to rely more on the source context).\nAs in Table 4, Admin (60L–12L) achieves a BLEU score of 43.80, the new state-of-the-art on this\nlong-standing benchmark. This model has a 60-layer encoder and a 12-layer decoder, which is significantly\ndeeper than other baselines. Still, since the number of parameters increases in a quadratic speed with\nregard to hidden dimensions and a linear speed with regard to layer numbers, our model has roughly the\nsame number of parameters with other baselines. It is worth mentioning that Admin even achieves better\nperformance than all variants of pre-trained T5 models, which demonstrates the great potential of our\nproposed method. Also, Admin achieves a better performance than Pre-LN (60L–12L), which further\nverifies that the Pre-LN architecture restricts deep models’ potential.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7985134720802307
    },
    {
      "name": "Residual",
      "score": 0.6827976703643799
    },
    {
      "name": "Computer science",
      "score": 0.6264784336090088
    },
    {
      "name": "Dependency (UML)",
      "score": 0.5456110835075378
    },
    {
      "name": "Training set",
      "score": 0.5123440027236938
    },
    {
      "name": "Implementation",
      "score": 0.48298752307891846
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43095868825912476
    },
    {
      "name": "Root cause",
      "score": 0.41330787539482117
    },
    {
      "name": "Machine learning",
      "score": 0.32428473234176636
    },
    {
      "name": "Algorithm",
      "score": 0.29226911067962646
    },
    {
      "name": "Voltage",
      "score": 0.16536816954612732
    },
    {
      "name": "Electrical engineering",
      "score": 0.16452184319496155
    },
    {
      "name": "Reliability engineering",
      "score": 0.1460513174533844
    },
    {
      "name": "Engineering",
      "score": 0.11724892258644104
    },
    {
      "name": "Software engineering",
      "score": 0.11686357855796814
    }
  ],
  "institutions": [],
  "cited_by": 28
}